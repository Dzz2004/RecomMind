#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸ä¾èµ–langchainçš„RAGå·¥ä½œæµç³»ç»Ÿ
åŸºäºTransformerså®ç°å®Œæ•´çš„RAGåŠŸèƒ½
"""

import os
import json
import torch
import chromadb
from typing import List, Dict, Any, Optional, Callable
from datetime import datetime
from dataclasses import dataclass
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TextIteratorStreamer
from threading import Thread
import numpy as np
from sentence_transformers import SentenceTransformer
import sys

# æ·»åŠ  dzz_retrieval è·¯å¾„ä»¥ä¾¿å¯¼å…¥
dzz_path = os.path.join(os.path.dirname(__file__), 'dzz_retrieval')
if os.path.exists(dzz_path):
    sys.path.insert(0, dzz_path)
    try:
        from rank_chunks_by_semantic import rank_chunks_by_description  # type: ignore
        print("âœ… æˆåŠŸå¯¼å…¥ rank_chunks_by_semantic")
    except ImportError:
        rank_chunks_by_description = None
        print("âš ï¸ è­¦å‘Š: æ— æ³•å¯¼å…¥ rank_chunks_by_semanticï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ£€ç´¢æ–¹æ³•")
else:
    rank_chunks_by_description = None
    print("âš ï¸ è­¦å‘Š: dzz_retrieval ç›®å½•ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ£€ç´¢æ–¹æ³•")

# ==================== æ•°æ®æ¨¡å‹ ====================

@dataclass
class ConversationMessage:
    """å¯¹è¯æ¶ˆæ¯æ¨¡å‹"""
    role: str  # "user" æˆ– "assistant"
    content: str
    timestamp: datetime
    metadata: Optional[Dict[str, Any]] = None

@dataclass
class RetrievedChunk:
    """æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ"""
    content: str
    source: str
    filename: str
    relative_path: str
    extension: str
    score: float
    metadata: Dict[str, Any]

@dataclass
class RetrievalSuggestion:
    """æ£€ç´¢å»ºè®®æ¨¡å‹"""
    original_query: str
    intent: str
    confidence: float
    search_keywords: List[str]
    suggested_queries: List[str]
    reasoning: str

@dataclass
class WorkflowResponse:
    """å·¥ä½œæµå“åº”æ¨¡å‹"""
    user_query: str
    retrieval_suggestion: Optional[RetrievalSuggestion]
    retrieved_chunks: List[RetrievedChunk]
    llm_response: str
    conversation_history: List[ConversationMessage]
    timestamp: datetime

# ==================== æ£€ç´¢å»ºè®®ç”Ÿæˆå™¨ ====================

class RetrievalSuggester:
    """æ£€ç´¢å»ºè®®ç”Ÿæˆå™¨"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def generate_suggestion(self, user_query: str, conversation_history: List[ConversationMessage]) -> RetrievalSuggestion:
        """ç”Ÿæˆæ£€ç´¢å»ºè®®"""
        
        # åˆ†æå¯¹è¯ä¸Šä¸‹æ–‡
        context_analysis = self._analyze_conversation_context(conversation_history)
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = self._create_suggestion_prompt(user_query, conversation_history, context_analysis)
        
        # ç”Ÿæˆå»ºè®®
        response = self._generate_response_with_history(system_prompt, user_query, conversation_history)
        print(f"æ£€ç´¢å»ºè®®ä¸­é—´è¿‡ç¨‹: {response}")
        
        # è§£æå“åº”
        suggestion = self._parse_response(user_query, response)
        
        # åå¤„ç†ï¼šåŸºäºå†å²è®°å½•ä¼˜åŒ–å»ºè®®
        suggestion = self._post_process_suggestion(suggestion, conversation_history)
        
        return suggestion
    
    def _create_suggestion_prompt(self, user_query: str, conversation_history: List[ConversationMessage], context_analysis: dict) -> str:
        """åˆ›å»ºæ£€ç´¢å»ºè®®æç¤ºè¯"""
        
        prompt = f"""
ä½ æ˜¯ä¸€åâ€œæ“ä½œç³»ç»Ÿè¯¾ç¨‹å†…å®¹æ™ºèƒ½æ£€ç´¢åŠ©æ‰‹â€ï¼Œä½ çš„ä»»åŠ¡æ˜¯ï¼šåˆ†æç”¨æˆ·çš„çœŸå®æŸ¥è¯¢æ„å›¾ï¼Œå¹¶ä¸º**RAGå‘é‡æ£€ç´¢ç³»ç»Ÿ**ç”Ÿæˆå¯ç”¨çš„ã€è¯­ä¹‰ä¸°å¯Œçš„æ£€ç´¢å»ºè®®ã€‚

è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆè¾“å‡ºï¼š
å¯¹è¯ä¸Šä¸‹æ–‡åˆ†æ: {context_analysis.get('summary', 'æ— ç‰¹æ®Šä¸Šä¸‹æ–‡')}
å†å²å…³é”®è¯: {', '.join(context_analysis.get('keywords', []))}

---

è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼çš„ JSON æ ¼å¼ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š
{{
    "intent": "ç”¨æˆ·æ„å›¾æè¿°",
    "confidence": 0.8,
    "search_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
    "suggested_queries": ["å»ºè®®æŸ¥è¯¢1", "å»ºè®®æŸ¥è¯¢2", "å»ºè®®æŸ¥è¯¢3"],
    "reasoning": "ç”Ÿæˆå»ºè®®çš„æ¨ç†è¿‡ç¨‹"
}}

---

### ç”Ÿæˆè¦æ±‚ï¼š
1. **intent**ï¼šç®€æ´æè¿°ç”¨æˆ·æ„å›¾ï¼ˆå¦‚â€œä¿¡æ¯æŸ¥è¯¢â€ã€â€œæ¦‚å¿µè§£é‡Šâ€ã€â€œæ¯”è¾ƒåˆ†æâ€ã€â€œåŸç†æ¢è®¨â€ã€â€œè€ƒè¯•å‡†å¤‡â€ç­‰ï¼‰
2. **confidence**ï¼šæ¨¡å‹å¯¹æ„å›¾è¯†åˆ«çš„ç½®ä¿¡åº¦ï¼ŒèŒƒå›´ 0~1
3. **search_keywords**ï¼šæå– 3â€“5 ä¸ªæ ¸å¿ƒæ¦‚å¿µæˆ–æœ¯è¯­ï¼Œç»“åˆå†å²è®°å½•å’Œå½“å‰é—®é¢˜ï¼ˆå…³é”®è¯å°½é‡é€šç”¨ã€è¯¾ç¨‹ç›¸å…³ï¼‰
4. **suggested_queries**ï¼š
   - ç”Ÿæˆ 3â€“5 æ¡ä¼˜åŒ–åçš„æ£€ç´¢æŸ¥è¯¢ï¼Œç”¨äºå‘é‡å¬å›ï¼›
   - æ¯æ¡å»ºè®®åº”æ˜¯å¯¹åŸé—®é¢˜çš„**è¯­ä¹‰æ”¹å†™ã€æ³›åŒ–æˆ–å»¶å±•**ï¼Œè€Œéç®€å•å¤è¿°ï¼›
   - ä¼˜å…ˆåŒ…å«æ ¸å¿ƒå…³é”®è¯ï¼Œç¡®ä¿å¯¹æ•™å­¦å†…å®¹çš„ç›¸å…³æ€§ï¼›
   - é¿å…æ— æ„ä¹‰çŸ­è¯­æˆ–ç”¨æˆ·è¾“å…¥çš„å™ªå£°ï¼ˆå¦‚â€œthinkâ€ã€â€œä¸Šä¸€ä¸ªé—®é¢˜â€ç­‰ï¼‰ã€‚
5. **reasoning**ï¼šç®€è¿°æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬å¦‚ä½•åˆ©ç”¨ä¸Šä¸‹æ–‡ã€å…³é”®è¯æ¥ç”Ÿæˆæ›´å…·å¬å›æ•ˆæœçš„æ£€ç´¢å»ºè®®ã€‚

---

### æ³¨æ„äº‹é¡¹ï¼š
- **æ³¨æ„æ£€ç´¢ç´ æ**ï¼šä½ çš„æ£€ç´¢èŒƒå›´é¢å‘çš„æ˜¯å‘é‡åŒ–åçš„æ“ä½œç³»ç»Ÿè¯¾ç¨‹æ•™æå†…å®¹ï¼Œè€Œéäº’è”ç½‘ï¼Œç”Ÿæˆçš„æ£€ç´¢å»ºè®®éœ€è¦å’Œæ£€ç´¢èŒƒå›´ç›¸é€‚åº”ã€‚
- **ä¸è¦ç›´æ¥å¤è¿°ç”¨æˆ·åŸå§‹ query**ï¼Œè€Œè¦ç”Ÿæˆâ€œè¯­ä¹‰ç­‰ä»·æˆ–æ›´å…·æ£€ç´¢ä»·å€¼â€çš„æŸ¥è¯¢å¥ã€‚
- **æ£€ç´¢å»ºè®®åº”æœ‰ç­–ç•¥æ€§**ï¼šå¯åŒ…æ‹¬åŒä¹‰æ”¹å†™ã€ç»†åŒ–é—®é¢˜ã€æˆ–æ‰©å±•åˆ°ç›¸å…³æ¦‚å¿µã€‚
- **ä»…è¾“å‡º JSONï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬æˆ–è§£é‡Š**ã€‚
"""

        return prompt
    
    def _generate_response_with_history(self, system_prompt: str, user_query: str, conversation_history: List[ConversationMessage]) -> str:
        """ä½¿ç”¨å¯¹è¯å†å²ç”ŸæˆLLMå“åº”"""
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘5è½®ï¼‰
            for msg in conversation_history[-5:]:
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æŸ¥è¯¢
            messages.append({
                "role": "user", 
                "content": user_query
            })
            
            # ä½¿ç”¨tokenizerçš„chat template
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(text, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=100000,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç è¾“å‡º
            response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            
            # æ¸…ç†è¾“å‡º
            response = response.strip()
            
            return response
            
        except Exception as e:
            print(f"ç”Ÿæˆæ£€ç´¢å»ºè®®æ—¶å‡ºé”™: {e}")
            # è¿”å›ä¸€ä¸ªç©ºçš„JSONï¼Œè®©_parse_responseæ–¹æ³•ä½¿ç”¨åŸå§‹æŸ¥è¯¢
            return '{"intent": "ä¿¡æ¯æŸ¥è¯¢", "confidence": 0.5, "search_keywords": [], "suggested_queries": [], "reasoning": "ç”Ÿæˆå¤±è´¥"}'
    
    def _analyze_conversation_context(self, conversation_history: List[ConversationMessage]) -> dict:
        """åˆ†æå¯¹è¯ä¸Šä¸‹æ–‡"""
        if not conversation_history:
            return {"summary": "æ–°å¯¹è¯å¼€å§‹", "keywords": [], "intent_pattern": "æœªçŸ¥"}
        
        # æå–æ‰€æœ‰æ–‡æœ¬
        all_text = ""
        user_queries = []
        assistant_responses = []
        
        for msg in conversation_history[-5:]:  # æœ€è¿‘5è½®å¯¹è¯
            all_text += msg.content + " "
            if msg.role == "user":
                user_queries.append(msg.content)
            else:
                assistant_responses.append(msg.content)
        
        # å…³é”®è¯æå–
        keywords = self._extract_keywords(all_text)
        
        # æ„å›¾æ¨¡å¼åˆ†æ
        intent_pattern = self._analyze_intent_pattern(user_queries)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self._generate_context_summary(user_queries, assistant_responses, keywords)
        
        return {
            "summary": summary,
            "keywords": keywords,
            "intent_pattern": intent_pattern,
            "user_queries": user_queries,
            "assistant_responses": assistant_responses
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        import re
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # åˆ†è¯
        words = text.split()
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {"çš„", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ä½†", "å› ä¸º", "æ‰€ä»¥", "å¦‚æœ", "é‚£ä¹ˆ", 
                     "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "è¿™ä¸ª", "é‚£ä¸ª", "ä¸€ä¸ª", "ä¸€äº›", "å¾ˆå¤š", 
                     "éå¸¸", "å¾ˆ", "å¤ª", "æ›´", "æœ€", "è¿˜", "ä¹Ÿ", "éƒ½", "å°±", "ä¼š", "è¦", "èƒ½", "å¯ä»¥"}
        
        keywords = []
        for word in words:
            if len(word) > 1 and word not in stop_words:
                keywords.append(word)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_keywords = list(set(keywords))[:8]
        return unique_keywords
    
    def _analyze_intent_pattern(self, user_queries: List[str]) -> str:
        """åˆ†æç”¨æˆ·æ„å›¾æ¨¡å¼"""
        if not user_queries:
            return "æœªçŸ¥"
        
        # åˆ†æé—®é¢˜ç±»å‹
        question_patterns = {
            "æ¦‚å¿µè§£é‡Š": ["ä»€ä¹ˆæ˜¯", "å®šä¹‰", "æ¦‚å¿µ", "å«ä¹‰", "æ„æ€"],
            "æ–¹æ³•æ­¥éª¤": ["å¦‚ä½•", "æ€ä¹ˆ", "æ­¥éª¤", "æ–¹æ³•", "æµç¨‹"],
            "åŸå› åˆ†æ": ["ä¸ºä»€ä¹ˆ", "åŸå› ", "ä¸ºä»€ä¹ˆ", "å¯¼è‡´"],
            "æ¯”è¾ƒåˆ†æ": ["åŒºåˆ«", "æ¯”è¾ƒ", "å¯¹æ¯”", "å·®å¼‚"],
            "æ·±å…¥æ¢è®¨": ["è¯¦ç»†", "æ·±å…¥", "å…·ä½“", "æ›´å¤š", "è¿›ä¸€æ­¥"]
        }
        
        last_query = user_queries[-1].lower()
        
        for intent, patterns in question_patterns.items():
            for pattern in patterns:
                if pattern in last_query:
                    return intent
        
        return "ä¿¡æ¯æŸ¥è¯¢"
    
    def _generate_context_summary(self, user_queries: List[str], assistant_responses: List[str], keywords: List[str]) -> str:
        """ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦"""
        if not user_queries:
            return "æ–°å¯¹è¯å¼€å§‹"
        
        # åˆ†æå¯¹è¯é•¿åº¦
        total_length = len(user_queries) + len(assistant_responses)
        
        if total_length <= 2:
            return f"å¯¹è¯åˆšå¼€å§‹ï¼Œç”¨æˆ·è¯¢é—®: {user_queries[-1][:50]}..."
        elif total_length <= 6:
            return f"ç®€çŸ­å¯¹è¯ï¼Œä¸»è¦è®¨è®º: {', '.join(keywords[:3])}"
        else:
            return f"æ·±å…¥å¯¹è¯ï¼Œå·²è¿›è¡Œ{total_length}è½®ï¼Œä¸»è¦è¯é¢˜: {', '.join(keywords[:3])}"
    
    def _post_process_suggestion(self, suggestion: RetrievalSuggestion, conversation_history: List[ConversationMessage]) -> RetrievalSuggestion:
        """åå¤„ç†æ£€ç´¢å»ºè®®"""
        if not conversation_history:
            return suggestion
        
        # ç¡®ä¿suggestion.suggested_queriesä¸ä¸ºç©º
        if not suggestion.suggested_queries:
            print("âš ï¸ è­¦å‘Š: suggested_queriesä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢ä½œä¸ºé»˜è®¤å€¼")
            suggestion.suggested_queries = [suggestion.original_query]
        
        # åŸºäºå†å²è®°å½•ä¼˜åŒ–å…³é”®è¯
        history_keywords = self._extract_keywords(" ".join([msg.content for msg in conversation_history[-3:]]))
        
        # åˆå¹¶å…³é”®è¯
        combined_keywords = list(set(suggestion.search_keywords + history_keywords[:3]))
        suggestion.search_keywords = combined_keywords[:5]  # é™åˆ¶ä¸º5ä¸ª
        
        # ä¼˜åŒ–å»ºè®®æŸ¥è¯¢
        if len(conversation_history) > 1 and suggestion.suggested_queries:
            # å¦‚æœæœ‰å†å²è®°å½•ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡ç›¸å…³çš„æŸ¥è¯¢
            context_queries = []
            for keyword in history_keywords[:2]:
                # å®‰å…¨è®¿é—®åˆ—è¡¨ï¼Œé¿å…index out of range
                if suggestion.suggested_queries and keyword not in suggestion.suggested_queries[0]:
                    context_queries.append(f"{keyword} {suggestion.original_query}")
            
            suggestion.suggested_queries = suggestion.suggested_queries + context_queries[:2]
            suggestion.suggested_queries = suggestion.suggested_queries[:5]  # é™åˆ¶ä¸º5ä¸ª
        
        return suggestion
    
    def _generate_response(self, prompt: str) -> str:
        """ç”ŸæˆLLMå“åº”ï¼ˆå·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨_generate_response_with_historyï¼‰"""
        # è¿™ä¸ªæ–¹æ³•å·²ç»ä¸å†ä½¿ç”¨ï¼Œä¿ç•™æ˜¯ä¸ºäº†å…¼å®¹æ€§
        # å®é™…ä½¿ç”¨çš„æ˜¯ _generate_response_with_history æ–¹æ³•
        return '{"intent": "ä¿¡æ¯æŸ¥è¯¢", "confidence": 0.5, "search_keywords": ["æŸ¥è¯¢"], "suggested_queries": ["æŸ¥è¯¢"], "reasoning": "ä½¿ç”¨é»˜è®¤æŸ¥è¯¢"}'
    
    def _parse_response(self, original_query: str, response: str) -> RetrievalSuggestion:
        """è§£æLLMå“åº”"""
        try:
            # æ¸…ç†å“åº”æ–‡æœ¬
            cleaned_response = self._clean_response_text(response)
            
            # å°è¯•æå–JSON
            import re
            json_matches = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned_response, re.DOTALL)
            
            # æ‰¾åˆ°æœ€å®Œæ•´çš„JSON
            best_json = None
            for json_str in json_matches:
                try:
                    data = json.loads(json_str)
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦å­—æ®µ
                    if all(key in data for key in ["intent", "confidence", "search_keywords", "suggested_queries"]):
                        best_json = data
                        break
                except:
                    continue
            
            if best_json:
                # å¦‚æœsuggested_queriesä¸ºç©ºåˆ—è¡¨ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                suggested_queries = best_json.get("suggested_queries", [original_query])
                if not suggested_queries:
                    suggested_queries = [original_query]
                    best_json["reasoning"] = "JSONè§£ææˆåŠŸä½†å»ºè®®ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢"
                
                # å¦‚æœsearch_keywordsä¸ºç©ºåˆ—è¡¨ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                search_keywords = best_json.get("search_keywords", [original_query])
                if not search_keywords:
                    search_keywords = [original_query]
                
                return RetrievalSuggestion(
                    original_query=original_query,
                    intent=best_json.get("intent", "ä¿¡æ¯æŸ¥è¯¢"),
                    confidence=float(best_json.get("confidence", 0.5)),
                    search_keywords=search_keywords,
                    suggested_queries=suggested_queries,
                    reasoning=best_json.get("reasoning", "è§£ææˆåŠŸ")
                )
            else:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                return RetrievalSuggestion(
                    original_query=original_query,
                    intent="ä¿¡æ¯æŸ¥è¯¢",
                    confidence=0.5,
                    search_keywords=[original_query],
                    suggested_queries=[original_query],
                    reasoning="JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢"
                )
                
        except Exception as e:
            print(f"è§£ææ£€ç´¢å»ºè®®æ—¶å‡ºé”™: {e}")
            return RetrievalSuggestion(
                original_query=original_query,
                intent="ä¿¡æ¯æŸ¥è¯¢",
                confidence=0.5,
                search_keywords=[original_query],
                suggested_queries=[original_query],
                reasoning=f"è§£æé”™è¯¯: {str(e)}"
            )
    
    def _clean_response_text(self, response: str) -> str:
        """æ¸…ç†å“åº”æ–‡æœ¬"""
        import re
        
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        response = re.sub(r'```json\s*', '', response)
        response = re.sub(r'```\s*', '', response)
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œå’Œæ¢è¡Œ
        response = re.sub(r'\n\s*\n', '\n', response)
        
        # ç§»é™¤Human:å’ŒAI:æ ‡è®°
        response = re.sub(r'Human:\s*', '', response)
        response = re.sub(r'AI:\s*', '', response)
        
        return response.strip()

# ==================== æºç æ£€ç´¢å»ºè®®ç”Ÿæˆå™¨ ====================

class CodeRetrievalSuggester:
    """æºç æ£€ç´¢å»ºè®®ç”Ÿæˆå™¨"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def generate_suggestion(self, user_query: str, conversation_history: List[ConversationMessage]) -> RetrievalSuggestion:
        """ç”Ÿæˆæºç æ£€ç´¢å»ºè®®"""
        
        # åˆ†æå¯¹è¯ä¸Šä¸‹æ–‡
        context_analysis = self._analyze_conversation_context(conversation_history)
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = self._create_suggestion_prompt(user_query, conversation_history, context_analysis)
        
        # ç”Ÿæˆå»ºè®®
        response = self._generate_response_with_history(system_prompt, user_query, conversation_history)
        print(f"æºç æ£€ç´¢å»ºè®®ä¸­é—´è¿‡ç¨‹: {response}")
        
        # è§£æå“åº”
        suggestion = self._parse_response(user_query, response)
        
        # åå¤„ç†ï¼šåŸºäºå†å²è®°å½•ä¼˜åŒ–å»ºè®®
        suggestion = self._post_process_suggestion(suggestion, conversation_history)
        
        return suggestion
    
    def _create_suggestion_prompt(self, user_query: str, conversation_history: List[ConversationMessage], context_analysis: dict) -> str:
        """åˆ›å»ºæºç æ£€ç´¢å»ºè®®æç¤ºè¯"""
        
        prompt = f"""
ä½ æ˜¯ä¸€å"æºä»£ç æ™ºèƒ½æ£€ç´¢åŠ©æ‰‹"ï¼Œä½ çš„ä»»åŠ¡æ˜¯ï¼šåˆ†æç”¨æˆ·çš„çœŸå®æŸ¥è¯¢æ„å›¾ï¼Œå¹¶ä¸º**æºç RAGå‘é‡æ£€ç´¢ç³»ç»Ÿ**ç”Ÿæˆå¯ç”¨çš„ã€è¯­ä¹‰ä¸°å¯Œçš„æ£€ç´¢å»ºè®®ã€‚

è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆè¾“å‡ºï¼š
å¯¹è¯ä¸Šä¸‹æ–‡åˆ†æ: {context_analysis.get('summary', 'æ— ç‰¹æ®Šä¸Šä¸‹æ–‡')}
å†å²å…³é”®è¯: {', '.join(context_analysis.get('keywords', []))}

---

è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼çš„ JSON æ ¼å¼ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š
{{
    "intent": "ç”¨æˆ·æ„å›¾æè¿°",
    "confidence": 0.8,
    "search_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
    "suggested_queries": ["å»ºè®®æŸ¥è¯¢1", "å»ºè®®æŸ¥è¯¢2", "å»ºè®®æŸ¥è¯¢3"],
    "reasoning": "ç”Ÿæˆå»ºè®®çš„æ¨ç†è¿‡ç¨‹"
}}

---

### ç”Ÿæˆè¦æ±‚ï¼š
1. **intent**ï¼šç®€æ´æè¿°ç”¨æˆ·æ„å›¾ï¼ˆå¦‚"å‡½æ•°æŸ¥æ‰¾"ã€"ç±»å®šä¹‰æŸ¥æ‰¾"ã€"APIä½¿ç”¨ç¤ºä¾‹"ã€"å®ç°åŸç†"ã€"ä»£ç é€»è¾‘åˆ†æ"ç­‰ï¼‰
2. **confidence**ï¼šæ¨¡å‹å¯¹æ„å›¾è¯†åˆ«çš„ç½®ä¿¡åº¦ï¼ŒèŒƒå›´ 0~1
3. **search_keywords**ï¼šæå– 3â€“5 ä¸ªæ ¸å¿ƒæ¦‚å¿µæˆ–æœ¯è¯­ï¼Œç»“åˆå†å²è®°å½•å’Œå½“å‰é—®é¢˜ï¼ˆå…³é”®è¯åº”åŒ…å«å‡½æ•°åã€ç±»åã€APIåç§°ã€æŠ€æœ¯æœ¯è¯­ç­‰ï¼‰
4. **suggested_queries**ï¼š
   - ç”Ÿæˆ 3â€“5 æ¡ä¼˜åŒ–åçš„æ£€ç´¢æŸ¥è¯¢ï¼Œç”¨äºå‘é‡å¬å›æºç ï¼›
   - æ¯æ¡å»ºè®®åº”æ˜¯å¯¹åŸé—®é¢˜çš„**è¯­ä¹‰æ”¹å†™ã€æ³›åŒ–æˆ–å»¶å±•**ï¼Œè€Œéç®€å•å¤è¿°ï¼›
   - ä¼˜å…ˆåŒ…å«æ ¸å¿ƒå…³é”®è¯ï¼ˆå‡½æ•°åã€ç±»åã€APIã€æŠ€æœ¯æœ¯è¯­ç­‰ï¼‰ï¼Œç¡®ä¿å¯¹æºä»£ç çš„ç›¸å…³æ€§ï¼›
   - é¿å…æ— æ„ä¹‰çŸ­è¯­æˆ–ç”¨æˆ·è¾“å…¥çš„å™ªå£°ï¼ˆå¦‚"think"ã€"ä¸Šä¸€ä¸ªé—®é¢˜"ç­‰ï¼‰ï¼›
   - è€ƒè™‘ä»£ç æ£€ç´¢çš„ç‰¹æ®Šæ€§ï¼Œå¯ä»¥åŒ…å«å‡½æ•°è°ƒç”¨ã€æ•°æ®ç»“æ„ã€ç®—æ³•åç§°ç­‰æŠ€æœ¯æœ¯è¯­ã€‚
5. **reasoning**ï¼šç®€è¿°æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬å¦‚ä½•åˆ©ç”¨ä¸Šä¸‹æ–‡ã€å…³é”®è¯æ¥ç”Ÿæˆæ›´å…·å¬å›æ•ˆæœçš„æ£€ç´¢å»ºè®®ã€‚

---

### æ³¨æ„äº‹é¡¹ï¼š
- **æ³¨æ„æ£€ç´¢ç´ æ**ï¼šä½ çš„æ£€ç´¢èŒƒå›´é¢å‘çš„æ˜¯å‘é‡åŒ–åçš„æºä»£ç å†…å®¹ï¼Œè€Œéæ•™ææˆ–æ–‡æ¡£ï¼Œç”Ÿæˆçš„æ£€ç´¢å»ºè®®éœ€è¦å’Œæºç æ£€ç´¢èŒƒå›´ç›¸é€‚åº”ã€‚
- **ä¸è¦ç›´æ¥å¤è¿°ç”¨æˆ·åŸå§‹ query**ï¼Œè€Œè¦ç”Ÿæˆ"è¯­ä¹‰ç­‰ä»·æˆ–æ›´å…·æ£€ç´¢ä»·å€¼"çš„æŸ¥è¯¢å¥ã€‚
- **æ£€ç´¢å»ºè®®åº”æœ‰ç­–ç•¥æ€§**ï¼šå¯åŒ…æ‹¬åŒä¹‰æ”¹å†™ã€ç»†åŒ–é—®é¢˜ã€æˆ–æ‰©å±•åˆ°ç›¸å…³æ¦‚å¿µã€‚
- **æºç æ£€ç´¢ç‰¹ç‚¹**ï¼šè€ƒè™‘ä»£ç æ£€ç´¢çš„ç‰¹æ®Šæ€§ï¼Œå¯ä»¥åŒ…å«ï¼š
  - å‡½æ•°åã€ç±»åã€å˜é‡åç­‰æ ‡è¯†ç¬¦
  - APIè°ƒç”¨æ¨¡å¼
  - æ•°æ®ç»“æ„åç§°
  - ç®—æ³•æˆ–è®¾è®¡æ¨¡å¼åç§°
  - æŠ€æœ¯æ ˆç›¸å…³æœ¯è¯­
- **ä»…è¾“å‡º JSONï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬æˆ–è§£é‡Š**ã€‚
"""
        
        return prompt
    
    def _generate_response_with_history(self, system_prompt: str, user_query: str, conversation_history: List[ConversationMessage]) -> str:
        """ä½¿ç”¨å¯¹è¯å†å²ç”ŸæˆLLMå“åº”"""
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘5è½®ï¼‰
            for msg in conversation_history[-5:]:
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æŸ¥è¯¢
            messages.append({
                "role": "user", 
                "content": user_query
            })
            
            # ä½¿ç”¨tokenizerçš„chat template
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(text, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=100000,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç è¾“å‡º
            response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            
            # æ¸…ç†è¾“å‡º
            response = response.strip()
            
            return response
            
        except Exception as e:
            print(f"ç”Ÿæˆæºç æ£€ç´¢å»ºè®®æ—¶å‡ºé”™: {e}")
            # è¿”å›ä¸€ä¸ªç©ºçš„JSONï¼Œè®©_parse_responseæ–¹æ³•ä½¿ç”¨åŸå§‹æŸ¥è¯¢
            return '{"intent": "å‡½æ•°æŸ¥æ‰¾", "confidence": 0.5, "search_keywords": [], "suggested_queries": [], "reasoning": "ç”Ÿæˆå¤±è´¥"}'
    
    def _analyze_conversation_context(self, conversation_history: List[ConversationMessage]) -> dict:
        """åˆ†æå¯¹è¯ä¸Šä¸‹æ–‡"""
        if not conversation_history:
            return {"summary": "æ–°å¯¹è¯å¼€å§‹", "keywords": [], "intent_pattern": "æœªçŸ¥"}
        
        # æå–æ‰€æœ‰æ–‡æœ¬
        all_text = ""
        user_queries = []
        assistant_responses = []
        
        for msg in conversation_history[-5:]:  # æœ€è¿‘5è½®å¯¹è¯
            all_text += msg.content + " "
            if msg.role == "user":
                user_queries.append(msg.content)
            else:
                assistant_responses.append(msg.content)
        
        # å…³é”®è¯æå–ï¼ˆé’ˆå¯¹ä»£ç ï¼‰
        keywords = self._extract_keywords(all_text)
        
        # æ„å›¾æ¨¡å¼åˆ†æ
        intent_pattern = self._analyze_intent_pattern(user_queries)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self._generate_context_summary(user_queries, assistant_responses, keywords)
        
        return {
            "summary": summary,
            "keywords": keywords,
            "intent_pattern": intent_pattern,
            "user_queries": user_queries,
            "assistant_responses": assistant_responses
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆé’ˆå¯¹ä»£ç æ£€ç´¢ï¼‰"""
        import re
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # åˆ†è¯
        words = text.split()
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {"çš„", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ä½†", "å› ä¸º", "æ‰€ä»¥", "å¦‚æœ", "é‚£ä¹ˆ", 
                     "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "è¿™ä¸ª", "é‚£ä¸ª", "ä¸€ä¸ª", "ä¸€äº›", "å¾ˆå¤š", 
                     "éå¸¸", "å¾ˆ", "å¤ª", "æ›´", "æœ€", "è¿˜", "ä¹Ÿ", "éƒ½", "å°±", "ä¼š", "è¦", "èƒ½", "å¯ä»¥",
                     "æŸ¥æ‰¾", "æ‰¾åˆ°", "æœç´¢", "æ£€ç´¢", "æºç ", "ä»£ç "}
        
        keywords = []
        for word in words:
            if len(word) > 1 and word not in stop_words:
                # ä¼˜å…ˆä¿ç•™å¯èƒ½çš„æŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚é©¼å³°å‘½åã€ä¸‹åˆ’çº¿å‘½åç­‰ï¼‰
                if re.match(r'^[A-Z][a-zA-Z0-9]*$', word) or re.match(r'^[a-z_][a-z0-9_]*$', word):
                    keywords.append(word)
                elif word not in stop_words:
                    keywords.append(word)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_keywords = list(set(keywords))[:8]
        return unique_keywords
    
    def _analyze_intent_pattern(self, user_queries: List[str]) -> str:
        """åˆ†æç”¨æˆ·æ„å›¾æ¨¡å¼"""
        if not user_queries:
            return "æœªçŸ¥"
        
        # åˆ†æé—®é¢˜ç±»å‹ï¼ˆé’ˆå¯¹ä»£ç ï¼‰
        question_patterns = {
            "å‡½æ•°æŸ¥æ‰¾": ["å‡½æ•°", "function", "å¦‚ä½•è°ƒç”¨", "å¦‚ä½•ä½¿ç”¨", "æ€ä¹ˆç”¨"],
            "ç±»å®šä¹‰": ["ç±»", "class", "å®šä¹‰", "ç»“æ„"],
            "å®ç°åŸç†": ["åŸç†", "å®ç°", "å¦‚ä½•å®ç°", "æ€æ ·", "æ€ä¹ˆ"],
            "ä»£ç é€»è¾‘": ["é€»è¾‘", "æµç¨‹", "è¿‡ç¨‹", "æ­¥éª¤"],
            "APIæŸ¥æ‰¾": ["API", "æ¥å£", "æ–¹æ³•", "method"],
            "é”™è¯¯æ’æŸ¥": ["é”™è¯¯", "bug", "é—®é¢˜", "å¼‚å¸¸", "ä¸ºä»€ä¹ˆ"]
        }
        
        last_query = user_queries[-1].lower()
        
        for intent, patterns in question_patterns.items():
            for pattern in patterns:
                if pattern in last_query:
                    return intent
        
        return "å‡½æ•°æŸ¥æ‰¾"
    
    def _generate_context_summary(self, user_queries: List[str], assistant_responses: List[str], keywords: List[str]) -> str:
        """ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦"""
        if not user_queries:
            return "æ–°å¯¹è¯å¼€å§‹"
        
        # åˆ†æå¯¹è¯é•¿åº¦
        total_length = len(user_queries) + len(assistant_responses)
        
        if total_length <= 2:
            return f"å¯¹è¯åˆšå¼€å§‹ï¼Œç”¨æˆ·è¯¢é—®: {user_queries[-1][:50]}..."
        elif total_length <= 6:
            return f"ç®€çŸ­å¯¹è¯ï¼Œä¸»è¦è®¨è®º: {', '.join(keywords[:3])}"
        else:
            return f"æ·±å…¥å¯¹è¯ï¼Œå·²è¿›è¡Œ{total_length}è½®ï¼Œä¸»è¦è¯é¢˜: {', '.join(keywords[:3])}"
    
    def _post_process_suggestion(self, suggestion: RetrievalSuggestion, conversation_history: List[ConversationMessage]) -> RetrievalSuggestion:
        """åå¤„ç†æ£€ç´¢å»ºè®®"""
        if not conversation_history:
            return suggestion
        
        # ç¡®ä¿suggestion.suggested_queriesä¸ä¸ºç©º
        if not suggestion.suggested_queries:
            print("âš ï¸ è­¦å‘Š: suggested_queriesä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢ä½œä¸ºé»˜è®¤å€¼")
            suggestion.suggested_queries = [suggestion.original_query]
        
        # åŸºäºå†å²è®°å½•ä¼˜åŒ–å…³é”®è¯
        history_keywords = self._extract_keywords(" ".join([msg.content for msg in conversation_history[-3:]]))
        
        # åˆå¹¶å…³é”®è¯
        combined_keywords = list(set(suggestion.search_keywords + history_keywords[:3]))
        suggestion.search_keywords = combined_keywords[:5]  # é™åˆ¶ä¸º5ä¸ª
        
        # ä¼˜åŒ–å»ºè®®æŸ¥è¯¢
        if len(conversation_history) > 1 and suggestion.suggested_queries:
            # å¦‚æœæœ‰å†å²è®°å½•ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡ç›¸å…³çš„æŸ¥è¯¢
            context_queries = []
            for keyword in history_keywords[:2]:
                # å®‰å…¨è®¿é—®åˆ—è¡¨ï¼Œé¿å…index out of range
                if suggestion.suggested_queries and keyword not in suggestion.suggested_queries[0]:
                    context_queries.append(f"{keyword} {suggestion.original_query}")
            
            suggestion.suggested_queries = suggestion.suggested_queries + context_queries[:2]
            suggestion.suggested_queries = suggestion.suggested_queries[:5]  # é™åˆ¶ä¸º5ä¸ª
        
        return suggestion
    
    def _parse_response(self, original_query: str, response: str) -> RetrievalSuggestion:
        """è§£æLLMå“åº”"""
        try:
            # æ¸…ç†å“åº”æ–‡æœ¬
            cleaned_response = self._clean_response_text(response)
            
            # å°è¯•æå–JSON
            import re
            json_matches = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned_response, re.DOTALL)
            
            # æ‰¾åˆ°æœ€å®Œæ•´çš„JSON
            best_json = None
            for json_str in json_matches:
                try:
                    data = json.loads(json_str)
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦å­—æ®µ
                    if all(key in data for key in ["intent", "confidence", "search_keywords", "suggested_queries"]):
                        best_json = data
                        break
                except:
                    continue
            
            if best_json:
                # å¦‚æœsuggested_queriesä¸ºç©ºåˆ—è¡¨ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                suggested_queries = best_json.get("suggested_queries", [original_query])
                if not suggested_queries:
                    suggested_queries = [original_query]
                    best_json["reasoning"] = "JSONè§£ææˆåŠŸä½†å»ºè®®ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢"
                
                # å¦‚æœsearch_keywordsä¸ºç©ºåˆ—è¡¨ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                search_keywords = best_json.get("search_keywords", [original_query])
                if not search_keywords:
                    search_keywords = [original_query]
                
                return RetrievalSuggestion(
                    original_query=original_query,
                    intent=best_json.get("intent", "å‡½æ•°æŸ¥æ‰¾"),
                    confidence=float(best_json.get("confidence", 0.5)),
                    search_keywords=search_keywords,
                    suggested_queries=suggested_queries,
                    reasoning=best_json.get("reasoning", "è§£ææˆåŠŸ")
                )
            else:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                return RetrievalSuggestion(
                    original_query=original_query,
                    intent="å‡½æ•°æŸ¥æ‰¾",
                    confidence=0.5,
                    search_keywords=[original_query],
                    suggested_queries=[original_query],
                    reasoning="JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢"
                )
                
        except Exception as e:
            print(f"è§£ææºç æ£€ç´¢å»ºè®®æ—¶å‡ºé”™: {e}")
            return RetrievalSuggestion(
                original_query=original_query,
                intent="å‡½æ•°æŸ¥æ‰¾",
                confidence=0.5,
                search_keywords=[original_query],
                suggested_queries=[original_query],
                reasoning=f"è§£æé”™è¯¯: {str(e)}"
            )
    
    def _clean_response_text(self, response: str) -> str:
        """æ¸…ç†å“åº”æ–‡æœ¬"""
        import re
        
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        response = re.sub(r'```json\s*', '', response)
        response = re.sub(r'```\s*', '', response)
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œå’Œæ¢è¡Œ
        response = re.sub(r'\n\s*\n', '\n', response)
        
        # ç§»é™¤Human:å’ŒAI:æ ‡è®°
        response = re.sub(r'Human:\s*', '', response)
        response = re.sub(r'AI:\s*', '', response)
        
        return response.strip()

# ==================== å¯¹è¯ç®¡ç†å™¨ ====================

class ConversationManager:
    """å¯¹è¯å†å²ç®¡ç†å™¨"""
    
    def __init__(self, max_history_length: int = 10):
        self.conversations: List[ConversationMessage] = []
        self.max_history_length = max_history_length
    
    def add_message(self, role: str, content: str, metadata: Optional[dict] = None) -> None:
        message = ConversationMessage(
            role=role,
            content=content,
            timestamp=datetime.now(),
            metadata=metadata or {}
        )
        self.conversations.append(message)
        
        # ä¿æŒå†å²è®°å½•é•¿åº¦é™åˆ¶
        if len(self.conversations) > self.max_history_length:
            self.conversations = self.conversations[-self.max_history_length:]
    
    def get_history(self, last_n: Optional[int] = None) -> List[ConversationMessage]:
        if last_n is None:
            return self.conversations.copy()
        else:
            return self.conversations[-last_n:] if last_n > 0 else []
    
    def get_context_string(self, last_n: Optional[int] = None) -> str:
        history = self.get_history(last_n)
        if not history:
            return ""
        
        context_parts = []
        for msg in history:
            role_name = "ç”¨æˆ·" if msg.role == "user" else "åŠ©æ‰‹"
            context_parts.append(f"{role_name}: {msg.content}")
        
        return "\n".join(context_parts)
    
    def clear(self) -> None:
        self.conversations.clear()
    
    def get_last_user_message(self) -> Optional[ConversationMessage]:
        for msg in reversed(self.conversations):
            if msg.role == "user":
                return msg
        return None

# ==================== ç®€åŒ–çš„RAGå¼•æ“ ====================

class SimpleRAGEngine:
    """ç®€åŒ–çš„RAGå¼•æ“ï¼Œä¸ä¾èµ–langchain"""
    
    def __init__(self, 
                 embedding_model_path: str = "/home/ubuntu/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/",
                 db_path: str = "./vector_db",
                 collection_name: str = "textbook_content",
                 similarity_threshold: float = 0.3):
        """
        åˆå§‹åŒ–RAGå¼•æ“
        
        Args:
            embedding_model_path: åµŒå…¥æ¨¡å‹è·¯å¾„
            db_path: å‘é‡æ•°æ®åº“è·¯å¾„
            collection_name: é›†åˆåç§°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç»“æœå°†è¢«è¿‡æ»¤
        """
        self.embedding_model_path = embedding_model_path
        self.db_path = db_path
        self.collection_name = collection_name
        self.similarity_threshold = similarity_threshold
        
        # åˆ›å»ºæ•°æ®åº“ç›®å½•
        os.makedirs(db_path, exist_ok=True)
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        print("ğŸ”§ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
        self.embedding_model = SentenceTransformer(embedding_model_path)
        print("   âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åˆå§‹åŒ–ChromaDB
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
        self.client = chromadb.PersistentClient(path=db_path)
        
        try:
            self.collection = self.client.get_collection(collection_name)
            print(f"   âœ… æ‰¾åˆ°ç°æœ‰é›†åˆ: {collection_name}")
        except:
            self.collection = self.client.create_collection(collection_name)
            print(f"   âœ… åˆ›å»ºæ–°é›†åˆ: {collection_name}")
        
        # æ˜¾ç¤ºé›†åˆä¿¡æ¯
        info = self.get_collection_info()
        print(f"   ğŸ“Š é›†åˆä¿¡æ¯: {info}")
    
    def get_collection_info(self) -> dict:
        """è·å–é›†åˆä¿¡æ¯"""
        try:
            count = self.collection.count()
            return {"collection_name": self.collection_name, "document_count": count}
        except:
            return {"collection_name": self.collection_name, "document_count": 0}
    
    def add_documents(self, documents: List[str], metadatas: List[dict], ids: List[str]):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        print(f"ğŸ“š æ­£åœ¨æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“...")
        
        # ç”ŸæˆåµŒå…¥
        embeddings = self.embedding_model.encode(documents).tolist()
        
        # æ·»åŠ åˆ°é›†åˆ
        self.collection.add(
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        
        print(f"   âœ… æ–‡æ¡£æ·»åŠ å®Œæˆ")
    
    def search_similar(self, query: str, n_results: int = 5) -> dict:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_model.encode([query]).tolist()
        
        # æœç´¢
        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=n_results
        )
        
        return results
    
    def query(self, query_text: str, top_k: int = 5) -> dict:
        """æŸ¥è¯¢RAGå¼•æ“"""
        results = self.search_similar(query_text, n_results=top_k)
        self._display_search_results(results, query_text)
        processed_results = self._handle_search_results(query_text, results)
        return processed_results
    
    def _display_search_results(self, results, query):
        """æ˜¾ç¤ºæœç´¢ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"æœç´¢æŸ¥è¯¢: '{query}'")
        print(f"{'='*60}")
        
        if results['documents'] and results['documents'][0]:
            print("æœç´¢ç»“æœ:")
            for i, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0], 
                results['metadatas'][0], 
                results['distances'][0]
            )):
                similarity = 1 - distance
                print(f"\nç»“æœ {i+1} (ç›¸ä¼¼åº¦: {similarity:.4f}):")
                print(f"  ç« èŠ‚: {metadata.get('section', 'N/A')}")
                print(f"  æ–‡ä»¶: {metadata.get('file_name', 'N/A')}")
                print(f"  å†…å®¹é¢„è§ˆ: {doc[:200]}...")
                print("-" * 40)
        else:
            print("  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
    
    def _handle_search_results(self, query, results) -> dict:
        """å¤„ç†æœç´¢ç»“æœï¼Œè¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ"""
        processed_results = {
            "query": [query],
            "similarities": [],
            "file_names": [],
            "sections": [],
            "page_ranges": [],
            "contents": []  
        }
        
        if results['documents'] and results['documents'][0]:
            filtered_count = 0
            for doc, metadata, distance in zip(
                results['documents'][0], 
                results['metadatas'][0], 
                results['distances'][0]
            ):
                similarity = 1 - distance
                
                # åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
                if similarity >= self.similarity_threshold:
                    processed_results["similarities"].append(similarity)
                    processed_results["file_names"].append(metadata.get('file_name', 'unknown'))
                    processed_results["sections"].append(metadata.get('section', 'unknown'))
                    processed_results["page_ranges"].append(metadata.get('page_range', ''))
                    processed_results["contents"].append(doc)
                else:
                    filtered_count += 1
            
            if filtered_count > 0:
                print(f"   ğŸ” è¿‡æ»¤äº† {filtered_count} ä¸ªä½ç›¸ä¼¼åº¦ç»“æœ (é˜ˆå€¼: {self.similarity_threshold:.2f})")
        
        return processed_results

# ==================== æºç RAGå¼•æ“ ====================

class CodeRAGEngine:
    """æºç RAGå¼•æ“ï¼Œç”¨äºæ£€ç´¢æºä»£ç """
    
    def __init__(self, 
                 embedding_model_path: str = "/home/ubuntu/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/",
                 db_path: str = "./vector_db",
                 collection_name: str = "source_code",
                 similarity_threshold: float = 0.3):
        """
        åˆå§‹åŒ–æºç RAGå¼•æ“
        
        Args:
            embedding_model_path: åµŒå…¥æ¨¡å‹è·¯å¾„
            db_path: å‘é‡æ•°æ®åº“è·¯å¾„
            collection_name: é›†åˆåç§°ï¼ˆé»˜è®¤ä¸º source_codeï¼‰
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç»“æœå°†è¢«è¿‡æ»¤
        """
        self.embedding_model_path = embedding_model_path
        self.db_path = db_path
        self.collection_name = collection_name
        self.similarity_threshold = similarity_threshold
        
        # åˆ›å»ºæ•°æ®åº“ç›®å½•
        os.makedirs(db_path, exist_ok=True)
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        print("ğŸ”§ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹ï¼ˆæºç æ£€ç´¢ï¼‰...")
        self.embedding_model = SentenceTransformer(embedding_model_path)
        print("   âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åˆå§‹åŒ–ChromaDB
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æºç å‘é‡æ•°æ®åº“...")
        self.client = chromadb.PersistentClient(path=db_path)
        
        try:
            self.collection = self.client.get_collection(collection_name)
            print(f"   âœ… æ‰¾åˆ°ç°æœ‰é›†åˆ: {collection_name}")
        except:
            self.collection = self.client.create_collection(collection_name)
            print(f"   âœ… åˆ›å»ºæ–°é›†åˆ: {collection_name}")
        
        # æ˜¾ç¤ºé›†åˆä¿¡æ¯
        info = self.get_collection_info()
        print(f"   ğŸ“Š é›†åˆä¿¡æ¯: {info}")
    
    def get_collection_info(self) -> dict:
        """è·å–é›†åˆä¿¡æ¯"""
        try:
            count = self.collection.count()
            return {"collection_name": self.collection_name, "document_count": count}
        except:
            return {"collection_name": self.collection_name, "document_count": 0}
    
    def add_documents(self, documents: List[str], metadatas: List[dict], ids: List[str]):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        print(f"ğŸ“š æ­£åœ¨æ·»åŠ  {len(documents)} ä¸ªæºç æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“...")
        
        # ç”ŸæˆåµŒå…¥
        embeddings = self.embedding_model.encode(documents).tolist()
        
        # æ·»åŠ åˆ°é›†åˆ
        self.collection.add(
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        
        print(f"   âœ… æºç æ–‡æ¡£æ·»åŠ å®Œæˆ")
    
    def search_similar(self, query: str, n_results: int = 5) -> dict:
        """æœç´¢ç›¸ä¼¼æºç """
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_model.encode([query]).tolist()
        
        # æœç´¢
        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=n_results
        )
        
        return results
    
    def query(self, query_text: str, top_k: int = 5) -> dict:
        """æŸ¥è¯¢æºç RAGå¼•æ“"""
        results = self.search_similar(query_text, n_results=top_k)
        self._display_search_results(results, query_text)
        processed_results = self._handle_search_results(query_text, results)
        return processed_results
    
    def _display_search_results(self, results, query):
        """æ˜¾ç¤ºæœç´¢ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"æºç æœç´¢æŸ¥è¯¢: '{query}'")
        print(f"{'='*60}")
        
        if results['documents'] and results['documents'][0]:
            print("æœç´¢ç»“æœ:")
            for i, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0], 
                results['metadatas'][0], 
                results['distances'][0]
            )):
                similarity = 1 - distance
                print(f"\nç»“æœ {i+1} (ç›¸ä¼¼åº¦: {similarity:.4f}):")
                print(f"  æ–‡ä»¶è·¯å¾„: {metadata.get('file_path', 'N/A')}")
                print(f"  æ–‡ä»¶å: {metadata.get('file_name', 'N/A')}")
                print(f"  è¡Œå·: {metadata.get('line_range', 'N/A')}")
                print(f"  è¯­è¨€: {metadata.get('language', 'N/A')}")
                print(f"  å†…å®¹é¢„è§ˆ: {doc[:200]}...")
                print("-" * 40)
        else:
            print("  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
    
    def _handle_search_results(self, query, results) -> dict:
        """å¤„ç†æœç´¢ç»“æœï¼Œè¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ"""
        processed_results = {
            "query": [query],
            "similarities": [],
            "file_names": [],
            "file_paths": [],
            "line_ranges": [],
            "languages": [],
            "contents": []  
        }
        
        if results['documents'] and results['documents'][0]:
            filtered_count = 0
            total_count = len(results['documents'][0])
            for doc, metadata, distance in zip(
                results['documents'][0], 
                results['metadatas'][0], 
                results['distances'][0]
            ):
                similarity = 1 - distance
                
                # åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
                if similarity >= self.similarity_threshold:
                    processed_results["similarities"].append(similarity)
                    processed_results["file_names"].append(metadata.get('file_name', 'unknown'))
                    processed_results["file_paths"].append(metadata.get('file_path', 'unknown'))
                    processed_results["line_ranges"].append(metadata.get('line_range', ''))
                    processed_results["languages"].append(metadata.get('language', 'unknown'))
                    processed_results["contents"].append(doc)
                else:
                    filtered_count += 1
            
            if filtered_count > 0:
                print(f"   ğŸ” è¿‡æ»¤äº† {filtered_count}/{total_count} ä¸ªä½ç›¸ä¼¼åº¦ç»“æœ (é˜ˆå€¼: {self.similarity_threshold:.2f})")
            elif total_count == 0:
                # æ£€æŸ¥é›†åˆæ˜¯å¦ä¸ºç©º
                collection_count = self.collection.count()
                if collection_count == 0:
                    print(f"   âš ï¸  é›†åˆ '{self.collection_name}' ä¸ºç©ºï¼Œæ²¡æœ‰å¯æ£€ç´¢çš„ä»£ç æ•°æ®")
                else:
                    print(f"   âš ï¸  æŸ¥è¯¢æœªæ‰¾åˆ°åŒ¹é…ç»“æœï¼ˆé›†åˆä¸­æœ‰ {collection_count} æ¡æ•°æ®ï¼Œä½†ç›¸ä¼¼åº¦éƒ½ä½äºé˜ˆå€¼ {self.similarity_threshold:.2f}ï¼‰")
        else:
            # æ£€æŸ¥é›†åˆæ˜¯å¦ä¸ºç©º
            collection_count = self.collection.count()
            if collection_count == 0:
                print(f"   âš ï¸  é›†åˆ '{self.collection_name}' ä¸ºç©ºï¼Œæ²¡æœ‰å¯æ£€ç´¢çš„ä»£ç æ•°æ®")
            else:
                print(f"   âš ï¸  æŸ¥è¯¢æœªæ‰¾åˆ°åŒ¹é…ç»“æœï¼ˆé›†åˆä¸­æœ‰ {collection_count} æ¡æ•°æ®ï¼‰")
        
        return processed_results

# ==================== æºç æ£€ç´¢å·¥ä½œæµç±» ====================
# ä¼˜å…ˆä½¿ç”¨ dzz_retrieval çš„ RetrievalEngineï¼Œå¤±è´¥åˆ™é™çº§å¤‡ç”¨
try:
    from dzz_retrieval import RetrievalEngine
    print("âœ… æˆåŠŸå¯¼å…¥ RetrievalEngine")
except ImportError:
    RetrievalEngine = None
    print("âš ï¸ è­¦å‘Š: æ— æ³•å¯¼å…¥ RetrievalEngineï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ£€ç´¢æ–¹æ³•")


class CodeRAGWorkflow:
    """æºç æ£€ç´¢å·¥ä½œæµï¼Œç”¨äºæ£€ç´¢æºä»£ç å¹¶ç”Ÿæˆæè¿°"""
    
    def __init__(self, 
                 llm_path: str = "../models--Qwen--Qwen3-8B/snapshots/9c925d64d72725edaf899c6cb9c377fd0709d9c5",
                 embedding_model_path: str = "/home/ubuntu/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/",
                 db_path: str = "./vector_db",
                 similarity_threshold: float = 0.3,
                 chroma_md_path: str = "./dzz_retrieval/chroma_md",
                 top_files: int = 3,
                 top_chunks: int = 5,
                 use_quantization: bool = True):
        # ...existing code...
        self.top_files = top_files
        self.top_chunks = top_chunks
        self.use_quantization = use_quantization
        self.llm_path = llm_path
        self.similarity_threshold = similarity_threshold
        self.chroma_md_path = chroma_md_path
        # åˆå§‹åŒ–ç»„ä»¶
        self.conversation_manager = ConversationManager()
        self.code_rag_engine = CodeRAGEngine(embedding_model_path, db_path, similarity_threshold=similarity_threshold)

        # ä½¿ç”¨ RetrievalEngine ä½œä¸ºä¸»æ£€ç´¢å…¥å£ï¼ˆè¦†ç›– kernel / mm / æœªæ¥æ‰©å±•ï¼‰
        self.retrieval_engine = None
        if RetrievalEngine is not None:
            try:
                self.retrieval_engine = RetrievalEngine(
                    chroma_md_path=chroma_md_path,
                    bge_model_path=embedding_model_path,
                    top_files=top_files,
                    top_chunks=top_chunks,
                )
                self.dzz_collections_info = self.retrieval_engine.get_collections_info()
            except Exception as e:
                print(f"âš ï¸ åˆå§‹åŒ– RetrievalEngine å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ£€ç´¢")
        
        # åˆå§‹åŒ–LLM
        self._load_llm()
        
        # åˆå§‹åŒ–æºç æ£€ç´¢å»ºè®®ç”Ÿæˆå™¨
        self.code_retrieval_suggester = CodeRetrievalSuggester(self.model, self.tokenizer)
        
        # ä¾›åç»­ä½¿ç”¨çš„æ–‡ä»¶æ‘˜è¦ç¼“å­˜
        self._retrieved_file_summaries: Dict[str, str] = {}
        
        print("âœ… æºç æ£€ç´¢å·¥ä½œæµåˆå§‹åŒ–å®Œæˆ!")

    
    def _load_llm(self):
        """åŠ è½½å¤§è¯­è¨€æ¨¡å‹"""
        print("ğŸ¤– æ­£åœ¨åŠ è½½å¤§è¯­è¨€æ¨¡å‹...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(self.llm_path, trust_remote_code=True)
        
        # é…ç½®é‡åŒ–ï¼ˆæ ¹æ® use_quantization å‚æ•°å†³å®šï¼‰
        quantization_config = None
        if self.use_quantization:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
            )
        
        model_kwargs = {
            "device_map": "auto",
            "trust_remote_code": True,
        }
        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.llm_path,
            **model_kwargs
        )
        
        print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}, é‡åŒ–: {'å¼€å¯' if self.use_quantization else 'å…³é—­'}")
    
    def _convert_retrieval_output(self, resp: Dict[str, Any], query_used: str, round_no: int = 1) -> List[RetrievedChunk]:
        """å°† RetrievalEngine.retrieve çš„ç»“æœè½¬ä¸º RetrievedChunk åˆ—è¡¨"""
        chunks: List[RetrievedChunk] = []
        if not resp:
            return chunks
        for f in resp.get("retrieved_files", []):
            src = f.get("source_file", "")
            for ch in f.get("chunks", []):
                file_path = ch.get("file_path", src) or src
                file_path = file_path.replace("\\", "/")
                content = ch.get("content", "")
                if isinstance(content, list):
                    content = "\n".join(content)
                score = ch.get("similarity", ch.get("_score", 0.0))
                start_line = ch.get("start_line")
                end_line = ch.get("end_line")
                line_range = ""
                if start_line or end_line:
                    s = start_line or ""
                    e = end_line or ""
                    line_range = f"{s}-{e}".strip("-")
                chunk = RetrievedChunk(
                    content=content,
                    source=file_path,
                    filename=os.path.basename(file_path) if file_path else "unknown",
                    relative_path=file_path,
                    extension=os.path.splitext(file_path)[1] or ".c",
                    score=score,
                    metadata={
                        "file_name": os.path.basename(file_path) if file_path else "unknown",
                        "file_path": file_path,
                        "line_range": line_range,
                        "start_line": start_line,
                        "end_line": end_line,
                        "function_name": ch.get("function_name", "N/A"),
                        "description": ch.get("description", ""),
                        "chunk_id": ch.get("chunk_id", ""),
                        "similarity": score,
                        "query_used": query_used,
                        "language": ch.get("language", "c"),
                        "round": round_no,
                    }
                )
                chunks.append(chunk)
        return chunks
    
    def _judge_sufficiency_and_suggest_keywords(
        self, 
        user_query: str, 
        retrieved_chunks: List[RetrievedChunk]
    ) -> Dict[str, Any]:
        """
        åˆ¤æ–­å·²æ£€ç´¢çš„å†…å®¹æ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜ï¼Œå¦‚æœä¸è¶³åˆ™ç”Ÿæˆæ–°çš„æ£€ç´¢å…³é”®è¯
        
        Args:
            user_query: ç”¨æˆ·é—®é¢˜
            retrieved_chunks: å·²æ£€ç´¢åˆ°çš„ä»£ç ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            å­—å…¸ï¼ŒåŒ…å«ï¼š
            - is_sufficient: æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜
            - new_keywords: å¦‚æœä¸è¶³ï¼Œæ–°çš„æ£€ç´¢å…³é”®è¯åˆ—è¡¨
            - reasoning: åˆ¤æ–­ç†ç”±
        """
        try:
            # æ„å»ºå·²æ£€ç´¢å†…å®¹çš„æ‘˜è¦
            if retrieved_chunks:
                chunks_summary = "\n\n".join([
                    f"ä»£ç ç‰‡æ®µ{i+1} (æ–‡ä»¶: {chunk.filename}, å‡½æ•°: {chunk.metadata.get('function_name', 'N/A')}):\n{chunk.content[:300]}..."
                    for i, chunk in enumerate(retrieved_chunks)
                ])
            else:
                chunks_summary = "æš‚æ— ç›¸å…³ä»£ç ç‰‡æ®µ"
            
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä»£ç ä¿¡æ¯å……åˆ†æ€§åˆ¤æ–­åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­å·²æ£€ç´¢åˆ°çš„ä»£ç å†…å®¹æ˜¯å¦è¶³ä»¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_query}

å·²æ£€ç´¢åˆ°çš„ç›¸å…³ä»£ç å†…å®¹ï¼š
{chunks_summary}

è¯·åˆ¤æ–­ï¼š
1. å·²æ£€ç´¢åˆ°çš„ä»£ç å†…å®¹æ˜¯å¦è¶³ä»¥å®Œæ•´å›ç­”ç”¨æˆ·é—®é¢˜
2. å¦‚æœä¸è¶³ï¼Œéœ€è¦è¡¥å……å“ªäº›æ–¹é¢çš„ä¿¡æ¯
3. å¦‚æœä¸è¶³ï¼Œè¯·æä¾›2-3ä¸ªæ–°çš„æ£€ç´¢å…³é”®è¯ï¼Œç”¨äºè¿›è¡Œç¬¬äºŒè½®æ£€ç´¢

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "is_sufficient": false,
    "reasoning": "åˆ¤æ–­ç†ç”±",
    "new_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"]
}}

å¦‚æœå†…å®¹å·²è¶³å¤Ÿï¼Œis_sufficientåº”ä¸ºtrueï¼Œnew_keywordså¯ä»¥ä¸ºç©ºæ•°ç»„ã€‚
åªè¾“å‡ºJSONï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚"""
            
            # æ„å»ºæ¶ˆæ¯
            messages = [{"role": "system", "content": system_prompt}]
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # ç”Ÿæˆåˆ¤æ–­ç»“æœ
            inputs = self.tokenizer(text, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=500,
                    temperature=0.3,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            
            # è§£æJSONç»“æœ
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return {
                    "is_sufficient": result.get("is_sufficient", True),
                    "new_keywords": result.get("new_keywords", []),
                    "reasoning": result.get("reasoning", "")
                }
            else:
                # å¦‚æœè§£æå¤±è´¥ï¼Œé»˜è®¤è®¤ä¸ºè¶³å¤Ÿ
                print(f"   âš ï¸ æ— æ³•è§£æå……åˆ†æ€§åˆ¤æ–­ç»“æœï¼Œé»˜è®¤å†…å®¹å·²è¶³å¤Ÿ")
                return {
                    "is_sufficient": True,
                    "new_keywords": [],
                    "reasoning": "æ— æ³•è§£æåˆ¤æ–­ç»“æœï¼Œé»˜è®¤è®¤ä¸ºå†…å®¹å·²è¶³å¤Ÿ"
                }
                
        except Exception as e:
            print(f"   âš ï¸ åˆ¤æ–­å†…å®¹å……åˆ†æ€§æ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶é»˜è®¤è®¤ä¸ºè¶³å¤Ÿ
            return {
                "is_sufficient": True,
                "new_keywords": [],
                "reasoning": f"åˆ¤æ–­è¿‡ç¨‹å‡ºé”™: {str(e)}"
            }
    
    def _retrieve_code_with_keywords(self, keywords: List[str], seen_chunk_ids: set = None) -> List[RetrievedChunk]:
        """
        ä½¿ç”¨å…³é”®è¯åˆ—è¡¨æ£€ç´¢æºç ï¼ˆç”¨äºç¬¬äºŒè½®æ£€ç´¢ï¼Œæ²¿ç”¨ RetrievalEngineï¼‰
        """
        if seen_chunk_ids is None:
            seen_chunk_ids = set()
        all_chunks: List[RetrievedChunk] = []
        for keyword in keywords[:3]:
            try:
                if self.retrieval_engine is not None:
                    resp = self.retrieval_engine.retrieve(keyword)
                    query_chunks = self._convert_retrieval_output(resp, keyword, round_no=2)
                else:
                    print(f"      âš ï¸ å¤‡ç”¨æ£€ç´¢ (å…³é”®è¯: '{keyword}')...")
                    results = self.code_rag_engine.query(keyword, top_k=self.top_files * self.top_chunks)
                    query_chunks = []
                    if results.get("contents"):
                        file_paths = results.get("file_paths", [""] * len(results["contents"]))
                        line_ranges = results.get("line_ranges", [""] * len(results["contents"]))
                        languages = results.get("languages", [""] * len(results["contents"]))
                        for content, similarity, file_name, file_path, line_range, language in zip(
                            results["contents"],
                            results["similarities"],
                            results["file_names"],
                            file_paths,
                            line_ranges,
                            languages,
                        ):
                            chunk = RetrievedChunk(
                                content=content,
                                source=file_path or file_name,
                                filename=file_name,
                                relative_path=file_path or file_name,
                                extension=self._get_extension_from_language(language) or ".txt",
                                score=similarity,
                                metadata={
                                    "file_name": file_name,
                                    "file_path": file_path,
                                    "line_range": line_range,
                                    "language": language,
                                    "similarity": similarity,
                                    "query_used": keyword,
                                    "round": 2,
                                },
                            )
                            query_chunks.append(chunk)
                query_chunks.sort(key=lambda x: x.score, reverse=True)
                for chunk in query_chunks:
                    cid = f"{chunk.metadata.get('file_path','')}_{chunk.metadata.get('chunk_id','')}"
                    if cid in seen_chunk_ids:
                        continue
                    seen_chunk_ids.add(cid)
                    all_chunks.append(chunk)
            except Exception as e:
                print(f"      âŒ ä½¿ç”¨å…³é”®è¯ '{keyword}' æ£€ç´¢æ—¶å‡ºé”™: {e}")
                continue
        all_chunks.sort(key=lambda x: x.score, reverse=True)
        return all_chunks[:5]
    
    def _generate_response_with_context(
        self,
        user_query: str,
        chunks: List[RetrievedChunk],
        token_callback: Optional[Callable[[str], None]] = None,
    ) -> str:
        """ç”Ÿæˆå¸¦æœ‰ä»£ç ä¸Šä¸‹æ–‡çš„å›ç­”"""
        try:
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…å«ä»£ç å—çš„å®é™…å†…å®¹ï¼‰
            context_parts = []
            for i, chunk in enumerate(chunks):
                file_path = chunk.metadata.get('file_path', chunk.filename or 'unknown')
                line_range = chunk.metadata.get('line_range', '')
                function_name = chunk.metadata.get('function_name', '')
                
                # æ„å»ºä»£ç å—æ ‡è¯†ä¿¡æ¯
                chunk_info = f"ä»£ç ç‰‡æ®µ{i+1}"
                if file_path:
                    chunk_info += f" (æ–‡ä»¶: {file_path}"
                    if line_range:
                        chunk_info += f", è¡Œå·: {line_range}"
                    if function_name and function_name != 'N/A':
                        chunk_info += f", å‡½æ•°: {function_name}"
                    chunk_info += ")"
                
                # æ·»åŠ ä»£ç å—çš„å®é™…å†…å®¹
                context_parts.append(
                    f"{chunk_info}:\n{chunk.content}"
                )
            context = "\n\n".join(context_parts)
            
            # è·å–å¯¹è¯å†å²
            conversation_history = self.conversation_manager.get_history()
            
            # æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼Œå¼ºè°ƒè¦åŸºäºä»£ç å†…å®¹å›ç­”
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æåŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„æºä»£ç å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æä¾›çš„æºä»£ç å†…å®¹ï¼š
{context}

è¯·ä»”ç»†åˆ†æè¿™äº›ä»£ç ï¼Œç†è§£å…¶åŠŸèƒ½ã€é€»è¾‘å’Œå®ç°ç»†èŠ‚ï¼Œç„¶ååŸºäºè¿™äº›å®é™…ä»£ç å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä»£ç å†…å®¹ä¸é—®é¢˜ç›¸å…³ï¼Œè¯·è¯¦ç»†è§£é‡Šä»£ç çš„å·¥ä½œåŸç†ï¼›å¦‚æœä»£ç å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜åŸå› ã€‚"""
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘3è½®ï¼‰
            for msg in conversation_history[-3:]:
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æŸ¥è¯¢
            messages.append({
                "role": "user", 
                "content": user_query
            })
            
            # æ ¼å¼åŒ–è¾“å…¥
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # ç¼–ç 
            inputs = self.tokenizer(text, return_tensors="pt")
            
            # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            # è®¾ç½®æµå¼è¾“å‡ºç»„ä»¶
            streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
            generation_kwargs = dict(inputs)
            generation_kwargs.update({
                "max_new_tokens": 3000,
                "temperature": 0.7,
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "streamer": streamer
            })
            response_chunks: List[str] = []

            def generate():
                """åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œç”Ÿæˆï¼Œé©±åŠ¨æµå¼è¾“å‡º"""
                with torch.no_grad():
                    self.model.generate(**generation_kwargs)

            generation_thread = Thread(target=generate, daemon=True)
            generation_thread.start()

            show_console = token_callback is None
            if show_console:
                print("   ğŸ’¬ å®æ—¶è¾“å‡º:", end=" ", flush=True)

            for new_text in streamer:
                if token_callback is not None:
                    token_callback(new_text)
                else:
                    print(new_text, end="", flush=True)
                response_chunks.append(new_text)

            generation_thread.join()
            if show_console:
                print()

            return "".join(response_chunks).strip()
            
        except Exception as e:
            print(f"      âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶é‡åˆ°é”™è¯¯: {str(e)}"
    
    def process_code_query(
        self,
        user_query: str,
        stream_callback: Optional[Callable[[Dict[str, Any]], None]] = None,
    ) -> WorkflowResponse:
        """
        å¤„ç†æºç æŸ¥è¯¢çš„å®Œæ•´å·¥ä½œæµï¼ˆè¿­ä»£å¼RAGï¼‰
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            stream_callback: æµå¼è¾“å‡ºå›è°ƒå‡½æ•°
            
        Returns:
            å·¥ä½œæµå“åº”å¯¹è±¡
        """
        print(f"\n{'='*60}")
        print("ğŸš€ å¼€å§‹å¤„ç†æºç æŸ¥è¯¢ï¼ˆè¿­ä»£å¼RAGï¼‰...")
        print(f"ğŸ“ ç”¨æˆ·é—®é¢˜: {user_query}")
        print(f"{'='*60}")
        
        # æ­¥éª¤1: è®°å½•ç”¨æˆ·è¾“å…¥
        self.conversation_manager.add_message("user", user_query)
        conversation_history = self.conversation_manager.get_history()
        
        # æ­¥éª¤2: ç”Ÿæˆåˆå§‹æºç æ£€ç´¢å»ºè®®
        print("\nğŸ” æ­¥éª¤1: ç”Ÿæˆåˆå§‹æºç æ£€ç´¢å»ºè®®...")
        retrieval_suggestion = self.code_retrieval_suggester.generate_suggestion(
            user_query, conversation_history[:-1]  # ä¸åŒ…å«å½“å‰ç”¨æˆ·æ¶ˆæ¯
        )
        
        print(f"   ğŸ“‹ åŸå§‹æŸ¥è¯¢: {retrieval_suggestion.original_query}")
        print(f"   ğŸ¯ æ„å›¾è¯†åˆ«: {retrieval_suggestion.intent}")
        print(f"   ğŸ“Š ç½®ä¿¡åº¦: {retrieval_suggestion.confidence:.2f}")
        print(f"   ğŸ”‘ å…³é”®è¯: {retrieval_suggestion.search_keywords}")
        print(f"   ğŸ“ å»ºè®®æŸ¥è¯¢: {retrieval_suggestion.suggested_queries}")
        
        # æ­¥éª¤3: ç¬¬ä¸€è½®æ£€ç´¢
        print("\nğŸ’» æ­¥éª¤2: æ‰§è¡Œç¬¬ä¸€è½®æºç å‘é‡æ£€ç´¢...")
        first_round_chunks = self._retrieve_code_with_suggestion(retrieval_suggestion)
        print(f"   âœ… ç¬¬ä¸€è½®æ£€ç´¢åˆ° {len(first_round_chunks)} ä¸ªä»£ç ç‰‡æ®µ")
        
        # å¦‚éœ€æµå¼è¾“å‡ºï¼Œé€šçŸ¥å¤–éƒ¨ç¬¬ä¸€è½®æ£€ç´¢ç»“æœ
        if stream_callback is not None:
            try:
                stream_callback({
                    "type": "code_retrieval",
                    "retrieved_chunks": first_round_chunks,
                    "user_query": user_query,
                    "round": 1,
                })
            except Exception as callback_error:
                print(f"   âš ï¸ æµå¼å›è°ƒå¼‚å¸¸: {callback_error}")
        
        # æ­¥éª¤4: åˆ¤æ–­å·²æ£€ç´¢çš„å†…å®¹æ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜
        print("\nğŸ” æ­¥éª¤3: åˆ¤æ–­å·²æ£€ç´¢å†…å®¹æ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜...")
        sufficiency_result = self._judge_sufficiency_and_suggest_keywords(user_query, first_round_chunks)
        is_sufficient = sufficiency_result.get("is_sufficient", True)
        new_keywords = sufficiency_result.get("new_keywords", [])
        reasoning = sufficiency_result.get("reasoning", "")
        
        print(f"   ğŸ“Š åˆ¤æ–­ç»“æœ: {'å†…å®¹å·²è¶³å¤Ÿ' if is_sufficient else 'å†…å®¹ä¸è¶³ï¼Œéœ€è¦è¡¥å……'}")
        if reasoning:
            print(f"   ğŸ’­ åˆ¤æ–­ç†ç”±: {reasoning}")
        
        # æ­¥éª¤5: å¦‚æœå†…å®¹ä¸è¶³ï¼Œè¿›è¡Œç¬¬äºŒè½®æ£€ç´¢
        all_chunks = first_round_chunks.copy()
        second_round_chunks = []
        seen_chunk_ids = set()
        
        # è®°å½•ç¬¬ä¸€è½®chunkçš„ID
        for chunk in first_round_chunks:
            chunk_id = f"{chunk.metadata.get('file_path', '')}_{chunk.metadata.get('chunk_id', '')}"
            seen_chunk_ids.add(chunk_id)
        
        if not is_sufficient and new_keywords:
            print(f"\nğŸ’» æ­¥éª¤4: æ‰§è¡Œç¬¬äºŒè½®æºç å‘é‡æ£€ç´¢ï¼ˆå…³é”®è¯: {new_keywords}ï¼‰...")
            # ä½¿ç”¨æ–°çš„å…³é”®è¯è¿›è¡Œæ£€ç´¢
            second_round_chunks = self._retrieve_code_with_keywords(new_keywords, seen_chunk_ids)
            print(f"   âœ… ç¬¬äºŒè½®æ£€ç´¢åˆ° {len(second_round_chunks)} ä¸ªæ–°çš„ä»£ç ç‰‡æ®µ")
            
            # åˆå¹¶ä¸¤è½®æ£€ç´¢çš„chunk
            all_chunks.extend(second_round_chunks)
            
            # å¦‚éœ€æµå¼è¾“å‡ºï¼Œé€šçŸ¥å¤–éƒ¨ç¬¬äºŒè½®æ£€ç´¢ç»“æœ
            if stream_callback is not None:
                try:
                    stream_callback({
                        "type": "code_retrieval",
                        "retrieved_chunks": second_round_chunks,
                        "user_query": user_query,
                        "round": 2,
                    })
                except Exception as callback_error:
                    print(f"   âš ï¸ æµå¼å›è°ƒå¼‚å¸¸: {callback_error}")
        else:
            print("\n   â„¹ï¸ å†…å®¹å·²è¶³å¤Ÿï¼Œè·³è¿‡ç¬¬äºŒè½®æ£€ç´¢")
        
        # æ­¥éª¤6: ä½¿ç”¨æ‰€æœ‰æ£€ç´¢åˆ°çš„chunkç”Ÿæˆæœ€ç»ˆå›ç­”
        print(f"\nğŸ¤– æ­¥éª¤{'4' if is_sufficient or not new_keywords else '5'}: åŸºäºæ£€ç´¢åˆ°çš„ä»£ç å†…å®¹ç”Ÿæˆå›å¤...")
        print(f"   ğŸ“š ä½¿ç”¨ {len(all_chunks)} ä¸ªä»£ç ç‰‡æ®µç”Ÿæˆå›ç­”")
        
        # è®¾ç½®æµå¼è¾“å‡ºå›è°ƒ
        token_callback = None
        if stream_callback is not None:
            def handle_token(token_text: str) -> None:
                try:
                    stream_callback({
                        "type": "code_description_chunk",
                        "chunk": token_text,
                    })
                except Exception as callback_error:
                    print(f"   âš ï¸ æµå¼å›è°ƒå¼‚å¸¸: {callback_error}")

            token_callback = handle_token

        # ä½¿ç”¨å¤§æ¨¡å‹åŸºäºæ£€ç´¢åˆ°çš„ä»£ç å—å†…å®¹ç”Ÿæˆå›å¤
        llm_response = self._generate_response_with_context(
            user_query,
            all_chunks,
            token_callback=token_callback,
        )
        print(f"   âœ… å›ç­”ç”Ÿæˆå®Œæˆ")
        
        # è®°å½•åŠ©æ‰‹å›ç­”
        self.conversation_manager.add_message("assistant", llm_response)
        
        # æ„å»ºå®Œæ•´å“åº”ï¼ˆåŒ…å«æ‰€æœ‰æ£€ç´¢åˆ°çš„chunkï¼‰
        workflow_response = WorkflowResponse(
            user_query=user_query,
            retrieval_suggestion=retrieval_suggestion,
            retrieved_chunks=all_chunks,  # ä½¿ç”¨æ‰€æœ‰æ£€ç´¢åˆ°çš„chunk
            llm_response=llm_response,
            conversation_history=self.conversation_manager.get_history(),
            timestamp=datetime.now()
        )
        
        print(f"\n{'='*60}")
        print("âœ… æºç æŸ¥è¯¢å¤„ç†å®Œæˆ!")
        print(f"{'='*60}")
        
        # æ‰“å°å®Œæ•´çš„å¤„ç†ç»“æœåˆ°åå°
        print(f"\n{'='*60}")
        print("ğŸ“Š æºç æŸ¥è¯¢å¤„ç†ç»“æœæ‘˜è¦ï¼ˆè¿­ä»£å¼RAGï¼‰")
        print(f"{'='*60}")
        print(f"ç”¨æˆ·æŸ¥è¯¢: {workflow_response.user_query}")
        print(f"åˆå§‹æ£€ç´¢å»ºè®®æ•°é‡: {len(workflow_response.retrieval_suggestion.suggested_queries) if workflow_response.retrieval_suggestion else 0}")
        print(f"ç¬¬ä¸€è½®æ£€ç´¢ç‰‡æ®µæ•°: {len(first_round_chunks)}")
        if second_round_chunks:
            print(f"ç¬¬äºŒè½®æ£€ç´¢ç‰‡æ®µæ•°: {len(second_round_chunks)}")
            print(f"ç¬¬äºŒè½®ç›¸å…³ç‰‡æ®µæ•°: {len([c for c in all_chunks if c.metadata.get('round') == 2])}")
        print(f"æœ€ç»ˆä½¿ç”¨çš„ä»£ç ç‰‡æ®µæ•°: {len(workflow_response.retrieved_chunks)}")
        print(f"ç”Ÿæˆå›å¤é•¿åº¦: {len(workflow_response.llm_response)} å­—ç¬¦")
        
        if workflow_response.retrieved_chunks:
            print(f"\næœ€ç»ˆä½¿ç”¨çš„ä»£ç ç‰‡æ®µè¯¦æƒ…:")
            for i, chunk in enumerate(workflow_response.retrieved_chunks[:5], 1):
                round_num = chunk.metadata.get('round', 1)
                print(f"  [{i}] {chunk.filename} (ç¬¬{round_num}è½®æ£€ç´¢)")
                print(f"      è·¯å¾„: {chunk.metadata.get('file_path', 'N/A')}")
                print(f"      è¡Œå·: {chunk.metadata.get('line_range', 'N/A')}")
                print(f"      å‡½æ•°: {chunk.metadata.get('function_name', 'N/A')}")
                print(f"      ç›¸ä¼¼åº¦: {chunk.score:.4f}")
        
        print(f"\nç”Ÿæˆå›å¤é¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰:")
        print(f"{workflow_response.llm_response[:500]}...")
        print(f"{'='*60}\n")
        
        return workflow_response
    
    def _retrieve_code_with_suggestion(self, suggestion: RetrievalSuggestion) -> List[RetrievedChunk]:
        """åŸºäºæ£€ç´¢å»ºè®®æ£€ç´¢æºç ï¼ˆä¼˜å…ˆä½¿ç”¨ RetrievalEngineï¼Œä¸¤é˜¶æ®µå·²å°è£…ï¼‰"""
        all_chunks: List[RetrievedChunk] = []
        seen_chunk_ids = set()
        self._retrieved_file_summaries = {}
        
        queries_to_try = suggestion.suggested_queries + [suggestion.original_query] if suggestion.suggested_queries else [suggestion.original_query]
        # å»é‡ä¿åº
        uniq = []
        seen_q = set()
        for q in queries_to_try:
            if q not in seen_q:
                seen_q.add(q)
                uniq.append(q)
        queries_to_try = uniq[:5]
        
        print(f"      ğŸ“‹ å°†ä½¿ç”¨ {len(queries_to_try)} ä¸ªæŸ¥è¯¢è¿›è¡Œé€ä¸ªæ£€ç´¢ï¼ˆæ¯ä¸ªæŸ¥è¯¢ç”± RetrievalEngine æ§åˆ¶ top_kï¼‰")
        
        for query in queries_to_try:
            try:
                if self.retrieval_engine is not None:
                    resp = self.retrieval_engine.retrieve(query)
                    # ç¼“å­˜æ–‡ä»¶æ‘˜è¦
                    for f in resp.get("retrieved_files", []):
                        if f.get("source_file") and f.get("md_summary"):
                            self._retrieved_file_summaries[f["source_file"]] = f["md_summary"]
                    query_chunks = self._convert_retrieval_output(resp, query, round_no=1)
                else:
                    # å¤‡ç”¨ï¼šåŸ CodeRAGEngine
                    print(f"      âš ï¸ ä½¿ç”¨å¤‡ç”¨æ£€ç´¢æ–¹æ³• (æŸ¥è¯¢: '{query}')...")
                    results = self.code_rag_engine.query(query, top_k=self.top_files * self.top_chunks)
                    query_chunks = []
                    if results.get("contents"):
                        file_paths = results.get("file_paths", [""] * len(results["contents"]))
                        line_ranges = results.get("line_ranges", [""] * len(results["contents"]))
                        languages = results.get("languages", [""] * len(results["contents"]))
                        for content, similarity, file_name, file_path, line_range, language in zip(
                            results["contents"],
                            results["similarities"],
                            results["file_names"],
                            file_paths,
                            line_ranges,
                            languages,
                        ):
                            chunk = RetrievedChunk(
                                content=content,
                                source=file_path or file_name,
                                filename=file_name,
                                relative_path=file_path or file_name,
                                extension=self._get_extension_from_language(language) or ".txt",
                                score=similarity,
                                metadata={
                                    "file_name": file_name,
                                    "file_path": file_path,
                                    "line_range": line_range,
                                    "language": language,
                                    "similarity": similarity,
                                    "query_used": query,
                                    "round": 1,
                                },
                            )
                            query_chunks.append(chunk)
                # å»é‡å¹¶åˆå¹¶
                query_chunks.sort(key=lambda x: x.score, reverse=True)
                for chunk in query_chunks:
                    cid = f"{chunk.metadata.get('file_path','')}_{chunk.metadata.get('chunk_id','')}"
                    if cid not in seen_chunk_ids:
                        seen_chunk_ids.add(cid)
                        all_chunks.append(chunk)
            except Exception as e:
                print(f"      âŒ æ£€ç´¢æŸ¥è¯¢ '{query}' æ—¶å‡ºé”™: {e}")
                import traceback; traceback.print_exc()
                continue
        
        all_chunks.sort(key=lambda x: x.score, reverse=True)
        return all_chunks[:8]
    
    def _extract_file_overview(self, summary: str) -> str:
        """ä»æ–‡ä»¶æ‘˜è¦ä¸­æå–"æ–‡ä»¶æ¦‚è¿°"éƒ¨åˆ†"""
        if not summary:
            return ""
        
        # æŸ¥æ‰¾"æ–‡ä»¶æ¦‚è¿°"éƒ¨åˆ†
        # å¯èƒ½çš„æ ‡è®°ï¼š## æ–‡ä»¶æ¦‚è¿°ã€## 1. æ–‡ä»¶æ¦‚è¿°ã€æ–‡ä»¶æ¦‚è¿°ç­‰
        import re
        patterns = [
            r'##\s*æ–‡ä»¶æ¦‚è¿°\s*\n\n(.*?)(?=\n##|\n#|$)',
            r'##\s*1\.\s*æ–‡ä»¶æ¦‚è¿°\s*\n\n(.*?)(?=\n##|\n#|$)',
            r'æ–‡ä»¶æ¦‚è¿°\s*\n\n(.*?)(?=\n##|\n#|$)',
            r'##\s*æ–‡ä»¶æ¦‚è¿°\s*\n(.*?)(?=\n##|\n#|$)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, summary, re.DOTALL | re.IGNORECASE)
            if match:
                overview = match.group(1).strip()
                # é™åˆ¶é•¿åº¦ï¼Œæœ€å¤š500å­—ç¬¦
                if len(overview) > 500:
                    overview = overview[:500] + "..."
                return overview
        
        # å¦‚æœæ‰¾ä¸åˆ°"æ–‡ä»¶æ¦‚è¿°"ï¼Œå°è¯•æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæ®µè½ï¼ˆåœ¨"æŠ€æœ¯æ–‡æ¡£"æ ‡é¢˜ä¹‹åï¼‰
        # æŸ¥æ‰¾"æŠ€æœ¯æ–‡æ¡£"æ ‡é¢˜åçš„ç¬¬ä¸€ä¸ªæ®µè½
        tech_doc_match = re.search(r'#.*?æŠ€æœ¯æ–‡æ¡£\s*\n\n(.*?)(?=\n##|\n#|$)', summary, re.DOTALL | re.IGNORECASE)
        if tech_doc_match:
            first_para = tech_doc_match.group(1).strip()
            # å–ç¬¬ä¸€ä¸ªæ®µè½ï¼Œæœ€å¤š500å­—ç¬¦
            if len(first_para) > 500:
                first_para = first_para[:500] + "..."
            return first_para
        
        return ""
    
    def _get_extension_from_language(self, language: str) -> str:
        """æ ¹æ®ç¼–ç¨‹è¯­è¨€è·å–æ–‡ä»¶æ‰©å±•å"""
        language_extensions = {
            'python': '.py',
            'javascript': '.js',
            'typescript': '.ts',
            'java': '.java',
            'cpp': '.cpp',
            'c': '.c',
            'go': '.go',
            'rust': '.rs',
            'php': '.php',
            'ruby': '.rb',
            'swift': '.swift',
            'kotlin': '.kt',
        }
        return language_extensions.get(language.lower(), '.txt')
    
    def _generate_code_description(
        self,
        user_query: str,
        chunks: List[RetrievedChunk],
        token_callback: Optional[Callable[[str], None]] = None,
    ) -> str:
        """ç”Ÿæˆä»£ç æè¿°"""
        try:
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_parts = []
            for i, chunk in enumerate(chunks):
                file_info = f"æ–‡ä»¶: {chunk.filename}"
                if chunk.metadata.get('line_range'):
                    file_info += f" (è¡Œå·: {chunk.metadata.get('line_range')})"
                if chunk.metadata.get('language'):
                    file_info += f" [è¯­è¨€: {chunk.metadata.get('language')}]"
                
                context_parts.append(
                    f"ä»£ç ç‰‡æ®µ{i+1} ({file_info}):\n```\n{chunk.content}\n```"
                )
            context = "\n\n".join(context_parts)
            
            # è·å–å¯¹è¯å†å²
            conversation_history = self.conversation_manager.get_history()
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [
                {"role": "system", "content": f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æåŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„æºä»£ç ç‰‡æ®µå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„ä»£ç æè¿°ã€‚

æºä»£ç ç‰‡æ®µï¼š
{context}

è¯·ç”Ÿæˆè¯¦ç»†çš„ä»£ç æè¿°ï¼ŒåŒ…æ‹¬ï¼š
1. ä»£ç çš„åŠŸèƒ½å’Œä½œç”¨
2. å…³é”®å‡½æ•°ã€ç±»æˆ–æ–¹æ³•çš„è¯´æ˜
3. ä»£ç é€»è¾‘æµç¨‹
4. é‡è¦çš„æŠ€æœ¯ç»†èŠ‚
5. ä¸ç”¨æˆ·é—®é¢˜çš„å…³è”æ€§

è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è¦ä¸“ä¸šä½†é€šä¿—æ˜“æ‡‚ã€‚"""}
            ]
            
            # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘3è½®ï¼‰
            for msg in conversation_history[-3:]:
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æŸ¥è¯¢
            messages.append({
                "role": "user", 
                "content": user_query
            })
            
            # æ ¼å¼åŒ–è¾“å…¥
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # ç¼–ç 
            inputs = self.tokenizer(text, return_tensors="pt")
            
            # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            # è®¾ç½®æµå¼è¾“å‡ºç»„ä»¶
            streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
            generation_kwargs = dict(inputs)
            generation_kwargs.update({
                "max_new_tokens": 3000,
                "temperature": 0.7,
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "streamer": streamer
            })
            response_chunks: List[str] = []

            def generate():
                """åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œç”Ÿæˆï¼Œé©±åŠ¨æµå¼è¾“å‡º"""
                with torch.no_grad():
                    self.model.generate(**generation_kwargs)

            generation_thread = Thread(target=generate, daemon=True)
            generation_thread.start()

            show_console = token_callback is None
            if show_console:
                print("   ğŸ’¬ å®æ—¶è¾“å‡º:", end=" ", flush=True)

            for new_text in streamer:
                if token_callback is not None:
                    token_callback(new_text)
                else:
                    print(new_text, end="", flush=True)
                response_chunks.append(new_text)

            generation_thread.join()
            if show_console:
                print()

            return "".join(response_chunks).strip()
            
        except Exception as e:
            print(f"      âŒ ç”Ÿæˆä»£ç æè¿°æ—¶å‡ºé”™: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆä»£ç æè¿°æ—¶é‡åˆ°é”™è¯¯: {str(e)}"
    
    def display_response(self, response: WorkflowResponse):
        """æ ¼å¼åŒ–æ˜¾ç¤ºå“åº”ç»“æœ"""
        print(f"\nğŸ¤– AIä»£ç æè¿°:")
        print("=" * 60)
        print(response.llm_response)
        print("=" * 60)
        
        print(f"\nğŸ’» æ£€ç´¢åˆ°çš„ä»£ç ç‰‡æ®µ ({len(response.retrieved_chunks)} ä¸ª):")
        print("-" * 60)
        for i, chunk in enumerate(response.retrieved_chunks):
            print(f"\nä»£ç ç‰‡æ®µ {i+1}:")
            print(f"   æ–‡ä»¶å: {chunk.filename}")
            print(f"   æ–‡ä»¶è·¯å¾„: {chunk.metadata.get('file_path', 'N/A')}")
            print(f"   è¡Œå·: {chunk.metadata.get('line_range', 'N/A')}")
            print(f"   è¯­è¨€: {chunk.metadata.get('language', 'N/A')}")
            print(f"   ç›¸ä¼¼åº¦: {chunk.score:.4f}")
            print(f"   å†…å®¹é¢„è§ˆ:")
            content = chunk.content
            if len(content) > 200:
                content = content[:200] + "..."
            print(f"      {content}")
            print("-" * 40)
    
    def clear_conversation(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_manager.clear()
        print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º")

# ==================== ä¸»å·¥ä½œæµç±» ====================

class SimpleRAGWorkflow:
    """ç®€åŒ–çš„RAGå·¥ä½œæµï¼Œä¸ä¾èµ–langchain"""
    
    def __init__(self, 
                 llm_path: str = "../models--Qwen--Qwen3-8B/snapshots/9c925d64d72725edaf899c6cb9c377fd0709d9c5",
                 embedding_model_path: str = "/home/ubuntu/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/",
                 db_path: str = "./vector_db",
                 similarity_threshold: float = 0.3,
                 use_quantization: bool = True):
        """
        åˆå§‹åŒ–RAGå·¥ä½œæµ
        
        Args:
            llm_path: å¤§è¯­è¨€æ¨¡å‹è·¯å¾„
            embedding_model_path: åµŒå…¥æ¨¡å‹è·¯å¾„
            db_path: å‘é‡æ•°æ®åº“è·¯å¾„
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç»“æœå°†è¢«è¿‡æ»¤
            use_quantization: æ˜¯å¦ä½¿ç”¨4ä½é‡åŒ–ï¼ŒTrueè¡¨ç¤ºä½¿ç”¨é‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰ï¼ŒFalseè¡¨ç¤ºä¸ä½¿ç”¨é‡åŒ–ï¼ˆæ›´é«˜ç²¾åº¦ï¼‰
        """
        self.llm_path = llm_path
        self.similarity_threshold = similarity_threshold
        self.use_quantization = use_quantization
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.conversation_manager = ConversationManager()
        self.rag_engine = SimpleRAGEngine(embedding_model_path, db_path, similarity_threshold=similarity_threshold)
        
        # åˆå§‹åŒ–LLM
        self._load_llm()
        
        # åˆå§‹åŒ–æ£€ç´¢å»ºè®®ç”Ÿæˆå™¨
        self.retrieval_suggester = RetrievalSuggester(self.model, self.tokenizer)
        
        print("âœ… ç®€åŒ–RAGå·¥ä½œæµåˆå§‹åŒ–å®Œæˆ!")
    
    def _load_llm(self):
        """åŠ è½½å¤§è¯­è¨€æ¨¡å‹"""
        print("ğŸ¤– æ­£åœ¨åŠ è½½å¤§è¯­è¨€æ¨¡å‹...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(self.llm_path, trust_remote_code=True)
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨é‡åŒ–
        if self.use_quantization:
            print("   ğŸ“¦ ä½¿ç”¨4ä½é‡åŒ–åŠ è½½æ¨¡å‹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰...")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                self.llm_path,
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True,
            )
        else:
            print("   ğŸš€ ä½¿ç”¨å…¨ç²¾åº¦åŠ è½½æ¨¡å‹ï¼ˆæ›´é«˜ç²¾åº¦ï¼Œéœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.llm_path,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            )
        
        print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}, é‡åŒ–: {'å¼€å¯' if self.use_quantization else 'å…³é—­'}")
    
    def process_user_query(
        self,
        user_query: str,
        stream_callback: Optional[Callable[[Dict[str, Any]], None]] = None,
    ) -> WorkflowResponse:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„å®Œæ•´å·¥ä½œæµï¼ˆè¿­ä»£å¼RAGï¼‰
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            stream_callback: æµå¼è¾“å‡ºå›è°ƒå‡½æ•°
            
        Returns:
            å·¥ä½œæµå“åº”å¯¹è±¡
        """
        print(f"\n{'='*60}")
        print("ğŸš€ å¼€å§‹å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆè¿­ä»£å¼RAGï¼‰...")
        print(f"ğŸ“ ç”¨æˆ·é—®é¢˜: {user_query}")
        print(f"{'='*60}")
        
        # æ­¥éª¤1: è®°å½•ç”¨æˆ·è¾“å…¥
        self.conversation_manager.add_message("user", user_query)
        conversation_history = self.conversation_manager.get_history()
        
        # æ­¥éª¤2: ç”Ÿæˆåˆå§‹æ£€ç´¢å»ºè®®
        print("\nğŸ” æ­¥éª¤1: ç”Ÿæˆåˆå§‹æ£€ç´¢å»ºè®®...")
        retrieval_suggestion = self.retrieval_suggester.generate_suggestion(
            user_query, conversation_history[:-1]  # ä¸åŒ…å«å½“å‰ç”¨æˆ·æ¶ˆæ¯
        )
        
        print(f"   ğŸ“‹ åŸå§‹æŸ¥è¯¢: {retrieval_suggestion.original_query}")
        print(f"   ğŸ¯ æ„å›¾è¯†åˆ«: {retrieval_suggestion.intent}")
        print(f"   ğŸ“Š ç½®ä¿¡åº¦: {retrieval_suggestion.confidence:.2f}")
        print(f"   ğŸ”‘ å…³é”®è¯: {retrieval_suggestion.search_keywords}")
        print(f"   ğŸ“ å»ºè®®æŸ¥è¯¢: {retrieval_suggestion.suggested_queries}")
        
        # æ­¥éª¤3: ç¬¬ä¸€è½®æ£€ç´¢
        print("\nğŸ“š æ­¥éª¤2: æ‰§è¡Œç¬¬ä¸€è½®å‘é‡æ£€ç´¢...")
        first_round_chunks = self._retrieve_documents_with_suggestion(retrieval_suggestion)
        print(f"   âœ… ç¬¬ä¸€è½®æ£€ç´¢åˆ° {len(first_round_chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        
        # æ­¥éª¤4: åˆ¤æ–­æ¯ä¸ªchunkæ˜¯å¦ä¸é—®é¢˜ç›¸å…³
        print("\nğŸ” æ­¥éª¤3: åˆ¤æ–­æ£€ç´¢åˆ°çš„chunkæ˜¯å¦ä¸é—®é¢˜ç›¸å…³...")
        relevance_flags = self._judge_chunk_relevance(user_query, first_round_chunks)
        relevant_chunks_round1 = [
            chunk for i, chunk in enumerate(first_round_chunks) 
            if (relevance_flags[i] if i < len(relevance_flags) else True)
        ]
        print(f"   âœ… ç¬¬ä¸€è½®æ£€ç´¢ä¸­ï¼Œ{len(relevant_chunks_round1)}/{len(first_round_chunks)} ä¸ªchunkè¢«åˆ¤å®šä¸ºç›¸å…³")
        
        # å¦‚éœ€æµå¼è¾“å‡ºï¼Œé€šçŸ¥å¤–éƒ¨ç¬¬ä¸€è½®æ£€ç´¢ç»“æœ
        if stream_callback is not None:
            try:
                stream_callback({
                    "type": "retrieval",
                    "retrieved_chunks": first_round_chunks,
                    "user_query": user_query,
                    "round": 1,
                })
            except Exception as callback_error:
                print(f"   âš ï¸ æµå¼å›è°ƒå¼‚å¸¸: {callback_error}")
        
        # æ­¥éª¤5: åˆ¤æ–­å·²æ£€ç´¢çš„å†…å®¹æ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜
        print("\nğŸ” æ­¥éª¤4: åˆ¤æ–­å·²æ£€ç´¢å†…å®¹æ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜...")
        sufficiency_result = self._judge_sufficiency_and_suggest_keywords(user_query, relevant_chunks_round1)
        is_sufficient = sufficiency_result.get("is_sufficient", True)
        new_keywords = sufficiency_result.get("new_keywords", [])
        reasoning = sufficiency_result.get("reasoning", "")
        
        print(f"   ğŸ“Š åˆ¤æ–­ç»“æœ: {'å†…å®¹å·²è¶³å¤Ÿ' if is_sufficient else 'å†…å®¹ä¸è¶³ï¼Œéœ€è¦è¡¥å……'}")
        if reasoning:
            print(f"   ğŸ’­ åˆ¤æ–­ç†ç”±: {reasoning}")
        
        # æ­¥éª¤6: å¦‚æœå†…å®¹ä¸è¶³ï¼Œè¿›è¡Œç¬¬äºŒè½®æ£€ç´¢
        all_relevant_chunks = relevant_chunks_round1.copy()
        second_round_chunks = []
        
        if not is_sufficient and new_keywords:
            print(f"\nğŸ“š æ­¥éª¤5: æ‰§è¡Œç¬¬äºŒè½®å‘é‡æ£€ç´¢ï¼ˆå…³é”®è¯: {new_keywords}ï¼‰...")
            # ä½¿ç”¨æ–°çš„å…³é”®è¯è¿›è¡Œæ£€ç´¢
            for keyword in new_keywords[:3]:  # æœ€å¤šä½¿ç”¨3ä¸ªå…³é”®è¯
                try:
                    results = self.rag_engine.query(keyword, top_k=3)
                    if results.get('contents'):
                        page_ranges = results.get('page_ranges', [''] * len(results['contents']))
                        seen_content = {chunk.content for chunk in all_relevant_chunks}
                        
                        for content, similarity, file_name, section, page_range in zip(
                            results['contents'],
                            results['similarities'],
                            results['file_names'],
                            results['sections'],
                            page_ranges
                        ):
                            # å»é‡ï¼šæ£€æŸ¥æ˜¯å¦å·²åœ¨ç¬¬ä¸€è½®æ£€ç´¢ç»“æœä¸­
                            if content in seen_content:
                                continue
                            seen_content.add(content)
                            
                            chunk = RetrievedChunk(
                                content=content,
                                source=file_name,
                                filename=file_name,
                                relative_path=file_name,
                                extension=".pdf",
                                score=similarity,
                                metadata={
                                    'file_name': file_name,
                                    'section': section,
                                    'page_range': page_range,
                                    'similarity': similarity,
                                    'query_used': keyword,
                                    'round': 2
                                }
                            )
                            second_round_chunks.append(chunk)
                except Exception as e:
                    print(f"      âŒ ä½¿ç”¨å…³é”®è¯ '{keyword}' æ£€ç´¢æ—¶å‡ºé”™: {e}")
                    continue
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            second_round_chunks.sort(key=lambda x: x.score, reverse=True)
            second_round_chunks = second_round_chunks[:5]  # æœ€å¤š5ä¸ª
            print(f"   âœ… ç¬¬äºŒè½®æ£€ç´¢åˆ° {len(second_round_chunks)} ä¸ªæ–°çš„æ–‡æ¡£ç‰‡æ®µ")
            
            # åˆ¤æ–­ç¬¬äºŒè½®æ£€ç´¢çš„chunkæ˜¯å¦ç›¸å…³
            if second_round_chunks:
                print("\nğŸ” æ­¥éª¤6: åˆ¤æ–­ç¬¬äºŒè½®æ£€ç´¢çš„chunkæ˜¯å¦ä¸é—®é¢˜ç›¸å…³...")
                relevance_flags_round2 = self._judge_chunk_relevance(user_query, second_round_chunks)
                relevant_chunks_round2 = [
                    chunk for i, chunk in enumerate(second_round_chunks) 
                    if (relevance_flags_round2[i] if i < len(relevance_flags_round2) else True)
                ]
                print(f"   âœ… ç¬¬äºŒè½®æ£€ç´¢ä¸­ï¼Œ{len(relevant_chunks_round2)}/{len(second_round_chunks)} ä¸ªchunkè¢«åˆ¤å®šä¸ºç›¸å…³")
                
                # åˆå¹¶ä¸¤è½®æ£€ç´¢çš„ç›¸å…³chunk
                all_relevant_chunks.extend(relevant_chunks_round2)
                
                # å¦‚éœ€æµå¼è¾“å‡ºï¼Œé€šçŸ¥å¤–éƒ¨ç¬¬äºŒè½®æ£€ç´¢ç»“æœ
                if stream_callback is not None:
                    try:
                        stream_callback({
                            "type": "retrieval",
                            "retrieved_chunks": second_round_chunks,
                            "user_query": user_query,
                            "round": 2,
                        })
                    except Exception as callback_error:
                        print(f"   âš ï¸ æµå¼å›è°ƒå¼‚å¸¸: {callback_error}")
        else:
            print("\n   â„¹ï¸ å†…å®¹å·²è¶³å¤Ÿï¼Œè·³è¿‡ç¬¬äºŒè½®æ£€ç´¢")
        
        # æ­¥éª¤7: ä½¿ç”¨æ‰€æœ‰ç›¸å…³chunkç”Ÿæˆæœ€ç»ˆå›ç­”
        print(f"\nğŸ¤– æ­¥éª¤{'6' if is_sufficient or not new_keywords else '7'}: ç”Ÿæˆæœ€ç»ˆå›ç­”...")
        print(f"   ğŸ“š ä½¿ç”¨ {len(all_relevant_chunks)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µç”Ÿæˆå›ç­”")
        
        token_callback = None
        if stream_callback is not None:
            def handle_token(token_text: str) -> None:
                try:
                    stream_callback({
                        "type": "answer_chunk",
                        "chunk": token_text,
                    })
                except Exception as callback_error:
                    print(f"   âš ï¸ æµå¼å›è°ƒå¼‚å¸¸: {callback_error}")

            token_callback = handle_token

        llm_response = self._generate_response_with_context(
            user_query,
            all_relevant_chunks,
            token_callback=token_callback,
        )
        print(f"   âœ… å›ç­”ç”Ÿæˆå®Œæˆ")
        
        # è®°å½•åŠ©æ‰‹å›ç­”
        self.conversation_manager.add_message("assistant", llm_response)
        
        # æ„å»ºå®Œæ•´å“åº”ï¼ˆåŒ…å«æ‰€æœ‰ç›¸å…³chunkï¼‰
        workflow_response = WorkflowResponse(
            user_query=user_query,
            retrieval_suggestion=retrieval_suggestion,
            retrieved_chunks=all_relevant_chunks,  # ä½¿ç”¨æ‰€æœ‰ç›¸å…³chunk
            llm_response=llm_response,
            conversation_history=self.conversation_manager.get_history(),
            timestamp=datetime.now()
        )
        
        print(f"\n{'='*60}")
        print("âœ… æŸ¥è¯¢å¤„ç†å®Œæˆ!")
        print(f"{'='*60}")
        
        # æ‰“å°å®Œæ•´çš„å¤„ç†ç»“æœåˆ°åå°
        print(f"\n{'='*60}")
        print("ğŸ“Š æ•™ææ£€ç´¢å¤„ç†ç»“æœæ‘˜è¦ï¼ˆè¿­ä»£å¼RAGï¼‰")
        print(f"{'='*60}")
        print(f"ç”¨æˆ·æŸ¥è¯¢: {workflow_response.user_query}")
        print(f"åˆå§‹æ£€ç´¢å»ºè®®æ•°é‡: {len(workflow_response.retrieval_suggestion.suggested_queries) if workflow_response.retrieval_suggestion else 0}")
        print(f"ç¬¬ä¸€è½®æ£€ç´¢ç‰‡æ®µæ•°: {len(first_round_chunks)}")
        print(f"ç¬¬ä¸€è½®ç›¸å…³ç‰‡æ®µæ•°: {len(relevant_chunks_round1)}")
        if second_round_chunks:
            print(f"ç¬¬äºŒè½®æ£€ç´¢ç‰‡æ®µæ•°: {len(second_round_chunks)}")
            print(f"ç¬¬äºŒè½®ç›¸å…³ç‰‡æ®µæ•°: {len([c for c in all_relevant_chunks if c.metadata.get('round') == 2])}")
        print(f"æœ€ç»ˆä½¿ç”¨çš„ç›¸å…³ç‰‡æ®µæ•°: {len(workflow_response.retrieved_chunks)}")
        print(f"å›ç­”é•¿åº¦: {len(workflow_response.llm_response)} å­—ç¬¦")
        
        if workflow_response.retrieved_chunks:
            print(f"\næœ€ç»ˆä½¿ç”¨çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µè¯¦æƒ…:")
            for i, chunk in enumerate(workflow_response.retrieved_chunks[:5], 1):
                round_num = chunk.metadata.get('round', 1)
                print(f"  [{i}] {chunk.filename} (ç¬¬{round_num}è½®æ£€ç´¢)")
                print(f"      é¡µç : {chunk.metadata.get('page', 'N/A')}")
                print(f"      é¡µç èŒƒå›´: {chunk.metadata.get('page_range', 'N/A')}")
                print(f"      ç« èŠ‚: {chunk.metadata.get('chapter', 'N/A')}")
                print(f"      ç›¸ä¼¼åº¦: {chunk.score:.4f}")
                # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                content_preview = chunk.content[:150] + "..." if len(chunk.content) > 150 else chunk.content
                print(f"      å†…å®¹é¢„è§ˆ: {content_preview}")
        
        print(f"\nå›ç­”é¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰:")
        print(f"{workflow_response.llm_response[:500]}...")
        print(f"{'='*60}\n")
        
        return workflow_response
    
    def _retrieve_documents_with_suggestion(self, suggestion: RetrievalSuggestion) -> List[RetrievedChunk]:
        """åŸºäºæ£€ç´¢å»ºè®®æ£€ç´¢æ–‡æ¡£"""
        all_chunks = []
        seen_content = set()
        
        # ä½¿ç”¨å»ºè®®çš„æŸ¥è¯¢è¿›è¡Œæ£€ç´¢
        queries_to_try = suggestion.suggested_queries + [suggestion.original_query]
        
        for query in queries_to_try:
            try:
                # ä½¿ç”¨RAGå¼•æ“æ£€ç´¢
                results = self.rag_engine.query(query, top_k=3)
                
                # å¤„ç†ç»“æœ
                if results.get('contents'):
                    # è·å–page_rangesä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ç©ºå­—ç¬¦ä¸²åˆ—è¡¨
                    page_ranges = results.get('page_ranges', [''] * len(results['contents']))
                    
                    for i, (content, similarity, file_name, section, page_range) in enumerate(zip(
                        results['contents'],
                        results['similarities'],
                        results['file_names'],
                        results['sections'],
                        page_ranges
                    )):
                        # å»é‡
                        if content in seen_content:
                            continue
                        seen_content.add(content)
                        
                        # åˆ›å»ºRetrievedChunkå¯¹è±¡
                        chunk = RetrievedChunk(
                            content=content,
                            source=file_name,
                            filename=file_name,
                            relative_path=file_name,
                            extension=".pdf",
                            score=similarity,
                            metadata={
                                'file_name': file_name,
                                'section': section,
                                'page_range': page_range,
                                'similarity': similarity,
                                'query_used': query
                            }
                        )
                        all_chunks.append(chunk)
                        
            except Exception as e:
                print(f"      âŒ æ£€ç´¢æŸ¥è¯¢ '{query}' æ—¶å‡ºé”™: {e}")
                continue
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°æ’åº
        all_chunks.sort(key=lambda x: x.score, reverse=True)
        
        return all_chunks[:8]  # æœ€å¤šè¿”å›8ä¸ªç‰‡æ®µ
    
    def _retrieve_documents(self, user_query: str) -> List[RetrievedChunk]:
        """æ£€ç´¢æ–‡æ¡£ï¼Œåº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤"""
        all_chunks = []
        
        try:
            # ä½¿ç”¨RAGå¼•æ“è¿›è¡Œæ£€ç´¢
            search_results = self.rag_engine.query(user_query)
            
            # å¤„ç†RAGå¼•æ“çš„è¿”å›ç»“æœ
            if search_results.get('contents'):
                for i, (content, similarity, file_name, section) in enumerate(zip(
                    search_results['contents'],
                    search_results['similarities'],
                    search_results['file_names'],
                    search_results['sections']
                )):
                    # åŒé‡é˜ˆå€¼æ£€æŸ¥ï¼ˆRAGå¼•æ“å·²ç»è¿‡æ»¤äº†ä¸€æ¬¡ï¼Œè¿™é‡Œå†æ¬¡ç¡®è®¤ï¼‰
                    if similarity >= self.similarity_threshold:
                        # åˆ›å»ºRetrievedChunkå¯¹è±¡
                        chunk = RetrievedChunk(
                            content=content,
                            source=file_name,
                            filename=file_name,
                            relative_path=file_name,
                            extension=".pdf",
                            score=similarity,
                            metadata={
                                'file_name': file_name,
                                'section': section,
                                'similarity': similarity
                            }
                        )
                        all_chunks.append(chunk)
                    else:
                        print(f"   âš ï¸  è·³è¿‡ä½ç›¸ä¼¼åº¦ç»“æœ: {similarity:.3f} < {self.similarity_threshold:.3f}")
                    
        except Exception as e:
            print(f"      âŒ æ£€ç´¢æŸ¥è¯¢ '{user_query}' æ—¶å‡ºé”™: {e}")
            return []
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        all_chunks.sort(key=lambda x: x.score, reverse=True)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¬¦åˆé˜ˆå€¼çš„ç»“æœï¼Œç»™å‡ºæç¤º
        if not all_chunks:
            print(f"   âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼åº¦ >= {self.similarity_threshold:.2f} çš„ç›¸å…³æ–‡æ¡£")
            print(f"   ğŸ’¡ å»ºè®®: å°è¯•é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼æˆ–é‡æ–°è¡¨è¿°é—®é¢˜")
        
        return all_chunks[:8]  # æœ€å¤šè¿”å›8ä¸ªç‰‡æ®µ
    
    def _judge_chunk_relevance(self, user_query: str, chunks: List[RetrievedChunk]) -> List[bool]:
        """
        åˆ¤æ–­æ¯ä¸ªæ£€ç´¢åˆ°çš„chunkæ˜¯å¦ä¸é—®é¢˜ç›¸å…³
        
        Args:
            user_query: ç”¨æˆ·é—®é¢˜
            chunks: æ£€ç´¢åˆ°çš„chunkåˆ—è¡¨
            
        Returns:
            å¸ƒå°”å€¼åˆ—è¡¨ï¼ŒTrueè¡¨ç¤ºç›¸å…³ï¼ŒFalseè¡¨ç¤ºä¸ç›¸å…³
        """
        if not chunks:
            return []
        
        try:
            # æ„å»ºåˆ¤æ–­æç¤ºè¯
            chunk_texts = []
            for i, chunk in enumerate(chunks):
                chunk_texts.append(f"æ–‡æ¡£ç‰‡æ®µ{i+1}:\n{chunk.content[:500]}...")  # é™åˆ¶é•¿åº¦é¿å…è¿‡é•¿
            
            chunks_text = "\n\n".join(chunk_texts)
            
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£ç›¸å…³æ€§åˆ¤æ–­åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­æ¯ä¸ªæ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µæ˜¯å¦ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_query}

æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µï¼š
{chunks_text}

è¯·å¯¹æ¯ä¸ªæ–‡æ¡£ç‰‡æ®µè¿›è¡Œåˆ¤æ–­ï¼Œåˆ¤æ–­æ ‡å‡†ï¼š
1. æ–‡æ¡£ç‰‡æ®µçš„å†…å®¹æ˜¯å¦ç›´æ¥æˆ–é—´æ¥å›ç­”äº†ç”¨æˆ·é—®é¢˜
2. æ–‡æ¡£ç‰‡æ®µæ˜¯å¦åŒ…å«ä¸é—®é¢˜ç›¸å…³çš„å…³é”®ä¿¡æ¯
3. æ–‡æ¡£ç‰‡æ®µæ˜¯å¦æœ‰åŠ©äºç†è§£æˆ–è§£å†³ç”¨æˆ·é—®é¢˜

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "judgments": [
        {{"chunk_index": 0, "is_relevant": true, "reason": "ç›¸å…³åŸå› "}},
        {{"chunk_index": 1, "is_relevant": false, "reason": "ä¸ç›¸å…³åŸå› "}},
        ...
    ]
}}

åªè¾“å‡ºJSONï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚"""
            
            # æ„å»ºæ¶ˆæ¯
            messages = [{"role": "system", "content": system_prompt}]
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # ç”Ÿæˆåˆ¤æ–­ç»“æœ
            inputs = self.tokenizer(text, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1000,
                    temperature=0.3,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            
            # è§£æJSONç»“æœ
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                judgments = result.get('judgments', [])
                relevance_list = [False] * len(chunks)
                for judgment in judgments:
                    idx = judgment.get('chunk_index', -1)
                    if 0 <= idx < len(chunks):
                        relevance_list[idx] = judgment.get('is_relevant', False)
                return relevance_list
            else:
                # å¦‚æœè§£æå¤±è´¥ï¼Œé»˜è®¤å…¨éƒ¨ç›¸å…³
                print(f"   âš ï¸ æ— æ³•è§£æç›¸å…³æ€§åˆ¤æ–­ç»“æœï¼Œé»˜è®¤æ‰€æœ‰chunkç›¸å…³")
                return [True] * len(chunks)
                
        except Exception as e:
            print(f"   âš ï¸ åˆ¤æ–­chunkç›¸å…³æ€§æ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶é»˜è®¤å…¨éƒ¨ç›¸å…³
            return [True] * len(chunks)
    
    def _judge_sufficiency_and_suggest_keywords(
        self, 
        user_query: str, 
        relevant_chunks: List[RetrievedChunk]
    ) -> Dict[str, Any]:
        """
        åˆ¤æ–­å·²æ£€ç´¢çš„å†…å®¹æ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜ï¼Œå¦‚æœä¸è¶³åˆ™ç”Ÿæˆæ–°çš„æ£€ç´¢å…³é”®è¯
        
        Args:
            user_query: ç”¨æˆ·é—®é¢˜
            relevant_chunks: è¢«åˆ¤å®šä¸ºç›¸å…³çš„chunkåˆ—è¡¨
            
        Returns:
            å­—å…¸ï¼ŒåŒ…å«ï¼š
            - is_sufficient: æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜
            - new_keywords: å¦‚æœä¸è¶³ï¼Œæ–°çš„æ£€ç´¢å…³é”®è¯åˆ—è¡¨
            - reasoning: åˆ¤æ–­ç†ç”±
        """
        try:
            # æ„å»ºå·²æ£€ç´¢å†…å®¹çš„æ‘˜è¦
            if relevant_chunks:
                chunks_summary = "\n\n".join([
                    f"æ–‡æ¡£ç‰‡æ®µ{i+1} (æ¥æº: {chunk.filename}):\n{chunk.content[:300]}..."
                    for i, chunk in enumerate(relevant_chunks)
                ])
            else:
                chunks_summary = "æš‚æ— ç›¸å…³æ–‡æ¡£ç‰‡æ®µ"
            
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯å……åˆ†æ€§åˆ¤æ–­åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­å·²æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ˜¯å¦è¶³ä»¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_query}

å·²æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š
{chunks_summary}

è¯·åˆ¤æ–­ï¼š
1. å·²æ£€ç´¢åˆ°çš„å†…å®¹æ˜¯å¦è¶³ä»¥å®Œæ•´å›ç­”ç”¨æˆ·é—®é¢˜
2. å¦‚æœä¸è¶³ï¼Œéœ€è¦è¡¥å……å“ªäº›æ–¹é¢çš„ä¿¡æ¯
3. å¦‚æœä¸è¶³ï¼Œè¯·æä¾›2-3ä¸ªæ–°çš„æ£€ç´¢å…³é”®è¯ï¼Œç”¨äºè¿›è¡Œç¬¬äºŒè½®æ£€ç´¢

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "is_sufficient": false,
    "reasoning": "åˆ¤æ–­ç†ç”±",
    "new_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"]
}}

å¦‚æœå†…å®¹å·²è¶³å¤Ÿï¼Œis_sufficientåº”ä¸ºtrueï¼Œnew_keywordså¯ä»¥ä¸ºç©ºæ•°ç»„ã€‚
åªè¾“å‡ºJSONï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚"""
            
            # æ„å»ºæ¶ˆæ¯
            messages = [{"role": "system", "content": system_prompt}]
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # ç”Ÿæˆåˆ¤æ–­ç»“æœ
            inputs = self.tokenizer(text, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=500,
                    temperature=0.3,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            
            # è§£æJSONç»“æœ
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return {
                    "is_sufficient": result.get("is_sufficient", True),
                    "new_keywords": result.get("new_keywords", []),
                    "reasoning": result.get("reasoning", "")
                }
            else:
                # å¦‚æœè§£æå¤±è´¥ï¼Œé»˜è®¤è®¤ä¸ºè¶³å¤Ÿ
                print(f"   âš ï¸ æ— æ³•è§£æå……åˆ†æ€§åˆ¤æ–­ç»“æœï¼Œé»˜è®¤å†…å®¹å·²è¶³å¤Ÿ")
                return {
                    "is_sufficient": True,
                    "new_keywords": [],
                    "reasoning": "æ— æ³•è§£æåˆ¤æ–­ç»“æœï¼Œé»˜è®¤è®¤ä¸ºå†…å®¹å·²è¶³å¤Ÿ"
                }
                
        except Exception as e:
            print(f"   âš ï¸ åˆ¤æ–­å†…å®¹å……åˆ†æ€§æ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶é»˜è®¤è®¤ä¸ºè¶³å¤Ÿ
            return {
                "is_sufficient": True,
                "new_keywords": [],
                "reasoning": f"åˆ¤æ–­è¿‡ç¨‹å‡ºé”™: {str(e)}"
            }
    
    def _generate_response_with_context(
        self,
        user_query: str,
        chunks: List[RetrievedChunk],
        token_callback: Optional[Callable[[str], None]] = None,
    ) -> str:
        """ç”Ÿæˆå¸¦æœ‰ä¸Šä¸‹æ–‡çš„å›ç­”"""
        try:
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_parts = []
            for i, chunk in enumerate(chunks):
                context_parts.append(
                    f"æ–‡æ¡£ç‰‡æ®µ{i+1} (æ¥æº: {chunk.filename}):\n{chunk.content}"
                )
            context = "\n\n".join(context_parts)
            
            # è·å–å¯¹è¯å†å²
            conversation_history = self.conversation_manager.get_history()
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [
                {"role": "system", "content": f"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n\næ–‡æ¡£å†…å®¹ï¼š\n{context}"}
            ]
            
            # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘3è½®ï¼‰
            for msg in conversation_history[-3:]:
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æŸ¥è¯¢
            messages.append({
                "role": "user", 
                "content": user_query
            })
            
            # æ ¼å¼åŒ–è¾“å…¥
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # ç¼–ç 
            inputs = self.tokenizer(text, return_tensors="pt")
            
            # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            # è®¾ç½®æµå¼è¾“å‡ºç»„ä»¶
            streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
            generation_kwargs = dict(inputs)
            generation_kwargs.update({
                "max_new_tokens": 3000,
                "temperature": 0.7,
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "streamer": streamer
            })
            response_chunks: List[str] = []

            def generate():
                """åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œç”Ÿæˆï¼Œé©±åŠ¨æµå¼è¾“å‡º"""
                with torch.no_grad():
                    self.model.generate(**generation_kwargs)

            generation_thread = Thread(target=generate, daemon=True)
            generation_thread.start()

            show_console = token_callback is None
            if show_console:
                print("   ğŸ’¬ å®æ—¶è¾“å‡º:", end=" ", flush=True)

            for new_text in streamer:
                if token_callback is not None:
                    token_callback(new_text)
                else:
                    print(new_text, end="", flush=True)
                response_chunks.append(new_text)

            generation_thread.join()
            if show_console:
                print()

            return "".join(response_chunks).strip()
            
        except Exception as e:
            print(f"      âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶é‡åˆ°é”™è¯¯: {str(e)}"
    
    def display_response(self, response: WorkflowResponse):
        """æ ¼å¼åŒ–æ˜¾ç¤ºå“åº”ç»“æœ"""
        print(f"\nğŸ¤– AIå›ç­”:")
        print("=" * 60)
        print(response.llm_response)
        print("=" * 60)
        
        print(f"\nğŸ“„ æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ ({len(response.retrieved_chunks)} ä¸ª):")
        print("-" * 60)
        for i, chunk in enumerate(response.retrieved_chunks):
            print(f"\næ–‡æ¡£ {i+1}:")
            print(f"   æ–‡ä»¶å: {chunk.filename}")
            print(f"   ç›¸ä¼¼åº¦: {chunk.score:.4f}")
            print(f"   å†…å®¹é¢„è§ˆ:")
            content = chunk.content
            if len(content) > 200:
                content = content[:200] + "..."
            print(f"      {content}")
            print("-" * 40)
    
    def clear_conversation(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_manager.clear()
        print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º")
    
    def get_conversation_summary(self) -> str:
        """è·å–å¯¹è¯æ‘˜è¦"""
        return f"å¯¹è¯æ¶ˆæ¯æ•°: {len(self.conversation_manager.conversations)}"
    
    def judge_answer(self, question_content: str, question_options: list, selected_answer: str, 
                    correct_answer: str = "", knowledge_point: str = "") -> dict:
        """
        ä½¿ç”¨å¤§æ¨¡å‹æ™ºèƒ½åˆ¤é¢˜ï¼ˆåŸºäºRAGæ£€ç´¢çš„ç‹¬ç«‹åˆ¤æ–­ï¼‰
        
        Args:
            question_content: é¢˜ç›®å†…å®¹
            question_options: é€‰é¡¹åˆ—è¡¨
            selected_answer: å­¦ç”Ÿé€‰æ‹©çš„ç­”æ¡ˆ
            correct_answer: æ­£ç¡®ç­”æ¡ˆï¼ˆä»…ç”¨äºåç«¯éªŒè¯ï¼Œä¸ä¼ é€’ç»™å¤§æ¨¡å‹ï¼‰
            knowledge_point: çŸ¥è¯†ç‚¹
            
        Returns:
            åˆ¤é¢˜ç»“æœå­—å…¸
        """
        try:
            print(f"\nğŸ¤– å¼€å§‹æ™ºèƒ½åˆ¤é¢˜...")
            print(f"é¢˜ç›®: {question_content[:100]}...")
            print(f"é€‰æ‹©ç­”æ¡ˆ: {selected_answer}")
            
            # æ­¥éª¤1: ä½¿ç”¨é¢˜ç›®å†…å®¹è¿›è¡ŒRAGæ£€ç´¢
            print("ğŸ“š æ­¥éª¤1: åŸºäºé¢˜ç›®å†…å®¹è¿›è¡ŒRAGæ£€ç´¢...")
            retrieved_chunks = self._retrieve_documents(question_content)
            print(f"   âœ… æ£€ç´¢åˆ° {len(retrieved_chunks)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
            
            # æ„å»ºåˆ¤é¢˜æç¤ºè¯
            options_text = ""
            if question_options:
                for i, option in enumerate(question_options):
                    if isinstance(option, dict):
                        key = option.get('key', chr(65 + i))
                        text = option.get('text', '')
                    else:
                        key = chr(65 + i)
                        text = str(option)
                    options_text += f"{key}. {text}\n"
            
            # æ„å»ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸Šä¸‹æ–‡
            context_parts = []
            for i, chunk in enumerate(retrieved_chunks):
                context_parts.append(
                    f"å‚è€ƒèµ„æ–™{i+1} (æ¥æº: {chunk.filename}, ç›¸ä¼¼åº¦: {chunk.score:.3f}):\n{chunk.content}"
                )
            context = "\n\n".join(context_parts)
            
            judge_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ“ä½œç³»ç»Ÿè¯¾ç¨‹åˆ¤é¢˜åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™ï¼Œä»”ç»†åˆ†æé¢˜ç›®å†…å®¹å’Œé€‰é¡¹ï¼Œç‹¬ç«‹åˆ¤æ–­æ­£ç¡®ç­”æ¡ˆï¼Œç„¶åè¯„ä¼°å­¦ç”Ÿé€‰æ‹©çš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚

å‚è€ƒèµ„æ–™ï¼š
{context}

é¢˜ç›®å†…å®¹ï¼š{question_content}

é€‰é¡¹ï¼š
{options_text}

å­¦ç”Ÿçš„ç­”æ¡ˆï¼š{selected_answer}
{f"ç›¸å…³çŸ¥è¯†ç‚¹ï¼š{knowledge_point}" if knowledge_point else ""}

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š
1. é¦–å…ˆåˆ†æé¢˜ç›®è€ƒæŸ¥çš„çŸ¥è¯†ç‚¹
2. ç»“åˆå‚è€ƒèµ„æ–™é€ä¸€åˆ†ææ¯ä¸ªé€‰é¡¹çš„æ­£ç¡®æ€§
3. ç¡®å®šæ­£ç¡®ç­”æ¡ˆ
4. è¯„ä¼°å­¦ç”Ÿç­”æ¡ˆçš„æ­£ç¡®æ€§
5. æä¾›è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºåˆ¤é¢˜ç»“æœï¼š

{{
    "isCorrect": true/false,
    "confidence": 0.0-1.0,
    "reasoning": "åˆ¤é¢˜æ¨ç†è¿‡ç¨‹",
    "correctAnswer": "æ­£ç¡®ç­”æ¡ˆé€‰é¡¹",
    "analysis": "è¯¦ç»†åˆ†æ",
    "knowledgePoint": "é¢˜ç›®è€ƒæŸ¥çš„çŸ¥è¯†ç‚¹",
    "optionAnalysis": "å„é€‰é¡¹åˆ†æ"
}}

è¦æ±‚ï¼š
1. isCorrect: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºå­¦ç”Ÿç­”æ¡ˆæ˜¯å¦æ­£ç¡®
2. confidence: 0-1ä¹‹é—´çš„æ•°å€¼ï¼Œè¡¨ç¤ºåˆ¤é¢˜ç½®ä¿¡åº¦
3. reasoning: ç®€æ´çš„åˆ¤é¢˜æ¨ç†è¿‡ç¨‹
4. correctAnswer: ä½ åˆ¤æ–­çš„æ­£ç¡®ç­”æ¡ˆé€‰é¡¹ï¼ˆå¦‚Aã€Bã€Cã€Dï¼‰
5. analysis: å¯¹é¢˜ç›®å’Œç­”æ¡ˆçš„è¯¦ç»†åˆ†æ
6. knowledgePoint: é¢˜ç›®è€ƒæŸ¥çš„ä¸»è¦çŸ¥è¯†ç‚¹
7. optionAnalysis: å¯¹å„é€‰é¡¹çš„è¯¦ç»†åˆ†æ

è¯·ç›´æ¥è¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š"""

            # ä½¿ç”¨LLMè¿›è¡Œåˆ¤é¢˜
            response = self.retrieval_suggester._generate_response_with_history(judge_prompt, "", [])
            
            # æ·»åŠ è°ƒè¯•è¾“å‡º
            print(f"ğŸ” LLMåŸå§‹å“åº”:")
            print(f"   {response[:200]}...")
            print(f"   å“åº”é•¿åº¦: {len(response)}")
            
            # è§£æå“åº”
            result = self._parse_judge_response(response, selected_answer, correct_answer)
            
            print(f"åˆ¤é¢˜ç»“æœ: {result['isCorrect']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
            print(f"AIåˆ¤æ–­çš„æ­£ç¡®ç­”æ¡ˆ: {result['correctAnswer']}")
            if correct_answer:
                print(f"é¢„è®¾æ­£ç¡®ç­”æ¡ˆ: {correct_answer}")
                if result['correctAnswer'] == correct_answer:
                    print("âœ… AIåˆ¤æ–­ä¸é¢„è®¾ç­”æ¡ˆä¸€è‡´")
                else:
                    print("âš ï¸ AIåˆ¤æ–­ä¸é¢„è®¾ç­”æ¡ˆä¸ä¸€è‡´")
            
            return result
            
        except Exception as e:
            print(f"æ™ºèƒ½åˆ¤é¢˜å¤±è´¥: {e}")
            # é™çº§åˆ°ç®€å•åˆ¤æ–­
            is_correct = selected_answer == correct_answer if correct_answer else False
            return {
                "isCorrect": is_correct,
                "confidence": 0.5,
                "reasoning": "æ™ºèƒ½åˆ¤é¢˜å¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ¤æ–­",
                "correctAnswer": correct_answer or "æœªçŸ¥",
                "analysis": f"å­¦ç”Ÿé€‰æ‹©äº†{selected_answer}ï¼Œ{'æ­£ç¡®' if is_correct else 'é”™è¯¯'}ã€‚",
                "knowledgePoint": knowledge_point or "æœªçŸ¥",
                "optionAnalysis": "åˆ†æå¤±è´¥"
            }
    
    def generate_explanation(self, question_content: str, question_options: list, selected_answer: str,
                           correct_answer: str = "", knowledge_point: str = "", is_correct: bool = False) -> str:
        """
        ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆé¢˜ç›®è§£æï¼ˆåŸºäºAIåˆ¤æ–­ç»“æœï¼‰
        
        Args:
            question_content: é¢˜ç›®å†…å®¹
            question_options: é€‰é¡¹åˆ—è¡¨
            selected_answer: å­¦ç”Ÿé€‰æ‹©çš„ç­”æ¡ˆ
            correct_answer: æ­£ç¡®ç­”æ¡ˆï¼ˆä»…ç”¨äºåç«¯éªŒè¯ï¼Œä¸ä¼ é€’ç»™å¤§æ¨¡å‹ï¼‰
            knowledge_point: çŸ¥è¯†ç‚¹
            is_correct: ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
            
        Returns:
            è§£ææ–‡æœ¬
        """
        try:
            print(f"\nğŸ“ å¼€å§‹ç”Ÿæˆè§£æ...")
            print(f"é¢˜ç›®: {question_content[:100]}...")
            print(f"å­¦ç”Ÿç­”æ¡ˆ: {selected_answer} ({'æ­£ç¡®' if is_correct else 'é”™è¯¯'})")
            
            # æ„å»ºè§£æç”Ÿæˆæç¤ºè¯
            options_text = ""
            if question_options:
                for i, option in enumerate(question_options):
                    if isinstance(option, dict):
                        key = option.get('key', chr(65 + i))
                        text = option.get('text', '')
                    else:
                        key = chr(65 + i)
                        text = str(option)
                    options_text += f"{key}. {text}\n"
            
            explanation_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ“ä½œç³»ç»Ÿè¯¾ç¨‹æ•™å­¦åŠ©æ‰‹ã€‚è¯·ä¸ºå­¦ç”Ÿç”Ÿæˆè¯¦ç»†çš„é¢˜ç›®è§£æã€‚

é¢˜ç›®å†…å®¹ï¼š{question_content}

é€‰é¡¹ï¼š
{options_text}

å­¦ç”Ÿçš„ç­”æ¡ˆï¼š{selected_answer}
{f"ç›¸å…³çŸ¥è¯†ç‚¹ï¼š{knowledge_point}" if knowledge_point else ""}
å­¦ç”Ÿç­”æ¡ˆï¼š{'æ­£ç¡®' if is_correct else 'é”™è¯¯'}

è¯·ç”Ÿæˆè¯¦ç»†çš„è§£æï¼ŒåŒ…æ‹¬ï¼š
1. é¢˜ç›®è€ƒæŸ¥çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹
2. å„é€‰é¡¹çš„è¯¦ç»†åˆ†æï¼ˆä¸ºä»€ä¹ˆå¯¹æˆ–é”™ï¼‰
3. æ­£ç¡®ç­”æ¡ˆçš„è¯¦ç»†è§£é‡Š
4. å¦‚æœå­¦ç”Ÿç­”é”™äº†ï¼Œè¯´æ˜é”™è¯¯åŸå› å’Œæ­£ç¡®æ€è·¯
5. ç›¸å…³çš„æ‰©å±•çŸ¥è¯†ç‚¹å’Œå­¦ä¹ å»ºè®®

è¦æ±‚ï¼š
- è§£æè¦è¯¦ç»†ã€å‡†ç¡®ã€æ˜“æ‡‚
- ä½¿ç”¨ä¸­æ–‡å›ç­”
- è¯­è¨€è¦ä¸“ä¸šä½†é€šä¿—æ˜“æ‡‚
- é€‚å½“ä¸¾ä¾‹è¯´æ˜
- å­—æ•°æ§åˆ¶åœ¨300-500å­—
- é‡ç‚¹çªå‡ºçŸ¥è¯†ç‚¹çš„ç†è§£å’Œåº”ç”¨

è¯·ç›´æ¥è¾“å‡ºè§£æå†…å®¹ï¼Œä¸è¦å…¶ä»–æ ¼å¼ï¼š"""

            # ä½¿ç”¨LLMç”Ÿæˆè§£æ
            explanation = self.retrieval_suggester._generate_response_with_history(explanation_prompt, "", [])
            
            # æ¸…ç†å’Œæ ¼å¼åŒ–è§£æ
            explanation = self._clean_explanation(explanation)
            
            print(f"è§£æç”Ÿæˆå®Œæˆ: {len(explanation)}å­—")
            
            return explanation
            
        except Exception as e:
            print(f"è§£æç”Ÿæˆå¤±è´¥: {e}")
            # é™çº§åˆ°ç®€å•è§£æ
            return self._generate_fallback_explanation(question_content, selected_answer, correct_answer, is_correct)
    
    def judge_text_answer(self, question_content: str, student_answer: str, 
                         question_type: str = "é—®ç­”é¢˜", knowledge_point: str = "") -> dict:
        """
        ä½¿ç”¨å¤§æ¨¡å‹æ™ºèƒ½åˆ¤é¢˜ï¼ˆåŸºäºRAGæ£€ç´¢çš„å¡«ç©ºé¢˜/é—®ç­”é¢˜ï¼‰
        
        Args:
            question_content: é¢˜ç›®å†…å®¹
            student_answer: å­¦ç”Ÿç­”æ¡ˆ
            question_type: é¢˜ç›®ç±»å‹ï¼ˆå¡«ç©ºé¢˜/é—®ç­”é¢˜ï¼‰
            knowledge_point: çŸ¥è¯†ç‚¹
            
        Returns:
            åˆ¤é¢˜ç»“æœå­—å…¸
        """
        try:
            print(f"\nğŸ¤– å¼€å§‹æ–‡æœ¬æ™ºèƒ½åˆ¤é¢˜...")
            print(f"é¢˜ç›®: {question_content[:100]}...")
            print(f"å­¦ç”Ÿç­”æ¡ˆ: {student_answer[:50]}...")
            print(f"é¢˜ç›®ç±»å‹: {question_type}")
            
            # æ­¥éª¤1: ä½¿ç”¨é¢˜ç›®å†…å®¹è¿›è¡ŒRAGæ£€ç´¢
            print("ğŸ“š æ­¥éª¤1: åŸºäºé¢˜ç›®å†…å®¹è¿›è¡ŒRAGæ£€ç´¢...")
            retrieved_chunks = self._retrieve_documents(question_content)
            print(f"   âœ… æ£€ç´¢åˆ° {len(retrieved_chunks)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
            
            # æ„å»ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸Šä¸‹æ–‡
            context_parts = []
            for i, chunk in enumerate(retrieved_chunks):
                context_parts.append(
                    f"å‚è€ƒèµ„æ–™{i+1} (æ¥æº: {chunk.filename}, ç›¸ä¼¼åº¦: {chunk.score:.3f}):\n{chunk.content}"
                )
            context = "\n\n".join(context_parts)
            
            # æ„å»ºæ–‡æœ¬åˆ¤é¢˜æç¤ºè¯
            judge_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ“ä½œç³»ç»Ÿè¯¾ç¨‹åˆ¤é¢˜åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™ï¼Œä»”ç»†åˆ†æé¢˜ç›®å†…å®¹å’Œå­¦ç”Ÿç­”æ¡ˆï¼Œç‹¬ç«‹åˆ¤æ–­å­¦ç”Ÿç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚

å‚è€ƒèµ„æ–™ï¼š
{context}

é¢˜ç›®å†…å®¹ï¼š{question_content}

å­¦ç”Ÿç­”æ¡ˆï¼š{student_answer}

é¢˜ç›®ç±»å‹ï¼š{question_type}
{f"ç›¸å…³çŸ¥è¯†ç‚¹ï¼š{knowledge_point}" if knowledge_point else ""}

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š
1. é¦–å…ˆåˆ†æé¢˜ç›®è€ƒæŸ¥çš„çŸ¥è¯†ç‚¹
2. ç»“åˆå‚è€ƒèµ„æ–™ç†è§£å­¦ç”Ÿç­”æ¡ˆçš„æ ¸å¿ƒå†…å®¹
3. åˆ¤æ–­å­¦ç”Ÿç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼ˆè€ƒè™‘è¡¨è¾¾æ–¹å¼çš„ä¸åŒï¼‰
4. è¯„ä¼°ç­”æ¡ˆçš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
5. æä¾›è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºåˆ¤é¢˜ç»“æœï¼š

{{
    "isCorrect": true/false,
    "confidence": 0.0-1.0,
    "reasoning": "åˆ¤é¢˜æ¨ç†è¿‡ç¨‹",
    "correctAnswer": "æ ‡å‡†ç­”æ¡ˆè¦ç‚¹",
    "analysis": "è¯¦ç»†åˆ†æ",
    "knowledgePoint": "é¢˜ç›®è€ƒæŸ¥çš„çŸ¥è¯†ç‚¹",
    "answerQuality": "ç­”æ¡ˆè´¨é‡è¯„ä¼°",
    "improvementSuggestions": "æ”¹è¿›å»ºè®®"
}}

è¦æ±‚ï¼š
1. isCorrect: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºå­¦ç”Ÿç­”æ¡ˆæ˜¯å¦æ­£ç¡®
2. confidence: 0-1ä¹‹é—´çš„æ•°å€¼ï¼Œè¡¨ç¤ºåˆ¤é¢˜ç½®ä¿¡åº¦
3. reasoning: ç®€æ´çš„åˆ¤é¢˜æ¨ç†è¿‡ç¨‹
4. correctAnswer: æ ‡å‡†ç­”æ¡ˆçš„è¦ç‚¹æ€»ç»“
5. analysis: å¯¹é¢˜ç›®å’Œç­”æ¡ˆçš„è¯¦ç»†åˆ†æ
6. knowledgePoint: é¢˜ç›®è€ƒæŸ¥çš„ä¸»è¦çŸ¥è¯†ç‚¹
7. answerQuality: ç­”æ¡ˆè´¨é‡è¯„ä¼°ï¼ˆä¼˜ç§€/è‰¯å¥½/ä¸€èˆ¬/è¾ƒå·®ï¼‰
8. improvementSuggestions: æ”¹è¿›å»ºè®®

è¯·ç›´æ¥è¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š"""

            # ä½¿ç”¨LLMè¿›è¡Œåˆ¤é¢˜
            response = self.retrieval_suggester._generate_response_with_history(judge_prompt, "", [])
            
            # æ·»åŠ è°ƒè¯•è¾“å‡º
            print(f"ğŸ” LLMåŸå§‹å“åº”:")
            print(f"   {response[:200]}...")
            print(f"   å“åº”é•¿åº¦: {len(response)}")
            
            # è§£æå“åº”
            result = self._parse_text_judge_response(response, student_answer)
            
            print(f"æ–‡æœ¬åˆ¤é¢˜ç»“æœ: {result['isCorrect']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
            print(f"ç­”æ¡ˆè´¨é‡: {result.get('answerQuality', 'æœªçŸ¥')}")
            
            return result
            
        except Exception as e:
            print(f"æ–‡æœ¬æ™ºèƒ½åˆ¤é¢˜å¤±è´¥: {e}")
            # é™çº§åˆ°ç®€å•åˆ¤æ–­
            return {
                "isCorrect": True,  # é»˜è®¤è®¤ä¸ºæ­£ç¡®ï¼Œé¿å…æ‰“å‡»å­¦ç”Ÿç§¯ææ€§
                "confidence": 0.5,
                "reasoning": "æ–‡æœ¬åˆ¤é¢˜å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ¤æ–­",
                "correctAnswer": "è¯·å‚è€ƒæ•™æç›¸å…³å†…å®¹",
                "analysis": f"å­¦ç”Ÿå›ç­”äº†{len(student_answer)}å­—çš„å†…å®¹ã€‚",
                "knowledgePoint": knowledge_point or "æœªçŸ¥",
                "answerQuality": "ä¸€èˆ¬",
                "improvementSuggestions": "å»ºè®®å‚è€ƒæ•™æç›¸å…³å†…å®¹å®Œå–„ç­”æ¡ˆ"
            }
    
    def _parse_text_judge_response(self, response: str, student_answer: str) -> dict:
        """è§£ææ–‡æœ¬åˆ¤é¢˜å“åº”"""
        try:
            import re
            import json
            
            print(f"ğŸ” å¼€å§‹è§£ææ–‡æœ¬åˆ¤é¢˜å“åº”...")
            print(f"   åŸå§‹å“åº”é•¿åº¦: {len(response)}")
            
            # æ¸…ç†å“åº”æ–‡æœ¬
            cleaned_response = self.retrieval_suggester._clean_response_text(response)
            print(f"   æ¸…ç†åå“åº”é•¿åº¦: {len(cleaned_response)}")
            print(f"   æ¸…ç†åå“åº”å‰200å­—ç¬¦: {cleaned_response}")
            
            # å°è¯•æå–JSON
            json_matches = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned_response, re.DOTALL)
            print(f"   æ‰¾åˆ° {len(json_matches)} ä¸ªJSONåŒ¹é…")
            
            # æ‰¾åˆ°æœ€å®Œæ•´çš„JSON
            best_json = None
            for i, json_str in enumerate(json_matches):
                print(f"   å°è¯•è§£æJSON {i+1}: {json_str[:100]}...")
                try:
                    data = json.loads(json_str)
                    print(f"   JSON {i+1} è§£ææˆåŠŸï¼ŒåŒ…å«å­—æ®µ: {list(data.keys())}")
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦å­—æ®µ
                    if all(key in data for key in ["isCorrect", "confidence", "reasoning"]):
                        best_json = data
                        print(f"   âœ… JSON {i+1} åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µï¼Œé€‰æ‹©æ­¤JSON")
                        break
                    else:
                        print(f"   âŒ JSON {i+1} ç¼ºå°‘å¿…è¦å­—æ®µ")
                except Exception as e:
                    print(f"   âŒ JSON {i+1} è§£æå¤±è´¥: {e}")
                    continue
            
            if best_json:
                return {
                    "isCorrect": bool(best_json.get("isCorrect", True)),
                    "confidence": float(best_json.get("confidence", 0.5)),
                    "reasoning": best_json.get("reasoning", "è§£ææˆåŠŸ"),
                    "correctAnswer": best_json.get("correctAnswer", "è¯·å‚è€ƒæ•™æç›¸å…³å†…å®¹"),
                    "analysis": best_json.get("analysis", "åˆ†æå®Œæˆ"),
                    "knowledgePoint": best_json.get("knowledgePoint", "æœªçŸ¥"),
                    "answerQuality": best_json.get("answerQuality", "ä¸€èˆ¬"),
                    "improvementSuggestions": best_json.get("improvementSuggestions", "å»ºè®®å‚è€ƒæ•™æç›¸å…³å†…å®¹")
                }
            else:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ¤æ–­
                return {
                    "isCorrect": True,
                    "confidence": 0.5,
                    "reasoning": "JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ¤æ–­",
                    "correctAnswer": "è¯·å‚è€ƒæ•™æç›¸å…³å†…å®¹",
                    "analysis": f"å­¦ç”Ÿå›ç­”äº†{len(student_answer)}å­—çš„å†…å®¹ã€‚",
                    "knowledgePoint": "æœªçŸ¥",
                    "answerQuality": "ä¸€èˆ¬",
                    "improvementSuggestions": "å»ºè®®å‚è€ƒæ•™æç›¸å…³å†…å®¹"
                }
                
        except Exception as e:
            print(f"è§£ææ–‡æœ¬åˆ¤é¢˜å“åº”æ—¶å‡ºé”™: {e}")
            return {
                "isCorrect": True,
                "confidence": 0.5,
                "reasoning": f"è§£æé”™è¯¯: {str(e)}",
                "correctAnswer": "è¯·å‚è€ƒæ•™æç›¸å…³å†…å®¹",
                "analysis": f"å­¦ç”Ÿå›ç­”äº†{len(student_answer)}å­—çš„å†…å®¹ã€‚",
                "knowledgePoint": "æœªçŸ¥",
                "answerQuality": "ä¸€èˆ¬",
                "improvementSuggestions": "å»ºè®®å‚è€ƒæ•™æç›¸å…³å†…å®¹"
            }
    
    def _parse_judge_response(self, response: str, selected_answer: str, correct_answer: str) -> dict:
        """è§£æåˆ¤é¢˜å“åº”"""
        try:
            import re
            import json
            
            # æ¸…ç†å“åº”æ–‡æœ¬
            cleaned_response = self.retrieval_suggester._clean_response_text(response)
            
            # å°è¯•æå–JSON
            json_matches = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned_response, re.DOTALL)
            
            # æ‰¾åˆ°æœ€å®Œæ•´çš„JSON
            best_json = None
            for json_str in json_matches:
                try:
                    data = json.loads(json_str)
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦å­—æ®µ
                    if all(key in data for key in ["isCorrect", "confidence", "reasoning"]):
                        best_json = data
                        break
                except:
                    continue
            
            if best_json:
                return {
                    "isCorrect": bool(best_json.get("isCorrect", False)),
                    "confidence": float(best_json.get("confidence", 0.5)),
                    "reasoning": best_json.get("reasoning", "è§£ææˆåŠŸ"),
                    "correctAnswer": best_json.get("correctAnswer", correct_answer or "æœªçŸ¥"),
                    "analysis": best_json.get("analysis", "åˆ†æå®Œæˆ"),
                    "knowledgePoint": best_json.get("knowledgePoint", "æœªçŸ¥"),
                    "optionAnalysis": best_json.get("optionAnalysis", "é€‰é¡¹åˆ†æå®Œæˆ")
                }
            else:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ¤æ–­
                is_correct = selected_answer == correct_answer if correct_answer else False
                return {
                    "isCorrect": is_correct,
                    "confidence": 0.5,
                    "reasoning": "JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ¤æ–­",
                    "correctAnswer": correct_answer or "æœªçŸ¥",
                    "analysis": f"å­¦ç”Ÿé€‰æ‹©äº†{selected_answer}ï¼Œ{'æ­£ç¡®' if is_correct else 'é”™è¯¯'}ã€‚",
                    "knowledgePoint": "æœªçŸ¥",
                    "optionAnalysis": "åˆ†æå¤±è´¥"
                }
                
        except Exception as e:
            print(f"è§£æåˆ¤é¢˜å“åº”æ—¶å‡ºé”™: {e}")
            is_correct = selected_answer == correct_answer if correct_answer else False
            return {
                "isCorrect": is_correct,
                "confidence": 0.5,
                "reasoning": f"è§£æé”™è¯¯: {str(e)}",
                "correctAnswer": correct_answer or "æœªçŸ¥",
                "analysis": f"å­¦ç”Ÿé€‰æ‹©äº†{selected_answer}ï¼Œ{'æ­£ç¡®' if is_correct else 'é”™è¯¯'}ã€‚",
                "knowledgePoint": "æœªçŸ¥",
                "optionAnalysis": "åˆ†æå¤±è´¥"
            }
    
    def _clean_explanation(self, explanation: str) -> str:
        """æ¸…ç†å’Œæ ¼å¼åŒ–è§£ææ–‡æœ¬"""
        import re
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        explanation = re.sub(r'\s+', ' ', explanation)
        
        # ç§»é™¤ç‰¹æ®Šæ ‡è®°
        explanation = re.sub(r'è§£æ[:ï¼š]\s*', '', explanation)
        explanation = re.sub(r'ç­”æ¡ˆ[:ï¼š]\s*', '', explanation)
        
        # ç¡®ä¿ä»¥å¥å·ç»“å°¾
        if explanation and not explanation.endswith(('ã€‚', '.', 'ï¼', '!')):
            explanation += 'ã€‚'
        
        return explanation.strip()
    
    def _generate_fallback_explanation(self, question_content: str, selected_answer: str, 
                                      correct_answer: str, is_correct: bool) -> str:
        """ç”Ÿæˆé™çº§è§£æ"""
        if is_correct:
            return f"æ­å–œï¼æ‚¨é€‰æ‹©äº†{selected_answer}ï¼Œè¿™æ˜¯æ­£ç¡®ç­”æ¡ˆã€‚è¿™é“é¢˜è€ƒæŸ¥äº†ç›¸å…³çš„æ“ä½œç³»ç»ŸçŸ¥è¯†ç‚¹ï¼Œæ‚¨çš„ç†è§£æ˜¯æ­£ç¡®çš„ã€‚"
        else:
            return f"å¾ˆé—æ†¾ï¼Œæ‚¨é€‰æ‹©äº†{selected_answer}ï¼Œä½†æ­£ç¡®ç­”æ¡ˆæ˜¯{correct_answer}ã€‚å»ºè®®æ‚¨é‡æ–°å­¦ä¹ ç›¸å…³çŸ¥è¯†ç‚¹ï¼ŒåŠ æ·±ç†è§£ã€‚"

# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•° - äº¤äº’å¼RAGå·¥ä½œæµç³»ç»Ÿ"""
    print("ğŸš€ ç®€åŒ–RAGå·¥ä½œæµç³»ç»Ÿå¯åŠ¨ä¸­...")
    print("å®ç°å·¥ä½œæµç¨‹:")
    print("1. ç”¨æˆ·ç»™å‡ºè¦æ±‚")
    print("2. ç›´æ¥ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¿›è¡Œå‘é‡æ£€ç´¢")
    print("3. æŠŠæ£€ç´¢åˆ°çš„chunkå†…å®¹å’Œå¯¹è¯å†å²ä½œä¸ºè¾“å…¥ç»™LLMç”Ÿæˆæœ€ç»ˆå›ç­”")
    print()
    
    # é…ç½®å‚æ•°
    config = {
        "llm_path": "../models--Qwen--Qwen3-8B/snapshots/9c925d64d72725edaf899c6cb9c377fd0709d9c5",
        "embedding_model_path": "/home/ubuntu/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/",
        "db_path": "./vector_db"
    }
    
    try:
        # åˆå§‹åŒ–å·¥ä½œæµ
        workflow = SimpleRAGWorkflow(**config)
        
        print("\nâœ… ç³»ç»Ÿå¯åŠ¨å®Œæˆ!")
        print("\nä½¿ç”¨è¯´æ˜:")
        print("  - è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œç³»ç»Ÿå°†æŒ‰ç…§2æ­¥å·¥ä½œæµå¤„ç†")
        print("  - è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
        print("  - è¾“å…¥ 'summary' æŸ¥çœ‹å¯¹è¯æ‘˜è¦")
        print("  - è¾“å…¥ 'exit' é€€å‡ºç³»ç»Ÿ")
        
        last_response = None
        
        while True:
            try:
                user_input = input("\nğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                
                if not user_input:
                    continue
                
                if user_input.lower() == 'exit':
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ç®€åŒ–RAGå·¥ä½œæµç³»ç»Ÿ!")
                    break
                elif user_input.lower() == 'clear':
                    workflow.clear_conversation()
                    continue
                elif user_input.lower() == 'summary':
                    summary = workflow.get_conversation_summary()
                    print(f"ğŸ“Š å¯¹è¯æ‘˜è¦: {summary}")
                    continue
                
                # å¤„ç†ç”¨æˆ·æŸ¥è¯¢
                response = workflow.process_user_query(user_input)
                last_response = response
                
                # æ˜¾ç¤ºç»“æœ
                workflow.display_response(response)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç³»ç»Ÿå·²ä¸­æ–­ï¼Œæ„Ÿè°¢ä½¿ç”¨!")
                break
            except Exception as e:
                print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                print("è¯·é‡è¯•æˆ–è¾“å…¥ 'exit' é€€å‡ºç³»ç»Ÿ")
                
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥:")
        print("1. æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. ä¾èµ–åŒ…æ˜¯å¦å®Œæ•´å®‰è£…")

if __name__ == "__main__":
    main()

