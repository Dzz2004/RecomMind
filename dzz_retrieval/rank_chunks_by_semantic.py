import os
import json
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer
import numpy as np

# === é…ç½® ===
BGE_MODEL_PATH = "/home/ubuntu/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/"

def _get_chunks_json_paths() -> List[str]:
    """è¿”å›å¯èƒ½å­˜åœ¨çš„ chunks JSON åˆ—è¡¨ï¼Œå­˜åœ¨å³åŠ è½½"""
    base = os.path.dirname(__file__)
    candidates = [
        os.path.join(base, "kernel_chunks_with_descriptions.json"),
        os.path.join(base, "mm_treesitter_chunks_with_descriptions.json"),
    ]
    # å…¼å®¹é¡¹ç›®å¤–å±‚ç»“æ„
    outer = os.path.dirname(os.path.dirname(base))
    candidates += [
        os.path.join(outer, "dzz_retrieval", "kernel_chunks_with_descriptions.json"),
        os.path.join(outer, "dzz_retrieval", "mm_treesitter_chunks_with_descriptions.json"),
    ]
    # å»é‡ä½†ä¿æŒé¡ºåº
    seen, ordered = set(), []
    for p in candidates:
        if p not in seen:
            seen.add(p)
            ordered.append(p)
    return ordered

CHUNKS_JSON_PATHS = _get_chunks_json_paths()

print("ğŸ§  åŠ è½½ BGE-M3 æ¨¡å‹ï¼ˆä»…ç”¨äº description è¯­ä¹‰æ‰“åˆ†ï¼‰...")
_embedder = SentenceTransformer(BGE_MODEL_PATH, device="cuda")
_embedder.max_seq_length = 512  # è¶³å¤Ÿè¦†ç›– description


def _load_one_chunks_json(path: str) -> Dict[str, List[dict]]:
    """åŠ è½½å•ä¸ª JSONï¼Œè¿”å› {file_path -> [chunks]}ï¼Œå¹¶ä¸ºæ¯ä¸ª chunk é™„å¸¦æ ‡å‡†åŒ– file_path"""
    if not os.path.exists(path):
        return {}

    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    file_to_chunks: Dict[str, List[dict]] = {}
    # å…¼å®¹å½¢å¦‚ {"directories": {...}} çš„ç»“æ„
    dirs = data.get("directories", {})
    for dir_info in dirs.values():
        for file_info in dir_info.get("files", []):
            src_path = str(file_info.get("file_path", "")).replace("\\", "/")
            if not src_path:
                continue
            file_to_chunks.setdefault(src_path, [])
            for chunk in file_info.get("chunks", []):
                chunk["file_path"] = src_path
                file_to_chunks[src_path].append(chunk)

    return file_to_chunks


def load_chunks_index() -> Dict[str, List[dict]]:
    """
    æ„å»ºç»Ÿä¸€çš„æºæ–‡ä»¶ -> chunks åˆ—è¡¨ç´¢å¼•ï¼ˆåˆå¹¶å¤šä¸ª JSONï¼‰
    e.g. "kernel/acct.c" -> [...], "mm/madvise.c" -> [...]
    """
    merged: Dict[str, List[dict]] = {}
    loaded_files = 0
    used_json = 0

    for path in CHUNKS_JSON_PATHS:
        if not os.path.exists(path):
            continue
        idx = _load_one_chunks_json(path)
        if not idx:
            continue
        used_json += 1
        for k, v in idx.items():
            merged.setdefault(k, []).extend(v)
        loaded_files += len(idx)

    print(f"âœ… å·²åŠ è½½ {loaded_files} ä¸ªæºæ–‡ä»¶çš„ chunks ç´¢å¼•ï¼ˆæ¥è‡ª {used_json} ä¸ª JSONï¼‰")
    return merged


# å…¨å±€ç´¢å¼•ï¼ˆå¯åŠ¨æ—¶åŠ è½½ä¸€æ¬¡ï¼‰
CHUNKS_INDEX = load_chunks_index()


def rank_chunks_by_description(query: str, candidate_source_files: List[str], top_k: int = 5) -> List[Dict]:
    """
    å¯¹å€™é€‰æºæ–‡ä»¶ä¸­çš„æ‰€æœ‰ chunksï¼ŒæŒ‰ description ä¸ query çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ’åº
    Args:
        query: ç”¨æˆ·è‡ªç„¶è¯­è¨€æŸ¥è¯¢
        candidate_source_files: é˜¶æ®µ1è¿”å›çš„æºæ–‡ä»¶åˆ—è¡¨ï¼ˆå¦‚ ["kernel/acct.c", "mm/madvise.c"]ï¼‰
        top_k: è¿”å› top-k chunks
    Returns:
        æ’åºåçš„ chunk åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« _score å­—æ®µ
    """
    if not candidate_source_files:
        return []

    # 1. ç¼–ç æŸ¥è¯¢
    query_emb = _embedder.encode([query], normalize_embeddings=True)[0]

    # 2. æ”¶é›†å€™é€‰ chunks
    all_candidate_chunks = []
    for src_file in candidate_source_files:
        key = str(src_file).replace("\\", "/")
        for chunk in CHUNKS_INDEX.get(key, []):
            desc = chunk.get("description", "").strip()
            if not desc or desc.startswith(("[ERROR", "è¯¥å—åŒ…å«å¤´æ–‡ä»¶")):
                continue
            all_candidate_chunks.append(chunk)

    if not all_candidate_chunks:
        return []

    # 3. æ‰¹é‡ç¼–ç  descriptions
    descriptions = [c["description"] for c in all_candidate_chunks]
    desc_embs = _embedder.encode(descriptions, normalize_embeddings=True)

    # 4. ç›¸ä¼¼åº¦ä¸æ’åº
    similarities = (desc_embs @ query_emb).tolist()
    for chunk, score in zip(all_candidate_chunks, similarities):
        chunk["_score"] = float(score)
    sorted_chunks = sorted(all_candidate_chunks, key=lambda x: x["_score"], reverse=True)
    return sorted_chunks[:top_k]


if __name__ == "__main__":
    # å°æµ‹
    top_files = ["kernel/acct.c", "mm/madvise.c"]
    query = "Linux å¦‚ä½•å®ç°è¿›ç¨‹è®°è´¦å’Œå†…å­˜å›æ”¶ç­–ç•¥ï¼Ÿ"
    top_chunks = rank_chunks_by_description(query, top_files, top_k=3)
    print(f"\nğŸ” æŸ¥è¯¢: {query}")
    print(f"ğŸ“‚ å€™é€‰æ–‡ä»¶: {', '.join(top_files)}\n")
    for i, chunk in enumerate(top_chunks, 1):
        print(f"[{i}] ç›¸ä¼¼åº¦: {chunk['_score']:.4f}")
        file_path = chunk.get('file_path', 'unknown').replace('\\', '/')
        print(f"    æ–‡ä»¶: {file_path}")
        print(f"    è¡Œå·: {chunk.get('start_line', 'N/A')} - {chunk.get('end_line', 'N/A')}")
        print(f"    å‡½æ•°: {chunk.get('function_name', 'N/A')}")
        print(f"    æè¿°: {chunk.get('description', '')[:200]}...\n")