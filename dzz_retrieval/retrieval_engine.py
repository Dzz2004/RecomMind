import os
import json
import time
from typing import List, Dict, Any, Optional, Tuple

import chromadb
from chromadb.utils import embedding_functions

from .rank_chunks_by_semantic import rank_chunks_by_description


class RetrievalEngine:
    def __init__(
        self,
        chroma_md_path: str = "./chroma_md",
        bge_model_path: str = "/home/ubuntu/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/",
        top_files: int = 3,
        top_chunks: int = 5,
        output_dir: str = "./retrieval_results",
        collections: Optional[Dict[str, str]] = None,
    ):
        """
        collections: å¯é€‰ï¼Œå½¢å¦‚ {"kernel_file_summaries": "kernel", "mm_file_summaries": "mm"}
        - key: Chroma é›†åˆå
        - value: è¯¥é›†åˆå¯¹åº”æºç å‰ç¼€ï¼ˆä¸ chunks JSON çš„ file_path å‰ç¼€ä¸€è‡´ï¼‰
        è‹¥ä¸º Noneï¼Œåˆ™å°è¯•åŠ è½½ä¸Šè¿°ä¸¤ä¸ªé»˜è®¤é›†åˆï¼Œå­˜åœ¨å³å¯ç”¨ã€‚
        """
        self.top_files = top_files
        self.top_chunks = top_chunks
        self.output_dir = output_dir
        self.result_path_last: Optional[str] = None
        os.makedirs(output_dir, exist_ok=True)

        # åˆå§‹åŒ– ChromaDB ä¸åµŒå…¥å‡½æ•°
        self.client = chromadb.PersistentClient(path=chroma_md_path)
        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=bge_model_path,
            device="cuda",
            normalize_embeddings=True
        )

        # è§£æå¹¶åŠ è½½é›†åˆ
        default_map: Dict[str, str] = {
            "kernel_file_summaries": "kernel",
            "mm_file_summaries": "mm",
        }
        self._domain_map: Dict[str, str] = collections or default_map
        self._collections: List[Tuple[str, chromadb.api.models.Collection.Collection]] = []

        for coll_name, prefix in self._domain_map.items():
            try:
                coll = self.client.get_collection(
                    name=coll_name,
                    embedding_function=self.embedding_fn
                )
                self._collections.append((prefix, coll))
            except Exception:
                # é›†åˆå¯èƒ½ä¸å­˜åœ¨ï¼Œè·³è¿‡
                continue

        if not self._collections:
            raise RuntimeError("æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„ Chroma é›†åˆï¼Œè¯·å…ˆæ„å»ºå‘é‡åº“æˆ–æ£€æŸ¥é›†åˆåç§°ã€‚")

        total = sum(c.count() for _, c in self._collections)
        enabled = ", ".join([f"{p}:{c.name}" for p, c in self._collections])
        print(f"âœ… RetrievalEngine åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨é›†åˆ: {enabled}ï¼Œæ€»æ–‡ä»¶æ•°: {total}")

    def get_collections_info(self) -> Dict[str, int]:
        """è¿”å›å½“å‰å¯ç”¨çš„é›†åˆåŠå…¶æ–‡ä»¶æ•°"""
        info = {}
        for prefix, coll in self._collections:
            info[f"{prefix}:{coll.name}"] = coll.count()
        return info

    def retrieve(self, query: str) -> Dict[str, Any]:
        """
        å®Œæ•´ä¸¤é˜¶æ®µæ£€ç´¢ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœ
        {
            "query": "...",
            "timestamp": "...",
            "retrieved_files": [
                {
                    "source_file": "kernel/acct.c" | "mm/madvise.c" | ...,
                    "md_summary": "...",
                    "similarity": 0.85,
                    "chunks": [
                        {
                            "chunk_id": 3,
                            "file_path": "kernel/acct.c",
                            "start_line": 544,
                            "end_line": 644,
                            "function_name": "...",
                            "description": "...",
                            "similarity": 0.89
                        }
                    ]
                }
            ]
        }
        """
        # === é˜¶æ®µ1ï¼šè·¨å¤šä¸ªé›†åˆçš„æ–‡ä»¶å¬å›å¹¶å…¨å±€åˆå¹¶ Top-K ===
        merged_hits: List[Dict[str, Any]] = []
        per_domain_k = max(self.top_files, 5)  # é€‚åº¦è¿‡é‡‡æ ·ï¼Œä¾¿äºå…¨å±€åˆå¹¶

        for prefix, coll in self._collections:
            try:
                md_results = coll.query(
                    query_texts=[query],
                    n_results=per_domain_k,
                    include=["documents", "metadatas", "distances"]
                )
            except Exception:
                continue

            docs = md_results.get("documents", [[]])[0]
            metas = md_results.get("metadatas", [[]])[0]
            dists = md_results.get("distances", [[]])[0]

            for doc, meta, dist in zip(docs, metas, dists):
                raw_src = str(meta.get("source_file", "")).replace("\\", "/").lstrip("/")
                # è‹¥æœªå¸¦å‰ç¼€ï¼Œåˆ™è¡¥é½ prefixï¼›å·²å¸¦å‰ç¼€åˆ™ä¿æŒ
                if raw_src and not raw_src.startswith(prefix + "/"):
                    source_file = f"{prefix}/{raw_src}"
                else:
                    source_file = raw_src or prefix

                merged_hits.append({
                    "source_file": source_file,
                    "md_summary": doc,
                    "similarity": 1 - float(dist),
                })

        merged_hits.sort(key=lambda x: x["similarity"], reverse=True)
        top_file_hits = merged_hits[: self.top_files]

        retrieved_files: List[Dict[str, Any]] = [
            {
                "source_file": h["source_file"],
                "md_summary": h["md_summary"],
                "similarity": h["similarity"],
                "chunks": []
            }
            for h in top_file_hits
        ]
        candidate_source_files = [h["source_file"] for h in top_file_hits]

        # === é˜¶æ®µ2ï¼šåœ¨å€™é€‰æ–‡ä»¶ä¸­æŒ‰ description è¯­ä¹‰æ‰“åˆ†ï¼Œè¿”å›è¯¥æ–‡ä»¶ Top-N chunks ===
        all_top_chunks = rank_chunks_by_description(query, candidate_source_files, top_k=1000)

        # æŒ‰æ–‡ä»¶åˆ†ç»„å¹¶æˆªæ–­
        file_to_chunks: Dict[str, List[Dict[str, Any]]] = {}
        for chunk in all_top_chunks:
            src_file = str(chunk.get("file_path", "")).replace("\\", "/")
            if not src_file:
                continue
            file_to_chunks.setdefault(src_file, []).append({
                "chunk_id": chunk["chunk_id"],
                "file_path": chunk["file_path"],
                "start_line": chunk["start_line"],
                "end_line": chunk["end_line"],
                "content": chunk.get("content", ""),
                "function_name": chunk.get("function_name", "N/A"),
                "description": chunk.get("description", ""),
                "similarity": chunk["_score"],
            })

        # å¡«å……åˆ°æ£€ç´¢ç»“æœä¸­
        for item in retrieved_files:
            src = item["source_file"].replace("\\", "/")
            item["chunks"] = file_to_chunks.get(src, [])[: self.top_chunks]

        # === æ„å»ºæœ€ç»ˆç»“æœ ===
        result = {
            "query": query,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "retrieved_files": retrieved_files
        }

        # === ä¿å­˜ JSON ===
        safe_query = "".join(c if c.isalnum() else "_" for c in query[:30])
        file_name = f"result_{safe_query}_{int(time.time())}.json"
        out_path = os.path.join(self.output_dir, file_name)
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        self.result_path_last = out_path

        return result

    @staticmethod
    def print_retrieval_summary(result: Dict[str, Any]):
        print(f"Query: {result.get('query')}")
        print(f"Time: {result.get('timestamp')}")
        for i, item in enumerate(result.get("retrieved_files", []), 1):
            print(f"\n[{i}] æºæ–‡ä»¶: {item.get('source_file')}")
            print(f"     ç›¸ä¼¼åº¦: {item.get('similarity'):.4f}")
            md = item.get("md_summary", "")
            print(f"     æ‘˜è¦é¢„è§ˆ:\n{md[:300]}{'...' if len(md) > 300 else ''}")
            for j, c in enumerate(item.get("chunks", []), 1):
                print(f"  - Chunk {j}: {c.get('function_name', 'N/A')}  "
                      f"({c.get('start_line', 'N/A')}-{c.get('end_line', 'N/A')})  "
                      f"score={c.get('similarity', 0):.4f}")

# === äº¤äº’ Demo ===
if __name__ == "__main__":
    engine = RetrievalEngine(
        chroma_md_path="./chroma_md",
        top_files=3,
        top_chunks=3,
        output_dir="./retrieval_results",
        # å¯æ˜¾å¼ä¼ å…¥æˆ–ç”¨é»˜è®¤ï¼škernel+mm æœ‰å“ªä¸ªåŠ è½½å“ªä¸ª
        # collections={"kernel_file_summaries":"kernel", "mm_file_summaries":"mm"}
    )

    print("ğŸš€ Linux å†…æ ¸æ•™å­¦æ£€ç´¢ç³»ç»Ÿ (RAG ä¸Šä¸‹æ–‡ç”Ÿæˆå™¨)")
    print("è¾“å…¥è‡ªç„¶è¯­è¨€é—®é¢˜ï¼ˆå¦‚ 'Linux å¦‚ä½•å®ç°è¿›ç¨‹è®°è´¦ï¼Ÿ'ï¼‰ï¼Œè¾“å…¥ 'quit' é€€å‡º\n")

    while True:
        query = input("â“ Query > ").strip()
        if query.lower() in {"quit", "exit", "q"}:
            break
        if not query:
            continue

        try:
            result = engine.retrieve(query)
            print("\nâœ… æ£€ç´¢å®Œæˆï¼è¿”å›ç»“æ„åŒ–ä¸Šä¸‹æ–‡ï¼ˆå¯ç”¨äºä¸‹æ¸¸ LLMï¼‰ï¼š")
            engine.print_retrieval_summary(result)
            print("\n" + "=" * 80 + "\n")
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")