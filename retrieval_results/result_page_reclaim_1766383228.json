{
  "query": "page reclaim",
  "timestamp": "2025-12-22 14:00:28",
  "retrieved_files": [
    {
      "source_file": "mm/damon/reclaim.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:50:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `damon\\reclaim.c`\n\n---\n\n# `damon/reclaim.c` 技术文档\n\n## 1. 文件概述\n\n`damon/reclaim.c` 是 Linux 内核中基于 **DAMON（Data Access MONitor）** 框架实现的**自动内存回收模块**。该模块通过监控物理内存区域的访问模式，识别长时间未被访问的“冷”内存页，并主动将其回收（page-out），从而释放系统内存资源。其核心目标是在不影响系统性能的前提下，智能地回收低价值内存，提升内存利用率。\n\n该模块以可加载内核模块（LKM）形式存在，通过一组可调参数控制其行为，并支持基于水位线（watermarks）的条件激活机制，避免在内存充足时进行不必要的回收操作。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`enabled`**: 全局开关，控制 DAMON_RECLAIM 功能是否启用。\n- **`commit_inputs`**: 触发参数重载的标志位，用于运行时动态更新配置（除 `enabled` 外）。\n- **`min_age`**: 冷内存判定阈值（微秒），默认 120 秒。\n- **`damon_reclaim_quota`**: 回收配额控制结构，限制单位时间内的最大回收量（默认每秒最多 128 MiB）和 CPU 时间开销（默认最多 10 ms）。\n- **`damon_reclaim_wmarks`**: 水位线配置，基于空闲内存比率决定是否激活回收（高/中/低水位分别为 50%/40%/20%）。\n- **`damon_reclaim_mon_attrs`**: DAMON 监控属性，定义采样间隔（5ms）、聚合间隔（100ms）等。\n- **`monitor_region_start/end`**: 目标监控内存区域的物理地址范围，默认为系统最大连续 RAM 区域。\n- **`skip_anon`**: 布尔标志，若为真则跳过匿名页（anonymous pages）的回收。\n- **`kdamond_pid`**: DAMON 工作线程的 PID，未启用时为 -1。\n- **`damon_reclaim_stat`**: 统计信息结构，记录尝试回收区域数、成功回收区域数及配额超限次数。\n\n### 主要函数\n\n- **`damon_reclaim_new_scheme()`**: 创建 DAMOS（DAMON Operation Scheme）策略，定义“冷内存”模式（大小 ≥ PAGE_SIZE、访问次数为 0、年龄 ≥ `min_age`）并指定操作为 `DAMOS_PAGEOUT`。\n- **`damon_reclaim_apply_parameters()`**: 应用所有用户配置参数到 DAMON 上下文（`ctx`），包括监控属性、回收策略、过滤器（如 `skip_anon`）和监控区域。\n- **`damon_reclaim_turn()`**: 启动或停止 DAMON_RECLAIM 的核心监控与回收逻辑。\n- **`damon_reclaim_enabled_store()`**: `enabled` 参数的 setter 回调，处理启用/禁用逻辑。\n- **`damon_reclaim_handle_commit_inputs()`**: 处理 `commit_inputs` 标志，触发运行时参数重载。\n- **`damon_reclaim_after_aggregation()` / `damon_reclaim_after_wmarks_check()`**: DAMON 回调函数，在聚合后和水位检查后更新统计信息并处理参数提交。\n- **`damon_reclaim_init()`**: 模块初始化函数，创建 DAMON 上下文和目标，注册回调，并根据初始 `enabled` 状态决定是否启动。\n\n## 3. 关键实现\n\n### 冷内存识别与回收策略\n- 通过 `damon_reclaim_new_scheme()` 定义 DAMOS 策略：\n  - **访问模式匹配**：区域大小 ≥ `PAGE_SIZE`、访问次数 = 0、年龄 ≥ `min_age / aggr_interval`（转换为聚合周期单位）。\n  - **操作类型**：`DAMOS_PAGEOUT`，即对匹配区域执行页面回收。\n  - **配额控制**：使用 `damon_reclaim_quota` 限制回收速度和 CPU 开销，确保系统稳定性。\n  - **水位激活**：仅当空闲内存比率低于 `high` 水位（50%）时激活策略，高于 `low` 水位（20%）时停用。\n\n### 动态参数更新机制\n- 用户可通过写入 `commit_inputs=Y` 触发运行时参数重载（`min_age`、配额、水位、监控区域等）。\n- `damon_reclaim_handle_commit_inputs()` 在 DAMON 的聚合后或水位检查后回调中执行重载，确保线程安全。\n- 重载时保留旧策略的配额状态（如已消耗的配额），避免统计中断。\n\n### 匿名页过滤\n- 若 `skip_anon=Y`，通过 `DAMOS_FILTER_TYPE_ANON` 过滤器排除匿名页（如进程堆栈、堆内存），仅回收文件缓存等页面。\n\n### 监控区域自动配置\n- 默认使用 `damon_set_region_biggest_system_ram_default()` 自动选择系统中最大的连续物理 RAM 区域作为监控目标，用户也可通过 `monitor_region_start/end` 手动指定。\n\n## 4. 依赖关系\n\n- **DAMON 核心框架** (`<linux/damon.h>`): 依赖 DAMON 提供的内存访问监控、策略引擎（DAMOS）、配额管理、水位控制等基础设施。\n- **内核模块通用接口** (`modules-common.h`): 使用 `DEFINE_DAMON_MODULES_*` 宏简化参数声明和统计暴露。\n- **内存管理子系统**: 通过 `DAMOS_PAGEOUT` 操作与 MM 子系统交互，实际执行页面回收。\n- **参数解析工具** (`<linux/kstrtox.h>`): 用于解析用户输入的布尔值和数值参数。\n\n## 5. 使用场景\n\n- **内存压力缓解**: 在内存紧张但尚未触发传统 LRU 回收或 OOM Killer 之前，提前回收长期未使用的冷内存，延缓内存压力。\n- **容器/虚拟机内存优化**: 在容器或 VM 中部署，自动回收应用未使用的缓存内存，提高宿主机内存密度。\n- **大内存系统调优**: 在 TB 级内存服务器上，减少因缓存膨胀导致的内存浪费，提升整体内存效率。\n- **低延迟敏感场景**: 通过配额限制（`ms=10`）确保回收操作不会显著影响关键任务的延迟。\n- **调试与监控**: 通过 `kdamond_pid` 和统计参数（`reclaim_tried_regions` 等）监控 DAMON_RECLAIM 的运行状态和效果。",
      "similarity": 0.5661492347717285,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/damon/reclaim.c",
          "start_line": 153,
          "end_line": 254,
          "content": [
            "static void damon_reclaim_copy_quota_status(struct damos_quota *dst,",
            "\t\tstruct damos_quota *src)",
            "{",
            "\tdst->total_charged_sz = src->total_charged_sz;",
            "\tdst->total_charged_ns = src->total_charged_ns;",
            "\tdst->charged_sz = src->charged_sz;",
            "\tdst->charged_from = src->charged_from;",
            "\tdst->charge_target_from = src->charge_target_from;",
            "\tdst->charge_addr_from = src->charge_addr_from;",
            "}",
            "static int damon_reclaim_apply_parameters(void)",
            "{",
            "\tstruct damos *scheme, *old_scheme;",
            "\tstruct damos_filter *filter;",
            "\tint err = 0;",
            "",
            "\terr = damon_set_attrs(ctx, &damon_reclaim_mon_attrs);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\t/* Will be freed by next 'damon_set_schemes()' below */",
            "\tscheme = damon_reclaim_new_scheme();",
            "\tif (!scheme)",
            "\t\treturn -ENOMEM;",
            "\tif (!list_empty(&ctx->schemes)) {",
            "\t\tdamon_for_each_scheme(old_scheme, ctx)",
            "\t\t\tdamon_reclaim_copy_quota_status(&scheme->quota,",
            "\t\t\t\t\t&old_scheme->quota);",
            "\t}",
            "\tif (skip_anon) {",
            "\t\tfilter = damos_new_filter(DAMOS_FILTER_TYPE_ANON, true);",
            "\t\tif (!filter) {",
            "\t\t\t/* Will be freed by next 'damon_set_schemes()' below */",
            "\t\t\tdamon_destroy_scheme(scheme);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "\t\tdamos_add_filter(scheme, filter);",
            "\t}",
            "\tdamon_set_schemes(ctx, &scheme, 1);",
            "",
            "\treturn damon_set_region_biggest_system_ram_default(target,",
            "\t\t\t\t\t&monitor_region_start,",
            "\t\t\t\t\t&monitor_region_end);",
            "}",
            "static int damon_reclaim_turn(bool on)",
            "{",
            "\tint err;",
            "",
            "\tif (!on) {",
            "\t\terr = damon_stop(&ctx, 1);",
            "\t\tif (!err)",
            "\t\t\tkdamond_pid = -1;",
            "\t\treturn err;",
            "\t}",
            "",
            "\terr = damon_reclaim_apply_parameters();",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\terr = damon_start(&ctx, 1, true);",
            "\tif (err)",
            "\t\treturn err;",
            "\tkdamond_pid = ctx->kdamond->pid;",
            "\treturn 0;",
            "}",
            "static int damon_reclaim_enabled_store(const char *val,",
            "\t\tconst struct kernel_param *kp)",
            "{",
            "\tbool is_enabled = enabled;",
            "\tbool enable;",
            "\tint err;",
            "",
            "\terr = kstrtobool(val, &enable);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (is_enabled == enable)",
            "\t\treturn 0;",
            "",
            "\t/* Called before init function.  The function will handle this. */",
            "\tif (!ctx)",
            "\t\tgoto set_param_out;",
            "",
            "\terr = damon_reclaim_turn(enable);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "set_param_out:",
            "\tenabled = enable;",
            "\treturn err;",
            "}",
            "static int damon_reclaim_handle_commit_inputs(void)",
            "{",
            "\tint err;",
            "",
            "\tif (!commit_inputs)",
            "\t\treturn 0;",
            "",
            "\terr = damon_reclaim_apply_parameters();",
            "\tcommit_inputs = false;",
            "\treturn err;",
            "}"
          ],
          "function_name": "damon_reclaim_copy_quota_status, damon_reclaim_apply_parameters, damon_reclaim_turn, damon_reclaim_enabled_store, damon_reclaim_handle_commit_inputs",
          "description": "实现DAMON_RECLAIM参数动态应用、启停切换及配额状态复制逻辑，通过回调机制协调监控上下文与回收策略，支持运行时参数更新和资源回收操作。",
          "similarity": 0.5502728819847107
        },
        {
          "chunk_id": 0,
          "file_path": "mm/damon/reclaim.c",
          "start_line": 1,
          "end_line": 152,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * DAMON-based page reclamation",
            " *",
            " * Author: SeongJae Park <sj@kernel.org>",
            " */",
            "",
            "#define pr_fmt(fmt) \"damon-reclaim: \" fmt",
            "",
            "#include <linux/damon.h>",
            "#include <linux/kstrtox.h>",
            "#include <linux/module.h>",
            "",
            "#include \"modules-common.h\"",
            "",
            "#ifdef MODULE_PARAM_PREFIX",
            "#undef MODULE_PARAM_PREFIX",
            "#endif",
            "#define MODULE_PARAM_PREFIX \"damon_reclaim.\"",
            "",
            "/*",
            " * Enable or disable DAMON_RECLAIM.",
            " *",
            " * You can enable DAMON_RCLAIM by setting the value of this parameter as ``Y``.",
            " * Setting it as ``N`` disables DAMON_RECLAIM.  Note that DAMON_RECLAIM could",
            " * do no real monitoring and reclamation due to the watermarks-based activation",
            " * condition.  Refer to below descriptions for the watermarks parameter for",
            " * this.",
            " */",
            "static bool enabled __read_mostly;",
            "",
            "/*",
            " * Make DAMON_RECLAIM reads the input parameters again, except ``enabled``.",
            " *",
            " * Input parameters that updated while DAMON_RECLAIM is running are not applied",
            " * by default.  Once this parameter is set as ``Y``, DAMON_RECLAIM reads values",
            " * of parametrs except ``enabled`` again.  Once the re-reading is done, this",
            " * parameter is set as ``N``.  If invalid parameters are found while the",
            " * re-reading, DAMON_RECLAIM will be disabled.",
            " */",
            "static bool commit_inputs __read_mostly;",
            "module_param(commit_inputs, bool, 0600);",
            "",
            "/*",
            " * Time threshold for cold memory regions identification in microseconds.",
            " *",
            " * If a memory region is not accessed for this or longer time, DAMON_RECLAIM",
            " * identifies the region as cold, and reclaims.  120 seconds by default.",
            " */",
            "static unsigned long min_age __read_mostly = 120000000;",
            "module_param(min_age, ulong, 0600);",
            "",
            "static struct damos_quota damon_reclaim_quota = {",
            "\t/* use up to 10 ms time, reclaim up to 128 MiB per 1 sec by default */",
            "\t.ms = 10,",
            "\t.sz = 128 * 1024 * 1024,",
            "\t.reset_interval = 1000,",
            "\t/* Within the quota, page out older regions first. */",
            "\t.weight_sz = 0,",
            "\t.weight_nr_accesses = 0,",
            "\t.weight_age = 1",
            "};",
            "DEFINE_DAMON_MODULES_DAMOS_QUOTAS(damon_reclaim_quota);",
            "",
            "static struct damos_watermarks damon_reclaim_wmarks = {",
            "\t.metric = DAMOS_WMARK_FREE_MEM_RATE,",
            "\t.interval = 5000000,\t/* 5 seconds */",
            "\t.high = 500,\t\t/* 50 percent */",
            "\t.mid = 400,\t\t/* 40 percent */",
            "\t.low = 200,\t\t/* 20 percent */",
            "};",
            "DEFINE_DAMON_MODULES_WMARKS_PARAMS(damon_reclaim_wmarks);",
            "",
            "static struct damon_attrs damon_reclaim_mon_attrs = {",
            "\t.sample_interval = 5000,\t/* 5 ms */",
            "\t.aggr_interval = 100000,\t/* 100 ms */",
            "\t.ops_update_interval = 0,",
            "\t.min_nr_regions = 10,",
            "\t.max_nr_regions = 1000,",
            "};",
            "DEFINE_DAMON_MODULES_MON_ATTRS_PARAMS(damon_reclaim_mon_attrs);",
            "",
            "/*",
            " * Start of the target memory region in physical address.",
            " *",
            " * The start physical address of memory region that DAMON_RECLAIM will do work",
            " * against.  By default, biggest System RAM is used as the region.",
            " */",
            "static unsigned long monitor_region_start __read_mostly;",
            "module_param(monitor_region_start, ulong, 0600);",
            "",
            "/*",
            " * End of the target memory region in physical address.",
            " *",
            " * The end physical address of memory region that DAMON_RECLAIM will do work",
            " * against.  By default, biggest System RAM is used as the region.",
            " */",
            "static unsigned long monitor_region_end __read_mostly;",
            "module_param(monitor_region_end, ulong, 0600);",
            "",
            "/*",
            " * Skip anonymous pages reclamation.",
            " *",
            " * If this parameter is set as ``Y``, DAMON_RECLAIM does not reclaim anonymous",
            " * pages.  By default, ``N``.",
            " */",
            "static bool skip_anon __read_mostly;",
            "module_param(skip_anon, bool, 0600);",
            "",
            "/*",
            " * PID of the DAMON thread",
            " *",
            " * If DAMON_RECLAIM is enabled, this becomes the PID of the worker thread.",
            " * Else, -1.",
            " */",
            "static int kdamond_pid __read_mostly = -1;",
            "module_param(kdamond_pid, int, 0400);",
            "",
            "static struct damos_stat damon_reclaim_stat;",
            "DEFINE_DAMON_MODULES_DAMOS_STATS_PARAMS(damon_reclaim_stat,",
            "\t\treclaim_tried_regions, reclaimed_regions, quota_exceeds);",
            "",
            "static struct damon_ctx *ctx;",
            "static struct damon_target *target;",
            "",
            "static struct damos *damon_reclaim_new_scheme(void)",
            "{",
            "\tstruct damos_access_pattern pattern = {",
            "\t\t/* Find regions having PAGE_SIZE or larger size */",
            "\t\t.min_sz_region = PAGE_SIZE,",
            "\t\t.max_sz_region = ULONG_MAX,",
            "\t\t/* and not accessed at all */",
            "\t\t.min_nr_accesses = 0,",
            "\t\t.max_nr_accesses = 0,",
            "\t\t/* for min_age or more micro-seconds */",
            "\t\t.min_age_region = min_age /",
            "\t\t\tdamon_reclaim_mon_attrs.aggr_interval,",
            "\t\t.max_age_region = UINT_MAX,",
            "\t};",
            "",
            "\treturn damon_new_scheme(",
            "\t\t\t&pattern,",
            "\t\t\t/* page out those, as soon as found */",
            "\t\t\tDAMOS_PAGEOUT,",
            "\t\t\t/* for each aggregation interval */",
            "\t\t\t0,",
            "\t\t\t/* under the quota. */",
            "\t\t\t&damon_reclaim_quota,",
            "\t\t\t/* (De)activate this according to the watermarks. */",
            "\t\t\t&damon_reclaim_wmarks);",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义DAMON_RECLAIM模块的全局参数和配置，包括启用状态、冷内存识别时间阈值、配额限制、水印条件及监控属性，用于控制基于DAMON的页面回收行为。",
          "similarity": 0.5437183380126953
        },
        {
          "chunk_id": 2,
          "file_path": "mm/damon/reclaim.c",
          "start_line": 269,
          "end_line": 298,
          "content": [
            "static int damon_reclaim_after_aggregation(struct damon_ctx *c)",
            "{",
            "\tstruct damos *s;",
            "",
            "\t/* update the stats parameter */",
            "\tdamon_for_each_scheme(s, c)",
            "\t\tdamon_reclaim_stat = s->stat;",
            "",
            "\treturn damon_reclaim_handle_commit_inputs();",
            "}",
            "static int damon_reclaim_after_wmarks_check(struct damon_ctx *c)",
            "{",
            "\treturn damon_reclaim_handle_commit_inputs();",
            "}",
            "static int __init damon_reclaim_init(void)",
            "{",
            "\tint err = damon_modules_new_paddr_ctx_target(&ctx, &target);",
            "",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tctx->callback.after_wmarks_check = damon_reclaim_after_wmarks_check;",
            "\tctx->callback.after_aggregation = damon_reclaim_after_aggregation;",
            "",
            "\t/* 'enabled' has set before this function, probably via command line */",
            "\tif (enabled)",
            "\t\terr = damon_reclaim_turn(true);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "damon_reclaim_after_aggregation, damon_reclaim_after_wmarks_check, damon_reclaim_init",
          "description": "注册DAMON框架的回调函数以实现回收策略的动态调整，初始化阶段绑定自定义回调至上下文，确保在监控周期关键节点触发参数重载和回收策略更新。",
          "similarity": 0.4899234175682068
        }
      ]
    },
    {
      "source_file": "mm/hugetlb_vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:07:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `hugetlb_vmemmap.c`\n\n---\n\n# hugetlb_vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`hugetlb_vmemmap.c` 实现了 **HugeTLB Vmemmap Optimization (HVO)** 功能，旨在优化与 HugeTLB 页面关联的 `vmemmap`（虚拟内存映射）结构所占用的物理内存。在 Linux 内核中，每个物理页都对应一个 `struct page` 结构，这些结构通过 `vmemmap` 虚拟地址空间进行线性映射。当使用大页（如 2MB 或 1GB HugeTLB 页面）时，为整个大页区域分配完整的 `struct page` 数组会造成大量内存浪费（因为大部分尾部页面不会被单独使用）。  \n\n本文件通过 **重映射（remap）** 技术，将大页对应的多个 `vmemmap` 页面中的尾部页面重新映射到同一个物理页（通常是头部页面），从而显著减少 `vmemmap` 所需的物理内存开销，同时保持内核对 `struct page` 的访问语义正确。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct vmemmap_remap_walk`**  \n  用于遍历和操作 `vmemmap` 页表的上下文结构：\n  - `remap_pte`: 回调函数，处理每个 PTE 条目\n  - `nr_walked`: 已遍历的 PTE 数量\n  - `reuse_page`: 用于重用的物理页（通常是头部页）\n  - `reuse_addr`: `reuse_page` 对应的虚拟地址\n  - `vmemmap_pages`: 可释放的 `vmemmap` 页面链表\n  - `flags`: 控制 TLB 刷新行为的标志位（`VMEMMAP_SPLIT_NO_TLB_FLUSH`, `VMEMMAP_REMAP_NO_TLB_FLUSH`）\n\n### 主要函数\n\n- **`vmemmap_split_pmd()`**  \n  将一个 PMD（Page Middle Directory）级别的大页映射拆分为 PTE 级别的细粒度映射，为后续重映射做准备。\n\n- **`vmemmap_pmd_entry()`**  \n  `mm_walk` 回调函数，在遍历到 PMD 条目时触发，负责检查是否需要拆分 PMD 并执行拆分操作。\n\n- **`vmemmap_pte_entry()`**  \n  `mm_walk` 回调函数，在遍历到 PTE 条目时触发，用于识别重用页并执行重映射逻辑。\n\n- **`vmemmap_remap_range()`**  \n  驱动整个重映射流程，使用 `walk_page_range_novma()` 遍历指定的 `vmemmap` 虚拟地址范围。\n\n- **`vmemmap_remap_pte()`**  \n  实际执行 PTE 重映射的核心函数：将尾部 `vmemmap` 页面的 PTE 指向 `reuse_page`，并设置为只读以防止非法写入。\n\n- **`vmemmap_restore_pte()`**  \n  用于恢复原始映射（例如在取消优化时），从可释放列表中取出原页面并恢复其内容。\n\n- **`free_vmemmap_page()` / `free_vmemmap_page_list()`**  \n  安全释放 `vmemmap` 页面，区分来自 `memblock`（启动内存）或 `buddy` 分配器的页面。\n\n- **`reset_struct_pages()`**  \n  重置 `struct page` 结构的关键字段，避免因重映射导致的元数据不一致问题（如“corrupted mapping in tail page”警告）。\n\n## 3. 关键实现\n\n### 重映射机制\n1. **PMD 拆分**：首先将覆盖目标 `vmemmap` 范围的 PMD 大页映射拆分为 PTE 映射，确保可以独立修改每个 `struct page` 对应的物理页。\n2. **重用页识别**：在遍历 PTE 时，第一个遇到的页面被选为 `reuse_page`（即头部页）。\n3. **尾页重映射**：后续所有 PTE 条目均被修改为指向 `reuse_page`，并设置为只读（`PAGE_KERNEL_RO`），防止对尾部 `struct page` 的意外写入。\n4. **元数据清理**：由于尾部 `struct page` 与头部共享物理内存，其元数据（如 `flags`、`mapping`）可能无效。通过 `reset_struct_pages()` 复制有效数据到尾部结构，避免内核校验失败。\n5. **安全释放**：被替换的原始尾部页面被加入 `vmemmap_pages` 链表，可在后续安全释放。\n\n### 自托管检测\n在内存热插拔场景下（`memmap_on_memory`），`vmemmap` 结构可能位于待优化的内存区域内（即“自托管”）。代码通过检查首个 `vmemmap` 页面的 `PageVmemmapSelfHosted()` 标志，若为真则拒绝优化（返回 `-ENOTSUPP`），防止破坏关键元数据。\n\n### 内存屏障与 TLB 刷新\n- 使用 `smp_wmb()` 确保页面内容更新在 PTE 修改前完成。\n- 在 PMD 拆分和 PTE 重映射后，默认执行 `flush_tlb_kernel_range()` 刷新 TLB，可通过标志位跳过以提升性能。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `pgtable.h`、`pagewalk.h`、`mmdebug.h` 等核心 MM 头文件。\n- **架构相关代码**：使用 `asm/pgalloc.h` 和 `asm/tlbflush.h` 提供的页表分配与 TLB 刷新接口。\n- **HugeTLB 子系统**：与 `hugetlb.h` 协同工作，优化 HugeTLB 页面的 `vmemmap` 开销。\n- **内存热插拔**：处理 `memmap_on_memory` 场景下的自托管 `vmemmap` 限制。\n- **启动内存管理**：通过 `bootmem_info.h` 区分 `memblock` 与 `buddy` 分配的页面。\n\n## 5. 使用场景\n\n- **HugeTLB 内存优化**：在系统配置大量 HugeTLB 页面时，显著减少 `vmemmap` 的物理内存占用（例如，2MB HugeTLB 页面可节省约 87.5% 的 `vmemmap` 内存）。\n- **内存受限环境**：在内存资源紧张的系统（如容器、嵌入式设备）中降低内核内存开销。\n- **内存热插拔**：在支持 `memmap_on_memory` 的热插拔场景中，安全地优化新插入内存区域的 `vmemmap`。\n- **内核调试与维护**：通过只读保护捕获对尾部 `struct page` 的非法写入，提升系统稳定性。",
      "similarity": 0.52037513256073,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 159,
          "end_line": 264,
          "content": [
            "static int vmemmap_remap_range(unsigned long start, unsigned long end,",
            "\t\t\t       struct vmemmap_remap_walk *walk)",
            "{",
            "\tint ret;",
            "",
            "\tVM_BUG_ON(!PAGE_ALIGNED(start | end));",
            "",
            "\tret = walk_page_range_novma(&init_mm, start, end, &vmemmap_remap_ops,",
            "\t\t\t\t    NULL, walk);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tif (walk->remap_pte && !(walk->flags & VMEMMAP_REMAP_NO_TLB_FLUSH))",
            "\t\tflush_tlb_kernel_range(start, end);",
            "",
            "\treturn 0;",
            "}",
            "static inline void free_vmemmap_page(struct page *page)",
            "{",
            "\tif (PageReserved(page))",
            "\t\tfree_bootmem_page(page);",
            "\telse",
            "\t\t__free_page(page);",
            "}",
            "static void free_vmemmap_page_list(struct list_head *list)",
            "{",
            "\tstruct page *page, *next;",
            "",
            "\tlist_for_each_entry_safe(page, next, list, lru)",
            "\t\tfree_vmemmap_page(page);",
            "}",
            "static void vmemmap_remap_pte(pte_t *pte, unsigned long addr,",
            "\t\t\t      struct vmemmap_remap_walk *walk)",
            "{",
            "\t/*",
            "\t * Remap the tail pages as read-only to catch illegal write operation",
            "\t * to the tail pages.",
            "\t */",
            "\tpgprot_t pgprot = PAGE_KERNEL_RO;",
            "\tstruct page *page = pte_page(ptep_get(pte));",
            "\tpte_t entry;",
            "",
            "\t/* Remapping the head page requires r/w */",
            "\tif (unlikely(addr == walk->reuse_addr)) {",
            "\t\tpgprot = PAGE_KERNEL;",
            "\t\tlist_del(&walk->reuse_page->lru);",
            "",
            "\t\t/*",
            "\t\t * Makes sure that preceding stores to the page contents from",
            "\t\t * vmemmap_remap_free() become visible before the set_pte_at()",
            "\t\t * write.",
            "\t\t */",
            "\t\tsmp_wmb();",
            "\t}",
            "",
            "\tentry = mk_pte(walk->reuse_page, pgprot);",
            "\tlist_add(&page->lru, walk->vmemmap_pages);",
            "\tset_pte_at(&init_mm, addr, pte, entry);",
            "}",
            "static inline void reset_struct_pages(struct page *start)",
            "{",
            "\tstruct page *from = start + NR_RESET_STRUCT_PAGE;",
            "",
            "\tBUILD_BUG_ON(NR_RESET_STRUCT_PAGE * 2 > PAGE_SIZE / sizeof(struct page));",
            "\tmemcpy(start, from, sizeof(*from) * NR_RESET_STRUCT_PAGE);",
            "}",
            "static void vmemmap_restore_pte(pte_t *pte, unsigned long addr,",
            "\t\t\t\tstruct vmemmap_remap_walk *walk)",
            "{",
            "\tpgprot_t pgprot = PAGE_KERNEL;",
            "\tstruct page *page;",
            "\tvoid *to;",
            "",
            "\tBUG_ON(pte_page(ptep_get(pte)) != walk->reuse_page);",
            "",
            "\tpage = list_first_entry(walk->vmemmap_pages, struct page, lru);",
            "\tlist_del(&page->lru);",
            "\tto = page_to_virt(page);",
            "\tcopy_page(to, (void *)walk->reuse_addr);",
            "\treset_struct_pages(to);",
            "",
            "\t/*",
            "\t * Makes sure that preceding stores to the page contents become visible",
            "\t * before the set_pte_at() write.",
            "\t */",
            "\tsmp_wmb();",
            "\tset_pte_at(&init_mm, addr, pte, mk_pte(page, pgprot));",
            "}",
            "static int vmemmap_remap_split(unsigned long start, unsigned long end,",
            "\t\t\t       unsigned long reuse)",
            "{",
            "\tint ret;",
            "\tstruct vmemmap_remap_walk walk = {",
            "\t\t.remap_pte\t= NULL,",
            "\t\t.flags\t\t= VMEMMAP_SPLIT_NO_TLB_FLUSH,",
            "\t};",
            "",
            "\t/* See the comment in the vmemmap_remap_free(). */",
            "\tBUG_ON(start - reuse != PAGE_SIZE);",
            "",
            "\tmmap_read_lock(&init_mm);",
            "\tret = vmemmap_remap_range(reuse, end, &walk);",
            "\tmmap_read_unlock(&init_mm);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "vmemmap_remap_range, free_vmemmap_page, free_vmemmap_page_list, vmemmap_remap_pte, reset_struct_pages, vmemmap_restore_pte, vmemmap_remap_split",
          "description": "提供vmemmap_remap_range范围重映射接口、free_vmemmap_page页面释放函数、reset_struct_pages结构页重置方法及remap_pte/restore_pte的映射更新逻辑，支持安全写保护和数据恢复。",
          "similarity": 0.609642505645752
        },
        {
          "chunk_id": 3,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 316,
          "end_line": 427,
          "content": [
            "static int vmemmap_remap_free(unsigned long start, unsigned long end,",
            "\t\t\t      unsigned long reuse,",
            "\t\t\t      struct list_head *vmemmap_pages,",
            "\t\t\t      unsigned long flags)",
            "{",
            "\tint ret;",
            "\tstruct vmemmap_remap_walk walk = {",
            "\t\t.remap_pte\t= vmemmap_remap_pte,",
            "\t\t.reuse_addr\t= reuse,",
            "\t\t.vmemmap_pages\t= vmemmap_pages,",
            "\t\t.flags\t\t= flags,",
            "\t};",
            "\tint nid = page_to_nid((struct page *)reuse);",
            "\tgfp_t gfp_mask = GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN;",
            "",
            "\t/*",
            "\t * Allocate a new head vmemmap page to avoid breaking a contiguous",
            "\t * block of struct page memory when freeing it back to page allocator",
            "\t * in free_vmemmap_page_list(). This will allow the likely contiguous",
            "\t * struct page backing memory to be kept contiguous and allowing for",
            "\t * more allocations of hugepages. Fallback to the currently",
            "\t * mapped head page in case should it fail to allocate.",
            "\t */",
            "\twalk.reuse_page = alloc_pages_node(nid, gfp_mask, 0);",
            "\tif (walk.reuse_page) {",
            "\t\tcopy_page(page_to_virt(walk.reuse_page),",
            "\t\t\t  (void *)walk.reuse_addr);",
            "\t\tlist_add(&walk.reuse_page->lru, vmemmap_pages);",
            "\t}",
            "",
            "\t/*",
            "\t * In order to make remapping routine most efficient for the huge pages,",
            "\t * the routine of vmemmap page table walking has the following rules",
            "\t * (see more details from the vmemmap_pte_range()):",
            "\t *",
            "\t * - The range [@start, @end) and the range [@reuse, @reuse + PAGE_SIZE)",
            "\t *   should be continuous.",
            "\t * - The @reuse address is part of the range [@reuse, @end) that we are",
            "\t *   walking which is passed to vmemmap_remap_range().",
            "\t * - The @reuse address is the first in the complete range.",
            "\t *",
            "\t * So we need to make sure that @start and @reuse meet the above rules.",
            "\t */",
            "\tBUG_ON(start - reuse != PAGE_SIZE);",
            "",
            "\tmmap_read_lock(&init_mm);",
            "\tret = vmemmap_remap_range(reuse, end, &walk);",
            "\tif (ret && walk.nr_walked) {",
            "\t\tend = reuse + walk.nr_walked * PAGE_SIZE;",
            "\t\t/*",
            "\t\t * vmemmap_pages contains pages from the previous",
            "\t\t * vmemmap_remap_range call which failed.  These",
            "\t\t * are pages which were removed from the vmemmap.",
            "\t\t * They will be restored in the following call.",
            "\t\t */",
            "\t\twalk = (struct vmemmap_remap_walk) {",
            "\t\t\t.remap_pte\t= vmemmap_restore_pte,",
            "\t\t\t.reuse_addr\t= reuse,",
            "\t\t\t.vmemmap_pages\t= vmemmap_pages,",
            "\t\t\t.flags\t\t= 0,",
            "\t\t};",
            "",
            "\t\tvmemmap_remap_range(reuse, end, &walk);",
            "\t}",
            "\tmmap_read_unlock(&init_mm);",
            "",
            "\treturn ret;",
            "}",
            "static int alloc_vmemmap_page_list(unsigned long start, unsigned long end,",
            "\t\t\t\t   struct list_head *list)",
            "{",
            "\tgfp_t gfp_mask = GFP_KERNEL | __GFP_RETRY_MAYFAIL;",
            "\tunsigned long nr_pages = (end - start) >> PAGE_SHIFT;",
            "\tint nid = page_to_nid((struct page *)start);",
            "\tstruct page *page, *next;",
            "",
            "\twhile (nr_pages--) {",
            "\t\tpage = alloc_pages_node(nid, gfp_mask, 0);",
            "\t\tif (!page)",
            "\t\t\tgoto out;",
            "\t\tlist_add(&page->lru, list);",
            "\t}",
            "",
            "\treturn 0;",
            "out:",
            "\tlist_for_each_entry_safe(page, next, list, lru)",
            "\t\t__free_page(page);",
            "\treturn -ENOMEM;",
            "}",
            "static int vmemmap_remap_alloc(unsigned long start, unsigned long end,",
            "\t\t\t       unsigned long reuse, unsigned long flags)",
            "{",
            "\tLIST_HEAD(vmemmap_pages);",
            "\tstruct vmemmap_remap_walk walk = {",
            "\t\t.remap_pte\t= vmemmap_restore_pte,",
            "\t\t.reuse_addr\t= reuse,",
            "\t\t.vmemmap_pages\t= &vmemmap_pages,",
            "\t\t.flags\t\t= flags,",
            "\t};",
            "",
            "\t/* See the comment in the vmemmap_remap_free(). */",
            "\tBUG_ON(start - reuse != PAGE_SIZE);",
            "",
            "\tif (alloc_vmemmap_page_list(start, end, &vmemmap_pages))",
            "\t\treturn -ENOMEM;",
            "",
            "\tmmap_read_lock(&init_mm);",
            "\tvmemmap_remap_range(reuse, end, &walk);",
            "\tmmap_read_unlock(&init_mm);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmemmap_remap_free, alloc_vmemmap_page_list, vmemmap_remap_alloc",
          "description": "包含vmemmap_remap_free释放大页时的映射回收逻辑、alloc_vmemmap_page_list页面列表分配函数及vmemmap_remap_alloc预分配页面的接口，实现高效的资源管理。",
          "similarity": 0.5789607763290405
        },
        {
          "chunk_id": 4,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 450,
          "end_line": 579,
          "content": [
            "static int __hugetlb_vmemmap_restore_folio(const struct hstate *h,",
            "\t\t\t\t\t   struct folio *folio, unsigned long flags)",
            "{",
            "\tint ret;",
            "\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;",
            "\tunsigned long vmemmap_reuse;",
            "",
            "\tVM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(folio), folio);",
            "\tVM_WARN_ON_ONCE_FOLIO(folio_ref_count(folio), folio);",
            "",
            "\tif (!folio_test_hugetlb_vmemmap_optimized(folio))",
            "\t\treturn 0;",
            "",
            "\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);",
            "\tvmemmap_reuse\t= vmemmap_start;",
            "\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;",
            "",
            "\t/*",
            "\t * The pages which the vmemmap virtual address range [@vmemmap_start,",
            "\t * @vmemmap_end) are mapped to are freed to the buddy allocator, and",
            "\t * the range is mapped to the page which @vmemmap_reuse is mapped to.",
            "\t * When a HugeTLB page is freed to the buddy allocator, previously",
            "\t * discarded vmemmap pages must be allocated and remapping.",
            "\t */",
            "\tret = vmemmap_remap_alloc(vmemmap_start, vmemmap_end, vmemmap_reuse, flags);",
            "\tif (!ret) {",
            "\t\tfolio_clear_hugetlb_vmemmap_optimized(folio);",
            "\t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "int hugetlb_vmemmap_restore_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\t/* avoid writes from page_ref_add_unless() while unfolding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\treturn __hugetlb_vmemmap_restore_folio(h, folio, 0);",
            "}",
            "long hugetlb_vmemmap_restore_folios(const struct hstate *h,",
            "\t\t\t\t\tstruct list_head *folio_list,",
            "\t\t\t\t\tstruct list_head *non_hvo_folios)",
            "{",
            "\tstruct folio *folio, *t_folio;",
            "\tlong restored = 0;",
            "\tlong ret = 0;",
            "",
            "\t/* avoid writes from page_ref_add_unless() while unfolding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\tlist_for_each_entry_safe(folio, t_folio, folio_list, lru) {",
            "\t\tif (folio_test_hugetlb_vmemmap_optimized(folio)) {",
            "\t\t\tret = __hugetlb_vmemmap_restore_folio(h, folio,",
            "\t\t\t\t\t\t\t      VMEMMAP_REMAP_NO_TLB_FLUSH);",
            "\t\t\tif (ret)",
            "\t\t\t\tbreak;",
            "\t\t\trestored++;",
            "\t\t}",
            "",
            "\t\t/* Add non-optimized folios to output list */",
            "\t\tlist_move(&folio->lru, non_hvo_folios);",
            "\t}",
            "",
            "\tif (restored)",
            "\t\tflush_tlb_all();",
            "\tif (!ret)",
            "\t\tret = restored;",
            "\treturn ret;",
            "}",
            "static bool vmemmap_should_optimize_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\tif (folio_test_hugetlb_vmemmap_optimized(folio))",
            "\t\treturn false;",
            "",
            "\tif (!READ_ONCE(vmemmap_optimize_enabled))",
            "\t\treturn false;",
            "",
            "\tif (!hugetlb_vmemmap_optimizable(h))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,",
            "\t\t\t\t\t    struct folio *folio,",
            "\t\t\t\t\t    struct list_head *vmemmap_pages,",
            "\t\t\t\t\t    unsigned long flags)",
            "{",
            "\tint ret = 0;",
            "\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;",
            "\tunsigned long vmemmap_reuse;",
            "",
            "\tVM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(folio), folio);",
            "\tVM_WARN_ON_ONCE_FOLIO(folio_ref_count(folio), folio);",
            "",
            "\tif (!vmemmap_should_optimize_folio(h, folio))",
            "\t\treturn ret;",
            "",
            "\tstatic_branch_inc(&hugetlb_optimize_vmemmap_key);",
            "\t/*",
            "\t * Very Subtle",
            "\t * If VMEMMAP_REMAP_NO_TLB_FLUSH is set, TLB flushing is not performed",
            "\t * immediately after remapping.  As a result, subsequent accesses",
            "\t * and modifications to struct pages associated with the hugetlb",
            "\t * page could be to the OLD struct pages.  Set the vmemmap optimized",
            "\t * flag here so that it is copied to the new head page.  This keeps",
            "\t * the old and new struct pages in sync.",
            "\t * If there is an error during optimization, we will immediately FLUSH",
            "\t * the TLB and clear the flag below.",
            "\t */",
            "\tfolio_set_hugetlb_vmemmap_optimized(folio);",
            "",
            "\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);",
            "\tvmemmap_reuse\t= vmemmap_start;",
            "\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;",
            "",
            "\t/*",
            "\t * Remap the vmemmap virtual address range [@vmemmap_start, @vmemmap_end)",
            "\t * to the page which @vmemmap_reuse is mapped to.  Add pages previously",
            "\t * mapping the range to vmemmap_pages list so that they can be freed by",
            "\t * the caller.",
            "\t */",
            "\tret = vmemmap_remap_free(vmemmap_start, vmemmap_end, vmemmap_reuse,",
            "\t\t\t\t vmemmap_pages, flags);",
            "\tif (ret) {",
            "\t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);",
            "\t\tfolio_clear_hugetlb_vmemmap_optimized(folio);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__hugetlb_vmemmap_restore_folio, hugetlb_vmemmap_restore_folio, hugetlb_vmemmap_restore_folios, vmemmap_should_optimize_folio, __hugetlb_vmemmap_optimize_folio",
          "description": "实现hugeTLB页的vmemmap优化控制逻辑，包含__hugetlb_vmemmap_restore_folio页回收恢复、hugetlb_vmemmap_restore_folios批量处理、vmemmap_should_optimize_folio优化判定及__hugetlb_vmemmap_optimize_folio优化执行路径。",
          "similarity": 0.5305604338645935
        },
        {
          "chunk_id": 0,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 1,
          "end_line": 48,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * HugeTLB Vmemmap Optimization (HVO)",
            " *",
            " * Copyright (c) 2020, ByteDance. All rights reserved.",
            " *",
            " *     Author: Muchun Song <songmuchun@bytedance.com>",
            " *",
            " * See Documentation/mm/vmemmap_dedup.rst",
            " */",
            "#define pr_fmt(fmt)\t\"HugeTLB: \" fmt",
            "",
            "#include <linux/pgtable.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/bootmem_info.h>",
            "#include <linux/mmdebug.h>",
            "#include <linux/pagewalk.h>",
            "#include <asm/pgalloc.h>",
            "#include <asm/tlbflush.h>",
            "#include \"hugetlb_vmemmap.h\"",
            "",
            "/**",
            " * struct vmemmap_remap_walk - walk vmemmap page table",
            " *",
            " * @remap_pte:\t\tcalled for each lowest-level entry (PTE).",
            " * @nr_walked:\t\tthe number of walked pte.",
            " * @reuse_page:\t\tthe page which is reused for the tail vmemmap pages.",
            " * @reuse_addr:\t\tthe virtual address of the @reuse_page page.",
            " * @vmemmap_pages:\tthe list head of the vmemmap pages that can be freed",
            " *\t\t\tor is mapped from.",
            " * @flags:\t\tused to modify behavior in vmemmap page table walking",
            " *\t\t\toperations.",
            " */",
            "struct vmemmap_remap_walk {",
            "\tvoid\t\t\t(*remap_pte)(pte_t *pte, unsigned long addr,",
            "\t\t\t\t\t     struct vmemmap_remap_walk *walk);",
            "\tunsigned long\t\tnr_walked;",
            "\tstruct page\t\t*reuse_page;",
            "\tunsigned long\t\treuse_addr;",
            "\tstruct list_head\t*vmemmap_pages;",
            "",
            "/* Skip the TLB flush when we split the PMD */",
            "#define VMEMMAP_SPLIT_NO_TLB_FLUSH\tBIT(0)",
            "/* Skip the TLB flush when we remap the PTE */",
            "#define VMEMMAP_REMAP_NO_TLB_FLUSH\tBIT(1)",
            "\tunsigned long\t\tflags;",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义了用于HugeTLB vmemmap优化的结构体vmemmap_remap_walk，包含回调函数指针、统计计数器、复用页面信息及控制标志位，用于跟踪页表遍历过程中的重映射操作。",
          "similarity": 0.5275269150733948
        },
        {
          "chunk_id": 1,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 49,
          "end_line": 150,
          "content": [
            "static int vmemmap_split_pmd(pmd_t *pmd, struct page *head, unsigned long start,",
            "\t\t\t     struct vmemmap_remap_walk *walk)",
            "{",
            "\tpmd_t __pmd;",
            "\tint i;",
            "\tunsigned long addr = start;",
            "\tpte_t *pgtable;",
            "",
            "\tpgtable = pte_alloc_one_kernel(&init_mm);",
            "\tif (!pgtable)",
            "\t\treturn -ENOMEM;",
            "",
            "\tpmd_populate_kernel(&init_mm, &__pmd, pgtable);",
            "",
            "\tfor (i = 0; i < PTRS_PER_PTE; i++, addr += PAGE_SIZE) {",
            "\t\tpte_t entry, *pte;",
            "\t\tpgprot_t pgprot = PAGE_KERNEL;",
            "",
            "\t\tentry = mk_pte(head + i, pgprot);",
            "\t\tpte = pte_offset_kernel(&__pmd, addr);",
            "\t\tset_pte_at(&init_mm, addr, pte, entry);",
            "\t}",
            "",
            "\tspin_lock(&init_mm.page_table_lock);",
            "\tif (likely(pmd_leaf(*pmd))) {",
            "\t\t/*",
            "\t\t * Higher order allocations from buddy allocator must be able to",
            "\t\t * be treated as indepdenent small pages (as they can be freed",
            "\t\t * individually).",
            "\t\t */",
            "\t\tif (!PageReserved(head))",
            "\t\t\tsplit_page(head, get_order(PMD_SIZE));",
            "",
            "\t\t/* Make pte visible before pmd. See comment in pmd_install(). */",
            "\t\tsmp_wmb();",
            "\t\tpmd_populate_kernel(&init_mm, pmd, pgtable);",
            "\t\tif (!(walk->flags & VMEMMAP_SPLIT_NO_TLB_FLUSH))",
            "\t\t\tflush_tlb_kernel_range(start, start + PMD_SIZE);",
            "\t} else {",
            "\t\tpte_free_kernel(&init_mm, pgtable);",
            "\t}",
            "\tspin_unlock(&init_mm.page_table_lock);",
            "",
            "\treturn 0;",
            "}",
            "static int vmemmap_pmd_entry(pmd_t *pmd, unsigned long addr,",
            "\t\t\t     unsigned long next, struct mm_walk *walk)",
            "{",
            "\tint ret = 0;",
            "\tstruct page *head;",
            "\tstruct vmemmap_remap_walk *vmemmap_walk = walk->private;",
            "",
            "\t/* Only splitting, not remapping the vmemmap pages. */",
            "\tif (!vmemmap_walk->remap_pte)",
            "\t\twalk->action = ACTION_CONTINUE;",
            "",
            "\tspin_lock(&init_mm.page_table_lock);",
            "\thead = pmd_leaf(*pmd) ? pmd_page(*pmd) : NULL;",
            "\t/*",
            "\t * Due to HugeTLB alignment requirements and the vmemmap",
            "\t * pages being at the start of the hotplugged memory",
            "\t * region in memory_hotplug.memmap_on_memory case. Checking",
            "\t * the vmemmap page associated with the first vmemmap page",
            "\t * if it is self-hosted is sufficient.",
            "\t *",
            "\t * [                  hotplugged memory                  ]",
            "\t * [        section        ][...][        section        ]",
            "\t * [ vmemmap ][              usable memory               ]",
            "\t *   ^  | ^                        |",
            "\t *   +--+ |                        |",
            "\t *        +------------------------+",
            "\t */",
            "\tif (unlikely(!vmemmap_walk->nr_walked)) {",
            "\t\tstruct page *page = head ? head + pte_index(addr) :",
            "\t\t\t\t    pte_page(ptep_get(pte_offset_kernel(pmd, addr)));",
            "",
            "\t\tif (PageVmemmapSelfHosted(page))",
            "\t\t\tret = -ENOTSUPP;",
            "\t}",
            "\tspin_unlock(&init_mm.page_table_lock);",
            "\tif (!head || ret)",
            "\t\treturn ret;",
            "",
            "\treturn vmemmap_split_pmd(pmd, head, addr & PMD_MASK, vmemmap_walk);",
            "}",
            "static int vmemmap_pte_entry(pte_t *pte, unsigned long addr,",
            "\t\t\t     unsigned long next, struct mm_walk *walk)",
            "{",
            "\tstruct vmemmap_remap_walk *vmemmap_walk = walk->private;",
            "",
            "\t/*",
            "\t * The reuse_page is found 'first' in page table walking before",
            "\t * starting remapping.",
            "\t */",
            "\tif (!vmemmap_walk->reuse_page)",
            "\t\tvmemmap_walk->reuse_page = pte_page(ptep_get(pte));",
            "\telse",
            "\t\tvmemmap_walk->remap_pte(pte, addr, vmemmap_walk);",
            "\tvmemmap_walk->nr_walked++;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmemmap_split_pmd, vmemmap_pmd_entry, vmemmap_pte_entry",
          "description": "实现了vmeammap_split_pmd分割PMD页表项、vmemmap_pmd_entry检查页表项有效性、vmemmap_pte_entry处理页目录项的逻辑，支持大页拆分与页表项初始化。",
          "similarity": 0.4920312762260437
        }
      ]
    },
    {
      "source_file": "mm/mremap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:55:57\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mremap.c`\n\n---\n\n# mremap.c 技术文档\n\n## 1. 文件概述\n\n`mremap.c` 是 Linux 内核内存管理子系统中的核心文件，主要实现 `mremap()` 系统调用的底层逻辑。该文件负责在用户地址空间中重新映射（移动或调整大小）已有的虚拟内存区域（VMA），包括页表项（PTE/PMD/PUD）的迁移、反向映射（rmap）锁的协调、软脏位（soft dirty）和用户态缺页处理（userfaultfd）等特性的维护。其目标是在保证内存一致性与并发安全的前提下，高效地完成虚拟内存布局的重排。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`get_old_pud()` / `get_old_pmd()`**  \n  获取指定虚拟地址对应的旧页表上层项（PUD/PMD），用于读取源地址空间的页表结构。\n\n- **`alloc_new_pud()` / `alloc_new_pmd()`**  \n  为新地址分配并初始化上层页表项（PUD/PMD），确保目标地址空间的页表结构就绪。\n\n- **`take_rmap_locks()` / `drop_rmap_locks()`**  \n  获取/释放反向映射所需的锁（`i_mmap_rwsem` 和 `anon_vma` 锁），防止在迁移过程中发生页回收或截断竞争。\n\n- **`move_soft_dirty_pte()`**  \n  在迁移 PTE 时保留或设置软脏位（`soft_dirty`），供用户态工具（如 checkpoint/restore）追踪页面修改状态。\n\n- **`move_ptes()`**  \n  **核心函数**：逐页迁移 PTE 项，处理 TLB 刷新、页表锁、userfaultfd-wp 标记清除、PTE 权限转换等。\n\n- **`move_normal_pmd()`**（条件编译）  \n  支持 PMD 粒度的大页迁移（需架构支持 `CONFIG_HAVE_MOVE_PMD`），提升大内存区域重映射性能。\n\n### 关键宏与辅助函数\n\n- **`arch_supports_page_table_move()`**  \n  判断当前架构是否支持 PMD/PUD 级别的页表项直接迁移。\n\n- **`vma_has_uffd_without_event_remap()`**  \n  检查 VMA 是否关联了禁用重映射事件的 userfaultfd，影响迁移策略。\n\n## 3. 关键实现\n\n### 页表迁移机制\n- **细粒度迁移（`move_ptes`）**：  \n  遍历源 PMD 覆盖的所有 PTE，通过 `ptep_get_and_clear()` 原子清空旧项，并在新位置通过 `set_pte_at()` 设置迁移后的 PTE。过程中：\n  - 调用 `move_pte()` 转换页保护属性（如缓存策略）。\n  - 通过 `move_soft_dirty_pte()` 保留软脏状态。\n  - 处理 userfaultfd 的写保护标记（`uffd-wp`），按需清除。\n  - 若存在有效页（`pte_present`），触发 TLB 刷新以保证一致性。\n\n- **大页迁移（`move_normal_pmd`）**：  \n  当源/目标地址对齐且架构支持时，直接迁移整个 PMD 项（跳过 PTE 级别），大幅提升效率。但需确保目标 PMD 为空（`pmd_none`），且不涉及 userfaultfd 的特殊处理。\n\n### 并发控制\n- **锁层次**：  \n  - 使用 `mmap_lock`（独占模式）防止 VMA 结构变动。\n  - 通过 `take_rmap_locks()` 获取 `anon_vma` 和 `i_mmap` 锁，避免迁移期间页被回收。\n  - 页表锁（`old_ptl`/`new_ptl`）采用嵌套锁（`SINGLE_DEPTH_NESTING`）防止死锁。\n\n- **TLB 一致性**：  \n  迁移后若存在有效页，调用 `flush_tlb_range()` 刷新旧地址范围的 TLB，确保 CPU 不再使用旧映射。\n\n### 特殊场景处理\n- **Execve 栈迁移**：  \n  允许源/目标区域重叠（向下移动），通过临时栈 VMA 标记（`vma_is_temporary_stack`）绕过常规 rmap 锁。\n- **Userfaultfd 兼容**：  \n  对禁用重映射事件的 uffd VMA，强制降级到 PTE 级迁移以清除所有 `uffd-wp` 标记。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- **内存管理核心**：`<linux/mm.h>`, `<linux/mman.h>`, `\"internal.h\"`\n- **页表操作**：`<asm/pgalloc.h>`, `<asm/tlb.h>`\n- **特殊内存特性**：  \n  - HugeTLB (`<linux/hugetlb.h>`)\n  - KSM (`<linux/ksm.h>`)\n  - Userfaultfd (`<linux/userfaultfd_k.h>`)\n  - 内存策略 (`<linux/mempolicy.h>`)\n- **安全与权限**：`<linux/security.h>`, `<linux/capability.h>`\n- **体系结构相关**：`<asm/cacheflush.h>`（TLB/缓存操作）\n\n### 子系统交互\n- **VMA 管理**：依赖 `mm_struct` 和 `vm_area_struct` 的完整性。\n- **反向映射（rmap）**：与匿名页（`anon_vma`）和文件页（`i_mmap`）的映射跟踪协同。\n- **页回收/迁移**：通过 rmap 锁避免与 `shrink_page_list()`、`migrate_pages()` 等竞争。\n- **用户态接口**：为 `sys_mremap()` 系统调用提供核心实现。\n\n## 5. 使用场景\n\n1. **`mremap()` 系统调用**  \n   用户程序调用 `mremap(old_addr, old_size, new_size, flags)` 时，内核通过此文件执行：\n   - 扩展/收缩现有 VMA（`MREMAP_MAYMOVE` 未置位时）。\n   - 将 VMA 移动到新地址（`MREMAP_MAYMOVE` 置位且新地址未指定时由内核分配）。\n   - 显式移动到指定地址（`MREMAP_FIXED` 模式）。\n\n2. **进程启动优化**  \n   `execve()` 加载新程序时，通过 `shift_arg_pages()` 移动初始栈位置，利用此文件的迁移能力。\n\n3. **内存热插拔/NUMA 迁移**  \n   配合内存策略（`mempolicy`）将页面迁移到不同节点，可能触发 VMA 重映射。\n\n4. **检查点/恢复（CRIU）**  \n   利用软脏位（`soft_dirty`）追踪页面修改，在恢复阶段通过 `mremap` 重建内存布局。\n\n5. **用户态缺页处理（Userfaultfd）**  \n   在 `UFFD_FEATURE_EVENT_REMAP` 未启用时，确保迁移过程正确处理写保护标记。",
      "similarity": 0.5064865350723267,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/mremap.c",
          "start_line": 1,
          "end_line": 104,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " *\tmm/mremap.c",
            " *",
            " *\t(C) Copyright 1996 Linus Torvalds",
            " *",
            " *\tAddress space accounting code\t<alan@lxorguk.ukuu.org.uk>",
            " *\t(C) Copyright 2002 Red Hat Inc, All Rights Reserved",
            " */",
            "",
            "#include <linux/mm.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/shm.h>",
            "#include <linux/ksm.h>",
            "#include <linux/mman.h>",
            "#include <linux/swap.h>",
            "#include <linux/capability.h>",
            "#include <linux/fs.h>",
            "#include <linux/swapops.h>",
            "#include <linux/highmem.h>",
            "#include <linux/security.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/mempolicy.h>",
            "",
            "#include <asm/cacheflush.h>",
            "#include <asm/tlb.h>",
            "#include <asm/pgalloc.h>",
            "",
            "#include \"internal.h\"",
            "",
            "static pud_t *get_old_pud(struct mm_struct *mm, unsigned long addr)",
            "{",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "",
            "\tpgd = pgd_offset(mm, addr);",
            "\tif (pgd_none_or_clear_bad(pgd))",
            "\t\treturn NULL;",
            "",
            "\tp4d = p4d_offset(pgd, addr);",
            "\tif (p4d_none_or_clear_bad(p4d))",
            "\t\treturn NULL;",
            "",
            "\tpud = pud_offset(p4d, addr);",
            "\tif (pud_none_or_clear_bad(pud))",
            "\t\treturn NULL;",
            "",
            "\treturn pud;",
            "}",
            "",
            "static pmd_t *get_old_pmd(struct mm_struct *mm, unsigned long addr)",
            "{",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tpud = get_old_pud(mm, addr);",
            "\tif (!pud)",
            "\t\treturn NULL;",
            "",
            "\tpmd = pmd_offset(pud, addr);",
            "\tif (pmd_none(*pmd))",
            "\t\treturn NULL;",
            "",
            "\treturn pmd;",
            "}",
            "",
            "static pud_t *alloc_new_pud(struct mm_struct *mm, struct vm_area_struct *vma,",
            "\t\t\t    unsigned long addr)",
            "{",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "",
            "\tpgd = pgd_offset(mm, addr);",
            "\tp4d = p4d_alloc(mm, pgd, addr);",
            "\tif (!p4d)",
            "\t\treturn NULL;",
            "",
            "\treturn pud_alloc(mm, p4d, addr);",
            "}",
            "",
            "static pmd_t *alloc_new_pmd(struct mm_struct *mm, struct vm_area_struct *vma,",
            "\t\t\t    unsigned long addr)",
            "{",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tpud = alloc_new_pud(mm, vma, addr);",
            "\tif (!pud)",
            "\t\treturn NULL;",
            "",
            "\tpmd = pmd_alloc(mm, pud, addr);",
            "\tif (!pmd)",
            "\t\treturn NULL;",
            "",
            "\tVM_BUG_ON(pmd_trans_huge(*pmd));",
            "",
            "\treturn pmd;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义获取和分配页表项的辅助函数，用于在mremap过程中处理PUD和PMD层级的页表结构调整。",
          "similarity": 0.5328617095947266
        },
        {
          "chunk_id": 3,
          "file_path": "mm/mremap.c",
          "start_line": 381,
          "end_line": 504,
          "content": [
            "static inline bool move_normal_pud(struct vm_area_struct *vma,",
            "\t\tunsigned long old_addr, unsigned long new_addr, pud_t *old_pud,",
            "\t\tpud_t *new_pud)",
            "{",
            "\treturn false;",
            "}",
            "static bool move_huge_pud(struct vm_area_struct *vma, unsigned long old_addr,",
            "\t\t\t  unsigned long new_addr, pud_t *old_pud, pud_t *new_pud)",
            "{",
            "\tspinlock_t *old_ptl, *new_ptl;",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tpud_t pud;",
            "",
            "\t/*",
            "\t * The destination pud shouldn't be established, free_pgtables()",
            "\t * should have released it.",
            "\t */",
            "\tif (WARN_ON_ONCE(!pud_none(*new_pud)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * We don't have to worry about the ordering of src and dst",
            "\t * ptlocks because exclusive mmap_lock prevents deadlock.",
            "\t */",
            "\told_ptl = pud_lock(vma->vm_mm, old_pud);",
            "\tnew_ptl = pud_lockptr(mm, new_pud);",
            "\tif (new_ptl != old_ptl)",
            "\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);",
            "",
            "\t/* Clear the pud */",
            "\tpud = *old_pud;",
            "\tpud_clear(old_pud);",
            "",
            "\tVM_BUG_ON(!pud_none(*new_pud));",
            "",
            "\t/* Set the new pud */",
            "\t/* mark soft_ditry when we add pud level soft dirty support */",
            "\tset_pud_at(mm, new_addr, new_pud, pud);",
            "\tflush_pud_tlb_range(vma, old_addr, old_addr + HPAGE_PUD_SIZE);",
            "\tif (new_ptl != old_ptl)",
            "\t\tspin_unlock(new_ptl);",
            "\tspin_unlock(old_ptl);",
            "",
            "\treturn true;",
            "}",
            "static bool move_huge_pud(struct vm_area_struct *vma, unsigned long old_addr,",
            "\t\t\t  unsigned long new_addr, pud_t *old_pud, pud_t *new_pud)",
            "{",
            "\tWARN_ON_ONCE(1);",
            "\treturn false;",
            "",
            "}",
            "static __always_inline unsigned long get_extent(enum pgt_entry entry,",
            "\t\t\tunsigned long old_addr, unsigned long old_end,",
            "\t\t\tunsigned long new_addr)",
            "{",
            "\tunsigned long next, extent, mask, size;",
            "",
            "\tswitch (entry) {",
            "\tcase HPAGE_PMD:",
            "\tcase NORMAL_PMD:",
            "\t\tmask = PMD_MASK;",
            "\t\tsize = PMD_SIZE;",
            "\t\tbreak;",
            "\tcase HPAGE_PUD:",
            "\tcase NORMAL_PUD:",
            "\t\tmask = PUD_MASK;",
            "\t\tsize = PUD_SIZE;",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tBUILD_BUG();",
            "\t\tbreak;",
            "\t}",
            "",
            "\tnext = (old_addr + size) & mask;",
            "\t/* even if next overflowed, extent below will be ok */",
            "\textent = next - old_addr;",
            "\tif (extent > old_end - old_addr)",
            "\t\textent = old_end - old_addr;",
            "\tnext = (new_addr + size) & mask;",
            "\tif (extent > next - new_addr)",
            "\t\textent = next - new_addr;",
            "\treturn extent;",
            "}",
            "static bool move_pgt_entry(enum pgt_entry entry, struct vm_area_struct *vma,",
            "\t\t\tunsigned long old_addr, unsigned long new_addr,",
            "\t\t\tvoid *old_entry, void *new_entry, bool need_rmap_locks)",
            "{",
            "\tbool moved = false;",
            "",
            "\t/* See comment in move_ptes() */",
            "\tif (need_rmap_locks)",
            "\t\ttake_rmap_locks(vma);",
            "",
            "\tswitch (entry) {",
            "\tcase NORMAL_PMD:",
            "\t\tmoved = move_normal_pmd(vma, old_addr, new_addr, old_entry,",
            "\t\t\t\t\tnew_entry);",
            "\t\tbreak;",
            "\tcase NORMAL_PUD:",
            "\t\tmoved = move_normal_pud(vma, old_addr, new_addr, old_entry,",
            "\t\t\t\t\tnew_entry);",
            "\t\tbreak;",
            "\tcase HPAGE_PMD:",
            "\t\tmoved = IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) &&",
            "\t\t\tmove_huge_pmd(vma, old_addr, new_addr, old_entry,",
            "\t\t\t\t      new_entry);",
            "\t\tbreak;",
            "\tcase HPAGE_PUD:",
            "\t\tmoved = IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) &&",
            "\t\t\tmove_huge_pud(vma, old_addr, new_addr, old_entry,",
            "\t\t\t\t      new_entry);",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(1);",
            "\t\tbreak;",
            "\t}",
            "",
            "\tif (need_rmap_locks)",
            "\t\tdrop_rmap_locks(vma);",
            "",
            "\treturn moved;",
            "}"
          ],
          "function_name": "move_normal_pud, move_huge_pud, move_huge_pud, get_extent, move_pgt_entry",
          "description": "处理巨型页（Huge Page）迁移逻辑，通过计算迁移范围并调用相应迁移函数，维护页表结构一致性。",
          "similarity": 0.518899142742157
        },
        {
          "chunk_id": 2,
          "file_path": "mm/mremap.c",
          "start_line": 240,
          "end_line": 372,
          "content": [
            "static inline bool arch_supports_page_table_move(void)",
            "{",
            "\treturn IS_ENABLED(CONFIG_HAVE_MOVE_PMD) ||",
            "\t\tIS_ENABLED(CONFIG_HAVE_MOVE_PUD);",
            "}",
            "static bool move_normal_pmd(struct vm_area_struct *vma, unsigned long old_addr,",
            "\t\t  unsigned long new_addr, pmd_t *old_pmd, pmd_t *new_pmd)",
            "{",
            "\tspinlock_t *old_ptl, *new_ptl;",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tbool res = false;",
            "\tpmd_t pmd;",
            "",
            "\tif (!arch_supports_page_table_move())",
            "\t\treturn false;",
            "\t/*",
            "\t * The destination pmd shouldn't be established, free_pgtables()",
            "\t * should have released it.",
            "\t *",
            "\t * However, there's a case during execve() where we use mremap",
            "\t * to move the initial stack, and in that case the target area",
            "\t * may overlap the source area (always moving down).",
            "\t *",
            "\t * If everything is PMD-aligned, that works fine, as moving",
            "\t * each pmd down will clear the source pmd. But if we first",
            "\t * have a few 4kB-only pages that get moved down, and then",
            "\t * hit the \"now the rest is PMD-aligned, let's do everything",
            "\t * one pmd at a time\", we will still have the old (now empty",
            "\t * of any 4kB pages, but still there) PMD in the page table",
            "\t * tree.",
            "\t *",
            "\t * Warn on it once - because we really should try to figure",
            "\t * out how to do this better - but then say \"I won't move",
            "\t * this pmd\".",
            "\t *",
            "\t * One alternative might be to just unmap the target pmd at",
            "\t * this point, and verify that it really is empty. We'll see.",
            "\t */",
            "\tif (WARN_ON_ONCE(!pmd_none(*new_pmd)))",
            "\t\treturn false;",
            "",
            "\t/* If this pmd belongs to a uffd vma with remap events disabled, we need",
            "\t * to ensure that the uffd-wp state is cleared from all pgtables. This",
            "\t * means recursing into lower page tables in move_page_tables(), and we",
            "\t * can reuse the existing code if we simply treat the entry as \"not",
            "\t * moved\".",
            "\t */",
            "\tif (vma_has_uffd_without_event_remap(vma))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * We don't have to worry about the ordering of src and dst",
            "\t * ptlocks because exclusive mmap_lock prevents deadlock.",
            "\t */",
            "\told_ptl = pmd_lock(vma->vm_mm, old_pmd);",
            "\tnew_ptl = pmd_lockptr(mm, new_pmd);",
            "\tif (new_ptl != old_ptl)",
            "\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);",
            "",
            "\tpmd = *old_pmd;",
            "",
            "\t/* Racing with collapse? */",
            "\tif (unlikely(!pmd_present(pmd) || pmd_leaf(pmd)))",
            "\t\tgoto out_unlock;",
            "\t/* Clear the pmd */",
            "\tpmd_clear(old_pmd);",
            "\tres = true;",
            "",
            "\tVM_BUG_ON(!pmd_none(*new_pmd));",
            "",
            "\tpmd_populate(mm, new_pmd, pmd_pgtable(pmd));",
            "\tflush_tlb_range(vma, old_addr, old_addr + PMD_SIZE);",
            "out_unlock:",
            "\tif (new_ptl != old_ptl)",
            "\t\tspin_unlock(new_ptl);",
            "\tspin_unlock(old_ptl);",
            "",
            "\treturn res;",
            "}",
            "static inline bool move_normal_pmd(struct vm_area_struct *vma,",
            "\t\tunsigned long old_addr, unsigned long new_addr, pmd_t *old_pmd,",
            "\t\tpmd_t *new_pmd)",
            "{",
            "\treturn false;",
            "}",
            "static bool move_normal_pud(struct vm_area_struct *vma, unsigned long old_addr,",
            "\t\t  unsigned long new_addr, pud_t *old_pud, pud_t *new_pud)",
            "{",
            "\tspinlock_t *old_ptl, *new_ptl;",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tpud_t pud;",
            "",
            "\tif (!arch_supports_page_table_move())",
            "\t\treturn false;",
            "\t/*",
            "\t * The destination pud shouldn't be established, free_pgtables()",
            "\t * should have released it.",
            "\t */",
            "\tif (WARN_ON_ONCE(!pud_none(*new_pud)))",
            "\t\treturn false;",
            "",
            "\t/* If this pud belongs to a uffd vma with remap events disabled, we need",
            "\t * to ensure that the uffd-wp state is cleared from all pgtables. This",
            "\t * means recursing into lower page tables in move_page_tables(), and we",
            "\t * can reuse the existing code if we simply treat the entry as \"not",
            "\t * moved\".",
            "\t */",
            "\tif (vma_has_uffd_without_event_remap(vma))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * We don't have to worry about the ordering of src and dst",
            "\t * ptlocks because exclusive mmap_lock prevents deadlock.",
            "\t */",
            "\told_ptl = pud_lock(vma->vm_mm, old_pud);",
            "\tnew_ptl = pud_lockptr(mm, new_pud);",
            "\tif (new_ptl != old_ptl)",
            "\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);",
            "",
            "\t/* Clear the pud */",
            "\tpud = *old_pud;",
            "\tpud_clear(old_pud);",
            "",
            "\tVM_BUG_ON(!pud_none(*new_pud));",
            "",
            "\tpud_populate(mm, new_pud, pud_pgtable(pud));",
            "\tflush_tlb_range(vma, old_addr, old_addr + PUD_SIZE);",
            "\tif (new_ptl != old_ptl)",
            "\t\tspin_unlock(new_ptl);",
            "\tspin_unlock(old_ptl);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "arch_supports_page_table_move, move_normal_pmd, move_normal_pmd, move_normal_pud",
          "description": "提供大页表项（PMD/PUD）直接迁移支持检测，当架构特性允许时尝试直接迁移大页以提升性能。",
          "similarity": 0.4992485046386719
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mremap.c",
          "start_line": 105,
          "end_line": 233,
          "content": [
            "static void take_rmap_locks(struct vm_area_struct *vma)",
            "{",
            "\tif (vma->vm_file)",
            "\t\ti_mmap_lock_write(vma->vm_file->f_mapping);",
            "\tif (vma->anon_vma)",
            "\t\tanon_vma_lock_write(vma->anon_vma);",
            "}",
            "static void drop_rmap_locks(struct vm_area_struct *vma)",
            "{",
            "\tif (vma->anon_vma)",
            "\t\tanon_vma_unlock_write(vma->anon_vma);",
            "\tif (vma->vm_file)",
            "\t\ti_mmap_unlock_write(vma->vm_file->f_mapping);",
            "}",
            "static pte_t move_soft_dirty_pte(pte_t pte)",
            "{",
            "\t/*",
            "\t * Set soft dirty bit so we can notice",
            "\t * in userspace the ptes were moved.",
            "\t */",
            "#ifdef CONFIG_MEM_SOFT_DIRTY",
            "\tif (pte_present(pte))",
            "\t\tpte = pte_mksoft_dirty(pte);",
            "\telse if (is_swap_pte(pte))",
            "\t\tpte = pte_swp_mksoft_dirty(pte);",
            "#endif",
            "\treturn pte;",
            "}",
            "static int move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,",
            "\t\tunsigned long old_addr, unsigned long old_end,",
            "\t\tstruct vm_area_struct *new_vma, pmd_t *new_pmd,",
            "\t\tunsigned long new_addr, bool need_rmap_locks)",
            "{",
            "\tbool need_clear_uffd_wp = vma_has_uffd_without_event_remap(vma);",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tpte_t *old_pte, *new_pte, pte;",
            "\tspinlock_t *old_ptl, *new_ptl;",
            "\tbool force_flush = false;",
            "\tunsigned long len = old_end - old_addr;",
            "\tint err = 0;",
            "",
            "\t/*",
            "\t * When need_rmap_locks is true, we take the i_mmap_rwsem and anon_vma",
            "\t * locks to ensure that rmap will always observe either the old or the",
            "\t * new ptes. This is the easiest way to avoid races with",
            "\t * truncate_pagecache(), page migration, etc...",
            "\t *",
            "\t * When need_rmap_locks is false, we use other ways to avoid",
            "\t * such races:",
            "\t *",
            "\t * - During exec() shift_arg_pages(), we use a specially tagged vma",
            "\t *   which rmap call sites look for using vma_is_temporary_stack().",
            "\t *",
            "\t * - During mremap(), new_vma is often known to be placed after vma",
            "\t *   in rmap traversal order. This ensures rmap will always observe",
            "\t *   either the old pte, or the new pte, or both (the page table locks",
            "\t *   serialize access to individual ptes, but only rmap traversal",
            "\t *   order guarantees that we won't miss both the old and new ptes).",
            "\t */",
            "\tif (need_rmap_locks)",
            "\t\ttake_rmap_locks(vma);",
            "",
            "\t/*",
            "\t * We don't have to worry about the ordering of src and dst",
            "\t * pte locks because exclusive mmap_lock prevents deadlock.",
            "\t */",
            "\told_pte = pte_offset_map_lock(mm, old_pmd, old_addr, &old_ptl);",
            "\tif (!old_pte) {",
            "\t\terr = -EAGAIN;",
            "\t\tgoto out;",
            "\t}",
            "\tnew_pte = pte_offset_map_nolock(mm, new_pmd, new_addr, &new_ptl);",
            "\tif (!new_pte) {",
            "\t\tpte_unmap_unlock(old_pte, old_ptl);",
            "\t\terr = -EAGAIN;",
            "\t\tgoto out;",
            "\t}",
            "\tif (new_ptl != old_ptl)",
            "\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);",
            "\tflush_tlb_batched_pending(vma->vm_mm);",
            "\tarch_enter_lazy_mmu_mode();",
            "",
            "\tfor (; old_addr < old_end; old_pte++, old_addr += PAGE_SIZE,",
            "\t\t\t\t   new_pte++, new_addr += PAGE_SIZE) {",
            "\t\tif (pte_none(ptep_get(old_pte)))",
            "\t\t\tcontinue;",
            "",
            "\t\tpte = ptep_get_and_clear(mm, old_addr, old_pte);",
            "\t\t/*",
            "\t\t * If we are remapping a valid PTE, make sure",
            "\t\t * to flush TLB before we drop the PTL for the",
            "\t\t * PTE.",
            "\t\t *",
            "\t\t * NOTE! Both old and new PTL matter: the old one",
            "\t\t * for racing with page_mkclean(), the new one to",
            "\t\t * make sure the physical page stays valid until",
            "\t\t * the TLB entry for the old mapping has been",
            "\t\t * flushed.",
            "\t\t */",
            "\t\tif (pte_present(pte))",
            "\t\t\tforce_flush = true;",
            "\t\tpte = move_pte(pte, new_vma->vm_page_prot, old_addr, new_addr);",
            "\t\tpte = move_soft_dirty_pte(pte);",
            "",
            "\t\tif (need_clear_uffd_wp && pte_marker_uffd_wp(pte))",
            "\t\t\tpte_clear(mm, new_addr, new_pte);",
            "\t\telse {",
            "\t\t\tif (need_clear_uffd_wp) {",
            "\t\t\t\tif (pte_present(pte))",
            "\t\t\t\t\tpte = pte_clear_uffd_wp(pte);",
            "\t\t\t\telse if (is_swap_pte(pte))",
            "\t\t\t\t\tpte = pte_swp_clear_uffd_wp(pte);",
            "\t\t\t}",
            "\t\t\tset_pte_at(mm, new_addr, new_pte, pte);",
            "\t\t}",
            "\t}",
            "",
            "\tarch_leave_lazy_mmu_mode();",
            "\tif (force_flush)",
            "\t\tflush_tlb_range(vma, old_end - len, old_end);",
            "\tif (new_ptl != old_ptl)",
            "\t\tspin_unlock(new_ptl);",
            "\tpte_unmap(new_pte - 1);",
            "\tpte_unmap_unlock(old_pte - 1, old_ptl);",
            "out:",
            "\tif (need_rmap_locks)",
            "\t\tdrop_rmap_locks(vma);",
            "\treturn err;",
            "}"
          ],
          "function_name": "take_rmap_locks, drop_rmap_locks, move_soft_dirty_pte, move_ptes",
          "description": "实现页表项迁移核心逻辑，包含获取/释放rmap锁、处理软脏位标记、批量迁移页表项及处理用户故障描述符（UFFD）相关状态。",
          "similarity": 0.49333253502845764
        },
        {
          "chunk_id": 4,
          "file_path": "mm/mremap.c",
          "start_line": 529,
          "end_line": 780,
          "content": [
            "unsigned long move_page_tables(struct vm_area_struct *vma,",
            "\t\tunsigned long old_addr, struct vm_area_struct *new_vma,",
            "\t\tunsigned long new_addr, unsigned long len,",
            "\t\tbool need_rmap_locks)",
            "{",
            "\tunsigned long extent, old_end;",
            "\tstruct mmu_notifier_range range;",
            "\tpmd_t *old_pmd, *new_pmd;",
            "\tpud_t *old_pud, *new_pud;",
            "",
            "\tif (!len)",
            "\t\treturn 0;",
            "",
            "\told_end = old_addr + len;",
            "",
            "\tif (is_vm_hugetlb_page(vma))",
            "\t\treturn move_hugetlb_page_tables(vma, new_vma, old_addr,",
            "\t\t\t\t\t\tnew_addr, len);",
            "",
            "\tflush_cache_range(vma, old_addr, old_end);",
            "\tmmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0, vma->vm_mm,",
            "\t\t\t\told_addr, old_end);",
            "\tmmu_notifier_invalidate_range_start(&range);",
            "",
            "\tfor (; old_addr < old_end; old_addr += extent, new_addr += extent) {",
            "\t\tcond_resched();",
            "\t\t/*",
            "\t\t * If extent is PUD-sized try to speed up the move by moving at the",
            "\t\t * PUD level if possible.",
            "\t\t */",
            "\t\textent = get_extent(NORMAL_PUD, old_addr, old_end, new_addr);",
            "",
            "\t\told_pud = get_old_pud(vma->vm_mm, old_addr);",
            "\t\tif (!old_pud)",
            "\t\t\tcontinue;",
            "\t\tnew_pud = alloc_new_pud(vma->vm_mm, vma, new_addr);",
            "\t\tif (!new_pud)",
            "\t\t\tbreak;",
            "\t\tif (pud_trans_huge(*old_pud) || pud_devmap(*old_pud)) {",
            "\t\t\tif (extent == HPAGE_PUD_SIZE) {",
            "\t\t\t\tmove_pgt_entry(HPAGE_PUD, vma, old_addr, new_addr,",
            "\t\t\t\t\t       old_pud, new_pud, need_rmap_locks);",
            "\t\t\t\t/* We ignore and continue on error? */",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t} else if (IS_ENABLED(CONFIG_HAVE_MOVE_PUD) && extent == PUD_SIZE) {",
            "",
            "\t\t\tif (move_pgt_entry(NORMAL_PUD, vma, old_addr, new_addr,",
            "\t\t\t\t\t   old_pud, new_pud, true))",
            "\t\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\textent = get_extent(NORMAL_PMD, old_addr, old_end, new_addr);",
            "\t\told_pmd = get_old_pmd(vma->vm_mm, old_addr);",
            "\t\tif (!old_pmd)",
            "\t\t\tcontinue;",
            "\t\tnew_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);",
            "\t\tif (!new_pmd)",
            "\t\t\tbreak;",
            "again:",
            "\t\tif (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd) ||",
            "\t\t    pmd_devmap(*old_pmd)) {",
            "\t\t\tif (extent == HPAGE_PMD_SIZE &&",
            "\t\t\t    move_pgt_entry(HPAGE_PMD, vma, old_addr, new_addr,",
            "\t\t\t\t\t   old_pmd, new_pmd, need_rmap_locks))",
            "\t\t\t\tcontinue;",
            "\t\t\tsplit_huge_pmd(vma, old_pmd, old_addr);",
            "\t\t} else if (IS_ENABLED(CONFIG_HAVE_MOVE_PMD) &&",
            "\t\t\t   extent == PMD_SIZE) {",
            "\t\t\t/*",
            "\t\t\t * If the extent is PMD-sized, try to speed the move by",
            "\t\t\t * moving at the PMD level if possible.",
            "\t\t\t */",
            "\t\t\tif (move_pgt_entry(NORMAL_PMD, vma, old_addr, new_addr,",
            "\t\t\t\t\t   old_pmd, new_pmd, true))",
            "\t\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (pmd_none(*old_pmd))",
            "\t\t\tcontinue;",
            "\t\tif (pte_alloc(new_vma->vm_mm, new_pmd))",
            "\t\t\tbreak;",
            "\t\tif (move_ptes(vma, old_pmd, old_addr, old_addr + extent,",
            "\t\t\t      new_vma, new_pmd, new_addr, need_rmap_locks) < 0)",
            "\t\t\tgoto again;",
            "\t}",
            "",
            "\tmmu_notifier_invalidate_range_end(&range);",
            "",
            "\treturn len + old_addr - old_end;\t/* how much done */",
            "}",
            "static unsigned long move_vma(struct vm_area_struct *vma,",
            "\t\tunsigned long old_addr, unsigned long old_len,",
            "\t\tunsigned long new_len, unsigned long new_addr,",
            "\t\tbool *locked, unsigned long flags,",
            "\t\tstruct vm_userfaultfd_ctx *uf, struct list_head *uf_unmap)",
            "{",
            "\tlong to_account = new_len - old_len;",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tstruct vm_area_struct *new_vma;",
            "\tunsigned long vm_flags = vma->vm_flags;",
            "\tunsigned long new_pgoff;",
            "\tunsigned long moved_len;",
            "\tbool account_start = false;",
            "\tbool account_end = false;",
            "\tunsigned long hiwater_vm;",
            "\tint err = 0;",
            "\tbool need_rmap_locks;",
            "\tstruct vma_iterator vmi;",
            "",
            "\t/*",
            "\t * We'd prefer to avoid failure later on in do_munmap:",
            "\t * which may split one vma into three before unmapping.",
            "\t */",
            "\tif (mm->map_count >= sysctl_max_map_count - 3)",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (unlikely(flags & MREMAP_DONTUNMAP))",
            "\t\tto_account = new_len;",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->may_split) {",
            "\t\tif (vma->vm_start != old_addr)",
            "\t\t\terr = vma->vm_ops->may_split(vma, old_addr);",
            "\t\tif (!err && vma->vm_end != old_addr + old_len)",
            "\t\t\terr = vma->vm_ops->may_split(vma, old_addr + old_len);",
            "\t\tif (err)",
            "\t\t\treturn err;",
            "\t}",
            "",
            "\t/*",
            "\t * Advise KSM to break any KSM pages in the area to be moved:",
            "\t * it would be confusing if they were to turn up at the new",
            "\t * location, where they happen to coincide with different KSM",
            "\t * pages recently unmapped.  But leave vma->vm_flags as it was,",
            "\t * so KSM can come around to merge on vma and new_vma afterwards.",
            "\t */",
            "\terr = ksm_madvise(vma, old_addr, old_addr + old_len,",
            "\t\t\t\t\t\tMADV_UNMERGEABLE, &vm_flags);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (vm_flags & VM_ACCOUNT) {",
            "\t\tif (security_vm_enough_memory_mm(mm, to_account >> PAGE_SHIFT))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\tvma_start_write(vma);",
            "\tnew_pgoff = vma->vm_pgoff + ((old_addr - vma->vm_start) >> PAGE_SHIFT);",
            "\tnew_vma = copy_vma(&vma, new_addr, new_len, new_pgoff,",
            "\t\t\t   &need_rmap_locks);",
            "\tif (!new_vma) {",
            "\t\tif (vm_flags & VM_ACCOUNT)",
            "\t\t\tvm_unacct_memory(to_account >> PAGE_SHIFT);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\tmoved_len = move_page_tables(vma, old_addr, new_vma, new_addr, old_len,",
            "\t\t\t\t     need_rmap_locks);",
            "\tif (moved_len < old_len) {",
            "\t\terr = -ENOMEM;",
            "\t} else if (vma->vm_ops && vma->vm_ops->mremap) {",
            "\t\terr = vma->vm_ops->mremap(new_vma);",
            "\t}",
            "",
            "\tif (unlikely(err)) {",
            "\t\t/*",
            "\t\t * On error, move entries back from new area to old,",
            "\t\t * which will succeed since page tables still there,",
            "\t\t * and then proceed to unmap new area instead of old.",
            "\t\t */",
            "\t\tmove_page_tables(new_vma, new_addr, vma, old_addr, moved_len,",
            "\t\t\t\t true);",
            "\t\tvma = new_vma;",
            "\t\told_len = new_len;",
            "\t\told_addr = new_addr;",
            "\t\tnew_addr = err;",
            "\t} else {",
            "\t\tmremap_userfaultfd_prep(new_vma, uf);",
            "\t}",
            "",
            "\tif (is_vm_hugetlb_page(vma)) {",
            "\t\tclear_vma_resv_huge_pages(vma);",
            "\t}",
            "",
            "\t/* Conceal VM_ACCOUNT so old reservation is not undone */",
            "\tif (vm_flags & VM_ACCOUNT && !(flags & MREMAP_DONTUNMAP)) {",
            "\t\tvm_flags_clear(vma, VM_ACCOUNT);",
            "\t\tif (vma->vm_start < old_addr)",
            "\t\t\taccount_start = true;",
            "\t\tif (vma->vm_end > old_addr + old_len)",
            "\t\t\taccount_end = true;",
            "\t}",
            "",
            "\t/*",
            "\t * If we failed to move page tables we still do total_vm increment",
            "\t * since do_munmap() will decrement it by old_len == new_len.",
            "\t *",
            "\t * Since total_vm is about to be raised artificially high for a",
            "\t * moment, we need to restore high watermark afterwards: if stats",
            "\t * are taken meanwhile, total_vm and hiwater_vm appear too high.",
            "\t * If this were a serious issue, we'd add a flag to do_munmap().",
            "\t */",
            "\thiwater_vm = mm->hiwater_vm;",
            "\tvm_stat_account(mm, vma->vm_flags, new_len >> PAGE_SHIFT);",
            "",
            "\t/* Tell pfnmap has moved from this vma */",
            "\tif (unlikely(vma->vm_flags & VM_PFNMAP))",
            "\t\tuntrack_pfn_clear(vma);",
            "",
            "\tif (unlikely(!err && (flags & MREMAP_DONTUNMAP))) {",
            "\t\t/* We always clear VM_LOCKED[ONFAULT] on the old vma */",
            "\t\tvm_flags_clear(vma, VM_LOCKED_MASK);",
            "",
            "\t\t/*",
            "\t\t * anon_vma links of the old vma is no longer needed after its page",
            "\t\t * table has been moved.",
            "\t\t */",
            "\t\tif (new_vma != vma && vma->vm_start == old_addr &&",
            "\t\t\tvma->vm_end == (old_addr + old_len))",
            "\t\t\tunlink_anon_vmas(vma);",
            "",
            "\t\t/* Because we won't unmap we don't need to touch locked_vm */",
            "\t\treturn new_addr;",
            "\t}",
            "",
            "\tvma_iter_init(&vmi, mm, old_addr);",
            "\tif (do_vmi_munmap(&vmi, mm, old_addr, old_len, uf_unmap, false) < 0) {",
            "\t\t/* OOM: unable to split vma, just get accounts right */",
            "\t\tif (vm_flags & VM_ACCOUNT && !(flags & MREMAP_DONTUNMAP))",
            "\t\t\tvm_acct_memory(old_len >> PAGE_SHIFT);",
            "\t\taccount_start = account_end = false;",
            "\t}",
            "",
            "\tif (vm_flags & VM_LOCKED) {",
            "\t\tmm->locked_vm += new_len >> PAGE_SHIFT;",
            "\t\t*locked = true;",
            "\t}",
            "",
            "\tmm->hiwater_vm = hiwater_vm;",
            "",
            "\t/* Restore VM_ACCOUNT if one or two pieces of vma left */",
            "\tif (account_start) {",
            "\t\tvma = vma_prev(&vmi);",
            "\t\tvm_flags_set(vma, VM_ACCOUNT);",
            "\t}",
            "",
            "\tif (account_end) {",
            "\t\tvma = vma_next(&vmi);",
            "\t\tvm_flags_set(vma, VM_ACCOUNT);",
            "\t}",
            "",
            "\treturn new_addr;",
            "}"
          ],
          "function_name": "move_page_tables, move_vma",
          "description": "实现虚拟内存区域（VMA）迁移主流程，包括页表迁移、VMA复制、内存统计更新及异常处理，支持普通页与巨型页迁移场景。",
          "similarity": 0.4479864835739136
        }
      ]
    }
  ]
}