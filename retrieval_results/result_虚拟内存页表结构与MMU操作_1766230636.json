{
  "query": "虚拟内存页表结构与MMU操作",
  "timestamp": "2025-12-20 19:37:16",
  "retrieved_files": [
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.6785576939582825,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.6827618479728699
        },
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.576849102973938
        },
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.5754854679107666
        }
      ]
    },
    {
      "source_file": "mm/nommu.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:57:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `nommu.c`\n\n---\n\n# nommu.c 技术文档\n\n## 1. 文件概述\n\n`nommu.c` 是 Linux 内核中为不支持内存管理单元（MMU）的 CPU 架构提供的内存管理替代实现。在无 MMU 的系统（如某些嵌入式处理器）中，无法使用虚拟内存机制，因此该文件提供了一套简化但功能完整的内存管理接口，以兼容标准内核 API。其核心目标是模拟 `mm/` 子系统中与虚拟内存相关的函数行为，同时避免依赖页表、地址转换等 MMU 特性。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `high_memory`：指向高内存区域起始地址（在 NOMMU 系统中通常为 NULL 或物理内存末尾）\n- `mem_map`：物理页描述符数组的起始地址\n- `max_mapnr`：系统中最大页帧号（PFN）\n- `highest_memmap_pfn`：`mem_map` 中最高有效 PFN\n- `sysctl_nr_trim_pages`：初始内存修剪阈值（用于释放未使用的内存块）\n- `heap_stack_gap`：堆与栈之间的最小间隙（NOMMU 下通常为 0）\n- `mmap_pages_allocated`：通过 mmap 分配的总页数（原子计数器）\n- `nommu_region_tree`：红黑树，用于管理已映射的共享内存区域\n- `nommu_region_sem`：读写信号量，保护 `nommu_region_tree` 的并发访问\n- `vm_region_jar`：slab 缓存，用于分配 `vm_region` 结构\n\n### 主要函数\n- `kobjsize(const void *objp)`：估算给定指针所占内存大小（支持 kmalloc、VMA 区域或普通页）\n- `follow_pfn(struct vm_area_struct *vma, unsigned long address, unsigned long *pfn)`：从用户虚拟地址获取物理页帧号（仅支持 IO 或 PFN 映射）\n- `vfree(const void *addr)`：释放由 vmalloc 分配的内存（实际调用 kfree）\n- `__vmalloc_noprof()` 及相关变体（`vmalloc`, `vzalloc`, `vmalloc_user` 等）：提供 vmalloc 系列函数的 NOMMU 实现（底层使用 kmalloc）\n- `vmalloc_to_page()` / `vmalloc_to_pfn()`：将 vmalloc 地址转换为 page 或 PFN（直接使用 `virt_to_page`）\n- `vread_iter()`：将内核地址内容拷贝到 iov_iter（用于 `/proc/vmallocinfo` 等）\n\n### 操作结构体\n- `generic_file_vm_ops`：空的 `vm_operations_struct`，作为 NOMMU 下文件映射的默认操作集\n\n## 3. 关键实现\n\n### 内存分配模型\n- **无虚拟地址空间**：所有“虚拟”地址实为物理地址的线性映射，`vmalloc` 系列函数退化为 `kmalloc` 调用。\n- **GFP 标志处理**：自动清除 `__GFP_HIGHMEM`（因 kmalloc 无法返回高端内存逻辑地址），并添加 `__GFP_COMP` 以支持复合页。\n- **零填充支持**：`vzalloc` 等函数通过 `__GFP_ZERO` 标志实现。\n\n### 内存区域管理\n- **共享区域跟踪**：使用红黑树 `nommu_region_tree` 和 slab 缓存 `vm_region_jar` 管理可共享的映射区域（如文件映射），确保多个进程可共享同一物理内存。\n- **VMA 标记**：`vmalloc_user` 分配的内存会标记 `VM_USERMAP`，允许后续通过 `remap_vmalloc_range` 映射到用户空间。\n\n### 地址转换简化\n- **PFN 获取**：`follow_pfn` 直接通过地址右移 `PAGE_SHIFT` 计算 PFN（因无虚拟地址转换）。\n- **vmalloc 地址转换**：`vmalloc_to_page` 直接调用 `virt_to_page`（因 vmalloc 地址即物理地址的线性映射）。\n\n### 安全与兼容性\n- **kobjsize 安全检查**：先验证地址有效性（`virt_addr_valid`），再根据页类型（Slab/Compound/VMA）返回不同大小。\n- **用户空间映射**：`vmalloc_user` 在分配后查找对应 VMA 并设置 `VM_USERMAP`，确保安全暴露给用户态。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心内存管理**：`<linux/mm.h>`, `<linux/mman.h>`, `\"internal.h\"`\n- **内存分配**：`<linux/slab.h>`, `<linux/vmalloc.h>`\n- **页与地址转换**：`<linux/highmem.h>`, `<linux/pagemap.h>`, `<asm/tlb.h>`\n- **进程与安全**：`<linux/sched/mm.h>`, `<linux/security.h>`, `<linux/audit.h>`\n- **I/O 与文件**：`<linux/file.h>`, `<linux/uio.h>`, `<linux/backing-dev.h>`\n\n### 符号导出\n- 导出关键符号供其他模块使用：`high_memory`, `max_mapnr`, `mem_map`, `follow_pfn`, `vfree`, `vmalloc` 系列函数等。\n\n### 架构依赖\n- 依赖架构特定头文件（如 `asm/tlb.h`, `asm/mmu_context.h`），但 NOMMU 架构下这些通常为空实现。\n\n## 5. 使用场景\n\n- **无 MMU 嵌入式系统**：运行于 uClinux 等无虚拟内存系统的设备（如早期 ARM7、Blackfin、m68knommu）。\n- **内核子系统兼容**：为依赖 `vmalloc`、`vfree`、`follow_pfn` 等接口的驱动或子系统（如 GPU 驱动、DMA 映射）提供 NOMMU 兼容层。\n- **用户空间内存映射**：支持 `mmap` 系统调用对文件或设备的映射（通过 `nommu_region_tree` 管理共享区域）。\n- **调试与监控**：`vread_iter` 支持 `/proc/vmallocinfo` 等接口读取内核内存布局。\n- **安全内存分配**：`vmalloc_user` 提供可安全映射到用户空间的零初始化内存（用于 IPC 或共享缓冲区）。",
      "similarity": 0.6488006711006165,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/nommu.c",
          "start_line": 72,
          "end_line": 179,
          "content": [
            "unsigned int kobjsize(const void *objp)",
            "{",
            "\tstruct page *page;",
            "",
            "\t/*",
            "\t * If the object we have should not have ksize performed on it,",
            "\t * return size of 0",
            "\t */",
            "\tif (!objp || !virt_addr_valid(objp))",
            "\t\treturn 0;",
            "",
            "\tpage = virt_to_head_page(objp);",
            "",
            "\t/*",
            "\t * If the allocator sets PageSlab, we know the pointer came from",
            "\t * kmalloc().",
            "\t */",
            "\tif (PageSlab(page))",
            "\t\treturn ksize(objp);",
            "",
            "\t/*",
            "\t * If it's not a compound page, see if we have a matching VMA",
            "\t * region. This test is intentionally done in reverse order,",
            "\t * so if there's no VMA, we still fall through and hand back",
            "\t * PAGE_SIZE for 0-order pages.",
            "\t */",
            "\tif (!PageCompound(page)) {",
            "\t\tstruct vm_area_struct *vma;",
            "",
            "\t\tvma = find_vma(current->mm, (unsigned long)objp);",
            "\t\tif (vma)",
            "\t\t\treturn vma->vm_end - vma->vm_start;",
            "\t}",
            "",
            "\t/*",
            "\t * The ksize() function is only guaranteed to work for pointers",
            "\t * returned by kmalloc(). So handle arbitrary pointers here.",
            "\t */",
            "\treturn page_size(page);",
            "}",
            "int follow_pfn(struct vm_area_struct *vma, unsigned long address,",
            "\tunsigned long *pfn)",
            "{",
            "\tif (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))",
            "\t\treturn -EINVAL;",
            "",
            "\t*pfn = address >> PAGE_SHIFT;",
            "\treturn 0;",
            "}",
            "void vfree(const void *addr)",
            "{",
            "\tkfree(addr);",
            "}",
            "unsigned long vmalloc_to_pfn(const void *addr)",
            "{",
            "\treturn page_to_pfn(virt_to_page(addr));",
            "}",
            "long vread_iter(struct iov_iter *iter, const char *addr, size_t count)",
            "{",
            "\t/* Don't allow overflow */",
            "\tif ((unsigned long) addr + count < count)",
            "\t\tcount = -(unsigned long) addr;",
            "",
            "\treturn copy_to_iter(addr, count, iter);",
            "}",
            "void vunmap(const void *addr)",
            "{",
            "\tBUG();",
            "}",
            "void vm_unmap_ram(const void *mem, unsigned int count)",
            "{",
            "\tBUG();",
            "}",
            "void vm_unmap_aliases(void)",
            "{",
            "}",
            "void free_vm_area(struct vm_struct *area)",
            "{",
            "\tBUG();",
            "}",
            "int vm_insert_page(struct vm_area_struct *vma, unsigned long addr,",
            "\t\t   struct page *page)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "int vm_insert_pages(struct vm_area_struct *vma, unsigned long addr,",
            "\t\t\tstruct page **pages, unsigned long *num)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "int vm_map_pages(struct vm_area_struct *vma, struct page **pages,",
            "\t\t\tunsigned long num)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "int vm_map_pages_zero(struct vm_area_struct *vma, struct page **pages,",
            "\t\t\t\tunsigned long num)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "void __init mmap_init(void)",
            "{",
            "\tint ret;",
            "",
            "\tret = percpu_counter_init(&vm_committed_as, 0, GFP_KERNEL);",
            "\tVM_BUG_ON(ret);",
            "\tvm_region_jar = KMEM_CACHE(vm_region, SLAB_PANIC|SLAB_ACCOUNT);",
            "}"
          ],
          "function_name": "kobjsize, follow_pfn, vfree, vmalloc_to_pfn, vread_iter, vunmap, vm_unmap_ram, vm_unmap_aliases, free_vm_area, vm_insert_page, vm_insert_pages, vm_map_pages, vm_map_pages_zero, mmap_init",
          "description": "提供非MMU环境下的虚拟内存操作替代函数，如kobjsize计算对象大小、follow_pfn获取物理帧号、vfree释放内存等。部分函数因缺乏MMU支持而直接返回错误或触发BUG。包含mmap_init初始化相关资源。",
          "similarity": 0.6763169765472412
        },
        {
          "chunk_id": 7,
          "file_path": "mm/nommu.c",
          "start_line": 1437,
          "end_line": 1569,
          "content": [
            "int do_munmap(struct mm_struct *mm, unsigned long start, size_t len, struct list_head *uf)",
            "{",
            "\tVMA_ITERATOR(vmi, mm, start);",
            "\tstruct vm_area_struct *vma;",
            "\tunsigned long end;",
            "\tint ret = 0;",
            "",
            "\tlen = PAGE_ALIGN(len);",
            "\tif (len == 0)",
            "\t\treturn -EINVAL;",
            "",
            "\tend = start + len;",
            "",
            "\t/* find the first potentially overlapping VMA */",
            "\tvma = vma_find(&vmi, end);",
            "\tif (!vma) {",
            "\t\tstatic int limit;",
            "\t\tif (limit < 5) {",
            "\t\t\tpr_warn(\"munmap of memory not mmapped by process %d (%s): 0x%lx-0x%lx\\n\",",
            "\t\t\t\t\tcurrent->pid, current->comm,",
            "\t\t\t\t\tstart, start + len - 1);",
            "\t\t\tlimit++;",
            "\t\t}",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\t/* we're allowed to split an anonymous VMA but not a file-backed one */",
            "\tif (vma->vm_file) {",
            "\t\tdo {",
            "\t\t\tif (start > vma->vm_start)",
            "\t\t\t\treturn -EINVAL;",
            "\t\t\tif (end == vma->vm_end)",
            "\t\t\t\tgoto erase_whole_vma;",
            "\t\t\tvma = vma_find(&vmi, end);",
            "\t\t} while (vma);",
            "\t\treturn -EINVAL;",
            "\t} else {",
            "\t\t/* the chunk must be a subset of the VMA found */",
            "\t\tif (start == vma->vm_start && end == vma->vm_end)",
            "\t\t\tgoto erase_whole_vma;",
            "\t\tif (start < vma->vm_start || end > vma->vm_end)",
            "\t\t\treturn -EINVAL;",
            "\t\tif (offset_in_page(start))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (end != vma->vm_end && offset_in_page(end))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (start != vma->vm_start && end != vma->vm_end) {",
            "\t\t\tret = split_vma(&vmi, vma, start, 1);",
            "\t\t\tif (ret < 0)",
            "\t\t\t\treturn ret;",
            "\t\t}",
            "\t\treturn vmi_shrink_vma(&vmi, vma, start, end);",
            "\t}",
            "",
            "erase_whole_vma:",
            "\tif (delete_vma_from_mm(vma))",
            "\t\tret = -ENOMEM;",
            "\telse",
            "\t\tdelete_vma(mm, vma);",
            "\treturn ret;",
            "}",
            "int vm_munmap(unsigned long addr, size_t len)",
            "{",
            "\tstruct mm_struct *mm = current->mm;",
            "\tint ret;",
            "",
            "\tmmap_write_lock(mm);",
            "\tret = do_munmap(mm, addr, len, NULL);",
            "\tmmap_write_unlock(mm);",
            "\treturn ret;",
            "}",
            "void exit_mmap(struct mm_struct *mm)",
            "{",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "\tstruct vm_area_struct *vma;",
            "",
            "\tif (!mm)",
            "\t\treturn;",
            "",
            "\tmm->total_vm = 0;",
            "",
            "\t/*",
            "\t * Lock the mm to avoid assert complaining even though this is the only",
            "\t * user of the mm",
            "\t */",
            "\tmmap_write_lock(mm);",
            "\tfor_each_vma(vmi, vma) {",
            "\t\tcleanup_vma_from_mm(vma);",
            "\t\tdelete_vma(mm, vma);",
            "\t\tcond_resched();",
            "\t}",
            "\t__mt_destroy(&mm->mm_mt);",
            "\tmmap_write_unlock(mm);",
            "}",
            "int vm_brk(unsigned long addr, unsigned long len)",
            "{",
            "\treturn -ENOMEM;",
            "}",
            "static unsigned long do_mremap(unsigned long addr,",
            "\t\t\tunsigned long old_len, unsigned long new_len,",
            "\t\t\tunsigned long flags, unsigned long new_addr)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "",
            "\t/* insanity checks first */",
            "\told_len = PAGE_ALIGN(old_len);",
            "\tnew_len = PAGE_ALIGN(new_len);",
            "\tif (old_len == 0 || new_len == 0)",
            "\t\treturn (unsigned long) -EINVAL;",
            "",
            "\tif (offset_in_page(addr))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (flags & MREMAP_FIXED && new_addr != addr)",
            "\t\treturn (unsigned long) -EINVAL;",
            "",
            "\tvma = find_vma_exact(current->mm, addr, old_len);",
            "\tif (!vma)",
            "\t\treturn (unsigned long) -EINVAL;",
            "",
            "\tif (vma->vm_end != vma->vm_start + old_len)",
            "\t\treturn (unsigned long) -EFAULT;",
            "",
            "\tif (is_nommu_shared_mapping(vma->vm_flags))",
            "\t\treturn (unsigned long) -EPERM;",
            "",
            "\tif (new_len > vma->vm_region->vm_end - vma->vm_region->vm_start)",
            "\t\treturn (unsigned long) -ENOMEM;",
            "",
            "\t/* all checks complete - do it */",
            "\tvma->vm_end = vma->vm_start + new_len;",
            "\treturn vma->vm_start;",
            "}"
          ],
          "function_name": "do_munmap, vm_munmap, exit_mmap, vm_brk, do_mremap",
          "description": "该代码块实现了NOMMU环境下虚拟内存区域（VMA）的管理和操作。do_munmap处理munmap系统调用，通过VMA迭代器定位并裁剪/删除指定范围的内存映射；vm_munmap是其公共接口封装，exit_mmap负责进程退出时的VMA清理，而do_mremap用于调整现有VMA大小。其中vm_brk未实现出错返回，表明NOMMU环境中brk操作可能受限。",
          "similarity": 0.6690860986709595
        },
        {
          "chunk_id": 6,
          "file_path": "mm/nommu.c",
          "start_line": 1260,
          "end_line": 1389,
          "content": [
            "unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,",
            "\t\t\t      unsigned long prot, unsigned long flags,",
            "\t\t\t      unsigned long fd, unsigned long pgoff)",
            "{",
            "\tstruct file *file = NULL;",
            "\tunsigned long retval = -EBADF;",
            "",
            "\taudit_mmap_fd(fd, flags);",
            "\tif (!(flags & MAP_ANONYMOUS)) {",
            "\t\tfile = fget(fd);",
            "\t\tif (!file)",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);",
            "",
            "\tif (file)",
            "\t\tfput(file);",
            "out:",
            "\treturn retval;",
            "}",
            "static int split_vma(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\t     unsigned long addr, int new_below)",
            "{",
            "\tstruct vm_area_struct *new;",
            "\tstruct vm_region *region;",
            "\tunsigned long npages;",
            "\tstruct mm_struct *mm;",
            "",
            "\t/* we're only permitted to split anonymous regions (these should have",
            "\t * only a single usage on the region) */",
            "\tif (vma->vm_file)",
            "\t\treturn -ENOMEM;",
            "",
            "\tmm = vma->vm_mm;",
            "\tif (mm->map_count >= sysctl_max_map_count)",
            "\t\treturn -ENOMEM;",
            "",
            "\tregion = kmem_cache_alloc(vm_region_jar, GFP_KERNEL);",
            "\tif (!region)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = vm_area_dup(vma);",
            "\tif (!new)",
            "\t\tgoto err_vma_dup;",
            "",
            "\t/* most fields are the same, copy all, and then fixup */",
            "\t*region = *vma->vm_region;",
            "\tnew->vm_region = region;",
            "",
            "\tnpages = (addr - vma->vm_start) >> PAGE_SHIFT;",
            "",
            "\tif (new_below) {",
            "\t\tregion->vm_top = region->vm_end = new->vm_end = addr;",
            "\t} else {",
            "\t\tregion->vm_start = new->vm_start = addr;",
            "\t\tregion->vm_pgoff = new->vm_pgoff += npages;",
            "\t}",
            "",
            "\tvma_iter_config(vmi, new->vm_start, new->vm_end);",
            "\tif (vma_iter_prealloc(vmi, vma)) {",
            "\t\tpr_warn(\"Allocation of vma tree for process %d failed\\n\",",
            "\t\t\tcurrent->pid);",
            "\t\tgoto err_vmi_preallocate;",
            "\t}",
            "",
            "\tif (new->vm_ops && new->vm_ops->open)",
            "\t\tnew->vm_ops->open(new);",
            "",
            "\tdown_write(&nommu_region_sem);",
            "\tdelete_nommu_region(vma->vm_region);",
            "\tif (new_below) {",
            "\t\tvma->vm_region->vm_start = vma->vm_start = addr;",
            "\t\tvma->vm_region->vm_pgoff = vma->vm_pgoff += npages;",
            "\t} else {",
            "\t\tvma->vm_region->vm_end = vma->vm_end = addr;",
            "\t\tvma->vm_region->vm_top = addr;",
            "\t}",
            "\tadd_nommu_region(vma->vm_region);",
            "\tadd_nommu_region(new->vm_region);",
            "\tup_write(&nommu_region_sem);",
            "",
            "\tsetup_vma_to_mm(vma, mm);",
            "\tsetup_vma_to_mm(new, mm);",
            "\tvma_iter_store(vmi, new);",
            "\tmm->map_count++;",
            "\treturn 0;",
            "",
            "err_vmi_preallocate:",
            "\tvm_area_free(new);",
            "err_vma_dup:",
            "\tkmem_cache_free(vm_region_jar, region);",
            "\treturn -ENOMEM;",
            "}",
            "static int vmi_shrink_vma(struct vma_iterator *vmi,",
            "\t\t      struct vm_area_struct *vma,",
            "\t\t      unsigned long from, unsigned long to)",
            "{",
            "\tstruct vm_region *region;",
            "",
            "\t/* adjust the VMA's pointers, which may reposition it in the MM's tree",
            "\t * and list */",
            "\tif (from > vma->vm_start) {",
            "\t\tif (vma_iter_clear_gfp(vmi, from, vma->vm_end, GFP_KERNEL))",
            "\t\t\treturn -ENOMEM;",
            "\t\tvma->vm_end = from;",
            "\t} else {",
            "\t\tif (vma_iter_clear_gfp(vmi, vma->vm_start, to, GFP_KERNEL))",
            "\t\t\treturn -ENOMEM;",
            "\t\tvma->vm_start = to;",
            "\t}",
            "",
            "\t/* cut the backing region down to size */",
            "\tregion = vma->vm_region;",
            "\tBUG_ON(region->vm_usage != 1);",
            "",
            "\tdown_write(&nommu_region_sem);",
            "\tdelete_nommu_region(region);",
            "\tif (from > region->vm_start) {",
            "\t\tto = region->vm_top;",
            "\t\tregion->vm_top = region->vm_end = from;",
            "\t} else {",
            "\t\tregion->vm_start = to;",
            "\t}",
            "\tadd_nommu_region(region);",
            "\tup_write(&nommu_region_sem);",
            "",
            "\tfree_page_series(from, to);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ksys_mmap_pgoff, split_vma, vmi_shrink_vma",
          "description": "该代码段实现了非MMU架构下虚拟内存区域（VMA）的管理功能，包含三个关键组件：\n\n1. `ksys_mmap_pgoff`作为内存映射入口，负责根据文件描述符或匿名标志调用`vm_mmap_pgoff`完成地址空间分配，并处理文件引用计数；\n2. `split_vma`用于分割现有匿名VMA区域，通过复制VMA结构并调整起始/终止地址，实现内存区域的拆分操作；\n3. `vmi_shrink_vma`用于缩减VMA范围，通过修改VMA边界及对应的region结构体，配合页面回收完成内存收缩。\n\n注：代码上下文完整，所有实现均基于非MMU架构特有的`vm_region`管理机制。",
          "similarity": 0.6545279026031494
        },
        {
          "chunk_id": 2,
          "file_path": "mm/nommu.c",
          "start_line": 433,
          "end_line": 545,
          "content": [
            "static noinline void validate_nommu_regions(void)",
            "{",
            "\tstruct vm_region *region, *last;",
            "\tstruct rb_node *p, *lastp;",
            "",
            "\tlastp = rb_first(&nommu_region_tree);",
            "\tif (!lastp)",
            "\t\treturn;",
            "",
            "\tlast = rb_entry(lastp, struct vm_region, vm_rb);",
            "\tBUG_ON(last->vm_end <= last->vm_start);",
            "\tBUG_ON(last->vm_top < last->vm_end);",
            "",
            "\twhile ((p = rb_next(lastp))) {",
            "\t\tregion = rb_entry(p, struct vm_region, vm_rb);",
            "\t\tlast = rb_entry(lastp, struct vm_region, vm_rb);",
            "",
            "\t\tBUG_ON(region->vm_end <= region->vm_start);",
            "\t\tBUG_ON(region->vm_top < region->vm_end);",
            "\t\tBUG_ON(region->vm_start < last->vm_top);",
            "",
            "\t\tlastp = p;",
            "\t}",
            "}",
            "static void validate_nommu_regions(void)",
            "{",
            "}",
            "static void add_nommu_region(struct vm_region *region)",
            "{",
            "\tstruct vm_region *pregion;",
            "\tstruct rb_node **p, *parent;",
            "",
            "\tvalidate_nommu_regions();",
            "",
            "\tparent = NULL;",
            "\tp = &nommu_region_tree.rb_node;",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tpregion = rb_entry(parent, struct vm_region, vm_rb);",
            "\t\tif (region->vm_start < pregion->vm_start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (region->vm_start > pregion->vm_start)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse if (pregion == region)",
            "\t\t\treturn;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "",
            "\trb_link_node(&region->vm_rb, parent, p);",
            "\trb_insert_color(&region->vm_rb, &nommu_region_tree);",
            "",
            "\tvalidate_nommu_regions();",
            "}",
            "static void delete_nommu_region(struct vm_region *region)",
            "{",
            "\tBUG_ON(!nommu_region_tree.rb_node);",
            "",
            "\tvalidate_nommu_regions();",
            "\trb_erase(&region->vm_rb, &nommu_region_tree);",
            "\tvalidate_nommu_regions();",
            "}",
            "static void free_page_series(unsigned long from, unsigned long to)",
            "{",
            "\tfor (; from < to; from += PAGE_SIZE) {",
            "\t\tstruct page *page = virt_to_page((void *)from);",
            "",
            "\t\tatomic_long_dec(&mmap_pages_allocated);",
            "\t\tput_page(page);",
            "\t}",
            "}",
            "static void __put_nommu_region(struct vm_region *region)",
            "\t__releases(nommu_region_sem)",
            "{",
            "\tBUG_ON(!nommu_region_tree.rb_node);",
            "",
            "\tif (--region->vm_usage == 0) {",
            "\t\tif (region->vm_top > region->vm_start)",
            "\t\t\tdelete_nommu_region(region);",
            "\t\tup_write(&nommu_region_sem);",
            "",
            "\t\tif (region->vm_file)",
            "\t\t\tfput(region->vm_file);",
            "",
            "\t\t/* IO memory and memory shared directly out of the pagecache",
            "\t\t * from ramfs/tmpfs mustn't be released here */",
            "\t\tif (region->vm_flags & VM_MAPPED_COPY)",
            "\t\t\tfree_page_series(region->vm_start, region->vm_top);",
            "\t\tkmem_cache_free(vm_region_jar, region);",
            "\t} else {",
            "\t\tup_write(&nommu_region_sem);",
            "\t}",
            "}",
            "static void put_nommu_region(struct vm_region *region)",
            "{",
            "\tdown_write(&nommu_region_sem);",
            "\t__put_nommu_region(region);",
            "}",
            "static void setup_vma_to_mm(struct vm_area_struct *vma, struct mm_struct *mm)",
            "{",
            "\tvma->vm_mm = mm;",
            "",
            "\t/* add the VMA to the mapping */",
            "\tif (vma->vm_file) {",
            "\t\tstruct address_space *mapping = vma->vm_file->f_mapping;",
            "",
            "\t\ti_mmap_lock_write(mapping);",
            "\t\tflush_dcache_mmap_lock(mapping);",
            "\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);",
            "\t\tflush_dcache_mmap_unlock(mapping);",
            "\t\ti_mmap_unlock_write(mapping);",
            "\t}",
            "}"
          ],
          "function_name": "validate_nommu_regions, validate_nommu_regions, add_nommu_region, delete_nommu_region, free_page_series, __put_nommu_region, put_nommu_region, setup_vma_to_mm",
          "description": "实现非MMU区域的管理功能，包括添加/删除内存区域至RB树、验证区域顺序、释放页面系列等操作。通过锁机制保护区域树并发访问，并关联VMA到MM结构。",
          "similarity": 0.6531447172164917
        },
        {
          "chunk_id": 8,
          "file_path": "mm/nommu.c",
          "start_line": 1612,
          "end_line": 1742,
          "content": [
            "int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,",
            "\t\tunsigned long pfn, unsigned long size, pgprot_t prot)",
            "{",
            "\tif (addr != (pfn << PAGE_SHIFT))",
            "\t\treturn -EINVAL;",
            "",
            "\tvm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);",
            "\treturn 0;",
            "}",
            "int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)",
            "{",
            "\tunsigned long pfn = start >> PAGE_SHIFT;",
            "\tunsigned long vm_len = vma->vm_end - vma->vm_start;",
            "",
            "\tpfn += vma->vm_pgoff;",
            "\treturn io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);",
            "}",
            "int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,",
            "\t\t\tunsigned long pgoff)",
            "{",
            "\tunsigned int size = vma->vm_end - vma->vm_start;",
            "",
            "\tif (!(vma->vm_flags & VM_USERMAP))",
            "\t\treturn -EINVAL;",
            "",
            "\tvma->vm_start = (unsigned long)(addr + (pgoff << PAGE_SHIFT));",
            "\tvma->vm_end = vma->vm_start + size;",
            "",
            "\treturn 0;",
            "}",
            "vm_fault_t filemap_fault(struct vm_fault *vmf)",
            "{",
            "\tBUG();",
            "\treturn 0;",
            "}",
            "vm_fault_t filemap_map_pages(struct vm_fault *vmf,",
            "\t\tpgoff_t start_pgoff, pgoff_t end_pgoff)",
            "{",
            "\tBUG();",
            "\treturn 0;",
            "}",
            "int __access_remote_vm(struct mm_struct *mm, unsigned long addr, void *buf,",
            "\t\t       int len, unsigned int gup_flags)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tint write = gup_flags & FOLL_WRITE;",
            "",
            "\tif (mmap_read_lock_killable(mm))",
            "\t\treturn 0;",
            "",
            "\t/* the access must start within one of the target process's mappings */",
            "\tvma = find_vma(mm, addr);",
            "\tif (vma) {",
            "\t\t/* don't overrun this mapping */",
            "\t\tif (addr + len >= vma->vm_end)",
            "\t\t\tlen = vma->vm_end - addr;",
            "",
            "\t\t/* only read or write mappings where it is permitted */",
            "\t\tif (write && vma->vm_flags & VM_MAYWRITE)",
            "\t\t\tcopy_to_user_page(vma, NULL, addr,",
            "\t\t\t\t\t (void *) addr, buf, len);",
            "\t\telse if (!write && vma->vm_flags & VM_MAYREAD)",
            "\t\t\tcopy_from_user_page(vma, NULL, addr,",
            "\t\t\t\t\t    buf, (void *) addr, len);",
            "\t\telse",
            "\t\t\tlen = 0;",
            "\t} else {",
            "\t\tlen = 0;",
            "\t}",
            "",
            "\tmmap_read_unlock(mm);",
            "",
            "\treturn len;",
            "}",
            "int access_remote_vm(struct mm_struct *mm, unsigned long addr,",
            "\t\tvoid *buf, int len, unsigned int gup_flags)",
            "{",
            "\treturn __access_remote_vm(mm, addr, buf, len, gup_flags);",
            "}",
            "int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len,",
            "\t\tunsigned int gup_flags)",
            "{",
            "\tstruct mm_struct *mm;",
            "",
            "\tif (addr + len < addr)",
            "\t\treturn 0;",
            "",
            "\tmm = get_task_mm(tsk);",
            "\tif (!mm)",
            "\t\treturn 0;",
            "",
            "\tlen = __access_remote_vm(mm, addr, buf, len, gup_flags);",
            "",
            "\tmmput(mm);",
            "\treturn len;",
            "}",
            "static int __copy_remote_vm_str(struct mm_struct *mm, unsigned long addr,",
            "\t\t\t\tvoid *buf, int len)",
            "{",
            "\tunsigned long addr_end;",
            "\tstruct vm_area_struct *vma;",
            "\tint ret = -EFAULT;",
            "",
            "\t*(char *)buf = '\\0';",
            "",
            "\tif (mmap_read_lock_killable(mm))",
            "\t\treturn ret;",
            "",
            "\t/* the access must start within one of the target process's mappings */",
            "\tvma = find_vma(mm, addr);",
            "\tif (!vma)",
            "\t\tgoto out;",
            "",
            "\tif (check_add_overflow(addr, len, &addr_end))",
            "\t\tgoto out;",
            "",
            "\t/* don't overrun this mapping */",
            "\tif (addr_end > vma->vm_end)",
            "\t\tlen = vma->vm_end - addr;",
            "",
            "\t/* only read mappings where it is permitted */",
            "\tif (vma->vm_flags & VM_MAYREAD) {",
            "\t\tret = strscpy(buf, (char *)addr, len);",
            "\t\tif (ret < 0)",
            "\t\t\tret = len - 1;",
            "\t}",
            "",
            "out:",
            "\tmmap_read_unlock(mm);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "remap_pfn_range, vm_iomap_memory, remap_vmalloc_range, filemap_fault, filemap_map_pages, __access_remote_vm, access_remote_vm, access_process_vm, __copy_remote_vm_str",
          "description": "该代码块主要处理非MMU环境下的物理地址到虚拟地址的映射及远程内存访问控制。  \n`remap_pfn_range`/`vm_iomap_memory`/`remap_vmalloc_range`实现物理页框或虚拟内存区域的映射配置，`__access_remote_vm`系列函数提供对目标进程虚拟内存的受限读写访问。  \n部分函数依赖未展示的辅助实现（如`io_remap_pfn_range`），且`filemap_fault`等函数通过`BUG()`表明仅在特定条件下触发。",
          "similarity": 0.6508268117904663
        }
      ]
    },
    {
      "source_file": "mm/mmu_gather.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:52:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mmu_gather.c`\n\n---\n\n# mmu_gather.c 技术文档\n\n## 1. 文件概述\n\n`mmu_gather.c` 是 Linux 内核内存管理子系统中的关键组件，负责在页表项（PTE）或更高层级页表被撤销映射（unmap）后，高效地批量释放对应的物理页面和页表结构。该文件实现了 **MMU gather** 机制，用于延迟并批量处理 TLB（Translation Lookaside Buffer）刷新、反向映射（rmap）清理以及页面回收操作，以减少频繁的 TLB 刷新开销和锁竞争，提升性能。\n\n当内核需要释放大量虚拟内存区域（如进程退出、mmap 区域销毁）时，不会立即释放每个页面，而是先将待释放的页面收集到 `mmu_gather` 结构中，待累积到一定数量或显式调用 flush 操作时，再统一执行 TLB 刷新、rmap 解除和页面释放。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `tlb_next_batch(struct mmu_gather *tlb)`  \n  分配新的批处理批次（batch），用于扩展可收集的页面数量上限。\n\n- `tlb_flush_rmaps(struct mmu_gather *tlb, struct vm_area_struct *vma)`  \n  （仅在 SMP 下）处理延迟的反向映射（delayed rmap）移除操作，在 TLB 刷新后调用。\n\n- `__tlb_batch_free_encoded_pages(struct mmu_gather_batch *batch)`  \n  批量释放编码后的页面（包括普通页面和 swap 缓存），支持防软锁定（soft lockup）的调度点。\n\n- `tlb_batch_pages_flush(struct mmu_gather *tlb)`  \n  遍历所有批次，释放其中收集的所有页面。\n\n- `tlb_batch_list_free(struct mmu_gather *tlb)`  \n  释放动态分配的批次内存（非本地批次）。\n\n- `__tlb_remove_folio_pages_size(...)` / `__tlb_remove_folio_pages(...)` / `__tlb_remove_page_size(...)`  \n  将页面（单页或多页 folio）加入当前 gather 批次，支持延迟 rmap 和不同页面大小。\n\n- `tlb_remove_table_sync_one(void)`  \n  （RCU 表释放模式下）触发 IPI 同步，确保软件页表遍历安全。\n\n- `tlb_remove_table_rcu(struct rcu_head *head)`  \n  RCU 回调函数，用于异步释放页表结构。\n\n- `tlb_remove_table_free(struct mmu_table_batch *batch)`  \n  将页表批次提交给 RCU 机制进行延迟释放。\n\n### 关键数据结构\n\n- `struct mmu_gather`  \n  核心上下文结构，包含本地批次（`local`）、当前活跃批次（`active`）、批次计数、延迟 rmap 标志等。\n\n- `struct mmu_gather_batch`  \n  页面批次结构，包含指向编码页面指针数组、当前数量（`nr`）、最大容量（`max`）及下一个批次指针。\n\n- `struct mmu_table_batch`  \n  页表结构批次，用于批量收集待释放的页表（如 PMD、PUD 等）。\n\n- `encoded_page` 相关机制  \n  使用指针低位编码额外信息（如是否延迟 rmap、是否后跟 nr_pages 字段），节省内存并提高缓存效率。\n\n## 3. 关键实现\n\n### 批处理与动态扩展\n- 默认使用栈上或局部存储的 `local` 批次（避免内存分配）。\n- 当 `local` 批次满时，通过 `__get_free_page()` 动态分配新批次（最多 `MAX_GATHER_BATCH_COUNT` 个）。\n- `tlb_next_batch()` 在存在延迟 rmap 时限制扩展，确保语义正确性。\n\n### 延迟反向映射（Delayed Rmap）\n- 当页面仍被其他 VMA 引用但当前 VMA 正在 unmap 时，不立即调用 `folio_remove_rmap_ptes()`，而是标记 `ENCODED_PAGE_BIT_DELAY_RMAP`。\n- 在 `tlb_flush_rmaps()` 中统一处理，确保在 TLB 刷新**之后**才解除 rmap，防止 CPU 访问已释放页面。\n\n### 安全释放与防软锁定\n- 页面释放循环中每处理最多 `MAX_NR_FOLIOS_PER_FREE`（512）个 folio 调用 `cond_resched()`，避免在非抢占内核中长时间占用 CPU。\n- 若启用 `page_poisoning` 或 `init_on_free`，则按实际内存大小（而非 folio 数量）限制单次释放量，因初始化开销与内存大小成正比。\n\n### 页表结构的安全释放（RCU 模式）\n- 在支持软件页表遍历（如 `gup_fast`）的架构上，页表释放需与遍历操作同步。\n- 使用 `call_rcu()` 延迟释放页表，配合 `smp_call_function()` 触发 IPI 确保所有 CPU 完成 TLB 刷新后再释放内存。\n- 若 RCU 批次分配失败，则回退到即时释放（代码未完整展示，但注释提及）。\n\n### 编码页面指针\n- 利用页面指针对齐特性（通常低 2~3 位为 0），将标志位（如 `DELAY_RMAP`、`NR_PAGES_NEXT`）存储在指针低位。\n- 支持多页 folio：若 `nr_pages > 1`，则连续两个条目分别存储页面指针（带标志）和页数。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm_types.h>`、`<linux/mm_inline.h>`、`<linux/rmap.h>` 等，与 folio、page、VMA 管理紧密集成。\n- **TLB 管理**：通过 `<asm/tlb.h>` 与架构相关 TLB 刷新接口交互。\n- **RCU 机制**：在 `CONFIG_MMU_GATHER_RCU_TABLE_FREE` 下依赖 `<linux/rcupdate.h>` 实现页表安全释放。\n- **SMP 支持**：`tlb_flush_rmaps` 和页表同步仅在 `CONFIG_SMP` 下编译。\n- **高阶内存与交换**：使用 `<linux/highmem.h>`、`<linux/swap.h>` 处理高端内存和 swap 缓存释放。\n- **内存分配器**：通过 `__get_free_page(GFP_NOWAIT)` 动态分配批次内存。\n\n## 5. 使用场景\n\n- **进程退出（exit_mmap）**：释放整个地址空间时，大量页面通过 mmu_gather 批量回收。\n- **munmap 系统调用**：解除大块内存映射时，避免逐页 TLB 刷新。\n- **内存回收（reclaim）**：在直接回收或 kswapd 中撤销映射时使用。\n- **透明大页（THP）拆分**：拆分大页时需撤销多个 PTE 映射并释放 sub-page。\n- **页表收缩（shrink_page_list）**：在页面回收路径中解除映射。\n- **KSM（Kernel Samepage Merging）**：合并或取消合并页面时更新 rmap。\n- **页表层级释放**：当上层页表（如 PGD/P4D/PUD/PMD）不再被引用时，通过 `tlb_remove_table` 机制安全释放。",
      "similarity": 0.6397745609283447,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/mmu_gather.c",
          "start_line": 144,
          "end_line": 244,
          "content": [
            "static void tlb_batch_pages_flush(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\tfor (batch = &tlb->local; batch && batch->nr; batch = batch->next)",
            "\t\t__tlb_batch_free_encoded_pages(batch);",
            "\ttlb->active = &tlb->local;",
            "}",
            "static void tlb_batch_list_free(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch, *next;",
            "",
            "\tfor (batch = tlb->local.next; batch; batch = next) {",
            "\t\tnext = batch->next;",
            "\t\tfree_pages((unsigned long)batch, 0);",
            "\t}",
            "\ttlb->local.next = NULL;",
            "}",
            "static bool __tlb_remove_folio_pages_size(struct mmu_gather *tlb,",
            "\t\tstruct page *page, unsigned int nr_pages, bool delay_rmap,",
            "\t\tint page_size)",
            "{",
            "\tint flags = delay_rmap ? ENCODED_PAGE_BIT_DELAY_RMAP : 0;",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\tVM_BUG_ON(!tlb->end);",
            "",
            "#ifdef CONFIG_MMU_GATHER_PAGE_SIZE",
            "\tVM_WARN_ON(tlb->page_size != page_size);",
            "\tVM_WARN_ON_ONCE(nr_pages != 1 && page_size != PAGE_SIZE);",
            "\tVM_WARN_ON_ONCE(page_folio(page) != page_folio(page + nr_pages - 1));",
            "#endif",
            "",
            "\tbatch = tlb->active;",
            "\t/*",
            "\t * Add the page and check if we are full. If so",
            "\t * force a flush.",
            "\t */",
            "\tif (likely(nr_pages == 1)) {",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_page(page, flags);",
            "\t} else {",
            "\t\tflags |= ENCODED_PAGE_BIT_NR_PAGES_NEXT;",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_page(page, flags);",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_nr_pages(nr_pages);",
            "\t}",
            "\t/*",
            "\t * Make sure that we can always add another \"page\" + \"nr_pages\",",
            "\t * requiring two entries instead of only a single one.",
            "\t */",
            "\tif (batch->nr >= batch->max - 1) {",
            "\t\tif (!tlb_next_batch(tlb))",
            "\t\t\treturn true;",
            "\t\tbatch = tlb->active;",
            "\t}",
            "\tVM_BUG_ON_PAGE(batch->nr > batch->max - 1, page);",
            "",
            "\treturn false;",
            "}",
            "bool __tlb_remove_folio_pages(struct mmu_gather *tlb, struct page *page,",
            "\t\tunsigned int nr_pages, bool delay_rmap)",
            "{",
            "\treturn __tlb_remove_folio_pages_size(tlb, page, nr_pages, delay_rmap,",
            "\t\t\t\t\t     PAGE_SIZE);",
            "}",
            "bool __tlb_remove_page_size(struct mmu_gather *tlb, struct page *page,",
            "\t\tbool delay_rmap, int page_size)",
            "{",
            "\treturn __tlb_remove_folio_pages_size(tlb, page, 1, delay_rmap, page_size);",
            "}",
            "static void __tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < batch->nr; i++)",
            "\t\t__tlb_remove_table(batch->tables[i]);",
            "",
            "\tfree_page((unsigned long)batch);",
            "}",
            "static void tlb_remove_table_smp_sync(void *arg)",
            "{",
            "\t/* Simply deliver the interrupt */",
            "}",
            "void tlb_remove_table_sync_one(void)",
            "{",
            "\t/*",
            "\t * This isn't an RCU grace period and hence the page-tables cannot be",
            "\t * assumed to be actually RCU-freed.",
            "\t *",
            "\t * It is however sufficient for software page-table walkers that rely on",
            "\t * IRQ disabling.",
            "\t */",
            "\tsmp_call_function(tlb_remove_table_smp_sync, NULL, 1);",
            "}",
            "static void tlb_remove_table_rcu(struct rcu_head *head)",
            "{",
            "\t__tlb_remove_table_free(container_of(head, struct mmu_table_batch, rcu));",
            "}",
            "static void tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\tcall_rcu(&batch->rcu, tlb_remove_table_rcu);",
            "}"
          ],
          "function_name": "tlb_batch_pages_flush, tlb_batch_list_free, __tlb_remove_folio_pages_size, __tlb_remove_folio_pages, __tlb_remove_page_size, __tlb_remove_table_free, tlb_remove_table_smp_sync, tlb_remove_table_sync_one, tlb_remove_table_rcu, tlb_remove_table_free",
          "description": "实现页表条目批量移除和内存表管理，包含多页面处理、NR_PAGES_NEXT 标记解析及 RCU 安全释放",
          "similarity": 0.6447904706001282
        },
        {
          "chunk_id": 0,
          "file_path": "mm/mmu_gather.c",
          "start_line": 1,
          "end_line": 17,
          "content": [
            "#include <linux/gfp.h>",
            "#include <linux/highmem.h>",
            "#include <linux/kernel.h>",
            "#include <linux/mmdebug.h>",
            "#include <linux/mm_types.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/smp.h>",
            "#include <linux/swap.h>",
            "#include <linux/rmap.h>",
            "",
            "#include <asm/pgalloc.h>",
            "#include <asm/tlb.h>",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            ""
          ],
          "function_name": null,
          "description": "声明 MMU 聚合功能所需头文件，根据配置条件包含架构相关实现",
          "similarity": 0.5886344909667969
        },
        {
          "chunk_id": 3,
          "file_path": "mm/mmu_gather.c",
          "start_line": 292,
          "end_line": 424,
          "content": [
            "static void tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\t__tlb_remove_table_free(batch);",
            "}",
            "static inline void tlb_table_invalidate(struct mmu_gather *tlb)",
            "{",
            "\tif (tlb_needs_table_invalidate()) {",
            "\t\t/*",
            "\t\t * Invalidate page-table caches used by hardware walkers. Then",
            "\t\t * we still need to RCU-sched wait while freeing the pages",
            "\t\t * because software walkers can still be in-flight.",
            "\t\t */",
            "\t\ttlb_flush_mmu_tlbonly(tlb);",
            "\t}",
            "}",
            "static void tlb_remove_table_one(void *table)",
            "{",
            "\ttlb_remove_table_sync_one();",
            "\t__tlb_remove_table(table);",
            "}",
            "static void tlb_table_flush(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_table_batch **batch = &tlb->batch;",
            "",
            "\tif (*batch) {",
            "\t\ttlb_table_invalidate(tlb);",
            "\t\ttlb_remove_table_free(*batch);",
            "\t\t*batch = NULL;",
            "\t}",
            "}",
            "void tlb_remove_table(struct mmu_gather *tlb, void *table)",
            "{",
            "\tstruct mmu_table_batch **batch = &tlb->batch;",
            "",
            "\tif (*batch == NULL) {",
            "\t\t*batch = (struct mmu_table_batch *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);",
            "\t\tif (*batch == NULL) {",
            "\t\t\ttlb_table_invalidate(tlb);",
            "\t\t\ttlb_remove_table_one(table);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\t(*batch)->nr = 0;",
            "\t}",
            "",
            "\t(*batch)->tables[(*batch)->nr++] = table;",
            "\tif ((*batch)->nr == MAX_TABLE_BATCH)",
            "\t\ttlb_table_flush(tlb);",
            "}",
            "static inline void tlb_table_init(struct mmu_gather *tlb)",
            "{",
            "\ttlb->batch = NULL;",
            "}",
            "static inline void tlb_table_flush(struct mmu_gather *tlb) { }",
            "static inline void tlb_table_init(struct mmu_gather *tlb) { }",
            "static void tlb_flush_mmu_free(struct mmu_gather *tlb)",
            "{",
            "\ttlb_table_flush(tlb);",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb_batch_pages_flush(tlb);",
            "#endif",
            "}",
            "void tlb_flush_mmu(struct mmu_gather *tlb)",
            "{",
            "\ttlb_flush_mmu_tlbonly(tlb);",
            "\ttlb_flush_mmu_free(tlb);",
            "}",
            "static void __tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,",
            "\t\t\t     bool fullmm)",
            "{",
            "\ttlb->mm = mm;",
            "\ttlb->fullmm = fullmm;",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb->need_flush_all = 0;",
            "\ttlb->local.next = NULL;",
            "\ttlb->local.nr   = 0;",
            "\ttlb->local.max  = ARRAY_SIZE(tlb->__pages);",
            "\ttlb->active     = &tlb->local;",
            "\ttlb->batch_count = 0;",
            "#endif",
            "\ttlb->delayed_rmap = 0;",
            "",
            "\ttlb_table_init(tlb);",
            "#ifdef CONFIG_MMU_GATHER_PAGE_SIZE",
            "\ttlb->page_size = 0;",
            "#endif",
            "",
            "\t__tlb_reset_range(tlb);",
            "\tinc_tlb_flush_pending(tlb->mm);",
            "}",
            "void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm)",
            "{",
            "\t__tlb_gather_mmu(tlb, mm, false);",
            "}",
            "void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm)",
            "{",
            "\t__tlb_gather_mmu(tlb, mm, true);",
            "}",
            "void tlb_finish_mmu(struct mmu_gather *tlb)",
            "{",
            "\t/*",
            "\t * If there are parallel threads are doing PTE changes on same range",
            "\t * under non-exclusive lock (e.g., mmap_lock read-side) but defer TLB",
            "\t * flush by batching, one thread may end up seeing inconsistent PTEs",
            "\t * and result in having stale TLB entries.  So flush TLB forcefully",
            "\t * if we detect parallel PTE batching threads.",
            "\t *",
            "\t * However, some syscalls, e.g. munmap(), may free page tables, this",
            "\t * needs force flush everything in the given range. Otherwise this",
            "\t * may result in having stale TLB entries for some architectures,",
            "\t * e.g. aarch64, that could specify flush what level TLB.",
            "\t */",
            "\tif (mm_tlb_flush_nested(tlb->mm)) {",
            "\t\t/*",
            "\t\t * The aarch64 yields better performance with fullmm by",
            "\t\t * avoiding multiple CPUs spamming TLBI messages at the",
            "\t\t * same time.",
            "\t\t *",
            "\t\t * On x86 non-fullmm doesn't yield significant difference",
            "\t\t * against fullmm.",
            "\t\t */",
            "\t\ttlb->fullmm = 1;",
            "\t\t__tlb_reset_range(tlb);",
            "\t\ttlb->freed_tables = 1;",
            "\t}",
            "",
            "\ttlb_flush_mmu(tlb);",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb_batch_list_free(tlb);",
            "#endif",
            "\tdec_tlb_flush_pending(tlb->mm);",
            "}"
          ],
          "function_name": "tlb_remove_table_free, tlb_table_invalidate, tlb_remove_table_one, tlb_table_flush, tlb_remove_table, tlb_table_init, tlb_table_flush, tlb_table_init, tlb_flush_mmu_free, tlb_flush_mmu, __tlb_gather_mmu, tlb_gather_mmu, tlb_gather_mmu_fullmm, tlb_finish_mmu",
          "description": "提供 TLB 无效化、页表批量释放及 MMU 收集器初始化/终止接口，包含跨架构的 TLB 同步机制",
          "similarity": 0.5790098905563354
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mmu_gather.c",
          "start_line": 18,
          "end_line": 120,
          "content": [
            "static bool tlb_next_batch(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\t/* Limit batching if we have delayed rmaps pending */",
            "\tif (tlb->delayed_rmap && tlb->active != &tlb->local)",
            "\t\treturn false;",
            "",
            "\tbatch = tlb->active;",
            "\tif (batch->next) {",
            "\t\ttlb->active = batch->next;",
            "\t\treturn true;",
            "\t}",
            "",
            "\tif (tlb->batch_count == MAX_GATHER_BATCH_COUNT)",
            "\t\treturn false;",
            "",
            "\tbatch = (void *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);",
            "\tif (!batch)",
            "\t\treturn false;",
            "",
            "\ttlb->batch_count++;",
            "\tbatch->next = NULL;",
            "\tbatch->nr   = 0;",
            "\tbatch->max  = MAX_GATHER_BATCH;",
            "",
            "\ttlb->active->next = batch;",
            "\ttlb->active = batch;",
            "",
            "\treturn true;",
            "}",
            "static void tlb_flush_rmap_batch(struct mmu_gather_batch *batch, struct vm_area_struct *vma)",
            "{",
            "\tstruct encoded_page **pages = batch->encoded_pages;",
            "",
            "\tfor (int i = 0; i < batch->nr; i++) {",
            "\t\tstruct encoded_page *enc = pages[i];",
            "",
            "\t\tif (encoded_page_flags(enc) & ENCODED_PAGE_BIT_DELAY_RMAP) {",
            "\t\t\tstruct page *page = encoded_page_ptr(enc);",
            "\t\t\tunsigned int nr_pages = 1;",
            "",
            "\t\t\tif (unlikely(encoded_page_flags(enc) &",
            "\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\tnr_pages = encoded_nr_pages(pages[++i]);",
            "",
            "\t\t\tfolio_remove_rmap_ptes(page_folio(page), page, nr_pages,",
            "\t\t\t\t\t       vma);",
            "\t\t}",
            "\t}",
            "}",
            "void tlb_flush_rmaps(struct mmu_gather *tlb, struct vm_area_struct *vma)",
            "{",
            "\tif (!tlb->delayed_rmap)",
            "\t\treturn;",
            "",
            "\ttlb_flush_rmap_batch(&tlb->local, vma);",
            "\tif (tlb->active != &tlb->local)",
            "\t\ttlb_flush_rmap_batch(tlb->active, vma);",
            "\ttlb->delayed_rmap = 0;",
            "}",
            "static void __tlb_batch_free_encoded_pages(struct mmu_gather_batch *batch)",
            "{",
            "\tstruct encoded_page **pages = batch->encoded_pages;",
            "\tunsigned int nr, nr_pages;",
            "",
            "\twhile (batch->nr) {",
            "\t\tif (!page_poisoning_enabled_static() && !want_init_on_free()) {",
            "\t\t\tnr = min(MAX_NR_FOLIOS_PER_FREE, batch->nr);",
            "",
            "\t\t\t/*",
            "\t\t\t * Make sure we cover page + nr_pages, and don't leave",
            "\t\t\t * nr_pages behind when capping the number of entries.",
            "\t\t\t */",
            "\t\t\tif (unlikely(encoded_page_flags(pages[nr - 1]) &",
            "\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\tnr++;",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * With page poisoning and init_on_free, the time it",
            "\t\t\t * takes to free memory grows proportionally with the",
            "\t\t\t * actual memory size. Therefore, limit based on the",
            "\t\t\t * actual memory size and not the number of involved",
            "\t\t\t * folios.",
            "\t\t\t */",
            "\t\t\tfor (nr = 0, nr_pages = 0;",
            "\t\t\t     nr < batch->nr && nr_pages < MAX_NR_FOLIOS_PER_FREE;",
            "\t\t\t     nr++) {",
            "\t\t\t\tif (unlikely(encoded_page_flags(pages[nr]) &",
            "\t\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\t\tnr_pages += encoded_nr_pages(pages[++nr]);",
            "\t\t\t\telse",
            "\t\t\t\t\tnr_pages++;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tfree_pages_and_swap_cache(pages, nr);",
            "\t\tpages += nr;",
            "\t\tbatch->nr -= nr;",
            "",
            "\t\tcond_resched();",
            "\t}",
            "}"
          ],
          "function_name": "tlb_next_batch, tlb_flush_rmap_batch, tlb_flush_rmaps, __tlb_batch_free_encoded_pages",
          "description": "管理 TLB 批量操作的延迟 RMAP 处理逻辑，包括批次链表管理、编码页面释放及 RMAP 标志清除",
          "similarity": 0.45520949363708496
        }
      ]
    }
  ]
}