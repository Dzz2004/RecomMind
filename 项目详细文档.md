# RAG工作流系统 - 详细项目文档

## 项目概述

这是一个完整的**检索增强生成（RAG）工作流系统**，集成了Qwen3-8B大语言模型和BGE-M3嵌入模型，提供基于文档内容的智能问答功能。系统采用前后端分离架构，包含Python后端API服务器和Vue.js前端界面。

## 系统架构

```
┌─────────────────┐    HTTP API    ┌─────────────────┐    RAG工作流    ┌─────────────────┐
│   Vue.js前端     │ ←─────────────→ │  Flask后端API   │ ←─────────────→ │  RAG工作流引擎  │
│   (TypeScript)  │                │   (Python)      │                │   (Python)      │
└─────────────────┘                └─────────────────┘                └─────────────────┘
         │                                   │                                   │
         │                                   │                                   │
         ▼                                   ▼                                   ▼
┌─────────────────┐                ┌─────────────────┐                ┌─────────────────┐
│   Element Plus  │                │   CORS支持      │                │  Qwen3-8B模型   │
│   UI组件库      │                │   错误处理      │                │  BGE-M3嵌入模型  │
└─────────────────┘                └─────────────────┘                │  ChromaDB数据库 │
                                                                     └─────────────────┘
```

## 核心功能

### 1. RAG检索增强生成
- **智能检索建议生成**：基于用户查询和对话历史，生成优化的检索查询
- **向量相似度检索**：使用BGE-M3嵌入模型进行语义检索
- **多轮对话管理**：维护对话上下文，支持连续问答
- **相似度阈值过滤**：动态调整检索结果的精确度

### 2. 大语言模型集成
- **Qwen3-8B模型**：支持中文对话和文本生成
- **4位量化**：使用BitsAndBytesConfig进行模型量化，减少内存占用
- **GPU加速**：支持CUDA加速（如果可用）
- **对话模板**：使用tokenizer的chat template格式化输入

### 3. 向量数据库
- **ChromaDB存储**：持久化向量数据库
- **文档分块**：支持PDF文档的智能分块
- **元数据管理**：存储文档来源、页码、章节等信息
- **相似度计算**：基于余弦相似度的检索排序

### 4. 智能判题系统
- **选择题判题**：基于RAG检索的智能选择题判题
- **填空题判题**：支持填空题和问答题的智能判题
- **解析生成**：自动生成详细的题目解析
- **置信度评估**：提供判题结果的置信度评分

## 项目结构

```
workflow_wxk/
├── 📁 核心文件
│   ├── qwen3_local.py              # 主演示脚本
│   ├── simple_rag_workflow.py      # RAG工作流核心实现
│   ├── backend_server.py           # Flask后端API服务器
│   └── requirements.txt             # Python依赖包
│
├── 📁 启动脚本
│   ├── start_backend.sh            # 后端启动脚本
│   └── start_frontend.sh           # 前端启动脚本
│
├── 📁 测试文件
│   ├── test_connection.py          # 前后端连接测试
│   └── test_threshold.py           # 相似度阈值测试
│
├── 📁 前端应用 (prototype/)
│   ├── package.json                # 前端依赖配置
│   ├── vite.config.ts              # Vite构建配置
│   ├── src/
│   │   ├── api/                    # API接口封装
│   │   ├── components/              # Vue组件
│   │   ├── views/                   # 页面视图
│   │   ├── stores/                  # Pinia状态管理
│   │   ├── router/                  # 路由配置
│   │   └── types/                   # TypeScript类型定义
│   └── public/                      # 静态资源
│
├── 📁 数据存储
│   └── vector_db/                  # ChromaDB向量数据库
│
└── 📁 文档
    ├── README.md                   # 项目说明
    └── 项目详细文档.md              # 本文档
```

## 技术栈

### 后端技术
- **Python 3.8+**：主要开发语言
- **Flask 2.3.3**：Web框架
- **Transformers 4.30+**：Hugging Face模型库
- **Sentence-Transformers 2.2+**：嵌入模型
- **ChromaDB 0.4+**：向量数据库
- **PyTorch 2.0+**：深度学习框架
- **BitsAndBytesConfig**：模型量化

### 前端技术
- **Vue.js 3.5+**：前端框架
- **TypeScript 5.8+**：类型安全的JavaScript
- **Vite 4.5+**：构建工具
- **Element Plus 2.11+**：UI组件库
- **Pinia 3.0+**：状态管理
- **Axios 1.11+**：HTTP客户端
- **Vue Router 4.5+**：路由管理

### 模型和算法
- **Qwen3-8B**：阿里云通义千问大语言模型
- **BGE-M3**：北京智源BGE多语言嵌入模型
- **余弦相似度**：向量相似度计算
- **4位量化**：模型压缩技术

## 核心组件详解

### 1. SimpleRAGWorkflow类
```python
class SimpleRAGWorkflow:
    """简化的RAG工作流，不依赖langchain"""
    
    def __init__(self, llm_path, embedding_model_path, db_path, similarity_threshold):
        # 初始化对话管理器、RAG引擎、LLM和检索建议生成器
        
    def process_user_query(self, user_query):
        # 完整的RAG工作流程：
        # 1. 生成检索建议
        # 2. 执行向量检索
        # 3. 生成最终回答
```

### 2. RetrievalSuggester类
```python
class RetrievalSuggester:
    """检索建议生成器"""
    
    def generate_suggestion(self, user_query, conversation_history):
        # 基于用户查询和对话历史生成优化的检索建议
        # 包括意图识别、关键词提取、查询重写等
```

### 3. SimpleRAGEngine类
```python
class SimpleRAGEngine:
    """简化的RAG引擎，不依赖langchain"""
    
    def query(self, query_text, top_k=5):
        # 执行向量检索，返回相似度排序的结果
        # 支持相似度阈值过滤
```

### 4. ConversationManager类
```python
class ConversationManager:
    """对话历史管理器"""
    
    def add_message(self, role, content, metadata):
        # 添加对话消息，维护对话上下文
        # 支持历史记录长度限制
```

## API接口文档

### 聊天接口
- **URL**: `POST /api/chat`
- **功能**: 处理用户聊天请求，支持RAG和非RAG模式
- **请求体**:
  ```json
  {
    "userInput": "用户问题",
    "useRag": true
  }
  ```
- **响应**:
  ```json
  {
    "code": 200,
    "message": "聊天处理成功",
    "data": {
      "thought": "AI思考过程",
      "answer": "AI回答",
      "documents": [
        {
          "source": "ch1.pdf",
          "page": 5,
          "content": "文档内容",
          "chapter": 1,
          "finalPage": 15
        }
      ]
    }
  }
  ```

### 智能判题接口
- **URL**: `POST /api/question/judge`
- **功能**: 基于RAG检索的智能判题（支持选择题、填空题、问答题）
- **请求体**:
  ```json
  {
    "questionContent": "题目内容",
    "studentAnswer": "学生答案",
    "questionType": "选择题",
    "questionOptions": ["选项A", "选项B", "选项C", "选项D"],
    "correctAnswer": "A",
    "knowledgePoint": "相关知识点"
  }
  ```

### 其他接口
- `POST /api/conversation/clear` - 清空对话历史
- `GET /api/conversation/summary` - 获取对话摘要
- `POST /api/config/similarity-threshold` - 更新相似度阈值
- `GET /api/rag/info` - 获取RAG系统信息
- `GET /api/health` - 健康检查

## 安装和部署

### 环境要求
- **Python 3.8+**
- **Node.js 18+**（推荐18.20+，与Vite 4.x兼容）
- **CUDA**（可选，用于GPU加速）
- **内存**: 至少16GB RAM（推荐32GB）
- **存储**: 至少20GB可用空间

### 后端部署
```bash
# 1. 克隆项目
cd /home/ubuntu/qj_temp/workflow_wxk

# 2. 安装Python依赖
pip3 install -r requirements.txt

# 3. 配置模型路径（在backend_server.py中修改）
config = {
    "llm_path": "你的Qwen3模型路径",
    "embedding_model_path": "你的BGE-M3模型路径",
    "db_path": "./vector_db",
    "similarity_threshold": 0.3
}

# 4. 启动后端服务器
python3 backend_server.py
# 或使用启动脚本
chmod +x start_backend.sh
./start_backend.sh
```

### 前端部署
```bash
# 1. 进入前端目录
cd prototype

# 2. 安装依赖
npm install

# 3. 启动开发服务器
npm run dev
# 或使用启动脚本
chmod +x start_frontend.sh
./start_frontend.sh
```

### 模型下载
```bash
# 下载Qwen3-8B模型
# 模型路径: ../models--Qwen--Qwen3-8B/snapshots/9c925d64d72725edaf899c6cb9c377fd0709d9c5

# 下载BGE-M3嵌入模型
# 模型路径: /home/ubuntu/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/
```

## 使用指南

### 1. 基本使用
```python
# 初始化RAG工作流
from simple_rag_workflow import SimpleRAGWorkflow

config = {
    "llm_path": "模型路径",
    "embedding_model_path": "嵌入模型路径",
    "db_path": "./vector_db",
    "similarity_threshold": 0.3
}

workflow = SimpleRAGWorkflow(**config)

# 处理用户查询
response = workflow.process_user_query("什么是机器学习？")
print(response.llm_response)
```

### 2. 命令行使用
```bash
# 运行主演示脚本
python3 qwen3_local.py

# 运行简单对话演示（无RAG）
python3 qwen3_local.py --simple

# 测试相似度阈值
python3 test_threshold.py

# 测试前后端连接
python3 test_connection.py
```

### 3. Web界面使用
1. 启动后端服务器：`http://localhost:5000`
2. 启动前端应用：`http://localhost:5173`
3. 在浏览器中访问前端界面
4. 输入问题，选择是否启用RAG模式
5. 查看AI回答和检索到的文档片段

## 配置说明

### 模型配置
```python
# 在backend_server.py中修改
config = {
    "llm_path": "../models--Qwen--Qwen3-8B/snapshots/9c925d64d72725edaf899c6cb9c377fd0709d9c5",
    "embedding_model_path": "/home/ubuntu/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/",
    "db_path": "./vector_db",
    "similarity_threshold": 0.3  # 相似度阈值，0.0-1.0
}
```

### 前端API配置
```typescript
// 在prototype/src/api/chatbot.ts中修改
const API_CONFIG = {
  baseURL: 'http://localhost:5000/api',
  timeout: 30000
}
```

### Vite代理配置
```typescript
// 在prototype/vite.config.ts中修改
export default defineConfig({
  server: {
    proxy: {
      '/api': {
        target: 'http://localhost:5000',
        changeOrigin: true,
        secure: false
      }
    }
  }
})
```

## 性能优化

### 1. 模型优化
- **量化配置**：使用4位量化减少内存占用
- **GPU加速**：启用CUDA加速（如果可用）
- **批处理**：调整批处理大小优化性能

### 2. 检索优化
- **相似度阈值**：根据需求调整阈值（0.1-0.9）
- **文档分块**：优化文档分块策略
- **缓存策略**：实现查询结果缓存

### 3. 系统优化
- **内存管理**：定期清理对话历史
- **并发处理**：使用Flask的threaded模式
- **错误处理**：完善的异常处理和降级机制

## 故障排除

### 常见问题

1. **模型加载失败**
   - 检查模型路径是否正确
   - 确保有足够的内存/显存
   - 检查CUDA环境（如果使用GPU）

2. **前端启动失败**
   - 确保使用 `npm run dev` 而不是 `npm start`
   - 检查Node.js版本是否 >= 18
   - 删除 `node_modules` 重新安装

3. **向量数据库错误**
   - 确保 `./vector_db` 目录存在且有写权限
   - 检查ChromaDB版本兼容性

4. **前后端连接失败**
   - 检查后端服务器是否在 `http://localhost:5000` 启动
   - 确认Vite代理配置正确
   - 检查浏览器控制台是否有CORS错误

### 调试工具
```bash
# 测试后端健康状态
curl http://localhost:5000/api/health

# 测试聊天API
curl -X POST http://localhost:5000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"userInput":"你好","useRag":false}'

# 运行连接测试
python3 test_connection.py
```

## 扩展开发

### 后端扩展
1. **添加新的API路由**：在 `backend_server.py` 中添加新的路由
2. **扩展RAG功能**：在 `simple_rag_workflow.py` 中添加新功能
3. **修改数据模型**：在数据类中添加新字段

### 前端扩展
1. **添加新的API调用**：在 `prototype/src/api/chatbot.ts` 中添加新接口
2. **定义新的数据类型**：在 `prototype/src/types/index.ts` 中添加类型定义
3. **集成新功能**：在前端组件中集成新功能

### 模型扩展
1. **支持更多模型**：修改模型加载逻辑支持其他LLM
2. **多模态支持**：添加图像、音频等多模态处理
3. **自定义嵌入模型**：支持其他嵌入模型

## 项目特色

### 1. 无依赖RAG实现
- 不依赖langchain等重型框架
- 基于Transformers直接实现RAG功能
- 代码简洁，易于理解和修改

### 2. 智能检索建议
- 基于对话历史的检索优化
- 意图识别和关键词提取
- 查询重写和语义扩展

### 3. 完整的判题系统
- 支持多种题型（选择题、填空题、问答题）
- 基于RAG检索的智能判题
- 自动生成详细解析

### 4. 前后端分离架构
- RESTful API设计
- Vue.js现代化前端
- 支持跨域请求和代理

### 5. 完善的测试和调试
- 连接测试脚本
- 阈值测试工具
- 详细的日志和错误处理

## 许可证

本项目采用MIT许可证，允许自由使用、修改和分发。

## 贡献指南

欢迎贡献代码、报告问题或提出改进建议。请遵循以下步骤：

1. Fork项目
2. 创建功能分支
3. 提交更改
4. 推送到分支
5. 创建Pull Request

## 联系方式

如有问题或建议，请通过以下方式联系：
- 提交Issue
- 发送邮件
- 创建Pull Request

---

**注意**: 本文档会随着项目的发展持续更新，请关注最新版本。
