#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆç®€åŒ–RAGå·¥ä½œæµçš„Qwen3æ¨¡å‹æ¼”ç¤ºè„šæœ¬
ä¸ä¾èµ–langchainï¼ŒåŸºäºTransformerså®ç°å®Œæ•´çš„RAGåŠŸèƒ½
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from simple_rag_workflow import SimpleRAGWorkflow

def main():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨é›†æˆRAGå·¥ä½œæµçš„Qwen3æ¨¡å‹"""
    
    print("ğŸš€ é›†æˆRAGå·¥ä½œæµçš„Qwen3æ¨¡å‹æ¼”ç¤º")
    print("="*60)
    print("åŠŸèƒ½ç‰¹ç‚¹:")
    print("1. ä¸ä¾èµ–langchainï¼Œçº¯Transformerså®ç°")
    print("2. æ”¯æŒå‘é‡æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)")
    print("3. å¤šè½®å¯¹è¯ç®¡ç†")
    print("4. åŸºäºæ–‡æ¡£å†…å®¹çš„æ™ºèƒ½é—®ç­”")
    print("="*60)
    
    # é…ç½®å‚æ•°
    config = {
        "llm_path": "../models--Qwen--Qwen3-8B/snapshots/9c925d64d72725edaf899c6cb9c377fd0709d9c5",
        "embedding_model_path": "/home/ubuntu/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181/",
        "db_path": "./vector_db",
        "similarity_threshold": 0.0  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç»“æœå°†è¢«è¿‡æ»¤
    }
    
    try:
        # åˆå§‹åŒ–RAGå·¥ä½œæµ
        print("\nğŸ”§ æ­£åœ¨åˆå§‹åŒ–RAGå·¥ä½œæµ...")
        workflow = SimpleRAGWorkflow(**config)
        
        print("\nâœ… ç³»ç»Ÿå¯åŠ¨å®Œæˆ!")
        print("\nä½¿ç”¨è¯´æ˜:")
        print("  - è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œç³»ç»Ÿå°†è¿›è¡Œå‘é‡æ£€ç´¢å¹¶ç”Ÿæˆå›ç­”")
        print("  - è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
        print("  - è¾“å…¥ 'summary' æŸ¥çœ‹å¯¹è¯æ‘˜è¦")
        print("  - è¾“å…¥ 'demo' è¿è¡Œæ¼”ç¤ºå¯¹è¯")
        print("  - è¾“å…¥ 'threshold <æ•°å€¼>' è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼ (å½“å‰: {:.2f})".format(config["similarity_threshold"]))
        print("  - è¾“å…¥ 'exit' é€€å‡ºç³»ç»Ÿ")
        
        while True:
            try:
                user_input = input("\nğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                
                if not user_input:
                    continue
                
                if user_input.lower() == 'exit':
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨RAGå·¥ä½œæµç³»ç»Ÿ!")
                    break
                elif user_input.lower() == 'clear':
                    workflow.clear_conversation()
                    continue
                elif user_input.lower() == 'summary':
                    summary = workflow.get_conversation_summary()
                    print(f"ğŸ“Š å¯¹è¯æ‘˜è¦: {summary}")
                    continue
                elif user_input.lower() == 'demo':
                    run_demo(workflow)
                    continue
                elif user_input.lower().startswith('threshold'):
                    try:
                        # è§£æé˜ˆå€¼å‘½ä»¤
                        parts = user_input.split()
                        if len(parts) == 2:
                            new_threshold = float(parts[1])
                            if 0.0 <= new_threshold <= 1.0:
                                workflow.similarity_threshold = new_threshold
                                workflow.rag_engine.similarity_threshold = new_threshold
                                config["similarity_threshold"] = new_threshold
                                print(f"âœ… ç›¸ä¼¼åº¦é˜ˆå€¼å·²æ›´æ–°ä¸º: {new_threshold:.2f}")
                            else:
                                print("âŒ é˜ˆå€¼å¿…é¡»åœ¨ 0.0 åˆ° 1.0 ä¹‹é—´")
                        else:
                            print("âŒ ç”¨æ³•: threshold <æ•°å€¼> (ä¾‹å¦‚: threshold 0.5)")
                    except ValueError:
                        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å€¼")
                    continue
                
                # å¤„ç†ç”¨æˆ·æŸ¥è¯¢
                response = workflow.process_user_query(user_input)
                
                # æ˜¾ç¤ºç»“æœ
                workflow.display_response(response)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç³»ç»Ÿå·²ä¸­æ–­ï¼Œæ„Ÿè°¢ä½¿ç”¨!")
                break
            except Exception as e:
                print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                print("è¯·é‡è¯•æˆ–è¾“å…¥ 'exit' é€€å‡ºç³»ç»Ÿ")
                
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥:")
        print("1. æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. ä¾èµ–åŒ…æ˜¯å¦å®Œæ•´å®‰è£…")
        print("3. å‘é‡æ•°æ®åº“æ˜¯å¦å·²åˆå§‹åŒ–")

def run_demo(workflow):
    """è¿è¡Œæ¼”ç¤ºå¯¹è¯"""
    print("\nğŸ­ å¼€å§‹æ¼”ç¤ºå¯¹è¯...")
    print("-" * 60)
    
    # æ¼”ç¤ºé—®é¢˜åˆ—è¡¨
    demo_questions = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
        "ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆï¼Ÿ",
        "è¯·è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "ä½ åˆšæ‰æåˆ°äº†ä»€ä¹ˆï¼Ÿ"
    ]
    
    for i, question in enumerate(demo_questions, 1):
        print(f"\nğŸ“ æ¼”ç¤ºé—®é¢˜ {i}: {question}")
        print("-" * 40)
        
        try:
            response = workflow.process_user_query(question)
            print(f"ğŸ¤– AIå›ç­”: {response.llm_response}")
            
            if response.retrieved_chunks:
                print(f"ğŸ“š æ£€ç´¢åˆ° {len(response.retrieved_chunks)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
                for j, chunk in enumerate(response.retrieved_chunks[:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª
                    print(f"   æ–‡æ¡£{j+1}: {chunk.filename} (ç›¸ä¼¼åº¦: {chunk.score:.3f})")
            
        except Exception as e:
            print(f"âŒ æ¼”ç¤ºé—®é¢˜å¤„ç†å¤±è´¥: {e}")
        
        print("-" * 40)
    
    print("\nâœ… æ¼”ç¤ºå¯¹è¯å®Œæˆ!")

def simple_chat_demo():
    """ç®€å•çš„å¯¹è¯æ¼”ç¤ºï¼ˆä¸åŒ…å«RAGåŠŸèƒ½ï¼‰"""
    print("\nğŸ’¬ ç®€å•å¯¹è¯æ¼”ç¤ºï¼ˆæ— RAGåŠŸèƒ½ï¼‰")
    print("-" * 40)
    
    # æ¨¡å‹è·¯å¾„
    model_path = "../models--Qwen--Qwen3-8B/snapshots/9c925d64d72725edaf899c6cb9c377fd0709d9c5"
    
    # åŠ è½½åˆ†è¯å™¨
    print("1. åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    # é…ç½®é‡åŒ–
    print("2. é…ç½®é‡åŒ–...")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )
    
    # åŠ è½½æ¨¡å‹
    print("3. åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
    )
    
    # åˆå§‹åŒ–å¯¹è¯å†å²
    chat_history = []
    
    def chat_with_model(user_input):
        """ä¸æ¨¡å‹è¿›è¡Œå•è½®å¯¹è¯"""
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        chat_history.append({"role": "user", "content": user_input})
        
        # æ„å»ºå®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"}
        ] + chat_history
        
        # æ ¼å¼åŒ–è¾“å…¥
        text = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # ç¼–ç 
        inputs = tokenizer(text, return_tensors="pt")
        
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=500,
                temperature=0.7,
                do_sample=True
            )
        
        # è§£ç è¾“å‡º
        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        
        # æ·»åŠ AIå›å¤åˆ°å†å²
        chat_history.append({"role": "assistant", "content": response})
        
        return response
    
    # æ¼”ç¤ºå¤šè½®å¯¹è¯
    print("\nå¼€å§‹å¤šè½®å¯¹è¯æ¼”ç¤º:")
    print("-" * 40)
    
    # ç¬¬ä¸€è½®å¯¹è¯
    user_msg1 = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
    print(f"user: {user_msg1}")
    response1 = chat_with_model(user_msg1)
    print(f"assistant: {response1}")
    print()
    
    # ç¬¬äºŒè½®å¯¹è¯ï¼ˆåŸºäºä¸Šä¸‹æ–‡ï¼‰
    user_msg2 = "ä½ åˆšæ‰æåˆ°äº†ä»€ä¹ˆï¼Ÿ"
    print(f"user: {user_msg2}")
    response2 = chat_with_model(user_msg2)
    print(f"assistant: {response2}")
    print()
    
    # ç¬¬ä¸‰è½®å¯¹è¯ï¼ˆç»§ç»­ä¸Šä¸‹æ–‡ï¼‰
    user_msg3 = "ä½ èƒ½å¸®æˆ‘å†™ä¸€ä¸ªPythonå‡½æ•°å—ï¼Ÿ"
    print(f"user: {user_msg3}")
    response3 = chat_with_model(user_msg3)
    print(f"assistant: {response3}")
    print()
    
    # æ˜¾ç¤ºå¯¹è¯å†å²
    print("å¯¹è¯å†å²:")
    print("-" * 40)
    for i, msg in enumerate(chat_history):
        role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
        print(f"{i+1}. {role}: {msg['content']}")
    print("-" * 40)
    
    print("\nâœ… ç®€å•å¯¹è¯æ¼”ç¤ºå®Œæˆ!")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--simple":
        # è¿è¡Œç®€å•å¯¹è¯æ¼”ç¤º
        simple_chat_demo()
    else:
        # è¿è¡ŒRAGå·¥ä½œæµ
        main()
