{
  "query": "disk scheduling algorithm",
  "timestamp": "2025-12-25 23:41:22",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/fair.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:09:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\fair.c`\n\n---\n\n# `sched/fair.c` 技术文档\n\n## 1. 文件概述\n\n`sched/fair.c` 是 Linux 内核中 **完全公平调度器**（Completely Fair Scheduler, CFS）的核心实现文件，负责实现 `SCHED_NORMAL` 和 `SCHED_BATCH` 调度策略。CFS 旨在通过红黑树（RB-tree）维护可运行任务的虚拟运行时间（vruntime），以实现 CPU 时间的公平分配。该文件实现了任务调度、负载跟踪、时间片计算、组调度（group scheduling）、NUMA 负载均衡、带宽控制等关键机制，是 Linux 通用调度子系统的核心组成部分。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_entity`：调度实体，代表一个可调度单元（任务或任务组）\n- `struct cfs_rq`：CFS 运行队列，管理一组调度实体\n- `struct load_weight`：负载权重结构，用于计算任务对系统负载的贡献\n\n### 关键函数与宏\n- `__calc_delta()` / `calc_delta_fair()`：计算基于权重的调度时间增量\n- `update_load_add()` / `update_load_sub()` / `update_load_set()`：更新负载权重\n- `__update_inv_weight()`：预计算权重的倒数以优化除法运算\n- `get_update_sysctl_factor()`：根据在线 CPU 数量动态调整调度参数\n- `update_sysctl()` / `sched_init_granularity()`：初始化和更新调度粒度参数\n- `for_each_sched_entity()`：遍历调度实体层级结构（用于组调度）\n\n### 可调参数（sysctl）\n- `sysctl_sched_base_slice`：基础时间片（默认 700,000 纳秒）\n- `sysctl_sched_tunable_scaling`：调度参数缩放策略（NONE/LOG/LINEAR）\n- `sysctl_sched_migration_cost`：任务迁移成本阈值（500 微秒）\n- `sysctl_sched_cfs_bandwidth_slice_us`（CFS 带宽控制切片，默认 5 毫秒）\n- `sysctl_numa_balancing_promote_rate_limit_MBps`（NUMA 页迁移速率限制）\n\n## 3. 关键实现\n\n### 虚拟时间与公平性\nCFS 使用 **虚拟运行时间**（vruntime）衡量任务已使用的 CPU 时间，并通过 `calc_delta_fair()` 将实际执行时间按任务权重归一化。权重由任务的 nice 值决定（`NICE_0_LOAD = 1024` 为基准）。调度器总是选择 vruntime 最小的任务运行，确保高优先级（高权重）任务获得更多 CPU 时间。\n\n### 高效除法优化\n为避免频繁除法运算，CFS 预计算 `inv_weight = WMULT_CONST / weight`（`WMULT_CONST = ~0U`），将除法转换为乘法和右移操作（`mul_u64_u32_shr`）。`__calc_delta()` 通过动态调整移位位数（`shift`）保证计算精度，适用于 32/64 位架构。\n\n### 动态粒度调整\n基础时间片 `sched_base_slice` 根据在线 CPU 数量动态缩放：\n- `SCHED_TUNABLESCALING_NONE`：固定值\n- `SCHED_TUNABLESCALING_LINEAR`：线性缩放（×ncpus）\n- `SCHED_TUNABLESCALING_LOG`（默认）：对数缩放（×(1 + ilog2(ncpus))）  \n此设计确保在多核系统中保持合理的调度延迟和交互性。\n\n### 组调度支持\n通过 `for_each_sched_entity()` 宏遍历任务所属的调度实体层级（任务 → 任务组 → 父任务组），实现 CPU 带宽在任务组间的公平分配。每个 `cfs_rq` 独立维护其子实体的红黑树。\n\n### SMP 相关优化\n- **非对称 CPU 优先级**：`arch_asym_cpu_priority()` 允许架构定义 CPU 能力差异（如大小核）\n- **容量比较宏**：`fits_capacity()`（20% 容差）和 `capacity_greater()`（5% 容差）用于负载均衡决策\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- 调度核心：`\"sched.h\"`、`\"stats.h\"`、`\"autogroup.h\"`\n- 系统服务：`<linux/sched/clock.h>`、`<linux/sched/nohz.h>`、`<linux/psi.h>`\n- 内存管理：`<linux/mem_policy.h>`、`<linux/energy_model.h>`\n- SMP 支持：`<linux/topology.h>`、`<linux/cpumask_api.h>`\n- 数据结构：`<linux/rbtree_augmented.h>`\n\n### 条件编译特性\n- `CONFIG_SMP`：多处理器调度优化\n- `CONFIG_CFS_BANDWIDTH`：CPU 带宽限制（cgroup v1/v2）\n- `CONFIG_NUMA_BALANCING`：NUMA 自动迁移\n- `CONFIG_FAIR_GROUP_SCHED`：CFS 组调度（cgroup 支持）\n\n## 5. 使用场景\n\n- **通用任务调度**：所有使用 `SCHED_NORMAL` 或 `SCHED_BATCH` 策略的用户态进程\n- **cgroup CPU 资源控制**：通过 `cpu.cfs_quota_us` 和 `cpu.cfs_period_us` 限制任务组带宽\n- **NUMA 优化**：自动迁移内存页以减少远程访问（`numa_balancing`）\n- **节能调度**：结合 `energy_model` 在满足性能前提下选择低功耗 CPU\n- **实时性保障**：通过 `cond_resched()` 在长循环中主动让出 CPU，避免内核抢占延迟过高\n- **系统调优**：管理员通过 `/proc/sys/kernel/` 下的 sysctl 参数动态调整调度行为",
      "similarity": 0.5880052447319031,
      "chunks": [
        {
          "chunk_id": 45,
          "file_path": "kernel/sched/fair.c",
          "start_line": 7352,
          "end_line": 7468,
          "content": [
            "static int",
            "wake_affine_weight(struct sched_domain *sd, struct task_struct *p,",
            "\t\t   int this_cpu, int prev_cpu, int sync)",
            "{",
            "\ts64 this_eff_load, prev_eff_load;",
            "\tunsigned long task_load;",
            "",
            "\tthis_eff_load = cpu_load(cpu_rq(this_cpu));",
            "",
            "\tif (sync) {",
            "\t\tunsigned long current_load = task_h_load(current);",
            "",
            "\t\tif (current_load > this_eff_load)",
            "\t\t\treturn this_cpu;",
            "",
            "\t\tthis_eff_load -= current_load;",
            "\t}",
            "",
            "\ttask_load = task_h_load(p);",
            "",
            "\tthis_eff_load += task_load;",
            "\tif (sched_feat(WA_BIAS))",
            "\t\tthis_eff_load *= 100;",
            "\tthis_eff_load *= capacity_of(prev_cpu);",
            "",
            "\tprev_eff_load = cpu_load(cpu_rq(prev_cpu));",
            "\tprev_eff_load -= task_load;",
            "\tif (sched_feat(WA_BIAS))",
            "\t\tprev_eff_load *= 100 + (sd->imbalance_pct - 100) / 2;",
            "\tprev_eff_load *= capacity_of(this_cpu);",
            "",
            "\t/*",
            "\t * If sync, adjust the weight of prev_eff_load such that if",
            "\t * prev_eff == this_eff that select_idle_sibling() will consider",
            "\t * stacking the wakee on top of the waker if no other CPU is",
            "\t * idle.",
            "\t */",
            "\tif (sync)",
            "\t\tprev_eff_load += 1;",
            "",
            "\treturn this_eff_load < prev_eff_load ? this_cpu : nr_cpumask_bits;",
            "}",
            "static int wake_affine(struct sched_domain *sd, struct task_struct *p,",
            "\t\t       int this_cpu, int prev_cpu, int sync)",
            "{",
            "\tint target = nr_cpumask_bits;",
            "",
            "\tif (sched_feat(WA_IDLE))",
            "\t\ttarget = wake_affine_idle(this_cpu, prev_cpu, sync);",
            "",
            "\tif (sched_feat(WA_WEIGHT) && target == nr_cpumask_bits)",
            "\t\ttarget = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);",
            "",
            "\tschedstat_inc(p->stats.nr_wakeups_affine_attempts);",
            "\tif (target != this_cpu)",
            "\t\treturn prev_cpu;",
            "",
            "\tschedstat_inc(sd->ttwu_move_affine);",
            "\tschedstat_inc(p->stats.nr_wakeups_affine);",
            "\treturn target;",
            "}",
            "static int",
            "sched_balance_find_dst_group_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)",
            "{",
            "\tunsigned long load, min_load = ULONG_MAX;",
            "\tunsigned int min_exit_latency = UINT_MAX;",
            "\tu64 latest_idle_timestamp = 0;",
            "\tint least_loaded_cpu = this_cpu;",
            "\tint shallowest_idle_cpu = -1;",
            "\tint i;",
            "",
            "\t/* Check if we have any choice: */",
            "\tif (group->group_weight == 1)",
            "\t\treturn cpumask_first(sched_group_span(group));",
            "",
            "\t/* Traverse only the allowed CPUs */",
            "\tfor_each_cpu_and(i, sched_group_span(group), p->cpus_ptr) {",
            "\t\tstruct rq *rq = cpu_rq(i);",
            "",
            "\t\tif (!sched_core_cookie_match(rq, p))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (sched_idle_cpu(i))",
            "\t\t\treturn i;",
            "",
            "\t\tif (available_idle_cpu(i)) {",
            "\t\t\tstruct cpuidle_state *idle = idle_get_state(rq);",
            "\t\t\tif (idle && idle->exit_latency < min_exit_latency) {",
            "\t\t\t\t/*",
            "\t\t\t\t * We give priority to a CPU whose idle state",
            "\t\t\t\t * has the smallest exit latency irrespective",
            "\t\t\t\t * of any idle timestamp.",
            "\t\t\t\t */",
            "\t\t\t\tmin_exit_latency = idle->exit_latency;",
            "\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;",
            "\t\t\t\tshallowest_idle_cpu = i;",
            "\t\t\t} else if ((!idle || idle->exit_latency == min_exit_latency) &&",
            "\t\t\t\t   rq->idle_stamp > latest_idle_timestamp) {",
            "\t\t\t\t/*",
            "\t\t\t\t * If equal or no active idle state, then",
            "\t\t\t\t * the most recently idled CPU might have",
            "\t\t\t\t * a warmer cache.",
            "\t\t\t\t */",
            "\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;",
            "\t\t\t\tshallowest_idle_cpu = i;",
            "\t\t\t}",
            "\t\t} else if (shallowest_idle_cpu == -1) {",
            "\t\t\tload = cpu_load(cpu_rq(i));",
            "\t\t\tif (load < min_load) {",
            "\t\t\t\tmin_load = load;",
            "\t\t\t\tleast_loaded_cpu = i;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\treturn shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu;",
            "}"
          ],
          "function_name": "wake_affine_weight, wake_affine, sched_balance_find_dst_group_cpu",
          "description": "wake_affine_weight计算任务迁移后的目标CPU负载差异，结合权重和缓存亲和性调整负载比对结果；wake_affine根据WA_IDLE/WA_WEIGHT特性选择目标CPU并统计唤醒次数；sched_balance_find_dst_group_cpu遍历调度组内CPU，优先选择空闲且具有最小退出延迟的CPU",
          "similarity": 0.5735771656036377
        },
        {
          "chunk_id": 46,
          "file_path": "kernel/sched/fair.c",
          "start_line": 7478,
          "end_line": 7603,
          "content": [
            "static inline int sched_balance_find_dst_cpu(struct sched_domain *sd, struct task_struct *p,",
            "\t\t\t\t  int cpu, int prev_cpu, int sd_flag)",
            "{",
            "\tint new_cpu = cpu;",
            "",
            "\tif (!cpumask_intersects(sched_domain_span(sd), p->cpus_ptr))",
            "\t\treturn prev_cpu;",
            "",
            "\t/*",
            "\t * We need task's util for cpu_util_without, sync it up to",
            "\t * prev_cpu's last_update_time.",
            "\t */",
            "\tif (!(sd_flag & SD_BALANCE_FORK))",
            "\t\tsync_entity_load_avg(&p->se);",
            "",
            "\twhile (sd) {",
            "\t\tstruct sched_group *group;",
            "\t\tstruct sched_domain *tmp;",
            "\t\tint weight;",
            "",
            "\t\tif (!(sd->flags & sd_flag)) {",
            "\t\t\tsd = sd->child;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tgroup = sched_balance_find_dst_group(sd, p, cpu);",
            "\t\tif (!group) {",
            "\t\t\tsd = sd->child;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tnew_cpu = sched_balance_find_dst_group_cpu(group, p, cpu);",
            "\t\tif (new_cpu == cpu) {",
            "\t\t\t/* Now try balancing at a lower domain level of 'cpu': */",
            "\t\t\tsd = sd->child;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/* Now try balancing at a lower domain level of 'new_cpu': */",
            "\t\tcpu = new_cpu;",
            "\t\tweight = sd->span_weight;",
            "\t\tsd = NULL;",
            "\t\tfor_each_domain(cpu, tmp) {",
            "\t\t\tif (weight <= tmp->span_weight)",
            "\t\t\t\tbreak;",
            "\t\t\tif (tmp->flags & sd_flag)",
            "\t\t\t\tsd = tmp;",
            "\t\t}",
            "\t}",
            "",
            "\treturn new_cpu;",
            "}",
            "static inline int __select_idle_cpu(int cpu, struct task_struct *p)",
            "{",
            "\tif ((available_idle_cpu(cpu) || sched_idle_cpu(cpu)) &&",
            "\t    sched_cpu_cookie_match(cpu_rq(cpu), p))",
            "\t\treturn cpu;",
            "",
            "\treturn -1;",
            "}",
            "static inline void set_idle_cores(int cpu, int val)",
            "{",
            "\tstruct sched_domain_shared *sds;",
            "",
            "\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));",
            "\tif (sds)",
            "\t\tWRITE_ONCE(sds->has_idle_cores, val);",
            "}",
            "static inline bool test_idle_cores(int cpu)",
            "{",
            "\tstruct sched_domain_shared *sds;",
            "",
            "\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));",
            "\tif (sds)",
            "\t\treturn READ_ONCE(sds->has_idle_cores);",
            "",
            "\treturn false;",
            "}",
            "void __update_idle_core(struct rq *rq)",
            "{",
            "\tint core = cpu_of(rq);",
            "\tint cpu;",
            "",
            "\trcu_read_lock();",
            "\tif (test_idle_cores(core))",
            "\t\tgoto unlock;",
            "",
            "\tfor_each_cpu(cpu, cpu_smt_mask(core)) {",
            "\t\tif (cpu == core)",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!available_idle_cpu(cpu))",
            "\t\t\tgoto unlock;",
            "\t}",
            "",
            "\tset_idle_cores(core, 1);",
            "unlock:",
            "\trcu_read_unlock();",
            "}",
            "static int select_idle_core(struct task_struct *p, int core, struct cpumask *cpus, int *idle_cpu)",
            "{",
            "\tbool idle = true;",
            "\tint cpu;",
            "",
            "\tfor_each_cpu(cpu, cpu_smt_mask(core)) {",
            "\t\tif (!available_idle_cpu(cpu)) {",
            "\t\t\tidle = false;",
            "\t\t\tif (*idle_cpu == -1) {",
            "\t\t\t\tif (sched_idle_cpu(cpu) && cpumask_test_cpu(cpu, cpus)) {",
            "\t\t\t\t\t*idle_cpu = cpu;",
            "\t\t\t\t\tbreak;",
            "\t\t\t\t}",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tif (*idle_cpu == -1 && cpumask_test_cpu(cpu, cpus))",
            "\t\t\t*idle_cpu = cpu;",
            "\t}",
            "",
            "\tif (idle)",
            "\t\treturn core;",
            "",
            "\tcpumask_andnot(cpus, cpus, cpu_smt_mask(core));",
            "\treturn -1;",
            "}"
          ],
          "function_name": "sched_balance_find_dst_cpu, __select_idle_cpu, set_idle_cores, test_idle_cores, __update_idle_core, select_idle_core",
          "description": "sched_balance_find_dst_cpu递归遍历调度域寻找最优迁移目标CPU；__select_idle_cpu检测CPU是否满足空闲及亲和性条件；set_idle_cores/test_idle_cores维护LLC共享域的空闲核心标志位；__update_idle_core标记当前核心是否存在空闲SMT线程",
          "similarity": 0.5711763501167297
        },
        {
          "chunk_id": 17,
          "file_path": "kernel/sched/fair.c",
          "start_line": 2878,
          "end_line": 2989,
          "content": [
            "static void task_numa_placement(struct task_struct *p)",
            "{",
            "\tint seq, nid, max_nid = NUMA_NO_NODE;",
            "\tunsigned long max_faults = 0;",
            "\tunsigned long fault_types[2] = { 0, 0 };",
            "\tunsigned long total_faults;",
            "\tu64 runtime, period;",
            "\tspinlock_t *group_lock = NULL;",
            "\tstruct numa_group *ng;",
            "",
            "\t/*",
            "\t * The p->mm->numa_scan_seq field gets updated without",
            "\t * exclusive access. Use READ_ONCE() here to ensure",
            "\t * that the field is read in a single access:",
            "\t */",
            "\tseq = READ_ONCE(p->mm->numa_scan_seq);",
            "\tif (p->numa_scan_seq == seq)",
            "\t\treturn;",
            "\tp->numa_scan_seq = seq;",
            "\tp->numa_scan_period_max = task_scan_max(p);",
            "",
            "\ttotal_faults = p->numa_faults_locality[0] +",
            "\t\t       p->numa_faults_locality[1];",
            "\truntime = numa_get_avg_runtime(p, &period);",
            "",
            "\t/* If the task is part of a group prevent parallel updates to group stats */",
            "\tng = deref_curr_numa_group(p);",
            "\tif (ng) {",
            "\t\tgroup_lock = &ng->lock;",
            "\t\tspin_lock_irq(group_lock);",
            "\t}",
            "",
            "\t/* Find the node with the highest number of faults */",
            "\tfor_each_online_node(nid) {",
            "\t\t/* Keep track of the offsets in numa_faults array */",
            "\t\tint mem_idx, membuf_idx, cpu_idx, cpubuf_idx;",
            "\t\tunsigned long faults = 0, group_faults = 0;",
            "\t\tint priv;",
            "",
            "\t\tfor (priv = 0; priv < NR_NUMA_HINT_FAULT_TYPES; priv++) {",
            "\t\t\tlong diff, f_diff, f_weight;",
            "",
            "\t\t\tmem_idx = task_faults_idx(NUMA_MEM, nid, priv);",
            "\t\t\tmembuf_idx = task_faults_idx(NUMA_MEMBUF, nid, priv);",
            "\t\t\tcpu_idx = task_faults_idx(NUMA_CPU, nid, priv);",
            "\t\t\tcpubuf_idx = task_faults_idx(NUMA_CPUBUF, nid, priv);",
            "",
            "\t\t\t/* Decay existing window, copy faults since last scan */",
            "\t\t\tdiff = p->numa_faults[membuf_idx] - p->numa_faults[mem_idx] / 2;",
            "\t\t\tfault_types[priv] += p->numa_faults[membuf_idx];",
            "\t\t\tp->numa_faults[membuf_idx] = 0;",
            "",
            "\t\t\t/*",
            "\t\t\t * Normalize the faults_from, so all tasks in a group",
            "\t\t\t * count according to CPU use, instead of by the raw",
            "\t\t\t * number of faults. Tasks with little runtime have",
            "\t\t\t * little over-all impact on throughput, and thus their",
            "\t\t\t * faults are less important.",
            "\t\t\t */",
            "\t\t\tf_weight = div64_u64(runtime << 16, period + 1);",
            "\t\t\tf_weight = (f_weight * p->numa_faults[cpubuf_idx]) /",
            "\t\t\t\t   (total_faults + 1);",
            "\t\t\tf_diff = f_weight - p->numa_faults[cpu_idx] / 2;",
            "\t\t\tp->numa_faults[cpubuf_idx] = 0;",
            "",
            "\t\t\tp->numa_faults[mem_idx] += diff;",
            "\t\t\tp->numa_faults[cpu_idx] += f_diff;",
            "\t\t\tfaults += p->numa_faults[mem_idx];",
            "\t\t\tp->total_numa_faults += diff;",
            "\t\t\tif (ng) {",
            "\t\t\t\t/*",
            "\t\t\t\t * safe because we can only change our own group",
            "\t\t\t\t *",
            "\t\t\t\t * mem_idx represents the offset for a given",
            "\t\t\t\t * nid and priv in a specific region because it",
            "\t\t\t\t * is at the beginning of the numa_faults array.",
            "\t\t\t\t */",
            "\t\t\t\tng->faults[mem_idx] += diff;",
            "\t\t\t\tng->faults[cpu_idx] += f_diff;",
            "\t\t\t\tng->total_faults += diff;",
            "\t\t\t\tgroup_faults += ng->faults[mem_idx];",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tif (!ng) {",
            "\t\t\tif (faults > max_faults) {",
            "\t\t\t\tmax_faults = faults;",
            "\t\t\t\tmax_nid = nid;",
            "\t\t\t}",
            "\t\t} else if (group_faults > max_faults) {",
            "\t\t\tmax_faults = group_faults;",
            "\t\t\tmax_nid = nid;",
            "\t\t}",
            "\t}",
            "",
            "\t/* Cannot migrate task to CPU-less node */",
            "\tmax_nid = numa_nearest_node(max_nid, N_CPU);",
            "",
            "\tif (ng) {",
            "\t\tnuma_group_count_active_nodes(ng);",
            "\t\tspin_unlock_irq(group_lock);",
            "\t\tmax_nid = preferred_group_nid(p, max_nid);",
            "\t}",
            "",
            "\tif (max_faults) {",
            "\t\t/* Set the new preferred node */",
            "\t\tif (max_nid != p->numa_preferred_nid)",
            "\t\t\tsched_setnuma(p, max_nid);",
            "\t}",
            "",
            "\tupdate_task_scan_period(p, fault_types[0], fault_types[1]);",
            "}"
          ],
          "function_name": "task_numa_placement",
          "description": "task_numa_placement遍历所有在线节点，根据NUMA故障次数确定首选节点，更新任务的numa_preferred_nid，并调整扫描周期参数。",
          "similarity": 0.5693316459655762
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/sched/fair.c",
          "start_line": 2430,
          "end_line": 2620,
          "content": [
            "static void task_numa_find_cpu(struct task_numa_env *env,",
            "\t\t\t\tlong taskimp, long groupimp)",
            "{",
            "\tbool maymove = false;",
            "\tint cpu;",
            "",
            "\t/*",
            "\t * If dst node has spare capacity, then check if there is an",
            "\t * imbalance that would be overruled by the load balancer.",
            "\t */",
            "\tif (env->dst_stats.node_type == node_has_spare) {",
            "\t\tunsigned int imbalance;",
            "\t\tint src_running, dst_running;",
            "",
            "\t\t/*",
            "\t\t * Would movement cause an imbalance? Note that if src has",
            "\t\t * more running tasks that the imbalance is ignored as the",
            "\t\t * move improves the imbalance from the perspective of the",
            "\t\t * CPU load balancer.",
            "\t\t * */",
            "\t\tsrc_running = env->src_stats.nr_running - 1;",
            "\t\tdst_running = env->dst_stats.nr_running + 1;",
            "\t\timbalance = max(0, dst_running - src_running);",
            "\t\timbalance = adjust_numa_imbalance(imbalance, dst_running,",
            "\t\t\t\t\t\t  env->imb_numa_nr);",
            "",
            "\t\t/* Use idle CPU if there is no imbalance */",
            "\t\tif (!imbalance) {",
            "\t\t\tmaymove = true;",
            "\t\t\tif (env->dst_stats.idle_cpu >= 0) {",
            "\t\t\t\tenv->dst_cpu = env->dst_stats.idle_cpu;",
            "\t\t\t\ttask_numa_assign(env, NULL, 0);",
            "\t\t\t\treturn;",
            "\t\t\t}",
            "\t\t}",
            "\t} else {",
            "\t\tlong src_load, dst_load, load;",
            "\t\t/*",
            "\t\t * If the improvement from just moving env->p direction is better",
            "\t\t * than swapping tasks around, check if a move is possible.",
            "\t\t */",
            "\t\tload = task_h_load(env->p);",
            "\t\tdst_load = env->dst_stats.load + load;",
            "\t\tsrc_load = env->src_stats.load - load;",
            "\t\tmaymove = !load_too_imbalanced(src_load, dst_load, env);",
            "\t}",
            "",
            "\tfor_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {",
            "\t\t/* Skip this CPU if the source task cannot migrate */",
            "\t\tif (!cpumask_test_cpu(cpu, env->p->cpus_ptr))",
            "\t\t\tcontinue;",
            "",
            "\t\tenv->dst_cpu = cpu;",
            "\t\tif (task_numa_compare(env, taskimp, groupimp, maymove))",
            "\t\t\tbreak;",
            "\t}",
            "}",
            "static int task_numa_migrate(struct task_struct *p)",
            "{",
            "\tstruct task_numa_env env = {",
            "\t\t.p = p,",
            "",
            "\t\t.src_cpu = task_cpu(p),",
            "\t\t.src_nid = task_node(p),",
            "",
            "\t\t.imbalance_pct = 112,",
            "",
            "\t\t.best_task = NULL,",
            "\t\t.best_imp = 0,",
            "\t\t.best_cpu = -1,",
            "\t};",
            "\tunsigned long taskweight, groupweight;",
            "\tstruct sched_domain *sd;",
            "\tlong taskimp, groupimp;",
            "\tstruct numa_group *ng;",
            "\tstruct rq *best_rq;",
            "\tint nid, ret, dist;",
            "",
            "\t/*",
            "\t * Pick the lowest SD_NUMA domain, as that would have the smallest",
            "\t * imbalance and would be the first to start moving tasks about.",
            "\t *",
            "\t * And we want to avoid any moving of tasks about, as that would create",
            "\t * random movement of tasks -- counter the numa conditions we're trying",
            "\t * to satisfy here.",
            "\t */",
            "\trcu_read_lock();",
            "\tsd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));",
            "\tif (sd) {",
            "\t\tenv.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;",
            "\t\tenv.imb_numa_nr = sd->imb_numa_nr;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * Cpusets can break the scheduler domain tree into smaller",
            "\t * balance domains, some of which do not cross NUMA boundaries.",
            "\t * Tasks that are \"trapped\" in such domains cannot be migrated",
            "\t * elsewhere, so there is no point in (re)trying.",
            "\t */",
            "\tif (unlikely(!sd)) {",
            "\t\tsched_setnuma(p, task_node(p));",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tenv.dst_nid = p->numa_preferred_nid;",
            "\tdist = env.dist = node_distance(env.src_nid, env.dst_nid);",
            "\ttaskweight = task_weight(p, env.src_nid, dist);",
            "\tgroupweight = group_weight(p, env.src_nid, dist);",
            "\tupdate_numa_stats(&env, &env.src_stats, env.src_nid, false);",
            "\ttaskimp = task_weight(p, env.dst_nid, dist) - taskweight;",
            "\tgroupimp = group_weight(p, env.dst_nid, dist) - groupweight;",
            "\tupdate_numa_stats(&env, &env.dst_stats, env.dst_nid, true);",
            "",
            "\t/* Try to find a spot on the preferred nid. */",
            "\ttask_numa_find_cpu(&env, taskimp, groupimp);",
            "",
            "\t/*",
            "\t * Look at other nodes in these cases:",
            "\t * - there is no space available on the preferred_nid",
            "\t * - the task is part of a numa_group that is interleaved across",
            "\t *   multiple NUMA nodes; in order to better consolidate the group,",
            "\t *   we need to check other locations.",
            "\t */",
            "\tng = deref_curr_numa_group(p);",
            "\tif (env.best_cpu == -1 || (ng && ng->active_nodes > 1)) {",
            "\t\tfor_each_node_state(nid, N_CPU) {",
            "\t\t\tif (nid == env.src_nid || nid == p->numa_preferred_nid)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tdist = node_distance(env.src_nid, env.dst_nid);",
            "\t\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&",
            "\t\t\t\t\t\tdist != env.dist) {",
            "\t\t\t\ttaskweight = task_weight(p, env.src_nid, dist);",
            "\t\t\t\tgroupweight = group_weight(p, env.src_nid, dist);",
            "\t\t\t}",
            "",
            "\t\t\t/* Only consider nodes where both task and groups benefit */",
            "\t\t\ttaskimp = task_weight(p, nid, dist) - taskweight;",
            "\t\t\tgroupimp = group_weight(p, nid, dist) - groupweight;",
            "\t\t\tif (taskimp < 0 && groupimp < 0)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tenv.dist = dist;",
            "\t\t\tenv.dst_nid = nid;",
            "\t\t\tupdate_numa_stats(&env, &env.dst_stats, env.dst_nid, true);",
            "\t\t\ttask_numa_find_cpu(&env, taskimp, groupimp);",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * If the task is part of a workload that spans multiple NUMA nodes,",
            "\t * and is migrating into one of the workload's active nodes, remember",
            "\t * this node as the task's preferred numa node, so the workload can",
            "\t * settle down.",
            "\t * A task that migrated to a second choice node will be better off",
            "\t * trying for a better one later. Do not set the preferred node here.",
            "\t */",
            "\tif (ng) {",
            "\t\tif (env.best_cpu == -1)",
            "\t\t\tnid = env.src_nid;",
            "\t\telse",
            "\t\t\tnid = cpu_to_node(env.best_cpu);",
            "",
            "\t\tif (nid != p->numa_preferred_nid)",
            "\t\t\tsched_setnuma(p, nid);",
            "\t}",
            "",
            "\t/* No better CPU than the current one was found. */",
            "\tif (env.best_cpu == -1) {",
            "\t\ttrace_sched_stick_numa(p, env.src_cpu, NULL, -1);",
            "\t\treturn -EAGAIN;",
            "\t}",
            "",
            "\tbest_rq = cpu_rq(env.best_cpu);",
            "\tif (env.best_task == NULL) {",
            "\t\tret = migrate_task_to(p, env.best_cpu);",
            "\t\tWRITE_ONCE(best_rq->numa_migrate_on, 0);",
            "\t\tif (ret != 0)",
            "\t\t\ttrace_sched_stick_numa(p, env.src_cpu, NULL, env.best_cpu);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu);",
            "\tWRITE_ONCE(best_rq->numa_migrate_on, 0);",
            "",
            "\tif (ret != 0)",
            "\t\ttrace_sched_stick_numa(p, env.src_cpu, env.best_task, env.best_cpu);",
            "\tput_task_struct(env.best_task);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "task_numa_find_cpu, task_numa_migrate",
          "description": "基于NUMA拓扑和调度域选择最佳目标节点；计算任务权重差异并触发迁移；若未找到合适节点则标记为无法迁移并返回错误码",
          "similarity": 0.5683445930480957
        },
        {
          "chunk_id": 68,
          "file_path": "kernel/sched/fair.c",
          "start_line": 12089,
          "end_line": 12257,
          "content": [
            "static inline int on_null_domain(struct rq *rq)",
            "{",
            "\treturn unlikely(!rcu_dereference_sched(rq->sd));",
            "}",
            "static inline int find_new_ilb(void)",
            "{",
            "\tint ilb;",
            "\tconst struct cpumask *hk_mask;",
            "",
            "\thk_mask = housekeeping_cpumask(HK_TYPE_MISC);",
            "",
            "\tfor_each_cpu_and(ilb, nohz.idle_cpus_mask, hk_mask) {",
            "",
            "\t\tif (ilb == smp_processor_id())",
            "\t\t\tcontinue;",
            "",
            "\t\tif (idle_cpu(ilb))",
            "\t\t\treturn ilb;",
            "\t}",
            "",
            "\treturn nr_cpu_ids;",
            "}",
            "static void kick_ilb(unsigned int flags)",
            "{",
            "\tint ilb_cpu;",
            "",
            "\t/*",
            "\t * Increase nohz.next_balance only when if full ilb is triggered but",
            "\t * not if we only update stats.",
            "\t */",
            "\tif (flags & NOHZ_BALANCE_KICK)",
            "\t\tnohz.next_balance = jiffies+1;",
            "",
            "\tilb_cpu = find_new_ilb();",
            "",
            "\tif (ilb_cpu >= nr_cpu_ids)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Access to rq::nohz_csd is serialized by NOHZ_KICK_MASK; he who sets",
            "\t * the first flag owns it; cleared by nohz_csd_func().",
            "\t */",
            "\tflags = atomic_fetch_or(flags, nohz_flags(ilb_cpu));",
            "\tif (flags & NOHZ_KICK_MASK)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This way we generate an IPI on the target CPU which",
            "\t * is idle. And the softirq performing nohz idle load balance",
            "\t * will be run before returning from the IPI.",
            "\t */",
            "\tsmp_call_function_single_async(ilb_cpu, &cpu_rq(ilb_cpu)->nohz_csd);",
            "}",
            "static void nohz_balancer_kick(struct rq *rq)",
            "{",
            "\tunsigned long now = jiffies;",
            "\tstruct sched_domain_shared *sds;",
            "\tstruct sched_domain *sd;",
            "\tint nr_busy, i, cpu = rq->cpu;",
            "\tunsigned int flags = 0;",
            "",
            "\tif (unlikely(rq->idle_balance))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * We may be recently in ticked or tickless idle mode. At the first",
            "\t * busy tick after returning from idle, we will update the busy stats.",
            "\t */",
            "\tnohz_balance_exit_idle(rq);",
            "",
            "\t/*",
            "\t * None are in tickless mode and hence no need for NOHZ idle load",
            "\t * balancing.",
            "\t */",
            "\tif (likely(!atomic_read(&nohz.nr_cpus)))",
            "\t\treturn;",
            "",
            "\tif (READ_ONCE(nohz.has_blocked) &&",
            "\t    time_after(now, READ_ONCE(nohz.next_blocked)))",
            "\t\tflags = NOHZ_STATS_KICK;",
            "",
            "\tif (time_before(now, nohz.next_balance))",
            "\t\tgoto out;",
            "",
            "\tif (rq->nr_running >= 2) {",
            "\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\trcu_read_lock();",
            "",
            "\tsd = rcu_dereference(rq->sd);",
            "\tif (sd) {",
            "\t\t/*",
            "\t\t * If there's a CFS task and the current CPU has reduced",
            "\t\t * capacity; kick the ILB to see if there's a better CPU to run",
            "\t\t * on.",
            "\t\t */",
            "\t\tif (rq->cfs.h_nr_running >= 1 && check_cpu_capacity(rq, sd)) {",
            "\t\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "\t}",
            "",
            "\tsd = rcu_dereference(per_cpu(sd_asym_packing, cpu));",
            "\tif (sd) {",
            "\t\t/*",
            "\t\t * When ASYM_PACKING; see if there's a more preferred CPU",
            "\t\t * currently idle; in which case, kick the ILB to move tasks",
            "\t\t * around.",
            "\t\t *",
            "\t\t * When balancing between cores, all the SMT siblings of the",
            "\t\t * preferred CPU must be idle.",
            "\t\t */",
            "\t\tfor_each_cpu_and(i, sched_domain_span(sd), nohz.idle_cpus_mask) {",
            "\t\t\tif (sched_asym(sd, i, cpu)) {",
            "\t\t\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;",
            "\t\t\t\tgoto unlock;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\tsd = rcu_dereference(per_cpu(sd_asym_cpucapacity, cpu));",
            "\tif (sd) {",
            "\t\t/*",
            "\t\t * When ASYM_CPUCAPACITY; see if there's a higher capacity CPU",
            "\t\t * to run the misfit task on.",
            "\t\t */",
            "\t\tif (check_misfit_status(rq)) {",
            "\t\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * For asymmetric systems, we do not want to nicely balance",
            "\t\t * cache use, instead we want to embrace asymmetry and only",
            "\t\t * ensure tasks have enough CPU capacity.",
            "\t\t *",
            "\t\t * Skip the LLC logic because it's not relevant in that case.",
            "\t\t */",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));",
            "\tif (sds) {",
            "\t\t/*",
            "\t\t * If there is an imbalance between LLC domains (IOW we could",
            "\t\t * increase the overall cache use), we need some less-loaded LLC",
            "\t\t * domain to pull some load. Likewise, we may need to spread",
            "\t\t * load within the current LLC domain (e.g. packed SMT cores but",
            "\t\t * other CPUs are idle). We can't really know from here how busy",
            "\t\t * the others are - so just get a nohz balance going if it looks",
            "\t\t * like this LLC domain has tasks we could move.",
            "\t\t */",
            "\t\tnr_busy = atomic_read(&sds->nr_busy_cpus);",
            "\t\tif (nr_busy > 1) {",
            "\t\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "\t}",
            "unlock:",
            "\trcu_read_unlock();",
            "out:",
            "\tif (READ_ONCE(nohz.needs_update))",
            "\t\tflags |= NOHZ_NEXT_KICK;",
            "",
            "\tif (flags)",
            "\t\tkick_ilb(flags);",
            "}"
          ],
          "function_name": "on_null_domain, find_new_ilb, kick_ilb, nohz_balancer_kick",
          "description": "在非抢占模式下查找可用空闲负载均衡器CPU，通过IPI触发跨CPU的任务迁移",
          "similarity": 0.5637122392654419
        }
      ]
    },
    {
      "source_file": "kernel/sched/deadline.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:06:40\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\deadline.c`\n\n---\n\n# `sched/deadline.c` 技术文档\n\n## 1. 文件概述\n\n`sched/deadline.c` 是 Linux 内核调度器中 **SCHED_DEADLINE** 调度类的核心实现文件。该调度类基于 **最早截止时间优先（Earliest Deadline First, EDF）** 算法，并结合 **恒定带宽服务器（Constant Bandwidth Server, CBS）** 机制，为具有严格实时性要求的任务提供可预测的调度保障。\n\n其核心目标是：  \n- 对于周期性任务，若其实际运行时间不超过所申请的运行时间（runtime），则保证不会错过任何截止时间（deadline）；  \n- 对于非周期性任务、突发任务或试图超出其预留带宽的任务，系统会对其进行节流（throttling），防止其影响其他任务的实时性保障。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_dl_entity`：表示一个 deadline 调度实体，包含任务的运行时间（runtime）、截止期限（deadline）、周期（period）、带宽（dl_bw）等关键参数。\n- `struct dl_rq`：每个 CPU 的 deadline 运行队列，维护该 CPU 上所有 deadline 任务的红黑树、当前带宽使用情况（`this_bw`、`running_bw`）等。\n- `struct dl_bw`：deadline 带宽管理结构，用于跟踪系统或调度域中已分配的总带宽（`total_bw`）。\n\n### 主要函数与辅助宏\n\n#### 调度实体与运行队列关联\n- `dl_task_of(dl_se)`：从 `sched_dl_entity` 获取对应的 `task_struct`（仅适用于普通任务，不适用于服务器实体）。\n- `rq_of_dl_rq(dl_rq)` / `rq_of_dl_se(dl_se)`：获取与 deadline 运行队列或调度实体关联的 `rq`（runqueue）。\n- `dl_rq_of_se(dl_se)`：获取调度实体所属的 `dl_rq`。\n- `on_dl_rq(dl_se)`：判断调度实体是否已在 deadline 运行队列中（通过红黑树节点是否为空判断）。\n\n#### 优先级继承（PI）支持（`CONFIG_RT_MUTEXES`）\n- `pi_of(dl_se)`：获取当前调度实体因优先级继承而提升后的“代理”实体。\n- `is_dl_boosted(dl_se)`：判断该 deadline 实体是否因优先级继承被提升。\n\n#### 带宽管理（SMP 与 UP 差异处理）\n- `dl_bw_of(cpu)`：获取指定 CPU 所属调度域（或本地）的 `dl_bw` 结构。\n- `dl_bw_cpus(cpu)`：返回该 CPU 所在调度域中活跃 CPU 的数量。\n- `dl_bw_capacity(cpu)`：计算调度域的总 CPU 容量（考虑异构 CPU 的 `arch_scale_cpu_capacity`）。\n- `__dl_add()` / `__dl_sub()`：向带宽池中添加或移除任务带宽，并更新 `extra_bw`（用于负载均衡）。\n- `__dl_overflow()`：检查新增带宽是否超出系统/调度域的可用带宽上限。\n\n#### 运行时带宽跟踪\n- `__add_running_bw()` / `__sub_running_bw()`：更新 `dl_rq->running_bw`（当前正在运行的 deadline 任务所消耗的带宽）。\n- `__add_rq_bw()` / `__sub_rq_bw()`：更新 `dl_rq->this_bw`（该运行队列上所有 deadline 任务的总预留带宽）。\n- `add_running_bw()` / `sub_running_bw()` / `add_rq_bw()` / `sub_rq_bw()`：带宽操作的封装，跳过“特殊”调度实体（如服务器）。\n\n#### 其他\n- `dl_server(dl_se)`：判断调度实体是否为 CBS 服务器（而非普通任务）。\n- `dl_bw_visited(cpu, gen)`：用于带宽遍历去重（SMP 场景）。\n\n### 系统控制接口（`CONFIG_SYSCTL`）\n- `sched_deadline_period_max_us`：deadline 任务周期上限（默认 ~4 秒）。\n- `sched_deadline_period_min_us`：deadline 任务周期下限（默认 100 微秒），防止定时器 DoS。\n\n## 3. 关键实现\n\n### EDF + CBS 调度模型\n- 每个 deadline 任务通过 `runtime`、`deadline`、`period` 三个参数定义其资源需求。\n- 调度器按 **绝对截止时间（absolute deadline）** 对任务排序，使用红黑树实现 O(log n) 的调度决策。\n- CBS 机制确保任务即使突发执行，也不会长期占用超过其 `runtime/period` 的 CPU 带宽，超限任务会被 throttled。\n\n### 带宽隔离与全局限制\n- 在 SMP 系统中，deadline 带宽按 **调度域（root domain）** 进行管理，防止跨 CPU 的带宽滥用。\n- 总带宽限制默认为 CPU 总容量的 95%（由 `sysctl_sched_util_clamp_min` 等机制间接控制，具体限制逻辑在带宽分配函数中体现）。\n- `dl_bw->total_bw` 跟踪已分配带宽，`__dl_overflow()` 用于在任务加入时检查是否超限。\n\n### 异构 CPU 支持\n- 通过 `arch_scale_cpu_capacity()` 获取每个 CPU 的相对性能权重。\n- `dl_bw_capacity()` 在异构系统中返回调度域内所有活跃 CPU 的容量总和，用于带宽比例计算（`cap_scale()`）。\n\n### 与 cpufreq 集成\n- 每次 `running_bw` 变化时调用 `cpufreq_update_util()`，通知 CPU 频率调节器当前 deadline 负载，确保满足实时性能需求。\n\n### 优先级继承（PI）\n- 当 deadline 任务因持有 mutex 而阻塞高优先级任务时，通过 `pi_se` 字段临时提升其调度参数，避免优先级反转。\n\n## 4. 依赖关系\n\n- **核心调度框架**：依赖 `kernel/sched/sched.h` 中定义的通用调度结构（如 `rq`、`task_struct`）和宏（如 `SCHED_CAPACITY_SCALE`）。\n- **CPU 拓扑与容量**：依赖 `arch_scale_cpu_capacity()`（由各架构实现）获取 CPU 性能信息。\n- **RCU 机制**：在 SMP 路径中大量使用 `rcu_read_lock_sched_held()` 进行锁依赖检查。\n- **cpufreq 子系统**：通过 `cpufreq_update_util()` 与 CPU 频率调节器交互。\n- **实时互斥锁**：`CONFIG_RT_MUTEXES` 启用时，支持 deadline 任务的优先级继承。\n- **Sysctl 接口**：`CONFIG_SYSCTL` 启用时，提供用户空间可调的 deadline 参数。\n\n## 5. 使用场景\n\n- **工业实时控制**：如机器人控制、数控机床等需要严格周期性和低延迟响应的场景。\n- **音视频处理**：专业音视频采集、编码、播放等对 jitter 敏感的应用。\n- **电信基础设施**：5G 基站、核心网网元中的高优先级信令处理。\n- **汽车电子**：ADAS、自动驾驶系统中的关键任务调度。\n- **科研与高性能计算**：需要确定性执行时间的实验或仿真任务。\n\n用户通过 `sched_setattr(2)` 系统调用设置任务的 `SCHED_DEADLINE` 策略及对应的 `runtime`、`deadline`、`period` 参数，内核则通过本文件实现的调度逻辑确保其满足实时性约束。",
      "similarity": 0.5651847124099731,
      "chunks": [
        {
          "chunk_id": 16,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2709,
          "end_line": 2879,
          "content": [
            "static int push_dl_task(struct rq *rq)",
            "{",
            "\tstruct task_struct *next_task;",
            "\tstruct rq *later_rq;",
            "\tint ret = 0;",
            "",
            "\tnext_task = pick_next_pushable_dl_task(rq);",
            "\tif (!next_task)",
            "\t\treturn 0;",
            "",
            "retry:",
            "\t/*",
            "\t * If next_task preempts rq->curr, and rq->curr",
            "\t * can move away, it makes sense to just reschedule",
            "\t * without going further in pushing next_task.",
            "\t */",
            "\tif (dl_task(rq->curr) &&",
            "\t    dl_time_before(next_task->dl.deadline, rq->curr->dl.deadline) &&",
            "\t    rq->curr->nr_cpus_allowed > 1) {",
            "\t\tresched_curr(rq);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (is_migration_disabled(next_task))",
            "\t\treturn 0;",
            "",
            "\tif (WARN_ON(next_task == rq->curr))",
            "\t\treturn 0;",
            "",
            "\t/* We might release rq lock */",
            "\tget_task_struct(next_task);",
            "",
            "\t/* Will lock the rq it'll find */",
            "\tlater_rq = find_lock_later_rq(next_task, rq);",
            "\tif (!later_rq) {",
            "\t\tstruct task_struct *task;",
            "",
            "\t\t/*",
            "\t\t * We must check all this again, since",
            "\t\t * find_lock_later_rq releases rq->lock and it is",
            "\t\t * then possible that next_task has migrated.",
            "\t\t */",
            "\t\ttask = pick_next_pushable_dl_task(rq);",
            "\t\tif (task == next_task) {",
            "\t\t\t/*",
            "\t\t\t * The task is still there. We don't try",
            "\t\t\t * again, some other CPU will pull it when ready.",
            "\t\t\t */",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tif (!task)",
            "\t\t\t/* No more tasks */",
            "\t\t\tgoto out;",
            "",
            "\t\tput_task_struct(next_task);",
            "\t\tnext_task = task;",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tdeactivate_task(rq, next_task, 0);",
            "\tset_task_cpu(next_task, later_rq->cpu);",
            "\tactivate_task(later_rq, next_task, 0);",
            "\tret = 1;",
            "",
            "\tresched_curr(later_rq);",
            "",
            "\tdouble_unlock_balance(rq, later_rq);",
            "",
            "out:",
            "\tput_task_struct(next_task);",
            "",
            "\treturn ret;",
            "}",
            "static void push_dl_tasks(struct rq *rq)",
            "{",
            "\t/* push_dl_task() will return true if it moved a -deadline task */",
            "\twhile (push_dl_task(rq))",
            "\t\t;",
            "}",
            "static void pull_dl_task(struct rq *this_rq)",
            "{",
            "\tint this_cpu = this_rq->cpu, cpu;",
            "\tstruct task_struct *p, *push_task;",
            "\tbool resched = false;",
            "\tstruct rq *src_rq;",
            "\tu64 dmin = LONG_MAX;",
            "",
            "\tif (likely(!dl_overloaded(this_rq)))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Match the barrier from dl_set_overloaded; this guarantees that if we",
            "\t * see overloaded we must also see the dlo_mask bit.",
            "\t */",
            "\tsmp_rmb();",
            "",
            "\tfor_each_cpu(cpu, this_rq->rd->dlo_mask) {",
            "\t\tif (this_cpu == cpu)",
            "\t\t\tcontinue;",
            "",
            "\t\tsrc_rq = cpu_rq(cpu);",
            "",
            "\t\t/*",
            "\t\t * It looks racy, abd it is! However, as in sched_rt.c,",
            "\t\t * we are fine with this.",
            "\t\t */",
            "\t\tif (this_rq->dl.dl_nr_running &&",
            "\t\t    dl_time_before(this_rq->dl.earliest_dl.curr,",
            "\t\t\t\t   src_rq->dl.earliest_dl.next))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Might drop this_rq->lock */",
            "\t\tpush_task = NULL;",
            "\t\tdouble_lock_balance(this_rq, src_rq);",
            "",
            "\t\t/*",
            "\t\t * If there are no more pullable tasks on the",
            "\t\t * rq, we're done with it.",
            "\t\t */",
            "\t\tif (src_rq->dl.dl_nr_running <= 1)",
            "\t\t\tgoto skip;",
            "",
            "\t\tp = pick_earliest_pushable_dl_task(src_rq, this_cpu);",
            "",
            "\t\t/*",
            "\t\t * We found a task to be pulled if:",
            "\t\t *  - it preempts our current (if there's one),",
            "\t\t *  - it will preempt the last one we pulled (if any).",
            "\t\t */",
            "\t\tif (p && dl_time_before(p->dl.deadline, dmin) &&",
            "\t\t    dl_task_is_earliest_deadline(p, this_rq)) {",
            "\t\t\tWARN_ON(p == src_rq->curr);",
            "\t\t\tWARN_ON(!task_on_rq_queued(p));",
            "",
            "\t\t\t/*",
            "\t\t\t * Then we pull iff p has actually an earlier",
            "\t\t\t * deadline than the current task of its runqueue.",
            "\t\t\t */",
            "\t\t\tif (dl_time_before(p->dl.deadline,",
            "\t\t\t\t\t   src_rq->curr->dl.deadline))",
            "\t\t\t\tgoto skip;",
            "",
            "\t\t\tif (is_migration_disabled(p)) {",
            "\t\t\t\tpush_task = get_push_task(src_rq);",
            "\t\t\t} else {",
            "\t\t\t\tdeactivate_task(src_rq, p, 0);",
            "\t\t\t\tset_task_cpu(p, this_cpu);",
            "\t\t\t\tactivate_task(this_rq, p, 0);",
            "\t\t\t\tdmin = p->dl.deadline;",
            "\t\t\t\tresched = true;",
            "\t\t\t}",
            "",
            "\t\t\t/* Is there any other task even earlier? */",
            "\t\t}",
            "skip:",
            "\t\tdouble_unlock_balance(this_rq, src_rq);",
            "",
            "\t\tif (push_task) {",
            "\t\t\tpreempt_disable();",
            "\t\t\traw_spin_rq_unlock(this_rq);",
            "\t\t\tstop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,",
            "\t\t\t\t\t    push_task, &src_rq->push_work);",
            "\t\t\tpreempt_enable();",
            "\t\t\traw_spin_rq_lock(this_rq);",
            "\t\t}",
            "\t}",
            "",
            "\tif (resched)",
            "\t\tresched_curr(this_rq);",
            "}"
          ],
          "function_name": "push_dl_task, push_dl_tasks, pull_dl_task",
          "description": "实现SCHED_DEADLINE任务的迁移逻辑，通过push_dl_task尝试将任务推送到更晚截止时间的CPU，pull_dl_task从过载CPU拉取任务以平衡负载",
          "similarity": 0.5756956338882446
        },
        {
          "chunk_id": 17,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2887,
          "end_line": 3034,
          "content": [
            "static void task_woken_dl(struct rq *rq, struct task_struct *p)",
            "{",
            "\tif (!task_on_cpu(rq, p) &&",
            "\t    !test_tsk_need_resched(rq->curr) &&",
            "\t    p->nr_cpus_allowed > 1 &&",
            "\t    dl_task(rq->curr) &&",
            "\t    (rq->curr->nr_cpus_allowed < 2 ||",
            "\t     !dl_entity_preempt(&p->dl, &rq->curr->dl))) {",
            "\t\tpush_dl_tasks(rq);",
            "\t}",
            "}",
            "static void set_cpus_allowed_dl(struct task_struct *p,",
            "\t\t\t\tstruct affinity_context *ctx)",
            "{",
            "\tstruct root_domain *src_rd;",
            "\tstruct rq *rq;",
            "",
            "\tWARN_ON_ONCE(!dl_task(p));",
            "",
            "\trq = task_rq(p);",
            "\tsrc_rd = rq->rd;",
            "\t/*",
            "\t * Migrating a SCHED_DEADLINE task between exclusive",
            "\t * cpusets (different root_domains) entails a bandwidth",
            "\t * update. We already made space for us in the destination",
            "\t * domain (see cpuset_can_attach()).",
            "\t */",
            "\tif (!cpumask_intersects(src_rd->span, ctx->new_mask)) {",
            "\t\tstruct dl_bw *src_dl_b;",
            "",
            "\t\tsrc_dl_b = dl_bw_of(cpu_of(rq));",
            "\t\t/*",
            "\t\t * We now free resources of the root_domain we are migrating",
            "\t\t * off. In the worst case, sched_setattr() may temporary fail",
            "\t\t * until we complete the update.",
            "\t\t */",
            "\t\traw_spin_lock(&src_dl_b->lock);",
            "\t\t__dl_sub(src_dl_b, p->dl.dl_bw, dl_bw_cpus(task_cpu(p)));",
            "\t\traw_spin_unlock(&src_dl_b->lock);",
            "\t}",
            "",
            "\tset_cpus_allowed_common(p, ctx);",
            "}",
            "static void rq_online_dl(struct rq *rq)",
            "{",
            "\tif (rq->dl.overloaded)",
            "\t\tdl_set_overload(rq);",
            "",
            "\tcpudl_set_freecpu(&rq->rd->cpudl, rq->cpu);",
            "\tif (rq->dl.dl_nr_running > 0)",
            "\t\tcpudl_set(&rq->rd->cpudl, rq->cpu, rq->dl.earliest_dl.curr);",
            "}",
            "static void rq_offline_dl(struct rq *rq)",
            "{",
            "\tif (rq->dl.overloaded)",
            "\t\tdl_clear_overload(rq);",
            "",
            "\tcpudl_clear(&rq->rd->cpudl, rq->cpu);",
            "\tcpudl_clear_freecpu(&rq->rd->cpudl, rq->cpu);",
            "}",
            "void __init init_sched_dl_class(void)",
            "{",
            "\tunsigned int i;",
            "",
            "\tfor_each_possible_cpu(i)",
            "\t\tzalloc_cpumask_var_node(&per_cpu(local_cpu_mask_dl, i),",
            "\t\t\t\t\tGFP_KERNEL, cpu_to_node(i));",
            "}",
            "void dl_add_task_root_domain(struct task_struct *p)",
            "{",
            "\tstruct rq_flags rf;",
            "\tstruct rq *rq;",
            "\tstruct dl_bw *dl_b;",
            "",
            "\traw_spin_lock_irqsave(&p->pi_lock, rf.flags);",
            "\tif (!dl_task(p)) {",
            "\t\traw_spin_unlock_irqrestore(&p->pi_lock, rf.flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\trq = __task_rq_lock(p, &rf);",
            "",
            "\tdl_b = &rq->rd->dl_bw;",
            "\traw_spin_lock(&dl_b->lock);",
            "",
            "\t__dl_add(dl_b, p->dl.dl_bw, cpumask_weight(rq->rd->span));",
            "",
            "\traw_spin_unlock(&dl_b->lock);",
            "",
            "\ttask_rq_unlock(rq, p, &rf);",
            "}",
            "void dl_clear_root_domain(struct root_domain *rd)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&rd->dl_bw.lock, flags);",
            "\trd->dl_bw.total_bw = 0;",
            "\traw_spin_unlock_irqrestore(&rd->dl_bw.lock, flags);",
            "}",
            "static void switched_from_dl(struct rq *rq, struct task_struct *p)",
            "{",
            "\t/*",
            "\t * task_non_contending() can start the \"inactive timer\" (if the 0-lag",
            "\t * time is in the future). If the task switches back to dl before",
            "\t * the \"inactive timer\" fires, it can continue to consume its current",
            "\t * runtime using its current deadline. If it stays outside of",
            "\t * SCHED_DEADLINE until the 0-lag time passes, inactive_task_timer()",
            "\t * will reset the task parameters.",
            "\t */",
            "\tif (task_on_rq_queued(p) && p->dl.dl_runtime)",
            "\t\ttask_non_contending(&p->dl);",
            "",
            "\t/*",
            "\t * In case a task is setscheduled out from SCHED_DEADLINE we need to",
            "\t * keep track of that on its cpuset (for correct bandwidth tracking).",
            "\t */",
            "\tdec_dl_tasks_cs(p);",
            "",
            "\tif (!task_on_rq_queued(p)) {",
            "\t\t/*",
            "\t\t * Inactive timer is armed. However, p is leaving DEADLINE and",
            "\t\t * might migrate away from this rq while continuing to run on",
            "\t\t * some other class. We need to remove its contribution from",
            "\t\t * this rq running_bw now, or sub_rq_bw (below) will complain.",
            "\t\t */",
            "\t\tif (p->dl.dl_non_contending)",
            "\t\t\tsub_running_bw(&p->dl, &rq->dl);",
            "\t\tsub_rq_bw(&p->dl, &rq->dl);",
            "\t}",
            "",
            "\t/*",
            "\t * We cannot use inactive_task_timer() to invoke sub_running_bw()",
            "\t * at the 0-lag time, because the task could have been migrated",
            "\t * while SCHED_OTHER in the meanwhile.",
            "\t */",
            "\tif (p->dl.dl_non_contending)",
            "\t\tp->dl.dl_non_contending = 0;",
            "",
            "\t/*",
            "\t * Since this might be the only -deadline task on the rq,",
            "\t * this is the right place to try to pull some other one",
            "\t * from an overloaded CPU, if any.",
            "\t */",
            "\tif (!task_on_rq_queued(p) || rq->dl.dl_nr_running)",
            "\t\treturn;",
            "",
            "\tdeadline_queue_pull_task(rq);",
            "}"
          ],
          "function_name": "task_woken_dl, set_cpus_allowed_dl, rq_online_dl, rq_offline_dl, init_sched_dl_class, dl_add_task_root_domain, dl_clear_root_domain, switched_from_dl",
          "description": "管理SCHED_DEADLINE任务唤醒后的处理、CPU亲和性变更、根域带宽分配及任务状态切换时的资源回收与重新计算",
          "similarity": 0.5499417185783386
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 514,
          "end_line": 616,
          "content": [
            "static inline int is_leftmost(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\treturn rb_first_cached(&dl_rq->root) == &dl_se->rb_node;",
            "}",
            "void init_dl_bw(struct dl_bw *dl_b)",
            "{",
            "\traw_spin_lock_init(&dl_b->lock);",
            "\tif (global_rt_runtime() == RUNTIME_INF)",
            "\t\tdl_b->bw = -1;",
            "\telse",
            "\t\tdl_b->bw = to_ratio(global_rt_period(), global_rt_runtime());",
            "\tdl_b->total_bw = 0;",
            "}",
            "void init_dl_rq(struct dl_rq *dl_rq)",
            "{",
            "\tdl_rq->root = RB_ROOT_CACHED;",
            "",
            "#ifdef CONFIG_SMP",
            "\t/* zero means no -deadline tasks */",
            "\tdl_rq->earliest_dl.curr = dl_rq->earliest_dl.next = 0;",
            "",
            "\tdl_rq->overloaded = 0;",
            "\tdl_rq->pushable_dl_tasks_root = RB_ROOT_CACHED;",
            "#else",
            "\tinit_dl_bw(&dl_rq->dl_bw);",
            "#endif",
            "",
            "\tdl_rq->running_bw = 0;",
            "\tdl_rq->this_bw = 0;",
            "\tinit_dl_rq_bw_ratio(dl_rq);",
            "}",
            "static inline int dl_overloaded(struct rq *rq)",
            "{",
            "\treturn atomic_read(&rq->rd->dlo_count);",
            "}",
            "static inline void dl_set_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tcpumask_set_cpu(rq->cpu, rq->rd->dlo_mask);",
            "\t/*",
            "\t * Must be visible before the overload count is",
            "\t * set (as in sched_rt.c).",
            "\t *",
            "\t * Matched by the barrier in pull_dl_task().",
            "\t */",
            "\tsmp_wmb();",
            "\tatomic_inc(&rq->rd->dlo_count);",
            "}",
            "static inline void dl_clear_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tatomic_dec(&rq->rd->dlo_count);",
            "\tcpumask_clear_cpu(rq->cpu, rq->rd->dlo_mask);",
            "}",
            "static inline bool __pushable_less(struct rb_node *a, const struct rb_node *b)",
            "{",
            "\treturn dl_entity_preempt(&__node_2_pdl(a)->dl, &__node_2_pdl(b)->dl);",
            "}",
            "static inline int has_pushable_dl_tasks(struct rq *rq)",
            "{",
            "\treturn !RB_EMPTY_ROOT(&rq->dl.pushable_dl_tasks_root.rb_root);",
            "}",
            "static void enqueue_pushable_dl_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tstruct rb_node *leftmost;",
            "",
            "\tWARN_ON_ONCE(!RB_EMPTY_NODE(&p->pushable_dl_tasks));",
            "",
            "\tleftmost = rb_add_cached(&p->pushable_dl_tasks,",
            "\t\t\t\t &rq->dl.pushable_dl_tasks_root,",
            "\t\t\t\t __pushable_less);",
            "\tif (leftmost)",
            "\t\trq->dl.earliest_dl.next = p->dl.deadline;",
            "",
            "\tif (!rq->dl.overloaded) {",
            "\t\tdl_set_overload(rq);",
            "\t\trq->dl.overloaded = 1;",
            "\t}",
            "}",
            "static void dequeue_pushable_dl_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tstruct dl_rq *dl_rq = &rq->dl;",
            "\tstruct rb_root_cached *root = &dl_rq->pushable_dl_tasks_root;",
            "\tstruct rb_node *leftmost;",
            "",
            "\tif (RB_EMPTY_NODE(&p->pushable_dl_tasks))",
            "\t\treturn;",
            "",
            "\tleftmost = rb_erase_cached(&p->pushable_dl_tasks, root);",
            "\tif (leftmost)",
            "\t\tdl_rq->earliest_dl.next = __node_2_pdl(leftmost)->dl.deadline;",
            "",
            "\tRB_CLEAR_NODE(&p->pushable_dl_tasks);",
            "",
            "\tif (!has_pushable_dl_tasks(rq) && rq->dl.overloaded) {",
            "\t\tdl_clear_overload(rq);",
            "\t\trq->dl.overloaded = 0;",
            "\t}",
            "}"
          ],
          "function_name": "is_leftmost, init_dl_bw, init_dl_rq, dl_overloaded, dl_set_overload, dl_clear_overload, __pushable_less, has_pushable_dl_tasks, enqueue_pushable_dl_task, dequeue_pushable_dl_task",
          "description": "实现截止时间调度的抢占判定和过载管理机制，包含任务优先级比较、过载标记维护及可推送任务的数据结构操作。",
          "similarity": 0.5472328662872314
        },
        {
          "chunk_id": 19,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 3216,
          "end_line": 3324,
          "content": [
            "static void init_dl_rq_bw_ratio(struct dl_rq *dl_rq)",
            "{",
            "\tif (global_rt_runtime() == RUNTIME_INF) {",
            "\t\tdl_rq->bw_ratio = 1 << RATIO_SHIFT;",
            "\t\tdl_rq->max_bw = dl_rq->extra_bw = 1 << BW_SHIFT;",
            "\t} else {",
            "\t\tdl_rq->bw_ratio = to_ratio(global_rt_runtime(),",
            "\t\t\t  global_rt_period()) >> (BW_SHIFT - RATIO_SHIFT);",
            "\t\tdl_rq->max_bw = dl_rq->extra_bw =",
            "\t\t\tto_ratio(global_rt_period(), global_rt_runtime());",
            "\t}",
            "}",
            "void sched_dl_do_global(void)",
            "{",
            "\tu64 new_bw = -1;",
            "\tu64 gen = ++dl_generation;",
            "\tstruct dl_bw *dl_b;",
            "\tint cpu;",
            "\tunsigned long flags;",
            "",
            "\tif (global_rt_runtime() != RUNTIME_INF)",
            "\t\tnew_bw = to_ratio(global_rt_period(), global_rt_runtime());",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trcu_read_lock_sched();",
            "",
            "\t\tif (dl_bw_visited(cpu, gen)) {",
            "\t\t\trcu_read_unlock_sched();",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tdl_b = dl_bw_of(cpu);",
            "",
            "\t\traw_spin_lock_irqsave(&dl_b->lock, flags);",
            "\t\tdl_b->bw = new_bw;",
            "\t\traw_spin_unlock_irqrestore(&dl_b->lock, flags);",
            "",
            "\t\trcu_read_unlock_sched();",
            "\t\tinit_dl_rq_bw_ratio(&cpu_rq(cpu)->dl);",
            "\t}",
            "}",
            "int sched_dl_overflow(struct task_struct *p, int policy,",
            "\t\t      const struct sched_attr *attr)",
            "{",
            "\tu64 period = attr->sched_period ?: attr->sched_deadline;",
            "\tu64 runtime = attr->sched_runtime;",
            "\tu64 new_bw = dl_policy(policy) ? to_ratio(period, runtime) : 0;",
            "\tint cpus, err = -1, cpu = task_cpu(p);",
            "\tstruct dl_bw *dl_b = dl_bw_of(cpu);",
            "\tunsigned long cap;",
            "",
            "\tif (attr->sched_flags & SCHED_FLAG_SUGOV)",
            "\t\treturn 0;",
            "",
            "\t/* !deadline task may carry old deadline bandwidth */",
            "\tif (new_bw == p->dl.dl_bw && task_has_dl_policy(p))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Either if a task, enters, leave, or stays -deadline but changes",
            "\t * its parameters, we may need to update accordingly the total",
            "\t * allocated bandwidth of the container.",
            "\t */",
            "\traw_spin_lock(&dl_b->lock);",
            "\tcpus = dl_bw_cpus(cpu);",
            "\tcap = dl_bw_capacity(cpu);",
            "",
            "\tif (dl_policy(policy) && !task_has_dl_policy(p) &&",
            "\t    !__dl_overflow(dl_b, cap, 0, new_bw)) {",
            "\t\tif (hrtimer_active(&p->dl.inactive_timer))",
            "\t\t\t__dl_sub(dl_b, p->dl.dl_bw, cpus);",
            "\t\t__dl_add(dl_b, new_bw, cpus);",
            "\t\terr = 0;",
            "\t} else if (dl_policy(policy) && task_has_dl_policy(p) &&",
            "\t\t   !__dl_overflow(dl_b, cap, p->dl.dl_bw, new_bw)) {",
            "\t\t/*",
            "\t\t * XXX this is slightly incorrect: when the task",
            "\t\t * utilization decreases, we should delay the total",
            "\t\t * utilization change until the task's 0-lag point.",
            "\t\t * But this would require to set the task's \"inactive",
            "\t\t * timer\" when the task is not inactive.",
            "\t\t */",
            "\t\t__dl_sub(dl_b, p->dl.dl_bw, cpus);",
            "\t\t__dl_add(dl_b, new_bw, cpus);",
            "\t\tdl_change_utilization(p, new_bw);",
            "\t\terr = 0;",
            "\t} else if (!dl_policy(policy) && task_has_dl_policy(p)) {",
            "\t\t/*",
            "\t\t * Do not decrease the total deadline utilization here,",
            "\t\t * switched_from_dl() will take care to do it at the correct",
            "\t\t * (0-lag) time.",
            "\t\t */",
            "\t\terr = 0;",
            "\t}",
            "\traw_spin_unlock(&dl_b->lock);",
            "",
            "\treturn err;",
            "}",
            "void __setparam_dl(struct task_struct *p, const struct sched_attr *attr)",
            "{",
            "\tstruct sched_dl_entity *dl_se = &p->dl;",
            "",
            "\tdl_se->dl_runtime = attr->sched_runtime;",
            "\tdl_se->dl_deadline = attr->sched_deadline;",
            "\tdl_se->dl_period = attr->sched_period ?: dl_se->dl_deadline;",
            "\tdl_se->flags = attr->sched_flags & SCHED_DL_FLAGS;",
            "\tdl_se->dl_bw = to_ratio(dl_se->dl_period, dl_se->dl_runtime);",
            "\tdl_se->dl_density = to_ratio(dl_se->dl_deadline, dl_se->dl_runtime);",
            "}"
          ],
          "function_name": "init_dl_rq_bw_ratio, sched_dl_do_global, sched_dl_overflow, __setparam_dl",
          "description": "维护SCHED_DEADLINE全局带宽比例配置，处理任务参数变更引发的带宽调整，确保任务利用率不超过系统总容量限制",
          "similarity": 0.5404619574546814
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2104,
          "end_line": 2251,
          "content": [
            "static void enqueue_task_dl(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tif (is_dl_boosted(&p->dl)) {",
            "\t\t/*",
            "\t\t * Because of delays in the detection of the overrun of a",
            "\t\t * thread's runtime, it might be the case that a thread",
            "\t\t * goes to sleep in a rt mutex with negative runtime. As",
            "\t\t * a consequence, the thread will be throttled.",
            "\t\t *",
            "\t\t * While waiting for the mutex, this thread can also be",
            "\t\t * boosted via PI, resulting in a thread that is throttled",
            "\t\t * and boosted at the same time.",
            "\t\t *",
            "\t\t * In this case, the boost overrides the throttle.",
            "\t\t */",
            "\t\tif (p->dl.dl_throttled) {",
            "\t\t\t/*",
            "\t\t\t * The replenish timer needs to be canceled. No",
            "\t\t\t * problem if it fires concurrently: boosted threads",
            "\t\t\t * are ignored in dl_task_timer().",
            "\t\t\t *",
            "\t\t\t * If the timer callback was running (hrtimer_try_to_cancel == -1),",
            "\t\t\t * it will eventually call put_task_struct().",
            "\t\t\t */",
            "\t\t\tif (hrtimer_try_to_cancel(&p->dl.dl_timer) == 1 &&",
            "\t\t\t    !dl_server(&p->dl))",
            "\t\t\t\tput_task_struct(p);",
            "\t\t\tp->dl.dl_throttled = 0;",
            "\t\t}",
            "\t} else if (!dl_prio(p->normal_prio)) {",
            "\t\t/*",
            "\t\t * Special case in which we have a !SCHED_DEADLINE task that is going",
            "\t\t * to be deboosted, but exceeds its runtime while doing so. No point in",
            "\t\t * replenishing it, as it's going to return back to its original",
            "\t\t * scheduling class after this. If it has been throttled, we need to",
            "\t\t * clear the flag, otherwise the task may wake up as throttled after",
            "\t\t * being boosted again with no means to replenish the runtime and clear",
            "\t\t * the throttle.",
            "\t\t */",
            "\t\tp->dl.dl_throttled = 0;",
            "\t\tif (!(flags & ENQUEUE_REPLENISH))",
            "\t\t\tprintk_deferred_once(\"sched: DL de-boosted task PID %d: REPLENISH flag missing\\n\",",
            "\t\t\t\t\t     task_pid_nr(p));",
            "",
            "\t\treturn;",
            "\t}",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_dl(dl_rq_of_se(&p->dl), &p->dl);",
            "",
            "\tif (p->on_rq == TASK_ON_RQ_MIGRATING)",
            "\t\tflags |= ENQUEUE_MIGRATING;",
            "",
            "\tenqueue_dl_entity(&p->dl, flags);",
            "",
            "\tif (dl_server(&p->dl))",
            "\t\treturn;",
            "",
            "\tif (!task_current(rq, p) && !p->dl.dl_throttled && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_dl_task(rq, p);",
            "}",
            "static bool dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tupdate_curr_dl(rq);",
            "",
            "\tif (p->on_rq == TASK_ON_RQ_MIGRATING)",
            "\t\tflags |= DEQUEUE_MIGRATING;",
            "",
            "\tdequeue_dl_entity(&p->dl, flags);",
            "\tif (!p->dl.dl_throttled && !dl_server(&p->dl))",
            "\t\tdequeue_pushable_dl_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void yield_task_dl(struct rq *rq)",
            "{",
            "\t/*",
            "\t * We make the task go to sleep until its current deadline by",
            "\t * forcing its runtime to zero. This way, update_curr_dl() stops",
            "\t * it and the bandwidth timer will wake it up and will give it",
            "\t * new scheduling parameters (thanks to dl_yielded=1).",
            "\t */",
            "\trq->curr->dl.dl_yielded = 1;",
            "",
            "\tupdate_rq_clock(rq);",
            "\tupdate_curr_dl(rq);",
            "\t/*",
            "\t * Tell update_rq_clock() that we've just updated,",
            "\t * so we don't do microscopic update in schedule()",
            "\t * and double the fastpath cost.",
            "\t */",
            "\trq_clock_skip_update(rq);",
            "}",
            "static inline bool dl_task_is_earliest_deadline(struct task_struct *p,",
            "\t\t\t\t\t\t struct rq *rq)",
            "{",
            "\treturn (!rq->dl.dl_nr_running ||",
            "\t\tdl_time_before(p->dl.deadline,",
            "\t\t\t       rq->dl.earliest_dl.curr));",
            "}",
            "static int",
            "select_task_rq_dl(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tbool select_rq;",
            "\tstruct rq *rq;",
            "",
            "\tif (!(flags & WF_TTWU))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If we are dealing with a -deadline task, we must",
            "\t * decide where to wake it up.",
            "\t * If it has a later deadline and the current task",
            "\t * on this rq can't move (provided the waking task",
            "\t * can!) we prefer to send it somewhere else. On the",
            "\t * other hand, if it has a shorter deadline, we",
            "\t * try to make it stay here, it might be important.",
            "\t */",
            "\tselect_rq = unlikely(dl_task(curr)) &&",
            "\t\t    (curr->nr_cpus_allowed < 2 ||",
            "\t\t     !dl_entity_preempt(&p->dl, &curr->dl)) &&",
            "\t\t    p->nr_cpus_allowed > 1;",
            "",
            "\t/*",
            "\t * Take the capacity of the CPU into account to",
            "\t * ensure it fits the requirement of the task.",
            "\t */",
            "\tif (sched_asym_cpucap_active())",
            "\t\tselect_rq |= !dl_task_fits_capacity(p, cpu);",
            "",
            "\tif (select_rq) {",
            "\t\tint target = find_later_rq(p);",
            "",
            "\t\tif (target != -1 &&",
            "\t\t    dl_task_is_earliest_deadline(p, cpu_rq(target)))",
            "\t\t\tcpu = target;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "enqueue_task_dl, dequeue_task_dl, yield_task_dl, dl_task_is_earliest_deadline, select_task_rq_dl",
          "description": "处理截止时间任务的调度决策，包含任务入队出队、抢占检查、CPU选择及负载均衡逻辑，通过dl_task_is_earliest_deadline判断任务截止时间优先级并选择合适CPU",
          "similarity": 0.5399859547615051
        }
      ]
    },
    {
      "source_file": "kernel/sched/sched.h",
      "md_summary": "> 自动生成时间: 2025-10-25 16:16:13\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\sched.h`\n\n---\n\n# `sched/sched.h` 技术文档\n\n## 1. 文件概述\n\n`sched/sched.h` 是 Linux 内核调度器（Scheduler）的核心内部头文件，定义了调度子系统内部使用的类型、宏、辅助函数和全局变量。该文件不对外暴露给其他子系统直接使用，而是作为调度器各组件（如 CFS、RT、Deadline 调度类）之间的内部接口和共享基础设施。它整合了任务状态管理、负载计算、策略判断、CPU 能力建模、cgroup 权重转换等关键调度逻辑，并为调试、性能追踪和平台适配提供支持。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct asym_cap_data`：用于描述非对称 CPU 架构中不同 CPU 集合的计算能力（capacity），支持异构多核系统（如 big.LITTLE）的调度优化。\n- `struct rq`（前向声明）：运行队列（runqueue）结构体，每个 CPU 对应一个，是调度器管理可运行任务的核心数据结构。\n- `struct cpuidle_state`（前向声明）：CPU 空闲状态信息，用于与调度器协同进行能效管理。\n\n### 关键全局变量\n- `scheduler_running`：标志调度器是否已启动。\n- `calc_load_update` / `calc_load_tasks`：用于全局负载（load average）计算的时间戳和任务计数。\n- `sysctl_sched_rt_period` / `sysctl_sched_rt_runtime`：实时任务带宽控制参数。\n- `sched_rr_timeslice`：SCHED_RR 策略的时间片长度。\n- `asym_cap_list`：非对称 CPU 能力数据的全局链表。\n\n### 核心辅助函数与宏\n- **任务策略判断函数**：\n  - `idle_policy()` / `task_has_idle_policy()`\n  - `normal_policy()` / `fair_policy()`\n  - `rt_policy()` / `task_has_rt_policy()`\n  - `dl_policy()` / `task_has_dl_policy()`\n  - `valid_policy()`\n- **负载与权重转换**：\n  - `scale_load()` / `scale_load_down()`：在内部高精度负载值与用户可见权重间转换。\n  - `sched_weight_from_cgroup()` / `sched_weight_to_cgroup()`：cgroup 权重与调度器内部权重的映射。\n- **时间与精度处理**：\n  - `NS_TO_JIFFIES()`：纳秒转 jiffies。\n  - `update_avg()`：指数移动平均（EMA）更新。\n  - `shr_bound()`：安全右移，避免未定义行为。\n- **特殊调度标志**：\n  - `SCHED_FLAG_SUGOV`：用于 schedutil 频率调节器的特殊标志，使相关 kworker 临时获得高于 SCHED_DEADLINE 的优先级。\n  - `dl_entity_is_special()`：判断 Deadline 实体是否为 SUGOV 特殊任务。\n\n### 重要宏定义\n- `TASK_ON_RQ_QUEUED` / `TASK_ON_RQ_MIGRATING`：`task_struct::on_rq` 字段的状态值。\n- `NICE_0_LOAD`：nice 值为 0 的任务对应的内部负载基准值。\n- `DL_SCALE`：SCHED_DEADLINE 内部计算的精度因子。\n- `RUNTIME_INF`：表示无限运行时间的常量。\n- `SCHED_WARN_ON()`：调度器专用的条件警告宏（仅在 `CONFIG_SCHED_DEBUG` 时生效）。\n\n## 3. 关键实现\n\n### 高精度负载计算（64 位优化）\n在 64 位架构上，通过 `NICE_0_LOAD_SHIFT = 2 * SCHED_FIXEDPOINT_SHIFT` 提升内部负载计算的精度，改善低权重任务组（如 nice +19）和深层 cgroup 层级的负载均衡效果。`scale_load()` 和 `scale_load_down()` 实现了用户权重与内部高精度负载值之间的无损转换。\n\n### 非对称 CPU 能力建模\n`asym_cap_data` 结构体结合 `cpu_capacity_span()` 宏，将具有相同计算能力的 CPU 归为一组，并通过全局链表 `asym_cap_list` 管理。这为调度器在异构系统中进行负载均衡和任务迁移提供关键拓扑信息。\n\n### cgroup 权重标准化\n通过 `sched_weight_from_cgroup()` 和 `sched_weight_to_cgroup()`，将 cgroup 接口的权重范围（1–10000，默认 100）映射到调度器内部使用的权重值（基于 1024 基准），确保用户配置与调度行为的一致性。\n\n### SCHED_DEADLINE 与频率调节协同\n引入 `SCHED_FLAG_SUGOV` 标志，允许 `schedutil` 频率调节器的工作线程在需要时临时突破 SCHED_DEADLINE 的优先级限制，以解决某些平台无法原子切换 CPU 频率的问题。这是一种临时性 workaround，依赖于 `dl_entity_is_special()` 进行识别。\n\n### 安全位运算\n`shr_bound()` 宏确保右移操作不会因移位数过大而触发未定义行为（UB），通过 `min_t()` 将移位数限制在 `BITS_PER_TYPE(val) - 1` 以内。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- **调度子系统内部**：包含多个调度相关子模块头文件（如 `affinity.h`, `deadline.h`, `topology.h`, `cpupri.h` 等）。\n- **核心内核设施**：依赖 `atomic.h`, `rcupdate.h`, `cpumask_api.h`, `ktime_api.h`, `trace/events/sched.h` 等。\n- **平台与虚拟化**：条件包含 `asm/paravirt.h`（半虚拟化支持）和 `asm/barrier.h`（内存屏障）。\n- **工作队列**：包含 `../workqueue_internal.h`，用于与工作队列子系统交互。\n\n### 配置选项依赖\n- `CONFIG_64BIT`：启用高精度负载计算。\n- `CONFIG_SCHED_DEBUG`：启用 `SCHED_WARN_ON()` 调试检查。\n- `CONFIG_CPU_FREQ_GOV_SCHEDUTIL`：启用 `SCHED_FLAG_SUGOV` 相关逻辑。\n- `CONFIG_SCHED_CLASS_EXT`：扩展调度类支持（影响 `normal_policy()` 判断）。\n- `CONFIG_PARAVIRT`：半虚拟化支持。\n\n## 5. 使用场景\n\n- **调度器初始化与运行**：`scheduler_running` 和负载计算变量在调度器启动和周期性负载更新中使用。\n- **任务调度策略处理**：所有调度类（CFS、RT、Deadline、Idle）在入队、出队、选择下一个任务时，通过策略判断函数确定任务类型。\n- **负载均衡与迁移**：`asym_cap_data` 和 CPU 拓扑信息用于跨 CPU 的任务迁移决策，尤其在异构系统中。\n- **cgroup 资源控制**：在设置或读取 cgroup 的 CPU 权重时，通过权重转换函数确保调度器内部表示与用户接口一致。\n- **实时带宽管理**：`sysctl_sched_rt_*` 参数用于限制 SCHED_FIFO/SCHED_RR 任务的 CPU 使用率。\n- **能效调度协同**：`SCHED_FLAG_SUGOV` 机制使频率调节器能及时响应 Deadline 任务的性能需求。\n- **内核调试与追踪**：`SCHED_WARN_ON()` 用于捕获调度器内部异常状态；tracepoint 定义支持调度事件追踪。",
      "similarity": 0.563927173614502,
      "chunks": []
    }
  ]
}