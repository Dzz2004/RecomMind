{
  "query": "缓存数据结构",
  "timestamp": "2025-12-26 01:42:02",
  "retrieved_files": [
    {
      "source_file": "kernel/trace/ring_buffer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:07:21\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `trace\\ring_buffer.c`\n\n---\n\n# `trace/ring_buffer.c` 技术文档\n\n## 1. 文件概述\n\n`trace/ring_buffer.c` 实现了 Linux 内核中通用的高性能环形缓冲区（ring buffer）机制，主要用于跟踪（tracing）子系统。该缓冲区支持多 CPU 并发写入、单读者或多读者无锁读取，并通过时间戳压缩、事件类型编码和页面交换等技术优化内存使用和性能。该实现是 ftrace、perf 和其他内核跟踪工具的核心基础设施。\n\n## 2. 核心功能\n\n### 主要函数\n- `ring_buffer_print_entry_header()`：输出环形缓冲区条目头部格式说明，用于调试或用户空间解析。\n- `ring_buffer_event_length()`：返回事件有效载荷（payload）的长度，对 TIME_EXTEND 类型自动跳过扩展头。\n- `rb_event_data()`（内联）：返回指向事件实际数据的指针，处理 TIME_EXTEND 和不同长度编码。\n- `rb_event_length()`：返回完整事件结构（含头部）的字节长度。\n- `rb_event_ts_length()`：返回 TIME_EXTEND 事件及其后续数据事件的总长度。\n- `rb_event_data_length()`：计算数据类型事件的总长度（含头部）。\n- `rb_null_event()` / `rb_event_set_padding()`：判断或设置空/填充事件。\n\n### 关键数据结构（隐含或引用）\n- `struct ring_buffer_event`：环形缓冲区中每个事件的通用头部结构。\n- `struct buffer_data_page`：每个 CPU 缓冲区页面的封装，包含数据和元数据。\n- 每 CPU 页面链表：每个 CPU 拥有独立的环形页面链，写者仅写本地 CPU 缓冲区。\n\n### 核心常量与宏\n- `RINGBUF_TYPE_PADDING`、`RINGBUF_TYPE_TIME_EXTEND`、`RINGBUF_TYPE_TIME_STAMP`、`RINGBUF_TYPE_DATA`：事件类型标识。\n- `RB_ALIGNMENT` / `RB_ARCH_ALIGNMENT`：数据对齐策略，根据架构是否支持 64 位对齐访问调整。\n- `RB_MAX_SMALL_DATA`：小数据事件的最大长度（基于 4 字节对齐和类型长度上限）。\n- `TS_MSB` / `ABS_TS_MASK`：用于处理 59 位时间戳的高位截断与恢复。\n\n## 3. 关键实现\n\n### 无锁读写架构\n- **写者**：每个 CPU 只能写入其对应的 per-CPU 缓冲区，通过原子操作和内存屏障保证写入一致性，无需全局锁。\n- **读者**：每个 per-CPU 缓冲区维护一个独立的“reader page”。当 reader page 被读完后，通过原子交换（未来使用 `cmpxchg`）将其与环形缓冲区中的一个页面互换。交换后，原 reader page 不再被写者访问，读者可安全地将其用于 splice、复制或释放。\n\n### 事件编码与压缩\n- 事件头部使用紧凑位域编码：\n  - `type_len`（5 位）：事件类型或小数据长度（≤31）。\n  - `time_delta`（27 位）：相对于前一事件的时间增量。\n  - `array`（32 位）：用于存储大长度值或事件数据。\n- **TIME_EXTEND 事件**：当时间增量超出 27 位或需要绝对时间戳时，插入一个 8 字节的 TIME_EXTEND 事件，后跟实际数据事件。\n- **数据长度编码**：\n  - 若 `type_len > 0` 且 ≤ `RINGBUF_TYPE_DATA_TYPE_LEN_MAX`，则数据长度 = `type_len * RB_ALIGNMENT`，数据从 `array[0]` 开始。\n  - 否则，数据长度存储在 `array[0]`，实际数据从 `array[1]` 开始。\n\n### 时间戳处理\n- 绝对时间戳仅保留低 59 位（`ABS_TS_MASK`），高 5 位（`TS_MSB`）若非零需单独保存并在读取时恢复，以支持长时间运行的跟踪。\n\n### 内存对齐优化\n- 在支持 64 位对齐访问的架构上（`CONFIG_HAVE_64BIT_ALIGNED_ACCESS`），强制 8 字节对齐（`RB_FORCE_8BYTE_ALIGNMENT`），提升访问性能；否则使用 4 字节对齐。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/ring_buffer.h>`：定义公共 API 和数据结构。\n  - `<linux/trace_clock.h>`、`<linux/sched/clock.h>`：提供高精度时间戳源。\n  - `<linux/percpu.h>`：支持 per-CPU 缓冲区分配。\n  - `<linux/spinlock.h>`、`<asm/local.h>`：提供底层原子操作和锁原语。\n  - `<linux/trace_recursion.h>`：防止跟踪递归。\n- **子系统依赖**：\n  - **ftrace**：主要消费者，用于函数跟踪、事件跟踪等。\n  - **perf**：通过 ring buffer 获取性能事件数据。\n  - **Security Module**：通过 `<linux/security.h>` 集成 LSM 钩子（如 trace 访问控制）。\n- **架构依赖**：依赖 `CONFIG_HAVE_64BIT_ALIGNED_ACCESS` 配置项优化对齐策略。\n\n## 5. 使用场景\n\n- **内核跟踪（ftrace）**：记录函数调用、上下文切换、中断等事件，数据写入 per-CPU ring buffer，用户通过 `tracefs` 读取。\n- **性能分析（perf）**：perf 工具通过 ring buffer 接收内核采样事件（如 PMU 中断、软件事件）。\n- **实时监控与调试**：开发者或运维人员通过读取 ring buffer 内容分析系统行为、延迟或错误。\n- **自测试（selftest）**：文件包含自测试逻辑（依赖 `<linux/kthread.h>`），用于验证 ring buffer 功能正确性。\n- **低开销事件记录**：由于其无锁设计和压缩编码，适用于高频事件记录场景（如每秒百万级事件）。",
      "similarity": 0.618321418762207,
      "chunks": [
        {
          "chunk_id": 10,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 2052,
          "end_line": 2363,
          "content": [
            "static bool",
            "rb_insert_pages(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tstruct list_head *pages = &cpu_buffer->new_pages;",
            "\tunsigned long flags;",
            "\tbool success;",
            "\tint retries;",
            "",
            "\t/* Can be called at early boot up, where interrupts must not been enabled */",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t/*",
            "\t * We are holding the reader lock, so the reader page won't be swapped",
            "\t * in the ring buffer. Now we are racing with the writer trying to",
            "\t * move head page and the tail page.",
            "\t * We are going to adapt the reader page update process where:",
            "\t * 1. We first splice the start and end of list of new pages between",
            "\t *    the head page and its previous page.",
            "\t * 2. We cmpxchg the prev_page->next to point from head page to the",
            "\t *    start of new pages list.",
            "\t * 3. Finally, we update the head->prev to the end of new list.",
            "\t *",
            "\t * We will try this process 10 times, to make sure that we don't keep",
            "\t * spinning.",
            "\t */",
            "\tretries = 10;",
            "\tsuccess = false;",
            "\twhile (retries--) {",
            "\t\tstruct list_head *head_page, *prev_page, *r;",
            "\t\tstruct list_head *last_page, *first_page;",
            "\t\tstruct list_head *head_page_with_bit;",
            "\t\tstruct buffer_page *hpage = rb_set_head_page(cpu_buffer);",
            "",
            "\t\tif (!hpage)",
            "\t\t\tbreak;",
            "\t\thead_page = &hpage->list;",
            "\t\tprev_page = head_page->prev;",
            "",
            "\t\tfirst_page = pages->next;",
            "\t\tlast_page  = pages->prev;",
            "",
            "\t\thead_page_with_bit = (struct list_head *)",
            "\t\t\t\t     ((unsigned long)head_page | RB_PAGE_HEAD);",
            "",
            "\t\tlast_page->next = head_page_with_bit;",
            "\t\tfirst_page->prev = prev_page;",
            "",
            "\t\tr = cmpxchg(&prev_page->next, head_page_with_bit, first_page);",
            "",
            "\t\tif (r == head_page_with_bit) {",
            "\t\t\t/*",
            "\t\t\t * yay, we replaced the page pointer to our new list,",
            "\t\t\t * now, we just have to update to head page's prev",
            "\t\t\t * pointer to point to end of list",
            "\t\t\t */",
            "\t\t\thead_page->prev = last_page;",
            "\t\t\tsuccess = true;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tif (success)",
            "\t\tINIT_LIST_HEAD(pages);",
            "\t/*",
            "\t * If we weren't successful in adding in new pages, warn and stop",
            "\t * tracing",
            "\t */",
            "\tRB_WARN_ON(cpu_buffer, !success);",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "",
            "\t/* free pages if they weren't inserted */",
            "\tif (!success) {",
            "\t\tstruct buffer_page *bpage, *tmp;",
            "\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,",
            "\t\t\t\t\t list) {",
            "\t\t\tlist_del_init(&bpage->list);",
            "\t\t\tfree_buffer_page(bpage);",
            "\t\t}",
            "\t}",
            "\treturn success;",
            "}",
            "static void rb_update_pages(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tbool success;",
            "",
            "\tif (cpu_buffer->nr_pages_to_update > 0)",
            "\t\tsuccess = rb_insert_pages(cpu_buffer);",
            "\telse",
            "\t\tsuccess = rb_remove_pages(cpu_buffer,",
            "\t\t\t\t\t-cpu_buffer->nr_pages_to_update);",
            "",
            "\tif (success)",
            "\t\tcpu_buffer->nr_pages += cpu_buffer->nr_pages_to_update;",
            "}",
            "static void update_pages_handler(struct work_struct *work)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = container_of(work,",
            "\t\t\tstruct ring_buffer_per_cpu, update_pages_work);",
            "\trb_update_pages(cpu_buffer);",
            "\tcomplete(&cpu_buffer->update_done);",
            "}",
            "int ring_buffer_resize(struct trace_buffer *buffer, unsigned long size,",
            "\t\t\tint cpu_id)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tunsigned long nr_pages;",
            "\tint cpu, err;",
            "",
            "\t/*",
            "\t * Always succeed at resizing a non-existent buffer:",
            "\t */",
            "\tif (!buffer)",
            "\t\treturn 0;",
            "",
            "\t/* Make sure the requested buffer exists */",
            "\tif (cpu_id != RING_BUFFER_ALL_CPUS &&",
            "\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))",
            "\t\treturn 0;",
            "",
            "\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);",
            "",
            "\t/* we need a minimum of two pages */",
            "\tif (nr_pages < 2)",
            "\t\tnr_pages = 2;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "\tatomic_inc(&buffer->resizing);",
            "",
            "\tif (cpu_id == RING_BUFFER_ALL_CPUS) {",
            "\t\t/*",
            "\t\t * Don't succeed if resizing is disabled, as a reader might be",
            "\t\t * manipulating the ring buffer and is expecting a sane state while",
            "\t\t * this is true.",
            "\t\t */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {",
            "\t\t\t\terr = -EBUSY;",
            "\t\t\t\tgoto out_err_unlock;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* calculate the pages to update */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -",
            "\t\t\t\t\t\t\tcpu_buffer->nr_pages;",
            "\t\t\t/*",
            "\t\t\t * nothing more to do for removing pages or no update",
            "\t\t\t */",
            "\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)",
            "\t\t\t\tcontinue;",
            "\t\t\t/*",
            "\t\t\t * to add pages, make sure all new pages can be",
            "\t\t\t * allocated without receiving ENOMEM",
            "\t\t\t */",
            "\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\t\t\tif (__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,",
            "\t\t\t\t\t\t&cpu_buffer->new_pages)) {",
            "\t\t\t\t/* not enough memory for new pages */",
            "\t\t\t\terr = -ENOMEM;",
            "\t\t\t\tgoto out_err;",
            "\t\t\t}",
            "",
            "\t\t\tcond_resched();",
            "\t\t}",
            "",
            "\t\tcpus_read_lock();",
            "\t\t/*",
            "\t\t * Fire off all the required work handlers",
            "\t\t * We can't schedule on offline CPUs, but it's not necessary",
            "\t\t * since we can change their buffer sizes without any race.",
            "\t\t */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (!cpu_buffer->nr_pages_to_update)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\t/* Can't run something on an offline CPU. */",
            "\t\t\tif (!cpu_online(cpu)) {",
            "\t\t\t\trb_update_pages(cpu_buffer);",
            "\t\t\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\t\t} else {",
            "\t\t\t\t/* Run directly if possible. */",
            "\t\t\t\tmigrate_disable();",
            "\t\t\t\tif (cpu != smp_processor_id()) {",
            "\t\t\t\t\tmigrate_enable();",
            "\t\t\t\t\tschedule_work_on(cpu,",
            "\t\t\t\t\t\t\t &cpu_buffer->update_pages_work);",
            "\t\t\t\t} else {",
            "\t\t\t\t\tupdate_pages_handler(&cpu_buffer->update_pages_work);",
            "\t\t\t\t\tmigrate_enable();",
            "\t\t\t\t}",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* wait for all the updates to complete */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (!cpu_buffer->nr_pages_to_update)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tif (cpu_online(cpu))",
            "\t\t\t\twait_for_completion(&cpu_buffer->update_done);",
            "\t\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\t}",
            "",
            "\t\tcpus_read_unlock();",
            "\t} else {",
            "\t\tcpu_buffer = buffer->buffers[cpu_id];",
            "",
            "\t\tif (nr_pages == cpu_buffer->nr_pages)",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * Don't succeed if resizing is disabled, as a reader might be",
            "\t\t * manipulating the ring buffer and is expecting a sane state while",
            "\t\t * this is true.",
            "\t\t */",
            "\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {",
            "\t\t\terr = -EBUSY;",
            "\t\t\tgoto out_err_unlock;",
            "\t\t}",
            "",
            "\t\tcpu_buffer->nr_pages_to_update = nr_pages -",
            "\t\t\t\t\t\tcpu_buffer->nr_pages;",
            "",
            "\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\t\tif (cpu_buffer->nr_pages_to_update > 0 &&",
            "\t\t\t__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,",
            "\t\t\t\t\t    &cpu_buffer->new_pages)) {",
            "\t\t\terr = -ENOMEM;",
            "\t\t\tgoto out_err;",
            "\t\t}",
            "",
            "\t\tcpus_read_lock();",
            "",
            "\t\t/* Can't run something on an offline CPU. */",
            "\t\tif (!cpu_online(cpu_id))",
            "\t\t\trb_update_pages(cpu_buffer);",
            "\t\telse {",
            "\t\t\t/* Run directly if possible. */",
            "\t\t\tmigrate_disable();",
            "\t\t\tif (cpu_id == smp_processor_id()) {",
            "\t\t\t\trb_update_pages(cpu_buffer);",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t} else {",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t\tschedule_work_on(cpu_id,",
            "\t\t\t\t\t\t &cpu_buffer->update_pages_work);",
            "\t\t\t\twait_for_completion(&cpu_buffer->update_done);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\tcpus_read_unlock();",
            "\t}",
            "",
            " out:",
            "\t/*",
            "\t * The ring buffer resize can happen with the ring buffer",
            "\t * enabled, so that the update disturbs the tracing as little",
            "\t * as possible. But if the buffer is disabled, we do not need",
            "\t * to worry about that, and we can take the time to verify",
            "\t * that the buffer is not corrupt.",
            "\t */",
            "\tif (atomic_read(&buffer->record_disabled)) {",
            "\t\tatomic_inc(&buffer->record_disabled);",
            "\t\t/*",
            "\t\t * Even though the buffer was disabled, we must make sure",
            "\t\t * that it is truly disabled before calling rb_check_pages.",
            "\t\t * There could have been a race between checking",
            "\t\t * record_disable and incrementing it.",
            "\t\t */",
            "\t\tsynchronize_rcu();",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tunsigned long flags;",
            "",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t\t\trb_check_pages(cpu_buffer);",
            "\t\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "\t\t}",
            "\t\tatomic_dec(&buffer->record_disabled);",
            "\t}",
            "",
            "\tatomic_dec(&buffer->resizing);",
            "\tmutex_unlock(&buffer->mutex);",
            "\treturn 0;",
            "",
            " out_err:",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tstruct buffer_page *bpage, *tmp;",
            "",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\tcpu_buffer->nr_pages_to_update = 0;",
            "",
            "\t\tif (list_empty(&cpu_buffer->new_pages))",
            "\t\t\tcontinue;",
            "",
            "\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,",
            "\t\t\t\t\tlist) {",
            "\t\t\tlist_del_init(&bpage->list);",
            "\t\t\tfree_buffer_page(bpage);",
            "\t\t}",
            "\t}",
            " out_err_unlock:",
            "\tatomic_dec(&buffer->resizing);",
            "\tmutex_unlock(&buffer->mutex);",
            "\treturn err;",
            "}"
          ],
          "function_name": "rb_insert_pages, rb_update_pages, update_pages_handler, ring_buffer_resize",
          "description": "实现将新分配的缓冲页插入到环形缓冲区的头部，通过CAS操作确保线程安全地更新链表结构，若失败则释放内存资源。包含调整缓冲区大小的核心逻辑，协调多CPU上的页面分配与更新操作。",
          "similarity": 0.6441346406936646
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 837,
          "end_line": 965,
          "content": [
            "size_t ring_buffer_nr_pages(struct trace_buffer *buffer, int cpu)",
            "{",
            "\treturn buffer->buffers[cpu]->nr_pages;",
            "}",
            "size_t ring_buffer_nr_dirty_pages(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tsize_t read;",
            "\tsize_t lost;",
            "\tsize_t cnt;",
            "",
            "\tread = local_read(&buffer->buffers[cpu]->pages_read);",
            "\tlost = local_read(&buffer->buffers[cpu]->pages_lost);",
            "\tcnt = local_read(&buffer->buffers[cpu]->pages_touched);",
            "",
            "\tif (WARN_ON_ONCE(cnt < lost))",
            "\t\treturn 0;",
            "",
            "\tcnt -= lost;",
            "",
            "\t/* The reader can read an empty page, but not more than that */",
            "\tif (cnt < read) {",
            "\t\tWARN_ON_ONCE(read > cnt + 1);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\treturn cnt - read;",
            "}",
            "static __always_inline bool full_hit(struct trace_buffer *buffer, int cpu, int full)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];",
            "\tsize_t nr_pages;",
            "\tsize_t dirty;",
            "",
            "\tnr_pages = cpu_buffer->nr_pages;",
            "\tif (!nr_pages || !full)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Add one as dirty will never equal nr_pages, as the sub-buffer",
            "\t * that the writer is on is not counted as dirty.",
            "\t * This is needed if \"buffer_percent\" is set to 100.",
            "\t */",
            "\tdirty = ring_buffer_nr_dirty_pages(buffer, cpu) + 1;",
            "",
            "\treturn (dirty * 100) >= (full * nr_pages);",
            "}",
            "static void rb_wake_up_waiters(struct irq_work *work)",
            "{",
            "\tstruct rb_irq_work *rbwork = container_of(work, struct rb_irq_work, work);",
            "",
            "\twake_up_all(&rbwork->waiters);",
            "\tif (rbwork->full_waiters_pending || rbwork->wakeup_full) {",
            "\t\t/* Only cpu_buffer sets the above flags */",
            "\t\tstruct ring_buffer_per_cpu *cpu_buffer =",
            "\t\t\tcontainer_of(rbwork, struct ring_buffer_per_cpu, irq_work);",
            "",
            "\t\t/* Called from interrupt context */",
            "\t\traw_spin_lock(&cpu_buffer->reader_lock);",
            "\t\trbwork->wakeup_full = false;",
            "\t\trbwork->full_waiters_pending = false;",
            "",
            "\t\t/* Waking up all waiters, they will reset the shortest full */",
            "\t\tcpu_buffer->shortest_full = 0;",
            "\t\traw_spin_unlock(&cpu_buffer->reader_lock);",
            "",
            "\t\twake_up_all(&rbwork->full_waiters);",
            "\t}",
            "}",
            "void ring_buffer_wake_waiters(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct rb_irq_work *rbwork;",
            "",
            "\tif (!buffer)",
            "\t\treturn;",
            "",
            "\tif (cpu == RING_BUFFER_ALL_CPUS) {",
            "",
            "\t\t/* Wake up individual ones too. One level recursion */",
            "\t\tfor_each_buffer_cpu(buffer, cpu)",
            "\t\t\tring_buffer_wake_waiters(buffer, cpu);",
            "",
            "\t\trbwork = &buffer->irq_work;",
            "\t} else {",
            "\t\tif (WARN_ON_ONCE(!buffer->buffers))",
            "\t\t\treturn;",
            "\t\tif (WARN_ON_ONCE(cpu >= nr_cpu_ids))",
            "\t\t\treturn;",
            "",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t/* The CPU buffer may not have been initialized yet */",
            "\t\tif (!cpu_buffer)",
            "\t\t\treturn;",
            "\t\trbwork = &cpu_buffer->irq_work;",
            "\t}",
            "",
            "\t/* This can be called in any context */",
            "\tirq_work_queue(&rbwork->work);",
            "}",
            "static bool rb_watermark_hit(struct trace_buffer *buffer, int cpu, int full)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tbool ret = false;",
            "",
            "\t/* Reads of all CPUs always waits for any data */",
            "\tif (cpu == RING_BUFFER_ALL_CPUS)",
            "\t\treturn !ring_buffer_empty(buffer);",
            "",
            "\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\tif (!ring_buffer_empty_cpu(buffer, cpu)) {",
            "\t\tunsigned long flags;",
            "\t\tbool pagebusy;",
            "",
            "\t\tif (!full)",
            "\t\t\treturn true;",
            "",
            "\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t\tpagebusy = cpu_buffer->reader_page == cpu_buffer->commit_page;",
            "\t\tret = !pagebusy && full_hit(buffer, cpu, full);",
            "",
            "\t\tif (!ret && (!cpu_buffer->shortest_full ||",
            "\t\t\t     cpu_buffer->shortest_full > full)) {",
            "\t\t    cpu_buffer->shortest_full = full;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "\t}",
            "\treturn ret;",
            "}"
          ],
          "function_name": "ring_buffer_nr_pages, ring_buffer_nr_dirty_pages, full_hit, rb_wake_up_waiters, ring_buffer_wake_waiters, rb_watermark_hit",
          "description": "实现缓冲区页数统计功能，包含满状态检测算法，提供唤醒等待者的中断处理函数，支持全CPU模式和特定CPU的唤醒操作，包含水位标记判断逻辑控制缓冲区回收行为",
          "similarity": 0.6299175024032593
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 45,
          "end_line": 148,
          "content": [
            "int ring_buffer_print_entry_header(struct trace_seq *s)",
            "{",
            "\ttrace_seq_puts(s, \"# compressed entry header\\n\");",
            "\ttrace_seq_puts(s, \"\\ttype_len    :    5 bits\\n\");",
            "\ttrace_seq_puts(s, \"\\ttime_delta  :   27 bits\\n\");",
            "\ttrace_seq_puts(s, \"\\tarray       :   32 bits\\n\");",
            "\ttrace_seq_putc(s, '\\n');",
            "\ttrace_seq_printf(s, \"\\tpadding     : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_PADDING);",
            "\ttrace_seq_printf(s, \"\\ttime_extend : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_TIME_EXTEND);",
            "\ttrace_seq_printf(s, \"\\ttime_stamp : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_TIME_STAMP);",
            "\ttrace_seq_printf(s, \"\\tdata max type_len  == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_DATA_TYPE_LEN_MAX);",
            "",
            "\treturn !trace_seq_has_overflowed(s);",
            "}",
            "static inline bool rb_null_event(struct ring_buffer_event *event)",
            "{",
            "\treturn event->type_len == RINGBUF_TYPE_PADDING && !event->time_delta;",
            "}",
            "static void rb_event_set_padding(struct ring_buffer_event *event)",
            "{",
            "\t/* padding has a NULL time_delta */",
            "\tevent->type_len = RINGBUF_TYPE_PADDING;",
            "\tevent->time_delta = 0;",
            "}",
            "static unsigned",
            "rb_event_data_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned length;",
            "",
            "\tif (event->type_len)",
            "\t\tlength = event->type_len * RB_ALIGNMENT;",
            "\telse",
            "\t\tlength = event->array[0];",
            "\treturn length + RB_EVNT_HDR_SIZE;",
            "}",
            "static inline unsigned",
            "rb_event_length(struct ring_buffer_event *event)",
            "{",
            "\tswitch (event->type_len) {",
            "\tcase RINGBUF_TYPE_PADDING:",
            "\t\tif (rb_null_event(event))",
            "\t\t\t/* undefined */",
            "\t\t\treturn -1;",
            "\t\treturn  event->array[0] + RB_EVNT_HDR_SIZE;",
            "",
            "\tcase RINGBUF_TYPE_TIME_EXTEND:",
            "\t\treturn RB_LEN_TIME_EXTEND;",
            "",
            "\tcase RINGBUF_TYPE_TIME_STAMP:",
            "\t\treturn RB_LEN_TIME_STAMP;",
            "",
            "\tcase RINGBUF_TYPE_DATA:",
            "\t\treturn rb_event_data_length(event);",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(1);",
            "\t}",
            "\t/* not hit */",
            "\treturn 0;",
            "}",
            "static inline unsigned",
            "rb_event_ts_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned len = 0;",
            "",
            "\tif (extended_time(event)) {",
            "\t\t/* time extends include the data event after it */",
            "\t\tlen = RB_LEN_TIME_EXTEND;",
            "\t\tevent = skip_time_extend(event);",
            "\t}",
            "\treturn len + rb_event_length(event);",
            "}",
            "unsigned ring_buffer_event_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned length;",
            "",
            "\tif (extended_time(event))",
            "\t\tevent = skip_time_extend(event);",
            "",
            "\tlength = rb_event_length(event);",
            "\tif (event->type_len > RINGBUF_TYPE_DATA_TYPE_LEN_MAX)",
            "\t\treturn length;",
            "\tlength -= RB_EVNT_HDR_SIZE;",
            "\tif (length > RB_MAX_SMALL_DATA + sizeof(event->array[0]))",
            "                length -= sizeof(event->array[0]);",
            "\treturn length;",
            "}",
            "static u64 rb_event_time_stamp(struct ring_buffer_event *event)",
            "{",
            "\tu64 ts;",
            "",
            "\tts = event->array[0];",
            "\tts <<= TS_SHIFT;",
            "\tts += event->time_delta;",
            "",
            "\treturn ts;",
            "}",
            "static void rb_init_page(struct buffer_data_page *bpage)",
            "{",
            "\tlocal_set(&bpage->commit, 0);",
            "}"
          ],
          "function_name": "ring_buffer_print_entry_header, rb_null_event, rb_event_set_padding, rb_event_data_length, rb_event_length, rb_event_ts_length, ring_buffer_event_length, rb_event_time_stamp, rb_init_page",
          "description": "实现环形缓冲区事件解析功能，包括打印事件头信息、识别空事件、设置填充事件、计算不同事件类型的数据长度及时间戳，提供事件长度和时间戳读取接口",
          "similarity": 0.6081811785697937
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 357,
          "end_line": 461,
          "content": [
            "static __always_inline unsigned int rb_page_commit(struct buffer_page *bpage)",
            "{",
            "\treturn local_read(&bpage->page->commit);",
            "}",
            "static void free_buffer_page(struct buffer_page *bpage)",
            "{",
            "\tfree_page((unsigned long)bpage->page);",
            "\tkfree(bpage);",
            "}",
            "static inline bool test_time_stamp(u64 delta)",
            "{",
            "\treturn !!(delta & TS_DELTA_TEST);",
            "}",
            "int ring_buffer_print_page_header(struct trace_seq *s)",
            "{",
            "\tstruct buffer_data_page field;",
            "",
            "\ttrace_seq_printf(s, \"\\tfield: u64 timestamp;\\t\"",
            "\t\t\t \"offset:0;\\tsize:%u;\\tsigned:%u;\\n\",",
            "\t\t\t (unsigned int)sizeof(field.time_stamp),",
            "\t\t\t (unsigned int)is_signed_type(u64));",
            "",
            "\ttrace_seq_printf(s, \"\\tfield: local_t commit;\\t\"",
            "\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",",
            "\t\t\t (unsigned int)offsetof(typeof(field), commit),",
            "\t\t\t (unsigned int)sizeof(field.commit),",
            "\t\t\t (unsigned int)is_signed_type(long));",
            "",
            "\ttrace_seq_printf(s, \"\\tfield: int overwrite;\\t\"",
            "\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",",
            "\t\t\t (unsigned int)offsetof(typeof(field), commit),",
            "\t\t\t 1,",
            "\t\t\t (unsigned int)is_signed_type(long));",
            "",
            "\ttrace_seq_printf(s, \"\\tfield: char data;\\t\"",
            "\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",",
            "\t\t\t (unsigned int)offsetof(typeof(field), data),",
            "\t\t\t (unsigned int)BUF_PAGE_SIZE,",
            "\t\t\t (unsigned int)is_signed_type(char));",
            "",
            "\treturn !trace_seq_has_overflowed(s);",
            "}",
            "static inline int rb_time_cnt(unsigned long val)",
            "{",
            "\treturn (val >> RB_TIME_SHIFT) & 3;",
            "}",
            "static inline u64 rb_time_val(unsigned long top, unsigned long bottom)",
            "{",
            "\tu64 val;",
            "",
            "\tval = top & RB_TIME_VAL_MASK;",
            "\tval <<= RB_TIME_SHIFT;",
            "\tval |= bottom & RB_TIME_VAL_MASK;",
            "",
            "\treturn val;",
            "}",
            "static inline bool __rb_time_read(rb_time_t *t, u64 *ret, unsigned long *cnt)",
            "{",
            "\tunsigned long top, bottom, msb;",
            "\tunsigned long c;",
            "",
            "\t/*",
            "\t * If the read is interrupted by a write, then the cnt will",
            "\t * be different. Loop until both top and bottom have been read",
            "\t * without interruption.",
            "\t */",
            "\tdo {",
            "\t\tc = local_read(&t->cnt);",
            "\t\ttop = local_read(&t->top);",
            "\t\tbottom = local_read(&t->bottom);",
            "\t\tmsb = local_read(&t->msb);",
            "\t} while (c != local_read(&t->cnt));",
            "",
            "\t*cnt = rb_time_cnt(top);",
            "",
            "\t/* If top, msb or bottom counts don't match, this interrupted a write */",
            "\tif (*cnt != rb_time_cnt(msb) || *cnt != rb_time_cnt(bottom))",
            "\t\treturn false;",
            "",
            "\t/* The shift to msb will lose its cnt bits */",
            "\t*ret = rb_time_val(top, bottom) | ((u64)msb << RB_TIME_MSB_SHIFT);",
            "\treturn true;",
            "}",
            "static bool rb_time_read(rb_time_t *t, u64 *ret)",
            "{",
            "\tunsigned long cnt;",
            "",
            "\treturn __rb_time_read(t, ret, &cnt);",
            "}",
            "static inline unsigned long rb_time_val_cnt(unsigned long val, unsigned long cnt)",
            "{",
            "\treturn (val & RB_TIME_VAL_MASK) | ((cnt & 3) << RB_TIME_SHIFT);",
            "}",
            "static inline void rb_time_split(u64 val, unsigned long *top, unsigned long *bottom,",
            "\t\t\t\t unsigned long *msb)",
            "{",
            "\t*top = (unsigned long)((val >> RB_TIME_SHIFT) & RB_TIME_VAL_MASK);",
            "\t*bottom = (unsigned long)(val & RB_TIME_VAL_MASK);",
            "\t*msb = (unsigned long)(val >> RB_TIME_MSB_SHIFT);",
            "}",
            "static inline void rb_time_val_set(local_t *t, unsigned long val, unsigned long cnt)",
            "{",
            "\tval = rb_time_val_cnt(val, cnt);",
            "\tlocal_set(t, val);",
            "}"
          ],
          "function_name": "rb_page_commit, free_buffer_page, test_time_stamp, ring_buffer_print_page_header, rb_time_cnt, rb_time_val, __rb_time_read, rb_time_read, rb_time_val_cnt, rb_time_split, rb_time_val_set",
          "description": "实现缓冲页提交计数器读取、页面释放、时间戳字段解析与组合操作，包含原子操作辅助函数用于安全的时间戳读写，支持多处理器环境下时间戳的精确捕获",
          "similarity": 0.5983003973960876
        },
        {
          "chunk_id": 23,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 5310,
          "end_line": 5415,
          "content": [
            "static void reset_disabled_cpu_buffer(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "",
            "\tif (RB_WARN_ON(cpu_buffer, local_read(&cpu_buffer->committing)))",
            "\t\tgoto out;",
            "",
            "\tarch_spin_lock(&cpu_buffer->lock);",
            "",
            "\trb_reset_cpu(cpu_buffer);",
            "",
            "\tarch_spin_unlock(&cpu_buffer->lock);",
            "",
            " out:",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "}",
            "void ring_buffer_reset_cpu(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];",
            "",
            "\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\treturn;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tatomic_inc(&cpu_buffer->resize_disabled);",
            "\tatomic_inc(&cpu_buffer->record_disabled);",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\tatomic_dec(&cpu_buffer->record_disabled);",
            "\tatomic_dec(&cpu_buffer->resize_disabled);",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}",
            "void ring_buffer_reset_online_cpus(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tfor_each_online_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\tatomic_add(RESET_BIT, &cpu_buffer->resize_disabled);",
            "\t\tatomic_inc(&cpu_buffer->record_disabled);",
            "\t}",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\t/*",
            "\t\t * If a CPU came online during the synchronize_rcu(), then",
            "\t\t * ignore it.",
            "\t\t */",
            "\t\tif (!(atomic_read(&cpu_buffer->resize_disabled) & RESET_BIT))",
            "\t\t\tcontinue;",
            "",
            "\t\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\t\tatomic_dec(&cpu_buffer->record_disabled);",
            "\t\tatomic_sub(RESET_BIT, &cpu_buffer->resize_disabled);",
            "\t}",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}",
            "void ring_buffer_reset(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\tatomic_inc(&cpu_buffer->resize_disabled);",
            "\t\tatomic_inc(&cpu_buffer->record_disabled);",
            "\t}",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\t\tatomic_dec(&cpu_buffer->record_disabled);",
            "\t\tatomic_dec(&cpu_buffer->resize_disabled);",
            "\t}",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}"
          ],
          "function_name": "reset_disabled_cpu_buffer, ring_buffer_reset_cpu, ring_buffer_reset_online_cpus, ring_buffer_reset",
          "description": "该代码段实现了对跟踪环形缓冲区的重置机制，通过原子操作与RCU同步确保多线程安全。  \n`reset_disabled_cpu_buffer`负责安全地重置指定CPU的缓冲区，`ring_buffer_reset_cpu`、`ring_buffer_reset_online_cpus`和`ring_buffer_reset`分别针对单个CPU、在线CPU及全系统CPU执行重置，均通过原子计数器控制访问权限并阻塞数据提交。  \n由于`rb_reset_cpu`未在当前代码片段中定义，故其具体行为依赖上下文信息。",
          "similarity": 0.5878772735595703
        }
      ]
    },
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.6182074546813965,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.5760208368301392
        },
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.5675686597824097
        },
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.5174078345298767
        }
      ]
    },
    {
      "source_file": "kernel/regset.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:51:57\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `regset.c`\n\n---\n\n# regset.c 技术文档\n\n## 1. 文件概述\n\n`regset.c` 是 Linux 内核中用于管理用户态寄存器集合（user register sets）的核心辅助实现文件。该文件提供了一组通用接口，用于从目标任务（通常是进程或线程）中安全地获取寄存器状态数据，并支持将这些数据复制到用户空间。它为架构无关的 ptrace、core dump 等调试和状态导出机制提供了统一的数据访问抽象。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `__regset_get()`：内部辅助函数，执行寄存器集的实际获取逻辑，支持传入预分配缓冲区或自动分配内存。\n- `regset_get()`：公开接口，使用调用者提供的缓冲区获取寄存器数据。\n- `regset_get_alloc()`：公开接口，自动分配内存用于存储寄存器数据，并通过指针返回分配的缓冲区。\n- `copy_regset_to_user()`：将指定寄存器集的部分或全部内容安全地复制到用户空间缓冲区。\n\n### 关键数据结构（引用自 `<linux/regset.h>`）\n\n- `struct user_regset`：描述一个寄存器集合的元数据，包括寄存器数量（`n`）、每个寄存器大小（`size`）以及获取/设置回调函数（如 `regset_get`）。\n- `struct user_regset_view`：描述特定架构下所有寄存器集合的视图，包含多个 `user_regset` 实例。\n- `struct membuf`：轻量级内存缓冲区描述符，包含当前写入指针 `p` 和剩余空间 `left`，用于传递给 `regset_get` 回调。\n\n## 3. 关键实现\n\n- **内存管理策略**：  \n  `__regset_get()` 支持两种内存使用模式：\n  - 若调用者传入非空 `data` 指针，则直接使用该缓冲区；\n  - 若传入空指针，则内部调用 `kzalloc()` 分配所需内存，并在出错时自动释放。\n\n- **边界检查与截断**：  \n  函数会校验请求的 `size` 是否超过寄存器集总大小（`n * size`），若超出则自动截断，防止越界访问。\n\n- **回调机制**：  \n  实际寄存器数据的填充由 `regset->regset_get` 回调完成，该回调接收 `struct membuf` 结构，按需向缓冲区写入数据，并返回未使用的字节数（即成功写入 `size - ret` 字节）。\n\n- **用户空间安全复制**：  \n  `copy_regset_to_user()` 先通过 `regset_get_alloc()` 获取内核缓冲区，再使用 `copy_to_user()` 安全地将数据传送到用户空间，确保地址有效性并处理可能的缺页异常。\n\n- **错误处理**：  \n  所有路径均进行错误检查，包括回调不支持（`-EOPNOTSUPP`）、内存分配失败（`-ENOMEM`）、用户空间访问失败（`-EFAULT`）等，并在失败时释放已分配资源。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/export.h>`：用于导出符号供其他内核模块使用（`EXPORT_SYMBOL`）。\n  - `<linux/slab.h>`：提供 `kzalloc()` 和 `\tkfree()` 内存分配/释放接口。\n  - `<linux/regset.h>`：定义 `user_regset`、`user_regset_view`、`membuf` 等核心数据结构和函数原型。\n\n- **架构依赖**：  \n  本文件为架构无关实现，实际的寄存器读取逻辑由各架构（如 x86、ARM64）在 `user_regset` 的 `regset_get` 回调中提供。\n\n- **模块交互**：  \n  被 `ptrace` 子系统、`/proc/<pid>/mem` 接口、core dump 生成器等依赖，用于读取进程寄存器状态。\n\n## 5. 使用场景\n\n- **调试器支持**：  \n  GDB 等调试器通过 `ptrace(PTRACE_GETREGSET, ...)` 系统调用间接调用 `copy_regset_to_user()` 获取目标进程的寄存器值。\n\n- **Core Dump 生成**：  \n  进程崩溃时，内核需将寄存器上下文写入 core 文件，通过 `regset_get_alloc()` 获取寄存器数据。\n\n- **性能分析工具**：  \n  如 `perf` 或 `ftrace` 在采样时可能需要读取特定线程的寄存器状态以进行上下文分析。\n\n- **容器/虚拟化监控**：  \n  宿主机或监控程序可通过此接口安全读取客户机或容器内进程的 CPU 状态，用于状态检查或迁移。",
      "similarity": 0.6150161027908325,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/regset.c",
          "start_line": 6,
          "end_line": 62,
          "content": [
            "static int __regset_get(struct task_struct *target,",
            "\t\t\tconst struct user_regset *regset,",
            "\t\t\tunsigned int size,",
            "\t\t\tvoid **data)",
            "{",
            "\tvoid *p = *data, *to_free = NULL;",
            "\tint res;",
            "",
            "\tif (!regset->regset_get)",
            "\t\treturn -EOPNOTSUPP;",
            "\tif (size > regset->n * regset->size)",
            "\t\tsize = regset->n * regset->size;",
            "\tif (!p) {",
            "\t\tto_free = p = kzalloc(size, GFP_KERNEL);",
            "\t\tif (!p)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\tres = regset->regset_get(target, regset,",
            "\t\t\t   (struct membuf){.p = p, .left = size});",
            "\tif (res < 0) {",
            "\t\tkfree(to_free);",
            "\t\treturn res;",
            "\t}",
            "\t*data = p;",
            "\treturn size - res;",
            "}",
            "int regset_get(struct task_struct *target,",
            "\t       const struct user_regset *regset,",
            "\t       unsigned int size,",
            "\t       void *data)",
            "{",
            "\treturn __regset_get(target, regset, size, &data);",
            "}",
            "int regset_get_alloc(struct task_struct *target,",
            "\t\t     const struct user_regset *regset,",
            "\t\t     unsigned int size,",
            "\t\t     void **data)",
            "{",
            "\t*data = NULL;",
            "\treturn __regset_get(target, regset, size, data);",
            "}",
            "int copy_regset_to_user(struct task_struct *target,",
            "\t\t\tconst struct user_regset_view *view,",
            "\t\t\tunsigned int setno,",
            "\t\t\tunsigned int offset, unsigned int size,",
            "\t\t\tvoid __user *data)",
            "{",
            "\tconst struct user_regset *regset = &view->regsets[setno];",
            "\tvoid *buf;",
            "\tint ret;",
            "",
            "\tret = regset_get_alloc(target, regset, size, &buf);",
            "\tif (ret > 0)",
            "\t\tret = copy_to_user(data, buf, ret) ? -EFAULT : 0;",
            "\tkfree(buf);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__regset_get, regset_get, regset_get_alloc, copy_regset_to_user",
          "description": "实现寄存器集合数据获取与拷贝机制，包含通用获取函数、内存分配接口及用户态数据复制函数，用于架构特定寄存器集合的统一访问",
          "similarity": 0.5759807825088501
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/regset.c",
          "start_line": 1,
          "end_line": 5,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "#include <linux/export.h>",
            "#include <linux/slab.h>",
            "#include <linux/regset.h>",
            ""
          ],
          "function_name": null,
          "description": "提供内核模块导出、内存分配及寄存器集合操作所需的基础头文件支持",
          "similarity": 0.5701419711112976
        }
      ]
    }
  ]
}