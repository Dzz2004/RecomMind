{
  "query": "逻辑地址到物理地址的转换过程",
  "timestamp": "2025-12-26 00:12:50",
  "retrieved_files": [
    {
      "source_file": "mm/ioremap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:11:43\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `ioremap.c`\n\n---\n\n# ioremap.c 技术文档\n\n## 1. 文件概述\n\n`ioremap.c` 是 Linux 内核中用于将物理 I/O 内存区域重新映射到内核虚拟地址空间的核心实现文件。该机制使得内核能够安全、高效地访问设备寄存器或高地址 PCI I/O 空间（这些区域通常未被直接映射到内核的低地址段，如 PC 架构中的 640KB–1MB 区域之外）。通过 `ioremap` 系列函数，驱动程序可以获取可直接读写的内核虚拟地址，从而操作硬件设备。\n\n## 2. 核心功能\n\n### 主要函数：\n\n- **`generic_ioremap_prot(phys_addr_t phys_addr, size_t size, pgprot_t prot)`**  \n  通用 I/O 内存重映射函数，根据指定的物理地址、大小和页保护属性创建内核虚拟映射。\n\n- **`ioremap_prot(phys_addr_t phys_addr, size_t size, unsigned long prot)`**  \n  对外导出的接口函数，封装 `generic_ioremap_prot`，接受原始的 `prot` 值并转换为 `pgprot_t` 类型。\n\n- **`generic_iounmap(volatile void __iomem *addr)`**  \n  通用 I/O 映射解除函数，释放由 `ioremap` 创建的虚拟地址映射。\n\n- **`iounmap(volatile void __iomem *addr)`**  \n  对外导出的接口函数，调用 `generic_iounmap` 完成实际的解映射操作。\n\n### 关键数据结构：\n\n- **`struct vm_struct`**：用于描述内核虚拟内存区域（vmalloc 区域）的结构体，记录映射的虚拟地址、物理地址、大小及标志等信息。\n- **`pgprot_t`**：页表项保护属性类型，用于控制映射页面的访问权限（如不可缓存、设备内存等）。\n\n## 3. 关键实现\n\n### 地址对齐与边界检查\n- 函数首先校验输入参数：拒绝零长度或地址溢出（wrap-around）的情况。\n- 将物理地址和映射大小按页对齐：提取原始地址在页内的偏移量 `offset`，将 `phys_addr` 下调至页边界，并将 `size` 扩展为包含偏移后的页对齐值。\n\n### 虚拟地址分配\n- 使用 `__get_vm_area_caller()` 在 `IOREMAP_START` 到 `IOREMAP_END` 的专用内核虚拟地址区间中分配一个 `vm_struct` 描述符。\n- 若分配失败，返回 `NULL`。\n\n### 页表建立\n- 调用 `ioremap_page_range()` 建立从分配的虚拟地址 `vaddr` 到对齐后物理地址 `phys_addr` 的页表映射，使用传入的保护属性 `prot`。\n- 若页表建立失败，则通过 `free_vm_area()` 释放已分配的虚拟区域并返回 `NULL`。\n\n### 返回用户可见地址\n- 最终返回的地址为 `vaddr + offset`，即保留原始物理地址的页内偏移，使调用者能精确访问目标 I/O 地址。\n\n### 解映射流程\n- `generic_iounmap()` 首先将传入地址对齐到页边界。\n- 通过 `is_ioremap_addr()` 验证该地址是否属于 ioremap 区域。\n- 若是，则调用 `vunmap()` 释放整个虚拟映射区域。\n\n### 条件编译支持\n- 使用 `#ifndef ioremap_prot` 和 `#ifndef iounmap` 确保在架构未提供自定义实现时，使用本文件提供的通用版本。\n- 通过 `EXPORT_SYMBOL` 导出符号，供内核模块使用。\n\n## 4. 依赖关系\n\n- **`<linux/vmalloc.h>`**：提供 `__get_vm_area_caller()`、`free_vm_area()`、`vunmap()` 等 vmalloc 子系统接口。\n- **`<linux/mm.h>`**：提供内存管理基础定义，如 `PAGE_MASK`、`PAGE_ALIGN`。\n- **`<linux/io.h>`**：定义 `__iomem` 注解及 I/O 访问相关宏。\n- **`<linux/ioremap.h>`**：声明 `ioremap` 相关接口和辅助函数（如 `is_ioremap_addr`）。\n- **`<linux/export.h>`**：提供 `EXPORT_SYMBOL` 宏，用于导出符号给模块使用。\n- 依赖内核 slab 分配器（通过 `slab_is_available()` 检查），确保在早期启动阶段不会因内存子系统未就绪而崩溃。\n\n## 5. 使用场景\n\n- **设备驱动开发**：驱动程序在初始化时调用 `ioremap()` 将设备寄存器的物理地址映射为内核可访问的虚拟地址，后续通过 `readl()`/`writel()` 等 I/O 访问函数操作硬件。\n- **PCI/平台设备资源访问**：当设备 BAR（Base Address Register）指向高物理地址（超出直接映射区）时，必须通过 ioremap 机制访问。\n- **ACPI/固件交互**：访问 ACPI 表或 UEFI 运行时服务所使用的物理内存区域。\n- **体系结构抽象层**：作为通用实现，被未提供特定优化版本的架构（如某些嵌入式平台）所采用。\n- **内核调试与诊断工具**：如 `/dev/mem` 的实现可能间接依赖此机制访问任意物理内存（需配置支持）。",
      "similarity": 0.5719252824783325,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/ioremap.c",
          "start_line": 1,
          "end_line": 59,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Re-map IO memory to kernel address space so that we can access it.",
            " * This is needed for high PCI addresses that aren't mapped in the",
            " * 640k-1MB IO memory area on PC's",
            " *",
            " * (C) Copyright 1995 1996 Linus Torvalds",
            " */",
            "#include <linux/vmalloc.h>",
            "#include <linux/mm.h>",
            "#include <linux/io.h>",
            "#include <linux/export.h>",
            "#include <linux/ioremap.h>",
            "",
            "void __iomem *generic_ioremap_prot(phys_addr_t phys_addr, size_t size,",
            "\t\t\t\t   pgprot_t prot)",
            "{",
            "\tunsigned long offset, vaddr;",
            "\tphys_addr_t last_addr;",
            "\tstruct vm_struct *area;",
            "",
            "\t/* An early platform driver might end up here */",
            "\tif (WARN_ON_ONCE(!slab_is_available()))",
            "\t\treturn NULL;",
            "",
            "\t/* Disallow wrap-around or zero size */",
            "\tlast_addr = phys_addr + size - 1;",
            "\tif (!size || last_addr < phys_addr)",
            "\t\treturn NULL;",
            "",
            "\t/* Page-align mappings */",
            "\toffset = phys_addr & (~PAGE_MASK);",
            "\tphys_addr -= offset;",
            "\tsize = PAGE_ALIGN(size + offset);",
            "",
            "\tarea = __get_vm_area_caller(size, VM_IOREMAP, IOREMAP_START,",
            "\t\t\t\t    IOREMAP_END, __builtin_return_address(0));",
            "\tif (!area)",
            "\t\treturn NULL;",
            "\tvaddr = (unsigned long)area->addr;",
            "\tarea->phys_addr = phys_addr;",
            "",
            "\tif (ioremap_page_range(vaddr, vaddr + size, phys_addr, prot)) {",
            "\t\tfree_vm_area(area);",
            "\t\treturn NULL;",
            "\t}",
            "",
            "\treturn (void __iomem *)(vaddr + offset);",
            "}",
            "",
            "#ifndef ioremap_prot",
            "void __iomem *ioremap_prot(phys_addr_t phys_addr, size_t size,",
            "\t\t\t   unsigned long prot)",
            "{",
            "\treturn generic_ioremap_prot(phys_addr, size, __pgprot(prot));",
            "}",
            "EXPORT_SYMBOL(ioremap_prot);",
            "#endif",
            ""
          ],
          "function_name": null,
          "description": "generic_ioremap_prot 函数实现通用I/O内存映射，接收物理地址、大小和保护属性参数，通过页面对齐计算偏移量，使用__get_vm_area分配虚拟内存区域，并调用ioremap_page_range建立映射，最终返回指向映射后虚拟地址的指针",
          "similarity": 0.5260297656059265
        },
        {
          "chunk_id": 1,
          "file_path": "mm/ioremap.c",
          "start_line": 60,
          "end_line": 70,
          "content": [
            "void generic_iounmap(volatile void __iomem *addr)",
            "{",
            "\tvoid *vaddr = (void *)((unsigned long)addr & PAGE_MASK);",
            "",
            "\tif (is_ioremap_addr(vaddr))",
            "\t\tvunmap(vaddr);",
            "}",
            "void iounmap(volatile void __iomem *addr)",
            "{",
            "\tgeneric_iounmap(addr);",
            "}"
          ],
          "function_name": "generic_iounmap, iounmap",
          "description": "generic_iounmap 函数用于解除I/O内存映射，通过判断地址是否属于ioremap区域，若是则调用vunmap释放对应的虚拟内存区域，iounmap函数作为包装接口调用generic_iounmap执行卸载操作",
          "similarity": 0.4366501569747925
        }
      ]
    },
    {
      "source_file": "kernel/dma/direct.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:12:42\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\direct.c`\n\n---\n\n# `dma/direct.c` 技术文档\n\n## 1. 文件概述\n\n`dma/direct.c` 实现了 Linux 内核中 **DMA 直接映射操作（DMA direct mapping）** 的核心逻辑。该文件提供了一套不依赖 IOMMU 的 DMA 内存分配与映射机制，适用于物理地址可直接被设备访问的平台（如 x86、ARM64 等无 IOMMU 或 IOMMU 被禁用的场景）。其核心思想是将物理内存地址直接转换为设备可见的 DMA 地址，避免复杂的地址转换开销。\n\n该实现支持多种内存分配策略，包括：\n- 连续物理内存分配（CMA 或 buddy allocator）\n- SWIOTLB 回退机制（用于处理高地址设备无法访问的情况）\n- 原子池分配（用于不可阻塞上下文）\n- 高端内存重映射\n- 内存加密/解密（如 AMD SEV、Intel TDX 等安全特性）\n\n## 2. 核心功能\n\n### 全局变量\n- `zone_dma_bits`：定义 ZONE_DMA 的地址位宽（默认 24 位，即 16MB），可由架构代码覆盖。\n\n### 主要函数\n| 函数名 | 功能描述 |\n|--------|--------|\n| `phys_to_dma_direct()` | 将物理地址转换为 DMA 地址，支持强制解密场景 |\n| `dma_direct_to_page()` | 通过 DMA 地址反查对应的 `struct page` |\n| `dma_direct_get_required_mask()` | 计算设备所需的 DMA 地址掩码（基于系统最大物理地址）|\n| `dma_coherent_ok()` | 检查给定物理地址范围是否在设备的 DMA 地址能力范围内 |\n| `dma_direct_alloc()` | **主入口函数**：为设备分配 DMA 内存，支持多种属性和回退策略 |\n| `__dma_direct_alloc_pages()` | 底层页面分配函数，尝试最优内存区域并回退到低地址区域 |\n| `dma_direct_alloc_from_pool()` | 从原子池分配不可阻塞的 DMA 内存 |\n| `dma_direct_alloc_no_mapping()` | 分配无内核虚拟映射的 DMA 内存（返回 `struct page*`）|\n| `dma_set_decrypted()` / `dma_set_encrypted()` | 设置内存页为解密/加密状态（用于安全虚拟化）|\n\n### 辅助函数\n- `dma_direct_optimal_gfp_mask()`：根据设备 DMA 限制选择最优的 GFP 标志（GFP_DMA / GFP_DMA32）\n- `__dma_direct_free_pages()`：释放通过直接分配获得的页面（优先尝试 SWIOTLB 释放）\n\n## 3. 关键实现\n\n### 3.1 DMA 地址空间约束处理\n- 使用 `dev->coherent_dma_mask` 和 `dev->bus_dma_limit` 确定设备可寻址的物理地址上限。\n- 通过 `dma_coherent_ok()` 验证分配的物理内存是否在设备可访问范围内。\n- 若分配的内存超出范围，则回退到更低地址区域（先尝试 GFP_DMA32，再尝试 GFP_DMA）。\n\n### 3.2 多层次内存分配策略\n1. **首选 CMA 连续内存**：通过 `dma_alloc_contiguous()` 分配。\n2. **回退到 buddy allocator**：使用 `alloc_pages_node()`。\n3. **SWIOTLB 支持**：当设备无法访问高地址时，通过 `swiotlb_alloc()` 分配 bounce buffer。\n4. **原子上下文支持**：在不可阻塞场景下使用 `dma_direct_alloc_from_pool()` 从预分配池中获取内存。\n\n### 3.3 非一致性缓存与内存属性处理\n- 对于非一致性缓存架构（`!dev_is_dma_coherent()`）：\n  - 优先使用全局一致性内存池（`CONFIG_DMA_GLOBAL_POOL`）\n  - 或启用重映射（`CONFIG_DMA_DIRECT_REMAP`）创建 uncached 映射\n  - 或调用架构特定的 `arch_dma_alloc()`\n- 调用 `arch_dma_prep_coherent()` 清理内核别名的脏缓存行。\n\n### 3.4 安全内存处理（加密/解密）\n- 当 `force_dma_unencrypted(dev)` 为真（如 SEV 环境），分配的内存需标记为解密。\n- 使用 `set_memory_decrypted()` / `set_memory_encrypted()` 修改页表属性。\n- 解密操作可能阻塞，因此在原子上下文中需使用内存池。\n\n### 3.5 高端内存（HighMem）处理\n- 若分配的页面位于高端内存（`PageHighMem`），则必须通过 `dma_common_contiguous_remap()` 创建内核虚拟地址映射。\n- 重映射时应用设备特定的页保护属性（`dma_pgprot()`）并处理解密需求。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- `<linux/memblock.h>`：获取 `max_pfn` 系统最大页帧号\n- `<linux/dma-map-ops.h>`：DMA 映射操作抽象接口\n- `<linux/scatterlist.h>`：SG 表支持（间接依赖）\n- `<linux/set_memory.h>`：内存加密/解密操作（`set_memory_decrypted` 等）\n- `<linux/vmalloc.h>`：高端内存重映射支持\n- `\"direct.h\"`：本地 DMA direct 实现的私有头文件\n\n### 配置选项依赖\n- `CONFIG_SWIOTLB`：SWIOTLB bounce buffer 支持\n- `CONFIG_DMA_COHERENT_POOL`：原子上下文 DMA 内存池\n- `CONFIG_DMA_GLOBAL_POOL`：全局一致性 DMA 内存池\n- `CONFIG_DMA_DIRECT_REMAP`：非一致性设备的重映射支持\n- `CONFIG_ZONE_DMA` / `CONFIG_ZONE_DMA32`：低地址内存区域支持\n- `CONFIG_ARCH_HAS_DMA_SET_UNCACHED`：架构特定 uncached 映射支持\n\n### 架构相关依赖\n- `phys_to_dma()` / `dma_to_phys()`：架构提供的物理地址与 DMA 地址转换函数\n- `arch_dma_prep_coherent()`：架构特定的缓存一致性准备\n- `arch_dma_alloc()`：架构特定的 DMA 分配回退路径\n\n## 5. 使用场景\n\n### 5.1 设备驱动 DMA 分配\n- 驱动调用 `dma_alloc_coherent()` 或 `dma_alloc_attrs()` 时，若系统未启用 IOMMU，则最终由 `dma_direct_alloc()` 处理。\n- 适用于大多数无 IOMMU 的嵌入式系统、传统 PC 平台或 IOMMU 被显式禁用的场景。\n\n### 5.2 安全虚拟化环境\n- 在 AMD SEV 或 Intel TDX 等机密计算环境中，DMA 内存需标记为“解密”，该文件通过 `force_dma_unencrypted()` 机制实现。\n\n### 5.3 资源受限或实时系统\n- 通过 `DMA_ATTR_NO_KERNEL_MAPPING` 属性分配无内核映射的 DMA 内存，减少 TLB 压力。\n- 在中断上下文等不可阻塞场景中，自动使用原子内存池分配。\n\n### 5.4 老旧设备兼容\n- 对仅支持 32 位或 24 位 DMA 地址的设备，自动分配低地址内存（通过 GFP_DMA32/GFP_DMA）并验证地址范围。\n\n### 5.5 高端内存平台\n- 在 32 位系统或内存大于直接映射区域的平台上，自动处理高端内存的重映射，确保返回有效的内核虚拟地址。",
      "similarity": 0.5578919649124146,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/direct.c",
          "start_line": 25,
          "end_line": 140,
          "content": [
            "static inline dma_addr_t phys_to_dma_direct(struct device *dev,",
            "\t\tphys_addr_t phys)",
            "{",
            "\tif (force_dma_unencrypted(dev))",
            "\t\treturn phys_to_dma_unencrypted(dev, phys);",
            "\treturn phys_to_dma(dev, phys);",
            "}",
            "u64 dma_direct_get_required_mask(struct device *dev)",
            "{",
            "\tphys_addr_t phys = (phys_addr_t)(max_pfn - 1) << PAGE_SHIFT;",
            "\tu64 max_dma = phys_to_dma_direct(dev, phys);",
            "",
            "\treturn (1ULL << (fls64(max_dma) - 1)) * 2 - 1;",
            "}",
            "static gfp_t dma_direct_optimal_gfp_mask(struct device *dev, u64 *phys_limit)",
            "{",
            "\tu64 dma_limit = min_not_zero(",
            "\t\tdev->coherent_dma_mask,",
            "\t\tdev->bus_dma_limit);",
            "",
            "\t/*",
            "\t * Optimistically try the zone that the physical address mask falls",
            "\t * into first.  If that returns memory that isn't actually addressable",
            "\t * we will fallback to the next lower zone and try again.",
            "\t *",
            "\t * Note that GFP_DMA32 and GFP_DMA are no ops without the corresponding",
            "\t * zones.",
            "\t */",
            "\t*phys_limit = dma_to_phys(dev, dma_limit);",
            "\tif (*phys_limit <= DMA_BIT_MASK(zone_dma_bits))",
            "\t\treturn GFP_DMA;",
            "\tif (*phys_limit <= DMA_BIT_MASK(32))",
            "\t\treturn GFP_DMA32;",
            "\treturn 0;",
            "}",
            "bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)",
            "{",
            "\tdma_addr_t dma_addr = phys_to_dma_direct(dev, phys);",
            "",
            "\tif (dma_addr == DMA_MAPPING_ERROR)",
            "\t\treturn false;",
            "\treturn dma_addr + size - 1 <=",
            "\t\tmin_not_zero(dev->coherent_dma_mask, dev->bus_dma_limit);",
            "}",
            "static int dma_set_decrypted(struct device *dev, void *vaddr, size_t size)",
            "{",
            "\tif (!force_dma_unencrypted(dev))",
            "\t\treturn 0;",
            "\treturn set_memory_decrypted((unsigned long)vaddr, PFN_UP(size));",
            "}",
            "static int dma_set_encrypted(struct device *dev, void *vaddr, size_t size)",
            "{",
            "\tint ret;",
            "",
            "\tif (!force_dma_unencrypted(dev))",
            "\t\treturn 0;",
            "\tret = set_memory_encrypted((unsigned long)vaddr, PFN_UP(size));",
            "\tif (ret)",
            "\t\tpr_warn_ratelimited(\"leaking DMA memory that can't be re-encrypted\\n\");",
            "\treturn ret;",
            "}",
            "static void __dma_direct_free_pages(struct device *dev, struct page *page,",
            "\t\t\t\t    size_t size)",
            "{",
            "\tif (swiotlb_free(dev, page, size))",
            "\t\treturn;",
            "\tdma_free_contiguous(dev, page, size);",
            "}",
            "static bool dma_direct_use_pool(struct device *dev, gfp_t gfp)",
            "{",
            "\treturn !gfpflags_allow_blocking(gfp) && !is_swiotlb_for_alloc(dev);",
            "}",
            "void dma_direct_free(struct device *dev, size_t size,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, unsigned long attrs)",
            "{",
            "\tunsigned int page_order = get_order(size);",
            "",
            "\tif ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) &&",
            "\t    !force_dma_unencrypted(dev) && !is_swiotlb_for_alloc(dev)) {",
            "\t\t/* cpu_addr is a struct page cookie, not a kernel address */",
            "\t\tdma_free_contiguous(dev, cpu_addr, size);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_SET_UNCACHED) &&",
            "\t    !IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&",
            "\t    !IS_ENABLED(CONFIG_DMA_GLOBAL_POOL) &&",
            "\t    !dev_is_dma_coherent(dev) &&",
            "\t    !is_swiotlb_for_alloc(dev)) {",
            "\t\tarch_dma_free(dev, size, cpu_addr, dma_addr, attrs);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_DMA_GLOBAL_POOL) &&",
            "\t    !dev_is_dma_coherent(dev)) {",
            "\t\tif (!dma_release_from_global_coherent(page_order, cpu_addr))",
            "\t\t\tWARN_ON_ONCE(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */",
            "\tif (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&",
            "\t    dma_free_from_pool(dev, cpu_addr, PAGE_ALIGN(size)))",
            "\t\treturn;",
            "",
            "\tif (is_vmalloc_addr(cpu_addr)) {",
            "\t\tvunmap(cpu_addr);",
            "\t} else {",
            "\t\tif (IS_ENABLED(CONFIG_ARCH_HAS_DMA_CLEAR_UNCACHED))",
            "\t\t\tarch_dma_clear_uncached(cpu_addr, size);",
            "\t\tif (dma_set_encrypted(dev, cpu_addr, size))",
            "\t\t\treturn;",
            "\t}",
            "",
            "\t__dma_direct_free_pages(dev, dma_direct_to_page(dev, dma_addr), size);",
            "}"
          ],
          "function_name": "phys_to_dma_direct, dma_direct_get_required_mask, dma_direct_optimal_gfp_mask, dma_coherent_ok, dma_set_decrypted, dma_set_encrypted, __dma_direct_free_pages, dma_direct_use_pool, dma_direct_free",
          "description": "实现了DMA直接映射的核心辅助函数，包括物理地址转DMA地址、计算DMA掩码、优化内存分配策略、检查DMA一致性及加密内存设置等功能。dma_direct_free处理不同条件下的内存释放逻辑，涉及SWIOTLB、原子池和架构特定的释放路径。",
          "similarity": 0.5329740047454834
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/direct.c",
          "start_line": 1,
          "end_line": 24,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 2018-2020 Christoph Hellwig.",
            " *",
            " * DMA operations that map physical memory directly without using an IOMMU.",
            " */",
            "#include <linux/memblock.h> /* for max_pfn */",
            "#include <linux/export.h>",
            "#include <linux/mm.h>",
            "#include <linux/dma-map-ops.h>",
            "#include <linux/scatterlist.h>",
            "#include <linux/pfn.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/slab.h>",
            "#include \"direct.h\"",
            "",
            "/*",
            " * Most architectures use ZONE_DMA for the first 16 Megabytes, but some use",
            " * it for entirely different regions. In that case the arch code needs to",
            " * override the variable below for dma-direct to work properly.",
            " */",
            "unsigned int zone_dma_bits __ro_after_init = 24;",
            ""
          ],
          "function_name": null,
          "description": "定义了用于DMA直接映射的全局变量zone_dma_bits，默认值为24位，表示DMA地址空间的位宽。该变量用于控制DMA操作的物理地址范围，架构代码可通过覆盖此变量调整DMA直接映射的行为。",
          "similarity": 0.45595842599868774
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/dma/direct.c",
          "start_line": 520,
          "end_line": 633,
          "content": [
            "dma_addr_t dma_direct_map_resource(struct device *dev, phys_addr_t paddr,",
            "\t\tsize_t size, enum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tdma_addr_t dma_addr = paddr;",
            "",
            "\tif (unlikely(!dma_capable(dev, dma_addr, size, false))) {",
            "\t\tdev_err_once(dev,",
            "\t\t\t     \"DMA addr %pad+%zu overflow (mask %llx, bus limit %llx).\\n\",",
            "\t\t\t     &dma_addr, size, *dev->dma_mask, dev->bus_dma_limit);",
            "\t\tWARN_ON_ONCE(1);",
            "\t\treturn DMA_MAPPING_ERROR;",
            "\t}",
            "",
            "\treturn dma_addr;",
            "}",
            "int dma_direct_get_sgtable(struct device *dev, struct sg_table *sgt,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "\tstruct page *page = dma_direct_to_page(dev, dma_addr);",
            "\tint ret;",
            "",
            "\tret = sg_alloc_table(sgt, 1, GFP_KERNEL);",
            "\tif (!ret)",
            "\t\tsg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);",
            "\treturn ret;",
            "}",
            "bool dma_direct_can_mmap(struct device *dev)",
            "{",
            "\treturn dev_is_dma_coherent(dev) ||",
            "\t\tIS_ENABLED(CONFIG_DMA_NONCOHERENT_MMAP);",
            "}",
            "int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "\tunsigned long user_count = vma_pages(vma);",
            "\tunsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;",
            "\tunsigned long pfn = PHYS_PFN(dma_to_phys(dev, dma_addr));",
            "\tint ret = -ENXIO;",
            "",
            "\tvma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);",
            "\tif (force_dma_unencrypted(dev))",
            "\t\tvma->vm_page_prot = pgprot_decrypted(vma->vm_page_prot);",
            "",
            "\tif (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))",
            "\t\treturn ret;",
            "\tif (dma_mmap_from_global_coherent(vma, cpu_addr, size, &ret))",
            "\t\treturn ret;",
            "",
            "\tif (vma->vm_pgoff >= count || user_count > count - vma->vm_pgoff)",
            "\t\treturn -ENXIO;",
            "\treturn remap_pfn_range(vma, vma->vm_start, pfn + vma->vm_pgoff,",
            "\t\t\tuser_count << PAGE_SHIFT, vma->vm_page_prot);",
            "}",
            "int dma_direct_supported(struct device *dev, u64 mask)",
            "{",
            "\tu64 min_mask = (max_pfn - 1) << PAGE_SHIFT;",
            "",
            "\t/*",
            "\t * Because 32-bit DMA masks are so common we expect every architecture",
            "\t * to be able to satisfy them - either by not supporting more physical",
            "\t * memory, or by providing a ZONE_DMA32.  If neither is the case, the",
            "\t * architecture needs to use an IOMMU instead of the direct mapping.",
            "\t */",
            "\tif (mask >= DMA_BIT_MASK(32))",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * This check needs to be against the actual bit mask value, so use",
            "\t * phys_to_dma_unencrypted() here so that the SME encryption mask isn't",
            "\t * part of the check.",
            "\t */",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA))",
            "\t\tmin_mask = min_t(u64, min_mask, DMA_BIT_MASK(zone_dma_bits));",
            "\treturn mask >= phys_to_dma_unencrypted(dev, min_mask);",
            "}",
            "size_t dma_direct_max_mapping_size(struct device *dev)",
            "{",
            "\t/* If SWIOTLB is active, use its maximum mapping size */",
            "\tif (is_swiotlb_active(dev) &&",
            "\t    (dma_addressing_limited(dev) || is_swiotlb_force_bounce(dev)))",
            "\t\treturn swiotlb_max_mapping_size(dev);",
            "\treturn SIZE_MAX;",
            "}",
            "bool dma_direct_need_sync(struct device *dev, dma_addr_t dma_addr)",
            "{",
            "\treturn !dev_is_dma_coherent(dev) ||",
            "\t       is_swiotlb_buffer(dev, dma_to_phys(dev, dma_addr));",
            "}",
            "int dma_direct_set_offset(struct device *dev, phys_addr_t cpu_start,",
            "\t\t\t dma_addr_t dma_start, u64 size)",
            "{",
            "\tstruct bus_dma_region *map;",
            "\tu64 offset = (u64)cpu_start - (u64)dma_start;",
            "",
            "\tif (dev->dma_range_map) {",
            "\t\tdev_err(dev, \"attempt to add DMA range to existing map\\n\");",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tif (!offset)",
            "\t\treturn 0;",
            "",
            "\tmap = kcalloc(2, sizeof(*map), GFP_KERNEL);",
            "\tif (!map)",
            "\t\treturn -ENOMEM;",
            "\tmap[0].cpu_start = cpu_start;",
            "\tmap[0].dma_start = dma_start;",
            "\tmap[0].offset = offset;",
            "\tmap[0].size = size;",
            "\tdev->dma_range_map = map;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "dma_direct_map_resource, dma_direct_get_sgtable, dma_direct_can_mmap, dma_direct_mmap, dma_direct_supported, dma_direct_max_mapping_size, dma_direct_need_sync, dma_direct_set_offset",
          "description": "包含DMA直接映射的资源映射、SG表构建、内存映射支持性检测及最大映射尺寸计算等功能。dma_direct_mmap实现设备内存的VMA映射，dma_direct_supported验证DMA掩码兼容性，dma_direct_set_offset用于配置DMA地址偏移量。",
          "similarity": 0.44254326820373535
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/dma/direct.c",
          "start_line": 391,
          "end_line": 503,
          "content": [
            "void dma_direct_free_pages(struct device *dev, size_t size,",
            "\t\tstruct page *page, dma_addr_t dma_addr,",
            "\t\tenum dma_data_direction dir)",
            "{",
            "\tvoid *vaddr = page_address(page);",
            "",
            "\t/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */",
            "\tif (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&",
            "\t    dma_free_from_pool(dev, vaddr, size))",
            "\t\treturn;",
            "",
            "\tif (dma_set_encrypted(dev, vaddr, size))",
            "\t\treturn;",
            "\t__dma_direct_free_pages(dev, page, size);",
            "}",
            "void dma_direct_sync_sg_for_device(struct device *dev,",
            "\t\tstruct scatterlist *sgl, int nents, enum dma_data_direction dir)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tphys_addr_t paddr = dma_to_phys(dev, sg_dma_address(sg));",
            "",
            "\t\tif (unlikely(is_swiotlb_buffer(dev, paddr)))",
            "\t\t\tswiotlb_sync_single_for_device(dev, paddr, sg->length,",
            "\t\t\t\t\t\t       dir);",
            "",
            "\t\tif (!dev_is_dma_coherent(dev))",
            "\t\t\tarch_sync_dma_for_device(paddr, sg->length,",
            "\t\t\t\t\tdir);",
            "\t}",
            "}",
            "void dma_direct_sync_sg_for_cpu(struct device *dev,",
            "\t\tstruct scatterlist *sgl, int nents, enum dma_data_direction dir)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tphys_addr_t paddr = dma_to_phys(dev, sg_dma_address(sg));",
            "",
            "\t\tif (!dev_is_dma_coherent(dev))",
            "\t\t\tarch_sync_dma_for_cpu(paddr, sg->length, dir);",
            "",
            "\t\tif (unlikely(is_swiotlb_buffer(dev, paddr)))",
            "\t\t\tswiotlb_sync_single_for_cpu(dev, paddr, sg->length,",
            "\t\t\t\t\t\t    dir);",
            "",
            "\t\tif (dir == DMA_FROM_DEVICE)",
            "\t\t\tarch_dma_mark_clean(paddr, sg->length);",
            "\t}",
            "",
            "\tif (!dev_is_dma_coherent(dev))",
            "\t\tarch_sync_dma_for_cpu_all();",
            "}",
            "void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,",
            "\t\tint nents, enum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl,  sg, nents, i) {",
            "\t\tif (sg_dma_is_bus_address(sg))",
            "\t\t\tsg_dma_unmark_bus_address(sg);",
            "\t\telse",
            "\t\t\tdma_direct_unmap_page(dev, sg->dma_address,",
            "\t\t\t\t\t      sg_dma_len(sg), dir, attrs);",
            "\t}",
            "}",
            "int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,",
            "\t\tenum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tstruct pci_p2pdma_map_state p2pdma_state = {};",
            "\tenum pci_p2pdma_map_type map;",
            "\tstruct scatterlist *sg;",
            "\tint i, ret;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tif (is_pci_p2pdma_page(sg_page(sg))) {",
            "\t\t\tmap = pci_p2pdma_map_segment(&p2pdma_state, dev, sg);",
            "\t\t\tswitch (map) {",
            "\t\t\tcase PCI_P2PDMA_MAP_BUS_ADDR:",
            "\t\t\t\tcontinue;",
            "\t\t\tcase PCI_P2PDMA_MAP_THRU_HOST_BRIDGE:",
            "\t\t\t\t/*",
            "\t\t\t\t * Any P2P mapping that traverses the PCI",
            "\t\t\t\t * host bridge must be mapped with CPU physical",
            "\t\t\t\t * address and not PCI bus addresses. This is",
            "\t\t\t\t * done with dma_direct_map_page() below.",
            "\t\t\t\t */",
            "\t\t\t\tbreak;",
            "\t\t\tdefault:",
            "\t\t\t\tret = -EREMOTEIO;",
            "\t\t\t\tgoto out_unmap;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tsg->dma_address = dma_direct_map_page(dev, sg_page(sg),",
            "\t\t\t\tsg->offset, sg->length, dir, attrs);",
            "\t\tif (sg->dma_address == DMA_MAPPING_ERROR) {",
            "\t\t\tret = -EIO;",
            "\t\t\tgoto out_unmap;",
            "\t\t}",
            "\t\tsg_dma_len(sg) = sg->length;",
            "\t}",
            "",
            "\treturn nents;",
            "",
            "out_unmap:",
            "\tdma_direct_unmap_sg(dev, sgl, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "dma_direct_free_pages, dma_direct_sync_sg_for_device, dma_direct_sync_sg_for_cpu, dma_direct_unmap_sg, dma_direct_map_sg",
          "description": "提供了SCATTERLIST的同步、解映射和映射操作实现，包括对非一致内存的架构同步、PCI P2P DMA的特殊处理以及SG表的构建。sync_sg系列函数负责设备与CPU之间的数据缓存一致性维护。",
          "similarity": 0.4145457446575165
        }
      ]
    },
    {
      "source_file": "kernel/trace/trace_probe_kernel.h",
      "md_summary": "> 自动生成时间: 2025-10-25 17:33:44\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `trace\\trace_probe_kernel.h`\n\n---\n\n# `trace/trace_probe_kernel.h` 技术文档\n\n## 1. 文件概述\n\n`trace_probe_kernel.h` 是 Linux 内核动态追踪子系统中的一个辅助头文件，主要用于提供在内核探针（如 kprobe、eprobe）上下文中安全地从内核或用户空间读取字符串和内存数据的通用内联函数。该文件特别关注在探针处理路径（probe context）中避免触发递归探针（通过 `nokprobe_inline` 限定），并支持跨地址空间（内核/用户）的数据读取，同时处理地址空间重叠架构与非重叠架构的差异。\n\n## 2. 核心功能\n\n文件中定义了以下主要内联函数：\n\n- **字符串长度获取**：\n  - `fetch_store_strlen_user()`：从用户空间安全读取以 null 结尾的字符串长度（含终止符）。\n  - `fetch_store_strlen()`：自动判断地址空间，从内核或用户空间读取字符串长度。\n\n- **字符串内容读取**：\n  - `fetch_store_string_user()`：从用户空间安全拷贝 null-terminated 字符串到目标缓冲区。\n  - `fetch_store_string()`：自动判断地址空间，从内核或用户空间拷贝字符串。\n\n- **通用内存读取**：\n  - `probe_mem_read_user()`：从用户空间安全读取任意内存块。\n  - `probe_mem_read()`：自动判断地址空间，安全读取内核或用户空间内存。\n\n- **辅助函数**：\n  - `set_data_loc()`：根据读取结果设置动态追踪中的“数据位置”（data location）字段，用于支持变长数据（如字符串）在追踪事件中的高效存储。\n\n> 所有函数均使用 `nokprobe_inline` 修饰，确保不会在探针上下文中被再次探测。\n\n## 3. 关键实现\n\n### 地址空间自动识别\n通过 `CONFIG_ARCH_HAS_NON_OVERLAPPING_ADDRESS_SPACE` 宏判断当前架构是否具有非重叠的内核/用户地址空间（如 x86_64）。若定义该宏且地址小于 `TASK_SIZE`，则视为用户空间地址，调用用户空间专用读取函数（如 `strnlen_user_nofault`、`strncpy_from_user_nofault`）；否则视为内核地址，使用内核空间安全读取函数（如 `copy_from_kernel_nofault`）。\n\n### 安全内存访问\n所有内存读取操作均使用 `_nofault` 系列函数（如 `copy_from_kernel_nofault`、`strncpy_from_user_nofault`），这些函数在访问非法或不可用地址时不会导致内核 oops，而是返回错误码，确保探针执行的健壮性。\n\n### 动态数据位置编码\n字符串等变长数据在追踪事件中不直接内联存储，而是采用“数据位置”（data location）机制：\n- 使用 `get_loc_len()` 从 `u32` 字段中提取预分配的最大长度。\n- 使用 `get_loc_data()` 计算实际数据存储地址（相对于事件记录基地址 `base` 的偏移）。\n- 通过 `make_data_loc(len, offset)` 将长度和偏移编码回 `u32` 字段，供后续解析使用。\n- `set_data_loc()` 封装了该逻辑，并处理读取失败（`ret < 0`）时将长度置零。\n\n### 字符串读取容错\n`fetch_store_string()` 在读取内核字符串时注释指出“字符串可能在探测过程中被修改”，因此使用 `strncpy_from_kernel_nofault` 而非简单 `strncpy`，以容忍并发修改导致的不一致，避免崩溃。\n\n## 4. 依赖关系\n\n- **前置头文件依赖**：  \n  本文件**不能直接包含** `trace_probe.h`，但其功能依赖于该头文件中定义的宏和函数（如 `get_loc_len`、`get_loc_data`、`make_data_loc`、`MAX_STRING_SIZE`）。因此，任何包含本文件的源文件**必须先包含 `trace_probe.h`**。\n\n- **架构依赖**：  \n  依赖 `CONFIG_ARCH_HAS_NON_OVERLAPPING_ADDRESS_SPACE` 配置选项，该选项在具有分离用户/内核地址空间的架构（如 x86、ARM64）上定义，在重叠地址空间架构（如某些 32 位 ARM）上未定义。\n\n- **内核 API 依赖**：  \n  依赖以下内核安全访问函数：\n  - `strnlen_user_nofault()`\n  - `strncpy_from_user_nofault()`\n  - `copy_from_user_nofault()`\n  - `copy_from_kernel_nofault()`\n  - `strncpy_from_kernel_nofault()`\n\n- **使用模块**：  \n  主要被 `trace_kprobe.c` 和 `trace_eprobe.c` 通过 `trace_probe_tmpl.h` 模板机制间接使用，为 kprobe 和 eprobe 提供统一的数据获取接口。\n\n## 5. 使用场景\n\n该头文件主要用于 Linux 内核的 **ftrace 动态事件追踪系统** 中，具体场景包括：\n\n- **Kprobe 事件定义**：当用户通过 `echo 'p:myprobe kernel_func +0(%di):string' > /sys/kernel/debug/tracing/kprobe_events` 定义一个捕获字符串参数的 kprobe 时，内核在触发探针时会调用 `fetch_store_string()` 从寄存器指向的地址（可能是内核或用户空间）安全读取字符串。\n\n- **Eprobe（Event Probe）事件**：类似 kprobe，用于附加到 tracepoint 或 ftrace function 事件上，并提取其参数中的字符串。\n\n- **用户空间参数追踪**：当被探测的内核函数接收用户空间指针（如系统调用参数）时，探针需通过 `_user` 系列函数安全访问用户内存。\n\n- **内核数据结构字段提取**：探针可读取内核数据结构中的字符串字段（如 `task_struct->comm`），此时调用非 `_user` 版本函数。\n\n所有操作均在中断上下文或 NMI 安全的探针处理路径中执行，因此必须避免睡眠、页错误和递归探针，本文件的设计完全满足这些约束。",
      "similarity": 0.5498759746551514,
      "chunks": []
    }
  ]
}