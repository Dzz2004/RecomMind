{
  "query": "多层协议栈中的负载均衡决策机制",
  "timestamp": "2025-12-26 01:24:45",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/fair.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:09:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\fair.c`\n\n---\n\n# `sched/fair.c` 技术文档\n\n## 1. 文件概述\n\n`sched/fair.c` 是 Linux 内核中 **完全公平调度器**（Completely Fair Scheduler, CFS）的核心实现文件，负责实现 `SCHED_NORMAL` 和 `SCHED_BATCH` 调度策略。CFS 旨在通过红黑树（RB-tree）维护可运行任务的虚拟运行时间（vruntime），以实现 CPU 时间的公平分配。该文件实现了任务调度、负载跟踪、时间片计算、组调度（group scheduling）、NUMA 负载均衡、带宽控制等关键机制，是 Linux 通用调度子系统的核心组成部分。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_entity`：调度实体，代表一个可调度单元（任务或任务组）\n- `struct cfs_rq`：CFS 运行队列，管理一组调度实体\n- `struct load_weight`：负载权重结构，用于计算任务对系统负载的贡献\n\n### 关键函数与宏\n- `__calc_delta()` / `calc_delta_fair()`：计算基于权重的调度时间增量\n- `update_load_add()` / `update_load_sub()` / `update_load_set()`：更新负载权重\n- `__update_inv_weight()`：预计算权重的倒数以优化除法运算\n- `get_update_sysctl_factor()`：根据在线 CPU 数量动态调整调度参数\n- `update_sysctl()` / `sched_init_granularity()`：初始化和更新调度粒度参数\n- `for_each_sched_entity()`：遍历调度实体层级结构（用于组调度）\n\n### 可调参数（sysctl）\n- `sysctl_sched_base_slice`：基础时间片（默认 700,000 纳秒）\n- `sysctl_sched_tunable_scaling`：调度参数缩放策略（NONE/LOG/LINEAR）\n- `sysctl_sched_migration_cost`：任务迁移成本阈值（500 微秒）\n- `sysctl_sched_cfs_bandwidth_slice_us`（CFS 带宽控制切片，默认 5 毫秒）\n- `sysctl_numa_balancing_promote_rate_limit_MBps`（NUMA 页迁移速率限制）\n\n## 3. 关键实现\n\n### 虚拟时间与公平性\nCFS 使用 **虚拟运行时间**（vruntime）衡量任务已使用的 CPU 时间，并通过 `calc_delta_fair()` 将实际执行时间按任务权重归一化。权重由任务的 nice 值决定（`NICE_0_LOAD = 1024` 为基准）。调度器总是选择 vruntime 最小的任务运行，确保高优先级（高权重）任务获得更多 CPU 时间。\n\n### 高效除法优化\n为避免频繁除法运算，CFS 预计算 `inv_weight = WMULT_CONST / weight`（`WMULT_CONST = ~0U`），将除法转换为乘法和右移操作（`mul_u64_u32_shr`）。`__calc_delta()` 通过动态调整移位位数（`shift`）保证计算精度，适用于 32/64 位架构。\n\n### 动态粒度调整\n基础时间片 `sched_base_slice` 根据在线 CPU 数量动态缩放：\n- `SCHED_TUNABLESCALING_NONE`：固定值\n- `SCHED_TUNABLESCALING_LINEAR`：线性缩放（×ncpus）\n- `SCHED_TUNABLESCALING_LOG`（默认）：对数缩放（×(1 + ilog2(ncpus))）  \n此设计确保在多核系统中保持合理的调度延迟和交互性。\n\n### 组调度支持\n通过 `for_each_sched_entity()` 宏遍历任务所属的调度实体层级（任务 → 任务组 → 父任务组），实现 CPU 带宽在任务组间的公平分配。每个 `cfs_rq` 独立维护其子实体的红黑树。\n\n### SMP 相关优化\n- **非对称 CPU 优先级**：`arch_asym_cpu_priority()` 允许架构定义 CPU 能力差异（如大小核）\n- **容量比较宏**：`fits_capacity()`（20% 容差）和 `capacity_greater()`（5% 容差）用于负载均衡决策\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- 调度核心：`\"sched.h\"`、`\"stats.h\"`、`\"autogroup.h\"`\n- 系统服务：`<linux/sched/clock.h>`、`<linux/sched/nohz.h>`、`<linux/psi.h>`\n- 内存管理：`<linux/mem_policy.h>`、`<linux/energy_model.h>`\n- SMP 支持：`<linux/topology.h>`、`<linux/cpumask_api.h>`\n- 数据结构：`<linux/rbtree_augmented.h>`\n\n### 条件编译特性\n- `CONFIG_SMP`：多处理器调度优化\n- `CONFIG_CFS_BANDWIDTH`：CPU 带宽限制（cgroup v1/v2）\n- `CONFIG_NUMA_BALANCING`：NUMA 自动迁移\n- `CONFIG_FAIR_GROUP_SCHED`：CFS 组调度（cgroup 支持）\n\n## 5. 使用场景\n\n- **通用任务调度**：所有使用 `SCHED_NORMAL` 或 `SCHED_BATCH` 策略的用户态进程\n- **cgroup CPU 资源控制**：通过 `cpu.cfs_quota_us` 和 `cpu.cfs_period_us` 限制任务组带宽\n- **NUMA 优化**：自动迁移内存页以减少远程访问（`numa_balancing`）\n- **节能调度**：结合 `energy_model` 在满足性能前提下选择低功耗 CPU\n- **实时性保障**：通过 `cond_resched()` 在长循环中主动让出 CPU，避免内核抢占延迟过高\n- **系统调优**：管理员通过 `/proc/sys/kernel/` 下的 sysctl 参数动态调整调度行为",
      "similarity": 0.5908909440040588,
      "chunks": [
        {
          "chunk_id": 52,
          "file_path": "kernel/sched/fair.c",
          "start_line": 8581,
          "end_line": 8709,
          "content": [
            "static void set_task_max_allowed_capacity(struct task_struct *p)",
            "{",
            "\tstruct asym_cap_data *entry;",
            "",
            "\tif (!sched_asym_cpucap_active())",
            "\t\treturn;",
            "",
            "\trcu_read_lock();",
            "\tlist_for_each_entry_rcu(entry, &asym_cap_list, link) {",
            "\t\tcpumask_t *cpumask;",
            "",
            "\t\tcpumask = cpu_capacity_span(entry);",
            "\t\tif (!cpumask_intersects(p->cpus_ptr, cpumask))",
            "\t\t\tcontinue;",
            "",
            "\t\tp->max_allowed_capacity = entry->capacity;",
            "\t\tbreak;",
            "\t}",
            "\trcu_read_unlock();",
            "}",
            "static void set_cpus_allowed_fair(struct task_struct *p, struct affinity_context *ctx)",
            "{",
            "\tset_cpus_allowed_common(p, ctx);",
            "\tset_task_max_allowed_capacity(p);",
            "}",
            "static int",
            "balance_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)",
            "{",
            "\tif (sched_fair_runnable(rq))",
            "\t\treturn 1;",
            "",
            "\treturn sched_balance_newidle(rq, rf) != 0;",
            "}",
            "static inline void set_task_max_allowed_capacity(struct task_struct *p) {}",
            "static void set_next_buddy(struct sched_entity *se)",
            "{",
            "\tfor_each_sched_entity(se) {",
            "\t\tif (SCHED_WARN_ON(!se->on_rq))",
            "\t\t\treturn;",
            "\t\tif (se_is_idle(se))",
            "\t\t\treturn;",
            "\t\tcfs_rq_of(se)->next = se;",
            "\t}",
            "}",
            "static void check_preempt_wakeup_fair(struct rq *rq, struct task_struct *p, int wake_flags)",
            "{",
            "\tstruct task_struct *curr = rq->curr;",
            "\tstruct sched_entity *se = &curr->se, *pse = &p->se;",
            "\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);",
            "\tint next_buddy_marked = 0;",
            "\tint cse_is_idle, pse_is_idle;",
            "",
            "\tif (unlikely(se == pse))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This is possible from callers such as attach_tasks(), in which we",
            "\t * unconditionally wakeup_preempt() after an enqueue (which may have",
            "\t * lead to a throttle).  This both saves work and prevents false",
            "\t * next-buddy nomination below.",
            "\t */",
            "\tif (unlikely(throttled_hierarchy(cfs_rq_of(pse))))",
            "\t\treturn;",
            "",
            "\tif (sched_feat(NEXT_BUDDY) && !(wake_flags & WF_FORK) && !pse->sched_delayed) {",
            "\t\tset_next_buddy(pse);",
            "\t\tnext_buddy_marked = 1;",
            "\t}",
            "",
            "\t/*",
            "\t * We can come here with TIF_NEED_RESCHED already set from new task",
            "\t * wake up path.",
            "\t *",
            "\t * Note: this also catches the edge-case of curr being in a throttled",
            "\t * group (e.g. via set_curr_task), since update_curr() (in the",
            "\t * enqueue of curr) will have resulted in resched being set.  This",
            "\t * prevents us from potentially nominating it as a false LAST_BUDDY",
            "\t * below.",
            "\t */",
            "\tif (test_tsk_need_resched(curr))",
            "\t\treturn;",
            "",
            "\tif (!sched_feat(WAKEUP_PREEMPTION))",
            "\t\treturn;",
            "",
            "\tfind_matching_se(&se, &pse);",
            "\tWARN_ON_ONCE(!pse);",
            "",
            "\tcse_is_idle = se_is_idle(se);",
            "\tpse_is_idle = se_is_idle(pse);",
            "",
            "\t/*",
            "\t * Preempt an idle entity in favor of a non-idle entity (and don't preempt",
            "\t * in the inverse case).",
            "\t */",
            "\tif (cse_is_idle && !pse_is_idle)",
            "\t\tgoto preempt;",
            "\tif (cse_is_idle != pse_is_idle)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * BATCH and IDLE tasks do not preempt others.",
            "\t */",
            "\tif (unlikely(!normal_policy(p->policy)))",
            "\t\treturn;",
            "",
            "\tcfs_rq = cfs_rq_of(se);",
            "\tupdate_curr(cfs_rq);",
            "\t/*",
            "\t * If @p has a shorter slice than current and @p is eligible, override",
            "\t * current's slice protection in order to allow preemption.",
            "\t *",
            "\t * Note that even if @p does not turn out to be the most eligible",
            "\t * task at this moment, current's slice protection will be lost.",
            "\t */",
            "\tif (do_preempt_short(cfs_rq, pse, se) && se->vlag == se->deadline)",
            "\t\tse->vlag = se->deadline + 1;",
            "",
            "\t/*",
            "\t * If @p has become the most eligible task, force preemption.",
            "\t */",
            "\tif (pick_eevdf(cfs_rq) == pse)",
            "\t\tgoto preempt;",
            "",
            "\treturn;",
            "",
            "preempt:",
            "\tresched_curr(rq);",
            "}"
          ],
          "function_name": "set_task_max_allowed_capacity, set_cpus_allowed_fair, balance_fair, set_task_max_allowed_capacity, set_next_buddy, check_preempt_wakeup_fair",
          "description": "设置任务最大允许容量，实现负载均衡判断逻辑，管理调度实体间的抢占关系和运行队列状态更新。",
          "similarity": 0.6665998697280884
        },
        {
          "chunk_id": 64,
          "file_path": "kernel/sched/fair.c",
          "start_line": 11428,
          "end_line": 11553,
          "content": [
            "static inline bool",
            "asym_active_balance(struct lb_env *env)",
            "{",
            "\t/*",
            "\t * ASYM_PACKING needs to force migrate tasks from busy but lower",
            "\t * priority CPUs in order to pack all tasks in the highest priority",
            "\t * CPUs. When done between cores, do it only if the whole core if the",
            "\t * whole core is idle.",
            "\t *",
            "\t * If @env::src_cpu is an SMT core with busy siblings, let",
            "\t * the lower priority @env::dst_cpu help it. Do not follow",
            "\t * CPU priority.",
            "\t */",
            "\treturn env->idle != CPU_NOT_IDLE && sched_use_asym_prio(env->sd, env->dst_cpu) &&",
            "\t       (sched_asym_prefer(env->dst_cpu, env->src_cpu) ||",
            "\t\t!sched_use_asym_prio(env->sd, env->src_cpu));",
            "}",
            "static inline bool",
            "imbalanced_active_balance(struct lb_env *env)",
            "{",
            "\tstruct sched_domain *sd = env->sd;",
            "",
            "\t/*",
            "\t * The imbalanced case includes the case of pinned tasks preventing a fair",
            "\t * distribution of the load on the system but also the even distribution of the",
            "\t * threads on a system with spare capacity",
            "\t */",
            "\tif ((env->migration_type == migrate_task) &&",
            "\t    (sd->nr_balance_failed > sd->cache_nice_tries+2))",
            "\t\treturn 1;",
            "",
            "\treturn 0;",
            "}",
            "static int need_active_balance(struct lb_env *env)",
            "{",
            "\tstruct sched_domain *sd = env->sd;",
            "",
            "\tif (asym_active_balance(env))",
            "\t\treturn 1;",
            "",
            "\tif (imbalanced_active_balance(env))",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * The dst_cpu is idle and the src_cpu CPU has only 1 CFS task.",
            "\t * It's worth migrating the task if the src_cpu's capacity is reduced",
            "\t * because of other sched_class or IRQs if more capacity stays",
            "\t * available on dst_cpu.",
            "\t */",
            "\tif ((env->idle != CPU_NOT_IDLE) &&",
            "\t    (env->src_rq->cfs.h_nr_running == 1)) {",
            "\t\tif ((check_cpu_capacity(env->src_rq, sd)) &&",
            "\t\t    (capacity_of(env->src_cpu)*sd->imbalance_pct < capacity_of(env->dst_cpu)*100))",
            "\t\t\treturn 1;",
            "\t}",
            "",
            "\tif (env->migration_type == migrate_misfit)",
            "\t\treturn 1;",
            "",
            "\treturn 0;",
            "}",
            "static int should_we_balance(struct lb_env *env)",
            "{",
            "\tstruct cpumask *swb_cpus = this_cpu_cpumask_var_ptr(should_we_balance_tmpmask);",
            "\tstruct sched_group *sg = env->sd->groups;",
            "\tint cpu, idle_smt = -1;",
            "",
            "\t/*",
            "\t * Ensure the balancing environment is consistent; can happen",
            "\t * when the softirq triggers 'during' hotplug.",
            "\t */",
            "\tif (!cpumask_test_cpu(env->dst_cpu, env->cpus))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * In the newly idle case, we will allow all the CPUs",
            "\t * to do the newly idle load balance.",
            "\t *",
            "\t * However, we bail out if we already have tasks or a wakeup pending,",
            "\t * to optimize wakeup latency.",
            "\t */",
            "\tif (env->idle == CPU_NEWLY_IDLE) {",
            "\t\tif (env->dst_rq->nr_running > 0 || env->dst_rq->ttwu_pending)",
            "\t\t\treturn 0;",
            "\t\treturn 1;",
            "\t}",
            "",
            "\tcpumask_copy(swb_cpus, group_balance_mask(sg));",
            "\t/* Try to find first idle CPU */",
            "\tfor_each_cpu_and(cpu, swb_cpus, env->cpus) {",
            "\t\tif (!idle_cpu(cpu))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * Don't balance to idle SMT in busy core right away when",
            "\t\t * balancing cores, but remember the first idle SMT CPU for",
            "\t\t * later consideration.  Find CPU on an idle core first.",
            "\t\t */",
            "\t\tif (!(env->sd->flags & SD_SHARE_CPUCAPACITY) && !is_core_idle(cpu)) {",
            "\t\t\tif (idle_smt == -1)",
            "\t\t\t\tidle_smt = cpu;",
            "\t\t\t/*",
            "\t\t\t * If the core is not idle, and first SMT sibling which is",
            "\t\t\t * idle has been found, then its not needed to check other",
            "\t\t\t * SMT siblings for idleness:",
            "\t\t\t */",
            "#ifdef CONFIG_SCHED_SMT",
            "\t\t\tcpumask_andnot(swb_cpus, swb_cpus, cpu_smt_mask(cpu));",
            "#endif",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Are we the first idle core in a non-SMT domain or higher,",
            "\t\t * or the first idle CPU in a SMT domain?",
            "\t\t */",
            "\t\treturn cpu == env->dst_cpu;",
            "\t}",
            "",
            "\t/* Are we the first idle CPU with busy siblings? */",
            "\tif (idle_smt != -1)",
            "\t\treturn idle_smt == env->dst_cpu;",
            "",
            "\t/* Are we the first CPU of this group ? */",
            "\treturn group_balance_cpu(sg) == env->dst_cpu;",
            "}"
          ],
          "function_name": "asym_active_balance, imbalanced_active_balance, need_active_balance, should_we_balance",
          "description": "该代码段实现了负载均衡决策逻辑，涵盖异步优先级迁移、不平衡检测及目标CPU选择。  \n`asym_active_balance` 判断是否需基于异步优先级强制迁移任务至高优先级CPU；`imbalanced_active_balance` 检测因钉扎任务导致的负载不平衡；`need_active_balance` 综合上述条件及CPU容量差异决定是否触发迁移，`should_we_balance` 根据空闲状态和SMT核心特性选择合适的目标CPU。  \n上下文不完整，依赖`lb_env`、`sched_domain`等结构体及未展示的宏定义（如`sched_use_asym_prio`）。",
          "similarity": 0.6562808156013489
        },
        {
          "chunk_id": 72,
          "file_path": "kernel/sched/fair.c",
          "start_line": 12739,
          "end_line": 12841,
          "content": [
            "static __latent_entropy void sched_balance_softirq(struct softirq_action *h)",
            "{",
            "\tstruct rq *this_rq = this_rq();",
            "\tenum cpu_idle_type idle = this_rq->idle_balance ?",
            "\t\t\t\t\t\tCPU_IDLE : CPU_NOT_IDLE;",
            "",
            "\t/*",
            "\t * If this CPU has a pending nohz_balance_kick, then do the",
            "\t * balancing on behalf of the other idle CPUs whose ticks are",
            "\t * stopped. Do nohz_idle_balance *before* sched_balance_domains to",
            "\t * give the idle CPUs a chance to load balance. Else we may",
            "\t * load balance only within the local sched_domain hierarchy",
            "\t * and abort nohz_idle_balance altogether if we pull some load.",
            "\t */",
            "\tif (nohz_idle_balance(this_rq, idle))",
            "\t\treturn;",
            "",
            "\t/* normal load balance */",
            "\tsched_balance_update_blocked_averages(this_rq->cpu);",
            "\tsched_balance_domains(this_rq, idle);",
            "}",
            "void sched_balance_trigger(struct rq *rq)",
            "{",
            "\t/*",
            "\t * Don't need to rebalance while attached to NULL domain or",
            "\t * runqueue CPU is not active",
            "\t */",
            "\tif (unlikely(on_null_domain(rq) || !cpu_active(cpu_of(rq))))",
            "\t\treturn;",
            "",
            "\tif (time_after_eq(jiffies, rq->next_balance))",
            "\t\traise_softirq(SCHED_SOFTIRQ);",
            "",
            "\tnohz_balancer_kick(rq);",
            "}",
            "static void rq_online_fair(struct rq *rq)",
            "{",
            "\tupdate_sysctl();",
            "",
            "\tupdate_runtime_enabled(rq);",
            "}",
            "static void rq_offline_fair(struct rq *rq)",
            "{",
            "\tupdate_sysctl();",
            "",
            "\t/* Ensure any throttled groups are reachable by pick_next_task */",
            "\tunthrottle_offline_cfs_rqs(rq);",
            "}",
            "static inline bool",
            "__entity_slice_used(struct sched_entity *se, int min_nr_tasks)",
            "{",
            "\tu64 rtime = se->sum_exec_runtime - se->prev_sum_exec_runtime;",
            "\tu64 slice = se->slice;",
            "",
            "\treturn (rtime * min_nr_tasks > slice);",
            "}",
            "static inline void task_tick_core(struct rq *rq, struct task_struct *curr)",
            "{",
            "\tif (!sched_core_enabled(rq))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If runqueue has only one task which used up its slice and",
            "\t * if the sibling is forced idle, then trigger schedule to",
            "\t * give forced idle task a chance.",
            "\t *",
            "\t * sched_slice() considers only this active rq and it gets the",
            "\t * whole slice. But during force idle, we have siblings acting",
            "\t * like a single runqueue and hence we need to consider runnable",
            "\t * tasks on this CPU and the forced idle CPU. Ideally, we should",
            "\t * go through the forced idle rq, but that would be a perf hit.",
            "\t * We can assume that the forced idle CPU has at least",
            "\t * MIN_NR_TASKS_DURING_FORCEIDLE - 1 tasks and use that to check",
            "\t * if we need to give up the CPU.",
            "\t */",
            "\tif (rq->core->core_forceidle_count && rq->cfs.nr_running == 1 &&",
            "\t    __entity_slice_used(&curr->se, MIN_NR_TASKS_DURING_FORCEIDLE))",
            "\t\tresched_curr(rq);",
            "}",
            "static void se_fi_update(const struct sched_entity *se, unsigned int fi_seq,",
            "\t\t\t bool forceidle)",
            "{",
            "\tfor_each_sched_entity(se) {",
            "\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);",
            "",
            "\t\tif (forceidle) {",
            "\t\t\tif (cfs_rq->forceidle_seq == fi_seq)",
            "\t\t\t\tbreak;",
            "\t\t\tcfs_rq->forceidle_seq = fi_seq;",
            "\t\t}",
            "",
            "\t\tcfs_rq->min_vruntime_fi = cfs_rq->min_vruntime;",
            "\t}",
            "}",
            "void task_vruntime_update(struct rq *rq, struct task_struct *p, bool in_fi)",
            "{",
            "\tstruct sched_entity *se = &p->se;",
            "",
            "\tif (p->sched_class != &fair_sched_class)",
            "\t\treturn;",
            "",
            "\tse_fi_update(se, rq->core->core_forceidle_seq, in_fi);",
            "}"
          ],
          "function_name": "sched_balance_softirq, sched_balance_trigger, rq_online_fair, rq_offline_fair, __entity_slice_used, task_tick_core, se_fi_update, task_vruntime_update",
          "description": "管理运行队列的负载平衡触发机制，在软中断中执行跨CPU的负载均衡，维护运行队列状态并在CPU上线/下线时更新相关参数，支持强制空闲态的任务调度决策。",
          "similarity": 0.6527351140975952
        },
        {
          "chunk_id": 71,
          "file_path": "kernel/sched/fair.c",
          "start_line": 12545,
          "end_line": 12717,
          "content": [
            "void nohz_run_idle_balance(int cpu)",
            "{",
            "\tunsigned int flags;",
            "",
            "\tflags = atomic_fetch_andnot(NOHZ_NEWILB_KICK, nohz_flags(cpu));",
            "",
            "\t/*",
            "\t * Update the blocked load only if no SCHED_SOFTIRQ is about to happen",
            "\t * (i.e. NOHZ_STATS_KICK set) and will do the same.",
            "\t */",
            "\tif ((flags == NOHZ_NEWILB_KICK) && !need_resched())",
            "\t\t_nohz_idle_balance(cpu_rq(cpu), NOHZ_STATS_KICK);",
            "}",
            "static void nohz_newidle_balance(struct rq *this_rq)",
            "{",
            "\tint this_cpu = this_rq->cpu;",
            "",
            "\t/*",
            "\t * This CPU doesn't want to be disturbed by scheduler",
            "\t * housekeeping",
            "\t */",
            "\tif (!housekeeping_cpu(this_cpu, HK_TYPE_SCHED))",
            "\t\treturn;",
            "",
            "\t/* Will wake up very soon. No time for doing anything else*/",
            "\tif (this_rq->avg_idle < sysctl_sched_migration_cost)",
            "\t\treturn;",
            "",
            "\t/* Don't need to update blocked load of idle CPUs*/",
            "\tif (!READ_ONCE(nohz.has_blocked) ||",
            "\t    time_before(jiffies, READ_ONCE(nohz.next_blocked)))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Set the need to trigger ILB in order to update blocked load",
            "\t * before entering idle state.",
            "\t */",
            "\tatomic_or(NOHZ_NEWILB_KICK, nohz_flags(this_cpu));",
            "}",
            "static inline void nohz_balancer_kick(struct rq *rq) { }",
            "static inline bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)",
            "{",
            "\treturn false;",
            "}",
            "static inline void nohz_newidle_balance(struct rq *this_rq) { }",
            "static int sched_balance_newidle(struct rq *this_rq, struct rq_flags *rf)",
            "{",
            "\tunsigned long next_balance = jiffies + HZ;",
            "\tint this_cpu = this_rq->cpu;",
            "\tint continue_balancing = 1;",
            "\tu64 t0, t1, curr_cost = 0;",
            "\tstruct sched_domain *sd;",
            "\tint pulled_task = 0;",
            "",
            "\tupdate_misfit_status(NULL, this_rq);",
            "",
            "\t/*",
            "\t * There is a task waiting to run. No need to search for one.",
            "\t * Return 0; the task will be enqueued when switching to idle.",
            "\t */",
            "\tif (this_rq->ttwu_pending)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * We must set idle_stamp _before_ calling sched_balance_rq()",
            "\t * for CPU_NEWLY_IDLE, such that we measure the this duration",
            "\t * as idle time.",
            "\t */",
            "\tthis_rq->idle_stamp = rq_clock(this_rq);",
            "",
            "\t/*",
            "\t * Do not pull tasks towards !active CPUs...",
            "\t */",
            "\tif (!cpu_active(this_cpu))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * This is OK, because current is on_cpu, which avoids it being picked",
            "\t * for load-balance and preemption/IRQs are still disabled avoiding",
            "\t * further scheduler activity on it and we're being very careful to",
            "\t * re-start the picking loop.",
            "\t */",
            "\trq_unpin_lock(this_rq, rf);",
            "",
            "\trcu_read_lock();",
            "\tsd = rcu_dereference_check_sched_domain(this_rq->sd);",
            "",
            "\tif (!get_rd_overloaded(this_rq->rd) ||",
            "\t    (sd && this_rq->avg_idle < sd->max_newidle_lb_cost)) {",
            "",
            "\t\tif (sd)",
            "\t\t\tupdate_next_balance(sd, &next_balance);",
            "\t\trcu_read_unlock();",
            "",
            "\t\tgoto out;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\traw_spin_rq_unlock(this_rq);",
            "",
            "\tt0 = sched_clock_cpu(this_cpu);",
            "\tsched_balance_update_blocked_averages(this_cpu);",
            "",
            "\trcu_read_lock();",
            "\tfor_each_domain(this_cpu, sd) {",
            "\t\tu64 domain_cost;",
            "",
            "\t\tupdate_next_balance(sd, &next_balance);",
            "",
            "\t\tif (this_rq->avg_idle < curr_cost + sd->max_newidle_lb_cost)",
            "\t\t\tbreak;",
            "",
            "\t\tif (sd->flags & SD_BALANCE_NEWIDLE) {",
            "",
            "\t\t\tpulled_task = sched_balance_rq(this_cpu, this_rq,",
            "\t\t\t\t\t\t   sd, CPU_NEWLY_IDLE,",
            "\t\t\t\t\t\t   &continue_balancing);",
            "",
            "\t\t\tt1 = sched_clock_cpu(this_cpu);",
            "\t\t\tdomain_cost = t1 - t0;",
            "\t\t\tcurr_cost += domain_cost;",
            "\t\t\tt0 = t1;",
            "",
            "\t\t\t/*",
            "\t\t\t * Failing newidle means it is not effective;",
            "\t\t\t * bump the cost so we end up doing less of it.",
            "\t\t\t */",
            "\t\t\tif (!pulled_task)",
            "\t\t\t\tdomain_cost = (3 * sd->max_newidle_lb_cost) / 2;",
            "",
            "\t\t\tupdate_newidle_cost(sd, domain_cost);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Stop searching for tasks to pull if there are",
            "\t\t * now runnable tasks on this rq.",
            "\t\t */",
            "\t\tif (pulled_task || !continue_balancing)",
            "\t\t\tbreak;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\traw_spin_rq_lock(this_rq);",
            "",
            "\tif (curr_cost > this_rq->max_idle_balance_cost)",
            "\t\tthis_rq->max_idle_balance_cost = curr_cost;",
            "",
            "\t/*",
            "\t * While browsing the domains, we released the rq lock, a task could",
            "\t * have been enqueued in the meantime. Since we're not going idle,",
            "\t * pretend we pulled a task.",
            "\t */",
            "\tif (this_rq->cfs.h_nr_running && !pulled_task)",
            "\t\tpulled_task = 1;",
            "",
            "\t/* Is there a task of a high priority class? */",
            "\tif (this_rq->nr_running != this_rq->cfs.h_nr_running)",
            "\t\tpulled_task = -1;",
            "",
            "out:",
            "\t/* Move the next balance forward */",
            "\tif (time_after(this_rq->next_balance, next_balance))",
            "\t\tthis_rq->next_balance = next_balance;",
            "",
            "\tif (pulled_task)",
            "\t\tthis_rq->idle_stamp = 0;",
            "\telse",
            "\t\tnohz_newidle_balance(this_rq);",
            "",
            "\trq_repin_lock(this_rq, rf);",
            "",
            "\treturn pulled_task;",
            "}"
          ],
          "function_name": "nohz_run_idle_balance, nohz_newidle_balance, nohz_balancer_kick, nohz_idle_balance, nohz_newidle_balance, sched_balance_newidle",
          "description": "处理新空闲状态下的负载平衡，当CPU即将进入空闲时触发负载调整，通过判断是否需要更新阻塞负载并设置相应标志位，协调多CPU间任务分布。",
          "similarity": 0.6509593725204468
        },
        {
          "chunk_id": 59,
          "file_path": "kernel/sched/fair.c",
          "start_line": 10031,
          "end_line": 10148,
          "content": [
            "static inline enum",
            "group_type group_classify(unsigned int imbalance_pct,",
            "\t\t\t  struct sched_group *group,",
            "\t\t\t  struct sg_lb_stats *sgs)",
            "{",
            "\tif (group_is_overloaded(imbalance_pct, sgs))",
            "\t\treturn group_overloaded;",
            "",
            "\tif (sg_imbalanced(group))",
            "\t\treturn group_imbalanced;",
            "",
            "\tif (sgs->group_asym_packing)",
            "\t\treturn group_asym_packing;",
            "",
            "\tif (sgs->group_smt_balance)",
            "\t\treturn group_smt_balance;",
            "",
            "\tif (sgs->group_misfit_task_load)",
            "\t\treturn group_misfit_task;",
            "",
            "\tif (!group_has_capacity(imbalance_pct, sgs))",
            "\t\treturn group_fully_busy;",
            "",
            "\treturn group_has_spare;",
            "}",
            "static bool sched_use_asym_prio(struct sched_domain *sd, int cpu)",
            "{",
            "\tif (!(sd->flags & SD_ASYM_PACKING))",
            "\t\treturn false;",
            "",
            "\tif (!sched_smt_active())",
            "\t\treturn true;",
            "",
            "\treturn sd->flags & SD_SHARE_CPUCAPACITY || is_core_idle(cpu);",
            "}",
            "static inline bool sched_asym(struct sched_domain *sd, int dst_cpu, int src_cpu)",
            "{",
            "\t/*",
            "\t * First check if @dst_cpu can do asym_packing load balance. Only do it",
            "\t * if it has higher priority than @src_cpu.",
            "\t */",
            "\treturn sched_use_asym_prio(sd, dst_cpu) &&",
            "\t\tsched_asym_prefer(dst_cpu, src_cpu);",
            "}",
            "static inline bool",
            "sched_group_asym(struct lb_env *env, struct sg_lb_stats *sgs, struct sched_group *group)",
            "{",
            "\t/*",
            "\t * CPU priorities do not make sense for SMT cores with more than one",
            "\t * busy sibling.",
            "\t */",
            "\tif ((group->flags & SD_SHARE_CPUCAPACITY) &&",
            "\t    (sgs->group_weight - sgs->idle_cpus != 1))",
            "\t\treturn false;",
            "",
            "\treturn sched_asym(env->sd, env->dst_cpu, READ_ONCE(group->asym_prefer_cpu));",
            "}",
            "static inline bool smt_vs_nonsmt_groups(struct sched_group *sg1,",
            "\t\t\t\t    struct sched_group *sg2)",
            "{",
            "\tif (!sg1 || !sg2)",
            "\t\treturn false;",
            "",
            "\treturn (sg1->flags & SD_SHARE_CPUCAPACITY) !=",
            "\t\t(sg2->flags & SD_SHARE_CPUCAPACITY);",
            "}",
            "static inline bool smt_balance(struct lb_env *env, struct sg_lb_stats *sgs,",
            "\t\t\t       struct sched_group *group)",
            "{",
            "\tif (env->idle == CPU_NOT_IDLE)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * For SMT source group, it is better to move a task",
            "\t * to a CPU that doesn't have multiple tasks sharing its CPU capacity.",
            "\t * Note that if a group has a single SMT, SD_SHARE_CPUCAPACITY",
            "\t * will not be on.",
            "\t */",
            "\tif (group->flags & SD_SHARE_CPUCAPACITY &&",
            "\t    sgs->sum_h_nr_running > 1)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "static inline long sibling_imbalance(struct lb_env *env,",
            "\t\t\t\t    struct sd_lb_stats *sds,",
            "\t\t\t\t    struct sg_lb_stats *busiest,",
            "\t\t\t\t    struct sg_lb_stats *local)",
            "{",
            "\tint ncores_busiest, ncores_local;",
            "\tlong imbalance;",
            "",
            "\tif (env->idle == CPU_NOT_IDLE || !busiest->sum_nr_running)",
            "\t\treturn 0;",
            "",
            "\tncores_busiest = sds->busiest->cores;",
            "\tncores_local = sds->local->cores;",
            "",
            "\tif (ncores_busiest == ncores_local) {",
            "\t\timbalance = busiest->sum_nr_running;",
            "\t\tlsub_positive(&imbalance, local->sum_nr_running);",
            "\t\treturn imbalance;",
            "\t}",
            "",
            "\t/* Balance such that nr_running/ncores ratio are same on both groups */",
            "\timbalance = ncores_local * busiest->sum_nr_running;",
            "\tlsub_positive(&imbalance, ncores_busiest * local->sum_nr_running);",
            "\t/* Normalize imbalance and do rounding on normalization */",
            "\timbalance = 2 * imbalance + ncores_local + ncores_busiest;",
            "\timbalance /= ncores_local + ncores_busiest;",
            "",
            "\t/* Take advantage of resource in an empty sched group */",
            "\tif (imbalance <= 1 && local->sum_nr_running == 0 &&",
            "\t    busiest->sum_nr_running > 1)",
            "\t\timbalance = 2;",
            "",
            "\treturn imbalance;",
            "}"
          ],
          "function_name": "group_classify, sched_use_asym_prio, sched_asym, sched_group_asym, smt_vs_nonsmt_groups, smt_balance, sibling_imbalance",
          "description": "该代码块实现调度策略分类和负载平衡决策。group_classify根据负载状态分类调度组类型，sched_asym和sibling_imbalance计算不同核心间任务迁移的收益，smt_balance评估SMT核心间的负载均衡需求，指导异构架构下的任务迁移决策。",
          "similarity": 0.6471322774887085
        }
      ]
    },
    {
      "source_file": "kernel/sched/pelt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:13:26\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\pelt.c`\n\n---\n\n# `sched/pelt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/pelt.c` 实现了 **Per-Entity Load Tracking（PELT）** 机制，这是 Linux 内核 CFS（Completely Fair Scheduler）调度器中用于精确跟踪每个调度实体（如任务或任务组）负载、可运行性和 CPU 利用率的核心算法。  \nPELT 将时间划分为约 1ms（1024ns）的周期段，使用指数衰减的几何级数对历史负载进行加权求和，使得近期负载权重更高，远期负载影响逐渐衰减。该机制为负载均衡、能效调度（如 EAS）和 CPU 频率调节等子系统提供关键的负载指标。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `decay_load(u64 val, u64 n)`  \n  计算负载值 `val` 经过 `n` 个时间单位后的衰减值，利用预计算的衰减系数表和位移优化实现高效指数衰减。\n\n- `__accumulate_pelt_segments(u64 periods, u32 d1, u32 d3)`  \n  计算跨越多个完整周期时，负载贡献的三部分之和：上一周期剩余部分（d1）、中间完整周期总和（d2）、当前周期已过部分（d3）。\n\n- `accumulate_sum(u64 delta, struct sched_avg *sa, unsigned long load, unsigned long runnable, int running)`  \n  核心累加函数，根据时间增量 `delta` 更新 `load_sum`、`runnable_sum` 和 `util_sum`，处理跨周期衰减与新贡献累加。\n\n- `___update_load_sum(u64 now, struct sched_avg *sa, unsigned long load, unsigned long runnable, int running)`  \n  入口函数，计算自上次更新以来的时间差，调用 `accumulate_sum` 更新负载总和，并处理时间回退等异常情况。\n\n- `___update_load_avg(struct sched_avg *sa, unsigned long load)`  \n  根据当前 `*_sum` 值和动态除数（divider）计算并更新 `load_avg`、`runnable_avg` 和 `util_avg`。\n\n### 关键数据结构\n\n- `struct sched_avg`  \n  存储 PELT 相关状态，包括：\n  - `load_sum` / `runnable_sum` / `util_sum`：衰减加权后的负载总和\n  - `load_avg` / `runnable_avg` / `util_avg`：归一化后的平均负载值\n  - `last_update_time`：上次更新时间戳\n  - `period_contrib`：当前周期内已累积的时间（<1024ns）\n\n## 3. 关键实现\n\n### 时间分段与衰减模型\n- 时间以 **1024ns（≈1μs）** 为基本单位，每 **1024 单位（≈1ms）** 构成一个 PELT 周期。\n- 衰减因子 `y` 满足 `y^32 ≈ 0.5`，即约 32ms 前的负载贡献衰减至当前的一半。\n- 负载历史表示为几何级数：`u₀ + u₁·y + u₂·y² + ...`，其中 `uᵢ` 是第 `i` 个周期内的可运行比例。\n\n### 高效衰减计算\n- `decay_load()` 利用 `y^32 = 1/2` 的特性，将 `y^n` 拆分为 `1/2^(n/32) * y^(n%32)`。\n- 通过右移操作快速计算 `1/2^k` 部分，再查表 `runnable_avg_yN_inv[]` 获取 `y^(n%32)` 的倒数，结合 `mul_u64_u32_shr` 完成乘法。\n\n### 负载累加三段式\n当时间增量跨越多个周期时，负载贡献分为：\n1. **d1**：上一周期未完成部分（`1024 - period_contrib`）\n2. **d2**：中间完整周期的理论最大贡献（`LOAD_AVG_MAX - decay_load(LOAD_AVG_MAX, periods) - 1024`）\n3. **d3**：当前周期已过部分（`delta % 1024`）\n\n### 动态归一化\n- 使用 `get_pelt_divider()` 获取当前周期位置对应的归一化除数，避免因周期未结束导致的平均值震荡。\n- 除数公式：`LOAD_AVG_MAX - 1024 + period_contrib`，确保最大负载值在 `[1002, 1024)` 区间稳定。\n\n### 状态一致性保障\n- 若 `load == 0`，强制 `runnable = running = 0`，避免已出队实体产生无效贡献。\n- 时间回退（如 TSC 切换）时直接重置 `last_update_time`，防止负时间差导致异常。\n\n## 4. 依赖关系\n\n- **头文件依赖**：  \n  依赖 `kernel/sched/sched.h` 中定义的 `struct sched_avg`、`SCHED_CAPACITY_SHIFT`、`LOAD_AVG_*` 常量及 `get_pelt_divider()` 等辅助函数。\n- **预计算表**：  \n  使用外部定义的 `runnable_avg_yN_inv[32]` 衰减系数表（通常在 `fair.c` 或 `pelt.h` 中初始化）。\n- **调度器集成**：  \n  被 `fair.c` 中的 CFS 调度实体（`sched_entity`）和 CFS 运行队列（`cfs_rq`）调用，用于更新任务/任务组的负载状态。\n- **能效调度**：  \n  为 Energy Aware Scheduling (EAS) 提供 `util_avg` 作为 CPU 需求预测依据。\n\n## 5. 使用场景\n\n- **任务负载跟踪**：  \n  每个 `task_struct` 的 `sched_entity` 通过 PELT 实时更新其 `load_avg` 和 `util_avg`，反映任务对 CPU 的历史需求。\n- **任务组调度**：  \n  CFS 任务组（`task_group`）的 `cfs_rq` 使用 PELT 聚合子任务的负载，实现层级化负载均衡。\n- **负载均衡决策**：  \n  `load_balance()` 等函数依据 `runnable_avg` 判断 CPU 间负载差异，触发任务迁移。\n- **CPU 频率调节**：  \n  CPUFreq 的 `schedutil` 调速器使用 `util_avg` 动态调整 CPU 频率，平衡性能与功耗。\n- **空闲负载处理**：  \n  在 `idle_balance()` 等场景中，即使任务已出队，仍需通过 PELT 正确衰减其历史负载贡献。",
      "similarity": 0.5890585780143738,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/pelt.c",
          "start_line": 256,
          "end_line": 384,
          "content": [
            "static __always_inline void",
            "___update_load_avg(struct sched_avg *sa, unsigned long load)",
            "{",
            "\tu32 divider = get_pelt_divider(sa);",
            "",
            "\t/*",
            "\t * Step 2: update *_avg.",
            "\t */",
            "\tsa->load_avg = div_u64(load * sa->load_sum, divider);",
            "\tsa->runnable_avg = div_u64(sa->runnable_sum, divider);",
            "\tWRITE_ONCE(sa->util_avg, sa->util_sum / divider);",
            "}",
            "int __update_load_avg_blocked_se(u64 now, struct sched_entity *se)",
            "{",
            "\tif (___update_load_sum(now, &se->avg, 0, 0, 0)) {",
            "\t\t___update_load_avg(&se->avg, se_weight(se));",
            "\t\ttrace_pelt_se_tp(se);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __update_load_avg_se(u64 now, struct cfs_rq *cfs_rq, struct sched_entity *se)",
            "{",
            "\tif (___update_load_sum(now, &se->avg, !!se->on_rq, se_runnable(se),",
            "\t\t\t\tcfs_rq->curr == se)) {",
            "",
            "\t\t___update_load_avg(&se->avg, se_weight(se));",
            "\t\tcfs_se_util_change(&se->avg);",
            "\t\ttrace_pelt_se_tp(se);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __update_load_avg_cfs_rq(u64 now, struct cfs_rq *cfs_rq)",
            "{",
            "\tif (___update_load_sum(now, &cfs_rq->avg,",
            "\t\t\t\tscale_load_down(cfs_rq->load.weight),",
            "\t\t\t\tcfs_rq->h_nr_running,",
            "\t\t\t\tcfs_rq->curr != NULL)) {",
            "",
            "\t\t___update_load_avg(&cfs_rq->avg, 1);",
            "\t\ttrace_pelt_cfs_tp(cfs_rq);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int update_rt_rq_load_avg(u64 now, struct rq *rq, int running)",
            "{",
            "\tif (___update_load_sum(now, &rq->avg_rt,",
            "\t\t\t\trunning,",
            "\t\t\t\trunning,",
            "\t\t\t\trunning)) {",
            "",
            "\t\t___update_load_avg(&rq->avg_rt, 1);",
            "\t\ttrace_pelt_rt_tp(rq);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int update_dl_rq_load_avg(u64 now, struct rq *rq, int running)",
            "{",
            "\tif (___update_load_sum(now, &rq->avg_dl,",
            "\t\t\t\trunning,",
            "\t\t\t\trunning,",
            "\t\t\t\trunning)) {",
            "",
            "\t\t___update_load_avg(&rq->avg_dl, 1);",
            "\t\ttrace_pelt_dl_tp(rq);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int update_hw_load_avg(u64 now, struct rq *rq, u64 capacity)",
            "{",
            "\tif (___update_load_sum(now, &rq->avg_hw,",
            "\t\t\t       capacity,",
            "\t\t\t       capacity,",
            "\t\t\t       capacity)) {",
            "\t\t___update_load_avg(&rq->avg_hw, 1);",
            "\t\ttrace_pelt_hw_tp(rq);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int update_irq_load_avg(struct rq *rq, u64 running)",
            "{",
            "\tint ret = 0;",
            "",
            "\t/*",
            "\t * We can't use clock_pelt because irq time is not accounted in",
            "\t * clock_task. Instead we directly scale the running time to",
            "\t * reflect the real amount of computation",
            "\t */",
            "\trunning = cap_scale(running, arch_scale_freq_capacity(cpu_of(rq)));",
            "\trunning = cap_scale(running, arch_scale_cpu_capacity(cpu_of(rq)));",
            "",
            "\t/*",
            "\t * We know the time that has been used by interrupt since last update",
            "\t * but we don't when. Let be pessimistic and assume that interrupt has",
            "\t * happened just before the update. This is not so far from reality",
            "\t * because interrupt will most probably wake up task and trig an update",
            "\t * of rq clock during which the metric is updated.",
            "\t * We start to decay with normal context time and then we add the",
            "\t * interrupt context time.",
            "\t * We can safely remove running from rq->clock because",
            "\t * rq->clock += delta with delta >= running",
            "\t */",
            "\tret = ___update_load_sum(rq->clock - running, &rq->avg_irq,",
            "\t\t\t\t0,",
            "\t\t\t\t0,",
            "\t\t\t\t0);",
            "\tret += ___update_load_sum(rq->clock, &rq->avg_irq,",
            "\t\t\t\t1,",
            "\t\t\t\t1,",
            "\t\t\t\t1);",
            "",
            "\tif (ret) {",
            "\t\t___update_load_avg(&rq->avg_irq, 1);",
            "\t\ttrace_pelt_irq_tp(rq);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "___update_load_avg, __update_load_avg_blocked_se, __update_load_avg_se, __update_load_avg_cfs_rq, update_rt_rq_load_avg, update_dl_rq_load_avg, update_hw_load_avg, update_irq_load_avg",
          "description": "提供多场景下的负载平均值更新接口，包含六个函数：___update_load_avg计算负载平均值；__update_load_avg_blocked_se更新阻塞任务实体；__update_load_avg_se处理CFS队列中任务实体；__update_load_avg_cfs_rq更新CFS队列负载；update_rt_rq_load_avg/update_dl_rq_load_avg分别处理实时/延迟调度队列；update_hw_load_avg和update_irq_load_avg分别更新硬件资源及中断负载统计",
          "similarity": 0.5745222568511963
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/pelt.c",
          "start_line": 31,
          "end_line": 178,
          "content": [
            "static u64 decay_load(u64 val, u64 n)",
            "{",
            "\tunsigned int local_n;",
            "",
            "\tif (unlikely(n > LOAD_AVG_PERIOD * 63))",
            "\t\treturn 0;",
            "",
            "\t/* after bounds checking we can collapse to 32-bit */",
            "\tlocal_n = n;",
            "",
            "\t/*",
            "\t * As y^PERIOD = 1/2, we can combine",
            "\t *    y^n = 1/2^(n/PERIOD) * y^(n%PERIOD)",
            "\t * With a look-up table which covers y^n (n<PERIOD)",
            "\t *",
            "\t * To achieve constant time decay_load.",
            "\t */",
            "\tif (unlikely(local_n >= LOAD_AVG_PERIOD)) {",
            "\t\tval >>= local_n / LOAD_AVG_PERIOD;",
            "\t\tlocal_n %= LOAD_AVG_PERIOD;",
            "\t}",
            "",
            "\tval = mul_u64_u32_shr(val, runnable_avg_yN_inv[local_n], 32);",
            "\treturn val;",
            "}",
            "static u32 __accumulate_pelt_segments(u64 periods, u32 d1, u32 d3)",
            "{",
            "\tu32 c1, c2, c3 = d3; /* y^0 == 1 */",
            "",
            "\t/*",
            "\t * c1 = d1 y^p",
            "\t */",
            "\tc1 = decay_load((u64)d1, periods);",
            "",
            "\t/*",
            "\t *            p-1",
            "\t * c2 = 1024 \\Sum y^n",
            "\t *            n=1",
            "\t *",
            "\t *              inf        inf",
            "\t *    = 1024 ( \\Sum y^n - \\Sum y^n - y^0 )",
            "\t *              n=0        n=p",
            "\t */",
            "\tc2 = LOAD_AVG_MAX - decay_load(LOAD_AVG_MAX, periods) - 1024;",
            "",
            "\treturn c1 + c2 + c3;",
            "}",
            "static __always_inline u32",
            "accumulate_sum(u64 delta, struct sched_avg *sa,",
            "\t       unsigned long load, unsigned long runnable, int running)",
            "{",
            "\tu32 contrib = (u32)delta; /* p == 0 -> delta < 1024 */",
            "\tu64 periods;",
            "",
            "\tdelta += sa->period_contrib;",
            "\tperiods = delta / 1024; /* A period is 1024us (~1ms) */",
            "",
            "\t/*",
            "\t * Step 1: decay old *_sum if we crossed period boundaries.",
            "\t */",
            "\tif (periods) {",
            "\t\tsa->load_sum = decay_load(sa->load_sum, periods);",
            "\t\tsa->runnable_sum =",
            "\t\t\tdecay_load(sa->runnable_sum, periods);",
            "\t\tsa->util_sum = decay_load((u64)(sa->util_sum), periods);",
            "",
            "\t\t/*",
            "\t\t * Step 2",
            "\t\t */",
            "\t\tdelta %= 1024;",
            "\t\tif (load) {",
            "\t\t\t/*",
            "\t\t\t * This relies on the:",
            "\t\t\t *",
            "\t\t\t * if (!load)",
            "\t\t\t *\trunnable = running = 0;",
            "\t\t\t *",
            "\t\t\t * clause from ___update_load_sum(); this results in",
            "\t\t\t * the below usage of @contrib to disappear entirely,",
            "\t\t\t * so no point in calculating it.",
            "\t\t\t */",
            "\t\t\tcontrib = __accumulate_pelt_segments(periods,",
            "\t\t\t\t\t1024 - sa->period_contrib, delta);",
            "\t\t}",
            "\t}",
            "\tsa->period_contrib = delta;",
            "",
            "\tif (load)",
            "\t\tsa->load_sum += load * contrib;",
            "\tif (runnable)",
            "\t\tsa->runnable_sum += runnable * contrib << SCHED_CAPACITY_SHIFT;",
            "\tif (running)",
            "\t\tsa->util_sum += contrib << SCHED_CAPACITY_SHIFT;",
            "",
            "\treturn periods;",
            "}",
            "static __always_inline int",
            "___update_load_sum(u64 now, struct sched_avg *sa,",
            "\t\t  unsigned long load, unsigned long runnable, int running)",
            "{",
            "\tu64 delta;",
            "",
            "\tdelta = now - sa->last_update_time;",
            "\t/*",
            "\t * This should only happen when time goes backwards, which it",
            "\t * unfortunately does during sched clock init when we swap over to TSC.",
            "\t */",
            "\tif ((s64)delta < 0) {",
            "\t\tsa->last_update_time = now;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/*",
            "\t * Use 1024ns as the unit of measurement since it's a reasonable",
            "\t * approximation of 1us and fast to compute.",
            "\t */",
            "\tdelta >>= 10;",
            "\tif (!delta)",
            "\t\treturn 0;",
            "",
            "\tsa->last_update_time += delta << 10;",
            "",
            "\t/*",
            "\t * running is a subset of runnable (weight) so running can't be set if",
            "\t * runnable is clear. But there are some corner cases where the current",
            "\t * se has been already dequeued but cfs_rq->curr still points to it.",
            "\t * This means that weight will be 0 but not running for a sched_entity",
            "\t * but also for a cfs_rq if the latter becomes idle. As an example,",
            "\t * this happens during idle_balance() which calls",
            "\t * sched_balance_update_blocked_averages().",
            "\t *",
            "\t * Also see the comment in accumulate_sum().",
            "\t */",
            "\tif (!load)",
            "\t\trunnable = running = 0;",
            "",
            "\t/*",
            "\t * Now we know we crossed measurement unit boundaries. The *_avg",
            "\t * accrues by two steps:",
            "\t *",
            "\t * Step 1: accumulate *_sum since last_update_time. If we haven't",
            "\t * crossed period boundaries, finish.",
            "\t */",
            "\tif (!accumulate_sum(delta, sa, load, runnable, running))",
            "\t\treturn 0;",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "decay_load, __accumulate_pelt_segments, accumulate_sum, ___update_load_sum",
          "description": "实现PELT核心算法，包含四个关键函数：decay_load通过位移运算模拟指数衰减；__accumulate_pelt_segments计算周期性负载贡献；accumulate_sum根据时间差更新负载、运行时和利用率的加权总和；___update_load_sum处理时间边界穿越时的衰减逻辑并触发更新。所有函数共同维护调度实体的动态负载统计",
          "similarity": 0.563750684261322
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/pelt.c",
          "start_line": 1,
          "end_line": 30,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Per Entity Load Tracking",
            " *",
            " *  Copyright (C) 2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>",
            " *",
            " *  Interactivity improvements by Mike Galbraith",
            " *  (C) 2007 Mike Galbraith <efault@gmx.de>",
            " *",
            " *  Various enhancements by Dmitry Adamushko.",
            " *  (C) 2007 Dmitry Adamushko <dmitry.adamushko@gmail.com>",
            " *",
            " *  Group scheduling enhancements by Srivatsa Vaddagiri",
            " *  Copyright IBM Corporation, 2007",
            " *  Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>",
            " *",
            " *  Scaled math optimizations by Thomas Gleixner",
            " *  Copyright (C) 2007, Thomas Gleixner <tglx@linutronix.de>",
            " *",
            " *  Adaptive scheduling granularity, math enhancements by Peter Zijlstra",
            " *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra",
            " *",
            " *  Move PELT related code from fair.c into this pelt.c file",
            " *  Author: Vincent Guittot <vincent.guittot@linaro.org>",
            " */",
            "",
            "/*",
            " * Approximate:",
            " *   val * y^n,    where y^32 ~= 0.5 (~1 scheduling period)",
            " */"
          ],
          "function_name": null,
          "description": "此代码块为PELT（Per-entity Load Tracking）模块的头部注释，声明了该模块的版权信息、作者及主要贡献者，并概述了PELT算法的目标，即通过时间衰减模型精确跟踪调度实体的负载变化，支持交互性优化、分组调度等功能。上下文不完整",
          "similarity": 0.5104526281356812
        }
      ]
    },
    {
      "source_file": "kernel/sched/sched.h",
      "md_summary": "> 自动生成时间: 2025-10-25 16:16:13\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\sched.h`\n\n---\n\n# `sched/sched.h` 技术文档\n\n## 1. 文件概述\n\n`sched/sched.h` 是 Linux 内核调度器（Scheduler）的核心内部头文件，定义了调度子系统内部使用的类型、宏、辅助函数和全局变量。该文件不对外暴露给其他子系统直接使用，而是作为调度器各组件（如 CFS、RT、Deadline 调度类）之间的内部接口和共享基础设施。它整合了任务状态管理、负载计算、策略判断、CPU 能力建模、cgroup 权重转换等关键调度逻辑，并为调试、性能追踪和平台适配提供支持。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct asym_cap_data`：用于描述非对称 CPU 架构中不同 CPU 集合的计算能力（capacity），支持异构多核系统（如 big.LITTLE）的调度优化。\n- `struct rq`（前向声明）：运行队列（runqueue）结构体，每个 CPU 对应一个，是调度器管理可运行任务的核心数据结构。\n- `struct cpuidle_state`（前向声明）：CPU 空闲状态信息，用于与调度器协同进行能效管理。\n\n### 关键全局变量\n- `scheduler_running`：标志调度器是否已启动。\n- `calc_load_update` / `calc_load_tasks`：用于全局负载（load average）计算的时间戳和任务计数。\n- `sysctl_sched_rt_period` / `sysctl_sched_rt_runtime`：实时任务带宽控制参数。\n- `sched_rr_timeslice`：SCHED_RR 策略的时间片长度。\n- `asym_cap_list`：非对称 CPU 能力数据的全局链表。\n\n### 核心辅助函数与宏\n- **任务策略判断函数**：\n  - `idle_policy()` / `task_has_idle_policy()`\n  - `normal_policy()` / `fair_policy()`\n  - `rt_policy()` / `task_has_rt_policy()`\n  - `dl_policy()` / `task_has_dl_policy()`\n  - `valid_policy()`\n- **负载与权重转换**：\n  - `scale_load()` / `scale_load_down()`：在内部高精度负载值与用户可见权重间转换。\n  - `sched_weight_from_cgroup()` / `sched_weight_to_cgroup()`：cgroup 权重与调度器内部权重的映射。\n- **时间与精度处理**：\n  - `NS_TO_JIFFIES()`：纳秒转 jiffies。\n  - `update_avg()`：指数移动平均（EMA）更新。\n  - `shr_bound()`：安全右移，避免未定义行为。\n- **特殊调度标志**：\n  - `SCHED_FLAG_SUGOV`：用于 schedutil 频率调节器的特殊标志，使相关 kworker 临时获得高于 SCHED_DEADLINE 的优先级。\n  - `dl_entity_is_special()`：判断 Deadline 实体是否为 SUGOV 特殊任务。\n\n### 重要宏定义\n- `TASK_ON_RQ_QUEUED` / `TASK_ON_RQ_MIGRATING`：`task_struct::on_rq` 字段的状态值。\n- `NICE_0_LOAD`：nice 值为 0 的任务对应的内部负载基准值。\n- `DL_SCALE`：SCHED_DEADLINE 内部计算的精度因子。\n- `RUNTIME_INF`：表示无限运行时间的常量。\n- `SCHED_WARN_ON()`：调度器专用的条件警告宏（仅在 `CONFIG_SCHED_DEBUG` 时生效）。\n\n## 3. 关键实现\n\n### 高精度负载计算（64 位优化）\n在 64 位架构上，通过 `NICE_0_LOAD_SHIFT = 2 * SCHED_FIXEDPOINT_SHIFT` 提升内部负载计算的精度，改善低权重任务组（如 nice +19）和深层 cgroup 层级的负载均衡效果。`scale_load()` 和 `scale_load_down()` 实现了用户权重与内部高精度负载值之间的无损转换。\n\n### 非对称 CPU 能力建模\n`asym_cap_data` 结构体结合 `cpu_capacity_span()` 宏，将具有相同计算能力的 CPU 归为一组，并通过全局链表 `asym_cap_list` 管理。这为调度器在异构系统中进行负载均衡和任务迁移提供关键拓扑信息。\n\n### cgroup 权重标准化\n通过 `sched_weight_from_cgroup()` 和 `sched_weight_to_cgroup()`，将 cgroup 接口的权重范围（1–10000，默认 100）映射到调度器内部使用的权重值（基于 1024 基准），确保用户配置与调度行为的一致性。\n\n### SCHED_DEADLINE 与频率调节协同\n引入 `SCHED_FLAG_SUGOV` 标志，允许 `schedutil` 频率调节器的工作线程在需要时临时突破 SCHED_DEADLINE 的优先级限制，以解决某些平台无法原子切换 CPU 频率的问题。这是一种临时性 workaround，依赖于 `dl_entity_is_special()` 进行识别。\n\n### 安全位运算\n`shr_bound()` 宏确保右移操作不会因移位数过大而触发未定义行为（UB），通过 `min_t()` 将移位数限制在 `BITS_PER_TYPE(val) - 1` 以内。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- **调度子系统内部**：包含多个调度相关子模块头文件（如 `affinity.h`, `deadline.h`, `topology.h`, `cpupri.h` 等）。\n- **核心内核设施**：依赖 `atomic.h`, `rcupdate.h`, `cpumask_api.h`, `ktime_api.h`, `trace/events/sched.h` 等。\n- **平台与虚拟化**：条件包含 `asm/paravirt.h`（半虚拟化支持）和 `asm/barrier.h`（内存屏障）。\n- **工作队列**：包含 `../workqueue_internal.h`，用于与工作队列子系统交互。\n\n### 配置选项依赖\n- `CONFIG_64BIT`：启用高精度负载计算。\n- `CONFIG_SCHED_DEBUG`：启用 `SCHED_WARN_ON()` 调试检查。\n- `CONFIG_CPU_FREQ_GOV_SCHEDUTIL`：启用 `SCHED_FLAG_SUGOV` 相关逻辑。\n- `CONFIG_SCHED_CLASS_EXT`：扩展调度类支持（影响 `normal_policy()` 判断）。\n- `CONFIG_PARAVIRT`：半虚拟化支持。\n\n## 5. 使用场景\n\n- **调度器初始化与运行**：`scheduler_running` 和负载计算变量在调度器启动和周期性负载更新中使用。\n- **任务调度策略处理**：所有调度类（CFS、RT、Deadline、Idle）在入队、出队、选择下一个任务时，通过策略判断函数确定任务类型。\n- **负载均衡与迁移**：`asym_cap_data` 和 CPU 拓扑信息用于跨 CPU 的任务迁移决策，尤其在异构系统中。\n- **cgroup 资源控制**：在设置或读取 cgroup 的 CPU 权重时，通过权重转换函数确保调度器内部表示与用户接口一致。\n- **实时带宽管理**：`sysctl_sched_rt_*` 参数用于限制 SCHED_FIFO/SCHED_RR 任务的 CPU 使用率。\n- **能效调度协同**：`SCHED_FLAG_SUGOV` 机制使频率调节器能及时响应 Deadline 任务的性能需求。\n- **内核调试与追踪**：`SCHED_WARN_ON()` 用于捕获调度器内部异常状态；tracepoint 定义支持调度事件追踪。",
      "similarity": 0.5732979774475098,
      "chunks": []
    }
  ]
}