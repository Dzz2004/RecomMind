{
  "query": "进程上下文切换实现原理",
  "timestamp": "2025-12-26 00:00:40",
  "retrieved_files": [
    {
      "source_file": "kernel/irq_work.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:11:23\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq_work.c`\n\n---\n\n# `irq_work.c` 技术文档\n\n## 1. 文件概述\n\n`irq_work.c` 实现了一个轻量级的中断上下文工作队列机制，允许在硬中断（hardirq）或 NMI（不可屏蔽中断）上下文中安全地调度回调函数，并在稍后的硬中断上下文或专用内核线程中执行。该机制的核心目标是提供一种 **NMI 安全** 的方式来延迟执行某些不能在 NMI 或硬中断中直接完成的操作。\n\n该框架特别适用于需要从 NMI 或硬中断中触发后续处理（如 perf 事件、ftrace、RCU 等子系统）但又不能阻塞或执行复杂逻辑的场景。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- `struct irq_work`：表示一个中断工作项，包含回调函数 `func` 和状态标志（如 `IRQ_WORK_PENDING`、`IRQ_WORK_CLAIMED`、`IRQ_WORK_BUSY`、`IRQ_WORK_LAZY`、`IRQ_WORK_HARD_IRQ`）。\n- 每 CPU 变量：\n  - `raised_list`：存放需在硬中断上下文中立即处理的工作项。\n  - `lazy_list`：存放“惰性”工作项，在非硬中断上下文（如 tick 或专用线程）中处理。\n  - `irq_workd`：指向每 CPU 的 `irq_work` 内核线程（仅在 `CONFIG_PREEMPT_RT` 下使用）。\n\n### 主要函数\n\n| 函数 | 功能 |\n|------|------|\n| `irq_work_queue(struct irq_work *work)` | 在当前 CPU 上排队一个 `irq_work`，若未被声明则声明并入队。 |\n| `irq_work_queue_on(struct irq_work *work, int cpu)` | 将 `irq_work` 排队到指定 CPU（支持跨 CPU 调度）。 |\n| `irq_work_run(void)` | 在当前 CPU 上执行所有 `raised_list` 和（非 RT 下的）`lazy_list` 中的工作项。 |\n| `irq_work_tick(void)` | 由时钟 tick 调用，处理未被硬中断处理的 `raised_list` 和 `lazy_list`。 |\n| `irq_work_sync(struct irq_work *work)` | 同步等待指定 `irq_work` 执行完毕。 |\n| `irq_work_single(void *arg)` | 执行单个工作项的回调函数，并清理状态。 |\n| `arch_irq_work_raise(void)` | 架构相关函数，用于触发 IPI 或中断以唤醒处理逻辑（弱符号，默认为空）。 |\n\n## 3. 关键实现\n\n### 状态管理与原子操作\n\n- 每个 `irq_work` 通过 `atomic_t node.a_flags` 管理状态：\n  - `IRQ_WORK_PENDING`：表示工作项已入队但尚未执行。\n  - `IRQ_WORK_CLAIMED`：表示已被声明，防止重复入队。\n  - `IRQ_WORK_BUSY`：表示正在执行中。\n- `irq_work_claim()` 使用 `atomic_fetch_or()` 原子地设置 `CLAIMED` 和 `PENDING` 标志，并检查是否已存在，避免重复入队。\n\n### 双队列设计\n\n- **`raised_list`**：用于需要尽快在硬中断上下文执行的工作（如标记为 `IRQ_WORK_HARD_IRQ` 的项）。\n- **`lazy_list`**：\n  - 在非 RT 内核中，由 `irq_work_tick()` 或 `irq_work_run()` 在软中断或进程上下文中处理。\n  - 在 `CONFIG_PREEMPT_RT` 下，由每 CPU 的 `irq_work/%u` 内核线程处理（以避免在硬中断中执行非硬实时代码）。\n\n### NMI 安全性\n\n- 入队操作（如 `irq_work_queue`）仅使用原子操作和每 CPU 链表（`llist`），不涉及锁或内存分配，因此可在 NMI 上下文中安全调用。\n- 跨 CPU 入队时（`irq_work_queue_on`）会检查 `in_nmi()`，防止在 NMI 中调用非 NMI 安全的 IPI 发送函数。\n\n### PREEMPT_RT 支持\n\n- 在 RT 内核中，非 `IRQ_WORK_HARD_IRQ` 的工作项被放入 `lazy_list`，并通过专用内核线程执行，以避免在硬中断中运行可能阻塞或延迟高的代码。\n- 使用 `rcuwait` 机制实现 `irq_work_sync()` 的睡眠等待。\n\n### IPI 触发机制\n\n- 若架构支持（通过 `arch_irq_work_has_interrupt()`），调用 `arch_irq_work_raise()` 触发本地中断处理。\n- 否则依赖时钟 tick（`irq_work_tick`）或显式调用 `irq_work_run` 来处理队列。\n\n## 4. 依赖关系\n\n- **架构依赖**：\n  - `arch_irq_work_raise()` 和 `arch_irq_work_has_interrupt()` 需由具体架构实现（如 x86 提供）。\n- **内核子系统**：\n  - `llist`（无锁链表）：用于高效、无锁的每 CPU 队列管理。\n  - `smpboot`：用于注册每 CPU 内核线程（RT 模式）。\n  - `rcu`：`rcuwait` 用于同步等待（RT 模式）。\n  - `tick`：`tick_nohz_tick_stopped()` 用于判断是否需要立即触发处理。\n  - `trace_events`：IPI 跟踪点 `trace_ipi_send_cpu`。\n- **配置选项**：\n  - `CONFIG_SMP`：启用跨 CPU 调度和 IPI 支持。\n  - `CONFIG_PREEMPT_RT`：启用 RT 模式下的线程化处理。\n\n## 5. 使用场景\n\n- **性能监控（perf）**：从 NMI 中记录采样后，通过 `irq_work` 安全地将数据传递到常规上下文处理。\n- **ftrace / tracing**：在中断上下文中触发延迟的跟踪事件处理。\n- **RCU**：某些 RCU 实现使用 `irq_work` 来触发宽限期处理。\n- **热插拔 CPU**：在 CPU 离线前通过 `flush_smp_call_function_queue()` 调用 `irq_work_run()` 确保工作项被清空。\n- **中断负载均衡或延迟处理**：将非关键中断处理逻辑延迟到更安全的上下文执行。\n\n该机制为内核提供了一种高效、安全且可扩展的中断后处理框架，尤其适用于实时性和可靠性要求高的子系统。",
      "similarity": 0.6299728155136108,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/irq_work.c",
          "start_line": 184,
          "end_line": 286,
          "content": [
            "bool irq_work_needs_cpu(void)",
            "{",
            "\tstruct llist_head *raised, *lazy;",
            "",
            "\traised = this_cpu_ptr(&raised_list);",
            "\tlazy = this_cpu_ptr(&lazy_list);",
            "",
            "\tif (llist_empty(raised) || arch_irq_work_has_interrupt())",
            "\t\tif (llist_empty(lazy))",
            "\t\t\treturn false;",
            "",
            "\t/* All work should have been flushed before going offline */",
            "\tWARN_ON_ONCE(cpu_is_offline(smp_processor_id()));",
            "",
            "\treturn true;",
            "}",
            "void irq_work_single(void *arg)",
            "{",
            "\tstruct irq_work *work = arg;",
            "\tint flags;",
            "",
            "\t/*",
            "\t * Clear the PENDING bit, after this point the @work can be re-used.",
            "\t * The PENDING bit acts as a lock, and we own it, so we can clear it",
            "\t * without atomic ops.",
            "\t */",
            "\tflags = atomic_read(&work->node.a_flags);",
            "\tflags &= ~IRQ_WORK_PENDING;",
            "\tatomic_set(&work->node.a_flags, flags);",
            "",
            "\t/*",
            "\t * See irq_work_claim().",
            "\t */",
            "\tsmp_mb();",
            "",
            "\tlockdep_irq_work_enter(flags);",
            "\twork->func(work);",
            "\tlockdep_irq_work_exit(flags);",
            "",
            "\t/*",
            "\t * Clear the BUSY bit, if set, and return to the free state if no-one",
            "\t * else claimed it meanwhile.",
            "\t */",
            "\t(void)atomic_cmpxchg(&work->node.a_flags, flags, flags & ~IRQ_WORK_BUSY);",
            "",
            "\tif ((IS_ENABLED(CONFIG_PREEMPT_RT) && !irq_work_is_hard(work)) ||",
            "\t    !arch_irq_work_has_interrupt())",
            "\t\trcuwait_wake_up(&work->irqwait);",
            "}",
            "static void irq_work_run_list(struct llist_head *list)",
            "{",
            "\tstruct irq_work *work, *tmp;",
            "\tstruct llist_node *llnode;",
            "",
            "\t/*",
            "\t * On PREEMPT_RT IRQ-work which is not marked as HARD will be processed",
            "\t * in a per-CPU thread in preemptible context. Only the items which are",
            "\t * marked as IRQ_WORK_HARD_IRQ will be processed in hardirq context.",
            "\t */",
            "\tBUG_ON(!irqs_disabled() && !IS_ENABLED(CONFIG_PREEMPT_RT));",
            "",
            "\tif (llist_empty(list))",
            "\t\treturn;",
            "",
            "\tllnode = llist_del_all(list);",
            "\tllist_for_each_entry_safe(work, tmp, llnode, node.llist)",
            "\t\tirq_work_single(work);",
            "}",
            "void irq_work_run(void)",
            "{",
            "\tirq_work_run_list(this_cpu_ptr(&raised_list));",
            "\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tirq_work_run_list(this_cpu_ptr(&lazy_list));",
            "\telse",
            "\t\twake_irq_workd();",
            "}",
            "void irq_work_tick(void)",
            "{",
            "\tstruct llist_head *raised = this_cpu_ptr(&raised_list);",
            "",
            "\tif (!llist_empty(raised) && !arch_irq_work_has_interrupt())",
            "\t\tirq_work_run_list(raised);",
            "",
            "\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tirq_work_run_list(this_cpu_ptr(&lazy_list));",
            "\telse",
            "\t\twake_irq_workd();",
            "}",
            "void irq_work_sync(struct irq_work *work)",
            "{",
            "\tlockdep_assert_irqs_enabled();",
            "\tmight_sleep();",
            "",
            "\tif ((IS_ENABLED(CONFIG_PREEMPT_RT) && !irq_work_is_hard(work)) ||",
            "\t    !arch_irq_work_has_interrupt()) {",
            "\t\trcuwait_wait_event(&work->irqwait, !irq_work_is_busy(work),",
            "\t\t\t\t   TASK_UNINTERRUPTIBLE);",
            "\t\treturn;",
            "\t}",
            "",
            "\twhile (irq_work_is_busy(work))",
            "\t\tcpu_relax();",
            "}"
          ],
          "function_name": "irq_work_needs_cpu, irq_work_single, irq_work_run_list, irq_work_run, irq_work_tick, irq_work_sync",
          "description": "处理工作项的实际执行流程，包含单次执行逻辑、链表遍历运行及同步等待机制，区分硬中断上下文与RCU等待状态的处理",
          "similarity": 0.6103541254997253
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/irq_work.c",
          "start_line": 31,
          "end_line": 157,
          "content": [
            "static void wake_irq_workd(void)",
            "{",
            "\tstruct task_struct *tsk = __this_cpu_read(irq_workd);",
            "",
            "\tif (!llist_empty(this_cpu_ptr(&lazy_list)) && tsk)",
            "\t\twake_up_process(tsk);",
            "}",
            "static void irq_work_wake(struct irq_work *entry)",
            "{",
            "\twake_irq_workd();",
            "}",
            "static int irq_workd_should_run(unsigned int cpu)",
            "{",
            "\treturn !llist_empty(this_cpu_ptr(&lazy_list));",
            "}",
            "static bool irq_work_claim(struct irq_work *work)",
            "{",
            "\tint oflags;",
            "",
            "\toflags = atomic_fetch_or(IRQ_WORK_CLAIMED | CSD_TYPE_IRQ_WORK, &work->node.a_flags);",
            "\t/*",
            "\t * If the work is already pending, no need to raise the IPI.",
            "\t * The pairing smp_mb() in irq_work_single() makes sure",
            "\t * everything we did before is visible.",
            "\t */",
            "\tif (oflags & IRQ_WORK_PENDING)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "void __weak arch_irq_work_raise(void)",
            "{",
            "\t/*",
            "\t * Lame architectures will get the timer tick callback",
            "\t */",
            "}",
            "static __always_inline void irq_work_raise(struct irq_work *work)",
            "{",
            "\tif (trace_ipi_send_cpu_enabled() && arch_irq_work_has_interrupt())",
            "\t\ttrace_ipi_send_cpu(smp_processor_id(), _RET_IP_, work->func);",
            "",
            "\tarch_irq_work_raise();",
            "}",
            "static void __irq_work_queue_local(struct irq_work *work)",
            "{",
            "\tstruct llist_head *list;",
            "\tbool rt_lazy_work = false;",
            "\tbool lazy_work = false;",
            "\tint work_flags;",
            "",
            "\twork_flags = atomic_read(&work->node.a_flags);",
            "\tif (work_flags & IRQ_WORK_LAZY)",
            "\t\tlazy_work = true;",
            "\telse if (IS_ENABLED(CONFIG_PREEMPT_RT) &&",
            "\t\t !(work_flags & IRQ_WORK_HARD_IRQ))",
            "\t\trt_lazy_work = true;",
            "",
            "\tif (lazy_work || rt_lazy_work)",
            "\t\tlist = this_cpu_ptr(&lazy_list);",
            "\telse",
            "\t\tlist = this_cpu_ptr(&raised_list);",
            "",
            "\tif (!llist_add(&work->node.llist, list))",
            "\t\treturn;",
            "",
            "\t/* If the work is \"lazy\", handle it from next tick if any */",
            "\tif (!lazy_work || tick_nohz_tick_stopped())",
            "\t\tirq_work_raise(work);",
            "}",
            "bool irq_work_queue(struct irq_work *work)",
            "{",
            "\t/* Only queue if not already pending */",
            "\tif (!irq_work_claim(work))",
            "\t\treturn false;",
            "",
            "\t/* Queue the entry and raise the IPI if needed. */",
            "\tpreempt_disable();",
            "\t__irq_work_queue_local(work);",
            "\tpreempt_enable();",
            "",
            "\treturn true;",
            "}",
            "bool irq_work_queue_on(struct irq_work *work, int cpu)",
            "{",
            "#ifndef CONFIG_SMP",
            "\treturn irq_work_queue(work);",
            "",
            "#else /* CONFIG_SMP: */",
            "\t/* All work should have been flushed before going offline */",
            "\tWARN_ON_ONCE(cpu_is_offline(cpu));",
            "",
            "\t/* Only queue if not already pending */",
            "\tif (!irq_work_claim(work))",
            "\t\treturn false;",
            "",
            "\tkasan_record_aux_stack_noalloc(work);",
            "",
            "\tpreempt_disable();",
            "\tif (cpu != smp_processor_id()) {",
            "\t\t/* Arch remote IPI send/receive backend aren't NMI safe */",
            "\t\tWARN_ON_ONCE(in_nmi());",
            "",
            "\t\t/*",
            "\t\t * On PREEMPT_RT the items which are not marked as",
            "\t\t * IRQ_WORK_HARD_IRQ are added to the lazy list and a HARD work",
            "\t\t * item is used on the remote CPU to wake the thread.",
            "\t\t */",
            "\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) &&",
            "\t\t    !(atomic_read(&work->node.a_flags) & IRQ_WORK_HARD_IRQ)) {",
            "",
            "\t\t\tif (!llist_add(&work->node.llist, &per_cpu(lazy_list, cpu)))",
            "\t\t\t\tgoto out;",
            "",
            "\t\t\twork = &per_cpu(irq_work_wakeup, cpu);",
            "\t\t\tif (!irq_work_claim(work))",
            "\t\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\t__smp_call_single_queue(cpu, &work->node.llist);",
            "\t} else {",
            "\t\t__irq_work_queue_local(work);",
            "\t}",
            "out:",
            "\tpreempt_enable();",
            "",
            "\treturn true;",
            "#endif /* CONFIG_SMP */",
            "}"
          ],
          "function_name": "wake_irq_workd, irq_work_wake, irq_workd_should_run, irq_work_claim, arch_irq_work_raise, irq_work_raise, __irq_work_queue_local, irq_work_queue, irq_work_queue_on",
          "description": "实现了中断工作项的排队逻辑，区分硬中断与延迟工作项，通过IPI或线程唤醒机制确保跨CPU执行，支持PREEMPT_RT配置下的延迟处理",
          "similarity": 0.577129065990448
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/irq_work.c",
          "start_line": 303,
          "end_line": 316,
          "content": [
            "static void run_irq_workd(unsigned int cpu)",
            "{",
            "\tirq_work_run_list(this_cpu_ptr(&lazy_list));",
            "}",
            "static void irq_workd_setup(unsigned int cpu)",
            "{",
            "\tsched_set_fifo_low(current);",
            "}",
            "static __init int irq_work_init_threads(void)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tBUG_ON(smpboot_register_percpu_thread(&irqwork_threads));",
            "\treturn 0;",
            "}"
          ],
          "function_name": "run_irq_workd, irq_workd_setup, irq_work_init_threads",
          "description": "初始化PREEMPT_RT环境下的per-CPU工作线程，注册并启动处理延迟工作项的专用线程，通过smpboot接口创建线程实体",
          "similarity": 0.5439569354057312
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq_work.c",
          "start_line": 1,
          "end_line": 30,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (C) 2010 Red Hat, Inc., Peter Zijlstra",
            " *",
            " * Provides a framework for enqueueing and running callbacks from hardirq",
            " * context. The enqueueing is NMI-safe.",
            " */",
            "",
            "#include <linux/bug.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/percpu.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/sched.h>",
            "#include <linux/tick.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/smp.h>",
            "#include <linux/smpboot.h>",
            "#include <asm/processor.h>",
            "#include <linux/kasan.h>",
            "",
            "#include <trace/events/ipi.h>",
            "",
            "static DEFINE_PER_CPU(struct llist_head, raised_list);",
            "static DEFINE_PER_CPU(struct llist_head, lazy_list);",
            "static DEFINE_PER_CPU(struct task_struct *, irq_workd);",
            ""
          ],
          "function_name": null,
          "description": "定义了用于管理中断工作队列的per-CPU链表（raised_list/lazy_list）和irq_workd线程指针，提供NMI安全的enqueue框架",
          "similarity": 0.5093541145324707
        }
      ]
    },
    {
      "source_file": "kernel/trace/blktrace.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:59:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `trace\\blktrace.c`\n\n---\n\n# `trace/blktrace.c` 技术文档\n\n## 1. 文件概述\n\n`trace/blktrace.c` 是 Linux 内核中块设备 I/O 跟踪（blktrace）机制的核心实现文件。该文件提供了对块设备 I/O 请求的全生命周期跟踪能力，包括请求的生成、调度、下发、完成等各个阶段。它支持两种输出模式：传统的 relayfs-based 输出（通过 `/sys/kernel/debug/block/`）和基于 ftrace 的统一跟踪输出。该机制广泛用于性能分析、I/O 行为调试和存储子系统优化。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- `struct blk_trace`：每个被跟踪的块设备对应的跟踪上下文，包含设备信息、过滤条件、relay 通道等。\n- `struct blk_io_trace`：I/O 跟踪记录的基本单元，包含时间戳、设备号、扇区、字节数、操作类型、PID、CPU 等信息。\n- `blk_tracer_opts` / `blk_tracer_flags`：控制跟踪输出格式的选项，如是否启用 cgroup 信息、是否使用经典格式等。\n- 全局变量：\n  - `blktrace_seq`：用于避免重复记录进程信息的序列号。\n  - `running_trace_list`：当前所有活跃的 `blk_trace` 实例链表。\n  - `blk_tracer_enabled`：指示是否启用了基于 ftrace 的 blk tracer。\n\n### 主要函数\n\n- `trace_note()`：通用通知记录函数，用于记录非 I/O 操作类事件（如进程名、时间戳、用户消息等）。\n- `trace_note_tsk()`：为当前任务记录一次进程信息（`BLK_TN_PROCESS`），避免重复记录。\n- `trace_note_time()`：记录绝对时间戳（`BLK_TN_TIMESTAMP`），用于同步跟踪时间。\n- `__blk_trace_note_message()`：允许内核其他模块向 blktrace 输出自定义消息（`BLK_TN_MESSAGE`），支持 cgroup 上下文。\n- `act_log_check()`：根据跟踪配置（动作掩码、LBA 范围、PID 过滤）判断是否应记录某次 I/O。\n- `__blk_add_trace()`：核心 I/O 跟踪函数，将 I/O 事件封装为 `blk_io_trace` 并写入 relay buffer 或 ftrace ring buffer。\n\n## 3. 关键实现\n\n### 双输出路径支持\n代码同时支持两种后端：\n- **RelayFS 路径**：传统 blktrace 使用 `relay_reserve()` 向 per-CPU relay buffer 写入原始二进制数据，用户空间通过 `blkparse` 解析。\n- **Ftrace 路径**：当 `blk_tracer_enabled` 为真时，使用 `trace_buffer_lock_reserve()` 将数据写入 ftrace 的全局 ring buffer，可通过 `trace_pipe` 或 `trace` 文件读取。\n\n### 动作类型编码\n通过位操作将 `bio`/`request` 的标志（如 `REQ_SYNC`、`REQ_META`、`REQ_FUA` 等）映射到 blktrace 的动作类型（`BLK_TC_*`）。使用宏 `MASK_TC_BIT` 实现高效转换，利用编译期常量优化。\n\n### 进程与 cgroup 上下文\n- **进程去重**：通过 `tsk->btrace_seq` 与全局 `blktrace_seq` 比较，确保每个进程在跟踪期间只记录一次名称。\n- **cgroup 支持**：当启用 `TRACE_BLK_OPT_CGROUP` 时，在跟踪记录中附加 cgroup ID（`cgid`），用于 I/O 资源隔离分析。\n\n### 中断上下文安全\n在 relay 路径中，使用 `local_irq_save()` 保护 `relay_reserve()`，防止中断处理程序干扰 per-CPU buffer 分配。\n\n### 过滤机制\n`act_log_check()` 实现三层过滤：\n1. **动作类型过滤**：通过 `act_mask` 位掩码控制记录哪些操作（读/写/flush 等）。\n2. **LBA 范围过滤**：仅记录指定扇区范围内的 I/O。\n3. **PID 过滤**：仅记录指定进程发起的 I/O。\n\n## 4. 依赖关系\n\n- **块设备层**：依赖 `<linux/blkdev.h>` 和 `../../block/blk.h` 获取块设备和请求队列信息。\n- **跟踪子系统**：\n  - 基于 ftrace 的实现依赖 `trace/events/block.h` 和 `trace_output.h`。\n  - 使用 `tracing_gen_ctx_flags()`、`trace_buffer_lock_reserve()` 等 ftrace 核心 API。\n- **cgroup 子系统**：当 `CONFIG_BLK_CGROUP` 启用时，依赖 `cgroup_id()` 获取 cgroup 标识。\n- **内存与同步原语**：使用 `percpu`、`slab`、`mutex`、`raw_spinlock` 等内核基础设施。\n- **用户空间接口**：通过 `debugfs` 暴露控制接口（虽未在本文件实现，但为 blktrace 整体机制的一部分）。\n\n## 5. 使用场景\n\n- **I/O 性能分析**：结合 `blktrace` + `blkparse` + `btt` 工具链，分析磁盘 I/O 延迟、队列深度、请求合并等行为。\n- **存储栈调试**：跟踪 I/O 在 block layer、设备驱动、硬件间的流转过程，定位性能瓶颈或异常。\n- **cgroup I/O 隔离验证**：通过 `blk_cgname` 选项，关联 I/O 请求与具体 cgroup，验证 blk-iocost 或 bfq 等控制器效果。\n- **内核开发与测试**：在块设备驱动或 I/O 调度器开发中，验证请求处理逻辑是否符合预期。\n- **系统监控**：通过 ftrace 接口实时监控关键 I/O 事件，集成到系统级性能监控框架中。",
      "similarity": 0.6253266334533691,
      "chunks": [
        {
          "chunk_id": 7,
          "file_path": "kernel/trace/blktrace.c",
          "start_line": 914,
          "end_line": 1031,
          "content": [
            "static void blk_add_trace_bio_complete(void *ignore,",
            "\t\t\t\t       struct request_queue *q, struct bio *bio)",
            "{",
            "\tblk_add_trace_bio(q, bio, BLK_TA_COMPLETE,",
            "\t\t\t  blk_status_to_errno(bio->bi_status));",
            "}",
            "static void blk_add_trace_bio_backmerge(void *ignore, struct bio *bio)",
            "{",
            "\tblk_add_trace_bio(bio->bi_bdev->bd_disk->queue, bio, BLK_TA_BACKMERGE,",
            "\t\t\t0);",
            "}",
            "static void blk_add_trace_bio_frontmerge(void *ignore, struct bio *bio)",
            "{",
            "\tblk_add_trace_bio(bio->bi_bdev->bd_disk->queue, bio, BLK_TA_FRONTMERGE,",
            "\t\t\t0);",
            "}",
            "static void blk_add_trace_bio_queue(void *ignore, struct bio *bio)",
            "{",
            "\tblk_add_trace_bio(bio->bi_bdev->bd_disk->queue, bio, BLK_TA_QUEUE, 0);",
            "}",
            "static void blk_add_trace_getrq(void *ignore, struct bio *bio)",
            "{",
            "\tblk_add_trace_bio(bio->bi_bdev->bd_disk->queue, bio, BLK_TA_GETRQ, 0);",
            "}",
            "static void blk_add_trace_plug(void *ignore, struct request_queue *q)",
            "{",
            "\tstruct blk_trace *bt;",
            "",
            "\trcu_read_lock();",
            "\tbt = rcu_dereference(q->blk_trace);",
            "\tif (bt)",
            "\t\t__blk_add_trace(bt, 0, 0, 0, BLK_TA_PLUG, 0, 0, NULL, 0);",
            "\trcu_read_unlock();",
            "}",
            "static void blk_add_trace_unplug(void *ignore, struct request_queue *q,",
            "\t\t\t\t    unsigned int depth, bool explicit)",
            "{",
            "\tstruct blk_trace *bt;",
            "",
            "\trcu_read_lock();",
            "\tbt = rcu_dereference(q->blk_trace);",
            "\tif (bt) {",
            "\t\t__be64 rpdu = cpu_to_be64(depth);",
            "\t\tu32 what;",
            "",
            "\t\tif (explicit)",
            "\t\t\twhat = BLK_TA_UNPLUG_IO;",
            "\t\telse",
            "\t\t\twhat = BLK_TA_UNPLUG_TIMER;",
            "",
            "\t\t__blk_add_trace(bt, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu, 0);",
            "\t}",
            "\trcu_read_unlock();",
            "}",
            "static void blk_add_trace_split(void *ignore, struct bio *bio, unsigned int pdu)",
            "{",
            "\tstruct request_queue *q = bio->bi_bdev->bd_disk->queue;",
            "\tstruct blk_trace *bt;",
            "",
            "\trcu_read_lock();",
            "\tbt = rcu_dereference(q->blk_trace);",
            "\tif (bt) {",
            "\t\t__be64 rpdu = cpu_to_be64(pdu);",
            "",
            "\t\t__blk_add_trace(bt, bio->bi_iter.bi_sector,",
            "\t\t\t\tbio->bi_iter.bi_size, bio->bi_opf, BLK_TA_SPLIT,",
            "\t\t\t\tblk_status_to_errno(bio->bi_status),",
            "\t\t\t\tsizeof(rpdu), &rpdu,",
            "\t\t\t\tblk_trace_bio_get_cgid(q, bio));",
            "\t}",
            "\trcu_read_unlock();",
            "}",
            "static void blk_add_trace_bio_remap(void *ignore, struct bio *bio, dev_t dev,",
            "\t\t\t\t    sector_t from)",
            "{",
            "\tstruct request_queue *q = bio->bi_bdev->bd_disk->queue;",
            "\tstruct blk_trace *bt;",
            "\tstruct blk_io_trace_remap r;",
            "",
            "\trcu_read_lock();",
            "\tbt = rcu_dereference(q->blk_trace);",
            "\tif (likely(!bt)) {",
            "\t\trcu_read_unlock();",
            "\t\treturn;",
            "\t}",
            "",
            "\tr.device_from = cpu_to_be32(dev);",
            "\tr.device_to   = cpu_to_be32(bio_dev(bio));",
            "\tr.sector_from = cpu_to_be64(from);",
            "",
            "\t__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,",
            "\t\t\tbio->bi_opf, BLK_TA_REMAP,",
            "\t\t\tblk_status_to_errno(bio->bi_status),",
            "\t\t\tsizeof(r), &r, blk_trace_bio_get_cgid(q, bio));",
            "\trcu_read_unlock();",
            "}",
            "static void blk_add_trace_rq_remap(void *ignore, struct request *rq, dev_t dev,",
            "\t\t\t\t   sector_t from)",
            "{",
            "\tstruct blk_trace *bt;",
            "\tstruct blk_io_trace_remap r;",
            "",
            "\trcu_read_lock();",
            "\tbt = rcu_dereference(rq->q->blk_trace);",
            "\tif (likely(!bt)) {",
            "\t\trcu_read_unlock();",
            "\t\treturn;",
            "\t}",
            "",
            "\tr.device_from = cpu_to_be32(dev);",
            "\tr.device_to   = cpu_to_be32(disk_devt(rq->q->disk));",
            "\tr.sector_from = cpu_to_be64(from);",
            "",
            "\t__blk_add_trace(bt, blk_rq_pos(rq), blk_rq_bytes(rq),",
            "\t\t\trq->cmd_flags, BLK_TA_REMAP, 0,",
            "\t\t\tsizeof(r), &r, blk_trace_request_get_cgid(rq));",
            "\trcu_read_unlock();",
            "}"
          ],
          "function_name": "blk_add_trace_bio_complete, blk_add_trace_bio_backmerge, blk_add_trace_bio_frontmerge, blk_add_trace_bio_queue, blk_add_trace_getrq, blk_add_trace_plug, blk_add_trace_unplug, blk_add_trace_split, blk_add_trace_bio_remap, blk_add_trace_rq_remap",
          "description": "该代码段实现了块设备子系统中多种I/O事件的跟踪记录机制，通过多个回调函数将不同阶段的I/O操作（如完成、合并、插拔、拆分、重映射等）转化为可追踪的数据。各函数通过RCU读锁访问`blk_trace`结构体，并调用`__blk_add_trace`填充特定事件类型的跟踪信息。由于缺少`blk_add_trace_bio`等关键函数的定义，上下文存在缺失。",
          "similarity": 0.5738651752471924
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/trace/blktrace.c",
          "start_line": 69,
          "end_line": 175,
          "content": [
            "static void trace_note(struct blk_trace *bt, pid_t pid, int action,",
            "\t\t       const void *data, size_t len, u64 cgid)",
            "{",
            "\tstruct blk_io_trace *t;",
            "\tstruct ring_buffer_event *event = NULL;",
            "\tstruct trace_buffer *buffer = NULL;",
            "\tunsigned int trace_ctx = 0;",
            "\tint cpu = smp_processor_id();",
            "\tbool blk_tracer = blk_tracer_enabled;",
            "\tssize_t cgid_len = cgid ? sizeof(cgid) : 0;",
            "",
            "\tif (blk_tracer) {",
            "\t\tbuffer = blk_tr->array_buffer.buffer;",
            "\t\ttrace_ctx = tracing_gen_ctx_flags(0);",
            "\t\tevent = trace_buffer_lock_reserve(buffer, TRACE_BLK,",
            "\t\t\t\t\t\t  sizeof(*t) + len + cgid_len,",
            "\t\t\t\t\t\t  trace_ctx);",
            "\t\tif (!event)",
            "\t\t\treturn;",
            "\t\tt = ring_buffer_event_data(event);",
            "\t\tgoto record_it;",
            "\t}",
            "",
            "\tif (!bt->rchan)",
            "\t\treturn;",
            "",
            "\tt = relay_reserve(bt->rchan, sizeof(*t) + len + cgid_len);",
            "\tif (t) {",
            "\t\tt->magic = BLK_IO_TRACE_MAGIC | BLK_IO_TRACE_VERSION;",
            "\t\tt->time = ktime_to_ns(ktime_get());",
            "record_it:",
            "\t\tt->device = bt->dev;",
            "\t\tt->action = action | (cgid ? __BLK_TN_CGROUP : 0);",
            "\t\tt->pid = pid;",
            "\t\tt->cpu = cpu;",
            "\t\tt->pdu_len = len + cgid_len;",
            "\t\tif (cgid_len)",
            "\t\t\tmemcpy((void *)t + sizeof(*t), &cgid, cgid_len);",
            "\t\tmemcpy((void *) t + sizeof(*t) + cgid_len, data, len);",
            "",
            "\t\tif (blk_tracer)",
            "\t\t\ttrace_buffer_unlock_commit(blk_tr, buffer, event, trace_ctx);",
            "\t}",
            "}",
            "static void trace_note_tsk(struct task_struct *tsk)",
            "{",
            "\tunsigned long flags;",
            "\tstruct blk_trace *bt;",
            "",
            "\ttsk->btrace_seq = blktrace_seq;",
            "\traw_spin_lock_irqsave(&running_trace_lock, flags);",
            "\tlist_for_each_entry(bt, &running_trace_list, running_list) {",
            "\t\ttrace_note(bt, tsk->pid, BLK_TN_PROCESS, tsk->comm,",
            "\t\t\t   sizeof(tsk->comm), 0);",
            "\t}",
            "\traw_spin_unlock_irqrestore(&running_trace_lock, flags);",
            "}",
            "static void trace_note_time(struct blk_trace *bt)",
            "{",
            "\tstruct timespec64 now;",
            "\tunsigned long flags;",
            "\tu32 words[2];",
            "",
            "\t/* need to check user space to see if this breaks in y2038 or y2106 */",
            "\tktime_get_real_ts64(&now);",
            "\twords[0] = (u32)now.tv_sec;",
            "\twords[1] = now.tv_nsec;",
            "",
            "\tlocal_irq_save(flags);",
            "\ttrace_note(bt, 0, BLK_TN_TIMESTAMP, words, sizeof(words), 0);",
            "\tlocal_irq_restore(flags);",
            "}",
            "void __blk_trace_note_message(struct blk_trace *bt,",
            "\t\tstruct cgroup_subsys_state *css, const char *fmt, ...)",
            "{",
            "\tint n;",
            "\tva_list args;",
            "\tunsigned long flags;",
            "\tchar *buf;",
            "\tu64 cgid = 0;",
            "",
            "\tif (unlikely(bt->trace_state != Blktrace_running &&",
            "\t\t     !blk_tracer_enabled))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If the BLK_TC_NOTIFY action mask isn't set, don't send any note",
            "\t * message to the trace.",
            "\t */",
            "\tif (!(bt->act_mask & BLK_TC_NOTIFY))",
            "\t\treturn;",
            "",
            "\tlocal_irq_save(flags);",
            "\tbuf = this_cpu_ptr(bt->msg_data);",
            "\tva_start(args, fmt);",
            "\tn = vscnprintf(buf, BLK_TN_MAX_MSG, fmt, args);",
            "\tva_end(args);",
            "",
            "#ifdef CONFIG_BLK_CGROUP",
            "\tif (css && (blk_tracer_flags.val & TRACE_BLK_OPT_CGROUP))",
            "\t\tcgid = cgroup_id(css->cgroup);",
            "\telse",
            "\t\tcgid = 1;",
            "#endif",
            "\ttrace_note(bt, current->pid, BLK_TN_MESSAGE, buf, n, cgid);",
            "\tlocal_irq_restore(flags);",
            "}"
          ],
          "function_name": "trace_note, trace_note_tsk, trace_note_time, __blk_trace_note_message",
          "description": "实现trace_note系列函数，将I/O事件记录到ring buffer或trace events，支持进程切换时间戳、消息记录等不同类型的跟踪数据采集。",
          "similarity": 0.550579309463501
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/trace/blktrace.c",
          "start_line": 1797,
          "end_line": 1900,
          "content": [
            "static ssize_t sysfs_blk_trace_attr_store(struct device *dev,",
            "\t\t\t\t\t  struct device_attribute *attr,",
            "\t\t\t\t\t  const char *buf, size_t count)",
            "{",
            "\tstruct block_device *bdev = dev_to_bdev(dev);",
            "\tstruct request_queue *q = bdev_get_queue(bdev);",
            "\tstruct blk_trace *bt;",
            "\tu64 value;",
            "\tssize_t ret = -EINVAL;",
            "",
            "\tif (count == 0)",
            "\t\tgoto out;",
            "",
            "\tif (attr == &dev_attr_act_mask) {",
            "\t\tif (kstrtoull(buf, 0, &value)) {",
            "\t\t\t/* Assume it is a list of trace category names */",
            "\t\t\tret = blk_trace_str2mask(buf);",
            "\t\t\tif (ret < 0)",
            "\t\t\t\tgoto out;",
            "\t\t\tvalue = ret;",
            "\t\t}",
            "\t} else {",
            "\t\tif (kstrtoull(buf, 0, &value))",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tmutex_lock(&q->debugfs_mutex);",
            "",
            "\tbt = rcu_dereference_protected(q->blk_trace,",
            "\t\t\t\t       lockdep_is_held(&q->debugfs_mutex));",
            "\tif (attr == &dev_attr_enable) {",
            "\t\tif (!!value == !!bt) {",
            "\t\t\tret = 0;",
            "\t\t\tgoto out_unlock_bdev;",
            "\t\t}",
            "\t\tif (value)",
            "\t\t\tret = blk_trace_setup_queue(q, bdev);",
            "\t\telse",
            "\t\t\tret = blk_trace_remove_queue(q);",
            "\t\tgoto out_unlock_bdev;",
            "\t}",
            "",
            "\tret = 0;",
            "\tif (bt == NULL) {",
            "\t\tret = blk_trace_setup_queue(q, bdev);",
            "\t\tbt = rcu_dereference_protected(q->blk_trace,",
            "\t\t\t\tlockdep_is_held(&q->debugfs_mutex));",
            "\t}",
            "",
            "\tif (ret == 0) {",
            "\t\tif (attr == &dev_attr_act_mask)",
            "\t\t\tbt->act_mask = value;",
            "\t\telse if (attr == &dev_attr_pid)",
            "\t\t\tbt->pid = value;",
            "\t\telse if (attr == &dev_attr_start_lba)",
            "\t\t\tbt->start_lba = value;",
            "\t\telse if (attr == &dev_attr_end_lba)",
            "\t\t\tbt->end_lba = value;",
            "\t}",
            "",
            "out_unlock_bdev:",
            "\tmutex_unlock(&q->debugfs_mutex);",
            "out:",
            "\treturn ret ? ret : count;",
            "}",
            "void blk_fill_rwbs(char *rwbs, blk_opf_t opf)",
            "{",
            "\tint i = 0;",
            "",
            "\tif (opf & REQ_PREFLUSH)",
            "\t\trwbs[i++] = 'F';",
            "",
            "\tswitch (opf & REQ_OP_MASK) {",
            "\tcase REQ_OP_WRITE:",
            "\t\trwbs[i++] = 'W';",
            "\t\tbreak;",
            "\tcase REQ_OP_DISCARD:",
            "\t\trwbs[i++] = 'D';",
            "\t\tbreak;",
            "\tcase REQ_OP_SECURE_ERASE:",
            "\t\trwbs[i++] = 'D';",
            "\t\trwbs[i++] = 'E';",
            "\t\tbreak;",
            "\tcase REQ_OP_FLUSH:",
            "\t\trwbs[i++] = 'F';",
            "\t\tbreak;",
            "\tcase REQ_OP_READ:",
            "\t\trwbs[i++] = 'R';",
            "\t\tbreak;",
            "\tdefault:",
            "\t\trwbs[i++] = 'N';",
            "\t}",
            "",
            "\tif (opf & REQ_FUA)",
            "\t\trwbs[i++] = 'F';",
            "\tif (opf & REQ_RAHEAD)",
            "\t\trwbs[i++] = 'A';",
            "\tif (opf & REQ_SYNC)",
            "\t\trwbs[i++] = 'S';",
            "\tif (opf & REQ_META)",
            "\t\trwbs[i++] = 'M';",
            "",
            "\trwbs[i] = '\\0';",
            "}"
          ],
          "function_name": "sysfs_blk_trace_attr_store, blk_fill_rwbs",
          "description": "实现跟踪参数的写入处理及I/O操作类型的字符表示生成，用于通过sysfs接口修改跟踪器配置并格式化请求操作类型到日志中。",
          "similarity": 0.5415788888931274
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/trace/blktrace.c",
          "start_line": 1342,
          "end_line": 1443,
          "content": [
            "static void blk_log_generic(struct trace_seq *s, const struct trace_entry *ent, bool has_cg)",
            "{",
            "\tchar cmd[TASK_COMM_LEN];",
            "",
            "\ttrace_find_cmdline(ent->pid, cmd);",
            "",
            "\tif (t_action(ent) & BLK_TC_ACT(BLK_TC_PC)) {",
            "\t\ttrace_seq_printf(s, \"%u \", t_bytes(ent));",
            "\t\tblk_log_dump_pdu(s, ent, has_cg);",
            "\t\ttrace_seq_printf(s, \"[%s]\\n\", cmd);",
            "\t} else {",
            "\t\tif (t_sec(ent))",
            "\t\t\ttrace_seq_printf(s, \"%llu + %u [%s]\\n\",",
            "\t\t\t\t\t\tt_sector(ent), t_sec(ent), cmd);",
            "\t\telse",
            "\t\t\ttrace_seq_printf(s, \"[%s]\\n\", cmd);",
            "\t}",
            "}",
            "static void blk_log_with_error(struct trace_seq *s,",
            "\t\t\t      const struct trace_entry *ent, bool has_cg)",
            "{",
            "\tif (t_action(ent) & BLK_TC_ACT(BLK_TC_PC)) {",
            "\t\tblk_log_dump_pdu(s, ent, has_cg);",
            "\t\ttrace_seq_printf(s, \"[%d]\\n\", t_error(ent));",
            "\t} else {",
            "\t\tif (t_sec(ent))",
            "\t\t\ttrace_seq_printf(s, \"%llu + %u [%d]\\n\",",
            "\t\t\t\t\t t_sector(ent),",
            "\t\t\t\t\t t_sec(ent), t_error(ent));",
            "\t\telse",
            "\t\t\ttrace_seq_printf(s, \"%llu [%d]\\n\",",
            "\t\t\t\t\t t_sector(ent), t_error(ent));",
            "\t}",
            "}",
            "static void blk_log_remap(struct trace_seq *s, const struct trace_entry *ent, bool has_cg)",
            "{",
            "\tconst struct blk_io_trace_remap *__r = pdu_start(ent, has_cg);",
            "",
            "\ttrace_seq_printf(s, \"%llu + %u <- (%d,%d) %llu\\n\",",
            "\t\t\t t_sector(ent), t_sec(ent),",
            "\t\t\t MAJOR(be32_to_cpu(__r->device_from)),",
            "\t\t\t MINOR(be32_to_cpu(__r->device_from)),",
            "\t\t\t be64_to_cpu(__r->sector_from));",
            "}",
            "static void blk_log_plug(struct trace_seq *s, const struct trace_entry *ent, bool has_cg)",
            "{",
            "\tchar cmd[TASK_COMM_LEN];",
            "",
            "\ttrace_find_cmdline(ent->pid, cmd);",
            "",
            "\ttrace_seq_printf(s, \"[%s]\\n\", cmd);",
            "}",
            "static void blk_log_unplug(struct trace_seq *s, const struct trace_entry *ent, bool has_cg)",
            "{",
            "\tchar cmd[TASK_COMM_LEN];",
            "",
            "\ttrace_find_cmdline(ent->pid, cmd);",
            "",
            "\ttrace_seq_printf(s, \"[%s] %llu\\n\", cmd, get_pdu_int(ent, has_cg));",
            "}",
            "static void blk_log_split(struct trace_seq *s, const struct trace_entry *ent, bool has_cg)",
            "{",
            "\tchar cmd[TASK_COMM_LEN];",
            "",
            "\ttrace_find_cmdline(ent->pid, cmd);",
            "",
            "\ttrace_seq_printf(s, \"%llu / %llu [%s]\\n\", t_sector(ent),",
            "\t\t\t get_pdu_int(ent, has_cg), cmd);",
            "}",
            "static void blk_log_msg(struct trace_seq *s, const struct trace_entry *ent,",
            "\t\t\tbool has_cg)",
            "{",
            "",
            "\ttrace_seq_putmem(s, pdu_start(ent, has_cg),",
            "\t\tpdu_real_len(ent, has_cg));",
            "\ttrace_seq_putc(s, '\\n');",
            "}",
            "static void blk_tracer_print_header(struct seq_file *m)",
            "{",
            "\tif (!(blk_tracer_flags.val & TRACE_BLK_OPT_CLASSIC))",
            "\t\treturn;",
            "\tseq_puts(m, \"# DEV   CPU TIMESTAMP     PID ACT FLG\\n\"",
            "\t\t    \"#  |     |     |           |   |   |\\n\");",
            "}",
            "static void blk_tracer_start(struct trace_array *tr)",
            "{",
            "\tblk_tracer_enabled = true;",
            "}",
            "static int blk_tracer_init(struct trace_array *tr)",
            "{",
            "\tblk_tr = tr;",
            "\tblk_tracer_start(tr);",
            "\treturn 0;",
            "}",
            "static void blk_tracer_stop(struct trace_array *tr)",
            "{",
            "\tblk_tracer_enabled = false;",
            "}",
            "static void blk_tracer_reset(struct trace_array *tr)",
            "{",
            "\tblk_tracer_stop(tr);",
            "}"
          ],
          "function_name": "blk_log_generic, blk_log_with_error, blk_log_remap, blk_log_plug, blk_log_unplug, blk_log_split, blk_log_msg, blk_tracer_print_header, blk_tracer_start, blk_tracer_init, blk_tracer_stop, blk_tracer_reset",
          "description": "定义了块设备跟踪的日志记录函数，用于将I/O操作、错误、重映射等事件格式化为可追踪的文本格式，包含对不同跟踪动作的处理逻辑及跟踪器启停控制。",
          "similarity": 0.53465735912323
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/trace/blktrace.c",
          "start_line": 1627,
          "end_line": 1735,
          "content": [
            "static int blk_trace_setup_queue(struct request_queue *q,",
            "\t\t\t\t struct block_device *bdev)",
            "{",
            "\tstruct blk_trace *bt = NULL;",
            "\tint ret = -ENOMEM;",
            "",
            "\tbt = kzalloc(sizeof(*bt), GFP_KERNEL);",
            "\tif (!bt)",
            "\t\treturn -ENOMEM;",
            "",
            "\tbt->msg_data = __alloc_percpu(BLK_TN_MAX_MSG, __alignof__(char));",
            "\tif (!bt->msg_data)",
            "\t\tgoto free_bt;",
            "",
            "\tbt->dev = bdev->bd_dev;",
            "\tbt->act_mask = (u16)-1;",
            "",
            "\tblk_trace_setup_lba(bt, bdev);",
            "",
            "\trcu_assign_pointer(q->blk_trace, bt);",
            "\tget_probe_ref();",
            "\treturn 0;",
            "",
            "free_bt:",
            "\tblk_trace_free(q, bt);",
            "\treturn ret;",
            "}",
            "static int blk_trace_str2mask(const char *str)",
            "{",
            "\tint i;",
            "\tint mask = 0;",
            "\tchar *buf, *s, *token;",
            "",
            "\tbuf = kstrdup(str, GFP_KERNEL);",
            "\tif (buf == NULL)",
            "\t\treturn -ENOMEM;",
            "\ts = strstrip(buf);",
            "",
            "\twhile (1) {",
            "\t\ttoken = strsep(&s, \",\");",
            "\t\tif (token == NULL)",
            "\t\t\tbreak;",
            "",
            "\t\tif (*token == '\\0')",
            "\t\t\tcontinue;",
            "",
            "\t\tfor (i = 0; i < ARRAY_SIZE(mask_maps); i++) {",
            "\t\t\tif (strcasecmp(token, mask_maps[i].str) == 0) {",
            "\t\t\t\tmask |= mask_maps[i].mask;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t\tif (i == ARRAY_SIZE(mask_maps)) {",
            "\t\t\tmask = -EINVAL;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tkfree(buf);",
            "",
            "\treturn mask;",
            "}",
            "static ssize_t blk_trace_mask2str(char *buf, int mask)",
            "{",
            "\tint i;",
            "\tchar *p = buf;",
            "",
            "\tfor (i = 0; i < ARRAY_SIZE(mask_maps); i++) {",
            "\t\tif (mask & mask_maps[i].mask) {",
            "\t\t\tp += sprintf(p, \"%s%s\",",
            "\t\t\t\t    (p == buf) ? \"\" : \",\", mask_maps[i].str);",
            "\t\t}",
            "\t}",
            "\t*p++ = '\\n';",
            "",
            "\treturn p - buf;",
            "}",
            "static ssize_t sysfs_blk_trace_attr_show(struct device *dev,",
            "\t\t\t\t\t struct device_attribute *attr,",
            "\t\t\t\t\t char *buf)",
            "{",
            "\tstruct block_device *bdev = dev_to_bdev(dev);",
            "\tstruct request_queue *q = bdev_get_queue(bdev);",
            "\tstruct blk_trace *bt;",
            "\tssize_t ret = -ENXIO;",
            "",
            "\tmutex_lock(&q->debugfs_mutex);",
            "",
            "\tbt = rcu_dereference_protected(q->blk_trace,",
            "\t\t\t\t       lockdep_is_held(&q->debugfs_mutex));",
            "\tif (attr == &dev_attr_enable) {",
            "\t\tret = sprintf(buf, \"%u\\n\", !!bt);",
            "\t\tgoto out_unlock_bdev;",
            "\t}",
            "",
            "\tif (bt == NULL)",
            "\t\tret = sprintf(buf, \"disabled\\n\");",
            "\telse if (attr == &dev_attr_act_mask)",
            "\t\tret = blk_trace_mask2str(buf, bt->act_mask);",
            "\telse if (attr == &dev_attr_pid)",
            "\t\tret = sprintf(buf, \"%u\\n\", bt->pid);",
            "\telse if (attr == &dev_attr_start_lba)",
            "\t\tret = sprintf(buf, \"%llu\\n\", bt->start_lba);",
            "\telse if (attr == &dev_attr_end_lba)",
            "\t\tret = sprintf(buf, \"%llu\\n\", bt->end_lba);",
            "",
            "out_unlock_bdev:",
            "\tmutex_unlock(&q->debugfs_mutex);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "blk_trace_setup_queue, blk_trace_str2mask, blk_trace_mask2str, sysfs_blk_trace_attr_show",
          "description": "提供了块设备跟踪参数的配置接口，包含跟踪器初始化/移除逻辑、动作掩码字符串转换函数及sysfs属性读取实现，用于动态配置跟踪行为。",
          "similarity": 0.5264527797698975
        }
      ]
    },
    {
      "source_file": "kernel/events/core.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:22:14\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `events\\core.c`\n\n---\n\n# `events/core.c` 技术文档\n\n## 1. 文件概述\n\n`events/core.c` 是 Linux 内核性能事件子系统（perf events subsystem）的核心实现文件，负责提供性能监控事件（Performance Monitoring Events）的通用基础设施。该文件实现了事件调度、上下文管理、跨 CPU 函数调用、事件安装/移除、以及与任务和 CPU 上下文交互的核心逻辑，为硬件性能计数器、软件事件、跟踪点（tracepoints）和 BPF 程序等提供统一的抽象接口。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct remote_function_call`**  \n  用于在指定 CPU 或任务上下文中执行远程函数调用的封装结构，包含目标任务、函数指针、参数和返回值。\n\n- **`struct perf_cpu_context`**  \n  每个 CPU 的性能事件上下文，管理该 CPU 上所有与 CPU 绑定的性能事件。\n\n- **`struct perf_event_context`**  \n  性能事件的上下文容器，可绑定到特定任务（task）或 CPU，用于组织和管理一组相关的性能事件。\n\n- **`struct event_function_struct`**  \n  封装对特定 `perf_event` 执行操作的函数调用，用于通过 IPI 在目标 CPU 上安全执行事件操作。\n\n- **`TASK_TOMBSTONE`**  \n  特殊标记值 `((void *)-1L)`，用于标识内核事件（无关联用户任务）或已销毁的任务上下文。\n\n### 主要函数\n\n- **`task_function_call()`**  \n  在指定任务当前运行的 CPU 上执行给定函数。若任务不在目标 CPU 上运行，则返回 `-ESRCH`；支持重试机制以应对 CPU 离线等并发情况。\n\n- **`cpu_function_call()`**  \n  在指定 CPU 上执行函数，若 CPU 离线则返回 `-ENXIO`。\n\n- **`perf_ctx_lock()` / `perf_ctx_unlock()`**  \n  获取/释放 CPU 上下文和任务上下文的自旋锁，确保对性能事件上下文的并发访问安全。\n\n- **`event_function_call()`**  \n  安全地在事件所属的 CPU 或任务上下文中调用指定操作函数，是修改性能事件状态的主要入口。\n\n- **`perf_cpu_task_ctx()`**  \n  获取当前 CPU 上绑定的任务性能事件上下文（`task_ctx`），需在中断关闭状态下调用。\n\n- **`is_kernel_event()`**  \n  判断一个性能事件是否为内核事件（即不绑定到任何用户任务）。\n\n## 3. 关键实现\n\n### 远程函数调用机制\n\n文件通过 `remote_function_call` 结构和 `smp_call_function_single()` 实现跨 CPU 的安全函数调用：\n- `task_function_call()` 用于任务绑定事件：先通过 `task_cpu()` 获取目标 CPU，再通过 IPI 在该 CPU 上执行 `remote_function()`。\n- `remote_function()` 在目标 CPU 上验证任务是否正在运行（`p == current`），防止因任务迁移导致操作错位。\n- 支持重试逻辑（`for (;;)` 循环 + `cond_resched()`），应对目标 CPU 离线或任务迁移等并发场景。\n\n### 上下文锁管理\n\n性能事件操作需同时锁定 CPU 上下文（`perf_cpu_context`）和任务上下文（`perf_event_context`）：\n- 使用 `raw_spin_lock` 保证中断上下文下的安全性。\n- 提供 RAII 风格的构造/析构辅助宏（`class_perf_ctx_lock_constructor/destructor`），虽未在片段中完整使用，但体现锁管理意图。\n- `event_function()` 中通过双重验证（`ctx->task == current` 且 `ctx->is_active`）确保操作上下文一致性。\n\n### 任务上下文调度逻辑\n\n- 当任务上下文中的事件数（`nr_events`）为 0 时，不参与调度，以提升性能。\n- 添加首个事件到空任务上下文时需特殊处理（见 `perf_install_in_context()`），因 `ctx->is_active` 尚未设置，不能使用常规的 `event_function_call()`。\n- `TASK_TOMBSTONE` 标记用于区分内核事件和已退出任务的上下文，避免无效操作。\n\n### 事件操作的安全执行\n\n`event_function_call()` 是修改事件状态的标准路径：\n- 对非子事件（`!event->parent`），要求调用者已持有 `ctx->mutex`，确保事件-上下文关系稳定。\n- 对任务事件，使用 `task_function_call()`；对 CPU 事件，使用 `cpu_function_call()`。\n- 在本地执行路径中（如重试时），显式获取上下文锁并重新验证 `ctx->task` 状态，防止并发修改。\n\n## 4. 依赖关系\n\n- **架构相关代码**：依赖 `<asm/irq_regs.h>` 获取中断寄存器状态。\n- **核心内核子系统**：\n  - 调度器（`<linux/sched/*.h>`）：任务状态、CPU 亲和性、上下文切换。\n  - 内存管理（`<linux/mm.h>`, `<linux/vmalloc.h>`）：事件缓冲区内存分配。\n  - 中断与 SMP（`<linux/smp.h>`, `<linux/hardirq.h>`）：跨 CPU 通信与中断控制。\n  - 文件系统（`<linux/fs.h>`, `<linux/anon_inodes.h>`）：perf 事件文件描述符创建。\n- **其他性能子系统**：\n  - `perf_event.h`：性能事件核心 API 与数据结构定义。\n  - `trace_events.h`：与 ftrace 集成。\n  - `hw_breakpoint.h`：硬件断点事件支持。\n  - `bpf.h` / `filter.h`：BPF 程序附加到性能事件。\n- **内部头文件**：`internal.h` 包含 perf 子系统私有定义。\n\n## 5. 使用场景\n\n- **性能分析工具**：`perf` 用户态工具通过系统调用（如 `perf_event_open`）创建事件，内核通过本文件管理其生命周期。\n- **动态跟踪**：附加 BPF 程序到 kprobe/uprobe/tracepoint 事件时，使用本文件提供的上下文管理机制。\n- **硬件性能监控**：CPU 性能计数器（如 Intel PMU）事件的启用、读取和溢出处理。\n- **任务级监控**：对特定进程的 CPU 周期、缓存未命中、分支预测失败等指标进行采样。\n- **系统级监控**：全局 CPU 或软件事件（如上下文切换、页错误）的收集。\n- **内核自检与调试**：内核内部使用 perf 事件进行性能剖析或行为追踪。",
      "similarity": 0.6222677230834961,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/events/core.c",
          "start_line": 72,
          "end_line": 200,
          "content": [
            "static void remote_function(void *data)",
            "{",
            "\tstruct remote_function_call *tfc = data;",
            "\tstruct task_struct *p = tfc->p;",
            "",
            "\tif (p) {",
            "\t\t/* -EAGAIN */",
            "\t\tif (task_cpu(p) != smp_processor_id())",
            "\t\t\treturn;",
            "",
            "\t\t/*",
            "\t\t * Now that we're on right CPU with IRQs disabled, we can test",
            "\t\t * if we hit the right task without races.",
            "\t\t */",
            "",
            "\t\ttfc->ret = -ESRCH; /* No such (running) process */",
            "\t\tif (p != current)",
            "\t\t\treturn;",
            "\t}",
            "",
            "\ttfc->ret = tfc->func(tfc->info);",
            "}",
            "static int",
            "task_function_call(struct task_struct *p, remote_function_f func, void *info)",
            "{",
            "\tstruct remote_function_call data = {",
            "\t\t.p\t= p,",
            "\t\t.func\t= func,",
            "\t\t.info\t= info,",
            "\t\t.ret\t= -EAGAIN,",
            "\t};",
            "\tint ret;",
            "",
            "\tfor (;;) {",
            "\t\tret = smp_call_function_single(task_cpu(p), remote_function,",
            "\t\t\t\t\t       &data, 1);",
            "\t\tif (!ret)",
            "\t\t\tret = data.ret;",
            "",
            "\t\tif (ret != -EAGAIN)",
            "\t\t\tbreak;",
            "",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "static int cpu_function_call(int cpu, remote_function_f func, void *info)",
            "{",
            "\tstruct remote_function_call data = {",
            "\t\t.p\t= NULL,",
            "\t\t.func\t= func,",
            "\t\t.info\t= info,",
            "\t\t.ret\t= -ENXIO, /* No such CPU */",
            "\t};",
            "",
            "\tsmp_call_function_single(cpu, remote_function, &data, 1);",
            "",
            "\treturn data.ret;",
            "}",
            "static void perf_ctx_lock(struct perf_cpu_context *cpuctx,",
            "\t\t\t  struct perf_event_context *ctx)",
            "{",
            "\traw_spin_lock(&cpuctx->ctx.lock);",
            "\tif (ctx)",
            "\t\traw_spin_lock(&ctx->lock);",
            "}",
            "static void perf_ctx_unlock(struct perf_cpu_context *cpuctx,",
            "\t\t\t    struct perf_event_context *ctx)",
            "{",
            "\tif (ctx)",
            "\t\traw_spin_unlock(&ctx->lock);",
            "\traw_spin_unlock(&cpuctx->ctx.lock);",
            "}",
            "static inline void class_perf_ctx_lock_destructor(class_perf_ctx_lock_t *_T)",
            "{ perf_ctx_unlock(_T->cpuctx, _T->ctx); }",
            "static inline class_perf_ctx_lock_t",
            "class_perf_ctx_lock_constructor(struct perf_cpu_context *cpuctx,",
            "\t\t\t\tstruct perf_event_context *ctx)",
            "{ perf_ctx_lock(cpuctx, ctx); return (class_perf_ctx_lock_t){ cpuctx, ctx }; }",
            "static bool is_kernel_event(struct perf_event *event)",
            "{",
            "\treturn READ_ONCE(event->owner) == TASK_TOMBSTONE;",
            "}",
            "static int event_function(void *info)",
            "{",
            "\tstruct event_function_struct *efs = info;",
            "\tstruct perf_event *event = efs->event;",
            "\tstruct perf_event_context *ctx = event->ctx;",
            "\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);",
            "\tstruct perf_event_context *task_ctx = cpuctx->task_ctx;",
            "\tint ret = 0;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tperf_ctx_lock(cpuctx, task_ctx);",
            "\t/*",
            "\t * Since we do the IPI call without holding ctx->lock things can have",
            "\t * changed, double check we hit the task we set out to hit.",
            "\t */",
            "\tif (ctx->task) {",
            "\t\tif (ctx->task != current) {",
            "\t\t\tret = -ESRCH;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * We only use event_function_call() on established contexts,",
            "\t\t * and event_function() is only ever called when active (or",
            "\t\t * rather, we'll have bailed in task_function_call() or the",
            "\t\t * above ctx->task != current test), therefore we must have",
            "\t\t * ctx->is_active here.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(!ctx->is_active);",
            "\t\t/*",
            "\t\t * And since we have ctx->is_active, cpuctx->task_ctx must",
            "\t\t * match.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(task_ctx != ctx);",
            "\t} else {",
            "\t\tWARN_ON_ONCE(&cpuctx->ctx != ctx);",
            "\t}",
            "",
            "\tefs->func(event, cpuctx, ctx, efs->data);",
            "unlock:",
            "\tperf_ctx_unlock(cpuctx, task_ctx);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "remote_function, task_function_call, cpu_function_call, perf_ctx_lock, perf_ctx_unlock, class_perf_ctx_lock_destructor, class_perf_ctx_lock_constructor, is_kernel_event, event_function",
          "description": "实现跨CPU/任务的函数调用机制，提供上下文锁操作及内核事件判断功能",
          "similarity": 0.7208239436149597
        },
        {
          "chunk_id": 29,
          "file_path": "kernel/events/core.c",
          "start_line": 4367,
          "end_line": 4477,
          "content": [
            "void perf_event_task_tick(void)",
            "{",
            "\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);",
            "\tstruct perf_event_context *ctx;",
            "\tint throttled;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\t__this_cpu_inc(perf_throttled_seq);",
            "\tthrottled = __this_cpu_xchg(perf_throttled_count, 0);",
            "\ttick_dep_clear_cpu(smp_processor_id(), TICK_DEP_BIT_PERF_EVENTS);",
            "",
            "\tperf_adjust_freq_unthr_context(&cpuctx->ctx, !!throttled);",
            "",
            "\trcu_read_lock();",
            "\tctx = rcu_dereference(current->perf_event_ctxp);",
            "\tif (ctx)",
            "\t\tperf_adjust_freq_unthr_context(ctx, !!throttled);",
            "\trcu_read_unlock();",
            "}",
            "static int event_enable_on_exec(struct perf_event *event,",
            "\t\t\t\tstruct perf_event_context *ctx)",
            "{",
            "\tif (!event->attr.enable_on_exec)",
            "\t\treturn 0;",
            "",
            "\tevent->attr.enable_on_exec = 0;",
            "\tif (event->state >= PERF_EVENT_STATE_INACTIVE)",
            "\t\treturn 0;",
            "",
            "\tperf_event_set_state(event, PERF_EVENT_STATE_INACTIVE);",
            "",
            "\treturn 1;",
            "}",
            "static void perf_event_enable_on_exec(struct perf_event_context *ctx)",
            "{",
            "\tstruct perf_event_context *clone_ctx = NULL;",
            "\tenum event_type_t event_type = 0;",
            "\tstruct perf_cpu_context *cpuctx;",
            "\tstruct perf_event *event;",
            "\tunsigned long flags;",
            "\tint enabled = 0;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (WARN_ON_ONCE(current->perf_event_ctxp != ctx))",
            "\t\tgoto out;",
            "",
            "\tif (!ctx->nr_events)",
            "\t\tgoto out;",
            "",
            "\tcpuctx = this_cpu_ptr(&perf_cpu_context);",
            "\tperf_ctx_lock(cpuctx, ctx);",
            "\tctx_sched_out(ctx, EVENT_TIME);",
            "",
            "\tlist_for_each_entry(event, &ctx->event_list, event_entry) {",
            "\t\tenabled |= event_enable_on_exec(event, ctx);",
            "\t\tevent_type |= get_event_type(event);",
            "\t}",
            "",
            "\t/*",
            "\t * Unclone and reschedule this context if we enabled any event.",
            "\t */",
            "\tif (enabled) {",
            "\t\tclone_ctx = unclone_ctx(ctx);",
            "\t\tctx_resched(cpuctx, ctx, event_type);",
            "\t} else {",
            "\t\tctx_sched_in(ctx, EVENT_TIME);",
            "\t}",
            "\tperf_ctx_unlock(cpuctx, ctx);",
            "",
            "out:",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (clone_ctx)",
            "\t\tput_ctx(clone_ctx);",
            "}",
            "static void perf_event_remove_on_exec(struct perf_event_context *ctx)",
            "{",
            "\tstruct perf_event_context *clone_ctx = NULL;",
            "\tstruct perf_event *event, *next;",
            "\tunsigned long flags;",
            "\tbool modified = false;",
            "",
            "\tmutex_lock(&ctx->mutex);",
            "",
            "\tif (WARN_ON_ONCE(ctx->task != current))",
            "\t\tgoto unlock;",
            "",
            "\tlist_for_each_entry_safe(event, next, &ctx->event_list, event_entry) {",
            "\t\tif (!event->attr.remove_on_exec)",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!is_kernel_event(event))",
            "\t\t\tperf_remove_from_owner(event);",
            "",
            "\t\tmodified = true;",
            "",
            "\t\tperf_event_exit_event(event, ctx);",
            "\t}",
            "",
            "\traw_spin_lock_irqsave(&ctx->lock, flags);",
            "\tif (modified)",
            "\t\tclone_ctx = unclone_ctx(ctx);",
            "\traw_spin_unlock_irqrestore(&ctx->lock, flags);",
            "",
            "unlock:",
            "\tmutex_unlock(&ctx->mutex);",
            "",
            "\tif (clone_ctx)",
            "\t\tput_ctx(clone_ctx);",
            "}"
          ],
          "function_name": "perf_event_task_tick, event_enable_on_exec, perf_event_enable_on_exec, perf_event_remove_on_exec",
          "description": "该代码块实现基于进程执行上下文的事件管理，perf_event_task_tick处理任务切换时的频率调整，event_enable_on_exec和perf_event_enable_on_exec在exec时启用事件，perf_event_remove_on_exec处理exec时的事件移除逻辑，维护事件生命周期状态。",
          "similarity": 0.6886057257652283
        },
        {
          "chunk_id": 81,
          "file_path": "kernel/events/core.c",
          "start_line": 13123,
          "end_line": 13281,
          "content": [
            "void perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)",
            "{",
            "\tstruct perf_event_context *src_ctx, *dst_ctx;",
            "\tLIST_HEAD(events);",
            "",
            "\t/*",
            "\t * Since per-cpu context is persistent, no need to grab an extra",
            "\t * reference.",
            "\t */",
            "\tsrc_ctx = &per_cpu_ptr(&perf_cpu_context, src_cpu)->ctx;",
            "\tdst_ctx = &per_cpu_ptr(&perf_cpu_context, dst_cpu)->ctx;",
            "",
            "\t/*",
            "\t * See perf_event_ctx_lock() for comments on the details",
            "\t * of swizzling perf_event::ctx.",
            "\t */",
            "\tmutex_lock_double(&src_ctx->mutex, &dst_ctx->mutex);",
            "",
            "\t__perf_pmu_remove(src_ctx, src_cpu, pmu, &src_ctx->pinned_groups, &events);",
            "\t__perf_pmu_remove(src_ctx, src_cpu, pmu, &src_ctx->flexible_groups, &events);",
            "",
            "\tif (!list_empty(&events)) {",
            "\t\t/*",
            "\t\t * Wait for the events to quiesce before re-instating them.",
            "\t\t */",
            "\t\tsynchronize_rcu();",
            "",
            "\t\t__perf_pmu_install(dst_ctx, dst_cpu, pmu, &events);",
            "\t}",
            "",
            "\tmutex_unlock(&dst_ctx->mutex);",
            "\tmutex_unlock(&src_ctx->mutex);",
            "}",
            "static void sync_child_event(struct perf_event *child_event)",
            "{",
            "\tstruct perf_event *parent_event = child_event->parent;",
            "\tu64 child_val;",
            "",
            "\tif (child_event->attr.inherit_stat) {",
            "\t\tstruct task_struct *task = child_event->ctx->task;",
            "",
            "\t\tif (task && task != TASK_TOMBSTONE)",
            "\t\t\tperf_event_read_event(child_event, task);",
            "\t}",
            "",
            "\tchild_val = perf_event_count(child_event, false);",
            "",
            "\t/*",
            "\t * Add back the child's count to the parent's count:",
            "\t */",
            "\tatomic64_add(child_val, &parent_event->child_count);",
            "\tatomic64_add(child_event->total_time_enabled,",
            "\t\t     &parent_event->child_total_time_enabled);",
            "\tatomic64_add(child_event->total_time_running,",
            "\t\t     &parent_event->child_total_time_running);",
            "}",
            "static void",
            "perf_event_exit_event(struct perf_event *event, struct perf_event_context *ctx)",
            "{",
            "\tstruct perf_event *parent_event = event->parent;",
            "\tunsigned long detach_flags = 0;",
            "",
            "\tif (parent_event) {",
            "\t\t/*",
            "\t\t * Do not destroy the 'original' grouping; because of the",
            "\t\t * context switch optimization the original events could've",
            "\t\t * ended up in a random child task.",
            "\t\t *",
            "\t\t * If we were to destroy the original group, all group related",
            "\t\t * operations would cease to function properly after this",
            "\t\t * random child dies.",
            "\t\t *",
            "\t\t * Do destroy all inherited groups, we don't care about those",
            "\t\t * and being thorough is better.",
            "\t\t */",
            "\t\tdetach_flags = DETACH_GROUP | DETACH_CHILD;",
            "\t\tmutex_lock(&parent_event->child_mutex);",
            "\t}",
            "",
            "\tperf_remove_from_context(event, detach_flags | DETACH_EXIT);",
            "",
            "\t/*",
            "\t * Child events can be freed.",
            "\t */",
            "\tif (parent_event) {",
            "\t\tmutex_unlock(&parent_event->child_mutex);",
            "\t\t/*",
            "\t\t * Kick perf_poll() for is_event_hup();",
            "\t\t */",
            "\t\tperf_event_wakeup(parent_event);",
            "\t\tput_event(event);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Parent events are governed by their filedesc, retain them.",
            "\t */",
            "\tperf_event_wakeup(event);",
            "}",
            "static void perf_event_exit_task_context(struct task_struct *child)",
            "{",
            "\tstruct perf_event_context *child_ctx, *clone_ctx = NULL;",
            "\tstruct perf_event *child_event, *next;",
            "",
            "\tWARN_ON_ONCE(child != current);",
            "",
            "\tchild_ctx = perf_pin_task_context(child);",
            "\tif (!child_ctx)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * In order to reduce the amount of tricky in ctx tear-down, we hold",
            "\t * ctx::mutex over the entire thing. This serializes against almost",
            "\t * everything that wants to access the ctx.",
            "\t *",
            "\t * The exception is sys_perf_event_open() /",
            "\t * perf_event_create_kernel_count() which does find_get_context()",
            "\t * without ctx::mutex (it cannot because of the move_group double mutex",
            "\t * lock thing). See the comments in perf_install_in_context().",
            "\t */",
            "\tmutex_lock(&child_ctx->mutex);",
            "",
            "\t/*",
            "\t * In a single ctx::lock section, de-schedule the events and detach the",
            "\t * context from the task such that we cannot ever get it scheduled back",
            "\t * in.",
            "\t */",
            "\traw_spin_lock_irq(&child_ctx->lock);",
            "\ttask_ctx_sched_out(child_ctx, EVENT_ALL);",
            "",
            "\t/*",
            "\t * Now that the context is inactive, destroy the task <-> ctx relation",
            "\t * and mark the context dead.",
            "\t */",
            "\tRCU_INIT_POINTER(child->perf_event_ctxp, NULL);",
            "\tput_ctx(child_ctx); /* cannot be last */",
            "\tWRITE_ONCE(child_ctx->task, TASK_TOMBSTONE);",
            "\tput_task_struct(current); /* cannot be last */",
            "",
            "\tclone_ctx = unclone_ctx(child_ctx);",
            "\traw_spin_unlock_irq(&child_ctx->lock);",
            "",
            "\tif (clone_ctx)",
            "\t\tput_ctx(clone_ctx);",
            "",
            "\t/*",
            "\t * Report the task dead after unscheduling the events so that we",
            "\t * won't get any samples after PERF_RECORD_EXIT. We can however still",
            "\t * get a few PERF_RECORD_READ events.",
            "\t */",
            "\tperf_event_task(child, child_ctx, 0);",
            "",
            "\tlist_for_each_entry_safe(child_event, next, &child_ctx->event_list, event_entry)",
            "\t\tperf_event_exit_event(child_event, child_ctx);",
            "",
            "\tmutex_unlock(&child_ctx->mutex);",
            "",
            "\tput_ctx(child_ctx);",
            "}"
          ],
          "function_name": "perf_pmu_migrate_context, sync_child_event, perf_event_exit_event, perf_event_exit_task_context",
          "description": "迁移PMU上下文至目标CPU，同步子事件计数器，处理任务退出时的事件分离与销毁逻辑，确保上下文安全卸载",
          "similarity": 0.6783562898635864
        },
        {
          "chunk_id": 28,
          "file_path": "kernel/events/core.c",
          "start_line": 4175,
          "end_line": 4309,
          "content": [
            "static void",
            "perf_adjust_freq_unthr_context(struct perf_event_context *ctx, bool unthrottle)",
            "{",
            "\tstruct perf_event *event;",
            "\tstruct hw_perf_event *hwc;",
            "\tu64 now, period = TICK_NSEC;",
            "\ts64 delta;",
            "",
            "\t/*",
            "\t * only need to iterate over all events iff:",
            "\t * - context have events in frequency mode (needs freq adjust)",
            "\t * - there are events to unthrottle on this cpu",
            "\t */",
            "\tif (!(ctx->nr_freq || unthrottle))",
            "\t\treturn;",
            "",
            "\traw_spin_lock(&ctx->lock);",
            "",
            "\tlist_for_each_entry_rcu(event, &ctx->event_list, event_entry) {",
            "\t\tif (event->state != PERF_EVENT_STATE_ACTIVE)",
            "\t\t\tcontinue;",
            "",
            "\t\t// XXX use visit thingy to avoid the -1,cpu match",
            "\t\tif (!event_filter_match(event))",
            "\t\t\tcontinue;",
            "",
            "\t\tperf_pmu_disable(event->pmu);",
            "",
            "\t\thwc = &event->hw;",
            "",
            "\t\tif (hwc->interrupts == MAX_INTERRUPTS) {",
            "\t\t\thwc->interrupts = 0;",
            "\t\t\tperf_log_throttle(event, 1);",
            "\t\t\tevent->pmu->start(event, 0);",
            "\t\t}",
            "",
            "\t\tif (!event->attr.freq || !event->attr.sample_freq)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * stop the event and update event->count",
            "\t\t */",
            "\t\tevent->pmu->stop(event, PERF_EF_UPDATE);",
            "",
            "\t\tnow = local64_read(&event->count);",
            "\t\tdelta = now - hwc->freq_count_stamp;",
            "\t\thwc->freq_count_stamp = now;",
            "",
            "\t\t/*",
            "\t\t * restart the event",
            "\t\t * reload only if value has changed",
            "\t\t * we have stopped the event so tell that",
            "\t\t * to perf_adjust_period() to avoid stopping it",
            "\t\t * twice.",
            "\t\t */",
            "\t\tif (delta > 0)",
            "\t\t\tperf_adjust_period(event, period, delta, false);",
            "",
            "\t\tevent->pmu->start(event, delta > 0 ? PERF_EF_RELOAD : 0);",
            "\tnext:",
            "\t\tperf_pmu_enable(event->pmu);",
            "\t}",
            "",
            "\traw_spin_unlock(&ctx->lock);",
            "}",
            "static void rotate_ctx(struct perf_event_context *ctx, struct perf_event *event)",
            "{",
            "\t/*",
            "\t * Rotate the first entry last of non-pinned groups. Rotation might be",
            "\t * disabled by the inheritance code.",
            "\t */",
            "\tif (ctx->rotate_disable)",
            "\t\treturn;",
            "",
            "\tperf_event_groups_delete(&ctx->flexible_groups, event);",
            "\tperf_event_groups_insert(&ctx->flexible_groups, event);",
            "}",
            "static bool perf_rotate_context(struct perf_cpu_pmu_context *cpc)",
            "{",
            "\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);",
            "\tstruct perf_event_pmu_context *cpu_epc, *task_epc = NULL;",
            "\tstruct perf_event *cpu_event = NULL, *task_event = NULL;",
            "\tint cpu_rotate, task_rotate;",
            "\tstruct pmu *pmu;",
            "",
            "\t/*",
            "\t * Since we run this from IRQ context, nobody can install new",
            "\t * events, thus the event count values are stable.",
            "\t */",
            "",
            "\tcpu_epc = &cpc->epc;",
            "\tpmu = cpu_epc->pmu;",
            "\ttask_epc = cpc->task_epc;",
            "",
            "\tcpu_rotate = cpu_epc->rotate_necessary;",
            "\ttask_rotate = task_epc ? task_epc->rotate_necessary : 0;",
            "",
            "\tif (!(cpu_rotate || task_rotate))",
            "\t\treturn false;",
            "",
            "\tperf_ctx_lock(cpuctx, cpuctx->task_ctx);",
            "\tperf_pmu_disable(pmu);",
            "",
            "\tif (task_rotate)",
            "\t\ttask_event = ctx_event_to_rotate(task_epc);",
            "\tif (cpu_rotate)",
            "\t\tcpu_event = ctx_event_to_rotate(cpu_epc);",
            "",
            "\t/*",
            "\t * As per the order given at ctx_resched() first 'pop' task flexible",
            "\t * and then, if needed CPU flexible.",
            "\t */",
            "\tif (task_event || (task_epc && cpu_event)) {",
            "\t\tupdate_context_time(task_epc->ctx);",
            "\t\t__pmu_ctx_sched_out(task_epc, EVENT_FLEXIBLE);",
            "\t}",
            "",
            "\tif (cpu_event) {",
            "\t\tupdate_context_time(&cpuctx->ctx);",
            "\t\t__pmu_ctx_sched_out(cpu_epc, EVENT_FLEXIBLE);",
            "\t\trotate_ctx(&cpuctx->ctx, cpu_event);",
            "\t\t__pmu_ctx_sched_in(&cpuctx->ctx, pmu);",
            "\t}",
            "",
            "\tif (task_event)",
            "\t\trotate_ctx(task_epc->ctx, task_event);",
            "",
            "\tif (task_event || (task_epc && cpu_event))",
            "\t\t__pmu_ctx_sched_in(task_epc->ctx, pmu);",
            "",
            "\tperf_pmu_enable(pmu);",
            "\tperf_ctx_unlock(cpuctx, cpuctx->task_ctx);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "perf_adjust_freq_unthr_context, rotate_ctx, perf_rotate_context",
          "description": "该代码块实现频率模式事件的动态调整和上下文轮转机制，perf_adjust_freq_unthr_context停止并重启需要频率调整的事件，rotate_ctx和perf_rotate_context通过事件组操作实现上下文轮转，平衡CPU和任务级事件的调度优先级。",
          "similarity": 0.6737537384033203
        },
        {
          "chunk_id": 83,
          "file_path": "kernel/events/core.c",
          "start_line": 13558,
          "end_line": 13722,
          "content": [
            "static int inherit_group(struct perf_event *parent_event,",
            "\t      struct task_struct *parent,",
            "\t      struct perf_event_context *parent_ctx,",
            "\t      struct task_struct *child,",
            "\t      struct perf_event_context *child_ctx)",
            "{",
            "\tstruct perf_event *leader;",
            "\tstruct perf_event *sub;",
            "\tstruct perf_event *child_ctr;",
            "",
            "\tleader = inherit_event(parent_event, parent, parent_ctx,",
            "\t\t\t\t child, NULL, child_ctx);",
            "\tif (IS_ERR(leader))",
            "\t\treturn PTR_ERR(leader);",
            "\t/*",
            "\t * @leader can be NULL here because of is_orphaned_event(). In this",
            "\t * case inherit_event() will create individual events, similar to what",
            "\t * perf_group_detach() would do anyway.",
            "\t */",
            "\tfor_each_sibling_event(sub, parent_event) {",
            "\t\tchild_ctr = inherit_event(sub, parent, parent_ctx,",
            "\t\t\t\t\t    child, leader, child_ctx);",
            "\t\tif (IS_ERR(child_ctr))",
            "\t\t\treturn PTR_ERR(child_ctr);",
            "",
            "\t\tif (sub->aux_event == parent_event && child_ctr &&",
            "\t\t    !perf_get_aux_event(child_ctr, leader))",
            "\t\t\treturn -EINVAL;",
            "\t}",
            "\tif (leader)",
            "\t\tleader->group_generation = parent_event->group_generation;",
            "\treturn 0;",
            "}",
            "static int",
            "inherit_task_group(struct perf_event *event, struct task_struct *parent,",
            "\t\t   struct perf_event_context *parent_ctx,",
            "\t\t   struct task_struct *child,",
            "\t\t   u64 clone_flags, int *inherited_all)",
            "{",
            "\tstruct perf_event_context *child_ctx;",
            "\tint ret;",
            "",
            "\tif (!event->attr.inherit ||",
            "\t    (event->attr.inherit_thread && !(clone_flags & CLONE_THREAD)) ||",
            "\t    /* Do not inherit if sigtrap and signal handlers were cleared. */",
            "\t    (event->attr.sigtrap && (clone_flags & CLONE_CLEAR_SIGHAND))) {",
            "\t\t*inherited_all = 0;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tchild_ctx = child->perf_event_ctxp;",
            "\tif (!child_ctx) {",
            "\t\t/*",
            "\t\t * This is executed from the parent task context, so",
            "\t\t * inherit events that have been marked for cloning.",
            "\t\t * First allocate and initialize a context for the",
            "\t\t * child.",
            "\t\t */",
            "\t\tchild_ctx = alloc_perf_context(child);",
            "\t\tif (!child_ctx)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tchild->perf_event_ctxp = child_ctx;",
            "\t}",
            "",
            "\tret = inherit_group(event, parent, parent_ctx, child, child_ctx);",
            "\tif (ret)",
            "\t\t*inherited_all = 0;",
            "",
            "\treturn ret;",
            "}",
            "static int perf_event_init_context(struct task_struct *child, u64 clone_flags)",
            "{",
            "\tstruct perf_event_context *child_ctx, *parent_ctx;",
            "\tstruct perf_event_context *cloned_ctx;",
            "\tstruct perf_event *event;",
            "\tstruct task_struct *parent = current;",
            "\tint inherited_all = 1;",
            "\tunsigned long flags;",
            "\tint ret = 0;",
            "",
            "\tif (likely(!parent->perf_event_ctxp))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * If the parent's context is a clone, pin it so it won't get",
            "\t * swapped under us.",
            "\t */",
            "\tparent_ctx = perf_pin_task_context(parent);",
            "\tif (!parent_ctx)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * No need to check if parent_ctx != NULL here; since we saw",
            "\t * it non-NULL earlier, the only reason for it to become NULL",
            "\t * is if we exit, and since we're currently in the middle of",
            "\t * a fork we can't be exiting at the same time.",
            "\t */",
            "",
            "\t/*",
            "\t * Lock the parent list. No need to lock the child - not PID",
            "\t * hashed yet and not running, so nobody can access it.",
            "\t */",
            "\tmutex_lock(&parent_ctx->mutex);",
            "",
            "\t/*",
            "\t * We dont have to disable NMIs - we are only looking at",
            "\t * the list, not manipulating it:",
            "\t */",
            "\tperf_event_groups_for_each(event, &parent_ctx->pinned_groups) {",
            "\t\tret = inherit_task_group(event, parent, parent_ctx,",
            "\t\t\t\t\t child, clone_flags, &inherited_all);",
            "\t\tif (ret)",
            "\t\t\tgoto out_unlock;",
            "\t}",
            "",
            "\t/*",
            "\t * We can't hold ctx->lock when iterating the ->flexible_group list due",
            "\t * to allocations, but we need to prevent rotation because",
            "\t * rotate_ctx() will change the list from interrupt context.",
            "\t */",
            "\traw_spin_lock_irqsave(&parent_ctx->lock, flags);",
            "\tparent_ctx->rotate_disable = 1;",
            "\traw_spin_unlock_irqrestore(&parent_ctx->lock, flags);",
            "",
            "\tperf_event_groups_for_each(event, &parent_ctx->flexible_groups) {",
            "\t\tret = inherit_task_group(event, parent, parent_ctx,",
            "\t\t\t\t\t child, clone_flags, &inherited_all);",
            "\t\tif (ret)",
            "\t\t\tgoto out_unlock;",
            "\t}",
            "",
            "\traw_spin_lock_irqsave(&parent_ctx->lock, flags);",
            "\tparent_ctx->rotate_disable = 0;",
            "",
            "\tchild_ctx = child->perf_event_ctxp;",
            "",
            "\tif (child_ctx && inherited_all) {",
            "\t\t/*",
            "\t\t * Mark the child context as a clone of the parent",
            "\t\t * context, or of whatever the parent is a clone of.",
            "\t\t *",
            "\t\t * Note that if the parent is a clone, the holding of",
            "\t\t * parent_ctx->lock avoids it from being uncloned.",
            "\t\t */",
            "\t\tcloned_ctx = parent_ctx->parent_ctx;",
            "\t\tif (cloned_ctx) {",
            "\t\t\tchild_ctx->parent_ctx = cloned_ctx;",
            "\t\t\tchild_ctx->parent_gen = parent_ctx->parent_gen;",
            "\t\t} else {",
            "\t\t\tchild_ctx->parent_ctx = parent_ctx;",
            "\t\t\tchild_ctx->parent_gen = parent_ctx->generation;",
            "\t\t}",
            "\t\tget_ctx(child_ctx->parent_ctx);",
            "\t}",
            "",
            "\traw_spin_unlock_irqrestore(&parent_ctx->lock, flags);",
            "out_unlock:",
            "\tmutex_unlock(&parent_ctx->mutex);",
            "",
            "\tperf_unpin_context(parent_ctx);",
            "\tput_ctx(parent_ctx);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "inherit_group, inherit_task_group, perf_event_init_context",
          "description": "继承父进程的性能事件组，初始化子进程的事件上下文，建立上下文父子关系并标记生成代号以支持事件组管理",
          "similarity": 0.6635059118270874
        }
      ]
    }
  ]
}