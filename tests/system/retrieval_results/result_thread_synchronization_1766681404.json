{
  "query": "thread synchronization",
  "timestamp": "2025-12-26 00:50:04",
  "retrieved_files": [
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.6121730208396912,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1067,
          "end_line": 1170,
          "content": [
            "static void rcu_gp_kthread_wake(void)",
            "{",
            "\tstruct task_struct *t = READ_ONCE(rcu_state.gp_kthread);",
            "",
            "\tif ((current == t && !in_hardirq() && !in_serving_softirq()) ||",
            "\t    !READ_ONCE(rcu_state.gp_flags) || !t)",
            "\t\treturn;",
            "\tWRITE_ONCE(rcu_state.gp_wake_time, jiffies);",
            "\tWRITE_ONCE(rcu_state.gp_wake_seq, READ_ONCE(rcu_state.gp_seq));",
            "\tswake_up_one_online(&rcu_state.gp_wq);",
            "}",
            "static bool rcu_accelerate_cbs(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\tunsigned long gp_seq_req;",
            "\tbool ret = false;",
            "",
            "\trcu_lockdep_assert_cblist_protected(rdp);",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* If no pending (not yet ready to invoke) callbacks, nothing to do. */",
            "\tif (!rcu_segcblist_pend_cbs(&rdp->cblist))",
            "\t\treturn false;",
            "",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCbPreAcc\"));",
            "",
            "\t/*",
            "\t * Callbacks are often registered with incomplete grace-period",
            "\t * information.  Something about the fact that getting exact",
            "\t * information requires acquiring a global lock...  RCU therefore",
            "\t * makes a conservative estimate of the grace period number at which",
            "\t * a given callback will become ready to invoke.\tThe following",
            "\t * code checks this estimate and improves it when possible, thus",
            "\t * accelerating callback invocation to an earlier grace-period",
            "\t * number.",
            "\t */",
            "\tgp_seq_req = rcu_seq_snap(&rcu_state.gp_seq);",
            "\tif (rcu_segcblist_accelerate(&rdp->cblist, gp_seq_req))",
            "\t\tret = rcu_start_this_gp(rnp, rdp, gp_seq_req);",
            "",
            "\t/* Trace depending on how much we were able to accelerate. */",
            "\tif (rcu_segcblist_restempty(&rdp->cblist, RCU_WAIT_TAIL))",
            "\t\ttrace_rcu_grace_period(rcu_state.name, gp_seq_req, TPS(\"AccWaitCB\"));",
            "\telse",
            "\t\ttrace_rcu_grace_period(rcu_state.name, gp_seq_req, TPS(\"AccReadyCB\"));",
            "",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCbPostAcc\"));",
            "",
            "\treturn ret;",
            "}",
            "static void rcu_accelerate_cbs_unlocked(struct rcu_node *rnp,",
            "\t\t\t\t\tstruct rcu_data *rdp)",
            "{",
            "\tunsigned long c;",
            "\tbool needwake;",
            "",
            "\trcu_lockdep_assert_cblist_protected(rdp);",
            "\tc = rcu_seq_snap(&rcu_state.gp_seq);",
            "\tif (!READ_ONCE(rdp->gpwrap) && ULONG_CMP_GE(rdp->gp_seq_needed, c)) {",
            "\t\t/* Old request still live, so mark recent callbacks. */",
            "\t\t(void)rcu_segcblist_accelerate(&rdp->cblist, c);",
            "\t\treturn;",
            "\t}",
            "\traw_spin_lock_rcu_node(rnp); /* irqs already disabled. */",
            "\tneedwake = rcu_accelerate_cbs(rnp, rdp);",
            "\traw_spin_unlock_rcu_node(rnp); /* irqs remain disabled. */",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "}",
            "static bool rcu_advance_cbs(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\trcu_lockdep_assert_cblist_protected(rdp);",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* If no pending (not yet ready to invoke) callbacks, nothing to do. */",
            "\tif (!rcu_segcblist_pend_cbs(&rdp->cblist))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Find all callbacks whose ->gp_seq numbers indicate that they",
            "\t * are ready to invoke, and put them into the RCU_DONE_TAIL sublist.",
            "\t */",
            "\trcu_segcblist_advance(&rdp->cblist, rnp->gp_seq);",
            "",
            "\t/* Classify any remaining callbacks. */",
            "\treturn rcu_accelerate_cbs(rnp, rdp);",
            "}",
            "static void __maybe_unused rcu_advance_cbs_nowake(struct rcu_node *rnp,",
            "\t\t\t\t\t\t  struct rcu_data *rdp)",
            "{",
            "\trcu_lockdep_assert_cblist_protected(rdp);",
            "\tif (!rcu_seq_state(rcu_seq_current(&rnp->gp_seq)) || !raw_spin_trylock_rcu_node(rnp))",
            "\t\treturn;",
            "\t// The grace period cannot end while we hold the rcu_node lock.",
            "\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))",
            "\t\tWARN_ON_ONCE(rcu_advance_cbs(rnp, rdp));",
            "\traw_spin_unlock_rcu_node(rnp);",
            "}",
            "static void rcu_strict_gp_check_qs(void)",
            "{",
            "\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD)) {",
            "\t\trcu_read_lock();",
            "\t\trcu_read_unlock();",
            "\t}",
            "}"
          ],
          "function_name": "rcu_gp_kthread_wake, rcu_accelerate_cbs, rcu_accelerate_cbs_unlocked, rcu_advance_cbs, rcu_advance_cbs_nowake, rcu_strict_gp_check_qs",
          "description": "实现RCU grace period管理，通过唤醒kthread、加速回调处理和更新状态来推进grace period进度，包含回调列表优化、quiescent state检测及状态同步等功能。",
          "similarity": 0.5973858833312988
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1840,
          "end_line": 1973,
          "content": [
            "static int __noreturn rcu_gp_kthread(void *unused)",
            "{",
            "\trcu_bind_gp_kthread();",
            "\tfor (;;) {",
            "",
            "\t\t/* Handle grace-period start. */",
            "\t\tfor (;;) {",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwait\"));",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_WAIT_GPS);",
            "\t\t\tswait_event_idle_exclusive(rcu_state.gp_wq,",
            "\t\t\t\t\t READ_ONCE(rcu_state.gp_flags) &",
            "\t\t\t\t\t RCU_GP_FLAG_INIT);",
            "\t\t\trcu_gp_torture_wait();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_DONE_GPS);",
            "\t\t\t/* Locking provides needed memory barrier. */",
            "\t\t\tif (rcu_gp_init())",
            "\t\t\t\tbreak;",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\t\t\tWARN_ON(signal_pending(current));",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwaitsig\"));",
            "\t\t}",
            "",
            "\t\t/* Handle quiescent-state forcing. */",
            "\t\trcu_gp_fqs_loop();",
            "",
            "\t\t/* Handle grace-period end. */",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANUP);",
            "\t\trcu_gp_cleanup();",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANED);",
            "\t}",
            "}",
            "static void rcu_report_qs_rsp(unsigned long flags)",
            "\t__releases(rcu_get_root()->lock)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rcu_get_root());",
            "\tWARN_ON_ONCE(!rcu_gp_in_progress());",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rcu_get_root(), flags);",
            "\trcu_gp_kthread_wake();",
            "}",
            "static void rcu_report_qs_rnp(unsigned long mask, struct rcu_node *rnp,",
            "\t\t\t      unsigned long gps, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long oldmask = 0;",
            "\tstruct rcu_node *rnp_c;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* Walk up the rcu_node hierarchy. */",
            "\tfor (;;) {",
            "\t\tif ((!(rnp->qsmask & mask) && mask) || rnp->gp_seq != gps) {",
            "",
            "\t\t\t/*",
            "\t\t\t * Our bit has already been cleared, or the",
            "\t\t\t * relevant grace period is already over, so done.",
            "\t\t\t */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */",
            "\t\tWARN_ON_ONCE(!rcu_is_leaf_node(rnp) &&",
            "\t\t\t     rcu_preempt_blocked_readers_cgp(rnp));",
            "\t\tWRITE_ONCE(rnp->qsmask, rnp->qsmask & ~mask);",
            "\t\ttrace_rcu_quiescent_state_report(rcu_state.name, rnp->gp_seq,",
            "\t\t\t\t\t\t mask, rnp->qsmask, rnp->level,",
            "\t\t\t\t\t\t rnp->grplo, rnp->grphi,",
            "\t\t\t\t\t\t !!rnp->gp_tasks);",
            "\t\tif (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {",
            "",
            "\t\t\t/* Other bits still set at this level, so done. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\trnp->completedqs = rnp->gp_seq;",
            "\t\tmask = rnp->grpmask;",
            "\t\tif (rnp->parent == NULL) {",
            "",
            "\t\t\t/* No more levels.  Exit loop holding root lock. */",
            "",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\trnp_c = rnp;",
            "\t\trnp = rnp->parent;",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\toldmask = READ_ONCE(rnp_c->qsmask);",
            "\t}",
            "",
            "\t/*",
            "\t * Get here if we are the last CPU to pass through a quiescent",
            "\t * state for this grace period.  Invoke rcu_report_qs_rsp()",
            "\t * to clean up and start the next grace period if one is needed.",
            "\t */",
            "\trcu_report_qs_rsp(flags); /* releases rnp->lock. */",
            "}",
            "static void __maybe_unused",
            "rcu_report_unblock_qs_rnp(struct rcu_node *rnp, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long gps;",
            "\tunsigned long mask;",
            "\tstruct rcu_node *rnp_p;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (WARN_ON_ONCE(!IS_ENABLED(CONFIG_PREEMPT_RCU)) ||",
            "\t    WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)) ||",
            "\t    rnp->qsmask != 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;  /* Still need more quiescent states! */",
            "\t}",
            "",
            "\trnp->completedqs = rnp->gp_seq;",
            "\trnp_p = rnp->parent;",
            "\tif (rnp_p == NULL) {",
            "\t\t/*",
            "\t\t * Only one rcu_node structure in the tree, so don't",
            "\t\t * try to report up to its nonexistent parent!",
            "\t\t */",
            "\t\trcu_report_qs_rsp(flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Report up the rest of the hierarchy, tracking current ->gp_seq. */",
            "\tgps = rnp->gp_seq;",
            "\tmask = rnp->grpmask;",
            "\traw_spin_unlock_rcu_node(rnp);\t/* irqs remain disabled. */",
            "\traw_spin_lock_rcu_node(rnp_p);\t/* irqs already disabled. */",
            "\trcu_report_qs_rnp(mask, rnp_p, gps, flags);",
            "}"
          ],
          "function_name": "rcu_gp_kthread, rcu_report_qs_rsp, rcu_report_qs_rnp, rcu_report_unblock_qs_rnp",
          "description": "实现RCU grace period的主线程循环，处理grace period启动、强制quiescent状态报告及结束逻辑，通过锁和状态标志协调各子系统同步",
          "similarity": 0.5926936864852905
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 908,
          "end_line": 1026,
          "content": [
            "static void trace_rcu_this_gp(struct rcu_node *rnp, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req, const char *s)",
            "{",
            "\ttrace_rcu_future_grace_period(rcu_state.name, READ_ONCE(rnp->gp_seq),",
            "\t\t\t\t      gp_seq_req, rnp->level,",
            "\t\t\t\t      rnp->grplo, rnp->grphi, s);",
            "}",
            "static bool rcu_start_this_gp(struct rcu_node *rnp_start, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req)",
            "{",
            "\tbool ret = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\t/*",
            "\t * Use funnel locking to either acquire the root rcu_node",
            "\t * structure's lock or bail out if the need for this grace period",
            "\t * has already been recorded -- or if that grace period has in",
            "\t * fact already started.  If there is already a grace period in",
            "\t * progress in a non-leaf node, no recording is needed because the",
            "\t * end of the grace period will scan the leaf rcu_node structures.",
            "\t * Note that rnp_start->lock must not be released.",
            "\t */",
            "\traw_lockdep_assert_held_rcu_node(rnp_start);",
            "\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req, TPS(\"Startleaf\"));",
            "\tfor (rnp = rnp_start; 1; rnp = rnp->parent) {",
            "\t\tif (rnp != rnp_start)",
            "\t\t\traw_spin_lock_rcu_node(rnp);",
            "\t\tif (ULONG_CMP_GE(rnp->gp_seq_needed, gp_seq_req) ||",
            "\t\t    rcu_seq_started(&rnp->gp_seq, gp_seq_req) ||",
            "\t\t    (rnp != rnp_start &&",
            "\t\t     rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))) {",
            "\t\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Prestarted\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tWRITE_ONCE(rnp->gp_seq_needed, gp_seq_req);",
            "\t\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq))) {",
            "\t\t\t/*",
            "\t\t\t * We just marked the leaf or internal node, and a",
            "\t\t\t * grace period is in progress, which means that",
            "\t\t\t * rcu_gp_cleanup() will see the marking.  Bail to",
            "\t\t\t * reduce contention.",
            "\t\t\t */",
            "\t\t\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Startedleaf\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tif (rnp != rnp_start && rnp->parent != NULL)",
            "\t\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\tif (!rnp->parent)",
            "\t\t\tbreak;  /* At root, and perhaps also leaf. */",
            "\t}",
            "",
            "\t/* If GP already in progress, just leave, otherwise start one. */",
            "\tif (rcu_gp_in_progress()) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedleafroot\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedroot\"));",
            "\tWRITE_ONCE(rcu_state.gp_flags, rcu_state.gp_flags | RCU_GP_FLAG_INIT);",
            "\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);",
            "\tif (!READ_ONCE(rcu_state.gp_kthread)) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"NoGPkthread\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_grace_period(rcu_state.name, data_race(rcu_state.gp_seq), TPS(\"newreq\"));",
            "\tret = true;  /* Caller must wake GP kthread. */",
            "unlock_out:",
            "\t/* Push furthest requested GP to leaf node and rcu_data structure. */",
            "\tif (ULONG_CMP_LT(gp_seq_req, rnp->gp_seq_needed)) {",
            "\t\tWRITE_ONCE(rnp_start->gp_seq_needed, rnp->gp_seq_needed);",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\t}",
            "\tif (rnp != rnp_start)",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\treturn ret;",
            "}",
            "static bool rcu_future_gp_cleanup(struct rcu_node *rnp)",
            "{",
            "\tbool needmore;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tneedmore = ULONG_CMP_LT(rnp->gp_seq, rnp->gp_seq_needed);",
            "\tif (!needmore)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq; /* Avoid counter wrap. */",
            "\ttrace_rcu_this_gp(rnp, rdp, rnp->gp_seq,",
            "\t\t\t  needmore ? TPS(\"CleanupMore\") : TPS(\"Cleanup\"));",
            "\treturn needmore;",
            "}",
            "static void swake_up_one_online_ipi(void *arg)",
            "{",
            "\tstruct swait_queue_head *wqh = arg;",
            "",
            "\tswake_up_one(wqh);",
            "}",
            "static void swake_up_one_online(struct swait_queue_head *wqh)",
            "{",
            "\tint cpu = get_cpu();",
            "",
            "\t/*",
            "\t * If called from rcutree_report_cpu_starting(), wake up",
            "\t * is dangerous that late in the CPU-down hotplug process. The",
            "\t * scheduler might queue an ignored hrtimer. Defer the wake up",
            "\t * to an online CPU instead.",
            "\t */",
            "\tif (unlikely(cpu_is_offline(cpu))) {",
            "\t\tint target;",
            "",
            "\t\ttarget = cpumask_any_and(housekeeping_cpumask(HK_TYPE_RCU),",
            "\t\t\t\t\t cpu_online_mask);",
            "",
            "\t\tsmp_call_function_single(target, swake_up_one_online_ipi,",
            "\t\t\t\t\t wqh, 0);",
            "\t\tput_cpu();",
            "\t} else {",
            "\t\tput_cpu();",
            "\t\tswake_up_one(wqh);",
            "\t}",
            "}"
          ],
          "function_name": "trace_rcu_this_gp, rcu_start_this_gp, rcu_future_gp_cleanup, swake_up_one_online_ipi, swake_up_one_online",
          "description": "实现RCU grace period事件追踪、新grace period启动逻辑及未来grace period清理机制，支持跨层级节点的同步状态传播。",
          "similarity": 0.5898447632789612
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 658,
          "end_line": 845,
          "content": [
            "int rcu_needs_cpu(void)",
            "{",
            "\treturn !rcu_segcblist_empty(&this_cpu_ptr(&rcu_data)->cblist) &&",
            "\t\t!rcu_rdp_is_offloaded(this_cpu_ptr(&rcu_data));",
            "}",
            "static void rcu_disable_urgency_upon_qs(struct rcu_data *rdp)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rdp->mynode);",
            "\tWRITE_ONCE(rdp->rcu_urgent_qs, false);",
            "\tWRITE_ONCE(rdp->rcu_need_heavy_qs, false);",
            "\tif (tick_nohz_full_cpu(rdp->cpu) && rdp->rcu_forced_tick) {",
            "\t\ttick_dep_clear_cpu(rdp->cpu, TICK_DEP_BIT_RCU);",
            "\t\tWRITE_ONCE(rdp->rcu_forced_tick, false);",
            "\t}",
            "}",
            "notrace bool rcu_is_watching(void)",
            "{",
            "\tbool ret;",
            "",
            "\tpreempt_disable_notrace();",
            "\tret = !rcu_dynticks_curr_cpu_in_eqs();",
            "\tpreempt_enable_notrace();",
            "\treturn ret;",
            "}",
            "void rcu_request_urgent_qs_task(struct task_struct *t)",
            "{",
            "\tint cpu;",
            "",
            "\tbarrier();",
            "\tcpu = task_cpu(t);",
            "\tif (!task_curr(t))",
            "\t\treturn; /* This task is not running on that CPU. */",
            "\tsmp_store_release(per_cpu_ptr(&rcu_data.rcu_urgent_qs, cpu), true);",
            "}",
            "static void rcu_gpnum_ovf(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (ULONG_CMP_LT(rcu_seq_current(&rdp->gp_seq) + ULONG_MAX / 4,",
            "\t\t\t rnp->gp_seq))",
            "\t\tWRITE_ONCE(rdp->gpwrap, true);",
            "\tif (ULONG_CMP_LT(rdp->rcu_iw_gp_seq + ULONG_MAX / 4, rnp->gp_seq))",
            "\t\trdp->rcu_iw_gp_seq = rnp->gp_seq + ULONG_MAX / 4;",
            "}",
            "static int dyntick_save_progress_counter(struct rcu_data *rdp)",
            "{",
            "\trdp->dynticks_snap = rcu_dynticks_snap(rdp->cpu);",
            "\tif (rcu_dynticks_in_eqs(rdp->dynticks_snap)) {",
            "\t\ttrace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS(\"dti\"));",
            "\t\trcu_gpnum_ovf(rdp->mynode, rdp);",
            "\t\treturn 1;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int rcu_implicit_dynticks_qs(struct rcu_data *rdp)",
            "{",
            "\tunsigned long jtsq;",
            "\tint ret = 0;",
            "\tstruct rcu_node *rnp = rdp->mynode;",
            "",
            "\t/*",
            "\t * If the CPU passed through or entered a dynticks idle phase with",
            "\t * no active irq/NMI handlers, then we can safely pretend that the CPU",
            "\t * already acknowledged the request to pass through a quiescent",
            "\t * state.  Either way, that CPU cannot possibly be in an RCU",
            "\t * read-side critical section that started before the beginning",
            "\t * of the current RCU grace period.",
            "\t */",
            "\tif (rcu_dynticks_in_eqs_since(rdp, rdp->dynticks_snap)) {",
            "\t\ttrace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS(\"dti\"));",
            "\t\trcu_gpnum_ovf(rnp, rdp);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\t/*",
            "\t * Complain if a CPU that is considered to be offline from RCU's",
            "\t * perspective has not yet reported a quiescent state.  After all,",
            "\t * the offline CPU should have reported a quiescent state during",
            "\t * the CPU-offline process, or, failing that, by rcu_gp_init()",
            "\t * if it ran concurrently with either the CPU going offline or the",
            "\t * last task on a leaf rcu_node structure exiting its RCU read-side",
            "\t * critical section while all CPUs corresponding to that structure",
            "\t * are offline.  This added warning detects bugs in any of these",
            "\t * code paths.",
            "\t *",
            "\t * The rcu_node structure's ->lock is held here, which excludes",
            "\t * the relevant portions the CPU-hotplug code, the grace-period",
            "\t * initialization code, and the rcu_read_unlock() code paths.",
            "\t *",
            "\t * For more detail, please refer to the \"Hotplug CPU\" section",
            "\t * of RCU's Requirements documentation.",
            "\t */",
            "\tif (WARN_ON_ONCE(!rcu_rdp_cpu_online(rdp))) {",
            "\t\tstruct rcu_node *rnp1;",
            "",
            "\t\tpr_info(\"%s: grp: %d-%d level: %d ->gp_seq %ld ->completedqs %ld\\n\",",
            "\t\t\t__func__, rnp->grplo, rnp->grphi, rnp->level,",
            "\t\t\t(long)rnp->gp_seq, (long)rnp->completedqs);",
            "\t\tfor (rnp1 = rnp; rnp1; rnp1 = rnp1->parent)",
            "\t\t\tpr_info(\"%s: %d:%d ->qsmask %#lx ->qsmaskinit %#lx ->qsmaskinitnext %#lx ->rcu_gp_init_mask %#lx\\n\",",
            "\t\t\t\t__func__, rnp1->grplo, rnp1->grphi, rnp1->qsmask, rnp1->qsmaskinit, rnp1->qsmaskinitnext, rnp1->rcu_gp_init_mask);",
            "\t\tpr_info(\"%s %d: %c online: %ld(%d) offline: %ld(%d)\\n\",",
            "\t\t\t__func__, rdp->cpu, \".o\"[rcu_rdp_cpu_online(rdp)],",
            "\t\t\t(long)rdp->rcu_onl_gp_seq, rdp->rcu_onl_gp_flags,",
            "\t\t\t(long)rdp->rcu_ofl_gp_seq, rdp->rcu_ofl_gp_flags);",
            "\t\treturn 1; /* Break things loose after complaining. */",
            "\t}",
            "",
            "\t/*",
            "\t * A CPU running for an extended time within the kernel can",
            "\t * delay RCU grace periods: (1) At age jiffies_to_sched_qs,",
            "\t * set .rcu_urgent_qs, (2) At age 2*jiffies_to_sched_qs, set",
            "\t * both .rcu_need_heavy_qs and .rcu_urgent_qs.  Note that the",
            "\t * unsynchronized assignments to the per-CPU rcu_need_heavy_qs",
            "\t * variable are safe because the assignments are repeated if this",
            "\t * CPU failed to pass through a quiescent state.  This code",
            "\t * also checks .jiffies_resched in case jiffies_to_sched_qs",
            "\t * is set way high.",
            "\t */",
            "\tjtsq = READ_ONCE(jiffies_to_sched_qs);",
            "\tif (!READ_ONCE(rdp->rcu_need_heavy_qs) &&",
            "\t    (time_after(jiffies, rcu_state.gp_start + jtsq * 2) ||",
            "\t     time_after(jiffies, rcu_state.jiffies_resched) ||",
            "\t     rcu_state.cbovld)) {",
            "\t\tWRITE_ONCE(rdp->rcu_need_heavy_qs, true);",
            "\t\t/* Store rcu_need_heavy_qs before rcu_urgent_qs. */",
            "\t\tsmp_store_release(&rdp->rcu_urgent_qs, true);",
            "\t} else if (time_after(jiffies, rcu_state.gp_start + jtsq)) {",
            "\t\tWRITE_ONCE(rdp->rcu_urgent_qs, true);",
            "\t}",
            "",
            "\t/*",
            "\t * NO_HZ_FULL CPUs can run in-kernel without rcu_sched_clock_irq!",
            "\t * The above code handles this, but only for straight cond_resched().",
            "\t * And some in-kernel loops check need_resched() before calling",
            "\t * cond_resched(), which defeats the above code for CPUs that are",
            "\t * running in-kernel with scheduling-clock interrupts disabled.",
            "\t * So hit them over the head with the resched_cpu() hammer!",
            "\t */",
            "\tif (tick_nohz_full_cpu(rdp->cpu) &&",
            "\t    (time_after(jiffies, READ_ONCE(rdp->last_fqs_resched) + jtsq * 3) ||",
            "\t     rcu_state.cbovld)) {",
            "\t\tWRITE_ONCE(rdp->rcu_urgent_qs, true);",
            "\t\tWRITE_ONCE(rdp->last_fqs_resched, jiffies);",
            "\t\tret = -1;",
            "\t}",
            "",
            "\t/*",
            "\t * If more than halfway to RCU CPU stall-warning time, invoke",
            "\t * resched_cpu() more frequently to try to loosen things up a bit.",
            "\t * Also check to see if the CPU is getting hammered with interrupts,",
            "\t * but only once per grace period, just to keep the IPIs down to",
            "\t * a dull roar.",
            "\t */",
            "\tif (time_after(jiffies, rcu_state.jiffies_resched)) {",
            "\t\tif (time_after(jiffies,",
            "\t\t\t       READ_ONCE(rdp->last_fqs_resched) + jtsq)) {",
            "\t\t\tWRITE_ONCE(rdp->last_fqs_resched, jiffies);",
            "\t\t\tret = -1;",
            "\t\t}",
            "\t\tif (IS_ENABLED(CONFIG_IRQ_WORK) &&",
            "\t\t    !rdp->rcu_iw_pending && rdp->rcu_iw_gp_seq != rnp->gp_seq &&",
            "\t\t    (rnp->ffmask & rdp->grpmask)) {",
            "\t\t\trdp->rcu_iw_pending = true;",
            "\t\t\trdp->rcu_iw_gp_seq = rnp->gp_seq;",
            "\t\t\tirq_work_queue_on(&rdp->rcu_iw, rdp->cpu);",
            "\t\t}",
            "",
            "\t\tif (rcu_cpu_stall_cputime && rdp->snap_record.gp_seq != rdp->gp_seq) {",
            "\t\t\tint cpu = rdp->cpu;",
            "\t\t\tstruct rcu_snap_record *rsrp;",
            "\t\t\tstruct kernel_cpustat *kcsp;",
            "",
            "\t\t\tkcsp = &kcpustat_cpu(cpu);",
            "",
            "\t\t\trsrp = &rdp->snap_record;",
            "\t\t\trsrp->cputime_irq     = kcpustat_field(kcsp, CPUTIME_IRQ, cpu);",
            "\t\t\trsrp->cputime_softirq = kcpustat_field(kcsp, CPUTIME_SOFTIRQ, cpu);",
            "\t\t\trsrp->cputime_system  = kcpustat_field(kcsp, CPUTIME_SYSTEM, cpu);",
            "\t\t\trsrp->nr_hardirqs = kstat_cpu_irqs_sum(cpu) + arch_irq_stat_cpu(cpu);",
            "\t\t\trsrp->nr_softirqs = kstat_cpu_softirqs_sum(cpu);",
            "\t\t\trsrp->nr_csw = nr_context_switches_cpu(cpu);",
            "\t\t\trsrp->jiffies = jiffies;",
            "\t\t\trsrp->gp_seq = rdp->gp_seq;",
            "\t\t}",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "rcu_needs_cpu, rcu_disable_urgency_upon_qs, rcu_is_watching, rcu_request_urgent_qs_task, rcu_gpnum_ovf, dyntick_save_progress_counter, rcu_implicit_dynticks_qs",
          "description": "处理RCU紧迫性需求判定和隐式动态tick quiescent状态检测，通过时间阈值触发CPU唤醒以避免RCU阻塞。",
          "similarity": 0.5811206102371216
        },
        {
          "chunk_id": 20,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3589,
          "end_line": 3697,
          "content": [
            "static unsigned long",
            "kfree_rcu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)",
            "{",
            "\tint cpu;",
            "\tunsigned long count = 0;",
            "",
            "\t/* Snapshot count of all CPUs */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tcount += krc_count(krcp);",
            "\t\tcount += READ_ONCE(krcp->nr_bkv_objs);",
            "\t\tatomic_set(&krcp->backoff_page_cache_fill, 1);",
            "\t}",
            "",
            "\treturn count == 0 ? SHRINK_EMPTY : count;",
            "}",
            "static unsigned long",
            "kfree_rcu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)",
            "{",
            "\tint cpu, freed = 0;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tint count;",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tcount = krc_count(krcp);",
            "\t\tcount += drain_page_cache(krcp);",
            "\t\tkfree_rcu_monitor(&krcp->monitor_work.work);",
            "",
            "\t\tsc->nr_to_scan -= count;",
            "\t\tfreed += count;",
            "",
            "\t\tif (sc->nr_to_scan <= 0)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn freed == 0 ? SHRINK_STOP : freed;",
            "}",
            "void __init kfree_rcu_scheduler_running(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tif (need_offload_krc(krcp))",
            "\t\t\tschedule_delayed_monitor_work(krcp);",
            "\t}",
            "}",
            "static int rcu_blocking_is_gp(void)",
            "{",
            "\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE) {",
            "\t\tmight_sleep();",
            "\t\treturn false;",
            "\t}",
            "\treturn true;",
            "}",
            "void synchronize_rcu(void)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_sched_lock_map),",
            "\t\t\t \"Illegal synchronize_rcu() in RCU read-side critical section\");",
            "\tif (!rcu_blocking_is_gp()) {",
            "\t\tif (rcu_gp_is_expedited())",
            "\t\t\tsynchronize_rcu_expedited();",
            "\t\telse",
            "\t\t\twait_rcu_gp(call_rcu_hurry);",
            "\t\treturn;",
            "\t}",
            "",
            "\t// Context allows vacuous grace periods.",
            "\t// Note well that this code runs with !PREEMPT && !SMP.",
            "\t// In addition, all code that advances grace periods runs at",
            "\t// process level.  Therefore, this normal GP overlaps with other",
            "\t// normal GPs only by being fully nested within them, which allows",
            "\t// reuse of ->gp_seq_polled_snap.",
            "\trcu_poll_gp_seq_start_unlocked(&rcu_state.gp_seq_polled_snap);",
            "\trcu_poll_gp_seq_end_unlocked(&rcu_state.gp_seq_polled_snap);",
            "",
            "\t// Update the normal grace-period counters to record",
            "\t// this grace period, but only those used by the boot CPU.",
            "\t// The rcu_scheduler_starting() will take care of the rest of",
            "\t// these counters.",
            "\tlocal_irq_save(flags);",
            "\tWARN_ON_ONCE(num_online_cpus() > 1);",
            "\trcu_state.gp_seq += (1 << RCU_SEQ_CTR_SHIFT);",
            "\tfor (rnp = this_cpu_ptr(&rcu_data)->mynode; rnp; rnp = rnp->parent)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq = rcu_state.gp_seq;",
            "\tlocal_irq_restore(flags);",
            "}",
            "void get_completed_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\trgosp->rgos_norm = RCU_GET_STATE_COMPLETED;",
            "\trgosp->rgos_exp = RCU_GET_STATE_COMPLETED;",
            "}",
            "unsigned long get_state_synchronize_rcu(void)",
            "{",
            "\t/*",
            "\t * Any prior manipulation of RCU-protected data must happen",
            "\t * before the load from ->gp_seq.",
            "\t */",
            "\tsmp_mb();  /* ^^^ */",
            "\treturn rcu_seq_snap(&rcu_state.gp_seq_polled);",
            "}"
          ],
          "function_name": "kfree_rcu_shrink_count, kfree_rcu_shrink_scan, kfree_rcu_scheduler_running, rcu_blocking_is_gp, synchronize_rcu, get_completed_synchronize_rcu_full, get_state_synchronize_rcu",
          "description": "实现RCU内存回收的shrinker接口，统计并扫描等待回收的RCU对象，调度监控工作，处理同步屏障逻辑，通过锁竞争检测确保安全访问",
          "similarity": 0.5806729793548584
        }
      ]
    },
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.6004803776741028,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/rt.c",
          "start_line": 529,
          "end_line": 633,
          "content": [
            "static void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\tstruct sched_rt_entity *rt_se;",
            "",
            "\tint cpu = cpu_of(rq);",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tif (!rt_se)",
            "\t\t\tenqueue_top_rt_rq(rt_rq);",
            "\t\telse if (!on_rt_rq(rt_se))",
            "\t\t\tenqueue_rt_entity(rt_se, 0);",
            "",
            "\t\tif (rt_rq->highest_prio.curr < curr->prio)",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "}",
            "static void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (!rt_se) {",
            "\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "\t\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);",
            "\t}",
            "\telse if (on_rt_rq(rt_se))",
            "\t\tdequeue_rt_entity(rt_se, 0);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;",
            "}",
            "static int rt_se_boosted(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "\tstruct task_struct *p;",
            "",
            "\tif (rt_rq)",
            "\t\treturn !!rt_rq->rt_nr_boosted;",
            "",
            "\tp = rt_task_of(rt_se);",
            "\treturn p->prio != p->normal_prio;",
            "}",
            "bool sched_rt_bandwidth_account(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\treturn (hrtimer_active(&rt_b->rt_period_timer) ||",
            "\t\trt_rq->rt_time < rt_b->rt_runtime);",
            "}",
            "static void do_balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;",
            "\tint i, weight;",
            "\tu64 rt_period;",
            "",
            "\tweight = cpumask_weight(rd->span);",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\trt_period = ktime_to_ns(rt_b->rt_period);",
            "\tfor_each_cpu(i, rd->span) {",
            "\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\ts64 diff;",
            "",
            "\t\tif (iter == rt_rq)",
            "\t\t\tcontinue;",
            "",
            "\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either all rqs have inf runtime and there's nothing to steal",
            "\t\t * or __disable_runtime() below sets a specific rq to inf to",
            "\t\t * indicate its been disabled and disallow stealing.",
            "\t\t */",
            "\t\tif (iter->rt_runtime == RUNTIME_INF)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * From runqueues with spare time, take 1/n part of their",
            "\t\t * spare time, but no more than our period.",
            "\t\t */",
            "\t\tdiff = iter->rt_runtime - iter->rt_time;",
            "\t\tif (diff > 0) {",
            "\t\t\tdiff = div_u64((u64)diff, weight);",
            "\t\t\tif (rt_rq->rt_runtime + diff > rt_period)",
            "\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;",
            "\t\t\titer->rt_runtime -= diff;",
            "\t\t\trt_rq->rt_runtime += diff;",
            "\t\t\tif (rt_rq->rt_runtime == rt_period) {",
            "\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "next:",
            "\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, rt_se_boosted, sched_rt_bandwidth_account, do_balance_runtime",
          "description": "实现实时任务队列的插入/移除逻辑，跟踪运行时间消耗，通过跨CPU运行时间平衡算法实现带宽公平分配。",
          "similarity": 0.6343889236450195
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/rt.c",
          "start_line": 352,
          "end_line": 453,
          "content": [
            "static inline void rt_clear_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\t/* the order here really doesn't matter */",
            "\tatomic_dec(&rq->rd->rto_count);",
            "\tcpumask_clear_cpu(rq->cpu, rq->rd->rto_mask);",
            "}",
            "static inline int has_pushable_tasks(struct rq *rq)",
            "{",
            "\treturn !plist_head_empty(&rq->rt.pushable_tasks);",
            "}",
            "static inline void rt_queue_push_tasks(struct rq *rq)",
            "{",
            "\tif (!has_pushable_tasks(rq))",
            "\t\treturn;",
            "",
            "\tqueue_balance_callback(rq, &per_cpu(rt_push_head, rq->cpu), push_rt_tasks);",
            "}",
            "static inline void rt_queue_pull_task(struct rq *rq)",
            "{",
            "\tqueue_balance_callback(rq, &per_cpu(rt_pull_head, rq->cpu), pull_rt_task);",
            "}",
            "static void enqueue_pushable_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tplist_del(&p->pushable_tasks, &rq->rt.pushable_tasks);",
            "\tplist_node_init(&p->pushable_tasks, p->prio);",
            "\tplist_add(&p->pushable_tasks, &rq->rt.pushable_tasks);",
            "",
            "\t/* Update the highest prio pushable task */",
            "\tif (p->prio < rq->rt.highest_prio.next)",
            "\t\trq->rt.highest_prio.next = p->prio;",
            "",
            "\tif (!rq->rt.overloaded) {",
            "\t\trt_set_overload(rq);",
            "\t\trq->rt.overloaded = 1;",
            "\t}",
            "}",
            "static void dequeue_pushable_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tplist_del(&p->pushable_tasks, &rq->rt.pushable_tasks);",
            "",
            "\t/* Update the new highest prio pushable task */",
            "\tif (has_pushable_tasks(rq)) {",
            "\t\tp = plist_first_entry(&rq->rt.pushable_tasks,",
            "\t\t\t\t      struct task_struct, pushable_tasks);",
            "\t\trq->rt.highest_prio.next = p->prio;",
            "\t} else {",
            "\t\trq->rt.highest_prio.next = MAX_RT_PRIO-1;",
            "",
            "\t\tif (rq->rt.overloaded) {",
            "\t\t\trt_clear_overload(rq);",
            "\t\t\trq->rt.overloaded = 0;",
            "\t\t}",
            "\t}",
            "}",
            "static inline void enqueue_pushable_task(struct rq *rq, struct task_struct *p)",
            "{",
            "}",
            "static inline void dequeue_pushable_task(struct rq *rq, struct task_struct *p)",
            "{",
            "}",
            "static inline void rt_queue_push_tasks(struct rq *rq)",
            "{",
            "}",
            "static inline int on_rt_rq(struct sched_rt_entity *rt_se)",
            "{",
            "\treturn rt_se->on_rq;",
            "}",
            "static inline bool rt_task_fits_capacity(struct task_struct *p, int cpu)",
            "{",
            "\tunsigned int min_cap;",
            "\tunsigned int max_cap;",
            "\tunsigned int cpu_cap;",
            "",
            "\t/* Only heterogeneous systems can benefit from this check */",
            "\tif (!sched_asym_cpucap_active())",
            "\t\treturn true;",
            "",
            "\tmin_cap = uclamp_eff_value(p, UCLAMP_MIN);",
            "\tmax_cap = uclamp_eff_value(p, UCLAMP_MAX);",
            "",
            "\tcpu_cap = arch_scale_cpu_capacity(cpu);",
            "",
            "\treturn cpu_cap >= min(min_cap, max_cap);",
            "}",
            "static inline bool rt_task_fits_capacity(struct task_struct *p, int cpu)",
            "{",
            "\treturn true;",
            "}",
            "static inline u64 sched_rt_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tif (!rt_rq->tg)",
            "\t\treturn RUNTIME_INF;",
            "",
            "\treturn rt_rq->rt_runtime;",
            "}",
            "static inline u64 sched_rt_period(struct rt_rq *rt_rq)",
            "{",
            "\treturn ktime_to_ns(rt_rq->tg->rt_bandwidth.rt_period);",
            "}"
          ],
          "function_name": "rt_clear_overload, has_pushable_tasks, rt_queue_push_tasks, rt_queue_pull_task, enqueue_pushable_task, dequeue_pushable_task, enqueue_pushable_task, dequeue_pushable_task, rt_queue_push_tasks, on_rt_rq, rt_task_fits_capacity, rt_task_fits_capacity, sched_rt_runtime, sched_rt_period",
          "description": "维护实时任务的可推送队列，检测并处理过载状态，提供任务兼容性检查及运行时间统计查询接口。",
          "similarity": 0.6062706708908081
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1449,
          "end_line": 1589,
          "content": [
            "static void dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rq *rq = rq_of_rt_se(rt_se);",
            "",
            "\tupdate_stats_dequeue_rt(rt_rq_of_se(rt_se), rt_se, flags);",
            "",
            "\tdequeue_rt_stack(rt_se, flags);",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\t\tif (rt_rq && rt_rq->rt_nr_running)",
            "\t\t\t__enqueue_rt_entity(rt_se, flags);",
            "\t}",
            "\tenqueue_top_rt_rq(&rq->rt);",
            "}",
            "static void",
            "enqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tif (flags & ENQUEUE_WAKEUP)",
            "\t\trt_se->timeout = 0;",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_rt(rt_rq_of_se(rt_se), rt_se);",
            "",
            "\tenqueue_rt_entity(rt_se, flags);",
            "",
            "\tif (!task_current(rq, p) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_task(rq, p);",
            "}",
            "static bool dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tdequeue_rt_entity(rt_se, flags);",
            "",
            "\tdequeue_pushable_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void",
            "requeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)",
            "{",
            "\tif (on_rt_rq(rt_se)) {",
            "\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "\t\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);",
            "",
            "\t\tif (head)",
            "\t\t\tlist_move(&rt_se->run_list, queue);",
            "\t\telse",
            "\t\t\tlist_move_tail(&rt_se->run_list, queue);",
            "\t}",
            "}",
            "static void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\trt_rq = rt_rq_of_se(rt_se);",
            "\t\trequeue_rt_entity(rt_rq, rt_se, head);",
            "\t}",
            "}",
            "static void yield_task_rt(struct rq *rq)",
            "{",
            "\trequeue_task_rt(rq, rq->curr, 0);",
            "}",
            "static int",
            "select_task_rq_rt(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tstruct rq *rq;",
            "\tbool test;",
            "",
            "\t/* For anything but wake ups, just return the task_cpu */",
            "\tif (!(flags & (WF_TTWU | WF_FORK)))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If the current task on @p's runqueue is an RT task, then",
            "\t * try to see if we can wake this RT task up on another",
            "\t * runqueue. Otherwise simply start this RT task",
            "\t * on its current runqueue.",
            "\t *",
            "\t * We want to avoid overloading runqueues. If the woken",
            "\t * task is a higher priority, then it will stay on this CPU",
            "\t * and the lower prio task should be moved to another CPU.",
            "\t * Even though this will probably make the lower prio task",
            "\t * lose its cache, we do not want to bounce a higher task",
            "\t * around just because it gave up its CPU, perhaps for a",
            "\t * lock?",
            "\t *",
            "\t * For equal prio tasks, we just let the scheduler sort it out.",
            "\t *",
            "\t * Otherwise, just let it ride on the affined RQ and the",
            "\t * post-schedule router will push the preempted task away",
            "\t *",
            "\t * This test is optimistic, if we get it wrong the load-balancer",
            "\t * will have to sort it out.",
            "\t *",
            "\t * We take into account the capacity of the CPU to ensure it fits the",
            "\t * requirement of the task - which is only important on heterogeneous",
            "\t * systems like big.LITTLE.",
            "\t */",
            "\ttest = curr &&",
            "\t       unlikely(rt_task(curr)) &&",
            "\t       (curr->nr_cpus_allowed < 2 || curr->prio <= p->prio);",
            "",
            "\tif (test || !rt_task_fits_capacity(p, cpu)) {",
            "\t\tint target = find_lowest_rq(p);",
            "",
            "\t\t/*",
            "\t\t * Bail out if we were forcing a migration to find a better",
            "\t\t * fitting CPU but our search failed.",
            "\t\t */",
            "\t\tif (!test && target != -1 && !rt_task_fits_capacity(p, target))",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\t/*",
            "\t\t * Don't bother moving it if the destination CPU is",
            "\t\t * not running a lower priority task.",
            "\t\t */",
            "\t\tif (target != -1 &&",
            "\t\t    p->prio < cpu_rq(target)->rt.highest_prio.curr)",
            "\t\t\tcpu = target;",
            "\t}",
            "",
            "out_unlock:",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "dequeue_rt_entity, enqueue_task_rt, dequeue_task_rt, requeue_rt_entity, requeue_task_rt, yield_task_rt, select_task_rq_rt",
          "description": "实现实时任务的出队逻辑、唤醒和迁移策略，提供CPU亲和性选择及负载均衡支持，维护优先级队列的动态调整。",
          "similarity": 0.6042530536651611
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/sched/rt.c",
          "start_line": 934,
          "end_line": 1036,
          "content": [
            "static inline void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "\tif (!rt_rq->rt_nr_running)",
            "\t\treturn;",
            "",
            "\tenqueue_top_rt_rq(rt_rq);",
            "\tresched_curr(rq);",
            "}",
            "static inline void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn false;",
            "}",
            "static void __enable_runtime(struct rq *rq) { }",
            "static void __disable_runtime(struct rq *rq) { }",
            "static inline int rt_se_prio(struct sched_rt_entity *rt_se)",
            "{",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\tif (rt_rq)",
            "\t\treturn rt_rq->highest_prio.curr;",
            "#endif",
            "",
            "\treturn rt_task_of(rt_se)->prio;",
            "}",
            "static void update_curr_rt(struct rq *rq)",
            "{",
            "\tstruct task_struct *curr = rq->curr;",
            "\ts64 delta_exec;",
            "",
            "\tif (curr->sched_class != &rt_sched_class)",
            "\t\treturn;",
            "",
            "\tdelta_exec = update_curr_common(rq);",
            "\tif (unlikely(delta_exec <= 0))",
            "\t\treturn;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\tstruct sched_rt_entity *rt_se = &curr->rt;",
            "",
            "\tif (!rt_bandwidth_enabled())",
            "\t\treturn;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);",
            "\t\tint exceeded;",
            "",
            "\t\tif (sched_rt_runtime(rt_rq) != RUNTIME_INF) {",
            "\t\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t\trt_rq->rt_time += delta_exec;",
            "\t\t\texceeded = sched_rt_runtime_exceeded(rt_rq);",
            "\t\t\tif (exceeded)",
            "\t\t\t\tresched_curr(rq);",
            "\t\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\t\tif (exceeded)",
            "\t\t\t\tdo_start_rt_bandwidth(sched_rt_bandwidth(rt_rq));",
            "\t\t}",
            "\t}",
            "#endif",
            "}",
            "static void",
            "dequeue_top_rt_rq(struct rt_rq *rt_rq, unsigned int count)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "\tBUG_ON(&rq->rt != rt_rq);",
            "",
            "\tif (!rt_rq->rt_queued)",
            "\t\treturn;",
            "",
            "\tBUG_ON(!rq->nr_running);",
            "",
            "\tsub_nr_running(rq, count);",
            "\trt_rq->rt_queued = 0;",
            "",
            "}",
            "static void",
            "enqueue_top_rt_rq(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "\tBUG_ON(&rq->rt != rt_rq);",
            "",
            "\tif (rt_rq->rt_queued)",
            "\t\treturn;",
            "",
            "\tif (rt_rq_throttled(rt_rq))",
            "\t\treturn;",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tadd_nr_running(rq, rt_rq->rt_nr_running);",
            "\t\trt_rq->rt_queued = 1;",
            "\t}",
            "",
            "\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\tcpufreq_update_util(rq, 0);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, __enable_runtime, __disable_runtime, rt_se_prio, update_curr_rt, dequeue_top_rt_rq, enqueue_top_rt_rq",
          "description": "`sched_rt_rq_enqueue/dequeue`管理实时队列的挂载/卸载，`rt_rq_throttled`判断是否节流，`update_curr_rt`更新当前任务的运行时间并检查节流策略",
          "similarity": 0.5839824676513672
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/rt.c",
          "start_line": 776,
          "end_line": 913,
          "content": [
            "static void balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tif (!sched_feat(RT_RUNTIME_SHARE))",
            "\t\treturn;",
            "",
            "\tif (rt_rq->rt_time > rt_rq->rt_runtime) {",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tdo_balance_runtime(rt_rq);",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t}",
            "}",
            "static inline void balance_runtime(struct rt_rq *rt_rq) {}",
            "static int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun)",
            "{",
            "\tint i, idle = 1, throttled = 0;",
            "\tconst struct cpumask *span;",
            "",
            "\tspan = sched_rt_period_mask();",
            "",
            "\t/*",
            "\t * FIXME: isolated CPUs should really leave the root task group,",
            "\t * whether they are isolcpus or were isolated via cpusets, lest",
            "\t * the timer run on a CPU which does not service all runqueues,",
            "\t * potentially leaving other CPUs indefinitely throttled.  If",
            "\t * isolation is really required, the user will turn the throttle",
            "\t * off to kill the perturbations it causes anyway.  Meanwhile,",
            "\t * this maintains functionality for boot and/or troubleshooting.",
            "\t */",
            "\tif (rt_b == &root_task_group.rt_bandwidth)",
            "\t\tspan = cpu_online_mask;",
            "",
            "\tfor_each_cpu(i, span) {",
            "\t\tint enqueue = 0;",
            "\t\tstruct rt_rq *rt_rq = sched_rt_period_rt_rq(rt_b, i);",
            "\t\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\t\tstruct rq_flags rf;",
            "\t\tint skip;",
            "",
            "\t\t/*",
            "\t\t * When span == cpu_online_mask, taking each rq->lock",
            "\t\t * can be time-consuming. Try to avoid it when possible.",
            "\t\t */",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\tif (!sched_feat(RT_RUNTIME_SHARE) && rt_rq->rt_runtime != RUNTIME_INF)",
            "\t\t\trt_rq->rt_runtime = rt_b->rt_runtime;",
            "\t\tskip = !rt_rq->rt_time && !rt_rq->rt_nr_running;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tif (skip)",
            "\t\t\tcontinue;",
            "",
            "\t\trq_lock(rq, &rf);",
            "\t\tupdate_rq_clock(rq);",
            "",
            "\t\tif (rt_rq->rt_time) {",
            "\t\t\tu64 runtime;",
            "",
            "\t\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t\tif (rt_rq->rt_throttled)",
            "\t\t\t\tbalance_runtime(rt_rq);",
            "\t\t\truntime = rt_rq->rt_runtime;",
            "\t\t\trt_rq->rt_time -= min(rt_rq->rt_time, overrun*runtime);",
            "\t\t\tif (rt_rq->rt_throttled && rt_rq->rt_time < runtime) {",
            "\t\t\t\trt_rq->rt_throttled = 0;",
            "\t\t\t\tenqueue = 1;",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * When we're idle and a woken (rt) task is",
            "\t\t\t\t * throttled wakeup_preempt() will set",
            "\t\t\t\t * skip_update and the time between the wakeup",
            "\t\t\t\t * and this unthrottle will get accounted as",
            "\t\t\t\t * 'runtime'.",
            "\t\t\t\t */",
            "\t\t\t\tif (rt_rq->rt_nr_running && rq->curr == rq->idle)",
            "\t\t\t\t\trq_clock_cancel_skipupdate(rq);",
            "\t\t\t}",
            "\t\t\tif (rt_rq->rt_time || rt_rq->rt_nr_running)",
            "\t\t\t\tidle = 0;",
            "\t\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\t} else if (rt_rq->rt_nr_running) {",
            "\t\t\tidle = 0;",
            "\t\t\tif (!rt_rq_throttled(rt_rq))",
            "\t\t\t\tenqueue = 1;",
            "\t\t}",
            "\t\tif (rt_rq->rt_throttled)",
            "\t\t\tthrottled = 1;",
            "",
            "\t\tif (enqueue)",
            "\t\t\tsched_rt_rq_enqueue(rt_rq);",
            "\t\trq_unlock(rq, &rf);",
            "\t}",
            "",
            "\tif (!throttled && (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF))",
            "\t\treturn 1;",
            "",
            "\treturn idle;",
            "}",
            "static int sched_rt_runtime_exceeded(struct rt_rq *rt_rq)",
            "{",
            "\tu64 runtime = sched_rt_runtime(rt_rq);",
            "",
            "\tif (rt_rq->rt_throttled)",
            "\t\treturn rt_rq_throttled(rt_rq);",
            "",
            "\tif (runtime >= sched_rt_period(rt_rq))",
            "\t\treturn 0;",
            "",
            "\tbalance_runtime(rt_rq);",
            "\truntime = sched_rt_runtime(rt_rq);",
            "\tif (runtime == RUNTIME_INF)",
            "\t\treturn 0;",
            "",
            "\tif (rt_rq->rt_time > runtime) {",
            "\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\t\t/*",
            "\t\t * Don't actually throttle groups that have no runtime assigned",
            "\t\t * but accrue some time due to boosting.",
            "\t\t */",
            "\t\tif (likely(rt_b->rt_runtime)) {",
            "\t\t\trt_rq->rt_throttled = 1;",
            "\t\t\tprintk_deferred_once(\"sched: RT throttling activated\\n\");",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * In case we did anyway, make it go away,",
            "\t\t\t * replenishment is a joke, since it will replenish us",
            "\t\t\t * with exactly 0 ns.",
            "\t\t\t */",
            "\t\t\trt_rq->rt_time = 0;",
            "\t\t}",
            "",
            "\t\tif (rt_rq_throttled(rt_rq)) {",
            "\t\t\tsched_rt_rq_dequeue(rt_rq);",
            "\t\t\treturn 1;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "balance_runtime, balance_runtime, do_sched_rt_period_timer, sched_rt_runtime_exceeded",
          "description": "`balance_runtime`在超时时触发重新平衡，`do_sched_rt_period_timer`周期性调整运行时并检查节流状态，`sched_rt_runtime_exceeded`判断是否超出运行时限制并标记节流",
          "similarity": 0.5827231407165527
        }
      ]
    },
    {
      "source_file": "kernel/sched/clock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:58:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\clock.c`\n\n---\n\n# `sched/clock.c` 技术文档\n\n## 1. 文件概述\n\n`sched/clock.c` 实现了 Linux 内核中用于调度器的高分辨率时间戳机制 `sched_clock()`，特别针对 **不稳定 CPU 时钟源**（如 TSC 在某些硬件上不可靠）的场景。该文件提供了一个在单 CPU 上单调递增、高精度（纳秒级）、可在任意上下文（包括 NMI）中调用的时间源，并通过混合全局时间（GTOD）与本地时钟（如 TSC）来在多核系统中尽量减少时钟漂移。\n\n**重要警告**：不同 CPU 上的 `cpu_clock(i)` 与 `cpu_clock(j)`（i ≠ j）之间 **不保证全局单调性**，时间可能“倒退”。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数 | 说明 |\n|------|------|\n| `sched_clock()` | 弱符号默认实现，基于 jiffies 提供低精度时间戳；架构可覆盖 |\n| `local_clock()` | 宏定义，等价于当前 CPU 的 `cpu_clock(smp_processor_id())` |\n| `cpu_clock(int cpu)` | 返回指定 CPU 的高分辨率时间戳（纳秒） |\n| `sched_clock_stable()` | 判断当前系统是否已进入“稳定时钟”模式（TSC 可靠） |\n| `clear_sched_clock_stable()` | 标记时钟为不稳定（如检测到 TSC 异常），触发修复流程 |\n| `sched_clock_init()` / `sched_clock_init_late()` | 初始化时钟子系统，分早期和晚期阶段 |\n\n### 关键数据结构\n\n```c\nstruct sched_clock_data {\n    u64 tick_raw;   // 上次更新时的原始 sched_clock() 值（如 TSC）\n    u64 tick_gtod;  // 上次更新时的全局时间（ktime_get_ns()）\n    u64 clock;      // 当前推算出的本地高精度单调时间\n};\n```\n\n- 每个 CPU 拥有一个 `sched_clock_data` 实例（`per_cpu` 变量）\n- 全局偏移量：\n  - `__sched_clock_offset`：原始时钟到稳定时间的偏移\n  - `__gtod_offset`：GTOD 到稳定时间的偏移\n\n### 静态键（Static Keys）\n\n- `sched_clock_running`：标记时钟子系统是否已初始化\n- `__sched_clock_stable`：标记时钟源是否稳定（TSC 可靠）\n- `__sched_clock_stable_early`：启动早期假设时钟稳定，避免多次切换\n\n## 3. 关键实现\n\n### 3.1 两种模式\n\n- **稳定模式**（`CONFIG_HAVE_UNSTABLE_SCHED_CLOCK` 未定义）：  \n  直接使用架构提供的 `sched_clock()`，假定其全局同步且高精度（如 ARM64 的 arch counter）。\n\n- **不稳定模式**（`CONFIG_HAVE_UNSTABLE_SCHED_CLOCK` 定义）：  \n  混合 GTOD（`ktime_get_ns()`）与原始 `sched_clock()`（如 TSC）：\n  - 以 GTOD 为基准，利用 `sched_clock()` 的高分辨率 delta 提升精度\n  - 通过 `__sched_clock_offset` 和 `__gtod_offset` 对齐两个时钟源\n\n### 3.2 时钟对齐与漂移控制\n\n- 初始化时通过 `__sched_clock_gtod_offset()` 计算初始偏移量，确保切换时连续\n- `sched_clock_local()` 函数实现核心逻辑：\n  - 计算自上次更新以来的原始时钟增量（`delta = now - tick_raw`）\n  - 将 GTOD 基准时间（`tick_gtod + __gtod_offset`）加上 `delta` 得到新时间\n  - 使用 `wrap_min`/`wrap_max` 处理 64 位回绕，并限制时间跳跃范围（防止 TSC 异常）\n\n### 3.3 稳定性动态切换\n\n- **启动时假设稳定**：`__sched_clock_stable_early = 1`\n- **晚期初始化**（`late_initcall`）：\n  - 若仍认为稳定，则调用 `__set_sched_clock_stable()` 完成对齐并启用稳定模式\n  - 若驱动（如 ACPI/Intel Idle）标记 TSC 不稳定，则调用 `clear_sched_clock_stable()`\n- **不稳定处理**：\n  - 调度工作队列 `sched_clock_work`\n  - 重新以 GTOD 为基准重置所有 CPU 的 `sched_clock_data`\n\n### 3.4 中断与抢占安全\n\n- 关键操作（如 stamp、offset 计算）使用 `local_irq_disable()` 保证原子性\n- `notrace` 属性避免被 ftrace 拦截，确保在 NMI 等上下文中可用\n\n## 4. 依赖关系\n\n- **时间子系统**：\n  - 依赖 `ktime_get_ns()`（GTOD，来自 `kernel/time/`）\n  - 依赖 `jiffies` 和 `HZ`（用于默认 `sched_clock` 实现）\n- **调度器**：为 `kernel/sched/` 提供高精度时间戳（如 `rq->clock` 更新）\n- **时钟事件/源**：与 `tick` 子系统交互（`TICK_DEP_BIT_CLOCK_UNSTABLE`）\n- **架构支持**：\n  - 若架构定义 `CONFIG_HAVE_UNSTABLE_SCHED_CLOCK`，则启用混合模式\n  - 架构可提供自己的 `sched_clock()` 实现（如 x86 使用 TSC）\n\n## 5. 使用场景\n\n- **调度器时间统计**：计算任务运行时间、就绪队列时钟等\n- **延迟跟踪**：`ftrace`、`perf` 等性能工具依赖 `local_clock()` 获取精确时间戳\n- **锁竞争分析**：`lockdep` 使用 `sched_clock()` 记录锁持有时间\n- **RCU、中断处理**：需要高精度、低开销时间戳的内核子系统\n- **虚拟化与电源管理**：在 CPU 进入/退出 idle 时校正时钟（通过 `sched_clock_idle_*` 钩子，虽未在本文件实现但相关）",
      "similarity": 0.5921031832695007,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/clock.c",
          "start_line": 247,
          "end_line": 369,
          "content": [
            "static __always_inline u64 wrap_min(u64 x, u64 y)",
            "{",
            "\treturn (s64)(x - y) < 0 ? x : y;",
            "}",
            "static __always_inline u64 wrap_max(u64 x, u64 y)",
            "{",
            "\treturn (s64)(x - y) > 0 ? x : y;",
            "}",
            "static __always_inline u64 sched_clock_local(struct sched_clock_data *scd)",
            "{",
            "\tu64 now, clock, old_clock, min_clock, max_clock, gtod;",
            "\ts64 delta;",
            "",
            "again:",
            "\tnow = sched_clock_noinstr();",
            "\tdelta = now - scd->tick_raw;",
            "\tif (unlikely(delta < 0))",
            "\t\tdelta = 0;",
            "",
            "\told_clock = scd->clock;",
            "",
            "\t/*",
            "\t * scd->clock = clamp(scd->tick_gtod + delta,",
            "\t *\t\t      max(scd->tick_gtod, scd->clock),",
            "\t *\t\t      scd->tick_gtod + TICK_NSEC);",
            "\t */",
            "",
            "\tgtod = scd->tick_gtod + __gtod_offset;",
            "\tclock = gtod + delta;",
            "\tmin_clock = wrap_max(gtod, old_clock);",
            "\tmax_clock = wrap_max(old_clock, gtod + TICK_NSEC);",
            "",
            "\tclock = wrap_max(clock, min_clock);",
            "\tclock = wrap_min(clock, max_clock);",
            "",
            "\tif (!raw_try_cmpxchg64(&scd->clock, &old_clock, clock))",
            "\t\tgoto again;",
            "",
            "\treturn clock;",
            "}",
            "noinstr u64 local_clock_noinstr(void)",
            "{",
            "\tu64 clock;",
            "",
            "\tif (static_branch_likely(&__sched_clock_stable))",
            "\t\treturn sched_clock_noinstr() + __sched_clock_offset;",
            "",
            "\tif (!static_branch_likely(&sched_clock_running))",
            "\t\treturn sched_clock_noinstr();",
            "",
            "\tclock = sched_clock_local(this_scd());",
            "",
            "\treturn clock;",
            "}",
            "u64 local_clock(void)",
            "{",
            "\tu64 now;",
            "\tpreempt_disable_notrace();",
            "\tnow = local_clock_noinstr();",
            "\tpreempt_enable_notrace();",
            "\treturn now;",
            "}",
            "static notrace u64 sched_clock_remote(struct sched_clock_data *scd)",
            "{",
            "\tstruct sched_clock_data *my_scd = this_scd();",
            "\tu64 this_clock, remote_clock;",
            "\tu64 *ptr, old_val, val;",
            "",
            "#if BITS_PER_LONG != 64",
            "again:",
            "\t/*",
            "\t * Careful here: The local and the remote clock values need to",
            "\t * be read out atomic as we need to compare the values and",
            "\t * then update either the local or the remote side. So the",
            "\t * cmpxchg64 below only protects one readout.",
            "\t *",
            "\t * We must reread via sched_clock_local() in the retry case on",
            "\t * 32-bit kernels as an NMI could use sched_clock_local() via the",
            "\t * tracer and hit between the readout of",
            "\t * the low 32-bit and the high 32-bit portion.",
            "\t */",
            "\tthis_clock = sched_clock_local(my_scd);",
            "\t/*",
            "\t * We must enforce atomic readout on 32-bit, otherwise the",
            "\t * update on the remote CPU can hit inbetween the readout of",
            "\t * the low 32-bit and the high 32-bit portion.",
            "\t */",
            "\tremote_clock = cmpxchg64(&scd->clock, 0, 0);",
            "#else",
            "\t/*",
            "\t * On 64-bit kernels the read of [my]scd->clock is atomic versus the",
            "\t * update, so we can avoid the above 32-bit dance.",
            "\t */",
            "\tsched_clock_local(my_scd);",
            "again:",
            "\tthis_clock = my_scd->clock;",
            "\tremote_clock = scd->clock;",
            "#endif",
            "",
            "\t/*",
            "\t * Use the opportunity that we have both locks",
            "\t * taken to couple the two clocks: we take the",
            "\t * larger time as the latest time for both",
            "\t * runqueues. (this creates monotonic movement)",
            "\t */",
            "\tif (likely((s64)(remote_clock - this_clock) < 0)) {",
            "\t\tptr = &scd->clock;",
            "\t\told_val = remote_clock;",
            "\t\tval = this_clock;",
            "\t} else {",
            "\t\t/*",
            "\t\t * Should be rare, but possible:",
            "\t\t */",
            "\t\tptr = &my_scd->clock;",
            "\t\told_val = this_clock;",
            "\t\tval = remote_clock;",
            "\t}",
            "",
            "\tif (!try_cmpxchg64(ptr, &old_val, val))",
            "\t\tgoto again;",
            "",
            "\treturn val;",
            "}"
          ],
          "function_name": "wrap_min, wrap_max, sched_clock_local, local_clock_noinstr, local_clock, sched_clock_remote",
          "description": "提供时间戳范围约束函数(wrap_min/wrap_max)及本地/远程时钟访问接口(sched_clock_local/local_clock_noinstr)，通过cmpxchg64原子操作保障多CPU间时间戳的一致性，特别处理32位系统下的原子读取问题，确保时间戳单调递增。",
          "similarity": 0.6096615791320801
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/clock.c",
          "start_line": 388,
          "end_line": 477,
          "content": [
            "notrace u64 sched_clock_cpu(int cpu)",
            "{",
            "\tstruct sched_clock_data *scd;",
            "\tu64 clock;",
            "",
            "\tif (sched_clock_stable())",
            "\t\treturn sched_clock() + __sched_clock_offset;",
            "",
            "\tif (!static_branch_likely(&sched_clock_running))",
            "\t\treturn sched_clock();",
            "",
            "\tpreempt_disable_notrace();",
            "\tscd = cpu_sdc(cpu);",
            "",
            "\tif (cpu != smp_processor_id())",
            "\t\tclock = sched_clock_remote(scd);",
            "\telse",
            "\t\tclock = sched_clock_local(scd);",
            "\tpreempt_enable_notrace();",
            "",
            "\treturn clock;",
            "}",
            "notrace void sched_clock_tick(void)",
            "{",
            "\tstruct sched_clock_data *scd;",
            "",
            "\tif (sched_clock_stable())",
            "\t\treturn;",
            "",
            "\tif (!static_branch_likely(&sched_clock_running))",
            "\t\treturn;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tscd = this_scd();",
            "\t__scd_stamp(scd);",
            "\tsched_clock_local(scd);",
            "}",
            "notrace void sched_clock_tick_stable(void)",
            "{",
            "\tif (!sched_clock_stable())",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Called under watchdog_lock.",
            "\t *",
            "\t * The watchdog just found this TSC to (still) be stable, so now is a",
            "\t * good moment to update our __gtod_offset. Because once we find the",
            "\t * TSC to be unstable, any computation will be computing crap.",
            "\t */",
            "\tlocal_irq_disable();",
            "\t__sched_clock_gtod_offset();",
            "\tlocal_irq_enable();",
            "}",
            "notrace void sched_clock_idle_sleep_event(void)",
            "{",
            "\tsched_clock_cpu(smp_processor_id());",
            "}",
            "notrace void sched_clock_idle_wakeup_event(void)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tif (sched_clock_stable())",
            "\t\treturn;",
            "",
            "\tif (unlikely(timekeeping_suspended))",
            "\t\treturn;",
            "",
            "\tlocal_irq_save(flags);",
            "\tsched_clock_tick();",
            "\tlocal_irq_restore(flags);",
            "}",
            "void __init sched_clock_init(void)",
            "{",
            "\tstatic_branch_inc(&sched_clock_running);",
            "\tlocal_irq_disable();",
            "\tgeneric_sched_clock_init();",
            "\tlocal_irq_enable();",
            "}",
            "notrace u64 sched_clock_cpu(int cpu)",
            "{",
            "\tif (!static_branch_likely(&sched_clock_running))",
            "\t\treturn 0;",
            "",
            "\treturn sched_clock();",
            "}",
            "notrace u64 __weak running_clock(void)",
            "{",
            "\treturn local_clock();",
            "}"
          ],
          "function_name": "sched_clock_cpu, sched_clock_tick, sched_clock_tick_stable, sched_clock_idle_sleep_event, sched_clock_idle_wakeup_event, sched_clock_init, sched_clock_cpu, running_clock",
          "description": "实现CPU级时间戳查询(sched_clock_cpu)、时钟滴答事件处理(sched_clock_tick)、空闲状态事件记录(sched_clock_idle_*event)及初始化流程(sched_clock_init)，包含稳定/非稳定时钟模式切换逻辑，通过running_clock弱定义提供默认本地时钟访问接口。",
          "similarity": 0.5870633721351624
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/clock.c",
          "start_line": 62,
          "end_line": 177,
          "content": [
            "notrace unsigned long long __weak sched_clock(void)",
            "{",
            "\treturn (unsigned long long)(jiffies - INITIAL_JIFFIES)",
            "\t\t\t\t\t* (NSEC_PER_SEC / HZ);",
            "}",
            "notrace int sched_clock_stable(void)",
            "{",
            "\treturn static_branch_likely(&__sched_clock_stable);",
            "}",
            "notrace static void __scd_stamp(struct sched_clock_data *scd)",
            "{",
            "\tscd->tick_gtod = ktime_get_ns();",
            "\tscd->tick_raw = sched_clock();",
            "}",
            "notrace static void __set_sched_clock_stable(void)",
            "{",
            "\tstruct sched_clock_data *scd;",
            "",
            "\t/*",
            "\t * Since we're still unstable and the tick is already running, we have",
            "\t * to disable IRQs in order to get a consistent scd->tick* reading.",
            "\t */",
            "\tlocal_irq_disable();",
            "\tscd = this_scd();",
            "\t/*",
            "\t * Attempt to make the (initial) unstable->stable transition continuous.",
            "\t */",
            "\t__sched_clock_offset = (scd->tick_gtod + __gtod_offset) - (scd->tick_raw);",
            "\tlocal_irq_enable();",
            "",
            "\tprintk(KERN_INFO \"sched_clock: Marking stable (%lld, %lld)->(%lld, %lld)\\n\",",
            "\t\t\tscd->tick_gtod, __gtod_offset,",
            "\t\t\tscd->tick_raw,  __sched_clock_offset);",
            "",
            "\tstatic_branch_enable(&__sched_clock_stable);",
            "\ttick_dep_clear(TICK_DEP_BIT_CLOCK_UNSTABLE);",
            "}",
            "notrace static void __sched_clock_work(struct work_struct *work)",
            "{",
            "\tstruct sched_clock_data *scd;",
            "\tint cpu;",
            "",
            "\t/* take a current timestamp and set 'now' */",
            "\tpreempt_disable();",
            "\tscd = this_scd();",
            "\t__scd_stamp(scd);",
            "\tscd->clock = scd->tick_gtod + __gtod_offset;",
            "\tpreempt_enable();",
            "",
            "\t/* clone to all CPUs */",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(sched_clock_data, cpu) = *scd;",
            "",
            "\tprintk(KERN_WARNING \"TSC found unstable after boot, most likely due to broken BIOS. Use 'tsc=unstable'.\\n\");",
            "\tprintk(KERN_INFO \"sched_clock: Marking unstable (%lld, %lld)<-(%lld, %lld)\\n\",",
            "\t\t\tscd->tick_gtod, __gtod_offset,",
            "\t\t\tscd->tick_raw,  __sched_clock_offset);",
            "",
            "\tstatic_branch_disable(&__sched_clock_stable);",
            "}",
            "notrace static void __clear_sched_clock_stable(void)",
            "{",
            "\tif (!sched_clock_stable())",
            "\t\treturn;",
            "",
            "\ttick_dep_set(TICK_DEP_BIT_CLOCK_UNSTABLE);",
            "\tschedule_work(&sched_clock_work);",
            "}",
            "notrace void clear_sched_clock_stable(void)",
            "{",
            "\t__sched_clock_stable_early = 0;",
            "",
            "\tsmp_mb(); /* matches sched_clock_init_late() */",
            "",
            "\tif (static_key_count(&sched_clock_running.key) == 2)",
            "\t\t__clear_sched_clock_stable();",
            "}",
            "notrace static void __sched_clock_gtod_offset(void)",
            "{",
            "\tstruct sched_clock_data *scd = this_scd();",
            "",
            "\t__scd_stamp(scd);",
            "\t__gtod_offset = (scd->tick_raw + __sched_clock_offset) - scd->tick_gtod;",
            "}",
            "void __init sched_clock_init(void)",
            "{",
            "\t/*",
            "\t * Set __gtod_offset such that once we mark sched_clock_running,",
            "\t * sched_clock_tick() continues where sched_clock() left off.",
            "\t *",
            "\t * Even if TSC is buggered, we're still UP at this point so it",
            "\t * can't really be out of sync.",
            "\t */",
            "\tlocal_irq_disable();",
            "\t__sched_clock_gtod_offset();",
            "\tlocal_irq_enable();",
            "",
            "\tstatic_branch_inc(&sched_clock_running);",
            "}",
            "static int __init sched_clock_init_late(void)",
            "{",
            "\tstatic_branch_inc(&sched_clock_running);",
            "\t/*",
            "\t * Ensure that it is impossible to not do a static_key update.",
            "\t *",
            "\t * Either {set,clear}_sched_clock_stable() must see sched_clock_running",
            "\t * and do the update, or we must see their __sched_clock_stable_early",
            "\t * and do the update, or both.",
            "\t */",
            "\tsmp_mb(); /* matches {set,clear}_sched_clock_stable() */",
            "",
            "\tif (__sched_clock_stable_early)",
            "\t\t__set_sched_clock_stable();",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sched_clock, sched_clock_stable, __scd_stamp, __set_sched_clock_stable, __sched_clock_work, __clear_sched_clock_stable, clear_sched_clock_stable, __sched_clock_gtod_offset, sched_clock_init, sched_clock_init_late",
          "description": "实现调度器时钟状态管理，包含sched_clock函数默认实现、稳定性检测、时钟数据同步逻辑(__set_sched_clock_stable/__clear_sched_clock_stable)、初始化流程(sched_clock_init/sched_clock_init_late)及工作队列处理(__sched_clock_work)，用于动态调整时钟偏移与稳定性标志。",
          "similarity": 0.5489128828048706
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/clock.c",
          "start_line": 1,
          "end_line": 61,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * sched_clock() for unstable CPU clocks",
            " *",
            " *  Copyright (C) 2008 Red Hat, Inc., Peter Zijlstra",
            " *",
            " *  Updates and enhancements:",
            " *    Copyright (C) 2008 Red Hat, Inc. Steven Rostedt <srostedt@redhat.com>",
            " *",
            " * Based on code by:",
            " *   Ingo Molnar <mingo@redhat.com>",
            " *   Guillaume Chazarain <guichaz@gmail.com>",
            " *",
            " *",
            " * What this file implements:",
            " *",
            " * cpu_clock(i) provides a fast (execution time) high resolution",
            " * clock with bounded drift between CPUs. The value of cpu_clock(i)",
            " * is monotonic for constant i. The timestamp returned is in nanoseconds.",
            " *",
            " * ######################### BIG FAT WARNING ##########################",
            " * # when comparing cpu_clock(i) to cpu_clock(j) for i != j, time can #",
            " * # go backwards !!                                                  #",
            " * ####################################################################",
            " *",
            " * There is no strict promise about the base, although it tends to start",
            " * at 0 on boot (but people really shouldn't rely on that).",
            " *",
            " * cpu_clock(i)       -- can be used from any context, including NMI.",
            " * local_clock()      -- is cpu_clock() on the current CPU.",
            " *",
            " * sched_clock_cpu(i)",
            " *",
            " * How it is implemented:",
            " *",
            " * The implementation either uses sched_clock() when",
            " * !CONFIG_HAVE_UNSTABLE_SCHED_CLOCK, which means in that case the",
            " * sched_clock() is assumed to provide these properties (mostly it means",
            " * the architecture provides a globally synchronized highres time source).",
            " *",
            " * Otherwise it tries to create a semi stable clock from a mixture of other",
            " * clocks, including:",
            " *",
            " *  - GTOD (clock monotonic)",
            " *  - sched_clock()",
            " *  - explicit idle events",
            " *",
            " * We use GTOD as base and use sched_clock() deltas to improve resolution. The",
            " * deltas are filtered to provide monotonicity and keeping it within an",
            " * expected window.",
            " *",
            " * Furthermore, explicit sleep and wakeup hooks allow us to account for time",
            " * that is otherwise invisible (TSC gets stopped).",
            " *",
            " */",
            "",
            "/*",
            " * Scheduler clock - returns current time in nanosec units.",
            " * This is default implementation.",
            " * Architectures and sub-architectures can override this.",
            " */"
          ],
          "function_name": null,
          "description": "定义sched_clock函数，用于获取当前时间戳，在非稳定时钟配置下依赖架构提供的全局同步高精度时钟，通过混合GTOD、sched_clock及显式空闲事件实现半稳定时钟，支持跨CPU比较但需注意非单调性警告。",
          "similarity": 0.533491849899292
        }
      ]
    }
  ]
}