{
  "query": "Intel VT-x虚拟化下的缓存一致性实现",
  "timestamp": "2025-12-26 01:48:29",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/qspinlock_paravirt.h",
      "md_summary": "> 自动生成时间: 2025-10-25 14:46:42\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\qspinlock_paravirt.h`\n\n---\n\n# `locking/qspinlock_paravirt.h` 技术文档\n\n## 1. 文件概述\n\n`qspinlock_paravirt.h` 是 Linux 内核中用于实现 **半虚拟化（paravirtualized, PV）队列自旋锁（qspinlock）** 的头文件。其核心目标是在虚拟化环境中优化自旋锁行为：当虚拟 CPU（vCPU）无法立即获取锁时，不进行忙等待（busy-waiting），而是通过 **挂起（halt）** 当前 vCPU 并等待被唤醒，从而显著降低在锁竞争激烈或宿主机过载（overcommitted）场景下的 CPU 资源浪费和延迟。\n\n该文件依赖架构层提供的两个关键半虚拟化超调用（hypercall）：\n- `pv_wait(u8 *ptr, u8 val)`：当 `*ptr == val` 时挂起当前 vCPU。\n- `pv_kick(cpu)`：唤醒指定的已挂起 vCPU。\n\n此文件 **不能直接包含**，必须通过定义 `_GEN_PV_LOCK_SLOWPATH` 宏后由其他文件（如 `qspinlock.c`）条件包含，以替换原生的慢路径锁实现。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`enum vcpu_state`**  \n  表示 vCPU 在锁等待队列中的状态：\n  - `vcpu_running`：正在运行（默认状态）。\n  - `vcpu_halted`：已挂起，等待被唤醒（仅用于 `pv_wait_node`）。\n  - `vcpu_hashed`：已挂起且其节点信息已加入哈希表（用于快速查找）。\n\n- **`struct pv_node`**  \n  扩展的 MCS 锁节点，包含：\n  - `mcs`：标准 MCS 自旋锁节点。\n  - `cpu`：关联的 CPU ID。\n  - `state`：当前 vCPU 状态（`vcpu_state` 枚举值）。\n\n- **`struct pv_hash_entry`**  \n  哈希表条目，用于快速映射锁地址到对应的 `pv_node`：\n  - `lock`：指向 `qspinlock` 的指针。\n  - `node`：指向 `pv_node` 的指针。\n\n### 主要函数与宏\n\n- **`pv_hybrid_queued_unfair_trylock()`**  \n  实现混合模式的锁尝试获取逻辑，结合了公平队列锁与非公平锁的优点。\n\n- **`set_pending()` / `trylock_clear_pending()`**  \n  操作锁的 `pending` 位，用于协调队列头 vCPU 与新到来的竞争者。\n\n- **`__pv_init_lock_hash()`**  \n  初始化 PV 锁哈希表，分配足够大的内存空间以支持所有可能的 CPU。\n\n- **`pv_hash()` / `pv_unhash()`**  \n  在哈希表中插入/删除锁与节点的映射关系，用于快速唤醒等待者。\n\n- **`pv_wait_early()`**  \n  （代码不完整）用于判断是否应提前检查前驱节点状态并挂起当前 vCPU。\n\n### 关键宏定义\n\n- **`PV_PREV_CHECK_MASK`**  \n  控制检查前驱节点状态的频率（每 256 次循环检查一次），避免缓存行抖动。\n\n- **`_Q_SLOW_VAL`**  \n  表示锁处于慢路径状态的值（`locked=1, pending=1`）。\n\n- **`queued_spin_trylock`**  \n  重定义为 `pv_hybrid_queued_unfair_trylock`，启用混合锁机制。\n\n## 3. 关键实现\n\n### 混合 PV 队列/非公平锁机制\n\n该实现采用 **混合策略**：\n- 当锁的 MCS 等待队列为空或 `pending` 位未设置时，新竞争者尝试 **非公平方式抢锁**（直接 CAS `locked` 位），提升低竞争场景性能。\n- 一旦有 vCPU 进入等待队列并成为队列头，它会设置 `pending` 位，**禁止后续抢锁**，强制新竞争者进入公平队列，避免锁饥饿。\n- 队列头 vCPU 在自旋等待锁释放时保持 `pending=1`，确保公平性。\n\n### 自适应挂起（Adaptive Spinning）\n\n- 等待队列中的 vCPU 会周期性（由 `PV_PREV_CHECK_MASK` 控制）检查 **前驱节点是否正在运行**。\n- 若前驱 **未运行**（如已挂起），当前 vCPU 也立即挂起，避免无意义的忙等。\n- 此机制在虚拟化过载环境中显著减少 CPU 浪费，同时在非过载场景下通过一次抢锁尝试维持性能。\n\n### 锁-节点哈希表\n\n- 为支持 `pv_kick()` 快速定位等待某锁的 vCPU，内核维护一个全局哈希表 `pv_lock_hash`。\n- 哈希表大小为 `4 * num_possible_cpus()`，确保即使在最大嵌套深度（4 层）下也有足够条目。\n- 使用 **开放寻址法**，每缓存行存放多个条目（`PV_HE_PER_LINE`），减少缓存未命中。\n- 锁持有者在释放锁前必须调用 `pv_unhash()` 移除映射，保证哈希表一致性。\n\n### Pending 位操作优化\n\n- 根据 `_Q_PENDING_BITS` 是否为 8（即 `pending` 字段是否独立字节），提供两种实现：\n  - **独立字节**：直接写 `pending` 字段，使用 `cmpxchg_acquire` 尝试获取锁。\n  - **共享字段**：使用原子位操作（`atomic_or` / `atomic_cmpxchg_acquire`）修改 `val`。\n\n## 4. 依赖关系\n\n- **架构依赖**：必须由底层架构（如 x86 KVM/Xen）提供 `pv_wait()` 和 `pv_kick()` 超调用。\n- **头文件依赖**：\n  - `<linux/hash.h>`：提供 `hash_ptr()` 哈希函数。\n  - `<linux/memblock.h>`：用于早期内存分配（`alloc_large_system_hash`）。\n  - `<linux/debug_locks.h>`：锁调试支持。\n- **锁核心依赖**：基于 `qspinlock` 和 `mcs_spinlock` 实现，需与 `locking/qspinlock.c` 协同工作。\n- **编译依赖**：必须由定义了 `_GEN_PV_LOCK_SLOWPATH` 的源文件包含，不能独立编译。\n\n## 5. 使用场景\n\n- **虚拟化环境**：主要在 KVM、Xen 等半虚拟化 Hypervisor 上启用，优化多 vCPU 虚拟机中的锁竞争。\n- **高竞争锁场景**：当多个 vCPU 频繁争用同一自旋锁时，避免忙等待导致的宿主机 CPU 资源耗尽。\n- **过载宿主机**：在物理 CPU 资源不足时，挂起等待锁的 vCPU 可减少调度开销和上下文切换延迟。\n- **混合工作负载**：通过混合锁机制，在低竞争时保持高性能，高竞争时保证公平性，适用于通用服务器场景。",
      "similarity": 0.5492725968360901,
      "chunks": []
    },
    {
      "source_file": "mm/ksm.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:34:25\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `ksm.c`\n\n---\n\n# ksm.c 技术文档\n\n## 1. 文件概述\n\n`ksm.c` 实现了内核同页合并（Kernel Samepage Merging, KSM）功能，该机制能够动态识别并合并内容完全相同的物理内存页，即使这些页属于不同的进程地址空间且未通过 `fork()` 共享。KSM 通过后台扫描线程周期性地遍历注册的内存区域，利用红黑树（rbtree）结构高效地比对页面内容，将重复页替换为只读的共享页，从而显著减少物理内存占用。此功能特别适用于虚拟化环境中多个相似虚拟机共存的场景。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct ksm_mm_slot`**  \n  表示一个被 KSM 扫描的内存描述符（`mm_struct`）的元数据，包含哈希槽位和反向映射项链表头。\n\n- **`struct ksm_scan`**  \n  全局扫描游标，记录当前扫描进度（包括当前 `mm_slot`、虚拟地址、反向映射项指针及完整扫描轮次计数）。\n\n- **`struct ksm_stable_node`**  \n  稳定红黑树中的节点，代表一个已合并的 KSM 页面。包含：\n  - 红黑树节点或迁移链表指针（联合体复用）\n  - 指向使用该 KSM 页的所有 `rmap_item` 的哈希链表头\n  - 物理页帧号（`kpfn`）或链修剪时间戳\n  - 反向映射项数量（支持链式扩展）\n  - NUMA 节点 ID（若启用 NUMA）\n\n- **`struct ksm_rmap_item`**  \n  反向映射项，跟踪一个虚拟地址到其物理页的映射关系。包含：\n  - 所属 `mm_struct` 和虚拟地址（低比特位用于标志）\n  - 匿名 VMA 指针（稳定状态）或 NUMA 节点 ID（不稳定状态）\n  - 校验和（不稳定状态）\n  - 红黑树节点（不稳定树）或指向 `stable_node` 的指针及哈希链表节点（稳定状态）\n\n### 关键宏定义\n\n- **`STABLE_NODE_CHAIN`**  \n  标识稳定节点为“链”类型（值为 -1024），用于高效管理大量相同内容的 KSM 页副本。\n  \n- **标志位**  \n  - `UNSTABLE_FLAG` (0x100)：标识 `rmap_item` 属于不稳定树\n  - `STABLE_FLAG` (0x200)：标识 `rmap_item` 已链接到稳定树\n  - `SEQNR_MASK` (0x0ff)：用于存储不稳定树序列号的低 8 位\n\n## 3. 关键实现\n\n### 双树架构设计\nKSM 采用**稳定树（stable tree）**与**不稳定树（unstable tree）**协同工作的机制：\n- **稳定树**：存储已确认可合并的只读 KSM 页，因写保护而内容恒定，支持高效精确匹配。\n- **不稳定树**：临时缓存近期未修改的普通页，因内容可能变化而需周期性重建。\n\n### 扫描与合并流程\n1. **增量扫描**：全局游标 `ksm_scan` 遍历所有注册的 `mm_slot` 及其内存区域。\n2. **校验和预筛**：计算页面内容的 `xxhash` 校验和，仅当与上次扫描一致时才尝试插入不稳定树。\n3. **双阶段匹配**：\n   - 优先在**稳定树**中查找完全匹配的 KSM 页\n   - 若未命中，则在**不稳定树**中查找潜在重复页\n4. **树维护策略**：\n   - 不稳定树在每轮全量扫描结束后**完全清空重建**\n   - 稳定树**持久保留**，通过反向映射（rmap）和链式节点优化大规模合并场景\n5. **NUMA 感知**：若 `merge_across_nodes=0`，则为每个 NUMA 节点维护独立的稳定/不稳定树，避免跨节点内存访问开销。\n\n### 内存安全机制\n- **写保护**：合并后的 KSM 页设为只读，任何写操作触发 COW（写时复制）并解除合并。\n- **RMAP 集成**：通过 `anon_vma` 和反向映射链表，在页解绑时高效更新所有相关虚拟地址。\n- **OOM 防护**：在内存压力下可释放 KSM 页以缓解系统压力。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：深度依赖 `mm.h`、`rmap.h`、`pagemap.h` 实现页表操作、反向映射和页生命周期管理。\n- **调度与进程管理**：通过 `sched/mm.h` 获取进程内存上下文，利用 `kthread.h` 创建后台扫描线程。\n- **NUMA 支持**：条件编译依赖 `CONFIG_NUMA`，使用 `numa.h` 实现节点亲和性。\n- **调试与追踪**：集成 `trace/events/ksm.h` 提供运行时事件追踪能力。\n- **哈希算法**：使用 `xxhash.h` 提供高效的内容指纹计算。\n- **内部辅助模块**：依赖 `mm_slot.h` 管理内存描述符槽位，`internal.h` 提供内核 MM 内部接口。\n\n## 5. 使用场景\n\n- **虚拟化环境**：在 KVM/Xen 等 Hypervisor 中合并多个相似虚拟机的内存页（如相同操作系统镜像）。\n- **内存密集型应用**：合并大型应用（如数据库、Web 服务器）中重复的静态数据或零页。\n- **容器化平台**：在 Docker/LXC 等容器运行时中减少同镜像容器的内存占用。\n- **内存超分场景**：在物理内存有限但允许超额分配的系统中提升内存利用率。\n- **开发调试**：通过 `/sys/kernel/mm/ksm/` 接口动态控制扫描速率、合并阈值等参数。",
      "similarity": 0.5450782775878906,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/ksm.c",
          "start_line": 312,
          "end_line": 422,
          "content": [
            "static int __init ksm_slab_init(void)",
            "{",
            "\trmap_item_cache = KSM_KMEM_CACHE(ksm_rmap_item, 0);",
            "\tif (!rmap_item_cache)",
            "\t\tgoto out;",
            "",
            "\tstable_node_cache = KSM_KMEM_CACHE(ksm_stable_node, 0);",
            "\tif (!stable_node_cache)",
            "\t\tgoto out_free1;",
            "",
            "\tmm_slot_cache = KSM_KMEM_CACHE(ksm_mm_slot, 0);",
            "\tif (!mm_slot_cache)",
            "\t\tgoto out_free2;",
            "",
            "\treturn 0;",
            "",
            "out_free2:",
            "\tkmem_cache_destroy(stable_node_cache);",
            "out_free1:",
            "\tkmem_cache_destroy(rmap_item_cache);",
            "out:",
            "\treturn -ENOMEM;",
            "}",
            "static void __init ksm_slab_free(void)",
            "{",
            "\tkmem_cache_destroy(mm_slot_cache);",
            "\tkmem_cache_destroy(stable_node_cache);",
            "\tkmem_cache_destroy(rmap_item_cache);",
            "\tmm_slot_cache = NULL;",
            "}",
            "static __always_inline bool is_stable_node_chain(struct ksm_stable_node *chain)",
            "{",
            "\treturn chain->rmap_hlist_len == STABLE_NODE_CHAIN;",
            "}",
            "static __always_inline bool is_stable_node_dup(struct ksm_stable_node *dup)",
            "{",
            "\treturn dup->head == STABLE_NODE_DUP_HEAD;",
            "}",
            "static inline void stable_node_chain_add_dup(struct ksm_stable_node *dup,",
            "\t\t\t\t\t     struct ksm_stable_node *chain)",
            "{",
            "\tVM_BUG_ON(is_stable_node_dup(dup));",
            "\tdup->head = STABLE_NODE_DUP_HEAD;",
            "\tVM_BUG_ON(!is_stable_node_chain(chain));",
            "\thlist_add_head(&dup->hlist_dup, &chain->hlist);",
            "\tksm_stable_node_dups++;",
            "}",
            "static inline void __stable_node_dup_del(struct ksm_stable_node *dup)",
            "{",
            "\tVM_BUG_ON(!is_stable_node_dup(dup));",
            "\thlist_del(&dup->hlist_dup);",
            "\tksm_stable_node_dups--;",
            "}",
            "static inline void stable_node_dup_del(struct ksm_stable_node *dup)",
            "{",
            "\tVM_BUG_ON(is_stable_node_chain(dup));",
            "\tif (is_stable_node_dup(dup))",
            "\t\t__stable_node_dup_del(dup);",
            "\telse",
            "\t\trb_erase(&dup->node, root_stable_tree + NUMA(dup->nid));",
            "#ifdef CONFIG_DEBUG_VM",
            "\tdup->head = NULL;",
            "#endif",
            "}",
            "static inline void free_rmap_item(struct ksm_rmap_item *rmap_item)",
            "{",
            "\tksm_rmap_items--;",
            "\trmap_item->mm->ksm_rmap_items--;",
            "\trmap_item->mm = NULL;\t/* debug safety */",
            "\tkmem_cache_free(rmap_item_cache, rmap_item);",
            "}",
            "static inline void free_stable_node(struct ksm_stable_node *stable_node)",
            "{",
            "\tVM_BUG_ON(stable_node->rmap_hlist_len &&",
            "\t\t  !is_stable_node_chain(stable_node));",
            "\tkmem_cache_free(stable_node_cache, stable_node);",
            "}",
            "static inline bool ksm_test_exit(struct mm_struct *mm)",
            "{",
            "\treturn atomic_read(&mm->mm_users) == 0;",
            "}",
            "static int break_ksm_pmd_entry(pmd_t *pmd, unsigned long addr, unsigned long next,",
            "\t\t\tstruct mm_walk *walk)",
            "{",
            "\tstruct page *page = NULL;",
            "\tspinlock_t *ptl;",
            "\tpte_t *pte;",
            "\tpte_t ptent;",
            "\tint ret;",
            "",
            "\tpte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);",
            "\tif (!pte)",
            "\t\treturn 0;",
            "\tptent = ptep_get(pte);",
            "\tif (pte_present(ptent)) {",
            "\t\tpage = vm_normal_page(walk->vma, addr, ptent);",
            "\t} else if (!pte_none(ptent)) {",
            "\t\tswp_entry_t entry = pte_to_swp_entry(ptent);",
            "",
            "\t\t/*",
            "\t\t * As KSM pages remain KSM pages until freed, no need to wait",
            "\t\t * here for migration to end.",
            "\t\t */",
            "\t\tif (is_migration_entry(entry))",
            "\t\t\tpage = pfn_swap_entry_to_page(entry);",
            "\t}",
            "\t/* return 1 if the page is an normal ksm page or KSM-placed zero page */",
            "\tret = (page && PageKsm(page)) || is_ksm_zero_pte(ptent);",
            "\tpte_unmap_unlock(pte, ptl);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "ksm_slab_init, ksm_slab_free, is_stable_node_chain, is_stable_node_dup, stable_node_chain_add_dup, __stable_node_dup_del, stable_node_dup_del, free_rmap_item, free_stable_node, ksm_test_exit, break_ksm_pmd_entry",
          "description": "实现KSM模块的 slab 缓存初始化与释放逻辑，包含稳定节点类型判断、反向映射项管理、页面分解操作及稳定树节点销毁等功能。核心作用是通过内存池管理KSM专用对象，提供稳定节点链表管理和页面分裂的底层支持。",
          "similarity": 0.5021432638168335
        },
        {
          "chunk_id": 9,
          "file_path": "mm/ksm.c",
          "start_line": 2608,
          "end_line": 2731,
          "content": [
            "int ksm_enable_merge_any(struct mm_struct *mm)",
            "{",
            "\tint err;",
            "",
            "\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn 0;",
            "",
            "\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {",
            "\t\terr = __ksm_enter(mm);",
            "\t\tif (err)",
            "\t\t\treturn err;",
            "\t}",
            "",
            "\tset_bit(MMF_VM_MERGE_ANY, &mm->flags);",
            "\tksm_add_vmas(mm);",
            "",
            "\treturn 0;",
            "}",
            "int ksm_disable_merge_any(struct mm_struct *mm)",
            "{",
            "\tint err;",
            "",
            "\tif (!test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn 0;",
            "",
            "\terr = ksm_del_vmas(mm);",
            "\tif (err) {",
            "\t\tksm_add_vmas(mm);",
            "\t\treturn err;",
            "\t}",
            "",
            "\tclear_bit(MMF_VM_MERGE_ANY, &mm->flags);",
            "\treturn 0;",
            "}",
            "int ksm_disable(struct mm_struct *mm)",
            "{",
            "\tmmap_assert_write_locked(mm);",
            "",
            "\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags))",
            "\t\treturn 0;",
            "\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn ksm_disable_merge_any(mm);",
            "\treturn ksm_del_vmas(mm);",
            "}",
            "int ksm_madvise(struct vm_area_struct *vma, unsigned long start,",
            "\t\tunsigned long end, int advice, unsigned long *vm_flags)",
            "{",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tint err;",
            "",
            "\tswitch (advice) {",
            "\tcase MADV_MERGEABLE:",
            "\t\tif (vma->vm_flags & VM_MERGEABLE)",
            "\t\t\treturn 0;",
            "\t\tif (!vma_ksm_compatible(vma))",
            "\t\t\treturn 0;",
            "",
            "\t\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {",
            "\t\t\terr = __ksm_enter(mm);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "",
            "\t\t*vm_flags |= VM_MERGEABLE;",
            "\t\tbreak;",
            "",
            "\tcase MADV_UNMERGEABLE:",
            "\t\tif (!(*vm_flags & VM_MERGEABLE))",
            "\t\t\treturn 0;\t\t/* just ignore the advice */",
            "",
            "\t\tif (vma->anon_vma) {",
            "\t\t\terr = unmerge_ksm_pages(vma, start, end, true);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "",
            "\t\t*vm_flags &= ~VM_MERGEABLE;",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __ksm_enter(struct mm_struct *mm)",
            "{",
            "\tstruct ksm_mm_slot *mm_slot;",
            "\tstruct mm_slot *slot;",
            "\tint needs_wakeup;",
            "",
            "\tmm_slot = mm_slot_alloc(mm_slot_cache);",
            "\tif (!mm_slot)",
            "\t\treturn -ENOMEM;",
            "",
            "\tslot = &mm_slot->slot;",
            "",
            "\t/* Check ksm_run too?  Would need tighter locking */",
            "\tneeds_wakeup = list_empty(&ksm_mm_head.slot.mm_node);",
            "",
            "\tspin_lock(&ksm_mmlist_lock);",
            "\tmm_slot_insert(mm_slots_hash, mm, slot);",
            "\t/*",
            "\t * When KSM_RUN_MERGE (or KSM_RUN_STOP),",
            "\t * insert just behind the scanning cursor, to let the area settle",
            "\t * down a little; when fork is followed by immediate exec, we don't",
            "\t * want ksmd to waste time setting up and tearing down an rmap_list.",
            "\t *",
            "\t * But when KSM_RUN_UNMERGE, it's important to insert ahead of its",
            "\t * scanning cursor, otherwise KSM pages in newly forked mms will be",
            "\t * missed: then we might as well insert at the end of the list.",
            "\t */",
            "\tif (ksm_run & KSM_RUN_UNMERGE)",
            "\t\tlist_add_tail(&slot->mm_node, &ksm_mm_head.slot.mm_node);",
            "\telse",
            "\t\tlist_add_tail(&slot->mm_node, &ksm_scan.mm_slot->slot.mm_node);",
            "\tspin_unlock(&ksm_mmlist_lock);",
            "",
            "\tset_bit(MMF_VM_MERGEABLE, &mm->flags);",
            "\tmmgrab(mm);",
            "",
            "\tif (needs_wakeup)",
            "\t\twake_up_interruptible(&ksm_thread_wait);",
            "",
            "\ttrace_ksm_enter(mm);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ksm_enable_merge_any, ksm_disable_merge_any, ksm_disable, ksm_madvise, __ksm_enter",
          "description": "ksm_enable_merge_any 启用全局合并标志并注册内存区域。ksm_disable_merge_any 禁用合并并清理配置。ksm_disable 移除所有KSM配置。ksm_madvise 处理MADV_MERGEABLE/MADV_UNMERGEABLE建议，调整页面属性。__ksm_enter 注册内存区域到KSM管理系统。",
          "similarity": 0.47935348749160767
        },
        {
          "chunk_id": 0,
          "file_path": "mm/ksm.c",
          "start_line": 1,
          "end_line": 311,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Memory merging support.",
            " *",
            " * This code enables dynamic sharing of identical pages found in different",
            " * memory areas, even if they are not shared by fork()",
            " *",
            " * Copyright (C) 2008-2009 Red Hat, Inc.",
            " * Authors:",
            " *\tIzik Eidus",
            " *\tAndrea Arcangeli",
            " *\tChris Wright",
            " *\tHugh Dickins",
            " */",
            "",
            "#include <linux/errno.h>",
            "#include <linux/mm.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/fs.h>",
            "#include <linux/mman.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/rmap.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/xxhash.h>",
            "#include <linux/delay.h>",
            "#include <linux/kthread.h>",
            "#include <linux/wait.h>",
            "#include <linux/slab.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/memory.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/swap.h>",
            "#include <linux/ksm.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/freezer.h>",
            "#include <linux/oom.h>",
            "#include <linux/numa.h>",
            "#include <linux/pagewalk.h>",
            "",
            "#include <asm/tlbflush.h>",
            "#include \"internal.h\"",
            "#include \"mm_slot.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/ksm.h>",
            "",
            "#ifdef CONFIG_NUMA",
            "#define NUMA(x)\t\t(x)",
            "#define DO_NUMA(x)\tdo { (x); } while (0)",
            "#else",
            "#define NUMA(x)\t\t(0)",
            "#define DO_NUMA(x)\tdo { } while (0)",
            "#endif",
            "",
            "/**",
            " * DOC: Overview",
            " *",
            " * A few notes about the KSM scanning process,",
            " * to make it easier to understand the data structures below:",
            " *",
            " * In order to reduce excessive scanning, KSM sorts the memory pages by their",
            " * contents into a data structure that holds pointers to the pages' locations.",
            " *",
            " * Since the contents of the pages may change at any moment, KSM cannot just",
            " * insert the pages into a normal sorted tree and expect it to find anything.",
            " * Therefore KSM uses two data structures - the stable and the unstable tree.",
            " *",
            " * The stable tree holds pointers to all the merged pages (ksm pages), sorted",
            " * by their contents.  Because each such page is write-protected, searching on",
            " * this tree is fully assured to be working (except when pages are unmapped),",
            " * and therefore this tree is called the stable tree.",
            " *",
            " * The stable tree node includes information required for reverse",
            " * mapping from a KSM page to virtual addresses that map this page.",
            " *",
            " * In order to avoid large latencies of the rmap walks on KSM pages,",
            " * KSM maintains two types of nodes in the stable tree:",
            " *",
            " * * the regular nodes that keep the reverse mapping structures in a",
            " *   linked list",
            " * * the \"chains\" that link nodes (\"dups\") that represent the same",
            " *   write protected memory content, but each \"dup\" corresponds to a",
            " *   different KSM page copy of that content",
            " *",
            " * Internally, the regular nodes, \"dups\" and \"chains\" are represented",
            " * using the same struct ksm_stable_node structure.",
            " *",
            " * In addition to the stable tree, KSM uses a second data structure called the",
            " * unstable tree: this tree holds pointers to pages which have been found to",
            " * be \"unchanged for a period of time\".  The unstable tree sorts these pages",
            " * by their contents, but since they are not write-protected, KSM cannot rely",
            " * upon the unstable tree to work correctly - the unstable tree is liable to",
            " * be corrupted as its contents are modified, and so it is called unstable.",
            " *",
            " * KSM solves this problem by several techniques:",
            " *",
            " * 1) The unstable tree is flushed every time KSM completes scanning all",
            " *    memory areas, and then the tree is rebuilt again from the beginning.",
            " * 2) KSM will only insert into the unstable tree, pages whose hash value",
            " *    has not changed since the previous scan of all memory areas.",
            " * 3) The unstable tree is a RedBlack Tree - so its balancing is based on the",
            " *    colors of the nodes and not on their contents, assuring that even when",
            " *    the tree gets \"corrupted\" it won't get out of balance, so scanning time",
            " *    remains the same (also, searching and inserting nodes in an rbtree uses",
            " *    the same algorithm, so we have no overhead when we flush and rebuild).",
            " * 4) KSM never flushes the stable tree, which means that even if it were to",
            " *    take 10 attempts to find a page in the unstable tree, once it is found,",
            " *    it is secured in the stable tree.  (When we scan a new page, we first",
            " *    compare it against the stable tree, and then against the unstable tree.)",
            " *",
            " * If the merge_across_nodes tunable is unset, then KSM maintains multiple",
            " * stable trees and multiple unstable trees: one of each for each NUMA node.",
            " */",
            "",
            "/**",
            " * struct ksm_mm_slot - ksm information per mm that is being scanned",
            " * @slot: hash lookup from mm to mm_slot",
            " * @rmap_list: head for this mm_slot's singly-linked list of rmap_items",
            " */",
            "struct ksm_mm_slot {",
            "\tstruct mm_slot slot;",
            "\tstruct ksm_rmap_item *rmap_list;",
            "};",
            "",
            "/**",
            " * struct ksm_scan - cursor for scanning",
            " * @mm_slot: the current mm_slot we are scanning",
            " * @address: the next address inside that to be scanned",
            " * @rmap_list: link to the next rmap to be scanned in the rmap_list",
            " * @seqnr: count of completed full scans (needed when removing unstable node)",
            " *",
            " * There is only the one ksm_scan instance of this cursor structure.",
            " */",
            "struct ksm_scan {",
            "\tstruct ksm_mm_slot *mm_slot;",
            "\tunsigned long address;",
            "\tstruct ksm_rmap_item **rmap_list;",
            "\tunsigned long seqnr;",
            "};",
            "",
            "/**",
            " * struct ksm_stable_node - node of the stable rbtree",
            " * @node: rb node of this ksm page in the stable tree",
            " * @head: (overlaying parent) &migrate_nodes indicates temporarily on that list",
            " * @hlist_dup: linked into the stable_node->hlist with a stable_node chain",
            " * @list: linked into migrate_nodes, pending placement in the proper node tree",
            " * @hlist: hlist head of rmap_items using this ksm page",
            " * @kpfn: page frame number of this ksm page (perhaps temporarily on wrong nid)",
            " * @chain_prune_time: time of the last full garbage collection",
            " * @rmap_hlist_len: number of rmap_item entries in hlist or STABLE_NODE_CHAIN",
            " * @nid: NUMA node id of stable tree in which linked (may not match kpfn)",
            " */",
            "struct ksm_stable_node {",
            "\tunion {",
            "\t\tstruct rb_node node;\t/* when node of stable tree */",
            "\t\tstruct {\t\t/* when listed for migration */",
            "\t\t\tstruct list_head *head;",
            "\t\t\tstruct {",
            "\t\t\t\tstruct hlist_node hlist_dup;",
            "\t\t\t\tstruct list_head list;",
            "\t\t\t};",
            "\t\t};",
            "\t};",
            "\tstruct hlist_head hlist;",
            "\tunion {",
            "\t\tunsigned long kpfn;",
            "\t\tunsigned long chain_prune_time;",
            "\t};",
            "\t/*",
            "\t * STABLE_NODE_CHAIN can be any negative number in",
            "\t * rmap_hlist_len negative range, but better not -1 to be able",
            "\t * to reliably detect underflows.",
            "\t */",
            "#define STABLE_NODE_CHAIN -1024",
            "\tint rmap_hlist_len;",
            "#ifdef CONFIG_NUMA",
            "\tint nid;",
            "#endif",
            "};",
            "",
            "/**",
            " * struct ksm_rmap_item - reverse mapping item for virtual addresses",
            " * @rmap_list: next rmap_item in mm_slot's singly-linked rmap_list",
            " * @anon_vma: pointer to anon_vma for this mm,address, when in stable tree",
            " * @nid: NUMA node id of unstable tree in which linked (may not match page)",
            " * @mm: the memory structure this rmap_item is pointing into",
            " * @address: the virtual address this rmap_item tracks (+ flags in low bits)",
            " * @oldchecksum: previous checksum of the page at that virtual address",
            " * @node: rb node of this rmap_item in the unstable tree",
            " * @head: pointer to stable_node heading this list in the stable tree",
            " * @hlist: link into hlist of rmap_items hanging off that stable_node",
            " */",
            "struct ksm_rmap_item {",
            "\tstruct ksm_rmap_item *rmap_list;",
            "\tunion {",
            "\t\tstruct anon_vma *anon_vma;\t/* when stable */",
            "#ifdef CONFIG_NUMA",
            "\t\tint nid;\t\t/* when node of unstable tree */",
            "#endif",
            "\t};",
            "\tstruct mm_struct *mm;",
            "\tunsigned long address;\t\t/* + low bits used for flags below */",
            "\tunsigned int oldchecksum;\t/* when unstable */",
            "\tunion {",
            "\t\tstruct rb_node node;\t/* when node of unstable tree */",
            "\t\tstruct {\t\t/* when listed from stable tree */",
            "\t\t\tstruct ksm_stable_node *head;",
            "\t\t\tstruct hlist_node hlist;",
            "\t\t};",
            "\t};",
            "};",
            "",
            "#define SEQNR_MASK\t0x0ff\t/* low bits of unstable tree seqnr */",
            "#define UNSTABLE_FLAG\t0x100\t/* is a node of the unstable tree */",
            "#define STABLE_FLAG\t0x200\t/* is listed from the stable tree */",
            "",
            "/* The stable and unstable tree heads */",
            "static struct rb_root one_stable_tree[1] = { RB_ROOT };",
            "static struct rb_root one_unstable_tree[1] = { RB_ROOT };",
            "static struct rb_root *root_stable_tree = one_stable_tree;",
            "static struct rb_root *root_unstable_tree = one_unstable_tree;",
            "",
            "/* Recently migrated nodes of stable tree, pending proper placement */",
            "static LIST_HEAD(migrate_nodes);",
            "#define STABLE_NODE_DUP_HEAD ((struct list_head *)&migrate_nodes.prev)",
            "",
            "#define MM_SLOTS_HASH_BITS 10",
            "static DEFINE_HASHTABLE(mm_slots_hash, MM_SLOTS_HASH_BITS);",
            "",
            "static struct ksm_mm_slot ksm_mm_head = {",
            "\t.slot.mm_node = LIST_HEAD_INIT(ksm_mm_head.slot.mm_node),",
            "};",
            "static struct ksm_scan ksm_scan = {",
            "\t.mm_slot = &ksm_mm_head,",
            "};",
            "",
            "static struct kmem_cache *rmap_item_cache;",
            "static struct kmem_cache *stable_node_cache;",
            "static struct kmem_cache *mm_slot_cache;",
            "",
            "/* The number of pages scanned */",
            "static unsigned long ksm_pages_scanned;",
            "",
            "/* The number of nodes in the stable tree */",
            "static unsigned long ksm_pages_shared;",
            "",
            "/* The number of page slots additionally sharing those nodes */",
            "static unsigned long ksm_pages_sharing;",
            "",
            "/* The number of nodes in the unstable tree */",
            "static unsigned long ksm_pages_unshared;",
            "",
            "/* The number of rmap_items in use: to calculate pages_volatile */",
            "static unsigned long ksm_rmap_items;",
            "",
            "/* The number of stable_node chains */",
            "static unsigned long ksm_stable_node_chains;",
            "",
            "/* The number of stable_node dups linked to the stable_node chains */",
            "static unsigned long ksm_stable_node_dups;",
            "",
            "/* Delay in pruning stale stable_node_dups in the stable_node_chains */",
            "static unsigned int ksm_stable_node_chains_prune_millisecs = 2000;",
            "",
            "/* Maximum number of page slots sharing a stable node */",
            "static int ksm_max_page_sharing = 256;",
            "",
            "/* Number of pages ksmd should scan in one batch */",
            "static unsigned int ksm_thread_pages_to_scan = 100;",
            "",
            "/* Milliseconds ksmd should sleep between batches */",
            "static unsigned int ksm_thread_sleep_millisecs = 20;",
            "",
            "/* Checksum of an empty (zeroed) page */",
            "static unsigned int zero_checksum __read_mostly;",
            "",
            "/* Whether to merge empty (zeroed) pages with actual zero pages */",
            "static bool ksm_use_zero_pages __read_mostly;",
            "",
            "/* The number of zero pages which is placed by KSM */",
            "atomic_long_t ksm_zero_pages = ATOMIC_LONG_INIT(0);",
            "",
            "#ifdef CONFIG_NUMA",
            "/* Zeroed when merging across nodes is not allowed */",
            "static unsigned int ksm_merge_across_nodes = 1;",
            "static int ksm_nr_node_ids = 1;",
            "#else",
            "#define ksm_merge_across_nodes\t1U",
            "#define ksm_nr_node_ids\t\t1",
            "#endif",
            "",
            "#define KSM_RUN_STOP\t0",
            "#define KSM_RUN_MERGE\t1",
            "#define KSM_RUN_UNMERGE\t2",
            "#define KSM_RUN_OFFLINE\t4",
            "static unsigned long ksm_run = KSM_RUN_STOP;",
            "static void wait_while_offlining(void);",
            "",
            "static DECLARE_WAIT_QUEUE_HEAD(ksm_thread_wait);",
            "static DECLARE_WAIT_QUEUE_HEAD(ksm_iter_wait);",
            "static DEFINE_MUTEX(ksm_thread_mutex);",
            "static DEFINE_SPINLOCK(ksm_mmlist_lock);",
            "",
            "#define KSM_KMEM_CACHE(__struct, __flags) kmem_cache_create(#__struct,\\",
            "\t\tsizeof(struct __struct), __alignof__(struct __struct),\\",
            "\t\t(__flags), NULL)",
            ""
          ],
          "function_name": null,
          "description": "定义了KSM（内核同一内存页合并）模块的核心数据结构和全局变量，包括稳定树和不稳定树的数据结构、NUMA支持相关定义、哈希表和红黑树操作接口，以及用于跟踪扫描进度的kscan结构。核心功能是建立KSM内存合并算法的基础框架。",
          "similarity": 0.4687630534172058
        },
        {
          "chunk_id": 12,
          "file_path": "mm/ksm.c",
          "start_line": 3034,
          "end_line": 3143,
          "content": [
            "static void ksm_check_stable_tree(unsigned long start_pfn,",
            "\t\t\t\t  unsigned long end_pfn)",
            "{",
            "\tstruct ksm_stable_node *stable_node, *next;",
            "\tstruct rb_node *node;",
            "\tint nid;",
            "",
            "\tfor (nid = 0; nid < ksm_nr_node_ids; nid++) {",
            "\t\tnode = rb_first(root_stable_tree + nid);",
            "\t\twhile (node) {",
            "\t\t\tstable_node = rb_entry(node, struct ksm_stable_node, node);",
            "\t\t\tif (stable_node_chain_remove_range(stable_node,",
            "\t\t\t\t\t\t\t   start_pfn, end_pfn,",
            "\t\t\t\t\t\t\t   root_stable_tree +",
            "\t\t\t\t\t\t\t   nid))",
            "\t\t\t\tnode = rb_first(root_stable_tree + nid);",
            "\t\t\telse",
            "\t\t\t\tnode = rb_next(node);",
            "\t\t\tcond_resched();",
            "\t\t}",
            "\t}",
            "\tlist_for_each_entry_safe(stable_node, next, &migrate_nodes, list) {",
            "\t\tif (stable_node->kpfn >= start_pfn &&",
            "\t\t    stable_node->kpfn < end_pfn)",
            "\t\t\tremove_node_from_stable_tree(stable_node);",
            "\t\tcond_resched();",
            "\t}",
            "}",
            "static int ksm_memory_callback(struct notifier_block *self,",
            "\t\t\t       unsigned long action, void *arg)",
            "{",
            "\tstruct memory_notify *mn = arg;",
            "",
            "\tswitch (action) {",
            "\tcase MEM_GOING_OFFLINE:",
            "\t\t/*",
            "\t\t * Prevent ksm_do_scan(), unmerge_and_remove_all_rmap_items()",
            "\t\t * and remove_all_stable_nodes() while memory is going offline:",
            "\t\t * it is unsafe for them to touch the stable tree at this time.",
            "\t\t * But unmerge_ksm_pages(), rmap lookups and other entry points",
            "\t\t * which do not need the ksm_thread_mutex are all safe.",
            "\t\t */",
            "\t\tmutex_lock(&ksm_thread_mutex);",
            "\t\tksm_run |= KSM_RUN_OFFLINE;",
            "\t\tmutex_unlock(&ksm_thread_mutex);",
            "\t\tbreak;",
            "",
            "\tcase MEM_OFFLINE:",
            "\t\t/*",
            "\t\t * Most of the work is done by page migration; but there might",
            "\t\t * be a few stable_nodes left over, still pointing to struct",
            "\t\t * pages which have been offlined: prune those from the tree,",
            "\t\t * otherwise get_ksm_page() might later try to access a",
            "\t\t * non-existent struct page.",
            "\t\t */",
            "\t\tksm_check_stable_tree(mn->start_pfn,",
            "\t\t\t\t      mn->start_pfn + mn->nr_pages);",
            "\t\tfallthrough;",
            "\tcase MEM_CANCEL_OFFLINE:",
            "\t\tmutex_lock(&ksm_thread_mutex);",
            "\t\tksm_run &= ~KSM_RUN_OFFLINE;",
            "\t\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\t\tsmp_mb();\t/* wake_up_bit advises this */",
            "\t\twake_up_bit(&ksm_run, ilog2(KSM_RUN_OFFLINE));",
            "\t\tbreak;",
            "\t}",
            "\treturn NOTIFY_OK;",
            "}",
            "static void wait_while_offlining(void)",
            "{",
            "}",
            "bool ksm_process_mergeable(struct mm_struct *mm)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "",
            "\tmmap_assert_locked(mm);",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "\tfor_each_vma(vmi, vma)",
            "\t\tif (vma->vm_flags & VM_MERGEABLE)",
            "\t\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "long ksm_process_profit(struct mm_struct *mm)",
            "{",
            "\treturn (long)(mm->ksm_merging_pages + mm_ksm_zero_pages(mm)) * PAGE_SIZE -",
            "\t\tmm->ksm_rmap_items * sizeof(struct ksm_rmap_item);",
            "}",
            "static ssize_t sleep_millisecs_show(struct kobject *kobj,",
            "\t\t\t\t    struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_thread_sleep_millisecs);",
            "}",
            "static ssize_t sleep_millisecs_store(struct kobject *kobj,",
            "\t\t\t\t     struct kobj_attribute *attr,",
            "\t\t\t\t     const char *buf, size_t count)",
            "{",
            "\tunsigned int msecs;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &msecs);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\tksm_thread_sleep_millisecs = msecs;",
            "\twake_up_interruptible(&ksm_iter_wait);",
            "",
            "\treturn count;",
            "}"
          ],
          "function_name": "ksm_check_stable_tree, ksm_memory_callback, wait_while_offlining, ksm_process_mergeable, ksm_process_profit, sleep_millisecs_show, sleep_millisecs_store",
          "description": "ksm_check_stable_tree扫描稳定树移除指定PFN范围节点；ksm_memory_callback处理内存离线/取消离线事件，同步KSMAPI状态；ksm_process_mergeable检测进程是否含可合并VMA；ksm_process_profit计算KSMAPI收益；sleep_millisecs_*控制KSMAPI线程休眠时间",
          "similarity": 0.4684508144855499
        },
        {
          "chunk_id": 15,
          "file_path": "mm/ksm.c",
          "start_line": 3427,
          "end_line": 3508,
          "content": [
            "static ssize_t stable_node_dups_show(struct kobject *kobj,",
            "\t\t\t\t     struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_stable_node_dups);",
            "}",
            "static ssize_t stable_node_chains_show(struct kobject *kobj,",
            "\t\t\t\t       struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_stable_node_chains);",
            "}",
            "static ssize_t",
            "stable_node_chains_prune_millisecs_show(struct kobject *kobj,",
            "\t\t\t\t\tstruct kobj_attribute *attr,",
            "\t\t\t\t\tchar *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_stable_node_chains_prune_millisecs);",
            "}",
            "static ssize_t",
            "stable_node_chains_prune_millisecs_store(struct kobject *kobj,",
            "\t\t\t\t\t struct kobj_attribute *attr,",
            "\t\t\t\t\t const char *buf, size_t count)",
            "{",
            "\tunsigned int msecs;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &msecs);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\tksm_stable_node_chains_prune_millisecs = msecs;",
            "",
            "\treturn count;",
            "}",
            "static ssize_t full_scans_show(struct kobject *kobj,",
            "\t\t\t       struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_scan.seqnr);",
            "}",
            "static int __init ksm_init(void)",
            "{",
            "\tstruct task_struct *ksm_thread;",
            "\tint err;",
            "",
            "\t/* The correct value depends on page size and endianness */",
            "\tzero_checksum = calc_checksum(ZERO_PAGE(0));",
            "\t/* Default to false for backwards compatibility */",
            "\tksm_use_zero_pages = false;",
            "",
            "\terr = ksm_slab_init();",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\tksm_thread = kthread_run(ksm_scan_thread, NULL, \"ksmd\");",
            "\tif (IS_ERR(ksm_thread)) {",
            "\t\tpr_err(\"ksm: creating kthread failed\\n\");",
            "\t\terr = PTR_ERR(ksm_thread);",
            "\t\tgoto out_free;",
            "\t}",
            "",
            "#ifdef CONFIG_SYSFS",
            "\terr = sysfs_create_group(mm_kobj, &ksm_attr_group);",
            "\tif (err) {",
            "\t\tpr_err(\"ksm: register sysfs failed\\n\");",
            "\t\tkthread_stop(ksm_thread);",
            "\t\tgoto out_free;",
            "\t}",
            "#else",
            "\tksm_run = KSM_RUN_MERGE;\t/* no way for user to start it */",
            "",
            "#endif /* CONFIG_SYSFS */",
            "",
            "#ifdef CONFIG_MEMORY_HOTREMOVE",
            "\t/* There is no significance to this priority 100 */",
            "\thotplug_memory_notifier(ksm_memory_callback, KSM_CALLBACK_PRI);",
            "#endif",
            "\treturn 0;",
            "",
            "out_free:",
            "\tksm_slab_free();",
            "out:",
            "\treturn err;",
            "}"
          ],
          "function_name": "stable_node_dups_show, stable_node_chains_show, stable_node_chains_prune_millisecs_show, stable_node_chains_prune_millisecs_store, full_scans_show, ksm_init",
          "description": "此代码段实现了KSM（内核同页合并）子系统的sysfs接口，用于暴露稳定节点重复计数、链表数量及修剪间隔等运行时参数。其中`stable_node_*`系列函数通过sysfs接口读取内部状态变量，`stable_node_chains_prune_millisecs`支持动态修改修剪超时时长，`ksm_init`初始化KSM线程并注册sysfs属性组。",
          "similarity": 0.45423153042793274
        }
      ]
    },
    {
      "source_file": "mm/mmu_notifier.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:53:38\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mmu_notifier.c`\n\n---\n\n# mmu_notifier.c 技术文档\n\n## 1. 文件概述\n\n`mmu_notifier.c` 是 Linux 内核中实现 **MMU Notifier（内存管理单元通知器）** 机制的核心文件。该机制允许内核子系统（如 KVM、RDMA、DAX 等）在用户虚拟地址空间发生页表变更（如页面回收、映射撤销等）时收到通知，从而同步维护其私有页表（如影子页表 SPTEs）或缓存状态。  \n本文件主要实现了基于 **区间树（interval tree）** 的高效范围监听机制，并通过一种类似 seqcount 的 **碰撞重试（collision-retry）读写同步模型**，在保证高并发性的同时避免在关键路径上使用阻塞锁。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct mmu_notifier_subscriptions`**  \n  每个 `mm_struct` 关联的订阅信息容器，包含：\n  - `list`：传统 MMU notifier 链表（用于非区间监听）\n  - `itree`：基于红黑树的区间树，存储 `mmu_interval_notifier`\n  - `invalidate_seq`：序列号，用于实现读写同步（奇数表示正在无效化）\n  - `active_invalidate_ranges`：当前活跃的无效化操作计数\n  - `deferred_list`：延迟处理的区间插入/删除队列\n  - `wq`：等待队列，用于唤醒等待无效化完成的读者\n  - `lock`：保护上述字段的自旋锁\n\n- **全局 SRCU 实例 `srcu`**  \n  用于安全地遍历和回调 MMU notifier 列表，避免在 RCU 临界区内睡眠。\n\n- **Lockdep 映射 `__mmu_notifier_invalidate_range_start_map`**  \n  用于死锁检测，标记 `invalidate_range_start` 的锁上下文。\n\n### 主要函数\n\n- **`mn_itree_inv_start_range()`**  \n  开始一个虚拟地址范围的无效化操作：增加计数、检查是否有监听者、若存在则将 `invalidate_seq` 设为奇数，并返回首个匹配的监听器。\n\n- **`mn_itree_inv_next()`**  \n  在无效化过程中迭代获取下一个匹配的区间监听器。\n\n- **`mn_itree_inv_end()`**  \n  结束无效化操作：减少计数；若为最后一个操作且处于完全排除状态，则将 `invalidate_seq` 加 1（变为偶数），并处理 `deferred_list` 中的延迟插入/删除，最后唤醒等待队列。\n\n- **`mmu_interval_read_begin()`**（片段）  \n  开始一个读端临界区：读取当前监听器的 `invalidate_seq`，用于后续与全局 `invalidate_seq` 比较以检测是否发生碰撞（即无效化操作介入）。\n\n> 注：`mmu_interval_read_retry()` 函数虽未完整给出，但其作用是比对 `interval_sub->invalidate_seq` 与读开始时保存的全局 `seq`，若不同则说明发生碰撞需重试。\n\n## 3. 关键实现\n\n### 碰撞重试同步机制（Collision-Retry Lock）\n\n- 使用 `invalidate_seq` 序列号模拟读写锁，但允许多个写者并发。\n- **写者（无效化操作）**：\n  - 进入时：`active_invalidate_ranges++`；若有监听者，则 `seq |= 1`（设为奇数）。\n  - 退出时：`active_invalidate_ranges--`；若为最后一个写者且 `seq` 为奇数，则 `seq++`（变为偶数）。\n- **读者（如获取 SPTE）**：\n  - 调用 `mmu_interval_read_begin()` 获取当前 `seq`。\n  - 在持有用户锁（如 mmap_lock）期间执行操作。\n  - 调用 `mmu_interval_read_retry()` 检查 `interval_sub->invalidate_seq` 是否等于初始 `seq`。若不等，说明无效化已发生，需重试。\n- **优势**：避免在 `invalidate_range_start` 中使用阻塞锁，提升 mm 路径性能。\n\n### 区间树与延迟更新\n\n- 所有 `mmu_interval_notifier` 按虚拟地址范围注册到 `itree` 中，支持高效范围查询。\n- 在无效化过程中（`invalidate_seq` 为奇数），禁止直接修改 `itree`。\n- 插入/删除操作被暂存到 `deferred_list`，在最后一个 `inv_end` 时批量处理，确保树结构一致性。\n\n### SRCU 用于安全回调\n\n- 全局 `srcu` 用于遍历 `mm->notifier_subscriptions->list` 并调用传统 notifier 回调。\n- 允许回调函数睡眠（相比 RCU 更灵活），同时保证在 `mm` 销毁前完成所有回调。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/mmu_notifier.h>`：核心 API 和数据结构定义\n  - `<linux/interval_tree.h>`：区间树操作\n  - `<linux/srcu.h>`、`<linux/rcupdate.h>`：同步原语\n  - `<linux/mm.h>`、`<linux/sched/mm.h>`：内存管理相关\n  - `<linux/rculist.h>`、`<linux/slab.h>`：链表和内存分配\n\n- **内核子系统依赖**：\n  - **Memory Management (MM)**：依赖 `mm_struct` 生命周期管理（`mmdrop` 释放 subscriptions）\n  - **KVM / VFIO / RDMA / DAX**：作为主要使用者，注册 `mmu_interval_notifier` 监听 VA 变更\n  - **Lockdep**：用于死锁检测（`CONFIG_LOCKDEP`）\n\n## 5. 使用场景\n\n1. **虚拟化（KVM）**  \n   当 Guest OS 的页表被 Host 回收或修改时，KVM 通过 MMU notifier 同步更新影子页表（SPTEs），避免访问已释放的物理页。\n\n2. **高性能计算（RDMA / InfiniBand）**  \n   用户态注册内存区域用于零拷贝 DMA。当该区域被 munmap 或 swap out 时，驱动需收到通知以撤销硬件映射，防止 DMA 访问非法内存。\n\n3. **持久内存（DAX）**  \n   DAX 直接映射持久内存到用户空间。当映射被撤销时，需刷新 CPU 缓存并确保数据持久化，MMU notifier 提供必要的同步点。\n\n4. **用户态页表管理**  \n   如用户态缺页处理（userfaultfd）或自定义内存管理器，需感知内核对 VA 的修改以维护一致性。\n\n> 总结：`mmu_notifier.c` 为需要与内核页表变更保持同步的子系统提供了高效、可扩展的通知框架，是现代 Linux 内核中虚拟化、高性能 I/O 和新型存储的关键基础设施。",
      "similarity": 0.5405924320220947,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 385,
          "end_line": 514,
          "content": [
            "int __mmu_notifier_clear_young(struct mm_struct *mm,",
            "\t\t\t       unsigned long start,",
            "\t\t\t       unsigned long end)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint young = 0, id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription,",
            "\t\t\t\t &mm->notifier_subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tif (subscription->ops->clear_young)",
            "\t\t\tyoung |= subscription->ops->clear_young(subscription,",
            "\t\t\t\t\t\t\t\tmm, start, end);",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "",
            "\treturn young;",
            "}",
            "int __mmu_notifier_test_young(struct mm_struct *mm,",
            "\t\t\t      unsigned long address)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint young = 0, id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription,",
            "\t\t\t\t &mm->notifier_subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tif (subscription->ops->test_young) {",
            "\t\t\tyoung = subscription->ops->test_young(subscription, mm,",
            "\t\t\t\t\t\t\t      address);",
            "\t\t\tif (young)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "",
            "\treturn young;",
            "}",
            "static int mn_itree_invalidate(struct mmu_notifier_subscriptions *subscriptions,",
            "\t\t\t       const struct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_interval_notifier *interval_sub;",
            "\tunsigned long cur_seq;",
            "",
            "\tfor (interval_sub =",
            "\t\t     mn_itree_inv_start_range(subscriptions, range, &cur_seq);",
            "\t     interval_sub;",
            "\t     interval_sub = mn_itree_inv_next(interval_sub, range)) {",
            "\t\tbool ret;",
            "",
            "\t\tret = interval_sub->ops->invalidate(interval_sub, range,",
            "\t\t\t\t\t\t    cur_seq);",
            "\t\tif (!ret) {",
            "\t\t\tif (WARN_ON(mmu_notifier_range_blockable(range)))",
            "\t\t\t\tcontinue;",
            "\t\t\tgoto out_would_block;",
            "\t\t}",
            "\t}",
            "\treturn 0;",
            "",
            "out_would_block:",
            "\t/*",
            "\t * On -EAGAIN the non-blocking caller is not allowed to call",
            "\t * invalidate_range_end()",
            "\t */",
            "\tmn_itree_inv_end(subscriptions);",
            "\treturn -EAGAIN;",
            "}",
            "static int mn_hlist_invalidate_range_start(",
            "\tstruct mmu_notifier_subscriptions *subscriptions,",
            "\tstruct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint ret = 0;",
            "\tint id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tconst struct mmu_notifier_ops *ops = subscription->ops;",
            "",
            "\t\tif (ops->invalidate_range_start) {",
            "\t\t\tint _ret;",
            "",
            "\t\t\tif (!mmu_notifier_range_blockable(range))",
            "\t\t\t\tnon_block_start();",
            "\t\t\t_ret = ops->invalidate_range_start(subscription, range);",
            "\t\t\tif (!mmu_notifier_range_blockable(range))",
            "\t\t\t\tnon_block_end();",
            "\t\t\tif (_ret) {",
            "\t\t\t\tpr_info(\"%pS callback failed with %d in %sblockable context.\\n\",",
            "\t\t\t\t\tops->invalidate_range_start, _ret,",
            "\t\t\t\t\t!mmu_notifier_range_blockable(range) ?",
            "\t\t\t\t\t\t\"non-\" :",
            "\t\t\t\t\t\t\"\");",
            "\t\t\t\tWARN_ON(mmu_notifier_range_blockable(range) ||",
            "\t\t\t\t\t_ret != -EAGAIN);",
            "\t\t\t\t/*",
            "\t\t\t\t * We call all the notifiers on any EAGAIN,",
            "\t\t\t\t * there is no way for a notifier to know if",
            "\t\t\t\t * its start method failed, thus a start that",
            "\t\t\t\t * does EAGAIN can't also do end.",
            "\t\t\t\t */",
            "\t\t\t\tWARN_ON(ops->invalidate_range_end);",
            "\t\t\t\tret = _ret;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\tif (ret) {",
            "\t\t/*",
            "\t\t * Must be non-blocking to get here.  If there are multiple",
            "\t\t * notifiers and one or more failed start, any that succeeded",
            "\t\t * start are expecting their end to be called.  Do so now.",
            "\t\t */",
            "\t\thlist_for_each_entry_rcu(subscription, &subscriptions->list,",
            "\t\t\t\t\t hlist, srcu_read_lock_held(&srcu)) {",
            "\t\t\tif (!subscription->ops->invalidate_range_end)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tsubscription->ops->invalidate_range_end(subscription,",
            "\t\t\t\t\t\t\t\trange);",
            "\t\t}",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__mmu_notifier_clear_young, __mmu_notifier_test_young, mn_itree_invalidate, mn_hlist_invalidate_range_start",
          "description": "实现了年轻位清除(test_young/clear_young)和范围无效化(invalidate_range_start/end)的通用框架，通过RCU遍历订阅列表并分发到具体实现，支持非阻塞场景下的错误处理与回滚。",
          "similarity": 0.49739134311676025
        },
        {
          "chunk_id": 6,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 970,
          "end_line": 1066,
          "content": [
            "int mmu_interval_notifier_insert(struct mmu_interval_notifier *interval_sub,",
            "\t\t\t\t struct mm_struct *mm, unsigned long start,",
            "\t\t\t\t unsigned long length,",
            "\t\t\t\t const struct mmu_interval_notifier_ops *ops)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions;",
            "\tint ret;",
            "",
            "\tmight_lock(&mm->mmap_lock);",
            "",
            "\tsubscriptions = smp_load_acquire(&mm->notifier_subscriptions);",
            "\tif (!subscriptions || !subscriptions->has_itree) {",
            "\t\tret = mmu_notifier_register(NULL, mm);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t\tsubscriptions = mm->notifier_subscriptions;",
            "\t}",
            "\treturn __mmu_interval_notifier_insert(interval_sub, mm, subscriptions,",
            "\t\t\t\t\t      start, length, ops);",
            "}",
            "int mmu_interval_notifier_insert_locked(",
            "\tstruct mmu_interval_notifier *interval_sub, struct mm_struct *mm,",
            "\tunsigned long start, unsigned long length,",
            "\tconst struct mmu_interval_notifier_ops *ops)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\tmm->notifier_subscriptions;",
            "\tint ret;",
            "",
            "\tmmap_assert_write_locked(mm);",
            "",
            "\tif (!subscriptions || !subscriptions->has_itree) {",
            "\t\tret = __mmu_notifier_register(NULL, mm);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t\tsubscriptions = mm->notifier_subscriptions;",
            "\t}",
            "\treturn __mmu_interval_notifier_insert(interval_sub, mm, subscriptions,",
            "\t\t\t\t\t      start, length, ops);",
            "}",
            "static bool",
            "mmu_interval_seq_released(struct mmu_notifier_subscriptions *subscriptions,",
            "\t\t\t  unsigned long seq)",
            "{",
            "\tbool ret;",
            "",
            "\tspin_lock(&subscriptions->lock);",
            "\tret = subscriptions->invalidate_seq != seq;",
            "\tspin_unlock(&subscriptions->lock);",
            "\treturn ret;",
            "}",
            "void mmu_interval_notifier_remove(struct mmu_interval_notifier *interval_sub)",
            "{",
            "\tstruct mm_struct *mm = interval_sub->mm;",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\tmm->notifier_subscriptions;",
            "\tunsigned long seq = 0;",
            "",
            "\tmight_sleep();",
            "",
            "\tspin_lock(&subscriptions->lock);",
            "\tif (mn_itree_is_invalidating(subscriptions)) {",
            "\t\t/*",
            "\t\t * remove is being called after insert put this on the",
            "\t\t * deferred list, but before the deferred list was processed.",
            "\t\t */",
            "\t\tif (RB_EMPTY_NODE(&interval_sub->interval_tree.rb)) {",
            "\t\t\thlist_del(&interval_sub->deferred_item);",
            "\t\t} else {",
            "\t\t\thlist_add_head(&interval_sub->deferred_item,",
            "\t\t\t\t       &subscriptions->deferred_list);",
            "\t\t\tseq = subscriptions->invalidate_seq;",
            "\t\t}",
            "\t} else {",
            "\t\tWARN_ON(RB_EMPTY_NODE(&interval_sub->interval_tree.rb));",
            "\t\tinterval_tree_remove(&interval_sub->interval_tree,",
            "\t\t\t\t     &subscriptions->itree);",
            "\t}",
            "\tspin_unlock(&subscriptions->lock);",
            "",
            "\t/*",
            "\t * The possible sleep on progress in the invalidation requires the",
            "\t * caller not hold any locks held by invalidation callbacks.",
            "\t */",
            "\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);",
            "\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);",
            "\tif (seq)",
            "\t\twait_event(subscriptions->wq,",
            "\t\t\t   mmu_interval_seq_released(subscriptions, seq));",
            "",
            "\t/* pairs with mmgrab in mmu_interval_notifier_insert() */",
            "\tmmdrop(mm);",
            "}",
            "void mmu_notifier_synchronize(void)",
            "{",
            "\tsynchronize_srcu(&srcu);",
            "}"
          ],
          "function_name": "mmu_interval_notifier_insert, mmu_interval_notifier_insert_locked, mmu_interval_seq_released, mmu_interval_notifier_remove, mmu_notifier_synchronize",
          "description": "管理基于区间树的MMU观察者插入与移除逻辑，通过间隔树跟踪虚拟地址范围，处理无效化期间的延迟删除操作，提供同步机制确保序列号变更后唤醒等待线程，最终通过mmdrop释放mm引用",
          "similarity": 0.4925554096698761
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 86,
          "end_line": 201,
          "content": [
            "static bool",
            "mn_itree_is_invalidating(struct mmu_notifier_subscriptions *subscriptions)",
            "{",
            "\tlockdep_assert_held(&subscriptions->lock);",
            "\treturn subscriptions->invalidate_seq & 1;",
            "}",
            "static void mn_itree_inv_end(struct mmu_notifier_subscriptions *subscriptions)",
            "{",
            "\tstruct mmu_interval_notifier *interval_sub;",
            "\tstruct hlist_node *next;",
            "",
            "\tspin_lock(&subscriptions->lock);",
            "\tif (--subscriptions->active_invalidate_ranges ||",
            "\t    !mn_itree_is_invalidating(subscriptions)) {",
            "\t\tspin_unlock(&subscriptions->lock);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Make invalidate_seq even */",
            "\tsubscriptions->invalidate_seq++;",
            "",
            "\t/*",
            "\t * The inv_end incorporates a deferred mechanism like rtnl_unlock().",
            "\t * Adds and removes are queued until the final inv_end happens then",
            "\t * they are progressed. This arrangement for tree updates is used to",
            "\t * avoid using a blocking lock during invalidate_range_start.",
            "\t */",
            "\thlist_for_each_entry_safe(interval_sub, next,",
            "\t\t\t\t  &subscriptions->deferred_list,",
            "\t\t\t\t  deferred_item) {",
            "\t\tif (RB_EMPTY_NODE(&interval_sub->interval_tree.rb))",
            "\t\t\tinterval_tree_insert(&interval_sub->interval_tree,",
            "\t\t\t\t\t     &subscriptions->itree);",
            "\t\telse",
            "\t\t\tinterval_tree_remove(&interval_sub->interval_tree,",
            "\t\t\t\t\t     &subscriptions->itree);",
            "\t\thlist_del(&interval_sub->deferred_item);",
            "\t}",
            "\tspin_unlock(&subscriptions->lock);",
            "",
            "\twake_up_all(&subscriptions->wq);",
            "}",
            "unsigned long",
            "mmu_interval_read_begin(struct mmu_interval_notifier *interval_sub)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\tinterval_sub->mm->notifier_subscriptions;",
            "\tunsigned long seq;",
            "\tbool is_invalidating;",
            "",
            "\t/*",
            "\t * If the subscription has a different seq value under the user_lock",
            "\t * than we started with then it has collided.",
            "\t *",
            "\t * If the subscription currently has the same seq value as the",
            "\t * subscriptions seq, then it is currently between",
            "\t * invalidate_start/end and is colliding.",
            "\t *",
            "\t * The locking looks broadly like this:",
            "\t *   mn_itree_inv_start():                 mmu_interval_read_begin():",
            "\t *                                         spin_lock",
            "\t *                                          seq = READ_ONCE(interval_sub->invalidate_seq);",
            "\t *                                          seq == subs->invalidate_seq",
            "\t *                                         spin_unlock",
            "\t *    spin_lock",
            "\t *     seq = ++subscriptions->invalidate_seq",
            "\t *    spin_unlock",
            "\t *     op->invalidate():",
            "\t *       user_lock",
            "\t *        mmu_interval_set_seq()",
            "\t *         interval_sub->invalidate_seq = seq",
            "\t *       user_unlock",
            "\t *",
            "\t *                          [Required: mmu_interval_read_retry() == true]",
            "\t *",
            "\t *   mn_itree_inv_end():",
            "\t *    spin_lock",
            "\t *     seq = ++subscriptions->invalidate_seq",
            "\t *    spin_unlock",
            "\t *",
            "\t *                                        user_lock",
            "\t *                                         mmu_interval_read_retry():",
            "\t *                                          interval_sub->invalidate_seq != seq",
            "\t *                                        user_unlock",
            "\t *",
            "\t * Barriers are not needed here as any races here are closed by an",
            "\t * eventual mmu_interval_read_retry(), which provides a barrier via the",
            "\t * user_lock.",
            "\t */",
            "\tspin_lock(&subscriptions->lock);",
            "\t/* Pairs with the WRITE_ONCE in mmu_interval_set_seq() */",
            "\tseq = READ_ONCE(interval_sub->invalidate_seq);",
            "\tis_invalidating = seq == subscriptions->invalidate_seq;",
            "\tspin_unlock(&subscriptions->lock);",
            "",
            "\t/*",
            "\t * interval_sub->invalidate_seq must always be set to an odd value via",
            "\t * mmu_interval_set_seq() using the provided cur_seq from",
            "\t * mn_itree_inv_start_range(). This ensures that if seq does wrap we",
            "\t * will always clear the below sleep in some reasonable time as",
            "\t * subscriptions->invalidate_seq is even in the idle state.",
            "\t */",
            "\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);",
            "\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);",
            "\tif (is_invalidating)",
            "\t\twait_event(subscriptions->wq,",
            "\t\t\t   READ_ONCE(subscriptions->invalidate_seq) != seq);",
            "",
            "\t/*",
            "\t * Notice that mmu_interval_read_retry() can already be true at this",
            "\t * point, avoiding loops here allows the caller to provide a global",
            "\t * time bound.",
            "\t */",
            "",
            "\treturn seq;",
            "}"
          ],
          "function_name": "mn_itree_is_invalidating, mn_itree_inv_end, mmu_interval_read_begin",
          "description": "实现了区间树无效化状态检测与结束逻辑，通过序列号比较判断是否处于无效化状态，并在无效化结束时处理延迟的树节点插入/移除操作，同时提供读取序列号的接口防止数据竞争。",
          "similarity": 0.47301146388053894
        },
        {
          "chunk_id": 4,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 519,
          "end_line": 666,
          "content": [
            "int __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\trange->mm->notifier_subscriptions;",
            "\tint ret;",
            "",
            "\tif (subscriptions->has_itree) {",
            "\t\tret = mn_itree_invalidate(subscriptions, range);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t}",
            "\tif (!hlist_empty(&subscriptions->list))",
            "\t\treturn mn_hlist_invalidate_range_start(subscriptions, range);",
            "\treturn 0;",
            "}",
            "static void",
            "mn_hlist_invalidate_end(struct mmu_notifier_subscriptions *subscriptions,",
            "\t\t\tstruct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tif (subscription->ops->invalidate_range_end) {",
            "\t\t\tif (!mmu_notifier_range_blockable(range))",
            "\t\t\t\tnon_block_start();",
            "\t\t\tsubscription->ops->invalidate_range_end(subscription,",
            "\t\t\t\t\t\t\t\trange);",
            "\t\t\tif (!mmu_notifier_range_blockable(range))",
            "\t\t\t\tnon_block_end();",
            "\t\t}",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "}",
            "void __mmu_notifier_invalidate_range_end(struct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\trange->mm->notifier_subscriptions;",
            "",
            "\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);",
            "\tif (subscriptions->has_itree)",
            "\t\tmn_itree_inv_end(subscriptions);",
            "",
            "\tif (!hlist_empty(&subscriptions->list))",
            "\t\tmn_hlist_invalidate_end(subscriptions, range);",
            "\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);",
            "}",
            "void __mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,",
            "\t\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription,",
            "\t\t\t\t &mm->notifier_subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tif (subscription->ops->arch_invalidate_secondary_tlbs)",
            "\t\t\tsubscription->ops->arch_invalidate_secondary_tlbs(",
            "\t\t\t\tsubscription, mm,",
            "\t\t\t\tstart, end);",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "}",
            "int __mmu_notifier_register(struct mmu_notifier *subscription,",
            "\t\t\t    struct mm_struct *mm)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions = NULL;",
            "\tint ret;",
            "",
            "\tmmap_assert_write_locked(mm);",
            "\tBUG_ON(atomic_read(&mm->mm_users) <= 0);",
            "",
            "\t/*",
            "\t * Subsystems should only register for invalidate_secondary_tlbs() or",
            "\t * invalidate_range_start()/end() callbacks, not both.",
            "\t */",
            "\tif (WARN_ON_ONCE(subscription &&",
            "\t\t\t (subscription->ops->arch_invalidate_secondary_tlbs &&",
            "\t\t\t (subscription->ops->invalidate_range_start ||",
            "\t\t\t  subscription->ops->invalidate_range_end))))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!mm->notifier_subscriptions) {",
            "\t\t/*",
            "\t\t * kmalloc cannot be called under mm_take_all_locks(), but we",
            "\t\t * know that mm->notifier_subscriptions can't change while we",
            "\t\t * hold the write side of the mmap_lock.",
            "\t\t */",
            "\t\tsubscriptions = kzalloc(",
            "\t\t\tsizeof(struct mmu_notifier_subscriptions), GFP_KERNEL);",
            "\t\tif (!subscriptions)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tINIT_HLIST_HEAD(&subscriptions->list);",
            "\t\tspin_lock_init(&subscriptions->lock);",
            "\t\tsubscriptions->invalidate_seq = 2;",
            "\t\tsubscriptions->itree = RB_ROOT_CACHED;",
            "\t\tinit_waitqueue_head(&subscriptions->wq);",
            "\t\tINIT_HLIST_HEAD(&subscriptions->deferred_list);",
            "\t}",
            "",
            "\tret = mm_take_all_locks(mm);",
            "\tif (unlikely(ret))",
            "\t\tgoto out_clean;",
            "",
            "\t/*",
            "\t * Serialize the update against mmu_notifier_unregister. A",
            "\t * side note: mmu_notifier_release can't run concurrently with",
            "\t * us because we hold the mm_users pin (either implicitly as",
            "\t * current->mm or explicitly with get_task_mm() or similar).",
            "\t * We can't race against any other mmu notifier method either",
            "\t * thanks to mm_take_all_locks().",
            "\t *",
            "\t * release semantics on the initialization of the",
            "\t * mmu_notifier_subscriptions's contents are provided for unlocked",
            "\t * readers.  acquire can only be used while holding the mmgrab or",
            "\t * mmget, and is safe because once created the",
            "\t * mmu_notifier_subscriptions is not freed until the mm is destroyed.",
            "\t * As above, users holding the mmap_lock or one of the",
            "\t * mm_take_all_locks() do not need to use acquire semantics.",
            "\t */",
            "\tif (subscriptions)",
            "\t\tsmp_store_release(&mm->notifier_subscriptions, subscriptions);",
            "",
            "\tif (subscription) {",
            "\t\t/* Pairs with the mmdrop in mmu_notifier_unregister_* */",
            "\t\tmmgrab(mm);",
            "\t\tsubscription->mm = mm;",
            "\t\tsubscription->users = 1;",
            "",
            "\t\tspin_lock(&mm->notifier_subscriptions->lock);",
            "\t\thlist_add_head_rcu(&subscription->hlist,",
            "\t\t\t\t   &mm->notifier_subscriptions->list);",
            "\t\tspin_unlock(&mm->notifier_subscriptions->lock);",
            "\t} else",
            "\t\tmm->notifier_subscriptions->has_itree = true;",
            "",
            "\tmm_drop_all_locks(mm);",
            "\tBUG_ON(atomic_read(&mm->mm_users) <= 0);",
            "\treturn 0;",
            "",
            "out_clean:",
            "\tkfree(subscriptions);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__mmu_notifier_invalidate_range_start, mn_hlist_invalidate_end, __mmu_notifier_invalidate_range_end, __mmu_notifier_arch_invalidate_secondary_tlbs, __mmu_notifier_register",
          "description": "封装了完整的MMU通知器生命周期管理，包含范围无效化启动/结束接口(__mmu_notifier_invalidate_range_start/end)、架构特定TLB刷新(__mmu_notifier_arch_invalidate_secondary_tlbs)以及注册接口(__mmu_notifier_register)。",
          "similarity": 0.47277402877807617
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 697,
          "end_line": 843,
          "content": [
            "int mmu_notifier_register(struct mmu_notifier *subscription,",
            "\t\t\t  struct mm_struct *mm)",
            "{",
            "\tint ret;",
            "",
            "\tmmap_write_lock(mm);",
            "\tret = __mmu_notifier_register(subscription, mm);",
            "\tmmap_write_unlock(mm);",
            "\treturn ret;",
            "}",
            "void __mmu_notifier_subscriptions_destroy(struct mm_struct *mm)",
            "{",
            "\tBUG_ON(!hlist_empty(&mm->notifier_subscriptions->list));",
            "\tkfree(mm->notifier_subscriptions);",
            "\tmm->notifier_subscriptions = LIST_POISON1; /* debug */",
            "}",
            "void mmu_notifier_unregister(struct mmu_notifier *subscription,",
            "\t\t\t     struct mm_struct *mm)",
            "{",
            "\tBUG_ON(atomic_read(&mm->mm_count) <= 0);",
            "",
            "\tif (!hlist_unhashed(&subscription->hlist)) {",
            "\t\t/*",
            "\t\t * SRCU here will force exit_mmap to wait for ->release to",
            "\t\t * finish before freeing the pages.",
            "\t\t */",
            "\t\tint id;",
            "",
            "\t\tid = srcu_read_lock(&srcu);",
            "\t\t/*",
            "\t\t * exit_mmap will block in mmu_notifier_release to guarantee",
            "\t\t * that ->release is called before freeing the pages.",
            "\t\t */",
            "\t\tif (subscription->ops->release)",
            "\t\t\tsubscription->ops->release(subscription, mm);",
            "\t\tsrcu_read_unlock(&srcu, id);",
            "",
            "\t\tspin_lock(&mm->notifier_subscriptions->lock);",
            "\t\t/*",
            "\t\t * Can not use list_del_rcu() since __mmu_notifier_release",
            "\t\t * can delete it before we hold the lock.",
            "\t\t */",
            "\t\thlist_del_init_rcu(&subscription->hlist);",
            "\t\tspin_unlock(&mm->notifier_subscriptions->lock);",
            "\t}",
            "",
            "\t/*",
            "\t * Wait for any running method to finish, of course including",
            "\t * ->release if it was run by mmu_notifier_release instead of us.",
            "\t */",
            "\tsynchronize_srcu(&srcu);",
            "",
            "\tBUG_ON(atomic_read(&mm->mm_count) <= 0);",
            "",
            "\tmmdrop(mm);",
            "}",
            "static void mmu_notifier_free_rcu(struct rcu_head *rcu)",
            "{",
            "\tstruct mmu_notifier *subscription =",
            "\t\tcontainer_of(rcu, struct mmu_notifier, rcu);",
            "\tstruct mm_struct *mm = subscription->mm;",
            "",
            "\tsubscription->ops->free_notifier(subscription);",
            "\t/* Pairs with the get in __mmu_notifier_register() */",
            "\tmmdrop(mm);",
            "}",
            "void mmu_notifier_put(struct mmu_notifier *subscription)",
            "{",
            "\tstruct mm_struct *mm = subscription->mm;",
            "",
            "\tspin_lock(&mm->notifier_subscriptions->lock);",
            "\tif (WARN_ON(!subscription->users) || --subscription->users)",
            "\t\tgoto out_unlock;",
            "\thlist_del_init_rcu(&subscription->hlist);",
            "\tspin_unlock(&mm->notifier_subscriptions->lock);",
            "",
            "\tcall_srcu(&srcu, &subscription->rcu, mmu_notifier_free_rcu);",
            "\treturn;",
            "",
            "out_unlock:",
            "\tspin_unlock(&mm->notifier_subscriptions->lock);",
            "}",
            "static int __mmu_interval_notifier_insert(",
            "\tstruct mmu_interval_notifier *interval_sub, struct mm_struct *mm,",
            "\tstruct mmu_notifier_subscriptions *subscriptions, unsigned long start,",
            "\tunsigned long length, const struct mmu_interval_notifier_ops *ops)",
            "{",
            "\tinterval_sub->mm = mm;",
            "\tinterval_sub->ops = ops;",
            "\tRB_CLEAR_NODE(&interval_sub->interval_tree.rb);",
            "\tinterval_sub->interval_tree.start = start;",
            "\t/*",
            "\t * Note that the representation of the intervals in the interval tree",
            "\t * considers the ending point as contained in the interval.",
            "\t */",
            "\tif (length == 0 ||",
            "\t    check_add_overflow(start, length - 1,",
            "\t\t\t       &interval_sub->interval_tree.last))",
            "\t\treturn -EOVERFLOW;",
            "",
            "\t/* Must call with a mmget() held */",
            "\tif (WARN_ON(atomic_read(&mm->mm_users) <= 0))",
            "\t\treturn -EINVAL;",
            "",
            "\t/* pairs with mmdrop in mmu_interval_notifier_remove() */",
            "\tmmgrab(mm);",
            "",
            "\t/*",
            "\t * If some invalidate_range_start/end region is going on in parallel",
            "\t * we don't know what VA ranges are affected, so we must assume this",
            "\t * new range is included.",
            "\t *",
            "\t * If the itree is invalidating then we are not allowed to change",
            "\t * it. Retrying until invalidation is done is tricky due to the",
            "\t * possibility for live lock, instead defer the add to",
            "\t * mn_itree_inv_end() so this algorithm is deterministic.",
            "\t *",
            "\t * In all cases the value for the interval_sub->invalidate_seq should be",
            "\t * odd, see mmu_interval_read_begin()",
            "\t */",
            "\tspin_lock(&subscriptions->lock);",
            "\tif (subscriptions->active_invalidate_ranges) {",
            "\t\tif (mn_itree_is_invalidating(subscriptions))",
            "\t\t\thlist_add_head(&interval_sub->deferred_item,",
            "\t\t\t\t       &subscriptions->deferred_list);",
            "\t\telse {",
            "\t\t\tsubscriptions->invalidate_seq |= 1;",
            "\t\t\tinterval_tree_insert(&interval_sub->interval_tree,",
            "\t\t\t\t\t     &subscriptions->itree);",
            "\t\t}",
            "\t\tinterval_sub->invalidate_seq = subscriptions->invalidate_seq;",
            "\t} else {",
            "\t\tWARN_ON(mn_itree_is_invalidating(subscriptions));",
            "\t\t/*",
            "\t\t * The starting seq for a subscription not under invalidation",
            "\t\t * should be odd, not equal to the current invalidate_seq and",
            "\t\t * invalidate_seq should not 'wrap' to the new seq any time",
            "\t\t * soon.",
            "\t\t */",
            "\t\tinterval_sub->invalidate_seq =",
            "\t\t\tsubscriptions->invalidate_seq - 1;",
            "\t\tinterval_tree_insert(&interval_sub->interval_tree,",
            "\t\t\t\t     &subscriptions->itree);",
            "\t}",
            "\tspin_unlock(&subscriptions->lock);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "mmu_notifier_register, __mmu_notifier_subscriptions_destroy, mmu_notifier_unregister, mmu_notifier_free_rcu, mmu_notifier_put, __mmu_interval_notifier_insert",
          "description": "实现MMU观察者的注册与注销流程，通过mmap锁保护订阅列表操作，利用SRCU机制确保退出mmap时调用release回调，通过RCU延迟释放订阅者资源，处理订阅列表销毁时验证为空并释放内存",
          "similarity": 0.42289039492607117
        }
      ]
    }
  ]
}