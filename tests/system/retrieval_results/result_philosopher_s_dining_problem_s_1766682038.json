{
  "query": "philosopher's dining problem solution",
  "timestamp": "2025-12-26 01:00:38",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/fair.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:09:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\fair.c`\n\n---\n\n# `sched/fair.c` 技术文档\n\n## 1. 文件概述\n\n`sched/fair.c` 是 Linux 内核中 **完全公平调度器**（Completely Fair Scheduler, CFS）的核心实现文件，负责实现 `SCHED_NORMAL` 和 `SCHED_BATCH` 调度策略。CFS 旨在通过红黑树（RB-tree）维护可运行任务的虚拟运行时间（vruntime），以实现 CPU 时间的公平分配。该文件实现了任务调度、负载跟踪、时间片计算、组调度（group scheduling）、NUMA 负载均衡、带宽控制等关键机制，是 Linux 通用调度子系统的核心组成部分。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_entity`：调度实体，代表一个可调度单元（任务或任务组）\n- `struct cfs_rq`：CFS 运行队列，管理一组调度实体\n- `struct load_weight`：负载权重结构，用于计算任务对系统负载的贡献\n\n### 关键函数与宏\n- `__calc_delta()` / `calc_delta_fair()`：计算基于权重的调度时间增量\n- `update_load_add()` / `update_load_sub()` / `update_load_set()`：更新负载权重\n- `__update_inv_weight()`：预计算权重的倒数以优化除法运算\n- `get_update_sysctl_factor()`：根据在线 CPU 数量动态调整调度参数\n- `update_sysctl()` / `sched_init_granularity()`：初始化和更新调度粒度参数\n- `for_each_sched_entity()`：遍历调度实体层级结构（用于组调度）\n\n### 可调参数（sysctl）\n- `sysctl_sched_base_slice`：基础时间片（默认 700,000 纳秒）\n- `sysctl_sched_tunable_scaling`：调度参数缩放策略（NONE/LOG/LINEAR）\n- `sysctl_sched_migration_cost`：任务迁移成本阈值（500 微秒）\n- `sysctl_sched_cfs_bandwidth_slice_us`（CFS 带宽控制切片，默认 5 毫秒）\n- `sysctl_numa_balancing_promote_rate_limit_MBps`（NUMA 页迁移速率限制）\n\n## 3. 关键实现\n\n### 虚拟时间与公平性\nCFS 使用 **虚拟运行时间**（vruntime）衡量任务已使用的 CPU 时间，并通过 `calc_delta_fair()` 将实际执行时间按任务权重归一化。权重由任务的 nice 值决定（`NICE_0_LOAD = 1024` 为基准）。调度器总是选择 vruntime 最小的任务运行，确保高优先级（高权重）任务获得更多 CPU 时间。\n\n### 高效除法优化\n为避免频繁除法运算，CFS 预计算 `inv_weight = WMULT_CONST / weight`（`WMULT_CONST = ~0U`），将除法转换为乘法和右移操作（`mul_u64_u32_shr`）。`__calc_delta()` 通过动态调整移位位数（`shift`）保证计算精度，适用于 32/64 位架构。\n\n### 动态粒度调整\n基础时间片 `sched_base_slice` 根据在线 CPU 数量动态缩放：\n- `SCHED_TUNABLESCALING_NONE`：固定值\n- `SCHED_TUNABLESCALING_LINEAR`：线性缩放（×ncpus）\n- `SCHED_TUNABLESCALING_LOG`（默认）：对数缩放（×(1 + ilog2(ncpus))）  \n此设计确保在多核系统中保持合理的调度延迟和交互性。\n\n### 组调度支持\n通过 `for_each_sched_entity()` 宏遍历任务所属的调度实体层级（任务 → 任务组 → 父任务组），实现 CPU 带宽在任务组间的公平分配。每个 `cfs_rq` 独立维护其子实体的红黑树。\n\n### SMP 相关优化\n- **非对称 CPU 优先级**：`arch_asym_cpu_priority()` 允许架构定义 CPU 能力差异（如大小核）\n- **容量比较宏**：`fits_capacity()`（20% 容差）和 `capacity_greater()`（5% 容差）用于负载均衡决策\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- 调度核心：`\"sched.h\"`、`\"stats.h\"`、`\"autogroup.h\"`\n- 系统服务：`<linux/sched/clock.h>`、`<linux/sched/nohz.h>`、`<linux/psi.h>`\n- 内存管理：`<linux/mem_policy.h>`、`<linux/energy_model.h>`\n- SMP 支持：`<linux/topology.h>`、`<linux/cpumask_api.h>`\n- 数据结构：`<linux/rbtree_augmented.h>`\n\n### 条件编译特性\n- `CONFIG_SMP`：多处理器调度优化\n- `CONFIG_CFS_BANDWIDTH`：CPU 带宽限制（cgroup v1/v2）\n- `CONFIG_NUMA_BALANCING`：NUMA 自动迁移\n- `CONFIG_FAIR_GROUP_SCHED`：CFS 组调度（cgroup 支持）\n\n## 5. 使用场景\n\n- **通用任务调度**：所有使用 `SCHED_NORMAL` 或 `SCHED_BATCH` 策略的用户态进程\n- **cgroup CPU 资源控制**：通过 `cpu.cfs_quota_us` 和 `cpu.cfs_period_us` 限制任务组带宽\n- **NUMA 优化**：自动迁移内存页以减少远程访问（`numa_balancing`）\n- **节能调度**：结合 `energy_model` 在满足性能前提下选择低功耗 CPU\n- **实时性保障**：通过 `cond_resched()` 在长循环中主动让出 CPU，避免内核抢占延迟过高\n- **系统调优**：管理员通过 `/proc/sys/kernel/` 下的 sysctl 参数动态调整调度行为",
      "similarity": 0.43094801902770996,
      "chunks": [
        {
          "chunk_id": 17,
          "file_path": "kernel/sched/fair.c",
          "start_line": 2878,
          "end_line": 2989,
          "content": [
            "static void task_numa_placement(struct task_struct *p)",
            "{",
            "\tint seq, nid, max_nid = NUMA_NO_NODE;",
            "\tunsigned long max_faults = 0;",
            "\tunsigned long fault_types[2] = { 0, 0 };",
            "\tunsigned long total_faults;",
            "\tu64 runtime, period;",
            "\tspinlock_t *group_lock = NULL;",
            "\tstruct numa_group *ng;",
            "",
            "\t/*",
            "\t * The p->mm->numa_scan_seq field gets updated without",
            "\t * exclusive access. Use READ_ONCE() here to ensure",
            "\t * that the field is read in a single access:",
            "\t */",
            "\tseq = READ_ONCE(p->mm->numa_scan_seq);",
            "\tif (p->numa_scan_seq == seq)",
            "\t\treturn;",
            "\tp->numa_scan_seq = seq;",
            "\tp->numa_scan_period_max = task_scan_max(p);",
            "",
            "\ttotal_faults = p->numa_faults_locality[0] +",
            "\t\t       p->numa_faults_locality[1];",
            "\truntime = numa_get_avg_runtime(p, &period);",
            "",
            "\t/* If the task is part of a group prevent parallel updates to group stats */",
            "\tng = deref_curr_numa_group(p);",
            "\tif (ng) {",
            "\t\tgroup_lock = &ng->lock;",
            "\t\tspin_lock_irq(group_lock);",
            "\t}",
            "",
            "\t/* Find the node with the highest number of faults */",
            "\tfor_each_online_node(nid) {",
            "\t\t/* Keep track of the offsets in numa_faults array */",
            "\t\tint mem_idx, membuf_idx, cpu_idx, cpubuf_idx;",
            "\t\tunsigned long faults = 0, group_faults = 0;",
            "\t\tint priv;",
            "",
            "\t\tfor (priv = 0; priv < NR_NUMA_HINT_FAULT_TYPES; priv++) {",
            "\t\t\tlong diff, f_diff, f_weight;",
            "",
            "\t\t\tmem_idx = task_faults_idx(NUMA_MEM, nid, priv);",
            "\t\t\tmembuf_idx = task_faults_idx(NUMA_MEMBUF, nid, priv);",
            "\t\t\tcpu_idx = task_faults_idx(NUMA_CPU, nid, priv);",
            "\t\t\tcpubuf_idx = task_faults_idx(NUMA_CPUBUF, nid, priv);",
            "",
            "\t\t\t/* Decay existing window, copy faults since last scan */",
            "\t\t\tdiff = p->numa_faults[membuf_idx] - p->numa_faults[mem_idx] / 2;",
            "\t\t\tfault_types[priv] += p->numa_faults[membuf_idx];",
            "\t\t\tp->numa_faults[membuf_idx] = 0;",
            "",
            "\t\t\t/*",
            "\t\t\t * Normalize the faults_from, so all tasks in a group",
            "\t\t\t * count according to CPU use, instead of by the raw",
            "\t\t\t * number of faults. Tasks with little runtime have",
            "\t\t\t * little over-all impact on throughput, and thus their",
            "\t\t\t * faults are less important.",
            "\t\t\t */",
            "\t\t\tf_weight = div64_u64(runtime << 16, period + 1);",
            "\t\t\tf_weight = (f_weight * p->numa_faults[cpubuf_idx]) /",
            "\t\t\t\t   (total_faults + 1);",
            "\t\t\tf_diff = f_weight - p->numa_faults[cpu_idx] / 2;",
            "\t\t\tp->numa_faults[cpubuf_idx] = 0;",
            "",
            "\t\t\tp->numa_faults[mem_idx] += diff;",
            "\t\t\tp->numa_faults[cpu_idx] += f_diff;",
            "\t\t\tfaults += p->numa_faults[mem_idx];",
            "\t\t\tp->total_numa_faults += diff;",
            "\t\t\tif (ng) {",
            "\t\t\t\t/*",
            "\t\t\t\t * safe because we can only change our own group",
            "\t\t\t\t *",
            "\t\t\t\t * mem_idx represents the offset for a given",
            "\t\t\t\t * nid and priv in a specific region because it",
            "\t\t\t\t * is at the beginning of the numa_faults array.",
            "\t\t\t\t */",
            "\t\t\t\tng->faults[mem_idx] += diff;",
            "\t\t\t\tng->faults[cpu_idx] += f_diff;",
            "\t\t\t\tng->total_faults += diff;",
            "\t\t\t\tgroup_faults += ng->faults[mem_idx];",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tif (!ng) {",
            "\t\t\tif (faults > max_faults) {",
            "\t\t\t\tmax_faults = faults;",
            "\t\t\t\tmax_nid = nid;",
            "\t\t\t}",
            "\t\t} else if (group_faults > max_faults) {",
            "\t\t\tmax_faults = group_faults;",
            "\t\t\tmax_nid = nid;",
            "\t\t}",
            "\t}",
            "",
            "\t/* Cannot migrate task to CPU-less node */",
            "\tmax_nid = numa_nearest_node(max_nid, N_CPU);",
            "",
            "\tif (ng) {",
            "\t\tnuma_group_count_active_nodes(ng);",
            "\t\tspin_unlock_irq(group_lock);",
            "\t\tmax_nid = preferred_group_nid(p, max_nid);",
            "\t}",
            "",
            "\tif (max_faults) {",
            "\t\t/* Set the new preferred node */",
            "\t\tif (max_nid != p->numa_preferred_nid)",
            "\t\t\tsched_setnuma(p, max_nid);",
            "\t}",
            "",
            "\tupdate_task_scan_period(p, fault_types[0], fault_types[1]);",
            "}"
          ],
          "function_name": "task_numa_placement",
          "description": "task_numa_placement遍历所有在线节点，根据NUMA故障次数确定首选节点，更新任务的numa_preferred_nid，并调整扫描周期参数。",
          "similarity": 0.4216834008693695
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/sched/fair.c",
          "start_line": 2430,
          "end_line": 2620,
          "content": [
            "static void task_numa_find_cpu(struct task_numa_env *env,",
            "\t\t\t\tlong taskimp, long groupimp)",
            "{",
            "\tbool maymove = false;",
            "\tint cpu;",
            "",
            "\t/*",
            "\t * If dst node has spare capacity, then check if there is an",
            "\t * imbalance that would be overruled by the load balancer.",
            "\t */",
            "\tif (env->dst_stats.node_type == node_has_spare) {",
            "\t\tunsigned int imbalance;",
            "\t\tint src_running, dst_running;",
            "",
            "\t\t/*",
            "\t\t * Would movement cause an imbalance? Note that if src has",
            "\t\t * more running tasks that the imbalance is ignored as the",
            "\t\t * move improves the imbalance from the perspective of the",
            "\t\t * CPU load balancer.",
            "\t\t * */",
            "\t\tsrc_running = env->src_stats.nr_running - 1;",
            "\t\tdst_running = env->dst_stats.nr_running + 1;",
            "\t\timbalance = max(0, dst_running - src_running);",
            "\t\timbalance = adjust_numa_imbalance(imbalance, dst_running,",
            "\t\t\t\t\t\t  env->imb_numa_nr);",
            "",
            "\t\t/* Use idle CPU if there is no imbalance */",
            "\t\tif (!imbalance) {",
            "\t\t\tmaymove = true;",
            "\t\t\tif (env->dst_stats.idle_cpu >= 0) {",
            "\t\t\t\tenv->dst_cpu = env->dst_stats.idle_cpu;",
            "\t\t\t\ttask_numa_assign(env, NULL, 0);",
            "\t\t\t\treturn;",
            "\t\t\t}",
            "\t\t}",
            "\t} else {",
            "\t\tlong src_load, dst_load, load;",
            "\t\t/*",
            "\t\t * If the improvement from just moving env->p direction is better",
            "\t\t * than swapping tasks around, check if a move is possible.",
            "\t\t */",
            "\t\tload = task_h_load(env->p);",
            "\t\tdst_load = env->dst_stats.load + load;",
            "\t\tsrc_load = env->src_stats.load - load;",
            "\t\tmaymove = !load_too_imbalanced(src_load, dst_load, env);",
            "\t}",
            "",
            "\tfor_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {",
            "\t\t/* Skip this CPU if the source task cannot migrate */",
            "\t\tif (!cpumask_test_cpu(cpu, env->p->cpus_ptr))",
            "\t\t\tcontinue;",
            "",
            "\t\tenv->dst_cpu = cpu;",
            "\t\tif (task_numa_compare(env, taskimp, groupimp, maymove))",
            "\t\t\tbreak;",
            "\t}",
            "}",
            "static int task_numa_migrate(struct task_struct *p)",
            "{",
            "\tstruct task_numa_env env = {",
            "\t\t.p = p,",
            "",
            "\t\t.src_cpu = task_cpu(p),",
            "\t\t.src_nid = task_node(p),",
            "",
            "\t\t.imbalance_pct = 112,",
            "",
            "\t\t.best_task = NULL,",
            "\t\t.best_imp = 0,",
            "\t\t.best_cpu = -1,",
            "\t};",
            "\tunsigned long taskweight, groupweight;",
            "\tstruct sched_domain *sd;",
            "\tlong taskimp, groupimp;",
            "\tstruct numa_group *ng;",
            "\tstruct rq *best_rq;",
            "\tint nid, ret, dist;",
            "",
            "\t/*",
            "\t * Pick the lowest SD_NUMA domain, as that would have the smallest",
            "\t * imbalance and would be the first to start moving tasks about.",
            "\t *",
            "\t * And we want to avoid any moving of tasks about, as that would create",
            "\t * random movement of tasks -- counter the numa conditions we're trying",
            "\t * to satisfy here.",
            "\t */",
            "\trcu_read_lock();",
            "\tsd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));",
            "\tif (sd) {",
            "\t\tenv.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;",
            "\t\tenv.imb_numa_nr = sd->imb_numa_nr;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * Cpusets can break the scheduler domain tree into smaller",
            "\t * balance domains, some of which do not cross NUMA boundaries.",
            "\t * Tasks that are \"trapped\" in such domains cannot be migrated",
            "\t * elsewhere, so there is no point in (re)trying.",
            "\t */",
            "\tif (unlikely(!sd)) {",
            "\t\tsched_setnuma(p, task_node(p));",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tenv.dst_nid = p->numa_preferred_nid;",
            "\tdist = env.dist = node_distance(env.src_nid, env.dst_nid);",
            "\ttaskweight = task_weight(p, env.src_nid, dist);",
            "\tgroupweight = group_weight(p, env.src_nid, dist);",
            "\tupdate_numa_stats(&env, &env.src_stats, env.src_nid, false);",
            "\ttaskimp = task_weight(p, env.dst_nid, dist) - taskweight;",
            "\tgroupimp = group_weight(p, env.dst_nid, dist) - groupweight;",
            "\tupdate_numa_stats(&env, &env.dst_stats, env.dst_nid, true);",
            "",
            "\t/* Try to find a spot on the preferred nid. */",
            "\ttask_numa_find_cpu(&env, taskimp, groupimp);",
            "",
            "\t/*",
            "\t * Look at other nodes in these cases:",
            "\t * - there is no space available on the preferred_nid",
            "\t * - the task is part of a numa_group that is interleaved across",
            "\t *   multiple NUMA nodes; in order to better consolidate the group,",
            "\t *   we need to check other locations.",
            "\t */",
            "\tng = deref_curr_numa_group(p);",
            "\tif (env.best_cpu == -1 || (ng && ng->active_nodes > 1)) {",
            "\t\tfor_each_node_state(nid, N_CPU) {",
            "\t\t\tif (nid == env.src_nid || nid == p->numa_preferred_nid)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tdist = node_distance(env.src_nid, env.dst_nid);",
            "\t\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&",
            "\t\t\t\t\t\tdist != env.dist) {",
            "\t\t\t\ttaskweight = task_weight(p, env.src_nid, dist);",
            "\t\t\t\tgroupweight = group_weight(p, env.src_nid, dist);",
            "\t\t\t}",
            "",
            "\t\t\t/* Only consider nodes where both task and groups benefit */",
            "\t\t\ttaskimp = task_weight(p, nid, dist) - taskweight;",
            "\t\t\tgroupimp = group_weight(p, nid, dist) - groupweight;",
            "\t\t\tif (taskimp < 0 && groupimp < 0)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tenv.dist = dist;",
            "\t\t\tenv.dst_nid = nid;",
            "\t\t\tupdate_numa_stats(&env, &env.dst_stats, env.dst_nid, true);",
            "\t\t\ttask_numa_find_cpu(&env, taskimp, groupimp);",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * If the task is part of a workload that spans multiple NUMA nodes,",
            "\t * and is migrating into one of the workload's active nodes, remember",
            "\t * this node as the task's preferred numa node, so the workload can",
            "\t * settle down.",
            "\t * A task that migrated to a second choice node will be better off",
            "\t * trying for a better one later. Do not set the preferred node here.",
            "\t */",
            "\tif (ng) {",
            "\t\tif (env.best_cpu == -1)",
            "\t\t\tnid = env.src_nid;",
            "\t\telse",
            "\t\t\tnid = cpu_to_node(env.best_cpu);",
            "",
            "\t\tif (nid != p->numa_preferred_nid)",
            "\t\t\tsched_setnuma(p, nid);",
            "\t}",
            "",
            "\t/* No better CPU than the current one was found. */",
            "\tif (env.best_cpu == -1) {",
            "\t\ttrace_sched_stick_numa(p, env.src_cpu, NULL, -1);",
            "\t\treturn -EAGAIN;",
            "\t}",
            "",
            "\tbest_rq = cpu_rq(env.best_cpu);",
            "\tif (env.best_task == NULL) {",
            "\t\tret = migrate_task_to(p, env.best_cpu);",
            "\t\tWRITE_ONCE(best_rq->numa_migrate_on, 0);",
            "\t\tif (ret != 0)",
            "\t\t\ttrace_sched_stick_numa(p, env.src_cpu, NULL, env.best_cpu);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu);",
            "\tWRITE_ONCE(best_rq->numa_migrate_on, 0);",
            "",
            "\tif (ret != 0)",
            "\t\ttrace_sched_stick_numa(p, env.src_cpu, env.best_task, env.best_cpu);",
            "\tput_task_struct(env.best_task);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "task_numa_find_cpu, task_numa_migrate",
          "description": "基于NUMA拓扑和调度域选择最佳目标节点；计算任务权重差异并触发迁移；若未找到合适节点则标记为无法迁移并返回错误码",
          "similarity": 0.41289037466049194
        },
        {
          "chunk_id": 45,
          "file_path": "kernel/sched/fair.c",
          "start_line": 7352,
          "end_line": 7468,
          "content": [
            "static int",
            "wake_affine_weight(struct sched_domain *sd, struct task_struct *p,",
            "\t\t   int this_cpu, int prev_cpu, int sync)",
            "{",
            "\ts64 this_eff_load, prev_eff_load;",
            "\tunsigned long task_load;",
            "",
            "\tthis_eff_load = cpu_load(cpu_rq(this_cpu));",
            "",
            "\tif (sync) {",
            "\t\tunsigned long current_load = task_h_load(current);",
            "",
            "\t\tif (current_load > this_eff_load)",
            "\t\t\treturn this_cpu;",
            "",
            "\t\tthis_eff_load -= current_load;",
            "\t}",
            "",
            "\ttask_load = task_h_load(p);",
            "",
            "\tthis_eff_load += task_load;",
            "\tif (sched_feat(WA_BIAS))",
            "\t\tthis_eff_load *= 100;",
            "\tthis_eff_load *= capacity_of(prev_cpu);",
            "",
            "\tprev_eff_load = cpu_load(cpu_rq(prev_cpu));",
            "\tprev_eff_load -= task_load;",
            "\tif (sched_feat(WA_BIAS))",
            "\t\tprev_eff_load *= 100 + (sd->imbalance_pct - 100) / 2;",
            "\tprev_eff_load *= capacity_of(this_cpu);",
            "",
            "\t/*",
            "\t * If sync, adjust the weight of prev_eff_load such that if",
            "\t * prev_eff == this_eff that select_idle_sibling() will consider",
            "\t * stacking the wakee on top of the waker if no other CPU is",
            "\t * idle.",
            "\t */",
            "\tif (sync)",
            "\t\tprev_eff_load += 1;",
            "",
            "\treturn this_eff_load < prev_eff_load ? this_cpu : nr_cpumask_bits;",
            "}",
            "static int wake_affine(struct sched_domain *sd, struct task_struct *p,",
            "\t\t       int this_cpu, int prev_cpu, int sync)",
            "{",
            "\tint target = nr_cpumask_bits;",
            "",
            "\tif (sched_feat(WA_IDLE))",
            "\t\ttarget = wake_affine_idle(this_cpu, prev_cpu, sync);",
            "",
            "\tif (sched_feat(WA_WEIGHT) && target == nr_cpumask_bits)",
            "\t\ttarget = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);",
            "",
            "\tschedstat_inc(p->stats.nr_wakeups_affine_attempts);",
            "\tif (target != this_cpu)",
            "\t\treturn prev_cpu;",
            "",
            "\tschedstat_inc(sd->ttwu_move_affine);",
            "\tschedstat_inc(p->stats.nr_wakeups_affine);",
            "\treturn target;",
            "}",
            "static int",
            "sched_balance_find_dst_group_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)",
            "{",
            "\tunsigned long load, min_load = ULONG_MAX;",
            "\tunsigned int min_exit_latency = UINT_MAX;",
            "\tu64 latest_idle_timestamp = 0;",
            "\tint least_loaded_cpu = this_cpu;",
            "\tint shallowest_idle_cpu = -1;",
            "\tint i;",
            "",
            "\t/* Check if we have any choice: */",
            "\tif (group->group_weight == 1)",
            "\t\treturn cpumask_first(sched_group_span(group));",
            "",
            "\t/* Traverse only the allowed CPUs */",
            "\tfor_each_cpu_and(i, sched_group_span(group), p->cpus_ptr) {",
            "\t\tstruct rq *rq = cpu_rq(i);",
            "",
            "\t\tif (!sched_core_cookie_match(rq, p))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (sched_idle_cpu(i))",
            "\t\t\treturn i;",
            "",
            "\t\tif (available_idle_cpu(i)) {",
            "\t\t\tstruct cpuidle_state *idle = idle_get_state(rq);",
            "\t\t\tif (idle && idle->exit_latency < min_exit_latency) {",
            "\t\t\t\t/*",
            "\t\t\t\t * We give priority to a CPU whose idle state",
            "\t\t\t\t * has the smallest exit latency irrespective",
            "\t\t\t\t * of any idle timestamp.",
            "\t\t\t\t */",
            "\t\t\t\tmin_exit_latency = idle->exit_latency;",
            "\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;",
            "\t\t\t\tshallowest_idle_cpu = i;",
            "\t\t\t} else if ((!idle || idle->exit_latency == min_exit_latency) &&",
            "\t\t\t\t   rq->idle_stamp > latest_idle_timestamp) {",
            "\t\t\t\t/*",
            "\t\t\t\t * If equal or no active idle state, then",
            "\t\t\t\t * the most recently idled CPU might have",
            "\t\t\t\t * a warmer cache.",
            "\t\t\t\t */",
            "\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;",
            "\t\t\t\tshallowest_idle_cpu = i;",
            "\t\t\t}",
            "\t\t} else if (shallowest_idle_cpu == -1) {",
            "\t\t\tload = cpu_load(cpu_rq(i));",
            "\t\t\tif (load < min_load) {",
            "\t\t\t\tmin_load = load;",
            "\t\t\t\tleast_loaded_cpu = i;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\treturn shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu;",
            "}"
          ],
          "function_name": "wake_affine_weight, wake_affine, sched_balance_find_dst_group_cpu",
          "description": "wake_affine_weight计算任务迁移后的目标CPU负载差异，结合权重和缓存亲和性调整负载比对结果；wake_affine根据WA_IDLE/WA_WEIGHT特性选择目标CPU并统计唤醒次数；sched_balance_find_dst_group_cpu遍历调度组内CPU，优先选择空闲且具有最小退出延迟的CPU",
          "similarity": 0.4119480848312378
        },
        {
          "chunk_id": 65,
          "file_path": "kernel/sched/fair.c",
          "start_line": 11564,
          "end_line": 11837,
          "content": [
            "static int sched_balance_rq(int this_cpu, struct rq *this_rq,",
            "\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,",
            "\t\t\tint *continue_balancing)",
            "{",
            "\tint ld_moved, cur_ld_moved, active_balance = 0;",
            "\tstruct sched_domain *sd_parent = sd->parent;",
            "\tstruct sched_group *group;",
            "\tstruct rq *busiest;",
            "\tstruct rq_flags rf;",
            "\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);",
            "\tstruct lb_env env = {",
            "\t\t.sd\t\t= sd,",
            "\t\t.dst_cpu\t= this_cpu,",
            "\t\t.dst_rq\t\t= this_rq,",
            "\t\t.dst_grpmask    = group_balance_mask(sd->groups),",
            "\t\t.idle\t\t= idle,",
            "\t\t.loop_break\t= SCHED_NR_MIGRATE_BREAK,",
            "\t\t.cpus\t\t= cpus,",
            "\t\t.fbq_type\t= all,",
            "\t\t.tasks\t\t= LIST_HEAD_INIT(env.tasks),",
            "\t};",
            "",
            "\tcpumask_and(cpus, sched_domain_span(sd), cpu_active_mask);",
            "",
            "\tschedstat_inc(sd->lb_count[idle]);",
            "",
            "redo:",
            "\tif (!should_we_balance(&env)) {",
            "\t\t*continue_balancing = 0;",
            "\t\tgoto out_balanced;",
            "\t}",
            "",
            "\tgroup = sched_balance_find_src_group(&env);",
            "\tif (!group) {",
            "\t\tschedstat_inc(sd->lb_nobusyg[idle]);",
            "\t\tgoto out_balanced;",
            "\t}",
            "",
            "\tbusiest = find_busiest_queue(&env, group);",
            "\tif (!busiest) {",
            "\t\tschedstat_inc(sd->lb_nobusyq[idle]);",
            "\t\tgoto out_balanced;",
            "\t}",
            "",
            "\tWARN_ON_ONCE(busiest == env.dst_rq);",
            "",
            "\tschedstat_add(sd->lb_imbalance[idle], env.imbalance);",
            "",
            "\tenv.src_cpu = busiest->cpu;",
            "\tenv.src_rq = busiest;",
            "",
            "\tld_moved = 0;",
            "\t/* Clear this flag as soon as we find a pullable task */",
            "\tenv.flags |= LBF_ALL_PINNED;",
            "\tif (busiest->nr_running > 1) {",
            "\t\t/*",
            "\t\t * Attempt to move tasks. If sched_balance_find_src_group has found",
            "\t\t * an imbalance but busiest->nr_running <= 1, the group is",
            "\t\t * still unbalanced. ld_moved simply stays zero, so it is",
            "\t\t * correctly treated as an imbalance.",
            "\t\t */",
            "\t\tenv.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);",
            "",
            "more_balance:",
            "\t\trq_lock_irqsave(busiest, &rf);",
            "\t\tupdate_rq_clock(busiest);",
            "",
            "\t\t/*",
            "\t\t * cur_ld_moved - load moved in current iteration",
            "\t\t * ld_moved     - cumulative load moved across iterations",
            "\t\t */",
            "\t\tcur_ld_moved = detach_tasks(&env);",
            "",
            "\t\t/*",
            "\t\t * We've detached some tasks from busiest_rq. Every",
            "\t\t * task is masked \"TASK_ON_RQ_MIGRATING\", so we can safely",
            "\t\t * unlock busiest->lock, and we are able to be sure",
            "\t\t * that nobody can manipulate the tasks in parallel.",
            "\t\t * See task_rq_lock() family for the details.",
            "\t\t */",
            "",
            "\t\trq_unlock(busiest, &rf);",
            "",
            "\t\tif (cur_ld_moved) {",
            "\t\t\tattach_tasks(&env);",
            "\t\t\tld_moved += cur_ld_moved;",
            "\t\t}",
            "",
            "\t\tlocal_irq_restore(rf.flags);",
            "",
            "\t\tif (env.flags & LBF_NEED_BREAK) {",
            "\t\t\tenv.flags &= ~LBF_NEED_BREAK;",
            "\t\t\tgoto more_balance;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Revisit (affine) tasks on src_cpu that couldn't be moved to",
            "\t\t * us and move them to an alternate dst_cpu in our sched_group",
            "\t\t * where they can run. The upper limit on how many times we",
            "\t\t * iterate on same src_cpu is dependent on number of CPUs in our",
            "\t\t * sched_group.",
            "\t\t *",
            "\t\t * This changes load balance semantics a bit on who can move",
            "\t\t * load to a given_cpu. In addition to the given_cpu itself",
            "\t\t * (or a ilb_cpu acting on its behalf where given_cpu is",
            "\t\t * nohz-idle), we now have balance_cpu in a position to move",
            "\t\t * load to given_cpu. In rare situations, this may cause",
            "\t\t * conflicts (balance_cpu and given_cpu/ilb_cpu deciding",
            "\t\t * _independently_ and at _same_ time to move some load to",
            "\t\t * given_cpu) causing excess load to be moved to given_cpu.",
            "\t\t * This however should not happen so much in practice and",
            "\t\t * moreover subsequent load balance cycles should correct the",
            "\t\t * excess load moved.",
            "\t\t */",
            "\t\tif ((env.flags & LBF_DST_PINNED) && env.imbalance > 0) {",
            "",
            "\t\t\t/* Prevent to re-select dst_cpu via env's CPUs */",
            "\t\t\t__cpumask_clear_cpu(env.dst_cpu, env.cpus);",
            "",
            "\t\t\tenv.dst_rq\t = cpu_rq(env.new_dst_cpu);",
            "\t\t\tenv.dst_cpu\t = env.new_dst_cpu;",
            "\t\t\tenv.flags\t&= ~LBF_DST_PINNED;",
            "\t\t\tenv.loop\t = 0;",
            "\t\t\tenv.loop_break\t = SCHED_NR_MIGRATE_BREAK;",
            "",
            "\t\t\t/*",
            "\t\t\t * Go back to \"more_balance\" rather than \"redo\" since we",
            "\t\t\t * need to continue with same src_cpu.",
            "\t\t\t */",
            "\t\t\tgoto more_balance;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * We failed to reach balance because of affinity.",
            "\t\t */",
            "\t\tif (sd_parent) {",
            "\t\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;",
            "",
            "\t\t\tif ((env.flags & LBF_SOME_PINNED) && env.imbalance > 0)",
            "\t\t\t\t*group_imbalance = 1;",
            "\t\t}",
            "",
            "\t\t/* All tasks on this runqueue were pinned by CPU affinity */",
            "\t\tif (unlikely(env.flags & LBF_ALL_PINNED)) {",
            "\t\t\t__cpumask_clear_cpu(cpu_of(busiest), cpus);",
            "\t\t\t/*",
            "\t\t\t * Attempting to continue load balancing at the current",
            "\t\t\t * sched_domain level only makes sense if there are",
            "\t\t\t * active CPUs remaining as possible busiest CPUs to",
            "\t\t\t * pull load from which are not contained within the",
            "\t\t\t * destination group that is receiving any migrated",
            "\t\t\t * load.",
            "\t\t\t */",
            "\t\t\tif (!cpumask_subset(cpus, env.dst_grpmask)) {",
            "\t\t\t\tenv.loop = 0;",
            "\t\t\t\tenv.loop_break = SCHED_NR_MIGRATE_BREAK;",
            "\t\t\t\tgoto redo;",
            "\t\t\t}",
            "\t\t\tgoto out_all_pinned;",
            "\t\t}",
            "\t}",
            "",
            "\tif (!ld_moved) {",
            "\t\tschedstat_inc(sd->lb_failed[idle]);",
            "\t\t/*",
            "\t\t * Increment the failure counter only on periodic balance.",
            "\t\t * We do not want newidle balance, which can be very",
            "\t\t * frequent, pollute the failure counter causing",
            "\t\t * excessive cache_hot migrations and active balances.",
            "\t\t *",
            "\t\t * Similarly for migration_misfit which is not related to",
            "\t\t * load/util migration, don't pollute nr_balance_failed.",
            "\t\t */",
            "\t\tif (idle != CPU_NEWLY_IDLE &&",
            "\t\t    env.migration_type != migrate_misfit)",
            "\t\t\tsd->nr_balance_failed++;",
            "",
            "\t\tif (need_active_balance(&env)) {",
            "\t\t\tunsigned long flags;",
            "",
            "\t\t\traw_spin_rq_lock_irqsave(busiest, flags);",
            "",
            "\t\t\t/*",
            "\t\t\t * Don't kick the active_load_balance_cpu_stop,",
            "\t\t\t * if the curr task on busiest CPU can't be",
            "\t\t\t * moved to this_cpu:",
            "\t\t\t */",
            "\t\t\tif (!cpumask_test_cpu(this_cpu, busiest->curr->cpus_ptr)) {",
            "\t\t\t\traw_spin_rq_unlock_irqrestore(busiest, flags);",
            "\t\t\t\tgoto out_one_pinned;",
            "\t\t\t}",
            "",
            "\t\t\t/* Record that we found at least one task that could run on this_cpu */",
            "\t\t\tenv.flags &= ~LBF_ALL_PINNED;",
            "",
            "\t\t\t/*",
            "\t\t\t * ->active_balance synchronizes accesses to",
            "\t\t\t * ->active_balance_work.  Once set, it's cleared",
            "\t\t\t * only after active load balance is finished.",
            "\t\t\t */",
            "\t\t\tif (!busiest->active_balance) {",
            "\t\t\t\tbusiest->active_balance = 1;",
            "\t\t\t\tbusiest->push_cpu = this_cpu;",
            "\t\t\t\tactive_balance = 1;",
            "\t\t\t}",
            "",
            "\t\t\tpreempt_disable();",
            "\t\t\traw_spin_rq_unlock_irqrestore(busiest, flags);",
            "\t\t\tif (active_balance) {",
            "\t\t\t\tstop_one_cpu_nowait(cpu_of(busiest),",
            "\t\t\t\t\tactive_load_balance_cpu_stop, busiest,",
            "\t\t\t\t\t&busiest->active_balance_work);",
            "\t\t\t}",
            "\t\t\tpreempt_enable();",
            "\t\t}",
            "\t} else {",
            "\t\tsd->nr_balance_failed = 0;",
            "\t}",
            "",
            "\tif (likely(!active_balance) || need_active_balance(&env)) {",
            "\t\t/* We were unbalanced, so reset the balancing interval */",
            "\t\tsd->balance_interval = sd->min_interval;",
            "\t}",
            "",
            "\tgoto out;",
            "",
            "out_balanced:",
            "\t/*",
            "\t * We reach balance although we may have faced some affinity",
            "\t * constraints. Clear the imbalance flag only if other tasks got",
            "\t * a chance to move and fix the imbalance.",
            "\t */",
            "\tif (sd_parent && !(env.flags & LBF_ALL_PINNED)) {",
            "\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;",
            "",
            "\t\tif (*group_imbalance)",
            "\t\t\t*group_imbalance = 0;",
            "\t}",
            "",
            "out_all_pinned:",
            "\t/*",
            "\t * We reach balance because all tasks are pinned at this level so",
            "\t * we can't migrate them. Let the imbalance flag set so parent level",
            "\t * can try to migrate them.",
            "\t */",
            "\tschedstat_inc(sd->lb_balanced[idle]);",
            "",
            "\tsd->nr_balance_failed = 0;",
            "",
            "out_one_pinned:",
            "\tld_moved = 0;",
            "",
            "\t/*",
            "\t * sched_balance_newidle() disregards balance intervals, so we could",
            "\t * repeatedly reach this code, which would lead to balance_interval",
            "\t * skyrocketing in a short amount of time. Skip the balance_interval",
            "\t * increase logic to avoid that.",
            "\t *",
            "\t * Similarly misfit migration which is not necessarily an indication of",
            "\t * the system being busy and requires lb to backoff to let it settle",
            "\t * down.",
            "\t */",
            "\tif (env.idle == CPU_NEWLY_IDLE ||",
            "\t    env.migration_type == migrate_misfit)",
            "\t\tgoto out;",
            "",
            "\t/* tune up the balancing interval */",
            "\tif ((env.flags & LBF_ALL_PINNED &&",
            "\t     sd->balance_interval < MAX_PINNED_INTERVAL) ||",
            "\t    sd->balance_interval < sd->max_interval)",
            "\t\tsd->balance_interval *= 2;",
            "out:",
            "\treturn ld_moved;",
            "}"
          ],
          "function_name": "sched_balance_rq",
          "description": "实现负载平衡逻辑，尝试从繁忙运行队列迁移任务到当前CPU，处理迁移失败场景，更新统计信息并调整平衡间隔",
          "similarity": 0.41021299362182617
        },
        {
          "chunk_id": 66,
          "file_path": "kernel/sched/fair.c",
          "start_line": 11839,
          "end_line": 11950,
          "content": [
            "static inline unsigned long",
            "get_sd_balance_interval(struct sched_domain *sd, int cpu_busy)",
            "{",
            "\tunsigned long interval = sd->balance_interval;",
            "",
            "\tif (cpu_busy)",
            "\t\tinterval *= sd->busy_factor;",
            "",
            "\t/* scale ms to jiffies */",
            "\tinterval = msecs_to_jiffies(interval);",
            "",
            "\t/*",
            "\t * Reduce likelihood of busy balancing at higher domains racing with",
            "\t * balancing at lower domains by preventing their balancing periods",
            "\t * from being multiples of each other.",
            "\t */",
            "\tif (cpu_busy)",
            "\t\tinterval -= 1;",
            "",
            "\tinterval = clamp(interval, 1UL, max_load_balance_interval);",
            "",
            "\treturn interval;",
            "}",
            "static inline void",
            "update_next_balance(struct sched_domain *sd, unsigned long *next_balance)",
            "{",
            "\tunsigned long interval, next;",
            "",
            "\t/* used by idle balance, so cpu_busy = 0 */",
            "\tinterval = get_sd_balance_interval(sd, 0);",
            "\tnext = sd->last_balance + interval;",
            "",
            "\tif (time_after(*next_balance, next))",
            "\t\t*next_balance = next;",
            "}",
            "static int active_load_balance_cpu_stop(void *data)",
            "{",
            "\tstruct rq *busiest_rq = data;",
            "\tint busiest_cpu = cpu_of(busiest_rq);",
            "\tint target_cpu = busiest_rq->push_cpu;",
            "\tstruct rq *target_rq = cpu_rq(target_cpu);",
            "\tstruct sched_domain *sd;",
            "\tstruct task_struct *p = NULL;",
            "\tstruct rq_flags rf;",
            "",
            "\trq_lock_irq(busiest_rq, &rf);",
            "\t/*",
            "\t * Between queueing the stop-work and running it is a hole in which",
            "\t * CPUs can become inactive. We should not move tasks from or to",
            "\t * inactive CPUs.",
            "\t */",
            "\tif (!cpu_active(busiest_cpu) || !cpu_active(target_cpu))",
            "\t\tgoto out_unlock;",
            "",
            "\t/* Make sure the requested CPU hasn't gone down in the meantime: */",
            "\tif (unlikely(busiest_cpu != smp_processor_id() ||",
            "\t\t     !busiest_rq->active_balance))",
            "\t\tgoto out_unlock;",
            "",
            "\t/* Is there any task to move? */",
            "\tif (busiest_rq->nr_running <= 1)",
            "\t\tgoto out_unlock;",
            "",
            "\t/*",
            "\t * This condition is \"impossible\", if it occurs",
            "\t * we need to fix it. Originally reported by",
            "\t * Bjorn Helgaas on a 128-CPU setup.",
            "\t */",
            "\tWARN_ON_ONCE(busiest_rq == target_rq);",
            "",
            "\t/* Search for an sd spanning us and the target CPU. */",
            "\trcu_read_lock();",
            "\tfor_each_domain(target_cpu, sd) {",
            "\t\tif (cpumask_test_cpu(busiest_cpu, sched_domain_span(sd)))",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tif (likely(sd)) {",
            "\t\tstruct lb_env env = {",
            "\t\t\t.sd\t\t= sd,",
            "\t\t\t.dst_cpu\t= target_cpu,",
            "\t\t\t.dst_rq\t\t= target_rq,",
            "\t\t\t.src_cpu\t= busiest_rq->cpu,",
            "\t\t\t.src_rq\t\t= busiest_rq,",
            "\t\t\t.idle\t\t= CPU_IDLE,",
            "\t\t\t.flags\t\t= LBF_ACTIVE_LB,",
            "\t\t};",
            "",
            "\t\tschedstat_inc(sd->alb_count);",
            "\t\tupdate_rq_clock(busiest_rq);",
            "",
            "\t\tp = detach_one_task(&env);",
            "\t\tif (p) {",
            "\t\t\tschedstat_inc(sd->alb_pushed);",
            "\t\t\t/* Active balancing done, reset the failure counter. */",
            "\t\t\tsd->nr_balance_failed = 0;",
            "\t\t} else {",
            "\t\t\tschedstat_inc(sd->alb_failed);",
            "\t\t}",
            "\t}",
            "\trcu_read_unlock();",
            "out_unlock:",
            "\tbusiest_rq->active_balance = 0;",
            "\trq_unlock(busiest_rq, &rf);",
            "",
            "\tif (p)",
            "\t\tattach_one_task(target_rq, p);",
            "",
            "\tlocal_irq_enable();",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_sd_balance_interval, update_next_balance, active_load_balance_cpu_stop",
          "description": "计算调度域平衡间隔，更新下次平衡时间戳，处理主动负载平衡的迁移任务终止逻辑",
          "similarity": 0.40562546253204346
        }
      ]
    },
    {
      "source_file": "kernel/bpf/bpf_iter.c",
      "md_summary": "> 自动生成时间: 2025-10-25 11:58:22\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\bpf_iter.c`\n\n---\n\n# bpf/bpf_iter.c 技术文档\n\n## 1. 文件概述\n\n`bpf_iter.c` 是 Linux 内核中 BPF（Berkeley Packet Filter）子系统的一部分，实现了 **BPF 迭代器（BPF Iterator）** 的核心机制。BPF 迭代器是一种特殊的 BPF 程序类型，允许用户空间通过文件接口（如 `/sys/fs/bpf/` 下的文件）安全、高效地遍历内核中的数据结构（如任务、映射、网络连接等），而无需引入新的系统调用或暴露原始内核指针。\n\n该文件负责：\n- 管理 BPF 迭代器目标（target）的注册与注销\n- 实现自定义的 `seq_file` 读取逻辑（`bpf_seq_read`）\n- 管理迭代器会话的私有数据和生命周期\n- 提供文件操作接口（`file_operations`）供用户空间读取\n\n## 2. 核心功能\n\n### 主要数据结构\n\n| 结构体 | 说明 |\n|--------|------|\n| `struct bpf_iter_target_info` | 表示一个 BPF 迭代器目标的注册信息，包含 `bpf_iter_reg` 和缓存的 BTF ID |\n| `struct bpf_iter_link` | 继承自 `bpf_link`，用于将 BPF 程序与特定迭代器目标关联 |\n| `struct bpf_iter_priv_data` | `seq_file` 的私有数据，包含目标信息、BPF 程序、会话 ID、序列号等状态 |\n\n### 主要函数\n\n| 函数 | 功能 |\n|------|------|\n| `bpf_iter_reg_target()` | 注册一个新的 BPF 迭代器目标类型 |\n| `bpf_iter_unreg_target()` | 注销已注册的 BPF 迭代器目标 |\n| `bpf_iter_prog_supported()` | 检查 BPF 程序是否为有效的迭代器程序（基于函数名前缀和 BTF ID 匹配） |\n| `bpf_seq_read()` | 自定义的 `seq_file` 读取实现，支持 BPF 程序在 `start/next/show/stop` 回调中执行 |\n| `iter_open()` / `iter_release()` | 文件打开/释放回调，初始化和清理迭代器会话 |\n| `bpf_iter_inc_seq_num()` / `bpf_iter_dec_seq_num()` | 管理当前迭代对象的序列号（用于跳过对象时回退） |\n| `bpf_iter_done_stop()` | 标记迭代已正常结束 |\n\n### 全局变量\n\n- `targets`：已注册迭代器目标的链表\n- `session_id`：全局递增的会话 ID，用于区分不同迭代会话\n- `bpf_iter_fops`：提供给 VFS 的文件操作接口\n\n## 3. 关键实现\n\n### BPF 迭代器工作流程\n1. **注册**：内核子系统（如 `task_iter`、`map_iter`）通过 `bpf_iter_reg_target()` 注册其迭代器实现（`bpf_iter_reg`）。\n2. **程序加载**：用户加载 BPF 程序时，若其 `attach_func_name` 以 `bpf_iter_` 开头，则调用 `bpf_iter_prog_supported()` 验证并绑定到对应目标。\n3. **文件打开**：用户打开 BPF 迭代器链接创建的文件时，`iter_open()` 调用 `prepare_seq_file()` 初始化 `seq_file`。\n4. **数据读取**：`bpf_seq_read()` 驱动迭代过程：\n   - 调用 `seq->op->start()` 获取首个对象\n   - 循环调用 `next()` 和 `show()`，其中 `show()` 可能执行 BPF 程序生成输出\n   - 支持在 `show()` 中返回 `>0` 跳过当前对象（通过 `bpf_iter_dec_seq_num()` 回退序列号）\n   - 限制最大迭代对象数（`MAX_ITER_OBJECTS = 1,000,000`）防止无限循环\n   - 支持可抢占目标（`BPF_ITER_RESCHED` 特性）在循环中调用 `cond_resched()`\n5. **结束**：`stop()` 回调被调用，可能再次执行 BPF 程序进行清理。\n\n### 安全与健壮性机制\n- **溢出保护**：检查 `seq_has_overflowed()` 防止缓冲区溢出，返回 `-E2BIG`\n- **对象数量限制**：防止恶意程序导致内核长时间占用 CPU\n- **序列号管理**：精确跟踪当前迭代位置，支持对象跳过\n- **会话隔离**：每个文件打开对应独立的 `session_id` 和私有数据\n\n### BTF ID 缓存优化\n- 首次匹配目标时，将程序的 `attach_btf_id` 缓存到 `tinfo->btf_id`\n- 后续程序可直接通过 BTF ID 快速匹配，避免字符串比较\n\n## 4. 依赖关系\n\n- **BPF 核心**：依赖 `linux/bpf.h`、`bpf_link` 机制和 BPF 程序管理（`bpf_prog_put`）\n- **VFS/seq_file**：基于 `linux/fs.h` 和 `seq_file` 框架实现迭代输出\n- **内存管理**：使用 `kvmalloc` 分配大缓冲区（`PAGE_SIZE << 3`）\n- **RCU/锁机制**：使用 `mutex`（`targets_mutex`, `link_mutex`）保护全局链表\n- **BTF（BPF Type Format）**：通过 `attach_btf_id` 和函数名匹配程序与目标\n- **调度器**：支持可抢占目标时调用 `cond_resched()`\n\n## 5. 使用场景\n\n1. **内核数据遍历**：用户空间安全读取内核内部数据结构，例如：\n   - 遍历所有进程（`bpf_iter_task`）\n   - 遍历 BPF 映射内容（`bpf_iter_map_elem`）\n   - 遍历网络连接（`bpf_iter_tcp`）\n2. **调试与监控**：替代 `/proc` 或 `debugfs` 接口，提供更灵活、可编程的数据导出\n3. **性能分析**：高效收集内核状态快照，避免频繁系统调用开销\n4. **安全审计**：以只读方式检查内核对象，无需暴露原始指针或增加系统调用\n\n> **注**：该文件被截断，实际 `bpf_iter_prog_supported()` 函数未完整显示，但核心逻辑已涵盖。完整实现还需处理 `ctx_arg_info` 等上下文参数配置。",
      "similarity": 0.42245984077453613,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/bpf_iter.c",
          "start_line": 44,
          "end_line": 226,
          "content": [
            "static void bpf_iter_inc_seq_num(struct seq_file *seq)",
            "{",
            "\tstruct bpf_iter_priv_data *iter_priv;",
            "",
            "\titer_priv = container_of(seq->private, struct bpf_iter_priv_data,",
            "\t\t\t\t target_private);",
            "\titer_priv->seq_num++;",
            "}",
            "static void bpf_iter_dec_seq_num(struct seq_file *seq)",
            "{",
            "\tstruct bpf_iter_priv_data *iter_priv;",
            "",
            "\titer_priv = container_of(seq->private, struct bpf_iter_priv_data,",
            "\t\t\t\t target_private);",
            "\titer_priv->seq_num--;",
            "}",
            "static void bpf_iter_done_stop(struct seq_file *seq)",
            "{",
            "\tstruct bpf_iter_priv_data *iter_priv;",
            "",
            "\titer_priv = container_of(seq->private, struct bpf_iter_priv_data,",
            "\t\t\t\t target_private);",
            "\titer_priv->done_stop = true;",
            "}",
            "static inline bool bpf_iter_target_support_resched(const struct bpf_iter_target_info *tinfo)",
            "{",
            "\treturn tinfo->reg_info->feature & BPF_ITER_RESCHED;",
            "}",
            "static bool bpf_iter_support_resched(struct seq_file *seq)",
            "{",
            "\tstruct bpf_iter_priv_data *iter_priv;",
            "",
            "\titer_priv = container_of(seq->private, struct bpf_iter_priv_data,",
            "\t\t\t\t target_private);",
            "\treturn bpf_iter_target_support_resched(iter_priv->tinfo);",
            "}",
            "static ssize_t bpf_seq_read(struct file *file, char __user *buf, size_t size,",
            "\t\t\t    loff_t *ppos)",
            "{",
            "\tstruct seq_file *seq = file->private_data;",
            "\tsize_t n, offs, copied = 0;",
            "\tint err = 0, num_objs = 0;",
            "\tbool can_resched;",
            "\tvoid *p;",
            "",
            "\tmutex_lock(&seq->lock);",
            "",
            "\tif (!seq->buf) {",
            "\t\tseq->size = PAGE_SIZE << 3;",
            "\t\tseq->buf = kvmalloc(seq->size, GFP_KERNEL);",
            "\t\tif (!seq->buf) {",
            "\t\t\terr = -ENOMEM;",
            "\t\t\tgoto done;",
            "\t\t}",
            "\t}",
            "",
            "\tif (seq->count) {",
            "\t\tn = min(seq->count, size);",
            "\t\terr = copy_to_user(buf, seq->buf + seq->from, n);",
            "\t\tif (err) {",
            "\t\t\terr = -EFAULT;",
            "\t\t\tgoto done;",
            "\t\t}",
            "\t\tseq->count -= n;",
            "\t\tseq->from += n;",
            "\t\tcopied = n;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\tseq->from = 0;",
            "\tp = seq->op->start(seq, &seq->index);",
            "\tif (!p)",
            "\t\tgoto stop;",
            "\tif (IS_ERR(p)) {",
            "\t\terr = PTR_ERR(p);",
            "\t\tseq->op->stop(seq, p);",
            "\t\tseq->count = 0;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\terr = seq->op->show(seq, p);",
            "\tif (err > 0) {",
            "\t\t/* object is skipped, decrease seq_num, so next",
            "\t\t * valid object can reuse the same seq_num.",
            "\t\t */",
            "\t\tbpf_iter_dec_seq_num(seq);",
            "\t\tseq->count = 0;",
            "\t} else if (err < 0 || seq_has_overflowed(seq)) {",
            "\t\tif (!err)",
            "\t\t\terr = -E2BIG;",
            "\t\tseq->op->stop(seq, p);",
            "\t\tseq->count = 0;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\tcan_resched = bpf_iter_support_resched(seq);",
            "\twhile (1) {",
            "\t\tloff_t pos = seq->index;",
            "",
            "\t\tnum_objs++;",
            "\t\toffs = seq->count;",
            "\t\tp = seq->op->next(seq, p, &seq->index);",
            "\t\tif (pos == seq->index) {",
            "\t\t\tpr_info_ratelimited(\"buggy seq_file .next function %ps \"",
            "\t\t\t\t\"did not updated position index\\n\",",
            "\t\t\t\tseq->op->next);",
            "\t\t\tseq->index++;",
            "\t\t}",
            "",
            "\t\tif (IS_ERR_OR_NULL(p))",
            "\t\t\tbreak;",
            "",
            "\t\t/* got a valid next object, increase seq_num */",
            "\t\tbpf_iter_inc_seq_num(seq);",
            "",
            "\t\tif (seq->count >= size)",
            "\t\t\tbreak;",
            "",
            "\t\tif (num_objs >= MAX_ITER_OBJECTS) {",
            "\t\t\tif (offs == 0) {",
            "\t\t\t\terr = -EAGAIN;",
            "\t\t\t\tseq->op->stop(seq, p);",
            "\t\t\t\tgoto done;",
            "\t\t\t}",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\terr = seq->op->show(seq, p);",
            "\t\tif (err > 0) {",
            "\t\t\tbpf_iter_dec_seq_num(seq);",
            "\t\t\tseq->count = offs;",
            "\t\t} else if (err < 0 || seq_has_overflowed(seq)) {",
            "\t\t\tseq->count = offs;",
            "\t\t\tif (offs == 0) {",
            "\t\t\t\tif (!err)",
            "\t\t\t\t\terr = -E2BIG;",
            "\t\t\t\tseq->op->stop(seq, p);",
            "\t\t\t\tgoto done;",
            "\t\t\t}",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tif (can_resched)",
            "\t\t\tcond_resched();",
            "\t}",
            "stop:",
            "\toffs = seq->count;",
            "\tif (IS_ERR(p)) {",
            "\t\tseq->op->stop(seq, NULL);",
            "\t\terr = PTR_ERR(p);",
            "\t\tgoto done;",
            "\t}",
            "\t/* bpf program called if !p */",
            "\tseq->op->stop(seq, p);",
            "\tif (!p) {",
            "\t\tif (!seq_has_overflowed(seq)) {",
            "\t\t\tbpf_iter_done_stop(seq);",
            "\t\t} else {",
            "\t\t\tseq->count = offs;",
            "\t\t\tif (offs == 0) {",
            "\t\t\t\terr = -E2BIG;",
            "\t\t\t\tgoto done;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\tn = min(seq->count, size);",
            "\terr = copy_to_user(buf, seq->buf, n);",
            "\tif (err) {",
            "\t\terr = -EFAULT;",
            "\t\tgoto done;",
            "\t}",
            "\tcopied = n;",
            "\tseq->count -= n;",
            "\tseq->from = n;",
            "done:",
            "\tif (!copied)",
            "\t\tcopied = err;",
            "\telse",
            "\t\t*ppos += copied;",
            "\tmutex_unlock(&seq->lock);",
            "\treturn copied;",
            "}"
          ],
          "function_name": "bpf_iter_inc_seq_num, bpf_iter_dec_seq_num, bpf_iter_done_stop, bpf_iter_target_support_resched, bpf_iter_support_resched, bpf_seq_read",
          "description": "实现BPF迭代器的序列文件操作辅助函数，包含计数器增减、支持重新调度判断和seq_read方法，通过维护seq_num和done_stop标志控制迭代流程并处理对象展示。",
          "similarity": 0.39853715896606445
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/bpf/bpf_iter.c",
          "start_line": 786,
          "end_line": 821,
          "content": [
            "__bpf_kfunc int bpf_iter_num_new(struct bpf_iter_num *it, int start, int end)",
            "{",
            "\tstruct bpf_iter_num_kern *s = (void *)it;",
            "",
            "\tBUILD_BUG_ON(sizeof(struct bpf_iter_num_kern) != sizeof(struct bpf_iter_num));",
            "\tBUILD_BUG_ON(__alignof__(struct bpf_iter_num_kern) != __alignof__(struct bpf_iter_num));",
            "",
            "\t/* start == end is legit, it's an empty range and we'll just get NULL",
            "\t * on first (and any subsequent) bpf_iter_num_next() call",
            "\t */",
            "\tif (start > end) {",
            "\t\ts->cur = s->end = 0;",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\t/* avoid overflows, e.g., if start == INT_MIN and end == INT_MAX */",
            "\tif ((s64)end - (s64)start > BPF_MAX_LOOPS) {",
            "\t\ts->cur = s->end = 0;",
            "\t\treturn -E2BIG;",
            "\t}",
            "",
            "\t/* user will call bpf_iter_num_next() first,",
            "\t * which will set s->cur to exactly start value;",
            "\t * underflow shouldn't matter",
            "\t */",
            "\ts->cur = start - 1;",
            "\ts->end = end;",
            "",
            "\treturn 0;",
            "}",
            "__bpf_kfunc void bpf_iter_num_destroy(struct bpf_iter_num *it)",
            "{",
            "\tstruct bpf_iter_num_kern *s = (void *)it;",
            "",
            "\ts->cur = s->end = 0;",
            "}"
          ],
          "function_name": "bpf_iter_num_new, bpf_iter_num_destroy",
          "description": "该代码段实现了BPF数值范围迭代器的创建与销毁功能。`bpf_iter_num_new`用于初始化一个指定起始/终止值的迭代器对象并进行边界检查，`bpf_iter_num_destroy`负责清空迭代器状态。因上下文不完整，未展示迭代器实际遍历逻辑及配套API。",
          "similarity": 0.3758716285228729
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/bpf/bpf_iter.c",
          "start_line": 577,
          "end_line": 699,
          "content": [
            "static void init_seq_meta(struct bpf_iter_priv_data *priv_data,",
            "\t\t\t  struct bpf_iter_target_info *tinfo,",
            "\t\t\t  const struct bpf_iter_seq_info *seq_info,",
            "\t\t\t  struct bpf_prog *prog)",
            "{",
            "\tpriv_data->tinfo = tinfo;",
            "\tpriv_data->seq_info = seq_info;",
            "\tpriv_data->prog = prog;",
            "\tpriv_data->session_id = atomic64_inc_return(&session_id);",
            "\tpriv_data->seq_num = 0;",
            "\tpriv_data->done_stop = false;",
            "}",
            "static int prepare_seq_file(struct file *file, struct bpf_iter_link *link,",
            "\t\t\t    const struct bpf_iter_seq_info *seq_info)",
            "{",
            "\tstruct bpf_iter_priv_data *priv_data;",
            "\tstruct bpf_iter_target_info *tinfo;",
            "\tstruct bpf_prog *prog;",
            "\tu32 total_priv_dsize;",
            "\tstruct seq_file *seq;",
            "\tint err = 0;",
            "",
            "\tmutex_lock(&link_mutex);",
            "\tprog = link->link.prog;",
            "\tbpf_prog_inc(prog);",
            "\tmutex_unlock(&link_mutex);",
            "",
            "\ttinfo = link->tinfo;",
            "\ttotal_priv_dsize = offsetof(struct bpf_iter_priv_data, target_private) +",
            "\t\t\t   seq_info->seq_priv_size;",
            "\tpriv_data = __seq_open_private(file, seq_info->seq_ops,",
            "\t\t\t\t       total_priv_dsize);",
            "\tif (!priv_data) {",
            "\t\terr = -ENOMEM;",
            "\t\tgoto release_prog;",
            "\t}",
            "",
            "\tif (seq_info->init_seq_private) {",
            "\t\terr = seq_info->init_seq_private(priv_data->target_private, &link->aux);",
            "\t\tif (err)",
            "\t\t\tgoto release_seq_file;",
            "\t}",
            "",
            "\tinit_seq_meta(priv_data, tinfo, seq_info, prog);",
            "\tseq = file->private_data;",
            "\tseq->private = priv_data->target_private;",
            "",
            "\treturn 0;",
            "",
            "release_seq_file:",
            "\tseq_release_private(file->f_inode, file);",
            "\tfile->private_data = NULL;",
            "release_prog:",
            "\tbpf_prog_put(prog);",
            "\treturn err;",
            "}",
            "int bpf_iter_new_fd(struct bpf_link *link)",
            "{",
            "\tstruct bpf_iter_link *iter_link;",
            "\tstruct file *file;",
            "\tunsigned int flags;",
            "\tint err, fd;",
            "",
            "\tif (link->ops != &bpf_iter_link_lops)",
            "\t\treturn -EINVAL;",
            "",
            "\tflags = O_RDONLY | O_CLOEXEC;",
            "\tfd = get_unused_fd_flags(flags);",
            "\tif (fd < 0)",
            "\t\treturn fd;",
            "",
            "\tfile = anon_inode_getfile(\"bpf_iter\", &bpf_iter_fops, NULL, flags);",
            "\tif (IS_ERR(file)) {",
            "\t\terr = PTR_ERR(file);",
            "\t\tgoto free_fd;",
            "\t}",
            "",
            "\titer_link = container_of(link, struct bpf_iter_link, link);",
            "\terr = prepare_seq_file(file, iter_link, __get_seq_info(iter_link));",
            "\tif (err)",
            "\t\tgoto free_file;",
            "",
            "\tfd_install(fd, file);",
            "\treturn fd;",
            "",
            "free_file:",
            "\tfput(file);",
            "free_fd:",
            "\tput_unused_fd(fd);",
            "\treturn err;",
            "}",
            "int bpf_iter_run_prog(struct bpf_prog *prog, void *ctx)",
            "{",
            "\tstruct bpf_run_ctx run_ctx, *old_run_ctx;",
            "\tint ret;",
            "",
            "\tif (prog->sleepable) {",
            "\t\trcu_read_lock_trace();",
            "\t\tmigrate_disable();",
            "\t\tmight_fault();",
            "\t\told_run_ctx = bpf_set_run_ctx(&run_ctx);",
            "\t\tret = bpf_prog_run(prog, ctx);",
            "\t\tbpf_reset_run_ctx(old_run_ctx);",
            "\t\tmigrate_enable();",
            "\t\trcu_read_unlock_trace();",
            "\t} else {",
            "\t\trcu_read_lock();",
            "\t\tmigrate_disable();",
            "\t\told_run_ctx = bpf_set_run_ctx(&run_ctx);",
            "\t\tret = bpf_prog_run(prog, ctx);",
            "\t\tbpf_reset_run_ctx(old_run_ctx);",
            "\t\tmigrate_enable();",
            "\t\trcu_read_unlock();",
            "\t}",
            "",
            "\t/* bpf program can only return 0 or 1:",
            "\t *  0 : okay",
            "\t *  1 : retry the same object",
            "\t * The bpf_iter_run_prog() return value",
            "\t * will be seq_ops->show() return value.",
            "\t */",
            "\treturn ret == 0 ? 0 : -EAGAIN;",
            "}"
          ],
          "function_name": "init_seq_meta, prepare_seq_file, bpf_iter_new_fd, bpf_iter_run_prog",
          "description": "实现序列文件元数据初始化、迭代器文件创建和BPF程序执行逻辑，通过prepare_seq_file准备序列上下文，bpf_iter_run_prog执行程序并根据返回值控制迭代行为。",
          "similarity": 0.362642377614975
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/bpf_iter.c",
          "start_line": 256,
          "end_line": 360,
          "content": [
            "static int iter_open(struct inode *inode, struct file *file)",
            "{",
            "\tstruct bpf_iter_link *link = inode->i_private;",
            "",
            "\treturn prepare_seq_file(file, link, __get_seq_info(link));",
            "}",
            "static int iter_release(struct inode *inode, struct file *file)",
            "{",
            "\tstruct bpf_iter_priv_data *iter_priv;",
            "\tstruct seq_file *seq;",
            "",
            "\tseq = file->private_data;",
            "\tif (!seq)",
            "\t\treturn 0;",
            "",
            "\titer_priv = container_of(seq->private, struct bpf_iter_priv_data,",
            "\t\t\t\t target_private);",
            "",
            "\tif (iter_priv->seq_info->fini_seq_private)",
            "\t\titer_priv->seq_info->fini_seq_private(seq->private);",
            "",
            "\tbpf_prog_put(iter_priv->prog);",
            "\tseq->private = iter_priv;",
            "",
            "\treturn seq_release_private(inode, file);",
            "}",
            "int bpf_iter_reg_target(const struct bpf_iter_reg *reg_info)",
            "{",
            "\tstruct bpf_iter_target_info *tinfo;",
            "",
            "\ttinfo = kzalloc(sizeof(*tinfo), GFP_KERNEL);",
            "\tif (!tinfo)",
            "\t\treturn -ENOMEM;",
            "",
            "\ttinfo->reg_info = reg_info;",
            "\tINIT_LIST_HEAD(&tinfo->list);",
            "",
            "\tmutex_lock(&targets_mutex);",
            "\tlist_add(&tinfo->list, &targets);",
            "\tmutex_unlock(&targets_mutex);",
            "",
            "\treturn 0;",
            "}",
            "void bpf_iter_unreg_target(const struct bpf_iter_reg *reg_info)",
            "{",
            "\tstruct bpf_iter_target_info *tinfo;",
            "\tbool found = false;",
            "",
            "\tmutex_lock(&targets_mutex);",
            "\tlist_for_each_entry(tinfo, &targets, list) {",
            "\t\tif (reg_info == tinfo->reg_info) {",
            "\t\t\tlist_del(&tinfo->list);",
            "\t\t\tkfree(tinfo);",
            "\t\t\tfound = true;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&targets_mutex);",
            "",
            "\tWARN_ON(found == false);",
            "}",
            "static void cache_btf_id(struct bpf_iter_target_info *tinfo,",
            "\t\t\t struct bpf_prog *prog)",
            "{",
            "\ttinfo->btf_id = prog->aux->attach_btf_id;",
            "}",
            "int bpf_iter_prog_supported(struct bpf_prog *prog)",
            "{",
            "\tconst char *attach_fname = prog->aux->attach_func_name;",
            "\tstruct bpf_iter_target_info *tinfo = NULL, *iter;",
            "\tu32 prog_btf_id = prog->aux->attach_btf_id;",
            "\tconst char *prefix = BPF_ITER_FUNC_PREFIX;",
            "\tint prefix_len = strlen(prefix);",
            "",
            "\tif (strncmp(attach_fname, prefix, prefix_len))",
            "\t\treturn -EINVAL;",
            "",
            "\tmutex_lock(&targets_mutex);",
            "\tlist_for_each_entry(iter, &targets, list) {",
            "\t\tif (iter->btf_id && iter->btf_id == prog_btf_id) {",
            "\t\t\ttinfo = iter;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tif (!strcmp(attach_fname + prefix_len, iter->reg_info->target)) {",
            "\t\t\tcache_btf_id(iter, prog);",
            "\t\t\ttinfo = iter;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&targets_mutex);",
            "",
            "\tif (!tinfo)",
            "\t\treturn -EINVAL;",
            "",
            "\treturn bpf_prog_ctx_arg_info_init(prog, tinfo->reg_info->ctx_arg_info,",
            "\t\t\t\t\t  tinfo->reg_info->ctx_arg_info_size);",
            "}",
            "static void bpf_iter_link_release(struct bpf_link *link)",
            "{",
            "\tstruct bpf_iter_link *iter_link =",
            "\t\tcontainer_of(link, struct bpf_iter_link, link);",
            "",
            "\tif (iter_link->tinfo->reg_info->detach_target)",
            "\t\titer_link->tinfo->reg_info->detach_target(&iter_link->aux);",
            "}"
          ],
          "function_name": "iter_open, iter_release, bpf_iter_reg_target, bpf_iter_unreg_target, cache_btf_id, bpf_iter_prog_supported, bpf_iter_link_release",
          "description": "提供BPF迭代器目标注册/注销接口，实现BTF ID缓存、程序兼容性校验及链接释放逻辑，用于管理迭代器目标信息和清理资源。",
          "similarity": 0.33451807498931885
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/bpf_iter.c",
          "start_line": 1,
          "end_line": 43,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2020 Facebook */",
            "",
            "#include <linux/fs.h>",
            "#include <linux/anon_inodes.h>",
            "#include <linux/filter.h>",
            "#include <linux/bpf.h>",
            "#include <linux/rcupdate_trace.h>",
            "",
            "struct bpf_iter_target_info {",
            "\tstruct list_head list;",
            "\tconst struct bpf_iter_reg *reg_info;",
            "\tu32 btf_id;\t/* cached value */",
            "};",
            "",
            "struct bpf_iter_link {",
            "\tstruct bpf_link link;",
            "\tstruct bpf_iter_aux_info aux;",
            "\tstruct bpf_iter_target_info *tinfo;",
            "};",
            "",
            "struct bpf_iter_priv_data {",
            "\tstruct bpf_iter_target_info *tinfo;",
            "\tconst struct bpf_iter_seq_info *seq_info;",
            "\tstruct bpf_prog *prog;",
            "\tu64 session_id;",
            "\tu64 seq_num;",
            "\tbool done_stop;",
            "\tu8 target_private[] __aligned(8);",
            "};",
            "",
            "static struct list_head targets = LIST_HEAD_INIT(targets);",
            "static DEFINE_MUTEX(targets_mutex);",
            "",
            "/* protect bpf_iter_link changes */",
            "static DEFINE_MUTEX(link_mutex);",
            "",
            "/* incremented on every opened seq_file */",
            "static atomic64_t session_id;",
            "",
            "static int prepare_seq_file(struct file *file, struct bpf_iter_link *link,",
            "\t\t\t    const struct bpf_iter_seq_info *seq_info);",
            ""
          ],
          "function_name": null,
          "description": "声明BPF迭代器相关结构体及全局变量，定义用于管理迭代器目标信息、链接和私有数据的结构体，并初始化保护目标列表的互斥锁和会话ID原子变量。",
          "similarity": 0.30171602964401245
        }
      ]
    },
    {
      "source_file": "kernel/locking/semaphore.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:52:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\semaphore.c`\n\n---\n\n# `locking/semaphore.c` 技术文档\n\n## 1. 文件概述\n\n`locking/semaphore.c` 实现了 Linux 内核中的**计数信号量（counting semaphore）**机制。计数信号量允许多个任务（最多为初始计数值）同时持有该锁，当计数值耗尽时，后续请求者将被阻塞，直到有其他任务释放信号量。与互斥锁（mutex）不同，信号量支持更灵活的并发控制，适用于资源池、限流等场景。该文件提供了多种获取和释放信号量的接口，包括可中断、可超时、不可中断等变体，并支持在中断上下文中调用部分函数。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `down(struct semaphore *sem)` | 不可中断地获取信号量，若不可用则睡眠。**已弃用**，建议使用可中断版本。 |\n| `down_interruptible(struct semaphore *sem)` | 可被普通信号中断的获取操作，成功返回 0，被信号中断返回 `-EINTR`。 |\n| `down_killable(struct semaphore *sem)` | 可被致命信号（fatal signal）中断的获取操作，返回值同上。 |\n| `down_trylock(struct semaphore *sem)` | 非阻塞尝试获取信号量，成功返回 0，失败返回 1（**注意返回值与 mutex/spinlock 相反**）。 |\n| `down_timeout(struct semaphore *sem, long timeout)` | 带超时的获取操作，超时返回 `-ETIME`，成功返回 0。 |\n| `up(struct semaphore *sem)` | 释放信号量，可由任意上下文（包括中断）调用，唤醒等待队列中的任务。 |\n\n### 静态辅助函数\n\n- `__down*()` 系列：处理信号量争用时的阻塞逻辑。\n- `__up()`：在有等待者时执行唤醒逻辑。\n- `___down_common()`：通用的阻塞等待实现，支持不同睡眠状态和超时。\n- `__sem_acquire()`：原子减少计数并记录持有者（用于 hung task 检测）。\n\n### 数据结构\n\n- `struct semaphore`（定义在 `<linux/semaphore.h>`）：\n  - `count`：当前可用资源数（>0 表示可立即获取）。\n  - `wait_list`：等待该信号量的任务链表。\n  - `lock`：保护上述成员的原始自旋锁（`raw_spinlock_t`）。\n  - `last_holder`（条件编译）：记录最后持有者，用于 `CONFIG_DETECT_HUNG_TASK_BLOCKER`。\n\n- `struct semaphore_waiter`：\n  - 用于将任务加入等待队列，包含任务指针和唤醒标志（`up`）。\n\n## 3. 关键实现\n\n### 中断安全与自旋锁\n- 所有对外接口（包括 `down*` 和 `up`）均使用 `raw_spin_lock_irqsave()` 获取自旋锁，确保在中断上下文安全。\n- 即使 `down()` 等函数通常在进程上下文调用，也使用 `irqsave` 变体，因为内核某些部分依赖在中断上下文成功调用 `down()`（当确定信号量可用时）。\n\n### 计数语义\n- `sem->count` 表示**还可被获取的次数**。初始值由 `sema_init()` 设置。\n- 获取时：若 `count > 0`，直接减 1；否则加入等待队列。\n- 释放时：若等待队列为空，`count++`；否则唤醒队首任务。\n\n### 等待与唤醒机制\n- 使用 `wake_q`（批量唤醒队列）优化唤醒路径，避免在持有自旋锁时调用 `wake_up_process()`。\n- 等待任务通过 `schedule_timeout()` 睡眠，并在循环中检查：\n  - 是否收到信号（根据睡眠状态判断）。\n  - 是否超时。\n  - 是否被 `__up()` 标记为 `waiter.up = true`（表示已被选中唤醒）。\n\n### Hung Task 支持\n- 当启用 `CONFIG_DETECT_HUNG_TASK_BLOCKER` 时：\n  - 获取信号量时记录当前任务为 `last_holder`。\n  - 释放时若当前任务是持有者，则清除记录。\n  - 提供 `sem_last_holder()` 供 hung task 检测模块查询阻塞源头。\n\n### 返回值约定\n- `down_trylock()` 返回 **0 表示成功**，**1 表示失败**，这与 `mutex_trylock()` 和 `spin_trylock()` **相反**，需特别注意。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/semaphore.h>`：信号量结构体和 API 声明。\n  - `<linux/spinlock.h>`：原始自旋锁实现。\n  - `<linux/sched.h>`、`<linux/sched/wake_q.h>`：任务调度和批量唤醒。\n  - `<trace/events/lock.h>`：锁争用跟踪点。\n  - `<linux/hung_task.h>`：hung task 检测支持。\n\n- **内核配置依赖**：\n  - `CONFIG_DETECT_HUNG_TASK_BLOCKER`：启用信号量持有者跟踪。\n\n- **与其他同步原语关系**：\n  - 与 `mutex.c` 形成对比：mutex 是二值、不可递归、带调试信息的互斥锁；信号量是计数、可被任意任务释放、更轻量。\n  - 底层依赖调度器（`schedule_timeout`）和中断管理（`irqsave`）。\n\n## 5. 使用场景\n\n- **资源池管理**：如限制同时访问某类硬件设备的任务数量。\n- **读写并发控制**：配合其他机制实现多读者/单写者模型。\n- **内核驱动**：设备驱动中控制对共享资源的并发访问。\n- **中断上下文释放**：因 `up()` 可在中断中调用，适用于中断处理程序释放资源的场景。\n- **不可睡眠路径**：使用 `down_trylock()` 在原子上下文尝试获取资源。\n\n> **注意**：由于信号量不强制所有权（任意任务可调用 `up()`），且缺乏死锁检测等调试特性，现代内核开发中更推荐使用 `mutex` 或 `rwsem`，除非明确需要计数语义或多释放者特性。",
      "similarity": 0.4219462275505066,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "\tWRITE_ONCE((sem)->last_holder, (unsigned long)current);",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "\tif (READ_ONCE((sem)->last_holder) == (unsigned long)current)",
            "\t\tWRITE_ONCE((sem)->last_holder, 0UL);",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn READ_ONCE(sem->last_holder);",
            "}",
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn 0UL;",
            "}",
            "static inline void __sem_acquire(struct semaphore *sem)",
            "{",
            "\tsem->count--;",
            "\thung_task_sem_set_holder(sem);",
            "}",
            "void __sched down(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\t__down(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "}",
            "int __sched down_interruptible(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_interruptible(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_killable(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_killable(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_trylock(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint count;",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tcount = sem->count - 1;",
            "\tif (likely(count >= 0))",
            "\t\t__sem_acquire(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn (count < 0);",
            "}",
            "int __sched down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_timeout(sem, timeout);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "void __sched up(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "",
            "\thung_task_sem_clear_if_holder(sem);",
            "",
            "\tif (likely(list_empty(&sem->wait_list)))",
            "\t\tsem->count++;",
            "\telse",
            "\t\t__up(sem, &wake_q);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "\tif (!wake_q_empty(&wake_q))",
            "\t\twake_up_q(&wake_q);",
            "}"
          ],
          "function_name": "hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, __sem_acquire, down, down_interruptible, down_killable, down_trylock, down_timeout, up",
          "description": "实现了信号量的获取与释放核心逻辑，包括down/down_interruptible/down_killable/down_trylock/down_timeout等接口，通过spinlock保护共享资源，维护等待队列并处理任务状态变更，其中包含Hung Task检测相关函数的条件性实现",
          "similarity": 0.42301785945892334
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 252,
          "end_line": 323,
          "content": [
            "static inline int __sched ___down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\t\t\t\tlong timeout)",
            "{",
            "\tstruct semaphore_waiter waiter;",
            "",
            "\tlist_add_tail(&waiter.list, &sem->wait_list);",
            "\twaiter.task = current;",
            "\twaiter.up = false;",
            "",
            "\tfor (;;) {",
            "\t\tif (signal_pending_state(state, current))",
            "\t\t\tgoto interrupted;",
            "\t\tif (unlikely(timeout <= 0))",
            "\t\t\tgoto timed_out;",
            "\t\t__set_current_state(state);",
            "\t\traw_spin_unlock_irq(&sem->lock);",
            "\t\ttimeout = schedule_timeout(timeout);",
            "\t\traw_spin_lock_irq(&sem->lock);",
            "\t\tif (waiter.up) {",
            "\t\t\thung_task_sem_set_holder(sem);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            " timed_out:",
            "\tlist_del(&waiter.list);",
            "\treturn -ETIME;",
            "",
            " interrupted:",
            "\tlist_del(&waiter.list);",
            "\treturn -EINTR;",
            "}",
            "static inline int __sched __down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\tlong timeout)",
            "{",
            "\tint ret;",
            "",
            "\thung_task_set_blocker(sem, BLOCKER_TYPE_SEM);",
            "",
            "\ttrace_contention_begin(sem, 0);",
            "\tret = ___down_common(sem, state, timeout);",
            "\ttrace_contention_end(sem, ret);",
            "",
            "\thung_task_clear_blocker();",
            "",
            "\treturn ret;",
            "}",
            "static noinline void __sched __down(struct semaphore *sem)",
            "{",
            "\t__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_interruptible(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_killable(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_KILLABLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\treturn __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);",
            "}",
            "static noinline void __sched __up(struct semaphore *sem,",
            "\t\t\t\t  struct wake_q_head *wake_q)",
            "{",
            "\tstruct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,",
            "\t\t\t\t\t\tstruct semaphore_waiter, list);",
            "\tlist_del(&waiter->list);",
            "\twaiter->up = true;",
            "\twake_q_add(wake_q, waiter->task);",
            "}"
          ],
          "function_name": "___down_common, __down_common, __down, __down_interruptible, __down_killable, __down_timeout, __up",
          "description": "实现了信号量的阻塞等待通用逻辑，包含___down_common/__down_common等辅助函数，处理信号量不足时的任务挂起、超时检测、信号处理及唤醒机制，通过循环等待并结合schedule_timeout实现阻塞式资源竞争解决",
          "similarity": 0.413073867559433
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2008 Intel Corporation",
            " * Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This file implements counting semaphores.",
            " * A counting semaphore may be acquired 'n' times before sleeping.",
            " * See mutex.c for single-acquisition sleeping locks which enforce",
            " * rules which allow code to be debugged more easily.",
            " */",
            "",
            "/*",
            " * Some notes on the implementation:",
            " *",
            " * The spinlock controls access to the other members of the semaphore.",
            " * down_trylock() and up() can be called from interrupt context, so we",
            " * have to disable interrupts when taking the lock.  It turns out various",
            " * parts of the kernel expect to be able to use down() on a semaphore in",
            " * interrupt context when they know it will succeed, so we have to use",
            " * irqsave variants for down(), down_interruptible() and down_killable()",
            " * too.",
            " *",
            " * The ->count variable represents how many more tasks can acquire this",
            " * semaphore.  If it's zero, there may be tasks waiting on the wait_list.",
            " */",
            "",
            "#include <linux/compiler.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/semaphore.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/ftrace.h>",
            "#include <trace/events/lock.h>",
            "#include <linux/hung_task.h>",
            "",
            "static noinline void __down(struct semaphore *sem);",
            "static noinline int __down_interruptible(struct semaphore *sem);",
            "static noinline int __down_killable(struct semaphore *sem);",
            "static noinline int __down_timeout(struct semaphore *sem, long timeout);",
            "static noinline void __up(struct semaphore *sem, struct wake_q_head *wake_q);",
            "",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER"
          ],
          "function_name": null,
          "description": "此代码块定义了计数信号量的基础框架，包含实现计数信号量所需的头文件和注释，声明了多个内联函数及辅助函数，用于处理信号量的获取、释放及Hung Task检测相关逻辑，但由于代码截断，CONFIG_DETECT_HUNG_TASK_BLOCKER部分缺失，上下文不完整",
          "similarity": 0.3510914742946625
        }
      ]
    }
  ]
}