{
  "query": "softirq与系统时间戳同步的关系",
  "timestamp": "2025-12-26 02:18:22",
  "retrieved_files": [
    {
      "source_file": "kernel/softirq.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:26:22\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `softirq.c`\n\n---\n\n# softirq.c 技术文档\n\n## 1. 文件概述\n\n`softirq.c` 是 Linux 内核中实现软中断（softirq）机制的核心文件。软中断是一种延迟执行中断处理下半部（bottom half）的机制，用于在中断上下文之外安全、高效地处理高频率、低延迟要求的任务。该文件负责软中断的注册、调度、执行以及与内核其他子系统（如调度器、RCU、SMP 等）的协同工作，并为每个 CPU 维护独立的软中断状态，确保无锁化和良好的 CPU 局部性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- `softirq_vec[NR_SOFTIRQS]`：全局数组，存储所有软中断类型的处理函数（`softirq_action`），每个软中断类型（如 `NET_RX`、`TIMER` 等）对应一个条目。\n- `ksoftirqd`：每 CPU 变量，指向该 CPU 上专用于处理软中断的内核线程（`ksoftirqd`）。\n- `softirq_to_name[NR_SOFTIRQS]`：软中断类型的名称字符串数组，用于调试和追踪。\n- `softirq_ctrl`（仅限 `CONFIG_PREEMPT_RT`）：每 CPU 结构体，包含本地锁（`local_lock_t`）和计数器（`cnt`），用于在实时内核中管理软中断禁用状态，支持抢占。\n- `irq_stat`：每 CPU 的中断统计结构体（若架构未提供）。\n\n### 主要函数\n\n- `wakeup_softirqd()`：唤醒当前 CPU 的 `ksoftirqd` 内核线程。\n- `__local_bh_disable_ip()` / `__local_bh_enable_ip()`：用于禁用/启用软中断（Bottom Half），并处理嵌套计数、RCU 锁、锁依赖追踪等。\n- `local_bh_blocked()`（仅限 RT）：检查当前 CPU 是否处于软中断被阻塞状态，用于 idle 任务避免误报。\n- `ksoftirqd_run_begin()` / `ksoftirqd_run_end()`：`ksoftirqd` 线程执行软中断前后的上下文管理。\n- `invoke_softirq()`：根据当前软中断状态决定是否唤醒 `ksoftirqd`。\n- `__do_softirq()`（声明在别处，但在此被调用）：实际执行挂起的软中断处理函数。\n\n## 3. 关键实现\n\n### 软中断执行模型\n- 软中断是 **CPU 本地** 的，无共享变量，天然支持 SMP。\n- 若软中断需要序列化（如 `TASKLET`），由其自身通过自旋锁实现。\n- 软中断执行具有 **弱 CPU 绑定**：仅在触发中断的 CPU 上标记为待执行，提升缓存局部性。\n\n### 实时内核（PREEMPT_RT）支持\n- 在 `CONFIG_PREEMPT_RT` 下，软中断禁用状态不再依赖抢占计数器，而是使用每任务（`task_struct::softirq_disable_cnt`）和每 CPU（`softirq_ctrl::cnt`）两个计数器。\n- 引入 `local_lock_t` 保护软中断临界区，允许在 BH 禁用期间被其他高优先级任务抢占。\n- `ksoftirqd` 线程通过 `ksoftirqd_run_begin/end` 获取本地锁，确保重入安全。\n\n### 软中断调度策略\n- 当软中断在原子上下文（不可抢占）中被启用且有待处理任务时，不直接执行，而是唤醒 `ksoftirqd` 线程处理，避免用户空间饥饿。\n- 在可抢占上下文中启用软中断时，若有待处理软中断，则立即调用 `__do_softirq()` 执行。\n\n### 调试与追踪\n- 集成 `lockdep` 锁依赖分析器，通过 `bh_lock_map` 跟踪软中断禁用区域。\n- 支持 `ftrace` 的 `irq` 事件追踪（通过 `trace/events/irq.h`）。\n- 提供 `in_softirq()`、`softirq_count()` 等宏用于上下文判断。\n\n## 4. 依赖关系\n\n- **中断子系统**：依赖 `irq.h`、`interrupt.h` 提供硬中断接口和状态管理。\n- **调度器**：与 `kthread.h` 协作创建和管理 `ksoftirqd` 内核线程；依赖 `sched.h` 相关机制进行唤醒和调度。\n- **RCU**：在 RT 模式下，软中断禁用区域需持有 `rcu_read_lock()`，确保 RCU 语义正确。\n- **SMP 支持**：使用 `smp.h`、`smpboot.h` 实现每 CPU 变量和 CPU 热插拔支持。\n- **内存管理**：依赖 `mm.h` 和 `percpu.h` 管理每 CPU 数据。\n- **调试设施**：集成 `lockdep`（`DEBUG_LOCK_ALLOC`）、`ftrace`、`irqflags tracing` 等调试框架。\n- **架构相关代码**：可能使用 `asm/softirq_stack.h` 提供的架构特定栈处理。\n\n## 5. 使用场景\n\n- **网络子系统**：`NET_RX` 和 `NET_TX` 软中断用于高效处理网络包接收和发送。\n- **块设备层**：`BLOCK` 软中断处理块 I/O 完成回调。\n- **定时器**：`TIMER` 和 `HRTIMER` 软中断用于执行高精度和普通定时器回调。\n- **RCU**：`RCU` 软中断用于执行宽限期（grace period）相关的回调。\n- **任务队列**：`TASKLET` 软中断提供轻量级、序列化的下半部机制。\n- **调度器事件**：`SCHED` 软中断用于处理调度相关的延迟任务（如负载均衡触发）。\n- **中断轮询**：`IRQ_POLL` 用于高吞吐场景下的中断合并与轮询。\n\n该机制广泛应用于需要在中断后快速、批量、低开销处理任务的内核子系统中，是 Linux 中断处理下半部的核心基础设施之一。",
      "similarity": 0.6288939118385315,
      "chunks": [
        {
          "chunk_id": 6,
          "file_path": "kernel/softirq.c",
          "start_line": 914,
          "end_line": 1021,
          "content": [
            "void tasklet_kill(struct tasklet_struct *t)",
            "{",
            "\tif (in_interrupt())",
            "\t\tpr_notice(\"Attempt to kill tasklet from interrupt\\n\");",
            "",
            "\twhile (test_and_set_bit(TASKLET_STATE_SCHED, &t->state))",
            "\t\twait_var_event(&t->state, !test_bit(TASKLET_STATE_SCHED, &t->state));",
            "",
            "\ttasklet_unlock_wait(t);",
            "\ttasklet_clear_sched(t);",
            "}",
            "void tasklet_unlock(struct tasklet_struct *t)",
            "{",
            "\tsmp_mb__before_atomic();",
            "\tclear_bit(TASKLET_STATE_RUN, &t->state);",
            "\tsmp_mb__after_atomic();",
            "\twake_up_var(&t->state);",
            "}",
            "void tasklet_unlock_wait(struct tasklet_struct *t)",
            "{",
            "\twait_var_event(&t->state, !test_bit(TASKLET_STATE_RUN, &t->state));",
            "}",
            "void __init softirq_init(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tper_cpu(tasklet_vec, cpu).tail =",
            "\t\t\t&per_cpu(tasklet_vec, cpu).head;",
            "\t\tper_cpu(tasklet_hi_vec, cpu).tail =",
            "\t\t\t&per_cpu(tasklet_hi_vec, cpu).head;",
            "\t}",
            "",
            "\topen_softirq(TASKLET_SOFTIRQ, tasklet_action);",
            "\topen_softirq(HI_SOFTIRQ, tasklet_hi_action);",
            "}",
            "static int ksoftirqd_should_run(unsigned int cpu)",
            "{",
            "\treturn local_softirq_pending();",
            "}",
            "static void run_ksoftirqd(unsigned int cpu)",
            "{",
            "\tksoftirqd_run_begin();",
            "\tif (local_softirq_pending()) {",
            "\t\t/*",
            "\t\t * We can safely run softirq on inline stack, as we are not deep",
            "\t\t * in the task stack here.",
            "\t\t */",
            "\t\thandle_softirqs(true);",
            "\t\tksoftirqd_run_end();",
            "\t\tcond_resched();",
            "\t\treturn;",
            "\t}",
            "\tksoftirqd_run_end();",
            "}",
            "static int takeover_tasklets(unsigned int cpu)",
            "{",
            "\t/* CPU is dead, so no lock needed. */",
            "\tlocal_irq_disable();",
            "",
            "\t/* Find end, append list for that CPU. */",
            "\tif (&per_cpu(tasklet_vec, cpu).head != per_cpu(tasklet_vec, cpu).tail) {",
            "\t\t*__this_cpu_read(tasklet_vec.tail) = per_cpu(tasklet_vec, cpu).head;",
            "\t\t__this_cpu_write(tasklet_vec.tail, per_cpu(tasklet_vec, cpu).tail);",
            "\t\tper_cpu(tasklet_vec, cpu).head = NULL;",
            "\t\tper_cpu(tasklet_vec, cpu).tail = &per_cpu(tasklet_vec, cpu).head;",
            "\t}",
            "\traise_softirq_irqoff(TASKLET_SOFTIRQ);",
            "",
            "\tif (&per_cpu(tasklet_hi_vec, cpu).head != per_cpu(tasklet_hi_vec, cpu).tail) {",
            "\t\t*__this_cpu_read(tasklet_hi_vec.tail) = per_cpu(tasklet_hi_vec, cpu).head;",
            "\t\t__this_cpu_write(tasklet_hi_vec.tail, per_cpu(tasklet_hi_vec, cpu).tail);",
            "\t\tper_cpu(tasklet_hi_vec, cpu).head = NULL;",
            "\t\tper_cpu(tasklet_hi_vec, cpu).tail = &per_cpu(tasklet_hi_vec, cpu).head;",
            "\t}",
            "\traise_softirq_irqoff(HI_SOFTIRQ);",
            "",
            "\tlocal_irq_enable();",
            "\treturn 0;",
            "}",
            "static void ktimerd_setup(unsigned int cpu)",
            "{",
            "\t/* Above SCHED_NORMAL to handle timers before regular tasks. */",
            "\tsched_set_fifo_low(current);",
            "}",
            "static int ktimerd_should_run(unsigned int cpu)",
            "{",
            "\treturn local_timers_pending_force_th();",
            "}",
            "void raise_ktimers_thread(unsigned int nr)",
            "{",
            "\ttrace_softirq_raise(nr);",
            "\t__this_cpu_or(pending_timer_softirq, BIT(nr));",
            "}",
            "static void run_ktimerd(unsigned int cpu)",
            "{",
            "\tunsigned int timer_si;",
            "",
            "\tksoftirqd_run_begin();",
            "",
            "\ttimer_si = local_timers_pending_force_th();",
            "\t__this_cpu_write(pending_timer_softirq, 0);",
            "\tor_softirq_pending(timer_si);",
            "",
            "\t__do_softirq();",
            "",
            "\tksoftirqd_run_end();",
            "}"
          ],
          "function_name": "tasklet_kill, tasklet_unlock, tasklet_unlock_wait, softirq_init, ksoftirqd_should_run, run_ksoftirqd, takeover_tasklets, ktimerd_setup, ktimerd_should_run, raise_ktimers_thread, run_ktimerd",
          "description": "提供tasklet终止、状态同步及软中断处理线程管理功能，softirq_init完成软中断处理函数注册，ksoftirqd_should_run/run_ksoftirqd控制软中断处理线程运行，takeover_tasklets实现CPU死亡时任务接管，ktimerd相关接口处理定时器任务",
          "similarity": 0.583791971206665
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/softirq.c",
          "start_line": 74,
          "end_line": 191,
          "content": [
            "static void wakeup_softirqd(void)",
            "{",
            "\t/* Interrupts are disabled: no need to stop preemption */",
            "\tstruct task_struct *tsk = __this_cpu_read(ksoftirqd);",
            "",
            "\tif (tsk)",
            "\t\twake_up_process(tsk);",
            "}",
            "bool local_bh_blocked(void)",
            "{",
            "\treturn __this_cpu_read(softirq_ctrl.cnt) != 0;",
            "}",
            "void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)",
            "{",
            "\tunsigned long flags;",
            "\tint newcnt;",
            "",
            "\tWARN_ON_ONCE(in_hardirq());",
            "",
            "\tlock_map_acquire_read(&bh_lock_map);",
            "",
            "\t/* First entry of a task into a BH disabled section? */",
            "\tif (!current->softirq_disable_cnt) {",
            "\t\tif (preemptible()) {",
            "\t\t\tlocal_lock(&softirq_ctrl.lock);",
            "\t\t\t/* Required to meet the RCU bottomhalf requirements. */",
            "\t\t\trcu_read_lock();",
            "\t\t} else {",
            "\t\t\tDEBUG_LOCKS_WARN_ON(this_cpu_read(softirq_ctrl.cnt));",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Track the per CPU softirq disabled state. On RT this is per CPU",
            "\t * state to allow preemption of bottom half disabled sections.",
            "\t */",
            "\tnewcnt = __this_cpu_add_return(softirq_ctrl.cnt, cnt);",
            "\t/*",
            "\t * Reflect the result in the task state to prevent recursion on the",
            "\t * local lock and to make softirq_count() & al work.",
            "\t */",
            "\tcurrent->softirq_disable_cnt = newcnt;",
            "",
            "\tif (IS_ENABLED(CONFIG_TRACE_IRQFLAGS) && newcnt == cnt) {",
            "\t\traw_local_irq_save(flags);",
            "\t\tlockdep_softirqs_off(ip);",
            "\t\traw_local_irq_restore(flags);",
            "\t}",
            "}",
            "static void __local_bh_enable(unsigned int cnt, bool unlock)",
            "{",
            "\tunsigned long flags;",
            "\tint newcnt;",
            "",
            "\tDEBUG_LOCKS_WARN_ON(current->softirq_disable_cnt !=",
            "\t\t\t    this_cpu_read(softirq_ctrl.cnt));",
            "",
            "\tif (IS_ENABLED(CONFIG_TRACE_IRQFLAGS) && softirq_count() == cnt) {",
            "\t\traw_local_irq_save(flags);",
            "\t\tlockdep_softirqs_on(_RET_IP_);",
            "\t\traw_local_irq_restore(flags);",
            "\t}",
            "",
            "\tnewcnt = __this_cpu_sub_return(softirq_ctrl.cnt, cnt);",
            "\tcurrent->softirq_disable_cnt = newcnt;",
            "",
            "\tif (!newcnt && unlock) {",
            "\t\trcu_read_unlock();",
            "\t\tlocal_unlock(&softirq_ctrl.lock);",
            "\t}",
            "}",
            "void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)",
            "{",
            "\tbool preempt_on = preemptible();",
            "\tunsigned long flags;",
            "\tu32 pending;",
            "\tint curcnt;",
            "",
            "\tWARN_ON_ONCE(in_hardirq());",
            "\tlockdep_assert_irqs_enabled();",
            "",
            "\tlock_map_release(&bh_lock_map);",
            "",
            "\tlocal_irq_save(flags);",
            "\tcurcnt = __this_cpu_read(softirq_ctrl.cnt);",
            "",
            "\t/*",
            "\t * If this is not reenabling soft interrupts, no point in trying to",
            "\t * run pending ones.",
            "\t */",
            "\tif (curcnt != cnt)",
            "\t\tgoto out;",
            "",
            "\tpending = local_softirq_pending();",
            "\tif (!pending)",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * If this was called from non preemptible context, wake up the",
            "\t * softirq daemon.",
            "\t */",
            "\tif (!preempt_on) {",
            "\t\twakeup_softirqd();",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/*",
            "\t * Adjust softirq count to SOFTIRQ_OFFSET which makes",
            "\t * in_serving_softirq() become true.",
            "\t */",
            "\tcnt = SOFTIRQ_OFFSET;",
            "\t__local_bh_enable(cnt, false);",
            "\t__do_softirq();",
            "",
            "out:",
            "\t__local_bh_enable(cnt, preempt_on);",
            "\tlocal_irq_restore(flags);",
            "}"
          ],
          "function_name": "wakeup_softirqd, local_bh_blocked, __local_bh_disable_ip, __local_bh_enable, __local_bh_enable_ip",
          "description": "实现软中断屏蔽/恢复逻辑，通过修改per-CPU计数器控制软中断状态，包含唤醒ksoftirqd线程的逻辑，处理抢占和锁依赖关系。",
          "similarity": 0.5167778730392456
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/softirq.c",
          "start_line": 270,
          "end_line": 380,
          "content": [
            "static inline void ksoftirqd_run_begin(void)",
            "{",
            "\t__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);",
            "\tlocal_irq_disable();",
            "}",
            "static inline void ksoftirqd_run_end(void)",
            "{",
            "\t/* pairs with the lock_map_acquire_read() in ksoftirqd_run_begin() */",
            "\tlock_map_release(&bh_lock_map);",
            "\t__local_bh_enable(SOFTIRQ_OFFSET, true);",
            "\tWARN_ON_ONCE(in_interrupt());",
            "\tlocal_irq_enable();",
            "}",
            "static inline void softirq_handle_begin(void) { }",
            "static inline void softirq_handle_end(void) { }",
            "static inline bool should_wake_ksoftirqd(void)",
            "{",
            "\treturn !this_cpu_read(softirq_ctrl.cnt);",
            "}",
            "static inline void invoke_softirq(void)",
            "{",
            "\tif (should_wake_ksoftirqd())",
            "\t\twakeup_softirqd();",
            "}",
            "void do_softirq_post_smp_call_flush(unsigned int was_pending)",
            "{",
            "\tunsigned int is_pending = local_softirq_pending();",
            "",
            "\tif (unlikely(was_pending != is_pending)) {",
            "\t\tWARN_ON_ONCE(was_pending != (is_pending & ~SCHED_SOFTIRQ_MASK));",
            "\t\tinvoke_softirq();",
            "\t}",
            "}",
            "void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tWARN_ON_ONCE(in_hardirq());",
            "",
            "\traw_local_irq_save(flags);",
            "\t/*",
            "\t * The preempt tracer hooks into preempt_count_add and will break",
            "\t * lockdep because it calls back into lockdep after SOFTIRQ_OFFSET",
            "\t * is set and before current->softirq_enabled is cleared.",
            "\t * We must manually increment preempt_count here and manually",
            "\t * call the trace_preempt_off later.",
            "\t */",
            "\t__preempt_count_add(cnt);",
            "\t/*",
            "\t * Were softirqs turned off above:",
            "\t */",
            "\tif (softirq_count() == (cnt & SOFTIRQ_MASK))",
            "\t\tlockdep_softirqs_off(ip);",
            "\traw_local_irq_restore(flags);",
            "",
            "\tif (preempt_count() == cnt) {",
            "#ifdef CONFIG_DEBUG_PREEMPT",
            "\t\tcurrent->preempt_disable_ip = get_lock_parent_ip();",
            "#endif",
            "\t\ttrace_preempt_off(CALLER_ADDR0, get_lock_parent_ip());",
            "\t}",
            "}",
            "static void __local_bh_enable(unsigned int cnt)",
            "{",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tif (preempt_count() == cnt)",
            "\t\ttrace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());",
            "",
            "\tif (softirq_count() == (cnt & SOFTIRQ_MASK))",
            "\t\tlockdep_softirqs_on(_RET_IP_);",
            "",
            "\t__preempt_count_sub(cnt);",
            "}",
            "void _local_bh_enable(void)",
            "{",
            "\tWARN_ON_ONCE(in_hardirq());",
            "\t__local_bh_enable(SOFTIRQ_DISABLE_OFFSET);",
            "}",
            "void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)",
            "{",
            "\tWARN_ON_ONCE(in_hardirq());",
            "\tlockdep_assert_irqs_enabled();",
            "#ifdef CONFIG_TRACE_IRQFLAGS",
            "\tlocal_irq_disable();",
            "#endif",
            "\t/*",
            "\t * Are softirqs going to be turned on now:",
            "\t */",
            "\tif (softirq_count() == SOFTIRQ_DISABLE_OFFSET)",
            "\t\tlockdep_softirqs_on(ip);",
            "\t/*",
            "\t * Keep preemption disabled until we are done with",
            "\t * softirq processing:",
            "\t */",
            "\t__preempt_count_sub(cnt - 1);",
            "",
            "\tif (unlikely(!in_interrupt() && local_softirq_pending())) {",
            "\t\t/*",
            "\t\t * Run softirq if any pending. And do it in its own stack",
            "\t\t * as we may be calling this deep in a task call stack already.",
            "\t\t */",
            "\t\tdo_softirq();",
            "\t}",
            "",
            "\tpreempt_count_dec();",
            "#ifdef CONFIG_TRACE_IRQFLAGS",
            "\tlocal_irq_enable();",
            "#endif",
            "\tpreempt_check_resched();",
            "}"
          ],
          "function_name": "ksoftirqd_run_begin, ksoftirqd_run_end, softirq_handle_begin, softirq_handle_end, should_wake_ksoftirqd, invoke_softirq, do_softirq_post_smp_call_flush, __local_bh_disable_ip, __local_bh_enable, _local_bh_enable, __local_bh_enable_ip",
          "description": "提供ksoftirqd线程运行时的上下文切换辅助函数，包含软中断处理入口标记、唤醒条件判断及实际执行路径选择逻辑。",
          "similarity": 0.5032699108123779
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/softirq.c",
          "start_line": 417,
          "end_line": 572,
          "content": [
            "static inline void softirq_handle_begin(void)",
            "{",
            "\t__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);",
            "}",
            "static inline void softirq_handle_end(void)",
            "{",
            "\t__local_bh_enable(SOFTIRQ_OFFSET);",
            "\tWARN_ON_ONCE(in_interrupt());",
            "}",
            "static inline void ksoftirqd_run_begin(void)",
            "{",
            "\tlocal_irq_disable();",
            "}",
            "static inline void ksoftirqd_run_end(void)",
            "{",
            "\tlocal_irq_enable();",
            "}",
            "static inline bool should_wake_ksoftirqd(void)",
            "{",
            "\treturn true;",
            "}",
            "static inline void invoke_softirq(void)",
            "{",
            "\tif (!force_irqthreads() || !__this_cpu_read(ksoftirqd)) {",
            "#ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK",
            "\t\t/*",
            "\t\t * We can safely execute softirq on the current stack if",
            "\t\t * it is the irq stack, because it should be near empty",
            "\t\t * at this stage.",
            "\t\t */",
            "\t\t__do_softirq();",
            "#else",
            "\t\t/*",
            "\t\t * Otherwise, irq_exit() is called on the task stack that can",
            "\t\t * be potentially deep already. So call softirq in its own stack",
            "\t\t * to prevent from any overrun.",
            "\t\t */",
            "\t\tdo_softirq_own_stack();",
            "#endif",
            "\t} else {",
            "\t\twakeup_softirqd();",
            "\t}",
            "}",
            "asmlinkage __visible void do_softirq(void)",
            "{",
            "\t__u32 pending;",
            "\tunsigned long flags;",
            "",
            "\tif (in_interrupt())",
            "\t\treturn;",
            "",
            "\tlocal_irq_save(flags);",
            "",
            "\tpending = local_softirq_pending();",
            "",
            "\tif (pending)",
            "\t\tdo_softirq_own_stack();",
            "",
            "\tlocal_irq_restore(flags);",
            "}",
            "static inline bool lockdep_softirq_start(void)",
            "{",
            "\tbool in_hardirq = false;",
            "",
            "\tif (lockdep_hardirq_context()) {",
            "\t\tin_hardirq = true;",
            "\t\tlockdep_hardirq_exit();",
            "\t}",
            "",
            "\tlockdep_softirq_enter();",
            "",
            "\treturn in_hardirq;",
            "}",
            "static inline void lockdep_softirq_end(bool in_hardirq)",
            "{",
            "\tlockdep_softirq_exit();",
            "",
            "\tif (in_hardirq)",
            "\t\tlockdep_hardirq_enter();",
            "}",
            "static inline bool lockdep_softirq_start(void) { return false; }",
            "static inline void lockdep_softirq_end(bool in_hardirq) { }",
            "static void handle_softirqs(bool ksirqd)",
            "{",
            "\tunsigned long end = jiffies + MAX_SOFTIRQ_TIME;",
            "\tunsigned long old_flags = current->flags;",
            "\tint max_restart = MAX_SOFTIRQ_RESTART;",
            "\tstruct softirq_action *h;",
            "\tbool in_hardirq;",
            "\t__u32 pending;",
            "\tint softirq_bit;",
            "",
            "\t/*",
            "\t * Mask out PF_MEMALLOC as the current task context is borrowed for the",
            "\t * softirq. A softirq handled, such as network RX, might set PF_MEMALLOC",
            "\t * again if the socket is related to swapping.",
            "\t */",
            "\tcurrent->flags &= ~PF_MEMALLOC;",
            "",
            "\tpending = local_softirq_pending();",
            "",
            "\tsoftirq_handle_begin();",
            "\tin_hardirq = lockdep_softirq_start();",
            "\taccount_softirq_enter(current);",
            "",
            "restart:",
            "\t/* Reset the pending bitmask before enabling irqs */",
            "\tset_softirq_pending(0);",
            "",
            "\tlocal_irq_enable();",
            "",
            "\th = softirq_vec;",
            "",
            "\twhile ((softirq_bit = ffs(pending))) {",
            "\t\tunsigned int vec_nr;",
            "\t\tint prev_count;",
            "",
            "\t\th += softirq_bit - 1;",
            "",
            "\t\tvec_nr = h - softirq_vec;",
            "\t\tprev_count = preempt_count();",
            "",
            "\t\tkstat_incr_softirqs_this_cpu(vec_nr);",
            "",
            "\t\ttrace_softirq_entry(vec_nr);",
            "\t\th->action(h);",
            "\t\ttrace_softirq_exit(vec_nr);",
            "\t\tif (unlikely(prev_count != preempt_count())) {",
            "\t\t\tpr_err(\"huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\\n\",",
            "\t\t\t       vec_nr, softirq_to_name[vec_nr], h->action,",
            "\t\t\t       prev_count, preempt_count());",
            "\t\t\tpreempt_count_set(prev_count);",
            "\t\t}",
            "\t\th++;",
            "\t\tpending >>= softirq_bit;",
            "\t}",
            "",
            "\tif (!IS_ENABLED(CONFIG_PREEMPT_RT) && ksirqd)",
            "\t\trcu_softirq_qs();",
            "",
            "\tlocal_irq_disable();",
            "",
            "\tpending = local_softirq_pending();",
            "\tif (pending) {",
            "\t\tif (time_before(jiffies, end) && !need_resched() &&",
            "\t\t    --max_restart)",
            "\t\t\tgoto restart;",
            "",
            "\t\twakeup_softirqd();",
            "\t}",
            "",
            "\taccount_softirq_exit(current);",
            "\tlockdep_softirq_end(in_hardirq);",
            "\tsoftirq_handle_end();",
            "\tcurrent_restore_flags(old_flags, PF_MEMALLOC);",
            "}"
          ],
          "function_name": "softirq_handle_begin, softirq_handle_end, ksoftirqd_run_begin, ksoftirqd_run_end, should_wake_ksoftirqd, invoke_softirq, do_softirq, lockdep_softirq_start, lockdep_softirq_end, lockdep_softirq_start, lockdep_softirq_end, handle_softirqs",
          "description": "实现软中断处理核心流程，包括pending位图扫描、动作执行、RCU状态更新及异常情况检测，包含硬中断上下文转换跟踪机制。",
          "similarity": 0.4761466085910797
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/softirq.c",
          "start_line": 1059,
          "end_line": 1085,
          "content": [
            "static __init int spawn_ksoftirqd(void)",
            "{",
            "\tcpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, \"softirq:dead\", NULL,",
            "\t\t\t\t  takeover_tasklets);",
            "\tBUG_ON(smpboot_register_percpu_thread(&softirq_threads));",
            "#ifdef CONFIG_IRQ_FORCED_THREADING",
            "\tif (force_irqthreads())",
            "\t\tBUG_ON(smpboot_register_percpu_thread(&timer_thread));",
            "#endif",
            "\treturn 0;",
            "}",
            "int __init __weak early_irq_init(void)",
            "{",
            "\treturn 0;",
            "}",
            "int __init __weak arch_probe_nr_irqs(void)",
            "{",
            "\treturn NR_IRQS_LEGACY;",
            "}",
            "int __init __weak arch_early_irq_init(void)",
            "{",
            "\treturn 0;",
            "}",
            "unsigned int __weak arch_dynirq_lower_bound(unsigned int from)",
            "{",
            "\treturn from;",
            "}"
          ],
          "function_name": "spawn_ksoftirqd, early_irq_init, arch_probe_nr_irqs, arch_early_irq_init, arch_dynirq_lower_bound",
          "description": "实现软中断处理线程的注册与早期中断初始化，spawn_ksoftirqd注册软中断处理线程并绑定takeover_tasklets，arch_*系列弱符号接口处理不同架构的中断探测与动态中断范围计算",
          "similarity": 0.46054551005363464
        }
      ]
    },
    {
      "source_file": "kernel/irq/manage.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:01:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\manage.c`\n\n---\n\n# `irq/manage.c` 技术文档\n\n## 1. 文件概述\n\n`irq/manage.c` 是 Linux 内核通用中断子系统（Generic IRQ Subsystem）的核心管理文件之一，主要提供驱动程序与中断子系统交互的 API 接口。该文件实现了中断同步、CPU 亲和性（affinity）设置、中断线程化控制等关键功能，用于确保中断处理的安全性、可调度性和可配置性。其目标是在多核系统中协调硬中断（hardirq）与线程化中断（threaded IRQ）的执行，并支持从用户空间或内核空间动态调整中断的 CPU 分配策略。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `synchronize_hardirq(unsigned int irq)`  \n  等待指定中断号的硬中断处理程序（在其他 CPU 上）执行完毕，但不等待线程化中断处理程序。返回值指示是否存在活跃的线程化处理程序。\n\n- `synchronize_irq(unsigned int irq)`  \n  等待指定中断的所有处理程序（包括硬中断和线程化中断）完成。该函数可能睡眠，仅可在可抢占上下文中调用。\n\n- `irq_can_set_affinity(unsigned int irq)`  \n  检查指定中断是否支持设置 CPU 亲和性（即是否具备有效的 `irq_set_affinity` 回调且未被禁止平衡）。\n\n- `irq_can_set_affinity_usr(unsigned int irq)`  \n  在 `irq_can_set_affinity` 基础上，额外检查中断是否未被标记为 `AFFINITY_MANAGED`，用于判断用户空间是否可修改其亲和性。\n\n- `irq_set_thread_affinity(struct irq_desc *desc)`  \n  通知与中断关联的内核线程更新其 CPU 亲和性，通过设置 `IRQTF_AFFINITY` 标志延迟执行。\n\n- `irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask, bool force)`  \n  实际执行中断亲和性设置，调用底层 `irq_chip` 的 `irq_set_affinity` 方法，并处理受管中断（managed IRQ）与 housekeeping CPU 的交互逻辑。\n\n### 关键数据结构与变量\n\n- `force_irqthreads_key`（条件编译）  \n  静态分支键，用于在启动参数 `threadirqs` 启用时强制将所有中断处理程序线程化（非 PREEMPT_RT 配置下）。\n\n- `irq_default_affinity`（SMP 下）  \n  全局默认中断亲和性掩码，通常初始化为所有在线 CPU。\n\n- `IRQTF_AFFINITY`  \n  中断线程标志位，用于通知线程需更新其 CPU 亲和性。\n\n## 3. 关键实现\n\n### 中断同步机制\n\n- `__synchronize_hardirq()` 通过双重检查机制确保硬中断处理完成：\n  1. 先通过 `irqd_irq_inprogress()` 忙等待退出临界区；\n  2. 再加锁检查，若启用 `sync_chip` 且底层芯片支持，还会查询硬件级中断是否仍处于活跃状态（通过 `__irq_get_irqchip_state`）。\n- `synchronize_irq()` 在硬中断同步基础上，通过 `wait_event()` 等待 `threads_active` 计数归零，确保线程化处理程序也完成。\n\n### 中断亲和性管理\n\n- **受管中断（Managed IRQ）处理**：当 `irqd_affinity_is_managed` 为真且启用了 `HK_TYPE_MANAGED_IRQ` housekeeping 时，`irq_do_set_affinity` 会将请求的亲和性掩码与 housekeeping CPU 掩码求交集，防止 I/O 中断被路由到隔离 CPU（isolated CPU），除非 I/O 本身由该隔离 CPU 发起。\n- **在线 CPU 过滤**：除非 `force` 参数为真，否则实际传递给 `irq_chip` 的掩码会与 `cpu_online_mask` 求交，确保仅指定在线 CPU。\n- **线程亲和性延迟更新**：由于 `irq_set_thread_affinity` 可能在硬中断上下文调用，无法直接调用 `set_cpus_allowed_ptr()`，故通过设置标志位，由中断线程自行处理。\n\n### 强制线程化支持\n\n- 在 `CONFIG_IRQ_FORCED_THREADING` 且非 `CONFIG_PREEMPT_RT` 配置下，通过 `early_param(\"threadirqs\", ...)` 解析内核启动参数，启用 `force_irqthreads_key` 静态分支，使所有中断默认以线程方式执行。\n\n### 有效亲和性验证\n\n- 若启用 `CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK`，在设置亲和性后会验证 `effective_affinity_mask` 是否非空，若为空则警告，确保底层 `irq_chip` 正确更新有效掩码。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/irq.h>`、`<linux/interrupt.h>`：提供中断子系统核心 API 和数据结构。\n  - `\"internals.h\"`：包含中断子系统内部实现细节（如 `irq_desc` 操作宏）。\n  - `<linux/irqdomain.h>`：支持 IRQ domain 映射。\n  - `<linux/sched/isolation.h>`：用于 housekeeping CPU 掩码获取。\n  - `<linux/task_work.h>`、`<linux/kthread.h>`：支持中断线程管理。\n\n- **模块交互**：\n  - 依赖底层 `irq_chip` 驱动实现 `irq_set_affinity` 和 `irq_get_irqchip_state` 回调。\n  - 与调度器子系统交互（通过 `set_cpus_allowed_ptr`、`wait_event` 等）。\n  - 与 CPU 热插拔子系统协同处理中断迁移（当 housekeeping CPU 离线/上线时）。\n\n## 5. 使用场景\n\n- **驱动卸载/模块移除**：调用 `synchronize_irq()` 确保所有中断处理完成后再释放资源，避免 UAF。\n- **实时性调优**：通过 `irq_set_affinity()` 将特定中断绑定到专用 CPU，减少干扰。\n- **系统隔离配置**：在启用了 CPU 隔离（如 `isolcpus`）的系统中，`irq_do_set_affinity` 自动将受管中断限制在 housekeeping CPU 上，保障隔离 CPU 的确定性。\n- **调试与诊断**：`synchronize_hardirq()` 用于仅同步硬中断路径，适用于对延迟敏感的场景。\n- **用户空间工具**：如 `irqbalance` 或 `/proc/irq/*/smp_affinity` 写入操作，依赖 `irq_can_set_affinity_usr()` 判断是否允许修改。",
      "similarity": 0.5813207030296326,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/manage.c",
          "start_line": 30,
          "end_line": 137,
          "content": [
            "static int __init setup_forced_irqthreads(char *arg)",
            "{",
            "\tstatic_branch_enable(&force_irqthreads_key);",
            "\treturn 0;",
            "}",
            "static void __synchronize_hardirq(struct irq_desc *desc, bool sync_chip)",
            "{",
            "\tstruct irq_data *irqd = irq_desc_get_irq_data(desc);",
            "\tbool inprogress;",
            "",
            "\tdo {",
            "\t\tunsigned long flags;",
            "",
            "\t\t/*",
            "\t\t * Wait until we're out of the critical section.  This might",
            "\t\t * give the wrong answer due to the lack of memory barriers.",
            "\t\t */",
            "\t\twhile (irqd_irq_inprogress(&desc->irq_data))",
            "\t\t\tcpu_relax();",
            "",
            "\t\t/* Ok, that indicated we're done: double-check carefully. */",
            "\t\traw_spin_lock_irqsave(&desc->lock, flags);",
            "\t\tinprogress = irqd_irq_inprogress(&desc->irq_data);",
            "",
            "\t\t/*",
            "\t\t * If requested and supported, check at the chip whether it",
            "\t\t * is in flight at the hardware level, i.e. already pending",
            "\t\t * in a CPU and waiting for service and acknowledge.",
            "\t\t */",
            "\t\tif (!inprogress && sync_chip) {",
            "\t\t\t/*",
            "\t\t\t * Ignore the return code. inprogress is only updated",
            "\t\t\t * when the chip supports it.",
            "\t\t\t */",
            "\t\t\t__irq_get_irqchip_state(irqd, IRQCHIP_STATE_ACTIVE,",
            "\t\t\t\t\t\t&inprogress);",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&desc->lock, flags);",
            "",
            "\t\t/* Oops, that failed? */",
            "\t} while (inprogress);",
            "}",
            "bool synchronize_hardirq(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tif (desc) {",
            "\t\t__synchronize_hardirq(desc, false);",
            "\t\treturn !atomic_read(&desc->threads_active);",
            "\t}",
            "",
            "\treturn true;",
            "}",
            "static void __synchronize_irq(struct irq_desc *desc)",
            "{",
            "\t__synchronize_hardirq(desc, true);",
            "\t/*",
            "\t * We made sure that no hardirq handler is running. Now verify that no",
            "\t * threaded handlers are active.",
            "\t */",
            "\twait_event(desc->wait_for_threads, !atomic_read(&desc->threads_active));",
            "}",
            "void synchronize_irq(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tif (desc)",
            "\t\t__synchronize_irq(desc);",
            "}",
            "static bool __irq_can_set_affinity(struct irq_desc *desc)",
            "{",
            "\tif (!desc || !irqd_can_balance(&desc->irq_data) ||",
            "\t    !desc->irq_data.chip || !desc->irq_data.chip->irq_set_affinity)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "int irq_can_set_affinity(unsigned int irq)",
            "{",
            "\treturn __irq_can_set_affinity(irq_to_desc(irq));",
            "}",
            "bool irq_can_set_affinity_usr(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\treturn __irq_can_set_affinity(desc) &&",
            "\t\t!irqd_affinity_is_managed(&desc->irq_data);",
            "}",
            "void irq_set_thread_affinity(struct irq_desc *desc)",
            "{",
            "\tstruct irqaction *action;",
            "",
            "\tfor_each_action_of_desc(desc, action) {",
            "\t\tif (action->thread)",
            "\t\t\tset_bit(IRQTF_AFFINITY, &action->thread_flags);",
            "\t\tif (action->secondary && action->secondary->thread)",
            "\t\t\tset_bit(IRQTF_AFFINITY, &action->secondary->thread_flags);",
            "\t}",
            "}",
            "static void irq_validate_effective_affinity(struct irq_data *data)",
            "{",
            "\tconst struct cpumask *m = irq_data_get_effective_affinity_mask(data);",
            "\tstruct irq_chip *chip = irq_data_get_irq_chip(data);",
            "",
            "\tif (!cpumask_empty(m))",
            "\t\treturn;",
            "\tpr_warn_once(\"irq_chip %s did not update eff. affinity mask of irq %u\\n\",",
            "\t\t     chip->name, data->irq);",
            "}"
          ],
          "function_name": "setup_forced_irqthreads, __synchronize_hardirq, synchronize_hardirq, __synchronize_irq, synchronize_irq, __irq_can_set_affinity, irq_can_set_affinity, irq_can_set_affinity_usr, irq_set_thread_affinity, irq_validate_effective_affinity",
          "description": "实现中断同步机制，包含等待硬中断结束、检查亲和性设置能力及设置线程亲和性等功能，通过锁保护保证并发安全。",
          "similarity": 0.5322527885437012
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/irq/manage.c",
          "start_line": 2145,
          "end_line": 2255,
          "content": [
            "int request_threaded_irq(unsigned int irq, irq_handler_t handler,",
            "\t\t\t irq_handler_t thread_fn, unsigned long irqflags,",
            "\t\t\t const char *devname, void *dev_id)",
            "{",
            "\tstruct irqaction *action;",
            "\tstruct irq_desc *desc;",
            "\tint retval;",
            "",
            "\tif (irq == IRQ_NOTCONNECTED)",
            "\t\treturn -ENOTCONN;",
            "",
            "\t/*",
            "\t * Sanity-check: shared interrupts must pass in a real dev-ID,",
            "\t * otherwise we'll have trouble later trying to figure out",
            "\t * which interrupt is which (messes up the interrupt freeing",
            "\t * logic etc).",
            "\t *",
            "\t * Also shared interrupts do not go well with disabling auto enable.",
            "\t * The sharing interrupt might request it while it's still disabled",
            "\t * and then wait for interrupts forever.",
            "\t *",
            "\t * Also IRQF_COND_SUSPEND only makes sense for shared interrupts and",
            "\t * it cannot be set along with IRQF_NO_SUSPEND.",
            "\t */",
            "\tif (((irqflags & IRQF_SHARED) && !dev_id) ||",
            "\t    ((irqflags & IRQF_SHARED) && (irqflags & IRQF_NO_AUTOEN)) ||",
            "\t    (!(irqflags & IRQF_SHARED) && (irqflags & IRQF_COND_SUSPEND)) ||",
            "\t    ((irqflags & IRQF_NO_SUSPEND) && (irqflags & IRQF_COND_SUSPEND)))",
            "\t\treturn -EINVAL;",
            "",
            "\tdesc = irq_to_desc(irq);",
            "\tif (!desc)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!irq_settings_can_request(desc) ||",
            "\t    WARN_ON(irq_settings_is_per_cpu_devid(desc)))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!handler) {",
            "\t\tif (!thread_fn)",
            "\t\t\treturn -EINVAL;",
            "\t\thandler = irq_default_primary_handler;",
            "\t}",
            "",
            "\taction = kzalloc(sizeof(struct irqaction), GFP_KERNEL);",
            "\tif (!action)",
            "\t\treturn -ENOMEM;",
            "",
            "\taction->handler = handler;",
            "\taction->thread_fn = thread_fn;",
            "\taction->flags = irqflags;",
            "\taction->name = devname;",
            "\taction->dev_id = dev_id;",
            "",
            "\tretval = irq_chip_pm_get(&desc->irq_data);",
            "\tif (retval < 0) {",
            "\t\tkfree(action);",
            "\t\treturn retval;",
            "\t}",
            "",
            "\tretval = __setup_irq(irq, desc, action);",
            "",
            "\tif (retval) {",
            "\t\tirq_chip_pm_put(&desc->irq_data);",
            "\t\tkfree(action->secondary);",
            "\t\tkfree(action);",
            "\t}",
            "",
            "#ifdef CONFIG_DEBUG_SHIRQ_FIXME",
            "\tif (!retval && (irqflags & IRQF_SHARED)) {",
            "\t\t/*",
            "\t\t * It's a shared IRQ -- the driver ought to be prepared for it",
            "\t\t * to happen immediately, so let's make sure....",
            "\t\t * We disable the irq to make sure that a 'real' IRQ doesn't",
            "\t\t * run in parallel with our fake.",
            "\t\t */",
            "\t\tunsigned long flags;",
            "",
            "\t\tdisable_irq(irq);",
            "\t\tlocal_irq_save(flags);",
            "",
            "\t\thandler(irq, dev_id);",
            "",
            "\t\tlocal_irq_restore(flags);",
            "\t\tenable_irq(irq);",
            "\t}",
            "#endif",
            "\treturn retval;",
            "}",
            "int request_any_context_irq(unsigned int irq, irq_handler_t handler,",
            "\t\t\t    unsigned long flags, const char *name, void *dev_id)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tint ret;",
            "",
            "\tif (irq == IRQ_NOTCONNECTED)",
            "\t\treturn -ENOTCONN;",
            "",
            "\tdesc = irq_to_desc(irq);",
            "\tif (!desc)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (irq_settings_is_nested_thread(desc)) {",
            "\t\tret = request_threaded_irq(irq, NULL, handler,",
            "\t\t\t\t\t   flags, name, dev_id);",
            "\t\treturn !ret ? IRQC_IS_NESTED : ret;",
            "\t}",
            "",
            "\tret = request_irq(irq, handler, flags, name, dev_id);",
            "\treturn !ret ? IRQC_IS_HARDIRQ : ret;",
            "}"
          ],
          "function_name": "request_threaded_irq, request_any_context_irq",
          "description": "request_threaded_irq请求带线程处理的中断，request_any_context_irq根据中断上下文选择硬中断或嵌套线程处理模式，均通过__setup_irq完成核心初始化逻辑。",
          "similarity": 0.5153864622116089
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/irq/manage.c",
          "start_line": 1459,
          "end_line": 1855,
          "content": [
            "static int",
            "setup_irq_thread(struct irqaction *new, unsigned int irq, bool secondary)",
            "{",
            "\tstruct task_struct *t;",
            "",
            "\tif (!secondary) {",
            "\t\tt = kthread_create(irq_thread, new, \"irq/%d-%s\", irq,",
            "\t\t\t\t   new->name);",
            "\t} else {",
            "\t\tt = kthread_create(irq_thread, new, \"irq/%d-s-%s\", irq,",
            "\t\t\t\t   new->name);",
            "\t}",
            "",
            "\tif (IS_ERR(t))",
            "\t\treturn PTR_ERR(t);",
            "",
            "\t/*",
            "\t * We keep the reference to the task struct even if",
            "\t * the thread dies to avoid that the interrupt code",
            "\t * references an already freed task_struct.",
            "\t */",
            "\tnew->thread = get_task_struct(t);",
            "\t/*",
            "\t * Tell the thread to set its affinity. This is",
            "\t * important for shared interrupt handlers as we do",
            "\t * not invoke setup_affinity() for the secondary",
            "\t * handlers as everything is already set up. Even for",
            "\t * interrupts marked with IRQF_NO_BALANCE this is",
            "\t * correct as we want the thread to move to the cpu(s)",
            "\t * on which the requesting code placed the interrupt.",
            "\t */",
            "\tset_bit(IRQTF_AFFINITY, &new->thread_flags);",
            "\treturn 0;",
            "}",
            "static int",
            "__setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)",
            "{",
            "\tstruct irqaction *old, **old_ptr;",
            "\tunsigned long flags, thread_mask = 0;",
            "\tint ret, nested, shared = 0;",
            "",
            "\tif (!desc)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (desc->irq_data.chip == &no_irq_chip)",
            "\t\treturn -ENOSYS;",
            "\tif (!try_module_get(desc->owner))",
            "\t\treturn -ENODEV;",
            "",
            "\tnew->irq = irq;",
            "",
            "\t/*",
            "\t * If the trigger type is not specified by the caller,",
            "\t * then use the default for this interrupt.",
            "\t */",
            "\tif (!(new->flags & IRQF_TRIGGER_MASK))",
            "\t\tnew->flags |= irqd_get_trigger_type(&desc->irq_data);",
            "",
            "\t/*",
            "\t * Check whether the interrupt nests into another interrupt",
            "\t * thread.",
            "\t */",
            "\tnested = irq_settings_is_nested_thread(desc);",
            "\tif (nested) {",
            "\t\tif (!new->thread_fn) {",
            "\t\t\tret = -EINVAL;",
            "\t\t\tgoto out_mput;",
            "\t\t}",
            "\t\t/*",
            "\t\t * Replace the primary handler which was provided from",
            "\t\t * the driver for non nested interrupt handling by the",
            "\t\t * dummy function which warns when called.",
            "\t\t */",
            "\t\tnew->handler = irq_nested_primary_handler;",
            "\t} else {",
            "\t\tif (irq_settings_can_thread(desc)) {",
            "\t\t\tret = irq_setup_forced_threading(new);",
            "\t\t\tif (ret)",
            "\t\t\t\tgoto out_mput;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Create a handler thread when a thread function is supplied",
            "\t * and the interrupt does not nest into another interrupt",
            "\t * thread.",
            "\t */",
            "\tif (new->thread_fn && !nested) {",
            "\t\tret = setup_irq_thread(new, irq, false);",
            "\t\tif (ret)",
            "\t\t\tgoto out_mput;",
            "\t\tif (new->secondary) {",
            "\t\t\tret = setup_irq_thread(new->secondary, irq, true);",
            "\t\t\tif (ret)",
            "\t\t\t\tgoto out_thread;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Drivers are often written to work w/o knowledge about the",
            "\t * underlying irq chip implementation, so a request for a",
            "\t * threaded irq without a primary hard irq context handler",
            "\t * requires the ONESHOT flag to be set. Some irq chips like",
            "\t * MSI based interrupts are per se one shot safe. Check the",
            "\t * chip flags, so we can avoid the unmask dance at the end of",
            "\t * the threaded handler for those.",
            "\t */",
            "\tif (desc->irq_data.chip->flags & IRQCHIP_ONESHOT_SAFE)",
            "\t\tnew->flags &= ~IRQF_ONESHOT;",
            "",
            "\t/*",
            "\t * Protects against a concurrent __free_irq() call which might wait",
            "\t * for synchronize_hardirq() to complete without holding the optional",
            "\t * chip bus lock and desc->lock. Also protects against handing out",
            "\t * a recycled oneshot thread_mask bit while it's still in use by",
            "\t * its previous owner.",
            "\t */",
            "\tmutex_lock(&desc->request_mutex);",
            "",
            "\t/*",
            "\t * Acquire bus lock as the irq_request_resources() callback below",
            "\t * might rely on the serialization or the magic power management",
            "\t * functions which are abusing the irq_bus_lock() callback,",
            "\t */",
            "\tchip_bus_lock(desc);",
            "",
            "\t/* First installed action requests resources. */",
            "\tif (!desc->action) {",
            "\t\tret = irq_request_resources(desc);",
            "\t\tif (ret) {",
            "\t\t\tpr_err(\"Failed to request resources for %s (irq %d) on irqchip %s\\n\",",
            "\t\t\t       new->name, irq, desc->irq_data.chip->name);",
            "\t\t\tgoto out_bus_unlock;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * The following block of code has to be executed atomically",
            "\t * protected against a concurrent interrupt and any of the other",
            "\t * management calls which are not serialized via",
            "\t * desc->request_mutex or the optional bus lock.",
            "\t */",
            "\traw_spin_lock_irqsave(&desc->lock, flags);",
            "\told_ptr = &desc->action;",
            "\told = *old_ptr;",
            "\tif (old) {",
            "\t\t/*",
            "\t\t * Can't share interrupts unless both agree to and are",
            "\t\t * the same type (level, edge, polarity). So both flag",
            "\t\t * fields must have IRQF_SHARED set and the bits which",
            "\t\t * set the trigger type must match. Also all must",
            "\t\t * agree on ONESHOT.",
            "\t\t * Interrupt lines used for NMIs cannot be shared.",
            "\t\t */",
            "\t\tunsigned int oldtype;",
            "",
            "\t\tif (desc->istate & IRQS_NMI) {",
            "\t\t\tpr_err(\"Invalid attempt to share NMI for %s (irq %d) on irqchip %s.\\n\",",
            "\t\t\t\tnew->name, irq, desc->irq_data.chip->name);",
            "\t\t\tret = -EINVAL;",
            "\t\t\tgoto out_unlock;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If nobody did set the configuration before, inherit",
            "\t\t * the one provided by the requester.",
            "\t\t */",
            "\t\tif (irqd_trigger_type_was_set(&desc->irq_data)) {",
            "\t\t\toldtype = irqd_get_trigger_type(&desc->irq_data);",
            "\t\t} else {",
            "\t\t\toldtype = new->flags & IRQF_TRIGGER_MASK;",
            "\t\t\tirqd_set_trigger_type(&desc->irq_data, oldtype);",
            "\t\t}",
            "",
            "\t\tif (!((old->flags & new->flags) & IRQF_SHARED) ||",
            "\t\t    (oldtype != (new->flags & IRQF_TRIGGER_MASK)) ||",
            "\t\t    ((old->flags ^ new->flags) & IRQF_ONESHOT))",
            "\t\t\tgoto mismatch;",
            "",
            "\t\t/* All handlers must agree on per-cpuness */",
            "\t\tif ((old->flags & IRQF_PERCPU) !=",
            "\t\t    (new->flags & IRQF_PERCPU))",
            "\t\t\tgoto mismatch;",
            "",
            "\t\t/* add new interrupt at end of irq queue */",
            "\t\tdo {",
            "\t\t\t/*",
            "\t\t\t * Or all existing action->thread_mask bits,",
            "\t\t\t * so we can find the next zero bit for this",
            "\t\t\t * new action.",
            "\t\t\t */",
            "\t\t\tthread_mask |= old->thread_mask;",
            "\t\t\told_ptr = &old->next;",
            "\t\t\told = *old_ptr;",
            "\t\t} while (old);",
            "\t\tshared = 1;",
            "\t}",
            "",
            "\t/*",
            "\t * Setup the thread mask for this irqaction for ONESHOT. For",
            "\t * !ONESHOT irqs the thread mask is 0 so we can avoid a",
            "\t * conditional in irq_wake_thread().",
            "\t */",
            "\tif (new->flags & IRQF_ONESHOT) {",
            "\t\t/*",
            "\t\t * Unlikely to have 32 resp 64 irqs sharing one line,",
            "\t\t * but who knows.",
            "\t\t */",
            "\t\tif (thread_mask == ~0UL) {",
            "\t\t\tret = -EBUSY;",
            "\t\t\tgoto out_unlock;",
            "\t\t}",
            "\t\t/*",
            "\t\t * The thread_mask for the action is or'ed to",
            "\t\t * desc->thread_active to indicate that the",
            "\t\t * IRQF_ONESHOT thread handler has been woken, but not",
            "\t\t * yet finished. The bit is cleared when a thread",
            "\t\t * completes. When all threads of a shared interrupt",
            "\t\t * line have completed desc->threads_active becomes",
            "\t\t * zero and the interrupt line is unmasked. See",
            "\t\t * handle.c:irq_wake_thread() for further information.",
            "\t\t *",
            "\t\t * If no thread is woken by primary (hard irq context)",
            "\t\t * interrupt handlers, then desc->threads_active is",
            "\t\t * also checked for zero to unmask the irq line in the",
            "\t\t * affected hard irq flow handlers",
            "\t\t * (handle_[fasteoi|level]_irq).",
            "\t\t *",
            "\t\t * The new action gets the first zero bit of",
            "\t\t * thread_mask assigned. See the loop above which or's",
            "\t\t * all existing action->thread_mask bits.",
            "\t\t */",
            "\t\tnew->thread_mask = 1UL << ffz(thread_mask);",
            "",
            "\t} else if (new->handler == irq_default_primary_handler &&",
            "\t\t   !(desc->irq_data.chip->flags & IRQCHIP_ONESHOT_SAFE)) {",
            "\t\t/*",
            "\t\t * The interrupt was requested with handler = NULL, so",
            "\t\t * we use the default primary handler for it. But it",
            "\t\t * does not have the oneshot flag set. In combination",
            "\t\t * with level interrupts this is deadly, because the",
            "\t\t * default primary handler just wakes the thread, then",
            "\t\t * the irq lines is reenabled, but the device still",
            "\t\t * has the level irq asserted. Rinse and repeat....",
            "\t\t *",
            "\t\t * While this works for edge type interrupts, we play",
            "\t\t * it safe and reject unconditionally because we can't",
            "\t\t * say for sure which type this interrupt really",
            "\t\t * has. The type flags are unreliable as the",
            "\t\t * underlying chip implementation can override them.",
            "\t\t */",
            "\t\tpr_err(\"Threaded irq requested with handler=NULL and !ONESHOT for %s (irq %d)\\n\",",
            "\t\t       new->name, irq);",
            "\t\tret = -EINVAL;",
            "\t\tgoto out_unlock;",
            "\t}",
            "",
            "\tif (!shared) {",
            "\t\t/* Setup the type (level, edge polarity) if configured: */",
            "\t\tif (new->flags & IRQF_TRIGGER_MASK) {",
            "\t\t\tret = __irq_set_trigger(desc,",
            "\t\t\t\t\t\tnew->flags & IRQF_TRIGGER_MASK);",
            "",
            "\t\t\tif (ret)",
            "\t\t\t\tgoto out_unlock;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Activate the interrupt. That activation must happen",
            "\t\t * independently of IRQ_NOAUTOEN. request_irq() can fail",
            "\t\t * and the callers are supposed to handle",
            "\t\t * that. enable_irq() of an interrupt requested with",
            "\t\t * IRQ_NOAUTOEN is not supposed to fail. The activation",
            "\t\t * keeps it in shutdown mode, it merily associates",
            "\t\t * resources if necessary and if that's not possible it",
            "\t\t * fails. Interrupts which are in managed shutdown mode",
            "\t\t * will simply ignore that activation request.",
            "\t\t */",
            "\t\tret = irq_activate(desc);",
            "\t\tif (ret)",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\tdesc->istate &= ~(IRQS_AUTODETECT | IRQS_SPURIOUS_DISABLED | \\",
            "\t\t\t\t  IRQS_ONESHOT | IRQS_WAITING);",
            "\t\tirqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);",
            "",
            "\t\tif (new->flags & IRQF_PERCPU) {",
            "\t\t\tirqd_set(&desc->irq_data, IRQD_PER_CPU);",
            "\t\t\tirq_settings_set_per_cpu(desc);",
            "\t\t\tif (new->flags & IRQF_NO_DEBUG)",
            "\t\t\t\tirq_settings_set_no_debug(desc);",
            "\t\t}",
            "",
            "\t\tif (noirqdebug)",
            "\t\t\tirq_settings_set_no_debug(desc);",
            "",
            "\t\tif (new->flags & IRQF_ONESHOT)",
            "\t\t\tdesc->istate |= IRQS_ONESHOT;",
            "",
            "\t\t/* Exclude IRQ from balancing if requested */",
            "\t\tif (new->flags & IRQF_NOBALANCING) {",
            "\t\t\tirq_settings_set_no_balancing(desc);",
            "\t\t\tirqd_set(&desc->irq_data, IRQD_NO_BALANCING);",
            "\t\t}",
            "",
            "\t\tif (!(new->flags & IRQF_NO_AUTOEN) &&",
            "\t\t    irq_settings_can_autoenable(desc)) {",
            "\t\t\tirq_startup(desc, IRQ_RESEND, IRQ_START_COND);",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * Shared interrupts do not go well with disabling",
            "\t\t\t * auto enable. The sharing interrupt might request",
            "\t\t\t * it while it's still disabled and then wait for",
            "\t\t\t * interrupts forever.",
            "\t\t\t */",
            "\t\t\tWARN_ON_ONCE(new->flags & IRQF_SHARED);",
            "\t\t\t/* Undo nested disables: */",
            "\t\t\tdesc->depth = 1;",
            "\t\t}",
            "",
            "\t} else if (new->flags & IRQF_TRIGGER_MASK) {",
            "\t\tunsigned int nmsk = new->flags & IRQF_TRIGGER_MASK;",
            "\t\tunsigned int omsk = irqd_get_trigger_type(&desc->irq_data);",
            "",
            "\t\tif (nmsk != omsk)",
            "\t\t\t/* hope the handler works with current  trigger mode */",
            "\t\t\tpr_warn(\"irq %d uses trigger mode %u; requested %u\\n\",",
            "\t\t\t\tirq, omsk, nmsk);",
            "\t}",
            "",
            "\t*old_ptr = new;",
            "",
            "\tirq_pm_install_action(desc, new);",
            "",
            "\t/* Reset broken irq detection when installing new handler */",
            "\tdesc->irq_count = 0;",
            "\tdesc->irqs_unhandled = 0;",
            "",
            "\t/*",
            "\t * Check whether we disabled the irq via the spurious handler",
            "\t * before. Reenable it and give it another chance.",
            "\t */",
            "\tif (shared && (desc->istate & IRQS_SPURIOUS_DISABLED)) {",
            "\t\tdesc->istate &= ~IRQS_SPURIOUS_DISABLED;",
            "\t\t__enable_irq(desc);",
            "\t}",
            "",
            "\traw_spin_unlock_irqrestore(&desc->lock, flags);",
            "\tchip_bus_sync_unlock(desc);",
            "\tmutex_unlock(&desc->request_mutex);",
            "",
            "\tirq_setup_timings(desc, new);",
            "",
            "\twake_up_and_wait_for_irq_thread_ready(desc, new);",
            "\twake_up_and_wait_for_irq_thread_ready(desc, new->secondary);",
            "",
            "\tregister_irq_proc(irq, desc);",
            "\tnew->dir = NULL;",
            "\tregister_handler_proc(irq, new);",
            "\treturn 0;",
            "",
            "mismatch:",
            "\tif (!(new->flags & IRQF_PROBE_SHARED)) {",
            "\t\tpr_err(\"Flags mismatch irq %d. %08x (%s) vs. %08x (%s)\\n\",",
            "\t\t       irq, new->flags, new->name, old->flags, old->name);",
            "#ifdef CONFIG_DEBUG_SHIRQ",
            "\t\tdump_stack();",
            "#endif",
            "\t}",
            "\tret = -EBUSY;",
            "",
            "out_unlock:",
            "\traw_spin_unlock_irqrestore(&desc->lock, flags);",
            "",
            "\tif (!desc->action)",
            "\t\tirq_release_resources(desc);",
            "out_bus_unlock:",
            "\tchip_bus_sync_unlock(desc);",
            "\tmutex_unlock(&desc->request_mutex);",
            "",
            "out_thread:",
            "\tif (new->thread) {",
            "\t\tstruct task_struct *t = new->thread;",
            "",
            "\t\tnew->thread = NULL;",
            "\t\tkthread_stop_put(t);",
            "\t}",
            "\tif (new->secondary && new->secondary->thread) {",
            "\t\tstruct task_struct *t = new->secondary->thread;",
            "",
            "\t\tnew->secondary->thread = NULL;",
            "\t\tkthread_stop_put(t);",
            "\t}",
            "out_mput:",
            "\tmodule_put(desc->owner);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "setup_irq_thread, __setup_irq",
          "description": "函数__setup_irq初始化中断处理程序，处理共享中断的兼容性检查，设置触发类型，创建线程处理程序并分配thread_mask位，负责中断激活及资源请求。",
          "similarity": 0.5114536285400391
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/irq/manage.c",
          "start_line": 1014,
          "end_line": 1150,
          "content": [
            "int irq_set_parent(int irq, int parent_irq)",
            "{",
            "\tunsigned long flags;",
            "\tstruct irq_desc *desc = irq_get_desc_lock(irq, &flags, 0);",
            "",
            "\tif (!desc)",
            "\t\treturn -EINVAL;",
            "",
            "\tdesc->parent_irq = parent_irq;",
            "",
            "\tirq_put_desc_unlock(desc, flags);",
            "\treturn 0;",
            "}",
            "static irqreturn_t irq_default_primary_handler(int irq, void *dev_id)",
            "{",
            "\treturn IRQ_WAKE_THREAD;",
            "}",
            "static irqreturn_t irq_nested_primary_handler(int irq, void *dev_id)",
            "{",
            "\tWARN(1, \"Primary handler called for nested irq %d\\n\", irq);",
            "\treturn IRQ_NONE;",
            "}",
            "static irqreturn_t irq_forced_secondary_handler(int irq, void *dev_id)",
            "{",
            "\tWARN(1, \"Secondary action handler called for irq %d\\n\", irq);",
            "\treturn IRQ_NONE;",
            "}",
            "static int irq_wait_for_interrupt(struct irqaction *action)",
            "{",
            "\tfor (;;) {",
            "\t\tset_current_state(TASK_INTERRUPTIBLE);",
            "",
            "\t\tif (kthread_should_stop()) {",
            "\t\t\t/* may need to run one last time */",
            "\t\t\tif (test_and_clear_bit(IRQTF_RUNTHREAD,",
            "\t\t\t\t\t       &action->thread_flags)) {",
            "\t\t\t\t__set_current_state(TASK_RUNNING);",
            "\t\t\t\treturn 0;",
            "\t\t\t}",
            "\t\t\t__set_current_state(TASK_RUNNING);",
            "\t\t\treturn -1;",
            "\t\t}",
            "",
            "\t\tif (test_and_clear_bit(IRQTF_RUNTHREAD,",
            "\t\t\t\t       &action->thread_flags)) {",
            "\t\t\t__set_current_state(TASK_RUNNING);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t\tschedule();",
            "\t}",
            "}",
            "static void irq_finalize_oneshot(struct irq_desc *desc,",
            "\t\t\t\t struct irqaction *action)",
            "{",
            "\tif (!(desc->istate & IRQS_ONESHOT) ||",
            "\t    action->handler == irq_forced_secondary_handler)",
            "\t\treturn;",
            "again:",
            "\tchip_bus_lock(desc);",
            "\traw_spin_lock_irq(&desc->lock);",
            "",
            "\t/*",
            "\t * Implausible though it may be we need to protect us against",
            "\t * the following scenario:",
            "\t *",
            "\t * The thread is faster done than the hard interrupt handler",
            "\t * on the other CPU. If we unmask the irq line then the",
            "\t * interrupt can come in again and masks the line, leaves due",
            "\t * to IRQS_INPROGRESS and the irq line is masked forever.",
            "\t *",
            "\t * This also serializes the state of shared oneshot handlers",
            "\t * versus \"desc->threads_oneshot |= action->thread_mask;\" in",
            "\t * irq_wake_thread(). See the comment there which explains the",
            "\t * serialization.",
            "\t */",
            "\tif (unlikely(irqd_irq_inprogress(&desc->irq_data))) {",
            "\t\traw_spin_unlock_irq(&desc->lock);",
            "\t\tchip_bus_sync_unlock(desc);",
            "\t\tcpu_relax();",
            "\t\tgoto again;",
            "\t}",
            "",
            "\t/*",
            "\t * Now check again, whether the thread should run. Otherwise",
            "\t * we would clear the threads_oneshot bit of this thread which",
            "\t * was just set.",
            "\t */",
            "\tif (test_bit(IRQTF_RUNTHREAD, &action->thread_flags))",
            "\t\tgoto out_unlock;",
            "",
            "\tdesc->threads_oneshot &= ~action->thread_mask;",
            "",
            "\tif (!desc->threads_oneshot && !irqd_irq_disabled(&desc->irq_data) &&",
            "\t    irqd_irq_masked(&desc->irq_data))",
            "\t\tunmask_threaded_irq(desc);",
            "",
            "out_unlock:",
            "\traw_spin_unlock_irq(&desc->lock);",
            "\tchip_bus_sync_unlock(desc);",
            "}",
            "static void",
            "irq_thread_check_affinity(struct irq_desc *desc, struct irqaction *action)",
            "{",
            "\tcpumask_var_t mask;",
            "\tbool valid = true;",
            "",
            "\tif (!test_and_clear_bit(IRQTF_AFFINITY, &action->thread_flags))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * In case we are out of memory we set IRQTF_AFFINITY again and",
            "\t * try again next time",
            "\t */",
            "\tif (!alloc_cpumask_var(&mask, GFP_KERNEL)) {",
            "\t\tset_bit(IRQTF_AFFINITY, &action->thread_flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\traw_spin_lock_irq(&desc->lock);",
            "\t/*",
            "\t * This code is triggered unconditionally. Check the affinity",
            "\t * mask pointer. For CPU_MASK_OFFSTACK=n this is optimized out.",
            "\t */",
            "\tif (cpumask_available(desc->irq_common_data.affinity)) {",
            "\t\tconst struct cpumask *m;",
            "",
            "\t\tm = irq_data_get_effective_affinity_mask(&desc->irq_data);",
            "\t\tcpumask_copy(mask, m);",
            "\t} else {",
            "\t\tvalid = false;",
            "\t}",
            "\traw_spin_unlock_irq(&desc->lock);",
            "",
            "\tif (valid)",
            "\t\tset_cpus_allowed_ptr(current, mask);",
            "\tfree_cpumask_var(mask);",
            "}"
          ],
          "function_name": "irq_set_parent, irq_default_primary_handler, irq_nested_primary_handler, irq_forced_secondary_handler, irq_wait_for_interrupt, irq_finalize_oneshot, irq_thread_check_affinity",
          "description": "定义中断父子关系设置(irq_set_parent)，实现默认/嵌套/强制次级中断处理程序，处理一次性中断终结逻辑(irq_finalize_oneshot)，提供线程亲和性检查(irq_thread_check_affinity)",
          "similarity": 0.5068293809890747
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/manage.c",
          "start_line": 1,
          "end_line": 29,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 1992, 1998-2006 Linus Torvalds, Ingo Molnar",
            " * Copyright (C) 2005-2006 Thomas Gleixner",
            " *",
            " * This file contains driver APIs to the irq subsystem.",
            " */",
            "",
            "#define pr_fmt(fmt) \"genirq: \" fmt",
            "",
            "#include <linux/irq.h>",
            "#include <linux/kthread.h>",
            "#include <linux/module.h>",
            "#include <linux/random.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/irqdomain.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/isolation.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/task_work.h>",
            "",
            "#include \"internals.h\"",
            "",
            "#if defined(CONFIG_IRQ_FORCED_THREADING) && !defined(CONFIG_PREEMPT_RT)",
            "DEFINE_STATIC_KEY_FALSE(force_irqthreads_key);",
            ""
          ],
          "function_name": null,
          "description": "定义用于控制强制中断线程化的静态键变量force_irqthreads_key，为后续相关功能启用做准备。",
          "similarity": 0.5055292844772339
        }
      ]
    },
    {
      "source_file": "kernel/sched/clock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:58:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\clock.c`\n\n---\n\n# `sched/clock.c` 技术文档\n\n## 1. 文件概述\n\n`sched/clock.c` 实现了 Linux 内核中用于调度器的高分辨率时间戳机制 `sched_clock()`，特别针对 **不稳定 CPU 时钟源**（如 TSC 在某些硬件上不可靠）的场景。该文件提供了一个在单 CPU 上单调递增、高精度（纳秒级）、可在任意上下文（包括 NMI）中调用的时间源，并通过混合全局时间（GTOD）与本地时钟（如 TSC）来在多核系统中尽量减少时钟漂移。\n\n**重要警告**：不同 CPU 上的 `cpu_clock(i)` 与 `cpu_clock(j)`（i ≠ j）之间 **不保证全局单调性**，时间可能“倒退”。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数 | 说明 |\n|------|------|\n| `sched_clock()` | 弱符号默认实现，基于 jiffies 提供低精度时间戳；架构可覆盖 |\n| `local_clock()` | 宏定义，等价于当前 CPU 的 `cpu_clock(smp_processor_id())` |\n| `cpu_clock(int cpu)` | 返回指定 CPU 的高分辨率时间戳（纳秒） |\n| `sched_clock_stable()` | 判断当前系统是否已进入“稳定时钟”模式（TSC 可靠） |\n| `clear_sched_clock_stable()` | 标记时钟为不稳定（如检测到 TSC 异常），触发修复流程 |\n| `sched_clock_init()` / `sched_clock_init_late()` | 初始化时钟子系统，分早期和晚期阶段 |\n\n### 关键数据结构\n\n```c\nstruct sched_clock_data {\n    u64 tick_raw;   // 上次更新时的原始 sched_clock() 值（如 TSC）\n    u64 tick_gtod;  // 上次更新时的全局时间（ktime_get_ns()）\n    u64 clock;      // 当前推算出的本地高精度单调时间\n};\n```\n\n- 每个 CPU 拥有一个 `sched_clock_data` 实例（`per_cpu` 变量）\n- 全局偏移量：\n  - `__sched_clock_offset`：原始时钟到稳定时间的偏移\n  - `__gtod_offset`：GTOD 到稳定时间的偏移\n\n### 静态键（Static Keys）\n\n- `sched_clock_running`：标记时钟子系统是否已初始化\n- `__sched_clock_stable`：标记时钟源是否稳定（TSC 可靠）\n- `__sched_clock_stable_early`：启动早期假设时钟稳定，避免多次切换\n\n## 3. 关键实现\n\n### 3.1 两种模式\n\n- **稳定模式**（`CONFIG_HAVE_UNSTABLE_SCHED_CLOCK` 未定义）：  \n  直接使用架构提供的 `sched_clock()`，假定其全局同步且高精度（如 ARM64 的 arch counter）。\n\n- **不稳定模式**（`CONFIG_HAVE_UNSTABLE_SCHED_CLOCK` 定义）：  \n  混合 GTOD（`ktime_get_ns()`）与原始 `sched_clock()`（如 TSC）：\n  - 以 GTOD 为基准，利用 `sched_clock()` 的高分辨率 delta 提升精度\n  - 通过 `__sched_clock_offset` 和 `__gtod_offset` 对齐两个时钟源\n\n### 3.2 时钟对齐与漂移控制\n\n- 初始化时通过 `__sched_clock_gtod_offset()` 计算初始偏移量，确保切换时连续\n- `sched_clock_local()` 函数实现核心逻辑：\n  - 计算自上次更新以来的原始时钟增量（`delta = now - tick_raw`）\n  - 将 GTOD 基准时间（`tick_gtod + __gtod_offset`）加上 `delta` 得到新时间\n  - 使用 `wrap_min`/`wrap_max` 处理 64 位回绕，并限制时间跳跃范围（防止 TSC 异常）\n\n### 3.3 稳定性动态切换\n\n- **启动时假设稳定**：`__sched_clock_stable_early = 1`\n- **晚期初始化**（`late_initcall`）：\n  - 若仍认为稳定，则调用 `__set_sched_clock_stable()` 完成对齐并启用稳定模式\n  - 若驱动（如 ACPI/Intel Idle）标记 TSC 不稳定，则调用 `clear_sched_clock_stable()`\n- **不稳定处理**：\n  - 调度工作队列 `sched_clock_work`\n  - 重新以 GTOD 为基准重置所有 CPU 的 `sched_clock_data`\n\n### 3.4 中断与抢占安全\n\n- 关键操作（如 stamp、offset 计算）使用 `local_irq_disable()` 保证原子性\n- `notrace` 属性避免被 ftrace 拦截，确保在 NMI 等上下文中可用\n\n## 4. 依赖关系\n\n- **时间子系统**：\n  - 依赖 `ktime_get_ns()`（GTOD，来自 `kernel/time/`）\n  - 依赖 `jiffies` 和 `HZ`（用于默认 `sched_clock` 实现）\n- **调度器**：为 `kernel/sched/` 提供高精度时间戳（如 `rq->clock` 更新）\n- **时钟事件/源**：与 `tick` 子系统交互（`TICK_DEP_BIT_CLOCK_UNSTABLE`）\n- **架构支持**：\n  - 若架构定义 `CONFIG_HAVE_UNSTABLE_SCHED_CLOCK`，则启用混合模式\n  - 架构可提供自己的 `sched_clock()` 实现（如 x86 使用 TSC）\n\n## 5. 使用场景\n\n- **调度器时间统计**：计算任务运行时间、就绪队列时钟等\n- **延迟跟踪**：`ftrace`、`perf` 等性能工具依赖 `local_clock()` 获取精确时间戳\n- **锁竞争分析**：`lockdep` 使用 `sched_clock()` 记录锁持有时间\n- **RCU、中断处理**：需要高精度、低开销时间戳的内核子系统\n- **虚拟化与电源管理**：在 CPU 进入/退出 idle 时校正时钟（通过 `sched_clock_idle_*` 钩子，虽未在本文件实现但相关）",
      "similarity": 0.5788925886154175,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/clock.c",
          "start_line": 247,
          "end_line": 369,
          "content": [
            "static __always_inline u64 wrap_min(u64 x, u64 y)",
            "{",
            "\treturn (s64)(x - y) < 0 ? x : y;",
            "}",
            "static __always_inline u64 wrap_max(u64 x, u64 y)",
            "{",
            "\treturn (s64)(x - y) > 0 ? x : y;",
            "}",
            "static __always_inline u64 sched_clock_local(struct sched_clock_data *scd)",
            "{",
            "\tu64 now, clock, old_clock, min_clock, max_clock, gtod;",
            "\ts64 delta;",
            "",
            "again:",
            "\tnow = sched_clock_noinstr();",
            "\tdelta = now - scd->tick_raw;",
            "\tif (unlikely(delta < 0))",
            "\t\tdelta = 0;",
            "",
            "\told_clock = scd->clock;",
            "",
            "\t/*",
            "\t * scd->clock = clamp(scd->tick_gtod + delta,",
            "\t *\t\t      max(scd->tick_gtod, scd->clock),",
            "\t *\t\t      scd->tick_gtod + TICK_NSEC);",
            "\t */",
            "",
            "\tgtod = scd->tick_gtod + __gtod_offset;",
            "\tclock = gtod + delta;",
            "\tmin_clock = wrap_max(gtod, old_clock);",
            "\tmax_clock = wrap_max(old_clock, gtod + TICK_NSEC);",
            "",
            "\tclock = wrap_max(clock, min_clock);",
            "\tclock = wrap_min(clock, max_clock);",
            "",
            "\tif (!raw_try_cmpxchg64(&scd->clock, &old_clock, clock))",
            "\t\tgoto again;",
            "",
            "\treturn clock;",
            "}",
            "noinstr u64 local_clock_noinstr(void)",
            "{",
            "\tu64 clock;",
            "",
            "\tif (static_branch_likely(&__sched_clock_stable))",
            "\t\treturn sched_clock_noinstr() + __sched_clock_offset;",
            "",
            "\tif (!static_branch_likely(&sched_clock_running))",
            "\t\treturn sched_clock_noinstr();",
            "",
            "\tclock = sched_clock_local(this_scd());",
            "",
            "\treturn clock;",
            "}",
            "u64 local_clock(void)",
            "{",
            "\tu64 now;",
            "\tpreempt_disable_notrace();",
            "\tnow = local_clock_noinstr();",
            "\tpreempt_enable_notrace();",
            "\treturn now;",
            "}",
            "static notrace u64 sched_clock_remote(struct sched_clock_data *scd)",
            "{",
            "\tstruct sched_clock_data *my_scd = this_scd();",
            "\tu64 this_clock, remote_clock;",
            "\tu64 *ptr, old_val, val;",
            "",
            "#if BITS_PER_LONG != 64",
            "again:",
            "\t/*",
            "\t * Careful here: The local and the remote clock values need to",
            "\t * be read out atomic as we need to compare the values and",
            "\t * then update either the local or the remote side. So the",
            "\t * cmpxchg64 below only protects one readout.",
            "\t *",
            "\t * We must reread via sched_clock_local() in the retry case on",
            "\t * 32-bit kernels as an NMI could use sched_clock_local() via the",
            "\t * tracer and hit between the readout of",
            "\t * the low 32-bit and the high 32-bit portion.",
            "\t */",
            "\tthis_clock = sched_clock_local(my_scd);",
            "\t/*",
            "\t * We must enforce atomic readout on 32-bit, otherwise the",
            "\t * update on the remote CPU can hit inbetween the readout of",
            "\t * the low 32-bit and the high 32-bit portion.",
            "\t */",
            "\tremote_clock = cmpxchg64(&scd->clock, 0, 0);",
            "#else",
            "\t/*",
            "\t * On 64-bit kernels the read of [my]scd->clock is atomic versus the",
            "\t * update, so we can avoid the above 32-bit dance.",
            "\t */",
            "\tsched_clock_local(my_scd);",
            "again:",
            "\tthis_clock = my_scd->clock;",
            "\tremote_clock = scd->clock;",
            "#endif",
            "",
            "\t/*",
            "\t * Use the opportunity that we have both locks",
            "\t * taken to couple the two clocks: we take the",
            "\t * larger time as the latest time for both",
            "\t * runqueues. (this creates monotonic movement)",
            "\t */",
            "\tif (likely((s64)(remote_clock - this_clock) < 0)) {",
            "\t\tptr = &scd->clock;",
            "\t\told_val = remote_clock;",
            "\t\tval = this_clock;",
            "\t} else {",
            "\t\t/*",
            "\t\t * Should be rare, but possible:",
            "\t\t */",
            "\t\tptr = &my_scd->clock;",
            "\t\told_val = this_clock;",
            "\t\tval = remote_clock;",
            "\t}",
            "",
            "\tif (!try_cmpxchg64(ptr, &old_val, val))",
            "\t\tgoto again;",
            "",
            "\treturn val;",
            "}"
          ],
          "function_name": "wrap_min, wrap_max, sched_clock_local, local_clock_noinstr, local_clock, sched_clock_remote",
          "description": "提供时间戳范围约束函数(wrap_min/wrap_max)及本地/远程时钟访问接口(sched_clock_local/local_clock_noinstr)，通过cmpxchg64原子操作保障多CPU间时间戳的一致性，特别处理32位系统下的原子读取问题，确保时间戳单调递增。",
          "similarity": 0.6240814924240112
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/clock.c",
          "start_line": 1,
          "end_line": 61,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * sched_clock() for unstable CPU clocks",
            " *",
            " *  Copyright (C) 2008 Red Hat, Inc., Peter Zijlstra",
            " *",
            " *  Updates and enhancements:",
            " *    Copyright (C) 2008 Red Hat, Inc. Steven Rostedt <srostedt@redhat.com>",
            " *",
            " * Based on code by:",
            " *   Ingo Molnar <mingo@redhat.com>",
            " *   Guillaume Chazarain <guichaz@gmail.com>",
            " *",
            " *",
            " * What this file implements:",
            " *",
            " * cpu_clock(i) provides a fast (execution time) high resolution",
            " * clock with bounded drift between CPUs. The value of cpu_clock(i)",
            " * is monotonic for constant i. The timestamp returned is in nanoseconds.",
            " *",
            " * ######################### BIG FAT WARNING ##########################",
            " * # when comparing cpu_clock(i) to cpu_clock(j) for i != j, time can #",
            " * # go backwards !!                                                  #",
            " * ####################################################################",
            " *",
            " * There is no strict promise about the base, although it tends to start",
            " * at 0 on boot (but people really shouldn't rely on that).",
            " *",
            " * cpu_clock(i)       -- can be used from any context, including NMI.",
            " * local_clock()      -- is cpu_clock() on the current CPU.",
            " *",
            " * sched_clock_cpu(i)",
            " *",
            " * How it is implemented:",
            " *",
            " * The implementation either uses sched_clock() when",
            " * !CONFIG_HAVE_UNSTABLE_SCHED_CLOCK, which means in that case the",
            " * sched_clock() is assumed to provide these properties (mostly it means",
            " * the architecture provides a globally synchronized highres time source).",
            " *",
            " * Otherwise it tries to create a semi stable clock from a mixture of other",
            " * clocks, including:",
            " *",
            " *  - GTOD (clock monotonic)",
            " *  - sched_clock()",
            " *  - explicit idle events",
            " *",
            " * We use GTOD as base and use sched_clock() deltas to improve resolution. The",
            " * deltas are filtered to provide monotonicity and keeping it within an",
            " * expected window.",
            " *",
            " * Furthermore, explicit sleep and wakeup hooks allow us to account for time",
            " * that is otherwise invisible (TSC gets stopped).",
            " *",
            " */",
            "",
            "/*",
            " * Scheduler clock - returns current time in nanosec units.",
            " * This is default implementation.",
            " * Architectures and sub-architectures can override this.",
            " */"
          ],
          "function_name": null,
          "description": "定义sched_clock函数，用于获取当前时间戳，在非稳定时钟配置下依赖架构提供的全局同步高精度时钟，通过混合GTOD、sched_clock及显式空闲事件实现半稳定时钟，支持跨CPU比较但需注意非单调性警告。",
          "similarity": 0.5519065260887146
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/clock.c",
          "start_line": 388,
          "end_line": 477,
          "content": [
            "notrace u64 sched_clock_cpu(int cpu)",
            "{",
            "\tstruct sched_clock_data *scd;",
            "\tu64 clock;",
            "",
            "\tif (sched_clock_stable())",
            "\t\treturn sched_clock() + __sched_clock_offset;",
            "",
            "\tif (!static_branch_likely(&sched_clock_running))",
            "\t\treturn sched_clock();",
            "",
            "\tpreempt_disable_notrace();",
            "\tscd = cpu_sdc(cpu);",
            "",
            "\tif (cpu != smp_processor_id())",
            "\t\tclock = sched_clock_remote(scd);",
            "\telse",
            "\t\tclock = sched_clock_local(scd);",
            "\tpreempt_enable_notrace();",
            "",
            "\treturn clock;",
            "}",
            "notrace void sched_clock_tick(void)",
            "{",
            "\tstruct sched_clock_data *scd;",
            "",
            "\tif (sched_clock_stable())",
            "\t\treturn;",
            "",
            "\tif (!static_branch_likely(&sched_clock_running))",
            "\t\treturn;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tscd = this_scd();",
            "\t__scd_stamp(scd);",
            "\tsched_clock_local(scd);",
            "}",
            "notrace void sched_clock_tick_stable(void)",
            "{",
            "\tif (!sched_clock_stable())",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Called under watchdog_lock.",
            "\t *",
            "\t * The watchdog just found this TSC to (still) be stable, so now is a",
            "\t * good moment to update our __gtod_offset. Because once we find the",
            "\t * TSC to be unstable, any computation will be computing crap.",
            "\t */",
            "\tlocal_irq_disable();",
            "\t__sched_clock_gtod_offset();",
            "\tlocal_irq_enable();",
            "}",
            "notrace void sched_clock_idle_sleep_event(void)",
            "{",
            "\tsched_clock_cpu(smp_processor_id());",
            "}",
            "notrace void sched_clock_idle_wakeup_event(void)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tif (sched_clock_stable())",
            "\t\treturn;",
            "",
            "\tif (unlikely(timekeeping_suspended))",
            "\t\treturn;",
            "",
            "\tlocal_irq_save(flags);",
            "\tsched_clock_tick();",
            "\tlocal_irq_restore(flags);",
            "}",
            "void __init sched_clock_init(void)",
            "{",
            "\tstatic_branch_inc(&sched_clock_running);",
            "\tlocal_irq_disable();",
            "\tgeneric_sched_clock_init();",
            "\tlocal_irq_enable();",
            "}",
            "notrace u64 sched_clock_cpu(int cpu)",
            "{",
            "\tif (!static_branch_likely(&sched_clock_running))",
            "\t\treturn 0;",
            "",
            "\treturn sched_clock();",
            "}",
            "notrace u64 __weak running_clock(void)",
            "{",
            "\treturn local_clock();",
            "}"
          ],
          "function_name": "sched_clock_cpu, sched_clock_tick, sched_clock_tick_stable, sched_clock_idle_sleep_event, sched_clock_idle_wakeup_event, sched_clock_init, sched_clock_cpu, running_clock",
          "description": "实现CPU级时间戳查询(sched_clock_cpu)、时钟滴答事件处理(sched_clock_tick)、空闲状态事件记录(sched_clock_idle_*event)及初始化流程(sched_clock_init)，包含稳定/非稳定时钟模式切换逻辑，通过running_clock弱定义提供默认本地时钟访问接口。",
          "similarity": 0.5473415851593018
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/clock.c",
          "start_line": 62,
          "end_line": 177,
          "content": [
            "notrace unsigned long long __weak sched_clock(void)",
            "{",
            "\treturn (unsigned long long)(jiffies - INITIAL_JIFFIES)",
            "\t\t\t\t\t* (NSEC_PER_SEC / HZ);",
            "}",
            "notrace int sched_clock_stable(void)",
            "{",
            "\treturn static_branch_likely(&__sched_clock_stable);",
            "}",
            "notrace static void __scd_stamp(struct sched_clock_data *scd)",
            "{",
            "\tscd->tick_gtod = ktime_get_ns();",
            "\tscd->tick_raw = sched_clock();",
            "}",
            "notrace static void __set_sched_clock_stable(void)",
            "{",
            "\tstruct sched_clock_data *scd;",
            "",
            "\t/*",
            "\t * Since we're still unstable and the tick is already running, we have",
            "\t * to disable IRQs in order to get a consistent scd->tick* reading.",
            "\t */",
            "\tlocal_irq_disable();",
            "\tscd = this_scd();",
            "\t/*",
            "\t * Attempt to make the (initial) unstable->stable transition continuous.",
            "\t */",
            "\t__sched_clock_offset = (scd->tick_gtod + __gtod_offset) - (scd->tick_raw);",
            "\tlocal_irq_enable();",
            "",
            "\tprintk(KERN_INFO \"sched_clock: Marking stable (%lld, %lld)->(%lld, %lld)\\n\",",
            "\t\t\tscd->tick_gtod, __gtod_offset,",
            "\t\t\tscd->tick_raw,  __sched_clock_offset);",
            "",
            "\tstatic_branch_enable(&__sched_clock_stable);",
            "\ttick_dep_clear(TICK_DEP_BIT_CLOCK_UNSTABLE);",
            "}",
            "notrace static void __sched_clock_work(struct work_struct *work)",
            "{",
            "\tstruct sched_clock_data *scd;",
            "\tint cpu;",
            "",
            "\t/* take a current timestamp and set 'now' */",
            "\tpreempt_disable();",
            "\tscd = this_scd();",
            "\t__scd_stamp(scd);",
            "\tscd->clock = scd->tick_gtod + __gtod_offset;",
            "\tpreempt_enable();",
            "",
            "\t/* clone to all CPUs */",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(sched_clock_data, cpu) = *scd;",
            "",
            "\tprintk(KERN_WARNING \"TSC found unstable after boot, most likely due to broken BIOS. Use 'tsc=unstable'.\\n\");",
            "\tprintk(KERN_INFO \"sched_clock: Marking unstable (%lld, %lld)<-(%lld, %lld)\\n\",",
            "\t\t\tscd->tick_gtod, __gtod_offset,",
            "\t\t\tscd->tick_raw,  __sched_clock_offset);",
            "",
            "\tstatic_branch_disable(&__sched_clock_stable);",
            "}",
            "notrace static void __clear_sched_clock_stable(void)",
            "{",
            "\tif (!sched_clock_stable())",
            "\t\treturn;",
            "",
            "\ttick_dep_set(TICK_DEP_BIT_CLOCK_UNSTABLE);",
            "\tschedule_work(&sched_clock_work);",
            "}",
            "notrace void clear_sched_clock_stable(void)",
            "{",
            "\t__sched_clock_stable_early = 0;",
            "",
            "\tsmp_mb(); /* matches sched_clock_init_late() */",
            "",
            "\tif (static_key_count(&sched_clock_running.key) == 2)",
            "\t\t__clear_sched_clock_stable();",
            "}",
            "notrace static void __sched_clock_gtod_offset(void)",
            "{",
            "\tstruct sched_clock_data *scd = this_scd();",
            "",
            "\t__scd_stamp(scd);",
            "\t__gtod_offset = (scd->tick_raw + __sched_clock_offset) - scd->tick_gtod;",
            "}",
            "void __init sched_clock_init(void)",
            "{",
            "\t/*",
            "\t * Set __gtod_offset such that once we mark sched_clock_running,",
            "\t * sched_clock_tick() continues where sched_clock() left off.",
            "\t *",
            "\t * Even if TSC is buggered, we're still UP at this point so it",
            "\t * can't really be out of sync.",
            "\t */",
            "\tlocal_irq_disable();",
            "\t__sched_clock_gtod_offset();",
            "\tlocal_irq_enable();",
            "",
            "\tstatic_branch_inc(&sched_clock_running);",
            "}",
            "static int __init sched_clock_init_late(void)",
            "{",
            "\tstatic_branch_inc(&sched_clock_running);",
            "\t/*",
            "\t * Ensure that it is impossible to not do a static_key update.",
            "\t *",
            "\t * Either {set,clear}_sched_clock_stable() must see sched_clock_running",
            "\t * and do the update, or we must see their __sched_clock_stable_early",
            "\t * and do the update, or both.",
            "\t */",
            "\tsmp_mb(); /* matches {set,clear}_sched_clock_stable() */",
            "",
            "\tif (__sched_clock_stable_early)",
            "\t\t__set_sched_clock_stable();",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sched_clock, sched_clock_stable, __scd_stamp, __set_sched_clock_stable, __sched_clock_work, __clear_sched_clock_stable, clear_sched_clock_stable, __sched_clock_gtod_offset, sched_clock_init, sched_clock_init_late",
          "description": "实现调度器时钟状态管理，包含sched_clock函数默认实现、稳定性检测、时钟数据同步逻辑(__set_sched_clock_stable/__clear_sched_clock_stable)、初始化流程(sched_clock_init/sched_clock_init_late)及工作队列处理(__sched_clock_work)，用于动态调整时钟偏移与稳定性标志。",
          "similarity": 0.5119156837463379
        }
      ]
    }
  ]
}