{
  "query": "real-time scheduling requirements",
  "timestamp": "2025-12-25 23:55:31",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.6134049892425537,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1,
          "end_line": 56,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Real-Time Scheduling Class (mapped to the SCHED_FIFO and SCHED_RR",
            " * policies)",
            " */",
            "",
            "int sched_rr_timeslice = RR_TIMESLICE;",
            "/* More than 4 hours if BW_SHIFT equals 20. */",
            "static const u64 max_rt_runtime = MAX_BW;",
            "",
            "/*",
            " * period over which we measure -rt task CPU usage in us.",
            " * default: 1s",
            " */",
            "int sysctl_sched_rt_period = 1000000;",
            "",
            "/*",
            " * part of the period that we allow rt tasks to run in us.",
            " * default: 0.95s",
            " */",
            "int sysctl_sched_rt_runtime = 950000;",
            "",
            "#ifdef CONFIG_SYSCTL",
            "static int sysctl_sched_rr_timeslice = (MSEC_PER_SEC * RR_TIMESLICE) / HZ;",
            "static int sched_rt_handler(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos);",
            "static int sched_rr_handler(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos);",
            "static struct ctl_table sched_rt_sysctls[] = {",
            "\t{",
            "\t\t.procname       = \"sched_rt_period_us\",",
            "\t\t.data           = &sysctl_sched_rt_period,",
            "\t\t.maxlen         = sizeof(int),",
            "\t\t.mode           = 0644,",
            "\t\t.proc_handler   = sched_rt_handler,",
            "\t\t.extra1         = SYSCTL_ONE,",
            "\t\t.extra2         = SYSCTL_INT_MAX,",
            "\t},",
            "\t{",
            "\t\t.procname       = \"sched_rt_runtime_us\",",
            "\t\t.data           = &sysctl_sched_rt_runtime,",
            "\t\t.maxlen         = sizeof(int),",
            "\t\t.mode           = 0644,",
            "\t\t.proc_handler   = sched_rt_handler,",
            "\t\t.extra1         = SYSCTL_NEG_ONE,",
            "\t\t.extra2         = (void *)&sysctl_sched_rt_period,",
            "\t},",
            "\t{",
            "\t\t.procname       = \"sched_rr_timeslice_ms\",",
            "\t\t.data           = &sysctl_sched_rr_timeslice,",
            "\t\t.maxlen         = sizeof(int),",
            "\t\t.mode           = 0644,",
            "\t\t.proc_handler   = sched_rr_handler,",
            "\t},",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义实时调度类的配置参数及sysctl接口，设置默认的实时任务周期和运行时间，并注册相应的proc_handler以允许动态调整。",
          "similarity": 0.6117488145828247
        },
        {
          "chunk_id": 17,
          "file_path": "kernel/sched/rt.c",
          "start_line": 2533,
          "end_line": 2686,
          "content": [
            "static void watchdog(struct rq *rq, struct task_struct *p)",
            "{",
            "\tunsigned long soft, hard;",
            "",
            "\t/* max may change after cur was read, this will be fixed next tick */",
            "\tsoft = task_rlimit(p, RLIMIT_RTTIME);",
            "\thard = task_rlimit_max(p, RLIMIT_RTTIME);",
            "",
            "\tif (soft != RLIM_INFINITY) {",
            "\t\tunsigned long next;",
            "",
            "\t\tif (p->rt.watchdog_stamp != jiffies) {",
            "\t\t\tp->rt.timeout++;",
            "\t\t\tp->rt.watchdog_stamp = jiffies;",
            "\t\t}",
            "",
            "\t\tnext = DIV_ROUND_UP(min(soft, hard), USEC_PER_SEC/HZ);",
            "\t\tif (p->rt.timeout > next) {",
            "\t\t\tposix_cputimers_rt_watchdog(&p->posix_cputimers,",
            "\t\t\t\t\t\t    p->se.sum_exec_runtime);",
            "\t\t}",
            "\t}",
            "}",
            "static inline void watchdog(struct rq *rq, struct task_struct *p) { }",
            "static void task_tick_rt(struct rq *rq, struct task_struct *p, int queued)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tupdate_rt_rq_load_avg(rq_clock_pelt(rq), rq, 1);",
            "",
            "\twatchdog(rq, p);",
            "",
            "\t/*",
            "\t * RR tasks need a special form of timeslice management.",
            "\t * FIFO tasks have no timeslices.",
            "\t */",
            "\tif (p->policy != SCHED_RR)",
            "\t\treturn;",
            "",
            "\tif (--p->rt.time_slice)",
            "\t\treturn;",
            "",
            "\tp->rt.time_slice = sched_rr_timeslice;",
            "",
            "\t/*",
            "\t * Requeue to the end of queue if we (and all of our ancestors) are not",
            "\t * the only element on the queue",
            "\t */",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tif (rt_se->run_list.prev != rt_se->run_list.next) {",
            "\t\t\trequeue_task_rt(rq, p, 0);",
            "\t\t\tresched_curr(rq);",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "}",
            "static unsigned int get_rr_interval_rt(struct rq *rq, struct task_struct *task)",
            "{",
            "\t/*",
            "\t * Time slice is 0 for SCHED_FIFO tasks",
            "\t */",
            "\tif (task->policy == SCHED_RR)",
            "\t\treturn sched_rr_timeslice;",
            "\telse",
            "\t\treturn 0;",
            "}",
            "static int task_is_throttled_rt(struct task_struct *p, int cpu)",
            "{",
            "\tstruct rt_rq *rt_rq;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\trt_rq = task_group(p)->rt_rq[cpu];",
            "#else",
            "\trt_rq = &cpu_rq(cpu)->rt;",
            "#endif",
            "",
            "\treturn rt_rq_throttled(rt_rq);",
            "}",
            "static inline int tg_has_rt_tasks(struct task_group *tg)",
            "{",
            "\tstruct task_struct *task;",
            "\tstruct css_task_iter it;",
            "\tint ret = 0;",
            "",
            "\t/*",
            "\t * Autogroups do not have RT tasks; see autogroup_create().",
            "\t */",
            "\tif (task_group_is_autogroup(tg))",
            "\t\treturn 0;",
            "",
            "\tcss_task_iter_start(&tg->css, 0, &it);",
            "\twhile (!ret && (task = css_task_iter_next(&it)))",
            "\t\tret |= rt_task(task);",
            "\tcss_task_iter_end(&it);",
            "",
            "\treturn ret;",
            "}",
            "static int tg_rt_schedulable(struct task_group *tg, void *data)",
            "{",
            "\tstruct rt_schedulable_data *d = data;",
            "\tstruct task_group *child;",
            "\tunsigned long total, sum = 0;",
            "\tu64 period, runtime;",
            "",
            "\tperiod = ktime_to_ns(tg->rt_bandwidth.rt_period);",
            "\truntime = tg->rt_bandwidth.rt_runtime;",
            "",
            "\tif (tg == d->tg) {",
            "\t\tperiod = d->rt_period;",
            "\t\truntime = d->rt_runtime;",
            "\t}",
            "",
            "\t/*",
            "\t * Cannot have more runtime than the period.",
            "\t */",
            "\tif (runtime > period && runtime != RUNTIME_INF)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Ensure we don't starve existing RT tasks if runtime turns zero.",
            "\t */",
            "\tif (rt_bandwidth_enabled() && !runtime &&",
            "\t    tg->rt_bandwidth.rt_runtime && tg_has_rt_tasks(tg))",
            "\t\treturn -EBUSY;",
            "",
            "\ttotal = to_ratio(period, runtime);",
            "",
            "\t/*",
            "\t * Nobody can have more than the global setting allows.",
            "\t */",
            "\tif (total > to_ratio(global_rt_period(), global_rt_runtime()))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * The sum of our children's runtime should not exceed our own.",
            "\t */",
            "\tlist_for_each_entry_rcu(child, &tg->children, siblings) {",
            "\t\tperiod = ktime_to_ns(child->rt_bandwidth.rt_period);",
            "\t\truntime = child->rt_bandwidth.rt_runtime;",
            "",
            "\t\tif (child == d->tg) {",
            "\t\t\tperiod = d->rt_period;",
            "\t\t\truntime = d->rt_runtime;",
            "\t\t}",
            "",
            "\t\tsum += to_ratio(period, runtime);",
            "\t}",
            "",
            "\tif (sum > total)",
            "\t\treturn -EINVAL;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "watchdog, watchdog, task_tick_rt, get_rr_interval_rt, task_is_throttled_rt, tg_has_rt_tasks, tg_rt_schedulable",
          "description": "实现实时任务的超时监控、时间片管理、任务组资源配额验证及实时任务运行时间统计功能，支持轮转调度下的时间片重置和任务重排。",
          "similarity": 0.608569860458374
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/rt.c",
          "start_line": 57,
          "end_line": 159,
          "content": [
            "static int __init sched_rt_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", sched_rt_sysctls);",
            "\treturn 0;",
            "}",
            "void init_rt_rq(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_prio_array *array;",
            "\tint i;",
            "",
            "\tarray = &rt_rq->active;",
            "\tfor (i = 0; i < MAX_RT_PRIO; i++) {",
            "\t\tINIT_LIST_HEAD(array->queue + i);",
            "\t\t__clear_bit(i, array->bitmap);",
            "\t}",
            "\t/* delimiter for bitsearch: */",
            "\t__set_bit(MAX_RT_PRIO, array->bitmap);",
            "",
            "#if defined CONFIG_SMP",
            "\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\trt_rq->highest_prio.next = MAX_RT_PRIO-1;",
            "\trt_rq->overloaded = 0;",
            "\tplist_head_init(&rt_rq->pushable_tasks);",
            "#endif /* CONFIG_SMP */",
            "\t/* We start is dequeued state, because no RT tasks are queued */",
            "\trt_rq->rt_queued = 0;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\trt_rq->rt_time = 0;",
            "\trt_rq->rt_throttled = 0;",
            "\trt_rq->rt_runtime = 0;",
            "\traw_spin_lock_init(&rt_rq->rt_runtime_lock);",
            "#endif",
            "}",
            "static enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)",
            "{",
            "\tstruct rt_bandwidth *rt_b =",
            "\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);",
            "\tint idle = 0;",
            "\tint overrun;",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tfor (;;) {",
            "\t\toverrun = hrtimer_forward_now(timer, rt_b->rt_period);",
            "\t\tif (!overrun)",
            "\t\t\tbreak;",
            "",
            "\t\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "\t\tidle = do_sched_rt_period_timer(rt_b, overrun);",
            "\t\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\t}",
            "\tif (idle)",
            "\t\trt_b->rt_period_active = 0;",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "",
            "\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;",
            "}",
            "void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)",
            "{",
            "\trt_b->rt_period = ns_to_ktime(period);",
            "\trt_b->rt_runtime = runtime;",
            "",
            "\traw_spin_lock_init(&rt_b->rt_runtime_lock);",
            "",
            "\thrtimer_init(&rt_b->rt_period_timer, CLOCK_MONOTONIC,",
            "\t\t     HRTIMER_MODE_REL_HARD);",
            "\trt_b->rt_period_timer.function = sched_rt_period_timer;",
            "}",
            "static inline void do_start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tif (!rt_b->rt_period_active) {",
            "\t\trt_b->rt_period_active = 1;",
            "\t\t/*",
            "\t\t * SCHED_DEADLINE updates the bandwidth, as a run away",
            "\t\t * RT task with a DL task could hog a CPU. But DL does",
            "\t\t * not reset the period. If a deadline task was running",
            "\t\t * without an RT task running, it can cause RT tasks to",
            "\t\t * throttle when they start up. Kick the timer right away",
            "\t\t * to update the period.",
            "\t\t */",
            "\t\thrtimer_forward_now(&rt_b->rt_period_timer, ns_to_ktime(0));",
            "\t\thrtimer_start_expires(&rt_b->rt_period_timer,",
            "\t\t\t\t      HRTIMER_MODE_ABS_PINNED_HARD);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}",
            "static void start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)",
            "\t\treturn;",
            "",
            "\tdo_start_rt_bandwidth(rt_b);",
            "}",
            "static void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\thrtimer_cancel(&rt_b->rt_period_timer);",
            "}",
            "void unregister_rt_sched_group(struct task_group *tg)",
            "{",
            "\tif (tg->rt_se)",
            "\t\tdestroy_rt_bandwidth(&tg->rt_bandwidth);",
            "}"
          ],
          "function_name": "sched_rt_sysctl_init, init_rt_rq, sched_rt_period_timer, init_rt_bandwidth, do_start_rt_bandwidth, start_rt_bandwidth, destroy_rt_bandwidth, unregister_rt_sched_group",
          "description": "初始化实时调度相关数据结构，管理实时任务周期定时器，控制实时带宽分配与回收，实现基于时间片轮转的调度策略。",
          "similarity": 0.5963677167892456
        },
        {
          "chunk_id": 19,
          "file_path": "kernel/sched/rt.c",
          "start_line": 2869,
          "end_line": 2975,
          "content": [
            "static int sched_rt_global_constraints(void)",
            "{",
            "\tint ret = 0;",
            "",
            "\tmutex_lock(&rt_constraints_mutex);",
            "\tret = __rt_schedulable(NULL, 0, 0);",
            "\tmutex_unlock(&rt_constraints_mutex);",
            "",
            "\treturn ret;",
            "}",
            "int sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk)",
            "{",
            "\t/* Don't accept realtime tasks when there is no way for them to run */",
            "\tif (rt_task(tsk) && tg->rt_bandwidth.rt_runtime == 0)",
            "\t\treturn 0;",
            "",
            "\treturn 1;",
            "}",
            "static int sched_rt_global_constraints(void)",
            "{",
            "\treturn 0;",
            "}",
            "static int sched_rt_global_validate(void)",
            "{",
            "\tif ((sysctl_sched_rt_runtime != RUNTIME_INF) &&",
            "\t\t((sysctl_sched_rt_runtime > sysctl_sched_rt_period) ||",
            "\t\t ((u64)sysctl_sched_rt_runtime *",
            "\t\t\tNSEC_PER_USEC > max_rt_runtime)))",
            "\t\treturn -EINVAL;",
            "",
            "\treturn 0;",
            "}",
            "static void sched_rt_do_global(void)",
            "{",
            "}",
            "static int sched_rt_handler(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos)",
            "{",
            "\tint old_period, old_runtime;",
            "\tstatic DEFINE_MUTEX(mutex);",
            "\tint ret;",
            "",
            "\tmutex_lock(&mutex);",
            "\told_period = sysctl_sched_rt_period;",
            "\told_runtime = sysctl_sched_rt_runtime;",
            "",
            "\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "",
            "\tif (!ret && write) {",
            "\t\tret = sched_rt_global_validate();",
            "\t\tif (ret)",
            "\t\t\tgoto undo;",
            "",
            "\t\tret = sched_dl_global_validate();",
            "\t\tif (ret)",
            "\t\t\tgoto undo;",
            "",
            "\t\tret = sched_rt_global_constraints();",
            "\t\tif (ret)",
            "\t\t\tgoto undo;",
            "",
            "\t\tsched_rt_do_global();",
            "\t\tsched_dl_do_global();",
            "\t}",
            "\tif (0) {",
            "undo:",
            "\t\tsysctl_sched_rt_period = old_period;",
            "\t\tsysctl_sched_rt_runtime = old_runtime;",
            "\t}",
            "\tmutex_unlock(&mutex);",
            "",
            "\treturn ret;",
            "}",
            "static int sched_rr_handler(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos)",
            "{",
            "\tint ret;",
            "\tstatic DEFINE_MUTEX(mutex);",
            "",
            "\tmutex_lock(&mutex);",
            "\tret = proc_dointvec(table, write, buffer, lenp, ppos);",
            "\t/*",
            "\t * Make sure that internally we keep jiffies.",
            "\t * Also, writing zero resets the timeslice to default:",
            "\t */",
            "\tif (!ret && write) {",
            "\t\tsched_rr_timeslice =",
            "\t\t\tsysctl_sched_rr_timeslice <= 0 ? RR_TIMESLICE :",
            "\t\t\tmsecs_to_jiffies(sysctl_sched_rr_timeslice);",
            "",
            "\t\tif (sysctl_sched_rr_timeslice <= 0)",
            "\t\t\tsysctl_sched_rr_timeslice = jiffies_to_msecs(RR_TIMESLICE);",
            "\t}",
            "\tmutex_unlock(&mutex);",
            "",
            "\treturn ret;",
            "}",
            "void print_rt_stats(struct seq_file *m, int cpu)",
            "{",
            "\trt_rq_iter_t iter;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\trcu_read_lock();",
            "\tfor_each_rt_rq(rt_rq, iter, cpu_rq(cpu))",
            "\t\tprint_rt_rq(m, cpu, rt_rq);",
            "\trcu_read_unlock();",
            "}"
          ],
          "function_name": "sched_rt_global_constraints, sched_rt_can_attach, sched_rt_global_constraints, sched_rt_global_validate, sched_rt_do_global, sched_rt_handler, sched_rr_handler, print_rt_stats",
          "description": "实现全局实时调度策略验证与参数控制，包括实时任务附加性检查、全局运行时间/周期合法性校验及系统参数修改时的约束检查与应用。",
          "similarity": 0.5913975238800049
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1449,
          "end_line": 1589,
          "content": [
            "static void dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rq *rq = rq_of_rt_se(rt_se);",
            "",
            "\tupdate_stats_dequeue_rt(rt_rq_of_se(rt_se), rt_se, flags);",
            "",
            "\tdequeue_rt_stack(rt_se, flags);",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\t\tif (rt_rq && rt_rq->rt_nr_running)",
            "\t\t\t__enqueue_rt_entity(rt_se, flags);",
            "\t}",
            "\tenqueue_top_rt_rq(&rq->rt);",
            "}",
            "static void",
            "enqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tif (flags & ENQUEUE_WAKEUP)",
            "\t\trt_se->timeout = 0;",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_rt(rt_rq_of_se(rt_se), rt_se);",
            "",
            "\tenqueue_rt_entity(rt_se, flags);",
            "",
            "\tif (!task_current(rq, p) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_task(rq, p);",
            "}",
            "static bool dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tdequeue_rt_entity(rt_se, flags);",
            "",
            "\tdequeue_pushable_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void",
            "requeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)",
            "{",
            "\tif (on_rt_rq(rt_se)) {",
            "\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "\t\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);",
            "",
            "\t\tif (head)",
            "\t\t\tlist_move(&rt_se->run_list, queue);",
            "\t\telse",
            "\t\t\tlist_move_tail(&rt_se->run_list, queue);",
            "\t}",
            "}",
            "static void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\trt_rq = rt_rq_of_se(rt_se);",
            "\t\trequeue_rt_entity(rt_rq, rt_se, head);",
            "\t}",
            "}",
            "static void yield_task_rt(struct rq *rq)",
            "{",
            "\trequeue_task_rt(rq, rq->curr, 0);",
            "}",
            "static int",
            "select_task_rq_rt(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tstruct rq *rq;",
            "\tbool test;",
            "",
            "\t/* For anything but wake ups, just return the task_cpu */",
            "\tif (!(flags & (WF_TTWU | WF_FORK)))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If the current task on @p's runqueue is an RT task, then",
            "\t * try to see if we can wake this RT task up on another",
            "\t * runqueue. Otherwise simply start this RT task",
            "\t * on its current runqueue.",
            "\t *",
            "\t * We want to avoid overloading runqueues. If the woken",
            "\t * task is a higher priority, then it will stay on this CPU",
            "\t * and the lower prio task should be moved to another CPU.",
            "\t * Even though this will probably make the lower prio task",
            "\t * lose its cache, we do not want to bounce a higher task",
            "\t * around just because it gave up its CPU, perhaps for a",
            "\t * lock?",
            "\t *",
            "\t * For equal prio tasks, we just let the scheduler sort it out.",
            "\t *",
            "\t * Otherwise, just let it ride on the affined RQ and the",
            "\t * post-schedule router will push the preempted task away",
            "\t *",
            "\t * This test is optimistic, if we get it wrong the load-balancer",
            "\t * will have to sort it out.",
            "\t *",
            "\t * We take into account the capacity of the CPU to ensure it fits the",
            "\t * requirement of the task - which is only important on heterogeneous",
            "\t * systems like big.LITTLE.",
            "\t */",
            "\ttest = curr &&",
            "\t       unlikely(rt_task(curr)) &&",
            "\t       (curr->nr_cpus_allowed < 2 || curr->prio <= p->prio);",
            "",
            "\tif (test || !rt_task_fits_capacity(p, cpu)) {",
            "\t\tint target = find_lowest_rq(p);",
            "",
            "\t\t/*",
            "\t\t * Bail out if we were forcing a migration to find a better",
            "\t\t * fitting CPU but our search failed.",
            "\t\t */",
            "\t\tif (!test && target != -1 && !rt_task_fits_capacity(p, target))",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\t/*",
            "\t\t * Don't bother moving it if the destination CPU is",
            "\t\t * not running a lower priority task.",
            "\t\t */",
            "\t\tif (target != -1 &&",
            "\t\t    p->prio < cpu_rq(target)->rt.highest_prio.curr)",
            "\t\t\tcpu = target;",
            "\t}",
            "",
            "out_unlock:",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "dequeue_rt_entity, enqueue_task_rt, dequeue_task_rt, requeue_rt_entity, requeue_task_rt, yield_task_rt, select_task_rq_rt",
          "description": "实现实时任务的出队逻辑、唤醒和迁移策略，提供CPU亲和性选择及负载均衡支持，维护优先级队列的动态调整。",
          "similarity": 0.5887526273727417
        }
      ]
    },
    {
      "source_file": "kernel/sched/deadline.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:06:40\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\deadline.c`\n\n---\n\n# `sched/deadline.c` 技术文档\n\n## 1. 文件概述\n\n`sched/deadline.c` 是 Linux 内核调度器中 **SCHED_DEADLINE** 调度类的核心实现文件。该调度类基于 **最早截止时间优先（Earliest Deadline First, EDF）** 算法，并结合 **恒定带宽服务器（Constant Bandwidth Server, CBS）** 机制，为具有严格实时性要求的任务提供可预测的调度保障。\n\n其核心目标是：  \n- 对于周期性任务，若其实际运行时间不超过所申请的运行时间（runtime），则保证不会错过任何截止时间（deadline）；  \n- 对于非周期性任务、突发任务或试图超出其预留带宽的任务，系统会对其进行节流（throttling），防止其影响其他任务的实时性保障。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_dl_entity`：表示一个 deadline 调度实体，包含任务的运行时间（runtime）、截止期限（deadline）、周期（period）、带宽（dl_bw）等关键参数。\n- `struct dl_rq`：每个 CPU 的 deadline 运行队列，维护该 CPU 上所有 deadline 任务的红黑树、当前带宽使用情况（`this_bw`、`running_bw`）等。\n- `struct dl_bw`：deadline 带宽管理结构，用于跟踪系统或调度域中已分配的总带宽（`total_bw`）。\n\n### 主要函数与辅助宏\n\n#### 调度实体与运行队列关联\n- `dl_task_of(dl_se)`：从 `sched_dl_entity` 获取对应的 `task_struct`（仅适用于普通任务，不适用于服务器实体）。\n- `rq_of_dl_rq(dl_rq)` / `rq_of_dl_se(dl_se)`：获取与 deadline 运行队列或调度实体关联的 `rq`（runqueue）。\n- `dl_rq_of_se(dl_se)`：获取调度实体所属的 `dl_rq`。\n- `on_dl_rq(dl_se)`：判断调度实体是否已在 deadline 运行队列中（通过红黑树节点是否为空判断）。\n\n#### 优先级继承（PI）支持（`CONFIG_RT_MUTEXES`）\n- `pi_of(dl_se)`：获取当前调度实体因优先级继承而提升后的“代理”实体。\n- `is_dl_boosted(dl_se)`：判断该 deadline 实体是否因优先级继承被提升。\n\n#### 带宽管理（SMP 与 UP 差异处理）\n- `dl_bw_of(cpu)`：获取指定 CPU 所属调度域（或本地）的 `dl_bw` 结构。\n- `dl_bw_cpus(cpu)`：返回该 CPU 所在调度域中活跃 CPU 的数量。\n- `dl_bw_capacity(cpu)`：计算调度域的总 CPU 容量（考虑异构 CPU 的 `arch_scale_cpu_capacity`）。\n- `__dl_add()` / `__dl_sub()`：向带宽池中添加或移除任务带宽，并更新 `extra_bw`（用于负载均衡）。\n- `__dl_overflow()`：检查新增带宽是否超出系统/调度域的可用带宽上限。\n\n#### 运行时带宽跟踪\n- `__add_running_bw()` / `__sub_running_bw()`：更新 `dl_rq->running_bw`（当前正在运行的 deadline 任务所消耗的带宽）。\n- `__add_rq_bw()` / `__sub_rq_bw()`：更新 `dl_rq->this_bw`（该运行队列上所有 deadline 任务的总预留带宽）。\n- `add_running_bw()` / `sub_running_bw()` / `add_rq_bw()` / `sub_rq_bw()`：带宽操作的封装，跳过“特殊”调度实体（如服务器）。\n\n#### 其他\n- `dl_server(dl_se)`：判断调度实体是否为 CBS 服务器（而非普通任务）。\n- `dl_bw_visited(cpu, gen)`：用于带宽遍历去重（SMP 场景）。\n\n### 系统控制接口（`CONFIG_SYSCTL`）\n- `sched_deadline_period_max_us`：deadline 任务周期上限（默认 ~4 秒）。\n- `sched_deadline_period_min_us`：deadline 任务周期下限（默认 100 微秒），防止定时器 DoS。\n\n## 3. 关键实现\n\n### EDF + CBS 调度模型\n- 每个 deadline 任务通过 `runtime`、`deadline`、`period` 三个参数定义其资源需求。\n- 调度器按 **绝对截止时间（absolute deadline）** 对任务排序，使用红黑树实现 O(log n) 的调度决策。\n- CBS 机制确保任务即使突发执行，也不会长期占用超过其 `runtime/period` 的 CPU 带宽，超限任务会被 throttled。\n\n### 带宽隔离与全局限制\n- 在 SMP 系统中，deadline 带宽按 **调度域（root domain）** 进行管理，防止跨 CPU 的带宽滥用。\n- 总带宽限制默认为 CPU 总容量的 95%（由 `sysctl_sched_util_clamp_min` 等机制间接控制，具体限制逻辑在带宽分配函数中体现）。\n- `dl_bw->total_bw` 跟踪已分配带宽，`__dl_overflow()` 用于在任务加入时检查是否超限。\n\n### 异构 CPU 支持\n- 通过 `arch_scale_cpu_capacity()` 获取每个 CPU 的相对性能权重。\n- `dl_bw_capacity()` 在异构系统中返回调度域内所有活跃 CPU 的容量总和，用于带宽比例计算（`cap_scale()`）。\n\n### 与 cpufreq 集成\n- 每次 `running_bw` 变化时调用 `cpufreq_update_util()`，通知 CPU 频率调节器当前 deadline 负载，确保满足实时性能需求。\n\n### 优先级继承（PI）\n- 当 deadline 任务因持有 mutex 而阻塞高优先级任务时，通过 `pi_se` 字段临时提升其调度参数，避免优先级反转。\n\n## 4. 依赖关系\n\n- **核心调度框架**：依赖 `kernel/sched/sched.h` 中定义的通用调度结构（如 `rq`、`task_struct`）和宏（如 `SCHED_CAPACITY_SCALE`）。\n- **CPU 拓扑与容量**：依赖 `arch_scale_cpu_capacity()`（由各架构实现）获取 CPU 性能信息。\n- **RCU 机制**：在 SMP 路径中大量使用 `rcu_read_lock_sched_held()` 进行锁依赖检查。\n- **cpufreq 子系统**：通过 `cpufreq_update_util()` 与 CPU 频率调节器交互。\n- **实时互斥锁**：`CONFIG_RT_MUTEXES` 启用时，支持 deadline 任务的优先级继承。\n- **Sysctl 接口**：`CONFIG_SYSCTL` 启用时，提供用户空间可调的 deadline 参数。\n\n## 5. 使用场景\n\n- **工业实时控制**：如机器人控制、数控机床等需要严格周期性和低延迟响应的场景。\n- **音视频处理**：专业音视频采集、编码、播放等对 jitter 敏感的应用。\n- **电信基础设施**：5G 基站、核心网网元中的高优先级信令处理。\n- **汽车电子**：ADAS、自动驾驶系统中的关键任务调度。\n- **科研与高性能计算**：需要确定性执行时间的实验或仿真任务。\n\n用户通过 `sched_setattr(2)` 系统调用设置任务的 `SCHED_DEADLINE` 策略及对应的 `runtime`、`deadline`、`period` 参数，内核则通过本文件实现的调度逻辑确保其满足实时性约束。",
      "similarity": 0.5984674096107483,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 514,
          "end_line": 616,
          "content": [
            "static inline int is_leftmost(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\treturn rb_first_cached(&dl_rq->root) == &dl_se->rb_node;",
            "}",
            "void init_dl_bw(struct dl_bw *dl_b)",
            "{",
            "\traw_spin_lock_init(&dl_b->lock);",
            "\tif (global_rt_runtime() == RUNTIME_INF)",
            "\t\tdl_b->bw = -1;",
            "\telse",
            "\t\tdl_b->bw = to_ratio(global_rt_period(), global_rt_runtime());",
            "\tdl_b->total_bw = 0;",
            "}",
            "void init_dl_rq(struct dl_rq *dl_rq)",
            "{",
            "\tdl_rq->root = RB_ROOT_CACHED;",
            "",
            "#ifdef CONFIG_SMP",
            "\t/* zero means no -deadline tasks */",
            "\tdl_rq->earliest_dl.curr = dl_rq->earliest_dl.next = 0;",
            "",
            "\tdl_rq->overloaded = 0;",
            "\tdl_rq->pushable_dl_tasks_root = RB_ROOT_CACHED;",
            "#else",
            "\tinit_dl_bw(&dl_rq->dl_bw);",
            "#endif",
            "",
            "\tdl_rq->running_bw = 0;",
            "\tdl_rq->this_bw = 0;",
            "\tinit_dl_rq_bw_ratio(dl_rq);",
            "}",
            "static inline int dl_overloaded(struct rq *rq)",
            "{",
            "\treturn atomic_read(&rq->rd->dlo_count);",
            "}",
            "static inline void dl_set_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tcpumask_set_cpu(rq->cpu, rq->rd->dlo_mask);",
            "\t/*",
            "\t * Must be visible before the overload count is",
            "\t * set (as in sched_rt.c).",
            "\t *",
            "\t * Matched by the barrier in pull_dl_task().",
            "\t */",
            "\tsmp_wmb();",
            "\tatomic_inc(&rq->rd->dlo_count);",
            "}",
            "static inline void dl_clear_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tatomic_dec(&rq->rd->dlo_count);",
            "\tcpumask_clear_cpu(rq->cpu, rq->rd->dlo_mask);",
            "}",
            "static inline bool __pushable_less(struct rb_node *a, const struct rb_node *b)",
            "{",
            "\treturn dl_entity_preempt(&__node_2_pdl(a)->dl, &__node_2_pdl(b)->dl);",
            "}",
            "static inline int has_pushable_dl_tasks(struct rq *rq)",
            "{",
            "\treturn !RB_EMPTY_ROOT(&rq->dl.pushable_dl_tasks_root.rb_root);",
            "}",
            "static void enqueue_pushable_dl_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tstruct rb_node *leftmost;",
            "",
            "\tWARN_ON_ONCE(!RB_EMPTY_NODE(&p->pushable_dl_tasks));",
            "",
            "\tleftmost = rb_add_cached(&p->pushable_dl_tasks,",
            "\t\t\t\t &rq->dl.pushable_dl_tasks_root,",
            "\t\t\t\t __pushable_less);",
            "\tif (leftmost)",
            "\t\trq->dl.earliest_dl.next = p->dl.deadline;",
            "",
            "\tif (!rq->dl.overloaded) {",
            "\t\tdl_set_overload(rq);",
            "\t\trq->dl.overloaded = 1;",
            "\t}",
            "}",
            "static void dequeue_pushable_dl_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tstruct dl_rq *dl_rq = &rq->dl;",
            "\tstruct rb_root_cached *root = &dl_rq->pushable_dl_tasks_root;",
            "\tstruct rb_node *leftmost;",
            "",
            "\tif (RB_EMPTY_NODE(&p->pushable_dl_tasks))",
            "\t\treturn;",
            "",
            "\tleftmost = rb_erase_cached(&p->pushable_dl_tasks, root);",
            "\tif (leftmost)",
            "\t\tdl_rq->earliest_dl.next = __node_2_pdl(leftmost)->dl.deadline;",
            "",
            "\tRB_CLEAR_NODE(&p->pushable_dl_tasks);",
            "",
            "\tif (!has_pushable_dl_tasks(rq) && rq->dl.overloaded) {",
            "\t\tdl_clear_overload(rq);",
            "\t\trq->dl.overloaded = 0;",
            "\t}",
            "}"
          ],
          "function_name": "is_leftmost, init_dl_bw, init_dl_rq, dl_overloaded, dl_set_overload, dl_clear_overload, __pushable_less, has_pushable_dl_tasks, enqueue_pushable_dl_task, dequeue_pushable_dl_task",
          "description": "实现截止时间调度的抢占判定和过载管理机制，包含任务优先级比较、过载标记维护及可推送任务的数据结构操作。",
          "similarity": 0.5783329010009766
        },
        {
          "chunk_id": 18,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 3051,
          "end_line": 3166,
          "content": [
            "static void switched_to_dl(struct rq *rq, struct task_struct *p)",
            "{",
            "\tif (hrtimer_try_to_cancel(&p->dl.inactive_timer) == 1)",
            "\t\tput_task_struct(p);",
            "",
            "\t/*",
            "\t * In case a task is setscheduled to SCHED_DEADLINE we need to keep",
            "\t * track of that on its cpuset (for correct bandwidth tracking).",
            "\t */",
            "\tinc_dl_tasks_cs(p);",
            "",
            "\t/* If p is not queued we will update its parameters at next wakeup. */",
            "\tif (!task_on_rq_queued(p)) {",
            "\t\tadd_rq_bw(&p->dl, &rq->dl);",
            "",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (rq->curr != p) {",
            "#ifdef CONFIG_SMP",
            "\t\tif (p->nr_cpus_allowed > 1 && rq->dl.overloaded)",
            "\t\t\tdeadline_queue_push_tasks(rq);",
            "#endif",
            "\t\tif (dl_task(rq->curr))",
            "\t\t\twakeup_preempt_dl(rq, p, 0);",
            "\t\telse",
            "\t\t\tresched_curr(rq);",
            "\t} else {",
            "\t\tupdate_dl_rq_load_avg(rq_clock_pelt(rq), rq, 0);",
            "\t}",
            "}",
            "static void prio_changed_dl(struct rq *rq, struct task_struct *p,",
            "\t\t\t    int oldprio)",
            "{",
            "\tif (!task_on_rq_queued(p))",
            "\t\treturn;",
            "",
            "#ifdef CONFIG_SMP",
            "\t/*",
            "\t * This might be too much, but unfortunately",
            "\t * we don't have the old deadline value, and",
            "\t * we can't argue if the task is increasing",
            "\t * or lowering its prio, so...",
            "\t */",
            "\tif (!rq->dl.overloaded)",
            "\t\tdeadline_queue_pull_task(rq);",
            "",
            "\tif (task_current(rq, p)) {",
            "\t\t/*",
            "\t\t * If we now have a earlier deadline task than p,",
            "\t\t * then reschedule, provided p is still on this",
            "\t\t * runqueue.",
            "\t\t */",
            "\t\tif (dl_time_before(rq->dl.earliest_dl.curr, p->dl.deadline))",
            "\t\t\tresched_curr(rq);",
            "\t} else {",
            "\t\t/*",
            "\t\t * Current may not be deadline in case p was throttled but we",
            "\t\t * have just replenished it (e.g. rt_mutex_setprio()).",
            "\t\t *",
            "\t\t * Otherwise, if p was given an earlier deadline, reschedule.",
            "\t\t */",
            "\t\tif (!dl_task(rq->curr) ||",
            "\t\t    dl_time_before(p->dl.deadline, rq->curr->dl.deadline))",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "#else",
            "\t/*",
            "\t * We don't know if p has a earlier or later deadline, so let's blindly",
            "\t * set a (maybe not needed) rescheduling point.",
            "\t */",
            "\tresched_curr(rq);",
            "#endif",
            "}",
            "static int task_is_throttled_dl(struct task_struct *p, int cpu)",
            "{",
            "\treturn p->dl.dl_throttled;",
            "}",
            "int sched_dl_global_validate(void)",
            "{",
            "\tu64 runtime = global_rt_runtime();",
            "\tu64 period = global_rt_period();",
            "\tu64 new_bw = to_ratio(period, runtime);",
            "\tu64 gen = ++dl_generation;",
            "\tstruct dl_bw *dl_b;",
            "\tint cpu, cpus, ret = 0;",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * Here we want to check the bandwidth not being set to some",
            "\t * value smaller than the currently allocated bandwidth in",
            "\t * any of the root_domains.",
            "\t */",
            "\tfor_each_online_cpu(cpu) {",
            "\t\trcu_read_lock_sched();",
            "",
            "\t\tif (dl_bw_visited(cpu, gen))",
            "\t\t\tgoto next;",
            "",
            "\t\tdl_b = dl_bw_of(cpu);",
            "\t\tcpus = dl_bw_cpus(cpu);",
            "",
            "\t\traw_spin_lock_irqsave(&dl_b->lock, flags);",
            "\t\tif (new_bw * cpus < dl_b->total_bw)",
            "\t\t\tret = -EBUSY;",
            "\t\traw_spin_unlock_irqrestore(&dl_b->lock, flags);",
            "",
            "next:",
            "\t\trcu_read_unlock_sched();",
            "",
            "\t\tif (ret)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "switched_to_dl, prio_changed_dl, task_is_throttled_dl, sched_dl_global_validate",
          "description": "处理SCHED_DEADLINE任务策略切换、优先级变更、节流状态检测及全局带宽验证，确保系统范围内的带宽约束有效性",
          "similarity": 0.5718578100204468
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 966,
          "end_line": 1100,
          "content": [
            "static bool dl_entity_overflow(struct sched_dl_entity *dl_se, u64 t)",
            "{",
            "\tu64 left, right;",
            "",
            "\t/*",
            "\t * left and right are the two sides of the equation above,",
            "\t * after a bit of shuffling to use multiplications instead",
            "\t * of divisions.",
            "\t *",
            "\t * Note that none of the time values involved in the two",
            "\t * multiplications are absolute: dl_deadline and dl_runtime",
            "\t * are the relative deadline and the maximum runtime of each",
            "\t * instance, runtime is the runtime left for the last instance",
            "\t * and (deadline - t), since t is rq->clock, is the time left",
            "\t * to the (absolute) deadline. Even if overflowing the u64 type",
            "\t * is very unlikely to occur in both cases, here we scale down",
            "\t * as we want to avoid that risk at all. Scaling down by 10",
            "\t * means that we reduce granularity to 1us. We are fine with it,",
            "\t * since this is only a true/false check and, anyway, thinking",
            "\t * of anything below microseconds resolution is actually fiction",
            "\t * (but still we want to give the user that illusion >;).",
            "\t */",
            "\tleft = (pi_of(dl_se)->dl_deadline >> DL_SCALE) * (dl_se->runtime >> DL_SCALE);",
            "\tright = ((dl_se->deadline - t) >> DL_SCALE) *",
            "\t\t(pi_of(dl_se)->dl_runtime >> DL_SCALE);",
            "",
            "\treturn dl_time_before(right, left);",
            "}",
            "static void",
            "update_dl_revised_wakeup(struct sched_dl_entity *dl_se, struct rq *rq)",
            "{",
            "\tu64 laxity = dl_se->deadline - rq_clock(rq);",
            "",
            "\t/*",
            "\t * If the task has deadline < period, and the deadline is in the past,",
            "\t * it should already be throttled before this check.",
            "\t *",
            "\t * See update_dl_entity() comments for further details.",
            "\t */",
            "\tWARN_ON(dl_time_before(dl_se->deadline, rq_clock(rq)));",
            "",
            "\tdl_se->runtime = (dl_se->dl_density * laxity) >> BW_SHIFT;",
            "}",
            "static inline bool dl_is_implicit(struct sched_dl_entity *dl_se)",
            "{",
            "\treturn dl_se->dl_deadline == dl_se->dl_period;",
            "}",
            "static void update_dl_entity(struct sched_dl_entity *dl_se)",
            "{",
            "\tstruct rq *rq = rq_of_dl_se(dl_se);",
            "",
            "\tif (dl_time_before(dl_se->deadline, rq_clock(rq)) ||",
            "\t    dl_entity_overflow(dl_se, rq_clock(rq))) {",
            "",
            "\t\tif (unlikely(!dl_is_implicit(dl_se) &&",
            "\t\t\t     !dl_time_before(dl_se->deadline, rq_clock(rq)) &&",
            "\t\t\t     !is_dl_boosted(dl_se))) {",
            "\t\t\tupdate_dl_revised_wakeup(dl_se, rq);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\treplenish_dl_new_period(dl_se, rq);",
            "\t} else if (dl_server(dl_se) && dl_se->dl_defer) {",
            "\t\t/*",
            "\t\t * The server can still use its previous deadline, so check if",
            "\t\t * it left the dl_defer_running state.",
            "\t\t */",
            "\t\tif (!dl_se->dl_defer_running) {",
            "\t\t\tdl_se->dl_defer_armed = 1;",
            "\t\t\tdl_se->dl_throttled = 1;",
            "\t\t}",
            "\t}",
            "}",
            "static inline u64 dl_next_period(struct sched_dl_entity *dl_se)",
            "{",
            "\treturn dl_se->deadline - dl_se->dl_deadline + dl_se->dl_period;",
            "}",
            "static int start_dl_timer(struct sched_dl_entity *dl_se)",
            "{",
            "\tstruct hrtimer *timer = &dl_se->dl_timer;",
            "\tstruct dl_rq *dl_rq = dl_rq_of_se(dl_se);",
            "\tstruct rq *rq = rq_of_dl_rq(dl_rq);",
            "\tktime_t now, act;",
            "\ts64 delta;",
            "",
            "\tlockdep_assert_rq_held(rq);",
            "",
            "\t/*",
            "\t * We want the timer to fire at the deadline, but considering",
            "\t * that it is actually coming from rq->clock and not from",
            "\t * hrtimer's time base reading.",
            "\t *",
            "\t * The deferred reservation will have its timer set to",
            "\t * (deadline - runtime). At that point, the CBS rule will decide",
            "\t * if the current deadline can be used, or if a replenishment is",
            "\t * required to avoid add too much pressure on the system",
            "\t * (current u > U).",
            "\t */",
            "\tif (dl_se->dl_defer_armed) {",
            "\t\tWARN_ON_ONCE(!dl_se->dl_throttled);",
            "\t\tact = ns_to_ktime(dl_se->deadline - dl_se->runtime);",
            "\t} else {",
            "\t\t/* act = deadline - rel-deadline + period */",
            "\t\tact = ns_to_ktime(dl_next_period(dl_se));",
            "\t}",
            "",
            "\tnow = hrtimer_cb_get_time(timer);",
            "\tdelta = ktime_to_ns(now) - rq_clock(rq);",
            "\tact = ktime_add_ns(act, delta);",
            "",
            "\t/*",
            "\t * If the expiry time already passed, e.g., because the value",
            "\t * chosen as the deadline is too small, don't even try to",
            "\t * start the timer in the past!",
            "\t */",
            "\tif (ktime_us_delta(act, now) < 0)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * !enqueued will guarantee another callback; even if one is already in",
            "\t * progress. This ensures a balanced {get,put}_task_struct().",
            "\t *",
            "\t * The race against __run_timer() clearing the enqueued state is",
            "\t * harmless because we're holding task_rq()->lock, therefore the timer",
            "\t * expiring after we've done the check will wait on its task_rq_lock()",
            "\t * and observe our state.",
            "\t */",
            "\tif (!hrtimer_is_queued(timer)) {",
            "\t\tif (!dl_server(dl_se))",
            "\t\t\tget_task_struct(dl_task_of(dl_se));",
            "\t\thrtimer_start(timer, act, HRTIMER_MODE_ABS_HARD);",
            "\t}",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "dl_entity_overflow, update_dl_revised_wakeup, dl_is_implicit, update_dl_entity, dl_next_period, start_dl_timer",
          "description": "实现了截止时间任务的溢出检测、唤醒时间调整、隐式截止时间判定及动态实体更新逻辑，包含基于带宽的调度策略和超时处理机制。",
          "similarity": 0.5691225528717041
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 224,
          "end_line": 332,
          "content": [
            "static inline",
            "void __dl_sub(struct dl_bw *dl_b, u64 tsk_bw, int cpus)",
            "{",
            "\tdl_b->total_bw -= tsk_bw;",
            "\t__dl_update(dl_b, (s32)tsk_bw / cpus);",
            "}",
            "static inline",
            "void __dl_add(struct dl_bw *dl_b, u64 tsk_bw, int cpus)",
            "{",
            "\tdl_b->total_bw += tsk_bw;",
            "\t__dl_update(dl_b, -((s32)tsk_bw / cpus));",
            "}",
            "static inline bool",
            "__dl_overflow(struct dl_bw *dl_b, unsigned long cap, u64 old_bw, u64 new_bw)",
            "{",
            "\treturn dl_b->bw != -1 &&",
            "\t       cap_scale(dl_b->bw, cap) < dl_b->total_bw - old_bw + new_bw;",
            "}",
            "static inline",
            "void __add_running_bw(u64 dl_bw, struct dl_rq *dl_rq)",
            "{",
            "\tu64 old = dl_rq->running_bw;",
            "",
            "\tlockdep_assert_rq_held(rq_of_dl_rq(dl_rq));",
            "\tdl_rq->running_bw += dl_bw;",
            "\tSCHED_WARN_ON(dl_rq->running_bw < old); /* overflow */",
            "\tSCHED_WARN_ON(dl_rq->running_bw > dl_rq->this_bw);",
            "\t/* kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\tcpufreq_update_util(rq_of_dl_rq(dl_rq), 0);",
            "}",
            "static inline",
            "void __sub_running_bw(u64 dl_bw, struct dl_rq *dl_rq)",
            "{",
            "\tu64 old = dl_rq->running_bw;",
            "",
            "\tlockdep_assert_rq_held(rq_of_dl_rq(dl_rq));",
            "\tdl_rq->running_bw -= dl_bw;",
            "\tSCHED_WARN_ON(dl_rq->running_bw > old); /* underflow */",
            "\tif (dl_rq->running_bw > old)",
            "\t\tdl_rq->running_bw = 0;",
            "\t/* kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\tcpufreq_update_util(rq_of_dl_rq(dl_rq), 0);",
            "}",
            "static inline",
            "void __add_rq_bw(u64 dl_bw, struct dl_rq *dl_rq)",
            "{",
            "\tu64 old = dl_rq->this_bw;",
            "",
            "\tlockdep_assert_rq_held(rq_of_dl_rq(dl_rq));",
            "\tdl_rq->this_bw += dl_bw;",
            "\tSCHED_WARN_ON(dl_rq->this_bw < old); /* overflow */",
            "}",
            "static inline",
            "void __sub_rq_bw(u64 dl_bw, struct dl_rq *dl_rq)",
            "{",
            "\tu64 old = dl_rq->this_bw;",
            "",
            "\tlockdep_assert_rq_held(rq_of_dl_rq(dl_rq));",
            "\tdl_rq->this_bw -= dl_bw;",
            "\tSCHED_WARN_ON(dl_rq->this_bw > old); /* underflow */",
            "\tif (dl_rq->this_bw > old)",
            "\t\tdl_rq->this_bw = 0;",
            "\tSCHED_WARN_ON(dl_rq->running_bw > dl_rq->this_bw);",
            "}",
            "static inline",
            "void add_rq_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\tif (!dl_entity_is_special(dl_se))",
            "\t\t__add_rq_bw(dl_se->dl_bw, dl_rq);",
            "}",
            "static inline",
            "void sub_rq_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\tif (!dl_entity_is_special(dl_se))",
            "\t\t__sub_rq_bw(dl_se->dl_bw, dl_rq);",
            "}",
            "static inline",
            "void add_running_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\tif (!dl_entity_is_special(dl_se))",
            "\t\t__add_running_bw(dl_se->dl_bw, dl_rq);",
            "}",
            "static inline",
            "void sub_running_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\tif (!dl_entity_is_special(dl_se))",
            "\t\t__sub_running_bw(dl_se->dl_bw, dl_rq);",
            "}",
            "static void dl_rq_change_utilization(struct rq *rq, struct sched_dl_entity *dl_se, u64 new_bw)",
            "{",
            "\tif (dl_se->dl_non_contending) {",
            "\t\tsub_running_bw(dl_se, &rq->dl);",
            "\t\tdl_se->dl_non_contending = 0;",
            "",
            "\t\t/*",
            "\t\t * If the timer handler is currently running and the",
            "\t\t * timer cannot be canceled, inactive_task_timer()",
            "\t\t * will see that dl_not_contending is not set, and",
            "\t\t * will not touch the rq's active utilization,",
            "\t\t * so we are still safe.",
            "\t\t */",
            "\t\tif (hrtimer_try_to_cancel(&dl_se->inactive_timer) == 1) {",
            "\t\t\tif (!dl_server(dl_se))",
            "\t\t\t\tput_task_struct(dl_task_of(dl_se));",
            "\t\t}",
            "\t}",
            "\t__sub_rq_bw(dl_se->dl_bw, &rq->dl);",
            "\t__add_rq_bw(new_bw, &rq->dl);",
            "}"
          ],
          "function_name": "__dl_sub, __dl_add, __dl_overflow, __add_running_bw, __sub_running_bw, __add_rq_bw, __sub_rq_bw, add_rq_bw, sub_rq_bw, add_running_bw, sub_running_bw, dl_rq_change_utilization",
          "description": "实现截止时间任务带宽的增减操作，包含溢出检测逻辑，用于动态调整运行时带宽和资源使用量。",
          "similarity": 0.568280816078186
        },
        {
          "chunk_id": 15,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2454,
          "end_line": 2581,
          "content": [
            "static void put_prev_task_dl(struct rq *rq, struct task_struct *p, struct task_struct *next)",
            "{",
            "\tstruct sched_dl_entity *dl_se = &p->dl;",
            "\tstruct dl_rq *dl_rq = &rq->dl;",
            "",
            "\tif (on_dl_rq(&p->dl))",
            "\t\tupdate_stats_wait_start_dl(dl_rq, dl_se);",
            "",
            "\tupdate_curr_dl(rq);",
            "",
            "\tupdate_dl_rq_load_avg(rq_clock_pelt(rq), rq, 1);",
            "\tif (on_dl_rq(&p->dl) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_dl_task(rq, p);",
            "}",
            "static void task_tick_dl(struct rq *rq, struct task_struct *p, int queued)",
            "{",
            "\tupdate_curr_dl(rq);",
            "",
            "\tupdate_dl_rq_load_avg(rq_clock_pelt(rq), rq, 1);",
            "\t/*",
            "\t * Even when we have runtime, update_curr_dl() might have resulted in us",
            "\t * not being the leftmost task anymore. In that case NEED_RESCHED will",
            "\t * be set and schedule() will start a new hrtick for the next task.",
            "\t */",
            "\tif (hrtick_enabled_dl(rq) && queued && p->dl.runtime > 0 &&",
            "\t    is_leftmost(&p->dl, &rq->dl))",
            "\t\tstart_hrtick_dl(rq, &p->dl);",
            "}",
            "static void task_fork_dl(struct task_struct *p)",
            "{",
            "\t/*",
            "\t * SCHED_DEADLINE tasks cannot fork and this is achieved through",
            "\t * sched_fork()",
            "\t */",
            "}",
            "static int pick_dl_task(struct rq *rq, struct task_struct *p, int cpu)",
            "{",
            "\tif (!task_on_cpu(rq, p) &&",
            "\t    cpumask_test_cpu(cpu, &p->cpus_mask))",
            "\t\treturn 1;",
            "\treturn 0;",
            "}",
            "static int find_later_rq(struct task_struct *task)",
            "{",
            "\tstruct sched_domain *sd;",
            "\tstruct cpumask *later_mask = this_cpu_cpumask_var_ptr(local_cpu_mask_dl);",
            "\tint this_cpu = smp_processor_id();",
            "\tint cpu = task_cpu(task);",
            "",
            "\t/* Make sure the mask is initialized first */",
            "\tif (unlikely(!later_mask))",
            "\t\treturn -1;",
            "",
            "\tif (task->nr_cpus_allowed == 1)",
            "\t\treturn -1;",
            "",
            "\t/*",
            "\t * We have to consider system topology and task affinity",
            "\t * first, then we can look for a suitable CPU.",
            "\t */",
            "\tif (!cpudl_find(&task_rq(task)->rd->cpudl, task, later_mask))",
            "\t\treturn -1;",
            "",
            "\t/*",
            "\t * If we are here, some targets have been found, including",
            "\t * the most suitable which is, among the runqueues where the",
            "\t * current tasks have later deadlines than the task's one, the",
            "\t * rq with the latest possible one.",
            "\t *",
            "\t * Now we check how well this matches with task's",
            "\t * affinity and system topology.",
            "\t *",
            "\t * The last CPU where the task run is our first",
            "\t * guess, since it is most likely cache-hot there.",
            "\t */",
            "\tif (cpumask_test_cpu(cpu, later_mask))",
            "\t\treturn cpu;",
            "\t/*",
            "\t * Check if this_cpu is to be skipped (i.e., it is",
            "\t * not in the mask) or not.",
            "\t */",
            "\tif (!cpumask_test_cpu(this_cpu, later_mask))",
            "\t\tthis_cpu = -1;",
            "",
            "\trcu_read_lock();",
            "\tfor_each_domain(cpu, sd) {",
            "\t\tif (sd->flags & SD_WAKE_AFFINE) {",
            "\t\t\tint best_cpu;",
            "",
            "\t\t\t/*",
            "\t\t\t * If possible, preempting this_cpu is",
            "\t\t\t * cheaper than migrating.",
            "\t\t\t */",
            "\t\t\tif (this_cpu != -1 &&",
            "\t\t\t    cpumask_test_cpu(this_cpu, sched_domain_span(sd))) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn this_cpu;",
            "\t\t\t}",
            "",
            "\t\t\tbest_cpu = cpumask_any_and_distribute(later_mask,",
            "\t\t\t\t\t\t\t      sched_domain_span(sd));",
            "\t\t\t/*",
            "\t\t\t * Last chance: if a CPU being in both later_mask",
            "\t\t\t * and current sd span is valid, that becomes our",
            "\t\t\t * choice. Of course, the latest possible CPU is",
            "\t\t\t * already under consideration through later_mask.",
            "\t\t\t */",
            "\t\t\tif (best_cpu < nr_cpu_ids) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn best_cpu;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * At this point, all our guesses failed, we just return",
            "\t * 'something', and let the caller sort the things out.",
            "\t */",
            "\tif (this_cpu != -1)",
            "\t\treturn this_cpu;",
            "",
            "\tcpu = cpumask_any_distribute(later_mask);",
            "\tif (cpu < nr_cpu_ids)",
            "\t\treturn cpu;",
            "",
            "\treturn -1;",
            "}"
          ],
          "function_name": "put_prev_task_dl, task_tick_dl, task_fork_dl, pick_dl_task, find_later_rq",
          "description": "处理SCHED_DEADLINE任务切换时的状态更新和负载均衡，包含任务统计更新、运行队列加载平均值调整及可推送任务的入队操作",
          "similarity": 0.5670915246009827
        }
      ]
    },
    {
      "source_file": "kernel/sched/cpufreq_schedutil.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:03:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\cpufreq_schedutil.c`\n\n---\n\n# `sched/cpufreq_schedutil.c` 技术文档\n\n## 1. 文件概述\n\n`sched/cpufreq_schedutil.c` 实现了 Linux 内核中基于调度器提供的 CPU 利用率数据的 **schedutil CPUFreq 调速器（governor）**。该调速器通过实时获取调度器计算的 CPU 利用率（包括 CFS、RT、DL 任务以及 I/O 等待状态），动态调整 CPU 频率，以在性能与能效之间取得平衡。其核心优势在于直接利用调度器的 `util` 信息，避免传统调速器依赖采样机制带来的延迟和不准确性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct sugov_tunables`**  \n  调速器可调参数，包含：\n  - `rate_limit_us`：频率更新的最小时间间隔（微秒），防止过于频繁的频率切换。\n\n- **`struct sugov_policy`**  \n  每个 `cpufreq_policy` 对应的 schedutil 策略实例，包含：\n  - `policy`：关联的 CPUFreq 策略。\n  - `update_lock`：保护频率更新的自旋锁。\n  - `last_freq_update_time` / `freq_update_delay_ns`：控制频率更新速率。\n  - `next_freq` / `cached_raw_freq`：目标频率与原始计算频率缓存。\n  - `irq_work` / `worker` / `thread`：用于慢速切换平台（非 fast-switch）的异步工作队列机制。\n  - `limits_changed` / `need_freq_update`：标志策略限制（如 min/max freq）是否变更。\n\n- **`struct sugov_cpu`**  \n  每个 CPU 的 schedutil 状态，包含：\n  - `update_util`：注册到调度器的回调接口（`update_util_data`）。\n  - `util` / `bw_min`：当前有效利用率及带宽最小值。\n  - `iowait_boost` / `iowait_boost_pending`：I/O 等待唤醒时的频率提升机制。\n  - `last_update`：上次更新时间戳。\n\n### 主要函数\n\n- **`sugov_should_update_freq()`**  \n  判断是否应执行频率更新，考虑硬件是否支持本 CPU 更新、策略限制变更、以及频率更新间隔限制。\n\n- **`sugov_update_next_freq()`**  \n  更新目标频率，处理策略限制变更场景，避免不必要的驱动回调。\n\n- **`get_next_freq()`**  \n  核心频率计算函数，根据 CPU 利用率、最大容量和参考频率，计算目标频率，并通过 `cpufreq_driver_resolve_freq()` 映射到驱动支持的频率。\n\n- **`sugov_get_util()`**  \n  获取当前 CPU 的综合利用率，整合 CFS/RT/DL 任务利用率、boost 值，并调用 `sugov_effective_cpu_perf()` 计算有效性能目标。\n\n- **`sugov_effective_cpu_perf()`**  \n  计算最终的有效性能目标，确保不低于最小性能要求，并限制不超过实际需求。\n\n- **`sugov_iowait_reset()` / `sugov_iowait_boost()`**  \n  实现 I/O 等待唤醒时的动态频率提升机制：短时间内连续 I/O 唤醒会逐步提升 boost 值（从 `IOWAIT_BOOST_MIN` 到最大 OPP），超过一个 tick 无 I/O 唤醒则重置。\n\n- **`get_capacity_ref_freq()`**  \n  获取用于计算 CPU 容量的参考频率，优先使用架构特定的 `arch_scale_freq_ref()`，其次为最大频率或当前频率。\n\n- **`sugov_deferred_update()`**  \n  在不支持 fast-switch 的平台上，通过 `irq_work` 触发异步频率更新。\n\n## 3. 关键实现\n\n### 频率计算算法\n- **频率不变性支持**：若系统支持频率不变调度（`arch_scale_freq_invariant()`），则直接使用调度器提供的频率不变利用率 `util`，按比例计算目标频率：  \n  `next_freq = C * max_freq * util / max`  \n  其中常数 `C = 1.25`，使在 `util/max = 0.8` 时达到 `max_freq`，提供性能余量。\n- **非频率不变性**：使用原始利用率 `util_raw` 乘以 `(curr_freq / max_freq)` 近似频率不变利用率，再计算目标频率。\n\n### I/O 等待 Boost 机制\n- 当任务因 I/O 完成而唤醒时，标记 `SCHED_CPUFREQ_IOWAIT`。\n- 若在 **一个 tick 内** 多次发生 I/O 唤醒，则 `iowait_boost` 值倍增（上限为最大 OPP 对应的利用率）。\n- 若超过一个 tick 无 I/O 唤醒，则重置 boost 值为 `IOWAIT_BOOST_MIN`（`SCHED_CAPACITY_SCALE / 8`），避免对偶发 I/O 过度响应，提升能效。\n\n### 快速切换（Fast-Switch）与异步更新\n- **Fast-Switch 平台**：支持在调度上下文中直接调用 `cpufreq_driver_fast_switch()` 更新频率，延迟最低。\n- **非 Fast-Switch 平台**：通过 `irq_work` 触发内核线程（`kthread_worker`）异步执行频率更新，避免在中断上下文或持有 rq 锁时调用可能阻塞的驱动接口。\n\n### 策略限制变更处理\n- 当用户空间修改 policy 的 min/max 频率时，`sugov_limits()` 设置 `limits_changed` 标志。\n- 下次更新时，强制重新计算频率，并通过内存屏障（`smp_mb()`）确保读取到最新的策略限制。\n\n## 4. 依赖关系\n\n- **调度器子系统**：\n  - 依赖 `update_util_data` 回调机制（通过 `cpufreq_add_update_util_hook()` 注册）。\n  - 调用 `cpu_util_cfs_boost()`、`effective_cpu_util()` 等函数获取综合利用率。\n  - 使用 `scx_cpuperf_target()`（若启用了 SCHED_CLASS_EXT）。\n- **CPUFreq 核心**：\n  - 依赖 `cpufreq_policy`、`cpufreq_driver_resolve_freq()`、`cpufreq_driver_fast_switch()` 等接口。\n  - 使用 `cpufreq_this_cpu_can_update()` 判断硬件更新能力。\n- **架构相关支持**：\n  - 依赖 `arch_scale_freq_ref()` 和 `arch_scale_freq_invariant()` 提供频率不变性信息。\n- **内核基础设施**：\n  - 使用 `irq_work`、`kthread_worker` 实现异步更新。\n  - 依赖 `TICK_NSEC` 定义 tick 时间。\n\n## 5. 使用场景\n\n- **默认高性能能效平衡场景**：现代 Linux 发行版通常将 `schedutil` 作为默认 CPUFreq 调速器，适用于大多数桌面、服务器和移动设备。\n- **实时性要求较高的系统**：由于其低延迟特性（尤其在 fast-switch 平台上），适合对响应时间敏感的应用。\n- **能效敏感设备**：通过 I/O boost 机制和精确的利用率跟踪，在保证交互性能的同时降低空闲功耗。\n- **异构多核系统（如 big.LITTLE）**：结合调度器的 CPU capacity 信息，为不同性能核提供差异化频率调整。",
      "similarity": 0.5824665427207947,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 62,
          "end_line": 168,
          "content": [
            "static bool sugov_should_update_freq(struct sugov_policy *sg_policy, u64 time)",
            "{",
            "\ts64 delta_ns;",
            "",
            "\t/*",
            "\t * Since cpufreq_update_util() is called with rq->lock held for",
            "\t * the @target_cpu, our per-CPU data is fully serialized.",
            "\t *",
            "\t * However, drivers cannot in general deal with cross-CPU",
            "\t * requests, so while get_next_freq() will work, our",
            "\t * sugov_update_commit() call may not for the fast switching platforms.",
            "\t *",
            "\t * Hence stop here for remote requests if they aren't supported",
            "\t * by the hardware, as calculating the frequency is pointless if",
            "\t * we cannot in fact act on it.",
            "\t *",
            "\t * This is needed on the slow switching platforms too to prevent CPUs",
            "\t * going offline from leaving stale IRQ work items behind.",
            "\t */",
            "\tif (!cpufreq_this_cpu_can_update(sg_policy->policy))",
            "\t\treturn false;",
            "",
            "\tif (unlikely(READ_ONCE(sg_policy->limits_changed))) {",
            "\t\tWRITE_ONCE(sg_policy->limits_changed, false);",
            "\t\tsg_policy->need_freq_update = true;",
            "",
            "\t\t/*",
            "\t\t * The above limits_changed update must occur before the reads",
            "\t\t * of policy limits in cpufreq_driver_resolve_freq() or a policy",
            "\t\t * limits update might be missed, so use a memory barrier to",
            "\t\t * ensure it.",
            "\t\t *",
            "\t\t * This pairs with the write memory barrier in sugov_limits().",
            "\t\t */",
            "\t\tsmp_mb();",
            "",
            "\t\treturn true;",
            "\t}",
            "",
            "\tdelta_ns = time - sg_policy->last_freq_update_time;",
            "",
            "\treturn delta_ns >= sg_policy->freq_update_delay_ns;",
            "}",
            "static bool sugov_update_next_freq(struct sugov_policy *sg_policy, u64 time,",
            "\t\t\t\t   unsigned int next_freq)",
            "{",
            "\tif (sg_policy->need_freq_update) {",
            "\t\tsg_policy->need_freq_update = false;",
            "\t\t/*",
            "\t\t * The policy limits have changed, but if the return value of",
            "\t\t * cpufreq_driver_resolve_freq() after applying the new limits",
            "\t\t * is still equal to the previously selected frequency, the",
            "\t\t * driver callback need not be invoked unless the driver",
            "\t\t * specifically wants that to happen on every update of the",
            "\t\t * policy limits.",
            "\t\t */",
            "\t\tif (sg_policy->next_freq == next_freq &&",
            "\t\t    !cpufreq_driver_test_flags(CPUFREQ_NEED_UPDATE_LIMITS))",
            "\t\t\treturn false;",
            "\t} else if (sg_policy->next_freq == next_freq) {",
            "\t\treturn false;",
            "\t}",
            "",
            "\tsg_policy->next_freq = next_freq;",
            "\tsg_policy->last_freq_update_time = time;",
            "",
            "\treturn true;",
            "}",
            "static void sugov_deferred_update(struct sugov_policy *sg_policy)",
            "{",
            "\tif (!sg_policy->work_in_progress) {",
            "\t\tsg_policy->work_in_progress = true;",
            "\t\tirq_work_queue(&sg_policy->irq_work);",
            "\t}",
            "}",
            "static __always_inline",
            "unsigned long get_capacity_ref_freq(struct cpufreq_policy *policy)",
            "{",
            "\tunsigned int freq = arch_scale_freq_ref(policy->cpu);",
            "",
            "\tif (freq)",
            "\t\treturn freq;",
            "",
            "\tif (arch_scale_freq_invariant())",
            "\t\treturn policy->cpuinfo.max_freq;",
            "",
            "\t/*",
            "\t * Apply a 25% margin so that we select a higher frequency than",
            "\t * the current one before the CPU is fully busy:",
            "\t */",
            "\treturn policy->cur + (policy->cur >> 2);",
            "}",
            "static unsigned int get_next_freq(struct sugov_policy *sg_policy,",
            "\t\t\t\t  unsigned long util, unsigned long max)",
            "{",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tunsigned int freq;",
            "",
            "\tfreq = get_capacity_ref_freq(policy);",
            "\tfreq = map_util_freq(util, freq, max);",
            "",
            "\tif (freq == sg_policy->cached_raw_freq && !sg_policy->need_freq_update)",
            "\t\treturn sg_policy->next_freq;",
            "",
            "\tsg_policy->cached_raw_freq = freq;",
            "\treturn cpufreq_driver_resolve_freq(policy, freq);",
            "}"
          ],
          "function_name": "sugov_should_update_freq, sugov_update_next_freq, sugov_deferred_update, get_capacity_ref_freq, get_next_freq",
          "description": "实现了频率更新核心逻辑，sugov_should_update_freq判断是否需要更新频率，sugov_update_next_freq计算并记录目标频率，sugov_deferred_update触发异步更新，get_capacity_ref_freq获取基准频率，get_next_freq结合利用率计算最终目标频率。",
          "similarity": 0.544014573097229
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 508,
          "end_line": 651,
          "content": [
            "static void",
            "sugov_update_shared(struct update_util_data *hook, u64 time, unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tunsigned int next_f;",
            "",
            "\traw_spin_lock(&sg_policy->update_lock);",
            "",
            "\tsugov_iowait_boost(sg_cpu, time, flags);",
            "\tsg_cpu->last_update = time;",
            "",
            "\tignore_dl_rate_limit(sg_cpu);",
            "",
            "\tif (sugov_should_update_freq(sg_policy, time)) {",
            "\t\tnext_f = sugov_next_freq_shared(sg_cpu, time);",
            "",
            "\t\tif (!sugov_update_next_freq(sg_policy, time, next_f))",
            "\t\t\tgoto unlock;",
            "",
            "\t\tif (sg_policy->policy->fast_switch_enabled)",
            "\t\t\tcpufreq_driver_fast_switch(sg_policy->policy, next_f);",
            "\t\telse",
            "\t\t\tsugov_deferred_update(sg_policy);",
            "\t}",
            "unlock:",
            "\traw_spin_unlock(&sg_policy->update_lock);",
            "}",
            "static void sugov_work(struct kthread_work *work)",
            "{",
            "\tstruct sugov_policy *sg_policy = container_of(work, struct sugov_policy, work);",
            "\tunsigned int freq;",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * Hold sg_policy->update_lock shortly to handle the case where:",
            "\t * in case sg_policy->next_freq is read here, and then updated by",
            "\t * sugov_deferred_update() just before work_in_progress is set to false",
            "\t * here, we may miss queueing the new update.",
            "\t *",
            "\t * Note: If a work was queued after the update_lock is released,",
            "\t * sugov_work() will just be called again by kthread_work code; and the",
            "\t * request will be proceed before the sugov thread sleeps.",
            "\t */",
            "\traw_spin_lock_irqsave(&sg_policy->update_lock, flags);",
            "\tfreq = sg_policy->next_freq;",
            "\tsg_policy->work_in_progress = false;",
            "\traw_spin_unlock_irqrestore(&sg_policy->update_lock, flags);",
            "",
            "\tmutex_lock(&sg_policy->work_lock);",
            "\t__cpufreq_driver_target(sg_policy->policy, freq, CPUFREQ_RELATION_L);",
            "\tmutex_unlock(&sg_policy->work_lock);",
            "}",
            "static void sugov_irq_work(struct irq_work *irq_work)",
            "{",
            "\tstruct sugov_policy *sg_policy;",
            "",
            "\tsg_policy = container_of(irq_work, struct sugov_policy, irq_work);",
            "",
            "\tkthread_queue_work(&sg_policy->worker, &sg_policy->work);",
            "}",
            "static ssize_t rate_limit_us_show(struct gov_attr_set *attr_set, char *buf)",
            "{",
            "\tstruct sugov_tunables *tunables = to_sugov_tunables(attr_set);",
            "",
            "\treturn sprintf(buf, \"%u\\n\", tunables->rate_limit_us);",
            "}",
            "static ssize_t",
            "rate_limit_us_store(struct gov_attr_set *attr_set, const char *buf, size_t count)",
            "{",
            "\tstruct sugov_tunables *tunables = to_sugov_tunables(attr_set);",
            "\tstruct sugov_policy *sg_policy;",
            "\tunsigned int rate_limit_us;",
            "",
            "\tif (kstrtouint(buf, 10, &rate_limit_us))",
            "\t\treturn -EINVAL;",
            "",
            "\ttunables->rate_limit_us = rate_limit_us;",
            "",
            "\tlist_for_each_entry(sg_policy, &attr_set->policy_list, tunables_hook)",
            "\t\tsg_policy->freq_update_delay_ns = rate_limit_us * NSEC_PER_USEC;",
            "",
            "\treturn count;",
            "}",
            "static void sugov_tunables_free(struct kobject *kobj)",
            "{",
            "\tstruct gov_attr_set *attr_set = to_gov_attr_set(kobj);",
            "",
            "\tkfree(to_sugov_tunables(attr_set));",
            "}",
            "static void sugov_policy_free(struct sugov_policy *sg_policy)",
            "{",
            "\tkfree(sg_policy);",
            "}",
            "static int sugov_kthread_create(struct sugov_policy *sg_policy)",
            "{",
            "\tstruct task_struct *thread;",
            "\tstruct sched_attr attr = {",
            "\t\t.size\t\t= sizeof(struct sched_attr),",
            "\t\t.sched_policy\t= SCHED_DEADLINE,",
            "\t\t.sched_flags\t= SCHED_FLAG_SUGOV,",
            "\t\t.sched_nice\t= 0,",
            "\t\t.sched_priority\t= 0,",
            "\t\t/*",
            "\t\t * Fake (unused) bandwidth; workaround to \"fix\"",
            "\t\t * priority inheritance.",
            "\t\t */",
            "\t\t.sched_runtime\t=  1000000,",
            "\t\t.sched_deadline = 10000000,",
            "\t\t.sched_period\t= 10000000,",
            "\t};",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tint ret;",
            "",
            "\t/* kthread only required for slow path */",
            "\tif (policy->fast_switch_enabled)",
            "\t\treturn 0;",
            "",
            "\tkthread_init_work(&sg_policy->work, sugov_work);",
            "\tkthread_init_worker(&sg_policy->worker);",
            "\tthread = kthread_create(kthread_worker_fn, &sg_policy->worker,",
            "\t\t\t\t\"sugov:%d\",",
            "\t\t\t\tcpumask_first(policy->related_cpus));",
            "\tif (IS_ERR(thread)) {",
            "\t\tpr_err(\"failed to create sugov thread: %ld\\n\", PTR_ERR(thread));",
            "\t\treturn PTR_ERR(thread);",
            "\t}",
            "",
            "\tret = sched_setattr_nocheck(thread, &attr);",
            "\tif (ret) {",
            "\t\tkthread_stop(thread);",
            "\t\tpr_warn(\"%s: failed to set SCHED_DEADLINE\\n\", __func__);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tsg_policy->thread = thread;",
            "\tkthread_bind_mask(thread, policy->related_cpus);",
            "\tinit_irq_work(&sg_policy->irq_work, sugov_irq_work);",
            "\tmutex_init(&sg_policy->work_lock);",
            "",
            "\twake_up_process(thread);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sugov_update_shared, sugov_work, sugov_irq_work, rate_limit_us_show, rate_limit_us_store, sugov_tunables_free, sugov_policy_free, sugov_kthread_create",
          "description": "管理频率调节的工作线程和参数配置，sugov_kthread_create创建慢速切换场景的后台线程，rate_limit_us_*/提供速率限制配置接口，sugov_work/sugov_irq_work处理异步频率更新任务。",
          "similarity": 0.5414737462997437
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 204,
          "end_line": 330,
          "content": [
            "unsigned long sugov_effective_cpu_perf(int cpu, unsigned long actual,",
            "\t\t\t\t unsigned long min,",
            "\t\t\t\t unsigned long max)",
            "{",
            "\t/* Add dvfs headroom to actual utilization */",
            "\tactual = map_util_perf(actual);",
            "\t/* Actually we don't need to target the max performance */",
            "\tif (actual < max)",
            "\t\tmax = actual;",
            "",
            "\t/*",
            "\t * Ensure at least minimum performance while providing more compute",
            "\t * capacity when possible.",
            "\t */",
            "\treturn max(min, max);",
            "}",
            "static void sugov_get_util(struct sugov_cpu *sg_cpu, unsigned long boost)",
            "{",
            "\tunsigned long min, max, util = scx_cpuperf_target(sg_cpu->cpu);",
            "",
            "\tif (!scx_switched_all())",
            "\t\tutil += cpu_util_cfs_boost(sg_cpu->cpu);",
            "\tutil = effective_cpu_util(sg_cpu->cpu, util, &min, &max);",
            "\tutil = max(util, boost);",
            "\tsg_cpu->bw_min = min;",
            "\tsg_cpu->util = sugov_effective_cpu_perf(sg_cpu->cpu, util, min, max);",
            "}",
            "static bool sugov_iowait_reset(struct sugov_cpu *sg_cpu, u64 time,",
            "\t\t\t       bool set_iowait_boost)",
            "{",
            "\ts64 delta_ns = time - sg_cpu->last_update;",
            "",
            "\t/* Reset boost only if a tick has elapsed since last request */",
            "\tif (delta_ns <= TICK_NSEC)",
            "\t\treturn false;",
            "",
            "\tsg_cpu->iowait_boost = set_iowait_boost ? IOWAIT_BOOST_MIN : 0;",
            "\tsg_cpu->iowait_boost_pending = set_iowait_boost;",
            "",
            "\treturn true;",
            "}",
            "static void sugov_iowait_boost(struct sugov_cpu *sg_cpu, u64 time,",
            "\t\t\t       unsigned int flags)",
            "{",
            "\tbool set_iowait_boost = flags & SCHED_CPUFREQ_IOWAIT;",
            "",
            "\t/* Reset boost if the CPU appears to have been idle enough */",
            "\tif (sg_cpu->iowait_boost &&",
            "\t    sugov_iowait_reset(sg_cpu, time, set_iowait_boost))",
            "\t\treturn;",
            "",
            "\t/* Boost only tasks waking up after IO */",
            "\tif (!set_iowait_boost)",
            "\t\treturn;",
            "",
            "\t/* Ensure boost doubles only one time at each request */",
            "\tif (sg_cpu->iowait_boost_pending)",
            "\t\treturn;",
            "\tsg_cpu->iowait_boost_pending = true;",
            "",
            "\t/* Double the boost at each request */",
            "\tif (sg_cpu->iowait_boost) {",
            "\t\tsg_cpu->iowait_boost =",
            "\t\t\tmin_t(unsigned int, sg_cpu->iowait_boost << 1, SCHED_CAPACITY_SCALE);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* First wakeup after IO: start with minimum boost */",
            "\tsg_cpu->iowait_boost = IOWAIT_BOOST_MIN;",
            "}",
            "static unsigned long sugov_iowait_apply(struct sugov_cpu *sg_cpu, u64 time,",
            "\t\t\t       unsigned long max_cap)",
            "{",
            "\t/* No boost currently required */",
            "\tif (!sg_cpu->iowait_boost)",
            "\t\treturn 0;",
            "",
            "\t/* Reset boost if the CPU appears to have been idle enough */",
            "\tif (sugov_iowait_reset(sg_cpu, time, false))",
            "\t\treturn 0;",
            "",
            "\tif (!sg_cpu->iowait_boost_pending) {",
            "\t\t/*",
            "\t\t * No boost pending; reduce the boost value.",
            "\t\t */",
            "\t\tsg_cpu->iowait_boost >>= 1;",
            "\t\tif (sg_cpu->iowait_boost < IOWAIT_BOOST_MIN) {",
            "\t\t\tsg_cpu->iowait_boost = 0;",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            "\tsg_cpu->iowait_boost_pending = false;",
            "",
            "\t/*",
            "\t * sg_cpu->util is already in capacity scale; convert iowait_boost",
            "\t * into the same scale so we can compare.",
            "\t */",
            "\treturn (sg_cpu->iowait_boost * max_cap) >> SCHED_CAPACITY_SHIFT;",
            "}",
            "static bool sugov_hold_freq(struct sugov_cpu *sg_cpu)",
            "{",
            "\tunsigned long idle_calls;",
            "\tbool ret;",
            "",
            "\t/*",
            "\t * The heuristics in this function is for the fair class. For SCX, the",
            "\t * performance target comes directly from the BPF scheduler. Let's just",
            "\t * follow it.",
            "\t */",
            "\tif (scx_switched_all())",
            "\t\treturn false;",
            "",
            "\t/* if capped by uclamp_max, always update to be in compliance */",
            "\tif (uclamp_rq_is_capped(cpu_rq(sg_cpu->cpu)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Maintain the frequency if the CPU has not been idle recently, as",
            "\t * reduction is likely to be premature.",
            "\t */",
            "\tidle_calls = tick_nohz_get_idle_calls_cpu(sg_cpu->cpu);",
            "\tret = idle_calls == sg_cpu->saved_idle_calls;",
            "",
            "\tsg_cpu->saved_idle_calls = idle_calls;",
            "\treturn ret;",
            "}"
          ],
          "function_name": "sugov_effective_cpu_perf, sugov_get_util, sugov_iowait_reset, sugov_iowait_boost, sugov_iowait_apply, sugov_hold_freq",
          "description": "处理利用率计算和I/O等待优化，sugov_effective_cpu_perf计算有效性能需求，sugov_get_util获取考虑boost后的利用率，sugov_iowait_*系列函数管理I/O等待场景下的频率提升机制。",
          "similarity": 0.5335610508918762
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 381,
          "end_line": 496,
          "content": [
            "static inline bool sugov_hold_freq(struct sugov_cpu *sg_cpu) { return false; }",
            "static inline void ignore_dl_rate_limit(struct sugov_cpu *sg_cpu)",
            "{",
            "\tif (cpu_bw_dl(cpu_rq(sg_cpu->cpu)) > sg_cpu->bw_min)",
            "\t\tWRITE_ONCE(sg_cpu->sg_policy->limits_changed, true);",
            "}",
            "static inline bool sugov_update_single_common(struct sugov_cpu *sg_cpu,",
            "\t\t\t\t\t      u64 time, unsigned long max_cap,",
            "\t\t\t\t\t      unsigned int flags)",
            "{",
            "\tunsigned long boost;",
            "",
            "\tsugov_iowait_boost(sg_cpu, time, flags);",
            "\tsg_cpu->last_update = time;",
            "",
            "\tignore_dl_rate_limit(sg_cpu);",
            "",
            "\tif (!sugov_should_update_freq(sg_cpu->sg_policy, time))",
            "\t\treturn false;",
            "",
            "\tboost = sugov_iowait_apply(sg_cpu, time, max_cap);",
            "\tsugov_get_util(sg_cpu, boost);",
            "",
            "\treturn true;",
            "}",
            "static void sugov_update_single_freq(struct update_util_data *hook, u64 time,",
            "\t\t\t\t     unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tunsigned int cached_freq = sg_policy->cached_raw_freq;",
            "\tunsigned long max_cap;",
            "\tunsigned int next_f;",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tif (!sugov_update_single_common(sg_cpu, time, max_cap, flags))",
            "\t\treturn;",
            "",
            "\tnext_f = get_next_freq(sg_policy, sg_cpu->util, max_cap);",
            "",
            "\tif (sugov_hold_freq(sg_cpu) && next_f < sg_policy->next_freq &&",
            "\t    !sg_policy->need_freq_update) {",
            "\t\tnext_f = sg_policy->next_freq;",
            "",
            "\t\t/* Restore cached freq as next_freq has changed */",
            "\t\tsg_policy->cached_raw_freq = cached_freq;",
            "\t}",
            "",
            "\tif (!sugov_update_next_freq(sg_policy, time, next_f))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This code runs under rq->lock for the target CPU, so it won't run",
            "\t * concurrently on two different CPUs for the same target and it is not",
            "\t * necessary to acquire the lock in the fast switch case.",
            "\t */",
            "\tif (sg_policy->policy->fast_switch_enabled) {",
            "\t\tcpufreq_driver_fast_switch(sg_policy->policy, next_f);",
            "\t} else {",
            "\t\traw_spin_lock(&sg_policy->update_lock);",
            "\t\tsugov_deferred_update(sg_policy);",
            "\t\traw_spin_unlock(&sg_policy->update_lock);",
            "\t}",
            "}",
            "static void sugov_update_single_perf(struct update_util_data *hook, u64 time,",
            "\t\t\t\t     unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tunsigned long prev_util = sg_cpu->util;",
            "\tunsigned long max_cap;",
            "",
            "\t/*",
            "\t * Fall back to the \"frequency\" path if frequency invariance is not",
            "\t * supported, because the direct mapping between the utilization and",
            "\t * the performance levels depends on the frequency invariance.",
            "\t */",
            "\tif (!arch_scale_freq_invariant()) {",
            "\t\tsugov_update_single_freq(hook, time, flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tif (!sugov_update_single_common(sg_cpu, time, max_cap, flags))",
            "\t\treturn;",
            "",
            "\tif (sugov_hold_freq(sg_cpu) && sg_cpu->util < prev_util)",
            "\t\tsg_cpu->util = prev_util;",
            "",
            "\tcpufreq_driver_adjust_perf(sg_cpu->cpu, sg_cpu->bw_min,",
            "\t\t\t\t   sg_cpu->util, max_cap);",
            "",
            "\tsg_cpu->sg_policy->last_freq_update_time = time;",
            "}",
            "static unsigned int sugov_next_freq_shared(struct sugov_cpu *sg_cpu, u64 time)",
            "{",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tunsigned long util = 0, max_cap;",
            "\tunsigned int j;",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tfor_each_cpu(j, policy->cpus) {",
            "\t\tstruct sugov_cpu *j_sg_cpu = &per_cpu(sugov_cpu, j);",
            "\t\tunsigned long boost;",
            "",
            "\t\tboost = sugov_iowait_apply(j_sg_cpu, time, max_cap);",
            "\t\tsugov_get_util(j_sg_cpu, boost);",
            "",
            "\t\tutil = max(j_sg_cpu->util, util);",
            "\t}",
            "",
            "\treturn get_next_freq(sg_policy, util, max_cap);",
            "}"
          ],
          "function_name": "sugov_hold_freq, ignore_dl_rate_limit, sugov_update_single_common, sugov_update_single_freq, sugov_update_single_perf, sugov_next_freq_shared",
          "description": "实现单核/多核频率调整逻辑，sugov_update_single_freq处理单核频率更新，sugov_update_single_perf处理性能调优路径，sugov_next_freq_shared计算多核共享场景下的全局目标频率。",
          "similarity": 0.5183700919151306
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 827,
          "end_line": 916,
          "content": [
            "static int sugov_start(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "\tvoid (*uu)(struct update_util_data *data, u64 time, unsigned int flags);",
            "\tunsigned int cpu;",
            "",
            "\tsg_policy->freq_update_delay_ns\t= sg_policy->tunables->rate_limit_us * NSEC_PER_USEC;",
            "\tsg_policy->last_freq_update_time\t= 0;",
            "\tsg_policy->next_freq\t\t\t= 0;",
            "\tsg_policy->work_in_progress\t\t= false;",
            "\tsg_policy->limits_changed\t\t= false;",
            "\tsg_policy->cached_raw_freq\t\t= 0;",
            "",
            "\tsg_policy->need_freq_update = cpufreq_driver_test_flags(CPUFREQ_NEED_UPDATE_LIMITS);",
            "",
            "\tfor_each_cpu(cpu, policy->cpus) {",
            "\t\tstruct sugov_cpu *sg_cpu = &per_cpu(sugov_cpu, cpu);",
            "",
            "\t\tmemset(sg_cpu, 0, sizeof(*sg_cpu));",
            "\t\tsg_cpu->cpu\t\t\t= cpu;",
            "\t\tsg_cpu->sg_policy\t\t= sg_policy;",
            "\t}",
            "",
            "\tif (policy_is_shared(policy))",
            "\t\tuu = sugov_update_shared;",
            "\telse if (policy->fast_switch_enabled && cpufreq_driver_has_adjust_perf())",
            "\t\tuu = sugov_update_single_perf;",
            "\telse",
            "\t\tuu = sugov_update_single_freq;",
            "",
            "\tfor_each_cpu(cpu, policy->cpus) {",
            "\t\tstruct sugov_cpu *sg_cpu = &per_cpu(sugov_cpu, cpu);",
            "",
            "\t\tcpufreq_add_update_util_hook(cpu, &sg_cpu->update_util, uu);",
            "\t}",
            "\treturn 0;",
            "}",
            "static void sugov_stop(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "\tunsigned int cpu;",
            "",
            "\tfor_each_cpu(cpu, policy->cpus)",
            "\t\tcpufreq_remove_update_util_hook(cpu);",
            "",
            "\tsynchronize_rcu();",
            "",
            "\tif (!policy->fast_switch_enabled) {",
            "\t\tirq_work_sync(&sg_policy->irq_work);",
            "\t\tkthread_cancel_work_sync(&sg_policy->work);",
            "\t}",
            "}",
            "static void sugov_limits(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "",
            "\tif (!policy->fast_switch_enabled) {",
            "\t\tmutex_lock(&sg_policy->work_lock);",
            "\t\tcpufreq_policy_apply_limits(policy);",
            "\t\tmutex_unlock(&sg_policy->work_lock);",
            "\t}",
            "",
            "\t/*",
            "\t * The limits_changed update below must take place before the updates",
            "\t * of policy limits in cpufreq_set_policy() or a policy limits update",
            "\t * might be missed, so use a memory barrier to ensure it.",
            "\t *",
            "\t * This pairs with the memory barrier in sugov_should_update_freq().",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\tWRITE_ONCE(sg_policy->limits_changed, true);",
            "}",
            "static void rebuild_sd_workfn(struct work_struct *work)",
            "{",
            "\trebuild_sched_domains_energy();",
            "}",
            "void sched_cpufreq_governor_change(struct cpufreq_policy *policy,",
            "\t\t\t\t  struct cpufreq_governor *old_gov)",
            "{",
            "\tif (old_gov == &schedutil_gov || policy->governor == &schedutil_gov) {",
            "\t\t/*",
            "\t\t * When called from the cpufreq_register_driver() path, the",
            "\t\t * cpu_hotplug_lock is already held, so use a work item to",
            "\t\t * avoid nested locking in rebuild_sched_domains().",
            "\t\t */",
            "\t\tschedule_work(&rebuild_sd_work);",
            "\t}",
            "",
            "}"
          ],
          "function_name": "sugov_start, sugov_stop, sugov_limits, rebuild_sd_workfn, sched_cpufreq_governor_change",
          "description": "sugov_start 注册CPU利用率更新钩子函数并初始化频率更新参数；sugov_stop 移除所有CPU的更新钩子并同步RCU状态；sugov_limits 应用频率限制并标记策略变更；rebuild_sd_workfn 触发调度域能量重新构建；sched_cpufreq_governor_change 在策略切换时安排调度域重建工作",
          "similarity": 0.49859619140625
        }
      ]
    }
  ]
}