{
  "query": "缓冲区与信号量的关系",
  "timestamp": "2025-12-25 23:48:56",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/semaphore.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:52:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\semaphore.c`\n\n---\n\n# `locking/semaphore.c` 技术文档\n\n## 1. 文件概述\n\n`locking/semaphore.c` 实现了 Linux 内核中的**计数信号量（counting semaphore）**机制。计数信号量允许多个任务（最多为初始计数值）同时持有该锁，当计数值耗尽时，后续请求者将被阻塞，直到有其他任务释放信号量。与互斥锁（mutex）不同，信号量支持更灵活的并发控制，适用于资源池、限流等场景。该文件提供了多种获取和释放信号量的接口，包括可中断、可超时、不可中断等变体，并支持在中断上下文中调用部分函数。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `down(struct semaphore *sem)` | 不可中断地获取信号量，若不可用则睡眠。**已弃用**，建议使用可中断版本。 |\n| `down_interruptible(struct semaphore *sem)` | 可被普通信号中断的获取操作，成功返回 0，被信号中断返回 `-EINTR`。 |\n| `down_killable(struct semaphore *sem)` | 可被致命信号（fatal signal）中断的获取操作，返回值同上。 |\n| `down_trylock(struct semaphore *sem)` | 非阻塞尝试获取信号量，成功返回 0，失败返回 1（**注意返回值与 mutex/spinlock 相反**）。 |\n| `down_timeout(struct semaphore *sem, long timeout)` | 带超时的获取操作，超时返回 `-ETIME`，成功返回 0。 |\n| `up(struct semaphore *sem)` | 释放信号量，可由任意上下文（包括中断）调用，唤醒等待队列中的任务。 |\n\n### 静态辅助函数\n\n- `__down*()` 系列：处理信号量争用时的阻塞逻辑。\n- `__up()`：在有等待者时执行唤醒逻辑。\n- `___down_common()`：通用的阻塞等待实现，支持不同睡眠状态和超时。\n- `__sem_acquire()`：原子减少计数并记录持有者（用于 hung task 检测）。\n\n### 数据结构\n\n- `struct semaphore`（定义在 `<linux/semaphore.h>`）：\n  - `count`：当前可用资源数（>0 表示可立即获取）。\n  - `wait_list`：等待该信号量的任务链表。\n  - `lock`：保护上述成员的原始自旋锁（`raw_spinlock_t`）。\n  - `last_holder`（条件编译）：记录最后持有者，用于 `CONFIG_DETECT_HUNG_TASK_BLOCKER`。\n\n- `struct semaphore_waiter`：\n  - 用于将任务加入等待队列，包含任务指针和唤醒标志（`up`）。\n\n## 3. 关键实现\n\n### 中断安全与自旋锁\n- 所有对外接口（包括 `down*` 和 `up`）均使用 `raw_spin_lock_irqsave()` 获取自旋锁，确保在中断上下文安全。\n- 即使 `down()` 等函数通常在进程上下文调用，也使用 `irqsave` 变体，因为内核某些部分依赖在中断上下文成功调用 `down()`（当确定信号量可用时）。\n\n### 计数语义\n- `sem->count` 表示**还可被获取的次数**。初始值由 `sema_init()` 设置。\n- 获取时：若 `count > 0`，直接减 1；否则加入等待队列。\n- 释放时：若等待队列为空，`count++`；否则唤醒队首任务。\n\n### 等待与唤醒机制\n- 使用 `wake_q`（批量唤醒队列）优化唤醒路径，避免在持有自旋锁时调用 `wake_up_process()`。\n- 等待任务通过 `schedule_timeout()` 睡眠，并在循环中检查：\n  - 是否收到信号（根据睡眠状态判断）。\n  - 是否超时。\n  - 是否被 `__up()` 标记为 `waiter.up = true`（表示已被选中唤醒）。\n\n### Hung Task 支持\n- 当启用 `CONFIG_DETECT_HUNG_TASK_BLOCKER` 时：\n  - 获取信号量时记录当前任务为 `last_holder`。\n  - 释放时若当前任务是持有者，则清除记录。\n  - 提供 `sem_last_holder()` 供 hung task 检测模块查询阻塞源头。\n\n### 返回值约定\n- `down_trylock()` 返回 **0 表示成功**，**1 表示失败**，这与 `mutex_trylock()` 和 `spin_trylock()` **相反**，需特别注意。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/semaphore.h>`：信号量结构体和 API 声明。\n  - `<linux/spinlock.h>`：原始自旋锁实现。\n  - `<linux/sched.h>`、`<linux/sched/wake_q.h>`：任务调度和批量唤醒。\n  - `<trace/events/lock.h>`：锁争用跟踪点。\n  - `<linux/hung_task.h>`：hung task 检测支持。\n\n- **内核配置依赖**：\n  - `CONFIG_DETECT_HUNG_TASK_BLOCKER`：启用信号量持有者跟踪。\n\n- **与其他同步原语关系**：\n  - 与 `mutex.c` 形成对比：mutex 是二值、不可递归、带调试信息的互斥锁；信号量是计数、可被任意任务释放、更轻量。\n  - 底层依赖调度器（`schedule_timeout`）和中断管理（`irqsave`）。\n\n## 5. 使用场景\n\n- **资源池管理**：如限制同时访问某类硬件设备的任务数量。\n- **读写并发控制**：配合其他机制实现多读者/单写者模型。\n- **内核驱动**：设备驱动中控制对共享资源的并发访问。\n- **中断上下文释放**：因 `up()` 可在中断中调用，适用于中断处理程序释放资源的场景。\n- **不可睡眠路径**：使用 `down_trylock()` 在原子上下文尝试获取资源。\n\n> **注意**：由于信号量不强制所有权（任意任务可调用 `up()`），且缺乏死锁检测等调试特性，现代内核开发中更推荐使用 `mutex` 或 `rwsem`，除非明确需要计数语义或多释放者特性。",
      "similarity": 0.5498709082603455,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 252,
          "end_line": 323,
          "content": [
            "static inline int __sched ___down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\t\t\t\tlong timeout)",
            "{",
            "\tstruct semaphore_waiter waiter;",
            "",
            "\tlist_add_tail(&waiter.list, &sem->wait_list);",
            "\twaiter.task = current;",
            "\twaiter.up = false;",
            "",
            "\tfor (;;) {",
            "\t\tif (signal_pending_state(state, current))",
            "\t\t\tgoto interrupted;",
            "\t\tif (unlikely(timeout <= 0))",
            "\t\t\tgoto timed_out;",
            "\t\t__set_current_state(state);",
            "\t\traw_spin_unlock_irq(&sem->lock);",
            "\t\ttimeout = schedule_timeout(timeout);",
            "\t\traw_spin_lock_irq(&sem->lock);",
            "\t\tif (waiter.up) {",
            "\t\t\thung_task_sem_set_holder(sem);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            " timed_out:",
            "\tlist_del(&waiter.list);",
            "\treturn -ETIME;",
            "",
            " interrupted:",
            "\tlist_del(&waiter.list);",
            "\treturn -EINTR;",
            "}",
            "static inline int __sched __down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\tlong timeout)",
            "{",
            "\tint ret;",
            "",
            "\thung_task_set_blocker(sem, BLOCKER_TYPE_SEM);",
            "",
            "\ttrace_contention_begin(sem, 0);",
            "\tret = ___down_common(sem, state, timeout);",
            "\ttrace_contention_end(sem, ret);",
            "",
            "\thung_task_clear_blocker();",
            "",
            "\treturn ret;",
            "}",
            "static noinline void __sched __down(struct semaphore *sem)",
            "{",
            "\t__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_interruptible(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_killable(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_KILLABLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\treturn __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);",
            "}",
            "static noinline void __sched __up(struct semaphore *sem,",
            "\t\t\t\t  struct wake_q_head *wake_q)",
            "{",
            "\tstruct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,",
            "\t\t\t\t\t\tstruct semaphore_waiter, list);",
            "\tlist_del(&waiter->list);",
            "\twaiter->up = true;",
            "\twake_q_add(wake_q, waiter->task);",
            "}"
          ],
          "function_name": "___down_common, __down_common, __down, __down_interruptible, __down_killable, __down_timeout, __up",
          "description": "实现了信号量的阻塞等待通用逻辑，包含___down_common/__down_common等辅助函数，处理信号量不足时的任务挂起、超时检测、信号处理及唤醒机制，通过循环等待并结合schedule_timeout实现阻塞式资源竞争解决",
          "similarity": 0.5413908958435059
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2008 Intel Corporation",
            " * Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This file implements counting semaphores.",
            " * A counting semaphore may be acquired 'n' times before sleeping.",
            " * See mutex.c for single-acquisition sleeping locks which enforce",
            " * rules which allow code to be debugged more easily.",
            " */",
            "",
            "/*",
            " * Some notes on the implementation:",
            " *",
            " * The spinlock controls access to the other members of the semaphore.",
            " * down_trylock() and up() can be called from interrupt context, so we",
            " * have to disable interrupts when taking the lock.  It turns out various",
            " * parts of the kernel expect to be able to use down() on a semaphore in",
            " * interrupt context when they know it will succeed, so we have to use",
            " * irqsave variants for down(), down_interruptible() and down_killable()",
            " * too.",
            " *",
            " * The ->count variable represents how many more tasks can acquire this",
            " * semaphore.  If it's zero, there may be tasks waiting on the wait_list.",
            " */",
            "",
            "#include <linux/compiler.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/semaphore.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/ftrace.h>",
            "#include <trace/events/lock.h>",
            "#include <linux/hung_task.h>",
            "",
            "static noinline void __down(struct semaphore *sem);",
            "static noinline int __down_interruptible(struct semaphore *sem);",
            "static noinline int __down_killable(struct semaphore *sem);",
            "static noinline int __down_timeout(struct semaphore *sem, long timeout);",
            "static noinline void __up(struct semaphore *sem, struct wake_q_head *wake_q);",
            "",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER"
          ],
          "function_name": null,
          "description": "此代码块定义了计数信号量的基础框架，包含实现计数信号量所需的头文件和注释，声明了多个内联函数及辅助函数，用于处理信号量的获取、释放及Hung Task检测相关逻辑，但由于代码截断，CONFIG_DETECT_HUNG_TASK_BLOCKER部分缺失，上下文不完整",
          "similarity": 0.5201323628425598
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "\tWRITE_ONCE((sem)->last_holder, (unsigned long)current);",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "\tif (READ_ONCE((sem)->last_holder) == (unsigned long)current)",
            "\t\tWRITE_ONCE((sem)->last_holder, 0UL);",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn READ_ONCE(sem->last_holder);",
            "}",
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn 0UL;",
            "}",
            "static inline void __sem_acquire(struct semaphore *sem)",
            "{",
            "\tsem->count--;",
            "\thung_task_sem_set_holder(sem);",
            "}",
            "void __sched down(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\t__down(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "}",
            "int __sched down_interruptible(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_interruptible(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_killable(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_killable(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_trylock(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint count;",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tcount = sem->count - 1;",
            "\tif (likely(count >= 0))",
            "\t\t__sem_acquire(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn (count < 0);",
            "}",
            "int __sched down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_timeout(sem, timeout);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "void __sched up(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "",
            "\thung_task_sem_clear_if_holder(sem);",
            "",
            "\tif (likely(list_empty(&sem->wait_list)))",
            "\t\tsem->count++;",
            "\telse",
            "\t\t__up(sem, &wake_q);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "\tif (!wake_q_empty(&wake_q))",
            "\t\twake_up_q(&wake_q);",
            "}"
          ],
          "function_name": "hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, __sem_acquire, down, down_interruptible, down_killable, down_trylock, down_timeout, up",
          "description": "实现了信号量的获取与释放核心逻辑，包括down/down_interruptible/down_killable/down_trylock/down_timeout等接口，通过spinlock保护共享资源，维护等待队列并处理任务状态变更，其中包含Hung Task检测相关函数的条件性实现",
          "similarity": 0.5113471150398254
        }
      ]
    },
    {
      "source_file": "mm/page_alloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_alloc.c`\n\n---\n\n# page_alloc.c 技术文档\n\n## 1. 文件概述\n\n`page_alloc.c` 是 Linux 内核内存管理子系统的核心文件之一，负责物理页面的分配与释放。该文件实现了基于区域（zone）和迁移类型（migratetype）的伙伴系统（Buddy System）内存分配器，管理系统的空闲页链表，并提供高效的页面分配/回收机制。它不处理小对象分配（由 slab/slub/slob 子系统负责），而是专注于以页为单位的大块物理内存管理。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct per_cpu_pages`**：每个 CPU 的每区（per-zone）页面缓存，用于减少锁竞争，提升分配性能。\n- **`node_states[NR_NODE_STATES]`**：全局节点状态掩码数组，跟踪各 NUMA 节点的状态（如在线、有内存等）。\n- **`sysctl_lowmem_reserve_ratio[MAX_NR_ZONES]`**：各内存区域的低内存保留比例，防止高优先级区域耗尽低优先级区域的内存。\n- **`zone_names[]` 和 `migratetype_names[]`**：内存区域和页面迁移类型的名称字符串，用于调试和日志。\n- **`gfp_allowed_mask`**：全局 GFP（Get Free Page）标志掩码，控制启动早期可使用的分配标志。\n\n### 主要函数（部分声明）\n- **`__free_pages_ok()`**：内部页面释放函数，执行实际的伙伴系统合并与链表插入逻辑。\n- 各种页面分配函数（如 `alloc_pages()`、`__alloc_pages()` 等，定义在其他位置但在此文件中实现核心逻辑）。\n- 每 CPU 页面列表操作辅助宏（如 `pcp_spin_lock()`、`pcp_spin_trylock()`）。\n\n### 关键常量与标志\n- **`fpi_t` 类型及标志**：\n  - `FPI_NONE`：无特殊要求。\n  - `FPI_SKIP_REPORT_NOTIFY`：跳过空闲页报告通知。\n  - `FPI_TO_TAIL`：将页面放回空闲链表尾部（用于优化场景如内存热插拔）。\n- **`min_free_kbytes`**：系统保留的最小空闲内存（KB），影响水位线计算。\n\n## 3. 关键实现\n\n### 每 CPU 页面缓存（Per-CPU Page Caching）\n- 通过 `struct per_cpu_pages` 为每个 CPU 维护热/冷页列表，避免频繁访问全局 zone 锁。\n- 使用 `pcpu_spin_lock` 宏族安全地访问每 CPU 数据，结合 `preempt_disable()`（非 RT）或 `migrate_disable()`（RT）防止任务迁移导致访问错误 CPU 的数据。\n- 在 UP 系统上，使用 IRQ 关闭防止重入；在 SMP/RT 系统上依赖自旋锁语义。\n\n### 内存区域（Zone）与 NUMA 支持\n- 支持多种内存区域（DMA、DMA32、Normal、HighMem、Movable、Device），通过 `zone_names` 标识。\n- 实现 `lowmem_reserve_ratio` 机制，确保高区域分配不会耗尽低区域的保留内存（如 ZONE_DMA 为设备保留）。\n- 通过 `node_states` 和 per-CPU 变量（如 `numa_node`、`_numa_mem_`）支持 NUMA 和无内存节点架构。\n\n### 空闲页管理优化\n- **`FPI_TO_TAIL` 标志**：允许将页面放回空闲链表尾部，配合内存打乱（shuffle）或热插拔时批量初始化。\n- **`FPI_SKIP_REPORT_NOTIFY` 标志**：在临时取出并归还页面时不触发空闲页报告机制，减少开销。\n- **水位线与保留内存**：`min_free_kbytes` 控制最低水位，影响 OOM（Out-Of-Memory）决策和内存回收行为。\n\n### 实时内核（PREEMPT_RT）适配\n- 在 RT 内核中使用 `migrate_disable()` 替代 `preempt_disable()`，避免干扰 RT 自旋锁的优先级继承机制。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心内存管理**：`<linux/mm.h>`, `<linux/highmem.h>`, `\"internal.h\"`\n- **同步机制**：`<linux/spinlock.h>`（隐含）、`<linux/mutex.h>`\n- **NUMA 与拓扑**：`<linux/topology.h>`, `<linux/nodemask.h>`\n- **调试与追踪**：`<linux/kasan.h>`, `<trace/events/kmem.h>`, `<linux/page_owner.h>`\n- **高级特性**：`<linux/compaction.h>`, `<linux/migrate.h>`, `<linux/memcontrol.h>`\n\n### 子系统交互\n- **Slab 分配器**：本文件不处理 kmalloc，由 `slab.c` 等负责。\n- **内存回收**：与 `vmscan.c` 协同，通过水位线触发 reclaim。\n- **内存热插拔**：通过 `memory_hotplug.h` 接口管理动态内存。\n- **OOM Killer**：通过 `oom.h` 和水位线机制触发 OOM。\n- **透明大页（THP）**：与 `khugepaged` 协同进行大页分配。\n\n## 5. 使用场景\n\n- **内核内存分配**：所有以页为单位的内核内存请求（如 `alloc_pages()`）最终由本文件处理。\n- **用户空间缺页处理**：匿名页、文件页的物理页分配。\n- **内存映射（mmap）**：大块物理内存的分配与管理。\n- **内存回收与迁移**：页面回收、压缩（compaction）、迁移（migration）过程中涉及的页面释放与重新分配。\n- **系统启动与热插拔**：初始化内存区域、处理动态添加/移除内存。\n- **实时系统**：在 PREEMPT_RT 内核中提供低延迟的页面分配路径。\n- **调试与监控**：通过 page owner、KASAN、tracepoint 等机制提供内存使用追踪。",
      "similarity": 0.5362311005592346,
      "chunks": [
        {
          "chunk_id": 28,
          "file_path": "mm/page_alloc.c",
          "start_line": 5556,
          "end_line": 5674,
          "content": [
            "static int zone_highsize(struct zone *zone, int batch, int cpu_online,",
            "\t\t\t int high_fraction)",
            "{",
            "#ifdef CONFIG_MMU",
            "\tint high;",
            "\tint nr_split_cpus;",
            "\tunsigned long total_pages;",
            "",
            "\tif (!high_fraction) {",
            "\t\t/*",
            "\t\t * By default, the high value of the pcp is based on the zone",
            "\t\t * low watermark so that if they are full then background",
            "\t\t * reclaim will not be started prematurely.",
            "\t\t */",
            "\t\ttotal_pages = low_wmark_pages(zone);",
            "\t} else {",
            "\t\t/*",
            "\t\t * If percpu_pagelist_high_fraction is configured, the high",
            "\t\t * value is based on a fraction of the managed pages in the",
            "\t\t * zone.",
            "\t\t */",
            "\t\ttotal_pages = zone_managed_pages(zone) / high_fraction;",
            "\t}",
            "",
            "\t/*",
            "\t * Split the high value across all online CPUs local to the zone. Note",
            "\t * that early in boot that CPUs may not be online yet and that during",
            "\t * CPU hotplug that the cpumask is not yet updated when a CPU is being",
            "\t * onlined. For memory nodes that have no CPUs, split the high value",
            "\t * across all online CPUs to mitigate the risk that reclaim is triggered",
            "\t * prematurely due to pages stored on pcp lists.",
            "\t */",
            "\tnr_split_cpus = cpumask_weight(cpumask_of_node(zone_to_nid(zone))) + cpu_online;",
            "\tif (!nr_split_cpus)",
            "\t\tnr_split_cpus = num_online_cpus();",
            "\thigh = total_pages / nr_split_cpus;",
            "",
            "\t/*",
            "\t * Ensure high is at least batch*4. The multiple is based on the",
            "\t * historical relationship between high and batch.",
            "\t */",
            "\thigh = max(high, batch << 2);",
            "",
            "\treturn high;",
            "#else",
            "\treturn 0;",
            "#endif",
            "}",
            "static void pageset_update(struct per_cpu_pages *pcp, unsigned long high_min,",
            "\t\t\t   unsigned long high_max, unsigned long batch)",
            "{",
            "\tWRITE_ONCE(pcp->batch, batch);",
            "\tWRITE_ONCE(pcp->high_min, high_min);",
            "\tWRITE_ONCE(pcp->high_max, high_max);",
            "}",
            "static void per_cpu_pages_init(struct per_cpu_pages *pcp, struct per_cpu_zonestat *pzstats)",
            "{",
            "\tint pindex;",
            "",
            "\tmemset(pcp, 0, sizeof(*pcp));",
            "\tmemset(pzstats, 0, sizeof(*pzstats));",
            "",
            "\tspin_lock_init(&pcp->lock);",
            "\tfor (pindex = 0; pindex < NR_PCP_LISTS; pindex++)",
            "\t\tINIT_LIST_HEAD(&pcp->lists[pindex]);",
            "",
            "\t/*",
            "\t * Set batch and high values safe for a boot pageset. A true percpu",
            "\t * pageset's initialization will update them subsequently. Here we don't",
            "\t * need to be as careful as pageset_update() as nobody can access the",
            "\t * pageset yet.",
            "\t */",
            "\tpcp->high_min = BOOT_PAGESET_HIGH;",
            "\tpcp->high_max = BOOT_PAGESET_HIGH;",
            "\tpcp->batch = BOOT_PAGESET_BATCH;",
            "\tpcp->free_count = 0;",
            "}",
            "static void __zone_set_pageset_high_and_batch(struct zone *zone, unsigned long high_min,",
            "\t\t\t\t\t      unsigned long high_max, unsigned long batch)",
            "{",
            "\tstruct per_cpu_pages *pcp;",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tpcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);",
            "\t\tpageset_update(pcp, high_min, high_max, batch);",
            "\t}",
            "}",
            "static void zone_set_pageset_high_and_batch(struct zone *zone, int cpu_online)",
            "{",
            "\tint new_high_min, new_high_max, new_batch;",
            "",
            "\tnew_batch = max(1, zone_batchsize(zone));",
            "\tif (percpu_pagelist_high_fraction) {",
            "\t\tnew_high_min = zone_highsize(zone, new_batch, cpu_online,",
            "\t\t\t\t\t     percpu_pagelist_high_fraction);",
            "\t\t/*",
            "\t\t * PCP high is tuned manually, disable auto-tuning via",
            "\t\t * setting high_min and high_max to the manual value.",
            "\t\t */",
            "\t\tnew_high_max = new_high_min;",
            "\t} else {",
            "\t\tnew_high_min = zone_highsize(zone, new_batch, cpu_online, 0);",
            "\t\tnew_high_max = zone_highsize(zone, new_batch, cpu_online,",
            "\t\t\t\t\t     MIN_PERCPU_PAGELIST_HIGH_FRACTION);",
            "\t}",
            "",
            "\tif (zone->pageset_high_min == new_high_min &&",
            "\t    zone->pageset_high_max == new_high_max &&",
            "\t    zone->pageset_batch == new_batch)",
            "\t\treturn;",
            "",
            "\tzone->pageset_high_min = new_high_min;",
            "\tzone->pageset_high_max = new_high_max;",
            "\tzone->pageset_batch = new_batch;",
            "",
            "\t__zone_set_pageset_high_and_batch(zone, new_high_min, new_high_max,",
            "\t\t\t\t\t  new_batch);",
            "}"
          ],
          "function_name": "zone_highsize, pageset_update, per_cpu_pages_init, __zone_set_pageset_high_and_batch, zone_set_pageset_high_and_batch",
          "description": "计算并设置各CPU的页集高水位和批量参数，根据缓存数据片大小调整批量标志位，控制PCP列表的行为特性。",
          "similarity": 0.5576837658882141
        },
        {
          "chunk_id": 32,
          "file_path": "mm/page_alloc.c",
          "start_line": 6106,
          "end_line": 6209,
          "content": [
            "void calculate_min_free_kbytes(void)",
            "{",
            "\tunsigned long lowmem_kbytes;",
            "\tint new_min_free_kbytes;",
            "",
            "\tlowmem_kbytes = nr_free_buffer_pages() * (PAGE_SIZE >> 10);",
            "\tnew_min_free_kbytes = int_sqrt(lowmem_kbytes * 16);",
            "",
            "\tif (new_min_free_kbytes > user_min_free_kbytes)",
            "\t\tmin_free_kbytes = clamp(new_min_free_kbytes, 128, 262144);",
            "\telse",
            "\t\tpr_warn(\"min_free_kbytes is not updated to %d because user defined value %d is preferred\\n\",",
            "\t\t\t\tnew_min_free_kbytes, user_min_free_kbytes);",
            "",
            "}",
            "int __meminit init_per_zone_wmark_min(void)",
            "{",
            "\tcalculate_min_free_kbytes();",
            "\tsetup_per_zone_wmarks();",
            "\trefresh_zone_stat_thresholds();",
            "\tsetup_per_zone_lowmem_reserve();",
            "",
            "#ifdef CONFIG_NUMA",
            "\tsetup_min_unmapped_ratio();",
            "\tsetup_min_slab_ratio();",
            "#endif",
            "",
            "\tkhugepaged_min_free_kbytes_update();",
            "",
            "\treturn 0;",
            "}",
            "postcore_initcall(init_per_zone_wmark_min)",
            "",
            "/*",
            " * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so",
            " *\tthat we can call two helper functions whenever min_free_kbytes",
            " *\tchanges.",
            " */",
            "static int min_free_kbytes_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tif (write) {",
            "\t\tuser_min_free_kbytes = min_free_kbytes;",
            "\t\tsetup_per_zone_wmarks();",
            "\t}",
            "\treturn 0;",
            "}",
            "static int watermark_scale_factor_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tif (write)",
            "\t\tsetup_per_zone_wmarks();",
            "",
            "\treturn 0;",
            "}",
            "static void setup_min_unmapped_ratio(void)",
            "{",
            "\tpg_data_t *pgdat;",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_online_pgdat(pgdat)",
            "\t\tpgdat->min_unmapped_pages = 0;",
            "",
            "\tfor_each_zone(zone)",
            "\t\tzone->zone_pgdat->min_unmapped_pages += (zone_managed_pages(zone) *",
            "\t\t\t\t\t\t         sysctl_min_unmapped_ratio) / 100;",
            "}",
            "static int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tsetup_min_unmapped_ratio();",
            "",
            "\treturn 0;",
            "}",
            "static void setup_min_slab_ratio(void)",
            "{",
            "\tpg_data_t *pgdat;",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_online_pgdat(pgdat)",
            "\t\tpgdat->min_slab_pages = 0;",
            "",
            "\tfor_each_zone(zone)",
            "\t\tzone->zone_pgdat->min_slab_pages += (zone_managed_pages(zone) *",
            "\t\t\t\t\t\t     sysctl_min_slab_ratio) / 100;",
            "}"
          ],
          "function_name": "calculate_min_free_kbytes, init_per_zone_wmark_min, min_free_kbytes_sysctl_handler, watermark_scale_factor_sysctl_handler, setup_min_unmapped_ratio, sysctl_min_unmapped_ratio_sysctl_handler, setup_min_slab_ratio",
          "description": "计算并更新最小空闲内存阈值，初始化区域水印参数，处理sysctl接口变更，动态调整内存管理策略参数。",
          "similarity": 0.5132782459259033
        },
        {
          "chunk_id": 19,
          "file_path": "mm/page_alloc.c",
          "start_line": 3493,
          "end_line": 3599,
          "content": [
            "static void warn_alloc_show_mem(gfp_t gfp_mask, nodemask_t *nodemask)",
            "{",
            "\tunsigned int filter = SHOW_MEM_FILTER_NODES;",
            "",
            "\t/*",
            "\t * This documents exceptions given to allocations in certain",
            "\t * contexts that are allowed to allocate outside current's set",
            "\t * of allowed nodes.",
            "\t */",
            "\tif (!(gfp_mask & __GFP_NOMEMALLOC))",
            "\t\tif (tsk_is_oom_victim(current) ||",
            "\t\t    (current->flags & (PF_MEMALLOC | PF_EXITING)))",
            "\t\t\tfilter &= ~SHOW_MEM_FILTER_NODES;",
            "\tif (!in_task() || !(gfp_mask & __GFP_DIRECT_RECLAIM))",
            "\t\tfilter &= ~SHOW_MEM_FILTER_NODES;",
            "",
            "\t__show_mem(filter, nodemask, gfp_zone(gfp_mask));",
            "}",
            "void warn_alloc(gfp_t gfp_mask, nodemask_t *nodemask, const char *fmt, ...)",
            "{",
            "\tstruct va_format vaf;",
            "\tva_list args;",
            "\tstatic DEFINE_RATELIMIT_STATE(nopage_rs, 10*HZ, 1);",
            "",
            "\tif ((gfp_mask & __GFP_NOWARN) ||",
            "\t     !__ratelimit(&nopage_rs) ||",
            "\t     ((gfp_mask & __GFP_DMA) && !has_managed_dma()))",
            "\t\treturn;",
            "",
            "\tva_start(args, fmt);",
            "\tvaf.fmt = fmt;",
            "\tvaf.va = &args;",
            "\tpr_warn(\"%s: %pV, mode:%#x(%pGg), nodemask=%*pbl\",",
            "\t\t\tcurrent->comm, &vaf, gfp_mask, &gfp_mask,",
            "\t\t\tnodemask_pr_args(nodemask));",
            "\tva_end(args);",
            "",
            "\tcpuset_print_current_mems_allowed();",
            "\tpr_cont(\"\\n\");",
            "\tdump_stack();",
            "\twarn_alloc_show_mem(gfp_mask, nodemask);",
            "}",
            "static inline bool",
            "should_compact_retry(struct alloc_context *ac, int order, int alloc_flags,",
            "\t\t     enum compact_result compact_result,",
            "\t\t     enum compact_priority *compact_priority,",
            "\t\t     int *compaction_retries)",
            "{",
            "\tint max_retries = MAX_COMPACT_RETRIES;",
            "\tint min_priority;",
            "\tbool ret = false;",
            "\tint retries = *compaction_retries;",
            "\tenum compact_priority priority = *compact_priority;",
            "",
            "\tif (!order)",
            "\t\treturn false;",
            "",
            "\tif (fatal_signal_pending(current))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Compaction was skipped due to a lack of free order-0",
            "\t * migration targets. Continue if reclaim can help.",
            "\t */",
            "\tif (compact_result == COMPACT_SKIPPED) {",
            "\t\tret = compaction_zonelist_suitable(ac, order, alloc_flags);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/*",
            "\t * Compaction managed to coalesce some page blocks, but the",
            "\t * allocation failed presumably due to a race. Retry some.",
            "\t */",
            "\tif (compact_result == COMPACT_SUCCESS) {",
            "\t\t/*",
            "\t\t * !costly requests are much more important than",
            "\t\t * __GFP_RETRY_MAYFAIL costly ones because they are de",
            "\t\t * facto nofail and invoke OOM killer to move on while",
            "\t\t * costly can fail and users are ready to cope with",
            "\t\t * that. 1/4 retries is rather arbitrary but we would",
            "\t\t * need much more detailed feedback from compaction to",
            "\t\t * make a better decision.",
            "\t\t */",
            "\t\tif (order > PAGE_ALLOC_COSTLY_ORDER)",
            "\t\t\tmax_retries /= 4;",
            "",
            "\t\tif (++(*compaction_retries) <= max_retries) {",
            "\t\t\tret = true;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Compaction failed. Retry with increasing priority.",
            "\t */",
            "\tmin_priority = (order > PAGE_ALLOC_COSTLY_ORDER) ?",
            "\t\t\tMIN_COMPACT_COSTLY_PRIORITY : MIN_COMPACT_PRIORITY;",
            "",
            "\tif (*compact_priority > min_priority) {",
            "\t\t(*compact_priority)--;",
            "\t\t*compaction_retries = 0;",
            "\t\tret = true;",
            "\t}",
            "out:",
            "\ttrace_compact_retry(order, priority, compact_result, retries, max_retries, ret);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "warn_alloc_show_mem, warn_alloc, should_compact_retry",
          "description": "warn_alloc_show_mem 显示详细内存状态信息用于警告；warn_alloc 在内存不足时记录警告日志并触发栈跟踪；should_compact_retry 根据紧凑结果和策略决定是否重试分配，针对不同订单级别调整重试次数和优先级。",
          "similarity": 0.4963303804397583
        },
        {
          "chunk_id": 30,
          "file_path": "mm/page_alloc.c",
          "start_line": 5824,
          "end_line": 5948,
          "content": [
            "unsigned long free_reserved_area(void *start, void *end, int poison, const char *s)",
            "{",
            "\tvoid *pos;",
            "\tunsigned long pages = 0;",
            "",
            "\tstart = (void *)PAGE_ALIGN((unsigned long)start);",
            "\tend = (void *)((unsigned long)end & PAGE_MASK);",
            "\tfor (pos = start; pos < end; pos += PAGE_SIZE, pages++) {",
            "\t\tstruct page *page = virt_to_page(pos);",
            "\t\tvoid *direct_map_addr;",
            "",
            "\t\t/*",
            "\t\t * 'direct_map_addr' might be different from 'pos'",
            "\t\t * because some architectures' virt_to_page()",
            "\t\t * work with aliases.  Getting the direct map",
            "\t\t * address ensures that we get a _writeable_",
            "\t\t * alias for the memset().",
            "\t\t */",
            "\t\tdirect_map_addr = page_address(page);",
            "\t\t/*",
            "\t\t * Perform a kasan-unchecked memset() since this memory",
            "\t\t * has not been initialized.",
            "\t\t */",
            "\t\tdirect_map_addr = kasan_reset_tag(direct_map_addr);",
            "\t\tif ((unsigned int)poison <= 0xFF)",
            "\t\t\tmemset(direct_map_addr, poison, PAGE_SIZE);",
            "",
            "\t\tfree_reserved_page(page);",
            "\t}",
            "",
            "\tif (pages && s)",
            "\t\tpr_info(\"Freeing %s memory: %ldK\\n\", s, K(pages));",
            "",
            "\treturn pages;",
            "}",
            "void free_reserved_page(struct page *page)",
            "{",
            "\tclear_page_tag_ref(page);",
            "\tClearPageReserved(page);",
            "\tinit_page_count(page);",
            "\t__free_page(page);",
            "\tadjust_managed_page_count(page, 1);",
            "}",
            "static int page_alloc_cpu_dead(unsigned int cpu)",
            "{",
            "\tstruct zone *zone;",
            "",
            "\tlru_add_drain_cpu(cpu);",
            "\tmlock_drain_remote(cpu);",
            "\tdrain_pages(cpu);",
            "",
            "\t/*",
            "\t * Spill the event counters of the dead processor",
            "\t * into the current processors event counters.",
            "\t * This artificially elevates the count of the current",
            "\t * processor.",
            "\t */",
            "\tvm_events_fold_cpu(cpu);",
            "",
            "\t/*",
            "\t * Zero the differential counters of the dead processor",
            "\t * so that the vm statistics are consistent.",
            "\t *",
            "\t * This is only okay since the processor is dead and cannot",
            "\t * race with what we are doing.",
            "\t */",
            "\tcpu_vm_stats_fold(cpu);",
            "",
            "\tfor_each_populated_zone(zone)",
            "\t\tzone_pcp_update(zone, 0);",
            "",
            "\treturn 0;",
            "}",
            "static int page_alloc_cpu_online(unsigned int cpu)",
            "{",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_populated_zone(zone)",
            "\t\tzone_pcp_update(zone, 1);",
            "\treturn 0;",
            "}",
            "void __init page_alloc_init_cpuhp(void)",
            "{",
            "\tint ret;",
            "",
            "\tret = cpuhp_setup_state_nocalls(CPUHP_PAGE_ALLOC,",
            "\t\t\t\t\t\"mm/page_alloc:pcp\",",
            "\t\t\t\t\tpage_alloc_cpu_online,",
            "\t\t\t\t\tpage_alloc_cpu_dead);",
            "\tWARN_ON(ret < 0);",
            "}",
            "static void calculate_totalreserve_pages(void)",
            "{",
            "\tstruct pglist_data *pgdat;",
            "\tunsigned long reserve_pages = 0;",
            "\tenum zone_type i, j;",
            "",
            "\tfor_each_online_pgdat(pgdat) {",
            "",
            "\t\tpgdat->totalreserve_pages = 0;",
            "",
            "\t\tfor (i = 0; i < MAX_NR_ZONES; i++) {",
            "\t\t\tstruct zone *zone = pgdat->node_zones + i;",
            "\t\t\tlong max = 0;",
            "\t\t\tunsigned long managed_pages = zone_managed_pages(zone);",
            "",
            "\t\t\t/* Find valid and maximum lowmem_reserve in the zone */",
            "\t\t\tfor (j = i; j < MAX_NR_ZONES; j++) {",
            "\t\t\t\tif (zone->lowmem_reserve[j] > max)",
            "\t\t\t\t\tmax = zone->lowmem_reserve[j];",
            "\t\t\t}",
            "",
            "\t\t\t/* we treat the high watermark as reserved pages. */",
            "\t\t\tmax += high_wmark_pages(zone);",
            "",
            "\t\t\tif (max > managed_pages)",
            "\t\t\t\tmax = managed_pages;",
            "",
            "\t\t\tpgdat->totalreserve_pages += max;",
            "",
            "\t\t\treserve_pages += max;",
            "\t\t}",
            "\t}",
            "\ttotalreserve_pages = reserve_pages;",
            "}"
          ],
          "function_name": "free_reserved_area, free_reserved_page, page_alloc_cpu_dead, page_alloc_cpu_online, page_alloc_init_cpuhp, calculate_totalreserve_pages",
          "description": "实现释放保留内存区域，遍历指定范围内的页面进行初始化和释放操作，并维护统计信息。包含释放保留页面、CPU在线/离线回调及计算总保留页数等功能。",
          "similarity": 0.4958198666572571
        },
        {
          "chunk_id": 17,
          "file_path": "mm/page_alloc.c",
          "start_line": 3075,
          "end_line": 3178,
          "content": [
            "static inline long __zone_watermark_unusable_free(struct zone *z,",
            "\t\t\t\tunsigned int order, unsigned int alloc_flags)",
            "{",
            "\tlong unusable_free = (1 << order) - 1;",
            "",
            "\t/*",
            "\t * If the caller does not have rights to reserves below the min",
            "\t * watermark then subtract the high-atomic reserves. This will",
            "\t * over-estimate the size of the atomic reserve but it avoids a search.",
            "\t */",
            "\tif (likely(!(alloc_flags & ALLOC_RESERVES)))",
            "\t\tunusable_free += z->nr_reserved_highatomic;",
            "",
            "#ifdef CONFIG_CMA",
            "\t/* If allocation can't use CMA areas don't use free CMA pages */",
            "\tif (!(alloc_flags & ALLOC_CMA))",
            "\t\tunusable_free += zone_page_state(z, NR_FREE_CMA_PAGES);",
            "#endif",
            "",
            "\treturn unusable_free;",
            "}",
            "bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,",
            "\t\t\t int highest_zoneidx, unsigned int alloc_flags,",
            "\t\t\t long free_pages)",
            "{",
            "\tlong min = mark;",
            "\tint o;",
            "",
            "\t/* free_pages may go negative - that's OK */",
            "\tfree_pages -= __zone_watermark_unusable_free(z, order, alloc_flags);",
            "",
            "\tif (unlikely(alloc_flags & ALLOC_RESERVES)) {",
            "\t\t/*",
            "\t\t * __GFP_HIGH allows access to 50% of the min reserve as well",
            "\t\t * as OOM.",
            "\t\t */",
            "\t\tif (alloc_flags & ALLOC_MIN_RESERVE) {",
            "\t\t\tmin -= min / 2;",
            "",
            "\t\t\t/*",
            "\t\t\t * Non-blocking allocations (e.g. GFP_ATOMIC) can",
            "\t\t\t * access more reserves than just __GFP_HIGH. Other",
            "\t\t\t * non-blocking allocations requests such as GFP_NOWAIT",
            "\t\t\t * or (GFP_KERNEL & ~__GFP_DIRECT_RECLAIM) do not get",
            "\t\t\t * access to the min reserve.",
            "\t\t\t */",
            "\t\t\tif (alloc_flags & ALLOC_NON_BLOCK)",
            "\t\t\t\tmin -= min / 4;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * OOM victims can try even harder than the normal reserve",
            "\t\t * users on the grounds that it's definitely going to be in",
            "\t\t * the exit path shortly and free memory. Any allocation it",
            "\t\t * makes during the free path will be small and short-lived.",
            "\t\t */",
            "\t\tif (alloc_flags & ALLOC_OOM)",
            "\t\t\tmin -= min / 2;",
            "\t}",
            "",
            "\t/*",
            "\t * Check watermarks for an order-0 allocation request. If these",
            "\t * are not met, then a high-order request also cannot go ahead",
            "\t * even if a suitable page happened to be free.",
            "\t */",
            "\tif (free_pages <= min + z->lowmem_reserve[highest_zoneidx])",
            "\t\treturn false;",
            "",
            "\t/* If this is an order-0 request then the watermark is fine */",
            "\tif (!order)",
            "\t\treturn true;",
            "",
            "\t/* For a high-order request, check at least one suitable page is free */",
            "\tfor (o = order; o < NR_PAGE_ORDERS; o++) {",
            "\t\tstruct free_area *area = &z->free_area[o];",
            "\t\tint mt;",
            "",
            "\t\tif (!area->nr_free)",
            "\t\t\tcontinue;",
            "",
            "\t\tfor (mt = 0; mt < MIGRATE_PCPTYPES; mt++) {",
            "\t\t\tif (!free_area_empty(area, mt))",
            "\t\t\t\treturn true;",
            "\t\t}",
            "",
            "#ifdef CONFIG_CMA",
            "\t\tif ((alloc_flags & ALLOC_CMA) &&",
            "\t\t    !free_area_empty(area, MIGRATE_CMA)) {",
            "\t\t\treturn true;",
            "\t\t}",
            "#endif",
            "\t\tif ((alloc_flags & (ALLOC_HIGHATOMIC|ALLOC_OOM)) &&",
            "\t\t    !free_area_empty(area, MIGRATE_HIGHATOMIC)) {",
            "\t\t\treturn true;",
            "\t\t}",
            "\t}",
            "\treturn false;",
            "}",
            "bool zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,",
            "\t\t      int highest_zoneidx, unsigned int alloc_flags)",
            "{",
            "\treturn __zone_watermark_ok(z, order, mark, highest_zoneidx, alloc_flags,",
            "\t\t\t\t\tzone_page_state(z, NR_FREE_PAGES));",
            "}"
          ],
          "function_name": "__zone_watermark_unusable_free, __zone_watermark_ok, zone_watermark_ok",
          "description": "__zone_watermark_unusable_free 计算不可用的空闲页数（含保留页和 CMA）；__zone_watermark_ok 检查当前区是否满足分配水位线，考虑不同分配标志（如 RESERVES、CMA）对最小水位线的修正；zone_watermark_ok 是其标准接口调用。",
          "similarity": 0.49552884697914124
        }
      ]
    },
    {
      "source_file": "kernel/trace/ring_buffer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:07:21\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `trace\\ring_buffer.c`\n\n---\n\n# `trace/ring_buffer.c` 技术文档\n\n## 1. 文件概述\n\n`trace/ring_buffer.c` 实现了 Linux 内核中通用的高性能环形缓冲区（ring buffer）机制，主要用于跟踪（tracing）子系统。该缓冲区支持多 CPU 并发写入、单读者或多读者无锁读取，并通过时间戳压缩、事件类型编码和页面交换等技术优化内存使用和性能。该实现是 ftrace、perf 和其他内核跟踪工具的核心基础设施。\n\n## 2. 核心功能\n\n### 主要函数\n- `ring_buffer_print_entry_header()`：输出环形缓冲区条目头部格式说明，用于调试或用户空间解析。\n- `ring_buffer_event_length()`：返回事件有效载荷（payload）的长度，对 TIME_EXTEND 类型自动跳过扩展头。\n- `rb_event_data()`（内联）：返回指向事件实际数据的指针，处理 TIME_EXTEND 和不同长度编码。\n- `rb_event_length()`：返回完整事件结构（含头部）的字节长度。\n- `rb_event_ts_length()`：返回 TIME_EXTEND 事件及其后续数据事件的总长度。\n- `rb_event_data_length()`：计算数据类型事件的总长度（含头部）。\n- `rb_null_event()` / `rb_event_set_padding()`：判断或设置空/填充事件。\n\n### 关键数据结构（隐含或引用）\n- `struct ring_buffer_event`：环形缓冲区中每个事件的通用头部结构。\n- `struct buffer_data_page`：每个 CPU 缓冲区页面的封装，包含数据和元数据。\n- 每 CPU 页面链表：每个 CPU 拥有独立的环形页面链，写者仅写本地 CPU 缓冲区。\n\n### 核心常量与宏\n- `RINGBUF_TYPE_PADDING`、`RINGBUF_TYPE_TIME_EXTEND`、`RINGBUF_TYPE_TIME_STAMP`、`RINGBUF_TYPE_DATA`：事件类型标识。\n- `RB_ALIGNMENT` / `RB_ARCH_ALIGNMENT`：数据对齐策略，根据架构是否支持 64 位对齐访问调整。\n- `RB_MAX_SMALL_DATA`：小数据事件的最大长度（基于 4 字节对齐和类型长度上限）。\n- `TS_MSB` / `ABS_TS_MASK`：用于处理 59 位时间戳的高位截断与恢复。\n\n## 3. 关键实现\n\n### 无锁读写架构\n- **写者**：每个 CPU 只能写入其对应的 per-CPU 缓冲区，通过原子操作和内存屏障保证写入一致性，无需全局锁。\n- **读者**：每个 per-CPU 缓冲区维护一个独立的“reader page”。当 reader page 被读完后，通过原子交换（未来使用 `cmpxchg`）将其与环形缓冲区中的一个页面互换。交换后，原 reader page 不再被写者访问，读者可安全地将其用于 splice、复制或释放。\n\n### 事件编码与压缩\n- 事件头部使用紧凑位域编码：\n  - `type_len`（5 位）：事件类型或小数据长度（≤31）。\n  - `time_delta`（27 位）：相对于前一事件的时间增量。\n  - `array`（32 位）：用于存储大长度值或事件数据。\n- **TIME_EXTEND 事件**：当时间增量超出 27 位或需要绝对时间戳时，插入一个 8 字节的 TIME_EXTEND 事件，后跟实际数据事件。\n- **数据长度编码**：\n  - 若 `type_len > 0` 且 ≤ `RINGBUF_TYPE_DATA_TYPE_LEN_MAX`，则数据长度 = `type_len * RB_ALIGNMENT`，数据从 `array[0]` 开始。\n  - 否则，数据长度存储在 `array[0]`，实际数据从 `array[1]` 开始。\n\n### 时间戳处理\n- 绝对时间戳仅保留低 59 位（`ABS_TS_MASK`），高 5 位（`TS_MSB`）若非零需单独保存并在读取时恢复，以支持长时间运行的跟踪。\n\n### 内存对齐优化\n- 在支持 64 位对齐访问的架构上（`CONFIG_HAVE_64BIT_ALIGNED_ACCESS`），强制 8 字节对齐（`RB_FORCE_8BYTE_ALIGNMENT`），提升访问性能；否则使用 4 字节对齐。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/ring_buffer.h>`：定义公共 API 和数据结构。\n  - `<linux/trace_clock.h>`、`<linux/sched/clock.h>`：提供高精度时间戳源。\n  - `<linux/percpu.h>`：支持 per-CPU 缓冲区分配。\n  - `<linux/spinlock.h>`、`<asm/local.h>`：提供底层原子操作和锁原语。\n  - `<linux/trace_recursion.h>`：防止跟踪递归。\n- **子系统依赖**：\n  - **ftrace**：主要消费者，用于函数跟踪、事件跟踪等。\n  - **perf**：通过 ring buffer 获取性能事件数据。\n  - **Security Module**：通过 `<linux/security.h>` 集成 LSM 钩子（如 trace 访问控制）。\n- **架构依赖**：依赖 `CONFIG_HAVE_64BIT_ALIGNED_ACCESS` 配置项优化对齐策略。\n\n## 5. 使用场景\n\n- **内核跟踪（ftrace）**：记录函数调用、上下文切换、中断等事件，数据写入 per-CPU ring buffer，用户通过 `tracefs` 读取。\n- **性能分析（perf）**：perf 工具通过 ring buffer 接收内核采样事件（如 PMU 中断、软件事件）。\n- **实时监控与调试**：开发者或运维人员通过读取 ring buffer 内容分析系统行为、延迟或错误。\n- **自测试（selftest）**：文件包含自测试逻辑（依赖 `<linux/kthread.h>`），用于验证 ring buffer 功能正确性。\n- **低开销事件记录**：由于其无锁设计和压缩编码，适用于高频事件记录场景（如每秒百万级事件）。",
      "similarity": 0.5300891995429993,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 837,
          "end_line": 965,
          "content": [
            "size_t ring_buffer_nr_pages(struct trace_buffer *buffer, int cpu)",
            "{",
            "\treturn buffer->buffers[cpu]->nr_pages;",
            "}",
            "size_t ring_buffer_nr_dirty_pages(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tsize_t read;",
            "\tsize_t lost;",
            "\tsize_t cnt;",
            "",
            "\tread = local_read(&buffer->buffers[cpu]->pages_read);",
            "\tlost = local_read(&buffer->buffers[cpu]->pages_lost);",
            "\tcnt = local_read(&buffer->buffers[cpu]->pages_touched);",
            "",
            "\tif (WARN_ON_ONCE(cnt < lost))",
            "\t\treturn 0;",
            "",
            "\tcnt -= lost;",
            "",
            "\t/* The reader can read an empty page, but not more than that */",
            "\tif (cnt < read) {",
            "\t\tWARN_ON_ONCE(read > cnt + 1);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\treturn cnt - read;",
            "}",
            "static __always_inline bool full_hit(struct trace_buffer *buffer, int cpu, int full)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];",
            "\tsize_t nr_pages;",
            "\tsize_t dirty;",
            "",
            "\tnr_pages = cpu_buffer->nr_pages;",
            "\tif (!nr_pages || !full)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Add one as dirty will never equal nr_pages, as the sub-buffer",
            "\t * that the writer is on is not counted as dirty.",
            "\t * This is needed if \"buffer_percent\" is set to 100.",
            "\t */",
            "\tdirty = ring_buffer_nr_dirty_pages(buffer, cpu) + 1;",
            "",
            "\treturn (dirty * 100) >= (full * nr_pages);",
            "}",
            "static void rb_wake_up_waiters(struct irq_work *work)",
            "{",
            "\tstruct rb_irq_work *rbwork = container_of(work, struct rb_irq_work, work);",
            "",
            "\twake_up_all(&rbwork->waiters);",
            "\tif (rbwork->full_waiters_pending || rbwork->wakeup_full) {",
            "\t\t/* Only cpu_buffer sets the above flags */",
            "\t\tstruct ring_buffer_per_cpu *cpu_buffer =",
            "\t\t\tcontainer_of(rbwork, struct ring_buffer_per_cpu, irq_work);",
            "",
            "\t\t/* Called from interrupt context */",
            "\t\traw_spin_lock(&cpu_buffer->reader_lock);",
            "\t\trbwork->wakeup_full = false;",
            "\t\trbwork->full_waiters_pending = false;",
            "",
            "\t\t/* Waking up all waiters, they will reset the shortest full */",
            "\t\tcpu_buffer->shortest_full = 0;",
            "\t\traw_spin_unlock(&cpu_buffer->reader_lock);",
            "",
            "\t\twake_up_all(&rbwork->full_waiters);",
            "\t}",
            "}",
            "void ring_buffer_wake_waiters(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct rb_irq_work *rbwork;",
            "",
            "\tif (!buffer)",
            "\t\treturn;",
            "",
            "\tif (cpu == RING_BUFFER_ALL_CPUS) {",
            "",
            "\t\t/* Wake up individual ones too. One level recursion */",
            "\t\tfor_each_buffer_cpu(buffer, cpu)",
            "\t\t\tring_buffer_wake_waiters(buffer, cpu);",
            "",
            "\t\trbwork = &buffer->irq_work;",
            "\t} else {",
            "\t\tif (WARN_ON_ONCE(!buffer->buffers))",
            "\t\t\treturn;",
            "\t\tif (WARN_ON_ONCE(cpu >= nr_cpu_ids))",
            "\t\t\treturn;",
            "",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t/* The CPU buffer may not have been initialized yet */",
            "\t\tif (!cpu_buffer)",
            "\t\t\treturn;",
            "\t\trbwork = &cpu_buffer->irq_work;",
            "\t}",
            "",
            "\t/* This can be called in any context */",
            "\tirq_work_queue(&rbwork->work);",
            "}",
            "static bool rb_watermark_hit(struct trace_buffer *buffer, int cpu, int full)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tbool ret = false;",
            "",
            "\t/* Reads of all CPUs always waits for any data */",
            "\tif (cpu == RING_BUFFER_ALL_CPUS)",
            "\t\treturn !ring_buffer_empty(buffer);",
            "",
            "\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\tif (!ring_buffer_empty_cpu(buffer, cpu)) {",
            "\t\tunsigned long flags;",
            "\t\tbool pagebusy;",
            "",
            "\t\tif (!full)",
            "\t\t\treturn true;",
            "",
            "\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t\tpagebusy = cpu_buffer->reader_page == cpu_buffer->commit_page;",
            "\t\tret = !pagebusy && full_hit(buffer, cpu, full);",
            "",
            "\t\tif (!ret && (!cpu_buffer->shortest_full ||",
            "\t\t\t     cpu_buffer->shortest_full > full)) {",
            "\t\t    cpu_buffer->shortest_full = full;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "\t}",
            "\treturn ret;",
            "}"
          ],
          "function_name": "ring_buffer_nr_pages, ring_buffer_nr_dirty_pages, full_hit, rb_wake_up_waiters, ring_buffer_wake_waiters, rb_watermark_hit",
          "description": "实现缓冲区页数统计功能，包含满状态检测算法，提供唤醒等待者的中断处理函数，支持全CPU模式和特定CPU的唤醒操作，包含水位标记判断逻辑控制缓冲区回收行为",
          "similarity": 0.5699200630187988
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 2052,
          "end_line": 2363,
          "content": [
            "static bool",
            "rb_insert_pages(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tstruct list_head *pages = &cpu_buffer->new_pages;",
            "\tunsigned long flags;",
            "\tbool success;",
            "\tint retries;",
            "",
            "\t/* Can be called at early boot up, where interrupts must not been enabled */",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t/*",
            "\t * We are holding the reader lock, so the reader page won't be swapped",
            "\t * in the ring buffer. Now we are racing with the writer trying to",
            "\t * move head page and the tail page.",
            "\t * We are going to adapt the reader page update process where:",
            "\t * 1. We first splice the start and end of list of new pages between",
            "\t *    the head page and its previous page.",
            "\t * 2. We cmpxchg the prev_page->next to point from head page to the",
            "\t *    start of new pages list.",
            "\t * 3. Finally, we update the head->prev to the end of new list.",
            "\t *",
            "\t * We will try this process 10 times, to make sure that we don't keep",
            "\t * spinning.",
            "\t */",
            "\tretries = 10;",
            "\tsuccess = false;",
            "\twhile (retries--) {",
            "\t\tstruct list_head *head_page, *prev_page, *r;",
            "\t\tstruct list_head *last_page, *first_page;",
            "\t\tstruct list_head *head_page_with_bit;",
            "\t\tstruct buffer_page *hpage = rb_set_head_page(cpu_buffer);",
            "",
            "\t\tif (!hpage)",
            "\t\t\tbreak;",
            "\t\thead_page = &hpage->list;",
            "\t\tprev_page = head_page->prev;",
            "",
            "\t\tfirst_page = pages->next;",
            "\t\tlast_page  = pages->prev;",
            "",
            "\t\thead_page_with_bit = (struct list_head *)",
            "\t\t\t\t     ((unsigned long)head_page | RB_PAGE_HEAD);",
            "",
            "\t\tlast_page->next = head_page_with_bit;",
            "\t\tfirst_page->prev = prev_page;",
            "",
            "\t\tr = cmpxchg(&prev_page->next, head_page_with_bit, first_page);",
            "",
            "\t\tif (r == head_page_with_bit) {",
            "\t\t\t/*",
            "\t\t\t * yay, we replaced the page pointer to our new list,",
            "\t\t\t * now, we just have to update to head page's prev",
            "\t\t\t * pointer to point to end of list",
            "\t\t\t */",
            "\t\t\thead_page->prev = last_page;",
            "\t\t\tsuccess = true;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tif (success)",
            "\t\tINIT_LIST_HEAD(pages);",
            "\t/*",
            "\t * If we weren't successful in adding in new pages, warn and stop",
            "\t * tracing",
            "\t */",
            "\tRB_WARN_ON(cpu_buffer, !success);",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "",
            "\t/* free pages if they weren't inserted */",
            "\tif (!success) {",
            "\t\tstruct buffer_page *bpage, *tmp;",
            "\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,",
            "\t\t\t\t\t list) {",
            "\t\t\tlist_del_init(&bpage->list);",
            "\t\t\tfree_buffer_page(bpage);",
            "\t\t}",
            "\t}",
            "\treturn success;",
            "}",
            "static void rb_update_pages(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tbool success;",
            "",
            "\tif (cpu_buffer->nr_pages_to_update > 0)",
            "\t\tsuccess = rb_insert_pages(cpu_buffer);",
            "\telse",
            "\t\tsuccess = rb_remove_pages(cpu_buffer,",
            "\t\t\t\t\t-cpu_buffer->nr_pages_to_update);",
            "",
            "\tif (success)",
            "\t\tcpu_buffer->nr_pages += cpu_buffer->nr_pages_to_update;",
            "}",
            "static void update_pages_handler(struct work_struct *work)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = container_of(work,",
            "\t\t\tstruct ring_buffer_per_cpu, update_pages_work);",
            "\trb_update_pages(cpu_buffer);",
            "\tcomplete(&cpu_buffer->update_done);",
            "}",
            "int ring_buffer_resize(struct trace_buffer *buffer, unsigned long size,",
            "\t\t\tint cpu_id)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tunsigned long nr_pages;",
            "\tint cpu, err;",
            "",
            "\t/*",
            "\t * Always succeed at resizing a non-existent buffer:",
            "\t */",
            "\tif (!buffer)",
            "\t\treturn 0;",
            "",
            "\t/* Make sure the requested buffer exists */",
            "\tif (cpu_id != RING_BUFFER_ALL_CPUS &&",
            "\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))",
            "\t\treturn 0;",
            "",
            "\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);",
            "",
            "\t/* we need a minimum of two pages */",
            "\tif (nr_pages < 2)",
            "\t\tnr_pages = 2;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "\tatomic_inc(&buffer->resizing);",
            "",
            "\tif (cpu_id == RING_BUFFER_ALL_CPUS) {",
            "\t\t/*",
            "\t\t * Don't succeed if resizing is disabled, as a reader might be",
            "\t\t * manipulating the ring buffer and is expecting a sane state while",
            "\t\t * this is true.",
            "\t\t */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {",
            "\t\t\t\terr = -EBUSY;",
            "\t\t\t\tgoto out_err_unlock;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* calculate the pages to update */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -",
            "\t\t\t\t\t\t\tcpu_buffer->nr_pages;",
            "\t\t\t/*",
            "\t\t\t * nothing more to do for removing pages or no update",
            "\t\t\t */",
            "\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)",
            "\t\t\t\tcontinue;",
            "\t\t\t/*",
            "\t\t\t * to add pages, make sure all new pages can be",
            "\t\t\t * allocated without receiving ENOMEM",
            "\t\t\t */",
            "\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\t\t\tif (__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,",
            "\t\t\t\t\t\t&cpu_buffer->new_pages)) {",
            "\t\t\t\t/* not enough memory for new pages */",
            "\t\t\t\terr = -ENOMEM;",
            "\t\t\t\tgoto out_err;",
            "\t\t\t}",
            "",
            "\t\t\tcond_resched();",
            "\t\t}",
            "",
            "\t\tcpus_read_lock();",
            "\t\t/*",
            "\t\t * Fire off all the required work handlers",
            "\t\t * We can't schedule on offline CPUs, but it's not necessary",
            "\t\t * since we can change their buffer sizes without any race.",
            "\t\t */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (!cpu_buffer->nr_pages_to_update)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\t/* Can't run something on an offline CPU. */",
            "\t\t\tif (!cpu_online(cpu)) {",
            "\t\t\t\trb_update_pages(cpu_buffer);",
            "\t\t\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\t\t} else {",
            "\t\t\t\t/* Run directly if possible. */",
            "\t\t\t\tmigrate_disable();",
            "\t\t\t\tif (cpu != smp_processor_id()) {",
            "\t\t\t\t\tmigrate_enable();",
            "\t\t\t\t\tschedule_work_on(cpu,",
            "\t\t\t\t\t\t\t &cpu_buffer->update_pages_work);",
            "\t\t\t\t} else {",
            "\t\t\t\t\tupdate_pages_handler(&cpu_buffer->update_pages_work);",
            "\t\t\t\t\tmigrate_enable();",
            "\t\t\t\t}",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* wait for all the updates to complete */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (!cpu_buffer->nr_pages_to_update)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tif (cpu_online(cpu))",
            "\t\t\t\twait_for_completion(&cpu_buffer->update_done);",
            "\t\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\t}",
            "",
            "\t\tcpus_read_unlock();",
            "\t} else {",
            "\t\tcpu_buffer = buffer->buffers[cpu_id];",
            "",
            "\t\tif (nr_pages == cpu_buffer->nr_pages)",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * Don't succeed if resizing is disabled, as a reader might be",
            "\t\t * manipulating the ring buffer and is expecting a sane state while",
            "\t\t * this is true.",
            "\t\t */",
            "\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {",
            "\t\t\terr = -EBUSY;",
            "\t\t\tgoto out_err_unlock;",
            "\t\t}",
            "",
            "\t\tcpu_buffer->nr_pages_to_update = nr_pages -",
            "\t\t\t\t\t\tcpu_buffer->nr_pages;",
            "",
            "\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\t\tif (cpu_buffer->nr_pages_to_update > 0 &&",
            "\t\t\t__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,",
            "\t\t\t\t\t    &cpu_buffer->new_pages)) {",
            "\t\t\terr = -ENOMEM;",
            "\t\t\tgoto out_err;",
            "\t\t}",
            "",
            "\t\tcpus_read_lock();",
            "",
            "\t\t/* Can't run something on an offline CPU. */",
            "\t\tif (!cpu_online(cpu_id))",
            "\t\t\trb_update_pages(cpu_buffer);",
            "\t\telse {",
            "\t\t\t/* Run directly if possible. */",
            "\t\t\tmigrate_disable();",
            "\t\t\tif (cpu_id == smp_processor_id()) {",
            "\t\t\t\trb_update_pages(cpu_buffer);",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t} else {",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t\tschedule_work_on(cpu_id,",
            "\t\t\t\t\t\t &cpu_buffer->update_pages_work);",
            "\t\t\t\twait_for_completion(&cpu_buffer->update_done);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\tcpus_read_unlock();",
            "\t}",
            "",
            " out:",
            "\t/*",
            "\t * The ring buffer resize can happen with the ring buffer",
            "\t * enabled, so that the update disturbs the tracing as little",
            "\t * as possible. But if the buffer is disabled, we do not need",
            "\t * to worry about that, and we can take the time to verify",
            "\t * that the buffer is not corrupt.",
            "\t */",
            "\tif (atomic_read(&buffer->record_disabled)) {",
            "\t\tatomic_inc(&buffer->record_disabled);",
            "\t\t/*",
            "\t\t * Even though the buffer was disabled, we must make sure",
            "\t\t * that it is truly disabled before calling rb_check_pages.",
            "\t\t * There could have been a race between checking",
            "\t\t * record_disable and incrementing it.",
            "\t\t */",
            "\t\tsynchronize_rcu();",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tunsigned long flags;",
            "",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t\t\trb_check_pages(cpu_buffer);",
            "\t\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "\t\t}",
            "\t\tatomic_dec(&buffer->record_disabled);",
            "\t}",
            "",
            "\tatomic_dec(&buffer->resizing);",
            "\tmutex_unlock(&buffer->mutex);",
            "\treturn 0;",
            "",
            " out_err:",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tstruct buffer_page *bpage, *tmp;",
            "",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\tcpu_buffer->nr_pages_to_update = 0;",
            "",
            "\t\tif (list_empty(&cpu_buffer->new_pages))",
            "\t\t\tcontinue;",
            "",
            "\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,",
            "\t\t\t\t\tlist) {",
            "\t\t\tlist_del_init(&bpage->list);",
            "\t\t\tfree_buffer_page(bpage);",
            "\t\t}",
            "\t}",
            " out_err_unlock:",
            "\tatomic_dec(&buffer->resizing);",
            "\tmutex_unlock(&buffer->mutex);",
            "\treturn err;",
            "}"
          ],
          "function_name": "rb_insert_pages, rb_update_pages, update_pages_handler, ring_buffer_resize",
          "description": "实现将新分配的缓冲页插入到环形缓冲区的头部，通过CAS操作确保线程安全地更新链表结构，若失败则释放内存资源。包含调整缓冲区大小的核心逻辑，协调多CPU上的页面分配与更新操作。",
          "similarity": 0.5492526292800903
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 45,
          "end_line": 148,
          "content": [
            "int ring_buffer_print_entry_header(struct trace_seq *s)",
            "{",
            "\ttrace_seq_puts(s, \"# compressed entry header\\n\");",
            "\ttrace_seq_puts(s, \"\\ttype_len    :    5 bits\\n\");",
            "\ttrace_seq_puts(s, \"\\ttime_delta  :   27 bits\\n\");",
            "\ttrace_seq_puts(s, \"\\tarray       :   32 bits\\n\");",
            "\ttrace_seq_putc(s, '\\n');",
            "\ttrace_seq_printf(s, \"\\tpadding     : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_PADDING);",
            "\ttrace_seq_printf(s, \"\\ttime_extend : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_TIME_EXTEND);",
            "\ttrace_seq_printf(s, \"\\ttime_stamp : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_TIME_STAMP);",
            "\ttrace_seq_printf(s, \"\\tdata max type_len  == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_DATA_TYPE_LEN_MAX);",
            "",
            "\treturn !trace_seq_has_overflowed(s);",
            "}",
            "static inline bool rb_null_event(struct ring_buffer_event *event)",
            "{",
            "\treturn event->type_len == RINGBUF_TYPE_PADDING && !event->time_delta;",
            "}",
            "static void rb_event_set_padding(struct ring_buffer_event *event)",
            "{",
            "\t/* padding has a NULL time_delta */",
            "\tevent->type_len = RINGBUF_TYPE_PADDING;",
            "\tevent->time_delta = 0;",
            "}",
            "static unsigned",
            "rb_event_data_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned length;",
            "",
            "\tif (event->type_len)",
            "\t\tlength = event->type_len * RB_ALIGNMENT;",
            "\telse",
            "\t\tlength = event->array[0];",
            "\treturn length + RB_EVNT_HDR_SIZE;",
            "}",
            "static inline unsigned",
            "rb_event_length(struct ring_buffer_event *event)",
            "{",
            "\tswitch (event->type_len) {",
            "\tcase RINGBUF_TYPE_PADDING:",
            "\t\tif (rb_null_event(event))",
            "\t\t\t/* undefined */",
            "\t\t\treturn -1;",
            "\t\treturn  event->array[0] + RB_EVNT_HDR_SIZE;",
            "",
            "\tcase RINGBUF_TYPE_TIME_EXTEND:",
            "\t\treturn RB_LEN_TIME_EXTEND;",
            "",
            "\tcase RINGBUF_TYPE_TIME_STAMP:",
            "\t\treturn RB_LEN_TIME_STAMP;",
            "",
            "\tcase RINGBUF_TYPE_DATA:",
            "\t\treturn rb_event_data_length(event);",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(1);",
            "\t}",
            "\t/* not hit */",
            "\treturn 0;",
            "}",
            "static inline unsigned",
            "rb_event_ts_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned len = 0;",
            "",
            "\tif (extended_time(event)) {",
            "\t\t/* time extends include the data event after it */",
            "\t\tlen = RB_LEN_TIME_EXTEND;",
            "\t\tevent = skip_time_extend(event);",
            "\t}",
            "\treturn len + rb_event_length(event);",
            "}",
            "unsigned ring_buffer_event_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned length;",
            "",
            "\tif (extended_time(event))",
            "\t\tevent = skip_time_extend(event);",
            "",
            "\tlength = rb_event_length(event);",
            "\tif (event->type_len > RINGBUF_TYPE_DATA_TYPE_LEN_MAX)",
            "\t\treturn length;",
            "\tlength -= RB_EVNT_HDR_SIZE;",
            "\tif (length > RB_MAX_SMALL_DATA + sizeof(event->array[0]))",
            "                length -= sizeof(event->array[0]);",
            "\treturn length;",
            "}",
            "static u64 rb_event_time_stamp(struct ring_buffer_event *event)",
            "{",
            "\tu64 ts;",
            "",
            "\tts = event->array[0];",
            "\tts <<= TS_SHIFT;",
            "\tts += event->time_delta;",
            "",
            "\treturn ts;",
            "}",
            "static void rb_init_page(struct buffer_data_page *bpage)",
            "{",
            "\tlocal_set(&bpage->commit, 0);",
            "}"
          ],
          "function_name": "ring_buffer_print_entry_header, rb_null_event, rb_event_set_padding, rb_event_data_length, rb_event_length, rb_event_ts_length, ring_buffer_event_length, rb_event_time_stamp, rb_init_page",
          "description": "实现环形缓冲区事件解析功能，包括打印事件头信息、识别空事件、设置填充事件、计算不同事件类型的数据长度及时间戳，提供事件长度和时间戳读取接口",
          "similarity": 0.525545060634613
        },
        {
          "chunk_id": 23,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 5310,
          "end_line": 5415,
          "content": [
            "static void reset_disabled_cpu_buffer(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "",
            "\tif (RB_WARN_ON(cpu_buffer, local_read(&cpu_buffer->committing)))",
            "\t\tgoto out;",
            "",
            "\tarch_spin_lock(&cpu_buffer->lock);",
            "",
            "\trb_reset_cpu(cpu_buffer);",
            "",
            "\tarch_spin_unlock(&cpu_buffer->lock);",
            "",
            " out:",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "}",
            "void ring_buffer_reset_cpu(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];",
            "",
            "\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\treturn;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tatomic_inc(&cpu_buffer->resize_disabled);",
            "\tatomic_inc(&cpu_buffer->record_disabled);",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\tatomic_dec(&cpu_buffer->record_disabled);",
            "\tatomic_dec(&cpu_buffer->resize_disabled);",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}",
            "void ring_buffer_reset_online_cpus(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tfor_each_online_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\tatomic_add(RESET_BIT, &cpu_buffer->resize_disabled);",
            "\t\tatomic_inc(&cpu_buffer->record_disabled);",
            "\t}",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\t/*",
            "\t\t * If a CPU came online during the synchronize_rcu(), then",
            "\t\t * ignore it.",
            "\t\t */",
            "\t\tif (!(atomic_read(&cpu_buffer->resize_disabled) & RESET_BIT))",
            "\t\t\tcontinue;",
            "",
            "\t\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\t\tatomic_dec(&cpu_buffer->record_disabled);",
            "\t\tatomic_sub(RESET_BIT, &cpu_buffer->resize_disabled);",
            "\t}",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}",
            "void ring_buffer_reset(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\tatomic_inc(&cpu_buffer->resize_disabled);",
            "\t\tatomic_inc(&cpu_buffer->record_disabled);",
            "\t}",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\t\tatomic_dec(&cpu_buffer->record_disabled);",
            "\t\tatomic_dec(&cpu_buffer->resize_disabled);",
            "\t}",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}"
          ],
          "function_name": "reset_disabled_cpu_buffer, ring_buffer_reset_cpu, ring_buffer_reset_online_cpus, ring_buffer_reset",
          "description": "该代码段实现了对跟踪环形缓冲区的重置机制，通过原子操作与RCU同步确保多线程安全。  \n`reset_disabled_cpu_buffer`负责安全地重置指定CPU的缓冲区，`ring_buffer_reset_cpu`、`ring_buffer_reset_online_cpus`和`ring_buffer_reset`分别针对单个CPU、在线CPU及全系统CPU执行重置，均通过原子计数器控制访问权限并阻塞数据提交。  \n由于`rb_reset_cpu`未在当前代码片段中定义，故其具体行为依赖上下文信息。",
          "similarity": 0.5248713493347168
        },
        {
          "chunk_id": 18,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 3995,
          "end_line": 4117,
          "content": [
            "static bool rb_per_cpu_empty(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tstruct buffer_page *reader = cpu_buffer->reader_page;",
            "\tstruct buffer_page *head = rb_set_head_page(cpu_buffer);",
            "\tstruct buffer_page *commit = cpu_buffer->commit_page;",
            "",
            "\t/* In case of error, head will be NULL */",
            "\tif (unlikely(!head))",
            "\t\treturn true;",
            "",
            "\t/* Reader should exhaust content in reader page */",
            "\tif (reader->read != rb_page_commit(reader))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * If writers are committing on the reader page, knowing all",
            "\t * committed content has been read, the ring buffer is empty.",
            "\t */",
            "\tif (commit == reader)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * If writers are committing on a page other than reader page",
            "\t * and head page, there should always be content to read.",
            "\t */",
            "\tif (commit != head)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Writers are committing on the head page, we just need",
            "\t * to care about there're committed data, and the reader will",
            "\t * swap reader page with head page when it is to read data.",
            "\t */",
            "\treturn rb_page_commit(commit) == 0;",
            "}",
            "void ring_buffer_record_disable(struct trace_buffer *buffer)",
            "{",
            "\tatomic_inc(&buffer->record_disabled);",
            "}",
            "void ring_buffer_record_enable(struct trace_buffer *buffer)",
            "{",
            "\tatomic_dec(&buffer->record_disabled);",
            "}",
            "void ring_buffer_record_off(struct trace_buffer *buffer)",
            "{",
            "\tunsigned int rd;",
            "\tunsigned int new_rd;",
            "",
            "\trd = atomic_read(&buffer->record_disabled);",
            "\tdo {",
            "\t\tnew_rd = rd | RB_BUFFER_OFF;",
            "\t} while (!atomic_try_cmpxchg(&buffer->record_disabled, &rd, new_rd));",
            "}",
            "void ring_buffer_record_on(struct trace_buffer *buffer)",
            "{",
            "\tunsigned int rd;",
            "\tunsigned int new_rd;",
            "",
            "\trd = atomic_read(&buffer->record_disabled);",
            "\tdo {",
            "\t\tnew_rd = rd & ~RB_BUFFER_OFF;",
            "\t} while (!atomic_try_cmpxchg(&buffer->record_disabled, &rd, new_rd));",
            "}",
            "bool ring_buffer_record_is_on(struct trace_buffer *buffer)",
            "{",
            "\treturn !atomic_read(&buffer->record_disabled);",
            "}",
            "bool ring_buffer_record_is_set_on(struct trace_buffer *buffer)",
            "{",
            "\treturn !(atomic_read(&buffer->record_disabled) & RB_BUFFER_OFF);",
            "}",
            "void ring_buffer_record_disable_cpu(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "",
            "\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\treturn;",
            "",
            "\tcpu_buffer = buffer->buffers[cpu];",
            "\tatomic_inc(&cpu_buffer->record_disabled);",
            "}",
            "void ring_buffer_record_enable_cpu(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "",
            "\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\treturn;",
            "",
            "\tcpu_buffer = buffer->buffers[cpu];",
            "\tatomic_dec(&cpu_buffer->record_disabled);",
            "}",
            "static inline unsigned long",
            "rb_num_of_entries(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\treturn local_read(&cpu_buffer->entries) -",
            "\t\t(local_read(&cpu_buffer->overrun) + cpu_buffer->read);",
            "}",
            "u64 ring_buffer_oldest_event_ts(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct buffer_page *bpage;",
            "\tu64 ret = 0;",
            "",
            "\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\treturn 0;",
            "",
            "\tcpu_buffer = buffer->buffers[cpu];",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t/*",
            "\t * if the tail is on reader_page, oldest time stamp is on the reader",
            "\t * page",
            "\t */",
            "\tif (cpu_buffer->tail_page == cpu_buffer->reader_page)",
            "\t\tbpage = cpu_buffer->reader_page;",
            "\telse",
            "\t\tbpage = rb_set_head_page(cpu_buffer);",
            "\tif (bpage)",
            "\t\tret = bpage->page->time_stamp;",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "rb_per_cpu_empty, ring_buffer_record_disable, ring_buffer_record_enable, ring_buffer_record_off, ring_buffer_record_on, ring_buffer_record_is_on, ring_buffer_record_is_set_on, ring_buffer_record_disable_cpu, ring_buffer_record_enable_cpu, rb_num_of_entries, ring_buffer_oldest_event_ts",
          "description": "rb_per_cpu_empty 判断当前CPU缓冲区是否为空；ring_buffer_record_disable/enable 控制全局记录状态；ring_buffer_record_off/on 设置记录开关标志位；rb_num_of_entries 计算当前缓冲区条目数；ring_buffer_oldest_event_ts 获取最早事件时间戳",
          "similarity": 0.5222182273864746
        }
      ]
    }
  ]
}