{
  "query": "分页如何解决碎片问题",
  "timestamp": "2025-12-26 00:11:12",
  "retrieved_files": [
    {
      "source_file": "mm/swap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:25:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `swap.c`\n\n---\n\n# swap.c 技术文档\n\n## 1. 文件概述\n\n`swap.c` 是 Linux 内核内存管理子系统（MM）中的核心文件之一，主要负责页面回收（page reclaim）、LRU（Least Recently Used）链表管理、页面释放以及与交换（swap）机制相关的底层支持逻辑。尽管文件名为 `swap.c`，但其功能不仅限于交换，而是涵盖了通用的页面生命周期管理、LRU 链表操作、页面引用计数释放、可回收性判断等关键内存管理任务。该文件为页面缓存（page cache）、匿名页（anonymous pages）和大页（huge pages）提供统一的释放与 LRU 管理接口。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `page_cluster`：控制一次 I/O 操作中尝试换入/换出的页面数量（以 2 的幂表示），默认值由系统配置决定。\n- `page_cluster_max`：`page_cluster` 的最大允许值（31，即最多 2^31 页，实际受架构限制）。\n\n### 主要数据结构\n- `struct lru_rotate`：每个 CPU 私有的结构，用于在中断禁用上下文中批量处理需移至 LRU 链表尾部的页面（如 `folio_rotate_reclaimable` 场景）。\n- `struct cpu_fbatches`：每个 CPU 私有的 folio 批处理结构，包含多个 folio_batch，用于高效地向 LRU 链表添加、停用或激活页面，避免频繁获取 LRU 锁。\n\n### 主要函数\n- `__folio_put()`：释放一个 folio 的核心函数，根据 folio 类型（设备内存、大页、普通页）调用相应的释放路径。\n- `put_pages_list()`：批量释放通过 `lru` 字段链接的页面列表，常用于网络子系统或 compound page 释放。\n- `lru_add_fn()`：将 folio 添加到对应 LRU 链表的回调函数，处理可回收性（evictable/unevictable）状态转换和统计计数。\n- `folio_batch_move_lru()`：批量执行 LRU 操作（如添加、移动），在持有 LRU 锁期间完成所有 folio 的处理。\n- `folio_rotate_reclaimable()`：在写回完成后，若页面仍可回收，则将其移至 inactive LRU 链表尾部，以延迟其被回收的时间。\n- `lru_note_cost()`：记录 LRU 扫描过程中的 I/O 和旋转（rotation）成本，用于后续调整 anon/file LRU 的扫描比例。\n\n## 3. 关键实现\n\n### LRU 批处理机制\n为减少 LRU 锁竞争，内核采用 per-CPU 批处理（`folio_batch`）方式暂存待处理的 folio。当批处理满或遇到大页（`folio_test_large`）时，才批量获取 LRU 锁并执行操作（如 `lru_add_fn`）。这显著提升了高并发场景下的性能。\n\n### 可回收性管理\n页面是否可回收由 `folio_evictable()` 判断，主要依据是否被 mlock 锁定。在添加到 LRU 时：\n- 若页面变为可回收（原为 unevictable），则增加 `UNEVICTABLE_PGRESCUED` 统计；\n- 若页面不可回收，则清除 active 标志，设置 unevictable 标志，并重置 `mlock_count`，同时增加 `UNEVICTABLE_PGCULLED` 统计。\n\n### 页面释放路径\n`__folio_put()` 是 folio 引用计数归零后的释放入口：\n1. 设备内存 folio 调用 `free_zone_device_folio()`\n2. 大页 folio 调用 `free_huge_folio()`\n3. 普通 folio 先从 LRU 移除（若在 LRU 上），然后解绑内存控制组（memcg），最后调用 `free_unref_page()` 释放到伙伴系统。\n\n### LRU 旋转优化\n`folio_rotate_reclaimable()` 在写回结束时，若页面干净且未锁定，则将其移至 inactive LRU 尾部。此操作通过 per-CPU 的 `lru_rotate` 批处理完成，仅在必要时获取 LRU 锁，避免影响写回关键路径性能。\n\n### 成本跟踪\n`lru_note_cost()` 通过累加 `nr_io * SWAP_CLUSTER_MAX + nr_rotated` 来量化扫描成本，用于动态调整匿名页与文件页 LRU 的扫描比例，优化内存回收效率。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/pagevec.h>`、`\"internal.h\"` 等，使用伙伴系统、LRU 框架、内存控制组（memcg）等基础组件。\n- **交换子系统**：虽不直接实现 swap read/write，但为 `vmscan.c` 中的页面回收提供 LRU 操作接口，是 swap 机制的支撑模块。\n- **大页支持**：通过 `hugetlb.h` 与大页子系统交互，特殊处理大页释放。\n- **设备内存**：通过 `memremap.h` 支持持久内存（pmem）等 zone device 页面的释放。\n- **跟踪与统计**：使用 tracepoint（`trace/events/pagemap.h`）和 VM 统计（`kernel_stat.h`）进行性能分析。\n- **SMP 支持**：大量使用 per-CPU 变量（`DEFINE_PER_CPU`）和本地锁（`local_lock_t`）优化多核性能。\n\n## 5. 使用场景\n\n- **页面回收（Reclaim）**：当内存压力触发 kswapd 或 direct reclaim 时，`vmscan.c` 调用本文件的 LRU 操作函数来隔离、释放页面。\n- **页面缓存释放**：文件系统或网络子系统在释放 page cache 页面时，通过 `__folio_put()` 或 `put_pages_list()` 触发 LRU 移除和内存释放。\n- **写回完成处理**：块设备或文件系统在完成脏页写回后，调用 `folio_rotate_reclaimable()` 更新页面在 LRU 中的位置。\n- **内存控制组（cgroup）**：memcg 回收内存时，复用本文件的 LRU 批处理和 folio 释放逻辑。\n- **大页与设备内存管理**：透明大页（THP）或持久内存应用释放页面时，通过统一的 `__folio_put()` 接口分发到专用释放函数。\n- **系统调优**：管理员通过 `/proc/sys/vm/page-cluster` 调整 `page_cluster` 值，影响 swap 和 page cache 的 I/O 批量大小。",
      "similarity": 0.5571019649505615,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/swap.c",
          "start_line": 77,
          "end_line": 190,
          "content": [
            "static void __page_cache_release(struct folio *folio, struct lruvec **lruvecp,",
            "\t\tunsigned long *flagsp)",
            "{",
            "\tif (folio_test_lru(folio)) {",
            "\t\tfolio_lruvec_relock_irqsave(folio, lruvecp, flagsp);",
            "\t\tlruvec_del_folio(*lruvecp, folio);",
            "\t\t__folio_clear_lru_flags(folio);",
            "\t}",
            "}",
            "static void page_cache_release(struct folio *folio)",
            "{",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags;",
            "",
            "\t__page_cache_release(folio, &lruvec, &flags);",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "}",
            "void __folio_put(struct folio *folio)",
            "{",
            "\tif (unlikely(folio_is_zone_device(folio))) {",
            "\t\tfree_zone_device_folio(folio);",
            "\t\treturn;",
            "\t} else if (folio_test_hugetlb(folio)) {",
            "\t\tfree_huge_folio(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tpage_cache_release(folio);",
            "\tfolio_unqueue_deferred_split(folio);",
            "\tmem_cgroup_uncharge(folio);",
            "\tfree_unref_page(&folio->page, folio_order(folio));",
            "}",
            "void put_pages_list(struct list_head *pages)",
            "{",
            "\tstruct folio_batch fbatch;",
            "\tstruct folio *folio, *next;",
            "",
            "\tfolio_batch_init(&fbatch);",
            "\tlist_for_each_entry_safe(folio, next, pages, lru) {",
            "\t\tif (!folio_put_testzero(folio))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_hugetlb(folio)) {",
            "\t\t\tfree_huge_folio(folio);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\t/* LRU flag must be clear because it's passed using the lru */",
            "\t\tif (folio_batch_add(&fbatch, folio) > 0)",
            "\t\t\tcontinue;",
            "\t\tfree_unref_folios(&fbatch);",
            "\t}",
            "",
            "\tif (fbatch.nr)",
            "\t\tfree_unref_folios(&fbatch);",
            "\tINIT_LIST_HEAD(pages);",
            "}",
            "static void lru_add_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tint was_unevictable = folio_test_clear_unevictable(folio);",
            "\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\t/*",
            "\t * Is an smp_mb__after_atomic() still required here, before",
            "\t * folio_evictable() tests the mlocked flag, to rule out the possibility",
            "\t * of stranding an evictable folio on an unevictable LRU?  I think",
            "\t * not, because __munlock_folio() only clears the mlocked flag",
            "\t * while the LRU lock is held.",
            "\t *",
            "\t * (That is not true of __page_cache_release(), and not necessarily",
            "\t * true of folios_put(): but those only clear the mlocked flag after",
            "\t * folio_put_testzero() has excluded any other users of the folio.)",
            "\t */",
            "\tif (folio_evictable(folio)) {",
            "\t\tif (was_unevictable)",
            "\t\t\t__count_vm_events(UNEVICTABLE_PGRESCUED, nr_pages);",
            "\t} else {",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_set_unevictable(folio);",
            "\t\t/*",
            "\t\t * folio->mlock_count = !!folio_test_mlocked(folio)?",
            "\t\t * But that leaves __mlock_folio() in doubt whether another",
            "\t\t * actor has already counted the mlock or not.  Err on the",
            "\t\t * safe side, underestimate, let page reclaim fix it, rather",
            "\t\t * than leaving a page on the unevictable LRU indefinitely.",
            "\t\t */",
            "\t\tfolio->mlock_count = 0;",
            "\t\tif (!was_unevictable)",
            "\t\t\t__count_vm_events(UNEVICTABLE_PGCULLED, nr_pages);",
            "\t}",
            "",
            "\tlruvec_add_folio(lruvec, folio);",
            "\ttrace_mm_lru_insertion(folio);",
            "}",
            "static void folio_batch_move_lru(struct folio_batch *fbatch, move_fn_t move_fn)",
            "{",
            "\tint i;",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags = 0;",
            "",
            "\tfor (i = 0; i < folio_batch_count(fbatch); i++) {",
            "\t\tstruct folio *folio = fbatch->folios[i];",
            "",
            "\t\tfolio_lruvec_relock_irqsave(folio, &lruvec, &flags);",
            "\t\tmove_fn(lruvec, folio);",
            "",
            "\t\tfolio_set_lru(folio);",
            "\t}",
            "",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\tfolios_put(fbatch);",
            "}"
          ],
          "function_name": "__page_cache_release, page_cache_release, __folio_put, put_pages_list, lru_add_fn, folio_batch_move_lru",
          "description": "实现了页面缓存释放和LRU列表维护逻辑，包含__page_cache_release用于从LRU列表移除页面，page_cache_release处理普通页面释放流程，__folio_put负责释放非设备映射和大页，put_pages_list批量处理页面释放，lru_add_fn将页面添加到LRU列表并根据是否可交换设置相应标志。",
          "similarity": 0.580698549747467
        },
        {
          "chunk_id": 8,
          "file_path": "mm/swap.c",
          "start_line": 1079,
          "end_line": 1111,
          "content": [
            "void __folio_batch_release(struct folio_batch *fbatch)",
            "{",
            "\tif (!fbatch->percpu_pvec_drained) {",
            "\t\tlru_add_drain();",
            "\t\tfbatch->percpu_pvec_drained = true;",
            "\t}",
            "\tfolios_put(fbatch);",
            "}",
            "void folio_batch_remove_exceptionals(struct folio_batch *fbatch)",
            "{",
            "\tunsigned int i, j;",
            "",
            "\tfor (i = 0, j = 0; i < folio_batch_count(fbatch); i++) {",
            "\t\tstruct folio *folio = fbatch->folios[i];",
            "\t\tif (!xa_is_value(folio))",
            "\t\t\tfbatch->folios[j++] = folio;",
            "\t}",
            "\tfbatch->nr = j;",
            "}",
            "void __init swap_setup(void)",
            "{",
            "\tunsigned long megs = totalram_pages() >> (20 - PAGE_SHIFT);",
            "",
            "\t/* Use a smaller cluster for small-memory machines */",
            "\tif (megs < 16)",
            "\t\tpage_cluster = 2;",
            "\telse",
            "\t\tpage_cluster = 3;",
            "\t/*",
            "\t * Right now other parts of the system means that we",
            "\t * _really_ don't want to cluster much more",
            "\t */",
            "}"
          ],
          "function_name": "__folio_batch_release, folio_batch_remove_exceptionals, swap_setup",
          "description": "__folio_batch_release 标记并释放页面批次引用，folio_batch_remove_exceptionals 清理异常条目；swap_setup 初始化页面聚类参数，根据内存大小调整page_cluster值。",
          "similarity": 0.5689963698387146
        },
        {
          "chunk_id": 5,
          "file_path": "mm/swap.c",
          "start_line": 641,
          "end_line": 743,
          "content": [
            "void lru_add_drain_cpu(int cpu)",
            "{",
            "\tstruct cpu_fbatches *fbatches = &per_cpu(cpu_fbatches, cpu);",
            "\tstruct folio_batch *fbatch = &fbatches->lru_add;",
            "",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, lru_add_fn);",
            "",
            "\tfbatch = &per_cpu(lru_rotate.fbatch, cpu);",
            "\t/* Disabling interrupts below acts as a compiler barrier. */",
            "\tif (data_race(folio_batch_count(fbatch))) {",
            "\t\tunsigned long flags;",
            "",
            "\t\t/* No harm done if a racing interrupt already did this */",
            "\t\tlocal_lock_irqsave(&lru_rotate.lock, flags);",
            "\t\tfolio_batch_move_lru(fbatch, lru_move_tail_fn);",
            "\t\tlocal_unlock_irqrestore(&lru_rotate.lock, flags);",
            "\t}",
            "",
            "\tfbatch = &fbatches->lru_deactivate_file;",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, lru_deactivate_file_fn);",
            "",
            "\tfbatch = &fbatches->lru_deactivate;",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, lru_deactivate_fn);",
            "",
            "\tfbatch = &fbatches->lru_lazyfree;",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, lru_lazyfree_fn);",
            "",
            "\tfolio_activate_drain(cpu);",
            "}",
            "void deactivate_file_folio(struct folio *folio)",
            "{",
            "\tstruct folio_batch *fbatch;",
            "",
            "\t/* Deactivating an unevictable folio will not accelerate reclaim */",
            "\tif (folio_test_unevictable(folio))",
            "\t\treturn;",
            "",
            "\tfolio_get(folio);",
            "\tif (!folio_test_clear_lru(folio)) {",
            "\t\tfolio_put(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_deactivate_file);",
            "\tfolio_batch_add_and_move(fbatch, folio, lru_deactivate_file_fn);",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "}",
            "void folio_deactivate(struct folio *folio)",
            "{",
            "\tif (!folio_test_unevictable(folio) && (folio_test_active(folio) ||",
            "\t    lru_gen_enabled())) {",
            "\t\tstruct folio_batch *fbatch;",
            "",
            "\t\tfolio_get(folio);",
            "\t\tif (!folio_test_clear_lru(folio)) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tlocal_lock(&cpu_fbatches.lock);",
            "\t\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_deactivate);",
            "\t\tfolio_batch_add_and_move(fbatch, folio, lru_deactivate_fn);",
            "\t\tlocal_unlock(&cpu_fbatches.lock);",
            "\t}",
            "}",
            "void folio_mark_lazyfree(struct folio *folio)",
            "{",
            "\tif (folio_test_anon(folio) && folio_test_swapbacked(folio) &&",
            "\t    !folio_test_swapcache(folio) && !folio_test_unevictable(folio)) {",
            "\t\tstruct folio_batch *fbatch;",
            "",
            "\t\tfolio_get(folio);",
            "\t\tif (!folio_test_clear_lru(folio)) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tlocal_lock(&cpu_fbatches.lock);",
            "\t\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_lazyfree);",
            "\t\tfolio_batch_add_and_move(fbatch, folio, lru_lazyfree_fn);",
            "\t\tlocal_unlock(&cpu_fbatches.lock);",
            "\t}",
            "}",
            "void lru_add_drain(void)",
            "{",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tlru_add_drain_cpu(smp_processor_id());",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "\tmlock_drain_local();",
            "}",
            "static void lru_add_and_bh_lrus_drain(void)",
            "{",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tlru_add_drain_cpu(smp_processor_id());",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "\tinvalidate_bh_lrus_cpu();",
            "\tmlock_drain_local();",
            "}"
          ],
          "function_name": "lru_add_drain_cpu, deactivate_file_folio, folio_deactivate, folio_mark_lazyfree, lru_add_drain, lru_add_and_bh_lrus_drain",
          "description": "lru_add_drain_cpu 处理CPU本地LRU批次的移动，将不同类型的页面（如lru_add、deactivate、lazyfree等）通过folio_batch_move_lru分发至相应LRU队列，同时协调中断禁用和锁保护以确保原子性。",
          "similarity": 0.5146039724349976
        },
        {
          "chunk_id": 7,
          "file_path": "mm/swap.c",
          "start_line": 912,
          "end_line": 1023,
          "content": [
            "void lru_add_drain_all(void)",
            "{",
            "\t__lru_add_drain_all(false);",
            "}",
            "void lru_add_drain_all(void)",
            "{",
            "\tlru_add_drain();",
            "}",
            "void lru_cache_disable(void)",
            "{",
            "\tatomic_inc(&lru_disable_count);",
            "\t/*",
            "\t * Readers of lru_disable_count are protected by either disabling",
            "\t * preemption or rcu_read_lock:",
            "\t *",
            "\t * preempt_disable, local_irq_disable  [bh_lru_lock()]",
            "\t * rcu_read_lock\t\t       [rt_spin_lock CONFIG_PREEMPT_RT]",
            "\t * preempt_disable\t\t       [local_lock !CONFIG_PREEMPT_RT]",
            "\t *",
            "\t * Since v5.1 kernel, synchronize_rcu() is guaranteed to wait on",
            "\t * preempt_disable() regions of code. So any CPU which sees",
            "\t * lru_disable_count = 0 will have exited the critical",
            "\t * section when synchronize_rcu() returns.",
            "\t */",
            "\tsynchronize_rcu_expedited();",
            "#ifdef CONFIG_SMP",
            "\t__lru_add_drain_all(true);",
            "#else",
            "\tlru_add_and_bh_lrus_drain();",
            "#endif",
            "}",
            "void folios_put_refs(struct folio_batch *folios, unsigned int *refs)",
            "{",
            "\tint i, j;",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags = 0;",
            "",
            "\tfor (i = 0, j = 0; i < folios->nr; i++) {",
            "\t\tstruct folio *folio = folios->folios[i];",
            "\t\tunsigned int nr_refs = refs ? refs[i] : 1;",
            "",
            "\t\tif (is_huge_zero_folio(folio))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (folio_is_zone_device(folio)) {",
            "\t\t\tif (lruvec) {",
            "\t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\t\t\t\tlruvec = NULL;",
            "\t\t\t}",
            "\t\t\tif (put_devmap_managed_folio_refs(folio, nr_refs))",
            "\t\t\t\tcontinue;",
            "\t\t\tif (folio_ref_sub_and_test(folio, nr_refs))",
            "\t\t\t\tfree_zone_device_folio(folio);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tif (!folio_ref_sub_and_test(folio, nr_refs))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* hugetlb has its own memcg */",
            "\t\tif (folio_test_hugetlb(folio)) {",
            "\t\t\tif (lruvec) {",
            "\t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\t\t\t\tlruvec = NULL;",
            "\t\t\t}",
            "\t\t\tfree_huge_folio(folio);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfolio_unqueue_deferred_split(folio);",
            "\t\t__page_cache_release(folio, &lruvec, &flags);",
            "",
            "\t\tif (j != i)",
            "\t\t\tfolios->folios[j] = folio;",
            "\t\tj++;",
            "\t}",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\tif (!j) {",
            "\t\tfolio_batch_reinit(folios);",
            "\t\treturn;",
            "\t}",
            "",
            "\tfolios->nr = j;",
            "\tmem_cgroup_uncharge_folios(folios);",
            "\tfree_unref_folios(folios);",
            "}",
            "void release_pages(release_pages_arg arg, int nr)",
            "{",
            "\tstruct folio_batch fbatch;",
            "\tint refs[PAGEVEC_SIZE];",
            "\tstruct encoded_page **encoded = arg.encoded_pages;",
            "\tint i;",
            "",
            "\tfolio_batch_init(&fbatch);",
            "\tfor (i = 0; i < nr; i++) {",
            "\t\t/* Turn any of the argument types into a folio */",
            "\t\tstruct folio *folio = page_folio(encoded_page_ptr(encoded[i]));",
            "",
            "\t\t/* Is our next entry actually \"nr_pages\" -> \"nr_refs\" ? */",
            "\t\trefs[fbatch.nr] = 1;",
            "\t\tif (unlikely(encoded_page_flags(encoded[i]) &",
            "\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\trefs[fbatch.nr] = encoded_nr_pages(encoded[++i]);",
            "",
            "\t\tif (folio_batch_add(&fbatch, folio) > 0)",
            "\t\t\tcontinue;",
            "\t\tfolios_put_refs(&fbatch, refs);",
            "\t}",
            "",
            "\tif (fbatch.nr)",
            "\t\tfolios_put_refs(&fbatch, refs);",
            "}"
          ],
          "function_name": "lru_add_drain_all, lru_add_drain_all, lru_cache_disable, folios_put_refs, release_pages",
          "description": "lru_add_drain_all 和 lru_cache_disable 控制全局LRU缓存状态切换，通过synchronize_rcu_expedited强制同步后，根据配置选择全CPU或单CPU模式执行页面释放；release_pages 批量释放页面引用并处理内存组计费。",
          "similarity": 0.48942339420318604
        },
        {
          "chunk_id": 4,
          "file_path": "mm/swap.c",
          "start_line": 496,
          "end_line": 600,
          "content": [
            "void folio_add_lru(struct folio *folio)",
            "{",
            "\tstruct folio_batch *fbatch;",
            "",
            "\tVM_BUG_ON_FOLIO(folio_test_active(folio) &&",
            "\t\t\tfolio_test_unevictable(folio), folio);",
            "\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\t/* see the comment in lru_gen_add_folio() */",
            "\tif (lru_gen_enabled() && !folio_test_unevictable(folio) &&",
            "\t    lru_gen_in_fault() && !(current->flags & PF_MEMALLOC))",
            "\t\tfolio_set_active(folio);",
            "",
            "\tfolio_get(folio);",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_add);",
            "\tfolio_batch_add_and_move(fbatch, folio, lru_add_fn);",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "}",
            "void folio_add_lru_vma(struct folio *folio, struct vm_area_struct *vma)",
            "{",
            "\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\tif (unlikely((vma->vm_flags & (VM_LOCKED | VM_SPECIAL)) == VM_LOCKED))",
            "\t\tmlock_new_folio(folio);",
            "\telse",
            "\t\tfolio_add_lru(folio);",
            "}",
            "static void lru_deactivate_file_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tbool active = folio_test_active(folio);",
            "\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\tif (folio_test_unevictable(folio))",
            "\t\treturn;",
            "",
            "\t/* Some processes are using the folio */",
            "\tif (folio_mapped(folio))",
            "\t\treturn;",
            "",
            "\tlruvec_del_folio(lruvec, folio);",
            "\tfolio_clear_active(folio);",
            "\tfolio_clear_referenced(folio);",
            "",
            "\tif (folio_test_writeback(folio) || folio_test_dirty(folio)) {",
            "\t\t/*",
            "\t\t * Setting the reclaim flag could race with",
            "\t\t * folio_end_writeback() and confuse readahead.  But the",
            "\t\t * race window is _really_ small and  it's not a critical",
            "\t\t * problem.",
            "\t\t */",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "\t\tfolio_set_reclaim(folio);",
            "\t} else {",
            "\t\t/*",
            "\t\t * The folio's writeback ended while it was in the batch.",
            "\t\t * We move that folio to the tail of the inactive list.",
            "\t\t */",
            "\t\tlruvec_add_folio_tail(lruvec, folio);",
            "\t\t__count_vm_events(PGROTATED, nr_pages);",
            "\t}",
            "",
            "\tif (active) {",
            "\t\t__count_vm_events(PGDEACTIVATE, nr_pages);",
            "\t\t__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE,",
            "\t\t\t\t     nr_pages);",
            "\t}",
            "}",
            "static void lru_deactivate_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tif (!folio_test_unevictable(folio) && (folio_test_active(folio) || lru_gen_enabled())) {",
            "\t\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_clear_referenced(folio);",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "",
            "\t\t__count_vm_events(PGDEACTIVATE, nr_pages);",
            "\t\t__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE,",
            "\t\t\t\t     nr_pages);",
            "\t}",
            "}",
            "static void lru_lazyfree_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tif (folio_test_anon(folio) && folio_test_swapbacked(folio) &&",
            "\t    !folio_test_swapcache(folio) && !folio_test_unevictable(folio)) {",
            "\t\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_clear_referenced(folio);",
            "\t\t/*",
            "\t\t * Lazyfree folios are clean anonymous folios.  They have",
            "\t\t * the swapbacked flag cleared, to distinguish them from normal",
            "\t\t * anonymous folios",
            "\t\t */",
            "\t\tfolio_clear_swapbacked(folio);",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "",
            "\t\t__count_vm_events(PGLAZYFREE, nr_pages);",
            "\t\t__count_memcg_events(lruvec_memcg(lruvec), PGLAZYFREE,",
            "\t\t\t\t     nr_pages);",
            "\t}",
            "}"
          ],
          "function_name": "folio_add_lru, folio_add_lru_vma, lru_deactivate_file_fn, lru_deactivate_fn, lru_lazyfree_fn",
          "description": "包含页面LRU列表插入和状态转换逻辑，folio_add_lru将页面加入LRU列表，folio_add_lru_vma处理VMA特定的页面添加，lru_deactivate_file_fn和lru_deactivate_fn处理文件页去激活操作，lru_lazyfree_fn处理延迟释放的匿名页面，均通过统一接口修改页面状态并触发统计事件。",
          "similarity": 0.4859708547592163
        }
      ]
    },
    {
      "source_file": "mm/compaction.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:44:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `compaction.c`\n\n---\n\n# compaction.c 技术文档\n\n## 1. 文件概述\n\n`compaction.c` 是 Linux 内核内存管理子系统中实现**内存压缩（Memory Compaction）**功能的核心文件。其主要目标是**减少外部碎片（external fragmentation）**，通过迁移可移动页面，将物理内存中的空闲页聚集形成连续的大块内存区域，从而满足高阶（high-order）内存分配请求（如透明大页 THP 或 HugeTLB）。该机制高度依赖页面迁移（page migration）来完成实际的页面移动工作。\n\n## 2. 核心功能\n\n### 主要函数\n- **`PageMovable()` / `__SetPageMovable()` / `__ClearPageMovable()`**: 管理页面的可移动性标记，用于标识页面是否可通过注册的 `movable_operations` 进行迁移。\n- **`defer_compaction()` / `compaction_deferred()` / `compaction_defer_reset()`**: 实现**压缩延迟（deferred compaction）**机制，避免在压缩失败后频繁重试，提升系统性能。\n- **`compaction_restarting()`**: 判断是否因多次失败后重新启动压缩流程。\n- **`isolation_suitable()`**: 检查指定 pageblock 是否适合进行页面隔离（用于迁移）。\n- **`reset_cached_positions()`**: 重置压缩控制结构中缓存的扫描位置（迁移源和空闲目标页的起始 PFN）。\n- **`release_free_list()`**: 将临时空闲页面列表中的页面释放回伙伴系统。\n- **`skip_offline_sections()` / `skip_offline_sections_reverse()`**: 在 SPARSEMEM 内存模型下跳过离线内存段。\n\n### 关键数据结构与宏\n- **`struct compact_control`**: 压缩操作的控制上下文（定义在 `internal.h` 中），包含扫描范围、迁移/空闲页面列表等。\n- **`COMPACTION_HPAGE_ORDER`**: 用于计算节点/区域“碎片分数（fragmentation score）”的参考阶数，通常为透明大页或 HugeTLB 的阶数。\n- **`COMPACT_MAX_DEFER_SHIFT`**: 压缩延迟的最大次数（64 次）。\n- **`zone->compact_*` 字段**: 包括 `compact_considered`, `compact_defer_shift`, `compact_order_failed`, `compact_cached_*_pfn` 等，用于跟踪压缩状态和优化扫描。\n\n## 3. 关键实现\n\n### 内存压缩流程\n1. **扫描阶段**：从区域两端向中间扫描：\n   - **迁移源扫描器（migrate scanner）**：从低地址向高地址扫描，寻找可迁移的页面。\n   - **空闲目标扫描器（free scanner）**：从高地址向低地址扫描，寻找空闲页面块。\n2. **页面隔离**：将找到的可迁移页面加入迁移列表，空闲页面加入空闲列表。\n3. **页面迁移**：调用 `migrate_pages()` 将迁移列表中的页面移动到空闲列表提供的目标位置。\n4. **结果处理**：迁移成功后，原位置变为空闲，可能形成更大的连续空闲块；失败则回滚。\n\n### 压缩延迟机制（Deferred Compaction）\n- 当压缩未能成功分配指定 `order` 的页面时，调用 `defer_compaction()` 增加延迟计数器 `compact_defer_shift`。\n- 后续分配请求会通过 `compaction_deferred()` 检查是否应跳过压缩（基于 `compact_considered` 计数和 `defer_limit = 1 << compact_defer_shift`）。\n- 若分配成功或预期成功，则通过 `compaction_defer_reset()` 重置延迟状态。\n- 当延迟达到最大值（`COMPACT_MAX_DEFER_SHIFT`）且考虑次数足够多时，`compaction_restarting()` 返回 true，强制重启完整压缩流程。\n\n### 碎片分数与主动压缩\n- 通过 `COMPACTION_HPAGE_ORDER` 定义的阶数评估区域的外部碎片程度。\n- 主动压缩（proactive compaction）定期（`HPAGE_FRAG_CHECK_INTERVAL_MSEC = 500ms`）检查碎片分数，并在需要时触发后台压缩。\n- `is_via_compact_memory()` 用于识别通过 `/proc/sys/vm/compact_memory` 等接口发起的全量压缩请求（`order = -1`）。\n\n### 页面可移动性管理\n- `__SetPageMovable()` 将 `movable_operations` 指针编码到 `page->mapping` 中，并设置 `PAGE_MAPPING_MOVABLE` 标志。\n- `PageMovable()` 验证页面是否被正确标记为可移动，并确保其操作集有效。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`, `internal.h`, `mm_inline.h` 提供的伙伴系统、页面分配/释放、zone 管理等基础功能。\n- **页面迁移**：紧密集成 `<linux/migrate.h>`，压缩的核心操作由 `migrate_pages()` 完成。\n- **内存模型**：针对 `CONFIG_SPARSEMEM` 提供离线内存段跳过逻辑。\n- **大页支持**：根据 `CONFIG_TRANSPARENT_HUGEPAGE` 或 `CONFIG_HUGETLBFS` 确定 `COMPACTION_HPAGE_ORDER`。\n- **调试与追踪**：使用 `kasan.h`, `page_owner.h` 进行调试；通过 `trace/events/compaction.h` 提供 ftrace 事件。\n- **系统调用与接口**：通过 sysctl (`/proc/sys/vm/`) 和 sysfs (`/sys/devices/system/node/`) 暴露用户空间控制接口。\n- **进程与调度**：使用 `kthread.h`, `freezer.h` 管理后台压缩线程；`psi.h` 用于压力状态监控。\n\n## 5. 使用场景\n\n1. **高阶内存分配失败时**：当 `alloc_pages()` 请求高阶页面（如 order ≥ 3）失败时，内核会尝试同步内存压缩以满足请求。\n2. **透明大页（THP）分配**：THP 需要连续的 2MB 物理内存，压缩是满足此类请求的关键机制。\n3. **主动后台压缩**：通过 `vm.compaction_proactiveness` sysctl 参数配置，内核定期评估内存碎片并主动压缩，预防分配延迟。\n4. **用户空间触发**：管理员可通过写入 `/proc/sys/vm/compact_memory` 或特定 NUMA 节点的 `compact` sysfs 文件，手动触发全量内存压缩。\n5. **CMA（Contiguous Memory Allocator）**：在 CMA 区域分配大块连续内存前，使用压缩机制迁移占用页面以腾出空间（需 `CONFIG_CMA`）。",
      "similarity": 0.5563406944274902,
      "chunks": [
        {
          "chunk_id": 10,
          "file_path": "mm/compaction.c",
          "start_line": 1888,
          "end_line": 2042,
          "content": [
            "static void compaction_free(struct folio *dst, unsigned long data)",
            "{",
            "\tstruct compact_control *cc = (struct compact_control *)data;",
            "\tint order = folio_order(dst);",
            "\tstruct page *page = &dst->page;",
            "",
            "\tif (folio_put_testzero(dst)) {",
            "\t\tfree_pages_prepare(page, order);",
            "\t\tlist_add(&dst->lru, &cc->freepages[order]);",
            "\t\tcc->nr_freepages += 1 << order;",
            "\t}",
            "\tcc->nr_migratepages += 1 << order;",
            "\t/*",
            "\t * someone else has referenced the page, we cannot take it back to our",
            "\t * free list.",
            "\t */",
            "}",
            "static inline void",
            "update_fast_start_pfn(struct compact_control *cc, unsigned long pfn)",
            "{",
            "\tif (cc->fast_start_pfn == ULONG_MAX)",
            "\t\treturn;",
            "",
            "\tif (!cc->fast_start_pfn)",
            "\t\tcc->fast_start_pfn = pfn;",
            "",
            "\tcc->fast_start_pfn = min(cc->fast_start_pfn, pfn);",
            "}",
            "static inline unsigned long",
            "reinit_migrate_pfn(struct compact_control *cc)",
            "{",
            "\tif (!cc->fast_start_pfn || cc->fast_start_pfn == ULONG_MAX)",
            "\t\treturn cc->migrate_pfn;",
            "",
            "\tcc->migrate_pfn = cc->fast_start_pfn;",
            "\tcc->fast_start_pfn = ULONG_MAX;",
            "",
            "\treturn cc->migrate_pfn;",
            "}",
            "static unsigned long fast_find_migrateblock(struct compact_control *cc)",
            "{",
            "\tunsigned int limit = freelist_scan_limit(cc);",
            "\tunsigned int nr_scanned = 0;",
            "\tunsigned long distance;",
            "\tunsigned long pfn = cc->migrate_pfn;",
            "\tunsigned long high_pfn;",
            "\tint order;",
            "\tbool found_block = false;",
            "",
            "\t/* Skip hints are relied on to avoid repeats on the fast search */",
            "\tif (cc->ignore_skip_hint)",
            "\t\treturn pfn;",
            "",
            "\t/*",
            "\t * If the pageblock should be finished then do not select a different",
            "\t * pageblock.",
            "\t */",
            "\tif (cc->finish_pageblock)",
            "\t\treturn pfn;",
            "",
            "\t/*",
            "\t * If the migrate_pfn is not at the start of a zone or the start",
            "\t * of a pageblock then assume this is a continuation of a previous",
            "\t * scan restarted due to COMPACT_CLUSTER_MAX.",
            "\t */",
            "\tif (pfn != cc->zone->zone_start_pfn && pfn != pageblock_start_pfn(pfn))",
            "\t\treturn pfn;",
            "",
            "\t/*",
            "\t * For smaller orders, just linearly scan as the number of pages",
            "\t * to migrate should be relatively small and does not necessarily",
            "\t * justify freeing up a large block for a small allocation.",
            "\t */",
            "\tif (cc->order <= PAGE_ALLOC_COSTLY_ORDER)",
            "\t\treturn pfn;",
            "",
            "\t/*",
            "\t * Only allow kcompactd and direct requests for movable pages to",
            "\t * quickly clear out a MOVABLE pageblock for allocation. This",
            "\t * reduces the risk that a large movable pageblock is freed for",
            "\t * an unmovable/reclaimable small allocation.",
            "\t */",
            "\tif (cc->direct_compaction && cc->migratetype != MIGRATE_MOVABLE)",
            "\t\treturn pfn;",
            "",
            "\t/*",
            "\t * When starting the migration scanner, pick any pageblock within the",
            "\t * first half of the search space. Otherwise try and pick a pageblock",
            "\t * within the first eighth to reduce the chances that a migration",
            "\t * target later becomes a source.",
            "\t */",
            "\tdistance = (cc->free_pfn - cc->migrate_pfn) >> 1;",
            "\tif (cc->migrate_pfn != cc->zone->zone_start_pfn)",
            "\t\tdistance >>= 2;",
            "\thigh_pfn = pageblock_start_pfn(cc->migrate_pfn + distance);",
            "",
            "\tfor (order = cc->order - 1;",
            "\t     order >= PAGE_ALLOC_COSTLY_ORDER && !found_block && nr_scanned < limit;",
            "\t     order--) {",
            "\t\tstruct free_area *area = &cc->zone->free_area[order];",
            "\t\tstruct list_head *freelist;",
            "\t\tunsigned long flags;",
            "\t\tstruct page *freepage;",
            "",
            "\t\tif (!area->nr_free)",
            "\t\t\tcontinue;",
            "",
            "\t\tspin_lock_irqsave(&cc->zone->lock, flags);",
            "\t\tfreelist = &area->free_list[MIGRATE_MOVABLE];",
            "\t\tlist_for_each_entry(freepage, freelist, buddy_list) {",
            "\t\t\tunsigned long free_pfn;",
            "",
            "\t\t\tif (nr_scanned++ >= limit) {",
            "\t\t\t\tmove_freelist_tail(freelist, freepage);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "",
            "\t\t\tfree_pfn = page_to_pfn(freepage);",
            "\t\t\tif (free_pfn < high_pfn) {",
            "\t\t\t\t/*",
            "\t\t\t\t * Avoid if skipped recently. Ideally it would",
            "\t\t\t\t * move to the tail but even safe iteration of",
            "\t\t\t\t * the list assumes an entry is deleted, not",
            "\t\t\t\t * reordered.",
            "\t\t\t\t */",
            "\t\t\t\tif (get_pageblock_skip(freepage))",
            "\t\t\t\t\tcontinue;",
            "",
            "\t\t\t\t/* Reorder to so a future search skips recent pages */",
            "\t\t\t\tmove_freelist_tail(freelist, freepage);",
            "",
            "\t\t\t\tupdate_fast_start_pfn(cc, free_pfn);",
            "\t\t\t\tpfn = pageblock_start_pfn(free_pfn);",
            "\t\t\t\tif (pfn < cc->zone->zone_start_pfn)",
            "\t\t\t\t\tpfn = cc->zone->zone_start_pfn;",
            "\t\t\t\tcc->fast_search_fail = 0;",
            "\t\t\t\tfound_block = true;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);",
            "\t}",
            "",
            "\tcc->total_migrate_scanned += nr_scanned;",
            "",
            "\t/*",
            "\t * If fast scanning failed then use a cached entry for a page block",
            "\t * that had free pages as the basis for starting a linear scan.",
            "\t */",
            "\tif (!found_block) {",
            "\t\tcc->fast_search_fail++;",
            "\t\tpfn = reinit_migrate_pfn(cc);",
            "\t}",
            "\treturn pfn;",
            "}"
          ],
          "function_name": "compaction_free, update_fast_start_pfn, reinit_migrate_pfn, fast_find_migrateblock",
          "description": "实现内存碎片整理中页面释放与迁移扫描逻辑，compaction_free处理页面回收，update_fast_start_pfn维护快速扫描起点，reinit_migrate_pfn重置迁移扫描位置，fast_find_migrateblock寻找适合迁移的页块，优先考虑可移动类型且未被跳过的页块",
          "similarity": 0.6163341999053955
        },
        {
          "chunk_id": 6,
          "file_path": "mm/compaction.c",
          "start_line": 867,
          "end_line": 1342,
          "content": [
            "static int",
            "isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,",
            "\t\t\tunsigned long end_pfn, isolate_mode_t mode)",
            "{",
            "\tpg_data_t *pgdat = cc->zone->zone_pgdat;",
            "\tunsigned long nr_scanned = 0, nr_isolated = 0;",
            "\tstruct lruvec *lruvec;",
            "\tunsigned long flags = 0;",
            "\tstruct lruvec *locked = NULL;",
            "\tstruct folio *folio = NULL;",
            "\tstruct page *page = NULL, *valid_page = NULL;",
            "\tstruct address_space *mapping;",
            "\tunsigned long start_pfn = low_pfn;",
            "\tbool skip_on_failure = false;",
            "\tunsigned long next_skip_pfn = 0;",
            "\tbool skip_updated = false;",
            "\tint ret = 0;",
            "",
            "\tcc->migrate_pfn = low_pfn;",
            "",
            "\t/*",
            "\t * Ensure that there are not too many pages isolated from the LRU",
            "\t * list by either parallel reclaimers or compaction. If there are,",
            "\t * delay for some time until fewer pages are isolated",
            "\t */",
            "\twhile (unlikely(too_many_isolated(cc))) {",
            "\t\t/* stop isolation if there are still pages not migrated */",
            "\t\tif (cc->nr_migratepages)",
            "\t\t\treturn -EAGAIN;",
            "",
            "\t\t/* async migration should just abort */",
            "\t\tif (cc->mode == MIGRATE_ASYNC)",
            "\t\t\treturn -EAGAIN;",
            "",
            "\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_ISOLATED);",
            "",
            "\t\tif (fatal_signal_pending(current))",
            "\t\t\treturn -EINTR;",
            "\t}",
            "",
            "\tcond_resched();",
            "",
            "\tif (cc->direct_compaction && (cc->mode == MIGRATE_ASYNC)) {",
            "\t\tskip_on_failure = true;",
            "\t\tnext_skip_pfn = block_end_pfn(low_pfn, cc->order);",
            "\t}",
            "",
            "\t/* Time to isolate some pages for migration */",
            "\tfor (; low_pfn < end_pfn; low_pfn++) {",
            "\t\tbool is_dirty, is_unevictable;",
            "",
            "\t\tif (skip_on_failure && low_pfn >= next_skip_pfn) {",
            "\t\t\t/*",
            "\t\t\t * We have isolated all migration candidates in the",
            "\t\t\t * previous order-aligned block, and did not skip it due",
            "\t\t\t * to failure. We should migrate the pages now and",
            "\t\t\t * hopefully succeed compaction.",
            "\t\t\t */",
            "\t\t\tif (nr_isolated)",
            "\t\t\t\tbreak;",
            "",
            "\t\t\t/*",
            "\t\t\t * We failed to isolate in the previous order-aligned",
            "\t\t\t * block. Set the new boundary to the end of the",
            "\t\t\t * current block. Note we can't simply increase",
            "\t\t\t * next_skip_pfn by 1 << order, as low_pfn might have",
            "\t\t\t * been incremented by a higher number due to skipping",
            "\t\t\t * a compound or a high-order buddy page in the",
            "\t\t\t * previous loop iteration.",
            "\t\t\t */",
            "\t\t\tnext_skip_pfn = block_end_pfn(low_pfn, cc->order);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Periodically drop the lock (if held) regardless of its",
            "\t\t * contention, to give chance to IRQs. Abort completely if",
            "\t\t * a fatal signal is pending.",
            "\t\t */",
            "\t\tif (!(low_pfn % COMPACT_CLUSTER_MAX)) {",
            "\t\t\tif (locked) {",
            "\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);",
            "\t\t\t\tlocked = NULL;",
            "\t\t\t}",
            "",
            "\t\t\tif (fatal_signal_pending(current)) {",
            "\t\t\t\tcc->contended = true;",
            "\t\t\t\tret = -EINTR;",
            "",
            "\t\t\t\tgoto fatal_pending;",
            "\t\t\t}",
            "",
            "\t\t\tcond_resched();",
            "\t\t}",
            "",
            "\t\tnr_scanned++;",
            "",
            "\t\tpage = pfn_to_page(low_pfn);",
            "",
            "\t\t/*",
            "\t\t * Check if the pageblock has already been marked skipped.",
            "\t\t * Only the first PFN is checked as the caller isolates",
            "\t\t * COMPACT_CLUSTER_MAX at a time so the second call must",
            "\t\t * not falsely conclude that the block should be skipped.",
            "\t\t */",
            "\t\tif (!valid_page && (pageblock_aligned(low_pfn) ||",
            "\t\t\t\t    low_pfn == cc->zone->zone_start_pfn)) {",
            "\t\t\tif (!isolation_suitable(cc, page)) {",
            "\t\t\t\tlow_pfn = end_pfn;",
            "\t\t\t\tfolio = NULL;",
            "\t\t\t\tgoto isolate_abort;",
            "\t\t\t}",
            "\t\t\tvalid_page = page;",
            "\t\t}",
            "",
            "\t\tif (PageHuge(page)) {",
            "\t\t\t/*",
            "\t\t\t * skip hugetlbfs if we are not compacting for pages",
            "\t\t\t * bigger than its order. THPs and other compound pages",
            "\t\t\t * are handled below.",
            "\t\t\t */",
            "\t\t\tif (!cc->alloc_contig) {",
            "\t\t\t\tconst unsigned int order = compound_order(page);",
            "",
            "\t\t\t\tif (order <= MAX_PAGE_ORDER) {",
            "\t\t\t\t\tlow_pfn += (1UL << order) - 1;",
            "\t\t\t\t\tnr_scanned += (1UL << order) - 1;",
            "\t\t\t\t}",
            "\t\t\t\tgoto isolate_fail;",
            "\t\t\t}",
            "\t\t\t/* for alloc_contig case */",
            "\t\t\tif (locked) {",
            "\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);",
            "\t\t\t\tlocked = NULL;",
            "\t\t\t}",
            "",
            "\t\t\tret = isolate_or_dissolve_huge_page(page, &cc->migratepages);",
            "",
            "\t\t\t/*",
            "\t\t\t * Fail isolation in case isolate_or_dissolve_huge_page()",
            "\t\t\t * reports an error. In case of -ENOMEM, abort right away.",
            "\t\t\t */",
            "\t\t\tif (ret < 0) {",
            "\t\t\t\t /* Do not report -EBUSY down the chain */",
            "\t\t\t\tif (ret == -EBUSY)",
            "\t\t\t\t\tret = 0;",
            "\t\t\t\tlow_pfn += compound_nr(page) - 1;",
            "\t\t\t\tnr_scanned += compound_nr(page) - 1;",
            "\t\t\t\tgoto isolate_fail;",
            "\t\t\t}",
            "",
            "\t\t\tif (PageHuge(page)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * Hugepage was successfully isolated and placed",
            "\t\t\t\t * on the cc->migratepages list.",
            "\t\t\t\t */",
            "\t\t\t\tfolio = page_folio(page);",
            "\t\t\t\tlow_pfn += folio_nr_pages(folio) - 1;",
            "\t\t\t\tgoto isolate_success_no_list;",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * Ok, the hugepage was dissolved. Now these pages are",
            "\t\t\t * Buddy and cannot be re-allocated because they are",
            "\t\t\t * isolated. Fall-through as the check below handles",
            "\t\t\t * Buddy pages.",
            "\t\t\t */",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Skip if free. We read page order here without zone lock",
            "\t\t * which is generally unsafe, but the race window is small and",
            "\t\t * the worst thing that can happen is that we skip some",
            "\t\t * potential isolation targets.",
            "\t\t */",
            "\t\tif (PageBuddy(page)) {",
            "\t\t\tunsigned long freepage_order = buddy_order_unsafe(page);",
            "",
            "\t\t\t/*",
            "\t\t\t * Without lock, we cannot be sure that what we got is",
            "\t\t\t * a valid page order. Consider only values in the",
            "\t\t\t * valid order range to prevent low_pfn overflow.",
            "\t\t\t */",
            "\t\t\tif (freepage_order > 0 && freepage_order <= MAX_PAGE_ORDER) {",
            "\t\t\t\tlow_pfn += (1UL << freepage_order) - 1;",
            "\t\t\t\tnr_scanned += (1UL << freepage_order) - 1;",
            "\t\t\t}",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Regardless of being on LRU, compound pages such as THP",
            "\t\t * (hugetlbfs is handled above) are not to be compacted unless",
            "\t\t * we are attempting an allocation larger than the compound",
            "\t\t * page size. We can potentially save a lot of iterations if we",
            "\t\t * skip them at once. The check is racy, but we can consider",
            "\t\t * only valid values and the only danger is skipping too much.",
            "\t\t */",
            "\t\tif (PageCompound(page) && !cc->alloc_contig) {",
            "\t\t\tconst unsigned int order = compound_order(page);",
            "",
            "\t\t\t/* Skip based on page order and compaction target order. */",
            "\t\t\tif (skip_isolation_on_order(order, cc->order)) {",
            "\t\t\t\tif (order <= MAX_PAGE_ORDER) {",
            "\t\t\t\t\tlow_pfn += (1UL << order) - 1;",
            "\t\t\t\t\tnr_scanned += (1UL << order) - 1;",
            "\t\t\t\t}",
            "\t\t\t\tgoto isolate_fail;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Check may be lockless but that's ok as we recheck later.",
            "\t\t * It's possible to migrate LRU and non-lru movable pages.",
            "\t\t * Skip any other type of page",
            "\t\t */",
            "\t\tif (!PageLRU(page)) {",
            "\t\t\t/*",
            "\t\t\t * __PageMovable can return false positive so we need",
            "\t\t\t * to verify it under page_lock.",
            "\t\t\t */",
            "\t\t\tif (unlikely(__PageMovable(page)) &&",
            "\t\t\t\t\t!PageIsolated(page)) {",
            "\t\t\t\tif (locked) {",
            "\t\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);",
            "\t\t\t\t\tlocked = NULL;",
            "\t\t\t\t}",
            "",
            "\t\t\t\tif (isolate_movable_page(page, mode)) {",
            "\t\t\t\t\tfolio = page_folio(page);",
            "\t\t\t\t\tgoto isolate_success;",
            "\t\t\t\t}",
            "\t\t\t}",
            "",
            "\t\t\tgoto isolate_fail;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Be careful not to clear PageLRU until after we're",
            "\t\t * sure the page is not being freed elsewhere -- the",
            "\t\t * page release code relies on it.",
            "\t\t */",
            "\t\tfolio = folio_get_nontail_page(page);",
            "\t\tif (unlikely(!folio))",
            "\t\t\tgoto isolate_fail;",
            "",
            "\t\t/*",
            "\t\t * Migration will fail if an anonymous page is pinned in memory,",
            "\t\t * so avoid taking lru_lock and isolating it unnecessarily in an",
            "\t\t * admittedly racy check.",
            "\t\t */",
            "\t\tmapping = folio_mapping(folio);",
            "\t\tif (!mapping && (folio_ref_count(folio) - 1) > folio_mapcount(folio))",
            "\t\t\tgoto isolate_fail_put;",
            "",
            "\t\t/*",
            "\t\t * Only allow to migrate anonymous pages in GFP_NOFS context",
            "\t\t * because those do not depend on fs locks.",
            "\t\t */",
            "\t\tif (!(cc->gfp_mask & __GFP_FS) && mapping)",
            "\t\t\tgoto isolate_fail_put;",
            "",
            "\t\t/* Only take pages on LRU: a check now makes later tests safe */",
            "\t\tif (!folio_test_lru(folio))",
            "\t\t\tgoto isolate_fail_put;",
            "",
            "\t\tis_unevictable = folio_test_unevictable(folio);",
            "",
            "\t\t/* Compaction might skip unevictable pages but CMA takes them */",
            "\t\tif (!(mode & ISOLATE_UNEVICTABLE) && is_unevictable)",
            "\t\t\tgoto isolate_fail_put;",
            "",
            "\t\t/*",
            "\t\t * To minimise LRU disruption, the caller can indicate with",
            "\t\t * ISOLATE_ASYNC_MIGRATE that it only wants to isolate pages",
            "\t\t * it will be able to migrate without blocking - clean pages",
            "\t\t * for the most part.  PageWriteback would require blocking.",
            "\t\t */",
            "\t\tif ((mode & ISOLATE_ASYNC_MIGRATE) && folio_test_writeback(folio))",
            "\t\t\tgoto isolate_fail_put;",
            "",
            "\t\tis_dirty = folio_test_dirty(folio);",
            "",
            "\t\tif (((mode & ISOLATE_ASYNC_MIGRATE) && is_dirty) ||",
            "\t\t    (mapping && is_unevictable)) {",
            "\t\t\tbool migrate_dirty = true;",
            "\t\t\tbool is_inaccessible;",
            "",
            "\t\t\t/*",
            "\t\t\t * Only folios without mappings or that have",
            "\t\t\t * a ->migrate_folio callback are possible to migrate",
            "\t\t\t * without blocking.",
            "\t\t\t *",
            "\t\t\t * Folios from inaccessible mappings are not migratable.",
            "\t\t\t *",
            "\t\t\t * However, we can be racing with truncation, which can",
            "\t\t\t * free the mapping that we need to check. Truncation",
            "\t\t\t * holds the folio lock until after the folio is removed",
            "\t\t\t * from the page so holding it ourselves is sufficient.",
            "\t\t\t *",
            "\t\t\t * To avoid locking the folio just to check inaccessible,",
            "\t\t\t * assume every inaccessible folio is also unevictable,",
            "\t\t\t * which is a cheaper test.  If our assumption goes",
            "\t\t\t * wrong, it's not a correctness bug, just potentially",
            "\t\t\t * wasted cycles.",
            "\t\t\t */",
            "\t\t\tif (!folio_trylock(folio))",
            "\t\t\t\tgoto isolate_fail_put;",
            "",
            "\t\t\tmapping = folio_mapping(folio);",
            "\t\t\tif ((mode & ISOLATE_ASYNC_MIGRATE) && is_dirty) {",
            "\t\t\t\tmigrate_dirty = !mapping ||",
            "\t\t\t\t\t\tmapping->a_ops->migrate_folio;",
            "\t\t\t}",
            "\t\t\tis_inaccessible = mapping && mapping_inaccessible(mapping);",
            "\t\t\tfolio_unlock(folio);",
            "\t\t\tif (!migrate_dirty || is_inaccessible)",
            "\t\t\t\tgoto isolate_fail_put;",
            "\t\t}",
            "",
            "\t\t/* Try isolate the folio */",
            "\t\tif (!folio_test_clear_lru(folio))",
            "\t\t\tgoto isolate_fail_put;",
            "",
            "\t\tlruvec = folio_lruvec(folio);",
            "",
            "\t\t/* If we already hold the lock, we can skip some rechecking */",
            "\t\tif (lruvec != locked) {",
            "\t\t\tif (locked)",
            "\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);",
            "",
            "\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);",
            "\t\t\tlocked = lruvec;",
            "",
            "\t\t\tlruvec_memcg_debug(lruvec, folio);",
            "",
            "\t\t\t/*",
            "\t\t\t * Try get exclusive access under lock. If marked for",
            "\t\t\t * skip, the scan is aborted unless the current context",
            "\t\t\t * is a rescan to reach the end of the pageblock.",
            "\t\t\t */",
            "\t\t\tif (!skip_updated && valid_page) {",
            "\t\t\t\tskip_updated = true;",
            "\t\t\t\tif (test_and_set_skip(cc, valid_page) &&",
            "\t\t\t\t    !cc->finish_pageblock) {",
            "\t\t\t\t\tlow_pfn = end_pfn;",
            "\t\t\t\t\tgoto isolate_abort;",
            "\t\t\t\t}",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * Check LRU folio order under the lock",
            "\t\t\t */",
            "\t\t\tif (unlikely(skip_isolation_on_order(folio_order(folio),",
            "\t\t\t\t\t\t\t     cc->order) &&",
            "\t\t\t\t     !cc->alloc_contig)) {",
            "\t\t\t\tlow_pfn += folio_nr_pages(folio) - 1;",
            "\t\t\t\tnr_scanned += folio_nr_pages(folio) - 1;",
            "\t\t\t\tfolio_set_lru(folio);",
            "\t\t\t\tgoto isolate_fail_put;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* The folio is taken off the LRU */",
            "\t\tif (folio_test_large(folio))",
            "\t\t\tlow_pfn += folio_nr_pages(folio) - 1;",
            "",
            "\t\t/* Successfully isolated */",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tnode_stat_mod_folio(folio,",
            "\t\t\t\tNR_ISOLATED_ANON + folio_is_file_lru(folio),",
            "\t\t\t\tfolio_nr_pages(folio));",
            "",
            "isolate_success:",
            "\t\tlist_add(&folio->lru, &cc->migratepages);",
            "isolate_success_no_list:",
            "\t\tcc->nr_migratepages += folio_nr_pages(folio);",
            "\t\tnr_isolated += folio_nr_pages(folio);",
            "\t\tnr_scanned += folio_nr_pages(folio) - 1;",
            "",
            "\t\t/*",
            "\t\t * Avoid isolating too much unless this block is being",
            "\t\t * fully scanned (e.g. dirty/writeback pages, parallel allocation)",
            "\t\t * or a lock is contended. For contention, isolate quickly to",
            "\t\t * potentially remove one source of contention.",
            "\t\t */",
            "\t\tif (cc->nr_migratepages >= COMPACT_CLUSTER_MAX &&",
            "\t\t    !cc->finish_pageblock && !cc->contended) {",
            "\t\t\t++low_pfn;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tcontinue;",
            "",
            "isolate_fail_put:",
            "\t\t/* Avoid potential deadlock in freeing page under lru_lock */",
            "\t\tif (locked) {",
            "\t\t\tunlock_page_lruvec_irqrestore(locked, flags);",
            "\t\t\tlocked = NULL;",
            "\t\t}",
            "\t\tfolio_put(folio);",
            "",
            "isolate_fail:",
            "\t\tif (!skip_on_failure && ret != -ENOMEM)",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * We have isolated some pages, but then failed. Release them",
            "\t\t * instead of migrating, as we cannot form the cc->order buddy",
            "\t\t * page anyway.",
            "\t\t */",
            "\t\tif (nr_isolated) {",
            "\t\t\tif (locked) {",
            "\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);",
            "\t\t\t\tlocked = NULL;",
            "\t\t\t}",
            "\t\t\tputback_movable_pages(&cc->migratepages);",
            "\t\t\tcc->nr_migratepages = 0;",
            "\t\t\tnr_isolated = 0;",
            "\t\t}",
            "",
            "\t\tif (low_pfn < next_skip_pfn) {",
            "\t\t\tlow_pfn = next_skip_pfn - 1;",
            "\t\t\t/*",
            "\t\t\t * The check near the loop beginning would have updated",
            "\t\t\t * next_skip_pfn too, but this is a bit simpler.",
            "\t\t\t */",
            "\t\t\tnext_skip_pfn += 1UL << cc->order;",
            "\t\t}",
            "",
            "\t\tif (ret == -ENOMEM)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\t/*",
            "\t * The PageBuddy() check could have potentially brought us outside",
            "\t * the range to be scanned.",
            "\t */",
            "\tif (unlikely(low_pfn > end_pfn))",
            "\t\tlow_pfn = end_pfn;",
            "",
            "\tfolio = NULL;",
            "",
            "isolate_abort:",
            "\tif (locked)",
            "\t\tunlock_page_lruvec_irqrestore(locked, flags);",
            "\tif (folio) {",
            "\t\tfolio_set_lru(folio);",
            "\t\tfolio_put(folio);",
            "\t}",
            "",
            "\t/*",
            "\t * Update the cached scanner pfn once the pageblock has been scanned.",
            "\t * Pages will either be migrated in which case there is no point",
            "\t * scanning in the near future or migration failed in which case the",
            "\t * failure reason may persist. The block is marked for skipping if",
            "\t * there were no pages isolated in the block or if the block is",
            "\t * rescanned twice in a row.",
            "\t */",
            "\tif (low_pfn == end_pfn && (!nr_isolated || cc->finish_pageblock)) {",
            "\t\tif (!cc->no_set_skip_hint && valid_page && !skip_updated)",
            "\t\t\tset_pageblock_skip(valid_page);",
            "\t\tupdate_cached_migrate(cc, low_pfn);",
            "\t}",
            "",
            "\ttrace_mm_compaction_isolate_migratepages(start_pfn, low_pfn,",
            "\t\t\t\t\t\tnr_scanned, nr_isolated);",
            "",
            "fatal_pending:",
            "\tcc->total_migrate_scanned += nr_scanned;",
            "\tif (nr_isolated)",
            "\t\tcount_compact_events(COMPACTISOLATED, nr_isolated);",
            "",
            "\tcc->migrate_pfn = low_pfn;",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "isolate_migratepages_block",
          "description": "isolate_migratepages_block隔离可迁移页面，处理大页、脏页、不可移动页等特殊情况，支持异步迁移模式并处理迁移失败后的回滚",
          "similarity": 0.5663940906524658
        },
        {
          "chunk_id": 1,
          "file_path": "mm/compaction.c",
          "start_line": 34,
          "end_line": 141,
          "content": [
            "static inline void count_compact_event(enum vm_event_item item)",
            "{",
            "\tcount_vm_event(item);",
            "}",
            "static inline void count_compact_events(enum vm_event_item item, long delta)",
            "{",
            "\tcount_vm_events(item, delta);",
            "}",
            "static inline bool is_via_compact_memory(int order)",
            "{",
            "\treturn order == -1;",
            "}",
            "static inline bool is_via_compact_memory(int order) { return false; }",
            "static unsigned long release_free_list(struct list_head *freepages)",
            "{",
            "\tint order;",
            "\tunsigned long high_pfn = 0;",
            "",
            "\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {",
            "\t\tstruct page *page, *next;",
            "",
            "\t\tlist_for_each_entry_safe(page, next, &freepages[order], lru) {",
            "\t\t\tunsigned long pfn = page_to_pfn(page);",
            "",
            "\t\t\tlist_del(&page->lru);",
            "\t\t\t/*",
            "\t\t\t * Convert free pages into post allocation pages, so",
            "\t\t\t * that we can free them via __free_page.",
            "\t\t\t */",
            "\t\t\tmark_allocated(page, order, __GFP_MOVABLE);",
            "\t\t\t__free_pages(page, order);",
            "\t\t\tif (pfn > high_pfn)",
            "\t\t\t\thigh_pfn = pfn;",
            "\t\t}",
            "\t}",
            "\treturn high_pfn;",
            "}",
            "bool PageMovable(struct page *page)",
            "{",
            "\tconst struct movable_operations *mops;",
            "",
            "\tVM_BUG_ON_PAGE(!PageLocked(page), page);",
            "\tif (!__PageMovable(page))",
            "\t\treturn false;",
            "",
            "\tmops = page_movable_ops(page);",
            "\tif (mops)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "void __SetPageMovable(struct page *page, const struct movable_operations *mops)",
            "{",
            "\tVM_BUG_ON_PAGE(!PageLocked(page), page);",
            "\tVM_BUG_ON_PAGE((unsigned long)mops & PAGE_MAPPING_MOVABLE, page);",
            "\tpage->mapping = (void *)((unsigned long)mops | PAGE_MAPPING_MOVABLE);",
            "}",
            "void __ClearPageMovable(struct page *page)",
            "{",
            "\tVM_BUG_ON_PAGE(!PageMovable(page), page);",
            "\t/*",
            "\t * This page still has the type of a movable page, but it's",
            "\t * actually not movable any more.",
            "\t */",
            "\tpage->mapping = (void *)PAGE_MAPPING_MOVABLE;",
            "}",
            "static void defer_compaction(struct zone *zone, int order)",
            "{",
            "\tzone->compact_considered = 0;",
            "\tzone->compact_defer_shift++;",
            "",
            "\tif (order < zone->compact_order_failed)",
            "\t\tzone->compact_order_failed = order;",
            "",
            "\tif (zone->compact_defer_shift > COMPACT_MAX_DEFER_SHIFT)",
            "\t\tzone->compact_defer_shift = COMPACT_MAX_DEFER_SHIFT;",
            "",
            "\ttrace_mm_compaction_defer_compaction(zone, order);",
            "}",
            "static bool compaction_deferred(struct zone *zone, int order)",
            "{",
            "\tunsigned long defer_limit = 1UL << zone->compact_defer_shift;",
            "",
            "\tif (order < zone->compact_order_failed)",
            "\t\treturn false;",
            "",
            "\t/* Avoid possible overflow */",
            "\tif (++zone->compact_considered >= defer_limit) {",
            "\t\tzone->compact_considered = defer_limit;",
            "\t\treturn false;",
            "\t}",
            "",
            "\ttrace_mm_compaction_deferred(zone, order);",
            "",
            "\treturn true;",
            "}",
            "void compaction_defer_reset(struct zone *zone, int order,",
            "\t\tbool alloc_success)",
            "{",
            "\tif (alloc_success) {",
            "\t\tzone->compact_considered = 0;",
            "\t\tzone->compact_defer_shift = 0;",
            "\t}",
            "\tif (order >= zone->compact_order_failed)",
            "\t\tzone->compact_order_failed = order + 1;",
            "",
            "\ttrace_mm_compaction_defer_reset(zone, order);",
            "}"
          ],
          "function_name": "count_compact_event, count_compact_events, is_via_compact_memory, is_via_compact_memory, release_free_list, PageMovable, __SetPageMovable, __ClearPageMovable, defer_compaction, compaction_deferred, compaction_defer_reset",
          "description": "提供内存压缩事件计数、页面可移动性检测、空闲列表释放、延迟压缩逻辑及页面块隔离辅助函数，包含重复定义可能导致冲突",
          "similarity": 0.5553358793258667
        },
        {
          "chunk_id": 12,
          "file_path": "mm/compaction.c",
          "start_line": 2200,
          "end_line": 2344,
          "content": [
            "static unsigned int fragmentation_score_zone(struct zone *zone)",
            "{",
            "\treturn extfrag_for_order(zone, COMPACTION_HPAGE_ORDER);",
            "}",
            "static unsigned int fragmentation_score_zone_weighted(struct zone *zone)",
            "{",
            "\tunsigned long score;",
            "",
            "\tscore = zone->present_pages * fragmentation_score_zone(zone);",
            "\treturn div64_ul(score, zone->zone_pgdat->node_present_pages + 1);",
            "}",
            "static unsigned int fragmentation_score_node(pg_data_t *pgdat)",
            "{",
            "\tunsigned int score = 0;",
            "\tint zoneid;",
            "",
            "\tfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {",
            "\t\tstruct zone *zone;",
            "",
            "\t\tzone = &pgdat->node_zones[zoneid];",
            "\t\tif (!populated_zone(zone))",
            "\t\t\tcontinue;",
            "\t\tscore += fragmentation_score_zone_weighted(zone);",
            "\t}",
            "",
            "\treturn score;",
            "}",
            "static unsigned int fragmentation_score_wmark(bool low)",
            "{",
            "\tunsigned int wmark_low;",
            "",
            "\t/*",
            "\t * Cap the low watermark to avoid excessive compaction",
            "\t * activity in case a user sets the proactiveness tunable",
            "\t * close to 100 (maximum).",
            "\t */",
            "\twmark_low = max(100U - sysctl_compaction_proactiveness, 5U);",
            "\treturn low ? wmark_low : min(wmark_low + 10, 100U);",
            "}",
            "static bool should_proactive_compact_node(pg_data_t *pgdat)",
            "{",
            "\tint wmark_high;",
            "",
            "\tif (!sysctl_compaction_proactiveness || kswapd_is_running(pgdat))",
            "\t\treturn false;",
            "",
            "\twmark_high = fragmentation_score_wmark(false);",
            "\treturn fragmentation_score_node(pgdat) > wmark_high;",
            "}",
            "static enum compact_result __compact_finished(struct compact_control *cc)",
            "{",
            "\tunsigned int order;",
            "\tconst int migratetype = cc->migratetype;",
            "\tint ret;",
            "",
            "\t/* Compaction run completes if the migrate and free scanner meet */",
            "\tif (compact_scanners_met(cc)) {",
            "\t\t/* Let the next compaction start anew. */",
            "\t\treset_cached_positions(cc->zone);",
            "",
            "\t\t/*",
            "\t\t * Mark that the PG_migrate_skip information should be cleared",
            "\t\t * by kswapd when it goes to sleep. kcompactd does not set the",
            "\t\t * flag itself as the decision to be clear should be directly",
            "\t\t * based on an allocation request.",
            "\t\t */",
            "\t\tif (cc->direct_compaction)",
            "\t\t\tcc->zone->compact_blockskip_flush = true;",
            "",
            "\t\tif (cc->whole_zone)",
            "\t\t\treturn COMPACT_COMPLETE;",
            "\t\telse",
            "\t\t\treturn COMPACT_PARTIAL_SKIPPED;",
            "\t}",
            "",
            "\tif (cc->proactive_compaction) {",
            "\t\tint score, wmark_low;",
            "\t\tpg_data_t *pgdat;",
            "",
            "\t\tpgdat = cc->zone->zone_pgdat;",
            "\t\tif (kswapd_is_running(pgdat))",
            "\t\t\treturn COMPACT_PARTIAL_SKIPPED;",
            "",
            "\t\tscore = fragmentation_score_zone(cc->zone);",
            "\t\twmark_low = fragmentation_score_wmark(true);",
            "",
            "\t\tif (score > wmark_low)",
            "\t\t\tret = COMPACT_CONTINUE;",
            "\t\telse",
            "\t\t\tret = COMPACT_SUCCESS;",
            "",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tif (is_via_compact_memory(cc->order))",
            "\t\treturn COMPACT_CONTINUE;",
            "",
            "\t/*",
            "\t * Always finish scanning a pageblock to reduce the possibility of",
            "\t * fallbacks in the future. This is particularly important when",
            "\t * migration source is unmovable/reclaimable but it's not worth",
            "\t * special casing.",
            "\t */",
            "\tif (!pageblock_aligned(cc->migrate_pfn))",
            "\t\treturn COMPACT_CONTINUE;",
            "",
            "\t/* Direct compactor: Is a suitable page free? */",
            "\tret = COMPACT_NO_SUITABLE_PAGE;",
            "\tfor (order = cc->order; order < NR_PAGE_ORDERS; order++) {",
            "\t\tstruct free_area *area = &cc->zone->free_area[order];",
            "\t\tbool can_steal;",
            "",
            "\t\t/* Job done if page is free of the right migratetype */",
            "\t\tif (!free_area_empty(area, migratetype))",
            "\t\t\treturn COMPACT_SUCCESS;",
            "",
            "#ifdef CONFIG_CMA",
            "\t\t/* MIGRATE_MOVABLE can fallback on MIGRATE_CMA */",
            "\t\tif (migratetype == MIGRATE_MOVABLE &&",
            "\t\t\t!free_area_empty(area, MIGRATE_CMA))",
            "\t\t\treturn COMPACT_SUCCESS;",
            "#endif",
            "\t\t/*",
            "\t\t * Job done if allocation would steal freepages from",
            "\t\t * other migratetype buddy lists.",
            "\t\t */",
            "\t\tif (find_suitable_fallback(area, order, migratetype,",
            "\t\t\t\t\t\ttrue, &can_steal) != -1)",
            "\t\t\t/*",
            "\t\t\t * Movable pages are OK in any pageblock. If we are",
            "\t\t\t * stealing for a non-movable allocation, make sure",
            "\t\t\t * we finish compacting the current pageblock first",
            "\t\t\t * (which is assured by the above migrate_pfn align",
            "\t\t\t * check) so it is as free as possible and we won't",
            "\t\t\t * have to steal another one soon.",
            "\t\t\t */",
            "\t\t\treturn COMPACT_SUCCESS;",
            "\t}",
            "",
            "out:",
            "\tif (cc->contended || fatal_signal_pending(current))",
            "\t\tret = COMPACT_CONTENDED;",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "fragmentation_score_zone, fragmentation_score_zone_weighted, fragmentation_score_node, fragmentation_score_wmark, should_proactive_compact_node, __compact_finished",
          "description": "fragmentation_score_zone计算区碎片评分，fragmentation_score_zone_weighted加权计算节点总分，fragmentation_score_wmark设置低水位阈值，should_proactive_compact_node根据评分判断是否需要主动压缩",
          "similarity": 0.5528404712677002
        },
        {
          "chunk_id": 5,
          "file_path": "mm/compaction.c",
          "start_line": 725,
          "end_line": 837,
          "content": [
            "unsigned long",
            "isolate_freepages_range(struct compact_control *cc,",
            "\t\t\tunsigned long start_pfn, unsigned long end_pfn)",
            "{",
            "\tunsigned long isolated, pfn, block_start_pfn, block_end_pfn;",
            "\tint order;",
            "",
            "\tfor (order = 0; order < NR_PAGE_ORDERS; order++)",
            "\t\tINIT_LIST_HEAD(&cc->freepages[order]);",
            "",
            "\tpfn = start_pfn;",
            "\tblock_start_pfn = pageblock_start_pfn(pfn);",
            "\tif (block_start_pfn < cc->zone->zone_start_pfn)",
            "\t\tblock_start_pfn = cc->zone->zone_start_pfn;",
            "\tblock_end_pfn = pageblock_end_pfn(pfn);",
            "",
            "\tfor (; pfn < end_pfn; pfn += isolated,",
            "\t\t\t\tblock_start_pfn = block_end_pfn,",
            "\t\t\t\tblock_end_pfn += pageblock_nr_pages) {",
            "\t\t/* Protect pfn from changing by isolate_freepages_block */",
            "\t\tunsigned long isolate_start_pfn = pfn;",
            "",
            "\t\t/*",
            "\t\t * pfn could pass the block_end_pfn if isolated freepage",
            "\t\t * is more than pageblock order. In this case, we adjust",
            "\t\t * scanning range to right one.",
            "\t\t */",
            "\t\tif (pfn >= block_end_pfn) {",
            "\t\t\tblock_start_pfn = pageblock_start_pfn(pfn);",
            "\t\t\tblock_end_pfn = pageblock_end_pfn(pfn);",
            "\t\t}",
            "",
            "\t\tblock_end_pfn = min(block_end_pfn, end_pfn);",
            "",
            "\t\tif (!pageblock_pfn_to_page(block_start_pfn,",
            "\t\t\t\t\tblock_end_pfn, cc->zone))",
            "\t\t\tbreak;",
            "",
            "\t\tisolated = isolate_freepages_block(cc, &isolate_start_pfn,",
            "\t\t\t\t\tblock_end_pfn, cc->freepages, 0, true);",
            "",
            "\t\t/*",
            "\t\t * In strict mode, isolate_freepages_block() returns 0 if",
            "\t\t * there are any holes in the block (ie. invalid PFNs or",
            "\t\t * non-free pages).",
            "\t\t */",
            "\t\tif (!isolated)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * If we managed to isolate pages, it is always (1 << n) *",
            "\t\t * pageblock_nr_pages for some non-negative n.  (Max order",
            "\t\t * page may span two pageblocks).",
            "\t\t */",
            "\t}",
            "",
            "\tif (pfn < end_pfn) {",
            "\t\t/* Loop terminated early, cleanup. */",
            "\t\trelease_free_list(cc->freepages);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/* We don't use freelists for anything. */",
            "\treturn pfn;",
            "}",
            "static bool too_many_isolated(struct compact_control *cc)",
            "{",
            "\tpg_data_t *pgdat = cc->zone->zone_pgdat;",
            "\tbool too_many;",
            "",
            "\tunsigned long active, inactive, isolated;",
            "",
            "\tinactive = node_page_state(pgdat, NR_INACTIVE_FILE) +",
            "\t\t\tnode_page_state(pgdat, NR_INACTIVE_ANON);",
            "\tactive = node_page_state(pgdat, NR_ACTIVE_FILE) +",
            "\t\t\tnode_page_state(pgdat, NR_ACTIVE_ANON);",
            "\tisolated = node_page_state(pgdat, NR_ISOLATED_FILE) +",
            "\t\t\tnode_page_state(pgdat, NR_ISOLATED_ANON);",
            "",
            "\t/*",
            "\t * Allow GFP_NOFS to isolate past the limit set for regular",
            "\t * compaction runs. This prevents an ABBA deadlock when other",
            "\t * compactors have already isolated to the limit, but are",
            "\t * blocked on filesystem locks held by the GFP_NOFS thread.",
            "\t */",
            "\tif (cc->gfp_mask & __GFP_FS) {",
            "\t\tinactive >>= 3;",
            "\t\tactive >>= 3;",
            "\t}",
            "",
            "\ttoo_many = isolated > (inactive + active) / 2;",
            "\tif (!too_many)",
            "\t\twake_throttle_isolated(pgdat);",
            "",
            "\treturn too_many;",
            "}",
            "static bool skip_isolation_on_order(int order, int target_order)",
            "{",
            "\t/*",
            "\t * Unless we are performing global compaction (i.e.,",
            "\t * is_via_compact_memory), skip any folios that are larger than the",
            "\t * target order: we wouldn't be here if we'd have a free folio with",
            "\t * the desired target_order, so migrating this folio would likely fail",
            "\t * later.",
            "\t */",
            "\tif (!is_via_compact_memory(target_order) && order >= target_order)",
            "\t\treturn true;",
            "\t/*",
            "\t * We limit memory compaction to pageblocks and won't try",
            "\t * creating free blocks of memory that are larger than that.",
            "\t */",
            "\treturn order >= pageblock_order;",
            "}"
          ],
          "function_name": "isolate_freepages_range, too_many_isolated, skip_isolation_on_order",
          "description": "isolate_freepages_range扫描指定PFN范围内的页面块，隔离空闲页至freepages数组；too_many_isolated检测当前系统是否已隔离过多页面以避免死锁；skip_isolation_on_order判断是否跳过大于目标顺序的页面",
          "similarity": 0.5464745163917542
        }
      ]
    },
    {
      "source_file": "mm/khugepaged.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:26:37\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `khugepaged.c`\n\n---\n\n# khugepaged.c 技术文档\n\n## 1. 文件概述\n\n`khugepaged.c` 是 Linux 内核中透明大页（Transparent Huge Page, THP）子系统的核心组件之一，负责在后台异步地将符合条件的小页（4KB）合并为大页（通常为 2MB 的 PMD 级别大页）。该文件实现了名为 `khugepaged` 的内核线程及其相关扫描、合并逻辑，旨在提升内存访问性能并减少 TLB 压力。通过周期性扫描进程地址空间，识别可合并区域，并尝试分配和填充大页，从而优化系统整体内存效率。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`enum scan_result`**  \n  定义了页面扫描过程中可能返回的各种结果状态码，用于控制合并流程的决策（如失败原因、成功条件等）。\n\n- **`struct collapse_control`**  \n  控制页面折叠（collapse）过程的上下文信息，包括是否由 `khugepaged` 发起、各 NUMA 节点的负载统计及分配回退掩码。\n\n- **`struct khugepaged_mm_slot`**  \n  表示正在被 `khugepaged` 扫描的每个 `mm_struct`（进程地址空间）的元数据槽位，继承自通用 `mm_slot` 结构。\n\n- **`struct khugepaged_scan`**  \n  全局扫描游标，记录当前扫描的 `mm` 列表头、当前 `mm_slot` 及下一次扫描的虚拟地址。\n\n### 全局变量\n\n- `khugepaged_thread`：指向后台 `khugepaged` 内核线程的 `task_struct`。\n- `khugepaged_pages_to_scan`：每次扫描迭代处理的 PTE 或 VMA 数量。\n- `khugepaged_scan_sleep_millisecs` / `khugepaged_alloc_sleep_millisecs`：控制扫描与内存分配的休眠间隔。\n- `khugepaged_max_ptes_none/swap/shared`：限制在合并过程中允许存在的未映射、交换或共享 PTE 的最大数量。\n- `mm_slots_hash`：哈希表，用于快速查找正在被扫描的 `mm_struct`。\n- `khugepaged_scan`：全局唯一的扫描状态结构体。\n\n### Sysfs 接口（CONFIG_SYSFS）\n\n提供用户空间可配置参数：\n- `scan_sleep_millisecs`：扫描间隔\n- `alloc_sleep_millisecs`：分配失败后的重试间隔\n- `pages_to_scan`：每次扫描的页数\n- `pages_collapsed` / `full_scans`：只读统计信息\n- `defrag`：是否启用内存碎片整理\n- `max_ptes_none` / `max_ptes_swap`：控制合并容忍度\n\n## 3. 关键实现\n\n### 后台扫描机制\n- 使用单一线程 `khugepaged` 循环遍历所有注册到 `mm_slots_hash` 中的进程地址空间。\n- 每次从 `khugepaged_scan.mm_head` 列表中取出一个 `mm_slot`，按虚拟地址顺序扫描其 VMA 区域。\n- 扫描粒度由 `khugepaged_pages_to_scan` 控制，默认为 4096 页（8×512），每轮扫描后休眠 `khugepaged_scan_sleep_millisecs` 毫秒。\n\n### 大页合并条件\n- 仅对支持透明大页的 VMA（如匿名私有映射）进行处理。\n- 检查目标 2MB 区域内：\n  - 已映射的小页数量足够多；\n  - 未映射（none）、交换（swap）或共享（shared）的 PTE 数量不超过 `khugepaged_max_ptes_*` 阈值；\n  - 所有页面满足可合并条件（如非 KSM、非 compound、已加入 LRU、引用计数合适等）。\n- 若满足条件，则分配一个新大页，复制小页内容，并更新页表。\n\n### 内存分配与回退策略\n- 优先在本地 NUMA 节点分配大页。\n- 若分配失败且启用了 `defrag`，则尝试内存压缩（compaction）。\n- 支持基于 `alloc_nmask` 的跨节点分配回退。\n\n### 并发与同步\n- 使用 `khugepaged_mutex` 保护关键操作（如添加/移除 mm slot）。\n- 通过 `mm_slot` 机制确保同一 `mm` 不被重复扫描。\n- 利用 RCU 和页锁（`trylock_page()`）避免与用户态访问或其它内核路径冲突。\n\n### 统计与追踪\n- 更新 `khugepaged_pages_collapsed` 和 `khugepaged_full_scans` 等统计计数器。\n- 集成 `trace/events/huge_memory.h` 提供详细的合并事件追踪点。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/rmap.h>`、`<linux/swap.h>` 等，进行页表遍历、反向映射、页面迁移等操作。\n- **透明大页框架**：与 `huge_memory.c` 协同工作，共享 THP 配置标志（如 `TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG`）。\n- **KSM（Kernel Samepage Merging）**：检查页面是否已被 KSM 标记，避免合并 KSM 页面。\n- **Userfaultfd**：检测 `UFFD_WP`（用户态写保护）标记，防止非法合并。\n- **NUMA 与内存策略**：使用 `nodemask_t` 和 NUMA 感知分配。\n- **内核线程与调度**：基于 `kthread` 框架实现后台任务，支持 freezer（挂起/恢复）。\n- **Sysfs**：通过 sysfs 向用户空间暴露 tunable 参数（需 `CONFIG_SYSFS`）。\n\n## 5. 使用场景\n\n- **通用服务器负载**：在数据库、虚拟化、大数据处理等内存密集型应用中，自动提升 TLB 覆盖率，降低缺页开销。\n- **延迟敏感型应用**：通过后台预合并，避免运行时同步分配大页导致的延迟毛刺。\n- **内存碎片整理**：配合 `defrag` 选项，在内存紧张时主动整理碎片以促进大页分配。\n- **动态调优**：管理员可通过 sysfs 实时调整扫描频率、合并激进程度等参数，平衡性能与内存开销。\n- **NUMA 系统优化**：在多节点系统中，结合本地分配策略提升内存访问局部性。",
      "similarity": 0.5507095456123352,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "mm/khugepaged.c",
          "start_line": 715,
          "end_line": 826,
          "content": [
            "static void __collapse_huge_page_copy_succeeded(pte_t *pte,",
            "\t\t\t\t\t\tstruct vm_area_struct *vma,",
            "\t\t\t\t\t\tunsigned long address,",
            "\t\t\t\t\t\tspinlock_t *ptl,",
            "\t\t\t\t\t\tstruct list_head *compound_pagelist)",
            "{",
            "\tstruct folio *src, *tmp;",
            "\tpte_t *_pte;",
            "\tpte_t pteval;",
            "",
            "\tfor (_pte = pte; _pte < pte + HPAGE_PMD_NR;",
            "\t     _pte++, address += PAGE_SIZE) {",
            "\t\tpteval = ptep_get(_pte);",
            "\t\tif (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {",
            "\t\t\tadd_mm_counter(vma->vm_mm, MM_ANONPAGES, 1);",
            "\t\t\tif (is_zero_pfn(pte_pfn(pteval))) {",
            "\t\t\t\t/*",
            "\t\t\t\t * ptl mostly unnecessary.",
            "\t\t\t\t */",
            "\t\t\t\tspin_lock(ptl);",
            "\t\t\t\tptep_clear(vma->vm_mm, address, _pte);",
            "\t\t\t\tspin_unlock(ptl);",
            "\t\t\t\tksm_might_unmap_zero_page(vma->vm_mm, pteval);",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tstruct page *src_page = pte_page(pteval);",
            "",
            "\t\t\tsrc = page_folio(src_page);",
            "\t\t\tif (!folio_test_large(src))",
            "\t\t\t\trelease_pte_folio(src);",
            "\t\t\t/*",
            "\t\t\t * ptl mostly unnecessary, but preempt has to",
            "\t\t\t * be disabled to update the per-cpu stats",
            "\t\t\t * inside folio_remove_rmap_pte().",
            "\t\t\t */",
            "\t\t\tspin_lock(ptl);",
            "\t\t\tptep_clear(vma->vm_mm, address, _pte);",
            "\t\t\tfolio_remove_rmap_pte(src, src_page, vma);",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\tfree_page_and_swap_cache(src_page);",
            "\t\t}",
            "\t}",
            "",
            "\tlist_for_each_entry_safe(src, tmp, compound_pagelist, lru) {",
            "\t\tlist_del(&src->lru);",
            "\t\tnode_stat_sub_folio(src, NR_ISOLATED_ANON +",
            "\t\t\t\tfolio_is_file_lru(src));",
            "\t\tfolio_unlock(src);",
            "\t\tfree_swap_cache(src);",
            "\t\tfolio_putback_lru(src);",
            "\t}",
            "}",
            "static void __collapse_huge_page_copy_failed(pte_t *pte,",
            "\t\t\t\t\t     pmd_t *pmd,",
            "\t\t\t\t\t     pmd_t orig_pmd,",
            "\t\t\t\t\t     struct vm_area_struct *vma,",
            "\t\t\t\t\t     struct list_head *compound_pagelist)",
            "{",
            "\tspinlock_t *pmd_ptl;",
            "",
            "\t/*",
            "\t * Re-establish the PMD to point to the original page table",
            "\t * entry. Restoring PMD needs to be done prior to releasing",
            "\t * pages. Since pages are still isolated and locked here,",
            "\t * acquiring anon_vma_lock_write is unnecessary.",
            "\t */",
            "\tpmd_ptl = pmd_lock(vma->vm_mm, pmd);",
            "\tpmd_populate(vma->vm_mm, pmd, pmd_pgtable(orig_pmd));",
            "\tspin_unlock(pmd_ptl);",
            "\t/*",
            "\t * Release both raw and compound pages isolated",
            "\t * in __collapse_huge_page_isolate.",
            "\t */",
            "\trelease_pte_pages(pte, pte + HPAGE_PMD_NR, compound_pagelist);",
            "}",
            "static int __collapse_huge_page_copy(pte_t *pte, struct folio *folio,",
            "\t\tpmd_t *pmd, pmd_t orig_pmd, struct vm_area_struct *vma,",
            "\t\tunsigned long address, spinlock_t *ptl,",
            "\t\tstruct list_head *compound_pagelist)",
            "{",
            "\tunsigned int i;",
            "\tint result = SCAN_SUCCEED;",
            "",
            "\t/*",
            "\t * Copying pages' contents is subject to memory poison at any iteration.",
            "\t */",
            "\tfor (i = 0; i < HPAGE_PMD_NR; i++) {",
            "\t\tpte_t pteval = ptep_get(pte + i);",
            "\t\tstruct page *page = folio_page(folio, i);",
            "\t\tunsigned long src_addr = address + i * PAGE_SIZE;",
            "\t\tstruct page *src_page;",
            "",
            "\t\tif (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {",
            "\t\t\tclear_user_highpage(page, src_addr);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tsrc_page = pte_page(pteval);",
            "\t\tif (copy_mc_user_highpage(page, src_page, src_addr, vma)) {",
            "\t\t\tresult = SCAN_COPY_MC;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tif (likely(result == SCAN_SUCCEED))",
            "\t\t__collapse_huge_page_copy_succeeded(pte, vma, address, ptl,",
            "\t\t\t\t\t\t    compound_pagelist);",
            "\telse",
            "\t\t__collapse_huge_page_copy_failed(pte, pmd, orig_pmd, vma,",
            "\t\t\t\t\t\t compound_pagelist);",
            "",
            "\treturn result;",
            "}"
          ],
          "function_name": "__collapse_huge_page_copy_succeeded, __collapse_huge_page_copy_failed, __collapse_huge_page_copy",
          "description": "处理将多个小页合并为大页后的成功与失败路径，成功时释放源页面并更新统计，失败时恢复原页表项",
          "similarity": 0.593684196472168
        },
        {
          "chunk_id": 4,
          "file_path": "mm/khugepaged.c",
          "start_line": 531,
          "end_line": 711,
          "content": [
            "static void release_pte_pages(pte_t *pte, pte_t *_pte,",
            "\t\tstruct list_head *compound_pagelist)",
            "{",
            "\tstruct folio *folio, *tmp;",
            "",
            "\twhile (--_pte >= pte) {",
            "\t\tpte_t pteval = ptep_get(_pte);",
            "\t\tunsigned long pfn;",
            "",
            "\t\tif (pte_none(pteval))",
            "\t\t\tcontinue;",
            "\t\tpfn = pte_pfn(pteval);",
            "\t\tif (is_zero_pfn(pfn))",
            "\t\t\tcontinue;",
            "\t\tfolio = pfn_folio(pfn);",
            "\t\tif (folio_test_large(folio))",
            "\t\t\tcontinue;",
            "\t\trelease_pte_folio(folio);",
            "\t}",
            "",
            "\tlist_for_each_entry_safe(folio, tmp, compound_pagelist, lru) {",
            "\t\tlist_del(&folio->lru);",
            "\t\trelease_pte_folio(folio);",
            "\t}",
            "}",
            "static bool is_refcount_suitable(struct folio *folio)",
            "{",
            "\tint expected_refcount = folio_mapcount(folio);",
            "",
            "\tif (!folio_test_anon(folio) || folio_test_swapcache(folio))",
            "\t\texpected_refcount += folio_nr_pages(folio);",
            "",
            "\tif (folio_test_private(folio))",
            "\t\texpected_refcount++;",
            "",
            "\treturn folio_ref_count(folio) == expected_refcount;",
            "}",
            "static int __collapse_huge_page_isolate(struct vm_area_struct *vma,",
            "\t\t\t\t\tunsigned long address,",
            "\t\t\t\t\tpte_t *pte,",
            "\t\t\t\t\tstruct collapse_control *cc,",
            "\t\t\t\t\tstruct list_head *compound_pagelist)",
            "{",
            "\tstruct page *page = NULL;",
            "\tstruct folio *folio = NULL;",
            "\tpte_t *_pte;",
            "\tint none_or_zero = 0, shared = 0, result = SCAN_FAIL, referenced = 0;",
            "\tbool writable = false;",
            "",
            "\tfor (_pte = pte; _pte < pte + HPAGE_PMD_NR;",
            "\t     _pte++, address += PAGE_SIZE) {",
            "\t\tpte_t pteval = ptep_get(_pte);",
            "\t\tif (pte_none(pteval) || (pte_present(pteval) &&",
            "\t\t\t\tis_zero_pfn(pte_pfn(pteval)))) {",
            "\t\t\t++none_or_zero;",
            "\t\t\tif (!userfaultfd_armed(vma) &&",
            "\t\t\t    (!cc->is_khugepaged ||",
            "\t\t\t     none_or_zero <= khugepaged_max_ptes_none)) {",
            "\t\t\t\tcontinue;",
            "\t\t\t} else {",
            "\t\t\t\tresult = SCAN_EXCEED_NONE_PTE;",
            "\t\t\t\tcount_vm_event(THP_SCAN_EXCEED_NONE_PTE);",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t}",
            "\t\tif (!pte_present(pteval)) {",
            "\t\t\tresult = SCAN_PTE_NON_PRESENT;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tif (pte_uffd_wp(pteval)) {",
            "\t\t\tresult = SCAN_PTE_UFFD_WP;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tpage = vm_normal_page(vma, address, pteval);",
            "\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {",
            "\t\t\tresult = SCAN_PAGE_NULL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tfolio = page_folio(page);",
            "\t\tVM_BUG_ON_FOLIO(!folio_test_anon(folio), folio);",
            "",
            "\t\tif (page_mapcount(page) > 1) {",
            "\t\t\t++shared;",
            "\t\t\tif (cc->is_khugepaged &&",
            "\t\t\t    shared > khugepaged_max_ptes_shared) {",
            "\t\t\t\tresult = SCAN_EXCEED_SHARED_PTE;",
            "\t\t\t\tcount_vm_event(THP_SCAN_EXCEED_SHARED_PTE);",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tif (folio_test_large(folio)) {",
            "\t\t\tstruct folio *f;",
            "",
            "\t\t\t/*",
            "\t\t\t * Check if we have dealt with the compound page",
            "\t\t\t * already",
            "\t\t\t */",
            "\t\t\tlist_for_each_entry(f, compound_pagelist, lru) {",
            "\t\t\t\tif (folio == f)",
            "\t\t\t\t\tgoto next;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * We can do it before isolate_lru_page because the",
            "\t\t * page can't be freed from under us. NOTE: PG_lock",
            "\t\t * is needed to serialize against split_huge_page",
            "\t\t * when invoked from the VM.",
            "\t\t */",
            "\t\tif (!folio_trylock(folio)) {",
            "\t\t\tresult = SCAN_PAGE_LOCK;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Check if the page has any GUP (or other external) pins.",
            "\t\t *",
            "\t\t * The page table that maps the page has been already unlinked",
            "\t\t * from the page table tree and this process cannot get",
            "\t\t * an additional pin on the page.",
            "\t\t *",
            "\t\t * New pins can come later if the page is shared across fork,",
            "\t\t * but not from this process. The other process cannot write to",
            "\t\t * the page, only trigger CoW.",
            "\t\t */",
            "\t\tif (!is_refcount_suitable(folio)) {",
            "\t\t\tfolio_unlock(folio);",
            "\t\t\tresult = SCAN_PAGE_COUNT;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Isolate the page to avoid collapsing an hugepage",
            "\t\t * currently in use by the VM.",
            "\t\t */",
            "\t\tif (!folio_isolate_lru(folio)) {",
            "\t\t\tfolio_unlock(folio);",
            "\t\t\tresult = SCAN_DEL_PAGE_LRU;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tnode_stat_mod_folio(folio,",
            "\t\t\t\tNR_ISOLATED_ANON + folio_is_file_lru(folio),",
            "\t\t\t\tfolio_nr_pages(folio));",
            "\t\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\t\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\t\tif (folio_test_large(folio))",
            "\t\t\tlist_add_tail(&folio->lru, compound_pagelist);",
            "next:",
            "\t\t/*",
            "\t\t * If collapse was initiated by khugepaged, check that there is",
            "\t\t * enough young pte to justify collapsing the page",
            "\t\t */",
            "\t\tif (cc->is_khugepaged &&",
            "\t\t    (pte_young(pteval) || folio_test_young(folio) ||",
            "\t\t     folio_test_referenced(folio) || mmu_notifier_test_young(vma->vm_mm,",
            "\t\t\t\t\t\t\t\t     address)))",
            "\t\t\treferenced++;",
            "",
            "\t\tif (pte_write(pteval))",
            "\t\t\twritable = true;",
            "\t}",
            "",
            "\tif (unlikely(!writable)) {",
            "\t\tresult = SCAN_PAGE_RO;",
            "\t} else if (unlikely(cc->is_khugepaged && !referenced)) {",
            "\t\tresult = SCAN_LACK_REFERENCED_PAGE;",
            "\t} else {",
            "\t\tresult = SCAN_SUCCEED;",
            "\t\ttrace_mm_collapse_huge_page_isolate(&folio->page, none_or_zero,",
            "\t\t\t\t\t\t    referenced, writable, result);",
            "\t\treturn result;",
            "\t}",
            "out:",
            "\trelease_pte_pages(pte, _pte, compound_pagelist);",
            "\ttrace_mm_collapse_huge_page_isolate(&folio->page, none_or_zero,",
            "\t\t\t\t\t    referenced, writable, result);",
            "\treturn result;",
            "}"
          ],
          "function_name": "release_pte_pages, is_refcount_suitable, __collapse_huge_page_isolate",
          "description": "释放页表项对应的小页面资源；验证页面引用计数合法性；尝试隔离符合条件的页面以执行大页合并操作，处理不同状态下的返回结果。",
          "similarity": 0.5840245485305786
        },
        {
          "chunk_id": 8,
          "file_path": "mm/khugepaged.c",
          "start_line": 1101,
          "end_line": 1261,
          "content": [
            "static int collapse_huge_page(struct mm_struct *mm, unsigned long address,",
            "\t\t\t      int referenced, int unmapped,",
            "\t\t\t      struct collapse_control *cc)",
            "{",
            "\tLIST_HEAD(compound_pagelist);",
            "\tpmd_t *pmd, _pmd;",
            "\tpte_t *pte;",
            "\tpgtable_t pgtable;",
            "\tstruct folio *folio;",
            "\tspinlock_t *pmd_ptl, *pte_ptl;",
            "\tint result = SCAN_FAIL;",
            "\tstruct vm_area_struct *vma;",
            "\tstruct mmu_notifier_range range;",
            "",
            "\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);",
            "",
            "\t/*",
            "\t * Before allocating the hugepage, release the mmap_lock read lock.",
            "\t * The allocation can take potentially a long time if it involves",
            "\t * sync compaction, and we do not need to hold the mmap_lock during",
            "\t * that. We will recheck the vma after taking it again in write mode.",
            "\t */",
            "\tmmap_read_unlock(mm);",
            "",
            "\tresult = alloc_charge_folio(&folio, mm, cc);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out_nolock;",
            "",
            "\tmmap_read_lock(mm);",
            "\tresult = hugepage_vma_revalidate(mm, address, true, &vma, cc);",
            "\tif (result != SCAN_SUCCEED) {",
            "\t\tmmap_read_unlock(mm);",
            "\t\tgoto out_nolock;",
            "\t}",
            "",
            "\tresult = find_pmd_or_thp_or_none(mm, address, &pmd);",
            "\tif (result != SCAN_SUCCEED) {",
            "\t\tmmap_read_unlock(mm);",
            "\t\tgoto out_nolock;",
            "\t}",
            "",
            "\tif (unmapped) {",
            "\t\t/*",
            "\t\t * __collapse_huge_page_swapin will return with mmap_lock",
            "\t\t * released when it fails. So we jump out_nolock directly in",
            "\t\t * that case.  Continuing to collapse causes inconsistency.",
            "\t\t */",
            "\t\tresult = __collapse_huge_page_swapin(mm, vma, address, pmd,",
            "\t\t\t\t\t\t     referenced);",
            "\t\tif (result != SCAN_SUCCEED)",
            "\t\t\tgoto out_nolock;",
            "\t}",
            "",
            "\tmmap_read_unlock(mm);",
            "\t/*",
            "\t * Prevent all access to pagetables with the exception of",
            "\t * gup_fast later handled by the ptep_clear_flush and the VM",
            "\t * handled by the anon_vma lock + PG_lock.",
            "\t *",
            "\t * UFFDIO_MOVE is prevented to race as well thanks to the",
            "\t * mmap_lock.",
            "\t */",
            "\tmmap_write_lock(mm);",
            "\tresult = hugepage_vma_revalidate(mm, address, true, &vma, cc);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out_up_write;",
            "\t/* check if the pmd is still valid */",
            "\tresult = check_pmd_still_valid(mm, address, pmd);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out_up_write;",
            "",
            "\tvma_start_write(vma);",
            "\tanon_vma_lock_write(vma->anon_vma);",
            "",
            "\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, address,",
            "\t\t\t\taddress + HPAGE_PMD_SIZE);",
            "\tmmu_notifier_invalidate_range_start(&range);",
            "",
            "\tpmd_ptl = pmd_lock(mm, pmd); /* probably unnecessary */",
            "\t/*",
            "\t * This removes any huge TLB entry from the CPU so we won't allow",
            "\t * huge and small TLB entries for the same virtual address to",
            "\t * avoid the risk of CPU bugs in that area.",
            "\t *",
            "\t * Parallel GUP-fast is fine since GUP-fast will back off when",
            "\t * it detects PMD is changed.",
            "\t */",
            "\t_pmd = pmdp_collapse_flush(vma, address, pmd);",
            "\tspin_unlock(pmd_ptl);",
            "\tmmu_notifier_invalidate_range_end(&range);",
            "\ttlb_remove_table_sync_one();",
            "",
            "\tpte = pte_offset_map_lock(mm, &_pmd, address, &pte_ptl);",
            "\tif (pte) {",
            "\t\tresult = __collapse_huge_page_isolate(vma, address, pte, cc,",
            "\t\t\t\t\t\t      &compound_pagelist);",
            "\t\tspin_unlock(pte_ptl);",
            "\t} else {",
            "\t\tresult = SCAN_PMD_NULL;",
            "\t}",
            "",
            "\tif (unlikely(result != SCAN_SUCCEED)) {",
            "\t\tif (pte)",
            "\t\t\tpte_unmap(pte);",
            "\t\tspin_lock(pmd_ptl);",
            "\t\tBUG_ON(!pmd_none(*pmd));",
            "\t\t/*",
            "\t\t * We can only use set_pmd_at when establishing",
            "\t\t * hugepmds and never for establishing regular pmds that",
            "\t\t * points to regular pagetables. Use pmd_populate for that",
            "\t\t */",
            "\t\tpmd_populate(mm, pmd, pmd_pgtable(_pmd));",
            "\t\tspin_unlock(pmd_ptl);",
            "\t\tanon_vma_unlock_write(vma->anon_vma);",
            "\t\tgoto out_up_write;",
            "\t}",
            "",
            "\t/*",
            "\t * All pages are isolated and locked so anon_vma rmap",
            "\t * can't run anymore.",
            "\t */",
            "\tanon_vma_unlock_write(vma->anon_vma);",
            "",
            "\tresult = __collapse_huge_page_copy(pte, folio, pmd, _pmd,",
            "\t\t\t\t\t   vma, address, pte_ptl,",
            "\t\t\t\t\t   &compound_pagelist);",
            "\tpte_unmap(pte);",
            "\tif (unlikely(result != SCAN_SUCCEED))",
            "\t\tgoto out_up_write;",
            "",
            "\t/*",
            "\t * The smp_wmb() inside __folio_mark_uptodate() ensures the",
            "\t * copy_huge_page writes become visible before the set_pmd_at()",
            "\t * write.",
            "\t */",
            "\t__folio_mark_uptodate(folio);",
            "\tpgtable = pmd_pgtable(_pmd);",
            "",
            "\t_pmd = mk_huge_pmd(&folio->page, vma->vm_page_prot);",
            "\t_pmd = maybe_pmd_mkwrite(pmd_mkdirty(_pmd), vma);",
            "",
            "\tspin_lock(pmd_ptl);",
            "\tBUG_ON(!pmd_none(*pmd));",
            "\tfolio_add_new_anon_rmap(folio, vma, address, RMAP_EXCLUSIVE);",
            "\tfolio_add_lru_vma(folio, vma);",
            "\tpgtable_trans_huge_deposit(mm, pmd, pgtable);",
            "\tset_pmd_at(mm, address, pmd, _pmd);",
            "\tupdate_mmu_cache_pmd(vma, address, pmd);",
            "\tspin_unlock(pmd_ptl);",
            "",
            "\tfolio = NULL;",
            "",
            "\tresult = SCAN_SUCCEED;",
            "out_up_write:",
            "\tmmap_write_unlock(mm);",
            "out_nolock:",
            "\tif (folio)",
            "\t\tfolio_put(folio);",
            "\ttrace_mm_collapse_huge_page(mm, result == SCAN_SUCCEED, result);",
            "\treturn result;",
            "}"
          ],
          "function_name": "collapse_huge_page",
          "description": "执行大页合并主流程，包括页表隔离、数据复制及新PMD设置",
          "similarity": 0.5779486298561096
        },
        {
          "chunk_id": 7,
          "file_path": "mm/khugepaged.c",
          "start_line": 980,
          "end_line": 1090,
          "content": [
            "static int check_pmd_still_valid(struct mm_struct *mm,",
            "\t\t\t\t unsigned long address,",
            "\t\t\t\t pmd_t *pmd)",
            "{",
            "\tpmd_t *new_pmd;",
            "\tint result = find_pmd_or_thp_or_none(mm, address, &new_pmd);",
            "",
            "\tif (result != SCAN_SUCCEED)",
            "\t\treturn result;",
            "\tif (new_pmd != pmd)",
            "\t\treturn SCAN_FAIL;",
            "\treturn SCAN_SUCCEED;",
            "}",
            "static int __collapse_huge_page_swapin(struct mm_struct *mm,",
            "\t\t\t\t       struct vm_area_struct *vma,",
            "\t\t\t\t       unsigned long haddr, pmd_t *pmd,",
            "\t\t\t\t       int referenced)",
            "{",
            "\tint swapped_in = 0;",
            "\tvm_fault_t ret = 0;",
            "\tunsigned long address, end = haddr + (HPAGE_PMD_NR * PAGE_SIZE);",
            "\tint result;",
            "\tpte_t *pte = NULL;",
            "\tspinlock_t *ptl;",
            "",
            "\tfor (address = haddr; address < end; address += PAGE_SIZE) {",
            "\t\tstruct vm_fault vmf = {",
            "\t\t\t.vma = vma,",
            "\t\t\t.address = address,",
            "\t\t\t.pgoff = linear_page_index(vma, address),",
            "\t\t\t.flags = FAULT_FLAG_ALLOW_RETRY,",
            "\t\t\t.pmd = pmd,",
            "\t\t};",
            "",
            "\t\tif (!pte++) {",
            "\t\t\tpte = pte_offset_map_nolock(mm, pmd, address, &ptl);",
            "\t\t\tif (!pte) {",
            "\t\t\t\tmmap_read_unlock(mm);",
            "\t\t\t\tresult = SCAN_PMD_NULL;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tvmf.orig_pte = ptep_get_lockless(pte);",
            "\t\tif (!is_swap_pte(vmf.orig_pte))",
            "\t\t\tcontinue;",
            "",
            "\t\tvmf.pte = pte;",
            "\t\tvmf.ptl = ptl;",
            "\t\tret = do_swap_page(&vmf);",
            "\t\t/* Which unmaps pte (after perhaps re-checking the entry) */",
            "\t\tpte = NULL;",
            "",
            "\t\t/*",
            "\t\t * do_swap_page returns VM_FAULT_RETRY with released mmap_lock.",
            "\t\t * Note we treat VM_FAULT_RETRY as VM_FAULT_ERROR here because",
            "\t\t * we do not retry here and swap entry will remain in pagetable",
            "\t\t * resulting in later failure.",
            "\t\t */",
            "\t\tif (ret & VM_FAULT_RETRY) {",
            "\t\t\t/* Likely, but not guaranteed, that page lock failed */",
            "\t\t\tresult = SCAN_PAGE_LOCK;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tif (ret & VM_FAULT_ERROR) {",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\tresult = SCAN_FAIL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tswapped_in++;",
            "\t}",
            "",
            "\tif (pte)",
            "\t\tpte_unmap(pte);",
            "",
            "\t/* Drain LRU cache to remove extra pin on the swapped in pages */",
            "\tif (swapped_in)",
            "\t\tlru_add_drain();",
            "",
            "\tresult = SCAN_SUCCEED;",
            "out:",
            "\ttrace_mm_collapse_huge_page_swapin(mm, swapped_in, referenced, result);",
            "\treturn result;",
            "}",
            "static int alloc_charge_folio(struct folio **foliop, struct mm_struct *mm,",
            "\t\t\t      struct collapse_control *cc)",
            "{",
            "\tgfp_t gfp = (cc->is_khugepaged ? alloc_hugepage_khugepaged_gfpmask() :",
            "\t\t     GFP_TRANSHUGE);",
            "\tint node = hpage_collapse_find_target_node(cc);",
            "\tstruct folio *folio;",
            "",
            "\tfolio = __folio_alloc(gfp, HPAGE_PMD_ORDER, node, &cc->alloc_nmask);",
            "\tif (!folio) {",
            "\t\t*foliop = NULL;",
            "\t\tcount_vm_event(THP_COLLAPSE_ALLOC_FAILED);",
            "\t\treturn SCAN_ALLOC_HUGE_PAGE_FAIL;",
            "\t}",
            "",
            "\tcount_vm_event(THP_COLLAPSE_ALLOC);",
            "\tif (unlikely(mem_cgroup_charge(folio, mm, gfp))) {",
            "\t\tfolio_put(folio);",
            "\t\t*foliop = NULL;",
            "\t\treturn SCAN_CGROUP_CHARGE_FAIL;",
            "\t}",
            "",
            "\tcount_memcg_folio_events(folio, THP_COLLAPSE_ALLOC, 1);",
            "",
            "\t*foliop = folio;",
            "\treturn SCAN_SUCCEED;",
            "}"
          ],
          "function_name": "check_pmd_still_valid, __collapse_huge_page_swapin, alloc_charge_folio",
          "description": "验证PMD有效性、交换页加载及大页分配的辅助函数",
          "similarity": 0.5484901666641235
        },
        {
          "chunk_id": 15,
          "file_path": "mm/khugepaged.c",
          "start_line": 2726,
          "end_line": 2874,
          "content": [
            "void khugepaged_min_free_kbytes_update(void)",
            "{",
            "\tmutex_lock(&khugepaged_mutex);",
            "\tif (hugepage_pmd_enabled() && khugepaged_thread)",
            "\t\tset_recommended_min_free_kbytes();",
            "\tmutex_unlock(&khugepaged_mutex);",
            "}",
            "bool current_is_khugepaged(void)",
            "{",
            "\treturn kthread_func(current) == khugepaged;",
            "}",
            "static int madvise_collapse_errno(enum scan_result r)",
            "{",
            "\t/*",
            "\t * MADV_COLLAPSE breaks from existing madvise(2) conventions to provide",
            "\t * actionable feedback to caller, so they may take an appropriate",
            "\t * fallback measure depending on the nature of the failure.",
            "\t */",
            "\tswitch (r) {",
            "\tcase SCAN_ALLOC_HUGE_PAGE_FAIL:",
            "\t\treturn -ENOMEM;",
            "\tcase SCAN_CGROUP_CHARGE_FAIL:",
            "\tcase SCAN_EXCEED_NONE_PTE:",
            "\t\treturn -EBUSY;",
            "\t/* Resource temporary unavailable - trying again might succeed */",
            "\tcase SCAN_PAGE_COUNT:",
            "\tcase SCAN_PAGE_LOCK:",
            "\tcase SCAN_PAGE_LRU:",
            "\tcase SCAN_DEL_PAGE_LRU:",
            "\tcase SCAN_PAGE_FILLED:",
            "\t\treturn -EAGAIN;",
            "\t/*",
            "\t * Other: Trying again likely not to succeed / error intrinsic to",
            "\t * specified memory range. khugepaged likely won't be able to collapse",
            "\t * either.",
            "\t */",
            "\tdefault:",
            "\t\treturn -EINVAL;",
            "\t}",
            "}",
            "int madvise_collapse(struct vm_area_struct *vma, struct vm_area_struct **prev,",
            "\t\t     unsigned long start, unsigned long end)",
            "{",
            "\tstruct collapse_control *cc;",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tunsigned long hstart, hend, addr;",
            "\tint thps = 0, last_fail = SCAN_FAIL;",
            "\tbool mmap_locked = true;",
            "",
            "\tBUG_ON(vma->vm_start > start);",
            "\tBUG_ON(vma->vm_end < end);",
            "",
            "\t*prev = vma;",
            "",
            "\tif (!thp_vma_allowable_order(vma, vma->vm_flags, 0, PMD_ORDER))",
            "\t\treturn -EINVAL;",
            "",
            "\tcc = kmalloc(sizeof(*cc), GFP_KERNEL);",
            "\tif (!cc)",
            "\t\treturn -ENOMEM;",
            "\tcc->is_khugepaged = false;",
            "",
            "\tmmgrab(mm);",
            "\tlru_add_drain_all();",
            "",
            "\thstart = (start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;",
            "\thend = end & HPAGE_PMD_MASK;",
            "",
            "\tfor (addr = hstart; addr < hend; addr += HPAGE_PMD_SIZE) {",
            "\t\tint result = SCAN_FAIL;",
            "",
            "\t\tif (!mmap_locked) {",
            "\t\t\tcond_resched();",
            "\t\t\tmmap_read_lock(mm);",
            "\t\t\tmmap_locked = true;",
            "\t\t\tresult = hugepage_vma_revalidate(mm, addr, false, &vma,",
            "\t\t\t\t\t\t\t cc);",
            "\t\t\tif (result  != SCAN_SUCCEED) {",
            "\t\t\t\tlast_fail = result;",
            "\t\t\t\tgoto out_nolock;",
            "\t\t\t}",
            "",
            "\t\t\thend = min(hend, vma->vm_end & HPAGE_PMD_MASK);",
            "\t\t}",
            "\t\tmmap_assert_locked(mm);",
            "\t\tmemset(cc->node_load, 0, sizeof(cc->node_load));",
            "\t\tnodes_clear(cc->alloc_nmask);",
            "\t\tif (IS_ENABLED(CONFIG_SHMEM) && !vma_is_anonymous(vma)) {",
            "\t\t\tstruct file *file = get_file(vma->vm_file);",
            "\t\t\tpgoff_t pgoff = linear_page_index(vma, addr);",
            "",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\tmmap_locked = false;",
            "\t\t\tresult = hpage_collapse_scan_file(mm, addr, file, pgoff,",
            "\t\t\t\t\t\t\t  cc);",
            "\t\t\tfput(file);",
            "\t\t} else {",
            "\t\t\tresult = hpage_collapse_scan_pmd(mm, vma, addr,",
            "\t\t\t\t\t\t\t &mmap_locked, cc);",
            "\t\t}",
            "\t\tif (!mmap_locked)",
            "\t\t\t*prev = NULL;  /* Tell caller we dropped mmap_lock */",
            "",
            "handle_result:",
            "\t\tswitch (result) {",
            "\t\tcase SCAN_SUCCEED:",
            "\t\tcase SCAN_PMD_MAPPED:",
            "\t\t\t++thps;",
            "\t\t\tbreak;",
            "\t\tcase SCAN_PTE_MAPPED_HUGEPAGE:",
            "\t\t\tBUG_ON(mmap_locked);",
            "\t\t\tBUG_ON(*prev);",
            "\t\t\tmmap_read_lock(mm);",
            "\t\t\tresult = collapse_pte_mapped_thp(mm, addr, true);",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\tgoto handle_result;",
            "\t\t/* Whitelisted set of results where continuing OK */",
            "\t\tcase SCAN_PMD_NULL:",
            "\t\tcase SCAN_PTE_NON_PRESENT:",
            "\t\tcase SCAN_PTE_UFFD_WP:",
            "\t\tcase SCAN_PAGE_RO:",
            "\t\tcase SCAN_LACK_REFERENCED_PAGE:",
            "\t\tcase SCAN_PAGE_NULL:",
            "\t\tcase SCAN_PAGE_COUNT:",
            "\t\tcase SCAN_PAGE_LOCK:",
            "\t\tcase SCAN_PAGE_COMPOUND:",
            "\t\tcase SCAN_PAGE_LRU:",
            "\t\tcase SCAN_DEL_PAGE_LRU:",
            "\t\t\tlast_fail = result;",
            "\t\t\tbreak;",
            "\t\tdefault:",
            "\t\t\tlast_fail = result;",
            "\t\t\t/* Other error, exit */",
            "\t\t\tgoto out_maybelock;",
            "\t\t}",
            "\t}",
            "",
            "out_maybelock:",
            "\t/* Caller expects us to hold mmap_lock on return */",
            "\tif (!mmap_locked)",
            "\t\tmmap_read_lock(mm);",
            "out_nolock:",
            "\tmmap_assert_locked(mm);",
            "\tmmdrop(mm);",
            "\tkfree(cc);",
            "",
            "\treturn thps == ((hend - hstart) >> HPAGE_PMD_SHIFT) ? 0",
            "\t\t\t: madvise_collapse_errno(last_fail);",
            "}"
          ],
          "function_name": "khugepaged_min_free_kbytes_update, current_is_khugepaged, madvise_collapse_errno, madvise_collapse",
          "description": "该代码段实现了与HugeTLB大页管理相关的功能。  \n`khugepaged_min_free_kbytes_update` 根据大页启用状态动态调整系统最小空闲内存阈值；`current_is_khugepaged` 用于判断当前进程是否为khugepaged线程；`madvise_collapse` 执行内存区域的大页折叠操作，通过遍历地址范围尝试合并普通页为大页，并根据扫描结果返回对应错误码。  \n注：代码上下文不完整，部分逻辑分支未展示，且`madvise_collapse`中涉及的辅助函数如`hpage_collapse_scan_file`等未在片段中出现。",
          "similarity": 0.5428229570388794
        }
      ]
    }
  ]
}