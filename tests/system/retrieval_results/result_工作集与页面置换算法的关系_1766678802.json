{
  "query": "工作集与页面置换算法的关系",
  "timestamp": "2025-12-26 00:06:42",
  "retrieved_files": [
    {
      "source_file": "mm/workingset.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:34:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workingset.c`\n\n---\n\n# workingset.c 技术文档\n\n## 1. 文件概述\n\n`workingset.c` 实现了 Linux 内核中的 **工作集检测（Workingset Detection）** 机制，用于优化页面回收（page reclaim）策略。该机制通过跟踪页面的访问模式和重故障距离（refault distance），智能判断哪些页面应保留在内存中，从而减少系统颠簸（thrashing）并提升缓存效率。核心思想是：若一个被换出的页面在短时间内再次被访问（即重故障），且其重故障距离小于当前活跃页面数量，则应将其重新激活，以取代可能已不再活跃的现有活跃页面。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **Shadow Entry（影子条目）**：存储在页缓存槽位中的元数据，包含页面被驱逐时的时间戳（eviction counter 快照）、内存控制组 ID、节点 ID 和工作集标志。\n- **node->nonresident_age**：每个 NUMA 节点维护的计数器，记录非驻留页面的“年龄”，用于计算重故障距离。\n\n### 关键宏定义\n- `WORKINGSET_SHIFT`：工作集标识位偏移。\n- `EVICTION_SHIFT` / `EVICTION_MASK`：用于在 xarray 条目中紧凑编码驱逐时间戳的位操作参数。\n- `bucket_order`：当时间戳位数不足时，用于对驱逐事件进行分桶聚合的粒度。\n\n### 核心函数（部分实现）\n- `pack_shadow()`：将内存控制组 ID、节点指针、驱逐计数器值和工作集标志打包成一个 shadow entry。\n- （注：代码片段未完整展示其他关键函数如 `workingset_refault()`、`workingset_activation()` 等，但文档基于完整机制描述）\n\n## 3. 关键实现\n\n### 双 CLOCK 列表模型\n- 每个 NUMA 节点为文件页维护两个 LRU 列表：**inactive list**（不活跃）和 **active list**（活跃）。\n- 新缺页页面加入 inactive list 头部；回收从 inactive list 尾部扫描。\n- 在 inactive list 上被二次访问的页面晋升至 active list；active list 过长时，尾部页面降级到 inactive list。\n\n### 重故障距离（Refault Distance）算法\n1. **驱逐时记录**：页面被驱逐时，将其所在节点的 `nonresident_age` 计数器值（代表累计的驱逐+激活次数）作为时间戳存入 shadow entry。\n2. **重故障时计算**：\n   - 当缺页发生且存在对应 shadow entry 时，读取当前 `nonresident_age` 值（R）与 shadow 中存储的值（E）。\n   - 重故障距离 = `R - E`，表示页面不在内存期间发生的最小页面访问次数。\n3. **激活决策**：\n   - 若 `重故障距离 <= 当前活跃页面总数（file + anon）`，则认为若当时有足够 inactive 空间，该页面本可被激活而避免驱逐。\n   - 因此**乐观地激活**该重故障页面，使其与现有活跃页面竞争内存空间。\n\n### 影子条目压缩存储\n- 利用 xarray 条目的有限位宽（`BITS_PER_XA_VALUE`），通过位域拼接存储：\n  - 节点 ID（`NODES_SHIFT` 位）\n  - 内存控制组 ID（`MEM_CGROUP_ID_SHIFT` 位）\n  - 工作集标志（`WORKINGSET_SHIFT` 位）\n  - 驱逐时间戳（剩余位，必要时通过 `bucket_order` 降低精度）\n\n## 4. 依赖关系\n\n- **内存管理核心**：`<linux/mm.h>`, `<linux/mm_inline.h>` — 提供页框、LRU 列表、页表操作等基础支持。\n- **内存控制组**：`<linux/memcontrol.h>` — 支持按 cgroup 隔离工作集统计。\n- **页缓存与交换**：`<linux/pagemap.h>`, `<linux/swap.h>`, `<linux/shmem_fs.h>` — 处理文件页、匿名页、tmpfs 页的回收逻辑。\n- **xarray 数据结构**：用于高效存储和检索 shadow entries（隐含在 `pack_shadow` 的位操作中）。\n- **DAX 支持**：`<linux/dax.h>` — 确保直接访问持久内存设备的页面也能参与工作集检测。\n\n## 5. 使用场景\n\n- **内存压力下的页面回收**：当系统内存紧张触发 kswapd 或直接回收时，工作集检测机制指导选择最优的牺牲页面。\n- **工作集切换检测**：识别应用程序工作集的动态变化（如新任务启动、旧任务结束），快速淘汰过时缓存。\n- **防止颠簸（Thrashing）**：在活跃工作集大小接近或超过可用内存时，通过重故障距离预测避免频繁换入换出。\n- **混合工作负载优化**：同时处理文件缓存（page cache）和匿名内存（anonymous pages）的工作集，平衡二者内存分配。\n- **容器化环境**：结合 memcg，在多租户系统中为每个容器独立维护工作集状态，避免相互干扰。",
      "similarity": 0.5635089874267578,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/workingset.c",
          "start_line": 418,
          "end_line": 526,
          "content": [
            "bool workingset_test_recent(void *shadow, bool file, bool *workingset,",
            "\t\t\t\tbool flush)",
            "{",
            "\tstruct mem_cgroup *eviction_memcg;",
            "\tstruct lruvec *eviction_lruvec;",
            "\tunsigned long refault_distance;",
            "\tunsigned long workingset_size;",
            "\tunsigned long refault;",
            "\tint memcgid;",
            "\tstruct pglist_data *pgdat;",
            "\tunsigned long eviction;",
            "",
            "\trcu_read_lock();",
            "",
            "\tif (lru_gen_enabled()) {",
            "\t\tbool recent = lru_gen_test_recent(shadow, file,",
            "\t\t\t\t&eviction_lruvec, &eviction, workingset);",
            "",
            "\t\trcu_read_unlock();",
            "\t\treturn recent;",
            "\t}",
            "",
            "",
            "\tunpack_shadow(shadow, &memcgid, &pgdat, &eviction, workingset);",
            "\teviction <<= bucket_order;",
            "",
            "\t/*",
            "\t * Look up the memcg associated with the stored ID. It might",
            "\t * have been deleted since the folio's eviction.",
            "\t *",
            "\t * Note that in rare events the ID could have been recycled",
            "\t * for a new cgroup that refaults a shared folio. This is",
            "\t * impossible to tell from the available data. However, this",
            "\t * should be a rare and limited disturbance, and activations",
            "\t * are always speculative anyway. Ultimately, it's the aging",
            "\t * algorithm's job to shake out the minimum access frequency",
            "\t * for the active cache.",
            "\t *",
            "\t * XXX: On !CONFIG_MEMCG, this will always return NULL; it",
            "\t * would be better if the root_mem_cgroup existed in all",
            "\t * configurations instead.",
            "\t */",
            "\teviction_memcg = mem_cgroup_from_id(memcgid);",
            "\tif (!mem_cgroup_disabled() &&",
            "\t    (!eviction_memcg || !mem_cgroup_tryget(eviction_memcg))) {",
            "\t\trcu_read_unlock();",
            "\t\treturn false;",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * Flush stats (and potentially sleep) outside the RCU read section.",
            "\t *",
            "\t * Note that workingset_test_recent() itself might be called in RCU read",
            "\t * section (for e.g, in cachestat) - these callers need to skip flushing",
            "\t * stats (via the flush argument).",
            "\t *",
            "\t * XXX: With per-memcg flushing and thresholding, is ratelimiting",
            "\t * still needed here?",
            "\t */",
            "\tif (flush)",
            "\t\tmem_cgroup_flush_stats_ratelimited(eviction_memcg);",
            "",
            "\teviction_lruvec = mem_cgroup_lruvec(eviction_memcg, pgdat);",
            "\trefault = atomic_long_read(&eviction_lruvec->nonresident_age);",
            "",
            "\t/*",
            "\t * Calculate the refault distance",
            "\t *",
            "\t * The unsigned subtraction here gives an accurate distance",
            "\t * across nonresident_age overflows in most cases. There is a",
            "\t * special case: usually, shadow entries have a short lifetime",
            "\t * and are either refaulted or reclaimed along with the inode",
            "\t * before they get too old.  But it is not impossible for the",
            "\t * nonresident_age to lap a shadow entry in the field, which",
            "\t * can then result in a false small refault distance, leading",
            "\t * to a false activation should this old entry actually",
            "\t * refault again.  However, earlier kernels used to deactivate",
            "\t * unconditionally with *every* reclaim invocation for the",
            "\t * longest time, so the occasional inappropriate activation",
            "\t * leading to pressure on the active list is not a problem.",
            "\t */",
            "\trefault_distance = (refault - eviction) & EVICTION_MASK;",
            "",
            "\t/*",
            "\t * Compare the distance to the existing workingset size. We",
            "\t * don't activate pages that couldn't stay resident even if",
            "\t * all the memory was available to the workingset. Whether",
            "\t * workingset competition needs to consider anon or not depends",
            "\t * on having free swap space.",
            "\t */",
            "\tworkingset_size = lruvec_page_state(eviction_lruvec, NR_ACTIVE_FILE);",
            "\tif (!file) {",
            "\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_INACTIVE_FILE);",
            "\t}",
            "\tif (mem_cgroup_get_nr_swap_pages(eviction_memcg) > 0) {",
            "\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_ACTIVE_ANON);",
            "\t\tif (file) {",
            "\t\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_INACTIVE_ANON);",
            "\t\t}",
            "\t}",
            "",
            "\tmem_cgroup_put(eviction_memcg);",
            "\treturn refault_distance <= workingset_size;",
            "}"
          ],
          "function_name": "workingset_test_recent",
          "description": "实现工作集测试最近访问的判断逻辑，通过对比参考距离与当前工作集大小决定是否激活页面，支持内存组场景下的统计信息处理。",
          "similarity": 0.6358806490898132
        },
        {
          "chunk_id": 3,
          "file_path": "mm/workingset.c",
          "start_line": 537,
          "end_line": 689,
          "content": [
            "void workingset_refault(struct folio *folio, void *shadow)",
            "{",
            "\tbool file = folio_is_file_lru(folio);",
            "\tstruct pglist_data *pgdat;",
            "\tstruct mem_cgroup *memcg;",
            "\tstruct lruvec *lruvec;",
            "\tbool workingset;",
            "\tlong nr;",
            "",
            "\tif (lru_gen_enabled()) {",
            "\t\tlru_gen_refault(folio, shadow);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * The activation decision for this folio is made at the level",
            "\t * where the eviction occurred, as that is where the LRU order",
            "\t * during folio reclaim is being determined.",
            "\t *",
            "\t * However, the cgroup that will own the folio is the one that",
            "\t * is actually experiencing the refault event. Make sure the folio is",
            "\t * locked to guarantee folio_memcg() stability throughout.",
            "\t */",
            "\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\tnr = folio_nr_pages(folio);",
            "\tmemcg = folio_memcg(folio);",
            "\tpgdat = folio_pgdat(folio);",
            "\tlruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "",
            "\tmod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + file, nr);",
            "",
            "\tif (!workingset_test_recent(shadow, file, &workingset, true))",
            "\t\treturn;",
            "",
            "\tfolio_set_active(folio);",
            "\tworkingset_age_nonresident(lruvec, nr);",
            "\tmod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + file, nr);",
            "",
            "\t/* Folio was active prior to eviction */",
            "\tif (workingset) {",
            "\t\tfolio_set_workingset(folio);",
            "\t\t/*",
            "\t\t * XXX: Move to folio_add_lru() when it supports new vs",
            "\t\t * putback",
            "\t\t */",
            "\t\tlru_note_cost_refault(folio);",
            "\t\tmod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + file, nr);",
            "\t}",
            "}",
            "void workingset_activation(struct folio *folio)",
            "{",
            "\tstruct mem_cgroup *memcg;",
            "",
            "\trcu_read_lock();",
            "\t/*",
            "\t * Filter non-memcg pages here, e.g. unmap can call",
            "\t * mark_page_accessed() on VDSO pages.",
            "\t *",
            "\t * XXX: See workingset_refault() - this should return",
            "\t * root_mem_cgroup even for !CONFIG_MEMCG.",
            "\t */",
            "\tmemcg = folio_memcg_rcu(folio);",
            "\tif (!mem_cgroup_disabled() && !memcg)",
            "\t\tgoto out;",
            "\tworkingset_age_nonresident(folio_lruvec(folio), folio_nr_pages(folio));",
            "out:",
            "\trcu_read_unlock();",
            "}",
            "void workingset_update_node(struct xa_node *node)",
            "{",
            "\tstruct address_space *mapping;",
            "\tstruct page *page = virt_to_page(node);",
            "",
            "\t/*",
            "\t * Track non-empty nodes that contain only shadow entries;",
            "\t * unlink those that contain pages or are being freed.",
            "\t *",
            "\t * Avoid acquiring the list_lru lock when the nodes are",
            "\t * already where they should be. The list_empty() test is safe",
            "\t * as node->private_list is protected by the i_pages lock.",
            "\t */",
            "\tmapping = container_of(node->array, struct address_space, i_pages);",
            "\tlockdep_assert_held(&mapping->i_pages.xa_lock);",
            "",
            "\tif (node->count && node->count == node->nr_values) {",
            "\t\tif (list_empty(&node->private_list)) {",
            "\t\t\tlist_lru_add_obj(&shadow_nodes, &node->private_list);",
            "\t\t\t__inc_node_page_state(page, WORKINGSET_NODES);",
            "\t\t}",
            "\t} else {",
            "\t\tif (!list_empty(&node->private_list)) {",
            "\t\t\tlist_lru_del_obj(&shadow_nodes, &node->private_list);",
            "\t\t\t__dec_node_page_state(page, WORKINGSET_NODES);",
            "\t\t}",
            "\t}",
            "}",
            "static unsigned long count_shadow_nodes(struct shrinker *shrinker,",
            "\t\t\t\t\tstruct shrink_control *sc)",
            "{",
            "\tunsigned long max_nodes;",
            "\tunsigned long nodes;",
            "\tunsigned long pages;",
            "",
            "\tnodes = list_lru_shrink_count(&shadow_nodes, sc);",
            "\tif (!nodes)",
            "\t\treturn SHRINK_EMPTY;",
            "",
            "\t/*",
            "\t * Approximate a reasonable limit for the nodes",
            "\t * containing shadow entries. We don't need to keep more",
            "\t * shadow entries than possible pages on the active list,",
            "\t * since refault distances bigger than that are dismissed.",
            "\t *",
            "\t * The size of the active list converges toward 100% of",
            "\t * overall page cache as memory grows, with only a tiny",
            "\t * inactive list. Assume the total cache size for that.",
            "\t *",
            "\t * Nodes might be sparsely populated, with only one shadow",
            "\t * entry in the extreme case. Obviously, we cannot keep one",
            "\t * node for every eligible shadow entry, so compromise on a",
            "\t * worst-case density of 1/8th. Below that, not all eligible",
            "\t * refaults can be detected anymore.",
            "\t *",
            "\t * On 64-bit with 7 xa_nodes per page and 64 slots",
            "\t * each, this will reclaim shadow entries when they consume",
            "\t * ~1.8% of available memory:",
            "\t *",
            "\t * PAGE_SIZE / xa_nodes / node_entries * 8 / PAGE_SIZE",
            "\t */",
            "#ifdef CONFIG_MEMCG",
            "\tif (sc->memcg) {",
            "\t\tstruct lruvec *lruvec;",
            "\t\tint i;",
            "",
            "\t\tmem_cgroup_flush_stats_ratelimited(sc->memcg);",
            "\t\tlruvec = mem_cgroup_lruvec(sc->memcg, NODE_DATA(sc->nid));",
            "\t\tfor (pages = 0, i = 0; i < NR_LRU_LISTS; i++)",
            "\t\t\tpages += lruvec_page_state_local(lruvec,",
            "\t\t\t\t\t\t\t NR_LRU_BASE + i);",
            "\t\tpages += lruvec_page_state_local(",
            "\t\t\tlruvec, NR_SLAB_RECLAIMABLE_B) >> PAGE_SHIFT;",
            "\t\tpages += lruvec_page_state_local(",
            "\t\t\tlruvec, NR_SLAB_UNRECLAIMABLE_B) >> PAGE_SHIFT;",
            "\t} else",
            "#endif",
            "\t\tpages = node_present_pages(sc->nid);",
            "",
            "\tmax_nodes = pages >> (XA_CHUNK_SHIFT - 3);",
            "",
            "\tif (nodes <= max_nodes)",
            "\t\treturn 0;",
            "\treturn nodes - max_nodes;",
            "}"
          ],
          "function_name": "workingset_refault, workingset_activation, workingset_update_node, count_shadow_nodes",
          "description": "提供页面故障后的激活处理、节点更新及影子节点追踪功能，包含基于工作集状态更新节点计数器和触发页面重新激活的逻辑。",
          "similarity": 0.5849034786224365
        },
        {
          "chunk_id": 4,
          "file_path": "mm/workingset.c",
          "start_line": 712,
          "end_line": 833,
          "content": [
            "static enum lru_status shadow_lru_isolate(struct list_head *item,",
            "\t\t\t\t\t  struct list_lru_one *lru,",
            "\t\t\t\t\t  spinlock_t *lru_lock,",
            "\t\t\t\t\t  void *arg) __must_hold(lru_lock)",
            "{",
            "\tstruct xa_node *node = container_of(item, struct xa_node, private_list);",
            "\tstruct address_space *mapping;",
            "\tint ret;",
            "",
            "\t/*",
            "\t * Page cache insertions and deletions synchronously maintain",
            "\t * the shadow node LRU under the i_pages lock and the",
            "\t * lru_lock.  Because the page cache tree is emptied before",
            "\t * the inode can be destroyed, holding the lru_lock pins any",
            "\t * address_space that has nodes on the LRU.",
            "\t *",
            "\t * We can then safely transition to the i_pages lock to",
            "\t * pin only the address_space of the particular node we want",
            "\t * to reclaim, take the node off-LRU, and drop the lru_lock.",
            "\t */",
            "",
            "\tmapping = container_of(node->array, struct address_space, i_pages);",
            "",
            "\t/* Coming from the list, invert the lock order */",
            "\tif (!xa_trylock(&mapping->i_pages)) {",
            "\t\tspin_unlock_irq(lru_lock);",
            "\t\tret = LRU_RETRY;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* For page cache we need to hold i_lock */",
            "\tif (mapping->host != NULL) {",
            "\t\tif (!spin_trylock(&mapping->host->i_lock)) {",
            "\t\t\txa_unlock(&mapping->i_pages);",
            "\t\t\tspin_unlock_irq(lru_lock);",
            "\t\t\tret = LRU_RETRY;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t}",
            "",
            "\tlist_lru_isolate(lru, item);",
            "\t__dec_node_page_state(virt_to_page(node), WORKINGSET_NODES);",
            "",
            "\tspin_unlock(lru_lock);",
            "",
            "\t/*",
            "\t * The nodes should only contain one or more shadow entries,",
            "\t * no pages, so we expect to be able to remove them all and",
            "\t * delete and free the empty node afterwards.",
            "\t */",
            "\tif (WARN_ON_ONCE(!node->nr_values))",
            "\t\tgoto out_invalid;",
            "\tif (WARN_ON_ONCE(node->count != node->nr_values))",
            "\t\tgoto out_invalid;",
            "\txa_delete_node(node, workingset_update_node);",
            "\t__inc_lruvec_kmem_state(node, WORKINGSET_NODERECLAIM);",
            "",
            "out_invalid:",
            "\txa_unlock_irq(&mapping->i_pages);",
            "\tif (mapping->host != NULL) {",
            "\t\tif (mapping_shrinkable(mapping))",
            "\t\t\tinode_add_lru(mapping->host);",
            "\t\tspin_unlock(&mapping->host->i_lock);",
            "\t}",
            "\tret = LRU_REMOVED_RETRY;",
            "out:",
            "\tcond_resched();",
            "\tspin_lock_irq(lru_lock);",
            "\treturn ret;",
            "}",
            "static unsigned long scan_shadow_nodes(struct shrinker *shrinker,",
            "\t\t\t\t       struct shrink_control *sc)",
            "{",
            "\t/* list_lru lock nests inside the IRQ-safe i_pages lock */",
            "\treturn list_lru_shrink_walk_irq(&shadow_nodes, sc, shadow_lru_isolate,",
            "\t\t\t\t\tNULL);",
            "}",
            "static int __init workingset_init(void)",
            "{",
            "\tstruct shrinker *workingset_shadow_shrinker;",
            "\tunsigned int timestamp_bits;",
            "\tunsigned int max_order;",
            "\tint ret = -ENOMEM;",
            "",
            "\tBUILD_BUG_ON(BITS_PER_LONG < EVICTION_SHIFT);",
            "\t/*",
            "\t * Calculate the eviction bucket size to cover the longest",
            "\t * actionable refault distance, which is currently half of",
            "\t * memory (totalram_pages/2). However, memory hotplug may add",
            "\t * some more pages at runtime, so keep working with up to",
            "\t * double the initial memory by using totalram_pages as-is.",
            "\t */",
            "\ttimestamp_bits = BITS_PER_LONG - EVICTION_SHIFT;",
            "\tmax_order = fls_long(totalram_pages() - 1);",
            "\tif (max_order > timestamp_bits)",
            "\t\tbucket_order = max_order - timestamp_bits;",
            "\tpr_info(\"workingset: timestamp_bits=%d max_order=%d bucket_order=%u\\n\",",
            "\t       timestamp_bits, max_order, bucket_order);",
            "",
            "\tworkingset_shadow_shrinker = shrinker_alloc(SHRINKER_NUMA_AWARE |",
            "\t\t\t\t\t\t    SHRINKER_MEMCG_AWARE,",
            "\t\t\t\t\t\t    \"mm-shadow\");",
            "\tif (!workingset_shadow_shrinker)",
            "\t\tgoto err;",
            "",
            "\tret = __list_lru_init(&shadow_nodes, true, &shadow_nodes_key,",
            "\t\t\t      workingset_shadow_shrinker);",
            "\tif (ret)",
            "\t\tgoto err_list_lru;",
            "",
            "\tworkingset_shadow_shrinker->count_objects = count_shadow_nodes;",
            "\tworkingset_shadow_shrinker->scan_objects = scan_shadow_nodes;",
            "\t/* ->count reports only fully expendable nodes */",
            "\tworkingset_shadow_shrinker->seeks = 0;",
            "",
            "\tshrinker_register(workingset_shadow_shrinker);",
            "\treturn 0;",
            "err_list_lru:",
            "\tshrinker_free(workingset_shadow_shrinker);",
            "err:",
            "\treturn ret;",
            "}"
          ],
          "function_name": "shadow_lru_isolate, scan_shadow_nodes, workingset_init",
          "description": "初始化工作集模块，注册收缩器管理影子节点，定义节点隔离和扫描机制，通过计算时间戳位宽确定桶阶参数以保证参考距离覆盖范围。",
          "similarity": 0.5322071313858032
        },
        {
          "chunk_id": 1,
          "file_path": "mm/workingset.c",
          "start_line": 209,
          "end_line": 314,
          "content": [
            "static void unpack_shadow(void *shadow, int *memcgidp, pg_data_t **pgdat,",
            "\t\t\t  unsigned long *evictionp, bool *workingsetp)",
            "{",
            "\tunsigned long entry = xa_to_value(shadow);",
            "\tint memcgid, nid;",
            "\tbool workingset;",
            "",
            "\tworkingset = entry & ((1UL << WORKINGSET_SHIFT) - 1);",
            "\tentry >>= WORKINGSET_SHIFT;",
            "\tnid = entry & ((1UL << NODES_SHIFT) - 1);",
            "\tentry >>= NODES_SHIFT;",
            "\tmemcgid = entry & ((1UL << MEM_CGROUP_ID_SHIFT) - 1);",
            "\tentry >>= MEM_CGROUP_ID_SHIFT;",
            "",
            "\t*memcgidp = memcgid;",
            "\t*pgdat = NODE_DATA(nid);",
            "\t*evictionp = entry;",
            "\t*workingsetp = workingset;",
            "}",
            "static bool lru_gen_test_recent(void *shadow, bool file, struct lruvec **lruvec,",
            "\t\t\t\tunsigned long *token, bool *workingset)",
            "{",
            "\tint memcg_id;",
            "\tunsigned long min_seq;",
            "\tstruct mem_cgroup *memcg;",
            "\tstruct pglist_data *pgdat;",
            "",
            "\tunpack_shadow(shadow, &memcg_id, &pgdat, token, workingset);",
            "",
            "\tmemcg = mem_cgroup_from_id(memcg_id);",
            "\t*lruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "",
            "\tmin_seq = READ_ONCE((*lruvec)->lrugen.min_seq[file]);",
            "\treturn (*token >> LRU_REFS_WIDTH) == (min_seq & (EVICTION_MASK >> LRU_REFS_WIDTH));",
            "}",
            "static void lru_gen_refault(struct folio *folio, void *shadow)",
            "{",
            "\tbool recent;",
            "\tint hist, tier, refs;",
            "\tbool workingset;",
            "\tunsigned long token;",
            "\tstruct lruvec *lruvec;",
            "\tstruct lru_gen_folio *lrugen;",
            "\tint type = folio_is_file_lru(folio);",
            "\tint delta = folio_nr_pages(folio);",
            "",
            "\trcu_read_lock();",
            "",
            "\trecent = lru_gen_test_recent(shadow, type, &lruvec, &token, &workingset);",
            "\tif (lruvec != folio_lruvec(folio))",
            "\t\tgoto unlock;",
            "",
            "\tmod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + type, delta);",
            "",
            "\tif (!recent)",
            "\t\tgoto unlock;",
            "",
            "\tlrugen = &lruvec->lrugen;",
            "",
            "\thist = lru_hist_from_seq(READ_ONCE(lrugen->min_seq[type]));",
            "\t/* see the comment in folio_lru_refs() */",
            "\trefs = (token & (BIT(LRU_REFS_WIDTH) - 1)) + workingset;",
            "\ttier = lru_tier_from_refs(refs);",
            "",
            "\tatomic_long_add(delta, &lrugen->refaulted[hist][type][tier]);",
            "\tmod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + type, delta);",
            "",
            "\t/*",
            "\t * Count the following two cases as stalls:",
            "\t * 1. For pages accessed through page tables, hotter pages pushed out",
            "\t *    hot pages which refaulted immediately.",
            "\t * 2. For pages accessed multiple times through file descriptors,",
            "\t *    they would have been protected by sort_folio().",
            "\t */",
            "\tif (lru_gen_in_fault() || refs >= BIT(LRU_REFS_WIDTH) - 1) {",
            "\t\tset_mask_bits(&folio->flags, 0, LRU_REFS_MASK | BIT(PG_workingset));",
            "\t\tmod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + type, delta);",
            "\t}",
            "unlock:",
            "\trcu_read_unlock();",
            "}",
            "static bool lru_gen_test_recent(void *shadow, bool file, struct lruvec **lruvec,",
            "\t\t\t\tunsigned long *token, bool *workingset)",
            "{",
            "\treturn false;",
            "}",
            "static void lru_gen_refault(struct folio *folio, void *shadow)",
            "{",
            "}",
            "void workingset_age_nonresident(struct lruvec *lruvec, unsigned long nr_pages)",
            "{",
            "\t/*",
            "\t * Reclaiming a cgroup means reclaiming all its children in a",
            "\t * round-robin fashion. That means that each cgroup has an LRU",
            "\t * order that is composed of the LRU orders of its child",
            "\t * cgroups; and every page has an LRU position not just in the",
            "\t * cgroup that owns it, but in all of that group's ancestors.",
            "\t *",
            "\t * So when the physical inactive list of a leaf cgroup ages,",
            "\t * the virtual inactive lists of all its parents, including",
            "\t * the root cgroup's, age as well.",
            "\t */",
            "\tdo {",
            "\t\tatomic_long_add(nr_pages, &lruvec->nonresident_age);",
            "\t} while ((lruvec = parent_lruvec(lruvec)));",
            "}"
          ],
          "function_name": "unpack_shadow, lru_gen_test_recent, lru_gen_refault, lru_gen_test_recent, lru_gen_refault, workingset_age_nonresident",
          "description": "包含解码影子条目、测试最近访问、处理页面故障的函数实现，但存在重复函数声明问题，实际功能涉及基于LRU生成的页面激活决策逻辑。",
          "similarity": 0.49908214807510376
        },
        {
          "chunk_id": 0,
          "file_path": "mm/workingset.c",
          "start_line": 1,
          "end_line": 208,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Workingset detection",
            " *",
            " * Copyright (C) 2013 Red Hat, Inc., Johannes Weiner",
            " */",
            "",
            "#include <linux/memcontrol.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/writeback.h>",
            "#include <linux/shmem_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/atomic.h>",
            "#include <linux/module.h>",
            "#include <linux/swap.h>",
            "#include <linux/dax.h>",
            "#include <linux/fs.h>",
            "#include <linux/mm.h>",
            "",
            "/*",
            " *\t\tDouble CLOCK lists",
            " *",
            " * Per node, two clock lists are maintained for file pages: the",
            " * inactive and the active list.  Freshly faulted pages start out at",
            " * the head of the inactive list and page reclaim scans pages from the",
            " * tail.  Pages that are accessed multiple times on the inactive list",
            " * are promoted to the active list, to protect them from reclaim,",
            " * whereas active pages are demoted to the inactive list when the",
            " * active list grows too big.",
            " *",
            " *   fault ------------------------+",
            " *                                 |",
            " *              +--------------+   |            +-------------+",
            " *   reclaim <- |   inactive   | <-+-- demotion |    active   | <--+",
            " *              +--------------+                +-------------+    |",
            " *                     |                                           |",
            " *                     +-------------- promotion ------------------+",
            " *",
            " *",
            " *\t\tAccess frequency and refault distance",
            " *",
            " * A workload is thrashing when its pages are frequently used but they",
            " * are evicted from the inactive list every time before another access",
            " * would have promoted them to the active list.",
            " *",
            " * In cases where the average access distance between thrashing pages",
            " * is bigger than the size of memory there is nothing that can be",
            " * done - the thrashing set could never fit into memory under any",
            " * circumstance.",
            " *",
            " * However, the average access distance could be bigger than the",
            " * inactive list, yet smaller than the size of memory.  In this case,",
            " * the set could fit into memory if it weren't for the currently",
            " * active pages - which may be used more, hopefully less frequently:",
            " *",
            " *      +-memory available to cache-+",
            " *      |                           |",
            " *      +-inactive------+-active----+",
            " *  a b | c d e f g h i | J K L M N |",
            " *      +---------------+-----------+",
            " *",
            " * It is prohibitively expensive to accurately track access frequency",
            " * of pages.  But a reasonable approximation can be made to measure",
            " * thrashing on the inactive list, after which refaulting pages can be",
            " * activated optimistically to compete with the existing active pages.",
            " *",
            " * Approximating inactive page access frequency - Observations:",
            " *",
            " * 1. When a page is accessed for the first time, it is added to the",
            " *    head of the inactive list, slides every existing inactive page",
            " *    towards the tail by one slot, and pushes the current tail page",
            " *    out of memory.",
            " *",
            " * 2. When a page is accessed for the second time, it is promoted to",
            " *    the active list, shrinking the inactive list by one slot.  This",
            " *    also slides all inactive pages that were faulted into the cache",
            " *    more recently than the activated page towards the tail of the",
            " *    inactive list.",
            " *",
            " * Thus:",
            " *",
            " * 1. The sum of evictions and activations between any two points in",
            " *    time indicate the minimum number of inactive pages accessed in",
            " *    between.",
            " *",
            " * 2. Moving one inactive page N page slots towards the tail of the",
            " *    list requires at least N inactive page accesses.",
            " *",
            " * Combining these:",
            " *",
            " * 1. When a page is finally evicted from memory, the number of",
            " *    inactive pages accessed while the page was in cache is at least",
            " *    the number of page slots on the inactive list.",
            " *",
            " * 2. In addition, measuring the sum of evictions and activations (E)",
            " *    at the time of a page's eviction, and comparing it to another",
            " *    reading (R) at the time the page faults back into memory tells",
            " *    the minimum number of accesses while the page was not cached.",
            " *    This is called the refault distance.",
            " *",
            " * Because the first access of the page was the fault and the second",
            " * access the refault, we combine the in-cache distance with the",
            " * out-of-cache distance to get the complete minimum access distance",
            " * of this page:",
            " *",
            " *      NR_inactive + (R - E)",
            " *",
            " * And knowing the minimum access distance of a page, we can easily",
            " * tell if the page would be able to stay in cache assuming all page",
            " * slots in the cache were available:",
            " *",
            " *   NR_inactive + (R - E) <= NR_inactive + NR_active",
            " *",
            " * If we have swap we should consider about NR_inactive_anon and",
            " * NR_active_anon, so for page cache and anonymous respectively:",
            " *",
            " *   NR_inactive_file + (R - E) <= NR_inactive_file + NR_active_file",
            " *   + NR_inactive_anon + NR_active_anon",
            " *",
            " *   NR_inactive_anon + (R - E) <= NR_inactive_anon + NR_active_anon",
            " *   + NR_inactive_file + NR_active_file",
            " *",
            " * Which can be further simplified to:",
            " *",
            " *   (R - E) <= NR_active_file + NR_inactive_anon + NR_active_anon",
            " *",
            " *   (R - E) <= NR_active_anon + NR_inactive_file + NR_active_file",
            " *",
            " * Put into words, the refault distance (out-of-cache) can be seen as",
            " * a deficit in inactive list space (in-cache).  If the inactive list",
            " * had (R - E) more page slots, the page would not have been evicted",
            " * in between accesses, but activated instead.  And on a full system,",
            " * the only thing eating into inactive list space is active pages.",
            " *",
            " *",
            " *\t\tRefaulting inactive pages",
            " *",
            " * All that is known about the active list is that the pages have been",
            " * accessed more than once in the past.  This means that at any given",
            " * time there is actually a good chance that pages on the active list",
            " * are no longer in active use.",
            " *",
            " * So when a refault distance of (R - E) is observed and there are at",
            " * least (R - E) pages in the userspace workingset, the refaulting page",
            " * is activated optimistically in the hope that (R - E) pages are actually",
            " * used less frequently than the refaulting page - or even not used at",
            " * all anymore.",
            " *",
            " * That means if inactive cache is refaulting with a suitable refault",
            " * distance, we assume the cache workingset is transitioning and put",
            " * pressure on the current workingset.",
            " *",
            " * If this is wrong and demotion kicks in, the pages which are truly",
            " * used more frequently will be reactivated while the less frequently",
            " * used once will be evicted from memory.",
            " *",
            " * But if this is right, the stale pages will be pushed out of memory",
            " * and the used pages get to stay in cache.",
            " *",
            " *\t\tRefaulting active pages",
            " *",
            " * If on the other hand the refaulting pages have recently been",
            " * deactivated, it means that the active list is no longer protecting",
            " * actively used cache from reclaim. The cache is NOT transitioning to",
            " * a different workingset; the existing workingset is thrashing in the",
            " * space allocated to the page cache.",
            " *",
            " *",
            " *\t\tImplementation",
            " *",
            " * For each node's LRU lists, a counter for inactive evictions and",
            " * activations is maintained (node->nonresident_age).",
            " *",
            " * On eviction, a snapshot of this counter (along with some bits to",
            " * identify the node) is stored in the now empty page cache",
            " * slot of the evicted page.  This is called a shadow entry.",
            " *",
            " * On cache misses for which there are shadow entries, an eligible",
            " * refault distance will immediately activate the refaulting page.",
            " */",
            "",
            "#define WORKINGSET_SHIFT 1",
            "#define EVICTION_SHIFT\t((BITS_PER_LONG - BITS_PER_XA_VALUE) +\t\\",
            "\t\t\t WORKINGSET_SHIFT + NODES_SHIFT + \\",
            "\t\t\t MEM_CGROUP_ID_SHIFT)",
            "#define EVICTION_MASK\t(~0UL >> EVICTION_SHIFT)",
            "",
            "/*",
            " * Eviction timestamps need to be able to cover the full range of",
            " * actionable refaults. However, bits are tight in the xarray",
            " * entry, and after storing the identifier for the lruvec there might",
            " * not be enough left to represent every single actionable refault. In",
            " * that case, we have to sacrifice granularity for distance, and group",
            " * evictions into coarser buckets by shaving off lower timestamp bits.",
            " */",
            "static unsigned int bucket_order __read_mostly;",
            "",
            "static void *pack_shadow(int memcgid, pg_data_t *pgdat, unsigned long eviction,",
            "\t\t\t bool workingset)",
            "{",
            "\teviction &= EVICTION_MASK;",
            "\teviction = (eviction << MEM_CGROUP_ID_SHIFT) | memcgid;",
            "\teviction = (eviction << NODES_SHIFT) | pgdat->node_id;",
            "\teviction = (eviction << WORKINGSET_SHIFT) | workingset;",
            "",
            "\treturn xa_mk_value(eviction);",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义影子条目打包函数，将内存组ID、节点ID、时间戳等信息编码到xarray值中，用于记录页面被驱逐时的状态信息。",
          "similarity": 0.40883851051330566
        }
      ]
    },
    {
      "source_file": "mm/swap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:25:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `swap.c`\n\n---\n\n# swap.c 技术文档\n\n## 1. 文件概述\n\n`swap.c` 是 Linux 内核内存管理子系统（MM）中的核心文件之一，主要负责页面回收（page reclaim）、LRU（Least Recently Used）链表管理、页面释放以及与交换（swap）机制相关的底层支持逻辑。尽管文件名为 `swap.c`，但其功能不仅限于交换，而是涵盖了通用的页面生命周期管理、LRU 链表操作、页面引用计数释放、可回收性判断等关键内存管理任务。该文件为页面缓存（page cache）、匿名页（anonymous pages）和大页（huge pages）提供统一的释放与 LRU 管理接口。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `page_cluster`：控制一次 I/O 操作中尝试换入/换出的页面数量（以 2 的幂表示），默认值由系统配置决定。\n- `page_cluster_max`：`page_cluster` 的最大允许值（31，即最多 2^31 页，实际受架构限制）。\n\n### 主要数据结构\n- `struct lru_rotate`：每个 CPU 私有的结构，用于在中断禁用上下文中批量处理需移至 LRU 链表尾部的页面（如 `folio_rotate_reclaimable` 场景）。\n- `struct cpu_fbatches`：每个 CPU 私有的 folio 批处理结构，包含多个 folio_batch，用于高效地向 LRU 链表添加、停用或激活页面，避免频繁获取 LRU 锁。\n\n### 主要函数\n- `__folio_put()`：释放一个 folio 的核心函数，根据 folio 类型（设备内存、大页、普通页）调用相应的释放路径。\n- `put_pages_list()`：批量释放通过 `lru` 字段链接的页面列表，常用于网络子系统或 compound page 释放。\n- `lru_add_fn()`：将 folio 添加到对应 LRU 链表的回调函数，处理可回收性（evictable/unevictable）状态转换和统计计数。\n- `folio_batch_move_lru()`：批量执行 LRU 操作（如添加、移动），在持有 LRU 锁期间完成所有 folio 的处理。\n- `folio_rotate_reclaimable()`：在写回完成后，若页面仍可回收，则将其移至 inactive LRU 链表尾部，以延迟其被回收的时间。\n- `lru_note_cost()`：记录 LRU 扫描过程中的 I/O 和旋转（rotation）成本，用于后续调整 anon/file LRU 的扫描比例。\n\n## 3. 关键实现\n\n### LRU 批处理机制\n为减少 LRU 锁竞争，内核采用 per-CPU 批处理（`folio_batch`）方式暂存待处理的 folio。当批处理满或遇到大页（`folio_test_large`）时，才批量获取 LRU 锁并执行操作（如 `lru_add_fn`）。这显著提升了高并发场景下的性能。\n\n### 可回收性管理\n页面是否可回收由 `folio_evictable()` 判断，主要依据是否被 mlock 锁定。在添加到 LRU 时：\n- 若页面变为可回收（原为 unevictable），则增加 `UNEVICTABLE_PGRESCUED` 统计；\n- 若页面不可回收，则清除 active 标志，设置 unevictable 标志，并重置 `mlock_count`，同时增加 `UNEVICTABLE_PGCULLED` 统计。\n\n### 页面释放路径\n`__folio_put()` 是 folio 引用计数归零后的释放入口：\n1. 设备内存 folio 调用 `free_zone_device_folio()`\n2. 大页 folio 调用 `free_huge_folio()`\n3. 普通 folio 先从 LRU 移除（若在 LRU 上），然后解绑内存控制组（memcg），最后调用 `free_unref_page()` 释放到伙伴系统。\n\n### LRU 旋转优化\n`folio_rotate_reclaimable()` 在写回结束时，若页面干净且未锁定，则将其移至 inactive LRU 尾部。此操作通过 per-CPU 的 `lru_rotate` 批处理完成，仅在必要时获取 LRU 锁，避免影响写回关键路径性能。\n\n### 成本跟踪\n`lru_note_cost()` 通过累加 `nr_io * SWAP_CLUSTER_MAX + nr_rotated` 来量化扫描成本，用于动态调整匿名页与文件页 LRU 的扫描比例，优化内存回收效率。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/pagevec.h>`、`\"internal.h\"` 等，使用伙伴系统、LRU 框架、内存控制组（memcg）等基础组件。\n- **交换子系统**：虽不直接实现 swap read/write，但为 `vmscan.c` 中的页面回收提供 LRU 操作接口，是 swap 机制的支撑模块。\n- **大页支持**：通过 `hugetlb.h` 与大页子系统交互，特殊处理大页释放。\n- **设备内存**：通过 `memremap.h` 支持持久内存（pmem）等 zone device 页面的释放。\n- **跟踪与统计**：使用 tracepoint（`trace/events/pagemap.h`）和 VM 统计（`kernel_stat.h`）进行性能分析。\n- **SMP 支持**：大量使用 per-CPU 变量（`DEFINE_PER_CPU`）和本地锁（`local_lock_t`）优化多核性能。\n\n## 5. 使用场景\n\n- **页面回收（Reclaim）**：当内存压力触发 kswapd 或 direct reclaim 时，`vmscan.c` 调用本文件的 LRU 操作函数来隔离、释放页面。\n- **页面缓存释放**：文件系统或网络子系统在释放 page cache 页面时，通过 `__folio_put()` 或 `put_pages_list()` 触发 LRU 移除和内存释放。\n- **写回完成处理**：块设备或文件系统在完成脏页写回后，调用 `folio_rotate_reclaimable()` 更新页面在 LRU 中的位置。\n- **内存控制组（cgroup）**：memcg 回收内存时，复用本文件的 LRU 批处理和 folio 释放逻辑。\n- **大页与设备内存管理**：透明大页（THP）或持久内存应用释放页面时，通过统一的 `__folio_put()` 接口分发到专用释放函数。\n- **系统调优**：管理员通过 `/proc/sys/vm/page-cluster` 调整 `page_cluster` 值，影响 swap 和 page cache 的 I/O 批量大小。",
      "similarity": 0.5514246225357056,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/swap.c",
          "start_line": 77,
          "end_line": 190,
          "content": [
            "static void __page_cache_release(struct folio *folio, struct lruvec **lruvecp,",
            "\t\tunsigned long *flagsp)",
            "{",
            "\tif (folio_test_lru(folio)) {",
            "\t\tfolio_lruvec_relock_irqsave(folio, lruvecp, flagsp);",
            "\t\tlruvec_del_folio(*lruvecp, folio);",
            "\t\t__folio_clear_lru_flags(folio);",
            "\t}",
            "}",
            "static void page_cache_release(struct folio *folio)",
            "{",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags;",
            "",
            "\t__page_cache_release(folio, &lruvec, &flags);",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "}",
            "void __folio_put(struct folio *folio)",
            "{",
            "\tif (unlikely(folio_is_zone_device(folio))) {",
            "\t\tfree_zone_device_folio(folio);",
            "\t\treturn;",
            "\t} else if (folio_test_hugetlb(folio)) {",
            "\t\tfree_huge_folio(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tpage_cache_release(folio);",
            "\tfolio_unqueue_deferred_split(folio);",
            "\tmem_cgroup_uncharge(folio);",
            "\tfree_unref_page(&folio->page, folio_order(folio));",
            "}",
            "void put_pages_list(struct list_head *pages)",
            "{",
            "\tstruct folio_batch fbatch;",
            "\tstruct folio *folio, *next;",
            "",
            "\tfolio_batch_init(&fbatch);",
            "\tlist_for_each_entry_safe(folio, next, pages, lru) {",
            "\t\tif (!folio_put_testzero(folio))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_hugetlb(folio)) {",
            "\t\t\tfree_huge_folio(folio);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\t/* LRU flag must be clear because it's passed using the lru */",
            "\t\tif (folio_batch_add(&fbatch, folio) > 0)",
            "\t\t\tcontinue;",
            "\t\tfree_unref_folios(&fbatch);",
            "\t}",
            "",
            "\tif (fbatch.nr)",
            "\t\tfree_unref_folios(&fbatch);",
            "\tINIT_LIST_HEAD(pages);",
            "}",
            "static void lru_add_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tint was_unevictable = folio_test_clear_unevictable(folio);",
            "\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\t/*",
            "\t * Is an smp_mb__after_atomic() still required here, before",
            "\t * folio_evictable() tests the mlocked flag, to rule out the possibility",
            "\t * of stranding an evictable folio on an unevictable LRU?  I think",
            "\t * not, because __munlock_folio() only clears the mlocked flag",
            "\t * while the LRU lock is held.",
            "\t *",
            "\t * (That is not true of __page_cache_release(), and not necessarily",
            "\t * true of folios_put(): but those only clear the mlocked flag after",
            "\t * folio_put_testzero() has excluded any other users of the folio.)",
            "\t */",
            "\tif (folio_evictable(folio)) {",
            "\t\tif (was_unevictable)",
            "\t\t\t__count_vm_events(UNEVICTABLE_PGRESCUED, nr_pages);",
            "\t} else {",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_set_unevictable(folio);",
            "\t\t/*",
            "\t\t * folio->mlock_count = !!folio_test_mlocked(folio)?",
            "\t\t * But that leaves __mlock_folio() in doubt whether another",
            "\t\t * actor has already counted the mlock or not.  Err on the",
            "\t\t * safe side, underestimate, let page reclaim fix it, rather",
            "\t\t * than leaving a page on the unevictable LRU indefinitely.",
            "\t\t */",
            "\t\tfolio->mlock_count = 0;",
            "\t\tif (!was_unevictable)",
            "\t\t\t__count_vm_events(UNEVICTABLE_PGCULLED, nr_pages);",
            "\t}",
            "",
            "\tlruvec_add_folio(lruvec, folio);",
            "\ttrace_mm_lru_insertion(folio);",
            "}",
            "static void folio_batch_move_lru(struct folio_batch *fbatch, move_fn_t move_fn)",
            "{",
            "\tint i;",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags = 0;",
            "",
            "\tfor (i = 0; i < folio_batch_count(fbatch); i++) {",
            "\t\tstruct folio *folio = fbatch->folios[i];",
            "",
            "\t\tfolio_lruvec_relock_irqsave(folio, &lruvec, &flags);",
            "\t\tmove_fn(lruvec, folio);",
            "",
            "\t\tfolio_set_lru(folio);",
            "\t}",
            "",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\tfolios_put(fbatch);",
            "}"
          ],
          "function_name": "__page_cache_release, page_cache_release, __folio_put, put_pages_list, lru_add_fn, folio_batch_move_lru",
          "description": "实现了页面缓存释放和LRU列表维护逻辑，包含__page_cache_release用于从LRU列表移除页面，page_cache_release处理普通页面释放流程，__folio_put负责释放非设备映射和大页，put_pages_list批量处理页面释放，lru_add_fn将页面添加到LRU列表并根据是否可交换设置相应标志。",
          "similarity": 0.542369544506073
        },
        {
          "chunk_id": 4,
          "file_path": "mm/swap.c",
          "start_line": 496,
          "end_line": 600,
          "content": [
            "void folio_add_lru(struct folio *folio)",
            "{",
            "\tstruct folio_batch *fbatch;",
            "",
            "\tVM_BUG_ON_FOLIO(folio_test_active(folio) &&",
            "\t\t\tfolio_test_unevictable(folio), folio);",
            "\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\t/* see the comment in lru_gen_add_folio() */",
            "\tif (lru_gen_enabled() && !folio_test_unevictable(folio) &&",
            "\t    lru_gen_in_fault() && !(current->flags & PF_MEMALLOC))",
            "\t\tfolio_set_active(folio);",
            "",
            "\tfolio_get(folio);",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_add);",
            "\tfolio_batch_add_and_move(fbatch, folio, lru_add_fn);",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "}",
            "void folio_add_lru_vma(struct folio *folio, struct vm_area_struct *vma)",
            "{",
            "\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\tif (unlikely((vma->vm_flags & (VM_LOCKED | VM_SPECIAL)) == VM_LOCKED))",
            "\t\tmlock_new_folio(folio);",
            "\telse",
            "\t\tfolio_add_lru(folio);",
            "}",
            "static void lru_deactivate_file_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tbool active = folio_test_active(folio);",
            "\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\tif (folio_test_unevictable(folio))",
            "\t\treturn;",
            "",
            "\t/* Some processes are using the folio */",
            "\tif (folio_mapped(folio))",
            "\t\treturn;",
            "",
            "\tlruvec_del_folio(lruvec, folio);",
            "\tfolio_clear_active(folio);",
            "\tfolio_clear_referenced(folio);",
            "",
            "\tif (folio_test_writeback(folio) || folio_test_dirty(folio)) {",
            "\t\t/*",
            "\t\t * Setting the reclaim flag could race with",
            "\t\t * folio_end_writeback() and confuse readahead.  But the",
            "\t\t * race window is _really_ small and  it's not a critical",
            "\t\t * problem.",
            "\t\t */",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "\t\tfolio_set_reclaim(folio);",
            "\t} else {",
            "\t\t/*",
            "\t\t * The folio's writeback ended while it was in the batch.",
            "\t\t * We move that folio to the tail of the inactive list.",
            "\t\t */",
            "\t\tlruvec_add_folio_tail(lruvec, folio);",
            "\t\t__count_vm_events(PGROTATED, nr_pages);",
            "\t}",
            "",
            "\tif (active) {",
            "\t\t__count_vm_events(PGDEACTIVATE, nr_pages);",
            "\t\t__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE,",
            "\t\t\t\t     nr_pages);",
            "\t}",
            "}",
            "static void lru_deactivate_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tif (!folio_test_unevictable(folio) && (folio_test_active(folio) || lru_gen_enabled())) {",
            "\t\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_clear_referenced(folio);",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "",
            "\t\t__count_vm_events(PGDEACTIVATE, nr_pages);",
            "\t\t__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE,",
            "\t\t\t\t     nr_pages);",
            "\t}",
            "}",
            "static void lru_lazyfree_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tif (folio_test_anon(folio) && folio_test_swapbacked(folio) &&",
            "\t    !folio_test_swapcache(folio) && !folio_test_unevictable(folio)) {",
            "\t\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_clear_referenced(folio);",
            "\t\t/*",
            "\t\t * Lazyfree folios are clean anonymous folios.  They have",
            "\t\t * the swapbacked flag cleared, to distinguish them from normal",
            "\t\t * anonymous folios",
            "\t\t */",
            "\t\tfolio_clear_swapbacked(folio);",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "",
            "\t\t__count_vm_events(PGLAZYFREE, nr_pages);",
            "\t\t__count_memcg_events(lruvec_memcg(lruvec), PGLAZYFREE,",
            "\t\t\t\t     nr_pages);",
            "\t}",
            "}"
          ],
          "function_name": "folio_add_lru, folio_add_lru_vma, lru_deactivate_file_fn, lru_deactivate_fn, lru_lazyfree_fn",
          "description": "包含页面LRU列表插入和状态转换逻辑，folio_add_lru将页面加入LRU列表，folio_add_lru_vma处理VMA特定的页面添加，lru_deactivate_file_fn和lru_deactivate_fn处理文件页去激活操作，lru_lazyfree_fn处理延迟释放的匿名页面，均通过统一接口修改页面状态并触发统计事件。",
          "similarity": 0.5329023599624634
        },
        {
          "chunk_id": 8,
          "file_path": "mm/swap.c",
          "start_line": 1079,
          "end_line": 1111,
          "content": [
            "void __folio_batch_release(struct folio_batch *fbatch)",
            "{",
            "\tif (!fbatch->percpu_pvec_drained) {",
            "\t\tlru_add_drain();",
            "\t\tfbatch->percpu_pvec_drained = true;",
            "\t}",
            "\tfolios_put(fbatch);",
            "}",
            "void folio_batch_remove_exceptionals(struct folio_batch *fbatch)",
            "{",
            "\tunsigned int i, j;",
            "",
            "\tfor (i = 0, j = 0; i < folio_batch_count(fbatch); i++) {",
            "\t\tstruct folio *folio = fbatch->folios[i];",
            "\t\tif (!xa_is_value(folio))",
            "\t\t\tfbatch->folios[j++] = folio;",
            "\t}",
            "\tfbatch->nr = j;",
            "}",
            "void __init swap_setup(void)",
            "{",
            "\tunsigned long megs = totalram_pages() >> (20 - PAGE_SHIFT);",
            "",
            "\t/* Use a smaller cluster for small-memory machines */",
            "\tif (megs < 16)",
            "\t\tpage_cluster = 2;",
            "\telse",
            "\t\tpage_cluster = 3;",
            "\t/*",
            "\t * Right now other parts of the system means that we",
            "\t * _really_ don't want to cluster much more",
            "\t */",
            "}"
          ],
          "function_name": "__folio_batch_release, folio_batch_remove_exceptionals, swap_setup",
          "description": "__folio_batch_release 标记并释放页面批次引用，folio_batch_remove_exceptionals 清理异常条目；swap_setup 初始化页面聚类参数，根据内存大小调整page_cluster值。",
          "similarity": 0.5271489024162292
        },
        {
          "chunk_id": 0,
          "file_path": "mm/swap.c",
          "start_line": 1,
          "end_line": 76,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  linux/mm/swap.c",
            " *",
            " *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds",
            " */",
            "",
            "/*",
            " * This file contains the default values for the operation of the",
            " * Linux VM subsystem. Fine-tuning documentation can be found in",
            " * Documentation/admin-guide/sysctl/vm.rst.",
            " * Started 18.12.91",
            " * Swap aging added 23.2.95, Stephen Tweedie.",
            " * Buffermem limits added 12.3.98, Rik van Riel.",
            " */",
            "",
            "#include <linux/mm.h>",
            "#include <linux/sched.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/pagevec.h>",
            "#include <linux/init.h>",
            "#include <linux/export.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/percpu_counter.h>",
            "#include <linux/memremap.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/gfp.h>",
            "#include <linux/uio.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/page_idle.h>",
            "#include <linux/local_lock.h>",
            "#include <linux/buffer_head.h>",
            "",
            "#include \"internal.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/pagemap.h>",
            "",
            "/* How many pages do we try to swap or page in/out together? As a power of 2 */",
            "int page_cluster;",
            "const int page_cluster_max = 31;",
            "",
            "/* Protecting only lru_rotate.fbatch which requires disabling interrupts */",
            "struct lru_rotate {",
            "\tlocal_lock_t lock;",
            "\tstruct folio_batch fbatch;",
            "};",
            "static DEFINE_PER_CPU(struct lru_rotate, lru_rotate) = {",
            "\t.lock = INIT_LOCAL_LOCK(lock),",
            "};",
            "",
            "/*",
            " * The following folio batches are grouped together because they are protected",
            " * by disabling preemption (and interrupts remain enabled).",
            " */",
            "struct cpu_fbatches {",
            "\tlocal_lock_t lock;",
            "\tstruct folio_batch lru_add;",
            "\tstruct folio_batch lru_deactivate_file;",
            "\tstruct folio_batch lru_deactivate;",
            "\tstruct folio_batch lru_lazyfree;",
            "#ifdef CONFIG_SMP",
            "\tstruct folio_batch activate;",
            "#endif",
            "};",
            "static DEFINE_PER_CPU(struct cpu_fbatches, cpu_fbatches) = {",
            "\t.lock = INIT_LOCAL_LOCK(lock),",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义了与Linux虚拟内存子系统默认操作相关的全局变量和结构体，包括控制页面集群大小的page_cluster参数，以及用于保护LRU旋转操作的本地锁结构lru_rotate和CPU级的folio批次结构cpu_fbatches，用于管理不同场景下的页面批量操作。",
          "similarity": 0.5143914222717285
        },
        {
          "chunk_id": 2,
          "file_path": "mm/swap.c",
          "start_line": 211,
          "end_line": 317,
          "content": [
            "static void folio_batch_add_and_move(struct folio_batch *fbatch,",
            "\t\tstruct folio *folio, move_fn_t move_fn)",
            "{",
            "\tif (folio_batch_add(fbatch, folio) && !folio_test_large(folio) &&",
            "\t    !lru_cache_disabled())",
            "\t\treturn;",
            "\tfolio_batch_move_lru(fbatch, move_fn);",
            "}",
            "static void lru_move_tail_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tif (!folio_test_unevictable(folio)) {",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tfolio_clear_active(folio);",
            "\t\tlruvec_add_folio_tail(lruvec, folio);",
            "\t\t__count_vm_events(PGROTATED, folio_nr_pages(folio));",
            "\t}",
            "}",
            "void folio_rotate_reclaimable(struct folio *folio)",
            "{",
            "\tif (!folio_test_locked(folio) && !folio_test_dirty(folio) &&",
            "\t    !folio_test_unevictable(folio)) {",
            "\t\tstruct folio_batch *fbatch;",
            "\t\tunsigned long flags;",
            "",
            "\t\tfolio_get(folio);",
            "\t\tif (!folio_test_clear_lru(folio)) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tlocal_lock_irqsave(&lru_rotate.lock, flags);",
            "\t\tfbatch = this_cpu_ptr(&lru_rotate.fbatch);",
            "\t\tfolio_batch_add_and_move(fbatch, folio, lru_move_tail_fn);",
            "\t\tlocal_unlock_irqrestore(&lru_rotate.lock, flags);",
            "\t}",
            "}",
            "void lru_note_cost(struct lruvec *lruvec, bool file,",
            "\t\t   unsigned int nr_io, unsigned int nr_rotated)",
            "{",
            "\tunsigned long cost;",
            "",
            "\t/*",
            "\t * Reflect the relative cost of incurring IO and spending CPU",
            "\t * time on rotations. This doesn't attempt to make a precise",
            "\t * comparison, it just says: if reloads are about comparable",
            "\t * between the LRU lists, or rotations are overwhelmingly",
            "\t * different between them, adjust scan balance for CPU work.",
            "\t */",
            "\tcost = nr_io * SWAP_CLUSTER_MAX + nr_rotated;",
            "",
            "\tdo {",
            "\t\tunsigned long lrusize;",
            "",
            "\t\t/*",
            "\t\t * Hold lruvec->lru_lock is safe here, since",
            "\t\t * 1) The pinned lruvec in reclaim, or",
            "\t\t * 2) From a pre-LRU page during refault (which also holds the",
            "\t\t *    rcu lock, so would be safe even if the page was on the LRU",
            "\t\t *    and could move simultaneously to a new lruvec).",
            "\t\t */",
            "\t\tspin_lock_irq(&lruvec->lru_lock);",
            "\t\t/* Record cost event */",
            "\t\tif (file)",
            "\t\t\tlruvec->file_cost += cost;",
            "\t\telse",
            "\t\t\tlruvec->anon_cost += cost;",
            "",
            "\t\t/*",
            "\t\t * Decay previous events",
            "\t\t *",
            "\t\t * Because workloads change over time (and to avoid",
            "\t\t * overflow) we keep these statistics as a floating",
            "\t\t * average, which ends up weighing recent refaults",
            "\t\t * more than old ones.",
            "\t\t */",
            "\t\tlrusize = lruvec_page_state(lruvec, NR_INACTIVE_ANON) +",
            "\t\t\t  lruvec_page_state(lruvec, NR_ACTIVE_ANON) +",
            "\t\t\t  lruvec_page_state(lruvec, NR_INACTIVE_FILE) +",
            "\t\t\t  lruvec_page_state(lruvec, NR_ACTIVE_FILE);",
            "",
            "\t\tif (lruvec->file_cost + lruvec->anon_cost > lrusize / 4) {",
            "\t\t\tlruvec->file_cost /= 2;",
            "\t\t\tlruvec->anon_cost /= 2;",
            "\t\t}",
            "\t\tspin_unlock_irq(&lruvec->lru_lock);",
            "\t} while ((lruvec = parent_lruvec(lruvec)));",
            "}",
            "void lru_note_cost_refault(struct folio *folio)",
            "{",
            "\tlru_note_cost(folio_lruvec(folio), folio_is_file_lru(folio),",
            "\t\t      folio_nr_pages(folio), 0);",
            "}",
            "static void folio_activate_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tif (!folio_test_active(folio) && !folio_test_unevictable(folio)) {",
            "\t\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tfolio_set_active(folio);",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "\t\ttrace_mm_lru_activate(folio);",
            "",
            "\t\t__count_vm_events(PGACTIVATE, nr_pages);",
            "\t\t__count_memcg_events(lruvec_memcg(lruvec), PGACTIVATE,",
            "\t\t\t\t     nr_pages);",
            "\t}",
            "}"
          ],
          "function_name": "folio_batch_add_and_move, lru_move_tail_fn, folio_rotate_reclaimable, lru_note_cost, lru_note_cost_refault, folio_activate_fn",
          "description": "提供LRU列表页面移动和成本统计功能，folio_batch_add_and_move处理页面批量移动，lru_move_tail_fn将页面移动到LRU尾部，folio_rotate_reclaimable将可回收页面转移到冷列表，lru_note_cost记录页面访问成本用于调整扫描策略，folio_activate_fn激活页面至活动列表。",
          "similarity": 0.5029963850975037
        }
      ]
    },
    {
      "source_file": "mm/balloon_compaction.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:41:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `balloon_compaction.c`\n\n---\n\n# balloon_compaction.c 技术文档\n\n## 1. 文件概述\n\n`balloon_compaction.c` 是 Linux 内核中用于支持内存气球（Memory Ballooning）机制与内存压缩（Compaction）协同工作的核心模块。该文件提供了通用接口，使得由气球驱动程序管理的页面可以被内存压缩子系统识别为可迁移（movable），从而在内存碎片整理过程中安全地移动这些页面，提升高阶内存分配的成功率。此机制主要用于虚拟化环境中，允许宿主机动态调整客户机（Guest）的可用内存。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`balloon_page_alloc()`**  \n  分配一个新的页面，专用于加入气球页面列表。使用特殊的 GFP 标志（如 `__GFP_NOMEMALLOC`, `__GFP_NORETRY`, `__GFP_NOWARN`）以避免在内存压力下触发 OOM 或重试。\n\n- **`balloon_page_enqueue()`**  \n  将单个通过 `balloon_page_alloc()` 分配的页面插入到指定气球设备的页面列表中，并增加 `BALLOON_INFLATE` 统计计数。\n\n- **`balloon_page_list_enqueue()`**  \n  批量将一个页面链表中的所有页面插入到气球设备的页面列表中，适用于高效批量操作。\n\n- **`balloon_page_dequeue()`**  \n  从气球设备的页面列表中移除并返回一个页面，供驱动释放回系统。若无法出队且无孤立页面，则触发 `BUG()` 防止死循环。\n\n- **`balloon_page_list_dequeue()`**  \n  批量从气球设备中取出最多 `n_req_pages` 个页面，放入调用者提供的链表中，用于批量释放。\n\n- **`balloon_page_isolate()`**（仅当 `CONFIG_BALLOON_COMPACTION` 启用）  \n  在内存压缩过程中，将气球页面从主列表中隔离，防止并发访问，并增加 `isolated_pages` 计数。\n\n- **`balloon_page_putback()`**（仅当 `CONFIG_BALLOON_COMPACTION` 启用）  \n  将被隔离的气球页面重新放回主页面列表，并减少 `isolated_pages` 计数。\n\n- **`balloon_page_migrate()`**（仅当 `CONFIG_BALLOON_COMPACTION` 启用）  \n  实现气球页面的迁移逻辑，作为内存压缩中 `move_to_new_page()` 的对应处理函数（代码片段未完整）。\n\n### 关键数据结构\n\n- **`struct balloon_dev_info`**  \n  气球设备信息结构体，包含：\n  - `pages`：已入队的气球页面链表\n  - `pages_lock`：保护页面列表的自旋锁\n  - `isolated_pages`：当前被压缩子系统隔离的页面数量（仅在 `CONFIG_BALLOON_COMPACTION` 下使用）\n\n## 3. 关键实现\n\n- **线程安全与并发控制**  \n  所有对 `balloon_dev_info->pages` 链表的操作均受 `pages_lock` 自旋锁保护，并在中断禁用上下文中执行（`spin_lock_irqsave`），确保在高并发或中断上下文中的安全性。\n\n- **页面锁定机制**  \n  在入队和出队时使用 `trylock_page()` 确保当前是唯一持有页面引用的实体。若加锁失败，说明存在并发访问，可能意味着内存损坏或状态不一致，此时会跳过或报错。\n\n- **与内存压缩集成**  \n  当启用 `CONFIG_BALLOON_COMPACTION` 时，气球页面可通过 `PageIsolated()` 标志被识别为正在被压缩子系统处理。出队操作会跳过这些页面，避免破坏压缩流程。\n\n- **统计计数**  \n  使用 `__count_vm_event(BALLOON_INFLATE)` 和 `__count_vm_event(BALLOON_DEFLATE)` 跟踪气球膨胀/收缩操作次数，便于性能监控和调试。\n\n- **错误检测与防御性编程**  \n  在 `balloon_page_dequeue()` 中，若页面列表为空且无孤立页面，说明页面丢失，触发 `BUG()` 以防止驱动陷入无限循环。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/mm.h>`：内存管理基础接口\n  - `<linux/slab.h>`：内存分配\n  - `<linux/balloon_compaction.h>`：气球压缩相关声明（如 `balloon_page_insert`, `balloon_page_delete`, `balloon_page_device` 等）\n\n- **内核配置依赖**：\n  - `CONFIG_MEMORY_BALLOONING`：启用内存气球机制\n  - `CONFIG_BALLOON_COMPACTION`：启用气球页面的可压缩支持（条件编译）\n\n- **与其他子系统交互**：\n  - **内存压缩子系统（mm/compaction.c）**：通过注册的 `isolate` / `migrate` 回调函数参与页面迁移\n  - **虚拟化驱动（如 virtio_balloon）**：作为使用者调用本模块提供的 enqueue/dequeue 接口管理气球内存\n\n## 5. 使用场景\n\n- **虚拟化环境中的内存动态调整**  \n  客户机操作系统通过气球驱动（如 `virtio_balloon`）向宿主机“归还”内存时，调用 `balloon_page_alloc()` + `balloon_page_enqueue()` 将页面加入气球列表；当宿主机释放内存给客户机时，驱动调用 `balloon_page_dequeue()` 获取页面并释放回 buddy allocator。\n\n- **高阶内存分配优化**  \n  当系统需要大块连续物理内存（如透明大页 THP）但存在碎片时，内存压缩子系统会尝试迁移可移动页面。气球页面因本模块支持而被视为可移动，从而被安全迁移，帮助形成连续内存区域。\n\n- **内存热插拔与 NUMA 迁移**  \n  在 NUMA 节点间迁移内存或热移除内存区域时，气球页面可被压缩机制迁移，提高操作成功率。\n\n- **OOM 避免与内存回收**  \n  气球机制本身是一种主动内存回收手段，配合压缩可进一步提升内存利用率，减少 OOM 发生概率。",
      "similarity": 0.5467194318771362,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/balloon_compaction.c",
          "start_line": 14,
          "end_line": 129,
          "content": [
            "static void balloon_page_enqueue_one(struct balloon_dev_info *b_dev_info,",
            "\t\t\t\t     struct page *page)",
            "{",
            "\t/*",
            "\t * Block others from accessing the 'page' when we get around to",
            "\t * establishing additional references. We should be the only one",
            "\t * holding a reference to the 'page' at this point. If we are not, then",
            "\t * memory corruption is possible and we should stop execution.",
            "\t */",
            "\tBUG_ON(!trylock_page(page));",
            "\tballoon_page_insert(b_dev_info, page);",
            "\tunlock_page(page);",
            "\t__count_vm_event(BALLOON_INFLATE);",
            "}",
            "size_t balloon_page_list_enqueue(struct balloon_dev_info *b_dev_info,",
            "\t\t\t\t struct list_head *pages)",
            "{",
            "\tstruct page *page, *tmp;",
            "\tunsigned long flags;",
            "\tsize_t n_pages = 0;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tlist_for_each_entry_safe(page, tmp, pages, lru) {",
            "\t\tlist_del(&page->lru);",
            "\t\tballoon_page_enqueue_one(b_dev_info, page);",
            "\t\tn_pages++;",
            "\t}",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "\treturn n_pages;",
            "}",
            "size_t balloon_page_list_dequeue(struct balloon_dev_info *b_dev_info,",
            "\t\t\t\t struct list_head *pages, size_t n_req_pages)",
            "{",
            "\tstruct page *page, *tmp;",
            "\tunsigned long flags;",
            "\tsize_t n_pages = 0;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tlist_for_each_entry_safe(page, tmp, &b_dev_info->pages, lru) {",
            "\t\tif (n_pages == n_req_pages)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * Block others from accessing the 'page' while we get around to",
            "\t\t * establishing additional references and preparing the 'page'",
            "\t\t * to be released by the balloon driver.",
            "\t\t */",
            "\t\tif (!trylock_page(page))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (IS_ENABLED(CONFIG_BALLOON_COMPACTION) &&",
            "\t\t    PageIsolated(page)) {",
            "\t\t\t/* raced with isolation */",
            "\t\t\tunlock_page(page);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tballoon_page_delete(page);",
            "\t\t__count_vm_event(BALLOON_DEFLATE);",
            "\t\tlist_add(&page->lru, pages);",
            "\t\tunlock_page(page);",
            "\t\tn_pages++;",
            "\t}",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "",
            "\treturn n_pages;",
            "}",
            "void balloon_page_enqueue(struct balloon_dev_info *b_dev_info,",
            "\t\t\t  struct page *page)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tballoon_page_enqueue_one(b_dev_info, page);",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "}",
            "static bool balloon_page_isolate(struct page *page, isolate_mode_t mode)",
            "",
            "{",
            "\tstruct balloon_dev_info *b_dev_info = balloon_page_device(page);",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tlist_del(&page->lru);",
            "\tb_dev_info->isolated_pages++;",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "",
            "\treturn true;",
            "}",
            "static void balloon_page_putback(struct page *page)",
            "{",
            "\tstruct balloon_dev_info *b_dev_info = balloon_page_device(page);",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tlist_add(&page->lru, &b_dev_info->pages);",
            "\tb_dev_info->isolated_pages--;",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "}",
            "static int balloon_page_migrate(struct page *newpage, struct page *page,",
            "\t\tenum migrate_mode mode)",
            "{",
            "\tstruct balloon_dev_info *balloon = balloon_page_device(page);",
            "",
            "\t/*",
            "\t * We can not easily support the no copy case here so ignore it as it",
            "\t * is unlikely to be used with balloon pages. See include/linux/hmm.h",
            "\t * for a user of the MIGRATE_SYNC_NO_COPY mode.",
            "\t */",
            "\tif (mode == MIGRATE_SYNC_NO_COPY)",
            "\t\treturn -EINVAL;",
            "",
            "\tVM_BUG_ON_PAGE(!PageLocked(page), page);",
            "\tVM_BUG_ON_PAGE(!PageLocked(newpage), newpage);",
            "",
            "\treturn balloon->migratepage(balloon, newpage, page, mode);",
            "}"
          ],
          "function_name": "balloon_page_enqueue_one, balloon_page_list_enqueue, balloon_page_list_dequeue, balloon_page_enqueue, balloon_page_isolate, balloon_page_putback, balloon_page_migrate",
          "description": "提供气球内存页的并发控制及迁移管理，通过自旋锁保护页表操作，实现页面在LRU链表间的移动、隔离和迁移，支持气球膨胀/收缩事件统计",
          "similarity": 0.501347541809082
        },
        {
          "chunk_id": 0,
          "file_path": "mm/balloon_compaction.c",
          "start_line": 1,
          "end_line": 13,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * mm/balloon_compaction.c",
            " *",
            " * Common interface for making balloon pages movable by compaction.",
            " *",
            " * Copyright (C) 2012, Red Hat, Inc.  Rafael Aquini <aquini@redhat.com>",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/slab.h>",
            "#include <linux/export.h>",
            "#include <linux/balloon_compaction.h>",
            ""
          ],
          "function_name": null,
          "description": "上下文不完整",
          "similarity": 0.4215412735939026
        }
      ]
    }
  ]
}