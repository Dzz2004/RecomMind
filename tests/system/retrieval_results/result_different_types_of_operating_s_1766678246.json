{
  "query": "different types of operating systems",
  "timestamp": "2025-12-25 23:57:26",
  "retrieved_files": [
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5317748188972473,
      "chunks": [
        {
          "chunk_id": 27,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4895,
          "end_line": 5082,
          "content": [
            "static void __init rcu_init_one(void)",
            "{",
            "\tstatic const char * const buf[] = RCU_NODE_NAME_INIT;",
            "\tstatic const char * const fqs[] = RCU_FQS_NAME_INIT;",
            "\tstatic struct lock_class_key rcu_node_class[RCU_NUM_LVLS];",
            "\tstatic struct lock_class_key rcu_fqs_class[RCU_NUM_LVLS];",
            "",
            "\tint levelspread[RCU_NUM_LVLS];\t\t/* kids/node in each level. */",
            "\tint cpustride = 1;",
            "\tint i;",
            "\tint j;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tBUILD_BUG_ON(RCU_NUM_LVLS > ARRAY_SIZE(buf));  /* Fix buf[] init! */",
            "",
            "\t/* Silence gcc 4.8 false positive about array index out of range. */",
            "\tif (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)",
            "\t\tpanic(\"rcu_init_one: rcu_num_lvls out of range\");",
            "",
            "\t/* Initialize the level-tracking arrays. */",
            "",
            "\tfor (i = 1; i < rcu_num_lvls; i++)",
            "\t\trcu_state.level[i] =",
            "\t\t\trcu_state.level[i - 1] + num_rcu_lvl[i - 1];",
            "\trcu_init_levelspread(levelspread, num_rcu_lvl);",
            "",
            "\t/* Initialize the elements themselves, starting from the leaves. */",
            "",
            "\tfor (i = rcu_num_lvls - 1; i >= 0; i--) {",
            "\t\tcpustride *= levelspread[i];",
            "\t\trnp = rcu_state.level[i];",
            "\t\tfor (j = 0; j < num_rcu_lvl[i]; j++, rnp++) {",
            "\t\t\traw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));",
            "\t\t\tlockdep_set_class_and_name(&ACCESS_PRIVATE(rnp, lock),",
            "\t\t\t\t\t\t   &rcu_node_class[i], buf[i]);",
            "\t\t\traw_spin_lock_init(&rnp->fqslock);",
            "\t\t\tlockdep_set_class_and_name(&rnp->fqslock,",
            "\t\t\t\t\t\t   &rcu_fqs_class[i], fqs[i]);",
            "\t\t\trnp->gp_seq = rcu_state.gp_seq;",
            "\t\t\trnp->gp_seq_needed = rcu_state.gp_seq;",
            "\t\t\trnp->completedqs = rcu_state.gp_seq;",
            "\t\t\trnp->qsmask = 0;",
            "\t\t\trnp->qsmaskinit = 0;",
            "\t\t\trnp->grplo = j * cpustride;",
            "\t\t\trnp->grphi = (j + 1) * cpustride - 1;",
            "\t\t\tif (rnp->grphi >= nr_cpu_ids)",
            "\t\t\t\trnp->grphi = nr_cpu_ids - 1;",
            "\t\t\tif (i == 0) {",
            "\t\t\t\trnp->grpnum = 0;",
            "\t\t\t\trnp->grpmask = 0;",
            "\t\t\t\trnp->parent = NULL;",
            "\t\t\t} else {",
            "\t\t\t\trnp->grpnum = j % levelspread[i - 1];",
            "\t\t\t\trnp->grpmask = BIT(rnp->grpnum);",
            "\t\t\t\trnp->parent = rcu_state.level[i - 1] +",
            "\t\t\t\t\t      j / levelspread[i - 1];",
            "\t\t\t}",
            "\t\t\trnp->level = i;",
            "\t\t\tINIT_LIST_HEAD(&rnp->blkd_tasks);",
            "\t\t\trcu_init_one_nocb(rnp);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[0]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[1]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[2]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[3]);",
            "\t\t\tspin_lock_init(&rnp->exp_lock);",
            "\t\t\tmutex_init(&rnp->boost_kthread_mutex);",
            "\t\t\traw_spin_lock_init(&rnp->exp_poll_lock);",
            "\t\t\trnp->exp_seq_poll_rq = RCU_GET_STATE_COMPLETED;",
            "\t\t\tINIT_WORK(&rnp->exp_poll_wq, sync_rcu_do_polled_gp);",
            "\t\t}",
            "\t}",
            "",
            "\tinit_swait_queue_head(&rcu_state.gp_wq);",
            "\tinit_swait_queue_head(&rcu_state.expedited_wq);",
            "\trnp = rcu_first_leaf_node();",
            "\tfor_each_possible_cpu(i) {",
            "\t\twhile (i > rnp->grphi)",
            "\t\t\trnp++;",
            "\t\tper_cpu_ptr(&rcu_data, i)->mynode = rnp;",
            "\t\trcu_boot_init_percpu_data(i);",
            "\t}",
            "}",
            "static void __init sanitize_kthread_prio(void)",
            "{",
            "\tint kthread_prio_in = kthread_prio;",
            "",
            "\tif (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 2",
            "\t    && IS_BUILTIN(CONFIG_RCU_TORTURE_TEST))",
            "\t\tkthread_prio = 2;",
            "\telse if (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 1)",
            "\t\tkthread_prio = 1;",
            "\telse if (kthread_prio < 0)",
            "\t\tkthread_prio = 0;",
            "\telse if (kthread_prio > 99)",
            "\t\tkthread_prio = 99;",
            "",
            "\tif (kthread_prio != kthread_prio_in)",
            "\t\tpr_alert(\"%s: Limited prio to %d from %d\\n\",",
            "\t\t\t __func__, kthread_prio, kthread_prio_in);",
            "}",
            "void rcu_init_geometry(void)",
            "{",
            "\tulong d;",
            "\tint i;",
            "\tstatic unsigned long old_nr_cpu_ids;",
            "\tint rcu_capacity[RCU_NUM_LVLS];",
            "\tstatic bool initialized;",
            "",
            "\tif (initialized) {",
            "\t\t/*",
            "\t\t * Warn if setup_nr_cpu_ids() had not yet been invoked,",
            "\t\t * unless nr_cpus_ids == NR_CPUS, in which case who cares?",
            "\t\t */",
            "\t\tWARN_ON_ONCE(old_nr_cpu_ids != nr_cpu_ids);",
            "\t\treturn;",
            "\t}",
            "",
            "\told_nr_cpu_ids = nr_cpu_ids;",
            "\tinitialized = true;",
            "",
            "\t/*",
            "\t * Initialize any unspecified boot parameters.",
            "\t * The default values of jiffies_till_first_fqs and",
            "\t * jiffies_till_next_fqs are set to the RCU_JIFFIES_TILL_FORCE_QS",
            "\t * value, which is a function of HZ, then adding one for each",
            "\t * RCU_JIFFIES_FQS_DIV CPUs that might be on the system.",
            "\t */",
            "\td = RCU_JIFFIES_TILL_FORCE_QS + nr_cpu_ids / RCU_JIFFIES_FQS_DIV;",
            "\tif (jiffies_till_first_fqs == ULONG_MAX)",
            "\t\tjiffies_till_first_fqs = d;",
            "\tif (jiffies_till_next_fqs == ULONG_MAX)",
            "\t\tjiffies_till_next_fqs = d;",
            "\tadjust_jiffies_till_sched_qs();",
            "",
            "\t/* If the compile-time values are accurate, just leave. */",
            "\tif (rcu_fanout_leaf == RCU_FANOUT_LEAF &&",
            "\t    nr_cpu_ids == NR_CPUS)",
            "\t\treturn;",
            "\tpr_info(\"Adjusting geometry for rcu_fanout_leaf=%d, nr_cpu_ids=%u\\n\",",
            "\t\trcu_fanout_leaf, nr_cpu_ids);",
            "",
            "\t/*",
            "\t * The boot-time rcu_fanout_leaf parameter must be at least two",
            "\t * and cannot exceed the number of bits in the rcu_node masks.",
            "\t * Complain and fall back to the compile-time values if this",
            "\t * limit is exceeded.",
            "\t */",
            "\tif (rcu_fanout_leaf < 2 ||",
            "\t    rcu_fanout_leaf > sizeof(unsigned long) * 8) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Compute number of nodes that can be handled an rcu_node tree",
            "\t * with the given number of levels.",
            "\t */",
            "\trcu_capacity[0] = rcu_fanout_leaf;",
            "\tfor (i = 1; i < RCU_NUM_LVLS; i++)",
            "\t\trcu_capacity[i] = rcu_capacity[i - 1] * RCU_FANOUT;",
            "",
            "\t/*",
            "\t * The tree must be able to accommodate the configured number of CPUs.",
            "\t * If this limit is exceeded, fall back to the compile-time values.",
            "\t */",
            "\tif (nr_cpu_ids > rcu_capacity[RCU_NUM_LVLS - 1]) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Calculate the number of levels in the tree. */",
            "\tfor (i = 0; nr_cpu_ids > rcu_capacity[i]; i++) {",
            "\t}",
            "\trcu_num_lvls = i + 1;",
            "",
            "\t/* Calculate the number of rcu_nodes at each level of the tree. */",
            "\tfor (i = 0; i < rcu_num_lvls; i++) {",
            "\t\tint cap = rcu_capacity[(rcu_num_lvls - 1) - i];",
            "\t\tnum_rcu_lvl[i] = DIV_ROUND_UP(nr_cpu_ids, cap);",
            "\t}",
            "",
            "\t/* Calculate the total number of rcu_node structures. */",
            "\trcu_num_nodes = 0;",
            "\tfor (i = 0; i < rcu_num_lvls; i++)",
            "\t\trcu_num_nodes += num_rcu_lvl[i];",
            "}"
          ],
          "function_name": "rcu_init_one, sanitize_kthread_prio, rcu_init_geometry",
          "description": "构建多级RCU节点树结构，初始化各层级的锁类和节点属性，动态调整RCU树的几何形态以适配当前CPU数量和层级分布需求。",
          "similarity": 0.49710413813591003
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2012,
          "end_line": 2249,
          "content": [
            "static void",
            "rcu_report_qs_rdp(struct rcu_data *rdp)",
            "{",
            "\tunsigned long flags;",
            "\tunsigned long mask;",
            "\tbool needacc = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tWARN_ON_ONCE(rdp->cpu != smp_processor_id());",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\tif (rdp->cpu_no_qs.b.norm || rdp->gp_seq != rnp->gp_seq ||",
            "\t    rdp->gpwrap) {",
            "",
            "\t\t/*",
            "\t\t * The grace period in which this quiescent state was",
            "\t\t * recorded has ended, so don't report it upwards.",
            "\t\t * We will instead need a new quiescent state that lies",
            "\t\t * within the current grace period.",
            "\t\t */",
            "\t\trdp->cpu_no_qs.b.norm = true;\t/* need qs for new gp. */",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;",
            "\t}",
            "\tmask = rdp->grpmask;",
            "\trdp->core_needs_qs = false;",
            "\tif ((rnp->qsmask & mask) == 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t} else {",
            "\t\t/*",
            "\t\t * This GP can't end until cpu checks in, so all of our",
            "\t\t * callbacks can be processed during the next GP.",
            "\t\t *",
            "\t\t * NOCB kthreads have their own way to deal with that...",
            "\t\t */",
            "\t\tif (!rcu_rdp_is_offloaded(rdp)) {",
            "\t\t\t/*",
            "\t\t\t * The current GP has not yet ended, so it",
            "\t\t\t * should not be possible for rcu_accelerate_cbs()",
            "\t\t\t * to return true.  So complain, but don't awaken.",
            "\t\t\t */",
            "\t\t\tWARN_ON_ONCE(rcu_accelerate_cbs(rnp, rdp));",
            "\t\t} else if (!rcu_segcblist_completely_offloaded(&rdp->cblist)) {",
            "\t\t\t/*",
            "\t\t\t * ...but NOCB kthreads may miss or delay callbacks acceleration",
            "\t\t\t * if in the middle of a (de-)offloading process.",
            "\t\t\t */",
            "\t\t\tneedacc = true;",
            "\t\t}",
            "",
            "\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\t/* ^^^ Released rnp->lock */",
            "",
            "\t\tif (needacc) {",
            "\t\t\trcu_nocb_lock_irqsave(rdp, flags);",
            "\t\t\trcu_accelerate_cbs_unlocked(rnp, rdp);",
            "\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "\t\t}",
            "\t}",
            "}",
            "static void",
            "rcu_check_quiescent_state(struct rcu_data *rdp)",
            "{",
            "\t/* Check for grace-period ends and beginnings. */",
            "\tnote_gp_changes(rdp);",
            "",
            "\t/*",
            "\t * Does this CPU still need to do its part for current grace period?",
            "\t * If no, return and let the other CPUs do their part as well.",
            "\t */",
            "\tif (!rdp->core_needs_qs)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Was there a quiescent state since the beginning of the grace",
            "\t * period? If no, then exit and wait for the next call.",
            "\t */",
            "\tif (rdp->cpu_no_qs.b.norm)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Tell RCU we are done (but rcu_report_qs_rdp() will be the",
            "\t * judge of that).",
            "\t */",
            "\trcu_report_qs_rdp(rdp);",
            "}",
            "static bool rcu_do_batch_check_time(long count, long tlimit,",
            "\t\t\t\t    bool jlimit_check, unsigned long jlimit)",
            "{",
            "\t// Invoke local_clock() only once per 32 consecutive callbacks.",
            "\treturn unlikely(tlimit) &&",
            "\t       (!likely(count & 31) ||",
            "\t\t(IS_ENABLED(CONFIG_RCU_DOUBLE_CHECK_CB_TIME) &&",
            "\t\t jlimit_check && time_after(jiffies, jlimit))) &&",
            "\t       local_clock() >= tlimit;",
            "}",
            "static void rcu_do_batch(struct rcu_data *rdp)",
            "{",
            "\tlong bl;",
            "\tlong count = 0;",
            "\tint div;",
            "\tbool __maybe_unused empty;",
            "\tunsigned long flags;",
            "\tunsigned long jlimit;",
            "\tbool jlimit_check = false;",
            "\tlong pending;",
            "\tstruct rcu_cblist rcl = RCU_CBLIST_INITIALIZER(rcl);",
            "\tstruct rcu_head *rhp;",
            "\tlong tlimit = 0;",
            "",
            "\t/* If no callbacks are ready, just return. */",
            "\tif (!rcu_segcblist_ready_cbs(&rdp->cblist)) {",
            "\t\ttrace_rcu_batch_start(rcu_state.name,",
            "\t\t\t\t      rcu_segcblist_n_cbs(&rdp->cblist), 0);",
            "\t\ttrace_rcu_batch_end(rcu_state.name, 0,",
            "\t\t\t\t    !rcu_segcblist_empty(&rdp->cblist),",
            "\t\t\t\t    need_resched(), is_idle_task(current),",
            "\t\t\t\t    rcu_is_callbacks_kthread(rdp));",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Extract the list of ready callbacks, disabling IRQs to prevent",
            "\t * races with call_rcu() from interrupt handlers.  Leave the",
            "\t * callback counts, as rcu_barrier() needs to be conservative.",
            "\t */",
            "\trcu_nocb_lock_irqsave(rdp, flags);",
            "\tWARN_ON_ONCE(cpu_is_offline(smp_processor_id()));",
            "\tpending = rcu_segcblist_get_seglen(&rdp->cblist, RCU_DONE_TAIL);",
            "\tdiv = READ_ONCE(rcu_divisor);",
            "\tdiv = div < 0 ? 7 : div > sizeof(long) * 8 - 2 ? sizeof(long) * 8 - 2 : div;",
            "\tbl = max(rdp->blimit, pending >> div);",
            "\tif ((in_serving_softirq() || rdp->rcu_cpu_kthread_status == RCU_KTHREAD_RUNNING) &&",
            "\t    (IS_ENABLED(CONFIG_RCU_DOUBLE_CHECK_CB_TIME) || unlikely(bl > 100))) {",
            "\t\tconst long npj = NSEC_PER_SEC / HZ;",
            "\t\tlong rrn = READ_ONCE(rcu_resched_ns);",
            "",
            "\t\trrn = rrn < NSEC_PER_MSEC ? NSEC_PER_MSEC : rrn > NSEC_PER_SEC ? NSEC_PER_SEC : rrn;",
            "\t\ttlimit = local_clock() + rrn;",
            "\t\tjlimit = jiffies + (rrn + npj + 1) / npj;",
            "\t\tjlimit_check = true;",
            "\t}",
            "\ttrace_rcu_batch_start(rcu_state.name,",
            "\t\t\t      rcu_segcblist_n_cbs(&rdp->cblist), bl);",
            "\trcu_segcblist_extract_done_cbs(&rdp->cblist, &rcl);",
            "\tif (rcu_rdp_is_offloaded(rdp))",
            "\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCbDequeued\"));",
            "\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "",
            "\t/* Invoke callbacks. */",
            "\ttick_dep_set_task(current, TICK_DEP_BIT_RCU);",
            "\trhp = rcu_cblist_dequeue(&rcl);",
            "",
            "\tfor (; rhp; rhp = rcu_cblist_dequeue(&rcl)) {",
            "\t\trcu_callback_t f;",
            "",
            "\t\tcount++;",
            "\t\tdebug_rcu_head_unqueue(rhp);",
            "",
            "\t\trcu_lock_acquire(&rcu_callback_map);",
            "\t\ttrace_rcu_invoke_callback(rcu_state.name, rhp);",
            "",
            "\t\tf = rhp->func;",
            "\t\tdebug_rcu_head_callback(rhp);",
            "\t\tWRITE_ONCE(rhp->func, (rcu_callback_t)0L);",
            "\t\tf(rhp);",
            "",
            "\t\trcu_lock_release(&rcu_callback_map);",
            "",
            "\t\t/*",
            "\t\t * Stop only if limit reached and CPU has something to do.",
            "\t\t */",
            "\t\tif (in_serving_softirq()) {",
            "\t\t\tif (count >= bl && (need_resched() || !is_idle_task(current)))",
            "\t\t\t\tbreak;",
            "\t\t\t/*",
            "\t\t\t * Make sure we don't spend too much time here and deprive other",
            "\t\t\t * softirq vectors of CPU cycles.",
            "\t\t\t */",
            "\t\t\tif (rcu_do_batch_check_time(count, tlimit, jlimit_check, jlimit))",
            "\t\t\t\tbreak;",
            "\t\t} else {",
            "\t\t\t// In rcuc/rcuoc context, so no worries about",
            "\t\t\t// depriving other softirq vectors of CPU cycles.",
            "\t\t\tlocal_bh_enable();",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\t\tlocal_bh_disable();",
            "\t\t\t// But rcuc kthreads can delay quiescent-state",
            "\t\t\t// reporting, so check time limits for them.",
            "\t\t\tif (rdp->rcu_cpu_kthread_status == RCU_KTHREAD_RUNNING &&",
            "\t\t\t    rcu_do_batch_check_time(count, tlimit, jlimit_check, jlimit)) {",
            "\t\t\t\trdp->rcu_cpu_has_work = 1;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\trcu_nocb_lock_irqsave(rdp, flags);",
            "\trdp->n_cbs_invoked += count;",
            "\ttrace_rcu_batch_end(rcu_state.name, count, !!rcl.head, need_resched(),",
            "\t\t\t    is_idle_task(current), rcu_is_callbacks_kthread(rdp));",
            "",
            "\t/* Update counts and requeue any remaining callbacks. */",
            "\trcu_segcblist_insert_done_cbs(&rdp->cblist, &rcl);",
            "\trcu_segcblist_add_len(&rdp->cblist, -count);",
            "",
            "\t/* Reinstate batch limit if we have worked down the excess. */",
            "\tcount = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\tif (rdp->blimit >= DEFAULT_MAX_RCU_BLIMIT && count <= qlowmark)",
            "\t\trdp->blimit = blimit;",
            "",
            "\t/* Reset ->qlen_last_fqs_check trigger if enough CBs have drained. */",
            "\tif (count == 0 && rdp->qlen_last_fqs_check != 0) {",
            "\t\trdp->qlen_last_fqs_check = 0;",
            "\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t} else if (count < rdp->qlen_last_fqs_check - qhimark)",
            "\t\trdp->qlen_last_fqs_check = count;",
            "",
            "\t/*",
            "\t * The following usually indicates a double call_rcu().  To track",
            "\t * this down, try building with CONFIG_DEBUG_OBJECTS_RCU_HEAD=y.",
            "\t */",
            "\tempty = rcu_segcblist_empty(&rdp->cblist);",
            "\tWARN_ON_ONCE(count == 0 && !empty);",
            "\tWARN_ON_ONCE(!IS_ENABLED(CONFIG_RCU_NOCB_CPU) &&",
            "\t\t     count != 0 && empty);",
            "\tWARN_ON_ONCE(count == 0 && rcu_segcblist_n_segment_cbs(&rdp->cblist) != 0);",
            "\tWARN_ON_ONCE(!empty && rcu_segcblist_n_segment_cbs(&rdp->cblist) == 0);",
            "",
            "\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "",
            "\ttick_dep_clear_task(current, TICK_DEP_BIT_RCU);",
            "}"
          ],
          "function_name": "rcu_report_qs_rdp, rcu_check_quiescent_state, rcu_do_batch_check_time, rcu_do_batch",
          "description": "提供CPU级quiescent状态检测与回调处理机制，包含quiescent状态上报、回调批量处理及性能监控功能，支持动态调整批处理参数",
          "similarity": 0.49268242716789246
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2523,
          "end_line": 2628,
          "content": [
            "static void rcu_cpu_kthread_park(unsigned int cpu)",
            "{",
            "\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;",
            "}",
            "static int rcu_cpu_kthread_should_run(unsigned int cpu)",
            "{",
            "\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);",
            "}",
            "static void rcu_cpu_kthread(unsigned int cpu)",
            "{",
            "\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);",
            "\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);",
            "\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);",
            "\tint spincnt;",
            "",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));",
            "\tfor (spincnt = 0; spincnt < 10; spincnt++) {",
            "\t\tWRITE_ONCE(*j, jiffies);",
            "\t\tlocal_bh_disable();",
            "\t\t*statusp = RCU_KTHREAD_RUNNING;",
            "\t\tlocal_irq_disable();",
            "\t\twork = *workp;",
            "\t\tWRITE_ONCE(*workp, 0);",
            "\t\tlocal_irq_enable();",
            "\t\tif (work)",
            "\t\t\trcu_core();",
            "\t\tlocal_bh_enable();",
            "\t\tif (!READ_ONCE(*workp)) {",
            "\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));",
            "\t\t\t*statusp = RCU_KTHREAD_WAITING;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "\t*statusp = RCU_KTHREAD_YIELDING;",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));",
            "\tschedule_timeout_idle(2);",
            "\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));",
            "\t*statusp = RCU_KTHREAD_WAITING;",
            "\tWRITE_ONCE(*j, jiffies);",
            "}",
            "static int __init rcu_spawn_core_kthreads(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;",
            "\tif (use_softirq)",
            "\t\treturn 0;",
            "\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),",
            "\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);",
            "\treturn 0;",
            "}",
            "static void rcutree_enqueue(struct rcu_data *rdp, struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\trcu_segcblist_enqueue(&rdp->cblist, head);",
            "\tif (__is_kvfree_rcu_offset((unsigned long)func))",
            "\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,",
            "\t\t\t\t\t (unsigned long)func,",
            "\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));",
            "\telse",
            "\t\ttrace_rcu_callback(rcu_state.name, head,",
            "\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));",
            "}",
            "static void call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,",
            "\t\t\t  rcu_callback_t func, unsigned long flags)",
            "{",
            "\trcutree_enqueue(rdp, head, func);",
            "\t/*",
            "\t * If called from an extended quiescent state, invoke the RCU",
            "\t * core in order to force a re-evaluation of RCU's idleness.",
            "\t */",
            "\tif (!rcu_is_watching())",
            "\t\tinvoke_rcu_core();",
            "",
            "\t/* If interrupts were disabled or CPU offline, don't invoke RCU core. */",
            "\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Force the grace period if too many callbacks or too long waiting.",
            "\t * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()",
            "\t * if some other CPU has recently done so.  Also, don't bother",
            "\t * invoking rcu_force_quiescent_state() if the newly enqueued callback",
            "\t * is the only one waiting for a grace period to complete.",
            "\t */",
            "\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >",
            "\t\t     rdp->qlen_last_fqs_check + qhimark)) {",
            "",
            "\t\t/* Are we ignoring a completed grace period? */",
            "\t\tnote_gp_changes(rdp);",
            "",
            "\t\t/* Start a new grace period if one not already started. */",
            "\t\tif (!rcu_gp_in_progress()) {",
            "\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);",
            "\t\t} else {",
            "\t\t\t/* Give the grace period a kick. */",
            "\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;",
            "\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&",
            "\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)",
            "\t\t\t\trcu_force_quiescent_state();",
            "\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "rcu_cpu_kthread_park, rcu_cpu_kthread_should_run, rcu_cpu_kthread, rcu_spawn_core_kthreads, rcutree_enqueue, call_rcu_core",
          "description": "实现RCU k线程管理与回调分发基础设施，包含线程启动、回调入队及触发条件判断逻辑，提供跨CPU的异步处理能力",
          "similarity": 0.4898608326911926
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1840,
          "end_line": 1973,
          "content": [
            "static int __noreturn rcu_gp_kthread(void *unused)",
            "{",
            "\trcu_bind_gp_kthread();",
            "\tfor (;;) {",
            "",
            "\t\t/* Handle grace-period start. */",
            "\t\tfor (;;) {",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwait\"));",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_WAIT_GPS);",
            "\t\t\tswait_event_idle_exclusive(rcu_state.gp_wq,",
            "\t\t\t\t\t READ_ONCE(rcu_state.gp_flags) &",
            "\t\t\t\t\t RCU_GP_FLAG_INIT);",
            "\t\t\trcu_gp_torture_wait();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_DONE_GPS);",
            "\t\t\t/* Locking provides needed memory barrier. */",
            "\t\t\tif (rcu_gp_init())",
            "\t\t\t\tbreak;",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\t\t\tWARN_ON(signal_pending(current));",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwaitsig\"));",
            "\t\t}",
            "",
            "\t\t/* Handle quiescent-state forcing. */",
            "\t\trcu_gp_fqs_loop();",
            "",
            "\t\t/* Handle grace-period end. */",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANUP);",
            "\t\trcu_gp_cleanup();",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANED);",
            "\t}",
            "}",
            "static void rcu_report_qs_rsp(unsigned long flags)",
            "\t__releases(rcu_get_root()->lock)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rcu_get_root());",
            "\tWARN_ON_ONCE(!rcu_gp_in_progress());",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rcu_get_root(), flags);",
            "\trcu_gp_kthread_wake();",
            "}",
            "static void rcu_report_qs_rnp(unsigned long mask, struct rcu_node *rnp,",
            "\t\t\t      unsigned long gps, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long oldmask = 0;",
            "\tstruct rcu_node *rnp_c;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* Walk up the rcu_node hierarchy. */",
            "\tfor (;;) {",
            "\t\tif ((!(rnp->qsmask & mask) && mask) || rnp->gp_seq != gps) {",
            "",
            "\t\t\t/*",
            "\t\t\t * Our bit has already been cleared, or the",
            "\t\t\t * relevant grace period is already over, so done.",
            "\t\t\t */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */",
            "\t\tWARN_ON_ONCE(!rcu_is_leaf_node(rnp) &&",
            "\t\t\t     rcu_preempt_blocked_readers_cgp(rnp));",
            "\t\tWRITE_ONCE(rnp->qsmask, rnp->qsmask & ~mask);",
            "\t\ttrace_rcu_quiescent_state_report(rcu_state.name, rnp->gp_seq,",
            "\t\t\t\t\t\t mask, rnp->qsmask, rnp->level,",
            "\t\t\t\t\t\t rnp->grplo, rnp->grphi,",
            "\t\t\t\t\t\t !!rnp->gp_tasks);",
            "\t\tif (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {",
            "",
            "\t\t\t/* Other bits still set at this level, so done. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\trnp->completedqs = rnp->gp_seq;",
            "\t\tmask = rnp->grpmask;",
            "\t\tif (rnp->parent == NULL) {",
            "",
            "\t\t\t/* No more levels.  Exit loop holding root lock. */",
            "",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\trnp_c = rnp;",
            "\t\trnp = rnp->parent;",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\toldmask = READ_ONCE(rnp_c->qsmask);",
            "\t}",
            "",
            "\t/*",
            "\t * Get here if we are the last CPU to pass through a quiescent",
            "\t * state for this grace period.  Invoke rcu_report_qs_rsp()",
            "\t * to clean up and start the next grace period if one is needed.",
            "\t */",
            "\trcu_report_qs_rsp(flags); /* releases rnp->lock. */",
            "}",
            "static void __maybe_unused",
            "rcu_report_unblock_qs_rnp(struct rcu_node *rnp, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long gps;",
            "\tunsigned long mask;",
            "\tstruct rcu_node *rnp_p;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (WARN_ON_ONCE(!IS_ENABLED(CONFIG_PREEMPT_RCU)) ||",
            "\t    WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)) ||",
            "\t    rnp->qsmask != 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;  /* Still need more quiescent states! */",
            "\t}",
            "",
            "\trnp->completedqs = rnp->gp_seq;",
            "\trnp_p = rnp->parent;",
            "\tif (rnp_p == NULL) {",
            "\t\t/*",
            "\t\t * Only one rcu_node structure in the tree, so don't",
            "\t\t * try to report up to its nonexistent parent!",
            "\t\t */",
            "\t\trcu_report_qs_rsp(flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Report up the rest of the hierarchy, tracking current ->gp_seq. */",
            "\tgps = rnp->gp_seq;",
            "\tmask = rnp->grpmask;",
            "\traw_spin_unlock_rcu_node(rnp);\t/* irqs remain disabled. */",
            "\traw_spin_lock_rcu_node(rnp_p);\t/* irqs already disabled. */",
            "\trcu_report_qs_rnp(mask, rnp_p, gps, flags);",
            "}"
          ],
          "function_name": "rcu_gp_kthread, rcu_report_qs_rsp, rcu_report_qs_rnp, rcu_report_unblock_qs_rnp",
          "description": "实现RCU grace period的主线程循环，处理grace period启动、强制quiescent状态报告及结束逻辑，通过锁和状态标志协调各子系统同步",
          "similarity": 0.4868857264518738
        },
        {
          "chunk_id": 24,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4444,
          "end_line": 4559,
          "content": [
            "static void __init",
            "rcu_boot_init_percpu_data(int cpu)",
            "{",
            "\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t/* Set up local state, ensuring consistent view of global state. */",
            "\trdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);",
            "\tINIT_WORK(&rdp->strict_work, strict_work_handler);",
            "\tWARN_ON_ONCE(ct->dynticks_nesting != 1);",
            "\tWARN_ON_ONCE(rcu_dynticks_in_eqs(rcu_dynticks_snap(cpu)));",
            "\trdp->barrier_seq_snap = rcu_state.barrier_sequence;",
            "\trdp->rcu_ofl_gp_seq = rcu_state.gp_seq;",
            "\trdp->rcu_ofl_gp_flags = RCU_GP_CLEANED;",
            "\trdp->rcu_onl_gp_seq = rcu_state.gp_seq;",
            "\trdp->rcu_onl_gp_flags = RCU_GP_CLEANED;",
            "\trdp->last_sched_clock = jiffies;",
            "\trdp->cpu = cpu;",
            "\trcu_boot_init_nocb_percpu_data(rdp);",
            "}",
            "int rcutree_prepare_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct context_tracking *ct = per_cpu_ptr(&context_tracking, cpu);",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\t/* Set up local state, ensuring consistent view of global state. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trdp->qlen_last_fqs_check = 0;",
            "\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\trdp->blimit = blimit;",
            "\tct->dynticks_nesting = 1;\t/* CPU not up, no tearing. */",
            "\traw_spin_unlock_rcu_node(rnp);\t\t/* irqs remain disabled. */",
            "",
            "\t/*",
            "\t * Only non-NOCB CPUs that didn't have early-boot callbacks need to be",
            "\t * (re-)initialized.",
            "\t */",
            "\tif (!rcu_segcblist_is_enabled(&rdp->cblist))",
            "\t\trcu_segcblist_init(&rdp->cblist);  /* Re-enable callbacks. */",
            "",
            "\t/*",
            "\t * Add CPU to leaf rcu_node pending-online bitmask.  Any needed",
            "\t * propagation up the rcu_node tree will happen at the beginning",
            "\t * of the next grace period.",
            "\t */",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_rcu_node(rnp);\t\t/* irqs already disabled. */",
            "\trdp->gp_seq = READ_ONCE(rnp->gp_seq);",
            "\trdp->gp_seq_needed = rdp->gp_seq;",
            "\trdp->cpu_no_qs.b.norm = true;",
            "\trdp->core_needs_qs = false;",
            "\trdp->rcu_iw_pending = false;",
            "\trdp->rcu_iw = IRQ_WORK_INIT_HARD(rcu_iw_handler);",
            "\trdp->rcu_iw_gp_seq = rdp->gp_seq - 1;",
            "\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuonl\"));",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "",
            "\trcu_preempt_deferred_qs_init(rdp);",
            "\trcu_spawn_one_boost_kthread(rnp);",
            "\trcu_spawn_cpu_nocb_kthread(cpu);",
            "\tWRITE_ONCE(rcu_state.n_online_cpus, rcu_state.n_online_cpus + 1);",
            "",
            "\treturn 0;",
            "}",
            "static void rcutree_affinity_setting(unsigned int cpu, int outgoing)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\trcu_boost_kthread_setaffinity(rdp->mynode, outgoing);",
            "}",
            "bool rcu_cpu_beenfullyonline(int cpu)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\treturn smp_load_acquire(&rdp->beenonline);",
            "}",
            "int rcutree_online_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trnp->ffmask |= rdp->grpmask;",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_INACTIVE)",
            "\t\treturn 0; /* Too early in boot for scheduler work. */",
            "\tsync_sched_exp_online_cleanup(cpu);",
            "\trcutree_affinity_setting(cpu, -1);",
            "",
            "\t// Stop-machine done, so allow nohz_full to disable tick.",
            "\ttick_dep_clear(TICK_DEP_BIT_RCU);",
            "\treturn 0;",
            "}",
            "int rcutree_offline_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trnp->ffmask &= ~rdp->grpmask;",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "",
            "\trcutree_affinity_setting(cpu, cpu);",
            "",
            "\t// nohz_full CPUs need the tick for stop-machine to work quickly",
            "\ttick_dep_set(TICK_DEP_BIT_RCU);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rcu_boot_init_percpu_data, rcutree_prepare_cpu, rcutree_affinity_setting, rcu_cpu_beenfullyonline, rcutree_online_cpu, rcutree_offline_cpu",
          "description": "初始化每个CPU的RCU私有数据结构，处理CPU上线/下线时的RCU状态同步，配置中断亲和性，更新全局在线CPU计数器",
          "similarity": 0.4718080461025238
        }
      ]
    },
    {
      "source_file": "kernel/bpf/hashtab.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:10:56\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\hashtab.c`\n\n---\n\n# bpf/hashtab.c 技术文档\n\n## 1. 文件概述\n\n`bpf/hashtab.c` 是 Linux 内核中 BPF（Berkeley Packet Filter）子系统的核心实现文件之一，负责提供基于哈希表（hash table）的 BPF map 类型支持。该文件实现了多种 BPF map 类型，包括普通哈希表（`BPF_MAP_TYPE_HASH`）、LRU 哈希表（`BPF_MAP_TYPE_LRU_HASH`）、每 CPU 哈希表（`BPF_MAP_TYPE_PERCPU_HASH`）及其 LRU 变体。它支持预分配（pre-allocated）和动态分配（non-preallocated）两种内存管理模式，并集成了 BPF 内存分配器（`bpf_mem_alloc`）、LRU 驱逐机制、每 CPU 自由列表（percpu freelist）等高级特性，以满足高性能、低延迟的 BPF 程序需求。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bucket`**  \n  哈希桶结构，包含一个 `hlist_nulls_head` 链表头和一个 `raw_spinlock_t` 原始自旋锁，用于保护桶内元素的并发访问。\n\n- **`struct bpf_htab`**  \n  BPF 哈希表的主控制结构，继承自 `struct bpf_map`，包含：\n  - 桶数组指针 `buckets`\n  - 元素存储区 `elems`\n  - 内存分配器 `ma`（主）和 `pcpu_ma`（每 CPU）\n  - LRU 或 percpu_freelist 联合体\n  - 元素计数器（`pcount` 或 `count`）\n  - 哈希种子 `hashrnd`\n  - 锁依赖类键 `lockdep_key`\n  - 每 CPU 锁状态数组 `map_locked`（用于防止递归）\n\n- **`struct htab_elem`**  \n  哈希表元素结构，包含：\n  - 哈希链表节点 `hash_node`\n  - LRU 节点或自由列表节点\n  - 指向每 CPU 指针的指针（用于 per-CPU map）\n  - 哈希值 `hash`\n  - 可变长键 `key[]`（后接值或 per-CPU 指针）\n\n### 关键辅助函数\n\n- `htab_is_prealloc()`：判断是否为预分配模式\n- `htab_is_lru()` / `htab_is_percpu()`：判断 map 类型是否为 LRU 或 per-CPU\n- `htab_init_buckets()`：初始化所有哈希桶\n- `htab_lock_bucket()` / `htab_unlock_bucket()`：带递归保护的桶锁操作\n- `htab_elem_set_ptr()` / `htab_elem_get_ptr()`：操作 per-CPU 指针\n- `get_htab_elem()`：从预分配区域获取第 i 个元素\n- `htab_has_extra_elems()`：判断是否包含额外元素（用于 per-CPU 扩展）\n- `htab_free_prealloced_timers_and_wq()`：释放预分配元素中的 BPF 定时器和工作队列资源\n\n### 批量操作宏\n\n- `BATCH_OPS(_name)`：定义批量操作函数指针，如 `map_lookup_batch`、`map_update_batch` 等。\n\n## 3. 关键实现\n\n### 并发控制与死锁预防\n\n- 使用 **原始自旋锁（`raw_spinlock_t`）** 保护每个哈希桶，确保在任意上下文（如 kprobe、perf、tracepoint）中安全使用。\n- 引入 **每 CPU 递归计数器 `map_locked[]`**，防止 BPF 程序在持有桶锁时再次进入（例如通过 `sys_bpf()` 或嵌套 BPF 调用），避免死锁。\n- 在 `PREEMPT_RT` 实时内核上，由于普通自旋锁可能睡眠，必须使用 `raw_spinlock` 以保证原子性；结合 `bpf_mem_alloc` 后，即使非预分配模式也可安全使用原始锁。\n\n### 内存管理\n\n- **预分配模式（`BPF_F_NO_PREALLOC` 未设置）**：启动时一次性分配所有元素，使用 `pcpu_freelist` 管理空闲元素。\n- **非预分配模式**：按需通过 `bpf_mem_alloc` 动态分配元素，支持 NUMA 感知和内存回收。\n- **Per-CPU 支持**：对于 `PERCPU_HASH` 类型，每个键对应一个 per-CPU 值数组，通过 `htab_elem_get_ptr()` 访问。\n\n### LRU 驱逐机制\n\n- 当 map 类型为 `LRU_HASH` 或 `LRU_PERCPU_HASH` 时，使用 `bpf_lru` 子系统管理元素生命周期，自动驱逐最近最少使用的条目以维持 `max_entries` 限制。\n\n### 扩展字段支持\n\n- 支持 BTF（BPF Type Format）描述的复杂值类型，如 `BPF_TIMER` 和 `BPF_WORKQUEUE`，在销毁 map 时自动释放相关资源（见 `htab_free_prealloced_timers_and_wq`）。\n\n### 哈希与对齐\n\n- 使用 `jhash` 算法计算键的哈希值，并通过 `hashrnd` 引入随机种子防止哈希碰撞攻击。\n- 键和值之间按 8 字节对齐（`__aligned(8)`），确保 per-CPU 指针正确对齐。\n\n## 4. 依赖关系\n\n- **内核头文件**：\n  - `<linux/bpf.h>`、`<linux/btf.h>`：BPF 和 BTF 核心接口\n  - `<linux/jhash.h>`：哈希函数\n  - `<linux/rculist_nulls.h>`：RCU 安全的空指针链表\n  - `<linux/percpu_freelist.h>`、`<linux/bpf_lru_list.h>`：内存管理子系统\n  - `<linux/bpf_mem_alloc.h>`：BPF 专用内存分配器\n\n- **内部模块**：\n  - `map_in_map.h`：支持 map-in-map 功能\n  - `bpf_lru_list.c`：LRU 驱逐实现\n  - `percpu_freelist.c`：每 CPU 自由列表管理\n\n- **BPF 子系统**：\n  - 与 `bpf_map` 通用框架集成，通过 `bpf_map_ops` 注册操作函数\n  - 依赖 `bpf_prog_active` 机制防止 BPF 递归\n\n## 5. 使用场景\n\n- **网络数据包过滤与监控**：eBPF 程序使用 `BPF_MAP_TYPE_HASH` 存储连接状态、统计信息等。\n- **性能分析**：通过 `PERCPU_HASH` 收集每 CPU 的性能计数器，避免锁竞争。\n- **资源限制与缓存**：`LRU_HASH` 用于实现有界缓存（如 DNS 缓存、会话表），自动淘汰旧条目。\n- **内核跟踪**：kprobe、tracepoint 等 attach 的 BPF 程序频繁读写哈希表，要求低延迟和高并发。\n- **用户空间交互**：通过 `bpf(2)` 系统调用进行 map 的创建、更新、查询和删除，支持批量操作提升效率。\n- **高级 BPF 功能**：支持包含定时器（`bpf_timer`）或工作队列（`bpf_workqueue`）的复杂 map 值类型，用于异步任务调度。",
      "similarity": 0.5251041650772095,
      "chunks": [
        {
          "chunk_id": 12,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 1941,
          "end_line": 2043,
          "content": [
            "static int",
            "htab_percpu_map_lookup_batch(struct bpf_map *map, const union bpf_attr *attr,",
            "\t\t\t     union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, false,",
            "\t\t\t\t\t\t  false, true);",
            "}",
            "static int",
            "htab_percpu_map_lookup_and_delete_batch(struct bpf_map *map,",
            "\t\t\t\t\tconst union bpf_attr *attr,",
            "\t\t\t\t\tunion bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, true,",
            "\t\t\t\t\t\t  false, true);",
            "}",
            "static int",
            "htab_map_lookup_batch(struct bpf_map *map, const union bpf_attr *attr,",
            "\t\t      union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, false,",
            "\t\t\t\t\t\t  false, false);",
            "}",
            "static int",
            "htab_map_lookup_and_delete_batch(struct bpf_map *map,",
            "\t\t\t\t const union bpf_attr *attr,",
            "\t\t\t\t union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, true,",
            "\t\t\t\t\t\t  false, false);",
            "}",
            "static int",
            "htab_lru_percpu_map_lookup_batch(struct bpf_map *map,",
            "\t\t\t\t const union bpf_attr *attr,",
            "\t\t\t\t union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, false,",
            "\t\t\t\t\t\t  true, true);",
            "}",
            "static int",
            "htab_lru_percpu_map_lookup_and_delete_batch(struct bpf_map *map,",
            "\t\t\t\t\t    const union bpf_attr *attr,",
            "\t\t\t\t\t    union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, true,",
            "\t\t\t\t\t\t  true, true);",
            "}",
            "static int",
            "htab_lru_map_lookup_batch(struct bpf_map *map, const union bpf_attr *attr,",
            "\t\t\t  union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, false,",
            "\t\t\t\t\t\t  true, false);",
            "}",
            "static int",
            "htab_lru_map_lookup_and_delete_batch(struct bpf_map *map,",
            "\t\t\t\t     const union bpf_attr *attr,",
            "\t\t\t\t     union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, true,",
            "\t\t\t\t\t\t  true, false);",
            "}",
            "static int __bpf_hash_map_seq_show(struct seq_file *seq, struct htab_elem *elem)",
            "{",
            "\tstruct bpf_iter_seq_hash_map_info *info = seq->private;",
            "\tu32 roundup_key_size, roundup_value_size;",
            "\tstruct bpf_iter__bpf_map_elem ctx = {};",
            "\tstruct bpf_map *map = info->map;",
            "\tstruct bpf_iter_meta meta;",
            "\tint ret = 0, off = 0, cpu;",
            "\tstruct bpf_prog *prog;",
            "\tvoid __percpu *pptr;",
            "",
            "\tmeta.seq = seq;",
            "\tprog = bpf_iter_get_info(&meta, elem == NULL);",
            "\tif (prog) {",
            "\t\tctx.meta = &meta;",
            "\t\tctx.map = info->map;",
            "\t\tif (elem) {",
            "\t\t\troundup_key_size = round_up(map->key_size, 8);",
            "\t\t\tctx.key = elem->key;",
            "\t\t\tif (!info->percpu_value_buf) {",
            "\t\t\t\tctx.value = elem->key + roundup_key_size;",
            "\t\t\t} else {",
            "\t\t\t\troundup_value_size = round_up(map->value_size, 8);",
            "\t\t\t\tpptr = htab_elem_get_ptr(elem, map->key_size);",
            "\t\t\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\t\t\tcopy_map_value_long(map, info->percpu_value_buf + off,",
            "\t\t\t\t\t\t\t    per_cpu_ptr(pptr, cpu));",
            "\t\t\t\t\tcheck_and_init_map_value(map, info->percpu_value_buf + off);",
            "\t\t\t\t\toff += roundup_value_size;",
            "\t\t\t\t}",
            "\t\t\t\tctx.value = info->percpu_value_buf;",
            "\t\t\t}",
            "\t\t}",
            "\t\tret = bpf_iter_run_prog(prog, &ctx);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "static int bpf_hash_map_seq_show(struct seq_file *seq, void *v)",
            "{",
            "\treturn __bpf_hash_map_seq_show(seq, v);",
            "}"
          ],
          "function_name": "htab_percpu_map_lookup_batch, htab_percpu_map_lookup_and_delete_batch, htab_map_lookup_batch, htab_map_lookup_and_delete_batch, htab_lru_percpu_map_lookup_batch, htab_lru_percpu_map_lookup_and_delete_batch, htab_lru_map_lookup_batch, htab_lru_map_lookup_and_delete_batch, __bpf_hash_map_seq_show, bpf_hash_map_seq_show",
          "description": "提供多种批量操作接口封装，统一调用__htab_map_lookup_and_delete_batch实现。包含序列化展示函数，处理PERCPU值的特殊复制逻辑，支持迭代器上下文管理。",
          "similarity": 0.5220838785171509
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 1701,
          "end_line": 1938,
          "content": [
            "static int htab_lru_percpu_map_lookup_and_delete_elem(struct bpf_map *map,",
            "\t\t\t\t\t\t      void *key, void *value,",
            "\t\t\t\t\t\t      u64 flags)",
            "{",
            "\treturn __htab_map_lookup_and_delete_elem(map, key, value, true, true,",
            "\t\t\t\t\t\t flags);",
            "}",
            "static int",
            "__htab_map_lookup_and_delete_batch(struct bpf_map *map,",
            "\t\t\t\t   const union bpf_attr *attr,",
            "\t\t\t\t   union bpf_attr __user *uattr,",
            "\t\t\t\t   bool do_delete, bool is_lru_map,",
            "\t\t\t\t   bool is_percpu)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tu32 bucket_cnt, total, key_size, value_size, roundup_key_size;",
            "\tvoid *keys = NULL, *values = NULL, *value, *dst_key, *dst_val;",
            "\tvoid __user *uvalues = u64_to_user_ptr(attr->batch.values);",
            "\tvoid __user *ukeys = u64_to_user_ptr(attr->batch.keys);",
            "\tvoid __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);",
            "\tu32 batch, max_count, size, bucket_size, map_id;",
            "\tstruct htab_elem *node_to_free = NULL;",
            "\tu64 elem_map_flags, map_flags;",
            "\tstruct hlist_nulls_head *head;",
            "\tstruct hlist_nulls_node *n;",
            "\tunsigned long flags = 0;",
            "\tbool locked = false;",
            "\tstruct htab_elem *l;",
            "\tstruct bucket *b;",
            "\tint ret = 0;",
            "",
            "\telem_map_flags = attr->batch.elem_flags;",
            "\tif ((elem_map_flags & ~BPF_F_LOCK) ||",
            "\t    ((elem_map_flags & BPF_F_LOCK) && !btf_record_has_field(map->record, BPF_SPIN_LOCK)))",
            "\t\treturn -EINVAL;",
            "",
            "\tmap_flags = attr->batch.flags;",
            "\tif (map_flags)",
            "\t\treturn -EINVAL;",
            "",
            "\tmax_count = attr->batch.count;",
            "\tif (!max_count)",
            "\t\treturn 0;",
            "",
            "\tif (put_user(0, &uattr->batch.count))",
            "\t\treturn -EFAULT;",
            "",
            "\tbatch = 0;",
            "\tif (ubatch && copy_from_user(&batch, ubatch, sizeof(batch)))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (batch >= htab->n_buckets)",
            "\t\treturn -ENOENT;",
            "",
            "\tkey_size = htab->map.key_size;",
            "\troundup_key_size = round_up(htab->map.key_size, 8);",
            "\tvalue_size = htab->map.value_size;",
            "\tsize = round_up(value_size, 8);",
            "\tif (is_percpu)",
            "\t\tvalue_size = size * num_possible_cpus();",
            "\ttotal = 0;",
            "\t/* while experimenting with hash tables with sizes ranging from 10 to",
            "\t * 1000, it was observed that a bucket can have up to 5 entries.",
            "\t */",
            "\tbucket_size = 5;",
            "",
            "alloc:",
            "\t/* We cannot do copy_from_user or copy_to_user inside",
            "\t * the rcu_read_lock. Allocate enough space here.",
            "\t */",
            "\tkeys = kvmalloc_array(key_size, bucket_size, GFP_USER | __GFP_NOWARN);",
            "\tvalues = kvmalloc_array(value_size, bucket_size, GFP_USER | __GFP_NOWARN);",
            "\tif (!keys || !values) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto after_loop;",
            "\t}",
            "",
            "again:",
            "\tbpf_disable_instrumentation();",
            "\trcu_read_lock();",
            "again_nocopy:",
            "\tdst_key = keys;",
            "\tdst_val = values;",
            "\tb = &htab->buckets[batch];",
            "\thead = &b->head;",
            "\t/* do not grab the lock unless need it (bucket_cnt > 0). */",
            "\tif (locked) {",
            "\t\tret = htab_lock_bucket(htab, b, batch, &flags);",
            "\t\tif (ret) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\tbpf_enable_instrumentation();",
            "\t\t\tgoto after_loop;",
            "\t\t}",
            "\t}",
            "",
            "\tbucket_cnt = 0;",
            "\thlist_nulls_for_each_entry_rcu(l, n, head, hash_node)",
            "\t\tbucket_cnt++;",
            "",
            "\tif (bucket_cnt && !locked) {",
            "\t\tlocked = true;",
            "\t\tgoto again_nocopy;",
            "\t}",
            "",
            "\tif (bucket_cnt > (max_count - total)) {",
            "\t\tif (total == 0)",
            "\t\t\tret = -ENOSPC;",
            "\t\t/* Note that since bucket_cnt > 0 here, it is implicit",
            "\t\t * that the locked was grabbed, so release it.",
            "\t\t */",
            "\t\thtab_unlock_bucket(htab, b, batch, flags);",
            "\t\trcu_read_unlock();",
            "\t\tbpf_enable_instrumentation();",
            "\t\tgoto after_loop;",
            "\t}",
            "",
            "\tif (bucket_cnt > bucket_size) {",
            "\t\tbucket_size = bucket_cnt;",
            "\t\t/* Note that since bucket_cnt > 0 here, it is implicit",
            "\t\t * that the locked was grabbed, so release it.",
            "\t\t */",
            "\t\thtab_unlock_bucket(htab, b, batch, flags);",
            "\t\trcu_read_unlock();",
            "\t\tbpf_enable_instrumentation();",
            "\t\tkvfree(keys);",
            "\t\tkvfree(values);",
            "\t\tgoto alloc;",
            "\t}",
            "",
            "\t/* Next block is only safe to run if you have grabbed the lock */",
            "\tif (!locked)",
            "\t\tgoto next_batch;",
            "",
            "\thlist_nulls_for_each_entry_safe(l, n, head, hash_node) {",
            "\t\tmemcpy(dst_key, l->key, key_size);",
            "",
            "\t\tif (is_percpu) {",
            "\t\t\tint off = 0, cpu;",
            "\t\t\tvoid __percpu *pptr;",
            "",
            "\t\t\tpptr = htab_elem_get_ptr(l, map->key_size);",
            "\t\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\t\tcopy_map_value_long(&htab->map, dst_val + off, per_cpu_ptr(pptr, cpu));",
            "\t\t\t\tcheck_and_init_map_value(&htab->map, dst_val + off);",
            "\t\t\t\toff += size;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tvalue = l->key + roundup_key_size;",
            "\t\t\tif (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {",
            "\t\t\t\tstruct bpf_map **inner_map = value;",
            "",
            "\t\t\t\t /* Actual value is the id of the inner map */",
            "\t\t\t\tmap_id = map->ops->map_fd_sys_lookup_elem(*inner_map);",
            "\t\t\t\tvalue = &map_id;",
            "\t\t\t}",
            "",
            "\t\t\tif (elem_map_flags & BPF_F_LOCK)",
            "\t\t\t\tcopy_map_value_locked(map, dst_val, value,",
            "\t\t\t\t\t\t      true);",
            "\t\t\telse",
            "\t\t\t\tcopy_map_value(map, dst_val, value);",
            "\t\t\t/* Zeroing special fields in the temp buffer */",
            "\t\t\tcheck_and_init_map_value(map, dst_val);",
            "\t\t}",
            "\t\tif (do_delete) {",
            "\t\t\thlist_nulls_del_rcu(&l->hash_node);",
            "",
            "\t\t\t/* bpf_lru_push_free() will acquire lru_lock, which",
            "\t\t\t * may cause deadlock. See comments in function",
            "\t\t\t * prealloc_lru_pop(). Let us do bpf_lru_push_free()",
            "\t\t\t * after releasing the bucket lock.",
            "\t\t\t *",
            "\t\t\t * For htab of maps, htab_put_fd_value() in",
            "\t\t\t * free_htab_elem() may acquire a spinlock with bucket",
            "\t\t\t * lock being held and it violates the lock rule, so",
            "\t\t\t * invoke free_htab_elem() after unlock as well.",
            "\t\t\t */",
            "\t\t\tl->batch_flink = node_to_free;",
            "\t\t\tnode_to_free = l;",
            "\t\t}",
            "\t\tdst_key += key_size;",
            "\t\tdst_val += value_size;",
            "\t}",
            "",
            "\thtab_unlock_bucket(htab, b, batch, flags);",
            "\tlocked = false;",
            "",
            "\twhile (node_to_free) {",
            "\t\tl = node_to_free;",
            "\t\tnode_to_free = node_to_free->batch_flink;",
            "\t\tif (is_lru_map)",
            "\t\t\thtab_lru_push_free(htab, l);",
            "\t\telse",
            "\t\t\tfree_htab_elem(htab, l);",
            "\t}",
            "",
            "next_batch:",
            "\t/* If we are not copying data, we can go to next bucket and avoid",
            "\t * unlocking the rcu.",
            "\t */",
            "\tif (!bucket_cnt && (batch + 1 < htab->n_buckets)) {",
            "\t\tbatch++;",
            "\t\tgoto again_nocopy;",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "\tbpf_enable_instrumentation();",
            "\tif (bucket_cnt && (copy_to_user(ukeys + total * key_size, keys,",
            "\t    key_size * bucket_cnt) ||",
            "\t    copy_to_user(uvalues + total * value_size, values,",
            "\t    value_size * bucket_cnt))) {",
            "\t\tret = -EFAULT;",
            "\t\tgoto after_loop;",
            "\t}",
            "",
            "\ttotal += bucket_cnt;",
            "\tbatch++;",
            "\tif (batch >= htab->n_buckets) {",
            "\t\tret = -ENOENT;",
            "\t\tgoto after_loop;",
            "\t}",
            "\tgoto again;",
            "",
            "after_loop:",
            "\tif (ret == -EFAULT)",
            "\t\tgoto out;",
            "",
            "\t/* copy # of entries and next batch */",
            "\tubatch = u64_to_user_ptr(attr->batch.out_batch);",
            "\tif (copy_to_user(ubatch, &batch, sizeof(batch)) ||",
            "\t    put_user(total, &uattr->batch.count))",
            "\t\tret = -EFAULT;",
            "",
            "out:",
            "\tkvfree(keys);",
            "\tkvfree(values);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "htab_lru_percpu_map_lookup_and_delete_elem, __htab_map_lookup_and_delete_batch",
          "description": "处理批量查找删除操作，通过RCU读锁遍历指定桶内元素，支持普通/PERCPU/LRU类型。动态分配缓冲区复制键值，处理锁竞争和内存溢出情况，返回操作结果及统计信息。",
          "similarity": 0.5114414691925049
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 132,
          "end_line": 247,
          "content": [
            "static inline bool htab_is_prealloc(const struct bpf_htab *htab)",
            "{",
            "\treturn !(htab->map.map_flags & BPF_F_NO_PREALLOC);",
            "}",
            "static void htab_init_buckets(struct bpf_htab *htab)",
            "{",
            "\tunsigned int i;",
            "",
            "\tfor (i = 0; i < htab->n_buckets; i++) {",
            "\t\tINIT_HLIST_NULLS_HEAD(&htab->buckets[i].head, i);",
            "\t\traw_spin_lock_init(&htab->buckets[i].raw_lock);",
            "\t\tlockdep_set_class(&htab->buckets[i].raw_lock,",
            "\t\t\t\t\t  &htab->lockdep_key);",
            "\t\tcond_resched();",
            "\t}",
            "}",
            "static inline int htab_lock_bucket(const struct bpf_htab *htab,",
            "\t\t\t\t   struct bucket *b, u32 hash,",
            "\t\t\t\t   unsigned long *pflags)",
            "{",
            "\tunsigned long flags;",
            "",
            "\thash = hash & min_t(u32, HASHTAB_MAP_LOCK_MASK, htab->n_buckets - 1);",
            "",
            "\tpreempt_disable();",
            "\tlocal_irq_save(flags);",
            "\tif (unlikely(__this_cpu_inc_return(*(htab->map_locked[hash])) != 1)) {",
            "\t\t__this_cpu_dec(*(htab->map_locked[hash]));",
            "\t\tlocal_irq_restore(flags);",
            "\t\tpreempt_enable();",
            "\t\treturn -EBUSY;",
            "\t}",
            "",
            "\traw_spin_lock(&b->raw_lock);",
            "\t*pflags = flags;",
            "",
            "\treturn 0;",
            "}",
            "static inline void htab_unlock_bucket(const struct bpf_htab *htab,",
            "\t\t\t\t      struct bucket *b, u32 hash,",
            "\t\t\t\t      unsigned long flags)",
            "{",
            "\thash = hash & min_t(u32, HASHTAB_MAP_LOCK_MASK, htab->n_buckets - 1);",
            "\traw_spin_unlock(&b->raw_lock);",
            "\t__this_cpu_dec(*(htab->map_locked[hash]));",
            "\tlocal_irq_restore(flags);",
            "\tpreempt_enable();",
            "}",
            "static bool htab_is_lru(const struct bpf_htab *htab)",
            "{",
            "\treturn htab->map.map_type == BPF_MAP_TYPE_LRU_HASH ||",
            "\t\thtab->map.map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH;",
            "}",
            "static bool htab_is_percpu(const struct bpf_htab *htab)",
            "{",
            "\treturn htab->map.map_type == BPF_MAP_TYPE_PERCPU_HASH ||",
            "\t\thtab->map.map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH;",
            "}",
            "static inline void htab_elem_set_ptr(struct htab_elem *l, u32 key_size,",
            "\t\t\t\t     void __percpu *pptr)",
            "{",
            "\t*(void __percpu **)(l->key + roundup(key_size, 8)) = pptr;",
            "}",
            "static bool htab_has_extra_elems(struct bpf_htab *htab)",
            "{",
            "\treturn !htab_is_percpu(htab) && !htab_is_lru(htab);",
            "}",
            "static void htab_free_prealloced_timers_and_wq(struct bpf_htab *htab)",
            "{",
            "\tu32 num_entries = htab->map.max_entries;",
            "\tint i;",
            "",
            "\tif (htab_has_extra_elems(htab))",
            "\t\tnum_entries += num_possible_cpus();",
            "",
            "\tfor (i = 0; i < num_entries; i++) {",
            "\t\tstruct htab_elem *elem;",
            "",
            "\t\telem = get_htab_elem(htab, i);",
            "\t\tif (btf_record_has_field(htab->map.record, BPF_TIMER))",
            "\t\t\tbpf_obj_free_timer(htab->map.record,",
            "\t\t\t\t\t   elem->key + round_up(htab->map.key_size, 8));",
            "\t\tif (btf_record_has_field(htab->map.record, BPF_WORKQUEUE))",
            "\t\t\tbpf_obj_free_workqueue(htab->map.record,",
            "\t\t\t\t\t       elem->key + round_up(htab->map.key_size, 8));",
            "\t\tcond_resched();",
            "\t}",
            "}",
            "static void htab_free_prealloced_fields(struct bpf_htab *htab)",
            "{",
            "\tu32 num_entries = htab->map.max_entries;",
            "\tint i;",
            "",
            "\tif (IS_ERR_OR_NULL(htab->map.record))",
            "\t\treturn;",
            "\tif (htab_has_extra_elems(htab))",
            "\t\tnum_entries += num_possible_cpus();",
            "\tfor (i = 0; i < num_entries; i++) {",
            "\t\tstruct htab_elem *elem;",
            "",
            "\t\telem = get_htab_elem(htab, i);",
            "\t\tif (htab_is_percpu(htab)) {",
            "\t\t\tvoid __percpu *pptr = htab_elem_get_ptr(elem, htab->map.key_size);",
            "\t\t\tint cpu;",
            "",
            "\t\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\t\tbpf_obj_free_fields(htab->map.record, per_cpu_ptr(pptr, cpu));",
            "\t\t\t\tcond_resched();",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tbpf_obj_free_fields(htab->map.record, elem->key + round_up(htab->map.key_size, 8));",
            "\t\t\tcond_resched();",
            "\t\t}",
            "\t\tcond_resched();",
            "\t}",
            "}"
          ],
          "function_name": "htab_is_prealloc, htab_init_buckets, htab_lock_bucket, htab_unlock_bucket, htab_is_lru, htab_is_percpu, htab_elem_set_ptr, htab_has_extra_elems, htab_free_prealloced_timers_and_wq, htab_free_prealloced_fields",
          "description": "实现哈希表的桶锁管理、预分配检测、LRU/PCPU类型判断等功能，包含桶初始化、锁获取/释放、预分配元素清理等关键逻辑，通过锁保护确保多线程环境下的操作原子性。",
          "similarity": 0.4986407160758972
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 1594,
          "end_line": 1695,
          "content": [
            "static void htab_map_seq_show_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t   struct seq_file *m)",
            "{",
            "\tvoid *value;",
            "",
            "\trcu_read_lock();",
            "",
            "\tvalue = htab_map_lookup_elem(map, key);",
            "\tif (!value) {",
            "\t\trcu_read_unlock();",
            "\t\treturn;",
            "\t}",
            "",
            "\tbtf_type_seq_show(map->btf, map->btf_key_type_id, key, m);",
            "\tseq_puts(m, \": \");",
            "\tbtf_type_seq_show(map->btf, map->btf_value_type_id, value, m);",
            "\tseq_puts(m, \"\\n\");",
            "",
            "\trcu_read_unlock();",
            "}",
            "static int __htab_map_lookup_and_delete_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\t     void *value, bool is_lru_map,",
            "\t\t\t\t\t     bool is_percpu, u64 flags)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tstruct hlist_nulls_head *head;",
            "\tunsigned long bflags;",
            "\tstruct htab_elem *l;",
            "\tu32 hash, key_size;",
            "\tstruct bucket *b;",
            "\tint ret;",
            "",
            "\tkey_size = map->key_size;",
            "",
            "\thash = htab_map_hash(key, key_size, htab->hashrnd);",
            "\tb = __select_bucket(htab, hash);",
            "\thead = &b->head;",
            "",
            "\tret = htab_lock_bucket(htab, b, hash, &bflags);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tl = lookup_elem_raw(head, hash, key, key_size);",
            "\tif (!l) {",
            "\t\tret = -ENOENT;",
            "\t} else {",
            "\t\tif (is_percpu) {",
            "\t\t\tu32 roundup_value_size = round_up(map->value_size, 8);",
            "\t\t\tvoid __percpu *pptr;",
            "\t\t\tint off = 0, cpu;",
            "",
            "\t\t\tpptr = htab_elem_get_ptr(l, key_size);",
            "\t\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\t\tcopy_map_value_long(&htab->map, value + off, per_cpu_ptr(pptr, cpu));",
            "\t\t\t\tcheck_and_init_map_value(&htab->map, value + off);",
            "\t\t\t\toff += roundup_value_size;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tu32 roundup_key_size = round_up(map->key_size, 8);",
            "",
            "\t\t\tif (flags & BPF_F_LOCK)",
            "\t\t\t\tcopy_map_value_locked(map, value, l->key +",
            "\t\t\t\t\t\t      roundup_key_size,",
            "\t\t\t\t\t\t      true);",
            "\t\t\telse",
            "\t\t\t\tcopy_map_value(map, value, l->key +",
            "\t\t\t\t\t       roundup_key_size);",
            "\t\t\t/* Zeroing special fields in the temp buffer */",
            "\t\t\tcheck_and_init_map_value(map, value);",
            "\t\t}",
            "",
            "\t\thlist_nulls_del_rcu(&l->hash_node);",
            "\t\tif (!is_lru_map)",
            "\t\t\tfree_htab_elem(htab, l);",
            "\t}",
            "",
            "\thtab_unlock_bucket(htab, b, hash, bflags);",
            "",
            "\tif (is_lru_map && l)",
            "\t\thtab_lru_push_free(htab, l);",
            "",
            "\treturn ret;",
            "}",
            "static int htab_map_lookup_and_delete_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\t   void *value, u64 flags)",
            "{",
            "\treturn __htab_map_lookup_and_delete_elem(map, key, value, false, false,",
            "\t\t\t\t\t\t flags);",
            "}",
            "static int htab_percpu_map_lookup_and_delete_elem(struct bpf_map *map,",
            "\t\t\t\t\t\t  void *key, void *value,",
            "\t\t\t\t\t\t  u64 flags)",
            "{",
            "\treturn __htab_map_lookup_and_delete_elem(map, key, value, false, true,",
            "\t\t\t\t\t\t flags);",
            "}",
            "static int htab_lru_map_lookup_and_delete_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\t       void *value, u64 flags)",
            "{",
            "\treturn __htab_map_lookup_and_delete_elem(map, key, value, true, false,",
            "\t\t\t\t\t\t flags);",
            "}"
          ],
          "function_name": "htab_map_seq_show_elem, __htab_map_lookup_and_delete_elem, htab_map_lookup_and_delete_elem, htab_percpu_map_lookup_and_delete_elem, htab_lru_map_lookup_and_delete_elem",
          "description": "实现哈希表元素的序列化展示与查找删除操作，支持普通/PERCPU/LRU三种映射类型。通过RCU读锁保护，对找到的元素执行值复制并删除，PERCPU场景下需遍历所有CPU复制值。",
          "similarity": 0.49253660440444946
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 2345,
          "end_line": 2464,
          "content": [
            "static int htab_percpu_map_gen_lookup(struct bpf_map *map, struct bpf_insn *insn_buf)",
            "{",
            "\tstruct bpf_insn *insn = insn_buf;",
            "",
            "\tif (!bpf_jit_supports_percpu_insn())",
            "\t\treturn -EOPNOTSUPP;",
            "",
            "\tBUILD_BUG_ON(!__same_type(&__htab_map_lookup_elem,",
            "\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));",
            "\t*insn++ = BPF_EMIT_CALL(__htab_map_lookup_elem);",
            "\t*insn++ = BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 3);",
            "\t*insn++ = BPF_ALU64_IMM(BPF_ADD, BPF_REG_0,",
            "\t\t\t\toffsetof(struct htab_elem, key) + roundup(map->key_size, 8));",
            "\t*insn++ = BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_0, 0);",
            "\t*insn++ = BPF_MOV64_PERCPU_REG(BPF_REG_0, BPF_REG_0);",
            "",
            "\treturn insn - insn_buf;",
            "}",
            "int bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value)",
            "{",
            "\tstruct htab_elem *l;",
            "\tvoid __percpu *pptr;",
            "\tint ret = -ENOENT;",
            "\tint cpu, off = 0;",
            "\tu32 size;",
            "",
            "\t/* per_cpu areas are zero-filled and bpf programs can only",
            "\t * access 'value_size' of them, so copying rounded areas",
            "\t * will not leak any kernel data",
            "\t */",
            "\tsize = round_up(map->value_size, 8);",
            "\trcu_read_lock();",
            "\tl = __htab_map_lookup_elem(map, key);",
            "\tif (!l)",
            "\t\tgoto out;",
            "\t/* We do not mark LRU map element here in order to not mess up",
            "\t * eviction heuristics when user space does a map walk.",
            "\t */",
            "\tpptr = htab_elem_get_ptr(l, map->key_size);",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tcopy_map_value_long(map, value + off, per_cpu_ptr(pptr, cpu));",
            "\t\tcheck_and_init_map_value(map, value + off);",
            "\t\toff += size;",
            "\t}",
            "\tret = 0;",
            "out:",
            "\trcu_read_unlock();",
            "\treturn ret;",
            "}",
            "int bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,",
            "\t\t\t   u64 map_flags)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tint ret;",
            "",
            "\trcu_read_lock();",
            "\tif (htab_is_lru(htab))",
            "\t\tret = __htab_lru_percpu_map_update_elem(map, key, value,",
            "\t\t\t\t\t\t\tmap_flags, true);",
            "\telse",
            "\t\tret = __htab_percpu_map_update_elem(map, key, value, map_flags,",
            "\t\t\t\t\t\t    true);",
            "\trcu_read_unlock();",
            "",
            "\treturn ret;",
            "}",
            "static void htab_percpu_map_seq_show_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\t  struct seq_file *m)",
            "{",
            "\tstruct htab_elem *l;",
            "\tvoid __percpu *pptr;",
            "\tint cpu;",
            "",
            "\trcu_read_lock();",
            "",
            "\tl = __htab_map_lookup_elem(map, key);",
            "\tif (!l) {",
            "\t\trcu_read_unlock();",
            "\t\treturn;",
            "\t}",
            "",
            "\tbtf_type_seq_show(map->btf, map->btf_key_type_id, key, m);",
            "\tseq_puts(m, \": {\\n\");",
            "\tpptr = htab_elem_get_ptr(l, map->key_size);",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tseq_printf(m, \"\\tcpu%d: \", cpu);",
            "\t\tbtf_type_seq_show(map->btf, map->btf_value_type_id,",
            "\t\t\t\t  per_cpu_ptr(pptr, cpu), m);",
            "\t\tseq_puts(m, \"\\n\");",
            "\t}",
            "\tseq_puts(m, \"}\\n\");",
            "",
            "\trcu_read_unlock();",
            "}",
            "static int fd_htab_map_alloc_check(union bpf_attr *attr)",
            "{",
            "\tif (attr->value_size != sizeof(u32))",
            "\t\treturn -EINVAL;",
            "\treturn htab_map_alloc_check(attr);",
            "}",
            "static void fd_htab_map_free(struct bpf_map *map)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tstruct hlist_nulls_node *n;",
            "\tstruct hlist_nulls_head *head;",
            "\tstruct htab_elem *l;",
            "\tint i;",
            "",
            "\tfor (i = 0; i < htab->n_buckets; i++) {",
            "\t\thead = select_bucket(htab, i);",
            "",
            "\t\thlist_nulls_for_each_entry_safe(l, n, head, hash_node) {",
            "\t\t\tvoid *ptr = fd_htab_map_get_ptr(map, l);",
            "",
            "\t\t\tmap->ops->map_fd_put_ptr(map, ptr, false);",
            "\t\t}",
            "\t}",
            "",
            "\thtab_map_free(map);",
            "}"
          ],
          "function_name": "htab_percpu_map_gen_lookup, bpf_percpu_hash_copy, bpf_percpu_hash_update, htab_percpu_map_seq_show_elem, fd_htab_map_alloc_check, fd_htab_map_free",
          "description": "生成PERCPU哈希表查找指令，实现值复制/更新操作，支持序列化展示多CPU值。包含资源分配校验和释放函数，确保PERCPU映射生命周期管理及内存安全。",
          "similarity": 0.4678995609283447
        }
      ]
    },
    {
      "source_file": "kernel/time/tick-internal.h",
      "md_summary": "> 自动生成时间: 2025-10-25 16:50:12\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `time\\tick-internal.h`\n\n---\n\n# `time/tick-internal.h` 技术文档\n\n## 1. 文件概述\n\n`tick-internal.h` 是 Linux 内核时间子系统中的一个内部头文件，主要用于定义与 **tick（时钟滴答）管理** 相关的内部变量、函数原型和辅助宏。该文件为高精度定时器（high-resolution timers）和低分辨率周期性 tick 提供统一的底层支持，是 `tick` 子系统在周期模式（periodic）和单次触发模式（oneshot）之间切换、广播机制（broadcast）、NO_HZ（动态 tick）等功能的核心接口定义文件。\n\n该头文件仅供内核时间子系统内部使用，不对外暴露给其他子系统直接调用。\n\n## 2. 核心功能\n\n### 2.1 全局变量\n\n- `DECLARE_PER_CPU(struct tick_device, tick_cpu_device)`  \n  每个 CPU 上的 tick 设备实例，封装了底层 `clock_event_device`。\n- `extern ktime_t tick_next_period`  \n  下一个周期性 tick 的绝对时间点。\n- `extern int tick_do_timer_cpu __read_mostly`  \n  指定哪个 CPU 负责全局时间更新（如 jiffies 更新），特殊值：\n  - `TICK_DO_TIMER_NONE (-1)`：无 CPU 负责\n  - `TICK_DO_TIMER_BOOT (-2)`：启动阶段使用\n- `extern unsigned long tick_nohz_active`（条件编译）  \n  标识 NO_HZ 模式是否激活。\n- `DECLARE_PER_CPU(struct hrtimer_cpu_base, hrtimer_bases)`  \n  每个 CPU 的高精度定时器基结构。\n\n### 2.2 主要函数（按功能分类）\n\n#### 周期性 Tick 管理（`CONFIG_GENERIC_CLOCKEVENTS`）\n- `tick_setup_periodic()`：配置设备为周期模式\n- `tick_handle_periodic()`：周期 tick 的事件处理函数\n- `tick_check_new_device()`：检查并注册新的 clock event 设备\n- `tick_shutdown()`：CPU 离线时关闭 tick\n- `tick_suspend()` / `tick_resume()`：系统挂起/恢复时的 tick 处理\n- `tick_install_replacement()`：替换当前 tick 设备\n- `tick_get_device()`：获取指定 CPU 的 tick 设备\n\n#### 单次触发模式（`CONFIG_TICK_ONESHOT`）\n- `tick_setup_oneshot()`：配置设备为 oneshot 模式\n- `tick_program_event()`：编程下一次事件\n- `tick_oneshot_notify()`：通知 oneshot 事件发生\n- `tick_switch_to_oneshot()`：切换到 oneshot 模式\n- `tick_resume_oneshot()`：恢复 oneshot 模式\n- `tick_oneshot_mode_active()`：检查是否处于 oneshot 模式\n- `tick_check_oneshot_change()`：检查是否可切换到 oneshot（用于 NO_HZ）\n\n#### 广播支持（`CONFIG_GENERIC_CLOCKEVENTS_BROADCAST`）\n- `tick_install_broadcast_device()`：安装广播设备\n- `tick_device_uses_broadcast()`：判断设备是否依赖广播\n- `tick_suspend_broadcast()` / `tick_resume_broadcast()`：广播设备的挂起/恢复\n- `tick_get_broadcast_device()` / `tick_get_broadcast_mask()`：获取广播设备和 CPU 掩码\n- `tick_set_periodic_handler()`：设置周期处理函数（区分广播/本地）\n\n#### Oneshot 广播（`BROADCAST && ONESHOT`）\n- `tick_broadcast_switch_to_oneshot()`：广播设备切换到 oneshot\n- `tick_broadcast_oneshot_active()`：检查广播 oneshot 是否激活\n- `tick_get_broadcast_oneshot_mask()`：获取使用 oneshot 广播的 CPU 掩码\n\n#### NO_HZ 支持\n- `tick_nohz_init()`：初始化 NO_HZ_FULL 功能\n- `timers_update_nohz()`：更新 NO_HZ 状态对定时器的影响\n- `timer_clear_idle()`：清除 CPU 空闲状态（用于中断唤醒）\n\n#### 时钟设置通知\n- `clock_was_set()` / `clock_was_set_delayed()`：通知系统时钟被修改\n- `hrtimers_resume_local()`：本地恢复高精度定时器\n- `get_next_timer_interrupt()`：获取下一个定时器中断时间\n\n#### 辅助函数\n- `tick_device_is_functional()`：判断设备是否为有效设备（非 DUMMY）\n- `clockevent_get_state()` / `clockevent_set_state()`：安全访问设备状态\n- `clockevents_shutdown()` / `clockevents_switch_state()`：管理 clock event 设备状态\n- `__clockevents_update_freq()`：更新设备频率\n\n## 3. 关键实现\n\n### 3.1 Tick 设备抽象\n通过 `struct tick_device` 封装 `clock_event_device`，实现 tick 逻辑与底层硬件解耦。每个 CPU 拥有独立的 `tick_cpu_device`，支持 per-CPU tick 管理。\n\n### 3.2 全局 Tick 责任分配\n`tick_do_timer_cpu` 用于指定唯一一个负责更新全局时间（如 `jiffies`）的 CPU，避免多核竞争。在 NO_HZ 或 CPU hotplug 场景下动态迁移。\n\n### 3.3 广播机制\n当某些 CPU 的本地 timer 在深度睡眠时失效，系统使用一个“广播设备”（通常为 HPET 或全局 timer）向所有 CPU 发送中断。通过 `tick_get_broadcast_mask()` 跟踪哪些 CPU 需要广播。\n\n### 3.4 Oneshot 与周期模式切换\n- 周期模式：固定间隔触发，用于传统 tick\n- Oneshot 模式：每次编程下一次事件时间，用于高精度定时和 NO_HZ\n- `tick_check_oneshot_change()` 决定是否可安全切换到 oneshot（需无活跃周期 timer）\n\n### 3.5 NO_HZ 支持\n- `tick_nohz_active` 全局标志控制动态 tick\n- `timers_update_nohz()` 在 NO_HZ 状态变化时调整定时器行为\n- `timer_clear_idle()` 用于中断唤醒时退出空闲状态\n\n### 3.6 时钟修改通知\n`CLOCK_SET_WALL` 和 `CLOCK_SET_BOOT` 定义了受系统时钟修改影响的 hrtimer 基类型。`clock_was_set()` 通知这些基重新计算到期时间。\n\n### 3.7 Jiffies 精度与 NTP\n通过 `JIFFIES_SHIFT`（通常为 8）确保 jiffies 的 NTP 调整精度，避免 HZ 较低时 32 位溢出。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/hrtimer.h>`：高精度定时器接口\n  - `<linux/tick.h>`：tick 子系统公共接口\n  - `\"timekeeping.h\"`：时间维护核心逻辑\n  - `\"tick-sched.h\"`：tick 调度相关结构\n\n- **配置依赖**：\n  - `CONFIG_GENERIC_CLOCKEVENTS`：通用 clock event 框架\n  - `CONFIG_TICK_ONESHOT`：单次触发 tick 支持\n  - `CONFIG_GENERIC_CLOCKEVENTS_BROADCAST`：广播 tick 支持\n  - `CONFIG_NO_HZ_COMMON` / `CONFIG_NO_HZ_FULL`：动态 tick 支持\n  - `CONFIG_HOTPLUG_CPU`：CPU 热插拔对广播的影响\n\n- **模块交互**：\n  - 与 `clockevents` 子系统紧密耦合（设备注册、状态管理）\n  - 被 `tick-sched.c`、`tick-broadcast.c`、`tick-oneshot.c` 等源文件包含\n  - 为 `hrtimer` 和 `timekeeping` 提供底层 tick 支持\n\n## 5. 使用场景\n\n1. **系统启动初始化**  \n   `tick_broadcast_init()` 初始化广播机制；`tick_nohz_init()` 初始化 NO_HZ。\n\n2. **CPU 热插拔**  \n   `tick_shutdown()` 在 CPU offline 时清理资源；`tick_broadcast_offline()` 处理广播掩码更新。\n\n3. **电源管理（挂起/恢复）**  \n   `tick_suspend()` / `tick_resume()` 保存和恢复 tick 状态；广播版本处理全局设备。\n\n4. **动态 Tick（NO_HZ）**  \n   当系统空闲时，通过 `tick_switch_to_oneshot()` 切换到 oneshot 模式停止周期 tick；`tick_oneshot_notify()` 在事件发生时唤醒。\n\n5. **高精度定时器启用**  \n   `tick_init_highres()` 尝试切换到高精度模式，依赖 oneshot 能力。\n\n6. **时钟修改处理**  \n   当用户通过 `settimeofday()` 修改系统时间时，调用 `clock_was_set()` 通知 hrtimer 重新调度。\n\n7. **多核系统广播**  \n   在 CPU 进入 C3+ 睡眠状态时，本地 timer 停止，依赖广播设备维持 tick 中断。",
      "similarity": 0.5195848941802979,
      "chunks": []
    }
  ]
}