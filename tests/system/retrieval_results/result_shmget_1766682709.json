{
  "query": "shmget",
  "timestamp": "2025-12-26 01:11:49",
  "retrieved_files": [
    {
      "source_file": "mm/shmem.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:17:29\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `shmem.c`\n\n---\n\n# shmem.c 技术文档\n\n## 1. 文件概述\n\n`shmem.c` 实现了 Linux 内核中的 **共享内存虚拟文件系统（tmpfs）**，它基于 `ramfs` 扩展而来，支持使用交换空间（swap）并遵守资源限制，从而成为一个完全可用的内存文件系统。该文件系统用于实现 POSIX 共享内存、匿名映射（如 `/dev/zero`）、`memfd_create()` 创建的内存文件以及 tmpfs 挂载点（如 `/tmp` 或 `/dev/shm`）。其核心特点是：数据存储在内存中，可被换出到 swap，支持稀疏文件，并受内存和 inode 配额限制。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct shmem_falloc`：用于 `fallocate` 操作与缺页处理之间的通信，记录预分配范围、已分配页数等。\n- `struct shmem_options`：解析挂载选项（如 size、nr_inodes、huge、uid/gid 等）时使用的临时结构。\n- `struct shmem_sb_info`：超级块私有信息，包含块/ inode 配额、内存策略、配额计数器等。\n- `struct shmem_inode_info`：inode 私有信息，扩展标准 inode 以支持共享内存特性。\n\n### 关键函数\n- `shmem_acct_size()` / `shmem_unacct_size()`：对固定大小 VM 对象进行内存预占（如共享内存映射）。\n- `shmem_acct_blocks()` / `shmem_unacct_blocks()`：对 tmpfs 稀疏文件按实际分配页进行内存核算。\n- `shmem_inode_acct_blocks()` / `shmem_inode_unacct_blocks()`：结合文件系统配额（`max_blocks`）和磁盘配额（`dquot`）进行块分配/释放。\n- `shmem_swapin_folio()`：从 swap 中换入指定页。\n- `vma_is_anon_shmem()`：判断 VMA 是否为匿名共享内存映射。\n- `SHMEM_SB()`：宏，快速获取超级块的 `shmem_sb_info`。\n\n### 全局操作结构体\n- `shmem_ops`：超级块操作（如 `statfs`、`put_super`）。\n- `shmem_aops`：地址空间操作（如 `readpage`、`writepage`、`set_page_dirty`）。\n- `shmem_file_operations`：文件操作（如 `read`、`write`、`mmap`）。\n- `shmem_inode_operations`：普通文件 inode 操作。\n- `shmem_dir_inode_operations`：目录 inode 操作。\n- `shmem_special_inode_operations`：特殊文件（设备、socket）inode 操作。\n- `shmem_vm_ops` / `shmem_anon_vm_ops`：VMA 操作结构体，分别用于 tmpfs 文件映射和匿名共享内存映射。\n\n## 3. 关键实现\n\n### 内存核算机制\n- **预占模式（Pre-accounting）**：用于 `shmem_file_setup()` 创建的固定大小对象（如 POSIX 共享内存），在创建时即核算全部内存（通过 `shmem_acct_size`），避免运行时 OOM。\n- **增量核算（Incremental accounting）**：用于 tmpfs 文件，仅在实际分配页面时核算（通过 `shmem_acct_blocks`），支持大稀疏文件。失败返回 `-ENOSPC` 而非 `-ENOMEM`，使用户态收到 `SIGBUS` 而非触发 OOM killer。\n\n### 配额管理\n- 使用 `percpu_counter` 高效跟踪已用块数（`used_blocks`），并与挂载时指定的 `max_blocks` 限制比较。\n- 集成内核通用配额子系统（`dquot_alloc_block_nodirty` / `dquot_free_block_nodirty`），支持用户/组配额。\n\n### 大页（Huge Page）支持\n- 通过 `huge` 挂载选项和 `madvise(MADV_HUGEPAGE)` 控制透明大页（THP）行为。\n- 维护多个位图（`huge_shmem_orders_*`）记录不同场景下允许的大页阶数。\n\n### fallocate 与缺页协同\n- `shmem_falloc` 结构通过 `inode->i_private` 在 `fallocate` 和 `shmem_fault`/`shmem_writepage` 之间传递状态。\n- 使用等待队列（`waitq`）确保在 punch hole 操作期间，访问空洞的缺页请求会等待操作完成。\n\n### 匿名 vs 命名共享内存\n- **匿名共享内存**：由 `shmem_zero_setup()` 创建（如 `/dev/zero` 映射），使用 `shmem_anon_vm_ops`。\n- **命名共享内存**：通过 tmpfs 文件系统接口创建（如 `shm_open()`），使用 `shmem_vm_ops`。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `mm/` 中的页分配、swap、rmap、mempolicy、hugetlb 等机制。\n- **VFS 层**：实现标准文件系统接口（`super_operations`, `inode_operations` 等）。\n- **安全模块**：调用 LSM 钩子（`security_vm_enough_memory_mm`）进行内存安全检查。\n- **配额子系统**：通过 `dquot_*` 函数集成磁盘配额功能。\n- **swap 子系统**：通过 `swap.h` 和 `swapops.h` 实现页面换入换出。\n- **其他**：依赖 `ramfs` 基础结构、`xattr`、`posix_acl`、`splice`、`falloc` 等通用内核组件。\n\n## 5. 使用场景\n\n- **POSIX 共享内存**：`shm_open()` / `shm_unlink()` 创建的共享内存对象。\n- **System V 共享内存**：`shmget()` / `shmat()` 使用的底层存储。\n- **匿名映射**：`mmap()` 映射 `/dev/zero` 或 `MAP_ANONYMOUS | MAP_SHARED` 创建的共享内存区域。\n- **tmpfs 文件系统**：挂载 tmpfs（如 `/dev/shm`）后创建的文件和目录。\n- **memfd 文件**：通过 `memfd_create()` 系统调用创建的匿名内存文件，支持密封（sealing）和共享。\n- **内核内部用途**：作为某些需要临时可换出内存缓冲区的子系统的后端存储。",
      "similarity": 0.49280261993408203,
      "chunks": [
        {
          "chunk_id": 16,
          "file_path": "mm/shmem.c",
          "start_line": 2837,
          "end_line": 3022,
          "content": [
            "static int shmem_set_policy(struct vm_area_struct *vma, struct mempolicy *mpol)",
            "{",
            "\tstruct inode *inode = file_inode(vma->vm_file);",
            "\treturn mpol_set_shared_policy(&SHMEM_I(inode)->policy, vma, mpol);",
            "}",
            "int shmem_lock(struct file *file, int lock, struct ucounts *ucounts)",
            "{",
            "\tstruct inode *inode = file_inode(file);",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "\tint retval = -ENOMEM;",
            "",
            "\t/*",
            "\t * What serializes the accesses to info->flags?",
            "\t * ipc_lock_object() when called from shmctl_do_lock(),",
            "\t * no serialization needed when called from shm_destroy().",
            "\t */",
            "\tif (lock && !(info->flags & VM_LOCKED)) {",
            "\t\tif (!user_shm_lock(inode->i_size, ucounts))",
            "\t\t\tgoto out_nomem;",
            "\t\tinfo->flags |= VM_LOCKED;",
            "\t\tmapping_set_unevictable(file->f_mapping);",
            "\t}",
            "\tif (!lock && (info->flags & VM_LOCKED) && ucounts) {",
            "\t\tuser_shm_unlock(inode->i_size, ucounts);",
            "\t\tinfo->flags &= ~VM_LOCKED;",
            "\t\tmapping_clear_unevictable(file->f_mapping);",
            "\t}",
            "\tretval = 0;",
            "",
            "out_nomem:",
            "\treturn retval;",
            "}",
            "static int shmem_mmap(struct file *file, struct vm_area_struct *vma)",
            "{",
            "\tstruct inode *inode = file_inode(file);",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "\tint ret;",
            "",
            "\tret = seal_check_write(info->seals, vma);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tfile_accessed(file);",
            "\t/* This is anonymous shared memory if it is unlinked at the time of mmap */",
            "\tif (inode->i_nlink)",
            "\t\tvma->vm_ops = &shmem_vm_ops;",
            "\telse",
            "\t\tvma->vm_ops = &shmem_anon_vm_ops;",
            "\treturn 0;",
            "}",
            "static int shmem_file_open(struct inode *inode, struct file *file)",
            "{",
            "\tfile->f_mode |= FMODE_CAN_ODIRECT;",
            "\treturn generic_file_open(inode, file);",
            "}",
            "static void shmem_set_inode_flags(struct inode *inode, unsigned int fsflags)",
            "{",
            "\tunsigned int i_flags = 0;",
            "",
            "\tif (fsflags & FS_NOATIME_FL)",
            "\t\ti_flags |= S_NOATIME;",
            "\tif (fsflags & FS_APPEND_FL)",
            "\t\ti_flags |= S_APPEND;",
            "\tif (fsflags & FS_IMMUTABLE_FL)",
            "\t\ti_flags |= S_IMMUTABLE;",
            "\t/*",
            "\t * But FS_NODUMP_FL does not require any action in i_flags.",
            "\t */",
            "\tinode_set_flags(inode, i_flags, S_NOATIME | S_APPEND | S_IMMUTABLE);",
            "}",
            "static void shmem_set_inode_flags(struct inode *inode, unsigned int fsflags)",
            "{",
            "}",
            "int shmem_mfill_atomic_pte(pmd_t *dst_pmd,",
            "\t\t\t   struct vm_area_struct *dst_vma,",
            "\t\t\t   unsigned long dst_addr,",
            "\t\t\t   unsigned long src_addr,",
            "\t\t\t   uffd_flags_t flags,",
            "\t\t\t   struct folio **foliop)",
            "{",
            "\tstruct inode *inode = file_inode(dst_vma->vm_file);",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "\tstruct address_space *mapping = inode->i_mapping;",
            "\tgfp_t gfp = mapping_gfp_mask(mapping);",
            "\tpgoff_t pgoff = linear_page_index(dst_vma, dst_addr);",
            "\tvoid *page_kaddr;",
            "\tstruct folio *folio;",
            "\tint ret;",
            "\tpgoff_t max_off;",
            "",
            "\tif (shmem_inode_acct_blocks(inode, 1)) {",
            "\t\t/*",
            "\t\t * We may have got a page, returned -ENOENT triggering a retry,",
            "\t\t * and now we find ourselves with -ENOMEM. Release the page, to",
            "\t\t * avoid a BUG_ON in our caller.",
            "\t\t */",
            "\t\tif (unlikely(*foliop)) {",
            "\t\t\tfolio_put(*foliop);",
            "\t\t\t*foliop = NULL;",
            "\t\t}",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\tif (!*foliop) {",
            "\t\tret = -ENOMEM;",
            "\t\tfolio = shmem_alloc_folio(gfp, 0, info, pgoff);",
            "\t\tif (!folio)",
            "\t\t\tgoto out_unacct_blocks;",
            "",
            "\t\tif (uffd_flags_mode_is(flags, MFILL_ATOMIC_COPY)) {",
            "\t\t\tpage_kaddr = kmap_local_folio(folio, 0);",
            "\t\t\t/*",
            "\t\t\t * The read mmap_lock is held here.  Despite the",
            "\t\t\t * mmap_lock being read recursive a deadlock is still",
            "\t\t\t * possible if a writer has taken a lock.  For example:",
            "\t\t\t *",
            "\t\t\t * process A thread 1 takes read lock on own mmap_lock",
            "\t\t\t * process A thread 2 calls mmap, blocks taking write lock",
            "\t\t\t * process B thread 1 takes page fault, read lock on own mmap lock",
            "\t\t\t * process B thread 2 calls mmap, blocks taking write lock",
            "\t\t\t * process A thread 1 blocks taking read lock on process B",
            "\t\t\t * process B thread 1 blocks taking read lock on process A",
            "\t\t\t *",
            "\t\t\t * Disable page faults to prevent potential deadlock",
            "\t\t\t * and retry the copy outside the mmap_lock.",
            "\t\t\t */",
            "\t\t\tpagefault_disable();",
            "\t\t\tret = copy_from_user(page_kaddr,",
            "\t\t\t\t\t     (const void __user *)src_addr,",
            "\t\t\t\t\t     PAGE_SIZE);",
            "\t\t\tpagefault_enable();",
            "\t\t\tkunmap_local(page_kaddr);",
            "",
            "\t\t\t/* fallback to copy_from_user outside mmap_lock */",
            "\t\t\tif (unlikely(ret)) {",
            "\t\t\t\t*foliop = folio;",
            "\t\t\t\tret = -ENOENT;",
            "\t\t\t\t/* don't free the page */",
            "\t\t\t\tgoto out_unacct_blocks;",
            "\t\t\t}",
            "",
            "\t\t\tflush_dcache_folio(folio);",
            "\t\t} else {\t\t/* ZEROPAGE */",
            "\t\t\tclear_user_highpage(&folio->page, dst_addr);",
            "\t\t}",
            "\t} else {",
            "\t\tfolio = *foliop;",
            "\t\tVM_BUG_ON_FOLIO(folio_test_large(folio), folio);",
            "\t\t*foliop = NULL;",
            "\t}",
            "",
            "\tVM_BUG_ON(folio_test_locked(folio));",
            "\tVM_BUG_ON(folio_test_swapbacked(folio));",
            "\t__folio_set_locked(folio);",
            "\t__folio_set_swapbacked(folio);",
            "\t__folio_mark_uptodate(folio);",
            "",
            "\tret = -EFAULT;",
            "\tmax_off = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);",
            "\tif (unlikely(pgoff >= max_off))",
            "\t\tgoto out_release;",
            "",
            "\tret = mem_cgroup_charge(folio, dst_vma->vm_mm, gfp);",
            "\tif (ret)",
            "\t\tgoto out_release;",
            "\tret = shmem_add_to_page_cache(folio, mapping, pgoff, NULL, gfp);",
            "\tif (ret)",
            "\t\tgoto out_release;",
            "",
            "\tret = mfill_atomic_install_pte(dst_pmd, dst_vma, dst_addr,",
            "\t\t\t\t       &folio->page, true, flags);",
            "\tif (ret)",
            "\t\tgoto out_delete_from_cache;",
            "",
            "\tshmem_recalc_inode(inode, 1, 0);",
            "\tfolio_unlock(folio);",
            "\treturn 0;",
            "out_delete_from_cache:",
            "\tfilemap_remove_folio(folio);",
            "out_release:",
            "\tfolio_unlock(folio);",
            "\tfolio_put(folio);",
            "out_unacct_blocks:",
            "\tshmem_inode_unacct_blocks(inode, 1);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "shmem_set_policy, shmem_lock, shmem_mmap, shmem_file_open, shmem_set_inode_flags, shmem_set_inode_flags, shmem_mfill_atomic_pte",
          "description": "提供共享内存策略设置、锁操作、mmap映射配置、文件打开及原子页填充功能，包含shmem_set_policy设置内存策略，shmem_lock管理内存锁定，shmem_mmap配置虚拟内存区域，shmem_mfill_atomic_pte执行原子页填充",
          "similarity": 0.5497984290122986
        },
        {
          "chunk_id": 14,
          "file_path": "mm/shmem.c",
          "start_line": 2441,
          "end_line": 2615,
          "content": [
            "static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,",
            "\t\tloff_t write_end, struct folio **foliop, enum sgp_type sgp,",
            "\t\tgfp_t gfp, struct vm_fault *vmf, vm_fault_t *fault_type)",
            "{",
            "\tstruct vm_area_struct *vma = vmf ? vmf->vma : NULL;",
            "\tstruct mm_struct *fault_mm;",
            "\tstruct folio *folio;",
            "\tint error;",
            "\tbool alloced;",
            "\tunsigned long orders = 0;",
            "",
            "\tif (index > (MAX_LFS_FILESIZE >> PAGE_SHIFT))",
            "\t\treturn -EFBIG;",
            "repeat:",
            "\tif (sgp <= SGP_CACHE &&",
            "\t    ((loff_t)index << PAGE_SHIFT) >= i_size_read(inode))",
            "\t\treturn -EINVAL;",
            "",
            "\talloced = false;",
            "\tfault_mm = vma ? vma->vm_mm : NULL;",
            "",
            "\tfolio = filemap_get_entry(inode->i_mapping, index);",
            "\tif (folio && vma && userfaultfd_minor(vma)) {",
            "\t\tif (!xa_is_value(folio))",
            "\t\t\tfolio_put(folio);",
            "\t\t*fault_type = handle_userfault(vmf, VM_UFFD_MINOR);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (xa_is_value(folio)) {",
            "\t\terror = shmem_swapin_folio(inode, index, &folio,",
            "\t\t\t\t\t   sgp, gfp, vma, fault_type);",
            "\t\tif (error == -EEXIST)",
            "\t\t\tgoto repeat;",
            "",
            "\t\t*foliop = folio;",
            "\t\treturn error;",
            "\t}",
            "",
            "\tif (folio) {",
            "\t\tfolio_lock(folio);",
            "",
            "\t\t/* Has the folio been truncated or swapped out? */",
            "\t\tif (unlikely(folio->mapping != inode->i_mapping)) {",
            "\t\t\tfolio_unlock(folio);",
            "\t\t\tfolio_put(folio);",
            "\t\t\tgoto repeat;",
            "\t\t}",
            "\t\tif (sgp == SGP_WRITE)",
            "\t\t\tfolio_mark_accessed(folio);",
            "\t\tif (folio_test_uptodate(folio))",
            "\t\t\tgoto out;",
            "\t\t/* fallocated folio */",
            "\t\tif (sgp != SGP_READ)",
            "\t\t\tgoto clear;",
            "\t\tfolio_unlock(folio);",
            "\t\tfolio_put(folio);",
            "\t}",
            "",
            "\t/*",
            "\t * SGP_READ: succeed on hole, with NULL folio, letting caller zero.",
            "\t * SGP_NOALLOC: fail on hole, with NULL folio, letting caller fail.",
            "\t */",
            "\t*foliop = NULL;",
            "\tif (sgp == SGP_READ)",
            "\t\treturn 0;",
            "\tif (sgp == SGP_NOALLOC)",
            "\t\treturn -ENOENT;",
            "",
            "\t/*",
            "\t * Fast cache lookup and swap lookup did not find it: allocate.",
            "\t */",
            "",
            "\tif (vma && userfaultfd_missing(vma)) {",
            "\t\t*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/* Find hugepage orders that are allowed for anonymous shmem and tmpfs. */",
            "\torders = shmem_allowable_huge_orders(inode, vma, index, write_end, false);",
            "\tif (orders > 0) {",
            "\t\tgfp_t huge_gfp;",
            "",
            "\t\thuge_gfp = vma_thp_gfp_mask(vma);",
            "\t\thuge_gfp = limit_gfp_mask(huge_gfp, gfp);",
            "\t\tfolio = shmem_alloc_and_add_folio(vmf, huge_gfp,",
            "\t\t\t\tinode, index, fault_mm, orders);",
            "\t\tif (!IS_ERR(folio)) {",
            "\t\t\tif (folio_test_pmd_mappable(folio))",
            "\t\t\t\tcount_vm_event(THP_FILE_ALLOC);",
            "\t\t\tcount_mthp_stat(folio_order(folio), MTHP_STAT_SHMEM_ALLOC);",
            "\t\t\tgoto alloced;",
            "\t\t}",
            "\t\tif (PTR_ERR(folio) == -EEXIST)",
            "\t\t\tgoto repeat;",
            "\t}",
            "",
            "\tfolio = shmem_alloc_and_add_folio(vmf, gfp, inode, index, fault_mm, 0);",
            "\tif (IS_ERR(folio)) {",
            "\t\terror = PTR_ERR(folio);",
            "\t\tif (error == -EEXIST)",
            "\t\t\tgoto repeat;",
            "\t\tfolio = NULL;",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "alloced:",
            "\talloced = true;",
            "\tif (folio_test_large(folio) &&",
            "\t    DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE) <",
            "\t\t\t\t\tfolio_next_index(folio) - 1) {",
            "\t\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);",
            "\t\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "\t\t/*",
            "\t\t * Part of the large folio is beyond i_size: subject",
            "\t\t * to shrink under memory pressure.",
            "\t\t */",
            "\t\tspin_lock(&sbinfo->shrinklist_lock);",
            "\t\t/*",
            "\t\t * _careful to defend against unlocked access to",
            "\t\t * ->shrink_list in shmem_unused_huge_shrink()",
            "\t\t */",
            "\t\tif (list_empty_careful(&info->shrinklist)) {",
            "\t\t\tlist_add_tail(&info->shrinklist,",
            "\t\t\t\t      &sbinfo->shrinklist);",
            "\t\t\tsbinfo->shrinklist_len++;",
            "\t\t}",
            "\t\tspin_unlock(&sbinfo->shrinklist_lock);",
            "\t}",
            "",
            "\tif (sgp == SGP_WRITE)",
            "\t\tfolio_set_referenced(folio);",
            "\t/*",
            "\t * Let SGP_FALLOC use the SGP_WRITE optimization on a new folio.",
            "\t */",
            "\tif (sgp == SGP_FALLOC)",
            "\t\tsgp = SGP_WRITE;",
            "clear:",
            "\t/*",
            "\t * Let SGP_WRITE caller clear ends if write does not fill folio;",
            "\t * but SGP_FALLOC on a folio fallocated earlier must initialize",
            "\t * it now, lest undo on failure cancel our earlier guarantee.",
            "\t */",
            "\tif (sgp != SGP_WRITE && !folio_test_uptodate(folio)) {",
            "\t\tlong i, n = folio_nr_pages(folio);",
            "",
            "\t\tfor (i = 0; i < n; i++)",
            "\t\t\tclear_highpage(folio_page(folio, i));",
            "\t\tflush_dcache_folio(folio);",
            "\t\tfolio_mark_uptodate(folio);",
            "\t}",
            "",
            "\t/* Perhaps the file has been truncated since we checked */",
            "\tif (sgp <= SGP_CACHE &&",
            "\t    ((loff_t)index << PAGE_SHIFT) >= i_size_read(inode)) {",
            "\t\terror = -EINVAL;",
            "\t\tgoto unlock;",
            "\t}",
            "out:",
            "\t*foliop = folio;",
            "\treturn 0;",
            "",
            "\t/*",
            "\t * Error recovery.",
            "\t */",
            "unlock:",
            "\tif (alloced)",
            "\t\tfilemap_remove_folio(folio);",
            "\tshmem_recalc_inode(inode, 0, 0);",
            "\tif (folio) {",
            "\t\tfolio_unlock(folio);",
            "\t\tfolio_put(folio);",
            "\t}",
            "\treturn error;",
            "}"
          ],
          "function_name": "shmem_get_folio_gfp",
          "description": "shmem_get_folio_gfp获取或分配页面，处理读取、写入及预分配场景，集成huge页面支持，检查文件截断并确保页面状态正确，管理页框初始化与回收流程。",
          "similarity": 0.5450640916824341
        },
        {
          "chunk_id": 17,
          "file_path": "mm/shmem.c",
          "start_line": 3202,
          "end_line": 3379,
          "content": [
            "static int",
            "shmem_write_begin(struct file *file, struct address_space *mapping,",
            "\t\t\tloff_t pos, unsigned len,",
            "\t\t\tstruct page **pagep, void **fsdata)",
            "{",
            "\tstruct inode *inode = mapping->host;",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "\tpgoff_t index = pos >> PAGE_SHIFT;",
            "\tstruct folio *folio;",
            "\tint ret = 0;",
            "",
            "\t/* i_rwsem is held by caller */",
            "\tif (unlikely(info->seals & (F_SEAL_GROW |",
            "\t\t\t\t   F_SEAL_WRITE | F_SEAL_FUTURE_WRITE))) {",
            "\t\tif (info->seals & (F_SEAL_WRITE | F_SEAL_FUTURE_WRITE))",
            "\t\t\treturn -EPERM;",
            "\t\tif ((info->seals & F_SEAL_GROW) && pos + len > inode->i_size)",
            "\t\t\treturn -EPERM;",
            "\t}",
            "",
            "\tret = shmem_get_folio(inode, index, pos + len, &folio, SGP_WRITE);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\t*pagep = folio_file_page(folio, index);",
            "\tif (PageHWPoison(*pagep)) {",
            "\t\tfolio_unlock(folio);",
            "\t\tfolio_put(folio);",
            "\t\t*pagep = NULL;",
            "\t\treturn -EIO;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int",
            "shmem_write_end(struct file *file, struct address_space *mapping,",
            "\t\t\tloff_t pos, unsigned len, unsigned copied,",
            "\t\t\tstruct page *page, void *fsdata)",
            "{",
            "\tstruct folio *folio = page_folio(page);",
            "\tstruct inode *inode = mapping->host;",
            "",
            "\tif (pos + copied > inode->i_size)",
            "\t\ti_size_write(inode, pos + copied);",
            "",
            "\tif (!folio_test_uptodate(folio)) {",
            "\t\tif (copied < folio_size(folio)) {",
            "\t\t\tsize_t from = offset_in_folio(folio, pos);",
            "\t\t\tfolio_zero_segments(folio, 0, from,",
            "\t\t\t\t\tfrom + copied, folio_size(folio));",
            "\t\t}",
            "\t\tfolio_mark_uptodate(folio);",
            "\t}",
            "\tfolio_mark_dirty(folio);",
            "\tfolio_unlock(folio);",
            "\tfolio_put(folio);",
            "",
            "\treturn copied;",
            "}",
            "static ssize_t shmem_file_read_iter(struct kiocb *iocb, struct iov_iter *to)",
            "{",
            "\tstruct file *file = iocb->ki_filp;",
            "\tstruct inode *inode = file_inode(file);",
            "\tstruct address_space *mapping = inode->i_mapping;",
            "\tpgoff_t index;",
            "\tunsigned long offset;",
            "\tint error = 0;",
            "\tssize_t retval = 0;",
            "",
            "\tfor (;;) {",
            "\t\tstruct folio *folio = NULL;",
            "\t\tstruct page *page = NULL;",
            "\t\tunsigned long nr, ret;",
            "\t\tloff_t end_offset, i_size = i_size_read(inode);",
            "\t\tbool fallback_page_copy = false;",
            "\t\tsize_t fsize;",
            "",
            "\t\tif (unlikely(iocb->ki_pos >= i_size))",
            "\t\t\tbreak;",
            "",
            "\t\tindex = iocb->ki_pos >> PAGE_SHIFT;",
            "\t\terror = shmem_get_folio(inode, index, 0, &folio, SGP_READ);",
            "\t\tif (error) {",
            "\t\t\tif (error == -EINVAL)",
            "\t\t\t\terror = 0;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tif (folio) {",
            "\t\t\tfolio_unlock(folio);",
            "",
            "\t\t\tpage = folio_file_page(folio, index);",
            "\t\t\tif (PageHWPoison(page)) {",
            "\t\t\t\tfolio_put(folio);",
            "\t\t\t\terror = -EIO;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "",
            "\t\t\tif (folio_test_large(folio) &&",
            "\t\t\t    folio_test_has_hwpoisoned(folio))",
            "\t\t\t\tfallback_page_copy = true;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * We must evaluate after, since reads (unlike writes)",
            "\t\t * are called without i_rwsem protection against truncate",
            "\t\t */",
            "\t\ti_size = i_size_read(inode);",
            "\t\tif (unlikely(iocb->ki_pos >= i_size)) {",
            "\t\t\tif (folio)",
            "\t\t\t\tfolio_put(folio);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tend_offset = min_t(loff_t, i_size, iocb->ki_pos + to->count);",
            "\t\tif (folio && likely(!fallback_page_copy))",
            "\t\t\tfsize = folio_size(folio);",
            "\t\telse",
            "\t\t\tfsize = PAGE_SIZE;",
            "\t\toffset = iocb->ki_pos & (fsize - 1);",
            "\t\tnr = min_t(loff_t, end_offset - iocb->ki_pos, fsize - offset);",
            "",
            "\t\tif (folio) {",
            "\t\t\t/*",
            "\t\t\t * If users can be writing to this page using arbitrary",
            "\t\t\t * virtual addresses, take care about potential aliasing",
            "\t\t\t * before reading the page on the kernel side.",
            "\t\t\t */",
            "\t\t\tif (mapping_writably_mapped(mapping)) {",
            "\t\t\t\tif (likely(!fallback_page_copy))",
            "\t\t\t\t\tflush_dcache_folio(folio);",
            "\t\t\t\telse",
            "\t\t\t\t\tflush_dcache_page(page);",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * Mark the folio accessed if we read the beginning.",
            "\t\t\t */",
            "\t\t\tif (!offset)",
            "\t\t\t\tfolio_mark_accessed(folio);",
            "\t\t\t/*",
            "\t\t\t * Ok, we have the page, and it's up-to-date, so",
            "\t\t\t * now we can copy it to user space...",
            "\t\t\t */",
            "\t\t\tif (likely(!fallback_page_copy))",
            "\t\t\t\tret = copy_folio_to_iter(folio, offset, nr, to);",
            "\t\t\telse",
            "\t\t\t\tret = copy_page_to_iter(page, offset, nr, to);",
            "\t\t\tfolio_put(folio);",
            "\t\t} else if (user_backed_iter(to)) {",
            "\t\t\t/*",
            "\t\t\t * Copy to user tends to be so well optimized, but",
            "\t\t\t * clear_user() not so much, that it is noticeably",
            "\t\t\t * faster to copy the zero page instead of clearing.",
            "\t\t\t */",
            "\t\t\tret = copy_page_to_iter(ZERO_PAGE(0), offset, nr, to);",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * But submitting the same page twice in a row to",
            "\t\t\t * splice() - or others? - can result in confusion:",
            "\t\t\t * so don't attempt that optimization on pipes etc.",
            "\t\t\t */",
            "\t\t\tret = iov_iter_zero(nr, to);",
            "\t\t}",
            "",
            "\t\tretval += ret;",
            "\t\tiocb->ki_pos += ret;",
            "",
            "\t\tif (!iov_iter_count(to))",
            "\t\t\tbreak;",
            "\t\tif (ret < nr) {",
            "\t\t\terror = -EFAULT;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\tfile_accessed(file);",
            "\treturn retval ? retval : error;",
            "}"
          ],
          "function_name": "shmem_write_begin, shmem_write_end, shmem_file_read_iter",
          "description": "实现共享内存读写操作，包含shmem_write_begin准备写入页面，shmem_write_end完成写入并更新文件大小，shmem_file_read_iter执行文件读取迭代操作，处理页面有效性检查和数据复制",
          "similarity": 0.5447816252708435
        },
        {
          "chunk_id": 22,
          "file_path": "mm/shmem.c",
          "start_line": 4055,
          "end_line": 4155,
          "content": [
            "static void shmem_put_link(void *arg)",
            "{",
            "\tfolio_mark_accessed(arg);",
            "\tfolio_put(arg);",
            "}",
            "static int shmem_fileattr_get(struct dentry *dentry, struct fileattr *fa)",
            "{",
            "\tstruct shmem_inode_info *info = SHMEM_I(d_inode(dentry));",
            "",
            "\tfileattr_fill_flags(fa, info->fsflags & SHMEM_FL_USER_VISIBLE);",
            "",
            "\treturn 0;",
            "}",
            "static int shmem_fileattr_set(struct mnt_idmap *idmap,",
            "\t\t\t      struct dentry *dentry, struct fileattr *fa)",
            "{",
            "\tstruct inode *inode = d_inode(dentry);",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "",
            "\tif (fileattr_has_fsx(fa))",
            "\t\treturn -EOPNOTSUPP;",
            "\tif (fa->flags & ~SHMEM_FL_USER_MODIFIABLE)",
            "\t\treturn -EOPNOTSUPP;",
            "",
            "\tinfo->fsflags = (info->fsflags & ~SHMEM_FL_USER_MODIFIABLE) |",
            "\t\t(fa->flags & SHMEM_FL_USER_MODIFIABLE);",
            "",
            "\tshmem_set_inode_flags(inode, info->fsflags);",
            "\tinode_set_ctime_current(inode);",
            "\tinode_inc_iversion(inode);",
            "\treturn 0;",
            "}",
            "static int shmem_initxattrs(struct inode *inode,",
            "\t\t\t    const struct xattr *xattr_array, void *fs_info)",
            "{",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);",
            "\tconst struct xattr *xattr;",
            "\tstruct simple_xattr *new_xattr;",
            "\tsize_t ispace = 0;",
            "\tsize_t len;",
            "",
            "\tif (sbinfo->max_inodes) {",
            "\t\tfor (xattr = xattr_array; xattr->name != NULL; xattr++) {",
            "\t\t\tispace += simple_xattr_space(xattr->name,",
            "\t\t\t\txattr->value_len + XATTR_SECURITY_PREFIX_LEN);",
            "\t\t}",
            "\t\tif (ispace) {",
            "\t\t\traw_spin_lock(&sbinfo->stat_lock);",
            "\t\t\tif (sbinfo->free_ispace < ispace)",
            "\t\t\t\tispace = 0;",
            "\t\t\telse",
            "\t\t\t\tsbinfo->free_ispace -= ispace;",
            "\t\t\traw_spin_unlock(&sbinfo->stat_lock);",
            "\t\t\tif (!ispace)",
            "\t\t\t\treturn -ENOSPC;",
            "\t\t}",
            "\t}",
            "",
            "\tfor (xattr = xattr_array; xattr->name != NULL; xattr++) {",
            "\t\tnew_xattr = simple_xattr_alloc(xattr->value, xattr->value_len);",
            "\t\tif (!new_xattr)",
            "\t\t\tbreak;",
            "",
            "\t\tlen = strlen(xattr->name) + 1;",
            "\t\tnew_xattr->name = kmalloc(XATTR_SECURITY_PREFIX_LEN + len,",
            "\t\t\t\t\t  GFP_KERNEL_ACCOUNT);",
            "\t\tif (!new_xattr->name) {",
            "\t\t\tkvfree(new_xattr);",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tmemcpy(new_xattr->name, XATTR_SECURITY_PREFIX,",
            "\t\t       XATTR_SECURITY_PREFIX_LEN);",
            "\t\tmemcpy(new_xattr->name + XATTR_SECURITY_PREFIX_LEN,",
            "\t\t       xattr->name, len);",
            "",
            "\t\tsimple_xattr_add(&info->xattrs, new_xattr);",
            "\t}",
            "",
            "\tif (xattr->name != NULL) {",
            "\t\tif (ispace) {",
            "\t\t\traw_spin_lock(&sbinfo->stat_lock);",
            "\t\t\tsbinfo->free_ispace += ispace;",
            "\t\t\traw_spin_unlock(&sbinfo->stat_lock);",
            "\t\t}",
            "\t\tsimple_xattrs_free(&info->xattrs, NULL);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int shmem_xattr_handler_get(const struct xattr_handler *handler,",
            "\t\t\t\t   struct dentry *unused, struct inode *inode,",
            "\t\t\t\t   const char *name, void *buffer, size_t size)",
            "{",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "",
            "\tname = xattr_full_name(handler, name);",
            "\treturn simple_xattr_get(&info->xattrs, name, buffer, size);",
            "}"
          ],
          "function_name": "shmem_put_link, shmem_fileattr_get, shmem_fileattr_set, shmem_initxattrs, shmem_xattr_handler_get",
          "description": "管理tmpfs文件的扩展属性与特殊标志位，shmem_fileattr_get/set用于获取/设置文件标志，shmem_initxattrs初始化安全扩展属性，处理空间不足情况，xattr_handler_get提供扩展属性访问接口",
          "similarity": 0.5423840880393982
        },
        {
          "chunk_id": 0,
          "file_path": "mm/shmem.c",
          "start_line": 1,
          "end_line": 142,
          "content": [
            "/*",
            " * Resizable virtual memory filesystem for Linux.",
            " *",
            " * Copyright (C) 2000 Linus Torvalds.",
            " *\t\t 2000 Transmeta Corp.",
            " *\t\t 2000-2001 Christoph Rohland",
            " *\t\t 2000-2001 SAP AG",
            " *\t\t 2002 Red Hat Inc.",
            " * Copyright (C) 2002-2011 Hugh Dickins.",
            " * Copyright (C) 2011 Google Inc.",
            " * Copyright (C) 2002-2005 VERITAS Software Corporation.",
            " * Copyright (C) 2004 Andi Kleen, SuSE Labs",
            " *",
            " * Extended attribute support for tmpfs:",
            " * Copyright (c) 2004, Luke Kenneth Casson Leighton <lkcl@lkcl.net>",
            " * Copyright (c) 2004 Red Hat, Inc., James Morris <jmorris@redhat.com>",
            " *",
            " * tiny-shmem:",
            " * Copyright (c) 2004, 2008 Matt Mackall <mpm@selenic.com>",
            " *",
            " * This file is released under the GPL.",
            " */",
            "",
            "#include <linux/fs.h>",
            "#include <linux/init.h>",
            "#include <linux/vfs.h>",
            "#include <linux/mount.h>",
            "#include <linux/ramfs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/file.h>",
            "#include <linux/fileattr.h>",
            "#include <linux/mm.h>",
            "#include <linux/random.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/shmem_fs.h>",
            "#include <linux/swap.h>",
            "#include <linux/uio.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/fs_parser.h>",
            "#include <linux/swapfile.h>",
            "#include <linux/iversion.h>",
            "#include \"swap.h\"",
            "",
            "static struct vfsmount *shm_mnt __ro_after_init;",
            "",
            "#ifdef CONFIG_SHMEM",
            "/*",
            " * This virtual memory filesystem is heavily based on the ramfs. It",
            " * extends ramfs by the ability to use swap and honor resource limits",
            " * which makes it a completely usable filesystem.",
            " */",
            "",
            "#include <linux/xattr.h>",
            "#include <linux/exportfs.h>",
            "#include <linux/posix_acl.h>",
            "#include <linux/posix_acl_xattr.h>",
            "#include <linux/mman.h>",
            "#include <linux/string.h>",
            "#include <linux/slab.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/writeback.h>",
            "#include <linux/pagevec.h>",
            "#include <linux/percpu_counter.h>",
            "#include <linux/falloc.h>",
            "#include <linux/splice.h>",
            "#include <linux/security.h>",
            "#include <linux/swapops.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/namei.h>",
            "#include <linux/ctype.h>",
            "#include <linux/migrate.h>",
            "#include <linux/highmem.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/magic.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/fcntl.h>",
            "#include <uapi/linux/memfd.h>",
            "#include <linux/rmap.h>",
            "#include <linux/uuid.h>",
            "#include <linux/quotaops.h>",
            "#include <linux/rcupdate_wait.h>",
            "",
            "#include <linux/uaccess.h>",
            "",
            "#include \"internal.h\"",
            "",
            "#define BLOCKS_PER_PAGE  (PAGE_SIZE/512)",
            "#define VM_ACCT(size)    (PAGE_ALIGN(size) >> PAGE_SHIFT)",
            "",
            "/* Pretend that each entry is of this size in directory's i_size */",
            "#define BOGO_DIRENT_SIZE 20",
            "",
            "/* Pretend that one inode + its dentry occupy this much memory */",
            "#define BOGO_INODE_SIZE 1024",
            "",
            "/* Symlink up to this size is kmalloc'ed instead of using a swappable page */",
            "#define SHORT_SYMLINK_LEN 128",
            "",
            "/*",
            " * shmem_fallocate communicates with shmem_fault or shmem_writepage via",
            " * inode->i_private (with i_rwsem making sure that it has only one user at",
            " * a time): we would prefer not to enlarge the shmem inode just for that.",
            " */",
            "struct shmem_falloc {",
            "\twait_queue_head_t *waitq; /* faults into hole wait for punch to end */",
            "\tpgoff_t start;\t\t/* start of range currently being fallocated */",
            "\tpgoff_t next;\t\t/* the next page offset to be fallocated */",
            "\tpgoff_t nr_falloced;\t/* how many new pages have been fallocated */",
            "\tpgoff_t nr_unswapped;\t/* how often writepage refused to swap out */",
            "};",
            "",
            "struct shmem_options {",
            "\tunsigned long long blocks;",
            "\tunsigned long long inodes;",
            "\tstruct mempolicy *mpol;",
            "\tkuid_t uid;",
            "\tkgid_t gid;",
            "\tumode_t mode;",
            "\tbool full_inums;",
            "\tint huge;",
            "\tint seen;",
            "\tbool noswap;",
            "\tunsigned short quota_types;",
            "\tstruct shmem_quota_limits qlimits;",
            "#define SHMEM_SEEN_BLOCKS 1",
            "#define SHMEM_SEEN_INODES 2",
            "#define SHMEM_SEEN_HUGE 4",
            "#define SHMEM_SEEN_INUMS 8",
            "#define SHMEM_SEEN_NOSWAP 16",
            "#define SHMEM_SEEN_QUOTA 32",
            "};",
            "",
            "#ifdef CONFIG_TRANSPARENT_HUGEPAGE",
            "static unsigned long huge_shmem_orders_always __read_mostly;",
            "static unsigned long huge_shmem_orders_madvise __read_mostly;",
            "static unsigned long huge_shmem_orders_inherit __read_mostly;",
            "static unsigned long huge_shmem_orders_within_size __read_mostly;",
            "static bool shmem_orders_configured __initdata;",
            "#endif",
            "",
            "#ifdef CONFIG_TMPFS"
          ],
          "function_name": null,
          "description": "定义shmem模块的基础结构和配置，声明全局变量shm_mnt，包含必要的头文件，定义用于管理shmem文件系统的宏、结构体（如shmem_falloc、shmem_options）及配置选项（如CONFIG_SHMEM）。",
          "similarity": 0.5386905074119568
        }
      ]
    },
    {
      "source_file": "mm/hugetlb_vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:07:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `hugetlb_vmemmap.c`\n\n---\n\n# hugetlb_vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`hugetlb_vmemmap.c` 实现了 **HugeTLB Vmemmap Optimization (HVO)** 功能，旨在优化与 HugeTLB 页面关联的 `vmemmap`（虚拟内存映射）结构所占用的物理内存。在 Linux 内核中，每个物理页都对应一个 `struct page` 结构，这些结构通过 `vmemmap` 虚拟地址空间进行线性映射。当使用大页（如 2MB 或 1GB HugeTLB 页面）时，为整个大页区域分配完整的 `struct page` 数组会造成大量内存浪费（因为大部分尾部页面不会被单独使用）。  \n\n本文件通过 **重映射（remap）** 技术，将大页对应的多个 `vmemmap` 页面中的尾部页面重新映射到同一个物理页（通常是头部页面），从而显著减少 `vmemmap` 所需的物理内存开销，同时保持内核对 `struct page` 的访问语义正确。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct vmemmap_remap_walk`**  \n  用于遍历和操作 `vmemmap` 页表的上下文结构：\n  - `remap_pte`: 回调函数，处理每个 PTE 条目\n  - `nr_walked`: 已遍历的 PTE 数量\n  - `reuse_page`: 用于重用的物理页（通常是头部页）\n  - `reuse_addr`: `reuse_page` 对应的虚拟地址\n  - `vmemmap_pages`: 可释放的 `vmemmap` 页面链表\n  - `flags`: 控制 TLB 刷新行为的标志位（`VMEMMAP_SPLIT_NO_TLB_FLUSH`, `VMEMMAP_REMAP_NO_TLB_FLUSH`）\n\n### 主要函数\n\n- **`vmemmap_split_pmd()`**  \n  将一个 PMD（Page Middle Directory）级别的大页映射拆分为 PTE 级别的细粒度映射，为后续重映射做准备。\n\n- **`vmemmap_pmd_entry()`**  \n  `mm_walk` 回调函数，在遍历到 PMD 条目时触发，负责检查是否需要拆分 PMD 并执行拆分操作。\n\n- **`vmemmap_pte_entry()`**  \n  `mm_walk` 回调函数，在遍历到 PTE 条目时触发，用于识别重用页并执行重映射逻辑。\n\n- **`vmemmap_remap_range()`**  \n  驱动整个重映射流程，使用 `walk_page_range_novma()` 遍历指定的 `vmemmap` 虚拟地址范围。\n\n- **`vmemmap_remap_pte()`**  \n  实际执行 PTE 重映射的核心函数：将尾部 `vmemmap` 页面的 PTE 指向 `reuse_page`，并设置为只读以防止非法写入。\n\n- **`vmemmap_restore_pte()`**  \n  用于恢复原始映射（例如在取消优化时），从可释放列表中取出原页面并恢复其内容。\n\n- **`free_vmemmap_page()` / `free_vmemmap_page_list()`**  \n  安全释放 `vmemmap` 页面，区分来自 `memblock`（启动内存）或 `buddy` 分配器的页面。\n\n- **`reset_struct_pages()`**  \n  重置 `struct page` 结构的关键字段，避免因重映射导致的元数据不一致问题（如“corrupted mapping in tail page”警告）。\n\n## 3. 关键实现\n\n### 重映射机制\n1. **PMD 拆分**：首先将覆盖目标 `vmemmap` 范围的 PMD 大页映射拆分为 PTE 映射，确保可以独立修改每个 `struct page` 对应的物理页。\n2. **重用页识别**：在遍历 PTE 时，第一个遇到的页面被选为 `reuse_page`（即头部页）。\n3. **尾页重映射**：后续所有 PTE 条目均被修改为指向 `reuse_page`，并设置为只读（`PAGE_KERNEL_RO`），防止对尾部 `struct page` 的意外写入。\n4. **元数据清理**：由于尾部 `struct page` 与头部共享物理内存，其元数据（如 `flags`、`mapping`）可能无效。通过 `reset_struct_pages()` 复制有效数据到尾部结构，避免内核校验失败。\n5. **安全释放**：被替换的原始尾部页面被加入 `vmemmap_pages` 链表，可在后续安全释放。\n\n### 自托管检测\n在内存热插拔场景下（`memmap_on_memory`），`vmemmap` 结构可能位于待优化的内存区域内（即“自托管”）。代码通过检查首个 `vmemmap` 页面的 `PageVmemmapSelfHosted()` 标志，若为真则拒绝优化（返回 `-ENOTSUPP`），防止破坏关键元数据。\n\n### 内存屏障与 TLB 刷新\n- 使用 `smp_wmb()` 确保页面内容更新在 PTE 修改前完成。\n- 在 PMD 拆分和 PTE 重映射后，默认执行 `flush_tlb_kernel_range()` 刷新 TLB，可通过标志位跳过以提升性能。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `pgtable.h`、`pagewalk.h`、`mmdebug.h` 等核心 MM 头文件。\n- **架构相关代码**：使用 `asm/pgalloc.h` 和 `asm/tlbflush.h` 提供的页表分配与 TLB 刷新接口。\n- **HugeTLB 子系统**：与 `hugetlb.h` 协同工作，优化 HugeTLB 页面的 `vmemmap` 开销。\n- **内存热插拔**：处理 `memmap_on_memory` 场景下的自托管 `vmemmap` 限制。\n- **启动内存管理**：通过 `bootmem_info.h` 区分 `memblock` 与 `buddy` 分配的页面。\n\n## 5. 使用场景\n\n- **HugeTLB 内存优化**：在系统配置大量 HugeTLB 页面时，显著减少 `vmemmap` 的物理内存占用（例如，2MB HugeTLB 页面可节省约 87.5% 的 `vmemmap` 内存）。\n- **内存受限环境**：在内存资源紧张的系统（如容器、嵌入式设备）中降低内核内存开销。\n- **内存热插拔**：在支持 `memmap_on_memory` 的热插拔场景中，安全地优化新插入内存区域的 `vmemmap`。\n- **内核调试与维护**：通过只读保护捕获对尾部 `struct page` 的非法写入，提升系统稳定性。",
      "similarity": 0.47013306617736816,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 450,
          "end_line": 579,
          "content": [
            "static int __hugetlb_vmemmap_restore_folio(const struct hstate *h,",
            "\t\t\t\t\t   struct folio *folio, unsigned long flags)",
            "{",
            "\tint ret;",
            "\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;",
            "\tunsigned long vmemmap_reuse;",
            "",
            "\tVM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(folio), folio);",
            "\tVM_WARN_ON_ONCE_FOLIO(folio_ref_count(folio), folio);",
            "",
            "\tif (!folio_test_hugetlb_vmemmap_optimized(folio))",
            "\t\treturn 0;",
            "",
            "\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);",
            "\tvmemmap_reuse\t= vmemmap_start;",
            "\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;",
            "",
            "\t/*",
            "\t * The pages which the vmemmap virtual address range [@vmemmap_start,",
            "\t * @vmemmap_end) are mapped to are freed to the buddy allocator, and",
            "\t * the range is mapped to the page which @vmemmap_reuse is mapped to.",
            "\t * When a HugeTLB page is freed to the buddy allocator, previously",
            "\t * discarded vmemmap pages must be allocated and remapping.",
            "\t */",
            "\tret = vmemmap_remap_alloc(vmemmap_start, vmemmap_end, vmemmap_reuse, flags);",
            "\tif (!ret) {",
            "\t\tfolio_clear_hugetlb_vmemmap_optimized(folio);",
            "\t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "int hugetlb_vmemmap_restore_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\t/* avoid writes from page_ref_add_unless() while unfolding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\treturn __hugetlb_vmemmap_restore_folio(h, folio, 0);",
            "}",
            "long hugetlb_vmemmap_restore_folios(const struct hstate *h,",
            "\t\t\t\t\tstruct list_head *folio_list,",
            "\t\t\t\t\tstruct list_head *non_hvo_folios)",
            "{",
            "\tstruct folio *folio, *t_folio;",
            "\tlong restored = 0;",
            "\tlong ret = 0;",
            "",
            "\t/* avoid writes from page_ref_add_unless() while unfolding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\tlist_for_each_entry_safe(folio, t_folio, folio_list, lru) {",
            "\t\tif (folio_test_hugetlb_vmemmap_optimized(folio)) {",
            "\t\t\tret = __hugetlb_vmemmap_restore_folio(h, folio,",
            "\t\t\t\t\t\t\t      VMEMMAP_REMAP_NO_TLB_FLUSH);",
            "\t\t\tif (ret)",
            "\t\t\t\tbreak;",
            "\t\t\trestored++;",
            "\t\t}",
            "",
            "\t\t/* Add non-optimized folios to output list */",
            "\t\tlist_move(&folio->lru, non_hvo_folios);",
            "\t}",
            "",
            "\tif (restored)",
            "\t\tflush_tlb_all();",
            "\tif (!ret)",
            "\t\tret = restored;",
            "\treturn ret;",
            "}",
            "static bool vmemmap_should_optimize_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\tif (folio_test_hugetlb_vmemmap_optimized(folio))",
            "\t\treturn false;",
            "",
            "\tif (!READ_ONCE(vmemmap_optimize_enabled))",
            "\t\treturn false;",
            "",
            "\tif (!hugetlb_vmemmap_optimizable(h))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,",
            "\t\t\t\t\t    struct folio *folio,",
            "\t\t\t\t\t    struct list_head *vmemmap_pages,",
            "\t\t\t\t\t    unsigned long flags)",
            "{",
            "\tint ret = 0;",
            "\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;",
            "\tunsigned long vmemmap_reuse;",
            "",
            "\tVM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(folio), folio);",
            "\tVM_WARN_ON_ONCE_FOLIO(folio_ref_count(folio), folio);",
            "",
            "\tif (!vmemmap_should_optimize_folio(h, folio))",
            "\t\treturn ret;",
            "",
            "\tstatic_branch_inc(&hugetlb_optimize_vmemmap_key);",
            "\t/*",
            "\t * Very Subtle",
            "\t * If VMEMMAP_REMAP_NO_TLB_FLUSH is set, TLB flushing is not performed",
            "\t * immediately after remapping.  As a result, subsequent accesses",
            "\t * and modifications to struct pages associated with the hugetlb",
            "\t * page could be to the OLD struct pages.  Set the vmemmap optimized",
            "\t * flag here so that it is copied to the new head page.  This keeps",
            "\t * the old and new struct pages in sync.",
            "\t * If there is an error during optimization, we will immediately FLUSH",
            "\t * the TLB and clear the flag below.",
            "\t */",
            "\tfolio_set_hugetlb_vmemmap_optimized(folio);",
            "",
            "\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);",
            "\tvmemmap_reuse\t= vmemmap_start;",
            "\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;",
            "",
            "\t/*",
            "\t * Remap the vmemmap virtual address range [@vmemmap_start, @vmemmap_end)",
            "\t * to the page which @vmemmap_reuse is mapped to.  Add pages previously",
            "\t * mapping the range to vmemmap_pages list so that they can be freed by",
            "\t * the caller.",
            "\t */",
            "\tret = vmemmap_remap_free(vmemmap_start, vmemmap_end, vmemmap_reuse,",
            "\t\t\t\t vmemmap_pages, flags);",
            "\tif (ret) {",
            "\t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);",
            "\t\tfolio_clear_hugetlb_vmemmap_optimized(folio);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__hugetlb_vmemmap_restore_folio, hugetlb_vmemmap_restore_folio, hugetlb_vmemmap_restore_folios, vmemmap_should_optimize_folio, __hugetlb_vmemmap_optimize_folio",
          "description": "实现hugeTLB页的vmemmap优化控制逻辑，包含__hugetlb_vmemmap_restore_folio页回收恢复、hugetlb_vmemmap_restore_folios批量处理、vmemmap_should_optimize_folio优化判定及__hugetlb_vmemmap_optimize_folio优化执行路径。",
          "similarity": 0.4526077210903168
        },
        {
          "chunk_id": 5,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 619,
          "end_line": 710,
          "content": [
            "void hugetlb_vmemmap_optimize_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\tLIST_HEAD(vmemmap_pages);",
            "",
            "\t/* avoid writes from page_ref_add_unless() while folding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\t__hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages, 0);",
            "\tfree_vmemmap_page_list(&vmemmap_pages);",
            "}",
            "static int hugetlb_vmemmap_split_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;",
            "\tunsigned long vmemmap_reuse;",
            "",
            "\tif (!vmemmap_should_optimize_folio(h, folio))",
            "\t\treturn 0;",
            "",
            "\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);",
            "\tvmemmap_reuse\t= vmemmap_start;",
            "\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;",
            "",
            "\t/*",
            "\t * Split PMDs on the vmemmap virtual address range [@vmemmap_start,",
            "\t * @vmemmap_end]",
            "\t */",
            "\treturn vmemmap_remap_split(vmemmap_start, vmemmap_end, vmemmap_reuse);",
            "}",
            "void hugetlb_vmemmap_optimize_folios(struct hstate *h, struct list_head *folio_list)",
            "{",
            "\tstruct folio *folio;",
            "\tLIST_HEAD(vmemmap_pages);",
            "",
            "\tlist_for_each_entry(folio, folio_list, lru) {",
            "\t\tint ret = hugetlb_vmemmap_split_folio(h, folio);",
            "",
            "\t\t/*",
            "\t\t * Spliting the PMD requires allocating a page, thus lets fail",
            "\t\t * early once we encounter the first OOM. No point in retrying",
            "\t\t * as it can be dynamically done on remap with the memory",
            "\t\t * we get back from the vmemmap deduplication.",
            "\t\t */",
            "\t\tif (ret == -ENOMEM)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tflush_tlb_all();",
            "",
            "\t/* avoid writes from page_ref_add_unless() while folding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\tlist_for_each_entry(folio, folio_list, lru) {",
            "\t\tint ret;",
            "",
            "\t\tret = __hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages,",
            "\t\t\t\t\t\t       VMEMMAP_REMAP_NO_TLB_FLUSH);",
            "",
            "\t\t/*",
            "\t\t * Pages to be freed may have been accumulated.  If we",
            "\t\t * encounter an ENOMEM,  free what we have and try again.",
            "\t\t * This can occur in the case that both spliting fails",
            "\t\t * halfway and head page allocation also failed. In this",
            "\t\t * case __hugetlb_vmemmap_optimize_folio() would free memory",
            "\t\t * allowing more vmemmap remaps to occur.",
            "\t\t */",
            "\t\tif (ret == -ENOMEM && !list_empty(&vmemmap_pages)) {",
            "\t\t\tflush_tlb_all();",
            "\t\t\tfree_vmemmap_page_list(&vmemmap_pages);",
            "\t\t\tINIT_LIST_HEAD(&vmemmap_pages);",
            "\t\t\t__hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages,",
            "\t\t\t\t\t\t\t VMEMMAP_REMAP_NO_TLB_FLUSH);",
            "\t\t}",
            "\t}",
            "",
            "\tflush_tlb_all();",
            "\tfree_vmemmap_page_list(&vmemmap_pages);",
            "}",
            "static int __init hugetlb_vmemmap_init(void)",
            "{",
            "\tconst struct hstate *h;",
            "",
            "\t/* HUGETLB_VMEMMAP_RESERVE_SIZE should cover all used struct pages */",
            "\tBUILD_BUG_ON(__NR_USED_SUBPAGE > HUGETLB_VMEMMAP_RESERVE_PAGES);",
            "",
            "\tfor_each_hstate(h) {",
            "\t\tif (hugetlb_vmemmap_optimizable(h)) {",
            "\t\t\tregister_sysctl_init(\"vm\", hugetlb_vmemmap_sysctls);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "hugetlb_vmemmap_optimize_folio, hugetlb_vmemmap_split_folio, hugetlb_vmemmap_optimize_folios, hugetlb_vmemmap_init",
          "description": "该代码段实现了HugeTLB大页面的虚拟内存映射优化机制。  \n`hugetlb_vmemmap_split_folio`负责分割PMDs以优化VMEMMAP区域空间，`hugetlb_vmemmap_optimize_folios`遍历folio列表执行拆分与内存回收操作，`hugetlb_vmemmap_init`注册系统控制接口用于动态调整优化策略。由于`__hugetlb_vmemmap_optimize_folio`等关键函数未完整展示，需结合上下文进一步验证实现细节。",
          "similarity": 0.44567549228668213
        },
        {
          "chunk_id": 1,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 49,
          "end_line": 150,
          "content": [
            "static int vmemmap_split_pmd(pmd_t *pmd, struct page *head, unsigned long start,",
            "\t\t\t     struct vmemmap_remap_walk *walk)",
            "{",
            "\tpmd_t __pmd;",
            "\tint i;",
            "\tunsigned long addr = start;",
            "\tpte_t *pgtable;",
            "",
            "\tpgtable = pte_alloc_one_kernel(&init_mm);",
            "\tif (!pgtable)",
            "\t\treturn -ENOMEM;",
            "",
            "\tpmd_populate_kernel(&init_mm, &__pmd, pgtable);",
            "",
            "\tfor (i = 0; i < PTRS_PER_PTE; i++, addr += PAGE_SIZE) {",
            "\t\tpte_t entry, *pte;",
            "\t\tpgprot_t pgprot = PAGE_KERNEL;",
            "",
            "\t\tentry = mk_pte(head + i, pgprot);",
            "\t\tpte = pte_offset_kernel(&__pmd, addr);",
            "\t\tset_pte_at(&init_mm, addr, pte, entry);",
            "\t}",
            "",
            "\tspin_lock(&init_mm.page_table_lock);",
            "\tif (likely(pmd_leaf(*pmd))) {",
            "\t\t/*",
            "\t\t * Higher order allocations from buddy allocator must be able to",
            "\t\t * be treated as indepdenent small pages (as they can be freed",
            "\t\t * individually).",
            "\t\t */",
            "\t\tif (!PageReserved(head))",
            "\t\t\tsplit_page(head, get_order(PMD_SIZE));",
            "",
            "\t\t/* Make pte visible before pmd. See comment in pmd_install(). */",
            "\t\tsmp_wmb();",
            "\t\tpmd_populate_kernel(&init_mm, pmd, pgtable);",
            "\t\tif (!(walk->flags & VMEMMAP_SPLIT_NO_TLB_FLUSH))",
            "\t\t\tflush_tlb_kernel_range(start, start + PMD_SIZE);",
            "\t} else {",
            "\t\tpte_free_kernel(&init_mm, pgtable);",
            "\t}",
            "\tspin_unlock(&init_mm.page_table_lock);",
            "",
            "\treturn 0;",
            "}",
            "static int vmemmap_pmd_entry(pmd_t *pmd, unsigned long addr,",
            "\t\t\t     unsigned long next, struct mm_walk *walk)",
            "{",
            "\tint ret = 0;",
            "\tstruct page *head;",
            "\tstruct vmemmap_remap_walk *vmemmap_walk = walk->private;",
            "",
            "\t/* Only splitting, not remapping the vmemmap pages. */",
            "\tif (!vmemmap_walk->remap_pte)",
            "\t\twalk->action = ACTION_CONTINUE;",
            "",
            "\tspin_lock(&init_mm.page_table_lock);",
            "\thead = pmd_leaf(*pmd) ? pmd_page(*pmd) : NULL;",
            "\t/*",
            "\t * Due to HugeTLB alignment requirements and the vmemmap",
            "\t * pages being at the start of the hotplugged memory",
            "\t * region in memory_hotplug.memmap_on_memory case. Checking",
            "\t * the vmemmap page associated with the first vmemmap page",
            "\t * if it is self-hosted is sufficient.",
            "\t *",
            "\t * [                  hotplugged memory                  ]",
            "\t * [        section        ][...][        section        ]",
            "\t * [ vmemmap ][              usable memory               ]",
            "\t *   ^  | ^                        |",
            "\t *   +--+ |                        |",
            "\t *        +------------------------+",
            "\t */",
            "\tif (unlikely(!vmemmap_walk->nr_walked)) {",
            "\t\tstruct page *page = head ? head + pte_index(addr) :",
            "\t\t\t\t    pte_page(ptep_get(pte_offset_kernel(pmd, addr)));",
            "",
            "\t\tif (PageVmemmapSelfHosted(page))",
            "\t\t\tret = -ENOTSUPP;",
            "\t}",
            "\tspin_unlock(&init_mm.page_table_lock);",
            "\tif (!head || ret)",
            "\t\treturn ret;",
            "",
            "\treturn vmemmap_split_pmd(pmd, head, addr & PMD_MASK, vmemmap_walk);",
            "}",
            "static int vmemmap_pte_entry(pte_t *pte, unsigned long addr,",
            "\t\t\t     unsigned long next, struct mm_walk *walk)",
            "{",
            "\tstruct vmemmap_remap_walk *vmemmap_walk = walk->private;",
            "",
            "\t/*",
            "\t * The reuse_page is found 'first' in page table walking before",
            "\t * starting remapping.",
            "\t */",
            "\tif (!vmemmap_walk->reuse_page)",
            "\t\tvmemmap_walk->reuse_page = pte_page(ptep_get(pte));",
            "\telse",
            "\t\tvmemmap_walk->remap_pte(pte, addr, vmemmap_walk);",
            "\tvmemmap_walk->nr_walked++;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmemmap_split_pmd, vmemmap_pmd_entry, vmemmap_pte_entry",
          "description": "实现了vmeammap_split_pmd分割PMD页表项、vmemmap_pmd_entry检查页表项有效性、vmemmap_pte_entry处理页目录项的逻辑，支持大页拆分与页表项初始化。",
          "similarity": 0.44559529423713684
        },
        {
          "chunk_id": 2,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 159,
          "end_line": 264,
          "content": [
            "static int vmemmap_remap_range(unsigned long start, unsigned long end,",
            "\t\t\t       struct vmemmap_remap_walk *walk)",
            "{",
            "\tint ret;",
            "",
            "\tVM_BUG_ON(!PAGE_ALIGNED(start | end));",
            "",
            "\tret = walk_page_range_novma(&init_mm, start, end, &vmemmap_remap_ops,",
            "\t\t\t\t    NULL, walk);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tif (walk->remap_pte && !(walk->flags & VMEMMAP_REMAP_NO_TLB_FLUSH))",
            "\t\tflush_tlb_kernel_range(start, end);",
            "",
            "\treturn 0;",
            "}",
            "static inline void free_vmemmap_page(struct page *page)",
            "{",
            "\tif (PageReserved(page))",
            "\t\tfree_bootmem_page(page);",
            "\telse",
            "\t\t__free_page(page);",
            "}",
            "static void free_vmemmap_page_list(struct list_head *list)",
            "{",
            "\tstruct page *page, *next;",
            "",
            "\tlist_for_each_entry_safe(page, next, list, lru)",
            "\t\tfree_vmemmap_page(page);",
            "}",
            "static void vmemmap_remap_pte(pte_t *pte, unsigned long addr,",
            "\t\t\t      struct vmemmap_remap_walk *walk)",
            "{",
            "\t/*",
            "\t * Remap the tail pages as read-only to catch illegal write operation",
            "\t * to the tail pages.",
            "\t */",
            "\tpgprot_t pgprot = PAGE_KERNEL_RO;",
            "\tstruct page *page = pte_page(ptep_get(pte));",
            "\tpte_t entry;",
            "",
            "\t/* Remapping the head page requires r/w */",
            "\tif (unlikely(addr == walk->reuse_addr)) {",
            "\t\tpgprot = PAGE_KERNEL;",
            "\t\tlist_del(&walk->reuse_page->lru);",
            "",
            "\t\t/*",
            "\t\t * Makes sure that preceding stores to the page contents from",
            "\t\t * vmemmap_remap_free() become visible before the set_pte_at()",
            "\t\t * write.",
            "\t\t */",
            "\t\tsmp_wmb();",
            "\t}",
            "",
            "\tentry = mk_pte(walk->reuse_page, pgprot);",
            "\tlist_add(&page->lru, walk->vmemmap_pages);",
            "\tset_pte_at(&init_mm, addr, pte, entry);",
            "}",
            "static inline void reset_struct_pages(struct page *start)",
            "{",
            "\tstruct page *from = start + NR_RESET_STRUCT_PAGE;",
            "",
            "\tBUILD_BUG_ON(NR_RESET_STRUCT_PAGE * 2 > PAGE_SIZE / sizeof(struct page));",
            "\tmemcpy(start, from, sizeof(*from) * NR_RESET_STRUCT_PAGE);",
            "}",
            "static void vmemmap_restore_pte(pte_t *pte, unsigned long addr,",
            "\t\t\t\tstruct vmemmap_remap_walk *walk)",
            "{",
            "\tpgprot_t pgprot = PAGE_KERNEL;",
            "\tstruct page *page;",
            "\tvoid *to;",
            "",
            "\tBUG_ON(pte_page(ptep_get(pte)) != walk->reuse_page);",
            "",
            "\tpage = list_first_entry(walk->vmemmap_pages, struct page, lru);",
            "\tlist_del(&page->lru);",
            "\tto = page_to_virt(page);",
            "\tcopy_page(to, (void *)walk->reuse_addr);",
            "\treset_struct_pages(to);",
            "",
            "\t/*",
            "\t * Makes sure that preceding stores to the page contents become visible",
            "\t * before the set_pte_at() write.",
            "\t */",
            "\tsmp_wmb();",
            "\tset_pte_at(&init_mm, addr, pte, mk_pte(page, pgprot));",
            "}",
            "static int vmemmap_remap_split(unsigned long start, unsigned long end,",
            "\t\t\t       unsigned long reuse)",
            "{",
            "\tint ret;",
            "\tstruct vmemmap_remap_walk walk = {",
            "\t\t.remap_pte\t= NULL,",
            "\t\t.flags\t\t= VMEMMAP_SPLIT_NO_TLB_FLUSH,",
            "\t};",
            "",
            "\t/* See the comment in the vmemmap_remap_free(). */",
            "\tBUG_ON(start - reuse != PAGE_SIZE);",
            "",
            "\tmmap_read_lock(&init_mm);",
            "\tret = vmemmap_remap_range(reuse, end, &walk);",
            "\tmmap_read_unlock(&init_mm);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "vmemmap_remap_range, free_vmemmap_page, free_vmemmap_page_list, vmemmap_remap_pte, reset_struct_pages, vmemmap_restore_pte, vmemmap_remap_split",
          "description": "提供vmemmap_remap_range范围重映射接口、free_vmemmap_page页面释放函数、reset_struct_pages结构页重置方法及remap_pte/restore_pte的映射更新逻辑，支持安全写保护和数据恢复。",
          "similarity": 0.4412945508956909
        },
        {
          "chunk_id": 3,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 316,
          "end_line": 427,
          "content": [
            "static int vmemmap_remap_free(unsigned long start, unsigned long end,",
            "\t\t\t      unsigned long reuse,",
            "\t\t\t      struct list_head *vmemmap_pages,",
            "\t\t\t      unsigned long flags)",
            "{",
            "\tint ret;",
            "\tstruct vmemmap_remap_walk walk = {",
            "\t\t.remap_pte\t= vmemmap_remap_pte,",
            "\t\t.reuse_addr\t= reuse,",
            "\t\t.vmemmap_pages\t= vmemmap_pages,",
            "\t\t.flags\t\t= flags,",
            "\t};",
            "\tint nid = page_to_nid((struct page *)reuse);",
            "\tgfp_t gfp_mask = GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN;",
            "",
            "\t/*",
            "\t * Allocate a new head vmemmap page to avoid breaking a contiguous",
            "\t * block of struct page memory when freeing it back to page allocator",
            "\t * in free_vmemmap_page_list(). This will allow the likely contiguous",
            "\t * struct page backing memory to be kept contiguous and allowing for",
            "\t * more allocations of hugepages. Fallback to the currently",
            "\t * mapped head page in case should it fail to allocate.",
            "\t */",
            "\twalk.reuse_page = alloc_pages_node(nid, gfp_mask, 0);",
            "\tif (walk.reuse_page) {",
            "\t\tcopy_page(page_to_virt(walk.reuse_page),",
            "\t\t\t  (void *)walk.reuse_addr);",
            "\t\tlist_add(&walk.reuse_page->lru, vmemmap_pages);",
            "\t}",
            "",
            "\t/*",
            "\t * In order to make remapping routine most efficient for the huge pages,",
            "\t * the routine of vmemmap page table walking has the following rules",
            "\t * (see more details from the vmemmap_pte_range()):",
            "\t *",
            "\t * - The range [@start, @end) and the range [@reuse, @reuse + PAGE_SIZE)",
            "\t *   should be continuous.",
            "\t * - The @reuse address is part of the range [@reuse, @end) that we are",
            "\t *   walking which is passed to vmemmap_remap_range().",
            "\t * - The @reuse address is the first in the complete range.",
            "\t *",
            "\t * So we need to make sure that @start and @reuse meet the above rules.",
            "\t */",
            "\tBUG_ON(start - reuse != PAGE_SIZE);",
            "",
            "\tmmap_read_lock(&init_mm);",
            "\tret = vmemmap_remap_range(reuse, end, &walk);",
            "\tif (ret && walk.nr_walked) {",
            "\t\tend = reuse + walk.nr_walked * PAGE_SIZE;",
            "\t\t/*",
            "\t\t * vmemmap_pages contains pages from the previous",
            "\t\t * vmemmap_remap_range call which failed.  These",
            "\t\t * are pages which were removed from the vmemmap.",
            "\t\t * They will be restored in the following call.",
            "\t\t */",
            "\t\twalk = (struct vmemmap_remap_walk) {",
            "\t\t\t.remap_pte\t= vmemmap_restore_pte,",
            "\t\t\t.reuse_addr\t= reuse,",
            "\t\t\t.vmemmap_pages\t= vmemmap_pages,",
            "\t\t\t.flags\t\t= 0,",
            "\t\t};",
            "",
            "\t\tvmemmap_remap_range(reuse, end, &walk);",
            "\t}",
            "\tmmap_read_unlock(&init_mm);",
            "",
            "\treturn ret;",
            "}",
            "static int alloc_vmemmap_page_list(unsigned long start, unsigned long end,",
            "\t\t\t\t   struct list_head *list)",
            "{",
            "\tgfp_t gfp_mask = GFP_KERNEL | __GFP_RETRY_MAYFAIL;",
            "\tunsigned long nr_pages = (end - start) >> PAGE_SHIFT;",
            "\tint nid = page_to_nid((struct page *)start);",
            "\tstruct page *page, *next;",
            "",
            "\twhile (nr_pages--) {",
            "\t\tpage = alloc_pages_node(nid, gfp_mask, 0);",
            "\t\tif (!page)",
            "\t\t\tgoto out;",
            "\t\tlist_add(&page->lru, list);",
            "\t}",
            "",
            "\treturn 0;",
            "out:",
            "\tlist_for_each_entry_safe(page, next, list, lru)",
            "\t\t__free_page(page);",
            "\treturn -ENOMEM;",
            "}",
            "static int vmemmap_remap_alloc(unsigned long start, unsigned long end,",
            "\t\t\t       unsigned long reuse, unsigned long flags)",
            "{",
            "\tLIST_HEAD(vmemmap_pages);",
            "\tstruct vmemmap_remap_walk walk = {",
            "\t\t.remap_pte\t= vmemmap_restore_pte,",
            "\t\t.reuse_addr\t= reuse,",
            "\t\t.vmemmap_pages\t= &vmemmap_pages,",
            "\t\t.flags\t\t= flags,",
            "\t};",
            "",
            "\t/* See the comment in the vmemmap_remap_free(). */",
            "\tBUG_ON(start - reuse != PAGE_SIZE);",
            "",
            "\tif (alloc_vmemmap_page_list(start, end, &vmemmap_pages))",
            "\t\treturn -ENOMEM;",
            "",
            "\tmmap_read_lock(&init_mm);",
            "\tvmemmap_remap_range(reuse, end, &walk);",
            "\tmmap_read_unlock(&init_mm);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmemmap_remap_free, alloc_vmemmap_page_list, vmemmap_remap_alloc",
          "description": "包含vmemmap_remap_free释放大页时的映射回收逻辑、alloc_vmemmap_page_list页面列表分配函数及vmemmap_remap_alloc预分配页面的接口，实现高效的资源管理。",
          "similarity": 0.41786664724349976
        }
      ]
    },
    {
      "source_file": "kernel/locking/semaphore.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:52:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\semaphore.c`\n\n---\n\n# `locking/semaphore.c` 技术文档\n\n## 1. 文件概述\n\n`locking/semaphore.c` 实现了 Linux 内核中的**计数信号量（counting semaphore）**机制。计数信号量允许多个任务（最多为初始计数值）同时持有该锁，当计数值耗尽时，后续请求者将被阻塞，直到有其他任务释放信号量。与互斥锁（mutex）不同，信号量支持更灵活的并发控制，适用于资源池、限流等场景。该文件提供了多种获取和释放信号量的接口，包括可中断、可超时、不可中断等变体，并支持在中断上下文中调用部分函数。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `down(struct semaphore *sem)` | 不可中断地获取信号量，若不可用则睡眠。**已弃用**，建议使用可中断版本。 |\n| `down_interruptible(struct semaphore *sem)` | 可被普通信号中断的获取操作，成功返回 0，被信号中断返回 `-EINTR`。 |\n| `down_killable(struct semaphore *sem)` | 可被致命信号（fatal signal）中断的获取操作，返回值同上。 |\n| `down_trylock(struct semaphore *sem)` | 非阻塞尝试获取信号量，成功返回 0，失败返回 1（**注意返回值与 mutex/spinlock 相反**）。 |\n| `down_timeout(struct semaphore *sem, long timeout)` | 带超时的获取操作，超时返回 `-ETIME`，成功返回 0。 |\n| `up(struct semaphore *sem)` | 释放信号量，可由任意上下文（包括中断）调用，唤醒等待队列中的任务。 |\n\n### 静态辅助函数\n\n- `__down*()` 系列：处理信号量争用时的阻塞逻辑。\n- `__up()`：在有等待者时执行唤醒逻辑。\n- `___down_common()`：通用的阻塞等待实现，支持不同睡眠状态和超时。\n- `__sem_acquire()`：原子减少计数并记录持有者（用于 hung task 检测）。\n\n### 数据结构\n\n- `struct semaphore`（定义在 `<linux/semaphore.h>`）：\n  - `count`：当前可用资源数（>0 表示可立即获取）。\n  - `wait_list`：等待该信号量的任务链表。\n  - `lock`：保护上述成员的原始自旋锁（`raw_spinlock_t`）。\n  - `last_holder`（条件编译）：记录最后持有者，用于 `CONFIG_DETECT_HUNG_TASK_BLOCKER`。\n\n- `struct semaphore_waiter`：\n  - 用于将任务加入等待队列，包含任务指针和唤醒标志（`up`）。\n\n## 3. 关键实现\n\n### 中断安全与自旋锁\n- 所有对外接口（包括 `down*` 和 `up`）均使用 `raw_spin_lock_irqsave()` 获取自旋锁，确保在中断上下文安全。\n- 即使 `down()` 等函数通常在进程上下文调用，也使用 `irqsave` 变体，因为内核某些部分依赖在中断上下文成功调用 `down()`（当确定信号量可用时）。\n\n### 计数语义\n- `sem->count` 表示**还可被获取的次数**。初始值由 `sema_init()` 设置。\n- 获取时：若 `count > 0`，直接减 1；否则加入等待队列。\n- 释放时：若等待队列为空，`count++`；否则唤醒队首任务。\n\n### 等待与唤醒机制\n- 使用 `wake_q`（批量唤醒队列）优化唤醒路径，避免在持有自旋锁时调用 `wake_up_process()`。\n- 等待任务通过 `schedule_timeout()` 睡眠，并在循环中检查：\n  - 是否收到信号（根据睡眠状态判断）。\n  - 是否超时。\n  - 是否被 `__up()` 标记为 `waiter.up = true`（表示已被选中唤醒）。\n\n### Hung Task 支持\n- 当启用 `CONFIG_DETECT_HUNG_TASK_BLOCKER` 时：\n  - 获取信号量时记录当前任务为 `last_holder`。\n  - 释放时若当前任务是持有者，则清除记录。\n  - 提供 `sem_last_holder()` 供 hung task 检测模块查询阻塞源头。\n\n### 返回值约定\n- `down_trylock()` 返回 **0 表示成功**，**1 表示失败**，这与 `mutex_trylock()` 和 `spin_trylock()` **相反**，需特别注意。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/semaphore.h>`：信号量结构体和 API 声明。\n  - `<linux/spinlock.h>`：原始自旋锁实现。\n  - `<linux/sched.h>`、`<linux/sched/wake_q.h>`：任务调度和批量唤醒。\n  - `<trace/events/lock.h>`：锁争用跟踪点。\n  - `<linux/hung_task.h>`：hung task 检测支持。\n\n- **内核配置依赖**：\n  - `CONFIG_DETECT_HUNG_TASK_BLOCKER`：启用信号量持有者跟踪。\n\n- **与其他同步原语关系**：\n  - 与 `mutex.c` 形成对比：mutex 是二值、不可递归、带调试信息的互斥锁；信号量是计数、可被任意任务释放、更轻量。\n  - 底层依赖调度器（`schedule_timeout`）和中断管理（`irqsave`）。\n\n## 5. 使用场景\n\n- **资源池管理**：如限制同时访问某类硬件设备的任务数量。\n- **读写并发控制**：配合其他机制实现多读者/单写者模型。\n- **内核驱动**：设备驱动中控制对共享资源的并发访问。\n- **中断上下文释放**：因 `up()` 可在中断中调用，适用于中断处理程序释放资源的场景。\n- **不可睡眠路径**：使用 `down_trylock()` 在原子上下文尝试获取资源。\n\n> **注意**：由于信号量不强制所有权（任意任务可调用 `up()`），且缺乏死锁检测等调试特性，现代内核开发中更推荐使用 `mutex` 或 `rwsem`，除非明确需要计数语义或多释放者特性。",
      "similarity": 0.46627575159072876,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 252,
          "end_line": 323,
          "content": [
            "static inline int __sched ___down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\t\t\t\tlong timeout)",
            "{",
            "\tstruct semaphore_waiter waiter;",
            "",
            "\tlist_add_tail(&waiter.list, &sem->wait_list);",
            "\twaiter.task = current;",
            "\twaiter.up = false;",
            "",
            "\tfor (;;) {",
            "\t\tif (signal_pending_state(state, current))",
            "\t\t\tgoto interrupted;",
            "\t\tif (unlikely(timeout <= 0))",
            "\t\t\tgoto timed_out;",
            "\t\t__set_current_state(state);",
            "\t\traw_spin_unlock_irq(&sem->lock);",
            "\t\ttimeout = schedule_timeout(timeout);",
            "\t\traw_spin_lock_irq(&sem->lock);",
            "\t\tif (waiter.up) {",
            "\t\t\thung_task_sem_set_holder(sem);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            " timed_out:",
            "\tlist_del(&waiter.list);",
            "\treturn -ETIME;",
            "",
            " interrupted:",
            "\tlist_del(&waiter.list);",
            "\treturn -EINTR;",
            "}",
            "static inline int __sched __down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\tlong timeout)",
            "{",
            "\tint ret;",
            "",
            "\thung_task_set_blocker(sem, BLOCKER_TYPE_SEM);",
            "",
            "\ttrace_contention_begin(sem, 0);",
            "\tret = ___down_common(sem, state, timeout);",
            "\ttrace_contention_end(sem, ret);",
            "",
            "\thung_task_clear_blocker();",
            "",
            "\treturn ret;",
            "}",
            "static noinline void __sched __down(struct semaphore *sem)",
            "{",
            "\t__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_interruptible(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_killable(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_KILLABLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\treturn __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);",
            "}",
            "static noinline void __sched __up(struct semaphore *sem,",
            "\t\t\t\t  struct wake_q_head *wake_q)",
            "{",
            "\tstruct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,",
            "\t\t\t\t\t\tstruct semaphore_waiter, list);",
            "\tlist_del(&waiter->list);",
            "\twaiter->up = true;",
            "\twake_q_add(wake_q, waiter->task);",
            "}"
          ],
          "function_name": "___down_common, __down_common, __down, __down_interruptible, __down_killable, __down_timeout, __up",
          "description": "实现了信号量的阻塞等待通用逻辑，包含___down_common/__down_common等辅助函数，处理信号量不足时的任务挂起、超时检测、信号处理及唤醒机制，通过循环等待并结合schedule_timeout实现阻塞式资源竞争解决",
          "similarity": 0.40941449999809265
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "\tWRITE_ONCE((sem)->last_holder, (unsigned long)current);",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "\tif (READ_ONCE((sem)->last_holder) == (unsigned long)current)",
            "\t\tWRITE_ONCE((sem)->last_holder, 0UL);",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn READ_ONCE(sem->last_holder);",
            "}",
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn 0UL;",
            "}",
            "static inline void __sem_acquire(struct semaphore *sem)",
            "{",
            "\tsem->count--;",
            "\thung_task_sem_set_holder(sem);",
            "}",
            "void __sched down(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\t__down(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "}",
            "int __sched down_interruptible(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_interruptible(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_killable(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_killable(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_trylock(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint count;",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tcount = sem->count - 1;",
            "\tif (likely(count >= 0))",
            "\t\t__sem_acquire(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn (count < 0);",
            "}",
            "int __sched down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_timeout(sem, timeout);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "void __sched up(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "",
            "\thung_task_sem_clear_if_holder(sem);",
            "",
            "\tif (likely(list_empty(&sem->wait_list)))",
            "\t\tsem->count++;",
            "\telse",
            "\t\t__up(sem, &wake_q);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "\tif (!wake_q_empty(&wake_q))",
            "\t\twake_up_q(&wake_q);",
            "}"
          ],
          "function_name": "hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, __sem_acquire, down, down_interruptible, down_killable, down_trylock, down_timeout, up",
          "description": "实现了信号量的获取与释放核心逻辑，包括down/down_interruptible/down_killable/down_trylock/down_timeout等接口，通过spinlock保护共享资源，维护等待队列并处理任务状态变更，其中包含Hung Task检测相关函数的条件性实现",
          "similarity": 0.4069041907787323
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2008 Intel Corporation",
            " * Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This file implements counting semaphores.",
            " * A counting semaphore may be acquired 'n' times before sleeping.",
            " * See mutex.c for single-acquisition sleeping locks which enforce",
            " * rules which allow code to be debugged more easily.",
            " */",
            "",
            "/*",
            " * Some notes on the implementation:",
            " *",
            " * The spinlock controls access to the other members of the semaphore.",
            " * down_trylock() and up() can be called from interrupt context, so we",
            " * have to disable interrupts when taking the lock.  It turns out various",
            " * parts of the kernel expect to be able to use down() on a semaphore in",
            " * interrupt context when they know it will succeed, so we have to use",
            " * irqsave variants for down(), down_interruptible() and down_killable()",
            " * too.",
            " *",
            " * The ->count variable represents how many more tasks can acquire this",
            " * semaphore.  If it's zero, there may be tasks waiting on the wait_list.",
            " */",
            "",
            "#include <linux/compiler.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/semaphore.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/ftrace.h>",
            "#include <trace/events/lock.h>",
            "#include <linux/hung_task.h>",
            "",
            "static noinline void __down(struct semaphore *sem);",
            "static noinline int __down_interruptible(struct semaphore *sem);",
            "static noinline int __down_killable(struct semaphore *sem);",
            "static noinline int __down_timeout(struct semaphore *sem, long timeout);",
            "static noinline void __up(struct semaphore *sem, struct wake_q_head *wake_q);",
            "",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER"
          ],
          "function_name": null,
          "description": "此代码块定义了计数信号量的基础框架，包含实现计数信号量所需的头文件和注释，声明了多个内联函数及辅助函数，用于处理信号量的获取、释放及Hung Task检测相关逻辑，但由于代码截断，CONFIG_DETECT_HUNG_TASK_BLOCKER部分缺失，上下文不完整",
          "similarity": 0.3305189907550812
        }
      ]
    }
  ]
}