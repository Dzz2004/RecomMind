{
  "query": "buddy system fragmentation",
  "timestamp": "2025-12-26 00:08:45",
  "retrieved_files": [
    {
      "source_file": "mm/zbud.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:36:35\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `zbud.c`\n\n---\n\n# zbud.c 技术文档\n\n## 1. 文件概述\n\n`zbud.c` 实现了一个专用于存储压缩页面（compressed pages）的特殊用途内存分配器——**zbud**。尽管名称中包含“buddy”，但它并非传统的伙伴系统分配器，而是通过将两个压缩页面（称为“zpages”）配对存放在同一个物理内存页（称为“zbud page”）中来实现高效管理。\n\n该设计在牺牲一定存储密度的前提下，提供了简单且可预测的内存回收特性，特别适用于需要频繁进行内存回收（reclaim）的场景（如 zswap、zcache 等压缩交换子系统）。zbud 保证其空间利用率不会低于 1:1（即不会比直接使用未压缩页面占用更多物理页），从而确保“不会造成损害”。\n\n此外，zbud 的 API 与传统分配器不同：`zbud_alloc()` 返回一个不透明句柄（handle），用户必须通过 `zbud_map()` 映射该句柄才能获得可访问的数据指针，并在操作完成后调用 `zbud_unmap()` 解除映射。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct zbud_pool`**  \n  表示一个 zbud 内存池，包含：\n  - `lock`：自旋锁，保护池内所有字段及其中 zbud 页面的元数据。\n  - `unbuddied[NCHUNKS]`：数组，每个元素是一个链表头，用于管理仅包含一个 buddy（单配对）的 zbud 页面；索引表示页面中空闲块的数量。\n  - `buddied`：链表头，管理已包含两个 buddy（满配对）的 zbud 页面（复用 `unbuddied[0]`）。\n  - `pages_nr`：池中 zbud 页面的总数。\n\n- **`struct zbud_header`**  \n  位于每个 zbud 页面的第一个 chunk 中，作为页面元数据，包含：\n  - `buddy`：用于将页面链接到 `unbuddied` 或 `buddied` 链表。\n  - `first_chunks`：第一个 buddy 占用的 chunk 数（为 0 表示空闲）。\n  - `last_chunks`：最后一个 buddy 占用的 chunk 数（为 0 表示空闲）。\n\n### 主要函数\n\n- **`zbud_create_pool(gfp_t gfp)`**  \n  创建并初始化一个新的 zbud 内存池。\n\n- **`zbud_destroy_pool(struct zbud_pool *pool)`**  \n  销毁指定的 zbud 内存池（要求池已清空）。\n\n- **`zbud_alloc(struct zbud_pool *pool, size_t size, gfp_t gfp, unsigned long *handle)`**  \n  在池中分配指定大小的内存区域，返回不透明句柄。\n\n- **`zbud_free(struct zbud_pool *pool, unsigned long handle)`**  \n  释放由句柄标识的分配区域。\n\n- **`zbud_map(struct zbud_pool *pool, unsigned long handle)`**  \n  将句柄映射为可访问的虚拟地址指针。\n\n- **`zbud_unmap(struct zbud_pool *pool, unsigned long handle)`**  \n  解除句柄的映射。\n\n- **辅助函数**：\n  - `size_to_chunks()`：将字节大小转换为 chunk 数量。\n  - `init_zbud_page()` / `free_zbud_page()`：初始化/释放 zbud 页面。\n  - `encode_handle()` / `handle_to_zbud_header()`：句柄编码与解码。\n  - `num_free_chunks()`：计算 zbud 页面中的空闲 chunk 数。\n\n## 3. 关键实现\n\n### 内存布局与配对机制\n\n- 每个 **zbud 页面**（物理页）被划分为固定大小的 **chunks**（默认 `PAGE_SIZE / 64`，由 `NCHUNKS_ORDER=6` 决定）。\n- 第一个 chunk 被 `zbud_header` 占用，剩余 `NCHUNKS = 63` 个 chunks 可用于存储数据。\n- **First buddy** 从页面起始位置（跳过 header）向右分配（左对齐）。\n- **Last buddy** 从页面末尾向左分配（右对齐）。\n- 当任一 buddy 被释放时，其空间会与中间的 slack space 合并，形成页面内最大的连续空闲区域，便于后续分配。\n\n### 空闲管理策略\n\n- 使用 **`unbuddied[NCHUNKS]` 数组** 管理单配对页面：\n  - 索引 `i` 对应空闲 chunk 数为 `i` 的页面。\n  - 分配时优先遍历满足需求的最小空闲列表（best-fit 策略）。\n- **`buddied` 链表** 管理已满（双配对）的页面，无法再分配。\n\n### 句柄机制\n\n- 句柄本质是数据在页面内的虚拟地址，但通过 `encode_handle()` 封装：\n  - First buddy 句柄 = `zhdr 地址 + ZHDR_SIZE_ALIGNED`\n  - Last buddy 句柄 = `页面起始地址 + PAGE_SIZE - (last_chunks << CHUNK_SHIFT)`\n- 通过 `handle & PAGE_MASK` 可快速还原出 `zbud_header` 指针。\n\n### 密度保证\n\n- 由于每个 zbud 页面至少可容纳一个压缩页，因此 **zpages : zbud pages ≥ 1**，确保不会因压缩反而增加内存消耗。\n\n## 4. 依赖关系\n\n- **内核头文件**：\n  - `<linux/atomic.h>`、`<linux/spinlock.h>`：提供原子操作和自旋锁支持。\n  - `<linux/list.h>`：链表操作。\n  - `<linux/mm.h>`、`<linux/slab.h>`：内存页和 slab 分配器接口。\n  - `<linux/zpool.h>`：zbud 作为 zpool API 的一种后端实现，需符合其接口规范。\n- **架构依赖**：使用 `PAGE_SHIFT`、`PAGE_MASK` 等与页大小相关的宏，依赖体系结构定义。\n- **内存属性限制**：分配时禁止使用 `__GFP_HIGHMEM`，因高内存页无法直接映射访问。\n\n## 5. 使用场景\n\n- **zswap**：Linux 内核的交换页压缩缓存机制，使用 zbud（或 z3fold/zsmalloc）作为后端分配器存储压缩后的交换页。\n- **zcache**（历史项目）：早期基于 transcendent memory 的压缩缓存，zbud 最初为其设计。\n- **其他需要确定性回收行为的压缩内存池**：适用于对内存回收延迟敏感、且能接受较低存储密度的场景。\n- **作为 zpool 的注册后端**：通过 `zpool_register_driver()` 注册，供上层子系统按需选择。",
      "similarity": 0.4983832836151123,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/zbud.c",
          "start_line": 1,
          "end_line": 126,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * zbud.c",
            " *",
            " * Copyright (C) 2013, Seth Jennings, IBM",
            " *",
            " * Concepts based on zcache internal zbud allocator by Dan Magenheimer.",
            " *",
            " * zbud is an special purpose allocator for storing compressed pages.  Contrary",
            " * to what its name may suggest, zbud is not a buddy allocator, but rather an",
            " * allocator that \"buddies\" two compressed pages together in a single memory",
            " * page.",
            " *",
            " * While this design limits storage density, it has simple and deterministic",
            " * reclaim properties that make it preferable to a higher density approach when",
            " * reclaim will be used.",
            " *",
            " * zbud works by storing compressed pages, or \"zpages\", together in pairs in a",
            " * single memory page called a \"zbud page\".  The first buddy is \"left",
            " * justified\" at the beginning of the zbud page, and the last buddy is \"right",
            " * justified\" at the end of the zbud page.  The benefit is that if either",
            " * buddy is freed, the freed buddy space, coalesced with whatever slack space",
            " * that existed between the buddies, results in the largest possible free region",
            " * within the zbud page.",
            " *",
            " * zbud also provides an attractive lower bound on density. The ratio of zpages",
            " * to zbud pages can not be less than 1.  This ensures that zbud can never \"do",
            " * harm\" by using more pages to store zpages than the uncompressed zpages would",
            " * have used on their own.",
            " *",
            " * zbud pages are divided into \"chunks\".  The size of the chunks is fixed at",
            " * compile time and determined by NCHUNKS_ORDER below.  Dividing zbud pages",
            " * into chunks allows organizing unbuddied zbud pages into a manageable number",
            " * of unbuddied lists according to the number of free chunks available in the",
            " * zbud page.",
            " *",
            " * The zbud API differs from that of conventional allocators in that the",
            " * allocation function, zbud_alloc(), returns an opaque handle to the user,",
            " * not a dereferenceable pointer.  The user must map the handle using",
            " * zbud_map() in order to get a usable pointer by which to access the",
            " * allocation data and unmap the handle with zbud_unmap() when operations",
            " * on the allocation data are complete.",
            " */",
            "",
            "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt",
            "",
            "#include <linux/atomic.h>",
            "#include <linux/list.h>",
            "#include <linux/mm.h>",
            "#include <linux/module.h>",
            "#include <linux/preempt.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/zpool.h>",
            "",
            "/*****************",
            " * Structures",
            "*****************/",
            "/*",
            " * NCHUNKS_ORDER determines the internal allocation granularity, effectively",
            " * adjusting internal fragmentation.  It also determines the number of",
            " * freelists maintained in each pool. NCHUNKS_ORDER of 6 means that the",
            " * allocation granularity will be in chunks of size PAGE_SIZE/64. As one chunk",
            " * in allocated page is occupied by zbud header, NCHUNKS will be calculated to",
            " * 63 which shows the max number of free chunks in zbud page, also there will be",
            " * 63 freelists per pool.",
            " */",
            "#define NCHUNKS_ORDER\t6",
            "",
            "#define CHUNK_SHIFT\t(PAGE_SHIFT - NCHUNKS_ORDER)",
            "#define CHUNK_SIZE\t(1 << CHUNK_SHIFT)",
            "#define ZHDR_SIZE_ALIGNED CHUNK_SIZE",
            "#define NCHUNKS\t\t((PAGE_SIZE - ZHDR_SIZE_ALIGNED) >> CHUNK_SHIFT)",
            "",
            "struct zbud_pool;",
            "",
            "/**",
            " * struct zbud_pool - stores metadata for each zbud pool",
            " * @lock:\tprotects all pool fields and first|last_chunk fields of any",
            " *\t\tzbud page in the pool",
            " * @unbuddied:\tarray of lists tracking zbud pages that only contain one buddy;",
            " *\t\tthe lists each zbud page is added to depends on the size of",
            " *\t\tits free region.",
            " * @buddied:\tlist tracking the zbud pages that contain two buddies;",
            " *\t\tthese zbud pages are full",
            " * @pages_nr:\tnumber of zbud pages in the pool.",
            " *",
            " * This structure is allocated at pool creation time and maintains metadata",
            " * pertaining to a particular zbud pool.",
            " */",
            "struct zbud_pool {",
            "\tspinlock_t lock;",
            "\tunion {",
            "\t\t/*",
            "\t\t * Reuse unbuddied[0] as buddied on the ground that",
            "\t\t * unbuddied[0] is unused.",
            "\t\t */",
            "\t\tstruct list_head buddied;",
            "\t\tstruct list_head unbuddied[NCHUNKS];",
            "\t};",
            "\tu64 pages_nr;",
            "};",
            "",
            "/*",
            " * struct zbud_header - zbud page metadata occupying the first chunk of each",
            " *\t\t\tzbud page.",
            " * @buddy:\tlinks the zbud page into the unbuddied/buddied lists in the pool",
            " * @first_chunks:\tthe size of the first buddy in chunks, 0 if free",
            " * @last_chunks:\tthe size of the last buddy in chunks, 0 if free",
            " */",
            "struct zbud_header {",
            "\tstruct list_head buddy;",
            "\tunsigned int first_chunks;",
            "\tunsigned int last_chunks;",
            "};",
            "",
            "/*****************",
            " * Helpers",
            "*****************/",
            "/* Just to make the code easier to read */",
            "enum buddy {",
            "\tFIRST,",
            "\tLAST",
            "};",
            "",
            "/* Converts an allocation size in bytes to size in zbud chunks */"
          ],
          "function_name": null,
          "description": "定义了zbud内存分配器的核心结构体和宏，其中NCHUNKS_ORDER决定分配粒度，struct zbud_pool维护池元数据及空闲列表，struct zbud_header存储页元信息，为后续分配和回收提供基础数据结构。",
          "similarity": 0.4462505280971527
        },
        {
          "chunk_id": 1,
          "file_path": "mm/zbud.c",
          "start_line": 127,
          "end_line": 252,
          "content": [
            "static int size_to_chunks(size_t size)",
            "{",
            "\treturn (size + CHUNK_SIZE - 1) >> CHUNK_SHIFT;",
            "}",
            "static void free_zbud_page(struct zbud_header *zhdr)",
            "{",
            "\t__free_page(virt_to_page(zhdr));",
            "}",
            "static unsigned long encode_handle(struct zbud_header *zhdr, enum buddy bud)",
            "{",
            "\tunsigned long handle;",
            "",
            "\t/*",
            "\t * For now, the encoded handle is actually just the pointer to the data",
            "\t * but this might not always be the case.  A little information hiding.",
            "\t * Add CHUNK_SIZE to the handle if it is the first allocation to jump",
            "\t * over the zbud header in the first chunk.",
            "\t */",
            "\thandle = (unsigned long)zhdr;",
            "\tif (bud == FIRST)",
            "\t\t/* skip over zbud header */",
            "\t\thandle += ZHDR_SIZE_ALIGNED;",
            "\telse /* bud == LAST */",
            "\t\thandle += PAGE_SIZE - (zhdr->last_chunks  << CHUNK_SHIFT);",
            "\treturn handle;",
            "}",
            "static int num_free_chunks(struct zbud_header *zhdr)",
            "{",
            "\t/*",
            "\t * Rather than branch for different situations, just use the fact that",
            "\t * free buddies have a length of zero to simplify everything.",
            "\t */",
            "\treturn NCHUNKS - zhdr->first_chunks - zhdr->last_chunks;",
            "}",
            "static void zbud_destroy_pool(struct zbud_pool *pool)",
            "{",
            "\tkfree(pool);",
            "}",
            "static int zbud_alloc(struct zbud_pool *pool, size_t size, gfp_t gfp,",
            "\t\t\tunsigned long *handle)",
            "{",
            "\tint chunks, i, freechunks;",
            "\tstruct zbud_header *zhdr = NULL;",
            "\tenum buddy bud;",
            "\tstruct page *page;",
            "",
            "\tif (!size || (gfp & __GFP_HIGHMEM))",
            "\t\treturn -EINVAL;",
            "\tif (size > PAGE_SIZE - ZHDR_SIZE_ALIGNED - CHUNK_SIZE)",
            "\t\treturn -ENOSPC;",
            "\tchunks = size_to_chunks(size);",
            "\tspin_lock(&pool->lock);",
            "",
            "\t/* First, try to find an unbuddied zbud page. */",
            "\tfor_each_unbuddied_list(i, chunks) {",
            "\t\tif (!list_empty(&pool->unbuddied[i])) {",
            "\t\t\tzhdr = list_first_entry(&pool->unbuddied[i],",
            "\t\t\t\t\tstruct zbud_header, buddy);",
            "\t\t\tlist_del(&zhdr->buddy);",
            "\t\t\tif (zhdr->first_chunks == 0)",
            "\t\t\t\tbud = FIRST;",
            "\t\t\telse",
            "\t\t\t\tbud = LAST;",
            "\t\t\tgoto found;",
            "\t\t}",
            "\t}",
            "",
            "\t/* Couldn't find unbuddied zbud page, create new one */",
            "\tspin_unlock(&pool->lock);",
            "\tpage = alloc_page(gfp);",
            "\tif (!page)",
            "\t\treturn -ENOMEM;",
            "\tspin_lock(&pool->lock);",
            "\tpool->pages_nr++;",
            "\tzhdr = init_zbud_page(page);",
            "\tbud = FIRST;",
            "",
            "found:",
            "\tif (bud == FIRST)",
            "\t\tzhdr->first_chunks = chunks;",
            "\telse",
            "\t\tzhdr->last_chunks = chunks;",
            "",
            "\tif (zhdr->first_chunks == 0 || zhdr->last_chunks == 0) {",
            "\t\t/* Add to unbuddied list */",
            "\t\tfreechunks = num_free_chunks(zhdr);",
            "\t\tlist_add(&zhdr->buddy, &pool->unbuddied[freechunks]);",
            "\t} else {",
            "\t\t/* Add to buddied list */",
            "\t\tlist_add(&zhdr->buddy, &pool->buddied);",
            "\t}",
            "",
            "\t*handle = encode_handle(zhdr, bud);",
            "\tspin_unlock(&pool->lock);",
            "",
            "\treturn 0;",
            "}",
            "static void zbud_free(struct zbud_pool *pool, unsigned long handle)",
            "{",
            "\tstruct zbud_header *zhdr;",
            "\tint freechunks;",
            "",
            "\tspin_lock(&pool->lock);",
            "\tzhdr = handle_to_zbud_header(handle);",
            "",
            "\t/* If first buddy, handle will be page aligned */",
            "\tif ((handle - ZHDR_SIZE_ALIGNED) & ~PAGE_MASK)",
            "\t\tzhdr->last_chunks = 0;",
            "\telse",
            "\t\tzhdr->first_chunks = 0;",
            "",
            "\t/* Remove from existing buddy list */",
            "\tlist_del(&zhdr->buddy);",
            "",
            "\tif (zhdr->first_chunks == 0 && zhdr->last_chunks == 0) {",
            "\t\t/* zbud page is empty, free */",
            "\t\tfree_zbud_page(zhdr);",
            "\t\tpool->pages_nr--;",
            "\t} else {",
            "\t\t/* Add to unbuddied list */",
            "\t\tfreechunks = num_free_chunks(zhdr);",
            "\t\tlist_add(&zhdr->buddy, &pool->unbuddied[freechunks]);",
            "\t}",
            "",
            "\tspin_unlock(&pool->lock);",
            "}"
          ],
          "function_name": "size_to_chunks, free_zbud_page, encode_handle, num_free_chunks, zbud_destroy_pool, zbud_alloc, zbud_free",
          "description": "实现了zbud分配器的关键功能，包含大小转块计算、页面释放、句柄编码、空闲块统计等辅助函数，核心函数zbud_alloc尝试从空闲列表获取页或新建页面并分割存储，zbud_free处理释放逻辑并重新加入空闲列表。",
          "similarity": 0.4379193186759949
        },
        {
          "chunk_id": 2,
          "file_path": "mm/zbud.c",
          "start_line": 363,
          "end_line": 405,
          "content": [
            "static void zbud_unmap(struct zbud_pool *pool, unsigned long handle)",
            "{",
            "}",
            "static u64 zbud_get_pool_size(struct zbud_pool *pool)",
            "{",
            "\treturn pool->pages_nr;",
            "}",
            "static void zbud_zpool_destroy(void *pool)",
            "{",
            "\tzbud_destroy_pool(pool);",
            "}",
            "static int zbud_zpool_malloc(void *pool, size_t size, gfp_t gfp,",
            "\t\t\tunsigned long *handle)",
            "{",
            "\treturn zbud_alloc(pool, size, gfp, handle);",
            "}",
            "static void zbud_zpool_free(void *pool, unsigned long handle)",
            "{",
            "\tzbud_free(pool, handle);",
            "}",
            "static void zbud_zpool_unmap(void *pool, unsigned long handle)",
            "{",
            "\tzbud_unmap(pool, handle);",
            "}",
            "static u64 zbud_zpool_total_size(void *pool)",
            "{",
            "\treturn zbud_get_pool_size(pool) * PAGE_SIZE;",
            "}",
            "static int __init init_zbud(void)",
            "{",
            "\t/* Make sure the zbud header will fit in one chunk */",
            "\tBUILD_BUG_ON(sizeof(struct zbud_header) > ZHDR_SIZE_ALIGNED);",
            "\tpr_info(\"loaded\\n\");",
            "",
            "\tzpool_register_driver(&zbud_zpool_driver);",
            "",
            "\treturn 0;",
            "}",
            "static void __exit exit_zbud(void)",
            "{",
            "\tzpool_unregister_driver(&zbud_zpool_driver);",
            "\tpr_info(\"unloaded\\n\");",
            "}"
          ],
          "function_name": "zbud_unmap, zbud_get_pool_size, zbud_zpool_destroy, zbud_zpool_malloc, zbud_zpool_free, zbud_zpool_unmap, zbud_zpool_total_size, init_zbud, exit_zbud",
          "description": "提供了zpool驱动接口实现，包含池大小查询、内存释放、句柄解码等操作，通过zpool_register_driver注册驱动，init_zbud和exit_zbud分别负责模块加载时的初始化和卸载清理工作。",
          "similarity": 0.37083348631858826
        }
      ]
    },
    {
      "source_file": "mm/z3fold.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:35:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `z3fold.c`\n\n---\n\n# z3fold.c 技术文档\n\n## 1. 文件概述\n\n`z3fold.c` 实现了一个专用于存储压缩页面的特殊用途内存分配器——**z3fold**。该分配器继承自 `zbud` 的设计理念，但进行了优化：每个物理页最多可容纳**三个**压缩对象（而 zbud 仅支持两个），从而提升了压缩密度，同时保留了“每页存储整数个对象”的确定性特性。这种设计在需要高效内存回收（reclaim）的场景下，相比无固定对象数量限制的高密度方案更具优势。z3fold 不直接对外暴露 API，而是通过 **zpool** 接口被上层（如 zswap、zram）调用。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct z3fold_header`**: 存储在每个 z3fold 页面起始位置的元数据结构（HEADLESS 页除外）。包含：\n  - 页面引用计数 (`refcount`) 和自旋锁 (`page_lock`)\n  - 指向所属池 (`pool`) 和 CPU 亲和性 (`cpu`)\n  - 三个 buddy 区域（first, middle, last）的大小及起始位置\n  - 状态标志（如映射计数 `mapped_count`、外部句柄标记 `foreign_handles`）\n  - 用于后台优化的工作队列项 (`work`)\n- **`struct z3fold_buddy_slots`**: 管理页面内对象句柄的槽位结构，包含读写锁 (`lock`) 和指向所属池的反向链接。\n- **`struct z3fold_pool`**: 代表一个 z3fold 内存池，包含：\n  - 每 CPU 的 `unbuddied` 列表数组（按空闲区域大小分类）\n  - 待释放的 `stale` 页面列表\n  - 专用 slab 缓存 (`c_handle`) 用于分配 `buddy_slots`\n  - 后台工作队列 (`compact_wq`, `release_wq`) 用于页面整理和安全释放\n\n### 关键枚举与宏\n- **`enum buddy`**: 定义页面内对象的三种类型：`FIRST`（页首）、`MIDDLE`（中间）、`LAST`（页尾），以及特殊的 `HEADLESS`（无头部元数据页）。\n- **`NCHUNKS_ORDER`**: 核心配置参数（默认为 6），决定内部分配粒度为 `PAGE_SIZE / 64`。\n- **页面标志 (`enum z3fold_page_flags`)**: 如 `PAGE_HEADLESS`、`NEEDS_COMPACTING`、`PAGE_STALE` 等，用于管理页面状态。\n- **句柄标志 (`enum z3fold_handle_flags`)**: 控制句柄行为（如 `HANDLES_NOFREE`）。\n\n### 核心辅助函数\n- **`size_to_chunks()`**: 将字节大小转换为 chunk 单位。\n- **`alloc_slots()` / `slots_to_pool()`**: 管理 `buddy_slots` 的分配与池关联。\n- **`handle_to_slots()` / `get_z3fold_header()`**: 从用户句柄解析出底层元数据结构，并处理并发访问与迁移。\n- **`z3fold_page_lock/unlock/trylock()`**: 提供页面级细粒度锁操作。\n\n## 3. 关键实现\n\n### 内存布局与分配策略\n- **Chunk 粒度**: 页面被划分为 `TOTAL_CHUNKS = PAGE_SIZE / CHUNK_SIZE` 个固定大小的 chunk（默认 64 字节）。前 `ZHDR_CHUNKS` 个 chunk 被 `z3fold_header` 占用，剩余 `NCHUNKS`（约 62-63）个用于存储数据。\n- **三 Buddy 设计**: \n  - **First Buddy**: 从 `header` 之后开始分配。\n  - **Last Buddy**: 从页面末尾向前分配。\n  - **Middle Buddy**: 在 First 和 Last 之间动态分配，最大化利用碎片空间。\n- **HEADLESS 页**: 当对象大小接近整个页面时，跳过 header 直接使用整页，减少元数据开销。\n\n### 并发与迁移安全\n- **双层锁机制**: \n  - `z3fold_header.page_lock` 保护单个页面的元数据修改。\n  - `z3fold_buddy_slots.lock` (rwlock) 保护句柄到地址的映射，支持高并发读取。\n- **迁移处理**: 在 `get_z3fold_header()` 中检查 `PAGE_MIGRATED` 标志，若页面正在迁移则重试获取锁，确保访问安全。\n\n### 后台优化\n- **页面整理 (`compact_page_work`)**: 通过工作队列异步移动 Middle Buddy 对象，尝试合并空闲区域以容纳更大对象。\n- **安全释放**: 使用独立工作队列 (`release_wq`) 延迟释放被标记为 `PAGE_STALE` 的页面，避免在中断上下文或持有锁时执行高开销操作。\n\n### 句柄编码\n- 用户句柄 (`handle`) 是一个 `unsigned long`，其低 2 位 (`HANDLE_FLAG_MASK`) 用于存储标志（如 `PAGE_HEADLESS`），其余位编码 `buddy_slots` 地址或直接指向页面。这允许在不解引用的情况下快速判断页面类型。\n\n## 4. 依赖关系\n\n- **核心依赖**: \n  - `<linux/zpool.h>`: 作为 zpool 驱动注册，提供 `zpool_ops` 接口。\n  - `<linux/slab.h>`: 使用 kmem_cache 分配 `z3fold_buddy_slots`。\n  - `<linux/workqueue.h>`: 依赖内核工作队列机制执行后台任务。\n- **内存管理子系统**: \n  - 依赖 `alloc_pages()`/`__free_pages()` 进行底层页分配。\n  - 使用 `page->private` 存储页面标志位。\n  - 与内存压缩 (`compaction.h`) 和迁移 (`migrate.h`) 机制交互。\n- **同步原语**: 大量使用 `spinlock_t` 和 `rwlock_t` 保证 SMP 安全。\n\n## 5. 使用场景\n\nz3fold 主要作为 **zpool** 的后端分配器，服务于需要高效压缩内存的子系统：\n- **zswap**: 作为交换页的压缩缓存，z3fold 的高密度存储可显著减少实际交换 I/O。\n- **zram**: 作为基于 RAM 的块设备，z3fold 提升其有效存储容量。\n- **其他内存压缩框架**: 任何需要将变长小对象（尤其是压缩数据）高效打包进物理页的场景。\n\n其**确定性回收特性**（每页对象数固定、布局简单）使其在内存压力大时能快速找到可回收页面，优于更复杂的分配器（如 zsmalloc），特别适合嵌入式或实时系统。",
      "similarity": 0.4816780686378479,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "mm/z3fold.c",
          "start_line": 1092,
          "end_line": 1219,
          "content": [
            "static void z3fold_free(struct z3fold_pool *pool, unsigned long handle)",
            "{",
            "\tstruct z3fold_header *zhdr;",
            "\tstruct page *page;",
            "\tenum buddy bud;",
            "\tbool page_claimed;",
            "",
            "\tzhdr = get_z3fold_header(handle);",
            "\tpage = virt_to_page(zhdr);",
            "\tpage_claimed = test_and_set_bit(PAGE_CLAIMED, &page->private);",
            "",
            "\tif (test_bit(PAGE_HEADLESS, &page->private)) {",
            "\t\t/* if a headless page is under reclaim, just leave.",
            "\t\t * NB: we use test_and_set_bit for a reason: if the bit",
            "\t\t * has not been set before, we release this page",
            "\t\t * immediately so we don't care about its value any more.",
            "\t\t */",
            "\t\tif (!page_claimed) {",
            "\t\t\tput_z3fold_header(zhdr);",
            "\t\t\tfree_z3fold_page(page, true);",
            "\t\t\tatomic64_dec(&pool->pages_nr);",
            "\t\t}",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Non-headless case */",
            "\tbud = handle_to_buddy(handle);",
            "",
            "\tswitch (bud) {",
            "\tcase FIRST:",
            "\t\tzhdr->first_chunks = 0;",
            "\t\tbreak;",
            "\tcase MIDDLE:",
            "\t\tzhdr->middle_chunks = 0;",
            "\t\tbreak;",
            "\tcase LAST:",
            "\t\tzhdr->last_chunks = 0;",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tpr_err(\"%s: unknown bud %d\\n\", __func__, bud);",
            "\t\tWARN_ON(1);",
            "\t\tput_z3fold_header(zhdr);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!page_claimed)",
            "\t\tfree_handle(handle, zhdr);",
            "\tif (put_z3fold_locked_list(zhdr))",
            "\t\treturn;",
            "\tif (page_claimed) {",
            "\t\t/* the page has not been claimed by us */",
            "\t\tput_z3fold_header(zhdr);",
            "\t\treturn;",
            "\t}",
            "\tif (test_and_set_bit(NEEDS_COMPACTING, &page->private)) {",
            "\t\tclear_bit(PAGE_CLAIMED, &page->private);",
            "\t\tput_z3fold_header(zhdr);",
            "\t\treturn;",
            "\t}",
            "\tif (zhdr->cpu < 0 || !cpu_online(zhdr->cpu)) {",
            "\t\tzhdr->cpu = -1;",
            "\t\tkref_get(&zhdr->refcount);",
            "\t\tclear_bit(PAGE_CLAIMED, &page->private);",
            "\t\tdo_compact_page(zhdr, true);",
            "\t\treturn;",
            "\t}",
            "\tkref_get(&zhdr->refcount);",
            "\tclear_bit(PAGE_CLAIMED, &page->private);",
            "\tqueue_work_on(zhdr->cpu, pool->compact_wq, &zhdr->work);",
            "\tput_z3fold_header(zhdr);",
            "}",
            "static void z3fold_unmap(struct z3fold_pool *pool, unsigned long handle)",
            "{",
            "\tstruct z3fold_header *zhdr;",
            "\tstruct page *page;",
            "\tenum buddy buddy;",
            "",
            "\tzhdr = get_z3fold_header(handle);",
            "\tpage = virt_to_page(zhdr);",
            "",
            "\tif (test_bit(PAGE_HEADLESS, &page->private))",
            "\t\treturn;",
            "",
            "\tbuddy = handle_to_buddy(handle);",
            "\tif (buddy == MIDDLE)",
            "\t\tclear_bit(MIDDLE_CHUNK_MAPPED, &page->private);",
            "\tzhdr->mapped_count--;",
            "\tput_z3fold_header(zhdr);",
            "}",
            "static u64 z3fold_get_pool_size(struct z3fold_pool *pool)",
            "{",
            "\treturn atomic64_read(&pool->pages_nr);",
            "}",
            "static bool z3fold_page_isolate(struct page *page, isolate_mode_t mode)",
            "{",
            "\tstruct z3fold_header *zhdr;",
            "\tstruct z3fold_pool *pool;",
            "",
            "\tVM_BUG_ON_PAGE(PageIsolated(page), page);",
            "",
            "\tif (test_bit(PAGE_HEADLESS, &page->private))",
            "\t\treturn false;",
            "",
            "\tzhdr = page_address(page);",
            "\tz3fold_page_lock(zhdr);",
            "\tif (test_bit(NEEDS_COMPACTING, &page->private) ||",
            "\t    test_bit(PAGE_STALE, &page->private))",
            "\t\tgoto out;",
            "",
            "\tif (zhdr->mapped_count != 0 || zhdr->foreign_handles != 0)",
            "\t\tgoto out;",
            "",
            "\tif (test_and_set_bit(PAGE_CLAIMED, &page->private))",
            "\t\tgoto out;",
            "\tpool = zhdr_to_pool(zhdr);",
            "\tspin_lock(&pool->lock);",
            "\tif (!list_empty(&zhdr->buddy))",
            "\t\tlist_del_init(&zhdr->buddy);",
            "\tspin_unlock(&pool->lock);",
            "",
            "\tkref_get(&zhdr->refcount);",
            "\tz3fold_page_unlock(zhdr);",
            "\treturn true;",
            "",
            "out:",
            "\tz3fold_page_unlock(zhdr);",
            "\treturn false;",
            "}"
          ],
          "function_name": "z3fold_free, z3fold_unmap, z3fold_get_pool_size, z3fold_page_isolate",
          "description": "z3fold_free处理Z3Fold页面释放逻辑，根据headless或非headless情况区分处理，清除buddy类型计数并触发页压缩或直接回收。z3fold_unmap减少映射计数并清除中间块映射标志。z3fold_get_pool_size返回当前池中页面总数。z3fold_page_isolate尝试将页面隔离至Z3Fold结构，检查是否满足条件并将其加入buddy链表。",
          "similarity": 0.44960451126098633
        },
        {
          "chunk_id": 0,
          "file_path": "mm/z3fold.c",
          "start_line": 1,
          "end_line": 185,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * z3fold.c",
            " *",
            " * Author: Vitaly Wool <vitaly.wool@konsulko.com>",
            " * Copyright (C) 2016, Sony Mobile Communications Inc.",
            " *",
            " * This implementation is based on zbud written by Seth Jennings.",
            " *",
            " * z3fold is an special purpose allocator for storing compressed pages. It",
            " * can store up to three compressed pages per page which improves the",
            " * compression ratio of zbud while retaining its main concepts (e. g. always",
            " * storing an integral number of objects per page) and simplicity.",
            " * It still has simple and deterministic reclaim properties that make it",
            " * preferable to a higher density approach (with no requirement on integral",
            " * number of object per page) when reclaim is used.",
            " *",
            " * As in zbud, pages are divided into \"chunks\".  The size of the chunks is",
            " * fixed at compile time and is determined by NCHUNKS_ORDER below.",
            " *",
            " * z3fold doesn't export any API and is meant to be used via zpool API.",
            " */",
            "",
            "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt",
            "",
            "#include <linux/atomic.h>",
            "#include <linux/sched.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/list.h>",
            "#include <linux/mm.h>",
            "#include <linux/module.h>",
            "#include <linux/page-flags.h>",
            "#include <linux/migrate.h>",
            "#include <linux/node.h>",
            "#include <linux/compaction.h>",
            "#include <linux/percpu.h>",
            "#include <linux/preempt.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/zpool.h>",
            "#include <linux/kmemleak.h>",
            "",
            "/*",
            " * NCHUNKS_ORDER determines the internal allocation granularity, effectively",
            " * adjusting internal fragmentation.  It also determines the number of",
            " * freelists maintained in each pool. NCHUNKS_ORDER of 6 means that the",
            " * allocation granularity will be in chunks of size PAGE_SIZE/64. Some chunks",
            " * in the beginning of an allocated page are occupied by z3fold header, so",
            " * NCHUNKS will be calculated to 63 (or 62 in case CONFIG_DEBUG_SPINLOCK=y),",
            " * which shows the max number of free chunks in z3fold page, also there will",
            " * be 63, or 62, respectively, freelists per pool.",
            " */",
            "#define NCHUNKS_ORDER\t6",
            "",
            "#define CHUNK_SHIFT\t(PAGE_SHIFT - NCHUNKS_ORDER)",
            "#define CHUNK_SIZE\t(1 << CHUNK_SHIFT)",
            "#define ZHDR_SIZE_ALIGNED round_up(sizeof(struct z3fold_header), CHUNK_SIZE)",
            "#define ZHDR_CHUNKS\t(ZHDR_SIZE_ALIGNED >> CHUNK_SHIFT)",
            "#define TOTAL_CHUNKS\t(PAGE_SIZE >> CHUNK_SHIFT)",
            "#define NCHUNKS\t\t(TOTAL_CHUNKS - ZHDR_CHUNKS)",
            "",
            "#define BUDDY_MASK\t(0x3)",
            "#define BUDDY_SHIFT\t2",
            "#define SLOTS_ALIGN\t(0x40)",
            "",
            "/*****************",
            " * Structures",
            "*****************/",
            "struct z3fold_pool;",
            "",
            "enum buddy {",
            "\tHEADLESS = 0,",
            "\tFIRST,",
            "\tMIDDLE,",
            "\tLAST,",
            "\tBUDDIES_MAX = LAST",
            "};",
            "",
            "struct z3fold_buddy_slots {",
            "\t/*",
            "\t * we are using BUDDY_MASK in handle_to_buddy etc. so there should",
            "\t * be enough slots to hold all possible variants",
            "\t */",
            "\tunsigned long slot[BUDDY_MASK + 1];",
            "\tunsigned long pool; /* back link */",
            "\trwlock_t lock;",
            "};",
            "#define HANDLE_FLAG_MASK\t(0x03)",
            "",
            "/*",
            " * struct z3fold_header - z3fold page metadata occupying first chunks of each",
            " *\t\t\tz3fold page, except for HEADLESS pages",
            " * @buddy:\t\tlinks the z3fold page into the relevant list in the",
            " *\t\t\tpool",
            " * @page_lock:\t\tper-page lock",
            " * @refcount:\t\treference count for the z3fold page",
            " * @work:\t\twork_struct for page layout optimization",
            " * @slots:\t\tpointer to the structure holding buddy slots",
            " * @pool:\t\tpointer to the containing pool",
            " * @cpu:\t\tCPU which this page \"belongs\" to",
            " * @first_chunks:\tthe size of the first buddy in chunks, 0 if free",
            " * @middle_chunks:\tthe size of the middle buddy in chunks, 0 if free",
            " * @last_chunks:\tthe size of the last buddy in chunks, 0 if free",
            " * @first_num:\t\tthe starting number (for the first handle)",
            " * @mapped_count:\tthe number of objects currently mapped",
            " */",
            "struct z3fold_header {",
            "\tstruct list_head buddy;",
            "\tspinlock_t page_lock;",
            "\tstruct kref refcount;",
            "\tstruct work_struct work;",
            "\tstruct z3fold_buddy_slots *slots;",
            "\tstruct z3fold_pool *pool;",
            "\tshort cpu;",
            "\tunsigned short first_chunks;",
            "\tunsigned short middle_chunks;",
            "\tunsigned short last_chunks;",
            "\tunsigned short start_middle;",
            "\tunsigned short first_num:2;",
            "\tunsigned short mapped_count:2;",
            "\tunsigned short foreign_handles:2;",
            "};",
            "",
            "/**",
            " * struct z3fold_pool - stores metadata for each z3fold pool",
            " * @name:\tpool name",
            " * @lock:\tprotects pool unbuddied lists",
            " * @stale_lock:\tprotects pool stale page list",
            " * @unbuddied:\tper-cpu array of lists tracking z3fold pages that contain 2-",
            " *\t\tbuddies; the list each z3fold page is added to depends on",
            " *\t\tthe size of its free region.",
            " * @stale:\tlist of pages marked for freeing",
            " * @pages_nr:\tnumber of z3fold pages in the pool.",
            " * @c_handle:\tcache for z3fold_buddy_slots allocation",
            " * @compact_wq:\tworkqueue for page layout background optimization",
            " * @release_wq:\tworkqueue for safe page release",
            " * @work:\twork_struct for safe page release",
            " *",
            " * This structure is allocated at pool creation time and maintains metadata",
            " * pertaining to a particular z3fold pool.",
            " */",
            "struct z3fold_pool {",
            "\tconst char *name;",
            "\tspinlock_t lock;",
            "\tspinlock_t stale_lock;",
            "\tstruct list_head *unbuddied;",
            "\tstruct list_head stale;",
            "\tatomic64_t pages_nr;",
            "\tstruct kmem_cache *c_handle;",
            "\tstruct workqueue_struct *compact_wq;",
            "\tstruct workqueue_struct *release_wq;",
            "\tstruct work_struct work;",
            "};",
            "",
            "/*",
            " * Internal z3fold page flags",
            " */",
            "enum z3fold_page_flags {",
            "\tPAGE_HEADLESS = 0,",
            "\tMIDDLE_CHUNK_MAPPED,",
            "\tNEEDS_COMPACTING,",
            "\tPAGE_STALE,",
            "\tPAGE_CLAIMED, /* by either reclaim or free */",
            "\tPAGE_MIGRATED, /* page is migrated and soon to be released */",
            "};",
            "",
            "/*",
            " * handle flags, go under HANDLE_FLAG_MASK",
            " */",
            "enum z3fold_handle_flags {",
            "\tHANDLES_NOFREE = 0,",
            "};",
            "",
            "/*",
            " * Forward declarations",
            " */",
            "static struct z3fold_header *__z3fold_alloc(struct z3fold_pool *, size_t, bool);",
            "static void compact_page_work(struct work_struct *w);",
            "",
            "/*****************",
            " * Helpers",
            "*****************/",
            "",
            "/* Converts an allocation size in bytes to size in z3fold chunks */"
          ],
          "function_name": null,
          "description": "定义z3fold内存分配器的基础参数和结构体，通过NCHUNKS_ORDER设置内部碎片控制粒度，建立z3fold_header和z3fold_pool结构体，用于管理压缩页面元数据和池级元数据。",
          "similarity": 0.4231160879135132
        },
        {
          "chunk_id": 4,
          "file_path": "mm/z3fold.c",
          "start_line": 776,
          "end_line": 881,
          "content": [
            "static void compact_page_work(struct work_struct *w)",
            "{",
            "\tstruct z3fold_header *zhdr = container_of(w, struct z3fold_header,",
            "\t\t\t\t\t\twork);",
            "",
            "\tdo_compact_page(zhdr, false);",
            "}",
            "static void z3fold_destroy_pool(struct z3fold_pool *pool)",
            "{",
            "\tkmem_cache_destroy(pool->c_handle);",
            "",
            "\t/*",
            "\t * We need to destroy pool->compact_wq before pool->release_wq,",
            "\t * as any pending work on pool->compact_wq will call",
            "\t * queue_work(pool->release_wq, &pool->work).",
            "\t *",
            "\t * There are still outstanding pages until both workqueues are drained,",
            "\t * so we cannot unregister migration until then.",
            "\t */",
            "",
            "\tdestroy_workqueue(pool->compact_wq);",
            "\tdestroy_workqueue(pool->release_wq);",
            "\tfree_percpu(pool->unbuddied);",
            "\tkfree(pool);",
            "}",
            "static int z3fold_alloc(struct z3fold_pool *pool, size_t size, gfp_t gfp,",
            "\t\t\tunsigned long *handle)",
            "{",
            "\tint chunks = size_to_chunks(size);",
            "\tstruct z3fold_header *zhdr = NULL;",
            "\tstruct page *page = NULL;",
            "\tenum buddy bud;",
            "\tbool can_sleep = gfpflags_allow_blocking(gfp);",
            "",
            "\tif (!size || (gfp & __GFP_HIGHMEM))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (size > PAGE_SIZE)",
            "\t\treturn -ENOSPC;",
            "",
            "\tif (size > PAGE_SIZE - ZHDR_SIZE_ALIGNED - CHUNK_SIZE)",
            "\t\tbud = HEADLESS;",
            "\telse {",
            "retry:",
            "\t\tzhdr = __z3fold_alloc(pool, size, can_sleep);",
            "\t\tif (zhdr) {",
            "\t\t\tbud = get_free_buddy(zhdr, chunks);",
            "\t\t\tif (bud == HEADLESS) {",
            "\t\t\t\tif (!put_z3fold_locked(zhdr))",
            "\t\t\t\t\tz3fold_page_unlock(zhdr);",
            "\t\t\t\tpr_err(\"No free chunks in unbuddied\\n\");",
            "\t\t\t\tWARN_ON(1);",
            "\t\t\t\tgoto retry;",
            "\t\t\t}",
            "\t\t\tpage = virt_to_page(zhdr);",
            "\t\t\tgoto found;",
            "\t\t}",
            "\t\tbud = FIRST;",
            "\t}",
            "",
            "\tpage = alloc_page(gfp);",
            "\tif (!page)",
            "\t\treturn -ENOMEM;",
            "",
            "\tzhdr = init_z3fold_page(page, bud == HEADLESS, pool, gfp);",
            "\tif (!zhdr) {",
            "\t\t__free_page(page);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tatomic64_inc(&pool->pages_nr);",
            "",
            "\tif (bud == HEADLESS) {",
            "\t\tset_bit(PAGE_HEADLESS, &page->private);",
            "\t\tgoto headless;",
            "\t}",
            "\tif (can_sleep) {",
            "\t\tlock_page(page);",
            "\t\t__SetPageMovable(page, &z3fold_mops);",
            "\t\tunlock_page(page);",
            "\t} else {",
            "\t\tWARN_ON(!trylock_page(page));",
            "\t\t__SetPageMovable(page, &z3fold_mops);",
            "\t\tunlock_page(page);",
            "\t}",
            "\tz3fold_page_lock(zhdr);",
            "",
            "found:",
            "\tif (bud == FIRST)",
            "\t\tzhdr->first_chunks = chunks;",
            "\telse if (bud == LAST)",
            "\t\tzhdr->last_chunks = chunks;",
            "\telse {",
            "\t\tzhdr->middle_chunks = chunks;",
            "\t\tzhdr->start_middle = zhdr->first_chunks + ZHDR_CHUNKS;",
            "\t}",
            "\tadd_to_unbuddied(pool, zhdr);",
            "",
            "headless:",
            "\tspin_lock(&pool->lock);",
            "\t*handle = encode_handle(zhdr, bud);",
            "\tspin_unlock(&pool->lock);",
            "\tif (bud != HEADLESS)",
            "\t\tz3fold_page_unlock(zhdr);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "compact_page_work, z3fold_destroy_pool, z3fold_alloc",
          "description": "实现内存池销毁和分配主逻辑，包含工作队列驱动的压缩任务、基于不同场景的页面分配策略，以及头部块与普通块的差异化处理机制。",
          "similarity": 0.4173341989517212
        },
        {
          "chunk_id": 3,
          "file_path": "mm/z3fold.c",
          "start_line": 538,
          "end_line": 668,
          "content": [
            "static inline void add_to_unbuddied(struct z3fold_pool *pool,",
            "\t\t\t\tstruct z3fold_header *zhdr)",
            "{",
            "\tif (zhdr->first_chunks == 0 || zhdr->last_chunks == 0 ||",
            "\t\t\tzhdr->middle_chunks == 0) {",
            "\t\tstruct list_head *unbuddied;",
            "\t\tint freechunks = num_free_chunks(zhdr);",
            "",
            "\t\tmigrate_disable();",
            "\t\tunbuddied = this_cpu_ptr(pool->unbuddied);",
            "\t\tspin_lock(&pool->lock);",
            "\t\tlist_add(&zhdr->buddy, &unbuddied[freechunks]);",
            "\t\tspin_unlock(&pool->lock);",
            "\t\tzhdr->cpu = smp_processor_id();",
            "\t\tmigrate_enable();",
            "\t}",
            "}",
            "static inline enum buddy get_free_buddy(struct z3fold_header *zhdr, int chunks)",
            "{",
            "\tenum buddy bud = HEADLESS;",
            "",
            "\tif (zhdr->middle_chunks) {",
            "\t\tif (!zhdr->first_chunks &&",
            "\t\t    chunks <= zhdr->start_middle - ZHDR_CHUNKS)",
            "\t\t\tbud = FIRST;",
            "\t\telse if (!zhdr->last_chunks)",
            "\t\t\tbud = LAST;",
            "\t} else {",
            "\t\tif (!zhdr->first_chunks)",
            "\t\t\tbud = FIRST;",
            "\t\telse if (!zhdr->last_chunks)",
            "\t\t\tbud = LAST;",
            "\t\telse",
            "\t\t\tbud = MIDDLE;",
            "\t}",
            "",
            "\treturn bud;",
            "}",
            "static inline bool buddy_single(struct z3fold_header *zhdr)",
            "{",
            "\treturn !((zhdr->first_chunks && zhdr->middle_chunks) ||",
            "\t\t\t(zhdr->first_chunks && zhdr->last_chunks) ||",
            "\t\t\t(zhdr->middle_chunks && zhdr->last_chunks));",
            "}",
            "static int z3fold_compact_page(struct z3fold_header *zhdr)",
            "{",
            "\tstruct page *page = virt_to_page(zhdr);",
            "",
            "\tif (test_bit(MIDDLE_CHUNK_MAPPED, &page->private))",
            "\t\treturn 0; /* can't move middle chunk, it's used */",
            "",
            "\tif (unlikely(PageIsolated(page)))",
            "\t\treturn 0;",
            "",
            "\tif (zhdr->middle_chunks == 0)",
            "\t\treturn 0; /* nothing to compact */",
            "",
            "\tif (zhdr->first_chunks == 0 && zhdr->last_chunks == 0) {",
            "\t\t/* move to the beginning */",
            "\t\tmchunk_memmove(zhdr, ZHDR_CHUNKS);",
            "\t\tzhdr->first_chunks = zhdr->middle_chunks;",
            "\t\tzhdr->middle_chunks = 0;",
            "\t\tzhdr->start_middle = 0;",
            "\t\tzhdr->first_num++;",
            "\t\treturn 1;",
            "\t}",
            "",
            "\t/*",
            "\t * moving data is expensive, so let's only do that if",
            "\t * there's substantial gain (at least BIG_CHUNK_GAP chunks)",
            "\t */",
            "\tif (zhdr->first_chunks != 0 && zhdr->last_chunks == 0 &&",
            "\t    zhdr->start_middle - (zhdr->first_chunks + ZHDR_CHUNKS) >=",
            "\t\t\tBIG_CHUNK_GAP) {",
            "\t\tmchunk_memmove(zhdr, zhdr->first_chunks + ZHDR_CHUNKS);",
            "\t\tzhdr->start_middle = zhdr->first_chunks + ZHDR_CHUNKS;",
            "\t\treturn 1;",
            "\t} else if (zhdr->last_chunks != 0 && zhdr->first_chunks == 0 &&",
            "\t\t   TOTAL_CHUNKS - (zhdr->last_chunks + zhdr->start_middle",
            "\t\t\t\t\t+ zhdr->middle_chunks) >=",
            "\t\t\tBIG_CHUNK_GAP) {",
            "\t\tunsigned short new_start = TOTAL_CHUNKS - zhdr->last_chunks -",
            "\t\t\tzhdr->middle_chunks;",
            "\t\tmchunk_memmove(zhdr, new_start);",
            "\t\tzhdr->start_middle = new_start;",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static void do_compact_page(struct z3fold_header *zhdr, bool locked)",
            "{",
            "\tstruct z3fold_pool *pool = zhdr_to_pool(zhdr);",
            "\tstruct page *page;",
            "",
            "\tpage = virt_to_page(zhdr);",
            "\tif (locked)",
            "\t\tWARN_ON(z3fold_page_trylock(zhdr));",
            "\telse",
            "\t\tz3fold_page_lock(zhdr);",
            "\tif (WARN_ON(!test_and_clear_bit(NEEDS_COMPACTING, &page->private))) {",
            "\t\tz3fold_page_unlock(zhdr);",
            "\t\treturn;",
            "\t}",
            "\tspin_lock(&pool->lock);",
            "\tlist_del_init(&zhdr->buddy);",
            "\tspin_unlock(&pool->lock);",
            "",
            "\tif (put_z3fold_locked(zhdr))",
            "\t\treturn;",
            "",
            "\tif (test_bit(PAGE_STALE, &page->private) ||",
            "\t    test_and_set_bit(PAGE_CLAIMED, &page->private)) {",
            "\t\tz3fold_page_unlock(zhdr);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!zhdr->foreign_handles && buddy_single(zhdr) &&",
            "\t    zhdr->mapped_count == 0 && compact_single_buddy(zhdr)) {",
            "\t\tif (!put_z3fold_locked(zhdr)) {",
            "\t\t\tclear_bit(PAGE_CLAIMED, &page->private);",
            "\t\t\tz3fold_page_unlock(zhdr);",
            "\t\t}",
            "\t\treturn;",
            "\t}",
            "",
            "\tz3fold_compact_page(zhdr);",
            "\tadd_to_unbuddied(pool, zhdr);",
            "\tclear_bit(PAGE_CLAIMED, &page->private);",
            "\tz3fold_page_unlock(zhdr);",
            "}"
          ],
          "function_name": "add_to_unbuddied, get_free_buddy, buddy_single, z3fold_compact_page, do_compact_page",
          "description": "实现页面迁移优化算法，通过检测可移动块位置、执行块数据迁移、维护未分块页面列表等机制提升空间利用率。",
          "similarity": 0.41347429156303406
        },
        {
          "chunk_id": 6,
          "file_path": "mm/z3fold.c",
          "start_line": 1285,
          "end_line": 1393,
          "content": [
            "static int z3fold_page_migrate(struct page *newpage, struct page *page,",
            "\t\tenum migrate_mode mode)",
            "{",
            "\tstruct z3fold_header *zhdr, *new_zhdr;",
            "\tstruct z3fold_pool *pool;",
            "",
            "\tVM_BUG_ON_PAGE(!PageIsolated(page), page);",
            "\tVM_BUG_ON_PAGE(!test_bit(PAGE_CLAIMED, &page->private), page);",
            "\tVM_BUG_ON_PAGE(!PageLocked(newpage), newpage);",
            "",
            "\tzhdr = page_address(page);",
            "\tpool = zhdr_to_pool(zhdr);",
            "",
            "\tif (!z3fold_page_trylock(zhdr))",
            "\t\treturn -EAGAIN;",
            "\tif (zhdr->mapped_count != 0 || zhdr->foreign_handles != 0) {",
            "\t\tclear_bit(PAGE_CLAIMED, &page->private);",
            "\t\tz3fold_page_unlock(zhdr);",
            "\t\treturn -EBUSY;",
            "\t}",
            "\tif (work_pending(&zhdr->work)) {",
            "\t\tz3fold_page_unlock(zhdr);",
            "\t\treturn -EAGAIN;",
            "\t}",
            "\tnew_zhdr = page_address(newpage);",
            "\tmemcpy(new_zhdr, zhdr, PAGE_SIZE);",
            "\tnewpage->private = page->private;",
            "\tset_bit(PAGE_MIGRATED, &page->private);",
            "\tz3fold_page_unlock(zhdr);",
            "\tspin_lock_init(&new_zhdr->page_lock);",
            "\tINIT_WORK(&new_zhdr->work, compact_page_work);",
            "\t/*",
            "\t * z3fold_page_isolate() ensures that new_zhdr->buddy is empty,",
            "\t * so we only have to reinitialize it.",
            "\t */",
            "\tINIT_LIST_HEAD(&new_zhdr->buddy);",
            "\t__ClearPageMovable(page);",
            "",
            "\tget_page(newpage);",
            "\tz3fold_page_lock(new_zhdr);",
            "\tif (new_zhdr->first_chunks)",
            "\t\tencode_handle(new_zhdr, FIRST);",
            "\tif (new_zhdr->last_chunks)",
            "\t\tencode_handle(new_zhdr, LAST);",
            "\tif (new_zhdr->middle_chunks)",
            "\t\tencode_handle(new_zhdr, MIDDLE);",
            "\tset_bit(NEEDS_COMPACTING, &newpage->private);",
            "\tnew_zhdr->cpu = smp_processor_id();",
            "\t__SetPageMovable(newpage, &z3fold_mops);",
            "\tz3fold_page_unlock(new_zhdr);",
            "",
            "\tqueue_work_on(new_zhdr->cpu, pool->compact_wq, &new_zhdr->work);",
            "",
            "\t/* PAGE_CLAIMED and PAGE_MIGRATED are cleared now. */",
            "\tpage->private = 0;",
            "\tput_page(page);",
            "\treturn 0;",
            "}",
            "static void z3fold_page_putback(struct page *page)",
            "{",
            "\tstruct z3fold_header *zhdr;",
            "\tstruct z3fold_pool *pool;",
            "",
            "\tzhdr = page_address(page);",
            "\tpool = zhdr_to_pool(zhdr);",
            "",
            "\tz3fold_page_lock(zhdr);",
            "\tif (!list_empty(&zhdr->buddy))",
            "\t\tlist_del_init(&zhdr->buddy);",
            "\tINIT_LIST_HEAD(&page->lru);",
            "\tif (put_z3fold_locked(zhdr))",
            "\t\treturn;",
            "\tif (list_empty(&zhdr->buddy))",
            "\t\tadd_to_unbuddied(pool, zhdr);",
            "\tclear_bit(PAGE_CLAIMED, &page->private);",
            "\tz3fold_page_unlock(zhdr);",
            "}",
            "static void z3fold_zpool_destroy(void *pool)",
            "{",
            "\tz3fold_destroy_pool(pool);",
            "}",
            "static int z3fold_zpool_malloc(void *pool, size_t size, gfp_t gfp,",
            "\t\t\tunsigned long *handle)",
            "{",
            "\treturn z3fold_alloc(pool, size, gfp, handle);",
            "}",
            "static void z3fold_zpool_free(void *pool, unsigned long handle)",
            "{",
            "\tz3fold_free(pool, handle);",
            "}",
            "static void z3fold_zpool_unmap(void *pool, unsigned long handle)",
            "{",
            "\tz3fold_unmap(pool, handle);",
            "}",
            "static u64 z3fold_zpool_total_size(void *pool)",
            "{",
            "\treturn z3fold_get_pool_size(pool) * PAGE_SIZE;",
            "}",
            "static int __init init_z3fold(void)",
            "{",
            "\t/*",
            "\t * Make sure the z3fold header is not larger than the page size and",
            "\t * there has remaining spaces for its buddy.",
            "\t */",
            "\tBUILD_BUG_ON(ZHDR_SIZE_ALIGNED > PAGE_SIZE - CHUNK_SIZE);",
            "\tzpool_register_driver(&z3fold_zpool_driver);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "z3fold_page_migrate, z3fold_page_putback, z3fold_zpool_destroy, z3fold_zpool_malloc, z3fold_zpool_free, z3fold_zpool_unmap, z3fold_zpool_total_size, init_z3fold",
          "description": "z3fold_page_migrate执行页面迁移，复制页头信息并更新新页面状态。z3fold_page_putback将页面从buddy列表移除并重新插入LRU列表。z3fold_zpool_*系列函数实现内存池的销毁、分配、释放、解映射和总大小查询。init_z3fold注册Z3Fold内存池驱动。",
          "similarity": 0.39517292380332947
        }
      ]
    },
    {
      "source_file": "kernel/bpf/stackmap.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:30:30\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\stackmap.c`\n\n---\n\n# `bpf/stackmap.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/stackmap.c` 实现了 BPF（Berkeley Packet Filter）子系统中的 **栈映射（stack map）** 功能，用于高效地存储和查询用户态或内核态的调用栈（call stack）信息。该映射支持两种模式：\n- **原始 IP 地址模式**：直接存储程序计数器（PC）地址。\n- **Build ID + 偏移量模式**：将 IP 地址转换为对应二进制文件的 Build ID 和相对于该文件的偏移量，便于符号化解析且不受 ASLR 影响。\n\n该文件为 BPF 程序提供 `bpf_get_stackid()` 辅助函数的后端实现，是性能分析、追踪和调试工具（如 perf、bpftrace）的关键组件。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct stack_map_bucket`**  \n  哈希桶中的条目，包含：\n  - `fnode`：用于 per-CPU 自由链表管理\n  - `hash`：调用栈内容的哈希值\n  - `nr`：栈帧数量\n  - `data[]`：变长数组，存储栈数据（`u64` IP 或 `struct bpf_stack_build_id`）\n\n- **`struct bpf_stack_map`**  \n  BPF 栈映射的私有结构，继承自 `struct bpf_map`，包含：\n  - `elems`：预分配的桶内存池\n  - `freelist`：per-CPU 自由链表，用于高效分配/回收桶\n  - `n_buckets`：哈希表桶数量（2 的幂）\n  - `buckets[]`：哈希桶指针数组\n\n- **`struct bpf_stack_build_id`**（外部定义）  \n  Build ID 模式下的栈帧表示，包含：\n  - `status`：状态（`BPF_STACK_BUILD_ID_IP` 或 `BPF_STACK_BUILD_ID_VALID`）\n  - `ip`：原始 IP（仅当 status 为 IP 时有效）\n  - `offset`：相对于映射起始地址的偏移\n  - `build_id[]`：Build ID 字节数组\n\n### 主要函数\n\n- **`stack_map_alloc()`**  \n  BPF 栈映射的分配器，验证属性、预分配内存、初始化自由链表和调用链缓冲区。\n\n- **`__bpf_get_stackid()`**  \n  核心逻辑：根据传入的调用栈生成唯一 ID，执行哈希查找、比较和插入。\n\n- **`stack_map_get_build_id_offset()`**  \n  将原始 IP 地址转换为 Build ID + 偏移量格式，需持有 mmap 读锁。\n\n- **`get_callchain_entry_for_task()`**  \n  从指定任务结构中提取内核态调用栈（仅在 `CONFIG_STACKTRACE` 启用时有效）。\n\n- **`bpf_get_stackid()`**（BPF_CALL_3 宏定义）  \n  BPF 程序调用的入口点，根据寄存器上下文获取当前调用栈并返回其 ID。\n\n## 3. 关键实现\n\n### 哈希表设计\n- 使用 **开放寻址 + 覆盖替换** 策略：每个桶 ID 对应唯一哈希桶，冲突时直接替换旧条目。\n- 哈希函数为 `jhash2()`，对栈 IP 数组进行哈希。\n- 桶数量为 `max_entries` 的 2 次幂，通过位掩码 `hash & (n_buckets - 1)` 快速定位。\n\n### 内存管理\n- **预分配内存池**：在映射创建时一次性分配所有桶内存（`smap->elems`）。\n- **Per-CPU 自由链表**：使用 `pcpu_freelist` 实现无锁、高效的桶分配/回收，避免运行时内存分配开销。\n\n### Build ID 转换\n- 通过 `find_vma()` 查找 IP 所属的 VMA（虚拟内存区域）。\n- 调用 `build_id_parse()` 从 VMA 的 ELF 头中提取 Build ID。\n- 计算偏移量：`offset = (vma->vm_pgoff << PAGE_SHIFT) + ip - vma->vm_start`。\n- **锁机制**：使用 `mmap_read_trylock()` 获取 mmap 读锁，失败时回退到原始 IP 模式。\n- **中断上下文安全**：通过 `mmap_unlock_irq_work` 机制确保在中断上下文中安全释放 mmap 锁。\n\n### 快速比较优化\n- 若设置 `BPF_F_FAST_STACK_CMP` 标志，仅比较哈希值，不进行内容 memcmp，适用于对哈希冲突不敏感的场景。\n\n### 栈深度限制\n- 最大栈深度由 `sysctl_perf_event_max_stack` 控制，映射的 `value_size` 必须是单个栈帧大小的整数倍且不超过该限制。\n\n## 4. 依赖关系\n\n- **BPF 核心**：`<linux/bpf.h>`、`bpf_map` 基础设施\n- **内存管理**：`<linux/percpu.h>`（per-CPU 自由链表）、`bpf_map_area_alloc/free`\n- **栈追踪**：`<linux/stacktrace.h>`、`<linux/perf_event.h>`（调用链缓冲区）\n- **Build ID 支持**：`<linux/buildid.h>`、VMA 操作（`find_vma`、`range_in_vma`）\n- **内存映射锁**：`mmap_unlock_work.h`（安全释放 mmap 锁）\n- **哈希函数**：`<linux/jhash.h>`\n- **配置选项**：`CONFIG_STACKTRACE`（内核栈追踪支持）\n\n## 5. 使用场景\n\n- **性能分析**：BPF 程序通过 `bpf_get_stackid()` 获取当前调用栈 ID，后续通过 `bpf_map_lookup_elem()` 读取完整栈内容，用于火焰图生成。\n- **系统追踪**：结合 kprobe/uprobe，记录特定函数调用时的完整调用上下文。\n- **安全监控**：检测异常调用路径（如敏感系统调用的调用者）。\n- **调试工具**：为 `perf`、`bpftrace`、`bcc` 等工具提供高效的栈存储后端。\n- **Build ID 模式**：在 ASLR（地址空间布局随机化）环境下，通过 Build ID + 偏移实现稳定的符号化解析，适用于长期运行的监控场景。",
      "similarity": 0.4804900884628296,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/stackmap.c",
          "start_line": 1,
          "end_line": 33,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2016 Facebook",
            " */",
            "#include <linux/bpf.h>",
            "#include <linux/jhash.h>",
            "#include <linux/filter.h>",
            "#include <linux/kernel.h>",
            "#include <linux/stacktrace.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/btf_ids.h>",
            "#include <linux/buildid.h>",
            "#include \"percpu_freelist.h\"",
            "#include \"mmap_unlock_work.h\"",
            "",
            "#define STACK_CREATE_FLAG_MASK\t\t\t\t\t\\",
            "\t(BPF_F_NUMA_NODE | BPF_F_RDONLY | BPF_F_WRONLY |\t\\",
            "\t BPF_F_STACK_BUILD_ID)",
            "",
            "struct stack_map_bucket {",
            "\tstruct pcpu_freelist_node fnode;",
            "\tu32 hash;",
            "\tu32 nr;",
            "\tu64 data[];",
            "};",
            "",
            "struct bpf_stack_map {",
            "\tstruct bpf_map map;",
            "\tvoid *elems;",
            "\tstruct pcpu_freelist freelist;",
            "\tu32 n_buckets;",
            "\tstruct stack_map_bucket *buckets[];",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义堆栈映射相关结构体，包含用于存储堆栈跟踪数据的stack_map_bucket和堆栈映射主结构bpf_stack_map，包含PCPU自由列表和桶数组",
          "similarity": 0.4394581913948059
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/stackmap.c",
          "start_line": 34,
          "end_line": 182,
          "content": [
            "static inline bool stack_map_use_build_id(struct bpf_map *map)",
            "{",
            "\treturn (map->map_flags & BPF_F_STACK_BUILD_ID);",
            "}",
            "static inline int stack_map_data_size(struct bpf_map *map)",
            "{",
            "\treturn stack_map_use_build_id(map) ?",
            "\t\tsizeof(struct bpf_stack_build_id) : sizeof(u64);",
            "}",
            "static int prealloc_elems_and_freelist(struct bpf_stack_map *smap)",
            "{",
            "\tu64 elem_size = sizeof(struct stack_map_bucket) +",
            "\t\t\t(u64)smap->map.value_size;",
            "\tint err;",
            "",
            "\tsmap->elems = bpf_map_area_alloc(elem_size * smap->map.max_entries,",
            "\t\t\t\t\t smap->map.numa_node);",
            "\tif (!smap->elems)",
            "\t\treturn -ENOMEM;",
            "",
            "\terr = pcpu_freelist_init(&smap->freelist);",
            "\tif (err)",
            "\t\tgoto free_elems;",
            "",
            "\tpcpu_freelist_populate(&smap->freelist, smap->elems, elem_size,",
            "\t\t\t       smap->map.max_entries);",
            "\treturn 0;",
            "",
            "free_elems:",
            "\tbpf_map_area_free(smap->elems);",
            "\treturn err;",
            "}",
            "static void stack_map_get_build_id_offset(struct bpf_stack_build_id *id_offs,",
            "\t\t\t\t\t  u64 *ips, u32 trace_nr, bool user)",
            "{",
            "\tint i;",
            "\tstruct mmap_unlock_irq_work *work = NULL;",
            "\tbool irq_work_busy = bpf_mmap_unlock_get_irq_work(&work);",
            "\tstruct vm_area_struct *vma, *prev_vma = NULL;",
            "\tconst char *prev_build_id;",
            "",
            "\t/* If the irq_work is in use, fall back to report ips. Same",
            "\t * fallback is used for kernel stack (!user) on a stackmap with",
            "\t * build_id.",
            "\t */",
            "\tif (!user || !current || !current->mm || irq_work_busy ||",
            "\t    !mmap_read_trylock(current->mm)) {",
            "\t\t/* cannot access current->mm, fall back to ips */",
            "\t\tfor (i = 0; i < trace_nr; i++) {",
            "\t\t\tid_offs[i].status = BPF_STACK_BUILD_ID_IP;",
            "\t\t\tid_offs[i].ip = ips[i];",
            "\t\t\tmemset(id_offs[i].build_id, 0, BUILD_ID_SIZE_MAX);",
            "\t\t}",
            "\t\treturn;",
            "\t}",
            "",
            "\tfor (i = 0; i < trace_nr; i++) {",
            "\t\tif (range_in_vma(prev_vma, ips[i], ips[i])) {",
            "\t\t\tvma = prev_vma;",
            "\t\t\tmemcpy(id_offs[i].build_id, prev_build_id,",
            "\t\t\t       BUILD_ID_SIZE_MAX);",
            "\t\t\tgoto build_id_valid;",
            "\t\t}",
            "\t\tvma = find_vma(current->mm, ips[i]);",
            "\t\tif (!vma || build_id_parse(vma, id_offs[i].build_id, NULL)) {",
            "\t\t\t/* per entry fall back to ips */",
            "\t\t\tid_offs[i].status = BPF_STACK_BUILD_ID_IP;",
            "\t\t\tid_offs[i].ip = ips[i];",
            "\t\t\tmemset(id_offs[i].build_id, 0, BUILD_ID_SIZE_MAX);",
            "\t\t\tcontinue;",
            "\t\t}",
            "build_id_valid:",
            "\t\tid_offs[i].offset = (vma->vm_pgoff << PAGE_SHIFT) + ips[i]",
            "\t\t\t- vma->vm_start;",
            "\t\tid_offs[i].status = BPF_STACK_BUILD_ID_VALID;",
            "\t\tprev_vma = vma;",
            "\t\tprev_build_id = id_offs[i].build_id;",
            "\t}",
            "\tbpf_mmap_unlock_mm(work, current->mm);",
            "}",
            "static long __bpf_get_stackid(struct bpf_map *map,",
            "\t\t\t      struct perf_callchain_entry *trace, u64 flags)",
            "{",
            "\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);",
            "\tstruct stack_map_bucket *bucket, *new_bucket, *old_bucket;",
            "\tu32 skip = flags & BPF_F_SKIP_FIELD_MASK;",
            "\tu32 hash, id, trace_nr, trace_len;",
            "\tbool user = flags & BPF_F_USER_STACK;",
            "\tu64 *ips;",
            "\tbool hash_matches;",
            "",
            "\tif (trace->nr <= skip)",
            "\t\t/* skipping more than usable stack trace */",
            "\t\treturn -EFAULT;",
            "",
            "\ttrace_nr = trace->nr - skip;",
            "\ttrace_len = trace_nr * sizeof(u64);",
            "\tips = trace->ip + skip;",
            "\thash = jhash2((u32 *)ips, trace_len / sizeof(u32), 0);",
            "\tid = hash & (smap->n_buckets - 1);",
            "\tbucket = READ_ONCE(smap->buckets[id]);",
            "",
            "\thash_matches = bucket && bucket->hash == hash;",
            "\t/* fast cmp */",
            "\tif (hash_matches && flags & BPF_F_FAST_STACK_CMP)",
            "\t\treturn id;",
            "",
            "\tif (stack_map_use_build_id(map)) {",
            "\t\t/* for build_id+offset, pop a bucket before slow cmp */",
            "\t\tnew_bucket = (struct stack_map_bucket *)",
            "\t\t\tpcpu_freelist_pop(&smap->freelist);",
            "\t\tif (unlikely(!new_bucket))",
            "\t\t\treturn -ENOMEM;",
            "\t\tnew_bucket->nr = trace_nr;",
            "\t\tstack_map_get_build_id_offset(",
            "\t\t\t(struct bpf_stack_build_id *)new_bucket->data,",
            "\t\t\tips, trace_nr, user);",
            "\t\ttrace_len = trace_nr * sizeof(struct bpf_stack_build_id);",
            "\t\tif (hash_matches && bucket->nr == trace_nr &&",
            "\t\t    memcmp(bucket->data, new_bucket->data, trace_len) == 0) {",
            "\t\t\tpcpu_freelist_push(&smap->freelist, &new_bucket->fnode);",
            "\t\t\treturn id;",
            "\t\t}",
            "\t\tif (bucket && !(flags & BPF_F_REUSE_STACKID)) {",
            "\t\t\tpcpu_freelist_push(&smap->freelist, &new_bucket->fnode);",
            "\t\t\treturn -EEXIST;",
            "\t\t}",
            "\t} else {",
            "\t\tif (hash_matches && bucket->nr == trace_nr &&",
            "\t\t    memcmp(bucket->data, ips, trace_len) == 0)",
            "\t\t\treturn id;",
            "\t\tif (bucket && !(flags & BPF_F_REUSE_STACKID))",
            "\t\t\treturn -EEXIST;",
            "",
            "\t\tnew_bucket = (struct stack_map_bucket *)",
            "\t\t\tpcpu_freelist_pop(&smap->freelist);",
            "\t\tif (unlikely(!new_bucket))",
            "\t\t\treturn -ENOMEM;",
            "\t\tmemcpy(new_bucket->data, ips, trace_len);",
            "\t}",
            "",
            "\tnew_bucket->hash = hash;",
            "\tnew_bucket->nr = trace_nr;",
            "",
            "\told_bucket = xchg(&smap->buckets[id], new_bucket);",
            "\tif (old_bucket)",
            "\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);",
            "\treturn id;",
            "}"
          ],
          "function_name": "stack_map_use_build_id, stack_map_data_size, prealloc_elems_and_freelist, stack_map_get_build_id_offset, __bpf_get_stackid",
          "description": "实现堆栈ID获取逻辑，包含构建ID解析、桶数据比较、哈希表查找等功能，支持通过构建ID或直接IP地址方式存储堆栈跟踪信息",
          "similarity": 0.4067263603210449
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/stackmap.c",
          "start_line": 601,
          "end_line": 671,
          "content": [
            "static int stack_map_get_next_key(struct bpf_map *map, void *key,",
            "\t\t\t\t  void *next_key)",
            "{",
            "\tstruct bpf_stack_map *smap = container_of(map,",
            "\t\t\t\t\t\t  struct bpf_stack_map, map);",
            "\tu32 id;",
            "",
            "\tWARN_ON_ONCE(!rcu_read_lock_held());",
            "",
            "\tif (!key) {",
            "\t\tid = 0;",
            "\t} else {",
            "\t\tid = *(u32 *)key;",
            "\t\tif (id >= smap->n_buckets || !smap->buckets[id])",
            "\t\t\tid = 0;",
            "\t\telse",
            "\t\t\tid++;",
            "\t}",
            "",
            "\twhile (id < smap->n_buckets && !smap->buckets[id])",
            "\t\tid++;",
            "",
            "\tif (id >= smap->n_buckets)",
            "\t\treturn -ENOENT;",
            "",
            "\t*(u32 *)next_key = id;",
            "\treturn 0;",
            "}",
            "static long stack_map_update_elem(struct bpf_map *map, void *key, void *value,",
            "\t\t\t\t  u64 map_flags)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "static long stack_map_delete_elem(struct bpf_map *map, void *key)",
            "{",
            "\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);",
            "\tstruct stack_map_bucket *old_bucket;",
            "\tu32 id = *(u32 *)key;",
            "",
            "\tif (unlikely(id >= smap->n_buckets))",
            "\t\treturn -E2BIG;",
            "",
            "\told_bucket = xchg(&smap->buckets[id], NULL);",
            "\tif (old_bucket) {",
            "\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);",
            "\t\treturn 0;",
            "\t} else {",
            "\t\treturn -ENOENT;",
            "\t}",
            "}",
            "static void stack_map_free(struct bpf_map *map)",
            "{",
            "\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);",
            "",
            "\tbpf_map_area_free(smap->elems);",
            "\tpcpu_freelist_destroy(&smap->freelist);",
            "\tbpf_map_area_free(smap);",
            "\tput_callchain_buffers();",
            "}",
            "static u64 stack_map_mem_usage(const struct bpf_map *map)",
            "{",
            "\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);",
            "\tu64 value_size = map->value_size;",
            "\tu64 n_buckets = smap->n_buckets;",
            "\tu64 enties = map->max_entries;",
            "\tu64 usage = sizeof(*smap);",
            "",
            "\tusage += n_buckets * sizeof(struct stack_map_bucket *);",
            "\tusage += enties * (sizeof(struct stack_map_bucket) + value_size);",
            "\treturn usage;",
            "}"
          ],
          "function_name": "stack_map_get_next_key, stack_map_update_elem, stack_map_delete_elem, stack_map_free, stack_map_mem_usage",
          "description": "实现堆栈映射的键值遍历、元素更新/删除、内存释放和内存使用统计功能，维护堆栈桶的生命周期管理",
          "similarity": 0.37635350227355957
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/stackmap.c",
          "start_line": 319,
          "end_line": 426,
          "content": [
            "static __u64 count_kernel_ip(struct perf_callchain_entry *trace)",
            "{",
            "\t__u64 nr_kernel = 0;",
            "",
            "\twhile (nr_kernel < trace->nr) {",
            "\t\tif (trace->ip[nr_kernel] == PERF_CONTEXT_USER)",
            "\t\t\tbreak;",
            "\t\tnr_kernel++;",
            "\t}",
            "\treturn nr_kernel;",
            "}",
            "static long __bpf_get_stack(struct pt_regs *regs, struct task_struct *task,",
            "\t\t\t    struct perf_callchain_entry *trace_in,",
            "\t\t\t    void *buf, u32 size, u64 flags)",
            "{",
            "\tu32 trace_nr, copy_len, elem_size, num_elem, max_depth;",
            "\tbool user_build_id = flags & BPF_F_USER_BUILD_ID;",
            "\tbool crosstask = task && task != current;",
            "\tu32 skip = flags & BPF_F_SKIP_FIELD_MASK;",
            "\tbool user = flags & BPF_F_USER_STACK;",
            "\tstruct perf_callchain_entry *trace;",
            "\tbool kernel = !user;",
            "\tint err = -EINVAL;",
            "\tu64 *ips;",
            "",
            "\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |",
            "\t\t\t       BPF_F_USER_BUILD_ID)))",
            "\t\tgoto clear;",
            "\tif (kernel && user_build_id)",
            "\t\tgoto clear;",
            "",
            "\telem_size = (user && user_build_id) ? sizeof(struct bpf_stack_build_id)",
            "\t\t\t\t\t    : sizeof(u64);",
            "\tif (unlikely(size % elem_size))",
            "\t\tgoto clear;",
            "",
            "\t/* cannot get valid user stack for task without user_mode regs */",
            "\tif (task && user && !user_mode(regs))",
            "\t\tgoto err_fault;",
            "",
            "\t/* get_perf_callchain does not support crosstask user stack walking",
            "\t * but returns an empty stack instead of NULL.",
            "\t */",
            "\tif (crosstask && user) {",
            "\t\terr = -EOPNOTSUPP;",
            "\t\tgoto clear;",
            "\t}",
            "",
            "\tnum_elem = size / elem_size;",
            "\tmax_depth = num_elem + skip;",
            "\tif (sysctl_perf_event_max_stack < max_depth)",
            "\t\tmax_depth = sysctl_perf_event_max_stack;",
            "",
            "\tif (trace_in)",
            "\t\ttrace = trace_in;",
            "\telse if (kernel && task)",
            "\t\ttrace = get_callchain_entry_for_task(task, max_depth);",
            "\telse",
            "\t\ttrace = get_perf_callchain(regs, 0, kernel, user, max_depth,",
            "\t\t\t\t\t   crosstask, false);",
            "\tif (unlikely(!trace))",
            "\t\tgoto err_fault;",
            "",
            "\tif (trace->nr < skip)",
            "\t\tgoto err_fault;",
            "",
            "\ttrace_nr = trace->nr - skip;",
            "\ttrace_nr = (trace_nr <= num_elem) ? trace_nr : num_elem;",
            "\tcopy_len = trace_nr * elem_size;",
            "",
            "\tips = trace->ip + skip;",
            "\tif (user && user_build_id)",
            "\t\tstack_map_get_build_id_offset(buf, ips, trace_nr, user);",
            "\telse",
            "\t\tmemcpy(buf, ips, copy_len);",
            "",
            "\tif (size > copy_len)",
            "\t\tmemset(buf + copy_len, 0, size - copy_len);",
            "\treturn copy_len;",
            "",
            "err_fault:",
            "\terr = -EFAULT;",
            "clear:",
            "\tmemset(buf, 0, size);",
            "\treturn err;",
            "}",
            "int bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)",
            "{",
            "\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);",
            "\tstruct stack_map_bucket *bucket, *old_bucket;",
            "\tu32 id = *(u32 *)key, trace_len;",
            "",
            "\tif (unlikely(id >= smap->n_buckets))",
            "\t\treturn -ENOENT;",
            "",
            "\tbucket = xchg(&smap->buckets[id], NULL);",
            "\tif (!bucket)",
            "\t\treturn -ENOENT;",
            "",
            "\ttrace_len = bucket->nr * stack_map_data_size(map);",
            "\tmemcpy(value, bucket->data, trace_len);",
            "\tmemset(value + trace_len, 0, map->value_size - trace_len);",
            "",
            "\told_bucket = xchg(&smap->buckets[id], bucket);",
            "\tif (old_bucket)",
            "\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "count_kernel_ip, __bpf_get_stack, bpf_stackmap_copy",
          "description": "提供堆栈数据复制接口，包含内核IP计数、用户/内核模式堆栈采集、构建ID偏移量解析，以及堆栈映射数据复制功能",
          "similarity": 0.3637939691543579
        }
      ]
    }
  ]
}