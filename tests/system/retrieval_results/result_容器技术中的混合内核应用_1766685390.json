{
  "query": "容器技术中的混合内核应用",
  "timestamp": "2025-12-26 01:56:30",
  "retrieved_files": [
    {
      "source_file": "mm/balloon_compaction.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:41:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `balloon_compaction.c`\n\n---\n\n# balloon_compaction.c 技术文档\n\n## 1. 文件概述\n\n`balloon_compaction.c` 是 Linux 内核中用于支持内存气球（Memory Ballooning）机制与内存压缩（Compaction）协同工作的核心模块。该文件提供了通用接口，使得由气球驱动程序管理的页面可以被内存压缩子系统识别为可迁移（movable），从而在内存碎片整理过程中安全地移动这些页面，提升高阶内存分配的成功率。此机制主要用于虚拟化环境中，允许宿主机动态调整客户机（Guest）的可用内存。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`balloon_page_alloc()`**  \n  分配一个新的页面，专用于加入气球页面列表。使用特殊的 GFP 标志（如 `__GFP_NOMEMALLOC`, `__GFP_NORETRY`, `__GFP_NOWARN`）以避免在内存压力下触发 OOM 或重试。\n\n- **`balloon_page_enqueue()`**  \n  将单个通过 `balloon_page_alloc()` 分配的页面插入到指定气球设备的页面列表中，并增加 `BALLOON_INFLATE` 统计计数。\n\n- **`balloon_page_list_enqueue()`**  \n  批量将一个页面链表中的所有页面插入到气球设备的页面列表中，适用于高效批量操作。\n\n- **`balloon_page_dequeue()`**  \n  从气球设备的页面列表中移除并返回一个页面，供驱动释放回系统。若无法出队且无孤立页面，则触发 `BUG()` 防止死循环。\n\n- **`balloon_page_list_dequeue()`**  \n  批量从气球设备中取出最多 `n_req_pages` 个页面，放入调用者提供的链表中，用于批量释放。\n\n- **`balloon_page_isolate()`**（仅当 `CONFIG_BALLOON_COMPACTION` 启用）  \n  在内存压缩过程中，将气球页面从主列表中隔离，防止并发访问，并增加 `isolated_pages` 计数。\n\n- **`balloon_page_putback()`**（仅当 `CONFIG_BALLOON_COMPACTION` 启用）  \n  将被隔离的气球页面重新放回主页面列表，并减少 `isolated_pages` 计数。\n\n- **`balloon_page_migrate()`**（仅当 `CONFIG_BALLOON_COMPACTION` 启用）  \n  实现气球页面的迁移逻辑，作为内存压缩中 `move_to_new_page()` 的对应处理函数（代码片段未完整）。\n\n### 关键数据结构\n\n- **`struct balloon_dev_info`**  \n  气球设备信息结构体，包含：\n  - `pages`：已入队的气球页面链表\n  - `pages_lock`：保护页面列表的自旋锁\n  - `isolated_pages`：当前被压缩子系统隔离的页面数量（仅在 `CONFIG_BALLOON_COMPACTION` 下使用）\n\n## 3. 关键实现\n\n- **线程安全与并发控制**  \n  所有对 `balloon_dev_info->pages` 链表的操作均受 `pages_lock` 自旋锁保护，并在中断禁用上下文中执行（`spin_lock_irqsave`），确保在高并发或中断上下文中的安全性。\n\n- **页面锁定机制**  \n  在入队和出队时使用 `trylock_page()` 确保当前是唯一持有页面引用的实体。若加锁失败，说明存在并发访问，可能意味着内存损坏或状态不一致，此时会跳过或报错。\n\n- **与内存压缩集成**  \n  当启用 `CONFIG_BALLOON_COMPACTION` 时，气球页面可通过 `PageIsolated()` 标志被识别为正在被压缩子系统处理。出队操作会跳过这些页面，避免破坏压缩流程。\n\n- **统计计数**  \n  使用 `__count_vm_event(BALLOON_INFLATE)` 和 `__count_vm_event(BALLOON_DEFLATE)` 跟踪气球膨胀/收缩操作次数，便于性能监控和调试。\n\n- **错误检测与防御性编程**  \n  在 `balloon_page_dequeue()` 中，若页面列表为空且无孤立页面，说明页面丢失，触发 `BUG()` 以防止驱动陷入无限循环。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/mm.h>`：内存管理基础接口\n  - `<linux/slab.h>`：内存分配\n  - `<linux/balloon_compaction.h>`：气球压缩相关声明（如 `balloon_page_insert`, `balloon_page_delete`, `balloon_page_device` 等）\n\n- **内核配置依赖**：\n  - `CONFIG_MEMORY_BALLOONING`：启用内存气球机制\n  - `CONFIG_BALLOON_COMPACTION`：启用气球页面的可压缩支持（条件编译）\n\n- **与其他子系统交互**：\n  - **内存压缩子系统（mm/compaction.c）**：通过注册的 `isolate` / `migrate` 回调函数参与页面迁移\n  - **虚拟化驱动（如 virtio_balloon）**：作为使用者调用本模块提供的 enqueue/dequeue 接口管理气球内存\n\n## 5. 使用场景\n\n- **虚拟化环境中的内存动态调整**  \n  客户机操作系统通过气球驱动（如 `virtio_balloon`）向宿主机“归还”内存时，调用 `balloon_page_alloc()` + `balloon_page_enqueue()` 将页面加入气球列表；当宿主机释放内存给客户机时，驱动调用 `balloon_page_dequeue()` 获取页面并释放回 buddy allocator。\n\n- **高阶内存分配优化**  \n  当系统需要大块连续物理内存（如透明大页 THP）但存在碎片时，内存压缩子系统会尝试迁移可移动页面。气球页面因本模块支持而被视为可移动，从而被安全迁移，帮助形成连续内存区域。\n\n- **内存热插拔与 NUMA 迁移**  \n  在 NUMA 节点间迁移内存或热移除内存区域时，气球页面可被压缩机制迁移，提高操作成功率。\n\n- **OOM 避免与内存回收**  \n  气球机制本身是一种主动内存回收手段，配合压缩可进一步提升内存利用率，减少 OOM 发生概率。",
      "similarity": 0.5405905246734619,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/balloon_compaction.c",
          "start_line": 14,
          "end_line": 129,
          "content": [
            "static void balloon_page_enqueue_one(struct balloon_dev_info *b_dev_info,",
            "\t\t\t\t     struct page *page)",
            "{",
            "\t/*",
            "\t * Block others from accessing the 'page' when we get around to",
            "\t * establishing additional references. We should be the only one",
            "\t * holding a reference to the 'page' at this point. If we are not, then",
            "\t * memory corruption is possible and we should stop execution.",
            "\t */",
            "\tBUG_ON(!trylock_page(page));",
            "\tballoon_page_insert(b_dev_info, page);",
            "\tunlock_page(page);",
            "\t__count_vm_event(BALLOON_INFLATE);",
            "}",
            "size_t balloon_page_list_enqueue(struct balloon_dev_info *b_dev_info,",
            "\t\t\t\t struct list_head *pages)",
            "{",
            "\tstruct page *page, *tmp;",
            "\tunsigned long flags;",
            "\tsize_t n_pages = 0;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tlist_for_each_entry_safe(page, tmp, pages, lru) {",
            "\t\tlist_del(&page->lru);",
            "\t\tballoon_page_enqueue_one(b_dev_info, page);",
            "\t\tn_pages++;",
            "\t}",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "\treturn n_pages;",
            "}",
            "size_t balloon_page_list_dequeue(struct balloon_dev_info *b_dev_info,",
            "\t\t\t\t struct list_head *pages, size_t n_req_pages)",
            "{",
            "\tstruct page *page, *tmp;",
            "\tunsigned long flags;",
            "\tsize_t n_pages = 0;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tlist_for_each_entry_safe(page, tmp, &b_dev_info->pages, lru) {",
            "\t\tif (n_pages == n_req_pages)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * Block others from accessing the 'page' while we get around to",
            "\t\t * establishing additional references and preparing the 'page'",
            "\t\t * to be released by the balloon driver.",
            "\t\t */",
            "\t\tif (!trylock_page(page))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (IS_ENABLED(CONFIG_BALLOON_COMPACTION) &&",
            "\t\t    PageIsolated(page)) {",
            "\t\t\t/* raced with isolation */",
            "\t\t\tunlock_page(page);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tballoon_page_delete(page);",
            "\t\t__count_vm_event(BALLOON_DEFLATE);",
            "\t\tlist_add(&page->lru, pages);",
            "\t\tunlock_page(page);",
            "\t\tn_pages++;",
            "\t}",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "",
            "\treturn n_pages;",
            "}",
            "void balloon_page_enqueue(struct balloon_dev_info *b_dev_info,",
            "\t\t\t  struct page *page)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tballoon_page_enqueue_one(b_dev_info, page);",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "}",
            "static bool balloon_page_isolate(struct page *page, isolate_mode_t mode)",
            "",
            "{",
            "\tstruct balloon_dev_info *b_dev_info = balloon_page_device(page);",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tlist_del(&page->lru);",
            "\tb_dev_info->isolated_pages++;",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "",
            "\treturn true;",
            "}",
            "static void balloon_page_putback(struct page *page)",
            "{",
            "\tstruct balloon_dev_info *b_dev_info = balloon_page_device(page);",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tlist_add(&page->lru, &b_dev_info->pages);",
            "\tb_dev_info->isolated_pages--;",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "}",
            "static int balloon_page_migrate(struct page *newpage, struct page *page,",
            "\t\tenum migrate_mode mode)",
            "{",
            "\tstruct balloon_dev_info *balloon = balloon_page_device(page);",
            "",
            "\t/*",
            "\t * We can not easily support the no copy case here so ignore it as it",
            "\t * is unlikely to be used with balloon pages. See include/linux/hmm.h",
            "\t * for a user of the MIGRATE_SYNC_NO_COPY mode.",
            "\t */",
            "\tif (mode == MIGRATE_SYNC_NO_COPY)",
            "\t\treturn -EINVAL;",
            "",
            "\tVM_BUG_ON_PAGE(!PageLocked(page), page);",
            "\tVM_BUG_ON_PAGE(!PageLocked(newpage), newpage);",
            "",
            "\treturn balloon->migratepage(balloon, newpage, page, mode);",
            "}"
          ],
          "function_name": "balloon_page_enqueue_one, balloon_page_list_enqueue, balloon_page_list_dequeue, balloon_page_enqueue, balloon_page_isolate, balloon_page_putback, balloon_page_migrate",
          "description": "提供气球内存页的并发控制及迁移管理，通过自旋锁保护页表操作，实现页面在LRU链表间的移动、隔离和迁移，支持气球膨胀/收缩事件统计",
          "similarity": 0.49444881081581116
        },
        {
          "chunk_id": 0,
          "file_path": "mm/balloon_compaction.c",
          "start_line": 1,
          "end_line": 13,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * mm/balloon_compaction.c",
            " *",
            " * Common interface for making balloon pages movable by compaction.",
            " *",
            " * Copyright (C) 2012, Red Hat, Inc.  Rafael Aquini <aquini@redhat.com>",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/slab.h>",
            "#include <linux/export.h>",
            "#include <linux/balloon_compaction.h>",
            ""
          ],
          "function_name": null,
          "description": "上下文不完整",
          "similarity": 0.3722584843635559
        }
      ]
    },
    {
      "source_file": "mm/ksm.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:34:25\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `ksm.c`\n\n---\n\n# ksm.c 技术文档\n\n## 1. 文件概述\n\n`ksm.c` 实现了内核同页合并（Kernel Samepage Merging, KSM）功能，该机制能够动态识别并合并内容完全相同的物理内存页，即使这些页属于不同的进程地址空间且未通过 `fork()` 共享。KSM 通过后台扫描线程周期性地遍历注册的内存区域，利用红黑树（rbtree）结构高效地比对页面内容，将重复页替换为只读的共享页，从而显著减少物理内存占用。此功能特别适用于虚拟化环境中多个相似虚拟机共存的场景。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct ksm_mm_slot`**  \n  表示一个被 KSM 扫描的内存描述符（`mm_struct`）的元数据，包含哈希槽位和反向映射项链表头。\n\n- **`struct ksm_scan`**  \n  全局扫描游标，记录当前扫描进度（包括当前 `mm_slot`、虚拟地址、反向映射项指针及完整扫描轮次计数）。\n\n- **`struct ksm_stable_node`**  \n  稳定红黑树中的节点，代表一个已合并的 KSM 页面。包含：\n  - 红黑树节点或迁移链表指针（联合体复用）\n  - 指向使用该 KSM 页的所有 `rmap_item` 的哈希链表头\n  - 物理页帧号（`kpfn`）或链修剪时间戳\n  - 反向映射项数量（支持链式扩展）\n  - NUMA 节点 ID（若启用 NUMA）\n\n- **`struct ksm_rmap_item`**  \n  反向映射项，跟踪一个虚拟地址到其物理页的映射关系。包含：\n  - 所属 `mm_struct` 和虚拟地址（低比特位用于标志）\n  - 匿名 VMA 指针（稳定状态）或 NUMA 节点 ID（不稳定状态）\n  - 校验和（不稳定状态）\n  - 红黑树节点（不稳定树）或指向 `stable_node` 的指针及哈希链表节点（稳定状态）\n\n### 关键宏定义\n\n- **`STABLE_NODE_CHAIN`**  \n  标识稳定节点为“链”类型（值为 -1024），用于高效管理大量相同内容的 KSM 页副本。\n  \n- **标志位**  \n  - `UNSTABLE_FLAG` (0x100)：标识 `rmap_item` 属于不稳定树\n  - `STABLE_FLAG` (0x200)：标识 `rmap_item` 已链接到稳定树\n  - `SEQNR_MASK` (0x0ff)：用于存储不稳定树序列号的低 8 位\n\n## 3. 关键实现\n\n### 双树架构设计\nKSM 采用**稳定树（stable tree）**与**不稳定树（unstable tree）**协同工作的机制：\n- **稳定树**：存储已确认可合并的只读 KSM 页，因写保护而内容恒定，支持高效精确匹配。\n- **不稳定树**：临时缓存近期未修改的普通页，因内容可能变化而需周期性重建。\n\n### 扫描与合并流程\n1. **增量扫描**：全局游标 `ksm_scan` 遍历所有注册的 `mm_slot` 及其内存区域。\n2. **校验和预筛**：计算页面内容的 `xxhash` 校验和，仅当与上次扫描一致时才尝试插入不稳定树。\n3. **双阶段匹配**：\n   - 优先在**稳定树**中查找完全匹配的 KSM 页\n   - 若未命中，则在**不稳定树**中查找潜在重复页\n4. **树维护策略**：\n   - 不稳定树在每轮全量扫描结束后**完全清空重建**\n   - 稳定树**持久保留**，通过反向映射（rmap）和链式节点优化大规模合并场景\n5. **NUMA 感知**：若 `merge_across_nodes=0`，则为每个 NUMA 节点维护独立的稳定/不稳定树，避免跨节点内存访问开销。\n\n### 内存安全机制\n- **写保护**：合并后的 KSM 页设为只读，任何写操作触发 COW（写时复制）并解除合并。\n- **RMAP 集成**：通过 `anon_vma` 和反向映射链表，在页解绑时高效更新所有相关虚拟地址。\n- **OOM 防护**：在内存压力下可释放 KSM 页以缓解系统压力。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：深度依赖 `mm.h`、`rmap.h`、`pagemap.h` 实现页表操作、反向映射和页生命周期管理。\n- **调度与进程管理**：通过 `sched/mm.h` 获取进程内存上下文，利用 `kthread.h` 创建后台扫描线程。\n- **NUMA 支持**：条件编译依赖 `CONFIG_NUMA`，使用 `numa.h` 实现节点亲和性。\n- **调试与追踪**：集成 `trace/events/ksm.h` 提供运行时事件追踪能力。\n- **哈希算法**：使用 `xxhash.h` 提供高效的内容指纹计算。\n- **内部辅助模块**：依赖 `mm_slot.h` 管理内存描述符槽位，`internal.h` 提供内核 MM 内部接口。\n\n## 5. 使用场景\n\n- **虚拟化环境**：在 KVM/Xen 等 Hypervisor 中合并多个相似虚拟机的内存页（如相同操作系统镜像）。\n- **内存密集型应用**：合并大型应用（如数据库、Web 服务器）中重复的静态数据或零页。\n- **容器化平台**：在 Docker/LXC 等容器运行时中减少同镜像容器的内存占用。\n- **内存超分场景**：在物理内存有限但允许超额分配的系统中提升内存利用率。\n- **开发调试**：通过 `/sys/kernel/mm/ksm/` 接口动态控制扫描速率、合并阈值等参数。",
      "similarity": 0.5203583240509033,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/ksm.c",
          "start_line": 1,
          "end_line": 311,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Memory merging support.",
            " *",
            " * This code enables dynamic sharing of identical pages found in different",
            " * memory areas, even if they are not shared by fork()",
            " *",
            " * Copyright (C) 2008-2009 Red Hat, Inc.",
            " * Authors:",
            " *\tIzik Eidus",
            " *\tAndrea Arcangeli",
            " *\tChris Wright",
            " *\tHugh Dickins",
            " */",
            "",
            "#include <linux/errno.h>",
            "#include <linux/mm.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/fs.h>",
            "#include <linux/mman.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/rmap.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/xxhash.h>",
            "#include <linux/delay.h>",
            "#include <linux/kthread.h>",
            "#include <linux/wait.h>",
            "#include <linux/slab.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/memory.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/swap.h>",
            "#include <linux/ksm.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/freezer.h>",
            "#include <linux/oom.h>",
            "#include <linux/numa.h>",
            "#include <linux/pagewalk.h>",
            "",
            "#include <asm/tlbflush.h>",
            "#include \"internal.h\"",
            "#include \"mm_slot.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/ksm.h>",
            "",
            "#ifdef CONFIG_NUMA",
            "#define NUMA(x)\t\t(x)",
            "#define DO_NUMA(x)\tdo { (x); } while (0)",
            "#else",
            "#define NUMA(x)\t\t(0)",
            "#define DO_NUMA(x)\tdo { } while (0)",
            "#endif",
            "",
            "/**",
            " * DOC: Overview",
            " *",
            " * A few notes about the KSM scanning process,",
            " * to make it easier to understand the data structures below:",
            " *",
            " * In order to reduce excessive scanning, KSM sorts the memory pages by their",
            " * contents into a data structure that holds pointers to the pages' locations.",
            " *",
            " * Since the contents of the pages may change at any moment, KSM cannot just",
            " * insert the pages into a normal sorted tree and expect it to find anything.",
            " * Therefore KSM uses two data structures - the stable and the unstable tree.",
            " *",
            " * The stable tree holds pointers to all the merged pages (ksm pages), sorted",
            " * by their contents.  Because each such page is write-protected, searching on",
            " * this tree is fully assured to be working (except when pages are unmapped),",
            " * and therefore this tree is called the stable tree.",
            " *",
            " * The stable tree node includes information required for reverse",
            " * mapping from a KSM page to virtual addresses that map this page.",
            " *",
            " * In order to avoid large latencies of the rmap walks on KSM pages,",
            " * KSM maintains two types of nodes in the stable tree:",
            " *",
            " * * the regular nodes that keep the reverse mapping structures in a",
            " *   linked list",
            " * * the \"chains\" that link nodes (\"dups\") that represent the same",
            " *   write protected memory content, but each \"dup\" corresponds to a",
            " *   different KSM page copy of that content",
            " *",
            " * Internally, the regular nodes, \"dups\" and \"chains\" are represented",
            " * using the same struct ksm_stable_node structure.",
            " *",
            " * In addition to the stable tree, KSM uses a second data structure called the",
            " * unstable tree: this tree holds pointers to pages which have been found to",
            " * be \"unchanged for a period of time\".  The unstable tree sorts these pages",
            " * by their contents, but since they are not write-protected, KSM cannot rely",
            " * upon the unstable tree to work correctly - the unstable tree is liable to",
            " * be corrupted as its contents are modified, and so it is called unstable.",
            " *",
            " * KSM solves this problem by several techniques:",
            " *",
            " * 1) The unstable tree is flushed every time KSM completes scanning all",
            " *    memory areas, and then the tree is rebuilt again from the beginning.",
            " * 2) KSM will only insert into the unstable tree, pages whose hash value",
            " *    has not changed since the previous scan of all memory areas.",
            " * 3) The unstable tree is a RedBlack Tree - so its balancing is based on the",
            " *    colors of the nodes and not on their contents, assuring that even when",
            " *    the tree gets \"corrupted\" it won't get out of balance, so scanning time",
            " *    remains the same (also, searching and inserting nodes in an rbtree uses",
            " *    the same algorithm, so we have no overhead when we flush and rebuild).",
            " * 4) KSM never flushes the stable tree, which means that even if it were to",
            " *    take 10 attempts to find a page in the unstable tree, once it is found,",
            " *    it is secured in the stable tree.  (When we scan a new page, we first",
            " *    compare it against the stable tree, and then against the unstable tree.)",
            " *",
            " * If the merge_across_nodes tunable is unset, then KSM maintains multiple",
            " * stable trees and multiple unstable trees: one of each for each NUMA node.",
            " */",
            "",
            "/**",
            " * struct ksm_mm_slot - ksm information per mm that is being scanned",
            " * @slot: hash lookup from mm to mm_slot",
            " * @rmap_list: head for this mm_slot's singly-linked list of rmap_items",
            " */",
            "struct ksm_mm_slot {",
            "\tstruct mm_slot slot;",
            "\tstruct ksm_rmap_item *rmap_list;",
            "};",
            "",
            "/**",
            " * struct ksm_scan - cursor for scanning",
            " * @mm_slot: the current mm_slot we are scanning",
            " * @address: the next address inside that to be scanned",
            " * @rmap_list: link to the next rmap to be scanned in the rmap_list",
            " * @seqnr: count of completed full scans (needed when removing unstable node)",
            " *",
            " * There is only the one ksm_scan instance of this cursor structure.",
            " */",
            "struct ksm_scan {",
            "\tstruct ksm_mm_slot *mm_slot;",
            "\tunsigned long address;",
            "\tstruct ksm_rmap_item **rmap_list;",
            "\tunsigned long seqnr;",
            "};",
            "",
            "/**",
            " * struct ksm_stable_node - node of the stable rbtree",
            " * @node: rb node of this ksm page in the stable tree",
            " * @head: (overlaying parent) &migrate_nodes indicates temporarily on that list",
            " * @hlist_dup: linked into the stable_node->hlist with a stable_node chain",
            " * @list: linked into migrate_nodes, pending placement in the proper node tree",
            " * @hlist: hlist head of rmap_items using this ksm page",
            " * @kpfn: page frame number of this ksm page (perhaps temporarily on wrong nid)",
            " * @chain_prune_time: time of the last full garbage collection",
            " * @rmap_hlist_len: number of rmap_item entries in hlist or STABLE_NODE_CHAIN",
            " * @nid: NUMA node id of stable tree in which linked (may not match kpfn)",
            " */",
            "struct ksm_stable_node {",
            "\tunion {",
            "\t\tstruct rb_node node;\t/* when node of stable tree */",
            "\t\tstruct {\t\t/* when listed for migration */",
            "\t\t\tstruct list_head *head;",
            "\t\t\tstruct {",
            "\t\t\t\tstruct hlist_node hlist_dup;",
            "\t\t\t\tstruct list_head list;",
            "\t\t\t};",
            "\t\t};",
            "\t};",
            "\tstruct hlist_head hlist;",
            "\tunion {",
            "\t\tunsigned long kpfn;",
            "\t\tunsigned long chain_prune_time;",
            "\t};",
            "\t/*",
            "\t * STABLE_NODE_CHAIN can be any negative number in",
            "\t * rmap_hlist_len negative range, but better not -1 to be able",
            "\t * to reliably detect underflows.",
            "\t */",
            "#define STABLE_NODE_CHAIN -1024",
            "\tint rmap_hlist_len;",
            "#ifdef CONFIG_NUMA",
            "\tint nid;",
            "#endif",
            "};",
            "",
            "/**",
            " * struct ksm_rmap_item - reverse mapping item for virtual addresses",
            " * @rmap_list: next rmap_item in mm_slot's singly-linked rmap_list",
            " * @anon_vma: pointer to anon_vma for this mm,address, when in stable tree",
            " * @nid: NUMA node id of unstable tree in which linked (may not match page)",
            " * @mm: the memory structure this rmap_item is pointing into",
            " * @address: the virtual address this rmap_item tracks (+ flags in low bits)",
            " * @oldchecksum: previous checksum of the page at that virtual address",
            " * @node: rb node of this rmap_item in the unstable tree",
            " * @head: pointer to stable_node heading this list in the stable tree",
            " * @hlist: link into hlist of rmap_items hanging off that stable_node",
            " */",
            "struct ksm_rmap_item {",
            "\tstruct ksm_rmap_item *rmap_list;",
            "\tunion {",
            "\t\tstruct anon_vma *anon_vma;\t/* when stable */",
            "#ifdef CONFIG_NUMA",
            "\t\tint nid;\t\t/* when node of unstable tree */",
            "#endif",
            "\t};",
            "\tstruct mm_struct *mm;",
            "\tunsigned long address;\t\t/* + low bits used for flags below */",
            "\tunsigned int oldchecksum;\t/* when unstable */",
            "\tunion {",
            "\t\tstruct rb_node node;\t/* when node of unstable tree */",
            "\t\tstruct {\t\t/* when listed from stable tree */",
            "\t\t\tstruct ksm_stable_node *head;",
            "\t\t\tstruct hlist_node hlist;",
            "\t\t};",
            "\t};",
            "};",
            "",
            "#define SEQNR_MASK\t0x0ff\t/* low bits of unstable tree seqnr */",
            "#define UNSTABLE_FLAG\t0x100\t/* is a node of the unstable tree */",
            "#define STABLE_FLAG\t0x200\t/* is listed from the stable tree */",
            "",
            "/* The stable and unstable tree heads */",
            "static struct rb_root one_stable_tree[1] = { RB_ROOT };",
            "static struct rb_root one_unstable_tree[1] = { RB_ROOT };",
            "static struct rb_root *root_stable_tree = one_stable_tree;",
            "static struct rb_root *root_unstable_tree = one_unstable_tree;",
            "",
            "/* Recently migrated nodes of stable tree, pending proper placement */",
            "static LIST_HEAD(migrate_nodes);",
            "#define STABLE_NODE_DUP_HEAD ((struct list_head *)&migrate_nodes.prev)",
            "",
            "#define MM_SLOTS_HASH_BITS 10",
            "static DEFINE_HASHTABLE(mm_slots_hash, MM_SLOTS_HASH_BITS);",
            "",
            "static struct ksm_mm_slot ksm_mm_head = {",
            "\t.slot.mm_node = LIST_HEAD_INIT(ksm_mm_head.slot.mm_node),",
            "};",
            "static struct ksm_scan ksm_scan = {",
            "\t.mm_slot = &ksm_mm_head,",
            "};",
            "",
            "static struct kmem_cache *rmap_item_cache;",
            "static struct kmem_cache *stable_node_cache;",
            "static struct kmem_cache *mm_slot_cache;",
            "",
            "/* The number of pages scanned */",
            "static unsigned long ksm_pages_scanned;",
            "",
            "/* The number of nodes in the stable tree */",
            "static unsigned long ksm_pages_shared;",
            "",
            "/* The number of page slots additionally sharing those nodes */",
            "static unsigned long ksm_pages_sharing;",
            "",
            "/* The number of nodes in the unstable tree */",
            "static unsigned long ksm_pages_unshared;",
            "",
            "/* The number of rmap_items in use: to calculate pages_volatile */",
            "static unsigned long ksm_rmap_items;",
            "",
            "/* The number of stable_node chains */",
            "static unsigned long ksm_stable_node_chains;",
            "",
            "/* The number of stable_node dups linked to the stable_node chains */",
            "static unsigned long ksm_stable_node_dups;",
            "",
            "/* Delay in pruning stale stable_node_dups in the stable_node_chains */",
            "static unsigned int ksm_stable_node_chains_prune_millisecs = 2000;",
            "",
            "/* Maximum number of page slots sharing a stable node */",
            "static int ksm_max_page_sharing = 256;",
            "",
            "/* Number of pages ksmd should scan in one batch */",
            "static unsigned int ksm_thread_pages_to_scan = 100;",
            "",
            "/* Milliseconds ksmd should sleep between batches */",
            "static unsigned int ksm_thread_sleep_millisecs = 20;",
            "",
            "/* Checksum of an empty (zeroed) page */",
            "static unsigned int zero_checksum __read_mostly;",
            "",
            "/* Whether to merge empty (zeroed) pages with actual zero pages */",
            "static bool ksm_use_zero_pages __read_mostly;",
            "",
            "/* The number of zero pages which is placed by KSM */",
            "atomic_long_t ksm_zero_pages = ATOMIC_LONG_INIT(0);",
            "",
            "#ifdef CONFIG_NUMA",
            "/* Zeroed when merging across nodes is not allowed */",
            "static unsigned int ksm_merge_across_nodes = 1;",
            "static int ksm_nr_node_ids = 1;",
            "#else",
            "#define ksm_merge_across_nodes\t1U",
            "#define ksm_nr_node_ids\t\t1",
            "#endif",
            "",
            "#define KSM_RUN_STOP\t0",
            "#define KSM_RUN_MERGE\t1",
            "#define KSM_RUN_UNMERGE\t2",
            "#define KSM_RUN_OFFLINE\t4",
            "static unsigned long ksm_run = KSM_RUN_STOP;",
            "static void wait_while_offlining(void);",
            "",
            "static DECLARE_WAIT_QUEUE_HEAD(ksm_thread_wait);",
            "static DECLARE_WAIT_QUEUE_HEAD(ksm_iter_wait);",
            "static DEFINE_MUTEX(ksm_thread_mutex);",
            "static DEFINE_SPINLOCK(ksm_mmlist_lock);",
            "",
            "#define KSM_KMEM_CACHE(__struct, __flags) kmem_cache_create(#__struct,\\",
            "\t\tsizeof(struct __struct), __alignof__(struct __struct),\\",
            "\t\t(__flags), NULL)",
            ""
          ],
          "function_name": null,
          "description": "定义了KSM（内核同一内存页合并）模块的核心数据结构和全局变量，包括稳定树和不稳定树的数据结构、NUMA支持相关定义、哈希表和红黑树操作接口，以及用于跟踪扫描进度的kscan结构。核心功能是建立KSM内存合并算法的基础框架。",
          "similarity": 0.49056005477905273
        },
        {
          "chunk_id": 13,
          "file_path": "mm/ksm.c",
          "start_line": 3171,
          "end_line": 3288,
          "content": [
            "static ssize_t pages_to_scan_show(struct kobject *kobj,",
            "\t\t\t\t  struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_thread_pages_to_scan);",
            "}",
            "static ssize_t pages_to_scan_store(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr,",
            "\t\t\t\t   const char *buf, size_t count)",
            "{",
            "\tunsigned int nr_pages;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &nr_pages);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\tksm_thread_pages_to_scan = nr_pages;",
            "",
            "\treturn count;",
            "}",
            "static ssize_t run_show(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\tchar *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_run);",
            "}",
            "static ssize_t run_store(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\t const char *buf, size_t count)",
            "{",
            "\tunsigned int flags;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &flags);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "\tif (flags > KSM_RUN_UNMERGE)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * KSM_RUN_MERGE sets ksmd running, and 0 stops it running.",
            "\t * KSM_RUN_UNMERGE stops it running and unmerges all rmap_items,",
            "\t * breaking COW to free the pages_shared (but leaves mm_slots",
            "\t * on the list for when ksmd may be set running again).",
            "\t */",
            "",
            "\tmutex_lock(&ksm_thread_mutex);",
            "\twait_while_offlining();",
            "\tif (ksm_run != flags) {",
            "\t\tksm_run = flags;",
            "\t\tif (flags & KSM_RUN_UNMERGE) {",
            "\t\t\tset_current_oom_origin();",
            "\t\t\terr = unmerge_and_remove_all_rmap_items();",
            "\t\t\tclear_current_oom_origin();",
            "\t\t\tif (err) {",
            "\t\t\t\tksm_run = KSM_RUN_STOP;",
            "\t\t\t\tcount = err;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\tif (flags & KSM_RUN_MERGE)",
            "\t\twake_up_interruptible(&ksm_thread_wait);",
            "",
            "\treturn count;",
            "}",
            "static ssize_t merge_across_nodes_show(struct kobject *kobj,",
            "\t\t\t\t       struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_merge_across_nodes);",
            "}",
            "static ssize_t merge_across_nodes_store(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr,",
            "\t\t\t\t   const char *buf, size_t count)",
            "{",
            "\tint err;",
            "\tunsigned long knob;",
            "",
            "\terr = kstrtoul(buf, 10, &knob);",
            "\tif (err)",
            "\t\treturn err;",
            "\tif (knob > 1)",
            "\t\treturn -EINVAL;",
            "",
            "\tmutex_lock(&ksm_thread_mutex);",
            "\twait_while_offlining();",
            "\tif (ksm_merge_across_nodes != knob) {",
            "\t\tif (ksm_pages_shared || remove_all_stable_nodes())",
            "\t\t\terr = -EBUSY;",
            "\t\telse if (root_stable_tree == one_stable_tree) {",
            "\t\t\tstruct rb_root *buf;",
            "\t\t\t/*",
            "\t\t\t * This is the first time that we switch away from the",
            "\t\t\t * default of merging across nodes: must now allocate",
            "\t\t\t * a buffer to hold as many roots as may be needed.",
            "\t\t\t * Allocate stable and unstable together:",
            "\t\t\t * MAXSMP NODES_SHIFT 10 will use 16kB.",
            "\t\t\t */",
            "\t\t\tbuf = kcalloc(nr_node_ids + nr_node_ids, sizeof(*buf),",
            "\t\t\t\t      GFP_KERNEL);",
            "\t\t\t/* Let us assume that RB_ROOT is NULL is zero */",
            "\t\t\tif (!buf)",
            "\t\t\t\terr = -ENOMEM;",
            "\t\t\telse {",
            "\t\t\t\troot_stable_tree = buf;",
            "\t\t\t\troot_unstable_tree = buf + nr_node_ids;",
            "\t\t\t\t/* Stable tree is empty but not the unstable */",
            "\t\t\t\troot_unstable_tree[0] = one_unstable_tree[0];",
            "\t\t\t}",
            "\t\t}",
            "\t\tif (!err) {",
            "\t\t\tksm_merge_across_nodes = knob;",
            "\t\t\tksm_nr_node_ids = knob ? 1 : nr_node_ids;",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\treturn err ? err : count;",
            "}"
          ],
          "function_name": "pages_to_scan_show, pages_to_scan_store, run_show, run_store, merge_across_nodes_show, merge_across_nodes_store",
          "description": "pages_to_scan_*控制KSMAPI扫描页数；run_*控制KSMAPI运行模式（启动/停止/解合并）；merge_across_nodes_*配置是否允许跨NUMA节点合并，切换时重构稳定树根节点数组",
          "similarity": 0.46774762868881226
        },
        {
          "chunk_id": 14,
          "file_path": "mm/ksm.c",
          "start_line": 3300,
          "end_line": 3407,
          "content": [
            "static ssize_t use_zero_pages_show(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_use_zero_pages);",
            "}",
            "static ssize_t use_zero_pages_store(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr,",
            "\t\t\t\t   const char *buf, size_t count)",
            "{",
            "\tint err;",
            "\tbool value;",
            "",
            "\terr = kstrtobool(buf, &value);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\tksm_use_zero_pages = value;",
            "",
            "\treturn count;",
            "}",
            "static ssize_t max_page_sharing_show(struct kobject *kobj,",
            "\t\t\t\t     struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_max_page_sharing);",
            "}",
            "static ssize_t max_page_sharing_store(struct kobject *kobj,",
            "\t\t\t\t      struct kobj_attribute *attr,",
            "\t\t\t\t      const char *buf, size_t count)",
            "{",
            "\tint err;",
            "\tint knob;",
            "",
            "\terr = kstrtoint(buf, 10, &knob);",
            "\tif (err)",
            "\t\treturn err;",
            "\t/*",
            "\t * When a KSM page is created it is shared by 2 mappings. This",
            "\t * being a signed comparison, it implicitly verifies it's not",
            "\t * negative.",
            "\t */",
            "\tif (knob < 2)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (READ_ONCE(ksm_max_page_sharing) == knob)",
            "\t\treturn count;",
            "",
            "\tmutex_lock(&ksm_thread_mutex);",
            "\twait_while_offlining();",
            "\tif (ksm_max_page_sharing != knob) {",
            "\t\tif (ksm_pages_shared || remove_all_stable_nodes())",
            "\t\t\terr = -EBUSY;",
            "\t\telse",
            "\t\t\tksm_max_page_sharing = knob;",
            "\t}",
            "\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\treturn err ? err : count;",
            "}",
            "static ssize_t pages_scanned_show(struct kobject *kobj,",
            "\t\t\t\t  struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_scanned);",
            "}",
            "static ssize_t pages_shared_show(struct kobject *kobj,",
            "\t\t\t\t struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_shared);",
            "}",
            "static ssize_t pages_sharing_show(struct kobject *kobj,",
            "\t\t\t\t  struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_sharing);",
            "}",
            "static ssize_t pages_unshared_show(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_unshared);",
            "}",
            "static ssize_t pages_volatile_show(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\tlong ksm_pages_volatile;",
            "",
            "\tksm_pages_volatile = ksm_rmap_items - ksm_pages_shared",
            "\t\t\t\t- ksm_pages_sharing - ksm_pages_unshared;",
            "\t/*",
            "\t * It was not worth any locking to calculate that statistic,",
            "\t * but it might therefore sometimes be negative: conceal that.",
            "\t */",
            "\tif (ksm_pages_volatile < 0)",
            "\t\tksm_pages_volatile = 0;",
            "\treturn sysfs_emit(buf, \"%ld\\n\", ksm_pages_volatile);",
            "}",
            "static ssize_t ksm_zero_pages_show(struct kobject *kobj,",
            "\t\t\t\tstruct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%ld\\n\", atomic_long_read(&ksm_zero_pages));",
            "}",
            "static ssize_t general_profit_show(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\tlong general_profit;",
            "",
            "\tgeneral_profit = (ksm_pages_sharing + atomic_long_read(&ksm_zero_pages)) * PAGE_SIZE -",
            "\t\t\t\tksm_rmap_items * sizeof(struct ksm_rmap_item);",
            "",
            "\treturn sysfs_emit(buf, \"%ld\\n\", general_profit);",
            "}"
          ],
          "function_name": "use_zero_pages_show, use_zero_pages_store, max_page_sharing_show, max_page_sharing_store, pages_scanned_show, pages_shared_show, pages_sharing_show, pages_unshared_show, pages_volatile_show, ksm_zero_pages_show, general_profit_show",
          "description": "use_zero_pages_*控制是否使用零填充页面；max_page_sharing_*设置最大共享页数；pages_*系列接口统计KSMAPI页扫描、共享、分裂等状态；general_profit_*计算整体内存优化收益",
          "similarity": 0.4609820246696472
        },
        {
          "chunk_id": 9,
          "file_path": "mm/ksm.c",
          "start_line": 2608,
          "end_line": 2731,
          "content": [
            "int ksm_enable_merge_any(struct mm_struct *mm)",
            "{",
            "\tint err;",
            "",
            "\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn 0;",
            "",
            "\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {",
            "\t\terr = __ksm_enter(mm);",
            "\t\tif (err)",
            "\t\t\treturn err;",
            "\t}",
            "",
            "\tset_bit(MMF_VM_MERGE_ANY, &mm->flags);",
            "\tksm_add_vmas(mm);",
            "",
            "\treturn 0;",
            "}",
            "int ksm_disable_merge_any(struct mm_struct *mm)",
            "{",
            "\tint err;",
            "",
            "\tif (!test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn 0;",
            "",
            "\terr = ksm_del_vmas(mm);",
            "\tif (err) {",
            "\t\tksm_add_vmas(mm);",
            "\t\treturn err;",
            "\t}",
            "",
            "\tclear_bit(MMF_VM_MERGE_ANY, &mm->flags);",
            "\treturn 0;",
            "}",
            "int ksm_disable(struct mm_struct *mm)",
            "{",
            "\tmmap_assert_write_locked(mm);",
            "",
            "\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags))",
            "\t\treturn 0;",
            "\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn ksm_disable_merge_any(mm);",
            "\treturn ksm_del_vmas(mm);",
            "}",
            "int ksm_madvise(struct vm_area_struct *vma, unsigned long start,",
            "\t\tunsigned long end, int advice, unsigned long *vm_flags)",
            "{",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tint err;",
            "",
            "\tswitch (advice) {",
            "\tcase MADV_MERGEABLE:",
            "\t\tif (vma->vm_flags & VM_MERGEABLE)",
            "\t\t\treturn 0;",
            "\t\tif (!vma_ksm_compatible(vma))",
            "\t\t\treturn 0;",
            "",
            "\t\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {",
            "\t\t\terr = __ksm_enter(mm);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "",
            "\t\t*vm_flags |= VM_MERGEABLE;",
            "\t\tbreak;",
            "",
            "\tcase MADV_UNMERGEABLE:",
            "\t\tif (!(*vm_flags & VM_MERGEABLE))",
            "\t\t\treturn 0;\t\t/* just ignore the advice */",
            "",
            "\t\tif (vma->anon_vma) {",
            "\t\t\terr = unmerge_ksm_pages(vma, start, end, true);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "",
            "\t\t*vm_flags &= ~VM_MERGEABLE;",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __ksm_enter(struct mm_struct *mm)",
            "{",
            "\tstruct ksm_mm_slot *mm_slot;",
            "\tstruct mm_slot *slot;",
            "\tint needs_wakeup;",
            "",
            "\tmm_slot = mm_slot_alloc(mm_slot_cache);",
            "\tif (!mm_slot)",
            "\t\treturn -ENOMEM;",
            "",
            "\tslot = &mm_slot->slot;",
            "",
            "\t/* Check ksm_run too?  Would need tighter locking */",
            "\tneeds_wakeup = list_empty(&ksm_mm_head.slot.mm_node);",
            "",
            "\tspin_lock(&ksm_mmlist_lock);",
            "\tmm_slot_insert(mm_slots_hash, mm, slot);",
            "\t/*",
            "\t * When KSM_RUN_MERGE (or KSM_RUN_STOP),",
            "\t * insert just behind the scanning cursor, to let the area settle",
            "\t * down a little; when fork is followed by immediate exec, we don't",
            "\t * want ksmd to waste time setting up and tearing down an rmap_list.",
            "\t *",
            "\t * But when KSM_RUN_UNMERGE, it's important to insert ahead of its",
            "\t * scanning cursor, otherwise KSM pages in newly forked mms will be",
            "\t * missed: then we might as well insert at the end of the list.",
            "\t */",
            "\tif (ksm_run & KSM_RUN_UNMERGE)",
            "\t\tlist_add_tail(&slot->mm_node, &ksm_mm_head.slot.mm_node);",
            "\telse",
            "\t\tlist_add_tail(&slot->mm_node, &ksm_scan.mm_slot->slot.mm_node);",
            "\tspin_unlock(&ksm_mmlist_lock);",
            "",
            "\tset_bit(MMF_VM_MERGEABLE, &mm->flags);",
            "\tmmgrab(mm);",
            "",
            "\tif (needs_wakeup)",
            "\t\twake_up_interruptible(&ksm_thread_wait);",
            "",
            "\ttrace_ksm_enter(mm);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ksm_enable_merge_any, ksm_disable_merge_any, ksm_disable, ksm_madvise, __ksm_enter",
          "description": "ksm_enable_merge_any 启用全局合并标志并注册内存区域。ksm_disable_merge_any 禁用合并并清理配置。ksm_disable 移除所有KSM配置。ksm_madvise 处理MADV_MERGEABLE/MADV_UNMERGEABLE建议，调整页面属性。__ksm_enter 注册内存区域到KSM管理系统。",
          "similarity": 0.44552624225616455
        },
        {
          "chunk_id": 12,
          "file_path": "mm/ksm.c",
          "start_line": 3034,
          "end_line": 3143,
          "content": [
            "static void ksm_check_stable_tree(unsigned long start_pfn,",
            "\t\t\t\t  unsigned long end_pfn)",
            "{",
            "\tstruct ksm_stable_node *stable_node, *next;",
            "\tstruct rb_node *node;",
            "\tint nid;",
            "",
            "\tfor (nid = 0; nid < ksm_nr_node_ids; nid++) {",
            "\t\tnode = rb_first(root_stable_tree + nid);",
            "\t\twhile (node) {",
            "\t\t\tstable_node = rb_entry(node, struct ksm_stable_node, node);",
            "\t\t\tif (stable_node_chain_remove_range(stable_node,",
            "\t\t\t\t\t\t\t   start_pfn, end_pfn,",
            "\t\t\t\t\t\t\t   root_stable_tree +",
            "\t\t\t\t\t\t\t   nid))",
            "\t\t\t\tnode = rb_first(root_stable_tree + nid);",
            "\t\t\telse",
            "\t\t\t\tnode = rb_next(node);",
            "\t\t\tcond_resched();",
            "\t\t}",
            "\t}",
            "\tlist_for_each_entry_safe(stable_node, next, &migrate_nodes, list) {",
            "\t\tif (stable_node->kpfn >= start_pfn &&",
            "\t\t    stable_node->kpfn < end_pfn)",
            "\t\t\tremove_node_from_stable_tree(stable_node);",
            "\t\tcond_resched();",
            "\t}",
            "}",
            "static int ksm_memory_callback(struct notifier_block *self,",
            "\t\t\t       unsigned long action, void *arg)",
            "{",
            "\tstruct memory_notify *mn = arg;",
            "",
            "\tswitch (action) {",
            "\tcase MEM_GOING_OFFLINE:",
            "\t\t/*",
            "\t\t * Prevent ksm_do_scan(), unmerge_and_remove_all_rmap_items()",
            "\t\t * and remove_all_stable_nodes() while memory is going offline:",
            "\t\t * it is unsafe for them to touch the stable tree at this time.",
            "\t\t * But unmerge_ksm_pages(), rmap lookups and other entry points",
            "\t\t * which do not need the ksm_thread_mutex are all safe.",
            "\t\t */",
            "\t\tmutex_lock(&ksm_thread_mutex);",
            "\t\tksm_run |= KSM_RUN_OFFLINE;",
            "\t\tmutex_unlock(&ksm_thread_mutex);",
            "\t\tbreak;",
            "",
            "\tcase MEM_OFFLINE:",
            "\t\t/*",
            "\t\t * Most of the work is done by page migration; but there might",
            "\t\t * be a few stable_nodes left over, still pointing to struct",
            "\t\t * pages which have been offlined: prune those from the tree,",
            "\t\t * otherwise get_ksm_page() might later try to access a",
            "\t\t * non-existent struct page.",
            "\t\t */",
            "\t\tksm_check_stable_tree(mn->start_pfn,",
            "\t\t\t\t      mn->start_pfn + mn->nr_pages);",
            "\t\tfallthrough;",
            "\tcase MEM_CANCEL_OFFLINE:",
            "\t\tmutex_lock(&ksm_thread_mutex);",
            "\t\tksm_run &= ~KSM_RUN_OFFLINE;",
            "\t\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\t\tsmp_mb();\t/* wake_up_bit advises this */",
            "\t\twake_up_bit(&ksm_run, ilog2(KSM_RUN_OFFLINE));",
            "\t\tbreak;",
            "\t}",
            "\treturn NOTIFY_OK;",
            "}",
            "static void wait_while_offlining(void)",
            "{",
            "}",
            "bool ksm_process_mergeable(struct mm_struct *mm)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "",
            "\tmmap_assert_locked(mm);",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "\tfor_each_vma(vmi, vma)",
            "\t\tif (vma->vm_flags & VM_MERGEABLE)",
            "\t\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "long ksm_process_profit(struct mm_struct *mm)",
            "{",
            "\treturn (long)(mm->ksm_merging_pages + mm_ksm_zero_pages(mm)) * PAGE_SIZE -",
            "\t\tmm->ksm_rmap_items * sizeof(struct ksm_rmap_item);",
            "}",
            "static ssize_t sleep_millisecs_show(struct kobject *kobj,",
            "\t\t\t\t    struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_thread_sleep_millisecs);",
            "}",
            "static ssize_t sleep_millisecs_store(struct kobject *kobj,",
            "\t\t\t\t     struct kobj_attribute *attr,",
            "\t\t\t\t     const char *buf, size_t count)",
            "{",
            "\tunsigned int msecs;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &msecs);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\tksm_thread_sleep_millisecs = msecs;",
            "\twake_up_interruptible(&ksm_iter_wait);",
            "",
            "\treturn count;",
            "}"
          ],
          "function_name": "ksm_check_stable_tree, ksm_memory_callback, wait_while_offlining, ksm_process_mergeable, ksm_process_profit, sleep_millisecs_show, sleep_millisecs_store",
          "description": "ksm_check_stable_tree扫描稳定树移除指定PFN范围节点；ksm_memory_callback处理内存离线/取消离线事件，同步KSMAPI状态；ksm_process_mergeable检测进程是否含可合并VMA；ksm_process_profit计算KSMAPI收益；sleep_millisecs_*控制KSMAPI线程休眠时间",
          "similarity": 0.438772588968277
        }
      ]
    },
    {
      "source_file": "kernel/dma/pool.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:15:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\pool.c`\n\n---\n\n# `dma/pool.c` 技术文档\n\n## 1. 文件概述\n\n`dma/pool.c` 实现了 Linux 内核中的 **DMA 原子内存池（atomic DMA pools）** 机制，用于在无法睡眠的上下文（如中断处理、原子上下文）中分配一致性（coherent）DMA 内存。该机制通过预分配多个按内存区域（ZONE_DMA、ZONE_DMA32、普通内核内存）划分的通用内存池（`gen_pool`），并在池空间不足时通过工作队列异步扩展，从而支持在 GFP_ATOMIC 等限制性分配标志下安全地分配 DMA 内存。\n\n该文件主要用于支持 `dma-direct` 子系统中的原子 DMA 分配路径，确保即使在内存压力大或无法睡眠的场景下，设备驱动仍能获得满足地址限制（如 32 位或 24 位寻址）的一致性 DMA 缓冲区。\n\n## 2. 核心功能\n\n### 全局变量\n- `atomic_pool_dma` / `pool_size_dma`：用于 `GFP_DMA` 区域的原子 DMA 池及其已分配大小。\n- `atomic_pool_dma32` / `pool_size_dma32`：用于 `GFP_DMA32` 区域的原子 DMA 池及其已分配大小。\n- `atomic_pool_kernel` / `pool_size_kernel`：用于普通内核区域（无特殊 DMA 限制）的原子 DMA 池及其已分配大小。\n- `atomic_pool_size`：每个池的初始目标大小，可通过内核命令行参数 `coherent_pool=` 设置。\n- `atomic_pool_work`：用于后台动态扩展内存池的工作项。\n\n### 主要函数\n- `early_coherent_pool()`：解析内核命令行参数 `coherent_pool`，设置 `atomic_pool_size`。\n- `dma_atomic_pool_init()`：初始化所有原子 DMA 池（postcore 阶段调用）。\n- `__dma_atomic_pool_init()`：创建并填充指定 GFP 标志的原子池。\n- `atomic_pool_expand()`：向指定池中添加一块连续物理内存。\n- `atomic_pool_resize()` / `atomic_pool_work_fn()`：检查池剩余空间，若不足则触发扩展。\n- `dma_alloc_from_pool()`：从合适的原子池中分配指定大小的 DMA 内存。\n- `dma_free_from_pool()`：将内存归还到对应的原子池。\n- `dma_guess_pool()`：根据 GFP 标志和尝试顺序选择合适的内存池。\n- `cma_in_zone()`：判断 CMA 区域是否位于指定 DMA 区域内，以决定是否优先从 CMA 分配。\n- `dma_atomic_pool_debugfs_init()`：在 debugfs 中导出各池的当前大小。\n\n## 3. 关键实现\n\n### 内存池初始化策略\n- 若未通过 `coherent_pool=` 指定大小，则默认按 **每 1GB 物理内存分配 128KB** 原子池，最小 128KB，最大不超过 `MAX_ORDER_NR_PAGES` 对应的内存。\n- 每个池使用 `gen_pool` 管理，分配算法为 `gen_pool_first_fit_order_align`，保证分配地址按页对齐。\n- 初始化时调用 `atomic_pool_expand()` 预分配内存。\n\n### 内存分配来源\n- 优先尝试从 **CMA（Contiguous Memory Allocator）** 区域分配（若 CMA 区域位于目标 DMA zone 内）。\n- 若 CMA 不可用或不在目标 zone，则回退到 `alloc_pages()`。\n- 分配的内存块大小不超过 `MAX_PAGE_ORDER`，通过降序尝试（从大到小）提高分配成功率。\n\n### 内存属性处理\n- 调用 `arch_dma_prep_coherent()` 通知架构层准备一致性内存。\n- 在支持内存加密（如 AMD SEV、Intel TDX）的系统上，显式调用 `set_memory_decrypted()` 确保 DMA 内存为 **未加密状态**，因为设备无法访问加密内存。\n- 若启用了 `CONFIG_DMA_DIRECT_REMAP`，则通过 `dma_common_contiguous_remap()` 建立非缓存或设备专用的页表映射。\n\n### 动态扩展机制\n- 每次从池中分配内存后，检查剩余空间是否小于 `atomic_pool_size`。\n- 若不足，则调度 `atomic_pool_work` 工作项，在进程上下文中异步扩展对应池。\n- 扩展时尝试分配与当前池总大小相当的新内存块，避免频繁小量扩展。\n\n### 多池选择逻辑\n- `dma_guess_pool()` 实现池选择策略：\n  1. 首选与 GFP 标志匹配的池（DMA32 > DMA > 普通内核）。\n  2. 若首次分配失败，按 `kernel → dma32 → dma` 顺序尝试其他池（fallback 机制）。\n- 释放时遍历所有池，通过 `gen_pool_has_addr()` 确定内存归属。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - 依赖 `genalloc`（`gen_pool`）实现内存池管理。\n  - 使用 `alloc_pages()`、`__free_pages()` 进行底层页分配。\n  - 依赖 CMA 接口（`dma_alloc_from_contiguous()`）获取大块连续内存。\n- **DMA 子系统**：\n  - 与 `dma-direct.c` 紧密集成，为其提供 `___dma_direct_alloc_pages()` 中的原子分配路径。\n  - 使用 `dma-map-ops.h` 和 `dma-direct.h` 中的辅助函数。\n- **架构相关支持**：\n  - 调用 `arch_dma_prep_coherent()`（架构可选实现）。\n  - 使用 `set_memory_decrypted()`/`set_memory_encrypted()`（x86/ARM64 等支持内存加密的架构）。\n  - 依赖 `DMA_BIT_MASK()` 和 `zone_dma_bits` 判断 DMA 地址范围。\n- **其他**：\n  - 使用 `debugfs` 导出调试信息。\n  - 依赖 `workqueue` 实现异步扩展。\n  - 使用 `slab.h` 中的内存分配器（间接）。\n\n## 5. 使用场景\n\n- **原子上下文 DMA 分配**：当设备驱动在中断处理程序、自旋锁保护区域或使用 `GFP_ATOMIC` 标志调用 `dma_alloc_coherent()` 时，若常规页分配器无法满足（如内存碎片），内核会回退到从原子池分配。\n- **满足地址限制的 DMA 缓冲区**：对于需要 24 位（ISA 设备）或 32 位（旧 PCIe 设备）寻址能力的设备，驱动使用 `DMA_BIT_MASK(24)` 或 `DMA_BIT_MASK(32)` 限制 DMA 地址范围，原子池确保分配的内存物理地址符合要求。\n- **一致性内存需求**：适用于需要 CPU 与设备之间缓存一致性的场景（如网络数据包缓冲区、音频流缓冲区），原子池分配的内存经过 `arch_dma_prep_coherent()` 处理，保证一致性。\n- **内存加密环境**：在启用内存加密的系统中，确保分配给设备的 DMA 内存处于解密状态，使设备能正常访问。",
      "similarity": 0.5184347629547119,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/pool.c",
          "start_line": 29,
          "end_line": 138,
          "content": [
            "static int __init early_coherent_pool(char *p)",
            "{",
            "\tatomic_pool_size = memparse(p, &p);",
            "\treturn 0;",
            "}",
            "static void __init dma_atomic_pool_debugfs_init(void)",
            "{",
            "\tstruct dentry *root;",
            "",
            "\troot = debugfs_create_dir(\"dma_pools\", NULL);",
            "\tdebugfs_create_ulong(\"pool_size_dma\", 0400, root, &pool_size_dma);",
            "\tdebugfs_create_ulong(\"pool_size_dma32\", 0400, root, &pool_size_dma32);",
            "\tdebugfs_create_ulong(\"pool_size_kernel\", 0400, root, &pool_size_kernel);",
            "}",
            "static void dma_atomic_pool_size_add(gfp_t gfp, size_t size)",
            "{",
            "\tif (gfp & __GFP_DMA)",
            "\t\tpool_size_dma += size;",
            "\telse if (gfp & __GFP_DMA32)",
            "\t\tpool_size_dma32 += size;",
            "\telse",
            "\t\tpool_size_kernel += size;",
            "}",
            "static bool cma_in_zone(gfp_t gfp)",
            "{",
            "\tunsigned long size;",
            "\tphys_addr_t end;",
            "\tstruct cma *cma;",
            "",
            "\tcma = dev_get_cma_area(NULL);",
            "\tif (!cma)",
            "\t\treturn false;",
            "",
            "\tsize = cma_get_size(cma);",
            "\tif (!size)",
            "\t\treturn false;",
            "",
            "\t/* CMA can't cross zone boundaries, see cma_activate_area() */",
            "\tend = cma_get_base(cma) + size - 1;",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA) && (gfp & GFP_DMA))",
            "\t\treturn end <= DMA_BIT_MASK(zone_dma_bits);",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA32) && (gfp & GFP_DMA32))",
            "\t\treturn end <= DMA_BIT_MASK(32);",
            "\treturn true;",
            "}",
            "static int atomic_pool_expand(struct gen_pool *pool, size_t pool_size,",
            "\t\t\t      gfp_t gfp)",
            "{",
            "\tunsigned int order;",
            "\tstruct page *page = NULL;",
            "\tvoid *addr;",
            "\tint ret = -ENOMEM;",
            "",
            "\t/* Cannot allocate larger than MAX_PAGE_ORDER */",
            "\torder = min(get_order(pool_size), MAX_PAGE_ORDER);",
            "",
            "\tdo {",
            "\t\tpool_size = 1 << (PAGE_SHIFT + order);",
            "\t\tif (cma_in_zone(gfp))",
            "\t\t\tpage = dma_alloc_from_contiguous(NULL, 1 << order,",
            "\t\t\t\t\t\t\t order, false);",
            "\t\tif (!page)",
            "\t\t\tpage = alloc_pages(gfp, order);",
            "\t} while (!page && order-- > 0);",
            "\tif (!page)",
            "\t\tgoto out;",
            "",
            "\tarch_dma_prep_coherent(page, pool_size);",
            "",
            "#ifdef CONFIG_DMA_DIRECT_REMAP",
            "\taddr = dma_common_contiguous_remap(page, pool_size,",
            "\t\t\tpgprot_decrypted(pgprot_dmacoherent(PAGE_KERNEL)),",
            "\t\t\t__builtin_return_address(0));",
            "\tif (!addr)",
            "\t\tgoto free_page;",
            "#else",
            "\taddr = page_to_virt(page);",
            "#endif",
            "\t/*",
            "\t * Memory in the atomic DMA pools must be unencrypted, the pools do not",
            "\t * shrink so no re-encryption occurs in dma_direct_free().",
            "\t */",
            "\tret = set_memory_decrypted((unsigned long)page_to_virt(page),",
            "\t\t\t\t   1 << order);",
            "\tif (ret)",
            "\t\tgoto remove_mapping;",
            "\tret = gen_pool_add_virt(pool, (unsigned long)addr, page_to_phys(page),",
            "\t\t\t\tpool_size, NUMA_NO_NODE);",
            "\tif (ret)",
            "\t\tgoto encrypt_mapping;",
            "",
            "\tdma_atomic_pool_size_add(gfp, pool_size);",
            "\treturn 0;",
            "",
            "encrypt_mapping:",
            "\tret = set_memory_encrypted((unsigned long)page_to_virt(page),",
            "\t\t\t\t   1 << order);",
            "\tif (WARN_ON_ONCE(ret)) {",
            "\t\t/* Decrypt succeeded but encrypt failed, purposely leak */",
            "\t\tgoto out;",
            "\t}",
            "remove_mapping:",
            "#ifdef CONFIG_DMA_DIRECT_REMAP",
            "\tdma_common_free_remap(addr, pool_size);",
            "free_page:",
            "\t__free_pages(page, order);",
            "#endif",
            "out:",
            "\treturn ret;",
            "}"
          ],
          "function_name": "early_coherent_pool, dma_atomic_pool_debugfs_init, dma_atomic_pool_size_add, cma_in_zone, atomic_pool_expand",
          "description": "实现DMA内存池的动态扩展逻辑，包含解析命令行参数、调试接口注册、内存分配策略选择、CMA区域有效性检测及池扩容操作，支持加密/解密内存映射管理。",
          "similarity": 0.49293169379234314
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/dma/pool.c",
          "start_line": 145,
          "end_line": 207,
          "content": [
            "static void atomic_pool_resize(struct gen_pool *pool, gfp_t gfp)",
            "{",
            "\tif (pool && gen_pool_avail(pool) < atomic_pool_size)",
            "\t\tatomic_pool_expand(pool, gen_pool_size(pool), gfp);",
            "}",
            "static void atomic_pool_work_fn(struct work_struct *work)",
            "{",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA))",
            "\t\tatomic_pool_resize(atomic_pool_dma,",
            "\t\t\t\t   GFP_KERNEL | GFP_DMA);",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA32))",
            "\t\tatomic_pool_resize(atomic_pool_dma32,",
            "\t\t\t\t   GFP_KERNEL | GFP_DMA32);",
            "\tatomic_pool_resize(atomic_pool_kernel, GFP_KERNEL);",
            "}",
            "static int __init dma_atomic_pool_init(void)",
            "{",
            "\tint ret = 0;",
            "",
            "\t/*",
            "\t * If coherent_pool was not used on the command line, default the pool",
            "\t * sizes to 128KB per 1GB of memory, min 128KB, max MAX_PAGE_ORDER.",
            "\t */",
            "\tif (!atomic_pool_size) {",
            "\t\tunsigned long pages = totalram_pages() / (SZ_1G / SZ_128K);",
            "\t\tpages = min_t(unsigned long, pages, MAX_ORDER_NR_PAGES);",
            "\t\tatomic_pool_size = max_t(size_t, pages << PAGE_SHIFT, SZ_128K);",
            "\t}",
            "\tINIT_WORK(&atomic_pool_work, atomic_pool_work_fn);",
            "",
            "\tatomic_pool_kernel = __dma_atomic_pool_init(atomic_pool_size,",
            "\t\t\t\t\t\t    GFP_KERNEL);",
            "\tif (!atomic_pool_kernel)",
            "\t\tret = -ENOMEM;",
            "\tif (has_managed_dma()) {",
            "\t\tatomic_pool_dma = __dma_atomic_pool_init(atomic_pool_size,",
            "\t\t\t\t\t\tGFP_KERNEL | GFP_DMA);",
            "\t\tif (!atomic_pool_dma)",
            "\t\t\tret = -ENOMEM;",
            "\t}",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA32)) {",
            "\t\tatomic_pool_dma32 = __dma_atomic_pool_init(atomic_pool_size,",
            "\t\t\t\t\t\tGFP_KERNEL | GFP_DMA32);",
            "\t\tif (!atomic_pool_dma32)",
            "\t\t\tret = -ENOMEM;",
            "\t}",
            "",
            "\tdma_atomic_pool_debugfs_init();",
            "\treturn ret;",
            "}",
            "bool dma_free_from_pool(struct device *dev, void *start, size_t size)",
            "{",
            "\tstruct gen_pool *pool = NULL;",
            "",
            "\twhile ((pool = dma_guess_pool(pool, 0))) {",
            "\t\tif (!gen_pool_has_addr(pool, (unsigned long)start, size))",
            "\t\t\tcontinue;",
            "\t\tgen_pool_free(pool, (unsigned long)start, size);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "atomic_pool_resize, atomic_pool_work_fn, dma_atomic_pool_init, dma_free_from_pool",
          "description": "实现内存池的初始化与维护机制，包含池大小自动调节逻辑、后台扩展任务调度、默认尺寸计算及内存释放查找功能，提供设备内存池的统一管理接口。",
          "similarity": 0.4824250340461731
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/pool.c",
          "start_line": 1,
          "end_line": 28,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 2012 ARM Ltd.",
            " * Copyright (C) 2020 Google LLC",
            " */",
            "#include <linux/cma.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/dma-map-ops.h>",
            "#include <linux/dma-direct.h>",
            "#include <linux/init.h>",
            "#include <linux/genalloc.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/slab.h>",
            "#include <linux/workqueue.h>",
            "",
            "static struct gen_pool *atomic_pool_dma __ro_after_init;",
            "static unsigned long pool_size_dma;",
            "static struct gen_pool *atomic_pool_dma32 __ro_after_init;",
            "static unsigned long pool_size_dma32;",
            "static struct gen_pool *atomic_pool_kernel __ro_after_init;",
            "static unsigned long pool_size_kernel;",
            "",
            "/* Size can be defined by the coherent_pool command line */",
            "static size_t atomic_pool_size;",
            "",
            "/* Dynamic background expansion when the atomic pool is near capacity */",
            "static struct work_struct atomic_pool_work;",
            ""
          ],
          "function_name": null,
          "description": "定义并初始化用于管理DMA内存池的全局变量，包括针对不同架构（DMA/DMA32/内核）的通用池指针、尺寸参数及动态扩展的工作队列。",
          "similarity": 0.4583635628223419
        }
      ]
    }
  ]
}