{
  "query": "库函数实现中系统调用的参数处理",
  "timestamp": "2025-12-26 02:06:50",
  "retrieved_files": [
    {
      "source_file": "kernel/entry/common.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:19:03\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `entry\\common.c`\n\n---\n\n# entry\\common.c 技术文档\n\n## 文件概述\n\n`entry\\common.c` 是 Linux 内核中处理系统调用入口/出口以及中断入口/出口路径的通用逻辑实现文件。该文件提供了一套架构无关的通用函数，用于在用户态与内核态之间切换时执行必要的上下文跟踪、审计、跟踪点、安全检查（如 seccomp）、信号处理、调度等工作。其目标是统一不同架构在系统调用和中断处理路径上的共性逻辑，减少重复代码。\n\n## 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `syscall_trace_enter()` | 系统调用进入时的通用处理函数，依次处理用户态分发、ptrace 跟踪、seccomp 安全检查、tracepoint 和审计 |\n| `syscall_enter_from_user_mode_prepare()` | 从用户模式进入系统调用前的准备，启用中断并进入内核上下文 |\n| `exit_to_user_mode_loop()` | 在返回用户空间前循环处理所有待办工作项（如调度、信号、uprobe、livepatch 等） |\n| `syscall_exit_work()` | 系统调用退出时的通用处理，包括审计、tracepoint、ptrace 退出报告等 |\n| `irqentry_enter()` / `irqentry_exit()` | 中断入口/出口的通用处理，管理 RCU、上下文跟踪、KMSAN、lockdep 等 |\n| `irqentry_enter_from_user_mode()` / `irqentry_exit_to_user_mode()` | 从中断上下文进入/退出用户模式的专用路径 |\n| `raw_irqentry_exit_cond_resched()` | 中断退出时的条件调度检查（仅在非抢占计数为 0 时） |\n\n### 关键数据结构\n\n- `irqentry_state_t`：记录中断入口状态，主要用于判断是否需要在退出时执行 RCU 相关操作。\n- `SYSCALL_WORK_*` 和 `_TIF_*` 标志位：用于标识待处理的工作类型（如 trace、seccomp、信号、调度等）。\n\n## 关键实现\n\n### 系统调用入口处理流程（`syscall_trace_enter`）\n\n1. **Syscall User Dispatch 优先处理**：若设置了 `SYSCALL_WORK_SYSCALL_USER_DISPATCH`，调用 `syscall_user_dispatch()`，若返回 true 则直接终止系统调用（返回 `-1`），因为此时 ABI 可能无效。\n2. **Ptrace 跟踪**：若设置了 `SYSCALL_WORK_SYSCALL_TRACE` 或 `SYSCALL_WORK_SYSCALL_EMU`，调用 `ptrace_report_syscall_entry()`。若 tracer 修改了行为或启用了 `SYSCALL_EMU`，则终止系统调用。\n3. **Seccomp 安全检查**：在 ptrace 之后执行，以捕获 tracer 可能引入的变更。调用 `__secure_computing()`，若返回 `-1` 则拒绝系统调用。\n4. **重新获取系统调用号**：上述步骤可能修改了系统调用号，需重新通过 `syscall_get_nr()` 获取。\n5. **Tracepoint 触发**：若启用 `SYSCALL_WORK_SYSCALL_TRACEPOINT`，触发 `trace_sys_enter`，并再次重新获取系统调用号（因 BPF 或 kprobe 可能修改）。\n6. **审计日志**：调用 `syscall_enter_audit()` 记录审计事件。\n7. **返回最终系统调用号或错误码**。\n\n### 返回用户空间前的工作循环（`exit_to_user_mode_loop`）\n\n- 使用 `while (ti_work & EXIT_TO_USER_MODE_WORK)` 循环处理所有待办工作，确保在返回用户态前完成：\n  - 调度（`_TIF_NEED_RESCHED`）\n  - Uprobe 通知（`_TIF_UPROBE`）\n  - Livepatch 状态更新（`_TIF_PATCH_PENDING`）\n  - 信号处理（`_TIF_SIGPENDING | _TIF_NOTIFY_SIGNAL`）\n  - 用户态恢复工作（`_TIF_NOTIFY_RESUME`）\n  - 架构特定工作（`arch_exit_to_user_mode_work`）\n- 每次循环启用中断（`local_irq_enable_exit_to_user`），处理完后再关闭中断并重新读取线程标志（`read_thread_flags()`），以应对处理过程中新产生的工作项。\n- 最后调用 `tick_nohz_user_enter_prepare()` 处理 NO_HZ 模式下的 tick 准备。\n\n### 中断入口/出口的 RCU 与上下文管理\n\n- **从中断进入用户态**：调用 `enter_from_user_mode()`，启用中断。\n- **从内核态中断入口**：\n  - 若当前是 idle 任务且非 `TINY_RCU`，无条件调用 `ct_irq_enter()` 以确保 RCU 状态一致（避免嵌套中断导致 grace period 错误结束）。\n  - 否则调用 `rcu_irq_enter_check_tick()`。\n- 所有路径均正确处理 `lockdep`、`KMSAN`（解除寄存器毒化）和 `trace_hardirqs_off` 的顺序，确保调试和安全工具正常工作。\n\n### 条件调度支持（Preemption）\n\n- `raw_irqentry_exit_cond_resched()` 在中断退出且 `preempt_count() == 0` 时检查是否需要调度。\n- 支持动态抢占（`CONFIG_PREEMPT_DYNAMIC`），通过 `static_call` 或 `static_key` 实现运行时切换，避免编译时硬编码。\n\n## 依赖关系\n\n### 头文件依赖\n- `<linux/context_tracking.h>`：上下文跟踪（用户/内核态切换）\n- `<linux/resume_user_mode.h>`：用户态恢复工作\n- `<linux/seccomp.h>`（隐式通过 `__secure_computing`）：系统调用过滤\n- `<linux/audit.h>`：审计子系统\n- `<linux/ptrace.h>`（隐式）：ptrace 跟踪\n- `<linux/livepatch.h>`：内核热补丁\n- `<linux/uprobes.h>`（隐式）：用户态探针\n- `<linux/rcupdate.h>`：RCU 机制\n- `<linux/kmsan.h>`：Kernel Memory Sanitizer 支持\n- `<trace/events/syscalls.h>`：系统调用跟踪点\n\n### 架构依赖\n- 依赖架构特定实现：\n  - `syscall_get_arguments()` / `syscall_get_nr()` / `syscall_get_return_value()`\n  - `user_mode()` / `regs_irqs_disabled()`\n  - `arch_do_signal_or_restart()`（弱符号，默认空实现）\n  - `arch_exit_to_user_mode_work()`\n\n### 子系统交互\n- **RCU**：管理中断和用户态切换时的宽限期\n- **Scheduler**：处理 `need_resched()` 和 `schedule()`\n- **Security**：seccomp、audit\n- **Tracing**：ftrace、kprobe、uprobe、BPF\n- **Livepatch**：动态补丁状态更新\n\n## 使用场景\n\n1. **系统调用入口路径**：  \n   当用户程序执行 `syscall` 指令（或其他系统调用机制）进入内核时，架构代码调用 `syscall_trace_enter()` 执行通用预处理。\n\n2. **系统调用出口路径**：  \n   系统调用返回前，若存在待处理工作（如审计、tracepoint），调用 `syscall_exit_work()`。\n\n3. **中断处理返回用户空间**：  \n   中断处理完成后，若返回用户态，调用 `irqentry_exit_to_user_mode()`，进而触发 `exit_to_user_mode_loop()` 处理所有 pending work。\n\n4. **中断嵌套与 idle 任务处理**：  \n   在 idle 任务中发生中断时，确保 RCU 正确进入 IRQ 上下文，防止 grace period 错误终止。\n\n5. **动态抢占支持**：  \n   在支持动态抢占的系统中，中断退出时根据运行时配置决定是否执行条件调度。\n\n6. **调试与安全工具集成**：  \n   为 KMSAN、Lockdep、ftrace、audit、seccomp 等子系统提供统一的入口/出口钩子，确保工具链在系统调用和中断路径上正常工作。",
      "similarity": 0.5809034109115601,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/entry/common.c",
          "start_line": 18,
          "end_line": 124,
          "content": [
            "static inline void syscall_enter_audit(struct pt_regs *regs, long syscall)",
            "{",
            "\tif (unlikely(audit_context())) {",
            "\t\tunsigned long args[6];",
            "",
            "\t\tsyscall_get_arguments(current, regs, args);",
            "\t\taudit_syscall_entry(syscall, args[0], args[1], args[2], args[3]);",
            "\t}",
            "}",
            "long syscall_trace_enter(struct pt_regs *regs, long syscall,",
            "\t\t\t\tunsigned long work)",
            "{",
            "\tlong ret = 0;",
            "",
            "\t/*",
            "\t * Handle Syscall User Dispatch.  This must comes first, since",
            "\t * the ABI here can be something that doesn't make sense for",
            "\t * other syscall_work features.",
            "\t */",
            "\tif (work & SYSCALL_WORK_SYSCALL_USER_DISPATCH) {",
            "\t\tif (syscall_user_dispatch(regs))",
            "\t\t\treturn -1L;",
            "\t}",
            "",
            "\t/* Handle ptrace */",
            "\tif (work & (SYSCALL_WORK_SYSCALL_TRACE | SYSCALL_WORK_SYSCALL_EMU)) {",
            "\t\tret = ptrace_report_syscall_entry(regs);",
            "\t\tif (ret || (work & SYSCALL_WORK_SYSCALL_EMU))",
            "\t\t\treturn -1L;",
            "\t}",
            "",
            "\t/* Do seccomp after ptrace, to catch any tracer changes. */",
            "\tif (work & SYSCALL_WORK_SECCOMP) {",
            "\t\tret = __secure_computing(NULL);",
            "\t\tif (ret == -1L)",
            "\t\t\treturn ret;",
            "\t}",
            "",
            "\t/* Either of the above might have changed the syscall number */",
            "\tsyscall = syscall_get_nr(current, regs);",
            "",
            "\tif (unlikely(work & SYSCALL_WORK_SYSCALL_TRACEPOINT)) {",
            "\t\ttrace_sys_enter(regs, syscall);",
            "\t\t/*",
            "\t\t * Probes or BPF hooks in the tracepoint may have changed the",
            "\t\t * system call number as well.",
            "\t\t */",
            "\t\tsyscall = syscall_get_nr(current, regs);",
            "\t}",
            "",
            "\tsyscall_enter_audit(regs, syscall);",
            "",
            "\treturn ret ? : syscall;",
            "}",
            "noinstr void syscall_enter_from_user_mode_prepare(struct pt_regs *regs)",
            "{",
            "\tenter_from_user_mode(regs);",
            "\tinstrumentation_begin();",
            "\tlocal_irq_enable();",
            "\tinstrumentation_end();",
            "}",
            "void __weak arch_do_signal_or_restart(struct pt_regs *regs) { }",
            "__always_inline unsigned long exit_to_user_mode_loop(struct pt_regs *regs,",
            "\t\t\t\t\t\t     unsigned long ti_work)",
            "{",
            "\t/*",
            "\t * Before returning to user space ensure that all pending work",
            "\t * items have been completed.",
            "\t */",
            "\twhile (ti_work & EXIT_TO_USER_MODE_WORK) {",
            "",
            "\t\tlocal_irq_enable_exit_to_user(ti_work);",
            "",
            "\t\tif (ti_work & _TIF_NEED_RESCHED)",
            "\t\t\tschedule();",
            "",
            "\t\tif (ti_work & _TIF_UPROBE)",
            "\t\t\tuprobe_notify_resume(regs);",
            "",
            "\t\tif (ti_work & _TIF_PATCH_PENDING)",
            "\t\t\tklp_update_patch_state(current);",
            "",
            "\t\tif (ti_work & (_TIF_SIGPENDING | _TIF_NOTIFY_SIGNAL))",
            "\t\t\tarch_do_signal_or_restart(regs);",
            "",
            "\t\tif (ti_work & _TIF_NOTIFY_RESUME)",
            "\t\t\tresume_user_mode_work(regs);",
            "",
            "\t\t/* Architecture specific TIF work */",
            "\t\tarch_exit_to_user_mode_work(regs, ti_work);",
            "",
            "\t\t/*",
            "\t\t * Disable interrupts and reevaluate the work flags as they",
            "\t\t * might have changed while interrupts and preemption was",
            "\t\t * enabled above.",
            "\t\t */",
            "\t\tlocal_irq_disable_exit_to_user();",
            "",
            "\t\t/* Check if any of the above work has queued a deferred wakeup */",
            "\t\ttick_nohz_user_enter_prepare();",
            "",
            "\t\tti_work = read_thread_flags();",
            "\t}",
            "",
            "\t/* Return the latest work state for arch_exit_to_user_mode() */",
            "\treturn ti_work;",
            "}"
          ],
          "function_name": "syscall_enter_audit, syscall_trace_enter, syscall_enter_from_user_mode_prepare, arch_do_signal_or_restart, exit_to_user_mode_loop",
          "description": "实现系统调用入口处理逻辑，依次处理审计追踪、指针观察、seccomp过滤及架构特定工作，通过位掩码控制不同安全机制的触发顺序并更新系统调用号",
          "similarity": 0.6006268262863159
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/entry/common.c",
          "start_line": 141,
          "end_line": 250,
          "content": [
            "static inline bool report_single_step(unsigned long work)",
            "{",
            "\tif (work & SYSCALL_WORK_SYSCALL_EMU)",
            "\t\treturn false;",
            "",
            "\treturn work & SYSCALL_WORK_SYSCALL_EXIT_TRAP;",
            "}",
            "void syscall_exit_work(struct pt_regs *regs, unsigned long work)",
            "{",
            "\tbool step;",
            "",
            "\t/*",
            "\t * If the syscall was rolled back due to syscall user dispatching,",
            "\t * then the tracers below are not invoked for the same reason as",
            "\t * the entry side was not invoked in syscall_trace_enter(): The ABI",
            "\t * of these syscalls is unknown.",
            "\t */",
            "\tif (work & SYSCALL_WORK_SYSCALL_USER_DISPATCH) {",
            "\t\tif (unlikely(current->syscall_dispatch.on_dispatch)) {",
            "\t\t\tcurrent->syscall_dispatch.on_dispatch = false;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "",
            "\taudit_syscall_exit(regs);",
            "",
            "\tif (work & SYSCALL_WORK_SYSCALL_TRACEPOINT)",
            "\t\ttrace_sys_exit(regs, syscall_get_return_value(current, regs));",
            "",
            "\tstep = report_single_step(work);",
            "\tif (step || work & SYSCALL_WORK_SYSCALL_TRACE)",
            "\t\tptrace_report_syscall_exit(regs, step);",
            "}",
            "noinstr void irqentry_enter_from_user_mode(struct pt_regs *regs)",
            "{",
            "\tenter_from_user_mode(regs);",
            "}",
            "noinstr void irqentry_exit_to_user_mode(struct pt_regs *regs)",
            "{",
            "\tinstrumentation_begin();",
            "\texit_to_user_mode_prepare(regs);",
            "\tinstrumentation_end();",
            "\texit_to_user_mode();",
            "}",
            "noinstr irqentry_state_t irqentry_enter(struct pt_regs *regs)",
            "{",
            "\tirqentry_state_t ret = {",
            "\t\t.exit_rcu = false,",
            "\t};",
            "",
            "\tif (user_mode(regs)) {",
            "\t\tirqentry_enter_from_user_mode(regs);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\t/*",
            "\t * If this entry hit the idle task invoke ct_irq_enter() whether",
            "\t * RCU is watching or not.",
            "\t *",
            "\t * Interrupts can nest when the first interrupt invokes softirq",
            "\t * processing on return which enables interrupts.",
            "\t *",
            "\t * Scheduler ticks in the idle task can mark quiescent state and",
            "\t * terminate a grace period, if and only if the timer interrupt is",
            "\t * not nested into another interrupt.",
            "\t *",
            "\t * Checking for rcu_is_watching() here would prevent the nesting",
            "\t * interrupt to invoke ct_irq_enter(). If that nested interrupt is",
            "\t * the tick then rcu_flavor_sched_clock_irq() would wrongfully",
            "\t * assume that it is the first interrupt and eventually claim",
            "\t * quiescent state and end grace periods prematurely.",
            "\t *",
            "\t * Unconditionally invoke ct_irq_enter() so RCU state stays",
            "\t * consistent.",
            "\t *",
            "\t * TINY_RCU does not support EQS, so let the compiler eliminate",
            "\t * this part when enabled.",
            "\t */",
            "\tif (!IS_ENABLED(CONFIG_TINY_RCU) && is_idle_task(current)) {",
            "\t\t/*",
            "\t\t * If RCU is not watching then the same careful",
            "\t\t * sequence vs. lockdep and tracing is required",
            "\t\t * as in irqentry_enter_from_user_mode().",
            "\t\t */",
            "\t\tlockdep_hardirqs_off(CALLER_ADDR0);",
            "\t\tct_irq_enter();",
            "\t\tinstrumentation_begin();",
            "\t\tkmsan_unpoison_entry_regs(regs);",
            "\t\ttrace_hardirqs_off_finish();",
            "\t\tinstrumentation_end();",
            "",
            "\t\tret.exit_rcu = true;",
            "\t\treturn ret;",
            "\t}",
            "",
            "\t/*",
            "\t * If RCU is watching then RCU only wants to check whether it needs",
            "\t * to restart the tick in NOHZ mode. rcu_irq_enter_check_tick()",
            "\t * already contains a warning when RCU is not watching, so no point",
            "\t * in having another one here.",
            "\t */",
            "\tlockdep_hardirqs_off(CALLER_ADDR0);",
            "\tinstrumentation_begin();",
            "\tkmsan_unpoison_entry_regs(regs);",
            "\trcu_irq_enter_check_tick();",
            "\ttrace_hardirqs_off_finish();",
            "\tinstrumentation_end();",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "report_single_step, syscall_exit_work, irqentry_enter_from_user_mode, irqentry_exit_to_user_mode, irqentry_enter",
          "description": "处理系统调用退出阶段工作，包括审计退出记录、单步调试报告和跟踪点事件，同时管理中断返回时的RCU状态转换和锁依赖检查",
          "similarity": 0.5838720798492432
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/entry/common.c",
          "start_line": 1,
          "end_line": 17,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "",
            "#include <linux/context_tracking.h>",
            "#include <linux/entry-common.h>",
            "#include <linux/resume_user_mode.h>",
            "#include <linux/highmem.h>",
            "#include <linux/jump_label.h>",
            "#include <linux/kmsan.h>",
            "#include <linux/livepatch.h>",
            "#include <linux/audit.h>",
            "#include <linux/tick.h>",
            "",
            "#include \"common.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/syscalls.h>",
            ""
          ],
          "function_name": null,
          "description": "声明系统调用通用功能所需头文件，包含审计、跟踪、KMSAN、LivePatch等模块的接口定义，为后续系统调用入口处理提供基础依赖",
          "similarity": 0.5830382108688354
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/entry/common.c",
          "start_line": 256,
          "end_line": 347,
          "content": [
            "void raw_irqentry_exit_cond_resched(void)",
            "{",
            "\tif (!preempt_count()) {",
            "\t\t/* Sanity check RCU and thread stack */",
            "\t\trcu_irq_exit_check_preempt();",
            "\t\tif (IS_ENABLED(CONFIG_DEBUG_ENTRY))",
            "\t\t\tWARN_ON_ONCE(!on_thread_stack());",
            "\t\tif (need_resched())",
            "\t\t\tpreempt_schedule_irq();",
            "\t}",
            "}",
            "void dynamic_irqentry_exit_cond_resched(void)",
            "{",
            "\tif (!static_branch_unlikely(&sk_dynamic_irqentry_exit_cond_resched))",
            "\t\treturn;",
            "\traw_irqentry_exit_cond_resched();",
            "}",
            "noinstr void irqentry_exit(struct pt_regs *regs, irqentry_state_t state)",
            "{",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\t/* Check whether this returns to user mode */",
            "\tif (user_mode(regs)) {",
            "\t\tirqentry_exit_to_user_mode(regs);",
            "\t} else if (!regs_irqs_disabled(regs)) {",
            "\t\t/*",
            "\t\t * If RCU was not watching on entry this needs to be done",
            "\t\t * carefully and needs the same ordering of lockdep/tracing",
            "\t\t * and RCU as the return to user mode path.",
            "\t\t */",
            "\t\tif (state.exit_rcu) {",
            "\t\t\tinstrumentation_begin();",
            "\t\t\t/* Tell the tracer that IRET will enable interrupts */",
            "\t\t\ttrace_hardirqs_on_prepare();",
            "\t\t\tlockdep_hardirqs_on_prepare();",
            "\t\t\tinstrumentation_end();",
            "\t\t\tct_irq_exit();",
            "\t\t\tlockdep_hardirqs_on(CALLER_ADDR0);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tinstrumentation_begin();",
            "\t\tif (IS_ENABLED(CONFIG_PREEMPTION))",
            "\t\t\tirqentry_exit_cond_resched();",
            "",
            "\t\t/* Covers both tracing and lockdep */",
            "\t\ttrace_hardirqs_on();",
            "\t\tinstrumentation_end();",
            "\t} else {",
            "\t\t/*",
            "\t\t * IRQ flags state is correct already. Just tell RCU if it",
            "\t\t * was not watching on entry.",
            "\t\t */",
            "\t\tif (state.exit_rcu)",
            "\t\t\tct_irq_exit();",
            "\t}",
            "}",
            "irqentry_state_t noinstr irqentry_nmi_enter(struct pt_regs *regs)",
            "{",
            "\tirqentry_state_t irq_state;",
            "",
            "\tirq_state.lockdep = lockdep_hardirqs_enabled();",
            "",
            "\t__nmi_enter();",
            "\tlockdep_hardirqs_off(CALLER_ADDR0);",
            "\tlockdep_hardirq_enter();",
            "\tct_nmi_enter();",
            "",
            "\tinstrumentation_begin();",
            "\tkmsan_unpoison_entry_regs(regs);",
            "\ttrace_hardirqs_off_finish();",
            "\tftrace_nmi_enter();",
            "\tinstrumentation_end();",
            "",
            "\treturn irq_state;",
            "}",
            "void noinstr irqentry_nmi_exit(struct pt_regs *regs, irqentry_state_t irq_state)",
            "{",
            "\tinstrumentation_begin();",
            "\tftrace_nmi_exit();",
            "\tif (irq_state.lockdep) {",
            "\t\ttrace_hardirqs_on_prepare();",
            "\t\tlockdep_hardirqs_on_prepare();",
            "\t}",
            "\tinstrumentation_end();",
            "",
            "\tct_nmi_exit();",
            "\tlockdep_hardirq_exit();",
            "\tif (irq_state.lockdep)",
            "\t\tlockdep_hardirqs_on(CALLER_ADDR0);",
            "\t__nmi_exit();",
            "}"
          ],
          "function_name": "raw_irqentry_exit_cond_resched, dynamic_irqentry_exit_cond_resched, irqentry_exit, irqentry_nmi_enter, irqentry_nmi_exit",
          "description": "实现中断退出路径的条件调度检查和NMI上下文切换，维护中断标志状态一致性，处理RCU状态转换、锁依赖标记恢复及异常中断返回时的上下文还原",
          "similarity": 0.5692585706710815
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.572031557559967,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 438,
          "end_line": 544,
          "content": [
            "static int param_set_first_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : j);",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "static int param_set_next_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : (j ?: 1));",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "unsigned long rcu_get_gp_seq(void)",
            "{",
            "\treturn READ_ONCE(rcu_state.gp_seq);",
            "}",
            "unsigned long rcu_exp_batches_completed(void)",
            "{",
            "\treturn rcu_state.expedited_sequence;",
            "}",
            "void rcutorture_get_gp_data(enum rcutorture_type test_type, int *flags,",
            "\t\t\t    unsigned long *gp_seq)",
            "{",
            "\tswitch (test_type) {",
            "\tcase RCU_FLAVOR:",
            "\t\t*flags = READ_ONCE(rcu_state.gp_flags);",
            "\t\t*gp_seq = rcu_seq_current(&rcu_state.gp_seq);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "}",
            "static void late_wakeup_func(struct irq_work *work)",
            "{",
            "}",
            "noinstr void rcu_irq_work_resched(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tif (IS_ENABLED(CONFIG_GENERIC_ENTRY) && !(current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tif (IS_ENABLED(CONFIG_KVM_XFER_TO_GUEST_WORK) && (current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tinstrumentation_begin();",
            "\tif (do_nocb_deferred_wakeup(rdp) && need_resched()) {",
            "\t\tirq_work_queue(this_cpu_ptr(&late_wakeup_work));",
            "\t}",
            "\tinstrumentation_end();",
            "}",
            "void rcu_irq_exit_check_preempt(void)",
            "{",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nesting() <= 0,",
            "\t\t\t \"RCU dynticks_nesting counter underflow/zero!\");",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nmi_nesting() !=",
            "\t\t\t DYNTICK_IRQ_NONIDLE,",
            "\t\t\t \"Bad RCU  dynticks_nmi_nesting counter\\n\");",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"RCU in extended quiescent state!\");",
            "}",
            "void __rcu_irq_enter_check_tick(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\t// If we're here from NMI there's nothing to do.",
            "\tif (in_nmi())",
            "\t\treturn;",
            "",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"Illegal rcu_irq_enter_check_tick() from extended quiescent state\");",
            "",
            "\tif (!tick_nohz_full_cpu(rdp->cpu) ||",
            "\t    !READ_ONCE(rdp->rcu_urgent_qs) ||",
            "\t    READ_ONCE(rdp->rcu_forced_tick)) {",
            "\t\t// RCU doesn't need nohz_full help from this CPU, or it is",
            "\t\t// already getting that help.",
            "\t\treturn;",
            "\t}",
            "",
            "\t// We get here only when not in an extended quiescent state and",
            "\t// from interrupts (as opposed to NMIs).  Therefore, (1) RCU is",
            "\t// already watching and (2) The fact that we are in an interrupt",
            "\t// handler and that the rcu_node lock is an irq-disabled lock",
            "\t// prevents self-deadlock.  So we can safely recheck under the lock.",
            "\t// Note that the nohz_full state currently cannot change.",
            "\traw_spin_lock_rcu_node(rdp->mynode);",
            "\tif (READ_ONCE(rdp->rcu_urgent_qs) && !rdp->rcu_forced_tick) {",
            "\t\t// A nohz_full CPU is in the kernel and RCU needs a",
            "\t\t// quiescent state.  Turn on the tick!",
            "\t\tWRITE_ONCE(rdp->rcu_forced_tick, true);",
            "\t\ttick_dep_set_cpu(rdp->cpu, TICK_DEP_BIT_RCU);",
            "\t}",
            "\traw_spin_unlock_rcu_node(rdp->mynode);",
            "}"
          ],
          "function_name": "param_set_first_fqs_jiffies, param_set_next_fqs_jiffies, rcu_get_gp_seq, rcu_exp_batches_completed, rcutorture_get_gp_data, late_wakeup_func, rcu_irq_work_resched, rcu_irq_exit_check_preempt, __rcu_irq_enter_check_tick",
          "description": "实现参数配置回调函数和中断上下文RCU工作重排逻辑，管理grace period序列号读取及nohz_full模式下的tick依赖关系。",
          "similarity": 0.6349610686302185
        },
        {
          "chunk_id": 15,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2658,
          "end_line": 2759,
          "content": [
            "static void rcu_leak_callback(struct rcu_head *rhp)",
            "{",
            "}",
            "static void check_cb_ovld_locked(struct rcu_data *rdp, struct rcu_node *rnp)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (qovld_calc <= 0)",
            "\t\treturn; // Early boot and wildcard value set.",
            "\tif (rcu_segcblist_n_cbs(&rdp->cblist) >= qovld_calc)",
            "\t\tWRITE_ONCE(rnp->cbovldmask, rnp->cbovldmask | rdp->grpmask);",
            "\telse",
            "\t\tWRITE_ONCE(rnp->cbovldmask, rnp->cbovldmask & ~rdp->grpmask);",
            "}",
            "static void check_cb_ovld(struct rcu_data *rdp)",
            "{",
            "\tstruct rcu_node *const rnp = rdp->mynode;",
            "",
            "\tif (qovld_calc <= 0 ||",
            "\t    ((rcu_segcblist_n_cbs(&rdp->cblist) >= qovld_calc) ==",
            "\t     !!(READ_ONCE(rnp->cbovldmask) & rdp->grpmask)))",
            "\t\treturn; // Early boot wildcard value or already set correctly.",
            "\traw_spin_lock_rcu_node(rnp);",
            "\tcheck_cb_ovld_locked(rdp, rnp);",
            "\traw_spin_unlock_rcu_node(rnp);",
            "}",
            "static void",
            "__call_rcu_common(struct rcu_head *head, rcu_callback_t func, bool lazy_in)",
            "{",
            "\tstatic atomic_t doublefrees;",
            "\tunsigned long flags;",
            "\tbool lazy;",
            "\tstruct rcu_data *rdp;",
            "",
            "\t/* Misaligned rcu_head! */",
            "\tWARN_ON_ONCE((unsigned long)head & (sizeof(void *) - 1));",
            "",
            "\t/* Avoid NULL dereference if callback is NULL. */",
            "\tif (WARN_ON_ONCE(!func))",
            "\t\treturn;",
            "",
            "\tif (debug_rcu_head_queue(head)) {",
            "\t\t/*",
            "\t\t * Probable double call_rcu(), so leak the callback.",
            "\t\t * Use rcu:rcu_callback trace event to find the previous",
            "\t\t * time callback was passed to call_rcu().",
            "\t\t */",
            "\t\tif (atomic_inc_return(&doublefrees) < 4) {",
            "\t\t\tpr_err(\"%s(): Double-freed CB %p->%pS()!!!  \", __func__, head, head->func);",
            "\t\t\tmem_dump_obj(head);",
            "\t\t}",
            "\t\tWRITE_ONCE(head->func, rcu_leak_callback);",
            "\t\treturn;",
            "\t}",
            "\thead->func = func;",
            "\thead->next = NULL;",
            "\tkasan_record_aux_stack_noalloc(head);",
            "\tlocal_irq_save(flags);",
            "\trdp = this_cpu_ptr(&rcu_data);",
            "\tlazy = lazy_in && !rcu_async_should_hurry();",
            "",
            "\t/* Add the callback to our list. */",
            "\tif (unlikely(!rcu_segcblist_is_enabled(&rdp->cblist))) {",
            "\t\t// This can trigger due to call_rcu() from offline CPU:",
            "\t\tWARN_ON_ONCE(rcu_scheduler_active != RCU_SCHEDULER_INACTIVE);",
            "\t\tWARN_ON_ONCE(!rcu_is_watching());",
            "\t\t// Very early boot, before rcu_init().  Initialize if needed",
            "\t\t// and then drop through to queue the callback.",
            "\t\tif (rcu_segcblist_empty(&rdp->cblist))",
            "\t\t\trcu_segcblist_init(&rdp->cblist);",
            "\t}",
            "",
            "\tcheck_cb_ovld(rdp);",
            "",
            "\tif (unlikely(rcu_rdp_is_offloaded(rdp)))",
            "\t\tcall_rcu_nocb(rdp, head, func, flags, lazy);",
            "\telse",
            "\t\tcall_rcu_core(rdp, head, func, flags);",
            "\tlocal_irq_restore(flags);",
            "}",
            "void call_rcu_hurry(struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\treturn __call_rcu_common(head, func, false);",
            "}",
            "void call_rcu(struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\treturn __call_rcu_common(head, func, IS_ENABLED(CONFIG_RCU_LAZY));",
            "}",
            "static __always_inline void",
            "debug_rcu_bhead_unqueue(struct kvfree_rcu_bulk_data *bhead)",
            "{",
            "#ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD",
            "\tint i;",
            "",
            "\tfor (i = 0; i < bhead->nr_records; i++)",
            "\t\tdebug_rcu_head_unqueue((struct rcu_head *)(bhead->records[i]));",
            "#endif",
            "}",
            "static inline void",
            "krc_this_cpu_unlock(struct kfree_rcu_cpu *krcp, unsigned long flags)",
            "{",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "}"
          ],
          "function_name": "rcu_leak_callback, check_cb_ovld_locked, check_cb_ovld, __call_rcu_common, call_rcu_hurry, call_rcu, debug_rcu_bhead_unqueue, krc_this_cpu_unlock",
          "description": "实现RCU回调管理，检查回调过载并处理双释放情况，__call_rcu_common负责将回调添加到RCU回调列表并处理延迟释放逻辑。",
          "similarity": 0.5661454200744629
        },
        {
          "chunk_id": 22,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4072,
          "end_line": 4226,
          "content": [
            "static void rcu_barrier_trace(const char *s, int cpu, unsigned long done)",
            "{",
            "\ttrace_rcu_barrier(rcu_state.name, s, cpu,",
            "\t\t\t  atomic_read(&rcu_state.barrier_cpu_count), done);",
            "}",
            "static void rcu_barrier_callback(struct rcu_head *rhp)",
            "{",
            "\tunsigned long __maybe_unused s = rcu_state.barrier_sequence;",
            "",
            "\tif (atomic_dec_and_test(&rcu_state.barrier_cpu_count)) {",
            "\t\trcu_barrier_trace(TPS(\"LastCB\"), -1, s);",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "\t} else {",
            "\t\trcu_barrier_trace(TPS(\"CB\"), -1, s);",
            "\t}",
            "}",
            "static void rcu_barrier_entrain(struct rcu_data *rdp)",
            "{",
            "\tunsigned long gseq = READ_ONCE(rcu_state.barrier_sequence);",
            "\tunsigned long lseq = READ_ONCE(rdp->barrier_seq_snap);",
            "\tbool wake_nocb = false;",
            "\tbool was_alldone = false;",
            "",
            "\tlockdep_assert_held(&rcu_state.barrier_lock);",
            "\tif (rcu_seq_state(lseq) || !rcu_seq_state(gseq) || rcu_seq_ctr(lseq) != rcu_seq_ctr(gseq))",
            "\t\treturn;",
            "\trcu_barrier_trace(TPS(\"IRQ\"), -1, rcu_state.barrier_sequence);",
            "\trdp->barrier_head.func = rcu_barrier_callback;",
            "\tdebug_rcu_head_queue(&rdp->barrier_head);",
            "\trcu_nocb_lock(rdp);",
            "\t/*",
            "\t * Flush bypass and wakeup rcuog if we add callbacks to an empty regular",
            "\t * queue. This way we don't wait for bypass timer that can reach seconds",
            "\t * if it's fully lazy.",
            "\t */",
            "\twas_alldone = rcu_rdp_is_offloaded(rdp) && !rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));",
            "\twake_nocb = was_alldone && rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tif (rcu_segcblist_entrain(&rdp->cblist, &rdp->barrier_head)) {",
            "\t\tatomic_inc(&rcu_state.barrier_cpu_count);",
            "\t} else {",
            "\t\tdebug_rcu_head_unqueue(&rdp->barrier_head);",
            "\t\trcu_barrier_trace(TPS(\"IRQNQ\"), -1, rcu_state.barrier_sequence);",
            "\t}",
            "\trcu_nocb_unlock(rdp);",
            "\tif (wake_nocb)",
            "\t\twake_nocb_gp(rdp, false);",
            "\tsmp_store_release(&rdp->barrier_seq_snap, gseq);",
            "}",
            "static void rcu_barrier_handler(void *cpu_in)",
            "{",
            "\tuintptr_t cpu = (uintptr_t)cpu_in;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\tWARN_ON_ONCE(cpu != rdp->cpu);",
            "\tWARN_ON_ONCE(cpu != smp_processor_id());",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\trcu_barrier_entrain(rdp);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "}",
            "void rcu_barrier(void)",
            "{",
            "\tuintptr_t cpu;",
            "\tunsigned long flags;",
            "\tunsigned long gseq;",
            "\tstruct rcu_data *rdp;",
            "\tunsigned long s = rcu_seq_snap(&rcu_state.barrier_sequence);",
            "",
            "\trcu_barrier_trace(TPS(\"Begin\"), -1, s);",
            "",
            "\t/* Take mutex to serialize concurrent rcu_barrier() requests. */",
            "\tmutex_lock(&rcu_state.barrier_mutex);",
            "",
            "\t/* Did someone else do our work for us? */",
            "\tif (rcu_seq_done(&rcu_state.barrier_sequence, s)) {",
            "\t\trcu_barrier_trace(TPS(\"EarlyExit\"), -1, rcu_state.barrier_sequence);",
            "\t\tsmp_mb(); /* caller's subsequent code after above check. */",
            "\t\tmutex_unlock(&rcu_state.barrier_mutex);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Mark the start of the barrier operation. */",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\trcu_seq_start(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\trcu_barrier_trace(TPS(\"Inc1\"), -1, rcu_state.barrier_sequence);",
            "",
            "\t/*",
            "\t * Initialize the count to two rather than to zero in order",
            "\t * to avoid a too-soon return to zero in case of an immediate",
            "\t * invocation of the just-enqueued callback (or preemption of",
            "\t * this task).  Exclude CPU-hotplug operations to ensure that no",
            "\t * offline non-offloaded CPU has callbacks queued.",
            "\t */",
            "\tinit_completion(&rcu_state.barrier_completion);",
            "\tatomic_set(&rcu_state.barrier_cpu_count, 2);",
            "\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "",
            "\t/*",
            "\t * Force each CPU with callbacks to register a new callback.",
            "\t * When that callback is invoked, we will know that all of the",
            "\t * corresponding CPU's preceding callbacks have been invoked.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "retry:",
            "\t\tif (smp_load_acquire(&rdp->barrier_seq_snap) == gseq)",
            "\t\t\tcontinue;",
            "\t\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\t\tif (!rcu_segcblist_n_cbs(&rdp->cblist)) {",
            "\t\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"NQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (!rcu_rdp_cpu_online(rdp)) {",
            "\t\t\trcu_barrier_entrain(rdp);",
            "\t\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"OfflineNoCBQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\tif (smp_call_function_single(cpu, rcu_barrier_handler, (void *)cpu, 1)) {",
            "\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\tgoto retry;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\trcu_barrier_trace(TPS(\"OnlineQ\"), cpu, rcu_state.barrier_sequence);",
            "\t}",
            "",
            "\t/*",
            "\t * Now that we have an rcu_barrier_callback() callback on each",
            "\t * CPU, and thus each counted, remove the initial count.",
            "\t */",
            "\tif (atomic_sub_and_test(2, &rcu_state.barrier_cpu_count))",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "",
            "\t/* Wait for all rcu_barrier_callback() callbacks to be invoked. */",
            "\twait_for_completion(&rcu_state.barrier_completion);",
            "",
            "\t/* Mark the end of the barrier operation. */",
            "\trcu_barrier_trace(TPS(\"Inc2\"), -1, rcu_state.barrier_sequence);",
            "\trcu_seq_end(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t}",
            "",
            "\t/* Other rcu_barrier() invocations can now safely proceed. */",
            "\tmutex_unlock(&rcu_state.barrier_mutex);",
            "}"
          ],
          "function_name": "rcu_barrier_trace, rcu_barrier_callback, rcu_barrier_entrain, rcu_barrier_handler, rcu_barrier",
          "description": "实现RCU屏障功能，通过分发回调函数强制所有CPU完成当前RCU操作，使用原子计数器跟踪完成状态，通过completion等待所有回调完成",
          "similarity": 0.5657033920288086
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2273,
          "end_line": 2388,
          "content": [
            "void rcu_sched_clock_irq(int user)",
            "{",
            "\tunsigned long j;",
            "",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU)) {",
            "\t\tj = jiffies;",
            "\t\tWARN_ON_ONCE(time_before(j, __this_cpu_read(rcu_data.last_sched_clock)));",
            "\t\t__this_cpu_write(rcu_data.last_sched_clock, j);",
            "\t}",
            "\ttrace_rcu_utilization(TPS(\"Start scheduler-tick\"));",
            "\tlockdep_assert_irqs_disabled();",
            "\traw_cpu_inc(rcu_data.ticks_this_gp);",
            "\t/* The load-acquire pairs with the store-release setting to true. */",
            "\tif (smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs))) {",
            "\t\t/* Idle and userspace execution already are quiescent states. */",
            "\t\tif (!rcu_is_cpu_rrupt_from_idle() && !user) {",
            "\t\t\tset_tsk_need_resched(current);",
            "\t\t\tset_preempt_need_resched();",
            "\t\t}",
            "\t\t__this_cpu_write(rcu_data.rcu_urgent_qs, false);",
            "\t}",
            "\trcu_flavor_sched_clock_irq(user);",
            "\tif (rcu_pending(user))",
            "\t\tinvoke_rcu_core();",
            "\tif (user || rcu_is_cpu_rrupt_from_idle())",
            "\t\trcu_note_voluntary_context_switch(current);",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\ttrace_rcu_utilization(TPS(\"End scheduler-tick\"));",
            "}",
            "static void force_qs_rnp(int (*f)(struct rcu_data *rdp))",
            "{",
            "\tint cpu;",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trcu_state.cbovld = rcu_state.cbovldnext;",
            "\trcu_state.cbovldnext = false;",
            "\trcu_for_each_leaf_node(rnp) {",
            "\t\tunsigned long mask = 0;",
            "\t\tunsigned long rsmask = 0;",
            "",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\trcu_state.cbovldnext |= !!rnp->cbovldmask;",
            "\t\tif (rnp->qsmask == 0) {",
            "\t\t\tif (rcu_preempt_blocked_readers_cgp(rnp)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No point in scanning bits because they",
            "\t\t\t\t * are all zero.  But we might need to",
            "\t\t\t\t * priority-boost blocked readers.",
            "\t\t\t\t */",
            "\t\t\t\trcu_initiate_boost(rnp, flags);",
            "\t\t\t\t/* rcu_initiate_boost() releases rnp->lock */",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rnp->qsmask) {",
            "\t\t\tstruct rcu_data *rdp;",
            "\t\t\tint ret;",
            "",
            "\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\t\t\tret = f(rdp);",
            "\t\t\tif (ret > 0) {",
            "\t\t\t\tmask |= rdp->grpmask;",
            "\t\t\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\t\t}",
            "\t\t\tif (ret < 0)",
            "\t\t\t\trsmask |= rdp->grpmask;",
            "\t\t}",
            "\t\tif (mask != 0) {",
            "\t\t\t/* Idle/offline CPUs, report (releases rnp->lock). */",
            "\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\t} else {",
            "\t\t\t/* Nothing to do here, so just drop the lock. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t}",
            "",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rsmask)",
            "\t\t\tresched_cpu(cpu);",
            "\t}",
            "}",
            "void rcu_force_quiescent_state(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool ret;",
            "\tstruct rcu_node *rnp;",
            "\tstruct rcu_node *rnp_old = NULL;",
            "",
            "\t/* Funnel through hierarchy to reduce memory contention. */",
            "\trnp = raw_cpu_read(rcu_data.mynode);",
            "\tfor (; rnp != NULL; rnp = rnp->parent) {",
            "\t\tret = (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) ||",
            "\t\t       !raw_spin_trylock(&rnp->fqslock);",
            "\t\tif (rnp_old != NULL)",
            "\t\t\traw_spin_unlock(&rnp_old->fqslock);",
            "\t\tif (ret)",
            "\t\t\treturn;",
            "\t\trnp_old = rnp;",
            "\t}",
            "\t/* rnp_old == rcu_get_root(), rnp == NULL. */",
            "",
            "\t/* Reached the root of the rcu_node tree, acquire lock. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp_old, flags);",
            "\traw_spin_unlock(&rnp_old->fqslock);",
            "\tif (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\t\treturn;  /* Someone beat us to it. */",
            "\t}",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\trcu_gp_kthread_wake();",
            "}"
          ],
          "function_name": "rcu_sched_clock_irq, force_qs_rnp, rcu_force_quiescent_state",
          "description": "实现强制quiescent状态触发与调度器中断处理，包含优先级提升、回调加速及grace period推进等关键控制流，维护RCU状态一致性",
          "similarity": 0.5596555471420288
        },
        {
          "chunk_id": 24,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4444,
          "end_line": 4559,
          "content": [
            "static void __init",
            "rcu_boot_init_percpu_data(int cpu)",
            "{",
            "\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t/* Set up local state, ensuring consistent view of global state. */",
            "\trdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);",
            "\tINIT_WORK(&rdp->strict_work, strict_work_handler);",
            "\tWARN_ON_ONCE(ct->dynticks_nesting != 1);",
            "\tWARN_ON_ONCE(rcu_dynticks_in_eqs(rcu_dynticks_snap(cpu)));",
            "\trdp->barrier_seq_snap = rcu_state.barrier_sequence;",
            "\trdp->rcu_ofl_gp_seq = rcu_state.gp_seq;",
            "\trdp->rcu_ofl_gp_flags = RCU_GP_CLEANED;",
            "\trdp->rcu_onl_gp_seq = rcu_state.gp_seq;",
            "\trdp->rcu_onl_gp_flags = RCU_GP_CLEANED;",
            "\trdp->last_sched_clock = jiffies;",
            "\trdp->cpu = cpu;",
            "\trcu_boot_init_nocb_percpu_data(rdp);",
            "}",
            "int rcutree_prepare_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct context_tracking *ct = per_cpu_ptr(&context_tracking, cpu);",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\t/* Set up local state, ensuring consistent view of global state. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trdp->qlen_last_fqs_check = 0;",
            "\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\trdp->blimit = blimit;",
            "\tct->dynticks_nesting = 1;\t/* CPU not up, no tearing. */",
            "\traw_spin_unlock_rcu_node(rnp);\t\t/* irqs remain disabled. */",
            "",
            "\t/*",
            "\t * Only non-NOCB CPUs that didn't have early-boot callbacks need to be",
            "\t * (re-)initialized.",
            "\t */",
            "\tif (!rcu_segcblist_is_enabled(&rdp->cblist))",
            "\t\trcu_segcblist_init(&rdp->cblist);  /* Re-enable callbacks. */",
            "",
            "\t/*",
            "\t * Add CPU to leaf rcu_node pending-online bitmask.  Any needed",
            "\t * propagation up the rcu_node tree will happen at the beginning",
            "\t * of the next grace period.",
            "\t */",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_rcu_node(rnp);\t\t/* irqs already disabled. */",
            "\trdp->gp_seq = READ_ONCE(rnp->gp_seq);",
            "\trdp->gp_seq_needed = rdp->gp_seq;",
            "\trdp->cpu_no_qs.b.norm = true;",
            "\trdp->core_needs_qs = false;",
            "\trdp->rcu_iw_pending = false;",
            "\trdp->rcu_iw = IRQ_WORK_INIT_HARD(rcu_iw_handler);",
            "\trdp->rcu_iw_gp_seq = rdp->gp_seq - 1;",
            "\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuonl\"));",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "",
            "\trcu_preempt_deferred_qs_init(rdp);",
            "\trcu_spawn_one_boost_kthread(rnp);",
            "\trcu_spawn_cpu_nocb_kthread(cpu);",
            "\tWRITE_ONCE(rcu_state.n_online_cpus, rcu_state.n_online_cpus + 1);",
            "",
            "\treturn 0;",
            "}",
            "static void rcutree_affinity_setting(unsigned int cpu, int outgoing)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\trcu_boost_kthread_setaffinity(rdp->mynode, outgoing);",
            "}",
            "bool rcu_cpu_beenfullyonline(int cpu)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\treturn smp_load_acquire(&rdp->beenonline);",
            "}",
            "int rcutree_online_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trnp->ffmask |= rdp->grpmask;",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_INACTIVE)",
            "\t\treturn 0; /* Too early in boot for scheduler work. */",
            "\tsync_sched_exp_online_cleanup(cpu);",
            "\trcutree_affinity_setting(cpu, -1);",
            "",
            "\t// Stop-machine done, so allow nohz_full to disable tick.",
            "\ttick_dep_clear(TICK_DEP_BIT_RCU);",
            "\treturn 0;",
            "}",
            "int rcutree_offline_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trnp->ffmask &= ~rdp->grpmask;",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "",
            "\trcutree_affinity_setting(cpu, cpu);",
            "",
            "\t// nohz_full CPUs need the tick for stop-machine to work quickly",
            "\ttick_dep_set(TICK_DEP_BIT_RCU);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rcu_boot_init_percpu_data, rcutree_prepare_cpu, rcutree_affinity_setting, rcu_cpu_beenfullyonline, rcutree_online_cpu, rcutree_offline_cpu",
          "description": "初始化每个CPU的RCU私有数据结构，处理CPU上线/下线时的RCU状态同步，配置中断亲和性，更新全局在线CPU计数器",
          "similarity": 0.547533392906189
        }
      ]
    },
    {
      "source_file": "kernel/trace/trace_probe_tmpl.h",
      "md_summary": "> 自动生成时间: 2025-10-25 17:34:21\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `trace\\trace_probe_tmpl.h`\n\n---\n\n# `trace/trace_probe_tmpl.h` 技术文档\n\n## 1. 文件概述\n\n`trace/trace_probe_tmpl.h` 是 Linux 内核动态追踪（ftrace/kprobe/uprobe）子系统中的一个内联函数模板头文件，主要用于实现 **trace probe 参数提取（fetch）机制**。该文件提供了一系列内联函数，用于从内核或用户空间中提取、转换、存储探针（probe）触发时的参数值（如寄存器值、内存内容、字符串、符号名等），并支持动态数据（如字符串）的处理和位域操作。这些函数被 `kprobe` 和 `uprobe` 的 trace event 实现所复用，是构建动态追踪事件参数处理逻辑的核心组件。\n\n## 2. 核心功能\n\n### 主要内联函数\n\n- **`fetch_store_raw`**  \n  根据指令指定的大小（1/2/4/8 字节或默认 `unsigned long`），将原始数值写入目标缓冲区。\n\n- **`fetch_apply_bitfield`**  \n  对已存储的值应用位域操作（左移 `lshift` 后右移 `rshift`），用于提取结构体中的位字段。\n\n- **`process_common_fetch_insn`**  \n  处理第一阶段的 fetch 指令（立即数、当前进程名、数据指针）。\n\n- **`process_fetch_insn_bottom`**  \n  **核心函数**：实现完整的四阶段参数提取流程（解引用、存储、位域修改、数组循环）。\n\n- **`fetch_store_string` / `fetch_store_string_user`**  \n  从内核/用户空间读取以 null 结尾的字符串并存储。\n\n- **`fetch_store_strlen` / `fetch_store_strlen_user`**  \n  计算内核/用户空间字符串长度（不含终止符）。\n\n- **`fetch_store_symstring` / `fetch_store_symstrlen`**  \n  将地址转换为符号名（如 `func+0x10/0x100`）并存储或计算其长度。\n\n- **`__get_data_size`**  \n  遍历所有动态参数（如字符串），计算所需动态数据区总大小。\n\n- **`store_trace_args`**  \n  遍历所有参数，调用 `process_fetch_insn` 提取并存储到 trace 记录中。\n\n### 关键数据结构（隐式依赖）\n\n- **`struct fetch_insn`**  \n  fetch 指令结构体，包含操作码（`op`）、偏移量（`offset`）、大小（`size`）、立即数（`immediate`）等字段，用于描述如何提取参数。\n\n- **`struct trace_probe`**  \n  trace probe 描述符，包含参数数量（`nr_args`）、参数数组（`args`）等。\n\n- **`struct probe_arg`**  \n  单个参数描述，包含 fetch 指令链（`code`）、偏移（`offset`）、是否为动态类型（`dynamic`）等。\n\n## 3. 关键实现\n\n### 四阶段参数提取流程（`process_fetch_insn_bottom`）\n\n1. **第一阶段（由调用者处理）**：  \n   通过 `process_common_fetch_insn` 获取初始值（如寄存器值、立即数等）。\n\n2. **第二阶段（解引用）**：  \n   循环处理 `FETCH_OP_DEREF`（内核空间）或 `FETCH_OP_UDEREF`（用户空间）指令，从地址读取指针值，支持多级解引用。\n\n3. **第三阶段（存储）**：  \n   根据操作码将值存入缓冲区：\n   - `ST_RAW`：直接存储原始值\n   - `ST_MEM`/`ST_UMEM`：从内存读取固定大小数据\n   - `ST_STRING`/`ST_USTRING`/`ST_SYMSTR`：处理动态字符串（使用 data location 机制）\n\n4. **第四阶段（位域修改）**：  \n   若存在 `FETCH_OP_MOD_BF` 指令，对存储值应用位域掩码。\n\n5. **数组处理**：  \n   若存在 `FETCH_OP_LP_ARRAY` 指令，循环处理数组元素，动态更新目标地址和值地址。\n\n### 动态数据处理机制\n\n- **Data Location 机制**：  \n  动态数据（如字符串）不直接存入主记录，而是在主记录中存储一个 32 位的 **data location** 值（高 16 位为长度，低 16 位为相对于 base 的偏移）。\n- **两阶段处理**：  \n  1. **预计算阶段**（`dest == NULL`）：调用 `process_fetch_insn` 仅计算所需动态数据大小\n  2. **存储阶段**：分配动态数据区，实际存储字符串内容\n\n### 内存安全访问\n\n- 使用 `probe_mem_read` 和 `probe_mem_read_user` 安全读取内核/用户内存，避免因无效地址导致系统崩溃。\n- 所有函数标记为 `nokprobe_inline`，确保可在 kprobe 上下文中安全执行。\n\n## 4. 依赖关系\n\n- **依赖头文件**：\n  - `<linux/kernel.h>`：基础内核 API\n  - `<linux/uaccess.h>`：用户空间内存访问（`probe_mem_read_user`）\n  - `<linux/kallsyms.h>`：符号解析（`sprint_symbol`）\n  - `trace_probe.h`：`struct trace_probe`、`struct probe_arg`、`struct fetch_insn` 等定义\n  - `trace.h`：trace event 相关宏和函数\n\n- **被依赖模块**：\n  - `kernel/trace/trace_kprobe.c`：kprobe trace event 实现\n  - `kernel/trace/trace_uprobe.c`：uprobe trace event 实现\n  - 其他基于 trace probe 的动态追踪模块\n\n## 5. 使用场景\n\n- **动态追踪事件参数捕获**：  \n  当用户通过 ftrace 接口（如 `/sys/kernel/debug/tracing/kprobe_events`）定义带有参数的 kprobe/uprobe 事件时，内核使用此文件中的函数提取指定参数值。\n\n- **复杂参数类型支持**：  \n  支持提取结构体成员（通过偏移和解引用）、位域、字符串、符号地址等复杂数据类型。\n\n- **用户空间和内核空间统一处理**：  \n  通过 `*_user` 系列函数，统一处理内核探针和用户空间探针（uprobe）的参数提取逻辑。\n\n- **高效内存管理**：  \n  通过 data location 机制和两阶段处理，高效管理变长数据（如字符串）的存储，避免 trace buffer 浪费。",
      "similarity": 0.5712656378746033,
      "chunks": []
    }
  ]
}