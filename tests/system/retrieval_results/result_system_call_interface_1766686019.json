{
  "query": "system call interface",
  "timestamp": "2025-12-26 02:06:59",
  "retrieved_files": [
    {
      "source_file": "kernel/entry/syscall_user_dispatch.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:20:47\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `entry\\syscall_user_dispatch.c`\n\n---\n\n# entry/syscall_user_dispatch.c 技术文档\n\n## 1. 文件概述\n\n`entry/syscall_user_dispatch.c` 实现了 **系统调用用户分发（Syscall User Dispatch, SUD）** 机制，该机制允许用户空间程序通过 `prctl()` 系统调用配置一个“选择器”（selector），用于在特定条件下拦截或允许系统调用的执行。当系统调用指令指针位于指定区域之外且选择器状态为“阻塞”时，内核会回滚该系统调用并向进程发送 `SIGSYS` 信号，从而实现对系统调用的细粒度控制。此功能常用于沙箱、安全监控或调试场景。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `trigger_sigsys(struct pt_regs *regs)`  \n  构造并强制发送 `SIGSYS` 信号，携带被拦截系统调用的详细信息（如地址、系统调用号、架构等）。\n\n- `syscall_user_dispatch(struct pt_regs *regs)`  \n  系统调用入口处的分发判断逻辑。根据当前指令指针位置和用户选择器状态决定是否拦截系统调用。\n\n- `task_set_syscall_user_dispatch(struct task_struct *task, ...)`  \n  为指定任务设置系统调用用户分发配置（开启/关闭、偏移、长度、选择器地址）。\n\n- `set_syscall_user_dispatch(...)`  \n  为当前任务设置系统调用用户分发配置的封装接口，供 `prctl()` 调用。\n\n- `syscall_user_dispatch_get_config(...)`  \n  通过 `ptrace` 获取指定任务的 SUD 配置。\n\n- `syscall_user_dispatch_set_config(...)`  \n  通过 `ptrace` 设置指定任务的 SUD 配置。\n\n### 关键数据结构\n\n- `struct syscall_user_dispatch`（定义在 `<linux/syscall_user_dispatch.h>`）  \n  存储每个任务的 SUD 配置：\n  - `selector`：指向用户空间选择器字节的指针\n  - `offset` / `len`：允许直接执行系统调用的代码区域（[offset, offset+len)）\n  - `on_dispatch`：标志位，表示当前是否处于分发拦截状态\n\n- `struct ptrace_sud_config`  \n  用于 `ptrace` 接口传递 SUD 配置的结构体，包含 `mode`、`offset`、`len` 和 `selector`。\n\n## 3. 关键实现\n\n### 系统调用拦截逻辑\n\n1. **区域检查**：若当前指令指针（`instruction_pointer(regs)`）落在 `[offset, offset + len)` 范围内，则**允许**系统调用直接执行，不进行拦截。\n2. **vdso 例外**：若系统调用来自 vDSO 中的 `sigreturn`（如 `arch_syscall_is_vdso_sigreturn()` 返回 true），则跳过拦截，避免干扰信号返回路径。\n3. **选择器读取**：若配置了 `selector`，则从用户空间读取一个字节的状态值：\n   - `SYSCALL_DISPATCH_FILTER_ALLOW`（0）：允许系统调用\n   - `SYSCALL_DISPATCH_FILTER_BLOCK`（1）：触发拦截\n   - 其他值：视为非法，发送 `SIGSYS`\n4. **拦截处理**：\n   - 设置 `on_dispatch = true`\n   - 调用 `syscall_rollback()` 回滚系统调用（恢复寄存器状态）\n   - 调用 `trigger_sigsys()` 发送 `SIGSYS` 信号\n\n### 安全与健壮性设计\n\n- **地址合法性校验**：在设置 `selector` 时使用 `access_ok(untagged_addr(selector), ...)`，确保地址可访问，并处理内存标记（如 ARM MTE）场景下调试器（tracer）与被调试进程（tracee）地址标记不一致的问题。\n- **溢出防护**：检查 `offset + len <= offset` 防止整数溢出导致无效区域。\n- **权限隔离**：`ptrace` 接口允许调试器配置其他进程的 SUD，但需具备相应权限。\n\n### 信号信息构造\n\n`trigger_sigsys()` 构造的 `siginfo_t` 包含：\n- `si_signo = SIGSYS`\n- `si_code = SYS_USER_DISPATCH`\n- `si_call_addr`：触发系统调用的用户空间地址\n- `si_syscall`：系统调用号\n- `si_arch`：系统调用架构（如 x86_64、AArch64）\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/prctl.h>`：定义 `PR_SYS_DISPATCH_*` 常量\n  - `<linux/syscall_user_dispatch.h>`：定义 `struct syscall_user_dispatch` 和相关常量\n  - `<asm/syscall.h>`：提供 `syscall_get_arch()`、`syscall_get_nr()` 等架构相关接口\n  - `\"common.h\"`：可能包含内核入口通用辅助函数\n- **内核子系统**：\n  - **调度器（sched）**：访问 `current` 任务结构\n  - **信号子系统（signal）**：发送 `SIGSYS` 信号\n  - **内存管理（uaccess）**：用户空间内存访问（`__get_user`, `access_ok`）\n  - **ptrace**：支持调试器配置 SUD\n  - **ELF**：可能用于架构识别（间接依赖）\n\n## 5. 使用场景\n\n- **沙箱环境**：限制应用只能在特定代码段发起系统调用，防止恶意代码绕过安全策略。\n- **动态二进制插桩（DBI）**：工具（如 Valgrind、Intel Pin）可拦截系统调用进行分析或重定向。\n- **安全监控**：监控程序可配置选择器为“阻塞”，在 `SIGSYS` 信号处理程序中记录或审查系统调用。\n- **调试与测试**：通过 `ptrace` 动态启用/禁用 SUD，用于测试系统调用拦截逻辑。\n- **W^X 策略增强**：结合代码段只读与 SUD，确保只有可信代码路径可发起系统调用。",
      "similarity": 0.5793018341064453,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/entry/syscall_user_dispatch.c",
          "start_line": 20,
          "end_line": 122,
          "content": [
            "static void trigger_sigsys(struct pt_regs *regs)",
            "{",
            "\tstruct kernel_siginfo info;",
            "",
            "\tclear_siginfo(&info);",
            "\tinfo.si_signo = SIGSYS;",
            "\tinfo.si_code = SYS_USER_DISPATCH;",
            "\tinfo.si_call_addr = (void __user *)KSTK_EIP(current);",
            "\tinfo.si_errno = 0;",
            "\tinfo.si_arch = syscall_get_arch(current);",
            "\tinfo.si_syscall = syscall_get_nr(current, regs);",
            "",
            "\tforce_sig_info(&info);",
            "}",
            "bool syscall_user_dispatch(struct pt_regs *regs)",
            "{",
            "\tstruct syscall_user_dispatch *sd = &current->syscall_dispatch;",
            "\tchar state;",
            "",
            "\tif (likely(instruction_pointer(regs) - sd->offset < sd->len))",
            "\t\treturn false;",
            "",
            "\tif (unlikely(arch_syscall_is_vdso_sigreturn(regs)))",
            "\t\treturn false;",
            "",
            "\tif (likely(sd->selector)) {",
            "\t\t/*",
            "\t\t * access_ok() is performed once, at prctl time, when",
            "\t\t * the selector is loaded by userspace.",
            "\t\t */",
            "\t\tif (unlikely(__get_user(state, sd->selector))) {",
            "\t\t\tforce_exit_sig(SIGSEGV);",
            "\t\t\treturn true;",
            "\t\t}",
            "",
            "\t\tif (likely(state == SYSCALL_DISPATCH_FILTER_ALLOW))",
            "\t\t\treturn false;",
            "",
            "\t\tif (state != SYSCALL_DISPATCH_FILTER_BLOCK) {",
            "\t\t\tforce_exit_sig(SIGSYS);",
            "\t\t\treturn true;",
            "\t\t}",
            "\t}",
            "",
            "\tsd->on_dispatch = true;",
            "\tsyscall_rollback(current, regs);",
            "\ttrigger_sigsys(regs);",
            "",
            "\treturn true;",
            "}",
            "static int task_set_syscall_user_dispatch(struct task_struct *task, unsigned long mode,",
            "\t\t\t\t\t  unsigned long offset, unsigned long len,",
            "\t\t\t\t\t  char __user *selector)",
            "{",
            "\tswitch (mode) {",
            "\tcase PR_SYS_DISPATCH_OFF:",
            "\t\tif (offset || len || selector)",
            "\t\t\treturn -EINVAL;",
            "\t\tbreak;",
            "\tcase PR_SYS_DISPATCH_ON:",
            "\t\t/*",
            "\t\t * Validate the direct dispatcher region just for basic",
            "\t\t * sanity against overflow and a 0-sized dispatcher",
            "\t\t * region.  If the user is able to submit a syscall from",
            "\t\t * an address, that address is obviously valid.",
            "\t\t */",
            "\t\tif (offset && offset + len <= offset)",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\t/*",
            "\t\t * access_ok() will clear memory tags for tagged addresses",
            "\t\t * if current has memory tagging enabled.",
            "",
            "\t\t * To enable a tracer to set a tracees selector the",
            "\t\t * selector address must be untagged for access_ok(),",
            "\t\t * otherwise an untagged tracer will always fail to set a",
            "\t\t * tagged tracees selector.",
            "\t\t */",
            "\t\tif (selector && !access_ok(untagged_addr(selector), sizeof(*selector)))",
            "\t\t\treturn -EFAULT;",
            "",
            "\t\tbreak;",
            "\tdefault:",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\ttask->syscall_dispatch.selector = selector;",
            "\ttask->syscall_dispatch.offset = offset;",
            "\ttask->syscall_dispatch.len = len;",
            "\ttask->syscall_dispatch.on_dispatch = false;",
            "",
            "\tif (mode == PR_SYS_DISPATCH_ON)",
            "\t\tset_task_syscall_work(task, SYSCALL_USER_DISPATCH);",
            "\telse",
            "\t\tclear_task_syscall_work(task, SYSCALL_USER_DISPATCH);",
            "",
            "\treturn 0;",
            "}",
            "int set_syscall_user_dispatch(unsigned long mode, unsigned long offset,",
            "\t\t\t      unsigned long len, char __user *selector)",
            "{",
            "\treturn task_set_syscall_user_dispatch(current, mode, offset, len, selector);",
            "}"
          ],
          "function_name": "trigger_sigsys, syscall_user_dispatch, task_set_syscall_user_dispatch, set_syscall_user_dispatch",
          "description": "实现系统调用用户分发核心逻辑，包含触发SIGSYS信号处理、配置验证、拦截判断及模式切换功能",
          "similarity": 0.5694460868835449
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/entry/syscall_user_dispatch.c",
          "start_line": 127,
          "end_line": 163,
          "content": [
            "int syscall_user_dispatch_get_config(struct task_struct *task, unsigned long size,",
            "\t\t\t\t     void __user *data)",
            "{",
            "\tstruct syscall_user_dispatch *sd = &task->syscall_dispatch;",
            "\tstruct ptrace_sud_config cfg;",
            "",
            "\tif (size != sizeof(cfg))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (test_task_syscall_work(task, SYSCALL_USER_DISPATCH))",
            "\t\tcfg.mode = PR_SYS_DISPATCH_ON;",
            "\telse",
            "\t\tcfg.mode = PR_SYS_DISPATCH_OFF;",
            "",
            "\tcfg.offset = sd->offset;",
            "\tcfg.len = sd->len;",
            "\tcfg.selector = (__u64)(uintptr_t)sd->selector;",
            "",
            "\tif (copy_to_user(data, &cfg, sizeof(cfg)))",
            "\t\treturn -EFAULT;",
            "",
            "\treturn 0;",
            "}",
            "int syscall_user_dispatch_set_config(struct task_struct *task, unsigned long size,",
            "\t\t\t\t     void __user *data)",
            "{",
            "\tstruct ptrace_sud_config cfg;",
            "",
            "\tif (size != sizeof(cfg))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (copy_from_user(&cfg, data, sizeof(cfg)))",
            "\t\treturn -EFAULT;",
            "",
            "\treturn task_set_syscall_user_dispatch(task, cfg.mode, cfg.offset, cfg.len,",
            "\t\t\t\t\t      (char __user *)(uintptr_t)cfg.selector);",
            "}"
          ],
          "function_name": "syscall_user_dispatch_get_config, syscall_user_dispatch_set_config",
          "description": "提供系统调用分发配置的获取与设置接口，通过用户态指针操作实现配置参数的双向传递",
          "similarity": 0.5478811264038086
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/entry/syscall_user_dispatch.c",
          "start_line": 1,
          "end_line": 19,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 2020 Collabora Ltd.",
            " */",
            "#include <linux/sched.h>",
            "#include <linux/prctl.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/syscall_user_dispatch.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/signal.h>",
            "#include <linux/elf.h>",
            "",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task_stack.h>",
            "",
            "#include <asm/syscall.h>",
            "",
            "#include \"common.h\"",
            ""
          ],
          "function_name": null,
          "description": "包含系统调用用户分发功能所需头文件及通用定义，提供架构相关接口和内核调度必要声明",
          "similarity": 0.49903005361557007
        }
      ]
    },
    {
      "source_file": "kernel/sched/membarrier.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:12:44\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\membarrier.c`\n\n---\n\n# `sched/membarrier.c` 技术文档\n\n## 1. 文件概述\n\n`sched/membarrier.c` 实现了 Linux 内核中的 `membarrier` 系统调用，该调用为用户空间程序提供了一种高效的全局内存屏障机制。与传统的在每个线程中显式插入内存屏障相比，`membarrier` 允许一个线程通过一次系统调用，强制所有运行在系统上的线程（或特定进程组内的线程）执行内存屏障操作，从而简化用户空间并发同步逻辑并提升性能。\n\n该文件的核心目标是在多核系统中确保内存操作的全局可见性顺序，尤其适用于需要跨线程强内存顺序保证的用户空间同步原语（如 RCU、无锁数据结构等）。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`ipi_mb(void *info)`**  \n  IPI（处理器间中断）处理函数，执行 `smp_mb()` 内存屏障，用于基础的全局内存屏障命令。\n\n- **`ipi_sync_core(void *info)`**  \n  用于 `MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE` 命令的 IPI 处理函数，在执行内存屏障后调用 `sync_core_before_usermode()`，确保 CPU 核心状态同步（如指令缓存一致性）。\n\n- **`ipi_rseq(void *info)`**  \n  用于 `MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ` 命令的 IPI 处理函数，在内存屏障后调用 `rseq_preempt(current)`，以支持 restartable sequences（rseq）机制的正确性。\n\n- **`ipi_sync_rq_state(void *info)`**  \n  用于同步 per-CPU runqueue 的 `membarrier_state` 字段，使其与指定 `mm_struct` 的状态一致，确保后续 `membarrier` 调用能正确识别注册状态。\n\n- **`membarrier_exec_mmap(struct mm_struct *mm)`**  \n  在进程执行 `exec` 系统调用时被调用，重置该内存描述符（`mm_struct`）的 `membarrier_state` 为 0，并同步 per-CPU runqueue 状态，防止 exec 后残留旧的注册状态。\n\n### 数据结构与宏\n\n- **`MEMBARRIER_CMD_BITMASK`**  \n  定义所有支持的 `membarrier` 命令的位掩码（不含 `QUERY`），用于命令合法性校验。\n\n- **`membarrier_ipi_mutex`**  \n  互斥锁，用于序列化 IPI 发送过程，防止多个 `membarrier` 调用并发执行导致 IPI 风暴或状态不一致。\n\n- **`SERIALIZE_IPI()`**  \n  宏封装，使用 `membarrier_ipi_mutex` 实现 IPI 发送的串行化。\n\n## 3. 关键实现\n\n### 内存屏障语义保证\n\n文件顶部的注释详细描述了五种关键内存顺序场景（A–E），说明为何在 `membarrier()` 调用前后必须插入 `smp_mb()`：\n\n- **场景 A**：确保调用者 CPU 在 `membarrier()` 之前的写操作，在其他 CPU 收到 IPI 并执行屏障后对其可见。\n- **场景 B**：确保其他 CPU 在 IPI 屏障前的写操作，在调用者 CPU 执行 `membarrier()` 后对其可见。\n- **场景 C–E**：处理线程切换、`exit_mm`、kthread 使用/释放 mm 等边界情况，确保 `membarrier` 能正确识别用户态上下文并施加屏障。\n\n这些场景共同要求 `membarrier()` 实现必须在发送 IPI **前**和**后**各执行一次 `smp_mb()`，以建立完整的全局内存顺序。\n\n### IPI 分发机制\n\n- 根据不同的 `membarrier` 命令类型（如全局、私有、带 rseq 或 sync_core），选择对应的 IPI 处理函数。\n- 使用 `mutex` 保护 IPI 发送过程，避免并发调用导致性能下降或状态竞争。\n- 对于私有命令（如 `PRIVATE_EXPEDITED`），仅向共享同一 `mm_struct` 的 CPU 发送 IPI。\n\n### 状态管理\n\n- 每个 `mm_struct` 包含一个 `membarrier_state` 原子变量，记录该地址空间已注册的 `membarrier` 命令类型。\n- 每个 per-CPU runqueue 也缓存一份 `membarrier_state`，通过 `ipi_sync_rq_state` 保持与 `mm_struct` 同步，加速后续命令的判断。\n- `exec` 时调用 `membarrier_exec_mmap` 重置状态，防止子进程继承父进程的注册状态。\n\n### 条件编译支持\n\n- `CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE`：启用 `SYNC_CORE` 相关命令。\n- `CONFIG_RSEQ`：启用 `RSEQ` 相关命令及 `rseq_preempt` 调用。\n\n## 4. 依赖关系\n\n- **调度子系统（sched）**：依赖 runqueue（`rq`）结构和 CPU 上下文切换逻辑，用于判断当前是否处于用户态及 mm 匹配。\n- **内存管理（mm）**：依赖 `mm_struct` 及其生命周期管理（如 `exec_mmap`、`exit_mm`）。\n- **RSEQ 子系统**：当启用 `CONFIG_RSEQ` 时，调用 `rseq_preempt()` 以维护 restartable sequences 的一致性。\n- **SMP 原语**：依赖 `smp_mb()`、`smp_call_function_many()` 等 SMP 内存屏障和 IPI 接口。\n- **架构支持**：部分命令（如 `SYNC_CORE`）依赖特定架构实现 `sync_core_before_usermode()`。\n\n## 5. 使用场景\n\n- **用户空间无锁编程**：应用程序使用 `membarrier(SYS_MEMBARRIER_CMD_GLOBAL_EXPEDITED)` 替代在每个读线程中插入 `smp_load_acquire()`，简化代码并提升性能。\n- **RSEQ（Restartable Sequences）**：配合 `membarrier(SYS_MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ)` 确保在抢占或迁移后 rseq 区域的原子性。\n- **实时或低延迟系统**：通过私有命令（`PRIVATE_EXPEDITED`）仅对特定进程组施加屏障，减少系统范围开销。\n- **动态代码生成/热更新**：使用 `SYNC_CORE` 命令确保指令缓存一致性，适用于 JIT 编译器等场景。\n- **进程生命周期管理**：在 `exec` 时自动清理 `membarrier` 注册状态，保证新程序映像的干净执行环境。",
      "similarity": 0.556275486946106,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 168,
          "end_line": 306,
          "content": [
            "static void ipi_mb(void *info)",
            "{",
            "\tsmp_mb();\t/* IPIs should be serializing but paranoid. */",
            "}",
            "static void ipi_sync_core(void *info)",
            "{",
            "\t/*",
            "\t * The smp_mb() in membarrier after all the IPIs is supposed to",
            "\t * ensure that memory on remote CPUs that occur before the IPI",
            "\t * become visible to membarrier()'s caller -- see scenario B in",
            "\t * the big comment at the top of this file.",
            "\t *",
            "\t * A sync_core() would provide this guarantee, but",
            "\t * sync_core_before_usermode() might end up being deferred until",
            "\t * after membarrier()'s smp_mb().",
            "\t */",
            "\tsmp_mb();\t/* IPIs should be serializing but paranoid. */",
            "",
            "\tsync_core_before_usermode();",
            "}",
            "static void ipi_rseq(void *info)",
            "{",
            "\t/*",
            "\t * Ensure that all stores done by the calling thread are visible",
            "\t * to the current task before the current task resumes.  We could",
            "\t * probably optimize this away on most architectures, but by the",
            "\t * time we've already sent an IPI, the cost of the extra smp_mb()",
            "\t * is negligible.",
            "\t */",
            "\tsmp_mb();",
            "\trseq_preempt(current);",
            "}",
            "static void ipi_sync_rq_state(void *info)",
            "{",
            "\tstruct mm_struct *mm = (struct mm_struct *) info;",
            "",
            "\tif (current->mm != mm)",
            "\t\treturn;",
            "\tthis_cpu_write(runqueues.membarrier_state,",
            "\t\t       atomic_read(&mm->membarrier_state));",
            "\t/*",
            "\t * Issue a memory barrier after setting",
            "\t * MEMBARRIER_STATE_GLOBAL_EXPEDITED in the current runqueue to",
            "\t * guarantee that no memory access following registration is reordered",
            "\t * before registration.",
            "\t */",
            "\tsmp_mb();",
            "}",
            "void membarrier_exec_mmap(struct mm_struct *mm)",
            "{",
            "\t/*",
            "\t * Issue a memory barrier before clearing membarrier_state to",
            "\t * guarantee that no memory access prior to exec is reordered after",
            "\t * clearing this state.",
            "\t */",
            "\tsmp_mb();",
            "\tatomic_set(&mm->membarrier_state, 0);",
            "\t/*",
            "\t * Keep the runqueue membarrier_state in sync with this mm",
            "\t * membarrier_state.",
            "\t */",
            "\tthis_cpu_write(runqueues.membarrier_state, 0);",
            "}",
            "void membarrier_update_current_mm(struct mm_struct *next_mm)",
            "{",
            "\tstruct rq *rq = this_rq();",
            "\tint membarrier_state = 0;",
            "",
            "\tif (next_mm)",
            "\t\tmembarrier_state = atomic_read(&next_mm->membarrier_state);",
            "\tif (READ_ONCE(rq->membarrier_state) == membarrier_state)",
            "\t\treturn;",
            "\tWRITE_ONCE(rq->membarrier_state, membarrier_state);",
            "}",
            "static int membarrier_global_expedited(void)",
            "{",
            "\tint cpu;",
            "\tcpumask_var_t tmpmask;",
            "",
            "\tif (num_online_cpus() == 1)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Matches memory barriers after rq->curr modification in",
            "\t * scheduler.",
            "\t */",
            "\tsmp_mb();\t/* system call entry is not a mb. */",
            "",
            "\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "\trcu_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\t/*",
            "\t\t * Skipping the current CPU is OK even through we can be",
            "\t\t * migrated at any point. The current CPU, at the point",
            "\t\t * where we read raw_smp_processor_id(), is ensured to",
            "\t\t * be in program order with respect to the caller",
            "\t\t * thread. Therefore, we can skip this CPU from the",
            "\t\t * iteration.",
            "\t\t */",
            "\t\tif (cpu == raw_smp_processor_id())",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!(READ_ONCE(cpu_rq(cpu)->membarrier_state) &",
            "\t\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * Skip the CPU if it runs a kernel thread which is not using",
            "\t\t * a task mm.",
            "\t\t */",
            "\t\tp = rcu_dereference(cpu_rq(cpu)->curr);",
            "\t\tif (!p->mm)",
            "\t\t\tcontinue;",
            "",
            "\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tpreempt_disable();",
            "\tsmp_call_function_many(tmpmask, ipi_mb, NULL, 1);",
            "\tpreempt_enable();",
            "",
            "\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\t/*",
            "\t * Memory barrier on the caller thread _after_ we finished",
            "\t * waiting for the last IPI. Matches memory barriers before",
            "\t * rq->curr modification in scheduler.",
            "\t */",
            "\tsmp_mb();\t/* exit from system call is not a mb */",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ipi_mb, ipi_sync_core, ipi_rseq, ipi_sync_rq_state, membarrier_exec_mmap, membarrier_update_current_mm, membarrier_global_expedited",
          "description": "实现多种IPI处理函数及全局快速内存屏障逻辑，通过跨CPU调用来确保内存访问顺序一致性。",
          "similarity": 0.5013725757598877
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 436,
          "end_line": 551,
          "content": [
            "static int sync_runqueues_membarrier_state(struct mm_struct *mm)",
            "{",
            "\tint membarrier_state = atomic_read(&mm->membarrier_state);",
            "\tcpumask_var_t tmpmask;",
            "\tint cpu;",
            "",
            "\tif (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1) {",
            "\t\tthis_cpu_write(runqueues.membarrier_state, membarrier_state);",
            "",
            "\t\t/*",
            "\t\t * For single mm user, we can simply issue a memory barrier",
            "\t\t * after setting MEMBARRIER_STATE_GLOBAL_EXPEDITED in the",
            "\t\t * mm and in the current runqueue to guarantee that no memory",
            "\t\t * access following registration is reordered before",
            "\t\t * registration.",
            "\t\t */",
            "\t\tsmp_mb();",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * For mm with multiple users, we need to ensure all future",
            "\t * scheduler executions will observe @mm's new membarrier",
            "\t * state.",
            "\t */",
            "\tsynchronize_rcu();",
            "",
            "\t/*",
            "\t * For each cpu runqueue, if the task's mm match @mm, ensure that all",
            "\t * @mm's membarrier state set bits are also set in the runqueue's",
            "\t * membarrier state. This ensures that a runqueue scheduling",
            "\t * between threads which are users of @mm has its membarrier state",
            "\t * updated.",
            "\t */",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "\trcu_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct rq *rq = cpu_rq(cpu);",
            "\t\tstruct task_struct *p;",
            "",
            "\t\tp = rcu_dereference(rq->curr);",
            "\t\tif (p && p->mm == mm)",
            "\t\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\ton_each_cpu_mask(tmpmask, ipi_sync_rq_state, mm, true);",
            "",
            "\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\treturn 0;",
            "}",
            "static int membarrier_register_global_expedited(void)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint ret;",
            "",
            "\tif (atomic_read(&mm->membarrier_state) &",
            "\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)",
            "\t\treturn 0;",
            "\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);",
            "\tret = sync_runqueues_membarrier_state(mm);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,",
            "\t\t  &mm->membarrier_state);",
            "",
            "\treturn 0;",
            "}",
            "static int membarrier_register_private_expedited(int flags)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint ready_state = MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY,",
            "\t    set_state = MEMBARRIER_STATE_PRIVATE_EXPEDITED,",
            "\t    ret;",
            "",
            "\tif (flags == MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))",
            "\t\t\treturn -EINVAL;",
            "\t\tready_state =",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY;",
            "\t} else if (flags == MEMBARRIER_FLAG_RSEQ) {",
            "\t\tif (!IS_ENABLED(CONFIG_RSEQ))",
            "\t\t\treturn -EINVAL;",
            "\t\tready_state =",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY;",
            "\t} else {",
            "\t\tWARN_ON_ONCE(flags);",
            "\t}",
            "",
            "\t/*",
            "\t * We need to consider threads belonging to different thread",
            "\t * groups, which use the same mm. (CLONE_VM but not",
            "\t * CLONE_THREAD).",
            "\t */",
            "\tif ((atomic_read(&mm->membarrier_state) & ready_state) == ready_state)",
            "\t\treturn 0;",
            "\tif (flags & MEMBARRIER_FLAG_SYNC_CORE)",
            "\t\tset_state |= MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE;",
            "\tif (flags & MEMBARRIER_FLAG_RSEQ)",
            "\t\tset_state |= MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ;",
            "\tatomic_or(set_state, &mm->membarrier_state);",
            "\tret = sync_runqueues_membarrier_state(mm);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tatomic_or(ready_state, &mm->membarrier_state);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sync_runqueues_membarrier_state, membarrier_register_global_expedited, membarrier_register_private_expedited",
          "description": "同步运行队列内存屏障状态，通过RCU和IPI确保多用户场景下状态传播的正确性。",
          "similarity": 0.49338728189468384
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 314,
          "end_line": 434,
          "content": [
            "static int membarrier_private_expedited(int flags, int cpu_id)",
            "{",
            "\tcpumask_var_t tmpmask;",
            "\tstruct mm_struct *mm = current->mm;",
            "\tsmp_call_func_t ipi_func = ipi_mb;",
            "",
            "\tif (flags == MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY))",
            "\t\t\treturn -EPERM;",
            "\t\tipi_func = ipi_sync_core;",
            "\t\tprepare_sync_core_cmd(mm);",
            "\t} else if (flags == MEMBARRIER_FLAG_RSEQ) {",
            "\t\tif (!IS_ENABLED(CONFIG_RSEQ))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY))",
            "\t\t\treturn -EPERM;",
            "\t\tipi_func = ipi_rseq;",
            "\t} else {",
            "\t\tWARN_ON_ONCE(flags);",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY))",
            "\t\t\treturn -EPERM;",
            "\t}",
            "",
            "\tif (flags != MEMBARRIER_FLAG_SYNC_CORE &&",
            "\t    (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Matches memory barriers after rq->curr modification in",
            "\t * scheduler.",
            "\t *",
            "\t * On RISC-V, this barrier pairing is also needed for the",
            "\t * SYNC_CORE command when switching between processes, cf.",
            "\t * the inline comments in membarrier_arch_switch_mm().",
            "\t */",
            "\tsmp_mb();\t/* system call entry is not a mb. */",
            "",
            "\tif (cpu_id < 0 && !zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "",
            "\tif (cpu_id >= 0) {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\tif (cpu_id >= nr_cpu_ids || !cpu_online(cpu_id))",
            "\t\t\tgoto out;",
            "\t\trcu_read_lock();",
            "\t\tp = rcu_dereference(cpu_rq(cpu_id)->curr);",
            "\t\tif (!p || p->mm != mm) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tint cpu;",
            "",
            "\t\trcu_read_lock();",
            "\t\tfor_each_online_cpu(cpu) {",
            "\t\t\tstruct task_struct *p;",
            "",
            "\t\t\tp = rcu_dereference(cpu_rq(cpu)->curr);",
            "\t\t\tif (p && p->mm == mm)",
            "\t\t\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t}",
            "",
            "\tif (cpu_id >= 0) {",
            "\t\t/*",
            "\t\t * smp_call_function_single() will call ipi_func() if cpu_id",
            "\t\t * is the calling CPU.",
            "\t\t */",
            "\t\tsmp_call_function_single(cpu_id, ipi_func, NULL, 1);",
            "\t} else {",
            "\t\t/*",
            "\t\t * For regular membarrier, we can save a few cycles by",
            "\t\t * skipping the current cpu -- we're about to do smp_mb()",
            "\t\t * below, and if we migrate to a different cpu, this cpu",
            "\t\t * and the new cpu will execute a full barrier in the",
            "\t\t * scheduler.",
            "\t\t *",
            "\t\t * For SYNC_CORE, we do need a barrier on the current cpu --",
            "\t\t * otherwise, if we are migrated and replaced by a different",
            "\t\t * task in the same mm just before, during, or after",
            "\t\t * membarrier, we will end up with some thread in the mm",
            "\t\t * running without a core sync.",
            "\t\t *",
            "\t\t * For RSEQ, don't rseq_preempt() the caller.  User code",
            "\t\t * is not supposed to issue syscalls at all from inside an",
            "\t\t * rseq critical section.",
            "\t\t */",
            "\t\tif (flags != MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\t\tpreempt_disable();",
            "\t\t\tsmp_call_function_many(tmpmask, ipi_func, NULL, true);",
            "\t\t\tpreempt_enable();",
            "\t\t} else {",
            "\t\t\ton_each_cpu_mask(tmpmask, ipi_func, NULL, true);",
            "\t\t}",
            "\t}",
            "",
            "out:",
            "\tif (cpu_id < 0)",
            "\t\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\t/*",
            "\t * Memory barrier on the caller thread _after_ we finished",
            "\t * waiting for the last IPI. Matches memory barriers before",
            "\t * rq->curr modification in scheduler.",
            "\t */",
            "\tsmp_mb();\t/* exit from system call is not a mb */",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "membarrier_private_expedited",
          "description": "处理私有快速内存屏障，根据配置标志选择不同同步方式并验证状态有效性。",
          "similarity": 0.46782001852989197
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 555,
          "end_line": 587,
          "content": [
            "static int membarrier_get_registrations(void)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint registrations_mask = 0, membarrier_state, i;",
            "\tstatic const int states[] = {",
            "\t\tMEMBARRIER_STATE_GLOBAL_EXPEDITED |",
            "\t\t\tMEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY",
            "\t};",
            "\tstatic const int registration_cmds[] = {",
            "\t\tMEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ",
            "\t};",
            "\tBUILD_BUG_ON(ARRAY_SIZE(states) != ARRAY_SIZE(registration_cmds));",
            "",
            "\tmembarrier_state = atomic_read(&mm->membarrier_state);",
            "\tfor (i = 0; i < ARRAY_SIZE(states); ++i) {",
            "\t\tif (membarrier_state & states[i]) {",
            "\t\t\tregistrations_mask |= registration_cmds[i];",
            "\t\t\tmembarrier_state &= ~states[i];",
            "\t\t}",
            "\t}",
            "\tWARN_ON_ONCE(membarrier_state != 0);",
            "\treturn registrations_mask;",
            "}"
          ],
          "function_name": "membarrier_get_registrations",
          "description": "解析当前进程的内存屏障注册状态，将有效状态转换为对应的注册命令掩码。",
          "similarity": 0.43756335973739624
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 1,
          "end_line": 167,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Copyright (C) 2010-2017 Mathieu Desnoyers <mathieu.desnoyers@efficios.com>",
            " *",
            " * membarrier system call",
            " */",
            "",
            "/*",
            " * For documentation purposes, here are some membarrier ordering",
            " * scenarios to keep in mind:",
            " *",
            " * A) Userspace thread execution after IPI vs membarrier's memory",
            " *    barrier before sending the IPI",
            " *",
            " * Userspace variables:",
            " *",
            " * int x = 0, y = 0;",
            " *",
            " * The memory barrier at the start of membarrier() on CPU0 is necessary in",
            " * order to enforce the guarantee that any writes occurring on CPU0 before",
            " * the membarrier() is executed will be visible to any code executing on",
            " * CPU1 after the IPI-induced memory barrier:",
            " *",
            " *         CPU0                              CPU1",
            " *",
            " *         x = 1",
            " *         membarrier():",
            " *           a: smp_mb()",
            " *           b: send IPI                       IPI-induced mb",
            " *           c: smp_mb()",
            " *         r2 = y",
            " *                                           y = 1",
            " *                                           barrier()",
            " *                                           r1 = x",
            " *",
            " *                     BUG_ON(r1 == 0 && r2 == 0)",
            " *",
            " * The write to y and load from x by CPU1 are unordered by the hardware,",
            " * so it's possible to have \"r1 = x\" reordered before \"y = 1\" at any",
            " * point after (b).  If the memory barrier at (a) is omitted, then \"x = 1\"",
            " * can be reordered after (a) (although not after (c)), so we get r1 == 0",
            " * and r2 == 0.  This violates the guarantee that membarrier() is",
            " * supposed by provide.",
            " *",
            " * The timing of the memory barrier at (a) has to ensure that it executes",
            " * before the IPI-induced memory barrier on CPU1.",
            " *",
            " * B) Userspace thread execution before IPI vs membarrier's memory",
            " *    barrier after completing the IPI",
            " *",
            " * Userspace variables:",
            " *",
            " * int x = 0, y = 0;",
            " *",
            " * The memory barrier at the end of membarrier() on CPU0 is necessary in",
            " * order to enforce the guarantee that any writes occurring on CPU1 before",
            " * the membarrier() is executed will be visible to any code executing on",
            " * CPU0 after the membarrier():",
            " *",
            " *         CPU0                              CPU1",
            " *",
            " *                                           x = 1",
            " *                                           barrier()",
            " *                                           y = 1",
            " *         r2 = y",
            " *         membarrier():",
            " *           a: smp_mb()",
            " *           b: send IPI                       IPI-induced mb",
            " *           c: smp_mb()",
            " *         r1 = x",
            " *         BUG_ON(r1 == 0 && r2 == 1)",
            " *",
            " * The writes to x and y are unordered by the hardware, so it's possible to",
            " * have \"r2 = 1\" even though the write to x doesn't execute until (b).  If",
            " * the memory barrier at (c) is omitted then \"r1 = x\" can be reordered",
            " * before (b) (although not before (a)), so we get \"r1 = 0\".  This violates",
            " * the guarantee that membarrier() is supposed to provide.",
            " *",
            " * The timing of the memory barrier at (c) has to ensure that it executes",
            " * after the IPI-induced memory barrier on CPU1.",
            " *",
            " * C) Scheduling userspace thread -> kthread -> userspace thread vs membarrier",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *           a: smp_mb()",
            " *                                           d: switch to kthread (includes mb)",
            " *           b: read rq->curr->mm == NULL",
            " *                                           e: switch to user (includes mb)",
            " *           c: smp_mb()",
            " *",
            " * Using the scenario from (A), we can show that (a) needs to be paired",
            " * with (e). Using the scenario from (B), we can show that (c) needs to",
            " * be paired with (d).",
            " *",
            " * D) exit_mm vs membarrier",
            " *",
            " * Two thread groups are created, A and B.  Thread group B is created by",
            " * issuing clone from group A with flag CLONE_VM set, but not CLONE_THREAD.",
            " * Let's assume we have a single thread within each thread group (Thread A",
            " * and Thread B).  Thread A runs on CPU0, Thread B runs on CPU1.",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *             a: smp_mb()",
            " *                                           exit_mm():",
            " *                                             d: smp_mb()",
            " *                                             e: current->mm = NULL",
            " *             b: read rq->curr->mm == NULL",
            " *             c: smp_mb()",
            " *",
            " * Using scenario (B), we can show that (c) needs to be paired with (d).",
            " *",
            " * E) kthread_{use,unuse}_mm vs membarrier",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *           a: smp_mb()",
            " *                                           kthread_unuse_mm()",
            " *                                             d: smp_mb()",
            " *                                             e: current->mm = NULL",
            " *           b: read rq->curr->mm == NULL",
            " *                                           kthread_use_mm()",
            " *                                             f: current->mm = mm",
            " *                                             g: smp_mb()",
            " *           c: smp_mb()",
            " *",
            " * Using the scenario from (A), we can show that (a) needs to be paired",
            " * with (g). Using the scenario from (B), we can show that (c) needs to",
            " * be paired with (d).",
            " */",
            "",
            "/*",
            " * Bitmask made from a \"or\" of all commands within enum membarrier_cmd,",
            " * except MEMBARRIER_CMD_QUERY.",
            " */",
            "#ifdef CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t\t\t\\",
            "\t(MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE)",
            "#else",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t0",
            "#endif",
            "",
            "#ifdef CONFIG_RSEQ",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t\t\\",
            "\t(MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ)",
            "#else",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t0",
            "#endif",
            "",
            "#define MEMBARRIER_CMD_BITMASK\t\t\t\t\t\t\\",
            "\t(MEMBARRIER_CMD_GLOBAL | MEMBARRIER_CMD_GLOBAL_EXPEDITED\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED\t\t\t\\",
            "\t| MEMBARRIER_CMD_PRIVATE_EXPEDITED\t\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED\t\t\t\\",
            "\t| MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t\t\\",
            "\t| MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t\t\t\\",
            "\t| MEMBARRIER_CMD_GET_REGISTRATIONS)",
            "",
            "static DEFINE_MUTEX(membarrier_ipi_mutex);",
            "#define SERIALIZE_IPI() guard(mutex)(&membarrier_ipi_mutex)",
            ""
          ],
          "function_name": null,
          "description": "定义内存屏障命令位掩码和互斥锁，用于协调多处理器间内存顺序保证。",
          "similarity": 0.3891761898994446
        }
      ]
    },
    {
      "source_file": "kernel/futex/syscalls.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:35:43\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `futex\\syscalls.c`\n\n---\n\n# futex/syscalls.c 技术文档\n\n## 1. 文件概述\n\n`futex/syscalls.c` 是 Linux 内核中实现 futex（Fast Userspace muTEX）系统调用的核心文件之一。该文件主要负责处理与 futex 相关的用户空间系统调用接口，包括基础 futex 操作、健壮 futex（robust futex）支持以及多 futex 等待（`futex_waitv`）功能。futex 是一种高效的用户态同步原语，内核仅在必要时（如竞争或阻塞）介入，从而在无竞争场景下实现零内核开销。\n\n## 2. 核心功能\n\n### 主要系统调用函数：\n\n- `sys_set_robust_list()`：为当前任务设置健壮 futex 列表头指针。\n- `sys_get_robust_list()`：获取指定任务（或当前任务）的健壮 futex 列表头指针。\n- `sys_futex()`：主 futex 系统调用，支持多种操作（如 `FUTEX_WAIT`、`FUTEX_WAKE`、`FUTEX_LOCK_PI` 等）。\n- `sys_futex_waitv()`：等待多个 futex 中任意一个被唤醒（尚未完整实现，仅声明）。\n\n### 辅助函数：\n\n- `do_futex()`：统一调度所有 futex 操作的核心分发函数。\n- `futex_cmd_has_timeout()`：判断指定 futex 命令是否支持超时。\n- `futex_init_timeout()`：解析并转换用户提供的超时时间（支持 `CLOCK_REALTIME` 和 `CLOCK_MONOTONIC`）。\n- `futex_parse_waitv()`：从用户空间解析 `futex_waitv` 数组，验证并初始化内核侧等待结构。\n- `futex2_setup_timeout()` / `futex2_destroy_timeout()`：为 `futex_waitv` 设置和销毁高精度定时器。\n\n### 关键数据结构（引用）：\n\n- `struct robust_list_head`：健壮 futex 列表头，由用户空间维护。\n- `struct futex_waitv`：用于 `futex_waitv` 的用户空间输入结构，包含地址、期望值和标志。\n- `struct futex_vector`：内核侧表示多个 futex 等待项的结构。\n\n## 3. 关键实现\n\n### 健壮 Futex（Robust Futex）机制\n- 用户空间为每个线程维护一个持有锁的链表（`robust_list`）。\n- 当线程异常退出时，内核遍历该链表，将所有属于该线程的 futex 标记为 `FUTEX_OWNER_DIED` 并唤醒等待者。\n- 通过 `list_op_pending` 字段处理“已加锁但尚未加入链表”的临界状态，确保清理完整性。\n- `set_robust_list` / `get_robust_list` 系统调用用于注册和查询该链表。\n\n### Futex 操作分发\n- `do_futex()` 根据操作码（`op & FUTEX_CMD_MASK`）分发到具体实现函数（如 `futex_wait`、`futex_wake`、`futex_lock_pi` 等）。\n- 支持多种 futex 类型：普通 futex、PI（优先级继承）futex、bitset futex、requeue 操作等。\n- 对于带超时的操作（如 `FUTEX_WAIT`），根据是否使用 `FUTEX_CLOCK_REALTIME` 标志决定使用绝对时间还是相对时间，并进行时间命名空间转换（`timens_ktime_to_host`）。\n\n### 多 Futex 等待（`futex_waitv`）\n- 允许线程同时等待多个 futex，任一 futex 被唤醒即返回。\n- 用户传入 `futex_waitv` 数组，每个元素包含独立的地址、期望值和标志（如私有/共享、size）。\n- 使用 `futex2_to_flags()` 将用户标志转换为内核内部标志，并验证合法性。\n- 超时使用高精度定时器（`hrtimer`），支持 `CLOCK_REALTIME` 或 `CLOCK_MONOTONIC`。\n\n### 时间处理\n- 超时参数通过 `__kernel_timespec` 传入，经 `get_timespec64()` 转换为 `timespec64`。\n- `futex_init_timeout()` 根据命令类型和时钟标志，将用户时间转换为内核 `ktime_t`，并处理绝对/相对时间语义。\n- 时间命名空间支持：非 `REALTIME` 的单调时钟时间会通过 `timens_ktime_to_host()` 转换为主机时间。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/syscalls.h>`：系统调用宏定义（如 `SYSCALL_DEFINE*`）。\n  - `<linux/time_namespace.h>`：时间命名空间支持（`timens_ktime_to_host`）。\n  - `\"futex.h\"`：futex 内部实现头文件，包含核心函数声明、标志定义和数据结构。\n\n- **内核模块依赖**：\n  - **Futex 核心逻辑**：实际的等待、唤醒、锁操作由 `futex.c` 中的函数（如 `futex_wait`、`futex_wake`）实现。\n  - **调度器与 PI 机制**：PI futex 依赖内核的优先级继承和 RT 调度支持。\n  - **高精度定时器（hrtimer）**：用于实现精确超时。\n  - **RCU 机制**：`get_robust_list` 使用 RCU 保护任务结构体访问。\n  - **Ptrace 安全检查**：`get_robust_list` 调用 `ptrace_may_access` 验证权限。\n\n## 5. 使用场景\n\n- **用户态同步原语实现**：glibc 的 `pthread_mutex`、`semaphore` 等在无竞争时完全在用户态运行，仅在需要阻塞或唤醒时调用 `futex` 系统调用。\n- **健壮互斥锁**：当持有互斥锁的线程崩溃时，其他线程可通过 `FUTEX_OWNER_DIED` 检测并恢复锁状态，避免死锁。\n- **高性能事件通知**：一个线程可等待多个事件源（如 I/O 完成、信号量、条件变量），通过 `futex_waitv` 实现“任一触发即返回”的语义。\n- **实时应用**：通过 `FUTEX_CLOCK_REALTIME` 支持基于系统实时钟的绝对超时，适用于需要与外部时间对齐的场景。\n- **容器与命名空间**：时间命名空间支持确保容器内 futex 超时行为符合容器视角的时间。",
      "similarity": 0.5560689568519592,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/futex/syscalls.c",
          "start_line": 1,
          "end_line": 83,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "",
            "#include <linux/syscalls.h>",
            "#include <linux/time_namespace.h>",
            "",
            "#include \"futex.h\"",
            "",
            "/*",
            " * Support for robust futexes: the kernel cleans up held futexes at",
            " * thread exit time.",
            " *",
            " * Implementation: user-space maintains a per-thread list of locks it",
            " * is holding. Upon do_exit(), the kernel carefully walks this list,",
            " * and marks all locks that are owned by this thread with the",
            " * FUTEX_OWNER_DIED bit, and wakes up a waiter (if any). The list is",
            " * always manipulated with the lock held, so the list is private and",
            " * per-thread. Userspace also maintains a per-thread 'list_op_pending'",
            " * field, to allow the kernel to clean up if the thread dies after",
            " * acquiring the lock, but just before it could have added itself to",
            " * the list. There can only be one such pending lock.",
            " */",
            "",
            "/**",
            " * sys_set_robust_list() - Set the robust-futex list head of a task",
            " * @head:\tpointer to the list-head",
            " * @len:\tlength of the list-head, as userspace expects",
            " */",
            "SYSCALL_DEFINE2(set_robust_list, struct robust_list_head __user *, head,",
            "\t\tsize_t, len)",
            "{",
            "\t/*",
            "\t * The kernel knows only one size for now:",
            "\t */",
            "\tif (unlikely(len != sizeof(*head)))",
            "\t\treturn -EINVAL;",
            "",
            "\tcurrent->robust_list = head;",
            "",
            "\treturn 0;",
            "}",
            "",
            "/**",
            " * sys_get_robust_list() - Get the robust-futex list head of a task",
            " * @pid:\tpid of the process [zero for current task]",
            " * @head_ptr:\tpointer to a list-head pointer, the kernel fills it in",
            " * @len_ptr:\tpointer to a length field, the kernel fills in the header size",
            " */",
            "SYSCALL_DEFINE3(get_robust_list, int, pid,",
            "\t\tstruct robust_list_head __user * __user *, head_ptr,",
            "\t\tsize_t __user *, len_ptr)",
            "{",
            "\tstruct robust_list_head __user *head;",
            "\tunsigned long ret;",
            "\tstruct task_struct *p;",
            "",
            "\trcu_read_lock();",
            "",
            "\tret = -ESRCH;",
            "\tif (!pid)",
            "\t\tp = current;",
            "\telse {",
            "\t\tp = find_task_by_vpid(pid);",
            "\t\tif (!p)",
            "\t\t\tgoto err_unlock;",
            "\t}",
            "",
            "\tret = -EPERM;",
            "\tif (!ptrace_may_access(p, PTRACE_MODE_READ_REALCREDS))",
            "\t\tgoto err_unlock;",
            "",
            "\thead = p->robust_list;",
            "\trcu_read_unlock();",
            "",
            "\tif (put_user(sizeof(*head), len_ptr))",
            "\t\treturn -EFAULT;",
            "\treturn put_user(head, head_ptr);",
            "",
            "err_unlock:",
            "\trcu_read_unlock();",
            "",
            "\treturn ret;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "实现set_robust_list和get_robust_list系统调用，用于设置/获取当前任务的robust-futex列表头，支持线程退出时自动清理持有锁的机制",
          "similarity": 0.45625990629196167
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/futex/syscalls.c",
          "start_line": 84,
          "end_line": 187,
          "content": [
            "long do_futex(u32 __user *uaddr, int op, u32 val, ktime_t *timeout,",
            "\t\tu32 __user *uaddr2, u32 val2, u32 val3)",
            "{",
            "\tunsigned int flags = futex_to_flags(op);",
            "\tint cmd = op & FUTEX_CMD_MASK;",
            "",
            "\tif (flags & FLAGS_CLOCKRT) {",
            "\t\tif (cmd != FUTEX_WAIT_BITSET &&",
            "\t\t    cmd != FUTEX_WAIT_REQUEUE_PI &&",
            "\t\t    cmd != FUTEX_LOCK_PI2)",
            "\t\t\treturn -ENOSYS;",
            "\t}",
            "",
            "\tswitch (cmd) {",
            "\tcase FUTEX_WAIT:",
            "\t\tval3 = FUTEX_BITSET_MATCH_ANY;",
            "\t\tfallthrough;",
            "\tcase FUTEX_WAIT_BITSET:",
            "\t\treturn futex_wait(uaddr, flags, val, timeout, val3);",
            "\tcase FUTEX_WAKE:",
            "\t\tval3 = FUTEX_BITSET_MATCH_ANY;",
            "\t\tfallthrough;",
            "\tcase FUTEX_WAKE_BITSET:",
            "\t\treturn futex_wake(uaddr, flags, val, val3);",
            "\tcase FUTEX_REQUEUE:",
            "\t\treturn futex_requeue(uaddr, flags, uaddr2, flags, val, val2, NULL, 0);",
            "\tcase FUTEX_CMP_REQUEUE:",
            "\t\treturn futex_requeue(uaddr, flags, uaddr2, flags, val, val2, &val3, 0);",
            "\tcase FUTEX_WAKE_OP:",
            "\t\treturn futex_wake_op(uaddr, flags, uaddr2, val, val2, val3);",
            "\tcase FUTEX_LOCK_PI:",
            "\t\tflags |= FLAGS_CLOCKRT;",
            "\t\tfallthrough;",
            "\tcase FUTEX_LOCK_PI2:",
            "\t\treturn futex_lock_pi(uaddr, flags, timeout, 0);",
            "\tcase FUTEX_UNLOCK_PI:",
            "\t\treturn futex_unlock_pi(uaddr, flags);",
            "\tcase FUTEX_TRYLOCK_PI:",
            "\t\treturn futex_lock_pi(uaddr, flags, NULL, 1);",
            "\tcase FUTEX_WAIT_REQUEUE_PI:",
            "\t\tval3 = FUTEX_BITSET_MATCH_ANY;",
            "\t\treturn futex_wait_requeue_pi(uaddr, flags, val, timeout, val3,",
            "\t\t\t\t\t     uaddr2);",
            "\tcase FUTEX_CMP_REQUEUE_PI:",
            "\t\treturn futex_requeue(uaddr, flags, uaddr2, flags, val, val2, &val3, 1);",
            "\t}",
            "\treturn -ENOSYS;",
            "}",
            "static __always_inline bool futex_cmd_has_timeout(u32 cmd)",
            "{",
            "\tswitch (cmd) {",
            "\tcase FUTEX_WAIT:",
            "\tcase FUTEX_LOCK_PI:",
            "\tcase FUTEX_LOCK_PI2:",
            "\tcase FUTEX_WAIT_BITSET:",
            "\tcase FUTEX_WAIT_REQUEUE_PI:",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "static __always_inline int",
            "futex_init_timeout(u32 cmd, u32 op, struct timespec64 *ts, ktime_t *t)",
            "{",
            "\tif (!timespec64_valid(ts))",
            "\t\treturn -EINVAL;",
            "",
            "\t*t = timespec64_to_ktime(*ts);",
            "\tif (cmd == FUTEX_WAIT)",
            "\t\t*t = ktime_add_safe(ktime_get(), *t);",
            "\telse if (cmd != FUTEX_LOCK_PI && !(op & FUTEX_CLOCK_REALTIME))",
            "\t\t*t = timens_ktime_to_host(CLOCK_MONOTONIC, *t);",
            "\treturn 0;",
            "}",
            "static int futex_parse_waitv(struct futex_vector *futexv,",
            "\t\t\t     struct futex_waitv __user *uwaitv,",
            "\t\t\t     unsigned int nr_futexes)",
            "{",
            "\tstruct futex_waitv aux;",
            "\tunsigned int i;",
            "",
            "\tfor (i = 0; i < nr_futexes; i++) {",
            "\t\tunsigned int flags;",
            "",
            "\t\tif (copy_from_user(&aux, &uwaitv[i], sizeof(aux)))",
            "\t\t\treturn -EFAULT;",
            "",
            "\t\tif ((aux.flags & ~FUTEX2_VALID_MASK) || aux.__reserved)",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\tflags = futex2_to_flags(aux.flags);",
            "\t\tif (!futex_flags_valid(flags))",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\tif (!futex_validate_input(flags, aux.val))",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\tfutexv[i].w.flags = flags;",
            "\t\tfutexv[i].w.val = aux.val;",
            "\t\tfutexv[i].w.uaddr = aux.uaddr;",
            "\t\tfutexv[i].q = futex_q_init;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "do_futex, futex_cmd_has_timeout, futex_init_timeout, futex_parse_waitv",
          "description": "do_futex处理各类futex操作命令，包含超时检测逻辑和waitv参数解析，通过条件判断分发到相应子函数实现不同操作",
          "similarity": 0.38853511214256287
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/futex/syscalls.c",
          "start_line": 224,
          "end_line": 261,
          "content": [
            "static int futex2_setup_timeout(struct __kernel_timespec __user *timeout,",
            "\t\t\t\tclockid_t clockid, struct hrtimer_sleeper *to)",
            "{",
            "\tint flag_clkid = 0, flag_init = 0;",
            "\tstruct timespec64 ts;",
            "\tktime_t time;",
            "\tint ret;",
            "",
            "\tif (!timeout)",
            "\t\treturn 0;",
            "",
            "\tif (clockid == CLOCK_REALTIME) {",
            "\t\tflag_clkid = FLAGS_CLOCKRT;",
            "\t\tflag_init = FUTEX_CLOCK_REALTIME;",
            "\t}",
            "",
            "\tif (clockid != CLOCK_REALTIME && clockid != CLOCK_MONOTONIC)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (get_timespec64(&ts, timeout))",
            "\t\treturn -EFAULT;",
            "",
            "\t/*",
            "\t * Since there's no opcode for futex_waitv, use",
            "\t * FUTEX_WAIT_BITSET that uses absolute timeout as well",
            "\t */",
            "\tret = futex_init_timeout(FUTEX_WAIT_BITSET, flag_init, &ts, &time);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tfutex_setup_timer(&time, to, flag_clkid, 0);",
            "\treturn 0;",
            "}",
            "static inline void futex2_destroy_timeout(struct hrtimer_sleeper *to)",
            "{",
            "\thrtimer_cancel(&to->timer);",
            "\tdestroy_hrtimer_on_stack(&to->timer);",
            "}"
          ],
          "function_name": "futex2_setup_timeout, futex2_destroy_timeout",
          "description": "futex2_setup_timeout将用户态超时参数转换为内核时间戳并初始化定时器，futex2_destroy_timeout负责安全地取消和销毁定时器资源",
          "similarity": 0.38716331124305725
        }
      ]
    }
  ]
}