{
  "query": "LRU implementation",
  "timestamp": "2025-12-25 23:39:20",
  "retrieved_files": [
    {
      "source_file": "mm/list_lru.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:35:23\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `list_lru.c`\n\n---\n\n# list_lru.c 技术文档\n\n## 1. 文件概述\n\n`list_lru.c` 实现了 Linux 内核中通用的 **List-based LRU（Least Recently Used）基础设施**，用于管理可回收对象的双向链表。该机制支持按 NUMA 节点（node）和内存控制组（memcg）进行细粒度组织，便于内存压力下的高效回收。主要服务于 slab 分配器等子系统，作为 shrinker 框架的一部分，在内存紧张时协助释放非活跃对象。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct list_lru`：顶层 LRU 管理结构，包含 per-node 的 `list_lru_node`\n- `struct list_lru_node`：每个 NUMA 节点对应的 LRU 节点，含自旋锁和总项数\n- `struct list_lru_one`：实际存储对象链表和计数的单元（per-memcg per-node）\n- `struct list_lru_memcg`：当启用 `CONFIG_MEMCG` 时，为每个 memcg 存储 per-node 的 `list_lru_one`\n\n### 主要导出函数\n- `list_lru_add()` / `list_lru_add_obj()`：向 LRU 添加对象\n- `list_lru_del()` / `list_lru_del_obj()`：从 LRU 删除对象\n- `list_lru_isolate()` / `list_lru_isolate_move()`：在回收过程中隔离对象\n- `list_lru_count_one()` / `list_lru_count_node()`：查询 LRU 中对象数量\n- `list_lru_walk_one()` / `list_lru_walk_node()`：遍历并处理 LRU 中的对象（用于 shrinker 回调）\n\n### 内部辅助函数\n- `list_lru_from_memcg_idx()`：根据 memcg ID 获取对应的 `list_lru_one`\n- `__list_lru_walk_one()`：带锁的 LRU 遍历核心逻辑\n- `list_lru_register()` / `list_lru_unregister()`：注册/注销 memcg-aware 的 LRU（用于全局追踪）\n\n## 3. 关键实现\n\n### 内存控制组（memcg）支持\n- 通过 `CONFIG_MEMCG` 条件编译控制 memcg 相关逻辑\n- 使用 XArray (`lru->xa`) 动态存储每个 memcg 对应的 `list_lru_memcg` 结构\n- 每个 memcg 在每个 NUMA 节点上拥有独立的 `list_lru_one`，实现资源隔离\n- 全局 `memcg_list_lrus` 链表和 `list_lrus_mutex` 用于跟踪所有 memcg-aware 的 LRU 实例\n\n### 并发控制\n- 每个 NUMA 节点 (`list_lru_node`) 拥有独立的自旋锁 (`nlru->lock`)\n- 所有对 LRU 链表的操作（增、删、遍历）均在对应节点锁保护下进行\n- 提供 `_irq` 版本的遍历函数（`list_lru_walk_one_irq`）用于中断上下文\n\n### 回收遍历机制\n- `list_lru_walk_*` 函数接受回调函数 `isolate`，由调用者定义回收策略\n- 回调返回值控制遍历行为：\n  - `LRU_REMOVED`：成功移除\n  - `LRU_REMOVED_RETRY`：移除后需重新开始遍历（锁曾被释放）\n  - `LRU_RETRY`：未移除但需重新开始遍历\n  - `LRU_ROTATE`：将对象移到链表尾部（标记为最近使用）\n  - `LRU_SKIP`：跳过当前对象\n  - `LRU_STOP`：立即停止遍历\n- 通过 `nr_to_walk` 限制单次遍历的最大对象数，防止长时间持锁\n\n### Shrinker 集成\n- 当向空的 `list_lru_one` 添加首个对象时，调用 `set_shrinker_bit()` 标记该 memcg/node 需要被 shrinker 处理\n- `lru_shrinker_id()` 返回关联的 shrinker ID，用于通知内存回收子系统\n\n### 对象归属识别\n- `list_lru_add_obj()` / `list_lru_del_obj()` 通过 `mem_cgroup_from_slab_obj()` 自动获取对象所属的 memcg\n- 使用 `page_to_nid(virt_to_page(item))` 确定对象所在的 NUMA 节点\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/list_lru.h>`：定义核心数据结构和 API\n  - `<linux/memcontrol.h>`：memcg 相关接口（如 `memcg_kmem_id`）\n  - `\"slab.h\"` 和 `\"internal.h\"`：slab 分配器内部接口（如 `mem_cgroup_from_slab_obj`）\n- **配置依赖**：\n  - `CONFIG_MEMCG`：决定是否编译 memcg 相关代码\n  - `CONFIG_NUMA`：影响 per-node 数据结构的大小（通过 `nr_node_ids`）\n- **子系统依赖**：\n  - Slab 分配器：作为主要使用者，管理可回收 slab 对象\n  - Memory Control Group (memcg)：提供内存隔离和记账\n  - Shrinker 框架：通过 shrinker 回调触发 LRU 遍历回收\n\n## 5. 使用场景\n\n- **Slab 对象回收**：当系统内存压力大时，shrinker 通过 `list_lru_walk_*` 遍历 inactive slab 对象链表，释放可回收对象\n- **Per-memcg 内存限制**：在 cgroup 内存超限时，仅遍历该 memcg 对应的 LRU 部分，实现精确回收\n- **NUMA 感知管理**：按 NUMA 节点分离 LRU 链表，减少远程内存访问，提升性能\n- **通用 LRU 容器**：任何需要按 LRU 策略管理可回收对象的内核子系统均可使用此基础设施（如 dentry、inode 缓存等）",
      "similarity": 0.5426756143569946,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "mm/list_lru.c",
          "start_line": 425,
          "end_line": 551,
          "content": [
            "static void memcg_reparent_list_lru(struct list_lru *lru,",
            "\t\t\t\t    int src_idx, struct mem_cgroup *dst_memcg)",
            "{",
            "\tint i;",
            "",
            "\tfor_each_node(i)",
            "\t\tmemcg_reparent_list_lru_node(lru, i, src_idx, dst_memcg);",
            "",
            "\tmemcg_list_lru_free(lru, src_idx);",
            "}",
            "void memcg_reparent_list_lrus(struct mem_cgroup *memcg, struct mem_cgroup *parent)",
            "{",
            "\tstruct cgroup_subsys_state *css;",
            "\tstruct list_lru *lru;",
            "\tint src_idx = memcg->kmemcg_id;",
            "",
            "\t/*",
            "\t * Change kmemcg_id of this cgroup and all its descendants to the",
            "\t * parent's id, and then move all entries from this cgroup's list_lrus",
            "\t * to ones of the parent.",
            "\t *",
            "\t * After we have finished, all list_lrus corresponding to this cgroup",
            "\t * are guaranteed to remain empty. So we can safely free this cgroup's",
            "\t * list lrus in memcg_list_lru_free().",
            "\t *",
            "\t * Changing ->kmemcg_id to the parent can prevent memcg_list_lru_alloc()",
            "\t * from allocating list lrus for this cgroup after memcg_list_lru_free()",
            "\t * call.",
            "\t */",
            "\trcu_read_lock();",
            "\tcss_for_each_descendant_pre(css, &memcg->css) {",
            "\t\tstruct mem_cgroup *child;",
            "",
            "\t\tchild = mem_cgroup_from_css(css);",
            "\t\tWRITE_ONCE(child->kmemcg_id, parent->kmemcg_id);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_for_each_entry(lru, &memcg_list_lrus, list)",
            "\t\tmemcg_reparent_list_lru(lru, src_idx, parent);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static inline bool memcg_list_lru_allocated(struct mem_cgroup *memcg,",
            "\t\t\t\t\t    struct list_lru *lru)",
            "{",
            "\tint idx = memcg->kmemcg_id;",
            "",
            "\treturn idx < 0 || xa_load(&lru->xa, idx);",
            "}",
            "int memcg_list_lru_alloc(struct mem_cgroup *memcg, struct list_lru *lru,",
            "\t\t\t gfp_t gfp)",
            "{",
            "\tint i;",
            "\tunsigned long flags;",
            "\tstruct list_lru_memcg_table {",
            "\t\tstruct list_lru_memcg *mlru;",
            "\t\tstruct mem_cgroup *memcg;",
            "\t} *table;",
            "\tXA_STATE(xas, &lru->xa, 0);",
            "",
            "\tif (!list_lru_memcg_aware(lru) || memcg_list_lru_allocated(memcg, lru))",
            "\t\treturn 0;",
            "",
            "\tgfp &= GFP_RECLAIM_MASK;",
            "\ttable = kmalloc_array(memcg->css.cgroup->level, sizeof(*table), gfp);",
            "\tif (!table)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * Because the list_lru can be reparented to the parent cgroup's",
            "\t * list_lru, we should make sure that this cgroup and all its",
            "\t * ancestors have allocated list_lru_memcg.",
            "\t */",
            "\tfor (i = 0; memcg; memcg = parent_mem_cgroup(memcg), i++) {",
            "\t\tif (memcg_list_lru_allocated(memcg, lru))",
            "\t\t\tbreak;",
            "",
            "\t\ttable[i].memcg = memcg;",
            "\t\ttable[i].mlru = memcg_init_list_lru_one(gfp);",
            "\t\tif (!table[i].mlru) {",
            "\t\t\twhile (i--)",
            "\t\t\t\tkfree(table[i].mlru);",
            "\t\t\tkfree(table);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "\t}",
            "",
            "\txas_lock_irqsave(&xas, flags);",
            "\twhile (i--) {",
            "\t\tint index = READ_ONCE(table[i].memcg->kmemcg_id);",
            "\t\tstruct list_lru_memcg *mlru = table[i].mlru;",
            "",
            "\t\txas_set(&xas, index);",
            "retry:",
            "\t\tif (unlikely(index < 0 || xas_error(&xas) || xas_load(&xas))) {",
            "\t\t\tkfree(mlru);",
            "\t\t} else {",
            "\t\t\txas_store(&xas, mlru);",
            "\t\t\tif (xas_error(&xas) == -ENOMEM) {",
            "\t\t\t\txas_unlock_irqrestore(&xas, flags);",
            "\t\t\t\tif (xas_nomem(&xas, gfp))",
            "\t\t\t\t\txas_set_err(&xas, 0);",
            "\t\t\t\txas_lock_irqsave(&xas, flags);",
            "\t\t\t\t/*",
            "\t\t\t\t * The xas lock has been released, this memcg",
            "\t\t\t\t * can be reparented before us. So reload",
            "\t\t\t\t * memcg id. More details see the comments",
            "\t\t\t\t * in memcg_reparent_list_lrus().",
            "\t\t\t\t */",
            "\t\t\t\tindex = READ_ONCE(table[i].memcg->kmemcg_id);",
            "\t\t\t\tif (index < 0)",
            "\t\t\t\t\txas_set_err(&xas, 0);",
            "\t\t\t\telse if (!xas_error(&xas) && index != xas.xa_index)",
            "\t\t\t\t\txas_set(&xas, index);",
            "\t\t\t\tgoto retry;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\t/* xas_nomem() is used to free memory instead of memory allocation. */",
            "\tif (xas.xa_alloc)",
            "\t\txas_nomem(&xas, gfp);",
            "\txas_unlock_irqrestore(&xas, flags);",
            "\tkfree(table);",
            "",
            "\treturn xas_error(&xas);",
            "}"
          ],
          "function_name": "memcg_reparent_list_lru, memcg_reparent_list_lrus, memcg_list_lru_allocated, memcg_list_lru_alloc",
          "description": "实现内存组层级间的LRU列表迁移与分配机制，包含递归子组处理、动态分配/释放LRU结构体及冲突解决逻辑。",
          "similarity": 0.5969463586807251
        },
        {
          "chunk_id": 1,
          "file_path": "mm/list_lru.c",
          "start_line": 22,
          "end_line": 129,
          "content": [
            "static inline bool list_lru_memcg_aware(struct list_lru *lru)",
            "{",
            "\treturn lru->memcg_aware;",
            "}",
            "static void list_lru_register(struct list_lru *lru)",
            "{",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_add(&lru->list, &memcg_list_lrus);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static void list_lru_unregister(struct list_lru *lru)",
            "{",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_del(&lru->list);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static int lru_shrinker_id(struct list_lru *lru)",
            "{",
            "\treturn lru->shrinker_id;",
            "}",
            "static void list_lru_register(struct list_lru *lru)",
            "{",
            "}",
            "static void list_lru_unregister(struct list_lru *lru)",
            "{",
            "}",
            "static int lru_shrinker_id(struct list_lru *lru)",
            "{",
            "\treturn -1;",
            "}",
            "static inline bool list_lru_memcg_aware(struct list_lru *lru)",
            "{",
            "\treturn false;",
            "}",
            "bool list_lru_add(struct list_lru *lru, struct list_head *item, int nid,",
            "\t\t    struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tif (list_empty(item)) {",
            "\t\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\t\tlist_add_tail(item, &l->list);",
            "\t\t/* Set shrinker bit if the first element was added */",
            "\t\tif (!l->nr_items++)",
            "\t\t\tset_shrinker_bit(memcg, nid, lru_shrinker_id(lru));",
            "\t\tnlru->nr_items++;",
            "\t\tspin_unlock(&nlru->lock);",
            "\t\treturn true;",
            "\t}",
            "\tspin_unlock(&nlru->lock);",
            "\treturn false;",
            "}",
            "bool list_lru_add_obj(struct list_lru *lru, struct list_head *item)",
            "{",
            "\tbool ret;",
            "\tint nid = page_to_nid(virt_to_page(item));",
            "",
            "\tif (list_lru_memcg_aware(lru)) {",
            "\t\trcu_read_lock();",
            "\t\tret = list_lru_add(lru, item, nid, mem_cgroup_from_slab_obj(item));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tret = list_lru_add(lru, item, nid, NULL);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "bool list_lru_del(struct list_lru *lru, struct list_head *item, int nid,",
            "\t\t    struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tif (!list_empty(item)) {",
            "\t\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\t\tlist_del_init(item);",
            "\t\tl->nr_items--;",
            "\t\tnlru->nr_items--;",
            "\t\tspin_unlock(&nlru->lock);",
            "\t\treturn true;",
            "\t}",
            "\tspin_unlock(&nlru->lock);",
            "\treturn false;",
            "}",
            "bool list_lru_del_obj(struct list_lru *lru, struct list_head *item)",
            "{",
            "\tbool ret;",
            "\tint nid = page_to_nid(virt_to_page(item));",
            "",
            "\tif (list_lru_memcg_aware(lru)) {",
            "\t\trcu_read_lock();",
            "\t\tret = list_lru_del(lru, item, nid, mem_cgroup_from_slab_obj(item));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tret = list_lru_del(lru, item, nid, NULL);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "list_lru_memcg_aware, list_lru_register, list_lru_unregister, lru_shrinker_id, list_lru_register, list_lru_unregister, lru_shrinker_id, list_lru_memcg_aware, list_lru_add, list_lru_add_obj, list_lru_del, list_lru_del_obj",
          "description": "实现了LRU列表的添加/删除操作，支持MemCG感知的节点和内存组粒度管理，包含处理多核、内存组切换及RCU安全访问的逻辑。",
          "similarity": 0.5923566818237305
        },
        {
          "chunk_id": 5,
          "file_path": "mm/list_lru.c",
          "start_line": 556,
          "end_line": 605,
          "content": [
            "static inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)",
            "{",
            "}",
            "static void memcg_destroy_list_lru(struct list_lru *lru)",
            "{",
            "}",
            "int __list_lru_init(struct list_lru *lru, bool memcg_aware,",
            "\t\t    struct lock_class_key *key, struct shrinker *shrinker)",
            "{",
            "\tint i;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tif (shrinker)",
            "\t\tlru->shrinker_id = shrinker->id;",
            "\telse",
            "\t\tlru->shrinker_id = -1;",
            "#endif",
            "",
            "\tlru->node = kcalloc(nr_node_ids, sizeof(*lru->node), GFP_KERNEL);",
            "\tif (!lru->node)",
            "\t\treturn -ENOMEM;",
            "",
            "\tfor_each_node(i) {",
            "\t\tspin_lock_init(&lru->node[i].lock);",
            "\t\tif (key)",
            "\t\t\tlockdep_set_class(&lru->node[i].lock, key);",
            "\t\tinit_one_lru(&lru->node[i].lru);",
            "\t}",
            "",
            "\tmemcg_init_list_lru(lru, memcg_aware);",
            "\tlist_lru_register(lru);",
            "",
            "\treturn 0;",
            "}",
            "void list_lru_destroy(struct list_lru *lru)",
            "{",
            "\t/* Already destroyed or not yet initialized? */",
            "\tif (!lru->node)",
            "\t\treturn;",
            "",
            "\tlist_lru_unregister(lru);",
            "",
            "\tmemcg_destroy_list_lru(lru);",
            "\tkfree(lru->node);",
            "\tlru->node = NULL;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tlru->shrinker_id = -1;",
            "#endif",
            "}"
          ],
          "function_name": "memcg_init_list_lru, memcg_destroy_list_lru, __list_lru_init, list_lru_destroy",
          "description": "该代码段实现了基于内存控制组（MEMCG）的LRU列表管理功能。  \n`__list_lru_init` 初始化 `list_lru` 结构体并注册到系统，其中包含 MEMCG 相关的 shrinker ID 设置及节点锁初始化；`list_lru_destroy` 反向清理资源，但 `memcg_init_list_lru` 和 `memcg_destroy_list_lru` 的具体实现缺失，上下文不完整。",
          "similarity": 0.5725371837615967
        },
        {
          "chunk_id": 2,
          "file_path": "mm/list_lru.c",
          "start_line": 166,
          "end_line": 277,
          "content": [
            "void list_lru_isolate(struct list_lru_one *list, struct list_head *item)",
            "{",
            "\tlist_del_init(item);",
            "\tlist->nr_items--;",
            "}",
            "void list_lru_isolate_move(struct list_lru_one *list, struct list_head *item,",
            "\t\t\t   struct list_head *head)",
            "{",
            "\tlist_move(item, head);",
            "\tlist->nr_items--;",
            "}",
            "unsigned long list_lru_count_one(struct list_lru *lru,",
            "\t\t\t\t int nid, struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_one *l;",
            "\tlong count;",
            "",
            "\trcu_read_lock();",
            "\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\tcount = l ? READ_ONCE(l->nr_items) : 0;",
            "\trcu_read_unlock();",
            "",
            "\tif (unlikely(count < 0))",
            "\t\tcount = 0;",
            "",
            "\treturn count;",
            "}",
            "unsigned long list_lru_count_node(struct list_lru *lru, int nid)",
            "{",
            "\tstruct list_lru_node *nlru;",
            "",
            "\tnlru = &lru->node[nid];",
            "\treturn nlru->nr_items;",
            "}",
            "static unsigned long",
            "__list_lru_walk_one(struct list_lru *lru, int nid, int memcg_idx,",
            "\t\t    list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t    unsigned long *nr_to_walk)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "\tstruct list_head *item, *n;",
            "\tunsigned long isolated = 0;",
            "",
            "restart:",
            "\tl = list_lru_from_memcg_idx(lru, nid, memcg_idx);",
            "\tif (!l)",
            "\t\tgoto out;",
            "",
            "\tlist_for_each_safe(item, n, &l->list) {",
            "\t\tenum lru_status ret;",
            "",
            "\t\t/*",
            "\t\t * decrement nr_to_walk first so that we don't livelock if we",
            "\t\t * get stuck on large numbers of LRU_RETRY items",
            "\t\t */",
            "\t\tif (!*nr_to_walk)",
            "\t\t\tbreak;",
            "\t\t--*nr_to_walk;",
            "",
            "\t\tret = isolate(item, l, &nlru->lock, cb_arg);",
            "\t\tswitch (ret) {",
            "\t\tcase LRU_REMOVED_RETRY:",
            "\t\t\tassert_spin_locked(&nlru->lock);",
            "\t\t\tfallthrough;",
            "\t\tcase LRU_REMOVED:",
            "\t\t\tisolated++;",
            "\t\t\tnlru->nr_items--;",
            "\t\t\t/*",
            "\t\t\t * If the lru lock has been dropped, our list",
            "\t\t\t * traversal is now invalid and so we have to",
            "\t\t\t * restart from scratch.",
            "\t\t\t */",
            "\t\t\tif (ret == LRU_REMOVED_RETRY)",
            "\t\t\t\tgoto restart;",
            "\t\t\tbreak;",
            "\t\tcase LRU_ROTATE:",
            "\t\t\tlist_move_tail(item, &l->list);",
            "\t\t\tbreak;",
            "\t\tcase LRU_SKIP:",
            "\t\t\tbreak;",
            "\t\tcase LRU_RETRY:",
            "\t\t\t/*",
            "\t\t\t * The lru lock has been dropped, our list traversal is",
            "\t\t\t * now invalid and so we have to restart from scratch.",
            "\t\t\t */",
            "\t\t\tassert_spin_locked(&nlru->lock);",
            "\t\t\tgoto restart;",
            "\t\tcase LRU_STOP:",
            "\t\t\tassert_spin_locked(&nlru->lock);",
            "\t\t\tgoto out;",
            "\t\tdefault:",
            "\t\t\tBUG();",
            "\t\t}",
            "\t}",
            "out:",
            "\treturn isolated;",
            "}",
            "unsigned long",
            "list_lru_walk_one(struct list_lru *lru, int nid, struct mem_cgroup *memcg,",
            "\t\t  list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t  unsigned long *nr_to_walk)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tunsigned long ret;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tret = __list_lru_walk_one(lru, nid, memcg_kmem_id(memcg), isolate,",
            "\t\t\t\t  cb_arg, nr_to_walk);",
            "\tspin_unlock(&nlru->lock);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "list_lru_isolate, list_lru_isolate_move, list_lru_count_one, list_lru_count_node, __list_lru_walk_one, list_lru_walk_one",
          "description": "提供LRU列表遍历与隔离功能，通过__list_lru_walk_one实现带状态反馈的遍历操作，支持旋转、跳过、重试等复杂行为处理。",
          "similarity": 0.5448712706565857
        },
        {
          "chunk_id": 3,
          "file_path": "mm/list_lru.c",
          "start_line": 289,
          "end_line": 400,
          "content": [
            "unsigned long",
            "list_lru_walk_one_irq(struct list_lru *lru, int nid, struct mem_cgroup *memcg,",
            "\t\t      list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t      unsigned long *nr_to_walk)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tunsigned long ret;",
            "",
            "\tspin_lock_irq(&nlru->lock);",
            "\tret = __list_lru_walk_one(lru, nid, memcg_kmem_id(memcg), isolate,",
            "\t\t\t\t  cb_arg, nr_to_walk);",
            "\tspin_unlock_irq(&nlru->lock);",
            "\treturn ret;",
            "}",
            "unsigned long list_lru_walk_node(struct list_lru *lru, int nid,",
            "\t\t\t\t list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t\t\t unsigned long *nr_to_walk)",
            "{",
            "\tlong isolated = 0;",
            "",
            "\tisolated += list_lru_walk_one(lru, nid, NULL, isolate, cb_arg,",
            "\t\t\t\t      nr_to_walk);",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tif (*nr_to_walk > 0 && list_lru_memcg_aware(lru)) {",
            "\t\tstruct list_lru_memcg *mlru;",
            "\t\tunsigned long index;",
            "",
            "\t\txa_for_each(&lru->xa, index, mlru) {",
            "\t\t\tstruct list_lru_node *nlru = &lru->node[nid];",
            "",
            "\t\t\tspin_lock(&nlru->lock);",
            "\t\t\tisolated += __list_lru_walk_one(lru, nid, index,",
            "\t\t\t\t\t\t\tisolate, cb_arg,",
            "\t\t\t\t\t\t\tnr_to_walk);",
            "\t\t\tspin_unlock(&nlru->lock);",
            "",
            "\t\t\tif (*nr_to_walk <= 0)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "#endif",
            "",
            "\treturn isolated;",
            "}",
            "static void init_one_lru(struct list_lru_one *l)",
            "{",
            "\tINIT_LIST_HEAD(&l->list);",
            "\tl->nr_items = 0;",
            "}",
            "static void memcg_list_lru_free(struct list_lru *lru, int src_idx)",
            "{",
            "\tstruct list_lru_memcg *mlru = xa_erase_irq(&lru->xa, src_idx);",
            "",
            "\t/*",
            "\t * The __list_lru_walk_one() can walk the list of this node.",
            "\t * We need kvfree_rcu() here. And the walking of the list",
            "\t * is under lru->node[nid]->lock, which can serve as a RCU",
            "\t * read-side critical section.",
            "\t */",
            "\tif (mlru)",
            "\t\tkvfree_rcu(mlru, rcu);",
            "}",
            "static inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)",
            "{",
            "\tif (memcg_aware)",
            "\t\txa_init_flags(&lru->xa, XA_FLAGS_LOCK_IRQ);",
            "\tlru->memcg_aware = memcg_aware;",
            "}",
            "static void memcg_destroy_list_lru(struct list_lru *lru)",
            "{",
            "\tXA_STATE(xas, &lru->xa, 0);",
            "\tstruct list_lru_memcg *mlru;",
            "",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\txas_lock_irq(&xas);",
            "\txas_for_each(&xas, mlru, ULONG_MAX) {",
            "\t\tkfree(mlru);",
            "\t\txas_store(&xas, NULL);",
            "\t}",
            "\txas_unlock_irq(&xas);",
            "}",
            "static void memcg_reparent_list_lru_node(struct list_lru *lru, int nid,",
            "\t\t\t\t\t int src_idx, struct mem_cgroup *dst_memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tint dst_idx = dst_memcg->kmemcg_id;",
            "\tstruct list_lru_one *src, *dst;",
            "",
            "\t/*",
            "\t * Since list_lru_{add,del} may be called under an IRQ-safe lock,",
            "\t * we have to use IRQ-safe primitives here to avoid deadlock.",
            "\t */",
            "\tspin_lock_irq(&nlru->lock);",
            "",
            "\tsrc = list_lru_from_memcg_idx(lru, nid, src_idx);",
            "\tif (!src)",
            "\t\tgoto out;",
            "\tdst = list_lru_from_memcg_idx(lru, nid, dst_idx);",
            "",
            "\tlist_splice_init(&src->list, &dst->list);",
            "",
            "\tif (src->nr_items) {",
            "\t\tdst->nr_items += src->nr_items;",
            "\t\tset_shrinker_bit(dst_memcg, nid, lru_shrinker_id(lru));",
            "\t\tsrc->nr_items = 0;",
            "\t}",
            "out:",
            "\tspin_unlock_irq(&nlru->lock);",
            "}"
          ],
          "function_name": "list_lru_walk_one_irq, list_lru_walk_node, init_one_lru, memcg_list_lru_free, memcg_init_list_lru, memcg_destroy_list_lru, memcg_reparent_list_lru_node",
          "description": "包含LRU节点初始化、内存组间列表迁移、资源释放等高级操作，涉及XA表管理、中断安全锁操作及内存组重新归属处理。",
          "similarity": 0.5410412549972534
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.4960871934890747,
      "chunks": [
        {
          "chunk_id": 14,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2523,
          "end_line": 2628,
          "content": [
            "static void rcu_cpu_kthread_park(unsigned int cpu)",
            "{",
            "\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;",
            "}",
            "static int rcu_cpu_kthread_should_run(unsigned int cpu)",
            "{",
            "\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);",
            "}",
            "static void rcu_cpu_kthread(unsigned int cpu)",
            "{",
            "\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);",
            "\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);",
            "\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);",
            "\tint spincnt;",
            "",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));",
            "\tfor (spincnt = 0; spincnt < 10; spincnt++) {",
            "\t\tWRITE_ONCE(*j, jiffies);",
            "\t\tlocal_bh_disable();",
            "\t\t*statusp = RCU_KTHREAD_RUNNING;",
            "\t\tlocal_irq_disable();",
            "\t\twork = *workp;",
            "\t\tWRITE_ONCE(*workp, 0);",
            "\t\tlocal_irq_enable();",
            "\t\tif (work)",
            "\t\t\trcu_core();",
            "\t\tlocal_bh_enable();",
            "\t\tif (!READ_ONCE(*workp)) {",
            "\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));",
            "\t\t\t*statusp = RCU_KTHREAD_WAITING;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "\t*statusp = RCU_KTHREAD_YIELDING;",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));",
            "\tschedule_timeout_idle(2);",
            "\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));",
            "\t*statusp = RCU_KTHREAD_WAITING;",
            "\tWRITE_ONCE(*j, jiffies);",
            "}",
            "static int __init rcu_spawn_core_kthreads(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;",
            "\tif (use_softirq)",
            "\t\treturn 0;",
            "\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),",
            "\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);",
            "\treturn 0;",
            "}",
            "static void rcutree_enqueue(struct rcu_data *rdp, struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\trcu_segcblist_enqueue(&rdp->cblist, head);",
            "\tif (__is_kvfree_rcu_offset((unsigned long)func))",
            "\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,",
            "\t\t\t\t\t (unsigned long)func,",
            "\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));",
            "\telse",
            "\t\ttrace_rcu_callback(rcu_state.name, head,",
            "\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));",
            "}",
            "static void call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,",
            "\t\t\t  rcu_callback_t func, unsigned long flags)",
            "{",
            "\trcutree_enqueue(rdp, head, func);",
            "\t/*",
            "\t * If called from an extended quiescent state, invoke the RCU",
            "\t * core in order to force a re-evaluation of RCU's idleness.",
            "\t */",
            "\tif (!rcu_is_watching())",
            "\t\tinvoke_rcu_core();",
            "",
            "\t/* If interrupts were disabled or CPU offline, don't invoke RCU core. */",
            "\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Force the grace period if too many callbacks or too long waiting.",
            "\t * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()",
            "\t * if some other CPU has recently done so.  Also, don't bother",
            "\t * invoking rcu_force_quiescent_state() if the newly enqueued callback",
            "\t * is the only one waiting for a grace period to complete.",
            "\t */",
            "\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >",
            "\t\t     rdp->qlen_last_fqs_check + qhimark)) {",
            "",
            "\t\t/* Are we ignoring a completed grace period? */",
            "\t\tnote_gp_changes(rdp);",
            "",
            "\t\t/* Start a new grace period if one not already started. */",
            "\t\tif (!rcu_gp_in_progress()) {",
            "\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);",
            "\t\t} else {",
            "\t\t\t/* Give the grace period a kick. */",
            "\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;",
            "\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&",
            "\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)",
            "\t\t\t\trcu_force_quiescent_state();",
            "\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "rcu_cpu_kthread_park, rcu_cpu_kthread_should_run, rcu_cpu_kthread, rcu_spawn_core_kthreads, rcutree_enqueue, call_rcu_core",
          "description": "实现RCU k线程管理与回调分发基础设施，包含线程启动、回调入队及触发条件判断逻辑，提供跨CPU的异步处理能力",
          "similarity": 0.5441977977752686
        },
        {
          "chunk_id": 22,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4072,
          "end_line": 4226,
          "content": [
            "static void rcu_barrier_trace(const char *s, int cpu, unsigned long done)",
            "{",
            "\ttrace_rcu_barrier(rcu_state.name, s, cpu,",
            "\t\t\t  atomic_read(&rcu_state.barrier_cpu_count), done);",
            "}",
            "static void rcu_barrier_callback(struct rcu_head *rhp)",
            "{",
            "\tunsigned long __maybe_unused s = rcu_state.barrier_sequence;",
            "",
            "\tif (atomic_dec_and_test(&rcu_state.barrier_cpu_count)) {",
            "\t\trcu_barrier_trace(TPS(\"LastCB\"), -1, s);",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "\t} else {",
            "\t\trcu_barrier_trace(TPS(\"CB\"), -1, s);",
            "\t}",
            "}",
            "static void rcu_barrier_entrain(struct rcu_data *rdp)",
            "{",
            "\tunsigned long gseq = READ_ONCE(rcu_state.barrier_sequence);",
            "\tunsigned long lseq = READ_ONCE(rdp->barrier_seq_snap);",
            "\tbool wake_nocb = false;",
            "\tbool was_alldone = false;",
            "",
            "\tlockdep_assert_held(&rcu_state.barrier_lock);",
            "\tif (rcu_seq_state(lseq) || !rcu_seq_state(gseq) || rcu_seq_ctr(lseq) != rcu_seq_ctr(gseq))",
            "\t\treturn;",
            "\trcu_barrier_trace(TPS(\"IRQ\"), -1, rcu_state.barrier_sequence);",
            "\trdp->barrier_head.func = rcu_barrier_callback;",
            "\tdebug_rcu_head_queue(&rdp->barrier_head);",
            "\trcu_nocb_lock(rdp);",
            "\t/*",
            "\t * Flush bypass and wakeup rcuog if we add callbacks to an empty regular",
            "\t * queue. This way we don't wait for bypass timer that can reach seconds",
            "\t * if it's fully lazy.",
            "\t */",
            "\twas_alldone = rcu_rdp_is_offloaded(rdp) && !rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));",
            "\twake_nocb = was_alldone && rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tif (rcu_segcblist_entrain(&rdp->cblist, &rdp->barrier_head)) {",
            "\t\tatomic_inc(&rcu_state.barrier_cpu_count);",
            "\t} else {",
            "\t\tdebug_rcu_head_unqueue(&rdp->barrier_head);",
            "\t\trcu_barrier_trace(TPS(\"IRQNQ\"), -1, rcu_state.barrier_sequence);",
            "\t}",
            "\trcu_nocb_unlock(rdp);",
            "\tif (wake_nocb)",
            "\t\twake_nocb_gp(rdp, false);",
            "\tsmp_store_release(&rdp->barrier_seq_snap, gseq);",
            "}",
            "static void rcu_barrier_handler(void *cpu_in)",
            "{",
            "\tuintptr_t cpu = (uintptr_t)cpu_in;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\tWARN_ON_ONCE(cpu != rdp->cpu);",
            "\tWARN_ON_ONCE(cpu != smp_processor_id());",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\trcu_barrier_entrain(rdp);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "}",
            "void rcu_barrier(void)",
            "{",
            "\tuintptr_t cpu;",
            "\tunsigned long flags;",
            "\tunsigned long gseq;",
            "\tstruct rcu_data *rdp;",
            "\tunsigned long s = rcu_seq_snap(&rcu_state.barrier_sequence);",
            "",
            "\trcu_barrier_trace(TPS(\"Begin\"), -1, s);",
            "",
            "\t/* Take mutex to serialize concurrent rcu_barrier() requests. */",
            "\tmutex_lock(&rcu_state.barrier_mutex);",
            "",
            "\t/* Did someone else do our work for us? */",
            "\tif (rcu_seq_done(&rcu_state.barrier_sequence, s)) {",
            "\t\trcu_barrier_trace(TPS(\"EarlyExit\"), -1, rcu_state.barrier_sequence);",
            "\t\tsmp_mb(); /* caller's subsequent code after above check. */",
            "\t\tmutex_unlock(&rcu_state.barrier_mutex);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Mark the start of the barrier operation. */",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\trcu_seq_start(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\trcu_barrier_trace(TPS(\"Inc1\"), -1, rcu_state.barrier_sequence);",
            "",
            "\t/*",
            "\t * Initialize the count to two rather than to zero in order",
            "\t * to avoid a too-soon return to zero in case of an immediate",
            "\t * invocation of the just-enqueued callback (or preemption of",
            "\t * this task).  Exclude CPU-hotplug operations to ensure that no",
            "\t * offline non-offloaded CPU has callbacks queued.",
            "\t */",
            "\tinit_completion(&rcu_state.barrier_completion);",
            "\tatomic_set(&rcu_state.barrier_cpu_count, 2);",
            "\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "",
            "\t/*",
            "\t * Force each CPU with callbacks to register a new callback.",
            "\t * When that callback is invoked, we will know that all of the",
            "\t * corresponding CPU's preceding callbacks have been invoked.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "retry:",
            "\t\tif (smp_load_acquire(&rdp->barrier_seq_snap) == gseq)",
            "\t\t\tcontinue;",
            "\t\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\t\tif (!rcu_segcblist_n_cbs(&rdp->cblist)) {",
            "\t\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"NQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (!rcu_rdp_cpu_online(rdp)) {",
            "\t\t\trcu_barrier_entrain(rdp);",
            "\t\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"OfflineNoCBQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\tif (smp_call_function_single(cpu, rcu_barrier_handler, (void *)cpu, 1)) {",
            "\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\tgoto retry;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\trcu_barrier_trace(TPS(\"OnlineQ\"), cpu, rcu_state.barrier_sequence);",
            "\t}",
            "",
            "\t/*",
            "\t * Now that we have an rcu_barrier_callback() callback on each",
            "\t * CPU, and thus each counted, remove the initial count.",
            "\t */",
            "\tif (atomic_sub_and_test(2, &rcu_state.barrier_cpu_count))",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "",
            "\t/* Wait for all rcu_barrier_callback() callbacks to be invoked. */",
            "\twait_for_completion(&rcu_state.barrier_completion);",
            "",
            "\t/* Mark the end of the barrier operation. */",
            "\trcu_barrier_trace(TPS(\"Inc2\"), -1, rcu_state.barrier_sequence);",
            "\trcu_seq_end(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t}",
            "",
            "\t/* Other rcu_barrier() invocations can now safely proceed. */",
            "\tmutex_unlock(&rcu_state.barrier_mutex);",
            "}"
          ],
          "function_name": "rcu_barrier_trace, rcu_barrier_callback, rcu_barrier_entrain, rcu_barrier_handler, rcu_barrier",
          "description": "实现RCU屏障功能，通过分发回调函数强制所有CPU完成当前RCU操作，使用原子计数器跟踪完成状态，通过completion等待所有回调完成",
          "similarity": 0.530463695526123
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2273,
          "end_line": 2388,
          "content": [
            "void rcu_sched_clock_irq(int user)",
            "{",
            "\tunsigned long j;",
            "",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU)) {",
            "\t\tj = jiffies;",
            "\t\tWARN_ON_ONCE(time_before(j, __this_cpu_read(rcu_data.last_sched_clock)));",
            "\t\t__this_cpu_write(rcu_data.last_sched_clock, j);",
            "\t}",
            "\ttrace_rcu_utilization(TPS(\"Start scheduler-tick\"));",
            "\tlockdep_assert_irqs_disabled();",
            "\traw_cpu_inc(rcu_data.ticks_this_gp);",
            "\t/* The load-acquire pairs with the store-release setting to true. */",
            "\tif (smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs))) {",
            "\t\t/* Idle and userspace execution already are quiescent states. */",
            "\t\tif (!rcu_is_cpu_rrupt_from_idle() && !user) {",
            "\t\t\tset_tsk_need_resched(current);",
            "\t\t\tset_preempt_need_resched();",
            "\t\t}",
            "\t\t__this_cpu_write(rcu_data.rcu_urgent_qs, false);",
            "\t}",
            "\trcu_flavor_sched_clock_irq(user);",
            "\tif (rcu_pending(user))",
            "\t\tinvoke_rcu_core();",
            "\tif (user || rcu_is_cpu_rrupt_from_idle())",
            "\t\trcu_note_voluntary_context_switch(current);",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\ttrace_rcu_utilization(TPS(\"End scheduler-tick\"));",
            "}",
            "static void force_qs_rnp(int (*f)(struct rcu_data *rdp))",
            "{",
            "\tint cpu;",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trcu_state.cbovld = rcu_state.cbovldnext;",
            "\trcu_state.cbovldnext = false;",
            "\trcu_for_each_leaf_node(rnp) {",
            "\t\tunsigned long mask = 0;",
            "\t\tunsigned long rsmask = 0;",
            "",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\trcu_state.cbovldnext |= !!rnp->cbovldmask;",
            "\t\tif (rnp->qsmask == 0) {",
            "\t\t\tif (rcu_preempt_blocked_readers_cgp(rnp)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No point in scanning bits because they",
            "\t\t\t\t * are all zero.  But we might need to",
            "\t\t\t\t * priority-boost blocked readers.",
            "\t\t\t\t */",
            "\t\t\t\trcu_initiate_boost(rnp, flags);",
            "\t\t\t\t/* rcu_initiate_boost() releases rnp->lock */",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rnp->qsmask) {",
            "\t\t\tstruct rcu_data *rdp;",
            "\t\t\tint ret;",
            "",
            "\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\t\t\tret = f(rdp);",
            "\t\t\tif (ret > 0) {",
            "\t\t\t\tmask |= rdp->grpmask;",
            "\t\t\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\t\t}",
            "\t\t\tif (ret < 0)",
            "\t\t\t\trsmask |= rdp->grpmask;",
            "\t\t}",
            "\t\tif (mask != 0) {",
            "\t\t\t/* Idle/offline CPUs, report (releases rnp->lock). */",
            "\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\t} else {",
            "\t\t\t/* Nothing to do here, so just drop the lock. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t}",
            "",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rsmask)",
            "\t\t\tresched_cpu(cpu);",
            "\t}",
            "}",
            "void rcu_force_quiescent_state(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool ret;",
            "\tstruct rcu_node *rnp;",
            "\tstruct rcu_node *rnp_old = NULL;",
            "",
            "\t/* Funnel through hierarchy to reduce memory contention. */",
            "\trnp = raw_cpu_read(rcu_data.mynode);",
            "\tfor (; rnp != NULL; rnp = rnp->parent) {",
            "\t\tret = (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) ||",
            "\t\t       !raw_spin_trylock(&rnp->fqslock);",
            "\t\tif (rnp_old != NULL)",
            "\t\t\traw_spin_unlock(&rnp_old->fqslock);",
            "\t\tif (ret)",
            "\t\t\treturn;",
            "\t\trnp_old = rnp;",
            "\t}",
            "\t/* rnp_old == rcu_get_root(), rnp == NULL. */",
            "",
            "\t/* Reached the root of the rcu_node tree, acquire lock. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp_old, flags);",
            "\traw_spin_unlock(&rnp_old->fqslock);",
            "\tif (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\t\treturn;  /* Someone beat us to it. */",
            "\t}",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\trcu_gp_kthread_wake();",
            "}"
          ],
          "function_name": "rcu_sched_clock_irq, force_qs_rnp, rcu_force_quiescent_state",
          "description": "实现强制quiescent状态触发与调度器中断处理，包含优先级提升、回调加速及grace period推进等关键控制流，维护RCU状态一致性",
          "similarity": 0.5274149775505066
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2406,
          "end_line": 2512,
          "content": [
            "static void strict_work_handler(struct work_struct *work)",
            "{",
            "\trcu_read_lock();",
            "\trcu_read_unlock();",
            "}",
            "static __latent_entropy void rcu_core(void)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp = raw_cpu_ptr(&rcu_data);",
            "\tstruct rcu_node *rnp = rdp->mynode;",
            "\t/*",
            "\t * On RT rcu_core() can be preempted when IRQs aren't disabled.",
            "\t * Therefore this function can race with concurrent NOCB (de-)offloading",
            "\t * on this CPU and the below condition must be considered volatile.",
            "\t * However if we race with:",
            "\t *",
            "\t * _ Offloading:   In the worst case we accelerate or process callbacks",
            "\t *                 concurrently with NOCB kthreads. We are guaranteed to",
            "\t *                 call rcu_nocb_lock() if that happens.",
            "\t *",
            "\t * _ Deoffloading: In the worst case we miss callbacks acceleration or",
            "\t *                 processing. This is fine because the early stage",
            "\t *                 of deoffloading invokes rcu_core() after setting",
            "\t *                 SEGCBLIST_RCU_CORE. So we guarantee that we'll process",
            "\t *                 what could have been dismissed without the need to wait",
            "\t *                 for the next rcu_pending() check in the next jiffy.",
            "\t */",
            "\tconst bool do_batch = !rcu_segcblist_completely_offloaded(&rdp->cblist);",
            "",
            "\tif (cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "\ttrace_rcu_utilization(TPS(\"Start RCU core\"));",
            "\tWARN_ON_ONCE(!rdp->beenonline);",
            "",
            "\t/* Report any deferred quiescent states if preemption enabled. */",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_COUNT) && (!(preempt_count() & PREEMPT_MASK))) {",
            "\t\trcu_preempt_deferred_qs(current);",
            "\t} else if (rcu_preempt_need_deferred_qs(current)) {",
            "\t\tset_tsk_need_resched(current);",
            "\t\tset_preempt_need_resched();",
            "\t}",
            "",
            "\t/* Update RCU state based on any recent quiescent states. */",
            "\trcu_check_quiescent_state(rdp);",
            "",
            "\t/* No grace period and unregistered callbacks? */",
            "\tif (!rcu_gp_in_progress() &&",
            "\t    rcu_segcblist_is_enabled(&rdp->cblist) && do_batch) {",
            "\t\trcu_nocb_lock_irqsave(rdp, flags);",
            "\t\tif (!rcu_segcblist_restempty(&rdp->cblist, RCU_NEXT_READY_TAIL))",
            "\t\t\trcu_accelerate_cbs_unlocked(rnp, rdp);",
            "\t\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "\t}",
            "",
            "\trcu_check_gp_start_stall(rnp, rdp, rcu_jiffies_till_stall_check());",
            "",
            "\t/* If there are callbacks ready, invoke them. */",
            "\tif (do_batch && rcu_segcblist_ready_cbs(&rdp->cblist) &&",
            "\t    likely(READ_ONCE(rcu_scheduler_fully_active))) {",
            "\t\trcu_do_batch(rdp);",
            "\t\t/* Re-invoke RCU core processing if there are callbacks remaining. */",
            "\t\tif (rcu_segcblist_ready_cbs(&rdp->cblist))",
            "\t\t\tinvoke_rcu_core();",
            "\t}",
            "",
            "\t/* Do any needed deferred wakeups of rcuo kthreads. */",
            "\tdo_nocb_deferred_wakeup(rdp);",
            "\ttrace_rcu_utilization(TPS(\"End RCU core\"));",
            "",
            "\t// If strict GPs, schedule an RCU reader in a clean environment.",
            "\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD))",
            "\t\tqueue_work_on(rdp->cpu, rcu_gp_wq, &rdp->strict_work);",
            "}",
            "static void rcu_core_si(struct softirq_action *h)",
            "{",
            "\trcu_core();",
            "}",
            "static void rcu_wake_cond(struct task_struct *t, int status)",
            "{",
            "\t/*",
            "\t * If the thread is yielding, only wake it when this",
            "\t * is invoked from idle",
            "\t */",
            "\tif (t && (status != RCU_KTHREAD_YIELDING || is_idle_task(current)))",
            "\t\twake_up_process(t);",
            "}",
            "static void invoke_rcu_core_kthread(void)",
            "{",
            "\tstruct task_struct *t;",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\t__this_cpu_write(rcu_data.rcu_cpu_has_work, 1);",
            "\tt = __this_cpu_read(rcu_data.rcu_cpu_kthread_task);",
            "\tif (t != NULL && t != current)",
            "\t\trcu_wake_cond(t, __this_cpu_read(rcu_data.rcu_cpu_kthread_status));",
            "\tlocal_irq_restore(flags);",
            "}",
            "static void invoke_rcu_core(void)",
            "{",
            "\tif (!cpu_online(smp_processor_id()))",
            "\t\treturn;",
            "\tif (use_softirq)",
            "\t\traise_softirq(RCU_SOFTIRQ);",
            "\telse",
            "\t\tinvoke_rcu_core_kthread();",
            "}"
          ],
          "function_name": "strict_work_handler, rcu_core, rcu_core_si, rcu_wake_cond, invoke_rcu_core_kthread, invoke_rcu_core",
          "description": "定义RCU核心处理流程，包含软中断模式下的回调处理、grace period状态验证及条件唤醒机制，支持严格grace period场景的特殊处理",
          "similarity": 0.5139927864074707
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 908,
          "end_line": 1026,
          "content": [
            "static void trace_rcu_this_gp(struct rcu_node *rnp, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req, const char *s)",
            "{",
            "\ttrace_rcu_future_grace_period(rcu_state.name, READ_ONCE(rnp->gp_seq),",
            "\t\t\t\t      gp_seq_req, rnp->level,",
            "\t\t\t\t      rnp->grplo, rnp->grphi, s);",
            "}",
            "static bool rcu_start_this_gp(struct rcu_node *rnp_start, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req)",
            "{",
            "\tbool ret = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\t/*",
            "\t * Use funnel locking to either acquire the root rcu_node",
            "\t * structure's lock or bail out if the need for this grace period",
            "\t * has already been recorded -- or if that grace period has in",
            "\t * fact already started.  If there is already a grace period in",
            "\t * progress in a non-leaf node, no recording is needed because the",
            "\t * end of the grace period will scan the leaf rcu_node structures.",
            "\t * Note that rnp_start->lock must not be released.",
            "\t */",
            "\traw_lockdep_assert_held_rcu_node(rnp_start);",
            "\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req, TPS(\"Startleaf\"));",
            "\tfor (rnp = rnp_start; 1; rnp = rnp->parent) {",
            "\t\tif (rnp != rnp_start)",
            "\t\t\traw_spin_lock_rcu_node(rnp);",
            "\t\tif (ULONG_CMP_GE(rnp->gp_seq_needed, gp_seq_req) ||",
            "\t\t    rcu_seq_started(&rnp->gp_seq, gp_seq_req) ||",
            "\t\t    (rnp != rnp_start &&",
            "\t\t     rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))) {",
            "\t\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Prestarted\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tWRITE_ONCE(rnp->gp_seq_needed, gp_seq_req);",
            "\t\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq))) {",
            "\t\t\t/*",
            "\t\t\t * We just marked the leaf or internal node, and a",
            "\t\t\t * grace period is in progress, which means that",
            "\t\t\t * rcu_gp_cleanup() will see the marking.  Bail to",
            "\t\t\t * reduce contention.",
            "\t\t\t */",
            "\t\t\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Startedleaf\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tif (rnp != rnp_start && rnp->parent != NULL)",
            "\t\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\tif (!rnp->parent)",
            "\t\t\tbreak;  /* At root, and perhaps also leaf. */",
            "\t}",
            "",
            "\t/* If GP already in progress, just leave, otherwise start one. */",
            "\tif (rcu_gp_in_progress()) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedleafroot\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedroot\"));",
            "\tWRITE_ONCE(rcu_state.gp_flags, rcu_state.gp_flags | RCU_GP_FLAG_INIT);",
            "\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);",
            "\tif (!READ_ONCE(rcu_state.gp_kthread)) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"NoGPkthread\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_grace_period(rcu_state.name, data_race(rcu_state.gp_seq), TPS(\"newreq\"));",
            "\tret = true;  /* Caller must wake GP kthread. */",
            "unlock_out:",
            "\t/* Push furthest requested GP to leaf node and rcu_data structure. */",
            "\tif (ULONG_CMP_LT(gp_seq_req, rnp->gp_seq_needed)) {",
            "\t\tWRITE_ONCE(rnp_start->gp_seq_needed, rnp->gp_seq_needed);",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\t}",
            "\tif (rnp != rnp_start)",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\treturn ret;",
            "}",
            "static bool rcu_future_gp_cleanup(struct rcu_node *rnp)",
            "{",
            "\tbool needmore;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tneedmore = ULONG_CMP_LT(rnp->gp_seq, rnp->gp_seq_needed);",
            "\tif (!needmore)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq; /* Avoid counter wrap. */",
            "\ttrace_rcu_this_gp(rnp, rdp, rnp->gp_seq,",
            "\t\t\t  needmore ? TPS(\"CleanupMore\") : TPS(\"Cleanup\"));",
            "\treturn needmore;",
            "}",
            "static void swake_up_one_online_ipi(void *arg)",
            "{",
            "\tstruct swait_queue_head *wqh = arg;",
            "",
            "\tswake_up_one(wqh);",
            "}",
            "static void swake_up_one_online(struct swait_queue_head *wqh)",
            "{",
            "\tint cpu = get_cpu();",
            "",
            "\t/*",
            "\t * If called from rcutree_report_cpu_starting(), wake up",
            "\t * is dangerous that late in the CPU-down hotplug process. The",
            "\t * scheduler might queue an ignored hrtimer. Defer the wake up",
            "\t * to an online CPU instead.",
            "\t */",
            "\tif (unlikely(cpu_is_offline(cpu))) {",
            "\t\tint target;",
            "",
            "\t\ttarget = cpumask_any_and(housekeeping_cpumask(HK_TYPE_RCU),",
            "\t\t\t\t\t cpu_online_mask);",
            "",
            "\t\tsmp_call_function_single(target, swake_up_one_online_ipi,",
            "\t\t\t\t\t wqh, 0);",
            "\t\tput_cpu();",
            "\t} else {",
            "\t\tput_cpu();",
            "\t\tswake_up_one(wqh);",
            "\t}",
            "}"
          ],
          "function_name": "trace_rcu_this_gp, rcu_start_this_gp, rcu_future_gp_cleanup, swake_up_one_online_ipi, swake_up_one_online",
          "description": "实现RCU grace period事件追踪、新grace period启动逻辑及未来grace period清理机制，支持跨层级节点的同步状态传播。",
          "similarity": 0.5095516443252563
        }
      ]
    },
    {
      "source_file": "kernel/bpf/bpf_lru_list.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:00:28\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\bpf_lru_list.c`\n\n---\n\n# `bpf/bpf_lru_list.c` 技术文档\n\n## 1. 文件概述\n\n`bpf_lru_list.c` 实现了 BPF（Berkeley Packet Filter）子系统中用于管理 LRU（Least Recently Used，最近最少使用）缓存的通用机制。该机制主要用于 BPF map（如 `lru_hash` 类型）中高效地回收不活跃或未被引用的条目，以控制内存使用并提升缓存命中率。文件提供了基于双链表的活跃/非活跃 LRU 链表管理、本地（per-CPU）缓存支持、引用位（ref bit）跟踪以及自动老化和回收策略。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct bpf_lru_node`：LRU 节点，嵌入在 BPF map 条目中，包含链表指针、类型和引用标志。\n- `struct bpf_lru_list`：全局 LRU 链表结构，维护活跃（ACTIVE）和非活跃（INACTIVE）链表，以及各类节点计数。\n- `struct bpf_lru_locallist`：每个 CPU 的本地 LRU 链表，用于减少锁竞争，包含 FREE 和 PENDING 本地链表。\n- `struct bpf_lru`：LRU 控制结构，包含回调函数（如 `del_from_htab`）、扫描数量（`nr_scans`）等配置。\n\n### 主要函数\n- `__bpf_lru_list_rotate_active()`：轮转活跃链表，将带引用位的节点保留在活跃链表头部，无引用位的移至非活跃链表。\n- `__bpf_lru_list_rotate_inactive()`：轮转非活跃链表，将带引用位的节点提升回活跃链表。\n- `__bpf_lru_list_shrink_inactive()`：从非活跃链表尾部回收无引用位且可删除的节点到指定 free 链表。\n- `__bpf_lru_list_shrink()`：尝试正常回收失败后，强制从非活跃或活跃链表中删除节点（忽略引用位）。\n- `__bpf_lru_node_move()` / `__bpf_lru_node_move_in()` / `__bpf_lru_node_move_to_free()`：节点在不同链表间移动的内部辅助函数。\n- `get_next_cpu()`：用于遍历所有可能 CPU 的辅助函数。\n\n### 关键常量\n- `LOCAL_FREE_TARGET` / `PERCPU_FREE_TARGET`：本地和 per-CPU 回收目标数量（分别为 128 和 4）。\n- `LOCAL_NR_SCANS` / `PERCPU_NR_SCANS`：本地和 per-CPU 扫描上限，等于各自目标值。\n- `BPF_LOCAL_LIST_T_OFFSET`：本地链表类型的偏移量，用于区分全局 LRU 类型和本地类型。\n\n## 3. 关键实现\n\n### LRU 双链表模型\n采用经典的 **Active/Inactive 双链表模型**：\n- **活跃链表（ACTIVE）**：存放近期被访问或引用的节点。\n- **非活跃链表（INACTIVE）**：存放较久未被访问的节点，是回收的主要候选区域。\n- 节点首次插入时通常进入活跃链表；经过一次老化周期后，若无引用则移至非活跃链表。\n\n### 引用位（ref bit）机制\n- 每个 `bpf_lru_node` 包含一个 `ref` 字段（原子读写），表示该节点是否在最近被访问。\n- 在轮转过程中：\n  - 若节点 `ref == 1`，则清零并保留在活跃链表（或从非活跃提升至活跃）。\n  - 若 `ref == 0`，则可能被移至非活跃链表或直接回收。\n- 该机制避免了频繁移动热数据，提高了缓存效率。\n\n### 老化与回收策略\n- **轮转（Rotate）**：\n  - 定期调用 `__bpf_lru_list_rotate()`。\n  - 当非活跃链表长度小于活跃链表时，触发活跃链表轮转。\n  - 非活跃链表总是轮转，从 `next_inactive_rotation` 指针开始，避免每次都从头扫描。\n- **回收（Shrink）**：\n  - 优先从非活跃链表尾部回收无引用且可删除（通过 `del_from_htab` 回调确认）的节点。\n  - 若回收不足，强制从非活跃链表（优先）或活跃链表中删除节点，**忽略引用位**，确保内存压力下能释放资源。\n\n### 本地（Local）与 Per-CPU 优化\n- 支持 **本地链表类型**（`BPF_LRU_LOCAL_LIST_T_FREE` / `PENDING`），用于暂存待处理或刚释放的节点。\n- 通过 `IS_LOCAL_LIST_TYPE()` 宏区分本地与全局类型，防止非法移动。\n- 减少全局锁竞争，提升多核性能。\n\n### 安全移动与指针维护\n- 在移动节点时，若该节点恰好是 `next_inactive_rotation` 指针指向的对象，则自动将其前移，避免悬空指针。\n- 使用 `list_move()` 安全地在链表间转移节点。\n- 所有计数操作（`bpf_lru_list_count_inc/dec`）均带边界检查。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/cpumask.h>`：用于 CPU 遍历（`get_next_cpu`）。\n  - `<linux/spinlock.h>` 和 `<linux/percpu.h>`：支持 per-CPU 数据结构和同步。\n- **内部依赖**：\n  - 依赖 `bpf_lru_list.h` 中定义的数据结构和枚举（如 `enum bpf_lru_list_type`）。\n- **外部回调**：\n  - 通过 `lru->del_from_htab(lru->del_arg, node)` 回调通知上层（如 BPF map 实现）删除哈希表中的条目，实现 LRU 与具体数据结构的解耦。\n\n## 5. 使用场景\n\n- **BPF LRU Hash Map**：该文件是 `BPF_MAP_TYPE_LRU_HASH` 和 `BPF_MAP_TYPE_LRU_PERCPU_HASH` 等 map 类型的核心内存管理组件。\n- **内存压力下的自动回收**：当 map 达到容量上限或系统内存紧张时，触发 shrink 操作释放条目。\n- **高并发环境优化**：通过本地链表和引用位机制，在多核系统上高效管理缓存，减少锁争用。\n- **内核网络与跟踪子系统**：被用于 eBPF 程序中需要高效键值存储且自动淘汰旧数据的场景，如连接跟踪、统计聚合等。",
      "similarity": 0.49478721618652344,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 22,
          "end_line": 126,
          "content": [
            "static int get_next_cpu(int cpu)",
            "{",
            "\tcpu = cpumask_next(cpu, cpu_possible_mask);",
            "\tif (cpu >= nr_cpu_ids)",
            "\t\tcpu = cpumask_first(cpu_possible_mask);",
            "\treturn cpu;",
            "}",
            "static bool bpf_lru_node_is_ref(const struct bpf_lru_node *node)",
            "{",
            "\treturn READ_ONCE(node->ref);",
            "}",
            "static void bpf_lru_node_clear_ref(struct bpf_lru_node *node)",
            "{",
            "\tWRITE_ONCE(node->ref, 0);",
            "}",
            "static void bpf_lru_list_count_inc(struct bpf_lru_list *l,",
            "\t\t\t\t   enum bpf_lru_list_type type)",
            "{",
            "\tif (type < NR_BPF_LRU_LIST_COUNT)",
            "\t\tl->counts[type]++;",
            "}",
            "static void bpf_lru_list_count_dec(struct bpf_lru_list *l,",
            "\t\t\t\t   enum bpf_lru_list_type type)",
            "{",
            "\tif (type < NR_BPF_LRU_LIST_COUNT)",
            "\t\tl->counts[type]--;",
            "}",
            "static void __bpf_lru_node_move_to_free(struct bpf_lru_list *l,",
            "\t\t\t\t\tstruct bpf_lru_node *node,",
            "\t\t\t\t\tstruct list_head *free_list,",
            "\t\t\t\t\tenum bpf_lru_list_type tgt_free_type)",
            "{",
            "\tif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)))",
            "\t\treturn;",
            "",
            "\t/* If the removing node is the next_inactive_rotation candidate,",
            "\t * move the next_inactive_rotation pointer also.",
            "\t */",
            "\tif (&node->list == l->next_inactive_rotation)",
            "\t\tl->next_inactive_rotation = l->next_inactive_rotation->prev;",
            "",
            "\tbpf_lru_list_count_dec(l, node->type);",
            "",
            "\tnode->type = tgt_free_type;",
            "\tlist_move(&node->list, free_list);",
            "}",
            "static void __bpf_lru_node_move_in(struct bpf_lru_list *l,",
            "\t\t\t\t   struct bpf_lru_node *node,",
            "\t\t\t\t   enum bpf_lru_list_type tgt_type)",
            "{",
            "\tif (WARN_ON_ONCE(!IS_LOCAL_LIST_TYPE(node->type)) ||",
            "\t    WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(tgt_type)))",
            "\t\treturn;",
            "",
            "\tbpf_lru_list_count_inc(l, tgt_type);",
            "\tnode->type = tgt_type;",
            "\tbpf_lru_node_clear_ref(node);",
            "\tlist_move(&node->list, &l->lists[tgt_type]);",
            "}",
            "static void __bpf_lru_node_move(struct bpf_lru_list *l,",
            "\t\t\t\tstruct bpf_lru_node *node,",
            "\t\t\t\tenum bpf_lru_list_type tgt_type)",
            "{",
            "\tif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)) ||",
            "\t    WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(tgt_type)))",
            "\t\treturn;",
            "",
            "\tif (node->type != tgt_type) {",
            "\t\tbpf_lru_list_count_dec(l, node->type);",
            "\t\tbpf_lru_list_count_inc(l, tgt_type);",
            "\t\tnode->type = tgt_type;",
            "\t}",
            "\tbpf_lru_node_clear_ref(node);",
            "",
            "\t/* If the moving node is the next_inactive_rotation candidate,",
            "\t * move the next_inactive_rotation pointer also.",
            "\t */",
            "\tif (&node->list == l->next_inactive_rotation)",
            "\t\tl->next_inactive_rotation = l->next_inactive_rotation->prev;",
            "",
            "\tlist_move(&node->list, &l->lists[tgt_type]);",
            "}",
            "static bool bpf_lru_list_inactive_low(const struct bpf_lru_list *l)",
            "{",
            "\treturn l->counts[BPF_LRU_LIST_T_INACTIVE] <",
            "\t\tl->counts[BPF_LRU_LIST_T_ACTIVE];",
            "}",
            "static void __bpf_lru_list_rotate_active(struct bpf_lru *lru,",
            "\t\t\t\t\t struct bpf_lru_list *l)",
            "{",
            "\tstruct list_head *active = &l->lists[BPF_LRU_LIST_T_ACTIVE];",
            "\tstruct bpf_lru_node *node, *tmp_node, *first_node;",
            "\tunsigned int i = 0;",
            "",
            "\tfirst_node = list_first_entry(active, struct bpf_lru_node, list);",
            "\tlist_for_each_entry_safe_reverse(node, tmp_node, active, list) {",
            "\t\tif (bpf_lru_node_is_ref(node))",
            "\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);",
            "\t\telse",
            "\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_INACTIVE);",
            "",
            "\t\tif (++i == lru->nr_scans || node == first_node)",
            "\t\t\tbreak;",
            "\t}",
            "}"
          ],
          "function_name": "get_next_cpu, bpf_lru_node_is_ref, bpf_lru_node_clear_ref, bpf_lru_list_count_inc, bpf_lru_list_count_dec, __bpf_lru_node_move_to_free, __bpf_lru_node_move_in, __bpf_lru_node_move, bpf_lru_list_inactive_low, __bpf_lru_list_rotate_active",
          "description": "实现LRU节点状态迁移、计数维护及列表旋转逻辑，包括节点引用判断、类型转换、活跃/非活跃列表平衡操作。",
          "similarity": 0.5948585867881775
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 556,
          "end_line": 689,
          "content": [
            "void bpf_lru_push_free(struct bpf_lru *lru, struct bpf_lru_node *node)",
            "{",
            "\tif (lru->percpu)",
            "\t\tbpf_percpu_lru_push_free(lru, node);",
            "\telse",
            "\t\tbpf_common_lru_push_free(lru, node);",
            "}",
            "static void bpf_common_lru_populate(struct bpf_lru *lru, void *buf,",
            "\t\t\t\t    u32 node_offset, u32 elem_size,",
            "\t\t\t\t    u32 nr_elems)",
            "{",
            "\tstruct bpf_lru_list *l = &lru->common_lru.lru_list;",
            "\tu32 i;",
            "",
            "\tfor (i = 0; i < nr_elems; i++) {",
            "\t\tstruct bpf_lru_node *node;",
            "",
            "\t\tnode = (struct bpf_lru_node *)(buf + node_offset);",
            "\t\tnode->type = BPF_LRU_LIST_T_FREE;",
            "\t\tbpf_lru_node_clear_ref(node);",
            "\t\tlist_add(&node->list, &l->lists[BPF_LRU_LIST_T_FREE]);",
            "\t\tbuf += elem_size;",
            "\t}",
            "",
            "\tlru->target_free = clamp((nr_elems / num_possible_cpus()) / 2,",
            "\t\t\t\t 1, LOCAL_FREE_TARGET);",
            "}",
            "static void bpf_percpu_lru_populate(struct bpf_lru *lru, void *buf,",
            "\t\t\t\t    u32 node_offset, u32 elem_size,",
            "\t\t\t\t    u32 nr_elems)",
            "{",
            "\tu32 i, pcpu_entries;",
            "\tint cpu;",
            "\tstruct bpf_lru_list *l;",
            "",
            "\tpcpu_entries = nr_elems / num_possible_cpus();",
            "",
            "\ti = 0;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct bpf_lru_node *node;",
            "",
            "\t\tl = per_cpu_ptr(lru->percpu_lru, cpu);",
            "again:",
            "\t\tnode = (struct bpf_lru_node *)(buf + node_offset);",
            "\t\tnode->cpu = cpu;",
            "\t\tnode->type = BPF_LRU_LIST_T_FREE;",
            "\t\tbpf_lru_node_clear_ref(node);",
            "\t\tlist_add(&node->list, &l->lists[BPF_LRU_LIST_T_FREE]);",
            "\t\ti++;",
            "\t\tbuf += elem_size;",
            "\t\tif (i == nr_elems)",
            "\t\t\tbreak;",
            "\t\tif (i % pcpu_entries)",
            "\t\t\tgoto again;",
            "\t}",
            "}",
            "void bpf_lru_populate(struct bpf_lru *lru, void *buf, u32 node_offset,",
            "\t\t      u32 elem_size, u32 nr_elems)",
            "{",
            "\tif (lru->percpu)",
            "\t\tbpf_percpu_lru_populate(lru, buf, node_offset, elem_size,",
            "\t\t\t\t\tnr_elems);",
            "\telse",
            "\t\tbpf_common_lru_populate(lru, buf, node_offset, elem_size,",
            "\t\t\t\t\tnr_elems);",
            "}",
            "static void bpf_lru_locallist_init(struct bpf_lru_locallist *loc_l, int cpu)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < NR_BPF_LRU_LOCAL_LIST_T; i++)",
            "\t\tINIT_LIST_HEAD(&loc_l->lists[i]);",
            "",
            "\tloc_l->next_steal = cpu;",
            "",
            "\traw_spin_lock_init(&loc_l->lock);",
            "}",
            "static void bpf_lru_list_init(struct bpf_lru_list *l)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < NR_BPF_LRU_LIST_T; i++)",
            "\t\tINIT_LIST_HEAD(&l->lists[i]);",
            "",
            "\tfor (i = 0; i < NR_BPF_LRU_LIST_COUNT; i++)",
            "\t\tl->counts[i] = 0;",
            "",
            "\tl->next_inactive_rotation = &l->lists[BPF_LRU_LIST_T_INACTIVE];",
            "",
            "\traw_spin_lock_init(&l->lock);",
            "}",
            "int bpf_lru_init(struct bpf_lru *lru, bool percpu, u32 hash_offset,",
            "\t\t del_from_htab_func del_from_htab, void *del_arg)",
            "{",
            "\tint cpu;",
            "",
            "\tif (percpu) {",
            "\t\tlru->percpu_lru = alloc_percpu(struct bpf_lru_list);",
            "\t\tif (!lru->percpu_lru)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tstruct bpf_lru_list *l;",
            "",
            "\t\t\tl = per_cpu_ptr(lru->percpu_lru, cpu);",
            "\t\t\tbpf_lru_list_init(l);",
            "\t\t}",
            "\t\tlru->nr_scans = PERCPU_NR_SCANS;",
            "\t} else {",
            "\t\tstruct bpf_common_lru *clru = &lru->common_lru;",
            "",
            "\t\tclru->local_list = alloc_percpu(struct bpf_lru_locallist);",
            "\t\tif (!clru->local_list)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tstruct bpf_lru_locallist *loc_l;",
            "",
            "\t\t\tloc_l = per_cpu_ptr(clru->local_list, cpu);",
            "\t\t\tbpf_lru_locallist_init(loc_l, cpu);",
            "\t\t}",
            "",
            "\t\tbpf_lru_list_init(&clru->lru_list);",
            "\t\tlru->nr_scans = LOCAL_NR_SCANS;",
            "\t}",
            "",
            "\tlru->percpu = percpu;",
            "\tlru->del_from_htab = del_from_htab;",
            "\tlru->del_arg = del_arg;",
            "\tlru->hash_offset = hash_offset;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "bpf_lru_push_free, bpf_common_lru_populate, bpf_percpu_lru_populate, bpf_lru_populate, bpf_lru_locallist_init, bpf_lru_list_init, bpf_lru_init",
          "description": "初始化LRU系统各组件，包含全局/Per-CPU列表结构初始化、节点预填充配置及运行时参数设置。",
          "similarity": 0.568498969078064
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 697,
          "end_line": 703,
          "content": [
            "void bpf_lru_destroy(struct bpf_lru *lru)",
            "{",
            "\tif (lru->percpu)",
            "\t\tfree_percpu(lru->percpu_lru);",
            "\telse",
            "\t\tfree_percpu(lru->common_lru.local_list);",
            "}"
          ],
          "function_name": "bpf_lru_destroy",
          "description": "该代码段实现了一个用于销毁BPF LRU结构体的函数，核心功能是释放与per-CPU关联的LRU列表内存。函数根据`percpu`标志选择性地调用`free_percpu`释放`percpu_lru`或`common_lru.local_list`。由于代码仅展示内存释放逻辑且未包含完整结构体定义，存在上下文不完整的风险。",
          "similarity": 0.5272649526596069
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 1,
          "end_line": 21,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2016 Facebook",
            " */",
            "#include <linux/cpumask.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/percpu.h>",
            "",
            "#include \"bpf_lru_list.h\"",
            "",
            "#define LOCAL_FREE_TARGET\t\t(128)",
            "#define LOCAL_NR_SCANS\t\t\tLOCAL_FREE_TARGET",
            "",
            "#define PERCPU_FREE_TARGET\t\t(4)",
            "#define PERCPU_NR_SCANS\t\t\tPERCPU_FREE_TARGET",
            "",
            "/* Helpers to get the local list index */",
            "#define LOCAL_LIST_IDX(t)\t((t) - BPF_LOCAL_LIST_T_OFFSET)",
            "#define LOCAL_FREE_LIST_IDX\tLOCAL_LIST_IDX(BPF_LRU_LOCAL_LIST_T_FREE)",
            "#define LOCAL_PENDING_LIST_IDX\tLOCAL_LIST_IDX(BPF_LRU_LOCAL_LIST_T_PENDING)",
            "#define IS_LOCAL_LIST_TYPE(t)\t((t) >= BPF_LOCAL_LIST_T_OFFSET)",
            ""
          ],
          "function_name": null,
          "description": "定义BPF LRU本地列表类型及其相关宏，用于标识不同类型的本地列表节点（FREE/PENDING）及获取其索引。",
          "similarity": 0.5169888734817505
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 295,
          "end_line": 407,
          "content": [
            "static void __local_list_flush(struct bpf_lru_list *l,",
            "\t\t\t       struct bpf_lru_locallist *loc_l)",
            "{",
            "\tstruct bpf_lru_node *node, *tmp_node;",
            "",
            "\tlist_for_each_entry_safe_reverse(node, tmp_node,",
            "\t\t\t\t\t local_pending_list(loc_l), list) {",
            "\t\tif (bpf_lru_node_is_ref(node))",
            "\t\t\t__bpf_lru_node_move_in(l, node, BPF_LRU_LIST_T_ACTIVE);",
            "\t\telse",
            "\t\t\t__bpf_lru_node_move_in(l, node,",
            "\t\t\t\t\t       BPF_LRU_LIST_T_INACTIVE);",
            "\t}",
            "}",
            "static void bpf_lru_list_push_free(struct bpf_lru_list *l,",
            "\t\t\t\t   struct bpf_lru_node *node)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)))",
            "\t\treturn;",
            "",
            "\traw_spin_lock_irqsave(&l->lock, flags);",
            "\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_FREE);",
            "\traw_spin_unlock_irqrestore(&l->lock, flags);",
            "}",
            "static void bpf_lru_list_pop_free_to_local(struct bpf_lru *lru,",
            "\t\t\t\t\t   struct bpf_lru_locallist *loc_l)",
            "{",
            "\tstruct bpf_lru_list *l = &lru->common_lru.lru_list;",
            "\tstruct bpf_lru_node *node, *tmp_node;",
            "\tunsigned int nfree = 0;",
            "",
            "\traw_spin_lock(&l->lock);",
            "",
            "\t__local_list_flush(l, loc_l);",
            "",
            "\t__bpf_lru_list_rotate(lru, l);",
            "",
            "\tlist_for_each_entry_safe(node, tmp_node, &l->lists[BPF_LRU_LIST_T_FREE],",
            "\t\t\t\t list) {",
            "\t\t__bpf_lru_node_move_to_free(l, node, local_free_list(loc_l),",
            "\t\t\t\t\t    BPF_LRU_LOCAL_LIST_T_FREE);",
            "\t\tif (++nfree == lru->target_free)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tif (nfree < lru->target_free)",
            "\t\t__bpf_lru_list_shrink(lru, l, lru->target_free - nfree,",
            "\t\t\t\t      local_free_list(loc_l),",
            "\t\t\t\t      BPF_LRU_LOCAL_LIST_T_FREE);",
            "",
            "\traw_spin_unlock(&l->lock);",
            "}",
            "static void __local_list_add_pending(struct bpf_lru *lru,",
            "\t\t\t\t     struct bpf_lru_locallist *loc_l,",
            "\t\t\t\t     int cpu,",
            "\t\t\t\t     struct bpf_lru_node *node,",
            "\t\t\t\t     u32 hash)",
            "{",
            "\t*(u32 *)((void *)node + lru->hash_offset) = hash;",
            "\tnode->cpu = cpu;",
            "\tnode->type = BPF_LRU_LOCAL_LIST_T_PENDING;",
            "\tbpf_lru_node_clear_ref(node);",
            "\tlist_add(&node->list, local_pending_list(loc_l));",
            "}",
            "static void bpf_common_lru_push_free(struct bpf_lru *lru,",
            "\t\t\t\t     struct bpf_lru_node *node)",
            "{",
            "\tu8 node_type = READ_ONCE(node->type);",
            "\tunsigned long flags;",
            "",
            "\tif (WARN_ON_ONCE(node_type == BPF_LRU_LIST_T_FREE) ||",
            "\t    WARN_ON_ONCE(node_type == BPF_LRU_LOCAL_LIST_T_FREE))",
            "\t\treturn;",
            "",
            "\tif (node_type == BPF_LRU_LOCAL_LIST_T_PENDING) {",
            "\t\tstruct bpf_lru_locallist *loc_l;",
            "",
            "\t\tloc_l = per_cpu_ptr(lru->common_lru.local_list, node->cpu);",
            "",
            "\t\traw_spin_lock_irqsave(&loc_l->lock, flags);",
            "",
            "\t\tif (unlikely(node->type != BPF_LRU_LOCAL_LIST_T_PENDING)) {",
            "\t\t\traw_spin_unlock_irqrestore(&loc_l->lock, flags);",
            "\t\t\tgoto check_lru_list;",
            "\t\t}",
            "",
            "\t\tnode->type = BPF_LRU_LOCAL_LIST_T_FREE;",
            "\t\tbpf_lru_node_clear_ref(node);",
            "\t\tlist_move(&node->list, local_free_list(loc_l));",
            "",
            "\t\traw_spin_unlock_irqrestore(&loc_l->lock, flags);",
            "\t\treturn;",
            "\t}",
            "",
            "check_lru_list:",
            "\tbpf_lru_list_push_free(&lru->common_lru.lru_list, node);",
            "}",
            "static void bpf_percpu_lru_push_free(struct bpf_lru *lru,",
            "\t\t\t\t     struct bpf_lru_node *node)",
            "{",
            "\tstruct bpf_lru_list *l;",
            "\tunsigned long flags;",
            "",
            "\tl = per_cpu_ptr(lru->percpu_lru, node->cpu);",
            "",
            "\traw_spin_lock_irqsave(&l->lock, flags);",
            "",
            "\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_FREE);",
            "",
            "\traw_spin_unlock_irqrestore(&l->lock, flags);",
            "}"
          ],
          "function_name": "__local_list_flush, bpf_lru_list_push_free, bpf_lru_list_pop_free_to_local, __local_list_add_pending, bpf_common_lru_push_free, bpf_percpu_lru_push_free",
          "description": "管理Per-CPU本地列表的刷新与节点分发，实现空闲节点向本地列表迁移及跨CPU的节点调度控制。",
          "similarity": 0.38083457946777344
        }
      ]
    }
  ]
}