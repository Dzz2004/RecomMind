{
  "query": "Linux内核管道通信的缓冲区分配",
  "timestamp": "2025-12-26 01:15:48",
  "retrieved_files": [
    {
      "source_file": "kernel/watch_queue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:50:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `watch_queue.c`\n\n---\n\n# watch_queue.c 技术文档\n\n## 文件概述\n\n`watch_queue.c` 实现了 Linux 内核中的**监视队列**（Watch Queue）机制，这是一种基于管道（pipe）构建的通用事件通知系统。该机制允许内核子系统（如文件系统、密钥管理、设备驱动等）向用户空间异步发送结构化通知。用户空间通过创建特殊类型的管道并关联监视队列，即可接收来自内核的各类事件通知。该文件定义了通知的投递、过滤、缓冲管理及与管道集成的核心逻辑。\n\n## 核心功能\n\n### 主要函数\n\n- **`__post_watch_notification()`**  \n  核心通知投递函数。遍历指定 `watch_list` 中所有匹配 `id` 的监视器（`watch`），对每个关联的 `watch_queue` 应用过滤规则、安全检查，并将通知写入底层管道。\n\n- **`post_one_notification()`**  \n  将单个通知写入指定 `watch_queue` 的底层管道缓冲区。负责从预分配的通知页中获取空闲槽位、填充数据、更新管道头指针并唤醒等待读取的进程。\n\n- **`filter_watch_notification()`**  \n  根据 `watch_filter` 中的类型、子类型和信息掩码规则，判断是否允许特定通知通过。\n\n- **`watch_queue_set_size()`**  \n  为监视队列分配预分配的通知缓冲区（页数组和位图），并调整底层管道的环形缓冲区大小。\n\n- **`watch_queue_pipe_buf_release()`**  \n  管道缓冲区释放回调。当用户空间读取完通知后，将对应的通知槽位在位图中标记为空闲，供后续复用。\n\n### 关键数据结构\n\n- **`struct watch_queue`**  \n  表示一个监视队列，包含：\n  - 指向底层 `pipe_inode_info` 的指针\n  - 预分配的通知页数组（`notes`）\n  - 通知槽位空闲位图（`notes_bitmap`）\n  - 通知过滤器（`filter`）\n  - 保护锁（`lock`）\n\n- **`struct watch_notification`**  \n  通用通知记录格式，包含类型（`type`）、子类型（`subtype`）、信息字段（`info`，含长度和ID）及可变负载。\n\n- **`struct watch_filter` / `struct watch_type_filter`**  \n  定义通知过滤规则，支持按类型、子类型及信息字段的位掩码进行精确过滤。\n\n- **`watch_queue_pipe_buf_ops`**  \n  自定义的 `pipe_buf_operations`，用于管理监视队列专用管道缓冲区的生命周期。\n\n## 关键实现\n\n### 基于管道的通知传输\n- 监视队列复用内核管道（`pipe_inode_info`）作为通知传输通道，利用其成熟的读写、轮询、异步通知机制。\n- 通过自定义 `pipe_buf_operations`（`watch_queue_pipe_buf_ops`）实现通知槽位的回收：当用户读取通知后，`release` 回调将对应槽位在 `notes_bitmap` 中置位，标记为空闲。\n\n### 预分配通知缓冲区\n- 通知数据存储在预分配的内核页（`notes`）中，每页划分为多个固定大小（128字节）的槽位（`WATCH_QUEUE_NOTE_SIZE`）。\n- 使用位图（`notes_bitmap`）跟踪槽位使用状态，1 表示空闲。投递通知时通过 `find_first_bit()` 快速查找空闲槽位。\n- 缓冲区大小由用户通过 `watch_queue_set_size()` 设置（1-512个通知），并受管道缓冲区配额限制。\n\n### 通知投递流程\n1. **匹配监视器**：遍历 `watch_list`，查找 `id` 匹配的 `watch`。\n2. **应用过滤**：若队列配置了过滤器，调用 `filter_watch_notification()` 决定是否丢弃。\n3. **安全检查**：调用 LSM 钩子 `security_post_notification()` 进行权限验证。\n4. **写入管道**：\n   - 获取空闲通知槽位，复制通知数据。\n   - 构造 `pipe_buffer` 指向该槽位，设置自定义操作集。\n   - 更新管道 `head` 指针，唤醒等待读取的进程。\n   - 若缓冲区满，标记前一个缓冲区为 `PIPE_BUF_FLAG_LOSS` 表示丢包。\n\n### 并发与同步\n- **RCU 保护**：`watch_list` 和 `watch_queue` 的访问通过 RCU 机制保护，确保遍历时结构体不被释放。\n- **自旋锁**：\n  - `wqueue->lock`：保护 `wqueue` 状态（如 `pipe` 指针有效性）。\n  - `pipe->rd_wait.lock`：保护管道环形缓冲区的读写操作。\n- **原子操作**：管道 `head` 指针使用 `smp_store_release()` 更新，确保与 `pipe_read()` 的同步。\n\n## 依赖关系\n\n- **管道子系统**（`fs/pipe.c`）  \n  依赖管道的核心数据结构（`pipe_inode_info`、`pipe_buffer`）和操作接口（`pipe_buf()`、`pipe_full()`、`generic_pipe_buf_*`）。\n\n- **内存管理**  \n  使用 `alloc_page()`、`kmap_atomic()` 管理通知缓冲区页，`bitmap_alloc()` 管理槽位位图。\n\n- **安全模块**（LSM）  \n  通过 `security_post_notification()` 钩子集成安全策略。\n\n- **用户空间接口**  \n  与 `fs/watch_queue.c` 中的系统调用（如 `watch_queue_set_size()`）协同工作，后者负责创建监视队列并与管道关联。\n\n- **头文件依赖**  \n  `linux/watch_queue.h`（核心数据结构定义）、`linux/pipe_fs_i.h`（管道内部接口）。\n\n## 使用场景\n\n- **文件系统事件监控**  \n  如 `fsnotify` 子系统可通过监视队列向用户空间报告文件访问、修改等事件。\n\n- **密钥管理通知**  \n  内核密钥环（`KEYS`）子系统使用该机制通知密钥状态变更（如过期、撤销）。\n\n- **设备事件上报**  \n  设备驱动可利用监视队列异步上报硬件状态变化或错误事件。\n\n- **通用内核事件分发**  \n  任何需要向特权用户空间守护进程（如 `systemd`）发送结构化事件的内核子系统均可集成此机制。\n\n- **用户空间消费**  \n  应用程序通过 `open(\"/dev/watch_queue\")` 获取监视队列文件描述符，调用 `ioctl()` 设置缓冲区大小和过滤器，然后像读取普通管道一样接收通知。",
      "similarity": 0.6607970595359802,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/watch_queue.c",
          "start_line": 42,
          "end_line": 154,
          "content": [
            "static inline bool lock_wqueue(struct watch_queue *wqueue)",
            "{",
            "\tspin_lock_bh(&wqueue->lock);",
            "\tif (unlikely(!wqueue->pipe)) {",
            "\t\tspin_unlock_bh(&wqueue->lock);",
            "\t\treturn false;",
            "\t}",
            "\treturn true;",
            "}",
            "static inline void unlock_wqueue(struct watch_queue *wqueue)",
            "{",
            "\tspin_unlock_bh(&wqueue->lock);",
            "}",
            "static void watch_queue_pipe_buf_release(struct pipe_inode_info *pipe,",
            "\t\t\t\t\t struct pipe_buffer *buf)",
            "{",
            "\tstruct watch_queue *wqueue = (struct watch_queue *)buf->private;",
            "\tstruct page *page;",
            "\tunsigned int bit;",
            "",
            "\t/* We need to work out which note within the page this refers to, but",
            "\t * the note might have been maximum size, so merely ANDing the offset",
            "\t * off doesn't work.  OTOH, the note must've been more than zero size.",
            "\t */",
            "\tbit = buf->offset + buf->len;",
            "\tif ((bit & (WATCH_QUEUE_NOTE_SIZE - 1)) == 0)",
            "\t\tbit -= WATCH_QUEUE_NOTE_SIZE;",
            "\tbit /= WATCH_QUEUE_NOTE_SIZE;",
            "",
            "\tpage = buf->page;",
            "\tbit += page->index;",
            "",
            "\tset_bit(bit, wqueue->notes_bitmap);",
            "\tgeneric_pipe_buf_release(pipe, buf);",
            "}",
            "static bool post_one_notification(struct watch_queue *wqueue,",
            "\t\t\t\t  struct watch_notification *n)",
            "{",
            "\tvoid *p;",
            "\tstruct pipe_inode_info *pipe = wqueue->pipe;",
            "\tstruct pipe_buffer *buf;",
            "\tstruct page *page;",
            "\tunsigned int head, tail, note, offset, len;",
            "\tbool done = false;",
            "",
            "\tspin_lock_irq(&pipe->rd_wait.lock);",
            "",
            "\thead = pipe->head;",
            "\ttail = pipe->tail;",
            "\tif (pipe_full(head, tail, pipe->ring_size))",
            "\t\tgoto lost;",
            "",
            "\tnote = find_first_bit(wqueue->notes_bitmap, wqueue->nr_notes);",
            "\tif (note >= wqueue->nr_notes)",
            "\t\tgoto lost;",
            "",
            "\tpage = wqueue->notes[note / WATCH_QUEUE_NOTES_PER_PAGE];",
            "\toffset = note % WATCH_QUEUE_NOTES_PER_PAGE * WATCH_QUEUE_NOTE_SIZE;",
            "\tget_page(page);",
            "\tlen = n->info & WATCH_INFO_LENGTH;",
            "\tp = kmap_atomic(page);",
            "\tmemcpy(p + offset, n, len);",
            "\tkunmap_atomic(p);",
            "",
            "\tbuf = pipe_buf(pipe, head);",
            "\tbuf->page = page;",
            "\tbuf->private = (unsigned long)wqueue;",
            "\tbuf->ops = &watch_queue_pipe_buf_ops;",
            "\tbuf->offset = offset;",
            "\tbuf->len = len;",
            "\tbuf->flags = PIPE_BUF_FLAG_WHOLE;",
            "\tsmp_store_release(&pipe->head, head + 1); /* vs pipe_read() */",
            "",
            "\tif (!test_and_clear_bit(note, wqueue->notes_bitmap)) {",
            "\t\tspin_unlock_irq(&pipe->rd_wait.lock);",
            "\t\tBUG();",
            "\t}",
            "\twake_up_interruptible_sync_poll_locked(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);",
            "\tdone = true;",
            "",
            "out:",
            "\tspin_unlock_irq(&pipe->rd_wait.lock);",
            "\tif (done)",
            "\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);",
            "\treturn done;",
            "",
            "lost:",
            "\tbuf = pipe_buf(pipe, head - 1);",
            "\tbuf->flags |= PIPE_BUF_FLAG_LOSS;",
            "\tgoto out;",
            "}",
            "static bool filter_watch_notification(const struct watch_filter *wf,",
            "\t\t\t\t      const struct watch_notification *n)",
            "{",
            "\tconst struct watch_type_filter *wt;",
            "\tunsigned int st_bits = sizeof(wt->subtype_filter[0]) * 8;",
            "\tunsigned int st_index = n->subtype / st_bits;",
            "\tunsigned int st_bit = 1U << (n->subtype % st_bits);",
            "\tint i;",
            "",
            "\tif (!test_bit(n->type, wf->type_filter))",
            "\t\treturn false;",
            "",
            "\tfor (i = 0; i < wf->nr_filters; i++) {",
            "\t\twt = &wf->filters[i];",
            "\t\tif (n->type == wt->type &&",
            "\t\t    (wt->subtype_filter[st_index] & st_bit) &&",
            "\t\t    (n->info & wt->info_mask) == wt->info_filter)",
            "\t\t\treturn true;",
            "\t}",
            "",
            "\treturn false; /* If there is a filter, the default is to reject. */",
            "}"
          ],
          "function_name": "lock_wqueue, unlock_wqueue, watch_queue_pipe_buf_release, post_one_notification, filter_watch_notification",
          "description": "实现了watch_queue的锁操作、缓冲区释放、通知提交及过滤逻辑。lock_wqueue/unlock_wqueue用于保护队列访问，watch_queue_pipe_buf_release处理缓冲区回收并更新位图，post_one_notification将通知数据写入管道，filter_watch_notification进行类型和子类型的匹配判断。",
          "similarity": 0.5634740591049194
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/watch_queue.c",
          "start_line": 193,
          "end_line": 304,
          "content": [
            "void __post_watch_notification(struct watch_list *wlist,",
            "\t\t\t       struct watch_notification *n,",
            "\t\t\t       const struct cred *cred,",
            "\t\t\t       u64 id)",
            "{",
            "\tconst struct watch_filter *wf;",
            "\tstruct watch_queue *wqueue;",
            "\tstruct watch *watch;",
            "",
            "\tif (((n->info & WATCH_INFO_LENGTH) >> WATCH_INFO_LENGTH__SHIFT) == 0) {",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\trcu_read_lock();",
            "",
            "\thlist_for_each_entry_rcu(watch, &wlist->watchers, list_node) {",
            "\t\tif (watch->id != id)",
            "\t\t\tcontinue;",
            "\t\tn->info &= ~WATCH_INFO_ID;",
            "\t\tn->info |= watch->info_id;",
            "",
            "\t\twqueue = rcu_dereference(watch->queue);",
            "\t\twf = rcu_dereference(wqueue->filter);",
            "\t\tif (wf && !filter_watch_notification(wf, n))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (security_post_notification(watch->cred, cred, n) < 0)",
            "\t\t\tcontinue;",
            "",
            "\t\tif (lock_wqueue(wqueue)) {",
            "\t\t\tpost_one_notification(wqueue, n);",
            "\t\t\tunlock_wqueue(wqueue);",
            "\t\t}",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "}",
            "long watch_queue_set_size(struct pipe_inode_info *pipe, unsigned int nr_notes)",
            "{",
            "\tstruct watch_queue *wqueue = pipe->watch_queue;",
            "\tstruct page **pages;",
            "\tunsigned long *bitmap;",
            "\tunsigned long user_bufs;",
            "\tint ret, i, nr_pages;",
            "",
            "\tif (!wqueue)",
            "\t\treturn -ENODEV;",
            "\tif (wqueue->notes)",
            "\t\treturn -EBUSY;",
            "",
            "\tif (nr_notes < 1 ||",
            "\t    nr_notes > 512) /* TODO: choose a better hard limit */",
            "\t\treturn -EINVAL;",
            "",
            "\tnr_pages = (nr_notes + WATCH_QUEUE_NOTES_PER_PAGE - 1);",
            "\tnr_pages /= WATCH_QUEUE_NOTES_PER_PAGE;",
            "\tuser_bufs = account_pipe_buffers(pipe->user, pipe->nr_accounted, nr_pages);",
            "",
            "\tif (nr_pages > pipe->max_usage &&",
            "\t    (too_many_pipe_buffers_hard(user_bufs) ||",
            "\t     too_many_pipe_buffers_soft(user_bufs)) &&",
            "\t    pipe_is_unprivileged_user()) {",
            "\t\tret = -EPERM;",
            "\t\tgoto error;",
            "\t}",
            "",
            "\tnr_notes = nr_pages * WATCH_QUEUE_NOTES_PER_PAGE;",
            "\tret = pipe_resize_ring(pipe, roundup_pow_of_two(nr_notes));",
            "\tif (ret < 0)",
            "\t\tgoto error;",
            "",
            "\t/*",
            "\t * pipe_resize_ring() does not update nr_accounted for watch_queue",
            "\t * pipes, because the above vastly overprovisions. Set nr_accounted on",
            "\t * and max_usage this pipe to the number that was actually charged to",
            "\t * the user above via account_pipe_buffers.",
            "\t */",
            "\tpipe->max_usage = nr_pages;",
            "\tpipe->nr_accounted = nr_pages;",
            "",
            "\tret = -ENOMEM;",
            "\tpages = kcalloc(sizeof(struct page *), nr_pages, GFP_KERNEL);",
            "\tif (!pages)",
            "\t\tgoto error;",
            "",
            "\tfor (i = 0; i < nr_pages; i++) {",
            "\t\tpages[i] = alloc_page(GFP_KERNEL);",
            "\t\tif (!pages[i])",
            "\t\t\tgoto error_p;",
            "\t\tpages[i]->index = i * WATCH_QUEUE_NOTES_PER_PAGE;",
            "\t}",
            "",
            "\tbitmap = bitmap_alloc(nr_notes, GFP_KERNEL);",
            "\tif (!bitmap)",
            "\t\tgoto error_p;",
            "",
            "\tbitmap_fill(bitmap, nr_notes);",
            "\twqueue->notes = pages;",
            "\twqueue->notes_bitmap = bitmap;",
            "\twqueue->nr_pages = nr_pages;",
            "\twqueue->nr_notes = nr_notes;",
            "\treturn 0;",
            "",
            "error_p:",
            "\twhile (--i >= 0)",
            "\t\t__free_page(pages[i]);",
            "\tkfree(pages);",
            "error:",
            "\t(void) account_pipe_buffers(pipe->user, nr_pages, pipe->nr_accounted);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__post_watch_notification, watch_queue_set_size",
          "description": "__post_watch_notification遍历watch列表并应用过滤器后提交通知，watch_queue_set_size动态调整管道容量，通过计算所需页数和位图分配，限制最大容量为512个笔记，支持扩展性需求。",
          "similarity": 0.5009714961051941
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/watch_queue.c",
          "start_line": 315,
          "end_line": 422,
          "content": [
            "long watch_queue_set_filter(struct pipe_inode_info *pipe,",
            "\t\t\t    struct watch_notification_filter __user *_filter)",
            "{",
            "\tstruct watch_notification_type_filter *tf;",
            "\tstruct watch_notification_filter filter;",
            "\tstruct watch_type_filter *q;",
            "\tstruct watch_filter *wfilter;",
            "\tstruct watch_queue *wqueue = pipe->watch_queue;",
            "\tint ret, nr_filter = 0, i;",
            "",
            "\tif (!wqueue)",
            "\t\treturn -ENODEV;",
            "",
            "\tif (!_filter) {",
            "\t\t/* Remove the old filter */",
            "\t\twfilter = NULL;",
            "\t\tgoto set;",
            "\t}",
            "",
            "\t/* Grab the user's filter specification */",
            "\tif (copy_from_user(&filter, _filter, sizeof(filter)) != 0)",
            "\t\treturn -EFAULT;",
            "\tif (filter.nr_filters == 0 ||",
            "\t    filter.nr_filters > 16 ||",
            "\t    filter.__reserved != 0)",
            "\t\treturn -EINVAL;",
            "",
            "\ttf = memdup_array_user(_filter->filters, filter.nr_filters, sizeof(*tf));",
            "\tif (IS_ERR(tf))",
            "\t\treturn PTR_ERR(tf);",
            "",
            "\tret = -EINVAL;",
            "\tfor (i = 0; i < filter.nr_filters; i++) {",
            "\t\tif ((tf[i].info_filter & ~tf[i].info_mask) ||",
            "\t\t    tf[i].info_mask & WATCH_INFO_LENGTH)",
            "\t\t\tgoto err_filter;",
            "\t\t/* Ignore any unknown types */",
            "\t\tif (tf[i].type >= WATCH_TYPE__NR)",
            "\t\t\tcontinue;",
            "\t\tnr_filter++;",
            "\t}",
            "",
            "\t/* Now we need to build the internal filter from only the relevant",
            "\t * user-specified filters.",
            "\t */",
            "\tret = -ENOMEM;",
            "\twfilter = kzalloc(struct_size(wfilter, filters, nr_filter), GFP_KERNEL);",
            "\tif (!wfilter)",
            "\t\tgoto err_filter;",
            "\twfilter->nr_filters = nr_filter;",
            "",
            "\tq = wfilter->filters;",
            "\tfor (i = 0; i < filter.nr_filters; i++) {",
            "\t\tif (tf[i].type >= WATCH_TYPE__NR)",
            "\t\t\tcontinue;",
            "",
            "\t\tq->type\t\t\t= tf[i].type;",
            "\t\tq->info_filter\t\t= tf[i].info_filter;",
            "\t\tq->info_mask\t\t= tf[i].info_mask;",
            "\t\tq->subtype_filter[0]\t= tf[i].subtype_filter[0];",
            "\t\t__set_bit(q->type, wfilter->type_filter);",
            "\t\tq++;",
            "\t}",
            "",
            "\tkfree(tf);",
            "set:",
            "\tpipe_lock(pipe);",
            "\twfilter = rcu_replace_pointer(wqueue->filter, wfilter,",
            "\t\t\t\t      lockdep_is_held(&pipe->mutex));",
            "\tpipe_unlock(pipe);",
            "\tif (wfilter)",
            "\t\tkfree_rcu(wfilter, rcu);",
            "\treturn 0;",
            "",
            "err_filter:",
            "\tkfree(tf);",
            "\treturn ret;",
            "}",
            "static void __put_watch_queue(struct kref *kref)",
            "{",
            "\tstruct watch_queue *wqueue =",
            "\t\tcontainer_of(kref, struct watch_queue, usage);",
            "\tstruct watch_filter *wfilter;",
            "\tint i;",
            "",
            "\tfor (i = 0; i < wqueue->nr_pages; i++)",
            "\t\t__free_page(wqueue->notes[i]);",
            "\tkfree(wqueue->notes);",
            "\tbitmap_free(wqueue->notes_bitmap);",
            "",
            "\twfilter = rcu_access_pointer(wqueue->filter);",
            "\tif (wfilter)",
            "\t\tkfree_rcu(wfilter, rcu);",
            "\tkfree_rcu(wqueue, rcu);",
            "}",
            "void put_watch_queue(struct watch_queue *wqueue)",
            "{",
            "\tkref_put(&wqueue->usage, __put_watch_queue);",
            "}",
            "static void free_watch(struct rcu_head *rcu)",
            "{",
            "\tstruct watch *watch = container_of(rcu, struct watch, rcu);",
            "",
            "\tput_watch_queue(rcu_access_pointer(watch->queue));",
            "\tatomic_dec(&watch->cred->user->nr_watches);",
            "\tput_cred(watch->cred);",
            "\tkfree(watch);",
            "}"
          ],
          "function_name": "watch_queue_set_filter, __put_watch_queue, put_watch_queue, free_watch",
          "description": "watch_queue_set_filter设置过滤规则并转换为内核内部结构，__put_watch_queue释放watch_queue相关资源包括页面、位图和过滤器，put_watch_queue通过引用计数管理watch_queue生命周期，free_watch执行RCU回调完成最终释放。",
          "similarity": 0.4694702625274658
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/watch_queue.c",
          "start_line": 1,
          "end_line": 41,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/* Watch queue and general notification mechanism, built on pipes",
            " *",
            " * Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.",
            " * Written by David Howells (dhowells@redhat.com)",
            " *",
            " * See Documentation/core-api/watch_queue.rst",
            " */",
            "",
            "#define pr_fmt(fmt) \"watchq: \" fmt",
            "#include <linux/module.h>",
            "#include <linux/init.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/printk.h>",
            "#include <linux/miscdevice.h>",
            "#include <linux/fs.h>",
            "#include <linux/mm.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/poll.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/file.h>",
            "#include <linux/security.h>",
            "#include <linux/cred.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/watch_queue.h>",
            "#include <linux/pipe_fs_i.h>",
            "",
            "MODULE_DESCRIPTION(\"Watch queue\");",
            "MODULE_AUTHOR(\"Red Hat, Inc.\");",
            "",
            "#define WATCH_QUEUE_NOTE_SIZE 128",
            "#define WATCH_QUEUE_NOTES_PER_PAGE (PAGE_SIZE / WATCH_QUEUE_NOTE_SIZE)",
            "",
            "/*",
            " * This must be called under the RCU read-lock, which makes",
            " * sure that the wqueue still exists. It can then take the lock,",
            " * and check that the wqueue hasn't been destroyed, which in",
            " * turn makes sure that the notification pipe still exists.",
            " */"
          ],
          "function_name": null,
          "description": "定义了watch_queue模块的头部信息，包含常量WATCH_QUEUE_NOTE_SIZE和NOTES_PER_PAGE，声明模块许可证及作者信息，并引入相关内核头文件，为后续实现提供基础框架。",
          "similarity": 0.46907296776771545
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/watch_queue.c",
          "start_line": 602,
          "end_line": 680,
          "content": [
            "void watch_queue_clear(struct watch_queue *wqueue)",
            "{",
            "\tstruct watch_list *wlist;",
            "\tstruct watch *watch;",
            "\tbool release;",
            "",
            "\trcu_read_lock();",
            "\tspin_lock_bh(&wqueue->lock);",
            "",
            "\t/*",
            "\t * This pipe can be freed by callers like free_pipe_info().",
            "\t * Removing this reference also prevents new notifications.",
            "\t */",
            "\twqueue->pipe = NULL;",
            "",
            "\twhile (!hlist_empty(&wqueue->watches)) {",
            "\t\twatch = hlist_entry(wqueue->watches.first, struct watch, queue_node);",
            "\t\thlist_del_init_rcu(&watch->queue_node);",
            "\t\t/* We now own a ref on the watch. */",
            "\t\tspin_unlock_bh(&wqueue->lock);",
            "",
            "\t\t/* We can't do the next bit under the queue lock as we need to",
            "\t\t * get the list lock - which would cause a deadlock if someone",
            "\t\t * was removing from the opposite direction at the same time or",
            "\t\t * posting a notification.",
            "\t\t */",
            "\t\twlist = rcu_dereference(watch->watch_list);",
            "\t\tif (wlist) {",
            "\t\t\tvoid (*release_watch)(struct watch *);",
            "",
            "\t\t\tspin_lock(&wlist->lock);",
            "",
            "\t\t\trelease = !hlist_unhashed(&watch->list_node);",
            "\t\t\tif (release) {",
            "\t\t\t\thlist_del_init_rcu(&watch->list_node);",
            "\t\t\t\trcu_assign_pointer(watch->watch_list, NULL);",
            "",
            "\t\t\t\t/* We now own a second ref on the watch. */",
            "\t\t\t}",
            "",
            "\t\t\trelease_watch = wlist->release_watch;",
            "\t\t\tspin_unlock(&wlist->lock);",
            "",
            "\t\t\tif (release) {",
            "\t\t\t\tif (release_watch) {",
            "\t\t\t\t\trcu_read_unlock();",
            "\t\t\t\t\t/* This might need to call dput(), so",
            "\t\t\t\t\t * we have to drop all the locks.",
            "\t\t\t\t\t */",
            "\t\t\t\t\t(*release_watch)(watch);",
            "\t\t\t\t\trcu_read_lock();",
            "\t\t\t\t}",
            "\t\t\t\tput_watch(watch);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tput_watch(watch);",
            "\t\tspin_lock_bh(&wqueue->lock);",
            "\t}",
            "",
            "\tspin_unlock_bh(&wqueue->lock);",
            "\trcu_read_unlock();",
            "}",
            "int watch_queue_init(struct pipe_inode_info *pipe)",
            "{",
            "\tstruct watch_queue *wqueue;",
            "",
            "\twqueue = kzalloc(sizeof(*wqueue), GFP_KERNEL);",
            "\tif (!wqueue)",
            "\t\treturn -ENOMEM;",
            "",
            "\twqueue->pipe = pipe;",
            "\tkref_init(&wqueue->usage);",
            "\tspin_lock_init(&wqueue->lock);",
            "\tINIT_HLIST_HEAD(&wqueue->watches);",
            "",
            "\tpipe->watch_queue = wqueue;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "watch_queue_clear, watch_queue_init",
          "description": "该代码实现了监视队列的初始化与清理功能。  \n`watch_queue_clear`通过RCU和自旋锁机制安全地移除所有监视项并释放资源，`watch_queue_init`初始化监视队列结构并绑定至管道对象。  \n上下文不完整：`release_watch`等关键函数依赖外部定义，部分RCU回调逻辑未完全展示。",
          "similarity": 0.4609728753566742
        }
      ]
    },
    {
      "source_file": "kernel/relay.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:52:35\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `relay.c`\n\n---\n\n# relay.c 技术文档\n\n## 1. 文件概述\n\n`relay.c` 实现了 Linux 内核中 **relay 通道（relay channel）** 的核心功能，用于高效地将内核空间的数据流式传输到用户空间。该机制通过预分配的环形缓冲区（由多个子缓冲区组成）实现零拷贝或最小拷贝的数据传递，特别适用于高性能追踪（tracing）、日志记录和监控等场景。用户空间可通过标准文件操作（如 `mmap`、`read`）访问这些缓冲区。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct rchan`：relay 通道的主结构体，包含通道配置、回调函数和 per-CPU 缓冲区指针。\n- `struct rchan_buf`：单个 CPU 的 relay 缓冲区结构体，包含实际数据缓冲区、状态信息（生产/消费计数器）和同步原语。\n\n### 关键函数\n- **缓冲区管理**：\n  - `relay_create_buf()`：为指定通道创建并初始化 per-CPU 缓冲区。\n  - `relay_destroy_buf()`：销毁缓冲区并释放资源。\n  - `relay_alloc_buf()`：分配物理页面并映射为连续虚拟地址的缓冲区。\n- **内存映射**：\n  - `relay_mmap_buf()`：将内核缓冲区映射到用户进程地址空间。\n  - `relay_buf_fault()`：处理用户空间访问映射区域时的缺页异常。\n- **状态控制**：\n  - `relay_reset()`：重置通道状态，清空所有缓冲区数据。\n  - `relay_buf_empty()` / `relay_buf_full()`：检查缓冲区是否为空或已满。\n- **资源回收**：\n  - `relay_destroy_channel()`：通过 `kref` 引用计数释放通道结构。\n  - `relay_remove_buf()`：通过 `kref` 释放缓冲区结构。\n- **辅助功能**：\n  - `wakeup_readers()`：通过 `irq_work` 机制唤醒等待读取数据的用户进程。\n\n## 3. 关键实现\n\n### 缓冲区分配策略\n- 使用 `alloc_page()` 分配离散物理页面，通过 `vmap()` 建立连续虚拟地址映射，避免大块连续物理内存分配失败。\n- 页面指针数组 (`page_array`) 用于管理物理页面，支持高效的 `vunmap()` 释放。\n\n### 用户空间映射机制\n- 通过 `vm_operations_struct.fault` 回调 (`relay_buf_fault`) 实现按需映射：用户访问映射区域时，动态将内核 `vmalloc` 区域的页面映射到用户页表。\n- 设置 `VM_DONTEXPAND` 标志防止用户空间扩展映射区域。\n\n### 环形缓冲区管理\n- 采用 **子缓冲区（subbuffer）** 作为基本单元，通过 `subbufs_produced` 和 `subbufs_consumed` 计数器实现生产者-消费者模型。\n- `subbuf_start` 回调允许用户自定义子缓冲区切换逻辑（如添加头部信息）。\n\n### 并发与同步\n- **Per-CPU 缓冲区**：每个 CPU 独立缓冲区避免锁竞争，提升多核性能。\n- **延迟唤醒**：使用 `irq_work` 机制将唤醒操作推迟到软中断上下文，避免在硬中断中调用 `wake_up_interruptible()`。\n- **引用计数**：通过 `kref` 确保通道和缓冲区在异步操作（如文件关闭）中安全释放。\n\n### CPU 热插拔支持\n- 全局链表 `relay_channels` 跟踪所有打开的通道，配合 `relay_channels_mutex` 锁，在 CPU 热插拔事件中动态创建/销毁 per-CPU 缓冲区。\n\n## 4. 依赖关系\n\n- **内存管理**：依赖 `vmalloc`、`vmap`/`vunmap`、`alloc_page` 等内存分配接口。\n- **同步原语**：使用 `mutex`（`relay_channels_mutex`）、`waitqueue`（`read_wait`）、`irq_work` 和 `kref`。\n- **CPU 热插拔**：通过 `for_each_possible_cpu` 和 per-CPU 变量 (`per_cpu_ptr`) 管理多核资源。\n- **VFS 层**：与文件系统交互（`mmap`、`splice`），但具体文件操作在 `relayfs` 或 `debugfs` 中实现。\n- **导出符号**：`relay_buf_full()` 通过 `EXPORT_SYMBOL_GPL` 供其他内核模块使用。\n\n## 5. 使用场景\n\n- **内核追踪系统**：如 `ftrace`、`perf` 使用 relay 通道高效导出追踪数据到用户空间。\n- **实时日志记录**：需要低延迟、高吞吐量的日志场景（如网络数据包捕获）。\n- **性能监控**：将内核统计信息（如调度事件、块设备 I/O）流式传输到用户态分析工具。\n- **调试工具**：通过 `debugfs` 暴露 relay 通道，供用户态调试器实时读取内核状态。",
      "similarity": 0.6281325817108154,
      "chunks": [
        {
          "chunk_id": 6,
          "file_path": "kernel/relay.c",
          "start_line": 1076,
          "end_line": 1186,
          "content": [
            "static void relay_consume_bytes(struct rchan_buf *rbuf, int bytes_consumed)",
            "{",
            "\trbuf->bytes_consumed += bytes_consumed;",
            "",
            "\tif (rbuf->bytes_consumed >= rbuf->chan->subbuf_size) {",
            "\t\trelay_subbufs_consumed(rbuf->chan, rbuf->cpu, 1);",
            "\t\trbuf->bytes_consumed %= rbuf->chan->subbuf_size;",
            "\t}",
            "}",
            "static void relay_pipe_buf_release(struct pipe_inode_info *pipe,",
            "\t\t\t\t   struct pipe_buffer *buf)",
            "{",
            "\tstruct rchan_buf *rbuf;",
            "",
            "\trbuf = (struct rchan_buf *)page_private(buf->page);",
            "\trelay_consume_bytes(rbuf, buf->private);",
            "}",
            "static void relay_page_release(struct splice_pipe_desc *spd, unsigned int i)",
            "{",
            "}",
            "static ssize_t subbuf_splice_actor(struct file *in,",
            "\t\t\t       loff_t *ppos,",
            "\t\t\t       struct pipe_inode_info *pipe,",
            "\t\t\t       size_t len,",
            "\t\t\t       unsigned int flags,",
            "\t\t\t       int *nonpad_ret)",
            "{",
            "\tunsigned int pidx, poff, total_len, subbuf_pages, nr_pages;",
            "\tstruct rchan_buf *rbuf = in->private_data;",
            "\tunsigned int subbuf_size = rbuf->chan->subbuf_size;",
            "\tuint64_t pos = (uint64_t) *ppos;",
            "\tuint32_t alloc_size = (uint32_t) rbuf->chan->alloc_size;",
            "\tsize_t read_start = (size_t) do_div(pos, alloc_size);",
            "\tsize_t read_subbuf = read_start / subbuf_size;",
            "\tsize_t padding = rbuf->padding[read_subbuf];",
            "\tsize_t nonpad_end = read_subbuf * subbuf_size + subbuf_size - padding;",
            "\tstruct page *pages[PIPE_DEF_BUFFERS];",
            "\tstruct partial_page partial[PIPE_DEF_BUFFERS];",
            "\tstruct splice_pipe_desc spd = {",
            "\t\t.pages = pages,",
            "\t\t.nr_pages = 0,",
            "\t\t.nr_pages_max = PIPE_DEF_BUFFERS,",
            "\t\t.partial = partial,",
            "\t\t.ops = &relay_pipe_buf_ops,",
            "\t\t.spd_release = relay_page_release,",
            "\t};",
            "\tssize_t ret;",
            "",
            "\tif (rbuf->subbufs_produced == rbuf->subbufs_consumed)",
            "\t\treturn 0;",
            "\tif (splice_grow_spd(pipe, &spd))",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * Adjust read len, if longer than what is available",
            "\t */",
            "\tif (len > (subbuf_size - read_start % subbuf_size))",
            "\t\tlen = subbuf_size - read_start % subbuf_size;",
            "",
            "\tsubbuf_pages = rbuf->chan->alloc_size >> PAGE_SHIFT;",
            "\tpidx = (read_start / PAGE_SIZE) % subbuf_pages;",
            "\tpoff = read_start & ~PAGE_MASK;",
            "\tnr_pages = min_t(unsigned int, subbuf_pages, spd.nr_pages_max);",
            "",
            "\tfor (total_len = 0; spd.nr_pages < nr_pages; spd.nr_pages++) {",
            "\t\tunsigned int this_len, this_end, private;",
            "\t\tunsigned int cur_pos = read_start + total_len;",
            "",
            "\t\tif (!len)",
            "\t\t\tbreak;",
            "",
            "\t\tthis_len = min_t(unsigned long, len, PAGE_SIZE - poff);",
            "\t\tprivate = this_len;",
            "",
            "\t\tspd.pages[spd.nr_pages] = rbuf->page_array[pidx];",
            "\t\tspd.partial[spd.nr_pages].offset = poff;",
            "",
            "\t\tthis_end = cur_pos + this_len;",
            "\t\tif (this_end >= nonpad_end) {",
            "\t\t\tthis_len = nonpad_end - cur_pos;",
            "\t\t\tprivate = this_len + padding;",
            "\t\t}",
            "\t\tspd.partial[spd.nr_pages].len = this_len;",
            "\t\tspd.partial[spd.nr_pages].private = private;",
            "",
            "\t\tlen -= this_len;",
            "\t\ttotal_len += this_len;",
            "\t\tpoff = 0;",
            "\t\tpidx = (pidx + 1) % subbuf_pages;",
            "",
            "\t\tif (this_end >= nonpad_end) {",
            "\t\t\tspd.nr_pages++;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tret = 0;",
            "\tif (!spd.nr_pages)",
            "\t\tgoto out;",
            "",
            "\tret = *nonpad_ret = splice_to_pipe(pipe, &spd);",
            "\tif (ret < 0 || ret < total_len)",
            "\t\tgoto out;",
            "",
            "        if (read_start + ret == nonpad_end)",
            "                ret += padding;",
            "",
            "out:",
            "\tsplice_shrink_spd(&spd);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "relay_consume_bytes, relay_pipe_buf_release, relay_page_release, subbuf_splice_actor",
          "description": "记录并更新 relay 缓冲区已消费字节数；释放管道缓冲区并触发 consume_bytes 操作；处理 splice 分页数据时的页面释放逻辑；实现分页数据复制到管道的 actor 函数，处理填充区域和页面分配。上下文不完整",
          "similarity": 0.6100348830223083
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/relay.c",
          "start_line": 956,
          "end_line": 1057,
          "content": [
            "static size_t relay_file_read_subbuf_avail(size_t read_pos,",
            "\t\t\t\t\t   struct rchan_buf *buf)",
            "{",
            "\tsize_t padding, avail = 0;",
            "\tsize_t read_subbuf, read_offset, write_subbuf, write_offset;",
            "\tsize_t subbuf_size = buf->chan->subbuf_size;",
            "",
            "\twrite_subbuf = (buf->data - buf->start) / subbuf_size;",
            "\twrite_offset = buf->offset > subbuf_size ? subbuf_size : buf->offset;",
            "\tread_subbuf = read_pos / subbuf_size;",
            "\tread_offset = read_pos % subbuf_size;",
            "\tpadding = buf->padding[read_subbuf];",
            "",
            "\tif (read_subbuf == write_subbuf) {",
            "\t\tif (read_offset + padding < write_offset)",
            "\t\t\tavail = write_offset - (read_offset + padding);",
            "\t} else",
            "\t\tavail = (subbuf_size - padding) - read_offset;",
            "",
            "\treturn avail;",
            "}",
            "static size_t relay_file_read_start_pos(struct rchan_buf *buf)",
            "{",
            "\tsize_t read_subbuf, padding, padding_start, padding_end;",
            "\tsize_t subbuf_size = buf->chan->subbuf_size;",
            "\tsize_t n_subbufs = buf->chan->n_subbufs;",
            "\tsize_t consumed = buf->subbufs_consumed % n_subbufs;",
            "\tsize_t read_pos = (consumed * subbuf_size + buf->bytes_consumed)",
            "\t\t\t% (n_subbufs * subbuf_size);",
            "",
            "\tread_subbuf = read_pos / subbuf_size;",
            "\tpadding = buf->padding[read_subbuf];",
            "\tpadding_start = (read_subbuf + 1) * subbuf_size - padding;",
            "\tpadding_end = (read_subbuf + 1) * subbuf_size;",
            "\tif (read_pos >= padding_start && read_pos < padding_end) {",
            "\t\tread_subbuf = (read_subbuf + 1) % n_subbufs;",
            "\t\tread_pos = read_subbuf * subbuf_size;",
            "\t}",
            "",
            "\treturn read_pos;",
            "}",
            "static size_t relay_file_read_end_pos(struct rchan_buf *buf,",
            "\t\t\t\t      size_t read_pos,",
            "\t\t\t\t      size_t count)",
            "{",
            "\tsize_t read_subbuf, padding, end_pos;",
            "\tsize_t subbuf_size = buf->chan->subbuf_size;",
            "\tsize_t n_subbufs = buf->chan->n_subbufs;",
            "",
            "\tread_subbuf = read_pos / subbuf_size;",
            "\tpadding = buf->padding[read_subbuf];",
            "\tif (read_pos % subbuf_size + count + padding == subbuf_size)",
            "\t\tend_pos = (read_subbuf + 1) * subbuf_size;",
            "\telse",
            "\t\tend_pos = read_pos + count;",
            "\tif (end_pos >= subbuf_size * n_subbufs)",
            "\t\tend_pos = 0;",
            "",
            "\treturn end_pos;",
            "}",
            "static ssize_t relay_file_read(struct file *filp,",
            "\t\t\t       char __user *buffer,",
            "\t\t\t       size_t count,",
            "\t\t\t       loff_t *ppos)",
            "{",
            "\tstruct rchan_buf *buf = filp->private_data;",
            "\tsize_t read_start, avail;",
            "\tsize_t written = 0;",
            "\tint ret;",
            "",
            "\tif (!count)",
            "\t\treturn 0;",
            "",
            "\tinode_lock(file_inode(filp));",
            "\tdo {",
            "\t\tvoid *from;",
            "",
            "\t\tif (!relay_file_read_avail(buf))",
            "\t\t\tbreak;",
            "",
            "\t\tread_start = relay_file_read_start_pos(buf);",
            "\t\tavail = relay_file_read_subbuf_avail(read_start, buf);",
            "\t\tif (!avail)",
            "\t\t\tbreak;",
            "",
            "\t\tavail = min(count, avail);",
            "\t\tfrom = buf->start + read_start;",
            "\t\tret = avail;",
            "\t\tif (copy_to_user(buffer, from, avail))",
            "\t\t\tbreak;",
            "",
            "\t\tbuffer += ret;",
            "\t\twritten += ret;",
            "\t\tcount -= ret;",
            "",
            "\t\trelay_file_read_consume(buf, read_start, ret);",
            "\t\t*ppos = relay_file_read_end_pos(buf, read_start, ret);",
            "\t} while (count);",
            "\tinode_unlock(file_inode(filp));",
            "",
            "\treturn written;",
            "}"
          ],
          "function_name": "relay_file_read_subbuf_avail, relay_file_read_start_pos, relay_file_read_end_pos, relay_file_read",
          "description": "计算 relay 缓冲区子缓冲区中可读字节数，考虑读取偏移和填充区域；确定读取起始位置并处理填充部分逻辑；计算读取结束位置并限制在缓冲区范围内；实现文件读取逻辑，从内核缓冲区复制数据至用户空间，更新读取状态和文件偏移量。上下文不完整",
          "similarity": 0.6088627576828003
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/relay.c",
          "start_line": 319,
          "end_line": 456,
          "content": [
            "void relay_reset(struct rchan *chan)",
            "{",
            "\tstruct rchan_buf *buf;",
            "\tunsigned int i;",
            "",
            "\tif (!chan)",
            "\t\treturn;",
            "",
            "\tif (chan->is_global && (buf = *per_cpu_ptr(chan->buf, 0))) {",
            "\t\t__relay_reset(buf, 0);",
            "\t\treturn;",
            "\t}",
            "",
            "\tmutex_lock(&relay_channels_mutex);",
            "\tfor_each_possible_cpu(i)",
            "\t\tif ((buf = *per_cpu_ptr(chan->buf, i)))",
            "\t\t\t__relay_reset(buf, 0);",
            "\tmutex_unlock(&relay_channels_mutex);",
            "}",
            "static inline void relay_set_buf_dentry(struct rchan_buf *buf,",
            "\t\t\t\t\tstruct dentry *dentry)",
            "{",
            "\tbuf->dentry = dentry;",
            "\td_inode(buf->dentry)->i_size = buf->early_bytes;",
            "}",
            "static void relay_close_buf(struct rchan_buf *buf)",
            "{",
            "\tbuf->finalized = 1;",
            "\tirq_work_sync(&buf->wakeup_work);",
            "\tbuf->chan->cb->remove_buf_file(buf->dentry);",
            "\tkref_put(&buf->kref, relay_remove_buf);",
            "}",
            "int relay_prepare_cpu(unsigned int cpu)",
            "{",
            "\tstruct rchan *chan;",
            "\tstruct rchan_buf *buf;",
            "",
            "\tmutex_lock(&relay_channels_mutex);",
            "\tlist_for_each_entry(chan, &relay_channels, list) {",
            "\t\tif (*per_cpu_ptr(chan->buf, cpu))",
            "\t\t\tcontinue;",
            "\t\tbuf = relay_open_buf(chan, cpu);",
            "\t\tif (!buf) {",
            "\t\t\tpr_err(\"relay: cpu %d buffer creation failed\\n\", cpu);",
            "\t\t\tmutex_unlock(&relay_channels_mutex);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "\t\t*per_cpu_ptr(chan->buf, cpu) = buf;",
            "\t}",
            "\tmutex_unlock(&relay_channels_mutex);",
            "\treturn 0;",
            "}",
            "static void __relay_set_buf_dentry(void *info)",
            "{",
            "\tstruct rchan_percpu_buf_dispatcher *p = info;",
            "",
            "\trelay_set_buf_dentry(p->buf, p->dentry);",
            "}",
            "int relay_late_setup_files(struct rchan *chan,",
            "\t\t\t   const char *base_filename,",
            "\t\t\t   struct dentry *parent)",
            "{",
            "\tint err = 0;",
            "\tunsigned int i, curr_cpu;",
            "\tunsigned long flags;",
            "\tstruct dentry *dentry;",
            "\tstruct rchan_buf *buf;",
            "\tstruct rchan_percpu_buf_dispatcher disp;",
            "",
            "\tif (!chan || !base_filename)",
            "\t\treturn -EINVAL;",
            "",
            "\tstrscpy(chan->base_filename, base_filename, NAME_MAX);",
            "",
            "\tmutex_lock(&relay_channels_mutex);",
            "\t/* Is chan already set up? */",
            "\tif (unlikely(chan->has_base_filename)) {",
            "\t\tmutex_unlock(&relay_channels_mutex);",
            "\t\treturn -EEXIST;",
            "\t}",
            "\tchan->has_base_filename = 1;",
            "\tchan->parent = parent;",
            "",
            "\tif (chan->is_global) {",
            "\t\terr = -EINVAL;",
            "\t\tbuf = *per_cpu_ptr(chan->buf, 0);",
            "\t\tif (!WARN_ON_ONCE(!buf)) {",
            "\t\t\tdentry = relay_create_buf_file(chan, buf, 0);",
            "\t\t\tif (dentry && !WARN_ON_ONCE(!chan->is_global)) {",
            "\t\t\t\trelay_set_buf_dentry(buf, dentry);",
            "\t\t\t\terr = 0;",
            "\t\t\t}",
            "\t\t}",
            "\t\tmutex_unlock(&relay_channels_mutex);",
            "\t\treturn err;",
            "\t}",
            "",
            "\tcurr_cpu = get_cpu();",
            "\t/*",
            "\t * The CPU hotplug notifier ran before us and created buffers with",
            "\t * no files associated. So it's safe to call relay_setup_buf_file()",
            "\t * on all currently online CPUs.",
            "\t */",
            "\tfor_each_online_cpu(i) {",
            "\t\tbuf = *per_cpu_ptr(chan->buf, i);",
            "\t\tif (unlikely(!buf)) {",
            "\t\t\tWARN_ONCE(1, KERN_ERR \"CPU has no buffer!\\n\");",
            "\t\t\terr = -EINVAL;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tdentry = relay_create_buf_file(chan, buf, i);",
            "\t\tif (unlikely(!dentry)) {",
            "\t\t\terr = -EINVAL;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tif (curr_cpu == i) {",
            "\t\t\tlocal_irq_save(flags);",
            "\t\t\trelay_set_buf_dentry(buf, dentry);",
            "\t\t\tlocal_irq_restore(flags);",
            "\t\t} else {",
            "\t\t\tdisp.buf = buf;",
            "\t\t\tdisp.dentry = dentry;",
            "\t\t\tsmp_mb();",
            "\t\t\t/* relay_channels_mutex must be held, so wait. */",
            "\t\t\terr = smp_call_function_single(i,",
            "\t\t\t\t\t\t       __relay_set_buf_dentry,",
            "\t\t\t\t\t\t       &disp, 1);",
            "\t\t}",
            "\t\tif (unlikely(err))",
            "\t\t\tbreak;",
            "\t}",
            "\tput_cpu();",
            "\tmutex_unlock(&relay_channels_mutex);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "relay_reset, relay_set_buf_dentry, relay_close_buf, relay_prepare_cpu, __relay_set_buf_dentry, relay_late_setup_files",
          "description": "处理Relay缓冲区的重置、Dentry绑定、CPU缓冲区准备及后期文件系统设置，通过互斥锁保护多CPU环境下的缓冲区一致性。",
          "similarity": 0.5962220430374146
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/relay.c",
          "start_line": 33,
          "end_line": 145,
          "content": [
            "static vm_fault_t relay_buf_fault(struct vm_fault *vmf)",
            "{",
            "\tstruct page *page;",
            "\tstruct rchan_buf *buf = vmf->vma->vm_private_data;",
            "\tpgoff_t pgoff = vmf->pgoff;",
            "",
            "\tif (!buf)",
            "\t\treturn VM_FAULT_OOM;",
            "",
            "\tpage = vmalloc_to_page(buf->start + (pgoff << PAGE_SHIFT));",
            "\tif (!page)",
            "\t\treturn VM_FAULT_SIGBUS;",
            "\tget_page(page);",
            "\tvmf->page = page;",
            "",
            "\treturn 0;",
            "}",
            "static void relay_free_page_array(struct page **array)",
            "{",
            "\tkvfree(array);",
            "}",
            "static int relay_mmap_buf(struct rchan_buf *buf, struct vm_area_struct *vma)",
            "{",
            "\tunsigned long length = vma->vm_end - vma->vm_start;",
            "",
            "\tif (!buf)",
            "\t\treturn -EBADF;",
            "",
            "\tif (length != (unsigned long)buf->chan->alloc_size)",
            "\t\treturn -EINVAL;",
            "",
            "\tvma->vm_ops = &relay_file_mmap_ops;",
            "\tvm_flags_set(vma, VM_DONTEXPAND);",
            "\tvma->vm_private_data = buf;",
            "",
            "\treturn 0;",
            "}",
            "static void relay_destroy_channel(struct kref *kref)",
            "{",
            "\tstruct rchan *chan = container_of(kref, struct rchan, kref);",
            "\tfree_percpu(chan->buf);",
            "\tkfree(chan);",
            "}",
            "static void relay_destroy_buf(struct rchan_buf *buf)",
            "{",
            "\tstruct rchan *chan = buf->chan;",
            "\tunsigned int i;",
            "",
            "\tif (likely(buf->start)) {",
            "\t\tvunmap(buf->start);",
            "\t\tfor (i = 0; i < buf->page_count; i++)",
            "\t\t\t__free_page(buf->page_array[i]);",
            "\t\trelay_free_page_array(buf->page_array);",
            "\t}",
            "\t*per_cpu_ptr(chan->buf, buf->cpu) = NULL;",
            "\tkfree(buf->padding);",
            "\tkfree(buf);",
            "\tkref_put(&chan->kref, relay_destroy_channel);",
            "}",
            "static void relay_remove_buf(struct kref *kref)",
            "{",
            "\tstruct rchan_buf *buf = container_of(kref, struct rchan_buf, kref);",
            "\trelay_destroy_buf(buf);",
            "}",
            "static int relay_buf_empty(struct rchan_buf *buf)",
            "{",
            "\treturn (buf->subbufs_produced - buf->subbufs_consumed) ? 0 : 1;",
            "}",
            "int relay_buf_full(struct rchan_buf *buf)",
            "{",
            "\tsize_t ready = buf->subbufs_produced - buf->subbufs_consumed;",
            "\treturn (ready >= buf->chan->n_subbufs) ? 1 : 0;",
            "}",
            "static int relay_subbuf_start(struct rchan_buf *buf, void *subbuf,",
            "\t\t\t      void *prev_subbuf, size_t prev_padding)",
            "{",
            "\tif (!buf->chan->cb->subbuf_start)",
            "\t\treturn !relay_buf_full(buf);",
            "",
            "\treturn buf->chan->cb->subbuf_start(buf, subbuf,",
            "\t\t\t\t\t   prev_subbuf, prev_padding);",
            "}",
            "static void wakeup_readers(struct irq_work *work)",
            "{",
            "\tstruct rchan_buf *buf;",
            "",
            "\tbuf = container_of(work, struct rchan_buf, wakeup_work);",
            "\twake_up_interruptible(&buf->read_wait);",
            "}",
            "static void __relay_reset(struct rchan_buf *buf, unsigned int init)",
            "{",
            "\tsize_t i;",
            "",
            "\tif (init) {",
            "\t\tinit_waitqueue_head(&buf->read_wait);",
            "\t\tkref_init(&buf->kref);",
            "\t\tinit_irq_work(&buf->wakeup_work, wakeup_readers);",
            "\t} else {",
            "\t\tirq_work_sync(&buf->wakeup_work);",
            "\t}",
            "",
            "\tbuf->subbufs_produced = 0;",
            "\tbuf->subbufs_consumed = 0;",
            "\tbuf->bytes_consumed = 0;",
            "\tbuf->finalized = 0;",
            "\tbuf->data = buf->start;",
            "\tbuf->offset = 0;",
            "",
            "\tfor (i = 0; i < buf->chan->n_subbufs; i++)",
            "\t\tbuf->padding[i] = 0;",
            "",
            "\trelay_subbuf_start(buf, buf->data, NULL, 0);",
            "}"
          ],
          "function_name": "relay_buf_fault, relay_free_page_array, relay_mmap_buf, relay_destroy_channel, relay_destroy_buf, relay_remove_buf, relay_buf_empty, relay_buf_full, relay_subbuf_start, wakeup_readers, __relay_reset",
          "description": "实现Relay缓冲区的页面故障处理、内存映射配置、通道销毁逻辑及缓冲区状态控制，包括子缓冲区满判断、唤醒读取线程等核心操作。",
          "similarity": 0.5811430215835571
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/relay.c",
          "start_line": 1,
          "end_line": 32,
          "content": [
            "/*",
            " * Public API and common code for kernel->userspace relay file support.",
            " *",
            " * See Documentation/filesystems/relay.rst for an overview.",
            " *",
            " * Copyright (C) 2002-2005 - Tom Zanussi (zanussi@us.ibm.com), IBM Corp",
            " * Copyright (C) 1999-2005 - Karim Yaghmour (karim@opersys.com)",
            " *",
            " * Moved to kernel/relay.c by Paul Mundt, 2006.",
            " * November 2006 - CPU hotplug support by Mathieu Desnoyers",
            " * \t(mathieu.desnoyers@polymtl.ca)",
            " *",
            " * This file is released under the GPL.",
            " */",
            "#include <linux/errno.h>",
            "#include <linux/stddef.h>",
            "#include <linux/slab.h>",
            "#include <linux/export.h>",
            "#include <linux/string.h>",
            "#include <linux/relay.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/mm.h>",
            "#include <linux/cpu.h>",
            "#include <linux/splice.h>",
            "",
            "/* list of open channels, for cpu hotplug */",
            "static DEFINE_MUTEX(relay_channels_mutex);",
            "static LIST_HEAD(relay_channels);",
            "",
            "/*",
            " * fault() vm_op implementation for relay file mapping.",
            " */"
          ],
          "function_name": null,
          "description": "定义了Relay子系统的公共API和通用代码，包含用于管理内核到用户空间数据中继的核心结构体、互斥锁及链表，用于CPU热插拔支持的通道列表管理。",
          "similarity": 0.5716491341590881
        }
      ]
    },
    {
      "source_file": "mm/page_alloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_alloc.c`\n\n---\n\n# page_alloc.c 技术文档\n\n## 1. 文件概述\n\n`page_alloc.c` 是 Linux 内核内存管理子系统的核心文件之一，负责物理页面的分配与释放。该文件实现了基于区域（zone）和迁移类型（migratetype）的伙伴系统（Buddy System）内存分配器，管理系统的空闲页链表，并提供高效的页面分配/回收机制。它不处理小对象分配（由 slab/slub/slob 子系统负责），而是专注于以页为单位的大块物理内存管理。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct per_cpu_pages`**：每个 CPU 的每区（per-zone）页面缓存，用于减少锁竞争，提升分配性能。\n- **`node_states[NR_NODE_STATES]`**：全局节点状态掩码数组，跟踪各 NUMA 节点的状态（如在线、有内存等）。\n- **`sysctl_lowmem_reserve_ratio[MAX_NR_ZONES]`**：各内存区域的低内存保留比例，防止高优先级区域耗尽低优先级区域的内存。\n- **`zone_names[]` 和 `migratetype_names[]`**：内存区域和页面迁移类型的名称字符串，用于调试和日志。\n- **`gfp_allowed_mask`**：全局 GFP（Get Free Page）标志掩码，控制启动早期可使用的分配标志。\n\n### 主要函数（部分声明）\n- **`__free_pages_ok()`**：内部页面释放函数，执行实际的伙伴系统合并与链表插入逻辑。\n- 各种页面分配函数（如 `alloc_pages()`、`__alloc_pages()` 等，定义在其他位置但在此文件中实现核心逻辑）。\n- 每 CPU 页面列表操作辅助宏（如 `pcp_spin_lock()`、`pcp_spin_trylock()`）。\n\n### 关键常量与标志\n- **`fpi_t` 类型及标志**：\n  - `FPI_NONE`：无特殊要求。\n  - `FPI_SKIP_REPORT_NOTIFY`：跳过空闲页报告通知。\n  - `FPI_TO_TAIL`：将页面放回空闲链表尾部（用于优化场景如内存热插拔）。\n- **`min_free_kbytes`**：系统保留的最小空闲内存（KB），影响水位线计算。\n\n## 3. 关键实现\n\n### 每 CPU 页面缓存（Per-CPU Page Caching）\n- 通过 `struct per_cpu_pages` 为每个 CPU 维护热/冷页列表，避免频繁访问全局 zone 锁。\n- 使用 `pcpu_spin_lock` 宏族安全地访问每 CPU 数据，结合 `preempt_disable()`（非 RT）或 `migrate_disable()`（RT）防止任务迁移导致访问错误 CPU 的数据。\n- 在 UP 系统上，使用 IRQ 关闭防止重入；在 SMP/RT 系统上依赖自旋锁语义。\n\n### 内存区域（Zone）与 NUMA 支持\n- 支持多种内存区域（DMA、DMA32、Normal、HighMem、Movable、Device），通过 `zone_names` 标识。\n- 实现 `lowmem_reserve_ratio` 机制，确保高区域分配不会耗尽低区域的保留内存（如 ZONE_DMA 为设备保留）。\n- 通过 `node_states` 和 per-CPU 变量（如 `numa_node`、`_numa_mem_`）支持 NUMA 和无内存节点架构。\n\n### 空闲页管理优化\n- **`FPI_TO_TAIL` 标志**：允许将页面放回空闲链表尾部，配合内存打乱（shuffle）或热插拔时批量初始化。\n- **`FPI_SKIP_REPORT_NOTIFY` 标志**：在临时取出并归还页面时不触发空闲页报告机制，减少开销。\n- **水位线与保留内存**：`min_free_kbytes` 控制最低水位，影响 OOM（Out-Of-Memory）决策和内存回收行为。\n\n### 实时内核（PREEMPT_RT）适配\n- 在 RT 内核中使用 `migrate_disable()` 替代 `preempt_disable()`，避免干扰 RT 自旋锁的优先级继承机制。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心内存管理**：`<linux/mm.h>`, `<linux/highmem.h>`, `\"internal.h\"`\n- **同步机制**：`<linux/spinlock.h>`（隐含）、`<linux/mutex.h>`\n- **NUMA 与拓扑**：`<linux/topology.h>`, `<linux/nodemask.h>`\n- **调试与追踪**：`<linux/kasan.h>`, `<trace/events/kmem.h>`, `<linux/page_owner.h>`\n- **高级特性**：`<linux/compaction.h>`, `<linux/migrate.h>`, `<linux/memcontrol.h>`\n\n### 子系统交互\n- **Slab 分配器**：本文件不处理 kmalloc，由 `slab.c` 等负责。\n- **内存回收**：与 `vmscan.c` 协同，通过水位线触发 reclaim。\n- **内存热插拔**：通过 `memory_hotplug.h` 接口管理动态内存。\n- **OOM Killer**：通过 `oom.h` 和水位线机制触发 OOM。\n- **透明大页（THP）**：与 `khugepaged` 协同进行大页分配。\n\n## 5. 使用场景\n\n- **内核内存分配**：所有以页为单位的内核内存请求（如 `alloc_pages()`）最终由本文件处理。\n- **用户空间缺页处理**：匿名页、文件页的物理页分配。\n- **内存映射（mmap）**：大块物理内存的分配与管理。\n- **内存回收与迁移**：页面回收、压缩（compaction）、迁移（migration）过程中涉及的页面释放与重新分配。\n- **系统启动与热插拔**：初始化内存区域、处理动态添加/移除内存。\n- **实时系统**：在 PREEMPT_RT 内核中提供低延迟的页面分配路径。\n- **调试与监控**：通过 page owner、KASAN、tracepoint 等机制提供内存使用追踪。",
      "similarity": 0.6275599002838135,
      "chunks": [
        {
          "chunk_id": 23,
          "file_path": "mm/page_alloc.c",
          "start_line": 4471,
          "end_line": 4660,
          "content": [
            "static inline bool prepare_alloc_pages(gfp_t gfp_mask, unsigned int order,",
            "\t\tint preferred_nid, nodemask_t *nodemask,",
            "\t\tstruct alloc_context *ac, gfp_t *alloc_gfp,",
            "\t\tunsigned int *alloc_flags)",
            "{",
            "\tac->highest_zoneidx = gfp_zone(gfp_mask);",
            "\tac->zonelist = node_zonelist(preferred_nid, gfp_mask);",
            "\tac->nodemask = nodemask;",
            "\tac->migratetype = gfp_migratetype(gfp_mask);",
            "",
            "\tif (cpusets_enabled()) {",
            "\t\t*alloc_gfp |= __GFP_HARDWALL;",
            "\t\t/*",
            "\t\t * When we are in the interrupt context, it is irrelevant",
            "\t\t * to the current task context. It means that any node ok.",
            "\t\t */",
            "\t\tif (in_task() && !ac->nodemask)",
            "\t\t\tac->nodemask = &cpuset_current_mems_allowed;",
            "\t\telse",
            "\t\t\t*alloc_flags |= ALLOC_CPUSET;",
            "\t}",
            "",
            "\tmight_alloc(gfp_mask);",
            "",
            "\tif (should_fail_alloc_page(gfp_mask, order))",
            "\t\treturn false;",
            "",
            "\t*alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, *alloc_flags);",
            "",
            "\t/* Dirty zone balancing only done in the fast path */",
            "\tac->spread_dirty_pages = (gfp_mask & __GFP_WRITE);",
            "",
            "\t/*",
            "\t * The preferred zone is used for statistics but crucially it is",
            "\t * also used as the starting point for the zonelist iterator. It",
            "\t * may get reset for allocations that ignore memory policies.",
            "\t */",
            "\tac->preferred_zoneref = first_zones_zonelist(ac->zonelist,",
            "\t\t\t\t\tac->highest_zoneidx, ac->nodemask);",
            "",
            "\treturn true;",
            "}",
            "unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,",
            "\t\t\tnodemask_t *nodemask, int nr_pages,",
            "\t\t\tstruct list_head *page_list,",
            "\t\t\tstruct page **page_array)",
            "{",
            "\tstruct page *page;",
            "\tunsigned long __maybe_unused UP_flags;",
            "\tstruct zone *zone;",
            "\tstruct zoneref *z;",
            "\tstruct per_cpu_pages *pcp;",
            "\tstruct list_head *pcp_list;",
            "\tstruct alloc_context ac;",
            "\tgfp_t alloc_gfp;",
            "\tunsigned int alloc_flags = ALLOC_WMARK_LOW;",
            "\tint nr_populated = 0, nr_account = 0;",
            "",
            "\t/*",
            "\t * Skip populated array elements to determine if any pages need",
            "\t * to be allocated before disabling IRQs.",
            "\t */",
            "\twhile (page_array && nr_populated < nr_pages && page_array[nr_populated])",
            "\t\tnr_populated++;",
            "",
            "\t/* No pages requested? */",
            "\tif (unlikely(nr_pages <= 0))",
            "\t\tgoto out;",
            "",
            "\t/* Already populated array? */",
            "\tif (unlikely(page_array && nr_pages - nr_populated == 0))",
            "\t\tgoto out;",
            "",
            "\t/* Bulk allocator does not support memcg accounting. */",
            "\tif (memcg_kmem_online() && (gfp & __GFP_ACCOUNT))",
            "\t\tgoto failed;",
            "",
            "\t/* Use the single page allocator for one page. */",
            "\tif (nr_pages - nr_populated == 1)",
            "\t\tgoto failed;",
            "",
            "#ifdef CONFIG_PAGE_OWNER",
            "\t/*",
            "\t * PAGE_OWNER may recurse into the allocator to allocate space to",
            "\t * save the stack with pagesets.lock held. Releasing/reacquiring",
            "\t * removes much of the performance benefit of bulk allocation so",
            "\t * force the caller to allocate one page at a time as it'll have",
            "\t * similar performance to added complexity to the bulk allocator.",
            "\t */",
            "\tif (static_branch_unlikely(&page_owner_inited))",
            "\t\tgoto failed;",
            "#endif",
            "",
            "\t/* May set ALLOC_NOFRAGMENT, fragmentation will return 1 page. */",
            "\tgfp &= gfp_allowed_mask;",
            "\talloc_gfp = gfp;",
            "\tif (!prepare_alloc_pages(gfp, 0, preferred_nid, nodemask, &ac, &alloc_gfp, &alloc_flags))",
            "\t\tgoto out;",
            "\tgfp = alloc_gfp;",
            "",
            "\t/* Find an allowed local zone that meets the low watermark. */",
            "\tz = ac.preferred_zoneref;",
            "\tfor_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {",
            "\t\tunsigned long mark;",
            "",
            "\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&",
            "\t\t    !__cpuset_zone_allowed(zone, gfp)) {",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tif (nr_online_nodes > 1 && zone != ac.preferred_zoneref->zone &&",
            "\t\t    zone_to_nid(zone) != zone_to_nid(ac.preferred_zoneref->zone)) {",
            "\t\t\tgoto failed;",
            "\t\t}",
            "",
            "\t\tmark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK) + nr_pages;",
            "\t\tif (zone_watermark_fast(zone, 0,  mark,",
            "\t\t\t\tzonelist_zone_idx(ac.preferred_zoneref),",
            "\t\t\t\talloc_flags, gfp)) {",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * If there are no allowed local zones that meets the watermarks then",
            "\t * try to allocate a single page and reclaim if necessary.",
            "\t */",
            "\tif (unlikely(!zone))",
            "\t\tgoto failed;",
            "",
            "\t/* spin_trylock may fail due to a parallel drain or IRQ reentrancy. */",
            "\tpcp_trylock_prepare(UP_flags);",
            "\tpcp = pcp_spin_trylock(zone->per_cpu_pageset);",
            "\tif (!pcp)",
            "\t\tgoto failed_irq;",
            "",
            "\t/* Attempt the batch allocation */",
            "\tpcp_list = &pcp->lists[order_to_pindex(ac.migratetype, 0)];",
            "\twhile (nr_populated < nr_pages) {",
            "",
            "\t\t/* Skip existing pages */",
            "\t\tif (page_array && page_array[nr_populated]) {",
            "\t\t\tnr_populated++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tpage = __rmqueue_pcplist(zone, 0, ac.migratetype, alloc_flags,",
            "\t\t\t\t\t\t\t\tpcp, pcp_list);",
            "\t\tif (unlikely(!page)) {",
            "\t\t\t/* Try and allocate at least one page */",
            "\t\t\tif (!nr_account) {",
            "\t\t\t\tpcp_spin_unlock(pcp);",
            "\t\t\t\tgoto failed_irq;",
            "\t\t\t}",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tnr_account++;",
            "",
            "\t\tprep_new_page(page, 0, gfp, 0);",
            "\t\tif (page_list)",
            "\t\t\tlist_add(&page->lru, page_list);",
            "\t\telse",
            "\t\t\tpage_array[nr_populated] = page;",
            "\t\tnr_populated++;",
            "\t}",
            "",
            "\tpcp_spin_unlock(pcp);",
            "\tpcp_trylock_finish(UP_flags);",
            "",
            "\t__count_zid_vm_events(PGALLOC, zone_idx(zone), nr_account);",
            "\tzone_statistics(ac.preferred_zoneref->zone, zone, nr_account);",
            "",
            "out:",
            "\treturn nr_populated;",
            "",
            "failed_irq:",
            "\tpcp_trylock_finish(UP_flags);",
            "",
            "failed:",
            "\tpage = __alloc_pages_noprof(gfp, 0, preferred_nid, nodemask);",
            "\tif (page) {",
            "\t\tif (page_list)",
            "\t\t\tlist_add(&page->lru, page_list);",
            "\t\telse",
            "\t\t\tpage_array[nr_populated] = page;",
            "\t\tnr_populated++;",
            "\t}",
            "",
            "\tgoto out;",
            "}"
          ],
          "function_name": "prepare_alloc_pages, alloc_pages_bulk_noprof",
          "description": "该代码段实现了内存页面的批量分配逻辑，其中`prepare_alloc_pages`用于初始化分配上下文参数并配置内存策略，`alloc_pages_bulk_noprof`则通过遍历内存区域尝试批量分配连续页面，优先使用本地节点且支持CPU集约束。  \n`prepare_alloc_pages`构建分配上下文，设置Zone列表、迁移类型及节点掩码，处理CPU集隔离与水位线检查；`alloc_pages_bulk_noprof`在满足水位线前提下批量分配页面，失败时回退至单页分配。  \n上下文完整，未引入未展示的API或机制。",
          "similarity": 0.595824122428894
        },
        {
          "chunk_id": 34,
          "file_path": "mm/page_alloc.c",
          "start_line": 6473,
          "end_line": 6610,
          "content": [
            "static void split_free_pages(struct list_head *list)",
            "{",
            "\tint order;",
            "",
            "\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {",
            "\t\tstruct page *page, *next;",
            "\t\tint nr_pages = 1 << order;",
            "",
            "\t\tlist_for_each_entry_safe(page, next, &list[order], lru) {",
            "\t\t\tint i;",
            "",
            "\t\t\tpost_alloc_hook(page, order, __GFP_MOVABLE);",
            "\t\t\tif (!order)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tsplit_page(page, order);",
            "",
            "\t\t\t/* Add all subpages to the order-0 head, in sequence. */",
            "\t\t\tlist_del(&page->lru);",
            "\t\t\tfor (i = 0; i < nr_pages; i++)",
            "\t\t\t\tlist_add_tail(&page[i].lru, &list[0]);",
            "\t\t}",
            "\t}",
            "}",
            "int alloc_contig_range_noprof(unsigned long start, unsigned long end,",
            "\t\t       unsigned migratetype, gfp_t gfp_mask)",
            "{",
            "\tunsigned long outer_start, outer_end;",
            "\tint ret = 0;",
            "",
            "\tstruct compact_control cc = {",
            "\t\t.nr_migratepages = 0,",
            "\t\t.order = -1,",
            "\t\t.zone = page_zone(pfn_to_page(start)),",
            "\t\t.mode = MIGRATE_SYNC,",
            "\t\t.ignore_skip_hint = true,",
            "\t\t.no_set_skip_hint = true,",
            "\t\t.gfp_mask = current_gfp_context(gfp_mask),",
            "\t\t.alloc_contig = true,",
            "\t};",
            "\tINIT_LIST_HEAD(&cc.migratepages);",
            "",
            "\t/*",
            "\t * What we do here is we mark all pageblocks in range as",
            "\t * MIGRATE_ISOLATE.  Because pageblock and max order pages may",
            "\t * have different sizes, and due to the way page allocator",
            "\t * work, start_isolate_page_range() has special handlings for this.",
            "\t *",
            "\t * Once the pageblocks are marked as MIGRATE_ISOLATE, we",
            "\t * migrate the pages from an unaligned range (ie. pages that",
            "\t * we are interested in). This will put all the pages in",
            "\t * range back to page allocator as MIGRATE_ISOLATE.",
            "\t *",
            "\t * When this is done, we take the pages in range from page",
            "\t * allocator removing them from the buddy system.  This way",
            "\t * page allocator will never consider using them.",
            "\t *",
            "\t * This lets us mark the pageblocks back as",
            "\t * MIGRATE_CMA/MIGRATE_MOVABLE so that free pages in the",
            "\t * aligned range but not in the unaligned, original range are",
            "\t * put back to page allocator so that buddy can use them.",
            "\t */",
            "",
            "\tret = start_isolate_page_range(start, end, migratetype, 0, gfp_mask);",
            "\tif (ret)",
            "\t\tgoto done;",
            "",
            "\tdrain_all_pages(cc.zone);",
            "",
            "\t/*",
            "\t * In case of -EBUSY, we'd like to know which page causes problem.",
            "\t * So, just fall through. test_pages_isolated() has a tracepoint",
            "\t * which will report the busy page.",
            "\t *",
            "\t * It is possible that busy pages could become available before",
            "\t * the call to test_pages_isolated, and the range will actually be",
            "\t * allocated.  So, if we fall through be sure to clear ret so that",
            "\t * -EBUSY is not accidentally used or returned to caller.",
            "\t */",
            "\tret = __alloc_contig_migrate_range(&cc, start, end, migratetype);",
            "\tif (ret && ret != -EBUSY)",
            "\t\tgoto done;",
            "\tret = 0;",
            "",
            "\t/*",
            "\t * Pages from [start, end) are within a pageblock_nr_pages",
            "\t * aligned blocks that are marked as MIGRATE_ISOLATE.  What's",
            "\t * more, all pages in [start, end) are free in page allocator.",
            "\t * What we are going to do is to allocate all pages from",
            "\t * [start, end) (that is remove them from page allocator).",
            "\t *",
            "\t * The only problem is that pages at the beginning and at the",
            "\t * end of interesting range may be not aligned with pages that",
            "\t * page allocator holds, ie. they can be part of higher order",
            "\t * pages.  Because of this, we reserve the bigger range and",
            "\t * once this is done free the pages we are not interested in.",
            "\t *",
            "\t * We don't have to hold zone->lock here because the pages are",
            "\t * isolated thus they won't get removed from buddy.",
            "\t */",
            "\touter_start = find_large_buddy(start);",
            "",
            "\t/* Make sure the range is really isolated. */",
            "\tif (test_pages_isolated(outer_start, end, 0)) {",
            "\t\tret = -EBUSY;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\t/* Grab isolated pages from freelists. */",
            "\touter_end = isolate_freepages_range(&cc, outer_start, end);",
            "\tif (!outer_end) {",
            "\t\tret = -EBUSY;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\tif (!(gfp_mask & __GFP_COMP)) {",
            "\t\tsplit_free_pages(cc.freepages);",
            "",
            "\t\t/* Free head and tail (if any) */",
            "\t\tif (start != outer_start)",
            "\t\t\tfree_contig_range(outer_start, start - outer_start);",
            "\t\tif (end != outer_end)",
            "\t\t\tfree_contig_range(end, outer_end - end);",
            "\t} else if (start == outer_start && end == outer_end && is_power_of_2(end - start)) {",
            "\t\tstruct page *head = pfn_to_page(start);",
            "\t\tint order = ilog2(end - start);",
            "",
            "\t\tcheck_new_pages(head, order);",
            "\t\tprep_new_page(head, order, gfp_mask, 0);",
            "\t} else {",
            "\t\tret = -EINVAL;",
            "\t\tWARN(true, \"PFN range: requested [%lu, %lu), allocated [%lu, %lu)\\n\",",
            "\t\t     start, end, outer_start, outer_end);",
            "\t}",
            "done:",
            "\tundo_isolate_page_range(start, end, migratetype);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "split_free_pages, alloc_contig_range_noprof",
          "description": "实现连续物理内存范围分配功能，通过隔离页面块、迁移页面和调整区域状态，确保目标范围内内存可被系统使用并处理分配异常情况。",
          "similarity": 0.592496395111084
        },
        {
          "chunk_id": 33,
          "file_path": "mm/page_alloc.c",
          "start_line": 6218,
          "end_line": 6373,
          "content": [
            "static int sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tsetup_min_slab_ratio();",
            "",
            "\treturn 0;",
            "}",
            "static int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *table,",
            "\t\tint write, void *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint i;",
            "",
            "\tproc_dointvec_minmax(table, write, buffer, length, ppos);",
            "",
            "\tfor (i = 0; i < MAX_NR_ZONES; i++) {",
            "\t\tif (sysctl_lowmem_reserve_ratio[i] < 1)",
            "\t\t\tsysctl_lowmem_reserve_ratio[i] = 0;",
            "\t}",
            "",
            "\tsetup_per_zone_lowmem_reserve();",
            "\treturn 0;",
            "}",
            "static int percpu_pagelist_high_fraction_sysctl_handler(struct ctl_table *table,",
            "\t\tint write, void *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tstruct zone *zone;",
            "\tint old_percpu_pagelist_high_fraction;",
            "\tint ret;",
            "",
            "\tmutex_lock(&pcp_batch_high_lock);",
            "\told_percpu_pagelist_high_fraction = percpu_pagelist_high_fraction;",
            "",
            "\tret = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (!write || ret < 0)",
            "\t\tgoto out;",
            "",
            "\t/* Sanity checking to avoid pcp imbalance */",
            "\tif (percpu_pagelist_high_fraction &&",
            "\t    percpu_pagelist_high_fraction < MIN_PERCPU_PAGELIST_HIGH_FRACTION) {",
            "\t\tpercpu_pagelist_high_fraction = old_percpu_pagelist_high_fraction;",
            "\t\tret = -EINVAL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* No change? */",
            "\tif (percpu_pagelist_high_fraction == old_percpu_pagelist_high_fraction)",
            "\t\tgoto out;",
            "",
            "\tfor_each_populated_zone(zone)",
            "\t\tzone_set_pageset_high_and_batch(zone, 0);",
            "out:",
            "\tmutex_unlock(&pcp_batch_high_lock);",
            "\treturn ret;",
            "}",
            "void __init page_alloc_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"vm\", page_alloc_sysctl_table);",
            "}",
            "static void alloc_contig_dump_pages(struct list_head *page_list)",
            "{",
            "\tDEFINE_DYNAMIC_DEBUG_METADATA(descriptor, \"migrate failure\");",
            "",
            "\tif (DYNAMIC_DEBUG_BRANCH(descriptor)) {",
            "\t\tstruct page *page;",
            "",
            "\t\tdump_stack();",
            "\t\tlist_for_each_entry(page, page_list, lru)",
            "\t\t\tdump_page(page, \"migration failure\");",
            "\t}",
            "}",
            "int __alloc_contig_migrate_range(struct compact_control *cc,",
            "\t\t\t\t\tunsigned long start, unsigned long end,",
            "\t\t\t\t\tint migratetype)",
            "{",
            "\t/* This function is based on compact_zone() from compaction.c. */",
            "\tunsigned int nr_reclaimed;",
            "\tunsigned long pfn = start;",
            "\tunsigned int tries = 0;",
            "\tint ret = 0;",
            "\tstruct migration_target_control mtc = {",
            "\t\t.nid = zone_to_nid(cc->zone),",
            "\t\t.gfp_mask = GFP_USER | __GFP_MOVABLE | __GFP_RETRY_MAYFAIL,",
            "\t\t.reason = MR_CONTIG_RANGE,",
            "\t};",
            "\tstruct page *page;",
            "\tunsigned long total_mapped = 0;",
            "\tunsigned long total_migrated = 0;",
            "\tunsigned long total_reclaimed = 0;",
            "",
            "\tlru_cache_disable();",
            "",
            "\twhile (pfn < end || !list_empty(&cc->migratepages)) {",
            "\t\tif (fatal_signal_pending(current)) {",
            "\t\t\tret = -EINTR;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tif (list_empty(&cc->migratepages)) {",
            "\t\t\tcc->nr_migratepages = 0;",
            "\t\t\tret = isolate_migratepages_range(cc, pfn, end);",
            "\t\t\tif (ret && ret != -EAGAIN)",
            "\t\t\t\tbreak;",
            "\t\t\tpfn = cc->migrate_pfn;",
            "\t\t\ttries = 0;",
            "\t\t} else if (++tries == 5) {",
            "\t\t\tret = -EBUSY;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tnr_reclaimed = reclaim_clean_pages_from_list(cc->zone,",
            "\t\t\t\t\t\t\t&cc->migratepages);",
            "\t\tcc->nr_migratepages -= nr_reclaimed;",
            "",
            "\t\tif (trace_mm_alloc_contig_migrate_range_info_enabled()) {",
            "\t\t\ttotal_reclaimed += nr_reclaimed;",
            "\t\t\tlist_for_each_entry(page, &cc->migratepages, lru) {",
            "\t\t\t\tstruct folio *folio = page_folio(page);",
            "",
            "\t\t\t\ttotal_mapped += folio_mapped(folio) *",
            "\t\t\t\t\t\tfolio_nr_pages(folio);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tret = migrate_pages(&cc->migratepages, alloc_migration_target,",
            "\t\t\tNULL, (unsigned long)&mtc, cc->mode, MR_CONTIG_RANGE, NULL);",
            "",
            "\t\tif (trace_mm_alloc_contig_migrate_range_info_enabled() && !ret)",
            "\t\t\ttotal_migrated += cc->nr_migratepages;",
            "",
            "\t\t/*",
            "\t\t * On -ENOMEM, migrate_pages() bails out right away. It is pointless",
            "\t\t * to retry again over this error, so do the same here.",
            "\t\t */",
            "\t\tif (ret == -ENOMEM)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tlru_cache_enable();",
            "\tif (ret < 0) {",
            "\t\tif (!(cc->gfp_mask & __GFP_NOWARN) && ret == -EBUSY)",
            "\t\t\talloc_contig_dump_pages(&cc->migratepages);",
            "\t\tputback_movable_pages(&cc->migratepages);",
            "\t}",
            "",
            "\ttrace_mm_alloc_contig_migrate_range_info(start, end, migratetype,",
            "\t\t\t\t\t\t total_migrated,",
            "\t\t\t\t\t\t total_reclaimed,",
            "\t\t\t\t\t\t total_mapped);",
            "\treturn (ret < 0) ? ret : 0;",
            "}"
          ],
          "function_name": "sysctl_min_slab_ratio_sysctl_handler, lowmem_reserve_ratio_sysctl_handler, percpu_pagelist_high_fraction_sysctl_handler, page_alloc_sysctl_init, alloc_contig_dump_pages, __alloc_contig_migrate_range",
          "description": "提供内存管理参数的sysctl控制接口，处理连续内存分配失败时的调试输出，实现内存迁移辅助函数以支持大块连续内存分配。",
          "similarity": 0.5733038187026978
        },
        {
          "chunk_id": 21,
          "file_path": "mm/page_alloc.c",
          "start_line": 3958,
          "end_line": 4058,
          "content": [
            "static void wake_all_kswapds(unsigned int order, gfp_t gfp_mask,",
            "\t\t\t     const struct alloc_context *ac)",
            "{",
            "\tstruct zoneref *z;",
            "\tstruct zone *zone;",
            "\tpg_data_t *last_pgdat = NULL;",
            "\tenum zone_type highest_zoneidx = ac->highest_zoneidx;",
            "",
            "\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist, highest_zoneidx,",
            "\t\t\t\t\tac->nodemask) {",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "\t\tif (last_pgdat != zone->zone_pgdat) {",
            "\t\t\twakeup_kswapd(zone, gfp_mask, order, highest_zoneidx);",
            "\t\t\tlast_pgdat = zone->zone_pgdat;",
            "\t\t}",
            "\t}",
            "}",
            "static inline unsigned int",
            "gfp_to_alloc_flags(gfp_t gfp_mask, unsigned int order)",
            "{",
            "\tunsigned int alloc_flags = ALLOC_WMARK_MIN | ALLOC_CPUSET;",
            "",
            "\t/*",
            "\t * __GFP_HIGH is assumed to be the same as ALLOC_MIN_RESERVE",
            "\t * and __GFP_KSWAPD_RECLAIM is assumed to be the same as ALLOC_KSWAPD",
            "\t * to save two branches.",
            "\t */",
            "\tBUILD_BUG_ON(__GFP_HIGH != (__force gfp_t) ALLOC_MIN_RESERVE);",
            "\tBUILD_BUG_ON(__GFP_KSWAPD_RECLAIM != (__force gfp_t) ALLOC_KSWAPD);",
            "",
            "\t/*",
            "\t * The caller may dip into page reserves a bit more if the caller",
            "\t * cannot run direct reclaim, or if the caller has realtime scheduling",
            "\t * policy or is asking for __GFP_HIGH memory.  GFP_ATOMIC requests will",
            "\t * set both ALLOC_NON_BLOCK and ALLOC_MIN_RESERVE(__GFP_HIGH).",
            "\t */",
            "\talloc_flags |= (__force int)",
            "\t\t(gfp_mask & (__GFP_HIGH | __GFP_KSWAPD_RECLAIM));",
            "",
            "\tif (!(gfp_mask & __GFP_DIRECT_RECLAIM)) {",
            "\t\t/*",
            "\t\t * Not worth trying to allocate harder for __GFP_NOMEMALLOC even",
            "\t\t * if it can't schedule.",
            "\t\t */",
            "\t\tif (!(gfp_mask & __GFP_NOMEMALLOC)) {",
            "\t\t\talloc_flags |= ALLOC_NON_BLOCK;",
            "",
            "\t\t\tif (order > 0)",
            "\t\t\t\talloc_flags |= ALLOC_HIGHATOMIC;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Ignore cpuset mems for non-blocking __GFP_HIGH (probably",
            "\t\t * GFP_ATOMIC) rather than fail, see the comment for",
            "\t\t * cpuset_node_allowed().",
            "\t\t */",
            "\t\tif (alloc_flags & ALLOC_MIN_RESERVE)",
            "\t\t\talloc_flags &= ~ALLOC_CPUSET;",
            "\t} else if (unlikely(rt_or_dl_task(current)) && in_task())",
            "\t\talloc_flags |= ALLOC_MIN_RESERVE;",
            "",
            "\talloc_flags = gfp_to_alloc_flags_cma(gfp_mask, alloc_flags);",
            "",
            "\treturn alloc_flags;",
            "}",
            "static bool oom_reserves_allowed(struct task_struct *tsk)",
            "{",
            "\tif (!tsk_is_oom_victim(tsk))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * !MMU doesn't have oom reaper so give access to memory reserves",
            "\t * only to the thread with TIF_MEMDIE set",
            "\t */",
            "\tif (!IS_ENABLED(CONFIG_MMU) && !test_thread_flag(TIF_MEMDIE))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static inline int __gfp_pfmemalloc_flags(gfp_t gfp_mask)",
            "{",
            "\tif (unlikely(gfp_mask & __GFP_NOMEMALLOC))",
            "\t\treturn 0;",
            "\tif (gfp_mask & __GFP_MEMALLOC)",
            "\t\treturn ALLOC_NO_WATERMARKS;",
            "\tif (in_serving_softirq() && (current->flags & PF_MEMALLOC))",
            "\t\treturn ALLOC_NO_WATERMARKS;",
            "\tif (!in_interrupt()) {",
            "\t\tif (current->flags & PF_MEMALLOC)",
            "\t\t\treturn ALLOC_NO_WATERMARKS;",
            "\t\telse if (oom_reserves_allowed(current))",
            "\t\t\treturn ALLOC_OOM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "bool gfp_pfmemalloc_allowed(gfp_t gfp_mask)",
            "{",
            "\treturn !!__gfp_pfmemalloc_flags(gfp_mask);",
            "}"
          ],
          "function_name": "wake_all_kswapds, gfp_to_alloc_flags, oom_reserves_allowed, __gfp_pfmemalloc_flags, gfp_pfmemalloc_allowed",
          "description": "该代码段主要处理内存分配时的策略配置与回收机制。  \n`wake_all_kswapds` 遍历内存区并唤醒对应 kswapd 线程以触发页面回收；`gfp_to_alloc_flags` 根据 GFP 标志计算分配策略标志，控制内存回收行为；`oom_reserves_allowed` 和 `__gfp_pfmemalloc_flags` 共同决定是否允许绕过内存水印限制访问 OOM 预留内存。",
          "similarity": 0.5670326352119446
        },
        {
          "chunk_id": 0,
          "file_path": "mm/page_alloc.c",
          "start_line": 1,
          "end_line": 302,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  linux/mm/page_alloc.c",
            " *",
            " *  Manages the free list, the system allocates free pages here.",
            " *  Note that kmalloc() lives in slab.c",
            " *",
            " *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds",
            " *  Swap reorganised 29.12.95, Stephen Tweedie",
            " *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999",
            " *  Reshaped it to be a zoned allocator, Ingo Molnar, Red Hat, 1999",
            " *  Discontiguous memory support, Kanoj Sarcar, SGI, Nov 1999",
            " *  Zone balancing, Kanoj Sarcar, SGI, Jan 2000",
            " *  Per cpu hot/cold page lists, bulk allocation, Martin J. Bligh, Sept 2002",
            " *          (lots of bits borrowed from Ingo Molnar & Andrew Morton)",
            " */",
            "",
            "#include <linux/stddef.h>",
            "#include <linux/mm.h>",
            "#include <linux/highmem.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/compiler.h>",
            "#include <linux/kernel.h>",
            "#include <linux/kasan.h>",
            "#include <linux/kmsan.h>",
            "#include <linux/module.h>",
            "#include <linux/suspend.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/oom.h>",
            "#include <linux/topology.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/cpu.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/pagevec.h>",
            "#include <linux/memory_hotplug.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/compaction.h>",
            "#include <trace/events/kmem.h>",
            "#include <trace/events/oom.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/migrate.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/page_owner.h>",
            "#include <linux/page_table_check.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/psi.h>",
            "#include <linux/khugepaged.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/cacheinfo.h>",
            "#include <linux/pgalloc_tag.h>",
            "#include <asm/div64.h>",
            "#include \"internal.h\"",
            "#include \"shuffle.h\"",
            "#include \"page_reporting.h\"",
            "",
            "/* Free Page Internal flags: for internal, non-pcp variants of free_pages(). */",
            "typedef int __bitwise fpi_t;",
            "",
            "/* No special request */",
            "#define FPI_NONE\t\t((__force fpi_t)0)",
            "",
            "/*",
            " * Skip free page reporting notification for the (possibly merged) page.",
            " * This does not hinder free page reporting from grabbing the page,",
            " * reporting it and marking it \"reported\" -  it only skips notifying",
            " * the free page reporting infrastructure about a newly freed page. For",
            " * example, used when temporarily pulling a page from a freelist and",
            " * putting it back unmodified.",
            " */",
            "#define FPI_SKIP_REPORT_NOTIFY\t((__force fpi_t)BIT(0))",
            "",
            "/*",
            " * Place the (possibly merged) page to the tail of the freelist. Will ignore",
            " * page shuffling (relevant code - e.g., memory onlining - is expected to",
            " * shuffle the whole zone).",
            " *",
            " * Note: No code should rely on this flag for correctness - it's purely",
            " *       to allow for optimizations when handing back either fresh pages",
            " *       (memory onlining) or untouched pages (page isolation, free page",
            " *       reporting).",
            " */",
            "#define FPI_TO_TAIL\t\t((__force fpi_t)BIT(1))",
            "",
            "/* prevent >1 _updater_ of zone percpu pageset ->high and ->batch fields */",
            "static DEFINE_MUTEX(pcp_batch_high_lock);",
            "#define MIN_PERCPU_PAGELIST_HIGH_FRACTION (8)",
            "",
            "#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)",
            "/*",
            " * On SMP, spin_trylock is sufficient protection.",
            " * On PREEMPT_RT, spin_trylock is equivalent on both SMP and UP.",
            " */",
            "#define pcp_trylock_prepare(flags)\tdo { } while (0)",
            "#define pcp_trylock_finish(flag)\tdo { } while (0)",
            "#else",
            "",
            "/* UP spin_trylock always succeeds so disable IRQs to prevent re-entrancy. */",
            "#define pcp_trylock_prepare(flags)\tlocal_irq_save(flags)",
            "#define pcp_trylock_finish(flags)\tlocal_irq_restore(flags)",
            "#endif",
            "",
            "/*",
            " * Locking a pcp requires a PCP lookup followed by a spinlock. To avoid",
            " * a migration causing the wrong PCP to be locked and remote memory being",
            " * potentially allocated, pin the task to the CPU for the lookup+lock.",
            " * preempt_disable is used on !RT because it is faster than migrate_disable.",
            " * migrate_disable is used on RT because otherwise RT spinlock usage is",
            " * interfered with and a high priority task cannot preempt the allocator.",
            " */",
            "#ifndef CONFIG_PREEMPT_RT",
            "#define pcpu_task_pin()\t\tpreempt_disable()",
            "#define pcpu_task_unpin()\tpreempt_enable()",
            "#else",
            "#define pcpu_task_pin()\t\tmigrate_disable()",
            "#define pcpu_task_unpin()\tmigrate_enable()",
            "#endif",
            "",
            "/*",
            " * Generic helper to lookup and a per-cpu variable with an embedded spinlock.",
            " * Return value should be used with equivalent unlock helper.",
            " */",
            "#define pcpu_spin_lock(type, member, ptr)\t\t\t\t\\",
            "({\t\t\t\t\t\t\t\t\t\\",
            "\ttype *_ret;\t\t\t\t\t\t\t\\",
            "\tpcpu_task_pin();\t\t\t\t\t\t\\",
            "\t_ret = this_cpu_ptr(ptr);\t\t\t\t\t\\",
            "\tspin_lock(&_ret->member);\t\t\t\t\t\\",
            "\t_ret;\t\t\t\t\t\t\t\t\\",
            "})",
            "",
            "#define pcpu_spin_trylock(type, member, ptr)\t\t\t\t\\",
            "({\t\t\t\t\t\t\t\t\t\\",
            "\ttype *_ret;\t\t\t\t\t\t\t\\",
            "\tpcpu_task_pin();\t\t\t\t\t\t\\",
            "\t_ret = this_cpu_ptr(ptr);\t\t\t\t\t\\",
            "\tif (!spin_trylock(&_ret->member)) {\t\t\t\t\\",
            "\t\tpcpu_task_unpin();\t\t\t\t\t\\",
            "\t\t_ret = NULL;\t\t\t\t\t\t\\",
            "\t}\t\t\t\t\t\t\t\t\\",
            "\t_ret;\t\t\t\t\t\t\t\t\\",
            "})",
            "",
            "#define pcpu_spin_unlock(member, ptr)\t\t\t\t\t\\",
            "({\t\t\t\t\t\t\t\t\t\\",
            "\tspin_unlock(&ptr->member);\t\t\t\t\t\\",
            "\tpcpu_task_unpin();\t\t\t\t\t\t\\",
            "})",
            "",
            "/* struct per_cpu_pages specific helpers. */",
            "#define pcp_spin_lock(ptr)\t\t\t\t\t\t\\",
            "\tpcpu_spin_lock(struct per_cpu_pages, lock, ptr)",
            "",
            "#define pcp_spin_trylock(ptr)\t\t\t\t\t\t\\",
            "\tpcpu_spin_trylock(struct per_cpu_pages, lock, ptr)",
            "",
            "#define pcp_spin_unlock(ptr)\t\t\t\t\t\t\\",
            "\tpcpu_spin_unlock(lock, ptr)",
            "",
            "#ifdef CONFIG_USE_PERCPU_NUMA_NODE_ID",
            "DEFINE_PER_CPU(int, numa_node);",
            "EXPORT_PER_CPU_SYMBOL(numa_node);",
            "#endif",
            "",
            "DEFINE_STATIC_KEY_TRUE(vm_numa_stat_key);",
            "",
            "#ifdef CONFIG_HAVE_MEMORYLESS_NODES",
            "/*",
            " * N.B., Do NOT reference the '_numa_mem_' per cpu variable directly.",
            " * It will not be defined when CONFIG_HAVE_MEMORYLESS_NODES is not defined.",
            " * Use the accessor functions set_numa_mem(), numa_mem_id() and cpu_to_mem()",
            " * defined in <linux/topology.h>.",
            " */",
            "DEFINE_PER_CPU(int, _numa_mem_);\t\t/* Kernel \"local memory\" node */",
            "EXPORT_PER_CPU_SYMBOL(_numa_mem_);",
            "#endif",
            "",
            "static DEFINE_MUTEX(pcpu_drain_mutex);",
            "",
            "#ifdef CONFIG_GCC_PLUGIN_LATENT_ENTROPY",
            "volatile unsigned long latent_entropy __latent_entropy;",
            "EXPORT_SYMBOL(latent_entropy);",
            "#endif",
            "",
            "/*",
            " * Array of node states.",
            " */",
            "nodemask_t node_states[NR_NODE_STATES] __read_mostly = {",
            "\t[N_POSSIBLE] = NODE_MASK_ALL,",
            "\t[N_ONLINE] = { { [0] = 1UL } },",
            "#ifndef CONFIG_NUMA",
            "\t[N_NORMAL_MEMORY] = { { [0] = 1UL } },",
            "#ifdef CONFIG_HIGHMEM",
            "\t[N_HIGH_MEMORY] = { { [0] = 1UL } },",
            "#endif",
            "\t[N_MEMORY] = { { [0] = 1UL } },",
            "\t[N_CPU] = { { [0] = 1UL } },",
            "#endif\t/* NUMA */",
            "};",
            "EXPORT_SYMBOL(node_states);",
            "",
            "gfp_t gfp_allowed_mask __read_mostly = GFP_BOOT_MASK;",
            "",
            "#ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE",
            "unsigned int pageblock_order __read_mostly;",
            "#endif",
            "",
            "static void __free_pages_ok(struct page *page, unsigned int order,",
            "\t\t\t    fpi_t fpi_flags);",
            "",
            "/*",
            " * results with 256, 32 in the lowmem_reserve sysctl:",
            " *\t1G machine -> (16M dma, 800M-16M normal, 1G-800M high)",
            " *\t1G machine -> (16M dma, 784M normal, 224M high)",
            " *\tNORMAL allocation will leave 784M/256 of ram reserved in the ZONE_DMA",
            " *\tHIGHMEM allocation will leave 224M/32 of ram reserved in ZONE_NORMAL",
            " *\tHIGHMEM allocation will leave (224M+784M)/256 of ram reserved in ZONE_DMA",
            " *",
            " * TBD: should special case ZONE_DMA32 machines here - in those we normally",
            " * don't need any ZONE_NORMAL reservation",
            " */",
            "static int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES] = {",
            "#ifdef CONFIG_ZONE_DMA",
            "\t[ZONE_DMA] = 256,",
            "#endif",
            "#ifdef CONFIG_ZONE_DMA32",
            "\t[ZONE_DMA32] = 256,",
            "#endif",
            "\t[ZONE_NORMAL] = 32,",
            "#ifdef CONFIG_HIGHMEM",
            "\t[ZONE_HIGHMEM] = 0,",
            "#endif",
            "\t[ZONE_MOVABLE] = 0,",
            "};",
            "",
            "char * const zone_names[MAX_NR_ZONES] = {",
            "#ifdef CONFIG_ZONE_DMA",
            "\t \"DMA\",",
            "#endif",
            "#ifdef CONFIG_ZONE_DMA32",
            "\t \"DMA32\",",
            "#endif",
            "\t \"Normal\",",
            "#ifdef CONFIG_HIGHMEM",
            "\t \"HighMem\",",
            "#endif",
            "\t \"Movable\",",
            "#ifdef CONFIG_ZONE_DEVICE",
            "\t \"Device\",",
            "#endif",
            "};",
            "",
            "const char * const migratetype_names[MIGRATE_TYPES] = {",
            "\t\"Unmovable\",",
            "\t\"Movable\",",
            "\t\"Reclaimable\",",
            "\t\"HighAtomic\",",
            "#ifdef CONFIG_CMA",
            "\t\"CMA\",",
            "#endif",
            "#ifdef CONFIG_MEMORY_ISOLATION",
            "\t\"Isolate\",",
            "#endif",
            "};",
            "",
            "int min_free_kbytes = 1024;",
            "int user_min_free_kbytes = -1;",
            "static int watermark_boost_factor __read_mostly = 15000;",
            "static int watermark_scale_factor = 10;",
            "",
            "/* movable_zone is the \"real\" zone pages in ZONE_MOVABLE are taken from */",
            "int movable_zone;",
            "EXPORT_SYMBOL(movable_zone);",
            "",
            "#if MAX_NUMNODES > 1",
            "unsigned int nr_node_ids __read_mostly = MAX_NUMNODES;",
            "unsigned int nr_online_nodes __read_mostly = 1;",
            "EXPORT_SYMBOL(nr_node_ids);",
            "EXPORT_SYMBOL(nr_online_nodes);",
            "#endif",
            "",
            "static bool page_contains_unaccepted(struct page *page, unsigned int order);",
            "static void accept_page(struct page *page, unsigned int order);",
            "static bool cond_accept_memory(struct zone *zone, unsigned int order);",
            "static bool __free_unaccepted(struct page *page);",
            "",
            "int page_group_by_mobility_disabled __read_mostly;",
            "",
            "#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT",
            "/*",
            " * During boot we initialize deferred pages on-demand, as needed, but once",
            " * page_alloc_init_late() has finished, the deferred pages are all initialized,",
            " * and we can permanently disable that path.",
            " */",
            "DEFINE_STATIC_KEY_TRUE(deferred_pages);",
            ""
          ],
          "function_name": null,
          "description": "此代码块定义了页面分配器的基础框架，包括空闲页面管理、NUMA支持、多处理器锁机制及内存区域状态数组。通过宏定义控制页面释放标志位，并实现了PCP（Per-CPU）页列表的互斥保护，为后续的页面分配提供底层基础设施。",
          "similarity": 0.5633769631385803
        }
      ]
    }
  ]
}