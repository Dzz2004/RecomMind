{
  "query": "slab fragmentation",
  "timestamp": "2025-12-26 00:08:45",
  "retrieved_files": [
    {
      "source_file": "mm/slab.h",
      "md_summary": "> 自动生成时间: 2025-12-07 17:22:03\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `slab.h`\n\n---\n\n# `slab.h` 技术文档\n\n## 1. 文件概述\n\n`slab.h` 是 Linux 内核内存管理子系统中 SLAB/SLUB 分配器的核心内部头文件，定义了 slab 分配器所使用的底层数据结构（如 `struct slab` 和 `struct kmem_cache`）、关键宏和辅助函数。该文件主要用于在页（`struct page`）与 slab 表示之间进行安全转换，并提供对 slab 元数据的原子访问机制，以支持高性能、可扩展的对象缓存分配。\n\n此头文件专供内核内存管理内部使用，不对外暴露给模块开发者，是实现 SLUB（默认）或 SLAB 分配器的关键基础设施。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`freelist_aba_t`**  \n  联合体，将空闲对象指针（`freelist`）与计数器（`counter`）打包为一个原子单元，用于避免 ABA 问题（Compare-and-Swap 中因值重复导致的逻辑错误）。\n\n- **`struct slab`**  \n  slab 的内部表示，复用 `struct page` 的内存布局。包含：\n  - 所属的 `kmem_cache`\n  - 空闲对象链表（`freelist`）\n  - 对象使用计数（`inuse`）、总对象数（`objects`）\n  - 冻结状态（`frozen`，用于调试）\n  - RCU 回收头（`rcu_head`）\n  - 引用计数（`__page_refcount`）\n  - 可选的 per-object 扩展数据（`obj_exts`）\n\n- **`struct kmem_cache_order_objects`**  \n  封装 slab 阶数（order）与对象数量的复合值，支持原子读写。\n\n- **`struct kmem_cache`**  \n  slab 缓存描述符，包含：\n  - 每 CPU 缓存（`cpu_slab`）\n  - 对象大小（`size`, `object_size`）\n  - 构造函数（`ctor`）\n  - 对齐要求（`align`）\n  - 分配标志（`allocflags`）\n  - NUMA 相关参数（如 `remote_node_defrag_ratio`）\n  - 安全特性（如 `random` 用于 freelist 加固）\n\n### 主要宏与辅助函数\n\n- **类型安全转换宏**：\n  - `folio_slab()` / `slab_folio()`：在 `folio` 与 `slab` 之间安全转换\n  - `page_slab()` / `slab_page()`：兼容旧代码，在 `page` 与 `slab` 之间转换\n\n- **slab 属性访问函数**：\n  - `slab_address()`：获取 slab 起始虚拟地址\n  - `slab_nid()` / `slab_pgdat()`：获取所属 NUMA 节点和内存域\n  - `slab_order()` / `slab_size()`：获取分配阶数和总字节数\n\n- **pfmemalloc 标志操作**：\n  - `slab_test_pfmemalloc()` / `slab_set_pfmemalloc()` 等：标记 slab 是否来自紧急内存预留区（用于网络交换等场景）\n\n- **每 CPU partial slab 支持（`CONFIG_SLUB_CPU_PARTIAL`）**：\n  - `slub_percpu_partial()` 等宏：管理每 CPU 的 partial slab 链表\n\n## 3. 关键实现\n\n### 内存布局复用与静态断言\n\n- `struct slab` 并非独立分配，而是直接复用 `struct page` 的内存空间。通过 `static_assert` 确保关键字段偏移一致（如 `flags` ↔ `__page_flags`），保证类型转换安全。\n- 整个 `struct slab` 大小不超过 `struct page`，确保无越界访问。\n\n### ABA 问题防护\n\n- 在支持 `cmpxchg128`（64 位）或 `cmpxchg64`（32 位）的架构上，启用 `freelist_aba_t` 结构，将 `freelist` 指针与递增计数器打包为单个原子单元。\n- 使用 `try_cmpxchg_freelist` 进行原子更新，防止因指针值循环重用导致的 ABA 错误。\n- 若系统不支持对齐的 `struct page`（`!CONFIG_HAVE_ALIGNED_STRUCT_PAGE`），则禁用此优化。\n\n### 类型安全转换\n\n- 使用 C11 `_Generic` 实现类型安全的 `folio`/`slab`/`page` 转换，避免强制类型转换带来的风险，并为未来重构（如完全迁移到 folio）预留接口。\n\n### pfmemalloc 标志复用\n\n- 利用 `folio` 的 `PG_active` 位存储 `pfmemalloc` 标志，指示该 slab 是否从紧急内存池分配，用于网络子系统在内存压力下仍能分配 skb 等关键结构。\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/page.h>` / `<linux/folio.h>`：通过 `folio_*` 系列函数操作底层内存\n  - `<linux/reciprocal_div.h>`：用于快速除法（计算对象索引）\n  - `<linux/rcupdate.h>`：通过 `rcu_head` 支持 RCU 安全的 slab 回收\n\n- **可选依赖（由 Kconfig 控制）**：\n  - `CONFIG_SLUB_CPU_PARTIAL`：每 CPU partial slab 优化\n  - `CONFIG_SLAB_OBJ_EXT`：per-object 扩展元数据\n  - `CONFIG_SLAB_FREELIST_HARDENED`：freelist 指针随机化加固\n  - `CONFIG_NUMA`：NUMA 感知分配与碎片整理\n  - `CONFIG_KASAN` / `CONFIG_KFENCE`：内存错误检测集成\n\n- **与内存控制器集成**：\n  - 通过 `memcg_data` 字段（复用 `obj_exts`）支持 memcg 内存统计\n\n## 5. 使用场景\n\n- **SLUB 分配器内部**：作为 `slub.c` 的核心数据结构定义，用于管理 slab 生命周期、对象分配/释放。\n- **内存回收路径**：在 direct reclaim 或 kswapd 中，通过 `slab_folio` 获取 folio 信息以决策回收策略。\n- **调试与监控**：sysfs (`kobj`)、KASAN/KFENCE 集成依赖此结构获取 slab 元数据。\n- **网络子系统**：通过 `pfmemalloc` 标志识别紧急内存分配，确保高优先级数据包处理不被阻塞。\n- **NUMA 优化**：在远程节点分配时使用 `remote_node_defrag_ratio` 参数控制跨节点分配行为。\n- **安全加固**：`SLAB_FREELIST_HARDENED` 利用 `random` 字段混淆 freelist 指针，防止堆利用攻击。",
      "similarity": 0.5433370471000671,
      "chunks": []
    },
    {
      "source_file": "mm/slub.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:23:28\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `slub.c`\n\n---\n\n# slub.c 技术文档\n\n## 1. 文件概述\n\n`slub.c` 是 Linux 内核中 SLUB（Simple Low-overhead Unqueued Allocator）内存分配器的核心实现文件。SLUB 是一种高效的 slab 分配器，旨在减少缓存行使用并避免在每个 CPU 和节点上维护复杂的对象队列。它通过 per-slab 锁或原子操作进行同步，仅在管理部分填充的 slab 池时使用集中式锁。该分配器优化了常见路径的性能，同时支持调试、内存检测和热插拔等高级功能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `kmem_cache`: slab 缓存描述符，包含对象大小、对齐方式、构造函数等元数据\n- `kmem_cache_cpu`: 每 CPU 的 slab 管理结构，包含当前 CPU 的活跃 slab 和局部 freelist\n- `slab`: slab 描述符（通常嵌入在 page 结构中），包含 freelist、inuse 计数、objects 总数和 frozen 状态\n\n### 关键机制\n- **CPU slab**: 每个 CPU 分配专用的 slab 进行快速分配\n- **Partial slab 列表**: 节点级别的部分填充 slab 列表\n- **CPU partial slab**: CPU 本地的部分填充 slab 缓存，用于加速释放操作\n- **Frozen slab**: 冻结状态的 slab，免于全局列表管理\n\n### 锁机制层次\n1. `slab_mutex` - 全局互斥锁，保护所有 slab 列表和元数据变更\n2. `node->list_lock` - 自旋锁，保护节点的 partial/full 列表\n3. `kmem_cache->cpu_slab->lock` - 本地锁，保护慢路径的 per-CPU 字段\n4. `slab_lock(slab)` - slab 锁（仅在不支持 `cmpxchg_double` 的架构上使用）\n5. `object_map_lock` - 调试用途的对象映射锁\n\n## 3. 关键实现\n\n### 锁无关快速路径\n- 分配 (`slab_alloc_node()`) 和释放 (`do_slab_free()`) 操作在满足条件时完全无锁\n- 使用事务 ID (tid) 字段检测抢占或 CPU 迁移\n- 在支持 `cmpxchg_double` 的架构上避免使用 slab_lock\n\n### Slab 状态管理\n- **Node partial slab**: `PG_Workingset && !frozen`\n- **CPU partial slab**: `!PG_Workingset && !frozen`\n- **CPU slab**: `!PG_Workingset && frozen`\n- **Full slab**: `!PG_Workingset && !frozen`\n\n### PREEMPT_RT 支持\n- 在 RT 内核中禁用锁无关快速路径\n- 使用 `migrate_disable()/enable()` 替代 `preempt_disable()/enable()`\n- 本地锁始终被获取以确保 RT 安全性\n\n### 内存管理优化\n- 最小化 slab 设置/拆卸开销，依赖页分配器的 per-CPU 缓存\n- 空 slab 直接释放回页分配器\n- CPU partial slab 机制加速批量释放操作\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **内存管理**: `<linux/mm.h>`, `<linux/swap.h>`, `<linux/memory.h>`\n- **同步原语**: `<linux/bit_spinlock.h>`, `<linux/interrupt.h>`\n- **调试支持**: `<linux/kasan.h>`, `<linux/kmsan.h>`, `<linux/kfence.h>`, `<linux/debugobjects.h>`\n- **系统设施**: `<linux/module.h>`, `<linux/proc_fs.h>`, `<linux/debugfs.h>`\n- **内存控制**: `<linux/memcontrol.h>`, `<linux/cpuset.h>`, `<linux/mempolicy.h>`\n- **测试框架**: `<kunit/test.h>`\n\n### 内部依赖\n- `\"slab.h\"` - slab 分配器通用接口\n- `\"internal.h\"` - 内存管理内部实现\n\n### 子系统交互\n- **页分配器**: 作为底层内存来源\n- **内存热插拔**: 通过 `slab_mutex` 同步回调\n- **内存控制器**: 集成 memcg 功能\n- **跟踪系统**: 通过 `trace/events/kmem.h` 提供分配事件跟踪\n\n## 5. 使用场景\n\n### 内核内存分配\n- 为内核对象（如 task_struct、inode、dentry 等）提供高效的小内存分配\n- 作为 `kmalloc()` 系列函数的底层实现\n- 支持不同大小类别的内存请求（8 字节到几 KB）\n\n### 性能关键路径\n- 中断上下文中的内存分配（通过适当的锁机制保证安全）\n- 高频分配/释放场景（利用 per-CPU slab 和 lockless 快速路径）\n- 批量分配操作（通过 CPU partial slab 优化）\n\n### 调试和监控\n- 内存错误检测（KASAN、KMSAN、KFENCE 集成）\n- 内存泄漏检测（kmemleak 集成）\n- 性能分析（通过 `/proc/slabinfo` 和 debugfs 接口）\n- 故障注入测试（fault-inject 支持）\n\n### 特殊环境支持\n- 实时系统（PREEMPT_RT 配置）\n- 内存受限系统（CONFIG_SLUB_TINY 优化）\n- NUMA 系统（节点感知分配）\n- 内存热插拔环境",
      "similarity": 0.5377923250198364,
      "chunks": [
        {
          "chunk_id": 25,
          "file_path": "mm/slub.c",
          "start_line": 4984,
          "end_line": 5095,
          "content": [
            "static void",
            "init_kmem_cache_node(struct kmem_cache_node *n)",
            "{",
            "\tn->nr_partial = 0;",
            "\tspin_lock_init(&n->list_lock);",
            "\tINIT_LIST_HEAD(&n->partial);",
            "#ifdef CONFIG_SLUB_DEBUG",
            "\tatomic_long_set(&n->nr_slabs, 0);",
            "\tatomic_long_set(&n->total_objects, 0);",
            "\tINIT_LIST_HEAD(&n->full);",
            "#endif",
            "}",
            "static inline int alloc_kmem_cache_cpus(struct kmem_cache *s)",
            "{",
            "\tBUILD_BUG_ON(PERCPU_DYNAMIC_EARLY_SIZE <",
            "\t\t\tNR_KMALLOC_TYPES * KMALLOC_SHIFT_HIGH *",
            "\t\t\tsizeof(struct kmem_cache_cpu));",
            "",
            "\t/*",
            "\t * Must align to double word boundary for the double cmpxchg",
            "\t * instructions to work; see __pcpu_double_call_return_bool().",
            "\t */",
            "\ts->cpu_slab = __alloc_percpu(sizeof(struct kmem_cache_cpu),",
            "\t\t\t\t     2 * sizeof(void *));",
            "",
            "\tif (!s->cpu_slab)",
            "\t\treturn 0;",
            "",
            "\tinit_kmem_cache_cpus(s);",
            "",
            "\treturn 1;",
            "}",
            "static inline int alloc_kmem_cache_cpus(struct kmem_cache *s)",
            "{",
            "\treturn 1;",
            "}",
            "static void early_kmem_cache_node_alloc(int node)",
            "{",
            "\tstruct slab *slab;",
            "\tstruct kmem_cache_node *n;",
            "",
            "\tBUG_ON(kmem_cache_node->size < sizeof(struct kmem_cache_node));",
            "",
            "\tslab = new_slab(kmem_cache_node, GFP_NOWAIT, node);",
            "",
            "\tBUG_ON(!slab);",
            "\tif (slab_nid(slab) != node) {",
            "\t\tpr_err(\"SLUB: Unable to allocate memory from node %d\\n\", node);",
            "\t\tpr_err(\"SLUB: Allocating a useless per node structure in order to be able to continue\\n\");",
            "\t}",
            "",
            "\tn = slab->freelist;",
            "\tBUG_ON(!n);",
            "#ifdef CONFIG_SLUB_DEBUG",
            "\tinit_object(kmem_cache_node, n, SLUB_RED_ACTIVE);",
            "\tinit_tracking(kmem_cache_node, n);",
            "#endif",
            "\tn = kasan_slab_alloc(kmem_cache_node, n, GFP_KERNEL, false);",
            "\tslab->freelist = get_freepointer(kmem_cache_node, n);",
            "\tslab->inuse = 1;",
            "\tkmem_cache_node->node[node] = n;",
            "\tinit_kmem_cache_node(n);",
            "\tinc_slabs_node(kmem_cache_node, node, slab->objects);",
            "",
            "\t/*",
            "\t * No locks need to be taken here as it has just been",
            "\t * initialized and there is no concurrent access.",
            "\t */",
            "\t__add_partial(n, slab, DEACTIVATE_TO_HEAD);",
            "}",
            "static void free_kmem_cache_nodes(struct kmem_cache *s)",
            "{",
            "\tint node;",
            "\tstruct kmem_cache_node *n;",
            "",
            "\tfor_each_kmem_cache_node(s, node, n) {",
            "\t\ts->node[node] = NULL;",
            "\t\tkmem_cache_free(kmem_cache_node, n);",
            "\t}",
            "}",
            "void __kmem_cache_release(struct kmem_cache *s)",
            "{",
            "\tcache_random_seq_destroy(s);",
            "#ifndef CONFIG_SLUB_TINY",
            "\tfree_percpu(s->cpu_slab);",
            "#endif",
            "\tfree_kmem_cache_nodes(s);",
            "}",
            "static int init_kmem_cache_nodes(struct kmem_cache *s)",
            "{",
            "\tint node;",
            "",
            "\tfor_each_node_mask(node, slab_nodes) {",
            "\t\tstruct kmem_cache_node *n;",
            "",
            "\t\tif (slab_state == DOWN) {",
            "\t\t\tearly_kmem_cache_node_alloc(node);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tn = kmem_cache_alloc_node(kmem_cache_node,",
            "\t\t\t\t\t\tGFP_KERNEL, node);",
            "",
            "\t\tif (!n) {",
            "\t\t\tfree_kmem_cache_nodes(s);",
            "\t\t\treturn 0;",
            "\t\t}",
            "",
            "\t\tinit_kmem_cache_node(n);",
            "\t\ts->node[node] = n;",
            "\t}",
            "\treturn 1;",
            "}"
          ],
          "function_name": "init_kmem_cache_node, alloc_kmem_cache_cpus, alloc_kmem_cache_cpus, early_kmem_cache_node_alloc, free_kmem_cache_nodes, __kmem_cache_release, init_kmem_cache_nodes",
          "description": "初始化和释放SLUB缓存节点及其关联的CPU局部结构，包含节点初始化、CPU slab分配、早期节点分配及释放逻辑，用于管理多节点环境下的SLUB缓存结构。",
          "similarity": 0.5250669121742249
        },
        {
          "chunk_id": 16,
          "file_path": "mm/slub.c",
          "start_line": 3059,
          "end_line": 3163,
          "content": [
            "static void put_partials(struct kmem_cache *s)",
            "{",
            "\tstruct slab *partial_slab;",
            "\tunsigned long flags;",
            "",
            "\tlocal_lock_irqsave(&s->cpu_slab->lock, flags);",
            "\tpartial_slab = this_cpu_read(s->cpu_slab->partial);",
            "\tthis_cpu_write(s->cpu_slab->partial, NULL);",
            "\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);",
            "",
            "\tif (partial_slab)",
            "\t\t__put_partials(s, partial_slab);",
            "}",
            "static void put_partials_cpu(struct kmem_cache *s,",
            "\t\t\t     struct kmem_cache_cpu *c)",
            "{",
            "\tstruct slab *partial_slab;",
            "",
            "\tpartial_slab = slub_percpu_partial(c);",
            "\tc->partial = NULL;",
            "",
            "\tif (partial_slab)",
            "\t\t__put_partials(s, partial_slab);",
            "}",
            "static void put_cpu_partial(struct kmem_cache *s, struct slab *slab, int drain)",
            "{",
            "\tstruct slab *oldslab;",
            "\tstruct slab *slab_to_put = NULL;",
            "\tunsigned long flags;",
            "\tint slabs = 0;",
            "",
            "\tlocal_lock_irqsave(&s->cpu_slab->lock, flags);",
            "",
            "\toldslab = this_cpu_read(s->cpu_slab->partial);",
            "",
            "\tif (oldslab) {",
            "\t\tif (drain && oldslab->slabs >= s->cpu_partial_slabs) {",
            "\t\t\t/*",
            "\t\t\t * Partial array is full. Move the existing set to the",
            "\t\t\t * per node partial list. Postpone the actual unfreezing",
            "\t\t\t * outside of the critical section.",
            "\t\t\t */",
            "\t\t\tslab_to_put = oldslab;",
            "\t\t\toldslab = NULL;",
            "\t\t} else {",
            "\t\t\tslabs = oldslab->slabs;",
            "\t\t}",
            "\t}",
            "",
            "\tslabs++;",
            "",
            "\tslab->slabs = slabs;",
            "\tslab->next = oldslab;",
            "",
            "\tthis_cpu_write(s->cpu_slab->partial, slab);",
            "",
            "\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);",
            "",
            "\tif (slab_to_put) {",
            "\t\t__put_partials(s, slab_to_put);",
            "\t\tstat(s, CPU_PARTIAL_DRAIN);",
            "\t}",
            "}",
            "static inline void put_partials(struct kmem_cache *s) { }",
            "static inline void put_partials_cpu(struct kmem_cache *s,",
            "\t\t\t\t    struct kmem_cache_cpu *c) { }",
            "static inline void flush_slab(struct kmem_cache *s, struct kmem_cache_cpu *c)",
            "{",
            "\tunsigned long flags;",
            "\tstruct slab *slab;",
            "\tvoid *freelist;",
            "",
            "\tlocal_lock_irqsave(&s->cpu_slab->lock, flags);",
            "",
            "\tslab = c->slab;",
            "\tfreelist = c->freelist;",
            "",
            "\tc->slab = NULL;",
            "\tc->freelist = NULL;",
            "\tc->tid = next_tid(c->tid);",
            "",
            "\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);",
            "",
            "\tif (slab) {",
            "\t\tdeactivate_slab(s, slab, freelist);",
            "\t\tstat(s, CPUSLAB_FLUSH);",
            "\t}",
            "}",
            "static inline void __flush_cpu_slab(struct kmem_cache *s, int cpu)",
            "{",
            "\tstruct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab, cpu);",
            "\tvoid *freelist = c->freelist;",
            "\tstruct slab *slab = c->slab;",
            "",
            "\tc->slab = NULL;",
            "\tc->freelist = NULL;",
            "\tc->tid = next_tid(c->tid);",
            "",
            "\tif (slab) {",
            "\t\tdeactivate_slab(s, slab, freelist);",
            "\t\tstat(s, CPUSLAB_FLUSH);",
            "\t}",
            "",
            "\tput_partials_cpu(s, c);",
            "}"
          ],
          "function_name": "put_partials, put_partials_cpu, put_cpu_partial, put_partials, put_partials_cpu, flush_slab, __flush_cpu_slab",
          "description": "该代码段实现了SLUB内存分配器中对CPU局部partial slab的管理与回收。  \n`put_cpu_partial`用于将新slab加入当前CPU的partial链表，必要时释放旧slab以防止溢出；`flush_slab`和`__flush_cpu_slab`负责清空CPU本地slab并触发全局回收，通过锁保护确保并发安全。  \n注：代码中`put_partials`等函数存在冗余声明，且缺少`__put_partials`等关键函数的完整实现，上下文不完整。",
          "similarity": 0.5119893550872803
        },
        {
          "chunk_id": 15,
          "file_path": "mm/slub.c",
          "start_line": 2916,
          "end_line": 3045,
          "content": [
            "static void init_kmem_cache_cpus(struct kmem_cache *s)",
            "{",
            "\tint cpu;",
            "\tstruct kmem_cache_cpu *c;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tc = per_cpu_ptr(s->cpu_slab, cpu);",
            "\t\tlocal_lock_init(&c->lock);",
            "\t\tc->tid = init_tid(cpu);",
            "\t}",
            "}",
            "static void deactivate_slab(struct kmem_cache *s, struct slab *slab,",
            "\t\t\t    void *freelist)",
            "{",
            "\tstruct kmem_cache_node *n = get_node(s, slab_nid(slab));",
            "\tint free_delta = 0;",
            "\tvoid *nextfree, *freelist_iter, *freelist_tail;",
            "\tint tail = DEACTIVATE_TO_HEAD;",
            "\tunsigned long flags = 0;",
            "\tstruct slab new;",
            "\tstruct slab old;",
            "",
            "\tif (READ_ONCE(slab->freelist)) {",
            "\t\tstat(s, DEACTIVATE_REMOTE_FREES);",
            "\t\ttail = DEACTIVATE_TO_TAIL;",
            "\t}",
            "",
            "\t/*",
            "\t * Stage one: Count the objects on cpu's freelist as free_delta and",
            "\t * remember the last object in freelist_tail for later splicing.",
            "\t */",
            "\tfreelist_tail = NULL;",
            "\tfreelist_iter = freelist;",
            "\twhile (freelist_iter) {",
            "\t\tnextfree = get_freepointer(s, freelist_iter);",
            "",
            "\t\t/*",
            "\t\t * If 'nextfree' is invalid, it is possible that the object at",
            "\t\t * 'freelist_iter' is already corrupted.  So isolate all objects",
            "\t\t * starting at 'freelist_iter' by skipping them.",
            "\t\t */",
            "\t\tif (freelist_corrupted(s, slab, &freelist_iter, nextfree))",
            "\t\t\tbreak;",
            "",
            "\t\tfreelist_tail = freelist_iter;",
            "\t\tfree_delta++;",
            "",
            "\t\tfreelist_iter = nextfree;",
            "\t}",
            "",
            "\t/*",
            "\t * Stage two: Unfreeze the slab while splicing the per-cpu",
            "\t * freelist to the head of slab's freelist.",
            "\t */",
            "\tdo {",
            "\t\told.freelist = READ_ONCE(slab->freelist);",
            "\t\told.counters = READ_ONCE(slab->counters);",
            "\t\tVM_BUG_ON(!old.frozen);",
            "",
            "\t\t/* Determine target state of the slab */",
            "\t\tnew.counters = old.counters;",
            "\t\tnew.frozen = 0;",
            "\t\tif (freelist_tail) {",
            "\t\t\tnew.inuse -= free_delta;",
            "\t\t\tset_freepointer(s, freelist_tail, old.freelist);",
            "\t\t\tnew.freelist = freelist;",
            "\t\t} else {",
            "\t\t\tnew.freelist = old.freelist;",
            "\t\t}",
            "\t} while (!slab_update_freelist(s, slab,",
            "\t\told.freelist, old.counters,",
            "\t\tnew.freelist, new.counters,",
            "\t\t\"unfreezing slab\"));",
            "",
            "\t/*",
            "\t * Stage three: Manipulate the slab list based on the updated state.",
            "\t */",
            "\tif (!new.inuse && n->nr_partial >= s->min_partial) {",
            "\t\tstat(s, DEACTIVATE_EMPTY);",
            "\t\tdiscard_slab(s, slab);",
            "\t\tstat(s, FREE_SLAB);",
            "\t} else if (new.freelist) {",
            "\t\tspin_lock_irqsave(&n->list_lock, flags);",
            "\t\tadd_partial(n, slab, tail);",
            "\t\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "\t\tstat(s, tail);",
            "\t} else {",
            "\t\tstat(s, DEACTIVATE_FULL);",
            "\t}",
            "}",
            "static void __put_partials(struct kmem_cache *s, struct slab *partial_slab)",
            "{",
            "\tstruct kmem_cache_node *n = NULL, *n2 = NULL;",
            "\tstruct slab *slab, *slab_to_discard = NULL;",
            "\tunsigned long flags = 0;",
            "",
            "\twhile (partial_slab) {",
            "\t\tslab = partial_slab;",
            "\t\tpartial_slab = slab->next;",
            "",
            "\t\tn2 = get_node(s, slab_nid(slab));",
            "\t\tif (n != n2) {",
            "\t\t\tif (n)",
            "\t\t\t\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "",
            "\t\t\tn = n2;",
            "\t\t\tspin_lock_irqsave(&n->list_lock, flags);",
            "\t\t}",
            "",
            "\t\tif (unlikely(!slab->inuse && n->nr_partial >= s->min_partial)) {",
            "\t\t\tslab->next = slab_to_discard;",
            "\t\t\tslab_to_discard = slab;",
            "\t\t} else {",
            "\t\t\tadd_partial(n, slab, DEACTIVATE_TO_TAIL);",
            "\t\t\tstat(s, FREE_ADD_PARTIAL);",
            "\t\t}",
            "\t}",
            "",
            "\tif (n)",
            "\t\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "",
            "\twhile (slab_to_discard) {",
            "\t\tslab = slab_to_discard;",
            "\t\tslab_to_discard = slab_to_discard->next;",
            "",
            "\t\tstat(s, DEACTIVATE_EMPTY);",
            "\t\tdiscard_slab(s, slab);",
            "\t\tstat(s, FREE_SLAB);",
            "\t}",
            "}"
          ],
          "function_name": "init_kmem_cache_cpus, deactivate_slab, __put_partials",
          "description": "该代码段实现SLUB内存分配器中多CPU协作与内存回收机制。  \n`init_kmem_cache_cpus` 初始化每个CPU的本地slab结构并设置tid；`deactivate_slab` 解冻slab并调整其freelist与计数器，决定是否丢弃或迁移至部分链表；`__put_partials` 管理部分填充slab的回收，判断是否满足空闲条件后进行丢弃。  \n上下文完整，涵盖SLUB分配器中CPU本地缓存同步、slab状态转换及部分对象回收逻辑。",
          "similarity": 0.4981499910354614
        },
        {
          "chunk_id": 17,
          "file_path": "mm/slub.c",
          "start_line": 3191,
          "end_line": 3333,
          "content": [
            "static void flush_cpu_slab(struct work_struct *w)",
            "{",
            "\tstruct kmem_cache *s;",
            "\tstruct kmem_cache_cpu *c;",
            "\tstruct slub_flush_work *sfw;",
            "",
            "\tsfw = container_of(w, struct slub_flush_work, work);",
            "",
            "\ts = sfw->s;",
            "\tc = this_cpu_ptr(s->cpu_slab);",
            "",
            "\tif (c->slab)",
            "\t\tflush_slab(s, c);",
            "",
            "\tput_partials(s);",
            "}",
            "static bool has_cpu_slab(int cpu, struct kmem_cache *s)",
            "{",
            "\tstruct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab, cpu);",
            "",
            "\treturn c->slab || slub_percpu_partial(c);",
            "}",
            "static void flush_all_cpus_locked(struct kmem_cache *s)",
            "{",
            "\tstruct slub_flush_work *sfw;",
            "\tunsigned int cpu;",
            "",
            "\tlockdep_assert_cpus_held();",
            "\tmutex_lock(&flush_lock);",
            "",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tsfw = &per_cpu(slub_flush, cpu);",
            "\t\tif (!has_cpu_slab(cpu, s)) {",
            "\t\t\tsfw->skip = true;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tINIT_WORK(&sfw->work, flush_cpu_slab);",
            "\t\tsfw->skip = false;",
            "\t\tsfw->s = s;",
            "\t\tqueue_work_on(cpu, flushwq, &sfw->work);",
            "\t}",
            "",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tsfw = &per_cpu(slub_flush, cpu);",
            "\t\tif (sfw->skip)",
            "\t\t\tcontinue;",
            "\t\tflush_work(&sfw->work);",
            "\t}",
            "",
            "\tmutex_unlock(&flush_lock);",
            "}",
            "static void flush_all(struct kmem_cache *s)",
            "{",
            "\tcpus_read_lock();",
            "\tflush_all_cpus_locked(s);",
            "\tcpus_read_unlock();",
            "}",
            "static int slub_cpu_dead(unsigned int cpu)",
            "{",
            "\tstruct kmem_cache *s;",
            "",
            "\tmutex_lock(&slab_mutex);",
            "\tlist_for_each_entry(s, &slab_caches, list)",
            "\t\t__flush_cpu_slab(s, cpu);",
            "\tmutex_unlock(&slab_mutex);",
            "\treturn 0;",
            "}",
            "static inline void flush_all_cpus_locked(struct kmem_cache *s) { }",
            "static inline void flush_all(struct kmem_cache *s) { }",
            "static inline void __flush_cpu_slab(struct kmem_cache *s, int cpu) { }",
            "static inline int slub_cpu_dead(unsigned int cpu) { return 0; }",
            "static inline int node_match(struct slab *slab, int node)",
            "{",
            "#ifdef CONFIG_NUMA",
            "\tif (node != NUMA_NO_NODE && slab_nid(slab) != node)",
            "\t\treturn 0;",
            "#endif",
            "\treturn 1;",
            "}",
            "static int count_free(struct slab *slab)",
            "{",
            "\treturn slab->objects - slab->inuse;",
            "}",
            "static inline unsigned long node_nr_objs(struct kmem_cache_node *n)",
            "{",
            "\treturn atomic_long_read(&n->total_objects);",
            "}",
            "static inline bool free_debug_processing(struct kmem_cache *s,",
            "\tstruct slab *slab, void *head, void *tail, int *bulk_cnt,",
            "\tunsigned long addr, depot_stack_handle_t handle)",
            "{",
            "\tbool checks_ok = false;",
            "\tvoid *object = head;",
            "\tint cnt = 0;",
            "",
            "\tif (s->flags & SLAB_CONSISTENCY_CHECKS) {",
            "\t\tif (!check_slab(s, slab))",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tif (slab->inuse < *bulk_cnt) {",
            "\t\tslab_err(s, slab, \"Slab has %d allocated objects but %d are to be freed\\n\",",
            "\t\t\t slab->inuse, *bulk_cnt);",
            "\t\tgoto out;",
            "\t}",
            "",
            "next_object:",
            "",
            "\tif (++cnt > *bulk_cnt)",
            "\t\tgoto out_cnt;",
            "",
            "\tif (s->flags & SLAB_CONSISTENCY_CHECKS) {",
            "\t\tif (!free_consistency_checks(s, slab, object, addr))",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tif (s->flags & SLAB_STORE_USER)",
            "\t\tset_track_update(s, object, TRACK_FREE, addr, handle);",
            "\ttrace(s, slab, object, 0);",
            "\t/* Freepointer not overwritten by init_object(), SLAB_POISON moved it */",
            "\tinit_object(s, object, SLUB_RED_INACTIVE);",
            "",
            "\t/* Reached end of constructed freelist yet? */",
            "\tif (object != tail) {",
            "\t\tobject = get_freepointer(s, object);",
            "\t\tgoto next_object;",
            "\t}",
            "\tchecks_ok = true;",
            "",
            "out_cnt:",
            "\tif (cnt != *bulk_cnt) {",
            "\t\tslab_err(s, slab, \"Bulk free expected %d objects but found %d\\n\",",
            "\t\t\t *bulk_cnt, cnt);",
            "\t\t*bulk_cnt = cnt;",
            "\t}",
            "",
            "out:",
            "",
            "\tif (!checks_ok)",
            "\t\tslab_fix(s, \"Object at 0x%p not freed\", object);",
            "",
            "\treturn checks_ok;",
            "}"
          ],
          "function_name": "flush_cpu_slab, has_cpu_slab, flush_all_cpus_locked, flush_all, slub_cpu_dead, flush_all_cpus_locked, flush_all, __flush_cpu_slab, slub_cpu_dead, node_match, count_free, node_nr_objs, free_debug_processing",
          "description": "该代码段实现了SLUB内存分配器中CPU本地缓存的刷新与清理机制，核心功能包括多CPU协作的slab数据同步及异常情况下（如CPU下线）的资源回收。  \n`flush_all_cpus_locked`协调所有CPU执行slab刷新任务，通过工作队列异步处理；`slub_cpu_dead`在CPU死亡时遍历全局slab进行清理，而`has_cpu_slab`等辅助函数用于判断CPU缓存状态。  \n注：部分函数（如`__flush_cpu_slab`）仅声明未实现，上下文不完整。",
          "similarity": 0.487151563167572
        },
        {
          "chunk_id": 6,
          "file_path": "mm/slub.c",
          "start_line": 1240,
          "end_line": 1377,
          "content": [
            "static int check_pad_bytes(struct kmem_cache *s, struct slab *slab, u8 *p)",
            "{",
            "\tunsigned long off = get_info_end(s);\t/* The end of info */",
            "",
            "\tif (s->flags & SLAB_STORE_USER) {",
            "\t\t/* We also have user information there */",
            "\t\toff += 2 * sizeof(struct track);",
            "",
            "\t\tif (s->flags & SLAB_KMALLOC)",
            "\t\t\toff += sizeof(unsigned int);",
            "\t}",
            "",
            "\toff += kasan_metadata_size(s, false);",
            "",
            "\tif (size_from_object(s) == off)",
            "\t\treturn 1;",
            "",
            "\treturn check_bytes_and_report(s, slab, p, \"Object padding\",",
            "\t\t\tp + off, POISON_INUSE, size_from_object(s) - off);",
            "}",
            "static void slab_pad_check(struct kmem_cache *s, struct slab *slab)",
            "{",
            "\tu8 *start;",
            "\tu8 *fault;",
            "\tu8 *end;",
            "\tu8 *pad;",
            "\tint length;",
            "\tint remainder;",
            "",
            "\tif (!(s->flags & SLAB_POISON))",
            "\t\treturn;",
            "",
            "\tstart = slab_address(slab);",
            "\tlength = slab_size(slab);",
            "\tend = start + length;",
            "\tremainder = length % s->size;",
            "\tif (!remainder)",
            "\t\treturn;",
            "",
            "\tpad = end - remainder;",
            "\tmetadata_access_enable();",
            "\tfault = memchr_inv(kasan_reset_tag(pad), POISON_INUSE, remainder);",
            "\tmetadata_access_disable();",
            "\tif (!fault)",
            "\t\treturn;",
            "\twhile (end > fault && end[-1] == POISON_INUSE)",
            "\t\tend--;",
            "",
            "\tslab_err(s, slab, \"Padding overwritten. 0x%p-0x%p @offset=%tu\",",
            "\t\t\tfault, end - 1, fault - start);",
            "\tprint_section(KERN_ERR, \"Padding \", pad, remainder);",
            "",
            "\trestore_bytes(s, \"slab padding\", POISON_INUSE, fault, end);",
            "}",
            "static int check_object(struct kmem_cache *s, struct slab *slab,",
            "\t\t\t\t\tvoid *object, u8 val)",
            "{",
            "\tu8 *p = object;",
            "\tu8 *endobject = object + s->object_size;",
            "\tunsigned int orig_size, kasan_meta_size;",
            "\tint ret = 1;",
            "",
            "\tif (s->flags & SLAB_RED_ZONE) {",
            "\t\tif (!check_bytes_and_report(s, slab, object, \"Left Redzone\",",
            "\t\t\tobject - s->red_left_pad, val, s->red_left_pad))",
            "\t\t\tret = 0;",
            "",
            "\t\tif (!check_bytes_and_report(s, slab, object, \"Right Redzone\",",
            "\t\t\tendobject, val, s->inuse - s->object_size))",
            "\t\t\tret = 0;",
            "",
            "\t\tif (slub_debug_orig_size(s) && val == SLUB_RED_ACTIVE) {",
            "\t\t\torig_size = get_orig_size(s, object);",
            "",
            "\t\t\tif (s->object_size > orig_size  &&",
            "\t\t\t\t!check_bytes_and_report(s, slab, object,",
            "\t\t\t\t\t\"kmalloc Redzone\", p + orig_size,",
            "\t\t\t\t\tval, s->object_size - orig_size)) {",
            "\t\t\t\tret = 0;",
            "\t\t\t}",
            "\t\t}",
            "\t} else {",
            "\t\tif ((s->flags & SLAB_POISON) && s->object_size < s->inuse) {",
            "\t\t\tif (!check_bytes_and_report(s, slab, p, \"Alignment padding\",",
            "\t\t\t\tendobject, POISON_INUSE,",
            "\t\t\t\ts->inuse - s->object_size))",
            "\t\t\t\tret = 0;",
            "\t\t}",
            "\t}",
            "",
            "\tif (s->flags & SLAB_POISON) {",
            "\t\tif (val != SLUB_RED_ACTIVE && (s->flags & __OBJECT_POISON)) {",
            "\t\t\t/*",
            "\t\t\t * KASAN can save its free meta data inside of the",
            "\t\t\t * object at offset 0. Thus, skip checking the part of",
            "\t\t\t * the redzone that overlaps with the meta data.",
            "\t\t\t */",
            "\t\t\tkasan_meta_size = kasan_metadata_size(s, true);",
            "\t\t\tif (kasan_meta_size < s->object_size - 1 &&",
            "\t\t\t    !check_bytes_and_report(s, slab, p, \"Poison\",",
            "\t\t\t\t\tp + kasan_meta_size, POISON_FREE,",
            "\t\t\t\t\ts->object_size - kasan_meta_size - 1))",
            "\t\t\t\tret = 0;",
            "\t\t\tif (kasan_meta_size < s->object_size &&",
            "\t\t\t    !check_bytes_and_report(s, slab, p, \"End Poison\",",
            "\t\t\t\t\tp + s->object_size - 1, POISON_END, 1))",
            "\t\t\t\tret = 0;",
            "\t\t}",
            "\t\t/*",
            "\t\t * check_pad_bytes cleans up on its own.",
            "\t\t */",
            "\t\tif (!check_pad_bytes(s, slab, p))",
            "\t\t\tret = 0;",
            "\t}",
            "",
            "\t/*",
            "\t * Cannot check freepointer while object is allocated if",
            "\t * object and freepointer overlap.",
            "\t */",
            "\tif ((freeptr_outside_object(s) || val != SLUB_RED_ACTIVE) &&",
            "\t    !check_valid_pointer(s, slab, get_freepointer(s, p))) {",
            "\t\tobject_err(s, slab, p, \"Freepointer corrupt\");",
            "\t\t/*",
            "\t\t * No choice but to zap it and thus lose the remainder",
            "\t\t * of the free objects in this slab. May cause",
            "\t\t * another error because the object count is now wrong.",
            "\t\t */",
            "\t\tset_freepointer(s, p, NULL);",
            "\t\tret = 0;",
            "\t}",
            "",
            "\tif (!ret && !slab_in_kunit_test()) {",
            "\t\tprint_trailer(s, slab, object);",
            "\t\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "check_pad_bytes, slab_pad_check, check_object",
          "description": "检查slab填充区域及对象边界完整性，通过毒化值校验防止越界覆盖并修复异常",
          "similarity": 0.48566126823425293
        }
      ]
    },
    {
      "source_file": "mm/slab_common.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:22:45\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `slab_common.c`\n\n---\n\n# slab_common.c 技术文档\n\n## 1. 文件概述\n\n`slab_common.c` 是 Linux 内核中 Slab 分配器的通用实现文件，包含与具体分配策略（如 SLAB、SLUB、SLOB）无关的公共函数和基础设施。该文件负责管理 Slab 缓存的创建、合并、销毁等核心逻辑，并提供统一的接口供上层使用。它实现了缓存注册、命名、对齐计算、调试支持以及与内存子系统交互的基础功能。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `slab_state`：表示 Slab 子系统的初始化状态（如 DOWN、PARTIAL、UP 等）\n- `slab_caches`：全局链表，维护所有已注册的 `kmem_cache` 实例\n- `slab_mutex`：保护 `slab_caches` 链表的互斥锁\n- `kmem_cache`：指向用于分配 `kmem_cache` 结构本身的缓存对象\n- `slab_caches_to_rcu_destroy`：待通过 RCU 安全方式销毁的缓存链表\n- `slab_nomerge`：控制是否禁止 Slab 缓存合并的布尔标志\n\n### 主要函数\n- `kmem_cache_size()`：返回指定缓存中对象的实际大小\n- `calculate_alignment()`：根据标志、用户对齐要求和对象大小计算最终对齐值\n- `slab_unmergeable()`：判断给定缓存是否可被合并\n- `find_mergeable()`：在现有缓存中查找可合并的目标缓存\n- `create_cache()`：创建新的 `kmem_cache` 实例\n- `__kmem_cache_create_args()`：带参数的缓存创建主入口函数\n\n### 关键宏定义\n- `SLAB_NEVER_MERGE`：包含禁止缓存合并的标志集合（如 RED_ZONE、POISON 等）\n- `SLAB_MERGE_SAME`：合并时必须相同的标志集合（如 DMA 相关、ACCOUNT 等）\n\n## 3. 关键实现\n\n### 缓存合并机制\n文件实现了智能的缓存合并策略：\n1. 通过 `slab_nomerge` 全局开关或内核启动参数（`slab_nomerge`/`slab_merge`）控制是否启用合并\n2. 使用 `SLAB_NEVER_MERGE` 排除带有调试或特殊语义标志的缓存\n3. 在 `find_mergeable()` 中遍历 `slab_caches` 链表，检查：\n   - 对象大小兼容性（新缓存 ≤ 现有缓存）\n   - 必须相同的标志位一致\n   - 对齐兼容性（现有缓存大小是新对齐的整数倍）\n   - 内存浪费不超过一个指针大小\n\n### 对齐计算\n`calculate_alignment()` 实现了分层对齐策略：\n- 若设置 `SLAB_HWCACHE_ALIGN`，则基于缓存行大小动态调整（对象越小，对齐粒度越细）\n- 始终满足架构最小对齐要求（`arch_slab_minalign()`）\n- 最终对齐值向上对齐到指针大小的倍数\n\n### 安全与调试支持\n- **完整性检查**：`kmem_cache_sanity_check()` 在 `CONFIG_DEBUG_VM` 下验证缓存名和大小合法性\n- **用户复制硬化**：集成 `CONFIG_HARDENED_USERCOPY` 检查用户区域偏移/大小有效性\n- **调试标志处理**：自动启用 `slub_debug_enabled` 静态分支和 `stack_depot` 初始化\n- **KFENCE/KASAN 集成**：通过头文件包含支持内存错误检测框架\n\n### RCU 安全销毁\n通过工作队列 `slab_caches_to_rcu_destroy_work` 延迟销毁缓存，确保在 RCU 宽限期结束后释放内存，避免并发访问已释放结构。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心子系统**：`<linux/slab.h>`, `<linux/mm.h>`, `<linux/memory.h>`\n- **调试设施**：`<linux/kasan.h>`, `<linux/kfence.h>`, `<linux/kmemleak.h>`\n- **架构相关**：`<asm/cacheflush.h>`, `<asm/page.h>`\n- **内部实现**：`\"internal.h\"`, `\"slab.h\"`（包含分配器特定接口）\n\n### 功能依赖\n- **内存管理**：依赖页分配器（`alloc_pages`）和内存控制组（`memcontrol`）\n- **同步机制**：使用 mutex、RCU 和 workqueue 实现并发控制\n- **调试框架**：与 KASAN、KFENCE、SLUB_DEBUG 等调试子系统深度集成\n- **DMA 支持**：通过 `SLAB_CACHE_DMA`/`SLAB_CACHE_DMA32` 标志与 DMA 映射子系统交互\n\n## 5. 使用场景\n\n### 内核初始化\n- 在 `start_kernel()` 早期阶段初始化 `kmem_cache` 自身（bootstrap 过程）\n- 通过 `__setup_param` 处理内核命令行参数（如 `slab_nomerge`）\n\n### 动态缓存创建\n- 当驱动或子系统调用 `kmem_cache_create()` 时，经由此文件的 `__kmem_cache_create_args()` 创建新缓存\n- 自动尝试合并相似缓存以减少内存碎片（除非显式禁用）\n\n### 调试与监控\n- `/proc/slabinfo` 和 debugfs 接口通过此文件获取缓存列表信息\n- 内存错误检测工具（KASAN/KFENCE）利用此文件的钩子注入检测逻辑\n\n### 特殊内存分配\n- 支持 DMA 缓存（`SLAB_CACHE_DMA`）、RCU 安全释放（`SLAB_TYPESAFE_BY_RCU`）等特殊场景\n- 为 hardened usercopy 提供对象边界验证所需元数据（`useroffset`/`usersize`）",
      "similarity": 0.4958198070526123,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/slab_common.c",
          "start_line": 63,
          "end_line": 176,
          "content": [
            "static int __init setup_slab_nomerge(char *str)",
            "{",
            "\tslab_nomerge = true;",
            "\treturn 1;",
            "}",
            "static int __init setup_slab_merge(char *str)",
            "{",
            "\tslab_nomerge = false;",
            "\treturn 1;",
            "}",
            "unsigned int kmem_cache_size(struct kmem_cache *s)",
            "{",
            "\treturn s->object_size;",
            "}",
            "static int kmem_cache_sanity_check(const char *name, unsigned int size)",
            "{",
            "\tif (!name || in_interrupt() || size > KMALLOC_MAX_SIZE) {",
            "\t\tpr_err(\"kmem_cache_create(%s) integrity check failed\\n\", name);",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tWARN_ON(strchr(name, ' '));\t/* It confuses parsers */",
            "\treturn 0;",
            "}",
            "static inline int kmem_cache_sanity_check(const char *name, unsigned int size)",
            "{",
            "\treturn 0;",
            "}",
            "static unsigned int calculate_alignment(slab_flags_t flags,",
            "\t\tunsigned int align, unsigned int size)",
            "{",
            "\t/*",
            "\t * If the user wants hardware cache aligned objects then follow that",
            "\t * suggestion if the object is sufficiently large.",
            "\t *",
            "\t * The hardware cache alignment cannot override the specified",
            "\t * alignment though. If that is greater then use it.",
            "\t */",
            "\tif (flags & SLAB_HWCACHE_ALIGN) {",
            "\t\tunsigned int ralign;",
            "",
            "\t\tralign = cache_line_size();",
            "\t\twhile (size <= ralign / 2)",
            "\t\t\tralign /= 2;",
            "\t\talign = max(align, ralign);",
            "\t}",
            "",
            "\talign = max(align, arch_slab_minalign());",
            "",
            "\treturn ALIGN(align, sizeof(void *));",
            "}",
            "int slab_unmergeable(struct kmem_cache *s)",
            "{",
            "\tif (slab_nomerge || (s->flags & SLAB_NEVER_MERGE))",
            "\t\treturn 1;",
            "",
            "\tif (s->ctor)",
            "\t\treturn 1;",
            "",
            "#ifdef CONFIG_HARDENED_USERCOPY",
            "\tif (s->usersize)",
            "\t\treturn 1;",
            "#endif",
            "",
            "\t/*",
            "\t * We may have set a slab to be unmergeable during bootstrap.",
            "\t */",
            "\tif (s->refcount < 0)",
            "\t\treturn 1;",
            "",
            "\treturn 0;",
            "}",
            "static void kmem_cache_release(struct kmem_cache *s)",
            "{",
            "\tif (slab_state >= FULL) {",
            "\t\tsysfs_slab_unlink(s);",
            "\t\tsysfs_slab_release(s);",
            "\t} else {",
            "\t\tslab_kmem_cache_release(s);",
            "\t}",
            "}",
            "static void kmem_cache_release(struct kmem_cache *s)",
            "{",
            "\tslab_kmem_cache_release(s);",
            "}",
            "static void slab_caches_to_rcu_destroy_workfn(struct work_struct *work)",
            "{",
            "\tLIST_HEAD(to_destroy);",
            "\tstruct kmem_cache *s, *s2;",
            "",
            "\t/*",
            "\t * On destruction, SLAB_TYPESAFE_BY_RCU kmem_caches are put on the",
            "\t * @slab_caches_to_rcu_destroy list.  The slab pages are freed",
            "\t * through RCU and the associated kmem_cache are dereferenced",
            "\t * while freeing the pages, so the kmem_caches should be freed only",
            "\t * after the pending RCU operations are finished.  As rcu_barrier()",
            "\t * is a pretty slow operation, we batch all pending destructions",
            "\t * asynchronously.",
            "\t */",
            "\tmutex_lock(&slab_mutex);",
            "\tlist_splice_init(&slab_caches_to_rcu_destroy, &to_destroy);",
            "\tmutex_unlock(&slab_mutex);",
            "",
            "\tif (list_empty(&to_destroy))",
            "\t\treturn;",
            "",
            "\trcu_barrier();",
            "",
            "\tlist_for_each_entry_safe(s, s2, &to_destroy, list) {",
            "\t\tdebugfs_slab_release(s);",
            "\t\tkfence_shutdown_cache(s);",
            "\t\tkmem_cache_release(s);",
            "\t}",
            "}"
          ],
          "function_name": "setup_slab_nomerge, setup_slab_merge, kmem_cache_size, kmem_cache_sanity_check, kmem_cache_sanity_check, calculate_alignment, slab_unmergeable, kmem_cache_release, kmem_cache_release, slab_caches_to_rcu_destroy_workfn",
          "description": "实现Slab缓存合并控制逻辑，通过setup_slab_nomerge/setup_slab_merge设置合并禁用标志，calculate_alignment计算对齐需求，slab_unmergeable判定缓存是否可合并，kmem_cache_release处理缓存释放并关联RCU安全销毁工作队列。",
          "similarity": 0.5010675191879272
        },
        {
          "chunk_id": 2,
          "file_path": "mm/slab_common.c",
          "start_line": 497,
          "end_line": 617,
          "content": [
            "static int shutdown_cache(struct kmem_cache *s)",
            "{",
            "\t/* free asan quarantined objects */",
            "\tkasan_cache_shutdown(s);",
            "",
            "\tif (__kmem_cache_shutdown(s) != 0)",
            "\t\treturn -EBUSY;",
            "",
            "\tlist_del(&s->list);",
            "",
            "\tif (s->flags & SLAB_TYPESAFE_BY_RCU) {",
            "\t\tlist_add_tail(&s->list, &slab_caches_to_rcu_destroy);",
            "\t\tschedule_work(&slab_caches_to_rcu_destroy_work);",
            "\t} else {",
            "\t\tkfence_shutdown_cache(s);",
            "\t\tdebugfs_slab_release(s);",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "void slab_kmem_cache_release(struct kmem_cache *s)",
            "{",
            "\t__kmem_cache_release(s);",
            "\tkfree_const(s->name);",
            "\tkmem_cache_free(kmem_cache, s);",
            "}",
            "void kmem_cache_destroy(struct kmem_cache *s)",
            "{",
            "\tint err = -EBUSY;",
            "\tbool rcu_set;",
            "",
            "\tif (unlikely(!s) || !kasan_check_byte(s))",
            "\t\treturn;",
            "",
            "\tcpus_read_lock();",
            "\tmutex_lock(&slab_mutex);",
            "",
            "\trcu_set = s->flags & SLAB_TYPESAFE_BY_RCU;",
            "",
            "\ts->refcount--;",
            "\tif (s->refcount)",
            "\t\tgoto out_unlock;",
            "",
            "\terr = shutdown_cache(s);",
            "\tWARN(err, \"%s %s: Slab cache still has objects when called from %pS\",",
            "\t     __func__, s->name, (void *)_RET_IP_);",
            "out_unlock:",
            "\tmutex_unlock(&slab_mutex);",
            "\tcpus_read_unlock();",
            "\tif (!err && !rcu_set)",
            "\t\tkmem_cache_release(s);",
            "}",
            "int kmem_cache_shrink(struct kmem_cache *cachep)",
            "{",
            "\tkasan_cache_shrink(cachep);",
            "",
            "\treturn __kmem_cache_shrink(cachep);",
            "}",
            "bool slab_is_available(void)",
            "{",
            "\treturn slab_state >= UP;",
            "}",
            "static void kmem_obj_info(struct kmem_obj_info *kpp, void *object, struct slab *slab)",
            "{",
            "\tif (__kfence_obj_info(kpp, object, slab))",
            "\t\treturn;",
            "\t__kmem_obj_info(kpp, object, slab);",
            "}",
            "bool kmem_dump_obj(void *object)",
            "{",
            "\tchar *cp = IS_ENABLED(CONFIG_MMU) ? \"\" : \"/vmalloc\";",
            "\tint i;",
            "\tstruct slab *slab;",
            "\tunsigned long ptroffset;",
            "\tstruct kmem_obj_info kp = { };",
            "",
            "\t/* Some arches consider ZERO_SIZE_PTR to be a valid address. */",
            "\tif (object < (void *)PAGE_SIZE || !virt_addr_valid(object))",
            "\t\treturn false;",
            "\tslab = virt_to_slab(object);",
            "\tif (!slab)",
            "\t\treturn false;",
            "",
            "\tkmem_obj_info(&kp, object, slab);",
            "\tif (kp.kp_slab_cache)",
            "\t\tpr_cont(\" slab%s %s\", cp, kp.kp_slab_cache->name);",
            "\telse",
            "\t\tpr_cont(\" slab%s\", cp);",
            "\tif (is_kfence_address(object))",
            "\t\tpr_cont(\" (kfence)\");",
            "\tif (kp.kp_objp)",
            "\t\tpr_cont(\" start %px\", kp.kp_objp);",
            "\tif (kp.kp_data_offset)",
            "\t\tpr_cont(\" data offset %lu\", kp.kp_data_offset);",
            "\tif (kp.kp_objp) {",
            "\t\tptroffset = ((char *)object - (char *)kp.kp_objp) - kp.kp_data_offset;",
            "\t\tpr_cont(\" pointer offset %lu\", ptroffset);",
            "\t}",
            "\tif (kp.kp_slab_cache && kp.kp_slab_cache->object_size)",
            "\t\tpr_cont(\" size %u\", kp.kp_slab_cache->object_size);",
            "\tif (kp.kp_ret)",
            "\t\tpr_cont(\" allocated at %pS\\n\", kp.kp_ret);",
            "\telse",
            "\t\tpr_cont(\"\\n\");",
            "\tfor (i = 0; i < ARRAY_SIZE(kp.kp_stack); i++) {",
            "\t\tif (!kp.kp_stack[i])",
            "\t\t\tbreak;",
            "\t\tpr_info(\"    %pS\\n\", kp.kp_stack[i]);",
            "\t}",
            "",
            "\tif (kp.kp_free_stack[0])",
            "\t\tpr_cont(\" Free path:\\n\");",
            "",
            "\tfor (i = 0; i < ARRAY_SIZE(kp.kp_free_stack); i++) {",
            "\t\tif (!kp.kp_free_stack[i])",
            "\t\t\tbreak;",
            "\t\tpr_info(\"    %pS\\n\", kp.kp_free_stack[i]);",
            "\t}",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "shutdown_cache, slab_kmem_cache_release, kmem_cache_destroy, kmem_cache_shrink, slab_is_available, kmem_obj_info, kmem_dump_obj",
          "description": "提供Slab缓存生命周期管理接口，shutdown_cache执行缓存关闭操作，slab_kmem_cache_release释放缓存资源，kmem_cache_destroy实现缓存销毁并校验引用计数，kmem_dump_obj输出对象详细调试信息。",
          "similarity": 0.4813219904899597
        },
        {
          "chunk_id": 0,
          "file_path": "mm/slab_common.c",
          "start_line": 1,
          "end_line": 62,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Slab allocator functions that are independent of the allocator strategy",
            " *",
            " * (C) 2012 Christoph Lameter <cl@linux.com>",
            " */",
            "#include <linux/slab.h>",
            "",
            "#include <linux/mm.h>",
            "#include <linux/poison.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/memory.h>",
            "#include <linux/cache.h>",
            "#include <linux/compiler.h>",
            "#include <linux/kfence.h>",
            "#include <linux/module.h>",
            "#include <linux/cpu.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/dma-mapping.h>",
            "#include <linux/swiotlb.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/kasan.h>",
            "#include <asm/cacheflush.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/page.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/stackdepot.h>",
            "",
            "#include \"internal.h\"",
            "#include \"slab.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/kmem.h>",
            "",
            "enum slab_state slab_state;",
            "LIST_HEAD(slab_caches);",
            "DEFINE_MUTEX(slab_mutex);",
            "struct kmem_cache *kmem_cache;",
            "",
            "static LIST_HEAD(slab_caches_to_rcu_destroy);",
            "static void slab_caches_to_rcu_destroy_workfn(struct work_struct *work);",
            "static DECLARE_WORK(slab_caches_to_rcu_destroy_work,",
            "\t\t    slab_caches_to_rcu_destroy_workfn);",
            "",
            "/*",
            " * Set of flags that will prevent slab merging",
            " */",
            "#define SLAB_NEVER_MERGE (SLAB_RED_ZONE | SLAB_POISON | SLAB_STORE_USER | \\",
            "\t\tSLAB_TRACE | SLAB_TYPESAFE_BY_RCU | SLAB_NOLEAKTRACE | \\",
            "\t\tSLAB_FAILSLAB | SLAB_NO_MERGE)",
            "",
            "#define SLAB_MERGE_SAME (SLAB_RECLAIM_ACCOUNT | SLAB_CACHE_DMA | \\",
            "\t\t\t SLAB_CACHE_DMA32 | SLAB_ACCOUNT)",
            "",
            "/*",
            " * Merge control. If this is set then no merging of slab caches will occur.",
            " */",
            "static bool slab_nomerge = !IS_ENABLED(CONFIG_SLAB_MERGE_DEFAULT);",
            ""
          ],
          "function_name": null,
          "description": "定义Slab分配器的基础结构，包括全局变量slab_state、slab_caches链表、互斥锁slab_mutex及kmem_cache指针，声明SLAB_NEVER_MERGE等合并控制相关标志位，用于后续Slab缓存合并策略的配置。",
          "similarity": 0.4729459285736084
        },
        {
          "chunk_id": 3,
          "file_path": "mm/slab_common.c",
          "start_line": 655,
          "end_line": 793,
          "content": [
            "void __init create_boot_cache(struct kmem_cache *s, const char *name,",
            "\t\tunsigned int size, slab_flags_t flags,",
            "\t\tunsigned int useroffset, unsigned int usersize)",
            "{",
            "\tint err;",
            "\tunsigned int align = ARCH_KMALLOC_MINALIGN;",
            "\tstruct kmem_cache_args kmem_args = {};",
            "",
            "\t/*",
            "\t * For power of two sizes, guarantee natural alignment for kmalloc",
            "\t * caches, regardless of SL*B debugging options.",
            "\t */",
            "\tif (is_power_of_2(size))",
            "\t\talign = max(align, size);",
            "\tkmem_args.align = calculate_alignment(flags, align, size);",
            "",
            "#ifdef CONFIG_HARDENED_USERCOPY",
            "\tkmem_args.useroffset = useroffset;",
            "\tkmem_args.usersize = usersize;",
            "#endif",
            "",
            "\terr = do_kmem_cache_create(s, name, size, &kmem_args, flags);",
            "",
            "\tif (err)",
            "\t\tpanic(\"Creation of kmalloc slab %s size=%u failed. Reason %d\\n\",",
            "\t\t\t\t\tname, size, err);",
            "",
            "\ts->refcount = -1;\t/* Exempt from merging for now */",
            "}",
            "size_t kmalloc_size_roundup(size_t size)",
            "{",
            "\tif (size && size <= KMALLOC_MAX_CACHE_SIZE) {",
            "\t\t/*",
            "\t\t * The flags don't matter since size_index is common to all.",
            "\t\t * Neither does the caller for just getting ->object_size.",
            "\t\t */",
            "\t\treturn kmalloc_slab(size, NULL, GFP_KERNEL, 0)->object_size;",
            "\t}",
            "",
            "\t/* Above the smaller buckets, size is a multiple of page size. */",
            "\tif (size && size <= KMALLOC_MAX_SIZE)",
            "\t\treturn PAGE_SIZE << get_order(size);",
            "",
            "\t/*",
            "\t * Return 'size' for 0 - kmalloc() returns ZERO_SIZE_PTR",
            "\t * and very large size - kmalloc() may fail.",
            "\t */",
            "\treturn size;",
            "",
            "}",
            "void __init setup_kmalloc_cache_index_table(void)",
            "{",
            "\tunsigned int i;",
            "",
            "\tBUILD_BUG_ON(KMALLOC_MIN_SIZE > 256 ||",
            "\t\t!is_power_of_2(KMALLOC_MIN_SIZE));",
            "",
            "\tfor (i = 8; i < KMALLOC_MIN_SIZE; i += 8) {",
            "\t\tunsigned int elem = size_index_elem(i);",
            "",
            "\t\tif (elem >= ARRAY_SIZE(kmalloc_size_index))",
            "\t\t\tbreak;",
            "\t\tkmalloc_size_index[elem] = KMALLOC_SHIFT_LOW;",
            "\t}",
            "",
            "\tif (KMALLOC_MIN_SIZE >= 64) {",
            "\t\t/*",
            "\t\t * The 96 byte sized cache is not used if the alignment",
            "\t\t * is 64 byte.",
            "\t\t */",
            "\t\tfor (i = 64 + 8; i <= 96; i += 8)",
            "\t\t\tkmalloc_size_index[size_index_elem(i)] = 7;",
            "",
            "\t}",
            "",
            "\tif (KMALLOC_MIN_SIZE >= 128) {",
            "\t\t/*",
            "\t\t * The 192 byte sized cache is not used if the alignment",
            "\t\t * is 128 byte. Redirect kmalloc to use the 256 byte cache",
            "\t\t * instead.",
            "\t\t */",
            "\t\tfor (i = 128 + 8; i <= 192; i += 8)",
            "\t\t\tkmalloc_size_index[size_index_elem(i)] = 8;",
            "\t}",
            "}",
            "static unsigned int __kmalloc_minalign(void)",
            "{",
            "\tunsigned int minalign = dma_get_cache_alignment();",
            "",
            "\tif (IS_ENABLED(CONFIG_DMA_BOUNCE_UNALIGNED_KMALLOC) &&",
            "\t    is_swiotlb_allocated())",
            "\t\tminalign = ARCH_KMALLOC_MINALIGN;",
            "",
            "\treturn max(minalign, arch_slab_minalign());",
            "}",
            "static void __init",
            "new_kmalloc_cache(int idx, enum kmalloc_cache_type type)",
            "{",
            "\tslab_flags_t flags = 0;",
            "\tunsigned int minalign = __kmalloc_minalign();",
            "\tunsigned int aligned_size = kmalloc_info[idx].size;",
            "\tint aligned_idx = idx;",
            "",
            "\tif ((KMALLOC_RECLAIM != KMALLOC_NORMAL) && (type == KMALLOC_RECLAIM)) {",
            "\t\tflags |= SLAB_RECLAIM_ACCOUNT;",
            "\t} else if (IS_ENABLED(CONFIG_MEMCG) && (type == KMALLOC_CGROUP)) {",
            "\t\tif (mem_cgroup_kmem_disabled()) {",
            "\t\t\tkmalloc_caches[type][idx] = kmalloc_caches[KMALLOC_NORMAL][idx];",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tflags |= SLAB_ACCOUNT;",
            "\t} else if (IS_ENABLED(CONFIG_ZONE_DMA) && (type == KMALLOC_DMA)) {",
            "\t\tflags |= SLAB_CACHE_DMA;",
            "\t}",
            "",
            "#ifdef CONFIG_RANDOM_KMALLOC_CACHES",
            "\tif (type >= KMALLOC_RANDOM_START && type <= KMALLOC_RANDOM_END)",
            "\t\tflags |= SLAB_NO_MERGE;",
            "#endif",
            "",
            "\t/*",
            "\t * If CONFIG_MEMCG is enabled, disable cache merging for",
            "\t * KMALLOC_NORMAL caches.",
            "\t */",
            "\tif (IS_ENABLED(CONFIG_MEMCG) && (type == KMALLOC_NORMAL))",
            "\t\tflags |= SLAB_NO_MERGE;",
            "",
            "\tif (minalign > ARCH_KMALLOC_MINALIGN) {",
            "\t\taligned_size = ALIGN(aligned_size, minalign);",
            "\t\taligned_idx = __kmalloc_index(aligned_size, false);",
            "\t}",
            "",
            "\tif (!kmalloc_caches[type][aligned_idx])",
            "\t\tkmalloc_caches[type][aligned_idx] = create_kmalloc_cache(",
            "\t\t\t\t\tkmalloc_info[aligned_idx].name[type],",
            "\t\t\t\t\taligned_size, flags);",
            "\tif (idx != aligned_idx)",
            "\t\tkmalloc_caches[type][idx] = kmalloc_caches[type][aligned_idx];",
            "}"
          ],
          "function_name": "create_boot_cache, kmalloc_size_roundup, setup_kmalloc_cache_index_table, __kmalloc_minalign, new_kmalloc_cache",
          "description": "初始化kmalloc专用Slab缓存，create_boot_cache创建启动时基础缓存，setup_kmalloc_cache_index_table构建大小到缓存索引的映射表，new_kmalloc_cache根据类型创建不同属性的缓存实例。",
          "similarity": 0.45058882236480713
        },
        {
          "chunk_id": 5,
          "file_path": "mm/slab_common.c",
          "start_line": 1080,
          "end_line": 1198,
          "content": [
            "static void print_slabinfo_header(struct seq_file *m)",
            "{",
            "\t/*",
            "\t * Output format version, so at least we can change it",
            "\t * without _too_ many complaints.",
            "\t */",
            "\tseq_puts(m, \"slabinfo - version: 2.1\\n\");",
            "\tseq_puts(m, \"# name            <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab>\");",
            "\tseq_puts(m, \" : tunables <limit> <batchcount> <sharedfactor>\");",
            "\tseq_puts(m, \" : slabdata <active_slabs> <num_slabs> <sharedavail>\");",
            "\tseq_putc(m, '\\n');",
            "}",
            "static void slab_stop(struct seq_file *m, void *p)",
            "{",
            "\tmutex_unlock(&slab_mutex);",
            "}",
            "static void cache_show(struct kmem_cache *s, struct seq_file *m)",
            "{",
            "\tstruct slabinfo sinfo;",
            "",
            "\tmemset(&sinfo, 0, sizeof(sinfo));",
            "\tget_slabinfo(s, &sinfo);",
            "",
            "\tseq_printf(m, \"%-17s %6lu %6lu %6u %4u %4d\",",
            "\t\t   s->name, sinfo.active_objs, sinfo.num_objs, s->size,",
            "\t\t   sinfo.objects_per_slab, (1 << sinfo.cache_order));",
            "",
            "\tseq_printf(m, \" : tunables %4u %4u %4u\",",
            "\t\t   sinfo.limit, sinfo.batchcount, sinfo.shared);",
            "\tseq_printf(m, \" : slabdata %6lu %6lu %6lu\",",
            "\t\t   sinfo.active_slabs, sinfo.num_slabs, sinfo.shared_avail);",
            "\tslabinfo_show_stats(m, s);",
            "\tseq_putc(m, '\\n');",
            "}",
            "static int slab_show(struct seq_file *m, void *p)",
            "{",
            "\tstruct kmem_cache *s = list_entry(p, struct kmem_cache, list);",
            "",
            "\tif (p == slab_caches.next)",
            "\t\tprint_slabinfo_header(m);",
            "\tcache_show(s, m);",
            "\treturn 0;",
            "}",
            "void dump_unreclaimable_slab(void)",
            "{",
            "\tstruct kmem_cache *s;",
            "\tstruct slabinfo sinfo;",
            "",
            "\t/*",
            "\t * Here acquiring slab_mutex is risky since we don't prefer to get",
            "\t * sleep in oom path. But, without mutex hold, it may introduce a",
            "\t * risk of crash.",
            "\t * Use mutex_trylock to protect the list traverse, dump nothing",
            "\t * without acquiring the mutex.",
            "\t */",
            "\tif (!mutex_trylock(&slab_mutex)) {",
            "\t\tpr_warn(\"excessive unreclaimable slab but cannot dump stats\\n\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tpr_info(\"Unreclaimable slab info:\\n\");",
            "\tpr_info(\"Name                      Used          Total\\n\");",
            "",
            "\tlist_for_each_entry(s, &slab_caches, list) {",
            "\t\tif (s->flags & SLAB_RECLAIM_ACCOUNT)",
            "\t\t\tcontinue;",
            "",
            "\t\tget_slabinfo(s, &sinfo);",
            "",
            "\t\tif (sinfo.num_objs > 0)",
            "\t\t\tpr_info(\"%-17s %10luKB %10luKB\\n\", s->name,",
            "\t\t\t\t(sinfo.active_objs * s->size) / 1024,",
            "\t\t\t\t(sinfo.num_objs * s->size) / 1024);",
            "\t}",
            "\tmutex_unlock(&slab_mutex);",
            "}",
            "static int slabinfo_open(struct inode *inode, struct file *file)",
            "{",
            "\treturn seq_open(file, &slabinfo_op);",
            "}",
            "static int __init slab_proc_init(void)",
            "{",
            "\tproc_create(\"slabinfo\", SLABINFO_RIGHTS, NULL, &slabinfo_proc_ops);",
            "\treturn 0;",
            "}",
            "void kfree_sensitive(const void *p)",
            "{",
            "\tsize_t ks;",
            "\tvoid *mem = (void *)p;",
            "",
            "\tks = ksize(mem);",
            "\tif (ks) {",
            "\t\tkasan_unpoison_range(mem, ks);",
            "\t\tmemzero_explicit(mem, ks);",
            "\t}",
            "\tkfree(mem);",
            "}",
            "size_t ksize(const void *objp)",
            "{",
            "\t/*",
            "\t * We need to first check that the pointer to the object is valid.",
            "\t * The KASAN report printed from ksize() is more useful, then when",
            "\t * it's printed later when the behaviour could be undefined due to",
            "\t * a potential use-after-free or double-free.",
            "\t *",
            "\t * We use kasan_check_byte(), which is supported for the hardware",
            "\t * tag-based KASAN mode, unlike kasan_check_read/write().",
            "\t *",
            "\t * If the pointed to memory is invalid, we return 0 to avoid users of",
            "\t * ksize() writing to and potentially corrupting the memory region.",
            "\t *",
            "\t * We want to perform the check before __ksize(), to avoid potentially",
            "\t * crashing in __ksize() due to accessing invalid metadata.",
            "\t */",
            "\tif (unlikely(ZERO_OR_NULL_PTR(objp)) || !kasan_check_byte(objp))",
            "\t\treturn 0;",
            "",
            "\treturn kfence_ksize(objp) ?: __ksize(objp);",
            "}"
          ],
          "function_name": "print_slabinfo_header, slab_stop, cache_show, slab_show, dump_unreclaimable_slab, slabinfo_open, slab_proc_init, kfree_sensitive, ksize",
          "description": "该代码段实现Linux内核slab分配器的信息导出与调试功能，包含slab状态统计、/proc接口注册及内存安全检查模块。其中`print_slabinfo_header`定义输出格式，`cache_show`和`slab_show`负责遍历并打印各slab缓存的详细统计信息，`dump_unreclaimable_slab`用于安全地导出不可回收slab的数据。`kfree_sensitive`和`ksize`协同实现对已释放内存的跟踪与安全清除机制。",
          "similarity": 0.4275834262371063
        }
      ]
    }
  ]
}