{
  "query": "I/O调度器",
  "timestamp": "2025-12-26 00:28:53",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/cpufreq_schedutil.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:03:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\cpufreq_schedutil.c`\n\n---\n\n# `sched/cpufreq_schedutil.c` 技术文档\n\n## 1. 文件概述\n\n`sched/cpufreq_schedutil.c` 实现了 Linux 内核中基于调度器提供的 CPU 利用率数据的 **schedutil CPUFreq 调速器（governor）**。该调速器通过实时获取调度器计算的 CPU 利用率（包括 CFS、RT、DL 任务以及 I/O 等待状态），动态调整 CPU 频率，以在性能与能效之间取得平衡。其核心优势在于直接利用调度器的 `util` 信息，避免传统调速器依赖采样机制带来的延迟和不准确性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct sugov_tunables`**  \n  调速器可调参数，包含：\n  - `rate_limit_us`：频率更新的最小时间间隔（微秒），防止过于频繁的频率切换。\n\n- **`struct sugov_policy`**  \n  每个 `cpufreq_policy` 对应的 schedutil 策略实例，包含：\n  - `policy`：关联的 CPUFreq 策略。\n  - `update_lock`：保护频率更新的自旋锁。\n  - `last_freq_update_time` / `freq_update_delay_ns`：控制频率更新速率。\n  - `next_freq` / `cached_raw_freq`：目标频率与原始计算频率缓存。\n  - `irq_work` / `worker` / `thread`：用于慢速切换平台（非 fast-switch）的异步工作队列机制。\n  - `limits_changed` / `need_freq_update`：标志策略限制（如 min/max freq）是否变更。\n\n- **`struct sugov_cpu`**  \n  每个 CPU 的 schedutil 状态，包含：\n  - `update_util`：注册到调度器的回调接口（`update_util_data`）。\n  - `util` / `bw_min`：当前有效利用率及带宽最小值。\n  - `iowait_boost` / `iowait_boost_pending`：I/O 等待唤醒时的频率提升机制。\n  - `last_update`：上次更新时间戳。\n\n### 主要函数\n\n- **`sugov_should_update_freq()`**  \n  判断是否应执行频率更新，考虑硬件是否支持本 CPU 更新、策略限制变更、以及频率更新间隔限制。\n\n- **`sugov_update_next_freq()`**  \n  更新目标频率，处理策略限制变更场景，避免不必要的驱动回调。\n\n- **`get_next_freq()`**  \n  核心频率计算函数，根据 CPU 利用率、最大容量和参考频率，计算目标频率，并通过 `cpufreq_driver_resolve_freq()` 映射到驱动支持的频率。\n\n- **`sugov_get_util()`**  \n  获取当前 CPU 的综合利用率，整合 CFS/RT/DL 任务利用率、boost 值，并调用 `sugov_effective_cpu_perf()` 计算有效性能目标。\n\n- **`sugov_effective_cpu_perf()`**  \n  计算最终的有效性能目标，确保不低于最小性能要求，并限制不超过实际需求。\n\n- **`sugov_iowait_reset()` / `sugov_iowait_boost()`**  \n  实现 I/O 等待唤醒时的动态频率提升机制：短时间内连续 I/O 唤醒会逐步提升 boost 值（从 `IOWAIT_BOOST_MIN` 到最大 OPP），超过一个 tick 无 I/O 唤醒则重置。\n\n- **`get_capacity_ref_freq()`**  \n  获取用于计算 CPU 容量的参考频率，优先使用架构特定的 `arch_scale_freq_ref()`，其次为最大频率或当前频率。\n\n- **`sugov_deferred_update()`**  \n  在不支持 fast-switch 的平台上，通过 `irq_work` 触发异步频率更新。\n\n## 3. 关键实现\n\n### 频率计算算法\n- **频率不变性支持**：若系统支持频率不变调度（`arch_scale_freq_invariant()`），则直接使用调度器提供的频率不变利用率 `util`，按比例计算目标频率：  \n  `next_freq = C * max_freq * util / max`  \n  其中常数 `C = 1.25`，使在 `util/max = 0.8` 时达到 `max_freq`，提供性能余量。\n- **非频率不变性**：使用原始利用率 `util_raw` 乘以 `(curr_freq / max_freq)` 近似频率不变利用率，再计算目标频率。\n\n### I/O 等待 Boost 机制\n- 当任务因 I/O 完成而唤醒时，标记 `SCHED_CPUFREQ_IOWAIT`。\n- 若在 **一个 tick 内** 多次发生 I/O 唤醒，则 `iowait_boost` 值倍增（上限为最大 OPP 对应的利用率）。\n- 若超过一个 tick 无 I/O 唤醒，则重置 boost 值为 `IOWAIT_BOOST_MIN`（`SCHED_CAPACITY_SCALE / 8`），避免对偶发 I/O 过度响应，提升能效。\n\n### 快速切换（Fast-Switch）与异步更新\n- **Fast-Switch 平台**：支持在调度上下文中直接调用 `cpufreq_driver_fast_switch()` 更新频率，延迟最低。\n- **非 Fast-Switch 平台**：通过 `irq_work` 触发内核线程（`kthread_worker`）异步执行频率更新，避免在中断上下文或持有 rq 锁时调用可能阻塞的驱动接口。\n\n### 策略限制变更处理\n- 当用户空间修改 policy 的 min/max 频率时，`sugov_limits()` 设置 `limits_changed` 标志。\n- 下次更新时，强制重新计算频率，并通过内存屏障（`smp_mb()`）确保读取到最新的策略限制。\n\n## 4. 依赖关系\n\n- **调度器子系统**：\n  - 依赖 `update_util_data` 回调机制（通过 `cpufreq_add_update_util_hook()` 注册）。\n  - 调用 `cpu_util_cfs_boost()`、`effective_cpu_util()` 等函数获取综合利用率。\n  - 使用 `scx_cpuperf_target()`（若启用了 SCHED_CLASS_EXT）。\n- **CPUFreq 核心**：\n  - 依赖 `cpufreq_policy`、`cpufreq_driver_resolve_freq()`、`cpufreq_driver_fast_switch()` 等接口。\n  - 使用 `cpufreq_this_cpu_can_update()` 判断硬件更新能力。\n- **架构相关支持**：\n  - 依赖 `arch_scale_freq_ref()` 和 `arch_scale_freq_invariant()` 提供频率不变性信息。\n- **内核基础设施**：\n  - 使用 `irq_work`、`kthread_worker` 实现异步更新。\n  - 依赖 `TICK_NSEC` 定义 tick 时间。\n\n## 5. 使用场景\n\n- **默认高性能能效平衡场景**：现代 Linux 发行版通常将 `schedutil` 作为默认 CPUFreq 调速器，适用于大多数桌面、服务器和移动设备。\n- **实时性要求较高的系统**：由于其低延迟特性（尤其在 fast-switch 平台上），适合对响应时间敏感的应用。\n- **能效敏感设备**：通过 I/O boost 机制和精确的利用率跟踪，在保证交互性能的同时降低空闲功耗。\n- **异构多核系统（如 big.LITTLE）**：结合调度器的 CPU capacity 信息，为不同性能核提供差异化频率调整。",
      "similarity": 0.5489303469657898,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 508,
          "end_line": 651,
          "content": [
            "static void",
            "sugov_update_shared(struct update_util_data *hook, u64 time, unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tunsigned int next_f;",
            "",
            "\traw_spin_lock(&sg_policy->update_lock);",
            "",
            "\tsugov_iowait_boost(sg_cpu, time, flags);",
            "\tsg_cpu->last_update = time;",
            "",
            "\tignore_dl_rate_limit(sg_cpu);",
            "",
            "\tif (sugov_should_update_freq(sg_policy, time)) {",
            "\t\tnext_f = sugov_next_freq_shared(sg_cpu, time);",
            "",
            "\t\tif (!sugov_update_next_freq(sg_policy, time, next_f))",
            "\t\t\tgoto unlock;",
            "",
            "\t\tif (sg_policy->policy->fast_switch_enabled)",
            "\t\t\tcpufreq_driver_fast_switch(sg_policy->policy, next_f);",
            "\t\telse",
            "\t\t\tsugov_deferred_update(sg_policy);",
            "\t}",
            "unlock:",
            "\traw_spin_unlock(&sg_policy->update_lock);",
            "}",
            "static void sugov_work(struct kthread_work *work)",
            "{",
            "\tstruct sugov_policy *sg_policy = container_of(work, struct sugov_policy, work);",
            "\tunsigned int freq;",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * Hold sg_policy->update_lock shortly to handle the case where:",
            "\t * in case sg_policy->next_freq is read here, and then updated by",
            "\t * sugov_deferred_update() just before work_in_progress is set to false",
            "\t * here, we may miss queueing the new update.",
            "\t *",
            "\t * Note: If a work was queued after the update_lock is released,",
            "\t * sugov_work() will just be called again by kthread_work code; and the",
            "\t * request will be proceed before the sugov thread sleeps.",
            "\t */",
            "\traw_spin_lock_irqsave(&sg_policy->update_lock, flags);",
            "\tfreq = sg_policy->next_freq;",
            "\tsg_policy->work_in_progress = false;",
            "\traw_spin_unlock_irqrestore(&sg_policy->update_lock, flags);",
            "",
            "\tmutex_lock(&sg_policy->work_lock);",
            "\t__cpufreq_driver_target(sg_policy->policy, freq, CPUFREQ_RELATION_L);",
            "\tmutex_unlock(&sg_policy->work_lock);",
            "}",
            "static void sugov_irq_work(struct irq_work *irq_work)",
            "{",
            "\tstruct sugov_policy *sg_policy;",
            "",
            "\tsg_policy = container_of(irq_work, struct sugov_policy, irq_work);",
            "",
            "\tkthread_queue_work(&sg_policy->worker, &sg_policy->work);",
            "}",
            "static ssize_t rate_limit_us_show(struct gov_attr_set *attr_set, char *buf)",
            "{",
            "\tstruct sugov_tunables *tunables = to_sugov_tunables(attr_set);",
            "",
            "\treturn sprintf(buf, \"%u\\n\", tunables->rate_limit_us);",
            "}",
            "static ssize_t",
            "rate_limit_us_store(struct gov_attr_set *attr_set, const char *buf, size_t count)",
            "{",
            "\tstruct sugov_tunables *tunables = to_sugov_tunables(attr_set);",
            "\tstruct sugov_policy *sg_policy;",
            "\tunsigned int rate_limit_us;",
            "",
            "\tif (kstrtouint(buf, 10, &rate_limit_us))",
            "\t\treturn -EINVAL;",
            "",
            "\ttunables->rate_limit_us = rate_limit_us;",
            "",
            "\tlist_for_each_entry(sg_policy, &attr_set->policy_list, tunables_hook)",
            "\t\tsg_policy->freq_update_delay_ns = rate_limit_us * NSEC_PER_USEC;",
            "",
            "\treturn count;",
            "}",
            "static void sugov_tunables_free(struct kobject *kobj)",
            "{",
            "\tstruct gov_attr_set *attr_set = to_gov_attr_set(kobj);",
            "",
            "\tkfree(to_sugov_tunables(attr_set));",
            "}",
            "static void sugov_policy_free(struct sugov_policy *sg_policy)",
            "{",
            "\tkfree(sg_policy);",
            "}",
            "static int sugov_kthread_create(struct sugov_policy *sg_policy)",
            "{",
            "\tstruct task_struct *thread;",
            "\tstruct sched_attr attr = {",
            "\t\t.size\t\t= sizeof(struct sched_attr),",
            "\t\t.sched_policy\t= SCHED_DEADLINE,",
            "\t\t.sched_flags\t= SCHED_FLAG_SUGOV,",
            "\t\t.sched_nice\t= 0,",
            "\t\t.sched_priority\t= 0,",
            "\t\t/*",
            "\t\t * Fake (unused) bandwidth; workaround to \"fix\"",
            "\t\t * priority inheritance.",
            "\t\t */",
            "\t\t.sched_runtime\t=  1000000,",
            "\t\t.sched_deadline = 10000000,",
            "\t\t.sched_period\t= 10000000,",
            "\t};",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tint ret;",
            "",
            "\t/* kthread only required for slow path */",
            "\tif (policy->fast_switch_enabled)",
            "\t\treturn 0;",
            "",
            "\tkthread_init_work(&sg_policy->work, sugov_work);",
            "\tkthread_init_worker(&sg_policy->worker);",
            "\tthread = kthread_create(kthread_worker_fn, &sg_policy->worker,",
            "\t\t\t\t\"sugov:%d\",",
            "\t\t\t\tcpumask_first(policy->related_cpus));",
            "\tif (IS_ERR(thread)) {",
            "\t\tpr_err(\"failed to create sugov thread: %ld\\n\", PTR_ERR(thread));",
            "\t\treturn PTR_ERR(thread);",
            "\t}",
            "",
            "\tret = sched_setattr_nocheck(thread, &attr);",
            "\tif (ret) {",
            "\t\tkthread_stop(thread);",
            "\t\tpr_warn(\"%s: failed to set SCHED_DEADLINE\\n\", __func__);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tsg_policy->thread = thread;",
            "\tkthread_bind_mask(thread, policy->related_cpus);",
            "\tinit_irq_work(&sg_policy->irq_work, sugov_irq_work);",
            "\tmutex_init(&sg_policy->work_lock);",
            "",
            "\twake_up_process(thread);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sugov_update_shared, sugov_work, sugov_irq_work, rate_limit_us_show, rate_limit_us_store, sugov_tunables_free, sugov_policy_free, sugov_kthread_create",
          "description": "管理频率调节的工作线程和参数配置，sugov_kthread_create创建慢速切换场景的后台线程，rate_limit_us_*/提供速率限制配置接口，sugov_work/sugov_irq_work处理异步频率更新任务。",
          "similarity": 0.5235909819602966
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 381,
          "end_line": 496,
          "content": [
            "static inline bool sugov_hold_freq(struct sugov_cpu *sg_cpu) { return false; }",
            "static inline void ignore_dl_rate_limit(struct sugov_cpu *sg_cpu)",
            "{",
            "\tif (cpu_bw_dl(cpu_rq(sg_cpu->cpu)) > sg_cpu->bw_min)",
            "\t\tWRITE_ONCE(sg_cpu->sg_policy->limits_changed, true);",
            "}",
            "static inline bool sugov_update_single_common(struct sugov_cpu *sg_cpu,",
            "\t\t\t\t\t      u64 time, unsigned long max_cap,",
            "\t\t\t\t\t      unsigned int flags)",
            "{",
            "\tunsigned long boost;",
            "",
            "\tsugov_iowait_boost(sg_cpu, time, flags);",
            "\tsg_cpu->last_update = time;",
            "",
            "\tignore_dl_rate_limit(sg_cpu);",
            "",
            "\tif (!sugov_should_update_freq(sg_cpu->sg_policy, time))",
            "\t\treturn false;",
            "",
            "\tboost = sugov_iowait_apply(sg_cpu, time, max_cap);",
            "\tsugov_get_util(sg_cpu, boost);",
            "",
            "\treturn true;",
            "}",
            "static void sugov_update_single_freq(struct update_util_data *hook, u64 time,",
            "\t\t\t\t     unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tunsigned int cached_freq = sg_policy->cached_raw_freq;",
            "\tunsigned long max_cap;",
            "\tunsigned int next_f;",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tif (!sugov_update_single_common(sg_cpu, time, max_cap, flags))",
            "\t\treturn;",
            "",
            "\tnext_f = get_next_freq(sg_policy, sg_cpu->util, max_cap);",
            "",
            "\tif (sugov_hold_freq(sg_cpu) && next_f < sg_policy->next_freq &&",
            "\t    !sg_policy->need_freq_update) {",
            "\t\tnext_f = sg_policy->next_freq;",
            "",
            "\t\t/* Restore cached freq as next_freq has changed */",
            "\t\tsg_policy->cached_raw_freq = cached_freq;",
            "\t}",
            "",
            "\tif (!sugov_update_next_freq(sg_policy, time, next_f))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This code runs under rq->lock for the target CPU, so it won't run",
            "\t * concurrently on two different CPUs for the same target and it is not",
            "\t * necessary to acquire the lock in the fast switch case.",
            "\t */",
            "\tif (sg_policy->policy->fast_switch_enabled) {",
            "\t\tcpufreq_driver_fast_switch(sg_policy->policy, next_f);",
            "\t} else {",
            "\t\traw_spin_lock(&sg_policy->update_lock);",
            "\t\tsugov_deferred_update(sg_policy);",
            "\t\traw_spin_unlock(&sg_policy->update_lock);",
            "\t}",
            "}",
            "static void sugov_update_single_perf(struct update_util_data *hook, u64 time,",
            "\t\t\t\t     unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tunsigned long prev_util = sg_cpu->util;",
            "\tunsigned long max_cap;",
            "",
            "\t/*",
            "\t * Fall back to the \"frequency\" path if frequency invariance is not",
            "\t * supported, because the direct mapping between the utilization and",
            "\t * the performance levels depends on the frequency invariance.",
            "\t */",
            "\tif (!arch_scale_freq_invariant()) {",
            "\t\tsugov_update_single_freq(hook, time, flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tif (!sugov_update_single_common(sg_cpu, time, max_cap, flags))",
            "\t\treturn;",
            "",
            "\tif (sugov_hold_freq(sg_cpu) && sg_cpu->util < prev_util)",
            "\t\tsg_cpu->util = prev_util;",
            "",
            "\tcpufreq_driver_adjust_perf(sg_cpu->cpu, sg_cpu->bw_min,",
            "\t\t\t\t   sg_cpu->util, max_cap);",
            "",
            "\tsg_cpu->sg_policy->last_freq_update_time = time;",
            "}",
            "static unsigned int sugov_next_freq_shared(struct sugov_cpu *sg_cpu, u64 time)",
            "{",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tunsigned long util = 0, max_cap;",
            "\tunsigned int j;",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tfor_each_cpu(j, policy->cpus) {",
            "\t\tstruct sugov_cpu *j_sg_cpu = &per_cpu(sugov_cpu, j);",
            "\t\tunsigned long boost;",
            "",
            "\t\tboost = sugov_iowait_apply(j_sg_cpu, time, max_cap);",
            "\t\tsugov_get_util(j_sg_cpu, boost);",
            "",
            "\t\tutil = max(j_sg_cpu->util, util);",
            "\t}",
            "",
            "\treturn get_next_freq(sg_policy, util, max_cap);",
            "}"
          ],
          "function_name": "sugov_hold_freq, ignore_dl_rate_limit, sugov_update_single_common, sugov_update_single_freq, sugov_update_single_perf, sugov_next_freq_shared",
          "description": "实现单核/多核频率调整逻辑，sugov_update_single_freq处理单核频率更新，sugov_update_single_perf处理性能调优路径，sugov_next_freq_shared计算多核共享场景下的全局目标频率。",
          "similarity": 0.49270665645599365
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 1,
          "end_line": 61,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * CPUFreq governor based on scheduler-provided CPU utilization data.",
            " *",
            " * Copyright (C) 2016, Intel Corporation",
            " * Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>",
            " */",
            "",
            "#define IOWAIT_BOOST_MIN\t(SCHED_CAPACITY_SCALE / 8)",
            "",
            "struct sugov_tunables {",
            "\tstruct gov_attr_set\tattr_set;",
            "\tunsigned int\t\trate_limit_us;",
            "};",
            "",
            "struct sugov_policy {",
            "\tstruct cpufreq_policy\t*policy;",
            "",
            "\tstruct sugov_tunables\t*tunables;",
            "\tstruct list_head\ttunables_hook;",
            "",
            "\traw_spinlock_t\t\tupdate_lock;",
            "\tu64\t\t\tlast_freq_update_time;",
            "\ts64\t\t\tfreq_update_delay_ns;",
            "\tunsigned int\t\tnext_freq;",
            "\tunsigned int\t\tcached_raw_freq;",
            "",
            "\t/* The next fields are only needed if fast switch cannot be used: */",
            "\tstruct\t\t\tirq_work irq_work;",
            "\tstruct\t\t\tkthread_work work;",
            "\tstruct\t\t\tmutex work_lock;",
            "\tstruct\t\t\tkthread_worker worker;",
            "\tstruct task_struct\t*thread;",
            "\tbool\t\t\twork_in_progress;",
            "",
            "\tbool\t\t\tlimits_changed;",
            "\tbool\t\t\tneed_freq_update;",
            "};",
            "",
            "struct sugov_cpu {",
            "\tstruct update_util_data\tupdate_util;",
            "\tstruct sugov_policy\t*sg_policy;",
            "\tunsigned int\t\tcpu;",
            "",
            "\tbool\t\t\tiowait_boost_pending;",
            "\tunsigned int\t\tiowait_boost;",
            "\tu64\t\t\tlast_update;",
            "",
            "\tunsigned long\t\tutil;",
            "\tunsigned long\t\tbw_min;",
            "",
            "\t/* The field below is for single-CPU policies only: */",
            "#ifdef CONFIG_NO_HZ_COMMON",
            "\tunsigned long\t\tsaved_idle_calls;",
            "#endif",
            "};",
            "",
            "static DEFINE_PER_CPU(struct sugov_cpu, sugov_cpu);",
            "",
            "/************************ Governor internals ***********************/",
            ""
          ],
          "function_name": null,
          "description": "定义了CPU频率调节器所需的结构体和宏，包括sugov_tunables保存策略参数，sugov_policy管理策略状态，sugov_cpu保存每个CPU的运行数据，以及相关锁和标志位，用于协调多核间的频率调整操作。",
          "similarity": 0.48694050312042236
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 827,
          "end_line": 916,
          "content": [
            "static int sugov_start(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "\tvoid (*uu)(struct update_util_data *data, u64 time, unsigned int flags);",
            "\tunsigned int cpu;",
            "",
            "\tsg_policy->freq_update_delay_ns\t= sg_policy->tunables->rate_limit_us * NSEC_PER_USEC;",
            "\tsg_policy->last_freq_update_time\t= 0;",
            "\tsg_policy->next_freq\t\t\t= 0;",
            "\tsg_policy->work_in_progress\t\t= false;",
            "\tsg_policy->limits_changed\t\t= false;",
            "\tsg_policy->cached_raw_freq\t\t= 0;",
            "",
            "\tsg_policy->need_freq_update = cpufreq_driver_test_flags(CPUFREQ_NEED_UPDATE_LIMITS);",
            "",
            "\tfor_each_cpu(cpu, policy->cpus) {",
            "\t\tstruct sugov_cpu *sg_cpu = &per_cpu(sugov_cpu, cpu);",
            "",
            "\t\tmemset(sg_cpu, 0, sizeof(*sg_cpu));",
            "\t\tsg_cpu->cpu\t\t\t= cpu;",
            "\t\tsg_cpu->sg_policy\t\t= sg_policy;",
            "\t}",
            "",
            "\tif (policy_is_shared(policy))",
            "\t\tuu = sugov_update_shared;",
            "\telse if (policy->fast_switch_enabled && cpufreq_driver_has_adjust_perf())",
            "\t\tuu = sugov_update_single_perf;",
            "\telse",
            "\t\tuu = sugov_update_single_freq;",
            "",
            "\tfor_each_cpu(cpu, policy->cpus) {",
            "\t\tstruct sugov_cpu *sg_cpu = &per_cpu(sugov_cpu, cpu);",
            "",
            "\t\tcpufreq_add_update_util_hook(cpu, &sg_cpu->update_util, uu);",
            "\t}",
            "\treturn 0;",
            "}",
            "static void sugov_stop(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "\tunsigned int cpu;",
            "",
            "\tfor_each_cpu(cpu, policy->cpus)",
            "\t\tcpufreq_remove_update_util_hook(cpu);",
            "",
            "\tsynchronize_rcu();",
            "",
            "\tif (!policy->fast_switch_enabled) {",
            "\t\tirq_work_sync(&sg_policy->irq_work);",
            "\t\tkthread_cancel_work_sync(&sg_policy->work);",
            "\t}",
            "}",
            "static void sugov_limits(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "",
            "\tif (!policy->fast_switch_enabled) {",
            "\t\tmutex_lock(&sg_policy->work_lock);",
            "\t\tcpufreq_policy_apply_limits(policy);",
            "\t\tmutex_unlock(&sg_policy->work_lock);",
            "\t}",
            "",
            "\t/*",
            "\t * The limits_changed update below must take place before the updates",
            "\t * of policy limits in cpufreq_set_policy() or a policy limits update",
            "\t * might be missed, so use a memory barrier to ensure it.",
            "\t *",
            "\t * This pairs with the memory barrier in sugov_should_update_freq().",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\tWRITE_ONCE(sg_policy->limits_changed, true);",
            "}",
            "static void rebuild_sd_workfn(struct work_struct *work)",
            "{",
            "\trebuild_sched_domains_energy();",
            "}",
            "void sched_cpufreq_governor_change(struct cpufreq_policy *policy,",
            "\t\t\t\t  struct cpufreq_governor *old_gov)",
            "{",
            "\tif (old_gov == &schedutil_gov || policy->governor == &schedutil_gov) {",
            "\t\t/*",
            "\t\t * When called from the cpufreq_register_driver() path, the",
            "\t\t * cpu_hotplug_lock is already held, so use a work item to",
            "\t\t * avoid nested locking in rebuild_sched_domains().",
            "\t\t */",
            "\t\tschedule_work(&rebuild_sd_work);",
            "\t}",
            "",
            "}"
          ],
          "function_name": "sugov_start, sugov_stop, sugov_limits, rebuild_sd_workfn, sched_cpufreq_governor_change",
          "description": "sugov_start 注册CPU利用率更新钩子函数并初始化频率更新参数；sugov_stop 移除所有CPU的更新钩子并同步RCU状态；sugov_limits 应用频率限制并标记策略变更；rebuild_sd_workfn 触发调度域能量重新构建；sched_cpufreq_governor_change 在策略切换时安排调度域重建工作",
          "similarity": 0.48515304923057556
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 204,
          "end_line": 330,
          "content": [
            "unsigned long sugov_effective_cpu_perf(int cpu, unsigned long actual,",
            "\t\t\t\t unsigned long min,",
            "\t\t\t\t unsigned long max)",
            "{",
            "\t/* Add dvfs headroom to actual utilization */",
            "\tactual = map_util_perf(actual);",
            "\t/* Actually we don't need to target the max performance */",
            "\tif (actual < max)",
            "\t\tmax = actual;",
            "",
            "\t/*",
            "\t * Ensure at least minimum performance while providing more compute",
            "\t * capacity when possible.",
            "\t */",
            "\treturn max(min, max);",
            "}",
            "static void sugov_get_util(struct sugov_cpu *sg_cpu, unsigned long boost)",
            "{",
            "\tunsigned long min, max, util = scx_cpuperf_target(sg_cpu->cpu);",
            "",
            "\tif (!scx_switched_all())",
            "\t\tutil += cpu_util_cfs_boost(sg_cpu->cpu);",
            "\tutil = effective_cpu_util(sg_cpu->cpu, util, &min, &max);",
            "\tutil = max(util, boost);",
            "\tsg_cpu->bw_min = min;",
            "\tsg_cpu->util = sugov_effective_cpu_perf(sg_cpu->cpu, util, min, max);",
            "}",
            "static bool sugov_iowait_reset(struct sugov_cpu *sg_cpu, u64 time,",
            "\t\t\t       bool set_iowait_boost)",
            "{",
            "\ts64 delta_ns = time - sg_cpu->last_update;",
            "",
            "\t/* Reset boost only if a tick has elapsed since last request */",
            "\tif (delta_ns <= TICK_NSEC)",
            "\t\treturn false;",
            "",
            "\tsg_cpu->iowait_boost = set_iowait_boost ? IOWAIT_BOOST_MIN : 0;",
            "\tsg_cpu->iowait_boost_pending = set_iowait_boost;",
            "",
            "\treturn true;",
            "}",
            "static void sugov_iowait_boost(struct sugov_cpu *sg_cpu, u64 time,",
            "\t\t\t       unsigned int flags)",
            "{",
            "\tbool set_iowait_boost = flags & SCHED_CPUFREQ_IOWAIT;",
            "",
            "\t/* Reset boost if the CPU appears to have been idle enough */",
            "\tif (sg_cpu->iowait_boost &&",
            "\t    sugov_iowait_reset(sg_cpu, time, set_iowait_boost))",
            "\t\treturn;",
            "",
            "\t/* Boost only tasks waking up after IO */",
            "\tif (!set_iowait_boost)",
            "\t\treturn;",
            "",
            "\t/* Ensure boost doubles only one time at each request */",
            "\tif (sg_cpu->iowait_boost_pending)",
            "\t\treturn;",
            "\tsg_cpu->iowait_boost_pending = true;",
            "",
            "\t/* Double the boost at each request */",
            "\tif (sg_cpu->iowait_boost) {",
            "\t\tsg_cpu->iowait_boost =",
            "\t\t\tmin_t(unsigned int, sg_cpu->iowait_boost << 1, SCHED_CAPACITY_SCALE);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* First wakeup after IO: start with minimum boost */",
            "\tsg_cpu->iowait_boost = IOWAIT_BOOST_MIN;",
            "}",
            "static unsigned long sugov_iowait_apply(struct sugov_cpu *sg_cpu, u64 time,",
            "\t\t\t       unsigned long max_cap)",
            "{",
            "\t/* No boost currently required */",
            "\tif (!sg_cpu->iowait_boost)",
            "\t\treturn 0;",
            "",
            "\t/* Reset boost if the CPU appears to have been idle enough */",
            "\tif (sugov_iowait_reset(sg_cpu, time, false))",
            "\t\treturn 0;",
            "",
            "\tif (!sg_cpu->iowait_boost_pending) {",
            "\t\t/*",
            "\t\t * No boost pending; reduce the boost value.",
            "\t\t */",
            "\t\tsg_cpu->iowait_boost >>= 1;",
            "\t\tif (sg_cpu->iowait_boost < IOWAIT_BOOST_MIN) {",
            "\t\t\tsg_cpu->iowait_boost = 0;",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            "\tsg_cpu->iowait_boost_pending = false;",
            "",
            "\t/*",
            "\t * sg_cpu->util is already in capacity scale; convert iowait_boost",
            "\t * into the same scale so we can compare.",
            "\t */",
            "\treturn (sg_cpu->iowait_boost * max_cap) >> SCHED_CAPACITY_SHIFT;",
            "}",
            "static bool sugov_hold_freq(struct sugov_cpu *sg_cpu)",
            "{",
            "\tunsigned long idle_calls;",
            "\tbool ret;",
            "",
            "\t/*",
            "\t * The heuristics in this function is for the fair class. For SCX, the",
            "\t * performance target comes directly from the BPF scheduler. Let's just",
            "\t * follow it.",
            "\t */",
            "\tif (scx_switched_all())",
            "\t\treturn false;",
            "",
            "\t/* if capped by uclamp_max, always update to be in compliance */",
            "\tif (uclamp_rq_is_capped(cpu_rq(sg_cpu->cpu)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Maintain the frequency if the CPU has not been idle recently, as",
            "\t * reduction is likely to be premature.",
            "\t */",
            "\tidle_calls = tick_nohz_get_idle_calls_cpu(sg_cpu->cpu);",
            "\tret = idle_calls == sg_cpu->saved_idle_calls;",
            "",
            "\tsg_cpu->saved_idle_calls = idle_calls;",
            "\treturn ret;",
            "}"
          ],
          "function_name": "sugov_effective_cpu_perf, sugov_get_util, sugov_iowait_reset, sugov_iowait_boost, sugov_iowait_apply, sugov_hold_freq",
          "description": "处理利用率计算和I/O等待优化，sugov_effective_cpu_perf计算有效性能需求，sugov_get_util获取考虑boost后的利用率，sugov_iowait_*系列函数管理I/O等待场景下的频率提升机制。",
          "similarity": 0.48165905475616455
        }
      ]
    },
    {
      "source_file": "kernel/sched/sched.h",
      "md_summary": "> 自动生成时间: 2025-10-25 16:16:13\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\sched.h`\n\n---\n\n# `sched/sched.h` 技术文档\n\n## 1. 文件概述\n\n`sched/sched.h` 是 Linux 内核调度器（Scheduler）的核心内部头文件，定义了调度子系统内部使用的类型、宏、辅助函数和全局变量。该文件不对外暴露给其他子系统直接使用，而是作为调度器各组件（如 CFS、RT、Deadline 调度类）之间的内部接口和共享基础设施。它整合了任务状态管理、负载计算、策略判断、CPU 能力建模、cgroup 权重转换等关键调度逻辑，并为调试、性能追踪和平台适配提供支持。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct asym_cap_data`：用于描述非对称 CPU 架构中不同 CPU 集合的计算能力（capacity），支持异构多核系统（如 big.LITTLE）的调度优化。\n- `struct rq`（前向声明）：运行队列（runqueue）结构体，每个 CPU 对应一个，是调度器管理可运行任务的核心数据结构。\n- `struct cpuidle_state`（前向声明）：CPU 空闲状态信息，用于与调度器协同进行能效管理。\n\n### 关键全局变量\n- `scheduler_running`：标志调度器是否已启动。\n- `calc_load_update` / `calc_load_tasks`：用于全局负载（load average）计算的时间戳和任务计数。\n- `sysctl_sched_rt_period` / `sysctl_sched_rt_runtime`：实时任务带宽控制参数。\n- `sched_rr_timeslice`：SCHED_RR 策略的时间片长度。\n- `asym_cap_list`：非对称 CPU 能力数据的全局链表。\n\n### 核心辅助函数与宏\n- **任务策略判断函数**：\n  - `idle_policy()` / `task_has_idle_policy()`\n  - `normal_policy()` / `fair_policy()`\n  - `rt_policy()` / `task_has_rt_policy()`\n  - `dl_policy()` / `task_has_dl_policy()`\n  - `valid_policy()`\n- **负载与权重转换**：\n  - `scale_load()` / `scale_load_down()`：在内部高精度负载值与用户可见权重间转换。\n  - `sched_weight_from_cgroup()` / `sched_weight_to_cgroup()`：cgroup 权重与调度器内部权重的映射。\n- **时间与精度处理**：\n  - `NS_TO_JIFFIES()`：纳秒转 jiffies。\n  - `update_avg()`：指数移动平均（EMA）更新。\n  - `shr_bound()`：安全右移，避免未定义行为。\n- **特殊调度标志**：\n  - `SCHED_FLAG_SUGOV`：用于 schedutil 频率调节器的特殊标志，使相关 kworker 临时获得高于 SCHED_DEADLINE 的优先级。\n  - `dl_entity_is_special()`：判断 Deadline 实体是否为 SUGOV 特殊任务。\n\n### 重要宏定义\n- `TASK_ON_RQ_QUEUED` / `TASK_ON_RQ_MIGRATING`：`task_struct::on_rq` 字段的状态值。\n- `NICE_0_LOAD`：nice 值为 0 的任务对应的内部负载基准值。\n- `DL_SCALE`：SCHED_DEADLINE 内部计算的精度因子。\n- `RUNTIME_INF`：表示无限运行时间的常量。\n- `SCHED_WARN_ON()`：调度器专用的条件警告宏（仅在 `CONFIG_SCHED_DEBUG` 时生效）。\n\n## 3. 关键实现\n\n### 高精度负载计算（64 位优化）\n在 64 位架构上，通过 `NICE_0_LOAD_SHIFT = 2 * SCHED_FIXEDPOINT_SHIFT` 提升内部负载计算的精度，改善低权重任务组（如 nice +19）和深层 cgroup 层级的负载均衡效果。`scale_load()` 和 `scale_load_down()` 实现了用户权重与内部高精度负载值之间的无损转换。\n\n### 非对称 CPU 能力建模\n`asym_cap_data` 结构体结合 `cpu_capacity_span()` 宏，将具有相同计算能力的 CPU 归为一组，并通过全局链表 `asym_cap_list` 管理。这为调度器在异构系统中进行负载均衡和任务迁移提供关键拓扑信息。\n\n### cgroup 权重标准化\n通过 `sched_weight_from_cgroup()` 和 `sched_weight_to_cgroup()`，将 cgroup 接口的权重范围（1–10000，默认 100）映射到调度器内部使用的权重值（基于 1024 基准），确保用户配置与调度行为的一致性。\n\n### SCHED_DEADLINE 与频率调节协同\n引入 `SCHED_FLAG_SUGOV` 标志，允许 `schedutil` 频率调节器的工作线程在需要时临时突破 SCHED_DEADLINE 的优先级限制，以解决某些平台无法原子切换 CPU 频率的问题。这是一种临时性 workaround，依赖于 `dl_entity_is_special()` 进行识别。\n\n### 安全位运算\n`shr_bound()` 宏确保右移操作不会因移位数过大而触发未定义行为（UB），通过 `min_t()` 将移位数限制在 `BITS_PER_TYPE(val) - 1` 以内。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- **调度子系统内部**：包含多个调度相关子模块头文件（如 `affinity.h`, `deadline.h`, `topology.h`, `cpupri.h` 等）。\n- **核心内核设施**：依赖 `atomic.h`, `rcupdate.h`, `cpumask_api.h`, `ktime_api.h`, `trace/events/sched.h` 等。\n- **平台与虚拟化**：条件包含 `asm/paravirt.h`（半虚拟化支持）和 `asm/barrier.h`（内存屏障）。\n- **工作队列**：包含 `../workqueue_internal.h`，用于与工作队列子系统交互。\n\n### 配置选项依赖\n- `CONFIG_64BIT`：启用高精度负载计算。\n- `CONFIG_SCHED_DEBUG`：启用 `SCHED_WARN_ON()` 调试检查。\n- `CONFIG_CPU_FREQ_GOV_SCHEDUTIL`：启用 `SCHED_FLAG_SUGOV` 相关逻辑。\n- `CONFIG_SCHED_CLASS_EXT`：扩展调度类支持（影响 `normal_policy()` 判断）。\n- `CONFIG_PARAVIRT`：半虚拟化支持。\n\n## 5. 使用场景\n\n- **调度器初始化与运行**：`scheduler_running` 和负载计算变量在调度器启动和周期性负载更新中使用。\n- **任务调度策略处理**：所有调度类（CFS、RT、Deadline、Idle）在入队、出队、选择下一个任务时，通过策略判断函数确定任务类型。\n- **负载均衡与迁移**：`asym_cap_data` 和 CPU 拓扑信息用于跨 CPU 的任务迁移决策，尤其在异构系统中。\n- **cgroup 资源控制**：在设置或读取 cgroup 的 CPU 权重时，通过权重转换函数确保调度器内部表示与用户接口一致。\n- **实时带宽管理**：`sysctl_sched_rt_*` 参数用于限制 SCHED_FIFO/SCHED_RR 任务的 CPU 使用率。\n- **能效调度协同**：`SCHED_FLAG_SUGOV` 机制使频率调节器能及时响应 Deadline 任务的性能需求。\n- **内核调试与追踪**：`SCHED_WARN_ON()` 用于捕获调度器内部异常状态；tracepoint 定义支持调度事件追踪。",
      "similarity": 0.5247490406036377,
      "chunks": []
    },
    {
      "source_file": "kernel/sched/core.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:00:02\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\core.c`\n\n---\n\n# `sched/core.c` 技术文档\n\n## 1. 文件概述\n\n`sched/core.c` 是 Linux 内核调度器的核心实现文件，负责 CPU 调度器的基础框架、任务入队/出队逻辑、调度类管理、上下文切换协调以及调度器全局状态维护。该文件是整个调度子系统（`kernel/sched/`）的中枢，为 CFS（完全公平调度器）、RT（实时调度器）、DL（截止时间调度器）以及新兴的 SCX（可扩展调度器）等调度类提供统一的调度接口和运行时支持。此外，该文件还实现了与 CPU 热插拔、负载均衡、能耗调度、调度调试等功能的集成。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct rq`（运行队列）：每个 CPU 对应一个，通过 `DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues)` 定义，是调度器操作的基本单元。\n- `core_cookie` 与 `core_node`：用于 **SCHED_CORE** 特性（任务协同调度）的任务分组标识和红黑树节点。\n- `sysctl_sched_features`：调度器调试特性开关（仅在 `CONFIG_SCHED_DEBUG` 下有效）。\n- `scheduler_running`：全局标志，指示调度器是否已初始化并运行。\n\n### 主要函数\n- `sched_core_enqueue(struct rq *rq, struct task_struct *p)`  \n  将任务加入 SCHED_CORE 协同调度红黑树。\n- `sched_core_dequeue(struct rq *rq, struct task_struct *p, int flags)`  \n  从 SCHED_CORE 红黑树中移除任务，并在特定条件下触发重调度。\n- `__task_prio(const struct task_struct *p)`  \n  返回任务的内部优先级数值，用于 SCHED_CORE 中的任务排序（数值越小优先级越高）。\n- `prio_less(const struct task_struct *a, const struct task_struct *b, bool in_fi)`  \n  比较两个任务的优先级，考虑调度类（stop、dl、rt、fair、idle、scx）及截止时间。\n- `__sched_core_less(const struct task_struct *a, const struct task_struct *b)`  \n  SCHED_CORE 专用的任务比较函数，先按 `core_cookie` 分组，组内按优先级排序（高优先级靠左）。\n- `sched_task_is_throttled(struct task_struct *p, int cpu)`  \n  查询任务是否被其调度类限流（如 CFS 带宽控制）。\n\n### 全局变量与宏\n- `sysctl_sched_nr_migrate`：单次负载均衡迁移任务数上限（IRQ 关闭期间执行）。\n- `sysctl_resched_latency_warn_ms` / `sysctl_resched_latency_warn_once`：调度延迟警告阈值与模式（调试用）。\n- `__sched_core_enabled`：静态键（static key），用于动态启用/禁用 SCHED_CORE 功能。\n\n### Tracepoint 导出\n导出多个调度相关 tracepoint 供外部模块（如 ftrace、perf）探测，包括：\n- PELT 负载跟踪（`pelt_cfs_tp`, `pelt_rt_tp` 等）\n- CPU 容量与过载状态（`sched_cpu_capacity_tp`, `sched_overutilized_tp`）\n- 能耗调度（`sched_compute_energy_tp`）\n- 运行任务数变化（`sched_update_nr_running_tp`）\n\n## 3. 关键实现\n\n### SCHED_CORE 协同调度机制\n- **任务分组**：通过 `core_cookie` 将需协同调度的任务归为一组（如来自同一 cgroup 或用户指定）。\n- **红黑树组织**：每 CPU 的 `rq->core_tree` 按 `core_cookie` 和优先级维护任务，确保同组高优先级任务优先执行。\n- **优先级映射**：\n  - Stop 任务：`-2`\n  - DL 任务（含 DL server）：`-1`\n  - RT 任务：`[0, 99]`\n  - Fair 任务：`119`\n  - SCX 任务：`120`\n  - Idle 任务：`140`\n- **强制空闲处理**：当 CPU 处于 forced-idle 状态且最后一个运行任务被移出时，调用 `resched_curr()` 触发重调度，以更新 forced-idle 的会计统计并重新评估状态。\n\n### 优先级比较逻辑\n- `prio_less()` 实现跨调度类的统一优先级比较：\n  - DL 任务按截止时间（deadline）排序（越早越高优）。\n  - Fair 任务委托给 `cfs_prio_less()`（通常基于虚拟运行时间）。\n  - SCX 任务委托给 `scx_prio_less()`（由 BPF 调度器定义）。\n- 在 SCHED_CORE 中，通过 `__sched_core_less()` **反转优先级顺序**，使红黑树左端为最高优先级任务，便于快速选取。\n\n### 调试与可观测性\n- `CONFIG_SCHED_DEBUG` 启用后，提供运行时可调参数（如 `sysctl_sched_nr_migrate`）和延迟警告机制。\n- 大量 tracepoint 覆盖调度器内部状态变化，支持性能分析与问题诊断。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **调度子系统内部**：`sched.h`, `pelt.h`, `smp.h`, `stats.h`, `autogroup.h`\n- **核心内核组件**：\n  - 内存管理：`highmem.h`, `mm.h`, `mmu_context.h`\n  - 锁与同步：`spinlock_api.h`, `mutex_api.h`, `rcuwait_api.h`\n  - 时间与定时器：`ktime_api.h`, `hrtimer_api.h`, `jiffies.h`\n  - CPU 拓扑与热插拔：`topology.h`, `sched/hotplug.h`\n  - RCU 与中断：`hardirq.h`, `softirq.h`, `nmi.h`\n- **架构相关**：`asm/switch_to.h`, `asm/tlb.h`, `asm/irq_regs.h`\n- **其他子系统**：`workqueue_internal.h`, `io_uring/io-wq.h`, `psi.h`, `perf_event_api.h`\n\n### 功能依赖\n- **调度类**：CFS (`fair.c`)、RT (`rt.c`)、DL (`deadline.c`)、SCX (`scx.c`)\n- **负载跟踪**：PELT（Per-Entity Load Tracking）算法\n- **CPU 管理**：CPU 热插拔、隔离（`isolcpus`）、NO_HZ（动态 tick）\n- **安全与权限**：`capability.h`, `cpuset.h`, `ioprio.h`\n\n## 5. 使用场景\n\n- **任务调度主路径**：每次调用 `schedule()` 时，调度器核心协调各调度类选择下一个运行任务。\n- **负载均衡**：在 `migration` 线程或定时器中断中，通过 `sysctl_sched_nr_migrate` 限制单次迁移任务数。\n- **SCHED_CORE 应用**：在需要多任务协同执行的场景（如 HPC、实时音视频处理），通过 `core.sched` 接口设置 `core_cookie` 实现组调度。\n- **调试与监控**：\n  - 通过 `/proc/sys/kernel/sched_*` 调整调度参数。\n  - 使用 `ftrace` 或 `perf` 捕获 tracepoint 数据分析调度行为。\n  - 启用 `LATENCY_WARN` 检测调度延迟异常。\n- **系统初始化**：`scheduler_running` 标志在 `sched_init()` 中设置，确保调度器在 SMP 启动前就绪。\n- **强制空闲管理**：在能耗敏感场景（如手机待机），配合 `core_forceidle_count` 实现 CPU 核心协同休眠。",
      "similarity": 0.516124963760376,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/core.c",
          "start_line": 281,
          "end_line": 384,
          "content": [
            "void sched_core_dequeue(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tif (p->se.sched_delayed)",
            "\t\treturn;",
            "",
            "\trq->core->core_task_seq++;",
            "",
            "\tif (sched_core_enqueued(p)) {",
            "\t\trb_erase(&p->core_node, &rq->core_tree);",
            "\t\tRB_CLEAR_NODE(&p->core_node);",
            "\t}",
            "",
            "\t/*",
            "\t * Migrating the last task off the cpu, with the cpu in forced idle",
            "\t * state. Reschedule to create an accounting edge for forced idle,",
            "\t * and re-examine whether the core is still in forced idle state.",
            "\t */",
            "\tif (!(flags & DEQUEUE_SAVE) && rq->nr_running == 1 &&",
            "\t    rq->core->core_forceidle_count && rq->curr == rq->idle)",
            "\t\tresched_curr(rq);",
            "}",
            "static int sched_task_is_throttled(struct task_struct *p, int cpu)",
            "{",
            "\tif (p->sched_class->task_is_throttled)",
            "\t\treturn p->sched_class->task_is_throttled(p, cpu);",
            "",
            "\treturn 0;",
            "}",
            "static void sched_core_lock(int cpu, unsigned long *flags)",
            "{",
            "\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);",
            "\tint t, i = 0;",
            "",
            "\tlocal_irq_save(*flags);",
            "\tfor_each_cpu(t, smt_mask)",
            "\t\traw_spin_lock_nested(&cpu_rq(t)->__lock, i++);",
            "}",
            "static void sched_core_unlock(int cpu, unsigned long *flags)",
            "{",
            "\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);",
            "\tint t;",
            "",
            "\tfor_each_cpu(t, smt_mask)",
            "\t\traw_spin_unlock(&cpu_rq(t)->__lock);",
            "\tlocal_irq_restore(*flags);",
            "}",
            "static void __sched_core_flip(bool enabled)",
            "{",
            "\tunsigned long flags;",
            "\tint cpu, t;",
            "",
            "\tcpus_read_lock();",
            "",
            "\t/*",
            "\t * Toggle the online cores, one by one.",
            "\t */",
            "\tcpumask_copy(&sched_core_mask, cpu_online_mask);",
            "\tfor_each_cpu(cpu, &sched_core_mask) {",
            "\t\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);",
            "",
            "\t\tsched_core_lock(cpu, &flags);",
            "",
            "\t\tfor_each_cpu(t, smt_mask)",
            "\t\t\tcpu_rq(t)->core_enabled = enabled;",
            "",
            "\t\tcpu_rq(cpu)->core->core_forceidle_start = 0;",
            "",
            "\t\tsched_core_unlock(cpu, &flags);",
            "",
            "\t\tcpumask_andnot(&sched_core_mask, &sched_core_mask, smt_mask);",
            "\t}",
            "",
            "\t/*",
            "\t * Toggle the offline CPUs.",
            "\t */",
            "\tfor_each_cpu_andnot(cpu, cpu_possible_mask, cpu_online_mask)",
            "\t\tcpu_rq(cpu)->core_enabled = enabled;",
            "",
            "\tcpus_read_unlock();",
            "}",
            "static void sched_core_assert_empty(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tWARN_ON_ONCE(!RB_EMPTY_ROOT(&cpu_rq(cpu)->core_tree));",
            "}",
            "static void __sched_core_enable(void)",
            "{",
            "\tstatic_branch_enable(&__sched_core_enabled);",
            "\t/*",
            "\t * Ensure all previous instances of raw_spin_rq_*lock() have finished",
            "\t * and future ones will observe !sched_core_disabled().",
            "\t */",
            "\tsynchronize_rcu();",
            "\t__sched_core_flip(true);",
            "\tsched_core_assert_empty();",
            "}",
            "static void __sched_core_disable(void)",
            "{",
            "\tsched_core_assert_empty();",
            "\t__sched_core_flip(false);",
            "\tstatic_branch_disable(&__sched_core_enabled);",
            "}"
          ],
          "function_name": "sched_core_dequeue, sched_task_is_throttled, sched_core_lock, sched_core_unlock, __sched_core_flip, sched_core_assert_empty, __sched_core_enable, __sched_core_disable",
          "description": "提供核心调度器的并发控制接口，sched_core_lock/sched_core_unlock实现SMT线程锁操作，__sched_core_flip切换核心在线状态，assert_empty检查核心树空置状态，enable/disable方法控制核心调度器启停",
          "similarity": 0.5660015940666199
        },
        {
          "chunk_id": 45,
          "file_path": "kernel/sched/core.c",
          "start_line": 8030,
          "end_line": 8152,
          "content": [
            "void set_rq_offline(struct rq *rq)",
            "{",
            "\tif (rq->online) {",
            "\t\tconst struct sched_class *class;",
            "",
            "\t\tupdate_rq_clock(rq);",
            "\t\tfor_each_class(class) {",
            "\t\t\tif (class->rq_offline)",
            "\t\t\t\tclass->rq_offline(rq);",
            "\t\t}",
            "",
            "\t\tcpumask_clear_cpu(rq->cpu, rq->rd->online);",
            "\t\trq->online = 0;",
            "\t}",
            "}",
            "static inline void sched_set_rq_online(struct rq *rq, int cpu)",
            "{",
            "\tstruct rq_flags rf;",
            "",
            "\trq_lock_irqsave(rq, &rf);",
            "\tif (rq->rd) {",
            "\t\tBUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));",
            "\t\tset_rq_online(rq);",
            "\t}",
            "\trq_unlock_irqrestore(rq, &rf);",
            "}",
            "static inline void sched_set_rq_offline(struct rq *rq, int cpu)",
            "{",
            "\tstruct rq_flags rf;",
            "",
            "\trq_lock_irqsave(rq, &rf);",
            "\tif (rq->rd) {",
            "\t\tBUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));",
            "\t\tset_rq_offline(rq);",
            "\t}",
            "\trq_unlock_irqrestore(rq, &rf);",
            "}",
            "static void cpuset_cpu_active(void)",
            "{",
            "\tif (cpuhp_tasks_frozen) {",
            "\t\t/*",
            "\t\t * num_cpus_frozen tracks how many CPUs are involved in suspend",
            "\t\t * resume sequence. As long as this is not the last online",
            "\t\t * operation in the resume sequence, just build a single sched",
            "\t\t * domain, ignoring cpusets.",
            "\t\t */",
            "\t\tpartition_sched_domains(1, NULL, NULL);",
            "\t\tif (--num_cpus_frozen)",
            "\t\t\treturn;",
            "\t\t/*",
            "\t\t * This is the last CPU online operation. So fall through and",
            "\t\t * restore the original sched domains by considering the",
            "\t\t * cpuset configurations.",
            "\t\t */",
            "\t\tcpuset_force_rebuild();",
            "\t}",
            "\tcpuset_update_active_cpus();",
            "}",
            "static int cpuset_cpu_inactive(unsigned int cpu)",
            "{",
            "\tif (!cpuhp_tasks_frozen) {",
            "\t\tint ret = dl_bw_check_overflow(cpu);",
            "",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t\tcpuset_update_active_cpus();",
            "\t} else {",
            "\t\tnum_cpus_frozen++;",
            "\t\tpartition_sched_domains(1, NULL, NULL);",
            "\t}",
            "\treturn 0;",
            "}",
            "static inline void sched_smt_present_inc(int cpu)",
            "{",
            "#ifdef CONFIG_SCHED_SMT",
            "\tif (cpumask_weight(cpu_smt_mask(cpu)) == 2)",
            "\t\tstatic_branch_inc_cpuslocked(&sched_smt_present);",
            "#endif",
            "}",
            "static inline void sched_smt_present_dec(int cpu)",
            "{",
            "#ifdef CONFIG_SCHED_SMT",
            "\tif (cpumask_weight(cpu_smt_mask(cpu)) == 2)",
            "\t\tstatic_branch_dec_cpuslocked(&sched_smt_present);",
            "#endif",
            "}",
            "int sched_cpu_activate(unsigned int cpu)",
            "{",
            "\tstruct rq *rq = cpu_rq(cpu);",
            "",
            "\t/*",
            "\t * Clear the balance_push callback and prepare to schedule",
            "\t * regular tasks.",
            "\t */",
            "\tbalance_push_set(cpu, false);",
            "",
            "\t/*",
            "\t * When going up, increment the number of cores with SMT present.",
            "\t */",
            "\tsched_smt_present_inc(cpu);",
            "\tset_cpu_active(cpu, true);",
            "",
            "\tif (sched_smp_initialized) {",
            "\t\tsched_update_numa(cpu, true);",
            "\t\tsched_domains_numa_masks_set(cpu);",
            "\t\tcpuset_cpu_active();",
            "\t}",
            "",
            "\tscx_rq_activate(rq);",
            "",
            "\t/*",
            "\t * Put the rq online, if not already. This happens:",
            "\t *",
            "\t * 1) In the early boot process, because we build the real domains",
            "\t *    after all CPUs have been brought up.",
            "\t *",
            "\t * 2) At runtime, if cpuset_cpu_active() fails to rebuild the",
            "\t *    domains.",
            "\t */",
            "\tsched_set_rq_online(rq, cpu);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "set_rq_offline, sched_set_rq_online, sched_set_rq_offline, cpuset_cpu_active, cpuset_cpu_inactive, sched_smt_present_inc, sched_smt_present_dec, sched_cpu_activate",
          "description": "管理运行队列在线状态，通过遍历调度类回调处理CFS/RT等调度器的offline操作，同步更新调度域掩码及SMT核心计数，协调CPU激活时的调度域重建与SMT状态变更",
          "similarity": 0.5392824411392212
        },
        {
          "chunk_id": 26,
          "file_path": "kernel/sched/core.c",
          "start_line": 4497,
          "end_line": 4597,
          "content": [
            "static void __set_numabalancing_state(bool enabled)",
            "{",
            "\tif (enabled)",
            "\t\tstatic_branch_enable(&sched_numa_balancing);",
            "\telse",
            "\t\tstatic_branch_disable(&sched_numa_balancing);",
            "}",
            "void set_numabalancing_state(bool enabled)",
            "{",
            "\tif (enabled)",
            "\t\tsysctl_numa_balancing_mode = NUMA_BALANCING_NORMAL;",
            "\telse",
            "\t\tsysctl_numa_balancing_mode = NUMA_BALANCING_DISABLED;",
            "\t__set_numabalancing_state(enabled);",
            "}",
            "static void reset_memory_tiering(void)",
            "{",
            "\tstruct pglist_data *pgdat;",
            "",
            "\tfor_each_online_pgdat(pgdat) {",
            "\t\tpgdat->nbp_threshold = 0;",
            "\t\tpgdat->nbp_th_nr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);",
            "\t\tpgdat->nbp_th_start = jiffies_to_msecs(jiffies);",
            "\t}",
            "}",
            "static int sysctl_numa_balancing(struct ctl_table *table, int write,",
            "\t\t\t  void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tstruct ctl_table t;",
            "\tint err;",
            "\tint state = sysctl_numa_balancing_mode;",
            "",
            "\tif (write && !capable(CAP_SYS_ADMIN))",
            "\t\treturn -EPERM;",
            "",
            "\tt = *table;",
            "\tt.data = &state;",
            "\terr = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);",
            "\tif (err < 0)",
            "\t\treturn err;",
            "\tif (write) {",
            "\t\tif (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&",
            "\t\t    (state & NUMA_BALANCING_MEMORY_TIERING))",
            "\t\t\treset_memory_tiering();",
            "\t\tsysctl_numa_balancing_mode = state;",
            "\t\t__set_numabalancing_state(state);",
            "\t}",
            "\treturn err;",
            "}",
            "static void set_schedstats(bool enabled)",
            "{",
            "\tif (enabled)",
            "\t\tstatic_branch_enable(&sched_schedstats);",
            "\telse",
            "\t\tstatic_branch_disable(&sched_schedstats);",
            "}",
            "void force_schedstat_enabled(void)",
            "{",
            "\tif (!schedstat_enabled()) {",
            "\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");",
            "\t\tstatic_branch_enable(&sched_schedstats);",
            "\t}",
            "}",
            "static int __init setup_schedstats(char *str)",
            "{",
            "\tint ret = 0;",
            "\tif (!str)",
            "\t\tgoto out;",
            "",
            "\tif (!strcmp(str, \"enable\")) {",
            "\t\tset_schedstats(true);",
            "\t\tret = 1;",
            "\t} else if (!strcmp(str, \"disable\")) {",
            "\t\tset_schedstats(false);",
            "\t\tret = 1;",
            "\t}",
            "out:",
            "\tif (!ret)",
            "\t\tpr_warn(\"Unable to parse schedstats=\\n\");",
            "",
            "\treturn ret;",
            "}",
            "static int sysctl_schedstats(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos)",
            "{",
            "\tstruct ctl_table t;",
            "\tint err;",
            "\tint state = static_branch_likely(&sched_schedstats);",
            "",
            "\tif (write && !capable(CAP_SYS_ADMIN))",
            "\t\treturn -EPERM;",
            "",
            "\tt = *table;",
            "\tt.data = &state;",
            "\terr = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);",
            "\tif (err < 0)",
            "\t\treturn err;",
            "\tif (write)",
            "\t\tset_schedstats(state);",
            "\treturn err;",
            "}"
          ],
          "function_name": "__set_numabalancing_state, set_numabalancing_state, reset_memory_tiering, sysctl_numa_balancing, set_schedstats, force_schedstat_enabled, setup_schedstats, sysctl_schedstats",
          "description": "控制NUMA平衡功能的启用/禁用状态，管理调度统计功能的开关，提供内存分层策略重置和系统参数调节的接口实现",
          "similarity": 0.5357856154441833
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/core.c",
          "start_line": 1,
          "end_line": 160,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  kernel/sched/core.c",
            " *",
            " *  Core kernel CPU scheduler code",
            " *",
            " *  Copyright (C) 1991-2002  Linus Torvalds",
            " *  Copyright (C) 1998-2024  Ingo Molnar, Red Hat",
            " */",
            "#include <linux/highmem.h>",
            "#include <linux/hrtimer_api.h>",
            "#include <linux/ktime_api.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/syscalls_api.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/capability.h>",
            "#include <linux/pgtable_api.h>",
            "#include <linux/wait_bit.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/spinlock_api.h>",
            "#include <linux/cpumask_api.h>",
            "#include <linux/lockdep_api.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/softirq.h>",
            "#include <linux/refcount_api.h>",
            "#include <linux/topology.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/cond_resched.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/rseq_api.h>",
            "#include <linux/sched/rt.h>",
            "",
            "#include <linux/blkdev.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/init_task.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/ioprio.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/kcov.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/llist_api.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/mutex_api.h>",
            "#include <linux/nmi.h>",
            "#include <linux/nospec.h>",
            "#include <linux/perf_event_api.h>",
            "#include <linux/profile.h>",
            "#include <linux/psi.h>",
            "#include <linux/rcuwait_api.h>",
            "#include <linux/rseq.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/scs.h>",
            "#include <linux/slab.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/vtime.h>",
            "#include <linux/wait_api.h>",
            "#include <linux/workqueue_api.h>",
            "",
            "#ifdef CONFIG_PREEMPT_DYNAMIC",
            "# ifdef CONFIG_GENERIC_ENTRY",
            "#  include <linux/entry-common.h>",
            "# endif",
            "#endif",
            "",
            "#include <uapi/linux/sched/types.h>",
            "",
            "#include <asm/irq_regs.h>",
            "#include <asm/switch_to.h>",
            "#include <asm/tlb.h>",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <linux/sched/rseq_api.h>",
            "#include <trace/events/sched.h>",
            "#include <trace/events/ipi.h>",
            "#undef CREATE_TRACE_POINTS",
            "",
            "#include \"sched.h\"",
            "#include \"stats.h\"",
            "",
            "#include \"autogroup.h\"",
            "#include \"pelt.h\"",
            "#include \"smp.h\"",
            "#include \"stats.h\"",
            "",
            "#include \"../workqueue_internal.h\"",
            "#include \"../../io_uring/io-wq.h\"",
            "#include \"../smpboot.h\"",
            "",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(ipi_send_cpu);",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(ipi_send_cpumask);",
            "",
            "/*",
            " * Export tracepoints that act as a bare tracehook (ie: have no trace event",
            " * associated with them) to allow external modules to probe them.",
            " */",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_cfs_tp);",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_rt_tp);",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_dl_tp);",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_irq_tp);",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_se_tp);",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_hw_tp);",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(sched_cpu_capacity_tp);",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(sched_overutilized_tp);",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(sched_util_est_cfs_tp);",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(sched_util_est_se_tp);",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(sched_update_nr_running_tp);",
            "EXPORT_TRACEPOINT_SYMBOL_GPL(sched_compute_energy_tp);",
            "",
            "DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);",
            "",
            "#ifdef CONFIG_SCHED_DEBUG",
            "/*",
            " * Debugging: various feature bits",
            " *",
            " * If SCHED_DEBUG is disabled, each compilation unit has its own copy of",
            " * sysctl_sched_features, defined in sched.h, to allow constants propagation",
            " * at compile time and compiler optimization based on features default.",
            " */",
            "#define SCHED_FEAT(name, enabled)\t\\",
            "\t(1UL << __SCHED_FEAT_##name) * enabled |",
            "const_debug unsigned int sysctl_sched_features =",
            "#include \"features.h\"",
            "\t0;",
            "#undef SCHED_FEAT",
            "",
            "/*",
            " * Print a warning if need_resched is set for the given duration (if",
            " * LATENCY_WARN is enabled).",
            " *",
            " * If sysctl_resched_latency_warn_once is set, only one warning will be shown",
            " * per boot.",
            " */",
            "__read_mostly int sysctl_resched_latency_warn_ms = 100;",
            "__read_mostly int sysctl_resched_latency_warn_once = 1;",
            "#endif /* CONFIG_SCHED_DEBUG */",
            "",
            "/*",
            " * Number of tasks to iterate in a single balance run.",
            " * Limited because this is done with IRQs disabled.",
            " */",
            "const_debug unsigned int sysctl_sched_nr_migrate = SCHED_NR_MIGRATE_BREAK;",
            "",
            "__read_mostly int scheduler_running;",
            "",
            "#ifdef CONFIG_SCHED_CORE",
            "",
            "DEFINE_STATIC_KEY_FALSE(__sched_core_enabled);",
            "",
            "/* kernel prio, less is more */"
          ],
          "function_name": null,
          "description": "定义调度器核心功能及相关宏，配置调度器调试特性（如SCHED_FEAT），声明全局变量scheduler_running和核心调度开关标志，包含多个调度子系统头文件以支持不同调度策略。",
          "similarity": 0.5349900722503662
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/core.c",
          "start_line": 734,
          "end_line": 836,
          "content": [
            "static void update_rq_clock_task(struct rq *rq, s64 delta)",
            "{",
            "/*",
            " * In theory, the compile should just see 0 here, and optimize out the call",
            " * to sched_rt_avg_update. But I don't trust it...",
            " */",
            "\ts64 __maybe_unused steal = 0, irq_delta = 0;",
            "",
            "#ifdef CONFIG_IRQ_TIME_ACCOUNTING",
            "\tirq_delta = irq_time_read(cpu_of(rq)) - rq->prev_irq_time;",
            "",
            "\t/*",
            "\t * Since irq_time is only updated on {soft,}irq_exit, we might run into",
            "\t * this case when a previous update_rq_clock() happened inside a",
            "\t * {soft,}irq region.",
            "\t *",
            "\t * When this happens, we stop ->clock_task and only update the",
            "\t * prev_irq_time stamp to account for the part that fit, so that a next",
            "\t * update will consume the rest. This ensures ->clock_task is",
            "\t * monotonic.",
            "\t *",
            "\t * It does however cause some slight miss-attribution of {soft,}irq",
            "\t * time, a more accurate solution would be to update the irq_time using",
            "\t * the current rq->clock timestamp, except that would require using",
            "\t * atomic ops.",
            "\t */",
            "\tif (irq_delta > delta)",
            "\t\tirq_delta = delta;",
            "",
            "\trq->prev_irq_time += irq_delta;",
            "\tdelta -= irq_delta;",
            "\tdelayacct_irq(rq->curr, irq_delta);",
            "#endif",
            "#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING",
            "\tif (static_key_false((&paravirt_steal_rq_enabled))) {",
            "\t\tu64 prev_steal;",
            "",
            "\t\tsteal = prev_steal = paravirt_steal_clock(cpu_of(rq));",
            "\t\tsteal -= rq->prev_steal_time_rq;",
            "",
            "\t\tif (unlikely(steal > delta))",
            "\t\t\tsteal = delta;",
            "",
            "\t\trq->prev_steal_time_rq = prev_steal;",
            "\t\tdelta -= steal;",
            "\t}",
            "#endif",
            "",
            "\trq->clock_task += delta;",
            "",
            "#ifdef CONFIG_HAVE_SCHED_AVG_IRQ",
            "\tif ((irq_delta + steal) && sched_feat(NONTASK_CAPACITY))",
            "\t\tupdate_irq_load_avg(rq, irq_delta + steal);",
            "#endif",
            "\tupdate_rq_clock_pelt(rq, delta);",
            "}",
            "void update_rq_clock(struct rq *rq)",
            "{",
            "\ts64 delta;",
            "",
            "\tlockdep_assert_rq_held(rq);",
            "",
            "\tif (rq->clock_update_flags & RQCF_ACT_SKIP)",
            "\t\treturn;",
            "",
            "#ifdef CONFIG_SCHED_DEBUG",
            "\tif (sched_feat(WARN_DOUBLE_CLOCK))",
            "\t\tSCHED_WARN_ON(rq->clock_update_flags & RQCF_UPDATED);",
            "\trq->clock_update_flags |= RQCF_UPDATED;",
            "#endif",
            "",
            "\tdelta = sched_clock_cpu(cpu_of(rq)) - rq->clock;",
            "\tif (delta < 0)",
            "\t\treturn;",
            "\trq->clock += delta;",
            "\tupdate_rq_clock_task(rq, delta);",
            "}",
            "static void hrtick_clear(struct rq *rq)",
            "{",
            "\tif (hrtimer_active(&rq->hrtick_timer))",
            "\t\thrtimer_cancel(&rq->hrtick_timer);",
            "}",
            "static enum hrtimer_restart hrtick(struct hrtimer *timer)",
            "{",
            "\tstruct rq *rq = container_of(timer, struct rq, hrtick_timer);",
            "\tstruct rq_flags rf;",
            "",
            "\tWARN_ON_ONCE(cpu_of(rq) != smp_processor_id());",
            "",
            "\trq_lock(rq, &rf);",
            "\tupdate_rq_clock(rq);",
            "\trq->curr->sched_class->task_tick(rq, rq->curr, 1);",
            "\trq_unlock(rq, &rf);",
            "",
            "\treturn HRTIMER_NORESTART;",
            "}",
            "static void __hrtick_restart(struct rq *rq)",
            "{",
            "\tstruct hrtimer *timer = &rq->hrtick_timer;",
            "\tktime_t time = rq->hrtick_time;",
            "",
            "\thrtimer_start(timer, time, HRTIMER_MODE_ABS_PINNED_HARD);",
            "}"
          ],
          "function_name": "update_rq_clock_task, update_rq_clock, hrtick_clear, hrtick, __hrtick_restart",
          "description": "处理调度器时间戳更新逻辑，update_rq_clock同步物理时钟到运行队列，hrtick定时器用于精确时钟中断唤醒，__hrtick_restart重新启动高精度定时器以维持调度时机准确性",
          "similarity": 0.5338717699050903
        }
      ]
    }
  ]
}