{
  "query": "虚拟文件系统VFS层设计原理",
  "timestamp": "2025-12-25 23:27:31",
  "retrieved_files": [
    {
      "source_file": "mm/vmpressure.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:33:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmpressure.c`\n\n---\n\n# vmpressure.c 技术文档\n\n## 1. 文件概述\n\n`vmpressure.c` 实现了 Linux 内核中的虚拟内存压力（VM pressure）监控机制。该机制通过跟踪页面扫描（scanned）与回收（reclaimed）的比率，评估系统或特定内存控制组（memcg）所面临的内存压力程度，并在达到预设阈值时向用户空间发送通知。此功能主要用于支持 cgroup v2 的 memory.pressure 接口，使用户空间程序（如容器运行时）能够感知内存紧张状况并作出响应（如释放缓存、限制内存使用等），从而避免系统进入 OOM（Out-Of-Memory）状态。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct vmpressure`**  \n  表示一个内存控制组的 VM 压力状态，包含：\n  - `tree_scanned` / `tree_reclaimed`：累积的扫描和回收页数（用于子树模式）\n  - `sr_lock`：保护上述计数器的自旋锁\n  - `events_lock`：保护事件监听列表的互斥锁\n  - `events`：注册的事件监听器链表（`struct vmpressure_event`）\n  - `work`：延迟处理工作项（`struct work_struct`）\n\n- **`struct vmpressure_event`**  \n  表示一个用户空间注册的压力事件监听器，包含：\n  - `efd`：关联的 eventfd 上下文，用于通知\n  - `level`：触发通知的最低压力等级（low/medium/critical）\n  - `mode`：通知模式（default/hierarchy/local）\n  - `node`：链表节点\n\n### 主要函数\n\n- **`vmpressure()`**  \n  核心接口函数，由内存回收路径（vmscan）调用，传入当前扫描和回收的页数，更新压力统计并可能调度异步处理。\n\n- **`vmpressure_work_fn()`**  \n  工作队列回调函数，负责计算压力等级、触发事件通知，并向上遍历内存控制组层级（支持层次化通知）。\n\n- **`vmpressure_calc_level()`**  \n  根据 `scanned` 和 `reclaimed` 计算压力百分比，并映射到离散的压力等级（low/medium/critical）。\n\n- **`vmpressure_event()`**  \n  遍历当前 memcg 注册的所有事件监听器，根据压力等级、通知模式和层级关系决定是否触发 eventfd 信号。\n\n- **辅助函数**  \n  - `vmpressure_parent()`：获取父级 memcg 对应的 `vmpressure` 结构\n  - `vmpressure_level()`：将压力百分比映射为枚举等级\n  - `work_to_vmpressure()`：从 work_struct 转换为 vmpressure 指针\n\n### 关键常量\n\n- **`vmpressure_win`**：压力计算窗口大小（512 页），用于速率限制和平均\n- **`vmpressure_level_med`**（60）和 **`vmpressure_level_critical`**（95）：中等和严重压力的百分比阈值\n- **`vmpressure_level_critical_prio`**：基于扫描优先级判断严重压力的备用机制\n\n## 3. 关键实现\n\n### 压力等级计算\n压力通过公式 `pressure = (scanned - reclaimed) * 100 / scanned` 计算（实际实现考虑了 `reclaimed > scanned` 的边界情况）。该值反映回收效率：值越高表示回收越困难，内存压力越大。\n\n### 异步处理机制\n`vmpressure()` 函数仅累加计数器并调度 `vmpressure_work_fn` 工作项，避免在内存回收关键路径上执行复杂逻辑。工作项在后台执行压力计算和通知。\n\n### 层级化通知（Hierarchy）\n支持三种通知模式：\n- **`local`**：仅当前 memcg 触发\n- **`hierarchy`**：当前及所有祖先 memcg 均可触发\n- **`default`（no passthrough）**：当前 memcg 触发后，阻止向祖先传递（避免重复通知）\n\n### 窗口与速率限制\n使用固定窗口（`vmpressure_win`）累积扫描/回收页数，确保压力评估具有时间局部性，同时防止高频通知。\n\n### 与 vmscan 集成\n直接接收 vmscan 传递的 `scanned` 和 `reclaimed` 参数，紧密耦合内存回收行为，提供实时压力反馈。\n\n## 4. 依赖关系\n\n- **内存控制组（memcg）**：通过 `mem_cgroup` 结构关联 `vmpressure` 实例，依赖 cgroup 子系统（`memory_cgrp_subsys`）\n- **内存管理核心（mm）**：依赖 `vmscan` 回收路径调用 `vmpressure()`，使用 `SWAP_CLUSTER_MAX` 常量\n- **事件通知机制**：使用 `eventfd` 向用户空间发送信号\n- **内核同步原语**：使用 `spinlock`（`sr_lock`）和 `mutex`（`events_lock`）保护数据\n- **通用内核组件**：依赖 `workqueue`（延迟处理）、`slab`（内存分配）、`printk`（调试）\n\n## 5. 使用场景\n\n1. **容器内存管理**  \n   容器运行时（如 Docker、systemd-nspawn）通过监听 cgroup v2 的 `memory.pressure` 文件，在内存压力升高时主动释放缓存或限制应用内存使用，避免被 OOM killer 终止。\n\n2. **系统级内存优化**  \n   用户空间守护进程（如 earlyoom、nohang）利用压力事件提前干预，例如在 `critical` 压力下终止低优先级进程。\n\n3. **内核子系统集成**  \n   其他内核模块可通过注册 `vmpressure` 事件监听器，在内存紧张时调整自身行为（如降低缓存占用）。\n\n4. **传统 cgroup v1 支持**  \n   通过 `tree` 参数兼容旧版 subtree 压力报告模式（尽管主要面向 cgroup v2 设计）。",
      "similarity": 0.5728054046630859,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/vmpressure.c",
          "start_line": 111,
          "end_line": 290,
          "content": [
            "static enum vmpressure_levels vmpressure_level(unsigned long pressure)",
            "{",
            "\tif (pressure >= vmpressure_level_critical)",
            "\t\treturn VMPRESSURE_CRITICAL;",
            "\telse if (pressure >= vmpressure_level_med)",
            "\t\treturn VMPRESSURE_MEDIUM;",
            "\treturn VMPRESSURE_LOW;",
            "}",
            "static enum vmpressure_levels vmpressure_calc_level(unsigned long scanned,",
            "\t\t\t\t\t\t    unsigned long reclaimed)",
            "{",
            "\tunsigned long scale = scanned + reclaimed;",
            "\tunsigned long pressure = 0;",
            "",
            "\t/*",
            "\t * reclaimed can be greater than scanned for things such as reclaimed",
            "\t * slab pages. shrink_node() just adds reclaimed pages without a",
            "\t * related increment to scanned pages.",
            "\t */",
            "\tif (reclaimed >= scanned)",
            "\t\tgoto out;",
            "\t/*",
            "\t * We calculate the ratio (in percents) of how many pages were",
            "\t * scanned vs. reclaimed in a given time frame (window). Note that",
            "\t * time is in VM reclaimer's \"ticks\", i.e. number of pages",
            "\t * scanned. This makes it possible to set desired reaction time",
            "\t * and serves as a ratelimit.",
            "\t */",
            "\tpressure = scale - (reclaimed * scale / scanned);",
            "\tpressure = pressure * 100 / scale;",
            "",
            "out:",
            "\tpr_debug(\"%s: %3lu  (s: %lu  r: %lu)\\n\", __func__, pressure,",
            "\t\t scanned, reclaimed);",
            "",
            "\treturn vmpressure_level(pressure);",
            "}",
            "static bool vmpressure_event(struct vmpressure *vmpr,",
            "\t\t\t     const enum vmpressure_levels level,",
            "\t\t\t     bool ancestor, bool signalled)",
            "{",
            "\tstruct vmpressure_event *ev;",
            "\tbool ret = false;",
            "",
            "\tmutex_lock(&vmpr->events_lock);",
            "\tlist_for_each_entry(ev, &vmpr->events, node) {",
            "\t\tif (ancestor && ev->mode == VMPRESSURE_LOCAL)",
            "\t\t\tcontinue;",
            "\t\tif (signalled && ev->mode == VMPRESSURE_NO_PASSTHROUGH)",
            "\t\t\tcontinue;",
            "\t\tif (level < ev->level)",
            "\t\t\tcontinue;",
            "\t\teventfd_signal(ev->efd);",
            "\t\tret = true;",
            "\t}",
            "\tmutex_unlock(&vmpr->events_lock);",
            "",
            "\treturn ret;",
            "}",
            "static void vmpressure_work_fn(struct work_struct *work)",
            "{",
            "\tstruct vmpressure *vmpr = work_to_vmpressure(work);",
            "\tunsigned long scanned;",
            "\tunsigned long reclaimed;",
            "\tenum vmpressure_levels level;",
            "\tbool ancestor = false;",
            "\tbool signalled = false;",
            "",
            "\tspin_lock(&vmpr->sr_lock);",
            "\t/*",
            "\t * Several contexts might be calling vmpressure(), so it is",
            "\t * possible that the work was rescheduled again before the old",
            "\t * work context cleared the counters. In that case we will run",
            "\t * just after the old work returns, but then scanned might be zero",
            "\t * here. No need for any locks here since we don't care if",
            "\t * vmpr->reclaimed is in sync.",
            "\t */",
            "\tscanned = vmpr->tree_scanned;",
            "\tif (!scanned) {",
            "\t\tspin_unlock(&vmpr->sr_lock);",
            "\t\treturn;",
            "\t}",
            "",
            "\treclaimed = vmpr->tree_reclaimed;",
            "\tvmpr->tree_scanned = 0;",
            "\tvmpr->tree_reclaimed = 0;",
            "\tspin_unlock(&vmpr->sr_lock);",
            "",
            "\tlevel = vmpressure_calc_level(scanned, reclaimed);",
            "",
            "\tdo {",
            "\t\tif (vmpressure_event(vmpr, level, ancestor, signalled))",
            "\t\t\tsignalled = true;",
            "\t\tancestor = true;",
            "\t} while ((vmpr = vmpressure_parent(vmpr)));",
            "}",
            "void vmpressure(gfp_t gfp, struct mem_cgroup *memcg, bool tree,",
            "\t\tunsigned long scanned, unsigned long reclaimed)",
            "{",
            "\tstruct vmpressure *vmpr;",
            "",
            "\tif (mem_cgroup_disabled())",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * The in-kernel users only care about the reclaim efficiency",
            "\t * for this @memcg rather than the whole subtree, and there",
            "\t * isn't and won't be any in-kernel user in a legacy cgroup.",
            "\t */",
            "\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys) && !tree)",
            "\t\treturn;",
            "",
            "\tvmpr = memcg_to_vmpressure(memcg);",
            "",
            "\t/*",
            "\t * Here we only want to account pressure that userland is able to",
            "\t * help us with. For example, suppose that DMA zone is under",
            "\t * pressure; if we notify userland about that kind of pressure,",
            "\t * then it will be mostly a waste as it will trigger unnecessary",
            "\t * freeing of memory by userland (since userland is more likely to",
            "\t * have HIGHMEM/MOVABLE pages instead of the DMA fallback). That",
            "\t * is why we include only movable, highmem and FS/IO pages.",
            "\t * Indirect reclaim (kswapd) sets sc->gfp_mask to GFP_KERNEL, so",
            "\t * we account it too.",
            "\t */",
            "\tif (!(gfp & (__GFP_HIGHMEM | __GFP_MOVABLE | __GFP_IO | __GFP_FS)))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If we got here with no pages scanned, then that is an indicator",
            "\t * that reclaimer was unable to find any shrinkable LRUs at the",
            "\t * current scanning depth. But it does not mean that we should",
            "\t * report the critical pressure, yet. If the scanning priority",
            "\t * (scanning depth) goes too high (deep), we will be notified",
            "\t * through vmpressure_prio(). But so far, keep calm.",
            "\t */",
            "\tif (!scanned)",
            "\t\treturn;",
            "",
            "\tif (tree) {",
            "\t\tspin_lock(&vmpr->sr_lock);",
            "\t\tscanned = vmpr->tree_scanned += scanned;",
            "\t\tvmpr->tree_reclaimed += reclaimed;",
            "\t\tspin_unlock(&vmpr->sr_lock);",
            "",
            "\t\tif (scanned < vmpressure_win)",
            "\t\t\treturn;",
            "\t\tschedule_work(&vmpr->work);",
            "\t} else {",
            "\t\tenum vmpressure_levels level;",
            "",
            "\t\t/* For now, no users for root-level efficiency */",
            "\t\tif (!memcg || mem_cgroup_is_root(memcg))",
            "\t\t\treturn;",
            "",
            "\t\tspin_lock(&vmpr->sr_lock);",
            "\t\tscanned = vmpr->scanned += scanned;",
            "\t\treclaimed = vmpr->reclaimed += reclaimed;",
            "\t\tif (scanned < vmpressure_win) {",
            "\t\t\tspin_unlock(&vmpr->sr_lock);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tvmpr->scanned = vmpr->reclaimed = 0;",
            "\t\tspin_unlock(&vmpr->sr_lock);",
            "",
            "\t\tlevel = vmpressure_calc_level(scanned, reclaimed);",
            "",
            "\t\tif (level > VMPRESSURE_LOW) {",
            "\t\t\t/*",
            "\t\t\t * Let the socket buffer allocator know that",
            "\t\t\t * we are having trouble reclaiming LRU pages.",
            "\t\t\t *",
            "\t\t\t * For hysteresis keep the pressure state",
            "\t\t\t * asserted for a second in which subsequent",
            "\t\t\t * pressure events can occur.",
            "\t\t\t */",
            "\t\t\tWRITE_ONCE(memcg->socket_pressure, jiffies + HZ);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "vmpressure_level, vmpressure_calc_level, vmpressure_event, vmpressure_work_fn, vmpressure",
          "description": "实现内存压力计算和事件触发逻辑，vmpressure_level计算压力等级，vmpressure_calc_level基于扫描与回收比确定压力值，vmpressure_event处理事件通知，vmpressure_work_fn执行压力分析并触发相应操作",
          "similarity": 0.5054160356521606
        },
        {
          "chunk_id": 2,
          "file_path": "mm/vmpressure.c",
          "start_line": 335,
          "end_line": 432,
          "content": [
            "void vmpressure_prio(gfp_t gfp, struct mem_cgroup *memcg, int prio)",
            "{",
            "\t/*",
            "\t * We only use prio for accounting critical level. For more info",
            "\t * see comment for vmpressure_level_critical_prio variable above.",
            "\t */",
            "\tif (prio > vmpressure_level_critical_prio)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * OK, the prio is below the threshold, updating vmpressure",
            "\t * information before shrinker dives into long shrinking of long",
            "\t * range vmscan. Passing scanned = vmpressure_win, reclaimed = 0",
            "\t * to the vmpressure() basically means that we signal 'critical'",
            "\t * level.",
            "\t */",
            "\tvmpressure(gfp, memcg, true, vmpressure_win, 0);",
            "}",
            "int vmpressure_register_event(struct mem_cgroup *memcg,",
            "\t\t\t      struct eventfd_ctx *eventfd, const char *args)",
            "{",
            "\tstruct vmpressure *vmpr = memcg_to_vmpressure(memcg);",
            "\tstruct vmpressure_event *ev;",
            "\tenum vmpressure_modes mode = VMPRESSURE_NO_PASSTHROUGH;",
            "\tenum vmpressure_levels level;",
            "\tchar *spec, *spec_orig;",
            "\tchar *token;",
            "\tint ret = 0;",
            "",
            "\tspec_orig = spec = kstrndup(args, MAX_VMPRESSURE_ARGS_LEN, GFP_KERNEL);",
            "\tif (!spec)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/* Find required level */",
            "\ttoken = strsep(&spec, \",\");",
            "\tret = match_string(vmpressure_str_levels, VMPRESSURE_NUM_LEVELS, token);",
            "\tif (ret < 0)",
            "\t\tgoto out;",
            "\tlevel = ret;",
            "",
            "\t/* Find optional mode */",
            "\ttoken = strsep(&spec, \",\");",
            "\tif (token) {",
            "\t\tret = match_string(vmpressure_str_modes, VMPRESSURE_NUM_MODES, token);",
            "\t\tif (ret < 0)",
            "\t\t\tgoto out;",
            "\t\tmode = ret;",
            "\t}",
            "",
            "\tev = kzalloc(sizeof(*ev), GFP_KERNEL);",
            "\tif (!ev) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tev->efd = eventfd;",
            "\tev->level = level;",
            "\tev->mode = mode;",
            "",
            "\tmutex_lock(&vmpr->events_lock);",
            "\tlist_add(&ev->node, &vmpr->events);",
            "\tmutex_unlock(&vmpr->events_lock);",
            "\tret = 0;",
            "out:",
            "\tkfree(spec_orig);",
            "\treturn ret;",
            "}",
            "void vmpressure_unregister_event(struct mem_cgroup *memcg,",
            "\t\t\t\t struct eventfd_ctx *eventfd)",
            "{",
            "\tstruct vmpressure *vmpr = memcg_to_vmpressure(memcg);",
            "\tstruct vmpressure_event *ev;",
            "",
            "\tmutex_lock(&vmpr->events_lock);",
            "\tlist_for_each_entry(ev, &vmpr->events, node) {",
            "\t\tif (ev->efd != eventfd)",
            "\t\t\tcontinue;",
            "\t\tlist_del(&ev->node);",
            "\t\tkfree(ev);",
            "\t\tbreak;",
            "\t}",
            "\tmutex_unlock(&vmpr->events_lock);",
            "}",
            "void vmpressure_init(struct vmpressure *vmpr)",
            "{",
            "\tspin_lock_init(&vmpr->sr_lock);",
            "\tmutex_init(&vmpr->events_lock);",
            "\tINIT_LIST_HEAD(&vmpr->events);",
            "\tINIT_WORK(&vmpr->work, vmpressure_work_fn);",
            "}",
            "void vmpressure_cleanup(struct vmpressure *vmpr)",
            "{",
            "\t/*",
            "\t * Make sure there is no pending work before eventfd infrastructure",
            "\t * goes away.",
            "\t */",
            "\tflush_work(&vmpr->work);",
            "}"
          ],
          "function_name": "vmpressure_prio, vmpressure_register_event, vmpressure_unregister_event, vmpressure_init, vmpressure_cleanup",
          "description": "提供压力优先级处理、事件注册注销及资源初始化清理功能，vmpressure_prio根据优先级触发压力检测，注册事件接口用于订阅压力状态变化，初始化函数配置锁和工作队列，清理函数确保资源正确释放",
          "similarity": 0.4660288691520691
        },
        {
          "chunk_id": 0,
          "file_path": "mm/vmpressure.c",
          "start_line": 1,
          "end_line": 110,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Linux VM pressure",
            " *",
            " * Copyright 2012 Linaro Ltd.",
            " *\t\t  Anton Vorontsov <anton.vorontsov@linaro.org>",
            " *",
            " * Based on ideas from Andrew Morton, David Rientjes, KOSAKI Motohiro,",
            " * Leonid Moiseichuk, Mel Gorman, Minchan Kim and Pekka Enberg.",
            " */",
            "",
            "#include <linux/cgroup.h>",
            "#include <linux/fs.h>",
            "#include <linux/log2.h>",
            "#include <linux/sched.h>",
            "#include <linux/mm.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/eventfd.h>",
            "#include <linux/slab.h>",
            "#include <linux/swap.h>",
            "#include <linux/printk.h>",
            "#include <linux/vmpressure.h>",
            "",
            "/*",
            " * The window size (vmpressure_win) is the number of scanned pages before",
            " * we try to analyze scanned/reclaimed ratio. So the window is used as a",
            " * rate-limit tunable for the \"low\" level notification, and also for",
            " * averaging the ratio for medium/critical levels. Using small window",
            " * sizes can cause lot of false positives, but too big window size will",
            " * delay the notifications.",
            " *",
            " * As the vmscan reclaimer logic works with chunks which are multiple of",
            " * SWAP_CLUSTER_MAX, it makes sense to use it for the window size as well.",
            " *",
            " * TODO: Make the window size depend on machine size, as we do for vmstat",
            " * thresholds. Currently we set it to 512 pages (2MB for 4KB pages).",
            " */",
            "static const unsigned long vmpressure_win = SWAP_CLUSTER_MAX * 16;",
            "",
            "/*",
            " * These thresholds are used when we account memory pressure through",
            " * scanned/reclaimed ratio. The current values were chosen empirically. In",
            " * essence, they are percents: the higher the value, the more number",
            " * unsuccessful reclaims there were.",
            " */",
            "static const unsigned int vmpressure_level_med = 60;",
            "static const unsigned int vmpressure_level_critical = 95;",
            "",
            "/*",
            " * When there are too little pages left to scan, vmpressure() may miss the",
            " * critical pressure as number of pages will be less than \"window size\".",
            " * However, in that case the vmscan priority will raise fast as the",
            " * reclaimer will try to scan LRUs more deeply.",
            " *",
            " * The vmscan logic considers these special priorities:",
            " *",
            " * prio == DEF_PRIORITY (12): reclaimer starts with that value",
            " * prio <= DEF_PRIORITY - 2 : kswapd becomes somewhat overwhelmed",
            " * prio == 0                : close to OOM, kernel scans every page in an lru",
            " *",
            " * Any value in this range is acceptable for this tunable (i.e. from 12 to",
            " * 0). Current value for the vmpressure_level_critical_prio is chosen",
            " * empirically, but the number, in essence, means that we consider",
            " * critical level when scanning depth is ~10% of the lru size (vmscan",
            " * scans 'lru_size >> prio' pages, so it is actually 12.5%, or one",
            " * eights).",
            " */",
            "static const unsigned int vmpressure_level_critical_prio = ilog2(100 / 10);",
            "",
            "static struct vmpressure *work_to_vmpressure(struct work_struct *work)",
            "{",
            "\treturn container_of(work, struct vmpressure, work);",
            "}",
            "",
            "static struct vmpressure *vmpressure_parent(struct vmpressure *vmpr)",
            "{",
            "\tstruct mem_cgroup *memcg = vmpressure_to_memcg(vmpr);",
            "",
            "\tmemcg = parent_mem_cgroup(memcg);",
            "\tif (!memcg)",
            "\t\treturn NULL;",
            "\treturn memcg_to_vmpressure(memcg);",
            "}",
            "",
            "enum vmpressure_levels {",
            "\tVMPRESSURE_LOW = 0,",
            "\tVMPRESSURE_MEDIUM,",
            "\tVMPRESSURE_CRITICAL,",
            "\tVMPRESSURE_NUM_LEVELS,",
            "};",
            "",
            "enum vmpressure_modes {",
            "\tVMPRESSURE_NO_PASSTHROUGH = 0,",
            "\tVMPRESSURE_HIERARCHY,",
            "\tVMPRESSURE_LOCAL,",
            "\tVMPRESSURE_NUM_MODES,",
            "};",
            "",
            "static const char * const vmpressure_str_levels[] = {",
            "\t[VMPRESSURE_LOW] = \"low\",",
            "\t[VMPRESSURE_MEDIUM] = \"medium\",",
            "\t[VMPRESSURE_CRITICAL] = \"critical\",",
            "};",
            "",
            "static const char * const vmpressure_str_modes[] = {",
            "\t[VMPRESSURE_NO_PASSTHROUGH] = \"default\",",
            "\t[VMPRESSURE_HIERARCHY] = \"hierarchy\",",
            "\t[VMPRESSURE_LOCAL] = \"local\",",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义内存压力监控的相关常量和枚举类型，其中vmpressure_win设置扫描窗口大小，vmpressure_level_med和vmpressure_level_critical定义压力阈值，枚举类型表示压力等级和模式，用于后续压力检测逻辑",
          "similarity": 0.4572879672050476
        }
      ]
    },
    {
      "source_file": "mm/vmalloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:32:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmalloc.c`\n\n---\n\n# vmalloc.c 技术文档\n\n## 1. 文件概述\n\n`vmalloc.c` 是 Linux 内核中实现虚拟内存分配（vmalloc）机制的核心源文件。该文件提供了在内核虚拟地址空间中非连续物理页映射为连续虚拟地址的功能，主要用于分配大块内存、I/O 映射（如 `ioremap`）以及需要页表特殊属性（如不可执行、缓存控制等）的场景。与 `kmalloc` 不同，`vmalloc` 分配的内存物理上不连续，但虚拟地址连续，适用于大内存分配或硬件寄存器映射。\n\n## 2. 核心功能\n\n### 主要函数\n- `is_vmalloc_addr(const void *x)`：判断给定地址是否位于 vmalloc 区域。\n- `vmap_page_range(unsigned long addr, unsigned long end, phys_addr_t phys_addr, pgprot_t prot)`：将指定物理地址范围映射到内核虚拟地址空间，支持普通页和大页。\n- `ioremap_page_range(...)`：用于 I/O 内存重映射（代码片段未完整展示）。\n- `vmap_range_noflush(...)`：执行实际的页表填充操作，不触发 TLB 刷新。\n- `vmap_pte_range`, `vmap_pmd_range`, `vmap_pud_range`, `vmap_p4d_range`：逐级填充页表项的辅助函数。\n- `vmap_try_huge_*` 系列函数（如 `vmap_try_huge_pmd`）：尝试使用大页（huge page）进行映射以提升性能。\n\n### 主要数据结构\n- `struct vfree_deferred`：用于延迟释放 vmalloc 内存的 per-CPU 工作队列结构。\n- `ioremap_max_page_shift`：控制 I/O 映射时允许的最大页面大小（受 `nohugeiomap` 启动参数影响）。\n- `vmap_allow_huge`：控制 vmalloc 是否允许使用大页（受 `nohugevmalloc` 启动参数影响）。\n\n## 3. 关键实现\n\n### 大页（Huge Page）支持\n- 通过 `CONFIG_HAVE_ARCH_HUGE_VMAP` 和 `CONFIG_HAVE_ARCH_HUGE_VMALLOC` 配置选项启用架构相关的大页映射能力。\n- 在页表填充过程中（如 `vmap_pmd_range`），优先尝试使用 PMD/PUD/P4D 级别的大页映射（通过 `vmap_try_huge_pmd` 等函数），前提是：\n  - 地址和物理地址对齐；\n  - 请求区域大小等于对应层级的大页尺寸；\n  - 架构支持该级别的大页（通过 `arch_vmap_*_supported` 判断）；\n  - 当前页表项未被占用或可安全释放下级页表。\n- 启动参数 `nohugeiomap` 和 `nohugevmalloc` 可分别禁用 I/O 映射和通用 vmalloc 的大页功能。\n\n### 页表操作与跟踪\n- 使用 `_track` 后缀的页表分配函数（如 `pte_alloc_kernel_track`）配合 `pgtbl_mod_mask` 标记修改的页表层级，便于后续同步（如 `arch_sync_kernel_mappings`）。\n- 映射完成后调用 `flush_cache_vmap` 确保缓存一致性，并集成 KMSAN（Kernel Memory Sanitizer）支持。\n\n### 安全与调试\n- 使用 `kasan_reset_tag` 处理 KASAN 的内存标记，确保地址比较正确。\n- 通过 `BUG_ON` 检查页表项是否为空，防止覆盖已有映射。\n- 支持 `kmemleak` 内存泄漏检测。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/highmem.h>`、`<linux/pfn.h>` 等提供基础内存操作。\n- **体系结构相关接口**：通过 `asm/tlbflush.h`、`asm/shmparam.h` 及 `arch_*` 函数（如 `arch_make_huge_pte`）适配不同 CPU 架构。\n- **内核基础设施**：\n  - RCU（`<linux/rcupdate.h>`）用于安全遍历；\n  - Per-CPU 变量（`DEFINE_PER_CPU`）实现无锁延迟释放；\n  - 工作队列（`work_struct`）处理异步释放；\n  - 调试工具（`debugobjects`、`kallsyms`、`trace/events/vmalloc.h`）。\n- **安全特性**：集成 KASAN、KMSAN、`set_memory.h`（页属性设置）等。\n\n## 5. 使用场景\n\n- **内核模块加载**：模块的代码和数据通常通过 `vmalloc` 分配。\n- **大内存分配**：当所需内存超过 `kmalloc` 的限制（通常几 MB）时使用。\n- **设备 I/O 映射**：通过 `ioremap` 将设备寄存器映射到内核地址空间，底层调用 `ioremap_page_range`。\n- **动态内核数据结构**：如网络协议栈的某些缓冲区、文件系统元数据缓存等。\n- **安全隔离**：为敏感数据分配具有特殊页属性（如不可执行 NX）的内存区域。",
      "similarity": 0.5704320669174194,
      "chunks": [
        {
          "chunk_id": 8,
          "file_path": "mm/vmalloc.c",
          "start_line": 1465,
          "end_line": 1624,
          "content": [
            "static __always_inline bool",
            "is_within_this_va(struct vmap_area *va, unsigned long size,",
            "\tunsigned long align, unsigned long vstart)",
            "{",
            "\tunsigned long nva_start_addr;",
            "",
            "\tif (va->va_start > vstart)",
            "\t\tnva_start_addr = ALIGN(va->va_start, align);",
            "\telse",
            "\t\tnva_start_addr = ALIGN(vstart, align);",
            "",
            "\t/* Can be overflowed due to big size or alignment. */",
            "\tif (nva_start_addr + size < nva_start_addr ||",
            "\t\t\tnva_start_addr < vstart)",
            "\t\treturn false;",
            "",
            "\treturn (nva_start_addr + size <= va->va_end);",
            "}",
            "static void",
            "find_vmap_lowest_match_check(struct rb_root *root, struct list_head *head,",
            "\t\t\t     unsigned long size, unsigned long align)",
            "{",
            "\tstruct vmap_area *va_1, *va_2;",
            "\tunsigned long vstart;",
            "\tunsigned int rnd;",
            "",
            "\tget_random_bytes(&rnd, sizeof(rnd));",
            "\tvstart = VMALLOC_START + rnd;",
            "",
            "\tva_1 = find_vmap_lowest_match(root, size, align, vstart, false);",
            "\tva_2 = find_vmap_lowest_linear_match(head, size, align, vstart);",
            "",
            "\tif (va_1 != va_2)",
            "\t\tpr_emerg(\"not lowest: t: 0x%p, l: 0x%p, v: 0x%lx\\n\",",
            "\t\t\tva_1, va_2, vstart);",
            "}",
            "static __always_inline enum fit_type",
            "classify_va_fit_type(struct vmap_area *va,",
            "\tunsigned long nva_start_addr, unsigned long size)",
            "{",
            "\tenum fit_type type;",
            "",
            "\t/* Check if it is within VA. */",
            "\tif (nva_start_addr < va->va_start ||",
            "\t\t\tnva_start_addr + size > va->va_end)",
            "\t\treturn NOTHING_FIT;",
            "",
            "\t/* Now classify. */",
            "\tif (va->va_start == nva_start_addr) {",
            "\t\tif (va->va_end == nva_start_addr + size)",
            "\t\t\ttype = FL_FIT_TYPE;",
            "\t\telse",
            "\t\t\ttype = LE_FIT_TYPE;",
            "\t} else if (va->va_end == nva_start_addr + size) {",
            "\t\ttype = RE_FIT_TYPE;",
            "\t} else {",
            "\t\ttype = NE_FIT_TYPE;",
            "\t}",
            "",
            "\treturn type;",
            "}",
            "static __always_inline int",
            "va_clip(struct rb_root *root, struct list_head *head,",
            "\t\tstruct vmap_area *va, unsigned long nva_start_addr,",
            "\t\tunsigned long size)",
            "{",
            "\tstruct vmap_area *lva = NULL;",
            "\tenum fit_type type = classify_va_fit_type(va, nva_start_addr, size);",
            "",
            "\tif (type == FL_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * No need to split VA, it fully fits.",
            "\t\t *",
            "\t\t * |               |",
            "\t\t * V      NVA      V",
            "\t\t * |---------------|",
            "\t\t */",
            "\t\tunlink_va_augment(va, root);",
            "\t\tkmem_cache_free(vmap_area_cachep, va);",
            "\t} else if (type == LE_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * Split left edge of fit VA.",
            "\t\t *",
            "\t\t * |       |",
            "\t\t * V  NVA  V   R",
            "\t\t * |-------|-------|",
            "\t\t */",
            "\t\tva->va_start += size;",
            "\t} else if (type == RE_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * Split right edge of fit VA.",
            "\t\t *",
            "\t\t *         |       |",
            "\t\t *     L   V  NVA  V",
            "\t\t * |-------|-------|",
            "\t\t */",
            "\t\tva->va_end = nva_start_addr;",
            "\t} else if (type == NE_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * Split no edge of fit VA.",
            "\t\t *",
            "\t\t *     |       |",
            "\t\t *   L V  NVA  V R",
            "\t\t * |---|-------|---|",
            "\t\t */",
            "\t\tlva = __this_cpu_xchg(ne_fit_preload_node, NULL);",
            "\t\tif (unlikely(!lva)) {",
            "\t\t\t/*",
            "\t\t\t * For percpu allocator we do not do any pre-allocation",
            "\t\t\t * and leave it as it is. The reason is it most likely",
            "\t\t\t * never ends up with NE_FIT_TYPE splitting. In case of",
            "\t\t\t * percpu allocations offsets and sizes are aligned to",
            "\t\t\t * fixed align request, i.e. RE_FIT_TYPE and FL_FIT_TYPE",
            "\t\t\t * are its main fitting cases.",
            "\t\t\t *",
            "\t\t\t * There are a few exceptions though, as an example it is",
            "\t\t\t * a first allocation (early boot up) when we have \"one\"",
            "\t\t\t * big free space that has to be split.",
            "\t\t\t *",
            "\t\t\t * Also we can hit this path in case of regular \"vmap\"",
            "\t\t\t * allocations, if \"this\" current CPU was not preloaded.",
            "\t\t\t * See the comment in alloc_vmap_area() why. If so, then",
            "\t\t\t * GFP_NOWAIT is used instead to get an extra object for",
            "\t\t\t * split purpose. That is rare and most time does not",
            "\t\t\t * occur.",
            "\t\t\t *",
            "\t\t\t * What happens if an allocation gets failed. Basically,",
            "\t\t\t * an \"overflow\" path is triggered to purge lazily freed",
            "\t\t\t * areas to free some memory, then, the \"retry\" path is",
            "\t\t\t * triggered to repeat one more time. See more details",
            "\t\t\t * in alloc_vmap_area() function.",
            "\t\t\t */",
            "\t\t\tlva = kmem_cache_alloc(vmap_area_cachep, GFP_NOWAIT);",
            "\t\t\tif (!lva)",
            "\t\t\t\treturn -1;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Build the remainder.",
            "\t\t */",
            "\t\tlva->va_start = va->va_start;",
            "\t\tlva->va_end = nva_start_addr;",
            "",
            "\t\t/*",
            "\t\t * Shrink this VA to remaining size.",
            "\t\t */",
            "\t\tva->va_start = nva_start_addr + size;",
            "\t} else {",
            "\t\treturn -1;",
            "\t}",
            "",
            "\tif (type != FL_FIT_TYPE) {",
            "\t\taugment_tree_propagate_from(va);",
            "",
            "\t\tif (lva)\t/* type == NE_FIT_TYPE */",
            "\t\t\tinsert_vmap_area_augment(lva, &va->rb_node, root, head);",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "is_within_this_va, find_vmap_lowest_match_check, classify_va_fit_type, va_clip",
          "description": "实现虚拟内存区域匹配算法，通过分类判断（FL/LE/RE/NE）进行地址裁剪和区域分割，支持不同场景下的内存分配策略选择",
          "similarity": 0.6016196012496948
        },
        {
          "chunk_id": 7,
          "file_path": "mm/vmalloc.c",
          "start_line": 1208,
          "end_line": 1309,
          "content": [
            "static __always_inline void",
            "link_va(struct vmap_area *va, struct rb_root *root,",
            "\tstruct rb_node *parent, struct rb_node **link,",
            "\tstruct list_head *head)",
            "{",
            "\t__link_va(va, root, parent, link, head, false);",
            "}",
            "static __always_inline void",
            "link_va_augment(struct vmap_area *va, struct rb_root *root,",
            "\tstruct rb_node *parent, struct rb_node **link,",
            "\tstruct list_head *head)",
            "{",
            "\t__link_va(va, root, parent, link, head, true);",
            "}",
            "static __always_inline void",
            "__unlink_va(struct vmap_area *va, struct rb_root *root, bool augment)",
            "{",
            "\tif (WARN_ON(RB_EMPTY_NODE(&va->rb_node)))",
            "\t\treturn;",
            "",
            "\tif (augment)",
            "\t\trb_erase_augmented(&va->rb_node,",
            "\t\t\troot, &free_vmap_area_rb_augment_cb);",
            "\telse",
            "\t\trb_erase(&va->rb_node, root);",
            "",
            "\tlist_del_init(&va->list);",
            "\tRB_CLEAR_NODE(&va->rb_node);",
            "}",
            "static __always_inline void",
            "unlink_va(struct vmap_area *va, struct rb_root *root)",
            "{",
            "\t__unlink_va(va, root, false);",
            "}",
            "static __always_inline void",
            "unlink_va_augment(struct vmap_area *va, struct rb_root *root)",
            "{",
            "\t__unlink_va(va, root, true);",
            "}",
            "static __always_inline unsigned long",
            "compute_subtree_max_size(struct vmap_area *va)",
            "{",
            "\treturn max3(va_size(va),",
            "\t\tget_subtree_max_size(va->rb_node.rb_left),",
            "\t\tget_subtree_max_size(va->rb_node.rb_right));",
            "}",
            "static void",
            "augment_tree_propagate_check(void)",
            "{",
            "\tstruct vmap_area *va;",
            "\tunsigned long computed_size;",
            "",
            "\tlist_for_each_entry(va, &free_vmap_area_list, list) {",
            "\t\tcomputed_size = compute_subtree_max_size(va);",
            "\t\tif (computed_size != va->subtree_max_size)",
            "\t\t\tpr_emerg(\"tree is corrupted: %lu, %lu\\n\",",
            "\t\t\t\tva_size(va), va->subtree_max_size);",
            "\t}",
            "}",
            "static __always_inline void",
            "augment_tree_propagate_from(struct vmap_area *va)",
            "{",
            "\t/*",
            "\t * Populate the tree from bottom towards the root until",
            "\t * the calculated maximum available size of checked node",
            "\t * is equal to its current one.",
            "\t */",
            "\tfree_vmap_area_rb_augment_cb_propagate(&va->rb_node, NULL);",
            "",
            "#if DEBUG_AUGMENT_PROPAGATE_CHECK",
            "\taugment_tree_propagate_check();",
            "#endif",
            "}",
            "static void",
            "insert_vmap_area(struct vmap_area *va,",
            "\tstruct rb_root *root, struct list_head *head)",
            "{",
            "\tstruct rb_node **link;",
            "\tstruct rb_node *parent;",
            "",
            "\tlink = find_va_links(va, root, NULL, &parent);",
            "\tif (link)",
            "\t\tlink_va(va, root, parent, link, head);",
            "}",
            "static void",
            "insert_vmap_area_augment(struct vmap_area *va,",
            "\tstruct rb_node *from, struct rb_root *root,",
            "\tstruct list_head *head)",
            "{",
            "\tstruct rb_node **link;",
            "\tstruct rb_node *parent;",
            "",
            "\tif (from)",
            "\t\tlink = find_va_links(va, NULL, from, &parent);",
            "\telse",
            "\t\tlink = find_va_links(va, root, NULL, &parent);",
            "",
            "\tif (link) {",
            "\t\tlink_va_augment(va, root, parent, link, head);",
            "\t\taugment_tree_propagate_from(va);",
            "\t}",
            "}"
          ],
          "function_name": "link_va, link_va_augment, __unlink_va, unlink_va, unlink_va_augment, compute_subtree_max_size, augment_tree_propagate_check, augment_tree_propagate_from, insert_vmap_area, insert_vmap_area_augment",
          "description": "实现基于红黑树的虚拟内存区域动态管理，包含节点插入/删除、子树最大尺寸计算及传播机制，确保树结构平衡性与查询效率",
          "similarity": 0.585816502571106
        },
        {
          "chunk_id": 6,
          "file_path": "mm/vmalloc.c",
          "start_line": 691,
          "end_line": 822,
          "content": [
            "int vm_area_map_pages(struct vm_struct *area, unsigned long start,",
            "\t\t      unsigned long end, struct page **pages)",
            "{",
            "\tint err;",
            "",
            "\terr = check_sparse_vm_area(area, start, end);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\treturn vmap_pages_range(start, end, PAGE_KERNEL, pages, PAGE_SHIFT);",
            "}",
            "void vm_area_unmap_pages(struct vm_struct *area, unsigned long start,",
            "\t\t\t unsigned long end)",
            "{",
            "\tif (check_sparse_vm_area(area, start, end))",
            "\t\treturn;",
            "",
            "\tvunmap_range(start, end);",
            "}",
            "int is_vmalloc_or_module_addr(const void *x)",
            "{",
            "\t/*",
            "\t * ARM, x86-64 and sparc64 put modules in a special place,",
            "\t * and fall back on vmalloc() if that fails. Others",
            "\t * just put it in the vmalloc space.",
            "\t */",
            "#if defined(CONFIG_MODULES) && defined(MODULES_VADDR)",
            "\tunsigned long addr = (unsigned long)kasan_reset_tag(x);",
            "\tif (addr >= MODULES_VADDR && addr < MODULES_END)",
            "\t\treturn 1;",
            "#endif",
            "\treturn is_vmalloc_addr(x);",
            "}",
            "unsigned long vmalloc_to_pfn(const void *vmalloc_addr)",
            "{",
            "\treturn page_to_pfn(vmalloc_to_page(vmalloc_addr));",
            "}",
            "static inline unsigned int",
            "addr_to_node_id(unsigned long addr)",
            "{",
            "\treturn (addr / vmap_zone_size) % nr_vmap_nodes;",
            "}",
            "static unsigned int",
            "encode_vn_id(unsigned int node_id)",
            "{",
            "\t/* Can store U8_MAX [0:254] nodes. */",
            "\tif (node_id < nr_vmap_nodes)",
            "\t\treturn (node_id + 1) << BITS_PER_BYTE;",
            "",
            "\t/* Warn and no node encoded. */",
            "\tWARN_ONCE(1, \"Encode wrong node id (%u)\\n\", node_id);",
            "\treturn 0;",
            "}",
            "static unsigned int",
            "decode_vn_id(unsigned int val)",
            "{",
            "\tunsigned int node_id = (val >> BITS_PER_BYTE) - 1;",
            "",
            "\t/* Can store U8_MAX [0:254] nodes. */",
            "\tif (node_id < nr_vmap_nodes)",
            "\t\treturn node_id;",
            "",
            "\t/* If it was _not_ zero, warn. */",
            "\tWARN_ONCE(node_id != UINT_MAX,",
            "\t\t\"Decode wrong node id (%d)\\n\", node_id);",
            "",
            "\treturn nr_vmap_nodes;",
            "}",
            "static bool",
            "is_vn_id_valid(unsigned int node_id)",
            "{",
            "\tif (node_id < nr_vmap_nodes)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "static __always_inline unsigned long",
            "va_size(struct vmap_area *va)",
            "{",
            "\treturn (va->va_end - va->va_start);",
            "}",
            "static __always_inline unsigned long",
            "get_subtree_max_size(struct rb_node *node)",
            "{",
            "\tstruct vmap_area *va;",
            "",
            "\tva = rb_entry_safe(node, struct vmap_area, rb_node);",
            "\treturn va ? va->subtree_max_size : 0;",
            "}",
            "unsigned long vmalloc_nr_pages(void)",
            "{",
            "\treturn atomic_long_read(&nr_vmalloc_pages);",
            "}",
            "static __always_inline void",
            "__link_va(struct vmap_area *va, struct rb_root *root,",
            "\tstruct rb_node *parent, struct rb_node **link,",
            "\tstruct list_head *head, bool augment)",
            "{",
            "\t/*",
            "\t * VA is still not in the list, but we can",
            "\t * identify its future previous list_head node.",
            "\t */",
            "\tif (likely(parent)) {",
            "\t\thead = &rb_entry(parent, struct vmap_area, rb_node)->list;",
            "\t\tif (&parent->rb_right != link)",
            "\t\t\thead = head->prev;",
            "\t}",
            "",
            "\t/* Insert to the rb-tree */",
            "\trb_link_node(&va->rb_node, parent, link);",
            "\tif (augment) {",
            "\t\t/*",
            "\t\t * Some explanation here. Just perform simple insertion",
            "\t\t * to the tree. We do not set va->subtree_max_size to",
            "\t\t * its current size before calling rb_insert_augmented().",
            "\t\t * It is because we populate the tree from the bottom",
            "\t\t * to parent levels when the node _is_ in the tree.",
            "\t\t *",
            "\t\t * Therefore we set subtree_max_size to zero after insertion,",
            "\t\t * to let __augment_tree_propagate_from() puts everything to",
            "\t\t * the correct order later on.",
            "\t\t */",
            "\t\trb_insert_augmented(&va->rb_node,",
            "\t\t\troot, &free_vmap_area_rb_augment_cb);",
            "\t\tva->subtree_max_size = 0;",
            "\t} else {",
            "\t\trb_insert_color(&va->rb_node, root);",
            "\t}",
            "",
            "\t/* Address-sort this list */",
            "\tlist_add(&va->list, head);",
            "}"
          ],
          "function_name": "vm_area_map_pages, vm_area_unmap_pages, is_vmalloc_or_module_addr, vmalloc_to_pfn, addr_to_node_id, encode_vn_id, decode_vn_id, is_vn_id_valid, va_size, get_subtree_max_size, vmalloc_nr_pages, __link_va",
          "description": "提供虚拟内存区域管理接口，包含区域有效性校验、地址转换、节点ID编码解码及基于红黑树的VA链表操作，用于跟踪和管理VMALLOC空间",
          "similarity": 0.5644229650497437
        },
        {
          "chunk_id": 9,
          "file_path": "mm/vmalloc.c",
          "start_line": 1728,
          "end_line": 1843,
          "content": [
            "static unsigned long",
            "va_alloc(struct vmap_area *va,",
            "\t\tstruct rb_root *root, struct list_head *head,",
            "\t\tunsigned long size, unsigned long align,",
            "\t\tunsigned long vstart, unsigned long vend)",
            "{",
            "\tunsigned long nva_start_addr;",
            "\tint ret;",
            "",
            "\tif (va->va_start > vstart)",
            "\t\tnva_start_addr = ALIGN(va->va_start, align);",
            "\telse",
            "\t\tnva_start_addr = ALIGN(vstart, align);",
            "",
            "\t/* Check the \"vend\" restriction. */",
            "\tif (nva_start_addr + size > vend)",
            "\t\treturn vend;",
            "",
            "\t/* Update the free vmap_area. */",
            "\tret = va_clip(root, head, va, nva_start_addr, size);",
            "\tif (WARN_ON_ONCE(ret))",
            "\t\treturn vend;",
            "",
            "\treturn nva_start_addr;",
            "}",
            "static __always_inline unsigned long",
            "__alloc_vmap_area(struct rb_root *root, struct list_head *head,",
            "\tunsigned long size, unsigned long align,",
            "\tunsigned long vstart, unsigned long vend)",
            "{",
            "\tbool adjust_search_size = true;",
            "\tunsigned long nva_start_addr;",
            "\tstruct vmap_area *va;",
            "",
            "\t/*",
            "\t * Do not adjust when:",
            "\t *   a) align <= PAGE_SIZE, because it does not make any sense.",
            "\t *      All blocks(their start addresses) are at least PAGE_SIZE",
            "\t *      aligned anyway;",
            "\t *   b) a short range where a requested size corresponds to exactly",
            "\t *      specified [vstart:vend] interval and an alignment > PAGE_SIZE.",
            "\t *      With adjusted search length an allocation would not succeed.",
            "\t */",
            "\tif (align <= PAGE_SIZE || (align > PAGE_SIZE && (vend - vstart) == size))",
            "\t\tadjust_search_size = false;",
            "",
            "\tva = find_vmap_lowest_match(root, size, align, vstart, adjust_search_size);",
            "\tif (unlikely(!va))",
            "\t\treturn vend;",
            "",
            "\tnva_start_addr = va_alloc(va, root, head, size, align, vstart, vend);",
            "\tif (nva_start_addr == vend)",
            "\t\treturn vend;",
            "",
            "#if DEBUG_AUGMENT_LOWEST_MATCH_CHECK",
            "\tfind_vmap_lowest_match_check(root, head, size, align);",
            "#endif",
            "",
            "\treturn nva_start_addr;",
            "}",
            "static void free_vmap_area(struct vmap_area *va)",
            "{",
            "\tstruct vmap_node *vn = addr_to_node(va->va_start);",
            "",
            "\t/*",
            "\t * Remove from the busy tree/list.",
            "\t */",
            "\tspin_lock(&vn->busy.lock);",
            "\tunlink_va(va, &vn->busy.root);",
            "\tspin_unlock(&vn->busy.lock);",
            "",
            "\t/*",
            "\t * Insert/Merge it back to the free tree/list.",
            "\t */",
            "\tspin_lock(&free_vmap_area_lock);",
            "\tmerge_or_add_vmap_area_augment(va, &free_vmap_area_root, &free_vmap_area_list);",
            "\tspin_unlock(&free_vmap_area_lock);",
            "}",
            "static inline void",
            "preload_this_cpu_lock(spinlock_t *lock, gfp_t gfp_mask, int node)",
            "{",
            "\tstruct vmap_area *va = NULL;",
            "",
            "\t/*",
            "\t * Preload this CPU with one extra vmap_area object. It is used",
            "\t * when fit type of free area is NE_FIT_TYPE. It guarantees that",
            "\t * a CPU that does an allocation is preloaded.",
            "\t *",
            "\t * We do it in non-atomic context, thus it allows us to use more",
            "\t * permissive allocation masks to be more stable under low memory",
            "\t * condition and high memory pressure.",
            "\t */",
            "\tif (!this_cpu_read(ne_fit_preload_node))",
            "\t\tva = kmem_cache_alloc_node(vmap_area_cachep, gfp_mask, node);",
            "",
            "\tspin_lock(lock);",
            "",
            "\tif (va && __this_cpu_cmpxchg(ne_fit_preload_node, NULL, va))",
            "\t\tkmem_cache_free(vmap_area_cachep, va);",
            "}",
            "static bool",
            "node_pool_add_va(struct vmap_node *n, struct vmap_area *va)",
            "{",
            "\tstruct vmap_pool *vp;",
            "",
            "\tvp = size_to_va_pool(n, va_size(va));",
            "\tif (!vp)",
            "\t\treturn false;",
            "",
            "\tspin_lock(&n->pool_lock);",
            "\tlist_add(&va->list, &vp->head);",
            "\tWRITE_ONCE(vp->len, vp->len + 1);",
            "\tspin_unlock(&n->pool_lock);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "va_alloc, __alloc_vmap_area, free_vmap_area, preload_this_cpu_lock, node_pool_add_va",
          "description": "实现虚拟内存区域的分配/回收核心逻辑，包含智能搜索算法、节点池管理及预加载机制，确保高并发场景下的内存分配稳定性",
          "similarity": 0.5632550716400146
        },
        {
          "chunk_id": 0,
          "file_path": "mm/vmalloc.c",
          "start_line": 1,
          "end_line": 54,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  Copyright (C) 1993  Linus Torvalds",
            " *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999",
            " *  SMP-safe vmalloc/vfree/ioremap, Tigran Aivazian <tigran@veritas.com>, May 2000",
            " *  Major rework to support vmap/vunmap, Christoph Hellwig, SGI, August 2002",
            " *  Numa awareness, Christoph Lameter, SGI, June 2005",
            " *  Improving global KVA allocator, Uladzislau Rezki, Sony, May 2019",
            " */",
            "",
            "#include <linux/vmalloc.h>",
            "#include <linux/mm.h>",
            "#include <linux/module.h>",
            "#include <linux/highmem.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/list.h>",
            "#include <linux/notifier.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/xarray.h>",
            "#include <linux/io.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/pfn.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/atomic.h>",
            "#include <linux/compiler.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/llist.h>",
            "#include <linux/uio.h>",
            "#include <linux/bitops.h>",
            "#include <linux/rbtree_augmented.h>",
            "#include <linux/overflow.h>",
            "#include <linux/pgtable.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/sched/mm.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/shmparam.h>",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/vmalloc.h>",
            "",
            "#include \"internal.h\"",
            "#include \"pgalloc-track.h\"",
            "",
            "#ifdef CONFIG_HAVE_ARCH_HUGE_VMAP",
            "static unsigned int __ro_after_init ioremap_max_page_shift = BITS_PER_LONG - 1;",
            ""
          ],
          "function_name": null,
          "description": "包含虚拟内存分配所需头文件并定义了用于控制巨页（Huge Page）大小的全局变量ioremap_max_page_shift，为后续虚拟内存管理和大页支持提供基础框架。",
          "similarity": 0.5520913600921631
        }
      ]
    },
    {
      "source_file": "mm/memory-tiers.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:41:40\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memory-tiers.c`\n\n---\n\n# memory-tiers.c 技术文档\n\n## 1. 文件概述\n\n`memory-tiers.c` 是 Linux 内核中实现 **内存层级（Memory Tiering）** 功能的核心模块。该文件负责根据抽象距离（abstract distance, adistance）对 NUMA 节点进行分层管理，支持将不同性能特性的内存（如 DRAM、PMEM、HBM 等）组织成层级结构，并为页面迁移（demotion/promotion）和 NUMA 平衡提供基础支持。通过 sysfs 暴露内存层级信息，便于用户空间监控和策略配置。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- `struct memory_tier`  \n  表示一个内存层级，包含：\n  - `adistance_start`：该层级的起始抽象距离（按 `MEMTIER_CHUNK_SIZE` 对齐）\n  - `memory_types`：属于该层级的所有内存设备类型（`memory_dev_type`）链表\n  - `lower_tier_mask`：所有更低层级（更高延迟/更低性能）节点的位掩码\n  - `dev`：对应的 sysfs 设备对象\n\n- `struct demotion_nodes`  \n  用于记录每个节点在页面降级（demotion）时的首选目标节点集合。\n\n- `struct node_memory_type_map`  \n  每个 NUMA 节点到其内存设备类型的映射及引用计数。\n\n- `node_demotion[]`（仅 `CONFIG_MIGRATION`）  \n  全局数组，存储每个节点的降级目标偏好。\n\n### 主要函数与接口\n\n- `find_create_memory_tier()`  \n  根据给定内存设备类型的抽象距离，查找或创建对应的 `memory_tier` 实例，并将其加入全局层级链表（按 `adistance_start` 升序排列）。\n\n- `__node_get_memory_tier()` / `node_is_toptier()`  \n  查询指定 NUMA 节点所属的内存层级；`node_is_toptier()` 判断节点是否属于顶层（最高性能）内存层级。\n\n- `node_get_allowed_targets()`  \n  获取指定节点在页面迁移时允许的目标节点集合（即其所在层级之下的所有节点）。\n\n- `next_demotion_node()`（未完整展示）  \n  返回从给定节点出发，在降级路径中的下一个目标节点 ID。\n\n- `folio_use_access_time()`（仅 `CONFIG_NUMA_BALANCING`）  \n  在启用内存层级模式的 NUMA 平衡中，判断是否将 folio 的 `_last_cpupid` 字段复用为访问时间戳（仅适用于非顶层内存节点）。\n\n- `nodelist_show()`  \n  sysfs 属性回调，输出当前内存层级包含的所有 NUMA 节点列表。\n\n## 3. 关键实现\n\n### 内存层级构建逻辑\n- 所有内存设备类型（`memory_dev_type`）通过其 `adistance` 值被归入特定层级。\n- 层级按 `adistance_start = round_down(adistance, MEMTIER_CHUNK_SIZE)` 分组，确保同一层级内设备具有相近的性能特征。\n- 全局链表 `memory_tiers` 维护层级顺序（从低 `adistance` 到高），反映从高性能到低性能的层级结构。\n\n### 层级间依赖关系\n- 每个 `memory_tier` 的 `lower_tier_mask` 记录了所有比它性能更低（`adistance` 更高）的层级所包含的节点集合，用于快速确定迁移目标范围。\n- 顶层层级由全局变量 `top_tier_adistance` 定义，通常对应最低 `adistance` 值的层级（如 CPU 本地 DRAM）。\n\n### RCU 与锁机制\n- 使用 `memory_tier_lock`（互斥锁）保护全局层级结构和设备注册。\n- 节点到层级的映射（`pgdat->memtier`）通过 RCU 机制更新和读取，确保在无锁路径（如页面访问）中的高效性。\n- `synchronize_rcu()` 用于在释放 `memory_tier` 前确保无并发 RCU 读者。\n\n### sysfs 集成\n- 每个 `memory_tier` 注册为 `memory_tiering` 子系统下的设备（`memory_tierX`）。\n- 通过 `nodelist` 属性暴露该层级包含的 NUMA 节点，格式为位图字符串（如 `\"0-3,8-11\"`）。\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/memory.h>`：NUMA 节点和内存管理基础\n  - `<linux/memory-tiers.h>`：内存层级公共接口定义\n  - `\"internal.h\"`：内部辅助函数\n- **可选依赖**：\n  - `CONFIG_NUMA_BALANCING`：提供 `folio_use_access_time()`，支持基于访问时间的页面迁移\n  - `CONFIG_MIGRATION`：提供 `node_demotion` 结构和 `next_demotion_node()` 等迁移相关功能\n- **子系统交互**：\n  - 与内存热插拔（`memory_hotplug`）协同，动态更新层级结构\n  - 为自动 NUMA 平衡（AutoNUMA）和页面迁移框架提供层级拓扑信息\n\n## 5. 使用场景\n\n- **异构内存系统管理**：在包含 DRAM、PMEM、CXL 内存等多类型内存的系统中，自动构建性能层级视图。\n- **智能页面迁移**：作为 `migrate_pages()` 和后台 demotion daemon 的决策依据，将冷页从高性能内存迁移到大容量低速内存。\n- **NUMA 负载均衡优化**：在 `CONFIG_NUMA_BALANCING` 启用时，结合访问时间戳和层级信息，优先将热页保留在顶层内存。\n- **用户空间监控与调优**：通过 `/sys/devices/memory_tiering/memory_tier*/nodelist` 查看各层级节点分布，辅助制定应用部署策略。",
      "similarity": 0.568032443523407,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/memory-tiers.c",
          "start_line": 357,
          "end_line": 472,
          "content": [
            "static void disable_all_demotion_targets(void)",
            "{",
            "\tstruct memory_tier *memtier;",
            "\tint node;",
            "",
            "\tfor_each_node_state(node, N_MEMORY) {",
            "\t\tnode_demotion[node].preferred = NODE_MASK_NONE;",
            "\t\t/*",
            "\t\t * We are holding memory_tier_lock, it is safe",
            "\t\t * to access pgda->memtier.",
            "\t\t */",
            "\t\tmemtier = __node_get_memory_tier(node);",
            "\t\tif (memtier)",
            "\t\t\tmemtier->lower_tier_mask = NODE_MASK_NONE;",
            "\t}",
            "\t/*",
            "\t * Ensure that the \"disable\" is visible across the system.",
            "\t * Readers will see either a combination of before+disable",
            "\t * state or disable+after.  They will never see before and",
            "\t * after state together.",
            "\t */",
            "\tsynchronize_rcu();",
            "}",
            "static void establish_demotion_targets(void)",
            "{",
            "\tstruct memory_tier *memtier;",
            "\tstruct demotion_nodes *nd;",
            "\tint target = NUMA_NO_NODE, node;",
            "\tint distance, best_distance;",
            "\tnodemask_t tier_nodes, lower_tier;",
            "",
            "\tlockdep_assert_held_once(&memory_tier_lock);",
            "",
            "\tif (!node_demotion)",
            "\t\treturn;",
            "",
            "\tdisable_all_demotion_targets();",
            "",
            "\tfor_each_node_state(node, N_MEMORY) {",
            "\t\tbest_distance = -1;",
            "\t\tnd = &node_demotion[node];",
            "",
            "\t\tmemtier = __node_get_memory_tier(node);",
            "\t\tif (!memtier || list_is_last(&memtier->list, &memory_tiers))",
            "\t\t\tcontinue;",
            "\t\t/*",
            "\t\t * Get the lower memtier to find the  demotion node list.",
            "\t\t */",
            "\t\tmemtier = list_next_entry(memtier, list);",
            "\t\ttier_nodes = get_memtier_nodemask(memtier);",
            "\t\t/*",
            "\t\t * find_next_best_node, use 'used' nodemask as a skip list.",
            "\t\t * Add all memory nodes except the selected memory tier",
            "\t\t * nodelist to skip list so that we find the best node from the",
            "\t\t * memtier nodelist.",
            "\t\t */",
            "\t\tnodes_andnot(tier_nodes, node_states[N_MEMORY], tier_nodes);",
            "",
            "\t\t/*",
            "\t\t * Find all the nodes in the memory tier node list of same best distance.",
            "\t\t * add them to the preferred mask. We randomly select between nodes",
            "\t\t * in the preferred mask when allocating pages during demotion.",
            "\t\t */",
            "\t\tdo {",
            "\t\t\ttarget = find_next_best_node(node, &tier_nodes);",
            "\t\t\tif (target == NUMA_NO_NODE)",
            "\t\t\t\tbreak;",
            "",
            "\t\t\tdistance = node_distance(node, target);",
            "\t\t\tif (distance == best_distance || best_distance == -1) {",
            "\t\t\t\tbest_distance = distance;",
            "\t\t\t\tnode_set(target, nd->preferred);",
            "\t\t\t} else {",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t} while (1);",
            "\t}",
            "\t/*",
            "\t * Promotion is allowed from a memory tier to higher",
            "\t * memory tier only if the memory tier doesn't include",
            "\t * compute. We want to skip promotion from a memory tier,",
            "\t * if any node that is part of the memory tier have CPUs.",
            "\t * Once we detect such a memory tier, we consider that tier",
            "\t * as top tiper from which promotion is not allowed.",
            "\t */",
            "\tlist_for_each_entry_reverse(memtier, &memory_tiers, list) {",
            "\t\ttier_nodes = get_memtier_nodemask(memtier);",
            "\t\tnodes_and(tier_nodes, node_states[N_CPU], tier_nodes);",
            "\t\tif (!nodes_empty(tier_nodes)) {",
            "\t\t\t/*",
            "\t\t\t * abstract distance below the max value of this memtier",
            "\t\t\t * is considered toptier.",
            "\t\t\t */",
            "\t\t\ttop_tier_adistance = memtier->adistance_start +",
            "\t\t\t\t\t\tMEMTIER_CHUNK_SIZE - 1;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\t/*",
            "\t * Now build the lower_tier mask for each node collecting node mask from",
            "\t * all memory tier below it. This allows us to fallback demotion page",
            "\t * allocation to a set of nodes that is closer the above selected",
            "\t * perferred node.",
            "\t */",
            "\tlower_tier = node_states[N_MEMORY];",
            "\tlist_for_each_entry(memtier, &memory_tiers, list) {",
            "\t\t/*",
            "\t\t * Keep removing current tier from lower_tier nodes,",
            "\t\t * This will remove all nodes in current and above",
            "\t\t * memory tier from the lower_tier mask.",
            "\t\t */",
            "\t\ttier_nodes = get_memtier_nodemask(memtier);",
            "\t\tnodes_andnot(lower_tier, lower_tier, tier_nodes);",
            "\t\tmemtier->lower_tier_mask = lower_tier;",
            "\t}",
            "}"
          ],
          "function_name": "disable_all_demotion_targets, establish_demotion_targets",
          "description": "负责维护内存分层间的降级目标关系，通过遍历所有节点计算最佳距离并更新lower_tier_mask。同步RCU保证内存分层变更的可见性，最终构建各节点的下层内存掩码供后续页面迁移使用。",
          "similarity": 0.5430954098701477
        },
        {
          "chunk_id": 0,
          "file_path": "mm/memory-tiers.c",
          "start_line": 1,
          "end_line": 58,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/slab.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/sysfs.h>",
            "#include <linux/kobject.h>",
            "#include <linux/memory.h>",
            "#include <linux/memory-tiers.h>",
            "#include <linux/notifier.h>",
            "#include <linux/sched/sysctl.h>",
            "",
            "#include \"internal.h\"",
            "",
            "struct memory_tier {",
            "\t/* hierarchy of memory tiers */",
            "\tstruct list_head list;",
            "\t/* list of all memory types part of this tier */",
            "\tstruct list_head memory_types;",
            "\t/*",
            "\t * start value of abstract distance. memory tier maps",
            "\t * an abstract distance  range,",
            "\t * adistance_start .. adistance_start + MEMTIER_CHUNK_SIZE",
            "\t */",
            "\tint adistance_start;",
            "\tstruct device dev;",
            "\t/* All the nodes that are part of all the lower memory tiers. */",
            "\tnodemask_t lower_tier_mask;",
            "};",
            "",
            "struct demotion_nodes {",
            "\tnodemask_t preferred;",
            "};",
            "",
            "struct node_memory_type_map {",
            "\tstruct memory_dev_type *memtype;",
            "\tint map_count;",
            "};",
            "",
            "static DEFINE_MUTEX(memory_tier_lock);",
            "static LIST_HEAD(memory_tiers);",
            "static struct node_memory_type_map node_memory_types[MAX_NUMNODES];",
            "struct memory_dev_type *default_dram_type;",
            "",
            "static struct bus_type memory_tier_subsys = {",
            "\t.name = \"memory_tiering\",",
            "\t.dev_name = \"memory_tier\",",
            "};",
            "",
            "#ifdef CONFIG_NUMA_BALANCING",
            "/**",
            " * folio_use_access_time - check if a folio reuses cpupid for page access time",
            " * @folio: folio to check",
            " *",
            " * folio's _last_cpupid field is repurposed by memory tiering. In memory",
            " * tiering mode, cpupid of slow memory folio (not toptier memory) is used to",
            " * record page access time.",
            " *",
            " * Return: the folio _last_cpupid is used to record page access time",
            " */"
          ],
          "function_name": null,
          "description": "定义memory_tier结构体及辅助数据结构，用于管理内存分层体系。声明全局锁和链表头，注册内存分层子系统，并定义默认DRAM类型指针。包含用于内存分层的抽象距离相关内联函数原型。",
          "similarity": 0.5427484512329102
        },
        {
          "chunk_id": 4,
          "file_path": "mm/memory-tiers.c",
          "start_line": 688,
          "end_line": 801,
          "content": [
            "int mt_perf_to_adistance(struct access_coordinate *perf, int *adist)",
            "{",
            "\tif (default_dram_perf_error)",
            "\t\treturn -EIO;",
            "",
            "\tif (default_dram_perf_ref_nid == NUMA_NO_NODE)",
            "\t\treturn -ENOENT;",
            "",
            "\tif (perf->read_latency + perf->write_latency == 0 ||",
            "\t    perf->read_bandwidth + perf->write_bandwidth == 0)",
            "\t\treturn -EINVAL;",
            "",
            "\tmutex_lock(&memory_tier_lock);",
            "\t/*",
            "\t * The abstract distance of a memory node is in direct proportion to",
            "\t * its memory latency (read + write) and inversely proportional to its",
            "\t * memory bandwidth (read + write).  The abstract distance, memory",
            "\t * latency, and memory bandwidth of the default DRAM nodes are used as",
            "\t * the base.",
            "\t */",
            "\t*adist = MEMTIER_ADISTANCE_DRAM *",
            "\t\t(perf->read_latency + perf->write_latency) /",
            "\t\t(default_dram_perf.read_latency + default_dram_perf.write_latency) *",
            "\t\t(default_dram_perf.read_bandwidth + default_dram_perf.write_bandwidth) /",
            "\t\t(perf->read_bandwidth + perf->write_bandwidth);",
            "\tmutex_unlock(&memory_tier_lock);",
            "",
            "\treturn 0;",
            "}",
            "int register_mt_adistance_algorithm(struct notifier_block *nb)",
            "{",
            "\treturn blocking_notifier_chain_register(&mt_adistance_algorithms, nb);",
            "}",
            "int unregister_mt_adistance_algorithm(struct notifier_block *nb)",
            "{",
            "\treturn blocking_notifier_chain_unregister(&mt_adistance_algorithms, nb);",
            "}",
            "int mt_calc_adistance(int node, int *adist)",
            "{",
            "\treturn blocking_notifier_call_chain(&mt_adistance_algorithms, node, adist);",
            "}",
            "static int __meminit memtier_hotplug_callback(struct notifier_block *self,",
            "\t\t\t\t\t      unsigned long action, void *_arg)",
            "{",
            "\tstruct memory_tier *memtier;",
            "\tstruct memory_notify *arg = _arg;",
            "",
            "\t/*",
            "\t * Only update the node migration order when a node is",
            "\t * changing status, like online->offline.",
            "\t */",
            "\tif (arg->status_change_nid < 0)",
            "\t\treturn notifier_from_errno(0);",
            "",
            "\tswitch (action) {",
            "\tcase MEM_OFFLINE:",
            "\t\tmutex_lock(&memory_tier_lock);",
            "\t\tif (clear_node_memory_tier(arg->status_change_nid))",
            "\t\t\testablish_demotion_targets();",
            "\t\tmutex_unlock(&memory_tier_lock);",
            "\t\tbreak;",
            "\tcase MEM_ONLINE:",
            "\t\tmutex_lock(&memory_tier_lock);",
            "\t\tmemtier = set_node_memory_tier(arg->status_change_nid);",
            "\t\tif (!IS_ERR(memtier))",
            "\t\t\testablish_demotion_targets();",
            "\t\tmutex_unlock(&memory_tier_lock);",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn notifier_from_errno(0);",
            "}",
            "static int __init memory_tier_init(void)",
            "{",
            "\tint ret, node;",
            "\tstruct memory_tier *memtier;",
            "",
            "\tret = subsys_virtual_register(&memory_tier_subsys, NULL);",
            "\tif (ret)",
            "\t\tpanic(\"%s() failed to register memory tier subsystem\\n\", __func__);",
            "",
            "#ifdef CONFIG_MIGRATION",
            "\tnode_demotion = kcalloc(nr_node_ids, sizeof(struct demotion_nodes),",
            "\t\t\t\tGFP_KERNEL);",
            "\tWARN_ON(!node_demotion);",
            "#endif",
            "\tmutex_lock(&memory_tier_lock);",
            "\t/*",
            "\t * For now we can have 4 faster memory tiers with smaller adistance",
            "\t * than default DRAM tier.",
            "\t */",
            "\tdefault_dram_type = alloc_memory_type(MEMTIER_ADISTANCE_DRAM);",
            "\tif (IS_ERR(default_dram_type))",
            "\t\tpanic(\"%s() failed to allocate default DRAM tier\\n\", __func__);",
            "",
            "\t/*",
            "\t * Look at all the existing N_MEMORY nodes and add them to",
            "\t * default memory tier or to a tier if we already have memory",
            "\t * types assigned.",
            "\t */",
            "\tfor_each_node_state(node, N_MEMORY) {",
            "\t\tmemtier = set_node_memory_tier(node);",
            "\t\tif (IS_ERR(memtier))",
            "\t\t\t/*",
            "\t\t\t * Continue with memtiers we are able to setup",
            "\t\t\t */",
            "\t\t\tbreak;",
            "\t}",
            "\testablish_demotion_targets();",
            "\tmutex_unlock(&memory_tier_lock);",
            "",
            "\thotplug_memory_notifier(memtier_hotplug_callback, MEMTIER_HOTPLUG_PRI);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "mt_perf_to_adistance, register_mt_adistance_algorithm, unregister_mt_adistance_algorithm, mt_calc_adistance, memtier_hotplug_callback, memory_tier_init",
          "description": "提供抽象距离计算接口和内存分层热插拔回调。初始化内存分层子系统，分配默认DRAM类型，遍历所有节点建立初始内存分层结构，并注册内存状态变化通知处理函数。",
          "similarity": 0.5203757286071777
        },
        {
          "chunk_id": 1,
          "file_path": "mm/memory-tiers.c",
          "start_line": 59,
          "end_line": 170,
          "content": [
            "bool folio_use_access_time(struct folio *folio)",
            "{",
            "\treturn (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&",
            "\t       !node_is_toptier(folio_nid(folio));",
            "}",
            "static __always_inline nodemask_t get_memtier_nodemask(struct memory_tier *memtier)",
            "{",
            "\tnodemask_t nodes = NODE_MASK_NONE;",
            "\tstruct memory_dev_type *memtype;",
            "",
            "\tlist_for_each_entry(memtype, &memtier->memory_types, tier_sibling)",
            "\t\tnodes_or(nodes, nodes, memtype->nodes);",
            "",
            "\treturn nodes;",
            "}",
            "static void memory_tier_device_release(struct device *dev)",
            "{",
            "\tstruct memory_tier *tier = to_memory_tier(dev);",
            "\t/*",
            "\t * synchronize_rcu in clear_node_memory_tier makes sure",
            "\t * we don't have rcu access to this memory tier.",
            "\t */",
            "\tkfree(tier);",
            "}",
            "static ssize_t nodelist_show(struct device *dev,",
            "\t\t\t     struct device_attribute *attr, char *buf)",
            "{",
            "\tint ret;",
            "\tnodemask_t nmask;",
            "",
            "\tmutex_lock(&memory_tier_lock);",
            "\tnmask = get_memtier_nodemask(to_memory_tier(dev));",
            "\tret = sysfs_emit(buf, \"%*pbl\\n\", nodemask_pr_args(&nmask));",
            "\tmutex_unlock(&memory_tier_lock);",
            "\treturn ret;",
            "}",
            "bool node_is_toptier(int node)",
            "{",
            "\tbool toptier;",
            "\tpg_data_t *pgdat;",
            "\tstruct memory_tier *memtier;",
            "",
            "\tpgdat = NODE_DATA(node);",
            "\tif (!pgdat)",
            "\t\treturn false;",
            "",
            "\trcu_read_lock();",
            "\tmemtier = rcu_dereference(pgdat->memtier);",
            "\tif (!memtier) {",
            "\t\ttoptier = true;",
            "\t\tgoto out;",
            "\t}",
            "\tif (memtier->adistance_start <= top_tier_adistance)",
            "\t\ttoptier = true;",
            "\telse",
            "\t\ttoptier = false;",
            "out:",
            "\trcu_read_unlock();",
            "\treturn toptier;",
            "}",
            "void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets)",
            "{",
            "\tstruct memory_tier *memtier;",
            "",
            "\t/*",
            "\t * pg_data_t.memtier updates includes a synchronize_rcu()",
            "\t * which ensures that we either find NULL or a valid memtier",
            "\t * in NODE_DATA. protect the access via rcu_read_lock();",
            "\t */",
            "\trcu_read_lock();",
            "\tmemtier = rcu_dereference(pgdat->memtier);",
            "\tif (memtier)",
            "\t\t*targets = memtier->lower_tier_mask;",
            "\telse",
            "\t\t*targets = NODE_MASK_NONE;",
            "\trcu_read_unlock();",
            "}",
            "int next_demotion_node(int node)",
            "{",
            "\tstruct demotion_nodes *nd;",
            "\tint target;",
            "",
            "\tif (!node_demotion)",
            "\t\treturn NUMA_NO_NODE;",
            "",
            "\tnd = &node_demotion[node];",
            "",
            "\t/*",
            "\t * node_demotion[] is updated without excluding this",
            "\t * function from running.",
            "\t *",
            "\t * Make sure to use RCU over entire code blocks if",
            "\t * node_demotion[] reads need to be consistent.",
            "\t */",
            "\trcu_read_lock();",
            "\t/*",
            "\t * If there are multiple target nodes, just select one",
            "\t * target node randomly.",
            "\t *",
            "\t * In addition, we can also use round-robin to select",
            "\t * target node, but we should introduce another variable",
            "\t * for node_demotion[] to record last selected target node,",
            "\t * that may cause cache ping-pong due to the changing of",
            "\t * last target node. Or introducing per-cpu data to avoid",
            "\t * caching issue, which seems more complicated. So selecting",
            "\t * target node randomly seems better until now.",
            "\t */",
            "\ttarget = node_random(&nd->preferred);",
            "\trcu_read_unlock();",
            "",
            "\treturn target;",
            "}"
          ],
          "function_name": "folio_use_access_time, get_memtier_nodemask, memory_tier_device_release, nodelist_show, node_is_toptier, node_get_allowed_targets, next_demotion_node",
          "description": "实现内存分层核心功能，包括判断页是否使用访问时间、获取内存分层节点掩码、设备释放、节点列表展示、判断是否为顶层节点、获取允许目标节点以及选择下一个降级节点的算法实现。",
          "similarity": 0.5103844404220581
        },
        {
          "chunk_id": 3,
          "file_path": "mm/memory-tiers.c",
          "start_line": 481,
          "end_line": 635,
          "content": [
            "static inline void establish_demotion_targets(void) {}",
            "static inline void __init_node_memory_type(int node, struct memory_dev_type *memtype)",
            "{",
            "\tif (!node_memory_types[node].memtype)",
            "\t\tnode_memory_types[node].memtype = memtype;",
            "\t/*",
            "\t * for each device getting added in the same NUMA node",
            "\t * with this specific memtype, bump the map count. We",
            "\t * Only take memtype device reference once, so that",
            "\t * changing a node memtype can be done by droping the",
            "\t * only reference count taken here.",
            "\t */",
            "",
            "\tif (node_memory_types[node].memtype == memtype) {",
            "\t\tif (!node_memory_types[node].map_count++)",
            "\t\t\tkref_get(&memtype->kref);",
            "\t}",
            "}",
            "static void destroy_memory_tier(struct memory_tier *memtier)",
            "{",
            "\tlist_del(&memtier->list);",
            "\tdevice_unregister(&memtier->dev);",
            "}",
            "static bool clear_node_memory_tier(int node)",
            "{",
            "\tbool cleared = false;",
            "\tpg_data_t *pgdat;",
            "\tstruct memory_tier *memtier;",
            "",
            "\tpgdat = NODE_DATA(node);",
            "\tif (!pgdat)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Make sure that anybody looking at NODE_DATA who finds",
            "\t * a valid memtier finds memory_dev_types with nodes still",
            "\t * linked to the memtier. We achieve this by waiting for",
            "\t * rcu read section to finish using synchronize_rcu.",
            "\t * This also enables us to free the destroyed memory tier",
            "\t * with kfree instead of kfree_rcu",
            "\t */",
            "\tmemtier = __node_get_memory_tier(node);",
            "\tif (memtier) {",
            "\t\tstruct memory_dev_type *memtype;",
            "",
            "\t\trcu_assign_pointer(pgdat->memtier, NULL);",
            "\t\tsynchronize_rcu();",
            "\t\tmemtype = node_memory_types[node].memtype;",
            "\t\tnode_clear(node, memtype->nodes);",
            "\t\tif (nodes_empty(memtype->nodes)) {",
            "\t\t\tlist_del_init(&memtype->tier_sibling);",
            "\t\t\tif (list_empty(&memtier->memory_types))",
            "\t\t\t\tdestroy_memory_tier(memtier);",
            "\t\t}",
            "\t\tcleared = true;",
            "\t}",
            "\treturn cleared;",
            "}",
            "static void release_memtype(struct kref *kref)",
            "{",
            "\tstruct memory_dev_type *memtype;",
            "",
            "\tmemtype = container_of(kref, struct memory_dev_type, kref);",
            "\tkfree(memtype);",
            "}",
            "void put_memory_type(struct memory_dev_type *memtype)",
            "{",
            "\tkref_put(&memtype->kref, release_memtype);",
            "}",
            "void init_node_memory_type(int node, struct memory_dev_type *memtype)",
            "{",
            "",
            "\tmutex_lock(&memory_tier_lock);",
            "\t__init_node_memory_type(node, memtype);",
            "\tmutex_unlock(&memory_tier_lock);",
            "}",
            "void clear_node_memory_type(int node, struct memory_dev_type *memtype)",
            "{",
            "\tmutex_lock(&memory_tier_lock);",
            "\tif (node_memory_types[node].memtype == memtype || !memtype)",
            "\t\tnode_memory_types[node].map_count--;",
            "\t/*",
            "\t * If we umapped all the attached devices to this node,",
            "\t * clear the node memory type.",
            "\t */",
            "\tif (!node_memory_types[node].map_count) {",
            "\t\tmemtype = node_memory_types[node].memtype;",
            "\t\tnode_memory_types[node].memtype = NULL;",
            "\t\tput_memory_type(memtype);",
            "\t}",
            "\tmutex_unlock(&memory_tier_lock);",
            "}",
            "static void dump_hmem_attrs(struct access_coordinate *coord, const char *prefix)",
            "{",
            "\tpr_info(",
            "\"%sread_latency: %u, write_latency: %u, read_bandwidth: %u, write_bandwidth: %u\\n\",",
            "\t\tprefix, coord->read_latency, coord->write_latency,",
            "\t\tcoord->read_bandwidth, coord->write_bandwidth);",
            "}",
            "int mt_set_default_dram_perf(int nid, struct access_coordinate *perf,",
            "\t\t\t     const char *source)",
            "{",
            "\tint rc = 0;",
            "",
            "\tmutex_lock(&memory_tier_lock);",
            "\tif (default_dram_perf_error) {",
            "\t\trc = -EIO;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tif (perf->read_latency + perf->write_latency == 0 ||",
            "\t    perf->read_bandwidth + perf->write_bandwidth == 0) {",
            "\t\trc = -EINVAL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tif (default_dram_perf_ref_nid == NUMA_NO_NODE) {",
            "\t\tdefault_dram_perf = *perf;",
            "\t\tdefault_dram_perf_ref_nid = nid;",
            "\t\tdefault_dram_perf_ref_source = kstrdup(source, GFP_KERNEL);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/*",
            "\t * The performance of all default DRAM nodes is expected to be",
            "\t * same (that is, the variation is less than 10%).  And it",
            "\t * will be used as base to calculate the abstract distance of",
            "\t * other memory nodes.",
            "\t */",
            "\tif (abs(perf->read_latency - default_dram_perf.read_latency) * 10 >",
            "\t    default_dram_perf.read_latency ||",
            "\t    abs(perf->write_latency - default_dram_perf.write_latency) * 10 >",
            "\t    default_dram_perf.write_latency ||",
            "\t    abs(perf->read_bandwidth - default_dram_perf.read_bandwidth) * 10 >",
            "\t    default_dram_perf.read_bandwidth ||",
            "\t    abs(perf->write_bandwidth - default_dram_perf.write_bandwidth) * 10 >",
            "\t    default_dram_perf.write_bandwidth) {",
            "\t\tpr_info(",
            "\"memory-tiers: the performance of DRAM node %d mismatches that of the reference\\n\"",
            "\"DRAM node %d.\\n\", nid, default_dram_perf_ref_nid);",
            "\t\tpr_info(\"  performance of reference DRAM node %d:\\n\",",
            "\t\t\tdefault_dram_perf_ref_nid);",
            "\t\tdump_hmem_attrs(&default_dram_perf, \"    \");",
            "\t\tpr_info(\"  performance of DRAM node %d:\\n\", nid);",
            "\t\tdump_hmem_attrs(perf, \"    \");",
            "\t\tpr_info(",
            "\"  disable default DRAM node performance based abstract distance algorithm.\\n\");",
            "\t\tdefault_dram_perf_error = true;",
            "\t\trc = -EINVAL;",
            "\t}",
            "",
            "out:",
            "\tmutex_unlock(&memory_tier_lock);",
            "\treturn rc;",
            "}"
          ],
          "function_name": "establish_demotion_targets, __init_node_memory_type, destroy_memory_tier, clear_node_memory_tier, release_memtype, put_memory_type, init_node_memory_type, clear_node_memory_type, dump_hmem_attrs, mt_set_default_dram_perf",
          "description": "实现内存类型管理系统，包含初始化/清除节点内存类型、引用计数管理、性能属性打印、默认DRAM性能设置等功能。通过kref机制跟踪内存类型引用，确保内存分层结构的正确释放。",
          "similarity": 0.49864092469215393
        }
      ]
    }
  ]
}