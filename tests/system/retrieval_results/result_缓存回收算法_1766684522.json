{
  "query": "缓存回收算法",
  "timestamp": "2025-12-26 01:42:02",
  "retrieved_files": [
    {
      "source_file": "mm/damon/reclaim.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:50:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `damon\\reclaim.c`\n\n---\n\n# `damon/reclaim.c` 技术文档\n\n## 1. 文件概述\n\n`damon/reclaim.c` 是 Linux 内核中基于 **DAMON（Data Access MONitor）** 框架实现的**自动内存回收模块**。该模块通过监控物理内存区域的访问模式，识别长时间未被访问的“冷”内存页，并主动将其回收（page-out），从而释放系统内存资源。其核心目标是在不影响系统性能的前提下，智能地回收低价值内存，提升内存利用率。\n\n该模块以可加载内核模块（LKM）形式存在，通过一组可调参数控制其行为，并支持基于水位线（watermarks）的条件激活机制，避免在内存充足时进行不必要的回收操作。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`enabled`**: 全局开关，控制 DAMON_RECLAIM 功能是否启用。\n- **`commit_inputs`**: 触发参数重载的标志位，用于运行时动态更新配置（除 `enabled` 外）。\n- **`min_age`**: 冷内存判定阈值（微秒），默认 120 秒。\n- **`damon_reclaim_quota`**: 回收配额控制结构，限制单位时间内的最大回收量（默认每秒最多 128 MiB）和 CPU 时间开销（默认最多 10 ms）。\n- **`damon_reclaim_wmarks`**: 水位线配置，基于空闲内存比率决定是否激活回收（高/中/低水位分别为 50%/40%/20%）。\n- **`damon_reclaim_mon_attrs`**: DAMON 监控属性，定义采样间隔（5ms）、聚合间隔（100ms）等。\n- **`monitor_region_start/end`**: 目标监控内存区域的物理地址范围，默认为系统最大连续 RAM 区域。\n- **`skip_anon`**: 布尔标志，若为真则跳过匿名页（anonymous pages）的回收。\n- **`kdamond_pid`**: DAMON 工作线程的 PID，未启用时为 -1。\n- **`damon_reclaim_stat`**: 统计信息结构，记录尝试回收区域数、成功回收区域数及配额超限次数。\n\n### 主要函数\n\n- **`damon_reclaim_new_scheme()`**: 创建 DAMOS（DAMON Operation Scheme）策略，定义“冷内存”模式（大小 ≥ PAGE_SIZE、访问次数为 0、年龄 ≥ `min_age`）并指定操作为 `DAMOS_PAGEOUT`。\n- **`damon_reclaim_apply_parameters()`**: 应用所有用户配置参数到 DAMON 上下文（`ctx`），包括监控属性、回收策略、过滤器（如 `skip_anon`）和监控区域。\n- **`damon_reclaim_turn()`**: 启动或停止 DAMON_RECLAIM 的核心监控与回收逻辑。\n- **`damon_reclaim_enabled_store()`**: `enabled` 参数的 setter 回调，处理启用/禁用逻辑。\n- **`damon_reclaim_handle_commit_inputs()`**: 处理 `commit_inputs` 标志，触发运行时参数重载。\n- **`damon_reclaim_after_aggregation()` / `damon_reclaim_after_wmarks_check()`**: DAMON 回调函数，在聚合后和水位检查后更新统计信息并处理参数提交。\n- **`damon_reclaim_init()`**: 模块初始化函数，创建 DAMON 上下文和目标，注册回调，并根据初始 `enabled` 状态决定是否启动。\n\n## 3. 关键实现\n\n### 冷内存识别与回收策略\n- 通过 `damon_reclaim_new_scheme()` 定义 DAMOS 策略：\n  - **访问模式匹配**：区域大小 ≥ `PAGE_SIZE`、访问次数 = 0、年龄 ≥ `min_age / aggr_interval`（转换为聚合周期单位）。\n  - **操作类型**：`DAMOS_PAGEOUT`，即对匹配区域执行页面回收。\n  - **配额控制**：使用 `damon_reclaim_quota` 限制回收速度和 CPU 开销，确保系统稳定性。\n  - **水位激活**：仅当空闲内存比率低于 `high` 水位（50%）时激活策略，高于 `low` 水位（20%）时停用。\n\n### 动态参数更新机制\n- 用户可通过写入 `commit_inputs=Y` 触发运行时参数重载（`min_age`、配额、水位、监控区域等）。\n- `damon_reclaim_handle_commit_inputs()` 在 DAMON 的聚合后或水位检查后回调中执行重载，确保线程安全。\n- 重载时保留旧策略的配额状态（如已消耗的配额），避免统计中断。\n\n### 匿名页过滤\n- 若 `skip_anon=Y`，通过 `DAMOS_FILTER_TYPE_ANON` 过滤器排除匿名页（如进程堆栈、堆内存），仅回收文件缓存等页面。\n\n### 监控区域自动配置\n- 默认使用 `damon_set_region_biggest_system_ram_default()` 自动选择系统中最大的连续物理 RAM 区域作为监控目标，用户也可通过 `monitor_region_start/end` 手动指定。\n\n## 4. 依赖关系\n\n- **DAMON 核心框架** (`<linux/damon.h>`): 依赖 DAMON 提供的内存访问监控、策略引擎（DAMOS）、配额管理、水位控制等基础设施。\n- **内核模块通用接口** (`modules-common.h`): 使用 `DEFINE_DAMON_MODULES_*` 宏简化参数声明和统计暴露。\n- **内存管理子系统**: 通过 `DAMOS_PAGEOUT` 操作与 MM 子系统交互，实际执行页面回收。\n- **参数解析工具** (`<linux/kstrtox.h>`): 用于解析用户输入的布尔值和数值参数。\n\n## 5. 使用场景\n\n- **内存压力缓解**: 在内存紧张但尚未触发传统 LRU 回收或 OOM Killer 之前，提前回收长期未使用的冷内存，延缓内存压力。\n- **容器/虚拟机内存优化**: 在容器或 VM 中部署，自动回收应用未使用的缓存内存，提高宿主机内存密度。\n- **大内存系统调优**: 在 TB 级内存服务器上，减少因缓存膨胀导致的内存浪费，提升整体内存效率。\n- **低延迟敏感场景**: 通过配额限制（`ms=10`）确保回收操作不会显著影响关键任务的延迟。\n- **调试与监控**: 通过 `kdamond_pid` 和统计参数（`reclaim_tried_regions` 等）监控 DAMON_RECLAIM 的运行状态和效果。",
      "similarity": 0.6290283799171448,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/damon/reclaim.c",
          "start_line": 153,
          "end_line": 254,
          "content": [
            "static void damon_reclaim_copy_quota_status(struct damos_quota *dst,",
            "\t\tstruct damos_quota *src)",
            "{",
            "\tdst->total_charged_sz = src->total_charged_sz;",
            "\tdst->total_charged_ns = src->total_charged_ns;",
            "\tdst->charged_sz = src->charged_sz;",
            "\tdst->charged_from = src->charged_from;",
            "\tdst->charge_target_from = src->charge_target_from;",
            "\tdst->charge_addr_from = src->charge_addr_from;",
            "}",
            "static int damon_reclaim_apply_parameters(void)",
            "{",
            "\tstruct damos *scheme, *old_scheme;",
            "\tstruct damos_filter *filter;",
            "\tint err = 0;",
            "",
            "\terr = damon_set_attrs(ctx, &damon_reclaim_mon_attrs);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\t/* Will be freed by next 'damon_set_schemes()' below */",
            "\tscheme = damon_reclaim_new_scheme();",
            "\tif (!scheme)",
            "\t\treturn -ENOMEM;",
            "\tif (!list_empty(&ctx->schemes)) {",
            "\t\tdamon_for_each_scheme(old_scheme, ctx)",
            "\t\t\tdamon_reclaim_copy_quota_status(&scheme->quota,",
            "\t\t\t\t\t&old_scheme->quota);",
            "\t}",
            "\tif (skip_anon) {",
            "\t\tfilter = damos_new_filter(DAMOS_FILTER_TYPE_ANON, true);",
            "\t\tif (!filter) {",
            "\t\t\t/* Will be freed by next 'damon_set_schemes()' below */",
            "\t\t\tdamon_destroy_scheme(scheme);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "\t\tdamos_add_filter(scheme, filter);",
            "\t}",
            "\tdamon_set_schemes(ctx, &scheme, 1);",
            "",
            "\treturn damon_set_region_biggest_system_ram_default(target,",
            "\t\t\t\t\t&monitor_region_start,",
            "\t\t\t\t\t&monitor_region_end);",
            "}",
            "static int damon_reclaim_turn(bool on)",
            "{",
            "\tint err;",
            "",
            "\tif (!on) {",
            "\t\terr = damon_stop(&ctx, 1);",
            "\t\tif (!err)",
            "\t\t\tkdamond_pid = -1;",
            "\t\treturn err;",
            "\t}",
            "",
            "\terr = damon_reclaim_apply_parameters();",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\terr = damon_start(&ctx, 1, true);",
            "\tif (err)",
            "\t\treturn err;",
            "\tkdamond_pid = ctx->kdamond->pid;",
            "\treturn 0;",
            "}",
            "static int damon_reclaim_enabled_store(const char *val,",
            "\t\tconst struct kernel_param *kp)",
            "{",
            "\tbool is_enabled = enabled;",
            "\tbool enable;",
            "\tint err;",
            "",
            "\terr = kstrtobool(val, &enable);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (is_enabled == enable)",
            "\t\treturn 0;",
            "",
            "\t/* Called before init function.  The function will handle this. */",
            "\tif (!ctx)",
            "\t\tgoto set_param_out;",
            "",
            "\terr = damon_reclaim_turn(enable);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "set_param_out:",
            "\tenabled = enable;",
            "\treturn err;",
            "}",
            "static int damon_reclaim_handle_commit_inputs(void)",
            "{",
            "\tint err;",
            "",
            "\tif (!commit_inputs)",
            "\t\treturn 0;",
            "",
            "\terr = damon_reclaim_apply_parameters();",
            "\tcommit_inputs = false;",
            "\treturn err;",
            "}"
          ],
          "function_name": "damon_reclaim_copy_quota_status, damon_reclaim_apply_parameters, damon_reclaim_turn, damon_reclaim_enabled_store, damon_reclaim_handle_commit_inputs",
          "description": "实现DAMON_RECLAIM参数动态应用、启停切换及配额状态复制逻辑，通过回调机制协调监控上下文与回收策略，支持运行时参数更新和资源回收操作。",
          "similarity": 0.5576175451278687
        },
        {
          "chunk_id": 2,
          "file_path": "mm/damon/reclaim.c",
          "start_line": 269,
          "end_line": 298,
          "content": [
            "static int damon_reclaim_after_aggregation(struct damon_ctx *c)",
            "{",
            "\tstruct damos *s;",
            "",
            "\t/* update the stats parameter */",
            "\tdamon_for_each_scheme(s, c)",
            "\t\tdamon_reclaim_stat = s->stat;",
            "",
            "\treturn damon_reclaim_handle_commit_inputs();",
            "}",
            "static int damon_reclaim_after_wmarks_check(struct damon_ctx *c)",
            "{",
            "\treturn damon_reclaim_handle_commit_inputs();",
            "}",
            "static int __init damon_reclaim_init(void)",
            "{",
            "\tint err = damon_modules_new_paddr_ctx_target(&ctx, &target);",
            "",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tctx->callback.after_wmarks_check = damon_reclaim_after_wmarks_check;",
            "\tctx->callback.after_aggregation = damon_reclaim_after_aggregation;",
            "",
            "\t/* 'enabled' has set before this function, probably via command line */",
            "\tif (enabled)",
            "\t\terr = damon_reclaim_turn(true);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "damon_reclaim_after_aggregation, damon_reclaim_after_wmarks_check, damon_reclaim_init",
          "description": "注册DAMON框架的回调函数以实现回收策略的动态调整，初始化阶段绑定自定义回调至上下文，确保在监控周期关键节点触发参数重载和回收策略更新。",
          "similarity": 0.5311352014541626
        },
        {
          "chunk_id": 0,
          "file_path": "mm/damon/reclaim.c",
          "start_line": 1,
          "end_line": 152,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * DAMON-based page reclamation",
            " *",
            " * Author: SeongJae Park <sj@kernel.org>",
            " */",
            "",
            "#define pr_fmt(fmt) \"damon-reclaim: \" fmt",
            "",
            "#include <linux/damon.h>",
            "#include <linux/kstrtox.h>",
            "#include <linux/module.h>",
            "",
            "#include \"modules-common.h\"",
            "",
            "#ifdef MODULE_PARAM_PREFIX",
            "#undef MODULE_PARAM_PREFIX",
            "#endif",
            "#define MODULE_PARAM_PREFIX \"damon_reclaim.\"",
            "",
            "/*",
            " * Enable or disable DAMON_RECLAIM.",
            " *",
            " * You can enable DAMON_RCLAIM by setting the value of this parameter as ``Y``.",
            " * Setting it as ``N`` disables DAMON_RECLAIM.  Note that DAMON_RECLAIM could",
            " * do no real monitoring and reclamation due to the watermarks-based activation",
            " * condition.  Refer to below descriptions for the watermarks parameter for",
            " * this.",
            " */",
            "static bool enabled __read_mostly;",
            "",
            "/*",
            " * Make DAMON_RECLAIM reads the input parameters again, except ``enabled``.",
            " *",
            " * Input parameters that updated while DAMON_RECLAIM is running are not applied",
            " * by default.  Once this parameter is set as ``Y``, DAMON_RECLAIM reads values",
            " * of parametrs except ``enabled`` again.  Once the re-reading is done, this",
            " * parameter is set as ``N``.  If invalid parameters are found while the",
            " * re-reading, DAMON_RECLAIM will be disabled.",
            " */",
            "static bool commit_inputs __read_mostly;",
            "module_param(commit_inputs, bool, 0600);",
            "",
            "/*",
            " * Time threshold for cold memory regions identification in microseconds.",
            " *",
            " * If a memory region is not accessed for this or longer time, DAMON_RECLAIM",
            " * identifies the region as cold, and reclaims.  120 seconds by default.",
            " */",
            "static unsigned long min_age __read_mostly = 120000000;",
            "module_param(min_age, ulong, 0600);",
            "",
            "static struct damos_quota damon_reclaim_quota = {",
            "\t/* use up to 10 ms time, reclaim up to 128 MiB per 1 sec by default */",
            "\t.ms = 10,",
            "\t.sz = 128 * 1024 * 1024,",
            "\t.reset_interval = 1000,",
            "\t/* Within the quota, page out older regions first. */",
            "\t.weight_sz = 0,",
            "\t.weight_nr_accesses = 0,",
            "\t.weight_age = 1",
            "};",
            "DEFINE_DAMON_MODULES_DAMOS_QUOTAS(damon_reclaim_quota);",
            "",
            "static struct damos_watermarks damon_reclaim_wmarks = {",
            "\t.metric = DAMOS_WMARK_FREE_MEM_RATE,",
            "\t.interval = 5000000,\t/* 5 seconds */",
            "\t.high = 500,\t\t/* 50 percent */",
            "\t.mid = 400,\t\t/* 40 percent */",
            "\t.low = 200,\t\t/* 20 percent */",
            "};",
            "DEFINE_DAMON_MODULES_WMARKS_PARAMS(damon_reclaim_wmarks);",
            "",
            "static struct damon_attrs damon_reclaim_mon_attrs = {",
            "\t.sample_interval = 5000,\t/* 5 ms */",
            "\t.aggr_interval = 100000,\t/* 100 ms */",
            "\t.ops_update_interval = 0,",
            "\t.min_nr_regions = 10,",
            "\t.max_nr_regions = 1000,",
            "};",
            "DEFINE_DAMON_MODULES_MON_ATTRS_PARAMS(damon_reclaim_mon_attrs);",
            "",
            "/*",
            " * Start of the target memory region in physical address.",
            " *",
            " * The start physical address of memory region that DAMON_RECLAIM will do work",
            " * against.  By default, biggest System RAM is used as the region.",
            " */",
            "static unsigned long monitor_region_start __read_mostly;",
            "module_param(monitor_region_start, ulong, 0600);",
            "",
            "/*",
            " * End of the target memory region in physical address.",
            " *",
            " * The end physical address of memory region that DAMON_RECLAIM will do work",
            " * against.  By default, biggest System RAM is used as the region.",
            " */",
            "static unsigned long monitor_region_end __read_mostly;",
            "module_param(monitor_region_end, ulong, 0600);",
            "",
            "/*",
            " * Skip anonymous pages reclamation.",
            " *",
            " * If this parameter is set as ``Y``, DAMON_RECLAIM does not reclaim anonymous",
            " * pages.  By default, ``N``.",
            " */",
            "static bool skip_anon __read_mostly;",
            "module_param(skip_anon, bool, 0600);",
            "",
            "/*",
            " * PID of the DAMON thread",
            " *",
            " * If DAMON_RECLAIM is enabled, this becomes the PID of the worker thread.",
            " * Else, -1.",
            " */",
            "static int kdamond_pid __read_mostly = -1;",
            "module_param(kdamond_pid, int, 0400);",
            "",
            "static struct damos_stat damon_reclaim_stat;",
            "DEFINE_DAMON_MODULES_DAMOS_STATS_PARAMS(damon_reclaim_stat,",
            "\t\treclaim_tried_regions, reclaimed_regions, quota_exceeds);",
            "",
            "static struct damon_ctx *ctx;",
            "static struct damon_target *target;",
            "",
            "static struct damos *damon_reclaim_new_scheme(void)",
            "{",
            "\tstruct damos_access_pattern pattern = {",
            "\t\t/* Find regions having PAGE_SIZE or larger size */",
            "\t\t.min_sz_region = PAGE_SIZE,",
            "\t\t.max_sz_region = ULONG_MAX,",
            "\t\t/* and not accessed at all */",
            "\t\t.min_nr_accesses = 0,",
            "\t\t.max_nr_accesses = 0,",
            "\t\t/* for min_age or more micro-seconds */",
            "\t\t.min_age_region = min_age /",
            "\t\t\tdamon_reclaim_mon_attrs.aggr_interval,",
            "\t\t.max_age_region = UINT_MAX,",
            "\t};",
            "",
            "\treturn damon_new_scheme(",
            "\t\t\t&pattern,",
            "\t\t\t/* page out those, as soon as found */",
            "\t\t\tDAMOS_PAGEOUT,",
            "\t\t\t/* for each aggregation interval */",
            "\t\t\t0,",
            "\t\t\t/* under the quota. */",
            "\t\t\t&damon_reclaim_quota,",
            "\t\t\t/* (De)activate this according to the watermarks. */",
            "\t\t\t&damon_reclaim_wmarks);",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义DAMON_RECLAIM模块的全局参数和配置，包括启用状态、冷内存识别时间阈值、配额限制、水印条件及监控属性，用于控制基于DAMON的页面回收行为。",
          "similarity": 0.5281839370727539
        }
      ]
    },
    {
      "source_file": "kernel/bpf/bpf_lru_list.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:00:28\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\bpf_lru_list.c`\n\n---\n\n# `bpf/bpf_lru_list.c` 技术文档\n\n## 1. 文件概述\n\n`bpf_lru_list.c` 实现了 BPF（Berkeley Packet Filter）子系统中用于管理 LRU（Least Recently Used，最近最少使用）缓存的通用机制。该机制主要用于 BPF map（如 `lru_hash` 类型）中高效地回收不活跃或未被引用的条目，以控制内存使用并提升缓存命中率。文件提供了基于双链表的活跃/非活跃 LRU 链表管理、本地（per-CPU）缓存支持、引用位（ref bit）跟踪以及自动老化和回收策略。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct bpf_lru_node`：LRU 节点，嵌入在 BPF map 条目中，包含链表指针、类型和引用标志。\n- `struct bpf_lru_list`：全局 LRU 链表结构，维护活跃（ACTIVE）和非活跃（INACTIVE）链表，以及各类节点计数。\n- `struct bpf_lru_locallist`：每个 CPU 的本地 LRU 链表，用于减少锁竞争，包含 FREE 和 PENDING 本地链表。\n- `struct bpf_lru`：LRU 控制结构，包含回调函数（如 `del_from_htab`）、扫描数量（`nr_scans`）等配置。\n\n### 主要函数\n- `__bpf_lru_list_rotate_active()`：轮转活跃链表，将带引用位的节点保留在活跃链表头部，无引用位的移至非活跃链表。\n- `__bpf_lru_list_rotate_inactive()`：轮转非活跃链表，将带引用位的节点提升回活跃链表。\n- `__bpf_lru_list_shrink_inactive()`：从非活跃链表尾部回收无引用位且可删除的节点到指定 free 链表。\n- `__bpf_lru_list_shrink()`：尝试正常回收失败后，强制从非活跃或活跃链表中删除节点（忽略引用位）。\n- `__bpf_lru_node_move()` / `__bpf_lru_node_move_in()` / `__bpf_lru_node_move_to_free()`：节点在不同链表间移动的内部辅助函数。\n- `get_next_cpu()`：用于遍历所有可能 CPU 的辅助函数。\n\n### 关键常量\n- `LOCAL_FREE_TARGET` / `PERCPU_FREE_TARGET`：本地和 per-CPU 回收目标数量（分别为 128 和 4）。\n- `LOCAL_NR_SCANS` / `PERCPU_NR_SCANS`：本地和 per-CPU 扫描上限，等于各自目标值。\n- `BPF_LOCAL_LIST_T_OFFSET`：本地链表类型的偏移量，用于区分全局 LRU 类型和本地类型。\n\n## 3. 关键实现\n\n### LRU 双链表模型\n采用经典的 **Active/Inactive 双链表模型**：\n- **活跃链表（ACTIVE）**：存放近期被访问或引用的节点。\n- **非活跃链表（INACTIVE）**：存放较久未被访问的节点，是回收的主要候选区域。\n- 节点首次插入时通常进入活跃链表；经过一次老化周期后，若无引用则移至非活跃链表。\n\n### 引用位（ref bit）机制\n- 每个 `bpf_lru_node` 包含一个 `ref` 字段（原子读写），表示该节点是否在最近被访问。\n- 在轮转过程中：\n  - 若节点 `ref == 1`，则清零并保留在活跃链表（或从非活跃提升至活跃）。\n  - 若 `ref == 0`，则可能被移至非活跃链表或直接回收。\n- 该机制避免了频繁移动热数据，提高了缓存效率。\n\n### 老化与回收策略\n- **轮转（Rotate）**：\n  - 定期调用 `__bpf_lru_list_rotate()`。\n  - 当非活跃链表长度小于活跃链表时，触发活跃链表轮转。\n  - 非活跃链表总是轮转，从 `next_inactive_rotation` 指针开始，避免每次都从头扫描。\n- **回收（Shrink）**：\n  - 优先从非活跃链表尾部回收无引用且可删除（通过 `del_from_htab` 回调确认）的节点。\n  - 若回收不足，强制从非活跃链表（优先）或活跃链表中删除节点，**忽略引用位**，确保内存压力下能释放资源。\n\n### 本地（Local）与 Per-CPU 优化\n- 支持 **本地链表类型**（`BPF_LRU_LOCAL_LIST_T_FREE` / `PENDING`），用于暂存待处理或刚释放的节点。\n- 通过 `IS_LOCAL_LIST_TYPE()` 宏区分本地与全局类型，防止非法移动。\n- 减少全局锁竞争，提升多核性能。\n\n### 安全移动与指针维护\n- 在移动节点时，若该节点恰好是 `next_inactive_rotation` 指针指向的对象，则自动将其前移，避免悬空指针。\n- 使用 `list_move()` 安全地在链表间转移节点。\n- 所有计数操作（`bpf_lru_list_count_inc/dec`）均带边界检查。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/cpumask.h>`：用于 CPU 遍历（`get_next_cpu`）。\n  - `<linux/spinlock.h>` 和 `<linux/percpu.h>`：支持 per-CPU 数据结构和同步。\n- **内部依赖**：\n  - 依赖 `bpf_lru_list.h` 中定义的数据结构和枚举（如 `enum bpf_lru_list_type`）。\n- **外部回调**：\n  - 通过 `lru->del_from_htab(lru->del_arg, node)` 回调通知上层（如 BPF map 实现）删除哈希表中的条目，实现 LRU 与具体数据结构的解耦。\n\n## 5. 使用场景\n\n- **BPF LRU Hash Map**：该文件是 `BPF_MAP_TYPE_LRU_HASH` 和 `BPF_MAP_TYPE_LRU_PERCPU_HASH` 等 map 类型的核心内存管理组件。\n- **内存压力下的自动回收**：当 map 达到容量上限或系统内存紧张时，触发 shrink 操作释放条目。\n- **高并发环境优化**：通过本地链表和引用位机制，在多核系统上高效管理缓存，减少锁争用。\n- **内核网络与跟踪子系统**：被用于 eBPF 程序中需要高效键值存储且自动淘汰旧数据的场景，如连接跟踪、统计聚合等。",
      "similarity": 0.6198841333389282,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 171,
          "end_line": 272,
          "content": [
            "static void __bpf_lru_list_rotate_inactive(struct bpf_lru *lru,",
            "\t\t\t\t\t   struct bpf_lru_list *l)",
            "{",
            "\tstruct list_head *inactive = &l->lists[BPF_LRU_LIST_T_INACTIVE];",
            "\tstruct list_head *cur, *last, *next = inactive;",
            "\tstruct bpf_lru_node *node;",
            "\tunsigned int i = 0;",
            "",
            "\tif (list_empty(inactive))",
            "\t\treturn;",
            "",
            "\tlast = l->next_inactive_rotation->next;",
            "\tif (last == inactive)",
            "\t\tlast = last->next;",
            "",
            "\tcur = l->next_inactive_rotation;",
            "\twhile (i < lru->nr_scans) {",
            "\t\tif (cur == inactive) {",
            "\t\t\tcur = cur->prev;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tnode = list_entry(cur, struct bpf_lru_node, list);",
            "\t\tnext = cur->prev;",
            "\t\tif (bpf_lru_node_is_ref(node))",
            "\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);",
            "\t\tif (cur == last)",
            "\t\t\tbreak;",
            "\t\tcur = next;",
            "\t\ti++;",
            "\t}",
            "",
            "\tl->next_inactive_rotation = next;",
            "}",
            "static unsigned int",
            "__bpf_lru_list_shrink_inactive(struct bpf_lru *lru,",
            "\t\t\t       struct bpf_lru_list *l,",
            "\t\t\t       unsigned int tgt_nshrink,",
            "\t\t\t       struct list_head *free_list,",
            "\t\t\t       enum bpf_lru_list_type tgt_free_type)",
            "{",
            "\tstruct list_head *inactive = &l->lists[BPF_LRU_LIST_T_INACTIVE];",
            "\tstruct bpf_lru_node *node, *tmp_node;",
            "\tunsigned int nshrinked = 0;",
            "\tunsigned int i = 0;",
            "",
            "\tlist_for_each_entry_safe_reverse(node, tmp_node, inactive, list) {",
            "\t\tif (bpf_lru_node_is_ref(node)) {",
            "\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);",
            "\t\t} else if (lru->del_from_htab(lru->del_arg, node)) {",
            "\t\t\t__bpf_lru_node_move_to_free(l, node, free_list,",
            "\t\t\t\t\t\t    tgt_free_type);",
            "\t\t\tif (++nshrinked == tgt_nshrink)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tif (++i == lru->nr_scans)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn nshrinked;",
            "}",
            "static void __bpf_lru_list_rotate(struct bpf_lru *lru, struct bpf_lru_list *l)",
            "{",
            "\tif (bpf_lru_list_inactive_low(l))",
            "\t\t__bpf_lru_list_rotate_active(lru, l);",
            "",
            "\t__bpf_lru_list_rotate_inactive(lru, l);",
            "}",
            "static unsigned int __bpf_lru_list_shrink(struct bpf_lru *lru,",
            "\t\t\t\t\t  struct bpf_lru_list *l,",
            "\t\t\t\t\t  unsigned int tgt_nshrink,",
            "\t\t\t\t\t  struct list_head *free_list,",
            "\t\t\t\t\t  enum bpf_lru_list_type tgt_free_type)",
            "",
            "{",
            "\tstruct bpf_lru_node *node, *tmp_node;",
            "\tstruct list_head *force_shrink_list;",
            "\tunsigned int nshrinked;",
            "",
            "\tnshrinked = __bpf_lru_list_shrink_inactive(lru, l, tgt_nshrink,",
            "\t\t\t\t\t\t   free_list, tgt_free_type);",
            "\tif (nshrinked)",
            "\t\treturn nshrinked;",
            "",
            "\t/* Do a force shrink by ignoring the reference bit */",
            "\tif (!list_empty(&l->lists[BPF_LRU_LIST_T_INACTIVE]))",
            "\t\tforce_shrink_list = &l->lists[BPF_LRU_LIST_T_INACTIVE];",
            "\telse",
            "\t\tforce_shrink_list = &l->lists[BPF_LRU_LIST_T_ACTIVE];",
            "",
            "\tlist_for_each_entry_safe_reverse(node, tmp_node, force_shrink_list,",
            "\t\t\t\t\t list) {",
            "\t\tif (lru->del_from_htab(lru->del_arg, node)) {",
            "\t\t\t__bpf_lru_node_move_to_free(l, node, free_list,",
            "\t\t\t\t\t\t    tgt_free_type);",
            "\t\t\treturn 1;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "__bpf_lru_list_rotate_inactive, __bpf_lru_list_shrink_inactive, __bpf_lru_list_rotate, __bpf_lru_list_shrink",
          "description": "执行非活跃列表回收操作，通过遍历节点依据引用状态或哈希表删除条件进行节点迁移和资源缩减。",
          "similarity": 0.6396265029907227
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 22,
          "end_line": 126,
          "content": [
            "static int get_next_cpu(int cpu)",
            "{",
            "\tcpu = cpumask_next(cpu, cpu_possible_mask);",
            "\tif (cpu >= nr_cpu_ids)",
            "\t\tcpu = cpumask_first(cpu_possible_mask);",
            "\treturn cpu;",
            "}",
            "static bool bpf_lru_node_is_ref(const struct bpf_lru_node *node)",
            "{",
            "\treturn READ_ONCE(node->ref);",
            "}",
            "static void bpf_lru_node_clear_ref(struct bpf_lru_node *node)",
            "{",
            "\tWRITE_ONCE(node->ref, 0);",
            "}",
            "static void bpf_lru_list_count_inc(struct bpf_lru_list *l,",
            "\t\t\t\t   enum bpf_lru_list_type type)",
            "{",
            "\tif (type < NR_BPF_LRU_LIST_COUNT)",
            "\t\tl->counts[type]++;",
            "}",
            "static void bpf_lru_list_count_dec(struct bpf_lru_list *l,",
            "\t\t\t\t   enum bpf_lru_list_type type)",
            "{",
            "\tif (type < NR_BPF_LRU_LIST_COUNT)",
            "\t\tl->counts[type]--;",
            "}",
            "static void __bpf_lru_node_move_to_free(struct bpf_lru_list *l,",
            "\t\t\t\t\tstruct bpf_lru_node *node,",
            "\t\t\t\t\tstruct list_head *free_list,",
            "\t\t\t\t\tenum bpf_lru_list_type tgt_free_type)",
            "{",
            "\tif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)))",
            "\t\treturn;",
            "",
            "\t/* If the removing node is the next_inactive_rotation candidate,",
            "\t * move the next_inactive_rotation pointer also.",
            "\t */",
            "\tif (&node->list == l->next_inactive_rotation)",
            "\t\tl->next_inactive_rotation = l->next_inactive_rotation->prev;",
            "",
            "\tbpf_lru_list_count_dec(l, node->type);",
            "",
            "\tnode->type = tgt_free_type;",
            "\tlist_move(&node->list, free_list);",
            "}",
            "static void __bpf_lru_node_move_in(struct bpf_lru_list *l,",
            "\t\t\t\t   struct bpf_lru_node *node,",
            "\t\t\t\t   enum bpf_lru_list_type tgt_type)",
            "{",
            "\tif (WARN_ON_ONCE(!IS_LOCAL_LIST_TYPE(node->type)) ||",
            "\t    WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(tgt_type)))",
            "\t\treturn;",
            "",
            "\tbpf_lru_list_count_inc(l, tgt_type);",
            "\tnode->type = tgt_type;",
            "\tbpf_lru_node_clear_ref(node);",
            "\tlist_move(&node->list, &l->lists[tgt_type]);",
            "}",
            "static void __bpf_lru_node_move(struct bpf_lru_list *l,",
            "\t\t\t\tstruct bpf_lru_node *node,",
            "\t\t\t\tenum bpf_lru_list_type tgt_type)",
            "{",
            "\tif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)) ||",
            "\t    WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(tgt_type)))",
            "\t\treturn;",
            "",
            "\tif (node->type != tgt_type) {",
            "\t\tbpf_lru_list_count_dec(l, node->type);",
            "\t\tbpf_lru_list_count_inc(l, tgt_type);",
            "\t\tnode->type = tgt_type;",
            "\t}",
            "\tbpf_lru_node_clear_ref(node);",
            "",
            "\t/* If the moving node is the next_inactive_rotation candidate,",
            "\t * move the next_inactive_rotation pointer also.",
            "\t */",
            "\tif (&node->list == l->next_inactive_rotation)",
            "\t\tl->next_inactive_rotation = l->next_inactive_rotation->prev;",
            "",
            "\tlist_move(&node->list, &l->lists[tgt_type]);",
            "}",
            "static bool bpf_lru_list_inactive_low(const struct bpf_lru_list *l)",
            "{",
            "\treturn l->counts[BPF_LRU_LIST_T_INACTIVE] <",
            "\t\tl->counts[BPF_LRU_LIST_T_ACTIVE];",
            "}",
            "static void __bpf_lru_list_rotate_active(struct bpf_lru *lru,",
            "\t\t\t\t\t struct bpf_lru_list *l)",
            "{",
            "\tstruct list_head *active = &l->lists[BPF_LRU_LIST_T_ACTIVE];",
            "\tstruct bpf_lru_node *node, *tmp_node, *first_node;",
            "\tunsigned int i = 0;",
            "",
            "\tfirst_node = list_first_entry(active, struct bpf_lru_node, list);",
            "\tlist_for_each_entry_safe_reverse(node, tmp_node, active, list) {",
            "\t\tif (bpf_lru_node_is_ref(node))",
            "\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);",
            "\t\telse",
            "\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_INACTIVE);",
            "",
            "\t\tif (++i == lru->nr_scans || node == first_node)",
            "\t\t\tbreak;",
            "\t}",
            "}"
          ],
          "function_name": "get_next_cpu, bpf_lru_node_is_ref, bpf_lru_node_clear_ref, bpf_lru_list_count_inc, bpf_lru_list_count_dec, __bpf_lru_node_move_to_free, __bpf_lru_node_move_in, __bpf_lru_node_move, bpf_lru_list_inactive_low, __bpf_lru_list_rotate_active",
          "description": "实现LRU节点状态迁移、计数维护及列表旋转逻辑，包括节点引用判断、类型转换、活跃/非活跃列表平衡操作。",
          "similarity": 0.5024926662445068
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 295,
          "end_line": 407,
          "content": [
            "static void __local_list_flush(struct bpf_lru_list *l,",
            "\t\t\t       struct bpf_lru_locallist *loc_l)",
            "{",
            "\tstruct bpf_lru_node *node, *tmp_node;",
            "",
            "\tlist_for_each_entry_safe_reverse(node, tmp_node,",
            "\t\t\t\t\t local_pending_list(loc_l), list) {",
            "\t\tif (bpf_lru_node_is_ref(node))",
            "\t\t\t__bpf_lru_node_move_in(l, node, BPF_LRU_LIST_T_ACTIVE);",
            "\t\telse",
            "\t\t\t__bpf_lru_node_move_in(l, node,",
            "\t\t\t\t\t       BPF_LRU_LIST_T_INACTIVE);",
            "\t}",
            "}",
            "static void bpf_lru_list_push_free(struct bpf_lru_list *l,",
            "\t\t\t\t   struct bpf_lru_node *node)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)))",
            "\t\treturn;",
            "",
            "\traw_spin_lock_irqsave(&l->lock, flags);",
            "\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_FREE);",
            "\traw_spin_unlock_irqrestore(&l->lock, flags);",
            "}",
            "static void bpf_lru_list_pop_free_to_local(struct bpf_lru *lru,",
            "\t\t\t\t\t   struct bpf_lru_locallist *loc_l)",
            "{",
            "\tstruct bpf_lru_list *l = &lru->common_lru.lru_list;",
            "\tstruct bpf_lru_node *node, *tmp_node;",
            "\tunsigned int nfree = 0;",
            "",
            "\traw_spin_lock(&l->lock);",
            "",
            "\t__local_list_flush(l, loc_l);",
            "",
            "\t__bpf_lru_list_rotate(lru, l);",
            "",
            "\tlist_for_each_entry_safe(node, tmp_node, &l->lists[BPF_LRU_LIST_T_FREE],",
            "\t\t\t\t list) {",
            "\t\t__bpf_lru_node_move_to_free(l, node, local_free_list(loc_l),",
            "\t\t\t\t\t    BPF_LRU_LOCAL_LIST_T_FREE);",
            "\t\tif (++nfree == lru->target_free)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tif (nfree < lru->target_free)",
            "\t\t__bpf_lru_list_shrink(lru, l, lru->target_free - nfree,",
            "\t\t\t\t      local_free_list(loc_l),",
            "\t\t\t\t      BPF_LRU_LOCAL_LIST_T_FREE);",
            "",
            "\traw_spin_unlock(&l->lock);",
            "}",
            "static void __local_list_add_pending(struct bpf_lru *lru,",
            "\t\t\t\t     struct bpf_lru_locallist *loc_l,",
            "\t\t\t\t     int cpu,",
            "\t\t\t\t     struct bpf_lru_node *node,",
            "\t\t\t\t     u32 hash)",
            "{",
            "\t*(u32 *)((void *)node + lru->hash_offset) = hash;",
            "\tnode->cpu = cpu;",
            "\tnode->type = BPF_LRU_LOCAL_LIST_T_PENDING;",
            "\tbpf_lru_node_clear_ref(node);",
            "\tlist_add(&node->list, local_pending_list(loc_l));",
            "}",
            "static void bpf_common_lru_push_free(struct bpf_lru *lru,",
            "\t\t\t\t     struct bpf_lru_node *node)",
            "{",
            "\tu8 node_type = READ_ONCE(node->type);",
            "\tunsigned long flags;",
            "",
            "\tif (WARN_ON_ONCE(node_type == BPF_LRU_LIST_T_FREE) ||",
            "\t    WARN_ON_ONCE(node_type == BPF_LRU_LOCAL_LIST_T_FREE))",
            "\t\treturn;",
            "",
            "\tif (node_type == BPF_LRU_LOCAL_LIST_T_PENDING) {",
            "\t\tstruct bpf_lru_locallist *loc_l;",
            "",
            "\t\tloc_l = per_cpu_ptr(lru->common_lru.local_list, node->cpu);",
            "",
            "\t\traw_spin_lock_irqsave(&loc_l->lock, flags);",
            "",
            "\t\tif (unlikely(node->type != BPF_LRU_LOCAL_LIST_T_PENDING)) {",
            "\t\t\traw_spin_unlock_irqrestore(&loc_l->lock, flags);",
            "\t\t\tgoto check_lru_list;",
            "\t\t}",
            "",
            "\t\tnode->type = BPF_LRU_LOCAL_LIST_T_FREE;",
            "\t\tbpf_lru_node_clear_ref(node);",
            "\t\tlist_move(&node->list, local_free_list(loc_l));",
            "",
            "\t\traw_spin_unlock_irqrestore(&loc_l->lock, flags);",
            "\t\treturn;",
            "\t}",
            "",
            "check_lru_list:",
            "\tbpf_lru_list_push_free(&lru->common_lru.lru_list, node);",
            "}",
            "static void bpf_percpu_lru_push_free(struct bpf_lru *lru,",
            "\t\t\t\t     struct bpf_lru_node *node)",
            "{",
            "\tstruct bpf_lru_list *l;",
            "\tunsigned long flags;",
            "",
            "\tl = per_cpu_ptr(lru->percpu_lru, node->cpu);",
            "",
            "\traw_spin_lock_irqsave(&l->lock, flags);",
            "",
            "\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_FREE);",
            "",
            "\traw_spin_unlock_irqrestore(&l->lock, flags);",
            "}"
          ],
          "function_name": "__local_list_flush, bpf_lru_list_push_free, bpf_lru_list_pop_free_to_local, __local_list_add_pending, bpf_common_lru_push_free, bpf_percpu_lru_push_free",
          "description": "管理Per-CPU本地列表的刷新与节点分发，实现空闲节点向本地列表迁移及跨CPU的节点调度控制。",
          "similarity": 0.5008429884910583
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 697,
          "end_line": 703,
          "content": [
            "void bpf_lru_destroy(struct bpf_lru *lru)",
            "{",
            "\tif (lru->percpu)",
            "\t\tfree_percpu(lru->percpu_lru);",
            "\telse",
            "\t\tfree_percpu(lru->common_lru.local_list);",
            "}"
          ],
          "function_name": "bpf_lru_destroy",
          "description": "该代码段实现了一个用于销毁BPF LRU结构体的函数，核心功能是释放与per-CPU关联的LRU列表内存。函数根据`percpu`标志选择性地调用`free_percpu`释放`percpu_lru`或`common_lru.local_list`。由于代码仅展示内存释放逻辑且未包含完整结构体定义，存在上下文不完整的风险。",
          "similarity": 0.48095470666885376
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 556,
          "end_line": 689,
          "content": [
            "void bpf_lru_push_free(struct bpf_lru *lru, struct bpf_lru_node *node)",
            "{",
            "\tif (lru->percpu)",
            "\t\tbpf_percpu_lru_push_free(lru, node);",
            "\telse",
            "\t\tbpf_common_lru_push_free(lru, node);",
            "}",
            "static void bpf_common_lru_populate(struct bpf_lru *lru, void *buf,",
            "\t\t\t\t    u32 node_offset, u32 elem_size,",
            "\t\t\t\t    u32 nr_elems)",
            "{",
            "\tstruct bpf_lru_list *l = &lru->common_lru.lru_list;",
            "\tu32 i;",
            "",
            "\tfor (i = 0; i < nr_elems; i++) {",
            "\t\tstruct bpf_lru_node *node;",
            "",
            "\t\tnode = (struct bpf_lru_node *)(buf + node_offset);",
            "\t\tnode->type = BPF_LRU_LIST_T_FREE;",
            "\t\tbpf_lru_node_clear_ref(node);",
            "\t\tlist_add(&node->list, &l->lists[BPF_LRU_LIST_T_FREE]);",
            "\t\tbuf += elem_size;",
            "\t}",
            "",
            "\tlru->target_free = clamp((nr_elems / num_possible_cpus()) / 2,",
            "\t\t\t\t 1, LOCAL_FREE_TARGET);",
            "}",
            "static void bpf_percpu_lru_populate(struct bpf_lru *lru, void *buf,",
            "\t\t\t\t    u32 node_offset, u32 elem_size,",
            "\t\t\t\t    u32 nr_elems)",
            "{",
            "\tu32 i, pcpu_entries;",
            "\tint cpu;",
            "\tstruct bpf_lru_list *l;",
            "",
            "\tpcpu_entries = nr_elems / num_possible_cpus();",
            "",
            "\ti = 0;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct bpf_lru_node *node;",
            "",
            "\t\tl = per_cpu_ptr(lru->percpu_lru, cpu);",
            "again:",
            "\t\tnode = (struct bpf_lru_node *)(buf + node_offset);",
            "\t\tnode->cpu = cpu;",
            "\t\tnode->type = BPF_LRU_LIST_T_FREE;",
            "\t\tbpf_lru_node_clear_ref(node);",
            "\t\tlist_add(&node->list, &l->lists[BPF_LRU_LIST_T_FREE]);",
            "\t\ti++;",
            "\t\tbuf += elem_size;",
            "\t\tif (i == nr_elems)",
            "\t\t\tbreak;",
            "\t\tif (i % pcpu_entries)",
            "\t\t\tgoto again;",
            "\t}",
            "}",
            "void bpf_lru_populate(struct bpf_lru *lru, void *buf, u32 node_offset,",
            "\t\t      u32 elem_size, u32 nr_elems)",
            "{",
            "\tif (lru->percpu)",
            "\t\tbpf_percpu_lru_populate(lru, buf, node_offset, elem_size,",
            "\t\t\t\t\tnr_elems);",
            "\telse",
            "\t\tbpf_common_lru_populate(lru, buf, node_offset, elem_size,",
            "\t\t\t\t\tnr_elems);",
            "}",
            "static void bpf_lru_locallist_init(struct bpf_lru_locallist *loc_l, int cpu)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < NR_BPF_LRU_LOCAL_LIST_T; i++)",
            "\t\tINIT_LIST_HEAD(&loc_l->lists[i]);",
            "",
            "\tloc_l->next_steal = cpu;",
            "",
            "\traw_spin_lock_init(&loc_l->lock);",
            "}",
            "static void bpf_lru_list_init(struct bpf_lru_list *l)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < NR_BPF_LRU_LIST_T; i++)",
            "\t\tINIT_LIST_HEAD(&l->lists[i]);",
            "",
            "\tfor (i = 0; i < NR_BPF_LRU_LIST_COUNT; i++)",
            "\t\tl->counts[i] = 0;",
            "",
            "\tl->next_inactive_rotation = &l->lists[BPF_LRU_LIST_T_INACTIVE];",
            "",
            "\traw_spin_lock_init(&l->lock);",
            "}",
            "int bpf_lru_init(struct bpf_lru *lru, bool percpu, u32 hash_offset,",
            "\t\t del_from_htab_func del_from_htab, void *del_arg)",
            "{",
            "\tint cpu;",
            "",
            "\tif (percpu) {",
            "\t\tlru->percpu_lru = alloc_percpu(struct bpf_lru_list);",
            "\t\tif (!lru->percpu_lru)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tstruct bpf_lru_list *l;",
            "",
            "\t\t\tl = per_cpu_ptr(lru->percpu_lru, cpu);",
            "\t\t\tbpf_lru_list_init(l);",
            "\t\t}",
            "\t\tlru->nr_scans = PERCPU_NR_SCANS;",
            "\t} else {",
            "\t\tstruct bpf_common_lru *clru = &lru->common_lru;",
            "",
            "\t\tclru->local_list = alloc_percpu(struct bpf_lru_locallist);",
            "\t\tif (!clru->local_list)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tstruct bpf_lru_locallist *loc_l;",
            "",
            "\t\t\tloc_l = per_cpu_ptr(clru->local_list, cpu);",
            "\t\t\tbpf_lru_locallist_init(loc_l, cpu);",
            "\t\t}",
            "",
            "\t\tbpf_lru_list_init(&clru->lru_list);",
            "\t\tlru->nr_scans = LOCAL_NR_SCANS;",
            "\t}",
            "",
            "\tlru->percpu = percpu;",
            "\tlru->del_from_htab = del_from_htab;",
            "\tlru->del_arg = del_arg;",
            "\tlru->hash_offset = hash_offset;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "bpf_lru_push_free, bpf_common_lru_populate, bpf_percpu_lru_populate, bpf_lru_populate, bpf_lru_locallist_init, bpf_lru_list_init, bpf_lru_init",
          "description": "初始化LRU系统各组件，包含全局/Per-CPU列表结构初始化、节点预填充配置及运行时参数设置。",
          "similarity": 0.4265724718570709
        }
      ]
    },
    {
      "source_file": "mm/vmscan.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:33:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmscan.c`\n\n---\n\n# vmscan.c 技术文档\n\n## 1. 文件概述\n\n`vmscan.c` 是 Linux 内核内存管理子系统中的核心文件，主要负责**页面回收（page reclaim）**机制的实现。该文件实现了内核在内存压力下如何选择并释放不再活跃或可回收的物理页帧（pages），以维持系统可用内存水位。其核心功能包括：\n\n- 实现 `kswapd` 内核线程，用于后台异步回收内存\n- 提供直接回收（direct reclaim）路径，供分配器在内存不足时同步触发\n- 管理匿名页（anonymous pages）和文件缓存页（file-backed pages）的回收策略\n- 支持基于内存控制组（memcg）的层级化内存回收\n- 与交换（swap）、压缩（compaction）、OOM killer 等子系统协同工作\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct scan_control`**  \n  页面回收上下文控制结构，包含本次回收操作的所有参数和状态：\n  - `nr_to_reclaim`：目标回收页数\n  - `target_mem_cgroup`：目标内存 cgroup（用于 memcg 回收）\n  - `may_unmap` / `may_swap` / `may_writepage`：控制是否允许解除映射、交换、写回\n  - `priority`：扫描优先级（0~12，值越低压力越大）\n  - `order`：请求分配的阶数（影响回收激进程度）\n  - `nr_scanned` / `nr_reclaimed`：已扫描和已回收页数统计\n  - `anon_cost` / `file_cost`：用于平衡匿名页与文件页回收比例\n\n- **全局变量**\n  - `vm_swappiness`（默认 60）：控制系统倾向于回收匿名页（需 swap）还是文件页（可丢弃）\n\n### 主要函数（部分在代码片段中体现）\n\n- `cgroup_reclaim()` / `root_reclaim()`：判断当前回收是否针对特定 memcg 或全局\n- `writeback_throttling_sane()`：判断是否可使用标准脏页限流机制\n- `set_task_reclaim_state()` / `flush_reclaim_state()`：管理任务的 slab 回收状态\n- （注：核心回收函数如 `shrink_lruvec()`、`kswapd()` 等未在片段中展示）\n\n## 3. 关键实现\n\n### 内存回收控制逻辑\n\n- **回收目标决策**：通过 `scan_control` 结构传递回收上下文，区分直接回收（分配失败触发）与 kswapd 后台回收。\n- **LRU 链管理**：利用 `prefetchw_prev_lru_folio` 宏优化 LRU 链遍历时的 CPU 缓存预取性能。\n- **Memcg 集成**：\n  - 若 `target_mem_cgroup` 非空，则优先回收该 cgroup 的内存\n  - 支持 `memory.low` 保护机制：当常规回收无法满足需求且跳过受保护 cgroup 时，会触发二次强制回收（`memcg_low_reclaim`）\n- **脏页处理策略**：\n  - 在传统 memcg 模式下，禁用标准 `balance_dirty_pages()` 限流，改用直接阻塞回收（`writeback_throttling_sane()` 判断）\n  - 通过 `may_writepage` 控制是否在 laptop mode 下批量写回脏页\n\n### 回收统计与状态同步\n\n- **Slab 回收计数**：通过 `reclaim_state` 结构将非 LRU 回收（如 slab 释放）计入全局统计，但**仅在全局回收时计入**，避免 memcg 回收时高估实际效果导致欠回收。\n- **PSI/Trace 集成**：包含 `<trace/events/vmscan.h>` 用于性能分析，支持压力状态指示器（PSI）监控内存压力。\n\n## 4. 依赖关系\n\n### 头文件依赖\n\n- **核心内存管理**：`<linux/mm.h>`, `<linux/gfp.h>`, `<linux/swap.h>`, `<linux/vmstat.h>`\n- **LRU 与反向映射**：`<linux/rmap.h>`, `<linux/pagemap.h>`\n- **内存控制组**：`<linux/memcontrol.h>`\n- **IO 与写回**：`<linux/writeback.h>`, `<linux/backing-dev.h>`\n- **压缩与迁移**：`<linux/compaction.h>`, `<linux/migrate.h>`\n- **体系结构相关**：`<asm/tlbflush.h>`\n\n### 子系统交互\n\n- **Swap 子系统**：通过 `swapops.h` 和 `swap.h` 实现匿名页换出\n- **Slab 分配器**：通过 `reclaim_state` 接收 slab 回收通知\n- **OOM Killer**：当回收无法释放足够内存时触发\n- **Khugepaged**：大页合并/拆分与回收协同\n- **Memory Tiering**：支持分层内存架构中的页降级（demotion）控制\n\n## 5. 使用场景\n\n- **内存分配失败时的直接回收**：当 `alloc_pages()` 等分配函数无法满足请求时，同步调用回收路径。\n- **kswapd 后台回收**：当空闲内存低于 `watermark[low]` 时，唤醒 `kswapd` 线程异步回收至 `watermark[high]`。\n- **Memcg 内存超限时的层级回收**：当某个 cgroup 超过其内存限制时，仅回收该 cgroup 及其子树的页面。\n- **系统休眠（Hibernation）**：通过 `hibernation_mode` 标志优化休眠过程中的内存回收。\n- **主动内存回收（Proactive Reclaim）**：用户空间通过 `memory.reclaim` 接口触发预清回收。\n- **内存压缩准备**：当 `compaction_ready` 置位时，回收操作会为后续内存压缩腾出连续空间。",
      "similarity": 0.6127020120620728,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/vmscan.c",
          "start_line": 343,
          "end_line": 443,
          "content": [
            "unsigned long zone_reclaimable_pages(struct zone *zone)",
            "{",
            "\tunsigned long nr;",
            "",
            "\tnr = zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_FILE) +",
            "\t\tzone_page_state_snapshot(zone, NR_ZONE_ACTIVE_FILE);",
            "\tif (can_reclaim_anon_pages(NULL, zone_to_nid(zone), NULL))",
            "\t\tnr += zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_ANON) +",
            "\t\t\tzone_page_state_snapshot(zone, NR_ZONE_ACTIVE_ANON);",
            "\t/*",
            "\t * If there are no reclaimable file-backed or anonymous pages,",
            "\t * ensure zones with sufficient free pages are not skipped.",
            "\t * This prevents zones like DMA32 from being ignored in reclaim",
            "\t * scenarios where they can still help alleviate memory pressure.",
            "\t */",
            "\tif (nr == 0)",
            "\t\tnr = zone_page_state_snapshot(zone, NR_FREE_PAGES);",
            "\treturn nr;",
            "}",
            "static unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru,",
            "\t\t\t\t     int zone_idx)",
            "{",
            "\tunsigned long size = 0;",
            "\tint zid;",
            "",
            "\tfor (zid = 0; zid <= zone_idx; zid++) {",
            "\t\tstruct zone *zone = &lruvec_pgdat(lruvec)->node_zones[zid];",
            "",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!mem_cgroup_disabled())",
            "\t\t\tsize += mem_cgroup_get_zone_lru_size(lruvec, lru, zid);",
            "\t\telse",
            "\t\t\tsize += zone_page_state(zone, NR_ZONE_LRU_BASE + lru);",
            "\t}",
            "\treturn size;",
            "}",
            "static unsigned long drop_slab_node(int nid)",
            "{",
            "\tunsigned long freed = 0;",
            "\tstruct mem_cgroup *memcg = NULL;",
            "",
            "\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);",
            "\tdo {",
            "\t\tfreed += shrink_slab(GFP_KERNEL, nid, memcg, 0);",
            "\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)) != NULL);",
            "",
            "\treturn freed;",
            "}",
            "void drop_slab(void)",
            "{",
            "\tint nid;",
            "\tint shift = 0;",
            "\tunsigned long freed;",
            "",
            "\tdo {",
            "\t\tfreed = 0;",
            "\t\tfor_each_online_node(nid) {",
            "\t\t\tif (fatal_signal_pending(current))",
            "\t\t\t\treturn;",
            "",
            "\t\t\tfreed += drop_slab_node(nid);",
            "\t\t}",
            "\t} while ((freed >> shift++) > 1);",
            "}",
            "static int reclaimer_offset(void)",
            "{",
            "\tBUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=",
            "\t\t\tPGDEMOTE_DIRECT - PGDEMOTE_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=",
            "\t\t\tPGDEMOTE_KHUGEPAGED - PGDEMOTE_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=",
            "\t\t\tPGSCAN_DIRECT - PGSCAN_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=",
            "\t\t\tPGSCAN_KHUGEPAGED - PGSCAN_KSWAPD);",
            "",
            "\tif (current_is_kswapd())",
            "\t\treturn 0;",
            "\tif (current_is_khugepaged())",
            "\t\treturn PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD;",
            "\treturn PGSTEAL_DIRECT - PGSTEAL_KSWAPD;",
            "}",
            "static inline int is_page_cache_freeable(struct folio *folio)",
            "{",
            "\t/*",
            "\t * A freeable page cache folio is referenced only by the caller",
            "\t * that isolated the folio, the page cache and optional filesystem",
            "\t * private data at folio->private.",
            "\t */",
            "\treturn folio_ref_count(folio) - folio_test_private(folio) ==",
            "\t\t1 + folio_nr_pages(folio);",
            "}",
            "static void handle_write_error(struct address_space *mapping,",
            "\t\t\t\tstruct folio *folio, int error)",
            "{",
            "\tfolio_lock(folio);",
            "\tif (folio_mapping(folio) == mapping)",
            "\t\tmapping_set_error(mapping, error);",
            "\tfolio_unlock(folio);",
            "}"
          ],
          "function_name": "zone_reclaimable_pages, lruvec_lru_size, drop_slab_node, drop_slab, reclaimer_offset, is_page_cache_freeable, handle_write_error",
          "description": "实现区域可回收页数计算、Slab对象释放及回收偏移量调整逻辑，通过遍历各内存区域统计潜在可回收页数，提供Slab内存碎片回收机制并维护回收进程优先级偏移量。",
          "similarity": 0.6755145788192749
        },
        {
          "chunk_id": 41,
          "file_path": "mm/vmscan.c",
          "start_line": 6709,
          "end_line": 6981,
          "content": [
            "static bool kswapd_shrink_node(pg_data_t *pgdat,",
            "\t\t\t       struct scan_control *sc)",
            "{",
            "\tstruct zone *zone;",
            "\tint z;",
            "\tunsigned long nr_reclaimed = sc->nr_reclaimed;",
            "",
            "\t/* Reclaim a number of pages proportional to the number of zones */",
            "\tsc->nr_to_reclaim = 0;",
            "\tfor (z = 0; z <= sc->reclaim_idx; z++) {",
            "\t\tzone = pgdat->node_zones + z;",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tsc->nr_to_reclaim += max(high_wmark_pages(zone), SWAP_CLUSTER_MAX);",
            "\t}",
            "",
            "\t/*",
            "\t * Historically care was taken to put equal pressure on all zones but",
            "\t * now pressure is applied based on node LRU order.",
            "\t */",
            "\tshrink_node(pgdat, sc);",
            "",
            "\t/*",
            "\t * Fragmentation may mean that the system cannot be rebalanced for",
            "\t * high-order allocations. If twice the allocation size has been",
            "\t * reclaimed then recheck watermarks only at order-0 to prevent",
            "\t * excessive reclaim. Assume that a process requested a high-order",
            "\t * can direct reclaim/compact.",
            "\t */",
            "\tif (sc->order && sc->nr_reclaimed >= compact_gap(sc->order))",
            "\t\tsc->order = 0;",
            "",
            "\t/* account for progress from mm_account_reclaimed_pages() */",
            "\treturn max(sc->nr_scanned, sc->nr_reclaimed - nr_reclaimed) >= sc->nr_to_reclaim;",
            "}",
            "static inline void",
            "update_reclaim_active(pg_data_t *pgdat, int highest_zoneidx, bool active)",
            "{",
            "\tint i;",
            "\tstruct zone *zone;",
            "",
            "\tfor (i = 0; i <= highest_zoneidx; i++) {",
            "\t\tzone = pgdat->node_zones + i;",
            "",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (active)",
            "\t\t\tset_bit(ZONE_RECLAIM_ACTIVE, &zone->flags);",
            "\t\telse",
            "\t\t\tclear_bit(ZONE_RECLAIM_ACTIVE, &zone->flags);",
            "\t}",
            "}",
            "static inline void",
            "set_reclaim_active(pg_data_t *pgdat, int highest_zoneidx)",
            "{",
            "\tupdate_reclaim_active(pgdat, highest_zoneidx, true);",
            "}",
            "static inline void",
            "clear_reclaim_active(pg_data_t *pgdat, int highest_zoneidx)",
            "{",
            "\tupdate_reclaim_active(pgdat, highest_zoneidx, false);",
            "}",
            "static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)",
            "{",
            "\tint i;",
            "\tunsigned long nr_soft_reclaimed;",
            "\tunsigned long nr_soft_scanned;",
            "\tunsigned long pflags;",
            "\tunsigned long nr_boost_reclaim;",
            "\tunsigned long zone_boosts[MAX_NR_ZONES] = { 0, };",
            "\tbool boosted;",
            "\tstruct zone *zone;",
            "\tstruct scan_control sc = {",
            "\t\t.gfp_mask = GFP_KERNEL,",
            "\t\t.order = order,",
            "\t\t.may_unmap = 1,",
            "\t};",
            "",
            "\tset_task_reclaim_state(current, &sc.reclaim_state);",
            "\tpsi_memstall_enter(&pflags);",
            "\t__fs_reclaim_acquire(_THIS_IP_);",
            "",
            "\tcount_vm_event(PAGEOUTRUN);",
            "",
            "\t/*",
            "\t * Account for the reclaim boost. Note that the zone boost is left in",
            "\t * place so that parallel allocations that are near the watermark will",
            "\t * stall or direct reclaim until kswapd is finished.",
            "\t */",
            "\tnr_boost_reclaim = 0;",
            "\tfor (i = 0; i <= highest_zoneidx; i++) {",
            "\t\tzone = pgdat->node_zones + i;",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tnr_boost_reclaim += zone->watermark_boost;",
            "\t\tzone_boosts[i] = zone->watermark_boost;",
            "\t}",
            "\tboosted = nr_boost_reclaim;",
            "",
            "restart:",
            "\tset_reclaim_active(pgdat, highest_zoneidx);",
            "\tsc.priority = DEF_PRIORITY;",
            "\tdo {",
            "\t\tunsigned long nr_reclaimed = sc.nr_reclaimed;",
            "\t\tbool raise_priority = true;",
            "\t\tbool balanced;",
            "\t\tbool ret;",
            "",
            "\t\tsc.reclaim_idx = highest_zoneidx;",
            "",
            "\t\t/*",
            "\t\t * If the number of buffer_heads exceeds the maximum allowed",
            "\t\t * then consider reclaiming from all zones. This has a dual",
            "\t\t * purpose -- on 64-bit systems it is expected that",
            "\t\t * buffer_heads are stripped during active rotation. On 32-bit",
            "\t\t * systems, highmem pages can pin lowmem memory and shrinking",
            "\t\t * buffers can relieve lowmem pressure. Reclaim may still not",
            "\t\t * go ahead if all eligible zones for the original allocation",
            "\t\t * request are balanced to avoid excessive reclaim from kswapd.",
            "\t\t */",
            "\t\tif (buffer_heads_over_limit) {",
            "\t\t\tfor (i = MAX_NR_ZONES - 1; i >= 0; i--) {",
            "\t\t\t\tzone = pgdat->node_zones + i;",
            "\t\t\t\tif (!managed_zone(zone))",
            "\t\t\t\t\tcontinue;",
            "",
            "\t\t\t\tsc.reclaim_idx = i;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If the pgdat is imbalanced then ignore boosting and preserve",
            "\t\t * the watermarks for a later time and restart. Note that the",
            "\t\t * zone watermarks will be still reset at the end of balancing",
            "\t\t * on the grounds that the normal reclaim should be enough to",
            "\t\t * re-evaluate if boosting is required when kswapd next wakes.",
            "\t\t */",
            "\t\tbalanced = pgdat_balanced(pgdat, sc.order, highest_zoneidx);",
            "\t\tif (!balanced && nr_boost_reclaim) {",
            "\t\t\tnr_boost_reclaim = 0;",
            "\t\t\tgoto restart;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If boosting is not active then only reclaim if there are no",
            "\t\t * eligible zones. Note that sc.reclaim_idx is not used as",
            "\t\t * buffer_heads_over_limit may have adjusted it.",
            "\t\t */",
            "\t\tif (!nr_boost_reclaim && balanced)",
            "\t\t\tgoto out;",
            "",
            "\t\t/* Limit the priority of boosting to avoid reclaim writeback */",
            "\t\tif (nr_boost_reclaim && sc.priority == DEF_PRIORITY - 2)",
            "\t\t\traise_priority = false;",
            "",
            "\t\t/*",
            "\t\t * Do not writeback or swap pages for boosted reclaim. The",
            "\t\t * intent is to relieve pressure not issue sub-optimal IO",
            "\t\t * from reclaim context. If no pages are reclaimed, the",
            "\t\t * reclaim will be aborted.",
            "\t\t */",
            "\t\tsc.may_writepage = !laptop_mode && !nr_boost_reclaim;",
            "\t\tsc.may_swap = !nr_boost_reclaim;",
            "",
            "\t\t/*",
            "\t\t * Do some background aging, to give pages a chance to be",
            "\t\t * referenced before reclaiming. All pages are rotated",
            "\t\t * regardless of classzone as this is about consistent aging.",
            "\t\t */",
            "\t\tkswapd_age_node(pgdat, &sc);",
            "",
            "\t\t/*",
            "\t\t * If we're getting trouble reclaiming, start doing writepage",
            "\t\t * even in laptop mode.",
            "\t\t */",
            "\t\tif (sc.priority < DEF_PRIORITY - 2)",
            "\t\t\tsc.may_writepage = 1;",
            "",
            "\t\t/* Call soft limit reclaim before calling shrink_node. */",
            "\t\tsc.nr_scanned = 0;",
            "\t\tnr_soft_scanned = 0;",
            "\t\tnr_soft_reclaimed = memcg1_soft_limit_reclaim(pgdat, sc.order,",
            "\t\t\t\t\t\t\t      sc.gfp_mask, &nr_soft_scanned);",
            "\t\tsc.nr_reclaimed += nr_soft_reclaimed;",
            "",
            "\t\t/*",
            "\t\t * There should be no need to raise the scanning priority if",
            "\t\t * enough pages are already being scanned that that high",
            "\t\t * watermark would be met at 100% efficiency.",
            "\t\t */",
            "\t\tif (kswapd_shrink_node(pgdat, &sc))",
            "\t\t\traise_priority = false;",
            "",
            "\t\t/*",
            "\t\t * If the low watermark is met there is no need for processes",
            "\t\t * to be throttled on pfmemalloc_wait as they should not be",
            "\t\t * able to safely make forward progress. Wake them",
            "\t\t */",
            "\t\tif (waitqueue_active(&pgdat->pfmemalloc_wait) &&",
            "\t\t\t\tallow_direct_reclaim(pgdat))",
            "\t\t\twake_up_all(&pgdat->pfmemalloc_wait);",
            "",
            "\t\t/* Check if kswapd should be suspending */",
            "\t\t__fs_reclaim_release(_THIS_IP_);",
            "\t\tret = try_to_freeze();",
            "\t\t__fs_reclaim_acquire(_THIS_IP_);",
            "\t\tif (ret || kthread_should_stop())",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * Raise priority if scanning rate is too low or there was no",
            "\t\t * progress in reclaiming pages",
            "\t\t */",
            "\t\tnr_reclaimed = sc.nr_reclaimed - nr_reclaimed;",
            "\t\tnr_boost_reclaim -= min(nr_boost_reclaim, nr_reclaimed);",
            "",
            "\t\t/*",
            "\t\t * If reclaim made no progress for a boost, stop reclaim as",
            "\t\t * IO cannot be queued and it could be an infinite loop in",
            "\t\t * extreme circumstances.",
            "\t\t */",
            "\t\tif (nr_boost_reclaim && !nr_reclaimed)",
            "\t\t\tbreak;",
            "",
            "\t\tif (raise_priority || !nr_reclaimed)",
            "\t\t\tsc.priority--;",
            "\t} while (sc.priority >= 1);",
            "",
            "\tif (!sc.nr_reclaimed)",
            "\t\tpgdat->kswapd_failures++;",
            "",
            "out:",
            "\tclear_reclaim_active(pgdat, highest_zoneidx);",
            "",
            "\t/* If reclaim was boosted, account for the reclaim done in this pass */",
            "\tif (boosted) {",
            "\t\tunsigned long flags;",
            "",
            "\t\tfor (i = 0; i <= highest_zoneidx; i++) {",
            "\t\t\tif (!zone_boosts[i])",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\t/* Increments are under the zone lock */",
            "\t\t\tzone = pgdat->node_zones + i;",
            "\t\t\tspin_lock_irqsave(&zone->lock, flags);",
            "\t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);",
            "\t\t\tspin_unlock_irqrestore(&zone->lock, flags);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * As there is now likely space, wakeup kcompact to defragment",
            "\t\t * pageblocks.",
            "\t\t */",
            "\t\twakeup_kcompactd(pgdat, pageblock_order, highest_zoneidx);",
            "\t}",
            "",
            "\tsnapshot_refaults(NULL, pgdat);",
            "\t__fs_reclaim_release(_THIS_IP_);",
            "\tpsi_memstall_leave(&pflags);",
            "\tset_task_reclaim_state(current, NULL);",
            "",
            "\t/*",
            "\t * Return the order kswapd stopped reclaiming at as",
            "\t * prepare_kswapd_sleep() takes it into account. If another caller",
            "\t * entered the allocator slow path while kswapd was awake, order will",
            "\t * remain at the higher level.",
            "\t */",
            "\treturn sc.order;",
            "}"
          ],
          "function_name": "kswapd_shrink_node, update_reclaim_active, set_reclaim_active, clear_reclaim_active, balance_pgdat",
          "description": "实现节点级内存回收逻辑，kswapd_shrink_node按比例回收页面，update_reclaim_active/set_reclaim_active/clear_reclaim_active管理重分配标志位，balance_pgdat执行多阶段内存回收，包含软限制回收、优先级调整和写回控制。",
          "similarity": 0.660647451877594
        },
        {
          "chunk_id": 44,
          "file_path": "mm/vmscan.c",
          "start_line": 7321,
          "end_line": 7463,
          "content": [
            "static int __init kswapd_init(void)",
            "{",
            "\tint nid;",
            "",
            "\tswap_setup();",
            "\tfor_each_node_state(nid, N_MEMORY)",
            " \t\tkswapd_run(nid);",
            "\treturn 0;",
            "}",
            "static inline unsigned long node_unmapped_file_pages(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long file_mapped = node_page_state(pgdat, NR_FILE_MAPPED);",
            "\tunsigned long file_lru = node_page_state(pgdat, NR_INACTIVE_FILE) +",
            "\t\tnode_page_state(pgdat, NR_ACTIVE_FILE);",
            "",
            "\t/*",
            "\t * It's possible for there to be more file mapped pages than",
            "\t * accounted for by the pages on the file LRU lists because",
            "\t * tmpfs pages accounted for as ANON can also be FILE_MAPPED",
            "\t */",
            "\treturn (file_lru > file_mapped) ? (file_lru - file_mapped) : 0;",
            "}",
            "static unsigned long node_pagecache_reclaimable(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long nr_pagecache_reclaimable;",
            "\tunsigned long delta = 0;",
            "",
            "\t/*",
            "\t * If RECLAIM_UNMAP is set, then all file pages are considered",
            "\t * potentially reclaimable. Otherwise, we have to worry about",
            "\t * pages like swapcache and node_unmapped_file_pages() provides",
            "\t * a better estimate",
            "\t */",
            "\tif (node_reclaim_mode & RECLAIM_UNMAP)",
            "\t\tnr_pagecache_reclaimable = node_page_state(pgdat, NR_FILE_PAGES);",
            "\telse",
            "\t\tnr_pagecache_reclaimable = node_unmapped_file_pages(pgdat);",
            "",
            "\t/* If we can't clean pages, remove dirty pages from consideration */",
            "\tif (!(node_reclaim_mode & RECLAIM_WRITE))",
            "\t\tdelta += node_page_state(pgdat, NR_FILE_DIRTY);",
            "",
            "\t/* Watch for any possible underflows due to delta */",
            "\tif (unlikely(delta > nr_pagecache_reclaimable))",
            "\t\tdelta = nr_pagecache_reclaimable;",
            "",
            "\treturn nr_pagecache_reclaimable - delta;",
            "}",
            "static int __node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)",
            "{",
            "\t/* Minimum pages needed in order to stay on node */",
            "\tconst unsigned long nr_pages = 1 << order;",
            "\tstruct task_struct *p = current;",
            "\tunsigned int noreclaim_flag;",
            "\tstruct scan_control sc = {",
            "\t\t.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),",
            "\t\t.gfp_mask = current_gfp_context(gfp_mask),",
            "\t\t.order = order,",
            "\t\t.priority = NODE_RECLAIM_PRIORITY,",
            "\t\t.may_writepage = !!(node_reclaim_mode & RECLAIM_WRITE),",
            "\t\t.may_unmap = !!(node_reclaim_mode & RECLAIM_UNMAP),",
            "\t\t.may_swap = 1,",
            "\t\t.reclaim_idx = gfp_zone(gfp_mask),",
            "\t};",
            "\tunsigned long pflags;",
            "",
            "\ttrace_mm_vmscan_node_reclaim_begin(pgdat->node_id, order,",
            "\t\t\t\t\t   sc.gfp_mask);",
            "",
            "\tcond_resched();",
            "\tpsi_memstall_enter(&pflags);",
            "\tfs_reclaim_acquire(sc.gfp_mask);",
            "\t/*",
            "\t * We need to be able to allocate from the reserves for RECLAIM_UNMAP",
            "\t */",
            "\tnoreclaim_flag = memalloc_noreclaim_save();",
            "\tset_task_reclaim_state(p, &sc.reclaim_state);",
            "",
            "\tif (node_pagecache_reclaimable(pgdat) > pgdat->min_unmapped_pages ||",
            "\t    node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) > pgdat->min_slab_pages) {",
            "\t\t/*",
            "\t\t * Free memory by calling shrink node with increasing",
            "\t\t * priorities until we have enough memory freed.",
            "\t\t */",
            "\t\tdo {",
            "\t\t\tshrink_node(pgdat, &sc);",
            "\t\t} while (sc.nr_reclaimed < nr_pages && --sc.priority >= 0);",
            "\t}",
            "",
            "\tset_task_reclaim_state(p, NULL);",
            "\tmemalloc_noreclaim_restore(noreclaim_flag);",
            "\tfs_reclaim_release(sc.gfp_mask);",
            "\tpsi_memstall_leave(&pflags);",
            "",
            "\ttrace_mm_vmscan_node_reclaim_end(sc.nr_reclaimed);",
            "",
            "\treturn sc.nr_reclaimed >= nr_pages;",
            "}",
            "int node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)",
            "{",
            "\tint ret;",
            "",
            "\t/*",
            "\t * Node reclaim reclaims unmapped file backed pages and",
            "\t * slab pages if we are over the defined limits.",
            "\t *",
            "\t * A small portion of unmapped file backed pages is needed for",
            "\t * file I/O otherwise pages read by file I/O will be immediately",
            "\t * thrown out if the node is overallocated. So we do not reclaim",
            "\t * if less than a specified percentage of the node is used by",
            "\t * unmapped file backed pages.",
            "\t */",
            "\tif (node_pagecache_reclaimable(pgdat) <= pgdat->min_unmapped_pages &&",
            "\t    node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) <=",
            "\t    pgdat->min_slab_pages)",
            "\t\treturn NODE_RECLAIM_FULL;",
            "",
            "\t/*",
            "\t * Do not scan if the allocation should not be delayed.",
            "\t */",
            "\tif (!gfpflags_allow_blocking(gfp_mask) || (current->flags & PF_MEMALLOC))",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\t/*",
            "\t * Only run node reclaim on the local node or on nodes that do not",
            "\t * have associated processors. This will favor the local processor",
            "\t * over remote processors and spread off node memory allocations",
            "\t * as wide as possible.",
            "\t */",
            "\tif (node_state(pgdat->node_id, N_CPU) && pgdat->node_id != numa_node_id())",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\tif (test_and_set_bit(PGDAT_RECLAIM_LOCKED, &pgdat->flags))",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\tret = __node_reclaim(pgdat, gfp_mask, order);",
            "\tclear_bit_unlock(PGDAT_RECLAIM_LOCKED, &pgdat->flags);",
            "",
            "\tif (!ret)",
            "\t\tcount_vm_event(PGSCAN_ZONE_RECLAIM_FAILED);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "kswapd_init, node_unmapped_file_pages, node_pagecache_reclaimable, __node_reclaim, node_reclaim",
          "description": "实现节点级别内存回收策略，kswapd_init初始化kswapd线程，node_unmapped_file_pages计算未映射文件页，node_pagecache_reclaimable评估可回收页面数量，__node_reclaim执行节点级回收，node_reclaim决定是否触发回收操作。",
          "similarity": 0.6584720611572266
        },
        {
          "chunk_id": 1,
          "file_path": "mm/vmscan.c",
          "start_line": 195,
          "end_line": 305,
          "content": [
            "static bool cgroup_reclaim(struct scan_control *sc)",
            "{",
            "\treturn sc->target_mem_cgroup;",
            "}",
            "static bool root_reclaim(struct scan_control *sc)",
            "{",
            "\treturn !sc->target_mem_cgroup || mem_cgroup_is_root(sc->target_mem_cgroup);",
            "}",
            "static bool writeback_throttling_sane(struct scan_control *sc)",
            "{",
            "\tif (!cgroup_reclaim(sc))",
            "\t\treturn true;",
            "#ifdef CONFIG_CGROUP_WRITEBACK",
            "\tif (cgroup_subsys_on_dfl(memory_cgrp_subsys))",
            "\t\treturn true;",
            "#endif",
            "\treturn false;",
            "}",
            "static bool cgroup_reclaim(struct scan_control *sc)",
            "{",
            "\treturn false;",
            "}",
            "static bool root_reclaim(struct scan_control *sc)",
            "{",
            "\treturn true;",
            "}",
            "static bool writeback_throttling_sane(struct scan_control *sc)",
            "{",
            "\treturn true;",
            "}",
            "static void set_task_reclaim_state(struct task_struct *task,",
            "\t\t\t\t   struct reclaim_state *rs)",
            "{",
            "\t/* Check for an overwrite */",
            "\tWARN_ON_ONCE(rs && task->reclaim_state);",
            "",
            "\t/* Check for the nulling of an already-nulled member */",
            "\tWARN_ON_ONCE(!rs && !task->reclaim_state);",
            "",
            "\ttask->reclaim_state = rs;",
            "}",
            "static void flush_reclaim_state(struct scan_control *sc)",
            "{",
            "\t/*",
            "\t * Currently, reclaim_state->reclaimed includes three types of pages",
            "\t * freed outside of vmscan:",
            "\t * (1) Slab pages.",
            "\t * (2) Clean file pages from pruned inodes (on highmem systems).",
            "\t * (3) XFS freed buffer pages.",
            "\t *",
            "\t * For all of these cases, we cannot universally link the pages to a",
            "\t * single memcg. For example, a memcg-aware shrinker can free one object",
            "\t * charged to the target memcg, causing an entire page to be freed.",
            "\t * If we count the entire page as reclaimed from the memcg, we end up",
            "\t * overestimating the reclaimed amount (potentially under-reclaiming).",
            "\t *",
            "\t * Only count such pages for global reclaim to prevent under-reclaiming",
            "\t * from the target memcg; preventing unnecessary retries during memcg",
            "\t * charging and false positives from proactive reclaim.",
            "\t *",
            "\t * For uncommon cases where the freed pages were actually mostly",
            "\t * charged to the target memcg, we end up underestimating the reclaimed",
            "\t * amount. This should be fine. The freed pages will be uncharged",
            "\t * anyway, even if they are not counted here properly, and we will be",
            "\t * able to make forward progress in charging (which is usually in a",
            "\t * retry loop).",
            "\t *",
            "\t * We can go one step further, and report the uncharged objcg pages in",
            "\t * memcg reclaim, to make reporting more accurate and reduce",
            "\t * underestimation, but it's probably not worth the complexity for now.",
            "\t */",
            "\tif (current->reclaim_state && root_reclaim(sc)) {",
            "\t\tsc->nr_reclaimed += current->reclaim_state->reclaimed;",
            "\t\tcurrent->reclaim_state->reclaimed = 0;",
            "\t}",
            "}",
            "static bool can_demote(int nid, struct scan_control *sc)",
            "{",
            "\tif (!numa_demotion_enabled)",
            "\t\treturn false;",
            "\tif (sc && sc->no_demotion)",
            "\t\treturn false;",
            "\tif (next_demotion_node(nid) == NUMA_NO_NODE)",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static inline bool can_reclaim_anon_pages(struct mem_cgroup *memcg,",
            "\t\t\t\t\t  int nid,",
            "\t\t\t\t\t  struct scan_control *sc)",
            "{",
            "\tif (memcg == NULL) {",
            "\t\t/*",
            "\t\t * For non-memcg reclaim, is there",
            "\t\t * space in any swap device?",
            "\t\t */",
            "\t\tif (get_nr_swap_pages() > 0)",
            "\t\t\treturn true;",
            "\t} else {",
            "\t\t/* Is the memcg below its swap limit? */",
            "\t\tif (mem_cgroup_get_nr_swap_pages(memcg) > 0)",
            "\t\t\treturn true;",
            "\t}",
            "",
            "\t/*",
            "\t * The page can not be swapped.",
            "\t *",
            "\t * Can it be reclaimed from this node via demotion?",
            "\t */",
            "\treturn can_demote(nid, sc);",
            "}"
          ],
          "function_name": "cgroup_reclaim, root_reclaim, writeback_throttling_sane, cgroup_reclaim, root_reclaim, writeback_throttling_sane, set_task_reclaim_state, flush_reclaim_state, can_demote, can_reclaim_anon_pages",
          "description": "提供内存组回收判定逻辑与回收状态管理接口，包含判断是否针对特定内存组回收、是否允许写回操作、设置/刷新任务回收状态等功能，支持多层级回收场景决策。",
          "similarity": 0.6577688455581665
        },
        {
          "chunk_id": 35,
          "file_path": "mm/vmscan.c",
          "start_line": 5914,
          "end_line": 6023,
          "content": [
            "static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)",
            "{",
            "\tunsigned long nr_reclaimed, nr_scanned, nr_node_reclaimed;",
            "\tstruct lruvec *target_lruvec;",
            "\tbool reclaimable = false;",
            "",
            "\tif (lru_gen_enabled() && root_reclaim(sc)) {",
            "\t\tlru_gen_shrink_node(pgdat, sc);",
            "\t\treturn;",
            "\t}",
            "",
            "\ttarget_lruvec = mem_cgroup_lruvec(sc->target_mem_cgroup, pgdat);",
            "",
            "again:",
            "\tmemset(&sc->nr, 0, sizeof(sc->nr));",
            "",
            "\tnr_reclaimed = sc->nr_reclaimed;",
            "\tnr_scanned = sc->nr_scanned;",
            "",
            "\tprepare_scan_control(pgdat, sc);",
            "",
            "\tshrink_node_memcgs(pgdat, sc);",
            "",
            "\tflush_reclaim_state(sc);",
            "",
            "\tnr_node_reclaimed = sc->nr_reclaimed - nr_reclaimed;",
            "",
            "\t/* Record the subtree's reclaim efficiency */",
            "\tif (!sc->proactive)",
            "\t\tvmpressure(sc->gfp_mask, sc->target_mem_cgroup, true,",
            "\t\t\t   sc->nr_scanned - nr_scanned, nr_node_reclaimed);",
            "",
            "\tif (nr_node_reclaimed)",
            "\t\treclaimable = true;",
            "",
            "\tif (current_is_kswapd()) {",
            "\t\t/*",
            "\t\t * If reclaim is isolating dirty pages under writeback,",
            "\t\t * it implies that the long-lived page allocation rate",
            "\t\t * is exceeding the page laundering rate. Either the",
            "\t\t * global limits are not being effective at throttling",
            "\t\t * processes due to the page distribution throughout",
            "\t\t * zones or there is heavy usage of a slow backing",
            "\t\t * device. The only option is to throttle from reclaim",
            "\t\t * context which is not ideal as there is no guarantee",
            "\t\t * the dirtying process is throttled in the same way",
            "\t\t * balance_dirty_pages() manages.",
            "\t\t *",
            "\t\t * Once a node is flagged PGDAT_WRITEBACK, kswapd will",
            "\t\t * count the number of pages under pages flagged for",
            "\t\t * immediate reclaim and stall if any are encountered",
            "\t\t * in the nr_immediate check below.",
            "\t\t */",
            "\t\tif (sc->nr.writeback && sc->nr.writeback == sc->nr.taken)",
            "\t\t\tset_bit(PGDAT_WRITEBACK, &pgdat->flags);",
            "",
            "\t\t/* Allow kswapd to start writing pages during reclaim.*/",
            "\t\tif (sc->nr.unqueued_dirty == sc->nr.file_taken)",
            "\t\t\tset_bit(PGDAT_DIRTY, &pgdat->flags);",
            "",
            "\t\t/*",
            "\t\t * If kswapd scans pages marked for immediate",
            "\t\t * reclaim and under writeback (nr_immediate), it",
            "\t\t * implies that pages are cycling through the LRU",
            "\t\t * faster than they are written so forcibly stall",
            "\t\t * until some pages complete writeback.",
            "\t\t */",
            "\t\tif (sc->nr.immediate)",
            "\t\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_WRITEBACK);",
            "\t}",
            "",
            "\t/*",
            "\t * Tag a node/memcg as congested if all the dirty pages were marked",
            "\t * for writeback and immediate reclaim (counted in nr.congested).",
            "\t *",
            "\t * Legacy memcg will stall in page writeback so avoid forcibly",
            "\t * stalling in reclaim_throttle().",
            "\t */",
            "\tif (sc->nr.dirty && sc->nr.dirty == sc->nr.congested) {",
            "\t\tif (cgroup_reclaim(sc) && writeback_throttling_sane(sc))",
            "\t\t\tset_bit(LRUVEC_CGROUP_CONGESTED, &target_lruvec->flags);",
            "",
            "\t\tif (current_is_kswapd())",
            "\t\t\tset_bit(LRUVEC_NODE_CONGESTED, &target_lruvec->flags);",
            "\t}",
            "",
            "\t/*",
            "\t * Stall direct reclaim for IO completions if the lruvec is",
            "\t * node is congested. Allow kswapd to continue until it",
            "\t * starts encountering unqueued dirty pages or cycling through",
            "\t * the LRU too quickly.",
            "\t */",
            "\tif (!current_is_kswapd() && current_may_throttle() &&",
            "\t    !sc->hibernation_mode &&",
            "\t    (test_bit(LRUVEC_CGROUP_CONGESTED, &target_lruvec->flags) ||",
            "\t     test_bit(LRUVEC_NODE_CONGESTED, &target_lruvec->flags)))",
            "\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_CONGESTED);",
            "",
            "\tif (should_continue_reclaim(pgdat, nr_node_reclaimed, sc))",
            "\t\tgoto again;",
            "",
            "\t/*",
            "\t * Kswapd gives up on balancing particular nodes after too",
            "\t * many failures to reclaim anything from them and goes to",
            "\t * sleep. On reclaim progress, reset the failure counter. A",
            "\t * successful direct reclaim run will revive a dormant kswapd.",
            "\t */",
            "\tif (reclaimable)",
            "\t\tpgdat->kswapd_failures = 0;",
            "}"
          ],
          "function_name": "shrink_node",
          "description": "该函数处理节点级别的内存回收，通过遍历各zone进行页面回收，并根据回收效率设置节点状态标志位，如PGDAT_WRITEBACK和CONGESTED。若当前为kswapd线程，则根据writeback和dirty页面状态触发节流机制。",
          "similarity": 0.656815767288208
        }
      ]
    }
  ]
}