{
  "query": "实时系统资源分配与缓冲区管理策略",
  "timestamp": "2025-12-25 23:55:20",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.616657018661499,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/mempolicy.c",
          "start_line": 168,
          "end_line": 268,
          "content": [
            "static u8 get_il_weight(int node)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tu8 weight = 1;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state)",
            "\t\tweight = state->iw_table[node];",
            "\trcu_read_unlock();",
            "\treturn weight;",
            "}",
            "static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)",
            "{",
            "\tu64 sum_bw = 0;",
            "\tunsigned int cast_sum_bw, scaling_factor = 1, iw_gcd = 0;",
            "\tint nid;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tsum_bw += bw[nid];",
            "",
            "\t/* Scale bandwidths to whole numbers in the range [1, weightiness] */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t/*",
            "\t\t * Try not to perform 64-bit division.",
            "\t\t * If sum_bw < scaling_factor, then sum_bw < U32_MAX.",
            "\t\t * If sum_bw > scaling_factor, then round the weight up to 1.",
            "\t\t */",
            "\t\tscaling_factor = weightiness * bw[nid];",
            "\t\tif (bw[nid] && sum_bw < scaling_factor) {",
            "\t\t\tcast_sum_bw = (unsigned int)sum_bw;",
            "\t\t\tnew_iw[nid] = scaling_factor / cast_sum_bw;",
            "\t\t} else {",
            "\t\t\tnew_iw[nid] = 1;",
            "\t\t}",
            "\t\tif (!iw_gcd)",
            "\t\t\tiw_gcd = new_iw[nid];",
            "\t\tiw_gcd = gcd(iw_gcd, new_iw[nid]);",
            "\t}",
            "",
            "\t/* 1:2 is strictly better than 16:32. Reduce by the weights' GCD. */",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tnew_iw[nid] /= iw_gcd;",
            "}",
            "int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tunsigned int *old_bw, *new_bw;",
            "\tunsigned int bw_val;",
            "\tint i;",
            "",
            "\tbw_val = min(coords->read_bandwidth, coords->write_bandwidth);",
            "\tnew_bw = kcalloc(nr_node_ids, sizeof(unsigned int), GFP_KERNEL);",
            "\tif (!new_bw)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew_wi_state = kmalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state) {",
            "\t\tkfree(new_bw);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tnew_wi_state->mode_auto = true;",
            "\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\tnew_wi_state->iw_table[i] = 1;",
            "",
            "\t/*",
            "\t * Update bandwidth info, even in manual mode. That way, when switching",
            "\t * to auto mode in the future, iw_table can be overwritten using",
            "\t * accurate bw data.",
            "\t */",
            "\tmutex_lock(&wi_state_lock);",
            "",
            "\told_bw = node_bw_table;",
            "\tif (old_bw)",
            "\t\tmemcpy(new_bw, old_bw, nr_node_ids * sizeof(*old_bw));",
            "\tnew_bw[node] = bw_val;",
            "\tnode_bw_table = new_bw;",
            "",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state && !old_wi_state->mode_auto) {",
            "\t\t/* Manual mode; skip reducing weights and updating wi_state */",
            "\t\tmutex_unlock(&wi_state_lock);",
            "\t\tkfree(new_wi_state);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* NULL wi_state assumes auto=true; reduce weights and update wi_state*/",
            "\treduce_interleave_weights(new_bw, new_wi_state->iw_table);",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "out:",
            "\tkfree(old_bw);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_il_weight, reduce_interleave_weights, mempolicy_set_node_perf",
          "description": "实现带权交错策略的权重计算与调整逻辑，通过获取节点带宽数据动态修改权重比例，支持根据性能参数更新节点间内存分配优先级。",
          "similarity": 0.6514959335327148
        },
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.647408664226532
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.6268080472946167
        },
        {
          "chunk_id": 11,
          "file_path": "mm/mempolicy.c",
          "start_line": 1855,
          "end_line": 1971,
          "content": [
            "static int kernel_get_mempolicy(int __user *policy,",
            "\t\t\t\tunsigned long __user *nmask,",
            "\t\t\t\tunsigned long maxnode,",
            "\t\t\t\tunsigned long addr,",
            "\t\t\t\tunsigned long flags)",
            "{",
            "\tint err;",
            "\tint pval;",
            "\tnodemask_t nodes;",
            "",
            "\tif (nmask != NULL && maxnode < nr_node_ids)",
            "\t\treturn -EINVAL;",
            "",
            "\taddr = untagged_addr(addr);",
            "",
            "\terr = do_get_mempolicy(&pval, &nodes, addr, flags);",
            "",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (policy && put_user(pval, policy))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (nmask)",
            "\t\terr = copy_nodes_to_user(nmask, maxnode, &nodes);",
            "",
            "\treturn err;",
            "}",
            "bool vma_migratable(struct vm_area_struct *vma)",
            "{",
            "\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * DAX device mappings require predictable access latency, so avoid",
            "\t * incurring periodic faults.",
            "\t */",
            "\tif (vma_is_dax(vma))",
            "\t\treturn false;",
            "",
            "\tif (is_vm_hugetlb_page(vma) &&",
            "\t\t!hugepage_migration_supported(hstate_vma(vma)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Migration allocates pages in the highest zone. If we cannot",
            "\t * do so then migration (at least from node to node) is not",
            "\t * possible.",
            "\t */",
            "\tif (vma->vm_file &&",
            "\t\tgfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))",
            "\t\t\t< policy_zone)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "bool vma_policy_mof(struct vm_area_struct *vma)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->get_policy) {",
            "\t\tbool ret = false;",
            "\t\tpgoff_t ilx;\t\t/* ignored here */",
            "",
            "\t\tpol = vma->vm_ops->get_policy(vma, vma->vm_start, &ilx);",
            "\t\tif (pol && (pol->flags & MPOL_F_MOF))",
            "\t\t\tret = true;",
            "\t\tmpol_cond_put(pol);",
            "",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tpol = vma->vm_policy;",
            "\tif (!pol)",
            "\t\tpol = get_task_policy(current);",
            "",
            "\treturn pol->flags & MPOL_F_MOF;",
            "}",
            "bool apply_policy_zone(struct mempolicy *policy, enum zone_type zone)",
            "{",
            "\tenum zone_type dynamic_policy_zone = policy_zone;",
            "",
            "\tBUG_ON(dynamic_policy_zone == ZONE_MOVABLE);",
            "",
            "\t/*",
            "\t * if policy->nodes has movable memory only,",
            "\t * we apply policy when gfp_zone(gfp) = ZONE_MOVABLE only.",
            "\t *",
            "\t * policy->nodes is intersect with node_states[N_MEMORY].",
            "\t * so if the following test fails, it implies",
            "\t * policy->nodes has movable memory only.",
            "\t */",
            "\tif (!nodes_intersects(policy->nodes, node_states[N_HIGH_MEMORY]))",
            "\t\tdynamic_policy_zone = ZONE_MOVABLE;",
            "",
            "\treturn zone >= dynamic_policy_zone;",
            "}",
            "static unsigned int weighted_interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int node;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "retry:",
            "\t/* to prevent miscount use tsk->mems_allowed_seq to detect rebind */",
            "\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\tnode = current->il_prev;",
            "\tif (!current->il_weight || !node_isset(node, policy->nodes)) {",
            "\t\tnode = next_node_in(node, policy->nodes);",
            "\t\tif (read_mems_allowed_retry(cpuset_mems_cookie))",
            "\t\t\tgoto retry;",
            "\t\tif (node == MAX_NUMNODES)",
            "\t\t\treturn node;",
            "\t\tcurrent->il_prev = node;",
            "\t\tcurrent->il_weight = get_il_weight(node);",
            "\t}",
            "\tcurrent->il_weight--;",
            "\treturn node;",
            "}"
          ],
          "function_name": "kernel_get_mempolicy, vma_migratable, vma_policy_mof, apply_policy_zone, weighted_interleave_nodes",
          "description": "kernel_get_mempolicy 获取当前内存策略参数并复制到用户空间；vma_migratable 判断虚拟内存区域是否支持迁移；vma_policy_mof 检查VMA是否启用了MOF（Migration On Fault）策略；apply_policy_zone 确定当前zone是否满足策略要求；weighted_interleave_nodes 计算加权交错分配的目标节点。",
          "similarity": 0.6257592439651489
        },
        {
          "chunk_id": 13,
          "file_path": "mm/mempolicy.c",
          "start_line": 2149,
          "end_line": 2255,
          "content": [
            "static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nnodes;",
            "\tint i;",
            "\tint nid;",
            "",
            "\tnnodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nnodes)",
            "\t\treturn numa_node_id();",
            "\ttarget = ilx % nnodes;",
            "\tnid = first_node(nodemask);",
            "\tfor (i = 0; i < target; i++)",
            "\t\tnid = next_node(nid, nodemask);",
            "\treturn nid;",
            "}",
            "int huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,",
            "\t\tstruct mempolicy **mpol, nodemask_t **nodemask)",
            "{",
            "\tpgoff_t ilx;",
            "\tint nid;",
            "",
            "\tnid = numa_node_id();",
            "\t*mpol = get_vma_policy(vma, addr, hstate_vma(vma)->order, &ilx);",
            "\t*nodemask = policy_nodemask(gfp_flags, *mpol, ilx, &nid);",
            "\treturn nid;",
            "}",
            "bool init_nodemask_of_mempolicy(nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "",
            "\tif (!(mask && current->mempolicy))",
            "\t\treturn false;",
            "",
            "\ttask_lock(current);",
            "\tmempolicy = current->mempolicy;",
            "\tswitch (mempolicy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*mask = mempolicy->nodes;",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tinit_nodemask_of_node(mask, numa_node_id());",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "\ttask_unlock(current);",
            "",
            "\treturn true;",
            "}",
            "bool mempolicy_in_oom_domain(struct task_struct *tsk,",
            "\t\t\t\t\tconst nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "\tbool ret = true;",
            "",
            "\tif (!mask)",
            "\t\treturn ret;",
            "",
            "\ttask_lock(tsk);",
            "\tmempolicy = tsk->mempolicy;",
            "\tif (mempolicy && mempolicy->mode == MPOL_BIND)",
            "\t\tret = nodes_intersects(mempolicy->nodes, *mask);",
            "\ttask_unlock(tsk);",
            "",
            "\treturn ret;",
            "}",
            "static unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tint nodes;",
            "\tunsigned long nr_pages_per_node;",
            "\tint delta;",
            "\tint i;",
            "\tunsigned long nr_allocated;",
            "\tunsigned long total_allocated = 0;",
            "",
            "\tnodes = nodes_weight(pol->nodes);",
            "\tnr_pages_per_node = nr_pages / nodes;",
            "\tdelta = nr_pages - nodes * nr_pages_per_node;",
            "",
            "\tfor (i = 0; i < nodes; i++) {",
            "\t\tif (delta) {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node + 1, NULL,",
            "\t\t\t\t\tpage_array);",
            "\t\t\tdelta--;",
            "\t\t} else {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node, NULL, page_array);",
            "\t\t}",
            "",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t}",
            "",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "interleave_nid, huge_node, init_nodemask_of_mempolicy, mempolicy_in_oom_domain, alloc_pages_bulk_array_interleave",
          "description": "interleave_nid 计算简单交错分配的目标节点；huge_node 结合HugeTLB策略确定大页分配节点；init_nodemask_of_mempolicy 初始化当前进程的内存策略节点掩码；mempolicy_in_oom_domain 检查策略节点是否与OOM域重叠；alloc_pages_bulk_array_interleave 执行批量交错分配。",
          "similarity": 0.6045598387718201
        }
      ]
    },
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.6071144938468933,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/rt.c",
          "start_line": 57,
          "end_line": 159,
          "content": [
            "static int __init sched_rt_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", sched_rt_sysctls);",
            "\treturn 0;",
            "}",
            "void init_rt_rq(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_prio_array *array;",
            "\tint i;",
            "",
            "\tarray = &rt_rq->active;",
            "\tfor (i = 0; i < MAX_RT_PRIO; i++) {",
            "\t\tINIT_LIST_HEAD(array->queue + i);",
            "\t\t__clear_bit(i, array->bitmap);",
            "\t}",
            "\t/* delimiter for bitsearch: */",
            "\t__set_bit(MAX_RT_PRIO, array->bitmap);",
            "",
            "#if defined CONFIG_SMP",
            "\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\trt_rq->highest_prio.next = MAX_RT_PRIO-1;",
            "\trt_rq->overloaded = 0;",
            "\tplist_head_init(&rt_rq->pushable_tasks);",
            "#endif /* CONFIG_SMP */",
            "\t/* We start is dequeued state, because no RT tasks are queued */",
            "\trt_rq->rt_queued = 0;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\trt_rq->rt_time = 0;",
            "\trt_rq->rt_throttled = 0;",
            "\trt_rq->rt_runtime = 0;",
            "\traw_spin_lock_init(&rt_rq->rt_runtime_lock);",
            "#endif",
            "}",
            "static enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)",
            "{",
            "\tstruct rt_bandwidth *rt_b =",
            "\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);",
            "\tint idle = 0;",
            "\tint overrun;",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tfor (;;) {",
            "\t\toverrun = hrtimer_forward_now(timer, rt_b->rt_period);",
            "\t\tif (!overrun)",
            "\t\t\tbreak;",
            "",
            "\t\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "\t\tidle = do_sched_rt_period_timer(rt_b, overrun);",
            "\t\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\t}",
            "\tif (idle)",
            "\t\trt_b->rt_period_active = 0;",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "",
            "\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;",
            "}",
            "void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)",
            "{",
            "\trt_b->rt_period = ns_to_ktime(period);",
            "\trt_b->rt_runtime = runtime;",
            "",
            "\traw_spin_lock_init(&rt_b->rt_runtime_lock);",
            "",
            "\thrtimer_init(&rt_b->rt_period_timer, CLOCK_MONOTONIC,",
            "\t\t     HRTIMER_MODE_REL_HARD);",
            "\trt_b->rt_period_timer.function = sched_rt_period_timer;",
            "}",
            "static inline void do_start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tif (!rt_b->rt_period_active) {",
            "\t\trt_b->rt_period_active = 1;",
            "\t\t/*",
            "\t\t * SCHED_DEADLINE updates the bandwidth, as a run away",
            "\t\t * RT task with a DL task could hog a CPU. But DL does",
            "\t\t * not reset the period. If a deadline task was running",
            "\t\t * without an RT task running, it can cause RT tasks to",
            "\t\t * throttle when they start up. Kick the timer right away",
            "\t\t * to update the period.",
            "\t\t */",
            "\t\thrtimer_forward_now(&rt_b->rt_period_timer, ns_to_ktime(0));",
            "\t\thrtimer_start_expires(&rt_b->rt_period_timer,",
            "\t\t\t\t      HRTIMER_MODE_ABS_PINNED_HARD);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}",
            "static void start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)",
            "\t\treturn;",
            "",
            "\tdo_start_rt_bandwidth(rt_b);",
            "}",
            "static void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\thrtimer_cancel(&rt_b->rt_period_timer);",
            "}",
            "void unregister_rt_sched_group(struct task_group *tg)",
            "{",
            "\tif (tg->rt_se)",
            "\t\tdestroy_rt_bandwidth(&tg->rt_bandwidth);",
            "}"
          ],
          "function_name": "sched_rt_sysctl_init, init_rt_rq, sched_rt_period_timer, init_rt_bandwidth, do_start_rt_bandwidth, start_rt_bandwidth, destroy_rt_bandwidth, unregister_rt_sched_group",
          "description": "初始化实时调度相关数据结构，管理实时任务周期定时器，控制实时带宽分配与回收，实现基于时间片轮转的调度策略。",
          "similarity": 0.6616257429122925
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/rt.c",
          "start_line": 529,
          "end_line": 633,
          "content": [
            "static void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\tstruct sched_rt_entity *rt_se;",
            "",
            "\tint cpu = cpu_of(rq);",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tif (!rt_se)",
            "\t\t\tenqueue_top_rt_rq(rt_rq);",
            "\t\telse if (!on_rt_rq(rt_se))",
            "\t\t\tenqueue_rt_entity(rt_se, 0);",
            "",
            "\t\tif (rt_rq->highest_prio.curr < curr->prio)",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "}",
            "static void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (!rt_se) {",
            "\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "\t\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);",
            "\t}",
            "\telse if (on_rt_rq(rt_se))",
            "\t\tdequeue_rt_entity(rt_se, 0);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;",
            "}",
            "static int rt_se_boosted(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "\tstruct task_struct *p;",
            "",
            "\tif (rt_rq)",
            "\t\treturn !!rt_rq->rt_nr_boosted;",
            "",
            "\tp = rt_task_of(rt_se);",
            "\treturn p->prio != p->normal_prio;",
            "}",
            "bool sched_rt_bandwidth_account(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\treturn (hrtimer_active(&rt_b->rt_period_timer) ||",
            "\t\trt_rq->rt_time < rt_b->rt_runtime);",
            "}",
            "static void do_balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;",
            "\tint i, weight;",
            "\tu64 rt_period;",
            "",
            "\tweight = cpumask_weight(rd->span);",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\trt_period = ktime_to_ns(rt_b->rt_period);",
            "\tfor_each_cpu(i, rd->span) {",
            "\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\ts64 diff;",
            "",
            "\t\tif (iter == rt_rq)",
            "\t\t\tcontinue;",
            "",
            "\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either all rqs have inf runtime and there's nothing to steal",
            "\t\t * or __disable_runtime() below sets a specific rq to inf to",
            "\t\t * indicate its been disabled and disallow stealing.",
            "\t\t */",
            "\t\tif (iter->rt_runtime == RUNTIME_INF)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * From runqueues with spare time, take 1/n part of their",
            "\t\t * spare time, but no more than our period.",
            "\t\t */",
            "\t\tdiff = iter->rt_runtime - iter->rt_time;",
            "\t\tif (diff > 0) {",
            "\t\t\tdiff = div_u64((u64)diff, weight);",
            "\t\t\tif (rt_rq->rt_runtime + diff > rt_period)",
            "\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;",
            "\t\t\titer->rt_runtime -= diff;",
            "\t\t\trt_rq->rt_runtime += diff;",
            "\t\t\tif (rt_rq->rt_runtime == rt_period) {",
            "\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "next:",
            "\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, rt_se_boosted, sched_rt_bandwidth_account, do_balance_runtime",
          "description": "实现实时任务队列的插入/移除逻辑，跟踪运行时间消耗，通过跨CPU运行时间平衡算法实现带宽公平分配。",
          "similarity": 0.6488240957260132
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1449,
          "end_line": 1589,
          "content": [
            "static void dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rq *rq = rq_of_rt_se(rt_se);",
            "",
            "\tupdate_stats_dequeue_rt(rt_rq_of_se(rt_se), rt_se, flags);",
            "",
            "\tdequeue_rt_stack(rt_se, flags);",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\t\tif (rt_rq && rt_rq->rt_nr_running)",
            "\t\t\t__enqueue_rt_entity(rt_se, flags);",
            "\t}",
            "\tenqueue_top_rt_rq(&rq->rt);",
            "}",
            "static void",
            "enqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tif (flags & ENQUEUE_WAKEUP)",
            "\t\trt_se->timeout = 0;",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_rt(rt_rq_of_se(rt_se), rt_se);",
            "",
            "\tenqueue_rt_entity(rt_se, flags);",
            "",
            "\tif (!task_current(rq, p) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_task(rq, p);",
            "}",
            "static bool dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tdequeue_rt_entity(rt_se, flags);",
            "",
            "\tdequeue_pushable_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void",
            "requeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)",
            "{",
            "\tif (on_rt_rq(rt_se)) {",
            "\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "\t\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);",
            "",
            "\t\tif (head)",
            "\t\t\tlist_move(&rt_se->run_list, queue);",
            "\t\telse",
            "\t\t\tlist_move_tail(&rt_se->run_list, queue);",
            "\t}",
            "}",
            "static void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\trt_rq = rt_rq_of_se(rt_se);",
            "\t\trequeue_rt_entity(rt_rq, rt_se, head);",
            "\t}",
            "}",
            "static void yield_task_rt(struct rq *rq)",
            "{",
            "\trequeue_task_rt(rq, rq->curr, 0);",
            "}",
            "static int",
            "select_task_rq_rt(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tstruct rq *rq;",
            "\tbool test;",
            "",
            "\t/* For anything but wake ups, just return the task_cpu */",
            "\tif (!(flags & (WF_TTWU | WF_FORK)))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If the current task on @p's runqueue is an RT task, then",
            "\t * try to see if we can wake this RT task up on another",
            "\t * runqueue. Otherwise simply start this RT task",
            "\t * on its current runqueue.",
            "\t *",
            "\t * We want to avoid overloading runqueues. If the woken",
            "\t * task is a higher priority, then it will stay on this CPU",
            "\t * and the lower prio task should be moved to another CPU.",
            "\t * Even though this will probably make the lower prio task",
            "\t * lose its cache, we do not want to bounce a higher task",
            "\t * around just because it gave up its CPU, perhaps for a",
            "\t * lock?",
            "\t *",
            "\t * For equal prio tasks, we just let the scheduler sort it out.",
            "\t *",
            "\t * Otherwise, just let it ride on the affined RQ and the",
            "\t * post-schedule router will push the preempted task away",
            "\t *",
            "\t * This test is optimistic, if we get it wrong the load-balancer",
            "\t * will have to sort it out.",
            "\t *",
            "\t * We take into account the capacity of the CPU to ensure it fits the",
            "\t * requirement of the task - which is only important on heterogeneous",
            "\t * systems like big.LITTLE.",
            "\t */",
            "\ttest = curr &&",
            "\t       unlikely(rt_task(curr)) &&",
            "\t       (curr->nr_cpus_allowed < 2 || curr->prio <= p->prio);",
            "",
            "\tif (test || !rt_task_fits_capacity(p, cpu)) {",
            "\t\tint target = find_lowest_rq(p);",
            "",
            "\t\t/*",
            "\t\t * Bail out if we were forcing a migration to find a better",
            "\t\t * fitting CPU but our search failed.",
            "\t\t */",
            "\t\tif (!test && target != -1 && !rt_task_fits_capacity(p, target))",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\t/*",
            "\t\t * Don't bother moving it if the destination CPU is",
            "\t\t * not running a lower priority task.",
            "\t\t */",
            "\t\tif (target != -1 &&",
            "\t\t    p->prio < cpu_rq(target)->rt.highest_prio.curr)",
            "\t\t\tcpu = target;",
            "\t}",
            "",
            "out_unlock:",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "dequeue_rt_entity, enqueue_task_rt, dequeue_task_rt, requeue_rt_entity, requeue_task_rt, yield_task_rt, select_task_rq_rt",
          "description": "实现实时任务的出队逻辑、唤醒和迁移策略，提供CPU亲和性选择及负载均衡支持，维护优先级队列的动态调整。",
          "similarity": 0.6441291570663452
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/rt.c",
          "start_line": 201,
          "end_line": 311,
          "content": [
            "void free_rt_sched_group(struct task_group *tg)",
            "{",
            "\tint i;",
            "",
            "\tfor_each_possible_cpu(i) {",
            "\t\tif (tg->rt_rq)",
            "\t\t\tkfree(tg->rt_rq[i]);",
            "\t\tif (tg->rt_se)",
            "\t\t\tkfree(tg->rt_se[i]);",
            "\t}",
            "",
            "\tkfree(tg->rt_rq);",
            "\tkfree(tg->rt_se);",
            "}",
            "void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,",
            "\t\tstruct sched_rt_entity *rt_se, int cpu,",
            "\t\tstruct sched_rt_entity *parent)",
            "{",
            "\tstruct rq *rq = cpu_rq(cpu);",
            "",
            "\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\trt_rq->rt_nr_boosted = 0;",
            "\trt_rq->rq = rq;",
            "\trt_rq->tg = tg;",
            "",
            "\ttg->rt_rq[cpu] = rt_rq;",
            "\ttg->rt_se[cpu] = rt_se;",
            "",
            "\tif (!rt_se)",
            "\t\treturn;",
            "",
            "\tif (!parent)",
            "\t\trt_se->rt_rq = &rq->rt;",
            "\telse",
            "\t\trt_se->rt_rq = parent->my_q;",
            "",
            "\trt_se->my_q = rt_rq;",
            "\trt_se->parent = parent;",
            "\tINIT_LIST_HEAD(&rt_se->run_list);",
            "}",
            "int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)",
            "{",
            "\tstruct rt_rq *rt_rq;",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint i;",
            "",
            "\ttg->rt_rq = kcalloc(nr_cpu_ids, sizeof(rt_rq), GFP_KERNEL);",
            "\tif (!tg->rt_rq)",
            "\t\tgoto err;",
            "\ttg->rt_se = kcalloc(nr_cpu_ids, sizeof(rt_se), GFP_KERNEL);",
            "\tif (!tg->rt_se)",
            "\t\tgoto err;",
            "",
            "\tinit_rt_bandwidth(&tg->rt_bandwidth, ktime_to_ns(global_rt_period()), 0);",
            "",
            "\tfor_each_possible_cpu(i) {",
            "\t\trt_rq = kzalloc_node(sizeof(struct rt_rq),",
            "\t\t\t\t     GFP_KERNEL, cpu_to_node(i));",
            "\t\tif (!rt_rq)",
            "\t\t\tgoto err;",
            "",
            "\t\trt_se = kzalloc_node(sizeof(struct sched_rt_entity),",
            "\t\t\t\t     GFP_KERNEL, cpu_to_node(i));",
            "\t\tif (!rt_se)",
            "\t\t\tgoto err_free_rq;",
            "",
            "\t\tinit_rt_rq(rt_rq);",
            "\t\trt_rq->rt_runtime = tg->rt_bandwidth.rt_runtime;",
            "\t\tinit_tg_rt_entry(tg, rt_rq, rt_se, i, parent->rt_se[i]);",
            "\t}",
            "",
            "\treturn 1;",
            "",
            "err_free_rq:",
            "\tkfree(rt_rq);",
            "err:",
            "\treturn 0;",
            "}",
            "void unregister_rt_sched_group(struct task_group *tg) { }",
            "void free_rt_sched_group(struct task_group *tg) { }",
            "int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)",
            "{",
            "\treturn 1;",
            "}",
            "static inline bool need_pull_rt_task(struct rq *rq, struct task_struct *prev)",
            "{",
            "\t/* Try to pull RT tasks here if we lower this rq's prio */",
            "\treturn rq->online && rq->rt.highest_prio.curr > prev->prio;",
            "}",
            "static inline int rt_overloaded(struct rq *rq)",
            "{",
            "\treturn atomic_read(&rq->rd->rto_count);",
            "}",
            "static inline void rt_set_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tcpumask_set_cpu(rq->cpu, rq->rd->rto_mask);",
            "\t/*",
            "\t * Make sure the mask is visible before we set",
            "\t * the overload count. That is checked to determine",
            "\t * if we should look at the mask. It would be a shame",
            "\t * if we looked at the mask, but the mask was not",
            "\t * updated yet.",
            "\t *",
            "\t * Matched by the barrier in pull_rt_task().",
            "\t */",
            "\tsmp_wmb();",
            "\tatomic_inc(&rq->rd->rto_count);",
            "}"
          ],
          "function_name": "free_rt_sched_group, init_tg_rt_entry, alloc_rt_sched_group, unregister_rt_sched_group, free_rt_sched_group, alloc_rt_sched_group, need_pull_rt_task, rt_overloaded, rt_set_overload",
          "description": "分配/释放实时调度组资源，初始化任务组内的运行队列和实体结构，处理实时任务提升（boost）状态的标记与管理。",
          "similarity": 0.6288602948188782
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/sched/rt.c",
          "start_line": 2102,
          "end_line": 2228,
          "content": [
            "static void push_rt_tasks(struct rq *rq)",
            "{",
            "\t/* push_rt_task will return true if it moved an RT */",
            "\twhile (push_rt_task(rq, false))",
            "\t\t;",
            "}",
            "static int rto_next_cpu(struct root_domain *rd)",
            "{",
            "\tint next;",
            "\tint cpu;",
            "",
            "\t/*",
            "\t * When starting the IPI RT pushing, the rto_cpu is set to -1,",
            "\t * rt_next_cpu() will simply return the first CPU found in",
            "\t * the rto_mask.",
            "\t *",
            "\t * If rto_next_cpu() is called with rto_cpu is a valid CPU, it",
            "\t * will return the next CPU found in the rto_mask.",
            "\t *",
            "\t * If there are no more CPUs left in the rto_mask, then a check is made",
            "\t * against rto_loop and rto_loop_next. rto_loop is only updated with",
            "\t * the rto_lock held, but any CPU may increment the rto_loop_next",
            "\t * without any locking.",
            "\t */",
            "\tfor (;;) {",
            "",
            "\t\t/* When rto_cpu is -1 this acts like cpumask_first() */",
            "\t\tcpu = cpumask_next(rd->rto_cpu, rd->rto_mask);",
            "",
            "\t\trd->rto_cpu = cpu;",
            "",
            "\t\tif (cpu < nr_cpu_ids)",
            "\t\t\treturn cpu;",
            "",
            "\t\trd->rto_cpu = -1;",
            "",
            "\t\t/*",
            "\t\t * ACQUIRE ensures we see the @rto_mask changes",
            "\t\t * made prior to the @next value observed.",
            "\t\t *",
            "\t\t * Matches WMB in rt_set_overload().",
            "\t\t */",
            "\t\tnext = atomic_read_acquire(&rd->rto_loop_next);",
            "",
            "\t\tif (rd->rto_loop == next)",
            "\t\t\tbreak;",
            "",
            "\t\trd->rto_loop = next;",
            "\t}",
            "",
            "\treturn -1;",
            "}",
            "static inline bool rto_start_trylock(atomic_t *v)",
            "{",
            "\treturn !atomic_cmpxchg_acquire(v, 0, 1);",
            "}",
            "static inline void rto_start_unlock(atomic_t *v)",
            "{",
            "\tatomic_set_release(v, 0);",
            "}",
            "static void tell_cpu_to_push(struct rq *rq)",
            "{",
            "\tint cpu = -1;",
            "",
            "\t/* Keep the loop going if the IPI is currently active */",
            "\tatomic_inc(&rq->rd->rto_loop_next);",
            "",
            "\t/* Only one CPU can initiate a loop at a time */",
            "\tif (!rto_start_trylock(&rq->rd->rto_loop_start))",
            "\t\treturn;",
            "",
            "\traw_spin_lock(&rq->rd->rto_lock);",
            "",
            "\t/*",
            "\t * The rto_cpu is updated under the lock, if it has a valid CPU",
            "\t * then the IPI is still running and will continue due to the",
            "\t * update to loop_next, and nothing needs to be done here.",
            "\t * Otherwise it is finishing up and an ipi needs to be sent.",
            "\t */",
            "\tif (rq->rd->rto_cpu < 0)",
            "\t\tcpu = rto_next_cpu(rq->rd);",
            "",
            "\traw_spin_unlock(&rq->rd->rto_lock);",
            "",
            "\trto_start_unlock(&rq->rd->rto_loop_start);",
            "",
            "\tif (cpu >= 0) {",
            "\t\t/* Make sure the rd does not get freed while pushing */",
            "\t\tsched_get_rd(rq->rd);",
            "\t\tirq_work_queue_on(&rq->rd->rto_push_work, cpu);",
            "\t}",
            "}",
            "void rto_push_irq_work_func(struct irq_work *work)",
            "{",
            "\tstruct root_domain *rd =",
            "\t\tcontainer_of(work, struct root_domain, rto_push_work);",
            "\tstruct rq *rq;",
            "\tint cpu;",
            "",
            "\trq = this_rq();",
            "",
            "\t/*",
            "\t * We do not need to grab the lock to check for has_pushable_tasks.",
            "\t * When it gets updated, a check is made if a push is possible.",
            "\t */",
            "\tif (has_pushable_tasks(rq)) {",
            "\t\traw_spin_rq_lock(rq);",
            "\t\twhile (push_rt_task(rq, true))",
            "\t\t\t;",
            "\t\traw_spin_rq_unlock(rq);",
            "\t}",
            "",
            "\traw_spin_lock(&rd->rto_lock);",
            "",
            "\t/* Pass the IPI to the next rt overloaded queue */",
            "\tcpu = rto_next_cpu(rd);",
            "",
            "\traw_spin_unlock(&rd->rto_lock);",
            "",
            "\tif (cpu < 0) {",
            "\t\tsched_put_rd(rd);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Try the next RT overloaded CPU */",
            "\tirq_work_queue_on(&rd->rto_push_work, cpu);",
            "}"
          ],
          "function_name": "push_rt_tasks, rto_next_cpu, rto_start_trylock, rto_start_unlock, tell_cpu_to_push, rto_push_irq_work_func",
          "description": "实现实时任务批量迁移、冗余CPU迭代选择、锁竞争控制及中断工作队列驱动的跨CPU负载均衡机制。",
          "similarity": 0.6163007020950317
        }
      ]
    },
    {
      "source_file": "mm/page_alloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_alloc.c`\n\n---\n\n# page_alloc.c 技术文档\n\n## 1. 文件概述\n\n`page_alloc.c` 是 Linux 内核内存管理子系统的核心文件之一，负责物理页面的分配与释放。该文件实现了基于区域（zone）和迁移类型（migratetype）的伙伴系统（Buddy System）内存分配器，管理系统的空闲页链表，并提供高效的页面分配/回收机制。它不处理小对象分配（由 slab/slub/slob 子系统负责），而是专注于以页为单位的大块物理内存管理。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct per_cpu_pages`**：每个 CPU 的每区（per-zone）页面缓存，用于减少锁竞争，提升分配性能。\n- **`node_states[NR_NODE_STATES]`**：全局节点状态掩码数组，跟踪各 NUMA 节点的状态（如在线、有内存等）。\n- **`sysctl_lowmem_reserve_ratio[MAX_NR_ZONES]`**：各内存区域的低内存保留比例，防止高优先级区域耗尽低优先级区域的内存。\n- **`zone_names[]` 和 `migratetype_names[]`**：内存区域和页面迁移类型的名称字符串，用于调试和日志。\n- **`gfp_allowed_mask`**：全局 GFP（Get Free Page）标志掩码，控制启动早期可使用的分配标志。\n\n### 主要函数（部分声明）\n- **`__free_pages_ok()`**：内部页面释放函数，执行实际的伙伴系统合并与链表插入逻辑。\n- 各种页面分配函数（如 `alloc_pages()`、`__alloc_pages()` 等，定义在其他位置但在此文件中实现核心逻辑）。\n- 每 CPU 页面列表操作辅助宏（如 `pcp_spin_lock()`、`pcp_spin_trylock()`）。\n\n### 关键常量与标志\n- **`fpi_t` 类型及标志**：\n  - `FPI_NONE`：无特殊要求。\n  - `FPI_SKIP_REPORT_NOTIFY`：跳过空闲页报告通知。\n  - `FPI_TO_TAIL`：将页面放回空闲链表尾部（用于优化场景如内存热插拔）。\n- **`min_free_kbytes`**：系统保留的最小空闲内存（KB），影响水位线计算。\n\n## 3. 关键实现\n\n### 每 CPU 页面缓存（Per-CPU Page Caching）\n- 通过 `struct per_cpu_pages` 为每个 CPU 维护热/冷页列表，避免频繁访问全局 zone 锁。\n- 使用 `pcpu_spin_lock` 宏族安全地访问每 CPU 数据，结合 `preempt_disable()`（非 RT）或 `migrate_disable()`（RT）防止任务迁移导致访问错误 CPU 的数据。\n- 在 UP 系统上，使用 IRQ 关闭防止重入；在 SMP/RT 系统上依赖自旋锁语义。\n\n### 内存区域（Zone）与 NUMA 支持\n- 支持多种内存区域（DMA、DMA32、Normal、HighMem、Movable、Device），通过 `zone_names` 标识。\n- 实现 `lowmem_reserve_ratio` 机制，确保高区域分配不会耗尽低区域的保留内存（如 ZONE_DMA 为设备保留）。\n- 通过 `node_states` 和 per-CPU 变量（如 `numa_node`、`_numa_mem_`）支持 NUMA 和无内存节点架构。\n\n### 空闲页管理优化\n- **`FPI_TO_TAIL` 标志**：允许将页面放回空闲链表尾部，配合内存打乱（shuffle）或热插拔时批量初始化。\n- **`FPI_SKIP_REPORT_NOTIFY` 标志**：在临时取出并归还页面时不触发空闲页报告机制，减少开销。\n- **水位线与保留内存**：`min_free_kbytes` 控制最低水位，影响 OOM（Out-Of-Memory）决策和内存回收行为。\n\n### 实时内核（PREEMPT_RT）适配\n- 在 RT 内核中使用 `migrate_disable()` 替代 `preempt_disable()`，避免干扰 RT 自旋锁的优先级继承机制。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心内存管理**：`<linux/mm.h>`, `<linux/highmem.h>`, `\"internal.h\"`\n- **同步机制**：`<linux/spinlock.h>`（隐含）、`<linux/mutex.h>`\n- **NUMA 与拓扑**：`<linux/topology.h>`, `<linux/nodemask.h>`\n- **调试与追踪**：`<linux/kasan.h>`, `<trace/events/kmem.h>`, `<linux/page_owner.h>`\n- **高级特性**：`<linux/compaction.h>`, `<linux/migrate.h>`, `<linux/memcontrol.h>`\n\n### 子系统交互\n- **Slab 分配器**：本文件不处理 kmalloc，由 `slab.c` 等负责。\n- **内存回收**：与 `vmscan.c` 协同，通过水位线触发 reclaim。\n- **内存热插拔**：通过 `memory_hotplug.h` 接口管理动态内存。\n- **OOM Killer**：通过 `oom.h` 和水位线机制触发 OOM。\n- **透明大页（THP）**：与 `khugepaged` 协同进行大页分配。\n\n## 5. 使用场景\n\n- **内核内存分配**：所有以页为单位的内核内存请求（如 `alloc_pages()`）最终由本文件处理。\n- **用户空间缺页处理**：匿名页、文件页的物理页分配。\n- **内存映射（mmap）**：大块物理内存的分配与管理。\n- **内存回收与迁移**：页面回收、压缩（compaction）、迁移（migration）过程中涉及的页面释放与重新分配。\n- **系统启动与热插拔**：初始化内存区域、处理动态添加/移除内存。\n- **实时系统**：在 PREEMPT_RT 内核中提供低延迟的页面分配路径。\n- **调试与监控**：通过 page owner、KASAN、tracepoint 等机制提供内存使用追踪。",
      "similarity": 0.589594841003418,
      "chunks": [
        {
          "chunk_id": 31,
          "file_path": "mm/page_alloc.c",
          "start_line": 5966,
          "end_line": 6071,
          "content": [
            "static void setup_per_zone_lowmem_reserve(void)",
            "{",
            "\tstruct pglist_data *pgdat;",
            "\tenum zone_type i, j;",
            "",
            "\tfor_each_online_pgdat(pgdat) {",
            "\t\tfor (i = 0; i < MAX_NR_ZONES - 1; i++) {",
            "\t\t\tstruct zone *zone = &pgdat->node_zones[i];",
            "\t\t\tint ratio = sysctl_lowmem_reserve_ratio[i];",
            "\t\t\tbool clear = !ratio || !zone_managed_pages(zone);",
            "\t\t\tunsigned long managed_pages = 0;",
            "",
            "\t\t\tfor (j = i + 1; j < MAX_NR_ZONES; j++) {",
            "\t\t\t\tstruct zone *upper_zone = &pgdat->node_zones[j];",
            "",
            "\t\t\t\tmanaged_pages += zone_managed_pages(upper_zone);",
            "",
            "\t\t\t\tif (clear)",
            "\t\t\t\t\tzone->lowmem_reserve[j] = 0;",
            "\t\t\t\telse",
            "\t\t\t\t\tzone->lowmem_reserve[j] = managed_pages / ratio;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\t/* update totalreserve_pages */",
            "\tcalculate_totalreserve_pages();",
            "}",
            "static void __setup_per_zone_wmarks(void)",
            "{",
            "\tunsigned long pages_min = min_free_kbytes >> (PAGE_SHIFT - 10);",
            "\tunsigned long lowmem_pages = 0;",
            "\tstruct zone *zone;",
            "\tunsigned long flags;",
            "",
            "\t/* Calculate total number of !ZONE_HIGHMEM and !ZONE_MOVABLE pages */",
            "\tfor_each_zone(zone) {",
            "\t\tif (!is_highmem(zone) && zone_idx(zone) != ZONE_MOVABLE)",
            "\t\t\tlowmem_pages += zone_managed_pages(zone);",
            "\t}",
            "",
            "\tfor_each_zone(zone) {",
            "\t\tu64 tmp;",
            "",
            "\t\tspin_lock_irqsave(&zone->lock, flags);",
            "\t\ttmp = (u64)pages_min * zone_managed_pages(zone);",
            "\t\tdo_div(tmp, lowmem_pages);",
            "\t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {",
            "\t\t\t/*",
            "\t\t\t * __GFP_HIGH and PF_MEMALLOC allocations usually don't",
            "\t\t\t * need highmem and movable zones pages, so cap pages_min",
            "\t\t\t * to a small  value here.",
            "\t\t\t *",
            "\t\t\t * The WMARK_HIGH-WMARK_LOW and (WMARK_LOW-WMARK_MIN)",
            "\t\t\t * deltas control async page reclaim, and so should",
            "\t\t\t * not be capped for highmem and movable zones.",
            "\t\t\t */",
            "\t\t\tunsigned long min_pages;",
            "",
            "\t\t\tmin_pages = zone_managed_pages(zone) / 1024;",
            "\t\t\tmin_pages = clamp(min_pages, SWAP_CLUSTER_MAX, 128UL);",
            "\t\t\tzone->_watermark[WMARK_MIN] = min_pages;",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * If it's a lowmem zone, reserve a number of pages",
            "\t\t\t * proportionate to the zone's size.",
            "\t\t\t */",
            "\t\t\tzone->_watermark[WMARK_MIN] = tmp;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Set the kswapd watermarks distance according to the",
            "\t\t * scale factor in proportion to available memory, but",
            "\t\t * ensure a minimum size on small systems.",
            "\t\t */",
            "\t\ttmp = max_t(u64, tmp >> 2,",
            "\t\t\t    mult_frac(zone_managed_pages(zone),",
            "\t\t\t\t      watermark_scale_factor, 10000));",
            "",
            "\t\tzone->watermark_boost = 0;",
            "\t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;",
            "\t\tzone->_watermark[WMARK_HIGH] = low_wmark_pages(zone) + tmp;",
            "\t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;",
            "",
            "\t\tspin_unlock_irqrestore(&zone->lock, flags);",
            "\t}",
            "",
            "\t/* update totalreserve_pages */",
            "\tcalculate_totalreserve_pages();",
            "}",
            "void setup_per_zone_wmarks(void)",
            "{",
            "\tstruct zone *zone;",
            "\tstatic DEFINE_SPINLOCK(lock);",
            "",
            "\tspin_lock(&lock);",
            "\t__setup_per_zone_wmarks();",
            "\tspin_unlock(&lock);",
            "",
            "\t/*",
            "\t * The watermark size have changed so update the pcpu batch",
            "\t * and high limits or the limits may be inappropriate.",
            "\t */",
            "\tfor_each_zone(zone)",
            "\t\tzone_pcp_update(zone, 0);",
            "}"
          ],
          "function_name": "setup_per_zone_lowmem_reserve, __setup_per_zone_wmarks, setup_per_zone_wmarks",
          "description": "设置各内存区域的低内存保留值和水印标记，根据系统配置调整内存管理参数，确保内存分配策略适应不同场景需求。",
          "similarity": 0.6299450397491455
        },
        {
          "chunk_id": 34,
          "file_path": "mm/page_alloc.c",
          "start_line": 6473,
          "end_line": 6610,
          "content": [
            "static void split_free_pages(struct list_head *list)",
            "{",
            "\tint order;",
            "",
            "\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {",
            "\t\tstruct page *page, *next;",
            "\t\tint nr_pages = 1 << order;",
            "",
            "\t\tlist_for_each_entry_safe(page, next, &list[order], lru) {",
            "\t\t\tint i;",
            "",
            "\t\t\tpost_alloc_hook(page, order, __GFP_MOVABLE);",
            "\t\t\tif (!order)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tsplit_page(page, order);",
            "",
            "\t\t\t/* Add all subpages to the order-0 head, in sequence. */",
            "\t\t\tlist_del(&page->lru);",
            "\t\t\tfor (i = 0; i < nr_pages; i++)",
            "\t\t\t\tlist_add_tail(&page[i].lru, &list[0]);",
            "\t\t}",
            "\t}",
            "}",
            "int alloc_contig_range_noprof(unsigned long start, unsigned long end,",
            "\t\t       unsigned migratetype, gfp_t gfp_mask)",
            "{",
            "\tunsigned long outer_start, outer_end;",
            "\tint ret = 0;",
            "",
            "\tstruct compact_control cc = {",
            "\t\t.nr_migratepages = 0,",
            "\t\t.order = -1,",
            "\t\t.zone = page_zone(pfn_to_page(start)),",
            "\t\t.mode = MIGRATE_SYNC,",
            "\t\t.ignore_skip_hint = true,",
            "\t\t.no_set_skip_hint = true,",
            "\t\t.gfp_mask = current_gfp_context(gfp_mask),",
            "\t\t.alloc_contig = true,",
            "\t};",
            "\tINIT_LIST_HEAD(&cc.migratepages);",
            "",
            "\t/*",
            "\t * What we do here is we mark all pageblocks in range as",
            "\t * MIGRATE_ISOLATE.  Because pageblock and max order pages may",
            "\t * have different sizes, and due to the way page allocator",
            "\t * work, start_isolate_page_range() has special handlings for this.",
            "\t *",
            "\t * Once the pageblocks are marked as MIGRATE_ISOLATE, we",
            "\t * migrate the pages from an unaligned range (ie. pages that",
            "\t * we are interested in). This will put all the pages in",
            "\t * range back to page allocator as MIGRATE_ISOLATE.",
            "\t *",
            "\t * When this is done, we take the pages in range from page",
            "\t * allocator removing them from the buddy system.  This way",
            "\t * page allocator will never consider using them.",
            "\t *",
            "\t * This lets us mark the pageblocks back as",
            "\t * MIGRATE_CMA/MIGRATE_MOVABLE so that free pages in the",
            "\t * aligned range but not in the unaligned, original range are",
            "\t * put back to page allocator so that buddy can use them.",
            "\t */",
            "",
            "\tret = start_isolate_page_range(start, end, migratetype, 0, gfp_mask);",
            "\tif (ret)",
            "\t\tgoto done;",
            "",
            "\tdrain_all_pages(cc.zone);",
            "",
            "\t/*",
            "\t * In case of -EBUSY, we'd like to know which page causes problem.",
            "\t * So, just fall through. test_pages_isolated() has a tracepoint",
            "\t * which will report the busy page.",
            "\t *",
            "\t * It is possible that busy pages could become available before",
            "\t * the call to test_pages_isolated, and the range will actually be",
            "\t * allocated.  So, if we fall through be sure to clear ret so that",
            "\t * -EBUSY is not accidentally used or returned to caller.",
            "\t */",
            "\tret = __alloc_contig_migrate_range(&cc, start, end, migratetype);",
            "\tif (ret && ret != -EBUSY)",
            "\t\tgoto done;",
            "\tret = 0;",
            "",
            "\t/*",
            "\t * Pages from [start, end) are within a pageblock_nr_pages",
            "\t * aligned blocks that are marked as MIGRATE_ISOLATE.  What's",
            "\t * more, all pages in [start, end) are free in page allocator.",
            "\t * What we are going to do is to allocate all pages from",
            "\t * [start, end) (that is remove them from page allocator).",
            "\t *",
            "\t * The only problem is that pages at the beginning and at the",
            "\t * end of interesting range may be not aligned with pages that",
            "\t * page allocator holds, ie. they can be part of higher order",
            "\t * pages.  Because of this, we reserve the bigger range and",
            "\t * once this is done free the pages we are not interested in.",
            "\t *",
            "\t * We don't have to hold zone->lock here because the pages are",
            "\t * isolated thus they won't get removed from buddy.",
            "\t */",
            "\touter_start = find_large_buddy(start);",
            "",
            "\t/* Make sure the range is really isolated. */",
            "\tif (test_pages_isolated(outer_start, end, 0)) {",
            "\t\tret = -EBUSY;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\t/* Grab isolated pages from freelists. */",
            "\touter_end = isolate_freepages_range(&cc, outer_start, end);",
            "\tif (!outer_end) {",
            "\t\tret = -EBUSY;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\tif (!(gfp_mask & __GFP_COMP)) {",
            "\t\tsplit_free_pages(cc.freepages);",
            "",
            "\t\t/* Free head and tail (if any) */",
            "\t\tif (start != outer_start)",
            "\t\t\tfree_contig_range(outer_start, start - outer_start);",
            "\t\tif (end != outer_end)",
            "\t\t\tfree_contig_range(end, outer_end - end);",
            "\t} else if (start == outer_start && end == outer_end && is_power_of_2(end - start)) {",
            "\t\tstruct page *head = pfn_to_page(start);",
            "\t\tint order = ilog2(end - start);",
            "",
            "\t\tcheck_new_pages(head, order);",
            "\t\tprep_new_page(head, order, gfp_mask, 0);",
            "\t} else {",
            "\t\tret = -EINVAL;",
            "\t\tWARN(true, \"PFN range: requested [%lu, %lu), allocated [%lu, %lu)\\n\",",
            "\t\t     start, end, outer_start, outer_end);",
            "\t}",
            "done:",
            "\tundo_isolate_page_range(start, end, migratetype);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "split_free_pages, alloc_contig_range_noprof",
          "description": "实现连续物理内存范围分配功能，通过隔离页面块、迁移页面和调整区域状态，确保目标范围内内存可被系统使用并处理分配异常情况。",
          "similarity": 0.6235471367835999
        },
        {
          "chunk_id": 21,
          "file_path": "mm/page_alloc.c",
          "start_line": 3958,
          "end_line": 4058,
          "content": [
            "static void wake_all_kswapds(unsigned int order, gfp_t gfp_mask,",
            "\t\t\t     const struct alloc_context *ac)",
            "{",
            "\tstruct zoneref *z;",
            "\tstruct zone *zone;",
            "\tpg_data_t *last_pgdat = NULL;",
            "\tenum zone_type highest_zoneidx = ac->highest_zoneidx;",
            "",
            "\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist, highest_zoneidx,",
            "\t\t\t\t\tac->nodemask) {",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "\t\tif (last_pgdat != zone->zone_pgdat) {",
            "\t\t\twakeup_kswapd(zone, gfp_mask, order, highest_zoneidx);",
            "\t\t\tlast_pgdat = zone->zone_pgdat;",
            "\t\t}",
            "\t}",
            "}",
            "static inline unsigned int",
            "gfp_to_alloc_flags(gfp_t gfp_mask, unsigned int order)",
            "{",
            "\tunsigned int alloc_flags = ALLOC_WMARK_MIN | ALLOC_CPUSET;",
            "",
            "\t/*",
            "\t * __GFP_HIGH is assumed to be the same as ALLOC_MIN_RESERVE",
            "\t * and __GFP_KSWAPD_RECLAIM is assumed to be the same as ALLOC_KSWAPD",
            "\t * to save two branches.",
            "\t */",
            "\tBUILD_BUG_ON(__GFP_HIGH != (__force gfp_t) ALLOC_MIN_RESERVE);",
            "\tBUILD_BUG_ON(__GFP_KSWAPD_RECLAIM != (__force gfp_t) ALLOC_KSWAPD);",
            "",
            "\t/*",
            "\t * The caller may dip into page reserves a bit more if the caller",
            "\t * cannot run direct reclaim, or if the caller has realtime scheduling",
            "\t * policy or is asking for __GFP_HIGH memory.  GFP_ATOMIC requests will",
            "\t * set both ALLOC_NON_BLOCK and ALLOC_MIN_RESERVE(__GFP_HIGH).",
            "\t */",
            "\talloc_flags |= (__force int)",
            "\t\t(gfp_mask & (__GFP_HIGH | __GFP_KSWAPD_RECLAIM));",
            "",
            "\tif (!(gfp_mask & __GFP_DIRECT_RECLAIM)) {",
            "\t\t/*",
            "\t\t * Not worth trying to allocate harder for __GFP_NOMEMALLOC even",
            "\t\t * if it can't schedule.",
            "\t\t */",
            "\t\tif (!(gfp_mask & __GFP_NOMEMALLOC)) {",
            "\t\t\talloc_flags |= ALLOC_NON_BLOCK;",
            "",
            "\t\t\tif (order > 0)",
            "\t\t\t\talloc_flags |= ALLOC_HIGHATOMIC;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Ignore cpuset mems for non-blocking __GFP_HIGH (probably",
            "\t\t * GFP_ATOMIC) rather than fail, see the comment for",
            "\t\t * cpuset_node_allowed().",
            "\t\t */",
            "\t\tif (alloc_flags & ALLOC_MIN_RESERVE)",
            "\t\t\talloc_flags &= ~ALLOC_CPUSET;",
            "\t} else if (unlikely(rt_or_dl_task(current)) && in_task())",
            "\t\talloc_flags |= ALLOC_MIN_RESERVE;",
            "",
            "\talloc_flags = gfp_to_alloc_flags_cma(gfp_mask, alloc_flags);",
            "",
            "\treturn alloc_flags;",
            "}",
            "static bool oom_reserves_allowed(struct task_struct *tsk)",
            "{",
            "\tif (!tsk_is_oom_victim(tsk))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * !MMU doesn't have oom reaper so give access to memory reserves",
            "\t * only to the thread with TIF_MEMDIE set",
            "\t */",
            "\tif (!IS_ENABLED(CONFIG_MMU) && !test_thread_flag(TIF_MEMDIE))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static inline int __gfp_pfmemalloc_flags(gfp_t gfp_mask)",
            "{",
            "\tif (unlikely(gfp_mask & __GFP_NOMEMALLOC))",
            "\t\treturn 0;",
            "\tif (gfp_mask & __GFP_MEMALLOC)",
            "\t\treturn ALLOC_NO_WATERMARKS;",
            "\tif (in_serving_softirq() && (current->flags & PF_MEMALLOC))",
            "\t\treturn ALLOC_NO_WATERMARKS;",
            "\tif (!in_interrupt()) {",
            "\t\tif (current->flags & PF_MEMALLOC)",
            "\t\t\treturn ALLOC_NO_WATERMARKS;",
            "\t\telse if (oom_reserves_allowed(current))",
            "\t\t\treturn ALLOC_OOM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "bool gfp_pfmemalloc_allowed(gfp_t gfp_mask)",
            "{",
            "\treturn !!__gfp_pfmemalloc_flags(gfp_mask);",
            "}"
          ],
          "function_name": "wake_all_kswapds, gfp_to_alloc_flags, oom_reserves_allowed, __gfp_pfmemalloc_flags, gfp_pfmemalloc_allowed",
          "description": "该代码段主要处理内存分配时的策略配置与回收机制。  \n`wake_all_kswapds` 遍历内存区并唤醒对应 kswapd 线程以触发页面回收；`gfp_to_alloc_flags` 根据 GFP 标志计算分配策略标志，控制内存回收行为；`oom_reserves_allowed` 和 `__gfp_pfmemalloc_flags` 共同决定是否允许绕过内存水印限制访问 OOM 预留内存。",
          "similarity": 0.5997978448867798
        },
        {
          "chunk_id": 32,
          "file_path": "mm/page_alloc.c",
          "start_line": 6106,
          "end_line": 6209,
          "content": [
            "void calculate_min_free_kbytes(void)",
            "{",
            "\tunsigned long lowmem_kbytes;",
            "\tint new_min_free_kbytes;",
            "",
            "\tlowmem_kbytes = nr_free_buffer_pages() * (PAGE_SIZE >> 10);",
            "\tnew_min_free_kbytes = int_sqrt(lowmem_kbytes * 16);",
            "",
            "\tif (new_min_free_kbytes > user_min_free_kbytes)",
            "\t\tmin_free_kbytes = clamp(new_min_free_kbytes, 128, 262144);",
            "\telse",
            "\t\tpr_warn(\"min_free_kbytes is not updated to %d because user defined value %d is preferred\\n\",",
            "\t\t\t\tnew_min_free_kbytes, user_min_free_kbytes);",
            "",
            "}",
            "int __meminit init_per_zone_wmark_min(void)",
            "{",
            "\tcalculate_min_free_kbytes();",
            "\tsetup_per_zone_wmarks();",
            "\trefresh_zone_stat_thresholds();",
            "\tsetup_per_zone_lowmem_reserve();",
            "",
            "#ifdef CONFIG_NUMA",
            "\tsetup_min_unmapped_ratio();",
            "\tsetup_min_slab_ratio();",
            "#endif",
            "",
            "\tkhugepaged_min_free_kbytes_update();",
            "",
            "\treturn 0;",
            "}",
            "postcore_initcall(init_per_zone_wmark_min)",
            "",
            "/*",
            " * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so",
            " *\tthat we can call two helper functions whenever min_free_kbytes",
            " *\tchanges.",
            " */",
            "static int min_free_kbytes_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tif (write) {",
            "\t\tuser_min_free_kbytes = min_free_kbytes;",
            "\t\tsetup_per_zone_wmarks();",
            "\t}",
            "\treturn 0;",
            "}",
            "static int watermark_scale_factor_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tif (write)",
            "\t\tsetup_per_zone_wmarks();",
            "",
            "\treturn 0;",
            "}",
            "static void setup_min_unmapped_ratio(void)",
            "{",
            "\tpg_data_t *pgdat;",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_online_pgdat(pgdat)",
            "\t\tpgdat->min_unmapped_pages = 0;",
            "",
            "\tfor_each_zone(zone)",
            "\t\tzone->zone_pgdat->min_unmapped_pages += (zone_managed_pages(zone) *",
            "\t\t\t\t\t\t         sysctl_min_unmapped_ratio) / 100;",
            "}",
            "static int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tsetup_min_unmapped_ratio();",
            "",
            "\treturn 0;",
            "}",
            "static void setup_min_slab_ratio(void)",
            "{",
            "\tpg_data_t *pgdat;",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_online_pgdat(pgdat)",
            "\t\tpgdat->min_slab_pages = 0;",
            "",
            "\tfor_each_zone(zone)",
            "\t\tzone->zone_pgdat->min_slab_pages += (zone_managed_pages(zone) *",
            "\t\t\t\t\t\t     sysctl_min_slab_ratio) / 100;",
            "}"
          ],
          "function_name": "calculate_min_free_kbytes, init_per_zone_wmark_min, min_free_kbytes_sysctl_handler, watermark_scale_factor_sysctl_handler, setup_min_unmapped_ratio, sysctl_min_unmapped_ratio_sysctl_handler, setup_min_slab_ratio",
          "description": "计算并更新最小空闲内存阈值，初始化区域水印参数，处理sysctl接口变更，动态调整内存管理策略参数。",
          "similarity": 0.5872619152069092
        },
        {
          "chunk_id": 11,
          "file_path": "mm/page_alloc.c",
          "start_line": 1842,
          "end_line": 1946,
          "content": [
            "static inline bool boost_watermark(struct zone *zone)",
            "{",
            "\tunsigned long max_boost;",
            "",
            "\tif (!watermark_boost_factor)",
            "\t\treturn false;",
            "\t/*",
            "\t * Don't bother in zones that are unlikely to produce results.",
            "\t * On small machines, including kdump capture kernels running",
            "\t * in a small area, boosting the watermark can cause an out of",
            "\t * memory situation immediately.",
            "\t */",
            "\tif ((pageblock_nr_pages * 4) > zone_managed_pages(zone))",
            "\t\treturn false;",
            "",
            "\tmax_boost = mult_frac(zone->_watermark[WMARK_HIGH],",
            "\t\t\twatermark_boost_factor, 10000);",
            "",
            "\t/*",
            "\t * high watermark may be uninitialised if fragmentation occurs",
            "\t * very early in boot so do not boost. We do not fall",
            "\t * through and boost by pageblock_nr_pages as failing",
            "\t * allocations that early means that reclaim is not going",
            "\t * to help and it may even be impossible to reclaim the",
            "\t * boosted watermark resulting in a hang.",
            "\t */",
            "\tif (!max_boost)",
            "\t\treturn false;",
            "",
            "\tmax_boost = max(pageblock_nr_pages, max_boost);",
            "",
            "\tzone->watermark_boost = min(zone->watermark_boost + pageblock_nr_pages,",
            "\t\tmax_boost);",
            "",
            "\treturn true;",
            "}",
            "int find_suitable_fallback(struct free_area *area, unsigned int order,",
            "\t\t\tint migratetype, bool only_stealable, bool *can_steal)",
            "{",
            "\tint i;",
            "\tint fallback_mt;",
            "",
            "\tif (area->nr_free == 0)",
            "\t\treturn -1;",
            "",
            "\t*can_steal = false;",
            "\tfor (i = 0; i < MIGRATE_PCPTYPES - 1 ; i++) {",
            "\t\tfallback_mt = fallbacks[migratetype][i];",
            "\t\tif (free_area_empty(area, fallback_mt))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (can_steal_fallback(order, migratetype))",
            "\t\t\t*can_steal = true;",
            "",
            "\t\tif (!only_stealable)",
            "\t\t\treturn fallback_mt;",
            "",
            "\t\tif (*can_steal)",
            "\t\t\treturn fallback_mt;",
            "\t}",
            "",
            "\treturn -1;",
            "}",
            "static void reserve_highatomic_pageblock(struct page *page, int order,",
            "\t\t\t\t\t struct zone *zone)",
            "{",
            "\tint mt;",
            "\tunsigned long max_managed, flags;",
            "",
            "\t/*",
            "\t * The number reserved as: minimum is 1 pageblock, maximum is",
            "\t * roughly 1% of a zone. But if 1% of a zone falls below a",
            "\t * pageblock size, then don't reserve any pageblocks.",
            "\t * Check is race-prone but harmless.",
            "\t */",
            "\tif ((zone_managed_pages(zone) / 100) < pageblock_nr_pages)",
            "\t\treturn;",
            "\tmax_managed = ALIGN((zone_managed_pages(zone) / 100), pageblock_nr_pages);",
            "\tif (zone->nr_reserved_highatomic >= max_managed)",
            "\t\treturn;",
            "",
            "\tspin_lock_irqsave(&zone->lock, flags);",
            "",
            "\t/* Recheck the nr_reserved_highatomic limit under the lock */",
            "\tif (zone->nr_reserved_highatomic >= max_managed)",
            "\t\tgoto out_unlock;",
            "",
            "\t/* Yoink! */",
            "\tmt = get_pageblock_migratetype(page);",
            "\t/* Only reserve normal pageblocks (i.e., they can merge with others) */",
            "\tif (!migratetype_is_mergeable(mt))",
            "\t\tgoto out_unlock;",
            "",
            "\tif (order < pageblock_order) {",
            "\t\tif (move_freepages_block(zone, page, mt, MIGRATE_HIGHATOMIC) == -1)",
            "\t\t\tgoto out_unlock;",
            "\t\tzone->nr_reserved_highatomic += pageblock_nr_pages;",
            "\t} else {",
            "\t\tchange_pageblock_range(page, order, MIGRATE_HIGHATOMIC);",
            "\t\tzone->nr_reserved_highatomic += 1 << order;",
            "\t}",
            "",
            "out_unlock:",
            "\tspin_unlock_irqrestore(&zone->lock, flags);",
            "}"
          ],
          "function_name": "boost_watermark, find_suitable_fallback, reserve_highatomic_pageblock",
          "description": "boost_watermark提升内存水位线以应对碎片化，find_suitable_fallback查找可回退的迁移类型，reserve_highatomic_pageblock预留高原子性页面块防止分配失败，用于内存压力管理。",
          "similarity": 0.5829681754112244
        }
      ]
    }
  ]
}