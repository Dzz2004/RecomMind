{
  "query": "types of buffers",
  "timestamp": "2025-12-25 23:49:05",
  "retrieved_files": [
    {
      "source_file": "kernel/bpf/ringbuf.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:29:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\ringbuf.c`\n\n---\n\n# `bpf/ringbuf.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/ringbuf.c` 实现了 BPF（Berkeley Packet Filter）子系统中的**环形缓冲区（Ring Buffer）**机制，用于在内核与用户空间之间高效、安全地传递数据。该机制支持两种生产者模式：**内核生产者**（如 BPF 程序）和**用户空间生产者**，并提供内存映射（`mmap`）、等待队列通知、并发控制等核心功能，是 BPF 数据输出（如 perf event 替代方案）的关键基础设施。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_ringbuf`**  \n  环形缓冲区的核心结构体，包含：\n  - `waitq`：等待队列，用于通知用户空间有新数据\n  - `work`：IRQ 工作项，用于异步唤醒等待队列\n  - `mask`：环形缓冲区大小掩码（`data_sz - 1`），用于快速取模\n  - `pages` / `nr_pages`：物理页数组，支持双映射\n  - `spinlock`：用于内核生产者的自旋锁（SMP 对齐）\n  - `busy`：原子变量，用于用户空间生产者的互斥访问（避免持有自旋锁过久）\n  - `consumer_pos` / `producer_pos` / `pending_pos`：消费者、生产者和待提交位置（各自独占一页，支持不同 mmap 权限）\n  - `data[]`：实际数据存储区域（页对齐）\n\n- **`struct bpf_ringbuf_map`**  \n  封装标准 `bpf_map`，关联一个 `bpf_ringbuf` 实例。\n\n- **`struct bpf_ringbuf_hdr`**  \n  8 字节记录头，包含：\n  - `len`：记录有效载荷长度\n  - `pg_off`：记录在页内的偏移（用于跨页处理）\n\n### 主要函数\n\n- **`bpf_ringbuf_area_alloc()`**  \n  分配并初始化环形缓冲区的虚拟内存区域，采用**双映射数据页**技术简化环绕处理。\n\n- **`bpf_ringbuf_alloc()`**  \n  初始化 `bpf_ringbuf` 结构体，设置锁、等待队列、IRQ 工作项及初始位置。\n\n- **`bpf_ringbuf_free()`**  \n  释放环形缓冲区占用的虚拟内存和物理页。\n\n- **`ringbuf_map_alloc()`**  \n  BPF map 分配器回调，验证参数并创建 `bpf_ringbuf_map`。\n\n- **`ringbuf_map_free()`**  \n  BPF map 释放器回调，清理资源。\n\n- **`ringbuf_map_*_elem()` / `ringbuf_map_get_next_key()`**  \n  禁用标准 map 操作（返回 `-ENOTSUPP`），因为 ringbuf 不支持键值操作。\n\n- **`bpf_ringbuf_notify()`**  \n  IRQ 工作回调，唤醒所有等待数据的用户进程。\n\n## 3. 关键实现\n\n### 双映射数据页（Double-Mapped Data Pages）\n\n为简化环形缓冲区**环绕（wrap-around）**时的数据读取逻辑，数据页被**连续映射两次**：\n```\n[meta pages][data pages][data pages (same as first copy)]\n```\n当读取跨越缓冲区末尾时，可直接线性读取第二份映射，无需特殊处理。此设计同时适用于内核和用户空间 `mmap`。\n\n### 权限隔离与安全\n\n- **`consumer_pos` 和 `producer_pos` 各占独立页**，允许通过 `mmap` 设置不同权限：\n  - **内核生产者模式**：`producer_pos` 和数据页对用户空间为**只读**，防止篡改。\n  - **用户空间生产者模式**：仅 `consumer_pos` 对用户空间为**只读**，内核需严格验证用户提交的记录。\n\n### 并发控制策略\n\n- **内核生产者**：使用 `raw_spinlock_t` 保证多生产者安全。\n- **用户空间生产者**：使用 `atomic_t busy` 原子变量，避免在 BPF 程序回调期间长期持有 IRQ 自旋锁（可能导致死锁或延迟）。若 `busy` 被占用，`__bpf_user_ringbuf_peek()` 返回 `-EBUSY`。\n\n### 内存布局与对齐\n\n- 非 `mmap` 部分（`waitq` 到 `pending_pos`）大小由 `RINGBUF_PGOFF` 定义。\n- `consumer_pos`、`producer_pos` 和 `data` 均按 `PAGE_SIZE` 对齐，确保可独立映射。\n- 总元数据页数：`RINGBUF_NR_META_PAGES = RINGBUF_PGOFF + 2`（含 consumer/producer 页）。\n\n### 大小限制\n\n- 最大记录大小：`RINGBUF_MAX_RECORD_SZ = UINT_MAX / 4`（约 1GB）。\n- 最大缓冲区大小受 `bpf_ringbuf_hdr.pg_off`（32 位页偏移）限制，理论最大约 **64GB**。\n\n## 4. 依赖关系\n\n- **BPF 子系统**：依赖 `bpf_map` 基础设施（`bpf_map_area_alloc/free`、`bpf_map_init_from_attr`）。\n- **内存管理**：使用 `alloc_pages_node`、`vmap`/`vunmap`、`__free_page` 管理物理页和虚拟映射。\n- **同步机制**：依赖 `wait_queue`、`irq_work`、`raw_spinlock` 和 `atomic_t`。\n- **BTF（BPF Type Format）**：包含 BTF 相关头文件，可能用于未来类型验证（当前未直接使用）。\n- **用户 API**：与 `uapi/linux/bpf.h` 中的 `BPF_F_NUMA_NODE` 等标志交互。\n\n## 5. 使用场景\n\n- **BPF 程序输出数据**：替代 `bpf_perf_event_output()`，提供更低开销、更高吞吐的内核到用户空间数据通道。\n- **用户空间主动提交数据**：允许用户程序通过 ringbuf 向内核提交样本（需内核验证）。\n- **实时监控与追踪**：用于 eBPF 监控工具（如 `bpftrace`、`libbpf` 应用）高效收集内核事件。\n- **NUMA 感知分配**：支持通过 `BPF_F_NUMA_NODE` 标志在指定 NUMA 节点分配内存，优化性能。",
      "similarity": 0.5705128908157349,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 1,
          "end_line": 149,
          "content": [
            "#include <linux/bpf.h>",
            "#include <linux/btf.h>",
            "#include <linux/err.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/slab.h>",
            "#include <linux/filter.h>",
            "#include <linux/mm.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/wait.h>",
            "#include <linux/poll.h>",
            "#include <linux/kmemleak.h>",
            "#include <uapi/linux/btf.h>",
            "#include <linux/btf_ids.h>",
            "",
            "#define RINGBUF_CREATE_FLAG_MASK (BPF_F_NUMA_NODE)",
            "",
            "/* non-mmap()'able part of bpf_ringbuf (everything up to consumer page) */",
            "#define RINGBUF_PGOFF \\",
            "\t(offsetof(struct bpf_ringbuf, consumer_pos) >> PAGE_SHIFT)",
            "/* consumer page and producer page */",
            "#define RINGBUF_POS_PAGES 2",
            "#define RINGBUF_NR_META_PAGES (RINGBUF_PGOFF + RINGBUF_POS_PAGES)",
            "",
            "#define RINGBUF_MAX_RECORD_SZ (UINT_MAX/4)",
            "",
            "struct bpf_ringbuf {",
            "\twait_queue_head_t waitq;",
            "\tstruct irq_work work;",
            "\tu64 mask;",
            "\tstruct page **pages;",
            "\tint nr_pages;",
            "\traw_spinlock_t spinlock ____cacheline_aligned_in_smp;",
            "\t/* For user-space producer ring buffers, an atomic_t busy bit is used",
            "\t * to synchronize access to the ring buffers in the kernel, rather than",
            "\t * the spinlock that is used for kernel-producer ring buffers. This is",
            "\t * done because the ring buffer must hold a lock across a BPF program's",
            "\t * callback:",
            "\t *",
            "\t *    __bpf_user_ringbuf_peek() // lock acquired",
            "\t * -> program callback_fn()",
            "\t * -> __bpf_user_ringbuf_sample_release() // lock released",
            "\t *",
            "\t * It is unsafe and incorrect to hold an IRQ spinlock across what could",
            "\t * be a long execution window, so we instead simply disallow concurrent",
            "\t * access to the ring buffer by kernel consumers, and return -EBUSY from",
            "\t * __bpf_user_ringbuf_peek() if the busy bit is held by another task.",
            "\t */",
            "\tatomic_t busy ____cacheline_aligned_in_smp;",
            "\t/* Consumer and producer counters are put into separate pages to",
            "\t * allow each position to be mapped with different permissions.",
            "\t * This prevents a user-space application from modifying the",
            "\t * position and ruining in-kernel tracking. The permissions of the",
            "\t * pages depend on who is producing samples: user-space or the",
            "\t * kernel. Note that the pending counter is placed in the same",
            "\t * page as the producer, so that it shares the same cache line.",
            "\t *",
            "\t * Kernel-producer",
            "\t * ---------------",
            "\t * The producer position and data pages are mapped as r/o in",
            "\t * userspace. For this approach, bits in the header of samples are",
            "\t * used to signal to user-space, and to other producers, whether a",
            "\t * sample is currently being written.",
            "\t *",
            "\t * User-space producer",
            "\t * -------------------",
            "\t * Only the page containing the consumer position is mapped r/o in",
            "\t * user-space. User-space producers also use bits of the header to",
            "\t * communicate to the kernel, but the kernel must carefully check and",
            "\t * validate each sample to ensure that they're correctly formatted, and",
            "\t * fully contained within the ring buffer.",
            "\t */",
            "\tunsigned long consumer_pos __aligned(PAGE_SIZE);",
            "\tunsigned long producer_pos __aligned(PAGE_SIZE);",
            "\tunsigned long pending_pos;",
            "\tchar data[] __aligned(PAGE_SIZE);",
            "};",
            "",
            "struct bpf_ringbuf_map {",
            "\tstruct bpf_map map;",
            "\tstruct bpf_ringbuf *rb;",
            "};",
            "",
            "/* 8-byte ring buffer record header structure */",
            "struct bpf_ringbuf_hdr {",
            "\tu32 len;",
            "\tu32 pg_off;",
            "};",
            "",
            "static struct bpf_ringbuf *bpf_ringbuf_area_alloc(size_t data_sz, int numa_node)",
            "{",
            "\tconst gfp_t flags = GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL |",
            "\t\t\t    __GFP_NOWARN | __GFP_ZERO;",
            "\tint nr_meta_pages = RINGBUF_NR_META_PAGES;",
            "\tint nr_data_pages = data_sz >> PAGE_SHIFT;",
            "\tint nr_pages = nr_meta_pages + nr_data_pages;",
            "\tstruct page **pages, *page;",
            "\tstruct bpf_ringbuf *rb;",
            "\tsize_t array_size;",
            "\tint i;",
            "",
            "\t/* Each data page is mapped twice to allow \"virtual\"",
            "\t * continuous read of samples wrapping around the end of ring",
            "\t * buffer area:",
            "\t * ------------------------------------------------------",
            "\t * | meta pages |  real data pages  |  same data pages  |",
            "\t * ------------------------------------------------------",
            "\t * |            | 1 2 3 4 5 6 7 8 9 | 1 2 3 4 5 6 7 8 9 |",
            "\t * ------------------------------------------------------",
            "\t * |            | TA             DA | TA             DA |",
            "\t * ------------------------------------------------------",
            "\t *                               ^^^^^^^",
            "\t *                                  |",
            "\t * Here, no need to worry about special handling of wrapped-around",
            "\t * data due to double-mapped data pages. This works both in kernel and",
            "\t * when mmap()'ed in user-space, simplifying both kernel and",
            "\t * user-space implementations significantly.",
            "\t */",
            "\tarray_size = (nr_meta_pages + 2 * nr_data_pages) * sizeof(*pages);",
            "\tpages = bpf_map_area_alloc(array_size, numa_node);",
            "\tif (!pages)",
            "\t\treturn NULL;",
            "",
            "\tfor (i = 0; i < nr_pages; i++) {",
            "\t\tpage = alloc_pages_node(numa_node, flags, 0);",
            "\t\tif (!page) {",
            "\t\t\tnr_pages = i;",
            "\t\t\tgoto err_free_pages;",
            "\t\t}",
            "\t\tpages[i] = page;",
            "\t\tif (i >= nr_meta_pages)",
            "\t\t\tpages[nr_data_pages + i] = page;",
            "\t}",
            "",
            "\trb = vmap(pages, nr_meta_pages + 2 * nr_data_pages,",
            "\t\t  VM_MAP | VM_USERMAP, PAGE_KERNEL);",
            "\tif (rb) {",
            "\t\tkmemleak_not_leak(pages);",
            "\t\trb->pages = pages;",
            "\t\trb->nr_pages = nr_pages;",
            "\t\treturn rb;",
            "\t}",
            "",
            "err_free_pages:",
            "\tfor (i = 0; i < nr_pages; i++)",
            "\t\t__free_page(pages[i]);",
            "\tbpf_map_area_free(pages);",
            "\treturn NULL;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了bpf_ringbuf结构体及其相关宏，用于管理BPF环形缓冲区的元数据和数据区域。通过页面数组实现环形缓冲区的虚拟连续读取，支持用户态和内核态生产者的差异化权限控制，其中包含消费者/生产者位置指针、忙位原子变量及锁保护的元数据。",
          "similarity": 0.5177100896835327
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 335,
          "end_line": 447,
          "content": [
            "static u64 ringbuf_map_mem_usage(const struct bpf_map *map)",
            "{",
            "\tstruct bpf_ringbuf *rb;",
            "\tint nr_data_pages;",
            "\tint nr_meta_pages;",
            "\tu64 usage = sizeof(struct bpf_ringbuf_map);",
            "",
            "\trb = container_of(map, struct bpf_ringbuf_map, map)->rb;",
            "\tusage += (u64)rb->nr_pages << PAGE_SHIFT;",
            "\tnr_meta_pages = RINGBUF_NR_META_PAGES;",
            "\tnr_data_pages = map->max_entries >> PAGE_SHIFT;",
            "\tusage += (nr_meta_pages + 2 * nr_data_pages) * sizeof(struct page *);",
            "\treturn usage;",
            "}",
            "static size_t bpf_ringbuf_rec_pg_off(struct bpf_ringbuf *rb,",
            "\t\t\t\t     struct bpf_ringbuf_hdr *hdr)",
            "{",
            "\treturn ((void *)hdr - (void *)rb) >> PAGE_SHIFT;",
            "}",
            "static void bpf_ringbuf_commit(void *sample, u64 flags, bool discard)",
            "{",
            "\tunsigned long rec_pos, cons_pos;",
            "\tstruct bpf_ringbuf_hdr *hdr;",
            "\tstruct bpf_ringbuf *rb;",
            "\tu32 new_len;",
            "",
            "\thdr = sample - BPF_RINGBUF_HDR_SZ;",
            "\trb = bpf_ringbuf_restore_from_rec(hdr);",
            "\tnew_len = hdr->len ^ BPF_RINGBUF_BUSY_BIT;",
            "\tif (discard)",
            "\t\tnew_len |= BPF_RINGBUF_DISCARD_BIT;",
            "",
            "\t/* update record header with correct final size prefix */",
            "\txchg(&hdr->len, new_len);",
            "",
            "\t/* if consumer caught up and is waiting for our record, notify about",
            "\t * new data availability",
            "\t */",
            "\trec_pos = (void *)hdr - (void *)rb->data;",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos) & rb->mask;",
            "",
            "\tif (flags & BPF_RB_FORCE_WAKEUP)",
            "\t\tirq_work_queue(&rb->work);",
            "\telse if (cons_pos == rec_pos && !(flags & BPF_RB_NO_WAKEUP))",
            "\t\tirq_work_queue(&rb->work);",
            "}",
            "static int __bpf_user_ringbuf_peek(struct bpf_ringbuf *rb, void **sample, u32 *size)",
            "{",
            "\tint err;",
            "\tu32 hdr_len, sample_len, total_len, flags, *hdr;",
            "\tu64 cons_pos, prod_pos;",
            "",
            "\t/* Synchronizes with smp_store_release() in user-space producer. */",
            "\tprod_pos = smp_load_acquire(&rb->producer_pos);",
            "\tif (prod_pos % 8)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Synchronizes with smp_store_release() in __bpf_user_ringbuf_sample_release() */",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos);",
            "\tif (cons_pos >= prod_pos)",
            "\t\treturn -ENODATA;",
            "",
            "\thdr = (u32 *)((uintptr_t)rb->data + (uintptr_t)(cons_pos & rb->mask));",
            "\t/* Synchronizes with smp_store_release() in user-space producer. */",
            "\thdr_len = smp_load_acquire(hdr);",
            "\tflags = hdr_len & (BPF_RINGBUF_BUSY_BIT | BPF_RINGBUF_DISCARD_BIT);",
            "\tsample_len = hdr_len & ~flags;",
            "\ttotal_len = round_up(sample_len + BPF_RINGBUF_HDR_SZ, 8);",
            "",
            "\t/* The sample must fit within the region advertised by the producer position. */",
            "\tif (total_len > prod_pos - cons_pos)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* The sample must fit within the data region of the ring buffer. */",
            "\tif (total_len > ringbuf_total_data_sz(rb))",
            "\t\treturn -E2BIG;",
            "",
            "\t/* The sample must fit into a struct bpf_dynptr. */",
            "\terr = bpf_dynptr_check_size(sample_len);",
            "\tif (err)",
            "\t\treturn -E2BIG;",
            "",
            "\tif (flags & BPF_RINGBUF_DISCARD_BIT) {",
            "\t\t/* If the discard bit is set, the sample should be skipped.",
            "\t\t *",
            "\t\t * Update the consumer pos, and return -EAGAIN so the caller",
            "\t\t * knows to skip this sample and try to read the next one.",
            "\t\t */",
            "\t\tsmp_store_release(&rb->consumer_pos, cons_pos + total_len);",
            "\t\treturn -EAGAIN;",
            "\t}",
            "",
            "\tif (flags & BPF_RINGBUF_BUSY_BIT)",
            "\t\treturn -ENODATA;",
            "",
            "\t*sample = (void *)((uintptr_t)rb->data +",
            "\t\t\t   (uintptr_t)((cons_pos + BPF_RINGBUF_HDR_SZ) & rb->mask));",
            "\t*size = sample_len;",
            "\treturn 0;",
            "}",
            "static void __bpf_user_ringbuf_sample_release(struct bpf_ringbuf *rb, size_t size, u64 flags)",
            "{",
            "\tu64 consumer_pos;",
            "\tu32 rounded_size = round_up(size + BPF_RINGBUF_HDR_SZ, 8);",
            "",
            "\t/* Using smp_load_acquire() is unnecessary here, as the busy-bit",
            "\t * prevents another task from writing to consumer_pos after it was read",
            "\t * by this task with smp_load_acquire() in __bpf_user_ringbuf_peek().",
            "\t */",
            "\tconsumer_pos = rb->consumer_pos;",
            "\t /* Synchronizes with smp_load_acquire() in user-space producer. */",
            "\tsmp_store_release(&rb->consumer_pos, consumer_pos + rounded_size);",
            "}"
          ],
          "function_name": "ringbuf_map_mem_usage, bpf_ringbuf_rec_pg_off, bpf_ringbuf_commit, __bpf_user_ringbuf_peek, __bpf_user_ringbuf_sample_release",
          "description": "提供了环形缓冲区的内存占用统计、记录位置转换、样本提交及消费操作。包含用户态生产者与消费者的同步机制，通过忙位防止竞态条件，确保样本数据完整性校验和消费进度更新的有序性。",
          "similarity": 0.48932307958602905
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 150,
          "end_line": 258,
          "content": [
            "static void bpf_ringbuf_notify(struct irq_work *work)",
            "{",
            "\tstruct bpf_ringbuf *rb = container_of(work, struct bpf_ringbuf, work);",
            "",
            "\twake_up_all(&rb->waitq);",
            "}",
            "static void bpf_ringbuf_free(struct bpf_ringbuf *rb)",
            "{",
            "\t/* copy pages pointer and nr_pages to local variable, as we are going",
            "\t * to unmap rb itself with vunmap() below",
            "\t */",
            "\tstruct page **pages = rb->pages;",
            "\tint i, nr_pages = rb->nr_pages;",
            "",
            "\tvunmap(rb);",
            "\tfor (i = 0; i < nr_pages; i++)",
            "\t\t__free_page(pages[i]);",
            "\tbpf_map_area_free(pages);",
            "}",
            "static void ringbuf_map_free(struct bpf_map *map)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tbpf_ringbuf_free(rb_map->rb);",
            "\tbpf_map_area_free(rb_map);",
            "}",
            "static long ringbuf_map_update_elem(struct bpf_map *map, void *key, void *value,",
            "\t\t\t\t    u64 flags)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static long ringbuf_map_delete_elem(struct bpf_map *map, void *key)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static int ringbuf_map_get_next_key(struct bpf_map *map, void *key,",
            "\t\t\t\t    void *next_key)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static int ringbuf_map_mmap_kern(struct bpf_map *map, struct vm_area_struct *vma)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "",
            "\tif (vma->vm_flags & VM_WRITE) {",
            "\t\t/* allow writable mapping for the consumer_pos only */",
            "\t\tif (vma->vm_pgoff != 0 || vma->vm_end - vma->vm_start != PAGE_SIZE)",
            "\t\t\treturn -EPERM;",
            "\t}",
            "\t/* remap_vmalloc_range() checks size and offset constraints */",
            "\treturn remap_vmalloc_range(vma, rb_map->rb,",
            "\t\t\t\t   vma->vm_pgoff + RINGBUF_PGOFF);",
            "}",
            "static int ringbuf_map_mmap_user(struct bpf_map *map, struct vm_area_struct *vma)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "",
            "\tif (vma->vm_flags & VM_WRITE) {",
            "\t\tif (vma->vm_pgoff == 0)",
            "\t\t\t/* Disallow writable mappings to the consumer pointer,",
            "\t\t\t * and allow writable mappings to both the producer",
            "\t\t\t * position, and the ring buffer data itself.",
            "\t\t\t */",
            "\t\t\treturn -EPERM;",
            "\t}",
            "\t/* remap_vmalloc_range() checks size and offset constraints */",
            "\treturn remap_vmalloc_range(vma, rb_map->rb, vma->vm_pgoff + RINGBUF_PGOFF);",
            "}",
            "static unsigned long ringbuf_avail_data_sz(struct bpf_ringbuf *rb)",
            "{",
            "\tunsigned long cons_pos, prod_pos;",
            "",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos);",
            "\tprod_pos = smp_load_acquire(&rb->producer_pos);",
            "\treturn prod_pos - cons_pos;",
            "}",
            "static u32 ringbuf_total_data_sz(const struct bpf_ringbuf *rb)",
            "{",
            "\treturn rb->mask + 1;",
            "}",
            "static __poll_t ringbuf_map_poll_kern(struct bpf_map *map, struct file *filp,",
            "\t\t\t\t      struct poll_table_struct *pts)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tpoll_wait(filp, &rb_map->rb->waitq, pts);",
            "",
            "\tif (ringbuf_avail_data_sz(rb_map->rb))",
            "\t\treturn EPOLLIN | EPOLLRDNORM;",
            "\treturn 0;",
            "}",
            "static __poll_t ringbuf_map_poll_user(struct bpf_map *map, struct file *filp,",
            "\t\t\t\t      struct poll_table_struct *pts)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tpoll_wait(filp, &rb_map->rb->waitq, pts);",
            "",
            "\tif (ringbuf_avail_data_sz(rb_map->rb) < ringbuf_total_data_sz(rb_map->rb))",
            "\t\treturn EPOLLOUT | EPOLLWRNORM;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "bpf_ringbuf_notify, bpf_ringbuf_free, ringbuf_map_free, ringbuf_map_update_elem, ringbuf_map_delete_elem, ringbuf_map_get_next_key, ringbuf_map_mmap_kern, ringbuf_map_mmap_user, ringbuf_avail_data_sz, ringbuf_total_data_sz, ringbuf_map_poll_kern, ringbuf_map_poll_user",
          "description": "实现了环形缓冲区的事件通知、资源释放、内存映射控制及I/O监控功能。包含针对用户态和内核态的差异化mmap处理逻辑，通过spinlock和atomic_t实现并发控制，提供poll接口检测缓冲区可用数据状态。",
          "similarity": 0.47802478075027466
        }
      ]
    },
    {
      "source_file": "kernel/bpf/memalloc.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:19:22\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\memalloc.c`\n\n---\n\n# `bpf/memalloc.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/memalloc.c` 实现了一个专用于 BPF（Berkeley Packet Filter）程序的内存分配器，支持在任意上下文（包括 NMI、中断、不可抢占上下文等）中安全地分配和释放小块内存。该分配器通过每 CPU 的多级缓存桶（per-CPU per-bucket free list）机制，避免在 BPF 程序执行路径中直接调用可能不安全的 `kmalloc()`。缓存桶的填充和回收由 `irq_work` 异步完成，确保主执行路径的低延迟和高可靠性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_mem_cache`**  \n  每个缓存桶的核心结构，包含：\n  - `free_llist` / `free_llist_extra`：无锁链表（llist），用于存储空闲对象。\n  - `active`：本地原子计数器，用于保护对 `free_llist` 的并发访问。\n  - `refill_work`：`irq_work` 结构，用于触发异步填充。\n  - `objcg`：对象 cgroup 指针，用于内存记账。\n  - `unit_size`：该缓存桶中对象的固定大小。\n  - `free_cnt`、`low_watermark`、`high_watermark`、`batch`：缓存管理参数。\n  - `percpu_size`：标识是否为 per-CPU 分配。\n  - RCU 相关字段（`free_by_rcu`、`rcu` 等）：用于延迟释放内存，避免在不可睡眠上下文中调用 `kfree`。\n\n- **`struct bpf_mem_caches`**  \n  包含 `NUM_CACHES`（11 个）不同大小的 `bpf_mem_cache` 实例，对应预定义的内存块尺寸。\n\n- **`sizes[NUM_CACHES]`**  \n  定义了 11 种支持的分配尺寸：`{96, 192, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096}` 字节。\n\n- **`size_index[24]`**  \n  查找表，将请求大小（≤192 字节）映射到对应的缓存桶索引。\n\n### 主要函数\n\n- **`bpf_mem_cache_idx(size_t size)`**  \n  根据请求大小返回对应的缓存桶索引（0~10），超出 `BPF_MEM_ALLOC_SIZE_MAX`（4096）则返回 -1。\n\n- **`__alloc()`**  \n  底层分配函数，根据是否为 per-CPU 类型调用 `kmalloc_node` 或 `__alloc_percpu_gfp`。\n\n- **`add_obj_to_free_list()`**  \n  将对象安全地加入当前 CPU 的空闲链表，使用 `active` 计数器保护。\n\n- **`alloc_bulk()`**  \n  批量分配对象并填充缓存桶，优先从延迟释放队列（如 `free_by_rcu_ttrace`）回收，再尝试从全局分配器分配。\n\n- **`free_one()` / `free_all()`**  \n  释放单个或多个对象，区分普通和 per-CPU 类型。\n\n- **`__free_rcu()` / `__free_rcu_tasks_trace()`**  \n  RCU 回调函数，用于在宽限期结束后真正释放内存。\n\n- **`enque_to_free()` / `do_call_rcu_ttrace()`**  \n  将待释放对象加入 RCU 延迟队列，并触发 RCU 宽限期。\n\n## 3. 关键实现\n\n### 内存布局与对齐\n- 每个分配的对象末尾附加 8 字节的 `struct llist_node`，用于无锁链表管理。\n- 所有分配均对齐至 8 字节边界。\n\n### 并发控制\n- 使用 `local_t active` 计数器保护对 `free_llist` 的访问。在分配/释放时，通过 `inc_active()`/`dec_active()` 禁用中断（尤其在 `CONFIG_PREEMPT_RT` 下），确保 NMI 或中断上下文不会破坏链表结构。\n- `free_llist_extra` 用于在 `active` 忙时暂存释放对象，避免失败。\n\n### 异步填充机制\n- 当缓存桶水位低于 `low_watermark` 时，通过 `irq_work` 触发 `alloc_bulk()`。\n- `alloc_bulk()` 优先从 RCU 延迟释放队列中回收对象，减少全局分配压力。\n- 使用 `set_active_memcg()` 确保内存分配计入正确的 memcg。\n\n### RCU 延迟释放\n- 在不可睡眠上下文（如 NMI）中释放内存时，对象被加入 `free_by_rcu_ttrace` 队列。\n- 通过 `call_rcu_tasks_trace()` 或 `call_rcu()` 触发宽限期，之后在软中断上下文中真正释放。\n- 支持 `rcu_trace_implies_rcu_gp()` 优化，避免双重 RCU 调用。\n\n### 尺寸映射策略\n- 对 ≤192 字节的请求，使用 `size_index` 查找表快速定位桶。\n- 对 >192 字节的请求，使用 `fls(size - 1) - 2` 计算桶索引，覆盖 256~4096 字节范围。\n\n## 4. 依赖关系\n\n- **内存管理**：依赖 `<linux/mm.h>`、`<linux/memcontrol.h>` 进行底层分配和 memcg 记账。\n- **BPF 子系统**：通过 `<linux/bpf.h>` 和 `<linux/bpf_mem_alloc.h>` 与 BPF 运行时集成。\n- **无锁数据结构**：使用 `<linux/llist.h>` 提供的无锁链表。\n- **中断与延迟执行**：依赖 `<linux/irq_work.h>` 实现异步填充。\n- **RCU 机制**：使用 RCU 和 RCU Tasks Trace 宽限期实现安全延迟释放。\n- **架构相关**：使用 `<asm/local.h>` 的 per-CPU 原子操作。\n\n## 5. 使用场景\n\n- **BPF tracing 程序**：当 BPF 程序 attach 到 `kprobe`、`fentry` 等 hook 点时，可能运行在任意内核上下文（包括 NMI、中断、不可抢占区域）。此时标准 `kmalloc` 不安全，必须使用本分配器。\n- **高可靠性内存分配**：在不允许睡眠、不能触发内存回收的上下文中，提供确定性的内存分配能力。\n- **低延迟要求**：通过 per-CPU 缓存避免锁竞争和全局分配器开销，满足 BPF 程序对性能的严苛要求。\n- **内存隔离与记账**：支持通过 `objcg` 将 BPF 内存消耗计入特定 cgroup，便于资源控制。",
      "similarity": 0.5625267028808594,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 1,
          "end_line": 67,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2022 Meta Platforms, Inc. and affiliates. */",
            "#include <linux/mm.h>",
            "#include <linux/llist.h>",
            "#include <linux/bpf.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/bpf_mem_alloc.h>",
            "#include <linux/memcontrol.h>",
            "#include <asm/local.h>",
            "",
            "/* Any context (including NMI) BPF specific memory allocator.",
            " *",
            " * Tracing BPF programs can attach to kprobe and fentry. Hence they",
            " * run in unknown context where calling plain kmalloc() might not be safe.",
            " *",
            " * Front-end kmalloc() with per-cpu per-bucket cache of free elements.",
            " * Refill this cache asynchronously from irq_work.",
            " *",
            " * CPU_0 buckets",
            " * 16 32 64 96 128 196 256 512 1024 2048 4096",
            " * ...",
            " * CPU_N buckets",
            " * 16 32 64 96 128 196 256 512 1024 2048 4096",
            " *",
            " * The buckets are prefilled at the start.",
            " * BPF programs always run with migration disabled.",
            " * It's safe to allocate from cache of the current cpu with irqs disabled.",
            " * Free-ing is always done into bucket of the current cpu as well.",
            " * irq_work trims extra free elements from buckets with kfree",
            " * and refills them with kmalloc, so global kmalloc logic takes care",
            " * of freeing objects allocated by one cpu and freed on another.",
            " *",
            " * Every allocated objected is padded with extra 8 bytes that contains",
            " * struct llist_node.",
            " */",
            "#define LLIST_NODE_SZ sizeof(struct llist_node)",
            "",
            "#define BPF_MEM_ALLOC_SIZE_MAX 4096",
            "",
            "/* similar to kmalloc, but sizeof == 8 bucket is gone */",
            "static u8 size_index[24] __ro_after_init = {",
            "\t3,\t/* 8 */",
            "\t3,\t/* 16 */",
            "\t4,\t/* 24 */",
            "\t4,\t/* 32 */",
            "\t5,\t/* 40 */",
            "\t5,\t/* 48 */",
            "\t5,\t/* 56 */",
            "\t5,\t/* 64 */",
            "\t1,\t/* 72 */",
            "\t1,\t/* 80 */",
            "\t1,\t/* 88 */",
            "\t1,\t/* 96 */",
            "\t6,\t/* 104 */",
            "\t6,\t/* 112 */",
            "\t6,\t/* 120 */",
            "\t6,\t/* 128 */",
            "\t2,\t/* 136 */",
            "\t2,\t/* 144 */",
            "\t2,\t/* 152 */",
            "\t2,\t/* 160 */",
            "\t2,\t/* 168 */",
            "\t2,\t/* 176 */",
            "\t2,\t/* 184 */",
            "\t2\t/* 192 */",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义BPF专用内存分配器基础结构，通过per-CPU缓存和异步填充机制管理小对象分配。建立大小到缓存索引的映射表，预分配不同大小的桶并注册到各CPU，支持NMI和中断上下文的安全内存分配。",
          "similarity": 0.5925919413566589
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 68,
          "end_line": 168,
          "content": [
            "static int bpf_mem_cache_idx(size_t size)",
            "{",
            "\tif (!size || size > BPF_MEM_ALLOC_SIZE_MAX)",
            "\t\treturn -1;",
            "",
            "\tif (size <= 192)",
            "\t\treturn size_index[(size - 1) / 8] - 1;",
            "",
            "\treturn fls(size - 1) - 2;",
            "}",
            "static void inc_active(struct bpf_mem_cache *c, unsigned long *flags)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\t/* In RT irq_work runs in per-cpu kthread, so disable",
            "\t\t * interrupts to avoid preemption and interrupts and",
            "\t\t * reduce the chance of bpf prog executing on this cpu",
            "\t\t * when active counter is busy.",
            "\t\t */",
            "\t\tlocal_irq_save(*flags);",
            "\t/* alloc_bulk runs from irq_work which will not preempt a bpf",
            "\t * program that does unit_alloc/unit_free since IRQs are",
            "\t * disabled there. There is no race to increment 'active'",
            "\t * counter. It protects free_llist from corruption in case NMI",
            "\t * bpf prog preempted this loop.",
            "\t */",
            "\tWARN_ON_ONCE(local_inc_return(&c->active) != 1);",
            "}",
            "static void dec_active(struct bpf_mem_cache *c, unsigned long *flags)",
            "{",
            "\tlocal_dec(&c->active);",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tlocal_irq_restore(*flags);",
            "}",
            "static void add_obj_to_free_list(struct bpf_mem_cache *c, void *obj)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tinc_active(c, &flags);",
            "\t__llist_add(obj, &c->free_llist);",
            "\tc->free_cnt++;",
            "\tdec_active(c, &flags);",
            "}",
            "static void alloc_bulk(struct bpf_mem_cache *c, int cnt, int node, bool atomic)",
            "{",
            "\tstruct mem_cgroup *memcg = NULL, *old_memcg;",
            "\tgfp_t gfp;",
            "\tvoid *obj;",
            "\tint i;",
            "",
            "\tgfp = __GFP_NOWARN | __GFP_ACCOUNT;",
            "\tgfp |= atomic ? GFP_NOWAIT : GFP_KERNEL;",
            "",
            "\tfor (i = 0; i < cnt; i++) {",
            "\t\t/*",
            "\t\t * For every 'c' llist_del_first(&c->free_by_rcu_ttrace); is",
            "\t\t * done only by one CPU == current CPU. Other CPUs might",
            "\t\t * llist_add() and llist_del_all() in parallel.",
            "\t\t */",
            "\t\tobj = llist_del_first(&c->free_by_rcu_ttrace);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tif (i >= cnt)",
            "\t\treturn;",
            "",
            "\tfor (; i < cnt; i++) {",
            "\t\tobj = llist_del_first(&c->waiting_for_gp_ttrace);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tif (i >= cnt)",
            "\t\treturn;",
            "",
            "\tmemcg = get_memcg(c);",
            "\told_memcg = set_active_memcg(memcg);",
            "\tfor (; i < cnt; i++) {",
            "\t\t/* Allocate, but don't deplete atomic reserves that typical",
            "\t\t * GFP_ATOMIC would do. irq_work runs on this cpu and kmalloc",
            "\t\t * will allocate from the current numa node which is what we",
            "\t\t * want here.",
            "\t\t */",
            "\t\tobj = __alloc(c, node, gfp);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tset_active_memcg(old_memcg);",
            "\tmem_cgroup_put(memcg);",
            "}",
            "static void free_one(void *obj, bool percpu)",
            "{",
            "\tif (percpu) {",
            "\t\tfree_percpu(((void __percpu **)obj)[1]);",
            "\t\tkfree(obj);",
            "\t\treturn;",
            "\t}",
            "",
            "\tkfree(obj);",
            "}"
          ],
          "function_name": "bpf_mem_cache_idx, inc_active, dec_active, add_obj_to_free_list, alloc_bulk, free_one",
          "description": "实现BPF内存缓存核心控制逻辑，包含大小索引计算、活跃计数器管理、对象回收到自由链表、批量分配与释放流程。通过irq_work异步补充缓存，处理多CPU间的内存对象迁移与回收。",
          "similarity": 0.5152314901351929
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 859,
          "end_line": 938,
          "content": [
            "static void notrace unit_free_rcu(struct bpf_mem_cache *c, void *ptr)",
            "{",
            "\tstruct llist_node *llnode = ptr - LLIST_NODE_SZ;",
            "\tunsigned long flags;",
            "",
            "\tc->tgt = *(struct bpf_mem_cache **)llnode;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (local_inc_return(&c->active) == 1) {",
            "\t\tif (__llist_add(llnode, &c->free_by_rcu))",
            "\t\t\tc->free_by_rcu_tail = llnode;",
            "\t} else {",
            "\t\tllist_add(llnode, &c->free_llist_extra_rcu);",
            "\t}",
            "\tlocal_dec(&c->active);",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (!atomic_read(&c->call_rcu_in_progress))",
            "\t\tirq_work_raise(c);",
            "}",
            "void notrace bpf_mem_free(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tstruct bpf_mem_cache *c;",
            "\tint idx;",
            "",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tc = *(void **)(ptr - LLIST_NODE_SZ);",
            "\tidx = bpf_mem_cache_idx(c->unit_size);",
            "\tif (WARN_ON_ONCE(idx < 0))",
            "\t\treturn;",
            "",
            "\tunit_free(this_cpu_ptr(ma->caches)->cache + idx, ptr);",
            "}",
            "void notrace bpf_mem_free_rcu(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tstruct bpf_mem_cache *c;",
            "\tint idx;",
            "",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tc = *(void **)(ptr - LLIST_NODE_SZ);",
            "\tidx = bpf_mem_cache_idx(c->unit_size);",
            "\tif (WARN_ON_ONCE(idx < 0))",
            "\t\treturn;",
            "",
            "\tunit_free_rcu(this_cpu_ptr(ma->caches)->cache + idx, ptr);",
            "}",
            "void notrace bpf_mem_cache_free(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tunit_free(this_cpu_ptr(ma->cache), ptr);",
            "}",
            "void notrace bpf_mem_cache_free_rcu(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tunit_free_rcu(this_cpu_ptr(ma->cache), ptr);",
            "}",
            "void bpf_mem_cache_raw_free(void *ptr)",
            "{",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tkfree(ptr - LLIST_NODE_SZ);",
            "}",
            "int bpf_mem_alloc_check_size(bool percpu, size_t size)",
            "{",
            "\t/* The size of percpu allocation doesn't have LLIST_NODE_SZ overhead */",
            "\tif ((percpu && size > BPF_MEM_ALLOC_SIZE_MAX) ||",
            "\t    (!percpu && size > BPF_MEM_ALLOC_SIZE_MAX - LLIST_NODE_SZ))",
            "\t\treturn -E2BIG;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "unit_free_rcu, bpf_mem_free, bpf_mem_free_rcu, bpf_mem_cache_free, bpf_mem_cache_free_rcu, bpf_mem_cache_raw_free, bpf_mem_alloc_check_size",
          "description": "提供基于RCU的内存释放接口，unit_free系列函数将对象加入链表实现批量回收，bpf_mem_free系列根据上下文选择普通或RCU释放路径，check_size验证分配尺寸合法性",
          "similarity": 0.5143848657608032
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 689,
          "end_line": 807,
          "content": [
            "static void free_mem_alloc(struct bpf_mem_alloc *ma)",
            "{",
            "\t/* waiting_for_gp[_ttrace] lists were drained, but RCU callbacks",
            "\t * might still execute. Wait for them.",
            "\t *",
            "\t * rcu_barrier_tasks_trace() doesn't imply synchronize_rcu_tasks_trace(),",
            "\t * but rcu_barrier_tasks_trace() and rcu_barrier() below are only used",
            "\t * to wait for the pending __free_rcu_tasks_trace() and __free_rcu(),",
            "\t * so if call_rcu(head, __free_rcu) is skipped due to",
            "\t * rcu_trace_implies_rcu_gp(), it will be OK to skip rcu_barrier() by",
            "\t * using rcu_trace_implies_rcu_gp() as well.",
            "\t */",
            "\trcu_barrier(); /* wait for __free_by_rcu */",
            "\trcu_barrier_tasks_trace(); /* wait for __free_rcu */",
            "\tif (!rcu_trace_implies_rcu_gp())",
            "\t\trcu_barrier();",
            "\tfree_mem_alloc_no_barrier(ma);",
            "}",
            "static void free_mem_alloc_deferred(struct work_struct *work)",
            "{",
            "\tstruct bpf_mem_alloc *ma = container_of(work, struct bpf_mem_alloc, work);",
            "",
            "\tfree_mem_alloc(ma);",
            "\tkfree(ma);",
            "}",
            "static void destroy_mem_alloc(struct bpf_mem_alloc *ma, int rcu_in_progress)",
            "{",
            "\tstruct bpf_mem_alloc *copy;",
            "",
            "\tif (!rcu_in_progress) {",
            "\t\t/* Fast path. No callbacks are pending, hence no need to do",
            "\t\t * rcu_barrier-s.",
            "\t\t */",
            "\t\tfree_mem_alloc_no_barrier(ma);",
            "\t\treturn;",
            "\t}",
            "",
            "\tcopy = kmemdup(ma, sizeof(*ma), GFP_KERNEL);",
            "\tif (!copy) {",
            "\t\t/* Slow path with inline barrier-s */",
            "\t\tfree_mem_alloc(ma);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Defer barriers into worker to let the rest of map memory to be freed */",
            "\tmemset(ma, 0, sizeof(*ma));",
            "\tINIT_WORK(&copy->work, free_mem_alloc_deferred);",
            "\tqueue_work(system_unbound_wq, &copy->work);",
            "}",
            "void bpf_mem_alloc_destroy(struct bpf_mem_alloc *ma)",
            "{",
            "\tstruct bpf_mem_caches *cc;",
            "\tstruct bpf_mem_cache *c;",
            "\tint cpu, i, rcu_in_progress;",
            "",
            "\tif (ma->cache) {",
            "\t\trcu_in_progress = 0;",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tc = per_cpu_ptr(ma->cache, cpu);",
            "\t\t\tWRITE_ONCE(c->draining, true);",
            "\t\t\tirq_work_sync(&c->refill_work);",
            "\t\t\tdrain_mem_cache(c);",
            "\t\t\trcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);",
            "\t\t\trcu_in_progress += atomic_read(&c->call_rcu_in_progress);",
            "\t\t}",
            "\t\tobj_cgroup_put(ma->objcg);",
            "\t\tdestroy_mem_alloc(ma, rcu_in_progress);",
            "\t}",
            "\tif (ma->caches) {",
            "\t\trcu_in_progress = 0;",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tcc = per_cpu_ptr(ma->caches, cpu);",
            "\t\t\tfor (i = 0; i < NUM_CACHES; i++) {",
            "\t\t\t\tc = &cc->cache[i];",
            "\t\t\t\tWRITE_ONCE(c->draining, true);",
            "\t\t\t\tirq_work_sync(&c->refill_work);",
            "\t\t\t\tdrain_mem_cache(c);",
            "\t\t\t\trcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);",
            "\t\t\t\trcu_in_progress += atomic_read(&c->call_rcu_in_progress);",
            "\t\t\t}",
            "\t\t}",
            "\t\tobj_cgroup_put(ma->objcg);",
            "\t\tdestroy_mem_alloc(ma, rcu_in_progress);",
            "\t}",
            "}",
            "static void notrace unit_free(struct bpf_mem_cache *c, void *ptr)",
            "{",
            "\tstruct llist_node *llnode = ptr - LLIST_NODE_SZ;",
            "\tunsigned long flags;",
            "\tint cnt = 0;",
            "",
            "\tBUILD_BUG_ON(LLIST_NODE_SZ > 8);",
            "",
            "\t/*",
            "\t * Remember bpf_mem_cache that allocated this object.",
            "\t * The hint is not accurate.",
            "\t */",
            "\tc->tgt = *(struct bpf_mem_cache **)llnode;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (local_inc_return(&c->active) == 1) {",
            "\t\t__llist_add(llnode, &c->free_llist);",
            "\t\tcnt = ++c->free_cnt;",
            "\t} else {",
            "\t\t/* unit_free() cannot fail. Therefore add an object to atomic",
            "\t\t * llist. free_bulk() will drain it. Though free_llist_extra is",
            "\t\t * a per-cpu list we have to use atomic llist_add here, since",
            "\t\t * it also can be interrupted by bpf nmi prog that does another",
            "\t\t * unit_free() into the same free_llist_extra.",
            "\t\t */",
            "\t\tllist_add(llnode, &c->free_llist_extra);",
            "\t}",
            "\tlocal_dec(&c->active);",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (cnt > c->high_watermark)",
            "\t\t/* free few objects from current cpu into global kmalloc pool */",
            "\t\tirq_work_raise(c);",
            "}"
          ],
          "function_name": "free_mem_alloc, free_mem_alloc_deferred, destroy_mem_alloc, bpf_mem_alloc_destroy, unit_free",
          "description": "实现BPF内存分配销毁逻辑，通过RCU屏障等待回调完成并安全释放资源，deferred路径利用workqueue异步释放，destroy_mem_alloc处理缓存清理和RCU状态同步。",
          "similarity": 0.49520111083984375
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 381,
          "end_line": 545,
          "content": [
            "static void check_free_by_rcu(struct bpf_mem_cache *c)",
            "{",
            "\tstruct llist_node *llnode, *t;",
            "\tunsigned long flags;",
            "",
            "\t/* drain free_llist_extra_rcu */",
            "\tif (unlikely(!llist_empty(&c->free_llist_extra_rcu))) {",
            "\t\tinc_active(c, &flags);",
            "\t\tllist_for_each_safe(llnode, t, llist_del_all(&c->free_llist_extra_rcu))",
            "\t\t\tif (__llist_add(llnode, &c->free_by_rcu))",
            "\t\t\t\tc->free_by_rcu_tail = llnode;",
            "\t\tdec_active(c, &flags);",
            "\t}",
            "",
            "\tif (llist_empty(&c->free_by_rcu))",
            "\t\treturn;",
            "",
            "\tif (atomic_xchg(&c->call_rcu_in_progress, 1)) {",
            "\t\t/*",
            "\t\t * Instead of kmalloc-ing new rcu_head and triggering 10k",
            "\t\t * call_rcu() to hit rcutree.qhimark and force RCU to notice",
            "\t\t * the overload just ask RCU to hurry up. There could be many",
            "\t\t * objects in free_by_rcu list.",
            "\t\t * This hint reduces memory consumption for an artificial",
            "\t\t * benchmark from 2 Gbyte to 150 Mbyte.",
            "\t\t */",
            "\t\trcu_request_urgent_qs_task(current);",
            "\t\treturn;",
            "\t}",
            "",
            "\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp));",
            "",
            "\tinc_active(c, &flags);",
            "\tWRITE_ONCE(c->waiting_for_gp.first, __llist_del_all(&c->free_by_rcu));",
            "\tc->waiting_for_gp_tail = c->free_by_rcu_tail;",
            "\tdec_active(c, &flags);",
            "",
            "\tif (unlikely(READ_ONCE(c->draining))) {",
            "\t\tfree_all(llist_del_all(&c->waiting_for_gp), !!c->percpu_size);",
            "\t\tatomic_set(&c->call_rcu_in_progress, 0);",
            "\t} else {",
            "\t\tcall_rcu_hurry(&c->rcu, __free_by_rcu);",
            "\t}",
            "}",
            "static void bpf_mem_refill(struct irq_work *work)",
            "{",
            "\tstruct bpf_mem_cache *c = container_of(work, struct bpf_mem_cache, refill_work);",
            "\tint cnt;",
            "",
            "\t/* Racy access to free_cnt. It doesn't need to be 100% accurate */",
            "\tcnt = c->free_cnt;",
            "\tif (cnt < c->low_watermark)",
            "\t\t/* irq_work runs on this cpu and kmalloc will allocate",
            "\t\t * from the current numa node which is what we want here.",
            "\t\t */",
            "\t\talloc_bulk(c, c->batch, NUMA_NO_NODE, true);",
            "\telse if (cnt > c->high_watermark)",
            "\t\tfree_bulk(c);",
            "",
            "\tcheck_free_by_rcu(c);",
            "}",
            "static void notrace irq_work_raise(struct bpf_mem_cache *c)",
            "{",
            "\tirq_work_queue(&c->refill_work);",
            "}",
            "static void init_refill_work(struct bpf_mem_cache *c)",
            "{",
            "\tinit_irq_work(&c->refill_work, bpf_mem_refill);",
            "\tif (c->percpu_size) {",
            "\t\tc->low_watermark = 1;",
            "\t\tc->high_watermark = 3;",
            "\t} else if (c->unit_size <= 256) {",
            "\t\tc->low_watermark = 32;",
            "\t\tc->high_watermark = 96;",
            "\t} else {",
            "\t\t/* When page_size == 4k, order-0 cache will have low_mark == 2",
            "\t\t * and high_mark == 6 with batch alloc of 3 individual pages at",
            "\t\t * a time.",
            "\t\t * 8k allocs and above low == 1, high == 3, batch == 1.",
            "\t\t */",
            "\t\tc->low_watermark = max(32 * 256 / c->unit_size, 1);",
            "\t\tc->high_watermark = max(96 * 256 / c->unit_size, 3);",
            "\t}",
            "\tc->batch = max((c->high_watermark - c->low_watermark) / 4 * 3, 1);",
            "}",
            "static void prefill_mem_cache(struct bpf_mem_cache *c, int cpu)",
            "{",
            "\tint cnt = 1;",
            "",
            "\t/* To avoid consuming memory, for non-percpu allocation, assume that",
            "\t * 1st run of bpf prog won't be doing more than 4 map_update_elem from",
            "\t * irq disabled region if unit size is less than or equal to 256.",
            "\t * For all other cases, let us just do one allocation.",
            "\t */",
            "\tif (!c->percpu_size && c->unit_size <= 256)",
            "\t\tcnt = 4;",
            "\talloc_bulk(c, cnt, cpu_to_node(cpu), false);",
            "}",
            "int bpf_mem_alloc_init(struct bpf_mem_alloc *ma, int size, bool percpu)",
            "{",
            "\tstruct bpf_mem_caches *cc; struct bpf_mem_caches __percpu *pcc;",
            "\tstruct bpf_mem_cache *c; struct bpf_mem_cache __percpu *pc;",
            "\tstruct obj_cgroup *objcg = NULL;",
            "\tint cpu, i, unit_size, percpu_size = 0;",
            "",
            "\tif (percpu && size == 0)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* room for llist_node and per-cpu pointer */",
            "\tif (percpu)",
            "\t\tpercpu_size = LLIST_NODE_SZ + sizeof(void *);",
            "\tma->percpu = percpu;",
            "",
            "\tif (size) {",
            "\t\tpc = __alloc_percpu_gfp(sizeof(*pc), 8, GFP_KERNEL);",
            "\t\tif (!pc)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tif (!percpu)",
            "\t\t\tsize += LLIST_NODE_SZ; /* room for llist_node */",
            "\t\tunit_size = size;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\t\tif (memcg_bpf_enabled())",
            "\t\t\tobjcg = get_obj_cgroup_from_current();",
            "#endif",
            "\t\tma->objcg = objcg;",
            "",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tc = per_cpu_ptr(pc, cpu);",
            "\t\t\tc->unit_size = unit_size;",
            "\t\t\tc->objcg = objcg;",
            "\t\t\tc->percpu_size = percpu_size;",
            "\t\t\tc->tgt = c;",
            "\t\t\tinit_refill_work(c);",
            "\t\t\tprefill_mem_cache(c, cpu);",
            "\t\t}",
            "\t\tma->cache = pc;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tpcc = __alloc_percpu_gfp(sizeof(*cc), 8, GFP_KERNEL);",
            "\tif (!pcc)",
            "\t\treturn -ENOMEM;",
            "#ifdef CONFIG_MEMCG",
            "\tobjcg = get_obj_cgroup_from_current();",
            "#endif",
            "\tma->objcg = objcg;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tcc = per_cpu_ptr(pcc, cpu);",
            "\t\tfor (i = 0; i < NUM_CACHES; i++) {",
            "\t\t\tc = &cc->cache[i];",
            "\t\t\tc->unit_size = sizes[i];",
            "\t\t\tc->objcg = objcg;",
            "\t\t\tc->percpu_size = percpu_size;",
            "\t\t\tc->tgt = c;",
            "",
            "\t\t\tinit_refill_work(c);",
            "\t\t\tprefill_mem_cache(c, cpu);",
            "\t\t}",
            "\t}",
            "",
            "\tma->caches = pcc;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "check_free_by_rcu, bpf_mem_refill, irq_work_raise, init_refill_work, prefill_mem_cache, bpf_mem_alloc_init",
          "description": "初始化内存缓存的刷新工作队列，设置水位标记控制缓存规模，实现动态扩容/缩容。包含预填充缓存的初始化函数，根据对象大小配置不同的低/高水位阈值，通过中断工作线程维护缓存状态。",
          "similarity": 0.46125325560569763
        }
      ]
    },
    {
      "source_file": "kernel/bpf/hashtab.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:10:56\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\hashtab.c`\n\n---\n\n# bpf/hashtab.c 技术文档\n\n## 1. 文件概述\n\n`bpf/hashtab.c` 是 Linux 内核中 BPF（Berkeley Packet Filter）子系统的核心实现文件之一，负责提供基于哈希表（hash table）的 BPF map 类型支持。该文件实现了多种 BPF map 类型，包括普通哈希表（`BPF_MAP_TYPE_HASH`）、LRU 哈希表（`BPF_MAP_TYPE_LRU_HASH`）、每 CPU 哈希表（`BPF_MAP_TYPE_PERCPU_HASH`）及其 LRU 变体。它支持预分配（pre-allocated）和动态分配（non-preallocated）两种内存管理模式，并集成了 BPF 内存分配器（`bpf_mem_alloc`）、LRU 驱逐机制、每 CPU 自由列表（percpu freelist）等高级特性，以满足高性能、低延迟的 BPF 程序需求。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bucket`**  \n  哈希桶结构，包含一个 `hlist_nulls_head` 链表头和一个 `raw_spinlock_t` 原始自旋锁，用于保护桶内元素的并发访问。\n\n- **`struct bpf_htab`**  \n  BPF 哈希表的主控制结构，继承自 `struct bpf_map`，包含：\n  - 桶数组指针 `buckets`\n  - 元素存储区 `elems`\n  - 内存分配器 `ma`（主）和 `pcpu_ma`（每 CPU）\n  - LRU 或 percpu_freelist 联合体\n  - 元素计数器（`pcount` 或 `count`）\n  - 哈希种子 `hashrnd`\n  - 锁依赖类键 `lockdep_key`\n  - 每 CPU 锁状态数组 `map_locked`（用于防止递归）\n\n- **`struct htab_elem`**  \n  哈希表元素结构，包含：\n  - 哈希链表节点 `hash_node`\n  - LRU 节点或自由列表节点\n  - 指向每 CPU 指针的指针（用于 per-CPU map）\n  - 哈希值 `hash`\n  - 可变长键 `key[]`（后接值或 per-CPU 指针）\n\n### 关键辅助函数\n\n- `htab_is_prealloc()`：判断是否为预分配模式\n- `htab_is_lru()` / `htab_is_percpu()`：判断 map 类型是否为 LRU 或 per-CPU\n- `htab_init_buckets()`：初始化所有哈希桶\n- `htab_lock_bucket()` / `htab_unlock_bucket()`：带递归保护的桶锁操作\n- `htab_elem_set_ptr()` / `htab_elem_get_ptr()`：操作 per-CPU 指针\n- `get_htab_elem()`：从预分配区域获取第 i 个元素\n- `htab_has_extra_elems()`：判断是否包含额外元素（用于 per-CPU 扩展）\n- `htab_free_prealloced_timers_and_wq()`：释放预分配元素中的 BPF 定时器和工作队列资源\n\n### 批量操作宏\n\n- `BATCH_OPS(_name)`：定义批量操作函数指针，如 `map_lookup_batch`、`map_update_batch` 等。\n\n## 3. 关键实现\n\n### 并发控制与死锁预防\n\n- 使用 **原始自旋锁（`raw_spinlock_t`）** 保护每个哈希桶，确保在任意上下文（如 kprobe、perf、tracepoint）中安全使用。\n- 引入 **每 CPU 递归计数器 `map_locked[]`**，防止 BPF 程序在持有桶锁时再次进入（例如通过 `sys_bpf()` 或嵌套 BPF 调用），避免死锁。\n- 在 `PREEMPT_RT` 实时内核上，由于普通自旋锁可能睡眠，必须使用 `raw_spinlock` 以保证原子性；结合 `bpf_mem_alloc` 后，即使非预分配模式也可安全使用原始锁。\n\n### 内存管理\n\n- **预分配模式（`BPF_F_NO_PREALLOC` 未设置）**：启动时一次性分配所有元素，使用 `pcpu_freelist` 管理空闲元素。\n- **非预分配模式**：按需通过 `bpf_mem_alloc` 动态分配元素，支持 NUMA 感知和内存回收。\n- **Per-CPU 支持**：对于 `PERCPU_HASH` 类型，每个键对应一个 per-CPU 值数组，通过 `htab_elem_get_ptr()` 访问。\n\n### LRU 驱逐机制\n\n- 当 map 类型为 `LRU_HASH` 或 `LRU_PERCPU_HASH` 时，使用 `bpf_lru` 子系统管理元素生命周期，自动驱逐最近最少使用的条目以维持 `max_entries` 限制。\n\n### 扩展字段支持\n\n- 支持 BTF（BPF Type Format）描述的复杂值类型，如 `BPF_TIMER` 和 `BPF_WORKQUEUE`，在销毁 map 时自动释放相关资源（见 `htab_free_prealloced_timers_and_wq`）。\n\n### 哈希与对齐\n\n- 使用 `jhash` 算法计算键的哈希值，并通过 `hashrnd` 引入随机种子防止哈希碰撞攻击。\n- 键和值之间按 8 字节对齐（`__aligned(8)`），确保 per-CPU 指针正确对齐。\n\n## 4. 依赖关系\n\n- **内核头文件**：\n  - `<linux/bpf.h>`、`<linux/btf.h>`：BPF 和 BTF 核心接口\n  - `<linux/jhash.h>`：哈希函数\n  - `<linux/rculist_nulls.h>`：RCU 安全的空指针链表\n  - `<linux/percpu_freelist.h>`、`<linux/bpf_lru_list.h>`：内存管理子系统\n  - `<linux/bpf_mem_alloc.h>`：BPF 专用内存分配器\n\n- **内部模块**：\n  - `map_in_map.h`：支持 map-in-map 功能\n  - `bpf_lru_list.c`：LRU 驱逐实现\n  - `percpu_freelist.c`：每 CPU 自由列表管理\n\n- **BPF 子系统**：\n  - 与 `bpf_map` 通用框架集成，通过 `bpf_map_ops` 注册操作函数\n  - 依赖 `bpf_prog_active` 机制防止 BPF 递归\n\n## 5. 使用场景\n\n- **网络数据包过滤与监控**：eBPF 程序使用 `BPF_MAP_TYPE_HASH` 存储连接状态、统计信息等。\n- **性能分析**：通过 `PERCPU_HASH` 收集每 CPU 的性能计数器，避免锁竞争。\n- **资源限制与缓存**：`LRU_HASH` 用于实现有界缓存（如 DNS 缓存、会话表），自动淘汰旧条目。\n- **内核跟踪**：kprobe、tracepoint 等 attach 的 BPF 程序频繁读写哈希表，要求低延迟和高并发。\n- **用户空间交互**：通过 `bpf(2)` 系统调用进行 map 的创建、更新、查询和删除，支持批量操作提升效率。\n- **高级 BPF 功能**：支持包含定时器（`bpf_timer`）或工作队列（`bpf_workqueue`）的复杂 map 值类型，用于异步任务调度。",
      "similarity": 0.5589359402656555,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 1,
          "end_line": 131,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com",
            " * Copyright (c) 2016 Facebook",
            " */",
            "#include <linux/bpf.h>",
            "#include <linux/btf.h>",
            "#include <linux/jhash.h>",
            "#include <linux/filter.h>",
            "#include <linux/rculist_nulls.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/random.h>",
            "#include <uapi/linux/btf.h>",
            "#include <linux/rcupdate_trace.h>",
            "#include <linux/btf_ids.h>",
            "#include \"percpu_freelist.h\"",
            "#include \"bpf_lru_list.h\"",
            "#include \"map_in_map.h\"",
            "#include <linux/bpf_mem_alloc.h>",
            "",
            "#define HTAB_CREATE_FLAG_MASK\t\t\t\t\t\t\\",
            "\t(BPF_F_NO_PREALLOC | BPF_F_NO_COMMON_LRU | BPF_F_NUMA_NODE |\t\\",
            "\t BPF_F_ACCESS_MASK | BPF_F_ZERO_SEED)",
            "",
            "#define BATCH_OPS(_name)\t\t\t\\",
            "\t.map_lookup_batch =\t\t\t\\",
            "\t_name##_map_lookup_batch,\t\t\\",
            "\t.map_lookup_and_delete_batch =\t\t\\",
            "\t_name##_map_lookup_and_delete_batch,\t\\",
            "\t.map_update_batch =\t\t\t\\",
            "\tgeneric_map_update_batch,\t\t\\",
            "\t.map_delete_batch =\t\t\t\\",
            "\tgeneric_map_delete_batch",
            "",
            "/*",
            " * The bucket lock has two protection scopes:",
            " *",
            " * 1) Serializing concurrent operations from BPF programs on different",
            " *    CPUs",
            " *",
            " * 2) Serializing concurrent operations from BPF programs and sys_bpf()",
            " *",
            " * BPF programs can execute in any context including perf, kprobes and",
            " * tracing. As there are almost no limits where perf, kprobes and tracing",
            " * can be invoked from the lock operations need to be protected against",
            " * deadlocks. Deadlocks can be caused by recursion and by an invocation in",
            " * the lock held section when functions which acquire this lock are invoked",
            " * from sys_bpf(). BPF recursion is prevented by incrementing the per CPU",
            " * variable bpf_prog_active, which prevents BPF programs attached to perf",
            " * events, kprobes and tracing to be invoked before the prior invocation",
            " * from one of these contexts completed. sys_bpf() uses the same mechanism",
            " * by pinning the task to the current CPU and incrementing the recursion",
            " * protection across the map operation.",
            " *",
            " * This has subtle implications on PREEMPT_RT. PREEMPT_RT forbids certain",
            " * operations like memory allocations (even with GFP_ATOMIC) from atomic",
            " * contexts. This is required because even with GFP_ATOMIC the memory",
            " * allocator calls into code paths which acquire locks with long held lock",
            " * sections. To ensure the deterministic behaviour these locks are regular",
            " * spinlocks, which are converted to 'sleepable' spinlocks on RT. The only",
            " * true atomic contexts on an RT kernel are the low level hardware",
            " * handling, scheduling, low level interrupt handling, NMIs etc. None of",
            " * these contexts should ever do memory allocations.",
            " *",
            " * As regular device interrupt handlers and soft interrupts are forced into",
            " * thread context, the existing code which does",
            " *   spin_lock*(); alloc(GFP_ATOMIC); spin_unlock*();",
            " * just works.",
            " *",
            " * In theory the BPF locks could be converted to regular spinlocks as well,",
            " * but the bucket locks and percpu_freelist locks can be taken from",
            " * arbitrary contexts (perf, kprobes, tracepoints) which are required to be",
            " * atomic contexts even on RT. Before the introduction of bpf_mem_alloc,",
            " * it is only safe to use raw spinlock for preallocated hash map on a RT kernel,",
            " * because there is no memory allocation within the lock held sections. However",
            " * after hash map was fully converted to use bpf_mem_alloc, there will be",
            " * non-synchronous memory allocation for non-preallocated hash map, so it is",
            " * safe to always use raw spinlock for bucket lock.",
            " */",
            "struct bucket {",
            "\tstruct hlist_nulls_head head;",
            "\traw_spinlock_t raw_lock;",
            "};",
            "",
            "#define HASHTAB_MAP_LOCK_COUNT 8",
            "#define HASHTAB_MAP_LOCK_MASK (HASHTAB_MAP_LOCK_COUNT - 1)",
            "",
            "struct bpf_htab {",
            "\tstruct bpf_map map;",
            "\tstruct bpf_mem_alloc ma;",
            "\tstruct bpf_mem_alloc pcpu_ma;",
            "\tstruct bucket *buckets;",
            "\tvoid *elems;",
            "\tunion {",
            "\t\tstruct pcpu_freelist freelist;",
            "\t\tstruct bpf_lru lru;",
            "\t};",
            "\tstruct htab_elem *__percpu *extra_elems;",
            "\t/* number of elements in non-preallocated hashtable are kept",
            "\t * in either pcount or count",
            "\t */",
            "\tstruct percpu_counter pcount;",
            "\tatomic_t count;",
            "\tbool use_percpu_counter;",
            "\tu32 n_buckets;\t/* number of hash buckets */",
            "\tu32 elem_size;\t/* size of each element in bytes */",
            "\tu32 hashrnd;",
            "\tstruct lock_class_key lockdep_key;",
            "\tint __percpu *map_locked[HASHTAB_MAP_LOCK_COUNT];",
            "};",
            "",
            "/* each htab element is struct htab_elem + key + value */",
            "struct htab_elem {",
            "\tunion {",
            "\t\tstruct hlist_nulls_node hash_node;",
            "\t\tstruct {",
            "\t\t\tvoid *padding;",
            "\t\t\tunion {",
            "\t\t\t\tstruct pcpu_freelist_node fnode;",
            "\t\t\t\tstruct htab_elem *batch_flink;",
            "\t\t\t};",
            "\t\t};",
            "\t};",
            "\tunion {",
            "\t\t/* pointer to per-cpu pointer */",
            "\t\tvoid *ptr_to_pptr;",
            "\t\tstruct bpf_lru_node lru_node;",
            "\t};",
            "\tu32 hash;",
            "\tchar key[] __aligned(8);",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义BPF哈希表相关的头文件和宏常量，声明bucket结构体及bpf_htab结构体，包含哈希表的桶锁、元素存储、LRU/PCPU管理等核心成员变量，描述了哈希表的并发控制机制和内存分配策略。",
          "similarity": 0.5153939723968506
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 1701,
          "end_line": 1938,
          "content": [
            "static int htab_lru_percpu_map_lookup_and_delete_elem(struct bpf_map *map,",
            "\t\t\t\t\t\t      void *key, void *value,",
            "\t\t\t\t\t\t      u64 flags)",
            "{",
            "\treturn __htab_map_lookup_and_delete_elem(map, key, value, true, true,",
            "\t\t\t\t\t\t flags);",
            "}",
            "static int",
            "__htab_map_lookup_and_delete_batch(struct bpf_map *map,",
            "\t\t\t\t   const union bpf_attr *attr,",
            "\t\t\t\t   union bpf_attr __user *uattr,",
            "\t\t\t\t   bool do_delete, bool is_lru_map,",
            "\t\t\t\t   bool is_percpu)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tu32 bucket_cnt, total, key_size, value_size, roundup_key_size;",
            "\tvoid *keys = NULL, *values = NULL, *value, *dst_key, *dst_val;",
            "\tvoid __user *uvalues = u64_to_user_ptr(attr->batch.values);",
            "\tvoid __user *ukeys = u64_to_user_ptr(attr->batch.keys);",
            "\tvoid __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);",
            "\tu32 batch, max_count, size, bucket_size, map_id;",
            "\tstruct htab_elem *node_to_free = NULL;",
            "\tu64 elem_map_flags, map_flags;",
            "\tstruct hlist_nulls_head *head;",
            "\tstruct hlist_nulls_node *n;",
            "\tunsigned long flags = 0;",
            "\tbool locked = false;",
            "\tstruct htab_elem *l;",
            "\tstruct bucket *b;",
            "\tint ret = 0;",
            "",
            "\telem_map_flags = attr->batch.elem_flags;",
            "\tif ((elem_map_flags & ~BPF_F_LOCK) ||",
            "\t    ((elem_map_flags & BPF_F_LOCK) && !btf_record_has_field(map->record, BPF_SPIN_LOCK)))",
            "\t\treturn -EINVAL;",
            "",
            "\tmap_flags = attr->batch.flags;",
            "\tif (map_flags)",
            "\t\treturn -EINVAL;",
            "",
            "\tmax_count = attr->batch.count;",
            "\tif (!max_count)",
            "\t\treturn 0;",
            "",
            "\tif (put_user(0, &uattr->batch.count))",
            "\t\treturn -EFAULT;",
            "",
            "\tbatch = 0;",
            "\tif (ubatch && copy_from_user(&batch, ubatch, sizeof(batch)))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (batch >= htab->n_buckets)",
            "\t\treturn -ENOENT;",
            "",
            "\tkey_size = htab->map.key_size;",
            "\troundup_key_size = round_up(htab->map.key_size, 8);",
            "\tvalue_size = htab->map.value_size;",
            "\tsize = round_up(value_size, 8);",
            "\tif (is_percpu)",
            "\t\tvalue_size = size * num_possible_cpus();",
            "\ttotal = 0;",
            "\t/* while experimenting with hash tables with sizes ranging from 10 to",
            "\t * 1000, it was observed that a bucket can have up to 5 entries.",
            "\t */",
            "\tbucket_size = 5;",
            "",
            "alloc:",
            "\t/* We cannot do copy_from_user or copy_to_user inside",
            "\t * the rcu_read_lock. Allocate enough space here.",
            "\t */",
            "\tkeys = kvmalloc_array(key_size, bucket_size, GFP_USER | __GFP_NOWARN);",
            "\tvalues = kvmalloc_array(value_size, bucket_size, GFP_USER | __GFP_NOWARN);",
            "\tif (!keys || !values) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto after_loop;",
            "\t}",
            "",
            "again:",
            "\tbpf_disable_instrumentation();",
            "\trcu_read_lock();",
            "again_nocopy:",
            "\tdst_key = keys;",
            "\tdst_val = values;",
            "\tb = &htab->buckets[batch];",
            "\thead = &b->head;",
            "\t/* do not grab the lock unless need it (bucket_cnt > 0). */",
            "\tif (locked) {",
            "\t\tret = htab_lock_bucket(htab, b, batch, &flags);",
            "\t\tif (ret) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\tbpf_enable_instrumentation();",
            "\t\t\tgoto after_loop;",
            "\t\t}",
            "\t}",
            "",
            "\tbucket_cnt = 0;",
            "\thlist_nulls_for_each_entry_rcu(l, n, head, hash_node)",
            "\t\tbucket_cnt++;",
            "",
            "\tif (bucket_cnt && !locked) {",
            "\t\tlocked = true;",
            "\t\tgoto again_nocopy;",
            "\t}",
            "",
            "\tif (bucket_cnt > (max_count - total)) {",
            "\t\tif (total == 0)",
            "\t\t\tret = -ENOSPC;",
            "\t\t/* Note that since bucket_cnt > 0 here, it is implicit",
            "\t\t * that the locked was grabbed, so release it.",
            "\t\t */",
            "\t\thtab_unlock_bucket(htab, b, batch, flags);",
            "\t\trcu_read_unlock();",
            "\t\tbpf_enable_instrumentation();",
            "\t\tgoto after_loop;",
            "\t}",
            "",
            "\tif (bucket_cnt > bucket_size) {",
            "\t\tbucket_size = bucket_cnt;",
            "\t\t/* Note that since bucket_cnt > 0 here, it is implicit",
            "\t\t * that the locked was grabbed, so release it.",
            "\t\t */",
            "\t\thtab_unlock_bucket(htab, b, batch, flags);",
            "\t\trcu_read_unlock();",
            "\t\tbpf_enable_instrumentation();",
            "\t\tkvfree(keys);",
            "\t\tkvfree(values);",
            "\t\tgoto alloc;",
            "\t}",
            "",
            "\t/* Next block is only safe to run if you have grabbed the lock */",
            "\tif (!locked)",
            "\t\tgoto next_batch;",
            "",
            "\thlist_nulls_for_each_entry_safe(l, n, head, hash_node) {",
            "\t\tmemcpy(dst_key, l->key, key_size);",
            "",
            "\t\tif (is_percpu) {",
            "\t\t\tint off = 0, cpu;",
            "\t\t\tvoid __percpu *pptr;",
            "",
            "\t\t\tpptr = htab_elem_get_ptr(l, map->key_size);",
            "\t\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\t\tcopy_map_value_long(&htab->map, dst_val + off, per_cpu_ptr(pptr, cpu));",
            "\t\t\t\tcheck_and_init_map_value(&htab->map, dst_val + off);",
            "\t\t\t\toff += size;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tvalue = l->key + roundup_key_size;",
            "\t\t\tif (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {",
            "\t\t\t\tstruct bpf_map **inner_map = value;",
            "",
            "\t\t\t\t /* Actual value is the id of the inner map */",
            "\t\t\t\tmap_id = map->ops->map_fd_sys_lookup_elem(*inner_map);",
            "\t\t\t\tvalue = &map_id;",
            "\t\t\t}",
            "",
            "\t\t\tif (elem_map_flags & BPF_F_LOCK)",
            "\t\t\t\tcopy_map_value_locked(map, dst_val, value,",
            "\t\t\t\t\t\t      true);",
            "\t\t\telse",
            "\t\t\t\tcopy_map_value(map, dst_val, value);",
            "\t\t\t/* Zeroing special fields in the temp buffer */",
            "\t\t\tcheck_and_init_map_value(map, dst_val);",
            "\t\t}",
            "\t\tif (do_delete) {",
            "\t\t\thlist_nulls_del_rcu(&l->hash_node);",
            "",
            "\t\t\t/* bpf_lru_push_free() will acquire lru_lock, which",
            "\t\t\t * may cause deadlock. See comments in function",
            "\t\t\t * prealloc_lru_pop(). Let us do bpf_lru_push_free()",
            "\t\t\t * after releasing the bucket lock.",
            "\t\t\t *",
            "\t\t\t * For htab of maps, htab_put_fd_value() in",
            "\t\t\t * free_htab_elem() may acquire a spinlock with bucket",
            "\t\t\t * lock being held and it violates the lock rule, so",
            "\t\t\t * invoke free_htab_elem() after unlock as well.",
            "\t\t\t */",
            "\t\t\tl->batch_flink = node_to_free;",
            "\t\t\tnode_to_free = l;",
            "\t\t}",
            "\t\tdst_key += key_size;",
            "\t\tdst_val += value_size;",
            "\t}",
            "",
            "\thtab_unlock_bucket(htab, b, batch, flags);",
            "\tlocked = false;",
            "",
            "\twhile (node_to_free) {",
            "\t\tl = node_to_free;",
            "\t\tnode_to_free = node_to_free->batch_flink;",
            "\t\tif (is_lru_map)",
            "\t\t\thtab_lru_push_free(htab, l);",
            "\t\telse",
            "\t\t\tfree_htab_elem(htab, l);",
            "\t}",
            "",
            "next_batch:",
            "\t/* If we are not copying data, we can go to next bucket and avoid",
            "\t * unlocking the rcu.",
            "\t */",
            "\tif (!bucket_cnt && (batch + 1 < htab->n_buckets)) {",
            "\t\tbatch++;",
            "\t\tgoto again_nocopy;",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "\tbpf_enable_instrumentation();",
            "\tif (bucket_cnt && (copy_to_user(ukeys + total * key_size, keys,",
            "\t    key_size * bucket_cnt) ||",
            "\t    copy_to_user(uvalues + total * value_size, values,",
            "\t    value_size * bucket_cnt))) {",
            "\t\tret = -EFAULT;",
            "\t\tgoto after_loop;",
            "\t}",
            "",
            "\ttotal += bucket_cnt;",
            "\tbatch++;",
            "\tif (batch >= htab->n_buckets) {",
            "\t\tret = -ENOENT;",
            "\t\tgoto after_loop;",
            "\t}",
            "\tgoto again;",
            "",
            "after_loop:",
            "\tif (ret == -EFAULT)",
            "\t\tgoto out;",
            "",
            "\t/* copy # of entries and next batch */",
            "\tubatch = u64_to_user_ptr(attr->batch.out_batch);",
            "\tif (copy_to_user(ubatch, &batch, sizeof(batch)) ||",
            "\t    put_user(total, &uattr->batch.count))",
            "\t\tret = -EFAULT;",
            "",
            "out:",
            "\tkvfree(keys);",
            "\tkvfree(values);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "htab_lru_percpu_map_lookup_and_delete_elem, __htab_map_lookup_and_delete_batch",
          "description": "处理批量查找删除操作，通过RCU读锁遍历指定桶内元素，支持普通/PERCPU/LRU类型。动态分配缓冲区复制键值，处理锁竞争和内存溢出情况，返回操作结果及统计信息。",
          "similarity": 0.5080671310424805
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 1594,
          "end_line": 1695,
          "content": [
            "static void htab_map_seq_show_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t   struct seq_file *m)",
            "{",
            "\tvoid *value;",
            "",
            "\trcu_read_lock();",
            "",
            "\tvalue = htab_map_lookup_elem(map, key);",
            "\tif (!value) {",
            "\t\trcu_read_unlock();",
            "\t\treturn;",
            "\t}",
            "",
            "\tbtf_type_seq_show(map->btf, map->btf_key_type_id, key, m);",
            "\tseq_puts(m, \": \");",
            "\tbtf_type_seq_show(map->btf, map->btf_value_type_id, value, m);",
            "\tseq_puts(m, \"\\n\");",
            "",
            "\trcu_read_unlock();",
            "}",
            "static int __htab_map_lookup_and_delete_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\t     void *value, bool is_lru_map,",
            "\t\t\t\t\t     bool is_percpu, u64 flags)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tstruct hlist_nulls_head *head;",
            "\tunsigned long bflags;",
            "\tstruct htab_elem *l;",
            "\tu32 hash, key_size;",
            "\tstruct bucket *b;",
            "\tint ret;",
            "",
            "\tkey_size = map->key_size;",
            "",
            "\thash = htab_map_hash(key, key_size, htab->hashrnd);",
            "\tb = __select_bucket(htab, hash);",
            "\thead = &b->head;",
            "",
            "\tret = htab_lock_bucket(htab, b, hash, &bflags);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tl = lookup_elem_raw(head, hash, key, key_size);",
            "\tif (!l) {",
            "\t\tret = -ENOENT;",
            "\t} else {",
            "\t\tif (is_percpu) {",
            "\t\t\tu32 roundup_value_size = round_up(map->value_size, 8);",
            "\t\t\tvoid __percpu *pptr;",
            "\t\t\tint off = 0, cpu;",
            "",
            "\t\t\tpptr = htab_elem_get_ptr(l, key_size);",
            "\t\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\t\tcopy_map_value_long(&htab->map, value + off, per_cpu_ptr(pptr, cpu));",
            "\t\t\t\tcheck_and_init_map_value(&htab->map, value + off);",
            "\t\t\t\toff += roundup_value_size;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tu32 roundup_key_size = round_up(map->key_size, 8);",
            "",
            "\t\t\tif (flags & BPF_F_LOCK)",
            "\t\t\t\tcopy_map_value_locked(map, value, l->key +",
            "\t\t\t\t\t\t      roundup_key_size,",
            "\t\t\t\t\t\t      true);",
            "\t\t\telse",
            "\t\t\t\tcopy_map_value(map, value, l->key +",
            "\t\t\t\t\t       roundup_key_size);",
            "\t\t\t/* Zeroing special fields in the temp buffer */",
            "\t\t\tcheck_and_init_map_value(map, value);",
            "\t\t}",
            "",
            "\t\thlist_nulls_del_rcu(&l->hash_node);",
            "\t\tif (!is_lru_map)",
            "\t\t\tfree_htab_elem(htab, l);",
            "\t}",
            "",
            "\thtab_unlock_bucket(htab, b, hash, bflags);",
            "",
            "\tif (is_lru_map && l)",
            "\t\thtab_lru_push_free(htab, l);",
            "",
            "\treturn ret;",
            "}",
            "static int htab_map_lookup_and_delete_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\t   void *value, u64 flags)",
            "{",
            "\treturn __htab_map_lookup_and_delete_elem(map, key, value, false, false,",
            "\t\t\t\t\t\t flags);",
            "}",
            "static int htab_percpu_map_lookup_and_delete_elem(struct bpf_map *map,",
            "\t\t\t\t\t\t  void *key, void *value,",
            "\t\t\t\t\t\t  u64 flags)",
            "{",
            "\treturn __htab_map_lookup_and_delete_elem(map, key, value, false, true,",
            "\t\t\t\t\t\t flags);",
            "}",
            "static int htab_lru_map_lookup_and_delete_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\t       void *value, u64 flags)",
            "{",
            "\treturn __htab_map_lookup_and_delete_elem(map, key, value, true, false,",
            "\t\t\t\t\t\t flags);",
            "}"
          ],
          "function_name": "htab_map_seq_show_elem, __htab_map_lookup_and_delete_elem, htab_map_lookup_and_delete_elem, htab_percpu_map_lookup_and_delete_elem, htab_lru_map_lookup_and_delete_elem",
          "description": "实现哈希表元素的序列化展示与查找删除操作，支持普通/PERCPU/LRU三种映射类型。通过RCU读锁保护，对找到的元素执行值复制并删除，PERCPU场景下需遍历所有CPU复制值。",
          "similarity": 0.4824581742286682
        },
        {
          "chunk_id": 8,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 1345,
          "end_line": 1459,
          "content": [
            "static long __htab_lru_percpu_map_update_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\t      void *value, u64 map_flags,",
            "\t\t\t\t\t      bool onallcpus)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tstruct htab_elem *l_new = NULL, *l_old;",
            "\tstruct hlist_nulls_head *head;",
            "\tunsigned long flags;",
            "\tstruct bucket *b;",
            "\tu32 key_size, hash;",
            "\tint ret;",
            "",
            "\tif (unlikely(map_flags > BPF_EXIST))",
            "\t\t/* unknown flags */",
            "\t\treturn -EINVAL;",
            "",
            "\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&",
            "\t\t     !rcu_read_lock_bh_held());",
            "",
            "\tkey_size = map->key_size;",
            "",
            "\thash = htab_map_hash(key, key_size, htab->hashrnd);",
            "",
            "\tb = __select_bucket(htab, hash);",
            "\thead = &b->head;",
            "",
            "\t/* For LRU, we need to alloc before taking bucket's",
            "\t * spinlock because LRU's elem alloc may need",
            "\t * to remove older elem from htab and this removal",
            "\t * operation will need a bucket lock.",
            "\t */",
            "\tif (map_flags != BPF_EXIST) {",
            "\t\tl_new = prealloc_lru_pop(htab, key, hash);",
            "\t\tif (!l_new)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\tret = htab_lock_bucket(htab, b, hash, &flags);",
            "\tif (ret)",
            "\t\tgoto err_lock_bucket;",
            "",
            "\tl_old = lookup_elem_raw(head, hash, key, key_size);",
            "",
            "\tret = check_flags(htab, l_old, map_flags);",
            "\tif (ret)",
            "\t\tgoto err;",
            "",
            "\tif (l_old) {",
            "\t\tbpf_lru_node_set_ref(&l_old->lru_node);",
            "",
            "\t\t/* per-cpu hash map can update value in-place */",
            "\t\tpcpu_copy_value(htab, htab_elem_get_ptr(l_old, key_size),",
            "\t\t\t\tvalue, onallcpus);",
            "\t} else {",
            "\t\tpcpu_init_value(htab, htab_elem_get_ptr(l_new, key_size),",
            "\t\t\t\tvalue, onallcpus);",
            "\t\thlist_nulls_add_head_rcu(&l_new->hash_node, head);",
            "\t\tl_new = NULL;",
            "\t}",
            "\tret = 0;",
            "err:",
            "\thtab_unlock_bucket(htab, b, hash, flags);",
            "err_lock_bucket:",
            "\tif (l_new) {",
            "\t\tbpf_map_dec_elem_count(&htab->map);",
            "\t\tbpf_lru_push_free(&htab->lru, &l_new->lru_node);",
            "\t}",
            "\treturn ret;",
            "}",
            "static long htab_percpu_map_update_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\tvoid *value, u64 map_flags)",
            "{",
            "\treturn __htab_percpu_map_update_elem(map, key, value, map_flags, false);",
            "}",
            "static long htab_lru_percpu_map_update_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\t    void *value, u64 map_flags)",
            "{",
            "\treturn __htab_lru_percpu_map_update_elem(map, key, value, map_flags,",
            "\t\t\t\t\t\t false);",
            "}",
            "static long htab_map_delete_elem(struct bpf_map *map, void *key)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tstruct hlist_nulls_head *head;",
            "\tstruct bucket *b;",
            "\tstruct htab_elem *l;",
            "\tunsigned long flags;",
            "\tu32 hash, key_size;",
            "\tint ret;",
            "",
            "\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&",
            "\t\t     !rcu_read_lock_bh_held());",
            "",
            "\tkey_size = map->key_size;",
            "",
            "\thash = htab_map_hash(key, key_size, htab->hashrnd);",
            "\tb = __select_bucket(htab, hash);",
            "\thead = &b->head;",
            "",
            "\tret = htab_lock_bucket(htab, b, hash, &flags);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tl = lookup_elem_raw(head, hash, key, key_size);",
            "\tif (l)",
            "\t\thlist_nulls_del_rcu(&l->hash_node);",
            "\telse",
            "\t\tret = -ENOENT;",
            "",
            "\thtab_unlock_bucket(htab, b, hash, flags);",
            "",
            "\tif (l)",
            "\t\tfree_htab_elem(htab, l);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__htab_lru_percpu_map_update_elem, htab_percpu_map_update_elem, htab_lru_percpu_map_update_elem, htab_map_delete_elem",
          "description": "处理带LRU和Per-CPU特性的元素更新与删除，包含元素锁定、值覆盖、桶解锁及元素释放流程，支持定时器/工作队列清理",
          "similarity": 0.4682885408401489
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 132,
          "end_line": 247,
          "content": [
            "static inline bool htab_is_prealloc(const struct bpf_htab *htab)",
            "{",
            "\treturn !(htab->map.map_flags & BPF_F_NO_PREALLOC);",
            "}",
            "static void htab_init_buckets(struct bpf_htab *htab)",
            "{",
            "\tunsigned int i;",
            "",
            "\tfor (i = 0; i < htab->n_buckets; i++) {",
            "\t\tINIT_HLIST_NULLS_HEAD(&htab->buckets[i].head, i);",
            "\t\traw_spin_lock_init(&htab->buckets[i].raw_lock);",
            "\t\tlockdep_set_class(&htab->buckets[i].raw_lock,",
            "\t\t\t\t\t  &htab->lockdep_key);",
            "\t\tcond_resched();",
            "\t}",
            "}",
            "static inline int htab_lock_bucket(const struct bpf_htab *htab,",
            "\t\t\t\t   struct bucket *b, u32 hash,",
            "\t\t\t\t   unsigned long *pflags)",
            "{",
            "\tunsigned long flags;",
            "",
            "\thash = hash & min_t(u32, HASHTAB_MAP_LOCK_MASK, htab->n_buckets - 1);",
            "",
            "\tpreempt_disable();",
            "\tlocal_irq_save(flags);",
            "\tif (unlikely(__this_cpu_inc_return(*(htab->map_locked[hash])) != 1)) {",
            "\t\t__this_cpu_dec(*(htab->map_locked[hash]));",
            "\t\tlocal_irq_restore(flags);",
            "\t\tpreempt_enable();",
            "\t\treturn -EBUSY;",
            "\t}",
            "",
            "\traw_spin_lock(&b->raw_lock);",
            "\t*pflags = flags;",
            "",
            "\treturn 0;",
            "}",
            "static inline void htab_unlock_bucket(const struct bpf_htab *htab,",
            "\t\t\t\t      struct bucket *b, u32 hash,",
            "\t\t\t\t      unsigned long flags)",
            "{",
            "\thash = hash & min_t(u32, HASHTAB_MAP_LOCK_MASK, htab->n_buckets - 1);",
            "\traw_spin_unlock(&b->raw_lock);",
            "\t__this_cpu_dec(*(htab->map_locked[hash]));",
            "\tlocal_irq_restore(flags);",
            "\tpreempt_enable();",
            "}",
            "static bool htab_is_lru(const struct bpf_htab *htab)",
            "{",
            "\treturn htab->map.map_type == BPF_MAP_TYPE_LRU_HASH ||",
            "\t\thtab->map.map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH;",
            "}",
            "static bool htab_is_percpu(const struct bpf_htab *htab)",
            "{",
            "\treturn htab->map.map_type == BPF_MAP_TYPE_PERCPU_HASH ||",
            "\t\thtab->map.map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH;",
            "}",
            "static inline void htab_elem_set_ptr(struct htab_elem *l, u32 key_size,",
            "\t\t\t\t     void __percpu *pptr)",
            "{",
            "\t*(void __percpu **)(l->key + roundup(key_size, 8)) = pptr;",
            "}",
            "static bool htab_has_extra_elems(struct bpf_htab *htab)",
            "{",
            "\treturn !htab_is_percpu(htab) && !htab_is_lru(htab);",
            "}",
            "static void htab_free_prealloced_timers_and_wq(struct bpf_htab *htab)",
            "{",
            "\tu32 num_entries = htab->map.max_entries;",
            "\tint i;",
            "",
            "\tif (htab_has_extra_elems(htab))",
            "\t\tnum_entries += num_possible_cpus();",
            "",
            "\tfor (i = 0; i < num_entries; i++) {",
            "\t\tstruct htab_elem *elem;",
            "",
            "\t\telem = get_htab_elem(htab, i);",
            "\t\tif (btf_record_has_field(htab->map.record, BPF_TIMER))",
            "\t\t\tbpf_obj_free_timer(htab->map.record,",
            "\t\t\t\t\t   elem->key + round_up(htab->map.key_size, 8));",
            "\t\tif (btf_record_has_field(htab->map.record, BPF_WORKQUEUE))",
            "\t\t\tbpf_obj_free_workqueue(htab->map.record,",
            "\t\t\t\t\t       elem->key + round_up(htab->map.key_size, 8));",
            "\t\tcond_resched();",
            "\t}",
            "}",
            "static void htab_free_prealloced_fields(struct bpf_htab *htab)",
            "{",
            "\tu32 num_entries = htab->map.max_entries;",
            "\tint i;",
            "",
            "\tif (IS_ERR_OR_NULL(htab->map.record))",
            "\t\treturn;",
            "\tif (htab_has_extra_elems(htab))",
            "\t\tnum_entries += num_possible_cpus();",
            "\tfor (i = 0; i < num_entries; i++) {",
            "\t\tstruct htab_elem *elem;",
            "",
            "\t\telem = get_htab_elem(htab, i);",
            "\t\tif (htab_is_percpu(htab)) {",
            "\t\t\tvoid __percpu *pptr = htab_elem_get_ptr(elem, htab->map.key_size);",
            "\t\t\tint cpu;",
            "",
            "\t\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\t\tbpf_obj_free_fields(htab->map.record, per_cpu_ptr(pptr, cpu));",
            "\t\t\t\tcond_resched();",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tbpf_obj_free_fields(htab->map.record, elem->key + round_up(htab->map.key_size, 8));",
            "\t\t\tcond_resched();",
            "\t\t}",
            "\t\tcond_resched();",
            "\t}",
            "}"
          ],
          "function_name": "htab_is_prealloc, htab_init_buckets, htab_lock_bucket, htab_unlock_bucket, htab_is_lru, htab_is_percpu, htab_elem_set_ptr, htab_has_extra_elems, htab_free_prealloced_timers_and_wq, htab_free_prealloced_fields",
          "description": "实现哈希表的桶锁管理、预分配检测、LRU/PCPU类型判断等功能，包含桶初始化、锁获取/释放、预分配元素清理等关键逻辑，通过锁保护确保多线程环境下的操作原子性。",
          "similarity": 0.4650907516479492
        }
      ]
    }
  ]
}