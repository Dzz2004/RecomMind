{
  "query": "工作集在操作系统中的定义",
  "timestamp": "2025-12-26 00:06:42",
  "retrieved_files": [
    {
      "source_file": "kernel/workqueue_internal.h",
      "md_summary": "> 自动生成时间: 2025-10-25 17:54:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workqueue_internal.h`\n\n---\n\n# `workqueue_internal.h` 技术文档\n\n## 1. 文件概述\n\n`workqueue_internal.h` 是 Linux 内核工作队列（workqueue）子系统的内部头文件，仅限工作队列核心代码及内核关键子系统（如 `async` 和调度器）包含使用。该文件定义了工作队列内部使用的 `struct worker` 数据结构，并声明了调度器与工作队列交互所需的钩子函数。其主要作用是封装工作线程（worker）的内部状态和行为，为并发管理型工作队列（Concurrency Managed Workqueue, CMWQ）提供底层支持。\n\n## 2. 核心功能\n\n### 数据结构\n\n- **`struct worker`**  \n  表示一个工作队列的工作线程（worker），包含其运行状态、当前处理的工作项、所属线程池、调度信息等。关键字段包括：\n  - `entry` / `hentry`：联合体，用于在空闲时挂入空闲链表，繁忙时挂入哈希表。\n  - `current_work` / `current_func`：当前正在执行的工作项及其回调函数。\n  - `current_pwq`：当前工作项所属的 `pool_workqueue`。\n  - `sleeping`：标识该 worker 是否处于睡眠状态。\n  - `scheduled`：已调度但尚未执行的工作项链表。\n  - `task`：对应的内核线程（kthread）任务结构。\n  - `pool`：所属的 `worker_pool`。\n  - `flags` / `id`：worker 的标志位和唯一标识。\n  - `desc`：用于调试的描述字符串（通过 `work_set_desc()` 设置）。\n  - `rescue_wq`：仅用于 rescuer worker，指向需要被救援的工作队列。\n\n- **内联函数**\n  - **`current_wq_worker()`**：判断当前执行上下文是否为工作队列 worker 线程。若是，则返回对应的 `struct worker` 指针；否则返回 `NULL`。通过检查 `current->flags & PF_WQ_WORKER` 并调用 `kthread_data()` 实现。\n\n### 函数声明（调度器钩子）\n\n- **`wq_worker_running(struct task_struct *task)`**  \n  通知工作队列子系统：指定 worker 线程已开始运行。\n\n- **`wq_worker_sleeping(struct task_struct *task)`**  \n  通知工作队列子系统：指定 worker 线程即将进入睡眠状态。\n\n- **`wq_worker_tick(struct task_struct *task)`**  \n  由调度器周期性调用，用于更新 worker 的运行时统计信息（如 CPU 时间）。\n\n- **`wq_worker_last_func(struct task_struct *task)`**  \n  返回指定 worker 线程最近执行的工作函数指针，供调度器或调试使用。\n\n## 3. 关键实现\n\n- **Worker 状态管理**  \n  `struct worker` 使用联合体 `entry/hentry` 实现状态复用：空闲时通过 `entry` 挂入 `worker_pool` 的空闲链表；执行工作时通过 `hentry` 挂入 busy 哈希表，便于快速查找和管理。\n\n- **并发管理支持**  \n  通过 `sleeping` 字段和调度器钩子函数（如 `wq_worker_sleeping`/`wq_worker_running`），工作队列子系统可精确跟踪 worker 的运行状态，从而动态调整线程池大小，实现高效的并发控制。\n\n- **调试支持**  \n  `desc` 字段允许通过 `work_set_desc()` 为工作项设置可读描述，在内核崩溃（WARN/BUG/panic）或 SysRq 调试时输出，便于定位问题。\n\n- **Rescuer 机制**  \n  `rescue_wq` 字段专用于 rescuer worker（用于处理内存压力下无法创建新 worker 的紧急情况），指向需要被“救援”的工作队列。\n\n- **锁注释约定**  \n  结构体字段注释中的字母（如 `L`, `K`, `I`, `A`, `S`）表示访问该字段所需的锁或上下文，具体含义需参考 `workqueue.c` 中的说明（例如 `L` 表示 pool->lock，`K` 表示需要关闭内核抢占等）。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/workqueue.h>`：提供工作队列公共接口和基础类型（如 `work_struct`、`work_func_t`）。\n  - `<linux/kthread.h>`：提供内核线程相关功能（如 `kthread_data()`）。\n  - `<linux/preempt.h>`：用于内核抢占控制。\n\n- **模块依赖**：\n  - **`workqueue.c`**：工作队列核心实现，定义了 `struct worker` 的操作逻辑。\n  - **`kernel/async.c`**：异步初始化框架，使用内部 worker 结构。\n  - **`kernel/sched/`**：调度器子系统，调用 `wq_worker_*` 钩子函数以集成工作队列状态管理。\n\n## 5. 使用场景\n\n- **工作队列执行路径**  \n  当工作项被调度执行时，内核从 `worker_pool` 中唤醒或创建 `worker`，通过 `current_wq_worker()` 获取当前上下文的 worker 结构，并更新其状态字段（如 `current_work`、`last_func`）。\n\n- **调度器集成**  \n  调度器在 worker 线程状态切换（运行/睡眠）或时钟滴答（tick）时调用相应钩子，使工作队列子系统能动态管理线程池并发度。\n\n- **内存压力恢复**  \n  在内存紧张无法创建新 worker 时，rescuer worker 被激活，通过 `rescue_wq` 字段处理阻塞的工作队列。\n\n- **内核调试与诊断**  \n  在系统崩溃或通过 SysRq 触发任务转储时，`desc` 字段提供工作项的语义信息，辅助开发者分析死锁或性能问题。",
      "similarity": 0.5779181122779846,
      "chunks": []
    },
    {
      "source_file": "kernel/workqueue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:53:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workqueue.c`\n\n---\n\n# workqueue.c 技术文档\n\n## 1. 文件概述\n\n`workqueue.c` 是 Linux 内核中实现通用异步执行机制的核心文件，提供基于共享工作线程池（worker pool）的延迟任务调度功能。工作项（work items）在进程上下文中执行，支持 CPU 绑定和非绑定两种模式。每个 CPU 默认拥有两个标准工作池（普通优先级和高优先级），同时支持动态创建非绑定工作池以满足不同工作队列的需求。该机制替代了早期的 taskqueue/keventd 实现，具有更高的可扩展性和资源利用率。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct worker_pool`**  \n  工作线程池结构体，管理一组工作线程（workers），包含：\n  - `lock`：保护池状态的自旋锁\n  - `cpu` / `node`：关联的 CPU 和 NUMA 节点（绑定池）\n  - `worklist`：待处理工作项队列\n  - `idle_list` / `busy_hash`：空闲和忙碌工作线程的管理结构\n  - `nr_workers` / `nr_idle`：工作线程数量统计\n  - `attrs`：工作线程属性（如优先级、CPU 亲和性）\n  - `mayday_timer`：紧急情况下的救援请求定时器\n\n- **`struct pool_workqueue`**  \n  工作队列与工作池之间的关联结构，每个工作队列在每个池中都有一个对应的 `pool_workqueue` 实例，用于：\n  - 管理工作项的入队和执行\n  - 实现 `max_active` 限制（控制并发执行数）\n  - 支持 flush 操作（等待所有工作完成）\n  - 统计性能指标（如启动/完成次数、CPU 时间等）\n\n- **`struct worker`**（定义在 `workqueue_internal.h`）  \n  工作线程的运行时上下文，包含状态标志（如 `WORKER_IDLE`, `WORKER_UNBOUND`）、当前执行的工作项等。\n\n### 关键枚举与常量\n\n- **池/工作线程标志**：\n  - `POOL_DISASSOCIATED`：CPU 离线时池进入非绑定状态\n  - `WORKER_UNBOUND`：工作线程可在任意 CPU 上运行\n  - `WORKER_CPU_INTENSIVE`：标记 CPU 密集型任务，影响并发控制\n\n- **配置参数**：\n  - `NR_STD_WORKER_POOLS = 2`：每 CPU 标准池数量（普通 + 高优先级）\n  - `IDLE_WORKER_TIMEOUT = 300 * HZ`：空闲线程保留时间（5 分钟）\n  - `MAYDAY_INITIAL_TIMEOUT`：工作积压时触发救援的延迟（10ms）\n\n- **统计指标**（`pool_workqueue_stats`）：\n  - `PWQ_STAT_STARTED` / `PWQ_STAT_COMPLETED`：工作项执行统计\n  - `PWQ_STAT_MAYDAY` / `PWQ_STAT_RESCUED`：紧急救援事件计数\n\n## 3. 关键实现\n\n### 工作池管理\n- **绑定池（Bound Pool）**：与特定 CPU 关联，工作线程默认绑定到该 CPU。当 CPU 离线时，池进入 `DISASSOCIATED` 状态，工作线程转为非绑定模式。\n- **非绑定池（Unbound Pool）**：动态创建，通过哈希表（`unbound_pool_hash`）按属性（`workqueue_attrs`）去重，支持跨 CPU 调度。\n- **并发控制**：通过 `nr_running` 计数器和 `max_active` 限制，防止工作项过度并发执行。\n\n### 工作线程生命周期\n- **空闲管理**：空闲线程加入 `idle_list`，超时（`IDLE_WORKER_TIMEOUT`）后被回收。\n- **动态伸缩**：当工作积压时，通过 `mayday_timer` 触发新线程创建；若创建失败，向全局救援线程（rescuer）求助。\n- **状态标志**：使用位标志（如 `WORKER_IDLE`, `WORKER_PREP`）高效管理线程状态，避免锁竞争。\n\n### 内存与同步\n- **RCU 保护**：工作池销毁通过 RCU 延迟释放，确保 `get_work_pool()` 等读取路径无锁安全。\n- **锁分层**：\n  - `pool->lock`（自旋锁）：保护池内部状态\n  - `wq_pool_mutex`：全局池管理互斥锁\n  - `wq_pool_attach_mutex`：防止 CPU 绑定状态变更冲突\n\n### 工作项调度\n- **数据指针复用**：`work_struct->data` 的高有效位存储 `pool_workqueue` 指针，低有效位用于标志位（如 `WORK_STRUCT_INACTIVE`）。\n- **优先级支持**：高优先级工作池使用 `HIGHPRI_NICE_LEVEL = MIN_NICE` 提升调度优先级。\n\n## 4. 依赖关系\n\n- **内核子系统**：\n  - **调度器**（`<linux/sched.h>`）：创建工作线程（kworker），管理 CPU 亲和性\n  - **内存管理**（`<linux/slab.h>`）：分配工作池、工作队列等结构\n  - **CPU 热插拔**（`<linux/cpu.h>`）：处理 CPU 上下线时的池绑定状态切换\n  - **RCU**（`<linux/rculist.h>`）：实现无锁读取路径\n  - **定时器**（`<linux/timer.h>`）：实现空闲超时和救援机制\n\n- **内部依赖**：\n  - `workqueue_internal.h`：定义 `struct worker` 等内部结构\n  - `Documentation/core-api/workqueue.rst`：详细设计文档\n\n## 5. 使用场景\n\n- **驱动程序延迟操作**：硬件中断后调度下半部处理（如网络包处理、磁盘 I/O 完成回调）。\n- **内核子系统异步任务**：文件系统元数据更新、内存回收、电源管理状态切换。\n- **高优先级任务**：使用 `WQ_HIGHPRI` 标志创建工作队列，确保关键任务及时执行（如死锁恢复）。\n- **CPU 密集型任务**：标记 `WQ_CPU_INTENSIVE` 避免占用过多并发槽位，提升系统响应性。\n- **NUMA 感知调度**：非绑定工作队列可指定 NUMA 节点，优化内存访问延迟。",
      "similarity": 0.5775156617164612,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "kernel/workqueue.c",
          "start_line": 1334,
          "end_line": 1463,
          "content": [
            "void wq_worker_tick(struct task_struct *task)",
            "{",
            "\tstruct worker *worker = kthread_data(task);",
            "\tstruct pool_workqueue *pwq = worker->current_pwq;",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tif (!pwq)",
            "\t\treturn;",
            "",
            "\tpwq->stats[PWQ_STAT_CPU_TIME] += TICK_USEC;",
            "",
            "\tif (!wq_cpu_intensive_thresh_us)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If the current worker is concurrency managed and hogged the CPU for",
            "\t * longer than wq_cpu_intensive_thresh_us, it's automatically marked",
            "\t * CPU_INTENSIVE to avoid stalling other concurrency-managed work items.",
            "\t *",
            "\t * Set @worker->sleeping means that @worker is in the process of",
            "\t * switching out voluntarily and won't be contributing to",
            "\t * @pool->nr_running until it wakes up. As wq_worker_sleeping() also",
            "\t * decrements ->nr_running, setting CPU_INTENSIVE here can lead to",
            "\t * double decrements. The task is releasing the CPU anyway. Let's skip.",
            "\t * We probably want to make this prettier in the future.",
            "\t */",
            "\tif ((worker->flags & WORKER_NOT_RUNNING) || READ_ONCE(worker->sleeping) ||",
            "\t    worker->task->se.sum_exec_runtime - worker->current_at <",
            "\t    wq_cpu_intensive_thresh_us * NSEC_PER_USEC)",
            "\t\treturn;",
            "",
            "\traw_spin_lock(&pool->lock);",
            "",
            "\tworker_set_flags(worker, WORKER_CPU_INTENSIVE);",
            "\twq_cpu_intensive_report(worker->current_func);",
            "\tpwq->stats[PWQ_STAT_CPU_INTENSIVE]++;",
            "",
            "\tif (kick_pool(pool))",
            "\t\tpwq->stats[PWQ_STAT_CM_WAKEUP]++;",
            "",
            "\traw_spin_unlock(&pool->lock);",
            "}",
            "work_func_t wq_worker_last_func(struct task_struct *task)",
            "{",
            "\tstruct worker *worker = kthread_data(task);",
            "",
            "\treturn worker->last_func;",
            "}",
            "static void get_pwq(struct pool_workqueue *pwq)",
            "{",
            "\tlockdep_assert_held(&pwq->pool->lock);",
            "\tWARN_ON_ONCE(pwq->refcnt <= 0);",
            "\tpwq->refcnt++;",
            "}",
            "static void put_pwq(struct pool_workqueue *pwq)",
            "{",
            "\tlockdep_assert_held(&pwq->pool->lock);",
            "\tif (likely(--pwq->refcnt))",
            "\t\treturn;",
            "\t/*",
            "\t * @pwq can't be released under pool->lock, bounce to a dedicated",
            "\t * kthread_worker to avoid A-A deadlocks.",
            "\t */",
            "\tkthread_queue_work(pwq_release_worker, &pwq->release_work);",
            "}",
            "static void put_pwq_unlocked(struct pool_workqueue *pwq)",
            "{",
            "\tif (pwq) {",
            "\t\t/*",
            "\t\t * As both pwqs and pools are RCU protected, the",
            "\t\t * following lock operations are safe.",
            "\t\t */",
            "\t\traw_spin_lock_irq(&pwq->pool->lock);",
            "\t\tput_pwq(pwq);",
            "\t\traw_spin_unlock_irq(&pwq->pool->lock);",
            "\t}",
            "}",
            "static void pwq_activate_inactive_work(struct work_struct *work)",
            "{",
            "\tstruct pool_workqueue *pwq = get_work_pwq(work);",
            "",
            "\ttrace_workqueue_activate_work(work);",
            "\tif (list_empty(&pwq->pool->worklist))",
            "\t\tpwq->pool->watchdog_ts = jiffies;",
            "\tmove_linked_works(work, &pwq->pool->worklist, NULL);",
            "\t__clear_bit(WORK_STRUCT_INACTIVE_BIT, work_data_bits(work));",
            "\tpwq->nr_active++;",
            "}",
            "static void pwq_activate_first_inactive(struct pool_workqueue *pwq)",
            "{",
            "\tstruct work_struct *work = list_first_entry(&pwq->inactive_works,",
            "\t\t\t\t\t\t    struct work_struct, entry);",
            "",
            "\tpwq_activate_inactive_work(work);",
            "}",
            "static void pwq_dec_nr_in_flight(struct pool_workqueue *pwq, unsigned long work_data)",
            "{",
            "\tint color = get_work_color(work_data);",
            "",
            "\tif (!(work_data & WORK_STRUCT_INACTIVE)) {",
            "\t\tpwq->nr_active--;",
            "\t\tif (!list_empty(&pwq->inactive_works)) {",
            "\t\t\t/* one down, submit an inactive one */",
            "\t\t\tif (pwq->nr_active < pwq->max_active)",
            "\t\t\t\tpwq_activate_first_inactive(pwq);",
            "\t\t}",
            "\t}",
            "",
            "\tpwq->nr_in_flight[color]--;",
            "",
            "\t/* is flush in progress and are we at the flushing tip? */",
            "\tif (likely(pwq->flush_color != color))",
            "\t\tgoto out_put;",
            "",
            "\t/* are there still in-flight works? */",
            "\tif (pwq->nr_in_flight[color])",
            "\t\tgoto out_put;",
            "",
            "\t/* this pwq is done, clear flush_color */",
            "\tpwq->flush_color = -1;",
            "",
            "\t/*",
            "\t * If this was the last pwq, wake up the first flusher.  It",
            "\t * will handle the rest.",
            "\t */",
            "\tif (atomic_dec_and_test(&pwq->wq->nr_pwqs_to_flush))",
            "\t\tcomplete(&pwq->wq->first_flusher->done);",
            "out_put:",
            "\tput_pwq(pwq);",
            "}"
          ],
          "function_name": "wq_worker_tick, wq_worker_last_func, get_pwq, put_pwq, put_pwq_unlocked, pwq_activate_inactive_work, pwq_activate_first_inactive, pwq_dec_nr_in_flight",
          "description": "该代码块定义了工作队列中worker的CPU使用监控及PWQ管理。wq_worker_tick检测worker是否成为CPU密集型任务并更新统计信息；get_pwq/put_pwq管理PWQ引用计数；pwq_activate_inactive_work激活处于inactive状态的工作项并更新nr_active计数。",
          "similarity": 0.5875351428985596
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/workqueue.c",
          "start_line": 1,
          "end_line": 524,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * kernel/workqueue.c - generic async execution with shared worker pool",
            " *",
            " * Copyright (C) 2002\t\tIngo Molnar",
            " *",
            " *   Derived from the taskqueue/keventd code by:",
            " *     David Woodhouse <dwmw2@infradead.org>",
            " *     Andrew Morton",
            " *     Kai Petzke <wpp@marie.physik.tu-berlin.de>",
            " *     Theodore Ts'o <tytso@mit.edu>",
            " *",
            " * Made to use alloc_percpu by Christoph Lameter.",
            " *",
            " * Copyright (C) 2010\t\tSUSE Linux Products GmbH",
            " * Copyright (C) 2010\t\tTejun Heo <tj@kernel.org>",
            " *",
            " * This is the generic async execution mechanism.  Work items as are",
            " * executed in process context.  The worker pool is shared and",
            " * automatically managed.  There are two worker pools for each CPU (one for",
            " * normal work items and the other for high priority ones) and some extra",
            " * pools for workqueues which are not bound to any specific CPU - the",
            " * number of these backing pools is dynamic.",
            " *",
            " * Please read Documentation/core-api/workqueue.rst for details.",
            " */",
            "",
            "#include <linux/export.h>",
            "#include <linux/kernel.h>",
            "#include <linux/sched.h>",
            "#include <linux/init.h>",
            "#include <linux/signal.h>",
            "#include <linux/completion.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/slab.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/freezer.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/idr.h>",
            "#include <linux/jhash.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/rculist.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/nmi.h>",
            "#include <linux/kvm_para.h>",
            "#include <linux/delay.h>",
            "",
            "#include \"workqueue_internal.h\"",
            "",
            "enum {",
            "\t/*",
            "\t * worker_pool flags",
            "\t *",
            "\t * A bound pool is either associated or disassociated with its CPU.",
            "\t * While associated (!DISASSOCIATED), all workers are bound to the",
            "\t * CPU and none has %WORKER_UNBOUND set and concurrency management",
            "\t * is in effect.",
            "\t *",
            "\t * While DISASSOCIATED, the cpu may be offline and all workers have",
            "\t * %WORKER_UNBOUND set and concurrency management disabled, and may",
            "\t * be executing on any CPU.  The pool behaves as an unbound one.",
            "\t *",
            "\t * Note that DISASSOCIATED should be flipped only while holding",
            "\t * wq_pool_attach_mutex to avoid changing binding state while",
            "\t * worker_attach_to_pool() is in progress.",
            "\t */",
            "\tPOOL_MANAGER_ACTIVE\t= 1 << 0,\t/* being managed */",
            "\tPOOL_DISASSOCIATED\t= 1 << 2,\t/* cpu can't serve workers */",
            "",
            "\t/* worker flags */",
            "\tWORKER_DIE\t\t= 1 << 1,\t/* die die die */",
            "\tWORKER_IDLE\t\t= 1 << 2,\t/* is idle */",
            "\tWORKER_PREP\t\t= 1 << 3,\t/* preparing to run works */",
            "\tWORKER_CPU_INTENSIVE\t= 1 << 6,\t/* cpu intensive */",
            "\tWORKER_UNBOUND\t\t= 1 << 7,\t/* worker is unbound */",
            "\tWORKER_REBOUND\t\t= 1 << 8,\t/* worker was rebound */",
            "",
            "\tWORKER_NOT_RUNNING\t= WORKER_PREP | WORKER_CPU_INTENSIVE |",
            "\t\t\t\t  WORKER_UNBOUND | WORKER_REBOUND,",
            "",
            "\tNR_STD_WORKER_POOLS\t= 2,\t\t/* # standard pools per cpu */",
            "",
            "\tUNBOUND_POOL_HASH_ORDER\t= 6,\t\t/* hashed by pool->attrs */",
            "\tBUSY_WORKER_HASH_ORDER\t= 6,\t\t/* 64 pointers */",
            "",
            "\tMAX_IDLE_WORKERS_RATIO\t= 4,\t\t/* 1/4 of busy can be idle */",
            "\tIDLE_WORKER_TIMEOUT\t= 300 * HZ,\t/* keep idle ones for 5 mins */",
            "",
            "\tMAYDAY_INITIAL_TIMEOUT  = HZ / 100 >= 2 ? HZ / 100 : 2,",
            "\t\t\t\t\t\t/* call for help after 10ms",
            "\t\t\t\t\t\t   (min two ticks) */",
            "\tMAYDAY_INTERVAL\t\t= HZ / 10,\t/* and then every 100ms */",
            "\tCREATE_COOLDOWN\t\t= HZ,\t\t/* time to breath after fail */",
            "",
            "\t/*",
            "\t * Rescue workers are used only on emergencies and shared by",
            "\t * all cpus.  Give MIN_NICE.",
            "\t */",
            "\tRESCUER_NICE_LEVEL\t= MIN_NICE,",
            "\tHIGHPRI_NICE_LEVEL\t= MIN_NICE,",
            "",
            "\tWQ_NAME_LEN\t\t= 24,",
            "};",
            "",
            "/*",
            " * Structure fields follow one of the following exclusion rules.",
            " *",
            " * I: Modifiable by initialization/destruction paths and read-only for",
            " *    everyone else.",
            " *",
            " * P: Preemption protected.  Disabling preemption is enough and should",
            " *    only be modified and accessed from the local cpu.",
            " *",
            " * L: pool->lock protected.  Access with pool->lock held.",
            " *",
            " * K: Only modified by worker while holding pool->lock. Can be safely read by",
            " *    self, while holding pool->lock or from IRQ context if %current is the",
            " *    kworker.",
            " *",
            " * S: Only modified by worker self.",
            " *",
            " * A: wq_pool_attach_mutex protected.",
            " *",
            " * PL: wq_pool_mutex protected.",
            " *",
            " * PR: wq_pool_mutex protected for writes.  RCU protected for reads.",
            " *",
            " * PW: wq_pool_mutex and wq->mutex protected for writes.  Either for reads.",
            " *",
            " * PWR: wq_pool_mutex and wq->mutex protected for writes.  Either or",
            " *      RCU for reads.",
            " *",
            " * WQ: wq->mutex protected.",
            " *",
            " * WR: wq->mutex protected for writes.  RCU protected for reads.",
            " *",
            " * MD: wq_mayday_lock protected.",
            " *",
            " * WD: Used internally by the watchdog.",
            " */",
            "",
            "/* struct worker is defined in workqueue_internal.h */",
            "",
            "struct worker_pool {",
            "\traw_spinlock_t\t\tlock;\t\t/* the pool lock */",
            "\tint\t\t\tcpu;\t\t/* I: the associated cpu */",
            "\tint\t\t\tnode;\t\t/* I: the associated node ID */",
            "\tint\t\t\tid;\t\t/* I: pool ID */",
            "\tunsigned int\t\tflags;\t\t/* L: flags */",
            "",
            "\tunsigned long\t\twatchdog_ts;\t/* L: watchdog timestamp */",
            "\tbool\t\t\tcpu_stall;\t/* WD: stalled cpu bound pool */",
            "",
            "\t/*",
            "\t * The counter is incremented in a process context on the associated CPU",
            "\t * w/ preemption disabled, and decremented or reset in the same context",
            "\t * but w/ pool->lock held. The readers grab pool->lock and are",
            "\t * guaranteed to see if the counter reached zero.",
            "\t */",
            "\tint\t\t\tnr_running;",
            "",
            "\tstruct list_head\tworklist;\t/* L: list of pending works */",
            "",
            "\tint\t\t\tnr_workers;\t/* L: total number of workers */",
            "\tint\t\t\tnr_idle;\t/* L: currently idle workers */",
            "",
            "\tstruct list_head\tidle_list;\t/* L: list of idle workers */",
            "\tstruct timer_list\tidle_timer;\t/* L: worker idle timeout */",
            "\tstruct work_struct      idle_cull_work; /* L: worker idle cleanup */",
            "",
            "\tstruct timer_list\tmayday_timer;\t  /* L: SOS timer for workers */",
            "",
            "\t/* a workers is either on busy_hash or idle_list, or the manager */",
            "\tDECLARE_HASHTABLE(busy_hash, BUSY_WORKER_HASH_ORDER);",
            "\t\t\t\t\t\t/* L: hash of busy workers */",
            "",
            "\tstruct worker\t\t*manager;\t/* L: purely informational */",
            "\tstruct list_head\tworkers;\t/* A: attached workers */",
            "\tstruct list_head        dying_workers;  /* A: workers about to die */",
            "\tstruct completion\t*detach_completion; /* all workers detached */",
            "",
            "\tstruct ida\t\tworker_ida;\t/* worker IDs for task name */",
            "",
            "\tstruct workqueue_attrs\t*attrs;\t\t/* I: worker attributes */",
            "\tstruct hlist_node\thash_node;\t/* PL: unbound_pool_hash node */",
            "\tint\t\t\trefcnt;\t\t/* PL: refcnt for unbound pools */",
            "",
            "\t/*",
            "\t * Destruction of pool is RCU protected to allow dereferences",
            "\t * from get_work_pool().",
            "\t */",
            "\tstruct rcu_head\t\trcu;",
            "};",
            "",
            "/*",
            " * Per-pool_workqueue statistics. These can be monitored using",
            " * tools/workqueue/wq_monitor.py.",
            " */",
            "enum pool_workqueue_stats {",
            "\tPWQ_STAT_STARTED,\t/* work items started execution */",
            "\tPWQ_STAT_COMPLETED,\t/* work items completed execution */",
            "\tPWQ_STAT_CPU_TIME,\t/* total CPU time consumed */",
            "\tPWQ_STAT_CPU_INTENSIVE,\t/* wq_cpu_intensive_thresh_us violations */",
            "\tPWQ_STAT_CM_WAKEUP,\t/* concurrency-management worker wakeups */",
            "\tPWQ_STAT_REPATRIATED,\t/* unbound workers brought back into scope */",
            "\tPWQ_STAT_MAYDAY,\t/* maydays to rescuer */",
            "\tPWQ_STAT_RESCUED,\t/* linked work items executed by rescuer */",
            "",
            "\tPWQ_NR_STATS,",
            "};",
            "",
            "/*",
            " * The per-pool workqueue.  While queued, the lower WORK_STRUCT_FLAG_BITS",
            " * of work_struct->data are used for flags and the remaining high bits",
            " * point to the pwq; thus, pwqs need to be aligned at two's power of the",
            " * number of flag bits.",
            " */",
            "struct pool_workqueue {",
            "\tstruct worker_pool\t*pool;\t\t/* I: the associated pool */",
            "\tstruct workqueue_struct *wq;\t\t/* I: the owning workqueue */",
            "\tint\t\t\twork_color;\t/* L: current color */",
            "\tint\t\t\tflush_color;\t/* L: flushing color */",
            "\tint\t\t\trefcnt;\t\t/* L: reference count */",
            "\tint\t\t\tnr_in_flight[WORK_NR_COLORS];",
            "\t\t\t\t\t\t/* L: nr of in_flight works */",
            "",
            "\t/*",
            "\t * nr_active management and WORK_STRUCT_INACTIVE:",
            "\t *",
            "\t * When pwq->nr_active >= max_active, new work item is queued to",
            "\t * pwq->inactive_works instead of pool->worklist and marked with",
            "\t * WORK_STRUCT_INACTIVE.",
            "\t *",
            "\t * All work items marked with WORK_STRUCT_INACTIVE do not participate",
            "\t * in pwq->nr_active and all work items in pwq->inactive_works are",
            "\t * marked with WORK_STRUCT_INACTIVE.  But not all WORK_STRUCT_INACTIVE",
            "\t * work items are in pwq->inactive_works.  Some of them are ready to",
            "\t * run in pool->worklist or worker->scheduled.  Those work itmes are",
            "\t * only struct wq_barrier which is used for flush_work() and should",
            "\t * not participate in pwq->nr_active.  For non-barrier work item, it",
            "\t * is marked with WORK_STRUCT_INACTIVE iff it is in pwq->inactive_works.",
            "\t */",
            "\tint\t\t\tnr_active;\t/* L: nr of active works */",
            "\tint\t\t\tmax_active;\t/* L: max active works */",
            "\tstruct list_head\tinactive_works;\t/* L: inactive works */",
            "\tstruct list_head\tpwqs_node;\t/* WR: node on wq->pwqs */",
            "\tstruct list_head\tmayday_node;\t/* MD: node on wq->maydays */",
            "",
            "\tu64\t\t\tstats[PWQ_NR_STATS];",
            "",
            "\t/*",
            "\t * Release of unbound pwq is punted to a kthread_worker. See put_pwq()",
            "\t * and pwq_release_workfn() for details. pool_workqueue itself is also",
            "\t * RCU protected so that the first pwq can be determined without",
            "\t * grabbing wq->mutex.",
            "\t */",
            "\tstruct kthread_work\trelease_work;",
            "\tstruct rcu_head\t\trcu;",
            "} __aligned(1 << WORK_STRUCT_FLAG_BITS);",
            "",
            "/*",
            " * Structure used to wait for workqueue flush.",
            " */",
            "struct wq_flusher {",
            "\tstruct list_head\tlist;\t\t/* WQ: list of flushers */",
            "\tint\t\t\tflush_color;\t/* WQ: flush color waiting for */",
            "\tstruct completion\tdone;\t\t/* flush completion */",
            "};",
            "",
            "struct wq_device;",
            "",
            "/*",
            " * The externally visible workqueue.  It relays the issued work items to",
            " * the appropriate worker_pool through its pool_workqueues.",
            " */",
            "struct workqueue_struct {",
            "\tstruct list_head\tpwqs;\t\t/* WR: all pwqs of this wq */",
            "\tstruct list_head\tlist;\t\t/* PR: list of all workqueues */",
            "",
            "\tstruct mutex\t\tmutex;\t\t/* protects this wq */",
            "\tint\t\t\twork_color;\t/* WQ: current work color */",
            "\tint\t\t\tflush_color;\t/* WQ: current flush color */",
            "\tatomic_t\t\tnr_pwqs_to_flush; /* flush in progress */",
            "\tstruct wq_flusher\t*first_flusher;\t/* WQ: first flusher */",
            "\tstruct list_head\tflusher_queue;\t/* WQ: flush waiters */",
            "\tstruct list_head\tflusher_overflow; /* WQ: flush overflow list */",
            "",
            "\tstruct list_head\tmaydays;\t/* MD: pwqs requesting rescue */",
            "\tstruct worker\t\t*rescuer;\t/* MD: rescue worker */",
            "",
            "\tint\t\t\tnr_drainers;\t/* WQ: drain in progress */",
            "\tint\t\t\tsaved_max_active; /* WQ: saved pwq max_active */",
            "",
            "\tstruct workqueue_attrs\t*unbound_attrs;\t/* PW: only for unbound wqs */",
            "\tstruct pool_workqueue\t*dfl_pwq;\t/* PW: only for unbound wqs */",
            "",
            "#ifdef CONFIG_SYSFS",
            "\tstruct wq_device\t*wq_dev;\t/* I: for sysfs interface */",
            "#endif",
            "#ifdef CONFIG_LOCKDEP",
            "\tchar\t\t\t*lock_name;",
            "\tstruct lock_class_key\tkey;",
            "\tstruct lockdep_map\tlockdep_map;",
            "#endif",
            "\tchar\t\t\tname[WQ_NAME_LEN]; /* I: workqueue name */",
            "",
            "\t/*",
            "\t * Destruction of workqueue_struct is RCU protected to allow walking",
            "\t * the workqueues list without grabbing wq_pool_mutex.",
            "\t * This is used to dump all workqueues from sysrq.",
            "\t */",
            "\tstruct rcu_head\t\trcu;",
            "",
            "\t/* hot fields used during command issue, aligned to cacheline */",
            "\tunsigned int\t\tflags ____cacheline_aligned; /* WQ: WQ_* flags */",
            "\tstruct pool_workqueue __percpu __rcu **cpu_pwq; /* I: per-cpu pwqs */",
            "};",
            "",
            "static struct kmem_cache *pwq_cache;",
            "",
            "/*",
            " * Each pod type describes how CPUs should be grouped for unbound workqueues.",
            " * See the comment above workqueue_attrs->affn_scope.",
            " */",
            "struct wq_pod_type {",
            "\tint\t\t\tnr_pods;\t/* number of pods */",
            "\tcpumask_var_t\t\t*pod_cpus;\t/* pod -> cpus */",
            "\tint\t\t\t*pod_node;\t/* pod -> node */",
            "\tint\t\t\t*cpu_pod;\t/* cpu -> pod */",
            "};",
            "",
            "static struct wq_pod_type wq_pod_types[WQ_AFFN_NR_TYPES];",
            "static enum wq_affn_scope wq_affn_dfl = WQ_AFFN_CACHE;",
            "",
            "static const char *wq_affn_names[WQ_AFFN_NR_TYPES] = {",
            "\t[WQ_AFFN_DFL]\t\t\t= \"default\",",
            "\t[WQ_AFFN_CPU]\t\t\t= \"cpu\",",
            "\t[WQ_AFFN_SMT]\t\t\t= \"smt\",",
            "\t[WQ_AFFN_CACHE]\t\t\t= \"cache\",",
            "\t[WQ_AFFN_NUMA]\t\t\t= \"numa\",",
            "\t[WQ_AFFN_SYSTEM]\t\t= \"system\",",
            "};",
            "",
            "/*",
            " * Per-cpu work items which run for longer than the following threshold are",
            " * automatically considered CPU intensive and excluded from concurrency",
            " * management to prevent them from noticeably delaying other per-cpu work items.",
            " * ULONG_MAX indicates that the user hasn't overridden it with a boot parameter.",
            " * The actual value is initialized in wq_cpu_intensive_thresh_init().",
            " */",
            "static unsigned long wq_cpu_intensive_thresh_us = ULONG_MAX;",
            "module_param_named(cpu_intensive_thresh_us, wq_cpu_intensive_thresh_us, ulong, 0644);",
            "",
            "/* see the comment above the definition of WQ_POWER_EFFICIENT */",
            "static bool wq_power_efficient = IS_ENABLED(CONFIG_WQ_POWER_EFFICIENT_DEFAULT);",
            "module_param_named(power_efficient, wq_power_efficient, bool, 0444);",
            "",
            "static bool wq_online;\t\t\t/* can kworkers be created yet? */",
            "",
            "/* buf for wq_update_unbound_pod_attrs(), protected by CPU hotplug exclusion */",
            "static struct workqueue_attrs *wq_update_pod_attrs_buf;",
            "",
            "static DEFINE_MUTEX(wq_pool_mutex);\t/* protects pools and workqueues list */",
            "static DEFINE_MUTEX(wq_pool_attach_mutex); /* protects worker attach/detach */",
            "static DEFINE_RAW_SPINLOCK(wq_mayday_lock);\t/* protects wq->maydays list */",
            "/* wait for manager to go away */",
            "static struct rcuwait manager_wait = __RCUWAIT_INITIALIZER(manager_wait);",
            "",
            "static LIST_HEAD(workqueues);\t\t/* PR: list of all workqueues */",
            "static bool workqueue_freezing;\t\t/* PL: have wqs started freezing? */",
            "",
            "/* PL&A: allowable cpus for unbound wqs and work items */",
            "static cpumask_var_t wq_unbound_cpumask;",
            "",
            "/* PL: user requested unbound cpumask via sysfs */",
            "static cpumask_var_t wq_requested_unbound_cpumask;",
            "",
            "/* PL: isolated cpumask to be excluded from unbound cpumask */",
            "static cpumask_var_t wq_isolated_cpumask;",
            "",
            "/* for further constrain wq_unbound_cpumask by cmdline parameter*/",
            "static struct cpumask wq_cmdline_cpumask __initdata;",
            "",
            "/* CPU where unbound work was last round robin scheduled from this CPU */",
            "static DEFINE_PER_CPU(int, wq_rr_cpu_last);",
            "",
            "/*",
            " * Local execution of unbound work items is no longer guaranteed.  The",
            " * following always forces round-robin CPU selection on unbound work items",
            " * to uncover usages which depend on it.",
            " */",
            "#ifdef CONFIG_DEBUG_WQ_FORCE_RR_CPU",
            "static bool wq_debug_force_rr_cpu = true;",
            "#else",
            "static bool wq_debug_force_rr_cpu = false;",
            "#endif",
            "module_param_named(debug_force_rr_cpu, wq_debug_force_rr_cpu, bool, 0644);",
            "",
            "/* the per-cpu worker pools */",
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS], cpu_worker_pools);",
            "",
            "static DEFINE_IDR(worker_pool_idr);\t/* PR: idr of all pools */",
            "",
            "/* PL: hash of all unbound pools keyed by pool->attrs */",
            "static DEFINE_HASHTABLE(unbound_pool_hash, UNBOUND_POOL_HASH_ORDER);",
            "",
            "/* I: attributes used when instantiating standard unbound pools on demand */",
            "static struct workqueue_attrs *unbound_std_wq_attrs[NR_STD_WORKER_POOLS];",
            "",
            "/* I: attributes used when instantiating ordered pools on demand */",
            "static struct workqueue_attrs *ordered_wq_attrs[NR_STD_WORKER_POOLS];",
            "",
            "/*",
            " * I: kthread_worker to release pwq's. pwq release needs to be bounced to a",
            " * process context while holding a pool lock. Bounce to a dedicated kthread",
            " * worker to avoid A-A deadlocks.",
            " */",
            "static struct kthread_worker *pwq_release_worker __ro_after_init;",
            "",
            "struct workqueue_struct *system_wq __ro_after_init;",
            "EXPORT_SYMBOL(system_wq);",
            "struct workqueue_struct *system_highpri_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_highpri_wq);",
            "struct workqueue_struct *system_long_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_long_wq);",
            "struct workqueue_struct *system_unbound_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_unbound_wq);",
            "struct workqueue_struct *system_freezable_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_freezable_wq);",
            "struct workqueue_struct *system_power_efficient_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_power_efficient_wq);",
            "struct workqueue_struct *system_freezable_power_efficient_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_freezable_power_efficient_wq);",
            "",
            "static int worker_thread(void *__worker);",
            "static void workqueue_sysfs_unregister(struct workqueue_struct *wq);",
            "static void show_pwq(struct pool_workqueue *pwq);",
            "static void show_one_worker_pool(struct worker_pool *pool);",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/workqueue.h>",
            "",
            "#define assert_rcu_or_pool_mutex()\t\t\t\t\t\\",
            "\tRCU_LOCKDEP_WARN(!rcu_read_lock_held() &&\t\t\t\\",
            "\t\t\t !lockdep_is_held(&wq_pool_mutex),\t\t\\",
            "\t\t\t \"RCU or wq_pool_mutex should be held\")",
            "",
            "#define assert_rcu_or_wq_mutex_or_pool_mutex(wq)\t\t\t\\",
            "\tRCU_LOCKDEP_WARN(!rcu_read_lock_held() &&\t\t\t\\",
            "\t\t\t !lockdep_is_held(&wq->mutex) &&\t\t\\",
            "\t\t\t !lockdep_is_held(&wq_pool_mutex),\t\t\\",
            "\t\t\t \"RCU, wq->mutex or wq_pool_mutex should be held\")",
            "",
            "#define for_each_cpu_worker_pool(pool, cpu)\t\t\t\t\\",
            "\tfor ((pool) = &per_cpu(cpu_worker_pools, cpu)[0];\t\t\\",
            "\t     (pool) < &per_cpu(cpu_worker_pools, cpu)[NR_STD_WORKER_POOLS]; \\",
            "\t     (pool)++)",
            "",
            "/**",
            " * for_each_pool - iterate through all worker_pools in the system",
            " * @pool: iteration cursor",
            " * @pi: integer used for iteration",
            " *",
            " * This must be called either with wq_pool_mutex held or RCU read",
            " * locked.  If the pool needs to be used beyond the locking in effect, the",
            " * caller is responsible for guaranteeing that the pool stays online.",
            " *",
            " * The if/else clause exists only for the lockdep assertion and can be",
            " * ignored.",
            " */",
            "#define for_each_pool(pool, pi)\t\t\t\t\t\t\\",
            "\tidr_for_each_entry(&worker_pool_idr, pool, pi)\t\t\t\\",
            "\t\tif (({ assert_rcu_or_pool_mutex(); false; })) { }\t\\",
            "\t\telse",
            "",
            "/**",
            " * for_each_pool_worker - iterate through all workers of a worker_pool",
            " * @worker: iteration cursor",
            " * @pool: worker_pool to iterate workers of",
            " *",
            " * This must be called with wq_pool_attach_mutex.",
            " *",
            " * The if/else clause exists only for the lockdep assertion and can be",
            " * ignored.",
            " */",
            "#define for_each_pool_worker(worker, pool)\t\t\t\t\\",
            "\tlist_for_each_entry((worker), &(pool)->workers, node)\t\t\\",
            "\t\tif (({ lockdep_assert_held(&wq_pool_attach_mutex); false; })) { } \\",
            "\t\telse",
            "",
            "/**",
            " * for_each_pwq - iterate through all pool_workqueues of the specified workqueue",
            " * @pwq: iteration cursor",
            " * @wq: the target workqueue",
            " *",
            " * This must be called either with wq->mutex held or RCU read locked.",
            " * If the pwq needs to be used beyond the locking in effect, the caller is",
            " * responsible for guaranteeing that the pwq stays online.",
            " *",
            " * The if/else clause exists only for the lockdep assertion and can be",
            " * ignored.",
            " */",
            "#define for_each_pwq(pwq, wq)\t\t\t\t\t\t\\",
            "\tlist_for_each_entry_rcu((pwq), &(wq)->pwqs, pwqs_node,\t\t\\",
            "\t\t\t\t lockdep_is_held(&(wq->mutex)))",
            "",
            "#ifdef CONFIG_DEBUG_OBJECTS_WORK",
            "",
            "static const struct debug_obj_descr work_debug_descr;",
            "",
            "static void *work_debug_hint(void *addr)",
            "{",
            "\treturn ((struct work_struct *) addr)->func;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义工作队列核心数据结构和全局变量，包括worker_pool和pool_workqueue结构体，用于管理工作者线程池及其属性，包含池标志位、工作项统计信息、锁保护字段和RCU相关成员，为异步任务调度提供基础设施。",
          "similarity": 0.5835892558097839
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/workqueue.c",
          "start_line": 2243,
          "end_line": 2353,
          "content": [
            "static void unbind_worker(struct worker *worker)",
            "{",
            "\tlockdep_assert_held(&wq_pool_attach_mutex);",
            "",
            "\tkthread_set_per_cpu(worker->task, -1);",
            "\tif (cpumask_intersects(wq_unbound_cpumask, cpu_active_mask))",
            "\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, wq_unbound_cpumask) < 0);",
            "\telse",
            "\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, cpu_possible_mask) < 0);",
            "}",
            "static void wake_dying_workers(struct list_head *cull_list)",
            "{",
            "\tstruct worker *worker, *tmp;",
            "",
            "\tlist_for_each_entry_safe(worker, tmp, cull_list, entry) {",
            "\t\tlist_del_init(&worker->entry);",
            "\t\tunbind_worker(worker);",
            "\t\t/*",
            "\t\t * If the worker was somehow already running, then it had to be",
            "\t\t * in pool->idle_list when set_worker_dying() happened or we",
            "\t\t * wouldn't have gotten here.",
            "\t\t *",
            "\t\t * Thus, the worker must either have observed the WORKER_DIE",
            "\t\t * flag, or have set its state to TASK_IDLE. Either way, the",
            "\t\t * below will be observed by the worker and is safe to do",
            "\t\t * outside of pool->lock.",
            "\t\t */",
            "\t\twake_up_process(worker->task);",
            "\t}",
            "}",
            "static void set_worker_dying(struct worker *worker, struct list_head *list)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "\tlockdep_assert_held(&wq_pool_attach_mutex);",
            "",
            "\t/* sanity check frenzy */",
            "\tif (WARN_ON(worker->current_work) ||",
            "\t    WARN_ON(!list_empty(&worker->scheduled)) ||",
            "\t    WARN_ON(!(worker->flags & WORKER_IDLE)))",
            "\t\treturn;",
            "",
            "\tpool->nr_workers--;",
            "\tpool->nr_idle--;",
            "",
            "\tworker->flags |= WORKER_DIE;",
            "",
            "\tlist_move(&worker->entry, list);",
            "\tlist_move(&worker->node, &pool->dying_workers);",
            "}",
            "static void idle_worker_timeout(struct timer_list *t)",
            "{",
            "\tstruct worker_pool *pool = from_timer(pool, t, idle_timer);",
            "\tbool do_cull = false;",
            "",
            "\tif (work_pending(&pool->idle_cull_work))",
            "\t\treturn;",
            "",
            "\traw_spin_lock_irq(&pool->lock);",
            "",
            "\tif (too_many_workers(pool)) {",
            "\t\tstruct worker *worker;",
            "\t\tunsigned long expires;",
            "",
            "\t\t/* idle_list is kept in LIFO order, check the last one */",
            "\t\tworker = list_entry(pool->idle_list.prev, struct worker, entry);",
            "\t\texpires = worker->last_active + IDLE_WORKER_TIMEOUT;",
            "\t\tdo_cull = !time_before(jiffies, expires);",
            "",
            "\t\tif (!do_cull)",
            "\t\t\tmod_timer(&pool->idle_timer, expires);",
            "\t}",
            "\traw_spin_unlock_irq(&pool->lock);",
            "",
            "\tif (do_cull)",
            "\t\tqueue_work(system_unbound_wq, &pool->idle_cull_work);",
            "}",
            "static void idle_cull_fn(struct work_struct *work)",
            "{",
            "\tstruct worker_pool *pool = container_of(work, struct worker_pool, idle_cull_work);",
            "\tLIST_HEAD(cull_list);",
            "",
            "\t/*",
            "\t * Grabbing wq_pool_attach_mutex here ensures an already-running worker",
            "\t * cannot proceed beyong worker_detach_from_pool() in its self-destruct",
            "\t * path. This is required as a previously-preempted worker could run after",
            "\t * set_worker_dying() has happened but before wake_dying_workers() did.",
            "\t */",
            "\tmutex_lock(&wq_pool_attach_mutex);",
            "\traw_spin_lock_irq(&pool->lock);",
            "",
            "\twhile (too_many_workers(pool)) {",
            "\t\tstruct worker *worker;",
            "\t\tunsigned long expires;",
            "",
            "\t\tworker = list_entry(pool->idle_list.prev, struct worker, entry);",
            "\t\texpires = worker->last_active + IDLE_WORKER_TIMEOUT;",
            "",
            "\t\tif (time_before(jiffies, expires)) {",
            "\t\t\tmod_timer(&pool->idle_timer, expires);",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tset_worker_dying(worker, &cull_list);",
            "\t}",
            "",
            "\traw_spin_unlock_irq(&pool->lock);",
            "\twake_dying_workers(&cull_list);",
            "\tmutex_unlock(&wq_pool_attach_mutex);",
            "}"
          ],
          "function_name": "unbind_worker, wake_dying_workers, set_worker_dying, idle_worker_timeout, idle_cull_fn",
          "description": "处理工作队列中工作者死亡后的清理，包括解除CPU绑定、唤醒任务并标记为死亡状态，用于管理空闲工作者超时后的回收",
          "similarity": 0.5476011037826538
        },
        {
          "chunk_id": 9,
          "file_path": "kernel/workqueue.c",
          "start_line": 1982,
          "end_line": 2082,
          "content": [
            "bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,",
            "\t\t\t   struct delayed_work *dwork, unsigned long delay)",
            "{",
            "\tstruct work_struct *work = &dwork->work;",
            "\tbool ret = false;",
            "\tunsigned long flags;",
            "",
            "\t/* read the comment in __queue_work() */",
            "\tlocal_irq_save(flags);",
            "",
            "\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {",
            "\t\t__queue_delayed_work(cpu, wq, dwork, delay);",
            "\t\tret = true;",
            "\t}",
            "",
            "\tlocal_irq_restore(flags);",
            "\treturn ret;",
            "}",
            "bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,",
            "\t\t\t struct delayed_work *dwork, unsigned long delay)",
            "{",
            "\tunsigned long flags;",
            "\tint ret;",
            "",
            "\tdo {",
            "\t\tret = try_to_grab_pending(&dwork->work, true, &flags);",
            "\t} while (unlikely(ret == -EAGAIN));",
            "",
            "\tif (likely(ret >= 0)) {",
            "\t\t__queue_delayed_work(cpu, wq, dwork, delay);",
            "\t\tlocal_irq_restore(flags);",
            "\t}",
            "",
            "\t/* -ENOENT from try_to_grab_pending() becomes %true */",
            "\treturn ret;",
            "}",
            "static void rcu_work_rcufn(struct rcu_head *rcu)",
            "{",
            "\tstruct rcu_work *rwork = container_of(rcu, struct rcu_work, rcu);",
            "",
            "\t/* read the comment in __queue_work() */",
            "\tlocal_irq_disable();",
            "\t__queue_work(WORK_CPU_UNBOUND, rwork->wq, &rwork->work);",
            "\tlocal_irq_enable();",
            "}",
            "bool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork)",
            "{",
            "\tstruct work_struct *work = &rwork->work;",
            "",
            "\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {",
            "\t\trwork->wq = wq;",
            "\t\tcall_rcu_hurry(&rwork->rcu, rcu_work_rcufn);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}",
            "static void worker_attach_to_pool(struct worker *worker,",
            "\t\t\t\t   struct worker_pool *pool)",
            "{",
            "\tmutex_lock(&wq_pool_attach_mutex);",
            "",
            "\t/*",
            "\t * The wq_pool_attach_mutex ensures %POOL_DISASSOCIATED remains",
            "\t * stable across this function.  See the comments above the flag",
            "\t * definition for details.",
            "\t */",
            "\tif (pool->flags & POOL_DISASSOCIATED)",
            "\t\tworker->flags |= WORKER_UNBOUND;",
            "\telse",
            "\t\tkthread_set_per_cpu(worker->task, pool->cpu);",
            "",
            "\tif (worker->rescue_wq)",
            "\t\tset_cpus_allowed_ptr(worker->task, pool_allowed_cpus(pool));",
            "",
            "\tlist_add_tail(&worker->node, &pool->workers);",
            "\tworker->pool = pool;",
            "",
            "\tmutex_unlock(&wq_pool_attach_mutex);",
            "}",
            "static void worker_detach_from_pool(struct worker *worker)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "\tstruct completion *detach_completion = NULL;",
            "",
            "\tmutex_lock(&wq_pool_attach_mutex);",
            "",
            "\tkthread_set_per_cpu(worker->task, -1);",
            "\tlist_del(&worker->node);",
            "\tworker->pool = NULL;",
            "",
            "\tif (list_empty(&pool->workers) && list_empty(&pool->dying_workers))",
            "\t\tdetach_completion = pool->detach_completion;",
            "\tmutex_unlock(&wq_pool_attach_mutex);",
            "",
            "\t/* clear leftover flags without pool->lock after it is detached */",
            "\tworker->flags &= ~(WORKER_UNBOUND | WORKER_REBOUND);",
            "",
            "\tif (detach_completion)",
            "\t\tcomplete(detach_completion);",
            "}"
          ],
          "function_name": "queue_delayed_work_on, mod_delayed_work_on, rcu_work_rcufn, queue_rcu_work, worker_attach_to_pool, worker_detach_from_pool",
          "description": "该代码块管理RCU安全的工作队列操作。queue_delayed_work_on/mod_delayed_work_on控制延迟工作提交；rcu_work_rcufn处理RCU回调；worker_attach_to_pool/detach_from_pool管理worker与worker池的绑定关系。",
          "similarity": 0.5439212322235107
        },
        {
          "chunk_id": 17,
          "file_path": "kernel/workqueue.c",
          "start_line": 3639,
          "end_line": 3741,
          "content": [
            "bool cancel_work(struct work_struct *work)",
            "{",
            "\treturn __cancel_work(work, false);",
            "}",
            "bool cancel_delayed_work(struct delayed_work *dwork)",
            "{",
            "\treturn __cancel_work(&dwork->work, true);",
            "}",
            "bool cancel_delayed_work_sync(struct delayed_work *dwork)",
            "{",
            "\treturn __cancel_work_timer(&dwork->work, true);",
            "}",
            "int schedule_on_each_cpu(work_func_t func)",
            "{",
            "\tint cpu;",
            "\tstruct work_struct __percpu *works;",
            "",
            "\tworks = alloc_percpu(struct work_struct);",
            "\tif (!works)",
            "\t\treturn -ENOMEM;",
            "",
            "\tcpus_read_lock();",
            "",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct work_struct *work = per_cpu_ptr(works, cpu);",
            "",
            "\t\tINIT_WORK(work, func);",
            "\t\tschedule_work_on(cpu, work);",
            "\t}",
            "",
            "\tfor_each_online_cpu(cpu)",
            "\t\tflush_work(per_cpu_ptr(works, cpu));",
            "",
            "\tcpus_read_unlock();",
            "\tfree_percpu(works);",
            "\treturn 0;",
            "}",
            "int execute_in_process_context(work_func_t fn, struct execute_work *ew)",
            "{",
            "\tif (!in_interrupt()) {",
            "\t\tfn(&ew->work);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tINIT_WORK(&ew->work, fn);",
            "\tschedule_work(&ew->work);",
            "",
            "\treturn 1;",
            "}",
            "void free_workqueue_attrs(struct workqueue_attrs *attrs)",
            "{",
            "\tif (attrs) {",
            "\t\tfree_cpumask_var(attrs->cpumask);",
            "\t\tfree_cpumask_var(attrs->__pod_cpumask);",
            "\t\tkfree(attrs);",
            "\t}",
            "}",
            "static void copy_workqueue_attrs(struct workqueue_attrs *to,",
            "\t\t\t\t const struct workqueue_attrs *from)",
            "{",
            "\tto->nice = from->nice;",
            "\tcpumask_copy(to->cpumask, from->cpumask);",
            "\tcpumask_copy(to->__pod_cpumask, from->__pod_cpumask);",
            "\tto->affn_strict = from->affn_strict;",
            "",
            "\t/*",
            "\t * Unlike hash and equality test, copying shouldn't ignore wq-only",
            "\t * fields as copying is used for both pool and wq attrs. Instead,",
            "\t * get_unbound_pool() explicitly clears the fields.",
            "\t */",
            "\tto->affn_scope = from->affn_scope;",
            "\tto->ordered = from->ordered;",
            "}",
            "static void wqattrs_clear_for_pool(struct workqueue_attrs *attrs)",
            "{",
            "\tattrs->affn_scope = WQ_AFFN_NR_TYPES;",
            "\tattrs->ordered = false;",
            "}",
            "static u32 wqattrs_hash(const struct workqueue_attrs *attrs)",
            "{",
            "\tu32 hash = 0;",
            "",
            "\thash = jhash_1word(attrs->nice, hash);",
            "\thash = jhash(cpumask_bits(attrs->cpumask),",
            "\t\t     BITS_TO_LONGS(nr_cpumask_bits) * sizeof(long), hash);",
            "\thash = jhash(cpumask_bits(attrs->__pod_cpumask),",
            "\t\t     BITS_TO_LONGS(nr_cpumask_bits) * sizeof(long), hash);",
            "\thash = jhash_1word(attrs->affn_strict, hash);",
            "\treturn hash;",
            "}",
            "static bool wqattrs_equal(const struct workqueue_attrs *a,",
            "\t\t\t  const struct workqueue_attrs *b)",
            "{",
            "\tif (a->nice != b->nice)",
            "\t\treturn false;",
            "\tif (!cpumask_equal(a->cpumask, b->cpumask))",
            "\t\treturn false;",
            "\tif (!cpumask_equal(a->__pod_cpumask, b->__pod_cpumask))",
            "\t\treturn false;",
            "\tif (a->affn_strict != b->affn_strict)",
            "\t\treturn false;",
            "\treturn true;",
            "}"
          ],
          "function_name": "cancel_work, cancel_delayed_work, cancel_delayed_work_sync, schedule_on_each_cpu, execute_in_process_context, free_workqueue_attrs, copy_workqueue_attrs, wqattrs_clear_for_pool, wqattrs_hash, wqattrs_equal",
          "description": "cancel_work 和 cancel_delayed_work 取消工作项，schedule_on_each_cpu 在每个 CPU 上调度工作函数，execute_in_process_context 允许在进程上下文执行工作。copy_workqueue_attrs 和 free_workqueue_attrs 管理工作队列属性的复制与释放。",
          "similarity": 0.5438083410263062
        }
      ]
    },
    {
      "source_file": "mm/workingset.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:34:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workingset.c`\n\n---\n\n# workingset.c 技术文档\n\n## 1. 文件概述\n\n`workingset.c` 实现了 Linux 内核中的 **工作集检测（Workingset Detection）** 机制，用于优化页面回收（page reclaim）策略。该机制通过跟踪页面的访问模式和重故障距离（refault distance），智能判断哪些页面应保留在内存中，从而减少系统颠簸（thrashing）并提升缓存效率。核心思想是：若一个被换出的页面在短时间内再次被访问（即重故障），且其重故障距离小于当前活跃页面数量，则应将其重新激活，以取代可能已不再活跃的现有活跃页面。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **Shadow Entry（影子条目）**：存储在页缓存槽位中的元数据，包含页面被驱逐时的时间戳（eviction counter 快照）、内存控制组 ID、节点 ID 和工作集标志。\n- **node->nonresident_age**：每个 NUMA 节点维护的计数器，记录非驻留页面的“年龄”，用于计算重故障距离。\n\n### 关键宏定义\n- `WORKINGSET_SHIFT`：工作集标识位偏移。\n- `EVICTION_SHIFT` / `EVICTION_MASK`：用于在 xarray 条目中紧凑编码驱逐时间戳的位操作参数。\n- `bucket_order`：当时间戳位数不足时，用于对驱逐事件进行分桶聚合的粒度。\n\n### 核心函数（部分实现）\n- `pack_shadow()`：将内存控制组 ID、节点指针、驱逐计数器值和工作集标志打包成一个 shadow entry。\n- （注：代码片段未完整展示其他关键函数如 `workingset_refault()`、`workingset_activation()` 等，但文档基于完整机制描述）\n\n## 3. 关键实现\n\n### 双 CLOCK 列表模型\n- 每个 NUMA 节点为文件页维护两个 LRU 列表：**inactive list**（不活跃）和 **active list**（活跃）。\n- 新缺页页面加入 inactive list 头部；回收从 inactive list 尾部扫描。\n- 在 inactive list 上被二次访问的页面晋升至 active list；active list 过长时，尾部页面降级到 inactive list。\n\n### 重故障距离（Refault Distance）算法\n1. **驱逐时记录**：页面被驱逐时，将其所在节点的 `nonresident_age` 计数器值（代表累计的驱逐+激活次数）作为时间戳存入 shadow entry。\n2. **重故障时计算**：\n   - 当缺页发生且存在对应 shadow entry 时，读取当前 `nonresident_age` 值（R）与 shadow 中存储的值（E）。\n   - 重故障距离 = `R - E`，表示页面不在内存期间发生的最小页面访问次数。\n3. **激活决策**：\n   - 若 `重故障距离 <= 当前活跃页面总数（file + anon）`，则认为若当时有足够 inactive 空间，该页面本可被激活而避免驱逐。\n   - 因此**乐观地激活**该重故障页面，使其与现有活跃页面竞争内存空间。\n\n### 影子条目压缩存储\n- 利用 xarray 条目的有限位宽（`BITS_PER_XA_VALUE`），通过位域拼接存储：\n  - 节点 ID（`NODES_SHIFT` 位）\n  - 内存控制组 ID（`MEM_CGROUP_ID_SHIFT` 位）\n  - 工作集标志（`WORKINGSET_SHIFT` 位）\n  - 驱逐时间戳（剩余位，必要时通过 `bucket_order` 降低精度）\n\n## 4. 依赖关系\n\n- **内存管理核心**：`<linux/mm.h>`, `<linux/mm_inline.h>` — 提供页框、LRU 列表、页表操作等基础支持。\n- **内存控制组**：`<linux/memcontrol.h>` — 支持按 cgroup 隔离工作集统计。\n- **页缓存与交换**：`<linux/pagemap.h>`, `<linux/swap.h>`, `<linux/shmem_fs.h>` — 处理文件页、匿名页、tmpfs 页的回收逻辑。\n- **xarray 数据结构**：用于高效存储和检索 shadow entries（隐含在 `pack_shadow` 的位操作中）。\n- **DAX 支持**：`<linux/dax.h>` — 确保直接访问持久内存设备的页面也能参与工作集检测。\n\n## 5. 使用场景\n\n- **内存压力下的页面回收**：当系统内存紧张触发 kswapd 或直接回收时，工作集检测机制指导选择最优的牺牲页面。\n- **工作集切换检测**：识别应用程序工作集的动态变化（如新任务启动、旧任务结束），快速淘汰过时缓存。\n- **防止颠簸（Thrashing）**：在活跃工作集大小接近或超过可用内存时，通过重故障距离预测避免频繁换入换出。\n- **混合工作负载优化**：同时处理文件缓存（page cache）和匿名内存（anonymous pages）的工作集，平衡二者内存分配。\n- **容器化环境**：结合 memcg，在多租户系统中为每个容器独立维护工作集状态，避免相互干扰。",
      "similarity": 0.5718329548835754,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/workingset.c",
          "start_line": 418,
          "end_line": 526,
          "content": [
            "bool workingset_test_recent(void *shadow, bool file, bool *workingset,",
            "\t\t\t\tbool flush)",
            "{",
            "\tstruct mem_cgroup *eviction_memcg;",
            "\tstruct lruvec *eviction_lruvec;",
            "\tunsigned long refault_distance;",
            "\tunsigned long workingset_size;",
            "\tunsigned long refault;",
            "\tint memcgid;",
            "\tstruct pglist_data *pgdat;",
            "\tunsigned long eviction;",
            "",
            "\trcu_read_lock();",
            "",
            "\tif (lru_gen_enabled()) {",
            "\t\tbool recent = lru_gen_test_recent(shadow, file,",
            "\t\t\t\t&eviction_lruvec, &eviction, workingset);",
            "",
            "\t\trcu_read_unlock();",
            "\t\treturn recent;",
            "\t}",
            "",
            "",
            "\tunpack_shadow(shadow, &memcgid, &pgdat, &eviction, workingset);",
            "\teviction <<= bucket_order;",
            "",
            "\t/*",
            "\t * Look up the memcg associated with the stored ID. It might",
            "\t * have been deleted since the folio's eviction.",
            "\t *",
            "\t * Note that in rare events the ID could have been recycled",
            "\t * for a new cgroup that refaults a shared folio. This is",
            "\t * impossible to tell from the available data. However, this",
            "\t * should be a rare and limited disturbance, and activations",
            "\t * are always speculative anyway. Ultimately, it's the aging",
            "\t * algorithm's job to shake out the minimum access frequency",
            "\t * for the active cache.",
            "\t *",
            "\t * XXX: On !CONFIG_MEMCG, this will always return NULL; it",
            "\t * would be better if the root_mem_cgroup existed in all",
            "\t * configurations instead.",
            "\t */",
            "\teviction_memcg = mem_cgroup_from_id(memcgid);",
            "\tif (!mem_cgroup_disabled() &&",
            "\t    (!eviction_memcg || !mem_cgroup_tryget(eviction_memcg))) {",
            "\t\trcu_read_unlock();",
            "\t\treturn false;",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * Flush stats (and potentially sleep) outside the RCU read section.",
            "\t *",
            "\t * Note that workingset_test_recent() itself might be called in RCU read",
            "\t * section (for e.g, in cachestat) - these callers need to skip flushing",
            "\t * stats (via the flush argument).",
            "\t *",
            "\t * XXX: With per-memcg flushing and thresholding, is ratelimiting",
            "\t * still needed here?",
            "\t */",
            "\tif (flush)",
            "\t\tmem_cgroup_flush_stats_ratelimited(eviction_memcg);",
            "",
            "\teviction_lruvec = mem_cgroup_lruvec(eviction_memcg, pgdat);",
            "\trefault = atomic_long_read(&eviction_lruvec->nonresident_age);",
            "",
            "\t/*",
            "\t * Calculate the refault distance",
            "\t *",
            "\t * The unsigned subtraction here gives an accurate distance",
            "\t * across nonresident_age overflows in most cases. There is a",
            "\t * special case: usually, shadow entries have a short lifetime",
            "\t * and are either refaulted or reclaimed along with the inode",
            "\t * before they get too old.  But it is not impossible for the",
            "\t * nonresident_age to lap a shadow entry in the field, which",
            "\t * can then result in a false small refault distance, leading",
            "\t * to a false activation should this old entry actually",
            "\t * refault again.  However, earlier kernels used to deactivate",
            "\t * unconditionally with *every* reclaim invocation for the",
            "\t * longest time, so the occasional inappropriate activation",
            "\t * leading to pressure on the active list is not a problem.",
            "\t */",
            "\trefault_distance = (refault - eviction) & EVICTION_MASK;",
            "",
            "\t/*",
            "\t * Compare the distance to the existing workingset size. We",
            "\t * don't activate pages that couldn't stay resident even if",
            "\t * all the memory was available to the workingset. Whether",
            "\t * workingset competition needs to consider anon or not depends",
            "\t * on having free swap space.",
            "\t */",
            "\tworkingset_size = lruvec_page_state(eviction_lruvec, NR_ACTIVE_FILE);",
            "\tif (!file) {",
            "\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_INACTIVE_FILE);",
            "\t}",
            "\tif (mem_cgroup_get_nr_swap_pages(eviction_memcg) > 0) {",
            "\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_ACTIVE_ANON);",
            "\t\tif (file) {",
            "\t\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_INACTIVE_ANON);",
            "\t\t}",
            "\t}",
            "",
            "\tmem_cgroup_put(eviction_memcg);",
            "\treturn refault_distance <= workingset_size;",
            "}"
          ],
          "function_name": "workingset_test_recent",
          "description": "实现工作集测试最近访问的判断逻辑，通过对比参考距离与当前工作集大小决定是否激活页面，支持内存组场景下的统计信息处理。",
          "similarity": 0.5940455198287964
        },
        {
          "chunk_id": 4,
          "file_path": "mm/workingset.c",
          "start_line": 712,
          "end_line": 833,
          "content": [
            "static enum lru_status shadow_lru_isolate(struct list_head *item,",
            "\t\t\t\t\t  struct list_lru_one *lru,",
            "\t\t\t\t\t  spinlock_t *lru_lock,",
            "\t\t\t\t\t  void *arg) __must_hold(lru_lock)",
            "{",
            "\tstruct xa_node *node = container_of(item, struct xa_node, private_list);",
            "\tstruct address_space *mapping;",
            "\tint ret;",
            "",
            "\t/*",
            "\t * Page cache insertions and deletions synchronously maintain",
            "\t * the shadow node LRU under the i_pages lock and the",
            "\t * lru_lock.  Because the page cache tree is emptied before",
            "\t * the inode can be destroyed, holding the lru_lock pins any",
            "\t * address_space that has nodes on the LRU.",
            "\t *",
            "\t * We can then safely transition to the i_pages lock to",
            "\t * pin only the address_space of the particular node we want",
            "\t * to reclaim, take the node off-LRU, and drop the lru_lock.",
            "\t */",
            "",
            "\tmapping = container_of(node->array, struct address_space, i_pages);",
            "",
            "\t/* Coming from the list, invert the lock order */",
            "\tif (!xa_trylock(&mapping->i_pages)) {",
            "\t\tspin_unlock_irq(lru_lock);",
            "\t\tret = LRU_RETRY;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* For page cache we need to hold i_lock */",
            "\tif (mapping->host != NULL) {",
            "\t\tif (!spin_trylock(&mapping->host->i_lock)) {",
            "\t\t\txa_unlock(&mapping->i_pages);",
            "\t\t\tspin_unlock_irq(lru_lock);",
            "\t\t\tret = LRU_RETRY;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t}",
            "",
            "\tlist_lru_isolate(lru, item);",
            "\t__dec_node_page_state(virt_to_page(node), WORKINGSET_NODES);",
            "",
            "\tspin_unlock(lru_lock);",
            "",
            "\t/*",
            "\t * The nodes should only contain one or more shadow entries,",
            "\t * no pages, so we expect to be able to remove them all and",
            "\t * delete and free the empty node afterwards.",
            "\t */",
            "\tif (WARN_ON_ONCE(!node->nr_values))",
            "\t\tgoto out_invalid;",
            "\tif (WARN_ON_ONCE(node->count != node->nr_values))",
            "\t\tgoto out_invalid;",
            "\txa_delete_node(node, workingset_update_node);",
            "\t__inc_lruvec_kmem_state(node, WORKINGSET_NODERECLAIM);",
            "",
            "out_invalid:",
            "\txa_unlock_irq(&mapping->i_pages);",
            "\tif (mapping->host != NULL) {",
            "\t\tif (mapping_shrinkable(mapping))",
            "\t\t\tinode_add_lru(mapping->host);",
            "\t\tspin_unlock(&mapping->host->i_lock);",
            "\t}",
            "\tret = LRU_REMOVED_RETRY;",
            "out:",
            "\tcond_resched();",
            "\tspin_lock_irq(lru_lock);",
            "\treturn ret;",
            "}",
            "static unsigned long scan_shadow_nodes(struct shrinker *shrinker,",
            "\t\t\t\t       struct shrink_control *sc)",
            "{",
            "\t/* list_lru lock nests inside the IRQ-safe i_pages lock */",
            "\treturn list_lru_shrink_walk_irq(&shadow_nodes, sc, shadow_lru_isolate,",
            "\t\t\t\t\tNULL);",
            "}",
            "static int __init workingset_init(void)",
            "{",
            "\tstruct shrinker *workingset_shadow_shrinker;",
            "\tunsigned int timestamp_bits;",
            "\tunsigned int max_order;",
            "\tint ret = -ENOMEM;",
            "",
            "\tBUILD_BUG_ON(BITS_PER_LONG < EVICTION_SHIFT);",
            "\t/*",
            "\t * Calculate the eviction bucket size to cover the longest",
            "\t * actionable refault distance, which is currently half of",
            "\t * memory (totalram_pages/2). However, memory hotplug may add",
            "\t * some more pages at runtime, so keep working with up to",
            "\t * double the initial memory by using totalram_pages as-is.",
            "\t */",
            "\ttimestamp_bits = BITS_PER_LONG - EVICTION_SHIFT;",
            "\tmax_order = fls_long(totalram_pages() - 1);",
            "\tif (max_order > timestamp_bits)",
            "\t\tbucket_order = max_order - timestamp_bits;",
            "\tpr_info(\"workingset: timestamp_bits=%d max_order=%d bucket_order=%u\\n\",",
            "\t       timestamp_bits, max_order, bucket_order);",
            "",
            "\tworkingset_shadow_shrinker = shrinker_alloc(SHRINKER_NUMA_AWARE |",
            "\t\t\t\t\t\t    SHRINKER_MEMCG_AWARE,",
            "\t\t\t\t\t\t    \"mm-shadow\");",
            "\tif (!workingset_shadow_shrinker)",
            "\t\tgoto err;",
            "",
            "\tret = __list_lru_init(&shadow_nodes, true, &shadow_nodes_key,",
            "\t\t\t      workingset_shadow_shrinker);",
            "\tif (ret)",
            "\t\tgoto err_list_lru;",
            "",
            "\tworkingset_shadow_shrinker->count_objects = count_shadow_nodes;",
            "\tworkingset_shadow_shrinker->scan_objects = scan_shadow_nodes;",
            "\t/* ->count reports only fully expendable nodes */",
            "\tworkingset_shadow_shrinker->seeks = 0;",
            "",
            "\tshrinker_register(workingset_shadow_shrinker);",
            "\treturn 0;",
            "err_list_lru:",
            "\tshrinker_free(workingset_shadow_shrinker);",
            "err:",
            "\treturn ret;",
            "}"
          ],
          "function_name": "shadow_lru_isolate, scan_shadow_nodes, workingset_init",
          "description": "初始化工作集模块，注册收缩器管理影子节点，定义节点隔离和扫描机制，通过计算时间戳位宽确定桶阶参数以保证参考距离覆盖范围。",
          "similarity": 0.5687201619148254
        },
        {
          "chunk_id": 3,
          "file_path": "mm/workingset.c",
          "start_line": 537,
          "end_line": 689,
          "content": [
            "void workingset_refault(struct folio *folio, void *shadow)",
            "{",
            "\tbool file = folio_is_file_lru(folio);",
            "\tstruct pglist_data *pgdat;",
            "\tstruct mem_cgroup *memcg;",
            "\tstruct lruvec *lruvec;",
            "\tbool workingset;",
            "\tlong nr;",
            "",
            "\tif (lru_gen_enabled()) {",
            "\t\tlru_gen_refault(folio, shadow);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * The activation decision for this folio is made at the level",
            "\t * where the eviction occurred, as that is where the LRU order",
            "\t * during folio reclaim is being determined.",
            "\t *",
            "\t * However, the cgroup that will own the folio is the one that",
            "\t * is actually experiencing the refault event. Make sure the folio is",
            "\t * locked to guarantee folio_memcg() stability throughout.",
            "\t */",
            "\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\tnr = folio_nr_pages(folio);",
            "\tmemcg = folio_memcg(folio);",
            "\tpgdat = folio_pgdat(folio);",
            "\tlruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "",
            "\tmod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + file, nr);",
            "",
            "\tif (!workingset_test_recent(shadow, file, &workingset, true))",
            "\t\treturn;",
            "",
            "\tfolio_set_active(folio);",
            "\tworkingset_age_nonresident(lruvec, nr);",
            "\tmod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + file, nr);",
            "",
            "\t/* Folio was active prior to eviction */",
            "\tif (workingset) {",
            "\t\tfolio_set_workingset(folio);",
            "\t\t/*",
            "\t\t * XXX: Move to folio_add_lru() when it supports new vs",
            "\t\t * putback",
            "\t\t */",
            "\t\tlru_note_cost_refault(folio);",
            "\t\tmod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + file, nr);",
            "\t}",
            "}",
            "void workingset_activation(struct folio *folio)",
            "{",
            "\tstruct mem_cgroup *memcg;",
            "",
            "\trcu_read_lock();",
            "\t/*",
            "\t * Filter non-memcg pages here, e.g. unmap can call",
            "\t * mark_page_accessed() on VDSO pages.",
            "\t *",
            "\t * XXX: See workingset_refault() - this should return",
            "\t * root_mem_cgroup even for !CONFIG_MEMCG.",
            "\t */",
            "\tmemcg = folio_memcg_rcu(folio);",
            "\tif (!mem_cgroup_disabled() && !memcg)",
            "\t\tgoto out;",
            "\tworkingset_age_nonresident(folio_lruvec(folio), folio_nr_pages(folio));",
            "out:",
            "\trcu_read_unlock();",
            "}",
            "void workingset_update_node(struct xa_node *node)",
            "{",
            "\tstruct address_space *mapping;",
            "\tstruct page *page = virt_to_page(node);",
            "",
            "\t/*",
            "\t * Track non-empty nodes that contain only shadow entries;",
            "\t * unlink those that contain pages or are being freed.",
            "\t *",
            "\t * Avoid acquiring the list_lru lock when the nodes are",
            "\t * already where they should be. The list_empty() test is safe",
            "\t * as node->private_list is protected by the i_pages lock.",
            "\t */",
            "\tmapping = container_of(node->array, struct address_space, i_pages);",
            "\tlockdep_assert_held(&mapping->i_pages.xa_lock);",
            "",
            "\tif (node->count && node->count == node->nr_values) {",
            "\t\tif (list_empty(&node->private_list)) {",
            "\t\t\tlist_lru_add_obj(&shadow_nodes, &node->private_list);",
            "\t\t\t__inc_node_page_state(page, WORKINGSET_NODES);",
            "\t\t}",
            "\t} else {",
            "\t\tif (!list_empty(&node->private_list)) {",
            "\t\t\tlist_lru_del_obj(&shadow_nodes, &node->private_list);",
            "\t\t\t__dec_node_page_state(page, WORKINGSET_NODES);",
            "\t\t}",
            "\t}",
            "}",
            "static unsigned long count_shadow_nodes(struct shrinker *shrinker,",
            "\t\t\t\t\tstruct shrink_control *sc)",
            "{",
            "\tunsigned long max_nodes;",
            "\tunsigned long nodes;",
            "\tunsigned long pages;",
            "",
            "\tnodes = list_lru_shrink_count(&shadow_nodes, sc);",
            "\tif (!nodes)",
            "\t\treturn SHRINK_EMPTY;",
            "",
            "\t/*",
            "\t * Approximate a reasonable limit for the nodes",
            "\t * containing shadow entries. We don't need to keep more",
            "\t * shadow entries than possible pages on the active list,",
            "\t * since refault distances bigger than that are dismissed.",
            "\t *",
            "\t * The size of the active list converges toward 100% of",
            "\t * overall page cache as memory grows, with only a tiny",
            "\t * inactive list. Assume the total cache size for that.",
            "\t *",
            "\t * Nodes might be sparsely populated, with only one shadow",
            "\t * entry in the extreme case. Obviously, we cannot keep one",
            "\t * node for every eligible shadow entry, so compromise on a",
            "\t * worst-case density of 1/8th. Below that, not all eligible",
            "\t * refaults can be detected anymore.",
            "\t *",
            "\t * On 64-bit with 7 xa_nodes per page and 64 slots",
            "\t * each, this will reclaim shadow entries when they consume",
            "\t * ~1.8% of available memory:",
            "\t *",
            "\t * PAGE_SIZE / xa_nodes / node_entries * 8 / PAGE_SIZE",
            "\t */",
            "#ifdef CONFIG_MEMCG",
            "\tif (sc->memcg) {",
            "\t\tstruct lruvec *lruvec;",
            "\t\tint i;",
            "",
            "\t\tmem_cgroup_flush_stats_ratelimited(sc->memcg);",
            "\t\tlruvec = mem_cgroup_lruvec(sc->memcg, NODE_DATA(sc->nid));",
            "\t\tfor (pages = 0, i = 0; i < NR_LRU_LISTS; i++)",
            "\t\t\tpages += lruvec_page_state_local(lruvec,",
            "\t\t\t\t\t\t\t NR_LRU_BASE + i);",
            "\t\tpages += lruvec_page_state_local(",
            "\t\t\tlruvec, NR_SLAB_RECLAIMABLE_B) >> PAGE_SHIFT;",
            "\t\tpages += lruvec_page_state_local(",
            "\t\t\tlruvec, NR_SLAB_UNRECLAIMABLE_B) >> PAGE_SHIFT;",
            "\t} else",
            "#endif",
            "\t\tpages = node_present_pages(sc->nid);",
            "",
            "\tmax_nodes = pages >> (XA_CHUNK_SHIFT - 3);",
            "",
            "\tif (nodes <= max_nodes)",
            "\t\treturn 0;",
            "\treturn nodes - max_nodes;",
            "}"
          ],
          "function_name": "workingset_refault, workingset_activation, workingset_update_node, count_shadow_nodes",
          "description": "提供页面故障后的激活处理、节点更新及影子节点追踪功能，包含基于工作集状态更新节点计数器和触发页面重新激活的逻辑。",
          "similarity": 0.5639832615852356
        },
        {
          "chunk_id": 0,
          "file_path": "mm/workingset.c",
          "start_line": 1,
          "end_line": 208,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Workingset detection",
            " *",
            " * Copyright (C) 2013 Red Hat, Inc., Johannes Weiner",
            " */",
            "",
            "#include <linux/memcontrol.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/writeback.h>",
            "#include <linux/shmem_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/atomic.h>",
            "#include <linux/module.h>",
            "#include <linux/swap.h>",
            "#include <linux/dax.h>",
            "#include <linux/fs.h>",
            "#include <linux/mm.h>",
            "",
            "/*",
            " *\t\tDouble CLOCK lists",
            " *",
            " * Per node, two clock lists are maintained for file pages: the",
            " * inactive and the active list.  Freshly faulted pages start out at",
            " * the head of the inactive list and page reclaim scans pages from the",
            " * tail.  Pages that are accessed multiple times on the inactive list",
            " * are promoted to the active list, to protect them from reclaim,",
            " * whereas active pages are demoted to the inactive list when the",
            " * active list grows too big.",
            " *",
            " *   fault ------------------------+",
            " *                                 |",
            " *              +--------------+   |            +-------------+",
            " *   reclaim <- |   inactive   | <-+-- demotion |    active   | <--+",
            " *              +--------------+                +-------------+    |",
            " *                     |                                           |",
            " *                     +-------------- promotion ------------------+",
            " *",
            " *",
            " *\t\tAccess frequency and refault distance",
            " *",
            " * A workload is thrashing when its pages are frequently used but they",
            " * are evicted from the inactive list every time before another access",
            " * would have promoted them to the active list.",
            " *",
            " * In cases where the average access distance between thrashing pages",
            " * is bigger than the size of memory there is nothing that can be",
            " * done - the thrashing set could never fit into memory under any",
            " * circumstance.",
            " *",
            " * However, the average access distance could be bigger than the",
            " * inactive list, yet smaller than the size of memory.  In this case,",
            " * the set could fit into memory if it weren't for the currently",
            " * active pages - which may be used more, hopefully less frequently:",
            " *",
            " *      +-memory available to cache-+",
            " *      |                           |",
            " *      +-inactive------+-active----+",
            " *  a b | c d e f g h i | J K L M N |",
            " *      +---------------+-----------+",
            " *",
            " * It is prohibitively expensive to accurately track access frequency",
            " * of pages.  But a reasonable approximation can be made to measure",
            " * thrashing on the inactive list, after which refaulting pages can be",
            " * activated optimistically to compete with the existing active pages.",
            " *",
            " * Approximating inactive page access frequency - Observations:",
            " *",
            " * 1. When a page is accessed for the first time, it is added to the",
            " *    head of the inactive list, slides every existing inactive page",
            " *    towards the tail by one slot, and pushes the current tail page",
            " *    out of memory.",
            " *",
            " * 2. When a page is accessed for the second time, it is promoted to",
            " *    the active list, shrinking the inactive list by one slot.  This",
            " *    also slides all inactive pages that were faulted into the cache",
            " *    more recently than the activated page towards the tail of the",
            " *    inactive list.",
            " *",
            " * Thus:",
            " *",
            " * 1. The sum of evictions and activations between any two points in",
            " *    time indicate the minimum number of inactive pages accessed in",
            " *    between.",
            " *",
            " * 2. Moving one inactive page N page slots towards the tail of the",
            " *    list requires at least N inactive page accesses.",
            " *",
            " * Combining these:",
            " *",
            " * 1. When a page is finally evicted from memory, the number of",
            " *    inactive pages accessed while the page was in cache is at least",
            " *    the number of page slots on the inactive list.",
            " *",
            " * 2. In addition, measuring the sum of evictions and activations (E)",
            " *    at the time of a page's eviction, and comparing it to another",
            " *    reading (R) at the time the page faults back into memory tells",
            " *    the minimum number of accesses while the page was not cached.",
            " *    This is called the refault distance.",
            " *",
            " * Because the first access of the page was the fault and the second",
            " * access the refault, we combine the in-cache distance with the",
            " * out-of-cache distance to get the complete minimum access distance",
            " * of this page:",
            " *",
            " *      NR_inactive + (R - E)",
            " *",
            " * And knowing the minimum access distance of a page, we can easily",
            " * tell if the page would be able to stay in cache assuming all page",
            " * slots in the cache were available:",
            " *",
            " *   NR_inactive + (R - E) <= NR_inactive + NR_active",
            " *",
            " * If we have swap we should consider about NR_inactive_anon and",
            " * NR_active_anon, so for page cache and anonymous respectively:",
            " *",
            " *   NR_inactive_file + (R - E) <= NR_inactive_file + NR_active_file",
            " *   + NR_inactive_anon + NR_active_anon",
            " *",
            " *   NR_inactive_anon + (R - E) <= NR_inactive_anon + NR_active_anon",
            " *   + NR_inactive_file + NR_active_file",
            " *",
            " * Which can be further simplified to:",
            " *",
            " *   (R - E) <= NR_active_file + NR_inactive_anon + NR_active_anon",
            " *",
            " *   (R - E) <= NR_active_anon + NR_inactive_file + NR_active_file",
            " *",
            " * Put into words, the refault distance (out-of-cache) can be seen as",
            " * a deficit in inactive list space (in-cache).  If the inactive list",
            " * had (R - E) more page slots, the page would not have been evicted",
            " * in between accesses, but activated instead.  And on a full system,",
            " * the only thing eating into inactive list space is active pages.",
            " *",
            " *",
            " *\t\tRefaulting inactive pages",
            " *",
            " * All that is known about the active list is that the pages have been",
            " * accessed more than once in the past.  This means that at any given",
            " * time there is actually a good chance that pages on the active list",
            " * are no longer in active use.",
            " *",
            " * So when a refault distance of (R - E) is observed and there are at",
            " * least (R - E) pages in the userspace workingset, the refaulting page",
            " * is activated optimistically in the hope that (R - E) pages are actually",
            " * used less frequently than the refaulting page - or even not used at",
            " * all anymore.",
            " *",
            " * That means if inactive cache is refaulting with a suitable refault",
            " * distance, we assume the cache workingset is transitioning and put",
            " * pressure on the current workingset.",
            " *",
            " * If this is wrong and demotion kicks in, the pages which are truly",
            " * used more frequently will be reactivated while the less frequently",
            " * used once will be evicted from memory.",
            " *",
            " * But if this is right, the stale pages will be pushed out of memory",
            " * and the used pages get to stay in cache.",
            " *",
            " *\t\tRefaulting active pages",
            " *",
            " * If on the other hand the refaulting pages have recently been",
            " * deactivated, it means that the active list is no longer protecting",
            " * actively used cache from reclaim. The cache is NOT transitioning to",
            " * a different workingset; the existing workingset is thrashing in the",
            " * space allocated to the page cache.",
            " *",
            " *",
            " *\t\tImplementation",
            " *",
            " * For each node's LRU lists, a counter for inactive evictions and",
            " * activations is maintained (node->nonresident_age).",
            " *",
            " * On eviction, a snapshot of this counter (along with some bits to",
            " * identify the node) is stored in the now empty page cache",
            " * slot of the evicted page.  This is called a shadow entry.",
            " *",
            " * On cache misses for which there are shadow entries, an eligible",
            " * refault distance will immediately activate the refaulting page.",
            " */",
            "",
            "#define WORKINGSET_SHIFT 1",
            "#define EVICTION_SHIFT\t((BITS_PER_LONG - BITS_PER_XA_VALUE) +\t\\",
            "\t\t\t WORKINGSET_SHIFT + NODES_SHIFT + \\",
            "\t\t\t MEM_CGROUP_ID_SHIFT)",
            "#define EVICTION_MASK\t(~0UL >> EVICTION_SHIFT)",
            "",
            "/*",
            " * Eviction timestamps need to be able to cover the full range of",
            " * actionable refaults. However, bits are tight in the xarray",
            " * entry, and after storing the identifier for the lruvec there might",
            " * not be enough left to represent every single actionable refault. In",
            " * that case, we have to sacrifice granularity for distance, and group",
            " * evictions into coarser buckets by shaving off lower timestamp bits.",
            " */",
            "static unsigned int bucket_order __read_mostly;",
            "",
            "static void *pack_shadow(int memcgid, pg_data_t *pgdat, unsigned long eviction,",
            "\t\t\t bool workingset)",
            "{",
            "\teviction &= EVICTION_MASK;",
            "\teviction = (eviction << MEM_CGROUP_ID_SHIFT) | memcgid;",
            "\teviction = (eviction << NODES_SHIFT) | pgdat->node_id;",
            "\teviction = (eviction << WORKINGSET_SHIFT) | workingset;",
            "",
            "\treturn xa_mk_value(eviction);",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义影子条目打包函数，将内存组ID、节点ID、时间戳等信息编码到xarray值中，用于记录页面被驱逐时的状态信息。",
          "similarity": 0.4225306808948517
        },
        {
          "chunk_id": 1,
          "file_path": "mm/workingset.c",
          "start_line": 209,
          "end_line": 314,
          "content": [
            "static void unpack_shadow(void *shadow, int *memcgidp, pg_data_t **pgdat,",
            "\t\t\t  unsigned long *evictionp, bool *workingsetp)",
            "{",
            "\tunsigned long entry = xa_to_value(shadow);",
            "\tint memcgid, nid;",
            "\tbool workingset;",
            "",
            "\tworkingset = entry & ((1UL << WORKINGSET_SHIFT) - 1);",
            "\tentry >>= WORKINGSET_SHIFT;",
            "\tnid = entry & ((1UL << NODES_SHIFT) - 1);",
            "\tentry >>= NODES_SHIFT;",
            "\tmemcgid = entry & ((1UL << MEM_CGROUP_ID_SHIFT) - 1);",
            "\tentry >>= MEM_CGROUP_ID_SHIFT;",
            "",
            "\t*memcgidp = memcgid;",
            "\t*pgdat = NODE_DATA(nid);",
            "\t*evictionp = entry;",
            "\t*workingsetp = workingset;",
            "}",
            "static bool lru_gen_test_recent(void *shadow, bool file, struct lruvec **lruvec,",
            "\t\t\t\tunsigned long *token, bool *workingset)",
            "{",
            "\tint memcg_id;",
            "\tunsigned long min_seq;",
            "\tstruct mem_cgroup *memcg;",
            "\tstruct pglist_data *pgdat;",
            "",
            "\tunpack_shadow(shadow, &memcg_id, &pgdat, token, workingset);",
            "",
            "\tmemcg = mem_cgroup_from_id(memcg_id);",
            "\t*lruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "",
            "\tmin_seq = READ_ONCE((*lruvec)->lrugen.min_seq[file]);",
            "\treturn (*token >> LRU_REFS_WIDTH) == (min_seq & (EVICTION_MASK >> LRU_REFS_WIDTH));",
            "}",
            "static void lru_gen_refault(struct folio *folio, void *shadow)",
            "{",
            "\tbool recent;",
            "\tint hist, tier, refs;",
            "\tbool workingset;",
            "\tunsigned long token;",
            "\tstruct lruvec *lruvec;",
            "\tstruct lru_gen_folio *lrugen;",
            "\tint type = folio_is_file_lru(folio);",
            "\tint delta = folio_nr_pages(folio);",
            "",
            "\trcu_read_lock();",
            "",
            "\trecent = lru_gen_test_recent(shadow, type, &lruvec, &token, &workingset);",
            "\tif (lruvec != folio_lruvec(folio))",
            "\t\tgoto unlock;",
            "",
            "\tmod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + type, delta);",
            "",
            "\tif (!recent)",
            "\t\tgoto unlock;",
            "",
            "\tlrugen = &lruvec->lrugen;",
            "",
            "\thist = lru_hist_from_seq(READ_ONCE(lrugen->min_seq[type]));",
            "\t/* see the comment in folio_lru_refs() */",
            "\trefs = (token & (BIT(LRU_REFS_WIDTH) - 1)) + workingset;",
            "\ttier = lru_tier_from_refs(refs);",
            "",
            "\tatomic_long_add(delta, &lrugen->refaulted[hist][type][tier]);",
            "\tmod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + type, delta);",
            "",
            "\t/*",
            "\t * Count the following two cases as stalls:",
            "\t * 1. For pages accessed through page tables, hotter pages pushed out",
            "\t *    hot pages which refaulted immediately.",
            "\t * 2. For pages accessed multiple times through file descriptors,",
            "\t *    they would have been protected by sort_folio().",
            "\t */",
            "\tif (lru_gen_in_fault() || refs >= BIT(LRU_REFS_WIDTH) - 1) {",
            "\t\tset_mask_bits(&folio->flags, 0, LRU_REFS_MASK | BIT(PG_workingset));",
            "\t\tmod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + type, delta);",
            "\t}",
            "unlock:",
            "\trcu_read_unlock();",
            "}",
            "static bool lru_gen_test_recent(void *shadow, bool file, struct lruvec **lruvec,",
            "\t\t\t\tunsigned long *token, bool *workingset)",
            "{",
            "\treturn false;",
            "}",
            "static void lru_gen_refault(struct folio *folio, void *shadow)",
            "{",
            "}",
            "void workingset_age_nonresident(struct lruvec *lruvec, unsigned long nr_pages)",
            "{",
            "\t/*",
            "\t * Reclaiming a cgroup means reclaiming all its children in a",
            "\t * round-robin fashion. That means that each cgroup has an LRU",
            "\t * order that is composed of the LRU orders of its child",
            "\t * cgroups; and every page has an LRU position not just in the",
            "\t * cgroup that owns it, but in all of that group's ancestors.",
            "\t *",
            "\t * So when the physical inactive list of a leaf cgroup ages,",
            "\t * the virtual inactive lists of all its parents, including",
            "\t * the root cgroup's, age as well.",
            "\t */",
            "\tdo {",
            "\t\tatomic_long_add(nr_pages, &lruvec->nonresident_age);",
            "\t} while ((lruvec = parent_lruvec(lruvec)));",
            "}"
          ],
          "function_name": "unpack_shadow, lru_gen_test_recent, lru_gen_refault, lru_gen_test_recent, lru_gen_refault, workingset_age_nonresident",
          "description": "包含解码影子条目、测试最近访问、处理页面故障的函数实现，但存在重复函数声明问题，实际功能涉及基于LRU生成的页面激活决策逻辑。",
          "similarity": 0.4160602390766144
        }
      ]
    }
  ]
}