{
  "query": "进程调度算法优先级计算方式",
  "timestamp": "2025-12-25 23:24:01",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/cpupri.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:04:45\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\cpupri.c`\n\n---\n\n# `sched/cpupri.c` 技术文档\n\n## 1. 文件概述\n\n`sched/cpupri.c` 实现了 **CPU 优先级管理（CPU Priority Management）** 机制，用于实时任务（RT tasks）的全局负载均衡和迁移决策。该机制通过维护一个二维位图结构，快速追踪每个 CPU 当前运行任务的最高优先级，从而在 O(1) 时间复杂度内为新唤醒或迁移的实时任务找到合适的 CPU 目标。该机制特别优化了无 CPU 亲和性限制的任务调度路径，同时支持带亲和性约束的场景。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `convert_prio(int prio)` | 将任务的调度优先级（`p->prio`）转换为内部 `cpupri` 优先级值（范围：-1 到 100） |\n| `__cpupri_find(struct cpupri *cp, struct task_struct *p, struct cpumask *lowest_mask, int idx)` | 在指定优先级层级 `idx` 中查找满足任务 `p` 的 CPU（考虑亲和性） |\n| `cpupri_find(struct cpupri *cp, struct task_struct *p, struct cpumask *lowest_mask)` | 查找系统中优先级 **低于或等于** 任务 `p` 的 CPU（即任务可运行的 CPU） |\n| `cpupri_find_fitness(...)` | 增强版查找函数，支持通过 `fitness_fn` 自定义 CPU 适配条件（如容量感知） |\n| `cpupri_set(struct cpupri *cp, int cpu, int newpri)` | 更新指定 CPU 的当前最高优先级状态 |\n| `cpupri_init(struct cpupri *cp)` | 初始化 `cpupri` 数据结构（声明但未在片段中实现） |\n\n### 关键数据结构（隐含）\n\n- `struct cpupri`：全局 CPU 优先级管理上下文\n  - `cpu_to_pri[]`：每个 CPU 当前的 `cpupri` 优先级\n  - `pri_to_cpu[]`：每个优先级对应的 `struct cpupri_vec`\n- `struct cpupri_vec`：\n  - `mask`：该优先级下所有 CPU 的位图\n  - `count`：该优先级下活跃 CPU 的数量（原子计数）\n\n### 优先级映射关系\n\n| 任务 `p->prio` | `cpupri` 值 | 含义 |\n|---------------|------------|------|\n| -1 | -1 (`CPUPRI_INVALID`) | 无效状态（CPU 不可调度） |\n| 0–98 | 99–1 | 实时优先级（数值越大，任务优先级越高，`cpupri` 值越小） |\n| 99 (`MAX_RT_PRIO-1`) | 0 (`CPUPRI_NORMAL`) | 普通（非实时）任务 |\n| 100 (`MAX_RT_PRIO`) | 100 (`CPUPRI_HIGHER`) | 高于所有 RT 任务的特殊优先级 |\n\n> **注意**：`cpupri` 值越小，表示 CPU 当前负载的优先级 **越高**。\n\n## 3. 关键实现\n\n### 优先级转换逻辑\n- `convert_prio()` 实现了任务调度优先级到 `cpupri` 内部表示的映射，确保实时任务（`p->prio` ∈ [0, 98]）被正确映射到 `cpupri` ∈ [1, 99]，且高优先级任务对应更小的 `cpupri` 值。\n\n### 快速查找算法\n- 使用 **二维位图**：第一维为优先级（0–100），第二维为 CPU 位图。\n- `cpupri_find_fitness()` 从最低优先级（`idx = 0`）开始遍历，找到第一个存在可用 CPU 的优先级层级。\n- 对于每个层级，通过 `cpumask_any_and()` 快速判断任务亲和性掩码与该优先级 CPU 掩码是否有交集。\n- 若提供 `fitness_fn`（如容量检查），会过滤掉不满足条件的 CPU；若过滤后无 CPU 可用，则继续搜索更高优先级层级。\n\n### 容错与回退策略\n- 如果启用了 `fitness_fn` 但未找到满足条件的 CPU，函数会 **忽略 fitness 条件重新搜索**，确保高优先级任务总能找到运行 CPU（优先保证实时性，而非最优资源匹配）。\n\n### 并发安全更新\n- `cpupri_set()` 使用 **内存屏障（`smp_mb__before/after_atomic()`）** 确保 CPU 位图和计数器的更新顺序：\n  1. **添加 CPU**：先设置位图 → 内存屏障 → 增加计数器\n  2. **移除 CPU**：先减少计数器 → 内存屏障 → 清除位图\n- 此顺序防止 `cpupri_find` 在并发读取时看到不一致状态（如计数器为 0 但位图仍置位）。\n\n### 亲和性处理\n- 所有查找操作均与任务的 `p->cpus_mask`（CPU 亲和性）和 `cpu_active_mask`（活跃 CPU）进行交集运算，确保只返回合法 CPU。\n\n## 4. 依赖关系\n\n- **调度器核心**：依赖 `task_struct`、`p->prio`、`p->cpus_mask` 等调度器基本结构。\n- **实时调度类（`rt.c`）**：`cpupri` 主要服务于 `SCHED_FIFO`/`SCHED_RR` 任务的负载均衡。\n- **CPU 掩码操作**：使用 `cpumask_*` 系列函数（如 `cpumask_and`, `cpumask_clear_cpu`）。\n- **内存屏障原语**：依赖 `smp_rmb()`、`smp_mb__before_atomic()` 等 SMP 同步机制。\n- **原子操作**：使用 `atomic_read/inc/dec` 管理优先级层级的 CPU 计数。\n\n## 5. 使用场景\n\n- **实时任务唤醒/迁移**：当高优先级 RT 任务被唤醒或需要迁移时，调用 `cpupri_find()` 快速定位可运行的最低优先级 CPU（减少抢占开销）。\n- **全局负载均衡**：RT 调度器的 `push_rt_task()` 和 `pull_rt_task()` 机制利用 `cpupri` 决定任务推送/拉取的目标 CPU。\n- **容量感知调度（Capacity Awareness）**：通过 `cpupri_find_fitness()` 的 `fitness_fn` 参数，集成 CPU 性能/能效信息（如 ARM big.LITTLE 架构），在满足优先级前提下选择合适 CPU。\n- **CPU 热插拔**：CPU 上下线时通过 `cpupri_set()` 更新其优先级状态（设为 `CPUPRI_INVALID` 或恢复）。",
      "similarity": 0.6080844402313232,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 42,
          "end_line": 178,
          "content": [
            "static int convert_prio(int prio)",
            "{",
            "\tint cpupri;",
            "",
            "\tswitch (prio) {",
            "\tcase CPUPRI_INVALID:",
            "\t\tcpupri = CPUPRI_INVALID;\t/* -1 */",
            "\t\tbreak;",
            "",
            "\tcase 0 ... 98:",
            "\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */",
            "\t\tbreak;",
            "",
            "\tcase MAX_RT_PRIO-1:",
            "\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */",
            "\t\tbreak;",
            "",
            "\tcase MAX_RT_PRIO:",
            "\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn cpupri;",
            "}",
            "static inline int __cpupri_find(struct cpupri *cp, struct task_struct *p,",
            "\t\t\t\tstruct cpumask *lowest_mask, int idx)",
            "{",
            "\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[idx];",
            "\tint skip = 0;",
            "",
            "\tif (!atomic_read(&(vec)->count))",
            "\t\tskip = 1;",
            "\t/*",
            "\t * When looking at the vector, we need to read the counter,",
            "\t * do a memory barrier, then read the mask.",
            "\t *",
            "\t * Note: This is still all racy, but we can deal with it.",
            "\t *  Ideally, we only want to look at masks that are set.",
            "\t *",
            "\t *  If a mask is not set, then the only thing wrong is that we",
            "\t *  did a little more work than necessary.",
            "\t *",
            "\t *  If we read a zero count but the mask is set, because of the",
            "\t *  memory barriers, that can only happen when the highest prio",
            "\t *  task for a run queue has left the run queue, in which case,",
            "\t *  it will be followed by a pull. If the task we are processing",
            "\t *  fails to find a proper place to go, that pull request will",
            "\t *  pull this task if the run queue is running at a lower",
            "\t *  priority.",
            "\t */",
            "\tsmp_rmb();",
            "",
            "\t/* Need to do the rmb for every iteration */",
            "\tif (skip)",
            "\t\treturn 0;",
            "",
            "\tif (cpumask_any_and(&p->cpus_mask, vec->mask) >= nr_cpu_ids)",
            "\t\treturn 0;",
            "",
            "\tif (lowest_mask) {",
            "\t\tcpumask_and(lowest_mask, &p->cpus_mask, vec->mask);",
            "\t\tcpumask_and(lowest_mask, lowest_mask, cpu_active_mask);",
            "",
            "\t\t/*",
            "\t\t * We have to ensure that we have at least one bit",
            "\t\t * still set in the array, since the map could have",
            "\t\t * been concurrently emptied between the first and",
            "\t\t * second reads of vec->mask.  If we hit this",
            "\t\t * condition, simply act as though we never hit this",
            "\t\t * priority level and continue on.",
            "\t\t */",
            "\t\tif (cpumask_empty(lowest_mask))",
            "\t\t\treturn 0;",
            "\t}",
            "",
            "\treturn 1;",
            "}",
            "int cpupri_find(struct cpupri *cp, struct task_struct *p,",
            "\t\tstruct cpumask *lowest_mask)",
            "{",
            "\treturn cpupri_find_fitness(cp, p, lowest_mask, NULL);",
            "}",
            "int cpupri_find_fitness(struct cpupri *cp, struct task_struct *p,",
            "\t\tstruct cpumask *lowest_mask,",
            "\t\tbool (*fitness_fn)(struct task_struct *p, int cpu))",
            "{",
            "\tint task_pri = convert_prio(p->prio);",
            "\tint idx, cpu;",
            "",
            "\tWARN_ON_ONCE(task_pri >= CPUPRI_NR_PRIORITIES);",
            "",
            "\tfor (idx = 0; idx < task_pri; idx++) {",
            "",
            "\t\tif (!__cpupri_find(cp, p, lowest_mask, idx))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!lowest_mask || !fitness_fn)",
            "\t\t\treturn 1;",
            "",
            "\t\t/* Ensure the capacity of the CPUs fit the task */",
            "\t\tfor_each_cpu(cpu, lowest_mask) {",
            "\t\t\tif (!fitness_fn(p, cpu))",
            "\t\t\t\tcpumask_clear_cpu(cpu, lowest_mask);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If no CPU at the current priority can fit the task",
            "\t\t * continue looking",
            "\t\t */",
            "\t\tif (cpumask_empty(lowest_mask))",
            "\t\t\tcontinue;",
            "",
            "\t\treturn 1;",
            "\t}",
            "",
            "\t/*",
            "\t * If we failed to find a fitting lowest_mask, kick off a new search",
            "\t * but without taking into account any fitness criteria this time.",
            "\t *",
            "\t * This rule favours honouring priority over fitting the task in the",
            "\t * correct CPU (Capacity Awareness being the only user now).",
            "\t * The idea is that if a higher priority task can run, then it should",
            "\t * run even if this ends up being on unfitting CPU.",
            "\t *",
            "\t * The cost of this trade-off is not entirely clear and will probably",
            "\t * be good for some workloads and bad for others.",
            "\t *",
            "\t * The main idea here is that if some CPUs were over-committed, we try",
            "\t * to spread which is what the scheduler traditionally did. Sys admins",
            "\t * must do proper RT planning to avoid overloading the system if they",
            "\t * really care.",
            "\t */",
            "\tif (fitness_fn)",
            "\t\treturn cpupri_find(cp, p, lowest_mask);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "convert_prio, __cpupri_find, cpupri_find, cpupri_find_fitness",
          "description": "convert_prio将任务优先级映射为CPU优先级数值；__cpupri_find检查特定优先级下是否存在可用CPU；cpupri_find_fitness遍历优先级层级寻找适配CPU，结合适应性判断与优先级策略决定最终选择",
          "similarity": 0.6507203578948975
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 210,
          "end_line": 304,
          "content": [
            "void cpupri_set(struct cpupri *cp, int cpu, int newpri)",
            "{",
            "\tint *currpri = &cp->cpu_to_pri[cpu];",
            "\tint oldpri = *currpri;",
            "\tint do_mb = 0;",
            "",
            "\tnewpri = convert_prio(newpri);",
            "",
            "\tBUG_ON(newpri >= CPUPRI_NR_PRIORITIES);",
            "",
            "\tif (newpri == oldpri)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If the CPU was currently mapped to a different value, we",
            "\t * need to map it to the new value then remove the old value.",
            "\t * Note, we must add the new value first, otherwise we risk the",
            "\t * cpu being missed by the priority loop in cpupri_find.",
            "\t */",
            "\tif (likely(newpri != CPUPRI_INVALID)) {",
            "\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[newpri];",
            "",
            "\t\tcpumask_set_cpu(cpu, vec->mask);",
            "\t\t/*",
            "\t\t * When adding a new vector, we update the mask first,",
            "\t\t * do a write memory barrier, and then update the count, to",
            "\t\t * make sure the vector is visible when count is set.",
            "\t\t */",
            "\t\tsmp_mb__before_atomic();",
            "\t\tatomic_inc(&(vec)->count);",
            "\t\tdo_mb = 1;",
            "\t}",
            "\tif (likely(oldpri != CPUPRI_INVALID)) {",
            "\t\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[oldpri];",
            "",
            "\t\t/*",
            "\t\t * Because the order of modification of the vec->count",
            "\t\t * is important, we must make sure that the update",
            "\t\t * of the new prio is seen before we decrement the",
            "\t\t * old prio. This makes sure that the loop sees",
            "\t\t * one or the other when we raise the priority of",
            "\t\t * the run queue. We don't care about when we lower the",
            "\t\t * priority, as that will trigger an rt pull anyway.",
            "\t\t *",
            "\t\t * We only need to do a memory barrier if we updated",
            "\t\t * the new priority vec.",
            "\t\t */",
            "\t\tif (do_mb)",
            "\t\t\tsmp_mb__after_atomic();",
            "",
            "\t\t/*",
            "\t\t * When removing from the vector, we decrement the counter first",
            "\t\t * do a memory barrier and then clear the mask.",
            "\t\t */",
            "\t\tatomic_dec(&(vec)->count);",
            "\t\tsmp_mb__after_atomic();",
            "\t\tcpumask_clear_cpu(cpu, vec->mask);",
            "\t}",
            "",
            "\t*currpri = newpri;",
            "}",
            "int cpupri_init(struct cpupri *cp)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++) {",
            "\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[i];",
            "",
            "\t\tatomic_set(&vec->count, 0);",
            "\t\tif (!zalloc_cpumask_var(&vec->mask, GFP_KERNEL))",
            "\t\t\tgoto cleanup;",
            "\t}",
            "",
            "\tcp->cpu_to_pri = kcalloc(nr_cpu_ids, sizeof(int), GFP_KERNEL);",
            "\tif (!cp->cpu_to_pri)",
            "\t\tgoto cleanup;",
            "",
            "\tfor_each_possible_cpu(i)",
            "\t\tcp->cpu_to_pri[i] = CPUPRI_INVALID;",
            "",
            "\treturn 0;",
            "",
            "cleanup:",
            "\tfor (i--; i >= 0; i--)",
            "\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);",
            "\treturn -ENOMEM;",
            "}",
            "void cpupri_cleanup(struct cpupri *cp)",
            "{",
            "\tint i;",
            "",
            "\tkfree(cp->cpu_to_pri);",
            "\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++)",
            "\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);",
            "}"
          ],
          "function_name": "cpupri_set, cpupri_init, cpupri_cleanup",
          "description": "cpupri_set更新CPU优先级状态，通过原子操作同步位图与计数器；cpupri_init初始化优先级到CPU的映射表与CPU到优先级的数组；cpupri_cleanup释放所有动态分配的位图资源与优先级数组",
          "similarity": 0.6007025241851807
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 1,
          "end_line": 41,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  kernel/sched/cpupri.c",
            " *",
            " *  CPU priority management",
            " *",
            " *  Copyright (C) 2007-2008 Novell",
            " *",
            " *  Author: Gregory Haskins <ghaskins@novell.com>",
            " *",
            " *  This code tracks the priority of each CPU so that global migration",
            " *  decisions are easy to calculate.  Each CPU can be in a state as follows:",
            " *",
            " *                 (INVALID), NORMAL, RT1, ... RT99, HIGHER",
            " *",
            " *  going from the lowest priority to the highest.  CPUs in the INVALID state",
            " *  are not eligible for routing.  The system maintains this state with",
            " *  a 2 dimensional bitmap (the first for priority class, the second for CPUs",
            " *  in that class).  Therefore a typical application without affinity",
            " *  restrictions can find a suitable CPU with O(1) complexity (e.g. two bit",
            " *  searches).  For tasks with affinity restrictions, the algorithm has a",
            " *  worst case complexity of O(min(101, nr_domcpus)), though the scenario that",
            " *  yields the worst case search is fairly contrived.",
            " */",
            "",
            "/*",
            " * p->rt_priority   p->prio   newpri   cpupri",
            " *",
            " *\t\t\t\t  -1       -1 (CPUPRI_INVALID)",
            " *",
            " *\t\t\t\t  99        0 (CPUPRI_NORMAL)",
            " *",
            " *\t\t1        98       98        1",
            " *\t      ...",
            " *\t       49        50       50       49",
            " *\t       50        49       49       50",
            " *\t      ...",
            " *\t       99         0        0       99",
            " *",
            " *\t\t\t\t 100\t  100 (CPUPRI_HIGHER)",
            " */"
          ],
          "function_name": null,
          "description": "定义CPU优先级管理模块，通过二维位图跟踪各CPU优先级状态，支持NORMAL、RT1至RT99及HIGHER五种优先级分类，INVALID状态表示CPU不可用，用于全局任务调度时快速计算迁移决策",
          "similarity": 0.5931844711303711
        }
      ]
    },
    {
      "source_file": "kernel/sched/syscalls.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:19:13\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\syscalls.c`\n\n---\n\n# `sched/syscalls.c` 技术文档\n\n## 1. 文件概述\n\n`sched/syscalls.c` 是 Linux 内核调度子系统的核心源文件之一，主要负责实现与调度相关的系统调用接口和优先级管理逻辑。该文件封装了任务优先级计算、nice 值设置、CPU 空闲状态判断等关键功能，为用户空间提供 `nice()` 等系统调用的内核支持，并为调度器内部模块提供优先级操作原语。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `__normal_prio()`：根据调度策略（SCHED_NORMAL/SCHED_BATCH/SCHED_IDLE、SCHED_FIFO/SCHED_RR、SCHED_DEADLINE）计算任务的“正常”优先级。\n- `normal_prio()`：基于任务当前策略、实时优先级和静态 nice 值计算其正常优先级。\n- `effective_prio()`：计算任务当前实际生效的调度优先级，考虑 RT 继承或提升。\n- `set_user_nice()`：安全地修改指定任务的 nice 值，更新其静态优先级和调度权重，并触发调度器重评估。\n- `is_nice_reduction()` / `can_nice()`：检查任务是否具备降低 nice 值（即提高优先级）的权限。\n- `sys_nice()`：实现 `nice(2)` 系统调用，允许当前进程调整自身优先级。\n- `task_prio()`：返回任务在 `/proc` 中对外暴露的用户可见优先级值。\n- `idle_cpu()` / `available_idle_cpu()`：判断指定 CPU 是否处于空闲状态。\n- `idle_task()`：获取指定 CPU 的 idle 任务结构体。\n- `update_other_load_avgs()`（SMP）：更新除 CFS 外其他调度类（RT、DL、IRQ）的负载平均值。\n- `effective_cpu_util()`（SMP）：计算 CPU 的有效利用率，用于频率调节（如 CPUFreq）。\n\n### 关键数据结构\n- 无独立定义的数据结构，主要操作 `struct task_struct` 和 `struct rq`（运行队列）。\n\n## 3. 关键实现\n\n### 优先级计算模型\n- **优先级映射**：\n  - 用户态 nice 值范围 `[-20, 19]` 映射到内核静态优先级 `[100, 139]`（通过 `NICE_TO_PRIO`）。\n  - 实时任务（RT/DL）使用 `[0, 99]` 的高优先级范围（`MAX_RT_PRIO = 100`）。\n  - `task_prio()` 返回值将内核优先级转换为用户可见格式：普通任务为 `[0,39]`，RT 任务为 `[-2,-100]`，DL 任务为 `-101`。\n- **有效优先级**：`effective_prio()` 区分“正常优先级”与“被提升的优先级”。若任务当前优先级为 RT/DL（即 `rt_or_dl_prio(p->prio)` 为真），则保留提升后的值；否则使用 `normal_prio`。\n\n### Nice 值修改安全机制\n- `set_user_nice()` 在修改 nice 值前：\n  1. 获取任务所在 CPU 的运行队列锁（`task_rq_lock`），防止并发调度。\n  2. 对 RT/DL 任务仅更新 `static_prio`（不影响调度行为）。\n  3. 对普通任务，先从运行队列中移除（若已入队或正在运行），更新 `static_prio` 和负载权重（`set_load_weight`），重新计算 `prio`，再重新入队。\n  4. 调用调度类的 `prio_changed` 回调，通知调度器优先级变更。\n\n### 权限控制\n- `can_nice()` 结合资源限制（`RLIMIT_NICE`）和特权（`CAP_SYS_NICE`）判断是否允许降低 nice 值（提高优先级）。\n- `nice_to_rlimit()` 将 nice 值 `[19,-20]` 转换为 rlimit 格式 `[1,40]` 以匹配 `RLIMIT_NICE` 的语义。\n\n### CPU 空闲判断\n- `idle_cpu()` 检查：\n  - 当前运行任务是否为 idle 任务。\n  - 运行队列中无其他可运行任务（`nr_running == 0`）。\n  - （SMP）无待处理的远程唤醒（`ttwu_pending == 0`）。\n- `available_idle_cpu()` 额外检查虚拟化场景下 CPU 是否被抢占（`vcpu_is_preempted`）。\n\n### 负载与利用率计算（SMP）\n- `update_other_load_avgs()` 周期性更新 RT、DL、IRQ 和硬件压力的负载平均值。\n- `effective_cpu_util()` 聚合 CFS、RT、DL、IRQ 的利用率，并考虑 DL 带宽预留，输出用于 CPU 频率调节的有效利用率。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/sched.h>`：核心调度数据结构和 API。\n  - `<linux/cpuset.h>`：CPU 亲和性相关（间接影响调度）。\n  - `\"sched.h\"`（本地）：调度器内部实现细节。\n  - `\"autogroup.h\"`：自动任务分组支持。\n- **调度类依赖**：\n  - 调用各调度类（CFS、RT、DL）的回调函数（如 `prio_changed`、`enqueue_task` 等）。\n- **安全模块**：调用 LSM 钩子 `security_task_setnice()`。\n- **架构相关**：\n  - `arch_scale_cpu_capacity()` / `arch_scale_hw_pressure()`：架构特定的 CPU 容量和硬件压力缩放。\n  - `__ARCH_WANT_SYS_NICE`：控制 `sys_nice` 是否编译进内核。\n\n## 5. 使用场景\n\n- **系统调用处理**：为 `nice(2)` 系统调用提供内核实现，允许用户进程动态调整自身优先级。\n- **调度器内部操作**：\n  - 在 `fork()`、`sched_setscheduler()` 等操作中计算任务优先级。\n  - 调度类在任务入队/出队时更新优先级和负载。\n- **资源监控与管理**：\n  - `/proc/[pid]/stat` 中的优先级字段通过 `task_prio()` 获取。\n  - 负载均衡器和 CPUFreq 驱动使用 `effective_cpu_util()` 获取 CPU 利用率。\n- **空闲检测**：\n  - 负载均衡、任务迁移、节能策略（如 cpuidle）依赖 `idle_cpu()` 和 `available_idle_cpu()` 判断 CPU 状态。\n- **权限控制**：在设置优先级时执行安全检查，防止非特权进程提升调度优先级。",
      "similarity": 0.5967701077461243,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/syscalls.c",
          "start_line": 19,
          "end_line": 130,
          "content": [
            "static inline int __normal_prio(int policy, int rt_prio, int nice)",
            "{",
            "\tint prio;",
            "",
            "\tif (dl_policy(policy))",
            "\t\tprio = MAX_DL_PRIO - 1;",
            "\telse if (rt_policy(policy))",
            "\t\tprio = MAX_RT_PRIO - 1 - rt_prio;",
            "\telse",
            "\t\tprio = NICE_TO_PRIO(nice);",
            "",
            "\treturn prio;",
            "}",
            "static inline int normal_prio(struct task_struct *p)",
            "{",
            "\treturn __normal_prio(p->policy, p->rt_priority, PRIO_TO_NICE(p->static_prio));",
            "}",
            "static int effective_prio(struct task_struct *p)",
            "{",
            "\tp->normal_prio = normal_prio(p);",
            "\t/*",
            "\t * If we are RT tasks or we were boosted to RT priority,",
            "\t * keep the priority unchanged. Otherwise, update priority",
            "\t * to the normal priority:",
            "\t */",
            "\tif (!rt_or_dl_prio(p->prio))",
            "\t\treturn p->normal_prio;",
            "\treturn p->prio;",
            "}",
            "void set_user_nice(struct task_struct *p, long nice)",
            "{",
            "\tbool queued, running;",
            "\tstruct rq *rq;",
            "\tint old_prio;",
            "",
            "\tif (task_nice(p) == nice || nice < MIN_NICE || nice > MAX_NICE)",
            "\t\treturn;",
            "\t/*",
            "\t * We have to be careful, if called from sys_setpriority(),",
            "\t * the task might be in the middle of scheduling on another CPU.",
            "\t */",
            "\tCLASS(task_rq_lock, rq_guard)(p);",
            "\trq = rq_guard.rq;",
            "",
            "\tupdate_rq_clock(rq);",
            "",
            "\t/*",
            "\t * The RT priorities are set via sched_setscheduler(), but we still",
            "\t * allow the 'normal' nice value to be set - but as expected",
            "\t * it won't have any effect on scheduling until the task is",
            "\t * SCHED_DEADLINE, SCHED_FIFO or SCHED_RR:",
            "\t */",
            "\tif (task_has_dl_policy(p) || task_has_rt_policy(p)) {",
            "\t\tp->static_prio = NICE_TO_PRIO(nice);",
            "\t\treturn;",
            "\t}",
            "",
            "\tqueued = task_on_rq_queued(p);",
            "\trunning = task_current(rq, p);",
            "\tif (queued)",
            "\t\tdequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);",
            "\tif (running)",
            "\t\tput_prev_task(rq, p);",
            "",
            "\tp->static_prio = NICE_TO_PRIO(nice);",
            "\tset_load_weight(p, true);",
            "\told_prio = p->prio;",
            "\tp->prio = effective_prio(p);",
            "",
            "\tif (queued)",
            "\t\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);",
            "\tif (running)",
            "\t\tset_next_task(rq, p);",
            "",
            "\t/*",
            "\t * If the task increased its priority or is running and",
            "\t * lowered its priority, then reschedule its CPU:",
            "\t */",
            "\tp->sched_class->prio_changed(rq, p, old_prio);",
            "}",
            "static bool is_nice_reduction(const struct task_struct *p, const int nice)",
            "{",
            "\t/* Convert nice value [19,-20] to rlimit style value [1,40]: */",
            "\tint nice_rlim = nice_to_rlimit(nice);",
            "",
            "\treturn (nice_rlim <= task_rlimit(p, RLIMIT_NICE));",
            "}",
            "int can_nice(const struct task_struct *p, const int nice)",
            "{",
            "\treturn is_nice_reduction(p, nice) || capable(CAP_SYS_NICE);",
            "}",
            "int task_prio(const struct task_struct *p)",
            "{",
            "\treturn p->prio - MAX_RT_PRIO;",
            "}",
            "int idle_cpu(int cpu)",
            "{",
            "\tstruct rq *rq = cpu_rq(cpu);",
            "",
            "\tif (rq->curr != rq->idle)",
            "\t\treturn 0;",
            "",
            "\tif (rq->nr_running)",
            "\t\treturn 0;",
            "",
            "#ifdef CONFIG_SMP",
            "\tif (rq->ttwu_pending)",
            "\t\treturn 0;",
            "#endif",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "__normal_prio, normal_prio, effective_prio, set_user_nice, is_nice_reduction, can_nice, task_prio, idle_cpu",
          "description": "实现优先级计算与调整逻辑，包含正常优先级计算、有效优先级判定、用户nice值修改、优先级变化检测及实时任务优先级处理等功能",
          "similarity": 0.6412018537521362
        },
        {
          "chunk_id": 8,
          "file_path": "kernel/sched/syscalls.c",
          "start_line": 1375,
          "end_line": 1517,
          "content": [
            "long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)",
            "{",
            "\tstruct affinity_context ac;",
            "\tstruct cpumask *user_mask;",
            "\tint retval;",
            "",
            "\tCLASS(find_get_task, p)(pid);",
            "\tif (!p)",
            "\t\treturn -ESRCH;",
            "",
            "\tif (p->flags & PF_NO_SETAFFINITY)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!check_same_owner(p)) {",
            "\t\tguard(rcu)();",
            "\t\tif (!ns_capable(__task_cred(p)->user_ns, CAP_SYS_NICE))",
            "\t\t\treturn -EPERM;",
            "\t}",
            "",
            "\tretval = security_task_setscheduler(p);",
            "\tif (retval)",
            "\t\treturn retval;",
            "",
            "\t/*",
            "\t * With non-SMP configs, user_cpus_ptr/user_mask isn't used and",
            "\t * alloc_user_cpus_ptr() returns NULL.",
            "\t */",
            "\tuser_mask = alloc_user_cpus_ptr(NUMA_NO_NODE);",
            "\tif (user_mask) {",
            "\t\tcpumask_copy(user_mask, in_mask);",
            "\t} else if (IS_ENABLED(CONFIG_SMP)) {",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\tac = (struct affinity_context){",
            "\t\t.new_mask  = in_mask,",
            "\t\t.user_mask = user_mask,",
            "\t\t.flags     = SCA_USER,",
            "\t};",
            "",
            "\tretval = __sched_setaffinity(p, &ac);",
            "\tkfree(ac.user_mask);",
            "",
            "\treturn retval;",
            "}",
            "static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,",
            "\t\t\t     struct cpumask *new_mask)",
            "{",
            "\tif (len < cpumask_size())",
            "\t\tcpumask_clear(new_mask);",
            "\telse if (len > cpumask_size())",
            "\t\tlen = cpumask_size();",
            "",
            "\treturn copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;",
            "}",
            "long sched_getaffinity(pid_t pid, struct cpumask *mask)",
            "{",
            "\tstruct task_struct *p;",
            "\tint retval;",
            "",
            "\tguard(rcu)();",
            "\tp = find_process_by_pid(pid);",
            "\tif (!p)",
            "\t\treturn -ESRCH;",
            "",
            "\tretval = security_task_getscheduler(p);",
            "\tif (retval)",
            "\t\treturn retval;",
            "",
            "\tguard(raw_spinlock_irqsave)(&p->pi_lock);",
            "\tcpumask_and(mask, &p->cpus_mask, cpu_active_mask);",
            "",
            "\treturn 0;",
            "}",
            "static void do_sched_yield(void)",
            "{",
            "\tstruct rq_flags rf;",
            "\tstruct rq *rq;",
            "",
            "\trq = this_rq_lock_irq(&rf);",
            "",
            "\tschedstat_inc(rq->yld_count);",
            "\tcurrent->sched_class->yield_task(rq);",
            "",
            "\tpreempt_disable();",
            "\trq_unlock_irq(rq, &rf);",
            "\tsched_preempt_enable_no_resched();",
            "",
            "\tschedule();",
            "}",
            "void __sched yield(void)",
            "{",
            "\tset_current_state(TASK_RUNNING);",
            "\tdo_sched_yield();",
            "}",
            "int __sched yield_to(struct task_struct *p, bool preempt)",
            "{",
            "\tstruct task_struct *curr = current;",
            "\tstruct rq *rq, *p_rq;",
            "\tint yielded = 0;",
            "",
            "\tscoped_guard (irqsave) {",
            "\t\trq = this_rq();",
            "",
            "again:",
            "\t\tp_rq = task_rq(p);",
            "\t\t/*",
            "\t\t * If we're the only runnable task on the rq and target rq also",
            "\t\t * has only one task, there's absolutely no point in yielding.",
            "\t\t */",
            "\t\tif (rq->nr_running == 1 && p_rq->nr_running == 1)",
            "\t\t\treturn -ESRCH;",
            "",
            "\t\tguard(double_rq_lock)(rq, p_rq);",
            "\t\tif (task_rq(p) != p_rq)",
            "\t\t\tgoto again;",
            "",
            "\t\tif (!curr->sched_class->yield_to_task)",
            "\t\t\treturn 0;",
            "",
            "\t\tif (curr->sched_class != p->sched_class)",
            "\t\t\treturn 0;",
            "",
            "\t\tif (task_on_cpu(p_rq, p) || !task_is_running(p))",
            "\t\t\treturn 0;",
            "",
            "\t\tyielded = curr->sched_class->yield_to_task(rq, p);",
            "\t\tif (yielded) {",
            "\t\t\tschedstat_inc(rq->yld_count);",
            "\t\t\t/*",
            "\t\t\t * Make p's CPU reschedule; pick_next_entity",
            "\t\t\t * takes care of fairness.",
            "\t\t\t */",
            "\t\t\tif (preempt && rq != p_rq)",
            "\t\t\t\tresched_curr(p_rq);",
            "\t\t}",
            "\t}",
            "",
            "\tif (yielded)",
            "\t\tschedule();",
            "",
            "\treturn yielded;",
            "}"
          ],
          "function_name": "sched_setaffinity, get_user_cpu_mask, sched_getaffinity, do_sched_yield, yield, yield_to",
          "description": "实现CPU亲和性设置/获取及调度让步功能。sched_setaffinity设置进程CPU亲和性掩码并进行权限检查，sched_getaffinity获取当前亲和性掩码。yield/yield_to实现调度器让步操作，do_sched_yield触发当前进程主动让出CPU。",
          "similarity": 0.57456374168396
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/sched/syscalls.c",
          "start_line": 646,
          "end_line": 887,
          "content": [
            "int __sched_setscheduler(struct task_struct *p,",
            "\t\t\t const struct sched_attr *attr,",
            "\t\t\t bool user, bool pi)",
            "{",
            "\tint oldpolicy = -1, policy = attr->sched_policy;",
            "\tint retval, oldprio, newprio, queued, running;",
            "\tconst struct sched_class *prev_class, *next_class;",
            "\tstruct balance_callback *head;",
            "\tstruct rq_flags rf;",
            "\tint reset_on_fork;",
            "\tint queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;",
            "\tstruct rq *rq;",
            "\tbool cpuset_locked = false;",
            "",
            "\t/* The pi code expects interrupts enabled */",
            "\tBUG_ON(pi && in_interrupt());",
            "recheck:",
            "\t/* Double check policy once rq lock held: */",
            "\tif (policy < 0) {",
            "\t\treset_on_fork = p->sched_reset_on_fork;",
            "\t\tpolicy = oldpolicy = p->policy;",
            "\t} else {",
            "\t\treset_on_fork = !!(attr->sched_flags & SCHED_FLAG_RESET_ON_FORK);",
            "",
            "\t\tif (!valid_policy(policy))",
            "\t\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tif (attr->sched_flags & ~(SCHED_FLAG_ALL | SCHED_FLAG_SUGOV))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Valid priorities for SCHED_FIFO and SCHED_RR are",
            "\t * 1..MAX_RT_PRIO-1, valid priority for SCHED_NORMAL,",
            "\t * SCHED_BATCH and SCHED_IDLE is 0.",
            "\t */",
            "\tif (attr->sched_priority > MAX_RT_PRIO-1)",
            "\t\treturn -EINVAL;",
            "\tif ((dl_policy(policy) && !__checkparam_dl(attr)) ||",
            "\t    (rt_policy(policy) != (attr->sched_priority != 0)))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (user) {",
            "\t\tretval = user_check_sched_setscheduler(p, attr, policy, reset_on_fork);",
            "\t\tif (retval)",
            "\t\t\treturn retval;",
            "",
            "\t\tif (attr->sched_flags & SCHED_FLAG_SUGOV)",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\tretval = security_task_setscheduler(p);",
            "\t\tif (retval)",
            "\t\t\treturn retval;",
            "\t}",
            "",
            "\t/* Update task specific \"requested\" clamps */",
            "\tif (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP) {",
            "\t\tretval = uclamp_validate(p, attr);",
            "\t\tif (retval)",
            "\t\t\treturn retval;",
            "\t}",
            "",
            "\t/*",
            "\t * SCHED_DEADLINE bandwidth accounting relies on stable cpusets",
            "\t * information.",
            "\t */",
            "\tif (dl_policy(policy) || dl_policy(p->policy)) {",
            "\t\tcpuset_locked = true;",
            "\t\tcpuset_lock();",
            "\t}",
            "",
            "\t/*",
            "\t * Make sure no PI-waiters arrive (or leave) while we are",
            "\t * changing the priority of the task:",
            "\t *",
            "\t * To be able to change p->policy safely, the appropriate",
            "\t * runqueue lock must be held.",
            "\t */",
            "\trq = task_rq_lock(p, &rf);",
            "\tupdate_rq_clock(rq);",
            "",
            "\t/*",
            "\t * Changing the policy of the stop threads its a very bad idea:",
            "\t */",
            "\tif (p == rq->stop) {",
            "\t\tretval = -EINVAL;",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\tretval = scx_check_setscheduler(p, policy);",
            "\tif (retval)",
            "\t\tgoto unlock;",
            "",
            "\t/*",
            "\t * If not changing anything there's no need to proceed further,",
            "\t * but store a possible modification of reset_on_fork.",
            "\t */",
            "\tif (unlikely(policy == p->policy)) {",
            "\t\tif (fair_policy(policy) &&",
            "\t\t    (attr->sched_nice != task_nice(p) ||",
            "\t\t     (attr->sched_runtime != p->se.slice)))",
            "\t\t\tgoto change;",
            "\t\tif (rt_policy(policy) && attr->sched_priority != p->rt_priority)",
            "\t\t\tgoto change;",
            "\t\tif (dl_policy(policy) && dl_param_changed(p, attr))",
            "\t\t\tgoto change;",
            "\t\tif (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)",
            "\t\t\tgoto change;",
            "",
            "\t\tp->sched_reset_on_fork = reset_on_fork;",
            "\t\tretval = 0;",
            "\t\tgoto unlock;",
            "\t}",
            "change:",
            "",
            "\tif (user) {",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\t\t/*",
            "\t\t * Do not allow realtime tasks into groups that have no runtime",
            "\t\t * assigned.",
            "\t\t */",
            "\t\tif (rt_bandwidth_enabled() && rt_policy(policy) &&",
            "\t\t\t\ttask_group(p)->rt_bandwidth.rt_runtime == 0 &&",
            "\t\t\t\t!task_group_is_autogroup(task_group(p))) {",
            "\t\t\tretval = -EPERM;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "#endif",
            "#ifdef CONFIG_SMP",
            "\t\tif (dl_bandwidth_enabled() && dl_policy(policy) &&",
            "\t\t\t\t!(attr->sched_flags & SCHED_FLAG_SUGOV)) {",
            "\t\t\tcpumask_t *span = rq->rd->span;",
            "",
            "\t\t\t/*",
            "\t\t\t * Don't allow tasks with an affinity mask smaller than",
            "\t\t\t * the entire root_domain to become SCHED_DEADLINE. We",
            "\t\t\t * will also fail if there's no bandwidth available.",
            "\t\t\t */",
            "\t\t\tif (!cpumask_subset(span, p->cpus_ptr) ||",
            "\t\t\t    rq->rd->dl_bw.bw == 0) {",
            "\t\t\t\tretval = -EPERM;",
            "\t\t\t\tgoto unlock;",
            "\t\t\t}",
            "\t\t}",
            "#endif",
            "\t}",
            "",
            "\t/* Re-check policy now with rq lock held: */",
            "\tif (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {",
            "\t\tpolicy = oldpolicy = -1;",
            "\t\ttask_rq_unlock(rq, p, &rf);",
            "\t\tif (cpuset_locked)",
            "\t\t\tcpuset_unlock();",
            "\t\tgoto recheck;",
            "\t}",
            "",
            "\t/*",
            "\t * If setscheduling to SCHED_DEADLINE (or changing the parameters",
            "\t * of a SCHED_DEADLINE task) we need to check if enough bandwidth",
            "\t * is available.",
            "\t */",
            "\tif ((dl_policy(policy) || dl_task(p)) && sched_dl_overflow(p, policy, attr)) {",
            "\t\tretval = -EBUSY;",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\tp->sched_reset_on_fork = reset_on_fork;",
            "\toldprio = p->prio;",
            "",
            "\tnewprio = __normal_prio(policy, attr->sched_priority, attr->sched_nice);",
            "\tif (pi) {",
            "\t\t/*",
            "\t\t * Take priority boosted tasks into account. If the new",
            "\t\t * effective priority is unchanged, we just store the new",
            "\t\t * normal parameters and do not touch the scheduler class and",
            "\t\t * the runqueue. This will be done when the task deboost",
            "\t\t * itself.",
            "\t\t */",
            "\t\tnewprio = rt_effective_prio(p, newprio);",
            "\t\tif (newprio == oldprio)",
            "\t\t\tqueue_flags &= ~DEQUEUE_MOVE;",
            "\t}",
            "",
            "\tprev_class = p->sched_class;",
            "\tnext_class = __setscheduler_class(policy, newprio);",
            "",
            "\tif (prev_class != next_class && p->se.sched_delayed)",
            "\t\tdequeue_task(rq, p, DEQUEUE_SLEEP | DEQUEUE_DELAYED | DEQUEUE_NOCLOCK);",
            "",
            "\tqueued = task_on_rq_queued(p);",
            "\trunning = task_current(rq, p);",
            "\tif (queued)",
            "\t\tdequeue_task(rq, p, queue_flags);",
            "\tif (running)",
            "\t\tput_prev_task(rq, p);",
            "",
            "\tif (!(attr->sched_flags & SCHED_FLAG_KEEP_PARAMS)) {",
            "\t\t__setscheduler_params(p, attr);",
            "\t\tp->sched_class = next_class;",
            "\t\tp->prio = newprio;",
            "\t}",
            "\t__setscheduler_uclamp(p, attr);",
            "\tcheck_class_changing(rq, p, prev_class);",
            "",
            "\tif (queued) {",
            "\t\t/*",
            "\t\t * We enqueue to tail when the priority of a task is",
            "\t\t * increased (user space view).",
            "\t\t */",
            "\t\tif (oldprio < p->prio)",
            "\t\t\tqueue_flags |= ENQUEUE_HEAD;",
            "",
            "\t\tenqueue_task(rq, p, queue_flags);",
            "\t}",
            "\tif (running)",
            "\t\tset_next_task(rq, p);",
            "",
            "\tcheck_class_changed(rq, p, prev_class, oldprio);",
            "",
            "\t/* Avoid rq from going away on us: */",
            "\tpreempt_disable();",
            "\thead = splice_balance_callbacks(rq);",
            "\ttask_rq_unlock(rq, p, &rf);",
            "",
            "\tif (pi) {",
            "\t\tif (cpuset_locked)",
            "\t\t\tcpuset_unlock();",
            "\t\trt_mutex_adjust_pi(p);",
            "\t}",
            "",
            "\t/* Run balance callbacks after we've adjusted the PI chain: */",
            "\tbalance_callbacks(rq, head);",
            "\tpreempt_enable();",
            "",
            "\treturn 0;",
            "",
            "unlock:",
            "\ttask_rq_unlock(rq, p, &rf);",
            "\tif (cpuset_locked)",
            "\t\tcpuset_unlock();",
            "\treturn retval;",
            "}"
          ],
          "function_name": "__sched_setscheduler",
          "description": "实现__sched_setscheduler函数，用于修改任务的调度策略及参数。该函数验证新策略有效性，更新任务优先级，切换调度类并调整运行队列状态，处理PI互斥锁及负载均衡回调。核心功能是安全地变更任务调度参数并维护调度器一致性。",
          "similarity": 0.5303232669830322
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/syscalls.c",
          "start_line": 1,
          "end_line": 18,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  kernel/sched/syscalls.c",
            " *",
            " *  Core kernel scheduler syscalls related code",
            " *",
            " *  Copyright (C) 1991-2002  Linus Torvalds",
            " *  Copyright (C) 1998-2024  Ingo Molnar, Red Hat",
            " */",
            "#include <linux/sched.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/sched/debug.h>",
            "",
            "#include <uapi/linux/sched/types.h>",
            "",
            "#include \"sched.h\"",
            "#include \"autogroup.h\"",
            ""
          ],
          "function_name": null,
          "description": "声明调度器系统调用相关头文件，包含调度策略、CPU集、调试信息及内核调度器和自动分组模块的实现依赖",
          "similarity": 0.5302765369415283
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/syscalls.c",
          "start_line": 531,
          "end_line": 634,
          "content": [
            "static void __setscheduler_uclamp(struct task_struct *p,",
            "\t\t\t\t  const struct sched_attr *attr)",
            "{",
            "\tenum uclamp_id clamp_id;",
            "",
            "\tfor_each_clamp_id(clamp_id) {",
            "\t\tstruct uclamp_se *uc_se = &p->uclamp_req[clamp_id];",
            "\t\tunsigned int value;",
            "",
            "\t\tif (!uclamp_reset(attr, clamp_id, uc_se))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * RT by default have a 100% boost value that could be modified",
            "\t\t * at runtime.",
            "\t\t */",
            "\t\tif (unlikely(rt_task(p) && clamp_id == UCLAMP_MIN))",
            "\t\t\tvalue = sysctl_sched_uclamp_util_min_rt_default;",
            "\t\telse",
            "\t\t\tvalue = uclamp_none(clamp_id);",
            "",
            "\t\tuclamp_se_set(uc_se, value, false);",
            "",
            "\t}",
            "",
            "\tif (likely(!(attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)))",
            "\t\treturn;",
            "",
            "\tif (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN &&",
            "\t    attr->sched_util_min != -1) {",
            "\t\tuclamp_se_set(&p->uclamp_req[UCLAMP_MIN],",
            "\t\t\t      attr->sched_util_min, true);",
            "\t}",
            "",
            "\tif (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX &&",
            "\t    attr->sched_util_max != -1) {",
            "\t\tuclamp_se_set(&p->uclamp_req[UCLAMP_MAX],",
            "\t\t\t      attr->sched_util_max, true);",
            "\t}",
            "}",
            "static inline int uclamp_validate(struct task_struct *p,",
            "\t\t\t\t  const struct sched_attr *attr)",
            "{",
            "\treturn -EOPNOTSUPP;",
            "}",
            "static void __setscheduler_uclamp(struct task_struct *p,",
            "\t\t\t\t  const struct sched_attr *attr) { }",
            "static int user_check_sched_setscheduler(struct task_struct *p,",
            "\t\t\t\t\t const struct sched_attr *attr,",
            "\t\t\t\t\t int policy, int reset_on_fork)",
            "{",
            "\tif (fair_policy(policy)) {",
            "\t\tif (attr->sched_nice < task_nice(p) &&",
            "\t\t    !is_nice_reduction(p, attr->sched_nice))",
            "\t\t\tgoto req_priv;",
            "\t}",
            "",
            "\tif (rt_policy(policy)) {",
            "\t\tunsigned long rlim_rtprio = task_rlimit(p, RLIMIT_RTPRIO);",
            "",
            "\t\t/* Can't set/change the rt policy: */",
            "\t\tif (policy != p->policy && !rlim_rtprio)",
            "\t\t\tgoto req_priv;",
            "",
            "\t\t/* Can't increase priority: */",
            "\t\tif (attr->sched_priority > p->rt_priority &&",
            "\t\t    attr->sched_priority > rlim_rtprio)",
            "\t\t\tgoto req_priv;",
            "\t}",
            "",
            "\t/*",
            "\t * Can't set/change SCHED_DEADLINE policy at all for now",
            "\t * (safest behavior); in the future we would like to allow",
            "\t * unprivileged DL tasks to increase their relative deadline",
            "\t * or reduce their runtime (both ways reducing utilization)",
            "\t */",
            "\tif (dl_policy(policy))",
            "\t\tgoto req_priv;",
            "",
            "\t/*",
            "\t * Treat SCHED_IDLE as nice 20. Only allow a switch to",
            "\t * SCHED_NORMAL if the RLIMIT_NICE would normally permit it.",
            "\t */",
            "\tif (task_has_idle_policy(p) && !idle_policy(policy)) {",
            "\t\tif (!is_nice_reduction(p, task_nice(p)))",
            "\t\t\tgoto req_priv;",
            "\t}",
            "",
            "\t/* Can't change other user's priorities: */",
            "\tif (!check_same_owner(p))",
            "\t\tgoto req_priv;",
            "",
            "\t/* Normal users shall not reset the sched_reset_on_fork flag: */",
            "\tif (p->sched_reset_on_fork && !reset_on_fork)",
            "\t\tgoto req_priv;",
            "",
            "\treturn 0;",
            "",
            "req_priv:",
            "\tif (!capable(CAP_SYS_NICE))",
            "\t\treturn -EPERM;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "__setscheduler_uclamp, uclamp_validate, __setscheduler_uclamp, user_check_sched_setscheduler",
          "description": "管理uclamp参数设置与验证，实现调度策略变更权限检查，包含实时任务优先级限制、SCHED_DEADLINE策略限制及跨用户优先级修改控制",
          "similarity": 0.526339590549469
        }
      ]
    },
    {
      "source_file": "kernel/sched/deadline.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:06:40\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\deadline.c`\n\n---\n\n# `sched/deadline.c` 技术文档\n\n## 1. 文件概述\n\n`sched/deadline.c` 是 Linux 内核调度器中 **SCHED_DEADLINE** 调度类的核心实现文件。该调度类基于 **最早截止时间优先（Earliest Deadline First, EDF）** 算法，并结合 **恒定带宽服务器（Constant Bandwidth Server, CBS）** 机制，为具有严格实时性要求的任务提供可预测的调度保障。\n\n其核心目标是：  \n- 对于周期性任务，若其实际运行时间不超过所申请的运行时间（runtime），则保证不会错过任何截止时间（deadline）；  \n- 对于非周期性任务、突发任务或试图超出其预留带宽的任务，系统会对其进行节流（throttling），防止其影响其他任务的实时性保障。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_dl_entity`：表示一个 deadline 调度实体，包含任务的运行时间（runtime）、截止期限（deadline）、周期（period）、带宽（dl_bw）等关键参数。\n- `struct dl_rq`：每个 CPU 的 deadline 运行队列，维护该 CPU 上所有 deadline 任务的红黑树、当前带宽使用情况（`this_bw`、`running_bw`）等。\n- `struct dl_bw`：deadline 带宽管理结构，用于跟踪系统或调度域中已分配的总带宽（`total_bw`）。\n\n### 主要函数与辅助宏\n\n#### 调度实体与运行队列关联\n- `dl_task_of(dl_se)`：从 `sched_dl_entity` 获取对应的 `task_struct`（仅适用于普通任务，不适用于服务器实体）。\n- `rq_of_dl_rq(dl_rq)` / `rq_of_dl_se(dl_se)`：获取与 deadline 运行队列或调度实体关联的 `rq`（runqueue）。\n- `dl_rq_of_se(dl_se)`：获取调度实体所属的 `dl_rq`。\n- `on_dl_rq(dl_se)`：判断调度实体是否已在 deadline 运行队列中（通过红黑树节点是否为空判断）。\n\n#### 优先级继承（PI）支持（`CONFIG_RT_MUTEXES`）\n- `pi_of(dl_se)`：获取当前调度实体因优先级继承而提升后的“代理”实体。\n- `is_dl_boosted(dl_se)`：判断该 deadline 实体是否因优先级继承被提升。\n\n#### 带宽管理（SMP 与 UP 差异处理）\n- `dl_bw_of(cpu)`：获取指定 CPU 所属调度域（或本地）的 `dl_bw` 结构。\n- `dl_bw_cpus(cpu)`：返回该 CPU 所在调度域中活跃 CPU 的数量。\n- `dl_bw_capacity(cpu)`：计算调度域的总 CPU 容量（考虑异构 CPU 的 `arch_scale_cpu_capacity`）。\n- `__dl_add()` / `__dl_sub()`：向带宽池中添加或移除任务带宽，并更新 `extra_bw`（用于负载均衡）。\n- `__dl_overflow()`：检查新增带宽是否超出系统/调度域的可用带宽上限。\n\n#### 运行时带宽跟踪\n- `__add_running_bw()` / `__sub_running_bw()`：更新 `dl_rq->running_bw`（当前正在运行的 deadline 任务所消耗的带宽）。\n- `__add_rq_bw()` / `__sub_rq_bw()`：更新 `dl_rq->this_bw`（该运行队列上所有 deadline 任务的总预留带宽）。\n- `add_running_bw()` / `sub_running_bw()` / `add_rq_bw()` / `sub_rq_bw()`：带宽操作的封装，跳过“特殊”调度实体（如服务器）。\n\n#### 其他\n- `dl_server(dl_se)`：判断调度实体是否为 CBS 服务器（而非普通任务）。\n- `dl_bw_visited(cpu, gen)`：用于带宽遍历去重（SMP 场景）。\n\n### 系统控制接口（`CONFIG_SYSCTL`）\n- `sched_deadline_period_max_us`：deadline 任务周期上限（默认 ~4 秒）。\n- `sched_deadline_period_min_us`：deadline 任务周期下限（默认 100 微秒），防止定时器 DoS。\n\n## 3. 关键实现\n\n### EDF + CBS 调度模型\n- 每个 deadline 任务通过 `runtime`、`deadline`、`period` 三个参数定义其资源需求。\n- 调度器按 **绝对截止时间（absolute deadline）** 对任务排序，使用红黑树实现 O(log n) 的调度决策。\n- CBS 机制确保任务即使突发执行，也不会长期占用超过其 `runtime/period` 的 CPU 带宽，超限任务会被 throttled。\n\n### 带宽隔离与全局限制\n- 在 SMP 系统中，deadline 带宽按 **调度域（root domain）** 进行管理，防止跨 CPU 的带宽滥用。\n- 总带宽限制默认为 CPU 总容量的 95%（由 `sysctl_sched_util_clamp_min` 等机制间接控制，具体限制逻辑在带宽分配函数中体现）。\n- `dl_bw->total_bw` 跟踪已分配带宽，`__dl_overflow()` 用于在任务加入时检查是否超限。\n\n### 异构 CPU 支持\n- 通过 `arch_scale_cpu_capacity()` 获取每个 CPU 的相对性能权重。\n- `dl_bw_capacity()` 在异构系统中返回调度域内所有活跃 CPU 的容量总和，用于带宽比例计算（`cap_scale()`）。\n\n### 与 cpufreq 集成\n- 每次 `running_bw` 变化时调用 `cpufreq_update_util()`，通知 CPU 频率调节器当前 deadline 负载，确保满足实时性能需求。\n\n### 优先级继承（PI）\n- 当 deadline 任务因持有 mutex 而阻塞高优先级任务时，通过 `pi_se` 字段临时提升其调度参数，避免优先级反转。\n\n## 4. 依赖关系\n\n- **核心调度框架**：依赖 `kernel/sched/sched.h` 中定义的通用调度结构（如 `rq`、`task_struct`）和宏（如 `SCHED_CAPACITY_SCALE`）。\n- **CPU 拓扑与容量**：依赖 `arch_scale_cpu_capacity()`（由各架构实现）获取 CPU 性能信息。\n- **RCU 机制**：在 SMP 路径中大量使用 `rcu_read_lock_sched_held()` 进行锁依赖检查。\n- **cpufreq 子系统**：通过 `cpufreq_update_util()` 与 CPU 频率调节器交互。\n- **实时互斥锁**：`CONFIG_RT_MUTEXES` 启用时，支持 deadline 任务的优先级继承。\n- **Sysctl 接口**：`CONFIG_SYSCTL` 启用时，提供用户空间可调的 deadline 参数。\n\n## 5. 使用场景\n\n- **工业实时控制**：如机器人控制、数控机床等需要严格周期性和低延迟响应的场景。\n- **音视频处理**：专业音视频采集、编码、播放等对 jitter 敏感的应用。\n- **电信基础设施**：5G 基站、核心网网元中的高优先级信令处理。\n- **汽车电子**：ADAS、自动驾驶系统中的关键任务调度。\n- **科研与高性能计算**：需要确定性执行时间的实验或仿真任务。\n\n用户通过 `sched_setattr(2)` 系统调用设置任务的 `SCHED_DEADLINE` 策略及对应的 `runtime`、`deadline`、`period` 参数，内核则通过本文件实现的调度逻辑确保其满足实时性约束。",
      "similarity": 0.5903849005699158,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 514,
          "end_line": 616,
          "content": [
            "static inline int is_leftmost(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\treturn rb_first_cached(&dl_rq->root) == &dl_se->rb_node;",
            "}",
            "void init_dl_bw(struct dl_bw *dl_b)",
            "{",
            "\traw_spin_lock_init(&dl_b->lock);",
            "\tif (global_rt_runtime() == RUNTIME_INF)",
            "\t\tdl_b->bw = -1;",
            "\telse",
            "\t\tdl_b->bw = to_ratio(global_rt_period(), global_rt_runtime());",
            "\tdl_b->total_bw = 0;",
            "}",
            "void init_dl_rq(struct dl_rq *dl_rq)",
            "{",
            "\tdl_rq->root = RB_ROOT_CACHED;",
            "",
            "#ifdef CONFIG_SMP",
            "\t/* zero means no -deadline tasks */",
            "\tdl_rq->earliest_dl.curr = dl_rq->earliest_dl.next = 0;",
            "",
            "\tdl_rq->overloaded = 0;",
            "\tdl_rq->pushable_dl_tasks_root = RB_ROOT_CACHED;",
            "#else",
            "\tinit_dl_bw(&dl_rq->dl_bw);",
            "#endif",
            "",
            "\tdl_rq->running_bw = 0;",
            "\tdl_rq->this_bw = 0;",
            "\tinit_dl_rq_bw_ratio(dl_rq);",
            "}",
            "static inline int dl_overloaded(struct rq *rq)",
            "{",
            "\treturn atomic_read(&rq->rd->dlo_count);",
            "}",
            "static inline void dl_set_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tcpumask_set_cpu(rq->cpu, rq->rd->dlo_mask);",
            "\t/*",
            "\t * Must be visible before the overload count is",
            "\t * set (as in sched_rt.c).",
            "\t *",
            "\t * Matched by the barrier in pull_dl_task().",
            "\t */",
            "\tsmp_wmb();",
            "\tatomic_inc(&rq->rd->dlo_count);",
            "}",
            "static inline void dl_clear_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tatomic_dec(&rq->rd->dlo_count);",
            "\tcpumask_clear_cpu(rq->cpu, rq->rd->dlo_mask);",
            "}",
            "static inline bool __pushable_less(struct rb_node *a, const struct rb_node *b)",
            "{",
            "\treturn dl_entity_preempt(&__node_2_pdl(a)->dl, &__node_2_pdl(b)->dl);",
            "}",
            "static inline int has_pushable_dl_tasks(struct rq *rq)",
            "{",
            "\treturn !RB_EMPTY_ROOT(&rq->dl.pushable_dl_tasks_root.rb_root);",
            "}",
            "static void enqueue_pushable_dl_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tstruct rb_node *leftmost;",
            "",
            "\tWARN_ON_ONCE(!RB_EMPTY_NODE(&p->pushable_dl_tasks));",
            "",
            "\tleftmost = rb_add_cached(&p->pushable_dl_tasks,",
            "\t\t\t\t &rq->dl.pushable_dl_tasks_root,",
            "\t\t\t\t __pushable_less);",
            "\tif (leftmost)",
            "\t\trq->dl.earliest_dl.next = p->dl.deadline;",
            "",
            "\tif (!rq->dl.overloaded) {",
            "\t\tdl_set_overload(rq);",
            "\t\trq->dl.overloaded = 1;",
            "\t}",
            "}",
            "static void dequeue_pushable_dl_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tstruct dl_rq *dl_rq = &rq->dl;",
            "\tstruct rb_root_cached *root = &dl_rq->pushable_dl_tasks_root;",
            "\tstruct rb_node *leftmost;",
            "",
            "\tif (RB_EMPTY_NODE(&p->pushable_dl_tasks))",
            "\t\treturn;",
            "",
            "\tleftmost = rb_erase_cached(&p->pushable_dl_tasks, root);",
            "\tif (leftmost)",
            "\t\tdl_rq->earliest_dl.next = __node_2_pdl(leftmost)->dl.deadline;",
            "",
            "\tRB_CLEAR_NODE(&p->pushable_dl_tasks);",
            "",
            "\tif (!has_pushable_dl_tasks(rq) && rq->dl.overloaded) {",
            "\t\tdl_clear_overload(rq);",
            "\t\trq->dl.overloaded = 0;",
            "\t}",
            "}"
          ],
          "function_name": "is_leftmost, init_dl_bw, init_dl_rq, dl_overloaded, dl_set_overload, dl_clear_overload, __pushable_less, has_pushable_dl_tasks, enqueue_pushable_dl_task, dequeue_pushable_dl_task",
          "description": "实现截止时间调度的抢占判定和过载管理机制，包含任务优先级比较、过载标记维护及可推送任务的数据结构操作。",
          "similarity": 0.6340646743774414
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2104,
          "end_line": 2251,
          "content": [
            "static void enqueue_task_dl(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tif (is_dl_boosted(&p->dl)) {",
            "\t\t/*",
            "\t\t * Because of delays in the detection of the overrun of a",
            "\t\t * thread's runtime, it might be the case that a thread",
            "\t\t * goes to sleep in a rt mutex with negative runtime. As",
            "\t\t * a consequence, the thread will be throttled.",
            "\t\t *",
            "\t\t * While waiting for the mutex, this thread can also be",
            "\t\t * boosted via PI, resulting in a thread that is throttled",
            "\t\t * and boosted at the same time.",
            "\t\t *",
            "\t\t * In this case, the boost overrides the throttle.",
            "\t\t */",
            "\t\tif (p->dl.dl_throttled) {",
            "\t\t\t/*",
            "\t\t\t * The replenish timer needs to be canceled. No",
            "\t\t\t * problem if it fires concurrently: boosted threads",
            "\t\t\t * are ignored in dl_task_timer().",
            "\t\t\t *",
            "\t\t\t * If the timer callback was running (hrtimer_try_to_cancel == -1),",
            "\t\t\t * it will eventually call put_task_struct().",
            "\t\t\t */",
            "\t\t\tif (hrtimer_try_to_cancel(&p->dl.dl_timer) == 1 &&",
            "\t\t\t    !dl_server(&p->dl))",
            "\t\t\t\tput_task_struct(p);",
            "\t\t\tp->dl.dl_throttled = 0;",
            "\t\t}",
            "\t} else if (!dl_prio(p->normal_prio)) {",
            "\t\t/*",
            "\t\t * Special case in which we have a !SCHED_DEADLINE task that is going",
            "\t\t * to be deboosted, but exceeds its runtime while doing so. No point in",
            "\t\t * replenishing it, as it's going to return back to its original",
            "\t\t * scheduling class after this. If it has been throttled, we need to",
            "\t\t * clear the flag, otherwise the task may wake up as throttled after",
            "\t\t * being boosted again with no means to replenish the runtime and clear",
            "\t\t * the throttle.",
            "\t\t */",
            "\t\tp->dl.dl_throttled = 0;",
            "\t\tif (!(flags & ENQUEUE_REPLENISH))",
            "\t\t\tprintk_deferred_once(\"sched: DL de-boosted task PID %d: REPLENISH flag missing\\n\",",
            "\t\t\t\t\t     task_pid_nr(p));",
            "",
            "\t\treturn;",
            "\t}",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_dl(dl_rq_of_se(&p->dl), &p->dl);",
            "",
            "\tif (p->on_rq == TASK_ON_RQ_MIGRATING)",
            "\t\tflags |= ENQUEUE_MIGRATING;",
            "",
            "\tenqueue_dl_entity(&p->dl, flags);",
            "",
            "\tif (dl_server(&p->dl))",
            "\t\treturn;",
            "",
            "\tif (!task_current(rq, p) && !p->dl.dl_throttled && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_dl_task(rq, p);",
            "}",
            "static bool dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tupdate_curr_dl(rq);",
            "",
            "\tif (p->on_rq == TASK_ON_RQ_MIGRATING)",
            "\t\tflags |= DEQUEUE_MIGRATING;",
            "",
            "\tdequeue_dl_entity(&p->dl, flags);",
            "\tif (!p->dl.dl_throttled && !dl_server(&p->dl))",
            "\t\tdequeue_pushable_dl_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void yield_task_dl(struct rq *rq)",
            "{",
            "\t/*",
            "\t * We make the task go to sleep until its current deadline by",
            "\t * forcing its runtime to zero. This way, update_curr_dl() stops",
            "\t * it and the bandwidth timer will wake it up and will give it",
            "\t * new scheduling parameters (thanks to dl_yielded=1).",
            "\t */",
            "\trq->curr->dl.dl_yielded = 1;",
            "",
            "\tupdate_rq_clock(rq);",
            "\tupdate_curr_dl(rq);",
            "\t/*",
            "\t * Tell update_rq_clock() that we've just updated,",
            "\t * so we don't do microscopic update in schedule()",
            "\t * and double the fastpath cost.",
            "\t */",
            "\trq_clock_skip_update(rq);",
            "}",
            "static inline bool dl_task_is_earliest_deadline(struct task_struct *p,",
            "\t\t\t\t\t\t struct rq *rq)",
            "{",
            "\treturn (!rq->dl.dl_nr_running ||",
            "\t\tdl_time_before(p->dl.deadline,",
            "\t\t\t       rq->dl.earliest_dl.curr));",
            "}",
            "static int",
            "select_task_rq_dl(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tbool select_rq;",
            "\tstruct rq *rq;",
            "",
            "\tif (!(flags & WF_TTWU))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If we are dealing with a -deadline task, we must",
            "\t * decide where to wake it up.",
            "\t * If it has a later deadline and the current task",
            "\t * on this rq can't move (provided the waking task",
            "\t * can!) we prefer to send it somewhere else. On the",
            "\t * other hand, if it has a shorter deadline, we",
            "\t * try to make it stay here, it might be important.",
            "\t */",
            "\tselect_rq = unlikely(dl_task(curr)) &&",
            "\t\t    (curr->nr_cpus_allowed < 2 ||",
            "\t\t     !dl_entity_preempt(&p->dl, &curr->dl)) &&",
            "\t\t    p->nr_cpus_allowed > 1;",
            "",
            "\t/*",
            "\t * Take the capacity of the CPU into account to",
            "\t * ensure it fits the requirement of the task.",
            "\t */",
            "\tif (sched_asym_cpucap_active())",
            "\t\tselect_rq |= !dl_task_fits_capacity(p, cpu);",
            "",
            "\tif (select_rq) {",
            "\t\tint target = find_later_rq(p);",
            "",
            "\t\tif (target != -1 &&",
            "\t\t    dl_task_is_earliest_deadline(p, cpu_rq(target)))",
            "\t\t\tcpu = target;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "enqueue_task_dl, dequeue_task_dl, yield_task_dl, dl_task_is_earliest_deadline, select_task_rq_dl",
          "description": "处理截止时间任务的调度决策，包含任务入队出队、抢占检查、CPU选择及负载均衡逻辑，通过dl_task_is_earliest_deadline判断任务截止时间优先级并选择合适CPU",
          "similarity": 0.6281309723854065
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 48,
          "end_line": 151,
          "content": [
            "static int __init sched_dl_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", sched_dl_sysctls);",
            "\treturn 0;",
            "}",
            "static bool dl_server(struct sched_dl_entity *dl_se)",
            "{",
            "\treturn dl_se->dl_server;",
            "}",
            "static inline int on_dl_rq(struct sched_dl_entity *dl_se)",
            "{",
            "\treturn !RB_EMPTY_NODE(&dl_se->rb_node);",
            "}",
            "static inline bool is_dl_boosted(struct sched_dl_entity *dl_se)",
            "{",
            "\treturn pi_of(dl_se) != dl_se;",
            "}",
            "static inline bool is_dl_boosted(struct sched_dl_entity *dl_se)",
            "{",
            "\treturn false;",
            "}",
            "static inline int dl_bw_cpus(int i)",
            "{",
            "\tstruct root_domain *rd = cpu_rq(i)->rd;",
            "\tint cpus;",
            "",
            "\tRCU_LOCKDEP_WARN(!rcu_read_lock_sched_held(),",
            "\t\t\t \"sched RCU must be held\");",
            "",
            "\tif (cpumask_subset(rd->span, cpu_active_mask))",
            "\t\treturn cpumask_weight(rd->span);",
            "",
            "\tcpus = 0;",
            "",
            "\tfor_each_cpu_and(i, rd->span, cpu_active_mask)",
            "\t\tcpus++;",
            "",
            "\treturn cpus;",
            "}",
            "static inline unsigned long __dl_bw_capacity(const struct cpumask *mask)",
            "{",
            "\tunsigned long cap = 0;",
            "\tint i;",
            "",
            "\tfor_each_cpu_and(i, mask, cpu_active_mask)",
            "\t\tcap += arch_scale_cpu_capacity(i);",
            "",
            "\treturn cap;",
            "}",
            "static inline unsigned long dl_bw_capacity(int i)",
            "{",
            "\tif (!sched_asym_cpucap_active() &&",
            "\t    arch_scale_cpu_capacity(i) == SCHED_CAPACITY_SCALE) {",
            "\t\treturn dl_bw_cpus(i) << SCHED_CAPACITY_SHIFT;",
            "\t} else {",
            "\t\tRCU_LOCKDEP_WARN(!rcu_read_lock_sched_held(),",
            "\t\t\t\t \"sched RCU must be held\");",
            "",
            "\t\treturn __dl_bw_capacity(cpu_rq(i)->rd->span);",
            "\t}",
            "}",
            "static inline bool dl_bw_visited(int cpu, u64 gen)",
            "{",
            "\tstruct root_domain *rd = cpu_rq(cpu)->rd;",
            "",
            "\tif (rd->visit_gen == gen)",
            "\t\treturn true;",
            "",
            "\trd->visit_gen = gen;",
            "\treturn false;",
            "}",
            "static inline",
            "void __dl_update(struct dl_bw *dl_b, s64 bw)",
            "{",
            "\tstruct root_domain *rd = container_of(dl_b, struct root_domain, dl_bw);",
            "\tint i;",
            "",
            "\tRCU_LOCKDEP_WARN(!rcu_read_lock_sched_held(),",
            "\t\t\t \"sched RCU must be held\");",
            "\tfor_each_cpu_and(i, rd->span, cpu_active_mask) {",
            "\t\tstruct rq *rq = cpu_rq(i);",
            "",
            "\t\trq->dl.extra_bw += bw;",
            "\t}",
            "}",
            "static inline int dl_bw_cpus(int i)",
            "{",
            "\treturn 1;",
            "}",
            "static inline unsigned long dl_bw_capacity(int i)",
            "{",
            "\treturn SCHED_CAPACITY_SCALE;",
            "}",
            "static inline bool dl_bw_visited(int cpu, u64 gen)",
            "{",
            "\treturn false;",
            "}",
            "static inline",
            "void __dl_update(struct dl_bw *dl_b, s64 bw)",
            "{",
            "\tstruct dl_rq *dl = container_of(dl_b, struct dl_rq, dl_bw);",
            "",
            "\tdl->extra_bw += bw;",
            "}"
          ],
          "function_name": "sched_dl_sysctl_init, dl_server, on_dl_rq, is_dl_boosted, is_dl_boosted, dl_bw_cpus, __dl_bw_capacity, dl_bw_capacity, dl_bw_visited, __dl_update, dl_bw_cpus, dl_bw_capacity, dl_bw_visited, __dl_update",
          "description": "提供与截止时间调度相关的辅助函数，包括判断任务属性、计算带宽、管理资源容量及更新带宽分配的操作。",
          "similarity": 0.6027393341064453
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 224,
          "end_line": 332,
          "content": [
            "static inline",
            "void __dl_sub(struct dl_bw *dl_b, u64 tsk_bw, int cpus)",
            "{",
            "\tdl_b->total_bw -= tsk_bw;",
            "\t__dl_update(dl_b, (s32)tsk_bw / cpus);",
            "}",
            "static inline",
            "void __dl_add(struct dl_bw *dl_b, u64 tsk_bw, int cpus)",
            "{",
            "\tdl_b->total_bw += tsk_bw;",
            "\t__dl_update(dl_b, -((s32)tsk_bw / cpus));",
            "}",
            "static inline bool",
            "__dl_overflow(struct dl_bw *dl_b, unsigned long cap, u64 old_bw, u64 new_bw)",
            "{",
            "\treturn dl_b->bw != -1 &&",
            "\t       cap_scale(dl_b->bw, cap) < dl_b->total_bw - old_bw + new_bw;",
            "}",
            "static inline",
            "void __add_running_bw(u64 dl_bw, struct dl_rq *dl_rq)",
            "{",
            "\tu64 old = dl_rq->running_bw;",
            "",
            "\tlockdep_assert_rq_held(rq_of_dl_rq(dl_rq));",
            "\tdl_rq->running_bw += dl_bw;",
            "\tSCHED_WARN_ON(dl_rq->running_bw < old); /* overflow */",
            "\tSCHED_WARN_ON(dl_rq->running_bw > dl_rq->this_bw);",
            "\t/* kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\tcpufreq_update_util(rq_of_dl_rq(dl_rq), 0);",
            "}",
            "static inline",
            "void __sub_running_bw(u64 dl_bw, struct dl_rq *dl_rq)",
            "{",
            "\tu64 old = dl_rq->running_bw;",
            "",
            "\tlockdep_assert_rq_held(rq_of_dl_rq(dl_rq));",
            "\tdl_rq->running_bw -= dl_bw;",
            "\tSCHED_WARN_ON(dl_rq->running_bw > old); /* underflow */",
            "\tif (dl_rq->running_bw > old)",
            "\t\tdl_rq->running_bw = 0;",
            "\t/* kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\tcpufreq_update_util(rq_of_dl_rq(dl_rq), 0);",
            "}",
            "static inline",
            "void __add_rq_bw(u64 dl_bw, struct dl_rq *dl_rq)",
            "{",
            "\tu64 old = dl_rq->this_bw;",
            "",
            "\tlockdep_assert_rq_held(rq_of_dl_rq(dl_rq));",
            "\tdl_rq->this_bw += dl_bw;",
            "\tSCHED_WARN_ON(dl_rq->this_bw < old); /* overflow */",
            "}",
            "static inline",
            "void __sub_rq_bw(u64 dl_bw, struct dl_rq *dl_rq)",
            "{",
            "\tu64 old = dl_rq->this_bw;",
            "",
            "\tlockdep_assert_rq_held(rq_of_dl_rq(dl_rq));",
            "\tdl_rq->this_bw -= dl_bw;",
            "\tSCHED_WARN_ON(dl_rq->this_bw > old); /* underflow */",
            "\tif (dl_rq->this_bw > old)",
            "\t\tdl_rq->this_bw = 0;",
            "\tSCHED_WARN_ON(dl_rq->running_bw > dl_rq->this_bw);",
            "}",
            "static inline",
            "void add_rq_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\tif (!dl_entity_is_special(dl_se))",
            "\t\t__add_rq_bw(dl_se->dl_bw, dl_rq);",
            "}",
            "static inline",
            "void sub_rq_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\tif (!dl_entity_is_special(dl_se))",
            "\t\t__sub_rq_bw(dl_se->dl_bw, dl_rq);",
            "}",
            "static inline",
            "void add_running_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\tif (!dl_entity_is_special(dl_se))",
            "\t\t__add_running_bw(dl_se->dl_bw, dl_rq);",
            "}",
            "static inline",
            "void sub_running_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\tif (!dl_entity_is_special(dl_se))",
            "\t\t__sub_running_bw(dl_se->dl_bw, dl_rq);",
            "}",
            "static void dl_rq_change_utilization(struct rq *rq, struct sched_dl_entity *dl_se, u64 new_bw)",
            "{",
            "\tif (dl_se->dl_non_contending) {",
            "\t\tsub_running_bw(dl_se, &rq->dl);",
            "\t\tdl_se->dl_non_contending = 0;",
            "",
            "\t\t/*",
            "\t\t * If the timer handler is currently running and the",
            "\t\t * timer cannot be canceled, inactive_task_timer()",
            "\t\t * will see that dl_not_contending is not set, and",
            "\t\t * will not touch the rq's active utilization,",
            "\t\t * so we are still safe.",
            "\t\t */",
            "\t\tif (hrtimer_try_to_cancel(&dl_se->inactive_timer) == 1) {",
            "\t\t\tif (!dl_server(dl_se))",
            "\t\t\t\tput_task_struct(dl_task_of(dl_se));",
            "\t\t}",
            "\t}",
            "\t__sub_rq_bw(dl_se->dl_bw, &rq->dl);",
            "\t__add_rq_bw(new_bw, &rq->dl);",
            "}"
          ],
          "function_name": "__dl_sub, __dl_add, __dl_overflow, __add_running_bw, __sub_running_bw, __add_rq_bw, __sub_rq_bw, add_rq_bw, sub_rq_bw, add_running_bw, sub_running_bw, dl_rq_change_utilization",
          "description": "实现截止时间任务带宽的增减操作，包含溢出检测逻辑，用于动态调整运行时带宽和资源使用量。",
          "similarity": 0.5617645382881165
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 1741,
          "end_line": 1857,
          "content": [
            "static void update_curr_dl(struct rq *rq)",
            "{",
            "\tstruct task_struct *curr = rq->curr;",
            "\tstruct sched_dl_entity *dl_se = &curr->dl;",
            "\ts64 delta_exec;",
            "",
            "\tif (!dl_task(curr) || !on_dl_rq(dl_se))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Consumed budget is computed considering the time as",
            "\t * observed by schedulable tasks (excluding time spent",
            "\t * in hardirq context, etc.). Deadlines are instead",
            "\t * computed using hard walltime. This seems to be the more",
            "\t * natural solution, but the full ramifications of this",
            "\t * approach need further study.",
            "\t */",
            "\tdelta_exec = update_curr_common(rq);",
            "\tupdate_curr_dl_se(rq, dl_se, delta_exec);",
            "}",
            "static enum hrtimer_restart inactive_task_timer(struct hrtimer *timer)",
            "{",
            "\tstruct sched_dl_entity *dl_se = container_of(timer,",
            "\t\t\t\t\t\t     struct sched_dl_entity,",
            "\t\t\t\t\t\t     inactive_timer);",
            "\tstruct task_struct *p = NULL;",
            "\tstruct rq_flags rf;",
            "\tstruct rq *rq;",
            "",
            "\tif (!dl_server(dl_se)) {",
            "\t\tp = dl_task_of(dl_se);",
            "\t\trq = task_rq_lock(p, &rf);",
            "\t} else {",
            "\t\trq = dl_se->rq;",
            "\t\trq_lock(rq, &rf);",
            "\t}",
            "",
            "\tsched_clock_tick();",
            "\tupdate_rq_clock(rq);",
            "",
            "\tif (dl_server(dl_se))",
            "\t\tgoto no_task;",
            "",
            "\tif (!dl_task(p) || READ_ONCE(p->__state) == TASK_DEAD) {",
            "\t\tstruct dl_bw *dl_b = dl_bw_of(task_cpu(p));",
            "",
            "\t\tif (READ_ONCE(p->__state) == TASK_DEAD && dl_se->dl_non_contending) {",
            "\t\t\tsub_running_bw(&p->dl, dl_rq_of_se(&p->dl));",
            "\t\t\tsub_rq_bw(&p->dl, dl_rq_of_se(&p->dl));",
            "\t\t\tdl_se->dl_non_contending = 0;",
            "\t\t}",
            "",
            "\t\traw_spin_lock(&dl_b->lock);",
            "\t\t__dl_sub(dl_b, p->dl.dl_bw, dl_bw_cpus(task_cpu(p)));",
            "\t\traw_spin_unlock(&dl_b->lock);",
            "\t\t__dl_clear_params(dl_se);",
            "",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "no_task:",
            "\tif (dl_se->dl_non_contending == 0)",
            "\t\tgoto unlock;",
            "",
            "\tsub_running_bw(dl_se, &rq->dl);",
            "\tdl_se->dl_non_contending = 0;",
            "unlock:",
            "",
            "\tif (!dl_server(dl_se)) {",
            "\t\ttask_rq_unlock(rq, p, &rf);",
            "\t\tput_task_struct(p);",
            "\t} else {",
            "\t\trq_unlock(rq, &rf);",
            "\t}",
            "",
            "\treturn HRTIMER_NORESTART;",
            "}",
            "static void init_dl_inactive_task_timer(struct sched_dl_entity *dl_se)",
            "{",
            "\tstruct hrtimer *timer = &dl_se->inactive_timer;",
            "",
            "\thrtimer_init(timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);",
            "\ttimer->function = inactive_task_timer;",
            "}",
            "static void inc_dl_deadline(struct dl_rq *dl_rq, u64 deadline)",
            "{",
            "\tstruct rq *rq = rq_of_dl_rq(dl_rq);",
            "",
            "\tif (dl_rq->earliest_dl.curr == 0 ||",
            "\t    dl_time_before(deadline, dl_rq->earliest_dl.curr)) {",
            "\t\tif (dl_rq->earliest_dl.curr == 0)",
            "\t\t\tcpupri_set(&rq->rd->cpupri, rq->cpu, CPUPRI_HIGHER);",
            "\t\tdl_rq->earliest_dl.curr = deadline;",
            "\t\tcpudl_set(&rq->rd->cpudl, rq->cpu, deadline);",
            "\t}",
            "}",
            "static void dec_dl_deadline(struct dl_rq *dl_rq, u64 deadline)",
            "{",
            "\tstruct rq *rq = rq_of_dl_rq(dl_rq);",
            "",
            "\t/*",
            "\t * Since we may have removed our earliest (and/or next earliest)",
            "\t * task we must recompute them.",
            "\t */",
            "\tif (!dl_rq->dl_nr_running) {",
            "\t\tdl_rq->earliest_dl.curr = 0;",
            "\t\tdl_rq->earliest_dl.next = 0;",
            "\t\tcpudl_clear(&rq->rd->cpudl, rq->cpu);",
            "\t\tcpupri_set(&rq->rd->cpupri, rq->cpu, rq->rt.highest_prio.curr);",
            "\t} else {",
            "\t\tstruct rb_node *leftmost = rb_first_cached(&dl_rq->root);",
            "\t\tstruct sched_dl_entity *entry = __node_2_dle(leftmost);",
            "",
            "\t\tdl_rq->earliest_dl.curr = entry->deadline;",
            "\t\tcpudl_set(&rq->rd->cpudl, rq->cpu, entry->deadline);",
            "\t}",
            "}"
          ],
          "function_name": "update_curr_dl, inactive_task_timer, init_dl_inactive_task_timer, inc_dl_deadline, dec_dl_deadline",
          "description": "实现截止时间调度器的核心逻辑，包含update_curr_dl更新当前任务执行时间，inactive_task_timer处理非竞争任务的定时器逻辑，init_dl_inactive_task_timer初始化定时器，inc_dl_deadline和dec_dl_deadline维护截止时间队列的最早截止时间",
          "similarity": 0.5564900636672974
        }
      ]
    }
  ]
}