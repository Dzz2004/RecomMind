{
  "query": "elevator algorithm",
  "timestamp": "2025-12-25 23:41:22",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/cpupri.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:04:45\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\cpupri.c`\n\n---\n\n# `sched/cpupri.c` 技术文档\n\n## 1. 文件概述\n\n`sched/cpupri.c` 实现了 **CPU 优先级管理（CPU Priority Management）** 机制，用于实时任务（RT tasks）的全局负载均衡和迁移决策。该机制通过维护一个二维位图结构，快速追踪每个 CPU 当前运行任务的最高优先级，从而在 O(1) 时间复杂度内为新唤醒或迁移的实时任务找到合适的 CPU 目标。该机制特别优化了无 CPU 亲和性限制的任务调度路径，同时支持带亲和性约束的场景。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `convert_prio(int prio)` | 将任务的调度优先级（`p->prio`）转换为内部 `cpupri` 优先级值（范围：-1 到 100） |\n| `__cpupri_find(struct cpupri *cp, struct task_struct *p, struct cpumask *lowest_mask, int idx)` | 在指定优先级层级 `idx` 中查找满足任务 `p` 的 CPU（考虑亲和性） |\n| `cpupri_find(struct cpupri *cp, struct task_struct *p, struct cpumask *lowest_mask)` | 查找系统中优先级 **低于或等于** 任务 `p` 的 CPU（即任务可运行的 CPU） |\n| `cpupri_find_fitness(...)` | 增强版查找函数，支持通过 `fitness_fn` 自定义 CPU 适配条件（如容量感知） |\n| `cpupri_set(struct cpupri *cp, int cpu, int newpri)` | 更新指定 CPU 的当前最高优先级状态 |\n| `cpupri_init(struct cpupri *cp)` | 初始化 `cpupri` 数据结构（声明但未在片段中实现） |\n\n### 关键数据结构（隐含）\n\n- `struct cpupri`：全局 CPU 优先级管理上下文\n  - `cpu_to_pri[]`：每个 CPU 当前的 `cpupri` 优先级\n  - `pri_to_cpu[]`：每个优先级对应的 `struct cpupri_vec`\n- `struct cpupri_vec`：\n  - `mask`：该优先级下所有 CPU 的位图\n  - `count`：该优先级下活跃 CPU 的数量（原子计数）\n\n### 优先级映射关系\n\n| 任务 `p->prio` | `cpupri` 值 | 含义 |\n|---------------|------------|------|\n| -1 | -1 (`CPUPRI_INVALID`) | 无效状态（CPU 不可调度） |\n| 0–98 | 99–1 | 实时优先级（数值越大，任务优先级越高，`cpupri` 值越小） |\n| 99 (`MAX_RT_PRIO-1`) | 0 (`CPUPRI_NORMAL`) | 普通（非实时）任务 |\n| 100 (`MAX_RT_PRIO`) | 100 (`CPUPRI_HIGHER`) | 高于所有 RT 任务的特殊优先级 |\n\n> **注意**：`cpupri` 值越小，表示 CPU 当前负载的优先级 **越高**。\n\n## 3. 关键实现\n\n### 优先级转换逻辑\n- `convert_prio()` 实现了任务调度优先级到 `cpupri` 内部表示的映射，确保实时任务（`p->prio` ∈ [0, 98]）被正确映射到 `cpupri` ∈ [1, 99]，且高优先级任务对应更小的 `cpupri` 值。\n\n### 快速查找算法\n- 使用 **二维位图**：第一维为优先级（0–100），第二维为 CPU 位图。\n- `cpupri_find_fitness()` 从最低优先级（`idx = 0`）开始遍历，找到第一个存在可用 CPU 的优先级层级。\n- 对于每个层级，通过 `cpumask_any_and()` 快速判断任务亲和性掩码与该优先级 CPU 掩码是否有交集。\n- 若提供 `fitness_fn`（如容量检查），会过滤掉不满足条件的 CPU；若过滤后无 CPU 可用，则继续搜索更高优先级层级。\n\n### 容错与回退策略\n- 如果启用了 `fitness_fn` 但未找到满足条件的 CPU，函数会 **忽略 fitness 条件重新搜索**，确保高优先级任务总能找到运行 CPU（优先保证实时性，而非最优资源匹配）。\n\n### 并发安全更新\n- `cpupri_set()` 使用 **内存屏障（`smp_mb__before/after_atomic()`）** 确保 CPU 位图和计数器的更新顺序：\n  1. **添加 CPU**：先设置位图 → 内存屏障 → 增加计数器\n  2. **移除 CPU**：先减少计数器 → 内存屏障 → 清除位图\n- 此顺序防止 `cpupri_find` 在并发读取时看到不一致状态（如计数器为 0 但位图仍置位）。\n\n### 亲和性处理\n- 所有查找操作均与任务的 `p->cpus_mask`（CPU 亲和性）和 `cpu_active_mask`（活跃 CPU）进行交集运算，确保只返回合法 CPU。\n\n## 4. 依赖关系\n\n- **调度器核心**：依赖 `task_struct`、`p->prio`、`p->cpus_mask` 等调度器基本结构。\n- **实时调度类（`rt.c`）**：`cpupri` 主要服务于 `SCHED_FIFO`/`SCHED_RR` 任务的负载均衡。\n- **CPU 掩码操作**：使用 `cpumask_*` 系列函数（如 `cpumask_and`, `cpumask_clear_cpu`）。\n- **内存屏障原语**：依赖 `smp_rmb()`、`smp_mb__before_atomic()` 等 SMP 同步机制。\n- **原子操作**：使用 `atomic_read/inc/dec` 管理优先级层级的 CPU 计数。\n\n## 5. 使用场景\n\n- **实时任务唤醒/迁移**：当高优先级 RT 任务被唤醒或需要迁移时，调用 `cpupri_find()` 快速定位可运行的最低优先级 CPU（减少抢占开销）。\n- **全局负载均衡**：RT 调度器的 `push_rt_task()` 和 `pull_rt_task()` 机制利用 `cpupri` 决定任务推送/拉取的目标 CPU。\n- **容量感知调度（Capacity Awareness）**：通过 `cpupri_find_fitness()` 的 `fitness_fn` 参数，集成 CPU 性能/能效信息（如 ARM big.LITTLE 架构），在满足优先级前提下选择合适 CPU。\n- **CPU 热插拔**：CPU 上下线时通过 `cpupri_set()` 更新其优先级状态（设为 `CPUPRI_INVALID` 或恢复）。",
      "similarity": 0.4694293737411499,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 1,
          "end_line": 41,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  kernel/sched/cpupri.c",
            " *",
            " *  CPU priority management",
            " *",
            " *  Copyright (C) 2007-2008 Novell",
            " *",
            " *  Author: Gregory Haskins <ghaskins@novell.com>",
            " *",
            " *  This code tracks the priority of each CPU so that global migration",
            " *  decisions are easy to calculate.  Each CPU can be in a state as follows:",
            " *",
            " *                 (INVALID), NORMAL, RT1, ... RT99, HIGHER",
            " *",
            " *  going from the lowest priority to the highest.  CPUs in the INVALID state",
            " *  are not eligible for routing.  The system maintains this state with",
            " *  a 2 dimensional bitmap (the first for priority class, the second for CPUs",
            " *  in that class).  Therefore a typical application without affinity",
            " *  restrictions can find a suitable CPU with O(1) complexity (e.g. two bit",
            " *  searches).  For tasks with affinity restrictions, the algorithm has a",
            " *  worst case complexity of O(min(101, nr_domcpus)), though the scenario that",
            " *  yields the worst case search is fairly contrived.",
            " */",
            "",
            "/*",
            " * p->rt_priority   p->prio   newpri   cpupri",
            " *",
            " *\t\t\t\t  -1       -1 (CPUPRI_INVALID)",
            " *",
            " *\t\t\t\t  99        0 (CPUPRI_NORMAL)",
            " *",
            " *\t\t1        98       98        1",
            " *\t      ...",
            " *\t       49        50       50       49",
            " *\t       50        49       49       50",
            " *\t      ...",
            " *\t       99         0        0       99",
            " *",
            " *\t\t\t\t 100\t  100 (CPUPRI_HIGHER)",
            " */"
          ],
          "function_name": null,
          "description": "定义CPU优先级管理模块，通过二维位图跟踪各CPU优先级状态，支持NORMAL、RT1至RT99及HIGHER五种优先级分类，INVALID状态表示CPU不可用，用于全局任务调度时快速计算迁移决策",
          "similarity": 0.4490000605583191
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 42,
          "end_line": 178,
          "content": [
            "static int convert_prio(int prio)",
            "{",
            "\tint cpupri;",
            "",
            "\tswitch (prio) {",
            "\tcase CPUPRI_INVALID:",
            "\t\tcpupri = CPUPRI_INVALID;\t/* -1 */",
            "\t\tbreak;",
            "",
            "\tcase 0 ... 98:",
            "\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */",
            "\t\tbreak;",
            "",
            "\tcase MAX_RT_PRIO-1:",
            "\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */",
            "\t\tbreak;",
            "",
            "\tcase MAX_RT_PRIO:",
            "\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn cpupri;",
            "}",
            "static inline int __cpupri_find(struct cpupri *cp, struct task_struct *p,",
            "\t\t\t\tstruct cpumask *lowest_mask, int idx)",
            "{",
            "\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[idx];",
            "\tint skip = 0;",
            "",
            "\tif (!atomic_read(&(vec)->count))",
            "\t\tskip = 1;",
            "\t/*",
            "\t * When looking at the vector, we need to read the counter,",
            "\t * do a memory barrier, then read the mask.",
            "\t *",
            "\t * Note: This is still all racy, but we can deal with it.",
            "\t *  Ideally, we only want to look at masks that are set.",
            "\t *",
            "\t *  If a mask is not set, then the only thing wrong is that we",
            "\t *  did a little more work than necessary.",
            "\t *",
            "\t *  If we read a zero count but the mask is set, because of the",
            "\t *  memory barriers, that can only happen when the highest prio",
            "\t *  task for a run queue has left the run queue, in which case,",
            "\t *  it will be followed by a pull. If the task we are processing",
            "\t *  fails to find a proper place to go, that pull request will",
            "\t *  pull this task if the run queue is running at a lower",
            "\t *  priority.",
            "\t */",
            "\tsmp_rmb();",
            "",
            "\t/* Need to do the rmb for every iteration */",
            "\tif (skip)",
            "\t\treturn 0;",
            "",
            "\tif (cpumask_any_and(&p->cpus_mask, vec->mask) >= nr_cpu_ids)",
            "\t\treturn 0;",
            "",
            "\tif (lowest_mask) {",
            "\t\tcpumask_and(lowest_mask, &p->cpus_mask, vec->mask);",
            "\t\tcpumask_and(lowest_mask, lowest_mask, cpu_active_mask);",
            "",
            "\t\t/*",
            "\t\t * We have to ensure that we have at least one bit",
            "\t\t * still set in the array, since the map could have",
            "\t\t * been concurrently emptied between the first and",
            "\t\t * second reads of vec->mask.  If we hit this",
            "\t\t * condition, simply act as though we never hit this",
            "\t\t * priority level and continue on.",
            "\t\t */",
            "\t\tif (cpumask_empty(lowest_mask))",
            "\t\t\treturn 0;",
            "\t}",
            "",
            "\treturn 1;",
            "}",
            "int cpupri_find(struct cpupri *cp, struct task_struct *p,",
            "\t\tstruct cpumask *lowest_mask)",
            "{",
            "\treturn cpupri_find_fitness(cp, p, lowest_mask, NULL);",
            "}",
            "int cpupri_find_fitness(struct cpupri *cp, struct task_struct *p,",
            "\t\tstruct cpumask *lowest_mask,",
            "\t\tbool (*fitness_fn)(struct task_struct *p, int cpu))",
            "{",
            "\tint task_pri = convert_prio(p->prio);",
            "\tint idx, cpu;",
            "",
            "\tWARN_ON_ONCE(task_pri >= CPUPRI_NR_PRIORITIES);",
            "",
            "\tfor (idx = 0; idx < task_pri; idx++) {",
            "",
            "\t\tif (!__cpupri_find(cp, p, lowest_mask, idx))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!lowest_mask || !fitness_fn)",
            "\t\t\treturn 1;",
            "",
            "\t\t/* Ensure the capacity of the CPUs fit the task */",
            "\t\tfor_each_cpu(cpu, lowest_mask) {",
            "\t\t\tif (!fitness_fn(p, cpu))",
            "\t\t\t\tcpumask_clear_cpu(cpu, lowest_mask);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If no CPU at the current priority can fit the task",
            "\t\t * continue looking",
            "\t\t */",
            "\t\tif (cpumask_empty(lowest_mask))",
            "\t\t\tcontinue;",
            "",
            "\t\treturn 1;",
            "\t}",
            "",
            "\t/*",
            "\t * If we failed to find a fitting lowest_mask, kick off a new search",
            "\t * but without taking into account any fitness criteria this time.",
            "\t *",
            "\t * This rule favours honouring priority over fitting the task in the",
            "\t * correct CPU (Capacity Awareness being the only user now).",
            "\t * The idea is that if a higher priority task can run, then it should",
            "\t * run even if this ends up being on unfitting CPU.",
            "\t *",
            "\t * The cost of this trade-off is not entirely clear and will probably",
            "\t * be good for some workloads and bad for others.",
            "\t *",
            "\t * The main idea here is that if some CPUs were over-committed, we try",
            "\t * to spread which is what the scheduler traditionally did. Sys admins",
            "\t * must do proper RT planning to avoid overloading the system if they",
            "\t * really care.",
            "\t */",
            "\tif (fitness_fn)",
            "\t\treturn cpupri_find(cp, p, lowest_mask);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "convert_prio, __cpupri_find, cpupri_find, cpupri_find_fitness",
          "description": "convert_prio将任务优先级映射为CPU优先级数值；__cpupri_find检查特定优先级下是否存在可用CPU；cpupri_find_fitness遍历优先级层级寻找适配CPU，结合适应性判断与优先级策略决定最终选择",
          "similarity": 0.44843196868896484
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 210,
          "end_line": 304,
          "content": [
            "void cpupri_set(struct cpupri *cp, int cpu, int newpri)",
            "{",
            "\tint *currpri = &cp->cpu_to_pri[cpu];",
            "\tint oldpri = *currpri;",
            "\tint do_mb = 0;",
            "",
            "\tnewpri = convert_prio(newpri);",
            "",
            "\tBUG_ON(newpri >= CPUPRI_NR_PRIORITIES);",
            "",
            "\tif (newpri == oldpri)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If the CPU was currently mapped to a different value, we",
            "\t * need to map it to the new value then remove the old value.",
            "\t * Note, we must add the new value first, otherwise we risk the",
            "\t * cpu being missed by the priority loop in cpupri_find.",
            "\t */",
            "\tif (likely(newpri != CPUPRI_INVALID)) {",
            "\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[newpri];",
            "",
            "\t\tcpumask_set_cpu(cpu, vec->mask);",
            "\t\t/*",
            "\t\t * When adding a new vector, we update the mask first,",
            "\t\t * do a write memory barrier, and then update the count, to",
            "\t\t * make sure the vector is visible when count is set.",
            "\t\t */",
            "\t\tsmp_mb__before_atomic();",
            "\t\tatomic_inc(&(vec)->count);",
            "\t\tdo_mb = 1;",
            "\t}",
            "\tif (likely(oldpri != CPUPRI_INVALID)) {",
            "\t\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[oldpri];",
            "",
            "\t\t/*",
            "\t\t * Because the order of modification of the vec->count",
            "\t\t * is important, we must make sure that the update",
            "\t\t * of the new prio is seen before we decrement the",
            "\t\t * old prio. This makes sure that the loop sees",
            "\t\t * one or the other when we raise the priority of",
            "\t\t * the run queue. We don't care about when we lower the",
            "\t\t * priority, as that will trigger an rt pull anyway.",
            "\t\t *",
            "\t\t * We only need to do a memory barrier if we updated",
            "\t\t * the new priority vec.",
            "\t\t */",
            "\t\tif (do_mb)",
            "\t\t\tsmp_mb__after_atomic();",
            "",
            "\t\t/*",
            "\t\t * When removing from the vector, we decrement the counter first",
            "\t\t * do a memory barrier and then clear the mask.",
            "\t\t */",
            "\t\tatomic_dec(&(vec)->count);",
            "\t\tsmp_mb__after_atomic();",
            "\t\tcpumask_clear_cpu(cpu, vec->mask);",
            "\t}",
            "",
            "\t*currpri = newpri;",
            "}",
            "int cpupri_init(struct cpupri *cp)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++) {",
            "\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[i];",
            "",
            "\t\tatomic_set(&vec->count, 0);",
            "\t\tif (!zalloc_cpumask_var(&vec->mask, GFP_KERNEL))",
            "\t\t\tgoto cleanup;",
            "\t}",
            "",
            "\tcp->cpu_to_pri = kcalloc(nr_cpu_ids, sizeof(int), GFP_KERNEL);",
            "\tif (!cp->cpu_to_pri)",
            "\t\tgoto cleanup;",
            "",
            "\tfor_each_possible_cpu(i)",
            "\t\tcp->cpu_to_pri[i] = CPUPRI_INVALID;",
            "",
            "\treturn 0;",
            "",
            "cleanup:",
            "\tfor (i--; i >= 0; i--)",
            "\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);",
            "\treturn -ENOMEM;",
            "}",
            "void cpupri_cleanup(struct cpupri *cp)",
            "{",
            "\tint i;",
            "",
            "\tkfree(cp->cpu_to_pri);",
            "\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++)",
            "\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);",
            "}"
          ],
          "function_name": "cpupri_set, cpupri_init, cpupri_cleanup",
          "description": "cpupri_set更新CPU优先级状态，通过原子操作同步位图与计数器；cpupri_init初始化优先级到CPU的映射表与CPU到优先级的数组；cpupri_cleanup释放所有动态分配的位图资源与优先级数组",
          "similarity": 0.4075707197189331
        }
      ]
    },
    {
      "source_file": "mm/balloon_compaction.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:41:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `balloon_compaction.c`\n\n---\n\n# balloon_compaction.c 技术文档\n\n## 1. 文件概述\n\n`balloon_compaction.c` 是 Linux 内核中用于支持内存气球（Memory Ballooning）机制与内存压缩（Compaction）协同工作的核心模块。该文件提供了通用接口，使得由气球驱动程序管理的页面可以被内存压缩子系统识别为可迁移（movable），从而在内存碎片整理过程中安全地移动这些页面，提升高阶内存分配的成功率。此机制主要用于虚拟化环境中，允许宿主机动态调整客户机（Guest）的可用内存。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`balloon_page_alloc()`**  \n  分配一个新的页面，专用于加入气球页面列表。使用特殊的 GFP 标志（如 `__GFP_NOMEMALLOC`, `__GFP_NORETRY`, `__GFP_NOWARN`）以避免在内存压力下触发 OOM 或重试。\n\n- **`balloon_page_enqueue()`**  \n  将单个通过 `balloon_page_alloc()` 分配的页面插入到指定气球设备的页面列表中，并增加 `BALLOON_INFLATE` 统计计数。\n\n- **`balloon_page_list_enqueue()`**  \n  批量将一个页面链表中的所有页面插入到气球设备的页面列表中，适用于高效批量操作。\n\n- **`balloon_page_dequeue()`**  \n  从气球设备的页面列表中移除并返回一个页面，供驱动释放回系统。若无法出队且无孤立页面，则触发 `BUG()` 防止死循环。\n\n- **`balloon_page_list_dequeue()`**  \n  批量从气球设备中取出最多 `n_req_pages` 个页面，放入调用者提供的链表中，用于批量释放。\n\n- **`balloon_page_isolate()`**（仅当 `CONFIG_BALLOON_COMPACTION` 启用）  \n  在内存压缩过程中，将气球页面从主列表中隔离，防止并发访问，并增加 `isolated_pages` 计数。\n\n- **`balloon_page_putback()`**（仅当 `CONFIG_BALLOON_COMPACTION` 启用）  \n  将被隔离的气球页面重新放回主页面列表，并减少 `isolated_pages` 计数。\n\n- **`balloon_page_migrate()`**（仅当 `CONFIG_BALLOON_COMPACTION` 启用）  \n  实现气球页面的迁移逻辑，作为内存压缩中 `move_to_new_page()` 的对应处理函数（代码片段未完整）。\n\n### 关键数据结构\n\n- **`struct balloon_dev_info`**  \n  气球设备信息结构体，包含：\n  - `pages`：已入队的气球页面链表\n  - `pages_lock`：保护页面列表的自旋锁\n  - `isolated_pages`：当前被压缩子系统隔离的页面数量（仅在 `CONFIG_BALLOON_COMPACTION` 下使用）\n\n## 3. 关键实现\n\n- **线程安全与并发控制**  \n  所有对 `balloon_dev_info->pages` 链表的操作均受 `pages_lock` 自旋锁保护，并在中断禁用上下文中执行（`spin_lock_irqsave`），确保在高并发或中断上下文中的安全性。\n\n- **页面锁定机制**  \n  在入队和出队时使用 `trylock_page()` 确保当前是唯一持有页面引用的实体。若加锁失败，说明存在并发访问，可能意味着内存损坏或状态不一致，此时会跳过或报错。\n\n- **与内存压缩集成**  \n  当启用 `CONFIG_BALLOON_COMPACTION` 时，气球页面可通过 `PageIsolated()` 标志被识别为正在被压缩子系统处理。出队操作会跳过这些页面，避免破坏压缩流程。\n\n- **统计计数**  \n  使用 `__count_vm_event(BALLOON_INFLATE)` 和 `__count_vm_event(BALLOON_DEFLATE)` 跟踪气球膨胀/收缩操作次数，便于性能监控和调试。\n\n- **错误检测与防御性编程**  \n  在 `balloon_page_dequeue()` 中，若页面列表为空且无孤立页面，说明页面丢失，触发 `BUG()` 以防止驱动陷入无限循环。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/mm.h>`：内存管理基础接口\n  - `<linux/slab.h>`：内存分配\n  - `<linux/balloon_compaction.h>`：气球压缩相关声明（如 `balloon_page_insert`, `balloon_page_delete`, `balloon_page_device` 等）\n\n- **内核配置依赖**：\n  - `CONFIG_MEMORY_BALLOONING`：启用内存气球机制\n  - `CONFIG_BALLOON_COMPACTION`：启用气球页面的可压缩支持（条件编译）\n\n- **与其他子系统交互**：\n  - **内存压缩子系统（mm/compaction.c）**：通过注册的 `isolate` / `migrate` 回调函数参与页面迁移\n  - **虚拟化驱动（如 virtio_balloon）**：作为使用者调用本模块提供的 enqueue/dequeue 接口管理气球内存\n\n## 5. 使用场景\n\n- **虚拟化环境中的内存动态调整**  \n  客户机操作系统通过气球驱动（如 `virtio_balloon`）向宿主机“归还”内存时，调用 `balloon_page_alloc()` + `balloon_page_enqueue()` 将页面加入气球列表；当宿主机释放内存给客户机时，驱动调用 `balloon_page_dequeue()` 获取页面并释放回 buddy allocator。\n\n- **高阶内存分配优化**  \n  当系统需要大块连续物理内存（如透明大页 THP）但存在碎片时，内存压缩子系统会尝试迁移可移动页面。气球页面因本模块支持而被视为可移动，从而被安全迁移，帮助形成连续内存区域。\n\n- **内存热插拔与 NUMA 迁移**  \n  在 NUMA 节点间迁移内存或热移除内存区域时，气球页面可被压缩机制迁移，提高操作成功率。\n\n- **OOM 避免与内存回收**  \n  气球机制本身是一种主动内存回收手段，配合压缩可进一步提升内存利用率，减少 OOM 发生概率。",
      "similarity": 0.46940964460372925,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/balloon_compaction.c",
          "start_line": 14,
          "end_line": 129,
          "content": [
            "static void balloon_page_enqueue_one(struct balloon_dev_info *b_dev_info,",
            "\t\t\t\t     struct page *page)",
            "{",
            "\t/*",
            "\t * Block others from accessing the 'page' when we get around to",
            "\t * establishing additional references. We should be the only one",
            "\t * holding a reference to the 'page' at this point. If we are not, then",
            "\t * memory corruption is possible and we should stop execution.",
            "\t */",
            "\tBUG_ON(!trylock_page(page));",
            "\tballoon_page_insert(b_dev_info, page);",
            "\tunlock_page(page);",
            "\t__count_vm_event(BALLOON_INFLATE);",
            "}",
            "size_t balloon_page_list_enqueue(struct balloon_dev_info *b_dev_info,",
            "\t\t\t\t struct list_head *pages)",
            "{",
            "\tstruct page *page, *tmp;",
            "\tunsigned long flags;",
            "\tsize_t n_pages = 0;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tlist_for_each_entry_safe(page, tmp, pages, lru) {",
            "\t\tlist_del(&page->lru);",
            "\t\tballoon_page_enqueue_one(b_dev_info, page);",
            "\t\tn_pages++;",
            "\t}",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "\treturn n_pages;",
            "}",
            "size_t balloon_page_list_dequeue(struct balloon_dev_info *b_dev_info,",
            "\t\t\t\t struct list_head *pages, size_t n_req_pages)",
            "{",
            "\tstruct page *page, *tmp;",
            "\tunsigned long flags;",
            "\tsize_t n_pages = 0;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tlist_for_each_entry_safe(page, tmp, &b_dev_info->pages, lru) {",
            "\t\tif (n_pages == n_req_pages)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * Block others from accessing the 'page' while we get around to",
            "\t\t * establishing additional references and preparing the 'page'",
            "\t\t * to be released by the balloon driver.",
            "\t\t */",
            "\t\tif (!trylock_page(page))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (IS_ENABLED(CONFIG_BALLOON_COMPACTION) &&",
            "\t\t    PageIsolated(page)) {",
            "\t\t\t/* raced with isolation */",
            "\t\t\tunlock_page(page);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tballoon_page_delete(page);",
            "\t\t__count_vm_event(BALLOON_DEFLATE);",
            "\t\tlist_add(&page->lru, pages);",
            "\t\tunlock_page(page);",
            "\t\tn_pages++;",
            "\t}",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "",
            "\treturn n_pages;",
            "}",
            "void balloon_page_enqueue(struct balloon_dev_info *b_dev_info,",
            "\t\t\t  struct page *page)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tballoon_page_enqueue_one(b_dev_info, page);",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "}",
            "static bool balloon_page_isolate(struct page *page, isolate_mode_t mode)",
            "",
            "{",
            "\tstruct balloon_dev_info *b_dev_info = balloon_page_device(page);",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tlist_del(&page->lru);",
            "\tb_dev_info->isolated_pages++;",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "",
            "\treturn true;",
            "}",
            "static void balloon_page_putback(struct page *page)",
            "{",
            "\tstruct balloon_dev_info *b_dev_info = balloon_page_device(page);",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&b_dev_info->pages_lock, flags);",
            "\tlist_add(&page->lru, &b_dev_info->pages);",
            "\tb_dev_info->isolated_pages--;",
            "\tspin_unlock_irqrestore(&b_dev_info->pages_lock, flags);",
            "}",
            "static int balloon_page_migrate(struct page *newpage, struct page *page,",
            "\t\tenum migrate_mode mode)",
            "{",
            "\tstruct balloon_dev_info *balloon = balloon_page_device(page);",
            "",
            "\t/*",
            "\t * We can not easily support the no copy case here so ignore it as it",
            "\t * is unlikely to be used with balloon pages. See include/linux/hmm.h",
            "\t * for a user of the MIGRATE_SYNC_NO_COPY mode.",
            "\t */",
            "\tif (mode == MIGRATE_SYNC_NO_COPY)",
            "\t\treturn -EINVAL;",
            "",
            "\tVM_BUG_ON_PAGE(!PageLocked(page), page);",
            "\tVM_BUG_ON_PAGE(!PageLocked(newpage), newpage);",
            "",
            "\treturn balloon->migratepage(balloon, newpage, page, mode);",
            "}"
          ],
          "function_name": "balloon_page_enqueue_one, balloon_page_list_enqueue, balloon_page_list_dequeue, balloon_page_enqueue, balloon_page_isolate, balloon_page_putback, balloon_page_migrate",
          "description": "提供气球内存页的并发控制及迁移管理，通过自旋锁保护页表操作，实现页面在LRU链表间的移动、隔离和迁移，支持气球膨胀/收缩事件统计",
          "similarity": 0.46441376209259033
        },
        {
          "chunk_id": 0,
          "file_path": "mm/balloon_compaction.c",
          "start_line": 1,
          "end_line": 13,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * mm/balloon_compaction.c",
            " *",
            " * Common interface for making balloon pages movable by compaction.",
            " *",
            " * Copyright (C) 2012, Red Hat, Inc.  Rafael Aquini <aquini@redhat.com>",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/slab.h>",
            "#include <linux/export.h>",
            "#include <linux/balloon_compaction.h>",
            ""
          ],
          "function_name": null,
          "description": "上下文不完整",
          "similarity": 0.45124486088752747
        }
      ]
    },
    {
      "source_file": "kernel/bpf/trampoline.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:36:55\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\trampoline.c`\n\n---\n\n# bpf/trampoline.c 技术文档\n\n## 1. 文件概述\n\n`bpf/trampoline.c` 是 Linux 内核 BPF（Berkeley Packet Filter）子系统中用于实现 **BPF Trampoline（跳板）机制** 的核心文件。该机制主要用于支持 **FENTRY/FEXIT**、**MODIFY_RETURN** 以及 **LSM（Linux Security Module）** 类型的 BPF 程序，通过动态生成或修改内核函数入口处的跳转代码（trampoline），实现对目标函数的无侵入式拦截与增强。该文件负责管理 trampoline 的生命周期、哈希表存储、与 ftrace 的集成、以及 JIT 生成的 trampoline 镜像的内存管理。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct bpf_trampoline`：表示一个 trampoline 实例，包含目标函数地址、程序链表、引用计数、互斥锁等。\n- `struct bpf_tramp_image`：表示 JIT 生成的 trampoline 机器码镜像，包含代码指针、大小、ksym 信息及引用计数。\n- `bpf_trampoline_table[]`：全局哈希表，用于根据函数地址（key）快速查找或创建对应的 trampoline。\n\n### 主要函数\n- `bpf_trampoline_lookup(u64 key)`：根据目标函数地址查找或创建 trampoline 实例。\n- `bpf_trampoline_update(struct bpf_trampoline *tr, bool lock_direct_mutex)`：更新 trampoline 的机器码以反映附加的 BPF 程序变更（声明但未在片段中定义）。\n- `register_fentry()` / `unregister_fentry()` / `modify_fentry()`：封装对 ftrace direct 或 `bpf_arch_text_poke` 的调用，用于安装/卸载/修改跳转指令。\n- `bpf_trampoline_get_progs()`：收集附加到 trampoline 上的所有 BPF 程序链接（按类型分类）。\n- `bpf_tramp_image_free()` 及相关 RCU/工作队列回调：安全释放 JIT 生成的 trampoline 镜像内存。\n- `bpf_prog_has_trampoline()`：判断给定 BPF 程序是否需要 trampoline 机制。\n- `bpf_image_ksym_*()`：管理 trampoline 镜像的内核符号（ksym）注册与 perf 事件通知。\n\n### 静态变量与常量\n- `TRAMPOLINE_HASH_BITS` / `TRAMPOLINE_TABLE_SIZE`：定义 trampoline 哈希表大小（1024 项）。\n- `trampoline_mutex`：保护全局 trampoline 哈希表的互斥锁。\n- `bpf_extension_verifier_ops` / `bpf_extension_prog_ops`：占位符操作结构体。\n\n### ftrace 集成回调（条件编译）\n- `bpf_tramp_ftrace_ops_func()`：处理 ftrace direct 模式下与 IP 修改共享相关的命令（`FTRACE_OPS_CMD_*`），协调 trampoline 更新。\n\n## 3. 关键实现\n\n### Trampoline 哈希表管理\n- 使用 `hash_64(key, TRAMPOLINE_HASH_BITS)` 将目标函数地址映射到 `trampoline_table` 的桶中。\n- `trampoline_mutex` 保护整个哈希表的查找、创建和插入操作，确保线程安全。\n- 每个 `bpf_trampoline` 实例通过 `refcount_t refcnt` 管理生命周期。\n\n### 与 ftrace/direct calls 集成\n- 在支持 `CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS` 的架构上，优先使用 ftrace 的 direct call 机制安装跳转。\n- `bpf_tramp_ftrace_ops_func()` 处理 ftrace 框架在启用 IP 修改共享时的特殊交互，通过返回 `-EAGAIN` 触发重试以协调 trampoline 更新。\n- 锁定顺序：`tr->mutex` → ftrace 的 `direct_mutex` → `ftrace_lock`，使用 `mutex_trylock` 避免死锁。\n\n### Trampoline 镜像生命周期管理\n- 使用 **三层延迟释放机制** 确保安全回收 JIT 生成的代码页：\n  1. **Per-CPU 引用计数 (`percpu_ref`)**：跟踪运行中的 trampoline 使用。\n  2. **RCU (`call_rcu_tasks`)**：等待所有可能执行该代码的 CPU 上下文完成。\n  3. **工作队列 (`schedule_work`)**：在进程上下文中执行最终的内存释放（包括 `arch_free_bpf_trampoline` 和 ksym 注销）。\n- 通过 `perf_event_ksymbol` 向用户态 perf 工具通知 BPF trampoline 镜像的加载/卸载。\n\n### 程序分类与收集\n- BPF 程序按类型（`BPF_TRAMP_FENTRY`, `BPF_TRAMP_FEXIT`, `BPF_TRAMP_MODIFY_RETURN`, `BPF_TRAMP_FENTRY_OPS`）存储在 `tr->progs_hlist[4]` 中。\n- `bpf_trampoline_get_progs()` 遍历这些链表，构建 `bpf_tramp_links` 结构供 JIT 编译器使用，并检查是否需要传递函数 IP 参数。\n\n## 4. 依赖关系\n\n- **BPF 核心**：依赖 `bpf.h`, `filter.h`, `bpf_verifier.h` 提供程序模型、验证器和通用操作。\n- **ftrace 子系统**：依赖 `ftrace.h` 实现函数跟踪和 direct call 注册（`register_ftrace_direct` 等）。\n- **BTF (BPF Type Format)**：依赖 `btf.h` 获取内核函数签名信息（虽未直接使用，但 trampoline 机制依赖 BTF 信息）。\n- **内存管理**：依赖 `rcupdate_trace.h`, `rcupdate_wait.h` 实现安全的延迟释放。\n- **架构相关代码**：依赖 `bpf_arch_text_poke()`（在 `arch/` 目录下实现）进行非 ftrace 路径的代码修补。\n- **性能事件**：依赖 `perf_event.h` 发送 KSYMBOL 事件。\n- **LSM 框架**：依赖 `bpf_lsm.h` 支持 LSM 类型的 BPF 程序。\n- **静态调用优化**：依赖 `static_call.h`（虽未直接使用，但相关机制可能被优化）。\n\n## 5. 使用场景\n\n- **FENTRY/FEXIT 程序**：当用户附加 `BPF_TRACE_FENTRY` 或 `BPF_TRACE_FEXIT` 类型的 BPF 程序到内核函数时，内核为该函数创建或复用一个 trampoline，动态生成包含调用 BPF 程序逻辑的入口/出口桩代码。\n- **MODIFY_RETURN 程序**：用于拦截并修改目标函数的返回值，同样通过 trampoline 机制实现。\n- **LSM 程序**：`BPF_LSM_MAC` 类型的程序通过 trampoline 挂接到 LSM hooks。\n- **动态更新**：当附加到同一函数的 BPF 程序集合发生变化（添加/删除）时，触发 `bpf_trampoline_update()` 重新生成 trampoline 代码。\n- **资源回收**：当所有引用 trampoline 的 BPF 程序被 detach 且无执行实例后，通过 RCU 和工作队列安全释放其 JIT 代码和相关资源。",
      "similarity": 0.46673768758773804,
      "chunks": [
        {
          "chunk_id": 6,
          "file_path": "kernel/bpf/trampoline.c",
          "start_line": 838,
          "end_line": 940,
          "content": [
            "void bpf_trampoline_put(struct bpf_trampoline *tr)",
            "{",
            "\tint i;",
            "",
            "\tif (!tr)",
            "\t\treturn;",
            "\tmutex_lock(&trampoline_mutex);",
            "\tif (!refcount_dec_and_test(&tr->refcnt))",
            "\t\tgoto out;",
            "\tWARN_ON_ONCE(mutex_is_locked(&tr->mutex));",
            "",
            "\tfor (i = 0; i < BPF_TRAMP_MAX; i++)",
            "\t\tif (WARN_ON_ONCE(!hlist_empty(&tr->progs_hlist[i])))",
            "\t\t\tgoto out;",
            "",
            "\t/* This code will be executed even when the last bpf_tramp_image",
            "\t * is alive. All progs are detached from the trampoline and the",
            "\t * trampoline image is patched with jmp into epilogue to skip",
            "\t * fexit progs. The fentry-only trampoline will be freed via",
            "\t * multiple rcu callbacks.",
            "\t */",
            "\thlist_del(&tr->hlist);",
            "\tif (tr->fops) {",
            "\t\tftrace_free_filter(tr->fops);",
            "\t\tkfree(tr->fops);",
            "\t}",
            "\tkfree(tr);",
            "out:",
            "\tmutex_unlock(&trampoline_mutex);",
            "}",
            "static __always_inline u64 notrace bpf_prog_start_time(void)",
            "{",
            "\tu64 start = NO_START_TIME;",
            "",
            "\tif (static_branch_unlikely(&bpf_stats_enabled_key)) {",
            "\t\tstart = sched_clock();",
            "\t\tif (unlikely(!start))",
            "\t\t\tstart = NO_START_TIME;",
            "\t}",
            "\treturn start;",
            "}",
            "static u64 notrace __bpf_prog_enter_recur(struct bpf_prog *prog, struct bpf_tramp_run_ctx *run_ctx)",
            "\t__acquires(RCU)",
            "{",
            "\trcu_read_lock();",
            "\tmigrate_disable();",
            "",
            "\trun_ctx->saved_run_ctx = bpf_set_run_ctx(&run_ctx->run_ctx);",
            "",
            "\tif (unlikely(this_cpu_inc_return(*(prog->active)) != 1)) {",
            "\t\tbpf_prog_inc_misses_counter(prog);",
            "\t\tif (prog->aux->recursion_detected)",
            "\t\t\tprog->aux->recursion_detected(prog);",
            "\t\treturn 0;",
            "\t}",
            "\treturn bpf_prog_start_time();",
            "}",
            "static void notrace update_prog_stats(struct bpf_prog *prog,",
            "\t\t\t\t      u64 start)",
            "{",
            "\tstruct bpf_prog_stats *stats;",
            "",
            "\tif (static_branch_unlikely(&bpf_stats_enabled_key) &&",
            "\t    /* static_key could be enabled in __bpf_prog_enter*",
            "\t     * and disabled in __bpf_prog_exit*.",
            "\t     * And vice versa.",
            "\t     * Hence check that 'start' is valid.",
            "\t     */",
            "\t    start > NO_START_TIME) {",
            "\t\tunsigned long flags;",
            "",
            "\t\tstats = this_cpu_ptr(prog->stats);",
            "\t\tflags = u64_stats_update_begin_irqsave(&stats->syncp);",
            "\t\tu64_stats_inc(&stats->cnt);",
            "\t\tu64_stats_add(&stats->nsecs, sched_clock() - start);",
            "\t\tu64_stats_update_end_irqrestore(&stats->syncp, flags);",
            "\t}",
            "}",
            "static void notrace __bpf_prog_exit_recur(struct bpf_prog *prog, u64 start,",
            "\t\t\t\t\t  struct bpf_tramp_run_ctx *run_ctx)",
            "\t__releases(RCU)",
            "{",
            "\tbpf_reset_run_ctx(run_ctx->saved_run_ctx);",
            "",
            "\tupdate_prog_stats(prog, start);",
            "\tthis_cpu_dec(*(prog->active));",
            "\tmigrate_enable();",
            "\trcu_read_unlock();",
            "}",
            "static u64 notrace __bpf_prog_enter_lsm_cgroup(struct bpf_prog *prog,",
            "\t\t\t\t\t       struct bpf_tramp_run_ctx *run_ctx)",
            "\t__acquires(RCU)",
            "{",
            "\t/* Runtime stats are exported via actual BPF_LSM_CGROUP",
            "\t * programs, not the shims.",
            "\t */",
            "\trcu_read_lock();",
            "\tmigrate_disable();",
            "",
            "\trun_ctx->saved_run_ctx = bpf_set_run_ctx(&run_ctx->run_ctx);",
            "",
            "\treturn NO_START_TIME;",
            "}"
          ],
          "function_name": "bpf_trampoline_put, bpf_prog_start_time, __bpf_prog_enter_recur, update_prog_stats, __bpf_prog_exit_recur, __bpf_prog_enter_lsm_cgroup",
          "description": "管理BPF陷阱对象的引用计数及统计信息收集，包含递归调用处理、运行时间记录与性能统计更新逻辑。",
          "similarity": 0.4568696618080139
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/trampoline.c",
          "start_line": 191,
          "end_line": 317,
          "content": [
            "static int modify_fentry(struct bpf_trampoline *tr, void *old_addr, void *new_addr,",
            "\t\t\t bool lock_direct_mutex)",
            "{",
            "\tvoid *ip = tr->func.addr;",
            "\tint ret;",
            "",
            "\tif (tr->func.ftrace_managed) {",
            "\t\tif (lock_direct_mutex)",
            "\t\t\tret = modify_ftrace_direct(tr->fops, (long)new_addr);",
            "\t\telse",
            "\t\t\tret = modify_ftrace_direct_nolock(tr->fops, (long)new_addr);",
            "\t} else {",
            "\t\tret = bpf_arch_text_poke(ip, BPF_MOD_CALL, old_addr, new_addr);",
            "\t}",
            "\treturn ret;",
            "}",
            "static int register_fentry(struct bpf_trampoline *tr, void *new_addr)",
            "{",
            "\tvoid *ip = tr->func.addr;",
            "\tunsigned long faddr;",
            "\tint ret;",
            "",
            "\tfaddr = ftrace_location((unsigned long)ip);",
            "\tif (faddr) {",
            "\t\tif (!tr->fops)",
            "\t\t\treturn -ENOTSUPP;",
            "\t\ttr->func.ftrace_managed = true;",
            "\t}",
            "",
            "\tif (tr->func.ftrace_managed) {",
            "\t\tftrace_set_filter_ip(tr->fops, (unsigned long)ip, 0, 1);",
            "\t\tret = register_ftrace_direct(tr->fops, (long)new_addr);",
            "\t} else {",
            "\t\tret = bpf_arch_text_poke(ip, BPF_MOD_CALL, NULL, new_addr);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "static void bpf_tramp_image_free(struct bpf_tramp_image *im)",
            "{",
            "\tbpf_image_ksym_del(&im->ksym);",
            "\tarch_free_bpf_trampoline(im->image, im->size);",
            "\tbpf_jit_uncharge_modmem(im->size);",
            "\tpercpu_ref_exit(&im->pcref);",
            "\tkfree_rcu(im, rcu);",
            "}",
            "static void __bpf_tramp_image_put_deferred(struct work_struct *work)",
            "{",
            "\tstruct bpf_tramp_image *im;",
            "",
            "\tim = container_of(work, struct bpf_tramp_image, work);",
            "\tbpf_tramp_image_free(im);",
            "}",
            "static void __bpf_tramp_image_put_rcu(struct rcu_head *rcu)",
            "{",
            "\tstruct bpf_tramp_image *im;",
            "",
            "\tim = container_of(rcu, struct bpf_tramp_image, rcu);",
            "\tINIT_WORK(&im->work, __bpf_tramp_image_put_deferred);",
            "\tschedule_work(&im->work);",
            "}",
            "static void __bpf_tramp_image_release(struct percpu_ref *pcref)",
            "{",
            "\tstruct bpf_tramp_image *im;",
            "",
            "\tim = container_of(pcref, struct bpf_tramp_image, pcref);",
            "\tcall_rcu_tasks(&im->rcu, __bpf_tramp_image_put_rcu);",
            "}",
            "static void __bpf_tramp_image_put_rcu_tasks(struct rcu_head *rcu)",
            "{",
            "\tstruct bpf_tramp_image *im;",
            "",
            "\tim = container_of(rcu, struct bpf_tramp_image, rcu);",
            "\tif (im->ip_after_call)",
            "\t\t/* the case of fmod_ret/fexit trampoline and CONFIG_PREEMPTION=y */",
            "\t\tpercpu_ref_kill(&im->pcref);",
            "\telse",
            "\t\t/* the case of fentry trampoline */",
            "\t\tcall_rcu_tasks(&im->rcu, __bpf_tramp_image_put_rcu);",
            "}",
            "static void bpf_tramp_image_put(struct bpf_tramp_image *im)",
            "{",
            "\t/* The trampoline image that calls original function is using:",
            "\t * rcu_read_lock_trace to protect sleepable bpf progs",
            "\t * rcu_read_lock to protect normal bpf progs",
            "\t * percpu_ref to protect trampoline itself",
            "\t * rcu tasks to protect trampoline asm not covered by percpu_ref",
            "\t * (which are few asm insns before __bpf_tramp_enter and",
            "\t *  after __bpf_tramp_exit)",
            "\t *",
            "\t * The trampoline is unreachable before bpf_tramp_image_put().",
            "\t *",
            "\t * First, patch the trampoline to avoid calling into fexit progs.",
            "\t * The progs will be freed even if the original function is still",
            "\t * executing or sleeping.",
            "\t * In case of CONFIG_PREEMPT=y use call_rcu_tasks() to wait on",
            "\t * first few asm instructions to execute and call into",
            "\t * __bpf_tramp_enter->percpu_ref_get.",
            "\t * Then use percpu_ref_kill to wait for the trampoline and the original",
            "\t * function to finish.",
            "\t * Then use call_rcu_tasks() to make sure few asm insns in",
            "\t * the trampoline epilogue are done as well.",
            "\t *",
            "\t * In !PREEMPT case the task that got interrupted in the first asm",
            "\t * insns won't go through an RCU quiescent state which the",
            "\t * percpu_ref_kill will be waiting for. Hence the first",
            "\t * call_rcu_tasks() is not necessary.",
            "\t */",
            "\tif (im->ip_after_call) {",
            "\t\tint err = bpf_arch_text_poke(im->ip_after_call, BPF_MOD_JUMP,",
            "\t\t\t\t\t     NULL, im->ip_epilogue);",
            "\t\tWARN_ON(err);",
            "\t\tif (IS_ENABLED(CONFIG_PREEMPTION))",
            "\t\t\tcall_rcu_tasks(&im->rcu, __bpf_tramp_image_put_rcu_tasks);",
            "\t\telse",
            "\t\t\tpercpu_ref_kill(&im->pcref);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* The trampoline without fexit and fmod_ret progs doesn't call original",
            "\t * function and doesn't use percpu_ref.",
            "\t * Use call_rcu_tasks_trace() to wait for sleepable progs to finish.",
            "\t * Then use call_rcu_tasks() to wait for the rest of trampoline asm",
            "\t * and normal progs.",
            "\t */",
            "\tcall_rcu_tasks_trace(&im->rcu, __bpf_tramp_image_put_rcu_tasks);",
            "}"
          ],
          "function_name": "modify_fentry, register_fentry, bpf_tramp_image_free, __bpf_tramp_image_put_deferred, __bpf_tramp_image_put_rcu, __bpf_tramp_image_release, __bpf_tramp_image_put_rcu_tasks, bpf_tramp_image_put",
          "description": "实现trampoline图像的释放流程，通过RCU和延迟工作队列安全释放资源，处理不同内核配置下的引用计数和内存回收路径。",
          "similarity": 0.4317547082901001
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/trampoline.c",
          "start_line": 1,
          "end_line": 34,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2019 Facebook */",
            "#include <linux/hash.h>",
            "#include <linux/bpf.h>",
            "#include <linux/filter.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/rbtree_latch.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/btf.h>",
            "#include <linux/rcupdate_trace.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/static_call.h>",
            "#include <linux/bpf_verifier.h>",
            "#include <linux/bpf_lsm.h>",
            "#include <linux/delay.h>",
            "",
            "/* dummy _ops. The verifier will operate on target program's ops. */",
            "const struct bpf_verifier_ops bpf_extension_verifier_ops = {",
            "};",
            "const struct bpf_prog_ops bpf_extension_prog_ops = {",
            "};",
            "",
            "/* btf_vmlinux has ~22k attachable functions. 1k htab is enough. */",
            "#define TRAMPOLINE_HASH_BITS 10",
            "#define TRAMPOLINE_TABLE_SIZE (1 << TRAMPOLINE_HASH_BITS)",
            "",
            "static struct hlist_head trampoline_table[TRAMPOLINE_TABLE_SIZE];",
            "",
            "/* serializes access to trampoline_table */",
            "static DEFINE_MUTEX(trampoline_mutex);",
            "",
            "#ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS",
            "static int bpf_trampoline_update(struct bpf_trampoline *tr, bool lock_direct_mutex);",
            ""
          ],
          "function_name": null,
          "description": "声明BPF trampoline相关常量、哈希表和互斥锁，定义空的验证器和程序操作接口，准备用于动态追踪的跳转表和同步机制。",
          "similarity": 0.42237043380737305
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/trampoline.c",
          "start_line": 398,
          "end_line": 505,
          "content": [
            "static int bpf_trampoline_update(struct bpf_trampoline *tr, bool lock_direct_mutex)",
            "{",
            "\tstruct bpf_tramp_image *im;",
            "\tstruct bpf_tramp_links *tlinks;",
            "\tu32 orig_flags = tr->flags;",
            "\tbool ip_arg = false;",
            "\tint err, total, size;",
            "",
            "\ttlinks = bpf_trampoline_get_progs(tr, &total, &ip_arg);",
            "\tif (IS_ERR(tlinks))",
            "\t\treturn PTR_ERR(tlinks);",
            "",
            "\tif (total == 0) {",
            "\t\terr = unregister_fentry(tr, tr->cur_image->image);",
            "\t\tbpf_tramp_image_put(tr->cur_image);",
            "\t\ttr->cur_image = NULL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* clear all bits except SHARE_IPMODIFY and TAIL_CALL_CTX */",
            "\ttr->flags &= (BPF_TRAMP_F_SHARE_IPMODIFY | BPF_TRAMP_F_TAIL_CALL_CTX);",
            "",
            "\tif (tlinks[BPF_TRAMP_FEXIT].nr_links ||",
            "\t    tlinks[BPF_TRAMP_MODIFY_RETURN].nr_links) {",
            "\t\t/* NOTE: BPF_TRAMP_F_RESTORE_REGS and BPF_TRAMP_F_SKIP_FRAME",
            "\t\t * should not be set together.",
            "\t\t */",
            "\t\ttr->flags |= BPF_TRAMP_F_CALL_ORIG | BPF_TRAMP_F_SKIP_FRAME;",
            "\t} else {",
            "\t\ttr->flags |= BPF_TRAMP_F_RESTORE_REGS;",
            "\t}",
            "",
            "\tif (ip_arg)",
            "\t\ttr->flags |= BPF_TRAMP_F_IP_ARG;",
            "",
            "#ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS",
            "again:",
            "\tif ((tr->flags & BPF_TRAMP_F_SHARE_IPMODIFY) &&",
            "\t    (tr->flags & BPF_TRAMP_F_CALL_ORIG))",
            "\t\ttr->flags |= BPF_TRAMP_F_ORIG_STACK;",
            "#endif",
            "",
            "\tsize = arch_bpf_trampoline_size(&tr->func.model, tr->flags,",
            "\t\t\t\t\ttlinks, tr->func.addr);",
            "\tif (size < 0) {",
            "\t\terr = size;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tif (size > PAGE_SIZE) {",
            "\t\terr = -E2BIG;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tim = bpf_tramp_image_alloc(tr->key, size);",
            "\tif (IS_ERR(im)) {",
            "\t\terr = PTR_ERR(im);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\terr = arch_prepare_bpf_trampoline(im, im->image, im->image + size,",
            "\t\t\t\t\t  &tr->func.model, tr->flags, tlinks,",
            "\t\t\t\t\t  tr->func.addr);",
            "\tif (err < 0)",
            "\t\tgoto out_free;",
            "",
            "\tarch_protect_bpf_trampoline(im->image, im->size);",
            "",
            "\tWARN_ON(tr->cur_image && total == 0);",
            "\tif (tr->cur_image)",
            "\t\t/* progs already running at this address */",
            "\t\terr = modify_fentry(tr, tr->cur_image->image, im->image, lock_direct_mutex);",
            "\telse",
            "\t\t/* first time registering */",
            "\t\terr = register_fentry(tr, im->image);",
            "",
            "#ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS",
            "\tif (err == -EAGAIN) {",
            "\t\t/* -EAGAIN from bpf_tramp_ftrace_ops_func. Now",
            "\t\t * BPF_TRAMP_F_SHARE_IPMODIFY is set, we can generate the",
            "\t\t * trampoline again, and retry register.",
            "\t\t */",
            "\t\t/* reset fops->func and fops->trampoline for re-register */",
            "\t\ttr->fops->func = NULL;",
            "\t\ttr->fops->trampoline = 0;",
            "",
            "\t\t/* free im memory and reallocate later */",
            "\t\tbpf_tramp_image_free(im);",
            "\t\tgoto again;",
            "\t}",
            "#endif",
            "\tif (err)",
            "\t\tgoto out_free;",
            "",
            "\tif (tr->cur_image)",
            "\t\tbpf_tramp_image_put(tr->cur_image);",
            "\ttr->cur_image = im;",
            "out:",
            "\t/* If any error happens, restore previous flags */",
            "\tif (err)",
            "\t\ttr->flags = orig_flags;",
            "\tkfree(tlinks);",
            "\treturn err;",
            "",
            "out_free:",
            "\tbpf_tramp_image_free(im);",
            "\tgoto out;",
            "}"
          ],
          "function_name": "bpf_trampoline_update",
          "description": "执行trampoline更新核心逻辑，根据当前挂载的程序链表重新生成并安装新trampoline，处理共享IP修改场景下的重试机制和错误恢复。",
          "similarity": 0.41851016879081726
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/trampoline.c",
          "start_line": 35,
          "end_line": 143,
          "content": [
            "static int bpf_tramp_ftrace_ops_func(struct ftrace_ops *ops, enum ftrace_ops_cmd cmd)",
            "{",
            "\tstruct bpf_trampoline *tr = ops->private;",
            "\tint ret = 0;",
            "",
            "\tif (cmd == FTRACE_OPS_CMD_ENABLE_SHARE_IPMODIFY_SELF) {",
            "\t\t/* This is called inside register_ftrace_direct_multi(), so",
            "\t\t * tr->mutex is already locked.",
            "\t\t */",
            "\t\tlockdep_assert_held_once(&tr->mutex);",
            "",
            "\t\t/* Instead of updating the trampoline here, we propagate",
            "\t\t * -EAGAIN to register_ftrace_direct(). Then we can",
            "\t\t * retry register_ftrace_direct() after updating the",
            "\t\t * trampoline.",
            "\t\t */",
            "\t\tif ((tr->flags & BPF_TRAMP_F_CALL_ORIG) &&",
            "\t\t    !(tr->flags & BPF_TRAMP_F_ORIG_STACK)) {",
            "\t\t\tif (WARN_ON_ONCE(tr->flags & BPF_TRAMP_F_SHARE_IPMODIFY))",
            "\t\t\t\treturn -EBUSY;",
            "",
            "\t\t\ttr->flags |= BPF_TRAMP_F_SHARE_IPMODIFY;",
            "\t\t\treturn -EAGAIN;",
            "\t\t}",
            "",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/* The normal locking order is",
            "\t *    tr->mutex => direct_mutex (ftrace.c) => ftrace_lock (ftrace.c)",
            "\t *",
            "\t * The following two commands are called from",
            "\t *",
            "\t *   prepare_direct_functions_for_ipmodify",
            "\t *   cleanup_direct_functions_after_ipmodify",
            "\t *",
            "\t * In both cases, direct_mutex is already locked. Use",
            "\t * mutex_trylock(&tr->mutex) to avoid deadlock in race condition",
            "\t * (something else is making changes to this same trampoline).",
            "\t */",
            "\tif (!mutex_trylock(&tr->mutex)) {",
            "\t\t/* sleep 1 ms to make sure whatever holding tr->mutex makes",
            "\t\t * some progress.",
            "\t\t */",
            "\t\tmsleep(1);",
            "\t\treturn -EAGAIN;",
            "\t}",
            "",
            "\tswitch (cmd) {",
            "\tcase FTRACE_OPS_CMD_ENABLE_SHARE_IPMODIFY_PEER:",
            "\t\ttr->flags |= BPF_TRAMP_F_SHARE_IPMODIFY;",
            "",
            "\t\tif ((tr->flags & BPF_TRAMP_F_CALL_ORIG) &&",
            "\t\t    !(tr->flags & BPF_TRAMP_F_ORIG_STACK))",
            "\t\t\tret = bpf_trampoline_update(tr, false /* lock_direct_mutex */);",
            "\t\tbreak;",
            "\tcase FTRACE_OPS_CMD_DISABLE_SHARE_IPMODIFY_PEER:",
            "\t\ttr->flags &= ~BPF_TRAMP_F_SHARE_IPMODIFY;",
            "",
            "\t\tif (tr->flags & BPF_TRAMP_F_ORIG_STACK)",
            "\t\t\tret = bpf_trampoline_update(tr, false /* lock_direct_mutex */);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tret = -EINVAL;",
            "\t\tbreak;",
            "\t}",
            "",
            "\tmutex_unlock(&tr->mutex);",
            "\treturn ret;",
            "}",
            "bool bpf_prog_has_trampoline(const struct bpf_prog *prog)",
            "{",
            "\tenum bpf_attach_type eatype = prog->expected_attach_type;",
            "\tenum bpf_prog_type ptype = prog->type;",
            "",
            "\treturn (ptype == BPF_PROG_TYPE_TRACING &&",
            "\t\t(eatype == BPF_TRACE_FENTRY || eatype == BPF_TRACE_FEXIT ||",
            "\t\t eatype == BPF_MODIFY_RETURN)) ||",
            "\t\t(ptype == BPF_PROG_TYPE_LSM && eatype == BPF_LSM_MAC);",
            "}",
            "void bpf_image_ksym_init(void *data, unsigned int size, struct bpf_ksym *ksym)",
            "{",
            "\tksym->start = (unsigned long) data;",
            "\tksym->end = ksym->start + size;",
            "}",
            "void bpf_image_ksym_add(struct bpf_ksym *ksym)",
            "{",
            "\tbpf_ksym_add(ksym);",
            "\tperf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_BPF, ksym->start,",
            "\t\t\t   PAGE_SIZE, false, ksym->name);",
            "}",
            "void bpf_image_ksym_del(struct bpf_ksym *ksym)",
            "{",
            "\tbpf_ksym_del(ksym);",
            "\tperf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_BPF, ksym->start,",
            "\t\t\t   PAGE_SIZE, true, ksym->name);",
            "}",
            "static int unregister_fentry(struct bpf_trampoline *tr, void *old_addr)",
            "{",
            "\tvoid *ip = tr->func.addr;",
            "\tint ret;",
            "",
            "\tif (tr->func.ftrace_managed)",
            "\t\tret = unregister_ftrace_direct(tr->fops, (long)old_addr, false);",
            "\telse",
            "\t\tret = bpf_arch_text_poke(ip, BPF_MOD_CALL, old_addr, NULL);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "bpf_tramp_ftrace_ops_func, bpf_prog_has_trampoline, bpf_image_ksym_init, bpf_image_ksym_add, bpf_image_ksym_del, unregister_fentry",
          "description": "实现FTRACE操作回调处理逻辑，控制共享IP修改标志位，提供BPF程序是否支持trampoline检测，管理KSYM符号注册与注销，实现卸载fentry的辅助函数。",
          "similarity": 0.4090198874473572
        }
      ]
    }
  ]
}