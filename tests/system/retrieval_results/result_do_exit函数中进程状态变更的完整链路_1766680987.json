{
  "query": "do_exit函数中进程状态变更的完整链路",
  "timestamp": "2025-12-26 00:43:07",
  "retrieved_files": [
    {
      "source_file": "kernel/exit.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:27:18\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `exit.c`\n\n---\n\n# `exit.c` 技术文档\n\n## 1. 文件概述\n\n`exit.c` 是 Linux 内核中负责进程退出（termination）核心逻辑的关键源文件，位于 `kernel/` 目录下。该文件实现了进程终止时的资源回收、信号处理、线程组清理、引用计数释放以及与用户空间和内核其他子系统的协调机制。其主要职责包括：\n\n- 安全地释放进程占用的内核资源（如内存、文件描述符、信号处理结构等）\n- 更新进程组和会话的统计信息\n- 通知父进程子进程已退出（通过 `SIGCHLD` 信号）\n- 管理僵尸进程（zombie）的生命周期\n- 支持线程组（thread group）的协同退出\n- 提供与 oops（内核异常）相关的计数和限制机制\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|---------|\n| `__unhash_process()` | 从内核的进程哈希表和链表中移除进程，减少线程计数 |\n| `__exit_signal()` | 清理进程的信号相关资源，累加 CPU 时间和 I/O 统计到 `signal_struct` |\n| `delayed_put_task_struct()` | RCU 回调函数，延迟释放 `task_struct` 及其关联资源 |\n| `put_task_struct_rcu_user()` | 安全地减少 `task_struct` 的 RCU 用户引用计数，并在为零时调度延迟释放 |\n| `release_thread()` | 架构相关的线程资源释放钩子（弱符号，可由架构代码覆盖） |\n| `release_task()` | 主进程释放入口函数，协调整个退出流程，包括通知父进程、释放资源等 |\n| `rcuwait_wake_up()` | 唤醒等待在 `rcuwait` 上的任务（代码片段未完整） |\n\n### 关键数据结构与变量\n\n| 名称 | 类型/说明 |\n|------|----------|\n| `oops_limit` | `unsigned int`，限制内核 oops 发生次数的阈值（默认 10000） |\n| `oops_count` | `atomic_t`，原子计数器，记录系统发生 oops 的总次数 |\n| `kern_exit_table` | `ctl_table`，用于 `/proc/sys/kernel/oops_limit` 的 sysctl 接口 |\n| `oops_count_attr` | `kobj_attribute`，用于 `/sys/kernel/oops_count` 的 sysfs 接口 |\n\n## 3. 关键实现\n\n### 进程退出流程\n\n1. **资源统计聚合**：  \n   在 `__exit_signal()` 中，将退出线程的 CPU 时间（`utime`/`stime`）、I/O 操作、上下文切换次数等统计信息累加到所属线程组的 `signal_struct` 中，确保即使线程组 leader 尚未退出，也能被 `wait4()` 等系统调用正确获取。\n\n2. **线程组协同退出**：  \n   - 若当前退出的是线程组 leader（`group_dead == true`），则清理整个线程组的 PID 类型（TGID、PGID、SID），并从全局任务链表中移除。\n   - 若非 leader，则仅减少线程组计数，并可能更新 `curr_target`（用于信号投递）。\n\n3. **僵尸进程处理**：  \n   在 `release_task()` 中，检查线程组 leader 是否已变为僵尸状态。若是且当前线程是最后一个成员，则调用 `do_notify_parent()` 通知其父进程。若父进程忽略 `SIGCHLD`，则直接将 leader 状态置为 `EXIT_DEAD` 并递归释放。\n\n4. **延迟释放机制**：  \n   通过 RCU（Read-Copy-Update）机制安全释放 `task_struct`。`put_task_struct_rcu_user()` 在引用计数归零时调用 `call_rcu()`，由 `delayed_put_task_struct()` 在 RCU 宽限期后执行实际释放，确保并发读取安全。\n\n5. **Oops 计数与限制**：  \n   提供 `oops_count`（只读）和 `oops_limit`（可调）两个接口，用于监控和限制内核异常次数，防止因频繁崩溃导致资源耗尽或引用计数溢出。\n\n### 锁与同步\n\n- **`tasklist_lock`**：写锁保护进程链表和 PID 哈希表的修改。\n- **`sighand->siglock`**：自旋锁保护信号处理结构。\n- **`signal->stats_lock`**：顺序锁（seqlock）保护线程组统计信息的聚合。\n- **RCU**：用于安全地延迟释放 `task_struct`，避免在遍历任务链表时访问已释放内存。\n\n## 4. 依赖关系\n\n`exit.c` 与内核多个子系统紧密耦合，主要依赖包括：\n\n- **调度器（SCHED）**：`<linux/sched/*.h>`，用于任务状态管理、CPU 时间统计、任务链表操作。\n- **内存管理（MM）**：`<linux/mm.h>`、`<linux/slab.h>`，用于内存释放和 slab 分配器交互。\n- **文件系统（VFS）**：`<linux/file.h>`、`<linux/fdtable.h>`、`<linux/fs_struct.h>`，用于关闭文件描述符和释放文件系统上下文。\n- **进程间通信（IPC）**：`<linux/shm.h>`、`<linux/posix-timers.h>`，用于清理共享内存和定时器资源。\n- **安全与审计**：`<linux/audit.h>`、`<linux/seccomp.h>`（通过 `seccomp_filter_release`），用于释放安全策略和审计上下文。\n- **cgroup 与资源控制**：`<linux/cgroup.h>`、`<linux/resource.h>`，用于资源计数释放和限制检查。\n- **跟踪与性能**：`<linux/perf_event.h>`、`<trace/events/sched.h>`，用于性能事件清理和调度跟踪点。\n- **架构相关代码**：`<asm/mmu_context.h>`、`release_thread()` 弱符号，允许架构层定制线程释放逻辑。\n\n## 5. 使用场景\n\n- **进程正常退出**：当用户程序调用 `exit()` 或 `exit_group()` 系统调用时，内核通过此文件执行清理。\n- **进程被信号终止**：如收到 `SIGKILL` 或 `SIGTERM` 后，内核调度退出路径。\n- **线程退出**：POSIX 线程（通过 `pthread_exit()` 或线程函数返回）触发 `release_task()` 清理单个线程。\n- **内核 Oops/panic 处理**：每次内核异常会递增 `oops_count`，用于监控系统稳定性。\n- **僵尸进程回收**：父进程调用 `wait()` 系列系统调用后，内核最终通过 `release_task()` 释放僵尸进程的内核结构。\n- **容器/命名空间退出**：在 PID 命名空间或 cgroup 中进程退出时，协调资源释放和通知机制。",
      "similarity": 0.5973396897315979,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "kernel/exit.c",
          "start_line": 791,
          "end_line": 942,
          "content": [
            "static void check_stack_usage(void)",
            "{",
            "\tstatic DEFINE_SPINLOCK(low_water_lock);",
            "\tstatic int lowest_to_date = THREAD_SIZE;",
            "\tunsigned long free;",
            "",
            "\tfree = stack_not_used(current);",
            "",
            "\tif (free >= lowest_to_date)",
            "\t\treturn;",
            "",
            "\tspin_lock(&low_water_lock);",
            "\tif (free < lowest_to_date) {",
            "\t\tpr_info(\"%s (%d) used greatest stack depth: %lu bytes left\\n\",",
            "\t\t\tcurrent->comm, task_pid_nr(current), free);",
            "\t\tlowest_to_date = free;",
            "\t}",
            "\tspin_unlock(&low_water_lock);",
            "}",
            "static inline void check_stack_usage(void) {}",
            "static void synchronize_group_exit(struct task_struct *tsk, long code)",
            "{",
            "\tstruct sighand_struct *sighand = tsk->sighand;",
            "\tstruct signal_struct *signal = tsk->signal;",
            "",
            "\tspin_lock_irq(&sighand->siglock);",
            "\tsignal->quick_threads--;",
            "\tif ((signal->quick_threads == 0) &&",
            "\t    !(signal->flags & SIGNAL_GROUP_EXIT)) {",
            "\t\tsignal->flags = SIGNAL_GROUP_EXIT;",
            "\t\tsignal->group_exit_code = code;",
            "\t\tsignal->group_stop_count = 0;",
            "\t}",
            "\tspin_unlock_irq(&sighand->siglock);",
            "}",
            "void __noreturn do_exit(long code)",
            "{",
            "\tstruct task_struct *tsk = current;",
            "\tint group_dead;",
            "",
            "\tWARN_ON(irqs_disabled());",
            "",
            "\tsynchronize_group_exit(tsk, code);",
            "",
            "\tWARN_ON(tsk->plug);",
            "",
            "\tkcov_task_exit(tsk);",
            "\tkmsan_task_exit(tsk);",
            "",
            "\tcoredump_task_exit(tsk);",
            "\tptrace_event(PTRACE_EVENT_EXIT, code);",
            "\tuser_events_exit(tsk);",
            "",
            "\tio_uring_files_cancel();",
            "\texit_signals(tsk);  /* sets PF_EXITING */",
            "",
            "\t/* sync mm's RSS info before statistics gathering */",
            "\tif (tsk->mm)",
            "\t\tsync_mm_rss(tsk->mm);",
            "\tacct_update_integrals(tsk);",
            "\tgroup_dead = atomic_dec_and_test(&tsk->signal->live);",
            "\tif (group_dead) {",
            "\t\t/*",
            "\t\t * If the last thread of global init has exited, panic",
            "\t\t * immediately to get a useable coredump.",
            "\t\t */",
            "\t\tif (unlikely(is_global_init(tsk)))",
            "\t\t\tpanic(\"Attempted to kill init! exitcode=0x%08x\\n\",",
            "\t\t\t\ttsk->signal->group_exit_code ?: (int)code);",
            "",
            "#ifdef CONFIG_POSIX_TIMERS",
            "\t\thrtimer_cancel(&tsk->signal->real_timer);",
            "\t\texit_itimers(tsk);",
            "#endif",
            "\t\tif (tsk->mm)",
            "\t\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);",
            "\t}",
            "\tacct_collect(code, group_dead);",
            "\tif (group_dead)",
            "\t\ttty_audit_exit();",
            "\taudit_free(tsk);",
            "",
            "\ttsk->exit_code = code;",
            "\ttaskstats_exit(tsk, group_dead);",
            "",
            "\t/*",
            "\t * Since sampling can touch ->mm, make sure to stop everything before we",
            "\t * tear it down.",
            "\t *",
            "\t * Also flushes inherited counters to the parent - before the parent",
            "\t * gets woken up by child-exit notifications.",
            "\t */",
            "\tperf_event_exit_task(tsk);",
            "",
            "\texit_mm();",
            "",
            "\tif (group_dead)",
            "\t\tacct_process();",
            "\ttrace_sched_process_exit(tsk);",
            "",
            "\texit_sem(tsk);",
            "\texit_shm(tsk);",
            "\texit_files(tsk);",
            "\texit_fs(tsk);",
            "\tif (group_dead)",
            "\t\tdisassociate_ctty(1);",
            "\texit_task_namespaces(tsk);",
            "\texit_task_work(tsk);",
            "\texit_thread(tsk);",
            "",
            "\tsched_autogroup_exit_task(tsk);",
            "\tcgroup_exit(tsk);",
            "",
            "\t/*",
            "\t * FIXME: do that only when needed, using sched_exit tracepoint",
            "\t */",
            "\tflush_ptrace_hw_breakpoint(tsk);",
            "",
            "\texit_tasks_rcu_start();",
            "\texit_notify(tsk, group_dead);",
            "\tproc_exit_connector(tsk);",
            "\tmpol_put_task_policy(tsk);",
            "#ifdef CONFIG_FUTEX",
            "\tif (unlikely(current->pi_state_cache))",
            "\t\tkfree(current->pi_state_cache);",
            "#endif",
            "\t/*",
            "\t * Make sure we are holding no locks:",
            "\t */",
            "\tdebug_check_no_locks_held();",
            "",
            "\tif (tsk->io_context)",
            "\t\texit_io_context(tsk);",
            "",
            "\tif (tsk->splice_pipe)",
            "\t\tfree_pipe_info(tsk->splice_pipe);",
            "",
            "\tif (tsk->task_frag.page)",
            "\t\tput_page(tsk->task_frag.page);",
            "",
            "\texit_task_stack_account(tsk);",
            "",
            "\tcheck_stack_usage();",
            "\tpreempt_disable();",
            "\tif (tsk->nr_dirtied)",
            "\t\t__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);",
            "\texit_rcu();",
            "\texit_tasks_rcu_finish();",
            "",
            "\tlockdep_free_task(tsk);",
            "\tdo_task_dead();",
            "}"
          ],
          "function_name": "check_stack_usage, check_stack_usage, synchronize_group_exit, do_exit",
          "description": "do_exit函数负责处理进程退出流程，包括同步线程组退出、释放资源、更新统计信息、清理内存映射、解除命名空间关联等操作。其中synchronize_group_exit用于减少信号量计数并标记线程组退出状态，check_stack_usage监控最大堆栈使用量。",
          "similarity": 0.7062464952468872
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/exit.c",
          "start_line": 948,
          "end_line": 1066,
          "content": [
            "void __noreturn make_task_dead(int signr)",
            "{",
            "\t/*",
            "\t * Take the task off the cpu after something catastrophic has",
            "\t * happened.",
            "\t *",
            "\t * We can get here from a kernel oops, sometimes with preemption off.",
            "\t * Start by checking for critical errors.",
            "\t * Then fix up important state like USER_DS and preemption.",
            "\t * Then do everything else.",
            "\t */",
            "\tstruct task_struct *tsk = current;",
            "\tunsigned int limit;",
            "",
            "\tif (unlikely(in_interrupt()))",
            "\t\tpanic(\"Aiee, killing interrupt handler!\");",
            "\tif (unlikely(!tsk->pid))",
            "\t\tpanic(\"Attempted to kill the idle task!\");",
            "",
            "\tif (unlikely(irqs_disabled())) {",
            "\t\tpr_info(\"note: %s[%d] exited with irqs disabled\\n\",",
            "\t\t\tcurrent->comm, task_pid_nr(current));",
            "\t\tlocal_irq_enable();",
            "\t}",
            "\tif (unlikely(in_atomic())) {",
            "\t\tpr_info(\"note: %s[%d] exited with preempt_count %d\\n\",",
            "\t\t\tcurrent->comm, task_pid_nr(current),",
            "\t\t\tpreempt_count());",
            "\t\tpreempt_count_set(PREEMPT_ENABLED);",
            "\t}",
            "",
            "\t/*",
            "\t * Every time the system oopses, if the oops happens while a reference",
            "\t * to an object was held, the reference leaks.",
            "\t * If the oops doesn't also leak memory, repeated oopsing can cause",
            "\t * reference counters to wrap around (if they're not using refcount_t).",
            "\t * This means that repeated oopsing can make unexploitable-looking bugs",
            "\t * exploitable through repeated oopsing.",
            "\t * To make sure this can't happen, place an upper bound on how often the",
            "\t * kernel may oops without panic().",
            "\t */",
            "\tlimit = READ_ONCE(oops_limit);",
            "\tif (atomic_inc_return(&oops_count) >= limit && limit)",
            "\t\tpanic(\"Oopsed too often (kernel.oops_limit is %d)\", limit);",
            "",
            "\t/*",
            "\t * We're taking recursive faults here in make_task_dead. Safest is to just",
            "\t * leave this task alone and wait for reboot.",
            "\t */",
            "\tif (unlikely(tsk->flags & PF_EXITING)) {",
            "\t\tpr_alert(\"Fixing recursive fault but reboot is needed!\\n\");",
            "\t\tfutex_exit_recursive(tsk);",
            "\t\ttsk->exit_state = EXIT_DEAD;",
            "\t\trefcount_inc(&tsk->rcu_users);",
            "\t\tdo_task_dead();",
            "\t}",
            "",
            "\tdo_exit(signr);",
            "}",
            "void __noreturn",
            "do_group_exit(int exit_code)",
            "{",
            "\tstruct signal_struct *sig = current->signal;",
            "",
            "\tif (sig->flags & SIGNAL_GROUP_EXIT)",
            "\t\texit_code = sig->group_exit_code;",
            "\telse if (sig->group_exec_task)",
            "\t\texit_code = 0;",
            "\telse {",
            "\t\tstruct sighand_struct *const sighand = current->sighand;",
            "",
            "\t\tspin_lock_irq(&sighand->siglock);",
            "\t\tif (sig->flags & SIGNAL_GROUP_EXIT)",
            "\t\t\t/* Another thread got here before we took the lock.  */",
            "\t\t\texit_code = sig->group_exit_code;",
            "\t\telse if (sig->group_exec_task)",
            "\t\t\texit_code = 0;",
            "\t\telse {",
            "\t\t\tsig->group_exit_code = exit_code;",
            "\t\t\tsig->flags = SIGNAL_GROUP_EXIT;",
            "\t\t\tzap_other_threads(current);",
            "\t\t}",
            "\t\tspin_unlock_irq(&sighand->siglock);",
            "\t}",
            "",
            "\tdo_exit(exit_code);",
            "\t/* NOTREACHED */",
            "}",
            "static int eligible_pid(struct wait_opts *wo, struct task_struct *p)",
            "{",
            "\treturn\two->wo_type == PIDTYPE_MAX ||",
            "\t\ttask_pid_type(p, wo->wo_type) == wo->wo_pid;",
            "}",
            "static int",
            "eligible_child(struct wait_opts *wo, bool ptrace, struct task_struct *p)",
            "{",
            "\tif (!eligible_pid(wo, p))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Wait for all children (clone and not) if __WALL is set or",
            "\t * if it is traced by us.",
            "\t */",
            "\tif (ptrace || (wo->wo_flags & __WALL))",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * Otherwise, wait for clone children *only* if __WCLONE is set;",
            "\t * otherwise, wait for non-clone children *only*.",
            "\t *",
            "\t * Note: a \"clone\" child here is one that reports to its parent",
            "\t * using a signal other than SIGCHLD, or a non-leader thread which",
            "\t * we can only see if it is traced by us.",
            "\t */",
            "\tif ((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))",
            "\t\treturn 0;",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "make_task_dead, do_group_exit, eligible_pid, eligible_child",
          "description": "make_task_dead处理致命错误导致的进程终止，通过do_exit完成退出流程；do_group_exit用于线程组统一退出，设置退出码并触发do_exit；eligible_pid和eligible_child用于过滤符合等待条件的子进程，根据PID类型和跟踪标志进行匹配。",
          "similarity": 0.6443734169006348
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/exit.c",
          "start_line": 546,
          "end_line": 686,
          "content": [
            "static void exit_mm(void)",
            "{",
            "\tstruct mm_struct *mm = current->mm;",
            "",
            "\texit_mm_release(current, mm);",
            "\tif (!mm)",
            "\t\treturn;",
            "\tsync_mm_rss(mm);",
            "\tmmap_read_lock(mm);",
            "\tmmgrab_lazy_tlb(mm);",
            "\tBUG_ON(mm != current->active_mm);",
            "\t/* more a memory barrier than a real lock */",
            "\ttask_lock(current);",
            "\t/*",
            "\t * When a thread stops operating on an address space, the loop",
            "\t * in membarrier_private_expedited() may not observe that",
            "\t * tsk->mm, and the loop in membarrier_global_expedited() may",
            "\t * not observe a MEMBARRIER_STATE_GLOBAL_EXPEDITED",
            "\t * rq->membarrier_state, so those would not issue an IPI.",
            "\t * Membarrier requires a memory barrier after accessing",
            "\t * user-space memory, before clearing tsk->mm or the",
            "\t * rq->membarrier_state.",
            "\t */",
            "\tsmp_mb__after_spinlock();",
            "\tlocal_irq_disable();",
            "\tcurrent->mm = NULL;",
            "\t#ifdef CONFIG_IEE",
            "\tiee_set_token_pgd(current, NULL);",
            "\t#endif",
            "\tmembarrier_update_current_mm(NULL);",
            "\tenter_lazy_tlb(mm, current);",
            "\tlocal_irq_enable();",
            "\ttask_unlock(current);",
            "\tmmap_read_unlock(mm);",
            "\tmm_update_next_owner(mm);",
            "\tmmput(mm);",
            "\tif (test_thread_flag(TIF_MEMDIE))",
            "\t\texit_oom_victim();",
            "}",
            "static void reparent_leader(struct task_struct *father, struct task_struct *p,",
            "\t\t\t\tstruct list_head *dead)",
            "{",
            "\tif (unlikely(p->exit_state == EXIT_DEAD))",
            "\t\treturn;",
            "",
            "\t/* We don't want people slaying init. */",
            "\tp->exit_signal = SIGCHLD;",
            "",
            "\t/* If it has exited notify the new parent about this child's death. */",
            "\tif (!p->ptrace &&",
            "\t    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {",
            "\t\tif (do_notify_parent(p, p->exit_signal)) {",
            "\t\t\tp->exit_state = EXIT_DEAD;",
            "\t\t\tlist_add(&p->ptrace_entry, dead);",
            "\t\t}",
            "\t}",
            "",
            "\tkill_orphaned_pgrp(p, father);",
            "}",
            "static void forget_original_parent(struct task_struct *father,",
            "\t\t\t\t\tstruct list_head *dead)",
            "{",
            "\tstruct task_struct *p, *t, *reaper;",
            "",
            "\tif (unlikely(!list_empty(&father->ptraced)))",
            "\t\texit_ptrace(father, dead);",
            "",
            "\t/* Can drop and reacquire tasklist_lock */",
            "\treaper = find_child_reaper(father, dead);",
            "\tif (list_empty(&father->children))",
            "\t\treturn;",
            "",
            "\treaper = find_new_reaper(father, reaper);",
            "\tlist_for_each_entry(p, &father->children, sibling) {",
            "\t\tfor_each_thread(p, t) {",
            "\t\t\tRCU_INIT_POINTER(t->real_parent, reaper);",
            "\t\t\tBUG_ON((!t->ptrace) != (rcu_access_pointer(t->parent) == father));",
            "\t\t\tif (likely(!t->ptrace))",
            "\t\t\t\tt->parent = t->real_parent;",
            "\t\t\tif (t->pdeath_signal)",
            "\t\t\t\tgroup_send_sig_info(t->pdeath_signal,",
            "\t\t\t\t\t\t    SEND_SIG_NOINFO, t,",
            "\t\t\t\t\t\t    PIDTYPE_TGID);",
            "\t\t}",
            "\t\t/*",
            "\t\t * If this is a threaded reparent there is no need to",
            "\t\t * notify anyone anything has happened.",
            "\t\t */",
            "\t\tif (!same_thread_group(reaper, father))",
            "\t\t\treparent_leader(father, p, dead);",
            "\t}",
            "\tlist_splice_tail_init(&father->children, &reaper->children);",
            "}",
            "static void exit_notify(struct task_struct *tsk, int group_dead)",
            "{",
            "\tbool autoreap;",
            "\tstruct task_struct *p, *n;",
            "\tLIST_HEAD(dead);",
            "",
            "\twrite_lock_irq(&tasklist_lock);",
            "\tforget_original_parent(tsk, &dead);",
            "",
            "\tif (group_dead)",
            "\t\tkill_orphaned_pgrp(tsk->group_leader, NULL);",
            "",
            "\ttsk->exit_state = EXIT_ZOMBIE;",
            "\t/*",
            "\t * sub-thread or delay_group_leader(), wake up the",
            "\t * PIDFD_THREAD waiters.",
            "\t */",
            "\tif (!thread_group_empty(tsk))",
            "\t\tdo_notify_pidfd(tsk);",
            "",
            "\tif (unlikely(tsk->ptrace)) {",
            "\t\tint sig = thread_group_leader(tsk) &&",
            "\t\t\t\tthread_group_empty(tsk) &&",
            "\t\t\t\t!ptrace_reparented(tsk) ?",
            "\t\t\ttsk->exit_signal : SIGCHLD;",
            "\t\tautoreap = do_notify_parent(tsk, sig);",
            "\t} else if (thread_group_leader(tsk)) {",
            "\t\tautoreap = thread_group_empty(tsk) &&",
            "\t\t\tdo_notify_parent(tsk, tsk->exit_signal);",
            "\t} else {",
            "\t\tautoreap = true;",
            "\t}",
            "",
            "\tif (autoreap) {",
            "\t\ttsk->exit_state = EXIT_DEAD;",
            "\t\tlist_add(&tsk->ptrace_entry, &dead);",
            "\t}",
            "",
            "\t/* mt-exec, de_thread() is waiting for group leader */",
            "\tif (unlikely(tsk->signal->notify_count < 0))",
            "\t\twake_up_process(tsk->signal->group_exec_task);",
            "\twrite_unlock_irq(&tasklist_lock);",
            "",
            "\tlist_for_each_entry_safe(p, n, &dead, ptrace_entry) {",
            "\t\tlist_del_init(&p->ptrace_entry);",
            "\t\trelease_task(p);",
            "\t}",
            "}"
          ],
          "function_name": "exit_mm, reparent_leader, forget_original_parent, exit_notify",
          "description": "完成内存映射释放、父进程重定位、原始父进程解除关联及进程退出状态通知流程。",
          "similarity": 0.6260838508605957
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/exit.c",
          "start_line": 1494,
          "end_line": 1580,
          "content": [
            "static int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)",
            "{",
            "\tstruct task_struct *p;",
            "",
            "\tlist_for_each_entry(p, &tsk->children, sibling) {",
            "\t\tint ret = wait_consider_task(wo, 0, p);",
            "",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)",
            "{",
            "\tstruct task_struct *p;",
            "",
            "\tlist_for_each_entry(p, &tsk->ptraced, ptrace_entry) {",
            "\t\tint ret = wait_consider_task(wo, 1, p);",
            "",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "bool pid_child_should_wake(struct wait_opts *wo, struct task_struct *p)",
            "{",
            "\tif (!eligible_pid(wo, p))",
            "\t\treturn false;",
            "",
            "\tif ((wo->wo_flags & __WNOTHREAD) && wo->child_wait.private != p->parent)",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static int child_wait_callback(wait_queue_entry_t *wait, unsigned mode,",
            "\t\t\t\tint sync, void *key)",
            "{",
            "\tstruct wait_opts *wo = container_of(wait, struct wait_opts,",
            "\t\t\t\t\t\tchild_wait);",
            "\tstruct task_struct *p = key;",
            "",
            "\tif (pid_child_should_wake(wo, p))",
            "\t\treturn default_wake_function(wait, mode, sync, key);",
            "",
            "\treturn 0;",
            "}",
            "void __wake_up_parent(struct task_struct *p, struct task_struct *parent)",
            "{",
            "\t__wake_up_sync_key(&parent->signal->wait_chldexit,",
            "\t\t\t   TASK_INTERRUPTIBLE, p);",
            "}",
            "static bool is_effectively_child(struct wait_opts *wo, bool ptrace,",
            "\t\t\t\t struct task_struct *target)",
            "{",
            "\tstruct task_struct *parent =",
            "\t\t!ptrace ? target->real_parent : target->parent;",
            "",
            "\treturn current == parent || (!(wo->wo_flags & __WNOTHREAD) &&",
            "\t\t\t\t     same_thread_group(current, parent));",
            "}",
            "static int do_wait_pid(struct wait_opts *wo)",
            "{",
            "\tbool ptrace;",
            "\tstruct task_struct *target;",
            "\tint retval;",
            "",
            "\tptrace = false;",
            "\ttarget = pid_task(wo->wo_pid, PIDTYPE_TGID);",
            "\tif (target && is_effectively_child(wo, ptrace, target)) {",
            "\t\tretval = wait_consider_task(wo, ptrace, target);",
            "\t\tif (retval)",
            "\t\t\treturn retval;",
            "\t}",
            "",
            "\tptrace = true;",
            "\ttarget = pid_task(wo->wo_pid, PIDTYPE_PID);",
            "\tif (target && target->ptrace &&",
            "\t    is_effectively_child(wo, ptrace, target)) {",
            "\t\tretval = wait_consider_task(wo, ptrace, target);",
            "\t\tif (retval)",
            "\t\t\treturn retval;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "do_wait_thread, ptrace_do_wait, pid_child_should_wake, child_wait_callback, __wake_up_parent, is_effectively_child, do_wait_pid",
          "description": "该代码段实现了父进程对子进程退出状态的等待逻辑，包含线程组子进程和被ptrace跟踪子进程的处理。do_wait_thread和ptrace_do_wait遍历子进程列表并调用wait_consider_task检查是否满足等待条件，而child_wait_callback与__wake_up_parent协同完成子进程退出时的唤醒机制。is_effectively_child用于判定当前进程是否为有效子进程，do_wait_pid则根据PID类型选择具体的目标子进程进行等待。",
          "similarity": 0.5786629915237427
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/exit.c",
          "start_line": 354,
          "end_line": 526,
          "content": [
            "int is_current_pgrp_orphaned(void)",
            "{",
            "\tint retval;",
            "",
            "\tread_lock(&tasklist_lock);",
            "\tretval = will_become_orphaned_pgrp(task_pgrp(current), NULL);",
            "\tread_unlock(&tasklist_lock);",
            "",
            "\treturn retval;",
            "}",
            "static bool has_stopped_jobs(struct pid *pgrp)",
            "{",
            "\tstruct task_struct *p;",
            "",
            "\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {",
            "\t\tif (p->signal->flags & SIGNAL_STOP_STOPPED)",
            "\t\t\treturn true;",
            "\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);",
            "",
            "\treturn false;",
            "}",
            "static void",
            "kill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)",
            "{",
            "\tstruct pid *pgrp = task_pgrp(tsk);",
            "\tstruct task_struct *ignored_task = tsk;",
            "",
            "\tif (!parent)",
            "\t\t/* exit: our father is in a different pgrp than",
            "\t\t * we are and we were the only connection outside.",
            "\t\t */",
            "\t\tparent = tsk->real_parent;",
            "\telse",
            "\t\t/* reparent: our child is in a different pgrp than",
            "\t\t * we are, and it was the only connection outside.",
            "\t\t */",
            "\t\tignored_task = NULL;",
            "",
            "\tif (task_pgrp(parent) != pgrp &&",
            "\t    task_session(parent) == task_session(tsk) &&",
            "\t    will_become_orphaned_pgrp(pgrp, ignored_task) &&",
            "\t    has_stopped_jobs(pgrp)) {",
            "\t\t__kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);",
            "\t\t__kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);",
            "\t}",
            "}",
            "static void coredump_task_exit(struct task_struct *tsk)",
            "{",
            "\tstruct core_state *core_state;",
            "",
            "\t/*",
            "\t * Serialize with any possible pending coredump.",
            "\t * We must hold siglock around checking core_state",
            "\t * and setting PF_POSTCOREDUMP.  The core-inducing thread",
            "\t * will increment ->nr_threads for each thread in the",
            "\t * group without PF_POSTCOREDUMP set.",
            "\t */",
            "\tspin_lock_irq(&tsk->sighand->siglock);",
            "\ttsk->flags |= PF_POSTCOREDUMP;",
            "\tcore_state = tsk->signal->core_state;",
            "\tspin_unlock_irq(&tsk->sighand->siglock);",
            "",
            "\t/* The vhost_worker does not particpate in coredumps */",
            "\tif (core_state &&",
            "\t    ((tsk->flags & (PF_IO_WORKER | PF_USER_WORKER)) != PF_USER_WORKER)) {",
            "\t\tstruct core_thread self;",
            "",
            "\t\tself.task = current;",
            "\t\tif (self.task->flags & PF_SIGNALED)",
            "\t\t\tself.next = xchg(&core_state->dumper.next, &self);",
            "\t\telse",
            "\t\t\tself.task = NULL;",
            "\t\t/*",
            "\t\t * Implies mb(), the result of xchg() must be visible",
            "\t\t * to core_state->dumper.",
            "\t\t */",
            "\t\tif (atomic_dec_and_test(&core_state->nr_threads))",
            "\t\t\tcomplete(&core_state->startup);",
            "",
            "\t\tfor (;;) {",
            "\t\t\tset_current_state(TASK_UNINTERRUPTIBLE|TASK_FREEZABLE);",
            "\t\t\tif (!self.task) /* see coredump_finish() */",
            "\t\t\t\tbreak;",
            "\t\t\tschedule();",
            "\t\t}",
            "\t\t__set_current_state(TASK_RUNNING);",
            "\t}",
            "}",
            "void mm_update_next_owner(struct mm_struct *mm)",
            "{",
            "\tstruct task_struct *c, *g, *p = current;",
            "",
            "retry:",
            "\t/*",
            "\t * If the exiting or execing task is not the owner, it's",
            "\t * someone else's problem.",
            "\t */",
            "\tif (mm->owner != p)",
            "\t\treturn;",
            "\t/*",
            "\t * The current owner is exiting/execing and there are no other",
            "\t * candidates.  Do not leave the mm pointing to a possibly",
            "\t * freed task structure.",
            "\t */",
            "\tif (atomic_read(&mm->mm_users) <= 1) {",
            "\t\tWRITE_ONCE(mm->owner, NULL);",
            "\t\treturn;",
            "\t}",
            "",
            "\tread_lock(&tasklist_lock);",
            "\t/*",
            "\t * Search in the children",
            "\t */",
            "\tlist_for_each_entry(c, &p->children, sibling) {",
            "\t\tif (c->mm == mm)",
            "\t\t\tgoto assign_new_owner;",
            "\t}",
            "",
            "\t/*",
            "\t * Search in the siblings",
            "\t */",
            "\tlist_for_each_entry(c, &p->real_parent->children, sibling) {",
            "\t\tif (c->mm == mm)",
            "\t\t\tgoto assign_new_owner;",
            "\t}",
            "",
            "\t/*",
            "\t * Search through everything else, we should not get here often.",
            "\t */",
            "\tfor_each_process(g) {",
            "\t\tif (atomic_read(&mm->mm_users) <= 1)",
            "\t\t\tbreak;",
            "\t\tif (g->flags & PF_KTHREAD)",
            "\t\t\tcontinue;",
            "\t\tfor_each_thread(g, c) {",
            "\t\t\tif (c->mm == mm)",
            "\t\t\t\tgoto assign_new_owner;",
            "\t\t\tif (c->mm)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tread_unlock(&tasklist_lock);",
            "\t/*",
            "\t * We found no owner yet mm_users > 1: this implies that we are",
            "\t * most likely racing with swapoff (try_to_unuse()) or /proc or",
            "\t * ptrace or page migration (get_task_mm()).  Mark owner as NULL.",
            "\t */",
            "\tWRITE_ONCE(mm->owner, NULL);",
            "\treturn;",
            "",
            "assign_new_owner:",
            "\tBUG_ON(c == p);",
            "\tget_task_struct(c);",
            "\t/*",
            "\t * The task_lock protects c->mm from changing.",
            "\t * We always want mm->owner->mm == mm",
            "\t */",
            "\ttask_lock(c);",
            "\t/*",
            "\t * Delay read_unlock() till we have the task_lock()",
            "\t * to ensure that c does not slip away underneath us",
            "\t */",
            "\tread_unlock(&tasklist_lock);",
            "\tif (c->mm != mm) {",
            "\t\ttask_unlock(c);",
            "\t\tput_task_struct(c);",
            "\t\tgoto retry;",
            "\t}",
            "\tWRITE_ONCE(mm->owner, c);",
            "\tlru_gen_migrate_mm(mm);",
            "\ttask_unlock(c);",
            "\tput_task_struct(c);",
            "}"
          ],
          "function_name": "is_current_pgrp_orphaned, has_stopped_jobs, kill_orphaned_pgrp, coredump_task_exit, mm_update_next_owner",
          "description": "实现进程组孤儿检测、停止状态判断、异常终止信号分发及核心转储协调逻辑。",
          "similarity": 0.5203055143356323
        }
      ]
    },
    {
      "source_file": "kernel/sched/ext.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:08:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\ext.c`\n\n---\n\n# `sched/ext.c` 技术文档\n\n## 文件概述\n\n`sched/ext.c` 是 Linux 内核中 **BPF 可扩展调度器（sched_ext）** 的核心实现文件之一，定义了调度器与 BPF 程序交互所需的数据结构、常量和操作接口。该文件为用户空间通过 BPF 实现自定义调度策略提供了内核侧的框架支持，允许将任务调度逻辑完全委托给加载的 BPF 程序，同时保留与内核调度子系统的安全集成。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`struct sched_ext_ops`**  \n  BPF 调度器的操作函数表，包含调度器必须或可选实现的回调函数，如 `select_cpu`、`enqueue`、`dequeue`、`dispatch` 等，用于控制任务的 CPU 选择、入队、出队和分发逻辑。\n\n- **`struct scx_exit_info`**  \n  描述 BPF 调度器退出原因的结构体，包含退出类型（`kind`）、退出码（`exit_code`）、错误信息（`reason`、`msg`）、回溯栈（`bt`）和调试转储（`dump`）。\n\n- **`struct scx_init_task_args` / `scx_exit_task_args`**  \n  分别用于 `ops.init_task()` 和 `ops.exit_task()` 回调的参数容器，传递任务初始化/退出上下文（如是否由 fork 触发、所属 cgroup 等）。\n\n- **`struct scx_cpu_acquire_args` / `scx_cpu_release_args`**  \n  用于 CPU 获取/释放回调的参数结构，其中 `cpu_release` 包含抢占原因（如 RT/DL 任务抢占）和即将运行的任务。\n\n- **`struct scx_dump_ctx`**  \n  为调度器转储（dump）操作提供上下文信息，包括退出类型、时间戳等。\n\n### 关键枚举与常量\n\n- **`enum scx_exit_kind`**  \n  定义调度器退出的类别，如正常退出（`SCX_EXIT_DONE`）、用户/BPF/内核主动注销（`SCX_EXIT_UNREG*`）、系统请求（`SCX_EXIT_SYSRQ`）或运行时错误（`SCX_EXIT_ERROR*`）。\n\n- **`enum scx_exit_code`**  \n  定义 64 位退出码的位域格式，支持系统原因（如 `SCX_ECODE_RSN_HOTPLUG`）和系统动作（如 `SCX_ECODE_ACT_RESTART`），允许用户自定义退出上下文。\n\n- **`enum scx_ops_flags`**  \n  调度器操作标志，控制调度行为：\n  - `SCX_OPS_KEEP_BUILTIN_IDLE`：保留内建空闲跟踪\n  - `SCX_OPS_ENQ_LAST`：切片到期后仍无任务时重新入队\n  - `SCX_OPS_ENQ_EXITING`：由 BPF 处理退出中任务\n  - `SCX_OPS_SWITCH_PARTIAL`：仅调度 `SCHED_EXT` 策略任务\n  - `SCX_OPS_HAS_CGROUP_WEIGHT`：支持 cgroup cpu.weight\n\n- **调度器常量**  \n  如 `SCX_DSP_DFL_MAX_BATCH`（默认分发批大小）、`SCX_WATCHDOG_MAX_TIMEOUT`（看门狗超时）、`SCX_OPS_TASK_ITER_BATCH`（任务迭代锁释放批次）等，用于控制调度器内部行为。\n\n## 关键实现\n\n- **BPF 调度器生命周期管理**  \n  通过 `scx_exit_info` 和退出码机制，支持多种退出路径（用户、BPF、内核、SysRq、错误），并提供详细的诊断信息（回溯、消息、转储）。\n\n- **任务入队优化**  \n  在 `select_cpu` 中允许直接插入 DSQ（如本地 DSQ），跳过后续 `enqueue` 调用，减少调度开销；同时通过 `SCX_OPS_ENQ_EXITING` 标志处理退出中任务的调度问题，避免 RCU 停顿。\n\n- **CPU 抢占通知**  \n  通过 `scx_cpu_release_args` 向 BPF 调度器传递 CPU 被高优先级调度类（RT/DL/Stop）抢占的原因，便于调度器做出相应调整。\n\n- **cgroup 集成**  \n  支持 cgroup 调度（`CONFIG_EXT_GROUP_SCHED`），在任务加入 cgroup 时传递权重信息（`scx_cgroup_init_args`），并通过 `SCX_OPS_HAS_CGROUP_WEIGHT` 标志启用。\n\n- **安全与鲁棒性**  \n  内核侧跟踪 BPF 是否拥有任务，可忽略无效分发；任务迭代时定期释放锁（`SCX_OPS_TASK_ITER_BATCH`），防止 RCU/CSD 停顿；看门狗机制（`SCX_WATCHDOG_MAX_TIMEOUT`）检测任务卡死。\n\n## 依赖关系\n\n- **BPF 子系统**：通过 `#include <linux/bpf.h>` 依赖 BPF 基础设施，用于加载和验证调度器 BPF 程序。\n- **调度核心**：与 `kernel/sched/` 下的核心调度代码（如 `core.c`、`rt.c`、`dl.c`）交互，处理任务入队、CPU 选择和抢占。\n- **cgroup 子系统**：当启用 `CONFIG_EXT_GROUP_SCHED` 时，依赖 cgroup CPU 控制器获取任务权重和层级信息。\n- **RCU 与锁机制**：使用 `scx_tasks_lock` 保护任务迭代，需与 RCU 同步机制协调。\n\n## 使用场景\n\n- **自定义调度策略开发**：用户通过 BPF 实现特定工作负载的调度逻辑（如延迟敏感型、批处理优化、NUMA 感知等），并注册到 `sched_ext`。\n- **系统调试与监控**：利用 `ops.dump()` 和退出信息结构体，在调度器异常退出时收集诊断数据。\n- **混合调度部署**：通过 `SCX_OPS_SWITCH_PARTIAL` 标志，仅对部分任务（`SCHED_EXT`）启用 BPF 调度，其余任务仍由 CFS 处理。\n- **资源隔离与 QoS**：结合 cgroup 支持，为不同 cgroup 配置不同的调度行为和资源权重。\n- **内核调度实验平台**：作为安全的沙箱环境，测试新型调度算法而无需修改核心调度代码。",
      "similarity": 0.5674186944961548,
      "chunks": [
        {
          "chunk_id": 22,
          "file_path": "kernel/sched/ext.c",
          "start_line": 4366,
          "end_line": 4528,
          "content": [
            "static void free_exit_info(struct scx_exit_info *ei)",
            "{",
            "\tkfree(ei->dump);",
            "\tkfree(ei->msg);",
            "\tkfree(ei->bt);",
            "\tkfree(ei);",
            "}",
            "static void scx_ops_disable_workfn(struct kthread_work *work)",
            "{",
            "\tstruct scx_exit_info *ei = scx_exit_info;",
            "\tstruct scx_task_iter sti;",
            "\tstruct task_struct *p;",
            "\tstruct rhashtable_iter rht_iter;",
            "\tstruct scx_dispatch_q *dsq;",
            "\tint i, kind;",
            "",
            "\tkind = atomic_read(&scx_exit_kind);",
            "\twhile (true) {",
            "\t\t/*",
            "\t\t * NONE indicates that a new scx_ops has been registered since",
            "\t\t * disable was scheduled - don't kill the new ops. DONE",
            "\t\t * indicates that the ops has already been disabled.",
            "\t\t */",
            "\t\tif (kind == SCX_EXIT_NONE || kind == SCX_EXIT_DONE)",
            "\t\t\treturn;",
            "\t\tif (atomic_try_cmpxchg(&scx_exit_kind, &kind, SCX_EXIT_DONE))",
            "\t\t\tbreak;",
            "\t}",
            "\tei->kind = kind;",
            "\tei->reason = scx_exit_reason(ei->kind);",
            "",
            "\t/* guarantee forward progress by bypassing scx_ops */",
            "\tscx_ops_bypass(true);",
            "",
            "\tswitch (scx_ops_set_enable_state(SCX_OPS_DISABLING)) {",
            "\tcase SCX_OPS_DISABLING:",
            "\t\tWARN_ONCE(true, \"sched_ext: duplicate disabling instance?\");",
            "\t\tbreak;",
            "\tcase SCX_OPS_DISABLED:",
            "\t\tpr_warn(\"sched_ext: ops error detected without ops (%s)\\n\",",
            "\t\t\tscx_exit_info->msg);",
            "\t\tWARN_ON_ONCE(scx_ops_set_enable_state(SCX_OPS_DISABLED) !=",
            "\t\t\t     SCX_OPS_DISABLING);",
            "\t\tgoto done;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "",
            "\t/*",
            "\t * Here, every runnable task is guaranteed to make forward progress and",
            "\t * we can safely use blocking synchronization constructs. Actually",
            "\t * disable ops.",
            "\t */",
            "\tmutex_lock(&scx_ops_enable_mutex);",
            "",
            "\tstatic_branch_disable(&__scx_switched_all);",
            "\tWRITE_ONCE(scx_switching_all, false);",
            "",
            "\t/*",
            "\t * Shut down cgroup support before tasks so that the cgroup attach path",
            "\t * doesn't race against scx_ops_exit_task().",
            "\t */",
            "\tscx_cgroup_lock();",
            "\tscx_cgroup_exit();",
            "\tscx_cgroup_unlock();",
            "",
            "\t/*",
            "\t * The BPF scheduler is going away. All tasks including %TASK_DEAD ones",
            "\t * must be switched out and exited synchronously.",
            "\t */",
            "\tpercpu_down_write(&scx_fork_rwsem);",
            "",
            "\tscx_ops_init_task_enabled = false;",
            "",
            "\tscx_task_iter_start(&sti);",
            "\twhile ((p = scx_task_iter_next_locked(&sti))) {",
            "\t\tconst struct sched_class *old_class = p->sched_class;",
            "\t\tconst struct sched_class *new_class =",
            "\t\t\t__setscheduler_class(p->policy, p->prio);",
            "\t\tstruct sched_enq_and_set_ctx ctx;",
            "",
            "\t\tif (old_class != new_class && p->se.sched_delayed)",
            "\t\t\tdequeue_task(task_rq(p), p, DEQUEUE_SLEEP | DEQUEUE_DELAYED);",
            "",
            "\t\tsched_deq_and_put_task(p, DEQUEUE_SAVE | DEQUEUE_MOVE, &ctx);",
            "",
            "\t\tp->sched_class = new_class;",
            "\t\tcheck_class_changing(task_rq(p), p, old_class);",
            "",
            "\t\tsched_enq_and_set_task(&ctx);",
            "",
            "\t\tcheck_class_changed(task_rq(p), p, old_class, p->prio);",
            "\t\tscx_ops_exit_task(p);",
            "\t}",
            "\tscx_task_iter_stop(&sti);",
            "\tpercpu_up_write(&scx_fork_rwsem);",
            "",
            "\t/* no task is on scx, turn off all the switches and flush in-progress calls */",
            "\tstatic_branch_disable(&__scx_ops_enabled);",
            "\tfor (i = SCX_OPI_BEGIN; i < SCX_OPI_END; i++)",
            "\t\tstatic_branch_disable(&scx_has_op[i]);",
            "\tstatic_branch_disable(&scx_ops_enq_last);",
            "\tstatic_branch_disable(&scx_ops_enq_exiting);",
            "\tstatic_branch_disable(&scx_ops_cpu_preempt);",
            "\tstatic_branch_disable(&scx_builtin_idle_enabled);",
            "\tsynchronize_rcu();",
            "",
            "\tif (ei->kind >= SCX_EXIT_ERROR) {",
            "\t\tpr_err(\"sched_ext: BPF scheduler \\\"%s\\\" disabled (%s)\\n\",",
            "\t\t       scx_ops.name, ei->reason);",
            "",
            "\t\tif (ei->msg[0] != '\\0')",
            "\t\t\tpr_err(\"sched_ext: %s: %s\\n\", scx_ops.name, ei->msg);",
            "#ifdef CONFIG_STACKTRACE",
            "\t\tstack_trace_print(ei->bt, ei->bt_len, 2);",
            "#endif",
            "\t} else {",
            "\t\tpr_info(\"sched_ext: BPF scheduler \\\"%s\\\" disabled (%s)\\n\",",
            "\t\t\tscx_ops.name, ei->reason);",
            "\t}",
            "",
            "\tif (scx_ops.exit)",
            "\t\tSCX_CALL_OP(SCX_KF_UNLOCKED, exit, ei);",
            "",
            "\tcancel_delayed_work_sync(&scx_watchdog_work);",
            "",
            "\t/*",
            "\t * Delete the kobject from the hierarchy eagerly in addition to just",
            "\t * dropping a reference. Otherwise, if the object is deleted",
            "\t * asynchronously, sysfs could observe an object of the same name still",
            "\t * in the hierarchy when another scheduler is loaded.",
            "\t */",
            "\tkobject_del(scx_root_kobj);",
            "\tkobject_put(scx_root_kobj);",
            "\tscx_root_kobj = NULL;",
            "",
            "\tmemset(&scx_ops, 0, sizeof(scx_ops));",
            "",
            "\trhashtable_walk_enter(&dsq_hash, &rht_iter);",
            "\tdo {",
            "\t\trhashtable_walk_start(&rht_iter);",
            "",
            "\t\twhile ((dsq = rhashtable_walk_next(&rht_iter)) && !IS_ERR(dsq))",
            "\t\t\tdestroy_dsq(dsq->id);",
            "",
            "\t\trhashtable_walk_stop(&rht_iter);",
            "\t} while (dsq == ERR_PTR(-EAGAIN));",
            "\trhashtable_walk_exit(&rht_iter);",
            "",
            "\tfree_percpu(scx_dsp_ctx);",
            "\tscx_dsp_ctx = NULL;",
            "\tscx_dsp_max_batch = 0;",
            "",
            "\tfree_exit_info(scx_exit_info);",
            "\tscx_exit_info = NULL;",
            "",
            "\tmutex_unlock(&scx_ops_enable_mutex);",
            "",
            "\tWARN_ON_ONCE(scx_ops_set_enable_state(SCX_OPS_DISABLED) !=",
            "\t\t     SCX_OPS_DISABLING);",
            "done:",
            "\tscx_ops_bypass(false);",
            "}"
          ],
          "function_name": "free_exit_info, scx_ops_disable_workfn",
          "description": "该代码段包含两个函数：  \n1. `free_exit_info` 用于释放 `scx_exit_info` 结构体关联的动态内存（`dump`/`msg`/`bt`）及自身；  \n2. `scx_ops_disable_workfn` 核心功能为安全禁用 SCX 操作，通过原子操作标记状态、强制切换所有任务至新调度类、清理 DSQ 和 kobject 等资源，并最终释放 `scx_exit_info`。  \n\n`scx_ops_disable_workfn` 实现了 SCX 操作的有序停用逻辑，确保任务迁移、状态同步及资源回收的完整性，同时处理异常情况下的错误日志输出。",
          "similarity": 0.5629723072052002
        },
        {
          "chunk_id": 16,
          "file_path": "kernel/sched/ext.c",
          "start_line": 3513,
          "end_line": 3625,
          "content": [
            "static void scx_ops_disable_task(struct task_struct *p)",
            "{",
            "\tlockdep_assert_rq_held(task_rq(p));",
            "\tWARN_ON_ONCE(scx_get_task_state(p) != SCX_TASK_ENABLED);",
            "",
            "\tif (SCX_HAS_OP(disable))",
            "\t\tSCX_CALL_OP(SCX_KF_REST, disable, p);",
            "\tscx_set_task_state(p, SCX_TASK_READY);",
            "}",
            "static void scx_ops_exit_task(struct task_struct *p)",
            "{",
            "\tstruct scx_exit_task_args args = {",
            "\t\t.cancelled = false,",
            "\t};",
            "",
            "\tlockdep_assert_rq_held(task_rq(p));",
            "",
            "\tswitch (scx_get_task_state(p)) {",
            "\tcase SCX_TASK_NONE:",
            "\t\treturn;",
            "\tcase SCX_TASK_INIT:",
            "\t\targs.cancelled = true;",
            "\t\tbreak;",
            "\tcase SCX_TASK_READY:",
            "\t\tbreak;",
            "\tcase SCX_TASK_ENABLED:",
            "\t\tscx_ops_disable_task(p);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(true);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (SCX_HAS_OP(exit_task))",
            "\t\tSCX_CALL_OP(SCX_KF_REST, exit_task, p, &args);",
            "\tscx_set_task_state(p, SCX_TASK_NONE);",
            "}",
            "void init_scx_entity(struct sched_ext_entity *scx)",
            "{",
            "\t/*",
            "\t * init_idle() calls this function again after fork sequence is",
            "\t * complete. Don't touch ->tasks_node as it's already linked.",
            "\t */",
            "\tmemset(scx, 0, offsetof(struct sched_ext_entity, tasks_node));",
            "",
            "\tINIT_LIST_HEAD(&scx->dsq_list.node);",
            "\tRB_CLEAR_NODE(&scx->dsq_priq);",
            "\tscx->sticky_cpu = -1;",
            "\tscx->holding_cpu = -1;",
            "\tINIT_LIST_HEAD(&scx->runnable_node);",
            "\tscx->runnable_at = jiffies;",
            "\tscx->ddsp_dsq_id = SCX_DSQ_INVALID;",
            "\tscx->slice = SCX_SLICE_DFL;",
            "}",
            "void scx_pre_fork(struct task_struct *p)",
            "{",
            "\t/*",
            "\t * BPF scheduler enable/disable paths want to be able to iterate and",
            "\t * update all tasks which can become complex when racing forks. As",
            "\t * enable/disable are very cold paths, let's use a percpu_rwsem to",
            "\t * exclude forks.",
            "\t */",
            "\tpercpu_down_read(&scx_fork_rwsem);",
            "}",
            "int scx_fork(struct task_struct *p)",
            "{",
            "\tpercpu_rwsem_assert_held(&scx_fork_rwsem);",
            "",
            "\tif (scx_ops_init_task_enabled)",
            "\t\treturn scx_ops_init_task(p, task_group(p), true);",
            "\telse",
            "\t\treturn 0;",
            "}",
            "void scx_post_fork(struct task_struct *p)",
            "{",
            "\tif (scx_ops_init_task_enabled) {",
            "\t\tscx_set_task_state(p, SCX_TASK_READY);",
            "",
            "\t\t/*",
            "\t\t * Enable the task immediately if it's running on sched_ext.",
            "\t\t * Otherwise, it'll be enabled in switching_to_scx() if and",
            "\t\t * when it's ever configured to run with a SCHED_EXT policy.",
            "\t\t */",
            "\t\tif (p->sched_class == &ext_sched_class) {",
            "\t\t\tstruct rq_flags rf;",
            "\t\t\tstruct rq *rq;",
            "",
            "\t\t\trq = task_rq_lock(p, &rf);",
            "\t\t\tscx_ops_enable_task(p);",
            "\t\t\ttask_rq_unlock(rq, p, &rf);",
            "\t\t}",
            "\t}",
            "",
            "\tspin_lock_irq(&scx_tasks_lock);",
            "\tlist_add_tail(&p->scx.tasks_node, &scx_tasks);",
            "\tspin_unlock_irq(&scx_tasks_lock);",
            "",
            "\tpercpu_up_read(&scx_fork_rwsem);",
            "}",
            "void scx_cancel_fork(struct task_struct *p)",
            "{",
            "\tif (scx_enabled()) {",
            "\t\tstruct rq *rq;",
            "\t\tstruct rq_flags rf;",
            "",
            "\t\trq = task_rq_lock(p, &rf);",
            "\t\tWARN_ON_ONCE(scx_get_task_state(p) >= SCX_TASK_READY);",
            "\t\tscx_ops_exit_task(p);",
            "\t\ttask_rq_unlock(rq, p, &rf);",
            "\t}",
            "",
            "\tpercpu_up_read(&scx_fork_rwsem);",
            "}"
          ],
          "function_name": "scx_ops_disable_task, scx_ops_exit_task, init_scx_entity, scx_pre_fork, scx_fork, scx_post_fork, scx_cancel_fork",
          "description": "提供任务禁用退出逻辑和fork流程控制，通过读锁保护并发访问，在分叉前后调整任务状态并维护全局任务列表。",
          "similarity": 0.5387437343597412
        },
        {
          "chunk_id": 28,
          "file_path": "kernel/sched/ext.c",
          "start_line": 5514,
          "end_line": 5615,
          "content": [
            "static int bpf_scx_validate(void *kdata)",
            "{",
            "\treturn 0;",
            "}",
            "static s32 select_cpu_stub(struct task_struct *p, s32 prev_cpu, u64 wake_flags) { return -EINVAL; }",
            "static void enqueue_stub(struct task_struct *p, u64 enq_flags) {}",
            "static void dequeue_stub(struct task_struct *p, u64 enq_flags) {}",
            "static void dispatch_stub(s32 prev_cpu, struct task_struct *p) {}",
            "static void tick_stub(struct task_struct *p) {}",
            "static void runnable_stub(struct task_struct *p, u64 enq_flags) {}",
            "static void running_stub(struct task_struct *p) {}",
            "static void stopping_stub(struct task_struct *p, bool runnable) {}",
            "static void quiescent_stub(struct task_struct *p, u64 deq_flags) {}",
            "static bool yield_stub(struct task_struct *from, struct task_struct *to) { return false; }",
            "static bool core_sched_before_stub(struct task_struct *a, struct task_struct *b) { return false; }",
            "static void set_weight_stub(struct task_struct *p, u32 weight) {}",
            "static void set_cpumask_stub(struct task_struct *p, const struct cpumask *mask) {}",
            "static void update_idle_stub(s32 cpu, bool idle) {}",
            "static void cpu_acquire_stub(s32 cpu, struct scx_cpu_acquire_args *args) {}",
            "static void cpu_release_stub(s32 cpu, struct scx_cpu_release_args *args) {}",
            "static s32 init_task_stub(struct task_struct *p, struct scx_init_task_args *args) { return -EINVAL; }",
            "static void exit_task_stub(struct task_struct *p, struct scx_exit_task_args *args) {}",
            "static void enable_stub(struct task_struct *p) {}",
            "static void disable_stub(struct task_struct *p) {}",
            "static s32 cgroup_init_stub(struct cgroup *cgrp, struct scx_cgroup_init_args *args) { return -EINVAL; }",
            "static void cgroup_exit_stub(struct cgroup *cgrp) {}",
            "static s32 cgroup_prep_move_stub(struct task_struct *p, struct cgroup *from, struct cgroup *to) { return -EINVAL; }",
            "static void cgroup_move_stub(struct task_struct *p, struct cgroup *from, struct cgroup *to) {}",
            "static void cgroup_cancel_move_stub(struct task_struct *p, struct cgroup *from, struct cgroup *to) {}",
            "static void cgroup_set_weight_stub(struct cgroup *cgrp, u32 weight) {}",
            "static void cpu_online_stub(s32 cpu) {}",
            "static void cpu_offline_stub(s32 cpu) {}",
            "static s32 init_stub(void) { return -EINVAL; }",
            "static void exit_stub(struct scx_exit_info *info) {}",
            "static void dump_stub(struct scx_dump_ctx *ctx) {}",
            "static void dump_cpu_stub(struct scx_dump_ctx *ctx, s32 cpu, bool idle) {}",
            "static void dump_task_stub(struct scx_dump_ctx *ctx, struct task_struct *p) {}",
            "static void sysrq_handle_sched_ext_reset(u8 key)",
            "{",
            "\tif (scx_ops_helper)",
            "\t\tscx_ops_disable(SCX_EXIT_SYSRQ);",
            "\telse",
            "\t\tpr_info(\"sched_ext: BPF scheduler not yet used\\n\");",
            "}",
            "static void sysrq_handle_sched_ext_dump(u8 key)",
            "{",
            "\tstruct scx_exit_info ei = { .kind = SCX_EXIT_NONE, .reason = \"SysRq-D\" };",
            "",
            "\tif (scx_enabled())",
            "\t\tscx_dump_state(&ei, 0);",
            "}",
            "static bool can_skip_idle_kick(struct rq *rq)",
            "{",
            "\tlockdep_assert_rq_held(rq);",
            "",
            "\t/*",
            "\t * We can skip idle kicking if @rq is going to go through at least one",
            "\t * full SCX scheduling cycle before going idle. Just checking whether",
            "\t * curr is not idle is insufficient because we could be racing",
            "\t * balance_one() trying to pull the next task from a remote rq, which",
            "\t * may fail, and @rq may become idle afterwards.",
            "\t *",
            "\t * The race window is small and we don't and can't guarantee that @rq is",
            "\t * only kicked while idle anyway. Skip only when sure.",
            "\t */",
            "\treturn !is_idle_task(rq->curr) && !(rq->scx.flags & SCX_RQ_IN_BALANCE);",
            "}",
            "static bool kick_one_cpu(s32 cpu, struct rq *this_rq, unsigned long *pseqs)",
            "{",
            "\tstruct rq *rq = cpu_rq(cpu);",
            "\tstruct scx_rq *this_scx = &this_rq->scx;",
            "\tbool should_wait = false;",
            "\tunsigned long flags;",
            "",
            "\traw_spin_rq_lock_irqsave(rq, flags);",
            "",
            "\t/*",
            "\t * During CPU hotplug, a CPU may depend on kicking itself to make",
            "\t * forward progress. Allow kicking self regardless of online state.",
            "\t */",
            "\tif (cpu_online(cpu) || cpu == cpu_of(this_rq)) {",
            "\t\tif (cpumask_test_cpu(cpu, this_scx->cpus_to_preempt)) {",
            "\t\t\tif (rq->curr->sched_class == &ext_sched_class)",
            "\t\t\t\trq->curr->scx.slice = 0;",
            "\t\t\tcpumask_clear_cpu(cpu, this_scx->cpus_to_preempt);",
            "\t\t}",
            "",
            "\t\tif (cpumask_test_cpu(cpu, this_scx->cpus_to_wait)) {",
            "\t\t\tpseqs[cpu] = rq->scx.pnt_seq;",
            "\t\t\tshould_wait = true;",
            "\t\t}",
            "",
            "\t\tresched_curr(rq);",
            "\t} else {",
            "\t\tcpumask_clear_cpu(cpu, this_scx->cpus_to_preempt);",
            "\t\tcpumask_clear_cpu(cpu, this_scx->cpus_to_wait);",
            "\t}",
            "",
            "\traw_spin_rq_unlock_irqrestore(rq, flags);",
            "",
            "\treturn should_wait;",
            "}"
          ],
          "function_name": "bpf_scx_validate, select_cpu_stub, enqueue_stub, dequeue_stub, dispatch_stub, tick_stub, runnable_stub, running_stub, stopping_stub, quiescent_stub, yield_stub, core_sched_before_stub, set_weight_stub, set_cpumask_stub, update_idle_stub, cpu_acquire_stub, cpu_release_stub, init_task_stub, exit_task_stub, enable_stub, disable_stub, cgroup_init_stub, cgroup_exit_stub, cgroup_prep_move_stub, cgroup_move_stub, cgroup_cancel_move_stub, cgroup_set_weight_stub, cpu_online_stub, cpu_offline_stub, init_stub, exit_stub, dump_stub, dump_cpu_stub, dump_task_stub, sysrq_handle_sched_ext_reset, sysrq_handle_sched_ext_dump, can_skip_idle_kick, kick_one_cpu",
          "description": "提供调度事件的stub函数实现，处理系统请求(sysrq)和电源管理事件，定义任务状态转换及运行时行为的钩子函数。",
          "similarity": 0.5326024293899536
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/ext.c",
          "start_line": 2075,
          "end_line": 2208,
          "content": [
            "static void clr_task_runnable(struct task_struct *p, bool reset_runnable_at)",
            "{",
            "\tlist_del_init(&p->scx.runnable_node);",
            "\tif (reset_runnable_at)",
            "\t\tp->scx.flags |= SCX_TASK_RESET_RUNNABLE_AT;",
            "}",
            "static void enqueue_task_scx(struct rq *rq, struct task_struct *p, int enq_flags)",
            "{",
            "\tint sticky_cpu = p->scx.sticky_cpu;",
            "",
            "\tif (enq_flags & ENQUEUE_WAKEUP)",
            "\t\trq->scx.flags |= SCX_RQ_IN_WAKEUP;",
            "",
            "\tenq_flags |= rq->scx.extra_enq_flags;",
            "",
            "\tif (sticky_cpu >= 0)",
            "\t\tp->scx.sticky_cpu = -1;",
            "",
            "\t/*",
            "\t * Restoring a running task will be immediately followed by",
            "\t * set_next_task_scx() which expects the task to not be on the BPF",
            "\t * scheduler as tasks can only start running through local DSQs. Force",
            "\t * direct-dispatch into the local DSQ by setting the sticky_cpu.",
            "\t */",
            "\tif (unlikely(enq_flags & ENQUEUE_RESTORE) && task_current(rq, p))",
            "\t\tsticky_cpu = cpu_of(rq);",
            "",
            "\tif (p->scx.flags & SCX_TASK_QUEUED) {",
            "\t\tWARN_ON_ONCE(!task_runnable(p));",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tset_task_runnable(rq, p);",
            "\tp->scx.flags |= SCX_TASK_QUEUED;",
            "\trq->scx.nr_running++;",
            "\tadd_nr_running(rq, 1);",
            "",
            "\tif (SCX_HAS_OP(runnable) && !task_on_rq_migrating(p))",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, runnable, p, enq_flags);",
            "",
            "\tif (enq_flags & SCX_ENQ_WAKEUP)",
            "\t\ttouch_core_sched(rq, p);",
            "",
            "\tdo_enqueue_task(rq, p, enq_flags, sticky_cpu);",
            "out:",
            "\trq->scx.flags &= ~SCX_RQ_IN_WAKEUP;",
            "}",
            "static void ops_dequeue(struct task_struct *p, u64 deq_flags)",
            "{",
            "\tunsigned long opss;",
            "",
            "\t/* dequeue is always temporary, don't reset runnable_at */",
            "\tclr_task_runnable(p, false);",
            "",
            "\t/* acquire ensures that we see the preceding updates on QUEUED */",
            "\topss = atomic_long_read_acquire(&p->scx.ops_state);",
            "",
            "\tswitch (opss & SCX_OPSS_STATE_MASK) {",
            "\tcase SCX_OPSS_NONE:",
            "\t\tbreak;",
            "\tcase SCX_OPSS_QUEUEING:",
            "\t\t/*",
            "\t\t * QUEUEING is started and finished while holding @p's rq lock.",
            "\t\t * As we're holding the rq lock now, we shouldn't see QUEUEING.",
            "\t\t */",
            "\t\tBUG();",
            "\tcase SCX_OPSS_QUEUED:",
            "\t\tif (SCX_HAS_OP(dequeue))",
            "\t\t\tSCX_CALL_OP_TASK(SCX_KF_REST, dequeue, p, deq_flags);",
            "",
            "\t\tif (atomic_long_try_cmpxchg(&p->scx.ops_state, &opss,",
            "\t\t\t\t\t    SCX_OPSS_NONE))",
            "\t\t\tbreak;",
            "\t\tfallthrough;",
            "\tcase SCX_OPSS_DISPATCHING:",
            "\t\t/*",
            "\t\t * If @p is being dispatched from the BPF scheduler to a DSQ,",
            "\t\t * wait for the transfer to complete so that @p doesn't get",
            "\t\t * added to its DSQ after dequeueing is complete.",
            "\t\t *",
            "\t\t * As we're waiting on DISPATCHING with the rq locked, the",
            "\t\t * dispatching side shouldn't try to lock the rq while",
            "\t\t * DISPATCHING is set. See dispatch_to_local_dsq().",
            "\t\t *",
            "\t\t * DISPATCHING shouldn't have qseq set and control can reach",
            "\t\t * here with NONE @opss from the above QUEUED case block.",
            "\t\t * Explicitly wait on %SCX_OPSS_DISPATCHING instead of @opss.",
            "\t\t */",
            "\t\twait_ops_state(p, SCX_OPSS_DISPATCHING);",
            "\t\tBUG_ON(atomic_long_read(&p->scx.ops_state) != SCX_OPSS_NONE);",
            "\t\tbreak;",
            "\t}",
            "}",
            "static bool dequeue_task_scx(struct rq *rq, struct task_struct *p, int deq_flags)",
            "{",
            "\tif (!(p->scx.flags & SCX_TASK_QUEUED)) {",
            "\t\tWARN_ON_ONCE(task_runnable(p));",
            "\t\treturn true;",
            "\t}",
            "",
            "\tops_dequeue(p, deq_flags);",
            "",
            "\t/*",
            "\t * A currently running task which is going off @rq first gets dequeued",
            "\t * and then stops running. As we want running <-> stopping transitions",
            "\t * to be contained within runnable <-> quiescent transitions, trigger",
            "\t * ->stopping() early here instead of in put_prev_task_scx().",
            "\t *",
            "\t * @p may go through multiple stopping <-> running transitions between",
            "\t * here and put_prev_task_scx() if task attribute changes occur while",
            "\t * balance_scx() leaves @rq unlocked. However, they don't contain any",
            "\t * information meaningful to the BPF scheduler and can be suppressed by",
            "\t * skipping the callbacks if the task is !QUEUED.",
            "\t */",
            "\tif (SCX_HAS_OP(stopping) && task_current(rq, p)) {",
            "\t\tupdate_curr_scx(rq);",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, stopping, p, false);",
            "\t}",
            "",
            "\tif (SCX_HAS_OP(quiescent) && !task_on_rq_migrating(p))",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, quiescent, p, deq_flags);",
            "",
            "\tif (deq_flags & SCX_DEQ_SLEEP)",
            "\t\tp->scx.flags |= SCX_TASK_DEQD_FOR_SLEEP;",
            "\telse",
            "\t\tp->scx.flags &= ~SCX_TASK_DEQD_FOR_SLEEP;",
            "",
            "\tp->scx.flags &= ~SCX_TASK_QUEUED;",
            "\trq->scx.nr_running--;",
            "\tsub_nr_running(rq, 1);",
            "",
            "\tdispatch_dequeue(rq, p);",
            "\treturn true;",
            "}"
          ],
          "function_name": "clr_task_runnable, enqueue_task_scx, ops_dequeue, dequeue_task_scx",
          "description": "处理任务入队和出队逻辑，enqueue_task_scx标记任务为排队状态并触发BPF操作，ops_dequeue根据状态执行解队操作，dequeue_task_scx移除任务并通知BPF调度器任务离开运行队列",
          "similarity": 0.5253676176071167
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/sched/ext.c",
          "start_line": 2213,
          "end_line": 2319,
          "content": [
            "static void yield_task_scx(struct rq *rq)",
            "{",
            "\tstruct task_struct *p = rq->curr;",
            "",
            "\tif (SCX_HAS_OP(yield))",
            "\t\tSCX_CALL_OP_2TASKS_RET(SCX_KF_REST, yield, p, NULL);",
            "\telse",
            "\t\tp->scx.slice = 0;",
            "}",
            "static bool yield_to_task_scx(struct rq *rq, struct task_struct *to)",
            "{",
            "\tstruct task_struct *from = rq->curr;",
            "",
            "\tif (SCX_HAS_OP(yield))",
            "\t\treturn SCX_CALL_OP_2TASKS_RET(SCX_KF_REST, yield, from, to);",
            "\telse",
            "\t\treturn false;",
            "}",
            "static void move_local_task_to_local_dsq(struct task_struct *p, u64 enq_flags,",
            "\t\t\t\t\t struct scx_dispatch_q *src_dsq,",
            "\t\t\t\t\t struct rq *dst_rq)",
            "{",
            "\tstruct scx_dispatch_q *dst_dsq = &dst_rq->scx.local_dsq;",
            "",
            "\t/* @dsq is locked and @p is on @dst_rq */",
            "\tlockdep_assert_held(&src_dsq->lock);",
            "\tlockdep_assert_rq_held(dst_rq);",
            "",
            "\tWARN_ON_ONCE(p->scx.holding_cpu >= 0);",
            "",
            "\tif (enq_flags & (SCX_ENQ_HEAD | SCX_ENQ_PREEMPT))",
            "\t\tlist_add(&p->scx.dsq_list.node, &dst_dsq->list);",
            "\telse",
            "\t\tlist_add_tail(&p->scx.dsq_list.node, &dst_dsq->list);",
            "",
            "\tdsq_mod_nr(dst_dsq, 1);",
            "\tp->scx.dsq = dst_dsq;",
            "}",
            "static void move_remote_task_to_local_dsq(struct task_struct *p, u64 enq_flags,",
            "\t\t\t\t\t  struct rq *src_rq, struct rq *dst_rq)",
            "{",
            "\tlockdep_assert_rq_held(src_rq);",
            "",
            "\t/* the following marks @p MIGRATING which excludes dequeue */",
            "\tdeactivate_task(src_rq, p, 0);",
            "\tset_task_cpu(p, cpu_of(dst_rq));",
            "\tp->scx.sticky_cpu = cpu_of(dst_rq);",
            "",
            "\traw_spin_rq_unlock(src_rq);",
            "\traw_spin_rq_lock(dst_rq);",
            "",
            "\t/*",
            "\t * We want to pass scx-specific enq_flags but activate_task() will",
            "\t * truncate the upper 32 bit. As we own @rq, we can pass them through",
            "\t * @rq->scx.extra_enq_flags instead.",
            "\t */",
            "\tWARN_ON_ONCE(!cpumask_test_cpu(cpu_of(dst_rq), p->cpus_ptr));",
            "\tWARN_ON_ONCE(dst_rq->scx.extra_enq_flags);",
            "\tdst_rq->scx.extra_enq_flags = enq_flags;",
            "\tactivate_task(dst_rq, p, 0);",
            "\tdst_rq->scx.extra_enq_flags = 0;",
            "}",
            "static bool task_can_run_on_remote_rq(struct task_struct *p, struct rq *rq,",
            "\t\t\t\t      bool trigger_error)",
            "{",
            "\tint cpu = cpu_of(rq);",
            "",
            "\t/*",
            "\t * We don't require the BPF scheduler to avoid dispatching to offline",
            "\t * CPUs mostly for convenience but also because CPUs can go offline",
            "\t * between scx_bpf_dsq_insert() calls and here. Trigger error iff the",
            "\t * picked CPU is outside the allowed mask.",
            "\t */",
            "\tif (!task_allowed_on_cpu(p, cpu)) {",
            "\t\tif (trigger_error)",
            "\t\t\tscx_ops_error(\"SCX_DSQ_LOCAL[_ON] verdict target cpu %d not allowed for %s[%d]\",",
            "\t\t\t\t      cpu_of(rq), p->comm, p->pid);",
            "\t\treturn false;",
            "\t}",
            "",
            "\tif (unlikely(is_migration_disabled(p)))",
            "\t\treturn false;",
            "",
            "\tif (!scx_rq_online(rq))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static bool unlink_dsq_and_lock_src_rq(struct task_struct *p,",
            "\t\t\t\t       struct scx_dispatch_q *dsq,",
            "\t\t\t\t       struct rq *src_rq)",
            "{",
            "\ts32 cpu = raw_smp_processor_id();",
            "",
            "\tlockdep_assert_held(&dsq->lock);",
            "",
            "\tWARN_ON_ONCE(p->scx.holding_cpu >= 0);",
            "\ttask_unlink_from_dsq(p, dsq);",
            "\tp->scx.holding_cpu = cpu;",
            "",
            "\traw_spin_unlock(&dsq->lock);",
            "\traw_spin_rq_lock(src_rq);",
            "",
            "\t/* task_rq couldn't have changed if we're still the holding cpu */",
            "\treturn likely(p->scx.holding_cpu == cpu) &&",
            "\t\t!WARN_ON_ONCE(src_rq != task_rq(p));",
            "}"
          ],
          "function_name": "yield_task_scx, yield_to_task_scx, move_local_task_to_local_dsq, move_remote_task_to_local_dsq, task_can_run_on_remote_rq, unlink_dsq_and_lock_src_rq",
          "description": "实现任务迁移和本地DSQ操作，yield_task_scx触发BPF的yield回调，move系列函数处理任务在本地/远程DSQ间的移动，task_can_run_on_remote_rq校验任务能否在远程CPU运行",
          "similarity": 0.5148836374282837
        }
      ]
    },
    {
      "source_file": "kernel/sched/completion.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:59:12\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\completion.c`\n\n---\n\n# `sched/completion.c` 技术文档\n\n## 1. 文件概述\n\n`sched/completion.c` 实现了 Linux 内核中的 **completion（完成量）** 同步原语。该机制用于一个或多个线程等待某个事件完成后再继续执行。与信号量（semaphore）不同，completion 的默认行为是阻塞等待，且其语义明确表示“等待某事完成”，而非用于互斥访问，因此避免了因互斥导致的优先级反转问题。completion 支持唤醒单个或所有等待者，并提供多种等待变体（不可中断、可中断、可被 kill、带超时、IO 等待等）。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`complete(struct completion *x)`**  \n  唤醒一个等待在 completion `x` 上的线程（按 FIFO 顺序）。\n\n- **`complete_all(struct completion *x)`**  \n  唤醒所有等待在 `x` 上的线程，并将 `x->done` 设为 `UINT_MAX`，表示永久完成。\n\n- **`complete_on_current_cpu(struct completion *x)`**  \n  在当前 CPU 上唤醒一个等待者，用于优化调度局部性。\n\n- **`wait_for_completion(struct completion *x)`**  \n  不可中断地无限期等待 completion 完成。\n\n- **`wait_for_completion_timeout(...)`**  \n  带超时的不可中断等待。\n\n- **`wait_for_completion_io(...)` / `wait_for_completion_io_timeout(...)`**  \n  用于 IO 上下文的等待，调度器将其归类为 IO 等待。\n\n- **`wait_for_completion_interruptible(...)`**  \n  可被信号中断的等待。\n\n- **`wait_for_completion_interruptible_timeout(...)`**  \n  可中断且带超时的等待。\n\n- **`wait_for_completion_killable(...)`**  \n  仅可被 kill 信号（如 SIGKILL）中断的等待。\n\n### 核心数据结构（隐含）\n\n- **`struct completion`**（定义在 `<linux/completion.h>`）  \n  包含：\n  - `unsigned int done`：完成计数，0 表示未完成，>0 表示可唤醒的等待者数量，`UINT_MAX` 表示已调用 `complete_all()`。\n  - `struct swait_queue_head wait`：基于 simple waitqueue 的等待队列。\n\n## 3. 关键实现\n\n### 完成信号机制\n- `complete()` 和 `complete_on_current_cpu()` 通过 `complete_with_flags()` 实现，增加 `done` 计数（除非已达 `UINT_MAX`），并调用 `swake_up_locked()` 唤醒一个等待者。\n- `complete_all()` 将 `done` 设为 `UINT_MAX`，并调用 `swake_up_all_locked()` 唤醒所有等待者。此后 `done` 不再递减，因此需调用 `reinit_completion()` 才能复用。\n\n### 等待逻辑\n- 所有 `wait_for_*` 函数最终调用 `__wait_for_common()`，后者：\n  1. 调用 `complete_acquire()`（用于 lockdep 跟踪）。\n  2. 获取自旋锁，进入 `do_wait_for_common()`。\n  3. 若 `done == 0`，则加入等待队列，设置任务状态（如 `TASK_UNINTERRUPTIBLE`），释放锁，调用调度函数（如 `schedule_timeout`）。\n  4. 被唤醒后重新获取锁，检查是否完成或超时。\n  5. 若成功完成且未调用 `complete_all()`，则 `done` 计数减 1。\n  6. 调用 `complete_release()` 结束 lockdep 跟踪。\n\n### 内存屏障与调度\n- 唤醒操作（`swake_up*`）内部包含完整的内存屏障，确保唤醒前的写操作对被唤醒任务可见。\n- `complete_on_current_cpu()` 使用 `WF_CURRENT_CPU` 标志，优化唤醒路径，避免跨 CPU 调度开销。\n\n### 中断与超时处理\n- 可中断/可 kill 等待在每次调度前检查信号（`signal_pending_state()`），若存在有效信号则返回 `-ERESTARTSYS`。\n- 超时值以 jiffies 为单位，返回值语义：\n  - `0`：超时；\n  - 正数：剩余 jiffies（至少为 1）；\n  - 负数：错误码（如 `-ERESTARTSYS`）。\n\n## 4. 依赖关系\n\n- **`<linux/completion.h>`**：定义 `struct completion` 及 API 声明。\n- **`<linux/swait.h>`**：使用 simple waitqueue（`swait_queue_head`、`swake_up*` 等）实现高效等待队列。\n- **`<linux/sched.h>`**：依赖任务状态（`TASK_*`）、调度函数（`schedule_timeout`、`io_schedule_timeout`）及内存屏障。\n- **Lockdep**：通过 `complete_acquire()`/`complete_release()` 集成死锁检测。\n- **RT 补丁支持**：`complete_all()` 中包含 `lockdep_assert_RT_in_threaded_ctx()`，用于实时内核上下文检查。\n\n## 5. 使用场景\n\n- **驱动程序同步**：设备操作完成后通知等待线程（如 DMA 传输完成）。\n- **内核线程协调**：一个线程等待另一个线程完成初始化或清理工作。\n- **异步操作完成通知**：如文件系统或网络子系统中等待后台任务结束。\n- **模块卸载同步**：确保所有使用模块的线程退出后再卸载。\n- **IO 路径等待**：使用 `wait_for_completion_io*` 变体，使调度器正确统计 IO 等待时间。\n\n> **注意**：`complete_all()` 后必须调用 `reinit_completion()` 才能复用 completion 对象，且需确保所有等待者已退出。`completion_done()` 不能用于判断 `complete_all()` 后是否仍有等待者。",
      "similarity": 0.565331757068634,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/completion.c",
          "start_line": 16,
          "end_line": 123,
          "content": [
            "static void complete_with_flags(struct completion *x, int wake_flags)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&x->wait.lock, flags);",
            "",
            "\tif (x->done != UINT_MAX)",
            "\t\tx->done++;",
            "\tswake_up_locked(&x->wait, wake_flags);",
            "\traw_spin_unlock_irqrestore(&x->wait.lock, flags);",
            "}",
            "void complete_on_current_cpu(struct completion *x)",
            "{",
            "\treturn complete_with_flags(x, WF_CURRENT_CPU);",
            "}",
            "void complete(struct completion *x)",
            "{",
            "\tcomplete_with_flags(x, 0);",
            "}",
            "void complete_all(struct completion *x)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tlockdep_assert_RT_in_threaded_ctx();",
            "",
            "\traw_spin_lock_irqsave(&x->wait.lock, flags);",
            "\tx->done = UINT_MAX;",
            "\tswake_up_all_locked(&x->wait);",
            "\traw_spin_unlock_irqrestore(&x->wait.lock, flags);",
            "}",
            "static inline long __sched",
            "do_wait_for_common(struct completion *x,",
            "\t\t   long (*action)(long), long timeout, int state)",
            "{",
            "\tif (!x->done) {",
            "\t\tDECLARE_SWAITQUEUE(wait);",
            "",
            "\t\tdo {",
            "\t\t\tif (signal_pending_state(state, current)) {",
            "\t\t\t\ttimeout = -ERESTARTSYS;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t\t__prepare_to_swait(&x->wait, &wait);",
            "\t\t\t__set_current_state(state);",
            "\t\t\traw_spin_unlock_irq(&x->wait.lock);",
            "\t\t\ttimeout = action(timeout);",
            "\t\t\traw_spin_lock_irq(&x->wait.lock);",
            "\t\t} while (!x->done && timeout);",
            "\t\t__finish_swait(&x->wait, &wait);",
            "\t\tif (!x->done)",
            "\t\t\treturn timeout;",
            "\t}",
            "\tif (x->done != UINT_MAX)",
            "\t\tx->done--;",
            "\treturn timeout ?: 1;",
            "}",
            "static inline long __sched",
            "__wait_for_common(struct completion *x,",
            "\t\t  long (*action)(long), long timeout, int state)",
            "{",
            "\tmight_sleep();",
            "",
            "\tcomplete_acquire(x);",
            "",
            "\traw_spin_lock_irq(&x->wait.lock);",
            "\ttimeout = do_wait_for_common(x, action, timeout, state);",
            "\traw_spin_unlock_irq(&x->wait.lock);",
            "",
            "\tcomplete_release(x);",
            "",
            "\treturn timeout;",
            "}",
            "static long __sched",
            "wait_for_common(struct completion *x, long timeout, int state)",
            "{",
            "\treturn __wait_for_common(x, schedule_timeout, timeout, state);",
            "}",
            "static long __sched",
            "wait_for_common_io(struct completion *x, long timeout, int state)",
            "{",
            "\treturn __wait_for_common(x, io_schedule_timeout, timeout, state);",
            "}",
            "void __sched wait_for_completion(struct completion *x)",
            "{",
            "\twait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);",
            "}",
            "unsigned long __sched",
            "wait_for_completion_timeout(struct completion *x, unsigned long timeout)",
            "{",
            "\treturn wait_for_common(x, timeout, TASK_UNINTERRUPTIBLE);",
            "}",
            "void __sched wait_for_completion_io(struct completion *x)",
            "{",
            "\twait_for_common_io(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);",
            "}",
            "unsigned long __sched",
            "wait_for_completion_io_timeout(struct completion *x, unsigned long timeout)",
            "{",
            "\treturn wait_for_common_io(x, timeout, TASK_UNINTERRUPTIBLE);",
            "}",
            "int __sched wait_for_completion_interruptible(struct completion *x)",
            "{",
            "\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_INTERRUPTIBLE);",
            "",
            "\tif (t == -ERESTARTSYS)",
            "\t\treturn t;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "complete_with_flags, complete_on_current_cpu, complete, complete_all, do_wait_for_common, __wait_for_common, wait_for_common, wait_for_common_io, wait_for_completion, wait_for_completion_timeout, wait_for_completion_io, wait_for_completion_io_timeout, wait_for_completion_interruptible",
          "description": "实现completion的完成通知和等待逻辑，包含complete/complete_all用于状态更新，wait_for_common系列函数处理超时/中断/睡眠状态切换，通过锁保护和swaitqueue实现并发控制",
          "similarity": 0.5496379137039185
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/completion.c",
          "start_line": 235,
          "end_line": 300,
          "content": [
            "long __sched",
            "wait_for_completion_interruptible_timeout(struct completion *x,",
            "\t\t\t\t\t  unsigned long timeout)",
            "{",
            "\treturn wait_for_common(x, timeout, TASK_INTERRUPTIBLE);",
            "}",
            "int __sched wait_for_completion_killable(struct completion *x)",
            "{",
            "\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_KILLABLE);",
            "",
            "\tif (t == -ERESTARTSYS)",
            "\t\treturn t;",
            "\treturn 0;",
            "}",
            "int __sched wait_for_completion_state(struct completion *x, unsigned int state)",
            "{",
            "\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, state);",
            "",
            "\tif (t == -ERESTARTSYS)",
            "\t\treturn t;",
            "\treturn 0;",
            "}",
            "long __sched",
            "wait_for_completion_killable_timeout(struct completion *x,",
            "\t\t\t\t     unsigned long timeout)",
            "{",
            "\treturn wait_for_common(x, timeout, TASK_KILLABLE);",
            "}",
            "bool try_wait_for_completion(struct completion *x)",
            "{",
            "\tunsigned long flags;",
            "\tbool ret = true;",
            "",
            "\t/*",
            "\t * Since x->done will need to be locked only",
            "\t * in the non-blocking case, we check x->done",
            "\t * first without taking the lock so we can",
            "\t * return early in the blocking case.",
            "\t */",
            "\tif (!READ_ONCE(x->done))",
            "\t\treturn false;",
            "",
            "\traw_spin_lock_irqsave(&x->wait.lock, flags);",
            "\tif (!x->done)",
            "\t\tret = false;",
            "\telse if (x->done != UINT_MAX)",
            "\t\tx->done--;",
            "\traw_spin_unlock_irqrestore(&x->wait.lock, flags);",
            "\treturn ret;",
            "}",
            "bool completion_done(struct completion *x)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tif (!READ_ONCE(x->done))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * If ->done, we need to wait for complete() to release ->wait.lock",
            "\t * otherwise we can end up freeing the completion before complete()",
            "\t * is done referencing it.",
            "\t */",
            "\traw_spin_lock_irqsave(&x->wait.lock, flags);",
            "\traw_spin_unlock_irqrestore(&x->wait.lock, flags);",
            "\treturn true;",
            "}"
          ],
          "function_name": "wait_for_completion_interruptible_timeout, wait_for_completion_killable, wait_for_completion_state, wait_for_completion_killable_timeout, try_wait_for_completion, completion_done",
          "description": "扩展等待行为控制接口，wait_for_completion_killable处理可终止等待，try_wait_for_completion尝试非阻塞式状态减操作，completion_done验证完成状态并保障内存安全访问",
          "similarity": 0.5189963579177856
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/completion.c",
          "start_line": 1,
          "end_line": 15,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "",
            "/*",
            " * Generic wait-for-completion handler;",
            " *",
            " * It differs from semaphores in that their default case is the opposite,",
            " * wait_for_completion default blocks whereas semaphore default non-block. The",
            " * interface also makes it easy to 'complete' multiple waiting threads,",
            " * something which isn't entirely natural for semaphores.",
            " *",
            " * But more importantly, the primitive documents the usage. Semaphores would",
            " * typically be used for exclusion which gives rise to priority inversion.",
            " * Waiting for completion is a typically sync point, but not an exclusion point.",
            " */",
            ""
          ],
          "function_name": null,
          "description": "定义completion机制的核心概念，作为同步原语用于标记任务完成事件，区别于信号量，默认阻塞等待并支持批量唤醒等待线程，适用于非互斥场景下的同步点管理",
          "similarity": 0.48837560415267944
        }
      ]
    }
  ]
}