{
  "query": "TLB与页表的关系",
  "timestamp": "2025-12-26 00:14:27",
  "retrieved_files": [
    {
      "source_file": "mm/mmu_gather.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:52:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mmu_gather.c`\n\n---\n\n# mmu_gather.c 技术文档\n\n## 1. 文件概述\n\n`mmu_gather.c` 是 Linux 内核内存管理子系统中的关键组件，负责在页表项（PTE）或更高层级页表被撤销映射（unmap）后，高效地批量释放对应的物理页面和页表结构。该文件实现了 **MMU gather** 机制，用于延迟并批量处理 TLB（Translation Lookaside Buffer）刷新、反向映射（rmap）清理以及页面回收操作，以减少频繁的 TLB 刷新开销和锁竞争，提升性能。\n\n当内核需要释放大量虚拟内存区域（如进程退出、mmap 区域销毁）时，不会立即释放每个页面，而是先将待释放的页面收集到 `mmu_gather` 结构中，待累积到一定数量或显式调用 flush 操作时，再统一执行 TLB 刷新、rmap 解除和页面释放。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `tlb_next_batch(struct mmu_gather *tlb)`  \n  分配新的批处理批次（batch），用于扩展可收集的页面数量上限。\n\n- `tlb_flush_rmaps(struct mmu_gather *tlb, struct vm_area_struct *vma)`  \n  （仅在 SMP 下）处理延迟的反向映射（delayed rmap）移除操作，在 TLB 刷新后调用。\n\n- `__tlb_batch_free_encoded_pages(struct mmu_gather_batch *batch)`  \n  批量释放编码后的页面（包括普通页面和 swap 缓存），支持防软锁定（soft lockup）的调度点。\n\n- `tlb_batch_pages_flush(struct mmu_gather *tlb)`  \n  遍历所有批次，释放其中收集的所有页面。\n\n- `tlb_batch_list_free(struct mmu_gather *tlb)`  \n  释放动态分配的批次内存（非本地批次）。\n\n- `__tlb_remove_folio_pages_size(...)` / `__tlb_remove_folio_pages(...)` / `__tlb_remove_page_size(...)`  \n  将页面（单页或多页 folio）加入当前 gather 批次，支持延迟 rmap 和不同页面大小。\n\n- `tlb_remove_table_sync_one(void)`  \n  （RCU 表释放模式下）触发 IPI 同步，确保软件页表遍历安全。\n\n- `tlb_remove_table_rcu(struct rcu_head *head)`  \n  RCU 回调函数，用于异步释放页表结构。\n\n- `tlb_remove_table_free(struct mmu_table_batch *batch)`  \n  将页表批次提交给 RCU 机制进行延迟释放。\n\n### 关键数据结构\n\n- `struct mmu_gather`  \n  核心上下文结构，包含本地批次（`local`）、当前活跃批次（`active`）、批次计数、延迟 rmap 标志等。\n\n- `struct mmu_gather_batch`  \n  页面批次结构，包含指向编码页面指针数组、当前数量（`nr`）、最大容量（`max`）及下一个批次指针。\n\n- `struct mmu_table_batch`  \n  页表结构批次，用于批量收集待释放的页表（如 PMD、PUD 等）。\n\n- `encoded_page` 相关机制  \n  使用指针低位编码额外信息（如是否延迟 rmap、是否后跟 nr_pages 字段），节省内存并提高缓存效率。\n\n## 3. 关键实现\n\n### 批处理与动态扩展\n- 默认使用栈上或局部存储的 `local` 批次（避免内存分配）。\n- 当 `local` 批次满时，通过 `__get_free_page()` 动态分配新批次（最多 `MAX_GATHER_BATCH_COUNT` 个）。\n- `tlb_next_batch()` 在存在延迟 rmap 时限制扩展，确保语义正确性。\n\n### 延迟反向映射（Delayed Rmap）\n- 当页面仍被其他 VMA 引用但当前 VMA 正在 unmap 时，不立即调用 `folio_remove_rmap_ptes()`，而是标记 `ENCODED_PAGE_BIT_DELAY_RMAP`。\n- 在 `tlb_flush_rmaps()` 中统一处理，确保在 TLB 刷新**之后**才解除 rmap，防止 CPU 访问已释放页面。\n\n### 安全释放与防软锁定\n- 页面释放循环中每处理最多 `MAX_NR_FOLIOS_PER_FREE`（512）个 folio 调用 `cond_resched()`，避免在非抢占内核中长时间占用 CPU。\n- 若启用 `page_poisoning` 或 `init_on_free`，则按实际内存大小（而非 folio 数量）限制单次释放量，因初始化开销与内存大小成正比。\n\n### 页表结构的安全释放（RCU 模式）\n- 在支持软件页表遍历（如 `gup_fast`）的架构上，页表释放需与遍历操作同步。\n- 使用 `call_rcu()` 延迟释放页表，配合 `smp_call_function()` 触发 IPI 确保所有 CPU 完成 TLB 刷新后再释放内存。\n- 若 RCU 批次分配失败，则回退到即时释放（代码未完整展示，但注释提及）。\n\n### 编码页面指针\n- 利用页面指针对齐特性（通常低 2~3 位为 0），将标志位（如 `DELAY_RMAP`、`NR_PAGES_NEXT`）存储在指针低位。\n- 支持多页 folio：若 `nr_pages > 1`，则连续两个条目分别存储页面指针（带标志）和页数。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm_types.h>`、`<linux/mm_inline.h>`、`<linux/rmap.h>` 等，与 folio、page、VMA 管理紧密集成。\n- **TLB 管理**：通过 `<asm/tlb.h>` 与架构相关 TLB 刷新接口交互。\n- **RCU 机制**：在 `CONFIG_MMU_GATHER_RCU_TABLE_FREE` 下依赖 `<linux/rcupdate.h>` 实现页表安全释放。\n- **SMP 支持**：`tlb_flush_rmaps` 和页表同步仅在 `CONFIG_SMP` 下编译。\n- **高阶内存与交换**：使用 `<linux/highmem.h>`、`<linux/swap.h>` 处理高端内存和 swap 缓存释放。\n- **内存分配器**：通过 `__get_free_page(GFP_NOWAIT)` 动态分配批次内存。\n\n## 5. 使用场景\n\n- **进程退出（exit_mmap）**：释放整个地址空间时，大量页面通过 mmu_gather 批量回收。\n- **munmap 系统调用**：解除大块内存映射时，避免逐页 TLB 刷新。\n- **内存回收（reclaim）**：在直接回收或 kswapd 中撤销映射时使用。\n- **透明大页（THP）拆分**：拆分大页时需撤销多个 PTE 映射并释放 sub-page。\n- **页表收缩（shrink_page_list）**：在页面回收路径中解除映射。\n- **KSM（Kernel Samepage Merging）**：合并或取消合并页面时更新 rmap。\n- **页表层级释放**：当上层页表（如 PGD/P4D/PUD/PMD）不再被引用时，通过 `tlb_remove_table` 机制安全释放。",
      "similarity": 0.6021803617477417,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "mm/mmu_gather.c",
          "start_line": 292,
          "end_line": 424,
          "content": [
            "static void tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\t__tlb_remove_table_free(batch);",
            "}",
            "static inline void tlb_table_invalidate(struct mmu_gather *tlb)",
            "{",
            "\tif (tlb_needs_table_invalidate()) {",
            "\t\t/*",
            "\t\t * Invalidate page-table caches used by hardware walkers. Then",
            "\t\t * we still need to RCU-sched wait while freeing the pages",
            "\t\t * because software walkers can still be in-flight.",
            "\t\t */",
            "\t\ttlb_flush_mmu_tlbonly(tlb);",
            "\t}",
            "}",
            "static void tlb_remove_table_one(void *table)",
            "{",
            "\ttlb_remove_table_sync_one();",
            "\t__tlb_remove_table(table);",
            "}",
            "static void tlb_table_flush(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_table_batch **batch = &tlb->batch;",
            "",
            "\tif (*batch) {",
            "\t\ttlb_table_invalidate(tlb);",
            "\t\ttlb_remove_table_free(*batch);",
            "\t\t*batch = NULL;",
            "\t}",
            "}",
            "void tlb_remove_table(struct mmu_gather *tlb, void *table)",
            "{",
            "\tstruct mmu_table_batch **batch = &tlb->batch;",
            "",
            "\tif (*batch == NULL) {",
            "\t\t*batch = (struct mmu_table_batch *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);",
            "\t\tif (*batch == NULL) {",
            "\t\t\ttlb_table_invalidate(tlb);",
            "\t\t\ttlb_remove_table_one(table);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\t(*batch)->nr = 0;",
            "\t}",
            "",
            "\t(*batch)->tables[(*batch)->nr++] = table;",
            "\tif ((*batch)->nr == MAX_TABLE_BATCH)",
            "\t\ttlb_table_flush(tlb);",
            "}",
            "static inline void tlb_table_init(struct mmu_gather *tlb)",
            "{",
            "\ttlb->batch = NULL;",
            "}",
            "static inline void tlb_table_flush(struct mmu_gather *tlb) { }",
            "static inline void tlb_table_init(struct mmu_gather *tlb) { }",
            "static void tlb_flush_mmu_free(struct mmu_gather *tlb)",
            "{",
            "\ttlb_table_flush(tlb);",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb_batch_pages_flush(tlb);",
            "#endif",
            "}",
            "void tlb_flush_mmu(struct mmu_gather *tlb)",
            "{",
            "\ttlb_flush_mmu_tlbonly(tlb);",
            "\ttlb_flush_mmu_free(tlb);",
            "}",
            "static void __tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,",
            "\t\t\t     bool fullmm)",
            "{",
            "\ttlb->mm = mm;",
            "\ttlb->fullmm = fullmm;",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb->need_flush_all = 0;",
            "\ttlb->local.next = NULL;",
            "\ttlb->local.nr   = 0;",
            "\ttlb->local.max  = ARRAY_SIZE(tlb->__pages);",
            "\ttlb->active     = &tlb->local;",
            "\ttlb->batch_count = 0;",
            "#endif",
            "\ttlb->delayed_rmap = 0;",
            "",
            "\ttlb_table_init(tlb);",
            "#ifdef CONFIG_MMU_GATHER_PAGE_SIZE",
            "\ttlb->page_size = 0;",
            "#endif",
            "",
            "\t__tlb_reset_range(tlb);",
            "\tinc_tlb_flush_pending(tlb->mm);",
            "}",
            "void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm)",
            "{",
            "\t__tlb_gather_mmu(tlb, mm, false);",
            "}",
            "void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm)",
            "{",
            "\t__tlb_gather_mmu(tlb, mm, true);",
            "}",
            "void tlb_finish_mmu(struct mmu_gather *tlb)",
            "{",
            "\t/*",
            "\t * If there are parallel threads are doing PTE changes on same range",
            "\t * under non-exclusive lock (e.g., mmap_lock read-side) but defer TLB",
            "\t * flush by batching, one thread may end up seeing inconsistent PTEs",
            "\t * and result in having stale TLB entries.  So flush TLB forcefully",
            "\t * if we detect parallel PTE batching threads.",
            "\t *",
            "\t * However, some syscalls, e.g. munmap(), may free page tables, this",
            "\t * needs force flush everything in the given range. Otherwise this",
            "\t * may result in having stale TLB entries for some architectures,",
            "\t * e.g. aarch64, that could specify flush what level TLB.",
            "\t */",
            "\tif (mm_tlb_flush_nested(tlb->mm)) {",
            "\t\t/*",
            "\t\t * The aarch64 yields better performance with fullmm by",
            "\t\t * avoiding multiple CPUs spamming TLBI messages at the",
            "\t\t * same time.",
            "\t\t *",
            "\t\t * On x86 non-fullmm doesn't yield significant difference",
            "\t\t * against fullmm.",
            "\t\t */",
            "\t\ttlb->fullmm = 1;",
            "\t\t__tlb_reset_range(tlb);",
            "\t\ttlb->freed_tables = 1;",
            "\t}",
            "",
            "\ttlb_flush_mmu(tlb);",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb_batch_list_free(tlb);",
            "#endif",
            "\tdec_tlb_flush_pending(tlb->mm);",
            "}"
          ],
          "function_name": "tlb_remove_table_free, tlb_table_invalidate, tlb_remove_table_one, tlb_table_flush, tlb_remove_table, tlb_table_init, tlb_table_flush, tlb_table_init, tlb_flush_mmu_free, tlb_flush_mmu, __tlb_gather_mmu, tlb_gather_mmu, tlb_gather_mmu_fullmm, tlb_finish_mmu",
          "description": "提供 TLB 无效化、页表批量释放及 MMU 收集器初始化/终止接口，包含跨架构的 TLB 同步机制",
          "similarity": 0.6667594909667969
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mmu_gather.c",
          "start_line": 18,
          "end_line": 120,
          "content": [
            "static bool tlb_next_batch(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\t/* Limit batching if we have delayed rmaps pending */",
            "\tif (tlb->delayed_rmap && tlb->active != &tlb->local)",
            "\t\treturn false;",
            "",
            "\tbatch = tlb->active;",
            "\tif (batch->next) {",
            "\t\ttlb->active = batch->next;",
            "\t\treturn true;",
            "\t}",
            "",
            "\tif (tlb->batch_count == MAX_GATHER_BATCH_COUNT)",
            "\t\treturn false;",
            "",
            "\tbatch = (void *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);",
            "\tif (!batch)",
            "\t\treturn false;",
            "",
            "\ttlb->batch_count++;",
            "\tbatch->next = NULL;",
            "\tbatch->nr   = 0;",
            "\tbatch->max  = MAX_GATHER_BATCH;",
            "",
            "\ttlb->active->next = batch;",
            "\ttlb->active = batch;",
            "",
            "\treturn true;",
            "}",
            "static void tlb_flush_rmap_batch(struct mmu_gather_batch *batch, struct vm_area_struct *vma)",
            "{",
            "\tstruct encoded_page **pages = batch->encoded_pages;",
            "",
            "\tfor (int i = 0; i < batch->nr; i++) {",
            "\t\tstruct encoded_page *enc = pages[i];",
            "",
            "\t\tif (encoded_page_flags(enc) & ENCODED_PAGE_BIT_DELAY_RMAP) {",
            "\t\t\tstruct page *page = encoded_page_ptr(enc);",
            "\t\t\tunsigned int nr_pages = 1;",
            "",
            "\t\t\tif (unlikely(encoded_page_flags(enc) &",
            "\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\tnr_pages = encoded_nr_pages(pages[++i]);",
            "",
            "\t\t\tfolio_remove_rmap_ptes(page_folio(page), page, nr_pages,",
            "\t\t\t\t\t       vma);",
            "\t\t}",
            "\t}",
            "}",
            "void tlb_flush_rmaps(struct mmu_gather *tlb, struct vm_area_struct *vma)",
            "{",
            "\tif (!tlb->delayed_rmap)",
            "\t\treturn;",
            "",
            "\ttlb_flush_rmap_batch(&tlb->local, vma);",
            "\tif (tlb->active != &tlb->local)",
            "\t\ttlb_flush_rmap_batch(tlb->active, vma);",
            "\ttlb->delayed_rmap = 0;",
            "}",
            "static void __tlb_batch_free_encoded_pages(struct mmu_gather_batch *batch)",
            "{",
            "\tstruct encoded_page **pages = batch->encoded_pages;",
            "\tunsigned int nr, nr_pages;",
            "",
            "\twhile (batch->nr) {",
            "\t\tif (!page_poisoning_enabled_static() && !want_init_on_free()) {",
            "\t\t\tnr = min(MAX_NR_FOLIOS_PER_FREE, batch->nr);",
            "",
            "\t\t\t/*",
            "\t\t\t * Make sure we cover page + nr_pages, and don't leave",
            "\t\t\t * nr_pages behind when capping the number of entries.",
            "\t\t\t */",
            "\t\t\tif (unlikely(encoded_page_flags(pages[nr - 1]) &",
            "\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\tnr++;",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * With page poisoning and init_on_free, the time it",
            "\t\t\t * takes to free memory grows proportionally with the",
            "\t\t\t * actual memory size. Therefore, limit based on the",
            "\t\t\t * actual memory size and not the number of involved",
            "\t\t\t * folios.",
            "\t\t\t */",
            "\t\t\tfor (nr = 0, nr_pages = 0;",
            "\t\t\t     nr < batch->nr && nr_pages < MAX_NR_FOLIOS_PER_FREE;",
            "\t\t\t     nr++) {",
            "\t\t\t\tif (unlikely(encoded_page_flags(pages[nr]) &",
            "\t\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\t\tnr_pages += encoded_nr_pages(pages[++nr]);",
            "\t\t\t\telse",
            "\t\t\t\t\tnr_pages++;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tfree_pages_and_swap_cache(pages, nr);",
            "\t\tpages += nr;",
            "\t\tbatch->nr -= nr;",
            "",
            "\t\tcond_resched();",
            "\t}",
            "}"
          ],
          "function_name": "tlb_next_batch, tlb_flush_rmap_batch, tlb_flush_rmaps, __tlb_batch_free_encoded_pages",
          "description": "管理 TLB 批量操作的延迟 RMAP 处理逻辑，包括批次链表管理、编码页面释放及 RMAP 标志清除",
          "similarity": 0.5292938947677612
        },
        {
          "chunk_id": 2,
          "file_path": "mm/mmu_gather.c",
          "start_line": 144,
          "end_line": 244,
          "content": [
            "static void tlb_batch_pages_flush(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\tfor (batch = &tlb->local; batch && batch->nr; batch = batch->next)",
            "\t\t__tlb_batch_free_encoded_pages(batch);",
            "\ttlb->active = &tlb->local;",
            "}",
            "static void tlb_batch_list_free(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch, *next;",
            "",
            "\tfor (batch = tlb->local.next; batch; batch = next) {",
            "\t\tnext = batch->next;",
            "\t\tfree_pages((unsigned long)batch, 0);",
            "\t}",
            "\ttlb->local.next = NULL;",
            "}",
            "static bool __tlb_remove_folio_pages_size(struct mmu_gather *tlb,",
            "\t\tstruct page *page, unsigned int nr_pages, bool delay_rmap,",
            "\t\tint page_size)",
            "{",
            "\tint flags = delay_rmap ? ENCODED_PAGE_BIT_DELAY_RMAP : 0;",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\tVM_BUG_ON(!tlb->end);",
            "",
            "#ifdef CONFIG_MMU_GATHER_PAGE_SIZE",
            "\tVM_WARN_ON(tlb->page_size != page_size);",
            "\tVM_WARN_ON_ONCE(nr_pages != 1 && page_size != PAGE_SIZE);",
            "\tVM_WARN_ON_ONCE(page_folio(page) != page_folio(page + nr_pages - 1));",
            "#endif",
            "",
            "\tbatch = tlb->active;",
            "\t/*",
            "\t * Add the page and check if we are full. If so",
            "\t * force a flush.",
            "\t */",
            "\tif (likely(nr_pages == 1)) {",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_page(page, flags);",
            "\t} else {",
            "\t\tflags |= ENCODED_PAGE_BIT_NR_PAGES_NEXT;",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_page(page, flags);",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_nr_pages(nr_pages);",
            "\t}",
            "\t/*",
            "\t * Make sure that we can always add another \"page\" + \"nr_pages\",",
            "\t * requiring two entries instead of only a single one.",
            "\t */",
            "\tif (batch->nr >= batch->max - 1) {",
            "\t\tif (!tlb_next_batch(tlb))",
            "\t\t\treturn true;",
            "\t\tbatch = tlb->active;",
            "\t}",
            "\tVM_BUG_ON_PAGE(batch->nr > batch->max - 1, page);",
            "",
            "\treturn false;",
            "}",
            "bool __tlb_remove_folio_pages(struct mmu_gather *tlb, struct page *page,",
            "\t\tunsigned int nr_pages, bool delay_rmap)",
            "{",
            "\treturn __tlb_remove_folio_pages_size(tlb, page, nr_pages, delay_rmap,",
            "\t\t\t\t\t     PAGE_SIZE);",
            "}",
            "bool __tlb_remove_page_size(struct mmu_gather *tlb, struct page *page,",
            "\t\tbool delay_rmap, int page_size)",
            "{",
            "\treturn __tlb_remove_folio_pages_size(tlb, page, 1, delay_rmap, page_size);",
            "}",
            "static void __tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < batch->nr; i++)",
            "\t\t__tlb_remove_table(batch->tables[i]);",
            "",
            "\tfree_page((unsigned long)batch);",
            "}",
            "static void tlb_remove_table_smp_sync(void *arg)",
            "{",
            "\t/* Simply deliver the interrupt */",
            "}",
            "void tlb_remove_table_sync_one(void)",
            "{",
            "\t/*",
            "\t * This isn't an RCU grace period and hence the page-tables cannot be",
            "\t * assumed to be actually RCU-freed.",
            "\t *",
            "\t * It is however sufficient for software page-table walkers that rely on",
            "\t * IRQ disabling.",
            "\t */",
            "\tsmp_call_function(tlb_remove_table_smp_sync, NULL, 1);",
            "}",
            "static void tlb_remove_table_rcu(struct rcu_head *head)",
            "{",
            "\t__tlb_remove_table_free(container_of(head, struct mmu_table_batch, rcu));",
            "}",
            "static void tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\tcall_rcu(&batch->rcu, tlb_remove_table_rcu);",
            "}"
          ],
          "function_name": "tlb_batch_pages_flush, tlb_batch_list_free, __tlb_remove_folio_pages_size, __tlb_remove_folio_pages, __tlb_remove_page_size, __tlb_remove_table_free, tlb_remove_table_smp_sync, tlb_remove_table_sync_one, tlb_remove_table_rcu, tlb_remove_table_free",
          "description": "实现页表条目批量移除和内存表管理，包含多页面处理、NR_PAGES_NEXT 标记解析及 RCU 安全释放",
          "similarity": 0.44651132822036743
        },
        {
          "chunk_id": 0,
          "file_path": "mm/mmu_gather.c",
          "start_line": 1,
          "end_line": 17,
          "content": [
            "#include <linux/gfp.h>",
            "#include <linux/highmem.h>",
            "#include <linux/kernel.h>",
            "#include <linux/mmdebug.h>",
            "#include <linux/mm_types.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/smp.h>",
            "#include <linux/swap.h>",
            "#include <linux/rmap.h>",
            "",
            "#include <asm/pgalloc.h>",
            "#include <asm/tlb.h>",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            ""
          ],
          "function_name": null,
          "description": "声明 MMU 聚合功能所需头文件，根据配置条件包含架构相关实现",
          "similarity": 0.4281301200389862
        }
      ]
    },
    {
      "source_file": "mm/hugetlb.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:06:09\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `hugetlb.c`\n\n---\n\n# hugetlb.c 技术文档\n\n## 1. 文件概述\n\n`hugetlb.c` 是 Linux 内核中实现通用大页（HugeTLB）内存管理的核心文件。该文件提供了对 HugeTLB 页面池的初始化、分配、释放、预留（reservation）、子池（subpool）管理以及与虚拟内存区域（VMA）相关的同步机制等关键功能。它支持多种 HugeTLB 页面大小，并通过灵活的配额和预留策略，满足不同应用场景对大页内存的需求，同时确保系统稳定性。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `hugetlb_max_hstate`: 系统中已注册的大页状态（hstate）数量。\n- `default_hstate_idx`: 默认使用的大页状态索引。\n- `hstates[HUGE_MAX_HSTATE]`: 存储所有已配置的大页状态（如页面大小、页面池信息等）的数组。\n- `huge_boot_pages`: 链表，用于在启动阶段收集通过 `memblock` 分配的大页。\n- `hugetlb_lock`: 自旋锁，保护大页池的关键数据结构（如空闲/活跃列表、页面计数等）。\n- `hugetlb_fault_mutex_table`: 故障互斥锁表，用于序列化同一逻辑大页上的缺页异常，防止因竞争导致的虚假 OOM。\n\n### 主要数据结构\n- `struct hstate`: 描述一种特定大小的大页配置，包括页面大小、节点分布、页面池统计等。\n- `struct hugepage_subpool`: 大页子池，用于实现基于挂载点或 inode 的配额控制（最大/最小页面数限制）。\n- `struct resv_map`: 预留映射，用于跟踪 VMA 中哪些大页已被预留但尚未分配。\n\n### 主要函数\n- **子池管理**:\n  - `hugepage_new_subpool()`: 创建新的大页子池。\n  - `hugepage_put_subpool()`: 释放子池引用，若无引用则销毁子池。\n  - `hugepage_subpool_get_pages()`: 从子池中获取页面，处理最大/最小配额逻辑。\n  - `hugepage_subpool_put_pages()`: 向子池归还页面，处理配额恢复逻辑。\n- **VMA 锁机制**:\n  - `hugetlb_vma_lock_read()/unlock_read()`: 对 VMA 进行读锁定。\n  - `hugetlb_vma_lock_write()/unlock_write()`: 对 VMA 进行写锁定。\n- **内存释放**:\n  - `hugetlb_free_folio()`: 释放大页 folio，优先尝试通过 CMA 释放。\n- **辅助函数**:\n  - `subpool_is_free()`: 判断子池是否可被安全释放。\n  - `unlock_or_release_subpool()`: 解锁并根据条件释放子池。\n\n## 3. 关键实现\n\n### 大页子池（Subpool）配额机制\n子池机制允许为不同的 hugetlbfs 挂载点设置独立的页面配额：\n- **最大配额 (`max_hpages`)**: 限制子池可使用的最大页面数。\n- **最小配额 (`min_hpages`)**: 启动时预占资源，确保最低可用页面数。\n- 获取页面时 (`hugepage_subpool_get_pages`)，先检查最大配额，再处理最小配额的预留抵扣。\n- 归还页面时 (`hugepage_subpool_put_pages`)，若使用量低于最小配额，则恢复预留计数。\n- 当子池引用计数归零且无活跃页面时，自动释放其最小配额并销毁子池。\n\n### VMA 同步锁设计\n为避免多个进程同时处理同一逻辑大页的缺页异常导致资源竞争或 OOM，内核采用两种锁策略：\n- **共享锁**: 多个 VMA 映射同一文件区域时，共享一个 `hugetlb_vma_lock` 结构。\n- **私有锁**: 私有映射使用 `resv_map` 中的读写信号量。\n- 通过 `__vma_shareable_lock()` 和 `__vma_private_lock()` 宏判断 VMA 类型，动态选择锁对象。\n\n### CMA 集成\n当启用 `CONFIG_CMA` 时，大页可从 CMA（Contiguous Memory Allocator）区域分配：\n- 每个 NUMA 节点维护独立的 `hugetlb_cma` 区域。\n- 释放大页时优先调用 `cma_free_folio()`，失败后才走通用路径 `folio_put()`。\n\n### 启动阶段大页分配\n通过 `huge_boot_pages` 链表在内核早期启动阶段收集大页，后续在 `hugetlb_init()` 中将其整合到各 `hstate` 的空闲列表中，确保大页池初始化完成前即可分配页面。\n\n## 4. 依赖关系\n\n- **内存管理子系统**: 依赖 `<linux/mm.h>`, `<linux/page-flags.h>`, `<linux/gfp.h>` 等基础内存管理接口。\n- **NUMA 支持**: 通过 `<linux/numa.h>`, `<linux/nodemask.h>` 实现节点感知的大页分配。\n- **hugetlbfs 文件系统**: 与 `fs/hugetlbfs/` 模块紧密耦合，通过 `HUGETLBFS_SB()` 获取子池信息。\n- **CMA 子系统**: 条件编译依赖 `<linux/cma.h>`，用于连续物理内存分配。\n- **内存控制组 (cgroup)**: 通过 `<linux/hugetlb_cgroup.h>` 集成资源限制。\n- **体系结构相关代码**: 依赖 `<asm/pgalloc.h>`, `<asm/tlb.h>` 处理页表操作和 TLB 刷新。\n\n## 5. 使用场景\n\n- **高性能计算 (HPC)**: 应用程序通过 `mmap(MAP_HUGETLB)` 或挂载 hugetlbfs 使用大页，减少 TLB 缺失开销。\n- **数据库系统**: 如 Oracle、MySQL 利用大页提升内存访问性能。\n- **虚拟化环境**: KVM/QEMU 为虚拟机分配大页作为后端内存，提高 I/O 性能。\n- **实时系统**: 通过预留大页确保关键任务的内存确定性。\n- **容器资源隔离**: 结合 cgroup v2 的 hugetlb 控制器，限制容器的大页使用量。\n- **内核启动参数配置**: 通过 `hugepagesz=`, `hugepages=` 等参数在启动时预分配大页池。",
      "similarity": 0.565432071685791,
      "chunks": [
        {
          "chunk_id": 38,
          "file_path": "mm/hugetlb.c",
          "start_line": 6649,
          "end_line": 6794,
          "content": [
            "long hugetlb_change_protection(struct vm_area_struct *vma,",
            "\t\tunsigned long address, unsigned long end,",
            "\t\tpgprot_t newprot, unsigned long cp_flags)",
            "{",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tunsigned long start = address;",
            "\tpte_t *ptep;",
            "\tpte_t pte;",
            "\tstruct hstate *h = hstate_vma(vma);",
            "\tlong pages = 0, psize = huge_page_size(h);",
            "\tbool shared_pmd = false;",
            "\tstruct mmu_notifier_range range;",
            "\tunsigned long last_addr_mask;",
            "\tbool uffd_wp = cp_flags & MM_CP_UFFD_WP;",
            "\tbool uffd_wp_resolve = cp_flags & MM_CP_UFFD_WP_RESOLVE;",
            "",
            "\t/*",
            "\t * In the case of shared PMDs, the area to flush could be beyond",
            "\t * start/end.  Set range.start/range.end to cover the maximum possible",
            "\t * range if PMD sharing is possible.",
            "\t */",
            "\tmmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_VMA,",
            "\t\t\t\t0, mm, start, end);",
            "\tadjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);",
            "",
            "\tBUG_ON(address >= end);",
            "\tflush_cache_range(vma, range.start, range.end);",
            "",
            "\tmmu_notifier_invalidate_range_start(&range);",
            "\thugetlb_vma_lock_write(vma);",
            "\ti_mmap_lock_write(vma->vm_file->f_mapping);",
            "\tlast_addr_mask = hugetlb_mask_last_page(h);",
            "\tfor (; address < end; address += psize) {",
            "\t\tspinlock_t *ptl;",
            "\t\tptep = hugetlb_walk(vma, address, psize);",
            "\t\tif (!ptep) {",
            "\t\t\tif (!uffd_wp) {",
            "\t\t\t\taddress |= last_addr_mask;",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\t/*",
            "\t\t\t * Userfaultfd wr-protect requires pgtable",
            "\t\t\t * pre-allocations to install pte markers.",
            "\t\t\t */",
            "\t\t\tptep = huge_pte_alloc(mm, vma, address, psize);",
            "\t\t\tif (!ptep) {",
            "\t\t\t\tpages = -ENOMEM;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t\tptl = huge_pte_lock(h, mm, ptep);",
            "\t\tif (huge_pmd_unshare(mm, vma, address, ptep)) {",
            "\t\t\t/*",
            "\t\t\t * When uffd-wp is enabled on the vma, unshare",
            "\t\t\t * shouldn't happen at all.  Warn about it if it",
            "\t\t\t * happened due to some reason.",
            "\t\t\t */",
            "\t\t\tWARN_ON_ONCE(uffd_wp || uffd_wp_resolve);",
            "\t\t\tpages++;",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\tshared_pmd = true;",
            "\t\t\taddress |= last_addr_mask;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tpte = huge_ptep_get(ptep);",
            "\t\tif (unlikely(is_hugetlb_entry_hwpoisoned(pte))) {",
            "\t\t\t/* Nothing to do. */",
            "\t\t} else if (unlikely(is_hugetlb_entry_migration(pte))) {",
            "\t\t\tswp_entry_t entry = pte_to_swp_entry(pte);",
            "\t\t\tstruct page *page = pfn_swap_entry_to_page(entry);",
            "\t\t\tpte_t newpte = pte;",
            "",
            "\t\t\tif (is_writable_migration_entry(entry)) {",
            "\t\t\t\tif (PageAnon(page))",
            "\t\t\t\t\tentry = make_readable_exclusive_migration_entry(",
            "\t\t\t\t\t\t\t\tswp_offset(entry));",
            "\t\t\t\telse",
            "\t\t\t\t\tentry = make_readable_migration_entry(",
            "\t\t\t\t\t\t\t\tswp_offset(entry));",
            "\t\t\t\tnewpte = swp_entry_to_pte(entry);",
            "\t\t\t\tpages++;",
            "\t\t\t}",
            "",
            "\t\t\tif (uffd_wp)",
            "\t\t\t\tnewpte = pte_swp_mkuffd_wp(newpte);",
            "\t\t\telse if (uffd_wp_resolve)",
            "\t\t\t\tnewpte = pte_swp_clear_uffd_wp(newpte);",
            "\t\t\tif (!pte_same(pte, newpte))",
            "\t\t\t\tset_huge_pte_at(mm, address, ptep, newpte, psize);",
            "\t\t} else if (unlikely(is_pte_marker(pte))) {",
            "\t\t\t/*",
            "\t\t\t * Do nothing on a poison marker; page is",
            "\t\t\t * corrupted, permissons do not apply.  Here",
            "\t\t\t * pte_marker_uffd_wp()==true implies !poison",
            "\t\t\t * because they're mutual exclusive.",
            "\t\t\t */",
            "\t\t\tif (pte_marker_uffd_wp(pte) && uffd_wp_resolve)",
            "\t\t\t\t/* Safe to modify directly (non-present->none). */",
            "\t\t\t\thuge_pte_clear(mm, address, ptep, psize);",
            "\t\t} else if (!huge_pte_none(pte)) {",
            "\t\t\tpte_t old_pte;",
            "\t\t\tunsigned int shift = huge_page_shift(hstate_vma(vma));",
            "",
            "\t\t\told_pte = huge_ptep_modify_prot_start(vma, address, ptep);",
            "\t\t\tpte = huge_pte_modify(old_pte, newprot);",
            "\t\t\tpte = arch_make_huge_pte(pte, shift, vma->vm_flags);",
            "\t\t\tif (uffd_wp)",
            "\t\t\t\tpte = huge_pte_mkuffd_wp(pte);",
            "\t\t\telse if (uffd_wp_resolve)",
            "\t\t\t\tpte = huge_pte_clear_uffd_wp(pte);",
            "\t\t\thuge_ptep_modify_prot_commit(vma, address, ptep, old_pte, pte);",
            "\t\t\tpages++;",
            "\t\t} else {",
            "\t\t\t/* None pte */",
            "\t\t\tif (unlikely(uffd_wp))",
            "\t\t\t\t/* Safe to modify directly (none->non-present). */",
            "\t\t\t\tset_huge_pte_at(mm, address, ptep,",
            "\t\t\t\t\t\tmake_pte_marker(PTE_MARKER_UFFD_WP),",
            "\t\t\t\t\t\tpsize);",
            "\t\t}",
            "\t\tspin_unlock(ptl);",
            "\t}",
            "\t/*",
            "\t * Must flush TLB before releasing i_mmap_rwsem: x86's huge_pmd_unshare",
            "\t * may have cleared our pud entry and done put_page on the page table:",
            "\t * once we release i_mmap_rwsem, another task can do the final put_page",
            "\t * and that page table be reused and filled with junk.  If we actually",
            "\t * did unshare a page of pmds, flush the range corresponding to the pud.",
            "\t */",
            "\tif (shared_pmd)",
            "\t\tflush_hugetlb_tlb_range(vma, range.start, range.end);",
            "\telse",
            "\t\tflush_hugetlb_tlb_range(vma, start, end);",
            "\t/*",
            "\t * No need to call mmu_notifier_arch_invalidate_secondary_tlbs() we are",
            "\t * downgrading page table protection not changing it to point to a new",
            "\t * page.",
            "\t *",
            "\t * See Documentation/mm/mmu_notifier.rst",
            "\t */",
            "\ti_mmap_unlock_write(vma->vm_file->f_mapping);",
            "\thugetlb_vma_unlock_write(vma);",
            "\tmmu_notifier_invalidate_range_end(&range);",
            "",
            "\treturn pages > 0 ? (pages << h->order) : pages;",
            "}"
          ],
          "function_name": "hugetlb_change_protection",
          "description": "修改HugeTLB页面的访问保护属性（如只读/可写），处理迁移入口、硬件污染标记等特殊情形，同步更新页表项并触发TLB刷新以保证可见性。",
          "similarity": 0.5857679843902588
        },
        {
          "chunk_id": 14,
          "file_path": "mm/hugetlb.c",
          "start_line": 2566,
          "end_line": 2686,
          "content": [
            "static void return_unused_surplus_pages(struct hstate *h,",
            "\t\t\t\t\tunsigned long unused_resv_pages)",
            "{",
            "\tunsigned long nr_pages;",
            "\tLIST_HEAD(page_list);",
            "",
            "\tlockdep_assert_held(&hugetlb_lock);",
            "\t/* Uncommit the reservation */",
            "\th->resv_huge_pages -= unused_resv_pages;",
            "",
            "\tif (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * Part (or even all) of the reservation could have been backed",
            "\t * by pre-allocated pages. Only free surplus pages.",
            "\t */",
            "\tnr_pages = min(unused_resv_pages, h->surplus_huge_pages);",
            "",
            "\t/*",
            "\t * We want to release as many surplus pages as possible, spread",
            "\t * evenly across all nodes with memory. Iterate across these nodes",
            "\t * until we can no longer free unreserved surplus pages. This occurs",
            "\t * when the nodes with surplus pages have no free pages.",
            "\t * remove_pool_hugetlb_folio() will balance the freed pages across the",
            "\t * on-line nodes with memory and will handle the hstate accounting.",
            "\t */",
            "\twhile (nr_pages--) {",
            "\t\tstruct folio *folio;",
            "",
            "\t\tfolio = remove_pool_hugetlb_folio(h, &node_states[N_MEMORY], 1);",
            "\t\tif (!folio)",
            "\t\t\tgoto out;",
            "",
            "\t\tlist_add(&folio->lru, &page_list);",
            "\t}",
            "",
            "out:",
            "\tspin_unlock_irq(&hugetlb_lock);",
            "\tupdate_and_free_pages_bulk(h, &page_list);",
            "\tspin_lock_irq(&hugetlb_lock);",
            "}",
            "static long __vma_reservation_common(struct hstate *h,",
            "\t\t\t\tstruct vm_area_struct *vma, unsigned long addr,",
            "\t\t\t\tenum vma_resv_mode mode)",
            "{",
            "\tstruct resv_map *resv;",
            "\tpgoff_t idx;",
            "\tlong ret;",
            "\tlong dummy_out_regions_needed;",
            "",
            "\tresv = vma_resv_map(vma);",
            "\tif (!resv)",
            "\t\treturn 1;",
            "",
            "\tidx = vma_hugecache_offset(h, vma, addr);",
            "\tswitch (mode) {",
            "\tcase VMA_NEEDS_RESV:",
            "\t\tret = region_chg(resv, idx, idx + 1, &dummy_out_regions_needed);",
            "\t\t/* We assume that vma_reservation_* routines always operate on",
            "\t\t * 1 page, and that adding to resv map a 1 page entry can only",
            "\t\t * ever require 1 region.",
            "\t\t */",
            "\t\tVM_BUG_ON(dummy_out_regions_needed != 1);",
            "\t\tbreak;",
            "\tcase VMA_COMMIT_RESV:",
            "\t\tret = region_add(resv, idx, idx + 1, 1, NULL, NULL);",
            "\t\t/* region_add calls of range 1 should never fail. */",
            "\t\tVM_BUG_ON(ret < 0);",
            "\t\tbreak;",
            "\tcase VMA_END_RESV:",
            "\t\tregion_abort(resv, idx, idx + 1, 1);",
            "\t\tret = 0;",
            "\t\tbreak;",
            "\tcase VMA_ADD_RESV:",
            "\t\tif (vma->vm_flags & VM_MAYSHARE) {",
            "\t\t\tret = region_add(resv, idx, idx + 1, 1, NULL, NULL);",
            "\t\t\t/* region_add calls of range 1 should never fail. */",
            "\t\t\tVM_BUG_ON(ret < 0);",
            "\t\t} else {",
            "\t\t\tregion_abort(resv, idx, idx + 1, 1);",
            "\t\t\tret = region_del(resv, idx, idx + 1);",
            "\t\t}",
            "\t\tbreak;",
            "\tcase VMA_DEL_RESV:",
            "\t\tif (vma->vm_flags & VM_MAYSHARE) {",
            "\t\t\tregion_abort(resv, idx, idx + 1, 1);",
            "\t\t\tret = region_del(resv, idx, idx + 1);",
            "\t\t} else {",
            "\t\t\tret = region_add(resv, idx, idx + 1, 1, NULL, NULL);",
            "\t\t\t/* region_add calls of range 1 should never fail. */",
            "\t\t\tVM_BUG_ON(ret < 0);",
            "\t\t}",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "",
            "\tif (vma->vm_flags & VM_MAYSHARE || mode == VMA_DEL_RESV)",
            "\t\treturn ret;",
            "\t/*",
            "\t * We know private mapping must have HPAGE_RESV_OWNER set.",
            "\t *",
            "\t * In most cases, reserves always exist for private mappings.",
            "\t * However, a file associated with mapping could have been",
            "\t * hole punched or truncated after reserves were consumed.",
            "\t * As subsequent fault on such a range will not use reserves.",
            "\t * Subtle - The reserve map for private mappings has the",
            "\t * opposite meaning than that of shared mappings.  If NO",
            "\t * entry is in the reserve map, it means a reservation exists.",
            "\t * If an entry exists in the reserve map, it means the",
            "\t * reservation has already been consumed.  As a result, the",
            "\t * return value of this routine is the opposite of the",
            "\t * value returned from reserve map manipulation routines above.",
            "\t */",
            "\tif (ret > 0)",
            "\t\treturn 0;",
            "\tif (ret == 0)",
            "\t\treturn 1;",
            "\treturn ret;",
            "}"
          ],
          "function_name": "return_unused_surplus_pages, __vma_reservation_common",
          "description": "该代码段涉及HugeTLB页面管理的两个关键组件：  \n1. `return_unused_surplus_pages`用于回收未使用的预留巨页，通过减少预留计数并释放多余页框至空闲列表，平衡节点间内存分布；  \n2. `__vma_reservation_common`负责根据VMA操作模式（如分配/释放预留区）维护resv_map，区分共享/私有映射的预留语义反向关系，确保预留状态与实际内存占用一致。  \n上下文完整，未引入额外假设或虚构API。",
          "similarity": 0.5768842101097107
        },
        {
          "chunk_id": 42,
          "file_path": "mm/hugetlb.c",
          "start_line": 7347,
          "end_line": 7451,
          "content": [
            "void folio_putback_active_hugetlb(struct folio *folio)",
            "{",
            "\tspin_lock_irq(&hugetlb_lock);",
            "\tfolio_set_hugetlb_migratable(folio);",
            "\tlist_move_tail(&folio->lru, &(folio_hstate(folio))->hugepage_activelist);",
            "\tspin_unlock_irq(&hugetlb_lock);",
            "\tfolio_put(folio);",
            "}",
            "void move_hugetlb_state(struct folio *old_folio, struct folio *new_folio, int reason)",
            "{",
            "\tstruct hstate *h = folio_hstate(old_folio);",
            "",
            "\thugetlb_cgroup_migrate(old_folio, new_folio);",
            "\tset_page_owner_migrate_reason(&new_folio->page, reason);",
            "",
            "\t/*",
            "\t * transfer temporary state of the new hugetlb folio. This is",
            "\t * reverse to other transitions because the newpage is going to",
            "\t * be final while the old one will be freed so it takes over",
            "\t * the temporary status.",
            "\t *",
            "\t * Also note that we have to transfer the per-node surplus state",
            "\t * here as well otherwise the global surplus count will not match",
            "\t * the per-node's.",
            "\t */",
            "\tif (folio_test_hugetlb_temporary(new_folio)) {",
            "\t\tint old_nid = folio_nid(old_folio);",
            "\t\tint new_nid = folio_nid(new_folio);",
            "",
            "\t\tfolio_set_hugetlb_temporary(old_folio);",
            "\t\tfolio_clear_hugetlb_temporary(new_folio);",
            "",
            "",
            "\t\t/*",
            "\t\t * There is no need to transfer the per-node surplus state",
            "\t\t * when we do not cross the node.",
            "\t\t */",
            "\t\tif (new_nid == old_nid)",
            "\t\t\treturn;",
            "\t\tspin_lock_irq(&hugetlb_lock);",
            "\t\tif (h->surplus_huge_pages_node[old_nid]) {",
            "\t\t\th->surplus_huge_pages_node[old_nid]--;",
            "\t\t\th->surplus_huge_pages_node[new_nid]++;",
            "\t\t}",
            "\t\tspin_unlock_irq(&hugetlb_lock);",
            "\t}",
            "}",
            "static void hugetlb_unshare_pmds(struct vm_area_struct *vma,",
            "\t\t\t\t   unsigned long start,",
            "\t\t\t\t   unsigned long end,",
            "\t\t\t\t   bool take_locks)",
            "{",
            "\tstruct hstate *h = hstate_vma(vma);",
            "\tunsigned long sz = huge_page_size(h);",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tstruct mmu_notifier_range range;",
            "\tunsigned long address;",
            "\tspinlock_t *ptl;",
            "\tpte_t *ptep;",
            "",
            "\tif (!(vma->vm_flags & VM_MAYSHARE))",
            "\t\treturn;",
            "",
            "\tif (start >= end)",
            "\t\treturn;",
            "",
            "\tflush_cache_range(vma, start, end);",
            "\t/*",
            "\t * No need to call adjust_range_if_pmd_sharing_possible(), because",
            "\t * we have already done the PUD_SIZE alignment.",
            "\t */",
            "\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm,",
            "\t\t\t\tstart, end);",
            "\tmmu_notifier_invalidate_range_start(&range);",
            "\tif (take_locks) {",
            "\t\thugetlb_vma_lock_write(vma);",
            "\t\ti_mmap_lock_write(vma->vm_file->f_mapping);",
            "\t} else {",
            "\t\ti_mmap_assert_write_locked(vma->vm_file->f_mapping);",
            "\t}",
            "\tfor (address = start; address < end; address += PUD_SIZE) {",
            "\t\tptep = hugetlb_walk(vma, address, sz);",
            "\t\tif (!ptep)",
            "\t\t\tcontinue;",
            "\t\tptl = huge_pte_lock(h, mm, ptep);",
            "\t\thuge_pmd_unshare(mm, vma, address, ptep);",
            "\t\tspin_unlock(ptl);",
            "\t}",
            "\tflush_hugetlb_tlb_range(vma, start, end);",
            "\tif (take_locks) {",
            "\t\ti_mmap_unlock_write(vma->vm_file->f_mapping);",
            "\t\thugetlb_vma_unlock_write(vma);",
            "\t}",
            "\t/*",
            "\t * No need to call mmu_notifier_arch_invalidate_secondary_tlbs(), see",
            "\t * Documentation/mm/mmu_notifier.rst.",
            "\t */",
            "\tmmu_notifier_invalidate_range_end(&range);",
            "}",
            "void hugetlb_unshare_all_pmds(struct vm_area_struct *vma)",
            "{",
            "\thugetlb_unshare_pmds(vma, ALIGN(vma->vm_start, PUD_SIZE),",
            "\t\t\tALIGN_DOWN(vma->vm_end, PUD_SIZE),",
            "\t\t\t/* take_locks = */ true);",
            "}"
          ],
          "function_name": "folio_putback_active_hugetlb, move_hugetlb_state, hugetlb_unshare_pmds, hugetlb_unshare_all_pmds",
          "description": "folio_putback_active_hugetlb 将 active 状态的 huge 页移回活动列表，move_hugetlb_state 迁移 huge 页状态及节点 surplus 计数，hugetlb_unshare_pmds/hugetlb_unshare_all_pmds 解除 PMD 共享并同步 TLB",
          "similarity": 0.5722371339797974
        },
        {
          "chunk_id": 13,
          "file_path": "mm/hugetlb.c",
          "start_line": 2268,
          "end_line": 2397,
          "content": [
            "int dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)",
            "{",
            "\tunsigned long pfn;",
            "\tstruct page *page;",
            "\tint rc = 0;",
            "\tunsigned int order;",
            "\tstruct hstate *h;",
            "",
            "\tif (!hugepages_supported())",
            "\t\treturn rc;",
            "",
            "\torder = huge_page_order(&default_hstate);",
            "\tfor_each_hstate(h)",
            "\t\torder = min(order, huge_page_order(h));",
            "",
            "\tfor (pfn = start_pfn; pfn < end_pfn; pfn += 1 << order) {",
            "\t\tpage = pfn_to_page(pfn);",
            "\t\trc = dissolve_free_huge_page(page);",
            "\t\tif (rc)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn rc;",
            "}",
            "static int gather_surplus_pages(struct hstate *h, long delta)",
            "\t__must_hold(&hugetlb_lock)",
            "{",
            "\tLIST_HEAD(surplus_list);",
            "\tstruct folio *folio, *tmp;",
            "\tint ret;",
            "\tlong i;",
            "\tlong needed, allocated;",
            "\tbool alloc_ok = true;",
            "\tint node;",
            "\tnodemask_t *mbind_nodemask, alloc_nodemask;",
            "",
            "\tmbind_nodemask = policy_mbind_nodemask(htlb_alloc_mask(h));",
            "\tif (mbind_nodemask)",
            "\t\tnodes_and(alloc_nodemask, *mbind_nodemask, cpuset_current_mems_allowed);",
            "\telse",
            "\t\talloc_nodemask = cpuset_current_mems_allowed;",
            "",
            "\tlockdep_assert_held(&hugetlb_lock);",
            "\tneeded = (h->resv_huge_pages + delta) - h->free_huge_pages;",
            "\tif (needed <= 0) {",
            "\t\th->resv_huge_pages += delta;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tallocated = 0;",
            "",
            "\tret = -ENOMEM;",
            "retry:",
            "\tspin_unlock_irq(&hugetlb_lock);",
            "\tfor (i = 0; i < needed; i++) {",
            "\t\tfolio = NULL;",
            "",
            "\t\t/* Prioritize current node */",
            "\t\tif (node_isset(numa_mem_id(), alloc_nodemask))",
            "\t\t\tfolio = alloc_surplus_hugetlb_folio(h, htlb_alloc_mask(h),",
            "\t\t\t\t\tnuma_mem_id(), NULL);",
            "",
            "\t\tif (!folio) {",
            "\t\t\tfor_each_node_mask(node, alloc_nodemask) {",
            "\t\t\t\tif (node == numa_mem_id())",
            "\t\t\t\t\tcontinue;",
            "\t\t\t\tfolio = alloc_surplus_hugetlb_folio(h, htlb_alloc_mask(h),",
            "\t\t\t\t\t\tnode, NULL);",
            "\t\t\t\tif (folio)",
            "\t\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t\tif (!folio) {",
            "\t\t\talloc_ok = false;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tlist_add(&folio->lru, &surplus_list);",
            "\t\tcond_resched();",
            "\t}",
            "\tallocated += i;",
            "",
            "\t/*",
            "\t * After retaking hugetlb_lock, we need to recalculate 'needed'",
            "\t * because either resv_huge_pages or free_huge_pages may have changed.",
            "\t */",
            "\tspin_lock_irq(&hugetlb_lock);",
            "\tneeded = (h->resv_huge_pages + delta) -",
            "\t\t\t(h->free_huge_pages + allocated);",
            "\tif (needed > 0) {",
            "\t\tif (alloc_ok)",
            "\t\t\tgoto retry;",
            "\t\t/*",
            "\t\t * We were not able to allocate enough pages to",
            "\t\t * satisfy the entire reservation so we free what",
            "\t\t * we've allocated so far.",
            "\t\t */",
            "\t\tgoto free;",
            "\t}",
            "\t/*",
            "\t * The surplus_list now contains _at_least_ the number of extra pages",
            "\t * needed to accommodate the reservation.  Add the appropriate number",
            "\t * of pages to the hugetlb pool and free the extras back to the buddy",
            "\t * allocator.  Commit the entire reservation here to prevent another",
            "\t * process from stealing the pages as they are added to the pool but",
            "\t * before they are reserved.",
            "\t */",
            "\tneeded += allocated;",
            "\th->resv_huge_pages += delta;",
            "\tret = 0;",
            "",
            "\t/* Free the needed pages to the hugetlb pool */",
            "\tlist_for_each_entry_safe(folio, tmp, &surplus_list, lru) {",
            "\t\tif ((--needed) < 0)",
            "\t\t\tbreak;",
            "\t\t/* Add the page to the hugetlb allocator */",
            "\t\tenqueue_hugetlb_folio(h, folio);",
            "\t}",
            "free:",
            "\tspin_unlock_irq(&hugetlb_lock);",
            "",
            "\t/*",
            "\t * Free unnecessary surplus pages to the buddy allocator.",
            "\t * Pages have no ref count, call free_huge_folio directly.",
            "\t */",
            "\tlist_for_each_entry_safe(folio, tmp, &surplus_list, lru)",
            "\t\tfree_huge_folio(folio);",
            "\tspin_lock_irq(&hugetlb_lock);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "dissolve_free_huge_pages, gather_surplus_pages",
          "description": "该代码段包含两个与HugeTLB页管理相关的函数：  \n1. `dissolve_free_huge_pages` 核心功能是回收指定PFN范围内的空闲大页内存，通过遍历并调用`dissolve_free_huge_page`释放资源；  \n2. `gather_surplus_pages` 的作用是动态调整预留的大页数量，通过跨NUMA节点分配/释放冗余页，确保预留页数满足需求，其逻辑依赖于互斥锁保护的共享状态更新。  \n\n注：代码上下文不完整，部分辅助函数（如`alloc_surplus_hugetlb_folio`）及数据结构未展示，需结合内核HugeTLB模块整体设计理解。",
          "similarity": 0.5669934749603271
        },
        {
          "chunk_id": 10,
          "file_path": "mm/hugetlb.c",
          "start_line": 1651,
          "end_line": 1754,
          "content": [
            "static void __update_and_free_hugetlb_folio(struct hstate *h,",
            "\t\t\t\t\t\tstruct folio *folio)",
            "{",
            "\tbool clear_flag = folio_test_hugetlb_vmemmap_optimized(folio);",
            "",
            "\tif (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If we don't know which subpages are hwpoisoned, we can't free",
            "\t * the hugepage, so it's leaked intentionally.",
            "\t */",
            "\tif (folio_test_hugetlb_raw_hwp_unreliable(folio))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If folio is not vmemmap optimized (!clear_flag), then the folio",
            "\t * is no longer identified as a hugetlb page.  hugetlb_vmemmap_restore_folio",
            "\t * can only be passed hugetlb pages and will BUG otherwise.",
            "\t */",
            "\tif (clear_flag && hugetlb_vmemmap_restore_folio(h, folio)) {",
            "\t\tspin_lock_irq(&hugetlb_lock);",
            "\t\t/*",
            "\t\t * If we cannot allocate vmemmap pages, just refuse to free the",
            "\t\t * page and put the page back on the hugetlb free list and treat",
            "\t\t * as a surplus page.",
            "\t\t */",
            "\t\tadd_hugetlb_folio(h, folio, true);",
            "\t\tspin_unlock_irq(&hugetlb_lock);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * If vmemmap pages were allocated above, then we need to clear the",
            "\t * hugetlb flag under the hugetlb lock.",
            "\t */",
            "\tif (folio_test_hugetlb(folio)) {",
            "\t\tspin_lock_irq(&hugetlb_lock);",
            "\t\t__folio_clear_hugetlb(folio);",
            "\t\tspin_unlock_irq(&hugetlb_lock);",
            "\t}",
            "",
            "\t/*",
            "\t * Move PageHWPoison flag from head page to the raw error pages,",
            "\t * which makes any healthy subpages reusable.",
            "\t */",
            "\tif (unlikely(folio_test_hwpoison(folio)))",
            "\t\tfolio_clear_hugetlb_hwpoison(folio);",
            "",
            "\tfolio_ref_unfreeze(folio, 1);",
            "",
            "\tINIT_LIST_HEAD(&folio->_deferred_list);",
            "\thugetlb_free_folio(folio);",
            "}",
            "static void free_hpage_workfn(struct work_struct *work)",
            "{",
            "\tstruct llist_node *node;",
            "",
            "\tnode = llist_del_all(&hpage_freelist);",
            "",
            "\twhile (node) {",
            "\t\tstruct folio *folio;",
            "\t\tstruct hstate *h;",
            "",
            "\t\tfolio = container_of((struct address_space **)node,",
            "\t\t\t\t     struct folio, mapping);",
            "\t\tnode = node->next;",
            "\t\tfolio->mapping = NULL;",
            "\t\t/*",
            "\t\t * The VM_BUG_ON_FOLIO(!folio_test_hugetlb(folio), folio) in",
            "\t\t * folio_hstate() is going to trigger because a previous call to",
            "\t\t * remove_hugetlb_folio() will clear the hugetlb bit, so do",
            "\t\t * not use folio_hstate() directly.",
            "\t\t */",
            "\t\th = size_to_hstate(folio_size(folio));",
            "",
            "\t\t__update_and_free_hugetlb_folio(h, folio);",
            "",
            "\t\tcond_resched();",
            "\t}",
            "}",
            "static inline void flush_free_hpage_work(struct hstate *h)",
            "{",
            "\tif (hugetlb_vmemmap_optimizable(h))",
            "\t\tflush_work(&free_hpage_work);",
            "}",
            "static void update_and_free_hugetlb_folio(struct hstate *h, struct folio *folio,",
            "\t\t\t\t bool atomic)",
            "{",
            "\tif (!folio_test_hugetlb_vmemmap_optimized(folio) || !atomic) {",
            "\t\t__update_and_free_hugetlb_folio(h, folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Defer freeing to avoid using GFP_ATOMIC to allocate vmemmap pages.",
            "\t *",
            "\t * Only call schedule_work() if hpage_freelist is previously",
            "\t * empty. Otherwise, schedule_work() had been called but the workfn",
            "\t * hasn't retrieved the list yet.",
            "\t */",
            "\tif (llist_add((struct llist_node *)&folio->mapping, &hpage_freelist))",
            "\t\tschedule_work(&free_hpage_work);",
            "}"
          ],
          "function_name": "__update_and_free_hugetlb_folio, free_hpage_workfn, flush_free_hpage_work, update_and_free_hugetlb_folio",
          "description": "该代码段实现了对HugeTLB页帧的清理与释放逻辑，主要处理巨型页面在不同状态下的回收策略。其中`__update_and_free_hugetlb_folio`负责清除hugeTLB标识并调用释放函数，而`free_hpage_workfn`作为延迟工作者线程，安全地遍历并释放缓存中的页帧。由于缺少对`hpage_freelist`初始化及`hugetlb_lock`等关键上下文的展示，需注意此代码片段仅体现部分实现细节。",
          "similarity": 0.5656893253326416
        }
      ]
    },
    {
      "source_file": "mm/hugetlb_cgroup.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:06:55\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `hugetlb_cgroup.c`\n\n---\n\n# hugetlb_cgroup.c 技术文档\n\n## 1. 文件概述\n\n`hugetlb_cgroup.c` 是 Linux 内核中用于实现 **HugeTLB（大页）内存资源控制组（cgroup）** 功能的核心文件。该文件通过 cgroup v1/v2 接口，对不同 HugeTLB 页面大小（如 2MB、1GB 等）的内存使用进行配额限制和统计追踪。它支持两种计费模式：普通分配（fault-based）和预留（reservation-based），并确保在 cgroup 被销毁时将资源正确迁移至父 cgroup，防止资源泄漏。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct hugetlb_cgroup`：每个 cgroup 实例对应的 HugeTLB 控制结构。\n  - 包含两个 `page_counter` 数组：`hugepage[]`（普通使用）和 `rsvd_hugepage[]`（预留使用），分别对应每种 HugeTLB 页面类型。\n  - 包含 per-node 信息 `nodeinfo[]`，用于 NUMA 感知。\n  - 包含事件计数器 `events[][]` 和 `events_local[][]`，用于触发 cgroup 通知。\n- `struct hugetlb_cgroup_per_node`：每个 NUMA 节点上的 HugeTLB cgroup 附加信息（当前未在代码片段中完整定义）。\n\n### 主要函数\n- `hugetlb_cgroup_css_alloc()` / `hugetlb_cgroup_css_free()`：cgroup 子系统实例的创建与销毁。\n- `hugetlb_cgroup_init()`：初始化新 cgroup 的 page_counter，设置最大限制为 `PAGE_COUNTER_MAX` 向下对齐到 HugeTLB 页面大小。\n- `__hugetlb_cgroup_charge_cgroup()` / `hugetlb_cgroup_charge_cgroup()`：对当前任务所属 cgroup 尝试 charge（计费）指定数量的 HugeTLB 页面。\n- `hugetlb_cgroup_move_parent()`：将属于某 cgroup 的 HugeTLB 页面迁移至其父 cgroup。\n- `hugetlb_cgroup_css_offline()`：在 cgroup 离线时，强制将其所有 HugeTLB 资源迁移至父级。\n- `hugetlb_event()`：向上冒泡记录 HugeTLB 事件（如达到限制 `HUGETLB_MAX`）并触发 cgroup 文件通知。\n- 辅助内联函数：\n  - `hugetlb_cgroup_from_css()` / `from_task()`：从 cgroup_subsys_state 或 task 获取 hugetlb_cgroup。\n  - `hugetlb_cgroup_counter_from_cgroup()` / `_rsvd()`：获取对应计费类型的 page_counter。\n  - `hugetlb_cgroup_is_root()` / `parent_hugetlb_cgroup()`：判断是否为根 cgroup 或获取父 cgroup。\n\n## 3. 关键实现\n\n### 资源计费机制\n- 使用 `page_counter` 子系统管理每种 HugeTLB 页面类型的用量和上限。\n- 支持两种独立的计费路径：\n  - **普通分配（fault）**：实际分配物理页面时计费。\n  - **预留（reservation）**：仅预留虚拟地址空间时计费（用于 mmap 等场景）。\n- 计费时通过 RCU 安全地获取当前任务的 cgroup，并使用 `css_tryget()` 确保引用有效性。\n- 若计费失败（超出限制），触发 `HUGETLB_MAX` 事件并通过 `cgroup_file_notify()` 通知用户空间。\n\n### cgroup 生命周期管理\n- **创建**：为每个在线 NUMA 节点分配 `hugetlb_cgroup_per_node` 结构；初始化所有 HugeTLB 类型的 page_counter，父子层级通过 `page_counter` 的 parent 字段建立级联关系。\n- **离线（offline）**：遍历所有 HugeTLB 页面的 active list，调用 `hugetlb_cgroup_move_parent()` 将页面所有权转移给父 cgroup。此过程循环执行直至当前 cgroup 无任何 HugeTLB 使用量，确保资源完全迁移。\n- **销毁**：释放 per-node 数据及主结构体。\n\n### 资源迁移（Reparenting）\n- `hugetlb_cgroup_move_parent()` 在持有 `hugetlb_lock` 时执行，确保页面不会被并发释放或迁移。\n- 仅处理属于目标 cgroup 的页面；若父 cgroup 为空（即目标为根），则 charge 到全局 root cgroup（无硬限制）。\n- 通过 `set_hugetlb_cgroup()` 更新页面所属的 cgroup。\n\n### 限制与优化\n- 对于 order 小于 `HUGETLB_CGROUP_MIN_ORDER`（通常为 3，即 8 个普通页 = 32KB）的 HugeTLB 页面，**不进行 cgroup 计费**，以减少小 HugeTLB 页面的开销。\n- 最大限制设为 `PAGE_COUNTER_MAX` 向下对齐到 HugeTLB 页面大小，避免跨页边界问题。\n- 当前 per-node 分配策略对 offline 节点也分配内存（使用 `NUMA_NO_NODE`），存在内存浪费，注释中指出未来可通过内存热插拔回调优化。\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/cgroup.h>`：cgroup 基础框架。\n  - `<linux/page_counter.h>`：提供层次化内存计数和限制功能。\n  - `<linux/hugetlb.h>`：HugeTLB 核心数据结构（如 `hstate`, `hugepage_activelist`）和锁（`hugetlb_lock`）。\n  - `<linux/hugetlb_cgroup.h>`：HugeTLB cgroup 的公共接口和数据结构定义。\n- **交互模块**：\n  - **HugeTLB 子系统**：在页面分配/释放、预留/取消预留等路径中调用本文件的 charge/uncharge 函数。\n  - **Memory cgroup (memcg)**：共享部分设计思想（如 page_counter），但 HugeTLB cgroup 是独立子系统。\n  - **Scheduler**：通过 `current` 获取当前任务的 cgroup 上下文。\n\n## 5. 使用场景\n\n- **容器资源隔离**：在 Kubernetes/Docker 等容器运行时中，通过 cgroup v1 的 `hugetlb` 子系统或 cgroup v2 的 `hugetlb.` 控制器，限制容器可使用的 HugeTLB 内存总量，防止单个容器耗尽系统大页资源。\n- **高性能计算（HPC）**：为不同 HPC 作业分配专用的 HugeTLB 内存配额，确保关键应用获得确定性内存性能。\n- **数据库优化**：Oracle、MySQL 等数据库使用 HugeTLB 提升 TLB 效率，通过 cgroup 限制其大页使用量，避免影响其他服务。\n- **资源监控与告警**：用户空间可通过读取 cgroup 的 `hugetlb.events` 文件监控 `max` 事件，实现基于 HugeTLB 使用量的自动扩缩容或告警。",
      "similarity": 0.5536854267120361,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 242,
          "end_line": 368,
          "content": [
            "static inline void hugetlb_event(struct hugetlb_cgroup *hugetlb, int idx,",
            "\t\t\t\t enum hugetlb_memory_event event)",
            "{",
            "\tatomic_long_inc(&hugetlb->events_local[idx][event]);",
            "\tcgroup_file_notify(&hugetlb->events_local_file[idx]);",
            "",
            "\tdo {",
            "\t\tatomic_long_inc(&hugetlb->events[idx][event]);",
            "\t\tcgroup_file_notify(&hugetlb->events_file[idx]);",
            "\t} while ((hugetlb = parent_hugetlb_cgroup(hugetlb)) &&",
            "\t\t !hugetlb_cgroup_is_root(hugetlb));",
            "}",
            "static int __hugetlb_cgroup_charge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t  struct hugetlb_cgroup **ptr,",
            "\t\t\t\t\t  bool rsvd)",
            "{",
            "\tint ret = 0;",
            "\tstruct page_counter *counter;",
            "\tstruct hugetlb_cgroup *h_cg = NULL;",
            "",
            "\tif (hugetlb_cgroup_disabled())",
            "\t\tgoto done;",
            "\t/*",
            "\t * We don't charge any cgroup if the compound page have less",
            "\t * than 3 pages.",
            "\t */",
            "\tif (huge_page_order(&hstates[idx]) < HUGETLB_CGROUP_MIN_ORDER)",
            "\t\tgoto done;",
            "again:",
            "\trcu_read_lock();",
            "\th_cg = hugetlb_cgroup_from_task(current);",
            "\tif (!css_tryget(&h_cg->css)) {",
            "\t\trcu_read_unlock();",
            "\t\tgoto again;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tif (!page_counter_try_charge(",
            "\t\t    __hugetlb_cgroup_counter_from_cgroup(h_cg, idx, rsvd),",
            "\t\t    nr_pages, &counter)) {",
            "\t\tret = -ENOMEM;",
            "\t\thugetlb_event(h_cg, idx, HUGETLB_MAX);",
            "\t\tcss_put(&h_cg->css);",
            "\t\tgoto done;",
            "\t}",
            "\t/* Reservations take a reference to the css because they do not get",
            "\t * reparented.",
            "\t */",
            "\tif (!rsvd)",
            "\t\tcss_put(&h_cg->css);",
            "done:",
            "\t*ptr = h_cg;",
            "\treturn ret;",
            "}",
            "int hugetlb_cgroup_charge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t struct hugetlb_cgroup **ptr)",
            "{",
            "\treturn __hugetlb_cgroup_charge_cgroup(idx, nr_pages, ptr, false);",
            "}",
            "int hugetlb_cgroup_charge_cgroup_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t      struct hugetlb_cgroup **ptr)",
            "{",
            "\treturn __hugetlb_cgroup_charge_cgroup(idx, nr_pages, ptr, true);",
            "}",
            "static void __hugetlb_cgroup_commit_charge(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t   struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t\t   struct folio *folio, bool rsvd)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !h_cg)",
            "\t\treturn;",
            "",
            "\t__set_hugetlb_cgroup(folio, h_cg, rsvd);",
            "\tif (!rsvd) {",
            "\t\tunsigned long usage =",
            "\t\t\th_cg->nodeinfo[folio_nid(folio)]->usage[idx];",
            "\t\t/*",
            "\t\t * This write is not atomic due to fetching usage and writing",
            "\t\t * to it, but that's fine because we call this with",
            "\t\t * hugetlb_lock held anyway.",
            "\t\t */",
            "\t\tWRITE_ONCE(h_cg->nodeinfo[folio_nid(folio)]->usage[idx],",
            "\t\t\t   usage + nr_pages);",
            "\t}",
            "}",
            "void hugetlb_cgroup_commit_charge(int idx, unsigned long nr_pages,",
            "\t\t\t\t  struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t  struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_commit_charge(idx, nr_pages, h_cg, folio, false);",
            "}",
            "void hugetlb_cgroup_commit_charge_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t       struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t       struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_commit_charge(idx, nr_pages, h_cg, folio, true);",
            "}",
            "static void __hugetlb_cgroup_uncharge_folio(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t   struct folio *folio, bool rsvd)",
            "{",
            "\tstruct hugetlb_cgroup *h_cg;",
            "",
            "\tif (hugetlb_cgroup_disabled())",
            "\t\treturn;",
            "\tlockdep_assert_held(&hugetlb_lock);",
            "\th_cg = __hugetlb_cgroup_from_folio(folio, rsvd);",
            "\tif (unlikely(!h_cg))",
            "\t\treturn;",
            "\t__set_hugetlb_cgroup(folio, NULL, rsvd);",
            "",
            "\tpage_counter_uncharge(__hugetlb_cgroup_counter_from_cgroup(h_cg, idx,",
            "\t\t\t\t\t\t\t\t   rsvd),",
            "\t\t\t      nr_pages);",
            "",
            "\tif (rsvd)",
            "\t\tcss_put(&h_cg->css);",
            "\telse {",
            "\t\tunsigned long usage =",
            "\t\t\th_cg->nodeinfo[folio_nid(folio)]->usage[idx];",
            "\t\t/*",
            "\t\t * This write is not atomic due to fetching usage and writing",
            "\t\t * to it, but that's fine because we call this with",
            "\t\t * hugetlb_lock held anyway.",
            "\t\t */",
            "\t\tWRITE_ONCE(h_cg->nodeinfo[folio_nid(folio)]->usage[idx],",
            "\t\t\t   usage - nr_pages);",
            "\t}",
            "}"
          ],
          "function_name": "hugetlb_event, __hugetlb_cgroup_charge_cgroup, hugetlb_cgroup_charge_cgroup, hugetlb_cgroup_charge_cgroup_rsvd, __hugetlb_cgroup_commit_charge, hugetlb_cgroup_commit_charge, hugetlb_cgroup_commit_charge_rsvd, __hugetlb_cgroup_uncharge_folio",
          "description": "处理HugeTLB页面分配时的计费操作，包含事件记录、尝试充电、提交充电等流程，区分普通页与保留页的计数逻辑，并维护各层级cgroup的使用统计。",
          "similarity": 0.5807563066482544
        },
        {
          "chunk_id": 3,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 381,
          "end_line": 500,
          "content": [
            "void hugetlb_cgroup_uncharge_folio(int idx, unsigned long nr_pages,",
            "\t\t\t\t  struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_uncharge_folio(idx, nr_pages, folio, false);",
            "}",
            "void hugetlb_cgroup_uncharge_folio_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t       struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_uncharge_folio(idx, nr_pages, folio, true);",
            "}",
            "static void __hugetlb_cgroup_uncharge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t     struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t\t     bool rsvd)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !h_cg)",
            "\t\treturn;",
            "",
            "\tif (huge_page_order(&hstates[idx]) < HUGETLB_CGROUP_MIN_ORDER)",
            "\t\treturn;",
            "",
            "\tpage_counter_uncharge(__hugetlb_cgroup_counter_from_cgroup(h_cg, idx,",
            "\t\t\t\t\t\t\t\t   rsvd),",
            "\t\t\t      nr_pages);",
            "",
            "\tif (rsvd)",
            "\t\tcss_put(&h_cg->css);",
            "}",
            "void hugetlb_cgroup_uncharge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t    struct hugetlb_cgroup *h_cg)",
            "{",
            "\t__hugetlb_cgroup_uncharge_cgroup(idx, nr_pages, h_cg, false);",
            "}",
            "void hugetlb_cgroup_uncharge_cgroup_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t struct hugetlb_cgroup *h_cg)",
            "{",
            "\t__hugetlb_cgroup_uncharge_cgroup(idx, nr_pages, h_cg, true);",
            "}",
            "void hugetlb_cgroup_uncharge_counter(struct resv_map *resv, unsigned long start,",
            "\t\t\t\t     unsigned long end)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !resv || !resv->reservation_counter ||",
            "\t    !resv->css)",
            "\t\treturn;",
            "",
            "\tpage_counter_uncharge(resv->reservation_counter,",
            "\t\t\t      (end - start) * resv->pages_per_hpage);",
            "\tcss_put(resv->css);",
            "}",
            "void hugetlb_cgroup_uncharge_file_region(struct resv_map *resv,",
            "\t\t\t\t\t struct file_region *rg,",
            "\t\t\t\t\t unsigned long nr_pages,",
            "\t\t\t\t\t bool region_del)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !resv || !rg || !nr_pages)",
            "\t\treturn;",
            "",
            "\tif (rg->reservation_counter && resv->pages_per_hpage &&",
            "\t    !resv->reservation_counter) {",
            "\t\tpage_counter_uncharge(rg->reservation_counter,",
            "\t\t\t\t      nr_pages * resv->pages_per_hpage);",
            "\t\t/*",
            "\t\t * Only do css_put(rg->css) when we delete the entire region",
            "\t\t * because one file_region must hold exactly one css reference.",
            "\t\t */",
            "\t\tif (region_del)",
            "\t\t\tcss_put(rg->css);",
            "\t}",
            "}",
            "static int hugetlb_cgroup_read_numa_stat(struct seq_file *seq, void *dummy)",
            "{",
            "\tint nid;",
            "\tstruct cftype *cft = seq_cft(seq);",
            "\tint idx = MEMFILE_IDX(cft->private);",
            "\tbool legacy = MEMFILE_ATTR(cft->private);",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(seq_css(seq));",
            "\tstruct cgroup_subsys_state *css;",
            "\tunsigned long usage;",
            "",
            "\tif (legacy) {",
            "\t\t/* Add up usage across all nodes for the non-hierarchical total. */",
            "\t\tusage = 0;",
            "\t\tfor_each_node_state(nid, N_MEMORY)",
            "\t\t\tusage += READ_ONCE(h_cg->nodeinfo[nid]->usage[idx]);",
            "\t\tseq_printf(seq, \"total=%lu\", usage * PAGE_SIZE);",
            "",
            "\t\t/* Simply print the per-node usage for the non-hierarchical total. */",
            "\t\tfor_each_node_state(nid, N_MEMORY)",
            "\t\t\tseq_printf(seq, \" N%d=%lu\", nid,",
            "\t\t\t\t   READ_ONCE(h_cg->nodeinfo[nid]->usage[idx]) *",
            "\t\t\t\t\t   PAGE_SIZE);",
            "\t\tseq_putc(seq, '\\n');",
            "\t}",
            "",
            "\t/*",
            "\t * The hierarchical total is pretty much the value recorded by the",
            "\t * counter, so use that.",
            "\t */",
            "\tseq_printf(seq, \"%stotal=%lu\", legacy ? \"hierarchical_\" : \"\",",
            "\t\t   page_counter_read(&h_cg->hugepage[idx]) * PAGE_SIZE);",
            "",
            "\t/*",
            "\t * For each node, transverse the css tree to obtain the hierarchical",
            "\t * node usage.",
            "\t */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\tusage = 0;",
            "\t\trcu_read_lock();",
            "\t\tcss_for_each_descendant_pre(css, &h_cg->css) {",
            "\t\t\tusage += READ_ONCE(hugetlb_cgroup_from_css(css)",
            "\t\t\t\t\t\t   ->nodeinfo[nid]",
            "\t\t\t\t\t\t   ->usage[idx]);",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t\tseq_printf(seq, \" N%d=%lu\", nid, usage * PAGE_SIZE);",
            "\t}",
            "",
            "\tseq_putc(seq, '\\n');",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "hugetlb_cgroup_uncharge_folio, hugetlb_cgroup_uncharge_folio_rsvd, __hugetlb_cgroup_uncharge_cgroup, hugetlb_cgroup_uncharge_cgroup, hugetlb_cgroup_uncharge_cgroup_rsvd, hugetlb_cgroup_uncharge_counter, hugetlb_cgroup_uncharge_file_region, hugetlb_cgroup_read_numa_stat",
          "description": "实现HugeTLB页面释放时的反向计费操作，包含对单个folio和整体cgroup的解除分配逻辑，以及读取NUMA节点统计信息的实现。",
          "similarity": 0.5665948390960693
        },
        {
          "chunk_id": 0,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 1,
          "end_line": 64,
          "content": [
            "/*",
            " *",
            " * Copyright IBM Corporation, 2012",
            " * Author Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>",
            " *",
            " * Cgroup v2",
            " * Copyright (C) 2019 Red Hat, Inc.",
            " * Author: Giuseppe Scrivano <gscrivan@redhat.com>",
            " *",
            " * This program is free software; you can redistribute it and/or modify it",
            " * under the terms of version 2.1 of the GNU Lesser General Public License",
            " * as published by the Free Software Foundation.",
            " *",
            " * This program is distributed in the hope that it would be useful, but",
            " * WITHOUT ANY WARRANTY; without even the implied warranty of",
            " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.",
            " *",
            " */",
            "",
            "#include <linux/cgroup.h>",
            "#include <linux/page_counter.h>",
            "#include <linux/slab.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/hugetlb_cgroup.h>",
            "",
            "#define MEMFILE_PRIVATE(x, val)\t(((x) << 16) | (val))",
            "#define MEMFILE_IDX(val)\t(((val) >> 16) & 0xffff)",
            "#define MEMFILE_ATTR(val)\t((val) & 0xffff)",
            "",
            "static struct hugetlb_cgroup *root_h_cgroup __read_mostly;",
            "",
            "static inline struct page_counter *",
            "__hugetlb_cgroup_counter_from_cgroup(struct hugetlb_cgroup *h_cg, int idx,",
            "\t\t\t\t     bool rsvd)",
            "{",
            "\tif (rsvd)",
            "\t\treturn &h_cg->rsvd_hugepage[idx];",
            "\treturn &h_cg->hugepage[idx];",
            "}",
            "",
            "static inline struct page_counter *",
            "hugetlb_cgroup_counter_from_cgroup(struct hugetlb_cgroup *h_cg, int idx)",
            "{",
            "\treturn __hugetlb_cgroup_counter_from_cgroup(h_cg, idx, false);",
            "}",
            "",
            "static inline struct page_counter *",
            "hugetlb_cgroup_counter_from_cgroup_rsvd(struct hugetlb_cgroup *h_cg, int idx)",
            "{",
            "\treturn __hugetlb_cgroup_counter_from_cgroup(h_cg, idx, true);",
            "}",
            "",
            "static inline",
            "struct hugetlb_cgroup *hugetlb_cgroup_from_css(struct cgroup_subsys_state *s)",
            "{",
            "\treturn s ? container_of(s, struct hugetlb_cgroup, css) : NULL;",
            "}",
            "",
            "static inline",
            "struct hugetlb_cgroup *hugetlb_cgroup_from_task(struct task_struct *task)",
            "{",
            "\treturn hugetlb_cgroup_from_css(task_css(task, hugetlb_cgrp_id));",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义HugeTLB cgroup子系统的辅助宏和基础结构，声明全局根cgroup指针及用于获取对应cgroup的内联函数，提供page_counter相关操作的封装接口。",
          "similarity": 0.5511230230331421
        },
        {
          "chunk_id": 1,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 65,
          "end_line": 177,
          "content": [
            "static inline bool hugetlb_cgroup_is_root(struct hugetlb_cgroup *h_cg)",
            "{",
            "\treturn (h_cg == root_h_cgroup);",
            "}",
            "static inline bool hugetlb_cgroup_have_usage(struct hugetlb_cgroup *h_cg)",
            "{",
            "\tstruct hstate *h;",
            "",
            "\tfor_each_hstate(h) {",
            "\t\tif (page_counter_read(",
            "\t\t    hugetlb_cgroup_counter_from_cgroup(h_cg, hstate_index(h))))",
            "\t\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "static void hugetlb_cgroup_init(struct hugetlb_cgroup *h_cgroup,",
            "\t\t\t\tstruct hugetlb_cgroup *parent_h_cgroup)",
            "{",
            "\tint idx;",
            "",
            "\tfor (idx = 0; idx < HUGE_MAX_HSTATE; idx++) {",
            "\t\tstruct page_counter *fault_parent = NULL;",
            "\t\tstruct page_counter *rsvd_parent = NULL;",
            "\t\tunsigned long limit;",
            "\t\tint ret;",
            "",
            "\t\tif (parent_h_cgroup) {",
            "\t\t\tfault_parent = hugetlb_cgroup_counter_from_cgroup(",
            "\t\t\t\tparent_h_cgroup, idx);",
            "\t\t\trsvd_parent = hugetlb_cgroup_counter_from_cgroup_rsvd(",
            "\t\t\t\tparent_h_cgroup, idx);",
            "\t\t}",
            "\t\tpage_counter_init(hugetlb_cgroup_counter_from_cgroup(h_cgroup,",
            "\t\t\t\t\t\t\t\t     idx),",
            "\t\t\t\t  fault_parent, false);",
            "\t\tpage_counter_init(",
            "\t\t\thugetlb_cgroup_counter_from_cgroup_rsvd(h_cgroup, idx),",
            "\t\t\trsvd_parent, false);",
            "",
            "\t\tlimit = round_down(PAGE_COUNTER_MAX,",
            "\t\t\t\t   pages_per_huge_page(&hstates[idx]));",
            "",
            "\t\tret = page_counter_set_max(",
            "\t\t\thugetlb_cgroup_counter_from_cgroup(h_cgroup, idx),",
            "\t\t\tlimit);",
            "\t\tVM_BUG_ON(ret);",
            "\t\tret = page_counter_set_max(",
            "\t\t\thugetlb_cgroup_counter_from_cgroup_rsvd(h_cgroup, idx),",
            "\t\t\tlimit);",
            "\t\tVM_BUG_ON(ret);",
            "\t}",
            "}",
            "static void hugetlb_cgroup_free(struct hugetlb_cgroup *h_cgroup)",
            "{",
            "\tint node;",
            "",
            "\tfor_each_node(node)",
            "\t\tkfree(h_cgroup->nodeinfo[node]);",
            "\tkfree(h_cgroup);",
            "}",
            "static void hugetlb_cgroup_css_free(struct cgroup_subsys_state *css)",
            "{",
            "\thugetlb_cgroup_free(hugetlb_cgroup_from_css(css));",
            "}",
            "static void hugetlb_cgroup_move_parent(int idx, struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t       struct page *page)",
            "{",
            "\tunsigned int nr_pages;",
            "\tstruct page_counter *counter;",
            "\tstruct hugetlb_cgroup *page_hcg;",
            "\tstruct hugetlb_cgroup *parent = parent_hugetlb_cgroup(h_cg);",
            "\tstruct folio *folio = page_folio(page);",
            "",
            "\tpage_hcg = hugetlb_cgroup_from_folio(folio);",
            "\t/*",
            "\t * We can have pages in active list without any cgroup",
            "\t * ie, hugepage with less than 3 pages. We can safely",
            "\t * ignore those pages.",
            "\t */",
            "\tif (!page_hcg || page_hcg != h_cg)",
            "\t\tgoto out;",
            "",
            "\tnr_pages = compound_nr(page);",
            "\tif (!parent) {",
            "\t\tparent = root_h_cgroup;",
            "\t\t/* root has no limit */",
            "\t\tpage_counter_charge(&parent->hugepage[idx], nr_pages);",
            "\t}",
            "\tcounter = &h_cg->hugepage[idx];",
            "\t/* Take the pages off the local counter */",
            "\tpage_counter_cancel(counter, nr_pages);",
            "",
            "\tset_hugetlb_cgroup(folio, parent);",
            "out:",
            "\treturn;",
            "}",
            "static void hugetlb_cgroup_css_offline(struct cgroup_subsys_state *css)",
            "{",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(css);",
            "\tstruct hstate *h;",
            "\tstruct page *page;",
            "",
            "\tdo {",
            "\t\tfor_each_hstate(h) {",
            "\t\t\tspin_lock_irq(&hugetlb_lock);",
            "\t\t\tlist_for_each_entry(page, &h->hugepage_activelist, lru)",
            "\t\t\t\thugetlb_cgroup_move_parent(hstate_index(h), h_cg, page);",
            "",
            "\t\t\tspin_unlock_irq(&hugetlb_lock);",
            "\t\t}",
            "\t\tcond_resched();",
            "\t} while (hugetlb_cgroup_have_usage(h_cg));",
            "}"
          ],
          "function_name": "hugetlb_cgroup_is_root, hugetlb_cgroup_have_usage, hugetlb_cgroup_init, hugetlb_cgroup_free, hugetlb_cgroup_css_free, hugetlb_cgroup_move_parent, hugetlb_cgroup_css_offline",
          "description": "实现HugeTLB cgroup的核心管理逻辑，包含判断是否为根节点、检测使用量、初始化/释放cgroup结构、迁移父节点、处理cgroup离线状态等功能。",
          "similarity": 0.46208953857421875
        },
        {
          "chunk_id": 4,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 520,
          "end_line": 626,
          "content": [
            "static u64 hugetlb_cgroup_read_u64(struct cgroup_subsys_state *css,",
            "\t\t\t\t   struct cftype *cft)",
            "{",
            "\tstruct page_counter *counter;",
            "\tstruct page_counter *rsvd_counter;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(css);",
            "",
            "\tcounter = &h_cg->hugepage[MEMFILE_IDX(cft->private)];",
            "\trsvd_counter = &h_cg->rsvd_hugepage[MEMFILE_IDX(cft->private)];",
            "",
            "\tswitch (MEMFILE_ATTR(cft->private)) {",
            "\tcase RES_USAGE:",
            "\t\treturn (u64)page_counter_read(counter) * PAGE_SIZE;",
            "\tcase RES_RSVD_USAGE:",
            "\t\treturn (u64)page_counter_read(rsvd_counter) * PAGE_SIZE;",
            "\tcase RES_LIMIT:",
            "\t\treturn (u64)counter->max * PAGE_SIZE;",
            "\tcase RES_RSVD_LIMIT:",
            "\t\treturn (u64)rsvd_counter->max * PAGE_SIZE;",
            "\tcase RES_MAX_USAGE:",
            "\t\treturn (u64)counter->watermark * PAGE_SIZE;",
            "\tcase RES_RSVD_MAX_USAGE:",
            "\t\treturn (u64)rsvd_counter->watermark * PAGE_SIZE;",
            "\tcase RES_FAILCNT:",
            "\t\treturn counter->failcnt;",
            "\tcase RES_RSVD_FAILCNT:",
            "\t\treturn rsvd_counter->failcnt;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static int hugetlb_cgroup_read_u64_max(struct seq_file *seq, void *v)",
            "{",
            "\tint idx;",
            "\tu64 val;",
            "\tstruct cftype *cft = seq_cft(seq);",
            "\tunsigned long limit;",
            "\tstruct page_counter *counter;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(seq_css(seq));",
            "",
            "\tidx = MEMFILE_IDX(cft->private);",
            "\tcounter = &h_cg->hugepage[idx];",
            "",
            "\tlimit = round_down(PAGE_COUNTER_MAX,",
            "\t\t\t   pages_per_huge_page(&hstates[idx]));",
            "",
            "\tswitch (MEMFILE_ATTR(cft->private)) {",
            "\tcase RES_RSVD_USAGE:",
            "\t\tcounter = &h_cg->rsvd_hugepage[idx];",
            "\t\tfallthrough;",
            "\tcase RES_USAGE:",
            "\t\tval = (u64)page_counter_read(counter);",
            "\t\tseq_printf(seq, \"%llu\\n\", val * PAGE_SIZE);",
            "\t\tbreak;",
            "\tcase RES_RSVD_LIMIT:",
            "\t\tcounter = &h_cg->rsvd_hugepage[idx];",
            "\t\tfallthrough;",
            "\tcase RES_LIMIT:",
            "\t\tval = (u64)counter->max;",
            "\t\tif (val == limit)",
            "\t\t\tseq_puts(seq, \"max\\n\");",
            "\t\telse",
            "\t\t\tseq_printf(seq, \"%llu\\n\", val * PAGE_SIZE);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static ssize_t hugetlb_cgroup_write(struct kernfs_open_file *of,",
            "\t\t\t\t    char *buf, size_t nbytes, loff_t off,",
            "\t\t\t\t    const char *max)",
            "{",
            "\tint ret, idx;",
            "\tunsigned long nr_pages;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(of_css(of));",
            "\tbool rsvd = false;",
            "",
            "\tif (hugetlb_cgroup_is_root(h_cg)) /* Can't set limit on root */",
            "\t\treturn -EINVAL;",
            "",
            "\tbuf = strstrip(buf);",
            "\tret = page_counter_memparse(buf, max, &nr_pages);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tidx = MEMFILE_IDX(of_cft(of)->private);",
            "\tnr_pages = round_down(nr_pages, pages_per_huge_page(&hstates[idx]));",
            "",
            "\tswitch (MEMFILE_ATTR(of_cft(of)->private)) {",
            "\tcase RES_RSVD_LIMIT:",
            "\t\trsvd = true;",
            "\t\tfallthrough;",
            "\tcase RES_LIMIT:",
            "\t\tmutex_lock(&hugetlb_limit_mutex);",
            "\t\tret = page_counter_set_max(",
            "\t\t\t__hugetlb_cgroup_counter_from_cgroup(h_cg, idx, rsvd),",
            "\t\t\tnr_pages);",
            "\t\tmutex_unlock(&hugetlb_limit_mutex);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tret = -EINVAL;",
            "\t\tbreak;",
            "\t}",
            "\treturn ret ?: nbytes;",
            "}"
          ],
          "function_name": "hugetlb_cgroup_read_u64, hugetlb_cgroup_read_u64_max, hugetlb_cgroup_write",
          "description": "提供HugeTLB cgroup的监控接口，实现读取当前使用量、限制等参数的功能，并支持通过接口设置内存限制参数。",
          "similarity": 0.4620281159877777
        }
      ]
    }
  ]
}