{
  "query": "RAID redundancy mechanisms",
  "timestamp": "2025-12-26 00:26:39",
  "retrieved_files": [
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.546879231929779,
      "chunks": [
        {
          "chunk_id": 22,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4072,
          "end_line": 4226,
          "content": [
            "static void rcu_barrier_trace(const char *s, int cpu, unsigned long done)",
            "{",
            "\ttrace_rcu_barrier(rcu_state.name, s, cpu,",
            "\t\t\t  atomic_read(&rcu_state.barrier_cpu_count), done);",
            "}",
            "static void rcu_barrier_callback(struct rcu_head *rhp)",
            "{",
            "\tunsigned long __maybe_unused s = rcu_state.barrier_sequence;",
            "",
            "\tif (atomic_dec_and_test(&rcu_state.barrier_cpu_count)) {",
            "\t\trcu_barrier_trace(TPS(\"LastCB\"), -1, s);",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "\t} else {",
            "\t\trcu_barrier_trace(TPS(\"CB\"), -1, s);",
            "\t}",
            "}",
            "static void rcu_barrier_entrain(struct rcu_data *rdp)",
            "{",
            "\tunsigned long gseq = READ_ONCE(rcu_state.barrier_sequence);",
            "\tunsigned long lseq = READ_ONCE(rdp->barrier_seq_snap);",
            "\tbool wake_nocb = false;",
            "\tbool was_alldone = false;",
            "",
            "\tlockdep_assert_held(&rcu_state.barrier_lock);",
            "\tif (rcu_seq_state(lseq) || !rcu_seq_state(gseq) || rcu_seq_ctr(lseq) != rcu_seq_ctr(gseq))",
            "\t\treturn;",
            "\trcu_barrier_trace(TPS(\"IRQ\"), -1, rcu_state.barrier_sequence);",
            "\trdp->barrier_head.func = rcu_barrier_callback;",
            "\tdebug_rcu_head_queue(&rdp->barrier_head);",
            "\trcu_nocb_lock(rdp);",
            "\t/*",
            "\t * Flush bypass and wakeup rcuog if we add callbacks to an empty regular",
            "\t * queue. This way we don't wait for bypass timer that can reach seconds",
            "\t * if it's fully lazy.",
            "\t */",
            "\twas_alldone = rcu_rdp_is_offloaded(rdp) && !rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));",
            "\twake_nocb = was_alldone && rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tif (rcu_segcblist_entrain(&rdp->cblist, &rdp->barrier_head)) {",
            "\t\tatomic_inc(&rcu_state.barrier_cpu_count);",
            "\t} else {",
            "\t\tdebug_rcu_head_unqueue(&rdp->barrier_head);",
            "\t\trcu_barrier_trace(TPS(\"IRQNQ\"), -1, rcu_state.barrier_sequence);",
            "\t}",
            "\trcu_nocb_unlock(rdp);",
            "\tif (wake_nocb)",
            "\t\twake_nocb_gp(rdp, false);",
            "\tsmp_store_release(&rdp->barrier_seq_snap, gseq);",
            "}",
            "static void rcu_barrier_handler(void *cpu_in)",
            "{",
            "\tuintptr_t cpu = (uintptr_t)cpu_in;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\tWARN_ON_ONCE(cpu != rdp->cpu);",
            "\tWARN_ON_ONCE(cpu != smp_processor_id());",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\trcu_barrier_entrain(rdp);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "}",
            "void rcu_barrier(void)",
            "{",
            "\tuintptr_t cpu;",
            "\tunsigned long flags;",
            "\tunsigned long gseq;",
            "\tstruct rcu_data *rdp;",
            "\tunsigned long s = rcu_seq_snap(&rcu_state.barrier_sequence);",
            "",
            "\trcu_barrier_trace(TPS(\"Begin\"), -1, s);",
            "",
            "\t/* Take mutex to serialize concurrent rcu_barrier() requests. */",
            "\tmutex_lock(&rcu_state.barrier_mutex);",
            "",
            "\t/* Did someone else do our work for us? */",
            "\tif (rcu_seq_done(&rcu_state.barrier_sequence, s)) {",
            "\t\trcu_barrier_trace(TPS(\"EarlyExit\"), -1, rcu_state.barrier_sequence);",
            "\t\tsmp_mb(); /* caller's subsequent code after above check. */",
            "\t\tmutex_unlock(&rcu_state.barrier_mutex);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Mark the start of the barrier operation. */",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\trcu_seq_start(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\trcu_barrier_trace(TPS(\"Inc1\"), -1, rcu_state.barrier_sequence);",
            "",
            "\t/*",
            "\t * Initialize the count to two rather than to zero in order",
            "\t * to avoid a too-soon return to zero in case of an immediate",
            "\t * invocation of the just-enqueued callback (or preemption of",
            "\t * this task).  Exclude CPU-hotplug operations to ensure that no",
            "\t * offline non-offloaded CPU has callbacks queued.",
            "\t */",
            "\tinit_completion(&rcu_state.barrier_completion);",
            "\tatomic_set(&rcu_state.barrier_cpu_count, 2);",
            "\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "",
            "\t/*",
            "\t * Force each CPU with callbacks to register a new callback.",
            "\t * When that callback is invoked, we will know that all of the",
            "\t * corresponding CPU's preceding callbacks have been invoked.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "retry:",
            "\t\tif (smp_load_acquire(&rdp->barrier_seq_snap) == gseq)",
            "\t\t\tcontinue;",
            "\t\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\t\tif (!rcu_segcblist_n_cbs(&rdp->cblist)) {",
            "\t\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"NQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (!rcu_rdp_cpu_online(rdp)) {",
            "\t\t\trcu_barrier_entrain(rdp);",
            "\t\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"OfflineNoCBQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\tif (smp_call_function_single(cpu, rcu_barrier_handler, (void *)cpu, 1)) {",
            "\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\tgoto retry;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\trcu_barrier_trace(TPS(\"OnlineQ\"), cpu, rcu_state.barrier_sequence);",
            "\t}",
            "",
            "\t/*",
            "\t * Now that we have an rcu_barrier_callback() callback on each",
            "\t * CPU, and thus each counted, remove the initial count.",
            "\t */",
            "\tif (atomic_sub_and_test(2, &rcu_state.barrier_cpu_count))",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "",
            "\t/* Wait for all rcu_barrier_callback() callbacks to be invoked. */",
            "\twait_for_completion(&rcu_state.barrier_completion);",
            "",
            "\t/* Mark the end of the barrier operation. */",
            "\trcu_barrier_trace(TPS(\"Inc2\"), -1, rcu_state.barrier_sequence);",
            "\trcu_seq_end(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t}",
            "",
            "\t/* Other rcu_barrier() invocations can now safely proceed. */",
            "\tmutex_unlock(&rcu_state.barrier_mutex);",
            "}"
          ],
          "function_name": "rcu_barrier_trace, rcu_barrier_callback, rcu_barrier_entrain, rcu_barrier_handler, rcu_barrier",
          "description": "实现RCU屏障功能，通过分发回调函数强制所有CPU完成当前RCU操作，使用原子计数器跟踪完成状态，通过completion等待所有回调完成",
          "similarity": 0.5583927631378174
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 908,
          "end_line": 1026,
          "content": [
            "static void trace_rcu_this_gp(struct rcu_node *rnp, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req, const char *s)",
            "{",
            "\ttrace_rcu_future_grace_period(rcu_state.name, READ_ONCE(rnp->gp_seq),",
            "\t\t\t\t      gp_seq_req, rnp->level,",
            "\t\t\t\t      rnp->grplo, rnp->grphi, s);",
            "}",
            "static bool rcu_start_this_gp(struct rcu_node *rnp_start, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req)",
            "{",
            "\tbool ret = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\t/*",
            "\t * Use funnel locking to either acquire the root rcu_node",
            "\t * structure's lock or bail out if the need for this grace period",
            "\t * has already been recorded -- or if that grace period has in",
            "\t * fact already started.  If there is already a grace period in",
            "\t * progress in a non-leaf node, no recording is needed because the",
            "\t * end of the grace period will scan the leaf rcu_node structures.",
            "\t * Note that rnp_start->lock must not be released.",
            "\t */",
            "\traw_lockdep_assert_held_rcu_node(rnp_start);",
            "\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req, TPS(\"Startleaf\"));",
            "\tfor (rnp = rnp_start; 1; rnp = rnp->parent) {",
            "\t\tif (rnp != rnp_start)",
            "\t\t\traw_spin_lock_rcu_node(rnp);",
            "\t\tif (ULONG_CMP_GE(rnp->gp_seq_needed, gp_seq_req) ||",
            "\t\t    rcu_seq_started(&rnp->gp_seq, gp_seq_req) ||",
            "\t\t    (rnp != rnp_start &&",
            "\t\t     rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))) {",
            "\t\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Prestarted\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tWRITE_ONCE(rnp->gp_seq_needed, gp_seq_req);",
            "\t\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq))) {",
            "\t\t\t/*",
            "\t\t\t * We just marked the leaf or internal node, and a",
            "\t\t\t * grace period is in progress, which means that",
            "\t\t\t * rcu_gp_cleanup() will see the marking.  Bail to",
            "\t\t\t * reduce contention.",
            "\t\t\t */",
            "\t\t\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Startedleaf\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tif (rnp != rnp_start && rnp->parent != NULL)",
            "\t\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\tif (!rnp->parent)",
            "\t\t\tbreak;  /* At root, and perhaps also leaf. */",
            "\t}",
            "",
            "\t/* If GP already in progress, just leave, otherwise start one. */",
            "\tif (rcu_gp_in_progress()) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedleafroot\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedroot\"));",
            "\tWRITE_ONCE(rcu_state.gp_flags, rcu_state.gp_flags | RCU_GP_FLAG_INIT);",
            "\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);",
            "\tif (!READ_ONCE(rcu_state.gp_kthread)) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"NoGPkthread\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_grace_period(rcu_state.name, data_race(rcu_state.gp_seq), TPS(\"newreq\"));",
            "\tret = true;  /* Caller must wake GP kthread. */",
            "unlock_out:",
            "\t/* Push furthest requested GP to leaf node and rcu_data structure. */",
            "\tif (ULONG_CMP_LT(gp_seq_req, rnp->gp_seq_needed)) {",
            "\t\tWRITE_ONCE(rnp_start->gp_seq_needed, rnp->gp_seq_needed);",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\t}",
            "\tif (rnp != rnp_start)",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\treturn ret;",
            "}",
            "static bool rcu_future_gp_cleanup(struct rcu_node *rnp)",
            "{",
            "\tbool needmore;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tneedmore = ULONG_CMP_LT(rnp->gp_seq, rnp->gp_seq_needed);",
            "\tif (!needmore)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq; /* Avoid counter wrap. */",
            "\ttrace_rcu_this_gp(rnp, rdp, rnp->gp_seq,",
            "\t\t\t  needmore ? TPS(\"CleanupMore\") : TPS(\"Cleanup\"));",
            "\treturn needmore;",
            "}",
            "static void swake_up_one_online_ipi(void *arg)",
            "{",
            "\tstruct swait_queue_head *wqh = arg;",
            "",
            "\tswake_up_one(wqh);",
            "}",
            "static void swake_up_one_online(struct swait_queue_head *wqh)",
            "{",
            "\tint cpu = get_cpu();",
            "",
            "\t/*",
            "\t * If called from rcutree_report_cpu_starting(), wake up",
            "\t * is dangerous that late in the CPU-down hotplug process. The",
            "\t * scheduler might queue an ignored hrtimer. Defer the wake up",
            "\t * to an online CPU instead.",
            "\t */",
            "\tif (unlikely(cpu_is_offline(cpu))) {",
            "\t\tint target;",
            "",
            "\t\ttarget = cpumask_any_and(housekeeping_cpumask(HK_TYPE_RCU),",
            "\t\t\t\t\t cpu_online_mask);",
            "",
            "\t\tsmp_call_function_single(target, swake_up_one_online_ipi,",
            "\t\t\t\t\t wqh, 0);",
            "\t\tput_cpu();",
            "\t} else {",
            "\t\tput_cpu();",
            "\t\tswake_up_one(wqh);",
            "\t}",
            "}"
          ],
          "function_name": "trace_rcu_this_gp, rcu_start_this_gp, rcu_future_gp_cleanup, swake_up_one_online_ipi, swake_up_one_online",
          "description": "实现RCU grace period事件追踪、新grace period启动逻辑及未来grace period清理机制，支持跨层级节点的同步状态传播。",
          "similarity": 0.5497298240661621
        },
        {
          "chunk_id": 21,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3787,
          "end_line": 3910,
          "content": [
            "void get_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\t/*",
            "\t * Any prior manipulation of RCU-protected data must happen",
            "\t * before the loads from ->gp_seq and ->expedited_sequence.",
            "\t */",
            "\tsmp_mb();  /* ^^^ */",
            "\trgosp->rgos_norm = rcu_seq_snap(&rnp->gp_seq);",
            "\trgosp->rgos_exp = rcu_seq_snap(&rcu_state.expedited_sequence);",
            "}",
            "static void start_poll_synchronize_rcu_common(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool needwake;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tlockdep_assert_irqs_enabled();",
            "\tlocal_irq_save(flags);",
            "\trdp = this_cpu_ptr(&rcu_data);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_rcu_node(rnp); // irqs already disabled.",
            "\t// Note it is possible for a grace period to have elapsed between",
            "\t// the above call to get_state_synchronize_rcu() and the below call",
            "\t// to rcu_seq_snap.  This is OK, the worst that happens is that we",
            "\t// get a grace period that no one needed.  These accesses are ordered",
            "\t// by smp_mb(), and we are accessing them in the opposite order",
            "\t// from which they are updated at grace-period start, as required.",
            "\tneedwake = rcu_start_this_gp(rnp, rdp, rcu_seq_snap(&rcu_state.gp_seq));",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "}",
            "unsigned long start_poll_synchronize_rcu(void)",
            "{",
            "\tunsigned long gp_seq = get_state_synchronize_rcu();",
            "",
            "\tstart_poll_synchronize_rcu_common();",
            "\treturn gp_seq;",
            "}",
            "void start_poll_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tget_state_synchronize_rcu_full(rgosp);",
            "",
            "\tstart_poll_synchronize_rcu_common();",
            "}",
            "bool poll_state_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\tif (oldstate == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rcu_state.gp_seq_polled, oldstate)) {",
            "\t\tsmp_mb(); /* Ensure GP ends before subsequent accesses. */",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "bool poll_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tsmp_mb(); // Order against root rcu_node structure grace-period cleanup.",
            "\tif (rgosp->rgos_norm == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rnp->gp_seq, rgosp->rgos_norm) ||",
            "\t    rgosp->rgos_exp == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rcu_state.expedited_sequence, rgosp->rgos_exp)) {",
            "\t\tsmp_mb(); /* Ensure GP ends before subsequent accesses. */",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "void cond_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\tif (!poll_state_synchronize_rcu(oldstate))",
            "\t\tsynchronize_rcu();",
            "}",
            "void cond_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tif (!poll_state_synchronize_rcu_full(rgosp))",
            "\t\tsynchronize_rcu();",
            "}",
            "static int rcu_pending(int user)",
            "{",
            "\tbool gp_in_progress;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "\tstruct rcu_node *rnp = rdp->mynode;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\t/* Check for CPU stalls, if enabled. */",
            "\tcheck_cpu_stall(rdp);",
            "",
            "\t/* Does this CPU need a deferred NOCB wakeup? */",
            "\tif (rcu_nocb_need_deferred_wakeup(rdp, RCU_NOCB_WAKE))",
            "\t\treturn 1;",
            "",
            "\t/* Is this a nohz_full CPU in userspace or idle?  (Ignore RCU if so.) */",
            "\tif ((user || rcu_is_cpu_rrupt_from_idle()) && rcu_nohz_full_cpu())",
            "\t\treturn 0;",
            "",
            "\t/* Is the RCU core waiting for a quiescent state from this CPU? */",
            "\tgp_in_progress = rcu_gp_in_progress();",
            "\tif (rdp->core_needs_qs && !rdp->cpu_no_qs.b.norm && gp_in_progress)",
            "\t\treturn 1;",
            "",
            "\t/* Does this CPU have callbacks ready to invoke? */",
            "\tif (!rcu_rdp_is_offloaded(rdp) &&",
            "\t    rcu_segcblist_ready_cbs(&rdp->cblist))",
            "\t\treturn 1;",
            "",
            "\t/* Has RCU gone idle with this CPU needing another grace period? */",
            "\tif (!gp_in_progress && rcu_segcblist_is_enabled(&rdp->cblist) &&",
            "\t    !rcu_rdp_is_offloaded(rdp) &&",
            "\t    !rcu_segcblist_restempty(&rdp->cblist, RCU_NEXT_READY_TAIL))",
            "\t\treturn 1;",
            "",
            "\t/* Have RCU grace period completed or started?  */",
            "\tif (rcu_seq_current(&rnp->gp_seq) != rdp->gp_seq ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) /* outside lock */",
            "\t\treturn 1;",
            "",
            "\t/* nothing to do */",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_state_synchronize_rcu_full, start_poll_synchronize_rcu_common, start_poll_synchronize_rcu, start_poll_synchronize_rcu_full, poll_state_synchronize_rcu, poll_state_synchronize_rcu_full, cond_synchronize_rcu, cond_synchronize_rcu_full, rcu_pending",
          "description": "提供RCU宽限期状态查询和触发机制，通过序列号比对判断是否需要启动新的宽限期，处理回调队列唤醒逻辑，实现条件同步检查",
          "similarity": 0.5469680428504944
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2523,
          "end_line": 2628,
          "content": [
            "static void rcu_cpu_kthread_park(unsigned int cpu)",
            "{",
            "\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;",
            "}",
            "static int rcu_cpu_kthread_should_run(unsigned int cpu)",
            "{",
            "\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);",
            "}",
            "static void rcu_cpu_kthread(unsigned int cpu)",
            "{",
            "\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);",
            "\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);",
            "\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);",
            "\tint spincnt;",
            "",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));",
            "\tfor (spincnt = 0; spincnt < 10; spincnt++) {",
            "\t\tWRITE_ONCE(*j, jiffies);",
            "\t\tlocal_bh_disable();",
            "\t\t*statusp = RCU_KTHREAD_RUNNING;",
            "\t\tlocal_irq_disable();",
            "\t\twork = *workp;",
            "\t\tWRITE_ONCE(*workp, 0);",
            "\t\tlocal_irq_enable();",
            "\t\tif (work)",
            "\t\t\trcu_core();",
            "\t\tlocal_bh_enable();",
            "\t\tif (!READ_ONCE(*workp)) {",
            "\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));",
            "\t\t\t*statusp = RCU_KTHREAD_WAITING;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "\t*statusp = RCU_KTHREAD_YIELDING;",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));",
            "\tschedule_timeout_idle(2);",
            "\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));",
            "\t*statusp = RCU_KTHREAD_WAITING;",
            "\tWRITE_ONCE(*j, jiffies);",
            "}",
            "static int __init rcu_spawn_core_kthreads(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;",
            "\tif (use_softirq)",
            "\t\treturn 0;",
            "\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),",
            "\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);",
            "\treturn 0;",
            "}",
            "static void rcutree_enqueue(struct rcu_data *rdp, struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\trcu_segcblist_enqueue(&rdp->cblist, head);",
            "\tif (__is_kvfree_rcu_offset((unsigned long)func))",
            "\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,",
            "\t\t\t\t\t (unsigned long)func,",
            "\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));",
            "\telse",
            "\t\ttrace_rcu_callback(rcu_state.name, head,",
            "\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));",
            "}",
            "static void call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,",
            "\t\t\t  rcu_callback_t func, unsigned long flags)",
            "{",
            "\trcutree_enqueue(rdp, head, func);",
            "\t/*",
            "\t * If called from an extended quiescent state, invoke the RCU",
            "\t * core in order to force a re-evaluation of RCU's idleness.",
            "\t */",
            "\tif (!rcu_is_watching())",
            "\t\tinvoke_rcu_core();",
            "",
            "\t/* If interrupts were disabled or CPU offline, don't invoke RCU core. */",
            "\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Force the grace period if too many callbacks or too long waiting.",
            "\t * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()",
            "\t * if some other CPU has recently done so.  Also, don't bother",
            "\t * invoking rcu_force_quiescent_state() if the newly enqueued callback",
            "\t * is the only one waiting for a grace period to complete.",
            "\t */",
            "\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >",
            "\t\t     rdp->qlen_last_fqs_check + qhimark)) {",
            "",
            "\t\t/* Are we ignoring a completed grace period? */",
            "\t\tnote_gp_changes(rdp);",
            "",
            "\t\t/* Start a new grace period if one not already started. */",
            "\t\tif (!rcu_gp_in_progress()) {",
            "\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);",
            "\t\t} else {",
            "\t\t\t/* Give the grace period a kick. */",
            "\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;",
            "\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&",
            "\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)",
            "\t\t\t\trcu_force_quiescent_state();",
            "\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "rcu_cpu_kthread_park, rcu_cpu_kthread_should_run, rcu_cpu_kthread, rcu_spawn_core_kthreads, rcutree_enqueue, call_rcu_core",
          "description": "实现RCU k线程管理与回调分发基础设施，包含线程启动、回调入队及触发条件判断逻辑，提供跨CPU的异步处理能力",
          "similarity": 0.5443238019943237
        },
        {
          "chunk_id": 19,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3436,
          "end_line": 3574,
          "content": [
            "void kvfree_call_rcu(struct rcu_head *head, void *ptr)",
            "{",
            "\tunsigned long flags;",
            "\tstruct kfree_rcu_cpu *krcp;",
            "\tbool success;",
            "",
            "\t/*",
            "\t * Please note there is a limitation for the head-less",
            "\t * variant, that is why there is a clear rule for such",
            "\t * objects: it can be used from might_sleep() context",
            "\t * only. For other places please embed an rcu_head to",
            "\t * your data.",
            "\t */",
            "\tif (!head)",
            "\t\tmight_sleep();",
            "",
            "\t// Queue the object but don't yet schedule the batch.",
            "\tif (debug_rcu_head_queue(ptr)) {",
            "\t\t// Probable double kfree_rcu(), just leak.",
            "\t\tWARN_ONCE(1, \"%s(): Double-freed call. rcu_head %p\\n\",",
            "\t\t\t  __func__, head);",
            "",
            "\t\t// Mark as success and leave.",
            "\t\treturn;",
            "\t}",
            "",
            "\tkasan_record_aux_stack_noalloc(ptr);",
            "\tsuccess = add_ptr_to_bulk_krc_lock(&krcp, &flags, ptr, !head);",
            "\tif (!success) {",
            "\t\trun_page_cache_worker(krcp);",
            "",
            "\t\tif (head == NULL)",
            "\t\t\t// Inline if kvfree_rcu(one_arg) call.",
            "\t\t\tgoto unlock_return;",
            "",
            "\t\thead->func = ptr;",
            "\t\thead->next = krcp->head;",
            "\t\tWRITE_ONCE(krcp->head, head);",
            "\t\tatomic_inc(&krcp->head_count);",
            "",
            "\t\t// Take a snapshot for this krcp.",
            "\t\tkrcp->head_gp_snap = get_state_synchronize_rcu();",
            "\t\tsuccess = true;",
            "\t}",
            "",
            "\t/*",
            "\t * The kvfree_rcu() caller considers the pointer freed at this point",
            "\t * and likely removes any references to it. Since the actual slab",
            "\t * freeing (and kmemleak_free()) is deferred, tell kmemleak to ignore",
            "\t * this object (no scanning or false positives reporting).",
            "\t */",
            "\tkmemleak_ignore(ptr);",
            "",
            "\t// Set timer to drain after KFREE_DRAIN_JIFFIES.",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_RUNNING)",
            "\t\t__schedule_delayed_monitor_work(krcp);",
            "",
            "unlock_return:",
            "\tkrc_this_cpu_unlock(krcp, flags);",
            "",
            "\t/*",
            "\t * Inline kvfree() after synchronize_rcu(). We can do",
            "\t * it from might_sleep() context only, so the current",
            "\t * CPU can pass the QS state.",
            "\t */",
            "\tif (!success) {",
            "\t\tdebug_rcu_head_unqueue((struct rcu_head *) ptr);",
            "\t\tsynchronize_rcu();",
            "\t\tkvfree(ptr);",
            "\t}",
            "}",
            "void kvfree_rcu_barrier(void)",
            "{",
            "\tstruct kfree_rcu_cpu_work *krwp;",
            "\tstruct kfree_rcu_cpu *krcp;",
            "\tbool queued;",
            "\tint i, cpu;",
            "",
            "\t/*",
            "\t * Firstly we detach objects and queue them over an RCU-batch",
            "\t * for all CPUs. Finally queued works are flushed for each CPU.",
            "\t *",
            "\t * Please note. If there are outstanding batches for a particular",
            "\t * CPU, those have to be finished first following by queuing a new.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tkrcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\t/*",
            "\t\t * Check if this CPU has any objects which have been queued for a",
            "\t\t * new GP completion. If not(means nothing to detach), we are done",
            "\t\t * with it. If any batch is pending/running for this \"krcp\", below",
            "\t\t * per-cpu flush_rcu_work() waits its completion(see last step).",
            "\t\t */",
            "\t\tif (!need_offload_krc(krcp))",
            "\t\t\tcontinue;",
            "",
            "\t\twhile (1) {",
            "\t\t\t/*",
            "\t\t\t * If we are not able to queue a new RCU work it means:",
            "\t\t\t * - batches for this CPU are still in flight which should",
            "\t\t\t *   be flushed first and then repeat;",
            "\t\t\t * - no objects to detach, because of concurrency.",
            "\t\t\t */",
            "\t\t\tqueued = kvfree_rcu_queue_batch(krcp);",
            "",
            "\t\t\t/*",
            "\t\t\t * Bail out, if there is no need to offload this \"krcp\"",
            "\t\t\t * anymore. As noted earlier it can run concurrently.",
            "\t\t\t */",
            "\t\t\tif (queued || !need_offload_krc(krcp))",
            "\t\t\t\tbreak;",
            "",
            "\t\t\t/* There are ongoing batches. */",
            "\t\t\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\t\t\tkrwp = &(krcp->krw_arr[i]);",
            "\t\t\t\tflush_rcu_work(&krwp->rcu_work);",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Now we guarantee that all objects are flushed.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tkrcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\t/*",
            "\t\t * A monitor work can drain ready to reclaim objects",
            "\t\t * directly. Wait its completion if running or pending.",
            "\t\t */",
            "\t\tcancel_delayed_work_sync(&krcp->monitor_work);",
            "",
            "\t\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\t\tkrwp = &(krcp->krw_arr[i]);",
            "\t\t\tflush_rcu_work(&krwp->rcu_work);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "kvfree_call_rcu, kvfree_rcu_barrier",
          "description": "实现基于RCU的延迟内存释放接口，提供安全释放路径并保证内存屏障语义，强制同步清理所有挂起的释放请求。",
          "similarity": 0.5380451679229736
        }
      ]
    },
    {
      "source_file": "kernel/cgroup/rdma.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:50:50\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `cgroup\\rdma.c`\n\n---\n\n# cgroup/rdma.c 技术文档\n\n## 文件概述\n\n`cgroup/rdma.c` 实现了 RDMA（Remote Direct Memory Access）资源限制控制器，作为 cgroup 子系统的一部分。该模块用于限制 cgroup 层级结构中的进程在达到指定资源上限后无法继续消耗额外的 RDMA 资源。通过为每个 cgroup 和每个 RDMA 设备维护资源池，实现对 RDMA 资源（如 HCA 句柄和对象）的精细化配额管理与层级化计费（charge/uncharge）。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`rdmacg_resource`**  \n  表示单个 RDMA 资源类型的使用情况，包含 `max`（最大限额）和 `usage`（当前使用量）。\n\n- **`rdmacg_resource_pool`**  \n  表示一个 cgroup 在特定 RDMA 设备上的资源池，包含：\n  - 指向 `rdmacg_device` 的指针\n  - 各类资源的 `rdmacg_resource` 数组\n  - 双向链表节点（分别链接到 cgroup 和设备的资源池列表）\n  - `usage_sum`：该池中所有资源的总使用计数\n  - `num_max_cnt`：设置为 `S32_MAX`（即无限制）的资源项数量\n\n- **`rdmacg_resource_names`**  \n  用户可见的资源名称映射表，当前支持：\n  - `\"hca_handle\"` → `RDMACG_RESOURCE_HCA_HANDLE`\n  - `\"hca_object\"` → `RDMACG_RESOURCE_HCA_OBJECT`\n\n### 主要函数\n\n- **`rdmacg_try_charge()`**  \n  尝试在 cgroup 层级中为指定 RDMA 设备和资源类型进行资源计费。从当前 cgroup 向上遍历至根，逐级检查并增加使用量。若任一层级超出限额，则回滚并返回 `-EAGAIN`。\n\n- **`rdmacg_uncharge()`**  \n  在 cgroup 层级中释放指定资源的使用量，从当前 cgroup 向上遍历至根，逐级减少使用量。\n\n- **`rdmacg_uncharge_hierarchy()`**  \n  支持在指定停止点（`stop_cg`）前的层级范围内执行资源释放，用于更灵活的资源回收场景。\n\n- **`get_cg_rpool_locked()` / `find_cg_rpool_locked()`**  \n  在加锁状态下查找或创建指定 cgroup 与设备对应的资源池。\n\n- **`free_cg_rpool_locked()`**  \n  当资源池的 `usage_sum` 为 0 且所有资源均设为 `max`（即未显式限制）时，安全释放该资源池。\n\n## 关键实现\n\n### 层级化资源计费机制\n\nRDMA cgroup 采用**自底向上计费、自顶向下限制**的策略：\n- **计费（charge）**：从当前任务所属 cgroup 开始，逐级向上（至根 cgroup）尝试增加资源使用量。任一祖先 cgroup 超限即失败。\n- **释放（uncharge）**：同样沿层级向上释放，确保资源使用量始终反映实际占用。\n\n### 资源池生命周期管理\n\n- 每个 `(cgroup, device)` 对应一个 `rdmacg_resource_pool`。\n- 资源池在首次计费时按需创建（`get_cg_rpool_locked`）。\n- 当 `usage_sum == 0` 且所有资源项均为 `max`（即无显式限制）时，自动释放资源池以节省内存。\n\n### 限额表示\n\n- 使用 `S32_MAX` 表示“无限制”（即 `max` 值）。\n- `num_max_cnt` 用于快速判断是否所有资源均为无限制状态，从而决定是否可安全释放资源池。\n\n### 并发控制\n\n- 全局互斥锁 `rdmacg_mutex` 保护：\n  - 所有 cgroup 的资源池链表（`cg->rpools`）\n  - 所有 RDMA 设备的资源池链表（`device->rpools`）\n  - 全局设备列表 `rdmacg_devices`\n- 所有资源池操作（创建、查找、释放）均在锁保护下进行。\n\n## 依赖关系\n\n- **内核头文件依赖**：\n  - `<linux/cgroup.h>`：cgroup 核心框架\n  - `<linux/cgroup_rdma.h>`：RDMA cgroup 接口定义（如 `rdma_cgroup`、`rdmacg_device` 等）\n  - `<linux/ib_verbs.h>`（隐含）：RDMA 资源类型定义（如 `RDMACG_RESOURCE_HCA_HANDLE`）\n- **导出符号**：\n  - `rdmacg_uncharge()`：供 RDMA 驱动（如 InfiniBand、RoCE 驱动）在释放资源时调用\n- **cgroup 子系统集成**：\n  - 通过 `rdma_cgrp_id` 获取当前任务的 cgroup 上下文\n  - 依赖 cgroup 的层级遍历机制（`css.parent`）\n\n## 使用场景\n\n1. **RDMA 驱动资源分配**  \n   当用户空间应用通过 verbs API 创建 QP、CQ、MR 等对象时，底层驱动调用 `rdmacg_try_charge()` 检查是否允许分配。若成功，则在对象销毁时调用 `rdmacg_uncharge()` 释放配额。\n\n2. **多租户 RDMA 资源隔离**  \n   在容器化或虚拟化环境中，管理员可通过 cgroup v1/v2 接口为不同租户设置 RDMA 资源上限（如最大 HCA 对象数），防止资源耗尽攻击。\n\n3. **动态资源回收**  \n   当 cgroup 中所有任务退出且无 RDMA 资源占用时，自动清理对应的资源池，避免内存泄漏。\n\n4. **层级配额继承**  \n   子 cgroup 的资源使用量计入所有祖先 cgroup，确保父级配额对整个子树生效，实现严格的资源隔离。",
      "similarity": 0.541973352432251,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/cgroup/rdma.c",
          "start_line": 82,
          "end_line": 214,
          "content": [
            "static void set_resource_limit(struct rdmacg_resource_pool *rpool,",
            "\t\t\t       int index, int new_max)",
            "{",
            "\tif (new_max == S32_MAX) {",
            "\t\tif (rpool->resources[index].max != S32_MAX)",
            "\t\t\trpool->num_max_cnt++;",
            "\t} else {",
            "\t\tif (rpool->resources[index].max == S32_MAX)",
            "\t\t\trpool->num_max_cnt--;",
            "\t}",
            "\trpool->resources[index].max = new_max;",
            "}",
            "static void set_all_resource_max_limit(struct rdmacg_resource_pool *rpool)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < RDMACG_RESOURCE_MAX; i++)",
            "\t\tset_resource_limit(rpool, i, S32_MAX);",
            "}",
            "static void free_cg_rpool_locked(struct rdmacg_resource_pool *rpool)",
            "{",
            "\tlockdep_assert_held(&rdmacg_mutex);",
            "",
            "\tlist_del(&rpool->cg_node);",
            "\tlist_del(&rpool->dev_node);",
            "\tkfree(rpool);",
            "}",
            "static void",
            "uncharge_cg_locked(struct rdma_cgroup *cg,",
            "\t\t   struct rdmacg_device *device,",
            "\t\t   enum rdmacg_resource_type index)",
            "{",
            "\tstruct rdmacg_resource_pool *rpool;",
            "",
            "\trpool = find_cg_rpool_locked(cg, device);",
            "",
            "\t/*",
            "\t * rpool cannot be null at this stage. Let kernel operate in case",
            "\t * if there a bug in IB stack or rdma controller, instead of crashing",
            "\t * the system.",
            "\t */",
            "\tif (unlikely(!rpool)) {",
            "\t\tpr_warn(\"Invalid device %p or rdma cgroup %p\\n\", cg, device);",
            "\t\treturn;",
            "\t}",
            "",
            "\trpool->resources[index].usage--;",
            "",
            "\t/*",
            "\t * A negative count (or overflow) is invalid,",
            "\t * it indicates a bug in the rdma controller.",
            "\t */",
            "\tWARN_ON_ONCE(rpool->resources[index].usage < 0);",
            "\trpool->usage_sum--;",
            "\tif (rpool->usage_sum == 0 &&",
            "\t    rpool->num_max_cnt == RDMACG_RESOURCE_MAX) {",
            "\t\t/*",
            "\t\t * No user of the rpool and all entries are set to max, so",
            "\t\t * safe to delete this rpool.",
            "\t\t */",
            "\t\tfree_cg_rpool_locked(rpool);",
            "\t}",
            "}",
            "static void rdmacg_uncharge_hierarchy(struct rdma_cgroup *cg,",
            "\t\t\t\t     struct rdmacg_device *device,",
            "\t\t\t\t     struct rdma_cgroup *stop_cg,",
            "\t\t\t\t     enum rdmacg_resource_type index)",
            "{",
            "\tstruct rdma_cgroup *p;",
            "",
            "\tmutex_lock(&rdmacg_mutex);",
            "",
            "\tfor (p = cg; p != stop_cg; p = parent_rdmacg(p))",
            "\t\tuncharge_cg_locked(p, device, index);",
            "",
            "\tmutex_unlock(&rdmacg_mutex);",
            "",
            "\tcss_put(&cg->css);",
            "}",
            "void rdmacg_uncharge(struct rdma_cgroup *cg,",
            "\t\t     struct rdmacg_device *device,",
            "\t\t     enum rdmacg_resource_type index)",
            "{",
            "\tif (index >= RDMACG_RESOURCE_MAX)",
            "\t\treturn;",
            "",
            "\trdmacg_uncharge_hierarchy(cg, device, NULL, index);",
            "}",
            "int rdmacg_try_charge(struct rdma_cgroup **rdmacg,",
            "\t\t      struct rdmacg_device *device,",
            "\t\t      enum rdmacg_resource_type index)",
            "{",
            "\tstruct rdma_cgroup *cg, *p;",
            "\tstruct rdmacg_resource_pool *rpool;",
            "\ts64 new;",
            "\tint ret = 0;",
            "",
            "\tif (index >= RDMACG_RESOURCE_MAX)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * hold on to css, as cgroup can be removed but resource",
            "\t * accounting happens on css.",
            "\t */",
            "\tcg = get_current_rdmacg();",
            "",
            "\tmutex_lock(&rdmacg_mutex);",
            "\tfor (p = cg; p; p = parent_rdmacg(p)) {",
            "\t\trpool = get_cg_rpool_locked(p, device);",
            "\t\tif (IS_ERR(rpool)) {",
            "\t\t\tret = PTR_ERR(rpool);",
            "\t\t\tgoto err;",
            "\t\t} else {",
            "\t\t\tnew = rpool->resources[index].usage + 1;",
            "\t\t\tif (new > rpool->resources[index].max) {",
            "\t\t\t\tret = -EAGAIN;",
            "\t\t\t\tgoto err;",
            "\t\t\t} else {",
            "\t\t\t\trpool->resources[index].usage = new;",
            "\t\t\t\trpool->usage_sum++;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&rdmacg_mutex);",
            "",
            "\t*rdmacg = cg;",
            "\treturn 0;",
            "",
            "err:",
            "\tmutex_unlock(&rdmacg_mutex);",
            "\trdmacg_uncharge_hierarchy(cg, device, p, index);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "set_resource_limit, set_all_resource_max_limit, free_cg_rpool_locked, uncharge_cg_locked, rdmacg_uncharge_hierarchy, rdmacg_uncharge, rdmacg_try_charge",
          "description": "实现资源限制调整、资源释放及充电逻辑，包含设置资源上限、释放资源池、递归层级资源释放、尝试充电检查及错误恢复机制，维护资源使用计数和最大值统计。",
          "similarity": 0.5116479396820068
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/cgroup/rdma.c",
          "start_line": 316,
          "end_line": 464,
          "content": [
            "void rdmacg_register_device(struct rdmacg_device *device)",
            "{",
            "\tINIT_LIST_HEAD(&device->dev_node);",
            "\tINIT_LIST_HEAD(&device->rpools);",
            "",
            "\tmutex_lock(&rdmacg_mutex);",
            "\tlist_add_tail(&device->dev_node, &rdmacg_devices);",
            "\tmutex_unlock(&rdmacg_mutex);",
            "}",
            "void rdmacg_unregister_device(struct rdmacg_device *device)",
            "{",
            "\tstruct rdmacg_resource_pool *rpool, *tmp;",
            "",
            "\t/*",
            "\t * Synchronize with any active resource settings,",
            "\t * usage query happening via configfs.",
            "\t */",
            "\tmutex_lock(&rdmacg_mutex);",
            "\tlist_del_init(&device->dev_node);",
            "",
            "\t/*",
            "\t * Now that this device is off the cgroup list, its safe to free",
            "\t * all the rpool resources.",
            "\t */",
            "\tlist_for_each_entry_safe(rpool, tmp, &device->rpools, dev_node)",
            "\t\tfree_cg_rpool_locked(rpool);",
            "",
            "\tmutex_unlock(&rdmacg_mutex);",
            "}",
            "static int parse_resource(char *c, int *intval)",
            "{",
            "\tsubstring_t argstr;",
            "\tchar *name, *value = c;",
            "\tsize_t len;",
            "\tint ret, i;",
            "",
            "\tname = strsep(&value, \"=\");",
            "\tif (!name || !value)",
            "\t\treturn -EINVAL;",
            "",
            "\ti = match_string(rdmacg_resource_names, RDMACG_RESOURCE_MAX, name);",
            "\tif (i < 0)",
            "\t\treturn i;",
            "",
            "\tlen = strlen(value);",
            "",
            "\targstr.from = value;",
            "\targstr.to = value + len;",
            "",
            "\tret = match_int(&argstr, intval);",
            "\tif (ret >= 0) {",
            "\t\tif (*intval < 0)",
            "\t\t\treturn -EINVAL;",
            "\t\treturn i;",
            "\t}",
            "\tif (strncmp(value, RDMACG_MAX_STR, len) == 0) {",
            "\t\t*intval = S32_MAX;",
            "\t\treturn i;",
            "\t}",
            "\treturn -EINVAL;",
            "}",
            "static int rdmacg_parse_limits(char *options,",
            "\t\t\t       int *new_limits, unsigned long *enables)",
            "{",
            "\tchar *c;",
            "\tint err = -EINVAL;",
            "",
            "\t/* parse resource options */",
            "\twhile ((c = strsep(&options, \" \")) != NULL) {",
            "\t\tint index, intval;",
            "",
            "\t\tindex = parse_resource(c, &intval);",
            "\t\tif (index < 0)",
            "\t\t\tgoto err;",
            "",
            "\t\tnew_limits[index] = intval;",
            "\t\t*enables |= BIT(index);",
            "\t}",
            "\treturn 0;",
            "",
            "err:",
            "\treturn err;",
            "}",
            "static ssize_t rdmacg_resource_set_max(struct kernfs_open_file *of,",
            "\t\t\t\t       char *buf, size_t nbytes, loff_t off)",
            "{",
            "\tstruct rdma_cgroup *cg = css_rdmacg(of_css(of));",
            "\tconst char *dev_name;",
            "\tstruct rdmacg_resource_pool *rpool;",
            "\tstruct rdmacg_device *device;",
            "\tchar *options = strstrip(buf);",
            "\tint *new_limits;",
            "\tunsigned long enables = 0;",
            "\tint i = 0, ret = 0;",
            "",
            "\t/* extract the device name first */",
            "\tdev_name = strsep(&options, \" \");",
            "\tif (!dev_name) {",
            "\t\tret = -EINVAL;",
            "\t\tgoto err;",
            "\t}",
            "",
            "\tnew_limits = kcalloc(RDMACG_RESOURCE_MAX, sizeof(int), GFP_KERNEL);",
            "\tif (!new_limits) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto err;",
            "\t}",
            "",
            "\tret = rdmacg_parse_limits(options, new_limits, &enables);",
            "\tif (ret)",
            "\t\tgoto parse_err;",
            "",
            "\t/* acquire lock to synchronize with hot plug devices */",
            "\tmutex_lock(&rdmacg_mutex);",
            "",
            "\tdevice = rdmacg_get_device_locked(dev_name);",
            "\tif (!device) {",
            "\t\tret = -ENODEV;",
            "\t\tgoto dev_err;",
            "\t}",
            "",
            "\trpool = get_cg_rpool_locked(cg, device);",
            "\tif (IS_ERR(rpool)) {",
            "\t\tret = PTR_ERR(rpool);",
            "\t\tgoto dev_err;",
            "\t}",
            "",
            "\t/* now set the new limits of the rpool */",
            "\tfor_each_set_bit(i, &enables, RDMACG_RESOURCE_MAX)",
            "\t\tset_resource_limit(rpool, i, new_limits[i]);",
            "",
            "\tif (rpool->usage_sum == 0 &&",
            "\t    rpool->num_max_cnt == RDMACG_RESOURCE_MAX) {",
            "\t\t/*",
            "\t\t * No user of the rpool and all entries are set to max, so",
            "\t\t * safe to delete this rpool.",
            "\t\t */",
            "\t\tfree_cg_rpool_locked(rpool);",
            "\t}",
            "",
            "dev_err:",
            "\tmutex_unlock(&rdmacg_mutex);",
            "",
            "parse_err:",
            "\tkfree(new_limits);",
            "",
            "err:",
            "\treturn ret ?: nbytes;",
            "}"
          ],
          "function_name": "rdmacg_register_device, rdmacg_unregister_device, parse_resource, rdmacg_parse_limits, rdmacg_resource_set_max",
          "description": "实现设备注册注销流程、资源参数解析及限制设置功能，通过parse_resource解析资源名与数值，rdmacg_parse_limits组织限制定义，rdmacg_resource_set_max应用新限制并触发资源池清理。",
          "similarity": 0.5086183547973633
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/cgroup/rdma.c",
          "start_line": 1,
          "end_line": 81,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * RDMA resource limiting controller for cgroups.",
            " *",
            " * Used to allow a cgroup hierarchy to stop processes from consuming",
            " * additional RDMA resources after a certain limit is reached.",
            " *",
            " * Copyright (C) 2016 Parav Pandit <pandit.parav@gmail.com>",
            " */",
            "",
            "#include <linux/bitops.h>",
            "#include <linux/slab.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/parser.h>",
            "#include <linux/cgroup_rdma.h>",
            "",
            "#define RDMACG_MAX_STR \"max\"",
            "",
            "/*",
            " * Protects list of resource pools maintained on per cgroup basis",
            " * and rdma device list.",
            " */",
            "static DEFINE_MUTEX(rdmacg_mutex);",
            "static LIST_HEAD(rdmacg_devices);",
            "",
            "enum rdmacg_file_type {",
            "\tRDMACG_RESOURCE_TYPE_MAX,",
            "\tRDMACG_RESOURCE_TYPE_STAT,",
            "};",
            "",
            "/*",
            " * resource table definition as to be seen by the user.",
            " * Need to add entries to it when more resources are",
            " * added/defined at IB verb/core layer.",
            " */",
            "static char const *rdmacg_resource_names[] = {",
            "\t[RDMACG_RESOURCE_HCA_HANDLE]\t= \"hca_handle\",",
            "\t[RDMACG_RESOURCE_HCA_OBJECT]\t= \"hca_object\",",
            "};",
            "",
            "/* resource tracker for each resource of rdma cgroup */",
            "struct rdmacg_resource {",
            "\tint max;",
            "\tint usage;",
            "};",
            "",
            "/*",
            " * resource pool object which represents per cgroup, per device",
            " * resources. There are multiple instances of this object per cgroup,",
            " * therefore it cannot be embedded within rdma_cgroup structure. It",
            " * is maintained as list.",
            " */",
            "struct rdmacg_resource_pool {",
            "\tstruct rdmacg_device\t*device;",
            "\tstruct rdmacg_resource\tresources[RDMACG_RESOURCE_MAX];",
            "",
            "\tstruct list_head\tcg_node;",
            "\tstruct list_head\tdev_node;",
            "",
            "\t/* count active user tasks of this pool */",
            "\tu64\t\t\tusage_sum;",
            "\t/* total number counts which are set to max */",
            "\tint\t\t\tnum_max_cnt;",
            "};",
            "",
            "static struct rdma_cgroup *css_rdmacg(struct cgroup_subsys_state *css)",
            "{",
            "\treturn container_of(css, struct rdma_cgroup, css);",
            "}",
            "",
            "static struct rdma_cgroup *parent_rdmacg(struct rdma_cgroup *cg)",
            "{",
            "\treturn css_rdmacg(cg->css.parent);",
            "}",
            "",
            "static inline struct rdma_cgroup *get_current_rdmacg(void)",
            "{",
            "\treturn css_rdmacg(task_get_css(current, rdma_cgrp_id));",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义RDMA cgroup资源管理的基础结构，包括互斥锁保护的设备列表、资源类型枚举、资源名称映射及rdmacg_resource_pool结构体，提供从css获取rdma_cgroup的辅助函数。",
          "similarity": 0.497907429933548
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/cgroup/rdma.c",
          "start_line": 494,
          "end_line": 562,
          "content": [
            "static void print_rpool_values(struct seq_file *sf,",
            "\t\t\t       struct rdmacg_resource_pool *rpool)",
            "{",
            "\tenum rdmacg_file_type sf_type;",
            "\tint i;",
            "\tu32 value;",
            "",
            "\tsf_type = seq_cft(sf)->private;",
            "",
            "\tfor (i = 0; i < RDMACG_RESOURCE_MAX; i++) {",
            "\t\tseq_puts(sf, rdmacg_resource_names[i]);",
            "\t\tseq_putc(sf, '=');",
            "\t\tif (sf_type == RDMACG_RESOURCE_TYPE_MAX) {",
            "\t\t\tif (rpool)",
            "\t\t\t\tvalue = rpool->resources[i].max;",
            "\t\t\telse",
            "\t\t\t\tvalue = S32_MAX;",
            "\t\t} else {",
            "\t\t\tif (rpool)",
            "\t\t\t\tvalue = rpool->resources[i].usage;",
            "\t\t\telse",
            "\t\t\t\tvalue = 0;",
            "\t\t}",
            "",
            "\t\tif (value == S32_MAX)",
            "\t\t\tseq_puts(sf, RDMACG_MAX_STR);",
            "\t\telse",
            "\t\t\tseq_printf(sf, \"%d\", value);",
            "\t\tseq_putc(sf, ' ');",
            "\t}",
            "}",
            "static int rdmacg_resource_read(struct seq_file *sf, void *v)",
            "{",
            "\tstruct rdmacg_device *device;",
            "\tstruct rdmacg_resource_pool *rpool;",
            "\tstruct rdma_cgroup *cg = css_rdmacg(seq_css(sf));",
            "",
            "\tmutex_lock(&rdmacg_mutex);",
            "",
            "\tlist_for_each_entry(device, &rdmacg_devices, dev_node) {",
            "\t\tseq_printf(sf, \"%s \", device->name);",
            "",
            "\t\trpool = find_cg_rpool_locked(cg, device);",
            "\t\tprint_rpool_values(sf, rpool);",
            "",
            "\t\tseq_putc(sf, '\\n');",
            "\t}",
            "",
            "\tmutex_unlock(&rdmacg_mutex);",
            "\treturn 0;",
            "}",
            "static void rdmacg_css_free(struct cgroup_subsys_state *css)",
            "{",
            "\tstruct rdma_cgroup *cg = css_rdmacg(css);",
            "",
            "\tkfree(cg);",
            "}",
            "static void rdmacg_css_offline(struct cgroup_subsys_state *css)",
            "{",
            "\tstruct rdma_cgroup *cg = css_rdmacg(css);",
            "\tstruct rdmacg_resource_pool *rpool;",
            "",
            "\tmutex_lock(&rdmacg_mutex);",
            "",
            "\tlist_for_each_entry(rpool, &cg->rpools, cg_node)",
            "\t\tset_all_resource_max_limit(rpool);",
            "",
            "\tmutex_unlock(&rdmacg_mutex);",
            "}"
          ],
          "function_name": "print_rpool_values, rdmacg_resource_read, rdmacg_css_free, rdmacg_css_offline",
          "description": "实现资源状态序列化输出、CSS销毁时资源强制设为最大及子系统离线处理，包含资源使用量打印函数和cgroup子系统状态清理逻辑。",
          "similarity": 0.4974130392074585
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tiny.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:45:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tiny.c`\n\n---\n\n# `rcu/tiny.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tiny.c` 是 Linux 内核中 **RCU（Read-Copy Update）机制的“精简版”（Tiny RCU）实现**，专为单处理器（UP）或资源受限系统（如嵌入式设备）设计。该实现去除了复杂的状态机、CPU 间通信和动态负载均衡等开销，仅保留 RCU 的核心语义：**在读端无锁、写端延迟回收**。由于系统只有一个 CPU，任何上下文切换或中断返回用户态都天然构成“宽限期”（Grace Period），因此无需复杂的跨 CPU 同步逻辑。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_ctrlblk`**：RCU 控制块，全局唯一，用于管理回调链表和宽限期状态。\n  - `rcucblist`：待处理的 RCU 回调链表头。\n  - `donetail`：指向最后一个已完成宽限期的回调的 `next` 指针。\n  - `curtail`：指向链表最后一个回调的 `next` 指针。\n  - `gp_seq`：宽限期序列号，每次宽限期结束递增 2（偶数表示完成状态）。\n\n### 主要函数\n\n| 函数 | 功能说明 |\n|------|--------|\n| `rcu_qs(void)` | 记录当前 CPU 的静默状态（Quiescent State），推进已完成回调指针并触发软中断处理回调。 |\n| `rcu_sched_clock_irq(int user)` | 调度时钟中断处理函数，若处于用户态则调用 `rcu_qs()`；否则标记当前任务需重新调度以尽快进入静默状态。 |\n| `call_rcu(struct rcu_head *head, rcu_callback_t func)` | 注册一个 RCU 回调，在下一个宽限期结束后执行。 |\n| `synchronize_rcu(void)` | 等待当前宽限期结束（在 UP 系统中立即完成，仅更新 `gp_seq`）。 |\n| `rcu_process_callbacks(struct softirq_action *unused)` | RCU 软中断处理函数，批量执行已完成宽限期的回调。 |\n| `rcu_barrier(void)` | 等待所有已注册的 RCU 回调执行完毕。 |\n| `get_state_synchronize_rcu()` / `start_poll_synchronize_rcu()` / `poll_state_synchronize_rcu()` | 支持轮询式宽限期检测的 API。 |\n| `rcu_init(void)` | RCU 子系统初始化，注册 RCU 软中断处理函数。 |\n\n## 3. 关键实现\n\n### 宽限期管理\n- **单 CPU 假设**：由于系统只有一个 CPU，任何从内核态返回用户态、发生上下文切换或空闲任务运行，都视为一个完整的宽限期。\n- **`gp_seq` 计数器**：初始值为 `0 - 300UL`（负数，确保早期调用 `get_state_synchronize_rcu()` 返回有效值）。每次调用 `rcu_qs()` 或 `synchronize_rcu()` 时递增 2，偶数值表示宽限期已完成。\n- **无实际等待**：`synchronize_rcu()` 不阻塞，仅更新 `gp_seq`，因为调用者本身已处于静默状态。\n\n### 回调队列管理\n- **双指针链表**：使用 `donetail` 和 `curtail` 实现无锁（在中断禁用下）的回调入队和出队。\n  - 新回调通过 `curtail` 追加到链表尾部。\n  - `rcu_qs()` 将 `donetail` 移至 `curtail`，表示此前所有回调已完成宽限期。\n  - 软中断 `rcu_process_callbacks()` 将 `donetail` 之前的所有回调移出并执行。\n\n### 回调执行\n- **软中断上下文**：回调在 `RCU_SOFTIRQ` 中执行，确保不在原子上下文。\n- **支持 `kvfree`**：通过 `__is_kvfree_rcu_offset` 判断是否为 `kvfree_rcu` 回调，若是则直接释放内存而非调用函数指针。\n- **调试支持**：包含双释放检测（`debug_rcu_head_queue`）和内存泄漏防护（`tiny_rcu_leak_callback`）。\n\n### 轮询 API 实现\n- `get_state_synchronize_rcu()` 返回当前 `gp_seq`。\n- `poll_state_synchronize_rcu(oldstate)` 判断 `oldstate` 是否已完成：若 `oldstate == RCU_GET_STATE_COMPLETED`（特殊值）或当前 `gp_seq != oldstate`，则返回 `true`。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/rcupdate_wait.h>`：提供 `wait_rcu_gp()` 实现 `rcu_barrier()`。\n  - `\"rcu.h\"`：RCU 内部头文件，定义调试宏、trace 点等。\n  - 其他通用内核头文件（如 `sched.h`, `softirq.h`, `slab.h` 等）。\n- **内核配置**：\n  - 仅在 `CONFIG_TINY_RCU` 或 `CONFIG_TINY_SRCU` 启用时编译。\n  - 与 `CONFIG_PREEMPT`、`CONFIG_SMP` 互斥（Tiny RCU 用于非抢占式 UP 系统）。\n- **软中断子系统**：依赖 `open_softirq()` 注册 `RCU_SOFTIRQ`。\n- **内存管理**：`kvfree_call_rcu()` 依赖 KASAN 的辅助栈记录（`CONFIG_KASAN_GENERIC`）。\n\n## 5. 使用场景\n\n- **单处理器嵌入式系统**：资源受限设备（如 IoT 设备、微控制器）中替代 Tree RCU，显著减少代码体积和运行时开销。\n- **内核测试与调试**：作为 RCU 行为的简化模型，用于验证 RCU 语义正确性。\n- **RCU 基础功能提供**：\n  - 为内核其他子系统（如网络、文件系统、设备驱动）提供 `call_rcu()` 和 `synchronize_rcu()` 接口。\n  - 支持延迟内存回收（如 `kfree_rcu()`）。\n  - 通过 `rcu_barrier()` 确保模块卸载前所有回调完成。\n- **轮询式同步**：适用于不能阻塞的上下文（如中断处理程序），通过 `poll_state_synchronize_rcu()` 轮询宽限期状态。",
      "similarity": 0.539230465888977,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/rcu/tiny.c",
          "start_line": 45,
          "end_line": 161,
          "content": [
            "void rcu_barrier(void)",
            "{",
            "\twait_rcu_gp(call_rcu_hurry);",
            "}",
            "void rcu_qs(void)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (rcu_ctrlblk.donetail != rcu_ctrlblk.curtail) {",
            "\t\trcu_ctrlblk.donetail = rcu_ctrlblk.curtail;",
            "\t\traise_softirq_irqoff(RCU_SOFTIRQ);",
            "\t}",
            "\tWRITE_ONCE(rcu_ctrlblk.gp_seq, rcu_ctrlblk.gp_seq + 2);",
            "\tlocal_irq_restore(flags);",
            "}",
            "void rcu_sched_clock_irq(int user)",
            "{",
            "\tif (user) {",
            "\t\trcu_qs();",
            "\t} else if (rcu_ctrlblk.donetail != rcu_ctrlblk.curtail) {",
            "\t\tset_tsk_need_resched(current);",
            "\t\tset_preempt_need_resched();",
            "\t}",
            "}",
            "static inline bool rcu_reclaim_tiny(struct rcu_head *head)",
            "{",
            "\trcu_callback_t f;",
            "\tunsigned long offset = (unsigned long)head->func;",
            "",
            "\trcu_lock_acquire(&rcu_callback_map);",
            "\tif (__is_kvfree_rcu_offset(offset)) {",
            "\t\ttrace_rcu_invoke_kvfree_callback(\"\", head, offset);",
            "\t\tkvfree((void *)head - offset);",
            "\t\trcu_lock_release(&rcu_callback_map);",
            "\t\treturn true;",
            "\t}",
            "",
            "\ttrace_rcu_invoke_callback(\"\", head);",
            "\tf = head->func;",
            "\tdebug_rcu_head_callback(head);",
            "\tWRITE_ONCE(head->func, (rcu_callback_t)0L);",
            "\tf(head);",
            "\trcu_lock_release(&rcu_callback_map);",
            "\treturn false;",
            "}",
            "static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused)",
            "{",
            "\tstruct rcu_head *next, *list;",
            "\tunsigned long flags;",
            "",
            "\t/* Move the ready-to-invoke callbacks to a local list. */",
            "\tlocal_irq_save(flags);",
            "\tif (rcu_ctrlblk.donetail == &rcu_ctrlblk.rcucblist) {",
            "\t\t/* No callbacks ready, so just leave. */",
            "\t\tlocal_irq_restore(flags);",
            "\t\treturn;",
            "\t}",
            "\tlist = rcu_ctrlblk.rcucblist;",
            "\trcu_ctrlblk.rcucblist = *rcu_ctrlblk.donetail;",
            "\t*rcu_ctrlblk.donetail = NULL;",
            "\tif (rcu_ctrlblk.curtail == rcu_ctrlblk.donetail)",
            "\t\trcu_ctrlblk.curtail = &rcu_ctrlblk.rcucblist;",
            "\trcu_ctrlblk.donetail = &rcu_ctrlblk.rcucblist;",
            "\tlocal_irq_restore(flags);",
            "",
            "\t/* Invoke the callbacks on the local list. */",
            "\twhile (list) {",
            "\t\tnext = list->next;",
            "\t\tprefetch(next);",
            "\t\tdebug_rcu_head_unqueue(list);",
            "\t\tlocal_bh_disable();",
            "\t\trcu_reclaim_tiny(list);",
            "\t\tlocal_bh_enable();",
            "\t\tlist = next;",
            "\t}",
            "}",
            "void synchronize_rcu(void)",
            "{",
            "\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_sched_lock_map),",
            "\t\t\t \"Illegal synchronize_rcu() in RCU read-side critical section\");",
            "\tWRITE_ONCE(rcu_ctrlblk.gp_seq, rcu_ctrlblk.gp_seq + 2);",
            "}",
            "static void tiny_rcu_leak_callback(struct rcu_head *rhp)",
            "{",
            "}",
            "void call_rcu(struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\tstatic atomic_t doublefrees;",
            "\tunsigned long flags;",
            "",
            "\tif (debug_rcu_head_queue(head)) {",
            "\t\tif (atomic_inc_return(&doublefrees) < 4) {",
            "\t\t\tpr_err(\"%s(): Double-freed CB %p->%pS()!!!  \", __func__, head, head->func);",
            "\t\t\tmem_dump_obj(head);",
            "\t\t}",
            "",
            "\t\tif (!__is_kvfree_rcu_offset((unsigned long)head->func))",
            "\t\t\tWRITE_ONCE(head->func, tiny_rcu_leak_callback);",
            "\t\treturn;",
            "\t}",
            "",
            "\thead->func = func;",
            "\thead->next = NULL;",
            "",
            "\tlocal_irq_save(flags);",
            "\t*rcu_ctrlblk.curtail = head;",
            "\trcu_ctrlblk.curtail = &head->next;",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (unlikely(is_idle_task(current))) {",
            "\t\t/* force scheduling for rcu_qs() */",
            "\t\tresched_cpu(0);",
            "\t}",
            "}"
          ],
          "function_name": "rcu_barrier, rcu_qs, rcu_sched_clock_irq, rcu_reclaim_tiny, rcu_process_callbacks, synchronize_rcu, tiny_rcu_leak_callback, call_rcu",
          "description": "实现RCU核心函数，包括rcu_barrier触发同步屏障、rcu_process_callbacks处理回调队列、synchronize_rcu更新grace period序列号、call_rcu添加回调到链表并处理空闲任务调度。",
          "similarity": 0.5445876121520996
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/rcu/tiny.c",
          "start_line": 206,
          "end_line": 239,
          "content": [
            "void get_completed_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\trgosp->rgos_norm = RCU_GET_STATE_COMPLETED;",
            "}",
            "unsigned long get_state_synchronize_rcu(void)",
            "{",
            "\treturn READ_ONCE(rcu_ctrlblk.gp_seq);",
            "}",
            "unsigned long start_poll_synchronize_rcu(void)",
            "{",
            "\tunsigned long gp_seq = get_state_synchronize_rcu();",
            "",
            "\tif (unlikely(is_idle_task(current))) {",
            "\t\t/* force scheduling for rcu_qs() */",
            "\t\tresched_cpu(0);",
            "\t}",
            "\treturn gp_seq;",
            "}",
            "bool poll_state_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\treturn oldstate == RCU_GET_STATE_COMPLETED || READ_ONCE(rcu_ctrlblk.gp_seq) != oldstate;",
            "}",
            "void kvfree_call_rcu(struct rcu_head *head, void *ptr)",
            "{",
            "\tif (head)",
            "\t\tkasan_record_aux_stack_noalloc(ptr);",
            "",
            "\t__kvfree_call_rcu(head, ptr);",
            "}",
            "void __init rcu_init(void)",
            "{",
            "\topen_softirq(RCU_SOFTIRQ, rcu_process_callbacks);",
            "\trcu_early_boot_tests();",
            "}"
          ],
          "function_name": "get_completed_synchronize_rcu_full, get_state_synchronize_rcu, start_poll_synchronize_rcu, poll_state_synchronize_rcu, kvfree_call_rcu, rcu_init",
          "description": "提供RCU状态查询接口，get_state_synchronize_rcu读取当前grace period序号，start_poll_synchronize_rcu用于轮询同步状态，rcu_init初始化RCU软中断处理函数注册。",
          "similarity": 0.4829230010509491
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/rcu/tiny.c",
          "start_line": 1,
          "end_line": 44,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0+",
            "/*",
            " * Read-Copy Update mechanism for mutual exclusion, the Bloatwatch edition.",
            " *",
            " * Copyright IBM Corporation, 2008",
            " *",
            " * Author: Paul E. McKenney <paulmck@linux.ibm.com>",
            " *",
            " * For detailed explanation of Read-Copy Update mechanism see -",
            " *\t\tDocumentation/RCU",
            " */",
            "#include <linux/completion.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/notifier.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/sched.h>",
            "#include <linux/types.h>",
            "#include <linux/init.h>",
            "#include <linux/time.h>",
            "#include <linux/cpu.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>",
            "",
            "#include \"rcu.h\"",
            "",
            "/* Global control variables for rcupdate callback mechanism. */",
            "struct rcu_ctrlblk {",
            "\tstruct rcu_head *rcucblist;\t/* List of pending callbacks (CBs). */",
            "\tstruct rcu_head **donetail;\t/* ->next pointer of last \"done\" CB. */",
            "\tstruct rcu_head **curtail;\t/* ->next pointer of last CB. */",
            "\tunsigned long gp_seq;\t\t/* Grace-period counter. */",
            "};",
            "",
            "/* Definition for rcupdate control block. */",
            "static struct rcu_ctrlblk rcu_ctrlblk = {",
            "\t.donetail\t= &rcu_ctrlblk.rcucblist,",
            "\t.curtail\t= &rcu_ctrlblk.rcucblist,",
            "\t.gp_seq\t\t= 0 - 300UL,",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义RCU控制块结构体rcu_ctrlblk，用于管理回调链表、尾指针及 grace period 序列号，初始化时设置 donetail 和 curtail 指向同一位置，gp_seq 初始化为负值以标记初始状态。",
          "similarity": 0.40467023849487305
        }
      ]
    }
  ]
}