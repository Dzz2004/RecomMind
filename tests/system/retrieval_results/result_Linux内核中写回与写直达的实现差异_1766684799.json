{
  "query": "Linux内核中写回与写直达的实现差异",
  "timestamp": "2025-12-26 01:46:39",
  "retrieved_files": [
    {
      "source_file": "kernel/dma/direct.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:12:42\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\direct.c`\n\n---\n\n# `dma/direct.c` 技术文档\n\n## 1. 文件概述\n\n`dma/direct.c` 实现了 Linux 内核中 **DMA 直接映射操作（DMA direct mapping）** 的核心逻辑。该文件提供了一套不依赖 IOMMU 的 DMA 内存分配与映射机制，适用于物理地址可直接被设备访问的平台（如 x86、ARM64 等无 IOMMU 或 IOMMU 被禁用的场景）。其核心思想是将物理内存地址直接转换为设备可见的 DMA 地址，避免复杂的地址转换开销。\n\n该实现支持多种内存分配策略，包括：\n- 连续物理内存分配（CMA 或 buddy allocator）\n- SWIOTLB 回退机制（用于处理高地址设备无法访问的情况）\n- 原子池分配（用于不可阻塞上下文）\n- 高端内存重映射\n- 内存加密/解密（如 AMD SEV、Intel TDX 等安全特性）\n\n## 2. 核心功能\n\n### 全局变量\n- `zone_dma_bits`：定义 ZONE_DMA 的地址位宽（默认 24 位，即 16MB），可由架构代码覆盖。\n\n### 主要函数\n| 函数名 | 功能描述 |\n|--------|--------|\n| `phys_to_dma_direct()` | 将物理地址转换为 DMA 地址，支持强制解密场景 |\n| `dma_direct_to_page()` | 通过 DMA 地址反查对应的 `struct page` |\n| `dma_direct_get_required_mask()` | 计算设备所需的 DMA 地址掩码（基于系统最大物理地址）|\n| `dma_coherent_ok()` | 检查给定物理地址范围是否在设备的 DMA 地址能力范围内 |\n| `dma_direct_alloc()` | **主入口函数**：为设备分配 DMA 内存，支持多种属性和回退策略 |\n| `__dma_direct_alloc_pages()` | 底层页面分配函数，尝试最优内存区域并回退到低地址区域 |\n| `dma_direct_alloc_from_pool()` | 从原子池分配不可阻塞的 DMA 内存 |\n| `dma_direct_alloc_no_mapping()` | 分配无内核虚拟映射的 DMA 内存（返回 `struct page*`）|\n| `dma_set_decrypted()` / `dma_set_encrypted()` | 设置内存页为解密/加密状态（用于安全虚拟化）|\n\n### 辅助函数\n- `dma_direct_optimal_gfp_mask()`：根据设备 DMA 限制选择最优的 GFP 标志（GFP_DMA / GFP_DMA32）\n- `__dma_direct_free_pages()`：释放通过直接分配获得的页面（优先尝试 SWIOTLB 释放）\n\n## 3. 关键实现\n\n### 3.1 DMA 地址空间约束处理\n- 使用 `dev->coherent_dma_mask` 和 `dev->bus_dma_limit` 确定设备可寻址的物理地址上限。\n- 通过 `dma_coherent_ok()` 验证分配的物理内存是否在设备可访问范围内。\n- 若分配的内存超出范围，则回退到更低地址区域（先尝试 GFP_DMA32，再尝试 GFP_DMA）。\n\n### 3.2 多层次内存分配策略\n1. **首选 CMA 连续内存**：通过 `dma_alloc_contiguous()` 分配。\n2. **回退到 buddy allocator**：使用 `alloc_pages_node()`。\n3. **SWIOTLB 支持**：当设备无法访问高地址时，通过 `swiotlb_alloc()` 分配 bounce buffer。\n4. **原子上下文支持**：在不可阻塞场景下使用 `dma_direct_alloc_from_pool()` 从预分配池中获取内存。\n\n### 3.3 非一致性缓存与内存属性处理\n- 对于非一致性缓存架构（`!dev_is_dma_coherent()`）：\n  - 优先使用全局一致性内存池（`CONFIG_DMA_GLOBAL_POOL`）\n  - 或启用重映射（`CONFIG_DMA_DIRECT_REMAP`）创建 uncached 映射\n  - 或调用架构特定的 `arch_dma_alloc()`\n- 调用 `arch_dma_prep_coherent()` 清理内核别名的脏缓存行。\n\n### 3.4 安全内存处理（加密/解密）\n- 当 `force_dma_unencrypted(dev)` 为真（如 SEV 环境），分配的内存需标记为解密。\n- 使用 `set_memory_decrypted()` / `set_memory_encrypted()` 修改页表属性。\n- 解密操作可能阻塞，因此在原子上下文中需使用内存池。\n\n### 3.5 高端内存（HighMem）处理\n- 若分配的页面位于高端内存（`PageHighMem`），则必须通过 `dma_common_contiguous_remap()` 创建内核虚拟地址映射。\n- 重映射时应用设备特定的页保护属性（`dma_pgprot()`）并处理解密需求。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- `<linux/memblock.h>`：获取 `max_pfn` 系统最大页帧号\n- `<linux/dma-map-ops.h>`：DMA 映射操作抽象接口\n- `<linux/scatterlist.h>`：SG 表支持（间接依赖）\n- `<linux/set_memory.h>`：内存加密/解密操作（`set_memory_decrypted` 等）\n- `<linux/vmalloc.h>`：高端内存重映射支持\n- `\"direct.h\"`：本地 DMA direct 实现的私有头文件\n\n### 配置选项依赖\n- `CONFIG_SWIOTLB`：SWIOTLB bounce buffer 支持\n- `CONFIG_DMA_COHERENT_POOL`：原子上下文 DMA 内存池\n- `CONFIG_DMA_GLOBAL_POOL`：全局一致性 DMA 内存池\n- `CONFIG_DMA_DIRECT_REMAP`：非一致性设备的重映射支持\n- `CONFIG_ZONE_DMA` / `CONFIG_ZONE_DMA32`：低地址内存区域支持\n- `CONFIG_ARCH_HAS_DMA_SET_UNCACHED`：架构特定 uncached 映射支持\n\n### 架构相关依赖\n- `phys_to_dma()` / `dma_to_phys()`：架构提供的物理地址与 DMA 地址转换函数\n- `arch_dma_prep_coherent()`：架构特定的缓存一致性准备\n- `arch_dma_alloc()`：架构特定的 DMA 分配回退路径\n\n## 5. 使用场景\n\n### 5.1 设备驱动 DMA 分配\n- 驱动调用 `dma_alloc_coherent()` 或 `dma_alloc_attrs()` 时，若系统未启用 IOMMU，则最终由 `dma_direct_alloc()` 处理。\n- 适用于大多数无 IOMMU 的嵌入式系统、传统 PC 平台或 IOMMU 被显式禁用的场景。\n\n### 5.2 安全虚拟化环境\n- 在 AMD SEV 或 Intel TDX 等机密计算环境中，DMA 内存需标记为“解密”，该文件通过 `force_dma_unencrypted()` 机制实现。\n\n### 5.3 资源受限或实时系统\n- 通过 `DMA_ATTR_NO_KERNEL_MAPPING` 属性分配无内核映射的 DMA 内存，减少 TLB 压力。\n- 在中断上下文等不可阻塞场景中，自动使用原子内存池分配。\n\n### 5.4 老旧设备兼容\n- 对仅支持 32 位或 24 位 DMA 地址的设备，自动分配低地址内存（通过 GFP_DMA32/GFP_DMA）并验证地址范围。\n\n### 5.5 高端内存平台\n- 在 32 位系统或内存大于直接映射区域的平台上，自动处理高端内存的重映射，确保返回有效的内核虚拟地址。",
      "similarity": 0.5872690677642822,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/direct.c",
          "start_line": 25,
          "end_line": 140,
          "content": [
            "static inline dma_addr_t phys_to_dma_direct(struct device *dev,",
            "\t\tphys_addr_t phys)",
            "{",
            "\tif (force_dma_unencrypted(dev))",
            "\t\treturn phys_to_dma_unencrypted(dev, phys);",
            "\treturn phys_to_dma(dev, phys);",
            "}",
            "u64 dma_direct_get_required_mask(struct device *dev)",
            "{",
            "\tphys_addr_t phys = (phys_addr_t)(max_pfn - 1) << PAGE_SHIFT;",
            "\tu64 max_dma = phys_to_dma_direct(dev, phys);",
            "",
            "\treturn (1ULL << (fls64(max_dma) - 1)) * 2 - 1;",
            "}",
            "static gfp_t dma_direct_optimal_gfp_mask(struct device *dev, u64 *phys_limit)",
            "{",
            "\tu64 dma_limit = min_not_zero(",
            "\t\tdev->coherent_dma_mask,",
            "\t\tdev->bus_dma_limit);",
            "",
            "\t/*",
            "\t * Optimistically try the zone that the physical address mask falls",
            "\t * into first.  If that returns memory that isn't actually addressable",
            "\t * we will fallback to the next lower zone and try again.",
            "\t *",
            "\t * Note that GFP_DMA32 and GFP_DMA are no ops without the corresponding",
            "\t * zones.",
            "\t */",
            "\t*phys_limit = dma_to_phys(dev, dma_limit);",
            "\tif (*phys_limit <= DMA_BIT_MASK(zone_dma_bits))",
            "\t\treturn GFP_DMA;",
            "\tif (*phys_limit <= DMA_BIT_MASK(32))",
            "\t\treturn GFP_DMA32;",
            "\treturn 0;",
            "}",
            "bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)",
            "{",
            "\tdma_addr_t dma_addr = phys_to_dma_direct(dev, phys);",
            "",
            "\tif (dma_addr == DMA_MAPPING_ERROR)",
            "\t\treturn false;",
            "\treturn dma_addr + size - 1 <=",
            "\t\tmin_not_zero(dev->coherent_dma_mask, dev->bus_dma_limit);",
            "}",
            "static int dma_set_decrypted(struct device *dev, void *vaddr, size_t size)",
            "{",
            "\tif (!force_dma_unencrypted(dev))",
            "\t\treturn 0;",
            "\treturn set_memory_decrypted((unsigned long)vaddr, PFN_UP(size));",
            "}",
            "static int dma_set_encrypted(struct device *dev, void *vaddr, size_t size)",
            "{",
            "\tint ret;",
            "",
            "\tif (!force_dma_unencrypted(dev))",
            "\t\treturn 0;",
            "\tret = set_memory_encrypted((unsigned long)vaddr, PFN_UP(size));",
            "\tif (ret)",
            "\t\tpr_warn_ratelimited(\"leaking DMA memory that can't be re-encrypted\\n\");",
            "\treturn ret;",
            "}",
            "static void __dma_direct_free_pages(struct device *dev, struct page *page,",
            "\t\t\t\t    size_t size)",
            "{",
            "\tif (swiotlb_free(dev, page, size))",
            "\t\treturn;",
            "\tdma_free_contiguous(dev, page, size);",
            "}",
            "static bool dma_direct_use_pool(struct device *dev, gfp_t gfp)",
            "{",
            "\treturn !gfpflags_allow_blocking(gfp) && !is_swiotlb_for_alloc(dev);",
            "}",
            "void dma_direct_free(struct device *dev, size_t size,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, unsigned long attrs)",
            "{",
            "\tunsigned int page_order = get_order(size);",
            "",
            "\tif ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) &&",
            "\t    !force_dma_unencrypted(dev) && !is_swiotlb_for_alloc(dev)) {",
            "\t\t/* cpu_addr is a struct page cookie, not a kernel address */",
            "\t\tdma_free_contiguous(dev, cpu_addr, size);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_SET_UNCACHED) &&",
            "\t    !IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&",
            "\t    !IS_ENABLED(CONFIG_DMA_GLOBAL_POOL) &&",
            "\t    !dev_is_dma_coherent(dev) &&",
            "\t    !is_swiotlb_for_alloc(dev)) {",
            "\t\tarch_dma_free(dev, size, cpu_addr, dma_addr, attrs);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_DMA_GLOBAL_POOL) &&",
            "\t    !dev_is_dma_coherent(dev)) {",
            "\t\tif (!dma_release_from_global_coherent(page_order, cpu_addr))",
            "\t\t\tWARN_ON_ONCE(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */",
            "\tif (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&",
            "\t    dma_free_from_pool(dev, cpu_addr, PAGE_ALIGN(size)))",
            "\t\treturn;",
            "",
            "\tif (is_vmalloc_addr(cpu_addr)) {",
            "\t\tvunmap(cpu_addr);",
            "\t} else {",
            "\t\tif (IS_ENABLED(CONFIG_ARCH_HAS_DMA_CLEAR_UNCACHED))",
            "\t\t\tarch_dma_clear_uncached(cpu_addr, size);",
            "\t\tif (dma_set_encrypted(dev, cpu_addr, size))",
            "\t\t\treturn;",
            "\t}",
            "",
            "\t__dma_direct_free_pages(dev, dma_direct_to_page(dev, dma_addr), size);",
            "}"
          ],
          "function_name": "phys_to_dma_direct, dma_direct_get_required_mask, dma_direct_optimal_gfp_mask, dma_coherent_ok, dma_set_decrypted, dma_set_encrypted, __dma_direct_free_pages, dma_direct_use_pool, dma_direct_free",
          "description": "实现了DMA直接映射的核心辅助函数，包括物理地址转DMA地址、计算DMA掩码、优化内存分配策略、检查DMA一致性及加密内存设置等功能。dma_direct_free处理不同条件下的内存释放逻辑，涉及SWIOTLB、原子池和架构特定的释放路径。",
          "similarity": 0.538298487663269
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/dma/direct.c",
          "start_line": 520,
          "end_line": 633,
          "content": [
            "dma_addr_t dma_direct_map_resource(struct device *dev, phys_addr_t paddr,",
            "\t\tsize_t size, enum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tdma_addr_t dma_addr = paddr;",
            "",
            "\tif (unlikely(!dma_capable(dev, dma_addr, size, false))) {",
            "\t\tdev_err_once(dev,",
            "\t\t\t     \"DMA addr %pad+%zu overflow (mask %llx, bus limit %llx).\\n\",",
            "\t\t\t     &dma_addr, size, *dev->dma_mask, dev->bus_dma_limit);",
            "\t\tWARN_ON_ONCE(1);",
            "\t\treturn DMA_MAPPING_ERROR;",
            "\t}",
            "",
            "\treturn dma_addr;",
            "}",
            "int dma_direct_get_sgtable(struct device *dev, struct sg_table *sgt,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "\tstruct page *page = dma_direct_to_page(dev, dma_addr);",
            "\tint ret;",
            "",
            "\tret = sg_alloc_table(sgt, 1, GFP_KERNEL);",
            "\tif (!ret)",
            "\t\tsg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);",
            "\treturn ret;",
            "}",
            "bool dma_direct_can_mmap(struct device *dev)",
            "{",
            "\treturn dev_is_dma_coherent(dev) ||",
            "\t\tIS_ENABLED(CONFIG_DMA_NONCOHERENT_MMAP);",
            "}",
            "int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "\tunsigned long user_count = vma_pages(vma);",
            "\tunsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;",
            "\tunsigned long pfn = PHYS_PFN(dma_to_phys(dev, dma_addr));",
            "\tint ret = -ENXIO;",
            "",
            "\tvma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);",
            "\tif (force_dma_unencrypted(dev))",
            "\t\tvma->vm_page_prot = pgprot_decrypted(vma->vm_page_prot);",
            "",
            "\tif (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))",
            "\t\treturn ret;",
            "\tif (dma_mmap_from_global_coherent(vma, cpu_addr, size, &ret))",
            "\t\treturn ret;",
            "",
            "\tif (vma->vm_pgoff >= count || user_count > count - vma->vm_pgoff)",
            "\t\treturn -ENXIO;",
            "\treturn remap_pfn_range(vma, vma->vm_start, pfn + vma->vm_pgoff,",
            "\t\t\tuser_count << PAGE_SHIFT, vma->vm_page_prot);",
            "}",
            "int dma_direct_supported(struct device *dev, u64 mask)",
            "{",
            "\tu64 min_mask = (max_pfn - 1) << PAGE_SHIFT;",
            "",
            "\t/*",
            "\t * Because 32-bit DMA masks are so common we expect every architecture",
            "\t * to be able to satisfy them - either by not supporting more physical",
            "\t * memory, or by providing a ZONE_DMA32.  If neither is the case, the",
            "\t * architecture needs to use an IOMMU instead of the direct mapping.",
            "\t */",
            "\tif (mask >= DMA_BIT_MASK(32))",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * This check needs to be against the actual bit mask value, so use",
            "\t * phys_to_dma_unencrypted() here so that the SME encryption mask isn't",
            "\t * part of the check.",
            "\t */",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA))",
            "\t\tmin_mask = min_t(u64, min_mask, DMA_BIT_MASK(zone_dma_bits));",
            "\treturn mask >= phys_to_dma_unencrypted(dev, min_mask);",
            "}",
            "size_t dma_direct_max_mapping_size(struct device *dev)",
            "{",
            "\t/* If SWIOTLB is active, use its maximum mapping size */",
            "\tif (is_swiotlb_active(dev) &&",
            "\t    (dma_addressing_limited(dev) || is_swiotlb_force_bounce(dev)))",
            "\t\treturn swiotlb_max_mapping_size(dev);",
            "\treturn SIZE_MAX;",
            "}",
            "bool dma_direct_need_sync(struct device *dev, dma_addr_t dma_addr)",
            "{",
            "\treturn !dev_is_dma_coherent(dev) ||",
            "\t       is_swiotlb_buffer(dev, dma_to_phys(dev, dma_addr));",
            "}",
            "int dma_direct_set_offset(struct device *dev, phys_addr_t cpu_start,",
            "\t\t\t dma_addr_t dma_start, u64 size)",
            "{",
            "\tstruct bus_dma_region *map;",
            "\tu64 offset = (u64)cpu_start - (u64)dma_start;",
            "",
            "\tif (dev->dma_range_map) {",
            "\t\tdev_err(dev, \"attempt to add DMA range to existing map\\n\");",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tif (!offset)",
            "\t\treturn 0;",
            "",
            "\tmap = kcalloc(2, sizeof(*map), GFP_KERNEL);",
            "\tif (!map)",
            "\t\treturn -ENOMEM;",
            "\tmap[0].cpu_start = cpu_start;",
            "\tmap[0].dma_start = dma_start;",
            "\tmap[0].offset = offset;",
            "\tmap[0].size = size;",
            "\tdev->dma_range_map = map;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "dma_direct_map_resource, dma_direct_get_sgtable, dma_direct_can_mmap, dma_direct_mmap, dma_direct_supported, dma_direct_max_mapping_size, dma_direct_need_sync, dma_direct_set_offset",
          "description": "包含DMA直接映射的资源映射、SG表构建、内存映射支持性检测及最大映射尺寸计算等功能。dma_direct_mmap实现设备内存的VMA映射，dma_direct_supported验证DMA掩码兼容性，dma_direct_set_offset用于配置DMA地址偏移量。",
          "similarity": 0.49312394857406616
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/dma/direct.c",
          "start_line": 391,
          "end_line": 503,
          "content": [
            "void dma_direct_free_pages(struct device *dev, size_t size,",
            "\t\tstruct page *page, dma_addr_t dma_addr,",
            "\t\tenum dma_data_direction dir)",
            "{",
            "\tvoid *vaddr = page_address(page);",
            "",
            "\t/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */",
            "\tif (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&",
            "\t    dma_free_from_pool(dev, vaddr, size))",
            "\t\treturn;",
            "",
            "\tif (dma_set_encrypted(dev, vaddr, size))",
            "\t\treturn;",
            "\t__dma_direct_free_pages(dev, page, size);",
            "}",
            "void dma_direct_sync_sg_for_device(struct device *dev,",
            "\t\tstruct scatterlist *sgl, int nents, enum dma_data_direction dir)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tphys_addr_t paddr = dma_to_phys(dev, sg_dma_address(sg));",
            "",
            "\t\tif (unlikely(is_swiotlb_buffer(dev, paddr)))",
            "\t\t\tswiotlb_sync_single_for_device(dev, paddr, sg->length,",
            "\t\t\t\t\t\t       dir);",
            "",
            "\t\tif (!dev_is_dma_coherent(dev))",
            "\t\t\tarch_sync_dma_for_device(paddr, sg->length,",
            "\t\t\t\t\tdir);",
            "\t}",
            "}",
            "void dma_direct_sync_sg_for_cpu(struct device *dev,",
            "\t\tstruct scatterlist *sgl, int nents, enum dma_data_direction dir)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tphys_addr_t paddr = dma_to_phys(dev, sg_dma_address(sg));",
            "",
            "\t\tif (!dev_is_dma_coherent(dev))",
            "\t\t\tarch_sync_dma_for_cpu(paddr, sg->length, dir);",
            "",
            "\t\tif (unlikely(is_swiotlb_buffer(dev, paddr)))",
            "\t\t\tswiotlb_sync_single_for_cpu(dev, paddr, sg->length,",
            "\t\t\t\t\t\t    dir);",
            "",
            "\t\tif (dir == DMA_FROM_DEVICE)",
            "\t\t\tarch_dma_mark_clean(paddr, sg->length);",
            "\t}",
            "",
            "\tif (!dev_is_dma_coherent(dev))",
            "\t\tarch_sync_dma_for_cpu_all();",
            "}",
            "void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,",
            "\t\tint nents, enum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl,  sg, nents, i) {",
            "\t\tif (sg_dma_is_bus_address(sg))",
            "\t\t\tsg_dma_unmark_bus_address(sg);",
            "\t\telse",
            "\t\t\tdma_direct_unmap_page(dev, sg->dma_address,",
            "\t\t\t\t\t      sg_dma_len(sg), dir, attrs);",
            "\t}",
            "}",
            "int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,",
            "\t\tenum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tstruct pci_p2pdma_map_state p2pdma_state = {};",
            "\tenum pci_p2pdma_map_type map;",
            "\tstruct scatterlist *sg;",
            "\tint i, ret;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tif (is_pci_p2pdma_page(sg_page(sg))) {",
            "\t\t\tmap = pci_p2pdma_map_segment(&p2pdma_state, dev, sg);",
            "\t\t\tswitch (map) {",
            "\t\t\tcase PCI_P2PDMA_MAP_BUS_ADDR:",
            "\t\t\t\tcontinue;",
            "\t\t\tcase PCI_P2PDMA_MAP_THRU_HOST_BRIDGE:",
            "\t\t\t\t/*",
            "\t\t\t\t * Any P2P mapping that traverses the PCI",
            "\t\t\t\t * host bridge must be mapped with CPU physical",
            "\t\t\t\t * address and not PCI bus addresses. This is",
            "\t\t\t\t * done with dma_direct_map_page() below.",
            "\t\t\t\t */",
            "\t\t\t\tbreak;",
            "\t\t\tdefault:",
            "\t\t\t\tret = -EREMOTEIO;",
            "\t\t\t\tgoto out_unmap;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tsg->dma_address = dma_direct_map_page(dev, sg_page(sg),",
            "\t\t\t\tsg->offset, sg->length, dir, attrs);",
            "\t\tif (sg->dma_address == DMA_MAPPING_ERROR) {",
            "\t\t\tret = -EIO;",
            "\t\t\tgoto out_unmap;",
            "\t\t}",
            "\t\tsg_dma_len(sg) = sg->length;",
            "\t}",
            "",
            "\treturn nents;",
            "",
            "out_unmap:",
            "\tdma_direct_unmap_sg(dev, sgl, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "dma_direct_free_pages, dma_direct_sync_sg_for_device, dma_direct_sync_sg_for_cpu, dma_direct_unmap_sg, dma_direct_map_sg",
          "description": "提供了SCATTERLIST的同步、解映射和映射操作实现，包括对非一致内存的架构同步、PCI P2P DMA的特殊处理以及SG表的构建。sync_sg系列函数负责设备与CPU之间的数据缓存一致性维护。",
          "similarity": 0.4823744297027588
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/direct.c",
          "start_line": 1,
          "end_line": 24,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 2018-2020 Christoph Hellwig.",
            " *",
            " * DMA operations that map physical memory directly without using an IOMMU.",
            " */",
            "#include <linux/memblock.h> /* for max_pfn */",
            "#include <linux/export.h>",
            "#include <linux/mm.h>",
            "#include <linux/dma-map-ops.h>",
            "#include <linux/scatterlist.h>",
            "#include <linux/pfn.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/slab.h>",
            "#include \"direct.h\"",
            "",
            "/*",
            " * Most architectures use ZONE_DMA for the first 16 Megabytes, but some use",
            " * it for entirely different regions. In that case the arch code needs to",
            " * override the variable below for dma-direct to work properly.",
            " */",
            "unsigned int zone_dma_bits __ro_after_init = 24;",
            ""
          ],
          "function_name": null,
          "description": "定义了用于DMA直接映射的全局变量zone_dma_bits，默认值为24位，表示DMA地址空间的位宽。该变量用于控制DMA操作的物理地址范围，架构代码可通过覆盖此变量调整DMA直接映射的行为。",
          "similarity": 0.42447197437286377
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tiny.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:45:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tiny.c`\n\n---\n\n# `rcu/tiny.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tiny.c` 是 Linux 内核中 **RCU（Read-Copy Update）机制的“精简版”（Tiny RCU）实现**，专为单处理器（UP）或资源受限系统（如嵌入式设备）设计。该实现去除了复杂的状态机、CPU 间通信和动态负载均衡等开销，仅保留 RCU 的核心语义：**在读端无锁、写端延迟回收**。由于系统只有一个 CPU，任何上下文切换或中断返回用户态都天然构成“宽限期”（Grace Period），因此无需复杂的跨 CPU 同步逻辑。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_ctrlblk`**：RCU 控制块，全局唯一，用于管理回调链表和宽限期状态。\n  - `rcucblist`：待处理的 RCU 回调链表头。\n  - `donetail`：指向最后一个已完成宽限期的回调的 `next` 指针。\n  - `curtail`：指向链表最后一个回调的 `next` 指针。\n  - `gp_seq`：宽限期序列号，每次宽限期结束递增 2（偶数表示完成状态）。\n\n### 主要函数\n\n| 函数 | 功能说明 |\n|------|--------|\n| `rcu_qs(void)` | 记录当前 CPU 的静默状态（Quiescent State），推进已完成回调指针并触发软中断处理回调。 |\n| `rcu_sched_clock_irq(int user)` | 调度时钟中断处理函数，若处于用户态则调用 `rcu_qs()`；否则标记当前任务需重新调度以尽快进入静默状态。 |\n| `call_rcu(struct rcu_head *head, rcu_callback_t func)` | 注册一个 RCU 回调，在下一个宽限期结束后执行。 |\n| `synchronize_rcu(void)` | 等待当前宽限期结束（在 UP 系统中立即完成，仅更新 `gp_seq`）。 |\n| `rcu_process_callbacks(struct softirq_action *unused)` | RCU 软中断处理函数，批量执行已完成宽限期的回调。 |\n| `rcu_barrier(void)` | 等待所有已注册的 RCU 回调执行完毕。 |\n| `get_state_synchronize_rcu()` / `start_poll_synchronize_rcu()` / `poll_state_synchronize_rcu()` | 支持轮询式宽限期检测的 API。 |\n| `rcu_init(void)` | RCU 子系统初始化，注册 RCU 软中断处理函数。 |\n\n## 3. 关键实现\n\n### 宽限期管理\n- **单 CPU 假设**：由于系统只有一个 CPU，任何从内核态返回用户态、发生上下文切换或空闲任务运行，都视为一个完整的宽限期。\n- **`gp_seq` 计数器**：初始值为 `0 - 300UL`（负数，确保早期调用 `get_state_synchronize_rcu()` 返回有效值）。每次调用 `rcu_qs()` 或 `synchronize_rcu()` 时递增 2，偶数值表示宽限期已完成。\n- **无实际等待**：`synchronize_rcu()` 不阻塞，仅更新 `gp_seq`，因为调用者本身已处于静默状态。\n\n### 回调队列管理\n- **双指针链表**：使用 `donetail` 和 `curtail` 实现无锁（在中断禁用下）的回调入队和出队。\n  - 新回调通过 `curtail` 追加到链表尾部。\n  - `rcu_qs()` 将 `donetail` 移至 `curtail`，表示此前所有回调已完成宽限期。\n  - 软中断 `rcu_process_callbacks()` 将 `donetail` 之前的所有回调移出并执行。\n\n### 回调执行\n- **软中断上下文**：回调在 `RCU_SOFTIRQ` 中执行，确保不在原子上下文。\n- **支持 `kvfree`**：通过 `__is_kvfree_rcu_offset` 判断是否为 `kvfree_rcu` 回调，若是则直接释放内存而非调用函数指针。\n- **调试支持**：包含双释放检测（`debug_rcu_head_queue`）和内存泄漏防护（`tiny_rcu_leak_callback`）。\n\n### 轮询 API 实现\n- `get_state_synchronize_rcu()` 返回当前 `gp_seq`。\n- `poll_state_synchronize_rcu(oldstate)` 判断 `oldstate` 是否已完成：若 `oldstate == RCU_GET_STATE_COMPLETED`（特殊值）或当前 `gp_seq != oldstate`，则返回 `true`。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/rcupdate_wait.h>`：提供 `wait_rcu_gp()` 实现 `rcu_barrier()`。\n  - `\"rcu.h\"`：RCU 内部头文件，定义调试宏、trace 点等。\n  - 其他通用内核头文件（如 `sched.h`, `softirq.h`, `slab.h` 等）。\n- **内核配置**：\n  - 仅在 `CONFIG_TINY_RCU` 或 `CONFIG_TINY_SRCU` 启用时编译。\n  - 与 `CONFIG_PREEMPT`、`CONFIG_SMP` 互斥（Tiny RCU 用于非抢占式 UP 系统）。\n- **软中断子系统**：依赖 `open_softirq()` 注册 `RCU_SOFTIRQ`。\n- **内存管理**：`kvfree_call_rcu()` 依赖 KASAN 的辅助栈记录（`CONFIG_KASAN_GENERIC`）。\n\n## 5. 使用场景\n\n- **单处理器嵌入式系统**：资源受限设备（如 IoT 设备、微控制器）中替代 Tree RCU，显著减少代码体积和运行时开销。\n- **内核测试与调试**：作为 RCU 行为的简化模型，用于验证 RCU 语义正确性。\n- **RCU 基础功能提供**：\n  - 为内核其他子系统（如网络、文件系统、设备驱动）提供 `call_rcu()` 和 `synchronize_rcu()` 接口。\n  - 支持延迟内存回收（如 `kfree_rcu()`）。\n  - 通过 `rcu_barrier()` 确保模块卸载前所有回调完成。\n- **轮询式同步**：适用于不能阻塞的上下文（如中断处理程序），通过 `poll_state_synchronize_rcu()` 轮询宽限期状态。",
      "similarity": 0.575939416885376,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/rcu/tiny.c",
          "start_line": 45,
          "end_line": 161,
          "content": [
            "void rcu_barrier(void)",
            "{",
            "\twait_rcu_gp(call_rcu_hurry);",
            "}",
            "void rcu_qs(void)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (rcu_ctrlblk.donetail != rcu_ctrlblk.curtail) {",
            "\t\trcu_ctrlblk.donetail = rcu_ctrlblk.curtail;",
            "\t\traise_softirq_irqoff(RCU_SOFTIRQ);",
            "\t}",
            "\tWRITE_ONCE(rcu_ctrlblk.gp_seq, rcu_ctrlblk.gp_seq + 2);",
            "\tlocal_irq_restore(flags);",
            "}",
            "void rcu_sched_clock_irq(int user)",
            "{",
            "\tif (user) {",
            "\t\trcu_qs();",
            "\t} else if (rcu_ctrlblk.donetail != rcu_ctrlblk.curtail) {",
            "\t\tset_tsk_need_resched(current);",
            "\t\tset_preempt_need_resched();",
            "\t}",
            "}",
            "static inline bool rcu_reclaim_tiny(struct rcu_head *head)",
            "{",
            "\trcu_callback_t f;",
            "\tunsigned long offset = (unsigned long)head->func;",
            "",
            "\trcu_lock_acquire(&rcu_callback_map);",
            "\tif (__is_kvfree_rcu_offset(offset)) {",
            "\t\ttrace_rcu_invoke_kvfree_callback(\"\", head, offset);",
            "\t\tkvfree((void *)head - offset);",
            "\t\trcu_lock_release(&rcu_callback_map);",
            "\t\treturn true;",
            "\t}",
            "",
            "\ttrace_rcu_invoke_callback(\"\", head);",
            "\tf = head->func;",
            "\tdebug_rcu_head_callback(head);",
            "\tWRITE_ONCE(head->func, (rcu_callback_t)0L);",
            "\tf(head);",
            "\trcu_lock_release(&rcu_callback_map);",
            "\treturn false;",
            "}",
            "static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused)",
            "{",
            "\tstruct rcu_head *next, *list;",
            "\tunsigned long flags;",
            "",
            "\t/* Move the ready-to-invoke callbacks to a local list. */",
            "\tlocal_irq_save(flags);",
            "\tif (rcu_ctrlblk.donetail == &rcu_ctrlblk.rcucblist) {",
            "\t\t/* No callbacks ready, so just leave. */",
            "\t\tlocal_irq_restore(flags);",
            "\t\treturn;",
            "\t}",
            "\tlist = rcu_ctrlblk.rcucblist;",
            "\trcu_ctrlblk.rcucblist = *rcu_ctrlblk.donetail;",
            "\t*rcu_ctrlblk.donetail = NULL;",
            "\tif (rcu_ctrlblk.curtail == rcu_ctrlblk.donetail)",
            "\t\trcu_ctrlblk.curtail = &rcu_ctrlblk.rcucblist;",
            "\trcu_ctrlblk.donetail = &rcu_ctrlblk.rcucblist;",
            "\tlocal_irq_restore(flags);",
            "",
            "\t/* Invoke the callbacks on the local list. */",
            "\twhile (list) {",
            "\t\tnext = list->next;",
            "\t\tprefetch(next);",
            "\t\tdebug_rcu_head_unqueue(list);",
            "\t\tlocal_bh_disable();",
            "\t\trcu_reclaim_tiny(list);",
            "\t\tlocal_bh_enable();",
            "\t\tlist = next;",
            "\t}",
            "}",
            "void synchronize_rcu(void)",
            "{",
            "\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_sched_lock_map),",
            "\t\t\t \"Illegal synchronize_rcu() in RCU read-side critical section\");",
            "\tWRITE_ONCE(rcu_ctrlblk.gp_seq, rcu_ctrlblk.gp_seq + 2);",
            "}",
            "static void tiny_rcu_leak_callback(struct rcu_head *rhp)",
            "{",
            "}",
            "void call_rcu(struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\tstatic atomic_t doublefrees;",
            "\tunsigned long flags;",
            "",
            "\tif (debug_rcu_head_queue(head)) {",
            "\t\tif (atomic_inc_return(&doublefrees) < 4) {",
            "\t\t\tpr_err(\"%s(): Double-freed CB %p->%pS()!!!  \", __func__, head, head->func);",
            "\t\t\tmem_dump_obj(head);",
            "\t\t}",
            "",
            "\t\tif (!__is_kvfree_rcu_offset((unsigned long)head->func))",
            "\t\t\tWRITE_ONCE(head->func, tiny_rcu_leak_callback);",
            "\t\treturn;",
            "\t}",
            "",
            "\thead->func = func;",
            "\thead->next = NULL;",
            "",
            "\tlocal_irq_save(flags);",
            "\t*rcu_ctrlblk.curtail = head;",
            "\trcu_ctrlblk.curtail = &head->next;",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (unlikely(is_idle_task(current))) {",
            "\t\t/* force scheduling for rcu_qs() */",
            "\t\tresched_cpu(0);",
            "\t}",
            "}"
          ],
          "function_name": "rcu_barrier, rcu_qs, rcu_sched_clock_irq, rcu_reclaim_tiny, rcu_process_callbacks, synchronize_rcu, tiny_rcu_leak_callback, call_rcu",
          "description": "实现RCU核心函数，包括rcu_barrier触发同步屏障、rcu_process_callbacks处理回调队列、synchronize_rcu更新grace period序列号、call_rcu添加回调到链表并处理空闲任务调度。",
          "similarity": 0.46479532122612
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/rcu/tiny.c",
          "start_line": 1,
          "end_line": 44,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0+",
            "/*",
            " * Read-Copy Update mechanism for mutual exclusion, the Bloatwatch edition.",
            " *",
            " * Copyright IBM Corporation, 2008",
            " *",
            " * Author: Paul E. McKenney <paulmck@linux.ibm.com>",
            " *",
            " * For detailed explanation of Read-Copy Update mechanism see -",
            " *\t\tDocumentation/RCU",
            " */",
            "#include <linux/completion.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/notifier.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/sched.h>",
            "#include <linux/types.h>",
            "#include <linux/init.h>",
            "#include <linux/time.h>",
            "#include <linux/cpu.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>",
            "",
            "#include \"rcu.h\"",
            "",
            "/* Global control variables for rcupdate callback mechanism. */",
            "struct rcu_ctrlblk {",
            "\tstruct rcu_head *rcucblist;\t/* List of pending callbacks (CBs). */",
            "\tstruct rcu_head **donetail;\t/* ->next pointer of last \"done\" CB. */",
            "\tstruct rcu_head **curtail;\t/* ->next pointer of last CB. */",
            "\tunsigned long gp_seq;\t\t/* Grace-period counter. */",
            "};",
            "",
            "/* Definition for rcupdate control block. */",
            "static struct rcu_ctrlblk rcu_ctrlblk = {",
            "\t.donetail\t= &rcu_ctrlblk.rcucblist,",
            "\t.curtail\t= &rcu_ctrlblk.rcucblist,",
            "\t.gp_seq\t\t= 0 - 300UL,",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义RCU控制块结构体rcu_ctrlblk，用于管理回调链表、尾指针及 grace period 序列号，初始化时设置 donetail 和 curtail 指向同一位置，gp_seq 初始化为负值以标记初始状态。",
          "similarity": 0.4394616484642029
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/rcu/tiny.c",
          "start_line": 206,
          "end_line": 239,
          "content": [
            "void get_completed_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\trgosp->rgos_norm = RCU_GET_STATE_COMPLETED;",
            "}",
            "unsigned long get_state_synchronize_rcu(void)",
            "{",
            "\treturn READ_ONCE(rcu_ctrlblk.gp_seq);",
            "}",
            "unsigned long start_poll_synchronize_rcu(void)",
            "{",
            "\tunsigned long gp_seq = get_state_synchronize_rcu();",
            "",
            "\tif (unlikely(is_idle_task(current))) {",
            "\t\t/* force scheduling for rcu_qs() */",
            "\t\tresched_cpu(0);",
            "\t}",
            "\treturn gp_seq;",
            "}",
            "bool poll_state_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\treturn oldstate == RCU_GET_STATE_COMPLETED || READ_ONCE(rcu_ctrlblk.gp_seq) != oldstate;",
            "}",
            "void kvfree_call_rcu(struct rcu_head *head, void *ptr)",
            "{",
            "\tif (head)",
            "\t\tkasan_record_aux_stack_noalloc(ptr);",
            "",
            "\t__kvfree_call_rcu(head, ptr);",
            "}",
            "void __init rcu_init(void)",
            "{",
            "\topen_softirq(RCU_SOFTIRQ, rcu_process_callbacks);",
            "\trcu_early_boot_tests();",
            "}"
          ],
          "function_name": "get_completed_synchronize_rcu_full, get_state_synchronize_rcu, start_poll_synchronize_rcu, poll_state_synchronize_rcu, kvfree_call_rcu, rcu_init",
          "description": "提供RCU状态查询接口，get_state_synchronize_rcu读取当前grace period序号，start_poll_synchronize_rcu用于轮询同步状态，rcu_init初始化RCU软中断处理函数注册。",
          "similarity": 0.40027451515197754
        }
      ]
    },
    {
      "source_file": "kernel/irq/resend.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:07:32\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\resend.c`\n\n---\n\n# `irq/resend.c` 技术文档\n\n## 1. 文件概述\n\n`irq/resend.c` 实现了 Linux 内核中断子系统中的 **中断重发（IRQ resend）机制**。该机制用于在中断未能被及时处理或需要重新触发时，尝试通过硬件重触发（retrigger）或软件任务（tasklet）方式重新投递中断。此功能主要用于支持那些无法通过硬件自动重发中断的中断控制器，以及用于调试和错误注入场景。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `check_irq_resend(struct irq_desc *desc, bool inject)` | 检查是否需要重发中断，并尝试通过硬件重触发或软件重发机制重新投递中断。 |\n| `irq_sw_resend(struct irq_desc *desc)` | 在不支持硬件重触发时，将中断描述符加入软件重发队列，并调度 tasklet 处理。 |\n| `resend_irqs(struct tasklet_struct *unused)` | tasklet 回调函数，遍历软件重发队列并调用对应中断的 `handle_irq` 处理函数。 |\n| `clear_irq_resend(struct irq_desc *desc)` | 从软件重发队列中移除指定中断描述符。 |\n| `irq_resend_init(struct irq_desc *desc)` | 初始化中断描述符的重发链表节点。 |\n| `irq_inject_interrupt(unsigned int irq)`（仅当 `CONFIG_GENERIC_IRQ_INJECTION` 启用） | 用于调试目的，主动注入指定中断。 |\n\n### 主要数据结构\n\n- `irq_resend_list`：全局哈希链表头，用于维护待软件重发的中断描述符。\n- `irq_resend_lock`：保护 `irq_resend_list` 的原始自旋锁（raw spinlock）。\n- `resend_tasklet`：用于异步执行软件重发逻辑的 tasklet。\n- `desc->resend_node`：`struct irq_desc` 中用于链入 `irq_resend_list` 的节点。\n\n## 3. 关键实现\n\n### 中断重发条件判断\n\n- **仅边沿触发中断支持重发**：`check_irq_resend()` 会跳过电平触发中断（`irq_settings_is_level()`），因为这类中断在电平有效期间会由硬件自动保持。\n- **避免重复重发**：若中断已处于 `IRQS_REPLAY` 状态，则返回 `-EBUSY`，防止重复处理。\n- **注入模式支持**：当 `inject == true`（如 `irq_inject_interrupt` 调用）时，即使 `IRQS_PENDING` 未置位也会尝试重发。\n\n### 硬件 vs 软件重发\n\n- **优先尝试硬件重触发**：调用 `try_retrigger()`，先检查中断芯片是否提供 `irq_retrigger` 回调；若无，则尝试层级中断域的 `irq_chip_retrigger_hierarchy`。\n- **回退到软件重发**：若硬件重触发不可用且 `CONFIG_HARDIRQS_SW_RESEND` 已启用，则调用 `irq_sw_resend()` 将中断加入软件队列。\n\n### 软件重发机制\n\n- 使用 **tasklet** 在软中断上下文中执行重发，避免在硬中断或原子上下文中直接调用 `handle_irq`。\n- 通过 `hlist` 管理待重发的 `irq_desc`，并用 `raw_spinlock` 保证并发安全。\n- 支持嵌套线程中断（nested threaded IRQ）：若目标中断是嵌套线程类型，则重发其父中断。\n\n### 安全性检查\n\n- `handle_enforce_irqctx()` 确保中断可在非中断上下文中安全注入。\n- 对嵌套线程中断，验证 `parent_irq` 有效性，防止空指针解引用。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/irq.h>`、`<linux/interrupt.h>`：提供中断核心 API 和数据结构。\n  - `\"internals.h\"`：内核中断子系统内部头文件，包含 `irq_desc` 等私有定义。\n- **配置依赖**：\n  - `CONFIG_HARDIRQS_SW_RESEND`：启用软件重发机制。\n  - `CONFIG_IRQ_DOMAIN_HIERARCHY`：支持层级中断域的重触发。\n  - `CONFIG_GENERIC_IRQ_INJECTION`：启用 `irq_inject_interrupt()` 调试接口。\n- **与其他模块交互**：\n  - 依赖中断控制器驱动实现的 `irq_retrigger` 回调。\n  - 与通用中断处理框架（如 `handle_irq`）紧密集成。\n\n## 5. 使用场景\n\n1. **中断丢失恢复**：在某些硬件平台或虚拟化环境中，中断可能因竞争条件或延迟而丢失，重发机制可提高可靠性。\n2. **调试与测试**：通过 `irq_inject_interrupt()` 主动注入中断，用于测试中断处理路径、驱动健壮性或错误恢复逻辑。\n3. **不支持硬件重触发的平台**：如部分 ARM SoC 或旧式 x86 芯片组，依赖软件 tasklet 机制模拟中断重发。\n4. **电源管理（suspend/resume）**：在系统恢复过程中，可能需要重放挂起期间未处理的边沿中断。\n5. **嵌套中断处理**：在 threaded IRQ 架构中，确保子中断能通过父中断正确重发。",
      "similarity": 0.5731532573699951,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/resend.c",
          "start_line": 31,
          "end_line": 137,
          "content": [
            "static void resend_irqs(struct tasklet_struct *unused)",
            "{",
            "\tstruct irq_desc *desc;",
            "",
            "\traw_spin_lock_irq(&irq_resend_lock);",
            "\twhile (!hlist_empty(&irq_resend_list)) {",
            "\t\tdesc = hlist_entry(irq_resend_list.first, struct irq_desc,",
            "\t\t\t\t   resend_node);",
            "\t\thlist_del_init(&desc->resend_node);",
            "\t\traw_spin_unlock(&irq_resend_lock);",
            "\t\tdesc->handle_irq(desc);",
            "\t\traw_spin_lock(&irq_resend_lock);",
            "\t}",
            "\traw_spin_unlock_irq(&irq_resend_lock);",
            "}",
            "static int irq_sw_resend(struct irq_desc *desc)",
            "{",
            "\t/*",
            "\t * Validate whether this interrupt can be safely injected from",
            "\t * non interrupt context",
            "\t */",
            "\tif (handle_enforce_irqctx(&desc->irq_data))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * If the interrupt is running in the thread context of the parent",
            "\t * irq we need to be careful, because we cannot trigger it",
            "\t * directly.",
            "\t */",
            "\tif (irq_settings_is_nested_thread(desc)) {",
            "\t\t/*",
            "\t\t * If the parent_irq is valid, we retrigger the parent,",
            "\t\t * otherwise we do nothing.",
            "\t\t */",
            "\t\tif (!desc->parent_irq)",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\tdesc = irq_to_desc(desc->parent_irq);",
            "\t\tif (!desc)",
            "\t\t\treturn -EINVAL;",
            "\t}",
            "",
            "\t/* Add to resend_list and activate the softirq: */",
            "\traw_spin_lock(&irq_resend_lock);",
            "\tif (hlist_unhashed(&desc->resend_node))",
            "\t\thlist_add_head(&desc->resend_node, &irq_resend_list);",
            "\traw_spin_unlock(&irq_resend_lock);",
            "\ttasklet_schedule(&resend_tasklet);",
            "\treturn 0;",
            "}",
            "void clear_irq_resend(struct irq_desc *desc)",
            "{",
            "\traw_spin_lock(&irq_resend_lock);",
            "\thlist_del_init(&desc->resend_node);",
            "\traw_spin_unlock(&irq_resend_lock);",
            "}",
            "void irq_resend_init(struct irq_desc *desc)",
            "{",
            "\tINIT_HLIST_NODE(&desc->resend_node);",
            "}",
            "void clear_irq_resend(struct irq_desc *desc) {}",
            "void irq_resend_init(struct irq_desc *desc) {}",
            "static int irq_sw_resend(struct irq_desc *desc)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "static int try_retrigger(struct irq_desc *desc)",
            "{",
            "\tif (desc->irq_data.chip->irq_retrigger)",
            "\t\treturn desc->irq_data.chip->irq_retrigger(&desc->irq_data);",
            "",
            "#ifdef CONFIG_IRQ_DOMAIN_HIERARCHY",
            "\treturn irq_chip_retrigger_hierarchy(&desc->irq_data);",
            "#else",
            "\treturn 0;",
            "#endif",
            "}",
            "int check_irq_resend(struct irq_desc *desc, bool inject)",
            "{",
            "\tint err = 0;",
            "",
            "\t/*",
            "\t * We do not resend level type interrupts. Level type interrupts",
            "\t * are resent by hardware when they are still active. Clear the",
            "\t * pending bit so suspend/resume does not get confused.",
            "\t */",
            "\tif (irq_settings_is_level(desc)) {",
            "\t\tdesc->istate &= ~IRQS_PENDING;",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tif (desc->istate & IRQS_REPLAY)",
            "\t\treturn -EBUSY;",
            "",
            "\tif (!(desc->istate & IRQS_PENDING) && !inject)",
            "\t\treturn 0;",
            "",
            "\tdesc->istate &= ~IRQS_PENDING;",
            "",
            "\tif (!try_retrigger(desc))",
            "\t\terr = irq_sw_resend(desc);",
            "",
            "\t/* If the retrigger was successful, mark it with the REPLAY bit */",
            "\tif (!err)",
            "\t\tdesc->istate |= IRQS_REPLAY;",
            "\treturn err;",
            "}"
          ],
          "function_name": "resend_irqs, irq_sw_resend, clear_irq_resend, irq_resend_init, clear_irq_resend, irq_resend_init, irq_sw_resend, try_retrigger, check_irq_resend",
          "description": "实现中断重发核心逻辑，包含中断添加至重发队列、任务调度、状态清理等功能，部分函数存在重复定义提示代码不完整",
          "similarity": 0.5226728916168213
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq/resend.c",
          "start_line": 173,
          "end_line": 200,
          "content": [
            "int irq_inject_interrupt(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tunsigned long flags;",
            "\tint err;",
            "",
            "\t/* Try the state injection hardware interface first */",
            "\tif (!irq_set_irqchip_state(irq, IRQCHIP_STATE_PENDING, true))",
            "\t\treturn 0;",
            "",
            "\t/* That failed, try via the resend mechanism */",
            "\tdesc = irq_get_desc_buslock(irq, &flags, 0);",
            "\tif (!desc)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Only try to inject when the interrupt is:",
            "\t *  - not NMI type",
            "\t *  - activated",
            "\t */",
            "\tif ((desc->istate & IRQS_NMI) || !irqd_is_activated(&desc->irq_data))",
            "\t\terr = -EINVAL;",
            "\telse",
            "\t\terr = check_irq_resend(desc, true);",
            "",
            "\tirq_put_desc_busunlock(desc, flags);",
            "\treturn err;",
            "}"
          ],
          "function_name": "irq_inject_interrupt",
          "description": "提供中断注入接口，通过硬件设置或软件重发机制将中断请求注入系统，需先获取中断描述符并验证中断属性",
          "similarity": 0.4919659495353699
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/resend.c",
          "start_line": 1,
          "end_line": 30,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 1992, 1998-2006 Linus Torvalds, Ingo Molnar",
            " * Copyright (C) 2005-2006, Thomas Gleixner",
            " *",
            " * This file contains the IRQ-resend code",
            " *",
            " * If the interrupt is waiting to be processed, we try to re-run it.",
            " * We can't directly run it from here since the caller might be in an",
            " * interrupt-protected region. Not all irq controller chips can",
            " * retrigger interrupts at the hardware level, so in those cases",
            " * we allow the resending of IRQs via a tasklet.",
            " */",
            "",
            "#include <linux/irq.h>",
            "#include <linux/module.h>",
            "#include <linux/random.h>",
            "#include <linux/interrupt.h>",
            "",
            "#include \"internals.h\"",
            "",
            "#ifdef CONFIG_HARDIRQS_SW_RESEND",
            "",
            "/* hlist_head to handle software resend of interrupts: */",
            "static HLIST_HEAD(irq_resend_list);",
            "static DEFINE_RAW_SPINLOCK(irq_resend_lock);",
            "",
            "/*",
            " * Run software resends of IRQ's",
            " */"
          ],
          "function_name": null,
          "description": "定义软件中断重发机制所需的全局数据结构和锁，用于管理待重发中断列表及同步访问",
          "similarity": 0.46121132373809814
        }
      ]
    }
  ]
}