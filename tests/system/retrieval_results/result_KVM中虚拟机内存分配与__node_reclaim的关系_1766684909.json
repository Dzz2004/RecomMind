{
  "query": "KVM中虚拟机内存分配与__node_reclaim的关系",
  "timestamp": "2025-12-26 01:48:29",
  "retrieved_files": [
    {
      "source_file": "mm/damon/reclaim.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:50:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `damon\\reclaim.c`\n\n---\n\n# `damon/reclaim.c` 技术文档\n\n## 1. 文件概述\n\n`damon/reclaim.c` 是 Linux 内核中基于 **DAMON（Data Access MONitor）** 框架实现的**自动内存回收模块**。该模块通过监控物理内存区域的访问模式，识别长时间未被访问的“冷”内存页，并主动将其回收（page-out），从而释放系统内存资源。其核心目标是在不影响系统性能的前提下，智能地回收低价值内存，提升内存利用率。\n\n该模块以可加载内核模块（LKM）形式存在，通过一组可调参数控制其行为，并支持基于水位线（watermarks）的条件激活机制，避免在内存充足时进行不必要的回收操作。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`enabled`**: 全局开关，控制 DAMON_RECLAIM 功能是否启用。\n- **`commit_inputs`**: 触发参数重载的标志位，用于运行时动态更新配置（除 `enabled` 外）。\n- **`min_age`**: 冷内存判定阈值（微秒），默认 120 秒。\n- **`damon_reclaim_quota`**: 回收配额控制结构，限制单位时间内的最大回收量（默认每秒最多 128 MiB）和 CPU 时间开销（默认最多 10 ms）。\n- **`damon_reclaim_wmarks`**: 水位线配置，基于空闲内存比率决定是否激活回收（高/中/低水位分别为 50%/40%/20%）。\n- **`damon_reclaim_mon_attrs`**: DAMON 监控属性，定义采样间隔（5ms）、聚合间隔（100ms）等。\n- **`monitor_region_start/end`**: 目标监控内存区域的物理地址范围，默认为系统最大连续 RAM 区域。\n- **`skip_anon`**: 布尔标志，若为真则跳过匿名页（anonymous pages）的回收。\n- **`kdamond_pid`**: DAMON 工作线程的 PID，未启用时为 -1。\n- **`damon_reclaim_stat`**: 统计信息结构，记录尝试回收区域数、成功回收区域数及配额超限次数。\n\n### 主要函数\n\n- **`damon_reclaim_new_scheme()`**: 创建 DAMOS（DAMON Operation Scheme）策略，定义“冷内存”模式（大小 ≥ PAGE_SIZE、访问次数为 0、年龄 ≥ `min_age`）并指定操作为 `DAMOS_PAGEOUT`。\n- **`damon_reclaim_apply_parameters()`**: 应用所有用户配置参数到 DAMON 上下文（`ctx`），包括监控属性、回收策略、过滤器（如 `skip_anon`）和监控区域。\n- **`damon_reclaim_turn()`**: 启动或停止 DAMON_RECLAIM 的核心监控与回收逻辑。\n- **`damon_reclaim_enabled_store()`**: `enabled` 参数的 setter 回调，处理启用/禁用逻辑。\n- **`damon_reclaim_handle_commit_inputs()`**: 处理 `commit_inputs` 标志，触发运行时参数重载。\n- **`damon_reclaim_after_aggregation()` / `damon_reclaim_after_wmarks_check()`**: DAMON 回调函数，在聚合后和水位检查后更新统计信息并处理参数提交。\n- **`damon_reclaim_init()`**: 模块初始化函数，创建 DAMON 上下文和目标，注册回调，并根据初始 `enabled` 状态决定是否启动。\n\n## 3. 关键实现\n\n### 冷内存识别与回收策略\n- 通过 `damon_reclaim_new_scheme()` 定义 DAMOS 策略：\n  - **访问模式匹配**：区域大小 ≥ `PAGE_SIZE`、访问次数 = 0、年龄 ≥ `min_age / aggr_interval`（转换为聚合周期单位）。\n  - **操作类型**：`DAMOS_PAGEOUT`，即对匹配区域执行页面回收。\n  - **配额控制**：使用 `damon_reclaim_quota` 限制回收速度和 CPU 开销，确保系统稳定性。\n  - **水位激活**：仅当空闲内存比率低于 `high` 水位（50%）时激活策略，高于 `low` 水位（20%）时停用。\n\n### 动态参数更新机制\n- 用户可通过写入 `commit_inputs=Y` 触发运行时参数重载（`min_age`、配额、水位、监控区域等）。\n- `damon_reclaim_handle_commit_inputs()` 在 DAMON 的聚合后或水位检查后回调中执行重载，确保线程安全。\n- 重载时保留旧策略的配额状态（如已消耗的配额），避免统计中断。\n\n### 匿名页过滤\n- 若 `skip_anon=Y`，通过 `DAMOS_FILTER_TYPE_ANON` 过滤器排除匿名页（如进程堆栈、堆内存），仅回收文件缓存等页面。\n\n### 监控区域自动配置\n- 默认使用 `damon_set_region_biggest_system_ram_default()` 自动选择系统中最大的连续物理 RAM 区域作为监控目标，用户也可通过 `monitor_region_start/end` 手动指定。\n\n## 4. 依赖关系\n\n- **DAMON 核心框架** (`<linux/damon.h>`): 依赖 DAMON 提供的内存访问监控、策略引擎（DAMOS）、配额管理、水位控制等基础设施。\n- **内核模块通用接口** (`modules-common.h`): 使用 `DEFINE_DAMON_MODULES_*` 宏简化参数声明和统计暴露。\n- **内存管理子系统**: 通过 `DAMOS_PAGEOUT` 操作与 MM 子系统交互，实际执行页面回收。\n- **参数解析工具** (`<linux/kstrtox.h>`): 用于解析用户输入的布尔值和数值参数。\n\n## 5. 使用场景\n\n- **内存压力缓解**: 在内存紧张但尚未触发传统 LRU 回收或 OOM Killer 之前，提前回收长期未使用的冷内存，延缓内存压力。\n- **容器/虚拟机内存优化**: 在容器或 VM 中部署，自动回收应用未使用的缓存内存，提高宿主机内存密度。\n- **大内存系统调优**: 在 TB 级内存服务器上，减少因缓存膨胀导致的内存浪费，提升整体内存效率。\n- **低延迟敏感场景**: 通过配额限制（`ms=10`）确保回收操作不会显著影响关键任务的延迟。\n- **调试与监控**: 通过 `kdamond_pid` 和统计参数（`reclaim_tried_regions` 等）监控 DAMON_RECLAIM 的运行状态和效果。",
      "similarity": 0.6067487001419067,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/damon/reclaim.c",
          "start_line": 153,
          "end_line": 254,
          "content": [
            "static void damon_reclaim_copy_quota_status(struct damos_quota *dst,",
            "\t\tstruct damos_quota *src)",
            "{",
            "\tdst->total_charged_sz = src->total_charged_sz;",
            "\tdst->total_charged_ns = src->total_charged_ns;",
            "\tdst->charged_sz = src->charged_sz;",
            "\tdst->charged_from = src->charged_from;",
            "\tdst->charge_target_from = src->charge_target_from;",
            "\tdst->charge_addr_from = src->charge_addr_from;",
            "}",
            "static int damon_reclaim_apply_parameters(void)",
            "{",
            "\tstruct damos *scheme, *old_scheme;",
            "\tstruct damos_filter *filter;",
            "\tint err = 0;",
            "",
            "\terr = damon_set_attrs(ctx, &damon_reclaim_mon_attrs);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\t/* Will be freed by next 'damon_set_schemes()' below */",
            "\tscheme = damon_reclaim_new_scheme();",
            "\tif (!scheme)",
            "\t\treturn -ENOMEM;",
            "\tif (!list_empty(&ctx->schemes)) {",
            "\t\tdamon_for_each_scheme(old_scheme, ctx)",
            "\t\t\tdamon_reclaim_copy_quota_status(&scheme->quota,",
            "\t\t\t\t\t&old_scheme->quota);",
            "\t}",
            "\tif (skip_anon) {",
            "\t\tfilter = damos_new_filter(DAMOS_FILTER_TYPE_ANON, true);",
            "\t\tif (!filter) {",
            "\t\t\t/* Will be freed by next 'damon_set_schemes()' below */",
            "\t\t\tdamon_destroy_scheme(scheme);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "\t\tdamos_add_filter(scheme, filter);",
            "\t}",
            "\tdamon_set_schemes(ctx, &scheme, 1);",
            "",
            "\treturn damon_set_region_biggest_system_ram_default(target,",
            "\t\t\t\t\t&monitor_region_start,",
            "\t\t\t\t\t&monitor_region_end);",
            "}",
            "static int damon_reclaim_turn(bool on)",
            "{",
            "\tint err;",
            "",
            "\tif (!on) {",
            "\t\terr = damon_stop(&ctx, 1);",
            "\t\tif (!err)",
            "\t\t\tkdamond_pid = -1;",
            "\t\treturn err;",
            "\t}",
            "",
            "\terr = damon_reclaim_apply_parameters();",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\terr = damon_start(&ctx, 1, true);",
            "\tif (err)",
            "\t\treturn err;",
            "\tkdamond_pid = ctx->kdamond->pid;",
            "\treturn 0;",
            "}",
            "static int damon_reclaim_enabled_store(const char *val,",
            "\t\tconst struct kernel_param *kp)",
            "{",
            "\tbool is_enabled = enabled;",
            "\tbool enable;",
            "\tint err;",
            "",
            "\terr = kstrtobool(val, &enable);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (is_enabled == enable)",
            "\t\treturn 0;",
            "",
            "\t/* Called before init function.  The function will handle this. */",
            "\tif (!ctx)",
            "\t\tgoto set_param_out;",
            "",
            "\terr = damon_reclaim_turn(enable);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "set_param_out:",
            "\tenabled = enable;",
            "\treturn err;",
            "}",
            "static int damon_reclaim_handle_commit_inputs(void)",
            "{",
            "\tint err;",
            "",
            "\tif (!commit_inputs)",
            "\t\treturn 0;",
            "",
            "\terr = damon_reclaim_apply_parameters();",
            "\tcommit_inputs = false;",
            "\treturn err;",
            "}"
          ],
          "function_name": "damon_reclaim_copy_quota_status, damon_reclaim_apply_parameters, damon_reclaim_turn, damon_reclaim_enabled_store, damon_reclaim_handle_commit_inputs",
          "description": "实现DAMON_RECLAIM参数动态应用、启停切换及配额状态复制逻辑，通过回调机制协调监控上下文与回收策略，支持运行时参数更新和资源回收操作。",
          "similarity": 0.5346866250038147
        },
        {
          "chunk_id": 0,
          "file_path": "mm/damon/reclaim.c",
          "start_line": 1,
          "end_line": 152,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * DAMON-based page reclamation",
            " *",
            " * Author: SeongJae Park <sj@kernel.org>",
            " */",
            "",
            "#define pr_fmt(fmt) \"damon-reclaim: \" fmt",
            "",
            "#include <linux/damon.h>",
            "#include <linux/kstrtox.h>",
            "#include <linux/module.h>",
            "",
            "#include \"modules-common.h\"",
            "",
            "#ifdef MODULE_PARAM_PREFIX",
            "#undef MODULE_PARAM_PREFIX",
            "#endif",
            "#define MODULE_PARAM_PREFIX \"damon_reclaim.\"",
            "",
            "/*",
            " * Enable or disable DAMON_RECLAIM.",
            " *",
            " * You can enable DAMON_RCLAIM by setting the value of this parameter as ``Y``.",
            " * Setting it as ``N`` disables DAMON_RECLAIM.  Note that DAMON_RECLAIM could",
            " * do no real monitoring and reclamation due to the watermarks-based activation",
            " * condition.  Refer to below descriptions for the watermarks parameter for",
            " * this.",
            " */",
            "static bool enabled __read_mostly;",
            "",
            "/*",
            " * Make DAMON_RECLAIM reads the input parameters again, except ``enabled``.",
            " *",
            " * Input parameters that updated while DAMON_RECLAIM is running are not applied",
            " * by default.  Once this parameter is set as ``Y``, DAMON_RECLAIM reads values",
            " * of parametrs except ``enabled`` again.  Once the re-reading is done, this",
            " * parameter is set as ``N``.  If invalid parameters are found while the",
            " * re-reading, DAMON_RECLAIM will be disabled.",
            " */",
            "static bool commit_inputs __read_mostly;",
            "module_param(commit_inputs, bool, 0600);",
            "",
            "/*",
            " * Time threshold for cold memory regions identification in microseconds.",
            " *",
            " * If a memory region is not accessed for this or longer time, DAMON_RECLAIM",
            " * identifies the region as cold, and reclaims.  120 seconds by default.",
            " */",
            "static unsigned long min_age __read_mostly = 120000000;",
            "module_param(min_age, ulong, 0600);",
            "",
            "static struct damos_quota damon_reclaim_quota = {",
            "\t/* use up to 10 ms time, reclaim up to 128 MiB per 1 sec by default */",
            "\t.ms = 10,",
            "\t.sz = 128 * 1024 * 1024,",
            "\t.reset_interval = 1000,",
            "\t/* Within the quota, page out older regions first. */",
            "\t.weight_sz = 0,",
            "\t.weight_nr_accesses = 0,",
            "\t.weight_age = 1",
            "};",
            "DEFINE_DAMON_MODULES_DAMOS_QUOTAS(damon_reclaim_quota);",
            "",
            "static struct damos_watermarks damon_reclaim_wmarks = {",
            "\t.metric = DAMOS_WMARK_FREE_MEM_RATE,",
            "\t.interval = 5000000,\t/* 5 seconds */",
            "\t.high = 500,\t\t/* 50 percent */",
            "\t.mid = 400,\t\t/* 40 percent */",
            "\t.low = 200,\t\t/* 20 percent */",
            "};",
            "DEFINE_DAMON_MODULES_WMARKS_PARAMS(damon_reclaim_wmarks);",
            "",
            "static struct damon_attrs damon_reclaim_mon_attrs = {",
            "\t.sample_interval = 5000,\t/* 5 ms */",
            "\t.aggr_interval = 100000,\t/* 100 ms */",
            "\t.ops_update_interval = 0,",
            "\t.min_nr_regions = 10,",
            "\t.max_nr_regions = 1000,",
            "};",
            "DEFINE_DAMON_MODULES_MON_ATTRS_PARAMS(damon_reclaim_mon_attrs);",
            "",
            "/*",
            " * Start of the target memory region in physical address.",
            " *",
            " * The start physical address of memory region that DAMON_RECLAIM will do work",
            " * against.  By default, biggest System RAM is used as the region.",
            " */",
            "static unsigned long monitor_region_start __read_mostly;",
            "module_param(monitor_region_start, ulong, 0600);",
            "",
            "/*",
            " * End of the target memory region in physical address.",
            " *",
            " * The end physical address of memory region that DAMON_RECLAIM will do work",
            " * against.  By default, biggest System RAM is used as the region.",
            " */",
            "static unsigned long monitor_region_end __read_mostly;",
            "module_param(monitor_region_end, ulong, 0600);",
            "",
            "/*",
            " * Skip anonymous pages reclamation.",
            " *",
            " * If this parameter is set as ``Y``, DAMON_RECLAIM does not reclaim anonymous",
            " * pages.  By default, ``N``.",
            " */",
            "static bool skip_anon __read_mostly;",
            "module_param(skip_anon, bool, 0600);",
            "",
            "/*",
            " * PID of the DAMON thread",
            " *",
            " * If DAMON_RECLAIM is enabled, this becomes the PID of the worker thread.",
            " * Else, -1.",
            " */",
            "static int kdamond_pid __read_mostly = -1;",
            "module_param(kdamond_pid, int, 0400);",
            "",
            "static struct damos_stat damon_reclaim_stat;",
            "DEFINE_DAMON_MODULES_DAMOS_STATS_PARAMS(damon_reclaim_stat,",
            "\t\treclaim_tried_regions, reclaimed_regions, quota_exceeds);",
            "",
            "static struct damon_ctx *ctx;",
            "static struct damon_target *target;",
            "",
            "static struct damos *damon_reclaim_new_scheme(void)",
            "{",
            "\tstruct damos_access_pattern pattern = {",
            "\t\t/* Find regions having PAGE_SIZE or larger size */",
            "\t\t.min_sz_region = PAGE_SIZE,",
            "\t\t.max_sz_region = ULONG_MAX,",
            "\t\t/* and not accessed at all */",
            "\t\t.min_nr_accesses = 0,",
            "\t\t.max_nr_accesses = 0,",
            "\t\t/* for min_age or more micro-seconds */",
            "\t\t.min_age_region = min_age /",
            "\t\t\tdamon_reclaim_mon_attrs.aggr_interval,",
            "\t\t.max_age_region = UINT_MAX,",
            "\t};",
            "",
            "\treturn damon_new_scheme(",
            "\t\t\t&pattern,",
            "\t\t\t/* page out those, as soon as found */",
            "\t\t\tDAMOS_PAGEOUT,",
            "\t\t\t/* for each aggregation interval */",
            "\t\t\t0,",
            "\t\t\t/* under the quota. */",
            "\t\t\t&damon_reclaim_quota,",
            "\t\t\t/* (De)activate this according to the watermarks. */",
            "\t\t\t&damon_reclaim_wmarks);",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义DAMON_RECLAIM模块的全局参数和配置，包括启用状态、冷内存识别时间阈值、配额限制、水印条件及监控属性，用于控制基于DAMON的页面回收行为。",
          "similarity": 0.533146858215332
        },
        {
          "chunk_id": 2,
          "file_path": "mm/damon/reclaim.c",
          "start_line": 269,
          "end_line": 298,
          "content": [
            "static int damon_reclaim_after_aggregation(struct damon_ctx *c)",
            "{",
            "\tstruct damos *s;",
            "",
            "\t/* update the stats parameter */",
            "\tdamon_for_each_scheme(s, c)",
            "\t\tdamon_reclaim_stat = s->stat;",
            "",
            "\treturn damon_reclaim_handle_commit_inputs();",
            "}",
            "static int damon_reclaim_after_wmarks_check(struct damon_ctx *c)",
            "{",
            "\treturn damon_reclaim_handle_commit_inputs();",
            "}",
            "static int __init damon_reclaim_init(void)",
            "{",
            "\tint err = damon_modules_new_paddr_ctx_target(&ctx, &target);",
            "",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tctx->callback.after_wmarks_check = damon_reclaim_after_wmarks_check;",
            "\tctx->callback.after_aggregation = damon_reclaim_after_aggregation;",
            "",
            "\t/* 'enabled' has set before this function, probably via command line */",
            "\tif (enabled)",
            "\t\terr = damon_reclaim_turn(true);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "damon_reclaim_after_aggregation, damon_reclaim_after_wmarks_check, damon_reclaim_init",
          "description": "注册DAMON框架的回调函数以实现回收策略的动态调整，初始化阶段绑定自定义回调至上下文，确保在监控周期关键节点触发参数重载和回收策略更新。",
          "similarity": 0.47425493597984314
        }
      ]
    },
    {
      "source_file": "mm/vmscan.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:33:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmscan.c`\n\n---\n\n# vmscan.c 技术文档\n\n## 1. 文件概述\n\n`vmscan.c` 是 Linux 内核内存管理子系统中的核心文件，主要负责**页面回收（page reclaim）**机制的实现。该文件实现了内核在内存压力下如何选择并释放不再活跃或可回收的物理页帧（pages），以维持系统可用内存水位。其核心功能包括：\n\n- 实现 `kswapd` 内核线程，用于后台异步回收内存\n- 提供直接回收（direct reclaim）路径，供分配器在内存不足时同步触发\n- 管理匿名页（anonymous pages）和文件缓存页（file-backed pages）的回收策略\n- 支持基于内存控制组（memcg）的层级化内存回收\n- 与交换（swap）、压缩（compaction）、OOM killer 等子系统协同工作\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct scan_control`**  \n  页面回收上下文控制结构，包含本次回收操作的所有参数和状态：\n  - `nr_to_reclaim`：目标回收页数\n  - `target_mem_cgroup`：目标内存 cgroup（用于 memcg 回收）\n  - `may_unmap` / `may_swap` / `may_writepage`：控制是否允许解除映射、交换、写回\n  - `priority`：扫描优先级（0~12，值越低压力越大）\n  - `order`：请求分配的阶数（影响回收激进程度）\n  - `nr_scanned` / `nr_reclaimed`：已扫描和已回收页数统计\n  - `anon_cost` / `file_cost`：用于平衡匿名页与文件页回收比例\n\n- **全局变量**\n  - `vm_swappiness`（默认 60）：控制系统倾向于回收匿名页（需 swap）还是文件页（可丢弃）\n\n### 主要函数（部分在代码片段中体现）\n\n- `cgroup_reclaim()` / `root_reclaim()`：判断当前回收是否针对特定 memcg 或全局\n- `writeback_throttling_sane()`：判断是否可使用标准脏页限流机制\n- `set_task_reclaim_state()` / `flush_reclaim_state()`：管理任务的 slab 回收状态\n- （注：核心回收函数如 `shrink_lruvec()`、`kswapd()` 等未在片段中展示）\n\n## 3. 关键实现\n\n### 内存回收控制逻辑\n\n- **回收目标决策**：通过 `scan_control` 结构传递回收上下文，区分直接回收（分配失败触发）与 kswapd 后台回收。\n- **LRU 链管理**：利用 `prefetchw_prev_lru_folio` 宏优化 LRU 链遍历时的 CPU 缓存预取性能。\n- **Memcg 集成**：\n  - 若 `target_mem_cgroup` 非空，则优先回收该 cgroup 的内存\n  - 支持 `memory.low` 保护机制：当常规回收无法满足需求且跳过受保护 cgroup 时，会触发二次强制回收（`memcg_low_reclaim`）\n- **脏页处理策略**：\n  - 在传统 memcg 模式下，禁用标准 `balance_dirty_pages()` 限流，改用直接阻塞回收（`writeback_throttling_sane()` 判断）\n  - 通过 `may_writepage` 控制是否在 laptop mode 下批量写回脏页\n\n### 回收统计与状态同步\n\n- **Slab 回收计数**：通过 `reclaim_state` 结构将非 LRU 回收（如 slab 释放）计入全局统计，但**仅在全局回收时计入**，避免 memcg 回收时高估实际效果导致欠回收。\n- **PSI/Trace 集成**：包含 `<trace/events/vmscan.h>` 用于性能分析，支持压力状态指示器（PSI）监控内存压力。\n\n## 4. 依赖关系\n\n### 头文件依赖\n\n- **核心内存管理**：`<linux/mm.h>`, `<linux/gfp.h>`, `<linux/swap.h>`, `<linux/vmstat.h>`\n- **LRU 与反向映射**：`<linux/rmap.h>`, `<linux/pagemap.h>`\n- **内存控制组**：`<linux/memcontrol.h>`\n- **IO 与写回**：`<linux/writeback.h>`, `<linux/backing-dev.h>`\n- **压缩与迁移**：`<linux/compaction.h>`, `<linux/migrate.h>`\n- **体系结构相关**：`<asm/tlbflush.h>`\n\n### 子系统交互\n\n- **Swap 子系统**：通过 `swapops.h` 和 `swap.h` 实现匿名页换出\n- **Slab 分配器**：通过 `reclaim_state` 接收 slab 回收通知\n- **OOM Killer**：当回收无法释放足够内存时触发\n- **Khugepaged**：大页合并/拆分与回收协同\n- **Memory Tiering**：支持分层内存架构中的页降级（demotion）控制\n\n## 5. 使用场景\n\n- **内存分配失败时的直接回收**：当 `alloc_pages()` 等分配函数无法满足请求时，同步调用回收路径。\n- **kswapd 后台回收**：当空闲内存低于 `watermark[low]` 时，唤醒 `kswapd` 线程异步回收至 `watermark[high]`。\n- **Memcg 内存超限时的层级回收**：当某个 cgroup 超过其内存限制时，仅回收该 cgroup 及其子树的页面。\n- **系统休眠（Hibernation）**：通过 `hibernation_mode` 标志优化休眠过程中的内存回收。\n- **主动内存回收（Proactive Reclaim）**：用户空间通过 `memory.reclaim` 接口触发预清回收。\n- **内存压缩准备**：当 `compaction_ready` 置位时，回收操作会为后续内存压缩腾出连续空间。",
      "similarity": 0.5961707234382629,
      "chunks": [
        {
          "chunk_id": 44,
          "file_path": "mm/vmscan.c",
          "start_line": 7321,
          "end_line": 7463,
          "content": [
            "static int __init kswapd_init(void)",
            "{",
            "\tint nid;",
            "",
            "\tswap_setup();",
            "\tfor_each_node_state(nid, N_MEMORY)",
            " \t\tkswapd_run(nid);",
            "\treturn 0;",
            "}",
            "static inline unsigned long node_unmapped_file_pages(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long file_mapped = node_page_state(pgdat, NR_FILE_MAPPED);",
            "\tunsigned long file_lru = node_page_state(pgdat, NR_INACTIVE_FILE) +",
            "\t\tnode_page_state(pgdat, NR_ACTIVE_FILE);",
            "",
            "\t/*",
            "\t * It's possible for there to be more file mapped pages than",
            "\t * accounted for by the pages on the file LRU lists because",
            "\t * tmpfs pages accounted for as ANON can also be FILE_MAPPED",
            "\t */",
            "\treturn (file_lru > file_mapped) ? (file_lru - file_mapped) : 0;",
            "}",
            "static unsigned long node_pagecache_reclaimable(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long nr_pagecache_reclaimable;",
            "\tunsigned long delta = 0;",
            "",
            "\t/*",
            "\t * If RECLAIM_UNMAP is set, then all file pages are considered",
            "\t * potentially reclaimable. Otherwise, we have to worry about",
            "\t * pages like swapcache and node_unmapped_file_pages() provides",
            "\t * a better estimate",
            "\t */",
            "\tif (node_reclaim_mode & RECLAIM_UNMAP)",
            "\t\tnr_pagecache_reclaimable = node_page_state(pgdat, NR_FILE_PAGES);",
            "\telse",
            "\t\tnr_pagecache_reclaimable = node_unmapped_file_pages(pgdat);",
            "",
            "\t/* If we can't clean pages, remove dirty pages from consideration */",
            "\tif (!(node_reclaim_mode & RECLAIM_WRITE))",
            "\t\tdelta += node_page_state(pgdat, NR_FILE_DIRTY);",
            "",
            "\t/* Watch for any possible underflows due to delta */",
            "\tif (unlikely(delta > nr_pagecache_reclaimable))",
            "\t\tdelta = nr_pagecache_reclaimable;",
            "",
            "\treturn nr_pagecache_reclaimable - delta;",
            "}",
            "static int __node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)",
            "{",
            "\t/* Minimum pages needed in order to stay on node */",
            "\tconst unsigned long nr_pages = 1 << order;",
            "\tstruct task_struct *p = current;",
            "\tunsigned int noreclaim_flag;",
            "\tstruct scan_control sc = {",
            "\t\t.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),",
            "\t\t.gfp_mask = current_gfp_context(gfp_mask),",
            "\t\t.order = order,",
            "\t\t.priority = NODE_RECLAIM_PRIORITY,",
            "\t\t.may_writepage = !!(node_reclaim_mode & RECLAIM_WRITE),",
            "\t\t.may_unmap = !!(node_reclaim_mode & RECLAIM_UNMAP),",
            "\t\t.may_swap = 1,",
            "\t\t.reclaim_idx = gfp_zone(gfp_mask),",
            "\t};",
            "\tunsigned long pflags;",
            "",
            "\ttrace_mm_vmscan_node_reclaim_begin(pgdat->node_id, order,",
            "\t\t\t\t\t   sc.gfp_mask);",
            "",
            "\tcond_resched();",
            "\tpsi_memstall_enter(&pflags);",
            "\tfs_reclaim_acquire(sc.gfp_mask);",
            "\t/*",
            "\t * We need to be able to allocate from the reserves for RECLAIM_UNMAP",
            "\t */",
            "\tnoreclaim_flag = memalloc_noreclaim_save();",
            "\tset_task_reclaim_state(p, &sc.reclaim_state);",
            "",
            "\tif (node_pagecache_reclaimable(pgdat) > pgdat->min_unmapped_pages ||",
            "\t    node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) > pgdat->min_slab_pages) {",
            "\t\t/*",
            "\t\t * Free memory by calling shrink node with increasing",
            "\t\t * priorities until we have enough memory freed.",
            "\t\t */",
            "\t\tdo {",
            "\t\t\tshrink_node(pgdat, &sc);",
            "\t\t} while (sc.nr_reclaimed < nr_pages && --sc.priority >= 0);",
            "\t}",
            "",
            "\tset_task_reclaim_state(p, NULL);",
            "\tmemalloc_noreclaim_restore(noreclaim_flag);",
            "\tfs_reclaim_release(sc.gfp_mask);",
            "\tpsi_memstall_leave(&pflags);",
            "",
            "\ttrace_mm_vmscan_node_reclaim_end(sc.nr_reclaimed);",
            "",
            "\treturn sc.nr_reclaimed >= nr_pages;",
            "}",
            "int node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)",
            "{",
            "\tint ret;",
            "",
            "\t/*",
            "\t * Node reclaim reclaims unmapped file backed pages and",
            "\t * slab pages if we are over the defined limits.",
            "\t *",
            "\t * A small portion of unmapped file backed pages is needed for",
            "\t * file I/O otherwise pages read by file I/O will be immediately",
            "\t * thrown out if the node is overallocated. So we do not reclaim",
            "\t * if less than a specified percentage of the node is used by",
            "\t * unmapped file backed pages.",
            "\t */",
            "\tif (node_pagecache_reclaimable(pgdat) <= pgdat->min_unmapped_pages &&",
            "\t    node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) <=",
            "\t    pgdat->min_slab_pages)",
            "\t\treturn NODE_RECLAIM_FULL;",
            "",
            "\t/*",
            "\t * Do not scan if the allocation should not be delayed.",
            "\t */",
            "\tif (!gfpflags_allow_blocking(gfp_mask) || (current->flags & PF_MEMALLOC))",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\t/*",
            "\t * Only run node reclaim on the local node or on nodes that do not",
            "\t * have associated processors. This will favor the local processor",
            "\t * over remote processors and spread off node memory allocations",
            "\t * as wide as possible.",
            "\t */",
            "\tif (node_state(pgdat->node_id, N_CPU) && pgdat->node_id != numa_node_id())",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\tif (test_and_set_bit(PGDAT_RECLAIM_LOCKED, &pgdat->flags))",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\tret = __node_reclaim(pgdat, gfp_mask, order);",
            "\tclear_bit_unlock(PGDAT_RECLAIM_LOCKED, &pgdat->flags);",
            "",
            "\tif (!ret)",
            "\t\tcount_vm_event(PGSCAN_ZONE_RECLAIM_FAILED);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "kswapd_init, node_unmapped_file_pages, node_pagecache_reclaimable, __node_reclaim, node_reclaim",
          "description": "实现节点级别内存回收策略，kswapd_init初始化kswapd线程，node_unmapped_file_pages计算未映射文件页，node_pagecache_reclaimable评估可回收页面数量，__node_reclaim执行节点级回收，node_reclaim决定是否触发回收操作。",
          "similarity": 0.6501941680908203
        },
        {
          "chunk_id": 41,
          "file_path": "mm/vmscan.c",
          "start_line": 6709,
          "end_line": 6981,
          "content": [
            "static bool kswapd_shrink_node(pg_data_t *pgdat,",
            "\t\t\t       struct scan_control *sc)",
            "{",
            "\tstruct zone *zone;",
            "\tint z;",
            "\tunsigned long nr_reclaimed = sc->nr_reclaimed;",
            "",
            "\t/* Reclaim a number of pages proportional to the number of zones */",
            "\tsc->nr_to_reclaim = 0;",
            "\tfor (z = 0; z <= sc->reclaim_idx; z++) {",
            "\t\tzone = pgdat->node_zones + z;",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tsc->nr_to_reclaim += max(high_wmark_pages(zone), SWAP_CLUSTER_MAX);",
            "\t}",
            "",
            "\t/*",
            "\t * Historically care was taken to put equal pressure on all zones but",
            "\t * now pressure is applied based on node LRU order.",
            "\t */",
            "\tshrink_node(pgdat, sc);",
            "",
            "\t/*",
            "\t * Fragmentation may mean that the system cannot be rebalanced for",
            "\t * high-order allocations. If twice the allocation size has been",
            "\t * reclaimed then recheck watermarks only at order-0 to prevent",
            "\t * excessive reclaim. Assume that a process requested a high-order",
            "\t * can direct reclaim/compact.",
            "\t */",
            "\tif (sc->order && sc->nr_reclaimed >= compact_gap(sc->order))",
            "\t\tsc->order = 0;",
            "",
            "\t/* account for progress from mm_account_reclaimed_pages() */",
            "\treturn max(sc->nr_scanned, sc->nr_reclaimed - nr_reclaimed) >= sc->nr_to_reclaim;",
            "}",
            "static inline void",
            "update_reclaim_active(pg_data_t *pgdat, int highest_zoneidx, bool active)",
            "{",
            "\tint i;",
            "\tstruct zone *zone;",
            "",
            "\tfor (i = 0; i <= highest_zoneidx; i++) {",
            "\t\tzone = pgdat->node_zones + i;",
            "",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (active)",
            "\t\t\tset_bit(ZONE_RECLAIM_ACTIVE, &zone->flags);",
            "\t\telse",
            "\t\t\tclear_bit(ZONE_RECLAIM_ACTIVE, &zone->flags);",
            "\t}",
            "}",
            "static inline void",
            "set_reclaim_active(pg_data_t *pgdat, int highest_zoneidx)",
            "{",
            "\tupdate_reclaim_active(pgdat, highest_zoneidx, true);",
            "}",
            "static inline void",
            "clear_reclaim_active(pg_data_t *pgdat, int highest_zoneidx)",
            "{",
            "\tupdate_reclaim_active(pgdat, highest_zoneidx, false);",
            "}",
            "static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)",
            "{",
            "\tint i;",
            "\tunsigned long nr_soft_reclaimed;",
            "\tunsigned long nr_soft_scanned;",
            "\tunsigned long pflags;",
            "\tunsigned long nr_boost_reclaim;",
            "\tunsigned long zone_boosts[MAX_NR_ZONES] = { 0, };",
            "\tbool boosted;",
            "\tstruct zone *zone;",
            "\tstruct scan_control sc = {",
            "\t\t.gfp_mask = GFP_KERNEL,",
            "\t\t.order = order,",
            "\t\t.may_unmap = 1,",
            "\t};",
            "",
            "\tset_task_reclaim_state(current, &sc.reclaim_state);",
            "\tpsi_memstall_enter(&pflags);",
            "\t__fs_reclaim_acquire(_THIS_IP_);",
            "",
            "\tcount_vm_event(PAGEOUTRUN);",
            "",
            "\t/*",
            "\t * Account for the reclaim boost. Note that the zone boost is left in",
            "\t * place so that parallel allocations that are near the watermark will",
            "\t * stall or direct reclaim until kswapd is finished.",
            "\t */",
            "\tnr_boost_reclaim = 0;",
            "\tfor (i = 0; i <= highest_zoneidx; i++) {",
            "\t\tzone = pgdat->node_zones + i;",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tnr_boost_reclaim += zone->watermark_boost;",
            "\t\tzone_boosts[i] = zone->watermark_boost;",
            "\t}",
            "\tboosted = nr_boost_reclaim;",
            "",
            "restart:",
            "\tset_reclaim_active(pgdat, highest_zoneidx);",
            "\tsc.priority = DEF_PRIORITY;",
            "\tdo {",
            "\t\tunsigned long nr_reclaimed = sc.nr_reclaimed;",
            "\t\tbool raise_priority = true;",
            "\t\tbool balanced;",
            "\t\tbool ret;",
            "",
            "\t\tsc.reclaim_idx = highest_zoneidx;",
            "",
            "\t\t/*",
            "\t\t * If the number of buffer_heads exceeds the maximum allowed",
            "\t\t * then consider reclaiming from all zones. This has a dual",
            "\t\t * purpose -- on 64-bit systems it is expected that",
            "\t\t * buffer_heads are stripped during active rotation. On 32-bit",
            "\t\t * systems, highmem pages can pin lowmem memory and shrinking",
            "\t\t * buffers can relieve lowmem pressure. Reclaim may still not",
            "\t\t * go ahead if all eligible zones for the original allocation",
            "\t\t * request are balanced to avoid excessive reclaim from kswapd.",
            "\t\t */",
            "\t\tif (buffer_heads_over_limit) {",
            "\t\t\tfor (i = MAX_NR_ZONES - 1; i >= 0; i--) {",
            "\t\t\t\tzone = pgdat->node_zones + i;",
            "\t\t\t\tif (!managed_zone(zone))",
            "\t\t\t\t\tcontinue;",
            "",
            "\t\t\t\tsc.reclaim_idx = i;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If the pgdat is imbalanced then ignore boosting and preserve",
            "\t\t * the watermarks for a later time and restart. Note that the",
            "\t\t * zone watermarks will be still reset at the end of balancing",
            "\t\t * on the grounds that the normal reclaim should be enough to",
            "\t\t * re-evaluate if boosting is required when kswapd next wakes.",
            "\t\t */",
            "\t\tbalanced = pgdat_balanced(pgdat, sc.order, highest_zoneidx);",
            "\t\tif (!balanced && nr_boost_reclaim) {",
            "\t\t\tnr_boost_reclaim = 0;",
            "\t\t\tgoto restart;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If boosting is not active then only reclaim if there are no",
            "\t\t * eligible zones. Note that sc.reclaim_idx is not used as",
            "\t\t * buffer_heads_over_limit may have adjusted it.",
            "\t\t */",
            "\t\tif (!nr_boost_reclaim && balanced)",
            "\t\t\tgoto out;",
            "",
            "\t\t/* Limit the priority of boosting to avoid reclaim writeback */",
            "\t\tif (nr_boost_reclaim && sc.priority == DEF_PRIORITY - 2)",
            "\t\t\traise_priority = false;",
            "",
            "\t\t/*",
            "\t\t * Do not writeback or swap pages for boosted reclaim. The",
            "\t\t * intent is to relieve pressure not issue sub-optimal IO",
            "\t\t * from reclaim context. If no pages are reclaimed, the",
            "\t\t * reclaim will be aborted.",
            "\t\t */",
            "\t\tsc.may_writepage = !laptop_mode && !nr_boost_reclaim;",
            "\t\tsc.may_swap = !nr_boost_reclaim;",
            "",
            "\t\t/*",
            "\t\t * Do some background aging, to give pages a chance to be",
            "\t\t * referenced before reclaiming. All pages are rotated",
            "\t\t * regardless of classzone as this is about consistent aging.",
            "\t\t */",
            "\t\tkswapd_age_node(pgdat, &sc);",
            "",
            "\t\t/*",
            "\t\t * If we're getting trouble reclaiming, start doing writepage",
            "\t\t * even in laptop mode.",
            "\t\t */",
            "\t\tif (sc.priority < DEF_PRIORITY - 2)",
            "\t\t\tsc.may_writepage = 1;",
            "",
            "\t\t/* Call soft limit reclaim before calling shrink_node. */",
            "\t\tsc.nr_scanned = 0;",
            "\t\tnr_soft_scanned = 0;",
            "\t\tnr_soft_reclaimed = memcg1_soft_limit_reclaim(pgdat, sc.order,",
            "\t\t\t\t\t\t\t      sc.gfp_mask, &nr_soft_scanned);",
            "\t\tsc.nr_reclaimed += nr_soft_reclaimed;",
            "",
            "\t\t/*",
            "\t\t * There should be no need to raise the scanning priority if",
            "\t\t * enough pages are already being scanned that that high",
            "\t\t * watermark would be met at 100% efficiency.",
            "\t\t */",
            "\t\tif (kswapd_shrink_node(pgdat, &sc))",
            "\t\t\traise_priority = false;",
            "",
            "\t\t/*",
            "\t\t * If the low watermark is met there is no need for processes",
            "\t\t * to be throttled on pfmemalloc_wait as they should not be",
            "\t\t * able to safely make forward progress. Wake them",
            "\t\t */",
            "\t\tif (waitqueue_active(&pgdat->pfmemalloc_wait) &&",
            "\t\t\t\tallow_direct_reclaim(pgdat))",
            "\t\t\twake_up_all(&pgdat->pfmemalloc_wait);",
            "",
            "\t\t/* Check if kswapd should be suspending */",
            "\t\t__fs_reclaim_release(_THIS_IP_);",
            "\t\tret = try_to_freeze();",
            "\t\t__fs_reclaim_acquire(_THIS_IP_);",
            "\t\tif (ret || kthread_should_stop())",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * Raise priority if scanning rate is too low or there was no",
            "\t\t * progress in reclaiming pages",
            "\t\t */",
            "\t\tnr_reclaimed = sc.nr_reclaimed - nr_reclaimed;",
            "\t\tnr_boost_reclaim -= min(nr_boost_reclaim, nr_reclaimed);",
            "",
            "\t\t/*",
            "\t\t * If reclaim made no progress for a boost, stop reclaim as",
            "\t\t * IO cannot be queued and it could be an infinite loop in",
            "\t\t * extreme circumstances.",
            "\t\t */",
            "\t\tif (nr_boost_reclaim && !nr_reclaimed)",
            "\t\t\tbreak;",
            "",
            "\t\tif (raise_priority || !nr_reclaimed)",
            "\t\t\tsc.priority--;",
            "\t} while (sc.priority >= 1);",
            "",
            "\tif (!sc.nr_reclaimed)",
            "\t\tpgdat->kswapd_failures++;",
            "",
            "out:",
            "\tclear_reclaim_active(pgdat, highest_zoneidx);",
            "",
            "\t/* If reclaim was boosted, account for the reclaim done in this pass */",
            "\tif (boosted) {",
            "\t\tunsigned long flags;",
            "",
            "\t\tfor (i = 0; i <= highest_zoneidx; i++) {",
            "\t\t\tif (!zone_boosts[i])",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\t/* Increments are under the zone lock */",
            "\t\t\tzone = pgdat->node_zones + i;",
            "\t\t\tspin_lock_irqsave(&zone->lock, flags);",
            "\t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);",
            "\t\t\tspin_unlock_irqrestore(&zone->lock, flags);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * As there is now likely space, wakeup kcompact to defragment",
            "\t\t * pageblocks.",
            "\t\t */",
            "\t\twakeup_kcompactd(pgdat, pageblock_order, highest_zoneidx);",
            "\t}",
            "",
            "\tsnapshot_refaults(NULL, pgdat);",
            "\t__fs_reclaim_release(_THIS_IP_);",
            "\tpsi_memstall_leave(&pflags);",
            "\tset_task_reclaim_state(current, NULL);",
            "",
            "\t/*",
            "\t * Return the order kswapd stopped reclaiming at as",
            "\t * prepare_kswapd_sleep() takes it into account. If another caller",
            "\t * entered the allocator slow path while kswapd was awake, order will",
            "\t * remain at the higher level.",
            "\t */",
            "\treturn sc.order;",
            "}"
          ],
          "function_name": "kswapd_shrink_node, update_reclaim_active, set_reclaim_active, clear_reclaim_active, balance_pgdat",
          "description": "实现节点级内存回收逻辑，kswapd_shrink_node按比例回收页面，update_reclaim_active/set_reclaim_active/clear_reclaim_active管理重分配标志位，balance_pgdat执行多阶段内存回收，包含软限制回收、优先级调整和写回控制。",
          "similarity": 0.6113972067832947
        },
        {
          "chunk_id": 34,
          "file_path": "mm/vmscan.c",
          "start_line": 5787,
          "end_line": 5903,
          "content": [
            "static bool in_reclaim_compaction(struct scan_control *sc)",
            "{",
            "\tif (gfp_compaction_allowed(sc->gfp_mask) && sc->order &&",
            "\t\t\t(sc->order > PAGE_ALLOC_COSTLY_ORDER ||",
            "\t\t\t sc->priority < DEF_PRIORITY - 2))",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "static inline bool should_continue_reclaim(struct pglist_data *pgdat,",
            "\t\t\t\t\tunsigned long nr_reclaimed,",
            "\t\t\t\t\tstruct scan_control *sc)",
            "{",
            "\tunsigned long pages_for_compaction;",
            "\tunsigned long inactive_lru_pages;",
            "\tint z;",
            "",
            "\t/* If not in reclaim/compaction mode, stop */",
            "\tif (!in_reclaim_compaction(sc))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Stop if we failed to reclaim any pages from the last SWAP_CLUSTER_MAX",
            "\t * number of pages that were scanned. This will return to the caller",
            "\t * with the risk reclaim/compaction and the resulting allocation attempt",
            "\t * fails. In the past we have tried harder for __GFP_RETRY_MAYFAIL",
            "\t * allocations through requiring that the full LRU list has been scanned",
            "\t * first, by assuming that zero delta of sc->nr_scanned means full LRU",
            "\t * scan, but that approximation was wrong, and there were corner cases",
            "\t * where always a non-zero amount of pages were scanned.",
            "\t */",
            "\tif (!nr_reclaimed)",
            "\t\treturn false;",
            "",
            "\t/* If compaction would go ahead or the allocation would succeed, stop */",
            "\tfor (z = 0; z <= sc->reclaim_idx; z++) {",
            "\t\tstruct zone *zone = &pgdat->node_zones[z];",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Allocation can already succeed, nothing to do */",
            "\t\tif (zone_watermark_ok(zone, sc->order, min_wmark_pages(zone),",
            "\t\t\t\t      sc->reclaim_idx, 0))",
            "\t\t\treturn false;",
            "",
            "\t\tif (compaction_suitable(zone, sc->order, sc->reclaim_idx))",
            "\t\t\treturn false;",
            "\t}",
            "",
            "\t/*",
            "\t * If we have not reclaimed enough pages for compaction and the",
            "\t * inactive lists are large enough, continue reclaiming",
            "\t */",
            "\tpages_for_compaction = compact_gap(sc->order);",
            "\tinactive_lru_pages = node_page_state(pgdat, NR_INACTIVE_FILE);",
            "\tif (can_reclaim_anon_pages(NULL, pgdat->node_id, sc))",
            "\t\tinactive_lru_pages += node_page_state(pgdat, NR_INACTIVE_ANON);",
            "",
            "\treturn inactive_lru_pages > pages_for_compaction;",
            "}",
            "static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)",
            "{",
            "\tstruct mem_cgroup *target_memcg = sc->target_mem_cgroup;",
            "\tstruct mem_cgroup *memcg;",
            "",
            "\tmemcg = mem_cgroup_iter(target_memcg, NULL, NULL);",
            "\tdo {",
            "\t\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "\t\tunsigned long reclaimed;",
            "\t\tunsigned long scanned;",
            "",
            "\t\t/*",
            "\t\t * This loop can become CPU-bound when target memcgs",
            "\t\t * aren't eligible for reclaim - either because they",
            "\t\t * don't have any reclaimable pages, or because their",
            "\t\t * memory is explicitly protected. Avoid soft lockups.",
            "\t\t */",
            "\t\tcond_resched();",
            "",
            "\t\tmem_cgroup_calculate_protection(target_memcg, memcg);",
            "",
            "\t\tif (mem_cgroup_below_min(target_memcg, memcg)) {",
            "\t\t\t/*",
            "\t\t\t * Hard protection.",
            "\t\t\t * If there is no reclaimable memory, OOM.",
            "\t\t\t */",
            "\t\t\tcontinue;",
            "\t\t} else if (mem_cgroup_below_low(target_memcg, memcg)) {",
            "\t\t\t/*",
            "\t\t\t * Soft protection.",
            "\t\t\t * Respect the protection only as long as",
            "\t\t\t * there is an unprotected supply",
            "\t\t\t * of reclaimable memory from other cgroups.",
            "\t\t\t */",
            "\t\t\tif (!sc->memcg_low_reclaim) {",
            "\t\t\t\tsc->memcg_low_skipped = 1;",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\tmemcg_memory_event(memcg, MEMCG_LOW);",
            "\t\t}",
            "",
            "\t\treclaimed = sc->nr_reclaimed;",
            "\t\tscanned = sc->nr_scanned;",
            "",
            "\t\tshrink_lruvec(lruvec, sc);",
            "",
            "\t\tshrink_slab(sc->gfp_mask, pgdat->node_id, memcg,",
            "\t\t\t    sc->priority);",
            "",
            "\t\t/* Record the group's reclaim efficiency */",
            "\t\tif (!sc->proactive)",
            "\t\t\tvmpressure(sc->gfp_mask, memcg, false,",
            "\t\t\t\t   sc->nr_scanned - scanned,",
            "\t\t\t\t   sc->nr_reclaimed - reclaimed);",
            "",
            "\t} while ((memcg = mem_cgroup_iter(target_memcg, memcg, NULL)));",
            "}"
          ],
          "function_name": "in_reclaim_compaction, should_continue_reclaim, shrink_node_memcgs",
          "description": "in_reclaim_compaction检测是否需要进行内存紧缩操作。should_continue_reclaim判断是否需继续回收以满足紧缩需求。shrink_node_memcgs遍历目标内存控制组，执行LRU回收和slab收缩操作，并记录压力事件。",
          "similarity": 0.6048805713653564
        },
        {
          "chunk_id": 38,
          "file_path": "mm/vmscan.c",
          "start_line": 6310,
          "end_line": 6427,
          "content": [
            "static bool allow_direct_reclaim(pg_data_t *pgdat)",
            "{",
            "\tstruct zone *zone;",
            "\tunsigned long pfmemalloc_reserve = 0;",
            "\tunsigned long free_pages = 0;",
            "\tint i;",
            "\tbool wmark_ok;",
            "",
            "\tif (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)",
            "\t\treturn true;",
            "",
            "\tfor (i = 0; i <= ZONE_NORMAL; i++) {",
            "\t\tzone = &pgdat->node_zones[i];",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!zone_reclaimable_pages(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tpfmemalloc_reserve += min_wmark_pages(zone);",
            "\t\tfree_pages += zone_page_state_snapshot(zone, NR_FREE_PAGES);",
            "\t}",
            "",
            "\t/* If there are no reserves (unexpected config) then do not throttle */",
            "\tif (!pfmemalloc_reserve)",
            "\t\treturn true;",
            "",
            "\twmark_ok = free_pages > pfmemalloc_reserve / 2;",
            "",
            "\t/* kswapd must be awake if processes are being throttled */",
            "\tif (!wmark_ok && waitqueue_active(&pgdat->kswapd_wait)) {",
            "\t\tif (READ_ONCE(pgdat->kswapd_highest_zoneidx) > ZONE_NORMAL)",
            "\t\t\tWRITE_ONCE(pgdat->kswapd_highest_zoneidx, ZONE_NORMAL);",
            "",
            "\t\twake_up_interruptible(&pgdat->kswapd_wait);",
            "\t}",
            "",
            "\treturn wmark_ok;",
            "}",
            "static bool throttle_direct_reclaim(gfp_t gfp_mask, struct zonelist *zonelist,",
            "\t\t\t\t\tnodemask_t *nodemask)",
            "{",
            "\tstruct zoneref *z;",
            "\tstruct zone *zone;",
            "\tpg_data_t *pgdat = NULL;",
            "",
            "\t/*",
            "\t * Kernel threads should not be throttled as they may be indirectly",
            "\t * responsible for cleaning pages necessary for reclaim to make forward",
            "\t * progress. kjournald for example may enter direct reclaim while",
            "\t * committing a transaction where throttling it could forcing other",
            "\t * processes to block on log_wait_commit().",
            "\t */",
            "\tif (current->flags & PF_KTHREAD)",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * If a fatal signal is pending, this process should not throttle.",
            "\t * It should return quickly so it can exit and free its memory",
            "\t */",
            "\tif (fatal_signal_pending(current))",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * Check if the pfmemalloc reserves are ok by finding the first node",
            "\t * with a usable ZONE_NORMAL or lower zone. The expectation is that",
            "\t * GFP_KERNEL will be required for allocating network buffers when",
            "\t * swapping over the network so ZONE_HIGHMEM is unusable.",
            "\t *",
            "\t * Throttling is based on the first usable node and throttled processes",
            "\t * wait on a queue until kswapd makes progress and wakes them. There",
            "\t * is an affinity then between processes waking up and where reclaim",
            "\t * progress has been made assuming the process wakes on the same node.",
            "\t * More importantly, processes running on remote nodes will not compete",
            "\t * for remote pfmemalloc reserves and processes on different nodes",
            "\t * should make reasonable progress.",
            "\t */",
            "\tfor_each_zone_zonelist_nodemask(zone, z, zonelist,",
            "\t\t\t\t\tgfp_zone(gfp_mask), nodemask) {",
            "\t\tif (zone_idx(zone) > ZONE_NORMAL)",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Throttle based on the first usable node */",
            "\t\tpgdat = zone->zone_pgdat;",
            "\t\tif (allow_direct_reclaim(pgdat))",
            "\t\t\tgoto out;",
            "\t\tbreak;",
            "\t}",
            "",
            "\t/* If no zone was usable by the allocation flags then do not throttle */",
            "\tif (!pgdat)",
            "\t\tgoto out;",
            "",
            "\t/* Account for the throttling */",
            "\tcount_vm_event(PGSCAN_DIRECT_THROTTLE);",
            "",
            "\t/*",
            "\t * If the caller cannot enter the filesystem, it's possible that it",
            "\t * is due to the caller holding an FS lock or performing a journal",
            "\t * transaction in the case of a filesystem like ext[3|4]. In this case,",
            "\t * it is not safe to block on pfmemalloc_wait as kswapd could be",
            "\t * blocked waiting on the same lock. Instead, throttle for up to a",
            "\t * second before continuing.",
            "\t */",
            "\tif (!(gfp_mask & __GFP_FS))",
            "\t\twait_event_interruptible_timeout(pgdat->pfmemalloc_wait,",
            "\t\t\tallow_direct_reclaim(pgdat), HZ);",
            "\telse",
            "\t\t/* Throttle until kswapd wakes the process */",
            "\t\twait_event_killable(zone->zone_pgdat->pfmemalloc_wait,",
            "\t\t\tallow_direct_reclaim(pgdat));",
            "",
            "\tif (fatal_signal_pending(current))",
            "\t\treturn true;",
            "",
            "out:",
            "\treturn false;",
            "}"
          ],
          "function_name": "allow_direct_reclaim, throttle_direct_reclaim",
          "description": "allow_direct_recheck检查是否有足够的pfmemalloc预留空间，throttle_direct_reclaim对非内核线程实施节流，基于首个可用节点的状态唤醒kswapd等待队列，防止过度竞争高内存区域。",
          "similarity": 0.6012279987335205
        },
        {
          "chunk_id": 35,
          "file_path": "mm/vmscan.c",
          "start_line": 5914,
          "end_line": 6023,
          "content": [
            "static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)",
            "{",
            "\tunsigned long nr_reclaimed, nr_scanned, nr_node_reclaimed;",
            "\tstruct lruvec *target_lruvec;",
            "\tbool reclaimable = false;",
            "",
            "\tif (lru_gen_enabled() && root_reclaim(sc)) {",
            "\t\tlru_gen_shrink_node(pgdat, sc);",
            "\t\treturn;",
            "\t}",
            "",
            "\ttarget_lruvec = mem_cgroup_lruvec(sc->target_mem_cgroup, pgdat);",
            "",
            "again:",
            "\tmemset(&sc->nr, 0, sizeof(sc->nr));",
            "",
            "\tnr_reclaimed = sc->nr_reclaimed;",
            "\tnr_scanned = sc->nr_scanned;",
            "",
            "\tprepare_scan_control(pgdat, sc);",
            "",
            "\tshrink_node_memcgs(pgdat, sc);",
            "",
            "\tflush_reclaim_state(sc);",
            "",
            "\tnr_node_reclaimed = sc->nr_reclaimed - nr_reclaimed;",
            "",
            "\t/* Record the subtree's reclaim efficiency */",
            "\tif (!sc->proactive)",
            "\t\tvmpressure(sc->gfp_mask, sc->target_mem_cgroup, true,",
            "\t\t\t   sc->nr_scanned - nr_scanned, nr_node_reclaimed);",
            "",
            "\tif (nr_node_reclaimed)",
            "\t\treclaimable = true;",
            "",
            "\tif (current_is_kswapd()) {",
            "\t\t/*",
            "\t\t * If reclaim is isolating dirty pages under writeback,",
            "\t\t * it implies that the long-lived page allocation rate",
            "\t\t * is exceeding the page laundering rate. Either the",
            "\t\t * global limits are not being effective at throttling",
            "\t\t * processes due to the page distribution throughout",
            "\t\t * zones or there is heavy usage of a slow backing",
            "\t\t * device. The only option is to throttle from reclaim",
            "\t\t * context which is not ideal as there is no guarantee",
            "\t\t * the dirtying process is throttled in the same way",
            "\t\t * balance_dirty_pages() manages.",
            "\t\t *",
            "\t\t * Once a node is flagged PGDAT_WRITEBACK, kswapd will",
            "\t\t * count the number of pages under pages flagged for",
            "\t\t * immediate reclaim and stall if any are encountered",
            "\t\t * in the nr_immediate check below.",
            "\t\t */",
            "\t\tif (sc->nr.writeback && sc->nr.writeback == sc->nr.taken)",
            "\t\t\tset_bit(PGDAT_WRITEBACK, &pgdat->flags);",
            "",
            "\t\t/* Allow kswapd to start writing pages during reclaim.*/",
            "\t\tif (sc->nr.unqueued_dirty == sc->nr.file_taken)",
            "\t\t\tset_bit(PGDAT_DIRTY, &pgdat->flags);",
            "",
            "\t\t/*",
            "\t\t * If kswapd scans pages marked for immediate",
            "\t\t * reclaim and under writeback (nr_immediate), it",
            "\t\t * implies that pages are cycling through the LRU",
            "\t\t * faster than they are written so forcibly stall",
            "\t\t * until some pages complete writeback.",
            "\t\t */",
            "\t\tif (sc->nr.immediate)",
            "\t\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_WRITEBACK);",
            "\t}",
            "",
            "\t/*",
            "\t * Tag a node/memcg as congested if all the dirty pages were marked",
            "\t * for writeback and immediate reclaim (counted in nr.congested).",
            "\t *",
            "\t * Legacy memcg will stall in page writeback so avoid forcibly",
            "\t * stalling in reclaim_throttle().",
            "\t */",
            "\tif (sc->nr.dirty && sc->nr.dirty == sc->nr.congested) {",
            "\t\tif (cgroup_reclaim(sc) && writeback_throttling_sane(sc))",
            "\t\t\tset_bit(LRUVEC_CGROUP_CONGESTED, &target_lruvec->flags);",
            "",
            "\t\tif (current_is_kswapd())",
            "\t\t\tset_bit(LRUVEC_NODE_CONGESTED, &target_lruvec->flags);",
            "\t}",
            "",
            "\t/*",
            "\t * Stall direct reclaim for IO completions if the lruvec is",
            "\t * node is congested. Allow kswapd to continue until it",
            "\t * starts encountering unqueued dirty pages or cycling through",
            "\t * the LRU too quickly.",
            "\t */",
            "\tif (!current_is_kswapd() && current_may_throttle() &&",
            "\t    !sc->hibernation_mode &&",
            "\t    (test_bit(LRUVEC_CGROUP_CONGESTED, &target_lruvec->flags) ||",
            "\t     test_bit(LRUVEC_NODE_CONGESTED, &target_lruvec->flags)))",
            "\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_CONGESTED);",
            "",
            "\tif (should_continue_reclaim(pgdat, nr_node_reclaimed, sc))",
            "\t\tgoto again;",
            "",
            "\t/*",
            "\t * Kswapd gives up on balancing particular nodes after too",
            "\t * many failures to reclaim anything from them and goes to",
            "\t * sleep. On reclaim progress, reset the failure counter. A",
            "\t * successful direct reclaim run will revive a dormant kswapd.",
            "\t */",
            "\tif (reclaimable)",
            "\t\tpgdat->kswapd_failures = 0;",
            "}"
          ],
          "function_name": "shrink_node",
          "description": "该函数处理节点级别的内存回收，通过遍历各zone进行页面回收，并根据回收效率设置节点状态标志位，如PGDAT_WRITEBACK和CONGESTED。若当前为kswapd线程，则根据writeback和dirty页面状态触发节流机制。",
          "similarity": 0.562240481376648
        }
      ]
    },
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.5949285626411438,
      "chunks": [
        {
          "chunk_id": 11,
          "file_path": "mm/mempolicy.c",
          "start_line": 1855,
          "end_line": 1971,
          "content": [
            "static int kernel_get_mempolicy(int __user *policy,",
            "\t\t\t\tunsigned long __user *nmask,",
            "\t\t\t\tunsigned long maxnode,",
            "\t\t\t\tunsigned long addr,",
            "\t\t\t\tunsigned long flags)",
            "{",
            "\tint err;",
            "\tint pval;",
            "\tnodemask_t nodes;",
            "",
            "\tif (nmask != NULL && maxnode < nr_node_ids)",
            "\t\treturn -EINVAL;",
            "",
            "\taddr = untagged_addr(addr);",
            "",
            "\terr = do_get_mempolicy(&pval, &nodes, addr, flags);",
            "",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (policy && put_user(pval, policy))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (nmask)",
            "\t\terr = copy_nodes_to_user(nmask, maxnode, &nodes);",
            "",
            "\treturn err;",
            "}",
            "bool vma_migratable(struct vm_area_struct *vma)",
            "{",
            "\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * DAX device mappings require predictable access latency, so avoid",
            "\t * incurring periodic faults.",
            "\t */",
            "\tif (vma_is_dax(vma))",
            "\t\treturn false;",
            "",
            "\tif (is_vm_hugetlb_page(vma) &&",
            "\t\t!hugepage_migration_supported(hstate_vma(vma)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Migration allocates pages in the highest zone. If we cannot",
            "\t * do so then migration (at least from node to node) is not",
            "\t * possible.",
            "\t */",
            "\tif (vma->vm_file &&",
            "\t\tgfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))",
            "\t\t\t< policy_zone)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "bool vma_policy_mof(struct vm_area_struct *vma)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->get_policy) {",
            "\t\tbool ret = false;",
            "\t\tpgoff_t ilx;\t\t/* ignored here */",
            "",
            "\t\tpol = vma->vm_ops->get_policy(vma, vma->vm_start, &ilx);",
            "\t\tif (pol && (pol->flags & MPOL_F_MOF))",
            "\t\t\tret = true;",
            "\t\tmpol_cond_put(pol);",
            "",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tpol = vma->vm_policy;",
            "\tif (!pol)",
            "\t\tpol = get_task_policy(current);",
            "",
            "\treturn pol->flags & MPOL_F_MOF;",
            "}",
            "bool apply_policy_zone(struct mempolicy *policy, enum zone_type zone)",
            "{",
            "\tenum zone_type dynamic_policy_zone = policy_zone;",
            "",
            "\tBUG_ON(dynamic_policy_zone == ZONE_MOVABLE);",
            "",
            "\t/*",
            "\t * if policy->nodes has movable memory only,",
            "\t * we apply policy when gfp_zone(gfp) = ZONE_MOVABLE only.",
            "\t *",
            "\t * policy->nodes is intersect with node_states[N_MEMORY].",
            "\t * so if the following test fails, it implies",
            "\t * policy->nodes has movable memory only.",
            "\t */",
            "\tif (!nodes_intersects(policy->nodes, node_states[N_HIGH_MEMORY]))",
            "\t\tdynamic_policy_zone = ZONE_MOVABLE;",
            "",
            "\treturn zone >= dynamic_policy_zone;",
            "}",
            "static unsigned int weighted_interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int node;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "retry:",
            "\t/* to prevent miscount use tsk->mems_allowed_seq to detect rebind */",
            "\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\tnode = current->il_prev;",
            "\tif (!current->il_weight || !node_isset(node, policy->nodes)) {",
            "\t\tnode = next_node_in(node, policy->nodes);",
            "\t\tif (read_mems_allowed_retry(cpuset_mems_cookie))",
            "\t\t\tgoto retry;",
            "\t\tif (node == MAX_NUMNODES)",
            "\t\t\treturn node;",
            "\t\tcurrent->il_prev = node;",
            "\t\tcurrent->il_weight = get_il_weight(node);",
            "\t}",
            "\tcurrent->il_weight--;",
            "\treturn node;",
            "}"
          ],
          "function_name": "kernel_get_mempolicy, vma_migratable, vma_policy_mof, apply_policy_zone, weighted_interleave_nodes",
          "description": "kernel_get_mempolicy 获取当前内存策略参数并复制到用户空间；vma_migratable 判断虚拟内存区域是否支持迁移；vma_policy_mof 检查VMA是否启用了MOF（Migration On Fault）策略；apply_policy_zone 确定当前zone是否满足策略要求；weighted_interleave_nodes 计算加权交错分配的目标节点。",
          "similarity": 0.602172315120697
        },
        {
          "chunk_id": 12,
          "file_path": "mm/mempolicy.c",
          "start_line": 2024,
          "end_line": 2135,
          "content": [
            "static unsigned int interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int nid;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "\t/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnid = next_node_in(current->il_prev, policy->nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\tif (nid < MAX_NUMNODES)",
            "\t\tcurrent->il_prev = nid;",
            "\treturn nid;",
            "}",
            "unsigned int mempolicy_slab_node(void)",
            "{",
            "\tstruct mempolicy *policy;",
            "\tint node = numa_mem_id();",
            "",
            "\tif (!in_task())",
            "\t\treturn node;",
            "",
            "\tpolicy = current->mempolicy;",
            "\tif (!policy)",
            "\t\treturn node;",
            "",
            "\tswitch (policy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\treturn first_node(policy->nodes);",
            "",
            "\tcase MPOL_INTERLEAVE:",
            "\t\treturn interleave_nodes(policy);",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn weighted_interleave_nodes(policy);",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t{",
            "\t\tstruct zoneref *z;",
            "",
            "\t\t/*",
            "\t\t * Follow bind policy behavior and start allocation at the",
            "\t\t * first node.",
            "\t\t */",
            "\t\tstruct zonelist *zonelist;",
            "\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);",
            "\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];",
            "\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,",
            "\t\t\t\t\t\t\t&policy->nodes);",
            "\t\treturn z->zone ? zone_to_nid(z->zone) : node;",
            "\t}",
            "\tcase MPOL_LOCAL:",
            "\t\treturn node;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static unsigned int read_once_policy_nodemask(struct mempolicy *pol,",
            "\t\t\t\t\t      nodemask_t *mask)",
            "{",
            "\t/*",
            "\t * barrier stabilizes the nodemask locally so that it can be iterated",
            "\t * over safely without concern for changes. Allocators validate node",
            "\t * selection does not violate mems_allowed, so this is safe.",
            "\t */",
            "\tbarrier();",
            "\tmemcpy(mask, &pol->nodes, sizeof(nodemask_t));",
            "\tbarrier();",
            "\treturn nodes_weight(*mask);",
            "}",
            "static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nr_nodes;",
            "\tu8 *table = NULL;",
            "\tunsigned int weight_total = 0;",
            "\tu8 weight;",
            "\tint nid = 0;",
            "",
            "\tnr_nodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nr_nodes)",
            "\t\treturn numa_node_id();",
            "",
            "\trcu_read_lock();",
            "",
            "\tstate = rcu_dereference(wi_state);",
            "\t/* Uninitialized wi_state means we should assume all weights are 1 */",
            "\tif (state)",
            "\t\ttable = state->iw_table;",
            "",
            "\t/* calculate the total weight */",
            "\tfor_each_node_mask(nid, nodemask)",
            "\t\tweight_total += table ? table[nid] : 1;",
            "",
            "\t/* Calculate the node offset based on totals */",
            "\ttarget = ilx % weight_total;",
            "\tnid = first_node(nodemask);",
            "\twhile (target) {",
            "\t\t/* detect system default usage */",
            "\t\tweight = table ? table[nid] : 1;",
            "\t\tif (target < weight)",
            "\t\t\tbreak;",
            "\t\ttarget -= weight;",
            "\t\tnid = next_node_in(nid, nodemask);",
            "\t}",
            "\trcu_read_unlock();",
            "\treturn nid;",
            "}"
          ],
          "function_name": "interleave_nodes, mempolicy_slab_node, read_once_policy_nodemask, weighted_interleave_nid",
          "description": "interleave_nodes 计算交错分配的下一个节点；mempolicy_slab_node 根据内存策略返回Slab分配的节点；read_once_policy_nodemask 安全读取策略节点掩码；weighted_interleave_nid 基于权重计算加权交错分配的目标节点。",
          "similarity": 0.5932685136795044
        },
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.5832295417785645
        },
        {
          "chunk_id": 6,
          "file_path": "mm/mempolicy.c",
          "start_line": 1012,
          "end_line": 1144,
          "content": [
            "static void get_policy_nodemask(struct mempolicy *pol, nodemask_t *nodes)",
            "{",
            "\tnodes_clear(*nodes);",
            "\tif (pol == &default_policy)",
            "\t\treturn;",
            "",
            "\tswitch (pol->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*nodes = pol->nodes;",
            "\t\tbreak;",
            "\tcase MPOL_LOCAL:",
            "\t\t/* return empty node mask for local allocation */",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static int lookup_node(struct mm_struct *mm, unsigned long addr)",
            "{",
            "\tstruct page *p = NULL;",
            "\tint ret;",
            "",
            "\tret = get_user_pages_fast(addr & PAGE_MASK, 1, 0, &p);",
            "\tif (ret > 0) {",
            "\t\tret = page_to_nid(p);",
            "\t\tput_page(p);",
            "\t}",
            "\treturn ret;",
            "}",
            "static long do_get_mempolicy(int *policy, nodemask_t *nmask,",
            "\t\t\t     unsigned long addr, unsigned long flags)",
            "{",
            "\tint err;",
            "\tstruct mm_struct *mm = current->mm;",
            "\tstruct vm_area_struct *vma = NULL;",
            "\tstruct mempolicy *pol = current->mempolicy, *pol_refcount = NULL;",
            "",
            "\tif (flags &",
            "\t\t~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (flags & MPOL_F_MEMS_ALLOWED) {",
            "\t\tif (flags & (MPOL_F_NODE|MPOL_F_ADDR))",
            "\t\t\treturn -EINVAL;",
            "\t\t*policy = 0;\t/* just so it's initialized */",
            "\t\ttask_lock(current);",
            "\t\t*nmask  = cpuset_current_mems_allowed;",
            "\t\ttask_unlock(current);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (flags & MPOL_F_ADDR) {",
            "\t\tpgoff_t ilx;\t\t/* ignored here */",
            "\t\t/*",
            "\t\t * Do NOT fall back to task policy if the",
            "\t\t * vma/shared policy at addr is NULL.  We",
            "\t\t * want to return MPOL_DEFAULT in this case.",
            "\t\t */",
            "\t\tmmap_read_lock(mm);",
            "\t\tvma = vma_lookup(mm, addr);",
            "\t\tif (!vma) {",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\treturn -EFAULT;",
            "\t\t}",
            "\t\tpol = __get_vma_policy(vma, addr, &ilx);",
            "\t} else if (addr)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!pol)",
            "\t\tpol = &default_policy;\t/* indicates default behavior */",
            "",
            "\tif (flags & MPOL_F_NODE) {",
            "\t\tif (flags & MPOL_F_ADDR) {",
            "\t\t\t/*",
            "\t\t\t * Take a refcount on the mpol, because we are about to",
            "\t\t\t * drop the mmap_lock, after which only \"pol\" remains",
            "\t\t\t * valid, \"vma\" is stale.",
            "\t\t\t */",
            "\t\t\tpol_refcount = pol;",
            "\t\t\tvma = NULL;",
            "\t\t\tmpol_get(pol);",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\terr = lookup_node(mm, addr);",
            "\t\t\tif (err < 0)",
            "\t\t\t\tgoto out;",
            "\t\t\t*policy = err;",
            "\t\t} else if (pol == current->mempolicy &&",
            "\t\t\t\tpol->mode == MPOL_INTERLEAVE) {",
            "\t\t\t*policy = next_node_in(current->il_prev, pol->nodes);",
            "\t\t} else if (pol == current->mempolicy &&",
            "\t\t\t\tpol->mode == MPOL_WEIGHTED_INTERLEAVE) {",
            "\t\t\tif (current->il_weight)",
            "\t\t\t\t*policy = current->il_prev;",
            "\t\t\telse",
            "\t\t\t\t*policy = next_node_in(current->il_prev,",
            "\t\t\t\t\t\t       pol->nodes);",
            "\t\t} else {",
            "\t\t\terr = -EINVAL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t} else {",
            "\t\t*policy = pol == &default_policy ? MPOL_DEFAULT :",
            "\t\t\t\t\t\tpol->mode;",
            "\t\t/*",
            "\t\t * Internal mempolicy flags must be masked off before exposing",
            "\t\t * the policy to userspace.",
            "\t\t */",
            "\t\t*policy |= (pol->flags & MPOL_MODE_FLAGS);",
            "\t}",
            "",
            "\terr = 0;",
            "\tif (nmask) {",
            "\t\tif (mpol_store_user_nodemask(pol)) {",
            "\t\t\t*nmask = pol->w.user_nodemask;",
            "\t\t} else {",
            "\t\t\ttask_lock(current);",
            "\t\t\tget_policy_nodemask(pol, nmask);",
            "\t\t\ttask_unlock(current);",
            "\t\t}",
            "\t}",
            "",
            " out:",
            "\tmpol_cond_put(pol);",
            "\tif (vma)",
            "\t\tmmap_read_unlock(mm);",
            "\tif (pol_refcount)",
            "\t\tmpol_put(pol_refcount);",
            "\treturn err;",
            "}"
          ],
          "function_name": "get_policy_nodemask, lookup_node, do_get_mempolicy",
          "description": "获取内存策略节点掩码，lookup_node查询页面所属节点，do_get_mempolicy根据地址或标志获取当前/VA内存策略并返回节点掩码",
          "similarity": 0.574062168598175
        },
        {
          "chunk_id": 13,
          "file_path": "mm/mempolicy.c",
          "start_line": 2149,
          "end_line": 2255,
          "content": [
            "static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nnodes;",
            "\tint i;",
            "\tint nid;",
            "",
            "\tnnodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nnodes)",
            "\t\treturn numa_node_id();",
            "\ttarget = ilx % nnodes;",
            "\tnid = first_node(nodemask);",
            "\tfor (i = 0; i < target; i++)",
            "\t\tnid = next_node(nid, nodemask);",
            "\treturn nid;",
            "}",
            "int huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,",
            "\t\tstruct mempolicy **mpol, nodemask_t **nodemask)",
            "{",
            "\tpgoff_t ilx;",
            "\tint nid;",
            "",
            "\tnid = numa_node_id();",
            "\t*mpol = get_vma_policy(vma, addr, hstate_vma(vma)->order, &ilx);",
            "\t*nodemask = policy_nodemask(gfp_flags, *mpol, ilx, &nid);",
            "\treturn nid;",
            "}",
            "bool init_nodemask_of_mempolicy(nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "",
            "\tif (!(mask && current->mempolicy))",
            "\t\treturn false;",
            "",
            "\ttask_lock(current);",
            "\tmempolicy = current->mempolicy;",
            "\tswitch (mempolicy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*mask = mempolicy->nodes;",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tinit_nodemask_of_node(mask, numa_node_id());",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "\ttask_unlock(current);",
            "",
            "\treturn true;",
            "}",
            "bool mempolicy_in_oom_domain(struct task_struct *tsk,",
            "\t\t\t\t\tconst nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "\tbool ret = true;",
            "",
            "\tif (!mask)",
            "\t\treturn ret;",
            "",
            "\ttask_lock(tsk);",
            "\tmempolicy = tsk->mempolicy;",
            "\tif (mempolicy && mempolicy->mode == MPOL_BIND)",
            "\t\tret = nodes_intersects(mempolicy->nodes, *mask);",
            "\ttask_unlock(tsk);",
            "",
            "\treturn ret;",
            "}",
            "static unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tint nodes;",
            "\tunsigned long nr_pages_per_node;",
            "\tint delta;",
            "\tint i;",
            "\tunsigned long nr_allocated;",
            "\tunsigned long total_allocated = 0;",
            "",
            "\tnodes = nodes_weight(pol->nodes);",
            "\tnr_pages_per_node = nr_pages / nodes;",
            "\tdelta = nr_pages - nodes * nr_pages_per_node;",
            "",
            "\tfor (i = 0; i < nodes; i++) {",
            "\t\tif (delta) {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node + 1, NULL,",
            "\t\t\t\t\tpage_array);",
            "\t\t\tdelta--;",
            "\t\t} else {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node, NULL, page_array);",
            "\t\t}",
            "",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t}",
            "",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "interleave_nid, huge_node, init_nodemask_of_mempolicy, mempolicy_in_oom_domain, alloc_pages_bulk_array_interleave",
          "description": "interleave_nid 计算简单交错分配的目标节点；huge_node 结合HugeTLB策略确定大页分配节点；init_nodemask_of_mempolicy 初始化当前进程的内存策略节点掩码；mempolicy_in_oom_domain 检查策略节点是否与OOM域重叠；alloc_pages_bulk_array_interleave 执行批量交错分配。",
          "similarity": 0.5651946067810059
        }
      ]
    }
  ]
}