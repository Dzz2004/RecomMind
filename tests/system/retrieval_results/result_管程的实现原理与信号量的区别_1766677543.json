{
  "query": "管程的实现原理与信号量的区别",
  "timestamp": "2025-12-25 23:45:43",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/semaphore.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:52:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\semaphore.c`\n\n---\n\n# `locking/semaphore.c` 技术文档\n\n## 1. 文件概述\n\n`locking/semaphore.c` 实现了 Linux 内核中的**计数信号量（counting semaphore）**机制。计数信号量允许多个任务（最多为初始计数值）同时持有该锁，当计数值耗尽时，后续请求者将被阻塞，直到有其他任务释放信号量。与互斥锁（mutex）不同，信号量支持更灵活的并发控制，适用于资源池、限流等场景。该文件提供了多种获取和释放信号量的接口，包括可中断、可超时、不可中断等变体，并支持在中断上下文中调用部分函数。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `down(struct semaphore *sem)` | 不可中断地获取信号量，若不可用则睡眠。**已弃用**，建议使用可中断版本。 |\n| `down_interruptible(struct semaphore *sem)` | 可被普通信号中断的获取操作，成功返回 0，被信号中断返回 `-EINTR`。 |\n| `down_killable(struct semaphore *sem)` | 可被致命信号（fatal signal）中断的获取操作，返回值同上。 |\n| `down_trylock(struct semaphore *sem)` | 非阻塞尝试获取信号量，成功返回 0，失败返回 1（**注意返回值与 mutex/spinlock 相反**）。 |\n| `down_timeout(struct semaphore *sem, long timeout)` | 带超时的获取操作，超时返回 `-ETIME`，成功返回 0。 |\n| `up(struct semaphore *sem)` | 释放信号量，可由任意上下文（包括中断）调用，唤醒等待队列中的任务。 |\n\n### 静态辅助函数\n\n- `__down*()` 系列：处理信号量争用时的阻塞逻辑。\n- `__up()`：在有等待者时执行唤醒逻辑。\n- `___down_common()`：通用的阻塞等待实现，支持不同睡眠状态和超时。\n- `__sem_acquire()`：原子减少计数并记录持有者（用于 hung task 检测）。\n\n### 数据结构\n\n- `struct semaphore`（定义在 `<linux/semaphore.h>`）：\n  - `count`：当前可用资源数（>0 表示可立即获取）。\n  - `wait_list`：等待该信号量的任务链表。\n  - `lock`：保护上述成员的原始自旋锁（`raw_spinlock_t`）。\n  - `last_holder`（条件编译）：记录最后持有者，用于 `CONFIG_DETECT_HUNG_TASK_BLOCKER`。\n\n- `struct semaphore_waiter`：\n  - 用于将任务加入等待队列，包含任务指针和唤醒标志（`up`）。\n\n## 3. 关键实现\n\n### 中断安全与自旋锁\n- 所有对外接口（包括 `down*` 和 `up`）均使用 `raw_spin_lock_irqsave()` 获取自旋锁，确保在中断上下文安全。\n- 即使 `down()` 等函数通常在进程上下文调用，也使用 `irqsave` 变体，因为内核某些部分依赖在中断上下文成功调用 `down()`（当确定信号量可用时）。\n\n### 计数语义\n- `sem->count` 表示**还可被获取的次数**。初始值由 `sema_init()` 设置。\n- 获取时：若 `count > 0`，直接减 1；否则加入等待队列。\n- 释放时：若等待队列为空，`count++`；否则唤醒队首任务。\n\n### 等待与唤醒机制\n- 使用 `wake_q`（批量唤醒队列）优化唤醒路径，避免在持有自旋锁时调用 `wake_up_process()`。\n- 等待任务通过 `schedule_timeout()` 睡眠，并在循环中检查：\n  - 是否收到信号（根据睡眠状态判断）。\n  - 是否超时。\n  - 是否被 `__up()` 标记为 `waiter.up = true`（表示已被选中唤醒）。\n\n### Hung Task 支持\n- 当启用 `CONFIG_DETECT_HUNG_TASK_BLOCKER` 时：\n  - 获取信号量时记录当前任务为 `last_holder`。\n  - 释放时若当前任务是持有者，则清除记录。\n  - 提供 `sem_last_holder()` 供 hung task 检测模块查询阻塞源头。\n\n### 返回值约定\n- `down_trylock()` 返回 **0 表示成功**，**1 表示失败**，这与 `mutex_trylock()` 和 `spin_trylock()` **相反**，需特别注意。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/semaphore.h>`：信号量结构体和 API 声明。\n  - `<linux/spinlock.h>`：原始自旋锁实现。\n  - `<linux/sched.h>`、`<linux/sched/wake_q.h>`：任务调度和批量唤醒。\n  - `<trace/events/lock.h>`：锁争用跟踪点。\n  - `<linux/hung_task.h>`：hung task 检测支持。\n\n- **内核配置依赖**：\n  - `CONFIG_DETECT_HUNG_TASK_BLOCKER`：启用信号量持有者跟踪。\n\n- **与其他同步原语关系**：\n  - 与 `mutex.c` 形成对比：mutex 是二值、不可递归、带调试信息的互斥锁；信号量是计数、可被任意任务释放、更轻量。\n  - 底层依赖调度器（`schedule_timeout`）和中断管理（`irqsave`）。\n\n## 5. 使用场景\n\n- **资源池管理**：如限制同时访问某类硬件设备的任务数量。\n- **读写并发控制**：配合其他机制实现多读者/单写者模型。\n- **内核驱动**：设备驱动中控制对共享资源的并发访问。\n- **中断上下文释放**：因 `up()` 可在中断中调用，适用于中断处理程序释放资源的场景。\n- **不可睡眠路径**：使用 `down_trylock()` 在原子上下文尝试获取资源。\n\n> **注意**：由于信号量不强制所有权（任意任务可调用 `up()`），且缺乏死锁检测等调试特性，现代内核开发中更推荐使用 `mutex` 或 `rwsem`，除非明确需要计数语义或多释放者特性。",
      "similarity": 0.5275782346725464,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2008 Intel Corporation",
            " * Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This file implements counting semaphores.",
            " * A counting semaphore may be acquired 'n' times before sleeping.",
            " * See mutex.c for single-acquisition sleeping locks which enforce",
            " * rules which allow code to be debugged more easily.",
            " */",
            "",
            "/*",
            " * Some notes on the implementation:",
            " *",
            " * The spinlock controls access to the other members of the semaphore.",
            " * down_trylock() and up() can be called from interrupt context, so we",
            " * have to disable interrupts when taking the lock.  It turns out various",
            " * parts of the kernel expect to be able to use down() on a semaphore in",
            " * interrupt context when they know it will succeed, so we have to use",
            " * irqsave variants for down(), down_interruptible() and down_killable()",
            " * too.",
            " *",
            " * The ->count variable represents how many more tasks can acquire this",
            " * semaphore.  If it's zero, there may be tasks waiting on the wait_list.",
            " */",
            "",
            "#include <linux/compiler.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/semaphore.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/ftrace.h>",
            "#include <trace/events/lock.h>",
            "#include <linux/hung_task.h>",
            "",
            "static noinline void __down(struct semaphore *sem);",
            "static noinline int __down_interruptible(struct semaphore *sem);",
            "static noinline int __down_killable(struct semaphore *sem);",
            "static noinline int __down_timeout(struct semaphore *sem, long timeout);",
            "static noinline void __up(struct semaphore *sem, struct wake_q_head *wake_q);",
            "",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER"
          ],
          "function_name": null,
          "description": "此代码块定义了计数信号量的基础框架，包含实现计数信号量所需的头文件和注释，声明了多个内联函数及辅助函数，用于处理信号量的获取、释放及Hung Task检测相关逻辑，但由于代码截断，CONFIG_DETECT_HUNG_TASK_BLOCKER部分缺失，上下文不完整",
          "similarity": 0.5607949495315552
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "\tWRITE_ONCE((sem)->last_holder, (unsigned long)current);",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "\tif (READ_ONCE((sem)->last_holder) == (unsigned long)current)",
            "\t\tWRITE_ONCE((sem)->last_holder, 0UL);",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn READ_ONCE(sem->last_holder);",
            "}",
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn 0UL;",
            "}",
            "static inline void __sem_acquire(struct semaphore *sem)",
            "{",
            "\tsem->count--;",
            "\thung_task_sem_set_holder(sem);",
            "}",
            "void __sched down(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\t__down(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "}",
            "int __sched down_interruptible(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_interruptible(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_killable(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_killable(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_trylock(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint count;",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tcount = sem->count - 1;",
            "\tif (likely(count >= 0))",
            "\t\t__sem_acquire(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn (count < 0);",
            "}",
            "int __sched down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_timeout(sem, timeout);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "void __sched up(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "",
            "\thung_task_sem_clear_if_holder(sem);",
            "",
            "\tif (likely(list_empty(&sem->wait_list)))",
            "\t\tsem->count++;",
            "\telse",
            "\t\t__up(sem, &wake_q);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "\tif (!wake_q_empty(&wake_q))",
            "\t\twake_up_q(&wake_q);",
            "}"
          ],
          "function_name": "hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, __sem_acquire, down, down_interruptible, down_killable, down_trylock, down_timeout, up",
          "description": "实现了信号量的获取与释放核心逻辑，包括down/down_interruptible/down_killable/down_trylock/down_timeout等接口，通过spinlock保护共享资源，维护等待队列并处理任务状态变更，其中包含Hung Task检测相关函数的条件性实现",
          "similarity": 0.5397465825080872
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 252,
          "end_line": 323,
          "content": [
            "static inline int __sched ___down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\t\t\t\tlong timeout)",
            "{",
            "\tstruct semaphore_waiter waiter;",
            "",
            "\tlist_add_tail(&waiter.list, &sem->wait_list);",
            "\twaiter.task = current;",
            "\twaiter.up = false;",
            "",
            "\tfor (;;) {",
            "\t\tif (signal_pending_state(state, current))",
            "\t\t\tgoto interrupted;",
            "\t\tif (unlikely(timeout <= 0))",
            "\t\t\tgoto timed_out;",
            "\t\t__set_current_state(state);",
            "\t\traw_spin_unlock_irq(&sem->lock);",
            "\t\ttimeout = schedule_timeout(timeout);",
            "\t\traw_spin_lock_irq(&sem->lock);",
            "\t\tif (waiter.up) {",
            "\t\t\thung_task_sem_set_holder(sem);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            " timed_out:",
            "\tlist_del(&waiter.list);",
            "\treturn -ETIME;",
            "",
            " interrupted:",
            "\tlist_del(&waiter.list);",
            "\treturn -EINTR;",
            "}",
            "static inline int __sched __down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\tlong timeout)",
            "{",
            "\tint ret;",
            "",
            "\thung_task_set_blocker(sem, BLOCKER_TYPE_SEM);",
            "",
            "\ttrace_contention_begin(sem, 0);",
            "\tret = ___down_common(sem, state, timeout);",
            "\ttrace_contention_end(sem, ret);",
            "",
            "\thung_task_clear_blocker();",
            "",
            "\treturn ret;",
            "}",
            "static noinline void __sched __down(struct semaphore *sem)",
            "{",
            "\t__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_interruptible(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_killable(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_KILLABLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\treturn __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);",
            "}",
            "static noinline void __sched __up(struct semaphore *sem,",
            "\t\t\t\t  struct wake_q_head *wake_q)",
            "{",
            "\tstruct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,",
            "\t\t\t\t\t\tstruct semaphore_waiter, list);",
            "\tlist_del(&waiter->list);",
            "\twaiter->up = true;",
            "\twake_q_add(wake_q, waiter->task);",
            "}"
          ],
          "function_name": "___down_common, __down_common, __down, __down_interruptible, __down_killable, __down_timeout, __up",
          "description": "实现了信号量的阻塞等待通用逻辑，包含___down_common/__down_common等辅助函数，处理信号量不足时的任务挂起、超时检测、信号处理及唤醒机制，通过循环等待并结合schedule_timeout实现阻塞式资源竞争解决",
          "similarity": 0.5170700550079346
        }
      ]
    },
    {
      "source_file": "kernel/workqueue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:53:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workqueue.c`\n\n---\n\n# workqueue.c 技术文档\n\n## 1. 文件概述\n\n`workqueue.c` 是 Linux 内核中实现通用异步执行机制的核心文件，提供基于共享工作线程池（worker pool）的延迟任务调度功能。工作项（work items）在进程上下文中执行，支持 CPU 绑定和非绑定两种模式。每个 CPU 默认拥有两个标准工作池（普通优先级和高优先级），同时支持动态创建非绑定工作池以满足不同工作队列的需求。该机制替代了早期的 taskqueue/keventd 实现，具有更高的可扩展性和资源利用率。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct worker_pool`**  \n  工作线程池结构体，管理一组工作线程（workers），包含：\n  - `lock`：保护池状态的自旋锁\n  - `cpu` / `node`：关联的 CPU 和 NUMA 节点（绑定池）\n  - `worklist`：待处理工作项队列\n  - `idle_list` / `busy_hash`：空闲和忙碌工作线程的管理结构\n  - `nr_workers` / `nr_idle`：工作线程数量统计\n  - `attrs`：工作线程属性（如优先级、CPU 亲和性）\n  - `mayday_timer`：紧急情况下的救援请求定时器\n\n- **`struct pool_workqueue`**  \n  工作队列与工作池之间的关联结构，每个工作队列在每个池中都有一个对应的 `pool_workqueue` 实例，用于：\n  - 管理工作项的入队和执行\n  - 实现 `max_active` 限制（控制并发执行数）\n  - 支持 flush 操作（等待所有工作完成）\n  - 统计性能指标（如启动/完成次数、CPU 时间等）\n\n- **`struct worker`**（定义在 `workqueue_internal.h`）  \n  工作线程的运行时上下文，包含状态标志（如 `WORKER_IDLE`, `WORKER_UNBOUND`）、当前执行的工作项等。\n\n### 关键枚举与常量\n\n- **池/工作线程标志**：\n  - `POOL_DISASSOCIATED`：CPU 离线时池进入非绑定状态\n  - `WORKER_UNBOUND`：工作线程可在任意 CPU 上运行\n  - `WORKER_CPU_INTENSIVE`：标记 CPU 密集型任务，影响并发控制\n\n- **配置参数**：\n  - `NR_STD_WORKER_POOLS = 2`：每 CPU 标准池数量（普通 + 高优先级）\n  - `IDLE_WORKER_TIMEOUT = 300 * HZ`：空闲线程保留时间（5 分钟）\n  - `MAYDAY_INITIAL_TIMEOUT`：工作积压时触发救援的延迟（10ms）\n\n- **统计指标**（`pool_workqueue_stats`）：\n  - `PWQ_STAT_STARTED` / `PWQ_STAT_COMPLETED`：工作项执行统计\n  - `PWQ_STAT_MAYDAY` / `PWQ_STAT_RESCUED`：紧急救援事件计数\n\n## 3. 关键实现\n\n### 工作池管理\n- **绑定池（Bound Pool）**：与特定 CPU 关联，工作线程默认绑定到该 CPU。当 CPU 离线时，池进入 `DISASSOCIATED` 状态，工作线程转为非绑定模式。\n- **非绑定池（Unbound Pool）**：动态创建，通过哈希表（`unbound_pool_hash`）按属性（`workqueue_attrs`）去重，支持跨 CPU 调度。\n- **并发控制**：通过 `nr_running` 计数器和 `max_active` 限制，防止工作项过度并发执行。\n\n### 工作线程生命周期\n- **空闲管理**：空闲线程加入 `idle_list`，超时（`IDLE_WORKER_TIMEOUT`）后被回收。\n- **动态伸缩**：当工作积压时，通过 `mayday_timer` 触发新线程创建；若创建失败，向全局救援线程（rescuer）求助。\n- **状态标志**：使用位标志（如 `WORKER_IDLE`, `WORKER_PREP`）高效管理线程状态，避免锁竞争。\n\n### 内存与同步\n- **RCU 保护**：工作池销毁通过 RCU 延迟释放，确保 `get_work_pool()` 等读取路径无锁安全。\n- **锁分层**：\n  - `pool->lock`（自旋锁）：保护池内部状态\n  - `wq_pool_mutex`：全局池管理互斥锁\n  - `wq_pool_attach_mutex`：防止 CPU 绑定状态变更冲突\n\n### 工作项调度\n- **数据指针复用**：`work_struct->data` 的高有效位存储 `pool_workqueue` 指针，低有效位用于标志位（如 `WORK_STRUCT_INACTIVE`）。\n- **优先级支持**：高优先级工作池使用 `HIGHPRI_NICE_LEVEL = MIN_NICE` 提升调度优先级。\n\n## 4. 依赖关系\n\n- **内核子系统**：\n  - **调度器**（`<linux/sched.h>`）：创建工作线程（kworker），管理 CPU 亲和性\n  - **内存管理**（`<linux/slab.h>`）：分配工作池、工作队列等结构\n  - **CPU 热插拔**（`<linux/cpu.h>`）：处理 CPU 上下线时的池绑定状态切换\n  - **RCU**（`<linux/rculist.h>`）：实现无锁读取路径\n  - **定时器**（`<linux/timer.h>`）：实现空闲超时和救援机制\n\n- **内部依赖**：\n  - `workqueue_internal.h`：定义 `struct worker` 等内部结构\n  - `Documentation/core-api/workqueue.rst`：详细设计文档\n\n## 5. 使用场景\n\n- **驱动程序延迟操作**：硬件中断后调度下半部处理（如网络包处理、磁盘 I/O 完成回调）。\n- **内核子系统异步任务**：文件系统元数据更新、内存回收、电源管理状态切换。\n- **高优先级任务**：使用 `WQ_HIGHPRI` 标志创建工作队列，确保关键任务及时执行（如死锁恢复）。\n- **CPU 密集型任务**：标记 `WQ_CPU_INTENSIVE` 避免占用过多并发槽位，提升系统响应性。\n- **NUMA 感知调度**：非绑定工作队列可指定 NUMA 节点，优化内存访问延迟。",
      "similarity": 0.4998304843902588,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/workqueue.c",
          "start_line": 1195,
          "end_line": 1307,
          "content": [
            "static void wq_cpu_intensive_report(work_func_t func)",
            "{",
            "\tstruct wci_ent *ent;",
            "",
            "restart:",
            "\tent = wci_find_ent(func);",
            "\tif (ent) {",
            "\t\tu64 cnt;",
            "",
            "\t\t/*",
            "\t\t * Start reporting from the fourth time and back off",
            "\t\t * exponentially.",
            "\t\t */",
            "\t\tcnt = atomic64_inc_return_relaxed(&ent->cnt);",
            "\t\tif (cnt >= 4 && is_power_of_2(cnt))",
            "\t\t\tprintk_deferred(KERN_WARNING \"workqueue: %ps hogged CPU for >%luus %llu times, consider switching to WQ_UNBOUND\\n\",",
            "\t\t\t\t\tent->func, wq_cpu_intensive_thresh_us,",
            "\t\t\t\t\tatomic64_read(&ent->cnt));",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * @func is a new violation. Allocate a new entry for it. If wcn_ents[]",
            "\t * is exhausted, something went really wrong and we probably made enough",
            "\t * noise already.",
            "\t */",
            "\tif (wci_nr_ents >= WCI_MAX_ENTS)",
            "\t\treturn;",
            "",
            "\traw_spin_lock(&wci_lock);",
            "",
            "\tif (wci_nr_ents >= WCI_MAX_ENTS) {",
            "\t\traw_spin_unlock(&wci_lock);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (wci_find_ent(func)) {",
            "\t\traw_spin_unlock(&wci_lock);",
            "\t\tgoto restart;",
            "\t}",
            "",
            "\tent = &wci_ents[wci_nr_ents++];",
            "\tent->func = func;",
            "\tatomic64_set(&ent->cnt, 1);",
            "\thash_add_rcu(wci_hash, &ent->hash_node, (unsigned long)func);",
            "",
            "\traw_spin_unlock(&wci_lock);",
            "}",
            "static void wq_cpu_intensive_report(work_func_t func) {}",
            "void wq_worker_running(struct task_struct *task)",
            "{",
            "\tstruct worker *worker = kthread_data(task);",
            "",
            "\tif (!READ_ONCE(worker->sleeping))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If preempted by unbind_workers() between the WORKER_NOT_RUNNING check",
            "\t * and the nr_running increment below, we may ruin the nr_running reset",
            "\t * and leave with an unexpected pool->nr_running == 1 on the newly unbound",
            "\t * pool. Protect against such race.",
            "\t */",
            "\tpreempt_disable();",
            "\tif (!(worker->flags & WORKER_NOT_RUNNING))",
            "\t\tworker->pool->nr_running++;",
            "\tpreempt_enable();",
            "",
            "\t/*",
            "\t * CPU intensive auto-detection cares about how long a work item hogged",
            "\t * CPU without sleeping. Reset the starting timestamp on wakeup.",
            "\t */",
            "\tworker->current_at = worker->task->se.sum_exec_runtime;",
            "",
            "\tWRITE_ONCE(worker->sleeping, 0);",
            "}",
            "void wq_worker_sleeping(struct task_struct *task)",
            "{",
            "\tstruct worker *worker = kthread_data(task);",
            "\tstruct worker_pool *pool;",
            "",
            "\t/*",
            "\t * Rescuers, which may not have all the fields set up like normal",
            "\t * workers, also reach here, let's not access anything before",
            "\t * checking NOT_RUNNING.",
            "\t */",
            "\tif (worker->flags & WORKER_NOT_RUNNING)",
            "\t\treturn;",
            "",
            "\tpool = worker->pool;",
            "",
            "\t/* Return if preempted before wq_worker_running() was reached */",
            "\tif (READ_ONCE(worker->sleeping))",
            "\t\treturn;",
            "",
            "\tWRITE_ONCE(worker->sleeping, 1);",
            "\traw_spin_lock_irq(&pool->lock);",
            "",
            "\t/*",
            "\t * Recheck in case unbind_workers() preempted us. We don't",
            "\t * want to decrement nr_running after the worker is unbound",
            "\t * and nr_running has been reset.",
            "\t */",
            "\tif (worker->flags & WORKER_NOT_RUNNING) {",
            "\t\traw_spin_unlock_irq(&pool->lock);",
            "\t\treturn;",
            "\t}",
            "",
            "\tpool->nr_running--;",
            "\tif (kick_pool(pool))",
            "\t\tworker->current_pwq->stats[PWQ_STAT_CM_WAKEUP]++;",
            "",
            "\traw_spin_unlock_irq(&pool->lock);",
            "}"
          ],
          "function_name": "wq_cpu_intensive_report, wq_cpu_intensive_report, wq_worker_running, wq_worker_sleeping",
          "description": "实现CPU密集型任务检测和工作者状态监控，包含运行时间统计、睡眠状态标记及唤醒通知逻辑，通过运行时间对比触发CPU占用警告，维护工作者线程活跃状态计数器以实现负载均衡策略。",
          "similarity": 0.4807991087436676
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/workqueue.c",
          "start_line": 692,
          "end_line": 797,
          "content": [
            "static void set_work_pool_and_clear_pending(struct work_struct *work,",
            "\t\t\t\t\t    int pool_id)",
            "{",
            "\t/*",
            "\t * The following wmb is paired with the implied mb in",
            "\t * test_and_set_bit(PENDING) and ensures all updates to @work made",
            "\t * here are visible to and precede any updates by the next PENDING",
            "\t * owner.",
            "\t */",
            "\tsmp_wmb();",
            "\tset_work_data(work, (unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT, 0);",
            "\t/*",
            "\t * The following mb guarantees that previous clear of a PENDING bit",
            "\t * will not be reordered with any speculative LOADS or STORES from",
            "\t * work->current_func, which is executed afterwards.  This possible",
            "\t * reordering can lead to a missed execution on attempt to queue",
            "\t * the same @work.  E.g. consider this case:",
            "\t *",
            "\t *   CPU#0                         CPU#1",
            "\t *   ----------------------------  --------------------------------",
            "\t *",
            "\t * 1  STORE event_indicated",
            "\t * 2  queue_work_on() {",
            "\t * 3    test_and_set_bit(PENDING)",
            "\t * 4 }                             set_..._and_clear_pending() {",
            "\t * 5                                 set_work_data() # clear bit",
            "\t * 6                                 smp_mb()",
            "\t * 7                               work->current_func() {",
            "\t * 8\t\t\t\t      LOAD event_indicated",
            "\t *\t\t\t\t   }",
            "\t *",
            "\t * Without an explicit full barrier speculative LOAD on line 8 can",
            "\t * be executed before CPU#0 does STORE on line 1.  If that happens,",
            "\t * CPU#0 observes the PENDING bit is still set and new execution of",
            "\t * a @work is not queued in a hope, that CPU#1 will eventually",
            "\t * finish the queued @work.  Meanwhile CPU#1 does not see",
            "\t * event_indicated is set, because speculative LOAD was executed",
            "\t * before actual STORE.",
            "\t */",
            "\tsmp_mb();",
            "}",
            "static void clear_work_data(struct work_struct *work)",
            "{",
            "\tsmp_wmb();\t/* see set_work_pool_and_clear_pending() */",
            "\tset_work_data(work, WORK_STRUCT_NO_POOL, 0);",
            "}",
            "static int get_work_pool_id(struct work_struct *work)",
            "{",
            "\tunsigned long data = atomic_long_read(&work->data);",
            "",
            "\tif (data & WORK_STRUCT_PWQ)",
            "\t\treturn work_struct_pwq(data)->pool->id;",
            "",
            "\treturn data >> WORK_OFFQ_POOL_SHIFT;",
            "}",
            "static void mark_work_canceling(struct work_struct *work)",
            "{",
            "\tunsigned long pool_id = get_work_pool_id(work);",
            "",
            "\tpool_id <<= WORK_OFFQ_POOL_SHIFT;",
            "\tset_work_data(work, pool_id | WORK_OFFQ_CANCELING, WORK_STRUCT_PENDING);",
            "}",
            "static bool work_is_canceling(struct work_struct *work)",
            "{",
            "\tunsigned long data = atomic_long_read(&work->data);",
            "",
            "\treturn !(data & WORK_STRUCT_PWQ) && (data & WORK_OFFQ_CANCELING);",
            "}",
            "static bool need_more_worker(struct worker_pool *pool)",
            "{",
            "\treturn !list_empty(&pool->worklist) && !pool->nr_running;",
            "}",
            "static bool may_start_working(struct worker_pool *pool)",
            "{",
            "\treturn pool->nr_idle;",
            "}",
            "static bool keep_working(struct worker_pool *pool)",
            "{",
            "\treturn !list_empty(&pool->worklist) && (pool->nr_running <= 1);",
            "}",
            "static bool need_to_create_worker(struct worker_pool *pool)",
            "{",
            "\treturn need_more_worker(pool) && !may_start_working(pool);",
            "}",
            "static bool too_many_workers(struct worker_pool *pool)",
            "{",
            "\tbool managing = pool->flags & POOL_MANAGER_ACTIVE;",
            "\tint nr_idle = pool->nr_idle + managing; /* manager is considered idle */",
            "\tint nr_busy = pool->nr_workers - nr_idle;",
            "",
            "\treturn nr_idle > 2 && (nr_idle - 2) * MAX_IDLE_WORKERS_RATIO >= nr_busy;",
            "}",
            "static inline void worker_set_flags(struct worker *worker, unsigned int flags)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\t/* If transitioning into NOT_RUNNING, adjust nr_running. */",
            "\tif ((flags & WORKER_NOT_RUNNING) &&",
            "\t    !(worker->flags & WORKER_NOT_RUNNING)) {",
            "\t\tpool->nr_running--;",
            "\t}",
            "",
            "\tworker->flags |= flags;",
            "}"
          ],
          "function_name": "set_work_pool_and_clear_pending, clear_work_data, get_work_pool_id, mark_work_canceling, work_is_canceling, need_more_worker, may_start_working, keep_working, need_to_create_worker, too_many_workers, worker_set_flags",
          "description": "实现工作者线程池状态控制逻辑，包含需要创建新工作者的判断条件、工作者空闲状态管理、工作项冲突检测及任务分配函数，通过锁保护保证池状态一致性，维护工作者线程与待处理工作项的匹配关系。",
          "similarity": 0.4715927243232727
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/workqueue.c",
          "start_line": 1,
          "end_line": 524,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * kernel/workqueue.c - generic async execution with shared worker pool",
            " *",
            " * Copyright (C) 2002\t\tIngo Molnar",
            " *",
            " *   Derived from the taskqueue/keventd code by:",
            " *     David Woodhouse <dwmw2@infradead.org>",
            " *     Andrew Morton",
            " *     Kai Petzke <wpp@marie.physik.tu-berlin.de>",
            " *     Theodore Ts'o <tytso@mit.edu>",
            " *",
            " * Made to use alloc_percpu by Christoph Lameter.",
            " *",
            " * Copyright (C) 2010\t\tSUSE Linux Products GmbH",
            " * Copyright (C) 2010\t\tTejun Heo <tj@kernel.org>",
            " *",
            " * This is the generic async execution mechanism.  Work items as are",
            " * executed in process context.  The worker pool is shared and",
            " * automatically managed.  There are two worker pools for each CPU (one for",
            " * normal work items and the other for high priority ones) and some extra",
            " * pools for workqueues which are not bound to any specific CPU - the",
            " * number of these backing pools is dynamic.",
            " *",
            " * Please read Documentation/core-api/workqueue.rst for details.",
            " */",
            "",
            "#include <linux/export.h>",
            "#include <linux/kernel.h>",
            "#include <linux/sched.h>",
            "#include <linux/init.h>",
            "#include <linux/signal.h>",
            "#include <linux/completion.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/slab.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/freezer.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/idr.h>",
            "#include <linux/jhash.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/rculist.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/nmi.h>",
            "#include <linux/kvm_para.h>",
            "#include <linux/delay.h>",
            "",
            "#include \"workqueue_internal.h\"",
            "",
            "enum {",
            "\t/*",
            "\t * worker_pool flags",
            "\t *",
            "\t * A bound pool is either associated or disassociated with its CPU.",
            "\t * While associated (!DISASSOCIATED), all workers are bound to the",
            "\t * CPU and none has %WORKER_UNBOUND set and concurrency management",
            "\t * is in effect.",
            "\t *",
            "\t * While DISASSOCIATED, the cpu may be offline and all workers have",
            "\t * %WORKER_UNBOUND set and concurrency management disabled, and may",
            "\t * be executing on any CPU.  The pool behaves as an unbound one.",
            "\t *",
            "\t * Note that DISASSOCIATED should be flipped only while holding",
            "\t * wq_pool_attach_mutex to avoid changing binding state while",
            "\t * worker_attach_to_pool() is in progress.",
            "\t */",
            "\tPOOL_MANAGER_ACTIVE\t= 1 << 0,\t/* being managed */",
            "\tPOOL_DISASSOCIATED\t= 1 << 2,\t/* cpu can't serve workers */",
            "",
            "\t/* worker flags */",
            "\tWORKER_DIE\t\t= 1 << 1,\t/* die die die */",
            "\tWORKER_IDLE\t\t= 1 << 2,\t/* is idle */",
            "\tWORKER_PREP\t\t= 1 << 3,\t/* preparing to run works */",
            "\tWORKER_CPU_INTENSIVE\t= 1 << 6,\t/* cpu intensive */",
            "\tWORKER_UNBOUND\t\t= 1 << 7,\t/* worker is unbound */",
            "\tWORKER_REBOUND\t\t= 1 << 8,\t/* worker was rebound */",
            "",
            "\tWORKER_NOT_RUNNING\t= WORKER_PREP | WORKER_CPU_INTENSIVE |",
            "\t\t\t\t  WORKER_UNBOUND | WORKER_REBOUND,",
            "",
            "\tNR_STD_WORKER_POOLS\t= 2,\t\t/* # standard pools per cpu */",
            "",
            "\tUNBOUND_POOL_HASH_ORDER\t= 6,\t\t/* hashed by pool->attrs */",
            "\tBUSY_WORKER_HASH_ORDER\t= 6,\t\t/* 64 pointers */",
            "",
            "\tMAX_IDLE_WORKERS_RATIO\t= 4,\t\t/* 1/4 of busy can be idle */",
            "\tIDLE_WORKER_TIMEOUT\t= 300 * HZ,\t/* keep idle ones for 5 mins */",
            "",
            "\tMAYDAY_INITIAL_TIMEOUT  = HZ / 100 >= 2 ? HZ / 100 : 2,",
            "\t\t\t\t\t\t/* call for help after 10ms",
            "\t\t\t\t\t\t   (min two ticks) */",
            "\tMAYDAY_INTERVAL\t\t= HZ / 10,\t/* and then every 100ms */",
            "\tCREATE_COOLDOWN\t\t= HZ,\t\t/* time to breath after fail */",
            "",
            "\t/*",
            "\t * Rescue workers are used only on emergencies and shared by",
            "\t * all cpus.  Give MIN_NICE.",
            "\t */",
            "\tRESCUER_NICE_LEVEL\t= MIN_NICE,",
            "\tHIGHPRI_NICE_LEVEL\t= MIN_NICE,",
            "",
            "\tWQ_NAME_LEN\t\t= 24,",
            "};",
            "",
            "/*",
            " * Structure fields follow one of the following exclusion rules.",
            " *",
            " * I: Modifiable by initialization/destruction paths and read-only for",
            " *    everyone else.",
            " *",
            " * P: Preemption protected.  Disabling preemption is enough and should",
            " *    only be modified and accessed from the local cpu.",
            " *",
            " * L: pool->lock protected.  Access with pool->lock held.",
            " *",
            " * K: Only modified by worker while holding pool->lock. Can be safely read by",
            " *    self, while holding pool->lock or from IRQ context if %current is the",
            " *    kworker.",
            " *",
            " * S: Only modified by worker self.",
            " *",
            " * A: wq_pool_attach_mutex protected.",
            " *",
            " * PL: wq_pool_mutex protected.",
            " *",
            " * PR: wq_pool_mutex protected for writes.  RCU protected for reads.",
            " *",
            " * PW: wq_pool_mutex and wq->mutex protected for writes.  Either for reads.",
            " *",
            " * PWR: wq_pool_mutex and wq->mutex protected for writes.  Either or",
            " *      RCU for reads.",
            " *",
            " * WQ: wq->mutex protected.",
            " *",
            " * WR: wq->mutex protected for writes.  RCU protected for reads.",
            " *",
            " * MD: wq_mayday_lock protected.",
            " *",
            " * WD: Used internally by the watchdog.",
            " */",
            "",
            "/* struct worker is defined in workqueue_internal.h */",
            "",
            "struct worker_pool {",
            "\traw_spinlock_t\t\tlock;\t\t/* the pool lock */",
            "\tint\t\t\tcpu;\t\t/* I: the associated cpu */",
            "\tint\t\t\tnode;\t\t/* I: the associated node ID */",
            "\tint\t\t\tid;\t\t/* I: pool ID */",
            "\tunsigned int\t\tflags;\t\t/* L: flags */",
            "",
            "\tunsigned long\t\twatchdog_ts;\t/* L: watchdog timestamp */",
            "\tbool\t\t\tcpu_stall;\t/* WD: stalled cpu bound pool */",
            "",
            "\t/*",
            "\t * The counter is incremented in a process context on the associated CPU",
            "\t * w/ preemption disabled, and decremented or reset in the same context",
            "\t * but w/ pool->lock held. The readers grab pool->lock and are",
            "\t * guaranteed to see if the counter reached zero.",
            "\t */",
            "\tint\t\t\tnr_running;",
            "",
            "\tstruct list_head\tworklist;\t/* L: list of pending works */",
            "",
            "\tint\t\t\tnr_workers;\t/* L: total number of workers */",
            "\tint\t\t\tnr_idle;\t/* L: currently idle workers */",
            "",
            "\tstruct list_head\tidle_list;\t/* L: list of idle workers */",
            "\tstruct timer_list\tidle_timer;\t/* L: worker idle timeout */",
            "\tstruct work_struct      idle_cull_work; /* L: worker idle cleanup */",
            "",
            "\tstruct timer_list\tmayday_timer;\t  /* L: SOS timer for workers */",
            "",
            "\t/* a workers is either on busy_hash or idle_list, or the manager */",
            "\tDECLARE_HASHTABLE(busy_hash, BUSY_WORKER_HASH_ORDER);",
            "\t\t\t\t\t\t/* L: hash of busy workers */",
            "",
            "\tstruct worker\t\t*manager;\t/* L: purely informational */",
            "\tstruct list_head\tworkers;\t/* A: attached workers */",
            "\tstruct list_head        dying_workers;  /* A: workers about to die */",
            "\tstruct completion\t*detach_completion; /* all workers detached */",
            "",
            "\tstruct ida\t\tworker_ida;\t/* worker IDs for task name */",
            "",
            "\tstruct workqueue_attrs\t*attrs;\t\t/* I: worker attributes */",
            "\tstruct hlist_node\thash_node;\t/* PL: unbound_pool_hash node */",
            "\tint\t\t\trefcnt;\t\t/* PL: refcnt for unbound pools */",
            "",
            "\t/*",
            "\t * Destruction of pool is RCU protected to allow dereferences",
            "\t * from get_work_pool().",
            "\t */",
            "\tstruct rcu_head\t\trcu;",
            "};",
            "",
            "/*",
            " * Per-pool_workqueue statistics. These can be monitored using",
            " * tools/workqueue/wq_monitor.py.",
            " */",
            "enum pool_workqueue_stats {",
            "\tPWQ_STAT_STARTED,\t/* work items started execution */",
            "\tPWQ_STAT_COMPLETED,\t/* work items completed execution */",
            "\tPWQ_STAT_CPU_TIME,\t/* total CPU time consumed */",
            "\tPWQ_STAT_CPU_INTENSIVE,\t/* wq_cpu_intensive_thresh_us violations */",
            "\tPWQ_STAT_CM_WAKEUP,\t/* concurrency-management worker wakeups */",
            "\tPWQ_STAT_REPATRIATED,\t/* unbound workers brought back into scope */",
            "\tPWQ_STAT_MAYDAY,\t/* maydays to rescuer */",
            "\tPWQ_STAT_RESCUED,\t/* linked work items executed by rescuer */",
            "",
            "\tPWQ_NR_STATS,",
            "};",
            "",
            "/*",
            " * The per-pool workqueue.  While queued, the lower WORK_STRUCT_FLAG_BITS",
            " * of work_struct->data are used for flags and the remaining high bits",
            " * point to the pwq; thus, pwqs need to be aligned at two's power of the",
            " * number of flag bits.",
            " */",
            "struct pool_workqueue {",
            "\tstruct worker_pool\t*pool;\t\t/* I: the associated pool */",
            "\tstruct workqueue_struct *wq;\t\t/* I: the owning workqueue */",
            "\tint\t\t\twork_color;\t/* L: current color */",
            "\tint\t\t\tflush_color;\t/* L: flushing color */",
            "\tint\t\t\trefcnt;\t\t/* L: reference count */",
            "\tint\t\t\tnr_in_flight[WORK_NR_COLORS];",
            "\t\t\t\t\t\t/* L: nr of in_flight works */",
            "",
            "\t/*",
            "\t * nr_active management and WORK_STRUCT_INACTIVE:",
            "\t *",
            "\t * When pwq->nr_active >= max_active, new work item is queued to",
            "\t * pwq->inactive_works instead of pool->worklist and marked with",
            "\t * WORK_STRUCT_INACTIVE.",
            "\t *",
            "\t * All work items marked with WORK_STRUCT_INACTIVE do not participate",
            "\t * in pwq->nr_active and all work items in pwq->inactive_works are",
            "\t * marked with WORK_STRUCT_INACTIVE.  But not all WORK_STRUCT_INACTIVE",
            "\t * work items are in pwq->inactive_works.  Some of them are ready to",
            "\t * run in pool->worklist or worker->scheduled.  Those work itmes are",
            "\t * only struct wq_barrier which is used for flush_work() and should",
            "\t * not participate in pwq->nr_active.  For non-barrier work item, it",
            "\t * is marked with WORK_STRUCT_INACTIVE iff it is in pwq->inactive_works.",
            "\t */",
            "\tint\t\t\tnr_active;\t/* L: nr of active works */",
            "\tint\t\t\tmax_active;\t/* L: max active works */",
            "\tstruct list_head\tinactive_works;\t/* L: inactive works */",
            "\tstruct list_head\tpwqs_node;\t/* WR: node on wq->pwqs */",
            "\tstruct list_head\tmayday_node;\t/* MD: node on wq->maydays */",
            "",
            "\tu64\t\t\tstats[PWQ_NR_STATS];",
            "",
            "\t/*",
            "\t * Release of unbound pwq is punted to a kthread_worker. See put_pwq()",
            "\t * and pwq_release_workfn() for details. pool_workqueue itself is also",
            "\t * RCU protected so that the first pwq can be determined without",
            "\t * grabbing wq->mutex.",
            "\t */",
            "\tstruct kthread_work\trelease_work;",
            "\tstruct rcu_head\t\trcu;",
            "} __aligned(1 << WORK_STRUCT_FLAG_BITS);",
            "",
            "/*",
            " * Structure used to wait for workqueue flush.",
            " */",
            "struct wq_flusher {",
            "\tstruct list_head\tlist;\t\t/* WQ: list of flushers */",
            "\tint\t\t\tflush_color;\t/* WQ: flush color waiting for */",
            "\tstruct completion\tdone;\t\t/* flush completion */",
            "};",
            "",
            "struct wq_device;",
            "",
            "/*",
            " * The externally visible workqueue.  It relays the issued work items to",
            " * the appropriate worker_pool through its pool_workqueues.",
            " */",
            "struct workqueue_struct {",
            "\tstruct list_head\tpwqs;\t\t/* WR: all pwqs of this wq */",
            "\tstruct list_head\tlist;\t\t/* PR: list of all workqueues */",
            "",
            "\tstruct mutex\t\tmutex;\t\t/* protects this wq */",
            "\tint\t\t\twork_color;\t/* WQ: current work color */",
            "\tint\t\t\tflush_color;\t/* WQ: current flush color */",
            "\tatomic_t\t\tnr_pwqs_to_flush; /* flush in progress */",
            "\tstruct wq_flusher\t*first_flusher;\t/* WQ: first flusher */",
            "\tstruct list_head\tflusher_queue;\t/* WQ: flush waiters */",
            "\tstruct list_head\tflusher_overflow; /* WQ: flush overflow list */",
            "",
            "\tstruct list_head\tmaydays;\t/* MD: pwqs requesting rescue */",
            "\tstruct worker\t\t*rescuer;\t/* MD: rescue worker */",
            "",
            "\tint\t\t\tnr_drainers;\t/* WQ: drain in progress */",
            "\tint\t\t\tsaved_max_active; /* WQ: saved pwq max_active */",
            "",
            "\tstruct workqueue_attrs\t*unbound_attrs;\t/* PW: only for unbound wqs */",
            "\tstruct pool_workqueue\t*dfl_pwq;\t/* PW: only for unbound wqs */",
            "",
            "#ifdef CONFIG_SYSFS",
            "\tstruct wq_device\t*wq_dev;\t/* I: for sysfs interface */",
            "#endif",
            "#ifdef CONFIG_LOCKDEP",
            "\tchar\t\t\t*lock_name;",
            "\tstruct lock_class_key\tkey;",
            "\tstruct lockdep_map\tlockdep_map;",
            "#endif",
            "\tchar\t\t\tname[WQ_NAME_LEN]; /* I: workqueue name */",
            "",
            "\t/*",
            "\t * Destruction of workqueue_struct is RCU protected to allow walking",
            "\t * the workqueues list without grabbing wq_pool_mutex.",
            "\t * This is used to dump all workqueues from sysrq.",
            "\t */",
            "\tstruct rcu_head\t\trcu;",
            "",
            "\t/* hot fields used during command issue, aligned to cacheline */",
            "\tunsigned int\t\tflags ____cacheline_aligned; /* WQ: WQ_* flags */",
            "\tstruct pool_workqueue __percpu __rcu **cpu_pwq; /* I: per-cpu pwqs */",
            "};",
            "",
            "static struct kmem_cache *pwq_cache;",
            "",
            "/*",
            " * Each pod type describes how CPUs should be grouped for unbound workqueues.",
            " * See the comment above workqueue_attrs->affn_scope.",
            " */",
            "struct wq_pod_type {",
            "\tint\t\t\tnr_pods;\t/* number of pods */",
            "\tcpumask_var_t\t\t*pod_cpus;\t/* pod -> cpus */",
            "\tint\t\t\t*pod_node;\t/* pod -> node */",
            "\tint\t\t\t*cpu_pod;\t/* cpu -> pod */",
            "};",
            "",
            "static struct wq_pod_type wq_pod_types[WQ_AFFN_NR_TYPES];",
            "static enum wq_affn_scope wq_affn_dfl = WQ_AFFN_CACHE;",
            "",
            "static const char *wq_affn_names[WQ_AFFN_NR_TYPES] = {",
            "\t[WQ_AFFN_DFL]\t\t\t= \"default\",",
            "\t[WQ_AFFN_CPU]\t\t\t= \"cpu\",",
            "\t[WQ_AFFN_SMT]\t\t\t= \"smt\",",
            "\t[WQ_AFFN_CACHE]\t\t\t= \"cache\",",
            "\t[WQ_AFFN_NUMA]\t\t\t= \"numa\",",
            "\t[WQ_AFFN_SYSTEM]\t\t= \"system\",",
            "};",
            "",
            "/*",
            " * Per-cpu work items which run for longer than the following threshold are",
            " * automatically considered CPU intensive and excluded from concurrency",
            " * management to prevent them from noticeably delaying other per-cpu work items.",
            " * ULONG_MAX indicates that the user hasn't overridden it with a boot parameter.",
            " * The actual value is initialized in wq_cpu_intensive_thresh_init().",
            " */",
            "static unsigned long wq_cpu_intensive_thresh_us = ULONG_MAX;",
            "module_param_named(cpu_intensive_thresh_us, wq_cpu_intensive_thresh_us, ulong, 0644);",
            "",
            "/* see the comment above the definition of WQ_POWER_EFFICIENT */",
            "static bool wq_power_efficient = IS_ENABLED(CONFIG_WQ_POWER_EFFICIENT_DEFAULT);",
            "module_param_named(power_efficient, wq_power_efficient, bool, 0444);",
            "",
            "static bool wq_online;\t\t\t/* can kworkers be created yet? */",
            "",
            "/* buf for wq_update_unbound_pod_attrs(), protected by CPU hotplug exclusion */",
            "static struct workqueue_attrs *wq_update_pod_attrs_buf;",
            "",
            "static DEFINE_MUTEX(wq_pool_mutex);\t/* protects pools and workqueues list */",
            "static DEFINE_MUTEX(wq_pool_attach_mutex); /* protects worker attach/detach */",
            "static DEFINE_RAW_SPINLOCK(wq_mayday_lock);\t/* protects wq->maydays list */",
            "/* wait for manager to go away */",
            "static struct rcuwait manager_wait = __RCUWAIT_INITIALIZER(manager_wait);",
            "",
            "static LIST_HEAD(workqueues);\t\t/* PR: list of all workqueues */",
            "static bool workqueue_freezing;\t\t/* PL: have wqs started freezing? */",
            "",
            "/* PL&A: allowable cpus for unbound wqs and work items */",
            "static cpumask_var_t wq_unbound_cpumask;",
            "",
            "/* PL: user requested unbound cpumask via sysfs */",
            "static cpumask_var_t wq_requested_unbound_cpumask;",
            "",
            "/* PL: isolated cpumask to be excluded from unbound cpumask */",
            "static cpumask_var_t wq_isolated_cpumask;",
            "",
            "/* for further constrain wq_unbound_cpumask by cmdline parameter*/",
            "static struct cpumask wq_cmdline_cpumask __initdata;",
            "",
            "/* CPU where unbound work was last round robin scheduled from this CPU */",
            "static DEFINE_PER_CPU(int, wq_rr_cpu_last);",
            "",
            "/*",
            " * Local execution of unbound work items is no longer guaranteed.  The",
            " * following always forces round-robin CPU selection on unbound work items",
            " * to uncover usages which depend on it.",
            " */",
            "#ifdef CONFIG_DEBUG_WQ_FORCE_RR_CPU",
            "static bool wq_debug_force_rr_cpu = true;",
            "#else",
            "static bool wq_debug_force_rr_cpu = false;",
            "#endif",
            "module_param_named(debug_force_rr_cpu, wq_debug_force_rr_cpu, bool, 0644);",
            "",
            "/* the per-cpu worker pools */",
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS], cpu_worker_pools);",
            "",
            "static DEFINE_IDR(worker_pool_idr);\t/* PR: idr of all pools */",
            "",
            "/* PL: hash of all unbound pools keyed by pool->attrs */",
            "static DEFINE_HASHTABLE(unbound_pool_hash, UNBOUND_POOL_HASH_ORDER);",
            "",
            "/* I: attributes used when instantiating standard unbound pools on demand */",
            "static struct workqueue_attrs *unbound_std_wq_attrs[NR_STD_WORKER_POOLS];",
            "",
            "/* I: attributes used when instantiating ordered pools on demand */",
            "static struct workqueue_attrs *ordered_wq_attrs[NR_STD_WORKER_POOLS];",
            "",
            "/*",
            " * I: kthread_worker to release pwq's. pwq release needs to be bounced to a",
            " * process context while holding a pool lock. Bounce to a dedicated kthread",
            " * worker to avoid A-A deadlocks.",
            " */",
            "static struct kthread_worker *pwq_release_worker __ro_after_init;",
            "",
            "struct workqueue_struct *system_wq __ro_after_init;",
            "EXPORT_SYMBOL(system_wq);",
            "struct workqueue_struct *system_highpri_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_highpri_wq);",
            "struct workqueue_struct *system_long_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_long_wq);",
            "struct workqueue_struct *system_unbound_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_unbound_wq);",
            "struct workqueue_struct *system_freezable_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_freezable_wq);",
            "struct workqueue_struct *system_power_efficient_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_power_efficient_wq);",
            "struct workqueue_struct *system_freezable_power_efficient_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_freezable_power_efficient_wq);",
            "",
            "static int worker_thread(void *__worker);",
            "static void workqueue_sysfs_unregister(struct workqueue_struct *wq);",
            "static void show_pwq(struct pool_workqueue *pwq);",
            "static void show_one_worker_pool(struct worker_pool *pool);",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/workqueue.h>",
            "",
            "#define assert_rcu_or_pool_mutex()\t\t\t\t\t\\",
            "\tRCU_LOCKDEP_WARN(!rcu_read_lock_held() &&\t\t\t\\",
            "\t\t\t !lockdep_is_held(&wq_pool_mutex),\t\t\\",
            "\t\t\t \"RCU or wq_pool_mutex should be held\")",
            "",
            "#define assert_rcu_or_wq_mutex_or_pool_mutex(wq)\t\t\t\\",
            "\tRCU_LOCKDEP_WARN(!rcu_read_lock_held() &&\t\t\t\\",
            "\t\t\t !lockdep_is_held(&wq->mutex) &&\t\t\\",
            "\t\t\t !lockdep_is_held(&wq_pool_mutex),\t\t\\",
            "\t\t\t \"RCU, wq->mutex or wq_pool_mutex should be held\")",
            "",
            "#define for_each_cpu_worker_pool(pool, cpu)\t\t\t\t\\",
            "\tfor ((pool) = &per_cpu(cpu_worker_pools, cpu)[0];\t\t\\",
            "\t     (pool) < &per_cpu(cpu_worker_pools, cpu)[NR_STD_WORKER_POOLS]; \\",
            "\t     (pool)++)",
            "",
            "/**",
            " * for_each_pool - iterate through all worker_pools in the system",
            " * @pool: iteration cursor",
            " * @pi: integer used for iteration",
            " *",
            " * This must be called either with wq_pool_mutex held or RCU read",
            " * locked.  If the pool needs to be used beyond the locking in effect, the",
            " * caller is responsible for guaranteeing that the pool stays online.",
            " *",
            " * The if/else clause exists only for the lockdep assertion and can be",
            " * ignored.",
            " */",
            "#define for_each_pool(pool, pi)\t\t\t\t\t\t\\",
            "\tidr_for_each_entry(&worker_pool_idr, pool, pi)\t\t\t\\",
            "\t\tif (({ assert_rcu_or_pool_mutex(); false; })) { }\t\\",
            "\t\telse",
            "",
            "/**",
            " * for_each_pool_worker - iterate through all workers of a worker_pool",
            " * @worker: iteration cursor",
            " * @pool: worker_pool to iterate workers of",
            " *",
            " * This must be called with wq_pool_attach_mutex.",
            " *",
            " * The if/else clause exists only for the lockdep assertion and can be",
            " * ignored.",
            " */",
            "#define for_each_pool_worker(worker, pool)\t\t\t\t\\",
            "\tlist_for_each_entry((worker), &(pool)->workers, node)\t\t\\",
            "\t\tif (({ lockdep_assert_held(&wq_pool_attach_mutex); false; })) { } \\",
            "\t\telse",
            "",
            "/**",
            " * for_each_pwq - iterate through all pool_workqueues of the specified workqueue",
            " * @pwq: iteration cursor",
            " * @wq: the target workqueue",
            " *",
            " * This must be called either with wq->mutex held or RCU read locked.",
            " * If the pwq needs to be used beyond the locking in effect, the caller is",
            " * responsible for guaranteeing that the pwq stays online.",
            " *",
            " * The if/else clause exists only for the lockdep assertion and can be",
            " * ignored.",
            " */",
            "#define for_each_pwq(pwq, wq)\t\t\t\t\t\t\\",
            "\tlist_for_each_entry_rcu((pwq), &(wq)->pwqs, pwqs_node,\t\t\\",
            "\t\t\t\t lockdep_is_held(&(wq->mutex)))",
            "",
            "#ifdef CONFIG_DEBUG_OBJECTS_WORK",
            "",
            "static const struct debug_obj_descr work_debug_descr;",
            "",
            "static void *work_debug_hint(void *addr)",
            "{",
            "\treturn ((struct work_struct *) addr)->func;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义工作队列核心数据结构和全局变量，包括worker_pool和pool_workqueue结构体，用于管理工作者线程池及其属性，包含池标志位、工作项统计信息、锁保护字段和RCU相关成员，为异步任务调度提供基础设施。",
          "similarity": 0.46372902393341064
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/workqueue.c",
          "start_line": 2965,
          "end_line": 3090,
          "content": [
            "static void check_flush_dependency(struct workqueue_struct *target_wq,",
            "\t\t\t\t   struct work_struct *target_work,",
            "\t\t\t\t   bool from_cancel)",
            "{",
            "\twork_func_t target_func;",
            "\tstruct worker *worker;",
            "",
            "\tif (from_cancel || target_wq->flags & WQ_MEM_RECLAIM)",
            "\t\treturn;",
            "",
            "\tworker = current_wq_worker();",
            "\ttarget_func = target_work ? target_work->func : NULL;",
            "",
            "\tWARN_ONCE(current->flags & PF_MEMALLOC,",
            "\t\t  \"workqueue: PF_MEMALLOC task %d(%s) is flushing !WQ_MEM_RECLAIM %s:%ps\",",
            "\t\t  current->pid, current->comm, target_wq->name, target_func);",
            "\tWARN_ONCE(worker && ((worker->current_pwq->wq->flags &",
            "\t\t\t      (WQ_MEM_RECLAIM | __WQ_LEGACY)) == WQ_MEM_RECLAIM),",
            "\t\t  \"workqueue: WQ_MEM_RECLAIM %s:%ps is flushing !WQ_MEM_RECLAIM %s:%ps\",",
            "\t\t  worker->current_pwq->wq->name, worker->current_func,",
            "\t\t  target_wq->name, target_func);",
            "}",
            "static void wq_barrier_func(struct work_struct *work)",
            "{",
            "\tstruct wq_barrier *barr = container_of(work, struct wq_barrier, work);",
            "\tcomplete(&barr->done);",
            "}",
            "static void insert_wq_barrier(struct pool_workqueue *pwq,",
            "\t\t\t      struct wq_barrier *barr,",
            "\t\t\t      struct work_struct *target, struct worker *worker)",
            "{",
            "\tunsigned int work_flags = 0;",
            "\tunsigned int work_color;",
            "\tstruct list_head *head;",
            "",
            "\t/*",
            "\t * debugobject calls are safe here even with pool->lock locked",
            "\t * as we know for sure that this will not trigger any of the",
            "\t * checks and call back into the fixup functions where we",
            "\t * might deadlock.",
            "\t */",
            "\tINIT_WORK_ONSTACK(&barr->work, wq_barrier_func);",
            "\t__set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(&barr->work));",
            "",
            "\tinit_completion_map(&barr->done, &target->lockdep_map);",
            "",
            "\tbarr->task = current;",
            "",
            "\t/* The barrier work item does not participate in pwq->nr_active. */",
            "\twork_flags |= WORK_STRUCT_INACTIVE;",
            "",
            "\t/*",
            "\t * If @target is currently being executed, schedule the",
            "\t * barrier to the worker; otherwise, put it after @target.",
            "\t */",
            "\tif (worker) {",
            "\t\thead = worker->scheduled.next;",
            "\t\twork_color = worker->current_color;",
            "\t} else {",
            "\t\tunsigned long *bits = work_data_bits(target);",
            "",
            "\t\thead = target->entry.next;",
            "\t\t/* there can already be other linked works, inherit and set */",
            "\t\twork_flags |= *bits & WORK_STRUCT_LINKED;",
            "\t\twork_color = get_work_color(*bits);",
            "\t\t__set_bit(WORK_STRUCT_LINKED_BIT, bits);",
            "\t}",
            "",
            "\tpwq->nr_in_flight[work_color]++;",
            "\twork_flags |= work_color_to_flags(work_color);",
            "",
            "\tinsert_work(pwq, &barr->work, head, work_flags);",
            "}",
            "static bool flush_workqueue_prep_pwqs(struct workqueue_struct *wq,",
            "\t\t\t\t      int flush_color, int work_color)",
            "{",
            "\tbool wait = false;",
            "\tstruct pool_workqueue *pwq;",
            "\tstruct worker_pool *current_pool = NULL;",
            "",
            "\tif (flush_color >= 0) {",
            "\t\tWARN_ON_ONCE(atomic_read(&wq->nr_pwqs_to_flush));",
            "\t\tatomic_set(&wq->nr_pwqs_to_flush, 1);",
            "\t}",
            "",
            "\t/*",
            "\t * For unbound workqueue, pwqs will map to only a few pools.",
            "\t * Most of the time, pwqs within the same pool will be linked",
            "\t * sequentially to wq->pwqs by cpu index. So in the majority",
            "\t * of pwq iters, the pool is the same, only doing lock/unlock",
            "\t * if the pool has changed. This can largely reduce expensive",
            "\t * lock operations.",
            "\t */",
            "\tfor_each_pwq(pwq, wq) {",
            "\t\tif (current_pool != pwq->pool) {",
            "\t\t\tif (likely(current_pool))",
            "\t\t\t\traw_spin_unlock_irq(&current_pool->lock);",
            "\t\t\tcurrent_pool = pwq->pool;",
            "\t\t\traw_spin_lock_irq(&current_pool->lock);",
            "\t\t}",
            "",
            "\t\tif (flush_color >= 0) {",
            "\t\t\tWARN_ON_ONCE(pwq->flush_color != -1);",
            "",
            "\t\t\tif (pwq->nr_in_flight[flush_color]) {",
            "\t\t\t\tpwq->flush_color = flush_color;",
            "\t\t\t\tatomic_inc(&wq->nr_pwqs_to_flush);",
            "\t\t\t\twait = true;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tif (work_color >= 0) {",
            "\t\t\tWARN_ON_ONCE(work_color != work_next_color(pwq->work_color));",
            "\t\t\tpwq->work_color = work_color;",
            "\t\t}",
            "",
            "\t}",
            "",
            "\tif (current_pool)",
            "\t\traw_spin_unlock_irq(&current_pool->lock);",
            "",
            "\tif (flush_color >= 0 && atomic_dec_and_test(&wq->nr_pwqs_to_flush))",
            "\t\tcomplete(&wq->first_flusher->done);",
            "",
            "\treturn wait;",
            "}"
          ],
          "function_name": "check_flush_dependency, wq_barrier_func, insert_wq_barrier, flush_workqueue_prep_pwqs",
          "description": "提供工作队列刷新时的依赖校验与屏障插入功能，用于协调不同颜色工作项的刷新顺序和状态同步",
          "similarity": 0.4570070803165436
        },
        {
          "chunk_id": 21,
          "file_path": "kernel/workqueue.c",
          "start_line": 4591,
          "end_line": 4693,
          "content": [
            "static int alloc_and_link_pwqs(struct workqueue_struct *wq)",
            "{",
            "\tbool highpri = wq->flags & WQ_HIGHPRI;",
            "\tint cpu, ret;",
            "",
            "\twq->cpu_pwq = alloc_percpu(struct pool_workqueue *);",
            "\tif (!wq->cpu_pwq)",
            "\t\tgoto enomem;",
            "",
            "\tif (!(wq->flags & WQ_UNBOUND)) {",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tstruct pool_workqueue **pwq_p =",
            "\t\t\t\tper_cpu_ptr(wq->cpu_pwq, cpu);",
            "\t\t\tstruct worker_pool *pool =",
            "\t\t\t\t&(per_cpu_ptr(cpu_worker_pools, cpu)[highpri]);",
            "",
            "\t\t\t*pwq_p = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL,",
            "\t\t\t\t\t\t       pool->node);",
            "\t\t\tif (!*pwq_p)",
            "\t\t\t\tgoto enomem;",
            "",
            "\t\t\tinit_pwq(*pwq_p, wq, pool);",
            "",
            "\t\t\tmutex_lock(&wq->mutex);",
            "\t\t\tlink_pwq(*pwq_p);",
            "\t\t\tmutex_unlock(&wq->mutex);",
            "\t\t}",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tcpus_read_lock();",
            "\tif (wq->flags & __WQ_ORDERED) {",
            "\t\tret = apply_workqueue_attrs(wq, ordered_wq_attrs[highpri]);",
            "\t\t/* there should only be single pwq for ordering guarantee */",
            "\t\tWARN(!ret && (wq->pwqs.next != &wq->dfl_pwq->pwqs_node ||",
            "\t\t\t      wq->pwqs.prev != &wq->dfl_pwq->pwqs_node),",
            "\t\t     \"ordering guarantee broken for workqueue %s\\n\", wq->name);",
            "\t} else {",
            "\t\tret = apply_workqueue_attrs(wq, unbound_std_wq_attrs[highpri]);",
            "\t}",
            "\tcpus_read_unlock();",
            "",
            "\t/* for unbound pwq, flush the pwq_release_worker ensures that the",
            "\t * pwq_release_workfn() completes before calling kfree(wq).",
            "\t */",
            "\tif (ret)",
            "\t\tkthread_flush_worker(pwq_release_worker);",
            "",
            "\treturn ret;",
            "",
            "enomem:",
            "\tif (wq->cpu_pwq) {",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tstruct pool_workqueue *pwq = *per_cpu_ptr(wq->cpu_pwq, cpu);",
            "",
            "\t\t\tif (pwq)",
            "\t\t\t\tkmem_cache_free(pwq_cache, pwq);",
            "\t\t}",
            "\t\tfree_percpu(wq->cpu_pwq);",
            "\t\twq->cpu_pwq = NULL;",
            "\t}",
            "\treturn -ENOMEM;",
            "}",
            "static int wq_clamp_max_active(int max_active, unsigned int flags,",
            "\t\t\t       const char *name)",
            "{",
            "\tif (max_active < 1 || max_active > WQ_MAX_ACTIVE)",
            "\t\tpr_warn(\"workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\\n\",",
            "\t\t\tmax_active, name, 1, WQ_MAX_ACTIVE);",
            "",
            "\treturn clamp_val(max_active, 1, WQ_MAX_ACTIVE);",
            "}",
            "static int init_rescuer(struct workqueue_struct *wq)",
            "{",
            "\tstruct worker *rescuer;",
            "\tint ret;",
            "",
            "\tif (!(wq->flags & WQ_MEM_RECLAIM))",
            "\t\treturn 0;",
            "",
            "\trescuer = alloc_worker(NUMA_NO_NODE);",
            "\tif (!rescuer) {",
            "\t\tpr_err(\"workqueue: Failed to allocate a rescuer for wq \\\"%s\\\"\\n\",",
            "\t\t       wq->name);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\trescuer->rescue_wq = wq;",
            "\trescuer->task = kthread_create(rescuer_thread, rescuer, \"kworker/R-%s\", wq->name);",
            "\tif (IS_ERR(rescuer->task)) {",
            "\t\tret = PTR_ERR(rescuer->task);",
            "\t\tpr_err(\"workqueue: Failed to create a rescuer kthread for wq \\\"%s\\\": %pe\",",
            "\t\t       wq->name, ERR_PTR(ret));",
            "\t\tkfree(rescuer);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\twq->rescuer = rescuer;",
            "\tkthread_bind_mask(rescuer->task, cpu_possible_mask);",
            "\twake_up_process(rescuer->task);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "alloc_and_link_pwqs, wq_clamp_max_active, init_rescuer",
          "description": "包含分配并链接池工作队列的alloc_and_link_pwqs函数，wq_clamp_max_active用于限制最大活跃任务数范围，init_rescuer初始化救援线程以处理内存回收场景下的异常情况。",
          "similarity": 0.4469435214996338
        }
      ]
    },
    {
      "source_file": "kernel/watch_queue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:50:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `watch_queue.c`\n\n---\n\n# watch_queue.c 技术文档\n\n## 文件概述\n\n`watch_queue.c` 实现了 Linux 内核中的**监视队列**（Watch Queue）机制，这是一种基于管道（pipe）构建的通用事件通知系统。该机制允许内核子系统（如文件系统、密钥管理、设备驱动等）向用户空间异步发送结构化通知。用户空间通过创建特殊类型的管道并关联监视队列，即可接收来自内核的各类事件通知。该文件定义了通知的投递、过滤、缓冲管理及与管道集成的核心逻辑。\n\n## 核心功能\n\n### 主要函数\n\n- **`__post_watch_notification()`**  \n  核心通知投递函数。遍历指定 `watch_list` 中所有匹配 `id` 的监视器（`watch`），对每个关联的 `watch_queue` 应用过滤规则、安全检查，并将通知写入底层管道。\n\n- **`post_one_notification()`**  \n  将单个通知写入指定 `watch_queue` 的底层管道缓冲区。负责从预分配的通知页中获取空闲槽位、填充数据、更新管道头指针并唤醒等待读取的进程。\n\n- **`filter_watch_notification()`**  \n  根据 `watch_filter` 中的类型、子类型和信息掩码规则，判断是否允许特定通知通过。\n\n- **`watch_queue_set_size()`**  \n  为监视队列分配预分配的通知缓冲区（页数组和位图），并调整底层管道的环形缓冲区大小。\n\n- **`watch_queue_pipe_buf_release()`**  \n  管道缓冲区释放回调。当用户空间读取完通知后，将对应的通知槽位在位图中标记为空闲，供后续复用。\n\n### 关键数据结构\n\n- **`struct watch_queue`**  \n  表示一个监视队列，包含：\n  - 指向底层 `pipe_inode_info` 的指针\n  - 预分配的通知页数组（`notes`）\n  - 通知槽位空闲位图（`notes_bitmap`）\n  - 通知过滤器（`filter`）\n  - 保护锁（`lock`）\n\n- **`struct watch_notification`**  \n  通用通知记录格式，包含类型（`type`）、子类型（`subtype`）、信息字段（`info`，含长度和ID）及可变负载。\n\n- **`struct watch_filter` / `struct watch_type_filter`**  \n  定义通知过滤规则，支持按类型、子类型及信息字段的位掩码进行精确过滤。\n\n- **`watch_queue_pipe_buf_ops`**  \n  自定义的 `pipe_buf_operations`，用于管理监视队列专用管道缓冲区的生命周期。\n\n## 关键实现\n\n### 基于管道的通知传输\n- 监视队列复用内核管道（`pipe_inode_info`）作为通知传输通道，利用其成熟的读写、轮询、异步通知机制。\n- 通过自定义 `pipe_buf_operations`（`watch_queue_pipe_buf_ops`）实现通知槽位的回收：当用户读取通知后，`release` 回调将对应槽位在 `notes_bitmap` 中置位，标记为空闲。\n\n### 预分配通知缓冲区\n- 通知数据存储在预分配的内核页（`notes`）中，每页划分为多个固定大小（128字节）的槽位（`WATCH_QUEUE_NOTE_SIZE`）。\n- 使用位图（`notes_bitmap`）跟踪槽位使用状态，1 表示空闲。投递通知时通过 `find_first_bit()` 快速查找空闲槽位。\n- 缓冲区大小由用户通过 `watch_queue_set_size()` 设置（1-512个通知），并受管道缓冲区配额限制。\n\n### 通知投递流程\n1. **匹配监视器**：遍历 `watch_list`，查找 `id` 匹配的 `watch`。\n2. **应用过滤**：若队列配置了过滤器，调用 `filter_watch_notification()` 决定是否丢弃。\n3. **安全检查**：调用 LSM 钩子 `security_post_notification()` 进行权限验证。\n4. **写入管道**：\n   - 获取空闲通知槽位，复制通知数据。\n   - 构造 `pipe_buffer` 指向该槽位，设置自定义操作集。\n   - 更新管道 `head` 指针，唤醒等待读取的进程。\n   - 若缓冲区满，标记前一个缓冲区为 `PIPE_BUF_FLAG_LOSS` 表示丢包。\n\n### 并发与同步\n- **RCU 保护**：`watch_list` 和 `watch_queue` 的访问通过 RCU 机制保护，确保遍历时结构体不被释放。\n- **自旋锁**：\n  - `wqueue->lock`：保护 `wqueue` 状态（如 `pipe` 指针有效性）。\n  - `pipe->rd_wait.lock`：保护管道环形缓冲区的读写操作。\n- **原子操作**：管道 `head` 指针使用 `smp_store_release()` 更新，确保与 `pipe_read()` 的同步。\n\n## 依赖关系\n\n- **管道子系统**（`fs/pipe.c`）  \n  依赖管道的核心数据结构（`pipe_inode_info`、`pipe_buffer`）和操作接口（`pipe_buf()`、`pipe_full()`、`generic_pipe_buf_*`）。\n\n- **内存管理**  \n  使用 `alloc_page()`、`kmap_atomic()` 管理通知缓冲区页，`bitmap_alloc()` 管理槽位位图。\n\n- **安全模块**（LSM）  \n  通过 `security_post_notification()` 钩子集成安全策略。\n\n- **用户空间接口**  \n  与 `fs/watch_queue.c` 中的系统调用（如 `watch_queue_set_size()`）协同工作，后者负责创建监视队列并与管道关联。\n\n- **头文件依赖**  \n  `linux/watch_queue.h`（核心数据结构定义）、`linux/pipe_fs_i.h`（管道内部接口）。\n\n## 使用场景\n\n- **文件系统事件监控**  \n  如 `fsnotify` 子系统可通过监视队列向用户空间报告文件访问、修改等事件。\n\n- **密钥管理通知**  \n  内核密钥环（`KEYS`）子系统使用该机制通知密钥状态变更（如过期、撤销）。\n\n- **设备事件上报**  \n  设备驱动可利用监视队列异步上报硬件状态变化或错误事件。\n\n- **通用内核事件分发**  \n  任何需要向特权用户空间守护进程（如 `systemd`）发送结构化事件的内核子系统均可集成此机制。\n\n- **用户空间消费**  \n  应用程序通过 `open(\"/dev/watch_queue\")` 获取监视队列文件描述符，调用 `ioctl()` 设置缓冲区大小和过滤器，然后像读取普通管道一样接收通知。",
      "similarity": 0.4939790368080139,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/watch_queue.c",
          "start_line": 1,
          "end_line": 41,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/* Watch queue and general notification mechanism, built on pipes",
            " *",
            " * Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.",
            " * Written by David Howells (dhowells@redhat.com)",
            " *",
            " * See Documentation/core-api/watch_queue.rst",
            " */",
            "",
            "#define pr_fmt(fmt) \"watchq: \" fmt",
            "#include <linux/module.h>",
            "#include <linux/init.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/printk.h>",
            "#include <linux/miscdevice.h>",
            "#include <linux/fs.h>",
            "#include <linux/mm.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/poll.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/file.h>",
            "#include <linux/security.h>",
            "#include <linux/cred.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/watch_queue.h>",
            "#include <linux/pipe_fs_i.h>",
            "",
            "MODULE_DESCRIPTION(\"Watch queue\");",
            "MODULE_AUTHOR(\"Red Hat, Inc.\");",
            "",
            "#define WATCH_QUEUE_NOTE_SIZE 128",
            "#define WATCH_QUEUE_NOTES_PER_PAGE (PAGE_SIZE / WATCH_QUEUE_NOTE_SIZE)",
            "",
            "/*",
            " * This must be called under the RCU read-lock, which makes",
            " * sure that the wqueue still exists. It can then take the lock,",
            " * and check that the wqueue hasn't been destroyed, which in",
            " * turn makes sure that the notification pipe still exists.",
            " */"
          ],
          "function_name": null,
          "description": "定义了watch_queue模块的头部信息，包含常量WATCH_QUEUE_NOTE_SIZE和NOTES_PER_PAGE，声明模块许可证及作者信息，并引入相关内核头文件，为后续实现提供基础框架。",
          "similarity": 0.4697619676589966
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/watch_queue.c",
          "start_line": 42,
          "end_line": 154,
          "content": [
            "static inline bool lock_wqueue(struct watch_queue *wqueue)",
            "{",
            "\tspin_lock_bh(&wqueue->lock);",
            "\tif (unlikely(!wqueue->pipe)) {",
            "\t\tspin_unlock_bh(&wqueue->lock);",
            "\t\treturn false;",
            "\t}",
            "\treturn true;",
            "}",
            "static inline void unlock_wqueue(struct watch_queue *wqueue)",
            "{",
            "\tspin_unlock_bh(&wqueue->lock);",
            "}",
            "static void watch_queue_pipe_buf_release(struct pipe_inode_info *pipe,",
            "\t\t\t\t\t struct pipe_buffer *buf)",
            "{",
            "\tstruct watch_queue *wqueue = (struct watch_queue *)buf->private;",
            "\tstruct page *page;",
            "\tunsigned int bit;",
            "",
            "\t/* We need to work out which note within the page this refers to, but",
            "\t * the note might have been maximum size, so merely ANDing the offset",
            "\t * off doesn't work.  OTOH, the note must've been more than zero size.",
            "\t */",
            "\tbit = buf->offset + buf->len;",
            "\tif ((bit & (WATCH_QUEUE_NOTE_SIZE - 1)) == 0)",
            "\t\tbit -= WATCH_QUEUE_NOTE_SIZE;",
            "\tbit /= WATCH_QUEUE_NOTE_SIZE;",
            "",
            "\tpage = buf->page;",
            "\tbit += page->index;",
            "",
            "\tset_bit(bit, wqueue->notes_bitmap);",
            "\tgeneric_pipe_buf_release(pipe, buf);",
            "}",
            "static bool post_one_notification(struct watch_queue *wqueue,",
            "\t\t\t\t  struct watch_notification *n)",
            "{",
            "\tvoid *p;",
            "\tstruct pipe_inode_info *pipe = wqueue->pipe;",
            "\tstruct pipe_buffer *buf;",
            "\tstruct page *page;",
            "\tunsigned int head, tail, note, offset, len;",
            "\tbool done = false;",
            "",
            "\tspin_lock_irq(&pipe->rd_wait.lock);",
            "",
            "\thead = pipe->head;",
            "\ttail = pipe->tail;",
            "\tif (pipe_full(head, tail, pipe->ring_size))",
            "\t\tgoto lost;",
            "",
            "\tnote = find_first_bit(wqueue->notes_bitmap, wqueue->nr_notes);",
            "\tif (note >= wqueue->nr_notes)",
            "\t\tgoto lost;",
            "",
            "\tpage = wqueue->notes[note / WATCH_QUEUE_NOTES_PER_PAGE];",
            "\toffset = note % WATCH_QUEUE_NOTES_PER_PAGE * WATCH_QUEUE_NOTE_SIZE;",
            "\tget_page(page);",
            "\tlen = n->info & WATCH_INFO_LENGTH;",
            "\tp = kmap_atomic(page);",
            "\tmemcpy(p + offset, n, len);",
            "\tkunmap_atomic(p);",
            "",
            "\tbuf = pipe_buf(pipe, head);",
            "\tbuf->page = page;",
            "\tbuf->private = (unsigned long)wqueue;",
            "\tbuf->ops = &watch_queue_pipe_buf_ops;",
            "\tbuf->offset = offset;",
            "\tbuf->len = len;",
            "\tbuf->flags = PIPE_BUF_FLAG_WHOLE;",
            "\tsmp_store_release(&pipe->head, head + 1); /* vs pipe_read() */",
            "",
            "\tif (!test_and_clear_bit(note, wqueue->notes_bitmap)) {",
            "\t\tspin_unlock_irq(&pipe->rd_wait.lock);",
            "\t\tBUG();",
            "\t}",
            "\twake_up_interruptible_sync_poll_locked(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);",
            "\tdone = true;",
            "",
            "out:",
            "\tspin_unlock_irq(&pipe->rd_wait.lock);",
            "\tif (done)",
            "\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);",
            "\treturn done;",
            "",
            "lost:",
            "\tbuf = pipe_buf(pipe, head - 1);",
            "\tbuf->flags |= PIPE_BUF_FLAG_LOSS;",
            "\tgoto out;",
            "}",
            "static bool filter_watch_notification(const struct watch_filter *wf,",
            "\t\t\t\t      const struct watch_notification *n)",
            "{",
            "\tconst struct watch_type_filter *wt;",
            "\tunsigned int st_bits = sizeof(wt->subtype_filter[0]) * 8;",
            "\tunsigned int st_index = n->subtype / st_bits;",
            "\tunsigned int st_bit = 1U << (n->subtype % st_bits);",
            "\tint i;",
            "",
            "\tif (!test_bit(n->type, wf->type_filter))",
            "\t\treturn false;",
            "",
            "\tfor (i = 0; i < wf->nr_filters; i++) {",
            "\t\twt = &wf->filters[i];",
            "\t\tif (n->type == wt->type &&",
            "\t\t    (wt->subtype_filter[st_index] & st_bit) &&",
            "\t\t    (n->info & wt->info_mask) == wt->info_filter)",
            "\t\t\treturn true;",
            "\t}",
            "",
            "\treturn false; /* If there is a filter, the default is to reject. */",
            "}"
          ],
          "function_name": "lock_wqueue, unlock_wqueue, watch_queue_pipe_buf_release, post_one_notification, filter_watch_notification",
          "description": "实现了watch_queue的锁操作、缓冲区释放、通知提交及过滤逻辑。lock_wqueue/unlock_wqueue用于保护队列访问，watch_queue_pipe_buf_release处理缓冲区回收并更新位图，post_one_notification将通知数据写入管道，filter_watch_notification进行类型和子类型的匹配判断。",
          "similarity": 0.4693271219730377
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/watch_queue.c",
          "start_line": 193,
          "end_line": 304,
          "content": [
            "void __post_watch_notification(struct watch_list *wlist,",
            "\t\t\t       struct watch_notification *n,",
            "\t\t\t       const struct cred *cred,",
            "\t\t\t       u64 id)",
            "{",
            "\tconst struct watch_filter *wf;",
            "\tstruct watch_queue *wqueue;",
            "\tstruct watch *watch;",
            "",
            "\tif (((n->info & WATCH_INFO_LENGTH) >> WATCH_INFO_LENGTH__SHIFT) == 0) {",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\trcu_read_lock();",
            "",
            "\thlist_for_each_entry_rcu(watch, &wlist->watchers, list_node) {",
            "\t\tif (watch->id != id)",
            "\t\t\tcontinue;",
            "\t\tn->info &= ~WATCH_INFO_ID;",
            "\t\tn->info |= watch->info_id;",
            "",
            "\t\twqueue = rcu_dereference(watch->queue);",
            "\t\twf = rcu_dereference(wqueue->filter);",
            "\t\tif (wf && !filter_watch_notification(wf, n))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (security_post_notification(watch->cred, cred, n) < 0)",
            "\t\t\tcontinue;",
            "",
            "\t\tif (lock_wqueue(wqueue)) {",
            "\t\t\tpost_one_notification(wqueue, n);",
            "\t\t\tunlock_wqueue(wqueue);",
            "\t\t}",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "}",
            "long watch_queue_set_size(struct pipe_inode_info *pipe, unsigned int nr_notes)",
            "{",
            "\tstruct watch_queue *wqueue = pipe->watch_queue;",
            "\tstruct page **pages;",
            "\tunsigned long *bitmap;",
            "\tunsigned long user_bufs;",
            "\tint ret, i, nr_pages;",
            "",
            "\tif (!wqueue)",
            "\t\treturn -ENODEV;",
            "\tif (wqueue->notes)",
            "\t\treturn -EBUSY;",
            "",
            "\tif (nr_notes < 1 ||",
            "\t    nr_notes > 512) /* TODO: choose a better hard limit */",
            "\t\treturn -EINVAL;",
            "",
            "\tnr_pages = (nr_notes + WATCH_QUEUE_NOTES_PER_PAGE - 1);",
            "\tnr_pages /= WATCH_QUEUE_NOTES_PER_PAGE;",
            "\tuser_bufs = account_pipe_buffers(pipe->user, pipe->nr_accounted, nr_pages);",
            "",
            "\tif (nr_pages > pipe->max_usage &&",
            "\t    (too_many_pipe_buffers_hard(user_bufs) ||",
            "\t     too_many_pipe_buffers_soft(user_bufs)) &&",
            "\t    pipe_is_unprivileged_user()) {",
            "\t\tret = -EPERM;",
            "\t\tgoto error;",
            "\t}",
            "",
            "\tnr_notes = nr_pages * WATCH_QUEUE_NOTES_PER_PAGE;",
            "\tret = pipe_resize_ring(pipe, roundup_pow_of_two(nr_notes));",
            "\tif (ret < 0)",
            "\t\tgoto error;",
            "",
            "\t/*",
            "\t * pipe_resize_ring() does not update nr_accounted for watch_queue",
            "\t * pipes, because the above vastly overprovisions. Set nr_accounted on",
            "\t * and max_usage this pipe to the number that was actually charged to",
            "\t * the user above via account_pipe_buffers.",
            "\t */",
            "\tpipe->max_usage = nr_pages;",
            "\tpipe->nr_accounted = nr_pages;",
            "",
            "\tret = -ENOMEM;",
            "\tpages = kcalloc(sizeof(struct page *), nr_pages, GFP_KERNEL);",
            "\tif (!pages)",
            "\t\tgoto error;",
            "",
            "\tfor (i = 0; i < nr_pages; i++) {",
            "\t\tpages[i] = alloc_page(GFP_KERNEL);",
            "\t\tif (!pages[i])",
            "\t\t\tgoto error_p;",
            "\t\tpages[i]->index = i * WATCH_QUEUE_NOTES_PER_PAGE;",
            "\t}",
            "",
            "\tbitmap = bitmap_alloc(nr_notes, GFP_KERNEL);",
            "\tif (!bitmap)",
            "\t\tgoto error_p;",
            "",
            "\tbitmap_fill(bitmap, nr_notes);",
            "\twqueue->notes = pages;",
            "\twqueue->notes_bitmap = bitmap;",
            "\twqueue->nr_pages = nr_pages;",
            "\twqueue->nr_notes = nr_notes;",
            "\treturn 0;",
            "",
            "error_p:",
            "\twhile (--i >= 0)",
            "\t\t__free_page(pages[i]);",
            "\tkfree(pages);",
            "error:",
            "\t(void) account_pipe_buffers(pipe->user, nr_pages, pipe->nr_accounted);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__post_watch_notification, watch_queue_set_size",
          "description": "__post_watch_notification遍历watch列表并应用过滤器后提交通知，watch_queue_set_size动态调整管道容量，通过计算所需页数和位图分配，限制最大容量为512个笔记，支持扩展性需求。",
          "similarity": 0.4274917542934418
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/watch_queue.c",
          "start_line": 602,
          "end_line": 680,
          "content": [
            "void watch_queue_clear(struct watch_queue *wqueue)",
            "{",
            "\tstruct watch_list *wlist;",
            "\tstruct watch *watch;",
            "\tbool release;",
            "",
            "\trcu_read_lock();",
            "\tspin_lock_bh(&wqueue->lock);",
            "",
            "\t/*",
            "\t * This pipe can be freed by callers like free_pipe_info().",
            "\t * Removing this reference also prevents new notifications.",
            "\t */",
            "\twqueue->pipe = NULL;",
            "",
            "\twhile (!hlist_empty(&wqueue->watches)) {",
            "\t\twatch = hlist_entry(wqueue->watches.first, struct watch, queue_node);",
            "\t\thlist_del_init_rcu(&watch->queue_node);",
            "\t\t/* We now own a ref on the watch. */",
            "\t\tspin_unlock_bh(&wqueue->lock);",
            "",
            "\t\t/* We can't do the next bit under the queue lock as we need to",
            "\t\t * get the list lock - which would cause a deadlock if someone",
            "\t\t * was removing from the opposite direction at the same time or",
            "\t\t * posting a notification.",
            "\t\t */",
            "\t\twlist = rcu_dereference(watch->watch_list);",
            "\t\tif (wlist) {",
            "\t\t\tvoid (*release_watch)(struct watch *);",
            "",
            "\t\t\tspin_lock(&wlist->lock);",
            "",
            "\t\t\trelease = !hlist_unhashed(&watch->list_node);",
            "\t\t\tif (release) {",
            "\t\t\t\thlist_del_init_rcu(&watch->list_node);",
            "\t\t\t\trcu_assign_pointer(watch->watch_list, NULL);",
            "",
            "\t\t\t\t/* We now own a second ref on the watch. */",
            "\t\t\t}",
            "",
            "\t\t\trelease_watch = wlist->release_watch;",
            "\t\t\tspin_unlock(&wlist->lock);",
            "",
            "\t\t\tif (release) {",
            "\t\t\t\tif (release_watch) {",
            "\t\t\t\t\trcu_read_unlock();",
            "\t\t\t\t\t/* This might need to call dput(), so",
            "\t\t\t\t\t * we have to drop all the locks.",
            "\t\t\t\t\t */",
            "\t\t\t\t\t(*release_watch)(watch);",
            "\t\t\t\t\trcu_read_lock();",
            "\t\t\t\t}",
            "\t\t\t\tput_watch(watch);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tput_watch(watch);",
            "\t\tspin_lock_bh(&wqueue->lock);",
            "\t}",
            "",
            "\tspin_unlock_bh(&wqueue->lock);",
            "\trcu_read_unlock();",
            "}",
            "int watch_queue_init(struct pipe_inode_info *pipe)",
            "{",
            "\tstruct watch_queue *wqueue;",
            "",
            "\twqueue = kzalloc(sizeof(*wqueue), GFP_KERNEL);",
            "\tif (!wqueue)",
            "\t\treturn -ENOMEM;",
            "",
            "\twqueue->pipe = pipe;",
            "\tkref_init(&wqueue->usage);",
            "\tspin_lock_init(&wqueue->lock);",
            "\tINIT_HLIST_HEAD(&wqueue->watches);",
            "",
            "\tpipe->watch_queue = wqueue;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "watch_queue_clear, watch_queue_init",
          "description": "该代码实现了监视队列的初始化与清理功能。  \n`watch_queue_clear`通过RCU和自旋锁机制安全地移除所有监视项并释放资源，`watch_queue_init`初始化监视队列结构并绑定至管道对象。  \n上下文不完整：`release_watch`等关键函数依赖外部定义，部分RCU回调逻辑未完全展示。",
          "similarity": 0.4253951609134674
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/watch_queue.c",
          "start_line": 315,
          "end_line": 422,
          "content": [
            "long watch_queue_set_filter(struct pipe_inode_info *pipe,",
            "\t\t\t    struct watch_notification_filter __user *_filter)",
            "{",
            "\tstruct watch_notification_type_filter *tf;",
            "\tstruct watch_notification_filter filter;",
            "\tstruct watch_type_filter *q;",
            "\tstruct watch_filter *wfilter;",
            "\tstruct watch_queue *wqueue = pipe->watch_queue;",
            "\tint ret, nr_filter = 0, i;",
            "",
            "\tif (!wqueue)",
            "\t\treturn -ENODEV;",
            "",
            "\tif (!_filter) {",
            "\t\t/* Remove the old filter */",
            "\t\twfilter = NULL;",
            "\t\tgoto set;",
            "\t}",
            "",
            "\t/* Grab the user's filter specification */",
            "\tif (copy_from_user(&filter, _filter, sizeof(filter)) != 0)",
            "\t\treturn -EFAULT;",
            "\tif (filter.nr_filters == 0 ||",
            "\t    filter.nr_filters > 16 ||",
            "\t    filter.__reserved != 0)",
            "\t\treturn -EINVAL;",
            "",
            "\ttf = memdup_array_user(_filter->filters, filter.nr_filters, sizeof(*tf));",
            "\tif (IS_ERR(tf))",
            "\t\treturn PTR_ERR(tf);",
            "",
            "\tret = -EINVAL;",
            "\tfor (i = 0; i < filter.nr_filters; i++) {",
            "\t\tif ((tf[i].info_filter & ~tf[i].info_mask) ||",
            "\t\t    tf[i].info_mask & WATCH_INFO_LENGTH)",
            "\t\t\tgoto err_filter;",
            "\t\t/* Ignore any unknown types */",
            "\t\tif (tf[i].type >= WATCH_TYPE__NR)",
            "\t\t\tcontinue;",
            "\t\tnr_filter++;",
            "\t}",
            "",
            "\t/* Now we need to build the internal filter from only the relevant",
            "\t * user-specified filters.",
            "\t */",
            "\tret = -ENOMEM;",
            "\twfilter = kzalloc(struct_size(wfilter, filters, nr_filter), GFP_KERNEL);",
            "\tif (!wfilter)",
            "\t\tgoto err_filter;",
            "\twfilter->nr_filters = nr_filter;",
            "",
            "\tq = wfilter->filters;",
            "\tfor (i = 0; i < filter.nr_filters; i++) {",
            "\t\tif (tf[i].type >= WATCH_TYPE__NR)",
            "\t\t\tcontinue;",
            "",
            "\t\tq->type\t\t\t= tf[i].type;",
            "\t\tq->info_filter\t\t= tf[i].info_filter;",
            "\t\tq->info_mask\t\t= tf[i].info_mask;",
            "\t\tq->subtype_filter[0]\t= tf[i].subtype_filter[0];",
            "\t\t__set_bit(q->type, wfilter->type_filter);",
            "\t\tq++;",
            "\t}",
            "",
            "\tkfree(tf);",
            "set:",
            "\tpipe_lock(pipe);",
            "\twfilter = rcu_replace_pointer(wqueue->filter, wfilter,",
            "\t\t\t\t      lockdep_is_held(&pipe->mutex));",
            "\tpipe_unlock(pipe);",
            "\tif (wfilter)",
            "\t\tkfree_rcu(wfilter, rcu);",
            "\treturn 0;",
            "",
            "err_filter:",
            "\tkfree(tf);",
            "\treturn ret;",
            "}",
            "static void __put_watch_queue(struct kref *kref)",
            "{",
            "\tstruct watch_queue *wqueue =",
            "\t\tcontainer_of(kref, struct watch_queue, usage);",
            "\tstruct watch_filter *wfilter;",
            "\tint i;",
            "",
            "\tfor (i = 0; i < wqueue->nr_pages; i++)",
            "\t\t__free_page(wqueue->notes[i]);",
            "\tkfree(wqueue->notes);",
            "\tbitmap_free(wqueue->notes_bitmap);",
            "",
            "\twfilter = rcu_access_pointer(wqueue->filter);",
            "\tif (wfilter)",
            "\t\tkfree_rcu(wfilter, rcu);",
            "\tkfree_rcu(wqueue, rcu);",
            "}",
            "void put_watch_queue(struct watch_queue *wqueue)",
            "{",
            "\tkref_put(&wqueue->usage, __put_watch_queue);",
            "}",
            "static void free_watch(struct rcu_head *rcu)",
            "{",
            "\tstruct watch *watch = container_of(rcu, struct watch, rcu);",
            "",
            "\tput_watch_queue(rcu_access_pointer(watch->queue));",
            "\tatomic_dec(&watch->cred->user->nr_watches);",
            "\tput_cred(watch->cred);",
            "\tkfree(watch);",
            "}"
          ],
          "function_name": "watch_queue_set_filter, __put_watch_queue, put_watch_queue, free_watch",
          "description": "watch_queue_set_filter设置过滤规则并转换为内核内部结构，__put_watch_queue释放watch_queue相关资源包括页面、位图和过滤器，put_watch_queue通过引用计数管理watch_queue生命周期，free_watch执行RCU回调完成最终释放。",
          "similarity": 0.40356385707855225
        }
      ]
    }
  ]
}