{
  "query": "对象分配策略",
  "timestamp": "2025-12-26 01:42:02",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.5670138001441956,
      "chunks": [
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.6320809125900269
        },
        {
          "chunk_id": 13,
          "file_path": "mm/mempolicy.c",
          "start_line": 2149,
          "end_line": 2255,
          "content": [
            "static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nnodes;",
            "\tint i;",
            "\tint nid;",
            "",
            "\tnnodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nnodes)",
            "\t\treturn numa_node_id();",
            "\ttarget = ilx % nnodes;",
            "\tnid = first_node(nodemask);",
            "\tfor (i = 0; i < target; i++)",
            "\t\tnid = next_node(nid, nodemask);",
            "\treturn nid;",
            "}",
            "int huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,",
            "\t\tstruct mempolicy **mpol, nodemask_t **nodemask)",
            "{",
            "\tpgoff_t ilx;",
            "\tint nid;",
            "",
            "\tnid = numa_node_id();",
            "\t*mpol = get_vma_policy(vma, addr, hstate_vma(vma)->order, &ilx);",
            "\t*nodemask = policy_nodemask(gfp_flags, *mpol, ilx, &nid);",
            "\treturn nid;",
            "}",
            "bool init_nodemask_of_mempolicy(nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "",
            "\tif (!(mask && current->mempolicy))",
            "\t\treturn false;",
            "",
            "\ttask_lock(current);",
            "\tmempolicy = current->mempolicy;",
            "\tswitch (mempolicy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*mask = mempolicy->nodes;",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tinit_nodemask_of_node(mask, numa_node_id());",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "\ttask_unlock(current);",
            "",
            "\treturn true;",
            "}",
            "bool mempolicy_in_oom_domain(struct task_struct *tsk,",
            "\t\t\t\t\tconst nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "\tbool ret = true;",
            "",
            "\tif (!mask)",
            "\t\treturn ret;",
            "",
            "\ttask_lock(tsk);",
            "\tmempolicy = tsk->mempolicy;",
            "\tif (mempolicy && mempolicy->mode == MPOL_BIND)",
            "\t\tret = nodes_intersects(mempolicy->nodes, *mask);",
            "\ttask_unlock(tsk);",
            "",
            "\treturn ret;",
            "}",
            "static unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tint nodes;",
            "\tunsigned long nr_pages_per_node;",
            "\tint delta;",
            "\tint i;",
            "\tunsigned long nr_allocated;",
            "\tunsigned long total_allocated = 0;",
            "",
            "\tnodes = nodes_weight(pol->nodes);",
            "\tnr_pages_per_node = nr_pages / nodes;",
            "\tdelta = nr_pages - nodes * nr_pages_per_node;",
            "",
            "\tfor (i = 0; i < nodes; i++) {",
            "\t\tif (delta) {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node + 1, NULL,",
            "\t\t\t\t\tpage_array);",
            "\t\t\tdelta--;",
            "\t\t} else {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node, NULL, page_array);",
            "\t\t}",
            "",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t}",
            "",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "interleave_nid, huge_node, init_nodemask_of_mempolicy, mempolicy_in_oom_domain, alloc_pages_bulk_array_interleave",
          "description": "interleave_nid 计算简单交错分配的目标节点；huge_node 结合HugeTLB策略确定大页分配节点；init_nodemask_of_mempolicy 初始化当前进程的内存策略节点掩码；mempolicy_in_oom_domain 检查策略节点是否与OOM域重叠；alloc_pages_bulk_array_interleave 执行批量交错分配。",
          "similarity": 0.6284761428833008
        },
        {
          "chunk_id": 12,
          "file_path": "mm/mempolicy.c",
          "start_line": 2024,
          "end_line": 2135,
          "content": [
            "static unsigned int interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int nid;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "\t/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnid = next_node_in(current->il_prev, policy->nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\tif (nid < MAX_NUMNODES)",
            "\t\tcurrent->il_prev = nid;",
            "\treturn nid;",
            "}",
            "unsigned int mempolicy_slab_node(void)",
            "{",
            "\tstruct mempolicy *policy;",
            "\tint node = numa_mem_id();",
            "",
            "\tif (!in_task())",
            "\t\treturn node;",
            "",
            "\tpolicy = current->mempolicy;",
            "\tif (!policy)",
            "\t\treturn node;",
            "",
            "\tswitch (policy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\treturn first_node(policy->nodes);",
            "",
            "\tcase MPOL_INTERLEAVE:",
            "\t\treturn interleave_nodes(policy);",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn weighted_interleave_nodes(policy);",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t{",
            "\t\tstruct zoneref *z;",
            "",
            "\t\t/*",
            "\t\t * Follow bind policy behavior and start allocation at the",
            "\t\t * first node.",
            "\t\t */",
            "\t\tstruct zonelist *zonelist;",
            "\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);",
            "\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];",
            "\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,",
            "\t\t\t\t\t\t\t&policy->nodes);",
            "\t\treturn z->zone ? zone_to_nid(z->zone) : node;",
            "\t}",
            "\tcase MPOL_LOCAL:",
            "\t\treturn node;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static unsigned int read_once_policy_nodemask(struct mempolicy *pol,",
            "\t\t\t\t\t      nodemask_t *mask)",
            "{",
            "\t/*",
            "\t * barrier stabilizes the nodemask locally so that it can be iterated",
            "\t * over safely without concern for changes. Allocators validate node",
            "\t * selection does not violate mems_allowed, so this is safe.",
            "\t */",
            "\tbarrier();",
            "\tmemcpy(mask, &pol->nodes, sizeof(nodemask_t));",
            "\tbarrier();",
            "\treturn nodes_weight(*mask);",
            "}",
            "static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nr_nodes;",
            "\tu8 *table = NULL;",
            "\tunsigned int weight_total = 0;",
            "\tu8 weight;",
            "\tint nid = 0;",
            "",
            "\tnr_nodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nr_nodes)",
            "\t\treturn numa_node_id();",
            "",
            "\trcu_read_lock();",
            "",
            "\tstate = rcu_dereference(wi_state);",
            "\t/* Uninitialized wi_state means we should assume all weights are 1 */",
            "\tif (state)",
            "\t\ttable = state->iw_table;",
            "",
            "\t/* calculate the total weight */",
            "\tfor_each_node_mask(nid, nodemask)",
            "\t\tweight_total += table ? table[nid] : 1;",
            "",
            "\t/* Calculate the node offset based on totals */",
            "\ttarget = ilx % weight_total;",
            "\tnid = first_node(nodemask);",
            "\twhile (target) {",
            "\t\t/* detect system default usage */",
            "\t\tweight = table ? table[nid] : 1;",
            "\t\tif (target < weight)",
            "\t\t\tbreak;",
            "\t\ttarget -= weight;",
            "\t\tnid = next_node_in(nid, nodemask);",
            "\t}",
            "\trcu_read_unlock();",
            "\treturn nid;",
            "}"
          ],
          "function_name": "interleave_nodes, mempolicy_slab_node, read_once_policy_nodemask, weighted_interleave_nid",
          "description": "interleave_nodes 计算交错分配的下一个节点；mempolicy_slab_node 根据内存策略返回Slab分配的节点；read_once_policy_nodemask 安全读取策略节点掩码；weighted_interleave_nid 基于权重计算加权交错分配的目标节点。",
          "similarity": 0.6155216693878174
        },
        {
          "chunk_id": 14,
          "file_path": "mm/mempolicy.c",
          "start_line": 2513,
          "end_line": 2629,
          "content": [
            "static unsigned long alloc_pages_bulk_array_weighted_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tstruct task_struct *me = current;",
            "\tunsigned int cpuset_mems_cookie;",
            "\tunsigned long total_allocated = 0;",
            "\tunsigned long nr_allocated = 0;",
            "\tunsigned long rounds;",
            "\tunsigned long node_pages, delta;",
            "\tu8 *weights, weight;",
            "\tunsigned int weight_total = 0;",
            "\tunsigned long rem_pages = nr_pages;",
            "\tnodemask_t nodes;",
            "\tint nnodes, node;",
            "\tint resume_node = MAX_NUMNODES - 1;",
            "\tu8 resume_weight = 0;",
            "\tint prev_node;",
            "\tint i;",
            "",
            "\tif (!nr_pages)",
            "\t\treturn 0;",
            "",
            "\t/* read the nodes onto the stack, retry if done during rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnnodes = read_once_policy_nodemask(pol, &nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\t/* if the nodemask has become invalid, we cannot do anything */",
            "\tif (!nnodes)",
            "\t\treturn 0;",
            "",
            "\t/* Continue allocating from most recent node and adjust the nr_pages */",
            "\tnode = me->il_prev;",
            "\tweight = me->il_weight;",
            "\tif (weight && node_isset(node, nodes)) {",
            "\t\tnode_pages = min(rem_pages, weight);",
            "\t\tnr_allocated = __alloc_pages_bulk(gfp, node, NULL, node_pages,",
            "\t\t\t\t\t\t  NULL, page_array);",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t\t/* if that's all the pages, no need to interleave */",
            "\t\tif (rem_pages <= weight) {",
            "\t\t\tme->il_weight -= rem_pages;",
            "\t\t\treturn total_allocated;",
            "\t\t}",
            "\t\t/* Otherwise we adjust remaining pages, continue from there */",
            "\t\trem_pages -= weight;",
            "\t}",
            "\t/* clear active weight in case of an allocation failure */",
            "\tme->il_weight = 0;",
            "\tprev_node = node;",
            "",
            "\t/* create a local copy of node weights to operate on outside rcu */",
            "\tweights = kzalloc(nr_node_ids, GFP_KERNEL);",
            "\tif (!weights)",
            "\t\treturn total_allocated;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state) {",
            "\t\tmemcpy(weights, state->iw_table, nr_node_ids * sizeof(u8));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\trcu_read_unlock();",
            "\t\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\t\tweights[i] = 1;",
            "\t}",
            "",
            "\t/* calculate total, detect system default usage */",
            "\tfor_each_node_mask(node, nodes)",
            "\t\tweight_total += weights[node];",
            "",
            "\t/*",
            "\t * Calculate rounds/partial rounds to minimize __alloc_pages_bulk calls.",
            "\t * Track which node weighted interleave should resume from.",
            "\t *",
            "\t * if (rounds > 0) and (delta == 0), resume_node will always be",
            "\t * the node following prev_node and its weight.",
            "\t */",
            "\trounds = rem_pages / weight_total;",
            "\tdelta = rem_pages % weight_total;",
            "\tresume_node = next_node_in(prev_node, nodes);",
            "\tresume_weight = weights[resume_node];",
            "\tfor (i = 0; i < nnodes; i++) {",
            "\t\tnode = next_node_in(prev_node, nodes);",
            "\t\tweight = weights[node];",
            "\t\tnode_pages = weight * rounds;",
            "\t\t/* If a delta exists, add this node's portion of the delta */",
            "\t\tif (delta > weight) {",
            "\t\t\tnode_pages += weight;",
            "\t\t\tdelta -= weight;",
            "\t\t} else if (delta) {",
            "\t\t\t/* when delta is depleted, resume from that node */",
            "\t\t\tnode_pages += delta;",
            "\t\t\tresume_node = node;",
            "\t\t\tresume_weight = weight - delta;",
            "\t\t\tdelta = 0;",
            "\t\t}",
            "\t\t/* node_pages can be 0 if an allocation fails and rounds == 0 */",
            "\t\tif (!node_pages)",
            "\t\t\tbreak;",
            "\t\tnr_allocated = __alloc_pages_bulk(gfp, node, NULL, node_pages,",
            "\t\t\t\t\t\t  NULL, page_array);",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t\tif (total_allocated == nr_pages)",
            "\t\t\tbreak;",
            "\t\tprev_node = node;",
            "\t}",
            "\tme->il_prev = resume_node;",
            "\tme->il_weight = resume_weight;",
            "\tkfree(weights);",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_weighted_interleave",
          "description": "alloc_pages_bulk_array_weighted_interleave 实现加权交错批量页面分配，基于策略权重计算各节点分配数量，优先使用最近使用的节点权重，处理剩余页面的分配逻辑。",
          "similarity": 0.6131470799446106
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mempolicy.c",
          "start_line": 168,
          "end_line": 268,
          "content": [
            "static u8 get_il_weight(int node)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tu8 weight = 1;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state)",
            "\t\tweight = state->iw_table[node];",
            "\trcu_read_unlock();",
            "\treturn weight;",
            "}",
            "static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)",
            "{",
            "\tu64 sum_bw = 0;",
            "\tunsigned int cast_sum_bw, scaling_factor = 1, iw_gcd = 0;",
            "\tint nid;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tsum_bw += bw[nid];",
            "",
            "\t/* Scale bandwidths to whole numbers in the range [1, weightiness] */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t/*",
            "\t\t * Try not to perform 64-bit division.",
            "\t\t * If sum_bw < scaling_factor, then sum_bw < U32_MAX.",
            "\t\t * If sum_bw > scaling_factor, then round the weight up to 1.",
            "\t\t */",
            "\t\tscaling_factor = weightiness * bw[nid];",
            "\t\tif (bw[nid] && sum_bw < scaling_factor) {",
            "\t\t\tcast_sum_bw = (unsigned int)sum_bw;",
            "\t\t\tnew_iw[nid] = scaling_factor / cast_sum_bw;",
            "\t\t} else {",
            "\t\t\tnew_iw[nid] = 1;",
            "\t\t}",
            "\t\tif (!iw_gcd)",
            "\t\t\tiw_gcd = new_iw[nid];",
            "\t\tiw_gcd = gcd(iw_gcd, new_iw[nid]);",
            "\t}",
            "",
            "\t/* 1:2 is strictly better than 16:32. Reduce by the weights' GCD. */",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tnew_iw[nid] /= iw_gcd;",
            "}",
            "int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tunsigned int *old_bw, *new_bw;",
            "\tunsigned int bw_val;",
            "\tint i;",
            "",
            "\tbw_val = min(coords->read_bandwidth, coords->write_bandwidth);",
            "\tnew_bw = kcalloc(nr_node_ids, sizeof(unsigned int), GFP_KERNEL);",
            "\tif (!new_bw)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew_wi_state = kmalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state) {",
            "\t\tkfree(new_bw);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tnew_wi_state->mode_auto = true;",
            "\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\tnew_wi_state->iw_table[i] = 1;",
            "",
            "\t/*",
            "\t * Update bandwidth info, even in manual mode. That way, when switching",
            "\t * to auto mode in the future, iw_table can be overwritten using",
            "\t * accurate bw data.",
            "\t */",
            "\tmutex_lock(&wi_state_lock);",
            "",
            "\told_bw = node_bw_table;",
            "\tif (old_bw)",
            "\t\tmemcpy(new_bw, old_bw, nr_node_ids * sizeof(*old_bw));",
            "\tnew_bw[node] = bw_val;",
            "\tnode_bw_table = new_bw;",
            "",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state && !old_wi_state->mode_auto) {",
            "\t\t/* Manual mode; skip reducing weights and updating wi_state */",
            "\t\tmutex_unlock(&wi_state_lock);",
            "\t\tkfree(new_wi_state);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* NULL wi_state assumes auto=true; reduce weights and update wi_state*/",
            "\treduce_interleave_weights(new_bw, new_wi_state->iw_table);",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "out:",
            "\tkfree(old_bw);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_il_weight, reduce_interleave_weights, mempolicy_set_node_perf",
          "description": "实现带权交错策略的权重计算与调整逻辑，通过获取节点带宽数据动态修改权重比例，支持根据性能参数更新节点间内存分配优先级。",
          "similarity": 0.6040432453155518
        }
      ]
    },
    {
      "source_file": "mm/zsmalloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:38:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `zsmalloc.c`\n\n---\n\n# zsmalloc.c 技术文档\n\n## 1. 文件概述\n\n`zsmalloc.c` 实现了 **zsmalloc** —— 一种专为压缩内存（如 zram、zswap）设计的物理页面级内存分配器。其核心目标是在不依赖虚拟内存连续性的前提下，高效地分配和管理小于页面大小的对象，同时最小化内部碎片。该分配器通过将多个物理页面组合成逻辑单元（zspage），并在其中紧凑地排列固定大小的对象，从而支持高效的对象分配、释放和迁移（用于内存压缩场景）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct zs_pool`**: 内存池对象，代表一个独立的 zsmalloc 实例。包含：\n  - 多个 `size_class` 数组（按对象大小分类）\n  - 统计信息（`pages_allocated`, `stats`）\n  - 缓存（`handle_cachep`, `zspage_cachep`）\n  - 自旋锁（`lock`）和压缩相关字段（`shrinker`, `free_work`）\n\n- **`struct size_class`**: 尺寸类别，管理相同大小对象的分配。包含：\n  - 按“满度”（fullness）分组的 zspage 链表（`fullness_list`）\n  - 对象大小（`size`）、每 zspage 对象数（`objs_per_zspage`）\n  - 每 zspage 页面数（`pages_per_zspage`）\n  - 统计信息（`stats`）\n\n- **`struct zspage`**: 逻辑内存页，由 1 到 `ZS_MAX_PAGES_PER_ZSPAGE` 个物理页组成。包含：\n  - 元数据位域（`huge`, `fullness`, `class`, `isolated`, `magic`）\n  - 使用计数（`inuse`）、首个空闲对象索引（`freeobj`）\n  - 首页指针（`first_page`）、所属池（`pool`）\n  - 读写锁（`lock`，用于迁移）\n\n- **`struct link_free`**: 嵌入在空闲对象中的单向链表节点，用于追踪空闲对象。\n\n- **`struct mapping_area`**: 每 CPU 映射区域，用于安全访问跨页对象（通过 `kmap_atomic` 和临时缓冲区）。\n\n### 关键函数（部分声明/定义）\n\n- **初始化与销毁**: `zs_create_pool()`, `zs_destroy_pool()`\n- **分配与释放**: `zs_malloc()`, `zs_free()`\n- **对象映射**: `zs_map_object()`, `zs_unmap_object()`\n- **压缩支持**: `migrate_*_lock()` 系列函数、`kick_deferred_free()`\n- **统计与调试**: `zs_size_classes_init()`, `zs_register_migration()`（未在片段中完整显示）\n\n## 3. 关键实现\n\n### 对象句柄编码\n- 使用 `unsigned long` 类型的 **handle** 编码对象位置：`<PFN><obj_idx><tag>`。\n- **PFN**: 物理页帧号（占用 `_PFN_BITS` 位）。\n- **obj_idx**: 对象在 zspage 内的索引（占用 `OBJ_INDEX_BITS` 位）。\n- **tag**: 最低位 (`OBJ_ALLOCATED_TAG`) 标记对象是否已分配。\n- 此设计允许在不解引用的情况下快速判断对象状态，并定位其物理页。\n\n### Zspage 管理\n- **多页组合**: 小对象类别的 zspage 可跨越多个物理页（最多 `ZS_MAX_PAGES_PER_ZSPAGE`），以提高空间利用率。\n- **满度分组**: 每个 `size_class` 维护 `NR_FULLNESS_GROUPS` (12) 个链表，按使用率（0%, ≤10%, ..., 100%）组织 zspage，便于快速查找合适页面并减少碎片。\n- **Huge Page 优化**: 当对象大小接近或等于页面时，使用单页 zspage (`huge=1`)，简化管理。\n\n### 跨页对象处理\n- 对象可能跨越两个物理页边界。\n- 通过 `struct mapping_area` 提供每 CPU 的原子映射区域 (`vm_addr`) 和临时缓冲区 (`vm_buf`)。\n- `zs_map_object()` 根据对象是否跨页，选择直接映射或通过缓冲区复制，确保访问安全。\n\n### 并发与迁移\n- **锁层次**: 定义了 `page_lock` → `pool->lock` → `zspage->lock` 的锁顺序。\n- **迁移锁**: `zspage->lock` 是读写锁，读锁用于常规访问，写锁用于压缩迁移（`CONFIG_COMPACTION`）。\n- **延迟释放**: 在压缩场景下，释放操作可能被延迟执行（`deferred_free`），以避免在迁移关键路径上持有锁。\n\n### 内存布局复用\n- 巧妙复用 `struct page` 的字段：\n  - `page->private`: 指向所属 `zspage`。\n  - `page->index`: 链接同一 zspage 的所有物理页；对 huge zspage 存储 handle。\n  - `page->page_type`: 存储子页中首个对象的偏移。\n- 使用页面标志位：\n  - `PG_private`: 标记 zspage 的首页。\n  - `PG_owner_priv_1`: 标记 huge zspage 的页。\n\n## 4. 依赖关系\n\n- **内核基础组件**:\n  - `<linux/slab.h>`: 用于分配 `zs_pool`、`zspage` 和 handle 的元数据。\n  - `<linux/highmem.h>`, `<asm/tlbflush.h>`: 支持高内存页的原子映射 (`kmap_atomic`)。\n  - `<linux/spinlock.h>`, `<linux/rwlock.h>`: 提供并发控制原语。\n  - `<linux/shrinker.h>`: 实现内存回收接口，供 VM 在内存压力下压缩 zsmalloc 池。\n- **压缩子系统**:\n  - **zram**: 主要用户，将压缩后的内存块存储在 zsmalloc 中。\n  - **zswap**: 作为交换缓存的后端，使用 zsmalloc 存储压缩页。\n  - **zpool API**: 通过 `zpool` 抽象层集成到压缩框架中。\n- **内存管理**:\n  - `<linux/migrate.h>`: 支持页面迁移，是内存压缩 (`CONFIG_COMPACTION`) 的关键。\n  - `<linux/pagemap.h>`: 操作页面标志和属性。\n\n## 5. 使用场景\n\n- **zram 块设备**: 作为 RAM-based 压缩块设备的后端存储，显著提高可用内存容量。\n- **zswap 交换缓存**: 在将内存页写入交换设备前，先压缩并暂存于 zsmalloc，减少 I/O。\n- **通用小对象分配**: 适用于需要大量小于页面大小、且生命周期较短的对象分配场景，尤其在内存受限环境中。\n- **内存压缩框架**: 作为 `zpool` 的一种实现，为内核提供可压缩的内存池服务。",
      "similarity": 0.5371010303497314,
      "chunks": [
        {
          "chunk_id": 8,
          "file_path": "mm/zsmalloc.c",
          "start_line": 1555,
          "end_line": 1661,
          "content": [
            "static unsigned long find_alloced_obj(struct size_class *class,",
            "\t\t\t\t      struct page *page, int *obj_idx)",
            "{",
            "\tunsigned int offset;",
            "\tint index = *obj_idx;",
            "\tunsigned long handle = 0;",
            "\tvoid *addr = kmap_atomic(page);",
            "",
            "\toffset = get_first_obj_offset(page);",
            "\toffset += class->size * index;",
            "",
            "\twhile (offset < PAGE_SIZE) {",
            "\t\tif (obj_allocated(page, addr + offset, &handle))",
            "\t\t\tbreak;",
            "",
            "\t\toffset += class->size;",
            "\t\tindex++;",
            "\t}",
            "",
            "\tkunmap_atomic(addr);",
            "",
            "\t*obj_idx = index;",
            "",
            "\treturn handle;",
            "}",
            "static void migrate_zspage(struct zs_pool *pool, struct zspage *src_zspage,",
            "\t\t\t   struct zspage *dst_zspage)",
            "{",
            "\tunsigned long used_obj, free_obj;",
            "\tunsigned long handle;",
            "\tint obj_idx = 0;",
            "\tstruct page *s_page = get_first_page(src_zspage);",
            "\tstruct size_class *class = pool->size_class[src_zspage->class];",
            "",
            "\twhile (1) {",
            "\t\thandle = find_alloced_obj(class, s_page, &obj_idx);",
            "\t\tif (!handle) {",
            "\t\t\ts_page = get_next_page(s_page);",
            "\t\t\tif (!s_page)",
            "\t\t\t\tbreak;",
            "\t\t\tobj_idx = 0;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tused_obj = handle_to_obj(handle);",
            "\t\tfree_obj = obj_malloc(pool, dst_zspage, handle);",
            "\t\tzs_object_copy(class, free_obj, used_obj);",
            "\t\tobj_idx++;",
            "\t\trecord_obj(handle, free_obj);",
            "\t\tobj_free(class->size, used_obj);",
            "",
            "\t\t/* Stop if there is no more space */",
            "\t\tif (zspage_full(class, dst_zspage))",
            "\t\t\tbreak;",
            "",
            "\t\t/* Stop if there are no more objects to migrate */",
            "\t\tif (zspage_empty(src_zspage))",
            "\t\t\tbreak;",
            "\t}",
            "}",
            "static int putback_zspage(struct size_class *class, struct zspage *zspage)",
            "{",
            "\tint fullness;",
            "",
            "\tfullness = get_fullness_group(class, zspage);",
            "\tinsert_zspage(class, zspage, fullness);",
            "\tset_zspage_mapping(zspage, class->index, fullness);",
            "",
            "\treturn fullness;",
            "}",
            "static void lock_zspage(struct zspage *zspage)",
            "{",
            "\tstruct page *curr_page, *page;",
            "",
            "\t/*",
            "\t * Pages we haven't locked yet can be migrated off the list while we're",
            "\t * trying to lock them, so we need to be careful and only attempt to",
            "\t * lock each page under migrate_read_lock(). Otherwise, the page we lock",
            "\t * may no longer belong to the zspage. This means that we may wait for",
            "\t * the wrong page to unlock, so we must take a reference to the page",
            "\t * prior to waiting for it to unlock outside migrate_read_lock().",
            "\t */",
            "\twhile (1) {",
            "\t\tmigrate_read_lock(zspage);",
            "\t\tpage = get_first_page(zspage);",
            "\t\tif (trylock_page(page))",
            "\t\t\tbreak;",
            "\t\tget_page(page);",
            "\t\tmigrate_read_unlock(zspage);",
            "\t\twait_on_page_locked(page);",
            "\t\tput_page(page);",
            "\t}",
            "",
            "\tcurr_page = page;",
            "\twhile ((page = get_next_page(curr_page))) {",
            "\t\tif (trylock_page(page)) {",
            "\t\t\tcurr_page = page;",
            "\t\t} else {",
            "\t\t\tget_page(page);",
            "\t\t\tmigrate_read_unlock(zspage);",
            "\t\t\twait_on_page_locked(page);",
            "\t\t\tput_page(page);",
            "\t\t\tmigrate_read_lock(zspage);",
            "\t\t}",
            "\t}",
            "\tmigrate_read_unlock(zspage);",
            "}"
          ],
          "function_name": "find_alloced_obj, migrate_zspage, putback_zspage, lock_zspage",
          "description": "find_alloced_obj查找已分配对象；migrate_zspage迁移zspage内容；putback_zspage重新插入zspage到合适组；lock_zspage获取页面独占锁防止迁移冲突",
          "similarity": 0.5570100545883179
        },
        {
          "chunk_id": 2,
          "file_path": "mm/zsmalloc.c",
          "start_line": 474,
          "end_line": 593,
          "content": [
            "static inline void set_freeobj(struct zspage *zspage, unsigned int obj)",
            "{",
            "\tzspage->freeobj = obj;",
            "}",
            "static void get_zspage_mapping(struct zspage *zspage,",
            "\t\t\t       unsigned int *class_idx,",
            "\t\t\t       int *fullness)",
            "{",
            "\tBUG_ON(zspage->magic != ZSPAGE_MAGIC);",
            "",
            "\t*fullness = zspage->fullness;",
            "\t*class_idx = zspage->class;",
            "}",
            "static void set_zspage_mapping(struct zspage *zspage,",
            "\t\t\t       unsigned int class_idx,",
            "\t\t\t       int fullness)",
            "{",
            "\tzspage->class = class_idx;",
            "\tzspage->fullness = fullness;",
            "}",
            "static int get_size_class_index(int size)",
            "{",
            "\tint idx = 0;",
            "",
            "\tif (likely(size > ZS_MIN_ALLOC_SIZE))",
            "\t\tidx = DIV_ROUND_UP(size - ZS_MIN_ALLOC_SIZE,",
            "\t\t\t\tZS_SIZE_CLASS_DELTA);",
            "",
            "\treturn min_t(int, ZS_SIZE_CLASSES - 1, idx);",
            "}",
            "static inline void class_stat_inc(struct size_class *class,",
            "\t\t\t\tint type, unsigned long cnt)",
            "{",
            "\tclass->stats.objs[type] += cnt;",
            "}",
            "static inline void class_stat_dec(struct size_class *class,",
            "\t\t\t\tint type, unsigned long cnt)",
            "{",
            "\tclass->stats.objs[type] -= cnt;",
            "}",
            "static inline unsigned long zs_stat_get(struct size_class *class, int type)",
            "{",
            "\treturn class->stats.objs[type];",
            "}",
            "static void __init zs_stat_init(void)",
            "{",
            "\tif (!debugfs_initialized()) {",
            "\t\tpr_warn(\"debugfs not available, stat dir not created\\n\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tzs_stat_root = debugfs_create_dir(\"zsmalloc\", NULL);",
            "}",
            "static void __exit zs_stat_exit(void)",
            "{",
            "\tdebugfs_remove_recursive(zs_stat_root);",
            "}",
            "static int zs_stats_size_show(struct seq_file *s, void *v)",
            "{",
            "\tint i, fg;",
            "\tstruct zs_pool *pool = s->private;",
            "\tstruct size_class *class;",
            "\tint objs_per_zspage;",
            "\tunsigned long obj_allocated, obj_used, pages_used, freeable;",
            "\tunsigned long total_objs = 0, total_used_objs = 0, total_pages = 0;",
            "\tunsigned long total_freeable = 0;",
            "\tunsigned long inuse_totals[NR_FULLNESS_GROUPS] = {0, };",
            "",
            "\tseq_printf(s, \" %5s %5s %9s %9s %9s %9s %9s %9s %9s %9s %9s %9s %9s %13s %10s %10s %16s %8s\\n\",",
            "\t\t\t\"class\", \"size\", \"10%\", \"20%\", \"30%\", \"40%\",",
            "\t\t\t\"50%\", \"60%\", \"70%\", \"80%\", \"90%\", \"99%\", \"100%\",",
            "\t\t\t\"obj_allocated\", \"obj_used\", \"pages_used\",",
            "\t\t\t\"pages_per_zspage\", \"freeable\");",
            "",
            "\tfor (i = 0; i < ZS_SIZE_CLASSES; i++) {",
            "",
            "\t\tclass = pool->size_class[i];",
            "",
            "\t\tif (class->index != i)",
            "\t\t\tcontinue;",
            "",
            "\t\tspin_lock(&pool->lock);",
            "",
            "\t\tseq_printf(s, \" %5u %5u \", i, class->size);",
            "\t\tfor (fg = ZS_INUSE_RATIO_10; fg < NR_FULLNESS_GROUPS; fg++) {",
            "\t\t\tinuse_totals[fg] += zs_stat_get(class, fg);",
            "\t\t\tseq_printf(s, \"%9lu \", zs_stat_get(class, fg));",
            "\t\t}",
            "",
            "\t\tobj_allocated = zs_stat_get(class, ZS_OBJS_ALLOCATED);",
            "\t\tobj_used = zs_stat_get(class, ZS_OBJS_INUSE);",
            "\t\tfreeable = zs_can_compact(class);",
            "\t\tspin_unlock(&pool->lock);",
            "",
            "\t\tobjs_per_zspage = class->objs_per_zspage;",
            "\t\tpages_used = obj_allocated / objs_per_zspage *",
            "\t\t\t\tclass->pages_per_zspage;",
            "",
            "\t\tseq_printf(s, \"%13lu %10lu %10lu %16d %8lu\\n\",",
            "\t\t\t   obj_allocated, obj_used, pages_used,",
            "\t\t\t   class->pages_per_zspage, freeable);",
            "",
            "\t\ttotal_objs += obj_allocated;",
            "\t\ttotal_used_objs += obj_used;",
            "\t\ttotal_pages += pages_used;",
            "\t\ttotal_freeable += freeable;",
            "\t}",
            "",
            "\tseq_puts(s, \"\\n\");",
            "\tseq_printf(s, \" %5s %5s \", \"Total\", \"\");",
            "",
            "\tfor (fg = ZS_INUSE_RATIO_10; fg < NR_FULLNESS_GROUPS; fg++)",
            "\t\tseq_printf(s, \"%9lu \", inuse_totals[fg]);",
            "",
            "\tseq_printf(s, \"%13lu %10lu %10lu %16s %8lu\\n\",",
            "\t\t   total_objs, total_used_objs, total_pages, \"\",",
            "\t\t   total_freeable);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "set_freeobj, get_zspage_mapping, set_zspage_mapping, get_size_class_index, class_stat_inc, class_stat_dec, zs_stat_get, zs_stat_init, zs_stat_exit, zs_stats_size_show",
          "description": "实现对象大小分类索引计算、统计增量/减量操作、调试统计初始化与展示逻辑，支持运行时性能监控。",
          "similarity": 0.5257018208503723
        },
        {
          "chunk_id": 7,
          "file_path": "mm/zsmalloc.c",
          "start_line": 1421,
          "end_line": 1546,
          "content": [
            "static void obj_free(int class_size, unsigned long obj)",
            "{",
            "\tstruct link_free *link;",
            "\tstruct zspage *zspage;",
            "\tstruct page *f_page;",
            "\tunsigned long f_offset;",
            "\tunsigned int f_objidx;",
            "\tvoid *vaddr;",
            "",
            "\tobj_to_location(obj, &f_page, &f_objidx);",
            "\tf_offset = offset_in_page(class_size * f_objidx);",
            "\tzspage = get_zspage(f_page);",
            "",
            "\tvaddr = kmap_atomic(f_page);",
            "\tlink = (struct link_free *)(vaddr + f_offset);",
            "",
            "\t/* Insert this object in containing zspage's freelist */",
            "\tif (likely(!ZsHugePage(zspage)))",
            "\t\tlink->next = get_freeobj(zspage) << OBJ_TAG_BITS;",
            "\telse",
            "\t\tf_page->index = 0;",
            "\tset_freeobj(zspage, f_objidx);",
            "",
            "\tkunmap_atomic(vaddr);",
            "\tmod_zspage_inuse(zspage, -1);",
            "}",
            "void zs_free(struct zs_pool *pool, unsigned long handle)",
            "{",
            "\tstruct zspage *zspage;",
            "\tstruct page *f_page;",
            "\tunsigned long obj;",
            "\tstruct size_class *class;",
            "\tint fullness;",
            "",
            "\tif (IS_ERR_OR_NULL((void *)handle))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * The pool->lock protects the race with zpage's migration",
            "\t * so it's safe to get the page from handle.",
            "\t */",
            "\tspin_lock(&pool->lock);",
            "\tobj = handle_to_obj(handle);",
            "\tobj_to_page(obj, &f_page);",
            "\tzspage = get_zspage(f_page);",
            "\tclass = zspage_class(pool, zspage);",
            "",
            "\tclass_stat_dec(class, ZS_OBJS_INUSE, 1);",
            "\tobj_free(class->size, obj);",
            "",
            "\tfullness = fix_fullness_group(class, zspage);",
            "\tif (fullness == ZS_INUSE_RATIO_0)",
            "\t\tfree_zspage(pool, class, zspage);",
            "",
            "\tspin_unlock(&pool->lock);",
            "\tcache_free_handle(pool, handle);",
            "}",
            "static void zs_object_copy(struct size_class *class, unsigned long dst,",
            "\t\t\t\tunsigned long src)",
            "{",
            "\tstruct page *s_page, *d_page;",
            "\tunsigned int s_objidx, d_objidx;",
            "\tunsigned long s_off, d_off;",
            "\tvoid *s_addr, *d_addr;",
            "\tint s_size, d_size, size;",
            "\tint written = 0;",
            "",
            "\ts_size = d_size = class->size;",
            "",
            "\tobj_to_location(src, &s_page, &s_objidx);",
            "\tobj_to_location(dst, &d_page, &d_objidx);",
            "",
            "\ts_off = offset_in_page(class->size * s_objidx);",
            "\td_off = offset_in_page(class->size * d_objidx);",
            "",
            "\tif (s_off + class->size > PAGE_SIZE)",
            "\t\ts_size = PAGE_SIZE - s_off;",
            "",
            "\tif (d_off + class->size > PAGE_SIZE)",
            "\t\td_size = PAGE_SIZE - d_off;",
            "",
            "\ts_addr = kmap_atomic(s_page);",
            "\td_addr = kmap_atomic(d_page);",
            "",
            "\twhile (1) {",
            "\t\tsize = min(s_size, d_size);",
            "\t\tmemcpy(d_addr + d_off, s_addr + s_off, size);",
            "\t\twritten += size;",
            "",
            "\t\tif (written == class->size)",
            "\t\t\tbreak;",
            "",
            "\t\ts_off += size;",
            "\t\ts_size -= size;",
            "\t\td_off += size;",
            "\t\td_size -= size;",
            "",
            "\t\t/*",
            "\t\t * Calling kunmap_atomic(d_addr) is necessary. kunmap_atomic()",
            "\t\t * calls must occurs in reverse order of calls to kmap_atomic().",
            "\t\t * So, to call kunmap_atomic(s_addr) we should first call",
            "\t\t * kunmap_atomic(d_addr). For more details see",
            "\t\t * Documentation/mm/highmem.rst.",
            "\t\t */",
            "\t\tif (s_off >= PAGE_SIZE) {",
            "\t\t\tkunmap_atomic(d_addr);",
            "\t\t\tkunmap_atomic(s_addr);",
            "\t\t\ts_page = get_next_page(s_page);",
            "\t\t\ts_addr = kmap_atomic(s_page);",
            "\t\t\td_addr = kmap_atomic(d_page);",
            "\t\t\ts_size = class->size - written;",
            "\t\t\ts_off = 0;",
            "\t\t}",
            "",
            "\t\tif (d_off >= PAGE_SIZE) {",
            "\t\t\tkunmap_atomic(d_addr);",
            "\t\t\td_page = get_next_page(d_page);",
            "\t\t\td_addr = kmap_atomic(d_page);",
            "\t\t\td_size = class->size - written;",
            "\t\t\td_off = 0;",
            "\t\t}",
            "\t}",
            "",
            "\tkunmap_atomic(d_addr);",
            "\tkunmap_atomic(s_addr);",
            "}"
          ],
          "function_name": "obj_free, zs_free, zs_object_copy",
          "description": "obj_free将释放对象插入自由链表；zs_free真正执行内存释放并更新统计信息；zs_object_copy在对象间复制数据，处理跨页拷贝场景",
          "similarity": 0.5216383934020996
        },
        {
          "chunk_id": 4,
          "file_path": "mm/zsmalloc.c",
          "start_line": 793,
          "end_line": 935,
          "content": [
            "static unsigned long handle_to_obj(unsigned long handle)",
            "{",
            "\treturn *(unsigned long *)handle;",
            "}",
            "static inline bool obj_allocated(struct page *page, void *obj,",
            "\t\t\t\t unsigned long *phandle)",
            "{",
            "\tunsigned long handle;",
            "\tstruct zspage *zspage = get_zspage(page);",
            "",
            "\tif (unlikely(ZsHugePage(zspage))) {",
            "\t\tVM_BUG_ON_PAGE(!is_first_page(page), page);",
            "\t\thandle = page->index;",
            "\t} else",
            "\t\thandle = *(unsigned long *)obj;",
            "",
            "\tif (!(handle & OBJ_ALLOCATED_TAG))",
            "\t\treturn false;",
            "",
            "\t/* Clear all tags before returning the handle */",
            "\t*phandle = handle & ~OBJ_TAG_MASK;",
            "\treturn true;",
            "}",
            "static void reset_page(struct page *page)",
            "{",
            "\t__ClearPageMovable(page);",
            "\tClearPagePrivate(page);",
            "\tset_page_private(page, 0);",
            "\tpage_mapcount_reset(page);",
            "\tpage->index = 0;",
            "}",
            "static int trylock_zspage(struct zspage *zspage)",
            "{",
            "\tstruct page *cursor, *fail;",
            "",
            "\tfor (cursor = get_first_page(zspage); cursor != NULL; cursor =",
            "\t\t\t\t\tget_next_page(cursor)) {",
            "\t\tif (!trylock_page(cursor)) {",
            "\t\t\tfail = cursor;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 1;",
            "unlock:",
            "\tfor (cursor = get_first_page(zspage); cursor != fail; cursor =",
            "\t\t\t\t\tget_next_page(cursor))",
            "\t\tunlock_page(cursor);",
            "",
            "\treturn 0;",
            "}",
            "static void __free_zspage(struct zs_pool *pool, struct size_class *class,",
            "\t\t\t\tstruct zspage *zspage)",
            "{",
            "\tstruct page *page, *next;",
            "\tint fg;",
            "\tunsigned int class_idx;",
            "",
            "\tget_zspage_mapping(zspage, &class_idx, &fg);",
            "",
            "\tassert_spin_locked(&pool->lock);",
            "",
            "\tVM_BUG_ON(get_zspage_inuse(zspage));",
            "\tVM_BUG_ON(fg != ZS_INUSE_RATIO_0);",
            "",
            "\tnext = page = get_first_page(zspage);",
            "\tdo {",
            "\t\tVM_BUG_ON_PAGE(!PageLocked(page), page);",
            "\t\tnext = get_next_page(page);",
            "\t\treset_page(page);",
            "\t\tunlock_page(page);",
            "\t\tdec_zone_page_state(page, NR_ZSPAGES);",
            "\t\tput_page(page);",
            "\t\tpage = next;",
            "\t} while (page != NULL);",
            "",
            "\tcache_free_zspage(pool, zspage);",
            "",
            "\tclass_stat_dec(class, ZS_OBJS_ALLOCATED, class->objs_per_zspage);",
            "\tatomic_long_sub(class->pages_per_zspage, &pool->pages_allocated);",
            "}",
            "static void free_zspage(struct zs_pool *pool, struct size_class *class,",
            "\t\t\t\tstruct zspage *zspage)",
            "{",
            "\tVM_BUG_ON(get_zspage_inuse(zspage));",
            "\tVM_BUG_ON(list_empty(&zspage->list));",
            "",
            "\t/*",
            "\t * Since zs_free couldn't be sleepable, this function cannot call",
            "\t * lock_page. The page locks trylock_zspage got will be released",
            "\t * by __free_zspage.",
            "\t */",
            "\tif (!trylock_zspage(zspage)) {",
            "\t\tkick_deferred_free(pool);",
            "\t\treturn;",
            "\t}",
            "",
            "\tremove_zspage(class, zspage, ZS_INUSE_RATIO_0);",
            "\t__free_zspage(pool, class, zspage);",
            "}",
            "static void init_zspage(struct size_class *class, struct zspage *zspage)",
            "{",
            "\tunsigned int freeobj = 1;",
            "\tunsigned long off = 0;",
            "\tstruct page *page = get_first_page(zspage);",
            "",
            "\twhile (page) {",
            "\t\tstruct page *next_page;",
            "\t\tstruct link_free *link;",
            "\t\tvoid *vaddr;",
            "",
            "\t\tset_first_obj_offset(page, off);",
            "",
            "\t\tvaddr = kmap_atomic(page);",
            "\t\tlink = (struct link_free *)vaddr + off / sizeof(*link);",
            "",
            "\t\twhile ((off += class->size) < PAGE_SIZE) {",
            "\t\t\tlink->next = freeobj++ << OBJ_TAG_BITS;",
            "\t\t\tlink += class->size / sizeof(*link);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * We now come to the last (full or partial) object on this",
            "\t\t * page, which must point to the first object on the next",
            "\t\t * page (if present)",
            "\t\t */",
            "\t\tnext_page = get_next_page(page);",
            "\t\tif (next_page) {",
            "\t\t\tlink->next = freeobj++ << OBJ_TAG_BITS;",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * Reset OBJ_TAG_BITS bit to last link to tell",
            "\t\t\t * whether it's allocated object or not.",
            "\t\t\t */",
            "\t\t\tlink->next = -1UL << OBJ_TAG_BITS;",
            "\t\t}",
            "\t\tkunmap_atomic(vaddr);",
            "\t\tpage = next_page;",
            "\t\toff %= PAGE_SIZE;",
            "\t}",
            "",
            "\tset_freeobj(zspage, 0);",
            "}"
          ],
          "function_name": "handle_to_obj, obj_allocated, reset_page, trylock_zspage, __free_zspage, free_zspage, init_zspage",
          "description": "实现对象定位解析、页面重置、zspage释放流程及初始化布局构建，完成从物理页面到逻辑对象的映射关系建立。",
          "similarity": 0.5019052624702454
        },
        {
          "chunk_id": 3,
          "file_path": "mm/zsmalloc.c",
          "start_line": 622,
          "end_line": 723,
          "content": [
            "static void zs_pool_stat_create(struct zs_pool *pool, const char *name)",
            "{",
            "\tif (!zs_stat_root) {",
            "\t\tpr_warn(\"no root stat dir, not creating <%s> stat dir\\n\", name);",
            "\t\treturn;",
            "\t}",
            "",
            "\tpool->stat_dentry = debugfs_create_dir(name, zs_stat_root);",
            "",
            "\tdebugfs_create_file(\"classes\", S_IFREG | 0444, pool->stat_dentry, pool,",
            "\t\t\t    &zs_stats_size_fops);",
            "}",
            "static void zs_pool_stat_destroy(struct zs_pool *pool)",
            "{",
            "\tdebugfs_remove_recursive(pool->stat_dentry);",
            "}",
            "static void __init zs_stat_init(void)",
            "{",
            "}",
            "static void __exit zs_stat_exit(void)",
            "{",
            "}",
            "static inline void zs_pool_stat_create(struct zs_pool *pool, const char *name)",
            "{",
            "}",
            "static inline void zs_pool_stat_destroy(struct zs_pool *pool)",
            "{",
            "}",
            "static int get_fullness_group(struct size_class *class, struct zspage *zspage)",
            "{",
            "\tint inuse, objs_per_zspage, ratio;",
            "",
            "\tinuse = get_zspage_inuse(zspage);",
            "\tobjs_per_zspage = class->objs_per_zspage;",
            "",
            "\tif (inuse == 0)",
            "\t\treturn ZS_INUSE_RATIO_0;",
            "\tif (inuse == objs_per_zspage)",
            "\t\treturn ZS_INUSE_RATIO_100;",
            "",
            "\tratio = 100 * inuse / objs_per_zspage;",
            "\t/*",
            "\t * Take integer division into consideration: a page with one inuse",
            "\t * object out of 127 possible, will end up having 0 usage ratio,",
            "\t * which is wrong as it belongs in ZS_INUSE_RATIO_10 fullness group.",
            "\t */",
            "\treturn ratio / 10 + 1;",
            "}",
            "static void insert_zspage(struct size_class *class,",
            "\t\t\t\tstruct zspage *zspage,",
            "\t\t\t\tint fullness)",
            "{",
            "\tclass_stat_inc(class, fullness, 1);",
            "\tlist_add(&zspage->list, &class->fullness_list[fullness]);",
            "}",
            "static void remove_zspage(struct size_class *class,",
            "\t\t\t\tstruct zspage *zspage,",
            "\t\t\t\tint fullness)",
            "{",
            "\tVM_BUG_ON(list_empty(&class->fullness_list[fullness]));",
            "",
            "\tlist_del_init(&zspage->list);",
            "\tclass_stat_dec(class, fullness, 1);",
            "}",
            "static int fix_fullness_group(struct size_class *class, struct zspage *zspage)",
            "{",
            "\tint class_idx;",
            "\tint currfg, newfg;",
            "",
            "\tget_zspage_mapping(zspage, &class_idx, &currfg);",
            "\tnewfg = get_fullness_group(class, zspage);",
            "\tif (newfg == currfg)",
            "\t\tgoto out;",
            "",
            "\tremove_zspage(class, zspage, currfg);",
            "\tinsert_zspage(class, zspage, newfg);",
            "\tset_zspage_mapping(zspage, class_idx, newfg);",
            "out:",
            "\treturn newfg;",
            "}",
            "static void obj_to_location(unsigned long obj, struct page **page,",
            "\t\t\t\tunsigned int *obj_idx)",
            "{",
            "\tobj >>= OBJ_TAG_BITS;",
            "\t*page = pfn_to_page(obj >> OBJ_INDEX_BITS);",
            "\t*obj_idx = (obj & OBJ_INDEX_MASK);",
            "}",
            "static void obj_to_page(unsigned long obj, struct page **page)",
            "{",
            "\tobj >>= OBJ_TAG_BITS;",
            "\t*page = pfn_to_page(obj >> OBJ_INDEX_BITS);",
            "}",
            "static unsigned long location_to_obj(struct page *page, unsigned int obj_idx)",
            "{",
            "\tunsigned long obj;",
            "",
            "\tobj = page_to_pfn(page) << OBJ_INDEX_BITS;",
            "\tobj |= obj_idx & OBJ_INDEX_MASK;",
            "\tobj <<= OBJ_TAG_BITS;",
            "",
            "\treturn obj;",
            "}"
          ],
          "function_name": "zs_pool_stat_create, zs_pool_stat_destroy, zs_stat_init, zs_stat_exit, zs_pool_stat_create, zs_pool_stat_destroy, get_fullness_group, insert_zspage, remove_zspage, fix_fullness_group, obj_to_location, obj_to_page, location_to_obj",
          "description": "实现页面满度分组管理、zspage插入/移除链表操作及页面状态自动调节机制，通过fullness组优化内存回收策略。",
          "similarity": 0.4923199415206909
        }
      ]
    },
    {
      "source_file": "mm/slab.h",
      "md_summary": "> 自动生成时间: 2025-12-07 17:22:03\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `slab.h`\n\n---\n\n# `slab.h` 技术文档\n\n## 1. 文件概述\n\n`slab.h` 是 Linux 内核内存管理子系统中 SLAB/SLUB 分配器的核心内部头文件，定义了 slab 分配器所使用的底层数据结构（如 `struct slab` 和 `struct kmem_cache`）、关键宏和辅助函数。该文件主要用于在页（`struct page`）与 slab 表示之间进行安全转换，并提供对 slab 元数据的原子访问机制，以支持高性能、可扩展的对象缓存分配。\n\n此头文件专供内核内存管理内部使用，不对外暴露给模块开发者，是实现 SLUB（默认）或 SLAB 分配器的关键基础设施。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`freelist_aba_t`**  \n  联合体，将空闲对象指针（`freelist`）与计数器（`counter`）打包为一个原子单元，用于避免 ABA 问题（Compare-and-Swap 中因值重复导致的逻辑错误）。\n\n- **`struct slab`**  \n  slab 的内部表示，复用 `struct page` 的内存布局。包含：\n  - 所属的 `kmem_cache`\n  - 空闲对象链表（`freelist`）\n  - 对象使用计数（`inuse`）、总对象数（`objects`）\n  - 冻结状态（`frozen`，用于调试）\n  - RCU 回收头（`rcu_head`）\n  - 引用计数（`__page_refcount`）\n  - 可选的 per-object 扩展数据（`obj_exts`）\n\n- **`struct kmem_cache_order_objects`**  \n  封装 slab 阶数（order）与对象数量的复合值，支持原子读写。\n\n- **`struct kmem_cache`**  \n  slab 缓存描述符，包含：\n  - 每 CPU 缓存（`cpu_slab`）\n  - 对象大小（`size`, `object_size`）\n  - 构造函数（`ctor`）\n  - 对齐要求（`align`）\n  - 分配标志（`allocflags`）\n  - NUMA 相关参数（如 `remote_node_defrag_ratio`）\n  - 安全特性（如 `random` 用于 freelist 加固）\n\n### 主要宏与辅助函数\n\n- **类型安全转换宏**：\n  - `folio_slab()` / `slab_folio()`：在 `folio` 与 `slab` 之间安全转换\n  - `page_slab()` / `slab_page()`：兼容旧代码，在 `page` 与 `slab` 之间转换\n\n- **slab 属性访问函数**：\n  - `slab_address()`：获取 slab 起始虚拟地址\n  - `slab_nid()` / `slab_pgdat()`：获取所属 NUMA 节点和内存域\n  - `slab_order()` / `slab_size()`：获取分配阶数和总字节数\n\n- **pfmemalloc 标志操作**：\n  - `slab_test_pfmemalloc()` / `slab_set_pfmemalloc()` 等：标记 slab 是否来自紧急内存预留区（用于网络交换等场景）\n\n- **每 CPU partial slab 支持（`CONFIG_SLUB_CPU_PARTIAL`）**：\n  - `slub_percpu_partial()` 等宏：管理每 CPU 的 partial slab 链表\n\n## 3. 关键实现\n\n### 内存布局复用与静态断言\n\n- `struct slab` 并非独立分配，而是直接复用 `struct page` 的内存空间。通过 `static_assert` 确保关键字段偏移一致（如 `flags` ↔ `__page_flags`），保证类型转换安全。\n- 整个 `struct slab` 大小不超过 `struct page`，确保无越界访问。\n\n### ABA 问题防护\n\n- 在支持 `cmpxchg128`（64 位）或 `cmpxchg64`（32 位）的架构上，启用 `freelist_aba_t` 结构，将 `freelist` 指针与递增计数器打包为单个原子单元。\n- 使用 `try_cmpxchg_freelist` 进行原子更新，防止因指针值循环重用导致的 ABA 错误。\n- 若系统不支持对齐的 `struct page`（`!CONFIG_HAVE_ALIGNED_STRUCT_PAGE`），则禁用此优化。\n\n### 类型安全转换\n\n- 使用 C11 `_Generic` 实现类型安全的 `folio`/`slab`/`page` 转换，避免强制类型转换带来的风险，并为未来重构（如完全迁移到 folio）预留接口。\n\n### pfmemalloc 标志复用\n\n- 利用 `folio` 的 `PG_active` 位存储 `pfmemalloc` 标志，指示该 slab 是否从紧急内存池分配，用于网络子系统在内存压力下仍能分配 skb 等关键结构。\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/page.h>` / `<linux/folio.h>`：通过 `folio_*` 系列函数操作底层内存\n  - `<linux/reciprocal_div.h>`：用于快速除法（计算对象索引）\n  - `<linux/rcupdate.h>`：通过 `rcu_head` 支持 RCU 安全的 slab 回收\n\n- **可选依赖（由 Kconfig 控制）**：\n  - `CONFIG_SLUB_CPU_PARTIAL`：每 CPU partial slab 优化\n  - `CONFIG_SLAB_OBJ_EXT`：per-object 扩展元数据\n  - `CONFIG_SLAB_FREELIST_HARDENED`：freelist 指针随机化加固\n  - `CONFIG_NUMA`：NUMA 感知分配与碎片整理\n  - `CONFIG_KASAN` / `CONFIG_KFENCE`：内存错误检测集成\n\n- **与内存控制器集成**：\n  - 通过 `memcg_data` 字段（复用 `obj_exts`）支持 memcg 内存统计\n\n## 5. 使用场景\n\n- **SLUB 分配器内部**：作为 `slub.c` 的核心数据结构定义，用于管理 slab 生命周期、对象分配/释放。\n- **内存回收路径**：在 direct reclaim 或 kswapd 中，通过 `slab_folio` 获取 folio 信息以决策回收策略。\n- **调试与监控**：sysfs (`kobj`)、KASAN/KFENCE 集成依赖此结构获取 slab 元数据。\n- **网络子系统**：通过 `pfmemalloc` 标志识别紧急内存分配，确保高优先级数据包处理不被阻塞。\n- **NUMA 优化**：在远程节点分配时使用 `remote_node_defrag_ratio` 参数控制跨节点分配行为。\n- **安全加固**：`SLAB_FREELIST_HARDENED` 利用 `random` 字段混淆 freelist 指针，防止堆利用攻击。",
      "similarity": 0.5251783132553101,
      "chunks": []
    }
  ]
}