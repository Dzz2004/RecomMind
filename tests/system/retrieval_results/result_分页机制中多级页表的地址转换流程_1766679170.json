{
  "query": "分页机制中多级页表的地址转换流程",
  "timestamp": "2025-12-26 00:12:50",
  "retrieved_files": [
    {
      "source_file": "mm/page_io.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:03:03\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_io.c`\n\n---\n\n# page_io.c 技术文档\n\n## 1. 文件概述\n\n`page_io.c` 是 Linux 内核内存管理子系统中负责页面交换 I/O 操作的核心文件。该文件实现了将匿名页写入交换设备（swap-out）和从交换设备读回内存（swap-in）的底层机制，包括基于 `bio` 的块设备交换路径和基于文件系统的直接 I/O 交换路径。此外，还提供了通用的交换文件激活逻辑，用于在启用交换文件时构建物理块到交换页的映射。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `__end_swap_bio_write()` / `end_swap_bio_write()`：处理交换写操作完成的回调，处理写错误并结束写回。\n- `__end_swap_bio_read()` / `end_swap_bio_read()`：处理交换读操作完成的回调，设置页面 uptodate 状态或报告读错误。\n- `generic_swapfile_activate()`：为基于文件的交换设备（如 swapfile）构建连续的物理块映射，填充 `swap_info_struct`。\n- `swap_writepage()`：页面写回交换区的主入口函数，支持 zswap 压缩缓存、内存控制组限制等特性。\n- `swap_writepage_fs()`：通过文件系统直接 I/O 路径（如 swap-over-NFS）执行交换写操作。\n- `sio_pool_init()`：初始化用于异步交换 I/O 的内存池。\n- `sio_write_complete()`：处理基于 kiocb 的异步交换写完成回调。\n\n### 关键数据结构\n\n- `struct swap_iocb`：封装用于文件系统交换 I/O 的 `kiocb` 和 `bio_vec` 数组，支持批量交换页写入。\n- `sio_pool`：`mempool_t` 类型的内存池，用于分配 `swap_iocb` 结构，避免高内存压力下分配失败。\n\n## 3. 关键实现\n\n### 交换 I/O 完成处理\n- 写操作失败时，页面被重新标记为 dirty 并清除 `PG_reclaim` 标志，防止被错误回收，同时输出限频警告日志。\n- 读操作失败仅输出警告；成功则设置 `PG_uptodate` 并解锁页面。\n\n### 交换文件激活 (`generic_swapfile_activate`)\n- 遍历交换文件的逻辑块，使用 `bmap()` 获取物理块号。\n- 验证每个 PAGE_SIZE 对齐区域的物理块是否连续且对齐。\n- 通过 `add_swap_extent()` 将有效的交换页范围注册到交换子系统。\n- 计算交换空间的物理跨度（`span`），用于优化交换分配策略。\n\n### 交换写入路径选择\n- 默认使用 `__swap_writepage()`（基于 `bio` 的块设备路径）。\n- 若启用了 zswap 且压缩存储成功，则跳过磁盘 I/O。\n- 若内存控制组禁用 zswap 回写，则返回 `AOP_WRITEPAGE_ACTIVATE` 以保留页面在内存中。\n- 对于 NFS 等不支持 `bmap` 的文件系统，使用 `swap_writepage_fs()` 路径，通过 `kiocb` 异步 DIO 写入。\n\n### 异步交换 I/O 批处理\n- `swap_writepage_fs()` 支持通过 `wbc->swap_plug` 合并多个相邻页面的写请求到同一个 `swap_iocb`。\n- 利用 `mempool` 保证在内存紧张时仍能分配 I/O 控制块。\n- 完成回调中处理部分写入错误，标记所有相关页面为 dirty 并结束写回。\n\n### 资源统计与控制\n- 通过 `count_swpout_vm_event()` 更新透明大页（THP）和普通页的交换出计数。\n- 在配置了 MEMCG 和 BLK_CGROUP 时，通过 `bio_associate_blkg_from_page()` 将 I/O 请求关联到页面所属的 blkcg，实现 I/O 资源隔离。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/swap.h>`、`<linux/pagemap.h>` 等，与页面回收、反向映射、内存控制组紧密集成。\n- **块设备层**：通过 `<linux/bio.h>`、`<linux/blkdev.h>` 与块 I/O 子系统交互。\n- **文件系统接口**：使用 `bmap()` 和 `kiocb` 与具体文件系统（如 ext4、xfs）或网络文件系统（如 NFS）协作。\n- **压缩子系统**：集成 `<linux/zswap.h>`，支持透明压缩交换缓存。\n- **资源控制器**：依赖 MEMCG 和 BLK_CGROUP 实现内存与 I/O 的多租户隔离。\n- **内部头文件**：包含本地 `\"swap.h\"` 获取交换子系统私有接口。\n\n## 5. 使用场景\n\n- **系统内存不足时**：页面回收机制调用 `swap_writepage()` 将匿名页换出到交换设备。\n- **启用交换文件时**：`swapon` 系统调用执行 `generic_swapfile_activate()` 初始化交换文件的物理布局。\n- **从交换区缺页中断**：当访问已换出页面时，内核通过 `end_swap_bio_read` 路径将数据读回内存。\n- **容器环境**：在启用内存和 I/O 控制组的系统中，确保交换 I/O 正确归属到对应 cgroup。\n- **使用压缩交换缓存**：当 zswap 启用时，优先尝试压缩页面而非立即写入慢速交换设备。\n- **网络交换场景**：在无本地块设备的环境中（如云实例使用 NFS 作为交换后端），通过 `swap_writepage_fs()` 路径完成交换。",
      "similarity": 0.5686158537864685,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "mm/page_io.c",
          "start_line": 406,
          "end_line": 537,
          "content": [
            "static void sio_read_complete(struct kiocb *iocb, long ret)",
            "{",
            "\tstruct swap_iocb *sio = container_of(iocb, struct swap_iocb, iocb);",
            "\tint p;",
            "",
            "\tif (ret == sio->len) {",
            "\t\tfor (p = 0; p < sio->pages; p++) {",
            "\t\t\tstruct folio *folio = page_folio(sio->bvec[p].bv_page);",
            "",
            "\t\t\tcount_mthp_stat(folio_order(folio), MTHP_STAT_SWPIN);",
            "\t\t\tcount_memcg_folio_events(folio, PSWPIN, folio_nr_pages(folio));",
            "\t\t\tfolio_mark_uptodate(folio);",
            "\t\t\tfolio_unlock(folio);",
            "\t\t}",
            "\t\tcount_vm_events(PSWPIN, sio->pages);",
            "\t} else {",
            "\t\tfor (p = 0; p < sio->pages; p++) {",
            "\t\t\tstruct folio *folio = page_folio(sio->bvec[p].bv_page);",
            "",
            "\t\t\tfolio_unlock(folio);",
            "\t\t}",
            "\t\tpr_alert_ratelimited(\"Read-error on swap-device\\n\");",
            "\t}",
            "\tmempool_free(sio, sio_pool);",
            "}",
            "static void swap_read_folio_fs(struct folio *folio, struct swap_iocb **plug)",
            "{",
            "\tstruct swap_info_struct *sis = swp_swap_info(folio->swap);",
            "\tstruct swap_iocb *sio = NULL;",
            "\tloff_t pos = folio_file_pos(folio);",
            "",
            "\tif (plug)",
            "\t\tsio = *plug;",
            "\tif (sio) {",
            "\t\tif (sio->iocb.ki_filp != sis->swap_file ||",
            "\t\t    sio->iocb.ki_pos + sio->len != pos) {",
            "\t\t\tswap_read_unplug(sio);",
            "\t\t\tsio = NULL;",
            "\t\t}",
            "\t}",
            "\tif (!sio) {",
            "\t\tsio = mempool_alloc(sio_pool, GFP_KERNEL);",
            "\t\tinit_sync_kiocb(&sio->iocb, sis->swap_file);",
            "\t\tsio->iocb.ki_pos = pos;",
            "\t\tsio->iocb.ki_complete = sio_read_complete;",
            "\t\tsio->pages = 0;",
            "\t\tsio->len = 0;",
            "\t}",
            "\tbvec_set_folio(&sio->bvec[sio->pages], folio, folio_size(folio), 0);",
            "\tsio->len += folio_size(folio);",
            "\tsio->pages += 1;",
            "\tif (sio->pages == ARRAY_SIZE(sio->bvec) || !plug) {",
            "\t\tswap_read_unplug(sio);",
            "\t\tsio = NULL;",
            "\t}",
            "\tif (plug)",
            "\t\t*plug = sio;",
            "}",
            "static void swap_read_folio_bdev_sync(struct folio *folio,",
            "\t\tstruct swap_info_struct *sis)",
            "{",
            "\tstruct bio_vec bv;",
            "\tstruct bio bio;",
            "",
            "\tbio_init(&bio, sis->bdev, &bv, 1, REQ_OP_READ);",
            "\tbio.bi_iter.bi_sector = swap_folio_sector(folio);",
            "\tbio_add_folio_nofail(&bio, folio, folio_size(folio), 0);",
            "\t/*",
            "\t * Keep this task valid during swap readpage because the oom killer may",
            "\t * attempt to access it in the page fault retry time check.",
            "\t */",
            "\tget_task_struct(current);",
            "\tcount_mthp_stat(folio_order(folio), MTHP_STAT_SWPIN);",
            "\tcount_memcg_folio_events(folio, PSWPIN, folio_nr_pages(folio));",
            "\tcount_vm_events(PSWPIN, folio_nr_pages(folio));",
            "\tsubmit_bio_wait(&bio);",
            "\t__end_swap_bio_read(&bio);",
            "\tput_task_struct(current);",
            "}",
            "static void swap_read_folio_bdev_async(struct folio *folio,",
            "\t\tstruct swap_info_struct *sis)",
            "{",
            "\tstruct bio *bio;",
            "",
            "\tbio = bio_alloc(sis->bdev, 1, REQ_OP_READ, GFP_KERNEL);",
            "\tbio->bi_iter.bi_sector = swap_folio_sector(folio);",
            "\tbio->bi_end_io = end_swap_bio_read;",
            "\tbio_add_folio_nofail(bio, folio, folio_size(folio), 0);",
            "\tcount_mthp_stat(folio_order(folio), MTHP_STAT_SWPIN);",
            "\tcount_memcg_folio_events(folio, PSWPIN, folio_nr_pages(folio));",
            "\tcount_vm_events(PSWPIN, folio_nr_pages(folio));",
            "\tsubmit_bio(bio);",
            "}",
            "void swap_read_folio(struct folio *folio, struct swap_iocb **plug)",
            "{",
            "\tstruct swap_info_struct *sis = swp_swap_info(folio->swap);",
            "\tbool synchronous = sis->flags & SWP_SYNCHRONOUS_IO;",
            "\tbool workingset = folio_test_workingset(folio);",
            "\tunsigned long pflags;",
            "\tbool in_thrashing;",
            "",
            "\tVM_BUG_ON_FOLIO(!folio_test_swapcache(folio) && !synchronous, folio);",
            "\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\tVM_BUG_ON_FOLIO(folio_test_uptodate(folio), folio);",
            "",
            "\t/*",
            "\t * Count submission time as memory stall and delay. When the device",
            "\t * is congested, or the submitting cgroup IO-throttled, submission",
            "\t * can be a significant part of overall IO time.",
            "\t */",
            "\tif (workingset) {",
            "\t\tdelayacct_thrashing_start(&in_thrashing);",
            "\t\tpsi_memstall_enter(&pflags);",
            "\t}",
            "\tdelayacct_swapin_start();",
            "",
            "\tif (zswap_load(folio)) {",
            "\t\tfolio_unlock(folio);",
            "\t} else if (data_race(sis->flags & SWP_FS_OPS)) {",
            "\t\tswap_read_folio_fs(folio, plug);",
            "\t} else if (synchronous) {",
            "\t\tswap_read_folio_bdev_sync(folio, sis);",
            "\t} else {",
            "\t\tswap_read_folio_bdev_async(folio, sis);",
            "\t}",
            "",
            "\tif (workingset) {",
            "\t\tdelayacct_thrashing_end(&in_thrashing);",
            "\t\tpsi_memstall_leave(&pflags);",
            "\t}",
            "\tdelayacct_swapin_end();",
            "}"
          ],
          "function_name": "sio_read_complete, swap_read_folio_fs, swap_read_folio_bdev_sync, swap_read_folio_bdev_async, swap_read_folio",
          "description": "swap_read_folio启动页面换入流程，根据配置选择文件级或块设备读取路径，处理交换数据加载及状态更新。",
          "similarity": 0.6026910543441772
        },
        {
          "chunk_id": 3,
          "file_path": "mm/page_io.c",
          "start_line": 298,
          "end_line": 400,
          "content": [
            "static void swap_writepage_fs(struct folio *folio, struct writeback_control *wbc)",
            "{",
            "\tstruct swap_iocb *sio = NULL;",
            "\tstruct swap_info_struct *sis = swp_swap_info(folio->swap);",
            "\tstruct file *swap_file = sis->swap_file;",
            "\tloff_t pos = folio_file_pos(folio);",
            "",
            "\tcount_swpout_vm_event(folio);",
            "\tfolio_start_writeback(folio);",
            "\tfolio_unlock(folio);",
            "\tif (wbc->swap_plug)",
            "\t\tsio = *wbc->swap_plug;",
            "\tif (sio) {",
            "\t\tif (sio->iocb.ki_filp != swap_file ||",
            "\t\t    sio->iocb.ki_pos + sio->len != pos) {",
            "\t\t\tswap_write_unplug(sio);",
            "\t\t\tsio = NULL;",
            "\t\t}",
            "\t}",
            "\tif (!sio) {",
            "\t\tsio = mempool_alloc(sio_pool, GFP_NOIO);",
            "\t\tinit_sync_kiocb(&sio->iocb, swap_file);",
            "\t\tsio->iocb.ki_complete = sio_write_complete;",
            "\t\tsio->iocb.ki_pos = pos;",
            "\t\tsio->pages = 0;",
            "\t\tsio->len = 0;",
            "\t}",
            "\tbvec_set_folio(&sio->bvec[sio->pages], folio, folio_size(folio), 0);",
            "\tsio->len += folio_size(folio);",
            "\tsio->pages += 1;",
            "\tif (sio->pages == ARRAY_SIZE(sio->bvec) || !wbc->swap_plug) {",
            "\t\tswap_write_unplug(sio);",
            "\t\tsio = NULL;",
            "\t}",
            "\tif (wbc->swap_plug)",
            "\t\t*wbc->swap_plug = sio;",
            "}",
            "static void swap_writepage_bdev_sync(struct folio *folio,",
            "\t\tstruct writeback_control *wbc, struct swap_info_struct *sis)",
            "{",
            "\tstruct bio_vec bv;",
            "\tstruct bio bio;",
            "",
            "\tbio_init(&bio, sis->bdev, &bv, 1,",
            "\t\t REQ_OP_WRITE | REQ_SWAP | wbc_to_write_flags(wbc));",
            "\tbio.bi_iter.bi_sector = swap_folio_sector(folio);",
            "\tbio_add_folio_nofail(&bio, folio, folio_size(folio), 0);",
            "",
            "\tbio_associate_blkg_from_page(&bio, folio);",
            "\tcount_swpout_vm_event(folio);",
            "",
            "\tfolio_start_writeback(folio);",
            "\tfolio_unlock(folio);",
            "",
            "\tsubmit_bio_wait(&bio);",
            "\t__end_swap_bio_write(&bio);",
            "}",
            "static void swap_writepage_bdev_async(struct folio *folio,",
            "\t\tstruct writeback_control *wbc, struct swap_info_struct *sis)",
            "{",
            "\tstruct bio *bio;",
            "",
            "\tbio = bio_alloc(sis->bdev, 1,",
            "\t\t\tREQ_OP_WRITE | REQ_SWAP | wbc_to_write_flags(wbc),",
            "\t\t\tGFP_NOIO);",
            "\tbio->bi_iter.bi_sector = swap_folio_sector(folio);",
            "\tbio->bi_end_io = end_swap_bio_write;",
            "\tbio_add_folio_nofail(bio, folio, folio_size(folio), 0);",
            "",
            "\tbio_associate_blkg_from_page(bio, folio);",
            "\tcount_swpout_vm_event(folio);",
            "\tfolio_start_writeback(folio);",
            "\tfolio_unlock(folio);",
            "\tsubmit_bio(bio);",
            "}",
            "void __swap_writepage(struct folio *folio, struct writeback_control *wbc)",
            "{",
            "\tstruct swap_info_struct *sis = swp_swap_info(folio->swap);",
            "",
            "\tVM_BUG_ON_FOLIO(!folio_test_swapcache(folio), folio);",
            "\t/*",
            "\t * ->flags can be updated non-atomicially (scan_swap_map_slots),",
            "\t * but that will never affect SWP_FS_OPS, so the data_race",
            "\t * is safe.",
            "\t */",
            "\tif (data_race(sis->flags & SWP_FS_OPS))",
            "\t\tswap_writepage_fs(folio, wbc);",
            "\telse if (sis->flags & SWP_SYNCHRONOUS_IO)",
            "\t\tswap_writepage_bdev_sync(folio, wbc, sis);",
            "\telse",
            "\t\tswap_writepage_bdev_async(folio, wbc, sis);",
            "}",
            "void swap_write_unplug(struct swap_iocb *sio)",
            "{",
            "\tstruct iov_iter from;",
            "\tstruct address_space *mapping = sio->iocb.ki_filp->f_mapping;",
            "\tint ret;",
            "",
            "\tiov_iter_bvec(&from, ITER_SOURCE, sio->bvec, sio->pages, sio->len);",
            "\tret = mapping->a_ops->swap_rw(&sio->iocb, &from);",
            "\tif (ret != -EIOCBQUEUED)",
            "\t\tsio_write_complete(&sio->iocb, ret);",
            "}"
          ],
          "function_name": "swap_writepage_fs, swap_writepage_bdev_sync, swap_writepage_bdev_async, __swap_writepage, swap_write_unplug",
          "description": "__swap_writepage根据配置选择同步/异步块设备写入路径，通过bio结构执行交换页面写入操作，支持批量提交优化。",
          "similarity": 0.6018823385238647
        },
        {
          "chunk_id": 0,
          "file_path": "mm/page_io.c",
          "start_line": 1,
          "end_line": 29,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " *  linux/mm/page_io.c",
            " *",
            " *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds",
            " *",
            " *  Swap reorganised 29.12.95, ",
            " *  Asynchronous swapping added 30.12.95. Stephen Tweedie",
            " *  Removed race in async swapping. 14.4.1996. Bruno Haible",
            " *  Add swap of shared pages through the page cache. 20.2.1998. Stephen Tweedie",
            " *  Always use brw_page, life becomes simpler. 12 May 1998 Eric Biederman",
            " */",
            "",
            "#include <linux/mm.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/gfp.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/swap.h>",
            "#include <linux/bio.h>",
            "#include <linux/swapops.h>",
            "#include <linux/writeback.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/psi.h>",
            "#include <linux/uio.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/zswap.h>",
            "#include \"swap.h\"",
            ""
          ],
          "function_name": null,
          "description": "声明头文件并引入交换页面管理模块的基本依赖，为后续交换操作提供基础框架。",
          "similarity": 0.5821625590324402
        },
        {
          "chunk_id": 5,
          "file_path": "mm/page_io.c",
          "start_line": 543,
          "end_line": 553,
          "content": [
            "void __swap_read_unplug(struct swap_iocb *sio)",
            "{",
            "\tstruct iov_iter from;",
            "\tstruct address_space *mapping = sio->iocb.ki_filp->f_mapping;",
            "\tint ret;",
            "",
            "\tiov_iter_bvec(&from, ITER_DEST, sio->bvec, sio->pages, sio->len);",
            "\tret = mapping->a_ops->swap_rw(&sio->iocb, &from);",
            "\tif (ret != -EIOCBQUEUED)",
            "\t\tsio_read_complete(&sio->iocb, ret);",
            "}"
          ],
          "function_name": "__swap_read_unplug",
          "description": "该函数是处理交换读取操作的关键函数，在页置换过程中将从交换设备读取的数据通过I/O向量传递至文件地址空间。  \n它通过构建iov_iter并调用address_space的a_ops->swap_rw接口进行实际的数据读取，并根据返回状态触发相应的完成回调。  \n注：swap_rw的具体实现依赖于对应文件系统的a_ops，此处上下文不完整。",
          "similarity": 0.5772098302841187
        },
        {
          "chunk_id": 2,
          "file_path": "mm/page_io.c",
          "start_line": 179,
          "end_line": 280,
          "content": [
            "int swap_writepage(struct page *page, struct writeback_control *wbc)",
            "{",
            "\tstruct folio *folio = page_folio(page);",
            "\tint ret;",
            "",
            "\tif (folio_free_swap(folio)) {",
            "\t\tfolio_unlock(folio);",
            "\t\treturn 0;",
            "\t}",
            "\t/*",
            "\t * Arch code may have to preserve more data than just the page",
            "\t * contents, e.g. memory tags.",
            "\t */",
            "\tret = arch_prepare_to_swap(folio);",
            "\tif (ret) {",
            "\t\tfolio_mark_dirty(folio);",
            "\t\tfolio_unlock(folio);",
            "\t\treturn ret;",
            "\t}",
            "\tif (zswap_store(folio)) {",
            "\t\tfolio_start_writeback(folio);",
            "\t\tfolio_unlock(folio);",
            "\t\tfolio_end_writeback(folio);",
            "\t\treturn 0;",
            "\t}",
            "\tif (!mem_cgroup_zswap_writeback_enabled(folio_memcg(folio))) {",
            "\t\tfolio_mark_dirty(folio);",
            "\t\treturn AOP_WRITEPAGE_ACTIVATE;",
            "\t}",
            "",
            "\t__swap_writepage(folio, wbc);",
            "\treturn 0;",
            "}",
            "static inline void count_swpout_vm_event(struct folio *folio)",
            "{",
            "#ifdef CONFIG_TRANSPARENT_HUGEPAGE",
            "\tif (unlikely(folio_test_pmd_mappable(folio))) {",
            "\t\tcount_memcg_folio_events(folio, THP_SWPOUT, 1);",
            "\t\tcount_vm_event(THP_SWPOUT);",
            "\t}",
            "\tcount_mthp_stat(folio_order(folio), MTHP_STAT_SWPOUT);",
            "#endif",
            "\tcount_memcg_folio_events(folio, PSWPOUT, folio_nr_pages(folio));",
            "\tcount_vm_events(PSWPOUT, folio_nr_pages(folio));",
            "}",
            "static void bio_associate_blkg_from_page(struct bio *bio, struct folio *folio)",
            "{",
            "\tstruct cgroup_subsys_state *css;",
            "\tstruct mem_cgroup *memcg;",
            "",
            "\tmemcg = folio_memcg(folio);",
            "\tif (!memcg)",
            "\t\treturn;",
            "",
            "\trcu_read_lock();",
            "\tcss = cgroup_e_css(memcg->css.cgroup, &io_cgrp_subsys);",
            "\tbio_associate_blkg_from_css(bio, css);",
            "\trcu_read_unlock();",
            "}",
            "int sio_pool_init(void)",
            "{",
            "\tif (!sio_pool) {",
            "\t\tmempool_t *pool = mempool_create_kmalloc_pool(",
            "\t\t\tSWAP_CLUSTER_MAX, sizeof(struct swap_iocb));",
            "\t\tif (cmpxchg(&sio_pool, NULL, pool))",
            "\t\t\tmempool_destroy(pool);",
            "\t}",
            "\tif (!sio_pool)",
            "\t\treturn -ENOMEM;",
            "\treturn 0;",
            "}",
            "static void sio_write_complete(struct kiocb *iocb, long ret)",
            "{",
            "\tstruct swap_iocb *sio = container_of(iocb, struct swap_iocb, iocb);",
            "\tstruct page *page = sio->bvec[0].bv_page;",
            "\tint p;",
            "",
            "\tif (ret != sio->len) {",
            "\t\t/*",
            "\t\t * In the case of swap-over-nfs, this can be a",
            "\t\t * temporary failure if the system has limited",
            "\t\t * memory for allocating transmit buffers.",
            "\t\t * Mark the page dirty and avoid",
            "\t\t * folio_rotate_reclaimable but rate-limit the",
            "\t\t * messages but do not flag PageError like",
            "\t\t * the normal direct-to-bio case as it could",
            "\t\t * be temporary.",
            "\t\t */",
            "\t\tpr_err_ratelimited(\"Write error %ld on dio swapfile (%llu)\\n\",",
            "\t\t\t\t   ret, page_file_offset(page));",
            "\t\tfor (p = 0; p < sio->pages; p++) {",
            "\t\t\tpage = sio->bvec[p].bv_page;",
            "\t\t\tset_page_dirty(page);",
            "\t\t\tClearPageReclaim(page);",
            "\t\t}",
            "\t}",
            "",
            "\tfor (p = 0; p < sio->pages; p++)",
            "\t\tend_page_writeback(sio->bvec[p].bv_page);",
            "",
            "\tmempool_free(sio, sio_pool);",
            "}"
          ],
          "function_name": "swap_writepage, count_swpout_vm_event, bio_associate_blkg_from_page, sio_pool_init, sio_write_complete",
          "description": "swap_writepage发起页面换出流程，调用架构特定准备函数并处理ZSwap缓存；sio_write_complete处理异步写入完成，标记脏页并释放资源。",
          "similarity": 0.5700883865356445
        }
      ]
    },
    {
      "source_file": "mm/page_alloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_alloc.c`\n\n---\n\n# page_alloc.c 技术文档\n\n## 1. 文件概述\n\n`page_alloc.c` 是 Linux 内核内存管理子系统的核心文件之一，负责物理页面的分配与释放。该文件实现了基于区域（zone）和迁移类型（migratetype）的伙伴系统（Buddy System）内存分配器，管理系统的空闲页链表，并提供高效的页面分配/回收机制。它不处理小对象分配（由 slab/slub/slob 子系统负责），而是专注于以页为单位的大块物理内存管理。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct per_cpu_pages`**：每个 CPU 的每区（per-zone）页面缓存，用于减少锁竞争，提升分配性能。\n- **`node_states[NR_NODE_STATES]`**：全局节点状态掩码数组，跟踪各 NUMA 节点的状态（如在线、有内存等）。\n- **`sysctl_lowmem_reserve_ratio[MAX_NR_ZONES]`**：各内存区域的低内存保留比例，防止高优先级区域耗尽低优先级区域的内存。\n- **`zone_names[]` 和 `migratetype_names[]`**：内存区域和页面迁移类型的名称字符串，用于调试和日志。\n- **`gfp_allowed_mask`**：全局 GFP（Get Free Page）标志掩码，控制启动早期可使用的分配标志。\n\n### 主要函数（部分声明）\n- **`__free_pages_ok()`**：内部页面释放函数，执行实际的伙伴系统合并与链表插入逻辑。\n- 各种页面分配函数（如 `alloc_pages()`、`__alloc_pages()` 等，定义在其他位置但在此文件中实现核心逻辑）。\n- 每 CPU 页面列表操作辅助宏（如 `pcp_spin_lock()`、`pcp_spin_trylock()`）。\n\n### 关键常量与标志\n- **`fpi_t` 类型及标志**：\n  - `FPI_NONE`：无特殊要求。\n  - `FPI_SKIP_REPORT_NOTIFY`：跳过空闲页报告通知。\n  - `FPI_TO_TAIL`：将页面放回空闲链表尾部（用于优化场景如内存热插拔）。\n- **`min_free_kbytes`**：系统保留的最小空闲内存（KB），影响水位线计算。\n\n## 3. 关键实现\n\n### 每 CPU 页面缓存（Per-CPU Page Caching）\n- 通过 `struct per_cpu_pages` 为每个 CPU 维护热/冷页列表，避免频繁访问全局 zone 锁。\n- 使用 `pcpu_spin_lock` 宏族安全地访问每 CPU 数据，结合 `preempt_disable()`（非 RT）或 `migrate_disable()`（RT）防止任务迁移导致访问错误 CPU 的数据。\n- 在 UP 系统上，使用 IRQ 关闭防止重入；在 SMP/RT 系统上依赖自旋锁语义。\n\n### 内存区域（Zone）与 NUMA 支持\n- 支持多种内存区域（DMA、DMA32、Normal、HighMem、Movable、Device），通过 `zone_names` 标识。\n- 实现 `lowmem_reserve_ratio` 机制，确保高区域分配不会耗尽低区域的保留内存（如 ZONE_DMA 为设备保留）。\n- 通过 `node_states` 和 per-CPU 变量（如 `numa_node`、`_numa_mem_`）支持 NUMA 和无内存节点架构。\n\n### 空闲页管理优化\n- **`FPI_TO_TAIL` 标志**：允许将页面放回空闲链表尾部，配合内存打乱（shuffle）或热插拔时批量初始化。\n- **`FPI_SKIP_REPORT_NOTIFY` 标志**：在临时取出并归还页面时不触发空闲页报告机制，减少开销。\n- **水位线与保留内存**：`min_free_kbytes` 控制最低水位，影响 OOM（Out-Of-Memory）决策和内存回收行为。\n\n### 实时内核（PREEMPT_RT）适配\n- 在 RT 内核中使用 `migrate_disable()` 替代 `preempt_disable()`，避免干扰 RT 自旋锁的优先级继承机制。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心内存管理**：`<linux/mm.h>`, `<linux/highmem.h>`, `\"internal.h\"`\n- **同步机制**：`<linux/spinlock.h>`（隐含）、`<linux/mutex.h>`\n- **NUMA 与拓扑**：`<linux/topology.h>`, `<linux/nodemask.h>`\n- **调试与追踪**：`<linux/kasan.h>`, `<trace/events/kmem.h>`, `<linux/page_owner.h>`\n- **高级特性**：`<linux/compaction.h>`, `<linux/migrate.h>`, `<linux/memcontrol.h>`\n\n### 子系统交互\n- **Slab 分配器**：本文件不处理 kmalloc，由 `slab.c` 等负责。\n- **内存回收**：与 `vmscan.c` 协同，通过水位线触发 reclaim。\n- **内存热插拔**：通过 `memory_hotplug.h` 接口管理动态内存。\n- **OOM Killer**：通过 `oom.h` 和水位线机制触发 OOM。\n- **透明大页（THP）**：与 `khugepaged` 协同进行大页分配。\n\n## 5. 使用场景\n\n- **内核内存分配**：所有以页为单位的内核内存请求（如 `alloc_pages()`）最终由本文件处理。\n- **用户空间缺页处理**：匿名页、文件页的物理页分配。\n- **内存映射（mmap）**：大块物理内存的分配与管理。\n- **内存回收与迁移**：页面回收、压缩（compaction）、迁移（migration）过程中涉及的页面释放与重新分配。\n- **系统启动与热插拔**：初始化内存区域、处理动态添加/移除内存。\n- **实时系统**：在 PREEMPT_RT 内核中提供低延迟的页面分配路径。\n- **调试与监控**：通过 page owner、KASAN、tracepoint 等机制提供内存使用追踪。",
      "similarity": 0.5535154342651367,
      "chunks": [
        {
          "chunk_id": 29,
          "file_path": "mm/page_alloc.c",
          "start_line": 5701,
          "end_line": 5807,
          "content": [
            "void __meminit setup_zone_pageset(struct zone *zone)",
            "{",
            "\tint cpu;",
            "",
            "\t/* Size may be 0 on !SMP && !NUMA */",
            "\tif (sizeof(struct per_cpu_zonestat) > 0)",
            "\t\tzone->per_cpu_zonestats = alloc_percpu(struct per_cpu_zonestat);",
            "",
            "\tzone->per_cpu_pageset = alloc_percpu(struct per_cpu_pages);",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct per_cpu_pages *pcp;",
            "\t\tstruct per_cpu_zonestat *pzstats;",
            "",
            "\t\tpcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);",
            "\t\tpzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);",
            "\t\tper_cpu_pages_init(pcp, pzstats);",
            "\t}",
            "",
            "\tzone_set_pageset_high_and_batch(zone, 0);",
            "}",
            "static void zone_pcp_update(struct zone *zone, int cpu_online)",
            "{",
            "\tmutex_lock(&pcp_batch_high_lock);",
            "\tzone_set_pageset_high_and_batch(zone, cpu_online);",
            "\tmutex_unlock(&pcp_batch_high_lock);",
            "}",
            "static void zone_pcp_update_cacheinfo(struct zone *zone, unsigned int cpu)",
            "{",
            "\tstruct per_cpu_pages *pcp;",
            "\tstruct cpu_cacheinfo *cci;",
            "",
            "\tpcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);",
            "\tcci = get_cpu_cacheinfo(cpu);",
            "\t/*",
            "\t * If data cache slice of CPU is large enough, \"pcp->batch\"",
            "\t * pages can be preserved in PCP before draining PCP for",
            "\t * consecutive high-order pages freeing without allocation.",
            "\t * This can reduce zone lock contention without hurting",
            "\t * cache-hot pages sharing.",
            "\t */",
            "\tspin_lock(&pcp->lock);",
            "\tif ((cci->per_cpu_data_slice_size >> PAGE_SHIFT) > 3 * pcp->batch)",
            "\t\tpcp->flags |= PCPF_FREE_HIGH_BATCH;",
            "\telse",
            "\t\tpcp->flags &= ~PCPF_FREE_HIGH_BATCH;",
            "\tspin_unlock(&pcp->lock);",
            "}",
            "void setup_pcp_cacheinfo(unsigned int cpu)",
            "{",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_populated_zone(zone)",
            "\t\tzone_pcp_update_cacheinfo(zone, cpu);",
            "}",
            "void __init setup_per_cpu_pageset(void)",
            "{",
            "\tstruct pglist_data *pgdat;",
            "\tstruct zone *zone;",
            "\tint __maybe_unused cpu;",
            "",
            "\tfor_each_populated_zone(zone)",
            "\t\tsetup_zone_pageset(zone);",
            "",
            "#ifdef CONFIG_NUMA",
            "\t/*",
            "\t * Unpopulated zones continue using the boot pagesets.",
            "\t * The numa stats for these pagesets need to be reset.",
            "\t * Otherwise, they will end up skewing the stats of",
            "\t * the nodes these zones are associated with.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct per_cpu_zonestat *pzstats = &per_cpu(boot_zonestats, cpu);",
            "\t\tmemset(pzstats->vm_numa_event, 0,",
            "\t\t       sizeof(pzstats->vm_numa_event));",
            "\t}",
            "#endif",
            "",
            "\tfor_each_online_pgdat(pgdat)",
            "\t\tpgdat->per_cpu_nodestats =",
            "\t\t\talloc_percpu(struct per_cpu_nodestat);",
            "}",
            "__meminit void zone_pcp_init(struct zone *zone)",
            "{",
            "\t/*",
            "\t * per cpu subsystem is not up at this point. The following code",
            "\t * relies on the ability of the linker to provide the",
            "\t * offset of a (static) per cpu variable into the per cpu area.",
            "\t */",
            "\tzone->per_cpu_pageset = &boot_pageset;",
            "\tzone->per_cpu_zonestats = &boot_zonestats;",
            "\tzone->pageset_high_min = BOOT_PAGESET_HIGH;",
            "\tzone->pageset_high_max = BOOT_PAGESET_HIGH;",
            "\tzone->pageset_batch = BOOT_PAGESET_BATCH;",
            "",
            "\tif (populated_zone(zone))",
            "\t\tpr_debug(\"  %s zone: %lu pages, LIFO batch:%u\\n\", zone->name,",
            "\t\t\t zone->present_pages, zone_batchsize(zone));",
            "}",
            "void adjust_managed_page_count(struct page *page, long count)",
            "{",
            "\tatomic_long_add(count, &page_zone(page)->managed_pages);",
            "\ttotalram_pages_add(count);",
            "#ifdef CONFIG_HIGHMEM",
            "\tif (PageHighMem(page))",
            "\t\ttotalhigh_pages_add(count);",
            "#endif",
            "}"
          ],
          "function_name": "setup_zone_pageset, zone_pcp_update, zone_pcp_update_cacheinfo, setup_pcp_cacheinfo, setup_per_cpu_pageset, zone_pcp_init, adjust_managed_page_count",
          "description": "分配并初始化每个区的页集结构，设置初始参数，根据CPU状态更新页集配置，维护管理页面计数统计信息。",
          "similarity": 0.6034018993377686
        },
        {
          "chunk_id": 27,
          "file_path": "mm/page_alloc.c",
          "start_line": 5441,
          "end_line": 5545,
          "content": [
            "static noinline void __init",
            "build_all_zonelists_init(void)",
            "{",
            "\tint cpu;",
            "",
            "\t__build_all_zonelists(NULL);",
            "",
            "\t/*",
            "\t * Initialize the boot_pagesets that are going to be used",
            "\t * for bootstrapping processors. The real pagesets for",
            "\t * each zone will be allocated later when the per cpu",
            "\t * allocator is available.",
            "\t *",
            "\t * boot_pagesets are used also for bootstrapping offline",
            "\t * cpus if the system is already booted because the pagesets",
            "\t * are needed to initialize allocators on a specific cpu too.",
            "\t * F.e. the percpu allocator needs the page allocator which",
            "\t * needs the percpu allocator in order to allocate its pagesets",
            "\t * (a chicken-egg dilemma).",
            "\t */",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu_pages_init(&per_cpu(boot_pageset, cpu), &per_cpu(boot_zonestats, cpu));",
            "",
            "\tmminit_verify_zonelist();",
            "\tcpuset_init_current_mems_allowed();",
            "}",
            "void __ref build_all_zonelists(pg_data_t *pgdat)",
            "{",
            "\tunsigned long vm_total_pages;",
            "",
            "\tif (system_state == SYSTEM_BOOTING) {",
            "\t\tbuild_all_zonelists_init();",
            "\t} else {",
            "\t\t__build_all_zonelists(pgdat);",
            "\t\t/* cpuset refresh routine should be here */",
            "\t}",
            "\t/* Get the number of free pages beyond high watermark in all zones. */",
            "\tvm_total_pages = nr_free_zone_pages(gfp_zone(GFP_HIGHUSER_MOVABLE));",
            "\t/*",
            "\t * Disable grouping by mobility if the number of pages in the",
            "\t * system is too low to allow the mechanism to work. It would be",
            "\t * more accurate, but expensive to check per-zone. This check is",
            "\t * made on memory-hotadd so a system can start with mobility",
            "\t * disabled and enable it later",
            "\t */",
            "\tif (vm_total_pages < (pageblock_nr_pages * MIGRATE_TYPES))",
            "\t\tpage_group_by_mobility_disabled = 1;",
            "\telse",
            "\t\tpage_group_by_mobility_disabled = 0;",
            "",
            "\tpr_info(\"Built %u zonelists, mobility grouping %s.  Total pages: %ld\\n\",",
            "\t\tnr_online_nodes,",
            "\t\tpage_group_by_mobility_disabled ? \"off\" : \"on\",",
            "\t\tvm_total_pages);",
            "#ifdef CONFIG_NUMA",
            "\tpr_info(\"Policy zone: %s\\n\", zone_names[policy_zone]);",
            "#endif",
            "}",
            "static int zone_batchsize(struct zone *zone)",
            "{",
            "#ifdef CONFIG_MMU",
            "\tint batch;",
            "",
            "\t/*",
            "\t * The number of pages to batch allocate is either ~0.1%",
            "\t * of the zone or 1MB, whichever is smaller. The batch",
            "\t * size is striking a balance between allocation latency",
            "\t * and zone lock contention.",
            "\t */",
            "\tbatch = min(zone_managed_pages(zone) >> 10, SZ_1M / PAGE_SIZE);",
            "\tbatch /= 4;\t\t/* We effectively *= 4 below */",
            "\tif (batch < 1)",
            "\t\tbatch = 1;",
            "",
            "\t/*",
            "\t * Clamp the batch to a 2^n - 1 value. Having a power",
            "\t * of 2 value was found to be more likely to have",
            "\t * suboptimal cache aliasing properties in some cases.",
            "\t *",
            "\t * For example if 2 tasks are alternately allocating",
            "\t * batches of pages, one task can end up with a lot",
            "\t * of pages of one half of the possible page colors",
            "\t * and the other with pages of the other colors.",
            "\t */",
            "\tbatch = rounddown_pow_of_two(batch + batch/2) - 1;",
            "",
            "\treturn batch;",
            "",
            "#else",
            "\t/* The deferral and batching of frees should be suppressed under NOMMU",
            "\t * conditions.",
            "\t *",
            "\t * The problem is that NOMMU needs to be able to allocate large chunks",
            "\t * of contiguous memory as there's no hardware page translation to",
            "\t * assemble apparent contiguous memory from discontiguous pages.",
            "\t *",
            "\t * Queueing large contiguous runs of pages for batching, however,",
            "\t * causes the pages to actually be freed in smaller chunks.  As there",
            "\t * can be a significant delay between the individual batches being",
            "\t * recycled, this leads to the once large chunks of space being",
            "\t * fragmented and becoming unavailable for high-order allocations.",
            "\t */",
            "\treturn 0;",
            "#endif",
            "}"
          ],
          "function_name": "build_all_zonelists_init, build_all_zonelists, zone_batchsize",
          "description": "初始化页集结构，计算并设置每个区的页面批量大小，根据系统状态决定是否启用移动性分组，打印zonelist构建信息。",
          "similarity": 0.5874533653259277
        },
        {
          "chunk_id": 16,
          "file_path": "mm/page_alloc.c",
          "start_line": 2768,
          "end_line": 2902,
          "content": [
            "void split_page(struct page *page, unsigned int order)",
            "{",
            "\tint i;",
            "",
            "\tVM_BUG_ON_PAGE(PageCompound(page), page);",
            "\tVM_BUG_ON_PAGE(!page_count(page), page);",
            "",
            "\tfor (i = 1; i < (1 << order); i++)",
            "\t\tset_page_refcounted(page + i);",
            "\tsplit_page_owner(page, order, 0);",
            "\tpgalloc_tag_split(page_folio(page), order, 0);",
            "\tsplit_page_memcg(page, order, 0);",
            "}",
            "int __isolate_free_page(struct page *page, unsigned int order)",
            "{",
            "\tstruct zone *zone = page_zone(page);",
            "\tint mt = get_pageblock_migratetype(page);",
            "",
            "\tif (!is_migrate_isolate(mt)) {",
            "\t\tunsigned long watermark;",
            "\t\t/*",
            "\t\t * Obey watermarks as if the page was being allocated. We can",
            "\t\t * emulate a high-order watermark check with a raised order-0",
            "\t\t * watermark, because we already know our high-order page",
            "\t\t * exists.",
            "\t\t */",
            "\t\twatermark = zone->_watermark[WMARK_MIN] + (1UL << order);",
            "\t\tif (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))",
            "\t\t\treturn 0;",
            "\t}",
            "",
            "\tdel_page_from_free_list(page, zone, order, mt);",
            "",
            "\t/*",
            "\t * Set the pageblock if the isolated page is at least half of a",
            "\t * pageblock",
            "\t */",
            "\tif (order >= pageblock_order - 1) {",
            "\t\tstruct page *endpage = page + (1 << order) - 1;",
            "\t\tfor (; page < endpage; page += pageblock_nr_pages) {",
            "\t\t\tint mt = get_pageblock_migratetype(page);",
            "\t\t\t/*",
            "\t\t\t * Only change normal pageblocks (i.e., they can merge",
            "\t\t\t * with others)",
            "\t\t\t */",
            "\t\t\tif (migratetype_is_mergeable(mt))",
            "\t\t\t\tmove_freepages_block(zone, page, mt,",
            "\t\t\t\t\t\t     MIGRATE_MOVABLE);",
            "\t\t}",
            "\t}",
            "",
            "\treturn 1UL << order;",
            "}",
            "void __putback_isolated_page(struct page *page, unsigned int order, int mt)",
            "{",
            "\tstruct zone *zone = page_zone(page);",
            "",
            "\t/* zone lock should be held when this function is called */",
            "\tlockdep_assert_held(&zone->lock);",
            "",
            "\t/* Return isolated page to tail of freelist. */",
            "\t__free_one_page(page, page_to_pfn(page), zone, order, mt,",
            "\t\t\tFPI_SKIP_REPORT_NOTIFY | FPI_TO_TAIL);",
            "}",
            "static inline void zone_statistics(struct zone *preferred_zone, struct zone *z,",
            "\t\t\t\t   long nr_account)",
            "{",
            "#ifdef CONFIG_NUMA",
            "\tenum numa_stat_item local_stat = NUMA_LOCAL;",
            "",
            "\t/* skip numa counters update if numa stats is disabled */",
            "\tif (!static_branch_likely(&vm_numa_stat_key))",
            "\t\treturn;",
            "",
            "\tif (zone_to_nid(z) != numa_node_id())",
            "\t\tlocal_stat = NUMA_OTHER;",
            "",
            "\tif (zone_to_nid(z) == zone_to_nid(preferred_zone))",
            "\t\t__count_numa_events(z, NUMA_HIT, nr_account);",
            "\telse {",
            "\t\t__count_numa_events(z, NUMA_MISS, nr_account);",
            "\t\t__count_numa_events(preferred_zone, NUMA_FOREIGN, nr_account);",
            "\t}",
            "\t__count_numa_events(z, local_stat, nr_account);",
            "#endif",
            "}",
            "static int nr_pcp_alloc(struct per_cpu_pages *pcp, struct zone *zone, int order)",
            "{",
            "\tint high, base_batch, batch, max_nr_alloc;",
            "\tint high_max, high_min;",
            "",
            "\tbase_batch = READ_ONCE(pcp->batch);",
            "\thigh_min = READ_ONCE(pcp->high_min);",
            "\thigh_max = READ_ONCE(pcp->high_max);",
            "\thigh = pcp->high = clamp(pcp->high, high_min, high_max);",
            "",
            "\t/* Check for PCP disabled or boot pageset */",
            "\tif (unlikely(high < base_batch))",
            "\t\treturn 1;",
            "",
            "\tif (order)",
            "\t\tbatch = base_batch;",
            "\telse",
            "\t\tbatch = (base_batch << pcp->alloc_factor);",
            "",
            "\t/*",
            "\t * If we had larger pcp->high, we could avoid to allocate from",
            "\t * zone.",
            "\t */",
            "\tif (high_min != high_max && !test_bit(ZONE_BELOW_HIGH, &zone->flags))",
            "\t\thigh = pcp->high = min(high + batch, high_max);",
            "",
            "\tif (!order) {",
            "\t\tmax_nr_alloc = max(high - pcp->count - base_batch, base_batch);",
            "\t\t/*",
            "\t\t * Double the number of pages allocated each time there is",
            "\t\t * subsequent allocation of order-0 pages without any freeing.",
            "\t\t */",
            "\t\tif (batch <= max_nr_alloc &&",
            "\t\t    pcp->alloc_factor < CONFIG_PCP_BATCH_SCALE_MAX)",
            "\t\t\tpcp->alloc_factor++;",
            "\t\tbatch = min(batch, max_nr_alloc);",
            "\t}",
            "",
            "\t/*",
            "\t * Scale batch relative to order if batch implies free pages",
            "\t * can be stored on the PCP. Batch can be 1 for small zones or",
            "\t * for boot pagesets which should never store free pages as",
            "\t * the pages may belong to arbitrary zones.",
            "\t */",
            "\tif (batch > 1)",
            "\t\tbatch = max(batch >> order, 2);",
            "",
            "\treturn batch;",
            "}"
          ],
          "function_name": "split_page, __isolate_free_page, __putback_isolated_page, zone_statistics, nr_pcp_alloc",
          "description": "split_page 分裂复合页面为独立页；__isolate_free_page 将隔离页从自由列表移除并调整页块类型；__putback_isolated_page 将隔离页重新放入自由列表尾部；nr_pcp_alloc 计算 PCP 列表可容纳的页面数量，动态调整批次大小以优化内存分配效率。",
          "similarity": 0.580874502658844
        },
        {
          "chunk_id": 10,
          "file_path": "mm/page_alloc.c",
          "start_line": 1699,
          "end_line": 1802,
          "content": [
            "static int move_freepages_block(struct zone *zone, struct page *page,",
            "\t\t\t\tint old_mt, int new_mt)",
            "{",
            "\tunsigned long start_pfn, end_pfn;",
            "",
            "\tif (!prep_move_freepages_block(zone, page, &start_pfn, &end_pfn,",
            "\t\t\t\t       NULL, NULL))",
            "\t\treturn -1;",
            "",
            "\treturn move_freepages(zone, start_pfn, end_pfn, old_mt, new_mt);",
            "}",
            "static unsigned long find_large_buddy(unsigned long start_pfn)",
            "{",
            "\tint order = 0;",
            "\tstruct page *page;",
            "\tunsigned long pfn = start_pfn;",
            "",
            "\twhile (!PageBuddy(page = pfn_to_page(pfn))) {",
            "\t\t/* Nothing found */",
            "\t\tif (++order > MAX_PAGE_ORDER)",
            "\t\t\treturn start_pfn;",
            "\t\tpfn &= ~0UL << order;",
            "\t}",
            "",
            "\t/*",
            "\t * Found a preceding buddy, but does it straddle?",
            "\t */",
            "\tif (pfn + (1 << buddy_order(page)) > start_pfn)",
            "\t\treturn pfn;",
            "",
            "\t/* Nothing found */",
            "\treturn start_pfn;",
            "}",
            "bool move_freepages_block_isolate(struct zone *zone, struct page *page,",
            "\t\t\t\t  int migratetype)",
            "{",
            "\tunsigned long start_pfn, end_pfn, pfn;",
            "",
            "\tif (!prep_move_freepages_block(zone, page, &start_pfn, &end_pfn,",
            "\t\t\t\t       NULL, NULL))",
            "\t\treturn false;",
            "",
            "\t/* No splits needed if buddies can't span multiple blocks */",
            "\tif (pageblock_order == MAX_PAGE_ORDER)",
            "\t\tgoto move;",
            "",
            "\t/* We're a tail block in a larger buddy */",
            "\tpfn = find_large_buddy(start_pfn);",
            "\tif (pfn != start_pfn) {",
            "\t\tstruct page *buddy = pfn_to_page(pfn);",
            "\t\tint order = buddy_order(buddy);",
            "",
            "\t\tdel_page_from_free_list(buddy, zone, order,",
            "\t\t\t\t\tget_pfnblock_migratetype(buddy, pfn));",
            "\t\tset_pageblock_migratetype(page, migratetype);",
            "\t\tsplit_large_buddy(zone, buddy, pfn, order, FPI_NONE);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/* We're the starting block of a larger buddy */",
            "\tif (PageBuddy(page) && buddy_order(page) > pageblock_order) {",
            "\t\tint order = buddy_order(page);",
            "",
            "\t\tdel_page_from_free_list(page, zone, order,",
            "\t\t\t\t\tget_pfnblock_migratetype(page, pfn));",
            "\t\tset_pageblock_migratetype(page, migratetype);",
            "\t\tsplit_large_buddy(zone, page, pfn, order, FPI_NONE);",
            "\t\treturn true;",
            "\t}",
            "move:",
            "\tmove_freepages(zone, start_pfn, end_pfn,",
            "\t\t       get_pfnblock_migratetype(page, start_pfn), migratetype);",
            "\treturn true;",
            "}",
            "static void change_pageblock_range(struct page *pageblock_page,",
            "\t\t\t\t\tint start_order, int migratetype)",
            "{",
            "\tint nr_pageblocks = 1 << (start_order - pageblock_order);",
            "",
            "\twhile (nr_pageblocks--) {",
            "\t\tset_pageblock_migratetype(pageblock_page, migratetype);",
            "\t\tpageblock_page += pageblock_nr_pages;",
            "\t}",
            "}",
            "static bool can_steal_fallback(unsigned int order, int start_mt)",
            "{",
            "\t/*",
            "\t * Leaving this order check is intended, although there is",
            "\t * relaxed order check in next check. The reason is that",
            "\t * we can actually steal whole pageblock if this condition met,",
            "\t * but, below check doesn't guarantee it and that is just heuristic",
            "\t * so could be changed anytime.",
            "\t */",
            "\tif (order >= pageblock_order)",
            "\t\treturn true;",
            "",
            "\tif (order >= pageblock_order / 2 ||",
            "\t\tstart_mt == MIGRATE_RECLAIMABLE ||",
            "\t\tstart_mt == MIGRATE_UNMOVABLE ||",
            "\t\tpage_group_by_mobility_disabled)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "move_freepages_block, find_large_buddy, move_freepages_block_isolate, change_pageblock_range, can_steal_fallback",
          "description": "实现页面块迁移逻辑，通过prep_move_freepages_block准备并调用move_freepages进行迁移，处理大块伙伴页框的拆分和迁移类型转换，用于内存碎片整理和分配策略调整。",
          "similarity": 0.5706820487976074
        },
        {
          "chunk_id": 23,
          "file_path": "mm/page_alloc.c",
          "start_line": 4471,
          "end_line": 4660,
          "content": [
            "static inline bool prepare_alloc_pages(gfp_t gfp_mask, unsigned int order,",
            "\t\tint preferred_nid, nodemask_t *nodemask,",
            "\t\tstruct alloc_context *ac, gfp_t *alloc_gfp,",
            "\t\tunsigned int *alloc_flags)",
            "{",
            "\tac->highest_zoneidx = gfp_zone(gfp_mask);",
            "\tac->zonelist = node_zonelist(preferred_nid, gfp_mask);",
            "\tac->nodemask = nodemask;",
            "\tac->migratetype = gfp_migratetype(gfp_mask);",
            "",
            "\tif (cpusets_enabled()) {",
            "\t\t*alloc_gfp |= __GFP_HARDWALL;",
            "\t\t/*",
            "\t\t * When we are in the interrupt context, it is irrelevant",
            "\t\t * to the current task context. It means that any node ok.",
            "\t\t */",
            "\t\tif (in_task() && !ac->nodemask)",
            "\t\t\tac->nodemask = &cpuset_current_mems_allowed;",
            "\t\telse",
            "\t\t\t*alloc_flags |= ALLOC_CPUSET;",
            "\t}",
            "",
            "\tmight_alloc(gfp_mask);",
            "",
            "\tif (should_fail_alloc_page(gfp_mask, order))",
            "\t\treturn false;",
            "",
            "\t*alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, *alloc_flags);",
            "",
            "\t/* Dirty zone balancing only done in the fast path */",
            "\tac->spread_dirty_pages = (gfp_mask & __GFP_WRITE);",
            "",
            "\t/*",
            "\t * The preferred zone is used for statistics but crucially it is",
            "\t * also used as the starting point for the zonelist iterator. It",
            "\t * may get reset for allocations that ignore memory policies.",
            "\t */",
            "\tac->preferred_zoneref = first_zones_zonelist(ac->zonelist,",
            "\t\t\t\t\tac->highest_zoneidx, ac->nodemask);",
            "",
            "\treturn true;",
            "}",
            "unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,",
            "\t\t\tnodemask_t *nodemask, int nr_pages,",
            "\t\t\tstruct list_head *page_list,",
            "\t\t\tstruct page **page_array)",
            "{",
            "\tstruct page *page;",
            "\tunsigned long __maybe_unused UP_flags;",
            "\tstruct zone *zone;",
            "\tstruct zoneref *z;",
            "\tstruct per_cpu_pages *pcp;",
            "\tstruct list_head *pcp_list;",
            "\tstruct alloc_context ac;",
            "\tgfp_t alloc_gfp;",
            "\tunsigned int alloc_flags = ALLOC_WMARK_LOW;",
            "\tint nr_populated = 0, nr_account = 0;",
            "",
            "\t/*",
            "\t * Skip populated array elements to determine if any pages need",
            "\t * to be allocated before disabling IRQs.",
            "\t */",
            "\twhile (page_array && nr_populated < nr_pages && page_array[nr_populated])",
            "\t\tnr_populated++;",
            "",
            "\t/* No pages requested? */",
            "\tif (unlikely(nr_pages <= 0))",
            "\t\tgoto out;",
            "",
            "\t/* Already populated array? */",
            "\tif (unlikely(page_array && nr_pages - nr_populated == 0))",
            "\t\tgoto out;",
            "",
            "\t/* Bulk allocator does not support memcg accounting. */",
            "\tif (memcg_kmem_online() && (gfp & __GFP_ACCOUNT))",
            "\t\tgoto failed;",
            "",
            "\t/* Use the single page allocator for one page. */",
            "\tif (nr_pages - nr_populated == 1)",
            "\t\tgoto failed;",
            "",
            "#ifdef CONFIG_PAGE_OWNER",
            "\t/*",
            "\t * PAGE_OWNER may recurse into the allocator to allocate space to",
            "\t * save the stack with pagesets.lock held. Releasing/reacquiring",
            "\t * removes much of the performance benefit of bulk allocation so",
            "\t * force the caller to allocate one page at a time as it'll have",
            "\t * similar performance to added complexity to the bulk allocator.",
            "\t */",
            "\tif (static_branch_unlikely(&page_owner_inited))",
            "\t\tgoto failed;",
            "#endif",
            "",
            "\t/* May set ALLOC_NOFRAGMENT, fragmentation will return 1 page. */",
            "\tgfp &= gfp_allowed_mask;",
            "\talloc_gfp = gfp;",
            "\tif (!prepare_alloc_pages(gfp, 0, preferred_nid, nodemask, &ac, &alloc_gfp, &alloc_flags))",
            "\t\tgoto out;",
            "\tgfp = alloc_gfp;",
            "",
            "\t/* Find an allowed local zone that meets the low watermark. */",
            "\tz = ac.preferred_zoneref;",
            "\tfor_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {",
            "\t\tunsigned long mark;",
            "",
            "\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&",
            "\t\t    !__cpuset_zone_allowed(zone, gfp)) {",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tif (nr_online_nodes > 1 && zone != ac.preferred_zoneref->zone &&",
            "\t\t    zone_to_nid(zone) != zone_to_nid(ac.preferred_zoneref->zone)) {",
            "\t\t\tgoto failed;",
            "\t\t}",
            "",
            "\t\tmark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK) + nr_pages;",
            "\t\tif (zone_watermark_fast(zone, 0,  mark,",
            "\t\t\t\tzonelist_zone_idx(ac.preferred_zoneref),",
            "\t\t\t\talloc_flags, gfp)) {",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * If there are no allowed local zones that meets the watermarks then",
            "\t * try to allocate a single page and reclaim if necessary.",
            "\t */",
            "\tif (unlikely(!zone))",
            "\t\tgoto failed;",
            "",
            "\t/* spin_trylock may fail due to a parallel drain or IRQ reentrancy. */",
            "\tpcp_trylock_prepare(UP_flags);",
            "\tpcp = pcp_spin_trylock(zone->per_cpu_pageset);",
            "\tif (!pcp)",
            "\t\tgoto failed_irq;",
            "",
            "\t/* Attempt the batch allocation */",
            "\tpcp_list = &pcp->lists[order_to_pindex(ac.migratetype, 0)];",
            "\twhile (nr_populated < nr_pages) {",
            "",
            "\t\t/* Skip existing pages */",
            "\t\tif (page_array && page_array[nr_populated]) {",
            "\t\t\tnr_populated++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tpage = __rmqueue_pcplist(zone, 0, ac.migratetype, alloc_flags,",
            "\t\t\t\t\t\t\t\tpcp, pcp_list);",
            "\t\tif (unlikely(!page)) {",
            "\t\t\t/* Try and allocate at least one page */",
            "\t\t\tif (!nr_account) {",
            "\t\t\t\tpcp_spin_unlock(pcp);",
            "\t\t\t\tgoto failed_irq;",
            "\t\t\t}",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tnr_account++;",
            "",
            "\t\tprep_new_page(page, 0, gfp, 0);",
            "\t\tif (page_list)",
            "\t\t\tlist_add(&page->lru, page_list);",
            "\t\telse",
            "\t\t\tpage_array[nr_populated] = page;",
            "\t\tnr_populated++;",
            "\t}",
            "",
            "\tpcp_spin_unlock(pcp);",
            "\tpcp_trylock_finish(UP_flags);",
            "",
            "\t__count_zid_vm_events(PGALLOC, zone_idx(zone), nr_account);",
            "\tzone_statistics(ac.preferred_zoneref->zone, zone, nr_account);",
            "",
            "out:",
            "\treturn nr_populated;",
            "",
            "failed_irq:",
            "\tpcp_trylock_finish(UP_flags);",
            "",
            "failed:",
            "\tpage = __alloc_pages_noprof(gfp, 0, preferred_nid, nodemask);",
            "\tif (page) {",
            "\t\tif (page_list)",
            "\t\t\tlist_add(&page->lru, page_list);",
            "\t\telse",
            "\t\t\tpage_array[nr_populated] = page;",
            "\t\tnr_populated++;",
            "\t}",
            "",
            "\tgoto out;",
            "}"
          ],
          "function_name": "prepare_alloc_pages, alloc_pages_bulk_noprof",
          "description": "该代码段实现了内存页面的批量分配逻辑，其中`prepare_alloc_pages`用于初始化分配上下文参数并配置内存策略，`alloc_pages_bulk_noprof`则通过遍历内存区域尝试批量分配连续页面，优先使用本地节点且支持CPU集约束。  \n`prepare_alloc_pages`构建分配上下文，设置Zone列表、迁移类型及节点掩码，处理CPU集隔离与水位线检查；`alloc_pages_bulk_noprof`在满足水位线前提下批量分配页面，失败时回退至单页分配。  \n上下文完整，未引入未展示的API或机制。",
          "similarity": 0.5696754455566406
        }
      ]
    },
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.5532779097557068,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.5376827716827393
        },
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.49306708574295044
        },
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.44261986017227173
        }
      ]
    }
  ]
}