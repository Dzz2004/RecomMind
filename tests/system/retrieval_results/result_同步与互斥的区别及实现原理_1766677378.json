{
  "query": "同步与互斥的区别及实现原理",
  "timestamp": "2025-12-25 23:42:58",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/mutex.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:42:50\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\mutex.c`\n\n---\n\n# Linux 内核互斥锁（mutex）实现文档\n\n## 1. 文件概述\n\n`locking/mutex.c` 是 Linux 内核中互斥锁（mutex）的核心实现文件，提供了基于阻塞的互斥同步原语。该文件实现了高效、可睡眠的互斥锁机制，支持自旋优化、锁移交（handoff）、调试功能以及与调度器、死锁检测等子系统的深度集成。互斥锁用于保护临界区，确保同一时间只有一个任务可以持有锁，适用于需要长时间持有锁或可能睡眠的场景。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `__mutex_init()`：初始化互斥锁对象\n- `mutex_is_locked()`：检查互斥锁是否已被持有\n- `mutex_get_owner()`：获取当前锁持有者的任务指针（仅用于调试）\n- `__mutex_trylock()`：尝试获取互斥锁（非阻塞）\n- `__mutex_trylock_fast()`：快速路径尝试获取未竞争的锁\n- `__mutex_unlock_fast()`：快速路径释放锁\n- `__mutex_lock_slowpath()`：慢速路径获取锁（包含睡眠和等待逻辑）\n- `__mutex_handoff()`：将锁所有权移交给指定任务\n- `__mutex_add_waiter()` / `__mutex_remove_waiter()`：管理等待队列\n\n### 关键数据结构\n\n- `struct mutex`：互斥锁核心结构体\n  - `atomic_long_t owner`：原子存储锁持有者指针和状态标志\n  - `raw_spinlock_t wait_lock`：保护等待队列的自旋锁\n  - `struct list_head wait_list`：等待获取锁的任务队列\n  - `struct optimistic_spin_queue osq`：用于自旋优化的队列（CONFIG_MUTEX_SPIN_ON_OWNER）\n\n### 状态标志位\n\n- `MUTEX_FLAG_WAITERS (0x01)`：表示存在等待者，解锁时需唤醒\n- `MUTEX_FLAG_HANDOFF (0x02)`：表示需要将锁移交给队首等待者\n- `MUTEX_FLAG_PICKUP (0x04)`：表示锁已被移交给特定任务，等待其获取\n\n## 3. 关键实现\n\n### 锁状态编码\n互斥锁的 `owner` 字段采用指针-标志位混合编码：利用 `task_struct` 指针的低 3 位（因内存对齐保证为 0）存储状态标志。这种设计避免了额外的内存访问，提高了原子操作效率。\n\n### 快慢路径分离\n- **快速路径**：针对无竞争场景，直接通过原子比较交换（cmpxchg）获取/释放锁，避免函数调用开销\n- **慢速路径**：处理竞争情况，包含自旋等待、任务阻塞、唤醒等复杂逻辑\n\n### 自适应自旋（Adaptive Spinning）\n在 `CONFIG_MUTEX_SPIN_ON_OWNER` 配置下，当检测到锁持有者正在运行时，当前任务会先自旋等待而非立即睡眠，减少上下文切换开销。使用 OSQ（Optimistic Spin Queue）机制协调多个自旋任务。\n\n### 锁移交机制（Handoff）\n通过 `MUTEX_FLAG_HANDOFF` 和 `MUTEX_FLAG_PICKUP` 标志实现高效的锁移交：\n1. 解锁者设置 `HANDOFF` 标志并唤醒队首等待者\n2. 被唤醒任务在获取锁时检测到 `HANDOFF`，设置 `PICKUP` 标志\n3. 解锁者通过 `__mutex_handoff()` 直接将所有权转移给指定任务\n避免了唤醒后再次竞争的问题，提高实时性。\n\n### 调试支持\n- `CONFIG_DEBUG_MUTEXES`：提供锁状态验证、死锁检测\n- `CONFIG_DETECT_HUNG_TASK_BLOCKER`：集成 hung task 检测，记录阻塞源\n- `lockdep`：通过 `debug_mutex_*` 函数集成锁依赖验证\n\n## 4. 依赖关系\n\n### 头文件依赖\n- `<linux/mutex.h>` / `<linux/ww_mutex.h>`：互斥锁接口定义\n- `<linux/sched/*.h>`：调度器相关功能（睡眠、唤醒、实时任务）\n- `<linux/spinlock.h>`：底层自旋锁实现\n- `<linux/osq_lock.h>`：乐观自旋队列支持\n- `<linux/hung_task.h>`：hung task 检测集成\n- `<trace/events/lock.h>`：锁事件跟踪点\n\n### 子系统交互\n- **调度器**：通过 `schedule()` 实现任务阻塞，`wake_q` 机制批量唤醒\n- **内存管理**：依赖 `task_struct` 的内存对齐特性\n- **实时补丁（PREEMPT_RT）**：非 RT 配置下编译此文件（`#ifndef CONFIG_PREEMPT_RT`）\n- **调试子系统**：与 lockdep、hung task detector 深度集成\n\n## 5. 使用场景\n\n### 典型应用场景\n- **长临界区保护**：当临界区执行时间较长或包含可能睡眠的操作（如内存分配、I/O）\n- **驱动程序同步**：设备驱动中保护硬件寄存器访问或共享数据结构\n- **文件系统操作**：保护 inode、dentry 等元数据结构\n- **内核子系统互斥**：如网络协议栈、块设备层等需要互斥访问的场景\n\n### 使用约束\n- **不可递归**：同一任务重复获取会导致死锁\n- **必须配对使用**：获取锁的任务必须负责释放\n- **禁止中断上下文使用**：因可能睡眠，只能在进程上下文使用\n- **内存生命周期**：锁对象内存不能在持有锁时释放\n\n### 性能考量\n- 无竞争场景：纳秒级延迟（快速路径原子操作）\n- 有竞争场景：微秒级延迟（自旋优化）或毫秒级（任务切换）\n- 适用于中低频竞争场景，高频竞争建议使用读写锁或 RCU",
      "similarity": 0.5558551549911499,
      "chunks": [
        {
          "chunk_id": 6,
          "file_path": "kernel/locking/mutex.c",
          "start_line": 1059,
          "end_line": 1129,
          "content": [
            "static noinline int __sched",
            "__mutex_lock_interruptible_slowpath(struct mutex *lock)",
            "{",
            "\treturn __mutex_lock(lock, TASK_INTERRUPTIBLE, 0, NULL, _RET_IP_);",
            "}",
            "static noinline int __sched",
            "__ww_mutex_lock_slowpath(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)",
            "{",
            "\treturn __ww_mutex_lock(&lock->base, TASK_UNINTERRUPTIBLE, 0,",
            "\t\t\t       _RET_IP_, ctx);",
            "}",
            "static noinline int __sched",
            "__ww_mutex_lock_interruptible_slowpath(struct ww_mutex *lock,",
            "\t\t\t\t\t    struct ww_acquire_ctx *ctx)",
            "{",
            "\treturn __ww_mutex_lock(&lock->base, TASK_INTERRUPTIBLE, 0,",
            "\t\t\t       _RET_IP_, ctx);",
            "}",
            "int __sched mutex_trylock(struct mutex *lock)",
            "{",
            "\tbool locked;",
            "",
            "\tMUTEX_WARN_ON(lock->magic != lock);",
            "",
            "\tlocked = __mutex_trylock(lock);",
            "\tif (locked)",
            "\t\tmutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);",
            "",
            "\treturn locked;",
            "}",
            "int __sched",
            "ww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)",
            "{",
            "\tmight_sleep();",
            "",
            "\tif (__mutex_trylock_fast(&lock->base)) {",
            "\t\tif (ctx)",
            "\t\t\tww_mutex_set_context_fastpath(lock, ctx);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\treturn __ww_mutex_lock_slowpath(lock, ctx);",
            "}",
            "int __sched",
            "ww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)",
            "{",
            "\tmight_sleep();",
            "",
            "\tif (__mutex_trylock_fast(&lock->base)) {",
            "\t\tif (ctx)",
            "\t\t\tww_mutex_set_context_fastpath(lock, ctx);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\treturn __ww_mutex_lock_interruptible_slowpath(lock, ctx);",
            "}",
            "int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock)",
            "{",
            "\t/* dec if we can't possibly hit 0 */",
            "\tif (atomic_add_unless(cnt, -1, 1))",
            "\t\treturn 0;",
            "\t/* we might hit 0, so take the lock */",
            "\tmutex_lock(lock);",
            "\tif (!atomic_dec_and_test(cnt)) {",
            "\t\t/* when we actually did the dec, we didn't hit 0 */",
            "\t\tmutex_unlock(lock);",
            "\t\treturn 0;",
            "\t}",
            "\t/* we hit 0, and we hold the lock */",
            "\treturn 1;",
            "}"
          ],
          "function_name": "__mutex_lock_interruptible_slowpath, __ww_mutex_lock_slowpath, __ww_mutex_lock_interruptible_slowpath, mutex_trylock, ww_mutex_lock, ww_mutex_lock_interruptible, atomic_dec_and_mutex_lock",
          "description": "提供互斥锁快速路径与慢速路径切换支持，包含原子计数器递减与锁获取协同机制",
          "similarity": 0.6280717253684998
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/mutex.c",
          "start_line": 46,
          "end_line": 151,
          "content": [
            "void",
            "__mutex_init(struct mutex *lock, const char *name, struct lock_class_key *key)",
            "{",
            "\tatomic_long_set(&lock->owner, 0);",
            "\traw_spin_lock_init(&lock->wait_lock);",
            "\tINIT_LIST_HEAD(&lock->wait_list);",
            "#ifdef CONFIG_MUTEX_SPIN_ON_OWNER",
            "\tosq_lock_init(&lock->osq);",
            "#endif",
            "",
            "\tdebug_mutex_init(lock, name, key);",
            "}",
            "bool mutex_is_locked(struct mutex *lock)",
            "{",
            "\treturn __mutex_owner(lock) != NULL;",
            "}",
            "static inline unsigned long __owner_flags(unsigned long owner)",
            "{",
            "\treturn owner & MUTEX_FLAGS;",
            "}",
            "unsigned long mutex_get_owner(struct mutex *lock)",
            "{",
            "\tunsigned long owner = atomic_long_read(&lock->owner);",
            "",
            "\treturn (unsigned long)__owner_task(owner);",
            "}",
            "static inline bool __mutex_trylock_or_handoff(struct mutex *lock, bool handoff)",
            "{",
            "\treturn !__mutex_trylock_common(lock, handoff);",
            "}",
            "static inline bool __mutex_trylock(struct mutex *lock)",
            "{",
            "\treturn !__mutex_trylock_common(lock, false);",
            "}",
            "static __always_inline bool __mutex_trylock_fast(struct mutex *lock)",
            "{",
            "\tunsigned long curr = (unsigned long)current;",
            "\tunsigned long zero = 0UL;",
            "",
            "\tif (atomic_long_try_cmpxchg_acquire(&lock->owner, &zero, curr))",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "static __always_inline bool __mutex_unlock_fast(struct mutex *lock)",
            "{",
            "\tunsigned long curr = (unsigned long)current;",
            "",
            "\treturn atomic_long_try_cmpxchg_release(&lock->owner, &curr, 0UL);",
            "}",
            "static inline void __mutex_set_flag(struct mutex *lock, unsigned long flag)",
            "{",
            "\tatomic_long_or(flag, &lock->owner);",
            "}",
            "static inline void __mutex_clear_flag(struct mutex *lock, unsigned long flag)",
            "{",
            "\tatomic_long_andnot(flag, &lock->owner);",
            "}",
            "static inline bool __mutex_waiter_is_first(struct mutex *lock, struct mutex_waiter *waiter)",
            "{",
            "\treturn list_first_entry(&lock->wait_list, struct mutex_waiter, list) == waiter;",
            "}",
            "static void",
            "__mutex_add_waiter(struct mutex *lock, struct mutex_waiter *waiter,",
            "\t\t   struct list_head *list)",
            "{",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER",
            "\thung_task_set_blocker(lock, BLOCKER_TYPE_MUTEX);",
            "#endif",
            "\tdebug_mutex_add_waiter(lock, waiter, current);",
            "",
            "\tlist_add_tail(&waiter->list, list);",
            "\tif (__mutex_waiter_is_first(lock, waiter))",
            "\t\t__mutex_set_flag(lock, MUTEX_FLAG_WAITERS);",
            "}",
            "static void",
            "__mutex_remove_waiter(struct mutex *lock, struct mutex_waiter *waiter)",
            "{",
            "\tlist_del(&waiter->list);",
            "\tif (likely(list_empty(&lock->wait_list)))",
            "\t\t__mutex_clear_flag(lock, MUTEX_FLAGS);",
            "",
            "\tdebug_mutex_remove_waiter(lock, waiter, current);",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER",
            "\thung_task_clear_blocker();",
            "#endif",
            "}",
            "static void __mutex_handoff(struct mutex *lock, struct task_struct *task)",
            "{",
            "\tunsigned long owner = atomic_long_read(&lock->owner);",
            "",
            "\tfor (;;) {",
            "\t\tunsigned long new;",
            "",
            "\t\tMUTEX_WARN_ON(__owner_task(owner) != current);",
            "\t\tMUTEX_WARN_ON(owner & MUTEX_FLAG_PICKUP);",
            "",
            "\t\tnew = (owner & MUTEX_FLAG_WAITERS);",
            "\t\tnew |= (unsigned long)task;",
            "\t\tif (task)",
            "\t\t\tnew |= MUTEX_FLAG_PICKUP;",
            "",
            "\t\tif (atomic_long_try_cmpxchg_release(&lock->owner, &owner, new))",
            "\t\t\tbreak;",
            "\t}",
            "}"
          ],
          "function_name": "__mutex_init, mutex_is_locked, __owner_flags, mutex_get_owner, __mutex_trylock_or_handoff, __mutex_trylock, __mutex_trylock_fast, __mutex_unlock_fast, __mutex_set_flag, __mutex_clear_flag, __mutex_waiter_is_first, __mutex_add_waiter, __mutex_remove_waiter, __mutex_handoff",
          "description": "实现互斥锁核心操作，包括初始化、状态检查、快速尝试加锁、标志位操作及等待者链表管理。",
          "similarity": 0.6154904365539551
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/locking/mutex.c",
          "start_line": 895,
          "end_line": 996,
          "content": [
            "int __sched",
            "ww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)",
            "{",
            "\tint ret;",
            "",
            "\tmight_sleep();",
            "\tret = __ww_mutex_lock(&lock->base, TASK_INTERRUPTIBLE,",
            "\t\t\t      0, _RET_IP_, ctx);",
            "",
            "\tif (!ret && ctx && ctx->acquired > 1)",
            "\t\treturn ww_mutex_deadlock_injection(lock, ctx);",
            "",
            "\treturn ret;",
            "}",
            "static noinline void __sched __mutex_unlock_slowpath(struct mutex *lock, unsigned long ip)",
            "{",
            "\tstruct task_struct *next = NULL;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "\tunsigned long owner;",
            "",
            "\tmutex_release(&lock->dep_map, ip);",
            "",
            "\t/*",
            "\t * Release the lock before (potentially) taking the spinlock such that",
            "\t * other contenders can get on with things ASAP.",
            "\t *",
            "\t * Except when HANDOFF, in that case we must not clear the owner field,",
            "\t * but instead set it to the top waiter.",
            "\t */",
            "\towner = atomic_long_read(&lock->owner);",
            "\tfor (;;) {",
            "\t\tMUTEX_WARN_ON(__owner_task(owner) != current);",
            "\t\tMUTEX_WARN_ON(owner & MUTEX_FLAG_PICKUP);",
            "",
            "\t\tif (owner & MUTEX_FLAG_HANDOFF)",
            "\t\t\tbreak;",
            "",
            "\t\tif (atomic_long_try_cmpxchg_release(&lock->owner, &owner, __owner_flags(owner))) {",
            "\t\t\tif (owner & MUTEX_FLAG_WAITERS)",
            "\t\t\t\tbreak;",
            "",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "",
            "\traw_spin_lock(&lock->wait_lock);",
            "\tdebug_mutex_unlock(lock);",
            "\tif (!list_empty(&lock->wait_list)) {",
            "\t\t/* get the first entry from the wait-list: */",
            "\t\tstruct mutex_waiter *waiter =",
            "\t\t\tlist_first_entry(&lock->wait_list,",
            "\t\t\t\t\t struct mutex_waiter, list);",
            "",
            "\t\tnext = waiter->task;",
            "",
            "\t\tdebug_mutex_wake_waiter(lock, waiter);",
            "\t\twake_q_add(&wake_q, next);",
            "\t}",
            "",
            "\tif (owner & MUTEX_FLAG_HANDOFF)",
            "\t\t__mutex_handoff(lock, next);",
            "",
            "\traw_spin_unlock(&lock->wait_lock);",
            "",
            "\twake_up_q(&wake_q);",
            "}",
            "int __sched mutex_lock_interruptible(struct mutex *lock)",
            "{",
            "\tmight_sleep();",
            "",
            "\tif (__mutex_trylock_fast(lock))",
            "\t\treturn 0;",
            "",
            "\treturn __mutex_lock_interruptible_slowpath(lock);",
            "}",
            "int __sched mutex_lock_killable(struct mutex *lock)",
            "{",
            "\tmight_sleep();",
            "",
            "\tif (__mutex_trylock_fast(lock))",
            "\t\treturn 0;",
            "",
            "\treturn __mutex_lock_killable_slowpath(lock);",
            "}",
            "void __sched mutex_lock_io(struct mutex *lock)",
            "{",
            "\tint token;",
            "",
            "\ttoken = io_schedule_prepare();",
            "\tmutex_lock(lock);",
            "\tio_schedule_finish(token);",
            "}",
            "static noinline void __sched",
            "__mutex_lock_slowpath(struct mutex *lock)",
            "{",
            "\t__mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);",
            "}",
            "static noinline int __sched",
            "__mutex_lock_killable_slowpath(struct mutex *lock)",
            "{",
            "\treturn __mutex_lock(lock, TASK_KILLABLE, 0, NULL, _RET_IP_);",
            "}"
          ],
          "function_name": "ww_mutex_lock_interruptible, __mutex_unlock_slowpath, mutex_lock_interruptible, mutex_lock_killable, mutex_lock_io, __mutex_lock_slowpath, __mutex_lock_killable_slowpath",
          "description": "实现带死锁检测的递归互斥锁中断获取逻辑，处理锁状态转换、唤醒等待线程及异常注入场景",
          "similarity": 0.5680389404296875
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/mutex.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * kernel/locking/mutex.c",
            " *",
            " * Mutexes: blocking mutual exclusion locks",
            " *",
            " * Started by Ingo Molnar:",
            " *",
            " *  Copyright (C) 2004, 2005, 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>",
            " *",
            " * Many thanks to Arjan van de Ven, Thomas Gleixner, Steven Rostedt and",
            " * David Howells for suggestions and improvements.",
            " *",
            " *  - Adaptive spinning for mutexes by Peter Zijlstra. (Ported to mainline",
            " *    from the -rt tree, where it was originally implemented for rtmutexes",
            " *    by Steven Rostedt, based on work by Gregory Haskins, Peter Morreale",
            " *    and Sven Dietrich.",
            " *",
            " * Also see Documentation/locking/mutex-design.rst.",
            " */",
            "#include <linux/mutex.h>",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/osq_lock.h>",
            "#include <linux/hung_task.h>",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/lock.h>",
            "",
            "#ifndef CONFIG_PREEMPT_RT",
            "#include \"mutex.h\"",
            "",
            "#ifdef CONFIG_DEBUG_MUTEXES",
            "# define MUTEX_WARN_ON(cond) DEBUG_LOCKS_WARN_ON(cond)",
            "#else",
            "# define MUTEX_WARN_ON(cond)",
            "#endif",
            ""
          ],
          "function_name": null,
          "description": "声明互斥锁模块的头文件和基本配置，初始化互斥锁结构体并设置等待队列及调试信息。",
          "similarity": 0.5520515441894531
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/locking/mutex.c",
          "start_line": 455,
          "end_line": 722,
          "content": [
            "static __always_inline bool",
            "mutex_optimistic_spin(struct mutex *lock, struct ww_acquire_ctx *ww_ctx,",
            "\t\t      struct mutex_waiter *waiter)",
            "{",
            "\tif (!waiter) {",
            "\t\t/*",
            "\t\t * The purpose of the mutex_can_spin_on_owner() function is",
            "\t\t * to eliminate the overhead of osq_lock() and osq_unlock()",
            "\t\t * in case spinning isn't possible. As a waiter-spinner",
            "\t\t * is not going to take OSQ lock anyway, there is no need",
            "\t\t * to call mutex_can_spin_on_owner().",
            "\t\t */",
            "\t\tif (!mutex_can_spin_on_owner(lock))",
            "\t\t\tgoto fail;",
            "",
            "\t\t/*",
            "\t\t * In order to avoid a stampede of mutex spinners trying to",
            "\t\t * acquire the mutex all at once, the spinners need to take a",
            "\t\t * MCS (queued) lock first before spinning on the owner field.",
            "\t\t */",
            "\t\tif (!osq_lock(&lock->osq))",
            "\t\t\tgoto fail;",
            "\t}",
            "",
            "\tfor (;;) {",
            "\t\tstruct task_struct *owner;",
            "",
            "\t\t/* Try to acquire the mutex... */",
            "\t\towner = __mutex_trylock_or_owner(lock);",
            "\t\tif (!owner)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * There's an owner, wait for it to either",
            "\t\t * release the lock or go to sleep.",
            "\t\t */",
            "\t\tif (!mutex_spin_on_owner(lock, owner, ww_ctx, waiter))",
            "\t\t\tgoto fail_unlock;",
            "",
            "\t\t/*",
            "\t\t * The cpu_relax() call is a compiler barrier which forces",
            "\t\t * everything in this loop to be re-loaded. We don't need",
            "\t\t * memory barriers as we'll eventually observe the right",
            "\t\t * values at the cost of a few extra spins.",
            "\t\t */",
            "\t\tcpu_relax();",
            "\t}",
            "",
            "\tif (!waiter)",
            "\t\tosq_unlock(&lock->osq);",
            "",
            "\treturn true;",
            "",
            "",
            "fail_unlock:",
            "\tif (!waiter)",
            "\t\tosq_unlock(&lock->osq);",
            "",
            "fail:",
            "\t/*",
            "\t * If we fell out of the spin path because of need_resched(),",
            "\t * reschedule now, before we try-lock the mutex. This avoids getting",
            "\t * scheduled out right after we obtained the mutex.",
            "\t */",
            "\tif (need_resched()) {",
            "\t\t/*",
            "\t\t * We _should_ have TASK_RUNNING here, but just in case",
            "\t\t * we do not, make it so, otherwise we might get stuck.",
            "\t\t */",
            "\t\t__set_current_state(TASK_RUNNING);",
            "\t\tschedule_preempt_disabled();",
            "\t}",
            "",
            "\treturn false;",
            "}",
            "static __always_inline bool",
            "mutex_optimistic_spin(struct mutex *lock, struct ww_acquire_ctx *ww_ctx,",
            "\t\t      struct mutex_waiter *waiter)",
            "{",
            "\treturn false;",
            "}",
            "void __sched mutex_unlock(struct mutex *lock)",
            "{",
            "#ifndef CONFIG_DEBUG_LOCK_ALLOC",
            "\tif (__mutex_unlock_fast(lock))",
            "\t\treturn;",
            "#endif",
            "\t__mutex_unlock_slowpath(lock, _RET_IP_);",
            "}",
            "void __sched ww_mutex_unlock(struct ww_mutex *lock)",
            "{",
            "\t__ww_mutex_unlock(lock);",
            "\tmutex_unlock(&lock->base);",
            "}",
            "static __always_inline int __sched",
            "__mutex_lock_common(struct mutex *lock, unsigned int state, unsigned int subclass,",
            "\t\t    struct lockdep_map *nest_lock, unsigned long ip,",
            "\t\t    struct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx)",
            "{",
            "\tstruct mutex_waiter waiter;",
            "\tstruct ww_mutex *ww;",
            "\tint ret;",
            "",
            "\tif (!use_ww_ctx)",
            "\t\tww_ctx = NULL;",
            "",
            "\tmight_sleep();",
            "",
            "\tMUTEX_WARN_ON(lock->magic != lock);",
            "",
            "\tww = container_of(lock, struct ww_mutex, base);",
            "\tif (ww_ctx) {",
            "\t\tif (unlikely(ww_ctx == READ_ONCE(ww->ctx)))",
            "\t\t\treturn -EALREADY;",
            "",
            "\t\t/*",
            "\t\t * Reset the wounded flag after a kill. No other process can",
            "\t\t * race and wound us here since they can't have a valid owner",
            "\t\t * pointer if we don't have any locks held.",
            "\t\t */",
            "\t\tif (ww_ctx->acquired == 0)",
            "\t\t\tww_ctx->wounded = 0;",
            "",
            "#ifdef CONFIG_DEBUG_LOCK_ALLOC",
            "\t\tnest_lock = &ww_ctx->dep_map;",
            "#endif",
            "\t}",
            "",
            "\tpreempt_disable();",
            "\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);",
            "",
            "\ttrace_contention_begin(lock, LCB_F_MUTEX | LCB_F_SPIN);",
            "\tif (__mutex_trylock(lock) ||",
            "\t    mutex_optimistic_spin(lock, ww_ctx, NULL)) {",
            "\t\t/* got the lock, yay! */",
            "\t\tlock_acquired(&lock->dep_map, ip);",
            "\t\tif (ww_ctx)",
            "\t\t\tww_mutex_set_context_fastpath(ww, ww_ctx);",
            "\t\ttrace_contention_end(lock, 0);",
            "\t\tpreempt_enable();",
            "\t\treturn 0;",
            "\t}",
            "",
            "\traw_spin_lock(&lock->wait_lock);",
            "\t/*",
            "\t * After waiting to acquire the wait_lock, try again.",
            "\t */",
            "\tif (__mutex_trylock(lock)) {",
            "\t\tif (ww_ctx)",
            "\t\t\t__ww_mutex_check_waiters(lock, ww_ctx);",
            "",
            "\t\tgoto skip_wait;",
            "\t}",
            "",
            "\tdebug_mutex_lock_common(lock, &waiter);",
            "\twaiter.task = current;",
            "\tif (use_ww_ctx)",
            "\t\twaiter.ww_ctx = ww_ctx;",
            "",
            "\tlock_contended(&lock->dep_map, ip);",
            "",
            "\tif (!use_ww_ctx) {",
            "\t\t/* add waiting tasks to the end of the waitqueue (FIFO): */",
            "\t\t__mutex_add_waiter(lock, &waiter, &lock->wait_list);",
            "\t} else {",
            "\t\t/*",
            "\t\t * Add in stamp order, waking up waiters that must kill",
            "\t\t * themselves.",
            "\t\t */",
            "\t\tret = __ww_mutex_add_waiter(&waiter, lock, ww_ctx);",
            "\t\tif (ret)",
            "\t\t\tgoto err_early_kill;",
            "\t}",
            "",
            "\tset_current_state(state);",
            "\ttrace_contention_begin(lock, LCB_F_MUTEX);",
            "\tfor (;;) {",
            "\t\tbool first;",
            "",
            "\t\t/*",
            "\t\t * Once we hold wait_lock, we're serialized against",
            "\t\t * mutex_unlock() handing the lock off to us, do a trylock",
            "\t\t * before testing the error conditions to make sure we pick up",
            "\t\t * the handoff.",
            "\t\t */",
            "\t\tif (__mutex_trylock(lock))",
            "\t\t\tgoto acquired;",
            "",
            "\t\t/*",
            "\t\t * Check for signals and kill conditions while holding",
            "\t\t * wait_lock. This ensures the lock cancellation is ordered",
            "\t\t * against mutex_unlock() and wake-ups do not go missing.",
            "\t\t */",
            "\t\tif (signal_pending_state(state, current)) {",
            "\t\t\tret = -EINTR;",
            "\t\t\tgoto err;",
            "\t\t}",
            "",
            "\t\tif (ww_ctx) {",
            "\t\t\tret = __ww_mutex_check_kill(lock, &waiter, ww_ctx);",
            "\t\t\tif (ret)",
            "\t\t\t\tgoto err;",
            "\t\t}",
            "",
            "\t\traw_spin_unlock(&lock->wait_lock);",
            "\t\tschedule_preempt_disabled();",
            "",
            "\t\tfirst = __mutex_waiter_is_first(lock, &waiter);",
            "",
            "\t\tset_current_state(state);",
            "\t\t/*",
            "\t\t * Here we order against unlock; we must either see it change",
            "\t\t * state back to RUNNING and fall through the next schedule(),",
            "\t\t * or we must see its unlock and acquire.",
            "\t\t */",
            "\t\tif (__mutex_trylock_or_handoff(lock, first))",
            "\t\t\tbreak;",
            "",
            "\t\tif (first) {",
            "\t\t\ttrace_contention_begin(lock, LCB_F_MUTEX | LCB_F_SPIN);",
            "\t\t\tif (mutex_optimistic_spin(lock, ww_ctx, &waiter))",
            "\t\t\t\tbreak;",
            "\t\t\ttrace_contention_begin(lock, LCB_F_MUTEX);",
            "\t\t}",
            "",
            "\t\traw_spin_lock(&lock->wait_lock);",
            "\t}",
            "\traw_spin_lock(&lock->wait_lock);",
            "acquired:",
            "\t__set_current_state(TASK_RUNNING);",
            "",
            "\tif (ww_ctx) {",
            "\t\t/*",
            "\t\t * Wound-Wait; we stole the lock (!first_waiter), check the",
            "\t\t * waiters as anyone might want to wound us.",
            "\t\t */",
            "\t\tif (!ww_ctx->is_wait_die &&",
            "\t\t    !__mutex_waiter_is_first(lock, &waiter))",
            "\t\t\t__ww_mutex_check_waiters(lock, ww_ctx);",
            "\t}",
            "",
            "\t__mutex_remove_waiter(lock, &waiter);",
            "",
            "\tdebug_mutex_free_waiter(&waiter);",
            "",
            "skip_wait:",
            "\t/* got the lock - cleanup and rejoice! */",
            "\tlock_acquired(&lock->dep_map, ip);",
            "\ttrace_contention_end(lock, 0);",
            "",
            "\tif (ww_ctx)",
            "\t\tww_mutex_lock_acquired(ww, ww_ctx);",
            "",
            "\traw_spin_unlock(&lock->wait_lock);",
            "\tpreempt_enable();",
            "\treturn 0;",
            "",
            "err:",
            "\t__set_current_state(TASK_RUNNING);",
            "\t__mutex_remove_waiter(lock, &waiter);",
            "err_early_kill:",
            "\ttrace_contention_end(lock, ret);",
            "\traw_spin_unlock(&lock->wait_lock);",
            "\tdebug_mutex_free_waiter(&waiter);",
            "\tmutex_release(&lock->dep_map, ip);",
            "\tpreempt_enable();",
            "\treturn ret;",
            "}"
          ],
          "function_name": "mutex_optimistic_spin, mutex_optimistic_spin, mutex_unlock, ww_mutex_unlock, __mutex_lock_common",
          "description": "实现乐观自旋逻辑和锁解除操作，通过原子操作和同步机制管理锁竞争，支持带资源检查的锁操作。",
          "similarity": 0.4957635700702667
        }
      ]
    },
    {
      "source_file": "kernel/async.c",
      "md_summary": "> 自动生成时间: 2025-10-25 11:49:14\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `async.c`\n\n---\n\n# async.c 技术文档\n\n## 1. 文件概述\n\n`async.c` 实现了 Linux 内核中的异步函数调用机制，主要用于优化系统启动性能。该机制允许在内核初始化阶段将原本串行执行的、相互独立的硬件探测和初始化操作并行化，从而显著缩短启动时间。其核心思想是在保持对外可见操作顺序一致性的前提下，内部执行过程可乱序进行，类似于乱序执行 CPU 的“按序提交”语义。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct async_entry`**：表示一个异步任务条目，包含：\n  - `domain_list` / `global_list`：分别链接到所属域和全局的待处理链表\n  - `work`：关联的 workqueue 工作项\n  - `cookie`：序列号，用于同步控制\n  - `func` / `data`：要执行的函数及其参数\n  - `domain`：所属的异步域\n\n- **`struct async_domain`**：异步执行域，用于将异步任务分组管理，默认使用 `async_dfl_domain`\n\n- **全局变量**：\n  - `next_cookie`：单调递增的序列号生成器\n  - `async_global_pending`：所有已注册域的全局待处理任务链表\n  - `async_dfl_domain`：默认异步域\n  - `async_lock`：保护异步任务队列的自旋锁\n  - `entry_count`：当前挂起的异步任务计数\n\n### 主要函数\n\n- **`async_schedule_node_domain()`**：在指定 NUMA 节点和异步域中调度异步函数\n- **`async_schedule_node()`**：在指定 NUMA 节点上调度异步函数（使用默认域）\n- **`async_schedule_dev_nocall()`**：基于设备的 NUMA 信息调度异步函数（失败时不回退到同步执行）\n- **`lowest_in_progress()`**：获取指定域或全局中最早（最小 cookie）的未完成任务\n- **`async_run_entry_fn()`**：workqueue 回调函数，实际执行异步任务并清理资源\n\n## 3. 关键实现\n\n### 序列 Cookie 机制\n- 每个异步任务分配一个单调递增的 `async_cookie_t`（64 位无符号整数）\n- 任务执行前可通过 `async_synchronize_cookie()` 等待所有小于等于指定 cookie 的任务完成\n- 保证对外部可见操作（如设备注册）的顺序一致性\n\n### 内存与负载控制\n- 使用 `GFP_ATOMIC` 分配内存，支持原子上下文调用\n- 当内存不足或挂起任务超过 `MAX_WORK`（32768）时，自动回退到同步执行\n- 通过 `entry_count` 原子计数器跟踪挂起任务数量\n\n### 双链表管理\n- 每个任务同时链接到：\n  - 所属域的 `domain->pending` 链表（按 cookie 顺序）\n  - 全局 `async_global_pending` 链表（仅当域已注册）\n- 保证域内和全局的同步操作都能正确等待\n\n### NUMA 感知调度\n- 通过 `queue_work_node()` 将任务调度到指定 NUMA 节点\n- 若节点无效则自动分发到可用 CPU\n\n### 资源清理与通知\n- 任务执行完成后从链表移除并释放内存\n- 通过 `wake_up(&async_done)` 唤醒等待同步完成的线程\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/async.h>`：异步 API 定义\n  - `<linux/workqueue.h>`：工作队列机制\n  - `\"workqueue_internal.h\"`：内部 workqueue 接口\n  - 其他基础内核头文件（atomic、slab、wait 等）\n\n- **核心子系统**：\n  - **Workqueue 子系统**：实际执行异步任务的底层机制\n  - **内存管理子系统**：任务结构体内存分配\n  - **调度器**：NUMA 节点感知的任务调度\n\n- **导出符号**：\n  - `async_schedule_node_domain`\n  - `async_schedule_node`\n\n## 5. 使用场景\n\n- **内核启动优化**：\n  - 并行执行设备探测（如 PCI、USB 控制器初始化）\n  - 异步加载固件或执行硬件自检\n\n- **驱动初始化**：\n  - 驱动可将耗时的初始化操作（如 PHY 配置、固件加载）放入异步任务\n  - 通过 `async_synchronize_full()` 确保在模块初始化完成前所有异步任务结束\n\n- **NUMA 优化**：\n  - 将设备相关的初始化任务调度到设备所在 NUMA 节点，减少远程内存访问\n\n- **资源受限环境**：\n  - 在内存压力下自动回退到同步执行，保证系统稳定性\n  - 通过 `MAX_WORK` 限制防止异步任务无限堆积",
      "similarity": 0.546024739742279,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/async.c",
          "start_line": 241,
          "end_line": 290,
          "content": [
            "async_cookie_t async_schedule_node(async_func_t func, void *data, int node)",
            "{",
            "\treturn async_schedule_node_domain(func, data, node, &async_dfl_domain);",
            "}",
            "bool async_schedule_dev_nocall(async_func_t func, struct device *dev)",
            "{",
            "\tstruct async_entry *entry;",
            "",
            "\tentry = kzalloc(sizeof(struct async_entry), GFP_KERNEL);",
            "",
            "\t/* Give up if there is no memory or too much work. */",
            "\tif (!entry || atomic_read(&entry_count) > MAX_WORK) {",
            "\t\tkfree(entry);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t__async_schedule_node_domain(func, dev, dev_to_node(dev),",
            "\t\t\t\t     &async_dfl_domain, entry);",
            "\treturn true;",
            "}",
            "void async_synchronize_full(void)",
            "{",
            "\tasync_synchronize_full_domain(NULL);",
            "}",
            "void async_synchronize_full_domain(struct async_domain *domain)",
            "{",
            "\tasync_synchronize_cookie_domain(ASYNC_COOKIE_MAX, domain);",
            "}",
            "void async_synchronize_cookie_domain(async_cookie_t cookie, struct async_domain *domain)",
            "{",
            "\tktime_t starttime;",
            "",
            "\tpr_debug(\"async_waiting @ %i\\n\", task_pid_nr(current));",
            "\tstarttime = ktime_get();",
            "",
            "\twait_event(async_done, lowest_in_progress(domain) >= cookie);",
            "",
            "\tpr_debug(\"async_continuing @ %i after %lli usec\\n\", task_pid_nr(current),",
            "\t\t microseconds_since(starttime));",
            "}",
            "void async_synchronize_cookie(async_cookie_t cookie)",
            "{",
            "\tasync_synchronize_cookie_domain(cookie, &async_dfl_domain);",
            "}",
            "bool current_is_async(void)",
            "{",
            "\tstruct worker *worker = current_wq_worker();",
            "",
            "\treturn worker && worker->current_func == async_run_entry_fn;",
            "}"
          ],
          "function_name": "async_schedule_node, async_schedule_dev_nocall, async_synchronize_full, async_synchronize_full_domain, async_synchronize_cookie_domain, async_synchronize_cookie, current_is_async",
          "description": "提供同步屏障接口和运行态检测，确保异步操作有序完成",
          "similarity": 0.5360616445541382
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/async.c",
          "start_line": 82,
          "end_line": 201,
          "content": [
            "static long long microseconds_since(ktime_t start)",
            "{",
            "\tktime_t now = ktime_get();",
            "\treturn ktime_to_ns(ktime_sub(now, start)) >> 10;",
            "}",
            "static async_cookie_t lowest_in_progress(struct async_domain *domain)",
            "{",
            "\tstruct async_entry *first = NULL;",
            "\tasync_cookie_t ret = ASYNC_COOKIE_MAX;",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&async_lock, flags);",
            "",
            "\tif (domain) {",
            "\t\tif (!list_empty(&domain->pending))",
            "\t\t\tfirst = list_first_entry(&domain->pending,",
            "\t\t\t\t\tstruct async_entry, domain_list);",
            "\t} else {",
            "\t\tif (!list_empty(&async_global_pending))",
            "\t\t\tfirst = list_first_entry(&async_global_pending,",
            "\t\t\t\t\tstruct async_entry, global_list);",
            "\t}",
            "",
            "\tif (first)",
            "\t\tret = first->cookie;",
            "",
            "\tspin_unlock_irqrestore(&async_lock, flags);",
            "\treturn ret;",
            "}",
            "static void async_run_entry_fn(struct work_struct *work)",
            "{",
            "\tstruct async_entry *entry =",
            "\t\tcontainer_of(work, struct async_entry, work);",
            "\tunsigned long flags;",
            "\tktime_t calltime;",
            "",
            "\t/* 1) run (and print duration) */",
            "\tpr_debug(\"calling  %lli_%pS @ %i\\n\", (long long)entry->cookie,",
            "\t\t entry->func, task_pid_nr(current));",
            "\tcalltime = ktime_get();",
            "",
            "\tentry->func(entry->data, entry->cookie);",
            "",
            "\tpr_debug(\"initcall %lli_%pS returned after %lld usecs\\n\",",
            "\t\t (long long)entry->cookie, entry->func,",
            "\t\t microseconds_since(calltime));",
            "",
            "\t/* 2) remove self from the pending queues */",
            "\tspin_lock_irqsave(&async_lock, flags);",
            "\tlist_del_init(&entry->domain_list);",
            "\tlist_del_init(&entry->global_list);",
            "",
            "\t/* 3) free the entry */",
            "\tkfree(entry);",
            "\tatomic_dec(&entry_count);",
            "",
            "\tspin_unlock_irqrestore(&async_lock, flags);",
            "",
            "\t/* 4) wake up any waiters */",
            "\twake_up(&async_done);",
            "}",
            "static async_cookie_t __async_schedule_node_domain(async_func_t func,",
            "\t\t\t\t\t\t   void *data, int node,",
            "\t\t\t\t\t\t   struct async_domain *domain,",
            "\t\t\t\t\t\t   struct async_entry *entry)",
            "{",
            "\tasync_cookie_t newcookie;",
            "\tunsigned long flags;",
            "",
            "\tINIT_LIST_HEAD(&entry->domain_list);",
            "\tINIT_LIST_HEAD(&entry->global_list);",
            "\tINIT_WORK(&entry->work, async_run_entry_fn);",
            "\tentry->func = func;",
            "\tentry->data = data;",
            "\tentry->domain = domain;",
            "",
            "\tspin_lock_irqsave(&async_lock, flags);",
            "",
            "\t/* allocate cookie and queue */",
            "\tnewcookie = entry->cookie = next_cookie++;",
            "",
            "\tlist_add_tail(&entry->domain_list, &domain->pending);",
            "\tif (domain->registered)",
            "\t\tlist_add_tail(&entry->global_list, &async_global_pending);",
            "",
            "\tatomic_inc(&entry_count);",
            "\tspin_unlock_irqrestore(&async_lock, flags);",
            "",
            "\t/* schedule for execution */",
            "\tqueue_work_node(node, system_unbound_wq, &entry->work);",
            "",
            "\treturn newcookie;",
            "}",
            "async_cookie_t async_schedule_node_domain(async_func_t func, void *data,",
            "\t\t\t\t\t  int node, struct async_domain *domain)",
            "{",
            "\tstruct async_entry *entry;",
            "\tunsigned long flags;",
            "\tasync_cookie_t newcookie;",
            "",
            "\t/* allow irq-off callers */",
            "\tentry = kzalloc(sizeof(struct async_entry), GFP_ATOMIC);",
            "",
            "\t/*",
            "\t * If we're out of memory or if there's too much work",
            "\t * pending already, we execute synchronously.",
            "\t */",
            "\tif (!entry || atomic_read(&entry_count) > MAX_WORK) {",
            "\t\tkfree(entry);",
            "\t\tspin_lock_irqsave(&async_lock, flags);",
            "\t\tnewcookie = next_cookie++;",
            "\t\tspin_unlock_irqrestore(&async_lock, flags);",
            "",
            "\t\t/* low on memory.. run synchronously */",
            "\t\tfunc(data, newcookie);",
            "\t\treturn newcookie;",
            "\t}",
            "",
            "\treturn __async_schedule_node_domain(func, data, node, domain, entry);",
            "}"
          ],
          "function_name": "microseconds_since, lowest_in_progress, async_run_entry_fn, __async_schedule_node_domain, async_schedule_node_domain",
          "description": "实现异步任务调度与执行逻辑，包含时间测量、任务排队及工作队列调度",
          "similarity": 0.5028534531593323
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/async.c",
          "start_line": 1,
          "end_line": 81,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * async.c: Asynchronous function calls for boot performance",
            " *",
            " * (C) Copyright 2009 Intel Corporation",
            " * Author: Arjan van de Ven <arjan@linux.intel.com>",
            " */",
            "",
            "",
            "/*",
            "",
            "Goals and Theory of Operation",
            "",
            "The primary goal of this feature is to reduce the kernel boot time,",
            "by doing various independent hardware delays and discovery operations",
            "decoupled and not strictly serialized.",
            "",
            "More specifically, the asynchronous function call concept allows",
            "certain operations (primarily during system boot) to happen",
            "asynchronously, out of order, while these operations still",
            "have their externally visible parts happen sequentially and in-order.",
            "(not unlike how out-of-order CPUs retire their instructions in order)",
            "",
            "Key to the asynchronous function call implementation is the concept of",
            "a \"sequence cookie\" (which, although it has an abstracted type, can be",
            "thought of as a monotonically incrementing number).",
            "",
            "The async core will assign each scheduled event such a sequence cookie and",
            "pass this to the called functions.",
            "",
            "The asynchronously called function should before doing a globally visible",
            "operation, such as registering device numbers, call the",
            "async_synchronize_cookie() function and pass in its own cookie. The",
            "async_synchronize_cookie() function will make sure that all asynchronous",
            "operations that were scheduled prior to the operation corresponding with the",
            "cookie have completed.",
            "",
            "Subsystem/driver initialization code that scheduled asynchronous probe",
            "functions, but which shares global resources with other drivers/subsystems",
            "that do not use the asynchronous call feature, need to do a full",
            "synchronization with the async_synchronize_full() function, before returning",
            "from their init function. This is to maintain strict ordering between the",
            "asynchronous and synchronous parts of the kernel.",
            "",
            "*/",
            "",
            "#include <linux/async.h>",
            "#include <linux/atomic.h>",
            "#include <linux/export.h>",
            "#include <linux/ktime.h>",
            "#include <linux/pid.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/wait.h>",
            "#include <linux/workqueue.h>",
            "",
            "#include \"workqueue_internal.h\"",
            "",
            "static async_cookie_t next_cookie = 1;",
            "",
            "#define MAX_WORK\t\t32768",
            "#define ASYNC_COOKIE_MAX\tULLONG_MAX\t/* infinity cookie */",
            "",
            "static LIST_HEAD(async_global_pending);\t/* pending from all registered doms */",
            "static ASYNC_DOMAIN(async_dfl_domain);",
            "static DEFINE_SPINLOCK(async_lock);",
            "",
            "struct async_entry {",
            "\tstruct list_head\tdomain_list;",
            "\tstruct list_head\tglobal_list;",
            "\tstruct work_struct\twork;",
            "\tasync_cookie_t\t\tcookie;",
            "\tasync_func_t\t\tfunc;",
            "\tvoid\t\t\t*data;",
            "\tstruct async_domain\t*domain;",
            "};",
            "",
            "static DECLARE_WAIT_QUEUE_HEAD(async_done);",
            "",
            "static atomic_t entry_count;",
            ""
          ],
          "function_name": null,
          "description": "定义异步任务结构体和核心变量，支持多域异步调度",
          "similarity": 0.4641740918159485
        }
      ]
    },
    {
      "source_file": "kernel/rcu/sync.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:44:18\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\sync.c`\n\n---\n\n# rcu/sync.c 技术文档\n\n## 文件概述\n\n`rcu/sync.c` 实现了一个基于 RCU（Read-Copy-Update）机制的轻量级读写同步基础设施，称为 `rcu_sync`。该机制允许写者（更新者）在需要时强制所有读者切换到“慢路径”（slow path），并在更新完成后经过一个 RCU 宽限期（grace period）后，允许读者重新使用“快路径”（fast path）。该设计特别适用于需要频繁但短暂地禁用读者快路径的场景，避免了传统读写锁的开销，同时利用 RCU 的无锁读取特性提升性能。\n\n## 核心功能\n\n### 数据结构\n\n- **`struct rcu_sync`**  \n  核心同步控制结构，包含以下关键字段：\n  - `gp_state`：当前同步状态（`GP_IDLE`, `GP_ENTER`, `GP_PASSED`, `GP_EXIT`, `GP_REPLAY`）\n  - `gp_count`：嵌套的 `rcu_sync_enter()` 调用计数\n  - `cb_head`：用于 RCU 回调的 `rcu_head`\n  - `gp_wait`：等待队列，用于阻塞等待状态转换完成\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `rcu_sync_init()` | 初始化 `rcu_sync` 结构体 |\n| `rcu_sync_enter_start()` | 预激活同步机制，使 `rcu_sync_is_idle()` 返回 false，且后续 enter/exit 成为 NO-OP |\n| `rcu_sync_enter()` | 强制读者进入慢路径，确保后续读者不会使用快路径 |\n| `rcu_sync_exit()` | 标记更新结束，安排在宽限期后恢复读者快路径 |\n| `rcu_sync_dtor()` | 销毁 `rcu_sync` 结构，确保所有 RCU 回调已完成 |\n| `rcu_sync_func()` | RCU 回调函数，根据当前状态推进状态机 |\n\n## 关键实现\n\n### 状态机设计\n\n`rcu_sync` 使用五种状态实现高效的状态转换：\n\n- **`GP_IDLE`**：初始状态，读者可使用快路径。\n- **`GP_ENTER`**：正在进入同步状态，需等待宽限期。\n- **`GP_PASSED`**：宽限期已过，读者已全部进入慢路径。\n- **`GP_EXIT`**：正在退出同步，需等待另一个宽限期以恢复快路径。\n- **`GP_REPLAY`**：在退出过程中又有新的 enter/exit 对发生，需重新调度回调。\n\n### 嵌套与优化\n\n- **嵌套支持**：通过 `gp_count` 支持 `rcu_sync_enter()` 的嵌套调用。只有当 `gp_count` 从 1 递减到 0 时，才触发退出流程。\n- **宽限期合并**：连续的 `enter/exit` 调用可避免多次等待宽限期。例如：\n  - 若在 `GP_PASSED` 状态下调用 `exit`，直接进入 `GP_EXIT` 并调度回调。\n  - 若在回调执行前再次调用 `enter/exit`，状态转为 `GP_REPLAY`，并在回调中重新调度，避免冗余宽限期。\n- **快速路径优化**：首次 `enter` 时若处于 `GP_IDLE`，直接调用 `synchronize_rcu()` 而非异步 `call_rcu()`，可利用 `rcu_expedited` 或 `rcu_blocking_is_gp()` 加速。\n\n### 同步与唤醒\n\n- 写者调用 `rcu_sync_enter()` 后，若非首次进入，会阻塞在 `wait_event()`，直到状态变为 `GP_PASSED` 或更高。\n- `rcu_sync_func()` 在宽限期后执行，根据 `gp_count` 和当前状态决定是唤醒等待者、重调度回调，还是恢复到 `GP_IDLE`。\n\n## 依赖关系\n\n- **`<linux/rcu_sync.h>`**：定义 `struct rcu_sync` 及相关 API。\n- **`<linux/sched.h>`**：提供 `wait_event()`、`wake_up_locked()` 等调度和等待队列原语。\n- **RCU 子系统**：\n  - `call_rcu_hurry()` / `call_rcu()`：用于注册宽限期后的回调。\n  - `synchronize_rcu()`：用于同步等待宽限期。\n  - `rcu_barrier()`：在析构时确保所有回调完成。\n- **自旋锁**：使用 `spin_lock_irqsave()` 保护状态和计数器，确保中断上下文安全。\n\n## 使用场景\n\n- **文件系统元数据更新**：如 overlayfs、btrfs 等在修改共享元数据结构时，临时禁止读者使用快路径缓存。\n- **动态配置更新**：内核模块或子系统在热更新全局配置时，确保读者看到一致状态。\n- **轻量级写者同步**：适用于写操作较少但需高效读者路径的场景，避免传统 rwlock 的读者竞争开销。\n- **替代 `synchronize_rcu()` 的批量操作**：当多个连续更新可合并为一次宽限期等待时，提升性能。",
      "similarity": 0.5425751209259033,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/rcu/sync.c",
          "start_line": 21,
          "end_line": 136,
          "content": [
            "void rcu_sync_init(struct rcu_sync *rsp)",
            "{",
            "\tmemset(rsp, 0, sizeof(*rsp));",
            "\tinit_waitqueue_head(&rsp->gp_wait);",
            "}",
            "void rcu_sync_enter_start(struct rcu_sync *rsp)",
            "{",
            "\trsp->gp_count++;",
            "\trsp->gp_state = GP_PASSED;",
            "}",
            "static void rcu_sync_call(struct rcu_sync *rsp)",
            "{",
            "\tcall_rcu_hurry(&rsp->cb_head, rcu_sync_func);",
            "}",
            "static void rcu_sync_func(struct rcu_head *rhp)",
            "{",
            "\tstruct rcu_sync *rsp = container_of(rhp, struct rcu_sync, cb_head);",
            "\tunsigned long flags;",
            "",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_state) == GP_IDLE);",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_state) == GP_PASSED);",
            "",
            "\tspin_lock_irqsave(&rsp->rss_lock, flags);",
            "\tif (rsp->gp_count) {",
            "\t\t/*",
            "\t\t * We're at least a GP after the GP_IDLE->GP_ENTER transition.",
            "\t\t */",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_PASSED);",
            "\t\twake_up_locked(&rsp->gp_wait);",
            "\t} else if (rsp->gp_state == GP_REPLAY) {",
            "\t\t/*",
            "\t\t * A new rcu_sync_exit() has happened; requeue the callback to",
            "\t\t * catch a later GP.",
            "\t\t */",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_EXIT);",
            "\t\trcu_sync_call(rsp);",
            "\t} else {",
            "\t\t/*",
            "\t\t * We're at least a GP after the last rcu_sync_exit(); everybody",
            "\t\t * will now have observed the write side critical section.",
            "\t\t * Let 'em rip!",
            "\t\t */",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_IDLE);",
            "\t}",
            "\tspin_unlock_irqrestore(&rsp->rss_lock, flags);",
            "}",
            "void rcu_sync_enter(struct rcu_sync *rsp)",
            "{",
            "\tint gp_state;",
            "",
            "\tspin_lock_irq(&rsp->rss_lock);",
            "\tgp_state = rsp->gp_state;",
            "\tif (gp_state == GP_IDLE) {",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_ENTER);",
            "\t\tWARN_ON_ONCE(rsp->gp_count);",
            "\t\t/*",
            "\t\t * Note that we could simply do rcu_sync_call(rsp) here and",
            "\t\t * avoid the \"if (gp_state == GP_IDLE)\" block below.",
            "\t\t *",
            "\t\t * However, synchronize_rcu() can be faster if rcu_expedited",
            "\t\t * or rcu_blocking_is_gp() is true.",
            "\t\t *",
            "\t\t * Another reason is that we can't wait for rcu callback if",
            "\t\t * we are called at early boot time but this shouldn't happen.",
            "\t\t */",
            "\t}",
            "\trsp->gp_count++;",
            "\tspin_unlock_irq(&rsp->rss_lock);",
            "",
            "\tif (gp_state == GP_IDLE) {",
            "\t\t/*",
            "\t\t * See the comment above, this simply does the \"synchronous\"",
            "\t\t * call_rcu(rcu_sync_func) which does GP_ENTER -> GP_PASSED.",
            "\t\t */",
            "\t\tsynchronize_rcu();",
            "\t\trcu_sync_func(&rsp->cb_head);",
            "\t\t/* Not really needed, wait_event() would see GP_PASSED. */",
            "\t\treturn;",
            "\t}",
            "",
            "\twait_event(rsp->gp_wait, READ_ONCE(rsp->gp_state) >= GP_PASSED);",
            "}",
            "void rcu_sync_exit(struct rcu_sync *rsp)",
            "{",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_state) == GP_IDLE);",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_count) == 0);",
            "",
            "\tspin_lock_irq(&rsp->rss_lock);",
            "\tif (!--rsp->gp_count) {",
            "\t\tif (rsp->gp_state == GP_PASSED) {",
            "\t\t\tWRITE_ONCE(rsp->gp_state, GP_EXIT);",
            "\t\t\trcu_sync_call(rsp);",
            "\t\t} else if (rsp->gp_state == GP_EXIT) {",
            "\t\t\tWRITE_ONCE(rsp->gp_state, GP_REPLAY);",
            "\t\t}",
            "\t}",
            "\tspin_unlock_irq(&rsp->rss_lock);",
            "}",
            "void rcu_sync_dtor(struct rcu_sync *rsp)",
            "{",
            "\tint gp_state;",
            "",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_count));",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_state) == GP_PASSED);",
            "",
            "\tspin_lock_irq(&rsp->rss_lock);",
            "\tif (rsp->gp_state == GP_REPLAY)",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_EXIT);",
            "\tgp_state = rsp->gp_state;",
            "\tspin_unlock_irq(&rsp->rss_lock);",
            "",
            "\tif (gp_state != GP_IDLE) {",
            "\t\trcu_barrier();",
            "\t\tWARN_ON_ONCE(rsp->gp_state != GP_IDLE);",
            "\t}",
            "}"
          ],
          "function_name": "rcu_sync_init, rcu_sync_enter_start, rcu_sync_call, rcu_sync_func, rcu_sync_enter, rcu_sync_exit, rcu_sync_dtor",
          "description": "实现了RCU同步核心函数，包括初始化、状态管理、回调触发和退出处理，通过spinlock保护状态机并利用RCU回调实现延迟同步，用于协调读者-写者并发访问的安全转换",
          "similarity": 0.5385680794715881
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/rcu/sync.c",
          "start_line": 1,
          "end_line": 20,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0+",
            "/*",
            " * RCU-based infrastructure for lightweight reader-writer locking",
            " *",
            " * Copyright (c) 2015, Red Hat, Inc.",
            " *",
            " * Author: Oleg Nesterov <oleg@redhat.com>",
            " */",
            "",
            "#include <linux/rcu_sync.h>",
            "#include <linux/sched.h>",
            "",
            "enum { GP_IDLE = 0, GP_ENTER, GP_PASSED, GP_EXIT, GP_REPLAY };",
            "",
            "#define\trss_lock\tgp_wait.lock",
            "",
            "/**",
            " * rcu_sync_init() - Initialize an rcu_sync structure",
            " * @rsp: Pointer to rcu_sync structure to be initialized",
            " */"
          ],
          "function_name": null,
          "description": "定义了RCU同步基础设施的枚举常量和rcu_sync_init函数声明，用于初始化rcu_sync结构体，但代码上下文不完整",
          "similarity": 0.5132296085357666
        }
      ]
    }
  ]
}