{
  "query": "MMU address translation",
  "timestamp": "2025-12-26 00:13:00",
  "retrieved_files": [
    {
      "source_file": "mm/mmu_notifier.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:53:38\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mmu_notifier.c`\n\n---\n\n# mmu_notifier.c 技术文档\n\n## 1. 文件概述\n\n`mmu_notifier.c` 是 Linux 内核中实现 **MMU Notifier（内存管理单元通知器）** 机制的核心文件。该机制允许内核子系统（如 KVM、RDMA、DAX 等）在用户虚拟地址空间发生页表变更（如页面回收、映射撤销等）时收到通知，从而同步维护其私有页表（如影子页表 SPTEs）或缓存状态。  \n本文件主要实现了基于 **区间树（interval tree）** 的高效范围监听机制，并通过一种类似 seqcount 的 **碰撞重试（collision-retry）读写同步模型**，在保证高并发性的同时避免在关键路径上使用阻塞锁。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct mmu_notifier_subscriptions`**  \n  每个 `mm_struct` 关联的订阅信息容器，包含：\n  - `list`：传统 MMU notifier 链表（用于非区间监听）\n  - `itree`：基于红黑树的区间树，存储 `mmu_interval_notifier`\n  - `invalidate_seq`：序列号，用于实现读写同步（奇数表示正在无效化）\n  - `active_invalidate_ranges`：当前活跃的无效化操作计数\n  - `deferred_list`：延迟处理的区间插入/删除队列\n  - `wq`：等待队列，用于唤醒等待无效化完成的读者\n  - `lock`：保护上述字段的自旋锁\n\n- **全局 SRCU 实例 `srcu`**  \n  用于安全地遍历和回调 MMU notifier 列表，避免在 RCU 临界区内睡眠。\n\n- **Lockdep 映射 `__mmu_notifier_invalidate_range_start_map`**  \n  用于死锁检测，标记 `invalidate_range_start` 的锁上下文。\n\n### 主要函数\n\n- **`mn_itree_inv_start_range()`**  \n  开始一个虚拟地址范围的无效化操作：增加计数、检查是否有监听者、若存在则将 `invalidate_seq` 设为奇数，并返回首个匹配的监听器。\n\n- **`mn_itree_inv_next()`**  \n  在无效化过程中迭代获取下一个匹配的区间监听器。\n\n- **`mn_itree_inv_end()`**  \n  结束无效化操作：减少计数；若为最后一个操作且处于完全排除状态，则将 `invalidate_seq` 加 1（变为偶数），并处理 `deferred_list` 中的延迟插入/删除，最后唤醒等待队列。\n\n- **`mmu_interval_read_begin()`**（片段）  \n  开始一个读端临界区：读取当前监听器的 `invalidate_seq`，用于后续与全局 `invalidate_seq` 比较以检测是否发生碰撞（即无效化操作介入）。\n\n> 注：`mmu_interval_read_retry()` 函数虽未完整给出，但其作用是比对 `interval_sub->invalidate_seq` 与读开始时保存的全局 `seq`，若不同则说明发生碰撞需重试。\n\n## 3. 关键实现\n\n### 碰撞重试同步机制（Collision-Retry Lock）\n\n- 使用 `invalidate_seq` 序列号模拟读写锁，但允许多个写者并发。\n- **写者（无效化操作）**：\n  - 进入时：`active_invalidate_ranges++`；若有监听者，则 `seq |= 1`（设为奇数）。\n  - 退出时：`active_invalidate_ranges--`；若为最后一个写者且 `seq` 为奇数，则 `seq++`（变为偶数）。\n- **读者（如获取 SPTE）**：\n  - 调用 `mmu_interval_read_begin()` 获取当前 `seq`。\n  - 在持有用户锁（如 mmap_lock）期间执行操作。\n  - 调用 `mmu_interval_read_retry()` 检查 `interval_sub->invalidate_seq` 是否等于初始 `seq`。若不等，说明无效化已发生，需重试。\n- **优势**：避免在 `invalidate_range_start` 中使用阻塞锁，提升 mm 路径性能。\n\n### 区间树与延迟更新\n\n- 所有 `mmu_interval_notifier` 按虚拟地址范围注册到 `itree` 中，支持高效范围查询。\n- 在无效化过程中（`invalidate_seq` 为奇数），禁止直接修改 `itree`。\n- 插入/删除操作被暂存到 `deferred_list`，在最后一个 `inv_end` 时批量处理，确保树结构一致性。\n\n### SRCU 用于安全回调\n\n- 全局 `srcu` 用于遍历 `mm->notifier_subscriptions->list` 并调用传统 notifier 回调。\n- 允许回调函数睡眠（相比 RCU 更灵活），同时保证在 `mm` 销毁前完成所有回调。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/mmu_notifier.h>`：核心 API 和数据结构定义\n  - `<linux/interval_tree.h>`：区间树操作\n  - `<linux/srcu.h>`、`<linux/rcupdate.h>`：同步原语\n  - `<linux/mm.h>`、`<linux/sched/mm.h>`：内存管理相关\n  - `<linux/rculist.h>`、`<linux/slab.h>`：链表和内存分配\n\n- **内核子系统依赖**：\n  - **Memory Management (MM)**：依赖 `mm_struct` 生命周期管理（`mmdrop` 释放 subscriptions）\n  - **KVM / VFIO / RDMA / DAX**：作为主要使用者，注册 `mmu_interval_notifier` 监听 VA 变更\n  - **Lockdep**：用于死锁检测（`CONFIG_LOCKDEP`）\n\n## 5. 使用场景\n\n1. **虚拟化（KVM）**  \n   当 Guest OS 的页表被 Host 回收或修改时，KVM 通过 MMU notifier 同步更新影子页表（SPTEs），避免访问已释放的物理页。\n\n2. **高性能计算（RDMA / InfiniBand）**  \n   用户态注册内存区域用于零拷贝 DMA。当该区域被 munmap 或 swap out 时，驱动需收到通知以撤销硬件映射，防止 DMA 访问非法内存。\n\n3. **持久内存（DAX）**  \n   DAX 直接映射持久内存到用户空间。当映射被撤销时，需刷新 CPU 缓存并确保数据持久化，MMU notifier 提供必要的同步点。\n\n4. **用户态页表管理**  \n   如用户态缺页处理（userfaultfd）或自定义内存管理器，需感知内核对 VA 的修改以维护一致性。\n\n> 总结：`mmu_notifier.c` 为需要与内核页表变更保持同步的子系统提供了高效、可扩展的通知框架，是现代 Linux 内核中虚拟化、高性能 I/O 和新型存储的关键基础设施。",
      "similarity": 0.4641110897064209,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 1,
          "end_line": 85,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  linux/mm/mmu_notifier.c",
            " *",
            " *  Copyright (C) 2008  Qumranet, Inc.",
            " *  Copyright (C) 2008  SGI",
            " *             Christoph Lameter <cl@linux.com>",
            " */",
            "",
            "#include <linux/rculist.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/export.h>",
            "#include <linux/mm.h>",
            "#include <linux/err.h>",
            "#include <linux/interval_tree.h>",
            "#include <linux/srcu.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/slab.h>",
            "",
            "/* global SRCU for all MMs */",
            "DEFINE_STATIC_SRCU(srcu);",
            "",
            "#ifdef CONFIG_LOCKDEP",
            "struct lockdep_map __mmu_notifier_invalidate_range_start_map = {",
            "\t.name = \"mmu_notifier_invalidate_range_start\"",
            "};",
            "#endif",
            "",
            "/*",
            " * The mmu_notifier_subscriptions structure is allocated and installed in",
            " * mm->notifier_subscriptions inside the mm_take_all_locks() protected",
            " * critical section and it's released only when mm_count reaches zero",
            " * in mmdrop().",
            " */",
            "struct mmu_notifier_subscriptions {",
            "\t/* all mmu notifiers registered in this mm are queued in this list */",
            "\tstruct hlist_head list;",
            "\tbool has_itree;",
            "\t/* to serialize the list modifications and hlist_unhashed */",
            "\tspinlock_t lock;",
            "\tunsigned long invalidate_seq;",
            "\tunsigned long active_invalidate_ranges;",
            "\tstruct rb_root_cached itree;",
            "\twait_queue_head_t wq;",
            "\tstruct hlist_head deferred_list;",
            "};",
            "",
            "/*",
            " * This is a collision-retry read-side/write-side 'lock', a lot like a",
            " * seqcount, however this allows multiple write-sides to hold it at",
            " * once. Conceptually the write side is protecting the values of the PTEs in",
            " * this mm, such that PTES cannot be read into SPTEs (shadow PTEs) while any",
            " * writer exists.",
            " *",
            " * Note that the core mm creates nested invalidate_range_start()/end() regions",
            " * within the same thread, and runs invalidate_range_start()/end() in parallel",
            " * on multiple CPUs. This is designed to not reduce concurrency or block",
            " * progress on the mm side.",
            " *",
            " * As a secondary function, holding the full write side also serves to prevent",
            " * writers for the itree, this is an optimization to avoid extra locking",
            " * during invalidate_range_start/end notifiers.",
            " *",
            " * The write side has two states, fully excluded:",
            " *  - mm->active_invalidate_ranges != 0",
            " *  - subscriptions->invalidate_seq & 1 == True (odd)",
            " *  - some range on the mm_struct is being invalidated",
            " *  - the itree is not allowed to change",
            " *",
            " * And partially excluded:",
            " *  - mm->active_invalidate_ranges != 0",
            " *  - subscriptions->invalidate_seq & 1 == False (even)",
            " *  - some range on the mm_struct is being invalidated",
            " *  - the itree is allowed to change",
            " *",
            " * Operations on notifier_subscriptions->invalidate_seq (under spinlock):",
            " *    seq |= 1  # Begin writing",
            " *    seq++     # Release the writing state",
            " *    seq & 1   # True if a writer exists",
            " *",
            " * The later state avoids some expensive work on inv_end in the common case of",
            " * no mmu_interval_notifier monitoring the VA.",
            " */"
          ],
          "function_name": null,
          "description": "定义了mmu_notifier_subscriptions结构体，用于管理进程地址空间中注册的MMU通知器订阅信息，包含红黑树、锁、序列号及等待队列等成员，用于协调多线程下的无效化范围操作。",
          "similarity": 0.46798819303512573
        },
        {
          "chunk_id": 4,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 519,
          "end_line": 666,
          "content": [
            "int __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\trange->mm->notifier_subscriptions;",
            "\tint ret;",
            "",
            "\tif (subscriptions->has_itree) {",
            "\t\tret = mn_itree_invalidate(subscriptions, range);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t}",
            "\tif (!hlist_empty(&subscriptions->list))",
            "\t\treturn mn_hlist_invalidate_range_start(subscriptions, range);",
            "\treturn 0;",
            "}",
            "static void",
            "mn_hlist_invalidate_end(struct mmu_notifier_subscriptions *subscriptions,",
            "\t\t\tstruct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tif (subscription->ops->invalidate_range_end) {",
            "\t\t\tif (!mmu_notifier_range_blockable(range))",
            "\t\t\t\tnon_block_start();",
            "\t\t\tsubscription->ops->invalidate_range_end(subscription,",
            "\t\t\t\t\t\t\t\trange);",
            "\t\t\tif (!mmu_notifier_range_blockable(range))",
            "\t\t\t\tnon_block_end();",
            "\t\t}",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "}",
            "void __mmu_notifier_invalidate_range_end(struct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\trange->mm->notifier_subscriptions;",
            "",
            "\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);",
            "\tif (subscriptions->has_itree)",
            "\t\tmn_itree_inv_end(subscriptions);",
            "",
            "\tif (!hlist_empty(&subscriptions->list))",
            "\t\tmn_hlist_invalidate_end(subscriptions, range);",
            "\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);",
            "}",
            "void __mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,",
            "\t\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription,",
            "\t\t\t\t &mm->notifier_subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tif (subscription->ops->arch_invalidate_secondary_tlbs)",
            "\t\t\tsubscription->ops->arch_invalidate_secondary_tlbs(",
            "\t\t\t\tsubscription, mm,",
            "\t\t\t\tstart, end);",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "}",
            "int __mmu_notifier_register(struct mmu_notifier *subscription,",
            "\t\t\t    struct mm_struct *mm)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions = NULL;",
            "\tint ret;",
            "",
            "\tmmap_assert_write_locked(mm);",
            "\tBUG_ON(atomic_read(&mm->mm_users) <= 0);",
            "",
            "\t/*",
            "\t * Subsystems should only register for invalidate_secondary_tlbs() or",
            "\t * invalidate_range_start()/end() callbacks, not both.",
            "\t */",
            "\tif (WARN_ON_ONCE(subscription &&",
            "\t\t\t (subscription->ops->arch_invalidate_secondary_tlbs &&",
            "\t\t\t (subscription->ops->invalidate_range_start ||",
            "\t\t\t  subscription->ops->invalidate_range_end))))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!mm->notifier_subscriptions) {",
            "\t\t/*",
            "\t\t * kmalloc cannot be called under mm_take_all_locks(), but we",
            "\t\t * know that mm->notifier_subscriptions can't change while we",
            "\t\t * hold the write side of the mmap_lock.",
            "\t\t */",
            "\t\tsubscriptions = kzalloc(",
            "\t\t\tsizeof(struct mmu_notifier_subscriptions), GFP_KERNEL);",
            "\t\tif (!subscriptions)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tINIT_HLIST_HEAD(&subscriptions->list);",
            "\t\tspin_lock_init(&subscriptions->lock);",
            "\t\tsubscriptions->invalidate_seq = 2;",
            "\t\tsubscriptions->itree = RB_ROOT_CACHED;",
            "\t\tinit_waitqueue_head(&subscriptions->wq);",
            "\t\tINIT_HLIST_HEAD(&subscriptions->deferred_list);",
            "\t}",
            "",
            "\tret = mm_take_all_locks(mm);",
            "\tif (unlikely(ret))",
            "\t\tgoto out_clean;",
            "",
            "\t/*",
            "\t * Serialize the update against mmu_notifier_unregister. A",
            "\t * side note: mmu_notifier_release can't run concurrently with",
            "\t * us because we hold the mm_users pin (either implicitly as",
            "\t * current->mm or explicitly with get_task_mm() or similar).",
            "\t * We can't race against any other mmu notifier method either",
            "\t * thanks to mm_take_all_locks().",
            "\t *",
            "\t * release semantics on the initialization of the",
            "\t * mmu_notifier_subscriptions's contents are provided for unlocked",
            "\t * readers.  acquire can only be used while holding the mmgrab or",
            "\t * mmget, and is safe because once created the",
            "\t * mmu_notifier_subscriptions is not freed until the mm is destroyed.",
            "\t * As above, users holding the mmap_lock or one of the",
            "\t * mm_take_all_locks() do not need to use acquire semantics.",
            "\t */",
            "\tif (subscriptions)",
            "\t\tsmp_store_release(&mm->notifier_subscriptions, subscriptions);",
            "",
            "\tif (subscription) {",
            "\t\t/* Pairs with the mmdrop in mmu_notifier_unregister_* */",
            "\t\tmmgrab(mm);",
            "\t\tsubscription->mm = mm;",
            "\t\tsubscription->users = 1;",
            "",
            "\t\tspin_lock(&mm->notifier_subscriptions->lock);",
            "\t\thlist_add_head_rcu(&subscription->hlist,",
            "\t\t\t\t   &mm->notifier_subscriptions->list);",
            "\t\tspin_unlock(&mm->notifier_subscriptions->lock);",
            "\t} else",
            "\t\tmm->notifier_subscriptions->has_itree = true;",
            "",
            "\tmm_drop_all_locks(mm);",
            "\tBUG_ON(atomic_read(&mm->mm_users) <= 0);",
            "\treturn 0;",
            "",
            "out_clean:",
            "\tkfree(subscriptions);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__mmu_notifier_invalidate_range_start, mn_hlist_invalidate_end, __mmu_notifier_invalidate_range_end, __mmu_notifier_arch_invalidate_secondary_tlbs, __mmu_notifier_register",
          "description": "封装了完整的MMU通知器生命周期管理，包含范围无效化启动/结束接口(__mmu_notifier_invalidate_range_start/end)、架构特定TLB刷新(__mmu_notifier_arch_invalidate_secondary_tlbs)以及注册接口(__mmu_notifier_register)。",
          "similarity": 0.45688360929489136
        },
        {
          "chunk_id": 6,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 970,
          "end_line": 1066,
          "content": [
            "int mmu_interval_notifier_insert(struct mmu_interval_notifier *interval_sub,",
            "\t\t\t\t struct mm_struct *mm, unsigned long start,",
            "\t\t\t\t unsigned long length,",
            "\t\t\t\t const struct mmu_interval_notifier_ops *ops)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions;",
            "\tint ret;",
            "",
            "\tmight_lock(&mm->mmap_lock);",
            "",
            "\tsubscriptions = smp_load_acquire(&mm->notifier_subscriptions);",
            "\tif (!subscriptions || !subscriptions->has_itree) {",
            "\t\tret = mmu_notifier_register(NULL, mm);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t\tsubscriptions = mm->notifier_subscriptions;",
            "\t}",
            "\treturn __mmu_interval_notifier_insert(interval_sub, mm, subscriptions,",
            "\t\t\t\t\t      start, length, ops);",
            "}",
            "int mmu_interval_notifier_insert_locked(",
            "\tstruct mmu_interval_notifier *interval_sub, struct mm_struct *mm,",
            "\tunsigned long start, unsigned long length,",
            "\tconst struct mmu_interval_notifier_ops *ops)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\tmm->notifier_subscriptions;",
            "\tint ret;",
            "",
            "\tmmap_assert_write_locked(mm);",
            "",
            "\tif (!subscriptions || !subscriptions->has_itree) {",
            "\t\tret = __mmu_notifier_register(NULL, mm);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t\tsubscriptions = mm->notifier_subscriptions;",
            "\t}",
            "\treturn __mmu_interval_notifier_insert(interval_sub, mm, subscriptions,",
            "\t\t\t\t\t      start, length, ops);",
            "}",
            "static bool",
            "mmu_interval_seq_released(struct mmu_notifier_subscriptions *subscriptions,",
            "\t\t\t  unsigned long seq)",
            "{",
            "\tbool ret;",
            "",
            "\tspin_lock(&subscriptions->lock);",
            "\tret = subscriptions->invalidate_seq != seq;",
            "\tspin_unlock(&subscriptions->lock);",
            "\treturn ret;",
            "}",
            "void mmu_interval_notifier_remove(struct mmu_interval_notifier *interval_sub)",
            "{",
            "\tstruct mm_struct *mm = interval_sub->mm;",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\tmm->notifier_subscriptions;",
            "\tunsigned long seq = 0;",
            "",
            "\tmight_sleep();",
            "",
            "\tspin_lock(&subscriptions->lock);",
            "\tif (mn_itree_is_invalidating(subscriptions)) {",
            "\t\t/*",
            "\t\t * remove is being called after insert put this on the",
            "\t\t * deferred list, but before the deferred list was processed.",
            "\t\t */",
            "\t\tif (RB_EMPTY_NODE(&interval_sub->interval_tree.rb)) {",
            "\t\t\thlist_del(&interval_sub->deferred_item);",
            "\t\t} else {",
            "\t\t\thlist_add_head(&interval_sub->deferred_item,",
            "\t\t\t\t       &subscriptions->deferred_list);",
            "\t\t\tseq = subscriptions->invalidate_seq;",
            "\t\t}",
            "\t} else {",
            "\t\tWARN_ON(RB_EMPTY_NODE(&interval_sub->interval_tree.rb));",
            "\t\tinterval_tree_remove(&interval_sub->interval_tree,",
            "\t\t\t\t     &subscriptions->itree);",
            "\t}",
            "\tspin_unlock(&subscriptions->lock);",
            "",
            "\t/*",
            "\t * The possible sleep on progress in the invalidation requires the",
            "\t * caller not hold any locks held by invalidation callbacks.",
            "\t */",
            "\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);",
            "\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);",
            "\tif (seq)",
            "\t\twait_event(subscriptions->wq,",
            "\t\t\t   mmu_interval_seq_released(subscriptions, seq));",
            "",
            "\t/* pairs with mmgrab in mmu_interval_notifier_insert() */",
            "\tmmdrop(mm);",
            "}",
            "void mmu_notifier_synchronize(void)",
            "{",
            "\tsynchronize_srcu(&srcu);",
            "}"
          ],
          "function_name": "mmu_interval_notifier_insert, mmu_interval_notifier_insert_locked, mmu_interval_seq_released, mmu_interval_notifier_remove, mmu_notifier_synchronize",
          "description": "管理基于区间树的MMU观察者插入与移除逻辑，通过间隔树跟踪虚拟地址范围，处理无效化期间的延迟删除操作，提供同步机制确保序列号变更后唤醒等待线程，最终通过mmdrop释放mm引用",
          "similarity": 0.44722771644592285
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 697,
          "end_line": 843,
          "content": [
            "int mmu_notifier_register(struct mmu_notifier *subscription,",
            "\t\t\t  struct mm_struct *mm)",
            "{",
            "\tint ret;",
            "",
            "\tmmap_write_lock(mm);",
            "\tret = __mmu_notifier_register(subscription, mm);",
            "\tmmap_write_unlock(mm);",
            "\treturn ret;",
            "}",
            "void __mmu_notifier_subscriptions_destroy(struct mm_struct *mm)",
            "{",
            "\tBUG_ON(!hlist_empty(&mm->notifier_subscriptions->list));",
            "\tkfree(mm->notifier_subscriptions);",
            "\tmm->notifier_subscriptions = LIST_POISON1; /* debug */",
            "}",
            "void mmu_notifier_unregister(struct mmu_notifier *subscription,",
            "\t\t\t     struct mm_struct *mm)",
            "{",
            "\tBUG_ON(atomic_read(&mm->mm_count) <= 0);",
            "",
            "\tif (!hlist_unhashed(&subscription->hlist)) {",
            "\t\t/*",
            "\t\t * SRCU here will force exit_mmap to wait for ->release to",
            "\t\t * finish before freeing the pages.",
            "\t\t */",
            "\t\tint id;",
            "",
            "\t\tid = srcu_read_lock(&srcu);",
            "\t\t/*",
            "\t\t * exit_mmap will block in mmu_notifier_release to guarantee",
            "\t\t * that ->release is called before freeing the pages.",
            "\t\t */",
            "\t\tif (subscription->ops->release)",
            "\t\t\tsubscription->ops->release(subscription, mm);",
            "\t\tsrcu_read_unlock(&srcu, id);",
            "",
            "\t\tspin_lock(&mm->notifier_subscriptions->lock);",
            "\t\t/*",
            "\t\t * Can not use list_del_rcu() since __mmu_notifier_release",
            "\t\t * can delete it before we hold the lock.",
            "\t\t */",
            "\t\thlist_del_init_rcu(&subscription->hlist);",
            "\t\tspin_unlock(&mm->notifier_subscriptions->lock);",
            "\t}",
            "",
            "\t/*",
            "\t * Wait for any running method to finish, of course including",
            "\t * ->release if it was run by mmu_notifier_release instead of us.",
            "\t */",
            "\tsynchronize_srcu(&srcu);",
            "",
            "\tBUG_ON(atomic_read(&mm->mm_count) <= 0);",
            "",
            "\tmmdrop(mm);",
            "}",
            "static void mmu_notifier_free_rcu(struct rcu_head *rcu)",
            "{",
            "\tstruct mmu_notifier *subscription =",
            "\t\tcontainer_of(rcu, struct mmu_notifier, rcu);",
            "\tstruct mm_struct *mm = subscription->mm;",
            "",
            "\tsubscription->ops->free_notifier(subscription);",
            "\t/* Pairs with the get in __mmu_notifier_register() */",
            "\tmmdrop(mm);",
            "}",
            "void mmu_notifier_put(struct mmu_notifier *subscription)",
            "{",
            "\tstruct mm_struct *mm = subscription->mm;",
            "",
            "\tspin_lock(&mm->notifier_subscriptions->lock);",
            "\tif (WARN_ON(!subscription->users) || --subscription->users)",
            "\t\tgoto out_unlock;",
            "\thlist_del_init_rcu(&subscription->hlist);",
            "\tspin_unlock(&mm->notifier_subscriptions->lock);",
            "",
            "\tcall_srcu(&srcu, &subscription->rcu, mmu_notifier_free_rcu);",
            "\treturn;",
            "",
            "out_unlock:",
            "\tspin_unlock(&mm->notifier_subscriptions->lock);",
            "}",
            "static int __mmu_interval_notifier_insert(",
            "\tstruct mmu_interval_notifier *interval_sub, struct mm_struct *mm,",
            "\tstruct mmu_notifier_subscriptions *subscriptions, unsigned long start,",
            "\tunsigned long length, const struct mmu_interval_notifier_ops *ops)",
            "{",
            "\tinterval_sub->mm = mm;",
            "\tinterval_sub->ops = ops;",
            "\tRB_CLEAR_NODE(&interval_sub->interval_tree.rb);",
            "\tinterval_sub->interval_tree.start = start;",
            "\t/*",
            "\t * Note that the representation of the intervals in the interval tree",
            "\t * considers the ending point as contained in the interval.",
            "\t */",
            "\tif (length == 0 ||",
            "\t    check_add_overflow(start, length - 1,",
            "\t\t\t       &interval_sub->interval_tree.last))",
            "\t\treturn -EOVERFLOW;",
            "",
            "\t/* Must call with a mmget() held */",
            "\tif (WARN_ON(atomic_read(&mm->mm_users) <= 0))",
            "\t\treturn -EINVAL;",
            "",
            "\t/* pairs with mmdrop in mmu_interval_notifier_remove() */",
            "\tmmgrab(mm);",
            "",
            "\t/*",
            "\t * If some invalidate_range_start/end region is going on in parallel",
            "\t * we don't know what VA ranges are affected, so we must assume this",
            "\t * new range is included.",
            "\t *",
            "\t * If the itree is invalidating then we are not allowed to change",
            "\t * it. Retrying until invalidation is done is tricky due to the",
            "\t * possibility for live lock, instead defer the add to",
            "\t * mn_itree_inv_end() so this algorithm is deterministic.",
            "\t *",
            "\t * In all cases the value for the interval_sub->invalidate_seq should be",
            "\t * odd, see mmu_interval_read_begin()",
            "\t */",
            "\tspin_lock(&subscriptions->lock);",
            "\tif (subscriptions->active_invalidate_ranges) {",
            "\t\tif (mn_itree_is_invalidating(subscriptions))",
            "\t\t\thlist_add_head(&interval_sub->deferred_item,",
            "\t\t\t\t       &subscriptions->deferred_list);",
            "\t\telse {",
            "\t\t\tsubscriptions->invalidate_seq |= 1;",
            "\t\t\tinterval_tree_insert(&interval_sub->interval_tree,",
            "\t\t\t\t\t     &subscriptions->itree);",
            "\t\t}",
            "\t\tinterval_sub->invalidate_seq = subscriptions->invalidate_seq;",
            "\t} else {",
            "\t\tWARN_ON(mn_itree_is_invalidating(subscriptions));",
            "\t\t/*",
            "\t\t * The starting seq for a subscription not under invalidation",
            "\t\t * should be odd, not equal to the current invalidate_seq and",
            "\t\t * invalidate_seq should not 'wrap' to the new seq any time",
            "\t\t * soon.",
            "\t\t */",
            "\t\tinterval_sub->invalidate_seq =",
            "\t\t\tsubscriptions->invalidate_seq - 1;",
            "\t\tinterval_tree_insert(&interval_sub->interval_tree,",
            "\t\t\t\t     &subscriptions->itree);",
            "\t}",
            "\tspin_unlock(&subscriptions->lock);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "mmu_notifier_register, __mmu_notifier_subscriptions_destroy, mmu_notifier_unregister, mmu_notifier_free_rcu, mmu_notifier_put, __mmu_interval_notifier_insert",
          "description": "实现MMU观察者的注册与注销流程，通过mmap锁保护订阅列表操作，利用SRCU机制确保退出mmap时调用release回调，通过RCU延迟释放订阅者资源，处理订阅列表销毁时验证为空并释放内存",
          "similarity": 0.4400767683982849
        },
        {
          "chunk_id": 2,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 261,
          "end_line": 363,
          "content": [
            "static void mn_itree_release(struct mmu_notifier_subscriptions *subscriptions,",
            "\t\t\t     struct mm_struct *mm)",
            "{",
            "\tstruct mmu_notifier_range range = {",
            "\t\t.flags = MMU_NOTIFIER_RANGE_BLOCKABLE,",
            "\t\t.event = MMU_NOTIFY_RELEASE,",
            "\t\t.mm = mm,",
            "\t\t.start = 0,",
            "\t\t.end = ULONG_MAX,",
            "\t};",
            "\tstruct mmu_interval_notifier *interval_sub;",
            "\tunsigned long cur_seq;",
            "\tbool ret;",
            "",
            "\tfor (interval_sub =",
            "\t\t     mn_itree_inv_start_range(subscriptions, &range, &cur_seq);",
            "\t     interval_sub;",
            "\t     interval_sub = mn_itree_inv_next(interval_sub, &range)) {",
            "\t\tret = interval_sub->ops->invalidate(interval_sub, &range,",
            "\t\t\t\t\t\t    cur_seq);",
            "\t\tWARN_ON(!ret);",
            "\t}",
            "",
            "\tmn_itree_inv_end(subscriptions);",
            "}",
            "static void mn_hlist_release(struct mmu_notifier_subscriptions *subscriptions,",
            "\t\t\t     struct mm_struct *mm)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint id;",
            "",
            "\t/*",
            "\t * SRCU here will block mmu_notifier_unregister until",
            "\t * ->release returns.",
            "\t */",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu))",
            "\t\t/*",
            "\t\t * If ->release runs before mmu_notifier_unregister it must be",
            "\t\t * handled, as it's the only way for the driver to flush all",
            "\t\t * existing sptes and stop the driver from establishing any more",
            "\t\t * sptes before all the pages in the mm are freed.",
            "\t\t */",
            "\t\tif (subscription->ops->release)",
            "\t\t\tsubscription->ops->release(subscription, mm);",
            "",
            "\tspin_lock(&subscriptions->lock);",
            "\twhile (unlikely(!hlist_empty(&subscriptions->list))) {",
            "\t\tsubscription = hlist_entry(subscriptions->list.first,",
            "\t\t\t\t\t   struct mmu_notifier, hlist);",
            "\t\t/*",
            "\t\t * We arrived before mmu_notifier_unregister so",
            "\t\t * mmu_notifier_unregister will do nothing other than to wait",
            "\t\t * for ->release to finish and for mmu_notifier_unregister to",
            "\t\t * return.",
            "\t\t */",
            "\t\thlist_del_init_rcu(&subscription->hlist);",
            "\t}",
            "\tspin_unlock(&subscriptions->lock);",
            "\tsrcu_read_unlock(&srcu, id);",
            "",
            "\t/*",
            "\t * synchronize_srcu here prevents mmu_notifier_release from returning to",
            "\t * exit_mmap (which would proceed with freeing all pages in the mm)",
            "\t * until the ->release method returns, if it was invoked by",
            "\t * mmu_notifier_unregister.",
            "\t *",
            "\t * The notifier_subscriptions can't go away from under us because",
            "\t * one mm_count is held by exit_mmap.",
            "\t */",
            "\tsynchronize_srcu(&srcu);",
            "}",
            "void __mmu_notifier_release(struct mm_struct *mm)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\tmm->notifier_subscriptions;",
            "",
            "\tif (subscriptions->has_itree)",
            "\t\tmn_itree_release(subscriptions, mm);",
            "",
            "\tif (!hlist_empty(&subscriptions->list))",
            "\t\tmn_hlist_release(subscriptions, mm);",
            "}",
            "int __mmu_notifier_clear_flush_young(struct mm_struct *mm,",
            "\t\t\t\t\tunsigned long start,",
            "\t\t\t\t\tunsigned long end)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint young = 0, id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription,",
            "\t\t\t\t &mm->notifier_subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tif (subscription->ops->clear_flush_young)",
            "\t\t\tyoung |= subscription->ops->clear_flush_young(",
            "\t\t\t\tsubscription, mm, start, end);",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "",
            "\treturn young;",
            "}"
          ],
          "function_name": "mn_itree_release, mn_hlist_release, __mmu_notifier_release, __mmu_notifier_clear_flush_young",
          "description": "提供了订阅资源释放机制，包含基于区间树的释放流程(mn_itree_release)和普通链表的释放流程(mn_hlist_release)，通过SRCU保护遍历所有注册的MMU通知器并调用其release回调进行资源清理。",
          "similarity": 0.42408251762390137
        }
      ]
    },
    {
      "source_file": "mm/nommu.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:57:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `nommu.c`\n\n---\n\n# nommu.c 技术文档\n\n## 1. 文件概述\n\n`nommu.c` 是 Linux 内核中为不支持内存管理单元（MMU）的 CPU 架构提供的内存管理替代实现。在无 MMU 的系统（如某些嵌入式处理器）中，无法使用虚拟内存机制，因此该文件提供了一套简化但功能完整的内存管理接口，以兼容标准内核 API。其核心目标是模拟 `mm/` 子系统中与虚拟内存相关的函数行为，同时避免依赖页表、地址转换等 MMU 特性。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `high_memory`：指向高内存区域起始地址（在 NOMMU 系统中通常为 NULL 或物理内存末尾）\n- `mem_map`：物理页描述符数组的起始地址\n- `max_mapnr`：系统中最大页帧号（PFN）\n- `highest_memmap_pfn`：`mem_map` 中最高有效 PFN\n- `sysctl_nr_trim_pages`：初始内存修剪阈值（用于释放未使用的内存块）\n- `heap_stack_gap`：堆与栈之间的最小间隙（NOMMU 下通常为 0）\n- `mmap_pages_allocated`：通过 mmap 分配的总页数（原子计数器）\n- `nommu_region_tree`：红黑树，用于管理已映射的共享内存区域\n- `nommu_region_sem`：读写信号量，保护 `nommu_region_tree` 的并发访问\n- `vm_region_jar`：slab 缓存，用于分配 `vm_region` 结构\n\n### 主要函数\n- `kobjsize(const void *objp)`：估算给定指针所占内存大小（支持 kmalloc、VMA 区域或普通页）\n- `follow_pfn(struct vm_area_struct *vma, unsigned long address, unsigned long *pfn)`：从用户虚拟地址获取物理页帧号（仅支持 IO 或 PFN 映射）\n- `vfree(const void *addr)`：释放由 vmalloc 分配的内存（实际调用 kfree）\n- `__vmalloc_noprof()` 及相关变体（`vmalloc`, `vzalloc`, `vmalloc_user` 等）：提供 vmalloc 系列函数的 NOMMU 实现（底层使用 kmalloc）\n- `vmalloc_to_page()` / `vmalloc_to_pfn()`：将 vmalloc 地址转换为 page 或 PFN（直接使用 `virt_to_page`）\n- `vread_iter()`：将内核地址内容拷贝到 iov_iter（用于 `/proc/vmallocinfo` 等）\n\n### 操作结构体\n- `generic_file_vm_ops`：空的 `vm_operations_struct`，作为 NOMMU 下文件映射的默认操作集\n\n## 3. 关键实现\n\n### 内存分配模型\n- **无虚拟地址空间**：所有“虚拟”地址实为物理地址的线性映射，`vmalloc` 系列函数退化为 `kmalloc` 调用。\n- **GFP 标志处理**：自动清除 `__GFP_HIGHMEM`（因 kmalloc 无法返回高端内存逻辑地址），并添加 `__GFP_COMP` 以支持复合页。\n- **零填充支持**：`vzalloc` 等函数通过 `__GFP_ZERO` 标志实现。\n\n### 内存区域管理\n- **共享区域跟踪**：使用红黑树 `nommu_region_tree` 和 slab 缓存 `vm_region_jar` 管理可共享的映射区域（如文件映射），确保多个进程可共享同一物理内存。\n- **VMA 标记**：`vmalloc_user` 分配的内存会标记 `VM_USERMAP`，允许后续通过 `remap_vmalloc_range` 映射到用户空间。\n\n### 地址转换简化\n- **PFN 获取**：`follow_pfn` 直接通过地址右移 `PAGE_SHIFT` 计算 PFN（因无虚拟地址转换）。\n- **vmalloc 地址转换**：`vmalloc_to_page` 直接调用 `virt_to_page`（因 vmalloc 地址即物理地址的线性映射）。\n\n### 安全与兼容性\n- **kobjsize 安全检查**：先验证地址有效性（`virt_addr_valid`），再根据页类型（Slab/Compound/VMA）返回不同大小。\n- **用户空间映射**：`vmalloc_user` 在分配后查找对应 VMA 并设置 `VM_USERMAP`，确保安全暴露给用户态。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心内存管理**：`<linux/mm.h>`, `<linux/mman.h>`, `\"internal.h\"`\n- **内存分配**：`<linux/slab.h>`, `<linux/vmalloc.h>`\n- **页与地址转换**：`<linux/highmem.h>`, `<linux/pagemap.h>`, `<asm/tlb.h>`\n- **进程与安全**：`<linux/sched/mm.h>`, `<linux/security.h>`, `<linux/audit.h>`\n- **I/O 与文件**：`<linux/file.h>`, `<linux/uio.h>`, `<linux/backing-dev.h>`\n\n### 符号导出\n- 导出关键符号供其他模块使用：`high_memory`, `max_mapnr`, `mem_map`, `follow_pfn`, `vfree`, `vmalloc` 系列函数等。\n\n### 架构依赖\n- 依赖架构特定头文件（如 `asm/tlb.h`, `asm/mmu_context.h`），但 NOMMU 架构下这些通常为空实现。\n\n## 5. 使用场景\n\n- **无 MMU 嵌入式系统**：运行于 uClinux 等无虚拟内存系统的设备（如早期 ARM7、Blackfin、m68knommu）。\n- **内核子系统兼容**：为依赖 `vmalloc`、`vfree`、`follow_pfn` 等接口的驱动或子系统（如 GPU 驱动、DMA 映射）提供 NOMMU 兼容层。\n- **用户空间内存映射**：支持 `mmap` 系统调用对文件或设备的映射（通过 `nommu_region_tree` 管理共享区域）。\n- **调试与监控**：`vread_iter` 支持 `/proc/vmallocinfo` 等接口读取内核内存布局。\n- **安全内存分配**：`vmalloc_user` 提供可安全映射到用户空间的零初始化内存（用于 IPC 或共享缓冲区）。",
      "similarity": 0.44846010208129883,
      "chunks": [
        {
          "chunk_id": 8,
          "file_path": "mm/nommu.c",
          "start_line": 1612,
          "end_line": 1742,
          "content": [
            "int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,",
            "\t\tunsigned long pfn, unsigned long size, pgprot_t prot)",
            "{",
            "\tif (addr != (pfn << PAGE_SHIFT))",
            "\t\treturn -EINVAL;",
            "",
            "\tvm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);",
            "\treturn 0;",
            "}",
            "int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)",
            "{",
            "\tunsigned long pfn = start >> PAGE_SHIFT;",
            "\tunsigned long vm_len = vma->vm_end - vma->vm_start;",
            "",
            "\tpfn += vma->vm_pgoff;",
            "\treturn io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);",
            "}",
            "int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,",
            "\t\t\tunsigned long pgoff)",
            "{",
            "\tunsigned int size = vma->vm_end - vma->vm_start;",
            "",
            "\tif (!(vma->vm_flags & VM_USERMAP))",
            "\t\treturn -EINVAL;",
            "",
            "\tvma->vm_start = (unsigned long)(addr + (pgoff << PAGE_SHIFT));",
            "\tvma->vm_end = vma->vm_start + size;",
            "",
            "\treturn 0;",
            "}",
            "vm_fault_t filemap_fault(struct vm_fault *vmf)",
            "{",
            "\tBUG();",
            "\treturn 0;",
            "}",
            "vm_fault_t filemap_map_pages(struct vm_fault *vmf,",
            "\t\tpgoff_t start_pgoff, pgoff_t end_pgoff)",
            "{",
            "\tBUG();",
            "\treturn 0;",
            "}",
            "int __access_remote_vm(struct mm_struct *mm, unsigned long addr, void *buf,",
            "\t\t       int len, unsigned int gup_flags)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tint write = gup_flags & FOLL_WRITE;",
            "",
            "\tif (mmap_read_lock_killable(mm))",
            "\t\treturn 0;",
            "",
            "\t/* the access must start within one of the target process's mappings */",
            "\tvma = find_vma(mm, addr);",
            "\tif (vma) {",
            "\t\t/* don't overrun this mapping */",
            "\t\tif (addr + len >= vma->vm_end)",
            "\t\t\tlen = vma->vm_end - addr;",
            "",
            "\t\t/* only read or write mappings where it is permitted */",
            "\t\tif (write && vma->vm_flags & VM_MAYWRITE)",
            "\t\t\tcopy_to_user_page(vma, NULL, addr,",
            "\t\t\t\t\t (void *) addr, buf, len);",
            "\t\telse if (!write && vma->vm_flags & VM_MAYREAD)",
            "\t\t\tcopy_from_user_page(vma, NULL, addr,",
            "\t\t\t\t\t    buf, (void *) addr, len);",
            "\t\telse",
            "\t\t\tlen = 0;",
            "\t} else {",
            "\t\tlen = 0;",
            "\t}",
            "",
            "\tmmap_read_unlock(mm);",
            "",
            "\treturn len;",
            "}",
            "int access_remote_vm(struct mm_struct *mm, unsigned long addr,",
            "\t\tvoid *buf, int len, unsigned int gup_flags)",
            "{",
            "\treturn __access_remote_vm(mm, addr, buf, len, gup_flags);",
            "}",
            "int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len,",
            "\t\tunsigned int gup_flags)",
            "{",
            "\tstruct mm_struct *mm;",
            "",
            "\tif (addr + len < addr)",
            "\t\treturn 0;",
            "",
            "\tmm = get_task_mm(tsk);",
            "\tif (!mm)",
            "\t\treturn 0;",
            "",
            "\tlen = __access_remote_vm(mm, addr, buf, len, gup_flags);",
            "",
            "\tmmput(mm);",
            "\treturn len;",
            "}",
            "static int __copy_remote_vm_str(struct mm_struct *mm, unsigned long addr,",
            "\t\t\t\tvoid *buf, int len)",
            "{",
            "\tunsigned long addr_end;",
            "\tstruct vm_area_struct *vma;",
            "\tint ret = -EFAULT;",
            "",
            "\t*(char *)buf = '\\0';",
            "",
            "\tif (mmap_read_lock_killable(mm))",
            "\t\treturn ret;",
            "",
            "\t/* the access must start within one of the target process's mappings */",
            "\tvma = find_vma(mm, addr);",
            "\tif (!vma)",
            "\t\tgoto out;",
            "",
            "\tif (check_add_overflow(addr, len, &addr_end))",
            "\t\tgoto out;",
            "",
            "\t/* don't overrun this mapping */",
            "\tif (addr_end > vma->vm_end)",
            "\t\tlen = vma->vm_end - addr;",
            "",
            "\t/* only read mappings where it is permitted */",
            "\tif (vma->vm_flags & VM_MAYREAD) {",
            "\t\tret = strscpy(buf, (char *)addr, len);",
            "\t\tif (ret < 0)",
            "\t\t\tret = len - 1;",
            "\t}",
            "",
            "out:",
            "\tmmap_read_unlock(mm);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "remap_pfn_range, vm_iomap_memory, remap_vmalloc_range, filemap_fault, filemap_map_pages, __access_remote_vm, access_remote_vm, access_process_vm, __copy_remote_vm_str",
          "description": "该代码块主要处理非MMU环境下的物理地址到虚拟地址的映射及远程内存访问控制。  \n`remap_pfn_range`/`vm_iomap_memory`/`remap_vmalloc_range`实现物理页框或虚拟内存区域的映射配置，`__access_remote_vm`系列函数提供对目标进程虚拟内存的受限读写访问。  \n部分函数依赖未展示的辅助实现（如`io_remap_pfn_range`），且`filemap_fault`等函数通过`BUG()`表明仅在特定条件下触发。",
          "similarity": 0.4277830719947815
        },
        {
          "chunk_id": 2,
          "file_path": "mm/nommu.c",
          "start_line": 433,
          "end_line": 545,
          "content": [
            "static noinline void validate_nommu_regions(void)",
            "{",
            "\tstruct vm_region *region, *last;",
            "\tstruct rb_node *p, *lastp;",
            "",
            "\tlastp = rb_first(&nommu_region_tree);",
            "\tif (!lastp)",
            "\t\treturn;",
            "",
            "\tlast = rb_entry(lastp, struct vm_region, vm_rb);",
            "\tBUG_ON(last->vm_end <= last->vm_start);",
            "\tBUG_ON(last->vm_top < last->vm_end);",
            "",
            "\twhile ((p = rb_next(lastp))) {",
            "\t\tregion = rb_entry(p, struct vm_region, vm_rb);",
            "\t\tlast = rb_entry(lastp, struct vm_region, vm_rb);",
            "",
            "\t\tBUG_ON(region->vm_end <= region->vm_start);",
            "\t\tBUG_ON(region->vm_top < region->vm_end);",
            "\t\tBUG_ON(region->vm_start < last->vm_top);",
            "",
            "\t\tlastp = p;",
            "\t}",
            "}",
            "static void validate_nommu_regions(void)",
            "{",
            "}",
            "static void add_nommu_region(struct vm_region *region)",
            "{",
            "\tstruct vm_region *pregion;",
            "\tstruct rb_node **p, *parent;",
            "",
            "\tvalidate_nommu_regions();",
            "",
            "\tparent = NULL;",
            "\tp = &nommu_region_tree.rb_node;",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tpregion = rb_entry(parent, struct vm_region, vm_rb);",
            "\t\tif (region->vm_start < pregion->vm_start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (region->vm_start > pregion->vm_start)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse if (pregion == region)",
            "\t\t\treturn;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "",
            "\trb_link_node(&region->vm_rb, parent, p);",
            "\trb_insert_color(&region->vm_rb, &nommu_region_tree);",
            "",
            "\tvalidate_nommu_regions();",
            "}",
            "static void delete_nommu_region(struct vm_region *region)",
            "{",
            "\tBUG_ON(!nommu_region_tree.rb_node);",
            "",
            "\tvalidate_nommu_regions();",
            "\trb_erase(&region->vm_rb, &nommu_region_tree);",
            "\tvalidate_nommu_regions();",
            "}",
            "static void free_page_series(unsigned long from, unsigned long to)",
            "{",
            "\tfor (; from < to; from += PAGE_SIZE) {",
            "\t\tstruct page *page = virt_to_page((void *)from);",
            "",
            "\t\tatomic_long_dec(&mmap_pages_allocated);",
            "\t\tput_page(page);",
            "\t}",
            "}",
            "static void __put_nommu_region(struct vm_region *region)",
            "\t__releases(nommu_region_sem)",
            "{",
            "\tBUG_ON(!nommu_region_tree.rb_node);",
            "",
            "\tif (--region->vm_usage == 0) {",
            "\t\tif (region->vm_top > region->vm_start)",
            "\t\t\tdelete_nommu_region(region);",
            "\t\tup_write(&nommu_region_sem);",
            "",
            "\t\tif (region->vm_file)",
            "\t\t\tfput(region->vm_file);",
            "",
            "\t\t/* IO memory and memory shared directly out of the pagecache",
            "\t\t * from ramfs/tmpfs mustn't be released here */",
            "\t\tif (region->vm_flags & VM_MAPPED_COPY)",
            "\t\t\tfree_page_series(region->vm_start, region->vm_top);",
            "\t\tkmem_cache_free(vm_region_jar, region);",
            "\t} else {",
            "\t\tup_write(&nommu_region_sem);",
            "\t}",
            "}",
            "static void put_nommu_region(struct vm_region *region)",
            "{",
            "\tdown_write(&nommu_region_sem);",
            "\t__put_nommu_region(region);",
            "}",
            "static void setup_vma_to_mm(struct vm_area_struct *vma, struct mm_struct *mm)",
            "{",
            "\tvma->vm_mm = mm;",
            "",
            "\t/* add the VMA to the mapping */",
            "\tif (vma->vm_file) {",
            "\t\tstruct address_space *mapping = vma->vm_file->f_mapping;",
            "",
            "\t\ti_mmap_lock_write(mapping);",
            "\t\tflush_dcache_mmap_lock(mapping);",
            "\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);",
            "\t\tflush_dcache_mmap_unlock(mapping);",
            "\t\ti_mmap_unlock_write(mapping);",
            "\t}",
            "}"
          ],
          "function_name": "validate_nommu_regions, validate_nommu_regions, add_nommu_region, delete_nommu_region, free_page_series, __put_nommu_region, put_nommu_region, setup_vma_to_mm",
          "description": "实现非MMU区域的管理功能，包括添加/删除内存区域至RB树、验证区域顺序、释放页面系列等操作。通过锁机制保护区域树并发访问，并关联VMA到MM结构。",
          "similarity": 0.42476487159729004
        },
        {
          "chunk_id": 7,
          "file_path": "mm/nommu.c",
          "start_line": 1437,
          "end_line": 1569,
          "content": [
            "int do_munmap(struct mm_struct *mm, unsigned long start, size_t len, struct list_head *uf)",
            "{",
            "\tVMA_ITERATOR(vmi, mm, start);",
            "\tstruct vm_area_struct *vma;",
            "\tunsigned long end;",
            "\tint ret = 0;",
            "",
            "\tlen = PAGE_ALIGN(len);",
            "\tif (len == 0)",
            "\t\treturn -EINVAL;",
            "",
            "\tend = start + len;",
            "",
            "\t/* find the first potentially overlapping VMA */",
            "\tvma = vma_find(&vmi, end);",
            "\tif (!vma) {",
            "\t\tstatic int limit;",
            "\t\tif (limit < 5) {",
            "\t\t\tpr_warn(\"munmap of memory not mmapped by process %d (%s): 0x%lx-0x%lx\\n\",",
            "\t\t\t\t\tcurrent->pid, current->comm,",
            "\t\t\t\t\tstart, start + len - 1);",
            "\t\t\tlimit++;",
            "\t\t}",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\t/* we're allowed to split an anonymous VMA but not a file-backed one */",
            "\tif (vma->vm_file) {",
            "\t\tdo {",
            "\t\t\tif (start > vma->vm_start)",
            "\t\t\t\treturn -EINVAL;",
            "\t\t\tif (end == vma->vm_end)",
            "\t\t\t\tgoto erase_whole_vma;",
            "\t\t\tvma = vma_find(&vmi, end);",
            "\t\t} while (vma);",
            "\t\treturn -EINVAL;",
            "\t} else {",
            "\t\t/* the chunk must be a subset of the VMA found */",
            "\t\tif (start == vma->vm_start && end == vma->vm_end)",
            "\t\t\tgoto erase_whole_vma;",
            "\t\tif (start < vma->vm_start || end > vma->vm_end)",
            "\t\t\treturn -EINVAL;",
            "\t\tif (offset_in_page(start))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (end != vma->vm_end && offset_in_page(end))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (start != vma->vm_start && end != vma->vm_end) {",
            "\t\t\tret = split_vma(&vmi, vma, start, 1);",
            "\t\t\tif (ret < 0)",
            "\t\t\t\treturn ret;",
            "\t\t}",
            "\t\treturn vmi_shrink_vma(&vmi, vma, start, end);",
            "\t}",
            "",
            "erase_whole_vma:",
            "\tif (delete_vma_from_mm(vma))",
            "\t\tret = -ENOMEM;",
            "\telse",
            "\t\tdelete_vma(mm, vma);",
            "\treturn ret;",
            "}",
            "int vm_munmap(unsigned long addr, size_t len)",
            "{",
            "\tstruct mm_struct *mm = current->mm;",
            "\tint ret;",
            "",
            "\tmmap_write_lock(mm);",
            "\tret = do_munmap(mm, addr, len, NULL);",
            "\tmmap_write_unlock(mm);",
            "\treturn ret;",
            "}",
            "void exit_mmap(struct mm_struct *mm)",
            "{",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "\tstruct vm_area_struct *vma;",
            "",
            "\tif (!mm)",
            "\t\treturn;",
            "",
            "\tmm->total_vm = 0;",
            "",
            "\t/*",
            "\t * Lock the mm to avoid assert complaining even though this is the only",
            "\t * user of the mm",
            "\t */",
            "\tmmap_write_lock(mm);",
            "\tfor_each_vma(vmi, vma) {",
            "\t\tcleanup_vma_from_mm(vma);",
            "\t\tdelete_vma(mm, vma);",
            "\t\tcond_resched();",
            "\t}",
            "\t__mt_destroy(&mm->mm_mt);",
            "\tmmap_write_unlock(mm);",
            "}",
            "int vm_brk(unsigned long addr, unsigned long len)",
            "{",
            "\treturn -ENOMEM;",
            "}",
            "static unsigned long do_mremap(unsigned long addr,",
            "\t\t\tunsigned long old_len, unsigned long new_len,",
            "\t\t\tunsigned long flags, unsigned long new_addr)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "",
            "\t/* insanity checks first */",
            "\told_len = PAGE_ALIGN(old_len);",
            "\tnew_len = PAGE_ALIGN(new_len);",
            "\tif (old_len == 0 || new_len == 0)",
            "\t\treturn (unsigned long) -EINVAL;",
            "",
            "\tif (offset_in_page(addr))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (flags & MREMAP_FIXED && new_addr != addr)",
            "\t\treturn (unsigned long) -EINVAL;",
            "",
            "\tvma = find_vma_exact(current->mm, addr, old_len);",
            "\tif (!vma)",
            "\t\treturn (unsigned long) -EINVAL;",
            "",
            "\tif (vma->vm_end != vma->vm_start + old_len)",
            "\t\treturn (unsigned long) -EFAULT;",
            "",
            "\tif (is_nommu_shared_mapping(vma->vm_flags))",
            "\t\treturn (unsigned long) -EPERM;",
            "",
            "\tif (new_len > vma->vm_region->vm_end - vma->vm_region->vm_start)",
            "\t\treturn (unsigned long) -ENOMEM;",
            "",
            "\t/* all checks complete - do it */",
            "\tvma->vm_end = vma->vm_start + new_len;",
            "\treturn vma->vm_start;",
            "}"
          ],
          "function_name": "do_munmap, vm_munmap, exit_mmap, vm_brk, do_mremap",
          "description": "该代码块实现了NOMMU环境下虚拟内存区域（VMA）的管理和操作。do_munmap处理munmap系统调用，通过VMA迭代器定位并裁剪/删除指定范围的内存映射；vm_munmap是其公共接口封装，exit_mmap负责进程退出时的VMA清理，而do_mremap用于调整现有VMA大小。其中vm_brk未实现出错返回，表明NOMMU环境中brk操作可能受限。",
          "similarity": 0.4134856164455414
        },
        {
          "chunk_id": 9,
          "file_path": "mm/nommu.c",
          "start_line": 1791,
          "end_line": 1879,
          "content": [
            "int copy_remote_vm_str(struct task_struct *tsk, unsigned long addr,",
            "\t\t       void *buf, int len, unsigned int gup_flags)",
            "{",
            "\tstruct mm_struct *mm;",
            "\tint ret;",
            "",
            "\tif (unlikely(len == 0))",
            "\t\treturn 0;",
            "",
            "\tmm = get_task_mm(tsk);",
            "\tif (!mm) {",
            "\t\t*(char *)buf = '\\0';",
            "\t\treturn -EFAULT;",
            "\t}",
            "",
            "\tret = __copy_remote_vm_str(mm, addr, buf, len);",
            "",
            "\tmmput(mm);",
            "",
            "\treturn ret;",
            "}",
            "int nommu_shrink_inode_mappings(struct inode *inode, size_t size,",
            "\t\t\t\tsize_t newsize)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tstruct vm_region *region;",
            "\tpgoff_t low, high;",
            "\tsize_t r_size, r_top;",
            "",
            "\tlow = newsize >> PAGE_SHIFT;",
            "\thigh = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;",
            "",
            "\tdown_write(&nommu_region_sem);",
            "\ti_mmap_lock_read(inode->i_mapping);",
            "",
            "\t/* search for VMAs that fall within the dead zone */",
            "\tvma_interval_tree_foreach(vma, &inode->i_mapping->i_mmap, low, high) {",
            "\t\t/* found one - only interested if it's shared out of the page",
            "\t\t * cache */",
            "\t\tif (vma->vm_flags & VM_SHARED) {",
            "\t\t\ti_mmap_unlock_read(inode->i_mapping);",
            "\t\t\tup_write(&nommu_region_sem);",
            "\t\t\treturn -ETXTBSY; /* not quite true, but near enough */",
            "\t\t}",
            "\t}",
            "",
            "\t/* reduce any regions that overlap the dead zone - if in existence,",
            "\t * these will be pointed to by VMAs that don't overlap the dead zone",
            "\t *",
            "\t * we don't check for any regions that start beyond the EOF as there",
            "\t * shouldn't be any",
            "\t */",
            "\tvma_interval_tree_foreach(vma, &inode->i_mapping->i_mmap, 0, ULONG_MAX) {",
            "\t\tif (!(vma->vm_flags & VM_SHARED))",
            "\t\t\tcontinue;",
            "",
            "\t\tregion = vma->vm_region;",
            "\t\tr_size = region->vm_top - region->vm_start;",
            "\t\tr_top = (region->vm_pgoff << PAGE_SHIFT) + r_size;",
            "",
            "\t\tif (r_top > newsize) {",
            "\t\t\tregion->vm_top -= r_top - newsize;",
            "\t\t\tif (region->vm_end > region->vm_top)",
            "\t\t\t\tregion->vm_end = region->vm_top;",
            "\t\t}",
            "\t}",
            "",
            "\ti_mmap_unlock_read(inode->i_mapping);",
            "\tup_write(&nommu_region_sem);",
            "\treturn 0;",
            "}",
            "static int __meminit init_user_reserve(void)",
            "{",
            "\tunsigned long free_kbytes;",
            "",
            "\tfree_kbytes = K(global_zone_page_state(NR_FREE_PAGES));",
            "",
            "\tsysctl_user_reserve_kbytes = min(free_kbytes / 32, 1UL << 17);",
            "\treturn 0;",
            "}",
            "static int __meminit init_admin_reserve(void)",
            "{",
            "\tunsigned long free_kbytes;",
            "",
            "\tfree_kbytes = K(global_zone_page_state(NR_FREE_PAGES));",
            "",
            "\tsysctl_admin_reserve_kbytes = min(free_kbytes / 32, 1UL << 13);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "copy_remote_vm_str, nommu_shrink_inode_mappings, init_user_reserve, init_admin_reserve",
          "description": "该代码段实现了NOMMU环境下的虚拟内存管理辅助功能。  \n`copy_remote_vm_str`通过获取目标进程的mm_struct完成远程字符串拷贝，`nommu_shrink_inode_mappings`遍历并裁剪文件映射区域以适配新大小，`init_user_reserve`与`init_admin_reserve`分别初始化用户级和管理员级内存保留阈值。  \n所有API均基于内核现有机制，未引入虚构组件。",
          "similarity": 0.40274471044540405
        },
        {
          "chunk_id": 5,
          "file_path": "mm/nommu.c",
          "start_line": 1023,
          "end_line": 1258,
          "content": [
            "unsigned long do_mmap(struct file *file,",
            "\t\t\tunsigned long addr,",
            "\t\t\tunsigned long len,",
            "\t\t\tunsigned long prot,",
            "\t\t\tunsigned long flags,",
            "\t\t\tvm_flags_t vm_flags,",
            "\t\t\tunsigned long pgoff,",
            "\t\t\tunsigned long *populate,",
            "\t\t\tstruct list_head *uf)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tstruct vm_region *region;",
            "\tstruct rb_node *rb;",
            "\tunsigned long capabilities, result;",
            "\tint ret;",
            "\tVMA_ITERATOR(vmi, current->mm, 0);",
            "",
            "\t*populate = 0;",
            "",
            "\t/* decide whether we should attempt the mapping, and if so what sort of",
            "\t * mapping */",
            "\tret = validate_mmap_request(file, addr, len, prot, flags, pgoff,",
            "\t\t\t\t    &capabilities);",
            "\tif (ret < 0)",
            "\t\treturn ret;",
            "",
            "\t/* we ignore the address hint */",
            "\taddr = 0;",
            "\tlen = PAGE_ALIGN(len);",
            "",
            "\t/* we've determined that we can make the mapping, now translate what we",
            "\t * now know into VMA flags */",
            "\tvm_flags |= determine_vm_flags(file, prot, flags, capabilities);",
            "",
            "",
            "\t/* we're going to need to record the mapping */",
            "\tregion = kmem_cache_zalloc(vm_region_jar, GFP_KERNEL);",
            "\tif (!region)",
            "\t\tgoto error_getting_region;",
            "",
            "\tvma = vm_area_alloc(current->mm);",
            "\tif (!vma)",
            "\t\tgoto error_getting_vma;",
            "",
            "\tregion->vm_usage = 1;",
            "\tregion->vm_flags = vm_flags;",
            "\tregion->vm_pgoff = pgoff;",
            "",
            "\tvm_flags_init(vma, vm_flags);",
            "\tvma->vm_pgoff = pgoff;",
            "",
            "\tif (file) {",
            "\t\tregion->vm_file = get_file(file);",
            "\t\tvma->vm_file = get_file(file);",
            "\t}",
            "",
            "\tdown_write(&nommu_region_sem);",
            "",
            "\t/* if we want to share, we need to check for regions created by other",
            "\t * mmap() calls that overlap with our proposed mapping",
            "\t * - we can only share with a superset match on most regular files",
            "\t * - shared mappings on character devices and memory backed files are",
            "\t *   permitted to overlap inexactly as far as we are concerned for in",
            "\t *   these cases, sharing is handled in the driver or filesystem rather",
            "\t *   than here",
            "\t */",
            "\tif (is_nommu_shared_mapping(vm_flags)) {",
            "\t\tstruct vm_region *pregion;",
            "\t\tunsigned long pglen, rpglen, pgend, rpgend, start;",
            "",
            "\t\tpglen = (len + PAGE_SIZE - 1) >> PAGE_SHIFT;",
            "\t\tpgend = pgoff + pglen;",
            "",
            "\t\tfor (rb = rb_first(&nommu_region_tree); rb; rb = rb_next(rb)) {",
            "\t\t\tpregion = rb_entry(rb, struct vm_region, vm_rb);",
            "",
            "\t\t\tif (!is_nommu_shared_mapping(pregion->vm_flags))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\t/* search for overlapping mappings on the same file */",
            "\t\t\tif (file_inode(pregion->vm_file) !=",
            "\t\t\t    file_inode(file))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tif (pregion->vm_pgoff >= pgend)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\trpglen = pregion->vm_end - pregion->vm_start;",
            "\t\t\trpglen = (rpglen + PAGE_SIZE - 1) >> PAGE_SHIFT;",
            "\t\t\trpgend = pregion->vm_pgoff + rpglen;",
            "\t\t\tif (pgoff >= rpgend)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\t/* handle inexactly overlapping matches between",
            "\t\t\t * mappings */",
            "\t\t\tif ((pregion->vm_pgoff != pgoff || rpglen != pglen) &&",
            "\t\t\t    !(pgoff >= pregion->vm_pgoff && pgend <= rpgend)) {",
            "\t\t\t\t/* new mapping is not a subset of the region */",
            "\t\t\t\tif (!(capabilities & NOMMU_MAP_DIRECT))",
            "\t\t\t\t\tgoto sharing_violation;",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\t/* we've found a region we can share */",
            "\t\t\tpregion->vm_usage++;",
            "\t\t\tvma->vm_region = pregion;",
            "\t\t\tstart = pregion->vm_start;",
            "\t\t\tstart += (pgoff - pregion->vm_pgoff) << PAGE_SHIFT;",
            "\t\t\tvma->vm_start = start;",
            "\t\t\tvma->vm_end = start + len;",
            "",
            "\t\t\tif (pregion->vm_flags & VM_MAPPED_COPY)",
            "\t\t\t\tvm_flags_set(vma, VM_MAPPED_COPY);",
            "\t\t\telse {",
            "\t\t\t\tret = do_mmap_shared_file(vma);",
            "\t\t\t\tif (ret < 0) {",
            "\t\t\t\t\tvma->vm_region = NULL;",
            "\t\t\t\t\tvma->vm_start = 0;",
            "\t\t\t\t\tvma->vm_end = 0;",
            "\t\t\t\t\tpregion->vm_usage--;",
            "\t\t\t\t\tpregion = NULL;",
            "\t\t\t\t\tgoto error_just_free;",
            "\t\t\t\t}",
            "\t\t\t}",
            "\t\t\tfput(region->vm_file);",
            "\t\t\tkmem_cache_free(vm_region_jar, region);",
            "\t\t\tregion = pregion;",
            "\t\t\tresult = start;",
            "\t\t\tgoto share;",
            "\t\t}",
            "",
            "\t\t/* obtain the address at which to make a shared mapping",
            "\t\t * - this is the hook for quasi-memory character devices to",
            "\t\t *   tell us the location of a shared mapping",
            "\t\t */",
            "\t\tif (capabilities & NOMMU_MAP_DIRECT) {",
            "\t\t\taddr = file->f_op->get_unmapped_area(file, addr, len,",
            "\t\t\t\t\t\t\t     pgoff, flags);",
            "\t\t\tif (IS_ERR_VALUE(addr)) {",
            "\t\t\t\tret = addr;",
            "\t\t\t\tif (ret != -ENOSYS)",
            "\t\t\t\t\tgoto error_just_free;",
            "",
            "\t\t\t\t/* the driver refused to tell us where to site",
            "\t\t\t\t * the mapping so we'll have to attempt to copy",
            "\t\t\t\t * it */",
            "\t\t\t\tret = -ENODEV;",
            "\t\t\t\tif (!(capabilities & NOMMU_MAP_COPY))",
            "\t\t\t\t\tgoto error_just_free;",
            "",
            "\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;",
            "\t\t\t} else {",
            "\t\t\t\tvma->vm_start = region->vm_start = addr;",
            "\t\t\t\tvma->vm_end = region->vm_end = addr + len;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\tvma->vm_region = region;",
            "",
            "\t/* set up the mapping",
            "\t * - the region is filled in if NOMMU_MAP_DIRECT is still set",
            "\t */",
            "\tif (file && vma->vm_flags & VM_SHARED)",
            "\t\tret = do_mmap_shared_file(vma);",
            "\telse",
            "\t\tret = do_mmap_private(vma, region, len, capabilities);",
            "\tif (ret < 0)",
            "\t\tgoto error_just_free;",
            "\tadd_nommu_region(region);",
            "",
            "\t/* clear anonymous mappings that don't ask for uninitialized data */",
            "\tif (!vma->vm_file &&",
            "\t    (!IS_ENABLED(CONFIG_MMAP_ALLOW_UNINITIALIZED) ||",
            "\t     !(flags & MAP_UNINITIALIZED)))",
            "\t\tmemset((void *)region->vm_start, 0,",
            "\t\t       region->vm_end - region->vm_start);",
            "",
            "\t/* okay... we have a mapping; now we have to register it */",
            "\tresult = vma->vm_start;",
            "",
            "\tcurrent->mm->total_vm += len >> PAGE_SHIFT;",
            "",
            "share:",
            "\tBUG_ON(!vma->vm_region);",
            "\tvma_iter_config(&vmi, vma->vm_start, vma->vm_end);",
            "\tif (vma_iter_prealloc(&vmi, vma))",
            "\t\tgoto error_just_free;",
            "",
            "\tsetup_vma_to_mm(vma, current->mm);",
            "\tcurrent->mm->map_count++;",
            "\t/* add the VMA to the tree */",
            "\tvma_iter_store(&vmi, vma);",
            "",
            "\t/* we flush the region from the icache only when the first executable",
            "\t * mapping of it is made  */",
            "\tif (vma->vm_flags & VM_EXEC && !region->vm_icache_flushed) {",
            "\t\tflush_icache_user_range(region->vm_start, region->vm_end);",
            "\t\tregion->vm_icache_flushed = true;",
            "\t}",
            "",
            "\tup_write(&nommu_region_sem);",
            "",
            "\treturn result;",
            "",
            "error_just_free:",
            "\tup_write(&nommu_region_sem);",
            "error:",
            "\tvma_iter_free(&vmi);",
            "\tif (region->vm_file)",
            "\t\tfput(region->vm_file);",
            "\tkmem_cache_free(vm_region_jar, region);",
            "\tif (vma->vm_file)",
            "\t\tfput(vma->vm_file);",
            "\tvm_area_free(vma);",
            "\treturn ret;",
            "",
            "sharing_violation:",
            "\tup_write(&nommu_region_sem);",
            "\tpr_warn(\"Attempt to share mismatched mappings\\n\");",
            "\tret = -EINVAL;",
            "\tgoto error;",
            "",
            "error_getting_vma:",
            "\tkmem_cache_free(vm_region_jar, region);",
            "\tpr_warn(\"Allocation of vma for %lu byte allocation from process %d failed\\n\",",
            "\t\t\tlen, current->pid);",
            "\tshow_mem();",
            "\treturn -ENOMEM;",
            "",
            "error_getting_region:",
            "\tpr_warn(\"Allocation of vm region for %lu byte allocation from process %d failed\\n\",",
            "\t\t\tlen, current->pid);",
            "\tshow_mem();",
            "\treturn -ENOMEM;",
            "}"
          ],
          "function_name": "do_mmap",
          "description": "该代码段实现了非MMU环境下`do_mmap`函数的核心逻辑，负责验证并创建共享/私有内存映射。其行为包括：通过RB树检测共享映射冲突，分配`vm_region`和`vma`结构体，并根据文件特性决定直接映射或复制操作。函数最终将新VMA插入进程地址空间树并更新相关统计信息。",
          "similarity": 0.3952057957649231
        }
      ]
    },
    {
      "source_file": "mm/migrate_device.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:47:38\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `migrate_device.c`\n\n---\n\n# `migrate_device.c` 技术文档\n\n## 1. 文件概述\n\n`migrate_device.c` 实现了 Linux 内核中设备内存（Device Memory）的迁移功能，主要用于支持异构内存架构（如 GPU、持久内存等）下的页面迁移机制。该文件提供了从用户虚拟地址空间收集可迁移页面的核心逻辑，是 `migrate_vma_*()` 系列 API 的关键组成部分，允许将设备私有内存（device private memory）或系统内存在不同内存域之间迁移。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`migrate_vma_collect_skip()`**  \n  跳过指定虚拟地址范围内的页面，将 `src` 和 `dst` 数组对应项置零，表示不参与迁移。\n\n- **`migrate_vma_collect_hole()`**  \n  处理页表中的“空洞”（即未映射区域），仅对匿名 VMA 允许填充可迁移标记（`MIGRATE_PFN_MIGRATE`），用于后续分配新页。\n\n- **`migrate_vma_collect_pmd()`**  \n  核心函数，遍历 PMD 页表项，识别并处理以下类型的页面：\n  - 普通系统内存页\n  - 设备私有页（`device private`）\n  - 设备一致页（`device coherent`）\n  - 零页（zero page）\n  - 匿名页中的空缺（holes）\n  \n  对符合条件的页面：\n  - 获取页引用并尝试加锁\n  - 替换 PTE 为迁移专用的 swap entry（migration entry）\n  - 更新 `migrate->src` 数组记录源页信息\n  - 维护脏位、年轻位、UFFD_WP 等元数据\n\n- **`migrate_vma_collect()`**  \n  启动页表遍历流程，使用 `mm_walk` 框架调用上述回调函数，完成整个 VMA 范围内可迁移页面的收集。\n\n### 关键数据结构\n\n- **`struct migrate_vma`**  \n  迁移上下文结构体，包含：\n  - `src[]` / `dst[]`：源页和目标页的 PFN 描述数组\n  - `npages`：已处理页面数\n  - `cpages`：候选迁移页面数\n  - `flags`：迁移选项（如 `MIGRATE_VMA_SELECT_SYSTEM`、`MIGRATE_VMA_SELECT_DEVICE_PRIVATE` 等）\n  - `pgmap_owner`：指定设备内存所属的驱动模块（用于权限过滤）\n\n- **`migrate_vma_walk_ops`**  \n  定义 `mm_walk` 回调操作集，指定 `.pmd_entry` 和 `.pte_hole` 处理函数，并使用读锁（`PGWALK_RDLOCK`）进行页表遍历。\n\n## 3. 关键实现\n\n### 页面筛选策略\n- **设备私有页**：仅当 `MIGRATE_VMA_SELECT_DEVICE_PRIVATE` 标志置位且 `pgmap->owner` 匹配时才纳入迁移。\n- **设备一致页**：需 `MIGRATE_VMA_SELECT_DEVICE_COHERENT` 标志及 owner 匹配。\n- **系统页**：通过 `MIGRATE_VMA_SELECT_SYSTEM` 控制是否包含普通内存页。\n- **零页**：仅在选择系统页时允许迁移（实际表现为占位符）。\n\n### 迁移入口设置\n- 成功锁定页面后，原 PTE 被替换为 **migration entry**（一种特殊的 swap entry）：\n  - 可写页 → `make_writable_migration_entry()`\n  - 匿名独占页 → `make_readable_exclusive_migration_entry()`\n  - 其他 → `make_readable_migration_entry()`\n- 同时保留原始 PTE 的 `young`、`dirty`、`soft_dirty`、`uffd_wp` 等属性到 migration entry 中。\n\n### 并发与锁机制\n- 使用 `folio_trylock()` 避免迁移死锁（失败则跳过该页，体现“尽力而为”语义）。\n- 通过 `folio_get()` 引用计数防止页面在迁移过程中被释放。\n- 在设置 migration entry 前调用 `flush_cache_page()` 和 `ptep_clear_flush()` 保证缓存一致性。\n\n### TLB 管理\n- 仅当实际修改了 PTE（即 `unmapped > 0`）时才调用 `flush_tlb_range()`，减少不必要的 TLB 刷新开销。\n\n### 特殊限制\n- 显式跳过透明大页（THP）：`PageTransCompound(page)` 返回 true 时放弃迁移（注释标明 “FIXME support THP”）。\n- 非 `vm_normal_page()` 或无 `mapping` 的页面不参与迁移。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/migrate.h>`：提供迁移框架定义\n  - `<linux/mm.h>` / `<linux/mm_inline.h>`：核心内存管理接口\n  - `<linux/rmap.h>`：反向映射操作（`folio_remove_rmap_pte`）\n  - `<linux/swapops.h>`：migration entry 构造函数\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`is_zone_device_page()`、`pgmap` 结构\n- **体系结构相关**：\n  - `<asm/tlbflush.h>`：TLB 刷新接口\n  - `arch_enter/leave_lazy_mmu_mode()`：批量 PTE 更新优化\n- **内部头文件**：\n  - `\"internal.h\"`：内核 MM 内部辅助函数\n\n## 5. 使用场景\n\n- **异构计算内存迁移**：GPU 驱动（如 NVIDIA HMM、AMD SVM）利用此机制将设备内存页迁移到主机内存，以响应 CPU 访问或内存压力。\n- **用户态驱动协作**：通过 `migrate_vma_pages()` 系统调用（基于本文件收集逻辑），用户程序可主动触发设备内存迁移。\n- **内存热插拔/故障恢复**：在设备内存失效时，将有效数据迁出至系统内存。\n- **统一虚拟内存（UVM）实现**：为 CPU/GPU 共享虚拟地址空间提供底层迁移支持。\n\n> 注：该文件仅为迁移流程的“收集阶段”，完整的迁移还需配合 `migrate_vma_pages()`（分配目标页）、`migrate_vma_finalize()`（提交或回滚）等后续步骤。",
      "similarity": 0.44738513231277466,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/migrate_device.c",
          "start_line": 1,
          "end_line": 19,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Device Memory Migration functionality.",
            " *",
            " * Originally written by Jérôme Glisse.",
            " */",
            "#include <linux/export.h>",
            "#include <linux/memremap.h>",
            "#include <linux/migrate.h>",
            "#include <linux/mm.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/oom.h>",
            "#include <linux/pagewalk.h>",
            "#include <linux/rmap.h>",
            "#include <linux/swapops.h>",
            "#include <asm/tlbflush.h>",
            "#include \"internal.h\"",
            ""
          ],
          "function_name": null,
          "description": "声明设备内存迁移模块的头文件和依赖项，提供必要的内核接口支持。",
          "similarity": 0.45260411500930786
        },
        {
          "chunk_id": 5,
          "file_path": "mm/migrate_device.c",
          "start_line": 869,
          "end_line": 932,
          "content": [
            "void migrate_vma_finalize(struct migrate_vma *migrate)",
            "{",
            "\tmigrate_device_finalize(migrate->src, migrate->dst, migrate->npages);",
            "}",
            "int migrate_device_range(unsigned long *src_pfns, unsigned long start,",
            "\t\t\tunsigned long npages)",
            "{",
            "\tunsigned long i, pfn;",
            "",
            "\tfor (pfn = start, i = 0; i < npages; pfn++, i++) {",
            "\t\tstruct page *page = pfn_to_page(pfn);",
            "",
            "\t\tif (!get_page_unless_zero(page)) {",
            "\t\t\tsrc_pfns[i] = 0;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tif (!trylock_page(page)) {",
            "\t\t\tsrc_pfns[i] = 0;",
            "\t\t\tput_page(page);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tsrc_pfns[i] = migrate_pfn(pfn) | MIGRATE_PFN_MIGRATE;",
            "\t}",
            "",
            "\tmigrate_device_unmap(src_pfns, npages, NULL);",
            "",
            "\treturn 0;",
            "}",
            "int migrate_device_coherent_page(struct page *page)",
            "{",
            "\tunsigned long src_pfn, dst_pfn = 0;",
            "\tstruct page *dpage;",
            "",
            "\tWARN_ON_ONCE(PageCompound(page));",
            "",
            "\tlock_page(page);",
            "\tsrc_pfn = migrate_pfn(page_to_pfn(page)) | MIGRATE_PFN_MIGRATE;",
            "",
            "\t/*",
            "\t * We don't have a VMA and don't need to walk the page tables to find",
            "\t * the source page. So call migrate_vma_unmap() directly to unmap the",
            "\t * page as migrate_vma_setup() will fail if args.vma == NULL.",
            "\t */",
            "\tmigrate_device_unmap(&src_pfn, 1, NULL);",
            "\tif (!(src_pfn & MIGRATE_PFN_MIGRATE))",
            "\t\treturn -EBUSY;",
            "",
            "\tdpage = alloc_page(GFP_USER | __GFP_NOWARN);",
            "\tif (dpage) {",
            "\t\tlock_page(dpage);",
            "\t\tdst_pfn = migrate_pfn(page_to_pfn(dpage));",
            "\t}",
            "",
            "\tmigrate_device_pages(&src_pfn, &dst_pfn, 1);",
            "\tif (src_pfn & MIGRATE_PFN_MIGRATE)",
            "\t\tcopy_highpage(dpage, page);",
            "\tmigrate_device_finalize(&src_pfn, &dst_pfn, 1);",
            "",
            "\tif (src_pfn & MIGRATE_PFN_MIGRATE)",
            "\t\treturn 0;",
            "\treturn -EBUSY;",
            "}"
          ],
          "function_name": "migrate_vma_finalize, migrate_device_range, migrate_device_coherent_page",
          "description": "该代码段实现了设备间内存页面的迁移逻辑，包含三个关键函数：  \n1. `migrate_vma_finalize` 负责迁移VMA（虚拟内存区域）的收尾操作，调用底层迁移接口完成资源清理；  \n2. `migrate_device_range` 遍历指定地址范围的页面，将其标记为可迁移格式并收集至输出数组，随后解映射以触发迁移；  \n3. `migrate_device_coherent_page` 处理一致性内存页面的迁移，通过直接内存拷贝与设备PTE更新实现跨设备的数据迁移。",
          "similarity": 0.426846444606781
        },
        {
          "chunk_id": 2,
          "file_path": "mm/migrate_device.c",
          "start_line": 296,
          "end_line": 422,
          "content": [
            "static void migrate_vma_collect(struct migrate_vma *migrate)",
            "{",
            "\tstruct mmu_notifier_range range;",
            "",
            "\t/*",
            "\t * Note that the pgmap_owner is passed to the mmu notifier callback so",
            "\t * that the registered device driver can skip invalidating device",
            "\t * private page mappings that won't be migrated.",
            "\t */",
            "\tmmu_notifier_range_init_owner(&range, MMU_NOTIFY_MIGRATE, 0,",
            "\t\tmigrate->vma->vm_mm, migrate->start, migrate->end,",
            "\t\tmigrate->pgmap_owner);",
            "\tmmu_notifier_invalidate_range_start(&range);",
            "",
            "\twalk_page_range(migrate->vma->vm_mm, migrate->start, migrate->end,",
            "\t\t\t&migrate_vma_walk_ops, migrate);",
            "",
            "\tmmu_notifier_invalidate_range_end(&range);",
            "\tmigrate->end = migrate->start + (migrate->npages << PAGE_SHIFT);",
            "}",
            "static bool migrate_vma_check_page(struct page *page, struct page *fault_page)",
            "{",
            "\tstruct folio *folio = page_folio(page);",
            "",
            "\t/*",
            "\t * One extra ref because caller holds an extra reference, either from",
            "\t * isolate_lru_page() for a regular page, or migrate_vma_collect() for",
            "\t * a device page.",
            "\t */",
            "\tint extra = 1 + (page == fault_page);",
            "",
            "\t/*",
            "\t * FIXME support THP (transparent huge page), it is bit more complex to",
            "\t * check them than regular pages, because they can be mapped with a pmd",
            "\t * or with a pte (split pte mapping).",
            "\t */",
            "\tif (folio_test_large(folio))",
            "\t\treturn false;",
            "",
            "\t/* Page from ZONE_DEVICE have one extra reference */",
            "\tif (folio_is_zone_device(folio))",
            "\t\textra++;",
            "",
            "\t/* For file back page */",
            "\tif (folio_mapping(folio))",
            "\t\textra += 1 + folio_has_private(folio);",
            "",
            "\tif ((folio_ref_count(folio) - extra) > folio_mapcount(folio))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static unsigned long migrate_device_unmap(unsigned long *src_pfns,",
            "\t\t\t\t\t  unsigned long npages,",
            "\t\t\t\t\t  struct page *fault_page)",
            "{",
            "\tunsigned long i, restore = 0;",
            "\tbool allow_drain = true;",
            "\tunsigned long unmapped = 0;",
            "",
            "\tlru_add_drain();",
            "",
            "\tfor (i = 0; i < npages; i++) {",
            "\t\tstruct page *page = migrate_pfn_to_page(src_pfns[i]);",
            "\t\tstruct folio *folio;",
            "",
            "\t\tif (!page) {",
            "\t\t\tif (src_pfns[i] & MIGRATE_PFN_MIGRATE)",
            "\t\t\t\tunmapped++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/* ZONE_DEVICE pages are not on LRU */",
            "\t\tif (!is_zone_device_page(page)) {",
            "\t\t\tif (!PageLRU(page) && allow_drain) {",
            "\t\t\t\t/* Drain CPU's lru cache */",
            "\t\t\t\tlru_add_drain_all();",
            "\t\t\t\tallow_drain = false;",
            "\t\t\t}",
            "",
            "\t\t\tif (!isolate_lru_page(page)) {",
            "\t\t\t\tsrc_pfns[i] &= ~MIGRATE_PFN_MIGRATE;",
            "\t\t\t\trestore++;",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\t/* Drop the reference we took in collect */",
            "\t\t\tput_page(page);",
            "\t\t}",
            "",
            "\t\tfolio = page_folio(page);",
            "\t\tif (folio_mapped(folio))",
            "\t\t\ttry_to_migrate(folio, 0);",
            "",
            "\t\tif (page_mapped(page) ||",
            "\t\t    !migrate_vma_check_page(page, fault_page)) {",
            "\t\t\tif (!is_zone_device_page(page)) {",
            "\t\t\t\tget_page(page);",
            "\t\t\t\tputback_lru_page(page);",
            "\t\t\t}",
            "",
            "\t\t\tsrc_pfns[i] &= ~MIGRATE_PFN_MIGRATE;",
            "\t\t\trestore++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tunmapped++;",
            "\t}",
            "",
            "\tfor (i = 0; i < npages && restore; i++) {",
            "\t\tstruct page *page = migrate_pfn_to_page(src_pfns[i]);",
            "\t\tstruct folio *folio;",
            "",
            "\t\tif (!page || (src_pfns[i] & MIGRATE_PFN_MIGRATE))",
            "\t\t\tcontinue;",
            "",
            "\t\tfolio = page_folio(page);",
            "\t\tremove_migration_ptes(folio, folio, false);",
            "",
            "\t\tsrc_pfns[i] = 0;",
            "\t\tfolio_unlock(folio);",
            "\t\tfolio_put(folio);",
            "\t\trestore--;",
            "\t}",
            "",
            "\treturn unmapped;",
            "}"
          ],
          "function_name": "migrate_vma_collect, migrate_vma_check_page, migrate_device_unmap",
          "description": "收集迁移页信息并校验页面有效性，解除映射并处理页面回收，维护迁移过程中的引用计数与LRU缓存交互。",
          "similarity": 0.4183807671070099
        },
        {
          "chunk_id": 1,
          "file_path": "mm/migrate_device.c",
          "start_line": 20,
          "end_line": 278,
          "content": [
            "static int migrate_vma_collect_skip(unsigned long start,",
            "\t\t\t\t    unsigned long end,",
            "\t\t\t\t    struct mm_walk *walk)",
            "{",
            "\tstruct migrate_vma *migrate = walk->private;",
            "\tunsigned long addr;",
            "",
            "\tfor (addr = start; addr < end; addr += PAGE_SIZE) {",
            "\t\tmigrate->dst[migrate->npages] = 0;",
            "\t\tmigrate->src[migrate->npages++] = 0;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int migrate_vma_collect_hole(unsigned long start,",
            "\t\t\t\t    unsigned long end,",
            "\t\t\t\t    __always_unused int depth,",
            "\t\t\t\t    struct mm_walk *walk)",
            "{",
            "\tstruct migrate_vma *migrate = walk->private;",
            "\tunsigned long addr;",
            "",
            "\t/* Only allow populating anonymous memory. */",
            "\tif (!vma_is_anonymous(walk->vma))",
            "\t\treturn migrate_vma_collect_skip(start, end, walk);",
            "",
            "\tfor (addr = start; addr < end; addr += PAGE_SIZE) {",
            "\t\tmigrate->src[migrate->npages] = MIGRATE_PFN_MIGRATE;",
            "\t\tmigrate->dst[migrate->npages] = 0;",
            "\t\tmigrate->npages++;",
            "\t\tmigrate->cpages++;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int migrate_vma_collect_pmd(pmd_t *pmdp,",
            "\t\t\t\t   unsigned long start,",
            "\t\t\t\t   unsigned long end,",
            "\t\t\t\t   struct mm_walk *walk)",
            "{",
            "\tstruct migrate_vma *migrate = walk->private;",
            "\tstruct vm_area_struct *vma = walk->vma;",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tunsigned long addr = start, unmapped = 0;",
            "\tspinlock_t *ptl;",
            "\tpte_t *ptep;",
            "",
            "again:",
            "\tif (pmd_none(*pmdp))",
            "\t\treturn migrate_vma_collect_hole(start, end, -1, walk);",
            "",
            "\tif (pmd_trans_huge(*pmdp)) {",
            "\t\tstruct folio *folio;",
            "",
            "\t\tptl = pmd_lock(mm, pmdp);",
            "\t\tif (unlikely(!pmd_trans_huge(*pmdp))) {",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\tgoto again;",
            "\t\t}",
            "",
            "\t\tfolio = pmd_folio(*pmdp);",
            "\t\tif (is_huge_zero_folio(folio)) {",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\tsplit_huge_pmd(vma, pmdp, addr);",
            "\t\t} else {",
            "\t\t\tint ret;",
            "",
            "\t\t\tfolio_get(folio);",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\tif (unlikely(!folio_trylock(folio)))",
            "\t\t\t\treturn migrate_vma_collect_skip(start, end,",
            "\t\t\t\t\t\t\t\twalk);",
            "\t\t\tret = split_folio(folio);",
            "\t\t\tfolio_unlock(folio);",
            "\t\t\tfolio_put(folio);",
            "\t\t\tif (ret)",
            "\t\t\t\treturn migrate_vma_collect_skip(start, end,",
            "\t\t\t\t\t\t\t\twalk);",
            "\t\t}",
            "\t}",
            "",
            "\tptep = pte_offset_map_lock(mm, pmdp, addr, &ptl);",
            "\tif (!ptep)",
            "\t\tgoto again;",
            "\tarch_enter_lazy_mmu_mode();",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE, ptep++) {",
            "\t\tunsigned long mpfn = 0, pfn;",
            "\t\tstruct folio *folio;",
            "\t\tstruct page *page;",
            "\t\tswp_entry_t entry;",
            "\t\tpte_t pte;",
            "",
            "\t\tpte = ptep_get(ptep);",
            "",
            "\t\tif (pte_none(pte)) {",
            "\t\t\tif (vma_is_anonymous(vma)) {",
            "\t\t\t\tmpfn = MIGRATE_PFN_MIGRATE;",
            "\t\t\t\tmigrate->cpages++;",
            "\t\t\t}",
            "\t\t\tgoto next;",
            "\t\t}",
            "",
            "\t\tif (!pte_present(pte)) {",
            "\t\t\t/*",
            "\t\t\t * Only care about unaddressable device page special",
            "\t\t\t * page table entry. Other special swap entries are not",
            "\t\t\t * migratable, and we ignore regular swapped page.",
            "\t\t\t */",
            "\t\t\tentry = pte_to_swp_entry(pte);",
            "\t\t\tif (!is_device_private_entry(entry))",
            "\t\t\t\tgoto next;",
            "",
            "\t\t\tpage = pfn_swap_entry_to_page(entry);",
            "\t\t\tif (!(migrate->flags &",
            "\t\t\t\tMIGRATE_VMA_SELECT_DEVICE_PRIVATE) ||",
            "\t\t\t    page->pgmap->owner != migrate->pgmap_owner)",
            "\t\t\t\tgoto next;",
            "",
            "\t\t\tmpfn = migrate_pfn(page_to_pfn(page)) |",
            "\t\t\t\t\tMIGRATE_PFN_MIGRATE;",
            "\t\t\tif (is_writable_device_private_entry(entry))",
            "\t\t\t\tmpfn |= MIGRATE_PFN_WRITE;",
            "\t\t} else {",
            "\t\t\tpfn = pte_pfn(pte);",
            "\t\t\tif (is_zero_pfn(pfn) &&",
            "\t\t\t    (migrate->flags & MIGRATE_VMA_SELECT_SYSTEM)) {",
            "\t\t\t\tmpfn = MIGRATE_PFN_MIGRATE;",
            "\t\t\t\tmigrate->cpages++;",
            "\t\t\t\tgoto next;",
            "\t\t\t}",
            "\t\t\tpage = vm_normal_page(migrate->vma, addr, pte);",
            "\t\t\tif (page && !is_zone_device_page(page) &&",
            "\t\t\t    !(migrate->flags & MIGRATE_VMA_SELECT_SYSTEM))",
            "\t\t\t\tgoto next;",
            "\t\t\telse if (page && is_device_coherent_page(page) &&",
            "\t\t\t    (!(migrate->flags & MIGRATE_VMA_SELECT_DEVICE_COHERENT) ||",
            "\t\t\t     page->pgmap->owner != migrate->pgmap_owner))",
            "\t\t\t\tgoto next;",
            "\t\t\tmpfn = migrate_pfn(pfn) | MIGRATE_PFN_MIGRATE;",
            "\t\t\tmpfn |= pte_write(pte) ? MIGRATE_PFN_WRITE : 0;",
            "\t\t}",
            "",
            "\t\t/* FIXME support THP */",
            "\t\tif (!page || !page->mapping || PageTransCompound(page)) {",
            "\t\t\tmpfn = 0;",
            "\t\t\tgoto next;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * By getting a reference on the folio we pin it and that blocks",
            "\t\t * any kind of migration. Side effect is that it \"freezes\" the",
            "\t\t * pte.",
            "\t\t *",
            "\t\t * We drop this reference after isolating the folio from the lru",
            "\t\t * for non device folio (device folio are not on the lru and thus",
            "\t\t * can't be dropped from it).",
            "\t\t */",
            "\t\tfolio = page_folio(page);",
            "\t\tfolio_get(folio);",
            "",
            "\t\t/*",
            "\t\t * We rely on folio_trylock() to avoid deadlock between",
            "\t\t * concurrent migrations where each is waiting on the others",
            "\t\t * folio lock. If we can't immediately lock the folio we fail this",
            "\t\t * migration as it is only best effort anyway.",
            "\t\t *",
            "\t\t * If we can lock the folio it's safe to set up a migration entry",
            "\t\t * now. In the common case where the folio is mapped once in a",
            "\t\t * single process setting up the migration entry now is an",
            "\t\t * optimisation to avoid walking the rmap later with",
            "\t\t * try_to_migrate().",
            "\t\t */",
            "\t\tif (folio_trylock(folio)) {",
            "\t\t\tbool anon_exclusive;",
            "\t\t\tpte_t swp_pte;",
            "",
            "\t\t\tflush_cache_page(vma, addr, pte_pfn(pte));",
            "\t\t\tanon_exclusive = folio_test_anon(folio) &&",
            "\t\t\t\t\t  PageAnonExclusive(page);",
            "\t\t\tif (anon_exclusive) {",
            "\t\t\t\tpte = ptep_clear_flush(vma, addr, ptep);",
            "",
            "\t\t\t\tif (folio_try_share_anon_rmap_pte(folio, page)) {",
            "\t\t\t\t\tset_pte_at(mm, addr, ptep, pte);",
            "\t\t\t\t\tfolio_unlock(folio);",
            "\t\t\t\t\tfolio_put(folio);",
            "\t\t\t\t\tmpfn = 0;",
            "\t\t\t\t\tgoto next;",
            "\t\t\t\t}",
            "\t\t\t} else {",
            "\t\t\t\tpte = ptep_get_and_clear(mm, addr, ptep);",
            "\t\t\t}",
            "",
            "\t\t\tmigrate->cpages++;",
            "",
            "\t\t\t/* Set the dirty flag on the folio now the pte is gone. */",
            "\t\t\tif (pte_dirty(pte))",
            "\t\t\t\tfolio_mark_dirty(folio);",
            "",
            "\t\t\t/* Setup special migration page table entry */",
            "\t\t\tif (mpfn & MIGRATE_PFN_WRITE)",
            "\t\t\t\tentry = make_writable_migration_entry(",
            "\t\t\t\t\t\t\tpage_to_pfn(page));",
            "\t\t\telse if (anon_exclusive)",
            "\t\t\t\tentry = make_readable_exclusive_migration_entry(",
            "\t\t\t\t\t\t\tpage_to_pfn(page));",
            "\t\t\telse",
            "\t\t\t\tentry = make_readable_migration_entry(",
            "\t\t\t\t\t\t\tpage_to_pfn(page));",
            "\t\t\tif (pte_present(pte)) {",
            "\t\t\t\tif (pte_young(pte))",
            "\t\t\t\t\tentry = make_migration_entry_young(entry);",
            "\t\t\t\tif (pte_dirty(pte))",
            "\t\t\t\t\tentry = make_migration_entry_dirty(entry);",
            "\t\t\t}",
            "\t\t\tswp_pte = swp_entry_to_pte(entry);",
            "\t\t\tif (pte_present(pte)) {",
            "\t\t\t\tif (pte_soft_dirty(pte))",
            "\t\t\t\t\tswp_pte = pte_swp_mksoft_dirty(swp_pte);",
            "\t\t\t\tif (pte_uffd_wp(pte))",
            "\t\t\t\t\tswp_pte = pte_swp_mkuffd_wp(swp_pte);",
            "\t\t\t} else {",
            "\t\t\t\tif (pte_swp_soft_dirty(pte))",
            "\t\t\t\t\tswp_pte = pte_swp_mksoft_dirty(swp_pte);",
            "\t\t\t\tif (pte_swp_uffd_wp(pte))",
            "\t\t\t\t\tswp_pte = pte_swp_mkuffd_wp(swp_pte);",
            "\t\t\t}",
            "\t\t\tset_pte_at(mm, addr, ptep, swp_pte);",
            "",
            "\t\t\t/*",
            "\t\t\t * This is like regular unmap: we remove the rmap and",
            "\t\t\t * drop the folio refcount. The folio won't be freed, as",
            "\t\t\t * we took a reference just above.",
            "\t\t\t */",
            "\t\t\tfolio_remove_rmap_pte(folio, page, vma);",
            "\t\t\tfolio_put(folio);",
            "",
            "\t\t\tif (pte_present(pte))",
            "\t\t\t\tunmapped++;",
            "\t\t} else {",
            "\t\t\tfolio_put(folio);",
            "\t\t\tmpfn = 0;",
            "\t\t}",
            "",
            "next:",
            "\t\tmigrate->dst[migrate->npages] = 0;",
            "\t\tmigrate->src[migrate->npages++] = mpfn;",
            "\t}",
            "",
            "\t/* Only flush the TLB if we actually modified any entries */",
            "\tif (unmapped)",
            "\t\tflush_tlb_range(walk->vma, start, end);",
            "",
            "\tarch_leave_lazy_mmu_mode();",
            "\tpte_unmap_unlock(ptep - 1, ptl);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "migrate_vma_collect_skip, migrate_vma_collect_hole, migrate_vma_collect_pmd",
          "description": "实现页面收集逻辑，遍历页表项识别可迁移页面，处理匿名内存、设备私有页及特殊页面迁移标记。",
          "similarity": 0.37508586049079895
        },
        {
          "chunk_id": 3,
          "file_path": "mm/migrate_device.c",
          "start_line": 449,
          "end_line": 606,
          "content": [
            "static void migrate_vma_unmap(struct migrate_vma *migrate)",
            "{",
            "\tmigrate->cpages = migrate_device_unmap(migrate->src, migrate->npages,",
            "\t\t\t\t\tmigrate->fault_page);",
            "}",
            "int migrate_vma_setup(struct migrate_vma *args)",
            "{",
            "\tlong nr_pages = (args->end - args->start) >> PAGE_SHIFT;",
            "",
            "\targs->start &= PAGE_MASK;",
            "\targs->end &= PAGE_MASK;",
            "\tif (!args->vma || is_vm_hugetlb_page(args->vma) ||",
            "\t    (args->vma->vm_flags & VM_SPECIAL) || vma_is_dax(args->vma))",
            "\t\treturn -EINVAL;",
            "\tif (nr_pages <= 0)",
            "\t\treturn -EINVAL;",
            "\tif (args->start < args->vma->vm_start ||",
            "\t    args->start >= args->vma->vm_end)",
            "\t\treturn -EINVAL;",
            "\tif (args->end <= args->vma->vm_start || args->end > args->vma->vm_end)",
            "\t\treturn -EINVAL;",
            "\tif (!args->src || !args->dst)",
            "\t\treturn -EINVAL;",
            "\tif (args->fault_page && !is_device_private_page(args->fault_page))",
            "\t\treturn -EINVAL;",
            "",
            "\tmemset(args->src, 0, sizeof(*args->src) * nr_pages);",
            "\targs->cpages = 0;",
            "\targs->npages = 0;",
            "",
            "\tmigrate_vma_collect(args);",
            "",
            "\tif (args->cpages)",
            "\t\tmigrate_vma_unmap(args);",
            "",
            "\t/*",
            "\t * At this point pages are locked and unmapped, and thus they have",
            "\t * stable content and can safely be copied to destination memory that",
            "\t * is allocated by the drivers.",
            "\t */",
            "\treturn 0;",
            "",
            "}",
            "static void migrate_vma_insert_page(struct migrate_vma *migrate,",
            "\t\t\t\t    unsigned long addr,",
            "\t\t\t\t    struct page *page,",
            "\t\t\t\t    unsigned long *src)",
            "{",
            "\tstruct folio *folio = page_folio(page);",
            "\tstruct vm_area_struct *vma = migrate->vma;",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tbool flush = false;",
            "\tspinlock_t *ptl;",
            "\tpte_t entry;",
            "\tpgd_t *pgdp;",
            "\tp4d_t *p4dp;",
            "\tpud_t *pudp;",
            "\tpmd_t *pmdp;",
            "\tpte_t *ptep;",
            "\tpte_t orig_pte;",
            "",
            "\t/* Only allow populating anonymous memory */",
            "\tif (!vma_is_anonymous(vma))",
            "\t\tgoto abort;",
            "",
            "\tpgdp = pgd_offset(mm, addr);",
            "\tp4dp = p4d_alloc(mm, pgdp, addr);",
            "\tif (!p4dp)",
            "\t\tgoto abort;",
            "\tpudp = pud_alloc(mm, p4dp, addr);",
            "\tif (!pudp)",
            "\t\tgoto abort;",
            "\tpmdp = pmd_alloc(mm, pudp, addr);",
            "\tif (!pmdp)",
            "\t\tgoto abort;",
            "\tif (pmd_trans_huge(*pmdp) || pmd_devmap(*pmdp))",
            "\t\tgoto abort;",
            "\tif (pte_alloc(mm, pmdp))",
            "\t\tgoto abort;",
            "\tif (unlikely(anon_vma_prepare(vma)))",
            "\t\tgoto abort;",
            "\tif (mem_cgroup_charge(folio, vma->vm_mm, GFP_KERNEL))",
            "\t\tgoto abort;",
            "",
            "\t/*",
            "\t * The memory barrier inside __folio_mark_uptodate makes sure that",
            "\t * preceding stores to the folio contents become visible before",
            "\t * the set_pte_at() write.",
            "\t */",
            "\t__folio_mark_uptodate(folio);",
            "",
            "\tif (folio_is_device_private(folio)) {",
            "\t\tswp_entry_t swp_entry;",
            "",
            "\t\tif (vma->vm_flags & VM_WRITE)",
            "\t\t\tswp_entry = make_writable_device_private_entry(",
            "\t\t\t\t\t\tpage_to_pfn(page));",
            "\t\telse",
            "\t\t\tswp_entry = make_readable_device_private_entry(",
            "\t\t\t\t\t\tpage_to_pfn(page));",
            "\t\tentry = swp_entry_to_pte(swp_entry);",
            "\t} else {",
            "\t\tif (folio_is_zone_device(folio) &&",
            "\t\t    !folio_is_device_coherent(folio)) {",
            "\t\t\tpr_warn_once(\"Unsupported ZONE_DEVICE page type.\\n\");",
            "\t\t\tgoto abort;",
            "\t\t}",
            "\t\tentry = mk_pte(page, vma->vm_page_prot);",
            "\t\tif (vma->vm_flags & VM_WRITE)",
            "\t\t\tentry = pte_mkwrite(pte_mkdirty(entry), vma);",
            "\t}",
            "",
            "\tptep = pte_offset_map_lock(mm, pmdp, addr, &ptl);",
            "\tif (!ptep)",
            "\t\tgoto abort;",
            "\torig_pte = ptep_get(ptep);",
            "",
            "\tif (check_stable_address_space(mm))",
            "\t\tgoto unlock_abort;",
            "",
            "\tif (pte_present(orig_pte)) {",
            "\t\tunsigned long pfn = pte_pfn(orig_pte);",
            "",
            "\t\tif (!is_zero_pfn(pfn))",
            "\t\t\tgoto unlock_abort;",
            "\t\tflush = true;",
            "\t} else if (!pte_none(orig_pte))",
            "\t\tgoto unlock_abort;",
            "",
            "\t/*",
            "\t * Check for userfaultfd but do not deliver the fault. Instead,",
            "\t * just back off.",
            "\t */",
            "\tif (userfaultfd_missing(vma))",
            "\t\tgoto unlock_abort;",
            "",
            "\tinc_mm_counter(mm, MM_ANONPAGES);",
            "\tfolio_add_new_anon_rmap(folio, vma, addr, RMAP_EXCLUSIVE);",
            "\tif (!folio_is_zone_device(folio))",
            "\t\tfolio_add_lru_vma(folio, vma);",
            "\tfolio_get(folio);",
            "",
            "\tif (flush) {",
            "\t\tflush_cache_page(vma, addr, pte_pfn(orig_pte));",
            "\t\tptep_clear_flush(vma, addr, ptep);",
            "\t}",
            "\tset_pte_at(mm, addr, ptep, entry);",
            "\tupdate_mmu_cache(vma, addr, ptep);",
            "",
            "\tpte_unmap_unlock(ptep, ptl);",
            "\t*src = MIGRATE_PFN_MIGRATE;",
            "\treturn;",
            "",
            "unlock_abort:",
            "\tpte_unmap_unlock(ptep, ptl);",
            "abort:",
            "\t*src &= ~MIGRATE_PFN_MIGRATE;",
            "}"
          ],
          "function_name": "migrate_vma_unmap, migrate_vma_setup, migrate_vma_insert_page",
          "description": "初始化迁移上下文并执行页面插入操作，处理不同类型的页面映射，包括设备私有页和普通页的页表项设置。",
          "similarity": 0.3667871057987213
        }
      ]
    }
  ]
}