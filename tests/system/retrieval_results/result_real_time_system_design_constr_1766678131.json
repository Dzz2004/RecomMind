{
  "query": "real-time system design constraints",
  "timestamp": "2025-12-25 23:55:31",
  "retrieved_files": [
    {
      "source_file": "kernel/time/namespace.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:40:48\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `time\\namespace.c`\n\n---\n\n# time/namespace.c 技术文档\n\n## 1. 文件概述\n\n`time/namespace.c` 实现了 Linux 内核中的 **时间命名空间（time namespace）** 功能，允许不同进程组拥有独立的时间视图。该机制主要用于容器化环境中，使容器内的进程能够看到与宿主机或其他容器不同的系统时间（特别是 `CLOCK_MONOTONIC` 和 `CLOCK_BOOTTIME` 等单调时钟）。时间命名空间通过偏移量（offset）机制实现，不影响真实硬件时钟，仅在用户空间通过 VDSO（虚拟动态共享对象）提供转换后的时间值。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `do_timens_ktime_to_host()` | 将时间命名空间中的时间值转换回宿主机时间（减去偏移量），用于内核内部时间比较 |\n| `clone_time_ns()` | 克隆一个时间命名空间，分配资源并初始化 VVAR 页面 |\n| `copy_time_ns()` | 根据 `CLONE_NEWTIME` 标志决定是克隆还是复用现有时间命名空间 |\n| `timens_setup_vdso_data()` | 在 VDSO 数据页中设置时间偏移量，供用户空间读取 |\n| `find_timens_vvar_page()` | 为进程查找其所属时间命名空间的 VVAR 页面 |\n| `timens_set_vvar_page()` | 初始化时间命名空间的 VVAR 页面（仅首次进入时执行） |\n| `free_time_ns()` | 释放时间命名空间占用的资源 |\n| `timens_commit()` | 在任务切换到新时间命名空间时提交配置（设置 VVAR 和 VDSO） |\n| `timens_install()` | 安装新的时间命名空间到当前进程（需权限检查） |\n| `timens_on_fork()` | 子进程 fork 时继承父进程的 `time_ns_for_children` |\n\n### 关键数据结构\n\n- `struct time_namespace`：时间命名空间的核心结构，包含：\n  - `vvar_page`：用于 VDSO 的特殊内存页\n  - `offsets`：`monotonic` 和 `boottime` 时钟的偏移量\n  - `frozen_offsets`：标志位，表示偏移量是否已固化（防止重复初始化）\n  - `user_ns`：所属的用户命名空间\n  - `ucounts`：资源计数器，限制时间命名空间创建数量\n\n- `struct timens_offsets`：存储 `CLOCK_MONOTONIC` 和 `CLOCK_BOOTTIME` 的偏移量（`timespec64` 格式）\n\n- `struct timens_offset`：VDSO 中使用的偏移量结构（`sec` + `nsec`）\n\n## 3. 关键实现\n\n### 时间偏移转换机制\n- `do_timens_ktime_to_host()` 负责将命名空间内的时间值（如定时器到期时间）转换为宿主机视角的时间。\n- 对于 `CLOCK_MONOTONIC` 和 `CLOCK_BOOTTIME`，减去对应的偏移量。\n- 若转换后时间小于 0，则视为已过期，返回 0。\n- 转换结果被限制在 `[0, KTIME_MAX]` 范围内。\n\n### VDSO 集成\n- 时间命名空间通过 **VVAR 页面** 向用户空间暴露偏移量。\n- 正常进程的 VDSO 布局：`VVAR → PVCLOCK → HVCLOCK`\n- 时间命名空间进程的 VDSO 布局：`TIMENS → PVCLOCK → HVCLOCK → VVAR`\n- `timens_setup_vdso_data()` 在 VVAR 页面中设置 `clock_mode = VDSO_CLOCKMODE_TIMENS` 并填充各时钟的偏移量。\n- 用户空间 VDSO 代码根据 `clock_mode` 决定是否应用偏移。\n\n### 偏移量初始化保护\n- 使用全局 `offset_lock` 互斥锁确保 `vvar_page` 仅被初始化一次。\n- `frozen_offsets` 标志位避免重复初始化，提高性能（快路径无锁）。\n\n### 资源管理与权限控制\n- 通过 `ucounts` 限制每个用户命名空间可创建的时间命名空间数量（防 DoS）。\n- `timens_install()` 要求调用者在**目标命名空间**和**当前命名空间**均具备 `CAP_SYS_ADMIN` 权限。\n- 仅允许单线程进程（`current_is_single_threaded()`）切换时间命名空间，避免多线程一致性问题。\n\n### 进程继承模型\n- 每个进程拥有两个时间命名空间指针：\n  - `time_ns`：当前生效的时间命名空间\n  - `time_ns_for_children`：子进程将继承的时间命名空间\n- `timens_on_fork()` 确保子进程正确继承父进程的 `time_ns_for_children`\n\n## 4. 依赖关系\n\n| 依赖模块 | 用途 |\n|---------|------|\n| `<linux/user_namespace.h>` | 用户命名空间支持，用于权限隔离和资源计数 |\n| `<linux/proc_ns.h>` | 命名空间 proc 接口（如 `/proc/PID/ns/time`） |\n| `<vdso/datapage.h>` | VDSO 数据页结构定义 |\n| `<linux/clocksource.h>` | 时钟源相关常量（如 `CS_BASES`） |\n| `<linux/sched/*.h>` | 进程调度和 nsproxy 管理 |\n| `<linux/cred.h>` | 凭据和权限检查（`ns_capable()`） |\n| `<linux/mm.h>` | 内存管理（`alloc_page()`、`vm_area_struct`） |\n\n## 5. 使用场景\n\n1. **容器时间隔离**  \n   容器运行时（如 LXC、systemd-nspawn）可通过 `unshare(CLONE_NEWTIME)` 创建独立时间视图，使容器内 `CLOCK_MONOTONIC` 从 0 开始计时，便于测试或迁移。\n\n2. **系统时间回滚测试**  \n   开发者可在时间命名空间中设置负偏移量，模拟系统时间回退场景，验证应用程序的健壮性。\n\n3. **沙箱环境**  \n   安全沙箱可限制进程看到的时间范围，防止基于时间的侧信道攻击。\n\n4. **VDSO 优化路径**  \n   用户空间通过 VDSO 直接读取偏移后的时间，无需系统调用，性能开销极低。\n\n5. **命名空间组合**  \n   时间命名空间通常与 PID、mount、user 等命名空间联合使用，构建完整的隔离环境。",
      "similarity": 0.5220478773117065,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/time/namespace.c",
          "start_line": 298,
          "end_line": 448,
          "content": [
            "void timens_commit(struct task_struct *tsk, struct time_namespace *ns)",
            "{",
            "\ttimens_set_vvar_page(tsk, ns);",
            "\tvdso_join_timens(tsk, ns);",
            "}",
            "static int timens_install(struct nsset *nsset, struct ns_common *new)",
            "{",
            "\tstruct nsproxy *nsproxy = nsset->nsproxy;",
            "\tstruct time_namespace *ns = to_time_ns(new);",
            "",
            "\tif (!current_is_single_threaded())",
            "\t\treturn -EUSERS;",
            "",
            "\tif (!ns_capable(ns->user_ns, CAP_SYS_ADMIN) ||",
            "\t    !ns_capable(nsset->cred->user_ns, CAP_SYS_ADMIN))",
            "\t\treturn -EPERM;",
            "",
            "\tget_time_ns(ns);",
            "\tput_time_ns(nsproxy->time_ns);",
            "\tnsproxy->time_ns = ns;",
            "",
            "\tget_time_ns(ns);",
            "\tput_time_ns(nsproxy->time_ns_for_children);",
            "\tnsproxy->time_ns_for_children = ns;",
            "\treturn 0;",
            "}",
            "void timens_on_fork(struct nsproxy *nsproxy, struct task_struct *tsk)",
            "{",
            "\tstruct ns_common *nsc = &nsproxy->time_ns_for_children->ns;",
            "\tstruct time_namespace *ns = to_time_ns(nsc);",
            "",
            "\t/* create_new_namespaces() already incremented the ref counter */",
            "\tif (nsproxy->time_ns == nsproxy->time_ns_for_children)",
            "\t\treturn;",
            "",
            "\tget_time_ns(ns);",
            "\tput_time_ns(nsproxy->time_ns);",
            "\tnsproxy->time_ns = ns;",
            "",
            "\ttimens_commit(tsk, ns);",
            "}",
            "static void show_offset(struct seq_file *m, int clockid, struct timespec64 *ts)",
            "{",
            "\tchar *clock;",
            "",
            "\tswitch (clockid) {",
            "\tcase CLOCK_BOOTTIME:",
            "\t\tclock = \"boottime\";",
            "\t\tbreak;",
            "\tcase CLOCK_MONOTONIC:",
            "\t\tclock = \"monotonic\";",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tclock = \"unknown\";",
            "\t\tbreak;",
            "\t}",
            "\tseq_printf(m, \"%-10s %10lld %9ld\\n\", clock, ts->tv_sec, ts->tv_nsec);",
            "}",
            "void proc_timens_show_offsets(struct task_struct *p, struct seq_file *m)",
            "{",
            "\tstruct ns_common *ns;",
            "\tstruct time_namespace *time_ns;",
            "",
            "\tns = timens_for_children_get(p);",
            "\tif (!ns)",
            "\t\treturn;",
            "\ttime_ns = to_time_ns(ns);",
            "",
            "\tshow_offset(m, CLOCK_MONOTONIC, &time_ns->offsets.monotonic);",
            "\tshow_offset(m, CLOCK_BOOTTIME, &time_ns->offsets.boottime);",
            "\tput_time_ns(time_ns);",
            "}",
            "int proc_timens_set_offset(struct file *file, struct task_struct *p,",
            "\t\t\t   struct proc_timens_offset *offsets, int noffsets)",
            "{",
            "\tstruct ns_common *ns;",
            "\tstruct time_namespace *time_ns;",
            "\tstruct timespec64 tp;",
            "\tint i, err;",
            "",
            "\tns = timens_for_children_get(p);",
            "\tif (!ns)",
            "\t\treturn -ESRCH;",
            "\ttime_ns = to_time_ns(ns);",
            "",
            "\tif (!file_ns_capable(file, time_ns->user_ns, CAP_SYS_TIME)) {",
            "\t\tput_time_ns(time_ns);",
            "\t\treturn -EPERM;",
            "\t}",
            "",
            "\tfor (i = 0; i < noffsets; i++) {",
            "\t\tstruct proc_timens_offset *off = &offsets[i];",
            "",
            "\t\tswitch (off->clockid) {",
            "\t\tcase CLOCK_MONOTONIC:",
            "\t\t\tktime_get_ts64(&tp);",
            "\t\t\tbreak;",
            "\t\tcase CLOCK_BOOTTIME:",
            "\t\t\tktime_get_boottime_ts64(&tp);",
            "\t\t\tbreak;",
            "\t\tdefault:",
            "\t\t\terr = -EINVAL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\terr = -ERANGE;",
            "",
            "\t\tif (off->val.tv_sec > KTIME_SEC_MAX ||",
            "\t\t    off->val.tv_sec < -KTIME_SEC_MAX)",
            "\t\t\tgoto out;",
            "",
            "\t\ttp = timespec64_add(tp, off->val);",
            "\t\t/*",
            "\t\t * KTIME_SEC_MAX is divided by 2 to be sure that KTIME_MAX is",
            "\t\t * still unreachable.",
            "\t\t */",
            "\t\tif (tp.tv_sec < 0 || tp.tv_sec > KTIME_SEC_MAX / 2)",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tmutex_lock(&offset_lock);",
            "\tif (time_ns->frozen_offsets) {",
            "\t\terr = -EACCES;",
            "\t\tgoto out_unlock;",
            "\t}",
            "",
            "\terr = 0;",
            "\t/* Don't report errors after this line */",
            "\tfor (i = 0; i < noffsets; i++) {",
            "\t\tstruct proc_timens_offset *off = &offsets[i];",
            "\t\tstruct timespec64 *offset = NULL;",
            "",
            "\t\tswitch (off->clockid) {",
            "\t\tcase CLOCK_MONOTONIC:",
            "\t\t\toffset = &time_ns->offsets.monotonic;",
            "\t\t\tbreak;",
            "\t\tcase CLOCK_BOOTTIME:",
            "\t\t\toffset = &time_ns->offsets.boottime;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t*offset = off->val;",
            "\t}",
            "",
            "out_unlock:",
            "\tmutex_unlock(&offset_lock);",
            "out:",
            "\tput_time_ns(time_ns);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "timens_commit, timens_install, timens_on_fork, show_offset, proc_timens_show_offsets, proc_timens_set_offset",
          "description": "实现时间命名空间的安装传播机制，包含命名空间继承处理、偏移量展示接口、时钟偏移量设置接口及其权限校验逻辑。",
          "similarity": 0.5211264491081238
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/time/namespace.c",
          "start_line": 23,
          "end_line": 124,
          "content": [
            "ktime_t do_timens_ktime_to_host(clockid_t clockid, ktime_t tim,",
            "\t\t\t\tstruct timens_offsets *ns_offsets)",
            "{",
            "\tktime_t offset;",
            "",
            "\tswitch (clockid) {",
            "\tcase CLOCK_MONOTONIC:",
            "\t\toffset = timespec64_to_ktime(ns_offsets->monotonic);",
            "\t\tbreak;",
            "\tcase CLOCK_BOOTTIME:",
            "\tcase CLOCK_BOOTTIME_ALARM:",
            "\t\toffset = timespec64_to_ktime(ns_offsets->boottime);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\treturn tim;",
            "\t}",
            "",
            "\t/*",
            "\t * Check that @tim value is in [offset, KTIME_MAX + offset]",
            "\t * and subtract offset.",
            "\t */",
            "\tif (tim < offset) {",
            "\t\t/*",
            "\t\t * User can specify @tim *absolute* value - if it's lesser than",
            "\t\t * the time namespace's offset - it's already expired.",
            "\t\t */",
            "\t\ttim = 0;",
            "\t} else {",
            "\t\ttim = ktime_sub(tim, offset);",
            "\t\tif (unlikely(tim > KTIME_MAX))",
            "\t\t\ttim = KTIME_MAX;",
            "\t}",
            "",
            "\treturn tim;",
            "}",
            "static void dec_time_namespaces(struct ucounts *ucounts)",
            "{",
            "\tdec_ucount(ucounts, UCOUNT_TIME_NAMESPACES);",
            "}",
            "static struct timens_offset offset_from_ts(struct timespec64 off)",
            "{",
            "\tstruct timens_offset ret;",
            "",
            "\tret.sec = off.tv_sec;",
            "\tret.nsec = off.tv_nsec;",
            "",
            "\treturn ret;",
            "}",
            "static void timens_setup_vdso_data(struct vdso_data *vdata,",
            "\t\t\t\t   struct time_namespace *ns)",
            "{",
            "\tstruct timens_offset *offset = vdata->offset;",
            "\tstruct timens_offset monotonic = offset_from_ts(ns->offsets.monotonic);",
            "\tstruct timens_offset boottime = offset_from_ts(ns->offsets.boottime);",
            "",
            "\tvdata->seq\t\t\t= 1;",
            "\tvdata->clock_mode\t\t= VDSO_CLOCKMODE_TIMENS;",
            "\toffset[CLOCK_MONOTONIC]\t\t= monotonic;",
            "\toffset[CLOCK_MONOTONIC_RAW]\t= monotonic;",
            "\toffset[CLOCK_MONOTONIC_COARSE]\t= monotonic;",
            "\toffset[CLOCK_BOOTTIME]\t\t= boottime;",
            "\toffset[CLOCK_BOOTTIME_ALARM]\t= boottime;",
            "}",
            "static void timens_set_vvar_page(struct task_struct *task,",
            "\t\t\t\tstruct time_namespace *ns)",
            "{",
            "\tstruct vdso_data *vdata;",
            "\tunsigned int i;",
            "",
            "\tif (ns == &init_time_ns)",
            "\t\treturn;",
            "",
            "\t/* Fast-path, taken by every task in namespace except the first. */",
            "\tif (likely(ns->frozen_offsets))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&offset_lock);",
            "\t/* Nothing to-do: vvar_page has been already initialized. */",
            "\tif (ns->frozen_offsets)",
            "\t\tgoto out;",
            "",
            "\tns->frozen_offsets = true;",
            "\tvdata = arch_get_vdso_data(page_address(ns->vvar_page));",
            "",
            "\tfor (i = 0; i < CS_BASES; i++)",
            "\t\ttimens_setup_vdso_data(&vdata[i], ns);",
            "",
            "out:",
            "\tmutex_unlock(&offset_lock);",
            "}",
            "void free_time_ns(struct time_namespace *ns)",
            "{",
            "\tdec_time_namespaces(ns->ucounts);",
            "\tput_user_ns(ns->user_ns);",
            "\tns_free_inum(&ns->ns);",
            "\t__free_page(ns->vvar_page);",
            "\tkfree(ns);",
            "}",
            "static void timens_put(struct ns_common *ns)",
            "{",
            "\tput_time_ns(to_time_ns(ns));",
            "}"
          ],
          "function_name": "do_timens_ktime_to_host, dec_time_namespaces, offset_from_ts, timens_setup_vdso_data, timens_set_vvar_page, free_time_ns, timens_put",
          "description": "实现时间命名空间偏移转换逻辑，包括时间转换、引用计数更新、VDSO数据初始化、页内存释放及命名空间引用计数管理等功能模块。",
          "similarity": 0.4653055667877197
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/time/namespace.c",
          "start_line": 1,
          "end_line": 22,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Author: Andrei Vagin <avagin@openvz.org>",
            " * Author: Dmitry Safonov <dima@arista.com>",
            " */",
            "",
            "#include <linux/time_namespace.h>",
            "#include <linux/user_namespace.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/clocksource.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/export.h>",
            "#include <linux/time.h>",
            "#include <linux/slab.h>",
            "#include <linux/cred.h>",
            "#include <linux/err.h>",
            "#include <linux/mm.h>",
            "",
            "#include <vdso/datapage.h>",
            ""
          ],
          "function_name": null,
          "description": "包含时间命名空间所需头文件，声明时间、用户命名空间及内核通用结构体，为后续时间命名空间实现提供类型和函数声明支持。",
          "similarity": 0.40346160531044006
        }
      ]
    },
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.5134556889533997,
      "chunks": [
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1776,
          "end_line": 1992,
          "content": [
            "static int pick_rt_task(struct rq *rq, struct task_struct *p, int cpu)",
            "{",
            "\tif (!task_on_cpu(rq, p) &&",
            "\t    cpumask_test_cpu(cpu, &p->cpus_mask))",
            "\t\treturn 1;",
            "",
            "\treturn 0;",
            "}",
            "static int find_lowest_rq(struct task_struct *task)",
            "{",
            "\tstruct sched_domain *sd;",
            "\tstruct cpumask *lowest_mask = this_cpu_cpumask_var_ptr(local_cpu_mask);",
            "\tint this_cpu = smp_processor_id();",
            "\tint cpu      = task_cpu(task);",
            "\tint ret;",
            "",
            "\t/* Make sure the mask is initialized first */",
            "\tif (unlikely(!lowest_mask))",
            "\t\treturn -1;",
            "",
            "\tif (task->nr_cpus_allowed == 1)",
            "\t\treturn -1; /* No other targets possible */",
            "",
            "\t/*",
            "\t * If we're on asym system ensure we consider the different capacities",
            "\t * of the CPUs when searching for the lowest_mask.",
            "\t */",
            "\tif (sched_asym_cpucap_active()) {",
            "",
            "\t\tret = cpupri_find_fitness(&task_rq(task)->rd->cpupri,",
            "\t\t\t\t\t  task, lowest_mask,",
            "\t\t\t\t\t  rt_task_fits_capacity);",
            "\t} else {",
            "",
            "\t\tret = cpupri_find(&task_rq(task)->rd->cpupri,",
            "\t\t\t\t  task, lowest_mask);",
            "\t}",
            "",
            "\tif (!ret)",
            "\t\treturn -1; /* No targets found */",
            "",
            "\t/*",
            "\t * At this point we have built a mask of CPUs representing the",
            "\t * lowest priority tasks in the system.  Now we want to elect",
            "\t * the best one based on our affinity and topology.",
            "\t *",
            "\t * We prioritize the last CPU that the task executed on since",
            "\t * it is most likely cache-hot in that location.",
            "\t */",
            "\tif (cpumask_test_cpu(cpu, lowest_mask))",
            "\t\treturn cpu;",
            "",
            "\t/*",
            "\t * Otherwise, we consult the sched_domains span maps to figure",
            "\t * out which CPU is logically closest to our hot cache data.",
            "\t */",
            "\tif (!cpumask_test_cpu(this_cpu, lowest_mask))",
            "\t\tthis_cpu = -1; /* Skip this_cpu opt if not among lowest */",
            "",
            "\trcu_read_lock();",
            "\tfor_each_domain(cpu, sd) {",
            "\t\tif (sd->flags & SD_WAKE_AFFINE) {",
            "\t\t\tint best_cpu;",
            "",
            "\t\t\t/*",
            "\t\t\t * \"this_cpu\" is cheaper to preempt than a",
            "\t\t\t * remote processor.",
            "\t\t\t */",
            "\t\t\tif (this_cpu != -1 &&",
            "\t\t\t    cpumask_test_cpu(this_cpu, sched_domain_span(sd))) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn this_cpu;",
            "\t\t\t}",
            "",
            "\t\t\tbest_cpu = cpumask_any_and_distribute(lowest_mask,",
            "\t\t\t\t\t\t\t      sched_domain_span(sd));",
            "\t\t\tif (best_cpu < nr_cpu_ids) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn best_cpu;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * And finally, if there were no matches within the domains",
            "\t * just give the caller *something* to work with from the compatible",
            "\t * locations.",
            "\t */",
            "\tif (this_cpu != -1)",
            "\t\treturn this_cpu;",
            "",
            "\tcpu = cpumask_any_distribute(lowest_mask);",
            "\tif (cpu < nr_cpu_ids)",
            "\t\treturn cpu;",
            "",
            "\treturn -1;",
            "}",
            "static int push_rt_task(struct rq *rq, bool pull)",
            "{",
            "\tstruct task_struct *next_task;",
            "\tstruct rq *lowest_rq;",
            "\tint ret = 0;",
            "",
            "\tif (!rq->rt.overloaded)",
            "\t\treturn 0;",
            "",
            "\tnext_task = pick_next_pushable_task(rq);",
            "\tif (!next_task)",
            "\t\treturn 0;",
            "",
            "retry:",
            "\t/*",
            "\t * It's possible that the next_task slipped in of",
            "\t * higher priority than current. If that's the case",
            "\t * just reschedule current.",
            "\t */",
            "\tif (unlikely(next_task->prio < rq->curr->prio)) {",
            "\t\tresched_curr(rq);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (is_migration_disabled(next_task)) {",
            "\t\tstruct task_struct *push_task = NULL;",
            "\t\tint cpu;",
            "",
            "\t\tif (!pull || rq->push_busy)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * Invoking find_lowest_rq() on anything but an RT task doesn't",
            "\t\t * make sense. Per the above priority check, curr has to",
            "\t\t * be of higher priority than next_task, so no need to",
            "\t\t * reschedule when bailing out.",
            "\t\t *",
            "\t\t * Note that the stoppers are masqueraded as SCHED_FIFO",
            "\t\t * (cf. sched_set_stop_task()), so we can't rely on rt_task().",
            "\t\t */",
            "\t\tif (rq->curr->sched_class != &rt_sched_class)",
            "\t\t\treturn 0;",
            "",
            "\t\tcpu = find_lowest_rq(rq->curr);",
            "\t\tif (cpu == -1 || cpu == rq->cpu)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * Given we found a CPU with lower priority than @next_task,",
            "\t\t * therefore it should be running. However we cannot migrate it",
            "\t\t * to this other CPU, instead attempt to push the current",
            "\t\t * running task on this CPU away.",
            "\t\t */",
            "\t\tpush_task = get_push_task(rq);",
            "\t\tif (push_task) {",
            "\t\t\tpreempt_disable();",
            "\t\t\traw_spin_rq_unlock(rq);",
            "\t\t\tstop_one_cpu_nowait(rq->cpu, push_cpu_stop,",
            "\t\t\t\t\t    push_task, &rq->push_work);",
            "\t\t\tpreempt_enable();",
            "\t\t\traw_spin_rq_lock(rq);",
            "\t\t}",
            "",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (WARN_ON(next_task == rq->curr))",
            "\t\treturn 0;",
            "",
            "\t/* We might release rq lock */",
            "\tget_task_struct(next_task);",
            "",
            "\t/* find_lock_lowest_rq locks the rq if found */",
            "\tlowest_rq = find_lock_lowest_rq(next_task, rq);",
            "\tif (!lowest_rq) {",
            "\t\tstruct task_struct *task;",
            "\t\t/*",
            "\t\t * find_lock_lowest_rq releases rq->lock",
            "\t\t * so it is possible that next_task has migrated.",
            "\t\t *",
            "\t\t * We need to make sure that the task is still on the same",
            "\t\t * run-queue and is also still the next task eligible for",
            "\t\t * pushing.",
            "\t\t */",
            "\t\ttask = pick_next_pushable_task(rq);",
            "\t\tif (task == next_task) {",
            "\t\t\t/*",
            "\t\t\t * The task hasn't migrated, and is still the next",
            "\t\t\t * eligible task, but we failed to find a run-queue",
            "\t\t\t * to push it to.  Do not retry in this case, since",
            "\t\t\t * other CPUs will pull from us when ready.",
            "\t\t\t */",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tif (!task)",
            "\t\t\t/* No more tasks, just exit */",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * Something has shifted, try again.",
            "\t\t */",
            "\t\tput_task_struct(next_task);",
            "\t\tnext_task = task;",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tdeactivate_task(rq, next_task, 0);",
            "\tset_task_cpu(next_task, lowest_rq->cpu);",
            "\tactivate_task(lowest_rq, next_task, 0);",
            "\tresched_curr(lowest_rq);",
            "\tret = 1;",
            "",
            "\tdouble_unlock_balance(rq, lowest_rq);",
            "out:",
            "\tput_task_struct(next_task);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "pick_rt_task, find_lowest_rq, push_rt_task",
          "description": "实现实时任务选择算法、低优先级CPU搜索及强制迁移逻辑，支持异构系统下的能效优化和拓扑感知调度。",
          "similarity": 0.5661501884460449
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/rt.c",
          "start_line": 776,
          "end_line": 913,
          "content": [
            "static void balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tif (!sched_feat(RT_RUNTIME_SHARE))",
            "\t\treturn;",
            "",
            "\tif (rt_rq->rt_time > rt_rq->rt_runtime) {",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tdo_balance_runtime(rt_rq);",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t}",
            "}",
            "static inline void balance_runtime(struct rt_rq *rt_rq) {}",
            "static int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun)",
            "{",
            "\tint i, idle = 1, throttled = 0;",
            "\tconst struct cpumask *span;",
            "",
            "\tspan = sched_rt_period_mask();",
            "",
            "\t/*",
            "\t * FIXME: isolated CPUs should really leave the root task group,",
            "\t * whether they are isolcpus or were isolated via cpusets, lest",
            "\t * the timer run on a CPU which does not service all runqueues,",
            "\t * potentially leaving other CPUs indefinitely throttled.  If",
            "\t * isolation is really required, the user will turn the throttle",
            "\t * off to kill the perturbations it causes anyway.  Meanwhile,",
            "\t * this maintains functionality for boot and/or troubleshooting.",
            "\t */",
            "\tif (rt_b == &root_task_group.rt_bandwidth)",
            "\t\tspan = cpu_online_mask;",
            "",
            "\tfor_each_cpu(i, span) {",
            "\t\tint enqueue = 0;",
            "\t\tstruct rt_rq *rt_rq = sched_rt_period_rt_rq(rt_b, i);",
            "\t\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\t\tstruct rq_flags rf;",
            "\t\tint skip;",
            "",
            "\t\t/*",
            "\t\t * When span == cpu_online_mask, taking each rq->lock",
            "\t\t * can be time-consuming. Try to avoid it when possible.",
            "\t\t */",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\tif (!sched_feat(RT_RUNTIME_SHARE) && rt_rq->rt_runtime != RUNTIME_INF)",
            "\t\t\trt_rq->rt_runtime = rt_b->rt_runtime;",
            "\t\tskip = !rt_rq->rt_time && !rt_rq->rt_nr_running;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tif (skip)",
            "\t\t\tcontinue;",
            "",
            "\t\trq_lock(rq, &rf);",
            "\t\tupdate_rq_clock(rq);",
            "",
            "\t\tif (rt_rq->rt_time) {",
            "\t\t\tu64 runtime;",
            "",
            "\t\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t\tif (rt_rq->rt_throttled)",
            "\t\t\t\tbalance_runtime(rt_rq);",
            "\t\t\truntime = rt_rq->rt_runtime;",
            "\t\t\trt_rq->rt_time -= min(rt_rq->rt_time, overrun*runtime);",
            "\t\t\tif (rt_rq->rt_throttled && rt_rq->rt_time < runtime) {",
            "\t\t\t\trt_rq->rt_throttled = 0;",
            "\t\t\t\tenqueue = 1;",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * When we're idle and a woken (rt) task is",
            "\t\t\t\t * throttled wakeup_preempt() will set",
            "\t\t\t\t * skip_update and the time between the wakeup",
            "\t\t\t\t * and this unthrottle will get accounted as",
            "\t\t\t\t * 'runtime'.",
            "\t\t\t\t */",
            "\t\t\t\tif (rt_rq->rt_nr_running && rq->curr == rq->idle)",
            "\t\t\t\t\trq_clock_cancel_skipupdate(rq);",
            "\t\t\t}",
            "\t\t\tif (rt_rq->rt_time || rt_rq->rt_nr_running)",
            "\t\t\t\tidle = 0;",
            "\t\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\t} else if (rt_rq->rt_nr_running) {",
            "\t\t\tidle = 0;",
            "\t\t\tif (!rt_rq_throttled(rt_rq))",
            "\t\t\t\tenqueue = 1;",
            "\t\t}",
            "\t\tif (rt_rq->rt_throttled)",
            "\t\t\tthrottled = 1;",
            "",
            "\t\tif (enqueue)",
            "\t\t\tsched_rt_rq_enqueue(rt_rq);",
            "\t\trq_unlock(rq, &rf);",
            "\t}",
            "",
            "\tif (!throttled && (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF))",
            "\t\treturn 1;",
            "",
            "\treturn idle;",
            "}",
            "static int sched_rt_runtime_exceeded(struct rt_rq *rt_rq)",
            "{",
            "\tu64 runtime = sched_rt_runtime(rt_rq);",
            "",
            "\tif (rt_rq->rt_throttled)",
            "\t\treturn rt_rq_throttled(rt_rq);",
            "",
            "\tif (runtime >= sched_rt_period(rt_rq))",
            "\t\treturn 0;",
            "",
            "\tbalance_runtime(rt_rq);",
            "\truntime = sched_rt_runtime(rt_rq);",
            "\tif (runtime == RUNTIME_INF)",
            "\t\treturn 0;",
            "",
            "\tif (rt_rq->rt_time > runtime) {",
            "\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\t\t/*",
            "\t\t * Don't actually throttle groups that have no runtime assigned",
            "\t\t * but accrue some time due to boosting.",
            "\t\t */",
            "\t\tif (likely(rt_b->rt_runtime)) {",
            "\t\t\trt_rq->rt_throttled = 1;",
            "\t\t\tprintk_deferred_once(\"sched: RT throttling activated\\n\");",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * In case we did anyway, make it go away,",
            "\t\t\t * replenishment is a joke, since it will replenish us",
            "\t\t\t * with exactly 0 ns.",
            "\t\t\t */",
            "\t\t\trt_rq->rt_time = 0;",
            "\t\t}",
            "",
            "\t\tif (rt_rq_throttled(rt_rq)) {",
            "\t\t\tsched_rt_rq_dequeue(rt_rq);",
            "\t\t\treturn 1;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "balance_runtime, balance_runtime, do_sched_rt_period_timer, sched_rt_runtime_exceeded",
          "description": "`balance_runtime`在超时时触发重新平衡，`do_sched_rt_period_timer`周期性调整运行时并检查节流状态，`sched_rt_runtime_exceeded`判断是否超出运行时限制并标记节流",
          "similarity": 0.5523254871368408
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/rt.c",
          "start_line": 57,
          "end_line": 159,
          "content": [
            "static int __init sched_rt_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", sched_rt_sysctls);",
            "\treturn 0;",
            "}",
            "void init_rt_rq(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_prio_array *array;",
            "\tint i;",
            "",
            "\tarray = &rt_rq->active;",
            "\tfor (i = 0; i < MAX_RT_PRIO; i++) {",
            "\t\tINIT_LIST_HEAD(array->queue + i);",
            "\t\t__clear_bit(i, array->bitmap);",
            "\t}",
            "\t/* delimiter for bitsearch: */",
            "\t__set_bit(MAX_RT_PRIO, array->bitmap);",
            "",
            "#if defined CONFIG_SMP",
            "\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\trt_rq->highest_prio.next = MAX_RT_PRIO-1;",
            "\trt_rq->overloaded = 0;",
            "\tplist_head_init(&rt_rq->pushable_tasks);",
            "#endif /* CONFIG_SMP */",
            "\t/* We start is dequeued state, because no RT tasks are queued */",
            "\trt_rq->rt_queued = 0;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\trt_rq->rt_time = 0;",
            "\trt_rq->rt_throttled = 0;",
            "\trt_rq->rt_runtime = 0;",
            "\traw_spin_lock_init(&rt_rq->rt_runtime_lock);",
            "#endif",
            "}",
            "static enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)",
            "{",
            "\tstruct rt_bandwidth *rt_b =",
            "\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);",
            "\tint idle = 0;",
            "\tint overrun;",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tfor (;;) {",
            "\t\toverrun = hrtimer_forward_now(timer, rt_b->rt_period);",
            "\t\tif (!overrun)",
            "\t\t\tbreak;",
            "",
            "\t\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "\t\tidle = do_sched_rt_period_timer(rt_b, overrun);",
            "\t\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\t}",
            "\tif (idle)",
            "\t\trt_b->rt_period_active = 0;",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "",
            "\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;",
            "}",
            "void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)",
            "{",
            "\trt_b->rt_period = ns_to_ktime(period);",
            "\trt_b->rt_runtime = runtime;",
            "",
            "\traw_spin_lock_init(&rt_b->rt_runtime_lock);",
            "",
            "\thrtimer_init(&rt_b->rt_period_timer, CLOCK_MONOTONIC,",
            "\t\t     HRTIMER_MODE_REL_HARD);",
            "\trt_b->rt_period_timer.function = sched_rt_period_timer;",
            "}",
            "static inline void do_start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tif (!rt_b->rt_period_active) {",
            "\t\trt_b->rt_period_active = 1;",
            "\t\t/*",
            "\t\t * SCHED_DEADLINE updates the bandwidth, as a run away",
            "\t\t * RT task with a DL task could hog a CPU. But DL does",
            "\t\t * not reset the period. If a deadline task was running",
            "\t\t * without an RT task running, it can cause RT tasks to",
            "\t\t * throttle when they start up. Kick the timer right away",
            "\t\t * to update the period.",
            "\t\t */",
            "\t\thrtimer_forward_now(&rt_b->rt_period_timer, ns_to_ktime(0));",
            "\t\thrtimer_start_expires(&rt_b->rt_period_timer,",
            "\t\t\t\t      HRTIMER_MODE_ABS_PINNED_HARD);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}",
            "static void start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)",
            "\t\treturn;",
            "",
            "\tdo_start_rt_bandwidth(rt_b);",
            "}",
            "static void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\thrtimer_cancel(&rt_b->rt_period_timer);",
            "}",
            "void unregister_rt_sched_group(struct task_group *tg)",
            "{",
            "\tif (tg->rt_se)",
            "\t\tdestroy_rt_bandwidth(&tg->rt_bandwidth);",
            "}"
          ],
          "function_name": "sched_rt_sysctl_init, init_rt_rq, sched_rt_period_timer, init_rt_bandwidth, do_start_rt_bandwidth, start_rt_bandwidth, destroy_rt_bandwidth, unregister_rt_sched_group",
          "description": "初始化实时调度相关数据结构，管理实时任务周期定时器，控制实时带宽分配与回收，实现基于时间片轮转的调度策略。",
          "similarity": 0.5503756999969482
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/rt.c",
          "start_line": 529,
          "end_line": 633,
          "content": [
            "static void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\tstruct sched_rt_entity *rt_se;",
            "",
            "\tint cpu = cpu_of(rq);",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tif (!rt_se)",
            "\t\t\tenqueue_top_rt_rq(rt_rq);",
            "\t\telse if (!on_rt_rq(rt_se))",
            "\t\t\tenqueue_rt_entity(rt_se, 0);",
            "",
            "\t\tif (rt_rq->highest_prio.curr < curr->prio)",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "}",
            "static void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (!rt_se) {",
            "\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "\t\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);",
            "\t}",
            "\telse if (on_rt_rq(rt_se))",
            "\t\tdequeue_rt_entity(rt_se, 0);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;",
            "}",
            "static int rt_se_boosted(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "\tstruct task_struct *p;",
            "",
            "\tif (rt_rq)",
            "\t\treturn !!rt_rq->rt_nr_boosted;",
            "",
            "\tp = rt_task_of(rt_se);",
            "\treturn p->prio != p->normal_prio;",
            "}",
            "bool sched_rt_bandwidth_account(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\treturn (hrtimer_active(&rt_b->rt_period_timer) ||",
            "\t\trt_rq->rt_time < rt_b->rt_runtime);",
            "}",
            "static void do_balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;",
            "\tint i, weight;",
            "\tu64 rt_period;",
            "",
            "\tweight = cpumask_weight(rd->span);",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\trt_period = ktime_to_ns(rt_b->rt_period);",
            "\tfor_each_cpu(i, rd->span) {",
            "\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\ts64 diff;",
            "",
            "\t\tif (iter == rt_rq)",
            "\t\t\tcontinue;",
            "",
            "\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either all rqs have inf runtime and there's nothing to steal",
            "\t\t * or __disable_runtime() below sets a specific rq to inf to",
            "\t\t * indicate its been disabled and disallow stealing.",
            "\t\t */",
            "\t\tif (iter->rt_runtime == RUNTIME_INF)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * From runqueues with spare time, take 1/n part of their",
            "\t\t * spare time, but no more than our period.",
            "\t\t */",
            "\t\tdiff = iter->rt_runtime - iter->rt_time;",
            "\t\tif (diff > 0) {",
            "\t\t\tdiff = div_u64((u64)diff, weight);",
            "\t\t\tif (rt_rq->rt_runtime + diff > rt_period)",
            "\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;",
            "\t\t\titer->rt_runtime -= diff;",
            "\t\t\trt_rq->rt_runtime += diff;",
            "\t\t\tif (rt_rq->rt_runtime == rt_period) {",
            "\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "next:",
            "\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, rt_se_boosted, sched_rt_bandwidth_account, do_balance_runtime",
          "description": "实现实时任务队列的插入/移除逻辑，跟踪运行时间消耗，通过跨CPU运行时间平衡算法实现带宽公平分配。",
          "similarity": 0.5378005504608154
        },
        {
          "chunk_id": 19,
          "file_path": "kernel/sched/rt.c",
          "start_line": 2869,
          "end_line": 2975,
          "content": [
            "static int sched_rt_global_constraints(void)",
            "{",
            "\tint ret = 0;",
            "",
            "\tmutex_lock(&rt_constraints_mutex);",
            "\tret = __rt_schedulable(NULL, 0, 0);",
            "\tmutex_unlock(&rt_constraints_mutex);",
            "",
            "\treturn ret;",
            "}",
            "int sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk)",
            "{",
            "\t/* Don't accept realtime tasks when there is no way for them to run */",
            "\tif (rt_task(tsk) && tg->rt_bandwidth.rt_runtime == 0)",
            "\t\treturn 0;",
            "",
            "\treturn 1;",
            "}",
            "static int sched_rt_global_constraints(void)",
            "{",
            "\treturn 0;",
            "}",
            "static int sched_rt_global_validate(void)",
            "{",
            "\tif ((sysctl_sched_rt_runtime != RUNTIME_INF) &&",
            "\t\t((sysctl_sched_rt_runtime > sysctl_sched_rt_period) ||",
            "\t\t ((u64)sysctl_sched_rt_runtime *",
            "\t\t\tNSEC_PER_USEC > max_rt_runtime)))",
            "\t\treturn -EINVAL;",
            "",
            "\treturn 0;",
            "}",
            "static void sched_rt_do_global(void)",
            "{",
            "}",
            "static int sched_rt_handler(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos)",
            "{",
            "\tint old_period, old_runtime;",
            "\tstatic DEFINE_MUTEX(mutex);",
            "\tint ret;",
            "",
            "\tmutex_lock(&mutex);",
            "\told_period = sysctl_sched_rt_period;",
            "\told_runtime = sysctl_sched_rt_runtime;",
            "",
            "\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "",
            "\tif (!ret && write) {",
            "\t\tret = sched_rt_global_validate();",
            "\t\tif (ret)",
            "\t\t\tgoto undo;",
            "",
            "\t\tret = sched_dl_global_validate();",
            "\t\tif (ret)",
            "\t\t\tgoto undo;",
            "",
            "\t\tret = sched_rt_global_constraints();",
            "\t\tif (ret)",
            "\t\t\tgoto undo;",
            "",
            "\t\tsched_rt_do_global();",
            "\t\tsched_dl_do_global();",
            "\t}",
            "\tif (0) {",
            "undo:",
            "\t\tsysctl_sched_rt_period = old_period;",
            "\t\tsysctl_sched_rt_runtime = old_runtime;",
            "\t}",
            "\tmutex_unlock(&mutex);",
            "",
            "\treturn ret;",
            "}",
            "static int sched_rr_handler(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos)",
            "{",
            "\tint ret;",
            "\tstatic DEFINE_MUTEX(mutex);",
            "",
            "\tmutex_lock(&mutex);",
            "\tret = proc_dointvec(table, write, buffer, lenp, ppos);",
            "\t/*",
            "\t * Make sure that internally we keep jiffies.",
            "\t * Also, writing zero resets the timeslice to default:",
            "\t */",
            "\tif (!ret && write) {",
            "\t\tsched_rr_timeslice =",
            "\t\t\tsysctl_sched_rr_timeslice <= 0 ? RR_TIMESLICE :",
            "\t\t\tmsecs_to_jiffies(sysctl_sched_rr_timeslice);",
            "",
            "\t\tif (sysctl_sched_rr_timeslice <= 0)",
            "\t\t\tsysctl_sched_rr_timeslice = jiffies_to_msecs(RR_TIMESLICE);",
            "\t}",
            "\tmutex_unlock(&mutex);",
            "",
            "\treturn ret;",
            "}",
            "void print_rt_stats(struct seq_file *m, int cpu)",
            "{",
            "\trt_rq_iter_t iter;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\trcu_read_lock();",
            "\tfor_each_rt_rq(rt_rq, iter, cpu_rq(cpu))",
            "\t\tprint_rt_rq(m, cpu, rt_rq);",
            "\trcu_read_unlock();",
            "}"
          ],
          "function_name": "sched_rt_global_constraints, sched_rt_can_attach, sched_rt_global_constraints, sched_rt_global_validate, sched_rt_do_global, sched_rt_handler, sched_rr_handler, print_rt_stats",
          "description": "实现全局实时调度策略验证与参数控制，包括实时任务附加性检查、全局运行时间/周期合法性校验及系统参数修改时的约束检查与应用。",
          "similarity": 0.5354923009872437
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5115556716918945,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 658,
          "end_line": 845,
          "content": [
            "int rcu_needs_cpu(void)",
            "{",
            "\treturn !rcu_segcblist_empty(&this_cpu_ptr(&rcu_data)->cblist) &&",
            "\t\t!rcu_rdp_is_offloaded(this_cpu_ptr(&rcu_data));",
            "}",
            "static void rcu_disable_urgency_upon_qs(struct rcu_data *rdp)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rdp->mynode);",
            "\tWRITE_ONCE(rdp->rcu_urgent_qs, false);",
            "\tWRITE_ONCE(rdp->rcu_need_heavy_qs, false);",
            "\tif (tick_nohz_full_cpu(rdp->cpu) && rdp->rcu_forced_tick) {",
            "\t\ttick_dep_clear_cpu(rdp->cpu, TICK_DEP_BIT_RCU);",
            "\t\tWRITE_ONCE(rdp->rcu_forced_tick, false);",
            "\t}",
            "}",
            "notrace bool rcu_is_watching(void)",
            "{",
            "\tbool ret;",
            "",
            "\tpreempt_disable_notrace();",
            "\tret = !rcu_dynticks_curr_cpu_in_eqs();",
            "\tpreempt_enable_notrace();",
            "\treturn ret;",
            "}",
            "void rcu_request_urgent_qs_task(struct task_struct *t)",
            "{",
            "\tint cpu;",
            "",
            "\tbarrier();",
            "\tcpu = task_cpu(t);",
            "\tif (!task_curr(t))",
            "\t\treturn; /* This task is not running on that CPU. */",
            "\tsmp_store_release(per_cpu_ptr(&rcu_data.rcu_urgent_qs, cpu), true);",
            "}",
            "static void rcu_gpnum_ovf(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (ULONG_CMP_LT(rcu_seq_current(&rdp->gp_seq) + ULONG_MAX / 4,",
            "\t\t\t rnp->gp_seq))",
            "\t\tWRITE_ONCE(rdp->gpwrap, true);",
            "\tif (ULONG_CMP_LT(rdp->rcu_iw_gp_seq + ULONG_MAX / 4, rnp->gp_seq))",
            "\t\trdp->rcu_iw_gp_seq = rnp->gp_seq + ULONG_MAX / 4;",
            "}",
            "static int dyntick_save_progress_counter(struct rcu_data *rdp)",
            "{",
            "\trdp->dynticks_snap = rcu_dynticks_snap(rdp->cpu);",
            "\tif (rcu_dynticks_in_eqs(rdp->dynticks_snap)) {",
            "\t\ttrace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS(\"dti\"));",
            "\t\trcu_gpnum_ovf(rdp->mynode, rdp);",
            "\t\treturn 1;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int rcu_implicit_dynticks_qs(struct rcu_data *rdp)",
            "{",
            "\tunsigned long jtsq;",
            "\tint ret = 0;",
            "\tstruct rcu_node *rnp = rdp->mynode;",
            "",
            "\t/*",
            "\t * If the CPU passed through or entered a dynticks idle phase with",
            "\t * no active irq/NMI handlers, then we can safely pretend that the CPU",
            "\t * already acknowledged the request to pass through a quiescent",
            "\t * state.  Either way, that CPU cannot possibly be in an RCU",
            "\t * read-side critical section that started before the beginning",
            "\t * of the current RCU grace period.",
            "\t */",
            "\tif (rcu_dynticks_in_eqs_since(rdp, rdp->dynticks_snap)) {",
            "\t\ttrace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS(\"dti\"));",
            "\t\trcu_gpnum_ovf(rnp, rdp);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\t/*",
            "\t * Complain if a CPU that is considered to be offline from RCU's",
            "\t * perspective has not yet reported a quiescent state.  After all,",
            "\t * the offline CPU should have reported a quiescent state during",
            "\t * the CPU-offline process, or, failing that, by rcu_gp_init()",
            "\t * if it ran concurrently with either the CPU going offline or the",
            "\t * last task on a leaf rcu_node structure exiting its RCU read-side",
            "\t * critical section while all CPUs corresponding to that structure",
            "\t * are offline.  This added warning detects bugs in any of these",
            "\t * code paths.",
            "\t *",
            "\t * The rcu_node structure's ->lock is held here, which excludes",
            "\t * the relevant portions the CPU-hotplug code, the grace-period",
            "\t * initialization code, and the rcu_read_unlock() code paths.",
            "\t *",
            "\t * For more detail, please refer to the \"Hotplug CPU\" section",
            "\t * of RCU's Requirements documentation.",
            "\t */",
            "\tif (WARN_ON_ONCE(!rcu_rdp_cpu_online(rdp))) {",
            "\t\tstruct rcu_node *rnp1;",
            "",
            "\t\tpr_info(\"%s: grp: %d-%d level: %d ->gp_seq %ld ->completedqs %ld\\n\",",
            "\t\t\t__func__, rnp->grplo, rnp->grphi, rnp->level,",
            "\t\t\t(long)rnp->gp_seq, (long)rnp->completedqs);",
            "\t\tfor (rnp1 = rnp; rnp1; rnp1 = rnp1->parent)",
            "\t\t\tpr_info(\"%s: %d:%d ->qsmask %#lx ->qsmaskinit %#lx ->qsmaskinitnext %#lx ->rcu_gp_init_mask %#lx\\n\",",
            "\t\t\t\t__func__, rnp1->grplo, rnp1->grphi, rnp1->qsmask, rnp1->qsmaskinit, rnp1->qsmaskinitnext, rnp1->rcu_gp_init_mask);",
            "\t\tpr_info(\"%s %d: %c online: %ld(%d) offline: %ld(%d)\\n\",",
            "\t\t\t__func__, rdp->cpu, \".o\"[rcu_rdp_cpu_online(rdp)],",
            "\t\t\t(long)rdp->rcu_onl_gp_seq, rdp->rcu_onl_gp_flags,",
            "\t\t\t(long)rdp->rcu_ofl_gp_seq, rdp->rcu_ofl_gp_flags);",
            "\t\treturn 1; /* Break things loose after complaining. */",
            "\t}",
            "",
            "\t/*",
            "\t * A CPU running for an extended time within the kernel can",
            "\t * delay RCU grace periods: (1) At age jiffies_to_sched_qs,",
            "\t * set .rcu_urgent_qs, (2) At age 2*jiffies_to_sched_qs, set",
            "\t * both .rcu_need_heavy_qs and .rcu_urgent_qs.  Note that the",
            "\t * unsynchronized assignments to the per-CPU rcu_need_heavy_qs",
            "\t * variable are safe because the assignments are repeated if this",
            "\t * CPU failed to pass through a quiescent state.  This code",
            "\t * also checks .jiffies_resched in case jiffies_to_sched_qs",
            "\t * is set way high.",
            "\t */",
            "\tjtsq = READ_ONCE(jiffies_to_sched_qs);",
            "\tif (!READ_ONCE(rdp->rcu_need_heavy_qs) &&",
            "\t    (time_after(jiffies, rcu_state.gp_start + jtsq * 2) ||",
            "\t     time_after(jiffies, rcu_state.jiffies_resched) ||",
            "\t     rcu_state.cbovld)) {",
            "\t\tWRITE_ONCE(rdp->rcu_need_heavy_qs, true);",
            "\t\t/* Store rcu_need_heavy_qs before rcu_urgent_qs. */",
            "\t\tsmp_store_release(&rdp->rcu_urgent_qs, true);",
            "\t} else if (time_after(jiffies, rcu_state.gp_start + jtsq)) {",
            "\t\tWRITE_ONCE(rdp->rcu_urgent_qs, true);",
            "\t}",
            "",
            "\t/*",
            "\t * NO_HZ_FULL CPUs can run in-kernel without rcu_sched_clock_irq!",
            "\t * The above code handles this, but only for straight cond_resched().",
            "\t * And some in-kernel loops check need_resched() before calling",
            "\t * cond_resched(), which defeats the above code for CPUs that are",
            "\t * running in-kernel with scheduling-clock interrupts disabled.",
            "\t * So hit them over the head with the resched_cpu() hammer!",
            "\t */",
            "\tif (tick_nohz_full_cpu(rdp->cpu) &&",
            "\t    (time_after(jiffies, READ_ONCE(rdp->last_fqs_resched) + jtsq * 3) ||",
            "\t     rcu_state.cbovld)) {",
            "\t\tWRITE_ONCE(rdp->rcu_urgent_qs, true);",
            "\t\tWRITE_ONCE(rdp->last_fqs_resched, jiffies);",
            "\t\tret = -1;",
            "\t}",
            "",
            "\t/*",
            "\t * If more than halfway to RCU CPU stall-warning time, invoke",
            "\t * resched_cpu() more frequently to try to loosen things up a bit.",
            "\t * Also check to see if the CPU is getting hammered with interrupts,",
            "\t * but only once per grace period, just to keep the IPIs down to",
            "\t * a dull roar.",
            "\t */",
            "\tif (time_after(jiffies, rcu_state.jiffies_resched)) {",
            "\t\tif (time_after(jiffies,",
            "\t\t\t       READ_ONCE(rdp->last_fqs_resched) + jtsq)) {",
            "\t\t\tWRITE_ONCE(rdp->last_fqs_resched, jiffies);",
            "\t\t\tret = -1;",
            "\t\t}",
            "\t\tif (IS_ENABLED(CONFIG_IRQ_WORK) &&",
            "\t\t    !rdp->rcu_iw_pending && rdp->rcu_iw_gp_seq != rnp->gp_seq &&",
            "\t\t    (rnp->ffmask & rdp->grpmask)) {",
            "\t\t\trdp->rcu_iw_pending = true;",
            "\t\t\trdp->rcu_iw_gp_seq = rnp->gp_seq;",
            "\t\t\tirq_work_queue_on(&rdp->rcu_iw, rdp->cpu);",
            "\t\t}",
            "",
            "\t\tif (rcu_cpu_stall_cputime && rdp->snap_record.gp_seq != rdp->gp_seq) {",
            "\t\t\tint cpu = rdp->cpu;",
            "\t\t\tstruct rcu_snap_record *rsrp;",
            "\t\t\tstruct kernel_cpustat *kcsp;",
            "",
            "\t\t\tkcsp = &kcpustat_cpu(cpu);",
            "",
            "\t\t\trsrp = &rdp->snap_record;",
            "\t\t\trsrp->cputime_irq     = kcpustat_field(kcsp, CPUTIME_IRQ, cpu);",
            "\t\t\trsrp->cputime_softirq = kcpustat_field(kcsp, CPUTIME_SOFTIRQ, cpu);",
            "\t\t\trsrp->cputime_system  = kcpustat_field(kcsp, CPUTIME_SYSTEM, cpu);",
            "\t\t\trsrp->nr_hardirqs = kstat_cpu_irqs_sum(cpu) + arch_irq_stat_cpu(cpu);",
            "\t\t\trsrp->nr_softirqs = kstat_cpu_softirqs_sum(cpu);",
            "\t\t\trsrp->nr_csw = nr_context_switches_cpu(cpu);",
            "\t\t\trsrp->jiffies = jiffies;",
            "\t\t\trsrp->gp_seq = rdp->gp_seq;",
            "\t\t}",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "rcu_needs_cpu, rcu_disable_urgency_upon_qs, rcu_is_watching, rcu_request_urgent_qs_task, rcu_gpnum_ovf, dyntick_save_progress_counter, rcu_implicit_dynticks_qs",
          "description": "处理RCU紧迫性需求判定和隐式动态tick quiescent状态检测，通过时间阈值触发CPU唤醒以避免RCU阻塞。",
          "similarity": 0.5662534832954407
        },
        {
          "chunk_id": 17,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3121,
          "end_line": 3265,
          "content": [
            "static bool",
            "need_offload_krc(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\tif (!list_empty(&krcp->bulk_head[i]))",
            "\t\t\treturn true;",
            "",
            "\treturn !!READ_ONCE(krcp->head);",
            "}",
            "static bool",
            "need_wait_for_krwp_work(struct kfree_rcu_cpu_work *krwp)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\tif (!list_empty(&krwp->bulk_head_free[i]))",
            "\t\t\treturn true;",
            "",
            "\treturn !!krwp->head_free;",
            "}",
            "static int krc_count(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tint sum = atomic_read(&krcp->head_count);",
            "\tint i;",
            "",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\tsum += atomic_read(&krcp->bulk_count[i]);",
            "",
            "\treturn sum;",
            "}",
            "static void",
            "__schedule_delayed_monitor_work(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tlong delay, delay_left;",
            "",
            "\tdelay = krc_count(krcp) >= KVFREE_BULK_MAX_ENTR ? 1:KFREE_DRAIN_JIFFIES;",
            "\tif (delayed_work_pending(&krcp->monitor_work)) {",
            "\t\tdelay_left = krcp->monitor_work.timer.expires - jiffies;",
            "\t\tif (delay < delay_left)",
            "\t\t\tmod_delayed_work(system_unbound_wq, &krcp->monitor_work, delay);",
            "\t\treturn;",
            "\t}",
            "\tqueue_delayed_work(system_unbound_wq, &krcp->monitor_work, delay);",
            "}",
            "static void",
            "schedule_delayed_monitor_work(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\t__schedule_delayed_monitor_work(krcp);",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "}",
            "static void",
            "kvfree_rcu_drain_ready(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tstruct list_head bulk_ready[FREE_N_CHANNELS];",
            "\tstruct kvfree_rcu_bulk_data *bnode, *n;",
            "\tstruct rcu_head *head_ready = NULL;",
            "\tunsigned long flags;",
            "\tint i;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++) {",
            "\t\tINIT_LIST_HEAD(&bulk_ready[i]);",
            "",
            "\t\tlist_for_each_entry_safe_reverse(bnode, n, &krcp->bulk_head[i], list) {",
            "\t\t\tif (!poll_state_synchronize_rcu_full(&bnode->gp_snap))",
            "\t\t\t\tbreak;",
            "",
            "\t\t\tatomic_sub(bnode->nr_records, &krcp->bulk_count[i]);",
            "\t\t\tlist_move(&bnode->list, &bulk_ready[i]);",
            "\t\t}",
            "\t}",
            "",
            "\tif (krcp->head && poll_state_synchronize_rcu(krcp->head_gp_snap)) {",
            "\t\thead_ready = krcp->head;",
            "\t\tatomic_set(&krcp->head_count, 0);",
            "\t\tWRITE_ONCE(krcp->head, NULL);",
            "\t}",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++) {",
            "\t\tlist_for_each_entry_safe(bnode, n, &bulk_ready[i], list)",
            "\t\t\tkvfree_rcu_bulk(krcp, bnode, i);",
            "\t}",
            "",
            "\tif (head_ready)",
            "\t\tkvfree_rcu_list(head_ready);",
            "}",
            "static bool",
            "kvfree_rcu_queue_batch(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tunsigned long flags;",
            "\tbool queued = false;",
            "\tint i, j;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "",
            "\t// Attempt to start a new batch.",
            "\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\tstruct kfree_rcu_cpu_work *krwp = &(krcp->krw_arr[i]);",
            "",
            "\t\t// Try to detach bulk_head or head and attach it, only when",
            "\t\t// all channels are free.  Any channel is not free means at krwp",
            "\t\t// there is on-going rcu work to handle krwp's free business.",
            "\t\tif (need_wait_for_krwp_work(krwp))",
            "\t\t\tcontinue;",
            "",
            "\t\t// kvfree_rcu_drain_ready() might handle this krcp, if so give up.",
            "\t\tif (need_offload_krc(krcp)) {",
            "\t\t\t// Channel 1 corresponds to the SLAB-pointer bulk path.",
            "\t\t\t// Channel 2 corresponds to vmalloc-pointer bulk path.",
            "\t\t\tfor (j = 0; j < FREE_N_CHANNELS; j++) {",
            "\t\t\t\tif (list_empty(&krwp->bulk_head_free[j])) {",
            "\t\t\t\t\tatomic_set(&krcp->bulk_count[j], 0);",
            "\t\t\t\t\tlist_replace_init(&krcp->bulk_head[j],",
            "\t\t\t\t\t\t&krwp->bulk_head_free[j]);",
            "\t\t\t\t}",
            "\t\t\t}",
            "",
            "\t\t\t// Channel 3 corresponds to both SLAB and vmalloc",
            "\t\t\t// objects queued on the linked list.",
            "\t\t\tif (!krwp->head_free) {",
            "\t\t\t\tkrwp->head_free = krcp->head;",
            "\t\t\t\tget_state_synchronize_rcu_full(&krwp->head_free_gp_snap);",
            "\t\t\t\tatomic_set(&krcp->head_count, 0);",
            "\t\t\t\tWRITE_ONCE(krcp->head, NULL);",
            "\t\t\t}",
            "",
            "\t\t\t// One work is per one batch, so there are three",
            "\t\t\t// \"free channels\", the batch can handle. Break",
            "\t\t\t// the loop since it is done with this CPU thus",
            "\t\t\t// queuing an RCU work is _always_ success here.",
            "\t\t\tqueued = queue_rcu_work(system_unbound_wq, &krwp->rcu_work);",
            "\t\t\tWARN_ON_ONCE(!queued);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "\treturn queued;",
            "}"
          ],
          "function_name": "need_offload_krc, need_wait_for_krwp_work, krc_count, __schedule_delayed_monitor_work, schedule_delayed_monitor_work, kvfree_rcu_drain_ready, kvfree_rcu_queue_batch",
          "description": "调度延迟清理工作，通过统计待处理对象数量动态调整清理时机，支持分批处理以优化系统资源利用。",
          "similarity": 0.4984939694404602
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 203,
          "end_line": 314,
          "content": [
            "int rcu_get_gp_kthreads_prio(void)",
            "{",
            "\treturn kthread_prio;",
            "}",
            "static int rcu_gp_in_progress(void)",
            "{",
            "\treturn rcu_seq_state(rcu_seq_current(&rcu_state.gp_seq));",
            "}",
            "static long rcu_get_n_cbs_cpu(int cpu)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\tif (rcu_segcblist_is_enabled(&rdp->cblist))",
            "\t\treturn rcu_segcblist_n_cbs(&rdp->cblist);",
            "\treturn 0;",
            "}",
            "void rcu_softirq_qs(void)",
            "{",
            "\trcu_qs();",
            "\trcu_preempt_deferred_qs(current);",
            "\trcu_tasks_qs(current, false);",
            "}",
            "static void rcu_dynticks_eqs_online(void)",
            "{",
            "\tif (ct_dynticks() & RCU_DYNTICKS_IDX)",
            "\t\treturn;",
            "\tct_state_inc(RCU_DYNTICKS_IDX);",
            "}",
            "static int rcu_dynticks_snap(int cpu)",
            "{",
            "\tsmp_mb();  // Fundamental RCU ordering guarantee.",
            "\treturn ct_dynticks_cpu_acquire(cpu);",
            "}",
            "static bool rcu_dynticks_in_eqs(int snap)",
            "{",
            "\treturn !(snap & RCU_DYNTICKS_IDX);",
            "}",
            "static bool rcu_dynticks_in_eqs_since(struct rcu_data *rdp, int snap)",
            "{",
            "\treturn snap != rcu_dynticks_snap(rdp->cpu);",
            "}",
            "bool rcu_dynticks_zero_in_eqs(int cpu, int *vp)",
            "{",
            "\tint snap;",
            "",
            "\t// If not quiescent, force back to earlier extended quiescent state.",
            "\tsnap = ct_dynticks_cpu(cpu) & ~RCU_DYNTICKS_IDX;",
            "\tsmp_rmb(); // Order ->dynticks and *vp reads.",
            "\tif (READ_ONCE(*vp))",
            "\t\treturn false;  // Non-zero, so report failure;",
            "\tsmp_rmb(); // Order *vp read and ->dynticks re-read.",
            "",
            "\t// If still in the same extended quiescent state, we are good!",
            "\treturn snap == ct_dynticks_cpu(cpu);",
            "}",
            "notrace void rcu_momentary_dyntick_idle(void)",
            "{",
            "\tint seq;",
            "",
            "\traw_cpu_write(rcu_data.rcu_need_heavy_qs, false);",
            "\tseq = ct_state_inc(2 * RCU_DYNTICKS_IDX);",
            "\t/* It is illegal to call this from idle state. */",
            "\tWARN_ON_ONCE(!(seq & RCU_DYNTICKS_IDX));",
            "\trcu_preempt_deferred_qs(current);",
            "}",
            "static int rcu_is_cpu_rrupt_from_idle(void)",
            "{",
            "\tlong nesting;",
            "",
            "\t/*",
            "\t * Usually called from the tick; but also used from smp_function_call()",
            "\t * for expedited grace periods. This latter can result in running from",
            "\t * the idle task, instead of an actual IPI.",
            "\t */",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\t/* Check for counter underflows */",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nesting() < 0,",
            "\t\t\t \"RCU dynticks_nesting counter underflow!\");",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nmi_nesting() <= 0,",
            "\t\t\t \"RCU dynticks_nmi_nesting counter underflow/zero!\");",
            "",
            "\t/* Are we at first interrupt nesting level? */",
            "\tnesting = ct_dynticks_nmi_nesting();",
            "\tif (nesting > 1)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * If we're not in an interrupt, we must be in the idle task!",
            "\t */",
            "\tWARN_ON_ONCE(!nesting && !is_idle_task(current));",
            "",
            "\t/* Does CPU appear to be idle from an RCU standpoint? */",
            "\treturn ct_dynticks_nesting() == 0;",
            "}",
            "static void adjust_jiffies_till_sched_qs(void)",
            "{",
            "\tunsigned long j;",
            "",
            "\t/* If jiffies_till_sched_qs was specified, respect the request. */",
            "\tif (jiffies_till_sched_qs != ULONG_MAX) {",
            "\t\tWRITE_ONCE(jiffies_to_sched_qs, jiffies_till_sched_qs);",
            "\t\treturn;",
            "\t}",
            "\t/* Otherwise, set to third fqs scan, but bound below on large system. */",
            "\tj = READ_ONCE(jiffies_till_first_fqs) +",
            "\t\t      2 * READ_ONCE(jiffies_till_next_fqs);",
            "\tif (j < HZ / 10 + nr_cpu_ids / RCU_JIFFIES_FQS_DIV)",
            "\t\tj = HZ / 10 + nr_cpu_ids / RCU_JIFFIES_FQS_DIV;",
            "\tpr_info(\"RCU calculated value of scheduler-enlistment delay is %ld jiffies.\\n\", j);",
            "\tWRITE_ONCE(jiffies_to_sched_qs, j);",
            "}"
          ],
          "function_name": "rcu_get_gp_kthreads_prio, rcu_gp_in_progress, rcu_get_n_cbs_cpu, rcu_softirq_qs, rcu_dynticks_eqs_online, rcu_dynticks_snap, rcu_dynticks_in_eqs, rcu_dynticks_in_eqs_since, rcu_dynticks_zero_in_eqs, rcu_momentary_dyntick_idle, rcu_is_cpu_rrupt_from_idle, adjust_jiffies_till_sched_qs",
          "description": "提供RCU grace period状态查询和动态tick管理接口，用于检测grace period进度并协调中断处理与延时策略。",
          "similarity": 0.49619707465171814
        },
        {
          "chunk_id": 19,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3436,
          "end_line": 3574,
          "content": [
            "void kvfree_call_rcu(struct rcu_head *head, void *ptr)",
            "{",
            "\tunsigned long flags;",
            "\tstruct kfree_rcu_cpu *krcp;",
            "\tbool success;",
            "",
            "\t/*",
            "\t * Please note there is a limitation for the head-less",
            "\t * variant, that is why there is a clear rule for such",
            "\t * objects: it can be used from might_sleep() context",
            "\t * only. For other places please embed an rcu_head to",
            "\t * your data.",
            "\t */",
            "\tif (!head)",
            "\t\tmight_sleep();",
            "",
            "\t// Queue the object but don't yet schedule the batch.",
            "\tif (debug_rcu_head_queue(ptr)) {",
            "\t\t// Probable double kfree_rcu(), just leak.",
            "\t\tWARN_ONCE(1, \"%s(): Double-freed call. rcu_head %p\\n\",",
            "\t\t\t  __func__, head);",
            "",
            "\t\t// Mark as success and leave.",
            "\t\treturn;",
            "\t}",
            "",
            "\tkasan_record_aux_stack_noalloc(ptr);",
            "\tsuccess = add_ptr_to_bulk_krc_lock(&krcp, &flags, ptr, !head);",
            "\tif (!success) {",
            "\t\trun_page_cache_worker(krcp);",
            "",
            "\t\tif (head == NULL)",
            "\t\t\t// Inline if kvfree_rcu(one_arg) call.",
            "\t\t\tgoto unlock_return;",
            "",
            "\t\thead->func = ptr;",
            "\t\thead->next = krcp->head;",
            "\t\tWRITE_ONCE(krcp->head, head);",
            "\t\tatomic_inc(&krcp->head_count);",
            "",
            "\t\t// Take a snapshot for this krcp.",
            "\t\tkrcp->head_gp_snap = get_state_synchronize_rcu();",
            "\t\tsuccess = true;",
            "\t}",
            "",
            "\t/*",
            "\t * The kvfree_rcu() caller considers the pointer freed at this point",
            "\t * and likely removes any references to it. Since the actual slab",
            "\t * freeing (and kmemleak_free()) is deferred, tell kmemleak to ignore",
            "\t * this object (no scanning or false positives reporting).",
            "\t */",
            "\tkmemleak_ignore(ptr);",
            "",
            "\t// Set timer to drain after KFREE_DRAIN_JIFFIES.",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_RUNNING)",
            "\t\t__schedule_delayed_monitor_work(krcp);",
            "",
            "unlock_return:",
            "\tkrc_this_cpu_unlock(krcp, flags);",
            "",
            "\t/*",
            "\t * Inline kvfree() after synchronize_rcu(). We can do",
            "\t * it from might_sleep() context only, so the current",
            "\t * CPU can pass the QS state.",
            "\t */",
            "\tif (!success) {",
            "\t\tdebug_rcu_head_unqueue((struct rcu_head *) ptr);",
            "\t\tsynchronize_rcu();",
            "\t\tkvfree(ptr);",
            "\t}",
            "}",
            "void kvfree_rcu_barrier(void)",
            "{",
            "\tstruct kfree_rcu_cpu_work *krwp;",
            "\tstruct kfree_rcu_cpu *krcp;",
            "\tbool queued;",
            "\tint i, cpu;",
            "",
            "\t/*",
            "\t * Firstly we detach objects and queue them over an RCU-batch",
            "\t * for all CPUs. Finally queued works are flushed for each CPU.",
            "\t *",
            "\t * Please note. If there are outstanding batches for a particular",
            "\t * CPU, those have to be finished first following by queuing a new.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tkrcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\t/*",
            "\t\t * Check if this CPU has any objects which have been queued for a",
            "\t\t * new GP completion. If not(means nothing to detach), we are done",
            "\t\t * with it. If any batch is pending/running for this \"krcp\", below",
            "\t\t * per-cpu flush_rcu_work() waits its completion(see last step).",
            "\t\t */",
            "\t\tif (!need_offload_krc(krcp))",
            "\t\t\tcontinue;",
            "",
            "\t\twhile (1) {",
            "\t\t\t/*",
            "\t\t\t * If we are not able to queue a new RCU work it means:",
            "\t\t\t * - batches for this CPU are still in flight which should",
            "\t\t\t *   be flushed first and then repeat;",
            "\t\t\t * - no objects to detach, because of concurrency.",
            "\t\t\t */",
            "\t\t\tqueued = kvfree_rcu_queue_batch(krcp);",
            "",
            "\t\t\t/*",
            "\t\t\t * Bail out, if there is no need to offload this \"krcp\"",
            "\t\t\t * anymore. As noted earlier it can run concurrently.",
            "\t\t\t */",
            "\t\t\tif (queued || !need_offload_krc(krcp))",
            "\t\t\t\tbreak;",
            "",
            "\t\t\t/* There are ongoing batches. */",
            "\t\t\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\t\t\tkrwp = &(krcp->krw_arr[i]);",
            "\t\t\t\tflush_rcu_work(&krwp->rcu_work);",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Now we guarantee that all objects are flushed.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tkrcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\t/*",
            "\t\t * A monitor work can drain ready to reclaim objects",
            "\t\t * directly. Wait its completion if running or pending.",
            "\t\t */",
            "\t\tcancel_delayed_work_sync(&krcp->monitor_work);",
            "",
            "\t\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\t\tkrwp = &(krcp->krw_arr[i]);",
            "\t\t\tflush_rcu_work(&krwp->rcu_work);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "kvfree_call_rcu, kvfree_rcu_barrier",
          "description": "实现基于RCU的延迟内存释放接口，提供安全释放路径并保证内存屏障语义，强制同步清理所有挂起的释放请求。",
          "similarity": 0.4961928725242615
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 438,
          "end_line": 544,
          "content": [
            "static int param_set_first_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : j);",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "static int param_set_next_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : (j ?: 1));",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "unsigned long rcu_get_gp_seq(void)",
            "{",
            "\treturn READ_ONCE(rcu_state.gp_seq);",
            "}",
            "unsigned long rcu_exp_batches_completed(void)",
            "{",
            "\treturn rcu_state.expedited_sequence;",
            "}",
            "void rcutorture_get_gp_data(enum rcutorture_type test_type, int *flags,",
            "\t\t\t    unsigned long *gp_seq)",
            "{",
            "\tswitch (test_type) {",
            "\tcase RCU_FLAVOR:",
            "\t\t*flags = READ_ONCE(rcu_state.gp_flags);",
            "\t\t*gp_seq = rcu_seq_current(&rcu_state.gp_seq);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "}",
            "static void late_wakeup_func(struct irq_work *work)",
            "{",
            "}",
            "noinstr void rcu_irq_work_resched(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tif (IS_ENABLED(CONFIG_GENERIC_ENTRY) && !(current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tif (IS_ENABLED(CONFIG_KVM_XFER_TO_GUEST_WORK) && (current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tinstrumentation_begin();",
            "\tif (do_nocb_deferred_wakeup(rdp) && need_resched()) {",
            "\t\tirq_work_queue(this_cpu_ptr(&late_wakeup_work));",
            "\t}",
            "\tinstrumentation_end();",
            "}",
            "void rcu_irq_exit_check_preempt(void)",
            "{",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nesting() <= 0,",
            "\t\t\t \"RCU dynticks_nesting counter underflow/zero!\");",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nmi_nesting() !=",
            "\t\t\t DYNTICK_IRQ_NONIDLE,",
            "\t\t\t \"Bad RCU  dynticks_nmi_nesting counter\\n\");",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"RCU in extended quiescent state!\");",
            "}",
            "void __rcu_irq_enter_check_tick(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\t// If we're here from NMI there's nothing to do.",
            "\tif (in_nmi())",
            "\t\treturn;",
            "",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"Illegal rcu_irq_enter_check_tick() from extended quiescent state\");",
            "",
            "\tif (!tick_nohz_full_cpu(rdp->cpu) ||",
            "\t    !READ_ONCE(rdp->rcu_urgent_qs) ||",
            "\t    READ_ONCE(rdp->rcu_forced_tick)) {",
            "\t\t// RCU doesn't need nohz_full help from this CPU, or it is",
            "\t\t// already getting that help.",
            "\t\treturn;",
            "\t}",
            "",
            "\t// We get here only when not in an extended quiescent state and",
            "\t// from interrupts (as opposed to NMIs).  Therefore, (1) RCU is",
            "\t// already watching and (2) The fact that we are in an interrupt",
            "\t// handler and that the rcu_node lock is an irq-disabled lock",
            "\t// prevents self-deadlock.  So we can safely recheck under the lock.",
            "\t// Note that the nohz_full state currently cannot change.",
            "\traw_spin_lock_rcu_node(rdp->mynode);",
            "\tif (READ_ONCE(rdp->rcu_urgent_qs) && !rdp->rcu_forced_tick) {",
            "\t\t// A nohz_full CPU is in the kernel and RCU needs a",
            "\t\t// quiescent state.  Turn on the tick!",
            "\t\tWRITE_ONCE(rdp->rcu_forced_tick, true);",
            "\t\ttick_dep_set_cpu(rdp->cpu, TICK_DEP_BIT_RCU);",
            "\t}",
            "\traw_spin_unlock_rcu_node(rdp->mynode);",
            "}"
          ],
          "function_name": "param_set_first_fqs_jiffies, param_set_next_fqs_jiffies, rcu_get_gp_seq, rcu_exp_batches_completed, rcutorture_get_gp_data, late_wakeup_func, rcu_irq_work_resched, rcu_irq_exit_check_preempt, __rcu_irq_enter_check_tick",
          "description": "实现参数配置回调函数和中断上下文RCU工作重排逻辑，管理grace period序列号读取及nohz_full模式下的tick依赖关系。",
          "similarity": 0.4955446124076843
        }
      ]
    }
  ]
}