{
  "query": "进程创建与RCU机制的交互",
  "timestamp": "2025-12-26 00:39:01",
  "retrieved_files": [
    {
      "source_file": "kernel/rcu/rcu.h",
      "md_summary": "> 自动生成时间: 2025-10-25 15:38:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\rcu.h`\n\n---\n\n# `rcu/rcu.h` 技术文档\n\n## 1. 文件概述\n\n`rcu/rcu.h` 是 Linux 内核中 RCU（Read-Copy Update）机制的核心头文件之一，定义了多个 RCU 实现（如 TREE_RCU、SRCU、 polled RCU 等）共享的通用数据结构、宏和内联函数。该文件主要负责 **grace-period（宽限期）序列号的管理逻辑**，通过将序列号的低两位用作状态标志、高位用作计数器，实现对 RCU 宽限期生命周期的精确跟踪和同步控制。\n\n## 2. 核心功能\n\n### 主要宏定义\n- `RCU_SEQ_CTR_SHIFT`：序列号中计数器部分的偏移位（值为 2）。\n- `RCU_SEQ_STATE_MASK`：低 2 位掩码，用于提取状态标志。\n- `RCU_GET_STATE_COMPLETED`：用于 polled RCU 的特殊完成状态值（0x1）。\n- `SRCU_SNP_INIT_SEQ` / `SRCU_STATE_IDLE` / `SRCU_STATE_SCAN1` / `SRCU_STATE_SCAN2`：SRCU 特定的状态值（注释中定义，实际值在其他文件中）。\n\n### 主要内联函数\n- **序列号解析**：\n  - `rcu_seq_ctr()`：提取序列号中的计数器部分。\n  - `rcu_seq_state()`：提取序列号中的状态标志部分。\n- **序列号更新**：\n  - `rcu_seq_set_state()`：设置序列号的状态位。\n  - `rcu_seq_start()`：标记宽限期开始。\n  - `rcu_seq_end()`：标记宽限期结束。\n  - `rcu_seq_endval()`：计算宽限期结束时的目标序列号。\n- **快照与判断**：\n  - `rcu_seq_snap()`：获取一个“安全快照”，用于判断未来何时宽限期完成。\n  - `rcu_seq_current()`：获取当前序列号（无内存屏障）。\n  - `rcu_seq_started()`：判断对应操作是否已开始。\n  - `rcu_seq_done()` / `rcu_seq_done_exact()`：判断宽限期是否已完成。\n  - `rcu_seq_completed_gp()`：判断自旧序列号以来是否已完成至少一个宽限期。\n  - `rcu_seq_new_gp()`：判断自旧序列号以来是否有新宽限期开始。\n  - `rcu_seq_diff()`：估算两个序列号之间经过的完整宽限期数量。\n- **调试支持**：\n  - `debug_rcu_head_queue()` / `debug_rcu_head_unqueue()`：在启用 `CONFIG_DEBUG_OBJECTS_RCU_HEAD` 时，用于调试 RCU 回调对象的状态转换。\n  - `debug_rcu_head_callback()`：检查 RCU 回调函数指针是否为空，若为空则打印对象信息用于调试。\n- **启动抑制**：\n  - `rcu_stall_is_suppressed_at_boot()`：判断是否在启动阶段抑制 RCU CPU 停滞检测。\n\n### 全局变量声明\n- `sysctl_sched_rt_runtime`：外部声明，与调度器相关（此处仅为引用）。\n- `rcu_cpu_stall_suppress_at_boot`：控制启动期间是否抑制 RCU 停滞警告。\n\n## 3. 关键实现\n\n### Grace-Period 序列号编码\n序列号 `unsigned long s` 被划分为两部分：\n- **高 `(sizeof(long)*8 - 2)` 位**：宽限期计数器（每次完整宽限期结束后递增）。\n- **低 2 位**：状态标志，用于表示宽限期的当前阶段：\n  - `00`：无宽限期进行中（空闲）。\n  - 非零（如 `01` 或 `10`）：宽限期正在进行中。\n\n### 宽限期生命周期管理\n- **开始**：`rcu_seq_start()` 将序列号加 1，使状态变为 `01`，并插入写内存屏障确保后续更新操作在计数器递增之后。\n- **结束**：`rcu_seq_end()` 先插入读内存屏障确保之前更新完成，然后将序列号设置为 `(当前值 | 0x3) + 1`，即清除状态位并递增计数器。\n\n### 快照机制 (`rcu_seq_snap`)\n该函数返回一个“未来安全值”：\n```c\ns = (当前序列号 + 2*RCU_SEQ_STATE_MASK + 1) & ~RCU_SEQ_STATE_MASK;\n```\n此值确保：当实际序列号 ≥ `s` 时，至少有一个完整的宽限期已覆盖调用 `rcu_seq_snap()` 之后的所有读端临界区。\n\n### 宽限期完成判断\n- `rcu_seq_done(sp, s)`：若当前序列号 ≥ 快照值 `s`，则认为宽限期已完成。\n- `rcu_seq_done_exact()`：在不考虑 `ULONG_MAX/2` 安全裕度的情况下进行精确判断，用于特定场景。\n\n### 宽限期差异计算 (`rcu_seq_diff`)\n通过位运算估算两个序列号之间经过的完整宽限期数量，考虑了状态位的影响，并保证最小返回值为 1（若确实未经过完整宽限期）。\n\n### 调试对象支持\n当启用 `CONFIG_DEBUG_OBJECTS_RCU_HEAD` 时，通过 `debug_obj` 框架跟踪 `struct rcu_head` 对象的状态（`READY` ↔ `QUEUED`），防止重复入队或非法释放。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/slab.h>`：用于 `kmem_dump_obj()`（调试回调函数为空时打印对象信息）。\n  - `<trace/events/rcu.h>`：RCU 相关的 tracepoint 定义（尽管本文件未直接使用，但为一致性包含）。\n- **配置依赖**：\n  - `CONFIG_DEBUG_OBJECTS_RCU_HEAD`：启用 RCU 回调对象的调试跟踪。\n- **外部符号**：\n  - `rcuhead_debug_descr`：调试对象描述符（定义在 `rcupdate.c`）。\n  - `rcu_cpu_stall_suppress_at_boot`：全局变量（定义在 RCU 主实现文件中）。\n\n## 5. 使用场景\n\n- **所有 RCU 实现共享**：TREE_RCU、TINY_RCU、SRCU、Tasks RCU 等均使用本文件提供的序列号管理原语。\n- **宽限期跟踪**：RCU 核心代码使用 `rcu_seq_start()`/`rcu_seq_end()` 标记宽限期边界，使用 `rcu_seq_snap()` 获取读者安全点。\n- **回调调度判断**：`call_rcu()` 及其变体使用 `rcu_seq_done()` 判断是否可安全执行回调。\n- **调试与诊断**：\n  - 启用 `CONFIG_DEBUG_OBJECTS` 时，防止 RCU 回调对象的误用。\n  - 启动阶段通过 `rcu_stall_is_suppressed_at_boot()` 避免误报 CPU 停滞。\n- **Polled RCU 支持**：`RCU_GET_STATE_COMPLETED` 用于 `get_state_synchronize_rcu()` / `poll_state_synchronize_rcu()` 等轮询 API。",
      "similarity": 0.6326934099197388,
      "chunks": []
    },
    {
      "source_file": "kernel/rcu/tasks.h",
      "md_summary": "> 自动生成时间: 2025-10-25 15:45:09\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tasks.h`\n\n---\n\n# `rcu/tasks.h` 技术文档\n\n## 1. 文件概述\n\n`rcu/tasks.h` 是 Linux 内核中实现 **基于任务的 RCU（Read-Copy-Update）机制** 的核心头文件。该机制扩展了传统 RCU 的语义，使其能够感知任务（task）状态（如用户态执行、阻塞、退出等），从而在特定场景（如用户空间 RCU、跟踪 RCU、通用任务 RCU）下提供更高效的宽限期（grace period）检测能力。此文件定义了通用的任务 RCU 框架所需的数据结构、回调函数类型和宏，为不同变体（如 `tasks_rcu`、`tasks_trace_rcu` 等）提供统一的实现基础。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_tasks_percpu`**  \n  每 CPU 的任务 RCU 组件，包含：\n  - 回调链表（`cblist`）及其保护锁（`lock`）\n  - 统计字段（`rtp_jiffies`, `rtp_n_lock_retries`）\n  - 懒回调定时器（`lazy_timer`）和紧急宽限期计数（`urgent_gp`）\n  - 工作队列（`rtp_work`）和中断上下文工作（`rtp_irq_work`）\n  - 屏障操作相关字段（`barrier_q_head`）\n  - 被阻塞任务列表（`rtp_blkd_tasks`）和退出任务列表（`rtp_exit_list`）\n  - CPU 编号及指向全局 `rcu_tasks` 实例的指针\n\n- **`struct rcu_tasks`**  \n  全局任务 RCU 实例定义，包含：\n  - 宽限期控制（`gp_func`, `tasks_gp_mutex`, `tasks_gp_seq`）\n  - 各阶段回调函数指针（`pregp_func`, `pertask_func`, `postscan_func`, `holdouts_func`, `postgp_func`）\n  - 回调管理（`call_func`, `rtpcpu`, `rtpcp_array`）\n  - 多队列回调分发控制（`percpu_enqueue_shift/lim`, `percpu_dequeue_lim`）\n  - 屏障同步机制（`barrier_q_mutex`, `barrier_q_count`, `barrier_q_completion`）\n  - 调试与统计字段（`gp_state`, `n_ipis`, `name`, `kname`）\n\n### 主要函数类型定义\n\n- `rcu_tasks_gp_func_t`：宽限期等待函数\n- `pregp_func_t`：宽限期前预处理函数\n- `pertask_func_t`：遍历每个任务的处理函数\n- `postscan_func_t`：任务扫描后处理函数\n- `holdouts_func_t`：检查未完成宽限期任务（holdouts）的函数\n- `postgp_func_t`：宽限期结束后处理函数\n\n### 宏定义\n\n- **`DEFINE_RCU_TASKS(rt_name, gp, call, n)`**  \n  用于静态定义一个完整的任务 RCU 实例，包括每 CPU 变量和全局结构体，并初始化关键字段（如锁、工作队列、名称、回调函数等）。\n\n### 全局参数（可通过 sysfs 调整）\n\n- `rcu_task_ipi_delay`：宽限期初期延迟发送 IPI 的时间（避免过早中断）\n- `rcu_task_stall_timeout`：宽限期卡住超时阈值（默认 10 分钟）\n- `rcu_task_stall_info`：卡住信息打印间隔（默认 10 秒）\n- `rcu_task_enqueue_lim`：回调入队 CPU 队列数量限制\n- `rcu_task_contend_lim` / `collapse_lim` / `lazy_lim`：用于动态调整回调队列行为的阈值\n\n### 调试状态常量\n\n- `RTGS_*` 系列宏（如 `RTGS_INIT`, `RTGS_SCAN_TASKLIST` 等）：用于跟踪任务 RCU 宽限期状态机的当前阶段，便于调试。\n\n## 3. 关键实现\n\n- **通用任务 RCU 框架**：通过 `struct rcu_tasks` 将不同变体（如用户态 RCU、跟踪 RCU）的共性抽象出来，使用函数指针实现策略定制。\n- **多队列回调分发**：支持将 RCU 回调分散到多个 per-CPU 队列（通过 `percpu_enqueue_shift` 控制），以减少锁竞争，提升可扩展性。\n- **懒回调机制**：通过 `lazy_timer` 和 `lazy_jiffies` 实现延迟执行非紧急回调，减少上下文切换开销。\n- **宽限期状态机**：使用 `gp_state` 字段记录宽限期执行阶段，配合 `RTGS_*` 常量实现清晰的状态流转，便于诊断卡住问题。\n- **屏障（Barrier）支持**：通过 `barrier_q_*` 字段实现 `rcu_barrier()` 类操作，确保所有已提交回调执行完毕。\n- **动态队列调整**：根据锁竞争情况（`rtp_n_lock_retries`）和系统负载，动态调整入队/出队队列数量（`rcu_task_cb_adjust` 相关逻辑，虽未在本文件完整体现，但结构已预留支持）。\n\n## 4. 依赖关系\n\n- **`rcu_segcblist.h`**：提供分段回调链表（`rcu_segcblist`）实现，用于高效管理不同状态的 RCU 回调。\n- **`CONFIG_TASKS_RCU_GENERIC`**：本文件功能的编译开关，需启用此配置。\n- **`CONFIG_TASKS_RCU`**：启用标准任务 RCU（用户态感知）时，会包含额外逻辑（如 `tasks_rcu_exit_srcu_stall_timer`）。\n- **`CONFIG_TASKS_TRACE_RCU_READ_MB`**：影响 `rcu_task_ipi_delay` 的默认值，用于跟踪 RCU 场景。\n- **内核基础组件**：依赖 `raw_spinlock_t`、`mutex`、`workqueue`、`irq_work`、`timer`、`completion` 等内核同步与调度原语。\n\n## 5. 使用场景\n\n- **用户空间 RCU（Tasks RCU）**：当需要等待所有曾经运行在用户空间的任务完成其 RCU 读端临界区时使用（例如模块卸载、内存回收）。\n- **跟踪 RCU（Tasks Trace RCU）**：用于 ftrace 等跟踪子系统，确保在修改跟踪点时所有可能执行跟踪代码的上下文（包括内核线程）都已完成。\n- **通用任务宽限期检测**：任何需要等待“所有可能持有某种资源引用的任务”完成的场景，均可基于此框架实现定制化 RCU 变体。\n- **内核模块与子系统同步**：为需要与任务生命周期强关联的内核组件提供高效、可扩展的同步原语。",
      "similarity": 0.6225053071975708,
      "chunks": []
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.6172274947166443,
      "chunks": [
        {
          "chunk_id": 21,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3787,
          "end_line": 3910,
          "content": [
            "void get_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\t/*",
            "\t * Any prior manipulation of RCU-protected data must happen",
            "\t * before the loads from ->gp_seq and ->expedited_sequence.",
            "\t */",
            "\tsmp_mb();  /* ^^^ */",
            "\trgosp->rgos_norm = rcu_seq_snap(&rnp->gp_seq);",
            "\trgosp->rgos_exp = rcu_seq_snap(&rcu_state.expedited_sequence);",
            "}",
            "static void start_poll_synchronize_rcu_common(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool needwake;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tlockdep_assert_irqs_enabled();",
            "\tlocal_irq_save(flags);",
            "\trdp = this_cpu_ptr(&rcu_data);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_rcu_node(rnp); // irqs already disabled.",
            "\t// Note it is possible for a grace period to have elapsed between",
            "\t// the above call to get_state_synchronize_rcu() and the below call",
            "\t// to rcu_seq_snap.  This is OK, the worst that happens is that we",
            "\t// get a grace period that no one needed.  These accesses are ordered",
            "\t// by smp_mb(), and we are accessing them in the opposite order",
            "\t// from which they are updated at grace-period start, as required.",
            "\tneedwake = rcu_start_this_gp(rnp, rdp, rcu_seq_snap(&rcu_state.gp_seq));",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "}",
            "unsigned long start_poll_synchronize_rcu(void)",
            "{",
            "\tunsigned long gp_seq = get_state_synchronize_rcu();",
            "",
            "\tstart_poll_synchronize_rcu_common();",
            "\treturn gp_seq;",
            "}",
            "void start_poll_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tget_state_synchronize_rcu_full(rgosp);",
            "",
            "\tstart_poll_synchronize_rcu_common();",
            "}",
            "bool poll_state_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\tif (oldstate == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rcu_state.gp_seq_polled, oldstate)) {",
            "\t\tsmp_mb(); /* Ensure GP ends before subsequent accesses. */",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "bool poll_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tsmp_mb(); // Order against root rcu_node structure grace-period cleanup.",
            "\tif (rgosp->rgos_norm == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rnp->gp_seq, rgosp->rgos_norm) ||",
            "\t    rgosp->rgos_exp == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rcu_state.expedited_sequence, rgosp->rgos_exp)) {",
            "\t\tsmp_mb(); /* Ensure GP ends before subsequent accesses. */",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "void cond_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\tif (!poll_state_synchronize_rcu(oldstate))",
            "\t\tsynchronize_rcu();",
            "}",
            "void cond_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tif (!poll_state_synchronize_rcu_full(rgosp))",
            "\t\tsynchronize_rcu();",
            "}",
            "static int rcu_pending(int user)",
            "{",
            "\tbool gp_in_progress;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "\tstruct rcu_node *rnp = rdp->mynode;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\t/* Check for CPU stalls, if enabled. */",
            "\tcheck_cpu_stall(rdp);",
            "",
            "\t/* Does this CPU need a deferred NOCB wakeup? */",
            "\tif (rcu_nocb_need_deferred_wakeup(rdp, RCU_NOCB_WAKE))",
            "\t\treturn 1;",
            "",
            "\t/* Is this a nohz_full CPU in userspace or idle?  (Ignore RCU if so.) */",
            "\tif ((user || rcu_is_cpu_rrupt_from_idle()) && rcu_nohz_full_cpu())",
            "\t\treturn 0;",
            "",
            "\t/* Is the RCU core waiting for a quiescent state from this CPU? */",
            "\tgp_in_progress = rcu_gp_in_progress();",
            "\tif (rdp->core_needs_qs && !rdp->cpu_no_qs.b.norm && gp_in_progress)",
            "\t\treturn 1;",
            "",
            "\t/* Does this CPU have callbacks ready to invoke? */",
            "\tif (!rcu_rdp_is_offloaded(rdp) &&",
            "\t    rcu_segcblist_ready_cbs(&rdp->cblist))",
            "\t\treturn 1;",
            "",
            "\t/* Has RCU gone idle with this CPU needing another grace period? */",
            "\tif (!gp_in_progress && rcu_segcblist_is_enabled(&rdp->cblist) &&",
            "\t    !rcu_rdp_is_offloaded(rdp) &&",
            "\t    !rcu_segcblist_restempty(&rdp->cblist, RCU_NEXT_READY_TAIL))",
            "\t\treturn 1;",
            "",
            "\t/* Have RCU grace period completed or started?  */",
            "\tif (rcu_seq_current(&rnp->gp_seq) != rdp->gp_seq ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) /* outside lock */",
            "\t\treturn 1;",
            "",
            "\t/* nothing to do */",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_state_synchronize_rcu_full, start_poll_synchronize_rcu_common, start_poll_synchronize_rcu, start_poll_synchronize_rcu_full, poll_state_synchronize_rcu, poll_state_synchronize_rcu_full, cond_synchronize_rcu, cond_synchronize_rcu_full, rcu_pending",
          "description": "提供RCU宽限期状态查询和触发机制，通过序列号比对判断是否需要启动新的宽限期，处理回调队列唤醒逻辑，实现条件同步检查",
          "similarity": 0.6403152942657471
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 908,
          "end_line": 1026,
          "content": [
            "static void trace_rcu_this_gp(struct rcu_node *rnp, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req, const char *s)",
            "{",
            "\ttrace_rcu_future_grace_period(rcu_state.name, READ_ONCE(rnp->gp_seq),",
            "\t\t\t\t      gp_seq_req, rnp->level,",
            "\t\t\t\t      rnp->grplo, rnp->grphi, s);",
            "}",
            "static bool rcu_start_this_gp(struct rcu_node *rnp_start, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req)",
            "{",
            "\tbool ret = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\t/*",
            "\t * Use funnel locking to either acquire the root rcu_node",
            "\t * structure's lock or bail out if the need for this grace period",
            "\t * has already been recorded -- or if that grace period has in",
            "\t * fact already started.  If there is already a grace period in",
            "\t * progress in a non-leaf node, no recording is needed because the",
            "\t * end of the grace period will scan the leaf rcu_node structures.",
            "\t * Note that rnp_start->lock must not be released.",
            "\t */",
            "\traw_lockdep_assert_held_rcu_node(rnp_start);",
            "\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req, TPS(\"Startleaf\"));",
            "\tfor (rnp = rnp_start; 1; rnp = rnp->parent) {",
            "\t\tif (rnp != rnp_start)",
            "\t\t\traw_spin_lock_rcu_node(rnp);",
            "\t\tif (ULONG_CMP_GE(rnp->gp_seq_needed, gp_seq_req) ||",
            "\t\t    rcu_seq_started(&rnp->gp_seq, gp_seq_req) ||",
            "\t\t    (rnp != rnp_start &&",
            "\t\t     rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))) {",
            "\t\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Prestarted\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tWRITE_ONCE(rnp->gp_seq_needed, gp_seq_req);",
            "\t\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq))) {",
            "\t\t\t/*",
            "\t\t\t * We just marked the leaf or internal node, and a",
            "\t\t\t * grace period is in progress, which means that",
            "\t\t\t * rcu_gp_cleanup() will see the marking.  Bail to",
            "\t\t\t * reduce contention.",
            "\t\t\t */",
            "\t\t\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Startedleaf\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tif (rnp != rnp_start && rnp->parent != NULL)",
            "\t\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\tif (!rnp->parent)",
            "\t\t\tbreak;  /* At root, and perhaps also leaf. */",
            "\t}",
            "",
            "\t/* If GP already in progress, just leave, otherwise start one. */",
            "\tif (rcu_gp_in_progress()) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedleafroot\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedroot\"));",
            "\tWRITE_ONCE(rcu_state.gp_flags, rcu_state.gp_flags | RCU_GP_FLAG_INIT);",
            "\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);",
            "\tif (!READ_ONCE(rcu_state.gp_kthread)) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"NoGPkthread\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_grace_period(rcu_state.name, data_race(rcu_state.gp_seq), TPS(\"newreq\"));",
            "\tret = true;  /* Caller must wake GP kthread. */",
            "unlock_out:",
            "\t/* Push furthest requested GP to leaf node and rcu_data structure. */",
            "\tif (ULONG_CMP_LT(gp_seq_req, rnp->gp_seq_needed)) {",
            "\t\tWRITE_ONCE(rnp_start->gp_seq_needed, rnp->gp_seq_needed);",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\t}",
            "\tif (rnp != rnp_start)",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\treturn ret;",
            "}",
            "static bool rcu_future_gp_cleanup(struct rcu_node *rnp)",
            "{",
            "\tbool needmore;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tneedmore = ULONG_CMP_LT(rnp->gp_seq, rnp->gp_seq_needed);",
            "\tif (!needmore)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq; /* Avoid counter wrap. */",
            "\ttrace_rcu_this_gp(rnp, rdp, rnp->gp_seq,",
            "\t\t\t  needmore ? TPS(\"CleanupMore\") : TPS(\"Cleanup\"));",
            "\treturn needmore;",
            "}",
            "static void swake_up_one_online_ipi(void *arg)",
            "{",
            "\tstruct swait_queue_head *wqh = arg;",
            "",
            "\tswake_up_one(wqh);",
            "}",
            "static void swake_up_one_online(struct swait_queue_head *wqh)",
            "{",
            "\tint cpu = get_cpu();",
            "",
            "\t/*",
            "\t * If called from rcutree_report_cpu_starting(), wake up",
            "\t * is dangerous that late in the CPU-down hotplug process. The",
            "\t * scheduler might queue an ignored hrtimer. Defer the wake up",
            "\t * to an online CPU instead.",
            "\t */",
            "\tif (unlikely(cpu_is_offline(cpu))) {",
            "\t\tint target;",
            "",
            "\t\ttarget = cpumask_any_and(housekeeping_cpumask(HK_TYPE_RCU),",
            "\t\t\t\t\t cpu_online_mask);",
            "",
            "\t\tsmp_call_function_single(target, swake_up_one_online_ipi,",
            "\t\t\t\t\t wqh, 0);",
            "\t\tput_cpu();",
            "\t} else {",
            "\t\tput_cpu();",
            "\t\tswake_up_one(wqh);",
            "\t}",
            "}"
          ],
          "function_name": "trace_rcu_this_gp, rcu_start_this_gp, rcu_future_gp_cleanup, swake_up_one_online_ipi, swake_up_one_online",
          "description": "实现RCU grace period事件追踪、新grace period启动逻辑及未来grace period清理机制，支持跨层级节点的同步状态传播。",
          "similarity": 0.6369805335998535
        },
        {
          "chunk_id": 28,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 5098,
          "end_line": 5206,
          "content": [
            "static void __init rcu_dump_rcu_node_tree(void)",
            "{",
            "\tint level = 0;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tpr_info(\"rcu_node tree layout dump\\n\");",
            "\tpr_info(\" \");",
            "\trcu_for_each_node_breadth_first(rnp) {",
            "\t\tif (rnp->level != level) {",
            "\t\t\tpr_cont(\"\\n\");",
            "\t\t\tpr_info(\" \");",
            "\t\t\tlevel = rnp->level;",
            "\t\t}",
            "\t\tpr_cont(\"%d:%d ^%d  \", rnp->grplo, rnp->grphi, rnp->grpnum);",
            "\t}",
            "\tpr_cont(\"\\n\");",
            "}",
            "static void __init kfree_rcu_batch_init(void)",
            "{",
            "\tint cpu;",
            "\tint i, j;",
            "\tstruct shrinker *kfree_rcu_shrinker;",
            "",
            "\t/* Clamp it to [0:100] seconds interval. */",
            "\tif (rcu_delay_page_cache_fill_msec < 0 ||",
            "\t\trcu_delay_page_cache_fill_msec > 100 * MSEC_PER_SEC) {",
            "",
            "\t\trcu_delay_page_cache_fill_msec =",
            "\t\t\tclamp(rcu_delay_page_cache_fill_msec, 0,",
            "\t\t\t\t(int) (100 * MSEC_PER_SEC));",
            "",
            "\t\tpr_info(\"Adjusting rcutree.rcu_delay_page_cache_fill_msec to %d ms.\\n\",",
            "\t\t\trcu_delay_page_cache_fill_msec);",
            "\t}",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\t\tINIT_RCU_WORK(&krcp->krw_arr[i].rcu_work, kfree_rcu_work);",
            "\t\t\tkrcp->krw_arr[i].krcp = krcp;",
            "",
            "\t\t\tfor (j = 0; j < FREE_N_CHANNELS; j++)",
            "\t\t\t\tINIT_LIST_HEAD(&krcp->krw_arr[i].bulk_head_free[j]);",
            "\t\t}",
            "",
            "\t\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\t\tINIT_LIST_HEAD(&krcp->bulk_head[i]);",
            "",
            "\t\tINIT_DELAYED_WORK(&krcp->monitor_work, kfree_rcu_monitor);",
            "\t\tINIT_DELAYED_WORK(&krcp->page_cache_work, fill_page_cache_func);",
            "\t\tkrcp->initialized = true;",
            "\t}",
            "",
            "\tkfree_rcu_shrinker = shrinker_alloc(0, \"rcu-kfree\");",
            "\tif (!kfree_rcu_shrinker) {",
            "\t\tpr_err(\"Failed to allocate kfree_rcu() shrinker!\\n\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tkfree_rcu_shrinker->count_objects = kfree_rcu_shrink_count;",
            "\tkfree_rcu_shrinker->scan_objects = kfree_rcu_shrink_scan;",
            "",
            "\tshrinker_register(kfree_rcu_shrinker);",
            "}",
            "void __init rcu_init(void)",
            "{",
            "\tint cpu = smp_processor_id();",
            "",
            "\trcu_early_boot_tests();",
            "",
            "\tkfree_rcu_batch_init();",
            "\trcu_bootup_announce();",
            "\tsanitize_kthread_prio();",
            "\trcu_init_geometry();",
            "\trcu_init_one();",
            "\tif (dump_tree)",
            "\t\trcu_dump_rcu_node_tree();",
            "\tif (use_softirq)",
            "\t\topen_softirq(RCU_SOFTIRQ, rcu_core_si);",
            "",
            "\t/*",
            "\t * We don't need protection against CPU-hotplug here because",
            "\t * this is called early in boot, before either interrupts",
            "\t * or the scheduler are operational.",
            "\t */",
            "\tpm_notifier(rcu_pm_notify, 0);",
            "\tWARN_ON(num_online_cpus() > 1); // Only one CPU this early in boot.",
            "\trcutree_prepare_cpu(cpu);",
            "\trcu_cpu_starting(cpu);",
            "\trcutree_online_cpu(cpu);",
            "",
            "\t/* Create workqueue for Tree SRCU and for expedited GPs. */",
            "\trcu_gp_wq = alloc_workqueue(\"rcu_gp\", WQ_MEM_RECLAIM, 0);",
            "\tWARN_ON(!rcu_gp_wq);",
            "\trcu_alloc_par_gp_wq();",
            "",
            "\t/* Fill in default value for rcutree.qovld boot parameter. */",
            "\t/* -After- the rcu_node ->lock fields are initialized! */",
            "\tif (qovld < 0)",
            "\t\tqovld_calc = DEFAULT_RCU_QOVLD_MULT * qhimark;",
            "\telse",
            "\t\tqovld_calc = qovld;",
            "",
            "\t// Kick-start in case any polled grace periods started early.",
            "\t(void)start_poll_synchronize_rcu_expedited();",
            "",
            "\trcu_test_sync_prims();",
            "}"
          ],
          "function_name": "rcu_dump_rcu_node_tree, kfree_rcu_batch_init, rcu_init",
          "description": "执行RCU节点树的调试输出，初始化内存回收批量机制，完成RCU子系统的整体初始化流程，包括资源分配、事件注册及核心组件启动。",
          "similarity": 0.6290401220321655
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1840,
          "end_line": 1973,
          "content": [
            "static int __noreturn rcu_gp_kthread(void *unused)",
            "{",
            "\trcu_bind_gp_kthread();",
            "\tfor (;;) {",
            "",
            "\t\t/* Handle grace-period start. */",
            "\t\tfor (;;) {",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwait\"));",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_WAIT_GPS);",
            "\t\t\tswait_event_idle_exclusive(rcu_state.gp_wq,",
            "\t\t\t\t\t READ_ONCE(rcu_state.gp_flags) &",
            "\t\t\t\t\t RCU_GP_FLAG_INIT);",
            "\t\t\trcu_gp_torture_wait();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_DONE_GPS);",
            "\t\t\t/* Locking provides needed memory barrier. */",
            "\t\t\tif (rcu_gp_init())",
            "\t\t\t\tbreak;",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\t\t\tWARN_ON(signal_pending(current));",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwaitsig\"));",
            "\t\t}",
            "",
            "\t\t/* Handle quiescent-state forcing. */",
            "\t\trcu_gp_fqs_loop();",
            "",
            "\t\t/* Handle grace-period end. */",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANUP);",
            "\t\trcu_gp_cleanup();",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANED);",
            "\t}",
            "}",
            "static void rcu_report_qs_rsp(unsigned long flags)",
            "\t__releases(rcu_get_root()->lock)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rcu_get_root());",
            "\tWARN_ON_ONCE(!rcu_gp_in_progress());",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rcu_get_root(), flags);",
            "\trcu_gp_kthread_wake();",
            "}",
            "static void rcu_report_qs_rnp(unsigned long mask, struct rcu_node *rnp,",
            "\t\t\t      unsigned long gps, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long oldmask = 0;",
            "\tstruct rcu_node *rnp_c;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* Walk up the rcu_node hierarchy. */",
            "\tfor (;;) {",
            "\t\tif ((!(rnp->qsmask & mask) && mask) || rnp->gp_seq != gps) {",
            "",
            "\t\t\t/*",
            "\t\t\t * Our bit has already been cleared, or the",
            "\t\t\t * relevant grace period is already over, so done.",
            "\t\t\t */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */",
            "\t\tWARN_ON_ONCE(!rcu_is_leaf_node(rnp) &&",
            "\t\t\t     rcu_preempt_blocked_readers_cgp(rnp));",
            "\t\tWRITE_ONCE(rnp->qsmask, rnp->qsmask & ~mask);",
            "\t\ttrace_rcu_quiescent_state_report(rcu_state.name, rnp->gp_seq,",
            "\t\t\t\t\t\t mask, rnp->qsmask, rnp->level,",
            "\t\t\t\t\t\t rnp->grplo, rnp->grphi,",
            "\t\t\t\t\t\t !!rnp->gp_tasks);",
            "\t\tif (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {",
            "",
            "\t\t\t/* Other bits still set at this level, so done. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\trnp->completedqs = rnp->gp_seq;",
            "\t\tmask = rnp->grpmask;",
            "\t\tif (rnp->parent == NULL) {",
            "",
            "\t\t\t/* No more levels.  Exit loop holding root lock. */",
            "",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\trnp_c = rnp;",
            "\t\trnp = rnp->parent;",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\toldmask = READ_ONCE(rnp_c->qsmask);",
            "\t}",
            "",
            "\t/*",
            "\t * Get here if we are the last CPU to pass through a quiescent",
            "\t * state for this grace period.  Invoke rcu_report_qs_rsp()",
            "\t * to clean up and start the next grace period if one is needed.",
            "\t */",
            "\trcu_report_qs_rsp(flags); /* releases rnp->lock. */",
            "}",
            "static void __maybe_unused",
            "rcu_report_unblock_qs_rnp(struct rcu_node *rnp, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long gps;",
            "\tunsigned long mask;",
            "\tstruct rcu_node *rnp_p;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (WARN_ON_ONCE(!IS_ENABLED(CONFIG_PREEMPT_RCU)) ||",
            "\t    WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)) ||",
            "\t    rnp->qsmask != 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;  /* Still need more quiescent states! */",
            "\t}",
            "",
            "\trnp->completedqs = rnp->gp_seq;",
            "\trnp_p = rnp->parent;",
            "\tif (rnp_p == NULL) {",
            "\t\t/*",
            "\t\t * Only one rcu_node structure in the tree, so don't",
            "\t\t * try to report up to its nonexistent parent!",
            "\t\t */",
            "\t\trcu_report_qs_rsp(flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Report up the rest of the hierarchy, tracking current ->gp_seq. */",
            "\tgps = rnp->gp_seq;",
            "\tmask = rnp->grpmask;",
            "\traw_spin_unlock_rcu_node(rnp);\t/* irqs remain disabled. */",
            "\traw_spin_lock_rcu_node(rnp_p);\t/* irqs already disabled. */",
            "\trcu_report_qs_rnp(mask, rnp_p, gps, flags);",
            "}"
          ],
          "function_name": "rcu_gp_kthread, rcu_report_qs_rsp, rcu_report_qs_rnp, rcu_report_unblock_qs_rnp",
          "description": "实现RCU grace period的主线程循环，处理grace period启动、强制quiescent状态报告及结束逻辑，通过锁和状态标志协调各子系统同步",
          "similarity": 0.6275709867477417
        },
        {
          "chunk_id": 22,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4072,
          "end_line": 4226,
          "content": [
            "static void rcu_barrier_trace(const char *s, int cpu, unsigned long done)",
            "{",
            "\ttrace_rcu_barrier(rcu_state.name, s, cpu,",
            "\t\t\t  atomic_read(&rcu_state.barrier_cpu_count), done);",
            "}",
            "static void rcu_barrier_callback(struct rcu_head *rhp)",
            "{",
            "\tunsigned long __maybe_unused s = rcu_state.barrier_sequence;",
            "",
            "\tif (atomic_dec_and_test(&rcu_state.barrier_cpu_count)) {",
            "\t\trcu_barrier_trace(TPS(\"LastCB\"), -1, s);",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "\t} else {",
            "\t\trcu_barrier_trace(TPS(\"CB\"), -1, s);",
            "\t}",
            "}",
            "static void rcu_barrier_entrain(struct rcu_data *rdp)",
            "{",
            "\tunsigned long gseq = READ_ONCE(rcu_state.barrier_sequence);",
            "\tunsigned long lseq = READ_ONCE(rdp->barrier_seq_snap);",
            "\tbool wake_nocb = false;",
            "\tbool was_alldone = false;",
            "",
            "\tlockdep_assert_held(&rcu_state.barrier_lock);",
            "\tif (rcu_seq_state(lseq) || !rcu_seq_state(gseq) || rcu_seq_ctr(lseq) != rcu_seq_ctr(gseq))",
            "\t\treturn;",
            "\trcu_barrier_trace(TPS(\"IRQ\"), -1, rcu_state.barrier_sequence);",
            "\trdp->barrier_head.func = rcu_barrier_callback;",
            "\tdebug_rcu_head_queue(&rdp->barrier_head);",
            "\trcu_nocb_lock(rdp);",
            "\t/*",
            "\t * Flush bypass and wakeup rcuog if we add callbacks to an empty regular",
            "\t * queue. This way we don't wait for bypass timer that can reach seconds",
            "\t * if it's fully lazy.",
            "\t */",
            "\twas_alldone = rcu_rdp_is_offloaded(rdp) && !rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));",
            "\twake_nocb = was_alldone && rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tif (rcu_segcblist_entrain(&rdp->cblist, &rdp->barrier_head)) {",
            "\t\tatomic_inc(&rcu_state.barrier_cpu_count);",
            "\t} else {",
            "\t\tdebug_rcu_head_unqueue(&rdp->barrier_head);",
            "\t\trcu_barrier_trace(TPS(\"IRQNQ\"), -1, rcu_state.barrier_sequence);",
            "\t}",
            "\trcu_nocb_unlock(rdp);",
            "\tif (wake_nocb)",
            "\t\twake_nocb_gp(rdp, false);",
            "\tsmp_store_release(&rdp->barrier_seq_snap, gseq);",
            "}",
            "static void rcu_barrier_handler(void *cpu_in)",
            "{",
            "\tuintptr_t cpu = (uintptr_t)cpu_in;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\tWARN_ON_ONCE(cpu != rdp->cpu);",
            "\tWARN_ON_ONCE(cpu != smp_processor_id());",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\trcu_barrier_entrain(rdp);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "}",
            "void rcu_barrier(void)",
            "{",
            "\tuintptr_t cpu;",
            "\tunsigned long flags;",
            "\tunsigned long gseq;",
            "\tstruct rcu_data *rdp;",
            "\tunsigned long s = rcu_seq_snap(&rcu_state.barrier_sequence);",
            "",
            "\trcu_barrier_trace(TPS(\"Begin\"), -1, s);",
            "",
            "\t/* Take mutex to serialize concurrent rcu_barrier() requests. */",
            "\tmutex_lock(&rcu_state.barrier_mutex);",
            "",
            "\t/* Did someone else do our work for us? */",
            "\tif (rcu_seq_done(&rcu_state.barrier_sequence, s)) {",
            "\t\trcu_barrier_trace(TPS(\"EarlyExit\"), -1, rcu_state.barrier_sequence);",
            "\t\tsmp_mb(); /* caller's subsequent code after above check. */",
            "\t\tmutex_unlock(&rcu_state.barrier_mutex);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Mark the start of the barrier operation. */",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\trcu_seq_start(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\trcu_barrier_trace(TPS(\"Inc1\"), -1, rcu_state.barrier_sequence);",
            "",
            "\t/*",
            "\t * Initialize the count to two rather than to zero in order",
            "\t * to avoid a too-soon return to zero in case of an immediate",
            "\t * invocation of the just-enqueued callback (or preemption of",
            "\t * this task).  Exclude CPU-hotplug operations to ensure that no",
            "\t * offline non-offloaded CPU has callbacks queued.",
            "\t */",
            "\tinit_completion(&rcu_state.barrier_completion);",
            "\tatomic_set(&rcu_state.barrier_cpu_count, 2);",
            "\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "",
            "\t/*",
            "\t * Force each CPU with callbacks to register a new callback.",
            "\t * When that callback is invoked, we will know that all of the",
            "\t * corresponding CPU's preceding callbacks have been invoked.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "retry:",
            "\t\tif (smp_load_acquire(&rdp->barrier_seq_snap) == gseq)",
            "\t\t\tcontinue;",
            "\t\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\t\tif (!rcu_segcblist_n_cbs(&rdp->cblist)) {",
            "\t\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"NQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (!rcu_rdp_cpu_online(rdp)) {",
            "\t\t\trcu_barrier_entrain(rdp);",
            "\t\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"OfflineNoCBQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\tif (smp_call_function_single(cpu, rcu_barrier_handler, (void *)cpu, 1)) {",
            "\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\tgoto retry;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\trcu_barrier_trace(TPS(\"OnlineQ\"), cpu, rcu_state.barrier_sequence);",
            "\t}",
            "",
            "\t/*",
            "\t * Now that we have an rcu_barrier_callback() callback on each",
            "\t * CPU, and thus each counted, remove the initial count.",
            "\t */",
            "\tif (atomic_sub_and_test(2, &rcu_state.barrier_cpu_count))",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "",
            "\t/* Wait for all rcu_barrier_callback() callbacks to be invoked. */",
            "\twait_for_completion(&rcu_state.barrier_completion);",
            "",
            "\t/* Mark the end of the barrier operation. */",
            "\trcu_barrier_trace(TPS(\"Inc2\"), -1, rcu_state.barrier_sequence);",
            "\trcu_seq_end(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t}",
            "",
            "\t/* Other rcu_barrier() invocations can now safely proceed. */",
            "\tmutex_unlock(&rcu_state.barrier_mutex);",
            "}"
          ],
          "function_name": "rcu_barrier_trace, rcu_barrier_callback, rcu_barrier_entrain, rcu_barrier_handler, rcu_barrier",
          "description": "实现RCU屏障功能，通过分发回调函数强制所有CPU完成当前RCU操作，使用原子计数器跟踪完成状态，通过completion等待所有回调完成",
          "similarity": 0.6237003803253174
        }
      ]
    }
  ]
}