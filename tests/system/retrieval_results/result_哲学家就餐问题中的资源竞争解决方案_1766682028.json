{
  "query": "哲学家就餐问题中的资源竞争解决方案",
  "timestamp": "2025-12-26 01:00:28",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/fair.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:09:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\fair.c`\n\n---\n\n# `sched/fair.c` 技术文档\n\n## 1. 文件概述\n\n`sched/fair.c` 是 Linux 内核中 **完全公平调度器**（Completely Fair Scheduler, CFS）的核心实现文件，负责实现 `SCHED_NORMAL` 和 `SCHED_BATCH` 调度策略。CFS 旨在通过红黑树（RB-tree）维护可运行任务的虚拟运行时间（vruntime），以实现 CPU 时间的公平分配。该文件实现了任务调度、负载跟踪、时间片计算、组调度（group scheduling）、NUMA 负载均衡、带宽控制等关键机制，是 Linux 通用调度子系统的核心组成部分。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_entity`：调度实体，代表一个可调度单元（任务或任务组）\n- `struct cfs_rq`：CFS 运行队列，管理一组调度实体\n- `struct load_weight`：负载权重结构，用于计算任务对系统负载的贡献\n\n### 关键函数与宏\n- `__calc_delta()` / `calc_delta_fair()`：计算基于权重的调度时间增量\n- `update_load_add()` / `update_load_sub()` / `update_load_set()`：更新负载权重\n- `__update_inv_weight()`：预计算权重的倒数以优化除法运算\n- `get_update_sysctl_factor()`：根据在线 CPU 数量动态调整调度参数\n- `update_sysctl()` / `sched_init_granularity()`：初始化和更新调度粒度参数\n- `for_each_sched_entity()`：遍历调度实体层级结构（用于组调度）\n\n### 可调参数（sysctl）\n- `sysctl_sched_base_slice`：基础时间片（默认 700,000 纳秒）\n- `sysctl_sched_tunable_scaling`：调度参数缩放策略（NONE/LOG/LINEAR）\n- `sysctl_sched_migration_cost`：任务迁移成本阈值（500 微秒）\n- `sysctl_sched_cfs_bandwidth_slice_us`（CFS 带宽控制切片，默认 5 毫秒）\n- `sysctl_numa_balancing_promote_rate_limit_MBps`（NUMA 页迁移速率限制）\n\n## 3. 关键实现\n\n### 虚拟时间与公平性\nCFS 使用 **虚拟运行时间**（vruntime）衡量任务已使用的 CPU 时间，并通过 `calc_delta_fair()` 将实际执行时间按任务权重归一化。权重由任务的 nice 值决定（`NICE_0_LOAD = 1024` 为基准）。调度器总是选择 vruntime 最小的任务运行，确保高优先级（高权重）任务获得更多 CPU 时间。\n\n### 高效除法优化\n为避免频繁除法运算，CFS 预计算 `inv_weight = WMULT_CONST / weight`（`WMULT_CONST = ~0U`），将除法转换为乘法和右移操作（`mul_u64_u32_shr`）。`__calc_delta()` 通过动态调整移位位数（`shift`）保证计算精度，适用于 32/64 位架构。\n\n### 动态粒度调整\n基础时间片 `sched_base_slice` 根据在线 CPU 数量动态缩放：\n- `SCHED_TUNABLESCALING_NONE`：固定值\n- `SCHED_TUNABLESCALING_LINEAR`：线性缩放（×ncpus）\n- `SCHED_TUNABLESCALING_LOG`（默认）：对数缩放（×(1 + ilog2(ncpus))）  \n此设计确保在多核系统中保持合理的调度延迟和交互性。\n\n### 组调度支持\n通过 `for_each_sched_entity()` 宏遍历任务所属的调度实体层级（任务 → 任务组 → 父任务组），实现 CPU 带宽在任务组间的公平分配。每个 `cfs_rq` 独立维护其子实体的红黑树。\n\n### SMP 相关优化\n- **非对称 CPU 优先级**：`arch_asym_cpu_priority()` 允许架构定义 CPU 能力差异（如大小核）\n- **容量比较宏**：`fits_capacity()`（20% 容差）和 `capacity_greater()`（5% 容差）用于负载均衡决策\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- 调度核心：`\"sched.h\"`、`\"stats.h\"`、`\"autogroup.h\"`\n- 系统服务：`<linux/sched/clock.h>`、`<linux/sched/nohz.h>`、`<linux/psi.h>`\n- 内存管理：`<linux/mem_policy.h>`、`<linux/energy_model.h>`\n- SMP 支持：`<linux/topology.h>`、`<linux/cpumask_api.h>`\n- 数据结构：`<linux/rbtree_augmented.h>`\n\n### 条件编译特性\n- `CONFIG_SMP`：多处理器调度优化\n- `CONFIG_CFS_BANDWIDTH`：CPU 带宽限制（cgroup v1/v2）\n- `CONFIG_NUMA_BALANCING`：NUMA 自动迁移\n- `CONFIG_FAIR_GROUP_SCHED`：CFS 组调度（cgroup 支持）\n\n## 5. 使用场景\n\n- **通用任务调度**：所有使用 `SCHED_NORMAL` 或 `SCHED_BATCH` 策略的用户态进程\n- **cgroup CPU 资源控制**：通过 `cpu.cfs_quota_us` 和 `cpu.cfs_period_us` 限制任务组带宽\n- **NUMA 优化**：自动迁移内存页以减少远程访问（`numa_balancing`）\n- **节能调度**：结合 `energy_model` 在满足性能前提下选择低功耗 CPU\n- **实时性保障**：通过 `cond_resched()` 在长循环中主动让出 CPU，避免内核抢占延迟过高\n- **系统调优**：管理员通过 `/proc/sys/kernel/` 下的 sysctl 参数动态调整调度行为",
      "similarity": 0.43940484523773193,
      "chunks": [
        {
          "chunk_id": 52,
          "file_path": "kernel/sched/fair.c",
          "start_line": 8581,
          "end_line": 8709,
          "content": [
            "static void set_task_max_allowed_capacity(struct task_struct *p)",
            "{",
            "\tstruct asym_cap_data *entry;",
            "",
            "\tif (!sched_asym_cpucap_active())",
            "\t\treturn;",
            "",
            "\trcu_read_lock();",
            "\tlist_for_each_entry_rcu(entry, &asym_cap_list, link) {",
            "\t\tcpumask_t *cpumask;",
            "",
            "\t\tcpumask = cpu_capacity_span(entry);",
            "\t\tif (!cpumask_intersects(p->cpus_ptr, cpumask))",
            "\t\t\tcontinue;",
            "",
            "\t\tp->max_allowed_capacity = entry->capacity;",
            "\t\tbreak;",
            "\t}",
            "\trcu_read_unlock();",
            "}",
            "static void set_cpus_allowed_fair(struct task_struct *p, struct affinity_context *ctx)",
            "{",
            "\tset_cpus_allowed_common(p, ctx);",
            "\tset_task_max_allowed_capacity(p);",
            "}",
            "static int",
            "balance_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)",
            "{",
            "\tif (sched_fair_runnable(rq))",
            "\t\treturn 1;",
            "",
            "\treturn sched_balance_newidle(rq, rf) != 0;",
            "}",
            "static inline void set_task_max_allowed_capacity(struct task_struct *p) {}",
            "static void set_next_buddy(struct sched_entity *se)",
            "{",
            "\tfor_each_sched_entity(se) {",
            "\t\tif (SCHED_WARN_ON(!se->on_rq))",
            "\t\t\treturn;",
            "\t\tif (se_is_idle(se))",
            "\t\t\treturn;",
            "\t\tcfs_rq_of(se)->next = se;",
            "\t}",
            "}",
            "static void check_preempt_wakeup_fair(struct rq *rq, struct task_struct *p, int wake_flags)",
            "{",
            "\tstruct task_struct *curr = rq->curr;",
            "\tstruct sched_entity *se = &curr->se, *pse = &p->se;",
            "\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);",
            "\tint next_buddy_marked = 0;",
            "\tint cse_is_idle, pse_is_idle;",
            "",
            "\tif (unlikely(se == pse))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This is possible from callers such as attach_tasks(), in which we",
            "\t * unconditionally wakeup_preempt() after an enqueue (which may have",
            "\t * lead to a throttle).  This both saves work and prevents false",
            "\t * next-buddy nomination below.",
            "\t */",
            "\tif (unlikely(throttled_hierarchy(cfs_rq_of(pse))))",
            "\t\treturn;",
            "",
            "\tif (sched_feat(NEXT_BUDDY) && !(wake_flags & WF_FORK) && !pse->sched_delayed) {",
            "\t\tset_next_buddy(pse);",
            "\t\tnext_buddy_marked = 1;",
            "\t}",
            "",
            "\t/*",
            "\t * We can come here with TIF_NEED_RESCHED already set from new task",
            "\t * wake up path.",
            "\t *",
            "\t * Note: this also catches the edge-case of curr being in a throttled",
            "\t * group (e.g. via set_curr_task), since update_curr() (in the",
            "\t * enqueue of curr) will have resulted in resched being set.  This",
            "\t * prevents us from potentially nominating it as a false LAST_BUDDY",
            "\t * below.",
            "\t */",
            "\tif (test_tsk_need_resched(curr))",
            "\t\treturn;",
            "",
            "\tif (!sched_feat(WAKEUP_PREEMPTION))",
            "\t\treturn;",
            "",
            "\tfind_matching_se(&se, &pse);",
            "\tWARN_ON_ONCE(!pse);",
            "",
            "\tcse_is_idle = se_is_idle(se);",
            "\tpse_is_idle = se_is_idle(pse);",
            "",
            "\t/*",
            "\t * Preempt an idle entity in favor of a non-idle entity (and don't preempt",
            "\t * in the inverse case).",
            "\t */",
            "\tif (cse_is_idle && !pse_is_idle)",
            "\t\tgoto preempt;",
            "\tif (cse_is_idle != pse_is_idle)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * BATCH and IDLE tasks do not preempt others.",
            "\t */",
            "\tif (unlikely(!normal_policy(p->policy)))",
            "\t\treturn;",
            "",
            "\tcfs_rq = cfs_rq_of(se);",
            "\tupdate_curr(cfs_rq);",
            "\t/*",
            "\t * If @p has a shorter slice than current and @p is eligible, override",
            "\t * current's slice protection in order to allow preemption.",
            "\t *",
            "\t * Note that even if @p does not turn out to be the most eligible",
            "\t * task at this moment, current's slice protection will be lost.",
            "\t */",
            "\tif (do_preempt_short(cfs_rq, pse, se) && se->vlag == se->deadline)",
            "\t\tse->vlag = se->deadline + 1;",
            "",
            "\t/*",
            "\t * If @p has become the most eligible task, force preemption.",
            "\t */",
            "\tif (pick_eevdf(cfs_rq) == pse)",
            "\t\tgoto preempt;",
            "",
            "\treturn;",
            "",
            "preempt:",
            "\tresched_curr(rq);",
            "}"
          ],
          "function_name": "set_task_max_allowed_capacity, set_cpus_allowed_fair, balance_fair, set_task_max_allowed_capacity, set_next_buddy, check_preempt_wakeup_fair",
          "description": "设置任务最大允许容量，实现负载均衡判断逻辑，管理调度实体间的抢占关系和运行队列状态更新。",
          "similarity": 0.5026320219039917
        },
        {
          "chunk_id": 68,
          "file_path": "kernel/sched/fair.c",
          "start_line": 12089,
          "end_line": 12257,
          "content": [
            "static inline int on_null_domain(struct rq *rq)",
            "{",
            "\treturn unlikely(!rcu_dereference_sched(rq->sd));",
            "}",
            "static inline int find_new_ilb(void)",
            "{",
            "\tint ilb;",
            "\tconst struct cpumask *hk_mask;",
            "",
            "\thk_mask = housekeeping_cpumask(HK_TYPE_MISC);",
            "",
            "\tfor_each_cpu_and(ilb, nohz.idle_cpus_mask, hk_mask) {",
            "",
            "\t\tif (ilb == smp_processor_id())",
            "\t\t\tcontinue;",
            "",
            "\t\tif (idle_cpu(ilb))",
            "\t\t\treturn ilb;",
            "\t}",
            "",
            "\treturn nr_cpu_ids;",
            "}",
            "static void kick_ilb(unsigned int flags)",
            "{",
            "\tint ilb_cpu;",
            "",
            "\t/*",
            "\t * Increase nohz.next_balance only when if full ilb is triggered but",
            "\t * not if we only update stats.",
            "\t */",
            "\tif (flags & NOHZ_BALANCE_KICK)",
            "\t\tnohz.next_balance = jiffies+1;",
            "",
            "\tilb_cpu = find_new_ilb();",
            "",
            "\tif (ilb_cpu >= nr_cpu_ids)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Access to rq::nohz_csd is serialized by NOHZ_KICK_MASK; he who sets",
            "\t * the first flag owns it; cleared by nohz_csd_func().",
            "\t */",
            "\tflags = atomic_fetch_or(flags, nohz_flags(ilb_cpu));",
            "\tif (flags & NOHZ_KICK_MASK)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This way we generate an IPI on the target CPU which",
            "\t * is idle. And the softirq performing nohz idle load balance",
            "\t * will be run before returning from the IPI.",
            "\t */",
            "\tsmp_call_function_single_async(ilb_cpu, &cpu_rq(ilb_cpu)->nohz_csd);",
            "}",
            "static void nohz_balancer_kick(struct rq *rq)",
            "{",
            "\tunsigned long now = jiffies;",
            "\tstruct sched_domain_shared *sds;",
            "\tstruct sched_domain *sd;",
            "\tint nr_busy, i, cpu = rq->cpu;",
            "\tunsigned int flags = 0;",
            "",
            "\tif (unlikely(rq->idle_balance))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * We may be recently in ticked or tickless idle mode. At the first",
            "\t * busy tick after returning from idle, we will update the busy stats.",
            "\t */",
            "\tnohz_balance_exit_idle(rq);",
            "",
            "\t/*",
            "\t * None are in tickless mode and hence no need for NOHZ idle load",
            "\t * balancing.",
            "\t */",
            "\tif (likely(!atomic_read(&nohz.nr_cpus)))",
            "\t\treturn;",
            "",
            "\tif (READ_ONCE(nohz.has_blocked) &&",
            "\t    time_after(now, READ_ONCE(nohz.next_blocked)))",
            "\t\tflags = NOHZ_STATS_KICK;",
            "",
            "\tif (time_before(now, nohz.next_balance))",
            "\t\tgoto out;",
            "",
            "\tif (rq->nr_running >= 2) {",
            "\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\trcu_read_lock();",
            "",
            "\tsd = rcu_dereference(rq->sd);",
            "\tif (sd) {",
            "\t\t/*",
            "\t\t * If there's a CFS task and the current CPU has reduced",
            "\t\t * capacity; kick the ILB to see if there's a better CPU to run",
            "\t\t * on.",
            "\t\t */",
            "\t\tif (rq->cfs.h_nr_running >= 1 && check_cpu_capacity(rq, sd)) {",
            "\t\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "\t}",
            "",
            "\tsd = rcu_dereference(per_cpu(sd_asym_packing, cpu));",
            "\tif (sd) {",
            "\t\t/*",
            "\t\t * When ASYM_PACKING; see if there's a more preferred CPU",
            "\t\t * currently idle; in which case, kick the ILB to move tasks",
            "\t\t * around.",
            "\t\t *",
            "\t\t * When balancing between cores, all the SMT siblings of the",
            "\t\t * preferred CPU must be idle.",
            "\t\t */",
            "\t\tfor_each_cpu_and(i, sched_domain_span(sd), nohz.idle_cpus_mask) {",
            "\t\t\tif (sched_asym(sd, i, cpu)) {",
            "\t\t\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;",
            "\t\t\t\tgoto unlock;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\tsd = rcu_dereference(per_cpu(sd_asym_cpucapacity, cpu));",
            "\tif (sd) {",
            "\t\t/*",
            "\t\t * When ASYM_CPUCAPACITY; see if there's a higher capacity CPU",
            "\t\t * to run the misfit task on.",
            "\t\t */",
            "\t\tif (check_misfit_status(rq)) {",
            "\t\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * For asymmetric systems, we do not want to nicely balance",
            "\t\t * cache use, instead we want to embrace asymmetry and only",
            "\t\t * ensure tasks have enough CPU capacity.",
            "\t\t *",
            "\t\t * Skip the LLC logic because it's not relevant in that case.",
            "\t\t */",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));",
            "\tif (sds) {",
            "\t\t/*",
            "\t\t * If there is an imbalance between LLC domains (IOW we could",
            "\t\t * increase the overall cache use), we need some less-loaded LLC",
            "\t\t * domain to pull some load. Likewise, we may need to spread",
            "\t\t * load within the current LLC domain (e.g. packed SMT cores but",
            "\t\t * other CPUs are idle). We can't really know from here how busy",
            "\t\t * the others are - so just get a nohz balance going if it looks",
            "\t\t * like this LLC domain has tasks we could move.",
            "\t\t */",
            "\t\tnr_busy = atomic_read(&sds->nr_busy_cpus);",
            "\t\tif (nr_busy > 1) {",
            "\t\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "\t}",
            "unlock:",
            "\trcu_read_unlock();",
            "out:",
            "\tif (READ_ONCE(nohz.needs_update))",
            "\t\tflags |= NOHZ_NEXT_KICK;",
            "",
            "\tif (flags)",
            "\t\tkick_ilb(flags);",
            "}"
          ],
          "function_name": "on_null_domain, find_new_ilb, kick_ilb, nohz_balancer_kick",
          "description": "在非抢占模式下查找可用空闲负载均衡器CPU，通过IPI触发跨CPU的任务迁移",
          "similarity": 0.4967217743396759
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/fair.c",
          "start_line": 2200,
          "end_line": 2414,
          "content": [
            "static bool load_too_imbalanced(long src_load, long dst_load,",
            "\t\t\t\tstruct task_numa_env *env)",
            "{",
            "\tlong imb, old_imb;",
            "\tlong orig_src_load, orig_dst_load;",
            "\tlong src_capacity, dst_capacity;",
            "",
            "\t/*",
            "\t * The load is corrected for the CPU capacity available on each node.",
            "\t *",
            "\t * src_load        dst_load",
            "\t * ------------ vs ---------",
            "\t * src_capacity    dst_capacity",
            "\t */",
            "\tsrc_capacity = env->src_stats.compute_capacity;",
            "\tdst_capacity = env->dst_stats.compute_capacity;",
            "",
            "\timb = abs(dst_load * src_capacity - src_load * dst_capacity);",
            "",
            "\torig_src_load = env->src_stats.load;",
            "\torig_dst_load = env->dst_stats.load;",
            "",
            "\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);",
            "",
            "\t/* Would this change make things worse? */",
            "\treturn (imb > old_imb);",
            "}",
            "static bool task_numa_compare(struct task_numa_env *env,",
            "\t\t\t      long taskimp, long groupimp, bool maymove)",
            "{",
            "\tstruct numa_group *cur_ng, *p_ng = deref_curr_numa_group(env->p);",
            "\tstruct rq *dst_rq = cpu_rq(env->dst_cpu);",
            "\tlong imp = p_ng ? groupimp : taskimp;",
            "\tstruct task_struct *cur;",
            "\tlong src_load, dst_load;",
            "\tint dist = env->dist;",
            "\tlong moveimp = imp;",
            "\tlong load;",
            "\tbool stopsearch = false;",
            "",
            "\tif (READ_ONCE(dst_rq->numa_migrate_on))",
            "\t\treturn false;",
            "",
            "\trcu_read_lock();",
            "\tcur = rcu_dereference(dst_rq->curr);",
            "\tif (cur && ((cur->flags & (PF_EXITING | PF_KTHREAD)) ||",
            "\t\t    !cur->mm))",
            "\t\tcur = NULL;",
            "",
            "\t/*",
            "\t * Because we have preemption enabled we can get migrated around and",
            "\t * end try selecting ourselves (current == env->p) as a swap candidate.",
            "\t */",
            "\tif (cur == env->p) {",
            "\t\tstopsearch = true;",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\tif (!cur) {",
            "\t\tif (maymove && moveimp >= env->best_imp)",
            "\t\t\tgoto assign;",
            "\t\telse",
            "\t\t\tgoto unlock;",
            "\t}",
            "",
            "\t/* Skip this swap candidate if cannot move to the source cpu. */",
            "\tif (!cpumask_test_cpu(env->src_cpu, cur->cpus_ptr))",
            "\t\tgoto unlock;",
            "",
            "\t/*",
            "\t * Skip this swap candidate if it is not moving to its preferred",
            "\t * node and the best task is.",
            "\t */",
            "\tif (env->best_task &&",
            "\t    env->best_task->numa_preferred_nid == env->src_nid &&",
            "\t    cur->numa_preferred_nid != env->src_nid) {",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\t/*",
            "\t * \"imp\" is the fault differential for the source task between the",
            "\t * source and destination node. Calculate the total differential for",
            "\t * the source task and potential destination task. The more negative",
            "\t * the value is, the more remote accesses that would be expected to",
            "\t * be incurred if the tasks were swapped.",
            "\t *",
            "\t * If dst and source tasks are in the same NUMA group, or not",
            "\t * in any group then look only at task weights.",
            "\t */",
            "\tcur_ng = rcu_dereference(cur->numa_group);",
            "\tif (cur_ng == p_ng) {",
            "\t\t/*",
            "\t\t * Do not swap within a group or between tasks that have",
            "\t\t * no group if there is spare capacity. Swapping does",
            "\t\t * not address the load imbalance and helps one task at",
            "\t\t * the cost of punishing another.",
            "\t\t */",
            "\t\tif (env->dst_stats.node_type == node_has_spare)",
            "\t\t\tgoto unlock;",
            "",
            "\t\timp = taskimp + task_weight(cur, env->src_nid, dist) -",
            "\t\t      task_weight(cur, env->dst_nid, dist);",
            "\t\t/*",
            "\t\t * Add some hysteresis to prevent swapping the",
            "\t\t * tasks within a group over tiny differences.",
            "\t\t */",
            "\t\tif (cur_ng)",
            "\t\t\timp -= imp / 16;",
            "\t} else {",
            "\t\t/*",
            "\t\t * Compare the group weights. If a task is all by itself",
            "\t\t * (not part of a group), use the task weight instead.",
            "\t\t */",
            "\t\tif (cur_ng && p_ng)",
            "\t\t\timp += group_weight(cur, env->src_nid, dist) -",
            "\t\t\t       group_weight(cur, env->dst_nid, dist);",
            "\t\telse",
            "\t\t\timp += task_weight(cur, env->src_nid, dist) -",
            "\t\t\t       task_weight(cur, env->dst_nid, dist);",
            "\t}",
            "",
            "\t/* Discourage picking a task already on its preferred node */",
            "\tif (cur->numa_preferred_nid == env->dst_nid)",
            "\t\timp -= imp / 16;",
            "",
            "\t/*",
            "\t * Encourage picking a task that moves to its preferred node.",
            "\t * This potentially makes imp larger than it's maximum of",
            "\t * 1998 (see SMALLIMP and task_weight for why) but in this",
            "\t * case, it does not matter.",
            "\t */",
            "\tif (cur->numa_preferred_nid == env->src_nid)",
            "\t\timp += imp / 8;",
            "",
            "\tif (maymove && moveimp > imp && moveimp > env->best_imp) {",
            "\t\timp = moveimp;",
            "\t\tcur = NULL;",
            "\t\tgoto assign;",
            "\t}",
            "",
            "\t/*",
            "\t * Prefer swapping with a task moving to its preferred node over a",
            "\t * task that is not.",
            "\t */",
            "\tif (env->best_task && cur->numa_preferred_nid == env->src_nid &&",
            "\t    env->best_task->numa_preferred_nid != env->src_nid) {",
            "\t\tgoto assign;",
            "\t}",
            "",
            "\t/*",
            "\t * If the NUMA importance is less than SMALLIMP,",
            "\t * task migration might only result in ping pong",
            "\t * of tasks and also hurt performance due to cache",
            "\t * misses.",
            "\t */",
            "\tif (imp < SMALLIMP || imp <= env->best_imp + SMALLIMP / 2)",
            "\t\tgoto unlock;",
            "",
            "\t/*",
            "\t * In the overloaded case, try and keep the load balanced.",
            "\t */",
            "\tload = task_h_load(env->p) - task_h_load(cur);",
            "\tif (!load)",
            "\t\tgoto assign;",
            "",
            "\tdst_load = env->dst_stats.load + load;",
            "\tsrc_load = env->src_stats.load - load;",
            "",
            "\tif (load_too_imbalanced(src_load, dst_load, env))",
            "\t\tgoto unlock;",
            "",
            "assign:",
            "\t/* Evaluate an idle CPU for a task numa move. */",
            "\tif (!cur) {",
            "\t\tint cpu = env->dst_stats.idle_cpu;",
            "",
            "\t\t/* Nothing cached so current CPU went idle since the search. */",
            "\t\tif (cpu < 0)",
            "\t\t\tcpu = env->dst_cpu;",
            "",
            "\t\t/*",
            "\t\t * If the CPU is no longer truly idle and the previous best CPU",
            "\t\t * is, keep using it.",
            "\t\t */",
            "\t\tif (!idle_cpu(cpu) && env->best_cpu >= 0 &&",
            "\t\t    idle_cpu(env->best_cpu)) {",
            "\t\t\tcpu = env->best_cpu;",
            "\t\t}",
            "",
            "\t\tenv->dst_cpu = cpu;",
            "\t}",
            "",
            "\ttask_numa_assign(env, cur, imp);",
            "",
            "\t/*",
            "\t * If a move to idle is allowed because there is capacity or load",
            "\t * balance improves then stop the search. While a better swap",
            "\t * candidate may exist, a search is not free.",
            "\t */",
            "\tif (maymove && !cur && env->best_cpu >= 0 && idle_cpu(env->best_cpu))",
            "\t\tstopsearch = true;",
            "",
            "\t/*",
            "\t * If a swap candidate must be identified and the current best task",
            "\t * moves its preferred node then stop the search.",
            "\t */",
            "\tif (!maymove && env->best_task &&",
            "\t    env->best_task->numa_preferred_nid == env->src_nid) {",
            "\t\tstopsearch = true;",
            "\t}",
            "unlock:",
            "\trcu_read_unlock();",
            "",
            "\treturn stopsearch;",
            "}"
          ],
          "function_name": "load_too_imbalanced, task_numa_compare",
          "description": "评估负载不平衡程度，通过任务权重差值对比候选任务，优先选择能改善负载且降低远程访问的迁移目标",
          "similarity": 0.46813422441482544
        },
        {
          "chunk_id": 65,
          "file_path": "kernel/sched/fair.c",
          "start_line": 11564,
          "end_line": 11837,
          "content": [
            "static int sched_balance_rq(int this_cpu, struct rq *this_rq,",
            "\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,",
            "\t\t\tint *continue_balancing)",
            "{",
            "\tint ld_moved, cur_ld_moved, active_balance = 0;",
            "\tstruct sched_domain *sd_parent = sd->parent;",
            "\tstruct sched_group *group;",
            "\tstruct rq *busiest;",
            "\tstruct rq_flags rf;",
            "\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);",
            "\tstruct lb_env env = {",
            "\t\t.sd\t\t= sd,",
            "\t\t.dst_cpu\t= this_cpu,",
            "\t\t.dst_rq\t\t= this_rq,",
            "\t\t.dst_grpmask    = group_balance_mask(sd->groups),",
            "\t\t.idle\t\t= idle,",
            "\t\t.loop_break\t= SCHED_NR_MIGRATE_BREAK,",
            "\t\t.cpus\t\t= cpus,",
            "\t\t.fbq_type\t= all,",
            "\t\t.tasks\t\t= LIST_HEAD_INIT(env.tasks),",
            "\t};",
            "",
            "\tcpumask_and(cpus, sched_domain_span(sd), cpu_active_mask);",
            "",
            "\tschedstat_inc(sd->lb_count[idle]);",
            "",
            "redo:",
            "\tif (!should_we_balance(&env)) {",
            "\t\t*continue_balancing = 0;",
            "\t\tgoto out_balanced;",
            "\t}",
            "",
            "\tgroup = sched_balance_find_src_group(&env);",
            "\tif (!group) {",
            "\t\tschedstat_inc(sd->lb_nobusyg[idle]);",
            "\t\tgoto out_balanced;",
            "\t}",
            "",
            "\tbusiest = find_busiest_queue(&env, group);",
            "\tif (!busiest) {",
            "\t\tschedstat_inc(sd->lb_nobusyq[idle]);",
            "\t\tgoto out_balanced;",
            "\t}",
            "",
            "\tWARN_ON_ONCE(busiest == env.dst_rq);",
            "",
            "\tschedstat_add(sd->lb_imbalance[idle], env.imbalance);",
            "",
            "\tenv.src_cpu = busiest->cpu;",
            "\tenv.src_rq = busiest;",
            "",
            "\tld_moved = 0;",
            "\t/* Clear this flag as soon as we find a pullable task */",
            "\tenv.flags |= LBF_ALL_PINNED;",
            "\tif (busiest->nr_running > 1) {",
            "\t\t/*",
            "\t\t * Attempt to move tasks. If sched_balance_find_src_group has found",
            "\t\t * an imbalance but busiest->nr_running <= 1, the group is",
            "\t\t * still unbalanced. ld_moved simply stays zero, so it is",
            "\t\t * correctly treated as an imbalance.",
            "\t\t */",
            "\t\tenv.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);",
            "",
            "more_balance:",
            "\t\trq_lock_irqsave(busiest, &rf);",
            "\t\tupdate_rq_clock(busiest);",
            "",
            "\t\t/*",
            "\t\t * cur_ld_moved - load moved in current iteration",
            "\t\t * ld_moved     - cumulative load moved across iterations",
            "\t\t */",
            "\t\tcur_ld_moved = detach_tasks(&env);",
            "",
            "\t\t/*",
            "\t\t * We've detached some tasks from busiest_rq. Every",
            "\t\t * task is masked \"TASK_ON_RQ_MIGRATING\", so we can safely",
            "\t\t * unlock busiest->lock, and we are able to be sure",
            "\t\t * that nobody can manipulate the tasks in parallel.",
            "\t\t * See task_rq_lock() family for the details.",
            "\t\t */",
            "",
            "\t\trq_unlock(busiest, &rf);",
            "",
            "\t\tif (cur_ld_moved) {",
            "\t\t\tattach_tasks(&env);",
            "\t\t\tld_moved += cur_ld_moved;",
            "\t\t}",
            "",
            "\t\tlocal_irq_restore(rf.flags);",
            "",
            "\t\tif (env.flags & LBF_NEED_BREAK) {",
            "\t\t\tenv.flags &= ~LBF_NEED_BREAK;",
            "\t\t\tgoto more_balance;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Revisit (affine) tasks on src_cpu that couldn't be moved to",
            "\t\t * us and move them to an alternate dst_cpu in our sched_group",
            "\t\t * where they can run. The upper limit on how many times we",
            "\t\t * iterate on same src_cpu is dependent on number of CPUs in our",
            "\t\t * sched_group.",
            "\t\t *",
            "\t\t * This changes load balance semantics a bit on who can move",
            "\t\t * load to a given_cpu. In addition to the given_cpu itself",
            "\t\t * (or a ilb_cpu acting on its behalf where given_cpu is",
            "\t\t * nohz-idle), we now have balance_cpu in a position to move",
            "\t\t * load to given_cpu. In rare situations, this may cause",
            "\t\t * conflicts (balance_cpu and given_cpu/ilb_cpu deciding",
            "\t\t * _independently_ and at _same_ time to move some load to",
            "\t\t * given_cpu) causing excess load to be moved to given_cpu.",
            "\t\t * This however should not happen so much in practice and",
            "\t\t * moreover subsequent load balance cycles should correct the",
            "\t\t * excess load moved.",
            "\t\t */",
            "\t\tif ((env.flags & LBF_DST_PINNED) && env.imbalance > 0) {",
            "",
            "\t\t\t/* Prevent to re-select dst_cpu via env's CPUs */",
            "\t\t\t__cpumask_clear_cpu(env.dst_cpu, env.cpus);",
            "",
            "\t\t\tenv.dst_rq\t = cpu_rq(env.new_dst_cpu);",
            "\t\t\tenv.dst_cpu\t = env.new_dst_cpu;",
            "\t\t\tenv.flags\t&= ~LBF_DST_PINNED;",
            "\t\t\tenv.loop\t = 0;",
            "\t\t\tenv.loop_break\t = SCHED_NR_MIGRATE_BREAK;",
            "",
            "\t\t\t/*",
            "\t\t\t * Go back to \"more_balance\" rather than \"redo\" since we",
            "\t\t\t * need to continue with same src_cpu.",
            "\t\t\t */",
            "\t\t\tgoto more_balance;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * We failed to reach balance because of affinity.",
            "\t\t */",
            "\t\tif (sd_parent) {",
            "\t\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;",
            "",
            "\t\t\tif ((env.flags & LBF_SOME_PINNED) && env.imbalance > 0)",
            "\t\t\t\t*group_imbalance = 1;",
            "\t\t}",
            "",
            "\t\t/* All tasks on this runqueue were pinned by CPU affinity */",
            "\t\tif (unlikely(env.flags & LBF_ALL_PINNED)) {",
            "\t\t\t__cpumask_clear_cpu(cpu_of(busiest), cpus);",
            "\t\t\t/*",
            "\t\t\t * Attempting to continue load balancing at the current",
            "\t\t\t * sched_domain level only makes sense if there are",
            "\t\t\t * active CPUs remaining as possible busiest CPUs to",
            "\t\t\t * pull load from which are not contained within the",
            "\t\t\t * destination group that is receiving any migrated",
            "\t\t\t * load.",
            "\t\t\t */",
            "\t\t\tif (!cpumask_subset(cpus, env.dst_grpmask)) {",
            "\t\t\t\tenv.loop = 0;",
            "\t\t\t\tenv.loop_break = SCHED_NR_MIGRATE_BREAK;",
            "\t\t\t\tgoto redo;",
            "\t\t\t}",
            "\t\t\tgoto out_all_pinned;",
            "\t\t}",
            "\t}",
            "",
            "\tif (!ld_moved) {",
            "\t\tschedstat_inc(sd->lb_failed[idle]);",
            "\t\t/*",
            "\t\t * Increment the failure counter only on periodic balance.",
            "\t\t * We do not want newidle balance, which can be very",
            "\t\t * frequent, pollute the failure counter causing",
            "\t\t * excessive cache_hot migrations and active balances.",
            "\t\t *",
            "\t\t * Similarly for migration_misfit which is not related to",
            "\t\t * load/util migration, don't pollute nr_balance_failed.",
            "\t\t */",
            "\t\tif (idle != CPU_NEWLY_IDLE &&",
            "\t\t    env.migration_type != migrate_misfit)",
            "\t\t\tsd->nr_balance_failed++;",
            "",
            "\t\tif (need_active_balance(&env)) {",
            "\t\t\tunsigned long flags;",
            "",
            "\t\t\traw_spin_rq_lock_irqsave(busiest, flags);",
            "",
            "\t\t\t/*",
            "\t\t\t * Don't kick the active_load_balance_cpu_stop,",
            "\t\t\t * if the curr task on busiest CPU can't be",
            "\t\t\t * moved to this_cpu:",
            "\t\t\t */",
            "\t\t\tif (!cpumask_test_cpu(this_cpu, busiest->curr->cpus_ptr)) {",
            "\t\t\t\traw_spin_rq_unlock_irqrestore(busiest, flags);",
            "\t\t\t\tgoto out_one_pinned;",
            "\t\t\t}",
            "",
            "\t\t\t/* Record that we found at least one task that could run on this_cpu */",
            "\t\t\tenv.flags &= ~LBF_ALL_PINNED;",
            "",
            "\t\t\t/*",
            "\t\t\t * ->active_balance synchronizes accesses to",
            "\t\t\t * ->active_balance_work.  Once set, it's cleared",
            "\t\t\t * only after active load balance is finished.",
            "\t\t\t */",
            "\t\t\tif (!busiest->active_balance) {",
            "\t\t\t\tbusiest->active_balance = 1;",
            "\t\t\t\tbusiest->push_cpu = this_cpu;",
            "\t\t\t\tactive_balance = 1;",
            "\t\t\t}",
            "",
            "\t\t\tpreempt_disable();",
            "\t\t\traw_spin_rq_unlock_irqrestore(busiest, flags);",
            "\t\t\tif (active_balance) {",
            "\t\t\t\tstop_one_cpu_nowait(cpu_of(busiest),",
            "\t\t\t\t\tactive_load_balance_cpu_stop, busiest,",
            "\t\t\t\t\t&busiest->active_balance_work);",
            "\t\t\t}",
            "\t\t\tpreempt_enable();",
            "\t\t}",
            "\t} else {",
            "\t\tsd->nr_balance_failed = 0;",
            "\t}",
            "",
            "\tif (likely(!active_balance) || need_active_balance(&env)) {",
            "\t\t/* We were unbalanced, so reset the balancing interval */",
            "\t\tsd->balance_interval = sd->min_interval;",
            "\t}",
            "",
            "\tgoto out;",
            "",
            "out_balanced:",
            "\t/*",
            "\t * We reach balance although we may have faced some affinity",
            "\t * constraints. Clear the imbalance flag only if other tasks got",
            "\t * a chance to move and fix the imbalance.",
            "\t */",
            "\tif (sd_parent && !(env.flags & LBF_ALL_PINNED)) {",
            "\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;",
            "",
            "\t\tif (*group_imbalance)",
            "\t\t\t*group_imbalance = 0;",
            "\t}",
            "",
            "out_all_pinned:",
            "\t/*",
            "\t * We reach balance because all tasks are pinned at this level so",
            "\t * we can't migrate them. Let the imbalance flag set so parent level",
            "\t * can try to migrate them.",
            "\t */",
            "\tschedstat_inc(sd->lb_balanced[idle]);",
            "",
            "\tsd->nr_balance_failed = 0;",
            "",
            "out_one_pinned:",
            "\tld_moved = 0;",
            "",
            "\t/*",
            "\t * sched_balance_newidle() disregards balance intervals, so we could",
            "\t * repeatedly reach this code, which would lead to balance_interval",
            "\t * skyrocketing in a short amount of time. Skip the balance_interval",
            "\t * increase logic to avoid that.",
            "\t *",
            "\t * Similarly misfit migration which is not necessarily an indication of",
            "\t * the system being busy and requires lb to backoff to let it settle",
            "\t * down.",
            "\t */",
            "\tif (env.idle == CPU_NEWLY_IDLE ||",
            "\t    env.migration_type == migrate_misfit)",
            "\t\tgoto out;",
            "",
            "\t/* tune up the balancing interval */",
            "\tif ((env.flags & LBF_ALL_PINNED &&",
            "\t     sd->balance_interval < MAX_PINNED_INTERVAL) ||",
            "\t    sd->balance_interval < sd->max_interval)",
            "\t\tsd->balance_interval *= 2;",
            "out:",
            "\treturn ld_moved;",
            "}"
          ],
          "function_name": "sched_balance_rq",
          "description": "实现负载平衡逻辑，尝试从繁忙运行队列迁移任务到当前CPU，处理迁移失败场景，更新统计信息并调整平衡间隔",
          "similarity": 0.4629243016242981
        },
        {
          "chunk_id": 45,
          "file_path": "kernel/sched/fair.c",
          "start_line": 7352,
          "end_line": 7468,
          "content": [
            "static int",
            "wake_affine_weight(struct sched_domain *sd, struct task_struct *p,",
            "\t\t   int this_cpu, int prev_cpu, int sync)",
            "{",
            "\ts64 this_eff_load, prev_eff_load;",
            "\tunsigned long task_load;",
            "",
            "\tthis_eff_load = cpu_load(cpu_rq(this_cpu));",
            "",
            "\tif (sync) {",
            "\t\tunsigned long current_load = task_h_load(current);",
            "",
            "\t\tif (current_load > this_eff_load)",
            "\t\t\treturn this_cpu;",
            "",
            "\t\tthis_eff_load -= current_load;",
            "\t}",
            "",
            "\ttask_load = task_h_load(p);",
            "",
            "\tthis_eff_load += task_load;",
            "\tif (sched_feat(WA_BIAS))",
            "\t\tthis_eff_load *= 100;",
            "\tthis_eff_load *= capacity_of(prev_cpu);",
            "",
            "\tprev_eff_load = cpu_load(cpu_rq(prev_cpu));",
            "\tprev_eff_load -= task_load;",
            "\tif (sched_feat(WA_BIAS))",
            "\t\tprev_eff_load *= 100 + (sd->imbalance_pct - 100) / 2;",
            "\tprev_eff_load *= capacity_of(this_cpu);",
            "",
            "\t/*",
            "\t * If sync, adjust the weight of prev_eff_load such that if",
            "\t * prev_eff == this_eff that select_idle_sibling() will consider",
            "\t * stacking the wakee on top of the waker if no other CPU is",
            "\t * idle.",
            "\t */",
            "\tif (sync)",
            "\t\tprev_eff_load += 1;",
            "",
            "\treturn this_eff_load < prev_eff_load ? this_cpu : nr_cpumask_bits;",
            "}",
            "static int wake_affine(struct sched_domain *sd, struct task_struct *p,",
            "\t\t       int this_cpu, int prev_cpu, int sync)",
            "{",
            "\tint target = nr_cpumask_bits;",
            "",
            "\tif (sched_feat(WA_IDLE))",
            "\t\ttarget = wake_affine_idle(this_cpu, prev_cpu, sync);",
            "",
            "\tif (sched_feat(WA_WEIGHT) && target == nr_cpumask_bits)",
            "\t\ttarget = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);",
            "",
            "\tschedstat_inc(p->stats.nr_wakeups_affine_attempts);",
            "\tif (target != this_cpu)",
            "\t\treturn prev_cpu;",
            "",
            "\tschedstat_inc(sd->ttwu_move_affine);",
            "\tschedstat_inc(p->stats.nr_wakeups_affine);",
            "\treturn target;",
            "}",
            "static int",
            "sched_balance_find_dst_group_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)",
            "{",
            "\tunsigned long load, min_load = ULONG_MAX;",
            "\tunsigned int min_exit_latency = UINT_MAX;",
            "\tu64 latest_idle_timestamp = 0;",
            "\tint least_loaded_cpu = this_cpu;",
            "\tint shallowest_idle_cpu = -1;",
            "\tint i;",
            "",
            "\t/* Check if we have any choice: */",
            "\tif (group->group_weight == 1)",
            "\t\treturn cpumask_first(sched_group_span(group));",
            "",
            "\t/* Traverse only the allowed CPUs */",
            "\tfor_each_cpu_and(i, sched_group_span(group), p->cpus_ptr) {",
            "\t\tstruct rq *rq = cpu_rq(i);",
            "",
            "\t\tif (!sched_core_cookie_match(rq, p))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (sched_idle_cpu(i))",
            "\t\t\treturn i;",
            "",
            "\t\tif (available_idle_cpu(i)) {",
            "\t\t\tstruct cpuidle_state *idle = idle_get_state(rq);",
            "\t\t\tif (idle && idle->exit_latency < min_exit_latency) {",
            "\t\t\t\t/*",
            "\t\t\t\t * We give priority to a CPU whose idle state",
            "\t\t\t\t * has the smallest exit latency irrespective",
            "\t\t\t\t * of any idle timestamp.",
            "\t\t\t\t */",
            "\t\t\t\tmin_exit_latency = idle->exit_latency;",
            "\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;",
            "\t\t\t\tshallowest_idle_cpu = i;",
            "\t\t\t} else if ((!idle || idle->exit_latency == min_exit_latency) &&",
            "\t\t\t\t   rq->idle_stamp > latest_idle_timestamp) {",
            "\t\t\t\t/*",
            "\t\t\t\t * If equal or no active idle state, then",
            "\t\t\t\t * the most recently idled CPU might have",
            "\t\t\t\t * a warmer cache.",
            "\t\t\t\t */",
            "\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;",
            "\t\t\t\tshallowest_idle_cpu = i;",
            "\t\t\t}",
            "\t\t} else if (shallowest_idle_cpu == -1) {",
            "\t\t\tload = cpu_load(cpu_rq(i));",
            "\t\t\tif (load < min_load) {",
            "\t\t\t\tmin_load = load;",
            "\t\t\t\tleast_loaded_cpu = i;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\treturn shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu;",
            "}"
          ],
          "function_name": "wake_affine_weight, wake_affine, sched_balance_find_dst_group_cpu",
          "description": "wake_affine_weight计算任务迁移后的目标CPU负载差异，结合权重和缓存亲和性调整负载比对结果；wake_affine根据WA_IDLE/WA_WEIGHT特性选择目标CPU并统计唤醒次数；sched_balance_find_dst_group_cpu遍历调度组内CPU，优先选择空闲且具有最小退出延迟的CPU",
          "similarity": 0.45141011476516724
        }
      ]
    },
    {
      "source_file": "kernel/sched/features.h",
      "md_summary": "> 自动生成时间: 2025-10-25 16:09:45\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\features.h`\n\n---\n\n# `sched/features.h` 技术文档\n\n## 1. 文件概述\n\n`sched/features.h` 是 Linux 内核调度器（CFS 和 EEVDF 调度类）中用于定义和管理**调度特性（Scheduling Features）** 的头文件。该文件通过宏 `SCHED_FEAT(name, enabled)` 声明一系列可配置的调度行为开关，用于控制调度器在运行时的各种策略，如任务放置、抢占、迁移、缓存局部性优化、延迟处理、利用率估计等。这些特性通常在编译时默认启用或禁用，但部分可通过 `/sys/kernel/debug/sched_features` 在运行时动态调整。\n\n## 2. 核心功能\n\n本文件不包含函数或数据结构定义，而是通过一系列 `SCHED_FEAT(feature_name, default_value)` 宏声明调度器的**可配置特性标志**。每个特性对应一个布尔开关，控制调度器某一方面的行为逻辑。主要特性包括：\n\n- **任务放置策略**：`PLACE_LAG`、`PLACE_DEADLINE_INITIAL`、`PLACE_REL_DEADLINE`\n- **抢占控制**：`RUN_TO_PARITY`、`PREEMPT_SHORT`、`WAKEUP_PREEMPTION`\n- **缓存局部性优化**：`NEXT_BUDDY`、`CACHE_HOT_BUDDY`\n- **延迟出队机制**：`DELAY_DEQUEUE`、`DELAY_ZERO`\n- **高精度定时器支持**：`HRTICK`、`HRTICK_DL`\n- **CPU 容量与负载管理**：`NONTASK_CAPACITY`、`ATTACH_AGE_LOAD`\n- **唤醒优化**：`TTWU_QUEUE`、`SIS_UTIL`\n- **实时任务调度优化**：`RT_PUSH_IPI`、`RT_RUNTIME_SHARE`\n- **利用率估计**：`UTIL_EST`、`UTIL_EST_FASTUP`\n- **调试与告警**：`WARN_DOUBLE_CLOCK`、`LATENCY_WARN`\n\n## 3. 关键实现\n\n- **`SCHED_FEAT` 宏机制**：  \n  该宏在 `kernel/sched/features.h` 中定义（通常通过 `#define SCHED_FEAT(x, enabled) SCHED_FEAT_##x`），最终在 `kernel/sched/core.c` 中展开为位图（`sysctl_sched_features`）中的位标志。调度器代码通过 `sched_feat(FEAT_NAME)` 宏查询某特性是否启用。\n\n- **EEVDF 相关特性**：\n  - `PLACE_LAG`：启用后，任务在睡眠/唤醒周期中保留其虚拟运行时间（avg_vruntime）的“滞后”（lag），确保公平性。这是 EEVDF（Earliest Eligible Virtual Deadline First）调度器的核心策略之一。\n  - `PLACE_DEADLINE_INITIAL`：新任务初始虚拟截止时间设为当前时间加半个时间片，避免新任务因虚拟截止时间过早而过度抢占。\n  - `PLACE_REL_DEADLINE`：任务迁移时保持其相对于当前虚拟时间的截止时间偏移，维持调度公平性。\n\n- **抢占抑制与唤醒抢占**：\n  - `RUN_TO_PARITY`：禁止唤醒抢占，直到当前任务达到“零滞后点”（即其虚拟运行时间追平队列平均值）或耗尽时间片。\n  - `PREEMPT_SHORT`：允许具有更短时间片的唤醒任务抢占当前任务，即使 `RESPECT_SLICE` 被设置。\n  - `WAKEUP_PREEMPTION`：启用唤醒时的抢占检查，是 CFS/EEVDF 实现低延迟响应的关键。\n\n- **缓存局部性优化**：\n  - `NEXT_BUDDY`（默认关闭）：优先调度最近被唤醒但未成功抢占的任务，因其可能复用刚访问的数据。\n  - `CACHE_HOT_BUDDY`：将 buddy 任务视为缓存热任务，降低其被迁移的概率。\n\n- **延迟出队（`DELAY_DEQUEUE`）**：  \n  非就绪任务（如睡眠中）不会立即从运行队列移除，使其保留在调度竞争中以“消耗”负滞后（negative lag），当选中时自然具有正滞后，提升调度平滑性。`DELAY_ZERO` 则在出队或唤醒时将滞后裁剪为 0。\n\n- **TTWU_QUEUE 优化**：  \n  在非 `PREEMPT_RT` 配置下，默认启用远程唤醒排队机制，通过调度 IPI 异步处理跨 CPU 唤醒，减少运行队列锁竞争。\n\n- **利用率估计（Utilization Estimation）**：  \n  `UTIL_EST` 启用基于 PELT（Per-Entity Load Tracking）信号的 CPU 利用率估计，`UTIL_EST_FASTUP` 允许利用率快速上升以响应突发负载，用于 EAS（Energy Aware Scheduling）等场景。\n\n- **RT 调度优化**：  \n  `RT_PUSH_IPI` 在支持 `HAVE_RT_PUSH_IPI` 的平台上启用，通过 IPI 推送高优先级 RT 任务，避免多 CPU 同时争抢单个运行队列锁导致的“惊群”问题。\n\n## 4. 依赖关系\n\n- **依赖头文件**：通常由 `kernel/sched/sched.h` 或 `kernel/sched/core.c` 包含。\n- **依赖配置选项**：\n  - `CONFIG_PREEMPT_RT`：影响 `TTWU_QUEUE` 默认值。\n  - `HAVE_RT_PUSH_IPI`：决定 `RT_PUSH_IPI` 特性是否定义。\n- **依赖调度核心模块**：特性标志在 `kernel/sched/core.c`、`kernel/sched/fair.c`（CFS/EEVDF）、`kernel/sched/rt.c`（实时调度）中被实际使用。\n- **依赖调试接口**：部分特性（如 `WARN_DOUBLE_CLOCK`、`LATENCY_WARN`）依赖内核调试基础设施。\n\n## 5. 使用场景\n\n- **调度策略调优**：系统管理员或开发者可通过 `/sys/kernel/debug/sched_features` 动态开启/关闭特性，以优化特定工作负载（如低延迟、高吞吐、能效）下的调度行为。\n- **EEVDF 调度器支持**：`PLACE_LAG`、`PLACE_DEADLINE_INITIAL` 等特性是 Linux 6.6+ 引入的 EEVDF 调度器实现公平性和响应性的关键。\n- **实时系统优化**：`RT_PUSH_IPI`、`RT_RUNTIME_SHARE` 等用于改善实时任务的调度延迟和 CPU 资源分配。\n- **能效调度（EAS）**：`UTIL_EST` 和 `UTIL_EST_FASTUP` 为 EAS 提供准确的 CPU 利用率预测，用于任务放置决策。\n- **性能调试**：`LATENCY_WARN`、`WARN_DOUBLE_CLOCK` 等特性用于检测调度器内部异常或性能瓶颈。\n- **多核扩展性优化**：`TTWU_QUEUE`、`SIS_UTIL` 减少跨 CPU 唤醒和 LLC 域扫描开销，提升大规模系统可扩展性。",
      "similarity": 0.42892980575561523,
      "chunks": []
    },
    {
      "source_file": "kernel/locking/semaphore.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:52:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\semaphore.c`\n\n---\n\n# `locking/semaphore.c` 技术文档\n\n## 1. 文件概述\n\n`locking/semaphore.c` 实现了 Linux 内核中的**计数信号量（counting semaphore）**机制。计数信号量允许多个任务（最多为初始计数值）同时持有该锁，当计数值耗尽时，后续请求者将被阻塞，直到有其他任务释放信号量。与互斥锁（mutex）不同，信号量支持更灵活的并发控制，适用于资源池、限流等场景。该文件提供了多种获取和释放信号量的接口，包括可中断、可超时、不可中断等变体，并支持在中断上下文中调用部分函数。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `down(struct semaphore *sem)` | 不可中断地获取信号量，若不可用则睡眠。**已弃用**，建议使用可中断版本。 |\n| `down_interruptible(struct semaphore *sem)` | 可被普通信号中断的获取操作，成功返回 0，被信号中断返回 `-EINTR`。 |\n| `down_killable(struct semaphore *sem)` | 可被致命信号（fatal signal）中断的获取操作，返回值同上。 |\n| `down_trylock(struct semaphore *sem)` | 非阻塞尝试获取信号量，成功返回 0，失败返回 1（**注意返回值与 mutex/spinlock 相反**）。 |\n| `down_timeout(struct semaphore *sem, long timeout)` | 带超时的获取操作，超时返回 `-ETIME`，成功返回 0。 |\n| `up(struct semaphore *sem)` | 释放信号量，可由任意上下文（包括中断）调用，唤醒等待队列中的任务。 |\n\n### 静态辅助函数\n\n- `__down*()` 系列：处理信号量争用时的阻塞逻辑。\n- `__up()`：在有等待者时执行唤醒逻辑。\n- `___down_common()`：通用的阻塞等待实现，支持不同睡眠状态和超时。\n- `__sem_acquire()`：原子减少计数并记录持有者（用于 hung task 检测）。\n\n### 数据结构\n\n- `struct semaphore`（定义在 `<linux/semaphore.h>`）：\n  - `count`：当前可用资源数（>0 表示可立即获取）。\n  - `wait_list`：等待该信号量的任务链表。\n  - `lock`：保护上述成员的原始自旋锁（`raw_spinlock_t`）。\n  - `last_holder`（条件编译）：记录最后持有者，用于 `CONFIG_DETECT_HUNG_TASK_BLOCKER`。\n\n- `struct semaphore_waiter`：\n  - 用于将任务加入等待队列，包含任务指针和唤醒标志（`up`）。\n\n## 3. 关键实现\n\n### 中断安全与自旋锁\n- 所有对外接口（包括 `down*` 和 `up`）均使用 `raw_spin_lock_irqsave()` 获取自旋锁，确保在中断上下文安全。\n- 即使 `down()` 等函数通常在进程上下文调用，也使用 `irqsave` 变体，因为内核某些部分依赖在中断上下文成功调用 `down()`（当确定信号量可用时）。\n\n### 计数语义\n- `sem->count` 表示**还可被获取的次数**。初始值由 `sema_init()` 设置。\n- 获取时：若 `count > 0`，直接减 1；否则加入等待队列。\n- 释放时：若等待队列为空，`count++`；否则唤醒队首任务。\n\n### 等待与唤醒机制\n- 使用 `wake_q`（批量唤醒队列）优化唤醒路径，避免在持有自旋锁时调用 `wake_up_process()`。\n- 等待任务通过 `schedule_timeout()` 睡眠，并在循环中检查：\n  - 是否收到信号（根据睡眠状态判断）。\n  - 是否超时。\n  - 是否被 `__up()` 标记为 `waiter.up = true`（表示已被选中唤醒）。\n\n### Hung Task 支持\n- 当启用 `CONFIG_DETECT_HUNG_TASK_BLOCKER` 时：\n  - 获取信号量时记录当前任务为 `last_holder`。\n  - 释放时若当前任务是持有者，则清除记录。\n  - 提供 `sem_last_holder()` 供 hung task 检测模块查询阻塞源头。\n\n### 返回值约定\n- `down_trylock()` 返回 **0 表示成功**，**1 表示失败**，这与 `mutex_trylock()` 和 `spin_trylock()` **相反**，需特别注意。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/semaphore.h>`：信号量结构体和 API 声明。\n  - `<linux/spinlock.h>`：原始自旋锁实现。\n  - `<linux/sched.h>`、`<linux/sched/wake_q.h>`：任务调度和批量唤醒。\n  - `<trace/events/lock.h>`：锁争用跟踪点。\n  - `<linux/hung_task.h>`：hung task 检测支持。\n\n- **内核配置依赖**：\n  - `CONFIG_DETECT_HUNG_TASK_BLOCKER`：启用信号量持有者跟踪。\n\n- **与其他同步原语关系**：\n  - 与 `mutex.c` 形成对比：mutex 是二值、不可递归、带调试信息的互斥锁；信号量是计数、可被任意任务释放、更轻量。\n  - 底层依赖调度器（`schedule_timeout`）和中断管理（`irqsave`）。\n\n## 5. 使用场景\n\n- **资源池管理**：如限制同时访问某类硬件设备的任务数量。\n- **读写并发控制**：配合其他机制实现多读者/单写者模型。\n- **内核驱动**：设备驱动中控制对共享资源的并发访问。\n- **中断上下文释放**：因 `up()` 可在中断中调用，适用于中断处理程序释放资源的场景。\n- **不可睡眠路径**：使用 `down_trylock()` 在原子上下文尝试获取资源。\n\n> **注意**：由于信号量不强制所有权（任意任务可调用 `up()`），且缺乏死锁检测等调试特性，现代内核开发中更推荐使用 `mutex` 或 `rwsem`，除非明确需要计数语义或多释放者特性。",
      "similarity": 0.428705096244812,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 252,
          "end_line": 323,
          "content": [
            "static inline int __sched ___down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\t\t\t\tlong timeout)",
            "{",
            "\tstruct semaphore_waiter waiter;",
            "",
            "\tlist_add_tail(&waiter.list, &sem->wait_list);",
            "\twaiter.task = current;",
            "\twaiter.up = false;",
            "",
            "\tfor (;;) {",
            "\t\tif (signal_pending_state(state, current))",
            "\t\t\tgoto interrupted;",
            "\t\tif (unlikely(timeout <= 0))",
            "\t\t\tgoto timed_out;",
            "\t\t__set_current_state(state);",
            "\t\traw_spin_unlock_irq(&sem->lock);",
            "\t\ttimeout = schedule_timeout(timeout);",
            "\t\traw_spin_lock_irq(&sem->lock);",
            "\t\tif (waiter.up) {",
            "\t\t\thung_task_sem_set_holder(sem);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            " timed_out:",
            "\tlist_del(&waiter.list);",
            "\treturn -ETIME;",
            "",
            " interrupted:",
            "\tlist_del(&waiter.list);",
            "\treturn -EINTR;",
            "}",
            "static inline int __sched __down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\tlong timeout)",
            "{",
            "\tint ret;",
            "",
            "\thung_task_set_blocker(sem, BLOCKER_TYPE_SEM);",
            "",
            "\ttrace_contention_begin(sem, 0);",
            "\tret = ___down_common(sem, state, timeout);",
            "\ttrace_contention_end(sem, ret);",
            "",
            "\thung_task_clear_blocker();",
            "",
            "\treturn ret;",
            "}",
            "static noinline void __sched __down(struct semaphore *sem)",
            "{",
            "\t__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_interruptible(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_killable(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_KILLABLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\treturn __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);",
            "}",
            "static noinline void __sched __up(struct semaphore *sem,",
            "\t\t\t\t  struct wake_q_head *wake_q)",
            "{",
            "\tstruct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,",
            "\t\t\t\t\t\tstruct semaphore_waiter, list);",
            "\tlist_del(&waiter->list);",
            "\twaiter->up = true;",
            "\twake_q_add(wake_q, waiter->task);",
            "}"
          ],
          "function_name": "___down_common, __down_common, __down, __down_interruptible, __down_killable, __down_timeout, __up",
          "description": "实现了信号量的阻塞等待通用逻辑，包含___down_common/__down_common等辅助函数，处理信号量不足时的任务挂起、超时检测、信号处理及唤醒机制，通过循环等待并结合schedule_timeout实现阻塞式资源竞争解决",
          "similarity": 0.4889010190963745
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "\tWRITE_ONCE((sem)->last_holder, (unsigned long)current);",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "\tif (READ_ONCE((sem)->last_holder) == (unsigned long)current)",
            "\t\tWRITE_ONCE((sem)->last_holder, 0UL);",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn READ_ONCE(sem->last_holder);",
            "}",
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn 0UL;",
            "}",
            "static inline void __sem_acquire(struct semaphore *sem)",
            "{",
            "\tsem->count--;",
            "\thung_task_sem_set_holder(sem);",
            "}",
            "void __sched down(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\t__down(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "}",
            "int __sched down_interruptible(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_interruptible(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_killable(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_killable(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_trylock(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint count;",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tcount = sem->count - 1;",
            "\tif (likely(count >= 0))",
            "\t\t__sem_acquire(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn (count < 0);",
            "}",
            "int __sched down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_timeout(sem, timeout);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "void __sched up(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "",
            "\thung_task_sem_clear_if_holder(sem);",
            "",
            "\tif (likely(list_empty(&sem->wait_list)))",
            "\t\tsem->count++;",
            "\telse",
            "\t\t__up(sem, &wake_q);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "\tif (!wake_q_empty(&wake_q))",
            "\t\twake_up_q(&wake_q);",
            "}"
          ],
          "function_name": "hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, __sem_acquire, down, down_interruptible, down_killable, down_trylock, down_timeout, up",
          "description": "实现了信号量的获取与释放核心逻辑，包括down/down_interruptible/down_killable/down_trylock/down_timeout等接口，通过spinlock保护共享资源，维护等待队列并处理任务状态变更，其中包含Hung Task检测相关函数的条件性实现",
          "similarity": 0.4441835284233093
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2008 Intel Corporation",
            " * Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This file implements counting semaphores.",
            " * A counting semaphore may be acquired 'n' times before sleeping.",
            " * See mutex.c for single-acquisition sleeping locks which enforce",
            " * rules which allow code to be debugged more easily.",
            " */",
            "",
            "/*",
            " * Some notes on the implementation:",
            " *",
            " * The spinlock controls access to the other members of the semaphore.",
            " * down_trylock() and up() can be called from interrupt context, so we",
            " * have to disable interrupts when taking the lock.  It turns out various",
            " * parts of the kernel expect to be able to use down() on a semaphore in",
            " * interrupt context when they know it will succeed, so we have to use",
            " * irqsave variants for down(), down_interruptible() and down_killable()",
            " * too.",
            " *",
            " * The ->count variable represents how many more tasks can acquire this",
            " * semaphore.  If it's zero, there may be tasks waiting on the wait_list.",
            " */",
            "",
            "#include <linux/compiler.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/semaphore.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/ftrace.h>",
            "#include <trace/events/lock.h>",
            "#include <linux/hung_task.h>",
            "",
            "static noinline void __down(struct semaphore *sem);",
            "static noinline int __down_interruptible(struct semaphore *sem);",
            "static noinline int __down_killable(struct semaphore *sem);",
            "static noinline int __down_timeout(struct semaphore *sem, long timeout);",
            "static noinline void __up(struct semaphore *sem, struct wake_q_head *wake_q);",
            "",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER"
          ],
          "function_name": null,
          "description": "此代码块定义了计数信号量的基础框架，包含实现计数信号量所需的头文件和注释，声明了多个内联函数及辅助函数，用于处理信号量的获取、释放及Hung Task检测相关逻辑，但由于代码截断，CONFIG_DETECT_HUNG_TASK_BLOCKER部分缺失，上下文不完整",
          "similarity": 0.32823988795280457
        }
      ]
    }
  ]
}