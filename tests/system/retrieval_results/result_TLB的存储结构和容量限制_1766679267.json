{
  "query": "TLB的存储结构和容量限制",
  "timestamp": "2025-12-26 00:14:27",
  "retrieved_files": [
    {
      "source_file": "mm/mmu_gather.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:52:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mmu_gather.c`\n\n---\n\n# mmu_gather.c 技术文档\n\n## 1. 文件概述\n\n`mmu_gather.c` 是 Linux 内核内存管理子系统中的关键组件，负责在页表项（PTE）或更高层级页表被撤销映射（unmap）后，高效地批量释放对应的物理页面和页表结构。该文件实现了 **MMU gather** 机制，用于延迟并批量处理 TLB（Translation Lookaside Buffer）刷新、反向映射（rmap）清理以及页面回收操作，以减少频繁的 TLB 刷新开销和锁竞争，提升性能。\n\n当内核需要释放大量虚拟内存区域（如进程退出、mmap 区域销毁）时，不会立即释放每个页面，而是先将待释放的页面收集到 `mmu_gather` 结构中，待累积到一定数量或显式调用 flush 操作时，再统一执行 TLB 刷新、rmap 解除和页面释放。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `tlb_next_batch(struct mmu_gather *tlb)`  \n  分配新的批处理批次（batch），用于扩展可收集的页面数量上限。\n\n- `tlb_flush_rmaps(struct mmu_gather *tlb, struct vm_area_struct *vma)`  \n  （仅在 SMP 下）处理延迟的反向映射（delayed rmap）移除操作，在 TLB 刷新后调用。\n\n- `__tlb_batch_free_encoded_pages(struct mmu_gather_batch *batch)`  \n  批量释放编码后的页面（包括普通页面和 swap 缓存），支持防软锁定（soft lockup）的调度点。\n\n- `tlb_batch_pages_flush(struct mmu_gather *tlb)`  \n  遍历所有批次，释放其中收集的所有页面。\n\n- `tlb_batch_list_free(struct mmu_gather *tlb)`  \n  释放动态分配的批次内存（非本地批次）。\n\n- `__tlb_remove_folio_pages_size(...)` / `__tlb_remove_folio_pages(...)` / `__tlb_remove_page_size(...)`  \n  将页面（单页或多页 folio）加入当前 gather 批次，支持延迟 rmap 和不同页面大小。\n\n- `tlb_remove_table_sync_one(void)`  \n  （RCU 表释放模式下）触发 IPI 同步，确保软件页表遍历安全。\n\n- `tlb_remove_table_rcu(struct rcu_head *head)`  \n  RCU 回调函数，用于异步释放页表结构。\n\n- `tlb_remove_table_free(struct mmu_table_batch *batch)`  \n  将页表批次提交给 RCU 机制进行延迟释放。\n\n### 关键数据结构\n\n- `struct mmu_gather`  \n  核心上下文结构，包含本地批次（`local`）、当前活跃批次（`active`）、批次计数、延迟 rmap 标志等。\n\n- `struct mmu_gather_batch`  \n  页面批次结构，包含指向编码页面指针数组、当前数量（`nr`）、最大容量（`max`）及下一个批次指针。\n\n- `struct mmu_table_batch`  \n  页表结构批次，用于批量收集待释放的页表（如 PMD、PUD 等）。\n\n- `encoded_page` 相关机制  \n  使用指针低位编码额外信息（如是否延迟 rmap、是否后跟 nr_pages 字段），节省内存并提高缓存效率。\n\n## 3. 关键实现\n\n### 批处理与动态扩展\n- 默认使用栈上或局部存储的 `local` 批次（避免内存分配）。\n- 当 `local` 批次满时，通过 `__get_free_page()` 动态分配新批次（最多 `MAX_GATHER_BATCH_COUNT` 个）。\n- `tlb_next_batch()` 在存在延迟 rmap 时限制扩展，确保语义正确性。\n\n### 延迟反向映射（Delayed Rmap）\n- 当页面仍被其他 VMA 引用但当前 VMA 正在 unmap 时，不立即调用 `folio_remove_rmap_ptes()`，而是标记 `ENCODED_PAGE_BIT_DELAY_RMAP`。\n- 在 `tlb_flush_rmaps()` 中统一处理，确保在 TLB 刷新**之后**才解除 rmap，防止 CPU 访问已释放页面。\n\n### 安全释放与防软锁定\n- 页面释放循环中每处理最多 `MAX_NR_FOLIOS_PER_FREE`（512）个 folio 调用 `cond_resched()`，避免在非抢占内核中长时间占用 CPU。\n- 若启用 `page_poisoning` 或 `init_on_free`，则按实际内存大小（而非 folio 数量）限制单次释放量，因初始化开销与内存大小成正比。\n\n### 页表结构的安全释放（RCU 模式）\n- 在支持软件页表遍历（如 `gup_fast`）的架构上，页表释放需与遍历操作同步。\n- 使用 `call_rcu()` 延迟释放页表，配合 `smp_call_function()` 触发 IPI 确保所有 CPU 完成 TLB 刷新后再释放内存。\n- 若 RCU 批次分配失败，则回退到即时释放（代码未完整展示，但注释提及）。\n\n### 编码页面指针\n- 利用页面指针对齐特性（通常低 2~3 位为 0），将标志位（如 `DELAY_RMAP`、`NR_PAGES_NEXT`）存储在指针低位。\n- 支持多页 folio：若 `nr_pages > 1`，则连续两个条目分别存储页面指针（带标志）和页数。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm_types.h>`、`<linux/mm_inline.h>`、`<linux/rmap.h>` 等，与 folio、page、VMA 管理紧密集成。\n- **TLB 管理**：通过 `<asm/tlb.h>` 与架构相关 TLB 刷新接口交互。\n- **RCU 机制**：在 `CONFIG_MMU_GATHER_RCU_TABLE_FREE` 下依赖 `<linux/rcupdate.h>` 实现页表安全释放。\n- **SMP 支持**：`tlb_flush_rmaps` 和页表同步仅在 `CONFIG_SMP` 下编译。\n- **高阶内存与交换**：使用 `<linux/highmem.h>`、`<linux/swap.h>` 处理高端内存和 swap 缓存释放。\n- **内存分配器**：通过 `__get_free_page(GFP_NOWAIT)` 动态分配批次内存。\n\n## 5. 使用场景\n\n- **进程退出（exit_mmap）**：释放整个地址空间时，大量页面通过 mmu_gather 批量回收。\n- **munmap 系统调用**：解除大块内存映射时，避免逐页 TLB 刷新。\n- **内存回收（reclaim）**：在直接回收或 kswapd 中撤销映射时使用。\n- **透明大页（THP）拆分**：拆分大页时需撤销多个 PTE 映射并释放 sub-page。\n- **页表收缩（shrink_page_list）**：在页面回收路径中解除映射。\n- **KSM（Kernel Samepage Merging）**：合并或取消合并页面时更新 rmap。\n- **页表层级释放**：当上层页表（如 PGD/P4D/PUD/PMD）不再被引用时，通过 `tlb_remove_table` 机制安全释放。",
      "similarity": 0.5623573660850525,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "mm/mmu_gather.c",
          "start_line": 292,
          "end_line": 424,
          "content": [
            "static void tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\t__tlb_remove_table_free(batch);",
            "}",
            "static inline void tlb_table_invalidate(struct mmu_gather *tlb)",
            "{",
            "\tif (tlb_needs_table_invalidate()) {",
            "\t\t/*",
            "\t\t * Invalidate page-table caches used by hardware walkers. Then",
            "\t\t * we still need to RCU-sched wait while freeing the pages",
            "\t\t * because software walkers can still be in-flight.",
            "\t\t */",
            "\t\ttlb_flush_mmu_tlbonly(tlb);",
            "\t}",
            "}",
            "static void tlb_remove_table_one(void *table)",
            "{",
            "\ttlb_remove_table_sync_one();",
            "\t__tlb_remove_table(table);",
            "}",
            "static void tlb_table_flush(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_table_batch **batch = &tlb->batch;",
            "",
            "\tif (*batch) {",
            "\t\ttlb_table_invalidate(tlb);",
            "\t\ttlb_remove_table_free(*batch);",
            "\t\t*batch = NULL;",
            "\t}",
            "}",
            "void tlb_remove_table(struct mmu_gather *tlb, void *table)",
            "{",
            "\tstruct mmu_table_batch **batch = &tlb->batch;",
            "",
            "\tif (*batch == NULL) {",
            "\t\t*batch = (struct mmu_table_batch *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);",
            "\t\tif (*batch == NULL) {",
            "\t\t\ttlb_table_invalidate(tlb);",
            "\t\t\ttlb_remove_table_one(table);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\t(*batch)->nr = 0;",
            "\t}",
            "",
            "\t(*batch)->tables[(*batch)->nr++] = table;",
            "\tif ((*batch)->nr == MAX_TABLE_BATCH)",
            "\t\ttlb_table_flush(tlb);",
            "}",
            "static inline void tlb_table_init(struct mmu_gather *tlb)",
            "{",
            "\ttlb->batch = NULL;",
            "}",
            "static inline void tlb_table_flush(struct mmu_gather *tlb) { }",
            "static inline void tlb_table_init(struct mmu_gather *tlb) { }",
            "static void tlb_flush_mmu_free(struct mmu_gather *tlb)",
            "{",
            "\ttlb_table_flush(tlb);",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb_batch_pages_flush(tlb);",
            "#endif",
            "}",
            "void tlb_flush_mmu(struct mmu_gather *tlb)",
            "{",
            "\ttlb_flush_mmu_tlbonly(tlb);",
            "\ttlb_flush_mmu_free(tlb);",
            "}",
            "static void __tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,",
            "\t\t\t     bool fullmm)",
            "{",
            "\ttlb->mm = mm;",
            "\ttlb->fullmm = fullmm;",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb->need_flush_all = 0;",
            "\ttlb->local.next = NULL;",
            "\ttlb->local.nr   = 0;",
            "\ttlb->local.max  = ARRAY_SIZE(tlb->__pages);",
            "\ttlb->active     = &tlb->local;",
            "\ttlb->batch_count = 0;",
            "#endif",
            "\ttlb->delayed_rmap = 0;",
            "",
            "\ttlb_table_init(tlb);",
            "#ifdef CONFIG_MMU_GATHER_PAGE_SIZE",
            "\ttlb->page_size = 0;",
            "#endif",
            "",
            "\t__tlb_reset_range(tlb);",
            "\tinc_tlb_flush_pending(tlb->mm);",
            "}",
            "void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm)",
            "{",
            "\t__tlb_gather_mmu(tlb, mm, false);",
            "}",
            "void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm)",
            "{",
            "\t__tlb_gather_mmu(tlb, mm, true);",
            "}",
            "void tlb_finish_mmu(struct mmu_gather *tlb)",
            "{",
            "\t/*",
            "\t * If there are parallel threads are doing PTE changes on same range",
            "\t * under non-exclusive lock (e.g., mmap_lock read-side) but defer TLB",
            "\t * flush by batching, one thread may end up seeing inconsistent PTEs",
            "\t * and result in having stale TLB entries.  So flush TLB forcefully",
            "\t * if we detect parallel PTE batching threads.",
            "\t *",
            "\t * However, some syscalls, e.g. munmap(), may free page tables, this",
            "\t * needs force flush everything in the given range. Otherwise this",
            "\t * may result in having stale TLB entries for some architectures,",
            "\t * e.g. aarch64, that could specify flush what level TLB.",
            "\t */",
            "\tif (mm_tlb_flush_nested(tlb->mm)) {",
            "\t\t/*",
            "\t\t * The aarch64 yields better performance with fullmm by",
            "\t\t * avoiding multiple CPUs spamming TLBI messages at the",
            "\t\t * same time.",
            "\t\t *",
            "\t\t * On x86 non-fullmm doesn't yield significant difference",
            "\t\t * against fullmm.",
            "\t\t */",
            "\t\ttlb->fullmm = 1;",
            "\t\t__tlb_reset_range(tlb);",
            "\t\ttlb->freed_tables = 1;",
            "\t}",
            "",
            "\ttlb_flush_mmu(tlb);",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb_batch_list_free(tlb);",
            "#endif",
            "\tdec_tlb_flush_pending(tlb->mm);",
            "}"
          ],
          "function_name": "tlb_remove_table_free, tlb_table_invalidate, tlb_remove_table_one, tlb_table_flush, tlb_remove_table, tlb_table_init, tlb_table_flush, tlb_table_init, tlb_flush_mmu_free, tlb_flush_mmu, __tlb_gather_mmu, tlb_gather_mmu, tlb_gather_mmu_fullmm, tlb_finish_mmu",
          "description": "提供 TLB 无效化、页表批量释放及 MMU 收集器初始化/终止接口，包含跨架构的 TLB 同步机制",
          "similarity": 0.5927436351776123
        },
        {
          "chunk_id": 0,
          "file_path": "mm/mmu_gather.c",
          "start_line": 1,
          "end_line": 17,
          "content": [
            "#include <linux/gfp.h>",
            "#include <linux/highmem.h>",
            "#include <linux/kernel.h>",
            "#include <linux/mmdebug.h>",
            "#include <linux/mm_types.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/smp.h>",
            "#include <linux/swap.h>",
            "#include <linux/rmap.h>",
            "",
            "#include <asm/pgalloc.h>",
            "#include <asm/tlb.h>",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            ""
          ],
          "function_name": null,
          "description": "声明 MMU 聚合功能所需头文件，根据配置条件包含架构相关实现",
          "similarity": 0.5275577902793884
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mmu_gather.c",
          "start_line": 18,
          "end_line": 120,
          "content": [
            "static bool tlb_next_batch(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\t/* Limit batching if we have delayed rmaps pending */",
            "\tif (tlb->delayed_rmap && tlb->active != &tlb->local)",
            "\t\treturn false;",
            "",
            "\tbatch = tlb->active;",
            "\tif (batch->next) {",
            "\t\ttlb->active = batch->next;",
            "\t\treturn true;",
            "\t}",
            "",
            "\tif (tlb->batch_count == MAX_GATHER_BATCH_COUNT)",
            "\t\treturn false;",
            "",
            "\tbatch = (void *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);",
            "\tif (!batch)",
            "\t\treturn false;",
            "",
            "\ttlb->batch_count++;",
            "\tbatch->next = NULL;",
            "\tbatch->nr   = 0;",
            "\tbatch->max  = MAX_GATHER_BATCH;",
            "",
            "\ttlb->active->next = batch;",
            "\ttlb->active = batch;",
            "",
            "\treturn true;",
            "}",
            "static void tlb_flush_rmap_batch(struct mmu_gather_batch *batch, struct vm_area_struct *vma)",
            "{",
            "\tstruct encoded_page **pages = batch->encoded_pages;",
            "",
            "\tfor (int i = 0; i < batch->nr; i++) {",
            "\t\tstruct encoded_page *enc = pages[i];",
            "",
            "\t\tif (encoded_page_flags(enc) & ENCODED_PAGE_BIT_DELAY_RMAP) {",
            "\t\t\tstruct page *page = encoded_page_ptr(enc);",
            "\t\t\tunsigned int nr_pages = 1;",
            "",
            "\t\t\tif (unlikely(encoded_page_flags(enc) &",
            "\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\tnr_pages = encoded_nr_pages(pages[++i]);",
            "",
            "\t\t\tfolio_remove_rmap_ptes(page_folio(page), page, nr_pages,",
            "\t\t\t\t\t       vma);",
            "\t\t}",
            "\t}",
            "}",
            "void tlb_flush_rmaps(struct mmu_gather *tlb, struct vm_area_struct *vma)",
            "{",
            "\tif (!tlb->delayed_rmap)",
            "\t\treturn;",
            "",
            "\ttlb_flush_rmap_batch(&tlb->local, vma);",
            "\tif (tlb->active != &tlb->local)",
            "\t\ttlb_flush_rmap_batch(tlb->active, vma);",
            "\ttlb->delayed_rmap = 0;",
            "}",
            "static void __tlb_batch_free_encoded_pages(struct mmu_gather_batch *batch)",
            "{",
            "\tstruct encoded_page **pages = batch->encoded_pages;",
            "\tunsigned int nr, nr_pages;",
            "",
            "\twhile (batch->nr) {",
            "\t\tif (!page_poisoning_enabled_static() && !want_init_on_free()) {",
            "\t\t\tnr = min(MAX_NR_FOLIOS_PER_FREE, batch->nr);",
            "",
            "\t\t\t/*",
            "\t\t\t * Make sure we cover page + nr_pages, and don't leave",
            "\t\t\t * nr_pages behind when capping the number of entries.",
            "\t\t\t */",
            "\t\t\tif (unlikely(encoded_page_flags(pages[nr - 1]) &",
            "\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\tnr++;",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * With page poisoning and init_on_free, the time it",
            "\t\t\t * takes to free memory grows proportionally with the",
            "\t\t\t * actual memory size. Therefore, limit based on the",
            "\t\t\t * actual memory size and not the number of involved",
            "\t\t\t * folios.",
            "\t\t\t */",
            "\t\t\tfor (nr = 0, nr_pages = 0;",
            "\t\t\t     nr < batch->nr && nr_pages < MAX_NR_FOLIOS_PER_FREE;",
            "\t\t\t     nr++) {",
            "\t\t\t\tif (unlikely(encoded_page_flags(pages[nr]) &",
            "\t\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\t\tnr_pages += encoded_nr_pages(pages[++nr]);",
            "\t\t\t\telse",
            "\t\t\t\t\tnr_pages++;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tfree_pages_and_swap_cache(pages, nr);",
            "\t\tpages += nr;",
            "\t\tbatch->nr -= nr;",
            "",
            "\t\tcond_resched();",
            "\t}",
            "}"
          ],
          "function_name": "tlb_next_batch, tlb_flush_rmap_batch, tlb_flush_rmaps, __tlb_batch_free_encoded_pages",
          "description": "管理 TLB 批量操作的延迟 RMAP 处理逻辑，包括批次链表管理、编码页面释放及 RMAP 标志清除",
          "similarity": 0.4934091866016388
        },
        {
          "chunk_id": 2,
          "file_path": "mm/mmu_gather.c",
          "start_line": 144,
          "end_line": 244,
          "content": [
            "static void tlb_batch_pages_flush(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\tfor (batch = &tlb->local; batch && batch->nr; batch = batch->next)",
            "\t\t__tlb_batch_free_encoded_pages(batch);",
            "\ttlb->active = &tlb->local;",
            "}",
            "static void tlb_batch_list_free(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch, *next;",
            "",
            "\tfor (batch = tlb->local.next; batch; batch = next) {",
            "\t\tnext = batch->next;",
            "\t\tfree_pages((unsigned long)batch, 0);",
            "\t}",
            "\ttlb->local.next = NULL;",
            "}",
            "static bool __tlb_remove_folio_pages_size(struct mmu_gather *tlb,",
            "\t\tstruct page *page, unsigned int nr_pages, bool delay_rmap,",
            "\t\tint page_size)",
            "{",
            "\tint flags = delay_rmap ? ENCODED_PAGE_BIT_DELAY_RMAP : 0;",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\tVM_BUG_ON(!tlb->end);",
            "",
            "#ifdef CONFIG_MMU_GATHER_PAGE_SIZE",
            "\tVM_WARN_ON(tlb->page_size != page_size);",
            "\tVM_WARN_ON_ONCE(nr_pages != 1 && page_size != PAGE_SIZE);",
            "\tVM_WARN_ON_ONCE(page_folio(page) != page_folio(page + nr_pages - 1));",
            "#endif",
            "",
            "\tbatch = tlb->active;",
            "\t/*",
            "\t * Add the page and check if we are full. If so",
            "\t * force a flush.",
            "\t */",
            "\tif (likely(nr_pages == 1)) {",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_page(page, flags);",
            "\t} else {",
            "\t\tflags |= ENCODED_PAGE_BIT_NR_PAGES_NEXT;",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_page(page, flags);",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_nr_pages(nr_pages);",
            "\t}",
            "\t/*",
            "\t * Make sure that we can always add another \"page\" + \"nr_pages\",",
            "\t * requiring two entries instead of only a single one.",
            "\t */",
            "\tif (batch->nr >= batch->max - 1) {",
            "\t\tif (!tlb_next_batch(tlb))",
            "\t\t\treturn true;",
            "\t\tbatch = tlb->active;",
            "\t}",
            "\tVM_BUG_ON_PAGE(batch->nr > batch->max - 1, page);",
            "",
            "\treturn false;",
            "}",
            "bool __tlb_remove_folio_pages(struct mmu_gather *tlb, struct page *page,",
            "\t\tunsigned int nr_pages, bool delay_rmap)",
            "{",
            "\treturn __tlb_remove_folio_pages_size(tlb, page, nr_pages, delay_rmap,",
            "\t\t\t\t\t     PAGE_SIZE);",
            "}",
            "bool __tlb_remove_page_size(struct mmu_gather *tlb, struct page *page,",
            "\t\tbool delay_rmap, int page_size)",
            "{",
            "\treturn __tlb_remove_folio_pages_size(tlb, page, 1, delay_rmap, page_size);",
            "}",
            "static void __tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < batch->nr; i++)",
            "\t\t__tlb_remove_table(batch->tables[i]);",
            "",
            "\tfree_page((unsigned long)batch);",
            "}",
            "static void tlb_remove_table_smp_sync(void *arg)",
            "{",
            "\t/* Simply deliver the interrupt */",
            "}",
            "void tlb_remove_table_sync_one(void)",
            "{",
            "\t/*",
            "\t * This isn't an RCU grace period and hence the page-tables cannot be",
            "\t * assumed to be actually RCU-freed.",
            "\t *",
            "\t * It is however sufficient for software page-table walkers that rely on",
            "\t * IRQ disabling.",
            "\t */",
            "\tsmp_call_function(tlb_remove_table_smp_sync, NULL, 1);",
            "}",
            "static void tlb_remove_table_rcu(struct rcu_head *head)",
            "{",
            "\t__tlb_remove_table_free(container_of(head, struct mmu_table_batch, rcu));",
            "}",
            "static void tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\tcall_rcu(&batch->rcu, tlb_remove_table_rcu);",
            "}"
          ],
          "function_name": "tlb_batch_pages_flush, tlb_batch_list_free, __tlb_remove_folio_pages_size, __tlb_remove_folio_pages, __tlb_remove_page_size, __tlb_remove_table_free, tlb_remove_table_smp_sync, tlb_remove_table_sync_one, tlb_remove_table_rcu, tlb_remove_table_free",
          "description": "实现页表条目批量移除和内存表管理，包含多页面处理、NR_PAGES_NEXT 标记解析及 RCU 安全释放",
          "similarity": 0.3869282305240631
        }
      ]
    },
    {
      "source_file": "mm/hugetlb_cgroup.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:06:55\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `hugetlb_cgroup.c`\n\n---\n\n# hugetlb_cgroup.c 技术文档\n\n## 1. 文件概述\n\n`hugetlb_cgroup.c` 是 Linux 内核中用于实现 **HugeTLB（大页）内存资源控制组（cgroup）** 功能的核心文件。该文件通过 cgroup v1/v2 接口，对不同 HugeTLB 页面大小（如 2MB、1GB 等）的内存使用进行配额限制和统计追踪。它支持两种计费模式：普通分配（fault-based）和预留（reservation-based），并确保在 cgroup 被销毁时将资源正确迁移至父 cgroup，防止资源泄漏。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct hugetlb_cgroup`：每个 cgroup 实例对应的 HugeTLB 控制结构。\n  - 包含两个 `page_counter` 数组：`hugepage[]`（普通使用）和 `rsvd_hugepage[]`（预留使用），分别对应每种 HugeTLB 页面类型。\n  - 包含 per-node 信息 `nodeinfo[]`，用于 NUMA 感知。\n  - 包含事件计数器 `events[][]` 和 `events_local[][]`，用于触发 cgroup 通知。\n- `struct hugetlb_cgroup_per_node`：每个 NUMA 节点上的 HugeTLB cgroup 附加信息（当前未在代码片段中完整定义）。\n\n### 主要函数\n- `hugetlb_cgroup_css_alloc()` / `hugetlb_cgroup_css_free()`：cgroup 子系统实例的创建与销毁。\n- `hugetlb_cgroup_init()`：初始化新 cgroup 的 page_counter，设置最大限制为 `PAGE_COUNTER_MAX` 向下对齐到 HugeTLB 页面大小。\n- `__hugetlb_cgroup_charge_cgroup()` / `hugetlb_cgroup_charge_cgroup()`：对当前任务所属 cgroup 尝试 charge（计费）指定数量的 HugeTLB 页面。\n- `hugetlb_cgroup_move_parent()`：将属于某 cgroup 的 HugeTLB 页面迁移至其父 cgroup。\n- `hugetlb_cgroup_css_offline()`：在 cgroup 离线时，强制将其所有 HugeTLB 资源迁移至父级。\n- `hugetlb_event()`：向上冒泡记录 HugeTLB 事件（如达到限制 `HUGETLB_MAX`）并触发 cgroup 文件通知。\n- 辅助内联函数：\n  - `hugetlb_cgroup_from_css()` / `from_task()`：从 cgroup_subsys_state 或 task 获取 hugetlb_cgroup。\n  - `hugetlb_cgroup_counter_from_cgroup()` / `_rsvd()`：获取对应计费类型的 page_counter。\n  - `hugetlb_cgroup_is_root()` / `parent_hugetlb_cgroup()`：判断是否为根 cgroup 或获取父 cgroup。\n\n## 3. 关键实现\n\n### 资源计费机制\n- 使用 `page_counter` 子系统管理每种 HugeTLB 页面类型的用量和上限。\n- 支持两种独立的计费路径：\n  - **普通分配（fault）**：实际分配物理页面时计费。\n  - **预留（reservation）**：仅预留虚拟地址空间时计费（用于 mmap 等场景）。\n- 计费时通过 RCU 安全地获取当前任务的 cgroup，并使用 `css_tryget()` 确保引用有效性。\n- 若计费失败（超出限制），触发 `HUGETLB_MAX` 事件并通过 `cgroup_file_notify()` 通知用户空间。\n\n### cgroup 生命周期管理\n- **创建**：为每个在线 NUMA 节点分配 `hugetlb_cgroup_per_node` 结构；初始化所有 HugeTLB 类型的 page_counter，父子层级通过 `page_counter` 的 parent 字段建立级联关系。\n- **离线（offline）**：遍历所有 HugeTLB 页面的 active list，调用 `hugetlb_cgroup_move_parent()` 将页面所有权转移给父 cgroup。此过程循环执行直至当前 cgroup 无任何 HugeTLB 使用量，确保资源完全迁移。\n- **销毁**：释放 per-node 数据及主结构体。\n\n### 资源迁移（Reparenting）\n- `hugetlb_cgroup_move_parent()` 在持有 `hugetlb_lock` 时执行，确保页面不会被并发释放或迁移。\n- 仅处理属于目标 cgroup 的页面；若父 cgroup 为空（即目标为根），则 charge 到全局 root cgroup（无硬限制）。\n- 通过 `set_hugetlb_cgroup()` 更新页面所属的 cgroup。\n\n### 限制与优化\n- 对于 order 小于 `HUGETLB_CGROUP_MIN_ORDER`（通常为 3，即 8 个普通页 = 32KB）的 HugeTLB 页面，**不进行 cgroup 计费**，以减少小 HugeTLB 页面的开销。\n- 最大限制设为 `PAGE_COUNTER_MAX` 向下对齐到 HugeTLB 页面大小，避免跨页边界问题。\n- 当前 per-node 分配策略对 offline 节点也分配内存（使用 `NUMA_NO_NODE`），存在内存浪费，注释中指出未来可通过内存热插拔回调优化。\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/cgroup.h>`：cgroup 基础框架。\n  - `<linux/page_counter.h>`：提供层次化内存计数和限制功能。\n  - `<linux/hugetlb.h>`：HugeTLB 核心数据结构（如 `hstate`, `hugepage_activelist`）和锁（`hugetlb_lock`）。\n  - `<linux/hugetlb_cgroup.h>`：HugeTLB cgroup 的公共接口和数据结构定义。\n- **交互模块**：\n  - **HugeTLB 子系统**：在页面分配/释放、预留/取消预留等路径中调用本文件的 charge/uncharge 函数。\n  - **Memory cgroup (memcg)**：共享部分设计思想（如 page_counter），但 HugeTLB cgroup 是独立子系统。\n  - **Scheduler**：通过 `current` 获取当前任务的 cgroup 上下文。\n\n## 5. 使用场景\n\n- **容器资源隔离**：在 Kubernetes/Docker 等容器运行时中，通过 cgroup v1 的 `hugetlb` 子系统或 cgroup v2 的 `hugetlb.` 控制器，限制容器可使用的 HugeTLB 内存总量，防止单个容器耗尽系统大页资源。\n- **高性能计算（HPC）**：为不同 HPC 作业分配专用的 HugeTLB 内存配额，确保关键应用获得确定性内存性能。\n- **数据库优化**：Oracle、MySQL 等数据库使用 HugeTLB 提升 TLB 效率，通过 cgroup 限制其大页使用量，避免影响其他服务。\n- **资源监控与告警**：用户空间可通过读取 cgroup 的 `hugetlb.events` 文件监控 `max` 事件，实现基于 HugeTLB 使用量的自动扩缩容或告警。",
      "similarity": 0.5618244409561157,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 520,
          "end_line": 626,
          "content": [
            "static u64 hugetlb_cgroup_read_u64(struct cgroup_subsys_state *css,",
            "\t\t\t\t   struct cftype *cft)",
            "{",
            "\tstruct page_counter *counter;",
            "\tstruct page_counter *rsvd_counter;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(css);",
            "",
            "\tcounter = &h_cg->hugepage[MEMFILE_IDX(cft->private)];",
            "\trsvd_counter = &h_cg->rsvd_hugepage[MEMFILE_IDX(cft->private)];",
            "",
            "\tswitch (MEMFILE_ATTR(cft->private)) {",
            "\tcase RES_USAGE:",
            "\t\treturn (u64)page_counter_read(counter) * PAGE_SIZE;",
            "\tcase RES_RSVD_USAGE:",
            "\t\treturn (u64)page_counter_read(rsvd_counter) * PAGE_SIZE;",
            "\tcase RES_LIMIT:",
            "\t\treturn (u64)counter->max * PAGE_SIZE;",
            "\tcase RES_RSVD_LIMIT:",
            "\t\treturn (u64)rsvd_counter->max * PAGE_SIZE;",
            "\tcase RES_MAX_USAGE:",
            "\t\treturn (u64)counter->watermark * PAGE_SIZE;",
            "\tcase RES_RSVD_MAX_USAGE:",
            "\t\treturn (u64)rsvd_counter->watermark * PAGE_SIZE;",
            "\tcase RES_FAILCNT:",
            "\t\treturn counter->failcnt;",
            "\tcase RES_RSVD_FAILCNT:",
            "\t\treturn rsvd_counter->failcnt;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static int hugetlb_cgroup_read_u64_max(struct seq_file *seq, void *v)",
            "{",
            "\tint idx;",
            "\tu64 val;",
            "\tstruct cftype *cft = seq_cft(seq);",
            "\tunsigned long limit;",
            "\tstruct page_counter *counter;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(seq_css(seq));",
            "",
            "\tidx = MEMFILE_IDX(cft->private);",
            "\tcounter = &h_cg->hugepage[idx];",
            "",
            "\tlimit = round_down(PAGE_COUNTER_MAX,",
            "\t\t\t   pages_per_huge_page(&hstates[idx]));",
            "",
            "\tswitch (MEMFILE_ATTR(cft->private)) {",
            "\tcase RES_RSVD_USAGE:",
            "\t\tcounter = &h_cg->rsvd_hugepage[idx];",
            "\t\tfallthrough;",
            "\tcase RES_USAGE:",
            "\t\tval = (u64)page_counter_read(counter);",
            "\t\tseq_printf(seq, \"%llu\\n\", val * PAGE_SIZE);",
            "\t\tbreak;",
            "\tcase RES_RSVD_LIMIT:",
            "\t\tcounter = &h_cg->rsvd_hugepage[idx];",
            "\t\tfallthrough;",
            "\tcase RES_LIMIT:",
            "\t\tval = (u64)counter->max;",
            "\t\tif (val == limit)",
            "\t\t\tseq_puts(seq, \"max\\n\");",
            "\t\telse",
            "\t\t\tseq_printf(seq, \"%llu\\n\", val * PAGE_SIZE);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static ssize_t hugetlb_cgroup_write(struct kernfs_open_file *of,",
            "\t\t\t\t    char *buf, size_t nbytes, loff_t off,",
            "\t\t\t\t    const char *max)",
            "{",
            "\tint ret, idx;",
            "\tunsigned long nr_pages;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(of_css(of));",
            "\tbool rsvd = false;",
            "",
            "\tif (hugetlb_cgroup_is_root(h_cg)) /* Can't set limit on root */",
            "\t\treturn -EINVAL;",
            "",
            "\tbuf = strstrip(buf);",
            "\tret = page_counter_memparse(buf, max, &nr_pages);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tidx = MEMFILE_IDX(of_cft(of)->private);",
            "\tnr_pages = round_down(nr_pages, pages_per_huge_page(&hstates[idx]));",
            "",
            "\tswitch (MEMFILE_ATTR(of_cft(of)->private)) {",
            "\tcase RES_RSVD_LIMIT:",
            "\t\trsvd = true;",
            "\t\tfallthrough;",
            "\tcase RES_LIMIT:",
            "\t\tmutex_lock(&hugetlb_limit_mutex);",
            "\t\tret = page_counter_set_max(",
            "\t\t\t__hugetlb_cgroup_counter_from_cgroup(h_cg, idx, rsvd),",
            "\t\t\tnr_pages);",
            "\t\tmutex_unlock(&hugetlb_limit_mutex);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tret = -EINVAL;",
            "\t\tbreak;",
            "\t}",
            "\treturn ret ?: nbytes;",
            "}"
          ],
          "function_name": "hugetlb_cgroup_read_u64, hugetlb_cgroup_read_u64_max, hugetlb_cgroup_write",
          "description": "提供HugeTLB cgroup的监控接口，实现读取当前使用量、限制等参数的功能，并支持通过接口设置内存限制参数。",
          "similarity": 0.5917946696281433
        },
        {
          "chunk_id": 2,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 242,
          "end_line": 368,
          "content": [
            "static inline void hugetlb_event(struct hugetlb_cgroup *hugetlb, int idx,",
            "\t\t\t\t enum hugetlb_memory_event event)",
            "{",
            "\tatomic_long_inc(&hugetlb->events_local[idx][event]);",
            "\tcgroup_file_notify(&hugetlb->events_local_file[idx]);",
            "",
            "\tdo {",
            "\t\tatomic_long_inc(&hugetlb->events[idx][event]);",
            "\t\tcgroup_file_notify(&hugetlb->events_file[idx]);",
            "\t} while ((hugetlb = parent_hugetlb_cgroup(hugetlb)) &&",
            "\t\t !hugetlb_cgroup_is_root(hugetlb));",
            "}",
            "static int __hugetlb_cgroup_charge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t  struct hugetlb_cgroup **ptr,",
            "\t\t\t\t\t  bool rsvd)",
            "{",
            "\tint ret = 0;",
            "\tstruct page_counter *counter;",
            "\tstruct hugetlb_cgroup *h_cg = NULL;",
            "",
            "\tif (hugetlb_cgroup_disabled())",
            "\t\tgoto done;",
            "\t/*",
            "\t * We don't charge any cgroup if the compound page have less",
            "\t * than 3 pages.",
            "\t */",
            "\tif (huge_page_order(&hstates[idx]) < HUGETLB_CGROUP_MIN_ORDER)",
            "\t\tgoto done;",
            "again:",
            "\trcu_read_lock();",
            "\th_cg = hugetlb_cgroup_from_task(current);",
            "\tif (!css_tryget(&h_cg->css)) {",
            "\t\trcu_read_unlock();",
            "\t\tgoto again;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tif (!page_counter_try_charge(",
            "\t\t    __hugetlb_cgroup_counter_from_cgroup(h_cg, idx, rsvd),",
            "\t\t    nr_pages, &counter)) {",
            "\t\tret = -ENOMEM;",
            "\t\thugetlb_event(h_cg, idx, HUGETLB_MAX);",
            "\t\tcss_put(&h_cg->css);",
            "\t\tgoto done;",
            "\t}",
            "\t/* Reservations take a reference to the css because they do not get",
            "\t * reparented.",
            "\t */",
            "\tif (!rsvd)",
            "\t\tcss_put(&h_cg->css);",
            "done:",
            "\t*ptr = h_cg;",
            "\treturn ret;",
            "}",
            "int hugetlb_cgroup_charge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t struct hugetlb_cgroup **ptr)",
            "{",
            "\treturn __hugetlb_cgroup_charge_cgroup(idx, nr_pages, ptr, false);",
            "}",
            "int hugetlb_cgroup_charge_cgroup_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t      struct hugetlb_cgroup **ptr)",
            "{",
            "\treturn __hugetlb_cgroup_charge_cgroup(idx, nr_pages, ptr, true);",
            "}",
            "static void __hugetlb_cgroup_commit_charge(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t   struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t\t   struct folio *folio, bool rsvd)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !h_cg)",
            "\t\treturn;",
            "",
            "\t__set_hugetlb_cgroup(folio, h_cg, rsvd);",
            "\tif (!rsvd) {",
            "\t\tunsigned long usage =",
            "\t\t\th_cg->nodeinfo[folio_nid(folio)]->usage[idx];",
            "\t\t/*",
            "\t\t * This write is not atomic due to fetching usage and writing",
            "\t\t * to it, but that's fine because we call this with",
            "\t\t * hugetlb_lock held anyway.",
            "\t\t */",
            "\t\tWRITE_ONCE(h_cg->nodeinfo[folio_nid(folio)]->usage[idx],",
            "\t\t\t   usage + nr_pages);",
            "\t}",
            "}",
            "void hugetlb_cgroup_commit_charge(int idx, unsigned long nr_pages,",
            "\t\t\t\t  struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t  struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_commit_charge(idx, nr_pages, h_cg, folio, false);",
            "}",
            "void hugetlb_cgroup_commit_charge_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t       struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t       struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_commit_charge(idx, nr_pages, h_cg, folio, true);",
            "}",
            "static void __hugetlb_cgroup_uncharge_folio(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t   struct folio *folio, bool rsvd)",
            "{",
            "\tstruct hugetlb_cgroup *h_cg;",
            "",
            "\tif (hugetlb_cgroup_disabled())",
            "\t\treturn;",
            "\tlockdep_assert_held(&hugetlb_lock);",
            "\th_cg = __hugetlb_cgroup_from_folio(folio, rsvd);",
            "\tif (unlikely(!h_cg))",
            "\t\treturn;",
            "\t__set_hugetlb_cgroup(folio, NULL, rsvd);",
            "",
            "\tpage_counter_uncharge(__hugetlb_cgroup_counter_from_cgroup(h_cg, idx,",
            "\t\t\t\t\t\t\t\t   rsvd),",
            "\t\t\t      nr_pages);",
            "",
            "\tif (rsvd)",
            "\t\tcss_put(&h_cg->css);",
            "\telse {",
            "\t\tunsigned long usage =",
            "\t\t\th_cg->nodeinfo[folio_nid(folio)]->usage[idx];",
            "\t\t/*",
            "\t\t * This write is not atomic due to fetching usage and writing",
            "\t\t * to it, but that's fine because we call this with",
            "\t\t * hugetlb_lock held anyway.",
            "\t\t */",
            "\t\tWRITE_ONCE(h_cg->nodeinfo[folio_nid(folio)]->usage[idx],",
            "\t\t\t   usage - nr_pages);",
            "\t}",
            "}"
          ],
          "function_name": "hugetlb_event, __hugetlb_cgroup_charge_cgroup, hugetlb_cgroup_charge_cgroup, hugetlb_cgroup_charge_cgroup_rsvd, __hugetlb_cgroup_commit_charge, hugetlb_cgroup_commit_charge, hugetlb_cgroup_commit_charge_rsvd, __hugetlb_cgroup_uncharge_folio",
          "description": "处理HugeTLB页面分配时的计费操作，包含事件记录、尝试充电、提交充电等流程，区分普通页与保留页的计数逻辑，并维护各层级cgroup的使用统计。",
          "similarity": 0.5250176191329956
        },
        {
          "chunk_id": 3,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 381,
          "end_line": 500,
          "content": [
            "void hugetlb_cgroup_uncharge_folio(int idx, unsigned long nr_pages,",
            "\t\t\t\t  struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_uncharge_folio(idx, nr_pages, folio, false);",
            "}",
            "void hugetlb_cgroup_uncharge_folio_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t       struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_uncharge_folio(idx, nr_pages, folio, true);",
            "}",
            "static void __hugetlb_cgroup_uncharge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t     struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t\t     bool rsvd)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !h_cg)",
            "\t\treturn;",
            "",
            "\tif (huge_page_order(&hstates[idx]) < HUGETLB_CGROUP_MIN_ORDER)",
            "\t\treturn;",
            "",
            "\tpage_counter_uncharge(__hugetlb_cgroup_counter_from_cgroup(h_cg, idx,",
            "\t\t\t\t\t\t\t\t   rsvd),",
            "\t\t\t      nr_pages);",
            "",
            "\tif (rsvd)",
            "\t\tcss_put(&h_cg->css);",
            "}",
            "void hugetlb_cgroup_uncharge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t    struct hugetlb_cgroup *h_cg)",
            "{",
            "\t__hugetlb_cgroup_uncharge_cgroup(idx, nr_pages, h_cg, false);",
            "}",
            "void hugetlb_cgroup_uncharge_cgroup_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t struct hugetlb_cgroup *h_cg)",
            "{",
            "\t__hugetlb_cgroup_uncharge_cgroup(idx, nr_pages, h_cg, true);",
            "}",
            "void hugetlb_cgroup_uncharge_counter(struct resv_map *resv, unsigned long start,",
            "\t\t\t\t     unsigned long end)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !resv || !resv->reservation_counter ||",
            "\t    !resv->css)",
            "\t\treturn;",
            "",
            "\tpage_counter_uncharge(resv->reservation_counter,",
            "\t\t\t      (end - start) * resv->pages_per_hpage);",
            "\tcss_put(resv->css);",
            "}",
            "void hugetlb_cgroup_uncharge_file_region(struct resv_map *resv,",
            "\t\t\t\t\t struct file_region *rg,",
            "\t\t\t\t\t unsigned long nr_pages,",
            "\t\t\t\t\t bool region_del)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !resv || !rg || !nr_pages)",
            "\t\treturn;",
            "",
            "\tif (rg->reservation_counter && resv->pages_per_hpage &&",
            "\t    !resv->reservation_counter) {",
            "\t\tpage_counter_uncharge(rg->reservation_counter,",
            "\t\t\t\t      nr_pages * resv->pages_per_hpage);",
            "\t\t/*",
            "\t\t * Only do css_put(rg->css) when we delete the entire region",
            "\t\t * because one file_region must hold exactly one css reference.",
            "\t\t */",
            "\t\tif (region_del)",
            "\t\t\tcss_put(rg->css);",
            "\t}",
            "}",
            "static int hugetlb_cgroup_read_numa_stat(struct seq_file *seq, void *dummy)",
            "{",
            "\tint nid;",
            "\tstruct cftype *cft = seq_cft(seq);",
            "\tint idx = MEMFILE_IDX(cft->private);",
            "\tbool legacy = MEMFILE_ATTR(cft->private);",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(seq_css(seq));",
            "\tstruct cgroup_subsys_state *css;",
            "\tunsigned long usage;",
            "",
            "\tif (legacy) {",
            "\t\t/* Add up usage across all nodes for the non-hierarchical total. */",
            "\t\tusage = 0;",
            "\t\tfor_each_node_state(nid, N_MEMORY)",
            "\t\t\tusage += READ_ONCE(h_cg->nodeinfo[nid]->usage[idx]);",
            "\t\tseq_printf(seq, \"total=%lu\", usage * PAGE_SIZE);",
            "",
            "\t\t/* Simply print the per-node usage for the non-hierarchical total. */",
            "\t\tfor_each_node_state(nid, N_MEMORY)",
            "\t\t\tseq_printf(seq, \" N%d=%lu\", nid,",
            "\t\t\t\t   READ_ONCE(h_cg->nodeinfo[nid]->usage[idx]) *",
            "\t\t\t\t\t   PAGE_SIZE);",
            "\t\tseq_putc(seq, '\\n');",
            "\t}",
            "",
            "\t/*",
            "\t * The hierarchical total is pretty much the value recorded by the",
            "\t * counter, so use that.",
            "\t */",
            "\tseq_printf(seq, \"%stotal=%lu\", legacy ? \"hierarchical_\" : \"\",",
            "\t\t   page_counter_read(&h_cg->hugepage[idx]) * PAGE_SIZE);",
            "",
            "\t/*",
            "\t * For each node, transverse the css tree to obtain the hierarchical",
            "\t * node usage.",
            "\t */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\tusage = 0;",
            "\t\trcu_read_lock();",
            "\t\tcss_for_each_descendant_pre(css, &h_cg->css) {",
            "\t\t\tusage += READ_ONCE(hugetlb_cgroup_from_css(css)",
            "\t\t\t\t\t\t   ->nodeinfo[nid]",
            "\t\t\t\t\t\t   ->usage[idx]);",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t\tseq_printf(seq, \" N%d=%lu\", nid, usage * PAGE_SIZE);",
            "\t}",
            "",
            "\tseq_putc(seq, '\\n');",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "hugetlb_cgroup_uncharge_folio, hugetlb_cgroup_uncharge_folio_rsvd, __hugetlb_cgroup_uncharge_cgroup, hugetlb_cgroup_uncharge_cgroup, hugetlb_cgroup_uncharge_cgroup_rsvd, hugetlb_cgroup_uncharge_counter, hugetlb_cgroup_uncharge_file_region, hugetlb_cgroup_read_numa_stat",
          "description": "实现HugeTLB页面释放时的反向计费操作，包含对单个folio和整体cgroup的解除分配逻辑，以及读取NUMA节点统计信息的实现。",
          "similarity": 0.5052833557128906
        },
        {
          "chunk_id": 6,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 785,
          "end_line": 901,
          "content": [
            "static void __init __hugetlb_cgroup_file_legacy_init(int idx)",
            "{",
            "\tchar buf[32];",
            "\tstruct cftype *cft;",
            "\tstruct hstate *h = &hstates[idx];",
            "",
            "\t/* format the size */",
            "\tmem_fmt(buf, sizeof(buf), huge_page_size(h));",
            "",
            "\t/* Add the limit file */",
            "\tcft = &h->cgroup_files_legacy[0];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.limit_in_bytes\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_LIMIT);",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "\tcft->write = hugetlb_cgroup_write_legacy;",
            "",
            "\t/* Add the reservation limit file */",
            "\tcft = &h->cgroup_files_legacy[1];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.rsvd.limit_in_bytes\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_RSVD_LIMIT);",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "\tcft->write = hugetlb_cgroup_write_legacy;",
            "",
            "\t/* Add the usage file */",
            "\tcft = &h->cgroup_files_legacy[2];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.usage_in_bytes\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_USAGE);",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "",
            "\t/* Add the reservation usage file */",
            "\tcft = &h->cgroup_files_legacy[3];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.rsvd.usage_in_bytes\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_RSVD_USAGE);",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "",
            "\t/* Add the MAX usage file */",
            "\tcft = &h->cgroup_files_legacy[4];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.max_usage_in_bytes\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_MAX_USAGE);",
            "\tcft->write = hugetlb_cgroup_reset;",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "",
            "\t/* Add the MAX reservation usage file */",
            "\tcft = &h->cgroup_files_legacy[5];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.rsvd.max_usage_in_bytes\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_RSVD_MAX_USAGE);",
            "\tcft->write = hugetlb_cgroup_reset;",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "",
            "\t/* Add the failcntfile */",
            "\tcft = &h->cgroup_files_legacy[6];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.failcnt\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_FAILCNT);",
            "\tcft->write = hugetlb_cgroup_reset;",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "",
            "\t/* Add the reservation failcntfile */",
            "\tcft = &h->cgroup_files_legacy[7];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.rsvd.failcnt\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_RSVD_FAILCNT);",
            "\tcft->write = hugetlb_cgroup_reset;",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "",
            "\t/* Add the numa stat file */",
            "\tcft = &h->cgroup_files_legacy[8];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.numa_stat\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, 1);",
            "\tcft->seq_show = hugetlb_cgroup_read_numa_stat;",
            "",
            "\t/* NULL terminate the last cft */",
            "\tcft = &h->cgroup_files_legacy[9];",
            "\tmemset(cft, 0, sizeof(*cft));",
            "",
            "\tWARN_ON(cgroup_add_legacy_cftypes(&hugetlb_cgrp_subsys,",
            "\t\t\t\t\t  h->cgroup_files_legacy));",
            "}",
            "static void __init __hugetlb_cgroup_file_init(int idx)",
            "{",
            "\t__hugetlb_cgroup_file_dfl_init(idx);",
            "\t__hugetlb_cgroup_file_legacy_init(idx);",
            "}",
            "void __init hugetlb_cgroup_file_init(void)",
            "{",
            "\tstruct hstate *h;",
            "",
            "\tfor_each_hstate(h) {",
            "\t\t/*",
            "\t\t * Add cgroup control files only if the huge page consists",
            "\t\t * of more than two normal pages. This is because we use",
            "\t\t * page[2].private for storing cgroup details.",
            "\t\t */",
            "\t\tif (huge_page_order(h) >= HUGETLB_CGROUP_MIN_ORDER)",
            "\t\t\t__hugetlb_cgroup_file_init(hstate_index(h));",
            "\t}",
            "}",
            "void hugetlb_cgroup_migrate(struct folio *old_folio, struct folio *new_folio)",
            "{",
            "\tstruct hugetlb_cgroup *h_cg;",
            "\tstruct hugetlb_cgroup *h_cg_rsvd;",
            "\tstruct hstate *h = folio_hstate(old_folio);",
            "",
            "\tif (hugetlb_cgroup_disabled())",
            "\t\treturn;",
            "",
            "\tspin_lock_irq(&hugetlb_lock);",
            "\th_cg = hugetlb_cgroup_from_folio(old_folio);",
            "\th_cg_rsvd = hugetlb_cgroup_from_folio_rsvd(old_folio);",
            "\tset_hugetlb_cgroup(old_folio, NULL);",
            "\tset_hugetlb_cgroup_rsvd(old_folio, NULL);",
            "",
            "\t/* move the h_cg details to new cgroup */",
            "\tset_hugetlb_cgroup(new_folio, h_cg);",
            "\tset_hugetlb_cgroup_rsvd(new_folio, h_cg_rsvd);",
            "\tlist_move(&new_folio->lru, &h->hugepage_activelist);",
            "\tspin_unlock_irq(&hugetlb_lock);",
            "\treturn;",
            "}"
          ],
          "function_name": "__hugetlb_cgroup_file_legacy_init, __hugetlb_cgroup_file_init, hugetlb_cgroup_file_init, hugetlb_cgroup_migrate",
          "description": "初始化传统模式下的hugetlb cgroup文件系统接口，包含限额、使用量、最大使用量等监控文件，并实现页面迁移时的cgroup上下文切换逻辑，通过hugetlb_cgroup_migrate维护huge页面与cgroup的关联关系",
          "similarity": 0.49509483575820923
        },
        {
          "chunk_id": 0,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 1,
          "end_line": 64,
          "content": [
            "/*",
            " *",
            " * Copyright IBM Corporation, 2012",
            " * Author Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>",
            " *",
            " * Cgroup v2",
            " * Copyright (C) 2019 Red Hat, Inc.",
            " * Author: Giuseppe Scrivano <gscrivan@redhat.com>",
            " *",
            " * This program is free software; you can redistribute it and/or modify it",
            " * under the terms of version 2.1 of the GNU Lesser General Public License",
            " * as published by the Free Software Foundation.",
            " *",
            " * This program is distributed in the hope that it would be useful, but",
            " * WITHOUT ANY WARRANTY; without even the implied warranty of",
            " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.",
            " *",
            " */",
            "",
            "#include <linux/cgroup.h>",
            "#include <linux/page_counter.h>",
            "#include <linux/slab.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/hugetlb_cgroup.h>",
            "",
            "#define MEMFILE_PRIVATE(x, val)\t(((x) << 16) | (val))",
            "#define MEMFILE_IDX(val)\t(((val) >> 16) & 0xffff)",
            "#define MEMFILE_ATTR(val)\t((val) & 0xffff)",
            "",
            "static struct hugetlb_cgroup *root_h_cgroup __read_mostly;",
            "",
            "static inline struct page_counter *",
            "__hugetlb_cgroup_counter_from_cgroup(struct hugetlb_cgroup *h_cg, int idx,",
            "\t\t\t\t     bool rsvd)",
            "{",
            "\tif (rsvd)",
            "\t\treturn &h_cg->rsvd_hugepage[idx];",
            "\treturn &h_cg->hugepage[idx];",
            "}",
            "",
            "static inline struct page_counter *",
            "hugetlb_cgroup_counter_from_cgroup(struct hugetlb_cgroup *h_cg, int idx)",
            "{",
            "\treturn __hugetlb_cgroup_counter_from_cgroup(h_cg, idx, false);",
            "}",
            "",
            "static inline struct page_counter *",
            "hugetlb_cgroup_counter_from_cgroup_rsvd(struct hugetlb_cgroup *h_cg, int idx)",
            "{",
            "\treturn __hugetlb_cgroup_counter_from_cgroup(h_cg, idx, true);",
            "}",
            "",
            "static inline",
            "struct hugetlb_cgroup *hugetlb_cgroup_from_css(struct cgroup_subsys_state *s)",
            "{",
            "\treturn s ? container_of(s, struct hugetlb_cgroup, css) : NULL;",
            "}",
            "",
            "static inline",
            "struct hugetlb_cgroup *hugetlb_cgroup_from_task(struct task_struct *task)",
            "{",
            "\treturn hugetlb_cgroup_from_css(task_css(task, hugetlb_cgrp_id));",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义HugeTLB cgroup子系统的辅助宏和基础结构，声明全局根cgroup指针及用于获取对应cgroup的内联函数，提供page_counter相关操作的封装接口。",
          "similarity": 0.4897541403770447
        }
      ]
    },
    {
      "source_file": "mm/hugetlb.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:06:09\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `hugetlb.c`\n\n---\n\n# hugetlb.c 技术文档\n\n## 1. 文件概述\n\n`hugetlb.c` 是 Linux 内核中实现通用大页（HugeTLB）内存管理的核心文件。该文件提供了对 HugeTLB 页面池的初始化、分配、释放、预留（reservation）、子池（subpool）管理以及与虚拟内存区域（VMA）相关的同步机制等关键功能。它支持多种 HugeTLB 页面大小，并通过灵活的配额和预留策略，满足不同应用场景对大页内存的需求，同时确保系统稳定性。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `hugetlb_max_hstate`: 系统中已注册的大页状态（hstate）数量。\n- `default_hstate_idx`: 默认使用的大页状态索引。\n- `hstates[HUGE_MAX_HSTATE]`: 存储所有已配置的大页状态（如页面大小、页面池信息等）的数组。\n- `huge_boot_pages`: 链表，用于在启动阶段收集通过 `memblock` 分配的大页。\n- `hugetlb_lock`: 自旋锁，保护大页池的关键数据结构（如空闲/活跃列表、页面计数等）。\n- `hugetlb_fault_mutex_table`: 故障互斥锁表，用于序列化同一逻辑大页上的缺页异常，防止因竞争导致的虚假 OOM。\n\n### 主要数据结构\n- `struct hstate`: 描述一种特定大小的大页配置，包括页面大小、节点分布、页面池统计等。\n- `struct hugepage_subpool`: 大页子池，用于实现基于挂载点或 inode 的配额控制（最大/最小页面数限制）。\n- `struct resv_map`: 预留映射，用于跟踪 VMA 中哪些大页已被预留但尚未分配。\n\n### 主要函数\n- **子池管理**:\n  - `hugepage_new_subpool()`: 创建新的大页子池。\n  - `hugepage_put_subpool()`: 释放子池引用，若无引用则销毁子池。\n  - `hugepage_subpool_get_pages()`: 从子池中获取页面，处理最大/最小配额逻辑。\n  - `hugepage_subpool_put_pages()`: 向子池归还页面，处理配额恢复逻辑。\n- **VMA 锁机制**:\n  - `hugetlb_vma_lock_read()/unlock_read()`: 对 VMA 进行读锁定。\n  - `hugetlb_vma_lock_write()/unlock_write()`: 对 VMA 进行写锁定。\n- **内存释放**:\n  - `hugetlb_free_folio()`: 释放大页 folio，优先尝试通过 CMA 释放。\n- **辅助函数**:\n  - `subpool_is_free()`: 判断子池是否可被安全释放。\n  - `unlock_or_release_subpool()`: 解锁并根据条件释放子池。\n\n## 3. 关键实现\n\n### 大页子池（Subpool）配额机制\n子池机制允许为不同的 hugetlbfs 挂载点设置独立的页面配额：\n- **最大配额 (`max_hpages`)**: 限制子池可使用的最大页面数。\n- **最小配额 (`min_hpages`)**: 启动时预占资源，确保最低可用页面数。\n- 获取页面时 (`hugepage_subpool_get_pages`)，先检查最大配额，再处理最小配额的预留抵扣。\n- 归还页面时 (`hugepage_subpool_put_pages`)，若使用量低于最小配额，则恢复预留计数。\n- 当子池引用计数归零且无活跃页面时，自动释放其最小配额并销毁子池。\n\n### VMA 同步锁设计\n为避免多个进程同时处理同一逻辑大页的缺页异常导致资源竞争或 OOM，内核采用两种锁策略：\n- **共享锁**: 多个 VMA 映射同一文件区域时，共享一个 `hugetlb_vma_lock` 结构。\n- **私有锁**: 私有映射使用 `resv_map` 中的读写信号量。\n- 通过 `__vma_shareable_lock()` 和 `__vma_private_lock()` 宏判断 VMA 类型，动态选择锁对象。\n\n### CMA 集成\n当启用 `CONFIG_CMA` 时，大页可从 CMA（Contiguous Memory Allocator）区域分配：\n- 每个 NUMA 节点维护独立的 `hugetlb_cma` 区域。\n- 释放大页时优先调用 `cma_free_folio()`，失败后才走通用路径 `folio_put()`。\n\n### 启动阶段大页分配\n通过 `huge_boot_pages` 链表在内核早期启动阶段收集大页，后续在 `hugetlb_init()` 中将其整合到各 `hstate` 的空闲列表中，确保大页池初始化完成前即可分配页面。\n\n## 4. 依赖关系\n\n- **内存管理子系统**: 依赖 `<linux/mm.h>`, `<linux/page-flags.h>`, `<linux/gfp.h>` 等基础内存管理接口。\n- **NUMA 支持**: 通过 `<linux/numa.h>`, `<linux/nodemask.h>` 实现节点感知的大页分配。\n- **hugetlbfs 文件系统**: 与 `fs/hugetlbfs/` 模块紧密耦合，通过 `HUGETLBFS_SB()` 获取子池信息。\n- **CMA 子系统**: 条件编译依赖 `<linux/cma.h>`，用于连续物理内存分配。\n- **内存控制组 (cgroup)**: 通过 `<linux/hugetlb_cgroup.h>` 集成资源限制。\n- **体系结构相关代码**: 依赖 `<asm/pgalloc.h>`, `<asm/tlb.h>` 处理页表操作和 TLB 刷新。\n\n## 5. 使用场景\n\n- **高性能计算 (HPC)**: 应用程序通过 `mmap(MAP_HUGETLB)` 或挂载 hugetlbfs 使用大页，减少 TLB 缺失开销。\n- **数据库系统**: 如 Oracle、MySQL 利用大页提升内存访问性能。\n- **虚拟化环境**: KVM/QEMU 为虚拟机分配大页作为后端内存，提高 I/O 性能。\n- **实时系统**: 通过预留大页确保关键任务的内存确定性。\n- **容器资源隔离**: 结合 cgroup v2 的 hugetlb 控制器，限制容器的大页使用量。\n- **内核启动参数配置**: 通过 `hugepagesz=`, `hugepages=` 等参数在启动时预分配大页池。",
      "similarity": 0.5542154312133789,
      "chunks": [
        {
          "chunk_id": 38,
          "file_path": "mm/hugetlb.c",
          "start_line": 6649,
          "end_line": 6794,
          "content": [
            "long hugetlb_change_protection(struct vm_area_struct *vma,",
            "\t\tunsigned long address, unsigned long end,",
            "\t\tpgprot_t newprot, unsigned long cp_flags)",
            "{",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tunsigned long start = address;",
            "\tpte_t *ptep;",
            "\tpte_t pte;",
            "\tstruct hstate *h = hstate_vma(vma);",
            "\tlong pages = 0, psize = huge_page_size(h);",
            "\tbool shared_pmd = false;",
            "\tstruct mmu_notifier_range range;",
            "\tunsigned long last_addr_mask;",
            "\tbool uffd_wp = cp_flags & MM_CP_UFFD_WP;",
            "\tbool uffd_wp_resolve = cp_flags & MM_CP_UFFD_WP_RESOLVE;",
            "",
            "\t/*",
            "\t * In the case of shared PMDs, the area to flush could be beyond",
            "\t * start/end.  Set range.start/range.end to cover the maximum possible",
            "\t * range if PMD sharing is possible.",
            "\t */",
            "\tmmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_VMA,",
            "\t\t\t\t0, mm, start, end);",
            "\tadjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);",
            "",
            "\tBUG_ON(address >= end);",
            "\tflush_cache_range(vma, range.start, range.end);",
            "",
            "\tmmu_notifier_invalidate_range_start(&range);",
            "\thugetlb_vma_lock_write(vma);",
            "\ti_mmap_lock_write(vma->vm_file->f_mapping);",
            "\tlast_addr_mask = hugetlb_mask_last_page(h);",
            "\tfor (; address < end; address += psize) {",
            "\t\tspinlock_t *ptl;",
            "\t\tptep = hugetlb_walk(vma, address, psize);",
            "\t\tif (!ptep) {",
            "\t\t\tif (!uffd_wp) {",
            "\t\t\t\taddress |= last_addr_mask;",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\t/*",
            "\t\t\t * Userfaultfd wr-protect requires pgtable",
            "\t\t\t * pre-allocations to install pte markers.",
            "\t\t\t */",
            "\t\t\tptep = huge_pte_alloc(mm, vma, address, psize);",
            "\t\t\tif (!ptep) {",
            "\t\t\t\tpages = -ENOMEM;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t\tptl = huge_pte_lock(h, mm, ptep);",
            "\t\tif (huge_pmd_unshare(mm, vma, address, ptep)) {",
            "\t\t\t/*",
            "\t\t\t * When uffd-wp is enabled on the vma, unshare",
            "\t\t\t * shouldn't happen at all.  Warn about it if it",
            "\t\t\t * happened due to some reason.",
            "\t\t\t */",
            "\t\t\tWARN_ON_ONCE(uffd_wp || uffd_wp_resolve);",
            "\t\t\tpages++;",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\tshared_pmd = true;",
            "\t\t\taddress |= last_addr_mask;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tpte = huge_ptep_get(ptep);",
            "\t\tif (unlikely(is_hugetlb_entry_hwpoisoned(pte))) {",
            "\t\t\t/* Nothing to do. */",
            "\t\t} else if (unlikely(is_hugetlb_entry_migration(pte))) {",
            "\t\t\tswp_entry_t entry = pte_to_swp_entry(pte);",
            "\t\t\tstruct page *page = pfn_swap_entry_to_page(entry);",
            "\t\t\tpte_t newpte = pte;",
            "",
            "\t\t\tif (is_writable_migration_entry(entry)) {",
            "\t\t\t\tif (PageAnon(page))",
            "\t\t\t\t\tentry = make_readable_exclusive_migration_entry(",
            "\t\t\t\t\t\t\t\tswp_offset(entry));",
            "\t\t\t\telse",
            "\t\t\t\t\tentry = make_readable_migration_entry(",
            "\t\t\t\t\t\t\t\tswp_offset(entry));",
            "\t\t\t\tnewpte = swp_entry_to_pte(entry);",
            "\t\t\t\tpages++;",
            "\t\t\t}",
            "",
            "\t\t\tif (uffd_wp)",
            "\t\t\t\tnewpte = pte_swp_mkuffd_wp(newpte);",
            "\t\t\telse if (uffd_wp_resolve)",
            "\t\t\t\tnewpte = pte_swp_clear_uffd_wp(newpte);",
            "\t\t\tif (!pte_same(pte, newpte))",
            "\t\t\t\tset_huge_pte_at(mm, address, ptep, newpte, psize);",
            "\t\t} else if (unlikely(is_pte_marker(pte))) {",
            "\t\t\t/*",
            "\t\t\t * Do nothing on a poison marker; page is",
            "\t\t\t * corrupted, permissons do not apply.  Here",
            "\t\t\t * pte_marker_uffd_wp()==true implies !poison",
            "\t\t\t * because they're mutual exclusive.",
            "\t\t\t */",
            "\t\t\tif (pte_marker_uffd_wp(pte) && uffd_wp_resolve)",
            "\t\t\t\t/* Safe to modify directly (non-present->none). */",
            "\t\t\t\thuge_pte_clear(mm, address, ptep, psize);",
            "\t\t} else if (!huge_pte_none(pte)) {",
            "\t\t\tpte_t old_pte;",
            "\t\t\tunsigned int shift = huge_page_shift(hstate_vma(vma));",
            "",
            "\t\t\told_pte = huge_ptep_modify_prot_start(vma, address, ptep);",
            "\t\t\tpte = huge_pte_modify(old_pte, newprot);",
            "\t\t\tpte = arch_make_huge_pte(pte, shift, vma->vm_flags);",
            "\t\t\tif (uffd_wp)",
            "\t\t\t\tpte = huge_pte_mkuffd_wp(pte);",
            "\t\t\telse if (uffd_wp_resolve)",
            "\t\t\t\tpte = huge_pte_clear_uffd_wp(pte);",
            "\t\t\thuge_ptep_modify_prot_commit(vma, address, ptep, old_pte, pte);",
            "\t\t\tpages++;",
            "\t\t} else {",
            "\t\t\t/* None pte */",
            "\t\t\tif (unlikely(uffd_wp))",
            "\t\t\t\t/* Safe to modify directly (none->non-present). */",
            "\t\t\t\tset_huge_pte_at(mm, address, ptep,",
            "\t\t\t\t\t\tmake_pte_marker(PTE_MARKER_UFFD_WP),",
            "\t\t\t\t\t\tpsize);",
            "\t\t}",
            "\t\tspin_unlock(ptl);",
            "\t}",
            "\t/*",
            "\t * Must flush TLB before releasing i_mmap_rwsem: x86's huge_pmd_unshare",
            "\t * may have cleared our pud entry and done put_page on the page table:",
            "\t * once we release i_mmap_rwsem, another task can do the final put_page",
            "\t * and that page table be reused and filled with junk.  If we actually",
            "\t * did unshare a page of pmds, flush the range corresponding to the pud.",
            "\t */",
            "\tif (shared_pmd)",
            "\t\tflush_hugetlb_tlb_range(vma, range.start, range.end);",
            "\telse",
            "\t\tflush_hugetlb_tlb_range(vma, start, end);",
            "\t/*",
            "\t * No need to call mmu_notifier_arch_invalidate_secondary_tlbs() we are",
            "\t * downgrading page table protection not changing it to point to a new",
            "\t * page.",
            "\t *",
            "\t * See Documentation/mm/mmu_notifier.rst",
            "\t */",
            "\ti_mmap_unlock_write(vma->vm_file->f_mapping);",
            "\thugetlb_vma_unlock_write(vma);",
            "\tmmu_notifier_invalidate_range_end(&range);",
            "",
            "\treturn pages > 0 ? (pages << h->order) : pages;",
            "}"
          ],
          "function_name": "hugetlb_change_protection",
          "description": "修改HugeTLB页面的访问保护属性（如只读/可写），处理迁移入口、硬件污染标记等特殊情形，同步更新页表项并触发TLB刷新以保证可见性。",
          "similarity": 0.586483359336853
        },
        {
          "chunk_id": 39,
          "file_path": "mm/hugetlb.c",
          "start_line": 6797,
          "end_line": 6955,
          "content": [
            "bool hugetlb_reserve_pages(struct inode *inode,",
            "\t\t\t\t\tlong from, long to,",
            "\t\t\t\t\tstruct vm_area_struct *vma,",
            "\t\t\t\t\tvm_flags_t vm_flags)",
            "{",
            "\tlong chg = -1, add = -1;",
            "\tstruct hstate *h = hstate_inode(inode);",
            "\tstruct hugepage_subpool *spool = subpool_inode(inode);",
            "\tstruct resv_map *resv_map;",
            "\tstruct hugetlb_cgroup *h_cg = NULL;",
            "\tlong gbl_reserve, regions_needed = 0;",
            "",
            "\t/* This should never happen */",
            "\tif (from > to) {",
            "\t\tVM_WARN(1, \"%s called with a negative range\\n\", __func__);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/*",
            "\t * vma specific semaphore used for pmd sharing and fault/truncation",
            "\t * synchronization",
            "\t */",
            "\thugetlb_vma_lock_alloc(vma);",
            "",
            "\t/*",
            "\t * Only apply hugepage reservation if asked. At fault time, an",
            "\t * attempt will be made for VM_NORESERVE to allocate a page",
            "\t * without using reserves",
            "\t */",
            "\tif (vm_flags & VM_NORESERVE)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Shared mappings base their reservation on the number of pages that",
            "\t * are already allocated on behalf of the file. Private mappings need",
            "\t * to reserve the full area even if read-only as mprotect() may be",
            "\t * called to make the mapping read-write. Assume !vma is a shm mapping",
            "\t */",
            "\tif (!vma || vma->vm_flags & VM_MAYSHARE) {",
            "\t\t/*",
            "\t\t * resv_map can not be NULL as hugetlb_reserve_pages is only",
            "\t\t * called for inodes for which resv_maps were created (see",
            "\t\t * hugetlbfs_get_inode).",
            "\t\t */",
            "\t\tresv_map = inode_resv_map(inode);",
            "",
            "\t\tchg = region_chg(resv_map, from, to, &regions_needed);",
            "\t} else {",
            "\t\t/* Private mapping. */",
            "\t\tresv_map = resv_map_alloc();",
            "\t\tif (!resv_map)",
            "\t\t\tgoto out_err;",
            "",
            "\t\tchg = to - from;",
            "",
            "\t\tset_vma_resv_map(vma, resv_map);",
            "\t\tset_vma_resv_flags(vma, HPAGE_RESV_OWNER);",
            "\t}",
            "",
            "\tif (chg < 0)",
            "\t\tgoto out_err;",
            "",
            "\tif (hugetlb_cgroup_charge_cgroup_rsvd(hstate_index(h),",
            "\t\t\t\tchg * pages_per_huge_page(h), &h_cg) < 0)",
            "\t\tgoto out_err;",
            "",
            "\tif (vma && !(vma->vm_flags & VM_MAYSHARE) && h_cg) {",
            "\t\t/* For private mappings, the hugetlb_cgroup uncharge info hangs",
            "\t\t * of the resv_map.",
            "\t\t */",
            "\t\tresv_map_set_hugetlb_cgroup_uncharge_info(resv_map, h_cg, h);",
            "\t}",
            "",
            "\t/*",
            "\t * There must be enough pages in the subpool for the mapping. If",
            "\t * the subpool has a minimum size, there may be some global",
            "\t * reservations already in place (gbl_reserve).",
            "\t */",
            "\tgbl_reserve = hugepage_subpool_get_pages(spool, chg);",
            "\tif (gbl_reserve < 0)",
            "\t\tgoto out_uncharge_cgroup;",
            "",
            "\t/*",
            "\t * Check enough hugepages are available for the reservation.",
            "\t * Hand the pages back to the subpool if there are not",
            "\t */",
            "\tif (hugetlb_acct_memory(h, gbl_reserve) < 0)",
            "\t\tgoto out_put_pages;",
            "",
            "\t/*",
            "\t * Account for the reservations made. Shared mappings record regions",
            "\t * that have reservations as they are shared by multiple VMAs.",
            "\t * When the last VMA disappears, the region map says how much",
            "\t * the reservation was and the page cache tells how much of",
            "\t * the reservation was consumed. Private mappings are per-VMA and",
            "\t * only the consumed reservations are tracked. When the VMA",
            "\t * disappears, the original reservation is the VMA size and the",
            "\t * consumed reservations are stored in the map. Hence, nothing",
            "\t * else has to be done for private mappings here",
            "\t */",
            "\tif (!vma || vma->vm_flags & VM_MAYSHARE) {",
            "\t\tadd = region_add(resv_map, from, to, regions_needed, h, h_cg);",
            "",
            "\t\tif (unlikely(add < 0)) {",
            "\t\t\thugetlb_acct_memory(h, -gbl_reserve);",
            "\t\t\tgoto out_put_pages;",
            "\t\t} else if (unlikely(chg > add)) {",
            "\t\t\t/*",
            "\t\t\t * pages in this range were added to the reserve",
            "\t\t\t * map between region_chg and region_add.  This",
            "\t\t\t * indicates a race with alloc_hugetlb_folio.  Adjust",
            "\t\t\t * the subpool and reserve counts modified above",
            "\t\t\t * based on the difference.",
            "\t\t\t */",
            "\t\t\tlong rsv_adjust;",
            "",
            "\t\t\t/*",
            "\t\t\t * hugetlb_cgroup_uncharge_cgroup_rsvd() will put the",
            "\t\t\t * reference to h_cg->css. See comment below for detail.",
            "\t\t\t */",
            "\t\t\thugetlb_cgroup_uncharge_cgroup_rsvd(",
            "\t\t\t\thstate_index(h),",
            "\t\t\t\t(chg - add) * pages_per_huge_page(h), h_cg);",
            "",
            "\t\t\trsv_adjust = hugepage_subpool_put_pages(spool,",
            "\t\t\t\t\t\t\t\tchg - add);",
            "\t\t\thugetlb_acct_memory(h, -rsv_adjust);",
            "\t\t} else if (h_cg) {",
            "\t\t\t/*",
            "\t\t\t * The file_regions will hold their own reference to",
            "\t\t\t * h_cg->css. So we should release the reference held",
            "\t\t\t * via hugetlb_cgroup_charge_cgroup_rsvd() when we are",
            "\t\t\t * done.",
            "\t\t\t */",
            "\t\t\thugetlb_cgroup_put_rsvd_cgroup(h_cg);",
            "\t\t}",
            "\t}",
            "\treturn true;",
            "",
            "out_put_pages:",
            "\t/* put back original number of pages, chg */",
            "\t(void)hugepage_subpool_put_pages(spool, chg);",
            "out_uncharge_cgroup:",
            "\thugetlb_cgroup_uncharge_cgroup_rsvd(hstate_index(h),",
            "\t\t\t\t\t    chg * pages_per_huge_page(h), h_cg);",
            "out_err:",
            "\thugetlb_vma_lock_free(vma);",
            "\tif (!vma || vma->vm_flags & VM_MAYSHARE)",
            "\t\t/* Only call region_abort if the region_chg succeeded but the",
            "\t\t * region_add failed or didn't run.",
            "\t\t */",
            "\t\tif (chg >= 0 && add < 0)",
            "\t\t\tregion_abort(resv_map, from, to, regions_needed);",
            "\tif (vma && is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {",
            "\t\tkref_put(&resv_map->refs, resv_map_release);",
            "\t\tset_vma_resv_map(vma, NULL);",
            "\t}",
            "\treturn false;",
            "}"
          ],
          "function_name": "hugetlb_reserve_pages",
          "description": "为文件映射区域预留HugeTLB页面资源，区分共享与私有映射类型进行全局和局部资源检查，处理资源不足场景并通过引用计数管理预留状态。",
          "similarity": 0.5791946649551392
        },
        {
          "chunk_id": 37,
          "file_path": "mm/hugetlb.c",
          "start_line": 6438,
          "end_line": 6646,
          "content": [
            "int hugetlb_mfill_atomic_pte(pte_t *dst_pte,",
            "\t\t\t     struct vm_area_struct *dst_vma,",
            "\t\t\t     unsigned long dst_addr,",
            "\t\t\t     unsigned long src_addr,",
            "\t\t\t     uffd_flags_t flags,",
            "\t\t\t     struct folio **foliop)",
            "{",
            "\tstruct mm_struct *dst_mm = dst_vma->vm_mm;",
            "\tbool is_continue = uffd_flags_mode_is(flags, MFILL_ATOMIC_CONTINUE);",
            "\tbool wp_enabled = (flags & MFILL_ATOMIC_WP);",
            "\tstruct hstate *h = hstate_vma(dst_vma);",
            "\tstruct address_space *mapping = dst_vma->vm_file->f_mapping;",
            "\tpgoff_t idx = vma_hugecache_offset(h, dst_vma, dst_addr);",
            "\tunsigned long size;",
            "\tint vm_shared = dst_vma->vm_flags & VM_SHARED;",
            "\tpte_t _dst_pte;",
            "\tspinlock_t *ptl;",
            "\tint ret = -ENOMEM;",
            "\tstruct folio *folio;",
            "\tint writable;",
            "\tbool folio_in_pagecache = false;",
            "",
            "\tif (uffd_flags_mode_is(flags, MFILL_ATOMIC_POISON)) {",
            "\t\tptl = huge_pte_lock(h, dst_mm, dst_pte);",
            "",
            "\t\t/* Don't overwrite any existing PTEs (even markers) */",
            "\t\tif (!huge_pte_none(huge_ptep_get(dst_pte))) {",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\treturn -EEXIST;",
            "\t\t}",
            "",
            "\t\t_dst_pte = make_pte_marker(PTE_MARKER_POISONED);",
            "\t\tset_huge_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte,",
            "\t\t\t\thuge_page_size(h));",
            "",
            "\t\t/* No need to invalidate - it was non-present before */",
            "\t\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);",
            "",
            "\t\tspin_unlock(ptl);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (is_continue) {",
            "\t\tret = -EFAULT;",
            "\t\tfolio = filemap_lock_hugetlb_folio(h, mapping, idx);",
            "\t\tif (IS_ERR(folio))",
            "\t\t\tgoto out;",
            "\t\tfolio_in_pagecache = true;",
            "\t} else if (!*foliop) {",
            "\t\t/* If a folio already exists, then it's UFFDIO_COPY for",
            "\t\t * a non-missing case. Return -EEXIST.",
            "\t\t */",
            "\t\tif (vm_shared &&",
            "\t\t    hugetlbfs_pagecache_present(h, dst_vma, dst_addr)) {",
            "\t\t\tret = -EEXIST;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tfolio = alloc_hugetlb_folio(dst_vma, dst_addr, 0);",
            "\t\tif (IS_ERR(folio)) {",
            "\t\t\tret = -ENOMEM;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tret = copy_folio_from_user(folio, (const void __user *) src_addr,",
            "\t\t\t\t\t   false);",
            "",
            "\t\t/* fallback to copy_from_user outside mmap_lock */",
            "\t\tif (unlikely(ret)) {",
            "\t\t\tret = -ENOENT;",
            "\t\t\t/* Free the allocated folio which may have",
            "\t\t\t * consumed a reservation.",
            "\t\t\t */",
            "\t\t\trestore_reserve_on_error(h, dst_vma, dst_addr, folio);",
            "\t\t\tfolio_put(folio);",
            "",
            "\t\t\t/* Allocate a temporary folio to hold the copied",
            "\t\t\t * contents.",
            "\t\t\t */",
            "\t\t\tfolio = alloc_hugetlb_folio_vma(h, dst_vma, dst_addr);",
            "\t\t\tif (!folio) {",
            "\t\t\t\tret = -ENOMEM;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t\t*foliop = folio;",
            "\t\t\t/* Set the outparam foliop and return to the caller to",
            "\t\t\t * copy the contents outside the lock. Don't free the",
            "\t\t\t * folio.",
            "\t\t\t */",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t} else {",
            "\t\tif (vm_shared &&",
            "\t\t    hugetlbfs_pagecache_present(h, dst_vma, dst_addr)) {",
            "\t\t\tfolio_put(*foliop);",
            "\t\t\tret = -EEXIST;",
            "\t\t\t*foliop = NULL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tfolio = alloc_hugetlb_folio(dst_vma, dst_addr, 0);",
            "\t\tif (IS_ERR(folio)) {",
            "\t\t\tfolio_put(*foliop);",
            "\t\t\tret = -ENOMEM;",
            "\t\t\t*foliop = NULL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tret = copy_user_large_folio(folio, *foliop, dst_addr, dst_vma);",
            "\t\tfolio_put(*foliop);",
            "\t\t*foliop = NULL;",
            "\t\tif (ret) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * The memory barrier inside __folio_mark_uptodate makes sure that",
            "\t * preceding stores to the page contents become visible before",
            "\t * the set_pte_at() write.",
            "\t */",
            "\t__folio_mark_uptodate(folio);",
            "",
            "\t/* Add shared, newly allocated pages to the page cache. */",
            "\tif (vm_shared && !is_continue) {",
            "\t\tsize = i_size_read(mapping->host) >> huge_page_shift(h);",
            "\t\tret = -EFAULT;",
            "\t\tif (idx >= size)",
            "\t\t\tgoto out_release_nounlock;",
            "",
            "\t\t/*",
            "\t\t * Serialization between remove_inode_hugepages() and",
            "\t\t * hugetlb_add_to_page_cache() below happens through the",
            "\t\t * hugetlb_fault_mutex_table that here must be hold by",
            "\t\t * the caller.",
            "\t\t */",
            "\t\tret = hugetlb_add_to_page_cache(folio, mapping, idx);",
            "\t\tif (ret)",
            "\t\t\tgoto out_release_nounlock;",
            "\t\tfolio_in_pagecache = true;",
            "\t}",
            "",
            "\tptl = huge_pte_lock(h, dst_mm, dst_pte);",
            "",
            "\tret = -EIO;",
            "\tif (folio_test_hwpoison(folio))",
            "\t\tgoto out_release_unlock;",
            "",
            "\t/*",
            "\t * We allow to overwrite a pte marker: consider when both MISSING|WP",
            "\t * registered, we firstly wr-protect a none pte which has no page cache",
            "\t * page backing it, then access the page.",
            "\t */",
            "\tret = -EEXIST;",
            "\tif (!huge_pte_none_mostly(huge_ptep_get(dst_pte)))",
            "\t\tgoto out_release_unlock;",
            "",
            "\tif (folio_in_pagecache)",
            "\t\thugetlb_add_file_rmap(folio);",
            "\telse",
            "\t\thugetlb_add_new_anon_rmap(folio, dst_vma, dst_addr);",
            "",
            "\t/*",
            "\t * For either: (1) CONTINUE on a non-shared VMA, or (2) UFFDIO_COPY",
            "\t * with wp flag set, don't set pte write bit.",
            "\t */",
            "\tif (wp_enabled || (is_continue && !vm_shared))",
            "\t\twritable = 0;",
            "\telse",
            "\t\twritable = dst_vma->vm_flags & VM_WRITE;",
            "",
            "\t_dst_pte = make_huge_pte(dst_vma, &folio->page, writable);",
            "\t/*",
            "\t * Always mark UFFDIO_COPY page dirty; note that this may not be",
            "\t * extremely important for hugetlbfs for now since swapping is not",
            "\t * supported, but we should still be clear in that this page cannot be",
            "\t * thrown away at will, even if write bit not set.",
            "\t */",
            "\t_dst_pte = huge_pte_mkdirty(_dst_pte);",
            "\t_dst_pte = pte_mkyoung(_dst_pte);",
            "",
            "\tif (wp_enabled)",
            "\t\t_dst_pte = huge_pte_mkuffd_wp(_dst_pte);",
            "",
            "\tset_huge_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte, huge_page_size(h));",
            "",
            "\thugetlb_count_add(pages_per_huge_page(h), dst_mm);",
            "",
            "\t/* No need to invalidate - it was non-present before */",
            "\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);",
            "",
            "\tspin_unlock(ptl);",
            "\tif (!is_continue)",
            "\t\tfolio_set_hugetlb_migratable(folio);",
            "\tif (vm_shared || is_continue)",
            "\t\tfolio_unlock(folio);",
            "\tret = 0;",
            "out:",
            "\treturn ret;",
            "out_release_unlock:",
            "\tspin_unlock(ptl);",
            "\tif (vm_shared || is_continue)",
            "\t\tfolio_unlock(folio);",
            "out_release_nounlock:",
            "\tif (!folio_in_pagecache)",
            "\t\trestore_reserve_on_error(h, dst_vma, dst_addr, folio);",
            "\tfolio_put(folio);",
            "\tgoto out;",
            "}"
          ],
          "function_name": "hugetlb_mfill_atomic_pte",
          "description": "原子方式将用户空间数据填充至新分配的HugeTLB页面中，支持连续操作模式及用户故障描述符标志，处理页面拷贝过程中的异常情况并释放资源。",
          "similarity": 0.560237467288971
        },
        {
          "chunk_id": 0,
          "file_path": "mm/hugetlb.c",
          "start_line": 1,
          "end_line": 91,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Generic hugetlb support.",
            " * (C) Nadia Yvette Chambers, April 2004",
            " */",
            "#include <linux/list.h>",
            "#include <linux/init.h>",
            "#include <linux/mm.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/highmem.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/compiler.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/mutex.h>",
            "#include <linux/memblock.h>",
            "#include <linux/sysfs.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mmdebug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/rmap.h>",
            "#include <linux/string_helpers.h>",
            "#include <linux/swap.h>",
            "#include <linux/swapops.h>",
            "#include <linux/jhash.h>",
            "#include <linux/numa.h>",
            "#include <linux/llist.h>",
            "#include <linux/cma.h>",
            "#include <linux/migrate.h>",
            "#include <linux/nospec.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/memory.h>",
            "#include <linux/mm_inline.h>",
            "",
            "#include <asm/page.h>",
            "#include <asm/pgalloc.h>",
            "#include <asm/tlb.h>",
            "",
            "#include <linux/io.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/hugetlb_cgroup.h>",
            "#include <linux/node.h>",
            "#include <linux/page_owner.h>",
            "#include \"internal.h\"",
            "#include \"hugetlb_vmemmap.h\"",
            "",
            "int hugetlb_max_hstate __read_mostly;",
            "unsigned int default_hstate_idx;",
            "struct hstate hstates[HUGE_MAX_HSTATE];",
            "",
            "#ifdef CONFIG_CMA",
            "static struct cma *hugetlb_cma[MAX_NUMNODES];",
            "static unsigned long hugetlb_cma_size_in_node[MAX_NUMNODES] __initdata;",
            "#endif",
            "static unsigned long hugetlb_cma_size __initdata;",
            "",
            "__initdata LIST_HEAD(huge_boot_pages);",
            "",
            "/* for command line parsing */",
            "static struct hstate * __initdata parsed_hstate;",
            "static unsigned long __initdata default_hstate_max_huge_pages;",
            "static bool __initdata parsed_valid_hugepagesz = true;",
            "static bool __initdata parsed_default_hugepagesz;",
            "static unsigned int default_hugepages_in_node[MAX_NUMNODES] __initdata;",
            "",
            "/*",
            " * Protects updates to hugepage_freelists, hugepage_activelist, nr_huge_pages,",
            " * free_huge_pages, and surplus_huge_pages.",
            " */",
            "DEFINE_SPINLOCK(hugetlb_lock);",
            "",
            "/*",
            " * Serializes faults on the same logical page.  This is used to",
            " * prevent spurious OOMs when the hugepage pool is fully utilized.",
            " */",
            "static int num_fault_mutexes;",
            "struct mutex *hugetlb_fault_mutex_table ____cacheline_aligned_in_smp;",
            "",
            "/* Forward declaration */",
            "static int hugetlb_acct_memory(struct hstate *h, long delta);",
            "static void hugetlb_vma_lock_free(struct vm_area_struct *vma);",
            "static void hugetlb_vma_lock_alloc(struct vm_area_struct *vma);",
            "static void __hugetlb_vma_unlock_write_free(struct vm_area_struct *vma);",
            "static void hugetlb_unshare_pmds(struct vm_area_struct *vma,",
            "\t\tunsigned long start, unsigned long end, bool take_locks);",
            "static struct resv_map *vma_resv_map(struct vm_area_struct *vma);",
            ""
          ],
          "function_name": null,
          "description": "定义了Hugetlb通用支持的全局变量和锁，包括hstates数组用于存储不同大小的hugepage状态，hugetlb_lock保护共享页列表更新，以及针对CMA（连续内存区）的相关配置。核心功能是初始化并管理hugepage的全局状态和同步机制。",
          "similarity": 0.5358929634094238
        },
        {
          "chunk_id": 14,
          "file_path": "mm/hugetlb.c",
          "start_line": 2566,
          "end_line": 2686,
          "content": [
            "static void return_unused_surplus_pages(struct hstate *h,",
            "\t\t\t\t\tunsigned long unused_resv_pages)",
            "{",
            "\tunsigned long nr_pages;",
            "\tLIST_HEAD(page_list);",
            "",
            "\tlockdep_assert_held(&hugetlb_lock);",
            "\t/* Uncommit the reservation */",
            "\th->resv_huge_pages -= unused_resv_pages;",
            "",
            "\tif (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * Part (or even all) of the reservation could have been backed",
            "\t * by pre-allocated pages. Only free surplus pages.",
            "\t */",
            "\tnr_pages = min(unused_resv_pages, h->surplus_huge_pages);",
            "",
            "\t/*",
            "\t * We want to release as many surplus pages as possible, spread",
            "\t * evenly across all nodes with memory. Iterate across these nodes",
            "\t * until we can no longer free unreserved surplus pages. This occurs",
            "\t * when the nodes with surplus pages have no free pages.",
            "\t * remove_pool_hugetlb_folio() will balance the freed pages across the",
            "\t * on-line nodes with memory and will handle the hstate accounting.",
            "\t */",
            "\twhile (nr_pages--) {",
            "\t\tstruct folio *folio;",
            "",
            "\t\tfolio = remove_pool_hugetlb_folio(h, &node_states[N_MEMORY], 1);",
            "\t\tif (!folio)",
            "\t\t\tgoto out;",
            "",
            "\t\tlist_add(&folio->lru, &page_list);",
            "\t}",
            "",
            "out:",
            "\tspin_unlock_irq(&hugetlb_lock);",
            "\tupdate_and_free_pages_bulk(h, &page_list);",
            "\tspin_lock_irq(&hugetlb_lock);",
            "}",
            "static long __vma_reservation_common(struct hstate *h,",
            "\t\t\t\tstruct vm_area_struct *vma, unsigned long addr,",
            "\t\t\t\tenum vma_resv_mode mode)",
            "{",
            "\tstruct resv_map *resv;",
            "\tpgoff_t idx;",
            "\tlong ret;",
            "\tlong dummy_out_regions_needed;",
            "",
            "\tresv = vma_resv_map(vma);",
            "\tif (!resv)",
            "\t\treturn 1;",
            "",
            "\tidx = vma_hugecache_offset(h, vma, addr);",
            "\tswitch (mode) {",
            "\tcase VMA_NEEDS_RESV:",
            "\t\tret = region_chg(resv, idx, idx + 1, &dummy_out_regions_needed);",
            "\t\t/* We assume that vma_reservation_* routines always operate on",
            "\t\t * 1 page, and that adding to resv map a 1 page entry can only",
            "\t\t * ever require 1 region.",
            "\t\t */",
            "\t\tVM_BUG_ON(dummy_out_regions_needed != 1);",
            "\t\tbreak;",
            "\tcase VMA_COMMIT_RESV:",
            "\t\tret = region_add(resv, idx, idx + 1, 1, NULL, NULL);",
            "\t\t/* region_add calls of range 1 should never fail. */",
            "\t\tVM_BUG_ON(ret < 0);",
            "\t\tbreak;",
            "\tcase VMA_END_RESV:",
            "\t\tregion_abort(resv, idx, idx + 1, 1);",
            "\t\tret = 0;",
            "\t\tbreak;",
            "\tcase VMA_ADD_RESV:",
            "\t\tif (vma->vm_flags & VM_MAYSHARE) {",
            "\t\t\tret = region_add(resv, idx, idx + 1, 1, NULL, NULL);",
            "\t\t\t/* region_add calls of range 1 should never fail. */",
            "\t\t\tVM_BUG_ON(ret < 0);",
            "\t\t} else {",
            "\t\t\tregion_abort(resv, idx, idx + 1, 1);",
            "\t\t\tret = region_del(resv, idx, idx + 1);",
            "\t\t}",
            "\t\tbreak;",
            "\tcase VMA_DEL_RESV:",
            "\t\tif (vma->vm_flags & VM_MAYSHARE) {",
            "\t\t\tregion_abort(resv, idx, idx + 1, 1);",
            "\t\t\tret = region_del(resv, idx, idx + 1);",
            "\t\t} else {",
            "\t\t\tret = region_add(resv, idx, idx + 1, 1, NULL, NULL);",
            "\t\t\t/* region_add calls of range 1 should never fail. */",
            "\t\t\tVM_BUG_ON(ret < 0);",
            "\t\t}",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "",
            "\tif (vma->vm_flags & VM_MAYSHARE || mode == VMA_DEL_RESV)",
            "\t\treturn ret;",
            "\t/*",
            "\t * We know private mapping must have HPAGE_RESV_OWNER set.",
            "\t *",
            "\t * In most cases, reserves always exist for private mappings.",
            "\t * However, a file associated with mapping could have been",
            "\t * hole punched or truncated after reserves were consumed.",
            "\t * As subsequent fault on such a range will not use reserves.",
            "\t * Subtle - The reserve map for private mappings has the",
            "\t * opposite meaning than that of shared mappings.  If NO",
            "\t * entry is in the reserve map, it means a reservation exists.",
            "\t * If an entry exists in the reserve map, it means the",
            "\t * reservation has already been consumed.  As a result, the",
            "\t * return value of this routine is the opposite of the",
            "\t * value returned from reserve map manipulation routines above.",
            "\t */",
            "\tif (ret > 0)",
            "\t\treturn 0;",
            "\tif (ret == 0)",
            "\t\treturn 1;",
            "\treturn ret;",
            "}"
          ],
          "function_name": "return_unused_surplus_pages, __vma_reservation_common",
          "description": "该代码段涉及HugeTLB页面管理的两个关键组件：  \n1. `return_unused_surplus_pages`用于回收未使用的预留巨页，通过减少预留计数并释放多余页框至空闲列表，平衡节点间内存分布；  \n2. `__vma_reservation_common`负责根据VMA操作模式（如分配/释放预留区）维护resv_map，区分共享/私有映射的预留语义反向关系，确保预留状态与实际内存占用一致。  \n上下文完整，未引入额外假设或虚构API。",
          "similarity": 0.529258131980896
        }
      ]
    }
  ]
}