{
  "query": "资源分配图限制在死锁预防中的应用",
  "timestamp": "2025-12-26 01:07:05",
  "retrieved_files": [
    {
      "source_file": "kernel/irq/matrix.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:03:08\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\matrix.c`\n\n---\n\n# `irq/matrix.c` 技术文档\n\n## 1. 文件概述\n\n`irq/matrix.c` 实现了一个通用的中断位图（IRQ matrix）管理机制，用于在多 CPU 系统中高效地分配和管理中断向量（或中断位）。该机制支持两类中断分配：\n\n- **普通分配（allocated）**：由设备驱动等动态申请的中断。\n- **托管分配（managed）**：由内核子系统（如 MSI/MSI-X）预先保留、按需激活的中断。\n\n该文件通过 per-CPU 的位图结构，结合全局状态跟踪，实现了跨 CPU 的中断资源分配、预留、释放和在线/离线管理，特别适用于中断向量数量有限（如 x86 的 256 个向量）的架构。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct cpumap`**：每个 CPU 的本地中断位图状态\n  - `available`：当前 CPU 可用的中断数量\n  - `allocated`：已分配的普通中断数量\n  - `managed` / `managed_allocated`：预留和已激活的托管中断数量\n  - `alloc_map[]`：记录已分配的普通中断位\n  - `managed_map[]`：记录预留的托管中断位\n  - `initialized` / `online`：CPU 初始化和在线状态\n\n- **`struct irq_matrix`**：全局中断矩阵控制结构\n  - `matrix_bits`：总位图大小（≤ `IRQ_MATRIX_BITS`）\n  - `alloc_start` / `alloc_end`：可分配范围\n  - `global_available`：全局可用中断总数\n  - `system_map[]`：系统保留位（如 APIC 自身使用的向量）\n  - `maps`：指向 per-CPU `cpumap` 的指针\n  - `scratch_map[]`：临时位图，用于分配时的合并计算\n\n### 主要函数\n\n| 函数 | 功能 |\n|------|------|\n| `irq_alloc_matrix()` | 分配并初始化一个 `irq_matrix` 结构 |\n| `irq_matrix_online()` / `irq_matrix_offline()` | 将本地 CPU 的中断矩阵置为在线/离线状态 |\n| `irq_matrix_assign_system()` | 在矩阵中保留系统级中断位（如 APIC 向量） |\n| `irq_matrix_reserve_managed()` | 在指定 CPU 掩码上为托管中断预留位 |\n| `irq_matrix_remove_managed()` | 移除托管中断的预留位 |\n| `irq_matrix_alloc_managed()` | 从预留的托管中断中分配一个实际使用的中断 |\n| `matrix_alloc_area()` | 内部辅助函数：在合并位图中查找连续空闲区域 |\n| `matrix_find_best_cpu()` / `matrix_find_best_cpu_managed()` | 选择最优 CPU（基于可用数或托管分配数最少） |\n\n## 3. 关键实现\n\n### 位图合并分配策略\n- 在分配中断时，`matrix_alloc_area()` 会临时合并三个位图：\n  1. 当前 CPU 的 `managed_map`（托管预留）\n  2. 全局 `system_map`（系统保留）\n  3. 当前 CPU 的 `alloc_map`（已分配）\n- 使用 `bitmap_find_next_zero_area()` 在合并后的位图中查找连续空闲区域，确保不会重复分配。\n\n### 托管中断（Managed IRQ）机制\n- **两阶段分配**：\n  1. **预留（reserve）**：调用 `irq_matrix_reserve_managed()` 在多个 CPU 上各预留一个位（不一定对齐）。\n  2. **激活（alloc）**：调用 `irq_matrix_alloc_managed()` 从预留位中选择一个未使用的位进行实际分配。\n- **动态 CPU 选择**：`matrix_find_best_cpu_managed()` 优先选择 `managed_allocated` 最少的 CPU，实现负载均衡。\n\n### 系统中断保留\n- `irq_matrix_assign_system()` 用于保留如 x86 的 `IRQ0_VECTOR`（时钟中断）等关键系统向量。\n- 通过 `BUG_ON()` 强制保证：系统中断只能在单 CPU 初始化阶段分配，防止运行时冲突。\n\n### 在线/离线管理\n- CPU 上线时，将其 `available` 计数加入 `global_available`。\n- CPU 离线时，从全局计数中减去，但保留其位图数据（支持重新上线）。\n\n### 跟踪与调试\n- 集成 `trace/events/irq_matrix.h`，提供分配、预留、系统保留等关键操作的 tracepoint，便于调试中断分配问题。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/bitmap.h>`：位图操作（`bitmap_set`, `bitmap_find_next_zero_area` 等）\n  - `<linux/percpu.h>`：Per-CPU 变量支持\n  - `<linux/cpu.h>`：CPU 在线/离线状态\n  - `<linux/irq.h>`：中断子系统基础定义\n  - `<trace/events/irq_matrix.h>`：自定义 tracepoint\n\n- **内核子系统**：\n  - **中断子系统**：作为底层分配器，被 `irqdomain`、MSI/MSI-X 驱动等使用。\n  - **x86 APIC 驱动**：典型使用者，用于管理 256 个中断向量的分配（如 `kernel/irq/vector.c`）。\n\n## 5. 使用场景\n\n- **x86 中断向量管理**：在 `CONFIG_X86_IO_APIC` 或 `CONFIG_X86_LOCAL_APIC` 下，用于分配 IRQ 向量（0-255），区分系统向量、普通设备中断和 MSI 中断。\n- **MSI/MSI-X 中断分配**：PCIe 设备的 MSI 中断通过托管机制预留和分配，确保每个设备在多个 CPU 上有可用向量。\n- **CPU 热插拔**：支持 CPU 动态上线/下线时的中断资源重新平衡。\n- **中断负载均衡**：通过 `matrix_find_best_cpu*` 函数，在多 CPU 间均匀分配中断，避免单 CPU 向量耗尽。",
      "similarity": 0.5586535334587097,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 418,
          "end_line": 483,
          "content": [
            "void irq_matrix_free(struct irq_matrix *m, unsigned int cpu,",
            "\t\t     unsigned int bit, bool managed)",
            "{",
            "\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\tif (WARN_ON_ONCE(bit < m->alloc_start || bit >= m->alloc_end))",
            "\t\treturn;",
            "",
            "\tif (WARN_ON_ONCE(!test_and_clear_bit(bit, cm->alloc_map)))",
            "\t\treturn;",
            "",
            "\tcm->allocated--;",
            "\tif(managed)",
            "\t\tcm->managed_allocated--;",
            "",
            "\tif (cm->online)",
            "\t\tm->total_allocated--;",
            "",
            "\tif (!managed) {",
            "\t\tcm->available++;",
            "\t\tif (cm->online)",
            "\t\t\tm->global_available++;",
            "\t}",
            "\ttrace_irq_matrix_free(bit, cpu, m, cm);",
            "}",
            "unsigned int irq_matrix_available(struct irq_matrix *m, bool cpudown)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tif (!cpudown)",
            "\t\treturn m->global_available;",
            "\treturn m->global_available - cm->available;",
            "}",
            "unsigned int irq_matrix_reserved(struct irq_matrix *m)",
            "{",
            "\treturn m->global_reserved;",
            "}",
            "unsigned int irq_matrix_allocated(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\treturn cm->allocated - cm->managed_allocated;",
            "}",
            "void irq_matrix_debug_show(struct seq_file *sf, struct irq_matrix *m, int ind)",
            "{",
            "\tunsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);",
            "\tint cpu;",
            "",
            "\tseq_printf(sf, \"Online bitmaps:   %6u\\n\", m->online_maps);",
            "\tseq_printf(sf, \"Global available: %6u\\n\", m->global_available);",
            "\tseq_printf(sf, \"Global reserved:  %6u\\n\", m->global_reserved);",
            "\tseq_printf(sf, \"Total allocated:  %6u\\n\", m->total_allocated);",
            "\tseq_printf(sf, \"System: %u: %*pbl\\n\", nsys, m->matrix_bits,",
            "\t\t   m->system_map);",
            "\tseq_printf(sf, \"%*s| CPU | avl | man | mac | act | vectors\\n\", ind, \" \");",
            "\tcpus_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tseq_printf(sf, \"%*s %4d  %4u  %4u  %4u %4u  %*pbl\\n\", ind, \" \",",
            "\t\t\t   cpu, cm->available, cm->managed,",
            "\t\t\t   cm->managed_allocated, cm->allocated,",
            "\t\t\t   m->matrix_bits, cm->alloc_map);",
            "\t}",
            "\tcpus_read_unlock();",
            "}"
          ],
          "function_name": "irq_matrix_free, irq_matrix_available, irq_matrix_reserved, irq_matrix_allocated, irq_matrix_debug_show",
          "description": "提供中断资源的释放接口，实现全局和CPU级的资源使用统计查询，包含调试信息展示功能，通过位图操作维护系统中断位的使用状态",
          "similarity": 0.5608103275299072
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 78,
          "end_line": 205,
          "content": [
            "void irq_matrix_online(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tBUG_ON(cm->online);",
            "",
            "\tif (!cm->initialized) {",
            "\t\tcm->available = m->alloc_size;",
            "\t\tcm->available -= cm->managed + m->systembits_inalloc;",
            "\t\tcm->initialized = true;",
            "\t}",
            "\tm->global_available += cm->available;",
            "\tcm->online = true;",
            "\tm->online_maps++;",
            "\ttrace_irq_matrix_online(m);",
            "}",
            "void irq_matrix_offline(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\t/* Update the global available size */",
            "\tm->global_available -= cm->available;",
            "\tcm->online = false;",
            "\tm->online_maps--;",
            "\ttrace_irq_matrix_offline(m);",
            "}",
            "static unsigned int matrix_alloc_area(struct irq_matrix *m, struct cpumap *cm,",
            "\t\t\t\t      unsigned int num, bool managed)",
            "{",
            "\tunsigned int area, start = m->alloc_start;",
            "\tunsigned int end = m->alloc_end;",
            "",
            "\tbitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);",
            "\tbitmap_or(m->scratch_map, m->scratch_map, cm->alloc_map, end);",
            "\tarea = bitmap_find_next_zero_area(m->scratch_map, end, start, num, 0);",
            "\tif (area >= end)",
            "\t\treturn area;",
            "\tif (managed)",
            "\t\tbitmap_set(cm->managed_map, area, num);",
            "\telse",
            "\t\tbitmap_set(cm->alloc_map, area, num);",
            "\treturn area;",
            "}",
            "static unsigned int matrix_find_best_cpu(struct irq_matrix *m,",
            "\t\t\t\t\tconst struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, best_cpu, maxavl = 0;",
            "\tstruct cpumap *cm;",
            "",
            "\tbest_cpu = UINT_MAX;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tcm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tif (!cm->online || cm->available <= maxavl)",
            "\t\t\tcontinue;",
            "",
            "\t\tbest_cpu = cpu;",
            "\t\tmaxavl = cm->available;",
            "\t}",
            "\treturn best_cpu;",
            "}",
            "static unsigned int matrix_find_best_cpu_managed(struct irq_matrix *m,",
            "\t\t\t\t\t\tconst struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, best_cpu, allocated = UINT_MAX;",
            "\tstruct cpumap *cm;",
            "",
            "\tbest_cpu = UINT_MAX;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tcm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tif (!cm->online || cm->managed_allocated > allocated)",
            "\t\t\tcontinue;",
            "",
            "\t\tbest_cpu = cpu;",
            "\t\tallocated = cm->managed_allocated;",
            "\t}",
            "\treturn best_cpu;",
            "}",
            "void irq_matrix_assign_system(struct irq_matrix *m, unsigned int bit,",
            "\t\t\t      bool replace)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tBUG_ON(bit > m->matrix_bits);",
            "\tBUG_ON(m->online_maps > 1 || (m->online_maps && !replace));",
            "",
            "\tset_bit(bit, m->system_map);",
            "\tif (replace) {",
            "\t\tBUG_ON(!test_and_clear_bit(bit, cm->alloc_map));",
            "\t\tcm->allocated--;",
            "\t\tm->total_allocated--;",
            "\t}",
            "\tif (bit >= m->alloc_start && bit < m->alloc_end)",
            "\t\tm->systembits_inalloc++;",
            "",
            "\ttrace_irq_matrix_assign_system(bit, m);",
            "}",
            "int irq_matrix_reserve_managed(struct irq_matrix *m, const struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, failed_cpu;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "\t\tunsigned int bit;",
            "",
            "\t\tbit = matrix_alloc_area(m, cm, 1, true);",
            "\t\tif (bit >= m->alloc_end)",
            "\t\t\tgoto cleanup;",
            "\t\tcm->managed++;",
            "\t\tif (cm->online) {",
            "\t\t\tcm->available--;",
            "\t\t\tm->global_available--;",
            "\t\t}",
            "\t\ttrace_irq_matrix_reserve_managed(bit, cpu, m, cm);",
            "\t}",
            "\treturn 0;",
            "cleanup:",
            "\tfailed_cpu = cpu;",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tif (cpu == failed_cpu)",
            "\t\t\tbreak;",
            "\t\tirq_matrix_remove_managed(m, cpumask_of(cpu));",
            "\t}",
            "\treturn -ENOSPC;",
            "}"
          ],
          "function_name": "irq_matrix_online, irq_matrix_offline, matrix_alloc_area, matrix_find_best_cpu, matrix_find_best_cpu_managed, irq_matrix_assign_system, irq_matrix_reserve_managed",
          "description": "实现CPU矩阵的上线/下线操作，通过bitmap操作实现中断位的分配策略，包含寻找最佳CPU的逻辑，支持系统位管理和保留区域的分配与追踪",
          "similarity": 0.5585302710533142
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 251,
          "end_line": 365,
          "content": [
            "void irq_matrix_remove_managed(struct irq_matrix *m, const struct cpumask *msk)",
            "{",
            "\tunsigned int cpu;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "\t\tunsigned int bit, end = m->alloc_end;",
            "",
            "\t\tif (WARN_ON_ONCE(!cm->managed))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Get managed bit which are not allocated */",
            "\t\tbitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);",
            "",
            "\t\tbit = find_first_bit(m->scratch_map, end);",
            "\t\tif (WARN_ON_ONCE(bit >= end))",
            "\t\t\tcontinue;",
            "",
            "\t\tclear_bit(bit, cm->managed_map);",
            "",
            "\t\tcm->managed--;",
            "\t\tif (cm->online) {",
            "\t\t\tcm->available++;",
            "\t\t\tm->global_available++;",
            "\t\t}",
            "\t\ttrace_irq_matrix_remove_managed(bit, cpu, m, cm);",
            "\t}",
            "}",
            "int irq_matrix_alloc_managed(struct irq_matrix *m, const struct cpumask *msk,",
            "\t\t\t     unsigned int *mapped_cpu)",
            "{",
            "\tunsigned int bit, cpu, end;",
            "\tstruct cpumap *cm;",
            "",
            "\tif (cpumask_empty(msk))",
            "\t\treturn -EINVAL;",
            "",
            "\tcpu = matrix_find_best_cpu_managed(m, msk);",
            "\tif (cpu == UINT_MAX)",
            "\t\treturn -ENOSPC;",
            "",
            "\tcm = per_cpu_ptr(m->maps, cpu);",
            "\tend = m->alloc_end;",
            "\t/* Get managed bit which are not allocated */",
            "\tbitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);",
            "\tbit = find_first_bit(m->scratch_map, end);",
            "\tif (bit >= end)",
            "\t\treturn -ENOSPC;",
            "\tset_bit(bit, cm->alloc_map);",
            "\tcm->allocated++;",
            "\tcm->managed_allocated++;",
            "\tm->total_allocated++;",
            "\t*mapped_cpu = cpu;",
            "\ttrace_irq_matrix_alloc_managed(bit, cpu, m, cm);",
            "\treturn bit;",
            "}",
            "void irq_matrix_assign(struct irq_matrix *m, unsigned int bit)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tif (WARN_ON_ONCE(bit < m->alloc_start || bit >= m->alloc_end))",
            "\t\treturn;",
            "\tif (WARN_ON_ONCE(test_and_set_bit(bit, cm->alloc_map)))",
            "\t\treturn;",
            "\tcm->allocated++;",
            "\tm->total_allocated++;",
            "\tcm->available--;",
            "\tm->global_available--;",
            "\ttrace_irq_matrix_assign(bit, smp_processor_id(), m, cm);",
            "}",
            "void irq_matrix_reserve(struct irq_matrix *m)",
            "{",
            "\tif (m->global_reserved == m->global_available)",
            "\t\tpr_warn(\"Interrupt reservation exceeds available resources\\n\");",
            "",
            "\tm->global_reserved++;",
            "\ttrace_irq_matrix_reserve(m);",
            "}",
            "void irq_matrix_remove_reserved(struct irq_matrix *m)",
            "{",
            "\tm->global_reserved--;",
            "\ttrace_irq_matrix_remove_reserved(m);",
            "}",
            "int irq_matrix_alloc(struct irq_matrix *m, const struct cpumask *msk,",
            "\t\t     bool reserved, unsigned int *mapped_cpu)",
            "{",
            "\tunsigned int cpu, bit;",
            "\tstruct cpumap *cm;",
            "",
            "\t/*",
            "\t * Not required in theory, but matrix_find_best_cpu() uses",
            "\t * for_each_cpu() which ignores the cpumask on UP .",
            "\t */",
            "\tif (cpumask_empty(msk))",
            "\t\treturn -EINVAL;",
            "",
            "\tcpu = matrix_find_best_cpu(m, msk);",
            "\tif (cpu == UINT_MAX)",
            "\t\treturn -ENOSPC;",
            "",
            "\tcm = per_cpu_ptr(m->maps, cpu);",
            "\tbit = matrix_alloc_area(m, cm, 1, false);",
            "\tif (bit >= m->alloc_end)",
            "\t\treturn -ENOSPC;",
            "\tcm->allocated++;",
            "\tcm->available--;",
            "\tm->total_allocated++;",
            "\tm->global_available--;",
            "\tif (reserved)",
            "\t\tm->global_reserved--;",
            "\t*mapped_cpu = cpu;",
            "\ttrace_irq_matrix_alloc(bit, cpu, m, cm);",
            "\treturn bit;",
            "",
            "}"
          ],
          "function_name": "irq_matrix_remove_managed, irq_matrix_alloc_managed, irq_matrix_assign, irq_matrix_reserve, irq_matrix_remove_reserved, irq_matrix_alloc",
          "description": "实现中断位的分配/回收机制，包含保留中断位的管理、跨CPU的中断分配逻辑，以及根据预留状态进行资源分配的控制流程",
          "similarity": 0.5515952110290527
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 1,
          "end_line": 77,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "// Copyright (C) 2017 Thomas Gleixner <tglx@linutronix.de>",
            "",
            "#include <linux/spinlock.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/bitmap.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpu.h>",
            "#include <linux/irq.h>",
            "",
            "#define IRQ_MATRIX_SIZE\t(BITS_TO_LONGS(IRQ_MATRIX_BITS))",
            "",
            "struct cpumap {",
            "\tunsigned int\t\tavailable;",
            "\tunsigned int\t\tallocated;",
            "\tunsigned int\t\tmanaged;",
            "\tunsigned int\t\tmanaged_allocated;",
            "\tbool\t\t\tinitialized;",
            "\tbool\t\t\tonline;",
            "\tunsigned long\t\talloc_map[IRQ_MATRIX_SIZE];",
            "\tunsigned long\t\tmanaged_map[IRQ_MATRIX_SIZE];",
            "};",
            "",
            "struct irq_matrix {",
            "\tunsigned int\t\tmatrix_bits;",
            "\tunsigned int\t\talloc_start;",
            "\tunsigned int\t\talloc_end;",
            "\tunsigned int\t\talloc_size;",
            "\tunsigned int\t\tglobal_available;",
            "\tunsigned int\t\tglobal_reserved;",
            "\tunsigned int\t\tsystembits_inalloc;",
            "\tunsigned int\t\ttotal_allocated;",
            "\tunsigned int\t\tonline_maps;",
            "\tstruct cpumap __percpu\t*maps;",
            "\tunsigned long\t\tscratch_map[IRQ_MATRIX_SIZE];",
            "\tunsigned long\t\tsystem_map[IRQ_MATRIX_SIZE];",
            "};",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/irq_matrix.h>",
            "",
            "/**",
            " * irq_alloc_matrix - Allocate a irq_matrix structure and initialize it",
            " * @matrix_bits:\tNumber of matrix bits must be <= IRQ_MATRIX_BITS",
            " * @alloc_start:\tFrom which bit the allocation search starts",
            " * @alloc_end:\t\tAt which bit the allocation search ends, i.e first",
            " *\t\t\tinvalid bit",
            " */",
            "__init struct irq_matrix *irq_alloc_matrix(unsigned int matrix_bits,",
            "\t\t\t\t\t   unsigned int alloc_start,",
            "\t\t\t\t\t   unsigned int alloc_end)",
            "{",
            "\tstruct irq_matrix *m;",
            "",
            "\tif (matrix_bits > IRQ_MATRIX_BITS)",
            "\t\treturn NULL;",
            "",
            "\tm = kzalloc(sizeof(*m), GFP_KERNEL);",
            "\tif (!m)",
            "\t\treturn NULL;",
            "",
            "\tm->matrix_bits = matrix_bits;",
            "\tm->alloc_start = alloc_start;",
            "\tm->alloc_end = alloc_end;",
            "\tm->alloc_size = alloc_end - alloc_start;",
            "\tm->maps = alloc_percpu(*m->maps);",
            "\tif (!m->maps) {",
            "\t\tkfree(m);",
            "\t\treturn NULL;",
            "\t}",
            "\treturn m;",
            "}",
            "",
            "/**",
            " * irq_matrix_online - Bring the local CPU matrix online",
            " * @m:\t\tMatrix pointer",
            " */"
          ],
          "function_name": null,
          "description": "定义irq_matrix结构体和相关辅助数据结构，提供irq_alloc_matrix函数用于初始化并分配irq_matrix实例，设置矩阵大小、起始结束位置等参数，并分配per-CPU的cpumap数组",
          "similarity": 0.4289337992668152
        }
      ]
    },
    {
      "source_file": "kernel/locking/ww_mutex.h",
      "md_summary": "> 自动生成时间: 2025-10-25 14:56:40\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\ww_mutex.h`\n\n---\n\n# `locking/ww_mutex.h` 技术文档\n\n## 1. 文件概述\n\n`ww_mutex.h` 是 Linux 内核中用于实现 **Wound-Wait (WW) 互斥锁**（`ww_mutex`）的头文件。该机制主要用于解决 **死锁问题**，特别是在图形子系统（如 DRM/KMS）和资源管理场景中，多个事务（transactions）需要以特定顺序获取多个锁时。  \nWW 互斥锁通过为每个锁请求关联一个 **获取上下文**（`ww_acquire_ctx`），并基于事务的优先级或时间戳实现 **Wait-Die** 或 **Wound-Wait** 死锁避免策略。\n\n该文件通过条件编译（`WW_RT` 宏）支持两种底层锁实现：\n- **普通互斥锁**（`mutex`）：用于非实时（non-RT）内核配置。\n- **实时互斥锁**（`rt_mutex`）：用于实时（RT）内核补丁配置，支持优先级继承。\n\n## 2. 核心功能\n\n### 2.1 主要宏定义\n- `MUTEX` / `MUTEX_WAITER`：根据 `WW_RT` 宏分别映射到 `mutex`/`rt_mutex` 及其等待者结构。\n\n### 2.2 等待者链表/红黑树操作函数（抽象接口）\n- `__ww_waiter_first()`：获取等待队列中的第一个等待者。\n- `__ww_waiter_next()` / `__ww_waiter_prev()`：获取下一个/上一个等待者。\n- `__ww_waiter_last()`：获取等待队列中的最后一个等待者。\n- `__ww_waiter_add()`：将等待者插入到指定位置（普通 mutex 使用链表，RT 使用红黑树）。\n\n### 2.3 锁状态查询函数\n- `__ww_mutex_owner()`：获取当前锁的持有者任务。\n- `__ww_mutex_has_waiters()`：检查锁是否有等待者。\n- `lock_wait_lock()` / `unlock_wait_lock()`：获取/释放锁的等待队列自旋锁（`wait_lock`）。\n- `lockdep_assert_wait_lock_held()`：调试时断言 `wait_lock` 已被持有。\n\n### 2.4 WW 互斥锁核心逻辑函数\n- `ww_mutex_lock_acquired()`：在成功获取 `ww_mutex` 后，将其与获取上下文（`ww_ctx`）关联，并执行调试检查。\n- `__ww_ctx_less()`：比较两个获取上下文的优先级（用于决定谁应“等待”或“死亡/被抢占”）。\n- `__ww_mutex_die()`：**Wait-Die 策略**实现：若当前请求者（新事务）发现等待队列中有更老的事务持有其他锁，则唤醒该老事务使其“死亡”（回滚）。\n- `__ww_mutex_wound()`：**Wound-Wait 策略**实现：若当前请求者（老事务）发现锁持有者是更年轻的事务，则“刺伤”（标记 `wounded=1`）该年轻事务，迫使其回滚。\n\n## 3. 关键实现\n\n### 3.1 死锁避免策略\n- **Wait-Die**（`is_wait_die=1`）：\n  - **新事务**请求**老事务**持有的锁 → **新事务等待**。\n  - **新事务**请求**老事务**等待的锁 → **新事务死亡**（回滚）。\n- **Wound-Wait**（`is_wait_die=0`）：\n  - **老事务**请求**新事务**持有的锁 → **新事务被刺伤**（回滚）。\n  - **老事务**请求**新事务**等待的锁 → **老事务等待**。\n\n### 3.2 上下文比较 (`__ww_ctx_less`)\n- **非 RT 模式**：仅基于时间戳（`stamp`），值越大表示事务越新。\n- **RT 模式**：\n  1. 优先比较 **实时优先级**（`prio`），数值越小优先级越高。\n  2. 若均为 **Deadline 调度类**，比较 **截止时间**（`deadline`），越早截止优先级越高。\n  3. 若优先级相同，回退到时间戳比较。\n\n### 3.3 RT 与非 RT 差异\n- **数据结构**：\n  - 非 RT：等待者使用 **双向链表**（`list_head`）。\n  - RT：等待者使用 **红黑树**（`rb_root`），按优先级排序。\n- **插入逻辑**：\n  - 非 RT：`__ww_waiter_add` 显式插入到指定位置。\n  - RT：`__ww_waiter_add` 为空（RT 互斥锁内部自动处理插入）。\n\n### 3.4 调试支持 (`DEBUG_WW_MUTEXES`)\n- 检查 `ww_mutex` 是否被错误地用普通 `mutex_unlock` 释放。\n- 验证上下文一致性（如 `ww_class` 匹配、`contending_lock` 状态等）。\n\n## 4. 依赖关系\n\n- **基础锁机制**：\n  - 非 RT 模式依赖 `<linux/mutex.h>`。\n  - RT 模式依赖 `<linux/rtmutex.h>`。\n- **调度器**：依赖任务结构（`task_struct`）、优先级（`prio`）、调度类（如 `dl_prio`）。\n- **调试框架**：依赖 `lockdep`（`lockdep_assert_held`）和 `DEBUG_LOCKS_WARN_ON`。\n- **原子操作**：使用 `atomic_long_read` 检查锁状态标志（`MUTEX_FLAG_WAITERS`）。\n\n## 5. 使用场景\n\n- **图形子系统**（DRM/KMS）：  \n  多个 GPU 作业（如渲染、合成）需按顺序获取多个缓冲区（buffer）或 CRTC 锁，避免死锁。\n- **资源分配器**：  \n  当多个客户端竞争一组有限资源（如内存区域、I/O 端口）时，通过 WW 互斥锁确保无死锁的分配顺序。\n- **实时系统**（RT 补丁）：  \n  在需要确定性延迟的场景中，结合优先级继承（PI）避免优先级反转，同时通过 WW 策略解决多锁死锁。\n- **文件系统**：  \n  某些文件系统（如 Btrfs）在元数据操作中使用 WW 互斥锁管理多个 extent 锁。",
      "similarity": 0.5582629442214966,
      "chunks": []
    },
    {
      "source_file": "mm/memcontrol-v1.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:38:55\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memcontrol-v1.c`\n\n---\n\n# memcontrol-v1.c 技术文档\n\n## 1. 文件概述\n\n`memcontrol-v1.c` 是 Linux 内核内存控制组（Memory Cgroup）v1 接口的核心实现文件之一，主要负责基于软限制（soft limit）的内存回收机制、OOM 事件通知以及与 cgroup v1 兼容的资源统计和管理功能。该文件维护了一个独立于 cgroup 层级结构的红黑树（RB-Tree），用于高效地追踪和选择超出软限制最多的内存控制组进行内存回收。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct mem_cgroup_tree_per_node`**  \n  每个 NUMA 节点对应的红黑树结构，用于存储超出软限制的 `mem_cgroup_per_node` 实例。\n  - `rb_root`: 红黑树根节点\n  - `rb_rightmost`: 指向使用量超出软限制最多的节点（树中最右侧节点）\n  - `lock`: 保护该树的自旋锁\n\n- **`struct mem_cgroup_tree`**  \n  全局软限制树结构，包含每个 NUMA 节点对应的 `mem_cgroup_tree_per_node`。\n\n- **`struct mem_cgroup_eventfd_list`**  \n  用于 OOM 事件通知的 eventfd 列表项。\n\n- **`struct mem_cgroup_event`**  \n  表示用户空间注册的内存事件（如 OOM、阈值触发等），支持通过 eventfd 通知用户空间。\n\n- **枚举常量 `RES_*`**  \n  定义了 cgroup v1 接口中可读写的资源属性类型（如使用量、限制、最大使用量、失败计数、软限制等）。\n\n### 主要函数\n\n- **`__mem_cgroup_insert_exceeded()` / `__mem_cgroup_remove_exceeded()`**  \n  在指定节点的软限制红黑树中插入或移除一个 `mem_cgroup_per_node` 节点。\n\n- **`memcg1_update_tree()`**  \n  根据当前内存使用量与软限制的差值，更新指定 memcg 及其所有祖先在软限制树中的位置。\n\n- **`memcg1_remove_from_trees()`**  \n  在 memcg 销毁时，将其从所有 NUMA 节点的软限制树中移除。\n\n- **`mem_cgroup_largest_soft_limit_node()`**  \n  从指定节点的软限制树中找出超出软限制最多的 memcg 节点，用于优先回收。\n\n- **`mem_cgroup_soft_reclaim()`**  \n  对指定 memcg 层级结构执行软限制驱动的内存回收。\n\n- **`memcg1_soft_limit_reclaim()`**（未完整显示）  \n  全局软限制回收入口函数，由内存短缺路径调用，尝试从超出软限制的 memcg 中回收内存。\n\n## 3. 关键实现\n\n### 软限制红黑树机制\n\n- 所有超出软限制（`memory.usage > soft_limit`）的 `mem_cgroup_per_node` 实例被组织到 per-NUMA-node 的红黑树中。\n- 树按 `usage_in_excess = usage - soft_limit` 升序排列，最右侧节点即为超出最多的 memcg。\n- 当 memcg 的内存使用量变化或软限制被修改时，调用 `memcg1_update_tree()` 更新其在树中的位置（先删除再重新插入）。\n- 回收时优先选择 `rb_rightmost` 节点，确保优先回收“最违规”的 memcg。\n\n### 层级遍历与祖先更新\n\n- 在启用 cgroup 层级模式时，子 memcg 的内存使用会影响父 memcg 的统计。\n- 因此，当子 memcg 的使用量变化时，需向上遍历所有祖先，更新它们在软限制树中的状态。\n\n### 防止无限循环的回收控制\n\n- `MEM_CGROUP_MAX_RECLAIM_LOOPS`（100）和 `MEM_CGROUP_MAX_SOFT_LIMIT_RECLAIM_LOOPS`（2）用于限制回收循环次数。\n- 若一轮遍历未回收足够内存（`total < excess >> 2`），最多再尝试一次。\n\n### 与 LRU_GEN 的集成\n\n- 若启用了多代 LRU（`lru_gen_enabled()`），则绕过红黑树机制，直接调用 `lru_gen_soft_reclaim()` 进行软限制回收。\n\n### 事件通知机制\n\n- 支持通过 `eventfd` 向用户空间发送 OOM 或其他内存事件通知。\n- 使用 `poll_table` 和 `wait_queue` 实现 eventfd 的自动注销（当 fd 关闭时）。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/memcontrol.h>`：内存控制组核心接口\n  - `<linux/swap.h>`, `\"swap.h\"`：交换子系统支持\n  - `<linux/eventfd.h>`, `<linux/poll.h>`：事件通知机制\n  - `\"internal.h\"`：内核内存管理内部接口\n\n- **功能依赖**：\n  - 依赖 `page_counter` 子系统进行内存使用量统计\n  - 依赖 `mem_cgroup_iter()` 实现层级遍历\n  - 依赖 `mem_cgroup_shrink_node()` 执行实际页面回收\n  - 可选依赖 `lru_gen` 多代 LRU 回收器\n\n- **配置依赖**：\n  - `CONFIG_MEMCG`：必须启用内存 cgroup\n  - `CONFIG_LOCKDEP`：仅在调试时定义锁依赖映射\n\n## 5. 使用场景\n\n- **内存压力下的软限制回收**：当系统内存紧张时，`kswapd` 或直接回收路径会调用 `memcg1_soft_limit_reclaim()`，优先从超出软限制的 memcg 中回收内存，以维持服务质量（QoS）。\n- **cgroup v1 接口兼容**：为 `/sys/fs/cgroup/memory/` 下的 `memory.soft_limit_in_bytes` 等文件提供后端支持。\n- **OOM 事件通知**：当 memcg 触发 OOM 时，通过预先注册的 eventfd 向用户空间守护进程（如容器运行时）发送通知。\n- **动态资源调整**：当用户通过写入 `memory.soft_limit_in_bytes` 修改软限制时，触发 `memcg1_update_tree()` 更新红黑树结构。\n- **memcg 销毁清理**：在 cgroup 被删除时，确保其从所有软限制树中正确移除，防止悬挂指针。",
      "similarity": 0.5295571684837341,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "mm/memcontrol-v1.c",
          "start_line": 404,
          "end_line": 520,
          "content": [
            "static u64 mem_cgroup_move_charge_read(struct cgroup_subsys_state *css,",
            "\t\t\t\tstruct cftype *cft)",
            "{",
            "\treturn 0;",
            "}",
            "static int mem_cgroup_move_charge_write(struct cgroup_subsys_state *css,",
            "\t\t\t\t struct cftype *cft, u64 val)",
            "{",
            "\tpr_warn_once(\"Cgroup memory moving (move_charge_at_immigrate) is deprecated. \"",
            "\t\t     \"Please report your usecase to linux-mm@kvack.org if you \"",
            "\t\t     \"depend on this functionality.\\n\");",
            "",
            "\tif (val != 0)",
            "\t\treturn -EINVAL;",
            "\treturn 0;",
            "}",
            "static int mem_cgroup_move_charge_write(struct cgroup_subsys_state *css,",
            "\t\t\t\t struct cftype *cft, u64 val)",
            "{",
            "\treturn -ENOSYS;",
            "}",
            "static void __mem_cgroup_threshold(struct mem_cgroup *memcg, bool swap)",
            "{",
            "\tstruct mem_cgroup_threshold_ary *t;",
            "\tunsigned long usage;",
            "\tint i;",
            "",
            "\trcu_read_lock();",
            "\tif (!swap)",
            "\t\tt = rcu_dereference(memcg->thresholds.primary);",
            "\telse",
            "\t\tt = rcu_dereference(memcg->memsw_thresholds.primary);",
            "",
            "\tif (!t)",
            "\t\tgoto unlock;",
            "",
            "\tusage = mem_cgroup_usage(memcg, swap);",
            "",
            "\t/*",
            "\t * current_threshold points to threshold just below or equal to usage.",
            "\t * If it's not true, a threshold was crossed after last",
            "\t * call of __mem_cgroup_threshold().",
            "\t */",
            "\ti = t->current_threshold;",
            "",
            "\t/*",
            "\t * Iterate backward over array of thresholds starting from",
            "\t * current_threshold and check if a threshold is crossed.",
            "\t * If none of thresholds below usage is crossed, we read",
            "\t * only one element of the array here.",
            "\t */",
            "\tfor (; i >= 0 && unlikely(t->entries[i].threshold > usage); i--)",
            "\t\teventfd_signal(t->entries[i].eventfd);",
            "",
            "\t/* i = current_threshold + 1 */",
            "\ti++;",
            "",
            "\t/*",
            "\t * Iterate forward over array of thresholds starting from",
            "\t * current_threshold+1 and check if a threshold is crossed.",
            "\t * If none of thresholds above usage is crossed, we read",
            "\t * only one element of the array here.",
            "\t */",
            "\tfor (; i < t->size && unlikely(t->entries[i].threshold <= usage); i++)",
            "\t\teventfd_signal(t->entries[i].eventfd);",
            "",
            "\t/* Update current_threshold */",
            "\tt->current_threshold = i - 1;",
            "unlock:",
            "\trcu_read_unlock();",
            "}",
            "static void mem_cgroup_threshold(struct mem_cgroup *memcg)",
            "{",
            "\twhile (memcg) {",
            "\t\t__mem_cgroup_threshold(memcg, false);",
            "\t\tif (do_memsw_account())",
            "\t\t\t__mem_cgroup_threshold(memcg, true);",
            "",
            "\t\tmemcg = parent_mem_cgroup(memcg);",
            "\t}",
            "}",
            "static void memcg1_charge_statistics(struct mem_cgroup *memcg, int nr_pages)",
            "{",
            "\t/* pagein of a big page is an event. So, ignore page size */",
            "\tif (nr_pages > 0)",
            "\t\t__count_memcg_events(memcg, PGPGIN, 1);",
            "\telse {",
            "\t\t__count_memcg_events(memcg, PGPGOUT, 1);",
            "\t\tnr_pages = -nr_pages; /* for event */",
            "\t}",
            "",
            "\t__this_cpu_add(memcg->events_percpu->nr_page_events, nr_pages);",
            "}",
            "static bool memcg1_event_ratelimit(struct mem_cgroup *memcg,",
            "\t\t\t\tenum mem_cgroup_events_target target)",
            "{",
            "\tunsigned long val, next;",
            "",
            "\tval = __this_cpu_read(memcg->events_percpu->nr_page_events);",
            "\tnext = __this_cpu_read(memcg->events_percpu->targets[target]);",
            "\t/* from time_after() in jiffies.h */",
            "\tif ((long)(next - val) < 0) {",
            "\t\tswitch (target) {",
            "\t\tcase MEM_CGROUP_TARGET_THRESH:",
            "\t\t\tnext = val + THRESHOLDS_EVENTS_TARGET;",
            "\t\t\tbreak;",
            "\t\tcase MEM_CGROUP_TARGET_SOFTLIMIT:",
            "\t\t\tnext = val + SOFTLIMIT_EVENTS_TARGET;",
            "\t\t\tbreak;",
            "\t\tdefault:",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\t__this_cpu_write(memcg->events_percpu->targets[target], next);",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}"
          ],
          "function_name": "mem_cgroup_move_charge_read, mem_cgroup_move_charge_write, mem_cgroup_move_charge_write, __mem_cgroup_threshold, mem_cgroup_threshold, memcg1_charge_statistics, memcg1_event_ratelimit",
          "description": "实现内存使用阈值监测与事件触发机制，包含阈值比较排序、事件信号发送及页面事件统计功能，通过事件限速机制控制通知频率，支持软限制和交换空间双重阈值监控。",
          "similarity": 0.5658770799636841
        },
        {
          "chunk_id": 0,
          "file_path": "mm/memcontrol-v1.c",
          "start_line": 1,
          "end_line": 108,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "",
            "#include <linux/memcontrol.h>",
            "#include <linux/swap.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/pagewalk.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/swap_cgroup.h>",
            "#include <linux/eventfd.h>",
            "#include <linux/poll.h>",
            "#include <linux/sort.h>",
            "#include <linux/file.h>",
            "#include <linux/seq_buf.h>",
            "",
            "#include \"internal.h\"",
            "#include \"swap.h\"",
            "#include \"memcontrol-v1.h\"",
            "",
            "/*",
            " * Cgroups above their limits are maintained in a RB-Tree, independent of",
            " * their hierarchy representation",
            " */",
            "",
            "struct mem_cgroup_tree_per_node {",
            "\tstruct rb_root rb_root;",
            "\tstruct rb_node *rb_rightmost;",
            "\tspinlock_t lock;",
            "};",
            "",
            "struct mem_cgroup_tree {",
            "\tstruct mem_cgroup_tree_per_node *rb_tree_per_node[MAX_NUMNODES];",
            "};",
            "",
            "static struct mem_cgroup_tree soft_limit_tree __read_mostly;",
            "",
            "/*",
            " * Maximum loops in mem_cgroup_soft_reclaim(), used for soft",
            " * limit reclaim to prevent infinite loops, if they ever occur.",
            " */",
            "#define\tMEM_CGROUP_MAX_RECLAIM_LOOPS\t\t100",
            "#define\tMEM_CGROUP_MAX_SOFT_LIMIT_RECLAIM_LOOPS\t2",
            "",
            "/* for OOM */",
            "struct mem_cgroup_eventfd_list {",
            "\tstruct list_head list;",
            "\tstruct eventfd_ctx *eventfd;",
            "};",
            "",
            "/*",
            " * cgroup_event represents events which userspace want to receive.",
            " */",
            "struct mem_cgroup_event {",
            "\t/*",
            "\t * memcg which the event belongs to.",
            "\t */",
            "\tstruct mem_cgroup *memcg;",
            "\t/*",
            "\t * eventfd to signal userspace about the event.",
            "\t */",
            "\tstruct eventfd_ctx *eventfd;",
            "\t/*",
            "\t * Each of these stored in a list by the cgroup.",
            "\t */",
            "\tstruct list_head list;",
            "\t/*",
            "\t * register_event() callback will be used to add new userspace",
            "\t * waiter for changes related to this event.  Use eventfd_signal()",
            "\t * on eventfd to send notification to userspace.",
            "\t */",
            "\tint (*register_event)(struct mem_cgroup *memcg,",
            "\t\t\t      struct eventfd_ctx *eventfd, const char *args);",
            "\t/*",
            "\t * unregister_event() callback will be called when userspace closes",
            "\t * the eventfd or on cgroup removing.  This callback must be set,",
            "\t * if you want provide notification functionality.",
            "\t */",
            "\tvoid (*unregister_event)(struct mem_cgroup *memcg,",
            "\t\t\t\t struct eventfd_ctx *eventfd);",
            "\t/*",
            "\t * All fields below needed to unregister event when",
            "\t * userspace closes eventfd.",
            "\t */",
            "\tpoll_table pt;",
            "\twait_queue_head_t *wqh;",
            "\twait_queue_entry_t wait;",
            "\tstruct work_struct remove;",
            "};",
            "",
            "#define MEMFILE_PRIVATE(x, val)\t((x) << 16 | (val))",
            "#define MEMFILE_TYPE(val)\t((val) >> 16 & 0xffff)",
            "#define MEMFILE_ATTR(val)\t((val) & 0xffff)",
            "",
            "enum {",
            "\tRES_USAGE,",
            "\tRES_LIMIT,",
            "\tRES_MAX_USAGE,",
            "\tRES_FAILCNT,",
            "\tRES_SOFT_LIMIT,",
            "};",
            "",
            "#ifdef CONFIG_LOCKDEP",
            "static struct lockdep_map memcg_oom_lock_dep_map = {",
            "\t.name = \"memcg_oom_lock\",",
            "};",
            "#endif",
            "",
            "DEFINE_SPINLOCK(memcg_oom_lock);",
            ""
          ],
          "function_name": null,
          "description": "定义内存控制组相关数据结构，包括基于节点的RB树结构和软限制树，用于跟踪超过内存限制的cgroup节点，提供软限制阈值管理和事件通知机制。",
          "similarity": 0.5556977987289429
        },
        {
          "chunk_id": 4,
          "file_path": "mm/memcontrol-v1.c",
          "start_line": 537,
          "end_line": 698,
          "content": [
            "static void memcg1_check_events(struct mem_cgroup *memcg, int nid)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\treturn;",
            "",
            "\t/* threshold event is triggered in finer grain than soft limit */",
            "\tif (unlikely(memcg1_event_ratelimit(memcg,",
            "\t\t\t\t\t\tMEM_CGROUP_TARGET_THRESH))) {",
            "\t\tbool do_softlimit;",
            "",
            "\t\tdo_softlimit = memcg1_event_ratelimit(memcg,",
            "\t\t\t\t\t\tMEM_CGROUP_TARGET_SOFTLIMIT);",
            "\t\tmem_cgroup_threshold(memcg);",
            "\t\tif (unlikely(do_softlimit))",
            "\t\t\tmemcg1_update_tree(memcg, nid);",
            "\t}",
            "}",
            "void memcg1_commit_charge(struct folio *folio, struct mem_cgroup *memcg)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\tmemcg1_charge_statistics(memcg, folio_nr_pages(folio));",
            "\tmemcg1_check_events(memcg, folio_nid(folio));",
            "\tlocal_irq_restore(flags);",
            "}",
            "void memcg1_swapout(struct folio *folio, struct mem_cgroup *memcg)",
            "{",
            "\t/*",
            "\t * Interrupts should be disabled here because the caller holds the",
            "\t * i_pages lock which is taken with interrupts-off. It is",
            "\t * important here to have the interrupts disabled because it is the",
            "\t * only synchronisation we have for updating the per-CPU variables.",
            "\t */",
            "\tpreempt_disable_nested();",
            "\tVM_WARN_ON_IRQS_ENABLED();",
            "\tmemcg1_charge_statistics(memcg, -folio_nr_pages(folio));",
            "\tpreempt_enable_nested();",
            "\tmemcg1_check_events(memcg, folio_nid(folio));",
            "}",
            "void memcg1_uncharge_batch(struct mem_cgroup *memcg, unsigned long pgpgout,",
            "\t\t\t   unsigned long nr_memory, int nid)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\t__count_memcg_events(memcg, PGPGOUT, pgpgout);",
            "\t__this_cpu_add(memcg->events_percpu->nr_page_events, nr_memory);",
            "\tmemcg1_check_events(memcg, nid);",
            "\tlocal_irq_restore(flags);",
            "}",
            "static int compare_thresholds(const void *a, const void *b)",
            "{",
            "\tconst struct mem_cgroup_threshold *_a = a;",
            "\tconst struct mem_cgroup_threshold *_b = b;",
            "",
            "\tif (_a->threshold > _b->threshold)",
            "\t\treturn 1;",
            "",
            "\tif (_a->threshold < _b->threshold)",
            "\t\treturn -1;",
            "",
            "\treturn 0;",
            "}",
            "static int mem_cgroup_oom_notify_cb(struct mem_cgroup *memcg)",
            "{",
            "\tstruct mem_cgroup_eventfd_list *ev;",
            "",
            "\tspin_lock(&memcg_oom_lock);",
            "",
            "\tlist_for_each_entry(ev, &memcg->oom_notify, list)",
            "\t\teventfd_signal(ev->eventfd);",
            "",
            "\tspin_unlock(&memcg_oom_lock);",
            "\treturn 0;",
            "}",
            "static void mem_cgroup_oom_notify(struct mem_cgroup *memcg)",
            "{",
            "\tstruct mem_cgroup *iter;",
            "",
            "\tfor_each_mem_cgroup_tree(iter, memcg)",
            "\t\tmem_cgroup_oom_notify_cb(iter);",
            "}",
            "static int __mem_cgroup_usage_register_event(struct mem_cgroup *memcg,",
            "\tstruct eventfd_ctx *eventfd, const char *args, enum res_type type)",
            "{",
            "\tstruct mem_cgroup_thresholds *thresholds;",
            "\tstruct mem_cgroup_threshold_ary *new;",
            "\tunsigned long threshold;",
            "\tunsigned long usage;",
            "\tint i, size, ret;",
            "",
            "\tret = page_counter_memparse(args, \"-1\", &threshold);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tmutex_lock(&memcg->thresholds_lock);",
            "",
            "\tif (type == _MEM) {",
            "\t\tthresholds = &memcg->thresholds;",
            "\t\tusage = mem_cgroup_usage(memcg, false);",
            "\t} else if (type == _MEMSWAP) {",
            "\t\tthresholds = &memcg->memsw_thresholds;",
            "\t\tusage = mem_cgroup_usage(memcg, true);",
            "\t} else",
            "\t\tBUG();",
            "",
            "\t/* Check if a threshold crossed before adding a new one */",
            "\tif (thresholds->primary)",
            "\t\t__mem_cgroup_threshold(memcg, type == _MEMSWAP);",
            "",
            "\tsize = thresholds->primary ? thresholds->primary->size + 1 : 1;",
            "",
            "\t/* Allocate memory for new array of thresholds */",
            "\tnew = kmalloc(struct_size(new, entries, size), GFP_KERNEL);",
            "\tif (!new) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto unlock;",
            "\t}",
            "\tnew->size = size;",
            "",
            "\t/* Copy thresholds (if any) to new array */",
            "\tif (thresholds->primary)",
            "\t\tmemcpy(new->entries, thresholds->primary->entries,",
            "\t\t       flex_array_size(new, entries, size - 1));",
            "",
            "\t/* Add new threshold */",
            "\tnew->entries[size - 1].eventfd = eventfd;",
            "\tnew->entries[size - 1].threshold = threshold;",
            "",
            "\t/* Sort thresholds. Registering of new threshold isn't time-critical */",
            "\tsort(new->entries, size, sizeof(*new->entries),",
            "\t\t\tcompare_thresholds, NULL);",
            "",
            "\t/* Find current threshold */",
            "\tnew->current_threshold = -1;",
            "\tfor (i = 0; i < size; i++) {",
            "\t\tif (new->entries[i].threshold <= usage) {",
            "\t\t\t/*",
            "\t\t\t * new->current_threshold will not be used until",
            "\t\t\t * rcu_assign_pointer(), so it's safe to increment",
            "\t\t\t * it here.",
            "\t\t\t */",
            "\t\t\t++new->current_threshold;",
            "\t\t} else",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\t/* Free old spare buffer and save old primary buffer as spare */",
            "\tkfree(thresholds->spare);",
            "\tthresholds->spare = thresholds->primary;",
            "",
            "\trcu_assign_pointer(thresholds->primary, new);",
            "",
            "\t/* To be sure that nobody uses thresholds */",
            "\tsynchronize_rcu();",
            "",
            "unlock:",
            "\tmutex_unlock(&memcg->thresholds_lock);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "memcg1_check_events, memcg1_commit_charge, memcg1_swapout, memcg1_uncharge_batch, compare_thresholds, mem_cgroup_oom_notify_cb, mem_cgroup_oom_notify, __mem_cgroup_usage_register_event",
          "description": "集成内存使用事件检测与通知系统，包含页面计数更新、事件触发检查、OOM通知传播等功能，通过RCU机制安全更新阈值数组，并处理内存分配/释放时的统计与监控任务。",
          "similarity": 0.5291523933410645
        },
        {
          "chunk_id": 7,
          "file_path": "mm/memcontrol-v1.c",
          "start_line": 1095,
          "end_line": 1203,
          "content": [
            "void memcg1_memcg_init(struct mem_cgroup *memcg)",
            "{",
            "\tINIT_LIST_HEAD(&memcg->oom_notify);",
            "\tmutex_init(&memcg->thresholds_lock);",
            "\tINIT_LIST_HEAD(&memcg->event_list);",
            "\tspin_lock_init(&memcg->event_list_lock);",
            "}",
            "void memcg1_css_offline(struct mem_cgroup *memcg)",
            "{",
            "\tstruct mem_cgroup_event *event, *tmp;",
            "",
            "\t/*",
            "\t * Unregister events and notify userspace.",
            "\t * Notify userspace about cgroup removing only after rmdir of cgroup",
            "\t * directory to avoid race between userspace and kernelspace.",
            "\t */",
            "\tspin_lock_irq(&memcg->event_list_lock);",
            "\tlist_for_each_entry_safe(event, tmp, &memcg->event_list, list) {",
            "\t\tlist_del_init(&event->list);",
            "\t\tschedule_work(&event->remove);",
            "\t}",
            "\tspin_unlock_irq(&memcg->event_list_lock);",
            "}",
            "static bool mem_cgroup_oom_trylock(struct mem_cgroup *memcg)",
            "{",
            "\tstruct mem_cgroup *iter, *failed = NULL;",
            "",
            "\tspin_lock(&memcg_oom_lock);",
            "",
            "\tfor_each_mem_cgroup_tree(iter, memcg) {",
            "\t\tif (iter->oom_lock) {",
            "\t\t\t/*",
            "\t\t\t * this subtree of our hierarchy is already locked",
            "\t\t\t * so we cannot give a lock.",
            "\t\t\t */",
            "\t\t\tfailed = iter;",
            "\t\t\tmem_cgroup_iter_break(memcg, iter);",
            "\t\t\tbreak;",
            "\t\t} else",
            "\t\t\titer->oom_lock = true;",
            "\t}",
            "",
            "\tif (failed) {",
            "\t\t/*",
            "\t\t * OK, we failed to lock the whole subtree so we have",
            "\t\t * to clean up what we set up to the failing subtree",
            "\t\t */",
            "\t\tfor_each_mem_cgroup_tree(iter, memcg) {",
            "\t\t\tif (iter == failed) {",
            "\t\t\t\tmem_cgroup_iter_break(memcg, iter);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t\titer->oom_lock = false;",
            "\t\t}",
            "\t} else",
            "\t\tmutex_acquire(&memcg_oom_lock_dep_map, 0, 1, _RET_IP_);",
            "",
            "\tspin_unlock(&memcg_oom_lock);",
            "",
            "\treturn !failed;",
            "}",
            "static void mem_cgroup_oom_unlock(struct mem_cgroup *memcg)",
            "{",
            "\tstruct mem_cgroup *iter;",
            "",
            "\tspin_lock(&memcg_oom_lock);",
            "\tmutex_release(&memcg_oom_lock_dep_map, _RET_IP_);",
            "\tfor_each_mem_cgroup_tree(iter, memcg)",
            "\t\titer->oom_lock = false;",
            "\tspin_unlock(&memcg_oom_lock);",
            "}",
            "static void mem_cgroup_mark_under_oom(struct mem_cgroup *memcg)",
            "{",
            "\tstruct mem_cgroup *iter;",
            "",
            "\tspin_lock(&memcg_oom_lock);",
            "\tfor_each_mem_cgroup_tree(iter, memcg)",
            "\t\titer->under_oom++;",
            "\tspin_unlock(&memcg_oom_lock);",
            "}",
            "static void mem_cgroup_unmark_under_oom(struct mem_cgroup *memcg)",
            "{",
            "\tstruct mem_cgroup *iter;",
            "",
            "\t/*",
            "\t * Be careful about under_oom underflows because a child memcg",
            "\t * could have been added after mem_cgroup_mark_under_oom.",
            "\t */",
            "\tspin_lock(&memcg_oom_lock);",
            "\tfor_each_mem_cgroup_tree(iter, memcg)",
            "\t\tif (iter->under_oom > 0)",
            "\t\t\titer->under_oom--;",
            "\tspin_unlock(&memcg_oom_lock);",
            "}",
            "static int memcg_oom_wake_function(wait_queue_entry_t *wait,",
            "\tunsigned mode, int sync, void *arg)",
            "{",
            "\tstruct mem_cgroup *wake_memcg = (struct mem_cgroup *)arg;",
            "\tstruct mem_cgroup *oom_wait_memcg;",
            "\tstruct oom_wait_info *oom_wait_info;",
            "",
            "\toom_wait_info = container_of(wait, struct oom_wait_info, wait);",
            "\toom_wait_memcg = oom_wait_info->memcg;",
            "",
            "\tif (!mem_cgroup_is_descendant(wake_memcg, oom_wait_memcg) &&",
            "\t    !mem_cgroup_is_descendant(oom_wait_memcg, wake_memcg))",
            "\t\treturn 0;",
            "\treturn autoremove_wake_function(wait, mode, sync, arg);",
            "}"
          ],
          "function_name": "memcg1_memcg_init, memcg1_css_offline, mem_cgroup_oom_trylock, mem_cgroup_oom_unlock, mem_cgroup_mark_under_oom, mem_cgroup_unmark_under_oom, memcg_oom_wake_function",
          "description": "初始化和管理内存控制组的OOM状态同步机制，包含OOM锁分配尝试、状态标记、事件列表离线处理等。通过遍历cgroup树实现跨层级的OOM状态传播控制。",
          "similarity": 0.5236923098564148
        },
        {
          "chunk_id": 1,
          "file_path": "mm/memcontrol-v1.c",
          "start_line": 109,
          "end_line": 216,
          "content": [
            "static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_node *mz,",
            "\t\t\t\t\t struct mem_cgroup_tree_per_node *mctz,",
            "\t\t\t\t\t unsigned long new_usage_in_excess)",
            "{",
            "\tstruct rb_node **p = &mctz->rb_root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct mem_cgroup_per_node *mz_node;",
            "\tbool rightmost = true;",
            "",
            "\tif (mz->on_tree)",
            "\t\treturn;",
            "",
            "\tmz->usage_in_excess = new_usage_in_excess;",
            "\tif (!mz->usage_in_excess)",
            "\t\treturn;",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tmz_node = rb_entry(parent, struct mem_cgroup_per_node,",
            "\t\t\t\t\ttree_node);",
            "\t\tif (mz->usage_in_excess < mz_node->usage_in_excess) {",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\t\trightmost = false;",
            "\t\t} else {",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\t}",
            "\t}",
            "",
            "\tif (rightmost)",
            "\t\tmctz->rb_rightmost = &mz->tree_node;",
            "",
            "\trb_link_node(&mz->tree_node, parent, p);",
            "\trb_insert_color(&mz->tree_node, &mctz->rb_root);",
            "\tmz->on_tree = true;",
            "}",
            "static void __mem_cgroup_remove_exceeded(struct mem_cgroup_per_node *mz,",
            "\t\t\t\t\t struct mem_cgroup_tree_per_node *mctz)",
            "{",
            "\tif (!mz->on_tree)",
            "\t\treturn;",
            "",
            "\tif (&mz->tree_node == mctz->rb_rightmost)",
            "\t\tmctz->rb_rightmost = rb_prev(&mz->tree_node);",
            "",
            "\trb_erase(&mz->tree_node, &mctz->rb_root);",
            "\tmz->on_tree = false;",
            "}",
            "static void mem_cgroup_remove_exceeded(struct mem_cgroup_per_node *mz,",
            "\t\t\t\t       struct mem_cgroup_tree_per_node *mctz)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&mctz->lock, flags);",
            "\t__mem_cgroup_remove_exceeded(mz, mctz);",
            "\tspin_unlock_irqrestore(&mctz->lock, flags);",
            "}",
            "static unsigned long soft_limit_excess(struct mem_cgroup *memcg)",
            "{",
            "\tunsigned long nr_pages = page_counter_read(&memcg->memory);",
            "\tunsigned long soft_limit = READ_ONCE(memcg->soft_limit);",
            "\tunsigned long excess = 0;",
            "",
            "\tif (nr_pages > soft_limit)",
            "\t\texcess = nr_pages - soft_limit;",
            "",
            "\treturn excess;",
            "}",
            "static void memcg1_update_tree(struct mem_cgroup *memcg, int nid)",
            "{",
            "\tunsigned long excess;",
            "\tstruct mem_cgroup_per_node *mz;",
            "\tstruct mem_cgroup_tree_per_node *mctz;",
            "",
            "\tif (lru_gen_enabled()) {",
            "\t\tif (soft_limit_excess(memcg))",
            "\t\t\tlru_gen_soft_reclaim(memcg, nid);",
            "\t\treturn;",
            "\t}",
            "",
            "\tmctz = soft_limit_tree.rb_tree_per_node[nid];",
            "\tif (!mctz)",
            "\t\treturn;",
            "\t/*",
            "\t * Necessary to update all ancestors when hierarchy is used.",
            "\t * because their event counter is not touched.",
            "\t */",
            "\tfor (; memcg; memcg = parent_mem_cgroup(memcg)) {",
            "\t\tmz = memcg->nodeinfo[nid];",
            "\t\texcess = soft_limit_excess(memcg);",
            "\t\t/*",
            "\t\t * We have to update the tree if mz is on RB-tree or",
            "\t\t * mem is over its softlimit.",
            "\t\t */",
            "\t\tif (excess || mz->on_tree) {",
            "\t\t\tunsigned long flags;",
            "",
            "\t\t\tspin_lock_irqsave(&mctz->lock, flags);",
            "\t\t\t/* if on-tree, remove it */",
            "\t\t\tif (mz->on_tree)",
            "\t\t\t\t__mem_cgroup_remove_exceeded(mz, mctz);",
            "\t\t\t/*",
            "\t\t\t * Insert again. mz->usage_in_excess will be updated.",
            "\t\t\t * If excess is 0, no tree ops.",
            "\t\t\t */",
            "\t\t\t__mem_cgroup_insert_exceeded(mz, mctz, excess);",
            "\t\t\tspin_unlock_irqrestore(&mctz->lock, flags);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "__mem_cgroup_insert_exceeded, __mem_cgroup_remove_exceeded, mem_cgroup_remove_exceeded, soft_limit_excess, memcg1_update_tree",
          "description": "实现RB树操作函数，用于将超出软限制的内存控制组节点插入或删除到RB树中，同时维护树的右极值指针，并通过遍历层级更新树结构以反映当前软限制状态。",
          "similarity": 0.5176557302474976
        }
      ]
    }
  ]
}