{
  "query": "因此操作系统会尽量减少频繁切换 页面错误处理",
  "timestamp": "2025-12-26 00:04:24",
  "retrieved_files": [
    {
      "source_file": "mm/shuffle.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:21:18\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `shuffle.c`\n\n---\n\n# shuffle.c 技术文档\n\n## 1. 文件概述\n\n`shuffle.c` 实现了 Linux 内核内存管理子系统中的**页面分配随机化（Page Allocation Shuffling）**功能。该机制通过在内存初始化阶段对空闲页面链表进行 Fisher-Yates 洗牌操作，降低物理页帧分配的可预测性，从而增强系统安全性，抵御基于内存布局预测的攻击（如堆喷射、地址泄露等）。该功能默认关闭，可通过内核启动参数 `shuffle=1` 启用。\n\n## 2. 核心功能\n\n### 数据结构与全局变量\n- `page_alloc_shuffle_key`：静态分支键（static key），用于运行时启用/禁用洗牌逻辑，减少未启用时的性能开销。\n- `shuffle_param`：模块参数布尔值，控制是否启用洗牌功能。\n- `shuffle_param_ops`：自定义模块参数操作集，用于处理 `shuffle` 参数的设置和读取。\n\n### 主要函数\n- `shuffle_param_set()`：解析并设置 `shuffle` 内核参数，若启用则激活 `page_alloc_shuffle_key`。\n- `shuffle_valid_page()`：验证指定 PFN 的页面是否满足洗牌条件（属于 buddy 系统、同 zone、空闲、相同 order 和 migratetype）。\n- `__shuffle_zone()`：对指定内存区域（zone）执行 Fisher-Yates 洗牌算法，随机交换同阶空闲页面。\n- `__shuffle_free_memory()`：遍历节点（pgdat）中所有 zone，依次调用 `shuffle_zone()` 进行洗牌。\n- `shuffle_pick_tail()`：提供轻量级随机位生成器，用于在分配时决定从链表头部还是尾部取页（增强运行时随机性）。\n\n## 3. 关键实现\n\n### 洗牌算法（Fisher-Yates）\n- **粒度**：以 `SHUFFLE_ORDER`（通常为 0，即单页）为单位进行洗牌。\n- **范围**：遍历 zone 内所有按 order 对齐的 PFN，对每个有效页面 `page_i` 随机选择另一个有效页面 `page_j` 进行交换。\n- **有效性校验**：通过 `shuffle_valid_page()` 确保交换双方均为 buddy 系统管理的空闲页，且具有相同的迁移类型（migratetype）。\n- **重试机制**：最多尝试 `SHUFFLE_RETRY`（10 次）寻找有效的随机目标页，避免因内存空洞导致失败。\n- **锁优化**：每处理 100 个页面后释放 zone 自旋锁并调度，防止长时间持锁影响系统响应。\n\n### 随机性来源\n- 使用 `get_random_long()` 获取高质量伪随机数作为洗牌索引。\n- `shuffle_pick_tail()` 使用无锁的 64 位随机状态生成器，每次返回最低位并右移，用于运行时分配策略的微调。\n\n### 安全性权衡\n- 明确承认不消除模运算偏差（modulo bias）或 PRNG 偏差，目标是“提高攻击门槛”而非完美随机。\n- 仅在内存初始化阶段（`__meminit`）执行一次洗牌，不影响运行时分配性能。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：内存管理核心数据结构（`struct zone`, `struct page`）。\n  - `<linux/random.h>`：提供 `get_random_long()` 和 `get_random_u64()`。\n  - `\"internal.h\"`、`\"shuffle.h\"`：内核 MM 子系统内部接口及洗牌功能声明。\n- **功能依赖**：\n  - Buddy 分配器：依赖 `PageBuddy()`、`buddy_order()` 等接口判断页面状态。\n  - 页面迁移类型（Migratetype）：确保洗牌不破坏不同迁移类型页面的隔离。\n  - 静态分支（Static Keys）：通过 `static_branch_enable()` 动态启用洗牌路径。\n\n## 5. 使用场景\n\n- **安全加固**：在需要防范物理地址预测攻击的场景（如虚拟化宿主机、安全敏感设备）中启用，增加攻击者利用内存布局漏洞的难度。\n- **内核初始化**：在 `free_area_init_core()` 等内存子系统初始化流程中调用 `__shuffle_free_memory()`，对初始空闲内存进行一次性洗牌。\n- **运行时分配辅助**：`shuffle_pick_tail()` 被页面分配器调用，决定从空闲链表头/尾取页，进一步增加分配时序的不可预测性。\n- **调试支持**：通过 `pr_debug()` 输出洗牌失败或迁移类型不匹配的日志，便于问题诊断（需开启 `DEBUG_SHUFFLE`）。",
      "similarity": 0.5740450620651245,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/shuffle.c",
          "start_line": 16,
          "end_line": 121,
          "content": [
            "static __meminit int shuffle_param_set(const char *val,",
            "\t\tconst struct kernel_param *kp)",
            "{",
            "\tif (param_set_bool(val, kp))",
            "\t\treturn -EINVAL;",
            "\tif (*(bool *)kp->arg)",
            "\t\tstatic_branch_enable(&page_alloc_shuffle_key);",
            "\treturn 0;",
            "}",
            "void __meminit __shuffle_zone(struct zone *z)",
            "{",
            "\tunsigned long i, flags;",
            "\tunsigned long start_pfn = z->zone_start_pfn;",
            "\tunsigned long end_pfn = zone_end_pfn(z);",
            "\tconst int order = SHUFFLE_ORDER;",
            "\tconst int order_pages = 1 << order;",
            "",
            "\tspin_lock_irqsave(&z->lock, flags);",
            "\tstart_pfn = ALIGN(start_pfn, order_pages);",
            "\tfor (i = start_pfn; i < end_pfn; i += order_pages) {",
            "\t\tunsigned long j;",
            "\t\tint migratetype, retry;",
            "\t\tstruct page *page_i, *page_j;",
            "",
            "\t\t/*",
            "\t\t * We expect page_i, in the sub-range of a zone being added",
            "\t\t * (@start_pfn to @end_pfn), to more likely be valid compared to",
            "\t\t * page_j randomly selected in the span @zone_start_pfn to",
            "\t\t * @spanned_pages.",
            "\t\t */",
            "\t\tpage_i = shuffle_valid_page(z, i, order);",
            "\t\tif (!page_i)",
            "\t\t\tcontinue;",
            "",
            "\t\tfor (retry = 0; retry < SHUFFLE_RETRY; retry++) {",
            "\t\t\t/*",
            "\t\t\t * Pick a random order aligned page in the zone span as",
            "\t\t\t * a swap target. If the selected pfn is a hole, retry",
            "\t\t\t * up to SHUFFLE_RETRY attempts find a random valid pfn",
            "\t\t\t * in the zone.",
            "\t\t\t */",
            "\t\t\tj = z->zone_start_pfn +",
            "\t\t\t\tALIGN_DOWN(get_random_long() % z->spanned_pages,",
            "\t\t\t\t\t\torder_pages);",
            "\t\t\tpage_j = shuffle_valid_page(z, j, order);",
            "\t\t\tif (page_j && page_j != page_i)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t\tif (retry >= SHUFFLE_RETRY) {",
            "\t\t\tpr_debug(\"%s: failed to swap %#lx\\n\", __func__, i);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Each migratetype corresponds to its own list, make sure the",
            "\t\t * types match otherwise we're moving pages to lists where they",
            "\t\t * do not belong.",
            "\t\t */",
            "\t\tmigratetype = get_pageblock_migratetype(page_i);",
            "\t\tif (get_pageblock_migratetype(page_j) != migratetype) {",
            "\t\t\tpr_debug(\"%s: migratetype mismatch %#lx\\n\", __func__, i);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tlist_swap(&page_i->lru, &page_j->lru);",
            "",
            "\t\tpr_debug(\"%s: swap: %#lx -> %#lx\\n\", __func__, i, j);",
            "",
            "\t\t/* take it easy on the zone lock */",
            "\t\tif ((i % (100 * order_pages)) == 0) {",
            "\t\t\tspin_unlock_irqrestore(&z->lock, flags);",
            "\t\t\tcond_resched();",
            "\t\t\tspin_lock_irqsave(&z->lock, flags);",
            "\t\t}",
            "\t}",
            "\tspin_unlock_irqrestore(&z->lock, flags);",
            "}",
            "void __meminit __shuffle_free_memory(pg_data_t *pgdat)",
            "{",
            "\tstruct zone *z;",
            "",
            "\tfor (z = pgdat->node_zones; z < pgdat->node_zones + MAX_NR_ZONES; z++)",
            "\t\tshuffle_zone(z);",
            "}",
            "bool shuffle_pick_tail(void)",
            "{",
            "\tstatic u64 rand;",
            "\tstatic u8 rand_bits;",
            "\tbool ret;",
            "",
            "\t/*",
            "\t * The lack of locking is deliberate. If 2 threads race to",
            "\t * update the rand state it just adds to the entropy.",
            "\t */",
            "\tif (rand_bits == 0) {",
            "\t\trand_bits = 64;",
            "\t\trand = get_random_u64();",
            "\t}",
            "",
            "\tret = rand & 1;",
            "",
            "\trand_bits--;",
            "\trand >>= 1;",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "shuffle_param_set, __shuffle_zone, __shuffle_free_memory, shuffle_pick_tail",
          "description": "shuffle_param_set设置参数并启用/禁用静态键；__shuffle_zone在内存区随机交换页面以打乱物理顺序；__shuffle_free_memory初始化时调用__shuffle_zone；shuffle_pick_tail生成随机布尔值用于选择尾部页",
          "similarity": 0.5642234086990356
        },
        {
          "chunk_id": 0,
          "file_path": "mm/shuffle.c",
          "start_line": 1,
          "end_line": 15,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "// Copyright(c) 2018 Intel Corporation. All rights reserved.",
            "",
            "#include <linux/mm.h>",
            "#include <linux/init.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/random.h>",
            "#include <linux/moduleparam.h>",
            "#include \"internal.h\"",
            "#include \"shuffle.h\"",
            "",
            "DEFINE_STATIC_KEY_FALSE(page_alloc_shuffle_key);",
            "",
            "static bool shuffle_param;",
            ""
          ],
          "function_name": null,
          "description": "定义静态键用于控制页面分配随机化功能，并声明参数变量shuffle_param，用于启用或禁用相关机制",
          "similarity": 0.4800439476966858
        }
      ]
    },
    {
      "source_file": "mm/swap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:25:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `swap.c`\n\n---\n\n# swap.c 技术文档\n\n## 1. 文件概述\n\n`swap.c` 是 Linux 内核内存管理子系统（MM）中的核心文件之一，主要负责页面回收（page reclaim）、LRU（Least Recently Used）链表管理、页面释放以及与交换（swap）机制相关的底层支持逻辑。尽管文件名为 `swap.c`，但其功能不仅限于交换，而是涵盖了通用的页面生命周期管理、LRU 链表操作、页面引用计数释放、可回收性判断等关键内存管理任务。该文件为页面缓存（page cache）、匿名页（anonymous pages）和大页（huge pages）提供统一的释放与 LRU 管理接口。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `page_cluster`：控制一次 I/O 操作中尝试换入/换出的页面数量（以 2 的幂表示），默认值由系统配置决定。\n- `page_cluster_max`：`page_cluster` 的最大允许值（31，即最多 2^31 页，实际受架构限制）。\n\n### 主要数据结构\n- `struct lru_rotate`：每个 CPU 私有的结构，用于在中断禁用上下文中批量处理需移至 LRU 链表尾部的页面（如 `folio_rotate_reclaimable` 场景）。\n- `struct cpu_fbatches`：每个 CPU 私有的 folio 批处理结构，包含多个 folio_batch，用于高效地向 LRU 链表添加、停用或激活页面，避免频繁获取 LRU 锁。\n\n### 主要函数\n- `__folio_put()`：释放一个 folio 的核心函数，根据 folio 类型（设备内存、大页、普通页）调用相应的释放路径。\n- `put_pages_list()`：批量释放通过 `lru` 字段链接的页面列表，常用于网络子系统或 compound page 释放。\n- `lru_add_fn()`：将 folio 添加到对应 LRU 链表的回调函数，处理可回收性（evictable/unevictable）状态转换和统计计数。\n- `folio_batch_move_lru()`：批量执行 LRU 操作（如添加、移动），在持有 LRU 锁期间完成所有 folio 的处理。\n- `folio_rotate_reclaimable()`：在写回完成后，若页面仍可回收，则将其移至 inactive LRU 链表尾部，以延迟其被回收的时间。\n- `lru_note_cost()`：记录 LRU 扫描过程中的 I/O 和旋转（rotation）成本，用于后续调整 anon/file LRU 的扫描比例。\n\n## 3. 关键实现\n\n### LRU 批处理机制\n为减少 LRU 锁竞争，内核采用 per-CPU 批处理（`folio_batch`）方式暂存待处理的 folio。当批处理满或遇到大页（`folio_test_large`）时，才批量获取 LRU 锁并执行操作（如 `lru_add_fn`）。这显著提升了高并发场景下的性能。\n\n### 可回收性管理\n页面是否可回收由 `folio_evictable()` 判断，主要依据是否被 mlock 锁定。在添加到 LRU 时：\n- 若页面变为可回收（原为 unevictable），则增加 `UNEVICTABLE_PGRESCUED` 统计；\n- 若页面不可回收，则清除 active 标志，设置 unevictable 标志，并重置 `mlock_count`，同时增加 `UNEVICTABLE_PGCULLED` 统计。\n\n### 页面释放路径\n`__folio_put()` 是 folio 引用计数归零后的释放入口：\n1. 设备内存 folio 调用 `free_zone_device_folio()`\n2. 大页 folio 调用 `free_huge_folio()`\n3. 普通 folio 先从 LRU 移除（若在 LRU 上），然后解绑内存控制组（memcg），最后调用 `free_unref_page()` 释放到伙伴系统。\n\n### LRU 旋转优化\n`folio_rotate_reclaimable()` 在写回结束时，若页面干净且未锁定，则将其移至 inactive LRU 尾部。此操作通过 per-CPU 的 `lru_rotate` 批处理完成，仅在必要时获取 LRU 锁，避免影响写回关键路径性能。\n\n### 成本跟踪\n`lru_note_cost()` 通过累加 `nr_io * SWAP_CLUSTER_MAX + nr_rotated` 来量化扫描成本，用于动态调整匿名页与文件页 LRU 的扫描比例，优化内存回收效率。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/pagevec.h>`、`\"internal.h\"` 等，使用伙伴系统、LRU 框架、内存控制组（memcg）等基础组件。\n- **交换子系统**：虽不直接实现 swap read/write，但为 `vmscan.c` 中的页面回收提供 LRU 操作接口，是 swap 机制的支撑模块。\n- **大页支持**：通过 `hugetlb.h` 与大页子系统交互，特殊处理大页释放。\n- **设备内存**：通过 `memremap.h` 支持持久内存（pmem）等 zone device 页面的释放。\n- **跟踪与统计**：使用 tracepoint（`trace/events/pagemap.h`）和 VM 统计（`kernel_stat.h`）进行性能分析。\n- **SMP 支持**：大量使用 per-CPU 变量（`DEFINE_PER_CPU`）和本地锁（`local_lock_t`）优化多核性能。\n\n## 5. 使用场景\n\n- **页面回收（Reclaim）**：当内存压力触发 kswapd 或 direct reclaim 时，`vmscan.c` 调用本文件的 LRU 操作函数来隔离、释放页面。\n- **页面缓存释放**：文件系统或网络子系统在释放 page cache 页面时，通过 `__folio_put()` 或 `put_pages_list()` 触发 LRU 移除和内存释放。\n- **写回完成处理**：块设备或文件系统在完成脏页写回后，调用 `folio_rotate_reclaimable()` 更新页面在 LRU 中的位置。\n- **内存控制组（cgroup）**：memcg 回收内存时，复用本文件的 LRU 批处理和 folio 释放逻辑。\n- **大页与设备内存管理**：透明大页（THP）或持久内存应用释放页面时，通过统一的 `__folio_put()` 接口分发到专用释放函数。\n- **系统调优**：管理员通过 `/proc/sys/vm/page-cluster` 调整 `page_cluster` 值，影响 swap 和 page cache 的 I/O 批量大小。",
      "similarity": 0.5635207891464233,
      "chunks": [
        {
          "chunk_id": 7,
          "file_path": "mm/swap.c",
          "start_line": 912,
          "end_line": 1023,
          "content": [
            "void lru_add_drain_all(void)",
            "{",
            "\t__lru_add_drain_all(false);",
            "}",
            "void lru_add_drain_all(void)",
            "{",
            "\tlru_add_drain();",
            "}",
            "void lru_cache_disable(void)",
            "{",
            "\tatomic_inc(&lru_disable_count);",
            "\t/*",
            "\t * Readers of lru_disable_count are protected by either disabling",
            "\t * preemption or rcu_read_lock:",
            "\t *",
            "\t * preempt_disable, local_irq_disable  [bh_lru_lock()]",
            "\t * rcu_read_lock\t\t       [rt_spin_lock CONFIG_PREEMPT_RT]",
            "\t * preempt_disable\t\t       [local_lock !CONFIG_PREEMPT_RT]",
            "\t *",
            "\t * Since v5.1 kernel, synchronize_rcu() is guaranteed to wait on",
            "\t * preempt_disable() regions of code. So any CPU which sees",
            "\t * lru_disable_count = 0 will have exited the critical",
            "\t * section when synchronize_rcu() returns.",
            "\t */",
            "\tsynchronize_rcu_expedited();",
            "#ifdef CONFIG_SMP",
            "\t__lru_add_drain_all(true);",
            "#else",
            "\tlru_add_and_bh_lrus_drain();",
            "#endif",
            "}",
            "void folios_put_refs(struct folio_batch *folios, unsigned int *refs)",
            "{",
            "\tint i, j;",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags = 0;",
            "",
            "\tfor (i = 0, j = 0; i < folios->nr; i++) {",
            "\t\tstruct folio *folio = folios->folios[i];",
            "\t\tunsigned int nr_refs = refs ? refs[i] : 1;",
            "",
            "\t\tif (is_huge_zero_folio(folio))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (folio_is_zone_device(folio)) {",
            "\t\t\tif (lruvec) {",
            "\t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\t\t\t\tlruvec = NULL;",
            "\t\t\t}",
            "\t\t\tif (put_devmap_managed_folio_refs(folio, nr_refs))",
            "\t\t\t\tcontinue;",
            "\t\t\tif (folio_ref_sub_and_test(folio, nr_refs))",
            "\t\t\t\tfree_zone_device_folio(folio);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tif (!folio_ref_sub_and_test(folio, nr_refs))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* hugetlb has its own memcg */",
            "\t\tif (folio_test_hugetlb(folio)) {",
            "\t\t\tif (lruvec) {",
            "\t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\t\t\t\tlruvec = NULL;",
            "\t\t\t}",
            "\t\t\tfree_huge_folio(folio);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfolio_unqueue_deferred_split(folio);",
            "\t\t__page_cache_release(folio, &lruvec, &flags);",
            "",
            "\t\tif (j != i)",
            "\t\t\tfolios->folios[j] = folio;",
            "\t\tj++;",
            "\t}",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\tif (!j) {",
            "\t\tfolio_batch_reinit(folios);",
            "\t\treturn;",
            "\t}",
            "",
            "\tfolios->nr = j;",
            "\tmem_cgroup_uncharge_folios(folios);",
            "\tfree_unref_folios(folios);",
            "}",
            "void release_pages(release_pages_arg arg, int nr)",
            "{",
            "\tstruct folio_batch fbatch;",
            "\tint refs[PAGEVEC_SIZE];",
            "\tstruct encoded_page **encoded = arg.encoded_pages;",
            "\tint i;",
            "",
            "\tfolio_batch_init(&fbatch);",
            "\tfor (i = 0; i < nr; i++) {",
            "\t\t/* Turn any of the argument types into a folio */",
            "\t\tstruct folio *folio = page_folio(encoded_page_ptr(encoded[i]));",
            "",
            "\t\t/* Is our next entry actually \"nr_pages\" -> \"nr_refs\" ? */",
            "\t\trefs[fbatch.nr] = 1;",
            "\t\tif (unlikely(encoded_page_flags(encoded[i]) &",
            "\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\trefs[fbatch.nr] = encoded_nr_pages(encoded[++i]);",
            "",
            "\t\tif (folio_batch_add(&fbatch, folio) > 0)",
            "\t\t\tcontinue;",
            "\t\tfolios_put_refs(&fbatch, refs);",
            "\t}",
            "",
            "\tif (fbatch.nr)",
            "\t\tfolios_put_refs(&fbatch, refs);",
            "}"
          ],
          "function_name": "lru_add_drain_all, lru_add_drain_all, lru_cache_disable, folios_put_refs, release_pages",
          "description": "lru_add_drain_all 和 lru_cache_disable 控制全局LRU缓存状态切换，通过synchronize_rcu_expedited强制同步后，根据配置选择全CPU或单CPU模式执行页面释放；release_pages 批量释放页面引用并处理内存组计费。",
          "similarity": 0.5834473371505737
        },
        {
          "chunk_id": 1,
          "file_path": "mm/swap.c",
          "start_line": 77,
          "end_line": 190,
          "content": [
            "static void __page_cache_release(struct folio *folio, struct lruvec **lruvecp,",
            "\t\tunsigned long *flagsp)",
            "{",
            "\tif (folio_test_lru(folio)) {",
            "\t\tfolio_lruvec_relock_irqsave(folio, lruvecp, flagsp);",
            "\t\tlruvec_del_folio(*lruvecp, folio);",
            "\t\t__folio_clear_lru_flags(folio);",
            "\t}",
            "}",
            "static void page_cache_release(struct folio *folio)",
            "{",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags;",
            "",
            "\t__page_cache_release(folio, &lruvec, &flags);",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "}",
            "void __folio_put(struct folio *folio)",
            "{",
            "\tif (unlikely(folio_is_zone_device(folio))) {",
            "\t\tfree_zone_device_folio(folio);",
            "\t\treturn;",
            "\t} else if (folio_test_hugetlb(folio)) {",
            "\t\tfree_huge_folio(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tpage_cache_release(folio);",
            "\tfolio_unqueue_deferred_split(folio);",
            "\tmem_cgroup_uncharge(folio);",
            "\tfree_unref_page(&folio->page, folio_order(folio));",
            "}",
            "void put_pages_list(struct list_head *pages)",
            "{",
            "\tstruct folio_batch fbatch;",
            "\tstruct folio *folio, *next;",
            "",
            "\tfolio_batch_init(&fbatch);",
            "\tlist_for_each_entry_safe(folio, next, pages, lru) {",
            "\t\tif (!folio_put_testzero(folio))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_hugetlb(folio)) {",
            "\t\t\tfree_huge_folio(folio);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\t/* LRU flag must be clear because it's passed using the lru */",
            "\t\tif (folio_batch_add(&fbatch, folio) > 0)",
            "\t\t\tcontinue;",
            "\t\tfree_unref_folios(&fbatch);",
            "\t}",
            "",
            "\tif (fbatch.nr)",
            "\t\tfree_unref_folios(&fbatch);",
            "\tINIT_LIST_HEAD(pages);",
            "}",
            "static void lru_add_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tint was_unevictable = folio_test_clear_unevictable(folio);",
            "\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\t/*",
            "\t * Is an smp_mb__after_atomic() still required here, before",
            "\t * folio_evictable() tests the mlocked flag, to rule out the possibility",
            "\t * of stranding an evictable folio on an unevictable LRU?  I think",
            "\t * not, because __munlock_folio() only clears the mlocked flag",
            "\t * while the LRU lock is held.",
            "\t *",
            "\t * (That is not true of __page_cache_release(), and not necessarily",
            "\t * true of folios_put(): but those only clear the mlocked flag after",
            "\t * folio_put_testzero() has excluded any other users of the folio.)",
            "\t */",
            "\tif (folio_evictable(folio)) {",
            "\t\tif (was_unevictable)",
            "\t\t\t__count_vm_events(UNEVICTABLE_PGRESCUED, nr_pages);",
            "\t} else {",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_set_unevictable(folio);",
            "\t\t/*",
            "\t\t * folio->mlock_count = !!folio_test_mlocked(folio)?",
            "\t\t * But that leaves __mlock_folio() in doubt whether another",
            "\t\t * actor has already counted the mlock or not.  Err on the",
            "\t\t * safe side, underestimate, let page reclaim fix it, rather",
            "\t\t * than leaving a page on the unevictable LRU indefinitely.",
            "\t\t */",
            "\t\tfolio->mlock_count = 0;",
            "\t\tif (!was_unevictable)",
            "\t\t\t__count_vm_events(UNEVICTABLE_PGCULLED, nr_pages);",
            "\t}",
            "",
            "\tlruvec_add_folio(lruvec, folio);",
            "\ttrace_mm_lru_insertion(folio);",
            "}",
            "static void folio_batch_move_lru(struct folio_batch *fbatch, move_fn_t move_fn)",
            "{",
            "\tint i;",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags = 0;",
            "",
            "\tfor (i = 0; i < folio_batch_count(fbatch); i++) {",
            "\t\tstruct folio *folio = fbatch->folios[i];",
            "",
            "\t\tfolio_lruvec_relock_irqsave(folio, &lruvec, &flags);",
            "\t\tmove_fn(lruvec, folio);",
            "",
            "\t\tfolio_set_lru(folio);",
            "\t}",
            "",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\tfolios_put(fbatch);",
            "}"
          ],
          "function_name": "__page_cache_release, page_cache_release, __folio_put, put_pages_list, lru_add_fn, folio_batch_move_lru",
          "description": "实现了页面缓存释放和LRU列表维护逻辑，包含__page_cache_release用于从LRU列表移除页面，page_cache_release处理普通页面释放流程，__folio_put负责释放非设备映射和大页，put_pages_list批量处理页面释放，lru_add_fn将页面添加到LRU列表并根据是否可交换设置相应标志。",
          "similarity": 0.5721584558486938
        },
        {
          "chunk_id": 6,
          "file_path": "mm/swap.c",
          "start_line": 781,
          "end_line": 896,
          "content": [
            "void lru_add_drain_cpu_zone(struct zone *zone)",
            "{",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tlru_add_drain_cpu(smp_processor_id());",
            "\tdrain_local_pages(zone);",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "\tmlock_drain_local();",
            "}",
            "static void lru_add_drain_per_cpu(struct work_struct *dummy)",
            "{",
            "\tlru_add_and_bh_lrus_drain();",
            "}",
            "static bool cpu_needs_drain(unsigned int cpu)",
            "{",
            "\tstruct cpu_fbatches *fbatches = &per_cpu(cpu_fbatches, cpu);",
            "",
            "\t/* Check these in order of likelihood that they're not zero */",
            "\treturn folio_batch_count(&fbatches->lru_add) ||",
            "\t\tdata_race(folio_batch_count(&per_cpu(lru_rotate.fbatch, cpu))) ||",
            "\t\tfolio_batch_count(&fbatches->lru_deactivate_file) ||",
            "\t\tfolio_batch_count(&fbatches->lru_deactivate) ||",
            "\t\tfolio_batch_count(&fbatches->lru_lazyfree) ||",
            "\t\tfolio_batch_count(&fbatches->activate) ||",
            "\t\tneed_mlock_drain(cpu) ||",
            "\t\thas_bh_in_lru(cpu, NULL);",
            "}",
            "static inline void __lru_add_drain_all(bool force_all_cpus)",
            "{",
            "\t/*",
            "\t * lru_drain_gen - Global pages generation number",
            "\t *",
            "\t * (A) Definition: global lru_drain_gen = x implies that all generations",
            "\t *     0 < n <= x are already *scheduled* for draining.",
            "\t *",
            "\t * This is an optimization for the highly-contended use case where a",
            "\t * user space workload keeps constantly generating a flow of pages for",
            "\t * each CPU.",
            "\t */",
            "\tstatic unsigned int lru_drain_gen;",
            "\tstatic struct cpumask has_work;",
            "\tstatic DEFINE_MUTEX(lock);",
            "\tunsigned cpu, this_gen;",
            "",
            "\t/*",
            "\t * Make sure nobody triggers this path before mm_percpu_wq is fully",
            "\t * initialized.",
            "\t */",
            "\tif (WARN_ON(!mm_percpu_wq))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Guarantee folio_batch counter stores visible by this CPU",
            "\t * are visible to other CPUs before loading the current drain",
            "\t * generation.",
            "\t */",
            "\tsmp_mb();",
            "",
            "\t/*",
            "\t * (B) Locally cache global LRU draining generation number",
            "\t *",
            "\t * The read barrier ensures that the counter is loaded before the mutex",
            "\t * is taken. It pairs with smp_mb() inside the mutex critical section",
            "\t * at (D).",
            "\t */",
            "\tthis_gen = smp_load_acquire(&lru_drain_gen);",
            "",
            "\tmutex_lock(&lock);",
            "",
            "\t/*",
            "\t * (C) Exit the draining operation if a newer generation, from another",
            "\t * lru_add_drain_all(), was already scheduled for draining. Check (A).",
            "\t */",
            "\tif (unlikely(this_gen != lru_drain_gen && !force_all_cpus))",
            "\t\tgoto done;",
            "",
            "\t/*",
            "\t * (D) Increment global generation number",
            "\t *",
            "\t * Pairs with smp_load_acquire() at (B), outside of the critical",
            "\t * section. Use a full memory barrier to guarantee that the",
            "\t * new global drain generation number is stored before loading",
            "\t * folio_batch counters.",
            "\t *",
            "\t * This pairing must be done here, before the for_each_online_cpu loop",
            "\t * below which drains the page vectors.",
            "\t *",
            "\t * Let x, y, and z represent some system CPU numbers, where x < y < z.",
            "\t * Assume CPU #z is in the middle of the for_each_online_cpu loop",
            "\t * below and has already reached CPU #y's per-cpu data. CPU #x comes",
            "\t * along, adds some pages to its per-cpu vectors, then calls",
            "\t * lru_add_drain_all().",
            "\t *",
            "\t * If the paired barrier is done at any later step, e.g. after the",
            "\t * loop, CPU #x will just exit at (C) and miss flushing out all of its",
            "\t * added pages.",
            "\t */",
            "\tWRITE_ONCE(lru_drain_gen, lru_drain_gen + 1);",
            "\tsmp_mb();",
            "",
            "\tcpumask_clear(&has_work);",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct work_struct *work = &per_cpu(lru_add_drain_work, cpu);",
            "",
            "\t\tif (cpu_needs_drain(cpu)) {",
            "\t\t\tINIT_WORK(work, lru_add_drain_per_cpu);",
            "\t\t\tqueue_work_on(cpu, mm_percpu_wq, work);",
            "\t\t\t__cpumask_set_cpu(cpu, &has_work);",
            "\t\t}",
            "\t}",
            "",
            "\tfor_each_cpu(cpu, &has_work)",
            "\t\tflush_work(&per_cpu(lru_add_drain_work, cpu));",
            "",
            "done:",
            "\tmutex_unlock(&lock);",
            "}"
          ],
          "function_name": "lru_add_drain_cpu_zone, lru_add_drain_per_cpu, cpu_needs_drain, __lru_add_drain_all",
          "description": "__lru_add_drain_all 管理全局LRU排水过程，通过generation-based机制避免重复处理，利用互斥锁和smp_mb屏障确保跨CPU数据可见性，并调度工作队列处理各CPU的待处理页面批次。",
          "similarity": 0.5568429231643677
        },
        {
          "chunk_id": 8,
          "file_path": "mm/swap.c",
          "start_line": 1079,
          "end_line": 1111,
          "content": [
            "void __folio_batch_release(struct folio_batch *fbatch)",
            "{",
            "\tif (!fbatch->percpu_pvec_drained) {",
            "\t\tlru_add_drain();",
            "\t\tfbatch->percpu_pvec_drained = true;",
            "\t}",
            "\tfolios_put(fbatch);",
            "}",
            "void folio_batch_remove_exceptionals(struct folio_batch *fbatch)",
            "{",
            "\tunsigned int i, j;",
            "",
            "\tfor (i = 0, j = 0; i < folio_batch_count(fbatch); i++) {",
            "\t\tstruct folio *folio = fbatch->folios[i];",
            "\t\tif (!xa_is_value(folio))",
            "\t\t\tfbatch->folios[j++] = folio;",
            "\t}",
            "\tfbatch->nr = j;",
            "}",
            "void __init swap_setup(void)",
            "{",
            "\tunsigned long megs = totalram_pages() >> (20 - PAGE_SHIFT);",
            "",
            "\t/* Use a smaller cluster for small-memory machines */",
            "\tif (megs < 16)",
            "\t\tpage_cluster = 2;",
            "\telse",
            "\t\tpage_cluster = 3;",
            "\t/*",
            "\t * Right now other parts of the system means that we",
            "\t * _really_ don't want to cluster much more",
            "\t */",
            "}"
          ],
          "function_name": "__folio_batch_release, folio_batch_remove_exceptionals, swap_setup",
          "description": "__folio_batch_release 标记并释放页面批次引用，folio_batch_remove_exceptionals 清理异常条目；swap_setup 初始化页面聚类参数，根据内存大小调整page_cluster值。",
          "similarity": 0.5551101565361023
        },
        {
          "chunk_id": 5,
          "file_path": "mm/swap.c",
          "start_line": 641,
          "end_line": 743,
          "content": [
            "void lru_add_drain_cpu(int cpu)",
            "{",
            "\tstruct cpu_fbatches *fbatches = &per_cpu(cpu_fbatches, cpu);",
            "\tstruct folio_batch *fbatch = &fbatches->lru_add;",
            "",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, lru_add_fn);",
            "",
            "\tfbatch = &per_cpu(lru_rotate.fbatch, cpu);",
            "\t/* Disabling interrupts below acts as a compiler barrier. */",
            "\tif (data_race(folio_batch_count(fbatch))) {",
            "\t\tunsigned long flags;",
            "",
            "\t\t/* No harm done if a racing interrupt already did this */",
            "\t\tlocal_lock_irqsave(&lru_rotate.lock, flags);",
            "\t\tfolio_batch_move_lru(fbatch, lru_move_tail_fn);",
            "\t\tlocal_unlock_irqrestore(&lru_rotate.lock, flags);",
            "\t}",
            "",
            "\tfbatch = &fbatches->lru_deactivate_file;",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, lru_deactivate_file_fn);",
            "",
            "\tfbatch = &fbatches->lru_deactivate;",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, lru_deactivate_fn);",
            "",
            "\tfbatch = &fbatches->lru_lazyfree;",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, lru_lazyfree_fn);",
            "",
            "\tfolio_activate_drain(cpu);",
            "}",
            "void deactivate_file_folio(struct folio *folio)",
            "{",
            "\tstruct folio_batch *fbatch;",
            "",
            "\t/* Deactivating an unevictable folio will not accelerate reclaim */",
            "\tif (folio_test_unevictable(folio))",
            "\t\treturn;",
            "",
            "\tfolio_get(folio);",
            "\tif (!folio_test_clear_lru(folio)) {",
            "\t\tfolio_put(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_deactivate_file);",
            "\tfolio_batch_add_and_move(fbatch, folio, lru_deactivate_file_fn);",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "}",
            "void folio_deactivate(struct folio *folio)",
            "{",
            "\tif (!folio_test_unevictable(folio) && (folio_test_active(folio) ||",
            "\t    lru_gen_enabled())) {",
            "\t\tstruct folio_batch *fbatch;",
            "",
            "\t\tfolio_get(folio);",
            "\t\tif (!folio_test_clear_lru(folio)) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tlocal_lock(&cpu_fbatches.lock);",
            "\t\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_deactivate);",
            "\t\tfolio_batch_add_and_move(fbatch, folio, lru_deactivate_fn);",
            "\t\tlocal_unlock(&cpu_fbatches.lock);",
            "\t}",
            "}",
            "void folio_mark_lazyfree(struct folio *folio)",
            "{",
            "\tif (folio_test_anon(folio) && folio_test_swapbacked(folio) &&",
            "\t    !folio_test_swapcache(folio) && !folio_test_unevictable(folio)) {",
            "\t\tstruct folio_batch *fbatch;",
            "",
            "\t\tfolio_get(folio);",
            "\t\tif (!folio_test_clear_lru(folio)) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tlocal_lock(&cpu_fbatches.lock);",
            "\t\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_lazyfree);",
            "\t\tfolio_batch_add_and_move(fbatch, folio, lru_lazyfree_fn);",
            "\t\tlocal_unlock(&cpu_fbatches.lock);",
            "\t}",
            "}",
            "void lru_add_drain(void)",
            "{",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tlru_add_drain_cpu(smp_processor_id());",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "\tmlock_drain_local();",
            "}",
            "static void lru_add_and_bh_lrus_drain(void)",
            "{",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tlru_add_drain_cpu(smp_processor_id());",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "\tinvalidate_bh_lrus_cpu();",
            "\tmlock_drain_local();",
            "}"
          ],
          "function_name": "lru_add_drain_cpu, deactivate_file_folio, folio_deactivate, folio_mark_lazyfree, lru_add_drain, lru_add_and_bh_lrus_drain",
          "description": "lru_add_drain_cpu 处理CPU本地LRU批次的移动，将不同类型的页面（如lru_add、deactivate、lazyfree等）通过folio_batch_move_lru分发至相应LRU队列，同时协调中断禁用和锁保护以确保原子性。",
          "similarity": 0.5408170223236084
        }
      ]
    },
    {
      "source_file": "kernel/irq/spurious.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:09:47\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\spurious.c`\n\n---\n\n# `irq/spurious.c` 技术文档\n\n## 1. 文件概述\n\n`irq/spurious.c` 是 Linux 内核中断子系统中的一个关键组件，负责处理**伪中断**（spurious interrupts）和**错误路由中断**（misrouted interrupts）。当硬件中断未被任何中断处理程序正确处理（返回 `IRQ_NONE`）时，内核会怀疑该中断是伪中断或被错误路由到当前 IRQ 线。该文件实现了检测、诊断和恢复机制，包括：\n\n- 统计未处理中断次数并判断是否为“卡住”的 IRQ\n- 在启用 `irqfixup` 选项时尝试在其他 IRQ 线上查找真正的中断源（中断错位恢复）\n- 定期轮询被禁用的伪中断线以尝试恢复共享中断设备\n- 提供诊断信息（如调用栈和注册的处理函数列表）\n\n该机制对于提高系统在硬件或固件存在缺陷时的鲁棒性至关重要。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `irqfixup`：模块参数，控制伪中断修复行为（0=禁用，1=仅对未处理中断尝试修复，2=对标记为 `IRQF_IRQPOLL` 的中断也尝试修复）\n- `poll_spurious_irq_timer`：定时器，用于定期轮询被标记为 `IRQS_SPURIOUS_DISABLED` 的中断线\n- `irq_poll_cpu`：记录当前正在执行轮询任务的 CPU ID\n- `irq_poll_active`：原子变量，防止多个 CPU 同时执行轮询\n\n### 主要函数\n- `irq_wait_for_poll(struct irq_desc *desc)`  \n  等待轮询操作完成，避免与轮询线程竞争。在 SMP 系统中自旋等待 `IRQS_POLL_INPROGRESS` 标志清除。\n  \n- `try_one_irq(struct irq_desc *desc, bool force)`  \n  尝试在指定中断描述符上执行中断处理。跳过 PER_CPU、嵌套线程和显式标记为轮询的中断。若中断被禁用，则仅在 `force=true` 时处理。支持共享中断的 `IRQS_PENDING` 重试机制。\n\n- `misrouted_irq(int irq)`  \n  遍历所有 IRQ（除 0 和当前 IRQ），调用 `try_one_irq()` 尝试在其他线上找到真正的中断源。用于中断错位恢复。\n\n- `poll_spurious_irqs(struct timer_list *unused)`  \n  定时器回调函数，轮询所有被标记为 `IRQS_SPURIOUS_DISABLED` 的中断线，强制尝试处理（`force=true`）。\n\n- `__report_bad_irq()` / `report_bad_irq()`  \n  打印伪中断诊断信息，包括中断号、错误返回值、调用栈及所有注册的处理函数。\n\n- `try_misrouted_irq()`  \n  根据 `irqfixup` 级别判断是否应尝试中断错位恢复。\n\n- `note_interrupt(struct irq_desc *desc, irqreturn_t action_ret)`  \n  中断处理结果分析入口。统计未处理中断，触发伪中断检测、诊断和恢复逻辑。\n\n## 3. 关键实现\n\n### 伪中断检测机制\n- 当 `note_interrupt()` 收到 `IRQ_NONE` 时，会递增中断描述符的未处理计数。\n- 若在 100,000 次中断中有 99,900 次未处理，则判定该 IRQ “卡住”，打印诊断信息并建议使用 `irqpoll` 启动参数。\n- 诊断信息包含所有注册的处理函数地址及符号名，便于调试。\n\n### 中断错位恢复（Misrouted IRQ Recovery）\n- 通过 `irqfixup` 内核参数启用（启动时传入 `irqfixup=1` 或 `2`）。\n- 当当前 IRQ 未被处理时，遍历其他所有 IRQ 线，尝试调用其处理函数（`try_one_irq()`）。\n- 仅适用于共享中断（`IRQF_SHARED`）且非 PER_CPU/嵌套线程类型。\n- 使用 `IRQS_POLL_INPROGRESS` 标志防止与正常中断处理冲突。\n\n### 轮询恢复机制\n- 被判定为伪中断的 IRQ 会被标记 `IRQS_SPURIOUS_DISABLED` 并禁用。\n- 启用 `irqfixup` 时，启动定时器 `poll_spurious_irq_timer`（间隔 100ms）。\n- 定时器回调 `poll_spurious_irqs()` 遍历所有 `IRQS_SPURIOUS_DISABLED` 的 IRQ，强制尝试处理（即使已禁用）。\n- 通过 `local_irq_disable/enable()` 保证轮询期间本地中断关闭，避免嵌套。\n\n### SMP 安全性\n- 使用 `irq_poll_active` 原子变量确保同一时间仅一个 CPU 执行轮询。\n- `irq_wait_for_poll()` 在 SMP 下自旋等待轮询完成，防止死锁。\n- 所有关键操作均在 `desc->lock` 保护下进行。\n\n### 线程化中断处理支持\n- 若主处理函数返回 `IRQ_WAKE_THREAD`，则延迟伪中断判断至下一次硬件中断，以等待线程处理结果。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/irq.h>`、`<linux/interrupt.h>`：中断核心数据结构和 API\n  - `<linux/timer.h>`：定时器支持（用于轮询）\n  - `\"internals.h\"`：中断子系统内部接口\n\n- **内核配置依赖**：\n  - `CONFIG_SMP`：影响 `irq_wait_for_poll()` 的实现\n  - `irqfixup` 模块参数：控制恢复行为\n\n- **与其他模块交互**：\n  - 被通用中断处理流程（如 `handle_irq_event()`）调用\n  - 与中断描述符管理（`irq_desc`）紧密集成\n  - 依赖内核打印和栈回溯机制（`dump_stack()`）\n\n## 5. 使用场景\n\n1. **硬件/固件缺陷处理**：  \n   当 BIOS 或硬件错误地将设备中断路由到错误的 IRQ 线时，通过 `irqfixup` 机制尝试在其他线上找到真正的处理函数。\n\n2. **共享中断线故障恢复**：  \n   在多个设备共享同一 IRQ 线时，若其中一个设备故障产生持续中断但无处理函数响应，内核可禁用该线并定期轮询，避免系统被中断风暴拖垮。\n\n3. **系统调试与诊断**：  \n   当出现“nobody cared”中断错误时，自动打印详细的处理函数列表和调用栈，帮助开发者定位问题设备或驱动。\n\n4. **高可用性系统**：  \n   在无法立即修复硬件问题的生产环境中，通过 `irqpoll` 启动参数启用轮询机制，维持系统基本运行。\n\n5. **传统 PC 兼容性**：  \n   特别处理 IRQ 0（系统定时器），因其在传统 PC 架构中的特殊地位，即使在 `irqfixup=2` 模式下也始终尝试恢复。",
      "similarity": 0.5619146227836609,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 36,
          "end_line": 136,
          "content": [
            "bool irq_wait_for_poll(struct irq_desc *desc)",
            "\t__must_hold(&desc->lock)",
            "{",
            "\tif (WARN_ONCE(irq_poll_cpu == smp_processor_id(),",
            "\t\t      \"irq poll in progress on cpu %d for irq %d\\n\",",
            "\t\t      smp_processor_id(), desc->irq_data.irq))",
            "\t\treturn false;",
            "",
            "#ifdef CONFIG_SMP",
            "\tdo {",
            "\t\traw_spin_unlock(&desc->lock);",
            "\t\twhile (irqd_irq_inprogress(&desc->irq_data))",
            "\t\t\tcpu_relax();",
            "\t\traw_spin_lock(&desc->lock);",
            "\t} while (irqd_irq_inprogress(&desc->irq_data));",
            "\t/* Might have been disabled in meantime */",
            "\treturn !irqd_irq_disabled(&desc->irq_data) && desc->action;",
            "#else",
            "\treturn false;",
            "#endif",
            "}",
            "static int try_one_irq(struct irq_desc *desc, bool force)",
            "{",
            "\tirqreturn_t ret = IRQ_NONE;",
            "\tstruct irqaction *action;",
            "",
            "\traw_spin_lock(&desc->lock);",
            "",
            "\t/*",
            "\t * PER_CPU, nested thread interrupts and interrupts explicitly",
            "\t * marked polled are excluded from polling.",
            "\t */",
            "\tif (irq_settings_is_per_cpu(desc) ||",
            "\t    irq_settings_is_nested_thread(desc) ||",
            "\t    irq_settings_is_polled(desc))",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * Do not poll disabled interrupts unless the spurious",
            "\t * disabled poller asks explicitly.",
            "\t */",
            "\tif (irqd_irq_disabled(&desc->irq_data) && !force)",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * All handlers must agree on IRQF_SHARED, so we test just the",
            "\t * first.",
            "\t */",
            "\taction = desc->action;",
            "\tif (!action || !(action->flags & IRQF_SHARED) ||",
            "\t    (action->flags & __IRQF_TIMER))",
            "\t\tgoto out;",
            "",
            "\t/* Already running on another processor */",
            "\tif (irqd_irq_inprogress(&desc->irq_data)) {",
            "\t\t/*",
            "\t\t * Already running: If it is shared get the other",
            "\t\t * CPU to go looking for our mystery interrupt too",
            "\t\t */",
            "\t\tdesc->istate |= IRQS_PENDING;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* Mark it poll in progress */",
            "\tdesc->istate |= IRQS_POLL_INPROGRESS;",
            "\tdo {",
            "\t\tif (handle_irq_event(desc) == IRQ_HANDLED)",
            "\t\t\tret = IRQ_HANDLED;",
            "\t\t/* Make sure that there is still a valid action */",
            "\t\taction = desc->action;",
            "\t} while ((desc->istate & IRQS_PENDING) && action);",
            "\tdesc->istate &= ~IRQS_POLL_INPROGRESS;",
            "out:",
            "\traw_spin_unlock(&desc->lock);",
            "\treturn ret == IRQ_HANDLED;",
            "}",
            "static int misrouted_irq(int irq)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tint i, ok = 0;",
            "",
            "\tif (atomic_inc_return(&irq_poll_active) != 1)",
            "\t\tgoto out;",
            "",
            "\tirq_poll_cpu = smp_processor_id();",
            "",
            "\tfor_each_irq_desc(i, desc) {",
            "\t\tif (!i)",
            "\t\t\t continue;",
            "",
            "\t\tif (i == irq)\t/* Already tried */",
            "\t\t\tcontinue;",
            "",
            "\t\tif (try_one_irq(desc, false))",
            "\t\t\tok = 1;",
            "\t}",
            "out:",
            "\tatomic_dec(&irq_poll_active);",
            "\t/* So the caller can adjust the irq error counts */",
            "\treturn ok;",
            "}"
          ],
          "function_name": "irq_wait_for_poll, try_one_irq, misrouted_irq",
          "description": "实现了irq_wait_for_poll用于等待轮询完成，try_one_irq尝试处理单个中断，misrouted_irq尝试修复误路由中断。",
          "similarity": 0.5450591444969177
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 144,
          "end_line": 256,
          "content": [
            "static void poll_spurious_irqs(struct timer_list *unused)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tint i;",
            "",
            "\tif (atomic_inc_return(&irq_poll_active) != 1)",
            "\t\tgoto out;",
            "\tirq_poll_cpu = smp_processor_id();",
            "",
            "\tfor_each_irq_desc(i, desc) {",
            "\t\tunsigned int state;",
            "",
            "\t\tif (!i)",
            "\t\t\t continue;",
            "",
            "\t\t/* Racy but it doesn't matter */",
            "\t\tstate = desc->istate;",
            "\t\tbarrier();",
            "\t\tif (!(state & IRQS_SPURIOUS_DISABLED))",
            "\t\t\tcontinue;",
            "",
            "\t\tlocal_irq_disable();",
            "\t\ttry_one_irq(desc, true);",
            "\t\tlocal_irq_enable();",
            "\t}",
            "out:",
            "\tatomic_dec(&irq_poll_active);",
            "\tmod_timer(&poll_spurious_irq_timer,",
            "\t\t  jiffies + POLL_SPURIOUS_IRQ_INTERVAL);",
            "}",
            "static inline int bad_action_ret(irqreturn_t action_ret)",
            "{",
            "\tunsigned int r = action_ret;",
            "",
            "\tif (likely(r <= (IRQ_HANDLED | IRQ_WAKE_THREAD)))",
            "\t\treturn 0;",
            "\treturn 1;",
            "}",
            "static void __report_bad_irq(struct irq_desc *desc, irqreturn_t action_ret)",
            "{",
            "\tunsigned int irq = irq_desc_get_irq(desc);",
            "\tstruct irqaction *action;",
            "\tunsigned long flags;",
            "",
            "\tif (bad_action_ret(action_ret)) {",
            "\t\tprintk(KERN_ERR \"irq event %d: bogus return value %x\\n\",",
            "\t\t\t\tirq, action_ret);",
            "\t} else {",
            "\t\tprintk(KERN_ERR \"irq %d: nobody cared (try booting with \"",
            "\t\t\t\t\"the \\\"irqpoll\\\" option)\\n\", irq);",
            "\t}",
            "\tdump_stack();",
            "\tprintk(KERN_ERR \"handlers:\\n\");",
            "",
            "\t/*",
            "\t * We need to take desc->lock here. note_interrupt() is called",
            "\t * w/o desc->lock held, but IRQ_PROGRESS set. We might race",
            "\t * with something else removing an action. It's ok to take",
            "\t * desc->lock here. See synchronize_irq().",
            "\t */",
            "\traw_spin_lock_irqsave(&desc->lock, flags);",
            "\tfor_each_action_of_desc(desc, action) {",
            "\t\tprintk(KERN_ERR \"[<%p>] %ps\", action->handler, action->handler);",
            "\t\tif (action->thread_fn)",
            "\t\t\tprintk(KERN_CONT \" threaded [<%p>] %ps\",",
            "\t\t\t\t\taction->thread_fn, action->thread_fn);",
            "\t\tprintk(KERN_CONT \"\\n\");",
            "\t}",
            "\traw_spin_unlock_irqrestore(&desc->lock, flags);",
            "}",
            "static void report_bad_irq(struct irq_desc *desc, irqreturn_t action_ret)",
            "{",
            "\tstatic int count = 100;",
            "",
            "\tif (count > 0) {",
            "\t\tcount--;",
            "\t\t__report_bad_irq(desc, action_ret);",
            "\t}",
            "}",
            "static inline int",
            "try_misrouted_irq(unsigned int irq, struct irq_desc *desc,",
            "\t\t  irqreturn_t action_ret)",
            "{",
            "\tstruct irqaction *action;",
            "",
            "\tif (!irqfixup)",
            "\t\treturn 0;",
            "",
            "\t/* We didn't actually handle the IRQ - see if it was misrouted? */",
            "\tif (action_ret == IRQ_NONE)",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * But for 'irqfixup == 2' we also do it for handled interrupts if",
            "\t * they are marked as IRQF_IRQPOLL (or for irq zero, which is the",
            "\t * traditional PC timer interrupt.. Legacy)",
            "\t */",
            "\tif (irqfixup < 2)",
            "\t\treturn 0;",
            "",
            "\tif (!irq)",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * Since we don't get the descriptor lock, \"action\" can",
            "\t * change under us.  We don't really care, but we don't",
            "\t * want to follow a NULL pointer. So tell the compiler to",
            "\t * just load it once by using a barrier.",
            "\t */",
            "\taction = desc->action;",
            "\tbarrier();",
            "\treturn action && (action->flags & IRQF_IRQPOLL);",
            "}"
          ],
          "function_name": "poll_spurious_irqs, bad_action_ret, __report_bad_irq, report_bad_irq, try_misrouted_irq",
          "description": "poll_spurious_irqs定时扫描中断描述符并处理疑似虚假中断，包含错误报告辅助函数和误路由检测逻辑。",
          "similarity": 0.531690239906311
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 1,
          "end_line": 35,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 1992, 1998-2004 Linus Torvalds, Ingo Molnar",
            " *",
            " * This file contains spurious interrupt handling.",
            " */",
            "",
            "#include <linux/jiffies.h>",
            "#include <linux/irq.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/timer.h>",
            "",
            "#include \"internals.h\"",
            "",
            "static int irqfixup __read_mostly;",
            "",
            "#define POLL_SPURIOUS_IRQ_INTERVAL (HZ/10)",
            "static void poll_spurious_irqs(struct timer_list *unused);",
            "static DEFINE_TIMER(poll_spurious_irq_timer, poll_spurious_irqs);",
            "static int irq_poll_cpu;",
            "static atomic_t irq_poll_active;",
            "",
            "/*",
            " * We wait here for a poller to finish.",
            " *",
            " * If the poll runs on this CPU, then we yell loudly and return",
            " * false. That will leave the interrupt line disabled in the worst",
            " * case, but it should never happen.",
            " *",
            " * We wait until the poller is done and then recheck disabled and",
            " * action (about to be disabled). Only if it's still active, we return",
            " * true and let the handler run.",
            " */"
          ],
          "function_name": null,
          "description": "定义了处理虚假中断的相关变量和定时器，用于周期性地扫描和处理可能存在的虚假中断。",
          "similarity": 0.5133386254310608
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 272,
          "end_line": 432,
          "content": [
            "void note_interrupt(struct irq_desc *desc, irqreturn_t action_ret)",
            "{",
            "\tunsigned int irq;",
            "",
            "\tif (desc->istate & IRQS_POLL_INPROGRESS ||",
            "\t    irq_settings_is_polled(desc))",
            "\t\treturn;",
            "",
            "\tif (bad_action_ret(action_ret)) {",
            "\t\treport_bad_irq(desc, action_ret);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * We cannot call note_interrupt from the threaded handler",
            "\t * because we need to look at the compound of all handlers",
            "\t * (primary and threaded). Aside of that in the threaded",
            "\t * shared case we have no serialization against an incoming",
            "\t * hardware interrupt while we are dealing with a threaded",
            "\t * result.",
            "\t *",
            "\t * So in case a thread is woken, we just note the fact and",
            "\t * defer the analysis to the next hardware interrupt.",
            "\t *",
            "\t * The threaded handlers store whether they successfully",
            "\t * handled an interrupt and we check whether that number",
            "\t * changed versus the last invocation.",
            "\t *",
            "\t * We could handle all interrupts with the delayed by one",
            "\t * mechanism, but for the non forced threaded case we'd just",
            "\t * add pointless overhead to the straight hardirq interrupts",
            "\t * for the sake of a few lines less code.",
            "\t */",
            "\tif (action_ret & IRQ_WAKE_THREAD) {",
            "\t\t/*",
            "\t\t * There is a thread woken. Check whether one of the",
            "\t\t * shared primary handlers returned IRQ_HANDLED. If",
            "\t\t * not we defer the spurious detection to the next",
            "\t\t * interrupt.",
            "\t\t */",
            "\t\tif (action_ret == IRQ_WAKE_THREAD) {",
            "\t\t\tint handled;",
            "\t\t\t/*",
            "\t\t\t * We use bit 31 of thread_handled_last to",
            "\t\t\t * denote the deferred spurious detection",
            "\t\t\t * active. No locking necessary as",
            "\t\t\t * thread_handled_last is only accessed here",
            "\t\t\t * and we have the guarantee that hard",
            "\t\t\t * interrupts are not reentrant.",
            "\t\t\t */",
            "\t\t\tif (!(desc->threads_handled_last & SPURIOUS_DEFERRED)) {",
            "\t\t\t\tdesc->threads_handled_last |= SPURIOUS_DEFERRED;",
            "\t\t\t\treturn;",
            "\t\t\t}",
            "\t\t\t/*",
            "\t\t\t * Check whether one of the threaded handlers",
            "\t\t\t * returned IRQ_HANDLED since the last",
            "\t\t\t * interrupt happened.",
            "\t\t\t *",
            "\t\t\t * For simplicity we just set bit 31, as it is",
            "\t\t\t * set in threads_handled_last as well. So we",
            "\t\t\t * avoid extra masking. And we really do not",
            "\t\t\t * care about the high bits of the handled",
            "\t\t\t * count. We just care about the count being",
            "\t\t\t * different than the one we saw before.",
            "\t\t\t */",
            "\t\t\thandled = atomic_read(&desc->threads_handled);",
            "\t\t\thandled |= SPURIOUS_DEFERRED;",
            "\t\t\tif (handled != desc->threads_handled_last) {",
            "\t\t\t\taction_ret = IRQ_HANDLED;",
            "\t\t\t\t/*",
            "\t\t\t\t * Note: We keep the SPURIOUS_DEFERRED",
            "\t\t\t\t * bit set. We are handling the",
            "\t\t\t\t * previous invocation right now.",
            "\t\t\t\t * Keep it for the current one, so the",
            "\t\t\t\t * next hardware interrupt will",
            "\t\t\t\t * account for it.",
            "\t\t\t\t */",
            "\t\t\t\tdesc->threads_handled_last = handled;",
            "\t\t\t} else {",
            "\t\t\t\t/*",
            "\t\t\t\t * None of the threaded handlers felt",
            "\t\t\t\t * responsible for the last interrupt",
            "\t\t\t\t *",
            "\t\t\t\t * We keep the SPURIOUS_DEFERRED bit",
            "\t\t\t\t * set in threads_handled_last as we",
            "\t\t\t\t * need to account for the current",
            "\t\t\t\t * interrupt as well.",
            "\t\t\t\t */",
            "\t\t\t\taction_ret = IRQ_NONE;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * One of the primary handlers returned",
            "\t\t\t * IRQ_HANDLED. So we don't care about the",
            "\t\t\t * threaded handlers on the same line. Clear",
            "\t\t\t * the deferred detection bit.",
            "\t\t\t *",
            "\t\t\t * In theory we could/should check whether the",
            "\t\t\t * deferred bit is set and take the result of",
            "\t\t\t * the previous run into account here as",
            "\t\t\t * well. But it's really not worth the",
            "\t\t\t * trouble. If every other interrupt is",
            "\t\t\t * handled we never trigger the spurious",
            "\t\t\t * detector. And if this is just the one out",
            "\t\t\t * of 100k unhandled ones which is handled",
            "\t\t\t * then we merily delay the spurious detection",
            "\t\t\t * by one hard interrupt. Not a real problem.",
            "\t\t\t */",
            "\t\t\tdesc->threads_handled_last &= ~SPURIOUS_DEFERRED;",
            "\t\t}",
            "\t}",
            "",
            "\tif (unlikely(action_ret == IRQ_NONE)) {",
            "\t\t/*",
            "\t\t * If we are seeing only the odd spurious IRQ caused by",
            "\t\t * bus asynchronicity then don't eventually trigger an error,",
            "\t\t * otherwise the counter becomes a doomsday timer for otherwise",
            "\t\t * working systems",
            "\t\t */",
            "\t\tif (time_after(jiffies, desc->last_unhandled + HZ/10))",
            "\t\t\tdesc->irqs_unhandled = 1;",
            "\t\telse",
            "\t\t\tdesc->irqs_unhandled++;",
            "\t\tdesc->last_unhandled = jiffies;",
            "\t}",
            "",
            "\tirq = irq_desc_get_irq(desc);",
            "\tif (unlikely(try_misrouted_irq(irq, desc, action_ret))) {",
            "\t\tint ok = misrouted_irq(irq);",
            "\t\tif (action_ret == IRQ_NONE)",
            "\t\t\tdesc->irqs_unhandled -= ok;",
            "\t}",
            "",
            "\tif (likely(!desc->irqs_unhandled))",
            "\t\treturn;",
            "",
            "\t/* Now getting into unhandled irq detection */",
            "\tdesc->irq_count++;",
            "\tif (likely(desc->irq_count < 100000))",
            "\t\treturn;",
            "",
            "\tdesc->irq_count = 0;",
            "\tif (unlikely(desc->irqs_unhandled > 99900)) {",
            "\t\t/*",
            "\t\t * The interrupt is stuck",
            "\t\t */",
            "\t\t__report_bad_irq(desc, action_ret);",
            "\t\t/*",
            "\t\t * Now kill the IRQ",
            "\t\t */",
            "\t\tprintk(KERN_EMERG \"Disabling IRQ #%d\\n\", irq);",
            "\t\tdesc->istate |= IRQS_SPURIOUS_DISABLED;",
            "\t\tdesc->depth++;",
            "\t\tirq_disable(desc);",
            "",
            "\t\tmod_timer(&poll_spurious_irq_timer,",
            "\t\t\t  jiffies + POLL_SPURIOUS_IRQ_INTERVAL);",
            "\t}",
            "\tdesc->irqs_unhandled = 0;",
            "}"
          ],
          "function_name": "note_interrupt",
          "description": "note_interrupt记录中断事件，检测未处理中断并触发报告，处理线程唤醒场景下的特殊逻辑。",
          "similarity": 0.48471441864967346
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 436,
          "end_line": 467,
          "content": [
            "int noirqdebug_setup(char *str)",
            "{",
            "\tnoirqdebug = 1;",
            "\tprintk(KERN_INFO \"IRQ lockup detection disabled\\n\");",
            "",
            "\treturn 1;",
            "}",
            "static int __init irqfixup_setup(char *str)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {",
            "\t\tpr_warn(\"irqfixup boot option not supported with PREEMPT_RT\\n\");",
            "\t\treturn 1;",
            "\t}",
            "\tirqfixup = 1;",
            "\tprintk(KERN_WARNING \"Misrouted IRQ fixup support enabled.\\n\");",
            "\tprintk(KERN_WARNING \"This may impact system performance.\\n\");",
            "",
            "\treturn 1;",
            "}",
            "static int __init irqpoll_setup(char *str)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {",
            "\t\tpr_warn(\"irqpoll boot option not supported with PREEMPT_RT\\n\");",
            "\t\treturn 1;",
            "\t}",
            "\tirqfixup = 2;",
            "\tprintk(KERN_WARNING \"Misrouted IRQ fixup and polling support \"",
            "\t\t\t\t\"enabled\\n\");",
            "\tprintk(KERN_WARNING \"This may significantly impact system \"",
            "\t\t\t\t\"performance\\n\");",
            "\treturn 1;",
            "}"
          ],
          "function_name": "noirqdebug_setup, irqfixup_setup, irqpoll_setup",
          "description": "提供启动参数配置接口，用于启用或禁用irqfixup和irqpoll功能，并输出相应警告信息。",
          "similarity": 0.4764223098754883
        }
      ]
    }
  ]
}