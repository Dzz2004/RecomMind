{
  "query": "CPU缓存写回和写直达的性能影响",
  "timestamp": "2025-12-26 01:46:39",
  "retrieved_files": [
    {
      "source_file": "kernel/bpf/cpumap.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:06:36\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\cpumap.c`\n\n---\n\n# bpf/cpumap.c 技术文档\n\n## 文件概述\n\n`bpf/cpumap.c` 实现了 BPF CPU Map（cpumap）这一内核数据结构，主要用于 XDP（eXpress Data Path）框架中的跨 CPU 重定向功能。该机制允许 XDP 程序通过 `bpf_redirect_map()` 辅助函数将原始 XDP 数据帧重定向到指定的目标 CPU，由目标 CPU 上运行的专用内核线程（kthread）接收并将其转换为 `sk_buff` 后送入常规网络协议栈处理。cpumap 的核心目标是实现高性能、低延迟的网络预过滤，通过将 XDP 处理阶段与主网络栈解耦，提升系统在 10Gbps 及以上速率下的可扩展性和隔离性。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_cpu_map`**  \n  表示整个 CPU Map 对象，继承自 `struct bpf_map`，包含一个指向 `bpf_cpu_map_entry` 指针数组的 `cpu_map` 成员，用于按 CPU ID 索引目标条目。\n\n- **`struct bpf_cpu_map_entry`**  \n  表示映射到单个目标 CPU 的条目，关键成员包括：\n  - `cpu`：目标 CPU 编号\n  - `queue`：`ptr_ring` 类型的无锁环形缓冲队列，用于暂存重定向的 XDP 帧\n  - `kthread`：绑定到该 CPU 的消费者内核线程\n  - `prog`：可选的附加 BPF 程序，在帧入队后、送入协议栈前执行二次处理\n  - `bulkq`：每 CPU 的批量入队缓存（`xdp_bulk_queue`），用于提升入队性能\n\n- **`struct xdp_bulk_queue`**  \n  每 CPU 的批量队列结构，用于暂存最多 `CPU_MAP_BULK_SIZE`（默认 8）个待入队的 XDP 帧，减少对共享 `ptr_ring` 的频繁访问。\n\n### 主要函数\n\n- **`cpu_map_alloc()`**  \n  分配并初始化一个新的 CPU Map 实例，校验属性合法性（如 key/value 大小、最大条目数不超过 `NR_CPUS`）。\n\n- **`cpu_map_kthread_run()`**  \n  目标 CPU 上运行的消费者内核线程主循环，从 `ptr_ring` 队列批量消费 XDP 帧，执行可选 BPF 程序，并将结果帧转换为 `sk_buff` 送入网络栈。\n\n- **`cpu_map_bpf_prog_run_xdp()`**  \n  在消费线程中执行附加的 XDP 类型 BPF 程序，支持 `XDP_PASS`、`XDP_REDIRECT`、`XDP_DROP` 等动作。\n\n- **`cpu_map_bpf_prog_run_skb()`**  \n  处理已转换为 `sk_buff` 的数据包（通常来自重定向失败或特殊路径），执行通用 XDP BPF 程序。\n\n- **`__cpu_map_ring_cleanup()`**  \n  安全清理 `ptr_ring` 队列中的残留帧，确保资源正确释放。\n\n## 关键实现\n\n### 无锁批量入队与消费\n\n- **生产者侧（XDP 程序）**：使用每 CPU 的 `xdp_bulk_queue` 缓存待入队帧。当缓存满或需要 flush 时，一次性将批量帧原子地推入目标 CPU 条目的 `ptr_ring` 队列，减少锁竞争。\n- **消费者侧（kthread）**：每个 `bpf_cpu_map_entry` 绑定一个专用 kthread，独占消费其 `ptr_ring`。通过 `__ptr_ring_consume_batched()` 批量获取帧，提升吞吐。\n\n### 跨 CPU 重定向流程\n\n1. XDP 程序调用 `bpf_redirect_map(map, cpu_id, 0)`。\n2. 内核将当前 XDP 帧暂存到当前 CPU 对应目标 `cpu_id` 的 `bulkq` 中。\n3. 在驱动 `->poll()` 结束或显式 flush 时，将 `bulkq` 中的帧批量推入目标 CPU 的 `ptr_ring`。\n4. 目标 CPU 的 kthread 被唤醒，消费帧，执行可选 BPF 程序，转换为 `sk_buff` 并调用 `netif_receive_skb_list()` 送入协议栈。\n\n### BPF 程序二次处理\n\n每个 `bpf_cpu_map_entry` 可关联一个 BPF 程序。消费线程在处理帧前会执行该程序，支持进一步过滤、修改或重定向（如再次重定向到设备或另一个 CPU），增强了灵活性。\n\n### 内存与资源管理\n\n- 使用 `__ptr_set_bit(0, &ptr)` 标记 `sk_buff` 指针（最低位为 1），普通 XDP 帧指针最低位为 0，便于在清理时区分类型。\n- kthread 在退出前确保队列为空，防止内存泄漏。\n- 通过 RCU 机制安全地更新和释放 map 条目。\n\n## 依赖关系\n\n- **BPF 子系统**：依赖 `bpf.h`、`filter.h` 提供 map 基础框架、BPF 程序执行接口。\n- **XDP 框架**：依赖 `xdp.h`、`xdp_frame` 结构及 `xdp_do_redirect()` 等重定向机制。\n- **网络核心**：使用 `netdevice.h` 的 `netif_receive_skb_list()` 将数据包送入协议栈。\n- **内核同步原语**：使用 `ptr_ring`（无锁环形缓冲区）、`completion`（线程启动同步）、`rcu_work`（延迟释放）。\n- **调度与线程**：依赖 `kthread` 创建 CPU 绑定的消费者线程。\n- **追踪**：集成 `trace/events/xdp.h` 提供 XDP 事件追踪。\n\n## 使用场景\n\n- **高性能网络预过滤**：在专用 CPU 上运行 XDP 程序进行 DDoS 防御、ACL 过滤等，将合法流量重定向到其他 CPU 的协议栈处理，避免主 CPU 过载。\n- **负载均衡**：将流量按策略分发到多个 CPU，提升多核系统的网络处理能力。\n- **服务链（Service Chaining）**：通过级联 cpumap 和 devmap，构建复杂的流量处理流水线。\n- **隔离关键路径**：将 XDP 处理与应用层网络处理隔离到不同 CPU，减少相互干扰，保障低延迟。",
      "similarity": 0.5822213292121887,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/bpf/cpumap.c",
          "start_line": 564,
          "end_line": 678,
          "content": [
            "static void cpu_map_free(struct bpf_map *map)",
            "{",
            "\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);",
            "\tu32 i;",
            "",
            "\t/* At this point bpf_prog->aux->refcnt == 0 and this map->refcnt == 0,",
            "\t * so the bpf programs (can be more than one that used this map) were",
            "\t * disconnected from events. Wait for outstanding critical sections in",
            "\t * these programs to complete. synchronize_rcu() below not only",
            "\t * guarantees no further \"XDP/bpf-side\" reads against",
            "\t * bpf_cpu_map->cpu_map, but also ensure pending flush operations",
            "\t * (if any) are completed.",
            "\t */",
            "\tsynchronize_rcu();",
            "",
            "\t/* The only possible user of bpf_cpu_map_entry is",
            "\t * cpu_map_kthread_run().",
            "\t */",
            "\tfor (i = 0; i < cmap->map.max_entries; i++) {",
            "\t\tstruct bpf_cpu_map_entry *rcpu;",
            "",
            "\t\trcpu = rcu_dereference_raw(cmap->cpu_map[i]);",
            "\t\tif (!rcpu)",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Stop kthread and cleanup entry directly */",
            "\t\t__cpu_map_entry_free(&rcpu->free_work.work);",
            "\t}",
            "\tbpf_map_area_free(cmap->cpu_map);",
            "\tbpf_map_area_free(cmap);",
            "}",
            "static int cpu_map_get_next_key(struct bpf_map *map, void *key, void *next_key)",
            "{",
            "\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);",
            "\tu32 index = key ? *(u32 *)key : U32_MAX;",
            "\tu32 *next = next_key;",
            "",
            "\tif (index >= cmap->map.max_entries) {",
            "\t\t*next = 0;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (index == cmap->map.max_entries - 1)",
            "\t\treturn -ENOENT;",
            "\t*next = index + 1;",
            "\treturn 0;",
            "}",
            "static long cpu_map_redirect(struct bpf_map *map, u64 index, u64 flags)",
            "{",
            "\treturn __bpf_xdp_redirect_map(map, index, flags, 0,",
            "\t\t\t\t      __cpu_map_lookup_elem);",
            "}",
            "static u64 cpu_map_mem_usage(const struct bpf_map *map)",
            "{",
            "\tu64 usage = sizeof(struct bpf_cpu_map);",
            "",
            "\t/* Currently the dynamically allocated elements are not counted */",
            "\tusage += (u64)map->max_entries * sizeof(struct bpf_cpu_map_entry *);",
            "\treturn usage;",
            "}",
            "static void bq_flush_to_queue(struct xdp_bulk_queue *bq)",
            "{",
            "\tstruct bpf_cpu_map_entry *rcpu = bq->obj;",
            "\tunsigned int processed = 0, drops = 0;",
            "\tconst int to_cpu = rcpu->cpu;",
            "\tstruct ptr_ring *q;",
            "\tint i;",
            "",
            "\tif (unlikely(!bq->count))",
            "\t\treturn;",
            "",
            "\tq = rcpu->queue;",
            "\tspin_lock(&q->producer_lock);",
            "",
            "\tfor (i = 0; i < bq->count; i++) {",
            "\t\tstruct xdp_frame *xdpf = bq->q[i];",
            "\t\tint err;",
            "",
            "\t\terr = __ptr_ring_produce(q, xdpf);",
            "\t\tif (err) {",
            "\t\t\tdrops++;",
            "\t\t\txdp_return_frame_rx_napi(xdpf);",
            "\t\t}",
            "\t\tprocessed++;",
            "\t}",
            "\tbq->count = 0;",
            "\tspin_unlock(&q->producer_lock);",
            "",
            "\t__list_del_clearprev(&bq->flush_node);",
            "",
            "\t/* Feedback loop via tracepoints */",
            "\ttrace_xdp_cpumap_enqueue(rcpu->map_id, processed, drops, to_cpu);",
            "}",
            "static void bq_enqueue(struct bpf_cpu_map_entry *rcpu, struct xdp_frame *xdpf)",
            "{",
            "\tstruct list_head *flush_list = this_cpu_ptr(&cpu_map_flush_list);",
            "\tstruct xdp_bulk_queue *bq = this_cpu_ptr(rcpu->bulkq);",
            "",
            "\tif (unlikely(bq->count == CPU_MAP_BULK_SIZE))",
            "\t\tbq_flush_to_queue(bq);",
            "",
            "\t/* Notice, xdp_buff/page MUST be queued here, long enough for",
            "\t * driver to code invoking us to finished, due to driver",
            "\t * (e.g. ixgbe) recycle tricks based on page-refcnt.",
            "\t *",
            "\t * Thus, incoming xdp_frame is always queued here (else we race",
            "\t * with another CPU on page-refcnt and remaining driver code).",
            "\t * Queue time is very short, as driver will invoke flush",
            "\t * operation, when completing napi->poll call.",
            "\t */",
            "\tbq->q[bq->count++] = xdpf;",
            "",
            "\tif (!bq->flush_node.prev)",
            "\t\tlist_add(&bq->flush_node, flush_list);",
            "}"
          ],
          "function_name": "cpu_map_free, cpu_map_get_next_key, cpu_map_redirect, cpu_map_mem_usage, bq_flush_to_queue, bq_enqueue",
          "description": "提供映射销毁(cpu_map_free)、键遍历(cpu_map_get_next_key)、内存使用统计(cpu_map_mem_usage)等功能，包含批量队列刷新(bq_flush_to_queue)和帧入队(bq_enqueue)的底层实现。",
          "similarity": 0.5259584188461304
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/bpf/cpumap.c",
          "start_line": 728,
          "end_line": 774,
          "content": [
            "int cpu_map_enqueue(struct bpf_cpu_map_entry *rcpu, struct xdp_frame *xdpf,",
            "\t\t    struct net_device *dev_rx)",
            "{",
            "\t/* Info needed when constructing SKB on remote CPU */",
            "\txdpf->dev_rx = dev_rx;",
            "",
            "\tbq_enqueue(rcpu, xdpf);",
            "\treturn 0;",
            "}",
            "int cpu_map_generic_redirect(struct bpf_cpu_map_entry *rcpu,",
            "\t\t\t     struct sk_buff *skb)",
            "{",
            "\tint ret;",
            "",
            "\t__skb_pull(skb, skb->mac_len);",
            "\tskb_set_redirected(skb, false);",
            "\t__ptr_set_bit(0, &skb);",
            "",
            "\tret = ptr_ring_produce(rcpu->queue, skb);",
            "\tif (ret < 0)",
            "\t\tgoto trace;",
            "",
            "\twake_up_process(rcpu->kthread);",
            "trace:",
            "\ttrace_xdp_cpumap_enqueue(rcpu->map_id, !ret, !!ret, rcpu->cpu);",
            "\treturn ret;",
            "}",
            "void __cpu_map_flush(void)",
            "{",
            "\tstruct list_head *flush_list = this_cpu_ptr(&cpu_map_flush_list);",
            "\tstruct xdp_bulk_queue *bq, *tmp;",
            "",
            "\tlist_for_each_entry_safe(bq, tmp, flush_list, flush_node) {",
            "\t\tbq_flush_to_queue(bq);",
            "",
            "\t\t/* If already running, costs spin_lock_irqsave + smb_mb */",
            "\t\twake_up_process(bq->obj->kthread);",
            "\t}",
            "}",
            "static int __init cpu_map_init(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tINIT_LIST_HEAD(&per_cpu(cpu_map_flush_list, cpu));",
            "\treturn 0;",
            "}"
          ],
          "function_name": "cpu_map_enqueue, cpu_map_generic_redirect, __cpu_map_flush, cpu_map_init",
          "description": "该代码块实现了CPU映射机制中的数据分发与同步功能。  \n`cpu_map_enqueue`将XDP帧注册到远程CPU队列，`cpu_map_generic_redirect`处理skb并将其分发至目标队列，`__cpu_map_flush`负责批量刷新队列以避免阻塞，`cpu_map_init`初始化各CPU的刷新链表。  \n上下文不完整：关键函数如`bq_enqueue`、`ptr_ring_produce`及结构体`struct bpf_cpu_map_entry`等未在片段中定义。",
          "similarity": 0.5207474827766418
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/cpumap.c",
          "start_line": 1,
          "end_line": 116,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* bpf/cpumap.c",
            " *",
            " * Copyright (c) 2017 Jesper Dangaard Brouer, Red Hat Inc.",
            " */",
            "",
            "/**",
            " * DOC: cpu map",
            " * The 'cpumap' is primarily used as a backend map for XDP BPF helper",
            " * call bpf_redirect_map() and XDP_REDIRECT action, like 'devmap'.",
            " *",
            " * Unlike devmap which redirects XDP frames out to another NIC device,",
            " * this map type redirects raw XDP frames to another CPU.  The remote",
            " * CPU will do SKB-allocation and call the normal network stack.",
            " */",
            "/*",
            " * This is a scalability and isolation mechanism, that allow",
            " * separating the early driver network XDP layer, from the rest of the",
            " * netstack, and assigning dedicated CPUs for this stage.  This",
            " * basically allows for 10G wirespeed pre-filtering via bpf.",
            " */",
            "#include <linux/bitops.h>",
            "#include <linux/bpf.h>",
            "#include <linux/filter.h>",
            "#include <linux/ptr_ring.h>",
            "#include <net/xdp.h>",
            "",
            "#include <linux/sched.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/kthread.h>",
            "#include <linux/completion.h>",
            "#include <trace/events/xdp.h>",
            "#include <linux/btf_ids.h>",
            "",
            "#include <linux/netdevice.h>   /* netif_receive_skb_list */",
            "#include <linux/etherdevice.h> /* eth_type_trans */",
            "",
            "/* General idea: XDP packets getting XDP redirected to another CPU,",
            " * will maximum be stored/queued for one driver ->poll() call.  It is",
            " * guaranteed that queueing the frame and the flush operation happen on",
            " * same CPU.  Thus, cpu_map_flush operation can deduct via this_cpu_ptr()",
            " * which queue in bpf_cpu_map_entry contains packets.",
            " */",
            "",
            "#define CPU_MAP_BULK_SIZE 8  /* 8 == one cacheline on 64-bit archs */",
            "struct bpf_cpu_map_entry;",
            "struct bpf_cpu_map;",
            "",
            "struct xdp_bulk_queue {",
            "\tvoid *q[CPU_MAP_BULK_SIZE];",
            "\tstruct list_head flush_node;",
            "\tstruct bpf_cpu_map_entry *obj;",
            "\tunsigned int count;",
            "};",
            "",
            "/* Struct for every remote \"destination\" CPU in map */",
            "struct bpf_cpu_map_entry {",
            "\tu32 cpu;    /* kthread CPU and map index */",
            "\tint map_id; /* Back reference to map */",
            "",
            "\t/* XDP can run multiple RX-ring queues, need __percpu enqueue store */",
            "\tstruct xdp_bulk_queue __percpu *bulkq;",
            "",
            "\t/* Queue with potential multi-producers, and single-consumer kthread */",
            "\tstruct ptr_ring *queue;",
            "\tstruct task_struct *kthread;",
            "",
            "\tstruct bpf_cpumap_val value;",
            "\tstruct bpf_prog *prog;",
            "",
            "\tstruct completion kthread_running;",
            "\tstruct rcu_work free_work;",
            "};",
            "",
            "struct bpf_cpu_map {",
            "\tstruct bpf_map map;",
            "\t/* Below members specific for map type */",
            "\tstruct bpf_cpu_map_entry __rcu **cpu_map;",
            "};",
            "",
            "static DEFINE_PER_CPU(struct list_head, cpu_map_flush_list);",
            "",
            "static struct bpf_map *cpu_map_alloc(union bpf_attr *attr)",
            "{",
            "\tu32 value_size = attr->value_size;",
            "\tstruct bpf_cpu_map *cmap;",
            "",
            "\t/* check sanity of attributes */",
            "\tif (attr->max_entries == 0 || attr->key_size != 4 ||",
            "\t    (value_size != offsetofend(struct bpf_cpumap_val, qsize) &&",
            "\t     value_size != offsetofend(struct bpf_cpumap_val, bpf_prog.fd)) ||",
            "\t    attr->map_flags & ~BPF_F_NUMA_NODE)",
            "\t\treturn ERR_PTR(-EINVAL);",
            "",
            "\t/* Pre-limit array size based on NR_CPUS, not final CPU check */",
            "\tif (attr->max_entries > NR_CPUS)",
            "\t\treturn ERR_PTR(-E2BIG);",
            "",
            "\tcmap = bpf_map_area_alloc(sizeof(*cmap), NUMA_NO_NODE);",
            "\tif (!cmap)",
            "\t\treturn ERR_PTR(-ENOMEM);",
            "",
            "\tbpf_map_init_from_attr(&cmap->map, attr);",
            "",
            "\t/* Alloc array for possible remote \"destination\" CPUs */",
            "\tcmap->cpu_map = bpf_map_area_alloc(cmap->map.max_entries *",
            "\t\t\t\t\t   sizeof(struct bpf_cpu_map_entry *),",
            "\t\t\t\t\t   cmap->map.numa_node);",
            "\tif (!cmap->cpu_map) {",
            "\t\tbpf_map_area_free(cmap);",
            "\t\treturn ERR_PTR(-ENOMEM);",
            "\t}",
            "",
            "\treturn &cmap->map;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了cpu_map的数据结构及初始化函数，用于XDP帧跨CPU重定向。实现基于RCU的cpu_map_entry数组管理，支持通过BPF程序过滤并转发网络数据包到指定CPU。",
          "similarity": 0.5122421383857727
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/cpumap.c",
          "start_line": 367,
          "end_line": 472,
          "content": [
            "static int __cpu_map_load_bpf_program(struct bpf_cpu_map_entry *rcpu,",
            "\t\t\t\t      struct bpf_map *map, int fd)",
            "{",
            "\tstruct bpf_prog *prog;",
            "",
            "\tprog = bpf_prog_get_type(fd, BPF_PROG_TYPE_XDP);",
            "\tif (IS_ERR(prog))",
            "\t\treturn PTR_ERR(prog);",
            "",
            "\tif (prog->expected_attach_type != BPF_XDP_CPUMAP ||",
            "\t    !bpf_prog_map_compatible(map, prog)) {",
            "\t\tbpf_prog_put(prog);",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\trcpu->value.bpf_prog.id = prog->aux->id;",
            "\trcpu->prog = prog;",
            "",
            "\treturn 0;",
            "}",
            "static void __cpu_map_entry_free(struct work_struct *work)",
            "{",
            "\tstruct bpf_cpu_map_entry *rcpu;",
            "",
            "\t/* This cpu_map_entry have been disconnected from map and one",
            "\t * RCU grace-period have elapsed. Thus, XDP cannot queue any",
            "\t * new packets and cannot change/set flush_needed that can",
            "\t * find this entry.",
            "\t */",
            "\trcpu = container_of(to_rcu_work(work), struct bpf_cpu_map_entry, free_work);",
            "",
            "\t/* kthread_stop will wake_up_process and wait for it to complete.",
            "\t * cpu_map_kthread_run() makes sure the pointer ring is empty",
            "\t * before exiting.",
            "\t */",
            "\tkthread_stop(rcpu->kthread);",
            "",
            "\tif (rcpu->prog)",
            "\t\tbpf_prog_put(rcpu->prog);",
            "\t/* The queue should be empty at this point */",
            "\t__cpu_map_ring_cleanup(rcpu->queue);",
            "\tptr_ring_cleanup(rcpu->queue, NULL);",
            "\tkfree(rcpu->queue);",
            "\tfree_percpu(rcpu->bulkq);",
            "\tkfree(rcpu);",
            "}",
            "static void __cpu_map_entry_replace(struct bpf_cpu_map *cmap,",
            "\t\t\t\t    u32 key_cpu, struct bpf_cpu_map_entry *rcpu)",
            "{",
            "\tstruct bpf_cpu_map_entry *old_rcpu;",
            "",
            "\told_rcpu = unrcu_pointer(xchg(&cmap->cpu_map[key_cpu], RCU_INITIALIZER(rcpu)));",
            "\tif (old_rcpu) {",
            "\t\tINIT_RCU_WORK(&old_rcpu->free_work, __cpu_map_entry_free);",
            "\t\tqueue_rcu_work(system_wq, &old_rcpu->free_work);",
            "\t}",
            "}",
            "static long cpu_map_delete_elem(struct bpf_map *map, void *key)",
            "{",
            "\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);",
            "\tu32 key_cpu = *(u32 *)key;",
            "",
            "\tif (key_cpu >= map->max_entries)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* notice caller map_delete_elem() uses rcu_read_lock() */",
            "\t__cpu_map_entry_replace(cmap, key_cpu, NULL);",
            "\treturn 0;",
            "}",
            "static long cpu_map_update_elem(struct bpf_map *map, void *key, void *value,",
            "\t\t\t\tu64 map_flags)",
            "{",
            "\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);",
            "\tstruct bpf_cpumap_val cpumap_value = {};",
            "\tstruct bpf_cpu_map_entry *rcpu;",
            "\t/* Array index key correspond to CPU number */",
            "\tu32 key_cpu = *(u32 *)key;",
            "",
            "\tmemcpy(&cpumap_value, value, map->value_size);",
            "",
            "\tif (unlikely(map_flags > BPF_EXIST))",
            "\t\treturn -EINVAL;",
            "\tif (unlikely(key_cpu >= cmap->map.max_entries))",
            "\t\treturn -E2BIG;",
            "\tif (unlikely(map_flags == BPF_NOEXIST))",
            "\t\treturn -EEXIST;",
            "\tif (unlikely(cpumap_value.qsize > 16384)) /* sanity limit on qsize */",
            "\t\treturn -EOVERFLOW;",
            "",
            "\t/* Make sure CPU is a valid possible cpu */",
            "\tif (key_cpu >= nr_cpumask_bits || !cpu_possible(key_cpu))",
            "\t\treturn -ENODEV;",
            "",
            "\tif (cpumap_value.qsize == 0) {",
            "\t\trcpu = NULL; /* Same as deleting */",
            "\t} else {",
            "\t\t/* Updating qsize cause re-allocation of bpf_cpu_map_entry */",
            "\t\trcpu = __cpu_map_entry_alloc(map, &cpumap_value, key_cpu);",
            "\t\tif (!rcpu)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\trcu_read_lock();",
            "\t__cpu_map_entry_replace(cmap, key_cpu, rcpu);",
            "\trcu_read_unlock();",
            "\treturn 0;",
            "}"
          ],
          "function_name": "__cpu_map_load_bpf_program, __cpu_map_entry_free, __cpu_map_entry_replace, cpu_map_delete_elem, cpu_map_update_elem",
          "description": "实现cpu_map_entry的生命周期管理，包括BPF程序加载验证(__cpu_map_load_bpf_program)、条目替换(__cpu_map_entry_replace)、资源释放(__cpu_map_entry_free)等核心操作。",
          "similarity": 0.5059530735015869
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/cpumap.c",
          "start_line": 238,
          "end_line": 364,
          "content": [
            "static int cpu_map_bpf_prog_run(struct bpf_cpu_map_entry *rcpu, void **frames,",
            "\t\t\t\tint xdp_n, struct xdp_cpumap_stats *stats,",
            "\t\t\t\tstruct list_head *list)",
            "{",
            "\tint nframes;",
            "",
            "\tif (!rcpu->prog)",
            "\t\treturn xdp_n;",
            "",
            "\trcu_read_lock_bh();",
            "",
            "\tnframes = cpu_map_bpf_prog_run_xdp(rcpu, frames, xdp_n, stats);",
            "",
            "\tif (stats->redirect)",
            "\t\txdp_do_flush();",
            "",
            "\tif (unlikely(!list_empty(list)))",
            "\t\tcpu_map_bpf_prog_run_skb(rcpu, list, stats);",
            "",
            "\trcu_read_unlock_bh(); /* resched point, may call do_softirq() */",
            "",
            "\treturn nframes;",
            "}",
            "static int cpu_map_kthread_run(void *data)",
            "{",
            "\tstruct bpf_cpu_map_entry *rcpu = data;",
            "\tunsigned long last_qs = jiffies;",
            "",
            "\tcomplete(&rcpu->kthread_running);",
            "\tset_current_state(TASK_INTERRUPTIBLE);",
            "",
            "\t/* When kthread gives stop order, then rcpu have been disconnected",
            "\t * from map, thus no new packets can enter. Remaining in-flight",
            "\t * per CPU stored packets are flushed to this queue.  Wait honoring",
            "\t * kthread_stop signal until queue is empty.",
            "\t */",
            "\twhile (!kthread_should_stop() || !__ptr_ring_empty(rcpu->queue)) {",
            "\t\tstruct xdp_cpumap_stats stats = {}; /* zero stats */",
            "\t\tunsigned int kmem_alloc_drops = 0, sched = 0;",
            "\t\tgfp_t gfp = __GFP_ZERO | GFP_ATOMIC;",
            "\t\tint i, n, m, nframes, xdp_n;",
            "\t\tvoid *frames[CPUMAP_BATCH];",
            "\t\tvoid *skbs[CPUMAP_BATCH];",
            "\t\tLIST_HEAD(list);",
            "",
            "\t\t/* Release CPU reschedule checks */",
            "\t\tif (__ptr_ring_empty(rcpu->queue)) {",
            "\t\t\tset_current_state(TASK_INTERRUPTIBLE);",
            "\t\t\t/* Recheck to avoid lost wake-up */",
            "\t\t\tif (__ptr_ring_empty(rcpu->queue)) {",
            "\t\t\t\tschedule();",
            "\t\t\t\tsched = 1;",
            "\t\t\t\tlast_qs = jiffies;",
            "\t\t\t} else {",
            "\t\t\t\t__set_current_state(TASK_RUNNING);",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\trcu_softirq_qs_periodic(last_qs);",
            "\t\t\tsched = cond_resched();",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * The bpf_cpu_map_entry is single consumer, with this",
            "\t\t * kthread CPU pinned. Lockless access to ptr_ring",
            "\t\t * consume side valid as no-resize allowed of queue.",
            "\t\t */",
            "\t\tn = __ptr_ring_consume_batched(rcpu->queue, frames,",
            "\t\t\t\t\t       CPUMAP_BATCH);",
            "\t\tfor (i = 0, xdp_n = 0; i < n; i++) {",
            "\t\t\tvoid *f = frames[i];",
            "\t\t\tstruct page *page;",
            "",
            "\t\t\tif (unlikely(__ptr_test_bit(0, &f))) {",
            "\t\t\t\tstruct sk_buff *skb = f;",
            "",
            "\t\t\t\t__ptr_clear_bit(0, &skb);",
            "\t\t\t\tlist_add_tail(&skb->list, &list);",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tframes[xdp_n++] = f;",
            "\t\t\tpage = virt_to_page(f);",
            "",
            "\t\t\t/* Bring struct page memory area to curr CPU. Read by",
            "\t\t\t * build_skb_around via page_is_pfmemalloc(), and when",
            "\t\t\t * freed written by page_frag_free call.",
            "\t\t\t */",
            "\t\t\tprefetchw(page);",
            "\t\t}",
            "",
            "\t\t/* Support running another XDP prog on this CPU */",
            "\t\tnframes = cpu_map_bpf_prog_run(rcpu, frames, xdp_n, &stats, &list);",
            "\t\tif (nframes) {",
            "\t\t\tm = kmem_cache_alloc_bulk(skbuff_cache, gfp, nframes, skbs);",
            "\t\t\tif (unlikely(m == 0)) {",
            "\t\t\t\tfor (i = 0; i < nframes; i++)",
            "\t\t\t\t\tskbs[i] = NULL; /* effect: xdp_return_frame */",
            "\t\t\t\tkmem_alloc_drops += nframes;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tlocal_bh_disable();",
            "\t\tfor (i = 0; i < nframes; i++) {",
            "\t\t\tstruct xdp_frame *xdpf = frames[i];",
            "\t\t\tstruct sk_buff *skb = skbs[i];",
            "",
            "\t\t\tskb = __xdp_build_skb_from_frame(xdpf, skb,",
            "\t\t\t\t\t\t\t xdpf->dev_rx);",
            "\t\t\tif (!skb) {",
            "\t\t\t\txdp_return_frame(xdpf);",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tlist_add_tail(&skb->list, &list);",
            "\t\t}",
            "\t\tnetif_receive_skb_list(&list);",
            "",
            "\t\t/* Feedback loop via tracepoint */",
            "\t\ttrace_xdp_cpumap_kthread(rcpu->map_id, n, kmem_alloc_drops,",
            "\t\t\t\t\t sched, &stats);",
            "",
            "\t\tlocal_bh_enable(); /* resched point, may call do_softirq() */",
            "\t}",
            "\t__set_current_state(TASK_RUNNING);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "cpu_map_bpf_prog_run, cpu_map_kthread_run",
          "description": "kthread线程处理逻辑，包含从环形缓冲区消费数据包、执行BPF程序、构建skb并提交到网络栈的全流程，包含批量处理优化和软中断调度点。",
          "similarity": 0.4900781810283661
        }
      ]
    },
    {
      "source_file": "mm/page-writeback.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:09\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page-writeback.c`\n\n---\n\n# page-writeback.c 技术文档\n\n## 1. 文件概述\n\n`page-writeback.c` 是 Linux 内核内存管理子系统（MM）中的核心文件，负责实现**脏页回写（dirty page writeback）机制**。该机制用于控制和协调将修改过的页面（即“脏页”）从内存写回到持久化存储（如磁盘）的过程，以确保数据一致性、防止内存耗尽，并在系统负载与 I/O 带宽之间取得平衡。\n\n该文件主要提供以下功能：\n- 脏页数量的全局与每 BDI（Backing Device Info）级别的阈值管理\n- 脏页生成速率的动态限流（throttling）\n- 后台回写线程（如 `writeback` 线程）的触发逻辑\n- 支持基于 cgroup 的内存回写控制（当启用 `CONFIG_CGROUP_WRITEBACK` 时）\n- 与 `/proc/sys/vm` 中可调参数的交互接口\n\n## 2. 核心功能\n\n### 主要全局变量（可通过 sysctl 调整）\n| 变量名 | 默认值 | 说明 |\n|--------|--------|------|\n| `dirty_background_ratio` | 10 | 当脏页占可用内存比例达到此值时，启动后台回写 |\n| `vm_dirty_ratio` | 20 | 脏页比例硬上限，超过则阻塞写进程进行同步回写 |\n| `dirty_background_bytes` | 0 | 以字节为单位指定后台回写阈值（优先级高于 ratio） |\n| `vm_dirty_bytes` | 0 | 以字节为单位指定脏页硬上限（优先级高于 ratio） |\n| `dirty_writeback_interval` | 500 (5秒) | 后台回写线程的唤醒间隔（单位：厘秒） |\n| `dirty_expire_interval` | 3000 (30秒) | 脏页最大存活时间，超时强制回写 |\n| `laptop_mode` | 0 | 笔记本模式开关，减少磁盘活动以省电 |\n| `ratelimit_pages` | 32 | 每 CPU 脏页速率限制阈值 |\n\n### 关键数据结构\n- **`struct wb_domain`**  \n  回写域（writeback domain），用于聚合多个 BDI 的回写状态，支持全局或 per-memcg 的回写控制。\n  \n- **`struct dirty_throttle_control` (dtc)**  \n  脏页限流控制上下文，包含：\n  - `avail`：当前可脏化的内存总量\n  - `dirty`：当前脏页数量\n  - `thresh` / `bg_thresh`：硬/软回写阈值\n  - `wb_dirty` / `wb_thresh` / `wb_bg_thresh`：per-BDI 级别的对应值\n  - `pos_ratio`：用于计算回写速率的比例因子\n\n- **条件编译支持**  \n  通过 `CONFIG_CGROUP_WRITEBACK` 区分是否支持 memcg 级别的回写控制，提供 `GDTC_INIT`、`MDTC_INIT` 等宏及辅助函数（如 `mdtc_valid()`、`wb_min_max_ratio()`）。\n\n### 核心辅助函数（部分在截断代码中未完整显示）\n- `node_dirtyable_memory()`：计算指定 NUMA 节点中可用于脏页缓存的内存总量（包括空闲页 + 文件缓存页 - 保留页）。\n- `balance_dirty_pages()`：主限流函数，在进程写入时被调用，根据当前脏页水位决定是否休眠或触发回写。\n- `balance_dirty_pages_ratelimited()`：带速率限制的脏页平衡入口，避免频繁调用开销。\n\n## 3. 关键实现\n\n### 脏页阈值计算逻辑\n- 脏页上限基于 **“dirtyable memory”** 计算，即 `(free pages + file cache pages - kernel reserves)`。\n- 支持两种配置方式：**百分比（ratio）** 或 **绝对字节数（bytes）**，后者优先。\n- 当启用 `vm_highmem_is_dirtyable` 时，highmem 区域的空闲页也计入 dirtyable memory。\n\n### 动态限流机制\n- 使用 **`MAX_PAUSE`（最大 200ms）** 限制单次 `balance_dirty_pages()` 的休眠时间。\n- 引入 **`DIRTY_POLL_THRESH`（128KB）** 作为调用间隔优化阈值：若脏页增长过快，则提升休眠时间至最大值。\n- 通过 **`BANDWIDTH_INTERVAL`（200ms）** 动态估算存储设备的写入带宽，用于调整回写速率。\n\n### cgroup writeback 支持\n- 在 `CONFIG_CGROUP_WRITEBACK` 启用时：\n  - 每个 memcg 有独立的 `wb_domain`\n  - `dirty_throttle_control` 可关联全局（gdtc）或 memcg（mdtc）上下文\n  - BDI 的 min/max_ratio 根据其实际带宽动态缩放，实现公平分配\n\n### 老化与完成计数\n- 使用 `fprop_local_percpu` 结构跟踪每个 BDI 的回写完成情况。\n- `VM_COMPLETIONS_PERIOD_LEN`（3 秒）定义了回写完成率的老化周期，影响带宽估算的响应速度。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/swap.h>`、`<linux/pagevec.h>` 等，与页分配、回收机制紧密集成。\n- **VFS 层**：通过 `<linux/fs.h>`、`<linux/pagemap.h>` 与 address_space 和 inode 交互。\n- **块设备层**：通过 `<linux/blkdev.h>`、`<linux/backing-dev.h>` 获取 BDI 信息和 I/O 能力。\n- **调度与同步**：使用 `<linux/sched.h>`、`<linux/spinlock.h>`、`<linux/timer.h>` 实现休眠、锁和定时器。\n- **追踪系统**：集成 `<trace/events/writeback.h>` 提供回写事件追踪点。\n- **内部头文件**：包含 `\"internal.h\"` 获取 MM 子系统内部接口。\n\n## 5. 使用场景\n\n1. **用户空间写入文件**  \n   当进程通过 `write()` 修改文件页时，页被标记为脏，随后调用 `balance_dirty_pages_ratelimited()` 触发脏页控制。\n\n2. **内存压力下的页面回收**  \n   kswapd 或直接回收路径在需要释放内存时，可能调用回写逻辑清理脏页。\n\n3. **定期后台回写**  \n   `writeback` 内核线程按 `dirty_writeback_interval` 周期唤醒，检查并回写超过 `dirty_expire_interval` 的脏页。\n\n4. **系统关闭或 sync 调用**  \n   虽然主要同步逻辑在其他文件，但本文件提供的阈值和状态是决策基础。\n\n5. **容器环境中的资源隔离**  \n   启用 cgroup writeback 后，不同 memcg 的脏页回写相互隔离，避免一个容器的大量写入影响其他容器性能。\n\n6. **笔记本省电模式**  \n   当 `laptop_mode` 启用时，延迟回写以减少磁盘旋转时间，延长电池寿命。",
      "similarity": 0.5744668841362,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "mm/page-writeback.c",
          "start_line": 495,
          "end_line": 600,
          "content": [
            "bool node_dirty_ok(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long limit = node_dirty_limit(pgdat);",
            "\tunsigned long nr_pages = 0;",
            "",
            "\tnr_pages += node_page_state(pgdat, NR_FILE_DIRTY);",
            "\tnr_pages += node_page_state(pgdat, NR_WRITEBACK);",
            "",
            "\treturn nr_pages <= limit;",
            "}",
            "static int dirty_background_ratio_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint ret;",
            "",
            "\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write)",
            "\t\tdirty_background_bytes = 0;",
            "\treturn ret;",
            "}",
            "static int dirty_background_bytes_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint ret;",
            "\tunsigned long old_bytes = dirty_background_bytes;",
            "",
            "\tret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write) {",
            "\t\tif (DIV_ROUND_UP(dirty_background_bytes, PAGE_SIZE) >",
            "\t\t\t\t\t\t\t\tUINT_MAX) {",
            "\t\t\tdirty_background_bytes = old_bytes;",
            "\t\t\treturn -ERANGE;",
            "\t\t}",
            "\t\tdirty_background_ratio = 0;",
            "\t}",
            "\treturn ret;",
            "}",
            "static int dirty_ratio_handler(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos)",
            "{",
            "\tint old_ratio = vm_dirty_ratio;",
            "\tint ret;",
            "",
            "\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write && vm_dirty_ratio != old_ratio) {",
            "\t\tvm_dirty_bytes = 0;",
            "\t\twriteback_set_ratelimit();",
            "\t}",
            "\treturn ret;",
            "}",
            "static int dirty_bytes_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tunsigned long old_bytes = vm_dirty_bytes;",
            "\tint ret;",
            "",
            "\tret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write && vm_dirty_bytes != old_bytes) {",
            "\t\tif (DIV_ROUND_UP(vm_dirty_bytes, PAGE_SIZE) > UINT_MAX) {",
            "\t\t\tvm_dirty_bytes = old_bytes;",
            "\t\t\treturn -ERANGE;",
            "\t\t}",
            "\t\twriteback_set_ratelimit();",
            "\t\tvm_dirty_ratio = 0;",
            "\t}",
            "\treturn ret;",
            "}",
            "static unsigned long wp_next_time(unsigned long cur_time)",
            "{",
            "\tcur_time += VM_COMPLETIONS_PERIOD_LEN;",
            "\t/* 0 has a special meaning... */",
            "\tif (!cur_time)",
            "\t\treturn 1;",
            "\treturn cur_time;",
            "}",
            "static void wb_domain_writeout_add(struct wb_domain *dom,",
            "\t\t\t\t   struct fprop_local_percpu *completions,",
            "\t\t\t\t   unsigned int max_prop_frac, long nr)",
            "{",
            "\t__fprop_add_percpu_max(&dom->completions, completions,",
            "\t\t\t       max_prop_frac, nr);",
            "\t/* First event after period switching was turned off? */",
            "\tif (unlikely(!dom->period_time)) {",
            "\t\t/*",
            "\t\t * We can race with other __bdi_writeout_inc calls here but",
            "\t\t * it does not cause any harm since the resulting time when",
            "\t\t * timer will fire and what is in writeout_period_time will be",
            "\t\t * roughly the same.",
            "\t\t */",
            "\t\tdom->period_time = wp_next_time(jiffies);",
            "\t\tmod_timer(&dom->period_timer, dom->period_time);",
            "\t}",
            "}",
            "static inline void __wb_writeout_add(struct bdi_writeback *wb, long nr)",
            "{",
            "\tstruct wb_domain *cgdom;",
            "",
            "\twb_stat_mod(wb, WB_WRITTEN, nr);",
            "\twb_domain_writeout_add(&global_wb_domain, &wb->completions,",
            "\t\t\t       wb->bdi->max_prop_frac, nr);",
            "",
            "\tcgdom = mem_cgroup_wb_domain(wb);",
            "\tif (cgdom)",
            "\t\twb_domain_writeout_add(cgdom, wb_memcg_completions(wb),",
            "\t\t\t\t       wb->bdi->max_prop_frac, nr);",
            "}"
          ],
          "function_name": "node_dirty_ok, dirty_background_ratio_handler, dirty_background_bytes_handler, dirty_ratio_handler, dirty_bytes_handler, wp_next_time, wb_domain_writeout_add, __wb_writeout_add",
          "description": "通过sysctl接口动态调整脏页写回参数，维护写回统计信息并周期性触发写回检查，确保系统内存使用符合预设策略。",
          "similarity": 0.5956469774246216
        },
        {
          "chunk_id": 4,
          "file_path": "mm/page-writeback.c",
          "start_line": 615,
          "end_line": 719,
          "content": [
            "void wb_writeout_inc(struct bdi_writeback *wb)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\t__wb_writeout_add(wb, 1);",
            "\tlocal_irq_restore(flags);",
            "}",
            "static void writeout_period(struct timer_list *t)",
            "{",
            "\tstruct wb_domain *dom = from_timer(dom, t, period_timer);",
            "\tint miss_periods = (jiffies - dom->period_time) /",
            "\t\t\t\t\t\t VM_COMPLETIONS_PERIOD_LEN;",
            "",
            "\tif (fprop_new_period(&dom->completions, miss_periods + 1)) {",
            "\t\tdom->period_time = wp_next_time(dom->period_time +",
            "\t\t\t\tmiss_periods * VM_COMPLETIONS_PERIOD_LEN);",
            "\t\tmod_timer(&dom->period_timer, dom->period_time);",
            "\t} else {",
            "\t\t/*",
            "\t\t * Aging has zeroed all fractions. Stop wasting CPU on period",
            "\t\t * updates.",
            "\t\t */",
            "\t\tdom->period_time = 0;",
            "\t}",
            "}",
            "int wb_domain_init(struct wb_domain *dom, gfp_t gfp)",
            "{",
            "\tmemset(dom, 0, sizeof(*dom));",
            "",
            "\tspin_lock_init(&dom->lock);",
            "",
            "\ttimer_setup(&dom->period_timer, writeout_period, TIMER_DEFERRABLE);",
            "",
            "\tdom->dirty_limit_tstamp = jiffies;",
            "",
            "\treturn fprop_global_init(&dom->completions, gfp);",
            "}",
            "void wb_domain_exit(struct wb_domain *dom)",
            "{",
            "\tdel_timer_sync(&dom->period_timer);",
            "\tfprop_global_destroy(&dom->completions);",
            "}",
            "static int bdi_check_pages_limit(unsigned long pages)",
            "{",
            "\tunsigned long max_dirty_pages = global_dirtyable_memory();",
            "",
            "\tif (pages > max_dirty_pages)",
            "\t\treturn -EINVAL;",
            "",
            "\treturn 0;",
            "}",
            "static unsigned long bdi_ratio_from_pages(unsigned long pages)",
            "{",
            "\tunsigned long background_thresh;",
            "\tunsigned long dirty_thresh;",
            "\tunsigned long ratio;",
            "",
            "\tglobal_dirty_limits(&background_thresh, &dirty_thresh);",
            "\tratio = div64_u64(pages * 100ULL * BDI_RATIO_SCALE, dirty_thresh);",
            "",
            "\treturn ratio;",
            "}",
            "static u64 bdi_get_bytes(unsigned int ratio)",
            "{",
            "\tunsigned long background_thresh;",
            "\tunsigned long dirty_thresh;",
            "\tu64 bytes;",
            "",
            "\tglobal_dirty_limits(&background_thresh, &dirty_thresh);",
            "\tbytes = (dirty_thresh * PAGE_SIZE * ratio) / BDI_RATIO_SCALE / 100;",
            "",
            "\treturn bytes;",
            "}",
            "static int __bdi_set_min_ratio(struct backing_dev_info *bdi, unsigned int min_ratio)",
            "{",
            "\tunsigned int delta;",
            "\tint ret = 0;",
            "",
            "\tif (min_ratio > 100 * BDI_RATIO_SCALE)",
            "\t\treturn -EINVAL;",
            "\tmin_ratio *= BDI_RATIO_SCALE;",
            "",
            "\tspin_lock_bh(&bdi_lock);",
            "\tif (min_ratio > bdi->max_ratio) {",
            "\t\tret = -EINVAL;",
            "\t} else {",
            "\t\tif (min_ratio < bdi->min_ratio) {",
            "\t\t\tdelta = bdi->min_ratio - min_ratio;",
            "\t\t\tbdi_min_ratio -= delta;",
            "\t\t\tbdi->min_ratio = min_ratio;",
            "\t\t} else {",
            "\t\t\tdelta = min_ratio - bdi->min_ratio;",
            "\t\t\tif (bdi_min_ratio + delta < 100 * BDI_RATIO_SCALE) {",
            "\t\t\t\tbdi_min_ratio += delta;",
            "\t\t\t\tbdi->min_ratio = min_ratio;",
            "\t\t\t} else {",
            "\t\t\t\tret = -EINVAL;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\tspin_unlock_bh(&bdi_lock);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "wb_writeout_inc, writeout_period, wb_domain_init, wb_domain_exit, bdi_check_pages_limit, bdi_ratio_from_pages, bdi_get_bytes, __bdi_set_min_ratio",
          "description": "实现写回统计增量记录、定时器驱动的写回周期校准及内存设备（bdi）的脏页限制验证与参数转换逻辑，支撑动态写回策略调整。",
          "similarity": 0.5722908973693848
        },
        {
          "chunk_id": 7,
          "file_path": "mm/page-writeback.c",
          "start_line": 1221,
          "end_line": 1472,
          "content": [
            "static void wb_update_write_bandwidth(struct bdi_writeback *wb,",
            "\t\t\t\t      unsigned long elapsed,",
            "\t\t\t\t      unsigned long written)",
            "{",
            "\tconst unsigned long period = roundup_pow_of_two(3 * HZ);",
            "\tunsigned long avg = wb->avg_write_bandwidth;",
            "\tunsigned long old = wb->write_bandwidth;",
            "\tu64 bw;",
            "",
            "\t/*",
            "\t * bw = written * HZ / elapsed",
            "\t *",
            "\t *                   bw * elapsed + write_bandwidth * (period - elapsed)",
            "\t * write_bandwidth = ---------------------------------------------------",
            "\t *                                          period",
            "\t *",
            "\t * @written may have decreased due to folio_redirty_for_writepage().",
            "\t * Avoid underflowing @bw calculation.",
            "\t */",
            "\tbw = written - min(written, wb->written_stamp);",
            "\tbw *= HZ;",
            "\tif (unlikely(elapsed > period)) {",
            "\t\tbw = div64_ul(bw, elapsed);",
            "\t\tavg = bw;",
            "\t\tgoto out;",
            "\t}",
            "\tbw += (u64)wb->write_bandwidth * (period - elapsed);",
            "\tbw >>= ilog2(period);",
            "",
            "\t/*",
            "\t * one more level of smoothing, for filtering out sudden spikes",
            "\t */",
            "\tif (avg > old && old >= (unsigned long)bw)",
            "\t\tavg -= (avg - old) >> 3;",
            "",
            "\tif (avg < old && old <= (unsigned long)bw)",
            "\t\tavg += (old - avg) >> 3;",
            "",
            "out:",
            "\t/* keep avg > 0 to guarantee that tot > 0 if there are dirty wbs */",
            "\tavg = max(avg, 1LU);",
            "\tif (wb_has_dirty_io(wb)) {",
            "\t\tlong delta = avg - wb->avg_write_bandwidth;",
            "\t\tWARN_ON_ONCE(atomic_long_add_return(delta,",
            "\t\t\t\t\t&wb->bdi->tot_write_bandwidth) <= 0);",
            "\t}",
            "\twb->write_bandwidth = bw;",
            "\tWRITE_ONCE(wb->avg_write_bandwidth, avg);",
            "}",
            "static void update_dirty_limit(struct dirty_throttle_control *dtc)",
            "{",
            "\tstruct wb_domain *dom = dtc_dom(dtc);",
            "\tunsigned long thresh = dtc->thresh;",
            "\tunsigned long limit = dom->dirty_limit;",
            "",
            "\t/*",
            "\t * Follow up in one step.",
            "\t */",
            "\tif (limit < thresh) {",
            "\t\tlimit = thresh;",
            "\t\tgoto update;",
            "\t}",
            "",
            "\t/*",
            "\t * Follow down slowly. Use the higher one as the target, because thresh",
            "\t * may drop below dirty. This is exactly the reason to introduce",
            "\t * dom->dirty_limit which is guaranteed to lie above the dirty pages.",
            "\t */",
            "\tthresh = max(thresh, dtc->dirty);",
            "\tif (limit > thresh) {",
            "\t\tlimit -= (limit - thresh) >> 5;",
            "\t\tgoto update;",
            "\t}",
            "\treturn;",
            "update:",
            "\tdom->dirty_limit = limit;",
            "}",
            "static void domain_update_dirty_limit(struct dirty_throttle_control *dtc,",
            "\t\t\t\t      unsigned long now)",
            "{",
            "\tstruct wb_domain *dom = dtc_dom(dtc);",
            "",
            "\t/*",
            "\t * check locklessly first to optimize away locking for the most time",
            "\t */",
            "\tif (time_before(now, dom->dirty_limit_tstamp + BANDWIDTH_INTERVAL))",
            "\t\treturn;",
            "",
            "\tspin_lock(&dom->lock);",
            "\tif (time_after_eq(now, dom->dirty_limit_tstamp + BANDWIDTH_INTERVAL)) {",
            "\t\tupdate_dirty_limit(dtc);",
            "\t\tdom->dirty_limit_tstamp = now;",
            "\t}",
            "\tspin_unlock(&dom->lock);",
            "}",
            "static void wb_update_dirty_ratelimit(struct dirty_throttle_control *dtc,",
            "\t\t\t\t      unsigned long dirtied,",
            "\t\t\t\t      unsigned long elapsed)",
            "{",
            "\tstruct bdi_writeback *wb = dtc->wb;",
            "\tunsigned long dirty = dtc->dirty;",
            "\tunsigned long freerun = dirty_freerun_ceiling(dtc->thresh, dtc->bg_thresh);",
            "\tunsigned long limit = hard_dirty_limit(dtc_dom(dtc), dtc->thresh);",
            "\tunsigned long setpoint = (freerun + limit) / 2;",
            "\tunsigned long write_bw = wb->avg_write_bandwidth;",
            "\tunsigned long dirty_ratelimit = wb->dirty_ratelimit;",
            "\tunsigned long dirty_rate;",
            "\tunsigned long task_ratelimit;",
            "\tunsigned long balanced_dirty_ratelimit;",
            "\tunsigned long step;",
            "\tunsigned long x;",
            "\tunsigned long shift;",
            "",
            "\t/*",
            "\t * The dirty rate will match the writeout rate in long term, except",
            "\t * when dirty pages are truncated by userspace or re-dirtied by FS.",
            "\t */",
            "\tdirty_rate = (dirtied - wb->dirtied_stamp) * HZ / elapsed;",
            "",
            "\t/*",
            "\t * task_ratelimit reflects each dd's dirty rate for the past 200ms.",
            "\t */",
            "\ttask_ratelimit = (u64)dirty_ratelimit *",
            "\t\t\t\t\tdtc->pos_ratio >> RATELIMIT_CALC_SHIFT;",
            "\ttask_ratelimit++; /* it helps rampup dirty_ratelimit from tiny values */",
            "",
            "\t/*",
            "\t * A linear estimation of the \"balanced\" throttle rate. The theory is,",
            "\t * if there are N dd tasks, each throttled at task_ratelimit, the wb's",
            "\t * dirty_rate will be measured to be (N * task_ratelimit). So the below",
            "\t * formula will yield the balanced rate limit (write_bw / N).",
            "\t *",
            "\t * Note that the expanded form is not a pure rate feedback:",
            "\t *\trate_(i+1) = rate_(i) * (write_bw / dirty_rate)\t\t     (1)",
            "\t * but also takes pos_ratio into account:",
            "\t *\trate_(i+1) = rate_(i) * (write_bw / dirty_rate) * pos_ratio  (2)",
            "\t *",
            "\t * (1) is not realistic because pos_ratio also takes part in balancing",
            "\t * the dirty rate.  Consider the state",
            "\t *\tpos_ratio = 0.5\t\t\t\t\t\t     (3)",
            "\t *\trate = 2 * (write_bw / N)\t\t\t\t     (4)",
            "\t * If (1) is used, it will stuck in that state! Because each dd will",
            "\t * be throttled at",
            "\t *\ttask_ratelimit = pos_ratio * rate = (write_bw / N)\t     (5)",
            "\t * yielding",
            "\t *\tdirty_rate = N * task_ratelimit = write_bw\t\t     (6)",
            "\t * put (6) into (1) we get",
            "\t *\trate_(i+1) = rate_(i)\t\t\t\t\t     (7)",
            "\t *",
            "\t * So we end up using (2) to always keep",
            "\t *\trate_(i+1) ~= (write_bw / N)\t\t\t\t     (8)",
            "\t * regardless of the value of pos_ratio. As long as (8) is satisfied,",
            "\t * pos_ratio is able to drive itself to 1.0, which is not only where",
            "\t * the dirty count meet the setpoint, but also where the slope of",
            "\t * pos_ratio is most flat and hence task_ratelimit is least fluctuated.",
            "\t */",
            "\tbalanced_dirty_ratelimit = div_u64((u64)task_ratelimit * write_bw,",
            "\t\t\t\t\t   dirty_rate | 1);",
            "\t/*",
            "\t * balanced_dirty_ratelimit ~= (write_bw / N) <= write_bw",
            "\t */",
            "\tif (unlikely(balanced_dirty_ratelimit > write_bw))",
            "\t\tbalanced_dirty_ratelimit = write_bw;",
            "",
            "\t/*",
            "\t * We could safely do this and return immediately:",
            "\t *",
            "\t *\twb->dirty_ratelimit = balanced_dirty_ratelimit;",
            "\t *",
            "\t * However to get a more stable dirty_ratelimit, the below elaborated",
            "\t * code makes use of task_ratelimit to filter out singular points and",
            "\t * limit the step size.",
            "\t *",
            "\t * The below code essentially only uses the relative value of",
            "\t *",
            "\t *\ttask_ratelimit - dirty_ratelimit",
            "\t *\t= (pos_ratio - 1) * dirty_ratelimit",
            "\t *",
            "\t * which reflects the direction and size of dirty position error.",
            "\t */",
            "",
            "\t/*",
            "\t * dirty_ratelimit will follow balanced_dirty_ratelimit iff",
            "\t * task_ratelimit is on the same side of dirty_ratelimit, too.",
            "\t * For example, when",
            "\t * - dirty_ratelimit > balanced_dirty_ratelimit",
            "\t * - dirty_ratelimit > task_ratelimit (dirty pages are above setpoint)",
            "\t * lowering dirty_ratelimit will help meet both the position and rate",
            "\t * control targets. Otherwise, don't update dirty_ratelimit if it will",
            "\t * only help meet the rate target. After all, what the users ultimately",
            "\t * feel and care are stable dirty rate and small position error.",
            "\t *",
            "\t * |task_ratelimit - dirty_ratelimit| is used to limit the step size",
            "\t * and filter out the singular points of balanced_dirty_ratelimit. Which",
            "\t * keeps jumping around randomly and can even leap far away at times",
            "\t * due to the small 200ms estimation period of dirty_rate (we want to",
            "\t * keep that period small to reduce time lags).",
            "\t */",
            "\tstep = 0;",
            "",
            "\t/*",
            "\t * For strictlimit case, calculations above were based on wb counters",
            "\t * and limits (starting from pos_ratio = wb_position_ratio() and up to",
            "\t * balanced_dirty_ratelimit = task_ratelimit * write_bw / dirty_rate).",
            "\t * Hence, to calculate \"step\" properly, we have to use wb_dirty as",
            "\t * \"dirty\" and wb_setpoint as \"setpoint\".",
            "\t *",
            "\t * We rampup dirty_ratelimit forcibly if wb_dirty is low because",
            "\t * it's possible that wb_thresh is close to zero due to inactivity",
            "\t * of backing device.",
            "\t */",
            "\tif (unlikely(wb->bdi->capabilities & BDI_CAP_STRICTLIMIT)) {",
            "\t\tdirty = dtc->wb_dirty;",
            "\t\tif (dtc->wb_dirty < 8)",
            "\t\t\tsetpoint = dtc->wb_dirty + 1;",
            "\t\telse",
            "\t\t\tsetpoint = (dtc->wb_thresh + dtc->wb_bg_thresh) / 2;",
            "\t}",
            "",
            "\tif (dirty < setpoint) {",
            "\t\tx = min3(wb->balanced_dirty_ratelimit,",
            "\t\t\t balanced_dirty_ratelimit, task_ratelimit);",
            "\t\tif (dirty_ratelimit < x)",
            "\t\t\tstep = x - dirty_ratelimit;",
            "\t} else {",
            "\t\tx = max3(wb->balanced_dirty_ratelimit,",
            "\t\t\t balanced_dirty_ratelimit, task_ratelimit);",
            "\t\tif (dirty_ratelimit > x)",
            "\t\t\tstep = dirty_ratelimit - x;",
            "\t}",
            "",
            "\t/*",
            "\t * Don't pursue 100% rate matching. It's impossible since the balanced",
            "\t * rate itself is constantly fluctuating. So decrease the track speed",
            "\t * when it gets close to the target. Helps eliminate pointless tremors.",
            "\t */",
            "\tshift = dirty_ratelimit / (2 * step + 1);",
            "\tif (shift < BITS_PER_LONG)",
            "\t\tstep = DIV_ROUND_UP(step >> shift, 8);",
            "\telse",
            "\t\tstep = 0;",
            "",
            "\tif (dirty_ratelimit < balanced_dirty_ratelimit)",
            "\t\tdirty_ratelimit += step;",
            "\telse",
            "\t\tdirty_ratelimit -= step;",
            "",
            "\tWRITE_ONCE(wb->dirty_ratelimit, max(dirty_ratelimit, 1UL));",
            "\twb->balanced_dirty_ratelimit = balanced_dirty_ratelimit;",
            "",
            "\ttrace_bdi_dirty_ratelimit(wb, dirty_rate, task_ratelimit);",
            "}"
          ],
          "function_name": "wb_update_write_bandwidth, update_dirty_limit, domain_update_dirty_limit, wb_update_dirty_ratelimit",
          "description": "实现写带宽统计更新逻辑，包含平滑写入速率变化的算法，更新脏页限制的动态调整机制，以及根据当前脏页速率与写入能力计算目标脏页产生速率的控制逻辑，通过分层限速策略维持系统稳定。",
          "similarity": 0.5513747930526733
        },
        {
          "chunk_id": 11,
          "file_path": "mm/page-writeback.c",
          "start_line": 2176,
          "end_line": 2404,
          "content": [
            "static int dirty_writeback_centisecs_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tunsigned int old_interval = dirty_writeback_interval;",
            "\tint ret;",
            "",
            "\tret = proc_dointvec(table, write, buffer, length, ppos);",
            "",
            "\t/*",
            "\t * Writing 0 to dirty_writeback_interval will disable periodic writeback",
            "\t * and a different non-zero value will wakeup the writeback threads.",
            "\t * wb_wakeup_delayed() would be more appropriate, but it's a pain to",
            "\t * iterate over all bdis and wbs.",
            "\t * The reason we do this is to make the change take effect immediately.",
            "\t */",
            "\tif (!ret && write && dirty_writeback_interval &&",
            "\t\tdirty_writeback_interval != old_interval)",
            "\t\twakeup_flusher_threads(WB_REASON_PERIODIC);",
            "",
            "\treturn ret;",
            "}",
            "void laptop_mode_timer_fn(struct timer_list *t)",
            "{",
            "\tstruct backing_dev_info *backing_dev_info =",
            "\t\tfrom_timer(backing_dev_info, t, laptop_mode_wb_timer);",
            "",
            "\twakeup_flusher_threads_bdi(backing_dev_info, WB_REASON_LAPTOP_TIMER);",
            "}",
            "void laptop_io_completion(struct backing_dev_info *info)",
            "{",
            "\tmod_timer(&info->laptop_mode_wb_timer, jiffies + laptop_mode);",
            "}",
            "void laptop_sync_completion(void)",
            "{",
            "\tstruct backing_dev_info *bdi;",
            "",
            "\trcu_read_lock();",
            "",
            "\tlist_for_each_entry_rcu(bdi, &bdi_list, bdi_list)",
            "\t\tdel_timer(&bdi->laptop_mode_wb_timer);",
            "",
            "\trcu_read_unlock();",
            "}",
            "void writeback_set_ratelimit(void)",
            "{",
            "\tstruct wb_domain *dom = &global_wb_domain;",
            "\tunsigned long background_thresh;",
            "\tunsigned long dirty_thresh;",
            "",
            "\tglobal_dirty_limits(&background_thresh, &dirty_thresh);",
            "\tdom->dirty_limit = dirty_thresh;",
            "\tratelimit_pages = dirty_thresh / (num_online_cpus() * 32);",
            "\tif (ratelimit_pages < 16)",
            "\t\tratelimit_pages = 16;",
            "}",
            "static int page_writeback_cpu_online(unsigned int cpu)",
            "{",
            "\twriteback_set_ratelimit();",
            "\treturn 0;",
            "}",
            "void __init page_writeback_init(void)",
            "{",
            "\tBUG_ON(wb_domain_init(&global_wb_domain, GFP_KERNEL));",
            "",
            "\tcpuhp_setup_state(CPUHP_AP_ONLINE_DYN, \"mm/writeback:online\",",
            "\t\t\t  page_writeback_cpu_online, NULL);",
            "\tcpuhp_setup_state(CPUHP_MM_WRITEBACK_DEAD, \"mm/writeback:dead\", NULL,",
            "\t\t\t  page_writeback_cpu_online);",
            "#ifdef CONFIG_SYSCTL",
            "\tregister_sysctl_init(\"vm\", vm_page_writeback_sysctls);",
            "#endif",
            "}",
            "void tag_pages_for_writeback(struct address_space *mapping,",
            "\t\t\t     pgoff_t start, pgoff_t end)",
            "{",
            "\tXA_STATE(xas, &mapping->i_pages, start);",
            "\tunsigned int tagged = 0;",
            "\tvoid *page;",
            "",
            "\txas_lock_irq(&xas);",
            "\txas_for_each_marked(&xas, page, end, PAGECACHE_TAG_DIRTY) {",
            "\t\txas_set_mark(&xas, PAGECACHE_TAG_TOWRITE);",
            "\t\tif (++tagged % XA_CHECK_SCHED)",
            "\t\t\tcontinue;",
            "",
            "\t\txas_pause(&xas);",
            "\t\txas_unlock_irq(&xas);",
            "\t\tcond_resched();",
            "\t\txas_lock_irq(&xas);",
            "\t}",
            "\txas_unlock_irq(&xas);",
            "}",
            "int write_cache_pages(struct address_space *mapping,",
            "\t\t      struct writeback_control *wbc, writepage_t writepage,",
            "\t\t      void *data)",
            "{",
            "\tint ret = 0;",
            "\tint done = 0;",
            "\tint error;",
            "\tstruct folio_batch fbatch;",
            "\tint nr_folios;",
            "\tpgoff_t index;",
            "\tpgoff_t end;\t\t/* Inclusive */",
            "\tpgoff_t done_index;",
            "\tint range_whole = 0;",
            "\txa_mark_t tag;",
            "",
            "\tfolio_batch_init(&fbatch);",
            "\tif (wbc->range_cyclic) {",
            "\t\tindex = mapping->writeback_index; /* prev offset */",
            "\t\tend = -1;",
            "\t} else {",
            "\t\tindex = wbc->range_start >> PAGE_SHIFT;",
            "\t\tend = wbc->range_end >> PAGE_SHIFT;",
            "\t\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)",
            "\t\t\trange_whole = 1;",
            "\t}",
            "\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages) {",
            "\t\ttag_pages_for_writeback(mapping, index, end);",
            "\t\ttag = PAGECACHE_TAG_TOWRITE;",
            "\t} else {",
            "\t\ttag = PAGECACHE_TAG_DIRTY;",
            "\t}",
            "\tdone_index = index;",
            "\twhile (!done && (index <= end)) {",
            "\t\tint i;",
            "",
            "\t\tnr_folios = filemap_get_folios_tag(mapping, &index, end,",
            "\t\t\t\ttag, &fbatch);",
            "",
            "\t\tif (nr_folios == 0)",
            "\t\t\tbreak;",
            "",
            "\t\tfor (i = 0; i < nr_folios; i++) {",
            "\t\t\tstruct folio *folio = fbatch.folios[i];",
            "\t\t\tunsigned long nr;",
            "",
            "\t\t\tdone_index = folio->index;",
            "",
            "\t\t\tfolio_lock(folio);",
            "",
            "\t\t\t/*",
            "\t\t\t * Page truncated or invalidated. We can freely skip it",
            "\t\t\t * then, even for data integrity operations: the page",
            "\t\t\t * has disappeared concurrently, so there could be no",
            "\t\t\t * real expectation of this data integrity operation",
            "\t\t\t * even if there is now a new, dirty page at the same",
            "\t\t\t * pagecache address.",
            "\t\t\t */",
            "\t\t\tif (unlikely(folio->mapping != mapping)) {",
            "continue_unlock:",
            "\t\t\t\tfolio_unlock(folio);",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tif (!folio_test_dirty(folio)) {",
            "\t\t\t\t/* someone wrote it for us */",
            "\t\t\t\tgoto continue_unlock;",
            "\t\t\t}",
            "",
            "\t\t\tif (folio_test_writeback(folio)) {",
            "\t\t\t\tif (wbc->sync_mode != WB_SYNC_NONE)",
            "\t\t\t\t\tfolio_wait_writeback(folio);",
            "\t\t\t\telse",
            "\t\t\t\t\tgoto continue_unlock;",
            "\t\t\t}",
            "",
            "\t\t\tBUG_ON(folio_test_writeback(folio));",
            "\t\t\tif (!folio_clear_dirty_for_io(folio))",
            "\t\t\t\tgoto continue_unlock;",
            "",
            "\t\t\ttrace_wbc_writepage(wbc, inode_to_bdi(mapping->host));",
            "\t\t\terror = writepage(folio, wbc, data);",
            "\t\t\tnr = folio_nr_pages(folio);",
            "\t\t\tif (unlikely(error)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * Handle errors according to the type of",
            "\t\t\t\t * writeback. There's no need to continue for",
            "\t\t\t\t * background writeback. Just push done_index",
            "\t\t\t\t * past this page so media errors won't choke",
            "\t\t\t\t * writeout for the entire file. For integrity",
            "\t\t\t\t * writeback, we must process the entire dirty",
            "\t\t\t\t * set regardless of errors because the fs may",
            "\t\t\t\t * still have state to clear for each page. In",
            "\t\t\t\t * that case we continue processing and return",
            "\t\t\t\t * the first error.",
            "\t\t\t\t */",
            "\t\t\t\tif (error == AOP_WRITEPAGE_ACTIVATE) {",
            "\t\t\t\t\tfolio_unlock(folio);",
            "\t\t\t\t\terror = 0;",
            "\t\t\t\t} else if (wbc->sync_mode != WB_SYNC_ALL) {",
            "\t\t\t\t\tret = error;",
            "\t\t\t\t\tdone_index = folio->index + nr;",
            "\t\t\t\t\tdone = 1;",
            "\t\t\t\t\tbreak;",
            "\t\t\t\t}",
            "\t\t\t\tif (!ret)",
            "\t\t\t\t\tret = error;",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * We stop writing back only if we are not doing",
            "\t\t\t * integrity sync. In case of integrity sync we have to",
            "\t\t\t * keep going until we have written all the pages",
            "\t\t\t * we tagged for writeback prior to entering this loop.",
            "\t\t\t */",
            "\t\t\twbc->nr_to_write -= nr;",
            "\t\t\tif (wbc->nr_to_write <= 0 &&",
            "\t\t\t    wbc->sync_mode == WB_SYNC_NONE) {",
            "\t\t\t\tdone = 1;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t\tfolio_batch_release(&fbatch);",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\t/*",
            "\t * If we hit the last page and there is more work to be done: wrap",
            "\t * back the index back to the start of the file for the next",
            "\t * time we are called.",
            "\t */",
            "\tif (wbc->range_cyclic && !done)",
            "\t\tdone_index = 0;",
            "\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))",
            "\t\tmapping->writeback_index = done_index;",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "dirty_writeback_centisecs_handler, laptop_mode_timer_fn, laptop_io_completion, laptop_sync_completion, writeback_set_ratelimit, page_writeback_cpu_online, page_writeback_init, tag_pages_for_writeback, write_cache_pages",
          "description": "dirty_writeback_centisecs_handler 设置脏页写回间隔，laptop_mode_timer_fn 处理笔记本模式下定时触发写回。writeback_set_ratelimit 根据CPU数量动态调整速率限制参数。tag_pages_for_writeback 标记待写回页面，write_cache_pages 执行实际的缓存页面写回流程。",
          "similarity": 0.5429425239562988
        },
        {
          "chunk_id": 0,
          "file_path": "mm/page-writeback.c",
          "start_line": 1,
          "end_line": 163,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * mm/page-writeback.c",
            " *",
            " * Copyright (C) 2002, Linus Torvalds.",
            " * Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra",
            " *",
            " * Contains functions related to writing back dirty pages at the",
            " * address_space level.",
            " *",
            " * 10Apr2002\tAndrew Morton",
            " *\t\tInitial version",
            " */",
            "",
            "#include <linux/kernel.h>",
            "#include <linux/math64.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/fs.h>",
            "#include <linux/mm.h>",
            "#include <linux/swap.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/writeback.h>",
            "#include <linux/init.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/mpage.h>",
            "#include <linux/rmap.h>",
            "#include <linux/percpu.h>",
            "#include <linux/smp.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/cpu.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/pagevec.h>",
            "#include <linux/timer.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm_inline.h>",
            "#include <trace/events/writeback.h>",
            "",
            "#include \"internal.h\"",
            "",
            "/*",
            " * Sleep at most 200ms at a time in balance_dirty_pages().",
            " */",
            "#define MAX_PAUSE\t\tmax(HZ/5, 1)",
            "",
            "/*",
            " * Try to keep balance_dirty_pages() call intervals higher than this many pages",
            " * by raising pause time to max_pause when falls below it.",
            " */",
            "#define DIRTY_POLL_THRESH\t(128 >> (PAGE_SHIFT - 10))",
            "",
            "/*",
            " * Estimate write bandwidth at 200ms intervals.",
            " */",
            "#define BANDWIDTH_INTERVAL\tmax(HZ/5, 1)",
            "",
            "#define RATELIMIT_CALC_SHIFT\t10",
            "",
            "/*",
            " * After a CPU has dirtied this many pages, balance_dirty_pages_ratelimited",
            " * will look to see if it needs to force writeback or throttling.",
            " */",
            "static long ratelimit_pages = 32;",
            "",
            "/* The following parameters are exported via /proc/sys/vm */",
            "",
            "/*",
            " * Start background writeback (via writeback threads) at this percentage",
            " */",
            "static int dirty_background_ratio = 10;",
            "",
            "/*",
            " * dirty_background_bytes starts at 0 (disabled) so that it is a function of",
            " * dirty_background_ratio * the amount of dirtyable memory",
            " */",
            "static unsigned long dirty_background_bytes;",
            "",
            "/*",
            " * free highmem will not be subtracted from the total free memory",
            " * for calculating free ratios if vm_highmem_is_dirtyable is true",
            " */",
            "static int vm_highmem_is_dirtyable;",
            "",
            "/*",
            " * The generator of dirty data starts writeback at this percentage",
            " */",
            "static int vm_dirty_ratio = 20;",
            "",
            "/*",
            " * vm_dirty_bytes starts at 0 (disabled) so that it is a function of",
            " * vm_dirty_ratio * the amount of dirtyable memory",
            " */",
            "static unsigned long vm_dirty_bytes;",
            "",
            "/*",
            " * The interval between `kupdate'-style writebacks",
            " */",
            "unsigned int dirty_writeback_interval = 5 * 100; /* centiseconds */",
            "",
            "EXPORT_SYMBOL_GPL(dirty_writeback_interval);",
            "",
            "/*",
            " * The longest time for which data is allowed to remain dirty",
            " */",
            "unsigned int dirty_expire_interval = 30 * 100; /* centiseconds */",
            "",
            "/*",
            " * Flag that puts the machine in \"laptop mode\". Doubles as a timeout in jiffies:",
            " * a full sync is triggered after this time elapses without any disk activity.",
            " */",
            "int laptop_mode;",
            "",
            "EXPORT_SYMBOL(laptop_mode);",
            "",
            "/* End of sysctl-exported parameters */",
            "",
            "struct wb_domain global_wb_domain;",
            "",
            "/* consolidated parameters for balance_dirty_pages() and its subroutines */",
            "struct dirty_throttle_control {",
            "#ifdef CONFIG_CGROUP_WRITEBACK",
            "\tstruct wb_domain\t*dom;",
            "\tstruct dirty_throttle_control *gdtc;\t/* only set in memcg dtc's */",
            "#endif",
            "\tstruct bdi_writeback\t*wb;",
            "\tstruct fprop_local_percpu *wb_completions;",
            "",
            "\tunsigned long\t\tavail;\t\t/* dirtyable */",
            "\tunsigned long\t\tdirty;\t\t/* file_dirty + write + nfs */",
            "\tunsigned long\t\tthresh;\t\t/* dirty threshold */",
            "\tunsigned long\t\tbg_thresh;\t/* dirty background threshold */",
            "",
            "\tunsigned long\t\twb_dirty;\t/* per-wb counterparts */",
            "\tunsigned long\t\twb_thresh;",
            "\tunsigned long\t\twb_bg_thresh;",
            "",
            "\tunsigned long\t\tpos_ratio;",
            "};",
            "",
            "/*",
            " * Length of period for aging writeout fractions of bdis. This is an",
            " * arbitrarily chosen number. The longer the period, the slower fractions will",
            " * reflect changes in current writeout rate.",
            " */",
            "#define VM_COMPLETIONS_PERIOD_LEN (3*HZ)",
            "",
            "#ifdef CONFIG_CGROUP_WRITEBACK",
            "",
            "#define GDTC_INIT(__wb)\t\t.wb = (__wb),\t\t\t\t\\",
            "\t\t\t\t.dom = &global_wb_domain,\t\t\\",
            "\t\t\t\t.wb_completions = &(__wb)->completions",
            "",
            "#define GDTC_INIT_NO_WB\t\t.dom = &global_wb_domain",
            "",
            "#define MDTC_INIT(__wb, __gdtc)\t.wb = (__wb),\t\t\t\t\\",
            "\t\t\t\t.dom = mem_cgroup_wb_domain(__wb),\t\\",
            "\t\t\t\t.wb_completions = &(__wb)->memcg_completions, \\",
            "\t\t\t\t.gdtc = __gdtc",
            ""
          ],
          "function_name": null,
          "description": "定义与页面回写相关的全局参数和结构体，包括脏页背景写回阈值、脏页比例限制及内存组（cgroups）相关控制参数，用于管理内存中脏页的写回策略。",
          "similarity": 0.5225217938423157
        }
      ]
    },
    {
      "source_file": "kernel/padata.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:13:27\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `padata.c`\n\n---\n\n# padata.c 技术文档\n\n## 文件概述\n\n`padata.c` 实现了 Linux 内核中的 **padata**（parallel data）框架，提供了一个通用接口，用于在多个 CPU 上并行处理数据流，同时保证结果按照原始顺序串行化输出。该机制常用于需要高吞吐量并行计算但又要求结果有序的场景（如加密、压缩等）。padata 框架通过将任务分发到多个 CPU 并行执行，并在专用的串行化 CPU 上按序回调，实现了并行与有序的统一。\n\n## 核心功能\n\n### 主要数据结构\n\n- `struct padata_work`：封装工作项，用于将并行任务提交到工作队列。\n- `struct padata_mt_job_state`：用于多线程作业（multi-threaded job）的状态管理，包含完成通知、工作计数和数据块大小。\n- `struct parallel_data`：并行数据上下文，包含 CPU 掩码、序列号、重排序队列等关键状态。\n- `struct padata_priv`：用户传递的任务私有数据结构，必须包含 `.parallel` 和 `.serial` 回调函数。\n\n### 主要函数\n\n- `padata_do_parallel()`：核心入口函数，将任务分发到并行工作队列。\n- `padata_parallel_worker()`：并行工作线程执行函数，调用用户提供的 `.parallel` 回调。\n- `padata_find_next()`：在重排序队列中查找下一个应被串行化的任务。\n- `padata_cpu_hash()`：根据序列号将任务哈希到特定的并行 CPU。\n- `padata_index_to_cpu()`：将逻辑 CPU 索引映射到实际的 CPU ID。\n- `padata_work_alloc()` / `padata_work_free()`：管理工作项的分配与回收。\n- `padata_work_alloc_mt()` / `padata_works_free()`：用于多线程作业的批量工作项管理。\n\n## 关键实现\n\n### 1. 并行-串行模型\npadata 采用“并行执行 + 有序串行化”模型：\n- **并行阶段**：任务通过 `padata_do_parallel()` 分发到 `parallel_wq` 工作队列，在 `pd->cpumask.pcpu` 指定的 CPU 上并行执行（BH 关闭）。\n- **串行阶段**：任务完成后进入 per-CPU 重排序队列，由 `padata_reorder()` 机制按 `seq_nr` 顺序触发 `.serial` 回调，回调在 `pd->cpumask.cbcpu` 指定的 CPU 上执行。\n\n### 2. 顺序保证机制\n- 每个任务分配唯一递增的 `seq_nr`。\n- 任务完成后按 `seq_nr % weight(pcpu_mask)` 哈希到特定 CPU 的重排序队列。\n- `padata_find_next()` 仅当 `seq_nr == pd->processed` 时才取出任务，确保严格 FIFO 顺序。\n\n### 3. 工作项管理\n- 全局预分配 `padata_work` 对象池（`padata_free_works` 链表），避免运行时内存分配。\n- 使用自旋锁 `padata_works_lock` 保护工作项分配/回收，支持 BH 上下文安全。\n\n### 4. CPU 掩码处理\n- 支持动态 CPU 掩码（`pcpu` 用于并行，`cbcpu` 用于串行回调）。\n- 若用户指定的 `cb_cpu` 不在 `cbcpu` 掩码中，自动选择回退 CPU（通过模运算和 `cpumask_next`）。\n\n### 5. 引用计数与生命周期\n- `parallel_data` 使用 `refcount_t` 管理生命周期，`padata_get_pd()` / `padata_put_pd()` 控制引用。\n- RCU 保护 `pd` 指针的读取（`rcu_dereference_bh`），确保并发安全。\n\n## 依赖关系\n\n- **内核子系统**：\n  - `workqueue`：依赖内核工作队列机制执行并行任务。\n  - `RCU`：用于 `parallel_data` 结构的并发读取保护。\n  - `percpu`：使用 per-CPU 变量存储重排序队列（`reorder_list`）。\n  - `sysfs`：支持通过 sysfs 接口配置 padata 实例（未在片段中体现，但头文件包含）。\n- **头文件依赖**：\n  - `<linux/padata.h>`：定义用户接口和核心结构。\n  - `<linux/completion.h>`、`<linux/cpumask.h>`、`<linux/rcupdate.h>` 等提供基础功能。\n\n## 使用场景\n\n1. **加密子系统**：如 IPsec 或 dm-crypt 使用 padata 并行处理多个数据块的加密/解密，同时保证输出顺序。\n2. **压缩/解压缩**：在需要高吞吐量的压缩场景中并行处理数据流。\n3. **批量数据处理**：任何需要将大数据集分片并行处理，但要求结果按输入顺序交付的内核模块。\n4. **多线程作业辅助**：通过 `padata_work_alloc_mt()` 接口，辅助实现内核态多线程任务分发（如大内存初始化）。\n\n> **注意**：所有通过 `padata_do_parallel()` 提交的任务**必须**调用 `padata_do_serial()` 完成串行化阶段，否则会导致资源泄漏和顺序错乱。",
      "similarity": 0.5689221620559692,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/padata.c",
          "start_line": 539,
          "end_line": 643,
          "content": [
            "static void padata_init_reorder_list(struct parallel_data *pd)",
            "{",
            "\tint cpu;",
            "\tstruct padata_list *list;",
            "",
            "\tfor_each_cpu(cpu, pd->cpumask.pcpu) {",
            "\t\tlist = per_cpu_ptr(pd->reorder_list, cpu);",
            "\t\t__padata_list_init(list);",
            "\t}",
            "}",
            "static void padata_free_pd(struct parallel_data *pd)",
            "{",
            "\tfree_cpumask_var(pd->cpumask.pcpu);",
            "\tfree_cpumask_var(pd->cpumask.cbcpu);",
            "\tfree_percpu(pd->reorder_list);",
            "\tfree_percpu(pd->squeue);",
            "\tkfree(pd);",
            "}",
            "static void __padata_start(struct padata_instance *pinst)",
            "{",
            "\tpinst->flags |= PADATA_INIT;",
            "}",
            "static void __padata_stop(struct padata_instance *pinst)",
            "{",
            "\tif (!(pinst->flags & PADATA_INIT))",
            "\t\treturn;",
            "",
            "\tpinst->flags &= ~PADATA_INIT;",
            "",
            "\tsynchronize_rcu();",
            "}",
            "static int padata_replace_one(struct padata_shell *ps)",
            "{",
            "\tstruct parallel_data *pd_new;",
            "",
            "\tpd_new = padata_alloc_pd(ps);",
            "\tif (!pd_new)",
            "\t\treturn -ENOMEM;",
            "",
            "\tps->opd = rcu_dereference_protected(ps->pd, 1);",
            "\trcu_assign_pointer(ps->pd, pd_new);",
            "",
            "\treturn 0;",
            "}",
            "static int padata_replace(struct padata_instance *pinst)",
            "{",
            "\tstruct padata_shell *ps;",
            "\tint err = 0;",
            "",
            "\tpinst->flags |= PADATA_RESET;",
            "",
            "\tlist_for_each_entry(ps, &pinst->pslist, list) {",
            "\t\terr = padata_replace_one(ps);",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tsynchronize_rcu();",
            "",
            "\tlist_for_each_entry_continue_reverse(ps, &pinst->pslist, list)",
            "\t\tpadata_put_pd(ps->opd);",
            "",
            "\tpinst->flags &= ~PADATA_RESET;",
            "",
            "\treturn err;",
            "}",
            "static bool padata_validate_cpumask(struct padata_instance *pinst,",
            "\t\t\t\t    const struct cpumask *cpumask)",
            "{",
            "\tif (!cpumask_intersects(cpumask, cpu_online_mask)) {",
            "\t\tpinst->flags |= PADATA_INVALID;",
            "\t\treturn false;",
            "\t}",
            "",
            "\tpinst->flags &= ~PADATA_INVALID;",
            "\treturn true;",
            "}",
            "static int __padata_set_cpumasks(struct padata_instance *pinst,",
            "\t\t\t\t cpumask_var_t pcpumask,",
            "\t\t\t\t cpumask_var_t cbcpumask)",
            "{",
            "\tint valid;",
            "\tint err;",
            "",
            "\tvalid = padata_validate_cpumask(pinst, pcpumask);",
            "\tif (!valid) {",
            "\t\t__padata_stop(pinst);",
            "\t\tgoto out_replace;",
            "\t}",
            "",
            "\tvalid = padata_validate_cpumask(pinst, cbcpumask);",
            "\tif (!valid)",
            "\t\t__padata_stop(pinst);",
            "",
            "out_replace:",
            "\tcpumask_copy(pinst->cpumask.pcpu, pcpumask);",
            "\tcpumask_copy(pinst->cpumask.cbcpu, cbcpumask);",
            "",
            "\terr = padata_setup_cpumasks(pinst) ?: padata_replace(pinst);",
            "",
            "\tif (valid)",
            "\t\t__padata_start(pinst);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "padata_init_reorder_list, padata_free_pd, __padata_start, __padata_stop, padata_replace_one, padata_replace, padata_validate_cpumask, __padata_set_cpumasks",
          "description": "初始化与释放重排列表资源，实现CPU掩码动态配置及有效性校验机制",
          "similarity": 0.5254104137420654
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/padata.c",
          "start_line": 712,
          "end_line": 818,
          "content": [
            "int padata_set_cpumask(struct padata_instance *pinst, int cpumask_type,",
            "\t\t       cpumask_var_t cpumask)",
            "{",
            "\tstruct cpumask *serial_mask, *parallel_mask;",
            "\tint err = -EINVAL;",
            "",
            "\tcpus_read_lock();",
            "\tmutex_lock(&pinst->lock);",
            "",
            "\tswitch (cpumask_type) {",
            "\tcase PADATA_CPU_PARALLEL:",
            "\t\tserial_mask = pinst->cpumask.cbcpu;",
            "\t\tparallel_mask = cpumask;",
            "\t\tbreak;",
            "\tcase PADATA_CPU_SERIAL:",
            "\t\tparallel_mask = pinst->cpumask.pcpu;",
            "\t\tserial_mask = cpumask;",
            "\t\tbreak;",
            "\tdefault:",
            "\t\t goto out;",
            "\t}",
            "",
            "\terr =  __padata_set_cpumasks(pinst, parallel_mask, serial_mask);",
            "",
            "out:",
            "\tmutex_unlock(&pinst->lock);",
            "\tcpus_read_unlock();",
            "",
            "\treturn err;",
            "}",
            "static int __padata_add_cpu(struct padata_instance *pinst, int cpu)",
            "{",
            "\tint err = 0;",
            "",
            "\tif (cpumask_test_cpu(cpu, cpu_online_mask)) {",
            "\t\terr = padata_replace(pinst);",
            "",
            "\t\tif (padata_validate_cpumask(pinst, pinst->cpumask.pcpu) &&",
            "\t\t    padata_validate_cpumask(pinst, pinst->cpumask.cbcpu))",
            "\t\t\t__padata_start(pinst);",
            "\t}",
            "",
            "\treturn err;",
            "}",
            "static int __padata_remove_cpu(struct padata_instance *pinst, int cpu)",
            "{",
            "\tint err = 0;",
            "",
            "\tif (!cpumask_test_cpu(cpu, cpu_online_mask)) {",
            "\t\tif (!padata_validate_cpumask(pinst, pinst->cpumask.pcpu) ||",
            "\t\t    !padata_validate_cpumask(pinst, pinst->cpumask.cbcpu))",
            "\t\t\t__padata_stop(pinst);",
            "",
            "\t\terr = padata_replace(pinst);",
            "\t}",
            "",
            "\treturn err;",
            "}",
            "static inline int pinst_has_cpu(struct padata_instance *pinst, int cpu)",
            "{",
            "\treturn cpumask_test_cpu(cpu, pinst->cpumask.pcpu) ||",
            "\t\tcpumask_test_cpu(cpu, pinst->cpumask.cbcpu);",
            "}",
            "static int padata_cpu_online(unsigned int cpu, struct hlist_node *node)",
            "{",
            "\tstruct padata_instance *pinst;",
            "\tint ret;",
            "",
            "\tpinst = hlist_entry_safe(node, struct padata_instance, cpu_online_node);",
            "\tif (!pinst_has_cpu(pinst, cpu))",
            "\t\treturn 0;",
            "",
            "\tmutex_lock(&pinst->lock);",
            "\tret = __padata_add_cpu(pinst, cpu);",
            "\tmutex_unlock(&pinst->lock);",
            "\treturn ret;",
            "}",
            "static int padata_cpu_dead(unsigned int cpu, struct hlist_node *node)",
            "{",
            "\tstruct padata_instance *pinst;",
            "\tint ret;",
            "",
            "\tpinst = hlist_entry_safe(node, struct padata_instance, cpu_dead_node);",
            "\tif (!pinst_has_cpu(pinst, cpu))",
            "\t\treturn 0;",
            "",
            "\tmutex_lock(&pinst->lock);",
            "\tret = __padata_remove_cpu(pinst, cpu);",
            "\tmutex_unlock(&pinst->lock);",
            "\treturn ret;",
            "}",
            "static void __padata_free(struct padata_instance *pinst)",
            "{",
            "#ifdef CONFIG_HOTPLUG_CPU",
            "\tcpuhp_state_remove_instance_nocalls(CPUHP_PADATA_DEAD,",
            "\t\t\t\t\t    &pinst->cpu_dead_node);",
            "\tcpuhp_state_remove_instance_nocalls(hp_online, &pinst->cpu_online_node);",
            "#endif",
            "",
            "\tWARN_ON(!list_empty(&pinst->pslist));",
            "",
            "\tfree_cpumask_var(pinst->cpumask.pcpu);",
            "\tfree_cpumask_var(pinst->cpumask.cbcpu);",
            "\tdestroy_workqueue(pinst->serial_wq);",
            "\tdestroy_workqueue(pinst->parallel_wq);",
            "\tkfree(pinst);",
            "}"
          ],
          "function_name": "padata_set_cpumask, __padata_add_cpu, __padata_remove_cpu, pinst_has_cpu, padata_cpu_online, padata_cpu_dead, __padata_free",
          "description": "padata_set_cpumask 根据cpumask_type设置并行或串行CPU掩码，调用__padata_set_cpumasks更新内核态掩码并加锁保护。__padata_add_cpu处理CPU上线时的掩码验证和工作队列启动，__padata_remove_cpu处理CPU下线时的掩码验证和工作队列停止。padata_cpu_online和padata_cpu_dead作为CPU热插拔回调，分别触发CPU添加和移除操作。__padata_free释放padata_instance资源，包括销毁工作队列和释放CPU掩码内存。",
          "similarity": 0.5038174390792847
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/padata.c",
          "start_line": 50,
          "end_line": 203,
          "content": [
            "static inline void padata_get_pd(struct parallel_data *pd)",
            "{",
            "\trefcount_inc(&pd->refcnt);",
            "}",
            "static inline void padata_put_pd_cnt(struct parallel_data *pd, int cnt)",
            "{",
            "\tif (refcount_sub_and_test(cnt, &pd->refcnt))",
            "\t\tpadata_free_pd(pd);",
            "}",
            "static inline void padata_put_pd(struct parallel_data *pd)",
            "{",
            "\tpadata_put_pd_cnt(pd, 1);",
            "}",
            "static int padata_index_to_cpu(struct parallel_data *pd, int cpu_index)",
            "{",
            "\tint cpu, target_cpu;",
            "",
            "\ttarget_cpu = cpumask_first(pd->cpumask.pcpu);",
            "\tfor (cpu = 0; cpu < cpu_index; cpu++)",
            "\t\ttarget_cpu = cpumask_next(target_cpu, pd->cpumask.pcpu);",
            "",
            "\treturn target_cpu;",
            "}",
            "static int padata_cpu_hash(struct parallel_data *pd, unsigned int seq_nr)",
            "{",
            "\t/*",
            "\t * Hash the sequence numbers to the cpus by taking",
            "\t * seq_nr mod. number of cpus in use.",
            "\t */",
            "\tint cpu_index = seq_nr % cpumask_weight(pd->cpumask.pcpu);",
            "",
            "\treturn padata_index_to_cpu(pd, cpu_index);",
            "}",
            "static void __ref padata_work_init(struct padata_work *pw, work_func_t work_fn,",
            "\t\t\t\t   void *data, int flags)",
            "{",
            "\tif (flags & PADATA_WORK_ONSTACK)",
            "\t\tINIT_WORK_ONSTACK(&pw->pw_work, work_fn);",
            "\telse",
            "\t\tINIT_WORK(&pw->pw_work, work_fn);",
            "\tpw->pw_data = data;",
            "}",
            "static int __init padata_work_alloc_mt(int nworks, void *data,",
            "\t\t\t\t       struct list_head *head)",
            "{",
            "\tint i;",
            "",
            "\tspin_lock_bh(&padata_works_lock);",
            "\t/* Start at 1 because the current task participates in the job. */",
            "\tfor (i = 1; i < nworks; ++i) {",
            "\t\tstruct padata_work *pw = padata_work_alloc();",
            "",
            "\t\tif (!pw)",
            "\t\t\tbreak;",
            "\t\tpadata_work_init(pw, padata_mt_helper, data, 0);",
            "\t\tlist_add(&pw->pw_list, head);",
            "\t}",
            "\tspin_unlock_bh(&padata_works_lock);",
            "",
            "\treturn i;",
            "}",
            "static void padata_work_free(struct padata_work *pw)",
            "{",
            "\tlockdep_assert_held(&padata_works_lock);",
            "\tlist_add(&pw->pw_list, &padata_free_works);",
            "}",
            "static void __init padata_works_free(struct list_head *works)",
            "{",
            "\tstruct padata_work *cur, *next;",
            "",
            "\tif (list_empty(works))",
            "\t\treturn;",
            "",
            "\tspin_lock_bh(&padata_works_lock);",
            "\tlist_for_each_entry_safe(cur, next, works, pw_list) {",
            "\t\tlist_del(&cur->pw_list);",
            "\t\tpadata_work_free(cur);",
            "\t}",
            "\tspin_unlock_bh(&padata_works_lock);",
            "}",
            "static void padata_parallel_worker(struct work_struct *parallel_work)",
            "{",
            "\tstruct padata_work *pw = container_of(parallel_work, struct padata_work,",
            "\t\t\t\t\t      pw_work);",
            "\tstruct padata_priv *padata = pw->pw_data;",
            "",
            "\tlocal_bh_disable();",
            "\tpadata->parallel(padata);",
            "\tspin_lock(&padata_works_lock);",
            "\tpadata_work_free(pw);",
            "\tspin_unlock(&padata_works_lock);",
            "\tlocal_bh_enable();",
            "}",
            "int padata_do_parallel(struct padata_shell *ps,",
            "\t\t       struct padata_priv *padata, int *cb_cpu)",
            "{",
            "\tstruct padata_instance *pinst = ps->pinst;",
            "\tint i, cpu, cpu_index, err;",
            "\tstruct parallel_data *pd;",
            "\tstruct padata_work *pw;",
            "",
            "\trcu_read_lock_bh();",
            "",
            "\tpd = rcu_dereference_bh(ps->pd);",
            "",
            "\terr = -EINVAL;",
            "\tif (!(pinst->flags & PADATA_INIT) || pinst->flags & PADATA_INVALID)",
            "\t\tgoto out;",
            "",
            "\tif (!cpumask_test_cpu(*cb_cpu, pd->cpumask.cbcpu)) {",
            "\t\tif (cpumask_empty(pd->cpumask.cbcpu))",
            "\t\t\tgoto out;",
            "",
            "\t\t/* Select an alternate fallback CPU and notify the caller. */",
            "\t\tcpu_index = *cb_cpu % cpumask_weight(pd->cpumask.cbcpu);",
            "",
            "\t\tcpu = cpumask_first(pd->cpumask.cbcpu);",
            "\t\tfor (i = 0; i < cpu_index; i++)",
            "\t\t\tcpu = cpumask_next(cpu, pd->cpumask.cbcpu);",
            "",
            "\t\t*cb_cpu = cpu;",
            "\t}",
            "",
            "\terr = -EBUSY;",
            "\tif ((pinst->flags & PADATA_RESET))",
            "\t\tgoto out;",
            "",
            "\tpadata_get_pd(pd);",
            "\tpadata->pd = pd;",
            "\tpadata->cb_cpu = *cb_cpu;",
            "",
            "\tspin_lock(&padata_works_lock);",
            "\tpadata->seq_nr = ++pd->seq_nr;",
            "\tpw = padata_work_alloc();",
            "\tspin_unlock(&padata_works_lock);",
            "",
            "\tif (!pw) {",
            "\t\t/* Maximum works limit exceeded, run in the current task. */",
            "\t\tpadata->parallel(padata);",
            "\t}",
            "",
            "\trcu_read_unlock_bh();",
            "",
            "\tif (pw) {",
            "\t\tpadata_work_init(pw, padata_parallel_worker, padata, 0);",
            "\t\tqueue_work(pinst->parallel_wq, &pw->pw_work);",
            "\t}",
            "",
            "\treturn 0;",
            "out:",
            "\trcu_read_unlock_bh();",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "padata_get_pd, padata_put_pd_cnt, padata_put_pd, padata_index_to_cpu, padata_cpu_hash, padata_work_init, padata_work_alloc_mt, padata_work_free, padata_works_free, padata_parallel_worker, padata_do_parallel",
          "description": "提供并行数据引用计数、CPU映射算法、工作项初始化与分配逻辑，以及并行任务执行流程",
          "similarity": 0.5028713345527649
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/padata.c",
          "start_line": 296,
          "end_line": 402,
          "content": [
            "static void padata_reorder(struct padata_priv *padata)",
            "{",
            "\tstruct parallel_data *pd = padata->pd;",
            "\tstruct padata_instance *pinst = pd->ps->pinst;",
            "\tunsigned int processed;",
            "\tint cpu;",
            "",
            "\tprocessed = pd->processed;",
            "\tcpu = pd->cpu;",
            "",
            "\tdo {",
            "\t\tstruct padata_serial_queue *squeue;",
            "\t\tint cb_cpu;",
            "",
            "\t\tcpu = cpumask_next_wrap(cpu, pd->cpumask.pcpu, -1, false);",
            "\t\tprocessed++;",
            "",
            "\t\tcb_cpu = padata->cb_cpu;",
            "\t\tsqueue = per_cpu_ptr(pd->squeue, cb_cpu);",
            "",
            "\t\tspin_lock(&squeue->serial.lock);",
            "\t\tlist_add_tail(&padata->list, &squeue->serial.list);",
            "\t\tqueue_work_on(cb_cpu, pinst->serial_wq, &squeue->work);",
            "",
            "\t\t/*",
            "\t\t * If the next object that needs serialization is parallel",
            "\t\t * processed by another cpu and is still on it's way to the",
            "\t\t * cpu's reorder queue, end the loop.",
            "\t\t */",
            "\t\tpadata = padata_find_next(pd, cpu, processed);",
            "\t\tspin_unlock(&squeue->serial.lock);",
            "\t} while (padata);",
            "}",
            "static void padata_serial_worker(struct work_struct *serial_work)",
            "{",
            "\tstruct padata_serial_queue *squeue;",
            "\tstruct parallel_data *pd;",
            "\tLIST_HEAD(local_list);",
            "\tint cnt;",
            "",
            "\tlocal_bh_disable();",
            "\tsqueue = container_of(serial_work, struct padata_serial_queue, work);",
            "\tpd = squeue->pd;",
            "",
            "\tspin_lock(&squeue->serial.lock);",
            "\tlist_replace_init(&squeue->serial.list, &local_list);",
            "\tspin_unlock(&squeue->serial.lock);",
            "",
            "\tcnt = 0;",
            "",
            "\twhile (!list_empty(&local_list)) {",
            "\t\tstruct padata_priv *padata;",
            "",
            "\t\tpadata = list_entry(local_list.next,",
            "\t\t\t\t    struct padata_priv, list);",
            "",
            "\t\tlist_del_init(&padata->list);",
            "",
            "\t\tpadata->serial(padata);",
            "\t\tcnt++;",
            "\t}",
            "\tlocal_bh_enable();",
            "",
            "\tpadata_put_pd_cnt(pd, cnt);",
            "}",
            "void padata_do_serial(struct padata_priv *padata)",
            "{",
            "\tstruct parallel_data *pd = padata->pd;",
            "\tint hashed_cpu = padata_cpu_hash(pd, padata->seq_nr);",
            "\tstruct padata_list *reorder = per_cpu_ptr(pd->reorder_list, hashed_cpu);",
            "\tstruct padata_priv *cur;",
            "\tstruct list_head *pos;",
            "\tbool gotit = true;",
            "",
            "\tspin_lock(&reorder->lock);",
            "\t/* Sort in ascending order of sequence number. */",
            "\tlist_for_each_prev(pos, &reorder->list) {",
            "\t\tcur = list_entry(pos, struct padata_priv, list);",
            "\t\t/* Compare by difference to consider integer wrap around */",
            "\t\tif ((signed int)(cur->seq_nr - padata->seq_nr) < 0)",
            "\t\t\tbreak;",
            "\t}",
            "\tif (padata->seq_nr != pd->processed) {",
            "\t\tgotit = false;",
            "\t\tlist_add(&padata->list, pos);",
            "\t}",
            "\tspin_unlock(&reorder->lock);",
            "",
            "\tif (gotit)",
            "\t\tpadata_reorder(padata);",
            "}",
            "static int padata_setup_cpumasks(struct padata_instance *pinst)",
            "{",
            "\tstruct workqueue_attrs *attrs;",
            "\tint err;",
            "",
            "\tattrs = alloc_workqueue_attrs();",
            "\tif (!attrs)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/* Restrict parallel_wq workers to pd->cpumask.pcpu. */",
            "\tcpumask_copy(attrs->cpumask, pinst->cpumask.pcpu);",
            "\terr = apply_workqueue_attrs(pinst->parallel_wq, attrs);",
            "\tfree_workqueue_attrs(attrs);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "padata_reorder, padata_serial_worker, padata_do_serial, padata_setup_cpumasks",
          "description": "实现串行化队列的排序插入、工作项处理函数及CPU掩码配置功能",
          "similarity": 0.4707874059677124
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/padata.c",
          "start_line": 837,
          "end_line": 969,
          "content": [
            "static void padata_sysfs_release(struct kobject *kobj)",
            "{",
            "\tstruct padata_instance *pinst = kobj2pinst(kobj);",
            "\t__padata_free(pinst);",
            "}",
            "static ssize_t show_cpumask(struct padata_instance *pinst,",
            "\t\t\t    struct attribute *attr,  char *buf)",
            "{",
            "\tstruct cpumask *cpumask;",
            "\tssize_t len;",
            "",
            "\tmutex_lock(&pinst->lock);",
            "\tif (!strcmp(attr->name, \"serial_cpumask\"))",
            "\t\tcpumask = pinst->cpumask.cbcpu;",
            "\telse",
            "\t\tcpumask = pinst->cpumask.pcpu;",
            "",
            "\tlen = snprintf(buf, PAGE_SIZE, \"%*pb\\n\",",
            "\t\t       nr_cpu_ids, cpumask_bits(cpumask));",
            "\tmutex_unlock(&pinst->lock);",
            "\treturn len < PAGE_SIZE ? len : -EINVAL;",
            "}",
            "static ssize_t store_cpumask(struct padata_instance *pinst,",
            "\t\t\t     struct attribute *attr,",
            "\t\t\t     const char *buf, size_t count)",
            "{",
            "\tcpumask_var_t new_cpumask;",
            "\tssize_t ret;",
            "\tint mask_type;",
            "",
            "\tif (!alloc_cpumask_var(&new_cpumask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tret = bitmap_parse(buf, count, cpumask_bits(new_cpumask),",
            "\t\t\t   nr_cpumask_bits);",
            "\tif (ret < 0)",
            "\t\tgoto out;",
            "",
            "\tmask_type = !strcmp(attr->name, \"serial_cpumask\") ?",
            "\t\tPADATA_CPU_SERIAL : PADATA_CPU_PARALLEL;",
            "\tret = padata_set_cpumask(pinst, mask_type, new_cpumask);",
            "\tif (!ret)",
            "\t\tret = count;",
            "",
            "out:",
            "\tfree_cpumask_var(new_cpumask);",
            "\treturn ret;",
            "}",
            "static ssize_t padata_sysfs_show(struct kobject *kobj,",
            "\t\t\t\t struct attribute *attr, char *buf)",
            "{",
            "\tstruct padata_instance *pinst;",
            "\tstruct padata_sysfs_entry *pentry;",
            "\tssize_t ret = -EIO;",
            "",
            "\tpinst = kobj2pinst(kobj);",
            "\tpentry = attr2pentry(attr);",
            "\tif (pentry->show)",
            "\t\tret = pentry->show(pinst, attr, buf);",
            "",
            "\treturn ret;",
            "}",
            "static ssize_t padata_sysfs_store(struct kobject *kobj, struct attribute *attr,",
            "\t\t\t\t  const char *buf, size_t count)",
            "{",
            "\tstruct padata_instance *pinst;",
            "\tstruct padata_sysfs_entry *pentry;",
            "\tssize_t ret = -EIO;",
            "",
            "\tpinst = kobj2pinst(kobj);",
            "\tpentry = attr2pentry(attr);",
            "\tif (pentry->store)",
            "\t\tret = pentry->store(pinst, attr, buf, count);",
            "",
            "\treturn ret;",
            "}",
            "void padata_free(struct padata_instance *pinst)",
            "{",
            "\tkobject_put(&pinst->kobj);",
            "}",
            "void padata_free_shell(struct padata_shell *ps)",
            "{",
            "\tstruct parallel_data *pd;",
            "",
            "\tif (!ps)",
            "\t\treturn;",
            "",
            "\tmutex_lock(&ps->pinst->lock);",
            "\tlist_del(&ps->list);",
            "\tpd = rcu_dereference_protected(ps->pd, 1);",
            "\tpadata_put_pd(pd);",
            "\tmutex_unlock(&ps->pinst->lock);",
            "",
            "\tkfree(ps);",
            "}",
            "void __init padata_init(void)",
            "{",
            "\tunsigned int i, possible_cpus;",
            "#ifdef CONFIG_HOTPLUG_CPU",
            "\tint ret;",
            "",
            "\tret = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN, \"padata:online\",",
            "\t\t\t\t      padata_cpu_online, NULL);",
            "\tif (ret < 0)",
            "\t\tgoto err;",
            "\thp_online = ret;",
            "",
            "\tret = cpuhp_setup_state_multi(CPUHP_PADATA_DEAD, \"padata:dead\",",
            "\t\t\t\t      NULL, padata_cpu_dead);",
            "\tif (ret < 0)",
            "\t\tgoto remove_online_state;",
            "#endif",
            "",
            "\tpossible_cpus = num_possible_cpus();",
            "\tpadata_works = kmalloc_array(possible_cpus, sizeof(struct padata_work),",
            "\t\t\t\t     GFP_KERNEL);",
            "\tif (!padata_works)",
            "\t\tgoto remove_dead_state;",
            "",
            "\tfor (i = 0; i < possible_cpus; ++i)",
            "\t\tlist_add(&padata_works[i].pw_list, &padata_free_works);",
            "",
            "\treturn;",
            "",
            "remove_dead_state:",
            "#ifdef CONFIG_HOTPLUG_CPU",
            "\tcpuhp_remove_multi_state(CPUHP_PADATA_DEAD);",
            "remove_online_state:",
            "\tcpuhp_remove_multi_state(hp_online);",
            "err:",
            "#endif",
            "\tpr_warn(\"padata: initialization failed\\n\");",
            "}"
          ],
          "function_name": "padata_sysfs_release, show_cpumask, store_cpumask, padata_sysfs_show, padata_sysfs_store, padata_free, padata_free_shell, padata_init",
          "description": "padata_sysfs_release通过kobject转换获取padata_instance并调用__padata_free完成资源回收。show_cpumask和store_cpumask分别实现sysfs属性的读取和写入，支持串行/并行CPU掩码的展示与配置。padata_sysfs_show/store作为通用sysfs接口，委派到具体实现函数。padata_init初始化模块，注册CPU热插拔回调并分配工作结构体数组。padata_free和padata_free_shell分别释放padata_instance及shell结构体，防止内存泄漏。",
          "similarity": 0.4683939814567566
        }
      ]
    }
  ]
}