{
  "query": "基于套接字的分布式系统负载均衡方案",
  "timestamp": "2025-12-26 01:24:44",
  "retrieved_files": [
    {
      "source_file": "kernel/irq/irqdesc.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:59:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\irqdesc.c`\n\n---\n\n# `irq/irqdesc.c` 技术文档\n\n## 1. 文件概述\n\n`irq/irqdesc.c` 是 Linux 内核通用中断子系统（Generic IRQ）的核心实现文件之一，负责中断描述符（`struct irq_desc`）的分配、初始化、管理和释放。该文件实现了中断描述符的生命周期管理，包括在稀疏 IRQ（`CONFIG_SPARSE_IRQ`）配置下的动态分配机制，以及与 SMP（对称多处理）相关的中断亲和性（affinity）管理。它为上层中断处理（如设备驱动注册中断处理函数）和底层硬件中断控制器（通过 `irq_chip`）之间提供了统一的抽象层。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct irq_desc`**：中断描述符，代表一个逻辑中断号（IRQ number），包含中断状态、处理函数、统计信息、锁、亲和性掩码等。\n- **`struct irq_data`**：嵌入在 `irq_desc` 中，包含与硬件中断控制器相关的数据（如 `irq_chip`、`hwirq`、`irq_domain` 等）。\n- **`struct irq_common_data`**：`irq_desc` 和 `irq_data` 共享的数据，如 MSI 描述符、亲和性掩码等。\n- **`sparse_irqs`**：基于 Maple Tree 的稀疏 IRQ 描述符存储结构，用于动态分配 IRQ 号。\n\n### 主要函数\n- **`init_desc()`**：初始化一个 `irq_desc` 实例，包括分配 per-CPU 统计结构、SMP 掩码、初始化锁和默认值。\n- **`desc_set_defaults()`**：设置 `irq_desc` 的默认初始状态（如禁用、屏蔽、默认处理函数为 `handle_bad_irq`）。\n- **`alloc_masks()` / `free_masks()` / `desc_smp_init()`**：SMP 相关的亲和性掩码（affinity、effective_affinity、pending_mask）的分配、释放和初始化。\n- **`irq_find_free_area()` / `irq_find_at_or_after()`**：在稀疏 IRQ 模式下查找可用的 IRQ 号范围或下一个可用 IRQ。\n- **`irq_insert_desc()` / `delete_irq_desc()`**：将 `irq_desc` 插入或从稀疏 IRQ 的 Maple Tree 中删除。\n- **`init_irq_default_affinity()`**：初始化默认的中断亲和性掩码（通常为所有 CPU）。\n- **`irq_kobj_release()` 及相关 sysfs 属性函数**：实现 IRQ 描述符的 sysfs 接口（如 `per_cpu_count`、`chip_name`、`hwirq` 等）。\n\n### 全局变量\n- **`nr_irqs`**：系统支持的最大 IRQ 数量，可被平台代码覆盖。\n- **`irq_default_affinity`**：默认的中断亲和性 CPU 掩码（SMP 模式下）。\n- **`irq_desc_lock_class`**：用于 lockdep 的 IRQ 描述符自旋锁的统一锁类。\n\n## 3. 关键实现\n\n### 稀疏 IRQ 管理（`CONFIG_SPARSE_IRQ`）\n- 使用 **Maple Tree** 数据结构（`sparse_irqs`）替代传统的静态数组，支持动态分配 IRQ 描述符。\n- `irq_find_free_area()` 利用 Maple Tree 的空闲区间查找功能，高效分配连续的 IRQ 号。\n- `irq_insert_desc()` 和 `delete_irq_desc()` 通过 RCU 安全地插入/删除描述符，支持运行时 IRQ 的动态增删。\n- 每个 `irq_desc` 作为独立的 kobject，通过 sysfs 暴露属性（如中断计数、芯片名称等）。\n\n### SMP 中断亲和性\n- **亲和性掩码**：每个 IRQ 可配置其允许运行的 CPU 集合（`affinity`），支持负载均衡和局部性优化。\n- **有效亲和性**（`effective_affinity`）：实际生效的亲和性（可能受中断迁移或 pending 状态影响）。\n- **Pending 掩码**（`pending_mask`）：用于在中断迁移过程中暂存中断事件。\n- 启动参数 `irqaffinity=` 可设置全局默认亲和性，但至少包含引导 CPU 以防配置错误。\n\n### 描述符初始化\n- `init_desc()` 完成描述符的完整初始化：\n  - 分配 per-CPU 中断统计结构（`kstat_irqs`）。\n  - 初始化 SMP 相关掩码（若启用）。\n  - 设置自旋锁（带 lockdep 类）和互斥锁（`request_mutex`）。\n  - 调用 `desc_set_defaults()` 设置默认状态（禁用、屏蔽、无效处理函数）。\n  - 初始化 RCU 回调（用于稀疏 IRQ 的延迟释放）。\n\n### 锁与并发控制\n- **`desc->lock`**：raw spinlock，保护描述符关键字段（如状态、处理函数），在中断上下文中使用。\n- **`desc->request_mutex`**：mutex，用于串行化中断请求/释放操作（如 `request_irq()`）。\n- **Maple Tree 操作**：通过外部互斥锁（`sparse_irq_lock`）和 RCU 保证并发安全。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/irq.h>`、`<linux/interrupt.h>`：IRQ 子系统核心 API 和数据结构。\n  - `<linux/irqdomain.h>`：硬件中断号（hwirq）到逻辑 IRQ 号的映射。\n  - `<linux/maple_tree.h>`：稀疏 IRQ 的底层存储实现。\n  - `<linux/sysfs.h>`：sysfs 属性支持。\n  - `\"internals.h\"`：IRQ 子系统内部函数和宏。\n- **配置依赖**：\n  - `CONFIG_SMP`：启用多处理器支持（亲和性掩码管理）。\n  - `CONFIG_SPARSE_IRQ`：启用动态 IRQ 分配（替代静态数组）。\n  - `CONFIG_GENERIC_PENDING_IRQ` / `CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK`：扩展的 SMP 中断管理功能。\n- **模块交互**：\n  - **中断控制器驱动**：通过 `irq_chip` 操作硬件，依赖 `irq_desc` 提供的抽象。\n  - **设备驱动**：通过 `request_irq()` 等接口注册中断处理函数，操作 `irq_desc`。\n  - **电源管理**：通过 `wakeup` 属性控制中断的唤醒能力。\n\n## 5. 使用场景\n\n- **系统启动阶段**：\n  - 初始化默认中断亲和性（`init_irq_default_affinity()`）。\n  - 预分配或动态创建平台所需的 IRQ 描述符（通过 `alloc_descs()` 等）。\n- **设备驱动加载/卸载**：\n  - 动态分配 IRQ 描述符（稀疏 IRQ 模式下通过 `irq_alloc_desc()`）。\n  - 注册/注销中断处理函数（修改 `handle_irq` 和 action 链表）。\n- **运行时中断管理**：\n  - 修改中断亲和性（`/proc/irq/<n>/smp_affinity`）。\n  - 查询中断统计信息（`/proc/interrupts`，通过 per-CPU 计数）。\n  - 通过 sysfs 查看 IRQ 属性（芯片名称、硬件 IRQ 号、触发类型等）。\n- **中断迁移**（SMP）：\n  - 在 CPU 热插拔或负载均衡时，更新 `affinity` 和 `pending_mask`。\n- **错误处理**：\n  - 未处理的中断由 `handle_bad_irq` 处理，记录到 `irqs_unhandled`。",
      "similarity": 0.5460495948791504,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 27,
          "end_line": 127,
          "content": [
            "static int __init irq_affinity_setup(char *str)",
            "{",
            "\talloc_bootmem_cpumask_var(&irq_default_affinity);",
            "\tcpulist_parse(str, irq_default_affinity);",
            "\t/*",
            "\t * Set at least the boot cpu. We don't want to end up with",
            "\t * bugreports caused by random commandline masks",
            "\t */",
            "\tcpumask_set_cpu(smp_processor_id(), irq_default_affinity);",
            "\treturn 1;",
            "}",
            "static void __init init_irq_default_affinity(void)",
            "{",
            "\tif (!cpumask_available(irq_default_affinity))",
            "\t\tzalloc_cpumask_var(&irq_default_affinity, GFP_NOWAIT);",
            "\tif (cpumask_empty(irq_default_affinity))",
            "\t\tcpumask_setall(irq_default_affinity);",
            "}",
            "static void __init init_irq_default_affinity(void)",
            "{",
            "}",
            "static int alloc_masks(struct irq_desc *desc, int node)",
            "{",
            "\tif (!zalloc_cpumask_var_node(&desc->irq_common_data.affinity,",
            "\t\t\t\t     GFP_KERNEL, node))",
            "\t\treturn -ENOMEM;",
            "",
            "#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK",
            "\tif (!zalloc_cpumask_var_node(&desc->irq_common_data.effective_affinity,",
            "\t\t\t\t     GFP_KERNEL, node)) {",
            "\t\tfree_cpumask_var(desc->irq_common_data.affinity);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "#endif",
            "",
            "#ifdef CONFIG_GENERIC_PENDING_IRQ",
            "\tif (!zalloc_cpumask_var_node(&desc->pending_mask, GFP_KERNEL, node)) {",
            "#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK",
            "\t\tfree_cpumask_var(desc->irq_common_data.effective_affinity);",
            "#endif",
            "\t\tfree_cpumask_var(desc->irq_common_data.affinity);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "#endif",
            "\treturn 0;",
            "}",
            "static void desc_smp_init(struct irq_desc *desc, int node,",
            "\t\t\t  const struct cpumask *affinity)",
            "{",
            "\tif (!affinity)",
            "\t\taffinity = irq_default_affinity;",
            "\tcpumask_copy(desc->irq_common_data.affinity, affinity);",
            "",
            "#ifdef CONFIG_GENERIC_PENDING_IRQ",
            "\tcpumask_clear(desc->pending_mask);",
            "#endif",
            "#ifdef CONFIG_NUMA",
            "\tdesc->irq_common_data.node = node;",
            "#endif",
            "}",
            "static void free_masks(struct irq_desc *desc)",
            "{",
            "#ifdef CONFIG_GENERIC_PENDING_IRQ",
            "\tfree_cpumask_var(desc->pending_mask);",
            "#endif",
            "\tfree_cpumask_var(desc->irq_common_data.affinity);",
            "#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK",
            "\tfree_cpumask_var(desc->irq_common_data.effective_affinity);",
            "#endif",
            "}",
            "static inline int",
            "alloc_masks(struct irq_desc *desc, int node) { return 0; }",
            "static inline void",
            "desc_smp_init(struct irq_desc *desc, int node, const struct cpumask *affinity) { }",
            "static inline void free_masks(struct irq_desc *desc) { }",
            "static void desc_set_defaults(unsigned int irq, struct irq_desc *desc, int node,",
            "\t\t\t      const struct cpumask *affinity, struct module *owner)",
            "{",
            "\tint cpu;",
            "",
            "\tdesc->irq_common_data.handler_data = NULL;",
            "\tdesc->irq_common_data.msi_desc = NULL;",
            "",
            "\tdesc->irq_data.common = &desc->irq_common_data;",
            "\tdesc->irq_data.irq = irq;",
            "\tdesc->irq_data.chip = &no_irq_chip;",
            "\tdesc->irq_data.chip_data = NULL;",
            "\tirq_settings_clr_and_set(desc, ~0, _IRQ_DEFAULT_INIT_FLAGS);",
            "\tirqd_set(&desc->irq_data, IRQD_IRQ_DISABLED);",
            "\tirqd_set(&desc->irq_data, IRQD_IRQ_MASKED);",
            "\tdesc->handle_irq = handle_bad_irq;",
            "\tdesc->depth = 1;",
            "\tdesc->irq_count = 0;",
            "\tdesc->irqs_unhandled = 0;",
            "\tdesc->tot_count = 0;",
            "\tdesc->name = NULL;",
            "\tdesc->owner = owner;",
            "\tfor_each_possible_cpu(cpu)",
            "\t\t*per_cpu_ptr(desc->kstat_irqs, cpu) = (struct irqstat) { };",
            "\tdesc_smp_init(desc, node, affinity);",
            "}"
          ],
          "function_name": "irq_affinity_setup, init_irq_default_affinity, init_irq_default_affinity, alloc_masks, desc_smp_init, free_masks, alloc_masks, desc_smp_init, free_masks, desc_set_defaults",
          "description": "包含中断亲和性初始化与内存分配相关函数，负责设置默认CPU亲和掩码、分配irq_desc结构体的affinity字段、初始化SMP相关信息及释放相关资源。存在多处函数重载实现，体现不同配置条件下的差异化处理。",
          "similarity": 0.5541790723800659
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 422,
          "end_line": 523,
          "content": [
            "static void irq_sysfs_add(int irq, struct irq_desc *desc) {}",
            "static void irq_sysfs_del(struct irq_desc *desc) {}",
            "void irq_lock_sparse(void)",
            "{",
            "\tmutex_lock(&sparse_irq_lock);",
            "}",
            "void irq_unlock_sparse(void)",
            "{",
            "\tmutex_unlock(&sparse_irq_lock);",
            "}",
            "static void irq_kobj_release(struct kobject *kobj)",
            "{",
            "\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);",
            "",
            "\tfree_masks(desc);",
            "\tfree_percpu(desc->kstat_irqs);",
            "\tkfree(desc);",
            "}",
            "static void delayed_free_desc(struct rcu_head *rhp)",
            "{",
            "\tstruct irq_desc *desc = container_of(rhp, struct irq_desc, rcu);",
            "",
            "\tkobject_put(&desc->kobj);",
            "}",
            "static void free_desc(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tirq_remove_debugfs_entry(desc);",
            "\tunregister_irq_proc(irq, desc);",
            "",
            "\t/*",
            "\t * sparse_irq_lock protects also show_interrupts() and",
            "\t * kstat_irq_usr(). Once we deleted the descriptor from the",
            "\t * sparse tree we can free it. Access in proc will fail to",
            "\t * lookup the descriptor.",
            "\t *",
            "\t * The sysfs entry must be serialized against a concurrent",
            "\t * irq_sysfs_init() as well.",
            "\t */",
            "\tirq_sysfs_del(desc);",
            "\tdelete_irq_desc(irq);",
            "",
            "\t/*",
            "\t * We free the descriptor, masks and stat fields via RCU. That",
            "\t * allows demultiplex interrupts to do rcu based management of",
            "\t * the child interrupts.",
            "\t * This also allows us to use rcu in kstat_irqs_usr().",
            "\t */",
            "\tcall_rcu(&desc->rcu, delayed_free_desc);",
            "}",
            "static int alloc_descs(unsigned int start, unsigned int cnt, int node,",
            "\t\t       const struct irq_affinity_desc *affinity,",
            "\t\t       struct module *owner)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tint i;",
            "",
            "\t/* Validate affinity mask(s) */",
            "\tif (affinity) {",
            "\t\tfor (i = 0; i < cnt; i++) {",
            "\t\t\tif (cpumask_empty(&affinity[i].mask))",
            "\t\t\t\treturn -EINVAL;",
            "\t\t}",
            "\t}",
            "",
            "\tfor (i = 0; i < cnt; i++) {",
            "\t\tconst struct cpumask *mask = NULL;",
            "\t\tunsigned int flags = 0;",
            "",
            "\t\tif (affinity) {",
            "\t\t\tif (affinity->is_managed) {",
            "\t\t\t\tflags = IRQD_AFFINITY_MANAGED |",
            "\t\t\t\t\tIRQD_MANAGED_SHUTDOWN;",
            "\t\t\t}",
            "\t\t\tflags |= IRQD_AFFINITY_SET;",
            "\t\t\tmask = &affinity->mask;",
            "\t\t\tnode = cpu_to_node(cpumask_first(mask));",
            "\t\t\taffinity++;",
            "\t\t}",
            "",
            "\t\tdesc = alloc_desc(start + i, node, flags, mask, owner);",
            "\t\tif (!desc)",
            "\t\t\tgoto err;",
            "\t\tirq_insert_desc(start + i, desc);",
            "\t\tirq_sysfs_add(start + i, desc);",
            "\t\tirq_add_debugfs_entry(start + i, desc);",
            "\t}",
            "\treturn start;",
            "",
            "err:",
            "\tfor (i--; i >= 0; i--)",
            "\t\tfree_desc(start + i);",
            "\treturn -ENOMEM;",
            "}",
            "static int irq_expand_nr_irqs(unsigned int nr)",
            "{",
            "\tif (nr > MAX_SPARSE_IRQS)",
            "\t\treturn -ENOMEM;",
            "\tnr_irqs = nr;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "irq_sysfs_add, irq_sysfs_del, irq_lock_sparse, irq_unlock_sparse, irq_kobj_release, delayed_free_desc, free_desc, alloc_descs, irq_expand_nr_irqs",
          "description": "包含中断描述符的延迟释放机制与批量分配逻辑，利用RCU机制安全释放资源，实现中断描述符的动态扩展与回收，支持多CPU环境下对中断资源的高效管理。",
          "similarity": 0.5298900008201599
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 705,
          "end_line": 824,
          "content": [
            "int generic_handle_irq(unsigned int irq)",
            "{",
            "\treturn handle_irq_desc(irq_to_desc(irq));",
            "}",
            "int generic_handle_irq_safe(unsigned int irq)",
            "{",
            "\tunsigned long flags;",
            "\tint ret;",
            "",
            "\tlocal_irq_save(flags);",
            "\tret = handle_irq_desc(irq_to_desc(irq));",
            "\tlocal_irq_restore(flags);",
            "\treturn ret;",
            "}",
            "int generic_handle_domain_irq(struct irq_domain *domain, unsigned int hwirq)",
            "{",
            "\treturn handle_irq_desc(irq_resolve_mapping(domain, hwirq));",
            "}",
            "int generic_handle_domain_irq_safe(struct irq_domain *domain, unsigned int hwirq)",
            "{",
            "\tunsigned long flags;",
            "\tint ret;",
            "",
            "\tlocal_irq_save(flags);",
            "\tret = handle_irq_desc(irq_resolve_mapping(domain, hwirq));",
            "\tlocal_irq_restore(flags);",
            "\treturn ret;",
            "}",
            "int generic_handle_domain_nmi(struct irq_domain *domain, unsigned int hwirq)",
            "{",
            "\tWARN_ON_ONCE(!in_nmi());",
            "\treturn handle_irq_desc(irq_resolve_mapping(domain, hwirq));",
            "}",
            "void irq_free_descs(unsigned int from, unsigned int cnt)",
            "{",
            "\tint i;",
            "",
            "\tif (from >= nr_irqs || (from + cnt) > nr_irqs)",
            "\t\treturn;",
            "",
            "\tmutex_lock(&sparse_irq_lock);",
            "\tfor (i = 0; i < cnt; i++)",
            "\t\tfree_desc(from + i);",
            "",
            "\tmutex_unlock(&sparse_irq_lock);",
            "}",
            "int __ref",
            "__irq_alloc_descs(int irq, unsigned int from, unsigned int cnt, int node,",
            "\t\t  struct module *owner, const struct irq_affinity_desc *affinity)",
            "{",
            "\tint start, ret;",
            "",
            "\tif (!cnt)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (irq >= 0) {",
            "\t\tif (from > irq)",
            "\t\t\treturn -EINVAL;",
            "\t\tfrom = irq;",
            "\t} else {",
            "\t\t/*",
            "\t\t * For interrupts which are freely allocated the",
            "\t\t * architecture can force a lower bound to the @from",
            "\t\t * argument. x86 uses this to exclude the GSI space.",
            "\t\t */",
            "\t\tfrom = arch_dynirq_lower_bound(from);",
            "\t}",
            "",
            "\tmutex_lock(&sparse_irq_lock);",
            "",
            "\tstart = irq_find_free_area(from, cnt);",
            "\tret = -EEXIST;",
            "\tif (irq >=0 && start != irq)",
            "\t\tgoto unlock;",
            "",
            "\tif (start + cnt > nr_irqs) {",
            "\t\tret = irq_expand_nr_irqs(start + cnt);",
            "\t\tif (ret)",
            "\t\t\tgoto unlock;",
            "\t}",
            "\tret = alloc_descs(start, cnt, node, affinity, owner);",
            "unlock:",
            "\tmutex_unlock(&sparse_irq_lock);",
            "\treturn ret;",
            "}",
            "unsigned int irq_get_next_irq(unsigned int offset)",
            "{",
            "\treturn irq_find_at_or_after(offset);",
            "}",
            "void __irq_put_desc_unlock(struct irq_desc *desc, unsigned long flags, bool bus)",
            "\t__releases(&desc->lock)",
            "{",
            "\traw_spin_unlock_irqrestore(&desc->lock, flags);",
            "\tif (bus)",
            "\t\tchip_bus_sync_unlock(desc);",
            "}",
            "int irq_set_percpu_devid_partition(unsigned int irq,",
            "\t\t\t\t   const struct cpumask *affinity)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tif (!desc)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (desc->percpu_enabled)",
            "\t\treturn -EINVAL;",
            "",
            "\tdesc->percpu_enabled = kzalloc(sizeof(*desc->percpu_enabled), GFP_KERNEL);",
            "",
            "\tif (!desc->percpu_enabled)",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (affinity)",
            "\t\tdesc->percpu_affinity = affinity;",
            "\telse",
            "\t\tdesc->percpu_affinity = cpu_possible_mask;",
            "",
            "\tirq_set_percpu_devid_flags(irq);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "generic_handle_irq, generic_handle_irq_safe, generic_handle_domain_irq, generic_handle_domain_irq_safe, generic_handle_domain_nmi, irq_free_descs, __irq_alloc_descs, irq_get_next_irq, __irq_put_desc_unlock, irq_set_percpu_devid_partition",
          "description": "generic_handle_irq 安全处理通用中断，调用handle_irq_desc。generic_handle_domain_irq 处理IRQ domain映射的硬件中断。irq_free_descs 释放指定范围的中断描述符。__irq_alloc_descs 动态分配连续中断号并初始化描述符。irq_get_next_irq 获取下一个可用中断号。__irq_put_desc_unlock 解锁中断描述符并同步总线。irq_set_percpu_devid_partition 设置中断亲和性分区。",
          "similarity": 0.4781503677368164
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 562,
          "end_line": 667,
          "content": [
            "int __init early_irq_init(void)",
            "{",
            "\tint i, initcnt, node = first_online_node;",
            "\tstruct irq_desc *desc;",
            "",
            "\tinit_irq_default_affinity();",
            "",
            "\t/* Let arch update nr_irqs and return the nr of preallocated irqs */",
            "\tinitcnt = arch_probe_nr_irqs();",
            "\tprintk(KERN_INFO \"NR_IRQS: %d, nr_irqs: %d, preallocated irqs: %d\\n\",",
            "\t       NR_IRQS, nr_irqs, initcnt);",
            "",
            "\tif (WARN_ON(nr_irqs > MAX_SPARSE_IRQS))",
            "\t\tnr_irqs = MAX_SPARSE_IRQS;",
            "",
            "\tif (WARN_ON(initcnt > MAX_SPARSE_IRQS))",
            "\t\tinitcnt = MAX_SPARSE_IRQS;",
            "",
            "\tif (initcnt > nr_irqs)",
            "\t\tnr_irqs = initcnt;",
            "",
            "\tfor (i = 0; i < initcnt; i++) {",
            "\t\tdesc = alloc_desc(i, node, 0, NULL, NULL);",
            "\t\tirq_insert_desc(i, desc);",
            "\t}",
            "\treturn arch_early_irq_init();",
            "}",
            "int __init early_irq_init(void)",
            "{",
            "\tint count, i, node = first_online_node;",
            "\tint ret;",
            "",
            "\tinit_irq_default_affinity();",
            "",
            "\tprintk(KERN_INFO \"NR_IRQS: %d\\n\", NR_IRQS);",
            "",
            "\tcount = ARRAY_SIZE(irq_desc);",
            "",
            "\tfor (i = 0; i < count; i++) {",
            "\t\tret = init_desc(irq_desc + i, i, node, 0, NULL, NULL);",
            "\t\tif (unlikely(ret))",
            "\t\t\tgoto __free_desc_res;",
            "\t}",
            "",
            "\treturn arch_early_irq_init();",
            "",
            "__free_desc_res:",
            "\twhile (--i >= 0) {",
            "\t\tfree_masks(irq_desc + i);",
            "\t\tfree_percpu(irq_desc[i].kstat_irqs);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "static void free_desc(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&desc->lock, flags);",
            "\tdesc_set_defaults(irq, desc, irq_desc_get_node(desc), NULL, NULL);",
            "\traw_spin_unlock_irqrestore(&desc->lock, flags);",
            "\tdelete_irq_desc(irq);",
            "}",
            "static inline int alloc_descs(unsigned int start, unsigned int cnt, int node,",
            "\t\t\t      const struct irq_affinity_desc *affinity,",
            "\t\t\t      struct module *owner)",
            "{",
            "\tu32 i;",
            "",
            "\tfor (i = 0; i < cnt; i++) {",
            "\t\tstruct irq_desc *desc = irq_to_desc(start + i);",
            "",
            "\t\tdesc->owner = owner;",
            "\t\tirq_insert_desc(start + i, desc);",
            "\t}",
            "\treturn start;",
            "}",
            "static int irq_expand_nr_irqs(unsigned int nr)",
            "{",
            "\treturn -ENOMEM;",
            "}",
            "void irq_mark_irq(unsigned int irq)",
            "{",
            "\tmutex_lock(&sparse_irq_lock);",
            "\tirq_insert_desc(irq, irq_desc + irq);",
            "\tmutex_unlock(&sparse_irq_lock);",
            "}",
            "void irq_init_desc(unsigned int irq)",
            "{",
            "\tfree_desc(irq);",
            "}",
            "int handle_irq_desc(struct irq_desc *desc)",
            "{",
            "\tstruct irq_data *data;",
            "",
            "\tif (!desc)",
            "\t\treturn -EINVAL;",
            "",
            "\tdata = irq_desc_get_irq_data(desc);",
            "\tif (WARN_ON_ONCE(!in_hardirq() && handle_enforce_irqctx(data)))",
            "\t\treturn -EPERM;",
            "",
            "\tgeneric_handle_irq_desc(desc);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "early_irq_init, early_irq_init, free_desc, alloc_descs, irq_expand_nr_irqs, irq_mark_irq, irq_init_desc, handle_irq_desc",
          "description": "early_irq_init 初始化早期中断描述符，调用架构特定函数确定中断数量并分配初始中断描述符。free_desc 释放中断描述符并重置为其默认状态。alloc_descs 批量分配中断描述符到指定范围。irq_mark_irq 将中断标记为已初始化。irq_init_desc 初始化指定中断描述符。handle_irq_desc 处理中断描述符，检查硬中断上下文并调用通用处理函数。",
          "similarity": 0.47460657358169556
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 938,
          "end_line": 1027,
          "content": [
            "int irq_set_percpu_devid(unsigned int irq)",
            "{",
            "\treturn irq_set_percpu_devid_partition(irq, NULL);",
            "}",
            "int irq_get_percpu_devid_partition(unsigned int irq, struct cpumask *affinity)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tif (!desc || !desc->percpu_enabled)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (affinity)",
            "\t\tcpumask_copy(affinity, desc->percpu_affinity);",
            "",
            "\treturn 0;",
            "}",
            "void kstat_incr_irq_this_cpu(unsigned int irq)",
            "{",
            "\tkstat_incr_irqs_this_cpu(irq_to_desc(irq));",
            "}",
            "unsigned int kstat_irqs_cpu(unsigned int irq, int cpu)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\treturn desc && desc->kstat_irqs ? per_cpu(desc->kstat_irqs->cnt, cpu) : 0;",
            "}",
            "static bool irq_is_nmi(struct irq_desc *desc)",
            "{",
            "\treturn desc->istate & IRQS_NMI;",
            "}",
            "unsigned int kstat_irqs_desc(struct irq_desc *desc, const struct cpumask *cpumask)",
            "{",
            "\tunsigned int sum = 0;",
            "\tint cpu;",
            "",
            "\tif (!irq_settings_is_per_cpu_devid(desc) &&",
            "\t    !irq_settings_is_per_cpu(desc) &&",
            "\t    !irq_is_nmi(desc))",
            "\t\treturn data_race(desc->tot_count);",
            "",
            "\tfor_each_cpu(cpu, cpumask)",
            "\t\tsum += data_race(per_cpu(desc->kstat_irqs->cnt, cpu));",
            "\treturn sum;",
            "}",
            "static unsigned int kstat_irqs(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tif (!desc || !desc->kstat_irqs)",
            "\t\treturn 0;",
            "\treturn kstat_irqs_desc(desc, cpu_possible_mask);",
            "}",
            "void kstat_snapshot_irqs(void)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tunsigned int irq;",
            "",
            "\tfor_each_irq_desc(irq, desc) {",
            "\t\tif (!desc->kstat_irqs)",
            "\t\t\tcontinue;",
            "\t\tthis_cpu_write(desc->kstat_irqs->ref, this_cpu_read(desc->kstat_irqs->cnt));",
            "\t}",
            "}",
            "unsigned int kstat_get_irq_since_snapshot(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tif (!desc || !desc->kstat_irqs)",
            "\t\treturn 0;",
            "\treturn this_cpu_read(desc->kstat_irqs->cnt) - this_cpu_read(desc->kstat_irqs->ref);",
            "}",
            "unsigned int kstat_irqs_usr(unsigned int irq)",
            "{",
            "\tunsigned int sum;",
            "",
            "\trcu_read_lock();",
            "\tsum = kstat_irqs(irq);",
            "\trcu_read_unlock();",
            "\treturn sum;",
            "}",
            "void __irq_set_lockdep_class(unsigned int irq, struct lock_class_key *lock_class,",
            "\t\t\t     struct lock_class_key *request_class)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tif (desc) {",
            "\t\tlockdep_set_class(&desc->lock, lock_class);",
            "\t\tlockdep_set_class(&desc->request_mutex, request_class);",
            "\t}",
            "}"
          ],
          "function_name": "irq_set_percpu_devid, irq_get_percpu_devid_partition, kstat_incr_irq_this_cpu, kstat_irqs_cpu, irq_is_nmi, kstat_irqs_desc, kstat_irqs, kstat_snapshot_irqs, kstat_get_irq_since_snapshot, kstat_irqs_usr, __irq_set_lockdep_class",
          "description": "irq_set_percpu_devid 设置中断为每个CPU专用模式。irq_get_percpu_devid_partition 获取中断的亲和性掩码。kstat_incr_irq_this_cpu 增加当前CPU的中断统计计数。kstat_irqs_cpu 查询指定CPU的中断次数。irq_is_nmi 判断中断是否为NMI。kstat_irqs_desc 计算指定CPU掩码下的中断总数。kstat_snapshot_irqs 快照中断统计数据。kstat_get_irq_since_snapshot 获取自快照后的中断次数。__irq_set_lockdep_class 设置中断描述符锁的锁跟踪类别。",
          "similarity": 0.47443920373916626
        }
      ]
    },
    {
      "source_file": "kernel/irq/matrix.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:03:08\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\matrix.c`\n\n---\n\n# `irq/matrix.c` 技术文档\n\n## 1. 文件概述\n\n`irq/matrix.c` 实现了一个通用的中断位图（IRQ matrix）管理机制，用于在多 CPU 系统中高效地分配和管理中断向量（或中断位）。该机制支持两类中断分配：\n\n- **普通分配（allocated）**：由设备驱动等动态申请的中断。\n- **托管分配（managed）**：由内核子系统（如 MSI/MSI-X）预先保留、按需激活的中断。\n\n该文件通过 per-CPU 的位图结构，结合全局状态跟踪，实现了跨 CPU 的中断资源分配、预留、释放和在线/离线管理，特别适用于中断向量数量有限（如 x86 的 256 个向量）的架构。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct cpumap`**：每个 CPU 的本地中断位图状态\n  - `available`：当前 CPU 可用的中断数量\n  - `allocated`：已分配的普通中断数量\n  - `managed` / `managed_allocated`：预留和已激活的托管中断数量\n  - `alloc_map[]`：记录已分配的普通中断位\n  - `managed_map[]`：记录预留的托管中断位\n  - `initialized` / `online`：CPU 初始化和在线状态\n\n- **`struct irq_matrix`**：全局中断矩阵控制结构\n  - `matrix_bits`：总位图大小（≤ `IRQ_MATRIX_BITS`）\n  - `alloc_start` / `alloc_end`：可分配范围\n  - `global_available`：全局可用中断总数\n  - `system_map[]`：系统保留位（如 APIC 自身使用的向量）\n  - `maps`：指向 per-CPU `cpumap` 的指针\n  - `scratch_map[]`：临时位图，用于分配时的合并计算\n\n### 主要函数\n\n| 函数 | 功能 |\n|------|------|\n| `irq_alloc_matrix()` | 分配并初始化一个 `irq_matrix` 结构 |\n| `irq_matrix_online()` / `irq_matrix_offline()` | 将本地 CPU 的中断矩阵置为在线/离线状态 |\n| `irq_matrix_assign_system()` | 在矩阵中保留系统级中断位（如 APIC 向量） |\n| `irq_matrix_reserve_managed()` | 在指定 CPU 掩码上为托管中断预留位 |\n| `irq_matrix_remove_managed()` | 移除托管中断的预留位 |\n| `irq_matrix_alloc_managed()` | 从预留的托管中断中分配一个实际使用的中断 |\n| `matrix_alloc_area()` | 内部辅助函数：在合并位图中查找连续空闲区域 |\n| `matrix_find_best_cpu()` / `matrix_find_best_cpu_managed()` | 选择最优 CPU（基于可用数或托管分配数最少） |\n\n## 3. 关键实现\n\n### 位图合并分配策略\n- 在分配中断时，`matrix_alloc_area()` 会临时合并三个位图：\n  1. 当前 CPU 的 `managed_map`（托管预留）\n  2. 全局 `system_map`（系统保留）\n  3. 当前 CPU 的 `alloc_map`（已分配）\n- 使用 `bitmap_find_next_zero_area()` 在合并后的位图中查找连续空闲区域，确保不会重复分配。\n\n### 托管中断（Managed IRQ）机制\n- **两阶段分配**：\n  1. **预留（reserve）**：调用 `irq_matrix_reserve_managed()` 在多个 CPU 上各预留一个位（不一定对齐）。\n  2. **激活（alloc）**：调用 `irq_matrix_alloc_managed()` 从预留位中选择一个未使用的位进行实际分配。\n- **动态 CPU 选择**：`matrix_find_best_cpu_managed()` 优先选择 `managed_allocated` 最少的 CPU，实现负载均衡。\n\n### 系统中断保留\n- `irq_matrix_assign_system()` 用于保留如 x86 的 `IRQ0_VECTOR`（时钟中断）等关键系统向量。\n- 通过 `BUG_ON()` 强制保证：系统中断只能在单 CPU 初始化阶段分配，防止运行时冲突。\n\n### 在线/离线管理\n- CPU 上线时，将其 `available` 计数加入 `global_available`。\n- CPU 离线时，从全局计数中减去，但保留其位图数据（支持重新上线）。\n\n### 跟踪与调试\n- 集成 `trace/events/irq_matrix.h`，提供分配、预留、系统保留等关键操作的 tracepoint，便于调试中断分配问题。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/bitmap.h>`：位图操作（`bitmap_set`, `bitmap_find_next_zero_area` 等）\n  - `<linux/percpu.h>`：Per-CPU 变量支持\n  - `<linux/cpu.h>`：CPU 在线/离线状态\n  - `<linux/irq.h>`：中断子系统基础定义\n  - `<trace/events/irq_matrix.h>`：自定义 tracepoint\n\n- **内核子系统**：\n  - **中断子系统**：作为底层分配器，被 `irqdomain`、MSI/MSI-X 驱动等使用。\n  - **x86 APIC 驱动**：典型使用者，用于管理 256 个中断向量的分配（如 `kernel/irq/vector.c`）。\n\n## 5. 使用场景\n\n- **x86 中断向量管理**：在 `CONFIG_X86_IO_APIC` 或 `CONFIG_X86_LOCAL_APIC` 下，用于分配 IRQ 向量（0-255），区分系统向量、普通设备中断和 MSI 中断。\n- **MSI/MSI-X 中断分配**：PCIe 设备的 MSI 中断通过托管机制预留和分配，确保每个设备在多个 CPU 上有可用向量。\n- **CPU 热插拔**：支持 CPU 动态上线/下线时的中断资源重新平衡。\n- **中断负载均衡**：通过 `matrix_find_best_cpu*` 函数，在多 CPU 间均匀分配中断，避免单 CPU 向量耗尽。",
      "similarity": 0.5418504476547241,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 251,
          "end_line": 365,
          "content": [
            "void irq_matrix_remove_managed(struct irq_matrix *m, const struct cpumask *msk)",
            "{",
            "\tunsigned int cpu;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "\t\tunsigned int bit, end = m->alloc_end;",
            "",
            "\t\tif (WARN_ON_ONCE(!cm->managed))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Get managed bit which are not allocated */",
            "\t\tbitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);",
            "",
            "\t\tbit = find_first_bit(m->scratch_map, end);",
            "\t\tif (WARN_ON_ONCE(bit >= end))",
            "\t\t\tcontinue;",
            "",
            "\t\tclear_bit(bit, cm->managed_map);",
            "",
            "\t\tcm->managed--;",
            "\t\tif (cm->online) {",
            "\t\t\tcm->available++;",
            "\t\t\tm->global_available++;",
            "\t\t}",
            "\t\ttrace_irq_matrix_remove_managed(bit, cpu, m, cm);",
            "\t}",
            "}",
            "int irq_matrix_alloc_managed(struct irq_matrix *m, const struct cpumask *msk,",
            "\t\t\t     unsigned int *mapped_cpu)",
            "{",
            "\tunsigned int bit, cpu, end;",
            "\tstruct cpumap *cm;",
            "",
            "\tif (cpumask_empty(msk))",
            "\t\treturn -EINVAL;",
            "",
            "\tcpu = matrix_find_best_cpu_managed(m, msk);",
            "\tif (cpu == UINT_MAX)",
            "\t\treturn -ENOSPC;",
            "",
            "\tcm = per_cpu_ptr(m->maps, cpu);",
            "\tend = m->alloc_end;",
            "\t/* Get managed bit which are not allocated */",
            "\tbitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);",
            "\tbit = find_first_bit(m->scratch_map, end);",
            "\tif (bit >= end)",
            "\t\treturn -ENOSPC;",
            "\tset_bit(bit, cm->alloc_map);",
            "\tcm->allocated++;",
            "\tcm->managed_allocated++;",
            "\tm->total_allocated++;",
            "\t*mapped_cpu = cpu;",
            "\ttrace_irq_matrix_alloc_managed(bit, cpu, m, cm);",
            "\treturn bit;",
            "}",
            "void irq_matrix_assign(struct irq_matrix *m, unsigned int bit)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tif (WARN_ON_ONCE(bit < m->alloc_start || bit >= m->alloc_end))",
            "\t\treturn;",
            "\tif (WARN_ON_ONCE(test_and_set_bit(bit, cm->alloc_map)))",
            "\t\treturn;",
            "\tcm->allocated++;",
            "\tm->total_allocated++;",
            "\tcm->available--;",
            "\tm->global_available--;",
            "\ttrace_irq_matrix_assign(bit, smp_processor_id(), m, cm);",
            "}",
            "void irq_matrix_reserve(struct irq_matrix *m)",
            "{",
            "\tif (m->global_reserved == m->global_available)",
            "\t\tpr_warn(\"Interrupt reservation exceeds available resources\\n\");",
            "",
            "\tm->global_reserved++;",
            "\ttrace_irq_matrix_reserve(m);",
            "}",
            "void irq_matrix_remove_reserved(struct irq_matrix *m)",
            "{",
            "\tm->global_reserved--;",
            "\ttrace_irq_matrix_remove_reserved(m);",
            "}",
            "int irq_matrix_alloc(struct irq_matrix *m, const struct cpumask *msk,",
            "\t\t     bool reserved, unsigned int *mapped_cpu)",
            "{",
            "\tunsigned int cpu, bit;",
            "\tstruct cpumap *cm;",
            "",
            "\t/*",
            "\t * Not required in theory, but matrix_find_best_cpu() uses",
            "\t * for_each_cpu() which ignores the cpumask on UP .",
            "\t */",
            "\tif (cpumask_empty(msk))",
            "\t\treturn -EINVAL;",
            "",
            "\tcpu = matrix_find_best_cpu(m, msk);",
            "\tif (cpu == UINT_MAX)",
            "\t\treturn -ENOSPC;",
            "",
            "\tcm = per_cpu_ptr(m->maps, cpu);",
            "\tbit = matrix_alloc_area(m, cm, 1, false);",
            "\tif (bit >= m->alloc_end)",
            "\t\treturn -ENOSPC;",
            "\tcm->allocated++;",
            "\tcm->available--;",
            "\tm->total_allocated++;",
            "\tm->global_available--;",
            "\tif (reserved)",
            "\t\tm->global_reserved--;",
            "\t*mapped_cpu = cpu;",
            "\ttrace_irq_matrix_alloc(bit, cpu, m, cm);",
            "\treturn bit;",
            "",
            "}"
          ],
          "function_name": "irq_matrix_remove_managed, irq_matrix_alloc_managed, irq_matrix_assign, irq_matrix_reserve, irq_matrix_remove_reserved, irq_matrix_alloc",
          "description": "实现中断位的分配/回收机制，包含保留中断位的管理、跨CPU的中断分配逻辑，以及根据预留状态进行资源分配的控制流程",
          "similarity": 0.5648923516273499
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 78,
          "end_line": 205,
          "content": [
            "void irq_matrix_online(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tBUG_ON(cm->online);",
            "",
            "\tif (!cm->initialized) {",
            "\t\tcm->available = m->alloc_size;",
            "\t\tcm->available -= cm->managed + m->systembits_inalloc;",
            "\t\tcm->initialized = true;",
            "\t}",
            "\tm->global_available += cm->available;",
            "\tcm->online = true;",
            "\tm->online_maps++;",
            "\ttrace_irq_matrix_online(m);",
            "}",
            "void irq_matrix_offline(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\t/* Update the global available size */",
            "\tm->global_available -= cm->available;",
            "\tcm->online = false;",
            "\tm->online_maps--;",
            "\ttrace_irq_matrix_offline(m);",
            "}",
            "static unsigned int matrix_alloc_area(struct irq_matrix *m, struct cpumap *cm,",
            "\t\t\t\t      unsigned int num, bool managed)",
            "{",
            "\tunsigned int area, start = m->alloc_start;",
            "\tunsigned int end = m->alloc_end;",
            "",
            "\tbitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);",
            "\tbitmap_or(m->scratch_map, m->scratch_map, cm->alloc_map, end);",
            "\tarea = bitmap_find_next_zero_area(m->scratch_map, end, start, num, 0);",
            "\tif (area >= end)",
            "\t\treturn area;",
            "\tif (managed)",
            "\t\tbitmap_set(cm->managed_map, area, num);",
            "\telse",
            "\t\tbitmap_set(cm->alloc_map, area, num);",
            "\treturn area;",
            "}",
            "static unsigned int matrix_find_best_cpu(struct irq_matrix *m,",
            "\t\t\t\t\tconst struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, best_cpu, maxavl = 0;",
            "\tstruct cpumap *cm;",
            "",
            "\tbest_cpu = UINT_MAX;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tcm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tif (!cm->online || cm->available <= maxavl)",
            "\t\t\tcontinue;",
            "",
            "\t\tbest_cpu = cpu;",
            "\t\tmaxavl = cm->available;",
            "\t}",
            "\treturn best_cpu;",
            "}",
            "static unsigned int matrix_find_best_cpu_managed(struct irq_matrix *m,",
            "\t\t\t\t\t\tconst struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, best_cpu, allocated = UINT_MAX;",
            "\tstruct cpumap *cm;",
            "",
            "\tbest_cpu = UINT_MAX;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tcm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tif (!cm->online || cm->managed_allocated > allocated)",
            "\t\t\tcontinue;",
            "",
            "\t\tbest_cpu = cpu;",
            "\t\tallocated = cm->managed_allocated;",
            "\t}",
            "\treturn best_cpu;",
            "}",
            "void irq_matrix_assign_system(struct irq_matrix *m, unsigned int bit,",
            "\t\t\t      bool replace)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tBUG_ON(bit > m->matrix_bits);",
            "\tBUG_ON(m->online_maps > 1 || (m->online_maps && !replace));",
            "",
            "\tset_bit(bit, m->system_map);",
            "\tif (replace) {",
            "\t\tBUG_ON(!test_and_clear_bit(bit, cm->alloc_map));",
            "\t\tcm->allocated--;",
            "\t\tm->total_allocated--;",
            "\t}",
            "\tif (bit >= m->alloc_start && bit < m->alloc_end)",
            "\t\tm->systembits_inalloc++;",
            "",
            "\ttrace_irq_matrix_assign_system(bit, m);",
            "}",
            "int irq_matrix_reserve_managed(struct irq_matrix *m, const struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, failed_cpu;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "\t\tunsigned int bit;",
            "",
            "\t\tbit = matrix_alloc_area(m, cm, 1, true);",
            "\t\tif (bit >= m->alloc_end)",
            "\t\t\tgoto cleanup;",
            "\t\tcm->managed++;",
            "\t\tif (cm->online) {",
            "\t\t\tcm->available--;",
            "\t\t\tm->global_available--;",
            "\t\t}",
            "\t\ttrace_irq_matrix_reserve_managed(bit, cpu, m, cm);",
            "\t}",
            "\treturn 0;",
            "cleanup:",
            "\tfailed_cpu = cpu;",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tif (cpu == failed_cpu)",
            "\t\t\tbreak;",
            "\t\tirq_matrix_remove_managed(m, cpumask_of(cpu));",
            "\t}",
            "\treturn -ENOSPC;",
            "}"
          ],
          "function_name": "irq_matrix_online, irq_matrix_offline, matrix_alloc_area, matrix_find_best_cpu, matrix_find_best_cpu_managed, irq_matrix_assign_system, irq_matrix_reserve_managed",
          "description": "实现CPU矩阵的上线/下线操作，通过bitmap操作实现中断位的分配策略，包含寻找最佳CPU的逻辑，支持系统位管理和保留区域的分配与追踪",
          "similarity": 0.5437948703765869
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 418,
          "end_line": 483,
          "content": [
            "void irq_matrix_free(struct irq_matrix *m, unsigned int cpu,",
            "\t\t     unsigned int bit, bool managed)",
            "{",
            "\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\tif (WARN_ON_ONCE(bit < m->alloc_start || bit >= m->alloc_end))",
            "\t\treturn;",
            "",
            "\tif (WARN_ON_ONCE(!test_and_clear_bit(bit, cm->alloc_map)))",
            "\t\treturn;",
            "",
            "\tcm->allocated--;",
            "\tif(managed)",
            "\t\tcm->managed_allocated--;",
            "",
            "\tif (cm->online)",
            "\t\tm->total_allocated--;",
            "",
            "\tif (!managed) {",
            "\t\tcm->available++;",
            "\t\tif (cm->online)",
            "\t\t\tm->global_available++;",
            "\t}",
            "\ttrace_irq_matrix_free(bit, cpu, m, cm);",
            "}",
            "unsigned int irq_matrix_available(struct irq_matrix *m, bool cpudown)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tif (!cpudown)",
            "\t\treturn m->global_available;",
            "\treturn m->global_available - cm->available;",
            "}",
            "unsigned int irq_matrix_reserved(struct irq_matrix *m)",
            "{",
            "\treturn m->global_reserved;",
            "}",
            "unsigned int irq_matrix_allocated(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\treturn cm->allocated - cm->managed_allocated;",
            "}",
            "void irq_matrix_debug_show(struct seq_file *sf, struct irq_matrix *m, int ind)",
            "{",
            "\tunsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);",
            "\tint cpu;",
            "",
            "\tseq_printf(sf, \"Online bitmaps:   %6u\\n\", m->online_maps);",
            "\tseq_printf(sf, \"Global available: %6u\\n\", m->global_available);",
            "\tseq_printf(sf, \"Global reserved:  %6u\\n\", m->global_reserved);",
            "\tseq_printf(sf, \"Total allocated:  %6u\\n\", m->total_allocated);",
            "\tseq_printf(sf, \"System: %u: %*pbl\\n\", nsys, m->matrix_bits,",
            "\t\t   m->system_map);",
            "\tseq_printf(sf, \"%*s| CPU | avl | man | mac | act | vectors\\n\", ind, \" \");",
            "\tcpus_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tseq_printf(sf, \"%*s %4d  %4u  %4u  %4u %4u  %*pbl\\n\", ind, \" \",",
            "\t\t\t   cpu, cm->available, cm->managed,",
            "\t\t\t   cm->managed_allocated, cm->allocated,",
            "\t\t\t   m->matrix_bits, cm->alloc_map);",
            "\t}",
            "\tcpus_read_unlock();",
            "}"
          ],
          "function_name": "irq_matrix_free, irq_matrix_available, irq_matrix_reserved, irq_matrix_allocated, irq_matrix_debug_show",
          "description": "提供中断资源的释放接口，实现全局和CPU级的资源使用统计查询，包含调试信息展示功能，通过位图操作维护系统中断位的使用状态",
          "similarity": 0.5354143381118774
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 1,
          "end_line": 77,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "// Copyright (C) 2017 Thomas Gleixner <tglx@linutronix.de>",
            "",
            "#include <linux/spinlock.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/bitmap.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpu.h>",
            "#include <linux/irq.h>",
            "",
            "#define IRQ_MATRIX_SIZE\t(BITS_TO_LONGS(IRQ_MATRIX_BITS))",
            "",
            "struct cpumap {",
            "\tunsigned int\t\tavailable;",
            "\tunsigned int\t\tallocated;",
            "\tunsigned int\t\tmanaged;",
            "\tunsigned int\t\tmanaged_allocated;",
            "\tbool\t\t\tinitialized;",
            "\tbool\t\t\tonline;",
            "\tunsigned long\t\talloc_map[IRQ_MATRIX_SIZE];",
            "\tunsigned long\t\tmanaged_map[IRQ_MATRIX_SIZE];",
            "};",
            "",
            "struct irq_matrix {",
            "\tunsigned int\t\tmatrix_bits;",
            "\tunsigned int\t\talloc_start;",
            "\tunsigned int\t\talloc_end;",
            "\tunsigned int\t\talloc_size;",
            "\tunsigned int\t\tglobal_available;",
            "\tunsigned int\t\tglobal_reserved;",
            "\tunsigned int\t\tsystembits_inalloc;",
            "\tunsigned int\t\ttotal_allocated;",
            "\tunsigned int\t\tonline_maps;",
            "\tstruct cpumap __percpu\t*maps;",
            "\tunsigned long\t\tscratch_map[IRQ_MATRIX_SIZE];",
            "\tunsigned long\t\tsystem_map[IRQ_MATRIX_SIZE];",
            "};",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/irq_matrix.h>",
            "",
            "/**",
            " * irq_alloc_matrix - Allocate a irq_matrix structure and initialize it",
            " * @matrix_bits:\tNumber of matrix bits must be <= IRQ_MATRIX_BITS",
            " * @alloc_start:\tFrom which bit the allocation search starts",
            " * @alloc_end:\t\tAt which bit the allocation search ends, i.e first",
            " *\t\t\tinvalid bit",
            " */",
            "__init struct irq_matrix *irq_alloc_matrix(unsigned int matrix_bits,",
            "\t\t\t\t\t   unsigned int alloc_start,",
            "\t\t\t\t\t   unsigned int alloc_end)",
            "{",
            "\tstruct irq_matrix *m;",
            "",
            "\tif (matrix_bits > IRQ_MATRIX_BITS)",
            "\t\treturn NULL;",
            "",
            "\tm = kzalloc(sizeof(*m), GFP_KERNEL);",
            "\tif (!m)",
            "\t\treturn NULL;",
            "",
            "\tm->matrix_bits = matrix_bits;",
            "\tm->alloc_start = alloc_start;",
            "\tm->alloc_end = alloc_end;",
            "\tm->alloc_size = alloc_end - alloc_start;",
            "\tm->maps = alloc_percpu(*m->maps);",
            "\tif (!m->maps) {",
            "\t\tkfree(m);",
            "\t\treturn NULL;",
            "\t}",
            "\treturn m;",
            "}",
            "",
            "/**",
            " * irq_matrix_online - Bring the local CPU matrix online",
            " * @m:\t\tMatrix pointer",
            " */"
          ],
          "function_name": null,
          "description": "定义irq_matrix结构体和相关辅助数据结构，提供irq_alloc_matrix函数用于初始化并分配irq_matrix实例，设置矩阵大小、起始结束位置等参数，并分配per-CPU的cpumap数组",
          "similarity": 0.4713030457496643
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5321747064590454,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 438,
          "end_line": 544,
          "content": [
            "static int param_set_first_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : j);",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "static int param_set_next_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : (j ?: 1));",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "unsigned long rcu_get_gp_seq(void)",
            "{",
            "\treturn READ_ONCE(rcu_state.gp_seq);",
            "}",
            "unsigned long rcu_exp_batches_completed(void)",
            "{",
            "\treturn rcu_state.expedited_sequence;",
            "}",
            "void rcutorture_get_gp_data(enum rcutorture_type test_type, int *flags,",
            "\t\t\t    unsigned long *gp_seq)",
            "{",
            "\tswitch (test_type) {",
            "\tcase RCU_FLAVOR:",
            "\t\t*flags = READ_ONCE(rcu_state.gp_flags);",
            "\t\t*gp_seq = rcu_seq_current(&rcu_state.gp_seq);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "}",
            "static void late_wakeup_func(struct irq_work *work)",
            "{",
            "}",
            "noinstr void rcu_irq_work_resched(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tif (IS_ENABLED(CONFIG_GENERIC_ENTRY) && !(current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tif (IS_ENABLED(CONFIG_KVM_XFER_TO_GUEST_WORK) && (current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tinstrumentation_begin();",
            "\tif (do_nocb_deferred_wakeup(rdp) && need_resched()) {",
            "\t\tirq_work_queue(this_cpu_ptr(&late_wakeup_work));",
            "\t}",
            "\tinstrumentation_end();",
            "}",
            "void rcu_irq_exit_check_preempt(void)",
            "{",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nesting() <= 0,",
            "\t\t\t \"RCU dynticks_nesting counter underflow/zero!\");",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nmi_nesting() !=",
            "\t\t\t DYNTICK_IRQ_NONIDLE,",
            "\t\t\t \"Bad RCU  dynticks_nmi_nesting counter\\n\");",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"RCU in extended quiescent state!\");",
            "}",
            "void __rcu_irq_enter_check_tick(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\t// If we're here from NMI there's nothing to do.",
            "\tif (in_nmi())",
            "\t\treturn;",
            "",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"Illegal rcu_irq_enter_check_tick() from extended quiescent state\");",
            "",
            "\tif (!tick_nohz_full_cpu(rdp->cpu) ||",
            "\t    !READ_ONCE(rdp->rcu_urgent_qs) ||",
            "\t    READ_ONCE(rdp->rcu_forced_tick)) {",
            "\t\t// RCU doesn't need nohz_full help from this CPU, or it is",
            "\t\t// already getting that help.",
            "\t\treturn;",
            "\t}",
            "",
            "\t// We get here only when not in an extended quiescent state and",
            "\t// from interrupts (as opposed to NMIs).  Therefore, (1) RCU is",
            "\t// already watching and (2) The fact that we are in an interrupt",
            "\t// handler and that the rcu_node lock is an irq-disabled lock",
            "\t// prevents self-deadlock.  So we can safely recheck under the lock.",
            "\t// Note that the nohz_full state currently cannot change.",
            "\traw_spin_lock_rcu_node(rdp->mynode);",
            "\tif (READ_ONCE(rdp->rcu_urgent_qs) && !rdp->rcu_forced_tick) {",
            "\t\t// A nohz_full CPU is in the kernel and RCU needs a",
            "\t\t// quiescent state.  Turn on the tick!",
            "\t\tWRITE_ONCE(rdp->rcu_forced_tick, true);",
            "\t\ttick_dep_set_cpu(rdp->cpu, TICK_DEP_BIT_RCU);",
            "\t}",
            "\traw_spin_unlock_rcu_node(rdp->mynode);",
            "}"
          ],
          "function_name": "param_set_first_fqs_jiffies, param_set_next_fqs_jiffies, rcu_get_gp_seq, rcu_exp_batches_completed, rcutorture_get_gp_data, late_wakeup_func, rcu_irq_work_resched, rcu_irq_exit_check_preempt, __rcu_irq_enter_check_tick",
          "description": "实现参数配置回调函数和中断上下文RCU工作重排逻辑，管理grace period序列号读取及nohz_full模式下的tick依赖关系。",
          "similarity": 0.5450439453125
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1840,
          "end_line": 1973,
          "content": [
            "static int __noreturn rcu_gp_kthread(void *unused)",
            "{",
            "\trcu_bind_gp_kthread();",
            "\tfor (;;) {",
            "",
            "\t\t/* Handle grace-period start. */",
            "\t\tfor (;;) {",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwait\"));",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_WAIT_GPS);",
            "\t\t\tswait_event_idle_exclusive(rcu_state.gp_wq,",
            "\t\t\t\t\t READ_ONCE(rcu_state.gp_flags) &",
            "\t\t\t\t\t RCU_GP_FLAG_INIT);",
            "\t\t\trcu_gp_torture_wait();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_DONE_GPS);",
            "\t\t\t/* Locking provides needed memory barrier. */",
            "\t\t\tif (rcu_gp_init())",
            "\t\t\t\tbreak;",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\t\t\tWARN_ON(signal_pending(current));",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwaitsig\"));",
            "\t\t}",
            "",
            "\t\t/* Handle quiescent-state forcing. */",
            "\t\trcu_gp_fqs_loop();",
            "",
            "\t\t/* Handle grace-period end. */",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANUP);",
            "\t\trcu_gp_cleanup();",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANED);",
            "\t}",
            "}",
            "static void rcu_report_qs_rsp(unsigned long flags)",
            "\t__releases(rcu_get_root()->lock)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rcu_get_root());",
            "\tWARN_ON_ONCE(!rcu_gp_in_progress());",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rcu_get_root(), flags);",
            "\trcu_gp_kthread_wake();",
            "}",
            "static void rcu_report_qs_rnp(unsigned long mask, struct rcu_node *rnp,",
            "\t\t\t      unsigned long gps, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long oldmask = 0;",
            "\tstruct rcu_node *rnp_c;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* Walk up the rcu_node hierarchy. */",
            "\tfor (;;) {",
            "\t\tif ((!(rnp->qsmask & mask) && mask) || rnp->gp_seq != gps) {",
            "",
            "\t\t\t/*",
            "\t\t\t * Our bit has already been cleared, or the",
            "\t\t\t * relevant grace period is already over, so done.",
            "\t\t\t */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */",
            "\t\tWARN_ON_ONCE(!rcu_is_leaf_node(rnp) &&",
            "\t\t\t     rcu_preempt_blocked_readers_cgp(rnp));",
            "\t\tWRITE_ONCE(rnp->qsmask, rnp->qsmask & ~mask);",
            "\t\ttrace_rcu_quiescent_state_report(rcu_state.name, rnp->gp_seq,",
            "\t\t\t\t\t\t mask, rnp->qsmask, rnp->level,",
            "\t\t\t\t\t\t rnp->grplo, rnp->grphi,",
            "\t\t\t\t\t\t !!rnp->gp_tasks);",
            "\t\tif (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {",
            "",
            "\t\t\t/* Other bits still set at this level, so done. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\trnp->completedqs = rnp->gp_seq;",
            "\t\tmask = rnp->grpmask;",
            "\t\tif (rnp->parent == NULL) {",
            "",
            "\t\t\t/* No more levels.  Exit loop holding root lock. */",
            "",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\trnp_c = rnp;",
            "\t\trnp = rnp->parent;",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\toldmask = READ_ONCE(rnp_c->qsmask);",
            "\t}",
            "",
            "\t/*",
            "\t * Get here if we are the last CPU to pass through a quiescent",
            "\t * state for this grace period.  Invoke rcu_report_qs_rsp()",
            "\t * to clean up and start the next grace period if one is needed.",
            "\t */",
            "\trcu_report_qs_rsp(flags); /* releases rnp->lock. */",
            "}",
            "static void __maybe_unused",
            "rcu_report_unblock_qs_rnp(struct rcu_node *rnp, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long gps;",
            "\tunsigned long mask;",
            "\tstruct rcu_node *rnp_p;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (WARN_ON_ONCE(!IS_ENABLED(CONFIG_PREEMPT_RCU)) ||",
            "\t    WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)) ||",
            "\t    rnp->qsmask != 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;  /* Still need more quiescent states! */",
            "\t}",
            "",
            "\trnp->completedqs = rnp->gp_seq;",
            "\trnp_p = rnp->parent;",
            "\tif (rnp_p == NULL) {",
            "\t\t/*",
            "\t\t * Only one rcu_node structure in the tree, so don't",
            "\t\t * try to report up to its nonexistent parent!",
            "\t\t */",
            "\t\trcu_report_qs_rsp(flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Report up the rest of the hierarchy, tracking current ->gp_seq. */",
            "\tgps = rnp->gp_seq;",
            "\tmask = rnp->grpmask;",
            "\traw_spin_unlock_rcu_node(rnp);\t/* irqs remain disabled. */",
            "\traw_spin_lock_rcu_node(rnp_p);\t/* irqs already disabled. */",
            "\trcu_report_qs_rnp(mask, rnp_p, gps, flags);",
            "}"
          ],
          "function_name": "rcu_gp_kthread, rcu_report_qs_rsp, rcu_report_qs_rnp, rcu_report_unblock_qs_rnp",
          "description": "实现RCU grace period的主线程循环，处理grace period启动、强制quiescent状态报告及结束逻辑，通过锁和状态标志协调各子系统同步",
          "similarity": 0.5171455144882202
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2273,
          "end_line": 2388,
          "content": [
            "void rcu_sched_clock_irq(int user)",
            "{",
            "\tunsigned long j;",
            "",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU)) {",
            "\t\tj = jiffies;",
            "\t\tWARN_ON_ONCE(time_before(j, __this_cpu_read(rcu_data.last_sched_clock)));",
            "\t\t__this_cpu_write(rcu_data.last_sched_clock, j);",
            "\t}",
            "\ttrace_rcu_utilization(TPS(\"Start scheduler-tick\"));",
            "\tlockdep_assert_irqs_disabled();",
            "\traw_cpu_inc(rcu_data.ticks_this_gp);",
            "\t/* The load-acquire pairs with the store-release setting to true. */",
            "\tif (smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs))) {",
            "\t\t/* Idle and userspace execution already are quiescent states. */",
            "\t\tif (!rcu_is_cpu_rrupt_from_idle() && !user) {",
            "\t\t\tset_tsk_need_resched(current);",
            "\t\t\tset_preempt_need_resched();",
            "\t\t}",
            "\t\t__this_cpu_write(rcu_data.rcu_urgent_qs, false);",
            "\t}",
            "\trcu_flavor_sched_clock_irq(user);",
            "\tif (rcu_pending(user))",
            "\t\tinvoke_rcu_core();",
            "\tif (user || rcu_is_cpu_rrupt_from_idle())",
            "\t\trcu_note_voluntary_context_switch(current);",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\ttrace_rcu_utilization(TPS(\"End scheduler-tick\"));",
            "}",
            "static void force_qs_rnp(int (*f)(struct rcu_data *rdp))",
            "{",
            "\tint cpu;",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trcu_state.cbovld = rcu_state.cbovldnext;",
            "\trcu_state.cbovldnext = false;",
            "\trcu_for_each_leaf_node(rnp) {",
            "\t\tunsigned long mask = 0;",
            "\t\tunsigned long rsmask = 0;",
            "",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\trcu_state.cbovldnext |= !!rnp->cbovldmask;",
            "\t\tif (rnp->qsmask == 0) {",
            "\t\t\tif (rcu_preempt_blocked_readers_cgp(rnp)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No point in scanning bits because they",
            "\t\t\t\t * are all zero.  But we might need to",
            "\t\t\t\t * priority-boost blocked readers.",
            "\t\t\t\t */",
            "\t\t\t\trcu_initiate_boost(rnp, flags);",
            "\t\t\t\t/* rcu_initiate_boost() releases rnp->lock */",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rnp->qsmask) {",
            "\t\t\tstruct rcu_data *rdp;",
            "\t\t\tint ret;",
            "",
            "\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\t\t\tret = f(rdp);",
            "\t\t\tif (ret > 0) {",
            "\t\t\t\tmask |= rdp->grpmask;",
            "\t\t\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\t\t}",
            "\t\t\tif (ret < 0)",
            "\t\t\t\trsmask |= rdp->grpmask;",
            "\t\t}",
            "\t\tif (mask != 0) {",
            "\t\t\t/* Idle/offline CPUs, report (releases rnp->lock). */",
            "\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\t} else {",
            "\t\t\t/* Nothing to do here, so just drop the lock. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t}",
            "",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rsmask)",
            "\t\t\tresched_cpu(cpu);",
            "\t}",
            "}",
            "void rcu_force_quiescent_state(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool ret;",
            "\tstruct rcu_node *rnp;",
            "\tstruct rcu_node *rnp_old = NULL;",
            "",
            "\t/* Funnel through hierarchy to reduce memory contention. */",
            "\trnp = raw_cpu_read(rcu_data.mynode);",
            "\tfor (; rnp != NULL; rnp = rnp->parent) {",
            "\t\tret = (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) ||",
            "\t\t       !raw_spin_trylock(&rnp->fqslock);",
            "\t\tif (rnp_old != NULL)",
            "\t\t\traw_spin_unlock(&rnp_old->fqslock);",
            "\t\tif (ret)",
            "\t\t\treturn;",
            "\t\trnp_old = rnp;",
            "\t}",
            "\t/* rnp_old == rcu_get_root(), rnp == NULL. */",
            "",
            "\t/* Reached the root of the rcu_node tree, acquire lock. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp_old, flags);",
            "\traw_spin_unlock(&rnp_old->fqslock);",
            "\tif (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\t\treturn;  /* Someone beat us to it. */",
            "\t}",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\trcu_gp_kthread_wake();",
            "}"
          ],
          "function_name": "rcu_sched_clock_irq, force_qs_rnp, rcu_force_quiescent_state",
          "description": "实现强制quiescent状态触发与调度器中断处理，包含优先级提升、回调加速及grace period推进等关键控制流，维护RCU状态一致性",
          "similarity": 0.5162690877914429
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2523,
          "end_line": 2628,
          "content": [
            "static void rcu_cpu_kthread_park(unsigned int cpu)",
            "{",
            "\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;",
            "}",
            "static int rcu_cpu_kthread_should_run(unsigned int cpu)",
            "{",
            "\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);",
            "}",
            "static void rcu_cpu_kthread(unsigned int cpu)",
            "{",
            "\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);",
            "\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);",
            "\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);",
            "\tint spincnt;",
            "",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));",
            "\tfor (spincnt = 0; spincnt < 10; spincnt++) {",
            "\t\tWRITE_ONCE(*j, jiffies);",
            "\t\tlocal_bh_disable();",
            "\t\t*statusp = RCU_KTHREAD_RUNNING;",
            "\t\tlocal_irq_disable();",
            "\t\twork = *workp;",
            "\t\tWRITE_ONCE(*workp, 0);",
            "\t\tlocal_irq_enable();",
            "\t\tif (work)",
            "\t\t\trcu_core();",
            "\t\tlocal_bh_enable();",
            "\t\tif (!READ_ONCE(*workp)) {",
            "\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));",
            "\t\t\t*statusp = RCU_KTHREAD_WAITING;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "\t*statusp = RCU_KTHREAD_YIELDING;",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));",
            "\tschedule_timeout_idle(2);",
            "\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));",
            "\t*statusp = RCU_KTHREAD_WAITING;",
            "\tWRITE_ONCE(*j, jiffies);",
            "}",
            "static int __init rcu_spawn_core_kthreads(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;",
            "\tif (use_softirq)",
            "\t\treturn 0;",
            "\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),",
            "\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);",
            "\treturn 0;",
            "}",
            "static void rcutree_enqueue(struct rcu_data *rdp, struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\trcu_segcblist_enqueue(&rdp->cblist, head);",
            "\tif (__is_kvfree_rcu_offset((unsigned long)func))",
            "\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,",
            "\t\t\t\t\t (unsigned long)func,",
            "\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));",
            "\telse",
            "\t\ttrace_rcu_callback(rcu_state.name, head,",
            "\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));",
            "}",
            "static void call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,",
            "\t\t\t  rcu_callback_t func, unsigned long flags)",
            "{",
            "\trcutree_enqueue(rdp, head, func);",
            "\t/*",
            "\t * If called from an extended quiescent state, invoke the RCU",
            "\t * core in order to force a re-evaluation of RCU's idleness.",
            "\t */",
            "\tif (!rcu_is_watching())",
            "\t\tinvoke_rcu_core();",
            "",
            "\t/* If interrupts were disabled or CPU offline, don't invoke RCU core. */",
            "\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Force the grace period if too many callbacks or too long waiting.",
            "\t * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()",
            "\t * if some other CPU has recently done so.  Also, don't bother",
            "\t * invoking rcu_force_quiescent_state() if the newly enqueued callback",
            "\t * is the only one waiting for a grace period to complete.",
            "\t */",
            "\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >",
            "\t\t     rdp->qlen_last_fqs_check + qhimark)) {",
            "",
            "\t\t/* Are we ignoring a completed grace period? */",
            "\t\tnote_gp_changes(rdp);",
            "",
            "\t\t/* Start a new grace period if one not already started. */",
            "\t\tif (!rcu_gp_in_progress()) {",
            "\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);",
            "\t\t} else {",
            "\t\t\t/* Give the grace period a kick. */",
            "\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;",
            "\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&",
            "\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)",
            "\t\t\t\trcu_force_quiescent_state();",
            "\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "rcu_cpu_kthread_park, rcu_cpu_kthread_should_run, rcu_cpu_kthread, rcu_spawn_core_kthreads, rcutree_enqueue, call_rcu_core",
          "description": "实现RCU k线程管理与回调分发基础设施，包含线程启动、回调入队及触发条件判断逻辑，提供跨CPU的异步处理能力",
          "similarity": 0.5154701471328735
        },
        {
          "chunk_id": 19,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3436,
          "end_line": 3574,
          "content": [
            "void kvfree_call_rcu(struct rcu_head *head, void *ptr)",
            "{",
            "\tunsigned long flags;",
            "\tstruct kfree_rcu_cpu *krcp;",
            "\tbool success;",
            "",
            "\t/*",
            "\t * Please note there is a limitation for the head-less",
            "\t * variant, that is why there is a clear rule for such",
            "\t * objects: it can be used from might_sleep() context",
            "\t * only. For other places please embed an rcu_head to",
            "\t * your data.",
            "\t */",
            "\tif (!head)",
            "\t\tmight_sleep();",
            "",
            "\t// Queue the object but don't yet schedule the batch.",
            "\tif (debug_rcu_head_queue(ptr)) {",
            "\t\t// Probable double kfree_rcu(), just leak.",
            "\t\tWARN_ONCE(1, \"%s(): Double-freed call. rcu_head %p\\n\",",
            "\t\t\t  __func__, head);",
            "",
            "\t\t// Mark as success and leave.",
            "\t\treturn;",
            "\t}",
            "",
            "\tkasan_record_aux_stack_noalloc(ptr);",
            "\tsuccess = add_ptr_to_bulk_krc_lock(&krcp, &flags, ptr, !head);",
            "\tif (!success) {",
            "\t\trun_page_cache_worker(krcp);",
            "",
            "\t\tif (head == NULL)",
            "\t\t\t// Inline if kvfree_rcu(one_arg) call.",
            "\t\t\tgoto unlock_return;",
            "",
            "\t\thead->func = ptr;",
            "\t\thead->next = krcp->head;",
            "\t\tWRITE_ONCE(krcp->head, head);",
            "\t\tatomic_inc(&krcp->head_count);",
            "",
            "\t\t// Take a snapshot for this krcp.",
            "\t\tkrcp->head_gp_snap = get_state_synchronize_rcu();",
            "\t\tsuccess = true;",
            "\t}",
            "",
            "\t/*",
            "\t * The kvfree_rcu() caller considers the pointer freed at this point",
            "\t * and likely removes any references to it. Since the actual slab",
            "\t * freeing (and kmemleak_free()) is deferred, tell kmemleak to ignore",
            "\t * this object (no scanning or false positives reporting).",
            "\t */",
            "\tkmemleak_ignore(ptr);",
            "",
            "\t// Set timer to drain after KFREE_DRAIN_JIFFIES.",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_RUNNING)",
            "\t\t__schedule_delayed_monitor_work(krcp);",
            "",
            "unlock_return:",
            "\tkrc_this_cpu_unlock(krcp, flags);",
            "",
            "\t/*",
            "\t * Inline kvfree() after synchronize_rcu(). We can do",
            "\t * it from might_sleep() context only, so the current",
            "\t * CPU can pass the QS state.",
            "\t */",
            "\tif (!success) {",
            "\t\tdebug_rcu_head_unqueue((struct rcu_head *) ptr);",
            "\t\tsynchronize_rcu();",
            "\t\tkvfree(ptr);",
            "\t}",
            "}",
            "void kvfree_rcu_barrier(void)",
            "{",
            "\tstruct kfree_rcu_cpu_work *krwp;",
            "\tstruct kfree_rcu_cpu *krcp;",
            "\tbool queued;",
            "\tint i, cpu;",
            "",
            "\t/*",
            "\t * Firstly we detach objects and queue them over an RCU-batch",
            "\t * for all CPUs. Finally queued works are flushed for each CPU.",
            "\t *",
            "\t * Please note. If there are outstanding batches for a particular",
            "\t * CPU, those have to be finished first following by queuing a new.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tkrcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\t/*",
            "\t\t * Check if this CPU has any objects which have been queued for a",
            "\t\t * new GP completion. If not(means nothing to detach), we are done",
            "\t\t * with it. If any batch is pending/running for this \"krcp\", below",
            "\t\t * per-cpu flush_rcu_work() waits its completion(see last step).",
            "\t\t */",
            "\t\tif (!need_offload_krc(krcp))",
            "\t\t\tcontinue;",
            "",
            "\t\twhile (1) {",
            "\t\t\t/*",
            "\t\t\t * If we are not able to queue a new RCU work it means:",
            "\t\t\t * - batches for this CPU are still in flight which should",
            "\t\t\t *   be flushed first and then repeat;",
            "\t\t\t * - no objects to detach, because of concurrency.",
            "\t\t\t */",
            "\t\t\tqueued = kvfree_rcu_queue_batch(krcp);",
            "",
            "\t\t\t/*",
            "\t\t\t * Bail out, if there is no need to offload this \"krcp\"",
            "\t\t\t * anymore. As noted earlier it can run concurrently.",
            "\t\t\t */",
            "\t\t\tif (queued || !need_offload_krc(krcp))",
            "\t\t\t\tbreak;",
            "",
            "\t\t\t/* There are ongoing batches. */",
            "\t\t\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\t\t\tkrwp = &(krcp->krw_arr[i]);",
            "\t\t\t\tflush_rcu_work(&krwp->rcu_work);",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Now we guarantee that all objects are flushed.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tkrcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\t/*",
            "\t\t * A monitor work can drain ready to reclaim objects",
            "\t\t * directly. Wait its completion if running or pending.",
            "\t\t */",
            "\t\tcancel_delayed_work_sync(&krcp->monitor_work);",
            "",
            "\t\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\t\tkrwp = &(krcp->krw_arr[i]);",
            "\t\t\tflush_rcu_work(&krwp->rcu_work);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "kvfree_call_rcu, kvfree_rcu_barrier",
          "description": "实现基于RCU的延迟内存释放接口，提供安全释放路径并保证内存屏障语义，强制同步清理所有挂起的释放请求。",
          "similarity": 0.5114330649375916
        }
      ]
    }
  ]
}