{
  "query": "混合内核设计目标与挑战",
  "timestamp": "2025-12-26 01:56:30",
  "retrieved_files": [
    {
      "source_file": "mm/ksm.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:34:25\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `ksm.c`\n\n---\n\n# ksm.c 技术文档\n\n## 1. 文件概述\n\n`ksm.c` 实现了内核同页合并（Kernel Samepage Merging, KSM）功能，该机制能够动态识别并合并内容完全相同的物理内存页，即使这些页属于不同的进程地址空间且未通过 `fork()` 共享。KSM 通过后台扫描线程周期性地遍历注册的内存区域，利用红黑树（rbtree）结构高效地比对页面内容，将重复页替换为只读的共享页，从而显著减少物理内存占用。此功能特别适用于虚拟化环境中多个相似虚拟机共存的场景。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct ksm_mm_slot`**  \n  表示一个被 KSM 扫描的内存描述符（`mm_struct`）的元数据，包含哈希槽位和反向映射项链表头。\n\n- **`struct ksm_scan`**  \n  全局扫描游标，记录当前扫描进度（包括当前 `mm_slot`、虚拟地址、反向映射项指针及完整扫描轮次计数）。\n\n- **`struct ksm_stable_node`**  \n  稳定红黑树中的节点，代表一个已合并的 KSM 页面。包含：\n  - 红黑树节点或迁移链表指针（联合体复用）\n  - 指向使用该 KSM 页的所有 `rmap_item` 的哈希链表头\n  - 物理页帧号（`kpfn`）或链修剪时间戳\n  - 反向映射项数量（支持链式扩展）\n  - NUMA 节点 ID（若启用 NUMA）\n\n- **`struct ksm_rmap_item`**  \n  反向映射项，跟踪一个虚拟地址到其物理页的映射关系。包含：\n  - 所属 `mm_struct` 和虚拟地址（低比特位用于标志）\n  - 匿名 VMA 指针（稳定状态）或 NUMA 节点 ID（不稳定状态）\n  - 校验和（不稳定状态）\n  - 红黑树节点（不稳定树）或指向 `stable_node` 的指针及哈希链表节点（稳定状态）\n\n### 关键宏定义\n\n- **`STABLE_NODE_CHAIN`**  \n  标识稳定节点为“链”类型（值为 -1024），用于高效管理大量相同内容的 KSM 页副本。\n  \n- **标志位**  \n  - `UNSTABLE_FLAG` (0x100)：标识 `rmap_item` 属于不稳定树\n  - `STABLE_FLAG` (0x200)：标识 `rmap_item` 已链接到稳定树\n  - `SEQNR_MASK` (0x0ff)：用于存储不稳定树序列号的低 8 位\n\n## 3. 关键实现\n\n### 双树架构设计\nKSM 采用**稳定树（stable tree）**与**不稳定树（unstable tree）**协同工作的机制：\n- **稳定树**：存储已确认可合并的只读 KSM 页，因写保护而内容恒定，支持高效精确匹配。\n- **不稳定树**：临时缓存近期未修改的普通页，因内容可能变化而需周期性重建。\n\n### 扫描与合并流程\n1. **增量扫描**：全局游标 `ksm_scan` 遍历所有注册的 `mm_slot` 及其内存区域。\n2. **校验和预筛**：计算页面内容的 `xxhash` 校验和，仅当与上次扫描一致时才尝试插入不稳定树。\n3. **双阶段匹配**：\n   - 优先在**稳定树**中查找完全匹配的 KSM 页\n   - 若未命中，则在**不稳定树**中查找潜在重复页\n4. **树维护策略**：\n   - 不稳定树在每轮全量扫描结束后**完全清空重建**\n   - 稳定树**持久保留**，通过反向映射（rmap）和链式节点优化大规模合并场景\n5. **NUMA 感知**：若 `merge_across_nodes=0`，则为每个 NUMA 节点维护独立的稳定/不稳定树，避免跨节点内存访问开销。\n\n### 内存安全机制\n- **写保护**：合并后的 KSM 页设为只读，任何写操作触发 COW（写时复制）并解除合并。\n- **RMAP 集成**：通过 `anon_vma` 和反向映射链表，在页解绑时高效更新所有相关虚拟地址。\n- **OOM 防护**：在内存压力下可释放 KSM 页以缓解系统压力。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：深度依赖 `mm.h`、`rmap.h`、`pagemap.h` 实现页表操作、反向映射和页生命周期管理。\n- **调度与进程管理**：通过 `sched/mm.h` 获取进程内存上下文，利用 `kthread.h` 创建后台扫描线程。\n- **NUMA 支持**：条件编译依赖 `CONFIG_NUMA`，使用 `numa.h` 实现节点亲和性。\n- **调试与追踪**：集成 `trace/events/ksm.h` 提供运行时事件追踪能力。\n- **哈希算法**：使用 `xxhash.h` 提供高效的内容指纹计算。\n- **内部辅助模块**：依赖 `mm_slot.h` 管理内存描述符槽位，`internal.h` 提供内核 MM 内部接口。\n\n## 5. 使用场景\n\n- **虚拟化环境**：在 KVM/Xen 等 Hypervisor 中合并多个相似虚拟机的内存页（如相同操作系统镜像）。\n- **内存密集型应用**：合并大型应用（如数据库、Web 服务器）中重复的静态数据或零页。\n- **容器化平台**：在 Docker/LXC 等容器运行时中减少同镜像容器的内存占用。\n- **内存超分场景**：在物理内存有限但允许超额分配的系统中提升内存利用率。\n- **开发调试**：通过 `/sys/kernel/mm/ksm/` 接口动态控制扫描速率、合并阈值等参数。",
      "similarity": 0.53304523229599,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/ksm.c",
          "start_line": 1,
          "end_line": 311,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Memory merging support.",
            " *",
            " * This code enables dynamic sharing of identical pages found in different",
            " * memory areas, even if they are not shared by fork()",
            " *",
            " * Copyright (C) 2008-2009 Red Hat, Inc.",
            " * Authors:",
            " *\tIzik Eidus",
            " *\tAndrea Arcangeli",
            " *\tChris Wright",
            " *\tHugh Dickins",
            " */",
            "",
            "#include <linux/errno.h>",
            "#include <linux/mm.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/fs.h>",
            "#include <linux/mman.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/rmap.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/xxhash.h>",
            "#include <linux/delay.h>",
            "#include <linux/kthread.h>",
            "#include <linux/wait.h>",
            "#include <linux/slab.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/memory.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/swap.h>",
            "#include <linux/ksm.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/freezer.h>",
            "#include <linux/oom.h>",
            "#include <linux/numa.h>",
            "#include <linux/pagewalk.h>",
            "",
            "#include <asm/tlbflush.h>",
            "#include \"internal.h\"",
            "#include \"mm_slot.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/ksm.h>",
            "",
            "#ifdef CONFIG_NUMA",
            "#define NUMA(x)\t\t(x)",
            "#define DO_NUMA(x)\tdo { (x); } while (0)",
            "#else",
            "#define NUMA(x)\t\t(0)",
            "#define DO_NUMA(x)\tdo { } while (0)",
            "#endif",
            "",
            "/**",
            " * DOC: Overview",
            " *",
            " * A few notes about the KSM scanning process,",
            " * to make it easier to understand the data structures below:",
            " *",
            " * In order to reduce excessive scanning, KSM sorts the memory pages by their",
            " * contents into a data structure that holds pointers to the pages' locations.",
            " *",
            " * Since the contents of the pages may change at any moment, KSM cannot just",
            " * insert the pages into a normal sorted tree and expect it to find anything.",
            " * Therefore KSM uses two data structures - the stable and the unstable tree.",
            " *",
            " * The stable tree holds pointers to all the merged pages (ksm pages), sorted",
            " * by their contents.  Because each such page is write-protected, searching on",
            " * this tree is fully assured to be working (except when pages are unmapped),",
            " * and therefore this tree is called the stable tree.",
            " *",
            " * The stable tree node includes information required for reverse",
            " * mapping from a KSM page to virtual addresses that map this page.",
            " *",
            " * In order to avoid large latencies of the rmap walks on KSM pages,",
            " * KSM maintains two types of nodes in the stable tree:",
            " *",
            " * * the regular nodes that keep the reverse mapping structures in a",
            " *   linked list",
            " * * the \"chains\" that link nodes (\"dups\") that represent the same",
            " *   write protected memory content, but each \"dup\" corresponds to a",
            " *   different KSM page copy of that content",
            " *",
            " * Internally, the regular nodes, \"dups\" and \"chains\" are represented",
            " * using the same struct ksm_stable_node structure.",
            " *",
            " * In addition to the stable tree, KSM uses a second data structure called the",
            " * unstable tree: this tree holds pointers to pages which have been found to",
            " * be \"unchanged for a period of time\".  The unstable tree sorts these pages",
            " * by their contents, but since they are not write-protected, KSM cannot rely",
            " * upon the unstable tree to work correctly - the unstable tree is liable to",
            " * be corrupted as its contents are modified, and so it is called unstable.",
            " *",
            " * KSM solves this problem by several techniques:",
            " *",
            " * 1) The unstable tree is flushed every time KSM completes scanning all",
            " *    memory areas, and then the tree is rebuilt again from the beginning.",
            " * 2) KSM will only insert into the unstable tree, pages whose hash value",
            " *    has not changed since the previous scan of all memory areas.",
            " * 3) The unstable tree is a RedBlack Tree - so its balancing is based on the",
            " *    colors of the nodes and not on their contents, assuring that even when",
            " *    the tree gets \"corrupted\" it won't get out of balance, so scanning time",
            " *    remains the same (also, searching and inserting nodes in an rbtree uses",
            " *    the same algorithm, so we have no overhead when we flush and rebuild).",
            " * 4) KSM never flushes the stable tree, which means that even if it were to",
            " *    take 10 attempts to find a page in the unstable tree, once it is found,",
            " *    it is secured in the stable tree.  (When we scan a new page, we first",
            " *    compare it against the stable tree, and then against the unstable tree.)",
            " *",
            " * If the merge_across_nodes tunable is unset, then KSM maintains multiple",
            " * stable trees and multiple unstable trees: one of each for each NUMA node.",
            " */",
            "",
            "/**",
            " * struct ksm_mm_slot - ksm information per mm that is being scanned",
            " * @slot: hash lookup from mm to mm_slot",
            " * @rmap_list: head for this mm_slot's singly-linked list of rmap_items",
            " */",
            "struct ksm_mm_slot {",
            "\tstruct mm_slot slot;",
            "\tstruct ksm_rmap_item *rmap_list;",
            "};",
            "",
            "/**",
            " * struct ksm_scan - cursor for scanning",
            " * @mm_slot: the current mm_slot we are scanning",
            " * @address: the next address inside that to be scanned",
            " * @rmap_list: link to the next rmap to be scanned in the rmap_list",
            " * @seqnr: count of completed full scans (needed when removing unstable node)",
            " *",
            " * There is only the one ksm_scan instance of this cursor structure.",
            " */",
            "struct ksm_scan {",
            "\tstruct ksm_mm_slot *mm_slot;",
            "\tunsigned long address;",
            "\tstruct ksm_rmap_item **rmap_list;",
            "\tunsigned long seqnr;",
            "};",
            "",
            "/**",
            " * struct ksm_stable_node - node of the stable rbtree",
            " * @node: rb node of this ksm page in the stable tree",
            " * @head: (overlaying parent) &migrate_nodes indicates temporarily on that list",
            " * @hlist_dup: linked into the stable_node->hlist with a stable_node chain",
            " * @list: linked into migrate_nodes, pending placement in the proper node tree",
            " * @hlist: hlist head of rmap_items using this ksm page",
            " * @kpfn: page frame number of this ksm page (perhaps temporarily on wrong nid)",
            " * @chain_prune_time: time of the last full garbage collection",
            " * @rmap_hlist_len: number of rmap_item entries in hlist or STABLE_NODE_CHAIN",
            " * @nid: NUMA node id of stable tree in which linked (may not match kpfn)",
            " */",
            "struct ksm_stable_node {",
            "\tunion {",
            "\t\tstruct rb_node node;\t/* when node of stable tree */",
            "\t\tstruct {\t\t/* when listed for migration */",
            "\t\t\tstruct list_head *head;",
            "\t\t\tstruct {",
            "\t\t\t\tstruct hlist_node hlist_dup;",
            "\t\t\t\tstruct list_head list;",
            "\t\t\t};",
            "\t\t};",
            "\t};",
            "\tstruct hlist_head hlist;",
            "\tunion {",
            "\t\tunsigned long kpfn;",
            "\t\tunsigned long chain_prune_time;",
            "\t};",
            "\t/*",
            "\t * STABLE_NODE_CHAIN can be any negative number in",
            "\t * rmap_hlist_len negative range, but better not -1 to be able",
            "\t * to reliably detect underflows.",
            "\t */",
            "#define STABLE_NODE_CHAIN -1024",
            "\tint rmap_hlist_len;",
            "#ifdef CONFIG_NUMA",
            "\tint nid;",
            "#endif",
            "};",
            "",
            "/**",
            " * struct ksm_rmap_item - reverse mapping item for virtual addresses",
            " * @rmap_list: next rmap_item in mm_slot's singly-linked rmap_list",
            " * @anon_vma: pointer to anon_vma for this mm,address, when in stable tree",
            " * @nid: NUMA node id of unstable tree in which linked (may not match page)",
            " * @mm: the memory structure this rmap_item is pointing into",
            " * @address: the virtual address this rmap_item tracks (+ flags in low bits)",
            " * @oldchecksum: previous checksum of the page at that virtual address",
            " * @node: rb node of this rmap_item in the unstable tree",
            " * @head: pointer to stable_node heading this list in the stable tree",
            " * @hlist: link into hlist of rmap_items hanging off that stable_node",
            " */",
            "struct ksm_rmap_item {",
            "\tstruct ksm_rmap_item *rmap_list;",
            "\tunion {",
            "\t\tstruct anon_vma *anon_vma;\t/* when stable */",
            "#ifdef CONFIG_NUMA",
            "\t\tint nid;\t\t/* when node of unstable tree */",
            "#endif",
            "\t};",
            "\tstruct mm_struct *mm;",
            "\tunsigned long address;\t\t/* + low bits used for flags below */",
            "\tunsigned int oldchecksum;\t/* when unstable */",
            "\tunion {",
            "\t\tstruct rb_node node;\t/* when node of unstable tree */",
            "\t\tstruct {\t\t/* when listed from stable tree */",
            "\t\t\tstruct ksm_stable_node *head;",
            "\t\t\tstruct hlist_node hlist;",
            "\t\t};",
            "\t};",
            "};",
            "",
            "#define SEQNR_MASK\t0x0ff\t/* low bits of unstable tree seqnr */",
            "#define UNSTABLE_FLAG\t0x100\t/* is a node of the unstable tree */",
            "#define STABLE_FLAG\t0x200\t/* is listed from the stable tree */",
            "",
            "/* The stable and unstable tree heads */",
            "static struct rb_root one_stable_tree[1] = { RB_ROOT };",
            "static struct rb_root one_unstable_tree[1] = { RB_ROOT };",
            "static struct rb_root *root_stable_tree = one_stable_tree;",
            "static struct rb_root *root_unstable_tree = one_unstable_tree;",
            "",
            "/* Recently migrated nodes of stable tree, pending proper placement */",
            "static LIST_HEAD(migrate_nodes);",
            "#define STABLE_NODE_DUP_HEAD ((struct list_head *)&migrate_nodes.prev)",
            "",
            "#define MM_SLOTS_HASH_BITS 10",
            "static DEFINE_HASHTABLE(mm_slots_hash, MM_SLOTS_HASH_BITS);",
            "",
            "static struct ksm_mm_slot ksm_mm_head = {",
            "\t.slot.mm_node = LIST_HEAD_INIT(ksm_mm_head.slot.mm_node),",
            "};",
            "static struct ksm_scan ksm_scan = {",
            "\t.mm_slot = &ksm_mm_head,",
            "};",
            "",
            "static struct kmem_cache *rmap_item_cache;",
            "static struct kmem_cache *stable_node_cache;",
            "static struct kmem_cache *mm_slot_cache;",
            "",
            "/* The number of pages scanned */",
            "static unsigned long ksm_pages_scanned;",
            "",
            "/* The number of nodes in the stable tree */",
            "static unsigned long ksm_pages_shared;",
            "",
            "/* The number of page slots additionally sharing those nodes */",
            "static unsigned long ksm_pages_sharing;",
            "",
            "/* The number of nodes in the unstable tree */",
            "static unsigned long ksm_pages_unshared;",
            "",
            "/* The number of rmap_items in use: to calculate pages_volatile */",
            "static unsigned long ksm_rmap_items;",
            "",
            "/* The number of stable_node chains */",
            "static unsigned long ksm_stable_node_chains;",
            "",
            "/* The number of stable_node dups linked to the stable_node chains */",
            "static unsigned long ksm_stable_node_dups;",
            "",
            "/* Delay in pruning stale stable_node_dups in the stable_node_chains */",
            "static unsigned int ksm_stable_node_chains_prune_millisecs = 2000;",
            "",
            "/* Maximum number of page slots sharing a stable node */",
            "static int ksm_max_page_sharing = 256;",
            "",
            "/* Number of pages ksmd should scan in one batch */",
            "static unsigned int ksm_thread_pages_to_scan = 100;",
            "",
            "/* Milliseconds ksmd should sleep between batches */",
            "static unsigned int ksm_thread_sleep_millisecs = 20;",
            "",
            "/* Checksum of an empty (zeroed) page */",
            "static unsigned int zero_checksum __read_mostly;",
            "",
            "/* Whether to merge empty (zeroed) pages with actual zero pages */",
            "static bool ksm_use_zero_pages __read_mostly;",
            "",
            "/* The number of zero pages which is placed by KSM */",
            "atomic_long_t ksm_zero_pages = ATOMIC_LONG_INIT(0);",
            "",
            "#ifdef CONFIG_NUMA",
            "/* Zeroed when merging across nodes is not allowed */",
            "static unsigned int ksm_merge_across_nodes = 1;",
            "static int ksm_nr_node_ids = 1;",
            "#else",
            "#define ksm_merge_across_nodes\t1U",
            "#define ksm_nr_node_ids\t\t1",
            "#endif",
            "",
            "#define KSM_RUN_STOP\t0",
            "#define KSM_RUN_MERGE\t1",
            "#define KSM_RUN_UNMERGE\t2",
            "#define KSM_RUN_OFFLINE\t4",
            "static unsigned long ksm_run = KSM_RUN_STOP;",
            "static void wait_while_offlining(void);",
            "",
            "static DECLARE_WAIT_QUEUE_HEAD(ksm_thread_wait);",
            "static DECLARE_WAIT_QUEUE_HEAD(ksm_iter_wait);",
            "static DEFINE_MUTEX(ksm_thread_mutex);",
            "static DEFINE_SPINLOCK(ksm_mmlist_lock);",
            "",
            "#define KSM_KMEM_CACHE(__struct, __flags) kmem_cache_create(#__struct,\\",
            "\t\tsizeof(struct __struct), __alignof__(struct __struct),\\",
            "\t\t(__flags), NULL)",
            ""
          ],
          "function_name": null,
          "description": "定义了KSM（内核同一内存页合并）模块的核心数据结构和全局变量，包括稳定树和不稳定树的数据结构、NUMA支持相关定义、哈希表和红黑树操作接口，以及用于跟踪扫描进度的kscan结构。核心功能是建立KSM内存合并算法的基础框架。",
          "similarity": 0.5307170152664185
        },
        {
          "chunk_id": 13,
          "file_path": "mm/ksm.c",
          "start_line": 3171,
          "end_line": 3288,
          "content": [
            "static ssize_t pages_to_scan_show(struct kobject *kobj,",
            "\t\t\t\t  struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_thread_pages_to_scan);",
            "}",
            "static ssize_t pages_to_scan_store(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr,",
            "\t\t\t\t   const char *buf, size_t count)",
            "{",
            "\tunsigned int nr_pages;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &nr_pages);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\tksm_thread_pages_to_scan = nr_pages;",
            "",
            "\treturn count;",
            "}",
            "static ssize_t run_show(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\tchar *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_run);",
            "}",
            "static ssize_t run_store(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\t const char *buf, size_t count)",
            "{",
            "\tunsigned int flags;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &flags);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "\tif (flags > KSM_RUN_UNMERGE)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * KSM_RUN_MERGE sets ksmd running, and 0 stops it running.",
            "\t * KSM_RUN_UNMERGE stops it running and unmerges all rmap_items,",
            "\t * breaking COW to free the pages_shared (but leaves mm_slots",
            "\t * on the list for when ksmd may be set running again).",
            "\t */",
            "",
            "\tmutex_lock(&ksm_thread_mutex);",
            "\twait_while_offlining();",
            "\tif (ksm_run != flags) {",
            "\t\tksm_run = flags;",
            "\t\tif (flags & KSM_RUN_UNMERGE) {",
            "\t\t\tset_current_oom_origin();",
            "\t\t\terr = unmerge_and_remove_all_rmap_items();",
            "\t\t\tclear_current_oom_origin();",
            "\t\t\tif (err) {",
            "\t\t\t\tksm_run = KSM_RUN_STOP;",
            "\t\t\t\tcount = err;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\tif (flags & KSM_RUN_MERGE)",
            "\t\twake_up_interruptible(&ksm_thread_wait);",
            "",
            "\treturn count;",
            "}",
            "static ssize_t merge_across_nodes_show(struct kobject *kobj,",
            "\t\t\t\t       struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_merge_across_nodes);",
            "}",
            "static ssize_t merge_across_nodes_store(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr,",
            "\t\t\t\t   const char *buf, size_t count)",
            "{",
            "\tint err;",
            "\tunsigned long knob;",
            "",
            "\terr = kstrtoul(buf, 10, &knob);",
            "\tif (err)",
            "\t\treturn err;",
            "\tif (knob > 1)",
            "\t\treturn -EINVAL;",
            "",
            "\tmutex_lock(&ksm_thread_mutex);",
            "\twait_while_offlining();",
            "\tif (ksm_merge_across_nodes != knob) {",
            "\t\tif (ksm_pages_shared || remove_all_stable_nodes())",
            "\t\t\terr = -EBUSY;",
            "\t\telse if (root_stable_tree == one_stable_tree) {",
            "\t\t\tstruct rb_root *buf;",
            "\t\t\t/*",
            "\t\t\t * This is the first time that we switch away from the",
            "\t\t\t * default of merging across nodes: must now allocate",
            "\t\t\t * a buffer to hold as many roots as may be needed.",
            "\t\t\t * Allocate stable and unstable together:",
            "\t\t\t * MAXSMP NODES_SHIFT 10 will use 16kB.",
            "\t\t\t */",
            "\t\t\tbuf = kcalloc(nr_node_ids + nr_node_ids, sizeof(*buf),",
            "\t\t\t\t      GFP_KERNEL);",
            "\t\t\t/* Let us assume that RB_ROOT is NULL is zero */",
            "\t\t\tif (!buf)",
            "\t\t\t\terr = -ENOMEM;",
            "\t\t\telse {",
            "\t\t\t\troot_stable_tree = buf;",
            "\t\t\t\troot_unstable_tree = buf + nr_node_ids;",
            "\t\t\t\t/* Stable tree is empty but not the unstable */",
            "\t\t\t\troot_unstable_tree[0] = one_unstable_tree[0];",
            "\t\t\t}",
            "\t\t}",
            "\t\tif (!err) {",
            "\t\t\tksm_merge_across_nodes = knob;",
            "\t\t\tksm_nr_node_ids = knob ? 1 : nr_node_ids;",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\treturn err ? err : count;",
            "}"
          ],
          "function_name": "pages_to_scan_show, pages_to_scan_store, run_show, run_store, merge_across_nodes_show, merge_across_nodes_store",
          "description": "pages_to_scan_*控制KSMAPI扫描页数；run_*控制KSMAPI运行模式（启动/停止/解合并）；merge_across_nodes_*配置是否允许跨NUMA节点合并，切换时重构稳定树根节点数组",
          "similarity": 0.5143605470657349
        },
        {
          "chunk_id": 9,
          "file_path": "mm/ksm.c",
          "start_line": 2608,
          "end_line": 2731,
          "content": [
            "int ksm_enable_merge_any(struct mm_struct *mm)",
            "{",
            "\tint err;",
            "",
            "\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn 0;",
            "",
            "\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {",
            "\t\terr = __ksm_enter(mm);",
            "\t\tif (err)",
            "\t\t\treturn err;",
            "\t}",
            "",
            "\tset_bit(MMF_VM_MERGE_ANY, &mm->flags);",
            "\tksm_add_vmas(mm);",
            "",
            "\treturn 0;",
            "}",
            "int ksm_disable_merge_any(struct mm_struct *mm)",
            "{",
            "\tint err;",
            "",
            "\tif (!test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn 0;",
            "",
            "\terr = ksm_del_vmas(mm);",
            "\tif (err) {",
            "\t\tksm_add_vmas(mm);",
            "\t\treturn err;",
            "\t}",
            "",
            "\tclear_bit(MMF_VM_MERGE_ANY, &mm->flags);",
            "\treturn 0;",
            "}",
            "int ksm_disable(struct mm_struct *mm)",
            "{",
            "\tmmap_assert_write_locked(mm);",
            "",
            "\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags))",
            "\t\treturn 0;",
            "\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn ksm_disable_merge_any(mm);",
            "\treturn ksm_del_vmas(mm);",
            "}",
            "int ksm_madvise(struct vm_area_struct *vma, unsigned long start,",
            "\t\tunsigned long end, int advice, unsigned long *vm_flags)",
            "{",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tint err;",
            "",
            "\tswitch (advice) {",
            "\tcase MADV_MERGEABLE:",
            "\t\tif (vma->vm_flags & VM_MERGEABLE)",
            "\t\t\treturn 0;",
            "\t\tif (!vma_ksm_compatible(vma))",
            "\t\t\treturn 0;",
            "",
            "\t\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {",
            "\t\t\terr = __ksm_enter(mm);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "",
            "\t\t*vm_flags |= VM_MERGEABLE;",
            "\t\tbreak;",
            "",
            "\tcase MADV_UNMERGEABLE:",
            "\t\tif (!(*vm_flags & VM_MERGEABLE))",
            "\t\t\treturn 0;\t\t/* just ignore the advice */",
            "",
            "\t\tif (vma->anon_vma) {",
            "\t\t\terr = unmerge_ksm_pages(vma, start, end, true);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "",
            "\t\t*vm_flags &= ~VM_MERGEABLE;",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __ksm_enter(struct mm_struct *mm)",
            "{",
            "\tstruct ksm_mm_slot *mm_slot;",
            "\tstruct mm_slot *slot;",
            "\tint needs_wakeup;",
            "",
            "\tmm_slot = mm_slot_alloc(mm_slot_cache);",
            "\tif (!mm_slot)",
            "\t\treturn -ENOMEM;",
            "",
            "\tslot = &mm_slot->slot;",
            "",
            "\t/* Check ksm_run too?  Would need tighter locking */",
            "\tneeds_wakeup = list_empty(&ksm_mm_head.slot.mm_node);",
            "",
            "\tspin_lock(&ksm_mmlist_lock);",
            "\tmm_slot_insert(mm_slots_hash, mm, slot);",
            "\t/*",
            "\t * When KSM_RUN_MERGE (or KSM_RUN_STOP),",
            "\t * insert just behind the scanning cursor, to let the area settle",
            "\t * down a little; when fork is followed by immediate exec, we don't",
            "\t * want ksmd to waste time setting up and tearing down an rmap_list.",
            "\t *",
            "\t * But when KSM_RUN_UNMERGE, it's important to insert ahead of its",
            "\t * scanning cursor, otherwise KSM pages in newly forked mms will be",
            "\t * missed: then we might as well insert at the end of the list.",
            "\t */",
            "\tif (ksm_run & KSM_RUN_UNMERGE)",
            "\t\tlist_add_tail(&slot->mm_node, &ksm_mm_head.slot.mm_node);",
            "\telse",
            "\t\tlist_add_tail(&slot->mm_node, &ksm_scan.mm_slot->slot.mm_node);",
            "\tspin_unlock(&ksm_mmlist_lock);",
            "",
            "\tset_bit(MMF_VM_MERGEABLE, &mm->flags);",
            "\tmmgrab(mm);",
            "",
            "\tif (needs_wakeup)",
            "\t\twake_up_interruptible(&ksm_thread_wait);",
            "",
            "\ttrace_ksm_enter(mm);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ksm_enable_merge_any, ksm_disable_merge_any, ksm_disable, ksm_madvise, __ksm_enter",
          "description": "ksm_enable_merge_any 启用全局合并标志并注册内存区域。ksm_disable_merge_any 禁用合并并清理配置。ksm_disable 移除所有KSM配置。ksm_madvise 处理MADV_MERGEABLE/MADV_UNMERGEABLE建议，调整页面属性。__ksm_enter 注册内存区域到KSM管理系统。",
          "similarity": 0.4783368706703186
        },
        {
          "chunk_id": 2,
          "file_path": "mm/ksm.c",
          "start_line": 486,
          "end_line": 622,
          "content": [
            "static int break_ksm(struct vm_area_struct *vma, unsigned long addr, bool lock_vma)",
            "{",
            "\tvm_fault_t ret = 0;",
            "\tconst struct mm_walk_ops *ops = lock_vma ?",
            "\t\t\t\t&break_ksm_lock_vma_ops : &break_ksm_ops;",
            "",
            "\tdo {",
            "\t\tint ksm_page;",
            "",
            "\t\tcond_resched();",
            "\t\tksm_page = walk_page_range_vma(vma, addr, addr + 1, ops, NULL);",
            "\t\tif (WARN_ON_ONCE(ksm_page < 0))",
            "\t\t\treturn ksm_page;",
            "\t\tif (!ksm_page)",
            "\t\t\treturn 0;",
            "\t\tret = handle_mm_fault(vma, addr,",
            "\t\t\t\t      FAULT_FLAG_UNSHARE | FAULT_FLAG_REMOTE,",
            "\t\t\t\t      NULL);",
            "\t} while (!(ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | VM_FAULT_OOM)));",
            "\t/*",
            "\t * We must loop until we no longer find a KSM page because",
            "\t * handle_mm_fault() may back out if there's any difficulty e.g. if",
            "\t * pte accessed bit gets updated concurrently.",
            "\t *",
            "\t * VM_FAULT_SIGBUS could occur if we race with truncation of the",
            "\t * backing file, which also invalidates anonymous pages: that's",
            "\t * okay, that truncation will have unmapped the PageKsm for us.",
            "\t *",
            "\t * VM_FAULT_OOM: at the time of writing (late July 2009), setting",
            "\t * aside mem_cgroup limits, VM_FAULT_OOM would only be set if the",
            "\t * current task has TIF_MEMDIE set, and will be OOM killed on return",
            "\t * to user; and ksmd, having no mm, would never be chosen for that.",
            "\t *",
            "\t * But if the mm is in a limited mem_cgroup, then the fault may fail",
            "\t * with VM_FAULT_OOM even if the current task is not TIF_MEMDIE; and",
            "\t * even ksmd can fail in this way - though it's usually breaking ksm",
            "\t * just to undo a merge it made a moment before, so unlikely to oom.",
            "\t *",
            "\t * That's a pity: we might therefore have more kernel pages allocated",
            "\t * than we're counting as nodes in the stable tree; but ksm_do_scan",
            "\t * will retry to break_cow on each pass, so should recover the page",
            "\t * in due course.  The important thing is to not let VM_MERGEABLE",
            "\t * be cleared while any such pages might remain in the area.",
            "\t */",
            "\treturn (ret & VM_FAULT_OOM) ? -ENOMEM : 0;",
            "}",
            "static bool vma_ksm_compatible(struct vm_area_struct *vma)",
            "{",
            "\tif (vma->vm_flags & (VM_SHARED  | VM_MAYSHARE   | VM_PFNMAP  |",
            "\t\t\t     VM_IO      | VM_DONTEXPAND | VM_HUGETLB |",
            "\t\t\t     VM_MIXEDMAP))",
            "\t\treturn false;\t\t/* just ignore the advice */",
            "",
            "\tif (vma_is_dax(vma))",
            "\t\treturn false;",
            "",
            "#ifdef VM_SAO",
            "\tif (vma->vm_flags & VM_SAO)",
            "\t\treturn false;",
            "#endif",
            "#ifdef VM_SPARC_ADI",
            "\tif (vma->vm_flags & VM_SPARC_ADI)",
            "\t\treturn false;",
            "#endif",
            "",
            "\treturn true;",
            "}",
            "static void break_cow(struct ksm_rmap_item *rmap_item)",
            "{",
            "\tstruct mm_struct *mm = rmap_item->mm;",
            "\tunsigned long addr = rmap_item->address;",
            "\tstruct vm_area_struct *vma;",
            "",
            "\t/*",
            "\t * It is not an accident that whenever we want to break COW",
            "\t * to undo, we also need to drop a reference to the anon_vma.",
            "\t */",
            "\tput_anon_vma(rmap_item->anon_vma);",
            "",
            "\tmmap_read_lock(mm);",
            "\tvma = find_mergeable_vma(mm, addr);",
            "\tif (vma)",
            "\t\tbreak_ksm(vma, addr, false);",
            "\tmmap_read_unlock(mm);",
            "}",
            "static inline int get_kpfn_nid(unsigned long kpfn)",
            "{",
            "\treturn ksm_merge_across_nodes ? 0 : NUMA(pfn_to_nid(kpfn));",
            "}",
            "static inline void free_stable_node_chain(struct ksm_stable_node *chain,",
            "\t\t\t\t\t  struct rb_root *root)",
            "{",
            "\trb_erase(&chain->node, root);",
            "\tfree_stable_node(chain);",
            "\tksm_stable_node_chains--;",
            "}",
            "static void remove_node_from_stable_tree(struct ksm_stable_node *stable_node)",
            "{",
            "\tstruct ksm_rmap_item *rmap_item;",
            "",
            "\t/* check it's not STABLE_NODE_CHAIN or negative */",
            "\tBUG_ON(stable_node->rmap_hlist_len < 0);",
            "",
            "\thlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {",
            "\t\tif (rmap_item->hlist.next) {",
            "\t\t\tksm_pages_sharing--;",
            "\t\t\ttrace_ksm_remove_rmap_item(stable_node->kpfn, rmap_item, rmap_item->mm);",
            "\t\t} else {",
            "\t\t\tksm_pages_shared--;",
            "\t\t}",
            "",
            "\t\trmap_item->mm->ksm_merging_pages--;",
            "",
            "\t\tVM_BUG_ON(stable_node->rmap_hlist_len <= 0);",
            "\t\tstable_node->rmap_hlist_len--;",
            "\t\tput_anon_vma(rmap_item->anon_vma);",
            "\t\trmap_item->address &= PAGE_MASK;",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\t/*",
            "\t * We need the second aligned pointer of the migrate_nodes",
            "\t * list_head to stay clear from the rb_parent_color union",
            "\t * (aligned and different than any node) and also different",
            "\t * from &migrate_nodes. This will verify that future list.h changes",
            "\t * don't break STABLE_NODE_DUP_HEAD. Only recent gcc can handle it.",
            "\t */",
            "\tBUILD_BUG_ON(STABLE_NODE_DUP_HEAD <= &migrate_nodes);",
            "\tBUILD_BUG_ON(STABLE_NODE_DUP_HEAD >= &migrate_nodes + 1);",
            "",
            "\ttrace_ksm_remove_ksm_page(stable_node->kpfn);",
            "\tif (stable_node->head == &migrate_nodes)",
            "\t\tlist_del(&stable_node->list);",
            "\telse",
            "\t\tstable_node_dup_del(stable_node);",
            "\tfree_stable_node(stable_node);",
            "}"
          ],
          "function_name": "break_ksm, vma_ksm_compatible, break_cow, get_kpfn_nid, free_stable_node_chain, remove_node_from_stable_tree",
          "description": "实现KSM页面分解逻辑，包含检查VMA是否兼容KSM、强制打破写时复制（COW）操作、获取KPFN NUMA节点ID以及稳定树节点链表清理等功能。核心作用是执行实际的KSM页面分离操作并维护稳定树结构。",
          "similarity": 0.46602773666381836
        },
        {
          "chunk_id": 15,
          "file_path": "mm/ksm.c",
          "start_line": 3427,
          "end_line": 3508,
          "content": [
            "static ssize_t stable_node_dups_show(struct kobject *kobj,",
            "\t\t\t\t     struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_stable_node_dups);",
            "}",
            "static ssize_t stable_node_chains_show(struct kobject *kobj,",
            "\t\t\t\t       struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_stable_node_chains);",
            "}",
            "static ssize_t",
            "stable_node_chains_prune_millisecs_show(struct kobject *kobj,",
            "\t\t\t\t\tstruct kobj_attribute *attr,",
            "\t\t\t\t\tchar *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_stable_node_chains_prune_millisecs);",
            "}",
            "static ssize_t",
            "stable_node_chains_prune_millisecs_store(struct kobject *kobj,",
            "\t\t\t\t\t struct kobj_attribute *attr,",
            "\t\t\t\t\t const char *buf, size_t count)",
            "{",
            "\tunsigned int msecs;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &msecs);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\tksm_stable_node_chains_prune_millisecs = msecs;",
            "",
            "\treturn count;",
            "}",
            "static ssize_t full_scans_show(struct kobject *kobj,",
            "\t\t\t       struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_scan.seqnr);",
            "}",
            "static int __init ksm_init(void)",
            "{",
            "\tstruct task_struct *ksm_thread;",
            "\tint err;",
            "",
            "\t/* The correct value depends on page size and endianness */",
            "\tzero_checksum = calc_checksum(ZERO_PAGE(0));",
            "\t/* Default to false for backwards compatibility */",
            "\tksm_use_zero_pages = false;",
            "",
            "\terr = ksm_slab_init();",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\tksm_thread = kthread_run(ksm_scan_thread, NULL, \"ksmd\");",
            "\tif (IS_ERR(ksm_thread)) {",
            "\t\tpr_err(\"ksm: creating kthread failed\\n\");",
            "\t\terr = PTR_ERR(ksm_thread);",
            "\t\tgoto out_free;",
            "\t}",
            "",
            "#ifdef CONFIG_SYSFS",
            "\terr = sysfs_create_group(mm_kobj, &ksm_attr_group);",
            "\tif (err) {",
            "\t\tpr_err(\"ksm: register sysfs failed\\n\");",
            "\t\tkthread_stop(ksm_thread);",
            "\t\tgoto out_free;",
            "\t}",
            "#else",
            "\tksm_run = KSM_RUN_MERGE;\t/* no way for user to start it */",
            "",
            "#endif /* CONFIG_SYSFS */",
            "",
            "#ifdef CONFIG_MEMORY_HOTREMOVE",
            "\t/* There is no significance to this priority 100 */",
            "\thotplug_memory_notifier(ksm_memory_callback, KSM_CALLBACK_PRI);",
            "#endif",
            "\treturn 0;",
            "",
            "out_free:",
            "\tksm_slab_free();",
            "out:",
            "\treturn err;",
            "}"
          ],
          "function_name": "stable_node_dups_show, stable_node_chains_show, stable_node_chains_prune_millisecs_show, stable_node_chains_prune_millisecs_store, full_scans_show, ksm_init",
          "description": "此代码段实现了KSM（内核同页合并）子系统的sysfs接口，用于暴露稳定节点重复计数、链表数量及修剪间隔等运行时参数。其中`stable_node_*`系列函数通过sysfs接口读取内部状态变量，`stable_node_chains_prune_millisecs`支持动态修改修剪超时时长，`ksm_init`初始化KSM线程并注册sysfs属性组。",
          "similarity": 0.4623270630836487
        }
      ]
    },
    {
      "source_file": "kernel/async.c",
      "md_summary": "> 自动生成时间: 2025-10-25 11:49:14\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `async.c`\n\n---\n\n# async.c 技术文档\n\n## 1. 文件概述\n\n`async.c` 实现了 Linux 内核中的异步函数调用机制，主要用于优化系统启动性能。该机制允许在内核初始化阶段将原本串行执行的、相互独立的硬件探测和初始化操作并行化，从而显著缩短启动时间。其核心思想是在保持对外可见操作顺序一致性的前提下，内部执行过程可乱序进行，类似于乱序执行 CPU 的“按序提交”语义。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct async_entry`**：表示一个异步任务条目，包含：\n  - `domain_list` / `global_list`：分别链接到所属域和全局的待处理链表\n  - `work`：关联的 workqueue 工作项\n  - `cookie`：序列号，用于同步控制\n  - `func` / `data`：要执行的函数及其参数\n  - `domain`：所属的异步域\n\n- **`struct async_domain`**：异步执行域，用于将异步任务分组管理，默认使用 `async_dfl_domain`\n\n- **全局变量**：\n  - `next_cookie`：单调递增的序列号生成器\n  - `async_global_pending`：所有已注册域的全局待处理任务链表\n  - `async_dfl_domain`：默认异步域\n  - `async_lock`：保护异步任务队列的自旋锁\n  - `entry_count`：当前挂起的异步任务计数\n\n### 主要函数\n\n- **`async_schedule_node_domain()`**：在指定 NUMA 节点和异步域中调度异步函数\n- **`async_schedule_node()`**：在指定 NUMA 节点上调度异步函数（使用默认域）\n- **`async_schedule_dev_nocall()`**：基于设备的 NUMA 信息调度异步函数（失败时不回退到同步执行）\n- **`lowest_in_progress()`**：获取指定域或全局中最早（最小 cookie）的未完成任务\n- **`async_run_entry_fn()`**：workqueue 回调函数，实际执行异步任务并清理资源\n\n## 3. 关键实现\n\n### 序列 Cookie 机制\n- 每个异步任务分配一个单调递增的 `async_cookie_t`（64 位无符号整数）\n- 任务执行前可通过 `async_synchronize_cookie()` 等待所有小于等于指定 cookie 的任务完成\n- 保证对外部可见操作（如设备注册）的顺序一致性\n\n### 内存与负载控制\n- 使用 `GFP_ATOMIC` 分配内存，支持原子上下文调用\n- 当内存不足或挂起任务超过 `MAX_WORK`（32768）时，自动回退到同步执行\n- 通过 `entry_count` 原子计数器跟踪挂起任务数量\n\n### 双链表管理\n- 每个任务同时链接到：\n  - 所属域的 `domain->pending` 链表（按 cookie 顺序）\n  - 全局 `async_global_pending` 链表（仅当域已注册）\n- 保证域内和全局的同步操作都能正确等待\n\n### NUMA 感知调度\n- 通过 `queue_work_node()` 将任务调度到指定 NUMA 节点\n- 若节点无效则自动分发到可用 CPU\n\n### 资源清理与通知\n- 任务执行完成后从链表移除并释放内存\n- 通过 `wake_up(&async_done)` 唤醒等待同步完成的线程\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/async.h>`：异步 API 定义\n  - `<linux/workqueue.h>`：工作队列机制\n  - `\"workqueue_internal.h\"`：内部 workqueue 接口\n  - 其他基础内核头文件（atomic、slab、wait 等）\n\n- **核心子系统**：\n  - **Workqueue 子系统**：实际执行异步任务的底层机制\n  - **内存管理子系统**：任务结构体内存分配\n  - **调度器**：NUMA 节点感知的任务调度\n\n- **导出符号**：\n  - `async_schedule_node_domain`\n  - `async_schedule_node`\n\n## 5. 使用场景\n\n- **内核启动优化**：\n  - 并行执行设备探测（如 PCI、USB 控制器初始化）\n  - 异步加载固件或执行硬件自检\n\n- **驱动初始化**：\n  - 驱动可将耗时的初始化操作（如 PHY 配置、固件加载）放入异步任务\n  - 通过 `async_synchronize_full()` 确保在模块初始化完成前所有异步任务结束\n\n- **NUMA 优化**：\n  - 将设备相关的初始化任务调度到设备所在 NUMA 节点，减少远程内存访问\n\n- **资源受限环境**：\n  - 在内存压力下自动回退到同步执行，保证系统稳定性\n  - 通过 `MAX_WORK` 限制防止异步任务无限堆积",
      "similarity": 0.5223855972290039,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/async.c",
          "start_line": 1,
          "end_line": 81,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * async.c: Asynchronous function calls for boot performance",
            " *",
            " * (C) Copyright 2009 Intel Corporation",
            " * Author: Arjan van de Ven <arjan@linux.intel.com>",
            " */",
            "",
            "",
            "/*",
            "",
            "Goals and Theory of Operation",
            "",
            "The primary goal of this feature is to reduce the kernel boot time,",
            "by doing various independent hardware delays and discovery operations",
            "decoupled and not strictly serialized.",
            "",
            "More specifically, the asynchronous function call concept allows",
            "certain operations (primarily during system boot) to happen",
            "asynchronously, out of order, while these operations still",
            "have their externally visible parts happen sequentially and in-order.",
            "(not unlike how out-of-order CPUs retire their instructions in order)",
            "",
            "Key to the asynchronous function call implementation is the concept of",
            "a \"sequence cookie\" (which, although it has an abstracted type, can be",
            "thought of as a monotonically incrementing number).",
            "",
            "The async core will assign each scheduled event such a sequence cookie and",
            "pass this to the called functions.",
            "",
            "The asynchronously called function should before doing a globally visible",
            "operation, such as registering device numbers, call the",
            "async_synchronize_cookie() function and pass in its own cookie. The",
            "async_synchronize_cookie() function will make sure that all asynchronous",
            "operations that were scheduled prior to the operation corresponding with the",
            "cookie have completed.",
            "",
            "Subsystem/driver initialization code that scheduled asynchronous probe",
            "functions, but which shares global resources with other drivers/subsystems",
            "that do not use the asynchronous call feature, need to do a full",
            "synchronization with the async_synchronize_full() function, before returning",
            "from their init function. This is to maintain strict ordering between the",
            "asynchronous and synchronous parts of the kernel.",
            "",
            "*/",
            "",
            "#include <linux/async.h>",
            "#include <linux/atomic.h>",
            "#include <linux/export.h>",
            "#include <linux/ktime.h>",
            "#include <linux/pid.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/wait.h>",
            "#include <linux/workqueue.h>",
            "",
            "#include \"workqueue_internal.h\"",
            "",
            "static async_cookie_t next_cookie = 1;",
            "",
            "#define MAX_WORK\t\t32768",
            "#define ASYNC_COOKIE_MAX\tULLONG_MAX\t/* infinity cookie */",
            "",
            "static LIST_HEAD(async_global_pending);\t/* pending from all registered doms */",
            "static ASYNC_DOMAIN(async_dfl_domain);",
            "static DEFINE_SPINLOCK(async_lock);",
            "",
            "struct async_entry {",
            "\tstruct list_head\tdomain_list;",
            "\tstruct list_head\tglobal_list;",
            "\tstruct work_struct\twork;",
            "\tasync_cookie_t\t\tcookie;",
            "\tasync_func_t\t\tfunc;",
            "\tvoid\t\t\t*data;",
            "\tstruct async_domain\t*domain;",
            "};",
            "",
            "static DECLARE_WAIT_QUEUE_HEAD(async_done);",
            "",
            "static atomic_t entry_count;",
            ""
          ],
          "function_name": null,
          "description": "定义异步任务结构体和核心变量，支持多域异步调度",
          "similarity": 0.5572049617767334
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/async.c",
          "start_line": 82,
          "end_line": 201,
          "content": [
            "static long long microseconds_since(ktime_t start)",
            "{",
            "\tktime_t now = ktime_get();",
            "\treturn ktime_to_ns(ktime_sub(now, start)) >> 10;",
            "}",
            "static async_cookie_t lowest_in_progress(struct async_domain *domain)",
            "{",
            "\tstruct async_entry *first = NULL;",
            "\tasync_cookie_t ret = ASYNC_COOKIE_MAX;",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&async_lock, flags);",
            "",
            "\tif (domain) {",
            "\t\tif (!list_empty(&domain->pending))",
            "\t\t\tfirst = list_first_entry(&domain->pending,",
            "\t\t\t\t\tstruct async_entry, domain_list);",
            "\t} else {",
            "\t\tif (!list_empty(&async_global_pending))",
            "\t\t\tfirst = list_first_entry(&async_global_pending,",
            "\t\t\t\t\tstruct async_entry, global_list);",
            "\t}",
            "",
            "\tif (first)",
            "\t\tret = first->cookie;",
            "",
            "\tspin_unlock_irqrestore(&async_lock, flags);",
            "\treturn ret;",
            "}",
            "static void async_run_entry_fn(struct work_struct *work)",
            "{",
            "\tstruct async_entry *entry =",
            "\t\tcontainer_of(work, struct async_entry, work);",
            "\tunsigned long flags;",
            "\tktime_t calltime;",
            "",
            "\t/* 1) run (and print duration) */",
            "\tpr_debug(\"calling  %lli_%pS @ %i\\n\", (long long)entry->cookie,",
            "\t\t entry->func, task_pid_nr(current));",
            "\tcalltime = ktime_get();",
            "",
            "\tentry->func(entry->data, entry->cookie);",
            "",
            "\tpr_debug(\"initcall %lli_%pS returned after %lld usecs\\n\",",
            "\t\t (long long)entry->cookie, entry->func,",
            "\t\t microseconds_since(calltime));",
            "",
            "\t/* 2) remove self from the pending queues */",
            "\tspin_lock_irqsave(&async_lock, flags);",
            "\tlist_del_init(&entry->domain_list);",
            "\tlist_del_init(&entry->global_list);",
            "",
            "\t/* 3) free the entry */",
            "\tkfree(entry);",
            "\tatomic_dec(&entry_count);",
            "",
            "\tspin_unlock_irqrestore(&async_lock, flags);",
            "",
            "\t/* 4) wake up any waiters */",
            "\twake_up(&async_done);",
            "}",
            "static async_cookie_t __async_schedule_node_domain(async_func_t func,",
            "\t\t\t\t\t\t   void *data, int node,",
            "\t\t\t\t\t\t   struct async_domain *domain,",
            "\t\t\t\t\t\t   struct async_entry *entry)",
            "{",
            "\tasync_cookie_t newcookie;",
            "\tunsigned long flags;",
            "",
            "\tINIT_LIST_HEAD(&entry->domain_list);",
            "\tINIT_LIST_HEAD(&entry->global_list);",
            "\tINIT_WORK(&entry->work, async_run_entry_fn);",
            "\tentry->func = func;",
            "\tentry->data = data;",
            "\tentry->domain = domain;",
            "",
            "\tspin_lock_irqsave(&async_lock, flags);",
            "",
            "\t/* allocate cookie and queue */",
            "\tnewcookie = entry->cookie = next_cookie++;",
            "",
            "\tlist_add_tail(&entry->domain_list, &domain->pending);",
            "\tif (domain->registered)",
            "\t\tlist_add_tail(&entry->global_list, &async_global_pending);",
            "",
            "\tatomic_inc(&entry_count);",
            "\tspin_unlock_irqrestore(&async_lock, flags);",
            "",
            "\t/* schedule for execution */",
            "\tqueue_work_node(node, system_unbound_wq, &entry->work);",
            "",
            "\treturn newcookie;",
            "}",
            "async_cookie_t async_schedule_node_domain(async_func_t func, void *data,",
            "\t\t\t\t\t  int node, struct async_domain *domain)",
            "{",
            "\tstruct async_entry *entry;",
            "\tunsigned long flags;",
            "\tasync_cookie_t newcookie;",
            "",
            "\t/* allow irq-off callers */",
            "\tentry = kzalloc(sizeof(struct async_entry), GFP_ATOMIC);",
            "",
            "\t/*",
            "\t * If we're out of memory or if there's too much work",
            "\t * pending already, we execute synchronously.",
            "\t */",
            "\tif (!entry || atomic_read(&entry_count) > MAX_WORK) {",
            "\t\tkfree(entry);",
            "\t\tspin_lock_irqsave(&async_lock, flags);",
            "\t\tnewcookie = next_cookie++;",
            "\t\tspin_unlock_irqrestore(&async_lock, flags);",
            "",
            "\t\t/* low on memory.. run synchronously */",
            "\t\tfunc(data, newcookie);",
            "\t\treturn newcookie;",
            "\t}",
            "",
            "\treturn __async_schedule_node_domain(func, data, node, domain, entry);",
            "}"
          ],
          "function_name": "microseconds_since, lowest_in_progress, async_run_entry_fn, __async_schedule_node_domain, async_schedule_node_domain",
          "description": "实现异步任务调度与执行逻辑，包含时间测量、任务排队及工作队列调度",
          "similarity": 0.5257581472396851
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/async.c",
          "start_line": 241,
          "end_line": 290,
          "content": [
            "async_cookie_t async_schedule_node(async_func_t func, void *data, int node)",
            "{",
            "\treturn async_schedule_node_domain(func, data, node, &async_dfl_domain);",
            "}",
            "bool async_schedule_dev_nocall(async_func_t func, struct device *dev)",
            "{",
            "\tstruct async_entry *entry;",
            "",
            "\tentry = kzalloc(sizeof(struct async_entry), GFP_KERNEL);",
            "",
            "\t/* Give up if there is no memory or too much work. */",
            "\tif (!entry || atomic_read(&entry_count) > MAX_WORK) {",
            "\t\tkfree(entry);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t__async_schedule_node_domain(func, dev, dev_to_node(dev),",
            "\t\t\t\t     &async_dfl_domain, entry);",
            "\treturn true;",
            "}",
            "void async_synchronize_full(void)",
            "{",
            "\tasync_synchronize_full_domain(NULL);",
            "}",
            "void async_synchronize_full_domain(struct async_domain *domain)",
            "{",
            "\tasync_synchronize_cookie_domain(ASYNC_COOKIE_MAX, domain);",
            "}",
            "void async_synchronize_cookie_domain(async_cookie_t cookie, struct async_domain *domain)",
            "{",
            "\tktime_t starttime;",
            "",
            "\tpr_debug(\"async_waiting @ %i\\n\", task_pid_nr(current));",
            "\tstarttime = ktime_get();",
            "",
            "\twait_event(async_done, lowest_in_progress(domain) >= cookie);",
            "",
            "\tpr_debug(\"async_continuing @ %i after %lli usec\\n\", task_pid_nr(current),",
            "\t\t microseconds_since(starttime));",
            "}",
            "void async_synchronize_cookie(async_cookie_t cookie)",
            "{",
            "\tasync_synchronize_cookie_domain(cookie, &async_dfl_domain);",
            "}",
            "bool current_is_async(void)",
            "{",
            "\tstruct worker *worker = current_wq_worker();",
            "",
            "\treturn worker && worker->current_func == async_run_entry_fn;",
            "}"
          ],
          "function_name": "async_schedule_node, async_schedule_dev_nocall, async_synchronize_full, async_synchronize_full_domain, async_synchronize_cookie_domain, async_synchronize_cookie, current_is_async",
          "description": "提供同步屏障接口和运行态检测，确保异步操作有序完成",
          "similarity": 0.5096811652183533
        }
      ]
    },
    {
      "source_file": "mm/khugepaged.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:26:37\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `khugepaged.c`\n\n---\n\n# khugepaged.c 技术文档\n\n## 1. 文件概述\n\n`khugepaged.c` 是 Linux 内核中透明大页（Transparent Huge Page, THP）子系统的核心组件之一，负责在后台异步地将符合条件的小页（4KB）合并为大页（通常为 2MB 的 PMD 级别大页）。该文件实现了名为 `khugepaged` 的内核线程及其相关扫描、合并逻辑，旨在提升内存访问性能并减少 TLB 压力。通过周期性扫描进程地址空间，识别可合并区域，并尝试分配和填充大页，从而优化系统整体内存效率。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`enum scan_result`**  \n  定义了页面扫描过程中可能返回的各种结果状态码，用于控制合并流程的决策（如失败原因、成功条件等）。\n\n- **`struct collapse_control`**  \n  控制页面折叠（collapse）过程的上下文信息，包括是否由 `khugepaged` 发起、各 NUMA 节点的负载统计及分配回退掩码。\n\n- **`struct khugepaged_mm_slot`**  \n  表示正在被 `khugepaged` 扫描的每个 `mm_struct`（进程地址空间）的元数据槽位，继承自通用 `mm_slot` 结构。\n\n- **`struct khugepaged_scan`**  \n  全局扫描游标，记录当前扫描的 `mm` 列表头、当前 `mm_slot` 及下一次扫描的虚拟地址。\n\n### 全局变量\n\n- `khugepaged_thread`：指向后台 `khugepaged` 内核线程的 `task_struct`。\n- `khugepaged_pages_to_scan`：每次扫描迭代处理的 PTE 或 VMA 数量。\n- `khugepaged_scan_sleep_millisecs` / `khugepaged_alloc_sleep_millisecs`：控制扫描与内存分配的休眠间隔。\n- `khugepaged_max_ptes_none/swap/shared`：限制在合并过程中允许存在的未映射、交换或共享 PTE 的最大数量。\n- `mm_slots_hash`：哈希表，用于快速查找正在被扫描的 `mm_struct`。\n- `khugepaged_scan`：全局唯一的扫描状态结构体。\n\n### Sysfs 接口（CONFIG_SYSFS）\n\n提供用户空间可配置参数：\n- `scan_sleep_millisecs`：扫描间隔\n- `alloc_sleep_millisecs`：分配失败后的重试间隔\n- `pages_to_scan`：每次扫描的页数\n- `pages_collapsed` / `full_scans`：只读统计信息\n- `defrag`：是否启用内存碎片整理\n- `max_ptes_none` / `max_ptes_swap`：控制合并容忍度\n\n## 3. 关键实现\n\n### 后台扫描机制\n- 使用单一线程 `khugepaged` 循环遍历所有注册到 `mm_slots_hash` 中的进程地址空间。\n- 每次从 `khugepaged_scan.mm_head` 列表中取出一个 `mm_slot`，按虚拟地址顺序扫描其 VMA 区域。\n- 扫描粒度由 `khugepaged_pages_to_scan` 控制，默认为 4096 页（8×512），每轮扫描后休眠 `khugepaged_scan_sleep_millisecs` 毫秒。\n\n### 大页合并条件\n- 仅对支持透明大页的 VMA（如匿名私有映射）进行处理。\n- 检查目标 2MB 区域内：\n  - 已映射的小页数量足够多；\n  - 未映射（none）、交换（swap）或共享（shared）的 PTE 数量不超过 `khugepaged_max_ptes_*` 阈值；\n  - 所有页面满足可合并条件（如非 KSM、非 compound、已加入 LRU、引用计数合适等）。\n- 若满足条件，则分配一个新大页，复制小页内容，并更新页表。\n\n### 内存分配与回退策略\n- 优先在本地 NUMA 节点分配大页。\n- 若分配失败且启用了 `defrag`，则尝试内存压缩（compaction）。\n- 支持基于 `alloc_nmask` 的跨节点分配回退。\n\n### 并发与同步\n- 使用 `khugepaged_mutex` 保护关键操作（如添加/移除 mm slot）。\n- 通过 `mm_slot` 机制确保同一 `mm` 不被重复扫描。\n- 利用 RCU 和页锁（`trylock_page()`）避免与用户态访问或其它内核路径冲突。\n\n### 统计与追踪\n- 更新 `khugepaged_pages_collapsed` 和 `khugepaged_full_scans` 等统计计数器。\n- 集成 `trace/events/huge_memory.h` 提供详细的合并事件追踪点。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/rmap.h>`、`<linux/swap.h>` 等，进行页表遍历、反向映射、页面迁移等操作。\n- **透明大页框架**：与 `huge_memory.c` 协同工作，共享 THP 配置标志（如 `TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG`）。\n- **KSM（Kernel Samepage Merging）**：检查页面是否已被 KSM 标记，避免合并 KSM 页面。\n- **Userfaultfd**：检测 `UFFD_WP`（用户态写保护）标记，防止非法合并。\n- **NUMA 与内存策略**：使用 `nodemask_t` 和 NUMA 感知分配。\n- **内核线程与调度**：基于 `kthread` 框架实现后台任务，支持 freezer（挂起/恢复）。\n- **Sysfs**：通过 sysfs 向用户空间暴露 tunable 参数（需 `CONFIG_SYSFS`）。\n\n## 5. 使用场景\n\n- **通用服务器负载**：在数据库、虚拟化、大数据处理等内存密集型应用中，自动提升 TLB 覆盖率，降低缺页开销。\n- **延迟敏感型应用**：通过后台预合并，避免运行时同步分配大页导致的延迟毛刺。\n- **内存碎片整理**：配合 `defrag` 选项，在内存紧张时主动整理碎片以促进大页分配。\n- **动态调优**：管理员可通过 sysfs 实时调整扫描频率、合并激进程度等参数，平衡性能与内存开销。\n- **NUMA 系统优化**：在多节点系统中，结合本地分配策略提升内存访问局部性。",
      "similarity": 0.521019458770752,
      "chunks": [
        {
          "chunk_id": 6,
          "file_path": "mm/khugepaged.c",
          "start_line": 845,
          "end_line": 955,
          "content": [
            "static void khugepaged_alloc_sleep(void)",
            "{",
            "\tDEFINE_WAIT(wait);",
            "",
            "\tadd_wait_queue(&khugepaged_wait, &wait);",
            "\t__set_current_state(TASK_INTERRUPTIBLE|TASK_FREEZABLE);",
            "\tschedule_timeout(msecs_to_jiffies(khugepaged_alloc_sleep_millisecs));",
            "\tremove_wait_queue(&khugepaged_wait, &wait);",
            "}",
            "static bool hpage_collapse_scan_abort(int nid, struct collapse_control *cc)",
            "{",
            "\tint i;",
            "",
            "\t/*",
            "\t * If node_reclaim_mode is disabled, then no extra effort is made to",
            "\t * allocate memory locally.",
            "\t */",
            "\tif (!node_reclaim_enabled())",
            "\t\treturn false;",
            "",
            "\t/* If there is a count for this node already, it must be acceptable */",
            "\tif (cc->node_load[nid])",
            "\t\treturn false;",
            "",
            "\tfor (i = 0; i < MAX_NUMNODES; i++) {",
            "\t\tif (!cc->node_load[i])",
            "\t\t\tcontinue;",
            "\t\tif (node_distance(nid, i) > node_reclaim_distance)",
            "\t\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "static inline gfp_t alloc_hugepage_khugepaged_gfpmask(void)",
            "{",
            "\treturn khugepaged_defrag() ? GFP_TRANSHUGE : GFP_TRANSHUGE_LIGHT;",
            "}",
            "static int hpage_collapse_find_target_node(struct collapse_control *cc)",
            "{",
            "\tint nid, target_node = 0, max_value = 0;",
            "",
            "\t/* find first node with max normal pages hit */",
            "\tfor (nid = 0; nid < MAX_NUMNODES; nid++)",
            "\t\tif (cc->node_load[nid] > max_value) {",
            "\t\t\tmax_value = cc->node_load[nid];",
            "\t\t\ttarget_node = nid;",
            "\t\t}",
            "",
            "\tfor_each_online_node(nid) {",
            "\t\tif (max_value == cc->node_load[nid])",
            "\t\t\tnode_set(nid, cc->alloc_nmask);",
            "\t}",
            "",
            "\treturn target_node;",
            "}",
            "static int hpage_collapse_find_target_node(struct collapse_control *cc)",
            "{",
            "\treturn 0;",
            "}",
            "static int hugepage_vma_revalidate(struct mm_struct *mm, unsigned long address,",
            "\t\t\t\t   bool expect_anon,",
            "\t\t\t\t   struct vm_area_struct **vmap,",
            "\t\t\t\t   struct collapse_control *cc)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tunsigned long tva_flags = cc->is_khugepaged ? TVA_ENFORCE_SYSFS : 0;",
            "",
            "\tif (unlikely(hpage_collapse_test_exit_or_disable(mm)))",
            "\t\treturn SCAN_ANY_PROCESS;",
            "",
            "\t*vmap = vma = find_vma(mm, address);",
            "\tif (!vma)",
            "\t\treturn SCAN_VMA_NULL;",
            "",
            "\tif (!thp_vma_suitable_order(vma, address, PMD_ORDER))",
            "\t\treturn SCAN_ADDRESS_RANGE;",
            "\tif (!thp_vma_allowable_order(vma, vma->vm_flags, tva_flags, PMD_ORDER))",
            "\t\treturn SCAN_VMA_CHECK;",
            "\t/*",
            "\t * Anon VMA expected, the address may be unmapped then",
            "\t * remapped to file after khugepaged reaquired the mmap_lock.",
            "\t *",
            "\t * thp_vma_allowable_order may return true for qualified file",
            "\t * vmas.",
            "\t */",
            "\tif (expect_anon && (!(*vmap)->anon_vma || !vma_is_anonymous(*vmap)))",
            "\t\treturn SCAN_PAGE_ANON;",
            "\treturn SCAN_SUCCEED;",
            "}",
            "static int find_pmd_or_thp_or_none(struct mm_struct *mm,",
            "\t\t\t\t   unsigned long address,",
            "\t\t\t\t   pmd_t **pmd)",
            "{",
            "\tpmd_t pmde;",
            "",
            "\t*pmd = mm_find_pmd(mm, address);",
            "\tif (!*pmd)",
            "\t\treturn SCAN_PMD_NULL;",
            "",
            "\tpmde = pmdp_get_lockless(*pmd);",
            "\tif (pmd_none(pmde))",
            "\t\treturn SCAN_PMD_NONE;",
            "\tif (!pmd_present(pmde))",
            "\t\treturn SCAN_PMD_NULL;",
            "\tif (pmd_trans_huge(pmde))",
            "\t\treturn SCAN_PMD_MAPPED;",
            "\tif (pmd_devmap(pmde))",
            "\t\treturn SCAN_PMD_NULL;",
            "\tif (pmd_bad(pmde))",
            "\t\treturn SCAN_PMD_NULL;",
            "\treturn SCAN_SUCCEED;",
            "}"
          ],
          "function_name": "khugepaged_alloc_sleep, hpage_collapse_scan_abort, alloc_hugepage_khugepaged_gfpmask, hpage_collapse_find_target_node, hpage_collapse_find_target_node, hugepage_vma_revalidate, find_pmd_or_thp_or_none",
          "description": "管理khugepaged内存分配睡眠、节点负载检查、目标节点选择及VMA重新验证逻辑",
          "similarity": 0.5012385845184326
        },
        {
          "chunk_id": 9,
          "file_path": "mm/khugepaged.c",
          "start_line": 1263,
          "end_line": 1432,
          "content": [
            "static int hpage_collapse_scan_pmd(struct mm_struct *mm,",
            "\t\t\t\t   struct vm_area_struct *vma,",
            "\t\t\t\t   unsigned long address, bool *mmap_locked,",
            "\t\t\t\t   struct collapse_control *cc)",
            "{",
            "\tpmd_t *pmd;",
            "\tpte_t *pte, *_pte;",
            "\tint result = SCAN_FAIL, referenced = 0;",
            "\tint none_or_zero = 0, shared = 0;",
            "\tstruct page *page = NULL;",
            "\tstruct folio *folio = NULL;",
            "\tunsigned long _address;",
            "\tspinlock_t *ptl;",
            "\tint node = NUMA_NO_NODE, unmapped = 0;",
            "\tbool writable = false;",
            "",
            "\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);",
            "",
            "\tresult = find_pmd_or_thp_or_none(mm, address, &pmd);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out;",
            "",
            "\tmemset(cc->node_load, 0, sizeof(cc->node_load));",
            "\tnodes_clear(cc->alloc_nmask);",
            "\tpte = pte_offset_map_lock(mm, pmd, address, &ptl);",
            "\tif (!pte) {",
            "\t\tresult = SCAN_PMD_NULL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tfor (_address = address, _pte = pte; _pte < pte + HPAGE_PMD_NR;",
            "\t     _pte++, _address += PAGE_SIZE) {",
            "\t\tpte_t pteval = ptep_get(_pte);",
            "\t\tif (is_swap_pte(pteval)) {",
            "\t\t\t++unmapped;",
            "\t\t\tif (!cc->is_khugepaged ||",
            "\t\t\t    unmapped <= khugepaged_max_ptes_swap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * Always be strict with uffd-wp",
            "\t\t\t\t * enabled swap entries.  Please see",
            "\t\t\t\t * comment below for pte_uffd_wp().",
            "\t\t\t\t */",
            "\t\t\t\tif (pte_swp_uffd_wp_any(pteval)) {",
            "\t\t\t\t\tresult = SCAN_PTE_UFFD_WP;",
            "\t\t\t\t\tgoto out_unmap;",
            "\t\t\t\t}",
            "\t\t\t\tcontinue;",
            "\t\t\t} else {",
            "\t\t\t\tresult = SCAN_EXCEED_SWAP_PTE;",
            "\t\t\t\tcount_vm_event(THP_SCAN_EXCEED_SWAP_PTE);",
            "\t\t\t\tgoto out_unmap;",
            "\t\t\t}",
            "\t\t}",
            "\t\tif (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {",
            "\t\t\t++none_or_zero;",
            "\t\t\tif (!userfaultfd_armed(vma) &&",
            "\t\t\t    (!cc->is_khugepaged ||",
            "\t\t\t     none_or_zero <= khugepaged_max_ptes_none)) {",
            "\t\t\t\tcontinue;",
            "\t\t\t} else {",
            "\t\t\t\tresult = SCAN_EXCEED_NONE_PTE;",
            "\t\t\t\tcount_vm_event(THP_SCAN_EXCEED_NONE_PTE);",
            "\t\t\t\tgoto out_unmap;",
            "\t\t\t}",
            "\t\t}",
            "\t\tif (pte_uffd_wp(pteval)) {",
            "\t\t\t/*",
            "\t\t\t * Don't collapse the page if any of the small",
            "\t\t\t * PTEs are armed with uffd write protection.",
            "\t\t\t * Here we can also mark the new huge pmd as",
            "\t\t\t * write protected if any of the small ones is",
            "\t\t\t * marked but that could bring unknown",
            "\t\t\t * userfault messages that falls outside of",
            "\t\t\t * the registered range.  So, just be simple.",
            "\t\t\t */",
            "\t\t\tresult = SCAN_PTE_UFFD_WP;",
            "\t\t\tgoto out_unmap;",
            "\t\t}",
            "\t\tif (pte_write(pteval))",
            "\t\t\twritable = true;",
            "",
            "\t\tpage = vm_normal_page(vma, _address, pteval);",
            "\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {",
            "\t\t\tresult = SCAN_PAGE_NULL;",
            "\t\t\tgoto out_unmap;",
            "\t\t}",
            "",
            "\t\tif (page_mapcount(page) > 1) {",
            "\t\t\t++shared;",
            "\t\t\tif (cc->is_khugepaged &&",
            "\t\t\t    shared > khugepaged_max_ptes_shared) {",
            "\t\t\t\tresult = SCAN_EXCEED_SHARED_PTE;",
            "\t\t\t\tcount_vm_event(THP_SCAN_EXCEED_SHARED_PTE);",
            "\t\t\t\tgoto out_unmap;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tfolio = page_folio(page);",
            "\t\t/*",
            "\t\t * Record which node the original page is from and save this",
            "\t\t * information to cc->node_load[].",
            "\t\t * Khugepaged will allocate hugepage from the node has the max",
            "\t\t * hit record.",
            "\t\t */",
            "\t\tnode = folio_nid(folio);",
            "\t\tif (hpage_collapse_scan_abort(node, cc)) {",
            "\t\t\tresult = SCAN_SCAN_ABORT;",
            "\t\t\tgoto out_unmap;",
            "\t\t}",
            "\t\tcc->node_load[node]++;",
            "\t\tif (!folio_test_lru(folio)) {",
            "\t\t\tresult = SCAN_PAGE_LRU;",
            "\t\t\tgoto out_unmap;",
            "\t\t}",
            "\t\tif (folio_test_locked(folio)) {",
            "\t\t\tresult = SCAN_PAGE_LOCK;",
            "\t\t\tgoto out_unmap;",
            "\t\t}",
            "\t\tif (!folio_test_anon(folio)) {",
            "\t\t\tresult = SCAN_PAGE_ANON;",
            "\t\t\tgoto out_unmap;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Check if the page has any GUP (or other external) pins.",
            "\t\t *",
            "\t\t * Here the check may be racy:",
            "\t\t * it may see folio_mapcount() > folio_ref_count().",
            "\t\t * But such case is ephemeral we could always retry collapse",
            "\t\t * later.  However it may report false positive if the page",
            "\t\t * has excessive GUP pins (i.e. 512).  Anyway the same check",
            "\t\t * will be done again later the risk seems low.",
            "\t\t */",
            "\t\tif (!is_refcount_suitable(folio)) {",
            "\t\t\tresult = SCAN_PAGE_COUNT;",
            "\t\t\tgoto out_unmap;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If collapse was initiated by khugepaged, check that there is",
            "\t\t * enough young pte to justify collapsing the page",
            "\t\t */",
            "\t\tif (cc->is_khugepaged &&",
            "\t\t    (pte_young(pteval) || folio_test_young(folio) ||",
            "\t\t     folio_test_referenced(folio) || mmu_notifier_test_young(vma->vm_mm,",
            "\t\t\t\t\t\t\t\t     address)))",
            "\t\t\treferenced++;",
            "\t}",
            "\tif (!writable) {",
            "\t\tresult = SCAN_PAGE_RO;",
            "\t} else if (cc->is_khugepaged &&",
            "\t\t   (!referenced ||",
            "\t\t    (unmapped && referenced < HPAGE_PMD_NR / 2))) {",
            "\t\tresult = SCAN_LACK_REFERENCED_PAGE;",
            "\t} else {",
            "\t\tresult = SCAN_SUCCEED;",
            "\t}",
            "out_unmap:",
            "\tpte_unmap_unlock(pte, ptl);",
            "\tif (result == SCAN_SUCCEED) {",
            "\t\tresult = collapse_huge_page(mm, address, referenced,",
            "\t\t\t\t\t    unmapped, cc);",
            "\t\t/* collapse_huge_page will return with the mmap_lock released */",
            "\t\t*mmap_locked = false;",
            "\t}",
            "out:",
            "\ttrace_mm_khugepaged_scan_pmd(mm, &folio->page, writable, referenced,",
            "\t\t\t\t     none_or_zero, result, unmapped);",
            "\treturn result;",
            "}"
          ],
          "function_name": "hpage_collapse_scan_pmd",
          "description": "扫描PMD条目检查合并条件，统计页面属性并决定是否触发合并操作",
          "similarity": 0.4796285331249237
        },
        {
          "chunk_id": 8,
          "file_path": "mm/khugepaged.c",
          "start_line": 1101,
          "end_line": 1261,
          "content": [
            "static int collapse_huge_page(struct mm_struct *mm, unsigned long address,",
            "\t\t\t      int referenced, int unmapped,",
            "\t\t\t      struct collapse_control *cc)",
            "{",
            "\tLIST_HEAD(compound_pagelist);",
            "\tpmd_t *pmd, _pmd;",
            "\tpte_t *pte;",
            "\tpgtable_t pgtable;",
            "\tstruct folio *folio;",
            "\tspinlock_t *pmd_ptl, *pte_ptl;",
            "\tint result = SCAN_FAIL;",
            "\tstruct vm_area_struct *vma;",
            "\tstruct mmu_notifier_range range;",
            "",
            "\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);",
            "",
            "\t/*",
            "\t * Before allocating the hugepage, release the mmap_lock read lock.",
            "\t * The allocation can take potentially a long time if it involves",
            "\t * sync compaction, and we do not need to hold the mmap_lock during",
            "\t * that. We will recheck the vma after taking it again in write mode.",
            "\t */",
            "\tmmap_read_unlock(mm);",
            "",
            "\tresult = alloc_charge_folio(&folio, mm, cc);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out_nolock;",
            "",
            "\tmmap_read_lock(mm);",
            "\tresult = hugepage_vma_revalidate(mm, address, true, &vma, cc);",
            "\tif (result != SCAN_SUCCEED) {",
            "\t\tmmap_read_unlock(mm);",
            "\t\tgoto out_nolock;",
            "\t}",
            "",
            "\tresult = find_pmd_or_thp_or_none(mm, address, &pmd);",
            "\tif (result != SCAN_SUCCEED) {",
            "\t\tmmap_read_unlock(mm);",
            "\t\tgoto out_nolock;",
            "\t}",
            "",
            "\tif (unmapped) {",
            "\t\t/*",
            "\t\t * __collapse_huge_page_swapin will return with mmap_lock",
            "\t\t * released when it fails. So we jump out_nolock directly in",
            "\t\t * that case.  Continuing to collapse causes inconsistency.",
            "\t\t */",
            "\t\tresult = __collapse_huge_page_swapin(mm, vma, address, pmd,",
            "\t\t\t\t\t\t     referenced);",
            "\t\tif (result != SCAN_SUCCEED)",
            "\t\t\tgoto out_nolock;",
            "\t}",
            "",
            "\tmmap_read_unlock(mm);",
            "\t/*",
            "\t * Prevent all access to pagetables with the exception of",
            "\t * gup_fast later handled by the ptep_clear_flush and the VM",
            "\t * handled by the anon_vma lock + PG_lock.",
            "\t *",
            "\t * UFFDIO_MOVE is prevented to race as well thanks to the",
            "\t * mmap_lock.",
            "\t */",
            "\tmmap_write_lock(mm);",
            "\tresult = hugepage_vma_revalidate(mm, address, true, &vma, cc);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out_up_write;",
            "\t/* check if the pmd is still valid */",
            "\tresult = check_pmd_still_valid(mm, address, pmd);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out_up_write;",
            "",
            "\tvma_start_write(vma);",
            "\tanon_vma_lock_write(vma->anon_vma);",
            "",
            "\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, address,",
            "\t\t\t\taddress + HPAGE_PMD_SIZE);",
            "\tmmu_notifier_invalidate_range_start(&range);",
            "",
            "\tpmd_ptl = pmd_lock(mm, pmd); /* probably unnecessary */",
            "\t/*",
            "\t * This removes any huge TLB entry from the CPU so we won't allow",
            "\t * huge and small TLB entries for the same virtual address to",
            "\t * avoid the risk of CPU bugs in that area.",
            "\t *",
            "\t * Parallel GUP-fast is fine since GUP-fast will back off when",
            "\t * it detects PMD is changed.",
            "\t */",
            "\t_pmd = pmdp_collapse_flush(vma, address, pmd);",
            "\tspin_unlock(pmd_ptl);",
            "\tmmu_notifier_invalidate_range_end(&range);",
            "\ttlb_remove_table_sync_one();",
            "",
            "\tpte = pte_offset_map_lock(mm, &_pmd, address, &pte_ptl);",
            "\tif (pte) {",
            "\t\tresult = __collapse_huge_page_isolate(vma, address, pte, cc,",
            "\t\t\t\t\t\t      &compound_pagelist);",
            "\t\tspin_unlock(pte_ptl);",
            "\t} else {",
            "\t\tresult = SCAN_PMD_NULL;",
            "\t}",
            "",
            "\tif (unlikely(result != SCAN_SUCCEED)) {",
            "\t\tif (pte)",
            "\t\t\tpte_unmap(pte);",
            "\t\tspin_lock(pmd_ptl);",
            "\t\tBUG_ON(!pmd_none(*pmd));",
            "\t\t/*",
            "\t\t * We can only use set_pmd_at when establishing",
            "\t\t * hugepmds and never for establishing regular pmds that",
            "\t\t * points to regular pagetables. Use pmd_populate for that",
            "\t\t */",
            "\t\tpmd_populate(mm, pmd, pmd_pgtable(_pmd));",
            "\t\tspin_unlock(pmd_ptl);",
            "\t\tanon_vma_unlock_write(vma->anon_vma);",
            "\t\tgoto out_up_write;",
            "\t}",
            "",
            "\t/*",
            "\t * All pages are isolated and locked so anon_vma rmap",
            "\t * can't run anymore.",
            "\t */",
            "\tanon_vma_unlock_write(vma->anon_vma);",
            "",
            "\tresult = __collapse_huge_page_copy(pte, folio, pmd, _pmd,",
            "\t\t\t\t\t   vma, address, pte_ptl,",
            "\t\t\t\t\t   &compound_pagelist);",
            "\tpte_unmap(pte);",
            "\tif (unlikely(result != SCAN_SUCCEED))",
            "\t\tgoto out_up_write;",
            "",
            "\t/*",
            "\t * The smp_wmb() inside __folio_mark_uptodate() ensures the",
            "\t * copy_huge_page writes become visible before the set_pmd_at()",
            "\t * write.",
            "\t */",
            "\t__folio_mark_uptodate(folio);",
            "\tpgtable = pmd_pgtable(_pmd);",
            "",
            "\t_pmd = mk_huge_pmd(&folio->page, vma->vm_page_prot);",
            "\t_pmd = maybe_pmd_mkwrite(pmd_mkdirty(_pmd), vma);",
            "",
            "\tspin_lock(pmd_ptl);",
            "\tBUG_ON(!pmd_none(*pmd));",
            "\tfolio_add_new_anon_rmap(folio, vma, address, RMAP_EXCLUSIVE);",
            "\tfolio_add_lru_vma(folio, vma);",
            "\tpgtable_trans_huge_deposit(mm, pmd, pgtable);",
            "\tset_pmd_at(mm, address, pmd, _pmd);",
            "\tupdate_mmu_cache_pmd(vma, address, pmd);",
            "\tspin_unlock(pmd_ptl);",
            "",
            "\tfolio = NULL;",
            "",
            "\tresult = SCAN_SUCCEED;",
            "out_up_write:",
            "\tmmap_write_unlock(mm);",
            "out_nolock:",
            "\tif (folio)",
            "\t\tfolio_put(folio);",
            "\ttrace_mm_collapse_huge_page(mm, result == SCAN_SUCCEED, result);",
            "\treturn result;",
            "}"
          ],
          "function_name": "collapse_huge_page",
          "description": "执行大页合并主流程，包括页表隔离、数据复制及新PMD设置",
          "similarity": 0.4693776071071625
        },
        {
          "chunk_id": 7,
          "file_path": "mm/khugepaged.c",
          "start_line": 980,
          "end_line": 1090,
          "content": [
            "static int check_pmd_still_valid(struct mm_struct *mm,",
            "\t\t\t\t unsigned long address,",
            "\t\t\t\t pmd_t *pmd)",
            "{",
            "\tpmd_t *new_pmd;",
            "\tint result = find_pmd_or_thp_or_none(mm, address, &new_pmd);",
            "",
            "\tif (result != SCAN_SUCCEED)",
            "\t\treturn result;",
            "\tif (new_pmd != pmd)",
            "\t\treturn SCAN_FAIL;",
            "\treturn SCAN_SUCCEED;",
            "}",
            "static int __collapse_huge_page_swapin(struct mm_struct *mm,",
            "\t\t\t\t       struct vm_area_struct *vma,",
            "\t\t\t\t       unsigned long haddr, pmd_t *pmd,",
            "\t\t\t\t       int referenced)",
            "{",
            "\tint swapped_in = 0;",
            "\tvm_fault_t ret = 0;",
            "\tunsigned long address, end = haddr + (HPAGE_PMD_NR * PAGE_SIZE);",
            "\tint result;",
            "\tpte_t *pte = NULL;",
            "\tspinlock_t *ptl;",
            "",
            "\tfor (address = haddr; address < end; address += PAGE_SIZE) {",
            "\t\tstruct vm_fault vmf = {",
            "\t\t\t.vma = vma,",
            "\t\t\t.address = address,",
            "\t\t\t.pgoff = linear_page_index(vma, address),",
            "\t\t\t.flags = FAULT_FLAG_ALLOW_RETRY,",
            "\t\t\t.pmd = pmd,",
            "\t\t};",
            "",
            "\t\tif (!pte++) {",
            "\t\t\tpte = pte_offset_map_nolock(mm, pmd, address, &ptl);",
            "\t\t\tif (!pte) {",
            "\t\t\t\tmmap_read_unlock(mm);",
            "\t\t\t\tresult = SCAN_PMD_NULL;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tvmf.orig_pte = ptep_get_lockless(pte);",
            "\t\tif (!is_swap_pte(vmf.orig_pte))",
            "\t\t\tcontinue;",
            "",
            "\t\tvmf.pte = pte;",
            "\t\tvmf.ptl = ptl;",
            "\t\tret = do_swap_page(&vmf);",
            "\t\t/* Which unmaps pte (after perhaps re-checking the entry) */",
            "\t\tpte = NULL;",
            "",
            "\t\t/*",
            "\t\t * do_swap_page returns VM_FAULT_RETRY with released mmap_lock.",
            "\t\t * Note we treat VM_FAULT_RETRY as VM_FAULT_ERROR here because",
            "\t\t * we do not retry here and swap entry will remain in pagetable",
            "\t\t * resulting in later failure.",
            "\t\t */",
            "\t\tif (ret & VM_FAULT_RETRY) {",
            "\t\t\t/* Likely, but not guaranteed, that page lock failed */",
            "\t\t\tresult = SCAN_PAGE_LOCK;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tif (ret & VM_FAULT_ERROR) {",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\tresult = SCAN_FAIL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tswapped_in++;",
            "\t}",
            "",
            "\tif (pte)",
            "\t\tpte_unmap(pte);",
            "",
            "\t/* Drain LRU cache to remove extra pin on the swapped in pages */",
            "\tif (swapped_in)",
            "\t\tlru_add_drain();",
            "",
            "\tresult = SCAN_SUCCEED;",
            "out:",
            "\ttrace_mm_collapse_huge_page_swapin(mm, swapped_in, referenced, result);",
            "\treturn result;",
            "}",
            "static int alloc_charge_folio(struct folio **foliop, struct mm_struct *mm,",
            "\t\t\t      struct collapse_control *cc)",
            "{",
            "\tgfp_t gfp = (cc->is_khugepaged ? alloc_hugepage_khugepaged_gfpmask() :",
            "\t\t     GFP_TRANSHUGE);",
            "\tint node = hpage_collapse_find_target_node(cc);",
            "\tstruct folio *folio;",
            "",
            "\tfolio = __folio_alloc(gfp, HPAGE_PMD_ORDER, node, &cc->alloc_nmask);",
            "\tif (!folio) {",
            "\t\t*foliop = NULL;",
            "\t\tcount_vm_event(THP_COLLAPSE_ALLOC_FAILED);",
            "\t\treturn SCAN_ALLOC_HUGE_PAGE_FAIL;",
            "\t}",
            "",
            "\tcount_vm_event(THP_COLLAPSE_ALLOC);",
            "\tif (unlikely(mem_cgroup_charge(folio, mm, gfp))) {",
            "\t\tfolio_put(folio);",
            "\t\t*foliop = NULL;",
            "\t\treturn SCAN_CGROUP_CHARGE_FAIL;",
            "\t}",
            "",
            "\tcount_memcg_folio_events(folio, THP_COLLAPSE_ALLOC, 1);",
            "",
            "\t*foliop = folio;",
            "\treturn SCAN_SUCCEED;",
            "}"
          ],
          "function_name": "check_pmd_still_valid, __collapse_huge_page_swapin, alloc_charge_folio",
          "description": "验证PMD有效性、交换页加载及大页分配的辅助函数",
          "similarity": 0.4677780568599701
        },
        {
          "chunk_id": 13,
          "file_path": "mm/khugepaged.c",
          "start_line": 2485,
          "end_line": 2587,
          "content": [
            "static int khugepaged_has_work(void)",
            "{",
            "\treturn !list_empty(&khugepaged_scan.mm_head) && hugepage_pmd_enabled();",
            "}",
            "static int khugepaged_wait_event(void)",
            "{",
            "\treturn !list_empty(&khugepaged_scan.mm_head) ||",
            "\t\tkthread_should_stop();",
            "}",
            "static void khugepaged_do_scan(struct collapse_control *cc)",
            "{",
            "\tunsigned int progress = 0, pass_through_head = 0;",
            "\tunsigned int pages = READ_ONCE(khugepaged_pages_to_scan);",
            "\tbool wait = true;",
            "\tint result = SCAN_SUCCEED;",
            "",
            "\tlru_add_drain_all();",
            "",
            "\twhile (true) {",
            "\t\tcond_resched();",
            "",
            "\t\tif (unlikely(kthread_should_stop() || try_to_freeze()))",
            "\t\t\tbreak;",
            "",
            "\t\tspin_lock(&khugepaged_mm_lock);",
            "\t\tif (!khugepaged_scan.mm_slot)",
            "\t\t\tpass_through_head++;",
            "\t\tif (khugepaged_has_work() &&",
            "\t\t    pass_through_head < 2)",
            "\t\t\tprogress += khugepaged_scan_mm_slot(pages - progress,",
            "\t\t\t\t\t\t\t    &result, cc);",
            "\t\telse",
            "\t\t\tprogress = pages;",
            "\t\tspin_unlock(&khugepaged_mm_lock);",
            "",
            "\t\tif (progress >= pages)",
            "\t\t\tbreak;",
            "",
            "\t\tif (result == SCAN_ALLOC_HUGE_PAGE_FAIL) {",
            "\t\t\t/*",
            "\t\t\t * If fail to allocate the first time, try to sleep for",
            "\t\t\t * a while.  When hit again, cancel the scan.",
            "\t\t\t */",
            "\t\t\tif (!wait)",
            "\t\t\t\tbreak;",
            "\t\t\twait = false;",
            "\t\t\tkhugepaged_alloc_sleep();",
            "\t\t}",
            "\t}",
            "}",
            "static bool khugepaged_should_wakeup(void)",
            "{",
            "\treturn kthread_should_stop() ||",
            "\t       time_after_eq(jiffies, khugepaged_sleep_expire);",
            "}",
            "static void khugepaged_wait_work(void)",
            "{",
            "\tif (khugepaged_has_work()) {",
            "\t\tconst unsigned long scan_sleep_jiffies =",
            "\t\t\tmsecs_to_jiffies(khugepaged_scan_sleep_millisecs);",
            "",
            "\t\tif (!scan_sleep_jiffies)",
            "\t\t\treturn;",
            "",
            "\t\tkhugepaged_sleep_expire = jiffies + scan_sleep_jiffies;",
            "\t\twait_event_freezable_timeout(khugepaged_wait,",
            "\t\t\t\t\t     khugepaged_should_wakeup(),",
            "\t\t\t\t\t     scan_sleep_jiffies);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (hugepage_pmd_enabled())",
            "\t\twait_event_freezable(khugepaged_wait, khugepaged_wait_event());",
            "}",
            "static int khugepaged_threshold_suit(void)",
            "{",
            "\t/* khugepaged_adopt feature enable? */",
            "\tif (!khugepaged_adapt_enable)",
            "\t\treturn false;",
            "\t/* PAGE & THP size threshold check */",
            "\tif (PAGE_SIZE == SZ_64K && (PAGE_SIZE << HPAGE_PMD_ORDER) >= SZ_512M)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "static void khugepaged_update_wmarks(void)",
            "{",
            "\tif (!khugepaged_threshold_suit())",
            "\t\treturn;",
            "",
            "\t/* __khugepaged_enter push khugepaged to work, raise watermark */",
            "\tif (khugepaged_has_work()) {",
            "\t\t/* Once set, do not repeat distrub watermark */",
            "\t\tif (!khugepaged_thp_scan_state) {",
            "\t\t\tkhugepaged_thp_scan_state = true;",
            "\t\t\tmutex_lock(&khugepaged_mutex);",
            "\t\t\tset_recommended_min_free_kbytes();",
            "\t\t\tmutex_unlock(&khugepaged_mutex);",
            "\t\t}",
            "\t} else {",
            "\t\tkhugepaged_thp_scan_state = false;",
            "\t}",
            "}"
          ],
          "function_name": "khugepaged_has_work, khugepaged_wait_event, khugepaged_do_scan, khugepaged_should_wakeup, khugepaged_wait_work, khugepaged_threshold_suit, khugepaged_update_wmarks",
          "description": "khugepaged_has_work 判断是否存在待处理的大页合并任务。khugepaged_wait_event 监控任务结束或线程终止信号。khugeped_do_scan 执行大页合并扫描，通过循环处理多个 mm 槽位，根据资源分配情况决定是否继续扫描。khugepaged_threshold_suit 判断是否启用动态阈值调整，khugepaged_update_wmarks 根据需求更新内存水印参数。",
          "similarity": 0.4659498333930969
        }
      ]
    }
  ]
}