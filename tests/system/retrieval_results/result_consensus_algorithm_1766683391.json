{
  "query": "consensus algorithm",
  "timestamp": "2025-12-26 01:23:11",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.4999392032623291,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/mempolicy.c",
          "start_line": 168,
          "end_line": 268,
          "content": [
            "static u8 get_il_weight(int node)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tu8 weight = 1;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state)",
            "\t\tweight = state->iw_table[node];",
            "\trcu_read_unlock();",
            "\treturn weight;",
            "}",
            "static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)",
            "{",
            "\tu64 sum_bw = 0;",
            "\tunsigned int cast_sum_bw, scaling_factor = 1, iw_gcd = 0;",
            "\tint nid;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tsum_bw += bw[nid];",
            "",
            "\t/* Scale bandwidths to whole numbers in the range [1, weightiness] */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t/*",
            "\t\t * Try not to perform 64-bit division.",
            "\t\t * If sum_bw < scaling_factor, then sum_bw < U32_MAX.",
            "\t\t * If sum_bw > scaling_factor, then round the weight up to 1.",
            "\t\t */",
            "\t\tscaling_factor = weightiness * bw[nid];",
            "\t\tif (bw[nid] && sum_bw < scaling_factor) {",
            "\t\t\tcast_sum_bw = (unsigned int)sum_bw;",
            "\t\t\tnew_iw[nid] = scaling_factor / cast_sum_bw;",
            "\t\t} else {",
            "\t\t\tnew_iw[nid] = 1;",
            "\t\t}",
            "\t\tif (!iw_gcd)",
            "\t\t\tiw_gcd = new_iw[nid];",
            "\t\tiw_gcd = gcd(iw_gcd, new_iw[nid]);",
            "\t}",
            "",
            "\t/* 1:2 is strictly better than 16:32. Reduce by the weights' GCD. */",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tnew_iw[nid] /= iw_gcd;",
            "}",
            "int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tunsigned int *old_bw, *new_bw;",
            "\tunsigned int bw_val;",
            "\tint i;",
            "",
            "\tbw_val = min(coords->read_bandwidth, coords->write_bandwidth);",
            "\tnew_bw = kcalloc(nr_node_ids, sizeof(unsigned int), GFP_KERNEL);",
            "\tif (!new_bw)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew_wi_state = kmalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state) {",
            "\t\tkfree(new_bw);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tnew_wi_state->mode_auto = true;",
            "\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\tnew_wi_state->iw_table[i] = 1;",
            "",
            "\t/*",
            "\t * Update bandwidth info, even in manual mode. That way, when switching",
            "\t * to auto mode in the future, iw_table can be overwritten using",
            "\t * accurate bw data.",
            "\t */",
            "\tmutex_lock(&wi_state_lock);",
            "",
            "\told_bw = node_bw_table;",
            "\tif (old_bw)",
            "\t\tmemcpy(new_bw, old_bw, nr_node_ids * sizeof(*old_bw));",
            "\tnew_bw[node] = bw_val;",
            "\tnode_bw_table = new_bw;",
            "",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state && !old_wi_state->mode_auto) {",
            "\t\t/* Manual mode; skip reducing weights and updating wi_state */",
            "\t\tmutex_unlock(&wi_state_lock);",
            "\t\tkfree(new_wi_state);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* NULL wi_state assumes auto=true; reduce weights and update wi_state*/",
            "\treduce_interleave_weights(new_bw, new_wi_state->iw_table);",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "out:",
            "\tkfree(old_bw);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_il_weight, reduce_interleave_weights, mempolicy_set_node_perf",
          "description": "实现带权交错策略的权重计算与调整逻辑，通过获取节点带宽数据动态修改权重比例，支持根据性能参数更新节点间内存分配优先级。",
          "similarity": 0.5025778412818909
        },
        {
          "chunk_id": 13,
          "file_path": "mm/mempolicy.c",
          "start_line": 2149,
          "end_line": 2255,
          "content": [
            "static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nnodes;",
            "\tint i;",
            "\tint nid;",
            "",
            "\tnnodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nnodes)",
            "\t\treturn numa_node_id();",
            "\ttarget = ilx % nnodes;",
            "\tnid = first_node(nodemask);",
            "\tfor (i = 0; i < target; i++)",
            "\t\tnid = next_node(nid, nodemask);",
            "\treturn nid;",
            "}",
            "int huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,",
            "\t\tstruct mempolicy **mpol, nodemask_t **nodemask)",
            "{",
            "\tpgoff_t ilx;",
            "\tint nid;",
            "",
            "\tnid = numa_node_id();",
            "\t*mpol = get_vma_policy(vma, addr, hstate_vma(vma)->order, &ilx);",
            "\t*nodemask = policy_nodemask(gfp_flags, *mpol, ilx, &nid);",
            "\treturn nid;",
            "}",
            "bool init_nodemask_of_mempolicy(nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "",
            "\tif (!(mask && current->mempolicy))",
            "\t\treturn false;",
            "",
            "\ttask_lock(current);",
            "\tmempolicy = current->mempolicy;",
            "\tswitch (mempolicy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*mask = mempolicy->nodes;",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tinit_nodemask_of_node(mask, numa_node_id());",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "\ttask_unlock(current);",
            "",
            "\treturn true;",
            "}",
            "bool mempolicy_in_oom_domain(struct task_struct *tsk,",
            "\t\t\t\t\tconst nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "\tbool ret = true;",
            "",
            "\tif (!mask)",
            "\t\treturn ret;",
            "",
            "\ttask_lock(tsk);",
            "\tmempolicy = tsk->mempolicy;",
            "\tif (mempolicy && mempolicy->mode == MPOL_BIND)",
            "\t\tret = nodes_intersects(mempolicy->nodes, *mask);",
            "\ttask_unlock(tsk);",
            "",
            "\treturn ret;",
            "}",
            "static unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tint nodes;",
            "\tunsigned long nr_pages_per_node;",
            "\tint delta;",
            "\tint i;",
            "\tunsigned long nr_allocated;",
            "\tunsigned long total_allocated = 0;",
            "",
            "\tnodes = nodes_weight(pol->nodes);",
            "\tnr_pages_per_node = nr_pages / nodes;",
            "\tdelta = nr_pages - nodes * nr_pages_per_node;",
            "",
            "\tfor (i = 0; i < nodes; i++) {",
            "\t\tif (delta) {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node + 1, NULL,",
            "\t\t\t\t\tpage_array);",
            "\t\t\tdelta--;",
            "\t\t} else {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node, NULL, page_array);",
            "\t\t}",
            "",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t}",
            "",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "interleave_nid, huge_node, init_nodemask_of_mempolicy, mempolicy_in_oom_domain, alloc_pages_bulk_array_interleave",
          "description": "interleave_nid 计算简单交错分配的目标节点；huge_node 结合HugeTLB策略确定大页分配节点；init_nodemask_of_mempolicy 初始化当前进程的内存策略节点掩码；mempolicy_in_oom_domain 检查策略节点是否与OOM域重叠；alloc_pages_bulk_array_interleave 执行批量交错分配。",
          "similarity": 0.4951159954071045
        },
        {
          "chunk_id": 12,
          "file_path": "mm/mempolicy.c",
          "start_line": 2024,
          "end_line": 2135,
          "content": [
            "static unsigned int interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int nid;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "\t/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnid = next_node_in(current->il_prev, policy->nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\tif (nid < MAX_NUMNODES)",
            "\t\tcurrent->il_prev = nid;",
            "\treturn nid;",
            "}",
            "unsigned int mempolicy_slab_node(void)",
            "{",
            "\tstruct mempolicy *policy;",
            "\tint node = numa_mem_id();",
            "",
            "\tif (!in_task())",
            "\t\treturn node;",
            "",
            "\tpolicy = current->mempolicy;",
            "\tif (!policy)",
            "\t\treturn node;",
            "",
            "\tswitch (policy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\treturn first_node(policy->nodes);",
            "",
            "\tcase MPOL_INTERLEAVE:",
            "\t\treturn interleave_nodes(policy);",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn weighted_interleave_nodes(policy);",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t{",
            "\t\tstruct zoneref *z;",
            "",
            "\t\t/*",
            "\t\t * Follow bind policy behavior and start allocation at the",
            "\t\t * first node.",
            "\t\t */",
            "\t\tstruct zonelist *zonelist;",
            "\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);",
            "\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];",
            "\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,",
            "\t\t\t\t\t\t\t&policy->nodes);",
            "\t\treturn z->zone ? zone_to_nid(z->zone) : node;",
            "\t}",
            "\tcase MPOL_LOCAL:",
            "\t\treturn node;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static unsigned int read_once_policy_nodemask(struct mempolicy *pol,",
            "\t\t\t\t\t      nodemask_t *mask)",
            "{",
            "\t/*",
            "\t * barrier stabilizes the nodemask locally so that it can be iterated",
            "\t * over safely without concern for changes. Allocators validate node",
            "\t * selection does not violate mems_allowed, so this is safe.",
            "\t */",
            "\tbarrier();",
            "\tmemcpy(mask, &pol->nodes, sizeof(nodemask_t));",
            "\tbarrier();",
            "\treturn nodes_weight(*mask);",
            "}",
            "static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nr_nodes;",
            "\tu8 *table = NULL;",
            "\tunsigned int weight_total = 0;",
            "\tu8 weight;",
            "\tint nid = 0;",
            "",
            "\tnr_nodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nr_nodes)",
            "\t\treturn numa_node_id();",
            "",
            "\trcu_read_lock();",
            "",
            "\tstate = rcu_dereference(wi_state);",
            "\t/* Uninitialized wi_state means we should assume all weights are 1 */",
            "\tif (state)",
            "\t\ttable = state->iw_table;",
            "",
            "\t/* calculate the total weight */",
            "\tfor_each_node_mask(nid, nodemask)",
            "\t\tweight_total += table ? table[nid] : 1;",
            "",
            "\t/* Calculate the node offset based on totals */",
            "\ttarget = ilx % weight_total;",
            "\tnid = first_node(nodemask);",
            "\twhile (target) {",
            "\t\t/* detect system default usage */",
            "\t\tweight = table ? table[nid] : 1;",
            "\t\tif (target < weight)",
            "\t\t\tbreak;",
            "\t\ttarget -= weight;",
            "\t\tnid = next_node_in(nid, nodemask);",
            "\t}",
            "\trcu_read_unlock();",
            "\treturn nid;",
            "}"
          ],
          "function_name": "interleave_nodes, mempolicy_slab_node, read_once_policy_nodemask, weighted_interleave_nid",
          "description": "interleave_nodes 计算交错分配的下一个节点；mempolicy_slab_node 根据内存策略返回Slab分配的节点；read_once_policy_nodemask 安全读取策略节点掩码；weighted_interleave_nid 基于权重计算加权交错分配的目标节点。",
          "similarity": 0.4912603497505188
        },
        {
          "chunk_id": 14,
          "file_path": "mm/mempolicy.c",
          "start_line": 2513,
          "end_line": 2629,
          "content": [
            "static unsigned long alloc_pages_bulk_array_weighted_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tstruct task_struct *me = current;",
            "\tunsigned int cpuset_mems_cookie;",
            "\tunsigned long total_allocated = 0;",
            "\tunsigned long nr_allocated = 0;",
            "\tunsigned long rounds;",
            "\tunsigned long node_pages, delta;",
            "\tu8 *weights, weight;",
            "\tunsigned int weight_total = 0;",
            "\tunsigned long rem_pages = nr_pages;",
            "\tnodemask_t nodes;",
            "\tint nnodes, node;",
            "\tint resume_node = MAX_NUMNODES - 1;",
            "\tu8 resume_weight = 0;",
            "\tint prev_node;",
            "\tint i;",
            "",
            "\tif (!nr_pages)",
            "\t\treturn 0;",
            "",
            "\t/* read the nodes onto the stack, retry if done during rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnnodes = read_once_policy_nodemask(pol, &nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\t/* if the nodemask has become invalid, we cannot do anything */",
            "\tif (!nnodes)",
            "\t\treturn 0;",
            "",
            "\t/* Continue allocating from most recent node and adjust the nr_pages */",
            "\tnode = me->il_prev;",
            "\tweight = me->il_weight;",
            "\tif (weight && node_isset(node, nodes)) {",
            "\t\tnode_pages = min(rem_pages, weight);",
            "\t\tnr_allocated = __alloc_pages_bulk(gfp, node, NULL, node_pages,",
            "\t\t\t\t\t\t  NULL, page_array);",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t\t/* if that's all the pages, no need to interleave */",
            "\t\tif (rem_pages <= weight) {",
            "\t\t\tme->il_weight -= rem_pages;",
            "\t\t\treturn total_allocated;",
            "\t\t}",
            "\t\t/* Otherwise we adjust remaining pages, continue from there */",
            "\t\trem_pages -= weight;",
            "\t}",
            "\t/* clear active weight in case of an allocation failure */",
            "\tme->il_weight = 0;",
            "\tprev_node = node;",
            "",
            "\t/* create a local copy of node weights to operate on outside rcu */",
            "\tweights = kzalloc(nr_node_ids, GFP_KERNEL);",
            "\tif (!weights)",
            "\t\treturn total_allocated;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state) {",
            "\t\tmemcpy(weights, state->iw_table, nr_node_ids * sizeof(u8));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\trcu_read_unlock();",
            "\t\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\t\tweights[i] = 1;",
            "\t}",
            "",
            "\t/* calculate total, detect system default usage */",
            "\tfor_each_node_mask(node, nodes)",
            "\t\tweight_total += weights[node];",
            "",
            "\t/*",
            "\t * Calculate rounds/partial rounds to minimize __alloc_pages_bulk calls.",
            "\t * Track which node weighted interleave should resume from.",
            "\t *",
            "\t * if (rounds > 0) and (delta == 0), resume_node will always be",
            "\t * the node following prev_node and its weight.",
            "\t */",
            "\trounds = rem_pages / weight_total;",
            "\tdelta = rem_pages % weight_total;",
            "\tresume_node = next_node_in(prev_node, nodes);",
            "\tresume_weight = weights[resume_node];",
            "\tfor (i = 0; i < nnodes; i++) {",
            "\t\tnode = next_node_in(prev_node, nodes);",
            "\t\tweight = weights[node];",
            "\t\tnode_pages = weight * rounds;",
            "\t\t/* If a delta exists, add this node's portion of the delta */",
            "\t\tif (delta > weight) {",
            "\t\t\tnode_pages += weight;",
            "\t\t\tdelta -= weight;",
            "\t\t} else if (delta) {",
            "\t\t\t/* when delta is depleted, resume from that node */",
            "\t\t\tnode_pages += delta;",
            "\t\t\tresume_node = node;",
            "\t\t\tresume_weight = weight - delta;",
            "\t\t\tdelta = 0;",
            "\t\t}",
            "\t\t/* node_pages can be 0 if an allocation fails and rounds == 0 */",
            "\t\tif (!node_pages)",
            "\t\t\tbreak;",
            "\t\tnr_allocated = __alloc_pages_bulk(gfp, node, NULL, node_pages,",
            "\t\t\t\t\t\t  NULL, page_array);",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t\tif (total_allocated == nr_pages)",
            "\t\t\tbreak;",
            "\t\tprev_node = node;",
            "\t}",
            "\tme->il_prev = resume_node;",
            "\tme->il_weight = resume_weight;",
            "\tkfree(weights);",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_weighted_interleave",
          "description": "alloc_pages_bulk_array_weighted_interleave 实现加权交错批量页面分配，基于策略权重计算各节点分配数量，优先使用最近使用的节点权重，处理剩余页面的分配逻辑。",
          "similarity": 0.48954641819000244
        },
        {
          "chunk_id": 3,
          "file_path": "mm/mempolicy.c",
          "start_line": 484,
          "end_line": 639,
          "content": [
            "static void mpol_rebind_preferred(struct mempolicy *pol,",
            "\t\t\t\t\t\tconst nodemask_t *nodes)",
            "{",
            "\tpol->w.cpuset_mems_allowed = *nodes;",
            "}",
            "static void mpol_rebind_policy(struct mempolicy *pol, const nodemask_t *newmask)",
            "{",
            "\tif (!pol || pol->mode == MPOL_LOCAL)",
            "\t\treturn;",
            "\tif (!mpol_store_user_nodemask(pol) &&",
            "\t    nodes_equal(pol->w.cpuset_mems_allowed, *newmask))",
            "\t\treturn;",
            "",
            "\tmpol_ops[pol->mode].rebind(pol, newmask);",
            "}",
            "void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new)",
            "{",
            "\tmpol_rebind_policy(tsk->mempolicy, new);",
            "}",
            "void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "",
            "\tmmap_write_lock(mm);",
            "\tfor_each_vma(vmi, vma) {",
            "\t\tvma_start_write(vma);",
            "\t\tmpol_rebind_policy(vma->vm_policy, new);",
            "\t}",
            "\tmmap_write_unlock(mm);",
            "}",
            "static bool strictly_unmovable(unsigned long flags)",
            "{",
            "\t/*",
            "\t * STRICT without MOVE flags lets do_mbind() fail immediately with -EIO",
            "\t * if any misplaced page is found.",
            "\t */",
            "\treturn (flags & (MPOL_MF_STRICT | MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ==",
            "\t\t\t MPOL_MF_STRICT;",
            "}",
            "static inline bool queue_folio_required(struct folio *folio,",
            "\t\t\t\t\tstruct queue_pages *qp)",
            "{",
            "\tint nid = folio_nid(folio);",
            "\tunsigned long flags = qp->flags;",
            "",
            "\treturn node_isset(nid, *qp->nmask) == !(flags & MPOL_MF_INVERT);",
            "}",
            "static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)",
            "{",
            "\tstruct folio *folio;",
            "\tstruct queue_pages *qp = walk->private;",
            "",
            "\tif (unlikely(is_pmd_migration_entry(*pmd))) {",
            "\t\tqp->nr_failed++;",
            "\t\treturn;",
            "\t}",
            "\tfolio = pmd_folio(*pmd);",
            "\tif (is_huge_zero_folio(folio)) {",
            "\t\twalk->action = ACTION_CONTINUE;",
            "\t\treturn;",
            "\t}",
            "\tif (!queue_folio_required(folio, qp))",
            "\t\treturn;",
            "\tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||",
            "\t    !vma_migratable(walk->vma) ||",
            "\t    !migrate_folio_add(folio, qp->pagelist, qp->flags))",
            "\t\tqp->nr_failed++;",
            "}",
            "static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,",
            "\t\t\tunsigned long end, struct mm_walk *walk)",
            "{",
            "\tconst fpb_t fpb_flags = FPB_IGNORE_DIRTY | FPB_IGNORE_SOFT_DIRTY;",
            "\tstruct vm_area_struct *vma = walk->vma;",
            "\tstruct folio *folio;",
            "\tstruct queue_pages *qp = walk->private;",
            "\tunsigned long flags = qp->flags;",
            "\tpte_t *pte, *mapped_pte;",
            "\tpte_t ptent;",
            "\tspinlock_t *ptl;",
            "\tint max_nr, nr;",
            "",
            "\tptl = pmd_trans_huge_lock(pmd, vma);",
            "\tif (ptl) {",
            "\t\tqueue_folios_pmd(pmd, walk);",
            "\t\tspin_unlock(ptl);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tmapped_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);",
            "\tif (!pte) {",
            "\t\twalk->action = ACTION_AGAIN;",
            "\t\treturn 0;",
            "\t}",
            "\tfor (; addr != end; pte += nr, addr += nr * PAGE_SIZE) {",
            "\t\tmax_nr = (end - addr) >> PAGE_SHIFT;",
            "\t\tnr = 1;",
            "\t\tptent = ptep_get(pte);",
            "\t\tif (pte_none(ptent))",
            "\t\t\tcontinue;",
            "\t\tif (!pte_present(ptent)) {",
            "\t\t\tif (is_migration_entry(pte_to_swp_entry(ptent)))",
            "\t\t\t\tqp->nr_failed++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfolio = vm_normal_folio(vma, addr, ptent);",
            "\t\tif (!folio || folio_is_zone_device(folio))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_large(folio) && max_nr != 1)",
            "\t\t\tnr = folio_pte_batch(folio, addr, pte, ptent,",
            "\t\t\t\t\t     max_nr, fpb_flags,",
            "\t\t\t\t\t     NULL, NULL, NULL);",
            "\t\t/*",
            "\t\t * vm_normal_folio() filters out zero pages, but there might",
            "\t\t * still be reserved folios to skip, perhaps in a VDSO.",
            "\t\t */",
            "\t\tif (folio_test_reserved(folio))",
            "\t\t\tcontinue;",
            "\t\tif (!queue_folio_required(folio, qp))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_large(folio)) {",
            "\t\t\t/*",
            "\t\t\t * A large folio can only be isolated from LRU once,",
            "\t\t\t * but may be mapped by many PTEs (and Copy-On-Write may",
            "\t\t\t * intersperse PTEs of other, order 0, folios).  This is",
            "\t\t\t * a common case, so don't mistake it for failure (but",
            "\t\t\t * there can be other cases of multi-mapped pages which",
            "\t\t\t * this quick check does not help to filter out - and a",
            "\t\t\t * search of the pagelist might grow to be prohibitive).",
            "\t\t\t *",
            "\t\t\t * migrate_pages(&pagelist) returns nr_failed folios, so",
            "\t\t\t * check \"large\" now so that queue_pages_range() returns",
            "\t\t\t * a comparable nr_failed folios.  This does imply that",
            "\t\t\t * if folio could not be isolated for some racy reason",
            "\t\t\t * at its first PTE, later PTEs will not give it another",
            "\t\t\t * chance of isolation; but keeps the accounting simple.",
            "\t\t\t */",
            "\t\t\tif (folio == qp->large)",
            "\t\t\t\tcontinue;",
            "\t\t\tqp->large = folio;",
            "\t\t}",
            "\t\tif (!(flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||",
            "\t\t    !vma_migratable(vma) ||",
            "\t\t    !migrate_folio_add(folio, qp->pagelist, flags)) {",
            "\t\t\tqp->nr_failed += nr;",
            "\t\t\tif (strictly_unmovable(flags))",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tpte_unmap_unlock(mapped_pte, ptl);",
            "\tcond_resched();",
            "out:",
            "\tif (qp->nr_failed && strictly_unmovable(flags))",
            "\t\treturn -EIO;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "mpol_rebind_preferred, mpol_rebind_policy, mpol_rebind_task, mpol_rebind_mm, strictly_unmovable, queue_folio_required, queue_folios_pmd, queue_folios_pte_range",
          "description": "处理页表项遍历与迁移操作，包含页帧队列检测、迁移标志验证及大页迁移逻辑，确保内存分配符合NUMA策略要求。",
          "similarity": 0.47901469469070435
        }
      ]
    },
    {
      "source_file": "kernel/cgroup/rstat.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:51:37\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `cgroup\\rstat.c`\n\n---\n\n# cgroup/rstat.c 技术文档\n\n## 1. 文件概述\n\n`cgroup/rstat.c` 实现了 cgroup 资源统计（rstat）的底层机制，用于高效地跟踪和聚合每个 CPU 上的 cgroup 资源使用情况（如 CPU 时间、内存等）。该机制采用延迟刷新（lazy flush）策略，通过 per-CPU 数据结构和精细的锁控制，减少高频更新路径的开销，并支持 BPF 程序集成以扩展统计能力。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `cgroup_rstat_updated(struct cgroup *cgrp, int cpu)`  \n  标记指定 cgroup 在某 CPU 上的统计信息已更新，并将其加入祖先链的“待刷新”链表中。\n\n- `cgroup_rstat_updated_list(struct cgroup *root, int cpu)`  \n  从指定根 cgroup 开始，遍历并提取该 CPU 上所有待刷新的 cgroup，返回一个按“子在前、父在后”顺序排列的单向链表。\n\n- `cgroup_rstat_push_children()`  \n  辅助函数，用于在遍历更新树时按层级“压栈”方式构建刷新链表，确保子 cgroup 先于父 cgroup 被处理。\n\n- `_cgroup_rstat_cpu_lock()` / `_cgroup_rstat_cpu_unlock()`  \n  封装 per-CPU 自旋锁操作，支持 fast-path 与 slow-path 的 tracepoint 区分，便于性能诊断。\n\n### 关键数据结构\n\n- `struct cgroup_rstat_cpu`（定义于 `cgroup-internal.h`）  \n  每个 cgroup 在每个 CPU 上的统计快照和链表指针，包含：\n  - `updated_next`：指向同级链表中的下一个 cgroup，链表以父 cgroup 为终止（根 cgroup 自环）。\n  - `updated_children`：指向该 cgroup 的第一个已更新子 cgroup。\n  - `rstat_flush_next`：临时用于构建刷新链表的指针。\n\n## 3. 关键实现\n\n### 延迟刷新机制（Lazy Flush）\n\n- **更新阶段**：当某 cgroup 在某 CPU 上的资源使用发生变化时，调用 `cgroup_rstat_updated()`，将其及其所有祖先加入 per-CPU 的“已更新”链表（`updated_children` / `updated_next`）。\n- **刷新阶段**：当需要获取准确统计值时（如用户读取 `/sys/fs/cgroup/.../cpu.stat`），调用 `cgroup_rstat_updated_list()` 获取待刷新 cgroup 列表，然后自底向上（子 → 父）聚合统计值并清空链表。\n\n### 链表结构设计\n\n- `updated_children`：每个 cgroup 维护一个指向其**第一个已更新子 cgroup**的指针。\n- `updated_next`：在子 cgroup 链表中，指向**下一个兄弟**；链表尾部指向**父 cgroup**（根 cgroup 指向自身），形成自终止结构，避免 NULL 检查。\n- 此设计允许 O(1) 插入，但删除需 O(n) 遍历（因单向链表），但整体因延迟刷新而摊销成本低。\n\n### 锁与并发控制\n\n- 使用 per-CPU `raw_spinlock_t`（`cgroup_rstat_cpu_lock`）保护每个 CPU 的更新链表。\n- 提供带 tracepoint 的锁封装，区分 fast-path（如 `cgroup_rstat_updated`）和 slow-path（如 flush 操作），便于生产环境性能分析。\n- 在 PREEMPT_RT 内核中，正确处理中断禁用（使用 `raw_spin_trylock_irqsave`）。\n\n### BPF 集成支持\n\n- `cgroup_rstat_updated` 标记为 `__bpf_kfunc`，允许 BPF 程序调用以通知内核其自定义统计已更新。\n- 预留 hook（代码末尾注释）用于 BPF 程序在 flush 阶段回调，实现完整的 BPF 统计收集与刷新流程。\n\n## 4. 依赖关系\n\n- **内部依赖**：\n  - `cgroup-internal.h`：定义 `struct cgroup_rstat_cpu` 及相关 cgroup 内部接口（如 `cgroup_parent()`）。\n- **内核子系统**：\n  - **cgroup 核心**：依赖 cgroup 层级结构和生命周期管理。\n  - **调度器**：通过 `linux/sched/cputime.h` 获取 CPU 时间统计。\n  - **BPF 子系统**：通过 `linux/bpf.h`、`linux/btf.h` 支持 BPF kfunc 和类型信息。\n  - **tracepoint**：使用 `trace/events/cgroup.h` 中定义的 tracepoint 用于锁竞争诊断。\n\n## 5. 使用场景\n\n- **资源控制器统计聚合**：如 CPU controller 在读取 `cpu.stat` 时，触发 rstat flush 以获取精确的 CPU 时间。\n- **BPF 资源监控程序**：BPF 程序可调用 `cgroup_rstat_updated()` 标记自定义指标更新，并在内核 flush 时同步数据。\n- **高频率更新优化**：适用于需要 per-CPU 快速更新（如进程切换时的 CPU 时间累加），但低频精确读取的场景，避免每次更新都遍历 cgroup 层级。\n- **性能分析**：通过 tracepoint 监控 rstat 锁竞争情况，诊断 cgroup 统计路径的性能瓶颈。",
      "similarity": 0.49250441789627075,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/cgroup/rstat.c",
          "start_line": 1,
          "end_line": 29,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "#include \"cgroup-internal.h\"",
            "",
            "#include <linux/sched/cputime.h>",
            "",
            "#include <linux/bpf.h>",
            "#include <linux/btf.h>",
            "#include <linux/btf_ids.h>",
            "",
            "#include <trace/events/cgroup.h>",
            "",
            "static DEFINE_SPINLOCK(cgroup_rstat_lock);",
            "static DEFINE_PER_CPU(raw_spinlock_t, cgroup_rstat_cpu_lock);",
            "",
            "static void cgroup_base_stat_flush(struct cgroup *cgrp, int cpu);",
            "",
            "static struct cgroup_rstat_cpu *cgroup_rstat_cpu(struct cgroup *cgrp, int cpu)",
            "{",
            "\treturn per_cpu_ptr(cgrp->rstat_cpu, cpu);",
            "}",
            "",
            "/*",
            " * Helper functions for rstat per CPU lock (cgroup_rstat_cpu_lock).",
            " *",
            " * This makes it easier to diagnose locking issues and contention in",
            " * production environments. The parameter @fast_path determine the",
            " * tracepoints being added, allowing us to diagnose \"flush\" related",
            " * operations without handling high-frequency fast-path \"update\" events.",
            " */"
          ],
          "function_name": null,
          "description": "定义了cgroup资源统计（rstat）的锁结构及辅助函数，包含针对多CPU环境的raw_spinlock_t类型锁，用于协调不同CPU上的资源统计更新操作，并声明了用于跟踪锁竞争状态的tracepoint。",
          "similarity": 0.48886629939079285
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/cgroup/rstat.c",
          "start_line": 437,
          "end_line": 544,
          "content": [
            "static void cgroup_base_stat_add(struct cgroup_base_stat *dst_bstat,",
            "\t\t\t\t struct cgroup_base_stat *src_bstat)",
            "{",
            "\tdst_bstat->cputime.utime += src_bstat->cputime.utime;",
            "\tdst_bstat->cputime.stime += src_bstat->cputime.stime;",
            "\tdst_bstat->cputime.sum_exec_runtime += src_bstat->cputime.sum_exec_runtime;",
            "#ifdef CONFIG_SCHED_CORE",
            "\tdst_bstat->forceidle_sum += src_bstat->forceidle_sum;",
            "#endif",
            "\tdst_bstat->ntime += src_bstat->ntime;",
            "}",
            "static void cgroup_base_stat_sub(struct cgroup_base_stat *dst_bstat,",
            "\t\t\t\t struct cgroup_base_stat *src_bstat)",
            "{",
            "\tdst_bstat->cputime.utime -= src_bstat->cputime.utime;",
            "\tdst_bstat->cputime.stime -= src_bstat->cputime.stime;",
            "\tdst_bstat->cputime.sum_exec_runtime -= src_bstat->cputime.sum_exec_runtime;",
            "#ifdef CONFIG_SCHED_CORE",
            "\tdst_bstat->forceidle_sum -= src_bstat->forceidle_sum;",
            "#endif",
            "\tdst_bstat->ntime -= src_bstat->ntime;",
            "}",
            "static void cgroup_base_stat_flush(struct cgroup *cgrp, int cpu)",
            "{",
            "\tstruct cgroup_rstat_cpu *rstatc = cgroup_rstat_cpu(cgrp, cpu);",
            "\tstruct cgroup *parent = cgroup_parent(cgrp);",
            "\tstruct cgroup_rstat_cpu *prstatc;",
            "\tstruct cgroup_base_stat delta;",
            "\tunsigned seq;",
            "",
            "\t/* Root-level stats are sourced from system-wide CPU stats */",
            "\tif (!parent)",
            "\t\treturn;",
            "",
            "\t/* fetch the current per-cpu values */",
            "\tdo {",
            "\t\tseq = __u64_stats_fetch_begin(&rstatc->bsync);",
            "\t\tdelta = rstatc->bstat;",
            "\t} while (__u64_stats_fetch_retry(&rstatc->bsync, seq));",
            "",
            "\t/* propagate per-cpu delta to cgroup and per-cpu global statistics */",
            "\tcgroup_base_stat_sub(&delta, &rstatc->last_bstat);",
            "\tcgroup_base_stat_add(&cgrp->bstat, &delta);",
            "\tcgroup_base_stat_add(&rstatc->last_bstat, &delta);",
            "\tcgroup_base_stat_add(&rstatc->subtree_bstat, &delta);",
            "",
            "\t/* propagate cgroup and per-cpu global delta to parent (unless that's root) */",
            "\tif (cgroup_parent(parent)) {",
            "\t\tdelta = cgrp->bstat;",
            "\t\tcgroup_base_stat_sub(&delta, &cgrp->last_bstat);",
            "\t\tcgroup_base_stat_add(&parent->bstat, &delta);",
            "\t\tcgroup_base_stat_add(&cgrp->last_bstat, &delta);",
            "",
            "\t\tdelta = rstatc->subtree_bstat;",
            "\t\tprstatc = cgroup_rstat_cpu(parent, cpu);",
            "\t\tcgroup_base_stat_sub(&delta, &rstatc->last_subtree_bstat);",
            "\t\tcgroup_base_stat_add(&prstatc->subtree_bstat, &delta);",
            "\t\tcgroup_base_stat_add(&rstatc->last_subtree_bstat, &delta);",
            "\t}",
            "}",
            "static void cgroup_base_stat_cputime_account_end(struct cgroup *cgrp,",
            "\t\t\t\t\t\t struct cgroup_rstat_cpu *rstatc,",
            "\t\t\t\t\t\t unsigned long flags)",
            "{",
            "\tu64_stats_update_end_irqrestore(&rstatc->bsync, flags);",
            "\tcgroup_rstat_updated(cgrp, smp_processor_id());",
            "\tput_cpu_ptr(rstatc);",
            "}",
            "void __cgroup_account_cputime(struct cgroup *cgrp, u64 delta_exec)",
            "{",
            "\tstruct cgroup_rstat_cpu *rstatc;",
            "\tunsigned long flags;",
            "",
            "\trstatc = cgroup_base_stat_cputime_account_begin(cgrp, &flags);",
            "\trstatc->bstat.cputime.sum_exec_runtime += delta_exec;",
            "\tcgroup_base_stat_cputime_account_end(cgrp, rstatc, flags);",
            "}",
            "void __cgroup_account_cputime_field(struct cgroup *cgrp,",
            "\t\t\t\t    enum cpu_usage_stat index, u64 delta_exec)",
            "{",
            "\tstruct cgroup_rstat_cpu *rstatc;",
            "\tunsigned long flags;",
            "",
            "\trstatc = cgroup_base_stat_cputime_account_begin(cgrp, &flags);",
            "",
            "\tswitch (index) {",
            "\tcase CPUTIME_NICE:",
            "\t\trstatc->bstat.ntime += delta_exec;",
            "\t\tfallthrough;",
            "\tcase CPUTIME_USER:",
            "\t\trstatc->bstat.cputime.utime += delta_exec;",
            "\t\tbreak;",
            "\tcase CPUTIME_SYSTEM:",
            "\tcase CPUTIME_IRQ:",
            "\tcase CPUTIME_SOFTIRQ:",
            "\t\trstatc->bstat.cputime.stime += delta_exec;",
            "\t\tbreak;",
            "#ifdef CONFIG_SCHED_CORE",
            "\tcase CPUTIME_FORCEIDLE:",
            "\t\trstatc->bstat.forceidle_sum += delta_exec;",
            "\t\tbreak;",
            "#endif",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "",
            "\tcgroup_base_stat_cputime_account_end(cgrp, rstatc, flags);",
            "}"
          ],
          "function_name": "cgroup_base_stat_add, cgroup_base_stat_sub, cgroup_base_stat_flush, cgroup_base_stat_cputime_account_end, __cgroup_account_cputime, __cgroup_account_cputime_field",
          "description": "实现cgroup基础统计值的算术运算（加减）和数据刷新逻辑，将per-CPU统计缓冲区的数据传播到cgroup层级，包含时间字段的增量计算和子树统计的传播过程。",
          "similarity": 0.4670718014240265
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/cgroup/rstat.c",
          "start_line": 295,
          "end_line": 398,
          "content": [
            "static inline void __cgroup_rstat_unlock(struct cgroup *cgrp, int cpu_in_loop)",
            "\t__releases(&cgroup_rstat_lock)",
            "{",
            "\ttrace_cgroup_rstat_unlock(cgrp, cpu_in_loop, false);",
            "\tspin_unlock_irq(&cgroup_rstat_lock);",
            "}",
            "static void cgroup_rstat_flush_locked(struct cgroup *cgrp)",
            "\t__releases(&cgroup_rstat_lock) __acquires(&cgroup_rstat_lock)",
            "{",
            "\tint cpu;",
            "",
            "\tlockdep_assert_held(&cgroup_rstat_lock);",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct cgroup *pos = cgroup_rstat_updated_list(cgrp, cpu);",
            "",
            "\t\tfor (; pos; pos = pos->rstat_flush_next) {",
            "\t\t\tstruct cgroup_subsys_state *css;",
            "",
            "\t\t\tcgroup_base_stat_flush(pos, cpu);",
            "\t\t\tbpf_rstat_flush(pos, cgroup_parent(pos), cpu);",
            "",
            "\t\t\trcu_read_lock();",
            "\t\t\tlist_for_each_entry_rcu(css, &pos->rstat_css_list,",
            "\t\t\t\t\t\trstat_css_node)",
            "\t\t\t\tcss->ss->css_rstat_flush(css, cpu);",
            "\t\t\trcu_read_unlock();",
            "\t\t}",
            "",
            "\t\t/* play nice and yield if necessary */",
            "\t\tif (need_resched() || spin_needbreak(&cgroup_rstat_lock)) {",
            "\t\t\t__cgroup_rstat_unlock(cgrp, cpu);",
            "\t\t\tif (!cond_resched())",
            "\t\t\t\tcpu_relax();",
            "\t\t\t__cgroup_rstat_lock(cgrp, cpu);",
            "\t\t}",
            "\t}",
            "}",
            "__bpf_kfunc void cgroup_rstat_flush(struct cgroup *cgrp)",
            "{",
            "\tmight_sleep();",
            "",
            "\t__cgroup_rstat_lock(cgrp, -1);",
            "\tcgroup_rstat_flush_locked(cgrp);",
            "\t__cgroup_rstat_unlock(cgrp, -1);",
            "}",
            "void cgroup_rstat_flush_hold(struct cgroup *cgrp)",
            "\t__acquires(&cgroup_rstat_lock)",
            "{",
            "\tmight_sleep();",
            "\t__cgroup_rstat_lock(cgrp, -1);",
            "\tcgroup_rstat_flush_locked(cgrp);",
            "}",
            "void cgroup_rstat_flush_release(struct cgroup *cgrp)",
            "\t__releases(&cgroup_rstat_lock)",
            "{",
            "\t__cgroup_rstat_unlock(cgrp, -1);",
            "}",
            "int cgroup_rstat_init(struct cgroup *cgrp)",
            "{",
            "\tint cpu;",
            "",
            "\t/* the root cgrp has rstat_cpu preallocated */",
            "\tif (!cgrp->rstat_cpu) {",
            "\t\tcgrp->rstat_cpu = alloc_percpu(struct cgroup_rstat_cpu);",
            "\t\tif (!cgrp->rstat_cpu)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\t/* ->updated_children list is self terminated */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct cgroup_rstat_cpu *rstatc = cgroup_rstat_cpu(cgrp, cpu);",
            "",
            "\t\trstatc->updated_children = cgrp;",
            "\t\tu64_stats_init(&rstatc->bsync);",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "void cgroup_rstat_exit(struct cgroup *cgrp)",
            "{",
            "\tint cpu;",
            "",
            "\tcgroup_rstat_flush(cgrp);",
            "",
            "\t/* sanity check */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct cgroup_rstat_cpu *rstatc = cgroup_rstat_cpu(cgrp, cpu);",
            "",
            "\t\tif (WARN_ON_ONCE(rstatc->updated_children != cgrp) ||",
            "\t\t    WARN_ON_ONCE(rstatc->updated_next))",
            "\t\t\treturn;",
            "\t}",
            "",
            "\tfree_percpu(cgrp->rstat_cpu);",
            "\tcgrp->rstat_cpu = NULL;",
            "}",
            "void __init cgroup_rstat_boot(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\traw_spin_lock_init(per_cpu_ptr(&cgroup_rstat_cpu_lock, cpu));",
            "}"
          ],
          "function_name": "__cgroup_rstat_unlock, cgroup_rstat_flush_locked, cgroup_rstat_flush, cgroup_rstat_flush_hold, cgroup_rstat_flush_release, cgroup_rstat_init, cgroup_rstat_exit, cgroup_rstat_boot",
          "description": "定义了cgroup统计信息的刷新机制，包含持有锁时的安全遍历更新列表、释放锁的操作，以及初始化/退出cgroup统计结构的函数，包含内存分配回收和状态校验逻辑。",
          "similarity": 0.45537039637565613
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/cgroup/rstat.c",
          "start_line": 30,
          "end_line": 135,
          "content": [
            "static __always_inline",
            "unsigned long _cgroup_rstat_cpu_lock(raw_spinlock_t *cpu_lock, int cpu,",
            "\t\t\t\t     struct cgroup *cgrp, const bool fast_path)",
            "{",
            "\tunsigned long flags;",
            "\tbool contended;",
            "",
            "\t/*",
            "\t * The _irqsave() is needed because cgroup_rstat_lock is",
            "\t * spinlock_t which is a sleeping lock on PREEMPT_RT. Acquiring",
            "\t * this lock with the _irq() suffix only disables interrupts on",
            "\t * a non-PREEMPT_RT kernel. The raw_spinlock_t below disables",
            "\t * interrupts on both configurations. The _irqsave() ensures",
            "\t * that interrupts are always disabled and later restored.",
            "\t */",
            "\tcontended = !raw_spin_trylock_irqsave(cpu_lock, flags);",
            "\tif (contended) {",
            "\t\tif (fast_path)",
            "\t\t\ttrace_cgroup_rstat_cpu_lock_contended_fastpath(cgrp, cpu, contended);",
            "\t\telse",
            "\t\t\ttrace_cgroup_rstat_cpu_lock_contended(cgrp, cpu, contended);",
            "",
            "\t\traw_spin_lock_irqsave(cpu_lock, flags);",
            "\t}",
            "",
            "\tif (fast_path)",
            "\t\ttrace_cgroup_rstat_cpu_locked_fastpath(cgrp, cpu, contended);",
            "\telse",
            "\t\ttrace_cgroup_rstat_cpu_locked(cgrp, cpu, contended);",
            "",
            "\treturn flags;",
            "}",
            "static __always_inline",
            "void _cgroup_rstat_cpu_unlock(raw_spinlock_t *cpu_lock, int cpu,",
            "\t\t\t      struct cgroup *cgrp, unsigned long flags,",
            "\t\t\t      const bool fast_path)",
            "{",
            "\tif (fast_path)",
            "\t\ttrace_cgroup_rstat_cpu_unlock_fastpath(cgrp, cpu, false);",
            "\telse",
            "\t\ttrace_cgroup_rstat_cpu_unlock(cgrp, cpu, false);",
            "",
            "\traw_spin_unlock_irqrestore(cpu_lock, flags);",
            "}",
            "__bpf_kfunc void cgroup_rstat_updated(struct cgroup *cgrp, int cpu)",
            "{",
            "\traw_spinlock_t *cpu_lock = per_cpu_ptr(&cgroup_rstat_cpu_lock, cpu);",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * Speculative already-on-list test. This may race leading to",
            "\t * temporary inaccuracies, which is fine.",
            "\t *",
            "\t * Because @parent's updated_children is terminated with @parent",
            "\t * instead of NULL, we can tell whether @cgrp is on the list by",
            "\t * testing the next pointer for NULL.",
            "\t */",
            "\tif (data_race(cgroup_rstat_cpu(cgrp, cpu)->updated_next))",
            "\t\treturn;",
            "",
            "\tflags = _cgroup_rstat_cpu_lock(cpu_lock, cpu, cgrp, true);",
            "",
            "\t/* put @cgrp and all ancestors on the corresponding updated lists */",
            "\twhile (true) {",
            "\t\tstruct cgroup_rstat_cpu *rstatc = cgroup_rstat_cpu(cgrp, cpu);",
            "\t\tstruct cgroup *parent = cgroup_parent(cgrp);",
            "\t\tstruct cgroup_rstat_cpu *prstatc;",
            "",
            "\t\t/*",
            "\t\t * Both additions and removals are bottom-up.  If a cgroup",
            "\t\t * is already in the tree, all ancestors are.",
            "\t\t */",
            "\t\tif (rstatc->updated_next)",
            "\t\t\tbreak;",
            "",
            "\t\t/* Root has no parent to link it to, but mark it busy */",
            "\t\tif (!parent) {",
            "\t\t\trstatc->updated_next = cgrp;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tprstatc = cgroup_rstat_cpu(parent, cpu);",
            "\t\trstatc->updated_next = prstatc->updated_children;",
            "\t\tprstatc->updated_children = cgrp;",
            "",
            "\t\tcgrp = parent;",
            "\t}",
            "",
            "\t_cgroup_rstat_cpu_unlock(cpu_lock, cpu, cgrp, flags, true);",
            "}",
            "__weak noinline void bpf_rstat_flush(struct cgroup *cgrp,",
            "\t\t\t\t     struct cgroup *parent, int cpu)",
            "{",
            "}",
            "static inline void __cgroup_rstat_lock(struct cgroup *cgrp, int cpu_in_loop)",
            "\t__acquires(&cgroup_rstat_lock)",
            "{",
            "\tbool contended;",
            "",
            "\tcontended = !spin_trylock_irq(&cgroup_rstat_lock);",
            "\tif (contended) {",
            "\t\ttrace_cgroup_rstat_lock_contended(cgrp, cpu_in_loop, contended);",
            "\t\tspin_lock_irq(&cgroup_rstat_lock);",
            "\t}",
            "\ttrace_cgroup_rstat_locked(cgrp, cpu_in_loop, contended);",
            "}"
          ],
          "function_name": "_cgroup_rstat_cpu_lock, _cgroup_rstat_cpu_unlock, cgroup_rstat_updated, bpf_rstat_flush, __cgroup_rstat_lock",
          "description": "实现了针对cgroup rstat的CPU级别锁操作函数，包含获取/释放锁的原子操作，通过tracepoint记录锁竞争状况，并提供了更新cgroup统计信息的接口，支持BPF程序的挂接点。",
          "similarity": 0.44701260328292847
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/cgroup/rstat.c",
          "start_line": 567,
          "end_line": 646,
          "content": [
            "static void root_cgroup_cputime(struct cgroup_base_stat *bstat)",
            "{",
            "\tstruct task_cputime *cputime = &bstat->cputime;",
            "\tint i;",
            "",
            "\tmemset(bstat, 0, sizeof(*bstat));",
            "\tfor_each_possible_cpu(i) {",
            "\t\tstruct kernel_cpustat kcpustat;",
            "\t\tu64 *cpustat = kcpustat.cpustat;",
            "\t\tu64 user = 0;",
            "\t\tu64 sys = 0;",
            "",
            "\t\tkcpustat_cpu_fetch(&kcpustat, i);",
            "",
            "\t\tuser += cpustat[CPUTIME_USER];",
            "\t\tuser += cpustat[CPUTIME_NICE];",
            "\t\tcputime->utime += user;",
            "",
            "\t\tsys += cpustat[CPUTIME_SYSTEM];",
            "\t\tsys += cpustat[CPUTIME_IRQ];",
            "\t\tsys += cpustat[CPUTIME_SOFTIRQ];",
            "\t\tcputime->stime += sys;",
            "",
            "\t\tcputime->sum_exec_runtime += user;",
            "\t\tcputime->sum_exec_runtime += sys;",
            "",
            "#ifdef CONFIG_SCHED_CORE",
            "\t\tbstat->forceidle_sum += cpustat[CPUTIME_FORCEIDLE];",
            "#endif",
            "\t\tbstat->ntime += cpustat[CPUTIME_NICE];",
            "\t}",
            "}",
            "static void cgroup_force_idle_show(struct seq_file *seq, struct cgroup_base_stat *bstat)",
            "{",
            "#ifdef CONFIG_SCHED_CORE",
            "\tu64 forceidle_time = bstat->forceidle_sum;",
            "",
            "\tdo_div(forceidle_time, NSEC_PER_USEC);",
            "\tseq_printf(seq, \"core_sched.force_idle_usec %llu\\n\", forceidle_time);",
            "#endif",
            "}",
            "void cgroup_base_stat_cputime_show(struct seq_file *seq)",
            "{",
            "\tstruct cgroup *cgrp = seq_css(seq)->cgroup;",
            "\tu64 usage, utime, stime, ntime;",
            "",
            "\tif (cgroup_parent(cgrp)) {",
            "\t\tcgroup_rstat_flush_hold(cgrp);",
            "\t\tusage = cgrp->bstat.cputime.sum_exec_runtime;",
            "\t\tcputime_adjust(&cgrp->bstat.cputime, &cgrp->prev_cputime,",
            "\t\t\t       &utime, &stime);",
            "\t\tntime = cgrp->bstat.ntime;",
            "\t\tcgroup_rstat_flush_release(cgrp);",
            "\t} else {",
            "\t\t/* cgrp->bstat of root is not actually used, reuse it */",
            "\t\troot_cgroup_cputime(&cgrp->bstat);",
            "\t\tusage = cgrp->bstat.cputime.sum_exec_runtime;",
            "\t\tutime = cgrp->bstat.cputime.utime;",
            "\t\tstime = cgrp->bstat.cputime.stime;",
            "\t\tntime = cgrp->bstat.ntime;",
            "\t}",
            "",
            "\tdo_div(usage, NSEC_PER_USEC);",
            "\tdo_div(utime, NSEC_PER_USEC);",
            "\tdo_div(stime, NSEC_PER_USEC);",
            "\tdo_div(ntime, NSEC_PER_USEC);",
            "",
            "\tseq_printf(seq, \"usage_usec %llu\\n\"",
            "\t\t\t\"user_usec %llu\\n\"",
            "\t\t\t\"system_usec %llu\\n\"",
            "\t\t\t\"nice_usec %llu\\n\",",
            "\t\t\tusage, utime, stime, ntime);",
            "",
            "\tcgroup_force_idle_show(seq, &cgrp->bstat);",
            "}",
            "static int __init bpf_rstat_kfunc_init(void)",
            "{",
            "\treturn register_btf_kfunc_id_set(BPF_PROG_TYPE_TRACING,",
            "\t\t\t\t\t &bpf_rstat_kfunc_set);",
            "}"
          ],
          "function_name": "root_cgroup_cputime, cgroup_force_idle_show, cgroup_base_stat_cputime_show, bpf_rstat_kfunc_init",
          "description": "提供cgroup统计数据显示接口，包含根cgroup的CPU时间统计构建逻辑，以及强制空闲时间的展示函数，注册BPF辅助函数集用于内核与eBPF程序的交互。",
          "similarity": 0.3948133587837219
        }
      ]
    },
    {
      "source_file": "kernel/sched/fair.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:09:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\fair.c`\n\n---\n\n# `sched/fair.c` 技术文档\n\n## 1. 文件概述\n\n`sched/fair.c` 是 Linux 内核中 **完全公平调度器**（Completely Fair Scheduler, CFS）的核心实现文件，负责实现 `SCHED_NORMAL` 和 `SCHED_BATCH` 调度策略。CFS 旨在通过红黑树（RB-tree）维护可运行任务的虚拟运行时间（vruntime），以实现 CPU 时间的公平分配。该文件实现了任务调度、负载跟踪、时间片计算、组调度（group scheduling）、NUMA 负载均衡、带宽控制等关键机制，是 Linux 通用调度子系统的核心组成部分。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_entity`：调度实体，代表一个可调度单元（任务或任务组）\n- `struct cfs_rq`：CFS 运行队列，管理一组调度实体\n- `struct load_weight`：负载权重结构，用于计算任务对系统负载的贡献\n\n### 关键函数与宏\n- `__calc_delta()` / `calc_delta_fair()`：计算基于权重的调度时间增量\n- `update_load_add()` / `update_load_sub()` / `update_load_set()`：更新负载权重\n- `__update_inv_weight()`：预计算权重的倒数以优化除法运算\n- `get_update_sysctl_factor()`：根据在线 CPU 数量动态调整调度参数\n- `update_sysctl()` / `sched_init_granularity()`：初始化和更新调度粒度参数\n- `for_each_sched_entity()`：遍历调度实体层级结构（用于组调度）\n\n### 可调参数（sysctl）\n- `sysctl_sched_base_slice`：基础时间片（默认 700,000 纳秒）\n- `sysctl_sched_tunable_scaling`：调度参数缩放策略（NONE/LOG/LINEAR）\n- `sysctl_sched_migration_cost`：任务迁移成本阈值（500 微秒）\n- `sysctl_sched_cfs_bandwidth_slice_us`（CFS 带宽控制切片，默认 5 毫秒）\n- `sysctl_numa_balancing_promote_rate_limit_MBps`（NUMA 页迁移速率限制）\n\n## 3. 关键实现\n\n### 虚拟时间与公平性\nCFS 使用 **虚拟运行时间**（vruntime）衡量任务已使用的 CPU 时间，并通过 `calc_delta_fair()` 将实际执行时间按任务权重归一化。权重由任务的 nice 值决定（`NICE_0_LOAD = 1024` 为基准）。调度器总是选择 vruntime 最小的任务运行，确保高优先级（高权重）任务获得更多 CPU 时间。\n\n### 高效除法优化\n为避免频繁除法运算，CFS 预计算 `inv_weight = WMULT_CONST / weight`（`WMULT_CONST = ~0U`），将除法转换为乘法和右移操作（`mul_u64_u32_shr`）。`__calc_delta()` 通过动态调整移位位数（`shift`）保证计算精度，适用于 32/64 位架构。\n\n### 动态粒度调整\n基础时间片 `sched_base_slice` 根据在线 CPU 数量动态缩放：\n- `SCHED_TUNABLESCALING_NONE`：固定值\n- `SCHED_TUNABLESCALING_LINEAR`：线性缩放（×ncpus）\n- `SCHED_TUNABLESCALING_LOG`（默认）：对数缩放（×(1 + ilog2(ncpus))）  \n此设计确保在多核系统中保持合理的调度延迟和交互性。\n\n### 组调度支持\n通过 `for_each_sched_entity()` 宏遍历任务所属的调度实体层级（任务 → 任务组 → 父任务组），实现 CPU 带宽在任务组间的公平分配。每个 `cfs_rq` 独立维护其子实体的红黑树。\n\n### SMP 相关优化\n- **非对称 CPU 优先级**：`arch_asym_cpu_priority()` 允许架构定义 CPU 能力差异（如大小核）\n- **容量比较宏**：`fits_capacity()`（20% 容差）和 `capacity_greater()`（5% 容差）用于负载均衡决策\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- 调度核心：`\"sched.h\"`、`\"stats.h\"`、`\"autogroup.h\"`\n- 系统服务：`<linux/sched/clock.h>`、`<linux/sched/nohz.h>`、`<linux/psi.h>`\n- 内存管理：`<linux/mem_policy.h>`、`<linux/energy_model.h>`\n- SMP 支持：`<linux/topology.h>`、`<linux/cpumask_api.h>`\n- 数据结构：`<linux/rbtree_augmented.h>`\n\n### 条件编译特性\n- `CONFIG_SMP`：多处理器调度优化\n- `CONFIG_CFS_BANDWIDTH`：CPU 带宽限制（cgroup v1/v2）\n- `CONFIG_NUMA_BALANCING`：NUMA 自动迁移\n- `CONFIG_FAIR_GROUP_SCHED`：CFS 组调度（cgroup 支持）\n\n## 5. 使用场景\n\n- **通用任务调度**：所有使用 `SCHED_NORMAL` 或 `SCHED_BATCH` 策略的用户态进程\n- **cgroup CPU 资源控制**：通过 `cpu.cfs_quota_us` 和 `cpu.cfs_period_us` 限制任务组带宽\n- **NUMA 优化**：自动迁移内存页以减少远程访问（`numa_balancing`）\n- **节能调度**：结合 `energy_model` 在满足性能前提下选择低功耗 CPU\n- **实时性保障**：通过 `cond_resched()` 在长循环中主动让出 CPU，避免内核抢占延迟过高\n- **系统调优**：管理员通过 `/proc/sys/kernel/` 下的 sysctl 参数动态调整调度行为",
      "similarity": 0.4902811050415039,
      "chunks": [
        {
          "chunk_id": 14,
          "file_path": "kernel/sched/fair.c",
          "start_line": 2430,
          "end_line": 2620,
          "content": [
            "static void task_numa_find_cpu(struct task_numa_env *env,",
            "\t\t\t\tlong taskimp, long groupimp)",
            "{",
            "\tbool maymove = false;",
            "\tint cpu;",
            "",
            "\t/*",
            "\t * If dst node has spare capacity, then check if there is an",
            "\t * imbalance that would be overruled by the load balancer.",
            "\t */",
            "\tif (env->dst_stats.node_type == node_has_spare) {",
            "\t\tunsigned int imbalance;",
            "\t\tint src_running, dst_running;",
            "",
            "\t\t/*",
            "\t\t * Would movement cause an imbalance? Note that if src has",
            "\t\t * more running tasks that the imbalance is ignored as the",
            "\t\t * move improves the imbalance from the perspective of the",
            "\t\t * CPU load balancer.",
            "\t\t * */",
            "\t\tsrc_running = env->src_stats.nr_running - 1;",
            "\t\tdst_running = env->dst_stats.nr_running + 1;",
            "\t\timbalance = max(0, dst_running - src_running);",
            "\t\timbalance = adjust_numa_imbalance(imbalance, dst_running,",
            "\t\t\t\t\t\t  env->imb_numa_nr);",
            "",
            "\t\t/* Use idle CPU if there is no imbalance */",
            "\t\tif (!imbalance) {",
            "\t\t\tmaymove = true;",
            "\t\t\tif (env->dst_stats.idle_cpu >= 0) {",
            "\t\t\t\tenv->dst_cpu = env->dst_stats.idle_cpu;",
            "\t\t\t\ttask_numa_assign(env, NULL, 0);",
            "\t\t\t\treturn;",
            "\t\t\t}",
            "\t\t}",
            "\t} else {",
            "\t\tlong src_load, dst_load, load;",
            "\t\t/*",
            "\t\t * If the improvement from just moving env->p direction is better",
            "\t\t * than swapping tasks around, check if a move is possible.",
            "\t\t */",
            "\t\tload = task_h_load(env->p);",
            "\t\tdst_load = env->dst_stats.load + load;",
            "\t\tsrc_load = env->src_stats.load - load;",
            "\t\tmaymove = !load_too_imbalanced(src_load, dst_load, env);",
            "\t}",
            "",
            "\tfor_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {",
            "\t\t/* Skip this CPU if the source task cannot migrate */",
            "\t\tif (!cpumask_test_cpu(cpu, env->p->cpus_ptr))",
            "\t\t\tcontinue;",
            "",
            "\t\tenv->dst_cpu = cpu;",
            "\t\tif (task_numa_compare(env, taskimp, groupimp, maymove))",
            "\t\t\tbreak;",
            "\t}",
            "}",
            "static int task_numa_migrate(struct task_struct *p)",
            "{",
            "\tstruct task_numa_env env = {",
            "\t\t.p = p,",
            "",
            "\t\t.src_cpu = task_cpu(p),",
            "\t\t.src_nid = task_node(p),",
            "",
            "\t\t.imbalance_pct = 112,",
            "",
            "\t\t.best_task = NULL,",
            "\t\t.best_imp = 0,",
            "\t\t.best_cpu = -1,",
            "\t};",
            "\tunsigned long taskweight, groupweight;",
            "\tstruct sched_domain *sd;",
            "\tlong taskimp, groupimp;",
            "\tstruct numa_group *ng;",
            "\tstruct rq *best_rq;",
            "\tint nid, ret, dist;",
            "",
            "\t/*",
            "\t * Pick the lowest SD_NUMA domain, as that would have the smallest",
            "\t * imbalance and would be the first to start moving tasks about.",
            "\t *",
            "\t * And we want to avoid any moving of tasks about, as that would create",
            "\t * random movement of tasks -- counter the numa conditions we're trying",
            "\t * to satisfy here.",
            "\t */",
            "\trcu_read_lock();",
            "\tsd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));",
            "\tif (sd) {",
            "\t\tenv.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;",
            "\t\tenv.imb_numa_nr = sd->imb_numa_nr;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * Cpusets can break the scheduler domain tree into smaller",
            "\t * balance domains, some of which do not cross NUMA boundaries.",
            "\t * Tasks that are \"trapped\" in such domains cannot be migrated",
            "\t * elsewhere, so there is no point in (re)trying.",
            "\t */",
            "\tif (unlikely(!sd)) {",
            "\t\tsched_setnuma(p, task_node(p));",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tenv.dst_nid = p->numa_preferred_nid;",
            "\tdist = env.dist = node_distance(env.src_nid, env.dst_nid);",
            "\ttaskweight = task_weight(p, env.src_nid, dist);",
            "\tgroupweight = group_weight(p, env.src_nid, dist);",
            "\tupdate_numa_stats(&env, &env.src_stats, env.src_nid, false);",
            "\ttaskimp = task_weight(p, env.dst_nid, dist) - taskweight;",
            "\tgroupimp = group_weight(p, env.dst_nid, dist) - groupweight;",
            "\tupdate_numa_stats(&env, &env.dst_stats, env.dst_nid, true);",
            "",
            "\t/* Try to find a spot on the preferred nid. */",
            "\ttask_numa_find_cpu(&env, taskimp, groupimp);",
            "",
            "\t/*",
            "\t * Look at other nodes in these cases:",
            "\t * - there is no space available on the preferred_nid",
            "\t * - the task is part of a numa_group that is interleaved across",
            "\t *   multiple NUMA nodes; in order to better consolidate the group,",
            "\t *   we need to check other locations.",
            "\t */",
            "\tng = deref_curr_numa_group(p);",
            "\tif (env.best_cpu == -1 || (ng && ng->active_nodes > 1)) {",
            "\t\tfor_each_node_state(nid, N_CPU) {",
            "\t\t\tif (nid == env.src_nid || nid == p->numa_preferred_nid)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tdist = node_distance(env.src_nid, env.dst_nid);",
            "\t\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&",
            "\t\t\t\t\t\tdist != env.dist) {",
            "\t\t\t\ttaskweight = task_weight(p, env.src_nid, dist);",
            "\t\t\t\tgroupweight = group_weight(p, env.src_nid, dist);",
            "\t\t\t}",
            "",
            "\t\t\t/* Only consider nodes where both task and groups benefit */",
            "\t\t\ttaskimp = task_weight(p, nid, dist) - taskweight;",
            "\t\t\tgroupimp = group_weight(p, nid, dist) - groupweight;",
            "\t\t\tif (taskimp < 0 && groupimp < 0)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tenv.dist = dist;",
            "\t\t\tenv.dst_nid = nid;",
            "\t\t\tupdate_numa_stats(&env, &env.dst_stats, env.dst_nid, true);",
            "\t\t\ttask_numa_find_cpu(&env, taskimp, groupimp);",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * If the task is part of a workload that spans multiple NUMA nodes,",
            "\t * and is migrating into one of the workload's active nodes, remember",
            "\t * this node as the task's preferred numa node, so the workload can",
            "\t * settle down.",
            "\t * A task that migrated to a second choice node will be better off",
            "\t * trying for a better one later. Do not set the preferred node here.",
            "\t */",
            "\tif (ng) {",
            "\t\tif (env.best_cpu == -1)",
            "\t\t\tnid = env.src_nid;",
            "\t\telse",
            "\t\t\tnid = cpu_to_node(env.best_cpu);",
            "",
            "\t\tif (nid != p->numa_preferred_nid)",
            "\t\t\tsched_setnuma(p, nid);",
            "\t}",
            "",
            "\t/* No better CPU than the current one was found. */",
            "\tif (env.best_cpu == -1) {",
            "\t\ttrace_sched_stick_numa(p, env.src_cpu, NULL, -1);",
            "\t\treturn -EAGAIN;",
            "\t}",
            "",
            "\tbest_rq = cpu_rq(env.best_cpu);",
            "\tif (env.best_task == NULL) {",
            "\t\tret = migrate_task_to(p, env.best_cpu);",
            "\t\tWRITE_ONCE(best_rq->numa_migrate_on, 0);",
            "\t\tif (ret != 0)",
            "\t\t\ttrace_sched_stick_numa(p, env.src_cpu, NULL, env.best_cpu);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu);",
            "\tWRITE_ONCE(best_rq->numa_migrate_on, 0);",
            "",
            "\tif (ret != 0)",
            "\t\ttrace_sched_stick_numa(p, env.src_cpu, env.best_task, env.best_cpu);",
            "\tput_task_struct(env.best_task);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "task_numa_find_cpu, task_numa_migrate",
          "description": "基于NUMA拓扑和调度域选择最佳目标节点；计算任务权重差异并触发迁移；若未找到合适节点则标记为无法迁移并返回错误码",
          "similarity": 0.5150371789932251
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/fair.c",
          "start_line": 2200,
          "end_line": 2414,
          "content": [
            "static bool load_too_imbalanced(long src_load, long dst_load,",
            "\t\t\t\tstruct task_numa_env *env)",
            "{",
            "\tlong imb, old_imb;",
            "\tlong orig_src_load, orig_dst_load;",
            "\tlong src_capacity, dst_capacity;",
            "",
            "\t/*",
            "\t * The load is corrected for the CPU capacity available on each node.",
            "\t *",
            "\t * src_load        dst_load",
            "\t * ------------ vs ---------",
            "\t * src_capacity    dst_capacity",
            "\t */",
            "\tsrc_capacity = env->src_stats.compute_capacity;",
            "\tdst_capacity = env->dst_stats.compute_capacity;",
            "",
            "\timb = abs(dst_load * src_capacity - src_load * dst_capacity);",
            "",
            "\torig_src_load = env->src_stats.load;",
            "\torig_dst_load = env->dst_stats.load;",
            "",
            "\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);",
            "",
            "\t/* Would this change make things worse? */",
            "\treturn (imb > old_imb);",
            "}",
            "static bool task_numa_compare(struct task_numa_env *env,",
            "\t\t\t      long taskimp, long groupimp, bool maymove)",
            "{",
            "\tstruct numa_group *cur_ng, *p_ng = deref_curr_numa_group(env->p);",
            "\tstruct rq *dst_rq = cpu_rq(env->dst_cpu);",
            "\tlong imp = p_ng ? groupimp : taskimp;",
            "\tstruct task_struct *cur;",
            "\tlong src_load, dst_load;",
            "\tint dist = env->dist;",
            "\tlong moveimp = imp;",
            "\tlong load;",
            "\tbool stopsearch = false;",
            "",
            "\tif (READ_ONCE(dst_rq->numa_migrate_on))",
            "\t\treturn false;",
            "",
            "\trcu_read_lock();",
            "\tcur = rcu_dereference(dst_rq->curr);",
            "\tif (cur && ((cur->flags & (PF_EXITING | PF_KTHREAD)) ||",
            "\t\t    !cur->mm))",
            "\t\tcur = NULL;",
            "",
            "\t/*",
            "\t * Because we have preemption enabled we can get migrated around and",
            "\t * end try selecting ourselves (current == env->p) as a swap candidate.",
            "\t */",
            "\tif (cur == env->p) {",
            "\t\tstopsearch = true;",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\tif (!cur) {",
            "\t\tif (maymove && moveimp >= env->best_imp)",
            "\t\t\tgoto assign;",
            "\t\telse",
            "\t\t\tgoto unlock;",
            "\t}",
            "",
            "\t/* Skip this swap candidate if cannot move to the source cpu. */",
            "\tif (!cpumask_test_cpu(env->src_cpu, cur->cpus_ptr))",
            "\t\tgoto unlock;",
            "",
            "\t/*",
            "\t * Skip this swap candidate if it is not moving to its preferred",
            "\t * node and the best task is.",
            "\t */",
            "\tif (env->best_task &&",
            "\t    env->best_task->numa_preferred_nid == env->src_nid &&",
            "\t    cur->numa_preferred_nid != env->src_nid) {",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\t/*",
            "\t * \"imp\" is the fault differential for the source task between the",
            "\t * source and destination node. Calculate the total differential for",
            "\t * the source task and potential destination task. The more negative",
            "\t * the value is, the more remote accesses that would be expected to",
            "\t * be incurred if the tasks were swapped.",
            "\t *",
            "\t * If dst and source tasks are in the same NUMA group, or not",
            "\t * in any group then look only at task weights.",
            "\t */",
            "\tcur_ng = rcu_dereference(cur->numa_group);",
            "\tif (cur_ng == p_ng) {",
            "\t\t/*",
            "\t\t * Do not swap within a group or between tasks that have",
            "\t\t * no group if there is spare capacity. Swapping does",
            "\t\t * not address the load imbalance and helps one task at",
            "\t\t * the cost of punishing another.",
            "\t\t */",
            "\t\tif (env->dst_stats.node_type == node_has_spare)",
            "\t\t\tgoto unlock;",
            "",
            "\t\timp = taskimp + task_weight(cur, env->src_nid, dist) -",
            "\t\t      task_weight(cur, env->dst_nid, dist);",
            "\t\t/*",
            "\t\t * Add some hysteresis to prevent swapping the",
            "\t\t * tasks within a group over tiny differences.",
            "\t\t */",
            "\t\tif (cur_ng)",
            "\t\t\timp -= imp / 16;",
            "\t} else {",
            "\t\t/*",
            "\t\t * Compare the group weights. If a task is all by itself",
            "\t\t * (not part of a group), use the task weight instead.",
            "\t\t */",
            "\t\tif (cur_ng && p_ng)",
            "\t\t\timp += group_weight(cur, env->src_nid, dist) -",
            "\t\t\t       group_weight(cur, env->dst_nid, dist);",
            "\t\telse",
            "\t\t\timp += task_weight(cur, env->src_nid, dist) -",
            "\t\t\t       task_weight(cur, env->dst_nid, dist);",
            "\t}",
            "",
            "\t/* Discourage picking a task already on its preferred node */",
            "\tif (cur->numa_preferred_nid == env->dst_nid)",
            "\t\timp -= imp / 16;",
            "",
            "\t/*",
            "\t * Encourage picking a task that moves to its preferred node.",
            "\t * This potentially makes imp larger than it's maximum of",
            "\t * 1998 (see SMALLIMP and task_weight for why) but in this",
            "\t * case, it does not matter.",
            "\t */",
            "\tif (cur->numa_preferred_nid == env->src_nid)",
            "\t\timp += imp / 8;",
            "",
            "\tif (maymove && moveimp > imp && moveimp > env->best_imp) {",
            "\t\timp = moveimp;",
            "\t\tcur = NULL;",
            "\t\tgoto assign;",
            "\t}",
            "",
            "\t/*",
            "\t * Prefer swapping with a task moving to its preferred node over a",
            "\t * task that is not.",
            "\t */",
            "\tif (env->best_task && cur->numa_preferred_nid == env->src_nid &&",
            "\t    env->best_task->numa_preferred_nid != env->src_nid) {",
            "\t\tgoto assign;",
            "\t}",
            "",
            "\t/*",
            "\t * If the NUMA importance is less than SMALLIMP,",
            "\t * task migration might only result in ping pong",
            "\t * of tasks and also hurt performance due to cache",
            "\t * misses.",
            "\t */",
            "\tif (imp < SMALLIMP || imp <= env->best_imp + SMALLIMP / 2)",
            "\t\tgoto unlock;",
            "",
            "\t/*",
            "\t * In the overloaded case, try and keep the load balanced.",
            "\t */",
            "\tload = task_h_load(env->p) - task_h_load(cur);",
            "\tif (!load)",
            "\t\tgoto assign;",
            "",
            "\tdst_load = env->dst_stats.load + load;",
            "\tsrc_load = env->src_stats.load - load;",
            "",
            "\tif (load_too_imbalanced(src_load, dst_load, env))",
            "\t\tgoto unlock;",
            "",
            "assign:",
            "\t/* Evaluate an idle CPU for a task numa move. */",
            "\tif (!cur) {",
            "\t\tint cpu = env->dst_stats.idle_cpu;",
            "",
            "\t\t/* Nothing cached so current CPU went idle since the search. */",
            "\t\tif (cpu < 0)",
            "\t\t\tcpu = env->dst_cpu;",
            "",
            "\t\t/*",
            "\t\t * If the CPU is no longer truly idle and the previous best CPU",
            "\t\t * is, keep using it.",
            "\t\t */",
            "\t\tif (!idle_cpu(cpu) && env->best_cpu >= 0 &&",
            "\t\t    idle_cpu(env->best_cpu)) {",
            "\t\t\tcpu = env->best_cpu;",
            "\t\t}",
            "",
            "\t\tenv->dst_cpu = cpu;",
            "\t}",
            "",
            "\ttask_numa_assign(env, cur, imp);",
            "",
            "\t/*",
            "\t * If a move to idle is allowed because there is capacity or load",
            "\t * balance improves then stop the search. While a better swap",
            "\t * candidate may exist, a search is not free.",
            "\t */",
            "\tif (maymove && !cur && env->best_cpu >= 0 && idle_cpu(env->best_cpu))",
            "\t\tstopsearch = true;",
            "",
            "\t/*",
            "\t * If a swap candidate must be identified and the current best task",
            "\t * moves its preferred node then stop the search.",
            "\t */",
            "\tif (!maymove && env->best_task &&",
            "\t    env->best_task->numa_preferred_nid == env->src_nid) {",
            "\t\tstopsearch = true;",
            "\t}",
            "unlock:",
            "\trcu_read_unlock();",
            "",
            "\treturn stopsearch;",
            "}"
          ],
          "function_name": "load_too_imbalanced, task_numa_compare",
          "description": "评估负载不平衡程度，通过任务权重差值对比候选任务，优先选择能改善负载且降低远程访问的迁移目标",
          "similarity": 0.5103037357330322
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/sched/fair.c",
          "start_line": 1758,
          "end_line": 1870,
          "content": [
            "static inline unsigned long task_weight(struct task_struct *p, int nid,",
            "\t\t\t\t\tint dist)",
            "{",
            "\tunsigned long faults, total_faults;",
            "",
            "\tif (!p->numa_faults)",
            "\t\treturn 0;",
            "",
            "\ttotal_faults = p->total_numa_faults;",
            "",
            "\tif (!total_faults)",
            "\t\treturn 0;",
            "",
            "\tfaults = task_faults(p, nid);",
            "\tfaults += score_nearby_nodes(p, nid, dist, true);",
            "",
            "\treturn 1000 * faults / total_faults;",
            "}",
            "static inline unsigned long group_weight(struct task_struct *p, int nid,",
            "\t\t\t\t\t int dist)",
            "{",
            "\tstruct numa_group *ng = deref_task_numa_group(p);",
            "\tunsigned long faults, total_faults;",
            "",
            "\tif (!ng)",
            "\t\treturn 0;",
            "",
            "\ttotal_faults = ng->total_faults;",
            "",
            "\tif (!total_faults)",
            "\t\treturn 0;",
            "",
            "\tfaults = group_faults(p, nid);",
            "\tfaults += score_nearby_nodes(p, nid, dist, false);",
            "",
            "\treturn 1000 * faults / total_faults;",
            "}",
            "static inline bool cpupid_valid(int cpupid)",
            "{",
            "\treturn cpupid_to_cpu(cpupid) < nr_cpu_ids;",
            "}",
            "static bool pgdat_free_space_enough(struct pglist_data *pgdat)",
            "{",
            "\tint z;",
            "\tunsigned long enough_wmark;",
            "",
            "\tenough_wmark = max(1UL * 1024 * 1024 * 1024 >> PAGE_SHIFT,",
            "\t\t\t   pgdat->node_present_pages >> 4);",
            "\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {",
            "\t\tstruct zone *zone = pgdat->node_zones + z;",
            "",
            "\t\tif (!populated_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (zone_watermark_ok(zone, 0,",
            "\t\t\t\t      wmark_pages(zone, WMARK_PROMO) + enough_wmark,",
            "\t\t\t\t      ZONE_MOVABLE, 0))",
            "\t\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "static int numa_hint_fault_latency(struct folio *folio)",
            "{",
            "\tint last_time, time;",
            "",
            "\ttime = jiffies_to_msecs(jiffies);",
            "\tlast_time = folio_xchg_access_time(folio, time);",
            "",
            "\treturn (time - last_time) & PAGE_ACCESS_TIME_MASK;",
            "}",
            "static bool numa_promotion_rate_limit(struct pglist_data *pgdat,",
            "\t\t\t\t      unsigned long rate_limit, int nr)",
            "{",
            "\tunsigned long nr_cand;",
            "\tunsigned int now, start;",
            "",
            "\tnow = jiffies_to_msecs(jiffies);",
            "\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE, nr);",
            "\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);",
            "\tstart = pgdat->nbp_rl_start;",
            "\tif (now - start > MSEC_PER_SEC &&",
            "\t    cmpxchg(&pgdat->nbp_rl_start, start, now) == start)",
            "\t\tpgdat->nbp_rl_nr_cand = nr_cand;",
            "\tif (nr_cand - pgdat->nbp_rl_nr_cand >= rate_limit)",
            "\t\treturn true;",
            "\treturn false;",
            "}",
            "static void numa_promotion_adjust_threshold(struct pglist_data *pgdat,",
            "\t\t\t\t\t    unsigned long rate_limit,",
            "\t\t\t\t\t    unsigned int ref_th)",
            "{",
            "\tunsigned int now, start, th_period, unit_th, th;",
            "\tunsigned long nr_cand, ref_cand, diff_cand;",
            "",
            "\tnow = jiffies_to_msecs(jiffies);",
            "\tth_period = sysctl_numa_balancing_scan_period_max;",
            "\tstart = pgdat->nbp_th_start;",
            "\tif (now - start > th_period &&",
            "\t    cmpxchg(&pgdat->nbp_th_start, start, now) == start) {",
            "\t\tref_cand = rate_limit *",
            "\t\t\tsysctl_numa_balancing_scan_period_max / MSEC_PER_SEC;",
            "\t\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);",
            "\t\tdiff_cand = nr_cand - pgdat->nbp_th_nr_cand;",
            "\t\tunit_th = ref_th * 2 / NUMA_MIGRATION_ADJUST_STEPS;",
            "\t\tth = pgdat->nbp_threshold ? : ref_th;",
            "\t\tif (diff_cand > ref_cand * 11 / 10)",
            "\t\t\tth = max(th - unit_th, unit_th);",
            "\t\telse if (diff_cand < ref_cand * 9 / 10)",
            "\t\t\tth = min(th + unit_th, ref_th * 2);",
            "\t\tpgdat->nbp_th_nr_cand = nr_cand;",
            "\t\tpgdat->nbp_threshold = th;",
            "\t}",
            "}"
          ],
          "function_name": "task_weight, group_weight, cpupid_valid, pgdat_free_space_enough, numa_hint_fault_latency, numa_promotion_rate_limit, numa_promotion_adjust_threshold",
          "description": "计算任务或NUMA组在特定节点的权重，评估内存访问模式；检查CPUPID有效性；判断节点是否有足够空闲空间支持内存迁移；计算页面访问延迟及迁移速率限制；动态调整迁移阈值以优化NUMA平衡",
          "similarity": 0.49840500950813293
        },
        {
          "chunk_id": 17,
          "file_path": "kernel/sched/fair.c",
          "start_line": 2878,
          "end_line": 2989,
          "content": [
            "static void task_numa_placement(struct task_struct *p)",
            "{",
            "\tint seq, nid, max_nid = NUMA_NO_NODE;",
            "\tunsigned long max_faults = 0;",
            "\tunsigned long fault_types[2] = { 0, 0 };",
            "\tunsigned long total_faults;",
            "\tu64 runtime, period;",
            "\tspinlock_t *group_lock = NULL;",
            "\tstruct numa_group *ng;",
            "",
            "\t/*",
            "\t * The p->mm->numa_scan_seq field gets updated without",
            "\t * exclusive access. Use READ_ONCE() here to ensure",
            "\t * that the field is read in a single access:",
            "\t */",
            "\tseq = READ_ONCE(p->mm->numa_scan_seq);",
            "\tif (p->numa_scan_seq == seq)",
            "\t\treturn;",
            "\tp->numa_scan_seq = seq;",
            "\tp->numa_scan_period_max = task_scan_max(p);",
            "",
            "\ttotal_faults = p->numa_faults_locality[0] +",
            "\t\t       p->numa_faults_locality[1];",
            "\truntime = numa_get_avg_runtime(p, &period);",
            "",
            "\t/* If the task is part of a group prevent parallel updates to group stats */",
            "\tng = deref_curr_numa_group(p);",
            "\tif (ng) {",
            "\t\tgroup_lock = &ng->lock;",
            "\t\tspin_lock_irq(group_lock);",
            "\t}",
            "",
            "\t/* Find the node with the highest number of faults */",
            "\tfor_each_online_node(nid) {",
            "\t\t/* Keep track of the offsets in numa_faults array */",
            "\t\tint mem_idx, membuf_idx, cpu_idx, cpubuf_idx;",
            "\t\tunsigned long faults = 0, group_faults = 0;",
            "\t\tint priv;",
            "",
            "\t\tfor (priv = 0; priv < NR_NUMA_HINT_FAULT_TYPES; priv++) {",
            "\t\t\tlong diff, f_diff, f_weight;",
            "",
            "\t\t\tmem_idx = task_faults_idx(NUMA_MEM, nid, priv);",
            "\t\t\tmembuf_idx = task_faults_idx(NUMA_MEMBUF, nid, priv);",
            "\t\t\tcpu_idx = task_faults_idx(NUMA_CPU, nid, priv);",
            "\t\t\tcpubuf_idx = task_faults_idx(NUMA_CPUBUF, nid, priv);",
            "",
            "\t\t\t/* Decay existing window, copy faults since last scan */",
            "\t\t\tdiff = p->numa_faults[membuf_idx] - p->numa_faults[mem_idx] / 2;",
            "\t\t\tfault_types[priv] += p->numa_faults[membuf_idx];",
            "\t\t\tp->numa_faults[membuf_idx] = 0;",
            "",
            "\t\t\t/*",
            "\t\t\t * Normalize the faults_from, so all tasks in a group",
            "\t\t\t * count according to CPU use, instead of by the raw",
            "\t\t\t * number of faults. Tasks with little runtime have",
            "\t\t\t * little over-all impact on throughput, and thus their",
            "\t\t\t * faults are less important.",
            "\t\t\t */",
            "\t\t\tf_weight = div64_u64(runtime << 16, period + 1);",
            "\t\t\tf_weight = (f_weight * p->numa_faults[cpubuf_idx]) /",
            "\t\t\t\t   (total_faults + 1);",
            "\t\t\tf_diff = f_weight - p->numa_faults[cpu_idx] / 2;",
            "\t\t\tp->numa_faults[cpubuf_idx] = 0;",
            "",
            "\t\t\tp->numa_faults[mem_idx] += diff;",
            "\t\t\tp->numa_faults[cpu_idx] += f_diff;",
            "\t\t\tfaults += p->numa_faults[mem_idx];",
            "\t\t\tp->total_numa_faults += diff;",
            "\t\t\tif (ng) {",
            "\t\t\t\t/*",
            "\t\t\t\t * safe because we can only change our own group",
            "\t\t\t\t *",
            "\t\t\t\t * mem_idx represents the offset for a given",
            "\t\t\t\t * nid and priv in a specific region because it",
            "\t\t\t\t * is at the beginning of the numa_faults array.",
            "\t\t\t\t */",
            "\t\t\t\tng->faults[mem_idx] += diff;",
            "\t\t\t\tng->faults[cpu_idx] += f_diff;",
            "\t\t\t\tng->total_faults += diff;",
            "\t\t\t\tgroup_faults += ng->faults[mem_idx];",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tif (!ng) {",
            "\t\t\tif (faults > max_faults) {",
            "\t\t\t\tmax_faults = faults;",
            "\t\t\t\tmax_nid = nid;",
            "\t\t\t}",
            "\t\t} else if (group_faults > max_faults) {",
            "\t\t\tmax_faults = group_faults;",
            "\t\t\tmax_nid = nid;",
            "\t\t}",
            "\t}",
            "",
            "\t/* Cannot migrate task to CPU-less node */",
            "\tmax_nid = numa_nearest_node(max_nid, N_CPU);",
            "",
            "\tif (ng) {",
            "\t\tnuma_group_count_active_nodes(ng);",
            "\t\tspin_unlock_irq(group_lock);",
            "\t\tmax_nid = preferred_group_nid(p, max_nid);",
            "\t}",
            "",
            "\tif (max_faults) {",
            "\t\t/* Set the new preferred node */",
            "\t\tif (max_nid != p->numa_preferred_nid)",
            "\t\t\tsched_setnuma(p, max_nid);",
            "\t}",
            "",
            "\tupdate_task_scan_period(p, fault_types[0], fault_types[1]);",
            "}"
          ],
          "function_name": "task_numa_placement",
          "description": "task_numa_placement遍历所有在线节点，根据NUMA故障次数确定首选节点，更新任务的numa_preferred_nid，并调整扫描周期参数。",
          "similarity": 0.49585315585136414
        },
        {
          "chunk_id": 31,
          "file_path": "kernel/sched/fair.c",
          "start_line": 5156,
          "end_line": 5323,
          "content": [
            "static inline int task_fits_cpu(struct task_struct *p, int cpu)",
            "{",
            "\tunsigned long uclamp_min = uclamp_eff_value(p, UCLAMP_MIN);",
            "\tunsigned long uclamp_max = uclamp_eff_value(p, UCLAMP_MAX);",
            "\tunsigned long util = task_util_est(p);",
            "\t/*",
            "\t * Return true only if the cpu fully fits the task requirements, which",
            "\t * include the utilization but also the performance hints.",
            "\t */",
            "\treturn (util_fits_cpu(util, uclamp_min, uclamp_max, cpu) > 0);",
            "}",
            "static inline void update_misfit_status(struct task_struct *p, struct rq *rq)",
            "{",
            "\tint cpu = cpu_of(rq);",
            "",
            "\tif (!sched_asym_cpucap_active())",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Affinity allows us to go somewhere higher?  Or are we on biggest",
            "\t * available CPU already? Or do we fit into this CPU ?",
            "\t */",
            "\tif (!p || (p->nr_cpus_allowed == 1) ||",
            "\t    (arch_scale_cpu_capacity(cpu) == p->max_allowed_capacity) ||",
            "\t    task_fits_cpu(p, cpu)) {",
            "",
            "\t\trq->misfit_task_load = 0;",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Make sure that misfit_task_load will not be null even if",
            "\t * task_h_load() returns 0.",
            "\t */",
            "\trq->misfit_task_load = max_t(unsigned long, task_h_load(p), 1);",
            "}",
            "static inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)",
            "{",
            "\treturn !cfs_rq->nr_running;",
            "}",
            "static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)",
            "{",
            "\tcfs_rq_util_change(cfs_rq, 0);",
            "}",
            "static inline void remove_entity_load_avg(struct sched_entity *se) {}",
            "static inline void",
            "attach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) {}",
            "static inline void",
            "detach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) {}",
            "static inline int sched_balance_newidle(struct rq *rq, struct rq_flags *rf)",
            "{",
            "\treturn 0;",
            "}",
            "static inline void",
            "util_est_enqueue(struct cfs_rq *cfs_rq, struct task_struct *p) {}",
            "static inline void",
            "util_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p) {}",
            "static inline void",
            "util_est_update(struct cfs_rq *cfs_rq, struct task_struct *p,",
            "\t\tbool task_sleep) {}",
            "static inline void update_misfit_status(struct task_struct *p, struct rq *rq) {}",
            "static void",
            "place_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)",
            "{",
            "\tu64 vslice, vruntime = avg_vruntime(cfs_rq);",
            "\ts64 lag = 0;",
            "",
            "\tif (!se->custom_slice)",
            "\t\tse->slice = sysctl_sched_base_slice;",
            "\tvslice = calc_delta_fair(se->slice, se);",
            "",
            "\t/*",
            "\t * Due to how V is constructed as the weighted average of entities,",
            "\t * adding tasks with positive lag, or removing tasks with negative lag",
            "\t * will move 'time' backwards, this can screw around with the lag of",
            "\t * other tasks.",
            "\t *",
            "\t * EEVDF: placement strategy #1 / #2",
            "\t */",
            "\tif (sched_feat(PLACE_LAG) && cfs_rq->nr_running) {",
            "\t\tstruct sched_entity *curr = cfs_rq->curr;",
            "\t\tunsigned long load;",
            "",
            "\t\tlag = se->vlag;",
            "",
            "\t\t/*",
            "\t\t * If we want to place a task and preserve lag, we have to",
            "\t\t * consider the effect of the new entity on the weighted",
            "\t\t * average and compensate for this, otherwise lag can quickly",
            "\t\t * evaporate.",
            "\t\t *",
            "\t\t * Lag is defined as:",
            "\t\t *",
            "\t\t *   lag_i = S - s_i = w_i * (V - v_i)",
            "\t\t *",
            "\t\t * To avoid the 'w_i' term all over the place, we only track",
            "\t\t * the virtual lag:",
            "\t\t *",
            "\t\t *   vl_i = V - v_i <=> v_i = V - vl_i",
            "\t\t *",
            "\t\t * And we take V to be the weighted average of all v:",
            "\t\t *",
            "\t\t *   V = (\\Sum w_j*v_j) / W",
            "\t\t *",
            "\t\t * Where W is: \\Sum w_j",
            "\t\t *",
            "\t\t * Then, the weighted average after adding an entity with lag",
            "\t\t * vl_i is given by:",
            "\t\t *",
            "\t\t *   V' = (\\Sum w_j*v_j + w_i*v_i) / (W + w_i)",
            "\t\t *      = (W*V + w_i*(V - vl_i)) / (W + w_i)",
            "\t\t *      = (W*V + w_i*V - w_i*vl_i) / (W + w_i)",
            "\t\t *      = (V*(W + w_i) - w_i*l) / (W + w_i)",
            "\t\t *      = V - w_i*vl_i / (W + w_i)",
            "\t\t *",
            "\t\t * And the actual lag after adding an entity with vl_i is:",
            "\t\t *",
            "\t\t *   vl'_i = V' - v_i",
            "\t\t *         = V - w_i*vl_i / (W + w_i) - (V - vl_i)",
            "\t\t *         = vl_i - w_i*vl_i / (W + w_i)",
            "\t\t *",
            "\t\t * Which is strictly less than vl_i. So in order to preserve lag",
            "\t\t * we should inflate the lag before placement such that the",
            "\t\t * effective lag after placement comes out right.",
            "\t\t *",
            "\t\t * As such, invert the above relation for vl'_i to get the vl_i",
            "\t\t * we need to use such that the lag after placement is the lag",
            "\t\t * we computed before dequeue.",
            "\t\t *",
            "\t\t *   vl'_i = vl_i - w_i*vl_i / (W + w_i)",
            "\t\t *         = ((W + w_i)*vl_i - w_i*vl_i) / (W + w_i)",
            "\t\t *",
            "\t\t *   (W + w_i)*vl'_i = (W + w_i)*vl_i - w_i*vl_i",
            "\t\t *                   = W*vl_i",
            "\t\t *",
            "\t\t *   vl_i = (W + w_i)*vl'_i / W",
            "\t\t */",
            "\t\tload = cfs_rq->avg_load;",
            "\t\tif (curr && curr->on_rq)",
            "\t\t\tload += scale_load_down(curr->load.weight);",
            "",
            "\t\tlag *= load + scale_load_down(se->load.weight);",
            "\t\tif (WARN_ON_ONCE(!load))",
            "\t\t\tload = 1;",
            "\t\tlag = div_s64(lag, load);",
            "\t}",
            "",
            "\tse->vruntime = vruntime - lag;",
            "",
            "\tif (sched_feat(PLACE_REL_DEADLINE) && se->rel_deadline) {",
            "\t\tse->deadline += se->vruntime;",
            "\t\tse->rel_deadline = 0;",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * When joining the competition; the existing tasks will be,",
            "\t * on average, halfway through their slice, as such start tasks",
            "\t * off with half a slice to ease into the competition.",
            "\t */",
            "\tif (sched_feat(PLACE_DEADLINE_INITIAL) && (flags & ENQUEUE_INITIAL))",
            "\t\tvslice /= 2;",
            "",
            "\t/*",
            "\t * EEVDF: vd_i = ve_i + r_i/w_i",
            "\t */",
            "\tse->deadline = se->vruntime + vslice;",
            "}"
          ],
          "function_name": "task_fits_cpu, update_misfit_status, cfs_rq_is_decayed, update_load_avg, remove_entity_load_avg, attach_entity_load_avg, detach_entity_load_avg, sched_balance_newidle, util_est_enqueue, util_est_dequeue, util_est_update, update_misfit_status, place_entity",
          "description": "task_fits_cpu 检查任务是否适配目标CPU；update_misfit_status 更新运行队列误配标记；cfs_rq_is_decayed 判断CFS队列是否空闲；update_load_avg 系列函数用于负载统计更新；sched_balance_newidle 新空闲CPU平衡逻辑；util_est系列函数处理利用率估计；place_entity 实现EEVDF算法的任务放置策略",
          "similarity": 0.49249276518821716
        }
      ]
    }
  ]
}