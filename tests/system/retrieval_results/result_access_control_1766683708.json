{
  "query": "access control",
  "timestamp": "2025-12-26 01:28:28",
  "retrieved_files": [
    {
      "source_file": "kernel/printk/nbcon.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:32:39\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `printk\\nbcon.c`\n\n---\n\n# printk/nbcon.c 技术文档\n\n## 文件概述\n\n`printk/nbcon.c` 实现了新一代的 printk 控制台（nbcon，即 \"new console\"）打印机制，该机制不依赖传统的 `console_lock` 互斥锁，而是采用基于原子操作的状态机模型来管理控制台访问。其核心目标是支持高优先级上下文（如中断、NMI、panic）安全、高效地抢占低优先级上下文对控制台的使用权，同时避免死锁和优先级反转问题。该机制特别适用于实时系统和 panic 场景下的可靠日志输出。\n\n## 核心功能\n\n### 主要数据结构\n- **`struct nbcon_state`**：封装控制台状态的原子变量，包含以下关键字段：\n  - `prio`：当前持有控制台的上下文优先级（0 表示未锁定）\n  - `cpu`：当前持有控制台的 CPU 编号\n  - `req_prio`：请求友好移交的更高优先级上下文的优先级\n  - `unsafe`：标志当前是否处于不安全状态（如正在操作共享资源）\n  - `unsafe_takeover`：标志是否发生过不安全的强制接管\n\n### 主要函数\n- **`nbcon_state_set()`**：初始化或重置控制台状态（仅限未注册或初始化阶段使用）\n- **`nbcon_state_read()`**：原子读取当前控制台状态\n- **`nbcon_state_try_cmpxchg()`**：原子比较并交换控制台状态\n- **`nbcon_seq_read()`**：读取控制台当前应打印的 printk 记录序列号\n- **`nbcon_seq_force()`**：强制设置控制台序列号（用于初始化或 panic 场景）\n- **`nbcon_seq_try_update()`**：尝试原子更新控制台序列号\n- **`nbcon_context_try_acquire_direct()`**：尝试直接获取控制台所有权（核心获取逻辑之一）\n\n## 关键实现\n\n### 控制台状态管理机制\n控制台状态通过 `nbcon_state` 原子变量管理，支持三种获取策略：\n1. **直接获取**：当控制台未被占用，或当前持有者优先级更低且处于安全状态时，直接抢占。\n2. **友好移交**：当持有者优先级更低但处于不安全状态时，请求者设置 `req_prio`，持有者在退出不安全状态后主动释放。\n3. **强制接管**：仅在 `panic()` 的最后尝试中使用，无视不安全状态强制接管（标记 `unsafe_takeover`）。\n\n### 安全状态标记\n- **`unsafe` 字段**：在操作共享资源或控制台设备时置位，操作完成后清除。确保高优先级上下文不会在设备不一致状态下接管。\n- **`unsafe_takeover` 字段**：记录强制接管事件，后续需重新初始化控制台状态。\n\n### 序列号管理\n- 使用 64 位序列号跟踪下一条待打印的 printk 记录。\n- 在 32 位系统上，仅存储低 32 位，高 32 位通过 ringbuffer 中的有效序列号推导。\n- `nbcon_seq_force()` 确保设置的序列号不低于 ringbuffer 中最早的有效记录。\n\n### 优先级与 CPU 绑定\n- 优先级数值越大表示优先级越高（`NBCON_PRIO_NONE = 0` 表示无持有者）。\n- `cpu` 字段防止同 CPU 上的忙等待，并处理复杂场景下优先级相同但 CPU 切换的情况。\n\n## 依赖关系\n\n- **内部依赖**：\n  - `printk_ringbuffer.h`：提供 printk 环形缓冲区操作接口（如 `prb_first_valid_seq()`）\n  - `internal.h`：包含 nbcon 内部定义的辅助宏和类型（如 `ACCESS_PRIVATE`、`__ulseq_to_u64seq`）\n- **内核核心模块**：\n  - `linux/atomic.h`：提供原子操作支持\n  - `linux/console.h`：定义 `struct console` 及相关常量\n  - `linux/irqflags.h`：用于中断上下文判断\n  - `linux/smp.h`：SMP 相关支持（如 `smp_processor_id()`）\n- **关键子系统**：\n  - **Printk 子系统**：作为 printk 输出后端，与 ringbuffer 紧密集成\n  - **调度器/中断子系统**：依赖上下文优先级模型（如 NMI > IRQ > 进程）\n\n## 使用场景\n\n1. **常规 printk 输出**：\n   - 高优先级中断/NMI 日志可安全抢占低优先级进程的控制台输出。\n   - 每条 printk 记录独立输出，被抢占时由新持有者重试整条记录。\n\n2. **Panic 处理**：\n   - 在 `panic()` 流程中，通过强制接管机制确保最后的日志能输出。\n   - 使用独立的 panic 记录缓冲区，避免因不安全状态导致数据损坏。\n\n3. **实时系统**：\n   - 避免传统 `console_lock` 导致的优先级反转问题。\n   - 保证高优先级任务/中断的日志能及时输出。\n\n4. **控制台驱动实现**：\n   - 控制台驱动需在关键操作（如设备寄存器访问）前后标记 `unsafe` 状态。\n   - 驱动需在每次输出字符后检查移交请求（`req_prio`），及时释放控制台。",
      "similarity": 0.5616209506988525,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/printk/nbcon.c",
          "start_line": 128,
          "end_line": 242,
          "content": [
            "static inline void nbcon_state_set(struct console *con, struct nbcon_state *new)",
            "{",
            "\tatomic_set(&ACCESS_PRIVATE(con, nbcon_state), new->atom);",
            "}",
            "static inline void nbcon_state_read(struct console *con, struct nbcon_state *state)",
            "{",
            "\tstate->atom = atomic_read(&ACCESS_PRIVATE(con, nbcon_state));",
            "}",
            "static inline bool nbcon_state_try_cmpxchg(struct console *con, struct nbcon_state *cur,",
            "\t\t\t\t\t   struct nbcon_state *new)",
            "{",
            "\treturn atomic_try_cmpxchg(&ACCESS_PRIVATE(con, nbcon_state), &cur->atom, new->atom);",
            "}",
            "u64 nbcon_seq_read(struct console *con)",
            "{",
            "\tunsigned long nbcon_seq = atomic_long_read(&ACCESS_PRIVATE(con, nbcon_seq));",
            "",
            "\treturn __ulseq_to_u64seq(prb, nbcon_seq);",
            "}",
            "void nbcon_seq_force(struct console *con, u64 seq)",
            "{",
            "\t/*",
            "\t * If the specified record no longer exists, the oldest available record",
            "\t * is chosen. This is especially important on 32bit systems because only",
            "\t * the lower 32 bits of the sequence number are stored. The upper 32 bits",
            "\t * are derived from the sequence numbers available in the ringbuffer.",
            "\t */",
            "\tu64 valid_seq = max_t(u64, seq, prb_first_valid_seq(prb));",
            "",
            "\tatomic_long_set(&ACCESS_PRIVATE(con, nbcon_seq), __u64seq_to_ulseq(valid_seq));",
            "}",
            "static void nbcon_seq_try_update(struct nbcon_context *ctxt, u64 new_seq)",
            "{",
            "\tunsigned long nbcon_seq = __u64seq_to_ulseq(ctxt->seq);",
            "\tstruct console *con = ctxt->console;",
            "",
            "\tif (atomic_long_try_cmpxchg(&ACCESS_PRIVATE(con, nbcon_seq), &nbcon_seq,",
            "\t\t\t\t    __u64seq_to_ulseq(new_seq))) {",
            "\t\tctxt->seq = new_seq;",
            "\t} else {",
            "\t\tctxt->seq = nbcon_seq_read(con);",
            "\t}",
            "}",
            "static int nbcon_context_try_acquire_direct(struct nbcon_context *ctxt,",
            "\t\t\t\t\t    struct nbcon_state *cur)",
            "{",
            "\tunsigned int cpu = smp_processor_id();",
            "\tstruct console *con = ctxt->console;",
            "\tstruct nbcon_state new;",
            "",
            "\tdo {",
            "\t\t/*",
            "\t\t * Panic does not imply that the console is owned. However, it",
            "\t\t * is critical that non-panic CPUs during panic are unable to",
            "\t\t * acquire ownership in order to satisfy the assumptions of",
            "\t\t * nbcon_waiter_matches(). In particular, the assumption that",
            "\t\t * lower priorities are ignored during panic.",
            "\t\t */",
            "\t\tif (other_cpu_in_panic())",
            "\t\t\treturn -EPERM;",
            "",
            "\t\tif (ctxt->prio <= cur->prio || ctxt->prio <= cur->req_prio)",
            "\t\t\treturn -EPERM;",
            "",
            "\t\tif (cur->unsafe)",
            "\t\t\treturn -EBUSY;",
            "",
            "\t\t/*",
            "\t\t * The console should never be safe for a direct acquire",
            "\t\t * if an unsafe hostile takeover has ever happened.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(cur->unsafe_takeover);",
            "",
            "\t\tnew.atom = cur->atom;",
            "\t\tnew.prio\t= ctxt->prio;",
            "\t\tnew.req_prio\t= NBCON_PRIO_NONE;",
            "\t\tnew.unsafe\t= cur->unsafe_takeover;",
            "\t\tnew.cpu\t\t= cpu;",
            "",
            "\t} while (!nbcon_state_try_cmpxchg(con, cur, &new));",
            "",
            "\treturn 0;",
            "}",
            "static bool nbcon_waiter_matches(struct nbcon_state *cur, int expected_prio)",
            "{",
            "\t/*",
            "\t * The request context is well defined by the @req_prio because:",
            "\t *",
            "\t * - Only a context with a priority higher than the owner can become",
            "\t *   a waiter.",
            "\t * - Only a context with a priority higher than the waiter can",
            "\t *   directly take over the request.",
            "\t * - There are only three priorities.",
            "\t * - Only one CPU is allowed to request PANIC priority.",
            "\t * - Lower priorities are ignored during panic() until reboot.",
            "\t *",
            "\t * As a result, the following scenario is *not* possible:",
            "\t *",
            "\t * 1. This context is currently a waiter.",
            "\t * 2. Another context with a higher priority than this context",
            "\t *    directly takes ownership.",
            "\t * 3. The higher priority context releases the ownership.",
            "\t * 4. Another lower priority context takes the ownership.",
            "\t * 5. Another context with the same priority as this context",
            "\t *    creates a request and starts waiting.",
            "\t *",
            "\t * Event #1 implies this context is EMERGENCY.",
            "\t * Event #2 implies the new context is PANIC.",
            "\t * Event #3 occurs when panic() has flushed the console.",
            "\t * Events #4 and #5 are not possible due to the other_cpu_in_panic()",
            "\t * check in nbcon_context_try_acquire_direct().",
            "\t */",
            "",
            "\treturn (cur->req_prio == expected_prio);",
            "}"
          ],
          "function_name": "nbcon_state_set, nbcon_state_read, nbcon_state_try_cmpxchg, nbcon_seq_read, nbcon_seq_force, nbcon_seq_try_update, nbcon_context_try_acquire_direct, nbcon_waiter_matches",
          "description": "实现直接获取控制台的逻辑，包含优先级检查、安全状态验证和状态原子更新，通过cmpxchg保证并发安全性，处理跨CPU和panic场景下的所有权转移规则。",
          "similarity": 0.6165627837181091
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/printk/nbcon.c",
          "start_line": 651,
          "end_line": 757,
          "content": [
            "static void nbcon_context_release(struct nbcon_context *ctxt)",
            "{",
            "\tunsigned int cpu = smp_processor_id();",
            "\tstruct console *con = ctxt->console;",
            "\tstruct nbcon_state cur;",
            "\tstruct nbcon_state new;",
            "",
            "\tnbcon_state_read(con, &cur);",
            "",
            "\tdo {",
            "\t\tif (!nbcon_owner_matches(&cur, cpu, ctxt->prio))",
            "\t\t\tbreak;",
            "",
            "\t\tnew.atom = cur.atom;",
            "\t\tnew.prio = NBCON_PRIO_NONE;",
            "",
            "\t\t/*",
            "\t\t * If @unsafe_takeover is set, it is kept set so that",
            "\t\t * the state remains permanently unsafe.",
            "\t\t */",
            "\t\tnew.unsafe |= cur.unsafe_takeover;",
            "",
            "\t} while (!nbcon_state_try_cmpxchg(con, &cur, &new));",
            "",
            "\tctxt->pbufs = NULL;",
            "}",
            "static bool nbcon_context_can_proceed(struct nbcon_context *ctxt, struct nbcon_state *cur)",
            "{",
            "\tunsigned int cpu = smp_processor_id();",
            "",
            "\t/* Make sure this context still owns the console. */",
            "\tif (!nbcon_owner_matches(cur, cpu, ctxt->prio))",
            "\t\treturn false;",
            "",
            "\t/* The console owner can proceed if there is no waiter. */",
            "\tif (cur->req_prio == NBCON_PRIO_NONE)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * A console owner within an unsafe region is always allowed to",
            "\t * proceed, even if there are waiters. It can perform a handover",
            "\t * when exiting the unsafe region. Otherwise the waiter will",
            "\t * need to perform an unsafe hostile takeover.",
            "\t */",
            "\tif (cur->unsafe)",
            "\t\treturn true;",
            "",
            "\t/* Waiters always have higher priorities than owners. */",
            "\tWARN_ON_ONCE(cur->req_prio <= cur->prio);",
            "",
            "\t/*",
            "\t * Having a safe point for take over and eventually a few",
            "\t * duplicated characters or a full line is way better than a",
            "\t * hostile takeover. Post processing can take care of the garbage.",
            "\t * Release and hand over.",
            "\t */",
            "\tnbcon_context_release(ctxt);",
            "",
            "\t/*",
            "\t * It is not clear whether the waiter really took over ownership. The",
            "\t * outermost callsite must make the final decision whether console",
            "\t * ownership is needed for it to proceed. If yes, it must reacquire",
            "\t * ownership (possibly hostile) before carefully proceeding.",
            "\t *",
            "\t * The calling context no longer owns the console so go back all the",
            "\t * way instead of trying to implement reacquire heuristics in tons of",
            "\t * places.",
            "\t */",
            "\treturn false;",
            "}",
            "bool nbcon_can_proceed(struct nbcon_write_context *wctxt)",
            "{",
            "\tstruct nbcon_context *ctxt = &ACCESS_PRIVATE(wctxt, ctxt);",
            "\tstruct console *con = ctxt->console;",
            "\tstruct nbcon_state cur;",
            "",
            "\tnbcon_state_read(con, &cur);",
            "",
            "\treturn nbcon_context_can_proceed(ctxt, &cur);",
            "}",
            "static bool __nbcon_context_update_unsafe(struct nbcon_context *ctxt, bool unsafe)",
            "{",
            "\tstruct console *con = ctxt->console;",
            "\tstruct nbcon_state cur;",
            "\tstruct nbcon_state new;",
            "",
            "\tnbcon_state_read(con, &cur);",
            "",
            "\tdo {",
            "\t\t/*",
            "\t\t * The unsafe bit must not be cleared if an",
            "\t\t * unsafe hostile takeover has occurred.",
            "\t\t */",
            "\t\tif (!unsafe && cur.unsafe_takeover)",
            "\t\t\tgoto out;",
            "",
            "\t\tif (!nbcon_context_can_proceed(ctxt, &cur))",
            "\t\t\treturn false;",
            "",
            "\t\tnew.atom = cur.atom;",
            "\t\tnew.unsafe = unsafe;",
            "\t} while (!nbcon_state_try_cmpxchg(con, &cur, &new));",
            "",
            "\tcur.atom = new.atom;",
            "out:",
            "\treturn nbcon_context_can_proceed(ctxt, &cur);",
            "}"
          ],
          "function_name": "nbcon_context_release, nbcon_context_can_proceed, nbcon_can_proceed, __nbcon_context_update_unsafe",
          "description": "实现控制台释放与状态检查逻辑，包含安全状态维护、所有权验证和异常处理，通过循环CAS更新状态并在不安全状态下阻止进一步操作。",
          "similarity": 0.5917773246765137
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/printk/nbcon.c",
          "start_line": 335,
          "end_line": 473,
          "content": [
            "static int nbcon_context_try_acquire_requested(struct nbcon_context *ctxt,",
            "\t\t\t\t\t       struct nbcon_state *cur)",
            "{",
            "\tunsigned int cpu = smp_processor_id();",
            "\tstruct console *con = ctxt->console;",
            "\tstruct nbcon_state new;",
            "",
            "\t/* Note that the caller must still remove the request! */",
            "\tif (other_cpu_in_panic())",
            "\t\treturn -EPERM;",
            "",
            "\t/*",
            "\t * Note that the waiter will also change if there was an unsafe",
            "\t * hostile takeover.",
            "\t */",
            "\tif (!nbcon_waiter_matches(cur, ctxt->prio))",
            "\t\treturn -EPERM;",
            "",
            "\t/* If still locked, caller should continue waiting. */",
            "\tif (cur->prio != NBCON_PRIO_NONE)",
            "\t\treturn -EBUSY;",
            "",
            "\t/*",
            "\t * The previous owner should have never released ownership",
            "\t * in an unsafe region.",
            "\t */",
            "\tWARN_ON_ONCE(cur->unsafe);",
            "",
            "\tnew.atom = cur->atom;",
            "\tnew.prio\t= ctxt->prio;",
            "\tnew.req_prio\t= NBCON_PRIO_NONE;",
            "\tnew.unsafe\t= cur->unsafe_takeover;",
            "\tnew.cpu\t\t= cpu;",
            "",
            "\tif (!nbcon_state_try_cmpxchg(con, cur, &new)) {",
            "\t\t/*",
            "\t\t * The acquire could fail only when it has been taken",
            "\t\t * over by a higher priority context.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(nbcon_waiter_matches(cur, ctxt->prio));",
            "\t\treturn -EPERM;",
            "\t}",
            "",
            "\t/* Handover success. This context now owns the console. */",
            "\treturn 0;",
            "}",
            "static int nbcon_context_try_acquire_handover(struct nbcon_context *ctxt,",
            "\t\t\t\t\t      struct nbcon_state *cur)",
            "{",
            "\tunsigned int cpu = smp_processor_id();",
            "\tstruct console *con = ctxt->console;",
            "\tstruct nbcon_state new;",
            "\tint timeout;",
            "\tint request_err = -EBUSY;",
            "",
            "\t/*",
            "\t * Check that the handover is called when the direct acquire failed",
            "\t * with -EBUSY.",
            "\t */",
            "\tWARN_ON_ONCE(ctxt->prio <= cur->prio || ctxt->prio <= cur->req_prio);",
            "\tWARN_ON_ONCE(!cur->unsafe);",
            "",
            "\t/* Handover is not possible on the same CPU. */",
            "\tif (cur->cpu == cpu)",
            "\t\treturn -EBUSY;",
            "",
            "\t/*",
            "\t * Console stays unsafe after an unsafe takeover until re-initialized.",
            "\t * Waiting is not going to help in this case.",
            "\t */",
            "\tif (cur->unsafe_takeover)",
            "\t\treturn -EBUSY;",
            "",
            "\t/* Is the caller willing to wait? */",
            "\tif (ctxt->spinwait_max_us == 0)",
            "\t\treturn -EBUSY;",
            "",
            "\t/*",
            "\t * Setup a request for the handover. The caller should try to acquire",
            "\t * the console directly when the current state has been modified.",
            "\t */",
            "\tnew.atom = cur->atom;",
            "\tnew.req_prio = ctxt->prio;",
            "\tif (!nbcon_state_try_cmpxchg(con, cur, &new))",
            "\t\treturn -EAGAIN;",
            "",
            "\tcur->atom = new.atom;",
            "",
            "\t/* Wait until there is no owner and then acquire the console. */",
            "\tfor (timeout = ctxt->spinwait_max_us; timeout >= 0; timeout--) {",
            "\t\t/* On successful acquire, this request is cleared. */",
            "\t\trequest_err = nbcon_context_try_acquire_requested(ctxt, cur);",
            "\t\tif (!request_err)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * If the acquire should be aborted, it must be ensured",
            "\t\t * that the request is removed before returning to caller.",
            "\t\t */",
            "\t\tif (request_err == -EPERM)",
            "\t\t\tbreak;",
            "",
            "\t\tudelay(1);",
            "",
            "\t\t/* Re-read the state because some time has passed. */",
            "\t\tnbcon_state_read(con, cur);",
            "\t}",
            "",
            "\t/* Timed out or aborted. Carefully remove handover request. */",
            "\tdo {",
            "\t\t/*",
            "\t\t * No need to remove request if there is a new waiter. This",
            "\t\t * can only happen if a higher priority context has taken over",
            "\t\t * the console or the handover request.",
            "\t\t */",
            "\t\tif (!nbcon_waiter_matches(cur, ctxt->prio))",
            "\t\t\treturn -EPERM;",
            "",
            "\t\t/* Unset request for handover. */",
            "\t\tnew.atom = cur->atom;",
            "\t\tnew.req_prio = NBCON_PRIO_NONE;",
            "\t\tif (nbcon_state_try_cmpxchg(con, cur, &new)) {",
            "\t\t\t/*",
            "\t\t\t * Request successfully unset. Report failure of",
            "\t\t\t * acquiring via handover.",
            "\t\t\t */",
            "\t\t\tcur->atom = new.atom;",
            "\t\t\treturn request_err;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Unable to remove request. Try to acquire in case",
            "\t\t * the owner has released the lock.",
            "\t\t */",
            "\t} while (nbcon_context_try_acquire_requested(ctxt, cur));",
            "",
            "\t/* Lucky timing. The acquire succeeded while removing the request. */",
            "\treturn 0;",
            "}"
          ],
          "function_name": "nbcon_context_try_acquire_requested, nbcon_context_try_acquire_handover",
          "description": "实现友好移交机制，通过设置请求优先级并轮询等待所有者释放，包含超时控制和状态同步，确保高优先级任务可中断低优先级控制台输出。",
          "similarity": 0.5836131572723389
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/printk/nbcon.c",
          "start_line": 524,
          "end_line": 629,
          "content": [
            "static int nbcon_context_try_acquire_hostile(struct nbcon_context *ctxt,",
            "\t\t\t\t\t     struct nbcon_state *cur)",
            "{",
            "\tunsigned int cpu = smp_processor_id();",
            "\tstruct console *con = ctxt->console;",
            "\tstruct nbcon_state new;",
            "",
            "\tif (!ctxt->allow_unsafe_takeover)",
            "\t\treturn -EPERM;",
            "",
            "\t/* Ensure caller is allowed to perform unsafe hostile takeovers. */",
            "\tif (WARN_ON_ONCE(ctxt->prio != NBCON_PRIO_PANIC))",
            "\t\treturn -EPERM;",
            "",
            "\t/*",
            "\t * Check that try_acquire_direct() and try_acquire_handover() returned",
            "\t * -EBUSY in the right situation.",
            "\t */",
            "\tWARN_ON_ONCE(ctxt->prio <= cur->prio || ctxt->prio <= cur->req_prio);",
            "\tWARN_ON_ONCE(cur->unsafe != true);",
            "",
            "\tdo {",
            "\t\tnew.atom = cur->atom;",
            "\t\tnew.cpu\t\t\t= cpu;",
            "\t\tnew.prio\t\t= ctxt->prio;",
            "\t\tnew.unsafe\t\t|= cur->unsafe_takeover;",
            "\t\tnew.unsafe_takeover\t|= cur->unsafe;",
            "",
            "\t} while (!nbcon_state_try_cmpxchg(con, cur, &new));",
            "",
            "\treturn 0;",
            "}",
            "static bool nbcon_context_try_acquire(struct nbcon_context *ctxt)",
            "{",
            "\tunsigned int cpu = smp_processor_id();",
            "\tstruct console *con = ctxt->console;",
            "\tstruct nbcon_state cur;",
            "\tint err;",
            "",
            "\tnbcon_state_read(con, &cur);",
            "try_again:",
            "\terr = nbcon_context_try_acquire_direct(ctxt, &cur);",
            "\tif (err != -EBUSY)",
            "\t\tgoto out;",
            "",
            "\terr = nbcon_context_try_acquire_handover(ctxt, &cur);",
            "\tif (err == -EAGAIN)",
            "\t\tgoto try_again;",
            "\tif (err != -EBUSY)",
            "\t\tgoto out;",
            "",
            "\terr = nbcon_context_try_acquire_hostile(ctxt, &cur);",
            "out:",
            "\tif (err)",
            "\t\treturn false;",
            "",
            "\t/* Acquire succeeded. */",
            "",
            "\t/* Assign the appropriate buffer for this context. */",
            "\tif (atomic_read(&panic_cpu) == cpu)",
            "\t\tctxt->pbufs = &panic_nbcon_pbufs;",
            "\telse",
            "\t\tctxt->pbufs = con->pbufs;",
            "",
            "\t/* Set the record sequence for this context to print. */",
            "\tctxt->seq = nbcon_seq_read(ctxt->console);",
            "",
            "\treturn true;",
            "}",
            "static bool nbcon_owner_matches(struct nbcon_state *cur, int expected_cpu,",
            "\t\t\t\tint expected_prio)",
            "{",
            "\t/*",
            "\t * A similar function, nbcon_waiter_matches(), only deals with",
            "\t * EMERGENCY and PANIC priorities. However, this function must also",
            "\t * deal with the NORMAL priority, which requires additional checks",
            "\t * and constraints.",
            "\t *",
            "\t * For the case where preemption and interrupts are disabled, it is",
            "\t * enough to also verify that the owning CPU has not changed.",
            "\t *",
            "\t * For the case where preemption or interrupts are enabled, an",
            "\t * external synchronization method *must* be used. In particular,",
            "\t * the driver-specific locking mechanism used in device_lock()",
            "\t * (including disabling migration) should be used. It prevents",
            "\t * scenarios such as:",
            "\t *",
            "\t * 1. [Task A] owns a context with NBCON_PRIO_NORMAL on [CPU X] and",
            "\t *    is scheduled out.",
            "\t * 2. Another context takes over the lock with NBCON_PRIO_EMERGENCY",
            "\t *    and releases it.",
            "\t * 3. [Task B] acquires a context with NBCON_PRIO_NORMAL on [CPU X]",
            "\t *    and is scheduled out.",
            "\t * 4. [Task A] gets running on [CPU X] and sees that the console is",
            "\t *    still owned by a task on [CPU X] with NBON_PRIO_NORMAL. Thus",
            "\t *    [Task A] thinks it is the owner when it is not.",
            "\t */",
            "",
            "\tif (cur->prio != expected_prio)",
            "\t\treturn false;",
            "",
            "\tif (cur->cpu != expected_cpu)",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "nbcon_context_try_acquire_hostile, nbcon_context_try_acquire, nbcon_owner_matches",
          "description": "处理强制接管（hostile takeover）逻辑，仅限panic优先级使用，标记控制台为永久不安全状态，提供统一的获取入口函数并关联缓冲区选择。",
          "similarity": 0.5816080570220947
        },
        {
          "chunk_id": 9,
          "file_path": "kernel/printk/nbcon.c",
          "start_line": 1619,
          "end_line": 1720,
          "content": [
            "void nbcon_atomic_flush_unsafe(void)",
            "{",
            "\t__nbcon_atomic_flush_pending(prb_next_reserve_seq(prb), true);",
            "}",
            "void nbcon_cpu_emergency_enter(void)",
            "{",
            "\tunsigned int *cpu_emergency_nesting;",
            "",
            "\tpreempt_disable();",
            "",
            "\tcpu_emergency_nesting = nbcon_get_cpu_emergency_nesting();",
            "\t(*cpu_emergency_nesting)++;",
            "}",
            "void nbcon_cpu_emergency_exit(void)",
            "{",
            "\tunsigned int *cpu_emergency_nesting;",
            "",
            "\tcpu_emergency_nesting = nbcon_get_cpu_emergency_nesting();",
            "",
            "\tif (!WARN_ON_ONCE(*cpu_emergency_nesting == 0))",
            "\t\t(*cpu_emergency_nesting)--;",
            "",
            "\tpreempt_enable();",
            "}",
            "bool nbcon_alloc(struct console *con)",
            "{",
            "\tstruct nbcon_state state = { };",
            "",
            "\t/* The write_thread() callback is mandatory. */",
            "\tif (WARN_ON(!con->write_thread))",
            "\t\treturn false;",
            "",
            "\trcuwait_init(&con->rcuwait);",
            "\tinit_irq_work(&con->irq_work, nbcon_irq_work);",
            "\tatomic_long_set(&ACCESS_PRIVATE(con, nbcon_prev_seq), -1UL);",
            "\tnbcon_state_set(con, &state);",
            "",
            "\t/*",
            "\t * Initialize @nbcon_seq to the highest possible sequence number so",
            "\t * that practically speaking it will have nothing to print until a",
            "\t * desired initial sequence number has been set via nbcon_seq_force().",
            "\t */",
            "\tatomic_long_set(&ACCESS_PRIVATE(con, nbcon_seq), ULSEQ_MAX(prb));",
            "",
            "\tif (con->flags & CON_BOOT) {",
            "\t\t/*",
            "\t\t * Boot console printing is synchronized with legacy console",
            "\t\t * printing, so boot consoles can share the same global printk",
            "\t\t * buffers.",
            "\t\t */",
            "\t\tcon->pbufs = &printk_shared_pbufs;",
            "\t} else {",
            "\t\tcon->pbufs = kmalloc(sizeof(*con->pbufs), GFP_KERNEL);",
            "\t\tif (!con->pbufs) {",
            "\t\t\tcon_printk(KERN_ERR, con, \"failed to allocate printing buffer\\n\");",
            "\t\t\treturn false;",
            "\t\t}",
            "",
            "\t\tif (printk_kthreads_running) {",
            "\t\t\tif (!nbcon_kthread_create(con)) {",
            "\t\t\t\tkfree(con->pbufs);",
            "\t\t\t\tcon->pbufs = NULL;",
            "\t\t\t\treturn false;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\treturn true;",
            "}",
            "void nbcon_free(struct console *con)",
            "{",
            "\tstruct nbcon_state state = { };",
            "",
            "\tif (printk_kthreads_running)",
            "\t\tnbcon_kthread_stop(con);",
            "",
            "\tnbcon_state_set(con, &state);",
            "",
            "\t/* Boot consoles share global printk buffers. */",
            "\tif (!(con->flags & CON_BOOT))",
            "\t\tkfree(con->pbufs);",
            "",
            "\tcon->pbufs = NULL;",
            "}",
            "bool nbcon_device_try_acquire(struct console *con)",
            "{",
            "\tstruct nbcon_context *ctxt = &ACCESS_PRIVATE(con, nbcon_device_ctxt);",
            "",
            "\tcant_migrate();",
            "",
            "\tmemset(ctxt, 0, sizeof(*ctxt));",
            "\tctxt->console\t= con;",
            "\tctxt->prio\t= NBCON_PRIO_NORMAL;",
            "",
            "\tif (!nbcon_context_try_acquire(ctxt))",
            "\t\treturn false;",
            "",
            "\tif (!nbcon_context_enter_unsafe(ctxt))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "nbcon_atomic_flush_unsafe, nbcon_cpu_emergency_enter, nbcon_cpu_emergency_exit, nbcon_alloc, nbcon_free, nbcon_device_try_acquire",
          "description": "包含CPU紧急模式的嵌套计数器增减接口，控制台资源分配释放逻辑(nbcon_alloc/free)，以及设备抢占尝试函数nbcon_device_try_acquire，用于协调控制台设备的并发访问。",
          "similarity": 0.5697212219238281
        }
      ]
    },
    {
      "source_file": "kernel/kcsan/core.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:17:10\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `kcsan\\core.c`\n\n---\n\n# kcsan/core.c 技术文档\n\n## 文件概述\n\n`kcsan/core.c` 是内核竞争条件检测器（KCSAN, Kernel Concurrency Sanitizer）的核心运行时实现文件。该文件负责管理观察点（watchpoints）的设置、查找与清除，实现对并发内存访问冲突的动态检测。KCSAN 通过概率性地监视内存访问，无需对所有内存访问进行插桩，从而在较低性能开销下检测数据竞争。\n\n## 核心功能\n\n### 主要全局变量\n- `kcsan_enabled`：控制 KCSAN 是否启用的全局开关。\n- `watchpoints[]`：原子长整型数组，用于存储编码后的观察点信息。\n- `kcsan_cpu_ctx`：每个 CPU 的上下文结构，用于中断上下文中的 KCSAN 状态管理。\n- 模块参数（可通过 `/sys/module/kcsan/parameters/` 调整）：\n  - `early_enable`：是否在早期启动阶段启用 KCSAN。\n  - `udelay_task` / `udelay_interrupt`：任务/中断上下文中检测前的延迟微秒数。\n  - `skip_watch`：跳过监视的指令计数器。\n  - `interrupt_watcher`：是否监视中断上下文中的访问。\n  - `weak_memory`（仅当 `CONFIG_KCSAN_WEAK_MEMORY` 启用）：是否启用弱内存序检测。\n\n### 主要函数\n- `find_watchpoint()`：在观察点表中查找与给定地址范围匹配的观察点。\n- `insert_watchpoint()`：尝试将新的观察点插入观察点表。\n- `try_consume_watchpoint()` / `consume_watchpoint()` / `remove_watchpoint()`：管理观察点的消费与移除。\n- `get_ctx()`：获取当前执行上下文（任务或中断）的 KCSAN 上下文。\n- `kcsan_check_scoped_accesses()`：检查当前上下文中注册的作用域访问（scoped accesses）。\n\n### 核心数据结构\n- `struct kcsan_ctx`：KCSAN 执行上下文，包含作用域访问链表、禁用标志等。\n- `struct kcsan_scoped_access`：表示一个作用域内的内存访问，用于延迟检查。\n\n## 关键实现\n\n### 观察点管理\n- **编码存储**：每个观察点通过 `encode_watchpoint()` 编码为单个 `atomic_long_t`，包含地址、大小和访问类型（读/写），避免使用锁。\n- **槽位索引策略**：\n  - 使用 `watchpoint_slot(addr)` 将地址映射到主槽位。\n  - 通过 `SLOT_IDX` 和 `SLOT_IDX_FAST` 宏支持检查相邻槽位（由 `KCSAN_CHECK_ADJACENT` 控制），以处理跨槽访问和槽位冲突。\n  - 数组大小为 `CONFIG_KCSAN_NUM_WATCHPOINTS + NUM_SLOTS - 1`，避免快速路径中的取模运算。\n- **无锁操作**：使用 `atomic_long_try_cmpxchg_relaxed()` 实现观察点的原子插入和消费。\n\n### 上下文管理\n- **双上下文支持**：任务上下文使用 `current->kcsan_ctx`，中断上下文使用 per-CPU 变量 `kcsan_cpu_ctx`。\n- **作用域访问**：通过链表管理延迟检查的访问（如 `kcsan_check_scoped_accesses()`），避免在禁用区域内重复检查。\n\n### 检测逻辑\n- **概率性检测**：通过 `kcsan_skip` 计数器和随机延迟（`kcsan_udelay_*`）控制检测频率，平衡开销与覆盖率。\n- **原子访问识别**：根据访问类型标志（`KCSAN_ACCESS_ATOMIC`、`KCSAN_ACCESS_ASSERT`）和配置（`CONFIG_KCSAN_ASSUME_PLAIN_WRITES_ATOMIC`）判断访问是否为原子操作，避免误报。\n\n## 依赖关系\n\n- **内部依赖**：\n  - `encoding.h`：提供观察点的编码/解码函数。\n  - `kcsan.h`：定义核心数据结构和常量。\n  - `permissive.h`：提供宽松模式下的行为控制。\n- **内核子系统**：\n  - 调度器（`linux/sched.h`）：用于获取当前任务上下文。\n  - 原子操作（`linux/atomic.h`）：实现无锁观察点管理。\n  - Per-CPU 变量（`linux/percpu.h`）：管理中断上下文状态。\n  - 内存访问（`linux/uaccess.h`）：处理用户空间访问区域。\n\n## 使用场景\n\n- **数据竞争检测**：在 SMP 系统中检测未同步的并发内存访问（如未加锁的共享变量读写）。\n- **开发与调试**：内核开发者启用 KCSAN 后，可在运行时捕获竞争条件，通过报告定位问题代码。\n- **中断与任务交互**：通过 `kcsan_interrupt_watcher` 参数控制是否检测中断与任务之间的竞争。\n- **弱内存序系统**：在启用 `CONFIG_KCSAN_WEAK_MEMORY` 的架构（如 ARM、RISC-V）上检测内存重排序导致的竞争。",
      "similarity": 0.5334430932998657,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/kcsan/core.c",
          "start_line": 718,
          "end_line": 821,
          "content": [
            "static __always_inline void",
            "check_access(const volatile void *ptr, size_t size, int type, unsigned long ip)",
            "{",
            "\tatomic_long_t *watchpoint;",
            "\tlong encoded_watchpoint;",
            "",
            "\t/*",
            "\t * Do nothing for 0 sized check; this comparison will be optimized out",
            "\t * for constant sized instrumentation (__tsan_{read,write}N).",
            "\t */",
            "\tif (unlikely(size == 0))",
            "\t\treturn;",
            "",
            "again:",
            "\t/*",
            "\t * Avoid user_access_save in fast-path: find_watchpoint is safe without",
            "\t * user_access_save, as the address that ptr points to is only used to",
            "\t * check if a watchpoint exists; ptr is never dereferenced.",
            "\t */",
            "\twatchpoint = find_watchpoint((unsigned long)ptr, size,",
            "\t\t\t\t     !(type & KCSAN_ACCESS_WRITE),",
            "\t\t\t\t     &encoded_watchpoint);",
            "\t/*",
            "\t * It is safe to check kcsan_is_enabled() after find_watchpoint in the",
            "\t * slow-path, as long as no state changes that cause a race to be",
            "\t * detected and reported have occurred until kcsan_is_enabled() is",
            "\t * checked.",
            "\t */",
            "",
            "\tif (unlikely(watchpoint != NULL))",
            "\t\tkcsan_found_watchpoint(ptr, size, type, ip, watchpoint, encoded_watchpoint);",
            "\telse {",
            "\t\tstruct kcsan_ctx *ctx = get_ctx(); /* Call only once in fast-path. */",
            "",
            "\t\tif (unlikely(should_watch(ctx, ptr, size, type))) {",
            "\t\t\tkcsan_setup_watchpoint(ptr, size, type, ip);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tif (!(type & KCSAN_ACCESS_SCOPED)) {",
            "\t\t\tstruct kcsan_scoped_access *reorder_access = get_reorder_access(ctx);",
            "",
            "\t\t\tif (reorder_access) {",
            "\t\t\t\t/*",
            "\t\t\t\t * reorder_access check: simulates reordering of",
            "\t\t\t\t * the access after subsequent operations.",
            "\t\t\t\t */",
            "\t\t\t\tptr = reorder_access->ptr;",
            "\t\t\t\ttype = reorder_access->type;",
            "\t\t\t\tip = reorder_access->ip;",
            "\t\t\t\t/*",
            "\t\t\t\t * Upon a nested interrupt, this context's",
            "\t\t\t\t * reorder_access can be modified (shared ctx).",
            "\t\t\t\t * We know that upon return, reorder_access is",
            "\t\t\t\t * always invalidated by setting size to 0 via",
            "\t\t\t\t * __tsan_func_exit(). Therefore we must read",
            "\t\t\t\t * and check size after the other fields.",
            "\t\t\t\t */",
            "\t\t\t\tbarrier();",
            "\t\t\t\tsize = READ_ONCE(reorder_access->size);",
            "\t\t\t\tif (size)",
            "\t\t\t\t\tgoto again;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Always checked last, right before returning from runtime;",
            "\t\t * if reorder_access is valid, checked after it was checked.",
            "\t\t */",
            "\t\tif (unlikely(ctx->scoped_accesses.prev))",
            "\t\t\tkcsan_check_scoped_accesses();",
            "\t}",
            "}",
            "void __init kcsan_init(void)",
            "{",
            "\tint cpu;",
            "",
            "\tBUG_ON(!in_task());",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(kcsan_rand_state, cpu) = (u32)get_cycles();",
            "",
            "\t/*",
            "\t * We are in the init task, and no other tasks should be running;",
            "\t * WRITE_ONCE without memory barrier is sufficient.",
            "\t */",
            "\tif (kcsan_early_enable) {",
            "\t\tpr_info(\"enabled early\\n\");",
            "\t\tWRITE_ONCE(kcsan_enabled, true);",
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_KCSAN_REPORT_VALUE_CHANGE_ONLY) ||",
            "\t    IS_ENABLED(CONFIG_KCSAN_ASSUME_PLAIN_WRITES_ATOMIC) ||",
            "\t    IS_ENABLED(CONFIG_KCSAN_PERMISSIVE) ||",
            "\t    IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS)) {",
            "\t\tpr_warn(\"non-strict mode configured - use CONFIG_KCSAN_STRICT=y to see all data races\\n\");",
            "\t} else {",
            "\t\tpr_info(\"strict mode configured\\n\");",
            "\t}",
            "}",
            "void kcsan_disable_current(void)",
            "{",
            "\t++get_ctx()->disable_count;",
            "}"
          ],
          "function_name": "check_access, kcsan_init, kcsan_disable_current",
          "description": "实现核心访问检查入口(check_access)及系统初始化(kcsan_init)与禁用接口(kcsan_disable_current)，建立上下文感知的监控机制并初始化必要的运行时环境。",
          "similarity": 0.6067280769348145
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/kcsan/core.c",
          "start_line": 182,
          "end_line": 286,
          "content": [
            "static __always_inline bool",
            "try_consume_watchpoint(atomic_long_t *watchpoint, long encoded_watchpoint)",
            "{",
            "\treturn atomic_long_try_cmpxchg_relaxed(watchpoint, &encoded_watchpoint, CONSUMED_WATCHPOINT);",
            "}",
            "static inline bool consume_watchpoint(atomic_long_t *watchpoint)",
            "{",
            "\treturn atomic_long_xchg_relaxed(watchpoint, CONSUMED_WATCHPOINT) != CONSUMED_WATCHPOINT;",
            "}",
            "static inline void remove_watchpoint(atomic_long_t *watchpoint)",
            "{",
            "\tatomic_long_set(watchpoint, INVALID_WATCHPOINT);",
            "}",
            "static noinline void kcsan_check_scoped_accesses(void)",
            "{",
            "\tstruct kcsan_ctx *ctx = get_ctx();",
            "\tstruct kcsan_scoped_access *scoped_access;",
            "",
            "\tif (ctx->disable_scoped)",
            "\t\treturn;",
            "",
            "\tctx->disable_scoped++;",
            "\tlist_for_each_entry(scoped_access, &ctx->scoped_accesses, list) {",
            "\t\tcheck_access(scoped_access->ptr, scoped_access->size,",
            "\t\t\t     scoped_access->type, scoped_access->ip);",
            "\t}",
            "\tctx->disable_scoped--;",
            "}",
            "static __always_inline bool",
            "is_atomic(struct kcsan_ctx *ctx, const volatile void *ptr, size_t size, int type)",
            "{",
            "\tif (type & KCSAN_ACCESS_ATOMIC)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Unless explicitly declared atomic, never consider an assertion access",
            "\t * as atomic. This allows using them also in atomic regions, such as",
            "\t * seqlocks, without implicitly changing their semantics.",
            "\t */",
            "\tif (type & KCSAN_ACCESS_ASSERT)",
            "\t\treturn false;",
            "",
            "\tif (IS_ENABLED(CONFIG_KCSAN_ASSUME_PLAIN_WRITES_ATOMIC) &&",
            "\t    (type & KCSAN_ACCESS_WRITE) && size <= sizeof(long) &&",
            "\t    !(type & KCSAN_ACCESS_COMPOUND) && IS_ALIGNED((unsigned long)ptr, size))",
            "\t\treturn true; /* Assume aligned writes up to word size are atomic. */",
            "",
            "\tif (ctx->atomic_next > 0) {",
            "\t\t/*",
            "\t\t * Because we do not have separate contexts for nested",
            "\t\t * interrupts, in case atomic_next is set, we simply assume that",
            "\t\t * the outer interrupt set atomic_next. In the worst case, we",
            "\t\t * will conservatively consider operations as atomic. This is a",
            "\t\t * reasonable trade-off to make, since this case should be",
            "\t\t * extremely rare; however, even if extremely rare, it could",
            "\t\t * lead to false positives otherwise.",
            "\t\t */",
            "\t\tif ((hardirq_count() >> HARDIRQ_SHIFT) < 2)",
            "\t\t\t--ctx->atomic_next; /* in task, or outer interrupt */",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn ctx->atomic_nest_count > 0 || ctx->in_flat_atomic;",
            "}",
            "static __always_inline bool",
            "should_watch(struct kcsan_ctx *ctx, const volatile void *ptr, size_t size, int type)",
            "{",
            "\t/*",
            "\t * Never set up watchpoints when memory operations are atomic.",
            "\t *",
            "\t * Need to check this first, before kcsan_skip check below: (1) atomics",
            "\t * should not count towards skipped instructions, and (2) to actually",
            "\t * decrement kcsan_atomic_next for consecutive instruction stream.",
            "\t */",
            "\tif (is_atomic(ctx, ptr, size, type))",
            "\t\treturn false;",
            "",
            "\tif (this_cpu_dec_return(kcsan_skip) >= 0)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * NOTE: If we get here, kcsan_skip must always be reset in slow path",
            "\t * via reset_kcsan_skip() to avoid underflow.",
            "\t */",
            "",
            "\t/* this operation should be watched */",
            "\treturn true;",
            "}",
            "static u32 kcsan_prandom_u32_max(u32 ep_ro)",
            "{",
            "\tu32 state = this_cpu_read(kcsan_rand_state);",
            "",
            "\tstate = 1664525 * state + 1013904223;",
            "\tthis_cpu_write(kcsan_rand_state, state);",
            "",
            "\treturn state % ep_ro;",
            "}",
            "static inline void reset_kcsan_skip(void)",
            "{",
            "\tlong skip_count = kcsan_skip_watch -",
            "\t\t\t  (IS_ENABLED(CONFIG_KCSAN_SKIP_WATCH_RANDOMIZE) ?",
            "\t\t\t\t   kcsan_prandom_u32_max(kcsan_skip_watch) :",
            "\t\t\t\t   0);",
            "\tthis_cpu_write(kcsan_skip, skip_count);",
            "}"
          ],
          "function_name": "try_consume_watchpoint, consume_watchpoint, remove_watchpoint, kcsan_check_scoped_accesses, is_atomic, should_watch, kcsan_prandom_u32_max, reset_kcsan_skip",
          "description": "实现了监视点状态管理相关函数（try_consume_watchpoint、consume_watchpoint等），提供原子性判断、跳过指令计数控制及随机数生成功能，用于动态决策是否需设置监视点。",
          "similarity": 0.5712968111038208
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/kcsan/core.c",
          "start_line": 830,
          "end_line": 931,
          "content": [
            "void kcsan_enable_current(void)",
            "{",
            "\tif (get_ctx()->disable_count-- == 0) {",
            "\t\t/*",
            "\t\t * Warn if kcsan_enable_current() calls are unbalanced with",
            "\t\t * kcsan_disable_current() calls, which causes disable_count to",
            "\t\t * become negative and should not happen.",
            "\t\t */",
            "\t\tkcsan_disable_current(); /* restore to 0, KCSAN still enabled */",
            "\t\tkcsan_disable_current(); /* disable to generate warning */",
            "\t\tWARN(1, \"Unbalanced %s()\", __func__);",
            "\t\tkcsan_enable_current();",
            "\t}",
            "}",
            "void kcsan_enable_current_nowarn(void)",
            "{",
            "\tif (get_ctx()->disable_count-- == 0)",
            "\t\tkcsan_disable_current();",
            "}",
            "void kcsan_nestable_atomic_begin(void)",
            "{",
            "\t/*",
            "\t * Do *not* check and warn if we are in a flat atomic region: nestable",
            "\t * and flat atomic regions are independent from each other.",
            "\t * See include/linux/kcsan.h: struct kcsan_ctx comments for more",
            "\t * comments.",
            "\t */",
            "",
            "\t++get_ctx()->atomic_nest_count;",
            "}",
            "void kcsan_nestable_atomic_end(void)",
            "{",
            "\tif (get_ctx()->atomic_nest_count-- == 0) {",
            "\t\t/*",
            "\t\t * Warn if kcsan_nestable_atomic_end() calls are unbalanced with",
            "\t\t * kcsan_nestable_atomic_begin() calls, which causes",
            "\t\t * atomic_nest_count to become negative and should not happen.",
            "\t\t */",
            "\t\tkcsan_nestable_atomic_begin(); /* restore to 0 */",
            "\t\tkcsan_disable_current(); /* disable to generate warning */",
            "\t\tWARN(1, \"Unbalanced %s()\", __func__);",
            "\t\tkcsan_enable_current();",
            "\t}",
            "}",
            "void kcsan_flat_atomic_begin(void)",
            "{",
            "\tget_ctx()->in_flat_atomic = true;",
            "}",
            "void kcsan_flat_atomic_end(void)",
            "{",
            "\tget_ctx()->in_flat_atomic = false;",
            "}",
            "void kcsan_atomic_next(int n)",
            "{",
            "\tget_ctx()->atomic_next = n;",
            "}",
            "void kcsan_set_access_mask(unsigned long mask)",
            "{",
            "\tget_ctx()->access_mask = mask;",
            "}",
            "void kcsan_end_scoped_access(struct kcsan_scoped_access *sa)",
            "{",
            "\tstruct kcsan_ctx *ctx = get_ctx();",
            "",
            "\tif (WARN(!ctx->scoped_accesses.prev, \"Unbalanced %s()?\", __func__))",
            "\t\treturn;",
            "",
            "\tctx->disable_count++; /* Disable KCSAN, in case list debugging is on. */",
            "",
            "\tlist_del(&sa->list);",
            "\tif (list_empty(&ctx->scoped_accesses))",
            "\t\t/*",
            "\t\t * Ensure we do not enter kcsan_check_scoped_accesses()",
            "\t\t * slow-path if unnecessary, and avoids requiring list_empty()",
            "\t\t * in the fast-path (to avoid a READ_ONCE() and potential",
            "\t\t * uaccess warning).",
            "\t\t */",
            "\t\tctx->scoped_accesses.prev = NULL;",
            "",
            "\tctx->disable_count--;",
            "",
            "\tcheck_access(sa->ptr, sa->size, sa->type, sa->ip);",
            "}",
            "void __kcsan_check_access(const volatile void *ptr, size_t size, int type)",
            "{",
            "\tcheck_access(ptr, size, type, _RET_IP_);",
            "}",
            "void __tsan_read_range(void *ptr, size_t size)",
            "{",
            "\tcheck_access(ptr, size, 0, _RET_IP_);",
            "}",
            "void __tsan_write_range(void *ptr, size_t size)",
            "{",
            "\tcheck_access(ptr, size, KCSAN_ACCESS_WRITE, _RET_IP_);",
            "}",
            "noinline void __tsan_func_entry(void *call_pc)",
            "{",
            "\tif (!IS_ENABLED(CONFIG_KCSAN_WEAK_MEMORY))",
            "\t\treturn;",
            "",
            "\tadd_kcsan_stack_depth(1);",
            "}"
          ],
          "function_name": "kcsan_enable_current, kcsan_enable_current_nowarn, kcsan_nestable_atomic_begin, kcsan_nestable_atomic_end, kcsan_flat_atomic_begin, kcsan_flat_atomic_end, kcsan_atomic_next, kcsan_set_access_mask, kcsan_end_scoped_access, __kcsan_check_access, __tsan_read_range, __tsan_write_range, __tsan_func_entry",
          "description": "该代码块定义了KCSAN工具的启用/禁用控制逻辑及原子操作追踪接口。kcsan_enable_current系列函数通过递减disable_count实现线程级启用状态管理，当计数归零时触发警告以检测未平衡的enable/disable调用。nestable_atomic_*系列函数跟踪嵌套原子操作层级，flat_atomic_*用于标记平坦原子区域状态。set_access_mask配置访问掩码，end_scoped_access处理作用域访问结束时的检查与清理，各内存访问检查函数（__tsan_read/write_range）调用通用check_access实现数据竞争检测。",
          "similarity": 0.5340367555618286
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/kcsan/core.c",
          "start_line": 315,
          "end_line": 418,
          "content": [
            "static __always_inline bool kcsan_is_enabled(struct kcsan_ctx *ctx)",
            "{",
            "\treturn READ_ONCE(kcsan_enabled) && !ctx->disable_count;",
            "}",
            "static void delay_access(int type)",
            "{",
            "\tunsigned int delay = in_task() ? kcsan_udelay_task : kcsan_udelay_interrupt;",
            "\t/* For certain access types, skew the random delay to be longer. */",
            "\tunsigned int skew_delay_order =",
            "\t\t(type & (KCSAN_ACCESS_COMPOUND | KCSAN_ACCESS_ASSERT)) ? 1 : 0;",
            "",
            "\tdelay -= IS_ENABLED(CONFIG_KCSAN_DELAY_RANDOMIZE) ?",
            "\t\t\t       kcsan_prandom_u32_max(delay >> skew_delay_order) :",
            "\t\t\t       0;",
            "\tudelay(delay);",
            "}",
            "static __always_inline u64 read_instrumented_memory(const volatile void *ptr, size_t size)",
            "{",
            "\t/*",
            "\t * In the below we don't necessarily need the read of the location to",
            "\t * be atomic, and we don't use READ_ONCE(), since all we need for race",
            "\t * detection is to observe 2 different values.",
            "\t *",
            "\t * Furthermore, on certain architectures (such as arm64), READ_ONCE()",
            "\t * may turn into more complex instructions than a plain load that cannot",
            "\t * do unaligned accesses.",
            "\t */",
            "\tswitch (size) {",
            "\tcase 1:  return *(const volatile u8 *)ptr;",
            "\tcase 2:  return *(const volatile u16 *)ptr;",
            "\tcase 4:  return *(const volatile u32 *)ptr;",
            "\tcase 8:  return *(const volatile u64 *)ptr;",
            "\tdefault: return 0; /* Ignore; we do not diff the values. */",
            "\t}",
            "}",
            "void kcsan_save_irqtrace(struct task_struct *task)",
            "{",
            "#ifdef CONFIG_TRACE_IRQFLAGS",
            "\ttask->kcsan_save_irqtrace = task->irqtrace;",
            "#endif",
            "}",
            "void kcsan_restore_irqtrace(struct task_struct *task)",
            "{",
            "#ifdef CONFIG_TRACE_IRQFLAGS",
            "\ttask->irqtrace = task->kcsan_save_irqtrace;",
            "#endif",
            "}",
            "static __always_inline int get_kcsan_stack_depth(void)",
            "{",
            "#ifdef CONFIG_KCSAN_WEAK_MEMORY",
            "\treturn current->kcsan_stack_depth;",
            "#else",
            "\tBUILD_BUG();",
            "\treturn 0;",
            "#endif",
            "}",
            "static __always_inline void add_kcsan_stack_depth(int val)",
            "{",
            "#ifdef CONFIG_KCSAN_WEAK_MEMORY",
            "\tcurrent->kcsan_stack_depth += val;",
            "#else",
            "\tBUILD_BUG();",
            "#endif",
            "}",
            "static __always_inline bool",
            "find_reorder_access(struct kcsan_ctx *ctx, const volatile void *ptr, size_t size,",
            "\t\t    int type, unsigned long ip)",
            "{",
            "\tstruct kcsan_scoped_access *reorder_access = get_reorder_access(ctx);",
            "",
            "\tif (!reorder_access)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Note: If accesses are repeated while reorder_access is identical,",
            "\t * never matches the new access, because !(type & KCSAN_ACCESS_SCOPED).",
            "\t */",
            "\treturn reorder_access->ptr == ptr && reorder_access->size == size &&",
            "\t       reorder_access->type == type && reorder_access->ip == ip;",
            "}",
            "static inline void",
            "set_reorder_access(struct kcsan_ctx *ctx, const volatile void *ptr, size_t size,",
            "\t\t   int type, unsigned long ip)",
            "{",
            "\tstruct kcsan_scoped_access *reorder_access = get_reorder_access(ctx);",
            "",
            "\tif (!reorder_access || !kcsan_weak_memory)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * To avoid nested interrupts or scheduler (which share kcsan_ctx)",
            "\t * reading an inconsistent reorder_access, ensure that the below has",
            "\t * exclusive access to reorder_access by disallowing concurrent use.",
            "\t */",
            "\tctx->disable_scoped++;",
            "\tbarrier();",
            "\treorder_access->ptr\t\t= ptr;",
            "\treorder_access->size\t\t= size;",
            "\treorder_access->type\t\t= type | KCSAN_ACCESS_SCOPED;",
            "\treorder_access->ip\t\t= ip;",
            "\treorder_access->stack_depth\t= get_kcsan_stack_depth();",
            "\tbarrier();",
            "\tctx->disable_scoped--;",
            "}"
          ],
          "function_name": "kcsan_is_enabled, delay_access, read_instrumented_memory, kcsan_save_irqtrace, kcsan_restore_irqtrace, get_kcsan_stack_depth, add_kcsan_stack_depth, find_reorder_access, set_reorder_access",
          "description": "包含内存访问检查基础设施，提供延迟注入、受监控内存读取、中断跟踪保护及重排访问追踪功能，支持栈深度管理与跨中断上下文的访问顺序检测。",
          "similarity": 0.5319004058837891
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/kcsan/core.c",
          "start_line": 1,
          "end_line": 181,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * KCSAN core runtime.",
            " *",
            " * Copyright (C) 2019, Google LLC.",
            " */",
            "",
            "#define pr_fmt(fmt) \"kcsan: \" fmt",
            "",
            "#include <linux/atomic.h>",
            "#include <linux/bug.h>",
            "#include <linux/delay.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/list.h>",
            "#include <linux/minmax.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/percpu.h>",
            "#include <linux/preempt.h>",
            "#include <linux/sched.h>",
            "#include <linux/string.h>",
            "#include <linux/uaccess.h>",
            "",
            "#include \"encoding.h\"",
            "#include \"kcsan.h\"",
            "#include \"permissive.h\"",
            "",
            "static bool kcsan_early_enable = IS_ENABLED(CONFIG_KCSAN_EARLY_ENABLE);",
            "unsigned int kcsan_udelay_task = CONFIG_KCSAN_UDELAY_TASK;",
            "unsigned int kcsan_udelay_interrupt = CONFIG_KCSAN_UDELAY_INTERRUPT;",
            "static long kcsan_skip_watch = CONFIG_KCSAN_SKIP_WATCH;",
            "static bool kcsan_interrupt_watcher = IS_ENABLED(CONFIG_KCSAN_INTERRUPT_WATCHER);",
            "",
            "#ifdef MODULE_PARAM_PREFIX",
            "#undef MODULE_PARAM_PREFIX",
            "#endif",
            "#define MODULE_PARAM_PREFIX \"kcsan.\"",
            "module_param_named(early_enable, kcsan_early_enable, bool, 0);",
            "module_param_named(udelay_task, kcsan_udelay_task, uint, 0644);",
            "module_param_named(udelay_interrupt, kcsan_udelay_interrupt, uint, 0644);",
            "module_param_named(skip_watch, kcsan_skip_watch, long, 0644);",
            "module_param_named(interrupt_watcher, kcsan_interrupt_watcher, bool, 0444);",
            "",
            "#ifdef CONFIG_KCSAN_WEAK_MEMORY",
            "static bool kcsan_weak_memory = true;",
            "module_param_named(weak_memory, kcsan_weak_memory, bool, 0644);",
            "#else",
            "#define kcsan_weak_memory false",
            "#endif",
            "",
            "bool kcsan_enabled;",
            "",
            "/* Per-CPU kcsan_ctx for interrupts */",
            "static DEFINE_PER_CPU(struct kcsan_ctx, kcsan_cpu_ctx) = {",
            "\t.scoped_accesses\t= {LIST_POISON1, NULL},",
            "};",
            "",
            "/*",
            " * Helper macros to index into adjacent slots, starting from address slot",
            " * itself, followed by the right and left slots.",
            " *",
            " * The purpose is 2-fold:",
            " *",
            " *\t1. if during insertion the address slot is already occupied, check if",
            " *\t   any adjacent slots are free;",
            " *\t2. accesses that straddle a slot boundary due to size that exceeds a",
            " *\t   slot's range may check adjacent slots if any watchpoint matches.",
            " *",
            " * Note that accesses with very large size may still miss a watchpoint; however,",
            " * given this should be rare, this is a reasonable trade-off to make, since this",
            " * will avoid:",
            " *",
            " *\t1. excessive contention between watchpoint checks and setup;",
            " *\t2. larger number of simultaneous watchpoints without sacrificing",
            " *\t   performance.",
            " *",
            " * Example: SLOT_IDX values for KCSAN_CHECK_ADJACENT=1, where i is [0, 1, 2]:",
            " *",
            " *   slot=0:  [ 1,  2,  0]",
            " *   slot=9:  [10, 11,  9]",
            " *   slot=63: [64, 65, 63]",
            " */",
            "#define SLOT_IDX(slot, i) (slot + ((i + KCSAN_CHECK_ADJACENT) % NUM_SLOTS))",
            "",
            "/*",
            " * SLOT_IDX_FAST is used in the fast-path. Not first checking the address's primary",
            " * slot (middle) is fine if we assume that races occur rarely. The set of",
            " * indices {SLOT_IDX(slot, i) | i in [0, NUM_SLOTS)} is equivalent to",
            " * {SLOT_IDX_FAST(slot, i) | i in [0, NUM_SLOTS)}.",
            " */",
            "#define SLOT_IDX_FAST(slot, i) (slot + i)",
            "",
            "/*",
            " * Watchpoints, with each entry encoded as defined in encoding.h: in order to be",
            " * able to safely update and access a watchpoint without introducing locking",
            " * overhead, we encode each watchpoint as a single atomic long. The initial",
            " * zero-initialized state matches INVALID_WATCHPOINT.",
            " *",
            " * Add NUM_SLOTS-1 entries to account for overflow; this helps avoid having to",
            " * use more complicated SLOT_IDX_FAST calculation with modulo in the fast-path.",
            " */",
            "static atomic_long_t watchpoints[CONFIG_KCSAN_NUM_WATCHPOINTS + NUM_SLOTS-1];",
            "",
            "/*",
            " * Instructions to skip watching counter, used in should_watch(). We use a",
            " * per-CPU counter to avoid excessive contention.",
            " */",
            "static DEFINE_PER_CPU(long, kcsan_skip);",
            "",
            "/* For kcsan_prandom_u32_max(). */",
            "static DEFINE_PER_CPU(u32, kcsan_rand_state);",
            "",
            "static __always_inline atomic_long_t *find_watchpoint(unsigned long addr,",
            "\t\t\t\t\t\t      size_t size,",
            "\t\t\t\t\t\t      bool expect_write,",
            "\t\t\t\t\t\t      long *encoded_watchpoint)",
            "{",
            "\tconst int slot = watchpoint_slot(addr);",
            "\tconst unsigned long addr_masked = addr & WATCHPOINT_ADDR_MASK;",
            "\tatomic_long_t *watchpoint;",
            "\tunsigned long wp_addr_masked;",
            "\tsize_t wp_size;",
            "\tbool is_write;",
            "\tint i;",
            "",
            "\tBUILD_BUG_ON(CONFIG_KCSAN_NUM_WATCHPOINTS < NUM_SLOTS);",
            "",
            "\tfor (i = 0; i < NUM_SLOTS; ++i) {",
            "\t\twatchpoint = &watchpoints[SLOT_IDX_FAST(slot, i)];",
            "\t\t*encoded_watchpoint = atomic_long_read(watchpoint);",
            "\t\tif (!decode_watchpoint(*encoded_watchpoint, &wp_addr_masked,",
            "\t\t\t\t       &wp_size, &is_write))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (expect_write && !is_write)",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Check if the watchpoint matches the access. */",
            "\t\tif (matching_access(wp_addr_masked, wp_size, addr_masked, size))",
            "\t\t\treturn watchpoint;",
            "\t}",
            "",
            "\treturn NULL;",
            "}",
            "",
            "static inline atomic_long_t *",
            "insert_watchpoint(unsigned long addr, size_t size, bool is_write)",
            "{",
            "\tconst int slot = watchpoint_slot(addr);",
            "\tconst long encoded_watchpoint = encode_watchpoint(addr, size, is_write);",
            "\tatomic_long_t *watchpoint;",
            "\tint i;",
            "",
            "\t/* Check slot index logic, ensuring we stay within array bounds. */",
            "\tBUILD_BUG_ON(SLOT_IDX(0, 0) != KCSAN_CHECK_ADJACENT);",
            "\tBUILD_BUG_ON(SLOT_IDX(0, KCSAN_CHECK_ADJACENT+1) != 0);",
            "\tBUILD_BUG_ON(SLOT_IDX(CONFIG_KCSAN_NUM_WATCHPOINTS-1, KCSAN_CHECK_ADJACENT) != ARRAY_SIZE(watchpoints)-1);",
            "\tBUILD_BUG_ON(SLOT_IDX(CONFIG_KCSAN_NUM_WATCHPOINTS-1, KCSAN_CHECK_ADJACENT+1) != ARRAY_SIZE(watchpoints) - NUM_SLOTS);",
            "",
            "\tfor (i = 0; i < NUM_SLOTS; ++i) {",
            "\t\tlong expect_val = INVALID_WATCHPOINT;",
            "",
            "\t\t/* Try to acquire this slot. */",
            "\t\twatchpoint = &watchpoints[SLOT_IDX(slot, i)];",
            "\t\tif (atomic_long_try_cmpxchg_relaxed(watchpoint, &expect_val, encoded_watchpoint))",
            "\t\t\treturn watchpoint;",
            "\t}",
            "",
            "\treturn NULL;",
            "}",
            "",
            "/*",
            " * Return true if watchpoint was successfully consumed, false otherwise.",
            " *",
            " * This may return false if:",
            " *",
            " *\t1. another thread already consumed the watchpoint;",
            " *\t2. the thread that set up the watchpoint already removed it;",
            " *\t3. the watchpoint was removed and then re-used.",
            " */"
          ],
          "function_name": null,
          "description": "定义了KCSAN的核心运行时变量和参数，包含监视点数组、上下文结构体及辅助宏，用于管理内存访问检查的槽位计算和监视点插入逻辑，但代码上下文不完整。",
          "similarity": 0.4949014484882355
        }
      ]
    },
    {
      "source_file": "kernel/sched/core_sched.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:00:47\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\core_sched.c`\n\n---\n\n# `sched/core_sched.c` 技术文档\n\n## 1. 文件概述\n\n`sched/core_sched.c` 是 Linux 内核调度器中用于实现 **核心调度（Core Scheduling）** 功能的核心文件之一。核心调度是一种安全机制，旨在防止来自不同安全上下文的任务在同一个物理 CPU 核心（特别是超线程/SMT 共享核心）上并发执行，从而缓解侧信道攻击（如 Spectre、MDS 等）。\n\n该文件主要负责管理任务的 **调度 cookie**（`core_cookie`），通过引用计数的 cookie 对象将具有相同安全上下文的任务分组，确保只有拥有相同 cookie 的任务才能在同一个 CPU 核心上并发运行。\n\n## 2. 核心功能\n\n### 数据结构\n\n- **`struct sched_core_cookie`**  \n  表示一个调度 cookie，仅包含一个引用计数器 `refcnt`。其内存地址本身即作为 cookie 值使用。\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `sched_core_alloc_cookie()` | 分配一个新的 `sched_core_cookie` 对象，初始化引用计数为 1，并启用核心调度全局状态。返回 cookie 地址（转换为 `unsigned long`）。 |\n| `sched_core_put_cookie(unsigned long cookie)` | 释放 cookie 引用；若引用计数归零，则释放内存并关闭核心调度全局状态。 |\n| `sched_core_get_cookie(unsigned long cookie)` | 增加 cookie 引用计数，返回原 cookie 值。 |\n| `sched_core_update_cookie(struct task_struct *p, unsigned long cookie)` | 原子地更新任务 `p` 的 `core_cookie`，处理任务在运行队列中的入队/出队，并在必要时触发重调度。 |\n| `sched_core_clone_cookie(struct task_struct *p)` | 安全地复制任务 `p` 的当前 cookie（带锁保护），用于 fork 或共享操作。 |\n| `sched_core_fork(struct task_struct *p)` | 在 `fork()` 时初始化子任务的核心调度状态，继承父进程的 cookie。 |\n| `sched_core_free(struct task_struct *p)` | 在任务退出时释放其持有的 cookie 引用。 |\n| `__sched_core_set(struct task_struct *p, unsigned long cookie)` | 设置任务 `p` 的 cookie，自动处理引用计数的获取与释放。 |\n| `sched_core_share_pid(...)` | 用户空间通过 `prctl(PR_SCHED_CORE, ...)` 调用的核心接口，支持创建、查询、共享 cookie。 |\n| `__sched_core_account_forceidle(struct rq *rq)` | （仅当 `CONFIG_SCHEDSTATS` 启用）统计核心强制空闲（force-idle）时间，并分摊到相关任务。 |\n| `__sched_core_tick(struct rq *rq)` | 在调度 tick 中调用，用于更新强制空闲时间统计。 |\n\n## 3. 关键实现\n\n### Cookie 生命周期管理\n- Cookie 通过 `kmalloc` 动态分配，其地址作为唯一标识。\n- 使用 `refcount_t` 实现线程安全的引用计数。\n- `sched_core_get()` / `sched_core_put()` 控制全局核心调度使能状态。\n\n### 任务 Cookie 更新\n- 在 `task_rq_lock()` 保护下更新 `p->core_cookie`，确保调度器一致性。\n- 若任务已在运行队列中，先出队再根据新 cookie 决定是否重新入队。\n- 若任务正在 CPU 上运行，调用 `resched_curr()` 触发重调度，以确保新 cookie 策略立即生效。\n\n### 安全访问控制\n- 通过 `ptrace_may_access()` 检查调用者是否有权限操作目标进程的 cookie。\n- 仅当系统存在 SMT（超线程）时（`sched_smt_present` 为真），才允许使用核心调度功能。\n\n### prctl 接口支持\n- 支持四种命令：\n  - `PR_SCHED_CORE_CREATE`：创建新 cookie。\n  - `PR_SCHED_CORE_SHARE_TO`：将当前进程的 cookie 应用于目标进程（或进程组）。\n  - `PR_SCHED_CORE_SHARE_FROM`：将目标进程的 cookie 应用于当前进程。\n  - `PR_SCHED_CORE_GET`：获取目标进程的 cookie 哈希值（用于用户空间识别）。\n- 支持作用域：线程（`PIDTYPE_PID`）、线程组（`PIDTYPE_TGID`）、进程组（`PIDTYPE_PGID`）。\n\n### 强制空闲时间统计（`CONFIG_SCHEDSTATS`）\n- 当核心因 cookie 不兼容而进入强制空闲状态时，记录空闲时间。\n- 时间按 `core_forceidle_count / core_forceidle_occupation` 比例分摊到所有相关 CPU 上的非 idle 任务。\n- 通过 `__account_forceidle_time()` 更新任务的调度统计信息。\n\n## 4. 依赖关系\n\n- **调度器核心**：依赖 `kernel/sched/` 下的通用调度器基础设施，如 `task_rq_lock()`、`resched_curr()`、`rq` 结构等。\n- **SMT 检测**：依赖 `sched_smt_present` 静态分支判断系统是否支持超线程。\n- **内存管理**：使用 `kmalloc`/`kfree` 进行动态内存分配。\n- **进程管理**：依赖 `find_task_by_vpid()`、`tasklist_lock`、`do_each_pid_thread` 等进程遍历机制。\n- **安全机制**：依赖 `ptrace_may_access()` 进行权限检查。\n- **调度统计**：`__sched_core_account_forceidle` 依赖 `CONFIG_SCHEDSTATS` 和 `__account_forceidle_time`。\n\n## 5. 使用场景\n\n- **安全敏感应用**：如浏览器、虚拟机监控器（VMM）、加密服务等，需防止跨任务的侧信道攻击。\n- **用户空间控制**：通过 `prctl(PR_SCHED_CORE, ...)` 接口，应用程序可显式创建和共享调度 cookie，将信任的任务分组。\n- **进程 fork 行为**：子进程自动继承父进程的 cookie，确保同源任务保持调度兼容性。\n- **系统资源隔离**：在多租户或容器环境中，确保不同租户的任务不会在同一个物理核心上并发执行。\n- **性能调优与监控**：通过 `CONFIG_SCHEDSTATS` 收集核心强制空闲开销，评估安全策略对性能的影响。",
      "similarity": 0.5326024293899536,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/core_sched.c",
          "start_line": 11,
          "end_line": 216,
          "content": [
            "static unsigned long sched_core_alloc_cookie(void)",
            "{",
            "\tstruct sched_core_cookie *ck = kmalloc(sizeof(*ck), GFP_KERNEL);",
            "\tif (!ck)",
            "\t\treturn 0;",
            "",
            "\trefcount_set(&ck->refcnt, 1);",
            "\tsched_core_get();",
            "",
            "\treturn (unsigned long)ck;",
            "}",
            "static void sched_core_put_cookie(unsigned long cookie)",
            "{",
            "\tstruct sched_core_cookie *ptr = (void *)cookie;",
            "",
            "\tif (ptr && refcount_dec_and_test(&ptr->refcnt)) {",
            "\t\tkfree(ptr);",
            "\t\tsched_core_put();",
            "\t}",
            "}",
            "static unsigned long sched_core_get_cookie(unsigned long cookie)",
            "{",
            "\tstruct sched_core_cookie *ptr = (void *)cookie;",
            "",
            "\tif (ptr)",
            "\t\trefcount_inc(&ptr->refcnt);",
            "",
            "\treturn cookie;",
            "}",
            "static unsigned long sched_core_update_cookie(struct task_struct *p,",
            "\t\t\t\t\t      unsigned long cookie)",
            "{",
            "\tunsigned long old_cookie;",
            "\tstruct rq_flags rf;",
            "\tstruct rq *rq;",
            "",
            "\trq = task_rq_lock(p, &rf);",
            "",
            "\t/*",
            "\t * Since creating a cookie implies sched_core_get(), and we cannot set",
            "\t * a cookie until after we've created it, similarly, we cannot destroy",
            "\t * a cookie until after we've removed it, we must have core scheduling",
            "\t * enabled here.",
            "\t */",
            "\tSCHED_WARN_ON((p->core_cookie || cookie) && !sched_core_enabled(rq));",
            "",
            "\tif (sched_core_enqueued(p))",
            "\t\tsched_core_dequeue(rq, p, DEQUEUE_SAVE);",
            "",
            "\told_cookie = p->core_cookie;",
            "\tp->core_cookie = cookie;",
            "",
            "\t/*",
            "\t * Consider the cases: !prev_cookie and !cookie.",
            "\t */",
            "\tif (cookie && task_on_rq_queued(p))",
            "\t\tsched_core_enqueue(rq, p);",
            "",
            "\t/*",
            "\t * If task is currently running, it may not be compatible anymore after",
            "\t * the cookie change, so enter the scheduler on its CPU to schedule it",
            "\t * away.",
            "\t *",
            "\t * Note that it is possible that as a result of this cookie change, the",
            "\t * core has now entered/left forced idle state. Defer accounting to the",
            "\t * next scheduling edge, rather than always forcing a reschedule here.",
            "\t */",
            "\tif (task_on_cpu(rq, p))",
            "\t\tresched_curr(rq);",
            "",
            "\ttask_rq_unlock(rq, p, &rf);",
            "",
            "\treturn old_cookie;",
            "}",
            "static unsigned long sched_core_clone_cookie(struct task_struct *p)",
            "{",
            "\tunsigned long cookie, flags;",
            "",
            "\traw_spin_lock_irqsave(&p->pi_lock, flags);",
            "\tcookie = sched_core_get_cookie(p->core_cookie);",
            "\traw_spin_unlock_irqrestore(&p->pi_lock, flags);",
            "",
            "\treturn cookie;",
            "}",
            "void sched_core_fork(struct task_struct *p)",
            "{",
            "\tRB_CLEAR_NODE(&p->core_node);",
            "\tp->core_cookie = sched_core_clone_cookie(current);",
            "}",
            "void sched_core_free(struct task_struct *p)",
            "{",
            "\tsched_core_put_cookie(p->core_cookie);",
            "}",
            "static void __sched_core_set(struct task_struct *p, unsigned long cookie)",
            "{",
            "\tcookie = sched_core_get_cookie(cookie);",
            "\tcookie = sched_core_update_cookie(p, cookie);",
            "\tsched_core_put_cookie(cookie);",
            "}",
            "int sched_core_share_pid(unsigned int cmd, pid_t pid, enum pid_type type,",
            "\t\t\t unsigned long uaddr)",
            "{",
            "\tunsigned long cookie = 0, id = 0;",
            "\tstruct task_struct *task, *p;",
            "\tstruct pid *grp;",
            "\tint err = 0;",
            "",
            "\tif (!static_branch_likely(&sched_smt_present))",
            "\t\treturn -ENODEV;",
            "",
            "\tBUILD_BUG_ON(PR_SCHED_CORE_SCOPE_THREAD != PIDTYPE_PID);",
            "\tBUILD_BUG_ON(PR_SCHED_CORE_SCOPE_THREAD_GROUP != PIDTYPE_TGID);",
            "\tBUILD_BUG_ON(PR_SCHED_CORE_SCOPE_PROCESS_GROUP != PIDTYPE_PGID);",
            "",
            "\tif (type > PIDTYPE_PGID || cmd >= PR_SCHED_CORE_MAX || pid < 0 ||",
            "\t    (cmd != PR_SCHED_CORE_GET && uaddr))",
            "\t\treturn -EINVAL;",
            "",
            "\trcu_read_lock();",
            "\tif (pid == 0) {",
            "\t\ttask = current;",
            "\t} else {",
            "\t\ttask = find_task_by_vpid(pid);",
            "\t\tif (!task) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\treturn -ESRCH;",
            "\t\t}",
            "\t}",
            "\tget_task_struct(task);",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * Check if this process has the right to modify the specified",
            "\t * process. Use the regular \"ptrace_may_access()\" checks.",
            "\t */",
            "\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {",
            "\t\terr = -EPERM;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tswitch (cmd) {",
            "\tcase PR_SCHED_CORE_GET:",
            "\t\tif (type != PIDTYPE_PID || uaddr & 7) {",
            "\t\t\terr = -EINVAL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tcookie = sched_core_clone_cookie(task);",
            "\t\tif (cookie) {",
            "\t\t\t/* XXX improve ? */",
            "\t\t\tptr_to_hashval((void *)cookie, &id);",
            "\t\t}",
            "\t\terr = put_user(id, (u64 __user *)uaddr);",
            "\t\tgoto out;",
            "",
            "\tcase PR_SCHED_CORE_CREATE:",
            "\t\tcookie = sched_core_alloc_cookie();",
            "\t\tif (!cookie) {",
            "\t\t\terr = -ENOMEM;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tbreak;",
            "",
            "\tcase PR_SCHED_CORE_SHARE_TO:",
            "\t\tcookie = sched_core_clone_cookie(current);",
            "\t\tbreak;",
            "",
            "\tcase PR_SCHED_CORE_SHARE_FROM:",
            "\t\tif (type != PIDTYPE_PID) {",
            "\t\t\terr = -EINVAL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tcookie = sched_core_clone_cookie(task);",
            "\t\t__sched_core_set(current, cookie);",
            "\t\tgoto out;",
            "",
            "\tdefault:",
            "\t\terr = -EINVAL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tif (type == PIDTYPE_PID) {",
            "\t\t__sched_core_set(task, cookie);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tread_lock(&tasklist_lock);",
            "\tgrp = task_pid_type(task, type);",
            "",
            "\tdo_each_pid_thread(grp, type, p) {",
            "\t\tif (!ptrace_may_access(p, PTRACE_MODE_READ_REALCREDS)) {",
            "\t\t\terr = -EPERM;",
            "\t\t\tgoto out_tasklist;",
            "\t\t}",
            "\t} while_each_pid_thread(grp, type, p);",
            "",
            "\tdo_each_pid_thread(grp, type, p) {",
            "\t\t__sched_core_set(p, cookie);",
            "\t} while_each_pid_thread(grp, type, p);",
            "out_tasklist:",
            "\tread_unlock(&tasklist_lock);",
            "",
            "out:",
            "\tsched_core_put_cookie(cookie);",
            "\tput_task_struct(task);",
            "\treturn err;",
            "}"
          ],
          "function_name": "sched_core_alloc_cookie, sched_core_put_cookie, sched_core_get_cookie, sched_core_update_cookie, sched_core_clone_cookie, sched_core_fork, sched_core_free, __sched_core_set, sched_core_share_pid",
          "description": "实现了核心调度 cookie 的分配、释放、获取和更新机制，包含 cookie 分配/回收、任务核心绑定变更、进程克隆共享及核心调度策略控制等功能",
          "similarity": 0.5190947651863098
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/core_sched.c",
          "start_line": 240,
          "end_line": 297,
          "content": [
            "void __sched_core_account_forceidle(struct rq *rq)",
            "{",
            "\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu_of(rq));",
            "\tu64 delta, now = rq_clock(rq->core);",
            "\tstruct rq *rq_i;",
            "\tstruct task_struct *p;",
            "\tint i;",
            "",
            "\tlockdep_assert_rq_held(rq);",
            "",
            "\tWARN_ON_ONCE(!rq->core->core_forceidle_count);",
            "",
            "\tif (rq->core->core_forceidle_start == 0)",
            "\t\treturn;",
            "",
            "\tdelta = now - rq->core->core_forceidle_start;",
            "\tif (unlikely((s64)delta <= 0))",
            "\t\treturn;",
            "",
            "\trq->core->core_forceidle_start = now;",
            "",
            "\tif (WARN_ON_ONCE(!rq->core->core_forceidle_occupation)) {",
            "\t\t/* can't be forced idle without a running task */",
            "\t} else if (rq->core->core_forceidle_count > 1 ||",
            "\t\t   rq->core->core_forceidle_occupation > 1) {",
            "\t\t/*",
            "\t\t * For larger SMT configurations, we need to scale the charged",
            "\t\t * forced idle amount since there can be more than one forced",
            "\t\t * idle sibling and more than one running cookied task.",
            "\t\t */",
            "\t\tdelta *= rq->core->core_forceidle_count;",
            "\t\tdelta = div_u64(delta, rq->core->core_forceidle_occupation);",
            "\t}",
            "",
            "\tfor_each_cpu(i, smt_mask) {",
            "\t\trq_i = cpu_rq(i);",
            "\t\tp = rq_i->core_pick ?: rq_i->curr;",
            "",
            "\t\tif (p == rq_i->idle)",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * Note: this will account forceidle to the current cpu, even",
            "\t\t * if it comes from our SMT sibling.",
            "\t\t */",
            "\t\t__account_forceidle_time(p, delta);",
            "\t}",
            "}",
            "void __sched_core_tick(struct rq *rq)",
            "{",
            "\tif (!rq->core->core_forceidle_count)",
            "\t\treturn;",
            "",
            "\tif (rq != rq->core)",
            "\t\tupdate_rq_clock(rq->core);",
            "",
            "\t__sched_core_account_forceidle(rq);",
            "}"
          ],
          "function_name": "__sched_core_account_forceidle, __sched_core_tick",
          "description": "提供强制空闲时间统计功能，通过遍历 SMT 核心计算并分摊强制空闲时间消耗，tick 中断触发强制空闲会计入逻辑",
          "similarity": 0.4732118844985962
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/core_sched.c",
          "start_line": 1,
          "end_line": 10,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "",
            "/*",
            " * A simple wrapper around refcount. An allocated sched_core_cookie's",
            " * address is used to compute the cookie of the task.",
            " */",
            "struct sched_core_cookie {",
            "\trefcount_t refcnt;",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义了 sched_core_cookie 结构体，用于核心调度系统中管理任务的 cookie 引用计数，通过结构体地址计算 cookie 值",
          "similarity": 0.4453411102294922
        }
      ]
    }
  ]
}