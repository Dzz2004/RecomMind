{
  "query": "库函数中错误处理与系统调用的差异",
  "timestamp": "2025-12-26 02:06:50",
  "retrieved_files": [
    {
      "source_file": "kernel/entry/syscall_user_dispatch.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:20:47\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `entry\\syscall_user_dispatch.c`\n\n---\n\n# entry/syscall_user_dispatch.c 技术文档\n\n## 1. 文件概述\n\n`entry/syscall_user_dispatch.c` 实现了 **系统调用用户分发（Syscall User Dispatch, SUD）** 机制，该机制允许用户空间程序通过 `prctl()` 系统调用配置一个“选择器”（selector），用于在特定条件下拦截或允许系统调用的执行。当系统调用指令指针位于指定区域之外且选择器状态为“阻塞”时，内核会回滚该系统调用并向进程发送 `SIGSYS` 信号，从而实现对系统调用的细粒度控制。此功能常用于沙箱、安全监控或调试场景。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `trigger_sigsys(struct pt_regs *regs)`  \n  构造并强制发送 `SIGSYS` 信号，携带被拦截系统调用的详细信息（如地址、系统调用号、架构等）。\n\n- `syscall_user_dispatch(struct pt_regs *regs)`  \n  系统调用入口处的分发判断逻辑。根据当前指令指针位置和用户选择器状态决定是否拦截系统调用。\n\n- `task_set_syscall_user_dispatch(struct task_struct *task, ...)`  \n  为指定任务设置系统调用用户分发配置（开启/关闭、偏移、长度、选择器地址）。\n\n- `set_syscall_user_dispatch(...)`  \n  为当前任务设置系统调用用户分发配置的封装接口，供 `prctl()` 调用。\n\n- `syscall_user_dispatch_get_config(...)`  \n  通过 `ptrace` 获取指定任务的 SUD 配置。\n\n- `syscall_user_dispatch_set_config(...)`  \n  通过 `ptrace` 设置指定任务的 SUD 配置。\n\n### 关键数据结构\n\n- `struct syscall_user_dispatch`（定义在 `<linux/syscall_user_dispatch.h>`）  \n  存储每个任务的 SUD 配置：\n  - `selector`：指向用户空间选择器字节的指针\n  - `offset` / `len`：允许直接执行系统调用的代码区域（[offset, offset+len)）\n  - `on_dispatch`：标志位，表示当前是否处于分发拦截状态\n\n- `struct ptrace_sud_config`  \n  用于 `ptrace` 接口传递 SUD 配置的结构体，包含 `mode`、`offset`、`len` 和 `selector`。\n\n## 3. 关键实现\n\n### 系统调用拦截逻辑\n\n1. **区域检查**：若当前指令指针（`instruction_pointer(regs)`）落在 `[offset, offset + len)` 范围内，则**允许**系统调用直接执行，不进行拦截。\n2. **vdso 例外**：若系统调用来自 vDSO 中的 `sigreturn`（如 `arch_syscall_is_vdso_sigreturn()` 返回 true），则跳过拦截，避免干扰信号返回路径。\n3. **选择器读取**：若配置了 `selector`，则从用户空间读取一个字节的状态值：\n   - `SYSCALL_DISPATCH_FILTER_ALLOW`（0）：允许系统调用\n   - `SYSCALL_DISPATCH_FILTER_BLOCK`（1）：触发拦截\n   - 其他值：视为非法，发送 `SIGSYS`\n4. **拦截处理**：\n   - 设置 `on_dispatch = true`\n   - 调用 `syscall_rollback()` 回滚系统调用（恢复寄存器状态）\n   - 调用 `trigger_sigsys()` 发送 `SIGSYS` 信号\n\n### 安全与健壮性设计\n\n- **地址合法性校验**：在设置 `selector` 时使用 `access_ok(untagged_addr(selector), ...)`，确保地址可访问，并处理内存标记（如 ARM MTE）场景下调试器（tracer）与被调试进程（tracee）地址标记不一致的问题。\n- **溢出防护**：检查 `offset + len <= offset` 防止整数溢出导致无效区域。\n- **权限隔离**：`ptrace` 接口允许调试器配置其他进程的 SUD，但需具备相应权限。\n\n### 信号信息构造\n\n`trigger_sigsys()` 构造的 `siginfo_t` 包含：\n- `si_signo = SIGSYS`\n- `si_code = SYS_USER_DISPATCH`\n- `si_call_addr`：触发系统调用的用户空间地址\n- `si_syscall`：系统调用号\n- `si_arch`：系统调用架构（如 x86_64、AArch64）\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/prctl.h>`：定义 `PR_SYS_DISPATCH_*` 常量\n  - `<linux/syscall_user_dispatch.h>`：定义 `struct syscall_user_dispatch` 和相关常量\n  - `<asm/syscall.h>`：提供 `syscall_get_arch()`、`syscall_get_nr()` 等架构相关接口\n  - `\"common.h\"`：可能包含内核入口通用辅助函数\n- **内核子系统**：\n  - **调度器（sched）**：访问 `current` 任务结构\n  - **信号子系统（signal）**：发送 `SIGSYS` 信号\n  - **内存管理（uaccess）**：用户空间内存访问（`__get_user`, `access_ok`）\n  - **ptrace**：支持调试器配置 SUD\n  - **ELF**：可能用于架构识别（间接依赖）\n\n## 5. 使用场景\n\n- **沙箱环境**：限制应用只能在特定代码段发起系统调用，防止恶意代码绕过安全策略。\n- **动态二进制插桩（DBI）**：工具（如 Valgrind、Intel Pin）可拦截系统调用进行分析或重定向。\n- **安全监控**：监控程序可配置选择器为“阻塞”，在 `SIGSYS` 信号处理程序中记录或审查系统调用。\n- **调试与测试**：通过 `ptrace` 动态启用/禁用 SUD，用于测试系统调用拦截逻辑。\n- **W^X 策略增强**：结合代码段只读与 SUD，确保只有可信代码路径可发起系统调用。",
      "similarity": 0.5711566805839539,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/entry/syscall_user_dispatch.c",
          "start_line": 20,
          "end_line": 122,
          "content": [
            "static void trigger_sigsys(struct pt_regs *regs)",
            "{",
            "\tstruct kernel_siginfo info;",
            "",
            "\tclear_siginfo(&info);",
            "\tinfo.si_signo = SIGSYS;",
            "\tinfo.si_code = SYS_USER_DISPATCH;",
            "\tinfo.si_call_addr = (void __user *)KSTK_EIP(current);",
            "\tinfo.si_errno = 0;",
            "\tinfo.si_arch = syscall_get_arch(current);",
            "\tinfo.si_syscall = syscall_get_nr(current, regs);",
            "",
            "\tforce_sig_info(&info);",
            "}",
            "bool syscall_user_dispatch(struct pt_regs *regs)",
            "{",
            "\tstruct syscall_user_dispatch *sd = &current->syscall_dispatch;",
            "\tchar state;",
            "",
            "\tif (likely(instruction_pointer(regs) - sd->offset < sd->len))",
            "\t\treturn false;",
            "",
            "\tif (unlikely(arch_syscall_is_vdso_sigreturn(regs)))",
            "\t\treturn false;",
            "",
            "\tif (likely(sd->selector)) {",
            "\t\t/*",
            "\t\t * access_ok() is performed once, at prctl time, when",
            "\t\t * the selector is loaded by userspace.",
            "\t\t */",
            "\t\tif (unlikely(__get_user(state, sd->selector))) {",
            "\t\t\tforce_exit_sig(SIGSEGV);",
            "\t\t\treturn true;",
            "\t\t}",
            "",
            "\t\tif (likely(state == SYSCALL_DISPATCH_FILTER_ALLOW))",
            "\t\t\treturn false;",
            "",
            "\t\tif (state != SYSCALL_DISPATCH_FILTER_BLOCK) {",
            "\t\t\tforce_exit_sig(SIGSYS);",
            "\t\t\treturn true;",
            "\t\t}",
            "\t}",
            "",
            "\tsd->on_dispatch = true;",
            "\tsyscall_rollback(current, regs);",
            "\ttrigger_sigsys(regs);",
            "",
            "\treturn true;",
            "}",
            "static int task_set_syscall_user_dispatch(struct task_struct *task, unsigned long mode,",
            "\t\t\t\t\t  unsigned long offset, unsigned long len,",
            "\t\t\t\t\t  char __user *selector)",
            "{",
            "\tswitch (mode) {",
            "\tcase PR_SYS_DISPATCH_OFF:",
            "\t\tif (offset || len || selector)",
            "\t\t\treturn -EINVAL;",
            "\t\tbreak;",
            "\tcase PR_SYS_DISPATCH_ON:",
            "\t\t/*",
            "\t\t * Validate the direct dispatcher region just for basic",
            "\t\t * sanity against overflow and a 0-sized dispatcher",
            "\t\t * region.  If the user is able to submit a syscall from",
            "\t\t * an address, that address is obviously valid.",
            "\t\t */",
            "\t\tif (offset && offset + len <= offset)",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\t/*",
            "\t\t * access_ok() will clear memory tags for tagged addresses",
            "\t\t * if current has memory tagging enabled.",
            "",
            "\t\t * To enable a tracer to set a tracees selector the",
            "\t\t * selector address must be untagged for access_ok(),",
            "\t\t * otherwise an untagged tracer will always fail to set a",
            "\t\t * tagged tracees selector.",
            "\t\t */",
            "\t\tif (selector && !access_ok(untagged_addr(selector), sizeof(*selector)))",
            "\t\t\treturn -EFAULT;",
            "",
            "\t\tbreak;",
            "\tdefault:",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\ttask->syscall_dispatch.selector = selector;",
            "\ttask->syscall_dispatch.offset = offset;",
            "\ttask->syscall_dispatch.len = len;",
            "\ttask->syscall_dispatch.on_dispatch = false;",
            "",
            "\tif (mode == PR_SYS_DISPATCH_ON)",
            "\t\tset_task_syscall_work(task, SYSCALL_USER_DISPATCH);",
            "\telse",
            "\t\tclear_task_syscall_work(task, SYSCALL_USER_DISPATCH);",
            "",
            "\treturn 0;",
            "}",
            "int set_syscall_user_dispatch(unsigned long mode, unsigned long offset,",
            "\t\t\t      unsigned long len, char __user *selector)",
            "{",
            "\treturn task_set_syscall_user_dispatch(current, mode, offset, len, selector);",
            "}"
          ],
          "function_name": "trigger_sigsys, syscall_user_dispatch, task_set_syscall_user_dispatch, set_syscall_user_dispatch",
          "description": "实现系统调用用户分发核心逻辑，包含触发SIGSYS信号处理、配置验证、拦截判断及模式切换功能",
          "similarity": 0.545140266418457
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/entry/syscall_user_dispatch.c",
          "start_line": 1,
          "end_line": 19,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 2020 Collabora Ltd.",
            " */",
            "#include <linux/sched.h>",
            "#include <linux/prctl.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/syscall_user_dispatch.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/signal.h>",
            "#include <linux/elf.h>",
            "",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task_stack.h>",
            "",
            "#include <asm/syscall.h>",
            "",
            "#include \"common.h\"",
            ""
          ],
          "function_name": null,
          "description": "包含系统调用用户分发功能所需头文件及通用定义，提供架构相关接口和内核调度必要声明",
          "similarity": 0.5408058762550354
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/entry/syscall_user_dispatch.c",
          "start_line": 127,
          "end_line": 163,
          "content": [
            "int syscall_user_dispatch_get_config(struct task_struct *task, unsigned long size,",
            "\t\t\t\t     void __user *data)",
            "{",
            "\tstruct syscall_user_dispatch *sd = &task->syscall_dispatch;",
            "\tstruct ptrace_sud_config cfg;",
            "",
            "\tif (size != sizeof(cfg))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (test_task_syscall_work(task, SYSCALL_USER_DISPATCH))",
            "\t\tcfg.mode = PR_SYS_DISPATCH_ON;",
            "\telse",
            "\t\tcfg.mode = PR_SYS_DISPATCH_OFF;",
            "",
            "\tcfg.offset = sd->offset;",
            "\tcfg.len = sd->len;",
            "\tcfg.selector = (__u64)(uintptr_t)sd->selector;",
            "",
            "\tif (copy_to_user(data, &cfg, sizeof(cfg)))",
            "\t\treturn -EFAULT;",
            "",
            "\treturn 0;",
            "}",
            "int syscall_user_dispatch_set_config(struct task_struct *task, unsigned long size,",
            "\t\t\t\t     void __user *data)",
            "{",
            "\tstruct ptrace_sud_config cfg;",
            "",
            "\tif (size != sizeof(cfg))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (copy_from_user(&cfg, data, sizeof(cfg)))",
            "\t\treturn -EFAULT;",
            "",
            "\treturn task_set_syscall_user_dispatch(task, cfg.mode, cfg.offset, cfg.len,",
            "\t\t\t\t\t      (char __user *)(uintptr_t)cfg.selector);",
            "}"
          ],
          "function_name": "syscall_user_dispatch_get_config, syscall_user_dispatch_set_config",
          "description": "提供系统调用分发配置的获取与设置接口，通过用户态指针操作实现配置参数的双向传递",
          "similarity": 0.5212379693984985
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5709402561187744,
      "chunks": [
        {
          "chunk_id": 15,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2658,
          "end_line": 2759,
          "content": [
            "static void rcu_leak_callback(struct rcu_head *rhp)",
            "{",
            "}",
            "static void check_cb_ovld_locked(struct rcu_data *rdp, struct rcu_node *rnp)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (qovld_calc <= 0)",
            "\t\treturn; // Early boot and wildcard value set.",
            "\tif (rcu_segcblist_n_cbs(&rdp->cblist) >= qovld_calc)",
            "\t\tWRITE_ONCE(rnp->cbovldmask, rnp->cbovldmask | rdp->grpmask);",
            "\telse",
            "\t\tWRITE_ONCE(rnp->cbovldmask, rnp->cbovldmask & ~rdp->grpmask);",
            "}",
            "static void check_cb_ovld(struct rcu_data *rdp)",
            "{",
            "\tstruct rcu_node *const rnp = rdp->mynode;",
            "",
            "\tif (qovld_calc <= 0 ||",
            "\t    ((rcu_segcblist_n_cbs(&rdp->cblist) >= qovld_calc) ==",
            "\t     !!(READ_ONCE(rnp->cbovldmask) & rdp->grpmask)))",
            "\t\treturn; // Early boot wildcard value or already set correctly.",
            "\traw_spin_lock_rcu_node(rnp);",
            "\tcheck_cb_ovld_locked(rdp, rnp);",
            "\traw_spin_unlock_rcu_node(rnp);",
            "}",
            "static void",
            "__call_rcu_common(struct rcu_head *head, rcu_callback_t func, bool lazy_in)",
            "{",
            "\tstatic atomic_t doublefrees;",
            "\tunsigned long flags;",
            "\tbool lazy;",
            "\tstruct rcu_data *rdp;",
            "",
            "\t/* Misaligned rcu_head! */",
            "\tWARN_ON_ONCE((unsigned long)head & (sizeof(void *) - 1));",
            "",
            "\t/* Avoid NULL dereference if callback is NULL. */",
            "\tif (WARN_ON_ONCE(!func))",
            "\t\treturn;",
            "",
            "\tif (debug_rcu_head_queue(head)) {",
            "\t\t/*",
            "\t\t * Probable double call_rcu(), so leak the callback.",
            "\t\t * Use rcu:rcu_callback trace event to find the previous",
            "\t\t * time callback was passed to call_rcu().",
            "\t\t */",
            "\t\tif (atomic_inc_return(&doublefrees) < 4) {",
            "\t\t\tpr_err(\"%s(): Double-freed CB %p->%pS()!!!  \", __func__, head, head->func);",
            "\t\t\tmem_dump_obj(head);",
            "\t\t}",
            "\t\tWRITE_ONCE(head->func, rcu_leak_callback);",
            "\t\treturn;",
            "\t}",
            "\thead->func = func;",
            "\thead->next = NULL;",
            "\tkasan_record_aux_stack_noalloc(head);",
            "\tlocal_irq_save(flags);",
            "\trdp = this_cpu_ptr(&rcu_data);",
            "\tlazy = lazy_in && !rcu_async_should_hurry();",
            "",
            "\t/* Add the callback to our list. */",
            "\tif (unlikely(!rcu_segcblist_is_enabled(&rdp->cblist))) {",
            "\t\t// This can trigger due to call_rcu() from offline CPU:",
            "\t\tWARN_ON_ONCE(rcu_scheduler_active != RCU_SCHEDULER_INACTIVE);",
            "\t\tWARN_ON_ONCE(!rcu_is_watching());",
            "\t\t// Very early boot, before rcu_init().  Initialize if needed",
            "\t\t// and then drop through to queue the callback.",
            "\t\tif (rcu_segcblist_empty(&rdp->cblist))",
            "\t\t\trcu_segcblist_init(&rdp->cblist);",
            "\t}",
            "",
            "\tcheck_cb_ovld(rdp);",
            "",
            "\tif (unlikely(rcu_rdp_is_offloaded(rdp)))",
            "\t\tcall_rcu_nocb(rdp, head, func, flags, lazy);",
            "\telse",
            "\t\tcall_rcu_core(rdp, head, func, flags);",
            "\tlocal_irq_restore(flags);",
            "}",
            "void call_rcu_hurry(struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\treturn __call_rcu_common(head, func, false);",
            "}",
            "void call_rcu(struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\treturn __call_rcu_common(head, func, IS_ENABLED(CONFIG_RCU_LAZY));",
            "}",
            "static __always_inline void",
            "debug_rcu_bhead_unqueue(struct kvfree_rcu_bulk_data *bhead)",
            "{",
            "#ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD",
            "\tint i;",
            "",
            "\tfor (i = 0; i < bhead->nr_records; i++)",
            "\t\tdebug_rcu_head_unqueue((struct rcu_head *)(bhead->records[i]));",
            "#endif",
            "}",
            "static inline void",
            "krc_this_cpu_unlock(struct kfree_rcu_cpu *krcp, unsigned long flags)",
            "{",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "}"
          ],
          "function_name": "rcu_leak_callback, check_cb_ovld_locked, check_cb_ovld, __call_rcu_common, call_rcu_hurry, call_rcu, debug_rcu_bhead_unqueue, krc_this_cpu_unlock",
          "description": "实现RCU回调管理，检查回调过载并处理双释放情况，__call_rcu_common负责将回调添加到RCU回调列表并处理延迟释放逻辑。",
          "similarity": 0.5357532501220703
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 438,
          "end_line": 544,
          "content": [
            "static int param_set_first_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : j);",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "static int param_set_next_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : (j ?: 1));",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "unsigned long rcu_get_gp_seq(void)",
            "{",
            "\treturn READ_ONCE(rcu_state.gp_seq);",
            "}",
            "unsigned long rcu_exp_batches_completed(void)",
            "{",
            "\treturn rcu_state.expedited_sequence;",
            "}",
            "void rcutorture_get_gp_data(enum rcutorture_type test_type, int *flags,",
            "\t\t\t    unsigned long *gp_seq)",
            "{",
            "\tswitch (test_type) {",
            "\tcase RCU_FLAVOR:",
            "\t\t*flags = READ_ONCE(rcu_state.gp_flags);",
            "\t\t*gp_seq = rcu_seq_current(&rcu_state.gp_seq);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "}",
            "static void late_wakeup_func(struct irq_work *work)",
            "{",
            "}",
            "noinstr void rcu_irq_work_resched(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tif (IS_ENABLED(CONFIG_GENERIC_ENTRY) && !(current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tif (IS_ENABLED(CONFIG_KVM_XFER_TO_GUEST_WORK) && (current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tinstrumentation_begin();",
            "\tif (do_nocb_deferred_wakeup(rdp) && need_resched()) {",
            "\t\tirq_work_queue(this_cpu_ptr(&late_wakeup_work));",
            "\t}",
            "\tinstrumentation_end();",
            "}",
            "void rcu_irq_exit_check_preempt(void)",
            "{",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nesting() <= 0,",
            "\t\t\t \"RCU dynticks_nesting counter underflow/zero!\");",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nmi_nesting() !=",
            "\t\t\t DYNTICK_IRQ_NONIDLE,",
            "\t\t\t \"Bad RCU  dynticks_nmi_nesting counter\\n\");",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"RCU in extended quiescent state!\");",
            "}",
            "void __rcu_irq_enter_check_tick(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\t// If we're here from NMI there's nothing to do.",
            "\tif (in_nmi())",
            "\t\treturn;",
            "",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"Illegal rcu_irq_enter_check_tick() from extended quiescent state\");",
            "",
            "\tif (!tick_nohz_full_cpu(rdp->cpu) ||",
            "\t    !READ_ONCE(rdp->rcu_urgent_qs) ||",
            "\t    READ_ONCE(rdp->rcu_forced_tick)) {",
            "\t\t// RCU doesn't need nohz_full help from this CPU, or it is",
            "\t\t// already getting that help.",
            "\t\treturn;",
            "\t}",
            "",
            "\t// We get here only when not in an extended quiescent state and",
            "\t// from interrupts (as opposed to NMIs).  Therefore, (1) RCU is",
            "\t// already watching and (2) The fact that we are in an interrupt",
            "\t// handler and that the rcu_node lock is an irq-disabled lock",
            "\t// prevents self-deadlock.  So we can safely recheck under the lock.",
            "\t// Note that the nohz_full state currently cannot change.",
            "\traw_spin_lock_rcu_node(rdp->mynode);",
            "\tif (READ_ONCE(rdp->rcu_urgent_qs) && !rdp->rcu_forced_tick) {",
            "\t\t// A nohz_full CPU is in the kernel and RCU needs a",
            "\t\t// quiescent state.  Turn on the tick!",
            "\t\tWRITE_ONCE(rdp->rcu_forced_tick, true);",
            "\t\ttick_dep_set_cpu(rdp->cpu, TICK_DEP_BIT_RCU);",
            "\t}",
            "\traw_spin_unlock_rcu_node(rdp->mynode);",
            "}"
          ],
          "function_name": "param_set_first_fqs_jiffies, param_set_next_fqs_jiffies, rcu_get_gp_seq, rcu_exp_batches_completed, rcutorture_get_gp_data, late_wakeup_func, rcu_irq_work_resched, rcu_irq_exit_check_preempt, __rcu_irq_enter_check_tick",
          "description": "实现参数配置回调函数和中断上下文RCU工作重排逻辑，管理grace period序列号读取及nohz_full模式下的tick依赖关系。",
          "similarity": 0.5215865969657898
        },
        {
          "chunk_id": 17,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3121,
          "end_line": 3265,
          "content": [
            "static bool",
            "need_offload_krc(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\tif (!list_empty(&krcp->bulk_head[i]))",
            "\t\t\treturn true;",
            "",
            "\treturn !!READ_ONCE(krcp->head);",
            "}",
            "static bool",
            "need_wait_for_krwp_work(struct kfree_rcu_cpu_work *krwp)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\tif (!list_empty(&krwp->bulk_head_free[i]))",
            "\t\t\treturn true;",
            "",
            "\treturn !!krwp->head_free;",
            "}",
            "static int krc_count(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tint sum = atomic_read(&krcp->head_count);",
            "\tint i;",
            "",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\tsum += atomic_read(&krcp->bulk_count[i]);",
            "",
            "\treturn sum;",
            "}",
            "static void",
            "__schedule_delayed_monitor_work(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tlong delay, delay_left;",
            "",
            "\tdelay = krc_count(krcp) >= KVFREE_BULK_MAX_ENTR ? 1:KFREE_DRAIN_JIFFIES;",
            "\tif (delayed_work_pending(&krcp->monitor_work)) {",
            "\t\tdelay_left = krcp->monitor_work.timer.expires - jiffies;",
            "\t\tif (delay < delay_left)",
            "\t\t\tmod_delayed_work(system_unbound_wq, &krcp->monitor_work, delay);",
            "\t\treturn;",
            "\t}",
            "\tqueue_delayed_work(system_unbound_wq, &krcp->monitor_work, delay);",
            "}",
            "static void",
            "schedule_delayed_monitor_work(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\t__schedule_delayed_monitor_work(krcp);",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "}",
            "static void",
            "kvfree_rcu_drain_ready(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tstruct list_head bulk_ready[FREE_N_CHANNELS];",
            "\tstruct kvfree_rcu_bulk_data *bnode, *n;",
            "\tstruct rcu_head *head_ready = NULL;",
            "\tunsigned long flags;",
            "\tint i;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++) {",
            "\t\tINIT_LIST_HEAD(&bulk_ready[i]);",
            "",
            "\t\tlist_for_each_entry_safe_reverse(bnode, n, &krcp->bulk_head[i], list) {",
            "\t\t\tif (!poll_state_synchronize_rcu_full(&bnode->gp_snap))",
            "\t\t\t\tbreak;",
            "",
            "\t\t\tatomic_sub(bnode->nr_records, &krcp->bulk_count[i]);",
            "\t\t\tlist_move(&bnode->list, &bulk_ready[i]);",
            "\t\t}",
            "\t}",
            "",
            "\tif (krcp->head && poll_state_synchronize_rcu(krcp->head_gp_snap)) {",
            "\t\thead_ready = krcp->head;",
            "\t\tatomic_set(&krcp->head_count, 0);",
            "\t\tWRITE_ONCE(krcp->head, NULL);",
            "\t}",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++) {",
            "\t\tlist_for_each_entry_safe(bnode, n, &bulk_ready[i], list)",
            "\t\t\tkvfree_rcu_bulk(krcp, bnode, i);",
            "\t}",
            "",
            "\tif (head_ready)",
            "\t\tkvfree_rcu_list(head_ready);",
            "}",
            "static bool",
            "kvfree_rcu_queue_batch(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tunsigned long flags;",
            "\tbool queued = false;",
            "\tint i, j;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "",
            "\t// Attempt to start a new batch.",
            "\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\tstruct kfree_rcu_cpu_work *krwp = &(krcp->krw_arr[i]);",
            "",
            "\t\t// Try to detach bulk_head or head and attach it, only when",
            "\t\t// all channels are free.  Any channel is not free means at krwp",
            "\t\t// there is on-going rcu work to handle krwp's free business.",
            "\t\tif (need_wait_for_krwp_work(krwp))",
            "\t\t\tcontinue;",
            "",
            "\t\t// kvfree_rcu_drain_ready() might handle this krcp, if so give up.",
            "\t\tif (need_offload_krc(krcp)) {",
            "\t\t\t// Channel 1 corresponds to the SLAB-pointer bulk path.",
            "\t\t\t// Channel 2 corresponds to vmalloc-pointer bulk path.",
            "\t\t\tfor (j = 0; j < FREE_N_CHANNELS; j++) {",
            "\t\t\t\tif (list_empty(&krwp->bulk_head_free[j])) {",
            "\t\t\t\t\tatomic_set(&krcp->bulk_count[j], 0);",
            "\t\t\t\t\tlist_replace_init(&krcp->bulk_head[j],",
            "\t\t\t\t\t\t&krwp->bulk_head_free[j]);",
            "\t\t\t\t}",
            "\t\t\t}",
            "",
            "\t\t\t// Channel 3 corresponds to both SLAB and vmalloc",
            "\t\t\t// objects queued on the linked list.",
            "\t\t\tif (!krwp->head_free) {",
            "\t\t\t\tkrwp->head_free = krcp->head;",
            "\t\t\t\tget_state_synchronize_rcu_full(&krwp->head_free_gp_snap);",
            "\t\t\t\tatomic_set(&krcp->head_count, 0);",
            "\t\t\t\tWRITE_ONCE(krcp->head, NULL);",
            "\t\t\t}",
            "",
            "\t\t\t// One work is per one batch, so there are three",
            "\t\t\t// \"free channels\", the batch can handle. Break",
            "\t\t\t// the loop since it is done with this CPU thus",
            "\t\t\t// queuing an RCU work is _always_ success here.",
            "\t\t\tqueued = queue_rcu_work(system_unbound_wq, &krwp->rcu_work);",
            "\t\t\tWARN_ON_ONCE(!queued);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "\treturn queued;",
            "}"
          ],
          "function_name": "need_offload_krc, need_wait_for_krwp_work, krc_count, __schedule_delayed_monitor_work, schedule_delayed_monitor_work, kvfree_rcu_drain_ready, kvfree_rcu_queue_batch",
          "description": "调度延迟清理工作，通过统计待处理对象数量动态调整清理时机，支持分批处理以优化系统资源利用。",
          "similarity": 0.509074330329895
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2273,
          "end_line": 2388,
          "content": [
            "void rcu_sched_clock_irq(int user)",
            "{",
            "\tunsigned long j;",
            "",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU)) {",
            "\t\tj = jiffies;",
            "\t\tWARN_ON_ONCE(time_before(j, __this_cpu_read(rcu_data.last_sched_clock)));",
            "\t\t__this_cpu_write(rcu_data.last_sched_clock, j);",
            "\t}",
            "\ttrace_rcu_utilization(TPS(\"Start scheduler-tick\"));",
            "\tlockdep_assert_irqs_disabled();",
            "\traw_cpu_inc(rcu_data.ticks_this_gp);",
            "\t/* The load-acquire pairs with the store-release setting to true. */",
            "\tif (smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs))) {",
            "\t\t/* Idle and userspace execution already are quiescent states. */",
            "\t\tif (!rcu_is_cpu_rrupt_from_idle() && !user) {",
            "\t\t\tset_tsk_need_resched(current);",
            "\t\t\tset_preempt_need_resched();",
            "\t\t}",
            "\t\t__this_cpu_write(rcu_data.rcu_urgent_qs, false);",
            "\t}",
            "\trcu_flavor_sched_clock_irq(user);",
            "\tif (rcu_pending(user))",
            "\t\tinvoke_rcu_core();",
            "\tif (user || rcu_is_cpu_rrupt_from_idle())",
            "\t\trcu_note_voluntary_context_switch(current);",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\ttrace_rcu_utilization(TPS(\"End scheduler-tick\"));",
            "}",
            "static void force_qs_rnp(int (*f)(struct rcu_data *rdp))",
            "{",
            "\tint cpu;",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trcu_state.cbovld = rcu_state.cbovldnext;",
            "\trcu_state.cbovldnext = false;",
            "\trcu_for_each_leaf_node(rnp) {",
            "\t\tunsigned long mask = 0;",
            "\t\tunsigned long rsmask = 0;",
            "",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\trcu_state.cbovldnext |= !!rnp->cbovldmask;",
            "\t\tif (rnp->qsmask == 0) {",
            "\t\t\tif (rcu_preempt_blocked_readers_cgp(rnp)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No point in scanning bits because they",
            "\t\t\t\t * are all zero.  But we might need to",
            "\t\t\t\t * priority-boost blocked readers.",
            "\t\t\t\t */",
            "\t\t\t\trcu_initiate_boost(rnp, flags);",
            "\t\t\t\t/* rcu_initiate_boost() releases rnp->lock */",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rnp->qsmask) {",
            "\t\t\tstruct rcu_data *rdp;",
            "\t\t\tint ret;",
            "",
            "\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\t\t\tret = f(rdp);",
            "\t\t\tif (ret > 0) {",
            "\t\t\t\tmask |= rdp->grpmask;",
            "\t\t\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\t\t}",
            "\t\t\tif (ret < 0)",
            "\t\t\t\trsmask |= rdp->grpmask;",
            "\t\t}",
            "\t\tif (mask != 0) {",
            "\t\t\t/* Idle/offline CPUs, report (releases rnp->lock). */",
            "\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\t} else {",
            "\t\t\t/* Nothing to do here, so just drop the lock. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t}",
            "",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rsmask)",
            "\t\t\tresched_cpu(cpu);",
            "\t}",
            "}",
            "void rcu_force_quiescent_state(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool ret;",
            "\tstruct rcu_node *rnp;",
            "\tstruct rcu_node *rnp_old = NULL;",
            "",
            "\t/* Funnel through hierarchy to reduce memory contention. */",
            "\trnp = raw_cpu_read(rcu_data.mynode);",
            "\tfor (; rnp != NULL; rnp = rnp->parent) {",
            "\t\tret = (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) ||",
            "\t\t       !raw_spin_trylock(&rnp->fqslock);",
            "\t\tif (rnp_old != NULL)",
            "\t\t\traw_spin_unlock(&rnp_old->fqslock);",
            "\t\tif (ret)",
            "\t\t\treturn;",
            "\t\trnp_old = rnp;",
            "\t}",
            "\t/* rnp_old == rcu_get_root(), rnp == NULL. */",
            "",
            "\t/* Reached the root of the rcu_node tree, acquire lock. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp_old, flags);",
            "\traw_spin_unlock(&rnp_old->fqslock);",
            "\tif (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\t\treturn;  /* Someone beat us to it. */",
            "\t}",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\trcu_gp_kthread_wake();",
            "}"
          ],
          "function_name": "rcu_sched_clock_irq, force_qs_rnp, rcu_force_quiescent_state",
          "description": "实现强制quiescent状态触发与调度器中断处理，包含优先级提升、回调加速及grace period推进等关键控制流，维护RCU状态一致性",
          "similarity": 0.5000430941581726
        },
        {
          "chunk_id": 25,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4602,
          "end_line": 4729,
          "content": [
            "void rcu_cpu_starting(unsigned int cpu)",
            "{",
            "\tunsigned long mask;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "\tbool newcpu;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tif (rdp->cpu_started)",
            "\t\treturn;",
            "\trdp->cpu_started = true;",
            "",
            "\trnp = rdp->mynode;",
            "\tmask = rdp->grpmask;",
            "\tarch_spin_lock(&rcu_state.ofl_lock);",
            "\trcu_dynticks_eqs_online();",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\traw_spin_lock_rcu_node(rnp);",
            "\tWRITE_ONCE(rnp->qsmaskinitnext, rnp->qsmaskinitnext | mask);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "\tnewcpu = !(rnp->expmaskinitnext & mask);",
            "\trnp->expmaskinitnext |= mask;",
            "\t/* Allow lockless access for expedited grace periods. */",
            "\tsmp_store_release(&rcu_state.ncpus, rcu_state.ncpus + newcpu); /* ^^^ */",
            "\tASSERT_EXCLUSIVE_WRITER(rcu_state.ncpus);",
            "\trcu_gpnum_ovf(rnp, rdp); /* Offline-induced counter wrap? */",
            "\trdp->rcu_onl_gp_seq = READ_ONCE(rcu_state.gp_seq);",
            "\trdp->rcu_onl_gp_flags = READ_ONCE(rcu_state.gp_flags);",
            "",
            "\t/* An incoming CPU should never be blocking a grace period. */",
            "\tif (WARN_ON_ONCE(rnp->qsmask & mask)) { /* RCU waiting on incoming CPU? */",
            "\t\t/* rcu_report_qs_rnp() *really* wants some flags to restore */",
            "\t\tunsigned long flags;",
            "",
            "\t\tlocal_irq_save(flags);",
            "\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\t/* Report QS -after- changing ->qsmaskinitnext! */",
            "\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t} else {",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\t}",
            "\tarch_spin_unlock(&rcu_state.ofl_lock);",
            "\tsmp_store_release(&rdp->beenonline, true);",
            "\tsmp_mb(); /* Ensure RCU read-side usage follows above initialization. */",
            "}",
            "void rcu_report_dead(unsigned int cpu)",
            "{",
            "\tunsigned long flags, seq_flags;",
            "\tunsigned long mask;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tstruct rcu_node *rnp = rdp->mynode;  /* Outgoing CPU's rdp & rnp. */",
            "",
            "\t// Do any dangling deferred wakeups.",
            "\tdo_nocb_deferred_wakeup(rdp);",
            "",
            "\trcu_preempt_deferred_qs(current);",
            "",
            "\t/* Remove outgoing CPU from mask in the leaf rcu_node structure. */",
            "\tmask = rdp->grpmask;",
            "\tlocal_irq_save(seq_flags);",
            "\tarch_spin_lock(&rcu_state.ofl_lock);",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags); /* Enforce GP memory-order guarantee. */",
            "\trdp->rcu_ofl_gp_seq = READ_ONCE(rcu_state.gp_seq);",
            "\trdp->rcu_ofl_gp_flags = READ_ONCE(rcu_state.gp_flags);",
            "\tif (rnp->qsmask & mask) { /* RCU waiting on outgoing CPU? */",
            "\t\t/* Report quiescent state -before- changing ->qsmaskinitnext! */",
            "\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t}",
            "\tWRITE_ONCE(rnp->qsmaskinitnext, rnp->qsmaskinitnext & ~mask);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tarch_spin_unlock(&rcu_state.ofl_lock);",
            "\tlocal_irq_restore(seq_flags);",
            "",
            "\trdp->cpu_started = false;",
            "}",
            "void rcutree_migrate_callbacks(int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *my_rdp;",
            "\tstruct rcu_node *my_rnp;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tbool needwake;",
            "",
            "\tif (rcu_rdp_is_offloaded(rdp))",
            "\t\treturn;",
            "",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\tif (rcu_segcblist_empty(&rdp->cblist)) {",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\treturn;  /* No callbacks to migrate. */",
            "\t}",
            "",
            "\tWARN_ON_ONCE(rcu_rdp_cpu_online(rdp));",
            "\trcu_barrier_entrain(rdp);",
            "\tmy_rdp = this_cpu_ptr(&rcu_data);",
            "\tmy_rnp = my_rdp->mynode;",
            "\trcu_nocb_lock(my_rdp); /* irqs already disabled. */",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(my_rdp, NULL, jiffies, false));",
            "\traw_spin_lock_rcu_node(my_rnp); /* irqs already disabled. */",
            "\t/* Leverage recent GPs and set GP for new callbacks. */",
            "\tneedwake = rcu_advance_cbs(my_rnp, rdp) ||",
            "\t\t   rcu_advance_cbs(my_rnp, my_rdp);",
            "\trcu_segcblist_merge(&my_rdp->cblist, &rdp->cblist);",
            "\traw_spin_unlock(&rcu_state.barrier_lock); /* irqs remain disabled. */",
            "\tneedwake = needwake || rcu_advance_cbs(my_rnp, my_rdp);",
            "\trcu_segcblist_disable(&rdp->cblist);",
            "\tWARN_ON_ONCE(rcu_segcblist_empty(&my_rdp->cblist) != !rcu_segcblist_n_cbs(&my_rdp->cblist));",
            "\tcheck_cb_ovld_locked(my_rdp, my_rnp);",
            "\tif (rcu_rdp_is_offloaded(my_rdp)) {",
            "\t\traw_spin_unlock_rcu_node(my_rnp); /* irqs remain disabled. */",
            "\t\t__call_rcu_nocb_wake(my_rdp, true, flags);",
            "\t} else {",
            "\t\trcu_nocb_unlock(my_rdp); /* irqs remain disabled. */",
            "\t\traw_spin_unlock_rcu_node(my_rnp); /* irqs remain disabled. */",
            "\t}",
            "\tlocal_irq_restore(flags);",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "\tlockdep_assert_irqs_enabled();",
            "\tWARN_ONCE(rcu_segcblist_n_cbs(&rdp->cblist) != 0 ||",
            "\t\t  !rcu_segcblist_empty(&rdp->cblist),",
            "\t\t  \"rcu_cleanup_dead_cpu: Callbacks on offline CPU %d: qlen=%lu, 1stCB=%p\\n\",",
            "\t\t  cpu, rcu_segcblist_n_cbs(&rdp->cblist),",
            "\t\t  rcu_segcblist_first_cb(&rdp->cblist));",
            "}"
          ],
          "function_name": "rcu_cpu_starting, rcu_report_dead, rcutree_migrate_callbacks",
          "description": "处理CPU上线时的RCU状态更新，通过修改rcu_node结构体中的掩码和序列号，确保并发访问的安全性，并在必要时触发静默状态报告。",
          "similarity": 0.49767056107521057
        }
      ]
    },
    {
      "source_file": "kernel/entry/common.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:19:03\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `entry\\common.c`\n\n---\n\n# entry\\common.c 技术文档\n\n## 文件概述\n\n`entry\\common.c` 是 Linux 内核中处理系统调用入口/出口以及中断入口/出口路径的通用逻辑实现文件。该文件提供了一套架构无关的通用函数，用于在用户态与内核态之间切换时执行必要的上下文跟踪、审计、跟踪点、安全检查（如 seccomp）、信号处理、调度等工作。其目标是统一不同架构在系统调用和中断处理路径上的共性逻辑，减少重复代码。\n\n## 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `syscall_trace_enter()` | 系统调用进入时的通用处理函数，依次处理用户态分发、ptrace 跟踪、seccomp 安全检查、tracepoint 和审计 |\n| `syscall_enter_from_user_mode_prepare()` | 从用户模式进入系统调用前的准备，启用中断并进入内核上下文 |\n| `exit_to_user_mode_loop()` | 在返回用户空间前循环处理所有待办工作项（如调度、信号、uprobe、livepatch 等） |\n| `syscall_exit_work()` | 系统调用退出时的通用处理，包括审计、tracepoint、ptrace 退出报告等 |\n| `irqentry_enter()` / `irqentry_exit()` | 中断入口/出口的通用处理，管理 RCU、上下文跟踪、KMSAN、lockdep 等 |\n| `irqentry_enter_from_user_mode()` / `irqentry_exit_to_user_mode()` | 从中断上下文进入/退出用户模式的专用路径 |\n| `raw_irqentry_exit_cond_resched()` | 中断退出时的条件调度检查（仅在非抢占计数为 0 时） |\n\n### 关键数据结构\n\n- `irqentry_state_t`：记录中断入口状态，主要用于判断是否需要在退出时执行 RCU 相关操作。\n- `SYSCALL_WORK_*` 和 `_TIF_*` 标志位：用于标识待处理的工作类型（如 trace、seccomp、信号、调度等）。\n\n## 关键实现\n\n### 系统调用入口处理流程（`syscall_trace_enter`）\n\n1. **Syscall User Dispatch 优先处理**：若设置了 `SYSCALL_WORK_SYSCALL_USER_DISPATCH`，调用 `syscall_user_dispatch()`，若返回 true 则直接终止系统调用（返回 `-1`），因为此时 ABI 可能无效。\n2. **Ptrace 跟踪**：若设置了 `SYSCALL_WORK_SYSCALL_TRACE` 或 `SYSCALL_WORK_SYSCALL_EMU`，调用 `ptrace_report_syscall_entry()`。若 tracer 修改了行为或启用了 `SYSCALL_EMU`，则终止系统调用。\n3. **Seccomp 安全检查**：在 ptrace 之后执行，以捕获 tracer 可能引入的变更。调用 `__secure_computing()`，若返回 `-1` 则拒绝系统调用。\n4. **重新获取系统调用号**：上述步骤可能修改了系统调用号，需重新通过 `syscall_get_nr()` 获取。\n5. **Tracepoint 触发**：若启用 `SYSCALL_WORK_SYSCALL_TRACEPOINT`，触发 `trace_sys_enter`，并再次重新获取系统调用号（因 BPF 或 kprobe 可能修改）。\n6. **审计日志**：调用 `syscall_enter_audit()` 记录审计事件。\n7. **返回最终系统调用号或错误码**。\n\n### 返回用户空间前的工作循环（`exit_to_user_mode_loop`）\n\n- 使用 `while (ti_work & EXIT_TO_USER_MODE_WORK)` 循环处理所有待办工作，确保在返回用户态前完成：\n  - 调度（`_TIF_NEED_RESCHED`）\n  - Uprobe 通知（`_TIF_UPROBE`）\n  - Livepatch 状态更新（`_TIF_PATCH_PENDING`）\n  - 信号处理（`_TIF_SIGPENDING | _TIF_NOTIFY_SIGNAL`）\n  - 用户态恢复工作（`_TIF_NOTIFY_RESUME`）\n  - 架构特定工作（`arch_exit_to_user_mode_work`）\n- 每次循环启用中断（`local_irq_enable_exit_to_user`），处理完后再关闭中断并重新读取线程标志（`read_thread_flags()`），以应对处理过程中新产生的工作项。\n- 最后调用 `tick_nohz_user_enter_prepare()` 处理 NO_HZ 模式下的 tick 准备。\n\n### 中断入口/出口的 RCU 与上下文管理\n\n- **从中断进入用户态**：调用 `enter_from_user_mode()`，启用中断。\n- **从内核态中断入口**：\n  - 若当前是 idle 任务且非 `TINY_RCU`，无条件调用 `ct_irq_enter()` 以确保 RCU 状态一致（避免嵌套中断导致 grace period 错误结束）。\n  - 否则调用 `rcu_irq_enter_check_tick()`。\n- 所有路径均正确处理 `lockdep`、`KMSAN`（解除寄存器毒化）和 `trace_hardirqs_off` 的顺序，确保调试和安全工具正常工作。\n\n### 条件调度支持（Preemption）\n\n- `raw_irqentry_exit_cond_resched()` 在中断退出且 `preempt_count() == 0` 时检查是否需要调度。\n- 支持动态抢占（`CONFIG_PREEMPT_DYNAMIC`），通过 `static_call` 或 `static_key` 实现运行时切换，避免编译时硬编码。\n\n## 依赖关系\n\n### 头文件依赖\n- `<linux/context_tracking.h>`：上下文跟踪（用户/内核态切换）\n- `<linux/resume_user_mode.h>`：用户态恢复工作\n- `<linux/seccomp.h>`（隐式通过 `__secure_computing`）：系统调用过滤\n- `<linux/audit.h>`：审计子系统\n- `<linux/ptrace.h>`（隐式）：ptrace 跟踪\n- `<linux/livepatch.h>`：内核热补丁\n- `<linux/uprobes.h>`（隐式）：用户态探针\n- `<linux/rcupdate.h>`：RCU 机制\n- `<linux/kmsan.h>`：Kernel Memory Sanitizer 支持\n- `<trace/events/syscalls.h>`：系统调用跟踪点\n\n### 架构依赖\n- 依赖架构特定实现：\n  - `syscall_get_arguments()` / `syscall_get_nr()` / `syscall_get_return_value()`\n  - `user_mode()` / `regs_irqs_disabled()`\n  - `arch_do_signal_or_restart()`（弱符号，默认空实现）\n  - `arch_exit_to_user_mode_work()`\n\n### 子系统交互\n- **RCU**：管理中断和用户态切换时的宽限期\n- **Scheduler**：处理 `need_resched()` 和 `schedule()`\n- **Security**：seccomp、audit\n- **Tracing**：ftrace、kprobe、uprobe、BPF\n- **Livepatch**：动态补丁状态更新\n\n## 使用场景\n\n1. **系统调用入口路径**：  \n   当用户程序执行 `syscall` 指令（或其他系统调用机制）进入内核时，架构代码调用 `syscall_trace_enter()` 执行通用预处理。\n\n2. **系统调用出口路径**：  \n   系统调用返回前，若存在待处理工作（如审计、tracepoint），调用 `syscall_exit_work()`。\n\n3. **中断处理返回用户空间**：  \n   中断处理完成后，若返回用户态，调用 `irqentry_exit_to_user_mode()`，进而触发 `exit_to_user_mode_loop()` 处理所有 pending work。\n\n4. **中断嵌套与 idle 任务处理**：  \n   在 idle 任务中发生中断时，确保 RCU 正确进入 IRQ 上下文，防止 grace period 错误终止。\n\n5. **动态抢占支持**：  \n   在支持动态抢占的系统中，中断退出时根据运行时配置决定是否执行条件调度。\n\n6. **调试与安全工具集成**：  \n   为 KMSAN、Lockdep、ftrace、audit、seccomp 等子系统提供统一的入口/出口钩子，确保工具链在系统调用和中断路径上正常工作。",
      "similarity": 0.5687812566757202,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/entry/common.c",
          "start_line": 18,
          "end_line": 124,
          "content": [
            "static inline void syscall_enter_audit(struct pt_regs *regs, long syscall)",
            "{",
            "\tif (unlikely(audit_context())) {",
            "\t\tunsigned long args[6];",
            "",
            "\t\tsyscall_get_arguments(current, regs, args);",
            "\t\taudit_syscall_entry(syscall, args[0], args[1], args[2], args[3]);",
            "\t}",
            "}",
            "long syscall_trace_enter(struct pt_regs *regs, long syscall,",
            "\t\t\t\tunsigned long work)",
            "{",
            "\tlong ret = 0;",
            "",
            "\t/*",
            "\t * Handle Syscall User Dispatch.  This must comes first, since",
            "\t * the ABI here can be something that doesn't make sense for",
            "\t * other syscall_work features.",
            "\t */",
            "\tif (work & SYSCALL_WORK_SYSCALL_USER_DISPATCH) {",
            "\t\tif (syscall_user_dispatch(regs))",
            "\t\t\treturn -1L;",
            "\t}",
            "",
            "\t/* Handle ptrace */",
            "\tif (work & (SYSCALL_WORK_SYSCALL_TRACE | SYSCALL_WORK_SYSCALL_EMU)) {",
            "\t\tret = ptrace_report_syscall_entry(regs);",
            "\t\tif (ret || (work & SYSCALL_WORK_SYSCALL_EMU))",
            "\t\t\treturn -1L;",
            "\t}",
            "",
            "\t/* Do seccomp after ptrace, to catch any tracer changes. */",
            "\tif (work & SYSCALL_WORK_SECCOMP) {",
            "\t\tret = __secure_computing(NULL);",
            "\t\tif (ret == -1L)",
            "\t\t\treturn ret;",
            "\t}",
            "",
            "\t/* Either of the above might have changed the syscall number */",
            "\tsyscall = syscall_get_nr(current, regs);",
            "",
            "\tif (unlikely(work & SYSCALL_WORK_SYSCALL_TRACEPOINT)) {",
            "\t\ttrace_sys_enter(regs, syscall);",
            "\t\t/*",
            "\t\t * Probes or BPF hooks in the tracepoint may have changed the",
            "\t\t * system call number as well.",
            "\t\t */",
            "\t\tsyscall = syscall_get_nr(current, regs);",
            "\t}",
            "",
            "\tsyscall_enter_audit(regs, syscall);",
            "",
            "\treturn ret ? : syscall;",
            "}",
            "noinstr void syscall_enter_from_user_mode_prepare(struct pt_regs *regs)",
            "{",
            "\tenter_from_user_mode(regs);",
            "\tinstrumentation_begin();",
            "\tlocal_irq_enable();",
            "\tinstrumentation_end();",
            "}",
            "void __weak arch_do_signal_or_restart(struct pt_regs *regs) { }",
            "__always_inline unsigned long exit_to_user_mode_loop(struct pt_regs *regs,",
            "\t\t\t\t\t\t     unsigned long ti_work)",
            "{",
            "\t/*",
            "\t * Before returning to user space ensure that all pending work",
            "\t * items have been completed.",
            "\t */",
            "\twhile (ti_work & EXIT_TO_USER_MODE_WORK) {",
            "",
            "\t\tlocal_irq_enable_exit_to_user(ti_work);",
            "",
            "\t\tif (ti_work & _TIF_NEED_RESCHED)",
            "\t\t\tschedule();",
            "",
            "\t\tif (ti_work & _TIF_UPROBE)",
            "\t\t\tuprobe_notify_resume(regs);",
            "",
            "\t\tif (ti_work & _TIF_PATCH_PENDING)",
            "\t\t\tklp_update_patch_state(current);",
            "",
            "\t\tif (ti_work & (_TIF_SIGPENDING | _TIF_NOTIFY_SIGNAL))",
            "\t\t\tarch_do_signal_or_restart(regs);",
            "",
            "\t\tif (ti_work & _TIF_NOTIFY_RESUME)",
            "\t\t\tresume_user_mode_work(regs);",
            "",
            "\t\t/* Architecture specific TIF work */",
            "\t\tarch_exit_to_user_mode_work(regs, ti_work);",
            "",
            "\t\t/*",
            "\t\t * Disable interrupts and reevaluate the work flags as they",
            "\t\t * might have changed while interrupts and preemption was",
            "\t\t * enabled above.",
            "\t\t */",
            "\t\tlocal_irq_disable_exit_to_user();",
            "",
            "\t\t/* Check if any of the above work has queued a deferred wakeup */",
            "\t\ttick_nohz_user_enter_prepare();",
            "",
            "\t\tti_work = read_thread_flags();",
            "\t}",
            "",
            "\t/* Return the latest work state for arch_exit_to_user_mode() */",
            "\treturn ti_work;",
            "}"
          ],
          "function_name": "syscall_enter_audit, syscall_trace_enter, syscall_enter_from_user_mode_prepare, arch_do_signal_or_restart, exit_to_user_mode_loop",
          "description": "实现系统调用入口处理逻辑，依次处理审计追踪、指针观察、seccomp过滤及架构特定工作，通过位掩码控制不同安全机制的触发顺序并更新系统调用号",
          "similarity": 0.5753939151763916
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/entry/common.c",
          "start_line": 141,
          "end_line": 250,
          "content": [
            "static inline bool report_single_step(unsigned long work)",
            "{",
            "\tif (work & SYSCALL_WORK_SYSCALL_EMU)",
            "\t\treturn false;",
            "",
            "\treturn work & SYSCALL_WORK_SYSCALL_EXIT_TRAP;",
            "}",
            "void syscall_exit_work(struct pt_regs *regs, unsigned long work)",
            "{",
            "\tbool step;",
            "",
            "\t/*",
            "\t * If the syscall was rolled back due to syscall user dispatching,",
            "\t * then the tracers below are not invoked for the same reason as",
            "\t * the entry side was not invoked in syscall_trace_enter(): The ABI",
            "\t * of these syscalls is unknown.",
            "\t */",
            "\tif (work & SYSCALL_WORK_SYSCALL_USER_DISPATCH) {",
            "\t\tif (unlikely(current->syscall_dispatch.on_dispatch)) {",
            "\t\t\tcurrent->syscall_dispatch.on_dispatch = false;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "",
            "\taudit_syscall_exit(regs);",
            "",
            "\tif (work & SYSCALL_WORK_SYSCALL_TRACEPOINT)",
            "\t\ttrace_sys_exit(regs, syscall_get_return_value(current, regs));",
            "",
            "\tstep = report_single_step(work);",
            "\tif (step || work & SYSCALL_WORK_SYSCALL_TRACE)",
            "\t\tptrace_report_syscall_exit(regs, step);",
            "}",
            "noinstr void irqentry_enter_from_user_mode(struct pt_regs *regs)",
            "{",
            "\tenter_from_user_mode(regs);",
            "}",
            "noinstr void irqentry_exit_to_user_mode(struct pt_regs *regs)",
            "{",
            "\tinstrumentation_begin();",
            "\texit_to_user_mode_prepare(regs);",
            "\tinstrumentation_end();",
            "\texit_to_user_mode();",
            "}",
            "noinstr irqentry_state_t irqentry_enter(struct pt_regs *regs)",
            "{",
            "\tirqentry_state_t ret = {",
            "\t\t.exit_rcu = false,",
            "\t};",
            "",
            "\tif (user_mode(regs)) {",
            "\t\tirqentry_enter_from_user_mode(regs);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\t/*",
            "\t * If this entry hit the idle task invoke ct_irq_enter() whether",
            "\t * RCU is watching or not.",
            "\t *",
            "\t * Interrupts can nest when the first interrupt invokes softirq",
            "\t * processing on return which enables interrupts.",
            "\t *",
            "\t * Scheduler ticks in the idle task can mark quiescent state and",
            "\t * terminate a grace period, if and only if the timer interrupt is",
            "\t * not nested into another interrupt.",
            "\t *",
            "\t * Checking for rcu_is_watching() here would prevent the nesting",
            "\t * interrupt to invoke ct_irq_enter(). If that nested interrupt is",
            "\t * the tick then rcu_flavor_sched_clock_irq() would wrongfully",
            "\t * assume that it is the first interrupt and eventually claim",
            "\t * quiescent state and end grace periods prematurely.",
            "\t *",
            "\t * Unconditionally invoke ct_irq_enter() so RCU state stays",
            "\t * consistent.",
            "\t *",
            "\t * TINY_RCU does not support EQS, so let the compiler eliminate",
            "\t * this part when enabled.",
            "\t */",
            "\tif (!IS_ENABLED(CONFIG_TINY_RCU) && is_idle_task(current)) {",
            "\t\t/*",
            "\t\t * If RCU is not watching then the same careful",
            "\t\t * sequence vs. lockdep and tracing is required",
            "\t\t * as in irqentry_enter_from_user_mode().",
            "\t\t */",
            "\t\tlockdep_hardirqs_off(CALLER_ADDR0);",
            "\t\tct_irq_enter();",
            "\t\tinstrumentation_begin();",
            "\t\tkmsan_unpoison_entry_regs(regs);",
            "\t\ttrace_hardirqs_off_finish();",
            "\t\tinstrumentation_end();",
            "",
            "\t\tret.exit_rcu = true;",
            "\t\treturn ret;",
            "\t}",
            "",
            "\t/*",
            "\t * If RCU is watching then RCU only wants to check whether it needs",
            "\t * to restart the tick in NOHZ mode. rcu_irq_enter_check_tick()",
            "\t * already contains a warning when RCU is not watching, so no point",
            "\t * in having another one here.",
            "\t */",
            "\tlockdep_hardirqs_off(CALLER_ADDR0);",
            "\tinstrumentation_begin();",
            "\tkmsan_unpoison_entry_regs(regs);",
            "\trcu_irq_enter_check_tick();",
            "\ttrace_hardirqs_off_finish();",
            "\tinstrumentation_end();",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "report_single_step, syscall_exit_work, irqentry_enter_from_user_mode, irqentry_exit_to_user_mode, irqentry_enter",
          "description": "处理系统调用退出阶段工作，包括审计退出记录、单步调试报告和跟踪点事件，同时管理中断返回时的RCU状态转换和锁依赖检查",
          "similarity": 0.5591833591461182
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/entry/common.c",
          "start_line": 1,
          "end_line": 17,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "",
            "#include <linux/context_tracking.h>",
            "#include <linux/entry-common.h>",
            "#include <linux/resume_user_mode.h>",
            "#include <linux/highmem.h>",
            "#include <linux/jump_label.h>",
            "#include <linux/kmsan.h>",
            "#include <linux/livepatch.h>",
            "#include <linux/audit.h>",
            "#include <linux/tick.h>",
            "",
            "#include \"common.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/syscalls.h>",
            ""
          ],
          "function_name": null,
          "description": "声明系统调用通用功能所需头文件，包含审计、跟踪、KMSAN、LivePatch等模块的接口定义，为后续系统调用入口处理提供基础依赖",
          "similarity": 0.5495390892028809
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/entry/common.c",
          "start_line": 256,
          "end_line": 347,
          "content": [
            "void raw_irqentry_exit_cond_resched(void)",
            "{",
            "\tif (!preempt_count()) {",
            "\t\t/* Sanity check RCU and thread stack */",
            "\t\trcu_irq_exit_check_preempt();",
            "\t\tif (IS_ENABLED(CONFIG_DEBUG_ENTRY))",
            "\t\t\tWARN_ON_ONCE(!on_thread_stack());",
            "\t\tif (need_resched())",
            "\t\t\tpreempt_schedule_irq();",
            "\t}",
            "}",
            "void dynamic_irqentry_exit_cond_resched(void)",
            "{",
            "\tif (!static_branch_unlikely(&sk_dynamic_irqentry_exit_cond_resched))",
            "\t\treturn;",
            "\traw_irqentry_exit_cond_resched();",
            "}",
            "noinstr void irqentry_exit(struct pt_regs *regs, irqentry_state_t state)",
            "{",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\t/* Check whether this returns to user mode */",
            "\tif (user_mode(regs)) {",
            "\t\tirqentry_exit_to_user_mode(regs);",
            "\t} else if (!regs_irqs_disabled(regs)) {",
            "\t\t/*",
            "\t\t * If RCU was not watching on entry this needs to be done",
            "\t\t * carefully and needs the same ordering of lockdep/tracing",
            "\t\t * and RCU as the return to user mode path.",
            "\t\t */",
            "\t\tif (state.exit_rcu) {",
            "\t\t\tinstrumentation_begin();",
            "\t\t\t/* Tell the tracer that IRET will enable interrupts */",
            "\t\t\ttrace_hardirqs_on_prepare();",
            "\t\t\tlockdep_hardirqs_on_prepare();",
            "\t\t\tinstrumentation_end();",
            "\t\t\tct_irq_exit();",
            "\t\t\tlockdep_hardirqs_on(CALLER_ADDR0);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tinstrumentation_begin();",
            "\t\tif (IS_ENABLED(CONFIG_PREEMPTION))",
            "\t\t\tirqentry_exit_cond_resched();",
            "",
            "\t\t/* Covers both tracing and lockdep */",
            "\t\ttrace_hardirqs_on();",
            "\t\tinstrumentation_end();",
            "\t} else {",
            "\t\t/*",
            "\t\t * IRQ flags state is correct already. Just tell RCU if it",
            "\t\t * was not watching on entry.",
            "\t\t */",
            "\t\tif (state.exit_rcu)",
            "\t\t\tct_irq_exit();",
            "\t}",
            "}",
            "irqentry_state_t noinstr irqentry_nmi_enter(struct pt_regs *regs)",
            "{",
            "\tirqentry_state_t irq_state;",
            "",
            "\tirq_state.lockdep = lockdep_hardirqs_enabled();",
            "",
            "\t__nmi_enter();",
            "\tlockdep_hardirqs_off(CALLER_ADDR0);",
            "\tlockdep_hardirq_enter();",
            "\tct_nmi_enter();",
            "",
            "\tinstrumentation_begin();",
            "\tkmsan_unpoison_entry_regs(regs);",
            "\ttrace_hardirqs_off_finish();",
            "\tftrace_nmi_enter();",
            "\tinstrumentation_end();",
            "",
            "\treturn irq_state;",
            "}",
            "void noinstr irqentry_nmi_exit(struct pt_regs *regs, irqentry_state_t irq_state)",
            "{",
            "\tinstrumentation_begin();",
            "\tftrace_nmi_exit();",
            "\tif (irq_state.lockdep) {",
            "\t\ttrace_hardirqs_on_prepare();",
            "\t\tlockdep_hardirqs_on_prepare();",
            "\t}",
            "\tinstrumentation_end();",
            "",
            "\tct_nmi_exit();",
            "\tlockdep_hardirq_exit();",
            "\tif (irq_state.lockdep)",
            "\t\tlockdep_hardirqs_on(CALLER_ADDR0);",
            "\t__nmi_exit();",
            "}"
          ],
          "function_name": "raw_irqentry_exit_cond_resched, dynamic_irqentry_exit_cond_resched, irqentry_exit, irqentry_nmi_enter, irqentry_nmi_exit",
          "description": "实现中断退出路径的条件调度检查和NMI上下文切换，维护中断标志状态一致性，处理RCU状态转换、锁依赖标记恢复及异常中断返回时的上下文还原",
          "similarity": 0.5070559978485107
        }
      ]
    }
  ]
}