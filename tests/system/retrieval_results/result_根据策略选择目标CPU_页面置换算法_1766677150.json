{
  "query": "根据策略选择目标CPU 页面置换算法",
  "timestamp": "2025-12-25 23:39:10",
  "retrieved_files": [
    {
      "source_file": "mm/shuffle.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:21:18\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `shuffle.c`\n\n---\n\n# shuffle.c 技术文档\n\n## 1. 文件概述\n\n`shuffle.c` 实现了 Linux 内核内存管理子系统中的**页面分配随机化（Page Allocation Shuffling）**功能。该机制通过在内存初始化阶段对空闲页面链表进行 Fisher-Yates 洗牌操作，降低物理页帧分配的可预测性，从而增强系统安全性，抵御基于内存布局预测的攻击（如堆喷射、地址泄露等）。该功能默认关闭，可通过内核启动参数 `shuffle=1` 启用。\n\n## 2. 核心功能\n\n### 数据结构与全局变量\n- `page_alloc_shuffle_key`：静态分支键（static key），用于运行时启用/禁用洗牌逻辑，减少未启用时的性能开销。\n- `shuffle_param`：模块参数布尔值，控制是否启用洗牌功能。\n- `shuffle_param_ops`：自定义模块参数操作集，用于处理 `shuffle` 参数的设置和读取。\n\n### 主要函数\n- `shuffle_param_set()`：解析并设置 `shuffle` 内核参数，若启用则激活 `page_alloc_shuffle_key`。\n- `shuffle_valid_page()`：验证指定 PFN 的页面是否满足洗牌条件（属于 buddy 系统、同 zone、空闲、相同 order 和 migratetype）。\n- `__shuffle_zone()`：对指定内存区域（zone）执行 Fisher-Yates 洗牌算法，随机交换同阶空闲页面。\n- `__shuffle_free_memory()`：遍历节点（pgdat）中所有 zone，依次调用 `shuffle_zone()` 进行洗牌。\n- `shuffle_pick_tail()`：提供轻量级随机位生成器，用于在分配时决定从链表头部还是尾部取页（增强运行时随机性）。\n\n## 3. 关键实现\n\n### 洗牌算法（Fisher-Yates）\n- **粒度**：以 `SHUFFLE_ORDER`（通常为 0，即单页）为单位进行洗牌。\n- **范围**：遍历 zone 内所有按 order 对齐的 PFN，对每个有效页面 `page_i` 随机选择另一个有效页面 `page_j` 进行交换。\n- **有效性校验**：通过 `shuffle_valid_page()` 确保交换双方均为 buddy 系统管理的空闲页，且具有相同的迁移类型（migratetype）。\n- **重试机制**：最多尝试 `SHUFFLE_RETRY`（10 次）寻找有效的随机目标页，避免因内存空洞导致失败。\n- **锁优化**：每处理 100 个页面后释放 zone 自旋锁并调度，防止长时间持锁影响系统响应。\n\n### 随机性来源\n- 使用 `get_random_long()` 获取高质量伪随机数作为洗牌索引。\n- `shuffle_pick_tail()` 使用无锁的 64 位随机状态生成器，每次返回最低位并右移，用于运行时分配策略的微调。\n\n### 安全性权衡\n- 明确承认不消除模运算偏差（modulo bias）或 PRNG 偏差，目标是“提高攻击门槛”而非完美随机。\n- 仅在内存初始化阶段（`__meminit`）执行一次洗牌，不影响运行时分配性能。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：内存管理核心数据结构（`struct zone`, `struct page`）。\n  - `<linux/random.h>`：提供 `get_random_long()` 和 `get_random_u64()`。\n  - `\"internal.h\"`、`\"shuffle.h\"`：内核 MM 子系统内部接口及洗牌功能声明。\n- **功能依赖**：\n  - Buddy 分配器：依赖 `PageBuddy()`、`buddy_order()` 等接口判断页面状态。\n  - 页面迁移类型（Migratetype）：确保洗牌不破坏不同迁移类型页面的隔离。\n  - 静态分支（Static Keys）：通过 `static_branch_enable()` 动态启用洗牌路径。\n\n## 5. 使用场景\n\n- **安全加固**：在需要防范物理地址预测攻击的场景（如虚拟化宿主机、安全敏感设备）中启用，增加攻击者利用内存布局漏洞的难度。\n- **内核初始化**：在 `free_area_init_core()` 等内存子系统初始化流程中调用 `__shuffle_free_memory()`，对初始空闲内存进行一次性洗牌。\n- **运行时分配辅助**：`shuffle_pick_tail()` 被页面分配器调用，决定从空闲链表头/尾取页，进一步增加分配时序的不可预测性。\n- **调试支持**：通过 `pr_debug()` 输出洗牌失败或迁移类型不匹配的日志，便于问题诊断（需开启 `DEBUG_SHUFFLE`）。",
      "similarity": 0.5869200229644775,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/shuffle.c",
          "start_line": 16,
          "end_line": 121,
          "content": [
            "static __meminit int shuffle_param_set(const char *val,",
            "\t\tconst struct kernel_param *kp)",
            "{",
            "\tif (param_set_bool(val, kp))",
            "\t\treturn -EINVAL;",
            "\tif (*(bool *)kp->arg)",
            "\t\tstatic_branch_enable(&page_alloc_shuffle_key);",
            "\treturn 0;",
            "}",
            "void __meminit __shuffle_zone(struct zone *z)",
            "{",
            "\tunsigned long i, flags;",
            "\tunsigned long start_pfn = z->zone_start_pfn;",
            "\tunsigned long end_pfn = zone_end_pfn(z);",
            "\tconst int order = SHUFFLE_ORDER;",
            "\tconst int order_pages = 1 << order;",
            "",
            "\tspin_lock_irqsave(&z->lock, flags);",
            "\tstart_pfn = ALIGN(start_pfn, order_pages);",
            "\tfor (i = start_pfn; i < end_pfn; i += order_pages) {",
            "\t\tunsigned long j;",
            "\t\tint migratetype, retry;",
            "\t\tstruct page *page_i, *page_j;",
            "",
            "\t\t/*",
            "\t\t * We expect page_i, in the sub-range of a zone being added",
            "\t\t * (@start_pfn to @end_pfn), to more likely be valid compared to",
            "\t\t * page_j randomly selected in the span @zone_start_pfn to",
            "\t\t * @spanned_pages.",
            "\t\t */",
            "\t\tpage_i = shuffle_valid_page(z, i, order);",
            "\t\tif (!page_i)",
            "\t\t\tcontinue;",
            "",
            "\t\tfor (retry = 0; retry < SHUFFLE_RETRY; retry++) {",
            "\t\t\t/*",
            "\t\t\t * Pick a random order aligned page in the zone span as",
            "\t\t\t * a swap target. If the selected pfn is a hole, retry",
            "\t\t\t * up to SHUFFLE_RETRY attempts find a random valid pfn",
            "\t\t\t * in the zone.",
            "\t\t\t */",
            "\t\t\tj = z->zone_start_pfn +",
            "\t\t\t\tALIGN_DOWN(get_random_long() % z->spanned_pages,",
            "\t\t\t\t\t\torder_pages);",
            "\t\t\tpage_j = shuffle_valid_page(z, j, order);",
            "\t\t\tif (page_j && page_j != page_i)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t\tif (retry >= SHUFFLE_RETRY) {",
            "\t\t\tpr_debug(\"%s: failed to swap %#lx\\n\", __func__, i);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Each migratetype corresponds to its own list, make sure the",
            "\t\t * types match otherwise we're moving pages to lists where they",
            "\t\t * do not belong.",
            "\t\t */",
            "\t\tmigratetype = get_pageblock_migratetype(page_i);",
            "\t\tif (get_pageblock_migratetype(page_j) != migratetype) {",
            "\t\t\tpr_debug(\"%s: migratetype mismatch %#lx\\n\", __func__, i);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tlist_swap(&page_i->lru, &page_j->lru);",
            "",
            "\t\tpr_debug(\"%s: swap: %#lx -> %#lx\\n\", __func__, i, j);",
            "",
            "\t\t/* take it easy on the zone lock */",
            "\t\tif ((i % (100 * order_pages)) == 0) {",
            "\t\t\tspin_unlock_irqrestore(&z->lock, flags);",
            "\t\t\tcond_resched();",
            "\t\t\tspin_lock_irqsave(&z->lock, flags);",
            "\t\t}",
            "\t}",
            "\tspin_unlock_irqrestore(&z->lock, flags);",
            "}",
            "void __meminit __shuffle_free_memory(pg_data_t *pgdat)",
            "{",
            "\tstruct zone *z;",
            "",
            "\tfor (z = pgdat->node_zones; z < pgdat->node_zones + MAX_NR_ZONES; z++)",
            "\t\tshuffle_zone(z);",
            "}",
            "bool shuffle_pick_tail(void)",
            "{",
            "\tstatic u64 rand;",
            "\tstatic u8 rand_bits;",
            "\tbool ret;",
            "",
            "\t/*",
            "\t * The lack of locking is deliberate. If 2 threads race to",
            "\t * update the rand state it just adds to the entropy.",
            "\t */",
            "\tif (rand_bits == 0) {",
            "\t\trand_bits = 64;",
            "\t\trand = get_random_u64();",
            "\t}",
            "",
            "\tret = rand & 1;",
            "",
            "\trand_bits--;",
            "\trand >>= 1;",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "shuffle_param_set, __shuffle_zone, __shuffle_free_memory, shuffle_pick_tail",
          "description": "shuffle_param_set设置参数并启用/禁用静态键；__shuffle_zone在内存区随机交换页面以打乱物理顺序；__shuffle_free_memory初始化时调用__shuffle_zone；shuffle_pick_tail生成随机布尔值用于选择尾部页",
          "similarity": 0.5838571786880493
        },
        {
          "chunk_id": 0,
          "file_path": "mm/shuffle.c",
          "start_line": 1,
          "end_line": 15,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "// Copyright(c) 2018 Intel Corporation. All rights reserved.",
            "",
            "#include <linux/mm.h>",
            "#include <linux/init.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/random.h>",
            "#include <linux/moduleparam.h>",
            "#include \"internal.h\"",
            "#include \"shuffle.h\"",
            "",
            "DEFINE_STATIC_KEY_FALSE(page_alloc_shuffle_key);",
            "",
            "static bool shuffle_param;",
            ""
          ],
          "function_name": null,
          "description": "定义静态键用于控制页面分配随机化功能，并声明参数变量shuffle_param，用于启用或禁用相关机制",
          "similarity": 0.4753475785255432
        }
      ]
    },
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.5778230428695679,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/mempolicy.c",
          "start_line": 168,
          "end_line": 268,
          "content": [
            "static u8 get_il_weight(int node)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tu8 weight = 1;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state)",
            "\t\tweight = state->iw_table[node];",
            "\trcu_read_unlock();",
            "\treturn weight;",
            "}",
            "static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)",
            "{",
            "\tu64 sum_bw = 0;",
            "\tunsigned int cast_sum_bw, scaling_factor = 1, iw_gcd = 0;",
            "\tint nid;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tsum_bw += bw[nid];",
            "",
            "\t/* Scale bandwidths to whole numbers in the range [1, weightiness] */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t/*",
            "\t\t * Try not to perform 64-bit division.",
            "\t\t * If sum_bw < scaling_factor, then sum_bw < U32_MAX.",
            "\t\t * If sum_bw > scaling_factor, then round the weight up to 1.",
            "\t\t */",
            "\t\tscaling_factor = weightiness * bw[nid];",
            "\t\tif (bw[nid] && sum_bw < scaling_factor) {",
            "\t\t\tcast_sum_bw = (unsigned int)sum_bw;",
            "\t\t\tnew_iw[nid] = scaling_factor / cast_sum_bw;",
            "\t\t} else {",
            "\t\t\tnew_iw[nid] = 1;",
            "\t\t}",
            "\t\tif (!iw_gcd)",
            "\t\t\tiw_gcd = new_iw[nid];",
            "\t\tiw_gcd = gcd(iw_gcd, new_iw[nid]);",
            "\t}",
            "",
            "\t/* 1:2 is strictly better than 16:32. Reduce by the weights' GCD. */",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tnew_iw[nid] /= iw_gcd;",
            "}",
            "int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tunsigned int *old_bw, *new_bw;",
            "\tunsigned int bw_val;",
            "\tint i;",
            "",
            "\tbw_val = min(coords->read_bandwidth, coords->write_bandwidth);",
            "\tnew_bw = kcalloc(nr_node_ids, sizeof(unsigned int), GFP_KERNEL);",
            "\tif (!new_bw)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew_wi_state = kmalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state) {",
            "\t\tkfree(new_bw);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tnew_wi_state->mode_auto = true;",
            "\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\tnew_wi_state->iw_table[i] = 1;",
            "",
            "\t/*",
            "\t * Update bandwidth info, even in manual mode. That way, when switching",
            "\t * to auto mode in the future, iw_table can be overwritten using",
            "\t * accurate bw data.",
            "\t */",
            "\tmutex_lock(&wi_state_lock);",
            "",
            "\told_bw = node_bw_table;",
            "\tif (old_bw)",
            "\t\tmemcpy(new_bw, old_bw, nr_node_ids * sizeof(*old_bw));",
            "\tnew_bw[node] = bw_val;",
            "\tnode_bw_table = new_bw;",
            "",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state && !old_wi_state->mode_auto) {",
            "\t\t/* Manual mode; skip reducing weights and updating wi_state */",
            "\t\tmutex_unlock(&wi_state_lock);",
            "\t\tkfree(new_wi_state);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* NULL wi_state assumes auto=true; reduce weights and update wi_state*/",
            "\treduce_interleave_weights(new_bw, new_wi_state->iw_table);",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "out:",
            "\tkfree(old_bw);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_il_weight, reduce_interleave_weights, mempolicy_set_node_perf",
          "description": "实现带权交错策略的权重计算与调整逻辑，通过获取节点带宽数据动态修改权重比例，支持根据性能参数更新节点间内存分配优先级。",
          "similarity": 0.6533993482589722
        },
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.6472684144973755
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.640689492225647
        },
        {
          "chunk_id": 16,
          "file_path": "mm/mempolicy.c",
          "start_line": 2846,
          "end_line": 2946,
          "content": [
            "static void sp_free(struct sp_node *n)",
            "{",
            "\tmpol_put(n->policy);",
            "\tkmem_cache_free(sn_cache, n);",
            "}",
            "int mpol_misplaced(struct folio *folio, struct vm_fault *vmf,",
            "\t\t   unsigned long addr)",
            "{",
            "\tstruct mempolicy *pol;",
            "\tpgoff_t ilx;",
            "\tstruct zoneref *z;",
            "\tint curnid = folio_nid(folio);",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tint thiscpu = raw_smp_processor_id();",
            "\tint thisnid = numa_node_id();",
            "\tint polnid = NUMA_NO_NODE;",
            "\tint ret = NUMA_NO_NODE;",
            "",
            "\t/*",
            "\t * Make sure ptl is held so that we don't preempt and we",
            "\t * have a stable smp processor id",
            "\t */",
            "\tlockdep_assert_held(vmf->ptl);",
            "\tpol = get_vma_policy(vma, addr, folio_order(folio), &ilx);",
            "\tif (!(pol->flags & MPOL_F_MOF))",
            "\t\tgoto out;",
            "",
            "\tswitch (pol->mode) {",
            "\tcase MPOL_INTERLEAVE:",
            "\t\tpolnid = interleave_nid(pol, ilx);",
            "\t\tbreak;",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\tpolnid = weighted_interleave_nid(pol, ilx);",
            "\t\tbreak;",
            "",
            "\tcase MPOL_PREFERRED:",
            "\t\tif (node_isset(curnid, pol->nodes))",
            "\t\t\tgoto out;",
            "\t\tpolnid = first_node(pol->nodes);",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tpolnid = numa_node_id();",
            "\t\tbreak;",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t\t/*",
            "\t\t * Even though MPOL_PREFERRED_MANY can allocate pages outside",
            "\t\t * policy nodemask we don't allow numa migration to nodes",
            "\t\t * outside policy nodemask for now. This is done so that if we",
            "\t\t * want demotion to slow memory to happen, before allocating",
            "\t\t * from some DRAM node say 'x', we will end up using a",
            "\t\t * MPOL_PREFERRED_MANY mask excluding node 'x'. In such scenario",
            "\t\t * we should not promote to node 'x' from slow memory node.",
            "\t\t */",
            "\t\tif (pol->flags & MPOL_F_MORON) {",
            "\t\t\t/*",
            "\t\t\t * Optimize placement among multiple nodes",
            "\t\t\t * via NUMA balancing",
            "\t\t\t */",
            "\t\t\tif (node_isset(thisnid, pol->nodes))",
            "\t\t\t\tbreak;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * use current page if in policy nodemask,",
            "\t\t * else select nearest allowed node, if any.",
            "\t\t * If no allowed nodes, use current [!misplaced].",
            "\t\t */",
            "\t\tif (node_isset(curnid, pol->nodes))",
            "\t\t\tgoto out;",
            "\t\tz = first_zones_zonelist(",
            "\t\t\t\tnode_zonelist(thisnid, GFP_HIGHUSER),",
            "\t\t\t\tgfp_zone(GFP_HIGHUSER),",
            "\t\t\t\t&pol->nodes);",
            "\t\tpolnid = zone_to_nid(z->zone);",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "",
            "\t/* Migrate the folio towards the node whose CPU is referencing it */",
            "\tif (pol->flags & MPOL_F_MORON) {",
            "\t\tpolnid = thisnid;",
            "",
            "\t\tif (!should_numa_migrate_memory(current, folio, curnid,",
            "\t\t\t\t\t\tthiscpu))",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tif (curnid != polnid)",
            "\t\tret = polnid;",
            "out:",
            "\tmpol_cond_put(pol);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "sp_free, mpol_misplaced",
          "description": "sp_free释放共享策略节点资源，mpol_misplaced判断页面是否符合内存策略节点要求，若不符合则计算应迁移的目标节点，支持多种策略模式下的节点选择逻辑。",
          "similarity": 0.6312258839607239
        },
        {
          "chunk_id": 3,
          "file_path": "mm/mempolicy.c",
          "start_line": 484,
          "end_line": 639,
          "content": [
            "static void mpol_rebind_preferred(struct mempolicy *pol,",
            "\t\t\t\t\t\tconst nodemask_t *nodes)",
            "{",
            "\tpol->w.cpuset_mems_allowed = *nodes;",
            "}",
            "static void mpol_rebind_policy(struct mempolicy *pol, const nodemask_t *newmask)",
            "{",
            "\tif (!pol || pol->mode == MPOL_LOCAL)",
            "\t\treturn;",
            "\tif (!mpol_store_user_nodemask(pol) &&",
            "\t    nodes_equal(pol->w.cpuset_mems_allowed, *newmask))",
            "\t\treturn;",
            "",
            "\tmpol_ops[pol->mode].rebind(pol, newmask);",
            "}",
            "void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new)",
            "{",
            "\tmpol_rebind_policy(tsk->mempolicy, new);",
            "}",
            "void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "",
            "\tmmap_write_lock(mm);",
            "\tfor_each_vma(vmi, vma) {",
            "\t\tvma_start_write(vma);",
            "\t\tmpol_rebind_policy(vma->vm_policy, new);",
            "\t}",
            "\tmmap_write_unlock(mm);",
            "}",
            "static bool strictly_unmovable(unsigned long flags)",
            "{",
            "\t/*",
            "\t * STRICT without MOVE flags lets do_mbind() fail immediately with -EIO",
            "\t * if any misplaced page is found.",
            "\t */",
            "\treturn (flags & (MPOL_MF_STRICT | MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ==",
            "\t\t\t MPOL_MF_STRICT;",
            "}",
            "static inline bool queue_folio_required(struct folio *folio,",
            "\t\t\t\t\tstruct queue_pages *qp)",
            "{",
            "\tint nid = folio_nid(folio);",
            "\tunsigned long flags = qp->flags;",
            "",
            "\treturn node_isset(nid, *qp->nmask) == !(flags & MPOL_MF_INVERT);",
            "}",
            "static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)",
            "{",
            "\tstruct folio *folio;",
            "\tstruct queue_pages *qp = walk->private;",
            "",
            "\tif (unlikely(is_pmd_migration_entry(*pmd))) {",
            "\t\tqp->nr_failed++;",
            "\t\treturn;",
            "\t}",
            "\tfolio = pmd_folio(*pmd);",
            "\tif (is_huge_zero_folio(folio)) {",
            "\t\twalk->action = ACTION_CONTINUE;",
            "\t\treturn;",
            "\t}",
            "\tif (!queue_folio_required(folio, qp))",
            "\t\treturn;",
            "\tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||",
            "\t    !vma_migratable(walk->vma) ||",
            "\t    !migrate_folio_add(folio, qp->pagelist, qp->flags))",
            "\t\tqp->nr_failed++;",
            "}",
            "static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,",
            "\t\t\tunsigned long end, struct mm_walk *walk)",
            "{",
            "\tconst fpb_t fpb_flags = FPB_IGNORE_DIRTY | FPB_IGNORE_SOFT_DIRTY;",
            "\tstruct vm_area_struct *vma = walk->vma;",
            "\tstruct folio *folio;",
            "\tstruct queue_pages *qp = walk->private;",
            "\tunsigned long flags = qp->flags;",
            "\tpte_t *pte, *mapped_pte;",
            "\tpte_t ptent;",
            "\tspinlock_t *ptl;",
            "\tint max_nr, nr;",
            "",
            "\tptl = pmd_trans_huge_lock(pmd, vma);",
            "\tif (ptl) {",
            "\t\tqueue_folios_pmd(pmd, walk);",
            "\t\tspin_unlock(ptl);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tmapped_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);",
            "\tif (!pte) {",
            "\t\twalk->action = ACTION_AGAIN;",
            "\t\treturn 0;",
            "\t}",
            "\tfor (; addr != end; pte += nr, addr += nr * PAGE_SIZE) {",
            "\t\tmax_nr = (end - addr) >> PAGE_SHIFT;",
            "\t\tnr = 1;",
            "\t\tptent = ptep_get(pte);",
            "\t\tif (pte_none(ptent))",
            "\t\t\tcontinue;",
            "\t\tif (!pte_present(ptent)) {",
            "\t\t\tif (is_migration_entry(pte_to_swp_entry(ptent)))",
            "\t\t\t\tqp->nr_failed++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfolio = vm_normal_folio(vma, addr, ptent);",
            "\t\tif (!folio || folio_is_zone_device(folio))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_large(folio) && max_nr != 1)",
            "\t\t\tnr = folio_pte_batch(folio, addr, pte, ptent,",
            "\t\t\t\t\t     max_nr, fpb_flags,",
            "\t\t\t\t\t     NULL, NULL, NULL);",
            "\t\t/*",
            "\t\t * vm_normal_folio() filters out zero pages, but there might",
            "\t\t * still be reserved folios to skip, perhaps in a VDSO.",
            "\t\t */",
            "\t\tif (folio_test_reserved(folio))",
            "\t\t\tcontinue;",
            "\t\tif (!queue_folio_required(folio, qp))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_large(folio)) {",
            "\t\t\t/*",
            "\t\t\t * A large folio can only be isolated from LRU once,",
            "\t\t\t * but may be mapped by many PTEs (and Copy-On-Write may",
            "\t\t\t * intersperse PTEs of other, order 0, folios).  This is",
            "\t\t\t * a common case, so don't mistake it for failure (but",
            "\t\t\t * there can be other cases of multi-mapped pages which",
            "\t\t\t * this quick check does not help to filter out - and a",
            "\t\t\t * search of the pagelist might grow to be prohibitive).",
            "\t\t\t *",
            "\t\t\t * migrate_pages(&pagelist) returns nr_failed folios, so",
            "\t\t\t * check \"large\" now so that queue_pages_range() returns",
            "\t\t\t * a comparable nr_failed folios.  This does imply that",
            "\t\t\t * if folio could not be isolated for some racy reason",
            "\t\t\t * at its first PTE, later PTEs will not give it another",
            "\t\t\t * chance of isolation; but keeps the accounting simple.",
            "\t\t\t */",
            "\t\t\tif (folio == qp->large)",
            "\t\t\t\tcontinue;",
            "\t\t\tqp->large = folio;",
            "\t\t}",
            "\t\tif (!(flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||",
            "\t\t    !vma_migratable(vma) ||",
            "\t\t    !migrate_folio_add(folio, qp->pagelist, flags)) {",
            "\t\t\tqp->nr_failed += nr;",
            "\t\t\tif (strictly_unmovable(flags))",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tpte_unmap_unlock(mapped_pte, ptl);",
            "\tcond_resched();",
            "out:",
            "\tif (qp->nr_failed && strictly_unmovable(flags))",
            "\t\treturn -EIO;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "mpol_rebind_preferred, mpol_rebind_policy, mpol_rebind_task, mpol_rebind_mm, strictly_unmovable, queue_folio_required, queue_folios_pmd, queue_folios_pte_range",
          "description": "处理页表项遍历与迁移操作，包含页帧队列检测、迁移标志验证及大页迁移逻辑，确保内存分配符合NUMA策略要求。",
          "similarity": 0.6289728879928589
        }
      ]
    },
    {
      "source_file": "kernel/sched/cpupri.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:04:45\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\cpupri.c`\n\n---\n\n# `sched/cpupri.c` 技术文档\n\n## 1. 文件概述\n\n`sched/cpupri.c` 实现了 **CPU 优先级管理（CPU Priority Management）** 机制，用于实时任务（RT tasks）的全局负载均衡和迁移决策。该机制通过维护一个二维位图结构，快速追踪每个 CPU 当前运行任务的最高优先级，从而在 O(1) 时间复杂度内为新唤醒或迁移的实时任务找到合适的 CPU 目标。该机制特别优化了无 CPU 亲和性限制的任务调度路径，同时支持带亲和性约束的场景。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `convert_prio(int prio)` | 将任务的调度优先级（`p->prio`）转换为内部 `cpupri` 优先级值（范围：-1 到 100） |\n| `__cpupri_find(struct cpupri *cp, struct task_struct *p, struct cpumask *lowest_mask, int idx)` | 在指定优先级层级 `idx` 中查找满足任务 `p` 的 CPU（考虑亲和性） |\n| `cpupri_find(struct cpupri *cp, struct task_struct *p, struct cpumask *lowest_mask)` | 查找系统中优先级 **低于或等于** 任务 `p` 的 CPU（即任务可运行的 CPU） |\n| `cpupri_find_fitness(...)` | 增强版查找函数，支持通过 `fitness_fn` 自定义 CPU 适配条件（如容量感知） |\n| `cpupri_set(struct cpupri *cp, int cpu, int newpri)` | 更新指定 CPU 的当前最高优先级状态 |\n| `cpupri_init(struct cpupri *cp)` | 初始化 `cpupri` 数据结构（声明但未在片段中实现） |\n\n### 关键数据结构（隐含）\n\n- `struct cpupri`：全局 CPU 优先级管理上下文\n  - `cpu_to_pri[]`：每个 CPU 当前的 `cpupri` 优先级\n  - `pri_to_cpu[]`：每个优先级对应的 `struct cpupri_vec`\n- `struct cpupri_vec`：\n  - `mask`：该优先级下所有 CPU 的位图\n  - `count`：该优先级下活跃 CPU 的数量（原子计数）\n\n### 优先级映射关系\n\n| 任务 `p->prio` | `cpupri` 值 | 含义 |\n|---------------|------------|------|\n| -1 | -1 (`CPUPRI_INVALID`) | 无效状态（CPU 不可调度） |\n| 0–98 | 99–1 | 实时优先级（数值越大，任务优先级越高，`cpupri` 值越小） |\n| 99 (`MAX_RT_PRIO-1`) | 0 (`CPUPRI_NORMAL`) | 普通（非实时）任务 |\n| 100 (`MAX_RT_PRIO`) | 100 (`CPUPRI_HIGHER`) | 高于所有 RT 任务的特殊优先级 |\n\n> **注意**：`cpupri` 值越小，表示 CPU 当前负载的优先级 **越高**。\n\n## 3. 关键实现\n\n### 优先级转换逻辑\n- `convert_prio()` 实现了任务调度优先级到 `cpupri` 内部表示的映射，确保实时任务（`p->prio` ∈ [0, 98]）被正确映射到 `cpupri` ∈ [1, 99]，且高优先级任务对应更小的 `cpupri` 值。\n\n### 快速查找算法\n- 使用 **二维位图**：第一维为优先级（0–100），第二维为 CPU 位图。\n- `cpupri_find_fitness()` 从最低优先级（`idx = 0`）开始遍历，找到第一个存在可用 CPU 的优先级层级。\n- 对于每个层级，通过 `cpumask_any_and()` 快速判断任务亲和性掩码与该优先级 CPU 掩码是否有交集。\n- 若提供 `fitness_fn`（如容量检查），会过滤掉不满足条件的 CPU；若过滤后无 CPU 可用，则继续搜索更高优先级层级。\n\n### 容错与回退策略\n- 如果启用了 `fitness_fn` 但未找到满足条件的 CPU，函数会 **忽略 fitness 条件重新搜索**，确保高优先级任务总能找到运行 CPU（优先保证实时性，而非最优资源匹配）。\n\n### 并发安全更新\n- `cpupri_set()` 使用 **内存屏障（`smp_mb__before/after_atomic()`）** 确保 CPU 位图和计数器的更新顺序：\n  1. **添加 CPU**：先设置位图 → 内存屏障 → 增加计数器\n  2. **移除 CPU**：先减少计数器 → 内存屏障 → 清除位图\n- 此顺序防止 `cpupri_find` 在并发读取时看到不一致状态（如计数器为 0 但位图仍置位）。\n\n### 亲和性处理\n- 所有查找操作均与任务的 `p->cpus_mask`（CPU 亲和性）和 `cpu_active_mask`（活跃 CPU）进行交集运算，确保只返回合法 CPU。\n\n## 4. 依赖关系\n\n- **调度器核心**：依赖 `task_struct`、`p->prio`、`p->cpus_mask` 等调度器基本结构。\n- **实时调度类（`rt.c`）**：`cpupri` 主要服务于 `SCHED_FIFO`/`SCHED_RR` 任务的负载均衡。\n- **CPU 掩码操作**：使用 `cpumask_*` 系列函数（如 `cpumask_and`, `cpumask_clear_cpu`）。\n- **内存屏障原语**：依赖 `smp_rmb()`、`smp_mb__before_atomic()` 等 SMP 同步机制。\n- **原子操作**：使用 `atomic_read/inc/dec` 管理优先级层级的 CPU 计数。\n\n## 5. 使用场景\n\n- **实时任务唤醒/迁移**：当高优先级 RT 任务被唤醒或需要迁移时，调用 `cpupri_find()` 快速定位可运行的最低优先级 CPU（减少抢占开销）。\n- **全局负载均衡**：RT 调度器的 `push_rt_task()` 和 `pull_rt_task()` 机制利用 `cpupri` 决定任务推送/拉取的目标 CPU。\n- **容量感知调度（Capacity Awareness）**：通过 `cpupri_find_fitness()` 的 `fitness_fn` 参数，集成 CPU 性能/能效信息（如 ARM big.LITTLE 架构），在满足优先级前提下选择合适 CPU。\n- **CPU 热插拔**：CPU 上下线时通过 `cpupri_set()` 更新其优先级状态（设为 `CPUPRI_INVALID` 或恢复）。",
      "similarity": 0.5773047804832458,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 42,
          "end_line": 178,
          "content": [
            "static int convert_prio(int prio)",
            "{",
            "\tint cpupri;",
            "",
            "\tswitch (prio) {",
            "\tcase CPUPRI_INVALID:",
            "\t\tcpupri = CPUPRI_INVALID;\t/* -1 */",
            "\t\tbreak;",
            "",
            "\tcase 0 ... 98:",
            "\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */",
            "\t\tbreak;",
            "",
            "\tcase MAX_RT_PRIO-1:",
            "\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */",
            "\t\tbreak;",
            "",
            "\tcase MAX_RT_PRIO:",
            "\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn cpupri;",
            "}",
            "static inline int __cpupri_find(struct cpupri *cp, struct task_struct *p,",
            "\t\t\t\tstruct cpumask *lowest_mask, int idx)",
            "{",
            "\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[idx];",
            "\tint skip = 0;",
            "",
            "\tif (!atomic_read(&(vec)->count))",
            "\t\tskip = 1;",
            "\t/*",
            "\t * When looking at the vector, we need to read the counter,",
            "\t * do a memory barrier, then read the mask.",
            "\t *",
            "\t * Note: This is still all racy, but we can deal with it.",
            "\t *  Ideally, we only want to look at masks that are set.",
            "\t *",
            "\t *  If a mask is not set, then the only thing wrong is that we",
            "\t *  did a little more work than necessary.",
            "\t *",
            "\t *  If we read a zero count but the mask is set, because of the",
            "\t *  memory barriers, that can only happen when the highest prio",
            "\t *  task for a run queue has left the run queue, in which case,",
            "\t *  it will be followed by a pull. If the task we are processing",
            "\t *  fails to find a proper place to go, that pull request will",
            "\t *  pull this task if the run queue is running at a lower",
            "\t *  priority.",
            "\t */",
            "\tsmp_rmb();",
            "",
            "\t/* Need to do the rmb for every iteration */",
            "\tif (skip)",
            "\t\treturn 0;",
            "",
            "\tif (cpumask_any_and(&p->cpus_mask, vec->mask) >= nr_cpu_ids)",
            "\t\treturn 0;",
            "",
            "\tif (lowest_mask) {",
            "\t\tcpumask_and(lowest_mask, &p->cpus_mask, vec->mask);",
            "\t\tcpumask_and(lowest_mask, lowest_mask, cpu_active_mask);",
            "",
            "\t\t/*",
            "\t\t * We have to ensure that we have at least one bit",
            "\t\t * still set in the array, since the map could have",
            "\t\t * been concurrently emptied between the first and",
            "\t\t * second reads of vec->mask.  If we hit this",
            "\t\t * condition, simply act as though we never hit this",
            "\t\t * priority level and continue on.",
            "\t\t */",
            "\t\tif (cpumask_empty(lowest_mask))",
            "\t\t\treturn 0;",
            "\t}",
            "",
            "\treturn 1;",
            "}",
            "int cpupri_find(struct cpupri *cp, struct task_struct *p,",
            "\t\tstruct cpumask *lowest_mask)",
            "{",
            "\treturn cpupri_find_fitness(cp, p, lowest_mask, NULL);",
            "}",
            "int cpupri_find_fitness(struct cpupri *cp, struct task_struct *p,",
            "\t\tstruct cpumask *lowest_mask,",
            "\t\tbool (*fitness_fn)(struct task_struct *p, int cpu))",
            "{",
            "\tint task_pri = convert_prio(p->prio);",
            "\tint idx, cpu;",
            "",
            "\tWARN_ON_ONCE(task_pri >= CPUPRI_NR_PRIORITIES);",
            "",
            "\tfor (idx = 0; idx < task_pri; idx++) {",
            "",
            "\t\tif (!__cpupri_find(cp, p, lowest_mask, idx))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!lowest_mask || !fitness_fn)",
            "\t\t\treturn 1;",
            "",
            "\t\t/* Ensure the capacity of the CPUs fit the task */",
            "\t\tfor_each_cpu(cpu, lowest_mask) {",
            "\t\t\tif (!fitness_fn(p, cpu))",
            "\t\t\t\tcpumask_clear_cpu(cpu, lowest_mask);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If no CPU at the current priority can fit the task",
            "\t\t * continue looking",
            "\t\t */",
            "\t\tif (cpumask_empty(lowest_mask))",
            "\t\t\tcontinue;",
            "",
            "\t\treturn 1;",
            "\t}",
            "",
            "\t/*",
            "\t * If we failed to find a fitting lowest_mask, kick off a new search",
            "\t * but without taking into account any fitness criteria this time.",
            "\t *",
            "\t * This rule favours honouring priority over fitting the task in the",
            "\t * correct CPU (Capacity Awareness being the only user now).",
            "\t * The idea is that if a higher priority task can run, then it should",
            "\t * run even if this ends up being on unfitting CPU.",
            "\t *",
            "\t * The cost of this trade-off is not entirely clear and will probably",
            "\t * be good for some workloads and bad for others.",
            "\t *",
            "\t * The main idea here is that if some CPUs were over-committed, we try",
            "\t * to spread which is what the scheduler traditionally did. Sys admins",
            "\t * must do proper RT planning to avoid overloading the system if they",
            "\t * really care.",
            "\t */",
            "\tif (fitness_fn)",
            "\t\treturn cpupri_find(cp, p, lowest_mask);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "convert_prio, __cpupri_find, cpupri_find, cpupri_find_fitness",
          "description": "convert_prio将任务优先级映射为CPU优先级数值；__cpupri_find检查特定优先级下是否存在可用CPU；cpupri_find_fitness遍历优先级层级寻找适配CPU，结合适应性判断与优先级策略决定最终选择",
          "similarity": 0.642427384853363
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 210,
          "end_line": 304,
          "content": [
            "void cpupri_set(struct cpupri *cp, int cpu, int newpri)",
            "{",
            "\tint *currpri = &cp->cpu_to_pri[cpu];",
            "\tint oldpri = *currpri;",
            "\tint do_mb = 0;",
            "",
            "\tnewpri = convert_prio(newpri);",
            "",
            "\tBUG_ON(newpri >= CPUPRI_NR_PRIORITIES);",
            "",
            "\tif (newpri == oldpri)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If the CPU was currently mapped to a different value, we",
            "\t * need to map it to the new value then remove the old value.",
            "\t * Note, we must add the new value first, otherwise we risk the",
            "\t * cpu being missed by the priority loop in cpupri_find.",
            "\t */",
            "\tif (likely(newpri != CPUPRI_INVALID)) {",
            "\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[newpri];",
            "",
            "\t\tcpumask_set_cpu(cpu, vec->mask);",
            "\t\t/*",
            "\t\t * When adding a new vector, we update the mask first,",
            "\t\t * do a write memory barrier, and then update the count, to",
            "\t\t * make sure the vector is visible when count is set.",
            "\t\t */",
            "\t\tsmp_mb__before_atomic();",
            "\t\tatomic_inc(&(vec)->count);",
            "\t\tdo_mb = 1;",
            "\t}",
            "\tif (likely(oldpri != CPUPRI_INVALID)) {",
            "\t\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[oldpri];",
            "",
            "\t\t/*",
            "\t\t * Because the order of modification of the vec->count",
            "\t\t * is important, we must make sure that the update",
            "\t\t * of the new prio is seen before we decrement the",
            "\t\t * old prio. This makes sure that the loop sees",
            "\t\t * one or the other when we raise the priority of",
            "\t\t * the run queue. We don't care about when we lower the",
            "\t\t * priority, as that will trigger an rt pull anyway.",
            "\t\t *",
            "\t\t * We only need to do a memory barrier if we updated",
            "\t\t * the new priority vec.",
            "\t\t */",
            "\t\tif (do_mb)",
            "\t\t\tsmp_mb__after_atomic();",
            "",
            "\t\t/*",
            "\t\t * When removing from the vector, we decrement the counter first",
            "\t\t * do a memory barrier and then clear the mask.",
            "\t\t */",
            "\t\tatomic_dec(&(vec)->count);",
            "\t\tsmp_mb__after_atomic();",
            "\t\tcpumask_clear_cpu(cpu, vec->mask);",
            "\t}",
            "",
            "\t*currpri = newpri;",
            "}",
            "int cpupri_init(struct cpupri *cp)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++) {",
            "\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[i];",
            "",
            "\t\tatomic_set(&vec->count, 0);",
            "\t\tif (!zalloc_cpumask_var(&vec->mask, GFP_KERNEL))",
            "\t\t\tgoto cleanup;",
            "\t}",
            "",
            "\tcp->cpu_to_pri = kcalloc(nr_cpu_ids, sizeof(int), GFP_KERNEL);",
            "\tif (!cp->cpu_to_pri)",
            "\t\tgoto cleanup;",
            "",
            "\tfor_each_possible_cpu(i)",
            "\t\tcp->cpu_to_pri[i] = CPUPRI_INVALID;",
            "",
            "\treturn 0;",
            "",
            "cleanup:",
            "\tfor (i--; i >= 0; i--)",
            "\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);",
            "\treturn -ENOMEM;",
            "}",
            "void cpupri_cleanup(struct cpupri *cp)",
            "{",
            "\tint i;",
            "",
            "\tkfree(cp->cpu_to_pri);",
            "\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++)",
            "\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);",
            "}"
          ],
          "function_name": "cpupri_set, cpupri_init, cpupri_cleanup",
          "description": "cpupri_set更新CPU优先级状态，通过原子操作同步位图与计数器；cpupri_init初始化优先级到CPU的映射表与CPU到优先级的数组；cpupri_cleanup释放所有动态分配的位图资源与优先级数组",
          "similarity": 0.527773380279541
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 1,
          "end_line": 41,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  kernel/sched/cpupri.c",
            " *",
            " *  CPU priority management",
            " *",
            " *  Copyright (C) 2007-2008 Novell",
            " *",
            " *  Author: Gregory Haskins <ghaskins@novell.com>",
            " *",
            " *  This code tracks the priority of each CPU so that global migration",
            " *  decisions are easy to calculate.  Each CPU can be in a state as follows:",
            " *",
            " *                 (INVALID), NORMAL, RT1, ... RT99, HIGHER",
            " *",
            " *  going from the lowest priority to the highest.  CPUs in the INVALID state",
            " *  are not eligible for routing.  The system maintains this state with",
            " *  a 2 dimensional bitmap (the first for priority class, the second for CPUs",
            " *  in that class).  Therefore a typical application without affinity",
            " *  restrictions can find a suitable CPU with O(1) complexity (e.g. two bit",
            " *  searches).  For tasks with affinity restrictions, the algorithm has a",
            " *  worst case complexity of O(min(101, nr_domcpus)), though the scenario that",
            " *  yields the worst case search is fairly contrived.",
            " */",
            "",
            "/*",
            " * p->rt_priority   p->prio   newpri   cpupri",
            " *",
            " *\t\t\t\t  -1       -1 (CPUPRI_INVALID)",
            " *",
            " *\t\t\t\t  99        0 (CPUPRI_NORMAL)",
            " *",
            " *\t\t1        98       98        1",
            " *\t      ...",
            " *\t       49        50       50       49",
            " *\t       50        49       49       50",
            " *\t      ...",
            " *\t       99         0        0       99",
            " *",
            " *\t\t\t\t 100\t  100 (CPUPRI_HIGHER)",
            " */"
          ],
          "function_name": null,
          "description": "定义CPU优先级管理模块，通过二维位图跟踪各CPU优先级状态，支持NORMAL、RT1至RT99及HIGHER五种优先级分类，INVALID状态表示CPU不可用，用于全局任务调度时快速计算迁移决策",
          "similarity": 0.5175764560699463
        }
      ]
    }
  ]
}