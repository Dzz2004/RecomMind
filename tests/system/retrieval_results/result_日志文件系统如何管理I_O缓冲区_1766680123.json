{
  "query": "日志文件系统如何管理I/O缓冲区",
  "timestamp": "2025-12-26 00:28:43",
  "retrieved_files": [
    {
      "source_file": "mm/page_io.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:03:03\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_io.c`\n\n---\n\n# page_io.c 技术文档\n\n## 1. 文件概述\n\n`page_io.c` 是 Linux 内核内存管理子系统中负责页面交换 I/O 操作的核心文件。该文件实现了将匿名页写入交换设备（swap-out）和从交换设备读回内存（swap-in）的底层机制，包括基于 `bio` 的块设备交换路径和基于文件系统的直接 I/O 交换路径。此外，还提供了通用的交换文件激活逻辑，用于在启用交换文件时构建物理块到交换页的映射。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `__end_swap_bio_write()` / `end_swap_bio_write()`：处理交换写操作完成的回调，处理写错误并结束写回。\n- `__end_swap_bio_read()` / `end_swap_bio_read()`：处理交换读操作完成的回调，设置页面 uptodate 状态或报告读错误。\n- `generic_swapfile_activate()`：为基于文件的交换设备（如 swapfile）构建连续的物理块映射，填充 `swap_info_struct`。\n- `swap_writepage()`：页面写回交换区的主入口函数，支持 zswap 压缩缓存、内存控制组限制等特性。\n- `swap_writepage_fs()`：通过文件系统直接 I/O 路径（如 swap-over-NFS）执行交换写操作。\n- `sio_pool_init()`：初始化用于异步交换 I/O 的内存池。\n- `sio_write_complete()`：处理基于 kiocb 的异步交换写完成回调。\n\n### 关键数据结构\n\n- `struct swap_iocb`：封装用于文件系统交换 I/O 的 `kiocb` 和 `bio_vec` 数组，支持批量交换页写入。\n- `sio_pool`：`mempool_t` 类型的内存池，用于分配 `swap_iocb` 结构，避免高内存压力下分配失败。\n\n## 3. 关键实现\n\n### 交换 I/O 完成处理\n- 写操作失败时，页面被重新标记为 dirty 并清除 `PG_reclaim` 标志，防止被错误回收，同时输出限频警告日志。\n- 读操作失败仅输出警告；成功则设置 `PG_uptodate` 并解锁页面。\n\n### 交换文件激活 (`generic_swapfile_activate`)\n- 遍历交换文件的逻辑块，使用 `bmap()` 获取物理块号。\n- 验证每个 PAGE_SIZE 对齐区域的物理块是否连续且对齐。\n- 通过 `add_swap_extent()` 将有效的交换页范围注册到交换子系统。\n- 计算交换空间的物理跨度（`span`），用于优化交换分配策略。\n\n### 交换写入路径选择\n- 默认使用 `__swap_writepage()`（基于 `bio` 的块设备路径）。\n- 若启用了 zswap 且压缩存储成功，则跳过磁盘 I/O。\n- 若内存控制组禁用 zswap 回写，则返回 `AOP_WRITEPAGE_ACTIVATE` 以保留页面在内存中。\n- 对于 NFS 等不支持 `bmap` 的文件系统，使用 `swap_writepage_fs()` 路径，通过 `kiocb` 异步 DIO 写入。\n\n### 异步交换 I/O 批处理\n- `swap_writepage_fs()` 支持通过 `wbc->swap_plug` 合并多个相邻页面的写请求到同一个 `swap_iocb`。\n- 利用 `mempool` 保证在内存紧张时仍能分配 I/O 控制块。\n- 完成回调中处理部分写入错误，标记所有相关页面为 dirty 并结束写回。\n\n### 资源统计与控制\n- 通过 `count_swpout_vm_event()` 更新透明大页（THP）和普通页的交换出计数。\n- 在配置了 MEMCG 和 BLK_CGROUP 时，通过 `bio_associate_blkg_from_page()` 将 I/O 请求关联到页面所属的 blkcg，实现 I/O 资源隔离。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/swap.h>`、`<linux/pagemap.h>` 等，与页面回收、反向映射、内存控制组紧密集成。\n- **块设备层**：通过 `<linux/bio.h>`、`<linux/blkdev.h>` 与块 I/O 子系统交互。\n- **文件系统接口**：使用 `bmap()` 和 `kiocb` 与具体文件系统（如 ext4、xfs）或网络文件系统（如 NFS）协作。\n- **压缩子系统**：集成 `<linux/zswap.h>`，支持透明压缩交换缓存。\n- **资源控制器**：依赖 MEMCG 和 BLK_CGROUP 实现内存与 I/O 的多租户隔离。\n- **内部头文件**：包含本地 `\"swap.h\"` 获取交换子系统私有接口。\n\n## 5. 使用场景\n\n- **系统内存不足时**：页面回收机制调用 `swap_writepage()` 将匿名页换出到交换设备。\n- **启用交换文件时**：`swapon` 系统调用执行 `generic_swapfile_activate()` 初始化交换文件的物理布局。\n- **从交换区缺页中断**：当访问已换出页面时，内核通过 `end_swap_bio_read` 路径将数据读回内存。\n- **容器环境**：在启用内存和 I/O 控制组的系统中，确保交换 I/O 正确归属到对应 cgroup。\n- **使用压缩交换缓存**：当 zswap 启用时，优先尝试压缩页面而非立即写入慢速交换设备。\n- **网络交换场景**：在无本地块设备的环境中（如云实例使用 NFS 作为交换后端），通过 `swap_writepage_fs()` 路径完成交换。",
      "similarity": 0.6024851202964783,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/page_io.c",
          "start_line": 30,
          "end_line": 169,
          "content": [
            "static void __end_swap_bio_write(struct bio *bio)",
            "{",
            "\tstruct folio *folio = bio_first_folio_all(bio);",
            "",
            "\tif (bio->bi_status) {",
            "\t\t/*",
            "\t\t * We failed to write the page out to swap-space.",
            "\t\t * Re-dirty the page in order to avoid it being reclaimed.",
            "\t\t * Also print a dire warning that things will go BAD (tm)",
            "\t\t * very quickly.",
            "\t\t *",
            "\t\t * Also clear PG_reclaim to avoid folio_rotate_reclaimable()",
            "\t\t */",
            "\t\tfolio_mark_dirty(folio);",
            "\t\tpr_alert_ratelimited(\"Write-error on swap-device (%u:%u:%llu)\\n\",",
            "\t\t\t\t     MAJOR(bio_dev(bio)), MINOR(bio_dev(bio)),",
            "\t\t\t\t     (unsigned long long)bio->bi_iter.bi_sector);",
            "\t\tfolio_clear_reclaim(folio);",
            "\t}",
            "\tfolio_end_writeback(folio);",
            "}",
            "static void end_swap_bio_write(struct bio *bio)",
            "{",
            "\t__end_swap_bio_write(bio);",
            "\tbio_put(bio);",
            "}",
            "static void __end_swap_bio_read(struct bio *bio)",
            "{",
            "\tstruct folio *folio = bio_first_folio_all(bio);",
            "",
            "\tif (bio->bi_status) {",
            "\t\tpr_alert_ratelimited(\"Read-error on swap-device (%u:%u:%llu)\\n\",",
            "\t\t\t\t     MAJOR(bio_dev(bio)), MINOR(bio_dev(bio)),",
            "\t\t\t\t     (unsigned long long)bio->bi_iter.bi_sector);",
            "\t} else {",
            "\t\tfolio_mark_uptodate(folio);",
            "\t}",
            "\tfolio_unlock(folio);",
            "}",
            "static void end_swap_bio_read(struct bio *bio)",
            "{",
            "\t__end_swap_bio_read(bio);",
            "\tbio_put(bio);",
            "}",
            "int generic_swapfile_activate(struct swap_info_struct *sis,",
            "\t\t\t\tstruct file *swap_file,",
            "\t\t\t\tsector_t *span)",
            "{",
            "\tstruct address_space *mapping = swap_file->f_mapping;",
            "\tstruct inode *inode = mapping->host;",
            "\tunsigned blocks_per_page;",
            "\tunsigned long page_no;",
            "\tunsigned blkbits;",
            "\tsector_t probe_block;",
            "\tsector_t last_block;",
            "\tsector_t lowest_block = -1;",
            "\tsector_t highest_block = 0;",
            "\tint nr_extents = 0;",
            "\tint ret;",
            "",
            "\tblkbits = inode->i_blkbits;",
            "\tblocks_per_page = PAGE_SIZE >> blkbits;",
            "",
            "\t/*",
            "\t * Map all the blocks into the extent tree.  This code doesn't try",
            "\t * to be very smart.",
            "\t */",
            "\tprobe_block = 0;",
            "\tpage_no = 0;",
            "\tlast_block = i_size_read(inode) >> blkbits;",
            "\twhile ((probe_block + blocks_per_page) <= last_block &&",
            "\t\t\tpage_no < sis->max) {",
            "\t\tunsigned block_in_page;",
            "\t\tsector_t first_block;",
            "",
            "\t\tcond_resched();",
            "",
            "\t\tfirst_block = probe_block;",
            "\t\tret = bmap(inode, &first_block);",
            "\t\tif (ret || !first_block)",
            "\t\t\tgoto bad_bmap;",
            "",
            "\t\t/*",
            "\t\t * It must be PAGE_SIZE aligned on-disk",
            "\t\t */",
            "\t\tif (first_block & (blocks_per_page - 1)) {",
            "\t\t\tprobe_block++;",
            "\t\t\tgoto reprobe;",
            "\t\t}",
            "",
            "\t\tfor (block_in_page = 1; block_in_page < blocks_per_page;",
            "\t\t\t\t\tblock_in_page++) {",
            "\t\t\tsector_t block;",
            "",
            "\t\t\tblock = probe_block + block_in_page;",
            "\t\t\tret = bmap(inode, &block);",
            "\t\t\tif (ret || !block)",
            "\t\t\t\tgoto bad_bmap;",
            "",
            "\t\t\tif (block != first_block + block_in_page) {",
            "\t\t\t\t/* Discontiguity */",
            "\t\t\t\tprobe_block++;",
            "\t\t\t\tgoto reprobe;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tfirst_block >>= (PAGE_SHIFT - blkbits);",
            "\t\tif (page_no) {\t/* exclude the header page */",
            "\t\t\tif (first_block < lowest_block)",
            "\t\t\t\tlowest_block = first_block;",
            "\t\t\tif (first_block > highest_block)",
            "\t\t\t\thighest_block = first_block;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * We found a PAGE_SIZE-length, PAGE_SIZE-aligned run of blocks",
            "\t\t */",
            "\t\tret = add_swap_extent(sis, page_no, 1, first_block);",
            "\t\tif (ret < 0)",
            "\t\t\tgoto out;",
            "\t\tnr_extents += ret;",
            "\t\tpage_no++;",
            "\t\tprobe_block += blocks_per_page;",
            "reprobe:",
            "\t\tcontinue;",
            "\t}",
            "\tret = nr_extents;",
            "\t*span = 1 + highest_block - lowest_block;",
            "\tif (page_no == 0)",
            "\t\tpage_no = 1;\t/* force Empty message */",
            "\tsis->max = page_no;",
            "\tsis->pages = page_no - 1;",
            "\tsis->highest_bit = page_no - 1;",
            "out:",
            "\treturn ret;",
            "bad_bmap:",
            "\tpr_err(\"swapon: swapfile has holes\\n\");",
            "\tret = -EINVAL;",
            "\tgoto out;",
            "}"
          ],
          "function_name": "__end_swap_bio_write, end_swap_bio_write, __end_swap_bio_read, end_swap_bio_read, generic_swapfile_activate",
          "description": "实现交换I/O完成回调函数，处理写入错误重标记脏页及读取错误日志；generic_swapfile_activate扫描交换文件块并构建交换区范围。",
          "similarity": 0.5692405104637146
        },
        {
          "chunk_id": 2,
          "file_path": "mm/page_io.c",
          "start_line": 179,
          "end_line": 280,
          "content": [
            "int swap_writepage(struct page *page, struct writeback_control *wbc)",
            "{",
            "\tstruct folio *folio = page_folio(page);",
            "\tint ret;",
            "",
            "\tif (folio_free_swap(folio)) {",
            "\t\tfolio_unlock(folio);",
            "\t\treturn 0;",
            "\t}",
            "\t/*",
            "\t * Arch code may have to preserve more data than just the page",
            "\t * contents, e.g. memory tags.",
            "\t */",
            "\tret = arch_prepare_to_swap(folio);",
            "\tif (ret) {",
            "\t\tfolio_mark_dirty(folio);",
            "\t\tfolio_unlock(folio);",
            "\t\treturn ret;",
            "\t}",
            "\tif (zswap_store(folio)) {",
            "\t\tfolio_start_writeback(folio);",
            "\t\tfolio_unlock(folio);",
            "\t\tfolio_end_writeback(folio);",
            "\t\treturn 0;",
            "\t}",
            "\tif (!mem_cgroup_zswap_writeback_enabled(folio_memcg(folio))) {",
            "\t\tfolio_mark_dirty(folio);",
            "\t\treturn AOP_WRITEPAGE_ACTIVATE;",
            "\t}",
            "",
            "\t__swap_writepage(folio, wbc);",
            "\treturn 0;",
            "}",
            "static inline void count_swpout_vm_event(struct folio *folio)",
            "{",
            "#ifdef CONFIG_TRANSPARENT_HUGEPAGE",
            "\tif (unlikely(folio_test_pmd_mappable(folio))) {",
            "\t\tcount_memcg_folio_events(folio, THP_SWPOUT, 1);",
            "\t\tcount_vm_event(THP_SWPOUT);",
            "\t}",
            "\tcount_mthp_stat(folio_order(folio), MTHP_STAT_SWPOUT);",
            "#endif",
            "\tcount_memcg_folio_events(folio, PSWPOUT, folio_nr_pages(folio));",
            "\tcount_vm_events(PSWPOUT, folio_nr_pages(folio));",
            "}",
            "static void bio_associate_blkg_from_page(struct bio *bio, struct folio *folio)",
            "{",
            "\tstruct cgroup_subsys_state *css;",
            "\tstruct mem_cgroup *memcg;",
            "",
            "\tmemcg = folio_memcg(folio);",
            "\tif (!memcg)",
            "\t\treturn;",
            "",
            "\trcu_read_lock();",
            "\tcss = cgroup_e_css(memcg->css.cgroup, &io_cgrp_subsys);",
            "\tbio_associate_blkg_from_css(bio, css);",
            "\trcu_read_unlock();",
            "}",
            "int sio_pool_init(void)",
            "{",
            "\tif (!sio_pool) {",
            "\t\tmempool_t *pool = mempool_create_kmalloc_pool(",
            "\t\t\tSWAP_CLUSTER_MAX, sizeof(struct swap_iocb));",
            "\t\tif (cmpxchg(&sio_pool, NULL, pool))",
            "\t\t\tmempool_destroy(pool);",
            "\t}",
            "\tif (!sio_pool)",
            "\t\treturn -ENOMEM;",
            "\treturn 0;",
            "}",
            "static void sio_write_complete(struct kiocb *iocb, long ret)",
            "{",
            "\tstruct swap_iocb *sio = container_of(iocb, struct swap_iocb, iocb);",
            "\tstruct page *page = sio->bvec[0].bv_page;",
            "\tint p;",
            "",
            "\tif (ret != sio->len) {",
            "\t\t/*",
            "\t\t * In the case of swap-over-nfs, this can be a",
            "\t\t * temporary failure if the system has limited",
            "\t\t * memory for allocating transmit buffers.",
            "\t\t * Mark the page dirty and avoid",
            "\t\t * folio_rotate_reclaimable but rate-limit the",
            "\t\t * messages but do not flag PageError like",
            "\t\t * the normal direct-to-bio case as it could",
            "\t\t * be temporary.",
            "\t\t */",
            "\t\tpr_err_ratelimited(\"Write error %ld on dio swapfile (%llu)\\n\",",
            "\t\t\t\t   ret, page_file_offset(page));",
            "\t\tfor (p = 0; p < sio->pages; p++) {",
            "\t\t\tpage = sio->bvec[p].bv_page;",
            "\t\t\tset_page_dirty(page);",
            "\t\t\tClearPageReclaim(page);",
            "\t\t}",
            "\t}",
            "",
            "\tfor (p = 0; p < sio->pages; p++)",
            "\t\tend_page_writeback(sio->bvec[p].bv_page);",
            "",
            "\tmempool_free(sio, sio_pool);",
            "}"
          ],
          "function_name": "swap_writepage, count_swpout_vm_event, bio_associate_blkg_from_page, sio_pool_init, sio_write_complete",
          "description": "swap_writepage发起页面换出流程，调用架构特定准备函数并处理ZSwap缓存；sio_write_complete处理异步写入完成，标记脏页并释放资源。",
          "similarity": 0.5515651702880859
        },
        {
          "chunk_id": 3,
          "file_path": "mm/page_io.c",
          "start_line": 298,
          "end_line": 400,
          "content": [
            "static void swap_writepage_fs(struct folio *folio, struct writeback_control *wbc)",
            "{",
            "\tstruct swap_iocb *sio = NULL;",
            "\tstruct swap_info_struct *sis = swp_swap_info(folio->swap);",
            "\tstruct file *swap_file = sis->swap_file;",
            "\tloff_t pos = folio_file_pos(folio);",
            "",
            "\tcount_swpout_vm_event(folio);",
            "\tfolio_start_writeback(folio);",
            "\tfolio_unlock(folio);",
            "\tif (wbc->swap_plug)",
            "\t\tsio = *wbc->swap_plug;",
            "\tif (sio) {",
            "\t\tif (sio->iocb.ki_filp != swap_file ||",
            "\t\t    sio->iocb.ki_pos + sio->len != pos) {",
            "\t\t\tswap_write_unplug(sio);",
            "\t\t\tsio = NULL;",
            "\t\t}",
            "\t}",
            "\tif (!sio) {",
            "\t\tsio = mempool_alloc(sio_pool, GFP_NOIO);",
            "\t\tinit_sync_kiocb(&sio->iocb, swap_file);",
            "\t\tsio->iocb.ki_complete = sio_write_complete;",
            "\t\tsio->iocb.ki_pos = pos;",
            "\t\tsio->pages = 0;",
            "\t\tsio->len = 0;",
            "\t}",
            "\tbvec_set_folio(&sio->bvec[sio->pages], folio, folio_size(folio), 0);",
            "\tsio->len += folio_size(folio);",
            "\tsio->pages += 1;",
            "\tif (sio->pages == ARRAY_SIZE(sio->bvec) || !wbc->swap_plug) {",
            "\t\tswap_write_unplug(sio);",
            "\t\tsio = NULL;",
            "\t}",
            "\tif (wbc->swap_plug)",
            "\t\t*wbc->swap_plug = sio;",
            "}",
            "static void swap_writepage_bdev_sync(struct folio *folio,",
            "\t\tstruct writeback_control *wbc, struct swap_info_struct *sis)",
            "{",
            "\tstruct bio_vec bv;",
            "\tstruct bio bio;",
            "",
            "\tbio_init(&bio, sis->bdev, &bv, 1,",
            "\t\t REQ_OP_WRITE | REQ_SWAP | wbc_to_write_flags(wbc));",
            "\tbio.bi_iter.bi_sector = swap_folio_sector(folio);",
            "\tbio_add_folio_nofail(&bio, folio, folio_size(folio), 0);",
            "",
            "\tbio_associate_blkg_from_page(&bio, folio);",
            "\tcount_swpout_vm_event(folio);",
            "",
            "\tfolio_start_writeback(folio);",
            "\tfolio_unlock(folio);",
            "",
            "\tsubmit_bio_wait(&bio);",
            "\t__end_swap_bio_write(&bio);",
            "}",
            "static void swap_writepage_bdev_async(struct folio *folio,",
            "\t\tstruct writeback_control *wbc, struct swap_info_struct *sis)",
            "{",
            "\tstruct bio *bio;",
            "",
            "\tbio = bio_alloc(sis->bdev, 1,",
            "\t\t\tREQ_OP_WRITE | REQ_SWAP | wbc_to_write_flags(wbc),",
            "\t\t\tGFP_NOIO);",
            "\tbio->bi_iter.bi_sector = swap_folio_sector(folio);",
            "\tbio->bi_end_io = end_swap_bio_write;",
            "\tbio_add_folio_nofail(bio, folio, folio_size(folio), 0);",
            "",
            "\tbio_associate_blkg_from_page(bio, folio);",
            "\tcount_swpout_vm_event(folio);",
            "\tfolio_start_writeback(folio);",
            "\tfolio_unlock(folio);",
            "\tsubmit_bio(bio);",
            "}",
            "void __swap_writepage(struct folio *folio, struct writeback_control *wbc)",
            "{",
            "\tstruct swap_info_struct *sis = swp_swap_info(folio->swap);",
            "",
            "\tVM_BUG_ON_FOLIO(!folio_test_swapcache(folio), folio);",
            "\t/*",
            "\t * ->flags can be updated non-atomicially (scan_swap_map_slots),",
            "\t * but that will never affect SWP_FS_OPS, so the data_race",
            "\t * is safe.",
            "\t */",
            "\tif (data_race(sis->flags & SWP_FS_OPS))",
            "\t\tswap_writepage_fs(folio, wbc);",
            "\telse if (sis->flags & SWP_SYNCHRONOUS_IO)",
            "\t\tswap_writepage_bdev_sync(folio, wbc, sis);",
            "\telse",
            "\t\tswap_writepage_bdev_async(folio, wbc, sis);",
            "}",
            "void swap_write_unplug(struct swap_iocb *sio)",
            "{",
            "\tstruct iov_iter from;",
            "\tstruct address_space *mapping = sio->iocb.ki_filp->f_mapping;",
            "\tint ret;",
            "",
            "\tiov_iter_bvec(&from, ITER_SOURCE, sio->bvec, sio->pages, sio->len);",
            "\tret = mapping->a_ops->swap_rw(&sio->iocb, &from);",
            "\tif (ret != -EIOCBQUEUED)",
            "\t\tsio_write_complete(&sio->iocb, ret);",
            "}"
          ],
          "function_name": "swap_writepage_fs, swap_writepage_bdev_sync, swap_writepage_bdev_async, __swap_writepage, swap_write_unplug",
          "description": "__swap_writepage根据配置选择同步/异步块设备写入路径，通过bio结构执行交换页面写入操作，支持批量提交优化。",
          "similarity": 0.5362640023231506
        },
        {
          "chunk_id": 5,
          "file_path": "mm/page_io.c",
          "start_line": 543,
          "end_line": 553,
          "content": [
            "void __swap_read_unplug(struct swap_iocb *sio)",
            "{",
            "\tstruct iov_iter from;",
            "\tstruct address_space *mapping = sio->iocb.ki_filp->f_mapping;",
            "\tint ret;",
            "",
            "\tiov_iter_bvec(&from, ITER_DEST, sio->bvec, sio->pages, sio->len);",
            "\tret = mapping->a_ops->swap_rw(&sio->iocb, &from);",
            "\tif (ret != -EIOCBQUEUED)",
            "\t\tsio_read_complete(&sio->iocb, ret);",
            "}"
          ],
          "function_name": "__swap_read_unplug",
          "description": "该函数是处理交换读取操作的关键函数，在页置换过程中将从交换设备读取的数据通过I/O向量传递至文件地址空间。  \n它通过构建iov_iter并调用address_space的a_ops->swap_rw接口进行实际的数据读取，并根据返回状态触发相应的完成回调。  \n注：swap_rw的具体实现依赖于对应文件系统的a_ops，此处上下文不完整。",
          "similarity": 0.5278915166854858
        },
        {
          "chunk_id": 0,
          "file_path": "mm/page_io.c",
          "start_line": 1,
          "end_line": 29,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " *  linux/mm/page_io.c",
            " *",
            " *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds",
            " *",
            " *  Swap reorganised 29.12.95, ",
            " *  Asynchronous swapping added 30.12.95. Stephen Tweedie",
            " *  Removed race in async swapping. 14.4.1996. Bruno Haible",
            " *  Add swap of shared pages through the page cache. 20.2.1998. Stephen Tweedie",
            " *  Always use brw_page, life becomes simpler. 12 May 1998 Eric Biederman",
            " */",
            "",
            "#include <linux/mm.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/gfp.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/swap.h>",
            "#include <linux/bio.h>",
            "#include <linux/swapops.h>",
            "#include <linux/writeback.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/psi.h>",
            "#include <linux/uio.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/zswap.h>",
            "#include \"swap.h\"",
            ""
          ],
          "function_name": null,
          "description": "声明头文件并引入交换页面管理模块的基本依赖，为后续交换操作提供基础框架。",
          "similarity": 0.5185059309005737
        }
      ]
    },
    {
      "source_file": "mm/shrinker.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:19:57\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `shrinker.c`\n\n---\n\n# shrinker.c 技术文档\n\n## 1. 文件概述\n\n`shrinker.c` 是 Linux 内核内存管理子系统中负责管理 **shrinker**（收缩器）机制的核心实现文件。Shrinker 是一种回调机制，允许内核子系统（如 dentry、inode、buffer cache 等）在系统内存压力下释放可回收的缓存对象。该文件主要实现了：\n\n- 全局 shrinker 列表的注册与管理\n- 基于 cgroup v2 的 memcg（memory control group）感知的 shrinker 支持\n- 每个 memcg 对每个 shrinker 的延迟扫描计数（`nr_deferred`）和活跃状态位图（`shrinker_bit`）的动态分配与维护\n- 在 memcg 层级结构变化（如 cgroup 删除）时，将子 cgroup 的 deferred 计数迁移至父 cgroup（reparent）\n\n该文件特别关注 **memcg-aware shrinker** 的可扩展性设计，通过分块（unit-based）数据结构支持动态增长的 shrinker ID 空间。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `shrinker_list`：全局链表，用于链接所有已注册的 `struct shrinker` 实例。\n- `shrinker_mutex`：保护 shrinker 注册/注销及 memcg shrinker info 扩展操作的互斥锁。\n- `shrinker_idr`（仅 CONFIG_MEMCG）：IDR 结构，为每个 memcg-aware shrinker 分配唯一 ID。\n- `shrinker_nr_max`（仅 CONFIG_MEMCG）：当前系统中 shrinker ID 的最大值（向上对齐到 `SHRINKER_UNIT_BITS`）。\n\n### 主要数据结构（仅 CONFIG_MEMCG）\n- `struct shrinker_info`：每个 memcg 每个 NUMA 节点维护的 shrinker 元数据容器。\n- `struct shrinker_info_unit`：`shrinker_info` 中的分块单元，包含：\n  - `map[SHRINKER_UNIT_BITS]`：位图，标记对应 shrinker 是否在此 memcg 中有可回收对象。\n  - `nr_deferred[SHRINKER_UNIT_BITS]`：原子长整型数组，记录每个 shrinker 在此 memcg 中的延迟扫描计数。\n\n### 主要函数\n\n#### 全局 shrinker 管理\n- `register_shrinker()` / `unregister_shrinker()`（定义在其他文件，但使用本文件的 list 和 mutex）\n\n#### Memcg Shrinker Info 生命周期管理（仅 CONFIG_MEMCG）\n- `alloc_shrinker_info(struct mem_cgroup *memcg)`：为指定 memcg 分配所有 NUMA 节点的 `shrinker_info`。\n- `free_shrinker_info(struct mem_cgroup *memcg)`：释放指定 memcg 的所有 `shrinker_info`。\n- `expand_shrinker_info(int new_id)`：当新 shrinker ID 超出当前 `shrinker_nr_max` 时，扩展所有 memcg 的 `shrinker_info` 容量。\n\n#### Shrinker 状态操作（仅 CONFIG_MEMCG）\n- `set_shrinker_bit(struct mem_cgroup *memcg, int nid, int shrinker_id)`：设置指定 memcg/nid/shrinker 的活跃位。\n- `xchg_nr_deferred_memcg()` / `add_nr_deferred_memcg()`：原子地交换或增加指定 shrinker 在 memcg 中的 deferred 计数。\n\n#### Memcg 层级维护\n- `reparent_shrinker_deferred(struct mem_cgroup *memcg)`：将被销毁 memcg 的 deferred 计数累加到其父 memcg。\n\n#### 辅助函数（条件编译）\n- `shrinker_memcg_alloc()` / `shrinker_memcg_remove()`：为 shrinker 分配/移除 memcg ID。\n- `xchg_nr_deferred()` / `add_nr_deferred()`：根据是否启用 memcg，调用全局或 memcg 特定的 deferred 操作。\n\n## 3. 关键实现\n\n### 动态可扩展的 Shrinker ID 管理\n- 使用 `idr` 为每个 memcg-aware shrinker 分配唯一 ID。\n- `shrinker_nr_max` 记录当前最大 ID（向上对齐到 `SHRINKER_UNIT_BITS`，通常为 `PAGE_SIZE * 8`）。\n- 当新 shrinker ID ≥ `shrinker_nr_max` 时，调用 `expand_shrinker_info()` 遍历所有 memcg，为其 `shrinker_info` 分配更多 `shrinker_info_unit` 块。\n\n### 分块存储设计（Unit-based Storage）\n- `shrinker_info` 不直接存储大数组，而是通过指针数组 `unit[]` 指向多个 `shrinker_info_unit`。\n- 每个 `unit` 管理 `SHRINKER_UNIT_BITS` 个 shrinker 的状态（位图 + deferred 计数）。\n- 扩展时只需分配新增的 unit 块，已有数据通过 `memcpy` 复用，避免全量重分配。\n\n### RCU 与锁协同\n- `shrinker_info` 的读取使用 RCU（`rcu_dereference`），保证扫描路径无锁。\n- 修改（分配、扩展、释放）受 `shrinker_mutex` 保护，并使用 `rcu_assign_pointer` / `kvfree_rcu` 实现安全替换。\n\n### Deferred 计数迁移（Reparenting）\n- 当 memcg 被销毁时，其所有 shrinker 的 deferred 计数需合并到父 memcg。\n- 通过 `reparent_shrinker_deferred()` 在 `shrinker_mutex` 保护下遍历所有 shrinker 单元，原子累加计数。\n\n### 内存节点（NUMA）感知\n- 每个 memcg 为每个 NUMA 节点维护独立的 `shrinker_info`，支持 `SHRINKER_NUMA_AWARE` shrinker 按节点回收。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/memcontrol.h>`：memcg 核心接口\n  - `<linux/shrinker.h>`：shrinker 结构定义\n  - `<linux/rculist.h>`：RCU 保护的链表操作\n  - `\"internal.h\"`：mm 子系统内部头文件\n- **内核配置依赖**：\n  - `CONFIG_MEMCG`：启用 memcg-aware shrinker 支持\n  - `CONFIG_SHRINKER_DEBUG`（间接）：可能影响 tracepoint 行为\n- **协作模块**：\n  - VFS（dentry/inode cache）、Buffer Cache、Tmpfs 等通过注册 shrinker 使用此机制\n  - Memory Cgroup（memcg）子系统提供层级结构和 per-node 数据\n  - VM writeback 和 kswapd 调用 shrinker 回调进行内存回收\n\n## 5. 使用场景\n\n1. **内存压力下的缓存回收**：\n   - 当系统内存不足时，kswapd 或 direct reclaim 路径调用 `shrink_slab()`，遍历 `shrinker_list` 并执行各 shrinker 的 `.scan_objects` 回调。\n\n2. **Memcg 内存限制回收**：\n   - 当某个 memcg 超过内存限制时，reclaim 过程会针对该 memcg 调用 shrinker，利用 `shrinker_bit` 快速判断哪些 shrinker 在此 memcg 中有对象，避免无效扫描。\n\n3. **Shrinker 注册/注销**：\n   - 子系统初始化时调用 `register_shrinker()`，将 shrinker 加入全局列表；模块卸载时调用 `unregister_shrinker()` 移除。\n\n4. **Cgroup 层级变更**：\n   - 当 memcg 被删除时，其资源（包括 shrinker deferred 计数）通过 `reparent_shrinker_deferred()` 迁移到父 cgroup，确保回收逻辑连续性。\n\n5. **动态 Shrinker 扩展**：\n   - 系统运行时加载新模块（如新文件系统）注册 shrinker，若 ID 超出现有范围，自动触发 `expand_shrinker_info()` 扩容所有 memcg 的元数据。",
      "similarity": 0.5965231657028198,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/shrinker.c",
          "start_line": 150,
          "end_line": 257,
          "content": [
            "static int expand_shrinker_info(int new_id)",
            "{",
            "\tint ret = 0;",
            "\tint new_nr_max = round_up(new_id + 1, SHRINKER_UNIT_BITS);",
            "\tint new_size, old_size = 0;",
            "\tstruct mem_cgroup *memcg;",
            "",
            "\tif (!root_mem_cgroup)",
            "\t\tgoto out;",
            "",
            "\tlockdep_assert_held(&shrinker_mutex);",
            "",
            "\tnew_size = shrinker_unit_size(new_nr_max);",
            "\told_size = shrinker_unit_size(shrinker_nr_max);",
            "",
            "\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);",
            "\tdo {",
            "\t\tret = expand_one_shrinker_info(memcg, new_size, old_size,",
            "\t\t\t\t\t       new_nr_max);",
            "\t\tif (ret) {",
            "\t\t\tmem_cgroup_iter_break(NULL, memcg);",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)) != NULL);",
            "out:",
            "\tif (!ret)",
            "\t\tshrinker_nr_max = new_nr_max;",
            "",
            "\treturn ret;",
            "}",
            "static inline int shrinker_id_to_index(int shrinker_id)",
            "{",
            "\treturn shrinker_id / SHRINKER_UNIT_BITS;",
            "}",
            "static inline int shrinker_id_to_offset(int shrinker_id)",
            "{",
            "\treturn shrinker_id % SHRINKER_UNIT_BITS;",
            "}",
            "static inline int calc_shrinker_id(int index, int offset)",
            "{",
            "\treturn index * SHRINKER_UNIT_BITS + offset;",
            "}",
            "void set_shrinker_bit(struct mem_cgroup *memcg, int nid, int shrinker_id)",
            "{",
            "\tif (shrinker_id >= 0 && memcg && !mem_cgroup_is_root(memcg)) {",
            "\t\tstruct shrinker_info *info;",
            "\t\tstruct shrinker_info_unit *unit;",
            "",
            "\t\trcu_read_lock();",
            "\t\tinfo = rcu_dereference(memcg->nodeinfo[nid]->shrinker_info);",
            "\t\tunit = info->unit[shrinker_id_to_index(shrinker_id)];",
            "\t\tif (!WARN_ON_ONCE(shrinker_id >= info->map_nr_max)) {",
            "\t\t\t/* Pairs with smp mb in shrink_slab() */",
            "\t\t\tsmp_mb__before_atomic();",
            "\t\t\tset_bit(shrinker_id_to_offset(shrinker_id), unit->map);",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t}",
            "}",
            "static int shrinker_memcg_alloc(struct shrinker *shrinker)",
            "{",
            "\tint id, ret = -ENOMEM;",
            "",
            "\tif (mem_cgroup_disabled())",
            "\t\treturn -ENOSYS;",
            "",
            "\tmutex_lock(&shrinker_mutex);",
            "\tid = idr_alloc(&shrinker_idr, shrinker, 0, 0, GFP_KERNEL);",
            "\tif (id < 0)",
            "\t\tgoto unlock;",
            "",
            "\tif (id >= shrinker_nr_max) {",
            "\t\tif (expand_shrinker_info(id)) {",
            "\t\t\tidr_remove(&shrinker_idr, id);",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "\t}",
            "\tshrinker->id = id;",
            "\tret = 0;",
            "unlock:",
            "\tmutex_unlock(&shrinker_mutex);",
            "\treturn ret;",
            "}",
            "static void shrinker_memcg_remove(struct shrinker *shrinker)",
            "{",
            "\tint id = shrinker->id;",
            "",
            "\tBUG_ON(id < 0);",
            "",
            "\tlockdep_assert_held(&shrinker_mutex);",
            "",
            "\tidr_remove(&shrinker_idr, id);",
            "}",
            "static long xchg_nr_deferred_memcg(int nid, struct shrinker *shrinker,",
            "\t\t\t\t   struct mem_cgroup *memcg)",
            "{",
            "\tstruct shrinker_info *info;",
            "\tstruct shrinker_info_unit *unit;",
            "\tlong nr_deferred;",
            "",
            "\trcu_read_lock();",
            "\tinfo = rcu_dereference(memcg->nodeinfo[nid]->shrinker_info);",
            "\tunit = info->unit[shrinker_id_to_index(shrinker->id)];",
            "\tnr_deferred = atomic_long_xchg(&unit->nr_deferred[shrinker_id_to_offset(shrinker->id)], 0);",
            "\trcu_read_unlock();",
            "",
            "\treturn nr_deferred;",
            "}"
          ],
          "function_name": "expand_shrinker_info, shrinker_id_to_index, shrinker_id_to_offset, calc_shrinker_id, set_shrinker_bit, shrinker_memcg_alloc, shrinker_memcg_remove, xchg_nr_deferred_memcg",
          "description": "提供缩小器ID映射管理功能，包含ID到索引/偏移转换逻辑，支持动态扩展缩小器信息表，并维护缩小器位图标记。",
          "similarity": 0.5274272561073303
        },
        {
          "chunk_id": 1,
          "file_path": "mm/shrinker.c",
          "start_line": 16,
          "end_line": 136,
          "content": [
            "static inline int shrinker_unit_size(int nr_items)",
            "{",
            "\treturn (DIV_ROUND_UP(nr_items, SHRINKER_UNIT_BITS) * sizeof(struct shrinker_info_unit *));",
            "}",
            "static inline void shrinker_unit_free(struct shrinker_info *info, int start)",
            "{",
            "\tstruct shrinker_info_unit **unit;",
            "\tint nr, i;",
            "",
            "\tif (!info)",
            "\t\treturn;",
            "",
            "\tunit = info->unit;",
            "\tnr = DIV_ROUND_UP(info->map_nr_max, SHRINKER_UNIT_BITS);",
            "",
            "\tfor (i = start; i < nr; i++) {",
            "\t\tif (!unit[i])",
            "\t\t\tbreak;",
            "",
            "\t\tkfree(unit[i]);",
            "\t\tunit[i] = NULL;",
            "\t}",
            "}",
            "static inline int shrinker_unit_alloc(struct shrinker_info *new,",
            "\t\t\t\t       struct shrinker_info *old, int nid)",
            "{",
            "\tstruct shrinker_info_unit *unit;",
            "\tint nr = DIV_ROUND_UP(new->map_nr_max, SHRINKER_UNIT_BITS);",
            "\tint start = old ? DIV_ROUND_UP(old->map_nr_max, SHRINKER_UNIT_BITS) : 0;",
            "\tint i;",
            "",
            "\tfor (i = start; i < nr; i++) {",
            "\t\tunit = kzalloc_node(sizeof(*unit), GFP_KERNEL, nid);",
            "\t\tif (!unit) {",
            "\t\t\tshrinker_unit_free(new, start);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "",
            "\t\tnew->unit[i] = unit;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "void free_shrinker_info(struct mem_cgroup *memcg)",
            "{",
            "\tstruct mem_cgroup_per_node *pn;",
            "\tstruct shrinker_info *info;",
            "\tint nid;",
            "",
            "\tfor_each_node(nid) {",
            "\t\tpn = memcg->nodeinfo[nid];",
            "\t\tinfo = rcu_dereference_protected(pn->shrinker_info, true);",
            "\t\tshrinker_unit_free(info, 0);",
            "\t\tkvfree(info);",
            "\t\trcu_assign_pointer(pn->shrinker_info, NULL);",
            "\t}",
            "}",
            "int alloc_shrinker_info(struct mem_cgroup *memcg)",
            "{",
            "\tint nid, ret = 0;",
            "\tint array_size = 0;",
            "",
            "\tmutex_lock(&shrinker_mutex);",
            "\tarray_size = shrinker_unit_size(shrinker_nr_max);",
            "\tfor_each_node(nid) {",
            "\t\tstruct shrinker_info *info = kvzalloc_node(sizeof(*info) + array_size,",
            "\t\t\t\t\t\t\t   GFP_KERNEL, nid);",
            "\t\tif (!info)",
            "\t\t\tgoto err;",
            "\t\tinfo->map_nr_max = shrinker_nr_max;",
            "\t\tif (shrinker_unit_alloc(info, NULL, nid)) {",
            "\t\t\tkvfree(info);",
            "\t\t\tgoto err;",
            "\t\t}",
            "\t\trcu_assign_pointer(memcg->nodeinfo[nid]->shrinker_info, info);",
            "\t}",
            "\tmutex_unlock(&shrinker_mutex);",
            "",
            "\treturn ret;",
            "",
            "err:",
            "\tmutex_unlock(&shrinker_mutex);",
            "\tfree_shrinker_info(memcg);",
            "\treturn -ENOMEM;",
            "}",
            "static int expand_one_shrinker_info(struct mem_cgroup *memcg, int new_size,",
            "\t\t\t\t    int old_size, int new_nr_max)",
            "{",
            "\tstruct shrinker_info *new, *old;",
            "\tstruct mem_cgroup_per_node *pn;",
            "\tint nid;",
            "",
            "\tfor_each_node(nid) {",
            "\t\tpn = memcg->nodeinfo[nid];",
            "\t\told = shrinker_info_protected(memcg, nid);",
            "\t\t/* Not yet online memcg */",
            "\t\tif (!old)",
            "\t\t\treturn 0;",
            "",
            "\t\t/* Already expanded this shrinker_info */",
            "\t\tif (new_nr_max <= old->map_nr_max)",
            "\t\t\tcontinue;",
            "",
            "\t\tnew = kvzalloc_node(sizeof(*new) + new_size, GFP_KERNEL, nid);",
            "\t\tif (!new)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tnew->map_nr_max = new_nr_max;",
            "",
            "\t\tmemcpy(new->unit, old->unit, old_size);",
            "\t\tif (shrinker_unit_alloc(new, old, nid)) {",
            "\t\t\tkvfree(new);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "",
            "\t\trcu_assign_pointer(pn->shrinker_info, new);",
            "\t\tkvfree_rcu(old, rcu);",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "shrinker_unit_size, shrinker_unit_free, shrinker_unit_alloc, free_shrinker_info, alloc_shrinker_info, expand_one_shrinker_info",
          "description": "实现shrinker_info单元的动态内存管理，包含单位尺寸计算、单元释放/分配逻辑，以及mem_cgroup的shrinker_info结构体的创建与销毁操作。",
          "similarity": 0.5160813927650452
        },
        {
          "chunk_id": 5,
          "file_path": "mm/shrinker.c",
          "start_line": 587,
          "end_line": 724,
          "content": [
            "static unsigned long shrink_slab_memcg(gfp_t gfp_mask, int nid,",
            "\t\t\tstruct mem_cgroup *memcg, int priority)",
            "{",
            "\treturn 0;",
            "}",
            "unsigned long shrink_slab(gfp_t gfp_mask, int nid, struct mem_cgroup *memcg,",
            "\t\t\t  int priority)",
            "{",
            "\tunsigned long ret, freed = 0;",
            "\tstruct shrinker *shrinker;",
            "",
            "\t/*",
            "\t * The root memcg might be allocated even though memcg is disabled",
            "\t * via \"cgroup_disable=memory\" boot parameter.  This could make",
            "\t * mem_cgroup_is_root() return false, then just run memcg slab",
            "\t * shrink, but skip global shrink.  This may result in premature",
            "\t * oom.",
            "\t */",
            "\tif (!mem_cgroup_disabled() && !mem_cgroup_is_root(memcg))",
            "\t\treturn shrink_slab_memcg(gfp_mask, nid, memcg, priority);",
            "",
            "\t/*",
            "\t * lockless algorithm of global shrink.",
            "\t *",
            "\t * In the unregistration setp, the shrinker will be freed asynchronously",
            "\t * via RCU after its refcount reaches 0. So both rcu_read_lock() and",
            "\t * shrinker_try_get() can be used to ensure the existence of the shrinker.",
            "\t *",
            "\t * So in the global shrink:",
            "\t *  step 1: use rcu_read_lock() to guarantee existence of the shrinker",
            "\t *          and the validity of the shrinker_list walk.",
            "\t *  step 2: use shrinker_try_get() to try get the refcount, if successful,",
            "\t *          then the existence of the shrinker can also be guaranteed,",
            "\t *          so we can release the RCU lock to do do_shrink_slab() that",
            "\t *          may sleep.",
            "\t *  step 3: *MUST* to reacquire the RCU lock before calling shrinker_put(),",
            "\t *          which ensures that neither this shrinker nor the next shrinker",
            "\t *          will be freed in the next traversal operation.",
            "\t *  step 4: do shrinker_put() paired with step 2 to put the refcount,",
            "\t *          if the refcount reaches 0, then wake up the waiter in",
            "\t *          shrinker_free() by calling complete().",
            "\t */",
            "\trcu_read_lock();",
            "\tlist_for_each_entry_rcu(shrinker, &shrinker_list, list) {",
            "\t\tstruct shrink_control sc = {",
            "\t\t\t.gfp_mask = gfp_mask,",
            "\t\t\t.nid = nid,",
            "\t\t\t.memcg = memcg,",
            "\t\t};",
            "",
            "\t\tif (!shrinker_try_get(shrinker))",
            "\t\t\tcontinue;",
            "",
            "\t\trcu_read_unlock();",
            "",
            "\t\tret = do_shrink_slab(&sc, shrinker, priority);",
            "\t\tif (ret == SHRINK_EMPTY)",
            "\t\t\tret = 0;",
            "\t\tfreed += ret;",
            "",
            "\t\trcu_read_lock();",
            "\t\tshrinker_put(shrinker);",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "\tcond_resched();",
            "\treturn freed;",
            "}",
            "void shrinker_register(struct shrinker *shrinker)",
            "{",
            "\tif (unlikely(!(shrinker->flags & SHRINKER_ALLOCATED))) {",
            "\t\tpr_warn(\"Must use shrinker_alloc() to dynamically allocate the shrinker\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tmutex_lock(&shrinker_mutex);",
            "\tlist_add_tail_rcu(&shrinker->list, &shrinker_list);",
            "\tshrinker->flags |= SHRINKER_REGISTERED;",
            "\tshrinker_debugfs_add(shrinker);",
            "\tmutex_unlock(&shrinker_mutex);",
            "",
            "\tinit_completion(&shrinker->done);",
            "\t/*",
            "\t * Now the shrinker is fully set up, take the first reference to it to",
            "\t * indicate that lookup operations are now allowed to use it via",
            "\t * shrinker_try_get().",
            "\t */",
            "\trefcount_set(&shrinker->refcount, 1);",
            "}",
            "static void shrinker_free_rcu_cb(struct rcu_head *head)",
            "{",
            "\tstruct shrinker *shrinker = container_of(head, struct shrinker, rcu);",
            "",
            "\tkfree(shrinker->nr_deferred);",
            "\tkfree(shrinker);",
            "}",
            "void shrinker_free(struct shrinker *shrinker)",
            "{",
            "\tstruct dentry *debugfs_entry = NULL;",
            "\tint debugfs_id;",
            "",
            "\tif (!shrinker)",
            "\t\treturn;",
            "",
            "\tif (shrinker->flags & SHRINKER_REGISTERED) {",
            "\t\t/* drop the initial refcount */",
            "\t\tshrinker_put(shrinker);",
            "\t\t/*",
            "\t\t * Wait for all lookups of the shrinker to complete, after that,",
            "\t\t * no shrinker is running or will run again, then we can safely",
            "\t\t * free it asynchronously via RCU and safely free the structure",
            "\t\t * where the shrinker is located, such as super_block etc.",
            "\t\t */",
            "\t\twait_for_completion(&shrinker->done);",
            "\t}",
            "",
            "\tmutex_lock(&shrinker_mutex);",
            "\tif (shrinker->flags & SHRINKER_REGISTERED) {",
            "\t\t/*",
            "\t\t * Now we can safely remove it from the shrinker_list and then",
            "\t\t * free it.",
            "\t\t */",
            "\t\tlist_del_rcu(&shrinker->list);",
            "\t\tdebugfs_entry = shrinker_debugfs_detach(shrinker, &debugfs_id);",
            "\t\tshrinker->flags &= ~SHRINKER_REGISTERED;",
            "\t}",
            "",
            "\tshrinker_debugfs_name_free(shrinker);",
            "",
            "\tif (shrinker->flags & SHRINKER_MEMCG_AWARE)",
            "\t\tshrinker_memcg_remove(shrinker);",
            "\tmutex_unlock(&shrinker_mutex);",
            "",
            "\tif (debugfs_entry)",
            "\t\tshrinker_debugfs_remove(debugfs_entry, debugfs_id);",
            "",
            "\tcall_rcu(&shrinker->rcu, shrinker_free_rcu_cb);",
            "}"
          ],
          "function_name": "shrink_slab_memcg, shrink_slab, shrinker_register, shrinker_free_rcu_cb, shrinker_free",
          "description": "该代码实现内存收缩机制的核心管理逻辑，包含注册、遍历与清理缩放器（shrinker）的全流程。  \n`shrink_slab` 函数通过 RCU 锁机制安全遍历全局缩放器列表，按优先级回收 slab 缓存，支持内存组（memcg）隔离场景下的差异化回收。  \n`shrinker_register` 注册缩放器至全局链表，`shrinker_free` 安全移除并释放缩放器资源，采用 RCU 延迟销毁以避免并发竞争。  \n注：`shrink_slab_memcg` 未实现具体逻辑，上下文不完整。",
          "similarity": 0.5088087320327759
        },
        {
          "chunk_id": 4,
          "file_path": "mm/shrinker.c",
          "start_line": 469,
          "end_line": 585,
          "content": [
            "static unsigned long shrink_slab_memcg(gfp_t gfp_mask, int nid,",
            "\t\t\tstruct mem_cgroup *memcg, int priority)",
            "{",
            "\tstruct shrinker_info *info;",
            "\tunsigned long ret, freed = 0;",
            "\tint offset, index = 0;",
            "",
            "\tif (!mem_cgroup_online(memcg))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * lockless algorithm of memcg shrink.",
            "\t *",
            "\t * The shrinker_info may be freed asynchronously via RCU in the",
            "\t * expand_one_shrinker_info(), so the rcu_read_lock() needs to be used",
            "\t * to ensure the existence of the shrinker_info.",
            "\t *",
            "\t * The shrinker_info_unit is never freed unless its corresponding memcg",
            "\t * is destroyed. Here we already hold the refcount of memcg, so the",
            "\t * memcg will not be destroyed, and of course shrinker_info_unit will",
            "\t * not be freed.",
            "\t *",
            "\t * So in the memcg shrink:",
            "\t *  step 1: use rcu_read_lock() to guarantee existence of the",
            "\t *          shrinker_info.",
            "\t *  step 2: after getting shrinker_info_unit we can safely release the",
            "\t *          RCU lock.",
            "\t *  step 3: traverse the bitmap and calculate shrinker_id",
            "\t *  step 4: use rcu_read_lock() to guarantee existence of the shrinker.",
            "\t *  step 5: use shrinker_id to find the shrinker, then use",
            "\t *          shrinker_try_get() to guarantee existence of the shrinker,",
            "\t *          then we can release the RCU lock to do do_shrink_slab() that",
            "\t *          may sleep.",
            "\t *  step 6: do shrinker_put() paired with step 5 to put the refcount,",
            "\t *          if the refcount reaches 0, then wake up the waiter in",
            "\t *          shrinker_free() by calling complete().",
            "\t *          Note: here is different from the global shrink, we don't",
            "\t *                need to acquire the RCU lock to guarantee existence of",
            "\t *                the shrinker, because we don't need to use this",
            "\t *                shrinker to traverse the next shrinker in the bitmap.",
            "\t *  step 7: we have already exited the read-side of rcu critical section",
            "\t *          before calling do_shrink_slab(), the shrinker_info may be",
            "\t *          released in expand_one_shrinker_info(), so go back to step 1",
            "\t *          to reacquire the shrinker_info.",
            "\t */",
            "again:",
            "\trcu_read_lock();",
            "\tinfo = rcu_dereference(memcg->nodeinfo[nid]->shrinker_info);",
            "\tif (unlikely(!info))",
            "\t\tgoto unlock;",
            "",
            "\tif (index < shrinker_id_to_index(info->map_nr_max)) {",
            "\t\tstruct shrinker_info_unit *unit;",
            "",
            "\t\tunit = info->unit[index];",
            "",
            "\t\trcu_read_unlock();",
            "",
            "\t\tfor_each_set_bit(offset, unit->map, SHRINKER_UNIT_BITS) {",
            "\t\t\tstruct shrink_control sc = {",
            "\t\t\t\t.gfp_mask = gfp_mask,",
            "\t\t\t\t.nid = nid,",
            "\t\t\t\t.memcg = memcg,",
            "\t\t\t};",
            "\t\t\tstruct shrinker *shrinker;",
            "\t\t\tint shrinker_id = calc_shrinker_id(index, offset);",
            "",
            "\t\t\trcu_read_lock();",
            "\t\t\tshrinker = idr_find(&shrinker_idr, shrinker_id);",
            "\t\t\tif (unlikely(!shrinker || !shrinker_try_get(shrinker))) {",
            "\t\t\t\tclear_bit(offset, unit->map);",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\trcu_read_unlock();",
            "",
            "\t\t\t/* Call non-slab shrinkers even though kmem is disabled */",
            "\t\t\tif (!memcg_kmem_online() &&",
            "\t\t\t    !(shrinker->flags & SHRINKER_NONSLAB))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tret = do_shrink_slab(&sc, shrinker, priority);",
            "\t\t\tif (ret == SHRINK_EMPTY) {",
            "\t\t\t\tclear_bit(offset, unit->map);",
            "\t\t\t\t/*",
            "\t\t\t\t * After the shrinker reported that it had no objects to",
            "\t\t\t\t * free, but before we cleared the corresponding bit in",
            "\t\t\t\t * the memcg shrinker map, a new object might have been",
            "\t\t\t\t * added. To make sure, we have the bit set in this",
            "\t\t\t\t * case, we invoke the shrinker one more time and reset",
            "\t\t\t\t * the bit if it reports that it is not empty anymore.",
            "\t\t\t\t * The memory barrier here pairs with the barrier in",
            "\t\t\t\t * set_shrinker_bit():",
            "\t\t\t\t *",
            "\t\t\t\t * list_lru_add()     shrink_slab_memcg()",
            "\t\t\t\t *   list_add_tail()    clear_bit()",
            "\t\t\t\t *   <MB>               <MB>",
            "\t\t\t\t *   set_bit()          do_shrink_slab()",
            "\t\t\t\t */",
            "\t\t\t\tsmp_mb__after_atomic();",
            "\t\t\t\tret = do_shrink_slab(&sc, shrinker, priority);",
            "\t\t\t\tif (ret == SHRINK_EMPTY)",
            "\t\t\t\t\tret = 0;",
            "\t\t\t\telse",
            "\t\t\t\t\tset_shrinker_bit(memcg, nid, shrinker_id);",
            "\t\t\t}",
            "\t\t\tfreed += ret;",
            "\t\t\tshrinker_put(shrinker);",
            "\t\t}",
            "",
            "\t\tindex++;",
            "\t\tgoto again;",
            "\t}",
            "unlock:",
            "\trcu_read_unlock();",
            "\treturn freed;",
            "}"
          ],
          "function_name": "shrink_slab_memcg",
          "description": "实现内存控制组级别的slab回收逻辑，通过RCU读锁保护访问缩小器信息表，遍历所有注册缩小器并执行相应的对象回收操作。",
          "similarity": 0.4933883547782898
        },
        {
          "chunk_id": 3,
          "file_path": "mm/shrinker.c",
          "start_line": 268,
          "end_line": 454,
          "content": [
            "static long add_nr_deferred_memcg(long nr, int nid, struct shrinker *shrinker,",
            "\t\t\t\t  struct mem_cgroup *memcg)",
            "{",
            "\tstruct shrinker_info *info;",
            "\tstruct shrinker_info_unit *unit;",
            "\tlong nr_deferred;",
            "",
            "\trcu_read_lock();",
            "\tinfo = rcu_dereference(memcg->nodeinfo[nid]->shrinker_info);",
            "\tunit = info->unit[shrinker_id_to_index(shrinker->id)];",
            "\tnr_deferred =",
            "\t\tatomic_long_add_return(nr, &unit->nr_deferred[shrinker_id_to_offset(shrinker->id)]);",
            "\trcu_read_unlock();",
            "",
            "\treturn nr_deferred;",
            "}",
            "void reparent_shrinker_deferred(struct mem_cgroup *memcg)",
            "{",
            "\tint nid, index, offset;",
            "\tlong nr;",
            "\tstruct mem_cgroup *parent;",
            "\tstruct shrinker_info *child_info, *parent_info;",
            "\tstruct shrinker_info_unit *child_unit, *parent_unit;",
            "",
            "\tparent = parent_mem_cgroup(memcg);",
            "\tif (!parent)",
            "\t\tparent = root_mem_cgroup;",
            "",
            "\t/* Prevent from concurrent shrinker_info expand */",
            "\tmutex_lock(&shrinker_mutex);",
            "\tfor_each_node(nid) {",
            "\t\tchild_info = shrinker_info_protected(memcg, nid);",
            "\t\tparent_info = shrinker_info_protected(parent, nid);",
            "\t\tfor (index = 0; index < shrinker_id_to_index(child_info->map_nr_max); index++) {",
            "\t\t\tchild_unit = child_info->unit[index];",
            "\t\t\tparent_unit = parent_info->unit[index];",
            "\t\t\tfor (offset = 0; offset < SHRINKER_UNIT_BITS; offset++) {",
            "\t\t\t\tnr = atomic_long_read(&child_unit->nr_deferred[offset]);",
            "\t\t\t\tatomic_long_add(nr, &parent_unit->nr_deferred[offset]);",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&shrinker_mutex);",
            "}",
            "static int shrinker_memcg_alloc(struct shrinker *shrinker)",
            "{",
            "\treturn -ENOSYS;",
            "}",
            "static void shrinker_memcg_remove(struct shrinker *shrinker)",
            "{",
            "}",
            "static long xchg_nr_deferred_memcg(int nid, struct shrinker *shrinker,",
            "\t\t\t\t   struct mem_cgroup *memcg)",
            "{",
            "\treturn 0;",
            "}",
            "static long add_nr_deferred_memcg(long nr, int nid, struct shrinker *shrinker,",
            "\t\t\t\t  struct mem_cgroup *memcg)",
            "{",
            "\treturn 0;",
            "}",
            "static long xchg_nr_deferred(struct shrinker *shrinker,",
            "\t\t\t     struct shrink_control *sc)",
            "{",
            "\tint nid = sc->nid;",
            "",
            "\tif (!(shrinker->flags & SHRINKER_NUMA_AWARE))",
            "\t\tnid = 0;",
            "",
            "\tif (sc->memcg &&",
            "\t    (shrinker->flags & SHRINKER_MEMCG_AWARE))",
            "\t\treturn xchg_nr_deferred_memcg(nid, shrinker,",
            "\t\t\t\t\t      sc->memcg);",
            "",
            "\treturn atomic_long_xchg(&shrinker->nr_deferred[nid], 0);",
            "}",
            "static long add_nr_deferred(long nr, struct shrinker *shrinker,",
            "\t\t\t    struct shrink_control *sc)",
            "{",
            "\tint nid = sc->nid;",
            "",
            "\tif (!(shrinker->flags & SHRINKER_NUMA_AWARE))",
            "\t\tnid = 0;",
            "",
            "\tif (sc->memcg &&",
            "\t    (shrinker->flags & SHRINKER_MEMCG_AWARE))",
            "\t\treturn add_nr_deferred_memcg(nr, nid, shrinker,",
            "\t\t\t\t\t     sc->memcg);",
            "",
            "\treturn atomic_long_add_return(nr, &shrinker->nr_deferred[nid]);",
            "}",
            "static unsigned long do_shrink_slab(struct shrink_control *shrinkctl,",
            "\t\t\t\t    struct shrinker *shrinker, int priority)",
            "{",
            "\tunsigned long freed = 0;",
            "\tunsigned long long delta;",
            "\tlong total_scan;",
            "\tlong freeable;",
            "\tlong nr;",
            "\tlong new_nr;",
            "\tlong batch_size = shrinker->batch ? shrinker->batch",
            "\t\t\t\t\t  : SHRINK_BATCH;",
            "\tlong scanned = 0, next_deferred;",
            "",
            "\tfreeable = shrinker->count_objects(shrinker, shrinkctl);",
            "\tif (freeable == 0 || freeable == SHRINK_EMPTY)",
            "\t\treturn freeable;",
            "",
            "\t/*",
            "\t * copy the current shrinker scan count into a local variable",
            "\t * and zero it so that other concurrent shrinker invocations",
            "\t * don't also do this scanning work.",
            "\t */",
            "\tnr = xchg_nr_deferred(shrinker, shrinkctl);",
            "",
            "\tif (shrinker->seeks) {",
            "\t\tdelta = freeable >> priority;",
            "\t\tdelta *= 4;",
            "\t\tdo_div(delta, shrinker->seeks);",
            "\t} else {",
            "\t\t/*",
            "\t\t * These objects don't require any IO to create. Trim",
            "\t\t * them aggressively under memory pressure to keep",
            "\t\t * them from causing refetches in the IO caches.",
            "\t\t */",
            "\t\tdelta = freeable / 2;",
            "\t}",
            "",
            "\ttotal_scan = nr >> priority;",
            "\ttotal_scan += delta;",
            "\ttotal_scan = min(total_scan, (2 * freeable));",
            "",
            "\ttrace_mm_shrink_slab_start(shrinker, shrinkctl, nr,",
            "\t\t\t\t   freeable, delta, total_scan, priority);",
            "",
            "\t/*",
            "\t * Normally, we should not scan less than batch_size objects in one",
            "\t * pass to avoid too frequent shrinker calls, but if the slab has less",
            "\t * than batch_size objects in total and we are really tight on memory,",
            "\t * we will try to reclaim all available objects, otherwise we can end",
            "\t * up failing allocations although there are plenty of reclaimable",
            "\t * objects spread over several slabs with usage less than the",
            "\t * batch_size.",
            "\t *",
            "\t * We detect the \"tight on memory\" situations by looking at the total",
            "\t * number of objects we want to scan (total_scan). If it is greater",
            "\t * than the total number of objects on slab (freeable), we must be",
            "\t * scanning at high prio and therefore should try to reclaim as much as",
            "\t * possible.",
            "\t */",
            "\twhile (total_scan >= batch_size ||",
            "\t       total_scan >= freeable) {",
            "\t\tunsigned long ret;",
            "\t\tunsigned long nr_to_scan = min(batch_size, total_scan);",
            "",
            "\t\tshrinkctl->nr_to_scan = nr_to_scan;",
            "\t\tshrinkctl->nr_scanned = nr_to_scan;",
            "\t\tret = shrinker->scan_objects(shrinker, shrinkctl);",
            "\t\tif (ret == SHRINK_STOP)",
            "\t\t\tbreak;",
            "\t\tfreed += ret;",
            "",
            "\t\tcount_vm_events(SLABS_SCANNED, shrinkctl->nr_scanned);",
            "\t\ttotal_scan -= shrinkctl->nr_scanned;",
            "\t\tscanned += shrinkctl->nr_scanned;",
            "",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\t/*",
            "\t * The deferred work is increased by any new work (delta) that wasn't",
            "\t * done, decreased by old deferred work that was done now.",
            "\t *",
            "\t * And it is capped to two times of the freeable items.",
            "\t */",
            "\tnext_deferred = max_t(long, (nr + delta - scanned), 0);",
            "\tnext_deferred = min(next_deferred, (2 * freeable));",
            "",
            "\t/*",
            "\t * move the unused scan count back into the shrinker in a",
            "\t * manner that handles concurrent updates.",
            "\t */",
            "\tnew_nr = add_nr_deferred(next_deferred, shrinker, shrinkctl);",
            "",
            "\ttrace_mm_shrink_slab_end(shrinker, shrinkctl->nid, freed, nr, new_nr, total_scan);",
            "\treturn freed;",
            "}"
          ],
          "function_name": "add_nr_deferred_memcg, reparent_shrinker_deferred, shrinker_memcg_alloc, shrinker_memcg_remove, xchg_nr_deferred_memcg, add_nr_deferred_memcg, xchg_nr_deferred, add_nr_deferred, do_shrink_slab",
          "description": "实现延迟扫描计数管理，包含跨内存控制组的缩减器数据迁移（reparent_shrinker_deferred），以及执行实际对象回收的do_shrink_slab函数。",
          "similarity": 0.4876977503299713
        }
      ]
    },
    {
      "source_file": "kernel/printk/printk_ringbuffer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:34:17\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `printk\\printk_ringbuffer.c`\n\n---\n\n# printk_ringbuffer.c 技术文档\n\n## 文件概述\n\n`printk_ringbuffer.c` 实现了 Linux 内核中用于日志记录的无锁环形缓冲区（printk ringbuffer）核心逻辑。该缓冲区用于高效、并发安全地存储内核日志消息（printk 输出），支持多写者-多读者模型，无需使用传统锁机制，从而在高并发或中断上下文中也能安全使用。该实现是现代 printk 子系统的基础组件，用于替代旧的 log_buf。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`printk_ringbuffer`**：顶层环形缓冲区结构，包含三个内部环形缓冲区：\n  - **`desc_ring`**：描述符环，存储每条日志记录的元数据（序列号、时间戳、日志级别、状态等）及指向文本数据的逻辑位置。\n  - **`text_data_ring`**：文本数据环，以字节为单位存储日志文本内容，每个数据块以描述符 ID 开头，后接实际文本。\n  - **`info` 数组**：与描述符一一对应的 `printk_info` 结构数组，存储日志记录的详细元数据。\n\n- **描述符状态（`state_var`）**：\n  - `reserved`：写者正在修改记录。\n  - `committed`：记录已提交，数据一致，但可被原写者重新打开修改。\n  - `finalized`：记录已最终确定，对读者可见，不可再修改。\n  - `reusable`：记录可被回收复用。\n  - `miss`（伪状态）：查询时发现描述符 ID 不匹配。\n\n- **`blk_lpos`**：逻辑位置结构，用于在数据环中定位数据块的起始和结束位置。\n\n### 主要函数（接口）\n\n- `prb_reserve()`：为新日志记录预留空间，返回保留条目。\n- `prb_commit()`：提交当前记录（可后续重新打开）。\n- `prb_final_commit()`：提交并最终确定记录，使其对读者可见。\n- `prb_read_valid()` / `prb_read_valid_info()`：安全读取指定序列号的日志记录及其元数据。\n- `prb_first_valid_seq()` / `prb_next_seq()`：获取有效日志序列范围。\n\n## 关键实现\n\n### 无锁同步机制\n\n通过原子操作更新描述符的 `state_var` 字段（将 ID 与状态位打包），实现写者与读者之间的无锁同步。状态转换遵循严格顺序：`reserved → committed → finalized → reusable`。\n\n### 描述符生命周期管理\n\n- **预留（Reserve）**：分配新描述符，状态设为 `reserved`。\n- **提交（Commit）**：写入完成后设为 `committed`，数据一致但可重入。\n- **最终确定（Finalize）**：在以下任一情况下自动或显式触发：\n  1. 调用 `prb_final_commit()`；\n  2. 下一条记录被预留且当前记录已 `committed`；\n  3. 提交一条记录时已有更新记录存在。\n- **回收（Reuse）**：缓冲区满时，将最旧的 `finalized` 或 `reusable` 记录状态转为 `reusable`，并推进 `tail_id`。\n\n### 数据环的环绕处理\n\n当日志文本跨越缓冲区末尾时，仅在末尾存储描述符 ID，完整数据块（ID + 文本）从缓冲区起始位置存储。`blk_lpos` 正确指向环绕前的 ID 位置，保证逻辑连续性。\n\n### 尾部推进安全约束\n\n`tail_id` 和 `tail_lpos` 仅在对应记录处于 `committed` 或 `reusable` 状态时才可推进，确保始终保留至少一条有效日志的序列号，避免读者读取到无效数据。\n\n### 元数据一致性保障\n\n读取 `printk_info` 时，需在读取前后两次检查对应描述符状态，确保元数据未在读取过程中被覆盖或修改（ABA 问题防护）。\n\n## 依赖关系\n\n- **内部依赖**：\n  - `printk_ringbuffer.h`：定义核心数据结构和 API。\n  - `internal.h`：包含 printk 子系统内部辅助函数和定义。\n- **内核头文件**：\n  - `<linux/kernel.h>`、`<linux/irqflags.h>`、`<linux/string.h>`、`<linux/bug.h>`：提供基础内核功能、原子操作、内存操作及调试支持。\n- **被 printk.c 调用**：作为 printk 日志后端，由 `printk.c` 中的 `vprintk_store()` 等函数调用其预留/提交接口。\n\n## 使用场景\n\n- **内核日志记录**：所有 `printk()` 调用最终通过此环形缓冲区存储日志消息。\n- **高并发环境**：在中断上下文、NMI、SMP 系统中安全记录日志，无需睡眠或持有自旋锁。\n- **日志读取**：`/dev/kmsg`、`dmesg` 命令及内核日志守护进程通过此缓冲区读取日志。\n- **崩溃转储**：在系统崩溃（如 panic）时，确保关键日志能被可靠记录和后续分析。\n- **动态日志扩展**：支持在提交后、最终确定前扩展日志内容（如追加堆栈信息），适用于延迟格式化场景。",
      "similarity": 0.5899805426597595,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 1006,
          "end_line": 1134,
          "content": [
            "static unsigned long get_next_lpos(struct prb_data_ring *data_ring,",
            "\t\t\t\t   unsigned long lpos, unsigned int size)",
            "{",
            "\tunsigned long begin_lpos;",
            "\tunsigned long next_lpos;",
            "",
            "\tbegin_lpos = lpos;",
            "\tnext_lpos = lpos + size;",
            "",
            "\t/* First check if the data block does not wrap. */",
            "\tif (DATA_WRAPS(data_ring, begin_lpos) == DATA_WRAPS(data_ring, next_lpos))",
            "\t\treturn next_lpos;",
            "",
            "\t/* Wrapping data blocks store their data at the beginning. */",
            "\treturn (DATA_THIS_WRAP_START_LPOS(data_ring, next_lpos) + size);",
            "}",
            "static unsigned int space_used(struct prb_data_ring *data_ring,",
            "\t\t\t       struct prb_data_blk_lpos *blk_lpos)",
            "{",
            "\t/* Data-less blocks take no space. */",
            "\tif (BLK_DATALESS(blk_lpos))",
            "\t\treturn 0;",
            "",
            "\tif (DATA_WRAPS(data_ring, blk_lpos->begin) == DATA_WRAPS(data_ring, blk_lpos->next)) {",
            "\t\t/* Data block does not wrap. */",
            "\t\treturn (DATA_INDEX(data_ring, blk_lpos->next) -",
            "\t\t\tDATA_INDEX(data_ring, blk_lpos->begin));",
            "\t}",
            "",
            "\t/*",
            "\t * For wrapping data blocks, the trailing (wasted) space is",
            "\t * also counted.",
            "\t */",
            "\treturn (DATA_INDEX(data_ring, blk_lpos->next) +",
            "\t\tDATA_SIZE(data_ring) - DATA_INDEX(data_ring, blk_lpos->begin));",
            "}",
            "bool prb_reserve_in_last(struct prb_reserved_entry *e, struct printk_ringbuffer *rb,",
            "\t\t\t struct printk_record *r, u32 caller_id, unsigned int max_size)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tstruct printk_info *info;",
            "\tunsigned int data_size;",
            "\tstruct prb_desc *d;",
            "\tunsigned long id;",
            "",
            "\tlocal_irq_save(e->irqflags);",
            "",
            "\t/* Transition the newest descriptor back to the reserved state. */",
            "\td = desc_reopen_last(desc_ring, caller_id, &id);",
            "\tif (!d) {",
            "\t\tlocal_irq_restore(e->irqflags);",
            "\t\tgoto fail_reopen;",
            "\t}",
            "",
            "\t/* Now the writer has exclusive access: LMM(prb_reserve_in_last:A) */",
            "",
            "\tinfo = to_info(desc_ring, id);",
            "",
            "\t/*",
            "\t * Set the @e fields here so that prb_commit() can be used if",
            "\t * anything fails from now on.",
            "\t */",
            "\te->rb = rb;",
            "\te->id = id;",
            "",
            "\t/*",
            "\t * desc_reopen_last() checked the caller_id, but there was no",
            "\t * exclusive access at that point. The descriptor may have",
            "\t * changed since then.",
            "\t */",
            "\tif (caller_id != info->caller_id)",
            "\t\tgoto fail;",
            "",
            "\tif (BLK_DATALESS(&d->text_blk_lpos)) {",
            "\t\tif (WARN_ON_ONCE(info->text_len != 0)) {",
            "\t\t\tpr_warn_once(\"wrong text_len value (%hu, expecting 0)\\n\",",
            "\t\t\t\t     info->text_len);",
            "\t\t\tinfo->text_len = 0;",
            "\t\t}",
            "",
            "\t\tif (!data_check_size(&rb->text_data_ring, r->text_buf_size))",
            "\t\t\tgoto fail;",
            "",
            "\t\tif (r->text_buf_size > max_size)",
            "\t\t\tgoto fail;",
            "",
            "\t\tr->text_buf = data_alloc(rb, r->text_buf_size,",
            "\t\t\t\t\t &d->text_blk_lpos, id);",
            "\t} else {",
            "\t\tif (!get_data(&rb->text_data_ring, &d->text_blk_lpos, &data_size))",
            "\t\t\tgoto fail;",
            "",
            "\t\t/*",
            "\t\t * Increase the buffer size to include the original size. If",
            "\t\t * the meta data (@text_len) is not sane, use the full data",
            "\t\t * block size.",
            "\t\t */",
            "\t\tif (WARN_ON_ONCE(info->text_len > data_size)) {",
            "\t\t\tpr_warn_once(\"wrong text_len value (%hu, expecting <=%u)\\n\",",
            "\t\t\t\t     info->text_len, data_size);",
            "\t\t\tinfo->text_len = data_size;",
            "\t\t}",
            "\t\tr->text_buf_size += info->text_len;",
            "",
            "\t\tif (!data_check_size(&rb->text_data_ring, r->text_buf_size))",
            "\t\t\tgoto fail;",
            "",
            "\t\tif (r->text_buf_size > max_size)",
            "\t\t\tgoto fail;",
            "",
            "\t\tr->text_buf = data_realloc(rb, r->text_buf_size,",
            "\t\t\t\t\t   &d->text_blk_lpos, id);",
            "\t}",
            "\tif (r->text_buf_size && !r->text_buf)",
            "\t\tgoto fail;",
            "",
            "\tr->info = info;",
            "",
            "\te->text_space = space_used(&rb->text_data_ring, &d->text_blk_lpos);",
            "",
            "\treturn true;",
            "fail:",
            "\tprb_commit(e);",
            "\t/* prb_commit() re-enabled interrupts. */",
            "fail_reopen:",
            "\t/* Make it clear to the caller that the re-reserve failed. */",
            "\tmemset(r, 0, sizeof(*r));",
            "\treturn false;",
            "}"
          ],
          "function_name": "get_next_lpos, space_used, prb_reserve_in_last",
          "description": "实现用于计算数据环形缓冲区中下一个逻辑位置、已用空间及预留数据块的操作。get_next_lpos确定数据块结束位置，space_used计算数据块占用空间，prb_reserve_in_last为记录预留数据空间并处理分配失败情况。",
          "similarity": 0.559526264667511
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 1477,
          "end_line": 1647,
          "content": [
            "static u64 desc_last_finalized_seq(struct printk_ringbuffer *rb)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tunsigned long ulseq;",
            "",
            "\t/*",
            "\t * Guarantee the sequence number is loaded before loading the",
            "\t * associated record in order to guarantee that the record can be",
            "\t * seen by this CPU. This pairs with desc_update_last_finalized:A.",
            "\t */",
            "\tulseq = atomic_long_read_acquire(&desc_ring->last_finalized_seq",
            "\t\t\t\t\t); /* LMM(desc_last_finalized_seq:A) */",
            "",
            "\treturn __ulseq_to_u64seq(rb, ulseq);",
            "}",
            "static void desc_update_last_finalized(struct printk_ringbuffer *rb)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tu64 old_seq = desc_last_finalized_seq(rb);",
            "\tunsigned long oldval;",
            "\tunsigned long newval;",
            "\tu64 finalized_seq;",
            "\tu64 try_seq;",
            "",
            "try_again:",
            "\tfinalized_seq = old_seq;",
            "\ttry_seq = finalized_seq + 1;",
            "",
            "\t/* Try to find later finalized records. */",
            "\twhile (_prb_read_valid(rb, &try_seq, NULL, NULL)) {",
            "\t\tfinalized_seq = try_seq;",
            "\t\ttry_seq++;",
            "\t}",
            "",
            "\t/* No update needed if no later finalized record was found. */",
            "\tif (finalized_seq == old_seq)",
            "\t\treturn;",
            "",
            "\toldval = __u64seq_to_ulseq(old_seq);",
            "\tnewval = __u64seq_to_ulseq(finalized_seq);",
            "",
            "\t/*",
            "\t * Set the sequence number of a later finalized record that has been",
            "\t * seen.",
            "\t *",
            "\t * Guarantee the record data is visible to other CPUs before storing",
            "\t * its sequence number. This pairs with desc_last_finalized_seq:A.",
            "\t *",
            "\t * Memory barrier involvement:",
            "\t *",
            "\t * If desc_last_finalized_seq:A reads from",
            "\t * desc_update_last_finalized:A, then desc_read:A reads from",
            "\t * _prb_commit:B.",
            "\t *",
            "\t * Relies on:",
            "\t *",
            "\t * RELEASE from _prb_commit:B to desc_update_last_finalized:A",
            "\t *    matching",
            "\t * ACQUIRE from desc_last_finalized_seq:A to desc_read:A",
            "\t *",
            "\t * Note: _prb_commit:B and desc_update_last_finalized:A can be",
            "\t *       different CPUs. However, the desc_update_last_finalized:A",
            "\t *       CPU (which performs the release) must have previously seen",
            "\t *       _prb_commit:B.",
            "\t */",
            "\tif (!atomic_long_try_cmpxchg_release(&desc_ring->last_finalized_seq,",
            "\t\t\t\t&oldval, newval)) { /* LMM(desc_update_last_finalized:A) */",
            "\t\told_seq = __ulseq_to_u64seq(rb, oldval);",
            "\t\tgoto try_again;",
            "\t}",
            "}",
            "static void desc_make_final(struct printk_ringbuffer *rb, unsigned long id)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tunsigned long prev_state_val = DESC_SV(id, desc_committed);",
            "\tstruct prb_desc *d = to_desc(desc_ring, id);",
            "",
            "\tif (atomic_long_try_cmpxchg_relaxed(&d->state_var, &prev_state_val,",
            "\t\t\tDESC_SV(id, desc_finalized))) { /* LMM(desc_make_final:A) */",
            "\t\tdesc_update_last_finalized(rb);",
            "\t}",
            "}",
            "bool prb_reserve(struct prb_reserved_entry *e, struct printk_ringbuffer *rb,",
            "\t\t struct printk_record *r)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tstruct printk_info *info;",
            "\tstruct prb_desc *d;",
            "\tunsigned long id;",
            "\tu64 seq;",
            "",
            "\tif (!data_check_size(&rb->text_data_ring, r->text_buf_size))",
            "\t\tgoto fail;",
            "",
            "\t/*",
            "\t * Descriptors in the reserved state act as blockers to all further",
            "\t * reservations once the desc_ring has fully wrapped. Disable",
            "\t * interrupts during the reserve/commit window in order to minimize",
            "\t * the likelihood of this happening.",
            "\t */",
            "\tlocal_irq_save(e->irqflags);",
            "",
            "\tif (!desc_reserve(rb, &id)) {",
            "\t\t/* Descriptor reservation failures are tracked. */",
            "\t\tatomic_long_inc(&rb->fail);",
            "\t\tlocal_irq_restore(e->irqflags);",
            "\t\tgoto fail;",
            "\t}",
            "",
            "\td = to_desc(desc_ring, id);",
            "\tinfo = to_info(desc_ring, id);",
            "",
            "\t/*",
            "\t * All @info fields (except @seq) are cleared and must be filled in",
            "\t * by the writer. Save @seq before clearing because it is used to",
            "\t * determine the new sequence number.",
            "\t */",
            "\tseq = info->seq;",
            "\tmemset(info, 0, sizeof(*info));",
            "",
            "\t/*",
            "\t * Set the @e fields here so that prb_commit() can be used if",
            "\t * text data allocation fails.",
            "\t */",
            "\te->rb = rb;",
            "\te->id = id;",
            "",
            "\t/*",
            "\t * Initialize the sequence number if it has \"never been set\".",
            "\t * Otherwise just increment it by a full wrap.",
            "\t *",
            "\t * @seq is considered \"never been set\" if it has a value of 0,",
            "\t * _except_ for @infos[0], which was specially setup by the ringbuffer",
            "\t * initializer and therefore is always considered as set.",
            "\t *",
            "\t * See the \"Bootstrap\" comment block in printk_ringbuffer.h for",
            "\t * details about how the initializer bootstraps the descriptors.",
            "\t */",
            "\tif (seq == 0 && DESC_INDEX(desc_ring, id) != 0)",
            "\t\tinfo->seq = DESC_INDEX(desc_ring, id);",
            "\telse",
            "\t\tinfo->seq = seq + DESCS_COUNT(desc_ring);",
            "",
            "\t/*",
            "\t * New data is about to be reserved. Once that happens, previous",
            "\t * descriptors are no longer able to be extended. Finalize the",
            "\t * previous descriptor now so that it can be made available to",
            "\t * readers. (For seq==0 there is no previous descriptor.)",
            "\t */",
            "\tif (info->seq > 0)",
            "\t\tdesc_make_final(rb, DESC_ID(id - 1));",
            "",
            "\tr->text_buf = data_alloc(rb, r->text_buf_size, &d->text_blk_lpos, id);",
            "\t/* If text data allocation fails, a data-less record is committed. */",
            "\tif (r->text_buf_size && !r->text_buf) {",
            "\t\tprb_commit(e);",
            "\t\t/* prb_commit() re-enabled interrupts. */",
            "\t\tgoto fail;",
            "\t}",
            "",
            "\tr->info = info;",
            "",
            "\t/* Record full text space used by record. */",
            "\te->text_space = space_used(&rb->text_data_ring, &d->text_blk_lpos);",
            "",
            "\treturn true;",
            "fail:",
            "\t/* Make it clear to the caller that the reserve failed. */",
            "\tmemset(r, 0, sizeof(*r));",
            "\treturn false;",
            "}"
          ],
          "function_name": "desc_last_finalized_seq, desc_update_last_finalized, desc_make_final, prb_reserve",
          "description": "管理描述符状态转换与序列号同步。desc_last_finalized_seq获取最后确认的序列号，desc_update_last_finalized更新该序列号以保证内存顺序，prb_reserve预留新记录并初始化描述符字段。",
          "similarity": 0.5380034446716309
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 879,
          "end_line": 1003,
          "content": [
            "static bool desc_reserve(struct printk_ringbuffer *rb, unsigned long *id_out)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tunsigned long prev_state_val;",
            "\tunsigned long id_prev_wrap;",
            "\tstruct prb_desc *desc;",
            "\tunsigned long head_id;",
            "\tunsigned long id;",
            "",
            "\thead_id = atomic_long_read(&desc_ring->head_id); /* LMM(desc_reserve:A) */",
            "",
            "\tdo {",
            "\t\tid = DESC_ID(head_id + 1);",
            "\t\tid_prev_wrap = DESC_ID_PREV_WRAP(desc_ring, id);",
            "",
            "\t\t/*",
            "\t\t * Guarantee the head ID is read before reading the tail ID.",
            "\t\t * Since the tail ID is updated before the head ID, this",
            "\t\t * guarantees that @id_prev_wrap is never ahead of the tail",
            "\t\t * ID. This pairs with desc_reserve:D.",
            "\t\t *",
            "\t\t * Memory barrier involvement:",
            "\t\t *",
            "\t\t * If desc_reserve:A reads from desc_reserve:D, then",
            "\t\t * desc_reserve:C reads from desc_push_tail:B.",
            "\t\t *",
            "\t\t * Relies on:",
            "\t\t *",
            "\t\t * MB from desc_push_tail:B to desc_reserve:D",
            "\t\t *    matching",
            "\t\t * RMB from desc_reserve:A to desc_reserve:C",
            "\t\t *",
            "\t\t * Note: desc_push_tail:B and desc_reserve:D can be different",
            "\t\t *       CPUs. However, the desc_reserve:D CPU (which performs",
            "\t\t *       the full memory barrier) must have previously seen",
            "\t\t *       desc_push_tail:B.",
            "\t\t */",
            "\t\tsmp_rmb(); /* LMM(desc_reserve:B) */",
            "",
            "\t\tif (id_prev_wrap == atomic_long_read(&desc_ring->tail_id",
            "\t\t\t\t\t\t    )) { /* LMM(desc_reserve:C) */",
            "\t\t\t/*",
            "\t\t\t * Make space for the new descriptor by",
            "\t\t\t * advancing the tail.",
            "\t\t\t */",
            "\t\t\tif (!desc_push_tail(rb, id_prev_wrap))",
            "\t\t\t\treturn false;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * 1. Guarantee the tail ID is read before validating the",
            "\t\t *    recycled descriptor state. A read memory barrier is",
            "\t\t *    sufficient for this. This pairs with desc_push_tail:B.",
            "\t\t *",
            "\t\t *    Memory barrier involvement:",
            "\t\t *",
            "\t\t *    If desc_reserve:C reads from desc_push_tail:B, then",
            "\t\t *    desc_reserve:E reads from desc_make_reusable:A.",
            "\t\t *",
            "\t\t *    Relies on:",
            "\t\t *",
            "\t\t *    MB from desc_make_reusable:A to desc_push_tail:B",
            "\t\t *       matching",
            "\t\t *    RMB from desc_reserve:C to desc_reserve:E",
            "\t\t *",
            "\t\t *    Note: desc_make_reusable:A and desc_push_tail:B can be",
            "\t\t *          different CPUs. However, the desc_push_tail:B CPU",
            "\t\t *          (which performs the full memory barrier) must have",
            "\t\t *          previously seen desc_make_reusable:A.",
            "\t\t *",
            "\t\t * 2. Guarantee the tail ID is stored before storing the head",
            "\t\t *    ID. This pairs with desc_reserve:B.",
            "\t\t *",
            "\t\t * 3. Guarantee any data ring tail changes are stored before",
            "\t\t *    recycling the descriptor. Data ring tail changes can",
            "\t\t *    happen via desc_push_tail()->data_push_tail(). A full",
            "\t\t *    memory barrier is needed since another CPU may have",
            "\t\t *    pushed the data ring tails. This pairs with",
            "\t\t *    data_push_tail:B.",
            "\t\t *",
            "\t\t * 4. Guarantee a new tail ID is stored before recycling the",
            "\t\t *    descriptor. A full memory barrier is needed since",
            "\t\t *    another CPU may have pushed the tail ID. This pairs",
            "\t\t *    with desc_push_tail:C and this also pairs with",
            "\t\t *    prb_first_seq:C.",
            "\t\t *",
            "\t\t * 5. Guarantee the head ID is stored before trying to",
            "\t\t *    finalize the previous descriptor. This pairs with",
            "\t\t *    _prb_commit:B.",
            "\t\t */",
            "\t} while (!atomic_long_try_cmpxchg(&desc_ring->head_id, &head_id,",
            "\t\t\t\t\t  id)); /* LMM(desc_reserve:D) */",
            "",
            "\tdesc = to_desc(desc_ring, id);",
            "",
            "\t/*",
            "\t * If the descriptor has been recycled, verify the old state val.",
            "\t * See \"ABA Issues\" about why this verification is performed.",
            "\t */",
            "\tprev_state_val = atomic_long_read(&desc->state_var); /* LMM(desc_reserve:E) */",
            "\tif (prev_state_val &&",
            "\t    get_desc_state(id_prev_wrap, prev_state_val) != desc_reusable) {",
            "\t\tWARN_ON_ONCE(1);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/*",
            "\t * Assign the descriptor a new ID and set its state to reserved.",
            "\t * See \"ABA Issues\" about why cmpxchg() instead of set() is used.",
            "\t *",
            "\t * Guarantee the new descriptor ID and state is stored before making",
            "\t * any other changes. A write memory barrier is sufficient for this.",
            "\t * This pairs with desc_read:D.",
            "\t */",
            "\tif (!atomic_long_try_cmpxchg(&desc->state_var, &prev_state_val,",
            "\t\t\tDESC_SV(id, desc_reserved))) { /* LMM(desc_reserve:F) */",
            "\t\tWARN_ON_ONCE(1);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/* Now data in @desc can be modified: LMM(desc_reserve:G) */",
            "",
            "\t*id_out = id;",
            "\treturn true;",
            "}"
          ],
          "function_name": "desc_reserve",
          "description": "实现描述符保留逻辑，通过CAS操作获取新ID并设置保留状态，含多重内存屏障保障状态变更顺序与数据一致性",
          "similarity": 0.5361891984939575
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 383,
          "end_line": 521,
          "content": [
            "static unsigned int to_blk_size(unsigned int size)",
            "{",
            "\tstruct prb_data_block *db = NULL;",
            "",
            "\tsize += sizeof(*db);",
            "\tsize = ALIGN(size, sizeof(db->id));",
            "\treturn size;",
            "}",
            "static bool data_check_size(struct prb_data_ring *data_ring, unsigned int size)",
            "{",
            "\tstruct prb_data_block *db = NULL;",
            "",
            "\tif (size == 0)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Ensure the alignment padded size could possibly fit in the data",
            "\t * array. The largest possible data block must still leave room for",
            "\t * at least the ID of the next block.",
            "\t */",
            "\tsize = to_blk_size(size);",
            "\tif (size > DATA_SIZE(data_ring) - sizeof(db->id))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static enum desc_state get_desc_state(unsigned long id,",
            "\t\t\t\t      unsigned long state_val)",
            "{",
            "\tif (id != DESC_ID(state_val))",
            "\t\treturn desc_miss;",
            "",
            "\treturn DESC_STATE(state_val);",
            "}",
            "static enum desc_state desc_read(struct prb_desc_ring *desc_ring,",
            "\t\t\t\t unsigned long id, struct prb_desc *desc_out,",
            "\t\t\t\t u64 *seq_out, u32 *caller_id_out)",
            "{",
            "\tstruct printk_info *info = to_info(desc_ring, id);",
            "\tstruct prb_desc *desc = to_desc(desc_ring, id);",
            "\tatomic_long_t *state_var = &desc->state_var;",
            "\tenum desc_state d_state;",
            "\tunsigned long state_val;",
            "",
            "\t/* Check the descriptor state. */",
            "\tstate_val = atomic_long_read(state_var); /* LMM(desc_read:A) */",
            "\td_state = get_desc_state(id, state_val);",
            "\tif (d_state == desc_miss || d_state == desc_reserved) {",
            "\t\t/*",
            "\t\t * The descriptor is in an inconsistent state. Set at least",
            "\t\t * @state_var so that the caller can see the details of",
            "\t\t * the inconsistent state.",
            "\t\t */",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/*",
            "\t * Guarantee the state is loaded before copying the descriptor",
            "\t * content. This avoids copying obsolete descriptor content that might",
            "\t * not apply to the descriptor state. This pairs with _prb_commit:B.",
            "\t *",
            "\t * Memory barrier involvement:",
            "\t *",
            "\t * If desc_read:A reads from _prb_commit:B, then desc_read:C reads",
            "\t * from _prb_commit:A.",
            "\t *",
            "\t * Relies on:",
            "\t *",
            "\t * WMB from _prb_commit:A to _prb_commit:B",
            "\t *    matching",
            "\t * RMB from desc_read:A to desc_read:C",
            "\t */",
            "\tsmp_rmb(); /* LMM(desc_read:B) */",
            "",
            "\t/*",
            "\t * Copy the descriptor data. The data is not valid until the",
            "\t * state has been re-checked. A memcpy() for all of @desc",
            "\t * cannot be used because of the atomic_t @state_var field.",
            "\t */",
            "\tif (desc_out) {",
            "\t\tmemcpy(&desc_out->text_blk_lpos, &desc->text_blk_lpos,",
            "\t\t       sizeof(desc_out->text_blk_lpos)); /* LMM(desc_read:C) */",
            "\t}",
            "\tif (seq_out)",
            "\t\t*seq_out = info->seq; /* also part of desc_read:C */",
            "\tif (caller_id_out)",
            "\t\t*caller_id_out = info->caller_id; /* also part of desc_read:C */",
            "",
            "\t/*",
            "\t * 1. Guarantee the descriptor content is loaded before re-checking",
            "\t *    the state. This avoids reading an obsolete descriptor state",
            "\t *    that may not apply to the copied content. This pairs with",
            "\t *    desc_reserve:F.",
            "\t *",
            "\t *    Memory barrier involvement:",
            "\t *",
            "\t *    If desc_read:C reads from desc_reserve:G, then desc_read:E",
            "\t *    reads from desc_reserve:F.",
            "\t *",
            "\t *    Relies on:",
            "\t *",
            "\t *    WMB from desc_reserve:F to desc_reserve:G",
            "\t *       matching",
            "\t *    RMB from desc_read:C to desc_read:E",
            "\t *",
            "\t * 2. Guarantee the record data is loaded before re-checking the",
            "\t *    state. This avoids reading an obsolete descriptor state that may",
            "\t *    not apply to the copied data. This pairs with data_alloc:A and",
            "\t *    data_realloc:A.",
            "\t *",
            "\t *    Memory barrier involvement:",
            "\t *",
            "\t *    If copy_data:A reads from data_alloc:B, then desc_read:E",
            "\t *    reads from desc_make_reusable:A.",
            "\t *",
            "\t *    Relies on:",
            "\t *",
            "\t *    MB from desc_make_reusable:A to data_alloc:B",
            "\t *       matching",
            "\t *    RMB from desc_read:C to desc_read:E",
            "\t *",
            "\t *    Note: desc_make_reusable:A and data_alloc:B can be different",
            "\t *          CPUs. However, the data_alloc:B CPU (which performs the",
            "\t *          full memory barrier) must have previously seen",
            "\t *          desc_make_reusable:A.",
            "\t */",
            "\tsmp_rmb(); /* LMM(desc_read:D) */",
            "",
            "\t/*",
            "\t * The data has been copied. Return the current descriptor state,",
            "\t * which may have changed since the load above.",
            "\t */",
            "\tstate_val = atomic_long_read(state_var); /* LMM(desc_read:E) */",
            "\td_state = get_desc_state(id, state_val);",
            "out:",
            "\tif (desc_out)",
            "\t\tatomic_long_set(&desc_out->state_var, state_val);",
            "\treturn d_state;",
            "}"
          ],
          "function_name": "to_blk_size, data_check_size, get_desc_state, desc_read",
          "description": "实现数据块大小计算、数据大小验证、描述符状态获取及描述符读取逻辑，含多处内存屏障保障状态一致性",
          "similarity": 0.5303966999053955
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 1690,
          "end_line": 1805,
          "content": [
            "static void _prb_commit(struct prb_reserved_entry *e, unsigned long state_val)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &e->rb->desc_ring;",
            "\tstruct prb_desc *d = to_desc(desc_ring, e->id);",
            "\tunsigned long prev_state_val = DESC_SV(e->id, desc_reserved);",
            "",
            "\t/* Now the writer has finished all writing: LMM(_prb_commit:A) */",
            "",
            "\t/*",
            "\t * Set the descriptor as committed. See \"ABA Issues\" about why",
            "\t * cmpxchg() instead of set() is used.",
            "\t *",
            "\t * 1  Guarantee all record data is stored before the descriptor state",
            "\t *    is stored as committed. A write memory barrier is sufficient",
            "\t *    for this. This pairs with desc_read:B and desc_reopen_last:A.",
            "\t *",
            "\t * 2. Guarantee the descriptor state is stored as committed before",
            "\t *    re-checking the head ID in order to possibly finalize this",
            "\t *    descriptor. This pairs with desc_reserve:D.",
            "\t *",
            "\t *    Memory barrier involvement:",
            "\t *",
            "\t *    If prb_commit:A reads from desc_reserve:D, then",
            "\t *    desc_make_final:A reads from _prb_commit:B.",
            "\t *",
            "\t *    Relies on:",
            "\t *",
            "\t *    MB _prb_commit:B to prb_commit:A",
            "\t *       matching",
            "\t *    MB desc_reserve:D to desc_make_final:A",
            "\t */",
            "\tif (!atomic_long_try_cmpxchg(&d->state_var, &prev_state_val,",
            "\t\t\tDESC_SV(e->id, state_val))) { /* LMM(_prb_commit:B) */",
            "\t\tWARN_ON_ONCE(1);",
            "\t}",
            "",
            "\t/* Restore interrupts, the reserve/commit window is finished. */",
            "\tlocal_irq_restore(e->irqflags);",
            "}",
            "void prb_commit(struct prb_reserved_entry *e)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &e->rb->desc_ring;",
            "\tunsigned long head_id;",
            "",
            "\t_prb_commit(e, desc_committed);",
            "",
            "\t/*",
            "\t * If this descriptor is no longer the head (i.e. a new record has",
            "\t * been allocated), extending the data for this record is no longer",
            "\t * allowed and therefore it must be finalized.",
            "\t */",
            "\thead_id = atomic_long_read(&desc_ring->head_id); /* LMM(prb_commit:A) */",
            "\tif (head_id != e->id)",
            "\t\tdesc_make_final(e->rb, e->id);",
            "}",
            "void prb_final_commit(struct prb_reserved_entry *e)",
            "{",
            "\t_prb_commit(e, desc_finalized);",
            "",
            "\tdesc_update_last_finalized(e->rb);",
            "}",
            "static unsigned int count_lines(const char *text, unsigned int text_size)",
            "{",
            "\tunsigned int next_size = text_size;",
            "\tunsigned int line_count = 1;",
            "\tconst char *next = text;",
            "",
            "\twhile (next_size) {",
            "\t\tnext = memchr(next, '\\n', next_size);",
            "\t\tif (!next)",
            "\t\t\tbreak;",
            "\t\tline_count++;",
            "\t\tnext++;",
            "\t\tnext_size = text_size - (next - text);",
            "\t}",
            "",
            "\treturn line_count;",
            "}",
            "static bool copy_data(struct prb_data_ring *data_ring,",
            "\t\t      struct prb_data_blk_lpos *blk_lpos, u16 len, char *buf,",
            "\t\t      unsigned int buf_size, unsigned int *line_count)",
            "{",
            "\tunsigned int data_size;",
            "\tconst char *data;",
            "",
            "\t/* Caller might not want any data. */",
            "\tif ((!buf || !buf_size) && !line_count)",
            "\t\treturn true;",
            "",
            "\tdata = get_data(data_ring, blk_lpos, &data_size);",
            "\tif (!data)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Actual cannot be less than expected. It can be more than expected",
            "\t * because of the trailing alignment padding.",
            "\t *",
            "\t * Note that invalid @len values can occur because the caller loads",
            "\t * the value during an allowed data race.",
            "\t */",
            "\tif (data_size < (unsigned int)len)",
            "\t\treturn false;",
            "",
            "\t/* Caller interested in the line count? */",
            "\tif (line_count)",
            "\t\t*line_count = count_lines(data, len);",
            "",
            "\t/* Caller interested in the data content? */",
            "\tif (!buf || !buf_size)",
            "\t\treturn true;",
            "",
            "\tdata_size = min_t(unsigned int, buf_size, len);",
            "",
            "\tmemcpy(&buf[0], data, data_size); /* LMM(copy_data:A) */",
            "\treturn true;",
            "}"
          ],
          "function_name": "_prb_commit, prb_commit, prb_final_commit, count_lines, copy_data",
          "description": "处理记录提交操作及数据复制。_prb_commit设置描述符状态并释放中断，prb_final_commit标记描述符为最终状态，count_lines统计文本行数，copy_data安全复制数据到缓冲区。",
          "similarity": 0.5283176898956299
        }
      ]
    }
  ]
}