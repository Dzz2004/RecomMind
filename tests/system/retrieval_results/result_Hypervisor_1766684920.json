{
  "query": "Hypervisor",
  "timestamp": "2025-12-26 01:48:40",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/qspinlock_paravirt.h",
      "md_summary": "> 自动生成时间: 2025-10-25 14:46:42\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\qspinlock_paravirt.h`\n\n---\n\n# `locking/qspinlock_paravirt.h` 技术文档\n\n## 1. 文件概述\n\n`qspinlock_paravirt.h` 是 Linux 内核中用于实现 **半虚拟化（paravirtualized, PV）队列自旋锁（qspinlock）** 的头文件。其核心目标是在虚拟化环境中优化自旋锁行为：当虚拟 CPU（vCPU）无法立即获取锁时，不进行忙等待（busy-waiting），而是通过 **挂起（halt）** 当前 vCPU 并等待被唤醒，从而显著降低在锁竞争激烈或宿主机过载（overcommitted）场景下的 CPU 资源浪费和延迟。\n\n该文件依赖架构层提供的两个关键半虚拟化超调用（hypercall）：\n- `pv_wait(u8 *ptr, u8 val)`：当 `*ptr == val` 时挂起当前 vCPU。\n- `pv_kick(cpu)`：唤醒指定的已挂起 vCPU。\n\n此文件 **不能直接包含**，必须通过定义 `_GEN_PV_LOCK_SLOWPATH` 宏后由其他文件（如 `qspinlock.c`）条件包含，以替换原生的慢路径锁实现。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`enum vcpu_state`**  \n  表示 vCPU 在锁等待队列中的状态：\n  - `vcpu_running`：正在运行（默认状态）。\n  - `vcpu_halted`：已挂起，等待被唤醒（仅用于 `pv_wait_node`）。\n  - `vcpu_hashed`：已挂起且其节点信息已加入哈希表（用于快速查找）。\n\n- **`struct pv_node`**  \n  扩展的 MCS 锁节点，包含：\n  - `mcs`：标准 MCS 自旋锁节点。\n  - `cpu`：关联的 CPU ID。\n  - `state`：当前 vCPU 状态（`vcpu_state` 枚举值）。\n\n- **`struct pv_hash_entry`**  \n  哈希表条目，用于快速映射锁地址到对应的 `pv_node`：\n  - `lock`：指向 `qspinlock` 的指针。\n  - `node`：指向 `pv_node` 的指针。\n\n### 主要函数与宏\n\n- **`pv_hybrid_queued_unfair_trylock()`**  \n  实现混合模式的锁尝试获取逻辑，结合了公平队列锁与非公平锁的优点。\n\n- **`set_pending()` / `trylock_clear_pending()`**  \n  操作锁的 `pending` 位，用于协调队列头 vCPU 与新到来的竞争者。\n\n- **`__pv_init_lock_hash()`**  \n  初始化 PV 锁哈希表，分配足够大的内存空间以支持所有可能的 CPU。\n\n- **`pv_hash()` / `pv_unhash()`**  \n  在哈希表中插入/删除锁与节点的映射关系，用于快速唤醒等待者。\n\n- **`pv_wait_early()`**  \n  （代码不完整）用于判断是否应提前检查前驱节点状态并挂起当前 vCPU。\n\n### 关键宏定义\n\n- **`PV_PREV_CHECK_MASK`**  \n  控制检查前驱节点状态的频率（每 256 次循环检查一次），避免缓存行抖动。\n\n- **`_Q_SLOW_VAL`**  \n  表示锁处于慢路径状态的值（`locked=1, pending=1`）。\n\n- **`queued_spin_trylock`**  \n  重定义为 `pv_hybrid_queued_unfair_trylock`，启用混合锁机制。\n\n## 3. 关键实现\n\n### 混合 PV 队列/非公平锁机制\n\n该实现采用 **混合策略**：\n- 当锁的 MCS 等待队列为空或 `pending` 位未设置时，新竞争者尝试 **非公平方式抢锁**（直接 CAS `locked` 位），提升低竞争场景性能。\n- 一旦有 vCPU 进入等待队列并成为队列头，它会设置 `pending` 位，**禁止后续抢锁**，强制新竞争者进入公平队列，避免锁饥饿。\n- 队列头 vCPU 在自旋等待锁释放时保持 `pending=1`，确保公平性。\n\n### 自适应挂起（Adaptive Spinning）\n\n- 等待队列中的 vCPU 会周期性（由 `PV_PREV_CHECK_MASK` 控制）检查 **前驱节点是否正在运行**。\n- 若前驱 **未运行**（如已挂起），当前 vCPU 也立即挂起，避免无意义的忙等。\n- 此机制在虚拟化过载环境中显著减少 CPU 浪费，同时在非过载场景下通过一次抢锁尝试维持性能。\n\n### 锁-节点哈希表\n\n- 为支持 `pv_kick()` 快速定位等待某锁的 vCPU，内核维护一个全局哈希表 `pv_lock_hash`。\n- 哈希表大小为 `4 * num_possible_cpus()`，确保即使在最大嵌套深度（4 层）下也有足够条目。\n- 使用 **开放寻址法**，每缓存行存放多个条目（`PV_HE_PER_LINE`），减少缓存未命中。\n- 锁持有者在释放锁前必须调用 `pv_unhash()` 移除映射，保证哈希表一致性。\n\n### Pending 位操作优化\n\n- 根据 `_Q_PENDING_BITS` 是否为 8（即 `pending` 字段是否独立字节），提供两种实现：\n  - **独立字节**：直接写 `pending` 字段，使用 `cmpxchg_acquire` 尝试获取锁。\n  - **共享字段**：使用原子位操作（`atomic_or` / `atomic_cmpxchg_acquire`）修改 `val`。\n\n## 4. 依赖关系\n\n- **架构依赖**：必须由底层架构（如 x86 KVM/Xen）提供 `pv_wait()` 和 `pv_kick()` 超调用。\n- **头文件依赖**：\n  - `<linux/hash.h>`：提供 `hash_ptr()` 哈希函数。\n  - `<linux/memblock.h>`：用于早期内存分配（`alloc_large_system_hash`）。\n  - `<linux/debug_locks.h>`：锁调试支持。\n- **锁核心依赖**：基于 `qspinlock` 和 `mcs_spinlock` 实现，需与 `locking/qspinlock.c` 协同工作。\n- **编译依赖**：必须由定义了 `_GEN_PV_LOCK_SLOWPATH` 的源文件包含，不能独立编译。\n\n## 5. 使用场景\n\n- **虚拟化环境**：主要在 KVM、Xen 等半虚拟化 Hypervisor 上启用，优化多 vCPU 虚拟机中的锁竞争。\n- **高竞争锁场景**：当多个 vCPU 频繁争用同一自旋锁时，避免忙等待导致的宿主机 CPU 资源耗尽。\n- **过载宿主机**：在物理 CPU 资源不足时，挂起等待锁的 vCPU 可减少调度开销和上下文切换延迟。\n- **混合工作负载**：通过混合锁机制，在低竞争时保持高性能，高竞争时保证公平性，适用于通用服务器场景。",
      "similarity": 0.49820542335510254,
      "chunks": []
    },
    {
      "source_file": "kernel/bpf/stackmap.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:30:30\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\stackmap.c`\n\n---\n\n# `bpf/stackmap.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/stackmap.c` 实现了 BPF（Berkeley Packet Filter）子系统中的 **栈映射（stack map）** 功能，用于高效地存储和查询用户态或内核态的调用栈（call stack）信息。该映射支持两种模式：\n- **原始 IP 地址模式**：直接存储程序计数器（PC）地址。\n- **Build ID + 偏移量模式**：将 IP 地址转换为对应二进制文件的 Build ID 和相对于该文件的偏移量，便于符号化解析且不受 ASLR 影响。\n\n该文件为 BPF 程序提供 `bpf_get_stackid()` 辅助函数的后端实现，是性能分析、追踪和调试工具（如 perf、bpftrace）的关键组件。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct stack_map_bucket`**  \n  哈希桶中的条目，包含：\n  - `fnode`：用于 per-CPU 自由链表管理\n  - `hash`：调用栈内容的哈希值\n  - `nr`：栈帧数量\n  - `data[]`：变长数组，存储栈数据（`u64` IP 或 `struct bpf_stack_build_id`）\n\n- **`struct bpf_stack_map`**  \n  BPF 栈映射的私有结构，继承自 `struct bpf_map`，包含：\n  - `elems`：预分配的桶内存池\n  - `freelist`：per-CPU 自由链表，用于高效分配/回收桶\n  - `n_buckets`：哈希表桶数量（2 的幂）\n  - `buckets[]`：哈希桶指针数组\n\n- **`struct bpf_stack_build_id`**（外部定义）  \n  Build ID 模式下的栈帧表示，包含：\n  - `status`：状态（`BPF_STACK_BUILD_ID_IP` 或 `BPF_STACK_BUILD_ID_VALID`）\n  - `ip`：原始 IP（仅当 status 为 IP 时有效）\n  - `offset`：相对于映射起始地址的偏移\n  - `build_id[]`：Build ID 字节数组\n\n### 主要函数\n\n- **`stack_map_alloc()`**  \n  BPF 栈映射的分配器，验证属性、预分配内存、初始化自由链表和调用链缓冲区。\n\n- **`__bpf_get_stackid()`**  \n  核心逻辑：根据传入的调用栈生成唯一 ID，执行哈希查找、比较和插入。\n\n- **`stack_map_get_build_id_offset()`**  \n  将原始 IP 地址转换为 Build ID + 偏移量格式，需持有 mmap 读锁。\n\n- **`get_callchain_entry_for_task()`**  \n  从指定任务结构中提取内核态调用栈（仅在 `CONFIG_STACKTRACE` 启用时有效）。\n\n- **`bpf_get_stackid()`**（BPF_CALL_3 宏定义）  \n  BPF 程序调用的入口点，根据寄存器上下文获取当前调用栈并返回其 ID。\n\n## 3. 关键实现\n\n### 哈希表设计\n- 使用 **开放寻址 + 覆盖替换** 策略：每个桶 ID 对应唯一哈希桶，冲突时直接替换旧条目。\n- 哈希函数为 `jhash2()`，对栈 IP 数组进行哈希。\n- 桶数量为 `max_entries` 的 2 次幂，通过位掩码 `hash & (n_buckets - 1)` 快速定位。\n\n### 内存管理\n- **预分配内存池**：在映射创建时一次性分配所有桶内存（`smap->elems`）。\n- **Per-CPU 自由链表**：使用 `pcpu_freelist` 实现无锁、高效的桶分配/回收，避免运行时内存分配开销。\n\n### Build ID 转换\n- 通过 `find_vma()` 查找 IP 所属的 VMA（虚拟内存区域）。\n- 调用 `build_id_parse()` 从 VMA 的 ELF 头中提取 Build ID。\n- 计算偏移量：`offset = (vma->vm_pgoff << PAGE_SHIFT) + ip - vma->vm_start`。\n- **锁机制**：使用 `mmap_read_trylock()` 获取 mmap 读锁，失败时回退到原始 IP 模式。\n- **中断上下文安全**：通过 `mmap_unlock_irq_work` 机制确保在中断上下文中安全释放 mmap 锁。\n\n### 快速比较优化\n- 若设置 `BPF_F_FAST_STACK_CMP` 标志，仅比较哈希值，不进行内容 memcmp，适用于对哈希冲突不敏感的场景。\n\n### 栈深度限制\n- 最大栈深度由 `sysctl_perf_event_max_stack` 控制，映射的 `value_size` 必须是单个栈帧大小的整数倍且不超过该限制。\n\n## 4. 依赖关系\n\n- **BPF 核心**：`<linux/bpf.h>`、`bpf_map` 基础设施\n- **内存管理**：`<linux/percpu.h>`（per-CPU 自由链表）、`bpf_map_area_alloc/free`\n- **栈追踪**：`<linux/stacktrace.h>`、`<linux/perf_event.h>`（调用链缓冲区）\n- **Build ID 支持**：`<linux/buildid.h>`、VMA 操作（`find_vma`、`range_in_vma`）\n- **内存映射锁**：`mmap_unlock_work.h`（安全释放 mmap 锁）\n- **哈希函数**：`<linux/jhash.h>`\n- **配置选项**：`CONFIG_STACKTRACE`（内核栈追踪支持）\n\n## 5. 使用场景\n\n- **性能分析**：BPF 程序通过 `bpf_get_stackid()` 获取当前调用栈 ID，后续通过 `bpf_map_lookup_elem()` 读取完整栈内容，用于火焰图生成。\n- **系统追踪**：结合 kprobe/uprobe，记录特定函数调用时的完整调用上下文。\n- **安全监控**：检测异常调用路径（如敏感系统调用的调用者）。\n- **调试工具**：为 `perf`、`bpftrace`、`bcc` 等工具提供高效的栈存储后端。\n- **Build ID 模式**：在 ASLR（地址空间布局随机化）环境下，通过 Build ID + 偏移实现稳定的符号化解析，适用于长期运行的监控场景。",
      "similarity": 0.49646222591400146,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/stackmap.c",
          "start_line": 319,
          "end_line": 426,
          "content": [
            "static __u64 count_kernel_ip(struct perf_callchain_entry *trace)",
            "{",
            "\t__u64 nr_kernel = 0;",
            "",
            "\twhile (nr_kernel < trace->nr) {",
            "\t\tif (trace->ip[nr_kernel] == PERF_CONTEXT_USER)",
            "\t\t\tbreak;",
            "\t\tnr_kernel++;",
            "\t}",
            "\treturn nr_kernel;",
            "}",
            "static long __bpf_get_stack(struct pt_regs *regs, struct task_struct *task,",
            "\t\t\t    struct perf_callchain_entry *trace_in,",
            "\t\t\t    void *buf, u32 size, u64 flags)",
            "{",
            "\tu32 trace_nr, copy_len, elem_size, num_elem, max_depth;",
            "\tbool user_build_id = flags & BPF_F_USER_BUILD_ID;",
            "\tbool crosstask = task && task != current;",
            "\tu32 skip = flags & BPF_F_SKIP_FIELD_MASK;",
            "\tbool user = flags & BPF_F_USER_STACK;",
            "\tstruct perf_callchain_entry *trace;",
            "\tbool kernel = !user;",
            "\tint err = -EINVAL;",
            "\tu64 *ips;",
            "",
            "\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |",
            "\t\t\t       BPF_F_USER_BUILD_ID)))",
            "\t\tgoto clear;",
            "\tif (kernel && user_build_id)",
            "\t\tgoto clear;",
            "",
            "\telem_size = (user && user_build_id) ? sizeof(struct bpf_stack_build_id)",
            "\t\t\t\t\t    : sizeof(u64);",
            "\tif (unlikely(size % elem_size))",
            "\t\tgoto clear;",
            "",
            "\t/* cannot get valid user stack for task without user_mode regs */",
            "\tif (task && user && !user_mode(regs))",
            "\t\tgoto err_fault;",
            "",
            "\t/* get_perf_callchain does not support crosstask user stack walking",
            "\t * but returns an empty stack instead of NULL.",
            "\t */",
            "\tif (crosstask && user) {",
            "\t\terr = -EOPNOTSUPP;",
            "\t\tgoto clear;",
            "\t}",
            "",
            "\tnum_elem = size / elem_size;",
            "\tmax_depth = num_elem + skip;",
            "\tif (sysctl_perf_event_max_stack < max_depth)",
            "\t\tmax_depth = sysctl_perf_event_max_stack;",
            "",
            "\tif (trace_in)",
            "\t\ttrace = trace_in;",
            "\telse if (kernel && task)",
            "\t\ttrace = get_callchain_entry_for_task(task, max_depth);",
            "\telse",
            "\t\ttrace = get_perf_callchain(regs, 0, kernel, user, max_depth,",
            "\t\t\t\t\t   crosstask, false);",
            "\tif (unlikely(!trace))",
            "\t\tgoto err_fault;",
            "",
            "\tif (trace->nr < skip)",
            "\t\tgoto err_fault;",
            "",
            "\ttrace_nr = trace->nr - skip;",
            "\ttrace_nr = (trace_nr <= num_elem) ? trace_nr : num_elem;",
            "\tcopy_len = trace_nr * elem_size;",
            "",
            "\tips = trace->ip + skip;",
            "\tif (user && user_build_id)",
            "\t\tstack_map_get_build_id_offset(buf, ips, trace_nr, user);",
            "\telse",
            "\t\tmemcpy(buf, ips, copy_len);",
            "",
            "\tif (size > copy_len)",
            "\t\tmemset(buf + copy_len, 0, size - copy_len);",
            "\treturn copy_len;",
            "",
            "err_fault:",
            "\terr = -EFAULT;",
            "clear:",
            "\tmemset(buf, 0, size);",
            "\treturn err;",
            "}",
            "int bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)",
            "{",
            "\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);",
            "\tstruct stack_map_bucket *bucket, *old_bucket;",
            "\tu32 id = *(u32 *)key, trace_len;",
            "",
            "\tif (unlikely(id >= smap->n_buckets))",
            "\t\treturn -ENOENT;",
            "",
            "\tbucket = xchg(&smap->buckets[id], NULL);",
            "\tif (!bucket)",
            "\t\treturn -ENOENT;",
            "",
            "\ttrace_len = bucket->nr * stack_map_data_size(map);",
            "\tmemcpy(value, bucket->data, trace_len);",
            "\tmemset(value + trace_len, 0, map->value_size - trace_len);",
            "",
            "\told_bucket = xchg(&smap->buckets[id], bucket);",
            "\tif (old_bucket)",
            "\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "count_kernel_ip, __bpf_get_stack, bpf_stackmap_copy",
          "description": "提供堆栈数据复制接口，包含内核IP计数、用户/内核模式堆栈采集、构建ID偏移量解析，以及堆栈映射数据复制功能",
          "similarity": 0.4753250479698181
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/stackmap.c",
          "start_line": 34,
          "end_line": 182,
          "content": [
            "static inline bool stack_map_use_build_id(struct bpf_map *map)",
            "{",
            "\treturn (map->map_flags & BPF_F_STACK_BUILD_ID);",
            "}",
            "static inline int stack_map_data_size(struct bpf_map *map)",
            "{",
            "\treturn stack_map_use_build_id(map) ?",
            "\t\tsizeof(struct bpf_stack_build_id) : sizeof(u64);",
            "}",
            "static int prealloc_elems_and_freelist(struct bpf_stack_map *smap)",
            "{",
            "\tu64 elem_size = sizeof(struct stack_map_bucket) +",
            "\t\t\t(u64)smap->map.value_size;",
            "\tint err;",
            "",
            "\tsmap->elems = bpf_map_area_alloc(elem_size * smap->map.max_entries,",
            "\t\t\t\t\t smap->map.numa_node);",
            "\tif (!smap->elems)",
            "\t\treturn -ENOMEM;",
            "",
            "\terr = pcpu_freelist_init(&smap->freelist);",
            "\tif (err)",
            "\t\tgoto free_elems;",
            "",
            "\tpcpu_freelist_populate(&smap->freelist, smap->elems, elem_size,",
            "\t\t\t       smap->map.max_entries);",
            "\treturn 0;",
            "",
            "free_elems:",
            "\tbpf_map_area_free(smap->elems);",
            "\treturn err;",
            "}",
            "static void stack_map_get_build_id_offset(struct bpf_stack_build_id *id_offs,",
            "\t\t\t\t\t  u64 *ips, u32 trace_nr, bool user)",
            "{",
            "\tint i;",
            "\tstruct mmap_unlock_irq_work *work = NULL;",
            "\tbool irq_work_busy = bpf_mmap_unlock_get_irq_work(&work);",
            "\tstruct vm_area_struct *vma, *prev_vma = NULL;",
            "\tconst char *prev_build_id;",
            "",
            "\t/* If the irq_work is in use, fall back to report ips. Same",
            "\t * fallback is used for kernel stack (!user) on a stackmap with",
            "\t * build_id.",
            "\t */",
            "\tif (!user || !current || !current->mm || irq_work_busy ||",
            "\t    !mmap_read_trylock(current->mm)) {",
            "\t\t/* cannot access current->mm, fall back to ips */",
            "\t\tfor (i = 0; i < trace_nr; i++) {",
            "\t\t\tid_offs[i].status = BPF_STACK_BUILD_ID_IP;",
            "\t\t\tid_offs[i].ip = ips[i];",
            "\t\t\tmemset(id_offs[i].build_id, 0, BUILD_ID_SIZE_MAX);",
            "\t\t}",
            "\t\treturn;",
            "\t}",
            "",
            "\tfor (i = 0; i < trace_nr; i++) {",
            "\t\tif (range_in_vma(prev_vma, ips[i], ips[i])) {",
            "\t\t\tvma = prev_vma;",
            "\t\t\tmemcpy(id_offs[i].build_id, prev_build_id,",
            "\t\t\t       BUILD_ID_SIZE_MAX);",
            "\t\t\tgoto build_id_valid;",
            "\t\t}",
            "\t\tvma = find_vma(current->mm, ips[i]);",
            "\t\tif (!vma || build_id_parse(vma, id_offs[i].build_id, NULL)) {",
            "\t\t\t/* per entry fall back to ips */",
            "\t\t\tid_offs[i].status = BPF_STACK_BUILD_ID_IP;",
            "\t\t\tid_offs[i].ip = ips[i];",
            "\t\t\tmemset(id_offs[i].build_id, 0, BUILD_ID_SIZE_MAX);",
            "\t\t\tcontinue;",
            "\t\t}",
            "build_id_valid:",
            "\t\tid_offs[i].offset = (vma->vm_pgoff << PAGE_SHIFT) + ips[i]",
            "\t\t\t- vma->vm_start;",
            "\t\tid_offs[i].status = BPF_STACK_BUILD_ID_VALID;",
            "\t\tprev_vma = vma;",
            "\t\tprev_build_id = id_offs[i].build_id;",
            "\t}",
            "\tbpf_mmap_unlock_mm(work, current->mm);",
            "}",
            "static long __bpf_get_stackid(struct bpf_map *map,",
            "\t\t\t      struct perf_callchain_entry *trace, u64 flags)",
            "{",
            "\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);",
            "\tstruct stack_map_bucket *bucket, *new_bucket, *old_bucket;",
            "\tu32 skip = flags & BPF_F_SKIP_FIELD_MASK;",
            "\tu32 hash, id, trace_nr, trace_len;",
            "\tbool user = flags & BPF_F_USER_STACK;",
            "\tu64 *ips;",
            "\tbool hash_matches;",
            "",
            "\tif (trace->nr <= skip)",
            "\t\t/* skipping more than usable stack trace */",
            "\t\treturn -EFAULT;",
            "",
            "\ttrace_nr = trace->nr - skip;",
            "\ttrace_len = trace_nr * sizeof(u64);",
            "\tips = trace->ip + skip;",
            "\thash = jhash2((u32 *)ips, trace_len / sizeof(u32), 0);",
            "\tid = hash & (smap->n_buckets - 1);",
            "\tbucket = READ_ONCE(smap->buckets[id]);",
            "",
            "\thash_matches = bucket && bucket->hash == hash;",
            "\t/* fast cmp */",
            "\tif (hash_matches && flags & BPF_F_FAST_STACK_CMP)",
            "\t\treturn id;",
            "",
            "\tif (stack_map_use_build_id(map)) {",
            "\t\t/* for build_id+offset, pop a bucket before slow cmp */",
            "\t\tnew_bucket = (struct stack_map_bucket *)",
            "\t\t\tpcpu_freelist_pop(&smap->freelist);",
            "\t\tif (unlikely(!new_bucket))",
            "\t\t\treturn -ENOMEM;",
            "\t\tnew_bucket->nr = trace_nr;",
            "\t\tstack_map_get_build_id_offset(",
            "\t\t\t(struct bpf_stack_build_id *)new_bucket->data,",
            "\t\t\tips, trace_nr, user);",
            "\t\ttrace_len = trace_nr * sizeof(struct bpf_stack_build_id);",
            "\t\tif (hash_matches && bucket->nr == trace_nr &&",
            "\t\t    memcmp(bucket->data, new_bucket->data, trace_len) == 0) {",
            "\t\t\tpcpu_freelist_push(&smap->freelist, &new_bucket->fnode);",
            "\t\t\treturn id;",
            "\t\t}",
            "\t\tif (bucket && !(flags & BPF_F_REUSE_STACKID)) {",
            "\t\t\tpcpu_freelist_push(&smap->freelist, &new_bucket->fnode);",
            "\t\t\treturn -EEXIST;",
            "\t\t}",
            "\t} else {",
            "\t\tif (hash_matches && bucket->nr == trace_nr &&",
            "\t\t    memcmp(bucket->data, ips, trace_len) == 0)",
            "\t\t\treturn id;",
            "\t\tif (bucket && !(flags & BPF_F_REUSE_STACKID))",
            "\t\t\treturn -EEXIST;",
            "",
            "\t\tnew_bucket = (struct stack_map_bucket *)",
            "\t\t\tpcpu_freelist_pop(&smap->freelist);",
            "\t\tif (unlikely(!new_bucket))",
            "\t\t\treturn -ENOMEM;",
            "\t\tmemcpy(new_bucket->data, ips, trace_len);",
            "\t}",
            "",
            "\tnew_bucket->hash = hash;",
            "\tnew_bucket->nr = trace_nr;",
            "",
            "\told_bucket = xchg(&smap->buckets[id], new_bucket);",
            "\tif (old_bucket)",
            "\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);",
            "\treturn id;",
            "}"
          ],
          "function_name": "stack_map_use_build_id, stack_map_data_size, prealloc_elems_and_freelist, stack_map_get_build_id_offset, __bpf_get_stackid",
          "description": "实现堆栈ID获取逻辑，包含构建ID解析、桶数据比较、哈希表查找等功能，支持通过构建ID或直接IP地址方式存储堆栈跟踪信息",
          "similarity": 0.4742507040500641
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/stackmap.c",
          "start_line": 601,
          "end_line": 671,
          "content": [
            "static int stack_map_get_next_key(struct bpf_map *map, void *key,",
            "\t\t\t\t  void *next_key)",
            "{",
            "\tstruct bpf_stack_map *smap = container_of(map,",
            "\t\t\t\t\t\t  struct bpf_stack_map, map);",
            "\tu32 id;",
            "",
            "\tWARN_ON_ONCE(!rcu_read_lock_held());",
            "",
            "\tif (!key) {",
            "\t\tid = 0;",
            "\t} else {",
            "\t\tid = *(u32 *)key;",
            "\t\tif (id >= smap->n_buckets || !smap->buckets[id])",
            "\t\t\tid = 0;",
            "\t\telse",
            "\t\t\tid++;",
            "\t}",
            "",
            "\twhile (id < smap->n_buckets && !smap->buckets[id])",
            "\t\tid++;",
            "",
            "\tif (id >= smap->n_buckets)",
            "\t\treturn -ENOENT;",
            "",
            "\t*(u32 *)next_key = id;",
            "\treturn 0;",
            "}",
            "static long stack_map_update_elem(struct bpf_map *map, void *key, void *value,",
            "\t\t\t\t  u64 map_flags)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "static long stack_map_delete_elem(struct bpf_map *map, void *key)",
            "{",
            "\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);",
            "\tstruct stack_map_bucket *old_bucket;",
            "\tu32 id = *(u32 *)key;",
            "",
            "\tif (unlikely(id >= smap->n_buckets))",
            "\t\treturn -E2BIG;",
            "",
            "\told_bucket = xchg(&smap->buckets[id], NULL);",
            "\tif (old_bucket) {",
            "\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);",
            "\t\treturn 0;",
            "\t} else {",
            "\t\treturn -ENOENT;",
            "\t}",
            "}",
            "static void stack_map_free(struct bpf_map *map)",
            "{",
            "\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);",
            "",
            "\tbpf_map_area_free(smap->elems);",
            "\tpcpu_freelist_destroy(&smap->freelist);",
            "\tbpf_map_area_free(smap);",
            "\tput_callchain_buffers();",
            "}",
            "static u64 stack_map_mem_usage(const struct bpf_map *map)",
            "{",
            "\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);",
            "\tu64 value_size = map->value_size;",
            "\tu64 n_buckets = smap->n_buckets;",
            "\tu64 enties = map->max_entries;",
            "\tu64 usage = sizeof(*smap);",
            "",
            "\tusage += n_buckets * sizeof(struct stack_map_bucket *);",
            "\tusage += enties * (sizeof(struct stack_map_bucket) + value_size);",
            "\treturn usage;",
            "}"
          ],
          "function_name": "stack_map_get_next_key, stack_map_update_elem, stack_map_delete_elem, stack_map_free, stack_map_mem_usage",
          "description": "实现堆栈映射的键值遍历、元素更新/删除、内存释放和内存使用统计功能，维护堆栈桶的生命周期管理",
          "similarity": 0.4628485143184662
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/stackmap.c",
          "start_line": 1,
          "end_line": 33,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2016 Facebook",
            " */",
            "#include <linux/bpf.h>",
            "#include <linux/jhash.h>",
            "#include <linux/filter.h>",
            "#include <linux/kernel.h>",
            "#include <linux/stacktrace.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/btf_ids.h>",
            "#include <linux/buildid.h>",
            "#include \"percpu_freelist.h\"",
            "#include \"mmap_unlock_work.h\"",
            "",
            "#define STACK_CREATE_FLAG_MASK\t\t\t\t\t\\",
            "\t(BPF_F_NUMA_NODE | BPF_F_RDONLY | BPF_F_WRONLY |\t\\",
            "\t BPF_F_STACK_BUILD_ID)",
            "",
            "struct stack_map_bucket {",
            "\tstruct pcpu_freelist_node fnode;",
            "\tu32 hash;",
            "\tu32 nr;",
            "\tu64 data[];",
            "};",
            "",
            "struct bpf_stack_map {",
            "\tstruct bpf_map map;",
            "\tvoid *elems;",
            "\tstruct pcpu_freelist freelist;",
            "\tu32 n_buckets;",
            "\tstruct stack_map_bucket *buckets[];",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义堆栈映射相关结构体，包含用于存储堆栈跟踪数据的stack_map_bucket和堆栈映射主结构bpf_stack_map，包含PCPU自由列表和桶数组",
          "similarity": 0.39169833064079285
        }
      ]
    },
    {
      "source_file": "mm/hugetlb_cgroup.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:06:55\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `hugetlb_cgroup.c`\n\n---\n\n# hugetlb_cgroup.c 技术文档\n\n## 1. 文件概述\n\n`hugetlb_cgroup.c` 是 Linux 内核中用于实现 **HugeTLB（大页）内存资源控制组（cgroup）** 功能的核心文件。该文件通过 cgroup v1/v2 接口，对不同 HugeTLB 页面大小（如 2MB、1GB 等）的内存使用进行配额限制和统计追踪。它支持两种计费模式：普通分配（fault-based）和预留（reservation-based），并确保在 cgroup 被销毁时将资源正确迁移至父 cgroup，防止资源泄漏。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct hugetlb_cgroup`：每个 cgroup 实例对应的 HugeTLB 控制结构。\n  - 包含两个 `page_counter` 数组：`hugepage[]`（普通使用）和 `rsvd_hugepage[]`（预留使用），分别对应每种 HugeTLB 页面类型。\n  - 包含 per-node 信息 `nodeinfo[]`，用于 NUMA 感知。\n  - 包含事件计数器 `events[][]` 和 `events_local[][]`，用于触发 cgroup 通知。\n- `struct hugetlb_cgroup_per_node`：每个 NUMA 节点上的 HugeTLB cgroup 附加信息（当前未在代码片段中完整定义）。\n\n### 主要函数\n- `hugetlb_cgroup_css_alloc()` / `hugetlb_cgroup_css_free()`：cgroup 子系统实例的创建与销毁。\n- `hugetlb_cgroup_init()`：初始化新 cgroup 的 page_counter，设置最大限制为 `PAGE_COUNTER_MAX` 向下对齐到 HugeTLB 页面大小。\n- `__hugetlb_cgroup_charge_cgroup()` / `hugetlb_cgroup_charge_cgroup()`：对当前任务所属 cgroup 尝试 charge（计费）指定数量的 HugeTLB 页面。\n- `hugetlb_cgroup_move_parent()`：将属于某 cgroup 的 HugeTLB 页面迁移至其父 cgroup。\n- `hugetlb_cgroup_css_offline()`：在 cgroup 离线时，强制将其所有 HugeTLB 资源迁移至父级。\n- `hugetlb_event()`：向上冒泡记录 HugeTLB 事件（如达到限制 `HUGETLB_MAX`）并触发 cgroup 文件通知。\n- 辅助内联函数：\n  - `hugetlb_cgroup_from_css()` / `from_task()`：从 cgroup_subsys_state 或 task 获取 hugetlb_cgroup。\n  - `hugetlb_cgroup_counter_from_cgroup()` / `_rsvd()`：获取对应计费类型的 page_counter。\n  - `hugetlb_cgroup_is_root()` / `parent_hugetlb_cgroup()`：判断是否为根 cgroup 或获取父 cgroup。\n\n## 3. 关键实现\n\n### 资源计费机制\n- 使用 `page_counter` 子系统管理每种 HugeTLB 页面类型的用量和上限。\n- 支持两种独立的计费路径：\n  - **普通分配（fault）**：实际分配物理页面时计费。\n  - **预留（reservation）**：仅预留虚拟地址空间时计费（用于 mmap 等场景）。\n- 计费时通过 RCU 安全地获取当前任务的 cgroup，并使用 `css_tryget()` 确保引用有效性。\n- 若计费失败（超出限制），触发 `HUGETLB_MAX` 事件并通过 `cgroup_file_notify()` 通知用户空间。\n\n### cgroup 生命周期管理\n- **创建**：为每个在线 NUMA 节点分配 `hugetlb_cgroup_per_node` 结构；初始化所有 HugeTLB 类型的 page_counter，父子层级通过 `page_counter` 的 parent 字段建立级联关系。\n- **离线（offline）**：遍历所有 HugeTLB 页面的 active list，调用 `hugetlb_cgroup_move_parent()` 将页面所有权转移给父 cgroup。此过程循环执行直至当前 cgroup 无任何 HugeTLB 使用量，确保资源完全迁移。\n- **销毁**：释放 per-node 数据及主结构体。\n\n### 资源迁移（Reparenting）\n- `hugetlb_cgroup_move_parent()` 在持有 `hugetlb_lock` 时执行，确保页面不会被并发释放或迁移。\n- 仅处理属于目标 cgroup 的页面；若父 cgroup 为空（即目标为根），则 charge 到全局 root cgroup（无硬限制）。\n- 通过 `set_hugetlb_cgroup()` 更新页面所属的 cgroup。\n\n### 限制与优化\n- 对于 order 小于 `HUGETLB_CGROUP_MIN_ORDER`（通常为 3，即 8 个普通页 = 32KB）的 HugeTLB 页面，**不进行 cgroup 计费**，以减少小 HugeTLB 页面的开销。\n- 最大限制设为 `PAGE_COUNTER_MAX` 向下对齐到 HugeTLB 页面大小，避免跨页边界问题。\n- 当前 per-node 分配策略对 offline 节点也分配内存（使用 `NUMA_NO_NODE`），存在内存浪费，注释中指出未来可通过内存热插拔回调优化。\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/cgroup.h>`：cgroup 基础框架。\n  - `<linux/page_counter.h>`：提供层次化内存计数和限制功能。\n  - `<linux/hugetlb.h>`：HugeTLB 核心数据结构（如 `hstate`, `hugepage_activelist`）和锁（`hugetlb_lock`）。\n  - `<linux/hugetlb_cgroup.h>`：HugeTLB cgroup 的公共接口和数据结构定义。\n- **交互模块**：\n  - **HugeTLB 子系统**：在页面分配/释放、预留/取消预留等路径中调用本文件的 charge/uncharge 函数。\n  - **Memory cgroup (memcg)**：共享部分设计思想（如 page_counter），但 HugeTLB cgroup 是独立子系统。\n  - **Scheduler**：通过 `current` 获取当前任务的 cgroup 上下文。\n\n## 5. 使用场景\n\n- **容器资源隔离**：在 Kubernetes/Docker 等容器运行时中，通过 cgroup v1 的 `hugetlb` 子系统或 cgroup v2 的 `hugetlb.` 控制器，限制容器可使用的 HugeTLB 内存总量，防止单个容器耗尽系统大页资源。\n- **高性能计算（HPC）**：为不同 HPC 作业分配专用的 HugeTLB 内存配额，确保关键应用获得确定性内存性能。\n- **数据库优化**：Oracle、MySQL 等数据库使用 HugeTLB 提升 TLB 效率，通过 cgroup 限制其大页使用量，避免影响其他服务。\n- **资源监控与告警**：用户空间可通过读取 cgroup 的 `hugetlb.events` 文件监控 `max` 事件，实现基于 HugeTLB 使用量的自动扩缩容或告警。",
      "similarity": 0.49226176738739014,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 65,
          "end_line": 177,
          "content": [
            "static inline bool hugetlb_cgroup_is_root(struct hugetlb_cgroup *h_cg)",
            "{",
            "\treturn (h_cg == root_h_cgroup);",
            "}",
            "static inline bool hugetlb_cgroup_have_usage(struct hugetlb_cgroup *h_cg)",
            "{",
            "\tstruct hstate *h;",
            "",
            "\tfor_each_hstate(h) {",
            "\t\tif (page_counter_read(",
            "\t\t    hugetlb_cgroup_counter_from_cgroup(h_cg, hstate_index(h))))",
            "\t\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "static void hugetlb_cgroup_init(struct hugetlb_cgroup *h_cgroup,",
            "\t\t\t\tstruct hugetlb_cgroup *parent_h_cgroup)",
            "{",
            "\tint idx;",
            "",
            "\tfor (idx = 0; idx < HUGE_MAX_HSTATE; idx++) {",
            "\t\tstruct page_counter *fault_parent = NULL;",
            "\t\tstruct page_counter *rsvd_parent = NULL;",
            "\t\tunsigned long limit;",
            "\t\tint ret;",
            "",
            "\t\tif (parent_h_cgroup) {",
            "\t\t\tfault_parent = hugetlb_cgroup_counter_from_cgroup(",
            "\t\t\t\tparent_h_cgroup, idx);",
            "\t\t\trsvd_parent = hugetlb_cgroup_counter_from_cgroup_rsvd(",
            "\t\t\t\tparent_h_cgroup, idx);",
            "\t\t}",
            "\t\tpage_counter_init(hugetlb_cgroup_counter_from_cgroup(h_cgroup,",
            "\t\t\t\t\t\t\t\t     idx),",
            "\t\t\t\t  fault_parent, false);",
            "\t\tpage_counter_init(",
            "\t\t\thugetlb_cgroup_counter_from_cgroup_rsvd(h_cgroup, idx),",
            "\t\t\trsvd_parent, false);",
            "",
            "\t\tlimit = round_down(PAGE_COUNTER_MAX,",
            "\t\t\t\t   pages_per_huge_page(&hstates[idx]));",
            "",
            "\t\tret = page_counter_set_max(",
            "\t\t\thugetlb_cgroup_counter_from_cgroup(h_cgroup, idx),",
            "\t\t\tlimit);",
            "\t\tVM_BUG_ON(ret);",
            "\t\tret = page_counter_set_max(",
            "\t\t\thugetlb_cgroup_counter_from_cgroup_rsvd(h_cgroup, idx),",
            "\t\t\tlimit);",
            "\t\tVM_BUG_ON(ret);",
            "\t}",
            "}",
            "static void hugetlb_cgroup_free(struct hugetlb_cgroup *h_cgroup)",
            "{",
            "\tint node;",
            "",
            "\tfor_each_node(node)",
            "\t\tkfree(h_cgroup->nodeinfo[node]);",
            "\tkfree(h_cgroup);",
            "}",
            "static void hugetlb_cgroup_css_free(struct cgroup_subsys_state *css)",
            "{",
            "\thugetlb_cgroup_free(hugetlb_cgroup_from_css(css));",
            "}",
            "static void hugetlb_cgroup_move_parent(int idx, struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t       struct page *page)",
            "{",
            "\tunsigned int nr_pages;",
            "\tstruct page_counter *counter;",
            "\tstruct hugetlb_cgroup *page_hcg;",
            "\tstruct hugetlb_cgroup *parent = parent_hugetlb_cgroup(h_cg);",
            "\tstruct folio *folio = page_folio(page);",
            "",
            "\tpage_hcg = hugetlb_cgroup_from_folio(folio);",
            "\t/*",
            "\t * We can have pages in active list without any cgroup",
            "\t * ie, hugepage with less than 3 pages. We can safely",
            "\t * ignore those pages.",
            "\t */",
            "\tif (!page_hcg || page_hcg != h_cg)",
            "\t\tgoto out;",
            "",
            "\tnr_pages = compound_nr(page);",
            "\tif (!parent) {",
            "\t\tparent = root_h_cgroup;",
            "\t\t/* root has no limit */",
            "\t\tpage_counter_charge(&parent->hugepage[idx], nr_pages);",
            "\t}",
            "\tcounter = &h_cg->hugepage[idx];",
            "\t/* Take the pages off the local counter */",
            "\tpage_counter_cancel(counter, nr_pages);",
            "",
            "\tset_hugetlb_cgroup(folio, parent);",
            "out:",
            "\treturn;",
            "}",
            "static void hugetlb_cgroup_css_offline(struct cgroup_subsys_state *css)",
            "{",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(css);",
            "\tstruct hstate *h;",
            "\tstruct page *page;",
            "",
            "\tdo {",
            "\t\tfor_each_hstate(h) {",
            "\t\t\tspin_lock_irq(&hugetlb_lock);",
            "\t\t\tlist_for_each_entry(page, &h->hugepage_activelist, lru)",
            "\t\t\t\thugetlb_cgroup_move_parent(hstate_index(h), h_cg, page);",
            "",
            "\t\t\tspin_unlock_irq(&hugetlb_lock);",
            "\t\t}",
            "\t\tcond_resched();",
            "\t} while (hugetlb_cgroup_have_usage(h_cg));",
            "}"
          ],
          "function_name": "hugetlb_cgroup_is_root, hugetlb_cgroup_have_usage, hugetlb_cgroup_init, hugetlb_cgroup_free, hugetlb_cgroup_css_free, hugetlb_cgroup_move_parent, hugetlb_cgroup_css_offline",
          "description": "实现HugeTLB cgroup的核心管理逻辑，包含判断是否为根节点、检测使用量、初始化/释放cgroup结构、迁移父节点、处理cgroup离线状态等功能。",
          "similarity": 0.48954880237579346
        },
        {
          "chunk_id": 6,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 785,
          "end_line": 901,
          "content": [
            "static void __init __hugetlb_cgroup_file_legacy_init(int idx)",
            "{",
            "\tchar buf[32];",
            "\tstruct cftype *cft;",
            "\tstruct hstate *h = &hstates[idx];",
            "",
            "\t/* format the size */",
            "\tmem_fmt(buf, sizeof(buf), huge_page_size(h));",
            "",
            "\t/* Add the limit file */",
            "\tcft = &h->cgroup_files_legacy[0];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.limit_in_bytes\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_LIMIT);",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "\tcft->write = hugetlb_cgroup_write_legacy;",
            "",
            "\t/* Add the reservation limit file */",
            "\tcft = &h->cgroup_files_legacy[1];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.rsvd.limit_in_bytes\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_RSVD_LIMIT);",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "\tcft->write = hugetlb_cgroup_write_legacy;",
            "",
            "\t/* Add the usage file */",
            "\tcft = &h->cgroup_files_legacy[2];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.usage_in_bytes\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_USAGE);",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "",
            "\t/* Add the reservation usage file */",
            "\tcft = &h->cgroup_files_legacy[3];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.rsvd.usage_in_bytes\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_RSVD_USAGE);",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "",
            "\t/* Add the MAX usage file */",
            "\tcft = &h->cgroup_files_legacy[4];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.max_usage_in_bytes\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_MAX_USAGE);",
            "\tcft->write = hugetlb_cgroup_reset;",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "",
            "\t/* Add the MAX reservation usage file */",
            "\tcft = &h->cgroup_files_legacy[5];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.rsvd.max_usage_in_bytes\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_RSVD_MAX_USAGE);",
            "\tcft->write = hugetlb_cgroup_reset;",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "",
            "\t/* Add the failcntfile */",
            "\tcft = &h->cgroup_files_legacy[6];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.failcnt\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_FAILCNT);",
            "\tcft->write = hugetlb_cgroup_reset;",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "",
            "\t/* Add the reservation failcntfile */",
            "\tcft = &h->cgroup_files_legacy[7];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.rsvd.failcnt\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_RSVD_FAILCNT);",
            "\tcft->write = hugetlb_cgroup_reset;",
            "\tcft->read_u64 = hugetlb_cgroup_read_u64;",
            "",
            "\t/* Add the numa stat file */",
            "\tcft = &h->cgroup_files_legacy[8];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.numa_stat\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, 1);",
            "\tcft->seq_show = hugetlb_cgroup_read_numa_stat;",
            "",
            "\t/* NULL terminate the last cft */",
            "\tcft = &h->cgroup_files_legacy[9];",
            "\tmemset(cft, 0, sizeof(*cft));",
            "",
            "\tWARN_ON(cgroup_add_legacy_cftypes(&hugetlb_cgrp_subsys,",
            "\t\t\t\t\t  h->cgroup_files_legacy));",
            "}",
            "static void __init __hugetlb_cgroup_file_init(int idx)",
            "{",
            "\t__hugetlb_cgroup_file_dfl_init(idx);",
            "\t__hugetlb_cgroup_file_legacy_init(idx);",
            "}",
            "void __init hugetlb_cgroup_file_init(void)",
            "{",
            "\tstruct hstate *h;",
            "",
            "\tfor_each_hstate(h) {",
            "\t\t/*",
            "\t\t * Add cgroup control files only if the huge page consists",
            "\t\t * of more than two normal pages. This is because we use",
            "\t\t * page[2].private for storing cgroup details.",
            "\t\t */",
            "\t\tif (huge_page_order(h) >= HUGETLB_CGROUP_MIN_ORDER)",
            "\t\t\t__hugetlb_cgroup_file_init(hstate_index(h));",
            "\t}",
            "}",
            "void hugetlb_cgroup_migrate(struct folio *old_folio, struct folio *new_folio)",
            "{",
            "\tstruct hugetlb_cgroup *h_cg;",
            "\tstruct hugetlb_cgroup *h_cg_rsvd;",
            "\tstruct hstate *h = folio_hstate(old_folio);",
            "",
            "\tif (hugetlb_cgroup_disabled())",
            "\t\treturn;",
            "",
            "\tspin_lock_irq(&hugetlb_lock);",
            "\th_cg = hugetlb_cgroup_from_folio(old_folio);",
            "\th_cg_rsvd = hugetlb_cgroup_from_folio_rsvd(old_folio);",
            "\tset_hugetlb_cgroup(old_folio, NULL);",
            "\tset_hugetlb_cgroup_rsvd(old_folio, NULL);",
            "",
            "\t/* move the h_cg details to new cgroup */",
            "\tset_hugetlb_cgroup(new_folio, h_cg);",
            "\tset_hugetlb_cgroup_rsvd(new_folio, h_cg_rsvd);",
            "\tlist_move(&new_folio->lru, &h->hugepage_activelist);",
            "\tspin_unlock_irq(&hugetlb_lock);",
            "\treturn;",
            "}"
          ],
          "function_name": "__hugetlb_cgroup_file_legacy_init, __hugetlb_cgroup_file_init, hugetlb_cgroup_file_init, hugetlb_cgroup_migrate",
          "description": "初始化传统模式下的hugetlb cgroup文件系统接口，包含限额、使用量、最大使用量等监控文件，并实现页面迁移时的cgroup上下文切换逻辑，通过hugetlb_cgroup_migrate维护huge页面与cgroup的关联关系",
          "similarity": 0.4859023988246918
        },
        {
          "chunk_id": 4,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 520,
          "end_line": 626,
          "content": [
            "static u64 hugetlb_cgroup_read_u64(struct cgroup_subsys_state *css,",
            "\t\t\t\t   struct cftype *cft)",
            "{",
            "\tstruct page_counter *counter;",
            "\tstruct page_counter *rsvd_counter;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(css);",
            "",
            "\tcounter = &h_cg->hugepage[MEMFILE_IDX(cft->private)];",
            "\trsvd_counter = &h_cg->rsvd_hugepage[MEMFILE_IDX(cft->private)];",
            "",
            "\tswitch (MEMFILE_ATTR(cft->private)) {",
            "\tcase RES_USAGE:",
            "\t\treturn (u64)page_counter_read(counter) * PAGE_SIZE;",
            "\tcase RES_RSVD_USAGE:",
            "\t\treturn (u64)page_counter_read(rsvd_counter) * PAGE_SIZE;",
            "\tcase RES_LIMIT:",
            "\t\treturn (u64)counter->max * PAGE_SIZE;",
            "\tcase RES_RSVD_LIMIT:",
            "\t\treturn (u64)rsvd_counter->max * PAGE_SIZE;",
            "\tcase RES_MAX_USAGE:",
            "\t\treturn (u64)counter->watermark * PAGE_SIZE;",
            "\tcase RES_RSVD_MAX_USAGE:",
            "\t\treturn (u64)rsvd_counter->watermark * PAGE_SIZE;",
            "\tcase RES_FAILCNT:",
            "\t\treturn counter->failcnt;",
            "\tcase RES_RSVD_FAILCNT:",
            "\t\treturn rsvd_counter->failcnt;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static int hugetlb_cgroup_read_u64_max(struct seq_file *seq, void *v)",
            "{",
            "\tint idx;",
            "\tu64 val;",
            "\tstruct cftype *cft = seq_cft(seq);",
            "\tunsigned long limit;",
            "\tstruct page_counter *counter;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(seq_css(seq));",
            "",
            "\tidx = MEMFILE_IDX(cft->private);",
            "\tcounter = &h_cg->hugepage[idx];",
            "",
            "\tlimit = round_down(PAGE_COUNTER_MAX,",
            "\t\t\t   pages_per_huge_page(&hstates[idx]));",
            "",
            "\tswitch (MEMFILE_ATTR(cft->private)) {",
            "\tcase RES_RSVD_USAGE:",
            "\t\tcounter = &h_cg->rsvd_hugepage[idx];",
            "\t\tfallthrough;",
            "\tcase RES_USAGE:",
            "\t\tval = (u64)page_counter_read(counter);",
            "\t\tseq_printf(seq, \"%llu\\n\", val * PAGE_SIZE);",
            "\t\tbreak;",
            "\tcase RES_RSVD_LIMIT:",
            "\t\tcounter = &h_cg->rsvd_hugepage[idx];",
            "\t\tfallthrough;",
            "\tcase RES_LIMIT:",
            "\t\tval = (u64)counter->max;",
            "\t\tif (val == limit)",
            "\t\t\tseq_puts(seq, \"max\\n\");",
            "\t\telse",
            "\t\t\tseq_printf(seq, \"%llu\\n\", val * PAGE_SIZE);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static ssize_t hugetlb_cgroup_write(struct kernfs_open_file *of,",
            "\t\t\t\t    char *buf, size_t nbytes, loff_t off,",
            "\t\t\t\t    const char *max)",
            "{",
            "\tint ret, idx;",
            "\tunsigned long nr_pages;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(of_css(of));",
            "\tbool rsvd = false;",
            "",
            "\tif (hugetlb_cgroup_is_root(h_cg)) /* Can't set limit on root */",
            "\t\treturn -EINVAL;",
            "",
            "\tbuf = strstrip(buf);",
            "\tret = page_counter_memparse(buf, max, &nr_pages);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tidx = MEMFILE_IDX(of_cft(of)->private);",
            "\tnr_pages = round_down(nr_pages, pages_per_huge_page(&hstates[idx]));",
            "",
            "\tswitch (MEMFILE_ATTR(of_cft(of)->private)) {",
            "\tcase RES_RSVD_LIMIT:",
            "\t\trsvd = true;",
            "\t\tfallthrough;",
            "\tcase RES_LIMIT:",
            "\t\tmutex_lock(&hugetlb_limit_mutex);",
            "\t\tret = page_counter_set_max(",
            "\t\t\t__hugetlb_cgroup_counter_from_cgroup(h_cg, idx, rsvd),",
            "\t\t\tnr_pages);",
            "\t\tmutex_unlock(&hugetlb_limit_mutex);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tret = -EINVAL;",
            "\t\tbreak;",
            "\t}",
            "\treturn ret ?: nbytes;",
            "}"
          ],
          "function_name": "hugetlb_cgroup_read_u64, hugetlb_cgroup_read_u64_max, hugetlb_cgroup_write",
          "description": "提供HugeTLB cgroup的监控接口，实现读取当前使用量、限制等参数的功能，并支持通过接口设置内存限制参数。",
          "similarity": 0.48401325941085815
        },
        {
          "chunk_id": 3,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 381,
          "end_line": 500,
          "content": [
            "void hugetlb_cgroup_uncharge_folio(int idx, unsigned long nr_pages,",
            "\t\t\t\t  struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_uncharge_folio(idx, nr_pages, folio, false);",
            "}",
            "void hugetlb_cgroup_uncharge_folio_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t       struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_uncharge_folio(idx, nr_pages, folio, true);",
            "}",
            "static void __hugetlb_cgroup_uncharge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t     struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t\t     bool rsvd)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !h_cg)",
            "\t\treturn;",
            "",
            "\tif (huge_page_order(&hstates[idx]) < HUGETLB_CGROUP_MIN_ORDER)",
            "\t\treturn;",
            "",
            "\tpage_counter_uncharge(__hugetlb_cgroup_counter_from_cgroup(h_cg, idx,",
            "\t\t\t\t\t\t\t\t   rsvd),",
            "\t\t\t      nr_pages);",
            "",
            "\tif (rsvd)",
            "\t\tcss_put(&h_cg->css);",
            "}",
            "void hugetlb_cgroup_uncharge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t    struct hugetlb_cgroup *h_cg)",
            "{",
            "\t__hugetlb_cgroup_uncharge_cgroup(idx, nr_pages, h_cg, false);",
            "}",
            "void hugetlb_cgroup_uncharge_cgroup_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t struct hugetlb_cgroup *h_cg)",
            "{",
            "\t__hugetlb_cgroup_uncharge_cgroup(idx, nr_pages, h_cg, true);",
            "}",
            "void hugetlb_cgroup_uncharge_counter(struct resv_map *resv, unsigned long start,",
            "\t\t\t\t     unsigned long end)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !resv || !resv->reservation_counter ||",
            "\t    !resv->css)",
            "\t\treturn;",
            "",
            "\tpage_counter_uncharge(resv->reservation_counter,",
            "\t\t\t      (end - start) * resv->pages_per_hpage);",
            "\tcss_put(resv->css);",
            "}",
            "void hugetlb_cgroup_uncharge_file_region(struct resv_map *resv,",
            "\t\t\t\t\t struct file_region *rg,",
            "\t\t\t\t\t unsigned long nr_pages,",
            "\t\t\t\t\t bool region_del)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !resv || !rg || !nr_pages)",
            "\t\treturn;",
            "",
            "\tif (rg->reservation_counter && resv->pages_per_hpage &&",
            "\t    !resv->reservation_counter) {",
            "\t\tpage_counter_uncharge(rg->reservation_counter,",
            "\t\t\t\t      nr_pages * resv->pages_per_hpage);",
            "\t\t/*",
            "\t\t * Only do css_put(rg->css) when we delete the entire region",
            "\t\t * because one file_region must hold exactly one css reference.",
            "\t\t */",
            "\t\tif (region_del)",
            "\t\t\tcss_put(rg->css);",
            "\t}",
            "}",
            "static int hugetlb_cgroup_read_numa_stat(struct seq_file *seq, void *dummy)",
            "{",
            "\tint nid;",
            "\tstruct cftype *cft = seq_cft(seq);",
            "\tint idx = MEMFILE_IDX(cft->private);",
            "\tbool legacy = MEMFILE_ATTR(cft->private);",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(seq_css(seq));",
            "\tstruct cgroup_subsys_state *css;",
            "\tunsigned long usage;",
            "",
            "\tif (legacy) {",
            "\t\t/* Add up usage across all nodes for the non-hierarchical total. */",
            "\t\tusage = 0;",
            "\t\tfor_each_node_state(nid, N_MEMORY)",
            "\t\t\tusage += READ_ONCE(h_cg->nodeinfo[nid]->usage[idx]);",
            "\t\tseq_printf(seq, \"total=%lu\", usage * PAGE_SIZE);",
            "",
            "\t\t/* Simply print the per-node usage for the non-hierarchical total. */",
            "\t\tfor_each_node_state(nid, N_MEMORY)",
            "\t\t\tseq_printf(seq, \" N%d=%lu\", nid,",
            "\t\t\t\t   READ_ONCE(h_cg->nodeinfo[nid]->usage[idx]) *",
            "\t\t\t\t\t   PAGE_SIZE);",
            "\t\tseq_putc(seq, '\\n');",
            "\t}",
            "",
            "\t/*",
            "\t * The hierarchical total is pretty much the value recorded by the",
            "\t * counter, so use that.",
            "\t */",
            "\tseq_printf(seq, \"%stotal=%lu\", legacy ? \"hierarchical_\" : \"\",",
            "\t\t   page_counter_read(&h_cg->hugepage[idx]) * PAGE_SIZE);",
            "",
            "\t/*",
            "\t * For each node, transverse the css tree to obtain the hierarchical",
            "\t * node usage.",
            "\t */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\tusage = 0;",
            "\t\trcu_read_lock();",
            "\t\tcss_for_each_descendant_pre(css, &h_cg->css) {",
            "\t\t\tusage += READ_ONCE(hugetlb_cgroup_from_css(css)",
            "\t\t\t\t\t\t   ->nodeinfo[nid]",
            "\t\t\t\t\t\t   ->usage[idx]);",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t\tseq_printf(seq, \" N%d=%lu\", nid, usage * PAGE_SIZE);",
            "\t}",
            "",
            "\tseq_putc(seq, '\\n');",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "hugetlb_cgroup_uncharge_folio, hugetlb_cgroup_uncharge_folio_rsvd, __hugetlb_cgroup_uncharge_cgroup, hugetlb_cgroup_uncharge_cgroup, hugetlb_cgroup_uncharge_cgroup_rsvd, hugetlb_cgroup_uncharge_counter, hugetlb_cgroup_uncharge_file_region, hugetlb_cgroup_read_numa_stat",
          "description": "实现HugeTLB页面释放时的反向计费操作，包含对单个folio和整体cgroup的解除分配逻辑，以及读取NUMA节点统计信息的实现。",
          "similarity": 0.4627758860588074
        },
        {
          "chunk_id": 2,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 242,
          "end_line": 368,
          "content": [
            "static inline void hugetlb_event(struct hugetlb_cgroup *hugetlb, int idx,",
            "\t\t\t\t enum hugetlb_memory_event event)",
            "{",
            "\tatomic_long_inc(&hugetlb->events_local[idx][event]);",
            "\tcgroup_file_notify(&hugetlb->events_local_file[idx]);",
            "",
            "\tdo {",
            "\t\tatomic_long_inc(&hugetlb->events[idx][event]);",
            "\t\tcgroup_file_notify(&hugetlb->events_file[idx]);",
            "\t} while ((hugetlb = parent_hugetlb_cgroup(hugetlb)) &&",
            "\t\t !hugetlb_cgroup_is_root(hugetlb));",
            "}",
            "static int __hugetlb_cgroup_charge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t  struct hugetlb_cgroup **ptr,",
            "\t\t\t\t\t  bool rsvd)",
            "{",
            "\tint ret = 0;",
            "\tstruct page_counter *counter;",
            "\tstruct hugetlb_cgroup *h_cg = NULL;",
            "",
            "\tif (hugetlb_cgroup_disabled())",
            "\t\tgoto done;",
            "\t/*",
            "\t * We don't charge any cgroup if the compound page have less",
            "\t * than 3 pages.",
            "\t */",
            "\tif (huge_page_order(&hstates[idx]) < HUGETLB_CGROUP_MIN_ORDER)",
            "\t\tgoto done;",
            "again:",
            "\trcu_read_lock();",
            "\th_cg = hugetlb_cgroup_from_task(current);",
            "\tif (!css_tryget(&h_cg->css)) {",
            "\t\trcu_read_unlock();",
            "\t\tgoto again;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tif (!page_counter_try_charge(",
            "\t\t    __hugetlb_cgroup_counter_from_cgroup(h_cg, idx, rsvd),",
            "\t\t    nr_pages, &counter)) {",
            "\t\tret = -ENOMEM;",
            "\t\thugetlb_event(h_cg, idx, HUGETLB_MAX);",
            "\t\tcss_put(&h_cg->css);",
            "\t\tgoto done;",
            "\t}",
            "\t/* Reservations take a reference to the css because they do not get",
            "\t * reparented.",
            "\t */",
            "\tif (!rsvd)",
            "\t\tcss_put(&h_cg->css);",
            "done:",
            "\t*ptr = h_cg;",
            "\treturn ret;",
            "}",
            "int hugetlb_cgroup_charge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t struct hugetlb_cgroup **ptr)",
            "{",
            "\treturn __hugetlb_cgroup_charge_cgroup(idx, nr_pages, ptr, false);",
            "}",
            "int hugetlb_cgroup_charge_cgroup_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t      struct hugetlb_cgroup **ptr)",
            "{",
            "\treturn __hugetlb_cgroup_charge_cgroup(idx, nr_pages, ptr, true);",
            "}",
            "static void __hugetlb_cgroup_commit_charge(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t   struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t\t   struct folio *folio, bool rsvd)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !h_cg)",
            "\t\treturn;",
            "",
            "\t__set_hugetlb_cgroup(folio, h_cg, rsvd);",
            "\tif (!rsvd) {",
            "\t\tunsigned long usage =",
            "\t\t\th_cg->nodeinfo[folio_nid(folio)]->usage[idx];",
            "\t\t/*",
            "\t\t * This write is not atomic due to fetching usage and writing",
            "\t\t * to it, but that's fine because we call this with",
            "\t\t * hugetlb_lock held anyway.",
            "\t\t */",
            "\t\tWRITE_ONCE(h_cg->nodeinfo[folio_nid(folio)]->usage[idx],",
            "\t\t\t   usage + nr_pages);",
            "\t}",
            "}",
            "void hugetlb_cgroup_commit_charge(int idx, unsigned long nr_pages,",
            "\t\t\t\t  struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t  struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_commit_charge(idx, nr_pages, h_cg, folio, false);",
            "}",
            "void hugetlb_cgroup_commit_charge_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t       struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t       struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_commit_charge(idx, nr_pages, h_cg, folio, true);",
            "}",
            "static void __hugetlb_cgroup_uncharge_folio(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t   struct folio *folio, bool rsvd)",
            "{",
            "\tstruct hugetlb_cgroup *h_cg;",
            "",
            "\tif (hugetlb_cgroup_disabled())",
            "\t\treturn;",
            "\tlockdep_assert_held(&hugetlb_lock);",
            "\th_cg = __hugetlb_cgroup_from_folio(folio, rsvd);",
            "\tif (unlikely(!h_cg))",
            "\t\treturn;",
            "\t__set_hugetlb_cgroup(folio, NULL, rsvd);",
            "",
            "\tpage_counter_uncharge(__hugetlb_cgroup_counter_from_cgroup(h_cg, idx,",
            "\t\t\t\t\t\t\t\t   rsvd),",
            "\t\t\t      nr_pages);",
            "",
            "\tif (rsvd)",
            "\t\tcss_put(&h_cg->css);",
            "\telse {",
            "\t\tunsigned long usage =",
            "\t\t\th_cg->nodeinfo[folio_nid(folio)]->usage[idx];",
            "\t\t/*",
            "\t\t * This write is not atomic due to fetching usage and writing",
            "\t\t * to it, but that's fine because we call this with",
            "\t\t * hugetlb_lock held anyway.",
            "\t\t */",
            "\t\tWRITE_ONCE(h_cg->nodeinfo[folio_nid(folio)]->usage[idx],",
            "\t\t\t   usage - nr_pages);",
            "\t}",
            "}"
          ],
          "function_name": "hugetlb_event, __hugetlb_cgroup_charge_cgroup, hugetlb_cgroup_charge_cgroup, hugetlb_cgroup_charge_cgroup_rsvd, __hugetlb_cgroup_commit_charge, hugetlb_cgroup_commit_charge, hugetlb_cgroup_commit_charge_rsvd, __hugetlb_cgroup_uncharge_folio",
          "description": "处理HugeTLB页面分配时的计费操作，包含事件记录、尝试充电、提交充电等流程，区分普通页与保留页的计数逻辑，并维护各层级cgroup的使用统计。",
          "similarity": 0.4537044167518616
        }
      ]
    }
  ]
}