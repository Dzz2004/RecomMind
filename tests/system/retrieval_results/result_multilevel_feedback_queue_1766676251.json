{
  "query": "multilevel feedback queue",
  "timestamp": "2025-12-25 23:24:11",
  "retrieved_files": [
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5262048244476318,
      "chunks": [
        {
          "chunk_id": 11,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2012,
          "end_line": 2249,
          "content": [
            "static void",
            "rcu_report_qs_rdp(struct rcu_data *rdp)",
            "{",
            "\tunsigned long flags;",
            "\tunsigned long mask;",
            "\tbool needacc = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tWARN_ON_ONCE(rdp->cpu != smp_processor_id());",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\tif (rdp->cpu_no_qs.b.norm || rdp->gp_seq != rnp->gp_seq ||",
            "\t    rdp->gpwrap) {",
            "",
            "\t\t/*",
            "\t\t * The grace period in which this quiescent state was",
            "\t\t * recorded has ended, so don't report it upwards.",
            "\t\t * We will instead need a new quiescent state that lies",
            "\t\t * within the current grace period.",
            "\t\t */",
            "\t\trdp->cpu_no_qs.b.norm = true;\t/* need qs for new gp. */",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;",
            "\t}",
            "\tmask = rdp->grpmask;",
            "\trdp->core_needs_qs = false;",
            "\tif ((rnp->qsmask & mask) == 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t} else {",
            "\t\t/*",
            "\t\t * This GP can't end until cpu checks in, so all of our",
            "\t\t * callbacks can be processed during the next GP.",
            "\t\t *",
            "\t\t * NOCB kthreads have their own way to deal with that...",
            "\t\t */",
            "\t\tif (!rcu_rdp_is_offloaded(rdp)) {",
            "\t\t\t/*",
            "\t\t\t * The current GP has not yet ended, so it",
            "\t\t\t * should not be possible for rcu_accelerate_cbs()",
            "\t\t\t * to return true.  So complain, but don't awaken.",
            "\t\t\t */",
            "\t\t\tWARN_ON_ONCE(rcu_accelerate_cbs(rnp, rdp));",
            "\t\t} else if (!rcu_segcblist_completely_offloaded(&rdp->cblist)) {",
            "\t\t\t/*",
            "\t\t\t * ...but NOCB kthreads may miss or delay callbacks acceleration",
            "\t\t\t * if in the middle of a (de-)offloading process.",
            "\t\t\t */",
            "\t\t\tneedacc = true;",
            "\t\t}",
            "",
            "\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\t/* ^^^ Released rnp->lock */",
            "",
            "\t\tif (needacc) {",
            "\t\t\trcu_nocb_lock_irqsave(rdp, flags);",
            "\t\t\trcu_accelerate_cbs_unlocked(rnp, rdp);",
            "\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "\t\t}",
            "\t}",
            "}",
            "static void",
            "rcu_check_quiescent_state(struct rcu_data *rdp)",
            "{",
            "\t/* Check for grace-period ends and beginnings. */",
            "\tnote_gp_changes(rdp);",
            "",
            "\t/*",
            "\t * Does this CPU still need to do its part for current grace period?",
            "\t * If no, return and let the other CPUs do their part as well.",
            "\t */",
            "\tif (!rdp->core_needs_qs)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Was there a quiescent state since the beginning of the grace",
            "\t * period? If no, then exit and wait for the next call.",
            "\t */",
            "\tif (rdp->cpu_no_qs.b.norm)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Tell RCU we are done (but rcu_report_qs_rdp() will be the",
            "\t * judge of that).",
            "\t */",
            "\trcu_report_qs_rdp(rdp);",
            "}",
            "static bool rcu_do_batch_check_time(long count, long tlimit,",
            "\t\t\t\t    bool jlimit_check, unsigned long jlimit)",
            "{",
            "\t// Invoke local_clock() only once per 32 consecutive callbacks.",
            "\treturn unlikely(tlimit) &&",
            "\t       (!likely(count & 31) ||",
            "\t\t(IS_ENABLED(CONFIG_RCU_DOUBLE_CHECK_CB_TIME) &&",
            "\t\t jlimit_check && time_after(jiffies, jlimit))) &&",
            "\t       local_clock() >= tlimit;",
            "}",
            "static void rcu_do_batch(struct rcu_data *rdp)",
            "{",
            "\tlong bl;",
            "\tlong count = 0;",
            "\tint div;",
            "\tbool __maybe_unused empty;",
            "\tunsigned long flags;",
            "\tunsigned long jlimit;",
            "\tbool jlimit_check = false;",
            "\tlong pending;",
            "\tstruct rcu_cblist rcl = RCU_CBLIST_INITIALIZER(rcl);",
            "\tstruct rcu_head *rhp;",
            "\tlong tlimit = 0;",
            "",
            "\t/* If no callbacks are ready, just return. */",
            "\tif (!rcu_segcblist_ready_cbs(&rdp->cblist)) {",
            "\t\ttrace_rcu_batch_start(rcu_state.name,",
            "\t\t\t\t      rcu_segcblist_n_cbs(&rdp->cblist), 0);",
            "\t\ttrace_rcu_batch_end(rcu_state.name, 0,",
            "\t\t\t\t    !rcu_segcblist_empty(&rdp->cblist),",
            "\t\t\t\t    need_resched(), is_idle_task(current),",
            "\t\t\t\t    rcu_is_callbacks_kthread(rdp));",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Extract the list of ready callbacks, disabling IRQs to prevent",
            "\t * races with call_rcu() from interrupt handlers.  Leave the",
            "\t * callback counts, as rcu_barrier() needs to be conservative.",
            "\t */",
            "\trcu_nocb_lock_irqsave(rdp, flags);",
            "\tWARN_ON_ONCE(cpu_is_offline(smp_processor_id()));",
            "\tpending = rcu_segcblist_get_seglen(&rdp->cblist, RCU_DONE_TAIL);",
            "\tdiv = READ_ONCE(rcu_divisor);",
            "\tdiv = div < 0 ? 7 : div > sizeof(long) * 8 - 2 ? sizeof(long) * 8 - 2 : div;",
            "\tbl = max(rdp->blimit, pending >> div);",
            "\tif ((in_serving_softirq() || rdp->rcu_cpu_kthread_status == RCU_KTHREAD_RUNNING) &&",
            "\t    (IS_ENABLED(CONFIG_RCU_DOUBLE_CHECK_CB_TIME) || unlikely(bl > 100))) {",
            "\t\tconst long npj = NSEC_PER_SEC / HZ;",
            "\t\tlong rrn = READ_ONCE(rcu_resched_ns);",
            "",
            "\t\trrn = rrn < NSEC_PER_MSEC ? NSEC_PER_MSEC : rrn > NSEC_PER_SEC ? NSEC_PER_SEC : rrn;",
            "\t\ttlimit = local_clock() + rrn;",
            "\t\tjlimit = jiffies + (rrn + npj + 1) / npj;",
            "\t\tjlimit_check = true;",
            "\t}",
            "\ttrace_rcu_batch_start(rcu_state.name,",
            "\t\t\t      rcu_segcblist_n_cbs(&rdp->cblist), bl);",
            "\trcu_segcblist_extract_done_cbs(&rdp->cblist, &rcl);",
            "\tif (rcu_rdp_is_offloaded(rdp))",
            "\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCbDequeued\"));",
            "\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "",
            "\t/* Invoke callbacks. */",
            "\ttick_dep_set_task(current, TICK_DEP_BIT_RCU);",
            "\trhp = rcu_cblist_dequeue(&rcl);",
            "",
            "\tfor (; rhp; rhp = rcu_cblist_dequeue(&rcl)) {",
            "\t\trcu_callback_t f;",
            "",
            "\t\tcount++;",
            "\t\tdebug_rcu_head_unqueue(rhp);",
            "",
            "\t\trcu_lock_acquire(&rcu_callback_map);",
            "\t\ttrace_rcu_invoke_callback(rcu_state.name, rhp);",
            "",
            "\t\tf = rhp->func;",
            "\t\tdebug_rcu_head_callback(rhp);",
            "\t\tWRITE_ONCE(rhp->func, (rcu_callback_t)0L);",
            "\t\tf(rhp);",
            "",
            "\t\trcu_lock_release(&rcu_callback_map);",
            "",
            "\t\t/*",
            "\t\t * Stop only if limit reached and CPU has something to do.",
            "\t\t */",
            "\t\tif (in_serving_softirq()) {",
            "\t\t\tif (count >= bl && (need_resched() || !is_idle_task(current)))",
            "\t\t\t\tbreak;",
            "\t\t\t/*",
            "\t\t\t * Make sure we don't spend too much time here and deprive other",
            "\t\t\t * softirq vectors of CPU cycles.",
            "\t\t\t */",
            "\t\t\tif (rcu_do_batch_check_time(count, tlimit, jlimit_check, jlimit))",
            "\t\t\t\tbreak;",
            "\t\t} else {",
            "\t\t\t// In rcuc/rcuoc context, so no worries about",
            "\t\t\t// depriving other softirq vectors of CPU cycles.",
            "\t\t\tlocal_bh_enable();",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\t\tlocal_bh_disable();",
            "\t\t\t// But rcuc kthreads can delay quiescent-state",
            "\t\t\t// reporting, so check time limits for them.",
            "\t\t\tif (rdp->rcu_cpu_kthread_status == RCU_KTHREAD_RUNNING &&",
            "\t\t\t    rcu_do_batch_check_time(count, tlimit, jlimit_check, jlimit)) {",
            "\t\t\t\trdp->rcu_cpu_has_work = 1;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\trcu_nocb_lock_irqsave(rdp, flags);",
            "\trdp->n_cbs_invoked += count;",
            "\ttrace_rcu_batch_end(rcu_state.name, count, !!rcl.head, need_resched(),",
            "\t\t\t    is_idle_task(current), rcu_is_callbacks_kthread(rdp));",
            "",
            "\t/* Update counts and requeue any remaining callbacks. */",
            "\trcu_segcblist_insert_done_cbs(&rdp->cblist, &rcl);",
            "\trcu_segcblist_add_len(&rdp->cblist, -count);",
            "",
            "\t/* Reinstate batch limit if we have worked down the excess. */",
            "\tcount = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\tif (rdp->blimit >= DEFAULT_MAX_RCU_BLIMIT && count <= qlowmark)",
            "\t\trdp->blimit = blimit;",
            "",
            "\t/* Reset ->qlen_last_fqs_check trigger if enough CBs have drained. */",
            "\tif (count == 0 && rdp->qlen_last_fqs_check != 0) {",
            "\t\trdp->qlen_last_fqs_check = 0;",
            "\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t} else if (count < rdp->qlen_last_fqs_check - qhimark)",
            "\t\trdp->qlen_last_fqs_check = count;",
            "",
            "\t/*",
            "\t * The following usually indicates a double call_rcu().  To track",
            "\t * this down, try building with CONFIG_DEBUG_OBJECTS_RCU_HEAD=y.",
            "\t */",
            "\tempty = rcu_segcblist_empty(&rdp->cblist);",
            "\tWARN_ON_ONCE(count == 0 && !empty);",
            "\tWARN_ON_ONCE(!IS_ENABLED(CONFIG_RCU_NOCB_CPU) &&",
            "\t\t     count != 0 && empty);",
            "\tWARN_ON_ONCE(count == 0 && rcu_segcblist_n_segment_cbs(&rdp->cblist) != 0);",
            "\tWARN_ON_ONCE(!empty && rcu_segcblist_n_segment_cbs(&rdp->cblist) == 0);",
            "",
            "\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "",
            "\ttick_dep_clear_task(current, TICK_DEP_BIT_RCU);",
            "}"
          ],
          "function_name": "rcu_report_qs_rdp, rcu_check_quiescent_state, rcu_do_batch_check_time, rcu_do_batch",
          "description": "提供CPU级quiescent状态检测与回调处理机制，包含quiescent状态上报、回调批量处理及性能监控功能，支持动态调整批处理参数",
          "similarity": 0.5751173496246338
        },
        {
          "chunk_id": 21,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3787,
          "end_line": 3910,
          "content": [
            "void get_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\t/*",
            "\t * Any prior manipulation of RCU-protected data must happen",
            "\t * before the loads from ->gp_seq and ->expedited_sequence.",
            "\t */",
            "\tsmp_mb();  /* ^^^ */",
            "\trgosp->rgos_norm = rcu_seq_snap(&rnp->gp_seq);",
            "\trgosp->rgos_exp = rcu_seq_snap(&rcu_state.expedited_sequence);",
            "}",
            "static void start_poll_synchronize_rcu_common(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool needwake;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tlockdep_assert_irqs_enabled();",
            "\tlocal_irq_save(flags);",
            "\trdp = this_cpu_ptr(&rcu_data);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_rcu_node(rnp); // irqs already disabled.",
            "\t// Note it is possible for a grace period to have elapsed between",
            "\t// the above call to get_state_synchronize_rcu() and the below call",
            "\t// to rcu_seq_snap.  This is OK, the worst that happens is that we",
            "\t// get a grace period that no one needed.  These accesses are ordered",
            "\t// by smp_mb(), and we are accessing them in the opposite order",
            "\t// from which they are updated at grace-period start, as required.",
            "\tneedwake = rcu_start_this_gp(rnp, rdp, rcu_seq_snap(&rcu_state.gp_seq));",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "}",
            "unsigned long start_poll_synchronize_rcu(void)",
            "{",
            "\tunsigned long gp_seq = get_state_synchronize_rcu();",
            "",
            "\tstart_poll_synchronize_rcu_common();",
            "\treturn gp_seq;",
            "}",
            "void start_poll_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tget_state_synchronize_rcu_full(rgosp);",
            "",
            "\tstart_poll_synchronize_rcu_common();",
            "}",
            "bool poll_state_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\tif (oldstate == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rcu_state.gp_seq_polled, oldstate)) {",
            "\t\tsmp_mb(); /* Ensure GP ends before subsequent accesses. */",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "bool poll_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tsmp_mb(); // Order against root rcu_node structure grace-period cleanup.",
            "\tif (rgosp->rgos_norm == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rnp->gp_seq, rgosp->rgos_norm) ||",
            "\t    rgosp->rgos_exp == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rcu_state.expedited_sequence, rgosp->rgos_exp)) {",
            "\t\tsmp_mb(); /* Ensure GP ends before subsequent accesses. */",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "void cond_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\tif (!poll_state_synchronize_rcu(oldstate))",
            "\t\tsynchronize_rcu();",
            "}",
            "void cond_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tif (!poll_state_synchronize_rcu_full(rgosp))",
            "\t\tsynchronize_rcu();",
            "}",
            "static int rcu_pending(int user)",
            "{",
            "\tbool gp_in_progress;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "\tstruct rcu_node *rnp = rdp->mynode;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\t/* Check for CPU stalls, if enabled. */",
            "\tcheck_cpu_stall(rdp);",
            "",
            "\t/* Does this CPU need a deferred NOCB wakeup? */",
            "\tif (rcu_nocb_need_deferred_wakeup(rdp, RCU_NOCB_WAKE))",
            "\t\treturn 1;",
            "",
            "\t/* Is this a nohz_full CPU in userspace or idle?  (Ignore RCU if so.) */",
            "\tif ((user || rcu_is_cpu_rrupt_from_idle()) && rcu_nohz_full_cpu())",
            "\t\treturn 0;",
            "",
            "\t/* Is the RCU core waiting for a quiescent state from this CPU? */",
            "\tgp_in_progress = rcu_gp_in_progress();",
            "\tif (rdp->core_needs_qs && !rdp->cpu_no_qs.b.norm && gp_in_progress)",
            "\t\treturn 1;",
            "",
            "\t/* Does this CPU have callbacks ready to invoke? */",
            "\tif (!rcu_rdp_is_offloaded(rdp) &&",
            "\t    rcu_segcblist_ready_cbs(&rdp->cblist))",
            "\t\treturn 1;",
            "",
            "\t/* Has RCU gone idle with this CPU needing another grace period? */",
            "\tif (!gp_in_progress && rcu_segcblist_is_enabled(&rdp->cblist) &&",
            "\t    !rcu_rdp_is_offloaded(rdp) &&",
            "\t    !rcu_segcblist_restempty(&rdp->cblist, RCU_NEXT_READY_TAIL))",
            "\t\treturn 1;",
            "",
            "\t/* Have RCU grace period completed or started?  */",
            "\tif (rcu_seq_current(&rnp->gp_seq) != rdp->gp_seq ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) /* outside lock */",
            "\t\treturn 1;",
            "",
            "\t/* nothing to do */",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_state_synchronize_rcu_full, start_poll_synchronize_rcu_common, start_poll_synchronize_rcu, start_poll_synchronize_rcu_full, poll_state_synchronize_rcu, poll_state_synchronize_rcu_full, cond_synchronize_rcu, cond_synchronize_rcu_full, rcu_pending",
          "description": "提供RCU宽限期状态查询和触发机制，通过序列号比对判断是否需要启动新的宽限期，处理回调队列唤醒逻辑，实现条件同步检查",
          "similarity": 0.5620958209037781
        },
        {
          "chunk_id": 27,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4895,
          "end_line": 5082,
          "content": [
            "static void __init rcu_init_one(void)",
            "{",
            "\tstatic const char * const buf[] = RCU_NODE_NAME_INIT;",
            "\tstatic const char * const fqs[] = RCU_FQS_NAME_INIT;",
            "\tstatic struct lock_class_key rcu_node_class[RCU_NUM_LVLS];",
            "\tstatic struct lock_class_key rcu_fqs_class[RCU_NUM_LVLS];",
            "",
            "\tint levelspread[RCU_NUM_LVLS];\t\t/* kids/node in each level. */",
            "\tint cpustride = 1;",
            "\tint i;",
            "\tint j;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tBUILD_BUG_ON(RCU_NUM_LVLS > ARRAY_SIZE(buf));  /* Fix buf[] init! */",
            "",
            "\t/* Silence gcc 4.8 false positive about array index out of range. */",
            "\tif (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)",
            "\t\tpanic(\"rcu_init_one: rcu_num_lvls out of range\");",
            "",
            "\t/* Initialize the level-tracking arrays. */",
            "",
            "\tfor (i = 1; i < rcu_num_lvls; i++)",
            "\t\trcu_state.level[i] =",
            "\t\t\trcu_state.level[i - 1] + num_rcu_lvl[i - 1];",
            "\trcu_init_levelspread(levelspread, num_rcu_lvl);",
            "",
            "\t/* Initialize the elements themselves, starting from the leaves. */",
            "",
            "\tfor (i = rcu_num_lvls - 1; i >= 0; i--) {",
            "\t\tcpustride *= levelspread[i];",
            "\t\trnp = rcu_state.level[i];",
            "\t\tfor (j = 0; j < num_rcu_lvl[i]; j++, rnp++) {",
            "\t\t\traw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));",
            "\t\t\tlockdep_set_class_and_name(&ACCESS_PRIVATE(rnp, lock),",
            "\t\t\t\t\t\t   &rcu_node_class[i], buf[i]);",
            "\t\t\traw_spin_lock_init(&rnp->fqslock);",
            "\t\t\tlockdep_set_class_and_name(&rnp->fqslock,",
            "\t\t\t\t\t\t   &rcu_fqs_class[i], fqs[i]);",
            "\t\t\trnp->gp_seq = rcu_state.gp_seq;",
            "\t\t\trnp->gp_seq_needed = rcu_state.gp_seq;",
            "\t\t\trnp->completedqs = rcu_state.gp_seq;",
            "\t\t\trnp->qsmask = 0;",
            "\t\t\trnp->qsmaskinit = 0;",
            "\t\t\trnp->grplo = j * cpustride;",
            "\t\t\trnp->grphi = (j + 1) * cpustride - 1;",
            "\t\t\tif (rnp->grphi >= nr_cpu_ids)",
            "\t\t\t\trnp->grphi = nr_cpu_ids - 1;",
            "\t\t\tif (i == 0) {",
            "\t\t\t\trnp->grpnum = 0;",
            "\t\t\t\trnp->grpmask = 0;",
            "\t\t\t\trnp->parent = NULL;",
            "\t\t\t} else {",
            "\t\t\t\trnp->grpnum = j % levelspread[i - 1];",
            "\t\t\t\trnp->grpmask = BIT(rnp->grpnum);",
            "\t\t\t\trnp->parent = rcu_state.level[i - 1] +",
            "\t\t\t\t\t      j / levelspread[i - 1];",
            "\t\t\t}",
            "\t\t\trnp->level = i;",
            "\t\t\tINIT_LIST_HEAD(&rnp->blkd_tasks);",
            "\t\t\trcu_init_one_nocb(rnp);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[0]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[1]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[2]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[3]);",
            "\t\t\tspin_lock_init(&rnp->exp_lock);",
            "\t\t\tmutex_init(&rnp->boost_kthread_mutex);",
            "\t\t\traw_spin_lock_init(&rnp->exp_poll_lock);",
            "\t\t\trnp->exp_seq_poll_rq = RCU_GET_STATE_COMPLETED;",
            "\t\t\tINIT_WORK(&rnp->exp_poll_wq, sync_rcu_do_polled_gp);",
            "\t\t}",
            "\t}",
            "",
            "\tinit_swait_queue_head(&rcu_state.gp_wq);",
            "\tinit_swait_queue_head(&rcu_state.expedited_wq);",
            "\trnp = rcu_first_leaf_node();",
            "\tfor_each_possible_cpu(i) {",
            "\t\twhile (i > rnp->grphi)",
            "\t\t\trnp++;",
            "\t\tper_cpu_ptr(&rcu_data, i)->mynode = rnp;",
            "\t\trcu_boot_init_percpu_data(i);",
            "\t}",
            "}",
            "static void __init sanitize_kthread_prio(void)",
            "{",
            "\tint kthread_prio_in = kthread_prio;",
            "",
            "\tif (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 2",
            "\t    && IS_BUILTIN(CONFIG_RCU_TORTURE_TEST))",
            "\t\tkthread_prio = 2;",
            "\telse if (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 1)",
            "\t\tkthread_prio = 1;",
            "\telse if (kthread_prio < 0)",
            "\t\tkthread_prio = 0;",
            "\telse if (kthread_prio > 99)",
            "\t\tkthread_prio = 99;",
            "",
            "\tif (kthread_prio != kthread_prio_in)",
            "\t\tpr_alert(\"%s: Limited prio to %d from %d\\n\",",
            "\t\t\t __func__, kthread_prio, kthread_prio_in);",
            "}",
            "void rcu_init_geometry(void)",
            "{",
            "\tulong d;",
            "\tint i;",
            "\tstatic unsigned long old_nr_cpu_ids;",
            "\tint rcu_capacity[RCU_NUM_LVLS];",
            "\tstatic bool initialized;",
            "",
            "\tif (initialized) {",
            "\t\t/*",
            "\t\t * Warn if setup_nr_cpu_ids() had not yet been invoked,",
            "\t\t * unless nr_cpus_ids == NR_CPUS, in which case who cares?",
            "\t\t */",
            "\t\tWARN_ON_ONCE(old_nr_cpu_ids != nr_cpu_ids);",
            "\t\treturn;",
            "\t}",
            "",
            "\told_nr_cpu_ids = nr_cpu_ids;",
            "\tinitialized = true;",
            "",
            "\t/*",
            "\t * Initialize any unspecified boot parameters.",
            "\t * The default values of jiffies_till_first_fqs and",
            "\t * jiffies_till_next_fqs are set to the RCU_JIFFIES_TILL_FORCE_QS",
            "\t * value, which is a function of HZ, then adding one for each",
            "\t * RCU_JIFFIES_FQS_DIV CPUs that might be on the system.",
            "\t */",
            "\td = RCU_JIFFIES_TILL_FORCE_QS + nr_cpu_ids / RCU_JIFFIES_FQS_DIV;",
            "\tif (jiffies_till_first_fqs == ULONG_MAX)",
            "\t\tjiffies_till_first_fqs = d;",
            "\tif (jiffies_till_next_fqs == ULONG_MAX)",
            "\t\tjiffies_till_next_fqs = d;",
            "\tadjust_jiffies_till_sched_qs();",
            "",
            "\t/* If the compile-time values are accurate, just leave. */",
            "\tif (rcu_fanout_leaf == RCU_FANOUT_LEAF &&",
            "\t    nr_cpu_ids == NR_CPUS)",
            "\t\treturn;",
            "\tpr_info(\"Adjusting geometry for rcu_fanout_leaf=%d, nr_cpu_ids=%u\\n\",",
            "\t\trcu_fanout_leaf, nr_cpu_ids);",
            "",
            "\t/*",
            "\t * The boot-time rcu_fanout_leaf parameter must be at least two",
            "\t * and cannot exceed the number of bits in the rcu_node masks.",
            "\t * Complain and fall back to the compile-time values if this",
            "\t * limit is exceeded.",
            "\t */",
            "\tif (rcu_fanout_leaf < 2 ||",
            "\t    rcu_fanout_leaf > sizeof(unsigned long) * 8) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Compute number of nodes that can be handled an rcu_node tree",
            "\t * with the given number of levels.",
            "\t */",
            "\trcu_capacity[0] = rcu_fanout_leaf;",
            "\tfor (i = 1; i < RCU_NUM_LVLS; i++)",
            "\t\trcu_capacity[i] = rcu_capacity[i - 1] * RCU_FANOUT;",
            "",
            "\t/*",
            "\t * The tree must be able to accommodate the configured number of CPUs.",
            "\t * If this limit is exceeded, fall back to the compile-time values.",
            "\t */",
            "\tif (nr_cpu_ids > rcu_capacity[RCU_NUM_LVLS - 1]) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Calculate the number of levels in the tree. */",
            "\tfor (i = 0; nr_cpu_ids > rcu_capacity[i]; i++) {",
            "\t}",
            "\trcu_num_lvls = i + 1;",
            "",
            "\t/* Calculate the number of rcu_nodes at each level of the tree. */",
            "\tfor (i = 0; i < rcu_num_lvls; i++) {",
            "\t\tint cap = rcu_capacity[(rcu_num_lvls - 1) - i];",
            "\t\tnum_rcu_lvl[i] = DIV_ROUND_UP(nr_cpu_ids, cap);",
            "\t}",
            "",
            "\t/* Calculate the total number of rcu_node structures. */",
            "\trcu_num_nodes = 0;",
            "\tfor (i = 0; i < rcu_num_lvls; i++)",
            "\t\trcu_num_nodes += num_rcu_lvl[i];",
            "}"
          ],
          "function_name": "rcu_init_one, sanitize_kthread_prio, rcu_init_geometry",
          "description": "构建多级RCU节点树结构，初始化各层级的锁类和节点属性，动态调整RCU树的几何形态以适配当前CPU数量和层级分布需求。",
          "similarity": 0.560571551322937
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 908,
          "end_line": 1026,
          "content": [
            "static void trace_rcu_this_gp(struct rcu_node *rnp, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req, const char *s)",
            "{",
            "\ttrace_rcu_future_grace_period(rcu_state.name, READ_ONCE(rnp->gp_seq),",
            "\t\t\t\t      gp_seq_req, rnp->level,",
            "\t\t\t\t      rnp->grplo, rnp->grphi, s);",
            "}",
            "static bool rcu_start_this_gp(struct rcu_node *rnp_start, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req)",
            "{",
            "\tbool ret = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\t/*",
            "\t * Use funnel locking to either acquire the root rcu_node",
            "\t * structure's lock or bail out if the need for this grace period",
            "\t * has already been recorded -- or if that grace period has in",
            "\t * fact already started.  If there is already a grace period in",
            "\t * progress in a non-leaf node, no recording is needed because the",
            "\t * end of the grace period will scan the leaf rcu_node structures.",
            "\t * Note that rnp_start->lock must not be released.",
            "\t */",
            "\traw_lockdep_assert_held_rcu_node(rnp_start);",
            "\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req, TPS(\"Startleaf\"));",
            "\tfor (rnp = rnp_start; 1; rnp = rnp->parent) {",
            "\t\tif (rnp != rnp_start)",
            "\t\t\traw_spin_lock_rcu_node(rnp);",
            "\t\tif (ULONG_CMP_GE(rnp->gp_seq_needed, gp_seq_req) ||",
            "\t\t    rcu_seq_started(&rnp->gp_seq, gp_seq_req) ||",
            "\t\t    (rnp != rnp_start &&",
            "\t\t     rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))) {",
            "\t\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Prestarted\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tWRITE_ONCE(rnp->gp_seq_needed, gp_seq_req);",
            "\t\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq))) {",
            "\t\t\t/*",
            "\t\t\t * We just marked the leaf or internal node, and a",
            "\t\t\t * grace period is in progress, which means that",
            "\t\t\t * rcu_gp_cleanup() will see the marking.  Bail to",
            "\t\t\t * reduce contention.",
            "\t\t\t */",
            "\t\t\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Startedleaf\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tif (rnp != rnp_start && rnp->parent != NULL)",
            "\t\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\tif (!rnp->parent)",
            "\t\t\tbreak;  /* At root, and perhaps also leaf. */",
            "\t}",
            "",
            "\t/* If GP already in progress, just leave, otherwise start one. */",
            "\tif (rcu_gp_in_progress()) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedleafroot\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedroot\"));",
            "\tWRITE_ONCE(rcu_state.gp_flags, rcu_state.gp_flags | RCU_GP_FLAG_INIT);",
            "\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);",
            "\tif (!READ_ONCE(rcu_state.gp_kthread)) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"NoGPkthread\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_grace_period(rcu_state.name, data_race(rcu_state.gp_seq), TPS(\"newreq\"));",
            "\tret = true;  /* Caller must wake GP kthread. */",
            "unlock_out:",
            "\t/* Push furthest requested GP to leaf node and rcu_data structure. */",
            "\tif (ULONG_CMP_LT(gp_seq_req, rnp->gp_seq_needed)) {",
            "\t\tWRITE_ONCE(rnp_start->gp_seq_needed, rnp->gp_seq_needed);",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\t}",
            "\tif (rnp != rnp_start)",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\treturn ret;",
            "}",
            "static bool rcu_future_gp_cleanup(struct rcu_node *rnp)",
            "{",
            "\tbool needmore;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tneedmore = ULONG_CMP_LT(rnp->gp_seq, rnp->gp_seq_needed);",
            "\tif (!needmore)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq; /* Avoid counter wrap. */",
            "\ttrace_rcu_this_gp(rnp, rdp, rnp->gp_seq,",
            "\t\t\t  needmore ? TPS(\"CleanupMore\") : TPS(\"Cleanup\"));",
            "\treturn needmore;",
            "}",
            "static void swake_up_one_online_ipi(void *arg)",
            "{",
            "\tstruct swait_queue_head *wqh = arg;",
            "",
            "\tswake_up_one(wqh);",
            "}",
            "static void swake_up_one_online(struct swait_queue_head *wqh)",
            "{",
            "\tint cpu = get_cpu();",
            "",
            "\t/*",
            "\t * If called from rcutree_report_cpu_starting(), wake up",
            "\t * is dangerous that late in the CPU-down hotplug process. The",
            "\t * scheduler might queue an ignored hrtimer. Defer the wake up",
            "\t * to an online CPU instead.",
            "\t */",
            "\tif (unlikely(cpu_is_offline(cpu))) {",
            "\t\tint target;",
            "",
            "\t\ttarget = cpumask_any_and(housekeeping_cpumask(HK_TYPE_RCU),",
            "\t\t\t\t\t cpu_online_mask);",
            "",
            "\t\tsmp_call_function_single(target, swake_up_one_online_ipi,",
            "\t\t\t\t\t wqh, 0);",
            "\t\tput_cpu();",
            "\t} else {",
            "\t\tput_cpu();",
            "\t\tswake_up_one(wqh);",
            "\t}",
            "}"
          ],
          "function_name": "trace_rcu_this_gp, rcu_start_this_gp, rcu_future_gp_cleanup, swake_up_one_online_ipi, swake_up_one_online",
          "description": "实现RCU grace period事件追踪、新grace period启动逻辑及未来grace period清理机制，支持跨层级节点的同步状态传播。",
          "similarity": 0.5532407760620117
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1221,
          "end_line": 1330,
          "content": [
            "static bool __note_gp_changes(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\tbool ret = false;",
            "\tbool need_qs;",
            "\tconst bool offloaded = rcu_rdp_is_offloaded(rdp);",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\tif (rdp->gp_seq == rnp->gp_seq)",
            "\t\treturn false; /* Nothing to do. */",
            "",
            "\t/* Handle the ends of any preceding grace periods first. */",
            "\tif (rcu_seq_completed_gp(rdp->gp_seq, rnp->gp_seq) ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) {",
            "\t\tif (!offloaded)",
            "\t\t\tret = rcu_advance_cbs(rnp, rdp); /* Advance CBs. */",
            "\t\trdp->core_needs_qs = false;",
            "\t\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuend\"));",
            "\t} else {",
            "\t\tif (!offloaded)",
            "\t\t\tret = rcu_accelerate_cbs(rnp, rdp); /* Recent CBs. */",
            "\t\tif (rdp->core_needs_qs)",
            "\t\t\trdp->core_needs_qs = !!(rnp->qsmask & rdp->grpmask);",
            "\t}",
            "",
            "\t/* Now handle the beginnings of any new-to-this-CPU grace periods. */",
            "\tif (rcu_seq_new_gp(rdp->gp_seq, rnp->gp_seq) ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) {",
            "\t\t/*",
            "\t\t * If the current grace period is waiting for this CPU,",
            "\t\t * set up to detect a quiescent state, otherwise don't",
            "\t\t * go looking for one.",
            "\t\t */",
            "\t\ttrace_rcu_grace_period(rcu_state.name, rnp->gp_seq, TPS(\"cpustart\"));",
            "\t\tneed_qs = !!(rnp->qsmask & rdp->grpmask);",
            "\t\trdp->cpu_no_qs.b.norm = need_qs;",
            "\t\trdp->core_needs_qs = need_qs;",
            "\t\tzero_cpu_stall_ticks(rdp);",
            "\t}",
            "\trdp->gp_seq = rnp->gp_seq;  /* Remember new grace-period state. */",
            "\tif (ULONG_CMP_LT(rdp->gp_seq_needed, rnp->gp_seq_needed) || rdp->gpwrap)",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU) && READ_ONCE(rdp->gpwrap))",
            "\t\tWRITE_ONCE(rdp->last_sched_clock, jiffies);",
            "\tWRITE_ONCE(rdp->gpwrap, false);",
            "\trcu_gpnum_ovf(rnp, rdp);",
            "\treturn ret;",
            "}",
            "static void note_gp_changes(struct rcu_data *rdp)",
            "{",
            "\tunsigned long flags;",
            "\tbool needwake;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tlocal_irq_save(flags);",
            "\trnp = rdp->mynode;",
            "\tif ((rdp->gp_seq == rcu_seq_current(&rnp->gp_seq) &&",
            "\t     !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */",
            "\t    !raw_spin_trylock_rcu_node(rnp)) { /* irqs already off, so later. */",
            "\t\tlocal_irq_restore(flags);",
            "\t\treturn;",
            "\t}",
            "\tneedwake = __note_gp_changes(rnp, rdp);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\trcu_strict_gp_check_qs();",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "}",
            "void rcu_gp_slow_register(atomic_t *rgssp)",
            "{",
            "\tWARN_ON_ONCE(rcu_gp_slow_suppress);",
            "",
            "\tWRITE_ONCE(rcu_gp_slow_suppress, rgssp);",
            "}",
            "void rcu_gp_slow_unregister(atomic_t *rgssp)",
            "{",
            "\tWARN_ON_ONCE(rgssp && rgssp != rcu_gp_slow_suppress && rcu_gp_slow_suppress != NULL);",
            "",
            "\tWRITE_ONCE(rcu_gp_slow_suppress, NULL);",
            "}",
            "static bool rcu_gp_slow_is_suppressed(void)",
            "{",
            "\tatomic_t *rgssp = READ_ONCE(rcu_gp_slow_suppress);",
            "",
            "\treturn rgssp && atomic_read(rgssp);",
            "}",
            "static void rcu_gp_slow(int delay)",
            "{",
            "\tif (!rcu_gp_slow_is_suppressed() && delay > 0 &&",
            "\t    !(rcu_seq_ctr(rcu_state.gp_seq) % (rcu_num_nodes * PER_RCU_NODE_PERIOD * delay)))",
            "\t\tschedule_timeout_idle(delay);",
            "}",
            "void rcu_gp_set_torture_wait(int duration)",
            "{",
            "\tif (IS_ENABLED(CONFIG_RCU_TORTURE_TEST) && duration > 0)",
            "\t\tWRITE_ONCE(sleep_duration, duration);",
            "}",
            "static void rcu_gp_torture_wait(void)",
            "{",
            "\tunsigned long duration;",
            "",
            "\tif (!IS_ENABLED(CONFIG_RCU_TORTURE_TEST))",
            "\t\treturn;",
            "\tduration = xchg(&sleep_duration, 0UL);",
            "\tif (duration > 0) {",
            "\t\tpr_alert(\"%s: Waiting %lu jiffies\\n\", __func__, duration);",
            "\t\tschedule_timeout_idle(duration);",
            "\t\tpr_alert(\"%s: Wait complete\\n\", __func__);",
            "\t}",
            "}"
          ],
          "function_name": "__note_gp_changes, note_gp_changes, rcu_gp_slow_register, rcu_gp_slow_unregister, rcu_gp_slow_is_suppressed, rcu_gp_slow, rcu_gp_set_torture_wait, rcu_gp_torture_wait",
          "description": "提供慢速模式下的grace period注册/注销接口，通过原子变量控制抑制状态，并实现基于延迟的调度控制逻辑，支持严格模式下的边界检查。",
          "similarity": 0.53279709815979
        }
      ]
    },
    {
      "source_file": "kernel/locking/semaphore.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:52:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\semaphore.c`\n\n---\n\n# `locking/semaphore.c` 技术文档\n\n## 1. 文件概述\n\n`locking/semaphore.c` 实现了 Linux 内核中的**计数信号量（counting semaphore）**机制。计数信号量允许多个任务（最多为初始计数值）同时持有该锁，当计数值耗尽时，后续请求者将被阻塞，直到有其他任务释放信号量。与互斥锁（mutex）不同，信号量支持更灵活的并发控制，适用于资源池、限流等场景。该文件提供了多种获取和释放信号量的接口，包括可中断、可超时、不可中断等变体，并支持在中断上下文中调用部分函数。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `down(struct semaphore *sem)` | 不可中断地获取信号量，若不可用则睡眠。**已弃用**，建议使用可中断版本。 |\n| `down_interruptible(struct semaphore *sem)` | 可被普通信号中断的获取操作，成功返回 0，被信号中断返回 `-EINTR`。 |\n| `down_killable(struct semaphore *sem)` | 可被致命信号（fatal signal）中断的获取操作，返回值同上。 |\n| `down_trylock(struct semaphore *sem)` | 非阻塞尝试获取信号量，成功返回 0，失败返回 1（**注意返回值与 mutex/spinlock 相反**）。 |\n| `down_timeout(struct semaphore *sem, long timeout)` | 带超时的获取操作，超时返回 `-ETIME`，成功返回 0。 |\n| `up(struct semaphore *sem)` | 释放信号量，可由任意上下文（包括中断）调用，唤醒等待队列中的任务。 |\n\n### 静态辅助函数\n\n- `__down*()` 系列：处理信号量争用时的阻塞逻辑。\n- `__up()`：在有等待者时执行唤醒逻辑。\n- `___down_common()`：通用的阻塞等待实现，支持不同睡眠状态和超时。\n- `__sem_acquire()`：原子减少计数并记录持有者（用于 hung task 检测）。\n\n### 数据结构\n\n- `struct semaphore`（定义在 `<linux/semaphore.h>`）：\n  - `count`：当前可用资源数（>0 表示可立即获取）。\n  - `wait_list`：等待该信号量的任务链表。\n  - `lock`：保护上述成员的原始自旋锁（`raw_spinlock_t`）。\n  - `last_holder`（条件编译）：记录最后持有者，用于 `CONFIG_DETECT_HUNG_TASK_BLOCKER`。\n\n- `struct semaphore_waiter`：\n  - 用于将任务加入等待队列，包含任务指针和唤醒标志（`up`）。\n\n## 3. 关键实现\n\n### 中断安全与自旋锁\n- 所有对外接口（包括 `down*` 和 `up`）均使用 `raw_spin_lock_irqsave()` 获取自旋锁，确保在中断上下文安全。\n- 即使 `down()` 等函数通常在进程上下文调用，也使用 `irqsave` 变体，因为内核某些部分依赖在中断上下文成功调用 `down()`（当确定信号量可用时）。\n\n### 计数语义\n- `sem->count` 表示**还可被获取的次数**。初始值由 `sema_init()` 设置。\n- 获取时：若 `count > 0`，直接减 1；否则加入等待队列。\n- 释放时：若等待队列为空，`count++`；否则唤醒队首任务。\n\n### 等待与唤醒机制\n- 使用 `wake_q`（批量唤醒队列）优化唤醒路径，避免在持有自旋锁时调用 `wake_up_process()`。\n- 等待任务通过 `schedule_timeout()` 睡眠，并在循环中检查：\n  - 是否收到信号（根据睡眠状态判断）。\n  - 是否超时。\n  - 是否被 `__up()` 标记为 `waiter.up = true`（表示已被选中唤醒）。\n\n### Hung Task 支持\n- 当启用 `CONFIG_DETECT_HUNG_TASK_BLOCKER` 时：\n  - 获取信号量时记录当前任务为 `last_holder`。\n  - 释放时若当前任务是持有者，则清除记录。\n  - 提供 `sem_last_holder()` 供 hung task 检测模块查询阻塞源头。\n\n### 返回值约定\n- `down_trylock()` 返回 **0 表示成功**，**1 表示失败**，这与 `mutex_trylock()` 和 `spin_trylock()` **相反**，需特别注意。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/semaphore.h>`：信号量结构体和 API 声明。\n  - `<linux/spinlock.h>`：原始自旋锁实现。\n  - `<linux/sched.h>`、`<linux/sched/wake_q.h>`：任务调度和批量唤醒。\n  - `<trace/events/lock.h>`：锁争用跟踪点。\n  - `<linux/hung_task.h>`：hung task 检测支持。\n\n- **内核配置依赖**：\n  - `CONFIG_DETECT_HUNG_TASK_BLOCKER`：启用信号量持有者跟踪。\n\n- **与其他同步原语关系**：\n  - 与 `mutex.c` 形成对比：mutex 是二值、不可递归、带调试信息的互斥锁；信号量是计数、可被任意任务释放、更轻量。\n  - 底层依赖调度器（`schedule_timeout`）和中断管理（`irqsave`）。\n\n## 5. 使用场景\n\n- **资源池管理**：如限制同时访问某类硬件设备的任务数量。\n- **读写并发控制**：配合其他机制实现多读者/单写者模型。\n- **内核驱动**：设备驱动中控制对共享资源的并发访问。\n- **中断上下文释放**：因 `up()` 可在中断中调用，适用于中断处理程序释放资源的场景。\n- **不可睡眠路径**：使用 `down_trylock()` 在原子上下文尝试获取资源。\n\n> **注意**：由于信号量不强制所有权（任意任务可调用 `up()`），且缺乏死锁检测等调试特性，现代内核开发中更推荐使用 `mutex` 或 `rwsem`，除非明确需要计数语义或多释放者特性。",
      "similarity": 0.5214986801147461,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "\tWRITE_ONCE((sem)->last_holder, (unsigned long)current);",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "\tif (READ_ONCE((sem)->last_holder) == (unsigned long)current)",
            "\t\tWRITE_ONCE((sem)->last_holder, 0UL);",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn READ_ONCE(sem->last_holder);",
            "}",
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn 0UL;",
            "}",
            "static inline void __sem_acquire(struct semaphore *sem)",
            "{",
            "\tsem->count--;",
            "\thung_task_sem_set_holder(sem);",
            "}",
            "void __sched down(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\t__down(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "}",
            "int __sched down_interruptible(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_interruptible(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_killable(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_killable(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_trylock(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint count;",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tcount = sem->count - 1;",
            "\tif (likely(count >= 0))",
            "\t\t__sem_acquire(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn (count < 0);",
            "}",
            "int __sched down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_timeout(sem, timeout);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "void __sched up(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "",
            "\thung_task_sem_clear_if_holder(sem);",
            "",
            "\tif (likely(list_empty(&sem->wait_list)))",
            "\t\tsem->count++;",
            "\telse",
            "\t\t__up(sem, &wake_q);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "\tif (!wake_q_empty(&wake_q))",
            "\t\twake_up_q(&wake_q);",
            "}"
          ],
          "function_name": "hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, __sem_acquire, down, down_interruptible, down_killable, down_trylock, down_timeout, up",
          "description": "实现了信号量的获取与释放核心逻辑，包括down/down_interruptible/down_killable/down_trylock/down_timeout等接口，通过spinlock保护共享资源，维护等待队列并处理任务状态变更，其中包含Hung Task检测相关函数的条件性实现",
          "similarity": 0.5592800378799438
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 252,
          "end_line": 323,
          "content": [
            "static inline int __sched ___down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\t\t\t\tlong timeout)",
            "{",
            "\tstruct semaphore_waiter waiter;",
            "",
            "\tlist_add_tail(&waiter.list, &sem->wait_list);",
            "\twaiter.task = current;",
            "\twaiter.up = false;",
            "",
            "\tfor (;;) {",
            "\t\tif (signal_pending_state(state, current))",
            "\t\t\tgoto interrupted;",
            "\t\tif (unlikely(timeout <= 0))",
            "\t\t\tgoto timed_out;",
            "\t\t__set_current_state(state);",
            "\t\traw_spin_unlock_irq(&sem->lock);",
            "\t\ttimeout = schedule_timeout(timeout);",
            "\t\traw_spin_lock_irq(&sem->lock);",
            "\t\tif (waiter.up) {",
            "\t\t\thung_task_sem_set_holder(sem);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            " timed_out:",
            "\tlist_del(&waiter.list);",
            "\treturn -ETIME;",
            "",
            " interrupted:",
            "\tlist_del(&waiter.list);",
            "\treturn -EINTR;",
            "}",
            "static inline int __sched __down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\tlong timeout)",
            "{",
            "\tint ret;",
            "",
            "\thung_task_set_blocker(sem, BLOCKER_TYPE_SEM);",
            "",
            "\ttrace_contention_begin(sem, 0);",
            "\tret = ___down_common(sem, state, timeout);",
            "\ttrace_contention_end(sem, ret);",
            "",
            "\thung_task_clear_blocker();",
            "",
            "\treturn ret;",
            "}",
            "static noinline void __sched __down(struct semaphore *sem)",
            "{",
            "\t__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_interruptible(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_killable(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_KILLABLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\treturn __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);",
            "}",
            "static noinline void __sched __up(struct semaphore *sem,",
            "\t\t\t\t  struct wake_q_head *wake_q)",
            "{",
            "\tstruct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,",
            "\t\t\t\t\t\tstruct semaphore_waiter, list);",
            "\tlist_del(&waiter->list);",
            "\twaiter->up = true;",
            "\twake_q_add(wake_q, waiter->task);",
            "}"
          ],
          "function_name": "___down_common, __down_common, __down, __down_interruptible, __down_killable, __down_timeout, __up",
          "description": "实现了信号量的阻塞等待通用逻辑，包含___down_common/__down_common等辅助函数，处理信号量不足时的任务挂起、超时检测、信号处理及唤醒机制，通过循环等待并结合schedule_timeout实现阻塞式资源竞争解决",
          "similarity": 0.5494697093963623
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2008 Intel Corporation",
            " * Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This file implements counting semaphores.",
            " * A counting semaphore may be acquired 'n' times before sleeping.",
            " * See mutex.c for single-acquisition sleeping locks which enforce",
            " * rules which allow code to be debugged more easily.",
            " */",
            "",
            "/*",
            " * Some notes on the implementation:",
            " *",
            " * The spinlock controls access to the other members of the semaphore.",
            " * down_trylock() and up() can be called from interrupt context, so we",
            " * have to disable interrupts when taking the lock.  It turns out various",
            " * parts of the kernel expect to be able to use down() on a semaphore in",
            " * interrupt context when they know it will succeed, so we have to use",
            " * irqsave variants for down(), down_interruptible() and down_killable()",
            " * too.",
            " *",
            " * The ->count variable represents how many more tasks can acquire this",
            " * semaphore.  If it's zero, there may be tasks waiting on the wait_list.",
            " */",
            "",
            "#include <linux/compiler.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/semaphore.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/ftrace.h>",
            "#include <trace/events/lock.h>",
            "#include <linux/hung_task.h>",
            "",
            "static noinline void __down(struct semaphore *sem);",
            "static noinline int __down_interruptible(struct semaphore *sem);",
            "static noinline int __down_killable(struct semaphore *sem);",
            "static noinline int __down_timeout(struct semaphore *sem, long timeout);",
            "static noinline void __up(struct semaphore *sem, struct wake_q_head *wake_q);",
            "",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER"
          ],
          "function_name": null,
          "description": "此代码块定义了计数信号量的基础框架，包含实现计数信号量所需的头文件和注释，声明了多个内联函数及辅助函数，用于处理信号量的获取、释放及Hung Task检测相关逻辑，但由于代码截断，CONFIG_DETECT_HUNG_TASK_BLOCKER部分缺失，上下文不完整",
          "similarity": 0.46301472187042236
        }
      ]
    },
    {
      "source_file": "kernel/watch_queue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:50:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `watch_queue.c`\n\n---\n\n# watch_queue.c 技术文档\n\n## 文件概述\n\n`watch_queue.c` 实现了 Linux 内核中的**监视队列**（Watch Queue）机制，这是一种基于管道（pipe）构建的通用事件通知系统。该机制允许内核子系统（如文件系统、密钥管理、设备驱动等）向用户空间异步发送结构化通知。用户空间通过创建特殊类型的管道并关联监视队列，即可接收来自内核的各类事件通知。该文件定义了通知的投递、过滤、缓冲管理及与管道集成的核心逻辑。\n\n## 核心功能\n\n### 主要函数\n\n- **`__post_watch_notification()`**  \n  核心通知投递函数。遍历指定 `watch_list` 中所有匹配 `id` 的监视器（`watch`），对每个关联的 `watch_queue` 应用过滤规则、安全检查，并将通知写入底层管道。\n\n- **`post_one_notification()`**  \n  将单个通知写入指定 `watch_queue` 的底层管道缓冲区。负责从预分配的通知页中获取空闲槽位、填充数据、更新管道头指针并唤醒等待读取的进程。\n\n- **`filter_watch_notification()`**  \n  根据 `watch_filter` 中的类型、子类型和信息掩码规则，判断是否允许特定通知通过。\n\n- **`watch_queue_set_size()`**  \n  为监视队列分配预分配的通知缓冲区（页数组和位图），并调整底层管道的环形缓冲区大小。\n\n- **`watch_queue_pipe_buf_release()`**  \n  管道缓冲区释放回调。当用户空间读取完通知后，将对应的通知槽位在位图中标记为空闲，供后续复用。\n\n### 关键数据结构\n\n- **`struct watch_queue`**  \n  表示一个监视队列，包含：\n  - 指向底层 `pipe_inode_info` 的指针\n  - 预分配的通知页数组（`notes`）\n  - 通知槽位空闲位图（`notes_bitmap`）\n  - 通知过滤器（`filter`）\n  - 保护锁（`lock`）\n\n- **`struct watch_notification`**  \n  通用通知记录格式，包含类型（`type`）、子类型（`subtype`）、信息字段（`info`，含长度和ID）及可变负载。\n\n- **`struct watch_filter` / `struct watch_type_filter`**  \n  定义通知过滤规则，支持按类型、子类型及信息字段的位掩码进行精确过滤。\n\n- **`watch_queue_pipe_buf_ops`**  \n  自定义的 `pipe_buf_operations`，用于管理监视队列专用管道缓冲区的生命周期。\n\n## 关键实现\n\n### 基于管道的通知传输\n- 监视队列复用内核管道（`pipe_inode_info`）作为通知传输通道，利用其成熟的读写、轮询、异步通知机制。\n- 通过自定义 `pipe_buf_operations`（`watch_queue_pipe_buf_ops`）实现通知槽位的回收：当用户读取通知后，`release` 回调将对应槽位在 `notes_bitmap` 中置位，标记为空闲。\n\n### 预分配通知缓冲区\n- 通知数据存储在预分配的内核页（`notes`）中，每页划分为多个固定大小（128字节）的槽位（`WATCH_QUEUE_NOTE_SIZE`）。\n- 使用位图（`notes_bitmap`）跟踪槽位使用状态，1 表示空闲。投递通知时通过 `find_first_bit()` 快速查找空闲槽位。\n- 缓冲区大小由用户通过 `watch_queue_set_size()` 设置（1-512个通知），并受管道缓冲区配额限制。\n\n### 通知投递流程\n1. **匹配监视器**：遍历 `watch_list`，查找 `id` 匹配的 `watch`。\n2. **应用过滤**：若队列配置了过滤器，调用 `filter_watch_notification()` 决定是否丢弃。\n3. **安全检查**：调用 LSM 钩子 `security_post_notification()` 进行权限验证。\n4. **写入管道**：\n   - 获取空闲通知槽位，复制通知数据。\n   - 构造 `pipe_buffer` 指向该槽位，设置自定义操作集。\n   - 更新管道 `head` 指针，唤醒等待读取的进程。\n   - 若缓冲区满，标记前一个缓冲区为 `PIPE_BUF_FLAG_LOSS` 表示丢包。\n\n### 并发与同步\n- **RCU 保护**：`watch_list` 和 `watch_queue` 的访问通过 RCU 机制保护，确保遍历时结构体不被释放。\n- **自旋锁**：\n  - `wqueue->lock`：保护 `wqueue` 状态（如 `pipe` 指针有效性）。\n  - `pipe->rd_wait.lock`：保护管道环形缓冲区的读写操作。\n- **原子操作**：管道 `head` 指针使用 `smp_store_release()` 更新，确保与 `pipe_read()` 的同步。\n\n## 依赖关系\n\n- **管道子系统**（`fs/pipe.c`）  \n  依赖管道的核心数据结构（`pipe_inode_info`、`pipe_buffer`）和操作接口（`pipe_buf()`、`pipe_full()`、`generic_pipe_buf_*`）。\n\n- **内存管理**  \n  使用 `alloc_page()`、`kmap_atomic()` 管理通知缓冲区页，`bitmap_alloc()` 管理槽位位图。\n\n- **安全模块**（LSM）  \n  通过 `security_post_notification()` 钩子集成安全策略。\n\n- **用户空间接口**  \n  与 `fs/watch_queue.c` 中的系统调用（如 `watch_queue_set_size()`）协同工作，后者负责创建监视队列并与管道关联。\n\n- **头文件依赖**  \n  `linux/watch_queue.h`（核心数据结构定义）、`linux/pipe_fs_i.h`（管道内部接口）。\n\n## 使用场景\n\n- **文件系统事件监控**  \n  如 `fsnotify` 子系统可通过监视队列向用户空间报告文件访问、修改等事件。\n\n- **密钥管理通知**  \n  内核密钥环（`KEYS`）子系统使用该机制通知密钥状态变更（如过期、撤销）。\n\n- **设备事件上报**  \n  设备驱动可利用监视队列异步上报硬件状态变化或错误事件。\n\n- **通用内核事件分发**  \n  任何需要向特权用户空间守护进程（如 `systemd`）发送结构化事件的内核子系统均可集成此机制。\n\n- **用户空间消费**  \n  应用程序通过 `open(\"/dev/watch_queue\")` 获取监视队列文件描述符，调用 `ioctl()` 设置缓冲区大小和过滤器，然后像读取普通管道一样接收通知。",
      "similarity": 0.5168547630310059,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/watch_queue.c",
          "start_line": 1,
          "end_line": 41,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/* Watch queue and general notification mechanism, built on pipes",
            " *",
            " * Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.",
            " * Written by David Howells (dhowells@redhat.com)",
            " *",
            " * See Documentation/core-api/watch_queue.rst",
            " */",
            "",
            "#define pr_fmt(fmt) \"watchq: \" fmt",
            "#include <linux/module.h>",
            "#include <linux/init.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/printk.h>",
            "#include <linux/miscdevice.h>",
            "#include <linux/fs.h>",
            "#include <linux/mm.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/poll.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/file.h>",
            "#include <linux/security.h>",
            "#include <linux/cred.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/watch_queue.h>",
            "#include <linux/pipe_fs_i.h>",
            "",
            "MODULE_DESCRIPTION(\"Watch queue\");",
            "MODULE_AUTHOR(\"Red Hat, Inc.\");",
            "",
            "#define WATCH_QUEUE_NOTE_SIZE 128",
            "#define WATCH_QUEUE_NOTES_PER_PAGE (PAGE_SIZE / WATCH_QUEUE_NOTE_SIZE)",
            "",
            "/*",
            " * This must be called under the RCU read-lock, which makes",
            " * sure that the wqueue still exists. It can then take the lock,",
            " * and check that the wqueue hasn't been destroyed, which in",
            " * turn makes sure that the notification pipe still exists.",
            " */"
          ],
          "function_name": null,
          "description": "定义了watch_queue模块的头部信息，包含常量WATCH_QUEUE_NOTE_SIZE和NOTES_PER_PAGE，声明模块许可证及作者信息，并引入相关内核头文件，为后续实现提供基础框架。",
          "similarity": 0.5496432781219482
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/watch_queue.c",
          "start_line": 42,
          "end_line": 154,
          "content": [
            "static inline bool lock_wqueue(struct watch_queue *wqueue)",
            "{",
            "\tspin_lock_bh(&wqueue->lock);",
            "\tif (unlikely(!wqueue->pipe)) {",
            "\t\tspin_unlock_bh(&wqueue->lock);",
            "\t\treturn false;",
            "\t}",
            "\treturn true;",
            "}",
            "static inline void unlock_wqueue(struct watch_queue *wqueue)",
            "{",
            "\tspin_unlock_bh(&wqueue->lock);",
            "}",
            "static void watch_queue_pipe_buf_release(struct pipe_inode_info *pipe,",
            "\t\t\t\t\t struct pipe_buffer *buf)",
            "{",
            "\tstruct watch_queue *wqueue = (struct watch_queue *)buf->private;",
            "\tstruct page *page;",
            "\tunsigned int bit;",
            "",
            "\t/* We need to work out which note within the page this refers to, but",
            "\t * the note might have been maximum size, so merely ANDing the offset",
            "\t * off doesn't work.  OTOH, the note must've been more than zero size.",
            "\t */",
            "\tbit = buf->offset + buf->len;",
            "\tif ((bit & (WATCH_QUEUE_NOTE_SIZE - 1)) == 0)",
            "\t\tbit -= WATCH_QUEUE_NOTE_SIZE;",
            "\tbit /= WATCH_QUEUE_NOTE_SIZE;",
            "",
            "\tpage = buf->page;",
            "\tbit += page->index;",
            "",
            "\tset_bit(bit, wqueue->notes_bitmap);",
            "\tgeneric_pipe_buf_release(pipe, buf);",
            "}",
            "static bool post_one_notification(struct watch_queue *wqueue,",
            "\t\t\t\t  struct watch_notification *n)",
            "{",
            "\tvoid *p;",
            "\tstruct pipe_inode_info *pipe = wqueue->pipe;",
            "\tstruct pipe_buffer *buf;",
            "\tstruct page *page;",
            "\tunsigned int head, tail, note, offset, len;",
            "\tbool done = false;",
            "",
            "\tspin_lock_irq(&pipe->rd_wait.lock);",
            "",
            "\thead = pipe->head;",
            "\ttail = pipe->tail;",
            "\tif (pipe_full(head, tail, pipe->ring_size))",
            "\t\tgoto lost;",
            "",
            "\tnote = find_first_bit(wqueue->notes_bitmap, wqueue->nr_notes);",
            "\tif (note >= wqueue->nr_notes)",
            "\t\tgoto lost;",
            "",
            "\tpage = wqueue->notes[note / WATCH_QUEUE_NOTES_PER_PAGE];",
            "\toffset = note % WATCH_QUEUE_NOTES_PER_PAGE * WATCH_QUEUE_NOTE_SIZE;",
            "\tget_page(page);",
            "\tlen = n->info & WATCH_INFO_LENGTH;",
            "\tp = kmap_atomic(page);",
            "\tmemcpy(p + offset, n, len);",
            "\tkunmap_atomic(p);",
            "",
            "\tbuf = pipe_buf(pipe, head);",
            "\tbuf->page = page;",
            "\tbuf->private = (unsigned long)wqueue;",
            "\tbuf->ops = &watch_queue_pipe_buf_ops;",
            "\tbuf->offset = offset;",
            "\tbuf->len = len;",
            "\tbuf->flags = PIPE_BUF_FLAG_WHOLE;",
            "\tsmp_store_release(&pipe->head, head + 1); /* vs pipe_read() */",
            "",
            "\tif (!test_and_clear_bit(note, wqueue->notes_bitmap)) {",
            "\t\tspin_unlock_irq(&pipe->rd_wait.lock);",
            "\t\tBUG();",
            "\t}",
            "\twake_up_interruptible_sync_poll_locked(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);",
            "\tdone = true;",
            "",
            "out:",
            "\tspin_unlock_irq(&pipe->rd_wait.lock);",
            "\tif (done)",
            "\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);",
            "\treturn done;",
            "",
            "lost:",
            "\tbuf = pipe_buf(pipe, head - 1);",
            "\tbuf->flags |= PIPE_BUF_FLAG_LOSS;",
            "\tgoto out;",
            "}",
            "static bool filter_watch_notification(const struct watch_filter *wf,",
            "\t\t\t\t      const struct watch_notification *n)",
            "{",
            "\tconst struct watch_type_filter *wt;",
            "\tunsigned int st_bits = sizeof(wt->subtype_filter[0]) * 8;",
            "\tunsigned int st_index = n->subtype / st_bits;",
            "\tunsigned int st_bit = 1U << (n->subtype % st_bits);",
            "\tint i;",
            "",
            "\tif (!test_bit(n->type, wf->type_filter))",
            "\t\treturn false;",
            "",
            "\tfor (i = 0; i < wf->nr_filters; i++) {",
            "\t\twt = &wf->filters[i];",
            "\t\tif (n->type == wt->type &&",
            "\t\t    (wt->subtype_filter[st_index] & st_bit) &&",
            "\t\t    (n->info & wt->info_mask) == wt->info_filter)",
            "\t\t\treturn true;",
            "\t}",
            "",
            "\treturn false; /* If there is a filter, the default is to reject. */",
            "}"
          ],
          "function_name": "lock_wqueue, unlock_wqueue, watch_queue_pipe_buf_release, post_one_notification, filter_watch_notification",
          "description": "实现了watch_queue的锁操作、缓冲区释放、通知提交及过滤逻辑。lock_wqueue/unlock_wqueue用于保护队列访问，watch_queue_pipe_buf_release处理缓冲区回收并更新位图，post_one_notification将通知数据写入管道，filter_watch_notification进行类型和子类型的匹配判断。",
          "similarity": 0.5465114116668701
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/watch_queue.c",
          "start_line": 315,
          "end_line": 422,
          "content": [
            "long watch_queue_set_filter(struct pipe_inode_info *pipe,",
            "\t\t\t    struct watch_notification_filter __user *_filter)",
            "{",
            "\tstruct watch_notification_type_filter *tf;",
            "\tstruct watch_notification_filter filter;",
            "\tstruct watch_type_filter *q;",
            "\tstruct watch_filter *wfilter;",
            "\tstruct watch_queue *wqueue = pipe->watch_queue;",
            "\tint ret, nr_filter = 0, i;",
            "",
            "\tif (!wqueue)",
            "\t\treturn -ENODEV;",
            "",
            "\tif (!_filter) {",
            "\t\t/* Remove the old filter */",
            "\t\twfilter = NULL;",
            "\t\tgoto set;",
            "\t}",
            "",
            "\t/* Grab the user's filter specification */",
            "\tif (copy_from_user(&filter, _filter, sizeof(filter)) != 0)",
            "\t\treturn -EFAULT;",
            "\tif (filter.nr_filters == 0 ||",
            "\t    filter.nr_filters > 16 ||",
            "\t    filter.__reserved != 0)",
            "\t\treturn -EINVAL;",
            "",
            "\ttf = memdup_array_user(_filter->filters, filter.nr_filters, sizeof(*tf));",
            "\tif (IS_ERR(tf))",
            "\t\treturn PTR_ERR(tf);",
            "",
            "\tret = -EINVAL;",
            "\tfor (i = 0; i < filter.nr_filters; i++) {",
            "\t\tif ((tf[i].info_filter & ~tf[i].info_mask) ||",
            "\t\t    tf[i].info_mask & WATCH_INFO_LENGTH)",
            "\t\t\tgoto err_filter;",
            "\t\t/* Ignore any unknown types */",
            "\t\tif (tf[i].type >= WATCH_TYPE__NR)",
            "\t\t\tcontinue;",
            "\t\tnr_filter++;",
            "\t}",
            "",
            "\t/* Now we need to build the internal filter from only the relevant",
            "\t * user-specified filters.",
            "\t */",
            "\tret = -ENOMEM;",
            "\twfilter = kzalloc(struct_size(wfilter, filters, nr_filter), GFP_KERNEL);",
            "\tif (!wfilter)",
            "\t\tgoto err_filter;",
            "\twfilter->nr_filters = nr_filter;",
            "",
            "\tq = wfilter->filters;",
            "\tfor (i = 0; i < filter.nr_filters; i++) {",
            "\t\tif (tf[i].type >= WATCH_TYPE__NR)",
            "\t\t\tcontinue;",
            "",
            "\t\tq->type\t\t\t= tf[i].type;",
            "\t\tq->info_filter\t\t= tf[i].info_filter;",
            "\t\tq->info_mask\t\t= tf[i].info_mask;",
            "\t\tq->subtype_filter[0]\t= tf[i].subtype_filter[0];",
            "\t\t__set_bit(q->type, wfilter->type_filter);",
            "\t\tq++;",
            "\t}",
            "",
            "\tkfree(tf);",
            "set:",
            "\tpipe_lock(pipe);",
            "\twfilter = rcu_replace_pointer(wqueue->filter, wfilter,",
            "\t\t\t\t      lockdep_is_held(&pipe->mutex));",
            "\tpipe_unlock(pipe);",
            "\tif (wfilter)",
            "\t\tkfree_rcu(wfilter, rcu);",
            "\treturn 0;",
            "",
            "err_filter:",
            "\tkfree(tf);",
            "\treturn ret;",
            "}",
            "static void __put_watch_queue(struct kref *kref)",
            "{",
            "\tstruct watch_queue *wqueue =",
            "\t\tcontainer_of(kref, struct watch_queue, usage);",
            "\tstruct watch_filter *wfilter;",
            "\tint i;",
            "",
            "\tfor (i = 0; i < wqueue->nr_pages; i++)",
            "\t\t__free_page(wqueue->notes[i]);",
            "\tkfree(wqueue->notes);",
            "\tbitmap_free(wqueue->notes_bitmap);",
            "",
            "\twfilter = rcu_access_pointer(wqueue->filter);",
            "\tif (wfilter)",
            "\t\tkfree_rcu(wfilter, rcu);",
            "\tkfree_rcu(wqueue, rcu);",
            "}",
            "void put_watch_queue(struct watch_queue *wqueue)",
            "{",
            "\tkref_put(&wqueue->usage, __put_watch_queue);",
            "}",
            "static void free_watch(struct rcu_head *rcu)",
            "{",
            "\tstruct watch *watch = container_of(rcu, struct watch, rcu);",
            "",
            "\tput_watch_queue(rcu_access_pointer(watch->queue));",
            "\tatomic_dec(&watch->cred->user->nr_watches);",
            "\tput_cred(watch->cred);",
            "\tkfree(watch);",
            "}"
          ],
          "function_name": "watch_queue_set_filter, __put_watch_queue, put_watch_queue, free_watch",
          "description": "watch_queue_set_filter设置过滤规则并转换为内核内部结构，__put_watch_queue释放watch_queue相关资源包括页面、位图和过滤器，put_watch_queue通过引用计数管理watch_queue生命周期，free_watch执行RCU回调完成最终释放。",
          "similarity": 0.5003195405006409
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/watch_queue.c",
          "start_line": 193,
          "end_line": 304,
          "content": [
            "void __post_watch_notification(struct watch_list *wlist,",
            "\t\t\t       struct watch_notification *n,",
            "\t\t\t       const struct cred *cred,",
            "\t\t\t       u64 id)",
            "{",
            "\tconst struct watch_filter *wf;",
            "\tstruct watch_queue *wqueue;",
            "\tstruct watch *watch;",
            "",
            "\tif (((n->info & WATCH_INFO_LENGTH) >> WATCH_INFO_LENGTH__SHIFT) == 0) {",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\trcu_read_lock();",
            "",
            "\thlist_for_each_entry_rcu(watch, &wlist->watchers, list_node) {",
            "\t\tif (watch->id != id)",
            "\t\t\tcontinue;",
            "\t\tn->info &= ~WATCH_INFO_ID;",
            "\t\tn->info |= watch->info_id;",
            "",
            "\t\twqueue = rcu_dereference(watch->queue);",
            "\t\twf = rcu_dereference(wqueue->filter);",
            "\t\tif (wf && !filter_watch_notification(wf, n))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (security_post_notification(watch->cred, cred, n) < 0)",
            "\t\t\tcontinue;",
            "",
            "\t\tif (lock_wqueue(wqueue)) {",
            "\t\t\tpost_one_notification(wqueue, n);",
            "\t\t\tunlock_wqueue(wqueue);",
            "\t\t}",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "}",
            "long watch_queue_set_size(struct pipe_inode_info *pipe, unsigned int nr_notes)",
            "{",
            "\tstruct watch_queue *wqueue = pipe->watch_queue;",
            "\tstruct page **pages;",
            "\tunsigned long *bitmap;",
            "\tunsigned long user_bufs;",
            "\tint ret, i, nr_pages;",
            "",
            "\tif (!wqueue)",
            "\t\treturn -ENODEV;",
            "\tif (wqueue->notes)",
            "\t\treturn -EBUSY;",
            "",
            "\tif (nr_notes < 1 ||",
            "\t    nr_notes > 512) /* TODO: choose a better hard limit */",
            "\t\treturn -EINVAL;",
            "",
            "\tnr_pages = (nr_notes + WATCH_QUEUE_NOTES_PER_PAGE - 1);",
            "\tnr_pages /= WATCH_QUEUE_NOTES_PER_PAGE;",
            "\tuser_bufs = account_pipe_buffers(pipe->user, pipe->nr_accounted, nr_pages);",
            "",
            "\tif (nr_pages > pipe->max_usage &&",
            "\t    (too_many_pipe_buffers_hard(user_bufs) ||",
            "\t     too_many_pipe_buffers_soft(user_bufs)) &&",
            "\t    pipe_is_unprivileged_user()) {",
            "\t\tret = -EPERM;",
            "\t\tgoto error;",
            "\t}",
            "",
            "\tnr_notes = nr_pages * WATCH_QUEUE_NOTES_PER_PAGE;",
            "\tret = pipe_resize_ring(pipe, roundup_pow_of_two(nr_notes));",
            "\tif (ret < 0)",
            "\t\tgoto error;",
            "",
            "\t/*",
            "\t * pipe_resize_ring() does not update nr_accounted for watch_queue",
            "\t * pipes, because the above vastly overprovisions. Set nr_accounted on",
            "\t * and max_usage this pipe to the number that was actually charged to",
            "\t * the user above via account_pipe_buffers.",
            "\t */",
            "\tpipe->max_usage = nr_pages;",
            "\tpipe->nr_accounted = nr_pages;",
            "",
            "\tret = -ENOMEM;",
            "\tpages = kcalloc(sizeof(struct page *), nr_pages, GFP_KERNEL);",
            "\tif (!pages)",
            "\t\tgoto error;",
            "",
            "\tfor (i = 0; i < nr_pages; i++) {",
            "\t\tpages[i] = alloc_page(GFP_KERNEL);",
            "\t\tif (!pages[i])",
            "\t\t\tgoto error_p;",
            "\t\tpages[i]->index = i * WATCH_QUEUE_NOTES_PER_PAGE;",
            "\t}",
            "",
            "\tbitmap = bitmap_alloc(nr_notes, GFP_KERNEL);",
            "\tif (!bitmap)",
            "\t\tgoto error_p;",
            "",
            "\tbitmap_fill(bitmap, nr_notes);",
            "\twqueue->notes = pages;",
            "\twqueue->notes_bitmap = bitmap;",
            "\twqueue->nr_pages = nr_pages;",
            "\twqueue->nr_notes = nr_notes;",
            "\treturn 0;",
            "",
            "error_p:",
            "\twhile (--i >= 0)",
            "\t\t__free_page(pages[i]);",
            "\tkfree(pages);",
            "error:",
            "\t(void) account_pipe_buffers(pipe->user, nr_pages, pipe->nr_accounted);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__post_watch_notification, watch_queue_set_size",
          "description": "__post_watch_notification遍历watch列表并应用过滤器后提交通知，watch_queue_set_size动态调整管道容量，通过计算所需页数和位图分配，限制最大容量为512个笔记，支持扩展性需求。",
          "similarity": 0.4908761978149414
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/watch_queue.c",
          "start_line": 602,
          "end_line": 680,
          "content": [
            "void watch_queue_clear(struct watch_queue *wqueue)",
            "{",
            "\tstruct watch_list *wlist;",
            "\tstruct watch *watch;",
            "\tbool release;",
            "",
            "\trcu_read_lock();",
            "\tspin_lock_bh(&wqueue->lock);",
            "",
            "\t/*",
            "\t * This pipe can be freed by callers like free_pipe_info().",
            "\t * Removing this reference also prevents new notifications.",
            "\t */",
            "\twqueue->pipe = NULL;",
            "",
            "\twhile (!hlist_empty(&wqueue->watches)) {",
            "\t\twatch = hlist_entry(wqueue->watches.first, struct watch, queue_node);",
            "\t\thlist_del_init_rcu(&watch->queue_node);",
            "\t\t/* We now own a ref on the watch. */",
            "\t\tspin_unlock_bh(&wqueue->lock);",
            "",
            "\t\t/* We can't do the next bit under the queue lock as we need to",
            "\t\t * get the list lock - which would cause a deadlock if someone",
            "\t\t * was removing from the opposite direction at the same time or",
            "\t\t * posting a notification.",
            "\t\t */",
            "\t\twlist = rcu_dereference(watch->watch_list);",
            "\t\tif (wlist) {",
            "\t\t\tvoid (*release_watch)(struct watch *);",
            "",
            "\t\t\tspin_lock(&wlist->lock);",
            "",
            "\t\t\trelease = !hlist_unhashed(&watch->list_node);",
            "\t\t\tif (release) {",
            "\t\t\t\thlist_del_init_rcu(&watch->list_node);",
            "\t\t\t\trcu_assign_pointer(watch->watch_list, NULL);",
            "",
            "\t\t\t\t/* We now own a second ref on the watch. */",
            "\t\t\t}",
            "",
            "\t\t\trelease_watch = wlist->release_watch;",
            "\t\t\tspin_unlock(&wlist->lock);",
            "",
            "\t\t\tif (release) {",
            "\t\t\t\tif (release_watch) {",
            "\t\t\t\t\trcu_read_unlock();",
            "\t\t\t\t\t/* This might need to call dput(), so",
            "\t\t\t\t\t * we have to drop all the locks.",
            "\t\t\t\t\t */",
            "\t\t\t\t\t(*release_watch)(watch);",
            "\t\t\t\t\trcu_read_lock();",
            "\t\t\t\t}",
            "\t\t\t\tput_watch(watch);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tput_watch(watch);",
            "\t\tspin_lock_bh(&wqueue->lock);",
            "\t}",
            "",
            "\tspin_unlock_bh(&wqueue->lock);",
            "\trcu_read_unlock();",
            "}",
            "int watch_queue_init(struct pipe_inode_info *pipe)",
            "{",
            "\tstruct watch_queue *wqueue;",
            "",
            "\twqueue = kzalloc(sizeof(*wqueue), GFP_KERNEL);",
            "\tif (!wqueue)",
            "\t\treturn -ENOMEM;",
            "",
            "\twqueue->pipe = pipe;",
            "\tkref_init(&wqueue->usage);",
            "\tspin_lock_init(&wqueue->lock);",
            "\tINIT_HLIST_HEAD(&wqueue->watches);",
            "",
            "\tpipe->watch_queue = wqueue;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "watch_queue_clear, watch_queue_init",
          "description": "该代码实现了监视队列的初始化与清理功能。  \n`watch_queue_clear`通过RCU和自旋锁机制安全地移除所有监视项并释放资源，`watch_queue_init`初始化监视队列结构并绑定至管道对象。  \n上下文不完整：`release_watch`等关键函数依赖外部定义，部分RCU回调逻辑未完全展示。",
          "similarity": 0.48569637537002563
        }
      ]
    }
  ]
}