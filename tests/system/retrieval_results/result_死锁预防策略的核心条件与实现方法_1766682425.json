{
  "query": "死锁预防策略的核心条件与实现方法",
  "timestamp": "2025-12-26 01:07:05",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/ww_mutex.h",
      "md_summary": "> 自动生成时间: 2025-10-25 14:56:40\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\ww_mutex.h`\n\n---\n\n# `locking/ww_mutex.h` 技术文档\n\n## 1. 文件概述\n\n`ww_mutex.h` 是 Linux 内核中用于实现 **Wound-Wait (WW) 互斥锁**（`ww_mutex`）的头文件。该机制主要用于解决 **死锁问题**，特别是在图形子系统（如 DRM/KMS）和资源管理场景中，多个事务（transactions）需要以特定顺序获取多个锁时。  \nWW 互斥锁通过为每个锁请求关联一个 **获取上下文**（`ww_acquire_ctx`），并基于事务的优先级或时间戳实现 **Wait-Die** 或 **Wound-Wait** 死锁避免策略。\n\n该文件通过条件编译（`WW_RT` 宏）支持两种底层锁实现：\n- **普通互斥锁**（`mutex`）：用于非实时（non-RT）内核配置。\n- **实时互斥锁**（`rt_mutex`）：用于实时（RT）内核补丁配置，支持优先级继承。\n\n## 2. 核心功能\n\n### 2.1 主要宏定义\n- `MUTEX` / `MUTEX_WAITER`：根据 `WW_RT` 宏分别映射到 `mutex`/`rt_mutex` 及其等待者结构。\n\n### 2.2 等待者链表/红黑树操作函数（抽象接口）\n- `__ww_waiter_first()`：获取等待队列中的第一个等待者。\n- `__ww_waiter_next()` / `__ww_waiter_prev()`：获取下一个/上一个等待者。\n- `__ww_waiter_last()`：获取等待队列中的最后一个等待者。\n- `__ww_waiter_add()`：将等待者插入到指定位置（普通 mutex 使用链表，RT 使用红黑树）。\n\n### 2.3 锁状态查询函数\n- `__ww_mutex_owner()`：获取当前锁的持有者任务。\n- `__ww_mutex_has_waiters()`：检查锁是否有等待者。\n- `lock_wait_lock()` / `unlock_wait_lock()`：获取/释放锁的等待队列自旋锁（`wait_lock`）。\n- `lockdep_assert_wait_lock_held()`：调试时断言 `wait_lock` 已被持有。\n\n### 2.4 WW 互斥锁核心逻辑函数\n- `ww_mutex_lock_acquired()`：在成功获取 `ww_mutex` 后，将其与获取上下文（`ww_ctx`）关联，并执行调试检查。\n- `__ww_ctx_less()`：比较两个获取上下文的优先级（用于决定谁应“等待”或“死亡/被抢占”）。\n- `__ww_mutex_die()`：**Wait-Die 策略**实现：若当前请求者（新事务）发现等待队列中有更老的事务持有其他锁，则唤醒该老事务使其“死亡”（回滚）。\n- `__ww_mutex_wound()`：**Wound-Wait 策略**实现：若当前请求者（老事务）发现锁持有者是更年轻的事务，则“刺伤”（标记 `wounded=1`）该年轻事务，迫使其回滚。\n\n## 3. 关键实现\n\n### 3.1 死锁避免策略\n- **Wait-Die**（`is_wait_die=1`）：\n  - **新事务**请求**老事务**持有的锁 → **新事务等待**。\n  - **新事务**请求**老事务**等待的锁 → **新事务死亡**（回滚）。\n- **Wound-Wait**（`is_wait_die=0`）：\n  - **老事务**请求**新事务**持有的锁 → **新事务被刺伤**（回滚）。\n  - **老事务**请求**新事务**等待的锁 → **老事务等待**。\n\n### 3.2 上下文比较 (`__ww_ctx_less`)\n- **非 RT 模式**：仅基于时间戳（`stamp`），值越大表示事务越新。\n- **RT 模式**：\n  1. 优先比较 **实时优先级**（`prio`），数值越小优先级越高。\n  2. 若均为 **Deadline 调度类**，比较 **截止时间**（`deadline`），越早截止优先级越高。\n  3. 若优先级相同，回退到时间戳比较。\n\n### 3.3 RT 与非 RT 差异\n- **数据结构**：\n  - 非 RT：等待者使用 **双向链表**（`list_head`）。\n  - RT：等待者使用 **红黑树**（`rb_root`），按优先级排序。\n- **插入逻辑**：\n  - 非 RT：`__ww_waiter_add` 显式插入到指定位置。\n  - RT：`__ww_waiter_add` 为空（RT 互斥锁内部自动处理插入）。\n\n### 3.4 调试支持 (`DEBUG_WW_MUTEXES`)\n- 检查 `ww_mutex` 是否被错误地用普通 `mutex_unlock` 释放。\n- 验证上下文一致性（如 `ww_class` 匹配、`contending_lock` 状态等）。\n\n## 4. 依赖关系\n\n- **基础锁机制**：\n  - 非 RT 模式依赖 `<linux/mutex.h>`。\n  - RT 模式依赖 `<linux/rtmutex.h>`。\n- **调度器**：依赖任务结构（`task_struct`）、优先级（`prio`）、调度类（如 `dl_prio`）。\n- **调试框架**：依赖 `lockdep`（`lockdep_assert_held`）和 `DEBUG_LOCKS_WARN_ON`。\n- **原子操作**：使用 `atomic_long_read` 检查锁状态标志（`MUTEX_FLAG_WAITERS`）。\n\n## 5. 使用场景\n\n- **图形子系统**（DRM/KMS）：  \n  多个 GPU 作业（如渲染、合成）需按顺序获取多个缓冲区（buffer）或 CRTC 锁，避免死锁。\n- **资源分配器**：  \n  当多个客户端竞争一组有限资源（如内存区域、I/O 端口）时，通过 WW 互斥锁确保无死锁的分配顺序。\n- **实时系统**（RT 补丁）：  \n  在需要确定性延迟的场景中，结合优先级继承（PI）避免优先级反转，同时通过 WW 策略解决多锁死锁。\n- **文件系统**：  \n  某些文件系统（如 Btrfs）在元数据操作中使用 WW 互斥锁管理多个 extent 锁。",
      "similarity": 0.5477944016456604,
      "chunks": []
    },
    {
      "source_file": "kernel/watchdog.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:51:17\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `watchdog.c`\n\n---\n\n# watchdog.c 技术文档\n\n## 1. 文件概述\n\n`watchdog.c` 是 Linux 内核中实现 **硬锁死（hard lockup）** 和 **软锁死（soft lockup）** 检测机制的核心文件。该机制用于监控系统中 CPU 是否因长时间禁用中断或陷入无限循环而无法响应，从而帮助诊断系统挂死问题。硬锁死指 CPU 完全停止响应中断（包括 NMI），软锁死指内核线程长时间占用 CPU 且未调度其他任务。本文件主要聚焦于硬锁死检测的通用框架和部分实现，软锁死检测逻辑主要在其他文件（如 `softlockup.c`）中实现，但两者共享部分配置和控制逻辑。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `watchdog_enabled`：位掩码，表示当前启用的 watchdog 类型（软/硬锁死检测）。\n- `watchdog_user_enabled`：用户空间是否启用 watchdog（默认 1）。\n- `watchdog_hardlockup_user_enabled`：用户空间是否启用硬锁死检测（默认值取决于架构）。\n- `watchdog_softlockup_user_enabled`：用户空间是否启用软锁死检测（默认 1）。\n- `watchdog_thresh`：锁死检测阈值（秒，默认 10 秒）。\n- `watchdog_cpumask`：参与 watchdog 检测的 CPU 掩码。\n- `hardlockup_panic`：硬锁死发生时是否触发内核 panic（默认由 `CONFIG_BOOTPARAM_HARDLOCKUP_PANIC` 决定）。\n- `sysctl_hardlockup_all_cpu_backtrace`（SMP）：硬锁死时是否打印所有 CPU 的 backtrace。\n- `hardlockup_count`（SYSFS）：记录硬锁死事件发生次数。\n\n### 主要函数\n- `hardlockup_detector_disable(void)`：在启动早期禁用硬锁死检测（例如虚拟机环境）。\n- `hardlockup_panic_setup(char *str)`：解析内核启动参数 `nmi_watchdog=`，配置硬锁死行为。\n- `arch_touch_nmi_watchdog(void)`：架构相关函数，用于在关键路径“触摸”硬 watchdog，防止误报（导出符号）。\n- `watchdog_hardlockup_touch_cpu(unsigned int cpu)`：标记指定 CPU 已被“触摸”。\n- `is_hardlockup(unsigned int cpu)`：检查指定 CPU 是否发生硬锁死（基于高精度定时器中断计数）。\n- `watchdog_hardlockup_kick(void)`：在高精度定时器中断中“踢”硬 watchdog（更新中断计数）。\n- `watchdog_hardlockup_check(unsigned int cpu, struct pt_regs *regs)`：执行硬锁死检测逻辑，打印诊断信息并可能触发 panic。\n- `watchdog_hardlockup_enable/disable(unsigned int cpu)`：弱符号函数，由具体硬 watchdog 实现（如 perf-based）覆盖，用于启停 per-CPU 检测。\n- `watchdog_hardlockup_probe(void)`：弱符号函数，由具体实现提供，用于探测硬 watchdog 硬件/机制是否可用。\n\n### 核心数据结构（Per-CPU）\n- `hrtimer_interrupts`：高精度定时器中断计数器（原子变量）。\n- `hrtimer_interrupts_saved`：上次保存的中断计数值。\n- `watchdog_hardlockup_warned`：是否已为该 CPU 打印过硬锁死警告。\n- `watchdog_hardlockup_touched`：该 CPU 是否被“触摸”过（用于豁免检测）。\n\n## 3. 关键实现\n\n### 硬锁死检测机制（基于高精度定时器）\n当配置 `CONFIG_HARDLOCKUP_DETECTOR_COUNTS_HRTIMER` 时，硬锁死检测通过监控 **高精度定时器（hrtimer）中断** 的发生频率实现：\n1. **计数更新**：每次 hrtimer 中断发生时，调用 `watchdog_hardlockup_kick()` 原子递增 per-CPU 计数器 `hrtimer_interrupts`。\n2. **检测逻辑**：在 NMI（不可屏蔽中断）上下文（或其他检测点）调用 `watchdog_hardlockup_check()`：\n   - 若 CPU 被“触摸”（`watchdog_hardlockup_touched` 为真），则清除此标记并跳过检测。\n   - 否则调用 `is_hardlockup()`：比较当前 `hrtimer_interrupts` 与上次保存值 `hrtimer_interrupts_saved`。若相等，说明在检测周期内无 hrtimer 中断，判定为硬锁死。\n3. **告警与处理**：\n   - 首次检测到硬锁死时，打印紧急日志（CPU 信息、模块列表、中断跟踪、寄存器状态或栈回溯）。\n   - 若启用 `sysctl_hardlockup_all_cpu_backtrace`，触发其他 CPU 的 backtrace。\n   - 若 `hardlockup_panic` 为真，调用 `nmi_panic()` 触发内核 panic。\n   - 设置 `watchdog_hardlockup_warned` 避免重复告警。\n\n### 启动参数与配置\n- **`nmi_watchdog=` 参数**：通过 `__setup` 宏注册，支持以下值：\n  - `panic`/`nopanic`：设置 `hardlockup_panic`。\n  - `0`/`1`：启用/禁用硬锁死检测。\n  - `r...`：传递参数给 perf-based 检测器（`hardlockup_config_perf_event`）。\n- **早期禁用**：`hardlockup_detector_disable()` 可在解析命令行前禁用硬检测（如 KVM guest）。\n\n### 架构交互与豁免\n- **`arch_touch_nmi_watchdog()`**：允许架构代码或关键内核路径（如 printk）临时豁免硬 watchdog 检测，防止在已知安全的长操作中误报。使用 `raw_cpu_write` 确保在抢占/中断使能环境下安全。\n\n### 弱符号扩展点\n- `watchdog_hardlockup_enable/disable/probe` 声明为 `__weak`，允许不同架构或检测方法（如基于 perf event 的 NMI watchdog）提供具体实现，实现检测机制的可插拔。\n\n## 4. 依赖关系\n\n- **内核子系统**：\n  - `<linux/nmi.h>`：NMI 处理框架，硬锁死检测通常在 NMI 上下文触发。\n  - `<linux/hrtimer.h>`（隐含）：高精度定时器中断作为检测心跳源。\n  - `<linux/sched/*.h>`：调度器相关（`print_irqtrace_events`, `dump_stack`）。\n  - `<linux/sysctl.h>`：提供 `sysctl_hardlockup_all_cpu_backtrace` 控制接口。\n  - `<linux/sysfs.h>`：暴露 `hardlockup_count` 到 sysfs。\n  - `<asm/irq_regs.h>`：获取中断上下文寄存器状态（`show_regs`）。\n- **配置选项**：\n  - `CONFIG_HARDLOCKUP_DETECTOR`：启用硬锁死检测框架。\n  - `CONFIG_HARDLOCKUP_DETECTOR_COUNTS_HRTIMER`：使用 hrtimer 中断计数实现检测。\n  - `CONFIG_HARDLOCKUP_DETECTOR_SPARC64`：SPARC64 架构默认启用硬检测。\n  - `CONFIG_BOOTPARAM_HARDLOCKUP_PANIC`：设置默认 panic 行为。\n  - `CONFIG_SMP`：多核支持（`all_cpu_backtrace` 功能）。\n  - `CONFIG_SYSFS`：sysfs 接口支持。\n- **其他模块**：依赖具体架构的 NMI 实现（如 x86 的 perf-based NMI watchdog）提供检测触发点。\n\n## 5. 使用场景\n\n- **系统稳定性监控**：在生产服务器或嵌入式设备中持续监控 CPU 响应性，及时发现硬件故障、驱动 bug 或内核死锁导致的系统挂死。\n- **内核调试**：开发人员通过 watchdog 触发的 backtrace 和寄存器转储，定位导致系统无响应的代码路径。\n- **虚拟化环境**：在 hypervisor guest 中可选择性禁用硬 watchdog（因虚拟化开销可能导致误报），通过 `hardlockup_detector_disable()` 或启动参数控制。\n- **实时系统**：结合 CPU 隔离（`isolcpus`）和 watchdog 配置，确保关键 CPU 核心的响应性，同时避免在非关键核上产生干扰。\n- **panic 策略**：通过 `hardlockup_panic` 配置，使系统在硬锁死时自动重启，提高无人值守系统的可用性。",
      "similarity": 0.5428292751312256,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/watchdog.c",
          "start_line": 73,
          "end_line": 217,
          "content": [
            "static ssize_t hardlockup_count_show(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\t\t     char *page)",
            "{",
            "\treturn sysfs_emit(page, \"%u\\n\", hardlockup_count);",
            "}",
            "static __init int kernel_hardlockup_sysfs_init(void)",
            "{",
            "\tsysfs_add_file_to_group(kernel_kobj, &hardlockup_count_attr.attr, NULL);",
            "\treturn 0;",
            "}",
            "void __init hardlockup_detector_disable(void)",
            "{",
            "\twatchdog_hardlockup_user_enabled = 0;",
            "}",
            "static int __init hardlockup_panic_setup(char *str)",
            "{",
            "next:",
            "\tif (!strncmp(str, \"panic\", 5))",
            "\t\thardlockup_panic = 1;",
            "\telse if (!strncmp(str, \"nopanic\", 7))",
            "\t\thardlockup_panic = 0;",
            "\telse if (!strncmp(str, \"0\", 1))",
            "\t\twatchdog_hardlockup_user_enabled = 0;",
            "\telse if (!strncmp(str, \"1\", 1))",
            "\t\twatchdog_hardlockup_user_enabled = 1;",
            "\telse if (!strncmp(str, \"r\", 1))",
            "\t\thardlockup_config_perf_event(str + 1);",
            "\twhile (*(str++)) {",
            "\t\tif (*str == ',') {",
            "\t\t\tstr++;",
            "\t\t\tgoto next;",
            "\t\t}",
            "\t}",
            "\treturn 1;",
            "}",
            "notrace void arch_touch_nmi_watchdog(void)",
            "{",
            "\t/*",
            "\t * Using __raw here because some code paths have",
            "\t * preemption enabled.  If preemption is enabled",
            "\t * then interrupts should be enabled too, in which",
            "\t * case we shouldn't have to worry about the watchdog",
            "\t * going off.",
            "\t */",
            "\traw_cpu_write(watchdog_hardlockup_touched, true);",
            "}",
            "void watchdog_hardlockup_touch_cpu(unsigned int cpu)",
            "{",
            "\tper_cpu(watchdog_hardlockup_touched, cpu) = true;",
            "}",
            "static bool is_hardlockup(unsigned int cpu)",
            "{",
            "\tint hrint = atomic_read(&per_cpu(hrtimer_interrupts, cpu));",
            "",
            "\tif (per_cpu(hrtimer_interrupts_saved, cpu) == hrint)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * NOTE: we don't need any fancy atomic_t or READ_ONCE/WRITE_ONCE",
            "\t * for hrtimer_interrupts_saved. hrtimer_interrupts_saved is",
            "\t * written/read by a single CPU.",
            "\t */",
            "\tper_cpu(hrtimer_interrupts_saved, cpu) = hrint;",
            "",
            "\treturn false;",
            "}",
            "static void watchdog_hardlockup_kick(void)",
            "{",
            "\tint new_interrupts;",
            "",
            "\tnew_interrupts = atomic_inc_return(this_cpu_ptr(&hrtimer_interrupts));",
            "\twatchdog_buddy_check_hardlockup(new_interrupts);",
            "}",
            "void watchdog_hardlockup_check(unsigned int cpu, struct pt_regs *regs)",
            "{",
            "\tif (per_cpu(watchdog_hardlockup_touched, cpu)) {",
            "\t\tper_cpu(watchdog_hardlockup_touched, cpu) = false;",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Check for a hardlockup by making sure the CPU's timer",
            "\t * interrupt is incrementing. The timer interrupt should have",
            "\t * fired multiple times before we overflow'd. If it hasn't",
            "\t * then this is a good indication the cpu is stuck",
            "\t */",
            "\tif (is_hardlockup(cpu)) {",
            "\t\tunsigned int this_cpu = smp_processor_id();",
            "\t\tunsigned long flags;",
            "",
            "#ifdef CONFIG_SYSFS",
            "\t\t++hardlockup_count;",
            "#endif",
            "",
            "\t\t/* Only print hardlockups once. */",
            "\t\tif (per_cpu(watchdog_hardlockup_warned, cpu))",
            "\t\t\treturn;",
            "",
            "\t\t/*",
            "\t\t * Prevent multiple hard-lockup reports if one cpu is already",
            "\t\t * engaged in dumping all cpu back traces.",
            "\t\t */",
            "\t\tif (sysctl_hardlockup_all_cpu_backtrace) {",
            "\t\t\tif (test_and_set_bit_lock(0, &hard_lockup_nmi_warn))",
            "\t\t\t\treturn;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * NOTE: we call printk_cpu_sync_get_irqsave() after printing",
            "\t\t * the lockup message. While it would be nice to serialize",
            "\t\t * that printout, we really want to make sure that if some",
            "\t\t * other CPU somehow locked up while holding the lock associated",
            "\t\t * with printk_cpu_sync_get_irqsave() that we can still at least",
            "\t\t * get the message about the lockup out.",
            "\t\t */",
            "\t\tpr_emerg(\"CPU%u: Watchdog detected hard LOCKUP on cpu %u\\n\", this_cpu, cpu);",
            "\t\tprintk_cpu_sync_get_irqsave(flags);",
            "",
            "\t\tprint_modules();",
            "\t\tprint_irqtrace_events(current);",
            "\t\tif (cpu == this_cpu) {",
            "\t\t\tif (regs)",
            "\t\t\t\tshow_regs(regs);",
            "\t\t\telse",
            "\t\t\t\tdump_stack();",
            "\t\t\tprintk_cpu_sync_put_irqrestore(flags);",
            "\t\t} else {",
            "\t\t\tprintk_cpu_sync_put_irqrestore(flags);",
            "\t\t\ttrigger_single_cpu_backtrace(cpu);",
            "\t\t}",
            "",
            "\t\tif (sysctl_hardlockup_all_cpu_backtrace) {",
            "\t\t\ttrigger_allbutcpu_cpu_backtrace(cpu);",
            "\t\t\tif (!hardlockup_panic)",
            "\t\t\t\tclear_bit_unlock(0, &hard_lockup_nmi_warn);",
            "\t\t}",
            "",
            "\t\tif (hardlockup_panic)",
            "\t\t\tnmi_panic(regs, \"Hard LOCKUP\");",
            "",
            "\t\tper_cpu(watchdog_hardlockup_warned, cpu) = true;",
            "\t} else {",
            "\t\tper_cpu(watchdog_hardlockup_warned, cpu) = false;",
            "\t}",
            "}"
          ],
          "function_name": "hardlockup_count_show, kernel_hardlockup_sysfs_init, hardlockup_detector_disable, hardlockup_panic_setup, arch_touch_nmi_watchdog, watchdog_hardlockup_touch_cpu, is_hardlockup, watchdog_hardlockup_kick, watchdog_hardlockup_check",
          "description": "实现硬锁检测核心逻辑，包含硬锁判断、计数统计、NMI触发电路及异常上报功能，通过中断计数器检测CPU卡顿。",
          "similarity": 0.5065209269523621
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/watchdog.c",
          "start_line": 1,
          "end_line": 72,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Detect hard and soft lockups on a system",
            " *",
            " * started by Don Zickus, Copyright (C) 2010 Red Hat, Inc.",
            " *",
            " * Note: Most of this code is borrowed heavily from the original softlockup",
            " * detector, so thanks to Ingo for the initial implementation.",
            " * Some chunks also taken from the old x86-specific nmi watchdog code, thanks",
            " * to those contributors as well.",
            " */",
            "",
            "#define pr_fmt(fmt) \"watchdog: \" fmt",
            "",
            "#include <linux/cpu.h>",
            "#include <linux/init.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/irq.h>",
            "#include <linux/irqdesc.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/kvm_para.h>",
            "#include <linux/math64.h>",
            "#include <linux/mm.h>",
            "#include <linux/module.h>",
            "#include <linux/nmi.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/tick.h>",
            "",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/isolation.h>",
            "",
            "#include <asm/irq_regs.h>",
            "",
            "static DEFINE_MUTEX(watchdog_mutex);",
            "",
            "#if defined(CONFIG_HARDLOCKUP_DETECTOR) || defined(CONFIG_HARDLOCKUP_DETECTOR_SPARC64)",
            "# define WATCHDOG_HARDLOCKUP_DEFAULT\t1",
            "#else",
            "# define WATCHDOG_HARDLOCKUP_DEFAULT\t0",
            "#endif",
            "",
            "#define NUM_SAMPLE_PERIODS\t5",
            "",
            "unsigned long __read_mostly watchdog_enabled;",
            "int __read_mostly watchdog_user_enabled = 1;",
            "static int __read_mostly watchdog_hardlockup_user_enabled = WATCHDOG_HARDLOCKUP_DEFAULT;",
            "static int __read_mostly watchdog_softlockup_user_enabled = 1;",
            "int __read_mostly watchdog_thresh = 10;",
            "static int __read_mostly watchdog_thresh_next;",
            "static int __read_mostly watchdog_hardlockup_available;",
            "",
            "struct cpumask watchdog_cpumask __read_mostly;",
            "unsigned long *watchdog_cpumask_bits = cpumask_bits(&watchdog_cpumask);",
            "",
            "#ifdef CONFIG_HARDLOCKUP_DETECTOR",
            "",
            "# ifdef CONFIG_SMP",
            "int __read_mostly sysctl_hardlockup_all_cpu_backtrace;",
            "# endif /* CONFIG_SMP */",
            "",
            "/*",
            " * Should we panic when a soft-lockup or hard-lockup occurs:",
            " */",
            "unsigned int __read_mostly hardlockup_panic =",
            "\t\t\tIS_ENABLED(CONFIG_BOOTPARAM_HARDLOCKUP_PANIC);",
            "",
            "#ifdef CONFIG_SYSFS",
            "",
            "static unsigned int hardlockup_count;",
            ""
          ],
          "function_name": null,
          "description": "定义硬锁和软锁检测模块的全局变量及配置选项，初始化硬锁检测相关数据结构和默认启用状态。",
          "similarity": 0.4915972352027893
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/watchdog.c",
          "start_line": 494,
          "end_line": 595,
          "content": [
            "static void tabulate_irq_count(struct irq_counts *irq_counts, int irq, u32 counts, int rank)",
            "{",
            "\tint i;",
            "\tstruct irq_counts new_count = {irq, counts};",
            "",
            "\tfor (i = 0; i < rank; i++) {",
            "\t\tif (counts > irq_counts[i].counts)",
            "\t\t\tswap(new_count, irq_counts[i]);",
            "\t}",
            "}",
            "static bool need_counting_irqs(void)",
            "{",
            "\tu8 util;",
            "\tint tail = __this_cpu_read(cpustat_tail);",
            "",
            "\ttail = (tail + NUM_HARDIRQ_REPORT - 1) % NUM_HARDIRQ_REPORT;",
            "\tutil = __this_cpu_read(cpustat_util[tail][STATS_HARDIRQ]);",
            "\treturn util > HARDIRQ_PERCENT_THRESH;",
            "}",
            "static void start_counting_irqs(void)",
            "{",
            "\tif (!__this_cpu_read(snapshot_taken)) {",
            "\t\tkstat_snapshot_irqs();",
            "\t\t__this_cpu_write(snapshot_taken, true);",
            "\t}",
            "}",
            "static void stop_counting_irqs(void)",
            "{",
            "\t__this_cpu_write(snapshot_taken, false);",
            "}",
            "static void print_irq_counts(void)",
            "{",
            "\tunsigned int i, count;",
            "\tstruct irq_counts irq_counts_sorted[NUM_HARDIRQ_REPORT] = {",
            "\t\t{-1, 0}, {-1, 0}, {-1, 0}, {-1, 0}, {-1, 0}",
            "\t};",
            "",
            "\tif (__this_cpu_read(snapshot_taken)) {",
            "\t\tfor_each_active_irq(i) {",
            "\t\t\tcount = kstat_get_irq_since_snapshot(i);",
            "\t\t\ttabulate_irq_count(irq_counts_sorted, i, count, NUM_HARDIRQ_REPORT);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Outputting the \"watchdog\" prefix on every line is redundant and not",
            "\t\t * concise, and the original alarm information is sufficient for",
            "\t\t * positioning in logs, hence here printk() is used instead of pr_crit().",
            "\t\t */",
            "\t\tprintk(KERN_CRIT \"CPU#%d Detect HardIRQ Time exceeds %d%%. Most frequent HardIRQs:\\n\",",
            "\t\t       smp_processor_id(), HARDIRQ_PERCENT_THRESH);",
            "",
            "\t\tfor (i = 0; i < NUM_HARDIRQ_REPORT; i++) {",
            "\t\t\tif (irq_counts_sorted[i].irq == -1)",
            "\t\t\t\tbreak;",
            "",
            "\t\t\tprintk(KERN_CRIT \"\\t#%u: %-10u\\tirq#%d\\n\",",
            "\t\t\t       i + 1, irq_counts_sorted[i].counts,",
            "\t\t\t       irq_counts_sorted[i].irq);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If the hardirq time is less than HARDIRQ_PERCENT_THRESH% in the last",
            "\t\t * sample_period, then we suspect the interrupt storm might be subsiding.",
            "\t\t */",
            "\t\tif (!need_counting_irqs())",
            "\t\t\tstop_counting_irqs();",
            "\t}",
            "}",
            "static void report_cpu_status(void)",
            "{",
            "\tprint_cpustat();",
            "\tprint_irq_counts();",
            "}",
            "static inline void update_cpustat(void) { }",
            "static inline void report_cpu_status(void) { }",
            "static inline bool need_counting_irqs(void) { return false; }",
            "static inline void start_counting_irqs(void) { }",
            "static inline void stop_counting_irqs(void) { }",
            "static int get_softlockup_thresh(void)",
            "{",
            "\treturn watchdog_thresh * 2;",
            "}",
            "static unsigned long get_timestamp(void)",
            "{",
            "\treturn running_clock() >> 30LL;  /* 2^30 ~= 10^9 */",
            "}",
            "static void set_sample_period(void)",
            "{",
            "\t/*",
            "\t * convert watchdog_thresh from seconds to ns",
            "\t * the divide by 5 is to give hrtimer several chances (two",
            "\t * or three with the current relation between the soft",
            "\t * and hard thresholds) to increment before the",
            "\t * hardlockup detector generates a warning",
            "\t */",
            "\tsample_period = get_softlockup_thresh() * ((u64)NSEC_PER_SEC / NUM_SAMPLE_PERIODS);",
            "\twatchdog_update_hrtimer_threshold(sample_period);",
            "}",
            "static void update_report_ts(void)",
            "{",
            "\t__this_cpu_write(watchdog_report_ts, get_timestamp());",
            "}"
          ],
          "function_name": "tabulate_irq_count, need_counting_irqs, start_counting_irqs, stop_counting_irqs, print_irq_counts, report_cpu_status, update_cpustat, report_cpu_status, need_counting_irqs, start_counting_irqs, stop_counting_irqs, get_softlockup_thresh, get_timestamp, set_sample_period, update_report_ts",
          "description": "实现中断统计分析模块，通过记录中断次数分布识别潜在中断风暴，为软锁检测提供上下文信息。",
          "similarity": 0.48577529191970825
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/watchdog.c",
          "start_line": 626,
          "end_line": 802,
          "content": [
            "static void update_touch_ts(void)",
            "{",
            "\t__this_cpu_write(watchdog_touch_ts, get_timestamp());",
            "\tupdate_report_ts();",
            "}",
            "notrace void touch_softlockup_watchdog_sched(void)",
            "{",
            "\t/*",
            "\t * Preemption can be enabled.  It doesn't matter which CPU's watchdog",
            "\t * report period gets restarted here, so use the raw_ operation.",
            "\t */",
            "\traw_cpu_write(watchdog_report_ts, SOFTLOCKUP_DELAY_REPORT);",
            "}",
            "notrace void touch_softlockup_watchdog(void)",
            "{",
            "\ttouch_softlockup_watchdog_sched();",
            "\twq_watchdog_touch(raw_smp_processor_id());",
            "}",
            "void touch_all_softlockup_watchdogs(void)",
            "{",
            "\tint cpu;",
            "",
            "\t/*",
            "\t * watchdog_mutex cannpt be taken here, as this might be called",
            "\t * from (soft)interrupt context, so the access to",
            "\t * watchdog_allowed_cpumask might race with a concurrent update.",
            "\t *",
            "\t * The watchdog time stamp can race against a concurrent real",
            "\t * update as well, the only side effect might be a cycle delay for",
            "\t * the softlockup check.",
            "\t */",
            "\tfor_each_cpu(cpu, &watchdog_allowed_mask) {",
            "\t\tper_cpu(watchdog_report_ts, cpu) = SOFTLOCKUP_DELAY_REPORT;",
            "\t\twq_watchdog_touch(cpu);",
            "\t}",
            "}",
            "void touch_softlockup_watchdog_sync(void)",
            "{",
            "\t__this_cpu_write(softlockup_touch_sync, true);",
            "\t__this_cpu_write(watchdog_report_ts, SOFTLOCKUP_DELAY_REPORT);",
            "}",
            "static int is_softlockup(unsigned long touch_ts,",
            "\t\t\t unsigned long period_ts,",
            "\t\t\t unsigned long now)",
            "{",
            "\tif ((watchdog_enabled & WATCHDOG_SOFTOCKUP_ENABLED) && watchdog_thresh) {",
            "\t\t/*",
            "\t\t * If period_ts has not been updated during a sample_period, then",
            "\t\t * in the subsequent few sample_periods, period_ts might also not",
            "\t\t * be updated, which could indicate a potential softlockup. In",
            "\t\t * this case, if we suspect the cause of the potential softlockup",
            "\t\t * might be interrupt storm, then we need to count the interrupts",
            "\t\t * to find which interrupt is storming.",
            "\t\t */",
            "\t\tif (time_after_eq(now, period_ts + get_softlockup_thresh() / NUM_SAMPLE_PERIODS) &&",
            "\t\t    need_counting_irqs())",
            "\t\t\tstart_counting_irqs();",
            "",
            "\t\t/* Warn about unreasonable delays. */",
            "\t\tif (time_after(now, period_ts + get_softlockup_thresh()))",
            "\t\t\treturn now - touch_ts;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int softlockup_fn(void *data)",
            "{",
            "\tupdate_touch_ts();",
            "\tstop_counting_irqs();",
            "\tcomplete(this_cpu_ptr(&softlockup_completion));",
            "",
            "\treturn 0;",
            "}",
            "static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)",
            "{",
            "\tunsigned long touch_ts, period_ts, now;",
            "\tstruct pt_regs *regs = get_irq_regs();",
            "\tint duration;",
            "\tint softlockup_all_cpu_backtrace = sysctl_softlockup_all_cpu_backtrace;",
            "\tunsigned long flags;",
            "",
            "\tif (!watchdog_enabled)",
            "\t\treturn HRTIMER_NORESTART;",
            "",
            "\twatchdog_hardlockup_kick();",
            "",
            "\t/* kick the softlockup detector */",
            "\tif (completion_done(this_cpu_ptr(&softlockup_completion))) {",
            "\t\treinit_completion(this_cpu_ptr(&softlockup_completion));",
            "\t\tstop_one_cpu_nowait(smp_processor_id(),",
            "\t\t\t\tsoftlockup_fn, NULL,",
            "\t\t\t\tthis_cpu_ptr(&softlockup_stop_work));",
            "\t}",
            "",
            "\t/* .. and repeat */",
            "\thrtimer_forward_now(hrtimer, ns_to_ktime(sample_period));",
            "",
            "\t/*",
            "\t * Read the current timestamp first. It might become invalid anytime",
            "\t * when a virtual machine is stopped by the host or when the watchog",
            "\t * is touched from NMI.",
            "\t */",
            "\tnow = get_timestamp();",
            "\t/*",
            "\t * If a virtual machine is stopped by the host it can look to",
            "\t * the watchdog like a soft lockup. This function touches the watchdog.",
            "\t */",
            "\tkvm_check_and_clear_guest_paused();",
            "\t/*",
            "\t * The stored timestamp is comparable with @now only when not touched.",
            "\t * It might get touched anytime from NMI. Make sure that is_softlockup()",
            "\t * uses the same (valid) value.",
            "\t */",
            "\tperiod_ts = READ_ONCE(*this_cpu_ptr(&watchdog_report_ts));",
            "",
            "\tupdate_cpustat();",
            "",
            "\t/* Reset the interval when touched by known problematic code. */",
            "\tif (period_ts == SOFTLOCKUP_DELAY_REPORT) {",
            "\t\tif (unlikely(__this_cpu_read(softlockup_touch_sync))) {",
            "\t\t\t/*",
            "\t\t\t * If the time stamp was touched atomically",
            "\t\t\t * make sure the scheduler tick is up to date.",
            "\t\t\t */",
            "\t\t\t__this_cpu_write(softlockup_touch_sync, false);",
            "\t\t\tsched_clock_tick();",
            "\t\t}",
            "",
            "\t\tupdate_report_ts();",
            "\t\treturn HRTIMER_RESTART;",
            "\t}",
            "",
            "\t/* Check for a softlockup. */",
            "\ttouch_ts = __this_cpu_read(watchdog_touch_ts);",
            "\tduration = is_softlockup(touch_ts, period_ts, now);",
            "\tif (unlikely(duration)) {",
            "#ifdef CONFIG_SYSFS",
            "\t\t++softlockup_count;",
            "#endif",
            "",
            "\t\t/*",
            "\t\t * Prevent multiple soft-lockup reports if one cpu is already",
            "\t\t * engaged in dumping all cpu back traces.",
            "\t\t */",
            "\t\tif (softlockup_all_cpu_backtrace) {",
            "\t\t\tif (test_and_set_bit_lock(0, &soft_lockup_nmi_warn))",
            "\t\t\t\treturn HRTIMER_RESTART;",
            "\t\t}",
            "",
            "\t\t/* Start period for the next softlockup warning. */",
            "\t\tupdate_report_ts();",
            "",
            "\t\tprintk_cpu_sync_get_irqsave(flags);",
            "\t\tpr_emerg(\"BUG: soft lockup - CPU#%d stuck for %us! [%s:%d]\\n\",",
            "\t\t\tsmp_processor_id(), duration,",
            "\t\t\tcurrent->comm, task_pid_nr(current));",
            "\t\treport_cpu_status();",
            "\t\tprint_modules();",
            "\t\tprint_irqtrace_events(current);",
            "\t\tif (regs)",
            "\t\t\tshow_regs(regs);",
            "\t\telse",
            "\t\t\tdump_stack();",
            "\t\tprintk_cpu_sync_put_irqrestore(flags);",
            "",
            "\t\tif (softlockup_all_cpu_backtrace) {",
            "\t\t\ttrigger_allbutcpu_cpu_backtrace(smp_processor_id());",
            "\t\t\tif (!softlockup_panic)",
            "\t\t\t\tclear_bit_unlock(0, &soft_lockup_nmi_warn);",
            "\t\t}",
            "",
            "\t\tadd_taint(TAINT_SOFTLOCKUP, LOCKDEP_STILL_OK);",
            "\t\tif (softlockup_panic)",
            "\t\t\tpanic(\"softlockup: hung tasks\");",
            "\t}",
            "",
            "\treturn HRTIMER_RESTART;",
            "}"
          ],
          "function_name": "update_touch_ts, touch_softlockup_watchdog_sched, touch_softlockup_watchdog, touch_all_softlockup_watchdogs, touch_softlockup_watchdog_sync, is_softlockup, softlockup_fn, watchdog_timer_fn",
          "description": "处理软锁检测时序逻辑，包含超时判定算法、任务栈回溯触发机制及异常处理流程，协调硬件定时器与软件检测模块。",
          "similarity": 0.4822577238082886
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/watchdog.c",
          "start_line": 1114,
          "end_line": 1190,
          "content": [
            "int proc_watchdog_cpumask(struct ctl_table *table, int write,",
            "\t\t\t  void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint err;",
            "",
            "\tmutex_lock(&watchdog_mutex);",
            "",
            "\terr = proc_do_large_bitmap(table, write, buffer, lenp, ppos);",
            "\tif (!err && write)",
            "\t\tproc_watchdog_update(false);",
            "",
            "\tmutex_unlock(&watchdog_mutex);",
            "\treturn err;",
            "}",
            "static void __init watchdog_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", watchdog_sysctls);",
            "",
            "\tif (watchdog_hardlockup_available)",
            "\t\twatchdog_hardlockup_sysctl[0].mode = 0644;",
            "\tregister_sysctl_init(\"kernel\", watchdog_hardlockup_sysctl);",
            "}",
            "static void __init lockup_detector_delay_init(struct work_struct *work)",
            "{",
            "\tint ret;",
            "",
            "\tret = watchdog_hardlockup_probe();",
            "\tif (ret) {",
            "\t\tif (ret == -ENODEV)",
            "\t\t\tpr_info(\"NMI not fully supported\\n\");",
            "\t\telse",
            "\t\t\tpr_info(\"Delayed init of the lockup detector failed: %d\\n\", ret);",
            "\t\tpr_info(\"Hard watchdog permanently disabled\\n\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tallow_lockup_detector_init_retry = false;",
            "",
            "\twatchdog_hardlockup_available = true;",
            "\tlockup_detector_setup();",
            "}",
            "void __init lockup_detector_retry_init(void)",
            "{",
            "\t/* Must be called before late init calls */",
            "\tif (!allow_lockup_detector_init_retry)",
            "\t\treturn;",
            "",
            "\tschedule_work(&detector_work);",
            "}",
            "static int __init lockup_detector_check(void)",
            "{",
            "\t/* Prevent any later retry. */",
            "\tallow_lockup_detector_init_retry = false;",
            "",
            "\t/* Make sure no work is pending. */",
            "\tflush_work(&detector_work);",
            "",
            "\twatchdog_sysctl_init();",
            "",
            "\treturn 0;",
            "",
            "}",
            "void __init lockup_detector_init(void)",
            "{",
            "\tif (tick_nohz_full_enabled())",
            "\t\tpr_info(\"Disabling watchdog on nohz_full cores by default\\n\");",
            "",
            "\tcpumask_copy(&watchdog_cpumask,",
            "\t\t     housekeeping_cpumask(HK_TYPE_TIMER));",
            "",
            "\tif (!watchdog_hardlockup_probe())",
            "\t\twatchdog_hardlockup_available = true;",
            "\telse",
            "\t\tallow_lockup_detector_init_retry = true;",
            "",
            "\tlockup_detector_setup();",
            "}"
          ],
          "function_name": "proc_watchdog_cpumask, watchdog_sysctl_init, lockup_detector_delay_init, lockup_detector_retry_init, lockup_detector_check, lockup_detector_init",
          "description": "实现看门狗子系统的初始化流程，包含sysctl参数注册、延迟初始化工作队列、CPU掩码配置及探测器状态同步机制",
          "similarity": 0.47512251138687134
        }
      ]
    },
    {
      "source_file": "kernel/locking/osq_lock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:43:41\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\osq_lock.c`\n\n---\n\n# `locking/osq_lock.c` 技术文档\n\n## 1. 文件概述\n\n`osq_lock.c` 实现了一种专为**乐观自旋（Optimistic Spinning）**设计的轻量级排队自旋锁机制，称为 **OSQ（Optimistic Spin Queue）锁**。该机制主要用于支持如互斥锁（mutex）、读写信号量（rwsem）等**可睡眠锁**在争用时进行乐观自旋，以避免不必要的上下文切换和调度开销。OSQ 锁基于 MCS（Mellor-Crummey and Scott）锁的思想，但针对 Linux 内核的调度和抢占模型进行了优化，利用每个 CPU 的静态 per-CPU 节点结构，确保在禁用抢占的自旋上下文中安全使用。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct optimistic_spin_node`：每个 CPU 对应一个静态节点，包含：\n  - `cpu`：编码后的 CPU 编号（实际值 = CPU ID + 1）\n  - `locked`：布尔标志，表示是否已获得锁\n  - `next`：指向队列中下一个节点的指针\n  - `prev`：指向前一个节点的指针\n- `struct optimistic_spin_queue`：OSQ 锁结构体，仅包含一个原子变量 `tail`，用于指向队列尾部（编码后的 CPU 编号），`OSQ_UNLOCKED_VAL`（值为 0）表示无锁。\n\n### 主要函数\n- `bool osq_lock(struct optimistic_spin_queue *lock)`  \n  尝试获取 OSQ 锁。若成功获得锁或决定放弃自旋（如需要调度或前驱被抢占），返回 `true`；若成功排队但未获得锁且需继续等待，则返回 `false`（实际逻辑中，失败路径最终也返回 `false` 表示未获得锁）。\n  \n- `void osq_unlock(struct optimistic_spin_queue *lock)`  \n  释放 OSQ 锁，唤醒队列中的下一个等待者（若存在）。\n\n- `static inline struct optimistic_spin_node *osq_wait_next(...)`  \n  辅助函数，用于在解锁或取消排队时安全地获取下一个节点，并处理队列尾部的原子更新。\n\n- `encode_cpu()` / `decode_cpu()` / `node_cpu()`  \n  用于在 CPU 编号与 per-CPU 节点指针之间进行编码/解码转换，其中 CPU 编号 0 被编码为 1，以 0 表示“无 CPU”（即锁空闲）。\n\n## 3. 关键实现\n\n### Per-CPU 静态节点设计\n- 每个 CPU 拥有一个静态的 `osq_node`（通过 `DEFINE_PER_CPU_SHARED_ALIGNED` 定义），避免动态分配开销。\n- 由于 OSQ 仅在**禁用抢占**的上下文中使用（如 mutex 的乐观自旋阶段），且**不可在中断上下文调用**，因此 per-CPU 节点的生命周期安全。\n\n### 锁获取流程 (`osq_lock`)\n1. **初始化本地节点**：设置 `locked=0`、`next=NULL`，并确保 `cpu` 字段为当前 CPU 编码值。\n2. **原子交换尾指针**：通过 `atomic_xchg(&lock->tail, curr)` 尝试入队。若原值为 `OSQ_UNLOCKED_VAL`，直接获得锁。\n3. **链接到前驱**：若已有前驱（`prev`），通过 `smp_wmb()` 确保内存顺序后，设置 `prev->next = node`。\n4. **自旋等待**：使用 `smp_cond_load_relaxed()` 等待 `node->locked` 变为 1，或满足退出条件（`need_resched()` 或前驱 CPU 被抢占 `vcpu_is_preempted()`）。\n5. **取消排队（Unqueue）**：若需退出自旋：\n   - **Step A**：尝试将 `prev->next` 置为 `NULL`，断开链接。\n   - **Step B**：调用 `osq_wait_next()` 确定下一个节点，并可能将锁尾指针回退。\n   - **Step C**：若存在 `next`，将其与 `prev` 直接链接，完成队列修复。\n\n### 锁释放流程 (`osq_unlock`)\n1. **快速路径**：若当前 CPU 是唯一持有者（`tail == curr`），直接将 `tail` 设为 `OSQ_UNLOCKED_VAL`。\n2. **慢速路径**：\n   - 若本地节点的 `next` 非空，直接设置 `next->locked = 1` 唤醒后继。\n   - 否则调用 `osq_wait_next()` 获取下一个节点（处理并发取消排队的情况），再唤醒。\n\n### 内存屏障与原子操作\n- 使用 `atomic_xchg`、`atomic_cmpxchg_acquire/release` 确保对 `lock->tail` 的操作具有适当的内存序。\n- `smp_wmb()` 保证在设置 `prev->next` 前，本地节点的初始化对其他 CPU 可见。\n- `WRITE_ONCE`/`READ_ONCE` 防止编译器优化破坏并发访问语义。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/percpu.h>`：提供 per-CPU 变量支持（`this_cpu_ptr`, `per_cpu_ptr`）。\n  - `<linux/sched.h>`：提供调度相关函数（`need_resched()`）和虚拟 CPU 抢占检测（`vcpu_is_preempted()`）。\n  - `<linux/osq_lock.h>`：定义 `struct optimistic_spin_queue`、`struct optimistic_spin_node` 及 `OSQ_UNLOCKED_VAL`。\n- **架构依赖**：依赖底层架构的原子操作（`atomic_*`）、内存屏障（`smp_wmb`, `smp_load_acquire`）和 CPU ID 获取（`smp_processor_id()`）。\n- **调度器集成**：与内核调度器紧密协作，通过 `need_resched()` 和 `vcpu_is_preempted()` 决定是否继续自旋。\n\n## 5. 使用场景\n\nOSQ 锁主要用于**可睡眠锁的乐观自旋优化**，典型场景包括：\n- **Mutex（互斥锁）**：在 `mutex_spin_on_owner()` 中，若锁持有者正在运行，当前 CPU 会尝试 OSQ 自旋而非立即睡眠。\n- **Rwsem（读写信号量）**：在写者争用时，若满足条件，会使用 OSQ 进行乐观自旋。\n- **其他睡眠锁**：任何希望在锁争用时避免立即进入睡眠、以降低延迟的同步原语。\n\n其核心价值在于：当锁持有者很可能在**另一个 CPU 上运行且未被抢占**时，通过短暂自旋可避免昂贵的上下文切换，提升性能；同时通过 `vcpu_is_preempted()` 检测虚拟化环境中的抢占，避免在持有者已让出 CPU 时无效自旋。",
      "similarity": 0.5339915752410889,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 20,
          "end_line": 149,
          "content": [
            "static inline int encode_cpu(int cpu_nr)",
            "{",
            "\treturn cpu_nr + 1;",
            "}",
            "static inline int node_cpu(struct optimistic_spin_node *node)",
            "{",
            "\treturn node->cpu - 1;",
            "}",
            "bool osq_lock(struct optimistic_spin_queue *lock)",
            "{",
            "\tstruct optimistic_spin_node *node = this_cpu_ptr(&osq_node);",
            "\tstruct optimistic_spin_node *prev, *next;",
            "\tint curr = encode_cpu(smp_processor_id());",
            "\tint old;",
            "",
            "\tnode->locked = 0;",
            "\tnode->next = NULL;",
            "\t/*",
            "\t * After this cpu member is initialized for the first time, it",
            "\t * would no longer change in fact. That could avoid cache misses",
            "\t * when spin and access the cpu member by other CPUs.",
            "\t */",
            "\tif (node->cpu != curr)",
            "\t\tnode->cpu = curr;",
            "",
            "\t/*",
            "\t * We need both ACQUIRE (pairs with corresponding RELEASE in",
            "\t * unlock() uncontended, or fastpath) and RELEASE (to publish",
            "\t * the node fields we just initialised) semantics when updating",
            "\t * the lock tail.",
            "\t */",
            "\told = atomic_xchg(&lock->tail, curr);",
            "\tif (old == OSQ_UNLOCKED_VAL)",
            "\t\treturn true;",
            "",
            "\tprev = decode_cpu(old);",
            "\tnode->prev = prev;",
            "",
            "\t/*",
            "\t * osq_lock()\t\t\tunqueue",
            "\t *",
            "\t * node->prev = prev\t\tosq_wait_next()",
            "\t * WMB\t\t\t\tMB",
            "\t * prev->next = node\t\tnext->prev = prev // unqueue-C",
            "\t *",
            "\t * Here 'node->prev' and 'next->prev' are the same variable and we need",
            "\t * to ensure these stores happen in-order to avoid corrupting the list.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\tWRITE_ONCE(prev->next, node);",
            "",
            "\t/*",
            "\t * Normally @prev is untouchable after the above store; because at that",
            "\t * moment unlock can proceed and wipe the node element from stack.",
            "\t *",
            "\t * However, since our nodes are static per-cpu storage, we're",
            "\t * guaranteed their existence -- this allows us to apply",
            "\t * cmpxchg in an attempt to undo our queueing.",
            "\t */",
            "",
            "\t/*",
            "\t * Wait to acquire the lock or cancellation. Note that need_resched()",
            "\t * will come with an IPI, which will wake smp_cond_load_relaxed() if it",
            "\t * is implemented with a monitor-wait. vcpu_is_preempted() relies on",
            "\t * polling, be careful.",
            "\t */",
            "\tif (smp_cond_load_relaxed(&node->locked, VAL || need_resched() ||",
            "\t\t\t\t  vcpu_is_preempted(node_cpu(node->prev))))",
            "\t\treturn true;",
            "",
            "\t/* unqueue */",
            "\t/*",
            "\t * Step - A  -- stabilize @prev",
            "\t *",
            "\t * Undo our @prev->next assignment; this will make @prev's",
            "\t * unlock()/unqueue() wait for a next pointer since @lock points to us",
            "\t * (or later).",
            "\t */",
            "",
            "\tfor (;;) {",
            "\t\t/*",
            "\t\t * cpu_relax() below implies a compiler barrier which would",
            "\t\t * prevent this comparison being optimized away.",
            "\t\t */",
            "\t\tif (data_race(prev->next) == node &&",
            "\t\t    cmpxchg(&prev->next, node, NULL) == node)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * We can only fail the cmpxchg() racing against an unlock(),",
            "\t\t * in which case we should observe @node->locked becoming",
            "\t\t * true.",
            "\t\t */",
            "\t\tif (smp_load_acquire(&node->locked))",
            "\t\t\treturn true;",
            "",
            "\t\tcpu_relax();",
            "",
            "\t\t/*",
            "\t\t * Or we race against a concurrent unqueue()'s step-B, in which",
            "\t\t * case its step-C will write us a new @node->prev pointer.",
            "\t\t */",
            "\t\tprev = READ_ONCE(node->prev);",
            "\t}",
            "",
            "\t/*",
            "\t * Step - B -- stabilize @next",
            "\t *",
            "\t * Similar to unlock(), wait for @node->next or move @lock from @node",
            "\t * back to @prev.",
            "\t */",
            "",
            "\tnext = osq_wait_next(lock, node, prev);",
            "\tif (!next)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Step - C -- unlink",
            "\t *",
            "\t * @prev is stable because its still waiting for a new @prev->next",
            "\t * pointer, @next is stable because our @node->next pointer is NULL and",
            "\t * it will wait in Step-A.",
            "\t */",
            "",
            "\tWRITE_ONCE(next->prev, prev);",
            "\tWRITE_ONCE(prev->next, next);",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "encode_cpu, node_cpu, osq_lock",
          "description": "实现osq_lock函数，负责获取乐观自旋锁。通过原子操作将当前节点插入队列，利用内存屏障保证顺序一致性，并通过循环等待条件满足或被唤醒，最终完成锁的获取过程。",
          "similarity": 0.4974510669708252
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 213,
          "end_line": 238,
          "content": [
            "void osq_unlock(struct optimistic_spin_queue *lock)",
            "{",
            "\tstruct optimistic_spin_node *node, *next;",
            "\tint curr = encode_cpu(smp_processor_id());",
            "",
            "\t/*",
            "\t * Fast path for the uncontended case.",
            "\t */",
            "\tif (likely(atomic_cmpxchg_release(&lock->tail, curr,",
            "\t\t\t\t\t  OSQ_UNLOCKED_VAL) == curr))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Second most likely case.",
            "\t */",
            "\tnode = this_cpu_ptr(&osq_node);",
            "\tnext = xchg(&node->next, NULL);",
            "\tif (next) {",
            "\t\tWRITE_ONCE(next->locked, 1);",
            "\t\treturn;",
            "\t}",
            "",
            "\tnext = osq_wait_next(lock, node, NULL);",
            "\tif (next)",
            "\t\tWRITE_ONCE(next->locked, 1);",
            "}"
          ],
          "function_name": "osq_unlock",
          "description": "实现osq_unlock函数，处理锁的释放。通过原子比较交换操作快速处理无竞争情况，否则查找并唤醒下一个等待节点，确保锁状态的正确性与线程安全。",
          "similarity": 0.49235421419143677
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 1,
          "end_line": 19,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/percpu.h>",
            "#include <linux/sched.h>",
            "#include <linux/osq_lock.h>",
            "",
            "/*",
            " * An MCS like lock especially tailored for optimistic spinning for sleeping",
            " * lock implementations (mutex, rwsem, etc).",
            " *",
            " * Using a single mcs node per CPU is safe because sleeping locks should not be",
            " * called from interrupt context and we have preemption disabled while",
            " * spinning.",
            " */",
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);",
            "",
            "/*",
            " * We use the value 0 to represent \"no CPU\", thus the encoded value",
            " * will be the CPU number incremented by 1.",
            " */"
          ],
          "function_name": null,
          "description": "定义全局的per-CPU乐观自旋节点osq_node，用于支持多CPU环境下乐观自旋锁的实现。通过encode_cpu和node_cpu函数处理CPU编号转换，为后续锁操作提供基础设施。",
          "similarity": 0.43798819184303284
        }
      ]
    }
  ]
}