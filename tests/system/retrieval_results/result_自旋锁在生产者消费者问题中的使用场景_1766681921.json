{
  "query": "自旋锁在生产者消费者问题中的使用场景",
  "timestamp": "2025-12-26 00:58:41",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/rwsem.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:51:36\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\rwsem.c`\n\n---\n\n# `locking/rwsem.c` 技术文档\n\n## 1. 文件概述\n\n`locking/rwsem.c` 是 Linux 内核中读写信号量（Read-Write Semaphore, rwsem）的核心实现文件，提供了对共享资源进行并发访问控制的机制。该机制允许多个读者并发访问资源，但写者必须独占访问。文件实现了 rwsem 的底层原子操作、锁获取/释放逻辑、乐观自旋（optimistic spinning）、写者锁抢占（lock-stealing）以及调试支持等功能，适用于高并发场景下的同步需求。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct rw_semaphore`：读写信号量的核心结构体，包含：\n  - `count`：原子长整型，编码了写者锁状态、等待者标志、移交标志、读者计数等信息。\n  - `owner`：记录当前锁持有者（写者任务指针或带标志的读者信息）。\n  - `wait_list`：等待队列，用于管理阻塞的读者和写者。\n  - `wait_lock`：保护等待队列的自旋锁。\n\n### 关键宏定义\n- **Owner 字段标志位**：\n  - `RWSEM_READER_OWNED`（bit 0）：表示当前由读者持有。\n  - `RWSEM_NONSPINNABLE`（bit 1）：禁止乐观自旋。\n- **Count 字段位布局**（64 位架构）：\n  - bit 0：`RWSEM_WRITER_LOCKED`（写者已加锁）\n  - bit 1：`RWSEM_FLAG_WAITERS`（存在等待者）\n  - bit 2：`RWSEM_FLAG_HANDOFF`（锁移交标志）\n  - bits 8–62：55 位读者计数\n  - bit 63：`RWSEM_FLAG_READFAIL`（读取失败标志，用于未来扩展）\n\n### 核心内联函数\n- `rwsem_set_owner()` / `rwsem_clear_owner()`：设置/清除写者所有者。\n- `rwsem_set_reader_owned()` / `rwsem_clear_reader_owned()`：标记/清除读者所有者（带调试支持）。\n- `is_rwsem_reader_owned()`：判断是否由读者持有。\n- `rwsem_set_nonspinnable()`：在读者持有时设置不可自旋标志。\n- `rwsem_test_oflags()`：测试 owner 字段中的标志位。\n\n## 3. 关键实现\n\n### 位域编码设计\n`count` 字段采用紧凑的位域编码，将写者锁状态、等待者存在标志、锁移交标志和读者计数集成在一个 `atomic_long_t` 中。这种设计使得 fast-path（快速路径）操作（如读者加锁）可通过单一原子加法完成，极大提升性能。\n\n### 乐观自旋（Optimistic Spinning）\n当写者尝试获取锁失败时，若满足条件（如锁由写者刚释放、无移交请求），会进入乐观自旋状态，避免立即进入睡眠。若自旋超时且锁仍为读者持有，则设置 `RWSEM_NONSPINNABLE` 标志，禁止后续写者自旋，防止 CPU 资源浪费。\n\n### 写者锁抢占（Writer Lock-Stealing）\n在特定条件下（如锁刚由写者释放、无等待者、无移交标志），新来的写者可直接抢占锁，无需排队，减少延迟。\n\n### 所有者追踪机制\n- **写者**：`owner` 字段直接存储 `task_struct*` 指针。\n- **读者**：`owner` 字段存储当前读者任务指针并置 `RWSEM_READER_OWNED` 位。由于性能考虑，仅记录**最后一个获取锁的读者**，而非所有读者。\n- 调试模式（`CONFIG_DEBUG_RWSEMS`）下，`rwsem_clear_reader_owned()` 确保只有真正的持有者才能清除其所有者记录。\n\n### 原子操作策略\n- **读者加锁**：使用 `atomic_long_fetch_add()` 原子增加读者计数。\n- **写者加锁**：使用 `atomic_long_cmpxchg()` 进行条件交换，确保互斥。\n\n### 锁移交（Handoff）机制\n当写者被唤醒时，若其位于等待队列头部，可设置 `RWSEM_FLAG_HANDOFF` 标志，确保该写者优先获得锁，避免“写者饥饿”。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/rwsem.h>`：定义 `rw_semaphore` 结构和公共 API。\n  - `<linux/atomic.h>`：提供原子操作原语。\n  - `<linux/sched/*.h>`：任务调度、唤醒队列、实时调度支持。\n  - `<trace/events/lock.h>`：锁事件追踪。\n- **配置依赖**：\n  - `CONFIG_PREEMPT_RT`：若启用，部分实现（如乐观自旋）被禁用。\n  - `CONFIG_DEBUG_RWSEMS`：启用所有者一致性检查和警告。\n- **内部依赖**：\n  - `lock_events.h`：统计锁事件（仅在非 `PREEMPT_RT` 下）。\n  - 内核调度器：用于任务阻塞/唤醒。\n\n## 5. 使用场景\n\n- **文件系统**：如 ext4、XFS 使用 rwsem 保护 inode 或目录结构，允许多读者并发访问元数据。\n- **内存管理**：`mm_struct` 的 `mmap_lock` 采用 rwsem，支持并发读（如页表遍历）与独占写（如内存映射修改）。\n- **模块加载**：内核模块的引用计数和符号表访问通过 rwsem 同步。\n- **RCU 替代场景**：在需要严格写者优先或不能使用 RCU 的上下文中，rwsem 提供强一致性保证。\n- **调试与死锁检测**：结合 `lockdep` 和 `DEBUG_RWSEMS`，用于检测读者/写者死锁或非法嵌套。",
      "similarity": 0.532318651676178,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/locking/rwsem.c",
          "start_line": 734,
          "end_line": 920,
          "content": [
            "static inline enum owner_state",
            "rwsem_owner_state(struct task_struct *owner, unsigned long flags)",
            "{",
            "\tif (flags & RWSEM_NONSPINNABLE)",
            "\t\treturn OWNER_NONSPINNABLE;",
            "",
            "\tif (flags & RWSEM_READER_OWNED)",
            "\t\treturn OWNER_READER;",
            "",
            "\treturn owner ? OWNER_WRITER : OWNER_NULL;",
            "}",
            "static noinline enum owner_state",
            "rwsem_spin_on_owner(struct rw_semaphore *sem)",
            "{",
            "\tstruct task_struct *new, *owner;",
            "\tunsigned long flags, new_flags;",
            "\tenum owner_state state;",
            "",
            "\tlockdep_assert_preemption_disabled();",
            "",
            "\towner = rwsem_owner_flags(sem, &flags);",
            "\tstate = rwsem_owner_state(owner, flags);",
            "\tif (state != OWNER_WRITER)",
            "\t\treturn state;",
            "",
            "\tfor (;;) {",
            "\t\t/*",
            "\t\t * When a waiting writer set the handoff flag, it may spin",
            "\t\t * on the owner as well. Once that writer acquires the lock,",
            "\t\t * we can spin on it. So we don't need to quit even when the",
            "\t\t * handoff bit is set.",
            "\t\t */",
            "\t\tnew = rwsem_owner_flags(sem, &new_flags);",
            "\t\tif ((new != owner) || (new_flags != flags)) {",
            "\t\t\tstate = rwsem_owner_state(new, new_flags);",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Ensure we emit the owner->on_cpu, dereference _after_",
            "\t\t * checking sem->owner still matches owner, if that fails,",
            "\t\t * owner might point to free()d memory, if it still matches,",
            "\t\t * our spinning context already disabled preemption which is",
            "\t\t * equal to RCU read-side crital section ensures the memory",
            "\t\t * stays valid.",
            "\t\t */",
            "\t\tbarrier();",
            "",
            "\t\tif (need_resched() || !owner_on_cpu(owner)) {",
            "\t\t\tstate = OWNER_NONSPINNABLE;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tcpu_relax();",
            "\t}",
            "",
            "\treturn state;",
            "}",
            "static inline u64 rwsem_rspin_threshold(struct rw_semaphore *sem)",
            "{",
            "\tlong count = atomic_long_read(&sem->count);",
            "\tint readers = count >> RWSEM_READER_SHIFT;",
            "\tu64 delta;",
            "",
            "\tif (readers > 30)",
            "\t\treaders = 30;",
            "\tdelta = (20 + readers) * NSEC_PER_USEC / 2;",
            "",
            "\treturn sched_clock() + delta;",
            "}",
            "static bool rwsem_optimistic_spin(struct rw_semaphore *sem)",
            "{",
            "\tbool taken = false;",
            "\tint prev_owner_state = OWNER_NULL;",
            "\tint loop = 0;",
            "\tu64 rspin_threshold = 0;",
            "",
            "\t/* sem->wait_lock should not be held when doing optimistic spinning */",
            "\tif (!osq_lock(&sem->osq))",
            "\t\tgoto done;",
            "",
            "\t/*",
            "\t * Optimistically spin on the owner field and attempt to acquire the",
            "\t * lock whenever the owner changes. Spinning will be stopped when:",
            "\t *  1) the owning writer isn't running; or",
            "\t *  2) readers own the lock and spinning time has exceeded limit.",
            "\t */",
            "\tfor (;;) {",
            "\t\tenum owner_state owner_state;",
            "",
            "\t\towner_state = rwsem_spin_on_owner(sem);",
            "\t\tif (!(owner_state & OWNER_SPINNABLE))",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * Try to acquire the lock",
            "\t\t */",
            "\t\ttaken = rwsem_try_write_lock_unqueued(sem);",
            "",
            "\t\tif (taken)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * Time-based reader-owned rwsem optimistic spinning",
            "\t\t */",
            "\t\tif (owner_state == OWNER_READER) {",
            "\t\t\t/*",
            "\t\t\t * Re-initialize rspin_threshold every time when",
            "\t\t\t * the owner state changes from non-reader to reader.",
            "\t\t\t * This allows a writer to steal the lock in between",
            "\t\t\t * 2 reader phases and have the threshold reset at",
            "\t\t\t * the beginning of the 2nd reader phase.",
            "\t\t\t */",
            "\t\t\tif (prev_owner_state != OWNER_READER) {",
            "\t\t\t\tif (rwsem_test_oflags(sem, RWSEM_NONSPINNABLE))",
            "\t\t\t\t\tbreak;",
            "\t\t\t\trspin_threshold = rwsem_rspin_threshold(sem);",
            "\t\t\t\tloop = 0;",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * Check time threshold once every 16 iterations to",
            "\t\t\t * avoid calling sched_clock() too frequently so",
            "\t\t\t * as to reduce the average latency between the times",
            "\t\t\t * when the lock becomes free and when the spinner",
            "\t\t\t * is ready to do a trylock.",
            "\t\t\t */",
            "\t\t\telse if (!(++loop & 0xf) && (sched_clock() > rspin_threshold)) {",
            "\t\t\t\trwsem_set_nonspinnable(sem);",
            "\t\t\t\tlockevent_inc(rwsem_opt_nospin);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * An RT task cannot do optimistic spinning if it cannot",
            "\t\t * be sure the lock holder is running or live-lock may",
            "\t\t * happen if the current task and the lock holder happen",
            "\t\t * to run in the same CPU. However, aborting optimistic",
            "\t\t * spinning while a NULL owner is detected may miss some",
            "\t\t * opportunity where spinning can continue without causing",
            "\t\t * problem.",
            "\t\t *",
            "\t\t * There are 2 possible cases where an RT task may be able",
            "\t\t * to continue spinning.",
            "\t\t *",
            "\t\t * 1) The lock owner is in the process of releasing the",
            "\t\t *    lock, sem->owner is cleared but the lock has not",
            "\t\t *    been released yet.",
            "\t\t * 2) The lock was free and owner cleared, but another",
            "\t\t *    task just comes in and acquire the lock before",
            "\t\t *    we try to get it. The new owner may be a spinnable",
            "\t\t *    writer.",
            "\t\t *",
            "\t\t * To take advantage of two scenarios listed above, the RT",
            "\t\t * task is made to retry one more time to see if it can",
            "\t\t * acquire the lock or continue spinning on the new owning",
            "\t\t * writer. Of course, if the time lag is long enough or the",
            "\t\t * new owner is not a writer or spinnable, the RT task will",
            "\t\t * quit spinning.",
            "\t\t *",
            "\t\t * If the owner is a writer, the need_resched() check is",
            "\t\t * done inside rwsem_spin_on_owner(). If the owner is not",
            "\t\t * a writer, need_resched() check needs to be done here.",
            "\t\t */",
            "\t\tif (owner_state != OWNER_WRITER) {",
            "\t\t\tif (need_resched())",
            "\t\t\t\tbreak;",
            "\t\t\tif (rt_or_dl_task(current) &&",
            "\t\t\t   (prev_owner_state != OWNER_WRITER))",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t\tprev_owner_state = owner_state;",
            "",
            "\t\t/*",
            "\t\t * The cpu_relax() call is a compiler barrier which forces",
            "\t\t * everything in this loop to be re-loaded. We don't need",
            "\t\t * memory barriers as we'll eventually observe the right",
            "\t\t * values at the cost of a few extra spins.",
            "\t\t */",
            "\t\tcpu_relax();",
            "\t}",
            "\tosq_unlock(&sem->osq);",
            "done:",
            "\tlockevent_cond_inc(rwsem_opt_fail, !taken);",
            "\treturn taken;",
            "}"
          ],
          "function_name": "rwsem_owner_state, rwsem_spin_on_owner, rwsem_rspin_threshold, rwsem_optimistic_spin",
          "description": "实现乐观自旋机制，通过检测所有者状态变化和时间阈值限制，优化锁竞争场景下的吞吐量",
          "similarity": 0.6201906204223633
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/locking/rwsem.c",
          "start_line": 576,
          "end_line": 698,
          "content": [
            "static inline void",
            "rwsem_del_wake_waiter(struct rw_semaphore *sem, struct rwsem_waiter *waiter,",
            "\t\t      struct wake_q_head *wake_q)",
            "\t\t      __releases(&sem->wait_lock)",
            "{",
            "\tbool first = rwsem_first_waiter(sem) == waiter;",
            "",
            "\twake_q_init(wake_q);",
            "",
            "\t/*",
            "\t * If the wait_list isn't empty and the waiter to be deleted is",
            "\t * the first waiter, we wake up the remaining waiters as they may",
            "\t * be eligible to acquire or spin on the lock.",
            "\t */",
            "\tif (rwsem_del_waiter(sem, waiter) && first)",
            "\t\trwsem_mark_wake(sem, RWSEM_WAKE_ANY, wake_q);",
            "\traw_spin_unlock_irq(&sem->wait_lock);",
            "\tif (!wake_q_empty(wake_q))",
            "\t\twake_up_q(wake_q);",
            "}",
            "static inline bool rwsem_try_write_lock(struct rw_semaphore *sem,",
            "\t\t\t\t\tstruct rwsem_waiter *waiter)",
            "{",
            "\tstruct rwsem_waiter *first = rwsem_first_waiter(sem);",
            "\tlong count, new;",
            "",
            "\tlockdep_assert_held(&sem->wait_lock);",
            "",
            "\tcount = atomic_long_read(&sem->count);",
            "\tdo {",
            "\t\tbool has_handoff = !!(count & RWSEM_FLAG_HANDOFF);",
            "",
            "\t\tif (has_handoff) {",
            "\t\t\t/*",
            "\t\t\t * Honor handoff bit and yield only when the first",
            "\t\t\t * waiter is the one that set it. Otherwisee, we",
            "\t\t\t * still try to acquire the rwsem.",
            "\t\t\t */",
            "\t\t\tif (first->handoff_set && (waiter != first))",
            "\t\t\t\treturn false;",
            "\t\t}",
            "",
            "\t\tnew = count;",
            "",
            "\t\tif (count & RWSEM_LOCK_MASK) {",
            "\t\t\t/*",
            "\t\t\t * A waiter (first or not) can set the handoff bit",
            "\t\t\t * if it is an RT task or wait in the wait queue",
            "\t\t\t * for too long.",
            "\t\t\t */",
            "\t\t\tif (has_handoff || (!rt_or_dl_task(waiter->task) &&",
            "\t\t\t\t\t    !time_after(jiffies, waiter->timeout)))",
            "\t\t\t\treturn false;",
            "",
            "\t\t\tnew |= RWSEM_FLAG_HANDOFF;",
            "\t\t} else {",
            "\t\t\tnew |= RWSEM_WRITER_LOCKED;",
            "\t\t\tnew &= ~RWSEM_FLAG_HANDOFF;",
            "",
            "\t\t\tif (list_is_singular(&sem->wait_list))",
            "\t\t\t\tnew &= ~RWSEM_FLAG_WAITERS;",
            "\t\t}",
            "\t} while (!atomic_long_try_cmpxchg_acquire(&sem->count, &count, new));",
            "",
            "\t/*",
            "\t * We have either acquired the lock with handoff bit cleared or set",
            "\t * the handoff bit. Only the first waiter can have its handoff_set",
            "\t * set here to enable optimistic spinning in slowpath loop.",
            "\t */",
            "\tif (new & RWSEM_FLAG_HANDOFF) {",
            "\t\tfirst->handoff_set = true;",
            "\t\tlockevent_inc(rwsem_wlock_handoff);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/*",
            "\t * Have rwsem_try_write_lock() fully imply rwsem_del_waiter() on",
            "\t * success.",
            "\t */",
            "\tlist_del(&waiter->list);",
            "\trwsem_set_owner(sem);",
            "\treturn true;",
            "}",
            "static inline bool rwsem_try_write_lock_unqueued(struct rw_semaphore *sem)",
            "{",
            "\tlong count = atomic_long_read(&sem->count);",
            "",
            "\twhile (!(count & (RWSEM_LOCK_MASK|RWSEM_FLAG_HANDOFF))) {",
            "\t\tif (atomic_long_try_cmpxchg_acquire(&sem->count, &count,",
            "\t\t\t\t\tcount | RWSEM_WRITER_LOCKED)) {",
            "\t\t\trwsem_set_owner(sem);",
            "\t\t\tlockevent_inc(rwsem_opt_lock);",
            "\t\t\treturn true;",
            "\t\t}",
            "\t}",
            "\treturn false;",
            "}",
            "static inline bool rwsem_can_spin_on_owner(struct rw_semaphore *sem)",
            "{",
            "\tstruct task_struct *owner;",
            "\tunsigned long flags;",
            "\tbool ret = true;",
            "",
            "\tif (need_resched()) {",
            "\t\tlockevent_inc(rwsem_opt_fail);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/*",
            "\t * Disable preemption is equal to the RCU read-side crital section,",
            "\t * thus the task_strcut structure won't go away.",
            "\t */",
            "\towner = rwsem_owner_flags(sem, &flags);",
            "\t/*",
            "\t * Don't check the read-owner as the entry may be stale.",
            "\t */",
            "\tif ((flags & RWSEM_NONSPINNABLE) ||",
            "\t    (owner && !(flags & RWSEM_READER_OWNED) && !owner_on_cpu(owner)))",
            "\t\tret = false;",
            "",
            "\tlockevent_cond_inc(rwsem_opt_fail, !ret);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "rwsem_del_wake_waiter, rwsem_try_write_lock, rwsem_try_write_lock_unqueued, rwsem_can_spin_on_owner",
          "description": "处理写锁的尝试获取逻辑，包含手柄标志判定、等待队列操作及自旋兼容性检查",
          "similarity": 0.5726088285446167
        },
        {
          "chunk_id": 8,
          "file_path": "kernel/locking/rwsem.c",
          "start_line": 1639,
          "end_line": 1703,
          "content": [
            "void downgrade_write(struct rw_semaphore *sem)",
            "{",
            "\tlock_downgrade(&sem->dep_map, _RET_IP_);",
            "\t__downgrade_write(sem);",
            "}",
            "void down_read_nested(struct rw_semaphore *sem, int subclass)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire_read(&sem->dep_map, subclass, 0, _RET_IP_);",
            "\tLOCK_CONTENDED(sem, __down_read_trylock, __down_read);",
            "}",
            "int down_read_killable_nested(struct rw_semaphore *sem, int subclass)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire_read(&sem->dep_map, subclass, 0, _RET_IP_);",
            "",
            "\tif (LOCK_CONTENDED_RETURN(sem, __down_read_trylock, __down_read_killable)) {",
            "\t\trwsem_release(&sem->dep_map, _RET_IP_);",
            "\t\treturn -EINTR;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "void _down_write_nest_lock(struct rw_semaphore *sem, struct lockdep_map *nest)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire_nest(&sem->dep_map, 0, 0, nest, _RET_IP_);",
            "\tLOCK_CONTENDED(sem, __down_write_trylock, __down_write);",
            "}",
            "void down_read_non_owner(struct rw_semaphore *sem)",
            "{",
            "\tmight_sleep();",
            "\t__down_read(sem);",
            "\t/*",
            "\t * The owner value for a reader-owned lock is mostly for debugging",
            "\t * purpose only and is not critical to the correct functioning of",
            "\t * rwsem. So it is perfectly fine to set it in a preempt-enabled",
            "\t * context here.",
            "\t */",
            "\t__rwsem_set_reader_owned(sem, NULL);",
            "}",
            "void down_write_nested(struct rw_semaphore *sem, int subclass)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire(&sem->dep_map, subclass, 0, _RET_IP_);",
            "\tLOCK_CONTENDED(sem, __down_write_trylock, __down_write);",
            "}",
            "int __sched down_write_killable_nested(struct rw_semaphore *sem, int subclass)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire(&sem->dep_map, subclass, 0, _RET_IP_);",
            "",
            "\tif (LOCK_CONTENDED_RETURN(sem, __down_write_trylock,",
            "\t\t\t\t  __down_write_killable)) {",
            "\t\trwsem_release(&sem->dep_map, _RET_IP_);",
            "\t\treturn -EINTR;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "void up_read_non_owner(struct rw_semaphore *sem)",
            "{",
            "\tDEBUG_RWSEMS_WARN_ON(!is_rwsem_reader_owned(sem), sem);",
            "\t__up_read(sem);",
            "}"
          ],
          "function_name": "downgrade_write, down_read_nested, down_read_killable_nested, _down_write_nest_lock, down_read_non_owner, down_write_nested, down_write_killable_nested, up_read_non_owner",
          "description": "实现锁的降级操作与嵌套锁管理功能，downgrade_write将写锁转换为读锁并调整计数器，nested版本接口支持锁子类嵌套，down_read_non_owner/up_read_non_owner处理非所有者场景的读操作，通过__rwsem_set_reader_owned设置调试用的所有者标识。",
          "similarity": 0.4863916039466858
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/locking/rwsem.c",
          "start_line": 1483,
          "end_line": 1585,
          "content": [
            "static inline int __sched __down_write_killable(struct rw_semaphore *sem)",
            "{",
            "\treturn rwbase_write_lock(&sem->rwbase, TASK_KILLABLE);",
            "}",
            "static inline int __down_write_trylock(struct rw_semaphore *sem)",
            "{",
            "\treturn rwbase_write_trylock(&sem->rwbase);",
            "}",
            "static inline void __up_write(struct rw_semaphore *sem)",
            "{",
            "\trwbase_write_unlock(&sem->rwbase);",
            "}",
            "static inline void __downgrade_write(struct rw_semaphore *sem)",
            "{",
            "\trwbase_write_downgrade(&sem->rwbase);",
            "}",
            "static inline void __rwsem_set_reader_owned(struct rw_semaphore *sem,",
            "\t\t\t\t\t    struct task_struct *owner)",
            "{",
            "}",
            "static inline bool is_rwsem_reader_owned(struct rw_semaphore *sem)",
            "{",
            "\tint count = atomic_read(&sem->rwbase.readers);",
            "",
            "\treturn count < 0 && count != READER_BIAS;",
            "}",
            "void __sched down_read(struct rw_semaphore *sem)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire_read(&sem->dep_map, 0, 0, _RET_IP_);",
            "",
            "\tLOCK_CONTENDED(sem, __down_read_trylock, __down_read);",
            "}",
            "int __sched down_read_interruptible(struct rw_semaphore *sem)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire_read(&sem->dep_map, 0, 0, _RET_IP_);",
            "",
            "\tif (LOCK_CONTENDED_RETURN(sem, __down_read_trylock, __down_read_interruptible)) {",
            "\t\trwsem_release(&sem->dep_map, _RET_IP_);",
            "\t\treturn -EINTR;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __sched down_read_killable(struct rw_semaphore *sem)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire_read(&sem->dep_map, 0, 0, _RET_IP_);",
            "",
            "\tif (LOCK_CONTENDED_RETURN(sem, __down_read_trylock, __down_read_killable)) {",
            "\t\trwsem_release(&sem->dep_map, _RET_IP_);",
            "\t\treturn -EINTR;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int down_read_trylock(struct rw_semaphore *sem)",
            "{",
            "\tint ret = __down_read_trylock(sem);",
            "",
            "\tif (ret == 1)",
            "\t\trwsem_acquire_read(&sem->dep_map, 0, 1, _RET_IP_);",
            "\treturn ret;",
            "}",
            "void __sched down_write(struct rw_semaphore *sem)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire(&sem->dep_map, 0, 0, _RET_IP_);",
            "\tLOCK_CONTENDED(sem, __down_write_trylock, __down_write);",
            "}",
            "int __sched down_write_killable(struct rw_semaphore *sem)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire(&sem->dep_map, 0, 0, _RET_IP_);",
            "",
            "\tif (LOCK_CONTENDED_RETURN(sem, __down_write_trylock,",
            "\t\t\t\t  __down_write_killable)) {",
            "\t\trwsem_release(&sem->dep_map, _RET_IP_);",
            "\t\treturn -EINTR;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int down_write_trylock(struct rw_semaphore *sem)",
            "{",
            "\tint ret = __down_write_trylock(sem);",
            "",
            "\tif (ret == 1)",
            "\t\trwsem_acquire(&sem->dep_map, 0, 1, _RET_IP_);",
            "",
            "\treturn ret;",
            "}",
            "void up_read(struct rw_semaphore *sem)",
            "{",
            "\trwsem_release(&sem->dep_map, _RET_IP_);",
            "\t__up_read(sem);",
            "}",
            "void up_write(struct rw_semaphore *sem)",
            "{",
            "\trwsem_release(&sem->dep_map, _RET_IP_);",
            "\t__up_write(sem);",
            "}"
          ],
          "function_name": "__down_write_killable, __down_write_trylock, __up_write, __downgrade_write, __rwsem_set_reader_owned, is_rwsem_reader_owned, down_read, down_read_interruptible, down_read_killable, down_read_trylock, down_write, down_write_killable, down_write_trylock, up_read, up_write",
          "description": "提供标准的读写信号量API接口，包含down_read/down_write等公共函数，通过LOCK_CONTENDED宏协调自旋尝试与阻塞等待，整合锁的获取/释放追踪机制（如rwsem_acquire/read_unlock），并处理中断可睡眠的锁获取场景。",
          "similarity": 0.4667501449584961
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/rwsem.c",
          "start_line": 140,
          "end_line": 249,
          "content": [
            "static inline void rwsem_set_owner(struct rw_semaphore *sem)",
            "{",
            "\tlockdep_assert_preemption_disabled();",
            "\tatomic_long_set(&sem->owner, (long)current);",
            "}",
            "static inline void rwsem_clear_owner(struct rw_semaphore *sem)",
            "{",
            "\tlockdep_assert_preemption_disabled();",
            "\tatomic_long_set(&sem->owner, 0);",
            "}",
            "static inline bool rwsem_test_oflags(struct rw_semaphore *sem, long flags)",
            "{",
            "\treturn atomic_long_read(&sem->owner) & flags;",
            "}",
            "static inline void __rwsem_set_reader_owned(struct rw_semaphore *sem,",
            "\t\t\t\t\t    struct task_struct *owner)",
            "{",
            "\tunsigned long val = (unsigned long)owner | RWSEM_READER_OWNED |",
            "\t\t(atomic_long_read(&sem->owner) & RWSEM_NONSPINNABLE);",
            "",
            "\tatomic_long_set(&sem->owner, val);",
            "}",
            "static inline void rwsem_set_reader_owned(struct rw_semaphore *sem)",
            "{",
            "\t__rwsem_set_reader_owned(sem, current);",
            "}",
            "static inline bool is_rwsem_reader_owned(struct rw_semaphore *sem)",
            "{",
            "#ifdef CONFIG_DEBUG_RWSEMS",
            "\t/*",
            "\t * Check the count to see if it is write-locked.",
            "\t */",
            "\tlong count = atomic_long_read(&sem->count);",
            "",
            "\tif (count & RWSEM_WRITER_MASK)",
            "\t\treturn false;",
            "#endif",
            "\treturn rwsem_test_oflags(sem, RWSEM_READER_OWNED);",
            "}",
            "static inline void rwsem_clear_reader_owned(struct rw_semaphore *sem)",
            "{",
            "\tunsigned long val = atomic_long_read(&sem->owner);",
            "",
            "\twhile ((val & ~RWSEM_OWNER_FLAGS_MASK) == (unsigned long)current) {",
            "\t\tif (atomic_long_try_cmpxchg(&sem->owner, &val,",
            "\t\t\t\t\t    val & RWSEM_OWNER_FLAGS_MASK))",
            "\t\t\treturn;",
            "\t}",
            "}",
            "static inline void rwsem_clear_reader_owned(struct rw_semaphore *sem)",
            "{",
            "}",
            "static inline void rwsem_set_nonspinnable(struct rw_semaphore *sem)",
            "{",
            "\tunsigned long owner = atomic_long_read(&sem->owner);",
            "",
            "\tdo {",
            "\t\tif (!(owner & RWSEM_READER_OWNED))",
            "\t\t\tbreak;",
            "\t\tif (owner & RWSEM_NONSPINNABLE)",
            "\t\t\tbreak;",
            "\t} while (!atomic_long_try_cmpxchg(&sem->owner, &owner,",
            "\t\t\t\t\t  owner | RWSEM_NONSPINNABLE));",
            "}",
            "static inline bool rwsem_read_trylock(struct rw_semaphore *sem, long *cntp)",
            "{",
            "\t*cntp = atomic_long_add_return_acquire(RWSEM_READER_BIAS, &sem->count);",
            "",
            "\tif (WARN_ON_ONCE(*cntp < 0))",
            "\t\trwsem_set_nonspinnable(sem);",
            "",
            "\tif (!(*cntp & RWSEM_READ_FAILED_MASK)) {",
            "\t\trwsem_set_reader_owned(sem);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}",
            "static inline bool rwsem_write_trylock(struct rw_semaphore *sem)",
            "{",
            "\tlong tmp = RWSEM_UNLOCKED_VALUE;",
            "",
            "\tif (atomic_long_try_cmpxchg_acquire(&sem->count, &tmp, RWSEM_WRITER_LOCKED)) {",
            "\t\trwsem_set_owner(sem);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}",
            "void __init_rwsem(struct rw_semaphore *sem, const char *name,",
            "\t\t  struct lock_class_key *key)",
            "{",
            "#ifdef CONFIG_DEBUG_LOCK_ALLOC",
            "\t/*",
            "\t * Make sure we are not reinitializing a held semaphore:",
            "\t */",
            "\tdebug_check_no_locks_freed((void *)sem, sizeof(*sem));",
            "\tlockdep_init_map_wait(&sem->dep_map, name, key, 0, LD_WAIT_SLEEP);",
            "#endif",
            "#ifdef CONFIG_DEBUG_RWSEMS",
            "\tsem->magic = sem;",
            "#endif",
            "\tatomic_long_set(&sem->count, RWSEM_UNLOCKED_VALUE);",
            "\traw_spin_lock_init(&sem->wait_lock);",
            "\tINIT_LIST_HEAD(&sem->wait_list);",
            "\tatomic_long_set(&sem->owner, 0L);",
            "#ifdef CONFIG_RWSEM_SPIN_ON_OWNER",
            "\tosq_lock_init(&sem->osq);",
            "#endif",
            "}"
          ],
          "function_name": "rwsem_set_owner, rwsem_clear_owner, rwsem_test_oflags, __rwsem_set_reader_owned, rwsem_set_reader_owned, is_rwsem_reader_owned, rwsem_clear_reader_owned, rwsem_clear_reader_owned, rwsem_set_nonspinnable, rwsem_read_trylock, rwsem_write_trylock, __init_rwsem",
          "description": "提供读写信号量的核心操作函数，包括设置/清除所有者、尝试加锁、初始化信号量结构体及其状态管理",
          "similarity": 0.45559942722320557
        }
      ]
    },
    {
      "source_file": "kernel/locking/osq_lock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:43:41\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\osq_lock.c`\n\n---\n\n# `locking/osq_lock.c` 技术文档\n\n## 1. 文件概述\n\n`osq_lock.c` 实现了一种专为**乐观自旋（Optimistic Spinning）**设计的轻量级排队自旋锁机制，称为 **OSQ（Optimistic Spin Queue）锁**。该机制主要用于支持如互斥锁（mutex）、读写信号量（rwsem）等**可睡眠锁**在争用时进行乐观自旋，以避免不必要的上下文切换和调度开销。OSQ 锁基于 MCS（Mellor-Crummey and Scott）锁的思想，但针对 Linux 内核的调度和抢占模型进行了优化，利用每个 CPU 的静态 per-CPU 节点结构，确保在禁用抢占的自旋上下文中安全使用。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct optimistic_spin_node`：每个 CPU 对应一个静态节点，包含：\n  - `cpu`：编码后的 CPU 编号（实际值 = CPU ID + 1）\n  - `locked`：布尔标志，表示是否已获得锁\n  - `next`：指向队列中下一个节点的指针\n  - `prev`：指向前一个节点的指针\n- `struct optimistic_spin_queue`：OSQ 锁结构体，仅包含一个原子变量 `tail`，用于指向队列尾部（编码后的 CPU 编号），`OSQ_UNLOCKED_VAL`（值为 0）表示无锁。\n\n### 主要函数\n- `bool osq_lock(struct optimistic_spin_queue *lock)`  \n  尝试获取 OSQ 锁。若成功获得锁或决定放弃自旋（如需要调度或前驱被抢占），返回 `true`；若成功排队但未获得锁且需继续等待，则返回 `false`（实际逻辑中，失败路径最终也返回 `false` 表示未获得锁）。\n  \n- `void osq_unlock(struct optimistic_spin_queue *lock)`  \n  释放 OSQ 锁，唤醒队列中的下一个等待者（若存在）。\n\n- `static inline struct optimistic_spin_node *osq_wait_next(...)`  \n  辅助函数，用于在解锁或取消排队时安全地获取下一个节点，并处理队列尾部的原子更新。\n\n- `encode_cpu()` / `decode_cpu()` / `node_cpu()`  \n  用于在 CPU 编号与 per-CPU 节点指针之间进行编码/解码转换，其中 CPU 编号 0 被编码为 1，以 0 表示“无 CPU”（即锁空闲）。\n\n## 3. 关键实现\n\n### Per-CPU 静态节点设计\n- 每个 CPU 拥有一个静态的 `osq_node`（通过 `DEFINE_PER_CPU_SHARED_ALIGNED` 定义），避免动态分配开销。\n- 由于 OSQ 仅在**禁用抢占**的上下文中使用（如 mutex 的乐观自旋阶段），且**不可在中断上下文调用**，因此 per-CPU 节点的生命周期安全。\n\n### 锁获取流程 (`osq_lock`)\n1. **初始化本地节点**：设置 `locked=0`、`next=NULL`，并确保 `cpu` 字段为当前 CPU 编码值。\n2. **原子交换尾指针**：通过 `atomic_xchg(&lock->tail, curr)` 尝试入队。若原值为 `OSQ_UNLOCKED_VAL`，直接获得锁。\n3. **链接到前驱**：若已有前驱（`prev`），通过 `smp_wmb()` 确保内存顺序后，设置 `prev->next = node`。\n4. **自旋等待**：使用 `smp_cond_load_relaxed()` 等待 `node->locked` 变为 1，或满足退出条件（`need_resched()` 或前驱 CPU 被抢占 `vcpu_is_preempted()`）。\n5. **取消排队（Unqueue）**：若需退出自旋：\n   - **Step A**：尝试将 `prev->next` 置为 `NULL`，断开链接。\n   - **Step B**：调用 `osq_wait_next()` 确定下一个节点，并可能将锁尾指针回退。\n   - **Step C**：若存在 `next`，将其与 `prev` 直接链接，完成队列修复。\n\n### 锁释放流程 (`osq_unlock`)\n1. **快速路径**：若当前 CPU 是唯一持有者（`tail == curr`），直接将 `tail` 设为 `OSQ_UNLOCKED_VAL`。\n2. **慢速路径**：\n   - 若本地节点的 `next` 非空，直接设置 `next->locked = 1` 唤醒后继。\n   - 否则调用 `osq_wait_next()` 获取下一个节点（处理并发取消排队的情况），再唤醒。\n\n### 内存屏障与原子操作\n- 使用 `atomic_xchg`、`atomic_cmpxchg_acquire/release` 确保对 `lock->tail` 的操作具有适当的内存序。\n- `smp_wmb()` 保证在设置 `prev->next` 前，本地节点的初始化对其他 CPU 可见。\n- `WRITE_ONCE`/`READ_ONCE` 防止编译器优化破坏并发访问语义。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/percpu.h>`：提供 per-CPU 变量支持（`this_cpu_ptr`, `per_cpu_ptr`）。\n  - `<linux/sched.h>`：提供调度相关函数（`need_resched()`）和虚拟 CPU 抢占检测（`vcpu_is_preempted()`）。\n  - `<linux/osq_lock.h>`：定义 `struct optimistic_spin_queue`、`struct optimistic_spin_node` 及 `OSQ_UNLOCKED_VAL`。\n- **架构依赖**：依赖底层架构的原子操作（`atomic_*`）、内存屏障（`smp_wmb`, `smp_load_acquire`）和 CPU ID 获取（`smp_processor_id()`）。\n- **调度器集成**：与内核调度器紧密协作，通过 `need_resched()` 和 `vcpu_is_preempted()` 决定是否继续自旋。\n\n## 5. 使用场景\n\nOSQ 锁主要用于**可睡眠锁的乐观自旋优化**，典型场景包括：\n- **Mutex（互斥锁）**：在 `mutex_spin_on_owner()` 中，若锁持有者正在运行，当前 CPU 会尝试 OSQ 自旋而非立即睡眠。\n- **Rwsem（读写信号量）**：在写者争用时，若满足条件，会使用 OSQ 进行乐观自旋。\n- **其他睡眠锁**：任何希望在锁争用时避免立即进入睡眠、以降低延迟的同步原语。\n\n其核心价值在于：当锁持有者很可能在**另一个 CPU 上运行且未被抢占**时，通过短暂自旋可避免昂贵的上下文切换，提升性能；同时通过 `vcpu_is_preempted()` 检测虚拟化环境中的抢占，避免在持有者已让出 CPU 时无效自旋。",
      "similarity": 0.5181293487548828,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 20,
          "end_line": 149,
          "content": [
            "static inline int encode_cpu(int cpu_nr)",
            "{",
            "\treturn cpu_nr + 1;",
            "}",
            "static inline int node_cpu(struct optimistic_spin_node *node)",
            "{",
            "\treturn node->cpu - 1;",
            "}",
            "bool osq_lock(struct optimistic_spin_queue *lock)",
            "{",
            "\tstruct optimistic_spin_node *node = this_cpu_ptr(&osq_node);",
            "\tstruct optimistic_spin_node *prev, *next;",
            "\tint curr = encode_cpu(smp_processor_id());",
            "\tint old;",
            "",
            "\tnode->locked = 0;",
            "\tnode->next = NULL;",
            "\t/*",
            "\t * After this cpu member is initialized for the first time, it",
            "\t * would no longer change in fact. That could avoid cache misses",
            "\t * when spin and access the cpu member by other CPUs.",
            "\t */",
            "\tif (node->cpu != curr)",
            "\t\tnode->cpu = curr;",
            "",
            "\t/*",
            "\t * We need both ACQUIRE (pairs with corresponding RELEASE in",
            "\t * unlock() uncontended, or fastpath) and RELEASE (to publish",
            "\t * the node fields we just initialised) semantics when updating",
            "\t * the lock tail.",
            "\t */",
            "\told = atomic_xchg(&lock->tail, curr);",
            "\tif (old == OSQ_UNLOCKED_VAL)",
            "\t\treturn true;",
            "",
            "\tprev = decode_cpu(old);",
            "\tnode->prev = prev;",
            "",
            "\t/*",
            "\t * osq_lock()\t\t\tunqueue",
            "\t *",
            "\t * node->prev = prev\t\tosq_wait_next()",
            "\t * WMB\t\t\t\tMB",
            "\t * prev->next = node\t\tnext->prev = prev // unqueue-C",
            "\t *",
            "\t * Here 'node->prev' and 'next->prev' are the same variable and we need",
            "\t * to ensure these stores happen in-order to avoid corrupting the list.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\tWRITE_ONCE(prev->next, node);",
            "",
            "\t/*",
            "\t * Normally @prev is untouchable after the above store; because at that",
            "\t * moment unlock can proceed and wipe the node element from stack.",
            "\t *",
            "\t * However, since our nodes are static per-cpu storage, we're",
            "\t * guaranteed their existence -- this allows us to apply",
            "\t * cmpxchg in an attempt to undo our queueing.",
            "\t */",
            "",
            "\t/*",
            "\t * Wait to acquire the lock or cancellation. Note that need_resched()",
            "\t * will come with an IPI, which will wake smp_cond_load_relaxed() if it",
            "\t * is implemented with a monitor-wait. vcpu_is_preempted() relies on",
            "\t * polling, be careful.",
            "\t */",
            "\tif (smp_cond_load_relaxed(&node->locked, VAL || need_resched() ||",
            "\t\t\t\t  vcpu_is_preempted(node_cpu(node->prev))))",
            "\t\treturn true;",
            "",
            "\t/* unqueue */",
            "\t/*",
            "\t * Step - A  -- stabilize @prev",
            "\t *",
            "\t * Undo our @prev->next assignment; this will make @prev's",
            "\t * unlock()/unqueue() wait for a next pointer since @lock points to us",
            "\t * (or later).",
            "\t */",
            "",
            "\tfor (;;) {",
            "\t\t/*",
            "\t\t * cpu_relax() below implies a compiler barrier which would",
            "\t\t * prevent this comparison being optimized away.",
            "\t\t */",
            "\t\tif (data_race(prev->next) == node &&",
            "\t\t    cmpxchg(&prev->next, node, NULL) == node)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * We can only fail the cmpxchg() racing against an unlock(),",
            "\t\t * in which case we should observe @node->locked becoming",
            "\t\t * true.",
            "\t\t */",
            "\t\tif (smp_load_acquire(&node->locked))",
            "\t\t\treturn true;",
            "",
            "\t\tcpu_relax();",
            "",
            "\t\t/*",
            "\t\t * Or we race against a concurrent unqueue()'s step-B, in which",
            "\t\t * case its step-C will write us a new @node->prev pointer.",
            "\t\t */",
            "\t\tprev = READ_ONCE(node->prev);",
            "\t}",
            "",
            "\t/*",
            "\t * Step - B -- stabilize @next",
            "\t *",
            "\t * Similar to unlock(), wait for @node->next or move @lock from @node",
            "\t * back to @prev.",
            "\t */",
            "",
            "\tnext = osq_wait_next(lock, node, prev);",
            "\tif (!next)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Step - C -- unlink",
            "\t *",
            "\t * @prev is stable because its still waiting for a new @prev->next",
            "\t * pointer, @next is stable because our @node->next pointer is NULL and",
            "\t * it will wait in Step-A.",
            "\t */",
            "",
            "\tWRITE_ONCE(next->prev, prev);",
            "\tWRITE_ONCE(prev->next, next);",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "encode_cpu, node_cpu, osq_lock",
          "description": "实现osq_lock函数，负责获取乐观自旋锁。通过原子操作将当前节点插入队列，利用内存屏障保证顺序一致性，并通过循环等待条件满足或被唤醒，最终完成锁的获取过程。",
          "similarity": 0.5339044332504272
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 1,
          "end_line": 19,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/percpu.h>",
            "#include <linux/sched.h>",
            "#include <linux/osq_lock.h>",
            "",
            "/*",
            " * An MCS like lock especially tailored for optimistic spinning for sleeping",
            " * lock implementations (mutex, rwsem, etc).",
            " *",
            " * Using a single mcs node per CPU is safe because sleeping locks should not be",
            " * called from interrupt context and we have preemption disabled while",
            " * spinning.",
            " */",
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);",
            "",
            "/*",
            " * We use the value 0 to represent \"no CPU\", thus the encoded value",
            " * will be the CPU number incremented by 1.",
            " */"
          ],
          "function_name": null,
          "description": "定义全局的per-CPU乐观自旋节点osq_node，用于支持多CPU环境下乐观自旋锁的实现。通过encode_cpu和node_cpu函数处理CPU编号转换，为后续锁操作提供基础设施。",
          "similarity": 0.4661407470703125
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 213,
          "end_line": 238,
          "content": [
            "void osq_unlock(struct optimistic_spin_queue *lock)",
            "{",
            "\tstruct optimistic_spin_node *node, *next;",
            "\tint curr = encode_cpu(smp_processor_id());",
            "",
            "\t/*",
            "\t * Fast path for the uncontended case.",
            "\t */",
            "\tif (likely(atomic_cmpxchg_release(&lock->tail, curr,",
            "\t\t\t\t\t  OSQ_UNLOCKED_VAL) == curr))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Second most likely case.",
            "\t */",
            "\tnode = this_cpu_ptr(&osq_node);",
            "\tnext = xchg(&node->next, NULL);",
            "\tif (next) {",
            "\t\tWRITE_ONCE(next->locked, 1);",
            "\t\treturn;",
            "\t}",
            "",
            "\tnext = osq_wait_next(lock, node, NULL);",
            "\tif (next)",
            "\t\tWRITE_ONCE(next->locked, 1);",
            "}"
          ],
          "function_name": "osq_unlock",
          "description": "实现osq_unlock函数，处理锁的释放。通过原子比较交换操作快速处理无竞争情况，否则查找并唤醒下一个等待节点，确保锁状态的正确性与线程安全。",
          "similarity": 0.45516011118888855
        }
      ]
    },
    {
      "source_file": "kernel/locking/rwbase_rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:50:50\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\rwbase_rt.c`\n\n---\n\n# `locking/rwbase_rt.c` 技术文档\n\n## 1. 文件概述\n\n`rwbase_rt.c` 是 Linux 内核实时（RT）补丁中用于实现 **实时读者-写者同步原语**（包括 `rw_semaphore` 和 `rwlock`）的通用底层代码。该文件为实时调度环境（如 `PREEMPT_RT`）提供了一套基于 `rtmutex` 的读写锁实现，以解决传统读写锁在实时系统中可能导致的优先级反转和不可预测延迟问题。\n\n该实现通过将写者操作与 `rtmutex` 绑定，利用 `rtmutex` 的优先级继承（PI）或截止时间（DL）调度机制，确保写者不会被低优先级读者无限阻塞，同时在多数情况下允许读者通过无锁快速路径（fast path）高效执行。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct rwbase_rt`：读写同步原语的通用底层结构，包含：\n  - `atomic_t readers`：读者计数器，使用偏置（bias）机制区分读/写状态\n  - `struct rt_mutex_base rtmutex`：底层实时互斥锁，用于串行化写者和阻塞读者\n\n### 关键常量\n- `READER_BIAS`：正偏置值（通常为 `0x7fffffff`），表示允许读者使用快速路径\n- `WRITER_BIAS`：负偏置值（通常为 `-0x80000000`），表示写者已持有锁\n\n### 主要函数\n\n| 函数 | 功能 |\n|------|------|\n| `rwbase_read_trylock()` | 尝试快速获取读锁（仅当 `READER_BIAS` 存在时） |\n| `__rwbase_read_lock()` / `rwbase_read_lock()` | 获取读锁（慢路径 + 快路径组合） |\n| `__rwbase_read_unlock()` / `rwbase_read_unlock()` | 释放读锁，必要时唤醒等待的写者 |\n| `__rwbase_write_unlock()` / `rwbase_write_unlock()` | 释放写锁，恢复 `READER_BIAS` 并释放 `rtmutex` |\n| `rwbase_write_downgrade()` | 将写锁降级为读锁 |\n| `__rwbase_write_trylock()` | 在持有 `wait_lock` 下尝试获取写锁 |\n| `rwbase_write_lock()` | 获取写锁（完整慢路径，含阻塞等待） |\n| `rwbase_write_trylock()` | 尝试非阻塞获取写锁（代码片段未完整） |\n\n## 3. 关键实现\n\n### 3.1 读者-写者状态管理\n- 使用 `atomic_t readers` 字段统一管理状态：\n  - **初始状态**：`readers = READER_BIAS`（正值），允许读者走快速路径\n  - **写者加锁时**：先获取 `rtmutex`，然后 `atomic_sub(READER_BIAS, &readers)`，使值变为负或零，强制后续读者进入慢路径\n  - **写者持有锁**：`readers = WRITER_BIAS`（最小负值）\n  - **读者持有锁**：`readers = READER_BIAS + N`（N 为活跃读者数）\n\n### 3.2 快速路径（Fast Path）\n- **读锁获取**：通过 `atomic_try_cmpxchg_acquire()` 原子递增 `readers`（仅当 `< 0` 不成立，即偏置存在）\n- **读锁释放**：`atomic_dec_and_test()`，仅当计数归零（即最后一个读者）时才需唤醒写者\n- 所有原子操作均使用 `_acquire` / `_release` 语义，确保内存顺序正确\n\n### 3.3 写者加锁流程\n1. 获取底层 `rtmutex`（可能阻塞）\n2. 清除 `READER_BIAS`，阻止新读者进入快速路径\n3. 在 `rtmutex.wait_lock` 保护下检查是否所有读者已退出（`readers == 0`）\n4. 若仍有读者，循环等待并调度，直到可安全设置 `WRITER_BIAS`\n\n### 3.4 非写者公平性\n- **明确不保证写者公平**：新到达的读者即使在写者等待时仍可获取读锁（若写者尚未清除 `READER_BIAS`）\n- 原因：实现完全公平需为每个读者代理锁定 `rtmutex` 并逐个继承优先级，这在 `SCHED_DEADLINE` 下不可行\n- 权衡：接受潜在写者饥饿风险，换取实现简洁性和典型 RT 场景下的低延迟\n\n### 3.5 与 `rtmutex` 的集成\n- 所有慢路径操作均在 `rtmutex.wait_lock`（raw spinlock）保护下进行\n- 写者通过 `rtmutex` 阻塞，天然获得 PI/DL 调度支持\n- 读者在慢路径中临时获取 `rtmutex` 以确保与写者互斥，成功后立即释放\n\n## 4. 依赖关系\n\n- **`rtmutex` 子系统**：依赖 `rt_mutex_base`、`rt_mutex_lock/unlock`、`rt_mutex_wake_q` 等接口实现阻塞/唤醒和优先级继承\n- **原子操作**：使用 `atomic_read`、`atomic_try_cmpxchg_acquire`、`atomic_add_return_release` 等提供内存序保证\n- **调度器**：调用 `rwbase_pre_schedule()` / `rwbase_post_schedule()`、`rwbase_schedule()` 与 RT 调度器交互\n- **跟踪机制**：使用 `trace_contention_begin/end` 提供锁竞争跟踪\n- **中断管理**：使用 `raw_spin_lock_irq{save/restore}` 保护关键区\n- **信号处理**：通过 `rwbase_signal_pending_state()` 支持可中断等待\n\n## 5. 使用场景\n\n- **实时任务中的读写同步**：适用于需要低延迟响应的 RT 任务访问共享数据结构\n- **`mmap_sem` 等内核锁的 RT 实现**：作为 `rw_semaphore` 的底层支持，用于内存管理等子系统\n- **避免优先级反转**：当高优先级写者需等待低优先级读者时，通过 `rtmutex` 的 PI 机制临时提升读者优先级\n- **高并发读场景**：允许多个读者无锁并发执行，仅在写者存在时才串行化\n- **不适用于强写者公平需求场景**：如需严格 FIFO 写者调度，应避免在 RT 任务中使用此类锁",
      "similarity": 0.5131123065948486,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/rwbase_rt.c",
          "start_line": 168,
          "end_line": 273,
          "content": [
            "static __always_inline void rwbase_read_unlock(struct rwbase_rt *rwb,",
            "\t\t\t\t\t       unsigned int state)",
            "{",
            "\t/*",
            "\t * rwb->readers can only hit 0 when a writer is waiting for the",
            "\t * active readers to leave the critical section.",
            "\t *",
            "\t * dec_and_test() is fully ordered, provides RELEASE.",
            "\t */",
            "\tif (unlikely(atomic_dec_and_test(&rwb->readers)))",
            "\t\t__rwbase_read_unlock(rwb, state);",
            "}",
            "static inline void __rwbase_write_unlock(struct rwbase_rt *rwb, int bias,",
            "\t\t\t\t\t unsigned long flags)",
            "{",
            "\tstruct rt_mutex_base *rtm = &rwb->rtmutex;",
            "",
            "\t/*",
            "\t * _release() is needed in case that reader is in fast path, pairing",
            "\t * with atomic_try_cmpxchg_acquire() in rwbase_read_trylock().",
            "\t */",
            "\t(void)atomic_add_return_release(READER_BIAS - bias, &rwb->readers);",
            "\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);",
            "\trwbase_rtmutex_unlock(rtm);",
            "}",
            "static inline void rwbase_write_unlock(struct rwbase_rt *rwb)",
            "{",
            "\tstruct rt_mutex_base *rtm = &rwb->rtmutex;",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&rtm->wait_lock, flags);",
            "\t__rwbase_write_unlock(rwb, WRITER_BIAS, flags);",
            "}",
            "static inline void rwbase_write_downgrade(struct rwbase_rt *rwb)",
            "{",
            "\tstruct rt_mutex_base *rtm = &rwb->rtmutex;",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&rtm->wait_lock, flags);",
            "\t/* Release it and account current as reader */",
            "\t__rwbase_write_unlock(rwb, WRITER_BIAS - 1, flags);",
            "}",
            "static inline bool __rwbase_write_trylock(struct rwbase_rt *rwb)",
            "{",
            "\t/* Can do without CAS because we're serialized by wait_lock. */",
            "\tlockdep_assert_held(&rwb->rtmutex.wait_lock);",
            "",
            "\t/*",
            "\t * _acquire is needed in case the reader is in the fast path, pairing",
            "\t * with rwbase_read_unlock(), provides ACQUIRE.",
            "\t */",
            "\tif (!atomic_read_acquire(&rwb->readers)) {",
            "\t\tatomic_set(&rwb->readers, WRITER_BIAS);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int __sched rwbase_write_lock(struct rwbase_rt *rwb,",
            "\t\t\t\t     unsigned int state)",
            "{",
            "\tstruct rt_mutex_base *rtm = &rwb->rtmutex;",
            "\tunsigned long flags;",
            "",
            "\t/* Take the rtmutex as a first step */",
            "\tif (rwbase_rtmutex_lock_state(rtm, state))",
            "\t\treturn -EINTR;",
            "",
            "\t/* Force readers into slow path */",
            "\tatomic_sub(READER_BIAS, &rwb->readers);",
            "",
            "\trwbase_pre_schedule();",
            "",
            "\traw_spin_lock_irqsave(&rtm->wait_lock, flags);",
            "\tif (__rwbase_write_trylock(rwb))",
            "\t\tgoto out_unlock;",
            "",
            "\trwbase_set_and_save_current_state(state);",
            "\ttrace_contention_begin(rwb, LCB_F_RT | LCB_F_WRITE);",
            "\tfor (;;) {",
            "\t\t/* Optimized out for rwlocks */",
            "\t\tif (rwbase_signal_pending_state(state, current)) {",
            "\t\t\trwbase_restore_current_state();",
            "\t\t\t__rwbase_write_unlock(rwb, 0, flags);",
            "\t\t\trwbase_post_schedule();",
            "\t\t\ttrace_contention_end(rwb, -EINTR);",
            "\t\t\treturn -EINTR;",
            "\t\t}",
            "",
            "\t\tif (__rwbase_write_trylock(rwb))",
            "\t\t\tbreak;",
            "",
            "\t\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);",
            "\t\trwbase_schedule();",
            "\t\traw_spin_lock_irqsave(&rtm->wait_lock, flags);",
            "",
            "\t\tset_current_state(state);",
            "\t}",
            "\trwbase_restore_current_state();",
            "\ttrace_contention_end(rwb, 0);",
            "",
            "out_unlock:",
            "\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);",
            "\trwbase_post_schedule();",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rwbase_read_unlock, __rwbase_write_unlock, rwbase_write_unlock, rwbase_write_downgrade, __rwbase_write_trylock, rwbase_write_lock",
          "description": "处理写锁的释放、降级与尝试获取逻辑，包含读者偏置调整、内存序保障、写锁竞争解决及状态转换的完整流程",
          "similarity": 0.515274703502655
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/rwbase_rt.c",
          "start_line": 53,
          "end_line": 163,
          "content": [
            "static __always_inline int rwbase_read_trylock(struct rwbase_rt *rwb)",
            "{",
            "\tint r;",
            "",
            "\t/*",
            "\t * Increment reader count, if sem->readers < 0, i.e. READER_BIAS is",
            "\t * set.",
            "\t */",
            "\tfor (r = atomic_read(&rwb->readers); r < 0;) {",
            "\t\tif (likely(atomic_try_cmpxchg_acquire(&rwb->readers, &r, r + 1)))",
            "\t\t\treturn 1;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int __sched __rwbase_read_lock(struct rwbase_rt *rwb,",
            "\t\t\t\t      unsigned int state)",
            "{",
            "\tstruct rt_mutex_base *rtm = &rwb->rtmutex;",
            "\tint ret;",
            "",
            "\trwbase_pre_schedule();",
            "\traw_spin_lock_irq(&rtm->wait_lock);",
            "",
            "\t/*",
            "\t * Call into the slow lock path with the rtmutex->wait_lock",
            "\t * held, so this can't result in the following race:",
            "\t *",
            "\t * Reader1\t\tReader2\t\tWriter",
            "\t *\t\t\tdown_read()",
            "\t *\t\t\t\t\tdown_write()",
            "\t *\t\t\t\t\trtmutex_lock(m)",
            "\t *\t\t\t\t\twait()",
            "\t * down_read()",
            "\t * unlock(m->wait_lock)",
            "\t *\t\t\tup_read()",
            "\t *\t\t\twake(Writer)",
            "\t *\t\t\t\t\tlock(m->wait_lock)",
            "\t *\t\t\t\t\tsem->writelocked=true",
            "\t *\t\t\t\t\tunlock(m->wait_lock)",
            "\t *",
            "\t *\t\t\t\t\tup_write()",
            "\t *\t\t\t\t\tsem->writelocked=false",
            "\t *\t\t\t\t\trtmutex_unlock(m)",
            "\t *\t\t\tdown_read()",
            "\t *\t\t\t\t\tdown_write()",
            "\t *\t\t\t\t\trtmutex_lock(m)",
            "\t *\t\t\t\t\twait()",
            "\t * rtmutex_lock(m)",
            "\t *",
            "\t * That would put Reader1 behind the writer waiting on",
            "\t * Reader2 to call up_read(), which might be unbound.",
            "\t */",
            "",
            "\ttrace_contention_begin(rwb, LCB_F_RT | LCB_F_READ);",
            "",
            "\t/*",
            "\t * For rwlocks this returns 0 unconditionally, so the below",
            "\t * !ret conditionals are optimized out.",
            "\t */",
            "\tret = rwbase_rtmutex_slowlock_locked(rtm, state);",
            "",
            "\t/*",
            "\t * On success the rtmutex is held, so there can't be a writer",
            "\t * active. Increment the reader count and immediately drop the",
            "\t * rtmutex again.",
            "\t *",
            "\t * rtmutex->wait_lock has to be unlocked in any case of course.",
            "\t */",
            "\tif (!ret)",
            "\t\tatomic_inc(&rwb->readers);",
            "\traw_spin_unlock_irq(&rtm->wait_lock);",
            "\tif (!ret)",
            "\t\trwbase_rtmutex_unlock(rtm);",
            "",
            "\ttrace_contention_end(rwb, ret);",
            "\trwbase_post_schedule();",
            "\treturn ret;",
            "}",
            "static __always_inline int rwbase_read_lock(struct rwbase_rt *rwb,",
            "\t\t\t\t\t    unsigned int state)",
            "{",
            "\tlockdep_assert(!current->pi_blocked_on);",
            "",
            "\tif (rwbase_read_trylock(rwb))",
            "\t\treturn 0;",
            "",
            "\treturn __rwbase_read_lock(rwb, state);",
            "}",
            "static void __sched __rwbase_read_unlock(struct rwbase_rt *rwb,",
            "\t\t\t\t\t unsigned int state)",
            "{",
            "\tstruct rt_mutex_base *rtm = &rwb->rtmutex;",
            "\tstruct task_struct *owner;",
            "\tDEFINE_RT_WAKE_Q(wqh);",
            "",
            "\traw_spin_lock_irq(&rtm->wait_lock);",
            "\t/*",
            "\t * Wake the writer, i.e. the rtmutex owner. It might release the",
            "\t * rtmutex concurrently in the fast path (due to a signal), but to",
            "\t * clean up rwb->readers it needs to acquire rtm->wait_lock. The",
            "\t * worst case which can happen is a spurious wakeup.",
            "\t */",
            "\towner = rt_mutex_owner(rtm);",
            "\tif (owner)",
            "\t\trt_mutex_wake_q_add_task(&wqh, owner, state);",
            "",
            "\t/* Pairs with the preempt_enable in rt_mutex_wake_up_q() */",
            "\tpreempt_disable();",
            "\traw_spin_unlock_irq(&rtm->wait_lock);",
            "\trt_mutex_wake_up_q(&wqh);",
            "}"
          ],
          "function_name": "rwbase_read_trylock, __rwbase_read_lock, rwbase_read_lock, __rwbase_read_unlock",
          "description": "实现读锁的尝试获取与锁定逻辑，包含原子操作更新读者计数、持有rtmutex_wait_lock保护临界区、唤醒等待写者的解锁逻辑",
          "similarity": 0.49135956168174744
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/locking/rwbase_rt.c",
          "start_line": 280,
          "end_line": 297,
          "content": [
            "static inline int rwbase_write_trylock(struct rwbase_rt *rwb)",
            "{",
            "\tstruct rt_mutex_base *rtm = &rwb->rtmutex;",
            "\tunsigned long flags;",
            "",
            "\tif (!rwbase_rtmutex_trylock(rtm))",
            "\t\treturn 0;",
            "",
            "\tatomic_sub(READER_BIAS, &rwb->readers);",
            "",
            "\traw_spin_lock_irqsave(&rtm->wait_lock, flags);",
            "\tif (__rwbase_write_trylock(rwb)) {",
            "\t\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);",
            "\t\treturn 1;",
            "\t}",
            "\t__rwbase_write_unlock(rwb, 0, flags);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rwbase_write_trylock",
          "description": "实现非阻塞式写锁尝试获取，通过先获取rtmutex再调整读者偏置，最终判断能否直接获得写锁的原子化操作流程",
          "similarity": 0.4855104386806488
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/rwbase_rt.c",
          "start_line": 1,
          "end_line": 52,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "",
            "/*",
            " * RT-specific reader/writer semaphores and reader/writer locks",
            " *",
            " * down_write/write_lock()",
            " *  1) Lock rtmutex",
            " *  2) Remove the reader BIAS to force readers into the slow path",
            " *  3) Wait until all readers have left the critical section",
            " *  4) Mark it write locked",
            " *",
            " * up_write/write_unlock()",
            " *  1) Remove the write locked marker",
            " *  2) Set the reader BIAS, so readers can use the fast path again",
            " *  3) Unlock rtmutex, to release blocked readers",
            " *",
            " * down_read/read_lock()",
            " *  1) Try fast path acquisition (reader BIAS is set)",
            " *  2) Take tmutex::wait_lock, which protects the writelocked flag",
            " *  3) If !writelocked, acquire it for read",
            " *  4) If writelocked, block on tmutex",
            " *  5) unlock rtmutex, goto 1)",
            " *",
            " * up_read/read_unlock()",
            " *  1) Try fast path release (reader count != 1)",
            " *  2) Wake the writer waiting in down_write()/write_lock() #3",
            " *",
            " * down_read/read_lock()#3 has the consequence, that rw semaphores and rw",
            " * locks on RT are not writer fair, but writers, which should be avoided in",
            " * RT tasks (think mmap_sem), are subject to the rtmutex priority/DL",
            " * inheritance mechanism.",
            " *",
            " * It's possible to make the rw primitives writer fair by keeping a list of",
            " * active readers. A blocked writer would force all newly incoming readers",
            " * to block on the rtmutex, but the rtmutex would have to be proxy locked",
            " * for one reader after the other. We can't use multi-reader inheritance",
            " * because there is no way to support that with SCHED_DEADLINE.",
            " * Implementing the one by one reader boosting/handover mechanism is a",
            " * major surgery for a very dubious value.",
            " *",
            " * The risk of writer starvation is there, but the pathological use cases",
            " * which trigger it are not necessarily the typical RT workloads.",
            " *",
            " * Fast-path orderings:",
            " * The lock/unlock of readers can run in fast paths: lock and unlock are only",
            " * atomic ops, and there is no inner lock to provide ACQUIRE and RELEASE",
            " * semantics of rwbase_rt. Atomic ops should thus provide _acquire()",
            " * and _release() (or stronger).",
            " *",
            " * Common code shared between RT rw_semaphore and rwlock",
            " */",
            ""
          ],
          "function_name": null,
          "description": "描述RT读写锁的实现机制，包括写锁强制读者进入慢路径、读锁快慢路径切换及优先级继承特性，说明其不支持公平写入但通过RT mutex机制控制并发",
          "similarity": 0.46925240755081177
        }
      ]
    }
  ]
}