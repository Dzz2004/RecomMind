{
  "query": "协议栈套接字状态机",
  "timestamp": "2025-12-26 01:19:33",
  "retrieved_files": [
    {
      "source_file": "kernel/rcu/sync.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:44:18\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\sync.c`\n\n---\n\n# rcu/sync.c 技术文档\n\n## 文件概述\n\n`rcu/sync.c` 实现了一个基于 RCU（Read-Copy-Update）机制的轻量级读写同步基础设施，称为 `rcu_sync`。该机制允许写者（更新者）在需要时强制所有读者切换到“慢路径”（slow path），并在更新完成后经过一个 RCU 宽限期（grace period）后，允许读者重新使用“快路径”（fast path）。该设计特别适用于需要频繁但短暂地禁用读者快路径的场景，避免了传统读写锁的开销，同时利用 RCU 的无锁读取特性提升性能。\n\n## 核心功能\n\n### 数据结构\n\n- **`struct rcu_sync`**  \n  核心同步控制结构，包含以下关键字段：\n  - `gp_state`：当前同步状态（`GP_IDLE`, `GP_ENTER`, `GP_PASSED`, `GP_EXIT`, `GP_REPLAY`）\n  - `gp_count`：嵌套的 `rcu_sync_enter()` 调用计数\n  - `cb_head`：用于 RCU 回调的 `rcu_head`\n  - `gp_wait`：等待队列，用于阻塞等待状态转换完成\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `rcu_sync_init()` | 初始化 `rcu_sync` 结构体 |\n| `rcu_sync_enter_start()` | 预激活同步机制，使 `rcu_sync_is_idle()` 返回 false，且后续 enter/exit 成为 NO-OP |\n| `rcu_sync_enter()` | 强制读者进入慢路径，确保后续读者不会使用快路径 |\n| `rcu_sync_exit()` | 标记更新结束，安排在宽限期后恢复读者快路径 |\n| `rcu_sync_dtor()` | 销毁 `rcu_sync` 结构，确保所有 RCU 回调已完成 |\n| `rcu_sync_func()` | RCU 回调函数，根据当前状态推进状态机 |\n\n## 关键实现\n\n### 状态机设计\n\n`rcu_sync` 使用五种状态实现高效的状态转换：\n\n- **`GP_IDLE`**：初始状态，读者可使用快路径。\n- **`GP_ENTER`**：正在进入同步状态，需等待宽限期。\n- **`GP_PASSED`**：宽限期已过，读者已全部进入慢路径。\n- **`GP_EXIT`**：正在退出同步，需等待另一个宽限期以恢复快路径。\n- **`GP_REPLAY`**：在退出过程中又有新的 enter/exit 对发生，需重新调度回调。\n\n### 嵌套与优化\n\n- **嵌套支持**：通过 `gp_count` 支持 `rcu_sync_enter()` 的嵌套调用。只有当 `gp_count` 从 1 递减到 0 时，才触发退出流程。\n- **宽限期合并**：连续的 `enter/exit` 调用可避免多次等待宽限期。例如：\n  - 若在 `GP_PASSED` 状态下调用 `exit`，直接进入 `GP_EXIT` 并调度回调。\n  - 若在回调执行前再次调用 `enter/exit`，状态转为 `GP_REPLAY`，并在回调中重新调度，避免冗余宽限期。\n- **快速路径优化**：首次 `enter` 时若处于 `GP_IDLE`，直接调用 `synchronize_rcu()` 而非异步 `call_rcu()`，可利用 `rcu_expedited` 或 `rcu_blocking_is_gp()` 加速。\n\n### 同步与唤醒\n\n- 写者调用 `rcu_sync_enter()` 后，若非首次进入，会阻塞在 `wait_event()`，直到状态变为 `GP_PASSED` 或更高。\n- `rcu_sync_func()` 在宽限期后执行，根据 `gp_count` 和当前状态决定是唤醒等待者、重调度回调，还是恢复到 `GP_IDLE`。\n\n## 依赖关系\n\n- **`<linux/rcu_sync.h>`**：定义 `struct rcu_sync` 及相关 API。\n- **`<linux/sched.h>`**：提供 `wait_event()`、`wake_up_locked()` 等调度和等待队列原语。\n- **RCU 子系统**：\n  - `call_rcu_hurry()` / `call_rcu()`：用于注册宽限期后的回调。\n  - `synchronize_rcu()`：用于同步等待宽限期。\n  - `rcu_barrier()`：在析构时确保所有回调完成。\n- **自旋锁**：使用 `spin_lock_irqsave()` 保护状态和计数器，确保中断上下文安全。\n\n## 使用场景\n\n- **文件系统元数据更新**：如 overlayfs、btrfs 等在修改共享元数据结构时，临时禁止读者使用快路径缓存。\n- **动态配置更新**：内核模块或子系统在热更新全局配置时，确保读者看到一致状态。\n- **轻量级写者同步**：适用于写操作较少但需高效读者路径的场景，避免传统 rwlock 的读者竞争开销。\n- **替代 `synchronize_rcu()` 的批量操作**：当多个连续更新可合并为一次宽限期等待时，提升性能。",
      "similarity": 0.5199832916259766,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/rcu/sync.c",
          "start_line": 21,
          "end_line": 136,
          "content": [
            "void rcu_sync_init(struct rcu_sync *rsp)",
            "{",
            "\tmemset(rsp, 0, sizeof(*rsp));",
            "\tinit_waitqueue_head(&rsp->gp_wait);",
            "}",
            "void rcu_sync_enter_start(struct rcu_sync *rsp)",
            "{",
            "\trsp->gp_count++;",
            "\trsp->gp_state = GP_PASSED;",
            "}",
            "static void rcu_sync_call(struct rcu_sync *rsp)",
            "{",
            "\tcall_rcu_hurry(&rsp->cb_head, rcu_sync_func);",
            "}",
            "static void rcu_sync_func(struct rcu_head *rhp)",
            "{",
            "\tstruct rcu_sync *rsp = container_of(rhp, struct rcu_sync, cb_head);",
            "\tunsigned long flags;",
            "",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_state) == GP_IDLE);",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_state) == GP_PASSED);",
            "",
            "\tspin_lock_irqsave(&rsp->rss_lock, flags);",
            "\tif (rsp->gp_count) {",
            "\t\t/*",
            "\t\t * We're at least a GP after the GP_IDLE->GP_ENTER transition.",
            "\t\t */",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_PASSED);",
            "\t\twake_up_locked(&rsp->gp_wait);",
            "\t} else if (rsp->gp_state == GP_REPLAY) {",
            "\t\t/*",
            "\t\t * A new rcu_sync_exit() has happened; requeue the callback to",
            "\t\t * catch a later GP.",
            "\t\t */",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_EXIT);",
            "\t\trcu_sync_call(rsp);",
            "\t} else {",
            "\t\t/*",
            "\t\t * We're at least a GP after the last rcu_sync_exit(); everybody",
            "\t\t * will now have observed the write side critical section.",
            "\t\t * Let 'em rip!",
            "\t\t */",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_IDLE);",
            "\t}",
            "\tspin_unlock_irqrestore(&rsp->rss_lock, flags);",
            "}",
            "void rcu_sync_enter(struct rcu_sync *rsp)",
            "{",
            "\tint gp_state;",
            "",
            "\tspin_lock_irq(&rsp->rss_lock);",
            "\tgp_state = rsp->gp_state;",
            "\tif (gp_state == GP_IDLE) {",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_ENTER);",
            "\t\tWARN_ON_ONCE(rsp->gp_count);",
            "\t\t/*",
            "\t\t * Note that we could simply do rcu_sync_call(rsp) here and",
            "\t\t * avoid the \"if (gp_state == GP_IDLE)\" block below.",
            "\t\t *",
            "\t\t * However, synchronize_rcu() can be faster if rcu_expedited",
            "\t\t * or rcu_blocking_is_gp() is true.",
            "\t\t *",
            "\t\t * Another reason is that we can't wait for rcu callback if",
            "\t\t * we are called at early boot time but this shouldn't happen.",
            "\t\t */",
            "\t}",
            "\trsp->gp_count++;",
            "\tspin_unlock_irq(&rsp->rss_lock);",
            "",
            "\tif (gp_state == GP_IDLE) {",
            "\t\t/*",
            "\t\t * See the comment above, this simply does the \"synchronous\"",
            "\t\t * call_rcu(rcu_sync_func) which does GP_ENTER -> GP_PASSED.",
            "\t\t */",
            "\t\tsynchronize_rcu();",
            "\t\trcu_sync_func(&rsp->cb_head);",
            "\t\t/* Not really needed, wait_event() would see GP_PASSED. */",
            "\t\treturn;",
            "\t}",
            "",
            "\twait_event(rsp->gp_wait, READ_ONCE(rsp->gp_state) >= GP_PASSED);",
            "}",
            "void rcu_sync_exit(struct rcu_sync *rsp)",
            "{",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_state) == GP_IDLE);",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_count) == 0);",
            "",
            "\tspin_lock_irq(&rsp->rss_lock);",
            "\tif (!--rsp->gp_count) {",
            "\t\tif (rsp->gp_state == GP_PASSED) {",
            "\t\t\tWRITE_ONCE(rsp->gp_state, GP_EXIT);",
            "\t\t\trcu_sync_call(rsp);",
            "\t\t} else if (rsp->gp_state == GP_EXIT) {",
            "\t\t\tWRITE_ONCE(rsp->gp_state, GP_REPLAY);",
            "\t\t}",
            "\t}",
            "\tspin_unlock_irq(&rsp->rss_lock);",
            "}",
            "void rcu_sync_dtor(struct rcu_sync *rsp)",
            "{",
            "\tint gp_state;",
            "",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_count));",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_state) == GP_PASSED);",
            "",
            "\tspin_lock_irq(&rsp->rss_lock);",
            "\tif (rsp->gp_state == GP_REPLAY)",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_EXIT);",
            "\tgp_state = rsp->gp_state;",
            "\tspin_unlock_irq(&rsp->rss_lock);",
            "",
            "\tif (gp_state != GP_IDLE) {",
            "\t\trcu_barrier();",
            "\t\tWARN_ON_ONCE(rsp->gp_state != GP_IDLE);",
            "\t}",
            "}"
          ],
          "function_name": "rcu_sync_init, rcu_sync_enter_start, rcu_sync_call, rcu_sync_func, rcu_sync_enter, rcu_sync_exit, rcu_sync_dtor",
          "description": "实现了RCU同步核心函数，包括初始化、状态管理、回调触发和退出处理，通过spinlock保护状态机并利用RCU回调实现延迟同步，用于协调读者-写者并发访问的安全转换",
          "similarity": 0.5696163177490234
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/rcu/sync.c",
          "start_line": 1,
          "end_line": 20,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0+",
            "/*",
            " * RCU-based infrastructure for lightweight reader-writer locking",
            " *",
            " * Copyright (c) 2015, Red Hat, Inc.",
            " *",
            " * Author: Oleg Nesterov <oleg@redhat.com>",
            " */",
            "",
            "#include <linux/rcu_sync.h>",
            "#include <linux/sched.h>",
            "",
            "enum { GP_IDLE = 0, GP_ENTER, GP_PASSED, GP_EXIT, GP_REPLAY };",
            "",
            "#define\trss_lock\tgp_wait.lock",
            "",
            "/**",
            " * rcu_sync_init() - Initialize an rcu_sync structure",
            " * @rsp: Pointer to rcu_sync structure to be initialized",
            " */"
          ],
          "function_name": null,
          "description": "定义了RCU同步基础设施的枚举常量和rcu_sync_init函数声明，用于初始化rcu_sync结构体，但代码上下文不完整",
          "similarity": 0.44888558983802795
        }
      ]
    },
    {
      "source_file": "kernel/livepatch/transition.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:34:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `livepatch\\transition.c`\n\n---\n\n# livepatch/transition.c 技术文档\n\n## 1. 文件概述\n\n`livepatch/transition.c` 是 Linux 内核实时补丁（Kernel Live Patching）子系统的核心组件之一，负责管理补丁状态转换过程。该文件实现了从旧代码到新补丁代码（或反向）的安全过渡机制，确保所有正在运行的任务（包括内核线程、用户态进程和 idle 线程）都能安全地切换到目标补丁状态，避免在函数栈中仍存在待替换函数时进行切换，从而防止系统崩溃或行为异常。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `klp_transition_patch`：指向当前正在进行状态转换的补丁对象。\n- `klp_target_state`：目标补丁状态（`KLP_PATCHED` 或 `KLP_UNPATCHED`），初始为 `KLP_UNDEFINED`。\n- `klp_signals_cnt`：用于统计信号处理相关计数（当前未在代码片段中完整使用）。\n- `klp_stack_entries`：每 CPU 栈追踪缓冲区，用于保存任务调用栈。\n\n### 主要函数\n- `klp_transition_work_fn()`：延迟工作队列回调，用于重试未能完成转换的“滞留”任务。\n- `klp_synchronize_transition()`：强制在所有 CPU 上执行调度同步，确保 RCU 不可见区域也能完成同步。\n- `klp_complete_transition()`：完成整个补丁状态转换，清理数据结构并调用回调。\n- `klp_cancel_transition()`：在转换开始前取消补丁操作。\n- `klp_update_patch_state()`：更新指定任务的补丁状态。\n- `klp_check_stack_func()`：检查给定函数是否出现在栈追踪中。\n- `klp_check_stack()`：检查任务栈中是否存在待替换/待移除的函数（代码片段中被截断）。\n\n### 静态键与调度集成\n- 在支持 `CONFIG_PREEMPT_DYNAMIC` 的系统上，通过 `sched_dynamic_klp_enable/disable()` 启用/禁用 cond_resched 中的栈检查。\n- 否则使用静态键 `klp_sched_try_switch_key` 控制是否在 `cond_resched()` 中进行补丁栈检查，以帮助 CPU 密集型内核线程完成补丁切换。\n\n## 3. 关键实现\n\n### 补丁状态转换流程\n1. **初始化阶段**：设置 `klp_transition_patch` 和 `klp_target_state`。\n2. **任务状态更新**：通过 `TIF_PATCH_PENDING` 标志标记需要更新状态的任务。\n3. **栈安全检查**：使用 `stack_trace_save_tsk_reliable()` 获取可靠栈追踪，检查是否存在待替换函数。\n4. **同步机制**：\n   - 使用 `klp_synchronize_transition()` 调用 `schedule_on_each_cpu(klp_sync)`，强制所有 CPU（包括 idle 和用户态）参与同步。\n   - 此机制绕过标准 RCU，适用于 RCU 不活跃的上下文（如 `user_exit()` 前）。\n5. **完成清理**：\n   - 清除所有任务的 `patch_state` 为 `KLP_UNDEFINED`。\n   - 调用对象级的 `post_patch` 或 `post_unpatch` 回调。\n   - 重置全局状态变量。\n\n### 栈检查逻辑\n- **打补丁时（KLP_PATCHED）**：检查栈中是否包含**旧函数**（原始函数或上一个补丁版本的函数）。\n- **卸补丁时（KLP_UNPATCHED）**：检查栈中是否包含**新函数**（当前补丁中的函数）。\n- 若发现相关函数在栈中，则返回 `-EAGAIN`，推迟该任务的状态切换。\n\n### 内存屏障与并发控制\n- `test_and_clear_tsk_thread_flag()` 不仅清除 `TIF_PATCH_PENDING`，还充当读屏障（`smp_rmb`），确保：\n  1. `klp_target_state` 的读取顺序正确。\n  2. 后续 `klp_ftrace_handler()` 能看到一致的 `func->transition` 状态。\n\n### 滞留任务处理\n- 通过 `DECLARE_DELAYED_WORK(klp_transition_work, ...)` 定期重试未能完成转换的任务，提高转换成功率。\n\n## 4. 依赖关系\n\n- **内部依赖**：\n  - `core.h`：提供 `klp_mutex`、`klp_for_each_object/func` 等核心宏和函数。\n  - `patch.h`：定义 `klp_func`、`klp_object`、`klp_patch` 等数据结构及操作函数（如 `klp_unpatch_objects`）。\n  - `transition.h`：声明本文件导出的接口（如 `klp_cancel_transition`）。\n- **内核子系统**：\n  - **RCU**：用于常规同步，但在 RCU 不活跃区域使用自定义同步。\n  - **调度器**：通过 `cond_resched()` 集成补丁检查，依赖 `CONFIG_PREEMPT_DYNAMIC` 或静态键。\n  - **栈追踪**：使用 `stack_trace_save_tsk_reliable()` 获取可靠调用栈。\n  - **CPU 热插拔**：通过 `for_each_possible_cpu` 处理所有可能的 CPU（包括离线 CPU 的 idle 任务）。\n\n## 5. 使用场景\n\n- **应用实时补丁**：当管理员通过 sysfs 启用一个 livepatch 模块时，内核调用此文件中的函数将所有任务从旧代码切换到新补丁代码。\n- **卸载实时补丁**：当禁用补丁时，安全地将所有任务切换回旧函数，并清理补丁数据结构。\n- **处理滞留任务**：对于因长时间运行或处于不可中断状态而未能及时切换的任务，通过延迟工作队列周期性重试。\n- **支持特殊上下文**：确保在 RCU 不活跃的上下文（如系统调用入口/出口、idle 循环）中也能安全完成补丁切换。\n- **错误恢复**：在补丁初始化后、实际切换前发生错误时，调用 `klp_cancel_transition()` 安全回滚。",
      "similarity": 0.5195190906524658,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/livepatch/transition.c",
          "start_line": 214,
          "end_line": 352,
          "content": [
            "static int klp_check_stack_func(struct klp_func *func, unsigned long *entries,",
            "\t\t\t\tunsigned int nr_entries)",
            "{",
            "\tunsigned long func_addr, func_size, address;",
            "\tstruct klp_ops *ops;",
            "\tint i;",
            "",
            "\tif (klp_target_state == KLP_UNPATCHED) {",
            "\t\t /*",
            "\t\t  * Check for the to-be-unpatched function",
            "\t\t  * (the func itself).",
            "\t\t  */",
            "\t\tfunc_addr = (unsigned long)func->new_func;",
            "\t\tfunc_size = func->new_size;",
            "\t} else {",
            "\t\t/*",
            "\t\t * Check for the to-be-patched function",
            "\t\t * (the previous func).",
            "\t\t */",
            "\t\tops = klp_find_ops(func->old_func);",
            "",
            "\t\tif (list_is_singular(&ops->func_stack)) {",
            "\t\t\t/* original function */",
            "\t\t\tfunc_addr = (unsigned long)func->old_func;",
            "\t\t\tfunc_size = func->old_size;",
            "\t\t} else {",
            "\t\t\t/* previously patched function */",
            "\t\t\tstruct klp_func *prev;",
            "",
            "\t\t\tprev = list_next_entry(func, stack_node);",
            "\t\t\tfunc_addr = (unsigned long)prev->new_func;",
            "\t\t\tfunc_size = prev->new_size;",
            "\t\t}",
            "\t}",
            "",
            "\tfor (i = 0; i < nr_entries; i++) {",
            "\t\taddress = entries[i];",
            "",
            "\t\tif (address >= func_addr && address < func_addr + func_size)",
            "\t\t\treturn -EAGAIN;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int klp_check_stack(struct task_struct *task, const char **oldname)",
            "{",
            "\tunsigned long *entries = this_cpu_ptr(klp_stack_entries);",
            "\tstruct klp_object *obj;",
            "\tstruct klp_func *func;",
            "\tint ret, nr_entries;",
            "",
            "\t/* Protect 'klp_stack_entries' */",
            "\tlockdep_assert_preemption_disabled();",
            "",
            "\tret = stack_trace_save_tsk_reliable(task, entries, MAX_STACK_ENTRIES);",
            "\tif (ret < 0)",
            "\t\treturn -EINVAL;",
            "\tnr_entries = ret;",
            "",
            "\tklp_for_each_object(klp_transition_patch, obj) {",
            "\t\tif (!obj->patched)",
            "\t\t\tcontinue;",
            "\t\tklp_for_each_func(obj, func) {",
            "\t\t\tret = klp_check_stack_func(func, entries, nr_entries);",
            "\t\t\tif (ret) {",
            "\t\t\t\t*oldname = func->old_name;",
            "\t\t\t\treturn -EADDRINUSE;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int klp_check_and_switch_task(struct task_struct *task, void *arg)",
            "{",
            "\tint ret;",
            "",
            "\tif (task_curr(task) && task != current)",
            "\t\treturn -EBUSY;",
            "",
            "\tret = klp_check_stack(task, arg);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tclear_tsk_thread_flag(task, TIF_PATCH_PENDING);",
            "\ttask->patch_state = klp_target_state;",
            "\treturn 0;",
            "}",
            "static bool klp_try_switch_task(struct task_struct *task)",
            "{",
            "\tconst char *old_name;",
            "\tint ret;",
            "",
            "\t/* check if this task has already switched over */",
            "\tif (task->patch_state == klp_target_state)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * For arches which don't have reliable stack traces, we have to rely",
            "\t * on other methods (e.g., switching tasks at kernel exit).",
            "\t */",
            "\tif (!klp_have_reliable_stack())",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Now try to check the stack for any to-be-patched or to-be-unpatched",
            "\t * functions.  If all goes well, switch the task to the target patch",
            "\t * state.",
            "\t */",
            "\tif (task == current)",
            "\t\tret = klp_check_and_switch_task(current, &old_name);",
            "\telse",
            "\t\tret = task_call_func(task, klp_check_and_switch_task, &old_name);",
            "",
            "\tswitch (ret) {",
            "\tcase 0:\t\t/* success */",
            "\t\tbreak;",
            "",
            "\tcase -EBUSY:\t/* klp_check_and_switch_task() */",
            "\t\tpr_debug(\"%s: %s:%d is running\\n\",",
            "\t\t\t __func__, task->comm, task->pid);",
            "\t\tbreak;",
            "\tcase -EINVAL:\t/* klp_check_and_switch_task() */",
            "\t\tpr_debug(\"%s: %s:%d has an unreliable stack\\n\",",
            "\t\t\t __func__, task->comm, task->pid);",
            "\t\tbreak;",
            "\tcase -EADDRINUSE: /* klp_check_and_switch_task() */",
            "\t\tpr_debug(\"%s: %s:%d is sleeping on function %s\\n\",",
            "\t\t\t __func__, task->comm, task->pid, old_name);",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tpr_debug(\"%s: Unknown error code (%d) when trying to switch %s:%d\\n\",",
            "\t\t\t __func__, ret, task->comm, task->pid);",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn !ret;",
            "}"
          ],
          "function_name": "klp_check_stack_func, klp_check_stack, klp_check_and_switch_task, klp_try_switch_task",
          "description": "提供堆栈检查与任务状态切换机制，验证当前线程堆栈中是否包含待修改函数地址，确保安全切换到目标补丁状态",
          "similarity": 0.58869469165802
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/livepatch/transition.c",
          "start_line": 53,
          "end_line": 172,
          "content": [
            "static void klp_transition_work_fn(struct work_struct *work)",
            "{",
            "\tmutex_lock(&klp_mutex);",
            "",
            "\tif (klp_transition_patch)",
            "\t\tklp_try_complete_transition();",
            "",
            "\tmutex_unlock(&klp_mutex);",
            "}",
            "static void klp_sync(struct work_struct *work)",
            "{",
            "}",
            "static void klp_synchronize_transition(void)",
            "{",
            "\tschedule_on_each_cpu(klp_sync);",
            "}",
            "static void klp_complete_transition(void)",
            "{",
            "\tstruct klp_object *obj;",
            "\tstruct klp_func *func;",
            "\tstruct task_struct *g, *task;",
            "\tunsigned int cpu;",
            "",
            "\tpr_debug(\"'%s': completing %s transition\\n\",",
            "\t\t klp_transition_patch->mod->name,",
            "\t\t klp_target_state == KLP_PATCHED ? \"patching\" : \"unpatching\");",
            "",
            "\tif (klp_transition_patch->replace && klp_target_state == KLP_PATCHED) {",
            "\t\tklp_unpatch_replaced_patches(klp_transition_patch);",
            "\t\tklp_discard_nops(klp_transition_patch);",
            "\t}",
            "",
            "\tif (klp_target_state == KLP_UNPATCHED) {",
            "\t\t/*",
            "\t\t * All tasks have transitioned to KLP_UNPATCHED so we can now",
            "\t\t * remove the new functions from the func_stack.",
            "\t\t */",
            "\t\tklp_unpatch_objects(klp_transition_patch);",
            "",
            "\t\t/*",
            "\t\t * Make sure klp_ftrace_handler() can no longer see functions",
            "\t\t * from this patch on the ops->func_stack.  Otherwise, after",
            "\t\t * func->transition gets cleared, the handler may choose a",
            "\t\t * removed function.",
            "\t\t */",
            "\t\tklp_synchronize_transition();",
            "\t}",
            "",
            "\tklp_for_each_object(klp_transition_patch, obj)",
            "\t\tklp_for_each_func(obj, func)",
            "\t\t\tfunc->transition = false;",
            "",
            "\t/* Prevent klp_ftrace_handler() from seeing KLP_UNDEFINED state */",
            "\tif (klp_target_state == KLP_PATCHED)",
            "\t\tklp_synchronize_transition();",
            "",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task) {",
            "\t\tWARN_ON_ONCE(test_tsk_thread_flag(task, TIF_PATCH_PENDING));",
            "\t\ttask->patch_state = KLP_UNDEFINED;",
            "\t}",
            "\tread_unlock(&tasklist_lock);",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\ttask = idle_task(cpu);",
            "\t\tWARN_ON_ONCE(test_tsk_thread_flag(task, TIF_PATCH_PENDING));",
            "\t\ttask->patch_state = KLP_UNDEFINED;",
            "\t}",
            "",
            "\tklp_for_each_object(klp_transition_patch, obj) {",
            "\t\tif (!klp_is_object_loaded(obj))",
            "\t\t\tcontinue;",
            "\t\tif (klp_target_state == KLP_PATCHED)",
            "\t\t\tklp_post_patch_callback(obj);",
            "\t\telse if (klp_target_state == KLP_UNPATCHED)",
            "\t\t\tklp_post_unpatch_callback(obj);",
            "\t}",
            "",
            "\tpr_notice(\"'%s': %s complete\\n\", klp_transition_patch->mod->name,",
            "\t\t  klp_target_state == KLP_PATCHED ? \"patching\" : \"unpatching\");",
            "",
            "\tklp_target_state = KLP_UNDEFINED;",
            "\tklp_transition_patch = NULL;",
            "}",
            "void klp_cancel_transition(void)",
            "{",
            "\tif (WARN_ON_ONCE(klp_target_state != KLP_PATCHED))",
            "\t\treturn;",
            "",
            "\tpr_debug(\"'%s': canceling patching transition, going to unpatch\\n\",",
            "\t\t klp_transition_patch->mod->name);",
            "",
            "\tklp_target_state = KLP_UNPATCHED;",
            "\tklp_complete_transition();",
            "}",
            "void klp_update_patch_state(struct task_struct *task)",
            "{",
            "\t/*",
            "\t * A variant of synchronize_rcu() is used to allow patching functions",
            "\t * where RCU is not watching, see klp_synchronize_transition().",
            "\t */",
            "\tpreempt_disable_notrace();",
            "",
            "\t/*",
            "\t * This test_and_clear_tsk_thread_flag() call also serves as a read",
            "\t * barrier (smp_rmb) for two cases:",
            "\t *",
            "\t * 1) Enforce the order of the TIF_PATCH_PENDING read and the",
            "\t *    klp_target_state read.  The corresponding write barriers are in",
            "\t *    klp_init_transition() and klp_reverse_transition().",
            "\t *",
            "\t * 2) Enforce the order of the TIF_PATCH_PENDING read and a future read",
            "\t *    of func->transition, if klp_ftrace_handler() is called later on",
            "\t *    the same CPU.  See __klp_disable_patch().",
            "\t */",
            "\tif (test_and_clear_tsk_thread_flag(task, TIF_PATCH_PENDING))",
            "\t\ttask->patch_state = READ_ONCE(klp_target_state);",
            "",
            "\tpreempt_enable_notrace();",
            "}"
          ],
          "function_name": "klp_transition_work_fn, klp_sync, klp_synchronize_transition, klp_complete_transition, klp_cancel_transition, klp_update_patch_state",
          "description": "实现热补丁过渡的核心协程逻辑，包含任务状态同步、补丁完成处理、取消操作及状态更新，通过循环遍历进程和空闲任务确保全部迁移",
          "similarity": 0.5053623914718628
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/livepatch/transition.c",
          "start_line": 366,
          "end_line": 509,
          "content": [
            "void __klp_sched_try_switch(void)",
            "{",
            "\tif (likely(!klp_patch_pending(current)))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This function is called from cond_resched() which is called in many",
            "\t * places throughout the kernel.  Using the klp_mutex here might",
            "\t * deadlock.",
            "\t *",
            "\t * Instead, disable preemption to prevent racing with other callers of",
            "\t * klp_try_switch_task().  Thanks to task_call_func() they won't be",
            "\t * able to switch this task while it's running.",
            "\t */",
            "\tpreempt_disable();",
            "",
            "\t/*",
            "\t * Make sure current didn't get patched between the above check and",
            "\t * preempt_disable().",
            "\t */",
            "\tif (unlikely(!klp_patch_pending(current)))",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * Enforce the order of the TIF_PATCH_PENDING read above and the",
            "\t * klp_target_state read in klp_try_switch_task().  The corresponding",
            "\t * write barriers are in klp_init_transition() and",
            "\t * klp_reverse_transition().",
            "\t */",
            "\tsmp_rmb();",
            "",
            "\tklp_try_switch_task(current);",
            "",
            "out:",
            "\tpreempt_enable();",
            "}",
            "static void klp_send_signals(void)",
            "{",
            "\tstruct task_struct *g, *task;",
            "",
            "\tif (klp_signals_cnt == SIGNALS_TIMEOUT)",
            "\t\tpr_notice(\"signaling remaining tasks\\n\");",
            "",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task) {",
            "\t\tif (!klp_patch_pending(task))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * There is a small race here. We could see TIF_PATCH_PENDING",
            "\t\t * set and decide to wake up a kthread or send a fake signal.",
            "\t\t * Meanwhile the task could migrate itself and the action",
            "\t\t * would be meaningless. It is not serious though.",
            "\t\t */",
            "\t\tif (task->flags & PF_KTHREAD) {",
            "\t\t\t/*",
            "\t\t\t * Wake up a kthread which sleeps interruptedly and",
            "\t\t\t * still has not been migrated.",
            "\t\t\t */",
            "\t\t\twake_up_state(task, TASK_INTERRUPTIBLE);",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * Send fake signal to all non-kthread tasks which are",
            "\t\t\t * still not migrated.",
            "\t\t\t */",
            "\t\t\tset_notify_signal(task);",
            "\t\t}",
            "\t}",
            "\tread_unlock(&tasklist_lock);",
            "}",
            "void klp_try_complete_transition(void)",
            "{",
            "\tunsigned int cpu;",
            "\tstruct task_struct *g, *task;",
            "\tstruct klp_patch *patch;",
            "\tbool complete = true;",
            "",
            "\tWARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);",
            "",
            "\t/*",
            "\t * Try to switch the tasks to the target patch state by walking their",
            "\t * stacks and looking for any to-be-patched or to-be-unpatched",
            "\t * functions.  If such functions are found on a stack, or if the stack",
            "\t * is deemed unreliable, the task can't be switched yet.",
            "\t *",
            "\t * Usually this will transition most (or all) of the tasks on a system",
            "\t * unless the patch includes changes to a very common function.",
            "\t */",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task)",
            "\t\tif (!klp_try_switch_task(task))",
            "\t\t\tcomplete = false;",
            "\tread_unlock(&tasklist_lock);",
            "",
            "\t/*",
            "\t * Ditto for the idle \"swapper\" tasks.",
            "\t */",
            "\tcpus_read_lock();",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\ttask = idle_task(cpu);",
            "\t\tif (cpu_online(cpu)) {",
            "\t\t\tif (!klp_try_switch_task(task)) {",
            "\t\t\t\tcomplete = false;",
            "\t\t\t\t/* Make idle task go through the main loop. */",
            "\t\t\t\twake_up_if_idle(cpu);",
            "\t\t\t}",
            "\t\t} else if (task->patch_state != klp_target_state) {",
            "\t\t\t/* offline idle tasks can be switched immediately */",
            "\t\t\tclear_tsk_thread_flag(task, TIF_PATCH_PENDING);",
            "\t\t\ttask->patch_state = klp_target_state;",
            "\t\t}",
            "\t}",
            "\tcpus_read_unlock();",
            "",
            "\tif (!complete) {",
            "\t\tif (klp_signals_cnt && !(klp_signals_cnt % SIGNALS_TIMEOUT))",
            "\t\t\tklp_send_signals();",
            "\t\tklp_signals_cnt++;",
            "",
            "\t\t/*",
            "\t\t * Some tasks weren't able to be switched over.  Try again",
            "\t\t * later and/or wait for other methods like kernel exit",
            "\t\t * switching.",
            "\t\t */",
            "\t\tschedule_delayed_work(&klp_transition_work,",
            "\t\t\t\t      round_jiffies_relative(HZ));",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Done!  Now cleanup the data structures. */",
            "\tklp_cond_resched_disable();",
            "\tpatch = klp_transition_patch;",
            "\tklp_complete_transition();",
            "",
            "\t/*",
            "\t * It would make more sense to free the unused patches in",
            "\t * klp_complete_transition() but it is called also",
            "\t * from klp_cancel_transition().",
            "\t */",
            "\tif (!patch->enabled)",
            "\t\tklp_free_patch_async(patch);",
            "\telse if (patch->replace)",
            "\t\tklp_free_replaced_patches_async(patch);",
            "}"
          ],
          "function_name": "__klp_sched_try_switch, klp_send_signals, klp_try_complete_transition",
          "description": "调度器层面的过渡辅助函数，通过禁止抢占防止竞态条件，并向未迁移任务发送信号触发上下文切换以完成补丁应用",
          "similarity": 0.5052216053009033
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/livepatch/transition.c",
          "start_line": 648,
          "end_line": 740,
          "content": [
            "void klp_reverse_transition(void)",
            "{",
            "\tunsigned int cpu;",
            "\tstruct task_struct *g, *task;",
            "",
            "\tpr_debug(\"'%s': reversing transition from %s\\n\",",
            "\t\t klp_transition_patch->mod->name,",
            "\t\t klp_target_state == KLP_PATCHED ? \"patching to unpatching\" :",
            "\t\t\t\t\t\t   \"unpatching to patching\");",
            "",
            "\t/*",
            "\t * Clear all TIF_PATCH_PENDING flags to prevent races caused by",
            "\t * klp_update_patch_state() or __klp_sched_try_switch() running in",
            "\t * parallel with the reverse transition.",
            "\t */",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task)",
            "\t\tclear_tsk_thread_flag(task, TIF_PATCH_PENDING);",
            "\tread_unlock(&tasklist_lock);",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tclear_tsk_thread_flag(idle_task(cpu), TIF_PATCH_PENDING);",
            "",
            "\t/*",
            "\t * Make sure all existing invocations of klp_update_patch_state() and",
            "\t * __klp_sched_try_switch() see the cleared TIF_PATCH_PENDING before",
            "\t * starting the reverse transition.",
            "\t */",
            "\tklp_synchronize_transition();",
            "",
            "\t/*",
            "\t * All patching has stopped, now re-initialize the global variables to",
            "\t * prepare for the reverse transition.",
            "\t */",
            "\tklp_transition_patch->enabled = !klp_transition_patch->enabled;",
            "\tklp_target_state = !klp_target_state;",
            "",
            "\t/*",
            "\t * Enforce the order of the klp_target_state write and the",
            "\t * TIF_PATCH_PENDING writes in klp_start_transition() to ensure",
            "\t * klp_update_patch_state() and __klp_sched_try_switch() don't set",
            "\t * task->patch_state to the wrong value.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\tklp_start_transition();",
            "}",
            "void klp_copy_process(struct task_struct *child)",
            "{",
            "",
            "\t/*",
            "\t * The parent process may have gone through a KLP transition since",
            "\t * the thread flag was copied in setup_thread_stack earlier. Bring",
            "\t * the task flag up to date with the parent here.",
            "\t *",
            "\t * The operation is serialized against all klp_*_transition()",
            "\t * operations by the tasklist_lock. The only exceptions are",
            "\t * klp_update_patch_state(current) and __klp_sched_try_switch(), but we",
            "\t * cannot race with them because we are current.",
            "\t */",
            "\tif (test_tsk_thread_flag(current, TIF_PATCH_PENDING))",
            "\t\tset_tsk_thread_flag(child, TIF_PATCH_PENDING);",
            "\telse",
            "\t\tclear_tsk_thread_flag(child, TIF_PATCH_PENDING);",
            "",
            "\tchild->patch_state = current->patch_state;",
            "}",
            "void klp_force_transition(void)",
            "{",
            "\tstruct klp_patch *patch;",
            "\tstruct task_struct *g, *task;",
            "\tunsigned int cpu;",
            "",
            "\tpr_warn(\"forcing remaining tasks to the patched state\\n\");",
            "",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task)",
            "\t\tklp_update_patch_state(task);",
            "\tread_unlock(&tasklist_lock);",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tklp_update_patch_state(idle_task(cpu));",
            "",
            "\t/* Set forced flag for patches being removed. */",
            "\tif (klp_target_state == KLP_UNPATCHED)",
            "\t\tklp_transition_patch->forced = true;",
            "\telse if (klp_transition_patch->replace) {",
            "\t\tklp_for_each_patch(patch) {",
            "\t\t\tif (patch != klp_transition_patch)",
            "\t\t\t\tpatch->forced = true;",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "klp_reverse_transition, klp_copy_process, klp_force_transition",
          "description": "该代码段实现Live Patching框架中的状态转换控制逻辑。  \n`klp_reverse_transition`负责反向转换补丁状态，清除所有任务的TIF_PATCH_PENDING标志并切换全局状态后启动反向迁移；`klp_copy_process`在进程复制时同步父进程的补丁状态标志；`klp_force_transition`强制将剩余任务设为目标状态，并标记待移除补丁的强制属性。  \n\n注：代码依赖`klp_transition_patch`、`klp_target_state`等全局变量及`tasklist_lock`等上下文，此处仅展示部分实现。",
          "similarity": 0.4845210909843445
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/livepatch/transition.c",
          "start_line": 530,
          "end_line": 634,
          "content": [
            "void klp_start_transition(void)",
            "{",
            "\tstruct task_struct *g, *task;",
            "\tunsigned int cpu;",
            "",
            "\tWARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);",
            "",
            "\tpr_notice(\"'%s': starting %s transition\\n\",",
            "\t\t  klp_transition_patch->mod->name,",
            "\t\t  klp_target_state == KLP_PATCHED ? \"patching\" : \"unpatching\");",
            "",
            "\t/*",
            "\t * Mark all normal tasks as needing a patch state update.  They'll",
            "\t * switch either in klp_try_complete_transition() or as they exit the",
            "\t * kernel.",
            "\t */",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task)",
            "\t\tif (task->patch_state != klp_target_state)",
            "\t\t\tset_tsk_thread_flag(task, TIF_PATCH_PENDING);",
            "\tread_unlock(&tasklist_lock);",
            "",
            "\t/*",
            "\t * Mark all idle tasks as needing a patch state update.  They'll switch",
            "\t * either in klp_try_complete_transition() or at the idle loop switch",
            "\t * point.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\ttask = idle_task(cpu);",
            "\t\tif (task->patch_state != klp_target_state)",
            "\t\t\tset_tsk_thread_flag(task, TIF_PATCH_PENDING);",
            "\t}",
            "",
            "\tklp_cond_resched_enable();",
            "",
            "\tklp_signals_cnt = 0;",
            "}",
            "void klp_init_transition(struct klp_patch *patch, int state)",
            "{",
            "\tstruct task_struct *g, *task;",
            "\tunsigned int cpu;",
            "\tstruct klp_object *obj;",
            "\tstruct klp_func *func;",
            "\tint initial_state = !state;",
            "",
            "\tWARN_ON_ONCE(klp_target_state != KLP_UNDEFINED);",
            "",
            "\tklp_transition_patch = patch;",
            "",
            "\t/*",
            "\t * Set the global target patch state which tasks will switch to.  This",
            "\t * has no effect until the TIF_PATCH_PENDING flags get set later.",
            "\t */",
            "\tklp_target_state = state;",
            "",
            "\tpr_debug(\"'%s': initializing %s transition\\n\", patch->mod->name,",
            "\t\t klp_target_state == KLP_PATCHED ? \"patching\" : \"unpatching\");",
            "",
            "\t/*",
            "\t * Initialize all tasks to the initial patch state to prepare them for",
            "\t * switching to the target state.",
            "\t */",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task) {",
            "\t\tWARN_ON_ONCE(task->patch_state != KLP_UNDEFINED);",
            "\t\ttask->patch_state = initial_state;",
            "\t}",
            "\tread_unlock(&tasklist_lock);",
            "",
            "\t/*",
            "\t * Ditto for the idle \"swapper\" tasks.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\ttask = idle_task(cpu);",
            "\t\tWARN_ON_ONCE(task->patch_state != KLP_UNDEFINED);",
            "\t\ttask->patch_state = initial_state;",
            "\t}",
            "",
            "\t/*",
            "\t * Enforce the order of the task->patch_state initializations and the",
            "\t * func->transition updates to ensure that klp_ftrace_handler() doesn't",
            "\t * see a func in transition with a task->patch_state of KLP_UNDEFINED.",
            "\t *",
            "\t * Also enforce the order of the klp_target_state write and future",
            "\t * TIF_PATCH_PENDING writes to ensure klp_update_patch_state() and",
            "\t * __klp_sched_try_switch() don't set a task->patch_state to",
            "\t * KLP_UNDEFINED.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\t/*",
            "\t * Set the func transition states so klp_ftrace_handler() will know to",
            "\t * switch to the transition logic.",
            "\t *",
            "\t * When patching, the funcs aren't yet in the func_stack and will be",
            "\t * made visible to the ftrace handler shortly by the calls to",
            "\t * klp_patch_object().",
            "\t *",
            "\t * When unpatching, the funcs are already in the func_stack and so are",
            "\t * already visible to the ftrace handler.",
            "\t */",
            "\tklp_for_each_object(patch, obj)",
            "\t\tklp_for_each_func(obj, func)",
            "\t\t\tfunc->transition = true;",
            "}"
          ],
          "function_name": "klp_start_transition, klp_init_transition",
          "description": "初始化热补丁过渡阶段，设置全局目标状态并批量标记所有任务需更新补丁状态，通过内存屏障保证状态更新顺序性",
          "similarity": 0.47064512968063354
        }
      ]
    },
    {
      "source_file": "kernel/futex/requeue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:34:56\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `futex\\requeue.c`\n\n---\n\n# futex/requeue.c 技术文档\n\n## 1. 文件概述\n\n`futex/requeue.c` 是 Linux 内核 FUTEX（Fast Userspace muTEX）子系统中的关键实现文件，专门负责 **FUTEX_REQUEUE** 和 **FUTEX_CMP_REQUEUE_PI** 等操作中涉及的 **futex 等待队列重排队（requeue）逻辑**，尤其针对 **PI（Priority Inheritance，优先级继承）futex** 的复杂场景。该文件的核心目标是在保证正确性和避免死锁的前提下，高效地将等待在源 futex（uaddr1）上的任务迁移到目标 futex（uaddr2）上，并处理 PI 相关的状态同步与唤醒机制。特别地，它解决了在 **PREEMPT_RT** 实时内核配置下，由于底层使用 rtmutex 实现自旋锁而引发的潜在状态冲突问题。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`enum requeue_pi_state`**: 定义 PI futex 重排队过程中的状态机，用于协调重排队操作者（requeue side）与被重排队任务（waiter side）之间的同步。\n  - `Q_REQUEUE_PI_NONE`: 初始状态。\n  - `Q_REQUEUE_PI_IGNORE`: 等待者已提前唤醒，应忽略此次重排队。\n  - `Q_REQUEUE_PI_IN_PROGRESS`: 重排队操作正在进行中。\n  - `Q_REQUEUE_PI_WAIT`: 等待者在重排队过程中被唤醒，需等待操作完成。\n  - `Q_REQUEUE_PI_DONE`: 重排队成功，任务在目标 futex 上等待。\n  - `Q_REQUEUE_PI_LOCKED`: 重排队成功，且任务已原子地获取了目标 futex 锁。\n- **`futex_q_init`**: `futex_q` 结构体的初始化模板，将 `requeue_state` 初始化为 `Q_REQUEUE_PI_NONE`。\n\n### 主要函数\n\n- **`requeue_futex()`**: 将一个 `futex_q` 从源哈希桶 (`hb1`) 移动到目标哈希桶 (`hb2`)，并更新其关联的 futex key。如果源和目标哈希桶相同，则跳过移动操作。\n- **`futex_requeue_pi_prepare()`**: 为 PI 重排队操作做准备。尝试将等待者的 `requeue_state` 从 `NONE` 设置为 `IN_PROGRESS`。如果状态已是 `IGNORE`，则返回 `false` 表示应跳过此等待者。\n- **`futex_requeue_pi_complete()`**: 完成 PI 重排队操作。根据操作结果（成功/失败/死锁）和当前状态，将 `requeue_state` 更新为 `DONE`、`LOCKED`、`NONE` 或 `IGNORE`。在 `PREEMPT_RT` 下，若存在状态交错（`WAIT`），会唤醒等待者。\n- **`futex_requeue_pi_wakeup_sync()`**: 由被重排队的等待者调用，用于处理在重排队过程中发生的提前唤醒（如超时或信号）。它会根据当前重排队状态，设置自身状态为 `IGNORE` 或 `WAIT`，并在必要时阻塞等待重排队操作完成。\n- **`requeue_pi_wake_futex()`**: 在重排队过程中，如果目标 futex 锁可以被原子获取（无竞争或通过锁窃取），则直接唤醒该等待者。此函数负责更新 `futex_q` 的状态（key、rt_waiter、lock_ptr），完成重排队状态机，并最终唤醒任务。\n- **`futex_proxy_trylock_atomic()`**: （声明未在片段中完整给出，但为关键函数）尝试为重排队目标 futex 的 top waiter 原子地获取锁，并建立 PI 状态（`pi_state`）。这是实现 `FUTEX_CMP_REQUEUE_PI` 无竞争快速路径的核心。\n\n## 3. 关键实现\n\n### PI 重排队状态机\n文件的核心是一个精心设计的无锁状态机，通过 `atomic_t requeue_state` 字段在重排队操作者和被操作的等待者之间进行同步。该状态机定义了双方允许的状态转换，确保了在并发唤醒（如信号、超时）和重排队操作交错发生时，系统状态的一致性和正确性。\n\n### PREEMPT_RT 兼容性\n在 `PREEMPT_RT` 内核中，哈希桶锁（`hb->lock`）底层是 `rtmutex`。一个任务不能同时阻塞在两个 `rtmutex` 上。如果一个刚被唤醒的任务（因信号/超时）试图获取源 futex 的哈希桶锁，而该锁正被重排队代码持有，就会与重排队过程中可能涉及的代理锁（proxy lock）操作冲突。本文件通过状态机机制，让提前唤醒的任务能“通知”重排队代码忽略它，从而避免了这种冲突和潜在的状态损坏。\n\n### 原子锁获取与唤醒\n`requeue_pi_wake_futex()` 函数实现了在重排队过程中直接获取目标 futex 锁并唤醒任务的优化路径。这避免了任务先被加入目标等待队列再被唤醒的开销，提高了性能。该函数必须在持有源和目标哈希桶锁的情况下调用，以保证操作的原子性。\n\n### 同步原语\n- **`atomic_try_cmpxchg`**: 用于无锁地更新 `requeue_state`，实现状态机的原子转换。\n- **`rcuwait`**: 在 `PREEMPT_RT` 下，当等待者需要等待重排队完成时，使用 `rcuwait` 机制进行高效等待。\n- **`atomic_cond_read_relaxed`**: 在非 `PREEMPT_RT` 下，用于自旋等待状态改变。\n\n## 4. 依赖关系\n\n- **`<linux/plist.h>`**: 提供优先级链表（`plist`）的实现，用于管理 futex 等待队列。\n- **`<linux/sched/signal.h>`**: 提供任务唤醒（`wake_up_state`）和信号处理相关的功能。\n- **`\"futex.h\"`**: 包含 FUTEX 子系统的核心定义，如 `futex_q`, `futex_hash_bucket`, `futex_key` 等。\n- **`../locking/rtmutex_common.h`**: 提供实时互斥锁（`rtmutex`）的通用接口，`PREEMPT_RT` 的关键依赖。\n- **内核调度器**: 依赖 `wake_up_state` 等函数与内核调度器交互，唤醒被阻塞的任务。\n- **内存屏障原语**: 代码中隐含使用了 `acquire`/`release` 语义的原子操作来保证内存访问顺序。\n\n## 5. 使用场景\n\n- **`FUTEX_REQUEUE` 系统调用**: 当用户空间调用 `futex(uaddr1, FUTEX_REQUEUE, ...)` 时，内核会调用此文件中的逻辑，将等待在 `uaddr1` 上的指定数量的任务移动到 `uaddr2` 的等待队列上。\n- **`FUTEX_CMP_REQUEUE_PI` 系统调用**: 这是 PI futex 的关键操作。它首先检查 `uaddr1` 的值，如果匹配，则尝试将 `uaddr1` 上的等待者重排队到 `uaddr2`。在此过程中，会尝试为 `uaddr2` 的 top waiter 原子地获取锁（通过 `futex_proxy_trylock_atomic`）。如果成功，则直接唤醒该任务（通过 `requeue_pi_wake_futex`）；如果失败，则将其加入 `uaddr2` 的等待队列。整个过程需要本文件提供的状态机来处理并发唤醒。\n- **PI futex 的唤醒与超时处理**: 当一个正在被重排队的 PI futex 等待者收到信号或发生超时时，会调用 `futex_requeue_pi_wakeup_sync` 来安全地退出等待，并与正在进行的重排队操作进行协调。",
      "similarity": 0.5141016840934753,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "kernel/futex/requeue.c",
          "start_line": 695,
          "end_line": 862,
          "content": [
            "static inline",
            "int handle_early_requeue_pi_wakeup(struct futex_hash_bucket *hb,",
            "\t\t\t\t   struct futex_q *q,",
            "\t\t\t\t   struct hrtimer_sleeper *timeout)",
            "{",
            "\tint ret;",
            "",
            "\t/*",
            "\t * With the hb lock held, we avoid races while we process the wakeup.",
            "\t * We only need to hold hb (and not hb2) to ensure atomicity as the",
            "\t * wakeup code can't change q.key from uaddr to uaddr2 if we hold hb.",
            "\t * It can't be requeued from uaddr2 to something else since we don't",
            "\t * support a PI aware source futex for requeue.",
            "\t */",
            "\tWARN_ON_ONCE(&hb->lock != q->lock_ptr);",
            "",
            "\t/*",
            "\t * We were woken prior to requeue by a timeout or a signal.",
            "\t * Unqueue the futex_q and determine which it was.",
            "\t */",
            "\tplist_del(&q->list, &hb->chain);",
            "\tfutex_hb_waiters_dec(hb);",
            "",
            "\t/* Handle spurious wakeups gracefully */",
            "\tret = -EWOULDBLOCK;",
            "\tif (timeout && !timeout->task)",
            "\t\tret = -ETIMEDOUT;",
            "\telse if (signal_pending(current))",
            "\t\tret = -ERESTARTNOINTR;",
            "\treturn ret;",
            "}",
            "int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,",
            "\t\t\t  u32 val, ktime_t *abs_time, u32 bitset,",
            "\t\t\t  u32 __user *uaddr2)",
            "{",
            "\tstruct hrtimer_sleeper timeout, *to;",
            "\tstruct rt_mutex_waiter rt_waiter;",
            "\tstruct futex_hash_bucket *hb;",
            "\tunion futex_key key2 = FUTEX_KEY_INIT;",
            "\tstruct futex_q q = futex_q_init;",
            "\tstruct rt_mutex_base *pi_mutex;",
            "\tint res, ret;",
            "",
            "\tif (!IS_ENABLED(CONFIG_FUTEX_PI))",
            "\t\treturn -ENOSYS;",
            "",
            "\tif (uaddr == uaddr2)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!bitset)",
            "\t\treturn -EINVAL;",
            "",
            "\tto = futex_setup_timer(abs_time, &timeout, flags,",
            "\t\t\t       current->timer_slack_ns);",
            "",
            "\t/*",
            "\t * The waiter is allocated on our stack, manipulated by the requeue",
            "\t * code while we sleep on uaddr.",
            "\t */",
            "\trt_mutex_init_waiter(&rt_waiter);",
            "",
            "\tret = get_futex_key(uaddr2, flags, &key2, FUTEX_WRITE);",
            "\tif (unlikely(ret != 0))",
            "\t\tgoto out;",
            "",
            "\tq.bitset = bitset;",
            "\tq.rt_waiter = &rt_waiter;",
            "\tq.requeue_pi_key = &key2;",
            "",
            "\t/*",
            "\t * Prepare to wait on uaddr. On success, it holds hb->lock and q",
            "\t * is initialized.",
            "\t */",
            "\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);",
            "\tif (ret)",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * The check above which compares uaddrs is not sufficient for",
            "\t * shared futexes. We need to compare the keys:",
            "\t */",
            "\tif (futex_match(&q.key, &key2)) {",
            "\t\tfutex_q_unlock(hb);",
            "\t\tret = -EINVAL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* Queue the futex_q, drop the hb lock, wait for wakeup. */",
            "\tfutex_wait_queue(hb, &q, to);",
            "",
            "\tswitch (futex_requeue_pi_wakeup_sync(&q)) {",
            "\tcase Q_REQUEUE_PI_IGNORE:",
            "\t\t/* The waiter is still on uaddr1 */",
            "\t\tspin_lock(&hb->lock);",
            "\t\tret = handle_early_requeue_pi_wakeup(hb, &q, to);",
            "\t\tspin_unlock(&hb->lock);",
            "\t\tbreak;",
            "",
            "\tcase Q_REQUEUE_PI_LOCKED:",
            "\t\t/* The requeue acquired the lock */",
            "\t\tif (q.pi_state && (q.pi_state->owner != current)) {",
            "\t\t\tspin_lock(q.lock_ptr);",
            "\t\t\tret = fixup_pi_owner(uaddr2, &q, true);",
            "\t\t\t/*",
            "\t\t\t * Drop the reference to the pi state which the",
            "\t\t\t * requeue_pi() code acquired for us.",
            "\t\t\t */",
            "\t\t\tput_pi_state(q.pi_state);",
            "\t\t\tspin_unlock(q.lock_ptr);",
            "\t\t\t/*",
            "\t\t\t * Adjust the return value. It's either -EFAULT or",
            "\t\t\t * success (1) but the caller expects 0 for success.",
            "\t\t\t */",
            "\t\t\tret = ret < 0 ? ret : 0;",
            "\t\t}",
            "\t\tbreak;",
            "",
            "\tcase Q_REQUEUE_PI_DONE:",
            "\t\t/* Requeue completed. Current is 'pi_blocked_on' the rtmutex */",
            "\t\tpi_mutex = &q.pi_state->pi_mutex;",
            "\t\tret = rt_mutex_wait_proxy_lock(pi_mutex, to, &rt_waiter);",
            "",
            "\t\t/*",
            "\t\t * See futex_unlock_pi()'s cleanup: comment.",
            "\t\t */",
            "\t\tif (ret && !rt_mutex_cleanup_proxy_lock(pi_mutex, &rt_waiter))",
            "\t\t\tret = 0;",
            "",
            "\t\tspin_lock(q.lock_ptr);",
            "\t\tdebug_rt_mutex_free_waiter(&rt_waiter);",
            "\t\t/*",
            "\t\t * Fixup the pi_state owner and possibly acquire the lock if we",
            "\t\t * haven't already.",
            "\t\t */",
            "\t\tres = fixup_pi_owner(uaddr2, &q, !ret);",
            "\t\t/*",
            "\t\t * If fixup_pi_owner() returned an error, propagate that.  If it",
            "\t\t * acquired the lock, clear -ETIMEDOUT or -EINTR.",
            "\t\t */",
            "\t\tif (res)",
            "\t\t\tret = (res < 0) ? res : 0;",
            "",
            "\t\tfutex_unqueue_pi(&q);",
            "\t\tspin_unlock(q.lock_ptr);",
            "",
            "\t\tif (ret == -EINTR) {",
            "\t\t\t/*",
            "\t\t\t * We've already been requeued, but cannot restart",
            "\t\t\t * by calling futex_lock_pi() directly. We could",
            "\t\t\t * restart this syscall, but it would detect that",
            "\t\t\t * the user space \"val\" changed and return",
            "\t\t\t * -EWOULDBLOCK.  Save the overhead of the restart",
            "\t\t\t * and return -EWOULDBLOCK directly.",
            "\t\t\t */",
            "\t\t\tret = -EWOULDBLOCK;",
            "\t\t}",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "",
            "out:",
            "\tif (to) {",
            "\t\thrtimer_cancel(&to->timer);",
            "\t\tdestroy_hrtimer_on_stack(&to->timer);",
            "\t}",
            "\treturn ret;",
            "}"
          ],
          "function_name": "handle_early_requeue_pi_wakeup, futex_wait_requeue_pi",
          "description": "处理基于优先级继承的等待超时或信号唤醒场景，通过锁保护确保状态转换正确性，并执行必要的清理和错误恢复操作。",
          "similarity": 0.5399850606918335
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/futex/requeue.c",
          "start_line": 1,
          "end_line": 73,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "",
            "#include <linux/plist.h>",
            "#include <linux/sched/signal.h>",
            "",
            "#include \"futex.h\"",
            "#include \"../locking/rtmutex_common.h\"",
            "",
            "/*",
            " * On PREEMPT_RT, the hash bucket lock is a 'sleeping' spinlock with an",
            " * underlying rtmutex. The task which is about to be requeued could have",
            " * just woken up (timeout, signal). After the wake up the task has to",
            " * acquire hash bucket lock, which is held by the requeue code.  As a task",
            " * can only be blocked on _ONE_ rtmutex at a time, the proxy lock blocking",
            " * and the hash bucket lock blocking would collide and corrupt state.",
            " *",
            " * On !PREEMPT_RT this is not a problem and everything could be serialized",
            " * on hash bucket lock, but aside of having the benefit of common code,",
            " * this allows to avoid doing the requeue when the task is already on the",
            " * way out and taking the hash bucket lock of the original uaddr1 when the",
            " * requeue has been completed.",
            " *",
            " * The following state transitions are valid:",
            " *",
            " * On the waiter side:",
            " *   Q_REQUEUE_PI_NONE\t\t-> Q_REQUEUE_PI_IGNORE",
            " *   Q_REQUEUE_PI_IN_PROGRESS\t-> Q_REQUEUE_PI_WAIT",
            " *",
            " * On the requeue side:",
            " *   Q_REQUEUE_PI_NONE\t\t-> Q_REQUEUE_PI_INPROGRESS",
            " *   Q_REQUEUE_PI_IN_PROGRESS\t-> Q_REQUEUE_PI_DONE/LOCKED",
            " *   Q_REQUEUE_PI_IN_PROGRESS\t-> Q_REQUEUE_PI_NONE (requeue failed)",
            " *   Q_REQUEUE_PI_WAIT\t\t-> Q_REQUEUE_PI_DONE/LOCKED",
            " *   Q_REQUEUE_PI_WAIT\t\t-> Q_REQUEUE_PI_IGNORE (requeue failed)",
            " *",
            " * The requeue side ignores a waiter with state Q_REQUEUE_PI_IGNORE as this",
            " * signals that the waiter is already on the way out. It also means that",
            " * the waiter is still on the 'wait' futex, i.e. uaddr1.",
            " *",
            " * The waiter side signals early wakeup to the requeue side either through",
            " * setting state to Q_REQUEUE_PI_IGNORE or to Q_REQUEUE_PI_WAIT depending",
            " * on the current state. In case of Q_REQUEUE_PI_IGNORE it can immediately",
            " * proceed to take the hash bucket lock of uaddr1. If it set state to WAIT,",
            " * which means the wakeup is interleaving with a requeue in progress it has",
            " * to wait for the requeue side to change the state. Either to DONE/LOCKED",
            " * or to IGNORE. DONE/LOCKED means the waiter q is now on the uaddr2 futex",
            " * and either blocked (DONE) or has acquired it (LOCKED). IGNORE is set by",
            " * the requeue side when the requeue attempt failed via deadlock detection",
            " * and therefore the waiter q is still on the uaddr1 futex.",
            " */",
            "enum {",
            "\tQ_REQUEUE_PI_NONE\t\t=  0,",
            "\tQ_REQUEUE_PI_IGNORE,",
            "\tQ_REQUEUE_PI_IN_PROGRESS,",
            "\tQ_REQUEUE_PI_WAIT,",
            "\tQ_REQUEUE_PI_DONE,",
            "\tQ_REQUEUE_PI_LOCKED,",
            "};",
            "",
            "const struct futex_q futex_q_init = {",
            "\t/* list gets initialized in futex_queue()*/",
            "\t.key\t\t= FUTEX_KEY_INIT,",
            "\t.bitset\t\t= FUTEX_BITSET_MATCH_ANY,",
            "\t.requeue_state\t= ATOMIC_INIT(Q_REQUEUE_PI_NONE),",
            "};",
            "",
            "/**",
            " * requeue_futex() - Requeue a futex_q from one hb to another",
            " * @q:\t\tthe futex_q to requeue",
            " * @hb1:\tthe source hash_bucket",
            " * @hb2:\tthe target hash_bucket",
            " * @key2:\tthe new key for the requeued futex_q",
            " */"
          ],
          "function_name": null,
          "description": "定义FUTEX_REQUEUE_PI状态常量并初始化futex_q结构体，用于管理基于优先级继承的futex重新排队操作的状态机。",
          "similarity": 0.5325427055358887
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/futex/requeue.c",
          "start_line": 74,
          "end_line": 190,
          "content": [
            "static inline",
            "void requeue_futex(struct futex_q *q, struct futex_hash_bucket *hb1,",
            "\t\t   struct futex_hash_bucket *hb2, union futex_key *key2)",
            "{",
            "",
            "\t/*",
            "\t * If key1 and key2 hash to the same bucket, no need to",
            "\t * requeue.",
            "\t */",
            "\tif (likely(&hb1->chain != &hb2->chain)) {",
            "\t\tplist_del(&q->list, &hb1->chain);",
            "\t\tfutex_hb_waiters_dec(hb1);",
            "\t\tfutex_hb_waiters_inc(hb2);",
            "\t\tplist_add(&q->list, &hb2->chain);",
            "\t\tq->lock_ptr = &hb2->lock;",
            "\t}",
            "\tq->key = *key2;",
            "}",
            "static inline bool futex_requeue_pi_prepare(struct futex_q *q,",
            "\t\t\t\t\t    struct futex_pi_state *pi_state)",
            "{",
            "\tint old, new;",
            "",
            "\t/*",
            "\t * Set state to Q_REQUEUE_PI_IN_PROGRESS unless an early wakeup has",
            "\t * already set Q_REQUEUE_PI_IGNORE to signal that requeue should",
            "\t * ignore the waiter.",
            "\t */",
            "\told = atomic_read_acquire(&q->requeue_state);",
            "\tdo {",
            "\t\tif (old == Q_REQUEUE_PI_IGNORE)",
            "\t\t\treturn false;",
            "",
            "\t\t/*",
            "\t\t * futex_proxy_trylock_atomic() might have set it to",
            "\t\t * IN_PROGRESS and a interleaved early wake to WAIT.",
            "\t\t *",
            "\t\t * It was considered to have an extra state for that",
            "\t\t * trylock, but that would just add more conditionals",
            "\t\t * all over the place for a dubious value.",
            "\t\t */",
            "\t\tif (old != Q_REQUEUE_PI_NONE)",
            "\t\t\tbreak;",
            "",
            "\t\tnew = Q_REQUEUE_PI_IN_PROGRESS;",
            "\t} while (!atomic_try_cmpxchg(&q->requeue_state, &old, new));",
            "",
            "\tq->pi_state = pi_state;",
            "\treturn true;",
            "}",
            "static inline void futex_requeue_pi_complete(struct futex_q *q, int locked)",
            "{",
            "\tint old, new;",
            "",
            "\told = atomic_read_acquire(&q->requeue_state);",
            "\tdo {",
            "\t\tif (old == Q_REQUEUE_PI_IGNORE)",
            "\t\t\treturn;",
            "",
            "\t\tif (locked >= 0) {",
            "\t\t\t/* Requeue succeeded. Set DONE or LOCKED */",
            "\t\t\tWARN_ON_ONCE(old != Q_REQUEUE_PI_IN_PROGRESS &&",
            "\t\t\t\t     old != Q_REQUEUE_PI_WAIT);",
            "\t\t\tnew = Q_REQUEUE_PI_DONE + locked;",
            "\t\t} else if (old == Q_REQUEUE_PI_IN_PROGRESS) {",
            "\t\t\t/* Deadlock, no early wakeup interleave */",
            "\t\t\tnew = Q_REQUEUE_PI_NONE;",
            "\t\t} else {",
            "\t\t\t/* Deadlock, early wakeup interleave. */",
            "\t\t\tWARN_ON_ONCE(old != Q_REQUEUE_PI_WAIT);",
            "\t\t\tnew = Q_REQUEUE_PI_IGNORE;",
            "\t\t}",
            "\t} while (!atomic_try_cmpxchg(&q->requeue_state, &old, new));",
            "",
            "#ifdef CONFIG_PREEMPT_RT",
            "\t/* If the waiter interleaved with the requeue let it know */",
            "\tif (unlikely(old == Q_REQUEUE_PI_WAIT))",
            "\t\trcuwait_wake_up(&q->requeue_wait);",
            "#endif",
            "}",
            "static inline int futex_requeue_pi_wakeup_sync(struct futex_q *q)",
            "{",
            "\tint old, new;",
            "",
            "\told = atomic_read_acquire(&q->requeue_state);",
            "\tdo {",
            "\t\t/* Is requeue done already? */",
            "\t\tif (old >= Q_REQUEUE_PI_DONE)",
            "\t\t\treturn old;",
            "",
            "\t\t/*",
            "\t\t * If not done, then tell the requeue code to either ignore",
            "\t\t * the waiter or to wake it up once the requeue is done.",
            "\t\t */",
            "\t\tnew = Q_REQUEUE_PI_WAIT;",
            "\t\tif (old == Q_REQUEUE_PI_NONE)",
            "\t\t\tnew = Q_REQUEUE_PI_IGNORE;",
            "\t} while (!atomic_try_cmpxchg(&q->requeue_state, &old, new));",
            "",
            "\t/* If the requeue was in progress, wait for it to complete */",
            "\tif (old == Q_REQUEUE_PI_IN_PROGRESS) {",
            "#ifdef CONFIG_PREEMPT_RT",
            "\t\trcuwait_wait_event(&q->requeue_wait,",
            "\t\t\t\t   atomic_read(&q->requeue_state) != Q_REQUEUE_PI_WAIT,",
            "\t\t\t\t   TASK_UNINTERRUPTIBLE);",
            "#else",
            "\t\t(void)atomic_cond_read_relaxed(&q->requeue_state, VAL != Q_REQUEUE_PI_WAIT);",
            "#endif",
            "\t}",
            "",
            "\t/*",
            "\t * Requeue is now either prohibited or complete. Reread state",
            "\t * because during the wait above it might have changed. Nothing",
            "\t * will modify q->requeue_state after this point.",
            "\t */",
            "\treturn atomic_read(&q->requeue_state);",
            "}"
          ],
          "function_name": "requeue_futex, futex_requeue_pi_prepare, futex_requeue_pi_complete, futex_requeue_pi_wakeup_sync",
          "description": "实现futex重新排队的核心逻辑，通过原子操作管理状态转换，处理等待队列迁移、锁竞争检测及状态同步问题。",
          "similarity": 0.5124051570892334
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/futex/requeue.c",
          "start_line": 223,
          "end_line": 635,
          "content": [
            "static inline",
            "void requeue_pi_wake_futex(struct futex_q *q, union futex_key *key,",
            "\t\t\t   struct futex_hash_bucket *hb)",
            "{",
            "\tstruct task_struct *task;",
            "",
            "\tq->key = *key;",
            "\t__futex_unqueue(q);",
            "",
            "\tWARN_ON(!q->rt_waiter);",
            "\tq->rt_waiter = NULL;",
            "",
            "\tq->lock_ptr = &hb->lock;",
            "\ttask = READ_ONCE(q->task);",
            "",
            "\t/* Signal locked state to the waiter */",
            "\tfutex_requeue_pi_complete(q, 1);",
            "\twake_up_state(task, TASK_NORMAL);",
            "}",
            "static int",
            "futex_proxy_trylock_atomic(u32 __user *pifutex, struct futex_hash_bucket *hb1,",
            "\t\t\t   struct futex_hash_bucket *hb2, union futex_key *key1,",
            "\t\t\t   union futex_key *key2, struct futex_pi_state **ps,",
            "\t\t\t   struct task_struct **exiting, int set_waiters)",
            "{",
            "\tstruct futex_q *top_waiter = NULL;",
            "\tu32 curval;",
            "\tint ret;",
            "",
            "\tif (futex_get_value_locked(&curval, pifutex))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (unlikely(should_fail_futex(true)))",
            "\t\treturn -EFAULT;",
            "",
            "\t/*",
            "\t * Find the top_waiter and determine if there are additional waiters.",
            "\t * If the caller intends to requeue more than 1 waiter to pifutex,",
            "\t * force futex_lock_pi_atomic() to set the FUTEX_WAITERS bit now,",
            "\t * as we have means to handle the possible fault.  If not, don't set",
            "\t * the bit unnecessarily as it will force the subsequent unlock to enter",
            "\t * the kernel.",
            "\t */",
            "\ttop_waiter = futex_top_waiter(hb1, key1);",
            "",
            "\t/* There are no waiters, nothing for us to do. */",
            "\tif (!top_waiter)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Ensure that this is a waiter sitting in futex_wait_requeue_pi()",
            "\t * and waiting on the 'waitqueue' futex which is always !PI.",
            "\t */",
            "\tif (!top_waiter->rt_waiter || top_waiter->pi_state)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Ensure we requeue to the expected futex. */",
            "\tif (!futex_match(top_waiter->requeue_pi_key, key2))",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Ensure that this does not race against an early wakeup */",
            "\tif (!futex_requeue_pi_prepare(top_waiter, NULL))",
            "\t\treturn -EAGAIN;",
            "",
            "\t/*",
            "\t * Try to take the lock for top_waiter and set the FUTEX_WAITERS bit",
            "\t * in the contended case or if @set_waiters is true.",
            "\t *",
            "\t * In the contended case PI state is attached to the lock owner. If",
            "\t * the user space lock can be acquired then PI state is attached to",
            "\t * the new owner (@top_waiter->task) when @set_waiters is true.",
            "\t */",
            "\tret = futex_lock_pi_atomic(pifutex, hb2, key2, ps, top_waiter->task,",
            "\t\t\t\t   exiting, set_waiters);",
            "\tif (ret == 1) {",
            "\t\t/*",
            "\t\t * Lock was acquired in user space and PI state was",
            "\t\t * attached to @top_waiter->task. That means state is fully",
            "\t\t * consistent and the waiter can return to user space",
            "\t\t * immediately after the wakeup.",
            "\t\t */",
            "\t\trequeue_pi_wake_futex(top_waiter, key2, hb2);",
            "\t} else if (ret < 0) {",
            "\t\t/* Rewind top_waiter::requeue_state */",
            "\t\tfutex_requeue_pi_complete(top_waiter, ret);",
            "\t} else {",
            "\t\t/*",
            "\t\t * futex_lock_pi_atomic() did not acquire the user space",
            "\t\t * futex, but managed to establish the proxy lock and pi",
            "\t\t * state. top_waiter::requeue_state cannot be fixed up here",
            "\t\t * because the waiter is not enqueued on the rtmutex",
            "\t\t * yet. This is handled at the callsite depending on the",
            "\t\t * result of rt_mutex_start_proxy_lock() which is",
            "\t\t * guaranteed to be reached with this function returning 0.",
            "\t\t */",
            "\t}",
            "\treturn ret;",
            "}",
            "int futex_requeue(u32 __user *uaddr1, unsigned int flags1,",
            "\t\t  u32 __user *uaddr2, unsigned int flags2,",
            "\t\t  int nr_wake, int nr_requeue, u32 *cmpval, int requeue_pi)",
            "{",
            "\tunion futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;",
            "\tint task_count = 0, ret;",
            "\tstruct futex_pi_state *pi_state = NULL;",
            "\tstruct futex_hash_bucket *hb1, *hb2;",
            "\tstruct futex_q *this, *next;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "",
            "\tif (nr_wake < 0 || nr_requeue < 0)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * When PI not supported: return -ENOSYS if requeue_pi is true,",
            "\t * consequently the compiler knows requeue_pi is always false past",
            "\t * this point which will optimize away all the conditional code",
            "\t * further down.",
            "\t */",
            "\tif (!IS_ENABLED(CONFIG_FUTEX_PI) && requeue_pi)",
            "\t\treturn -ENOSYS;",
            "",
            "\tif (requeue_pi) {",
            "\t\t/*",
            "\t\t * Requeue PI only works on two distinct uaddrs. This",
            "\t\t * check is only valid for private futexes. See below.",
            "\t\t */",
            "\t\tif (uaddr1 == uaddr2)",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\t/*",
            "\t\t * futex_requeue() allows the caller to define the number",
            "\t\t * of waiters to wake up via the @nr_wake argument. With",
            "\t\t * REQUEUE_PI, waking up more than one waiter is creating",
            "\t\t * more problems than it solves. Waking up a waiter makes",
            "\t\t * only sense if the PI futex @uaddr2 is uncontended as",
            "\t\t * this allows the requeue code to acquire the futex",
            "\t\t * @uaddr2 before waking the waiter. The waiter can then",
            "\t\t * return to user space without further action. A secondary",
            "\t\t * wakeup would just make the futex_wait_requeue_pi()",
            "\t\t * handling more complex, because that code would have to",
            "\t\t * look up pi_state and do more or less all the handling",
            "\t\t * which the requeue code has to do for the to be requeued",
            "\t\t * waiters. So restrict the number of waiters to wake to",
            "\t\t * one, and only wake it up when the PI futex is",
            "\t\t * uncontended. Otherwise requeue it and let the unlock of",
            "\t\t * the PI futex handle the wakeup.",
            "\t\t *",
            "\t\t * All REQUEUE_PI users, e.g. pthread_cond_signal() and",
            "\t\t * pthread_cond_broadcast() must use nr_wake=1.",
            "\t\t */",
            "\t\tif (nr_wake != 1)",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\t/*",
            "\t\t * requeue_pi requires a pi_state, try to allocate it now",
            "\t\t * without any locks in case it fails.",
            "\t\t */",
            "\t\tif (refill_pi_state_cache())",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "retry:",
            "\tret = get_futex_key(uaddr1, flags1, &key1, FUTEX_READ);",
            "\tif (unlikely(ret != 0))",
            "\t\treturn ret;",
            "\tret = get_futex_key(uaddr2, flags2, &key2,",
            "\t\t\t    requeue_pi ? FUTEX_WRITE : FUTEX_READ);",
            "\tif (unlikely(ret != 0))",
            "\t\treturn ret;",
            "",
            "\t/*",
            "\t * The check above which compares uaddrs is not sufficient for",
            "\t * shared futexes. We need to compare the keys:",
            "\t */",
            "\tif (requeue_pi && futex_match(&key1, &key2))",
            "\t\treturn -EINVAL;",
            "",
            "\thb1 = futex_hash(&key1);",
            "\thb2 = futex_hash(&key2);",
            "",
            "retry_private:",
            "\tfutex_hb_waiters_inc(hb2);",
            "\tdouble_lock_hb(hb1, hb2);",
            "",
            "\tif (likely(cmpval != NULL)) {",
            "\t\tu32 curval;",
            "",
            "\t\tret = futex_get_value_locked(&curval, uaddr1);",
            "",
            "\t\tif (unlikely(ret)) {",
            "\t\t\tdouble_unlock_hb(hb1, hb2);",
            "\t\t\tfutex_hb_waiters_dec(hb2);",
            "",
            "\t\t\tret = get_user(curval, uaddr1);",
            "\t\t\tif (ret)",
            "\t\t\t\treturn ret;",
            "",
            "\t\t\tif (!(flags1 & FLAGS_SHARED))",
            "\t\t\t\tgoto retry_private;",
            "",
            "\t\t\tgoto retry;",
            "\t\t}",
            "\t\tif (curval != *cmpval) {",
            "\t\t\tret = -EAGAIN;",
            "\t\t\tgoto out_unlock;",
            "\t\t}",
            "\t}",
            "",
            "\tif (requeue_pi) {",
            "\t\tstruct task_struct *exiting = NULL;",
            "",
            "\t\t/*",
            "\t\t * Attempt to acquire uaddr2 and wake the top waiter. If we",
            "\t\t * intend to requeue waiters, force setting the FUTEX_WAITERS",
            "\t\t * bit.  We force this here where we are able to easily handle",
            "\t\t * faults rather in the requeue loop below.",
            "\t\t *",
            "\t\t * Updates topwaiter::requeue_state if a top waiter exists.",
            "\t\t */",
            "\t\tret = futex_proxy_trylock_atomic(uaddr2, hb1, hb2, &key1,",
            "\t\t\t\t\t\t &key2, &pi_state,",
            "\t\t\t\t\t\t &exiting, nr_requeue);",
            "",
            "\t\t/*",
            "\t\t * At this point the top_waiter has either taken uaddr2 or",
            "\t\t * is waiting on it. In both cases pi_state has been",
            "\t\t * established and an initial refcount on it. In case of an",
            "\t\t * error there's nothing.",
            "\t\t *",
            "\t\t * The top waiter's requeue_state is up to date:",
            "\t\t *",
            "\t\t *  - If the lock was acquired atomically (ret == 1), then",
            "\t\t *    the state is Q_REQUEUE_PI_LOCKED.",
            "\t\t *",
            "\t\t *    The top waiter has been dequeued and woken up and can",
            "\t\t *    return to user space immediately. The kernel/user",
            "\t\t *    space state is consistent. In case that there must be",
            "\t\t *    more waiters requeued the WAITERS bit in the user",
            "\t\t *    space futex is set so the top waiter task has to go",
            "\t\t *    into the syscall slowpath to unlock the futex. This",
            "\t\t *    will block until this requeue operation has been",
            "\t\t *    completed and the hash bucket locks have been",
            "\t\t *    dropped.",
            "\t\t *",
            "\t\t *  - If the trylock failed with an error (ret < 0) then",
            "\t\t *    the state is either Q_REQUEUE_PI_NONE, i.e. \"nothing",
            "\t\t *    happened\", or Q_REQUEUE_PI_IGNORE when there was an",
            "\t\t *    interleaved early wakeup.",
            "\t\t *",
            "\t\t *  - If the trylock did not succeed (ret == 0) then the",
            "\t\t *    state is either Q_REQUEUE_PI_IN_PROGRESS or",
            "\t\t *    Q_REQUEUE_PI_WAIT if an early wakeup interleaved.",
            "\t\t *    This will be cleaned up in the loop below, which",
            "\t\t *    cannot fail because futex_proxy_trylock_atomic() did",
            "\t\t *    the same sanity checks for requeue_pi as the loop",
            "\t\t *    below does.",
            "\t\t */",
            "\t\tswitch (ret) {",
            "\t\tcase 0:",
            "\t\t\t/* We hold a reference on the pi state. */",
            "\t\t\tbreak;",
            "",
            "\t\tcase 1:",
            "\t\t\t/*",
            "\t\t\t * futex_proxy_trylock_atomic() acquired the user space",
            "\t\t\t * futex. Adjust task_count.",
            "\t\t\t */",
            "\t\t\ttask_count++;",
            "\t\t\tret = 0;",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * If the above failed, then pi_state is NULL and",
            "\t\t * waiter::requeue_state is correct.",
            "\t\t */",
            "\t\tcase -EFAULT:",
            "\t\t\tdouble_unlock_hb(hb1, hb2);",
            "\t\t\tfutex_hb_waiters_dec(hb2);",
            "\t\t\tret = fault_in_user_writeable(uaddr2);",
            "\t\t\tif (!ret)",
            "\t\t\t\tgoto retry;",
            "\t\t\treturn ret;",
            "\t\tcase -EBUSY:",
            "\t\tcase -EAGAIN:",
            "\t\t\t/*",
            "\t\t\t * Two reasons for this:",
            "\t\t\t * - EBUSY: Owner is exiting and we just wait for the",
            "\t\t\t *   exit to complete.",
            "\t\t\t * - EAGAIN: The user space value changed.",
            "\t\t\t */",
            "\t\t\tdouble_unlock_hb(hb1, hb2);",
            "\t\t\tfutex_hb_waiters_dec(hb2);",
            "\t\t\t/*",
            "\t\t\t * Handle the case where the owner is in the middle of",
            "\t\t\t * exiting. Wait for the exit to complete otherwise",
            "\t\t\t * this task might loop forever, aka. live lock.",
            "\t\t\t */",
            "\t\t\twait_for_owner_exiting(ret, exiting);",
            "\t\t\tcond_resched();",
            "\t\t\tgoto retry;",
            "\t\tdefault:",
            "\t\t\tgoto out_unlock;",
            "\t\t}",
            "\t}",
            "",
            "\tplist_for_each_entry_safe(this, next, &hb1->chain, list) {",
            "\t\tif (task_count - nr_wake >= nr_requeue)",
            "\t\t\tbreak;",
            "",
            "\t\tif (!futex_match(&this->key, &key1))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * FUTEX_WAIT_REQUEUE_PI and FUTEX_CMP_REQUEUE_PI should always",
            "\t\t * be paired with each other and no other futex ops.",
            "\t\t *",
            "\t\t * We should never be requeueing a futex_q with a pi_state,",
            "\t\t * which is awaiting a futex_unlock_pi().",
            "\t\t */",
            "\t\tif ((requeue_pi && !this->rt_waiter) ||",
            "\t\t    (!requeue_pi && this->rt_waiter) ||",
            "\t\t    this->pi_state) {",
            "\t\t\tret = -EINVAL;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t/* Plain futexes just wake or requeue and are done */",
            "\t\tif (!requeue_pi) {",
            "\t\t\tif (++task_count <= nr_wake)",
            "\t\t\t\tfutex_wake_mark(&wake_q, this);",
            "\t\t\telse",
            "\t\t\t\trequeue_futex(this, hb1, hb2, &key2);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/* Ensure we requeue to the expected futex for requeue_pi. */",
            "\t\tif (!futex_match(this->requeue_pi_key, &key2)) {",
            "\t\t\tret = -EINVAL;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Requeue nr_requeue waiters and possibly one more in the case",
            "\t\t * of requeue_pi if we couldn't acquire the lock atomically.",
            "\t\t *",
            "\t\t * Prepare the waiter to take the rt_mutex. Take a refcount",
            "\t\t * on the pi_state and store the pointer in the futex_q",
            "\t\t * object of the waiter.",
            "\t\t */",
            "\t\tget_pi_state(pi_state);",
            "",
            "\t\t/* Don't requeue when the waiter is already on the way out. */",
            "\t\tif (!futex_requeue_pi_prepare(this, pi_state)) {",
            "\t\t\t/*",
            "\t\t\t * Early woken waiter signaled that it is on the",
            "\t\t\t * way out. Drop the pi_state reference and try the",
            "\t\t\t * next waiter. @this->pi_state is still NULL.",
            "\t\t\t */",
            "\t\t\tput_pi_state(pi_state);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tret = rt_mutex_start_proxy_lock(&pi_state->pi_mutex,",
            "\t\t\t\t\t\tthis->rt_waiter,",
            "\t\t\t\t\t\tthis->task);",
            "",
            "\t\tif (ret == 1) {",
            "\t\t\t/*",
            "\t\t\t * We got the lock. We do neither drop the refcount",
            "\t\t\t * on pi_state nor clear this->pi_state because the",
            "\t\t\t * waiter needs the pi_state for cleaning up the",
            "\t\t\t * user space value. It will drop the refcount",
            "\t\t\t * after doing so. this::requeue_state is updated",
            "\t\t\t * in the wakeup as well.",
            "\t\t\t */",
            "\t\t\trequeue_pi_wake_futex(this, &key2, hb2);",
            "\t\t\ttask_count++;",
            "\t\t} else if (!ret) {",
            "\t\t\t/* Waiter is queued, move it to hb2 */",
            "\t\t\trequeue_futex(this, hb1, hb2, &key2);",
            "\t\t\tfutex_requeue_pi_complete(this, 0);",
            "\t\t\ttask_count++;",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * rt_mutex_start_proxy_lock() detected a potential",
            "\t\t\t * deadlock when we tried to queue that waiter.",
            "\t\t\t * Drop the pi_state reference which we took above",
            "\t\t\t * and remove the pointer to the state from the",
            "\t\t\t * waiters futex_q object.",
            "\t\t\t */",
            "\t\t\tthis->pi_state = NULL;",
            "\t\t\tput_pi_state(pi_state);",
            "\t\t\tfutex_requeue_pi_complete(this, ret);",
            "\t\t\t/*",
            "\t\t\t * We stop queueing more waiters and let user space",
            "\t\t\t * deal with the mess.",
            "\t\t\t */",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * We took an extra initial reference to the pi_state in",
            "\t * futex_proxy_trylock_atomic(). We need to drop it here again.",
            "\t */",
            "\tput_pi_state(pi_state);",
            "",
            "out_unlock:",
            "\tdouble_unlock_hb(hb1, hb2);",
            "\twake_up_q(&wake_q);",
            "\tfutex_hb_waiters_dec(hb2);",
            "\treturn ret ? ret : task_count;",
            "}"
          ],
          "function_name": "requeue_pi_wake_futex, futex_proxy_trylock_atomic, futex_requeue",
          "description": "实现futex_requeue函数，负责将等待队列从源哈希桶迁移到目标哈希桶，支持PI模式下的锁抢占与状态一致性维护。",
          "similarity": 0.4979212284088135
        }
      ]
    }
  ]
}