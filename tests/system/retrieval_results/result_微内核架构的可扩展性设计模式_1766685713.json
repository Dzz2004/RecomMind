{
  "query": "微内核架构的可扩展性设计模式",
  "timestamp": "2025-12-26 02:01:53",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.5610678195953369,
      "chunks": [
        {
          "chunk_id": 11,
          "file_path": "mm/mempolicy.c",
          "start_line": 1855,
          "end_line": 1971,
          "content": [
            "static int kernel_get_mempolicy(int __user *policy,",
            "\t\t\t\tunsigned long __user *nmask,",
            "\t\t\t\tunsigned long maxnode,",
            "\t\t\t\tunsigned long addr,",
            "\t\t\t\tunsigned long flags)",
            "{",
            "\tint err;",
            "\tint pval;",
            "\tnodemask_t nodes;",
            "",
            "\tif (nmask != NULL && maxnode < nr_node_ids)",
            "\t\treturn -EINVAL;",
            "",
            "\taddr = untagged_addr(addr);",
            "",
            "\terr = do_get_mempolicy(&pval, &nodes, addr, flags);",
            "",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (policy && put_user(pval, policy))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (nmask)",
            "\t\terr = copy_nodes_to_user(nmask, maxnode, &nodes);",
            "",
            "\treturn err;",
            "}",
            "bool vma_migratable(struct vm_area_struct *vma)",
            "{",
            "\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * DAX device mappings require predictable access latency, so avoid",
            "\t * incurring periodic faults.",
            "\t */",
            "\tif (vma_is_dax(vma))",
            "\t\treturn false;",
            "",
            "\tif (is_vm_hugetlb_page(vma) &&",
            "\t\t!hugepage_migration_supported(hstate_vma(vma)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Migration allocates pages in the highest zone. If we cannot",
            "\t * do so then migration (at least from node to node) is not",
            "\t * possible.",
            "\t */",
            "\tif (vma->vm_file &&",
            "\t\tgfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))",
            "\t\t\t< policy_zone)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "bool vma_policy_mof(struct vm_area_struct *vma)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->get_policy) {",
            "\t\tbool ret = false;",
            "\t\tpgoff_t ilx;\t\t/* ignored here */",
            "",
            "\t\tpol = vma->vm_ops->get_policy(vma, vma->vm_start, &ilx);",
            "\t\tif (pol && (pol->flags & MPOL_F_MOF))",
            "\t\t\tret = true;",
            "\t\tmpol_cond_put(pol);",
            "",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tpol = vma->vm_policy;",
            "\tif (!pol)",
            "\t\tpol = get_task_policy(current);",
            "",
            "\treturn pol->flags & MPOL_F_MOF;",
            "}",
            "bool apply_policy_zone(struct mempolicy *policy, enum zone_type zone)",
            "{",
            "\tenum zone_type dynamic_policy_zone = policy_zone;",
            "",
            "\tBUG_ON(dynamic_policy_zone == ZONE_MOVABLE);",
            "",
            "\t/*",
            "\t * if policy->nodes has movable memory only,",
            "\t * we apply policy when gfp_zone(gfp) = ZONE_MOVABLE only.",
            "\t *",
            "\t * policy->nodes is intersect with node_states[N_MEMORY].",
            "\t * so if the following test fails, it implies",
            "\t * policy->nodes has movable memory only.",
            "\t */",
            "\tif (!nodes_intersects(policy->nodes, node_states[N_HIGH_MEMORY]))",
            "\t\tdynamic_policy_zone = ZONE_MOVABLE;",
            "",
            "\treturn zone >= dynamic_policy_zone;",
            "}",
            "static unsigned int weighted_interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int node;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "retry:",
            "\t/* to prevent miscount use tsk->mems_allowed_seq to detect rebind */",
            "\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\tnode = current->il_prev;",
            "\tif (!current->il_weight || !node_isset(node, policy->nodes)) {",
            "\t\tnode = next_node_in(node, policy->nodes);",
            "\t\tif (read_mems_allowed_retry(cpuset_mems_cookie))",
            "\t\t\tgoto retry;",
            "\t\tif (node == MAX_NUMNODES)",
            "\t\t\treturn node;",
            "\t\tcurrent->il_prev = node;",
            "\t\tcurrent->il_weight = get_il_weight(node);",
            "\t}",
            "\tcurrent->il_weight--;",
            "\treturn node;",
            "}"
          ],
          "function_name": "kernel_get_mempolicy, vma_migratable, vma_policy_mof, apply_policy_zone, weighted_interleave_nodes",
          "description": "kernel_get_mempolicy 获取当前内存策略参数并复制到用户空间；vma_migratable 判断虚拟内存区域是否支持迁移；vma_policy_mof 检查VMA是否启用了MOF（Migration On Fault）策略；apply_policy_zone 确定当前zone是否满足策略要求；weighted_interleave_nodes 计算加权交错分配的目标节点。",
          "similarity": 0.5347496271133423
        },
        {
          "chunk_id": 19,
          "file_path": "mm/mempolicy.c",
          "start_line": 3268,
          "end_line": 3394,
          "content": [
            "void numa_default_policy(void)",
            "{",
            "\tdo_set_mempolicy(MPOL_DEFAULT, 0, NULL);",
            "}",
            "int mpol_parse_str(char *str, struct mempolicy **mpol)",
            "{",
            "\tstruct mempolicy *new = NULL;",
            "\tunsigned short mode_flags;",
            "\tnodemask_t nodes;",
            "\tchar *nodelist = strchr(str, ':');",
            "\tchar *flags = strchr(str, '=');",
            "\tint err = 1, mode;",
            "",
            "\tif (flags)",
            "\t\t*flags++ = '\\0';\t/* terminate mode string */",
            "",
            "\tif (nodelist) {",
            "\t\t/* NUL-terminate mode or flags string */",
            "\t\t*nodelist++ = '\\0';",
            "\t\tif (nodelist_parse(nodelist, nodes))",
            "\t\t\tgoto out;",
            "\t\tif (!nodes_subset(nodes, node_states[N_MEMORY]))",
            "\t\t\tgoto out;",
            "\t} else",
            "\t\tnodes_clear(nodes);",
            "",
            "\tmode = match_string(policy_modes, MPOL_MAX, str);",
            "\tif (mode < 0)",
            "\t\tgoto out;",
            "",
            "\tswitch (mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\t/*",
            "\t\t * Insist on a nodelist of one node only, although later",
            "\t\t * we use first_node(nodes) to grab a single node, so here",
            "\t\t * nodelist (or nodes) cannot be empty.",
            "\t\t */",
            "\t\tif (nodelist) {",
            "\t\t\tchar *rest = nodelist;",
            "\t\t\twhile (isdigit(*rest))",
            "\t\t\t\trest++;",
            "\t\t\tif (*rest)",
            "\t\t\t\tgoto out;",
            "\t\t\tif (nodes_empty(nodes))",
            "\t\t\t\tgoto out;",
            "\t\t}",
            "\t\tbreak;",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t/*",
            "\t\t * Default to online nodes with memory if no nodelist",
            "\t\t */",
            "\t\tif (!nodelist)",
            "\t\t\tnodes = node_states[N_MEMORY];",
            "\t\tbreak;",
            "\tcase MPOL_LOCAL:",
            "\t\t/*",
            "\t\t * Don't allow a nodelist;  mpol_new() checks flags",
            "\t\t */",
            "\t\tif (nodelist)",
            "\t\t\tgoto out;",
            "\t\tbreak;",
            "\tcase MPOL_DEFAULT:",
            "\t\t/*",
            "\t\t * Insist on a empty nodelist",
            "\t\t */",
            "\t\tif (!nodelist)",
            "\t\t\terr = 0;",
            "\t\tgoto out;",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\t\t/*",
            "\t\t * Insist on a nodelist",
            "\t\t */",
            "\t\tif (!nodelist)",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tmode_flags = 0;",
            "\tif (flags) {",
            "\t\t/*",
            "\t\t * Currently, we only support two mutually exclusive",
            "\t\t * mode flags.",
            "\t\t */",
            "\t\tif (!strcmp(flags, \"static\"))",
            "\t\t\tmode_flags |= MPOL_F_STATIC_NODES;",
            "\t\telse if (!strcmp(flags, \"relative\"))",
            "\t\t\tmode_flags |= MPOL_F_RELATIVE_NODES;",
            "\t\telse",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tnew = mpol_new(mode, mode_flags, &nodes);",
            "\tif (IS_ERR(new))",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * Save nodes for mpol_to_str() to show the tmpfs mount options",
            "\t * for /proc/mounts, /proc/pid/mounts and /proc/pid/mountinfo.",
            "\t */",
            "\tif (mode != MPOL_PREFERRED) {",
            "\t\tnew->nodes = nodes;",
            "\t} else if (nodelist) {",
            "\t\tnodes_clear(new->nodes);",
            "\t\tnode_set(first_node(nodes), new->nodes);",
            "\t} else {",
            "\t\tnew->mode = MPOL_LOCAL;",
            "\t}",
            "",
            "\t/*",
            "\t * Save nodes for contextualization: this will be used to \"clone\"",
            "\t * the mempolicy in a specific context [cpuset] at a later time.",
            "\t */",
            "\tnew->w.user_nodemask = nodes;",
            "",
            "\terr = 0;",
            "",
            "out:",
            "\t/* Restore string for error message */",
            "\tif (nodelist)",
            "\t\t*--nodelist = ':';",
            "\tif (flags)",
            "\t\t*--flags = '=';",
            "\tif (!err)",
            "\t\t*mpol = new;",
            "\treturn err;",
            "}"
          ],
          "function_name": "numa_default_policy, mpol_parse_str",
          "description": "numa_default_policy设置系统默认内存策略为默认模式，mpol_parse_str解析内存策略字符串，根据模式类型处理节点列表和标志位，构建对应的内存策略结构体。",
          "similarity": 0.5268122553825378
        },
        {
          "chunk_id": 20,
          "file_path": "mm/mempolicy.c",
          "start_line": 3434,
          "end_line": 3538,
          "content": [
            "void mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol)",
            "{",
            "\tchar *p = buffer;",
            "\tnodemask_t nodes = NODE_MASK_NONE;",
            "\tunsigned short mode = MPOL_DEFAULT;",
            "\tunsigned short flags = 0;",
            "",
            "\tif (pol &&",
            "\t    pol != &default_policy &&",
            "\t    !(pol >= &preferred_node_policy[0] &&",
            "\t      pol <= &preferred_node_policy[ARRAY_SIZE(preferred_node_policy) - 1])) {",
            "\t\tmode = pol->mode;",
            "\t\tflags = pol->flags;",
            "\t}",
            "",
            "\tswitch (mode) {",
            "\tcase MPOL_DEFAULT:",
            "\tcase MPOL_LOCAL:",
            "\t\tbreak;",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\tnodes = pol->nodes;",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(1);",
            "\t\tsnprintf(p, maxlen, \"unknown\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tp += snprintf(p, maxlen, \"%s\", policy_modes[mode]);",
            "",
            "\tif (flags & MPOL_MODE_FLAGS) {",
            "\t\tp += snprintf(p, buffer + maxlen - p, \"=\");",
            "",
            "\t\t/*",
            "\t\t * Static and relative are mutually exclusive.",
            "\t\t */",
            "\t\tif (flags & MPOL_F_STATIC_NODES)",
            "\t\t\tp += snprintf(p, buffer + maxlen - p, \"static\");",
            "\t\telse if (flags & MPOL_F_RELATIVE_NODES)",
            "\t\t\tp += snprintf(p, buffer + maxlen - p, \"relative\");",
            "",
            "\t\tif (flags & MPOL_F_NUMA_BALANCING) {",
            "\t\t\tif (!is_power_of_2(flags & MPOL_MODE_FLAGS))",
            "\t\t\t\tp += snprintf(p, buffer + maxlen - p, \"|\");",
            "\t\t\tp += snprintf(p, buffer + maxlen - p, \"balancing\");",
            "\t\t}",
            "\t}",
            "",
            "\tif (!nodes_empty(nodes))",
            "\t\tp += scnprintf(p, buffer + maxlen - p, \":%*pbl\",",
            "\t\t\t       nodemask_pr_args(&nodes));",
            "}",
            "static ssize_t node_show(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\t char *buf)",
            "{",
            "\tstruct iw_node_attr *node_attr;",
            "\tu8 weight;",
            "",
            "\tnode_attr = container_of(attr, struct iw_node_attr, kobj_attr);",
            "\tweight = get_il_weight(node_attr->nid);",
            "\treturn sysfs_emit(buf, \"%d\\n\", weight);",
            "}",
            "static ssize_t node_store(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\t  const char *buf, size_t count)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tstruct iw_node_attr *node_attr;",
            "\tu8 weight = 0;",
            "\tint i;",
            "",
            "\tnode_attr = container_of(attr, struct iw_node_attr, kobj_attr);",
            "\tif (count == 0 || sysfs_streq(buf, \"\") ||",
            "\t    kstrtou8(buf, 0, &weight) || weight == 0)",
            "\t\treturn -EINVAL;",
            "",
            "\tnew_wi_state = kzalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state)",
            "\t\treturn -ENOMEM;",
            "",
            "\tmutex_lock(&wi_state_lock);",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state) {",
            "\t\tmemcpy(new_wi_state->iw_table, old_wi_state->iw_table,",
            "\t\t\t\t\tnr_node_ids * sizeof(u8));",
            "\t} else {",
            "\t\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\t\tnew_wi_state->iw_table[i] = 1;",
            "\t}",
            "\tnew_wi_state->iw_table[node_attr->nid] = weight;",
            "\tnew_wi_state->mode_auto = false;",
            "",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "\treturn count;",
            "}"
          ],
          "function_name": "mpol_to_str, node_show, node_store",
          "description": "mpol_to_str 将内存策略结构体转换为可打印字符串，识别策略模式和节点掩码信息。node_show 和 node_store 分别实现 sysfs 属性的读取和写入逻辑，用于动态配置加权交错节点权重。",
          "similarity": 0.5247005820274353
        },
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.5228536128997803
        },
        {
          "chunk_id": 12,
          "file_path": "mm/mempolicy.c",
          "start_line": 2024,
          "end_line": 2135,
          "content": [
            "static unsigned int interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int nid;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "\t/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnid = next_node_in(current->il_prev, policy->nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\tif (nid < MAX_NUMNODES)",
            "\t\tcurrent->il_prev = nid;",
            "\treturn nid;",
            "}",
            "unsigned int mempolicy_slab_node(void)",
            "{",
            "\tstruct mempolicy *policy;",
            "\tint node = numa_mem_id();",
            "",
            "\tif (!in_task())",
            "\t\treturn node;",
            "",
            "\tpolicy = current->mempolicy;",
            "\tif (!policy)",
            "\t\treturn node;",
            "",
            "\tswitch (policy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\treturn first_node(policy->nodes);",
            "",
            "\tcase MPOL_INTERLEAVE:",
            "\t\treturn interleave_nodes(policy);",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn weighted_interleave_nodes(policy);",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t{",
            "\t\tstruct zoneref *z;",
            "",
            "\t\t/*",
            "\t\t * Follow bind policy behavior and start allocation at the",
            "\t\t * first node.",
            "\t\t */",
            "\t\tstruct zonelist *zonelist;",
            "\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);",
            "\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];",
            "\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,",
            "\t\t\t\t\t\t\t&policy->nodes);",
            "\t\treturn z->zone ? zone_to_nid(z->zone) : node;",
            "\t}",
            "\tcase MPOL_LOCAL:",
            "\t\treturn node;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static unsigned int read_once_policy_nodemask(struct mempolicy *pol,",
            "\t\t\t\t\t      nodemask_t *mask)",
            "{",
            "\t/*",
            "\t * barrier stabilizes the nodemask locally so that it can be iterated",
            "\t * over safely without concern for changes. Allocators validate node",
            "\t * selection does not violate mems_allowed, so this is safe.",
            "\t */",
            "\tbarrier();",
            "\tmemcpy(mask, &pol->nodes, sizeof(nodemask_t));",
            "\tbarrier();",
            "\treturn nodes_weight(*mask);",
            "}",
            "static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nr_nodes;",
            "\tu8 *table = NULL;",
            "\tunsigned int weight_total = 0;",
            "\tu8 weight;",
            "\tint nid = 0;",
            "",
            "\tnr_nodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nr_nodes)",
            "\t\treturn numa_node_id();",
            "",
            "\trcu_read_lock();",
            "",
            "\tstate = rcu_dereference(wi_state);",
            "\t/* Uninitialized wi_state means we should assume all weights are 1 */",
            "\tif (state)",
            "\t\ttable = state->iw_table;",
            "",
            "\t/* calculate the total weight */",
            "\tfor_each_node_mask(nid, nodemask)",
            "\t\tweight_total += table ? table[nid] : 1;",
            "",
            "\t/* Calculate the node offset based on totals */",
            "\ttarget = ilx % weight_total;",
            "\tnid = first_node(nodemask);",
            "\twhile (target) {",
            "\t\t/* detect system default usage */",
            "\t\tweight = table ? table[nid] : 1;",
            "\t\tif (target < weight)",
            "\t\t\tbreak;",
            "\t\ttarget -= weight;",
            "\t\tnid = next_node_in(nid, nodemask);",
            "\t}",
            "\trcu_read_unlock();",
            "\treturn nid;",
            "}"
          ],
          "function_name": "interleave_nodes, mempolicy_slab_node, read_once_policy_nodemask, weighted_interleave_nid",
          "description": "interleave_nodes 计算交错分配的下一个节点；mempolicy_slab_node 根据内存策略返回Slab分配的节点；read_once_policy_nodemask 安全读取策略节点掩码；weighted_interleave_nid 基于权重计算加权交错分配的目标节点。",
          "similarity": 0.5182292461395264
        }
      ]
    },
    {
      "source_file": "mm/memblock.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:38:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memblock.c`\n\n---\n\n# memblock.c 技术文档\n\n## 1. 文件概述\n\n`memblock.c` 实现了 Linux 内核早期启动阶段的内存管理机制——**memblock**。该机制用于在常规内存分配器（如 buddy allocator）尚未初始化之前，对物理内存进行粗粒度的区域管理。它将系统内存抽象为若干连续的内存区域（regions），支持“可用内存”（memory）、“保留内存”（reserved）和“物理内存”（physmem，部分架构支持）三种类型，为内核早期初始化提供内存添加、查询和分配能力。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct memblock_region`：表示一个连续的物理内存区域，包含基地址（base）、大小（size）、NUMA 节点 ID 和属性标志。\n- `struct memblock_type`：管理一类内存区域的集合，包含区域数组、当前数量（cnt）、最大容量（max）和名称。\n- `struct memblock`：全局 memblock 管理结构，包含 `memory` 和 `reserved` 两种类型的 `memblock_type`，以及分配方向（bottom_up）和当前分配上限（current_limit）。\n- `physmem`（条件编译）：描述不受 `mem=` 参数限制的实际物理内存布局。\n\n### 主要函数与变量\n- `memblock_add()` / `memblock_add_node()`：向 memblock 添加可用内存区域。\n- `memblock_reserve()`：标记内存区域为保留（不可用于动态分配）。\n- `memblock_phys_alloc*()` / `memblock_alloc*()`：分配物理或虚拟地址的内存。\n- `memblock_overlaps_region()`：判断指定区域是否与某类 memblock 区域重叠。\n- `__memblock_find_range_bottom_up()`：从低地址向高地址查找满足条件的空闲内存范围。\n- 全局变量 `memblock`：静态初始化的主 memblock 结构体。\n- `max_low_pfn`, `min_low_pfn`, `max_pfn`, `max_possible_pfn`：记录 PFN（页帧号）边界信息。\n\n### 配置宏\n- `INIT_MEMBLOCK_REGIONS`：初始内存/保留区域数组大小（默认 128）。\n- `CONFIG_HAVE_MEMBLOCK_PHYS_MAP`：启用 `physmem` 类型支持。\n- `CONFIG_MEMBLOCK_KHO_SCRATCH`：支持仅从特定标记（KHO_SCRATCH）区域分配内存。\n- `CONFIG_ARCH_KEEP_MEMBLOCK`：决定是否在初始化完成后保留 memblock 数据结构。\n\n## 3. 关键实现\n\n### 初始化与存储\n- `memblock` 结构体在编译时静态初始化，其 `memory` 和 `reserved` 的区域数组分别使用 `memblock_memory_init_regions` 和 `memblock_reserved_init_regions`，初始容量由 `INIT_MEMBLOCK_*_REGIONS` 定义。\n- 每个 `memblock_type` 的 `cnt` 初始设为 1，但实际第一个条目为空的占位符，有效区域从索引 1 开始（后续代码处理）。\n- 支持通过 `memblock_allow_resize()` 动态扩容区域数组，但需谨慎避免与 initrd 等关键区域冲突。\n\n### 内存区域管理\n- 使用 `for_each_memblock_type` 宏遍历指定类型的区域。\n- `memblock_addrs_overlap()` 通过比较区间端点判断两个物理内存区域是否重叠。\n- `memblock_overlaps_region()` 封装了对某类所有区域的重叠检测。\n\n### 分配策略\n- 默认采用 **top-down**（从高地址向低地址）分配策略，可通过 `memblock_set_bottom_up(true)` 切换为 **bottom-up**。\n- 分配时受 `current_limit` 限制（默认 `MEMBLOCK_ALLOC_ANYWHERE` 表示无限制）。\n- 支持基于 NUMA 节点、对齐要求、内存属性（如 `MEMBLOCK_MIRROR`、`MEMBLOCK_KHO_SCRATCH`）的精细控制。\n- `choose_memblock_flags()` 根据 `kho_scratch_only` 和镜像内存存在性动态选择分配标志。\n\n### 安全与调试\n- `memblock_cap_size()` 防止地址计算溢出（确保 `base + size <= PHYS_ADDR_MAX`）。\n- 条件编译的 `memblock_dbg()` 宏用于调试输出（需开启 `memblock_debug`）。\n- 使用 `__initdata_memblock` 属性标记仅在初始化阶段使用的数据，便于后续释放。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/memblock.h>`：定义 memblock API 和数据结构。\n  - `<linux/kernel.h>`, `<linux/init.h>`：提供基础内核功能和初始化宏。\n  - `<linux/pfn.h>`：PFN 相关操作。\n  - `<asm/sections.h>`：访问内核链接段信息。\n  - 架构相关头文件（如 `internal.h`）。\n- **配置依赖**：\n  - `CONFIG_NUMA`：影响 `contig_page_data` 的定义。\n  - `CONFIG_KEXEC_HANDOVER`：引入 kexec 相关头文件。\n  - `CONFIG_HAVE_MEMBLOCK_PHYS_MAP`：启用 `physmem` 支持。\n- **后续移交**：在 `mem_init()` 中，memblock 管理的内存会被释放给 buddy allocator，完成内存管理权移交。\n\n## 5. 使用场景\n\n- **内核早期初始化**：在 `start_kernel()` 初期，架构代码（如 `setup_arch()`）调用 `memblock_add()` 注册可用物理内存，调用 `memblock_reserve()` 保留内核镜像、设备树、initrd 等关键区域。\n- **早期内存分配**：在 slab/buddy 分配器就绪前，使用 `memblock_alloc()` 分配大块连续内存（如页表、中断向量表、ACPI 表解析缓冲区）。\n- **内存布局查询**：通过 `for_each_memblock()` 等宏遍历内存区域，用于构建 e820 表、EFI 内存映射或 NUMA 拓扑。\n- **特殊分配需求**：支持从镜像内存（`MEMBLOCK_MIRROR`）或 KHO scratch 区域分配，满足安全启动或崩溃转储等场景。\n- **调试与分析**：通过 debugfs 接口（未在片段中体现）导出 memblock 布局，辅助内存问题诊断。",
      "similarity": 0.5599526762962341,
      "chunks": [
        {
          "chunk_id": 12,
          "file_path": "mm/memblock.c",
          "start_line": 2233,
          "end_line": 2340,
          "content": [
            "static void __init __free_pages_memory(unsigned long start, unsigned long end)",
            "{",
            "\tint order;",
            "",
            "\twhile (start < end) {",
            "\t\t/*",
            "\t\t * Free the pages in the largest chunks alignment allows.",
            "\t\t *",
            "\t\t * __ffs() behaviour is undefined for 0. start == 0 is",
            "\t\t * MAX_PAGE_ORDER-aligned, set order to MAX_PAGE_ORDER for",
            "\t\t * the case.",
            "\t\t */",
            "\t\tif (start)",
            "\t\t\torder = min_t(int, MAX_PAGE_ORDER, __ffs(start));",
            "\t\telse",
            "\t\t\torder = MAX_PAGE_ORDER;",
            "",
            "\t\twhile (start + (1UL << order) > end)",
            "\t\t\torder--;",
            "",
            "\t\tmemblock_free_pages(pfn_to_page(start), start, order);",
            "",
            "\t\tstart += (1UL << order);",
            "\t}",
            "}",
            "static unsigned long __init __free_memory_core(phys_addr_t start,",
            "\t\t\t\t phys_addr_t end)",
            "{",
            "\tunsigned long start_pfn = PFN_UP(start);",
            "\tunsigned long end_pfn = min_t(unsigned long,",
            "\t\t\t\t      PFN_DOWN(end), max_low_pfn);",
            "",
            "\tif (start_pfn >= end_pfn)",
            "\t\treturn 0;",
            "",
            "\t__free_pages_memory(start_pfn, end_pfn);",
            "",
            "\treturn end_pfn - start_pfn;",
            "}",
            "static void __init memmap_init_reserved_pages(void)",
            "{",
            "\tstruct memblock_region *region;",
            "\tphys_addr_t start, end;",
            "\tint nid;",
            "\tunsigned long max_reserved;",
            "",
            "\t/*",
            "\t * set nid on all reserved pages and also treat struct",
            "\t * pages for the NOMAP regions as PageReserved",
            "\t */",
            "repeat:",
            "\tmax_reserved = memblock.reserved.max;",
            "\tfor_each_mem_region(region) {",
            "\t\tnid = memblock_get_region_node(region);",
            "\t\tstart = region->base;",
            "\t\tend = start + region->size;",
            "",
            "\t\tif (memblock_is_nomap(region))",
            "\t\t\treserve_bootmem_region(start, end, nid);",
            "",
            "\t\tmemblock_set_node(start, region->size, &memblock.reserved, nid);",
            "\t}",
            "\t/*",
            "\t * 'max' is changed means memblock.reserved has been doubled its",
            "\t * array, which may result a new reserved region before current",
            "\t * 'start'. Now we should repeat the procedure to set its node id.",
            "\t */",
            "\tif (max_reserved != memblock.reserved.max)",
            "\t\tgoto repeat;",
            "",
            "\t/*",
            "\t * initialize struct pages for reserved regions that don't have",
            "\t * the MEMBLOCK_RSRV_NOINIT flag set",
            "\t */",
            "\tfor_each_reserved_mem_region(region) {",
            "\t\tif (!memblock_is_reserved_noinit(region)) {",
            "\t\t\tnid = memblock_get_region_node(region);",
            "\t\t\tstart = region->base;",
            "\t\t\tend = start + region->size;",
            "",
            "\t\t\tif (!numa_valid_node(nid))",
            "\t\t\t\tnid = early_pfn_to_nid(PFN_DOWN(start));",
            "",
            "\t\t\treserve_bootmem_region(start, end, nid);",
            "\t\t}",
            "\t}",
            "}",
            "static unsigned long __init free_low_memory_core_early(void)",
            "{",
            "\tunsigned long count = 0;",
            "\tphys_addr_t start, end;",
            "\tu64 i;",
            "",
            "\tmemblock_clear_hotplug(0, -1);",
            "",
            "\tmemmap_init_reserved_pages();",
            "",
            "\t/*",
            "\t * We need to use NUMA_NO_NODE instead of NODE_DATA(0)->node_id",
            "\t *  because in some case like Node0 doesn't have RAM installed",
            "\t *  low ram will be on Node1",
            "\t */",
            "\tfor_each_free_mem_range(i, NUMA_NO_NODE, MEMBLOCK_NONE, &start, &end,",
            "\t\t\t\tNULL)",
            "\t\tcount += __free_memory_core(start, end);",
            "",
            "\treturn count;",
            "}"
          ],
          "function_name": "__free_pages_memory, __free_memory_core, memmap_init_reserved_pages, free_low_memory_core_early",
          "description": "核心实现内存页面释放逻辑，初始化保留区域页结构并处理低内存核心区域的提前释放操作",
          "similarity": 0.5830858945846558
        },
        {
          "chunk_id": 1,
          "file_path": "mm/memblock.c",
          "start_line": 192,
          "end_line": 297,
          "content": [
            "static inline phys_addr_t memblock_cap_size(phys_addr_t base, phys_addr_t *size)",
            "{",
            "\treturn *size = min(*size, PHYS_ADDR_MAX - base);",
            "}",
            "unsigned long __init_memblock",
            "memblock_addrs_overlap(phys_addr_t base1, phys_addr_t size1, phys_addr_t base2,",
            "\t\t       phys_addr_t size2)",
            "{",
            "\treturn ((base1 < (base2 + size2)) && (base2 < (base1 + size1)));",
            "}",
            "bool __init_memblock memblock_overlaps_region(struct memblock_type *type,",
            "\t\t\t\t\tphys_addr_t base, phys_addr_t size)",
            "{",
            "\tunsigned long i;",
            "",
            "\tmemblock_cap_size(base, &size);",
            "",
            "\tfor (i = 0; i < type->cnt; i++)",
            "\t\tif (memblock_addrs_overlap(base, size, type->regions[i].base,",
            "\t\t\t\t\t   type->regions[i].size))",
            "\t\t\tbreak;",
            "\treturn i < type->cnt;",
            "}",
            "static phys_addr_t __init_memblock",
            "__memblock_find_range_bottom_up(phys_addr_t start, phys_addr_t end,",
            "\t\t\t\tphys_addr_t size, phys_addr_t align, int nid,",
            "\t\t\t\tenum memblock_flags flags)",
            "{",
            "\tphys_addr_t this_start, this_end, cand;",
            "\tu64 i;",
            "",
            "\tfor_each_free_mem_range(i, nid, flags, &this_start, &this_end, NULL) {",
            "\t\tthis_start = clamp(this_start, start, end);",
            "\t\tthis_end = clamp(this_end, start, end);",
            "",
            "\t\tcand = round_up(this_start, align);",
            "\t\tif (cand < this_end && this_end - cand >= size)",
            "\t\t\treturn cand;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static phys_addr_t __init_memblock",
            "__memblock_find_range_top_down(phys_addr_t start, phys_addr_t end,",
            "\t\t\t       phys_addr_t size, phys_addr_t align, int nid,",
            "\t\t\t       enum memblock_flags flags)",
            "{",
            "\tphys_addr_t this_start, this_end, cand;",
            "\tu64 i;",
            "",
            "\tfor_each_free_mem_range_reverse(i, nid, flags, &this_start, &this_end,",
            "\t\t\t\t\tNULL) {",
            "\t\tthis_start = clamp(this_start, start, end);",
            "\t\tthis_end = clamp(this_end, start, end);",
            "",
            "\t\tif (this_end < size)",
            "\t\t\tcontinue;",
            "",
            "\t\tcand = round_down(this_end - size, align);",
            "\t\tif (cand >= this_start)",
            "\t\t\treturn cand;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static phys_addr_t __init_memblock memblock_find_in_range_node(phys_addr_t size,",
            "\t\t\t\t\tphys_addr_t align, phys_addr_t start,",
            "\t\t\t\t\tphys_addr_t end, int nid,",
            "\t\t\t\t\tenum memblock_flags flags)",
            "{",
            "\t/* pump up @end */",
            "\tif (end == MEMBLOCK_ALLOC_ACCESSIBLE ||",
            "\t    end == MEMBLOCK_ALLOC_NOLEAKTRACE)",
            "\t\tend = memblock.current_limit;",
            "",
            "\t/* avoid allocating the first page */",
            "\tstart = max_t(phys_addr_t, start, PAGE_SIZE);",
            "\tend = max(start, end);",
            "",
            "\tif (memblock_bottom_up())",
            "\t\treturn __memblock_find_range_bottom_up(start, end, size, align,",
            "\t\t\t\t\t\t       nid, flags);",
            "\telse",
            "\t\treturn __memblock_find_range_top_down(start, end, size, align,",
            "\t\t\t\t\t\t      nid, flags);",
            "}",
            "static phys_addr_t __init_memblock memblock_find_in_range(phys_addr_t start,",
            "\t\t\t\t\tphys_addr_t end, phys_addr_t size,",
            "\t\t\t\t\tphys_addr_t align)",
            "{",
            "\tphys_addr_t ret;",
            "\tenum memblock_flags flags = choose_memblock_flags();",
            "",
            "again:",
            "\tret = memblock_find_in_range_node(size, align, start, end,",
            "\t\t\t\t\t    NUMA_NO_NODE, flags);",
            "",
            "\tif (!ret && (flags & MEMBLOCK_MIRROR)) {",
            "\t\tpr_warn_ratelimited(\"Could not allocate %pap bytes of mirrored memory\\n\",",
            "\t\t\t&size);",
            "\t\tflags &= ~MEMBLOCK_MIRROR;",
            "\t\tgoto again;",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "memblock_cap_size, memblock_addrs_overlap, memblock_overlaps_region, __memblock_find_range_bottom_up, __memblock_find_range_top_down, memblock_find_in_range_node, memblock_find_in_range",
          "description": "实现内存区域地址重叠检测与分配策略选择逻辑，包含范围查找算法（底向顶/顶向底）及镜像内存分配失败回退机制。",
          "similarity": 0.573936939239502
        },
        {
          "chunk_id": 8,
          "file_path": "mm/memblock.c",
          "start_line": 1424,
          "end_line": 1548,
          "content": [
            "int __init_memblock memblock_set_node(phys_addr_t base, phys_addr_t size,",
            "\t\t\t\t      struct memblock_type *type, int nid)",
            "{",
            "#ifdef CONFIG_NUMA",
            "\tint start_rgn, end_rgn;",
            "\tint i, ret;",
            "",
            "\tret = memblock_isolate_range(type, base, size, &start_rgn, &end_rgn);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tfor (i = start_rgn; i < end_rgn; i++)",
            "\t\tmemblock_set_region_node(&type->regions[i], nid);",
            "",
            "\tmemblock_merge_regions(type, start_rgn, end_rgn);",
            "#endif",
            "\treturn 0;",
            "}",
            "void __init_memblock",
            "__next_mem_pfn_range_in_zone(u64 *idx, struct zone *zone,",
            "\t\t\t     unsigned long *out_spfn, unsigned long *out_epfn)",
            "{",
            "\tint zone_nid = zone_to_nid(zone);",
            "\tphys_addr_t spa, epa;",
            "",
            "\t__next_mem_range(idx, zone_nid, MEMBLOCK_NONE,",
            "\t\t\t &memblock.memory, &memblock.reserved,",
            "\t\t\t &spa, &epa, NULL);",
            "",
            "\twhile (*idx != U64_MAX) {",
            "\t\tunsigned long epfn = PFN_DOWN(epa);",
            "\t\tunsigned long spfn = PFN_UP(spa);",
            "",
            "\t\t/*",
            "\t\t * Verify the end is at least past the start of the zone and",
            "\t\t * that we have at least one PFN to initialize.",
            "\t\t */",
            "\t\tif (zone->zone_start_pfn < epfn && spfn < epfn) {",
            "\t\t\t/* if we went too far just stop searching */",
            "\t\t\tif (zone_end_pfn(zone) <= spfn) {",
            "\t\t\t\t*idx = U64_MAX;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "",
            "\t\t\tif (out_spfn)",
            "\t\t\t\t*out_spfn = max(zone->zone_start_pfn, spfn);",
            "\t\t\tif (out_epfn)",
            "\t\t\t\t*out_epfn = min(zone_end_pfn(zone), epfn);",
            "",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\t__next_mem_range(idx, zone_nid, MEMBLOCK_NONE,",
            "\t\t\t\t &memblock.memory, &memblock.reserved,",
            "\t\t\t\t &spa, &epa, NULL);",
            "\t}",
            "",
            "\t/* signal end of iteration */",
            "\tif (out_spfn)",
            "\t\t*out_spfn = ULONG_MAX;",
            "\tif (out_epfn)",
            "\t\t*out_epfn = 0;",
            "}",
            "phys_addr_t __init memblock_alloc_range_nid(phys_addr_t size,",
            "\t\t\t\t\tphys_addr_t align, phys_addr_t start,",
            "\t\t\t\t\tphys_addr_t end, int nid,",
            "\t\t\t\t\tbool exact_nid)",
            "{",
            "\tenum memblock_flags flags = choose_memblock_flags();",
            "\tphys_addr_t found;",
            "",
            "\tif (!align) {",
            "\t\t/* Can't use WARNs this early in boot on powerpc */",
            "\t\tdump_stack();",
            "\t\talign = SMP_CACHE_BYTES;",
            "\t}",
            "",
            "again:",
            "\tfound = memblock_find_in_range_node(size, align, start, end, nid,",
            "\t\t\t\t\t    flags);",
            "\tif (found && !__memblock_reserve(found, size, nid, MEMBLOCK_RSRV_KERN))",
            "\t\tgoto done;",
            "",
            "\tif (numa_valid_node(nid) && !exact_nid) {",
            "\t\tfound = memblock_find_in_range_node(size, align, start,",
            "\t\t\t\t\t\t    end, NUMA_NO_NODE,",
            "\t\t\t\t\t\t    flags);",
            "\t\tif (found && !memblock_reserve_kern(found, size))",
            "\t\t\tgoto done;",
            "\t}",
            "",
            "\tif (flags & MEMBLOCK_MIRROR) {",
            "\t\tflags &= ~MEMBLOCK_MIRROR;",
            "\t\tpr_warn_ratelimited(\"Could not allocate %pap bytes of mirrored memory\\n\",",
            "\t\t\t&size);",
            "\t\tgoto again;",
            "\t}",
            "",
            "\treturn 0;",
            "",
            "done:",
            "\t/*",
            "\t * Skip kmemleak for those places like kasan_init() and",
            "\t * early_pgtable_alloc() due to high volume.",
            "\t */",
            "\tif (end != MEMBLOCK_ALLOC_NOLEAKTRACE)",
            "\t\t/*",
            "\t\t * Memblock allocated blocks are never reported as",
            "\t\t * leaks. This is because many of these blocks are",
            "\t\t * only referred via the physical address which is",
            "\t\t * not looked up by kmemleak.",
            "\t\t */",
            "\t\tkmemleak_alloc_phys(found, size, 0);",
            "",
            "\t/*",
            "\t * Some Virtual Machine platforms, such as Intel TDX or AMD SEV-SNP,",
            "\t * require memory to be accepted before it can be used by the",
            "\t * guest.",
            "\t *",
            "\t * Accept the memory of the allocated buffer.",
            "\t */",
            "\taccept_memory(found, found + size);",
            "",
            "\treturn found;",
            "}"
          ],
          "function_name": "memblock_set_node, __next_mem_pfn_range_in_zone, memblock_alloc_range_nid",
          "description": "提供内存节点绑定、ZONE级PFN范围查询及带节点约束的内存分配接口，包含镜像内存分配重试机制和内存泄漏追踪处理",
          "similarity": 0.5496296286582947
        },
        {
          "chunk_id": 3,
          "file_path": "mm/memblock.c",
          "start_line": 537,
          "end_line": 685,
          "content": [
            "static void __init_memblock memblock_merge_regions(struct memblock_type *type,",
            "\t\t\t\t\t\t   unsigned long start_rgn,",
            "\t\t\t\t\t\t   unsigned long end_rgn)",
            "{",
            "\tint i = 0;",
            "\tif (start_rgn)",
            "\t\ti = start_rgn - 1;",
            "\tend_rgn = min(end_rgn, type->cnt - 1);",
            "\twhile (i < end_rgn) {",
            "\t\tstruct memblock_region *this = &type->regions[i];",
            "\t\tstruct memblock_region *next = &type->regions[i + 1];",
            "",
            "\t\tif (this->base + this->size != next->base ||",
            "\t\t    memblock_get_region_node(this) !=",
            "\t\t    memblock_get_region_node(next) ||",
            "\t\t    this->flags != next->flags) {",
            "\t\t\tBUG_ON(this->base + this->size > next->base);",
            "\t\t\ti++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tthis->size += next->size;",
            "\t\t/* move forward from next + 1, index of which is i + 2 */",
            "\t\tmemmove(next, next + 1, (type->cnt - (i + 2)) * sizeof(*next));",
            "\t\ttype->cnt--;",
            "\t\tend_rgn--;",
            "\t}",
            "}",
            "static void __init_memblock memblock_insert_region(struct memblock_type *type,",
            "\t\t\t\t\t\t   int idx, phys_addr_t base,",
            "\t\t\t\t\t\t   phys_addr_t size,",
            "\t\t\t\t\t\t   int nid,",
            "\t\t\t\t\t\t   enum memblock_flags flags)",
            "{",
            "\tstruct memblock_region *rgn = &type->regions[idx];",
            "",
            "\tBUG_ON(type->cnt >= type->max);",
            "\tmemmove(rgn + 1, rgn, (type->cnt - idx) * sizeof(*rgn));",
            "\trgn->base = base;",
            "\trgn->size = size;",
            "\trgn->flags = flags;",
            "\tmemblock_set_region_node(rgn, nid);",
            "\ttype->cnt++;",
            "\ttype->total_size += size;",
            "}",
            "static int __init_memblock memblock_add_range(struct memblock_type *type,",
            "\t\t\t\tphys_addr_t base, phys_addr_t size,",
            "\t\t\t\tint nid, enum memblock_flags flags)",
            "{",
            "\tbool insert = false;",
            "\tphys_addr_t obase = base;",
            "\tphys_addr_t end = base + memblock_cap_size(base, &size);",
            "\tint idx, nr_new, start_rgn = -1, end_rgn;",
            "\tstruct memblock_region *rgn;",
            "",
            "\tif (!size)",
            "\t\treturn 0;",
            "",
            "\t/* special case for empty array */",
            "\tif (type->regions[0].size == 0) {",
            "\t\tWARN_ON(type->cnt != 1 || type->total_size);",
            "\t\ttype->regions[0].base = base;",
            "\t\ttype->regions[0].size = size;",
            "\t\ttype->regions[0].flags = flags;",
            "\t\tmemblock_set_region_node(&type->regions[0], nid);",
            "\t\ttype->total_size = size;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/*",
            "\t * The worst case is when new range overlaps all existing regions,",
            "\t * then we'll need type->cnt + 1 empty regions in @type. So if",
            "\t * type->cnt * 2 + 1 is less than or equal to type->max, we know",
            "\t * that there is enough empty regions in @type, and we can insert",
            "\t * regions directly.",
            "\t */",
            "\tif (type->cnt * 2 + 1 <= type->max)",
            "\t\tinsert = true;",
            "",
            "repeat:",
            "\t/*",
            "\t * The following is executed twice.  Once with %false @insert and",
            "\t * then with %true.  The first counts the number of regions needed",
            "\t * to accommodate the new area.  The second actually inserts them.",
            "\t */",
            "\tbase = obase;",
            "\tnr_new = 0;",
            "",
            "\tfor_each_memblock_type(idx, type, rgn) {",
            "\t\tphys_addr_t rbase = rgn->base;",
            "\t\tphys_addr_t rend = rbase + rgn->size;",
            "",
            "\t\tif (rbase >= end)",
            "\t\t\tbreak;",
            "\t\tif (rend <= base)",
            "\t\t\tcontinue;",
            "\t\t/*",
            "\t\t * @rgn overlaps.  If it separates the lower part of new",
            "\t\t * area, insert that portion.",
            "\t\t */",
            "\t\tif (rbase > base) {",
            "#ifdef CONFIG_NUMA",
            "\t\t\tWARN_ON(nid != memblock_get_region_node(rgn));",
            "#endif",
            "\t\t\tWARN_ON(flags != MEMBLOCK_NONE && flags != rgn->flags);",
            "\t\t\tnr_new++;",
            "\t\t\tif (insert) {",
            "\t\t\t\tif (start_rgn == -1)",
            "\t\t\t\t\tstart_rgn = idx;",
            "\t\t\t\tend_rgn = idx + 1;",
            "\t\t\t\tmemblock_insert_region(type, idx++, base,",
            "\t\t\t\t\t\t       rbase - base, nid,",
            "\t\t\t\t\t\t       flags);",
            "\t\t\t}",
            "\t\t}",
            "\t\t/* area below @rend is dealt with, forget about it */",
            "\t\tbase = min(rend, end);",
            "\t}",
            "",
            "\t/* insert the remaining portion */",
            "\tif (base < end) {",
            "\t\tnr_new++;",
            "\t\tif (insert) {",
            "\t\t\tif (start_rgn == -1)",
            "\t\t\t\tstart_rgn = idx;",
            "\t\t\tend_rgn = idx + 1;",
            "\t\t\tmemblock_insert_region(type, idx, base, end - base,",
            "\t\t\t\t\t       nid, flags);",
            "\t\t}",
            "\t}",
            "",
            "\tif (!nr_new)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * If this was the first round, resize array and repeat for actual",
            "\t * insertions; otherwise, merge and return.",
            "\t */",
            "\tif (!insert) {",
            "\t\twhile (type->cnt + nr_new > type->max)",
            "\t\t\tif (memblock_double_array(type, obase, size) < 0)",
            "\t\t\t\treturn -ENOMEM;",
            "\t\tinsert = true;",
            "\t\tgoto repeat;",
            "\t} else {",
            "\t\tmemblock_merge_regions(type, start_rgn, end_rgn);",
            "\t\treturn 0;",
            "\t}",
            "}"
          ],
          "function_name": "memblock_merge_regions, memblock_insert_region, memblock_add_range",
          "description": "实现内存区域合并（merge_regions）与插入（insert_region）逻辑，处理新增内存范围的拆分与整合，优化连续区域管理。",
          "similarity": 0.5474545955657959
        },
        {
          "chunk_id": 2,
          "file_path": "mm/memblock.c",
          "start_line": 366,
          "end_line": 506,
          "content": [
            "static void __init_memblock memblock_remove_region(struct memblock_type *type, unsigned long r)",
            "{",
            "\ttype->total_size -= type->regions[r].size;",
            "\tmemmove(&type->regions[r], &type->regions[r + 1],",
            "\t\t(type->cnt - (r + 1)) * sizeof(type->regions[r]));",
            "\ttype->cnt--;",
            "",
            "\t/* Special case for empty arrays */",
            "\tif (type->cnt == 0) {",
            "\t\tWARN_ON(type->total_size != 0);",
            "\t\ttype->cnt = 1;",
            "\t\ttype->regions[0].base = 0;",
            "\t\ttype->regions[0].size = 0;",
            "\t\ttype->regions[0].flags = 0;",
            "\t\tmemblock_set_region_node(&type->regions[0], MAX_NUMNODES);",
            "\t}",
            "}",
            "void __init memblock_discard(void)",
            "{",
            "\tphys_addr_t addr, size;",
            "",
            "\tif (memblock.reserved.regions != memblock_reserved_init_regions) {",
            "\t\taddr = __pa(memblock.reserved.regions);",
            "\t\tsize = PAGE_ALIGN(sizeof(struct memblock_region) *",
            "\t\t\t\t  memblock.reserved.max);",
            "\t\tif (memblock_reserved_in_slab)",
            "\t\t\tkfree(memblock.reserved.regions);",
            "\t\telse",
            "\t\t\tmemblock_free_late(addr, size);",
            "\t}",
            "",
            "\tif (memblock.memory.regions != memblock_memory_init_regions) {",
            "\t\taddr = __pa(memblock.memory.regions);",
            "\t\tsize = PAGE_ALIGN(sizeof(struct memblock_region) *",
            "\t\t\t\t  memblock.memory.max);",
            "\t\tif (memblock_memory_in_slab)",
            "\t\t\tkfree(memblock.memory.regions);",
            "\t\telse",
            "\t\t\tmemblock_free_late(addr, size);",
            "\t}",
            "",
            "\tmemblock_memory = NULL;",
            "}",
            "static int __init_memblock memblock_double_array(struct memblock_type *type,",
            "\t\t\t\t\t\tphys_addr_t new_area_start,",
            "\t\t\t\t\t\tphys_addr_t new_area_size)",
            "{",
            "\tstruct memblock_region *new_array, *old_array;",
            "\tphys_addr_t old_alloc_size, new_alloc_size;",
            "\tphys_addr_t old_size, new_size, addr, new_end;",
            "\tint use_slab = slab_is_available();",
            "\tint *in_slab;",
            "",
            "\t/* We don't allow resizing until we know about the reserved regions",
            "\t * of memory that aren't suitable for allocation",
            "\t */",
            "\tif (!memblock_can_resize)",
            "\t\treturn -1;",
            "",
            "\t/* Calculate new doubled size */",
            "\told_size = type->max * sizeof(struct memblock_region);",
            "\tnew_size = old_size << 1;",
            "\t/*",
            "\t * We need to allocated new one align to PAGE_SIZE,",
            "\t *   so we can free them completely later.",
            "\t */",
            "\told_alloc_size = PAGE_ALIGN(old_size);",
            "\tnew_alloc_size = PAGE_ALIGN(new_size);",
            "",
            "\t/* Retrieve the slab flag */",
            "\tif (type == &memblock.memory)",
            "\t\tin_slab = &memblock_memory_in_slab;",
            "\telse",
            "\t\tin_slab = &memblock_reserved_in_slab;",
            "",
            "\t/* Try to find some space for it */",
            "\tif (use_slab) {",
            "\t\tnew_array = kmalloc(new_size, GFP_KERNEL);",
            "\t\taddr = new_array ? __pa(new_array) : 0;",
            "\t} else {",
            "\t\t/* only exclude range when trying to double reserved.regions */",
            "\t\tif (type != &memblock.reserved)",
            "\t\t\tnew_area_start = new_area_size = 0;",
            "",
            "\t\taddr = memblock_find_in_range(new_area_start + new_area_size,",
            "\t\t\t\t\t\tmemblock.current_limit,",
            "\t\t\t\t\t\tnew_alloc_size, PAGE_SIZE);",
            "\t\tif (!addr && new_area_size)",
            "\t\t\taddr = memblock_find_in_range(0,",
            "\t\t\t\tmin(new_area_start, memblock.current_limit),",
            "\t\t\t\tnew_alloc_size, PAGE_SIZE);",
            "",
            "\t\tif (addr) {",
            "\t\t\t/* The memory may not have been accepted, yet. */",
            "\t\t\taccept_memory(addr, new_alloc_size);",
            "",
            "\t\t\tnew_array = __va(addr);",
            "\t\t} else {",
            "\t\t\tnew_array = NULL;",
            "\t\t}",
            "\t}",
            "\tif (!addr) {",
            "\t\tpr_err(\"memblock: Failed to double %s array from %ld to %ld entries !\\n\",",
            "\t\t       type->name, type->max, type->max * 2);",
            "\t\treturn -1;",
            "\t}",
            "",
            "\tnew_end = addr + new_size - 1;",
            "\tmemblock_dbg(\"memblock: %s is doubled to %ld at [%pa-%pa]\",",
            "\t\t\ttype->name, type->max * 2, &addr, &new_end);",
            "",
            "\t/*",
            "\t * Found space, we now need to move the array over before we add the",
            "\t * reserved region since it may be our reserved array itself that is",
            "\t * full.",
            "\t */",
            "\tmemcpy(new_array, type->regions, old_size);",
            "\tmemset(new_array + type->max, 0, old_size);",
            "\told_array = type->regions;",
            "\ttype->regions = new_array;",
            "\ttype->max <<= 1;",
            "",
            "\t/* Free old array. We needn't free it if the array is the static one */",
            "\tif (*in_slab)",
            "\t\tkfree(old_array);",
            "\telse if (old_array != memblock_memory_init_regions &&",
            "\t\t old_array != memblock_reserved_init_regions)",
            "\t\tmemblock_free(old_array, old_alloc_size);",
            "",
            "\t/*",
            "\t * Reserve the new array if that comes from the memblock.  Otherwise, we",
            "\t * needn't do it",
            "\t */",
            "\tif (!use_slab)",
            "\t\tBUG_ON(memblock_reserve_kern(addr, new_alloc_size));",
            "",
            "\t/* Update slab flag */",
            "\t*in_slab = use_slab;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "memblock_remove_region, memblock_discard, memblock_double_array",
          "description": "实现内存区域数组的动态扩容（double_array）、旧区域释放（discard）及区域移除操作，维护内存类型总大小统计。",
          "similarity": 0.5465692281723022
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5584216117858887,
      "chunks": [
        {
          "chunk_id": 27,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4895,
          "end_line": 5082,
          "content": [
            "static void __init rcu_init_one(void)",
            "{",
            "\tstatic const char * const buf[] = RCU_NODE_NAME_INIT;",
            "\tstatic const char * const fqs[] = RCU_FQS_NAME_INIT;",
            "\tstatic struct lock_class_key rcu_node_class[RCU_NUM_LVLS];",
            "\tstatic struct lock_class_key rcu_fqs_class[RCU_NUM_LVLS];",
            "",
            "\tint levelspread[RCU_NUM_LVLS];\t\t/* kids/node in each level. */",
            "\tint cpustride = 1;",
            "\tint i;",
            "\tint j;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tBUILD_BUG_ON(RCU_NUM_LVLS > ARRAY_SIZE(buf));  /* Fix buf[] init! */",
            "",
            "\t/* Silence gcc 4.8 false positive about array index out of range. */",
            "\tif (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)",
            "\t\tpanic(\"rcu_init_one: rcu_num_lvls out of range\");",
            "",
            "\t/* Initialize the level-tracking arrays. */",
            "",
            "\tfor (i = 1; i < rcu_num_lvls; i++)",
            "\t\trcu_state.level[i] =",
            "\t\t\trcu_state.level[i - 1] + num_rcu_lvl[i - 1];",
            "\trcu_init_levelspread(levelspread, num_rcu_lvl);",
            "",
            "\t/* Initialize the elements themselves, starting from the leaves. */",
            "",
            "\tfor (i = rcu_num_lvls - 1; i >= 0; i--) {",
            "\t\tcpustride *= levelspread[i];",
            "\t\trnp = rcu_state.level[i];",
            "\t\tfor (j = 0; j < num_rcu_lvl[i]; j++, rnp++) {",
            "\t\t\traw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));",
            "\t\t\tlockdep_set_class_and_name(&ACCESS_PRIVATE(rnp, lock),",
            "\t\t\t\t\t\t   &rcu_node_class[i], buf[i]);",
            "\t\t\traw_spin_lock_init(&rnp->fqslock);",
            "\t\t\tlockdep_set_class_and_name(&rnp->fqslock,",
            "\t\t\t\t\t\t   &rcu_fqs_class[i], fqs[i]);",
            "\t\t\trnp->gp_seq = rcu_state.gp_seq;",
            "\t\t\trnp->gp_seq_needed = rcu_state.gp_seq;",
            "\t\t\trnp->completedqs = rcu_state.gp_seq;",
            "\t\t\trnp->qsmask = 0;",
            "\t\t\trnp->qsmaskinit = 0;",
            "\t\t\trnp->grplo = j * cpustride;",
            "\t\t\trnp->grphi = (j + 1) * cpustride - 1;",
            "\t\t\tif (rnp->grphi >= nr_cpu_ids)",
            "\t\t\t\trnp->grphi = nr_cpu_ids - 1;",
            "\t\t\tif (i == 0) {",
            "\t\t\t\trnp->grpnum = 0;",
            "\t\t\t\trnp->grpmask = 0;",
            "\t\t\t\trnp->parent = NULL;",
            "\t\t\t} else {",
            "\t\t\t\trnp->grpnum = j % levelspread[i - 1];",
            "\t\t\t\trnp->grpmask = BIT(rnp->grpnum);",
            "\t\t\t\trnp->parent = rcu_state.level[i - 1] +",
            "\t\t\t\t\t      j / levelspread[i - 1];",
            "\t\t\t}",
            "\t\t\trnp->level = i;",
            "\t\t\tINIT_LIST_HEAD(&rnp->blkd_tasks);",
            "\t\t\trcu_init_one_nocb(rnp);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[0]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[1]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[2]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[3]);",
            "\t\t\tspin_lock_init(&rnp->exp_lock);",
            "\t\t\tmutex_init(&rnp->boost_kthread_mutex);",
            "\t\t\traw_spin_lock_init(&rnp->exp_poll_lock);",
            "\t\t\trnp->exp_seq_poll_rq = RCU_GET_STATE_COMPLETED;",
            "\t\t\tINIT_WORK(&rnp->exp_poll_wq, sync_rcu_do_polled_gp);",
            "\t\t}",
            "\t}",
            "",
            "\tinit_swait_queue_head(&rcu_state.gp_wq);",
            "\tinit_swait_queue_head(&rcu_state.expedited_wq);",
            "\trnp = rcu_first_leaf_node();",
            "\tfor_each_possible_cpu(i) {",
            "\t\twhile (i > rnp->grphi)",
            "\t\t\trnp++;",
            "\t\tper_cpu_ptr(&rcu_data, i)->mynode = rnp;",
            "\t\trcu_boot_init_percpu_data(i);",
            "\t}",
            "}",
            "static void __init sanitize_kthread_prio(void)",
            "{",
            "\tint kthread_prio_in = kthread_prio;",
            "",
            "\tif (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 2",
            "\t    && IS_BUILTIN(CONFIG_RCU_TORTURE_TEST))",
            "\t\tkthread_prio = 2;",
            "\telse if (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 1)",
            "\t\tkthread_prio = 1;",
            "\telse if (kthread_prio < 0)",
            "\t\tkthread_prio = 0;",
            "\telse if (kthread_prio > 99)",
            "\t\tkthread_prio = 99;",
            "",
            "\tif (kthread_prio != kthread_prio_in)",
            "\t\tpr_alert(\"%s: Limited prio to %d from %d\\n\",",
            "\t\t\t __func__, kthread_prio, kthread_prio_in);",
            "}",
            "void rcu_init_geometry(void)",
            "{",
            "\tulong d;",
            "\tint i;",
            "\tstatic unsigned long old_nr_cpu_ids;",
            "\tint rcu_capacity[RCU_NUM_LVLS];",
            "\tstatic bool initialized;",
            "",
            "\tif (initialized) {",
            "\t\t/*",
            "\t\t * Warn if setup_nr_cpu_ids() had not yet been invoked,",
            "\t\t * unless nr_cpus_ids == NR_CPUS, in which case who cares?",
            "\t\t */",
            "\t\tWARN_ON_ONCE(old_nr_cpu_ids != nr_cpu_ids);",
            "\t\treturn;",
            "\t}",
            "",
            "\told_nr_cpu_ids = nr_cpu_ids;",
            "\tinitialized = true;",
            "",
            "\t/*",
            "\t * Initialize any unspecified boot parameters.",
            "\t * The default values of jiffies_till_first_fqs and",
            "\t * jiffies_till_next_fqs are set to the RCU_JIFFIES_TILL_FORCE_QS",
            "\t * value, which is a function of HZ, then adding one for each",
            "\t * RCU_JIFFIES_FQS_DIV CPUs that might be on the system.",
            "\t */",
            "\td = RCU_JIFFIES_TILL_FORCE_QS + nr_cpu_ids / RCU_JIFFIES_FQS_DIV;",
            "\tif (jiffies_till_first_fqs == ULONG_MAX)",
            "\t\tjiffies_till_first_fqs = d;",
            "\tif (jiffies_till_next_fqs == ULONG_MAX)",
            "\t\tjiffies_till_next_fqs = d;",
            "\tadjust_jiffies_till_sched_qs();",
            "",
            "\t/* If the compile-time values are accurate, just leave. */",
            "\tif (rcu_fanout_leaf == RCU_FANOUT_LEAF &&",
            "\t    nr_cpu_ids == NR_CPUS)",
            "\t\treturn;",
            "\tpr_info(\"Adjusting geometry for rcu_fanout_leaf=%d, nr_cpu_ids=%u\\n\",",
            "\t\trcu_fanout_leaf, nr_cpu_ids);",
            "",
            "\t/*",
            "\t * The boot-time rcu_fanout_leaf parameter must be at least two",
            "\t * and cannot exceed the number of bits in the rcu_node masks.",
            "\t * Complain and fall back to the compile-time values if this",
            "\t * limit is exceeded.",
            "\t */",
            "\tif (rcu_fanout_leaf < 2 ||",
            "\t    rcu_fanout_leaf > sizeof(unsigned long) * 8) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Compute number of nodes that can be handled an rcu_node tree",
            "\t * with the given number of levels.",
            "\t */",
            "\trcu_capacity[0] = rcu_fanout_leaf;",
            "\tfor (i = 1; i < RCU_NUM_LVLS; i++)",
            "\t\trcu_capacity[i] = rcu_capacity[i - 1] * RCU_FANOUT;",
            "",
            "\t/*",
            "\t * The tree must be able to accommodate the configured number of CPUs.",
            "\t * If this limit is exceeded, fall back to the compile-time values.",
            "\t */",
            "\tif (nr_cpu_ids > rcu_capacity[RCU_NUM_LVLS - 1]) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Calculate the number of levels in the tree. */",
            "\tfor (i = 0; nr_cpu_ids > rcu_capacity[i]; i++) {",
            "\t}",
            "\trcu_num_lvls = i + 1;",
            "",
            "\t/* Calculate the number of rcu_nodes at each level of the tree. */",
            "\tfor (i = 0; i < rcu_num_lvls; i++) {",
            "\t\tint cap = rcu_capacity[(rcu_num_lvls - 1) - i];",
            "\t\tnum_rcu_lvl[i] = DIV_ROUND_UP(nr_cpu_ids, cap);",
            "\t}",
            "",
            "\t/* Calculate the total number of rcu_node structures. */",
            "\trcu_num_nodes = 0;",
            "\tfor (i = 0; i < rcu_num_lvls; i++)",
            "\t\trcu_num_nodes += num_rcu_lvl[i];",
            "}"
          ],
          "function_name": "rcu_init_one, sanitize_kthread_prio, rcu_init_geometry",
          "description": "构建多级RCU节点树结构，初始化各层级的锁类和节点属性，动态调整RCU树的几何形态以适配当前CPU数量和层级分布需求。",
          "similarity": 0.5760790705680847
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2523,
          "end_line": 2628,
          "content": [
            "static void rcu_cpu_kthread_park(unsigned int cpu)",
            "{",
            "\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;",
            "}",
            "static int rcu_cpu_kthread_should_run(unsigned int cpu)",
            "{",
            "\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);",
            "}",
            "static void rcu_cpu_kthread(unsigned int cpu)",
            "{",
            "\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);",
            "\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);",
            "\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);",
            "\tint spincnt;",
            "",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));",
            "\tfor (spincnt = 0; spincnt < 10; spincnt++) {",
            "\t\tWRITE_ONCE(*j, jiffies);",
            "\t\tlocal_bh_disable();",
            "\t\t*statusp = RCU_KTHREAD_RUNNING;",
            "\t\tlocal_irq_disable();",
            "\t\twork = *workp;",
            "\t\tWRITE_ONCE(*workp, 0);",
            "\t\tlocal_irq_enable();",
            "\t\tif (work)",
            "\t\t\trcu_core();",
            "\t\tlocal_bh_enable();",
            "\t\tif (!READ_ONCE(*workp)) {",
            "\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));",
            "\t\t\t*statusp = RCU_KTHREAD_WAITING;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "\t*statusp = RCU_KTHREAD_YIELDING;",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));",
            "\tschedule_timeout_idle(2);",
            "\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));",
            "\t*statusp = RCU_KTHREAD_WAITING;",
            "\tWRITE_ONCE(*j, jiffies);",
            "}",
            "static int __init rcu_spawn_core_kthreads(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;",
            "\tif (use_softirq)",
            "\t\treturn 0;",
            "\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),",
            "\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);",
            "\treturn 0;",
            "}",
            "static void rcutree_enqueue(struct rcu_data *rdp, struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\trcu_segcblist_enqueue(&rdp->cblist, head);",
            "\tif (__is_kvfree_rcu_offset((unsigned long)func))",
            "\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,",
            "\t\t\t\t\t (unsigned long)func,",
            "\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));",
            "\telse",
            "\t\ttrace_rcu_callback(rcu_state.name, head,",
            "\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));",
            "}",
            "static void call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,",
            "\t\t\t  rcu_callback_t func, unsigned long flags)",
            "{",
            "\trcutree_enqueue(rdp, head, func);",
            "\t/*",
            "\t * If called from an extended quiescent state, invoke the RCU",
            "\t * core in order to force a re-evaluation of RCU's idleness.",
            "\t */",
            "\tif (!rcu_is_watching())",
            "\t\tinvoke_rcu_core();",
            "",
            "\t/* If interrupts were disabled or CPU offline, don't invoke RCU core. */",
            "\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Force the grace period if too many callbacks or too long waiting.",
            "\t * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()",
            "\t * if some other CPU has recently done so.  Also, don't bother",
            "\t * invoking rcu_force_quiescent_state() if the newly enqueued callback",
            "\t * is the only one waiting for a grace period to complete.",
            "\t */",
            "\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >",
            "\t\t     rdp->qlen_last_fqs_check + qhimark)) {",
            "",
            "\t\t/* Are we ignoring a completed grace period? */",
            "\t\tnote_gp_changes(rdp);",
            "",
            "\t\t/* Start a new grace period if one not already started. */",
            "\t\tif (!rcu_gp_in_progress()) {",
            "\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);",
            "\t\t} else {",
            "\t\t\t/* Give the grace period a kick. */",
            "\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;",
            "\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&",
            "\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)",
            "\t\t\t\trcu_force_quiescent_state();",
            "\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "rcu_cpu_kthread_park, rcu_cpu_kthread_should_run, rcu_cpu_kthread, rcu_spawn_core_kthreads, rcutree_enqueue, call_rcu_core",
          "description": "实现RCU k线程管理与回调分发基础设施，包含线程启动、回调入队及触发条件判断逻辑，提供跨CPU的异步处理能力",
          "similarity": 0.5369649529457092
        },
        {
          "chunk_id": 20,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3589,
          "end_line": 3697,
          "content": [
            "static unsigned long",
            "kfree_rcu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)",
            "{",
            "\tint cpu;",
            "\tunsigned long count = 0;",
            "",
            "\t/* Snapshot count of all CPUs */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tcount += krc_count(krcp);",
            "\t\tcount += READ_ONCE(krcp->nr_bkv_objs);",
            "\t\tatomic_set(&krcp->backoff_page_cache_fill, 1);",
            "\t}",
            "",
            "\treturn count == 0 ? SHRINK_EMPTY : count;",
            "}",
            "static unsigned long",
            "kfree_rcu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)",
            "{",
            "\tint cpu, freed = 0;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tint count;",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tcount = krc_count(krcp);",
            "\t\tcount += drain_page_cache(krcp);",
            "\t\tkfree_rcu_monitor(&krcp->monitor_work.work);",
            "",
            "\t\tsc->nr_to_scan -= count;",
            "\t\tfreed += count;",
            "",
            "\t\tif (sc->nr_to_scan <= 0)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn freed == 0 ? SHRINK_STOP : freed;",
            "}",
            "void __init kfree_rcu_scheduler_running(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tif (need_offload_krc(krcp))",
            "\t\t\tschedule_delayed_monitor_work(krcp);",
            "\t}",
            "}",
            "static int rcu_blocking_is_gp(void)",
            "{",
            "\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE) {",
            "\t\tmight_sleep();",
            "\t\treturn false;",
            "\t}",
            "\treturn true;",
            "}",
            "void synchronize_rcu(void)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_sched_lock_map),",
            "\t\t\t \"Illegal synchronize_rcu() in RCU read-side critical section\");",
            "\tif (!rcu_blocking_is_gp()) {",
            "\t\tif (rcu_gp_is_expedited())",
            "\t\t\tsynchronize_rcu_expedited();",
            "\t\telse",
            "\t\t\twait_rcu_gp(call_rcu_hurry);",
            "\t\treturn;",
            "\t}",
            "",
            "\t// Context allows vacuous grace periods.",
            "\t// Note well that this code runs with !PREEMPT && !SMP.",
            "\t// In addition, all code that advances grace periods runs at",
            "\t// process level.  Therefore, this normal GP overlaps with other",
            "\t// normal GPs only by being fully nested within them, which allows",
            "\t// reuse of ->gp_seq_polled_snap.",
            "\trcu_poll_gp_seq_start_unlocked(&rcu_state.gp_seq_polled_snap);",
            "\trcu_poll_gp_seq_end_unlocked(&rcu_state.gp_seq_polled_snap);",
            "",
            "\t// Update the normal grace-period counters to record",
            "\t// this grace period, but only those used by the boot CPU.",
            "\t// The rcu_scheduler_starting() will take care of the rest of",
            "\t// these counters.",
            "\tlocal_irq_save(flags);",
            "\tWARN_ON_ONCE(num_online_cpus() > 1);",
            "\trcu_state.gp_seq += (1 << RCU_SEQ_CTR_SHIFT);",
            "\tfor (rnp = this_cpu_ptr(&rcu_data)->mynode; rnp; rnp = rnp->parent)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq = rcu_state.gp_seq;",
            "\tlocal_irq_restore(flags);",
            "}",
            "void get_completed_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\trgosp->rgos_norm = RCU_GET_STATE_COMPLETED;",
            "\trgosp->rgos_exp = RCU_GET_STATE_COMPLETED;",
            "}",
            "unsigned long get_state_synchronize_rcu(void)",
            "{",
            "\t/*",
            "\t * Any prior manipulation of RCU-protected data must happen",
            "\t * before the load from ->gp_seq.",
            "\t */",
            "\tsmp_mb();  /* ^^^ */",
            "\treturn rcu_seq_snap(&rcu_state.gp_seq_polled);",
            "}"
          ],
          "function_name": "kfree_rcu_shrink_count, kfree_rcu_shrink_scan, kfree_rcu_scheduler_running, rcu_blocking_is_gp, synchronize_rcu, get_completed_synchronize_rcu_full, get_state_synchronize_rcu",
          "description": "实现RCU内存回收的shrinker接口，统计并扫描等待回收的RCU对象，调度监控工作，处理同步屏障逻辑，通过锁竞争检测确保安全访问",
          "similarity": 0.5270559787750244
        },
        {
          "chunk_id": 19,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3436,
          "end_line": 3574,
          "content": [
            "void kvfree_call_rcu(struct rcu_head *head, void *ptr)",
            "{",
            "\tunsigned long flags;",
            "\tstruct kfree_rcu_cpu *krcp;",
            "\tbool success;",
            "",
            "\t/*",
            "\t * Please note there is a limitation for the head-less",
            "\t * variant, that is why there is a clear rule for such",
            "\t * objects: it can be used from might_sleep() context",
            "\t * only. For other places please embed an rcu_head to",
            "\t * your data.",
            "\t */",
            "\tif (!head)",
            "\t\tmight_sleep();",
            "",
            "\t// Queue the object but don't yet schedule the batch.",
            "\tif (debug_rcu_head_queue(ptr)) {",
            "\t\t// Probable double kfree_rcu(), just leak.",
            "\t\tWARN_ONCE(1, \"%s(): Double-freed call. rcu_head %p\\n\",",
            "\t\t\t  __func__, head);",
            "",
            "\t\t// Mark as success and leave.",
            "\t\treturn;",
            "\t}",
            "",
            "\tkasan_record_aux_stack_noalloc(ptr);",
            "\tsuccess = add_ptr_to_bulk_krc_lock(&krcp, &flags, ptr, !head);",
            "\tif (!success) {",
            "\t\trun_page_cache_worker(krcp);",
            "",
            "\t\tif (head == NULL)",
            "\t\t\t// Inline if kvfree_rcu(one_arg) call.",
            "\t\t\tgoto unlock_return;",
            "",
            "\t\thead->func = ptr;",
            "\t\thead->next = krcp->head;",
            "\t\tWRITE_ONCE(krcp->head, head);",
            "\t\tatomic_inc(&krcp->head_count);",
            "",
            "\t\t// Take a snapshot for this krcp.",
            "\t\tkrcp->head_gp_snap = get_state_synchronize_rcu();",
            "\t\tsuccess = true;",
            "\t}",
            "",
            "\t/*",
            "\t * The kvfree_rcu() caller considers the pointer freed at this point",
            "\t * and likely removes any references to it. Since the actual slab",
            "\t * freeing (and kmemleak_free()) is deferred, tell kmemleak to ignore",
            "\t * this object (no scanning or false positives reporting).",
            "\t */",
            "\tkmemleak_ignore(ptr);",
            "",
            "\t// Set timer to drain after KFREE_DRAIN_JIFFIES.",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_RUNNING)",
            "\t\t__schedule_delayed_monitor_work(krcp);",
            "",
            "unlock_return:",
            "\tkrc_this_cpu_unlock(krcp, flags);",
            "",
            "\t/*",
            "\t * Inline kvfree() after synchronize_rcu(). We can do",
            "\t * it from might_sleep() context only, so the current",
            "\t * CPU can pass the QS state.",
            "\t */",
            "\tif (!success) {",
            "\t\tdebug_rcu_head_unqueue((struct rcu_head *) ptr);",
            "\t\tsynchronize_rcu();",
            "\t\tkvfree(ptr);",
            "\t}",
            "}",
            "void kvfree_rcu_barrier(void)",
            "{",
            "\tstruct kfree_rcu_cpu_work *krwp;",
            "\tstruct kfree_rcu_cpu *krcp;",
            "\tbool queued;",
            "\tint i, cpu;",
            "",
            "\t/*",
            "\t * Firstly we detach objects and queue them over an RCU-batch",
            "\t * for all CPUs. Finally queued works are flushed for each CPU.",
            "\t *",
            "\t * Please note. If there are outstanding batches for a particular",
            "\t * CPU, those have to be finished first following by queuing a new.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tkrcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\t/*",
            "\t\t * Check if this CPU has any objects which have been queued for a",
            "\t\t * new GP completion. If not(means nothing to detach), we are done",
            "\t\t * with it. If any batch is pending/running for this \"krcp\", below",
            "\t\t * per-cpu flush_rcu_work() waits its completion(see last step).",
            "\t\t */",
            "\t\tif (!need_offload_krc(krcp))",
            "\t\t\tcontinue;",
            "",
            "\t\twhile (1) {",
            "\t\t\t/*",
            "\t\t\t * If we are not able to queue a new RCU work it means:",
            "\t\t\t * - batches for this CPU are still in flight which should",
            "\t\t\t *   be flushed first and then repeat;",
            "\t\t\t * - no objects to detach, because of concurrency.",
            "\t\t\t */",
            "\t\t\tqueued = kvfree_rcu_queue_batch(krcp);",
            "",
            "\t\t\t/*",
            "\t\t\t * Bail out, if there is no need to offload this \"krcp\"",
            "\t\t\t * anymore. As noted earlier it can run concurrently.",
            "\t\t\t */",
            "\t\t\tif (queued || !need_offload_krc(krcp))",
            "\t\t\t\tbreak;",
            "",
            "\t\t\t/* There are ongoing batches. */",
            "\t\t\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\t\t\tkrwp = &(krcp->krw_arr[i]);",
            "\t\t\t\tflush_rcu_work(&krwp->rcu_work);",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Now we guarantee that all objects are flushed.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tkrcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\t/*",
            "\t\t * A monitor work can drain ready to reclaim objects",
            "\t\t * directly. Wait its completion if running or pending.",
            "\t\t */",
            "\t\tcancel_delayed_work_sync(&krcp->monitor_work);",
            "",
            "\t\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\t\tkrwp = &(krcp->krw_arr[i]);",
            "\t\t\tflush_rcu_work(&krwp->rcu_work);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "kvfree_call_rcu, kvfree_rcu_barrier",
          "description": "实现基于RCU的延迟内存释放接口，提供安全释放路径并保证内存屏障语义，强制同步清理所有挂起的释放请求。",
          "similarity": 0.5262561440467834
        },
        {
          "chunk_id": 18,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3279,
          "end_line": 3412,
          "content": [
            "static void kfree_rcu_monitor(struct work_struct *work)",
            "{",
            "\tstruct kfree_rcu_cpu *krcp = container_of(work,",
            "\t\tstruct kfree_rcu_cpu, monitor_work.work);",
            "",
            "\t// Drain ready for reclaim.",
            "\tkvfree_rcu_drain_ready(krcp);",
            "",
            "\t// Queue a batch for a rest.",
            "\tkvfree_rcu_queue_batch(krcp);",
            "",
            "\t// If there is nothing to detach, it means that our job is",
            "\t// successfully done here. In case of having at least one",
            "\t// of the channels that is still busy we should rearm the",
            "\t// work to repeat an attempt. Because previous batches are",
            "\t// still in progress.",
            "\tif (need_offload_krc(krcp))",
            "\t\tschedule_delayed_monitor_work(krcp);",
            "}",
            "static enum hrtimer_restart",
            "schedule_page_work_fn(struct hrtimer *t)",
            "{",
            "\tstruct kfree_rcu_cpu *krcp =",
            "\t\tcontainer_of(t, struct kfree_rcu_cpu, hrtimer);",
            "",
            "\tqueue_delayed_work(system_highpri_wq, &krcp->page_cache_work, 0);",
            "\treturn HRTIMER_NORESTART;",
            "}",
            "static void fill_page_cache_func(struct work_struct *work)",
            "{",
            "\tstruct kvfree_rcu_bulk_data *bnode;",
            "\tstruct kfree_rcu_cpu *krcp =",
            "\t\tcontainer_of(work, struct kfree_rcu_cpu,",
            "\t\t\tpage_cache_work.work);",
            "\tunsigned long flags;",
            "\tint nr_pages;",
            "\tbool pushed;",
            "\tint i;",
            "",
            "\tnr_pages = atomic_read(&krcp->backoff_page_cache_fill) ?",
            "\t\t1 : rcu_min_cached_objs;",
            "",
            "\tfor (i = READ_ONCE(krcp->nr_bkv_objs); i < nr_pages; i++) {",
            "\t\tbnode = (struct kvfree_rcu_bulk_data *)",
            "\t\t\t__get_free_page(GFP_KERNEL | __GFP_NORETRY | __GFP_NOMEMALLOC | __GFP_NOWARN);",
            "",
            "\t\tif (!bnode)",
            "\t\t\tbreak;",
            "",
            "\t\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\t\tpushed = put_cached_bnode(krcp, bnode);",
            "\t\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "",
            "\t\tif (!pushed) {",
            "\t\t\tfree_page((unsigned long) bnode);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tatomic_set(&krcp->work_in_progress, 0);",
            "\tatomic_set(&krcp->backoff_page_cache_fill, 0);",
            "}",
            "static void",
            "run_page_cache_worker(struct kfree_rcu_cpu *krcp)",
            "{",
            "\t// If cache disabled, bail out.",
            "\tif (!rcu_min_cached_objs)",
            "\t\treturn;",
            "",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_RUNNING &&",
            "\t\t\t!atomic_xchg(&krcp->work_in_progress, 1)) {",
            "\t\tif (atomic_read(&krcp->backoff_page_cache_fill)) {",
            "\t\t\tqueue_delayed_work(system_unbound_wq,",
            "\t\t\t\t&krcp->page_cache_work,",
            "\t\t\t\t\tmsecs_to_jiffies(rcu_delay_page_cache_fill_msec));",
            "\t\t} else {",
            "\t\t\thrtimer_init(&krcp->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);",
            "\t\t\tkrcp->hrtimer.function = schedule_page_work_fn;",
            "\t\t\thrtimer_start(&krcp->hrtimer, 0, HRTIMER_MODE_REL);",
            "\t\t}",
            "\t}",
            "}",
            "static inline bool",
            "add_ptr_to_bulk_krc_lock(struct kfree_rcu_cpu **krcp,",
            "\tunsigned long *flags, void *ptr, bool can_alloc)",
            "{",
            "\tstruct kvfree_rcu_bulk_data *bnode;",
            "\tint idx;",
            "",
            "\t*krcp = krc_this_cpu_lock(flags);",
            "\tif (unlikely(!(*krcp)->initialized))",
            "\t\treturn false;",
            "",
            "\tidx = !!is_vmalloc_addr(ptr);",
            "\tbnode = list_first_entry_or_null(&(*krcp)->bulk_head[idx],",
            "\t\tstruct kvfree_rcu_bulk_data, list);",
            "",
            "\t/* Check if a new block is required. */",
            "\tif (!bnode || bnode->nr_records == KVFREE_BULK_MAX_ENTR) {",
            "\t\tbnode = get_cached_bnode(*krcp);",
            "\t\tif (!bnode && can_alloc) {",
            "\t\t\tkrc_this_cpu_unlock(*krcp, *flags);",
            "",
            "\t\t\t// __GFP_NORETRY - allows a light-weight direct reclaim",
            "\t\t\t// what is OK from minimizing of fallback hitting point of",
            "\t\t\t// view. Apart of that it forbids any OOM invoking what is",
            "\t\t\t// also beneficial since we are about to release memory soon.",
            "\t\t\t//",
            "\t\t\t// __GFP_NOMEMALLOC - prevents from consuming of all the",
            "\t\t\t// memory reserves. Please note we have a fallback path.",
            "\t\t\t//",
            "\t\t\t// __GFP_NOWARN - it is supposed that an allocation can",
            "\t\t\t// be failed under low memory or high memory pressure",
            "\t\t\t// scenarios.",
            "\t\t\tbnode = (struct kvfree_rcu_bulk_data *)",
            "\t\t\t\t__get_free_page(GFP_KERNEL | __GFP_NORETRY | __GFP_NOMEMALLOC | __GFP_NOWARN);",
            "\t\t\traw_spin_lock_irqsave(&(*krcp)->lock, *flags);",
            "\t\t}",
            "",
            "\t\tif (!bnode)",
            "\t\t\treturn false;",
            "",
            "\t\t// Initialize the new block and attach it.",
            "\t\tbnode->nr_records = 0;",
            "\t\tlist_add(&bnode->list, &(*krcp)->bulk_head[idx]);",
            "\t}",
            "",
            "\t// Finally insert and update the GP for this page.",
            "\tbnode->records[bnode->nr_records++] = ptr;",
            "\tget_state_synchronize_rcu_full(&bnode->gp_snap);",
            "\tatomic_inc(&(*krcp)->bulk_count[idx]);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "kfree_rcu_monitor, schedule_page_work_fn, fill_page_cache_func, run_page_cache_worker, add_ptr_to_bulk_krc_lock",
          "description": "维护页面缓存池并提供动态扩展能力，通过工作队列机制定期补充缓存页，支持SLAB与vmalloc指针的分类存储。",
          "similarity": 0.5210535526275635
        }
      ]
    }
  ]
}