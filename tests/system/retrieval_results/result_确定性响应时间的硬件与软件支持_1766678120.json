{
  "query": "确定性响应时间的硬件与软件支持",
  "timestamp": "2025-12-25 23:55:20",
  "retrieved_files": [
    {
      "source_file": "kernel/time/tick-sched.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:51:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `time\\tick-sched.c`\n\n---\n\n# `time/tick-sched.c` 技术文档\n\n## 1. 文件概述\n\n`tick-sched.c` 是 Linux 内核中实现 **无滴答（tickless）调度** 的核心文件，主要用于支持 **NO_HZ（无周期性时钟中断）** 功能。该机制允许系统在空闲或特定负载条件下动态停止周期性的时钟中断（tick），从而降低功耗、减少 CPU 干扰，并提升实时性能。文件同时支持 **低分辨率定时器（NO_HZ_COMMON）** 和 **高分辨率定时器（HIGH_RES_TIMERS）** 场景下的无滴答行为。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- `struct tick_sched`  \n  每个 CPU 的无滴答调度控制结构，记录当前 CPU 的 tick 状态，包括是否处于空闲、tick 是否已停止、依赖项、空闲 jiffies 计数等。\n\n- `tick_cpu_sched`（per-CPU 变量）  \n  每个 CPU 对应的 `tick_sched` 实例。\n\n- `last_jiffies_update`（全局）  \n  记录上一次 jiffies 更新的时间点，用于在无滴答期间计算应推进的 jiffies 数量。\n\n- `tick_nohz_full_mask`（仅 CONFIG_NO_HZ_FULL）  \n  标识启用了 **完全无滴答（NO_HZ_FULL）** 模式的 CPU 集合。\n\n- `tick_dep_mask`（原子变量，仅 CONFIG_NO_HZ_FULL）  \n  全局 tick 依赖掩码，用于跟踪系统级阻止 tick 停止的原因（如 POSIX 定时器、RCU、调度器等）。\n\n### 主要函数\n\n- `tick_get_tick_sched(int cpu)`  \n  获取指定 CPU 的 `tick_sched` 结构指针。\n\n- `tick_do_update_jiffies64(ktime_t now)`  \n  在无滴答模式下，根据当前时间 `now` 计算并更新全局 `jiffies_64` 值，确保时间推进的准确性。支持 32/64 位架构的内存序优化。\n\n- `tick_init_jiffy_update(void)`  \n  初始化 jiffies 更新机制，确保 `last_jiffies_update` 与 `TICK_NSEC` 对齐。\n\n- `tick_sched_do_timer(struct tick_sched *ts, ktime_t now)`  \n  处理与全局时间维护相关的逻辑，包括：\n  - 在 `tick_do_timer_cpu` 为 `NONE` 时接管 jiffies 更新职责；\n  - 调用 `tick_do_update_jiffies64()`；\n  - 检测 jiffies 更新是否停滞（如因虚拟机暂停），并在停滞过久时强制更新。\n\n- `tick_sched_handle(struct tick_sched *ts, struct pt_regs *regs)`  \n  处理每个 tick 中断的常规任务，包括：\n  - 更新进程时间统计（`update_process_times`）；\n  - 触发性能剖析（`profile_tick`）；\n  - 在 tick 停止期间维护软锁定看门狗（`touch_softlockup_watchdog_sched`）；\n  - 更新空闲任务的 jiffies 计数。\n\n- `check_tick_dependency(atomic_t *dep)`  \n  （仅 CONFIG_NO_HZ_FULL）检查 tick 依赖掩码，判断是否存在阻止 tick 停止的条件（如 POSIX 定时器、RCU、调度器活动等）。\n\n- `can_stop_full_tick(int cpu, struct tick_sched *ts)`  \n  （片段未完整）用于判断在 NO_HZ_FULL 模式下是否可以安全停止指定 CPU 的 tick。\n\n## 3. 关键实现\n\n### 无滴答 Jiffies 更新机制\n\n- 使用 `jiffies_lock` 和 `jiffies_seq`（顺序锁）保护 `jiffies_64` 和 `last_jiffies_update` 的更新。\n- 在 64 位系统上，通过 `smp_load_acquire()` / `smp_store_release()` 实现无锁快速路径检查，避免不必要的锁竞争。\n- 在 32 位系统上，由于 64 位变量非原子写入，必须通过 `seqcount` 保证读取一致性。\n- 支持“慢路径”处理：当系统长时间睡眠（delta ≥ TICK_NSEC），通过除法计算应推进的 tick 数量。\n\n### Jiffies 停滞检测\n\n- 通过 `ts->last_tick_jiffies` 和 `ts->stalled_jiffies` 跟踪 jiffies 是否长时间未更新。\n- 若连续 `MAX_STALLED_JIFFIES`（默认 5）次未更新，则强制调用 `tick_do_update_jiffies64()`，防止时间漂移（如虚拟机暂停或 stop_machine 场景）。\n\n### NO_HZ_FULL 依赖管理\n\n- 使用位掩码（`TICK_DEP_MASK_*`）标识阻止 tick 停止的原因。\n- 每次尝试停止 tick 前，检查全局和 per-CPU 的依赖掩码。\n- 通过 tracepoint `trace_tick_stop()` 记录阻止原因，便于调试。\n\n### 空闲状态处理\n\n- 当 `tick_stopped` 为真时（即处于无滴答空闲状态）：\n  - 调用 `touch_softlockup_watchdog_sched()` 防止软锁定误报；\n  - 若当前任务为空闲任务，则递增 `ts->idle_jiffies`，用于后续空闲时间统计校正；\n  - 重置 `ts->next_tick = 0`，确保下次 tick 编程不会跳过。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/hrtimer.h>`：高分辨率定时器支持。\n  - `<linux/sched/nohz.h>`：NO_HZ 相关调度接口。\n  - `<linux/sched/clock.h>`、`<linux/kernel_stat.h>`：时间与统计信息。\n  - `<linux/seqlock.h>`（隐含）：通过 `jiffies_seq` 实现顺序锁。\n  - `\"tick-internal.h\"`：内部 tick 管理函数和变量（如 `tick_next_period`, `tick_do_timer_cpu`）。\n\n- **内核子系统交互**：\n  - **时间子系统**：与 `timekeeping.c` 协同更新 `wall_time`（通过 `update_wall_time()`）。\n  - **调度器**：通过 `update_process_times()` 更新进程 CPU 时间。\n  - **RCU**：NO_HZ_FULL 模式下需确保 RCU 宽限期推进。\n  - **性能剖析**：触发 `CPU_PROFILING` 事件。\n  - **软锁定检测**：维护看门狗状态。\n\n## 5. 使用场景\n\n- **移动/嵌入式设备**：在 CPU 空闲时停止 tick，显著降低功耗。\n- **高性能计算/实时系统**：减少周期性中断对关键任务的干扰，提升确定性（尤其在 NO_HZ_FULL 模式下）。\n- **虚拟化环境**：处理 VM 暂停导致的长时间无 tick 场景，通过停滞检测机制恢复时间同步。\n- **服务器负载波动**：在低负载期间进入无滴答状态，提升能效比。\n- **内核调试与追踪**：通过 `trace_tick_stop` 等 tracepoint 分析 tick 停止失败原因。",
      "similarity": 0.5817821621894836,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/time/tick-sched.c",
          "start_line": 57,
          "end_line": 177,
          "content": [
            "static void tick_do_update_jiffies64(ktime_t now)",
            "{",
            "\tunsigned long ticks = 1;",
            "\tktime_t delta, nextp;",
            "",
            "\t/*",
            "\t * 64bit can do a quick check without holding jiffies lock and",
            "\t * without looking at the sequence count. The smp_load_acquire()",
            "\t * pairs with the update done later in this function.",
            "\t *",
            "\t * 32bit cannot do that because the store of tick_next_period",
            "\t * consists of two 32bit stores and the first store could move it",
            "\t * to a random point in the future.",
            "\t */",
            "\tif (IS_ENABLED(CONFIG_64BIT)) {",
            "\t\tif (ktime_before(now, smp_load_acquire(&tick_next_period)))",
            "\t\t\treturn;",
            "\t} else {",
            "\t\tunsigned int seq;",
            "",
            "\t\t/*",
            "\t\t * Avoid contention on jiffies_lock and protect the quick",
            "\t\t * check with the sequence count.",
            "\t\t */",
            "\t\tdo {",
            "\t\t\tseq = read_seqcount_begin(&jiffies_seq);",
            "\t\t\tnextp = tick_next_period;",
            "\t\t} while (read_seqcount_retry(&jiffies_seq, seq));",
            "",
            "\t\tif (ktime_before(now, nextp))",
            "\t\t\treturn;",
            "\t}",
            "",
            "\t/* Quick check failed, i.e. update is required. */",
            "\traw_spin_lock(&jiffies_lock);",
            "\t/*",
            "\t * Reevaluate with the lock held. Another CPU might have done the",
            "\t * update already.",
            "\t */",
            "\tif (ktime_before(now, tick_next_period)) {",
            "\t\traw_spin_unlock(&jiffies_lock);",
            "\t\treturn;",
            "\t}",
            "",
            "\twrite_seqcount_begin(&jiffies_seq);",
            "",
            "\tdelta = ktime_sub(now, tick_next_period);",
            "\tif (unlikely(delta >= TICK_NSEC)) {",
            "\t\t/* Slow path for long idle sleep times */",
            "\t\ts64 incr = TICK_NSEC;",
            "",
            "\t\tticks += ktime_divns(delta, incr);",
            "",
            "\t\tlast_jiffies_update = ktime_add_ns(last_jiffies_update,",
            "\t\t\t\t\t\t   incr * ticks);",
            "\t} else {",
            "\t\tlast_jiffies_update = ktime_add_ns(last_jiffies_update,",
            "\t\t\t\t\t\t   TICK_NSEC);",
            "\t}",
            "",
            "\t/* Advance jiffies to complete the jiffies_seq protected job */",
            "\tjiffies_64 += ticks;",
            "",
            "\t/*",
            "\t * Keep the tick_next_period variable up to date.",
            "\t */",
            "\tnextp = ktime_add_ns(last_jiffies_update, TICK_NSEC);",
            "",
            "\tif (IS_ENABLED(CONFIG_64BIT)) {",
            "\t\t/*",
            "\t\t * Pairs with smp_load_acquire() in the lockless quick",
            "\t\t * check above and ensures that the update to jiffies_64 is",
            "\t\t * not reordered vs. the store to tick_next_period, neither",
            "\t\t * by the compiler nor by the CPU.",
            "\t\t */",
            "\t\tsmp_store_release(&tick_next_period, nextp);",
            "\t} else {",
            "\t\t/*",
            "\t\t * A plain store is good enough on 32bit as the quick check",
            "\t\t * above is protected by the sequence count.",
            "\t\t */",
            "\t\ttick_next_period = nextp;",
            "\t}",
            "",
            "\t/*",
            "\t * Release the sequence count. calc_global_load() below is not",
            "\t * protected by it, but jiffies_lock needs to be held to prevent",
            "\t * concurrent invocations.",
            "\t */",
            "\twrite_seqcount_end(&jiffies_seq);",
            "",
            "\tcalc_global_load();",
            "",
            "\traw_spin_unlock(&jiffies_lock);",
            "\tupdate_wall_time();",
            "}",
            "static ktime_t tick_init_jiffy_update(void)",
            "{",
            "\tktime_t period;",
            "",
            "\traw_spin_lock(&jiffies_lock);",
            "\twrite_seqcount_begin(&jiffies_seq);",
            "\t/* Did we start the jiffies update yet ? */",
            "\tif (last_jiffies_update == 0) {",
            "\t\tu32 rem;",
            "",
            "\t\t/*",
            "\t\t * Ensure that the tick is aligned to a multiple of",
            "\t\t * TICK_NSEC.",
            "\t\t */",
            "\t\tdiv_u64_rem(tick_next_period, TICK_NSEC, &rem);",
            "\t\tif (rem)",
            "\t\t\ttick_next_period += TICK_NSEC - rem;",
            "",
            "\t\tlast_jiffies_update = tick_next_period;",
            "\t}",
            "\tperiod = last_jiffies_update;",
            "\twrite_seqcount_end(&jiffies_seq);",
            "\traw_spin_unlock(&jiffies_lock);",
            "\treturn period;",
            "}"
          ],
          "function_name": "tick_do_update_jiffies64, tick_init_jiffy_update",
          "description": "实现jiffies计数器更新逻辑，通过比较当前时间与预定周期判断是否需要更新jiffies，处理32位和64位平台的不同同步机制",
          "similarity": 0.5724552869796753
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/time/tick-sched.c",
          "start_line": 185,
          "end_line": 291,
          "content": [
            "static void tick_sched_do_timer(struct tick_sched *ts, ktime_t now)",
            "{",
            "\tint cpu = smp_processor_id();",
            "",
            "#ifdef CONFIG_NO_HZ_COMMON",
            "\t/*",
            "\t * Check if the do_timer duty was dropped. We don't care about",
            "\t * concurrency: This happens only when the CPU in charge went",
            "\t * into a long sleep. If two CPUs happen to assign themselves to",
            "\t * this duty, then the jiffies update is still serialized by",
            "\t * jiffies_lock.",
            "\t *",
            "\t * If nohz_full is enabled, this should not happen because the",
            "\t * tick_do_timer_cpu never relinquishes.",
            "\t */",
            "\tif (unlikely(tick_do_timer_cpu == TICK_DO_TIMER_NONE)) {",
            "#ifdef CONFIG_NO_HZ_FULL",
            "\t\tWARN_ON_ONCE(tick_nohz_full_running);",
            "#endif",
            "\t\ttick_do_timer_cpu = cpu;",
            "\t}",
            "#endif",
            "",
            "\t/* Check, if the jiffies need an update */",
            "\tif (tick_do_timer_cpu == cpu)",
            "\t\ttick_do_update_jiffies64(now);",
            "",
            "\t/*",
            "\t * If jiffies update stalled for too long (timekeeper in stop_machine()",
            "\t * or VMEXIT'ed for several msecs), force an update.",
            "\t */",
            "\tif (ts->last_tick_jiffies != jiffies) {",
            "\t\tts->stalled_jiffies = 0;",
            "\t\tts->last_tick_jiffies = READ_ONCE(jiffies);",
            "\t} else {",
            "\t\tif (++ts->stalled_jiffies == MAX_STALLED_JIFFIES) {",
            "\t\t\ttick_do_update_jiffies64(now);",
            "\t\t\tts->stalled_jiffies = 0;",
            "\t\t\tts->last_tick_jiffies = READ_ONCE(jiffies);",
            "\t\t}",
            "\t}",
            "",
            "\tif (ts->inidle)",
            "\t\tts->got_idle_tick = 1;",
            "}",
            "static void tick_sched_handle(struct tick_sched *ts, struct pt_regs *regs)",
            "{",
            "#ifdef CONFIG_NO_HZ_COMMON",
            "\t/*",
            "\t * When we are idle and the tick is stopped, we have to touch",
            "\t * the watchdog as we might not schedule for a really long",
            "\t * time. This happens on complete idle SMP systems while",
            "\t * waiting on the login prompt. We also increment the \"start of",
            "\t * idle\" jiffy stamp so the idle accounting adjustment we do",
            "\t * when we go busy again does not account too much ticks.",
            "\t */",
            "\tif (ts->tick_stopped) {",
            "\t\ttouch_softlockup_watchdog_sched();",
            "\t\tif (is_idle_task(current))",
            "\t\t\tts->idle_jiffies++;",
            "\t\t/*",
            "\t\t * In case the current tick fired too early past its expected",
            "\t\t * expiration, make sure we don't bypass the next clock reprogramming",
            "\t\t * to the same deadline.",
            "\t\t */",
            "\t\tts->next_tick = 0;",
            "\t}",
            "#endif",
            "\tupdate_process_times(user_mode(regs));",
            "\tprofile_tick(CPU_PROFILING);",
            "}",
            "static bool check_tick_dependency(atomic_t *dep)",
            "{",
            "\tint val = atomic_read(dep);",
            "",
            "\tif (val & TICK_DEP_MASK_POSIX_TIMER) {",
            "\t\ttrace_tick_stop(0, TICK_DEP_MASK_POSIX_TIMER);",
            "\t\treturn true;",
            "\t}",
            "",
            "\tif (val & TICK_DEP_MASK_PERF_EVENTS) {",
            "\t\ttrace_tick_stop(0, TICK_DEP_MASK_PERF_EVENTS);",
            "\t\treturn true;",
            "\t}",
            "",
            "\tif (val & TICK_DEP_MASK_SCHED) {",
            "\t\ttrace_tick_stop(0, TICK_DEP_MASK_SCHED);",
            "\t\treturn true;",
            "\t}",
            "",
            "\tif (val & TICK_DEP_MASK_CLOCK_UNSTABLE) {",
            "\t\ttrace_tick_stop(0, TICK_DEP_MASK_CLOCK_UNSTABLE);",
            "\t\treturn true;",
            "\t}",
            "",
            "\tif (val & TICK_DEP_MASK_RCU) {",
            "\t\ttrace_tick_stop(0, TICK_DEP_MASK_RCU);",
            "\t\treturn true;",
            "\t}",
            "",
            "\tif (val & TICK_DEP_MASK_RCU_EXP) {",
            "\t\ttrace_tick_stop(0, TICK_DEP_MASK_RCU_EXP);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "tick_sched_do_timer, tick_sched_handle, check_tick_dependency",
          "description": "处理tick中断服务程序，包含jiffies更新检测、空闲超时监控以及依赖项检查逻辑，维护tick依赖状态标识",
          "similarity": 0.5635954141616821
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/time/tick-sched.c",
          "start_line": 802,
          "end_line": 962,
          "content": [
            "static inline bool local_timer_softirq_pending(void)",
            "{",
            "\treturn local_timers_pending() & BIT(TIMER_SOFTIRQ);",
            "}",
            "static ktime_t tick_nohz_next_event(struct tick_sched *ts, int cpu)",
            "{",
            "\tu64 basemono, next_tick, delta, expires;",
            "\tunsigned long basejiff;",
            "\tunsigned int seq;",
            "",
            "\t/* Read jiffies and the time when jiffies were updated last */",
            "\tdo {",
            "\t\tseq = read_seqcount_begin(&jiffies_seq);",
            "\t\tbasemono = last_jiffies_update;",
            "\t\tbasejiff = jiffies;",
            "\t} while (read_seqcount_retry(&jiffies_seq, seq));",
            "\tts->last_jiffies = basejiff;",
            "\tts->timer_expires_base = basemono;",
            "",
            "\t/*",
            "\t * Keep the periodic tick, when RCU, architecture or irq_work",
            "\t * requests it.",
            "\t * Aside of that check whether the local timer softirq is",
            "\t * pending. If so its a bad idea to call get_next_timer_interrupt()",
            "\t * because there is an already expired timer, so it will request",
            "\t * immediate expiry, which rearms the hardware timer with a",
            "\t * minimal delta which brings us back to this place",
            "\t * immediately. Lather, rinse and repeat...",
            "\t */",
            "\tif (rcu_needs_cpu() || arch_needs_cpu() ||",
            "\t    irq_work_needs_cpu() || local_timer_softirq_pending()) {",
            "\t\tnext_tick = basemono + TICK_NSEC;",
            "\t} else {",
            "\t\t/*",
            "\t\t * Get the next pending timer. If high resolution",
            "\t\t * timers are enabled this only takes the timer wheel",
            "\t\t * timers into account. If high resolution timers are",
            "\t\t * disabled this also looks at the next expiring",
            "\t\t * hrtimer.",
            "\t\t */",
            "\t\tnext_tick = get_next_timer_interrupt(basejiff, basemono);",
            "\t\tts->next_timer = next_tick;",
            "\t}",
            "",
            "\t/*",
            "\t * If the tick is due in the next period, keep it ticking or",
            "\t * force prod the timer.",
            "\t */",
            "\tdelta = next_tick - basemono;",
            "\tif (delta <= (u64)TICK_NSEC) {",
            "\t\t/*",
            "\t\t * Tell the timer code that the base is not idle, i.e. undo",
            "\t\t * the effect of get_next_timer_interrupt():",
            "\t\t */",
            "\t\ttimer_clear_idle();",
            "\t\t/*",
            "\t\t * We've not stopped the tick yet, and there's a timer in the",
            "\t\t * next period, so no point in stopping it either, bail.",
            "\t\t */",
            "\t\tif (!ts->tick_stopped) {",
            "\t\t\tts->timer_expires = 0;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * If this CPU is the one which had the do_timer() duty last, we limit",
            "\t * the sleep time to the timekeeping max_deferment value.",
            "\t * Otherwise we can sleep as long as we want.",
            "\t */",
            "\tdelta = timekeeping_max_deferment();",
            "\tif (cpu != tick_do_timer_cpu &&",
            "\t    (tick_do_timer_cpu != TICK_DO_TIMER_NONE || !ts->do_timer_last))",
            "\t\tdelta = KTIME_MAX;",
            "",
            "\t/* Calculate the next expiry time */",
            "\tif (delta < (KTIME_MAX - basemono))",
            "\t\texpires = basemono + delta;",
            "\telse",
            "\t\texpires = KTIME_MAX;",
            "",
            "\tts->timer_expires = min_t(u64, expires, next_tick);",
            "",
            "out:",
            "\treturn ts->timer_expires;",
            "}",
            "static void tick_nohz_stop_tick(struct tick_sched *ts, int cpu)",
            "{",
            "\tstruct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);",
            "\tu64 basemono = ts->timer_expires_base;",
            "\tu64 expires = ts->timer_expires;",
            "\tktime_t tick = expires;",
            "",
            "\t/* Make sure we won't be trying to stop it twice in a row. */",
            "\tts->timer_expires_base = 0;",
            "",
            "\t/*",
            "\t * If this CPU is the one which updates jiffies, then give up",
            "\t * the assignment and let it be taken by the CPU which runs",
            "\t * the tick timer next, which might be this CPU as well. If we",
            "\t * don't drop this here the jiffies might be stale and",
            "\t * do_timer() never invoked. Keep track of the fact that it",
            "\t * was the one which had the do_timer() duty last.",
            "\t */",
            "\tif (cpu == tick_do_timer_cpu) {",
            "\t\ttick_do_timer_cpu = TICK_DO_TIMER_NONE;",
            "\t\tts->do_timer_last = 1;",
            "\t} else if (tick_do_timer_cpu != TICK_DO_TIMER_NONE) {",
            "\t\tts->do_timer_last = 0;",
            "\t}",
            "",
            "\t/* Skip reprogram of event if its not changed */",
            "\tif (ts->tick_stopped && (expires == ts->next_tick)) {",
            "\t\t/* Sanity check: make sure clockevent is actually programmed */",
            "\t\tif (tick == KTIME_MAX || ts->next_tick == hrtimer_get_expires(&ts->sched_timer))",
            "\t\t\treturn;",
            "",
            "\t\tWARN_ON_ONCE(1);",
            "\t\tprintk_once(\"basemono: %llu ts->next_tick: %llu dev->next_event: %llu timer->active: %d timer->expires: %llu\\n\",",
            "\t\t\t    basemono, ts->next_tick, dev->next_event,",
            "\t\t\t    hrtimer_active(&ts->sched_timer), hrtimer_get_expires(&ts->sched_timer));",
            "\t}",
            "",
            "\t/*",
            "\t * nohz_stop_sched_tick can be called several times before",
            "\t * the nohz_restart_sched_tick is called. This happens when",
            "\t * interrupts arrive which do not cause a reschedule. In the",
            "\t * first call we save the current tick time, so we can restart",
            "\t * the scheduler tick in nohz_restart_sched_tick.",
            "\t */",
            "\tif (!ts->tick_stopped) {",
            "\t\tcalc_load_nohz_start();",
            "\t\tquiet_vmstat();",
            "",
            "\t\tts->last_tick = hrtimer_get_expires(&ts->sched_timer);",
            "\t\tts->tick_stopped = 1;",
            "\t\ttrace_tick_stop(1, TICK_DEP_MASK_NONE);",
            "\t}",
            "",
            "\tts->next_tick = tick;",
            "",
            "\t/*",
            "\t * If the expiration time == KTIME_MAX, then we simply stop",
            "\t * the tick timer.",
            "\t */",
            "\tif (unlikely(expires == KTIME_MAX)) {",
            "\t\tif (ts->nohz_mode == NOHZ_MODE_HIGHRES)",
            "\t\t\thrtimer_cancel(&ts->sched_timer);",
            "\t\telse",
            "\t\t\ttick_program_event(KTIME_MAX, 1);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (ts->nohz_mode == NOHZ_MODE_HIGHRES) {",
            "\t\thrtimer_start(&ts->sched_timer, tick,",
            "\t\t\t      HRTIMER_MODE_ABS_PINNED_HARD);",
            "\t} else {",
            "\t\thrtimer_set_expires(&ts->sched_timer, tick);",
            "\t\ttick_program_event(tick, 1);",
            "\t}",
            "}"
          ],
          "function_name": "local_timer_softirq_pending, tick_nohz_next_event, tick_nohz_stop_tick",
          "description": "tick_nohz_next_event计算下一事件时间点并更新ts->timer_expires字段，tick_nohz_stop_tick根据条件判断是否停止定时器并更新相关状态，通过local_timer_softirq_pending检测是否存在需要立即处理的软中断，影响定时器停止决策",
          "similarity": 0.5600637793540955
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/time/tick-sched.c",
          "start_line": 1400,
          "end_line": 1518,
          "content": [
            "static inline void tick_nohz_activate(struct tick_sched *ts, int mode)",
            "{",
            "\tif (!tick_nohz_enabled)",
            "\t\treturn;",
            "\tts->nohz_mode = mode;",
            "\t/* One update is enough */",
            "\tif (!test_and_set_bit(0, &tick_nohz_active))",
            "\t\ttimers_update_nohz();",
            "}",
            "static void tick_nohz_switch_to_nohz(void)",
            "{",
            "\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);",
            "\tktime_t next;",
            "",
            "\tif (!tick_nohz_enabled)",
            "\t\treturn;",
            "",
            "\tif (tick_switch_to_oneshot(tick_nohz_handler))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Recycle the hrtimer in ts, so we can share the",
            "\t * hrtimer_forward with the highres code.",
            "\t */",
            "\thrtimer_init(&ts->sched_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_HARD);",
            "\t/* Get the next period */",
            "\tnext = tick_init_jiffy_update();",
            "",
            "\thrtimer_set_expires(&ts->sched_timer, next);",
            "\thrtimer_forward_now(&ts->sched_timer, TICK_NSEC);",
            "\ttick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);",
            "\ttick_nohz_activate(ts, NOHZ_MODE_LOWRES);",
            "}",
            "static inline void tick_nohz_irq_enter(void)",
            "{",
            "\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);",
            "\tktime_t now;",
            "",
            "\tif (!ts->idle_active && !ts->tick_stopped)",
            "\t\treturn;",
            "\tnow = ktime_get();",
            "\tif (ts->idle_active)",
            "\t\ttick_nohz_stop_idle(ts, now);",
            "\t/*",
            "\t * If all CPUs are idle. We may need to update a stale jiffies value.",
            "\t * Note nohz_full is a special case: a timekeeper is guaranteed to stay",
            "\t * alive but it might be busy looping with interrupts disabled in some",
            "\t * rare case (typically stop machine). So we must make sure we have a",
            "\t * last resort.",
            "\t */",
            "\tif (ts->tick_stopped)",
            "\t\ttick_nohz_update_jiffies(now);",
            "}",
            "static inline void tick_nohz_switch_to_nohz(void) { }",
            "static inline void tick_nohz_irq_enter(void) { }",
            "static inline void tick_nohz_activate(struct tick_sched *ts, int mode) { }",
            "void tick_irq_enter(void)",
            "{",
            "\ttick_check_oneshot_broadcast_this_cpu();",
            "\ttick_nohz_irq_enter();",
            "}",
            "static enum hrtimer_restart tick_sched_timer(struct hrtimer *timer)",
            "{",
            "\tstruct tick_sched *ts =",
            "\t\tcontainer_of(timer, struct tick_sched, sched_timer);",
            "\tstruct pt_regs *regs = get_irq_regs();",
            "\tktime_t now = ktime_get();",
            "",
            "\ttick_sched_do_timer(ts, now);",
            "",
            "\t/*",
            "\t * Do not call, when we are not in irq context and have",
            "\t * no valid regs pointer",
            "\t */",
            "\tif (regs)",
            "\t\ttick_sched_handle(ts, regs);",
            "\telse",
            "\t\tts->next_tick = 0;",
            "",
            "\t/* No need to reprogram if we are in idle or full dynticks mode */",
            "\tif (unlikely(ts->tick_stopped))",
            "\t\treturn HRTIMER_NORESTART;",
            "",
            "\thrtimer_forward(timer, now, TICK_NSEC);",
            "",
            "\treturn HRTIMER_RESTART;",
            "}",
            "static int __init skew_tick(char *str)",
            "{",
            "\tget_option(&str, &sched_skew_tick);",
            "",
            "\treturn 0;",
            "}",
            "void tick_setup_sched_timer(void)",
            "{",
            "\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);",
            "\tktime_t now = ktime_get();",
            "",
            "\t/*",
            "\t * Emulate tick processing via per-CPU hrtimers:",
            "\t */",
            "\thrtimer_init(&ts->sched_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_HARD);",
            "\tts->sched_timer.function = tick_sched_timer;",
            "",
            "\t/* Get the next period (per-CPU) */",
            "\thrtimer_set_expires(&ts->sched_timer, tick_init_jiffy_update());",
            "",
            "\t/* Offset the tick to avert jiffies_lock contention. */",
            "\tif (sched_skew_tick) {",
            "\t\tu64 offset = TICK_NSEC >> 1;",
            "\t\tdo_div(offset, num_possible_cpus());",
            "\t\toffset *= smp_processor_id();",
            "\t\thrtimer_add_expires_ns(&ts->sched_timer, offset);",
            "\t}",
            "",
            "\thrtimer_forward(&ts->sched_timer, now, TICK_NSEC);",
            "\thrtimer_start_expires(&ts->sched_timer, HRTIMER_MODE_ABS_PINNED_HARD);",
            "\ttick_nohz_activate(ts, NOHZ_MODE_HIGHRES);",
            "}"
          ],
          "function_name": "tick_nohz_activate, tick_nohz_switch_to_nohz, tick_nohz_irq_enter, tick_nohz_switch_to_nohz, tick_nohz_irq_enter, tick_nohz_activate, tick_irq_enter, tick_sched_timer, skew_tick, tick_setup_sched_timer",
          "description": "实现无时钟模式下定时器激活与中断处理逻辑，通过hrtimer管理动态时钟更新，处理空闲态停止、jiffies更新及模式切换操作",
          "similarity": 0.5574852824211121
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/time/tick-sched.c",
          "start_line": 436,
          "end_line": 574,
          "content": [
            "void tick_nohz_dep_set(enum tick_dep_bits bit)",
            "{",
            "\ttick_nohz_dep_set_all(&tick_dep_mask, bit);",
            "}",
            "void tick_nohz_dep_clear(enum tick_dep_bits bit)",
            "{",
            "\tatomic_andnot(BIT(bit), &tick_dep_mask);",
            "}",
            "void tick_nohz_dep_set_cpu(int cpu, enum tick_dep_bits bit)",
            "{",
            "\tint prev;",
            "\tstruct tick_sched *ts;",
            "",
            "\tts = per_cpu_ptr(&tick_cpu_sched, cpu);",
            "",
            "\tprev = atomic_fetch_or(BIT(bit), &ts->tick_dep_mask);",
            "\tif (!prev) {",
            "\t\tpreempt_disable();",
            "\t\t/* Perf needs local kick that is NMI safe */",
            "\t\tif (cpu == smp_processor_id()) {",
            "\t\t\ttick_nohz_full_kick();",
            "\t\t} else {",
            "\t\t\t/* Remote irq work not NMI-safe */",
            "\t\t\tif (!WARN_ON_ONCE(in_nmi()))",
            "\t\t\t\ttick_nohz_full_kick_cpu(cpu);",
            "\t\t}",
            "\t\tpreempt_enable();",
            "\t}",
            "}",
            "void tick_nohz_dep_clear_cpu(int cpu, enum tick_dep_bits bit)",
            "{",
            "\tstruct tick_sched *ts = per_cpu_ptr(&tick_cpu_sched, cpu);",
            "",
            "\tatomic_andnot(BIT(bit), &ts->tick_dep_mask);",
            "}",
            "void tick_nohz_dep_set_task(struct task_struct *tsk, enum tick_dep_bits bit)",
            "{",
            "\tif (!atomic_fetch_or(BIT(bit), &tsk->tick_dep_mask))",
            "\t\ttick_nohz_kick_task(tsk);",
            "}",
            "void tick_nohz_dep_clear_task(struct task_struct *tsk, enum tick_dep_bits bit)",
            "{",
            "\tatomic_andnot(BIT(bit), &tsk->tick_dep_mask);",
            "}",
            "void tick_nohz_dep_set_signal(struct task_struct *tsk,",
            "\t\t\t      enum tick_dep_bits bit)",
            "{",
            "\tint prev;",
            "\tstruct signal_struct *sig = tsk->signal;",
            "",
            "\tprev = atomic_fetch_or(BIT(bit), &sig->tick_dep_mask);",
            "\tif (!prev) {",
            "\t\tstruct task_struct *t;",
            "",
            "\t\tlockdep_assert_held(&tsk->sighand->siglock);",
            "\t\t__for_each_thread(sig, t)",
            "\t\t\ttick_nohz_kick_task(t);",
            "\t}",
            "}",
            "void tick_nohz_dep_clear_signal(struct signal_struct *sig, enum tick_dep_bits bit)",
            "{",
            "\tatomic_andnot(BIT(bit), &sig->tick_dep_mask);",
            "}",
            "void __tick_nohz_task_switch(void)",
            "{",
            "\tstruct tick_sched *ts;",
            "",
            "\tif (!tick_nohz_full_cpu(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\tts = this_cpu_ptr(&tick_cpu_sched);",
            "",
            "\tif (ts->tick_stopped) {",
            "\t\tif (atomic_read(&current->tick_dep_mask) ||",
            "\t\t    atomic_read(&current->signal->tick_dep_mask))",
            "\t\t\ttick_nohz_full_kick();",
            "\t}",
            "}",
            "void __init tick_nohz_full_setup(cpumask_var_t cpumask)",
            "{",
            "\talloc_bootmem_cpumask_var(&tick_nohz_full_mask);",
            "\tcpumask_copy(tick_nohz_full_mask, cpumask);",
            "\ttick_nohz_full_running = true;",
            "}",
            "bool tick_nohz_cpu_hotpluggable(unsigned int cpu)",
            "{",
            "\t/*",
            "\t * The tick_do_timer_cpu CPU handles housekeeping duty (unbound",
            "\t * timers, workqueues, timekeeping, ...) on behalf of full dynticks",
            "\t * CPUs. It must remain online when nohz full is enabled.",
            "\t */",
            "\tif (tick_nohz_full_running && tick_do_timer_cpu == cpu)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "static int tick_nohz_cpu_down(unsigned int cpu)",
            "{",
            "\treturn tick_nohz_cpu_hotpluggable(cpu) ? 0 : -EBUSY;",
            "}",
            "void __init tick_nohz_init(void)",
            "{",
            "\tint cpu, ret;",
            "",
            "\tif (!tick_nohz_full_running)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Full dynticks uses irq work to drive the tick rescheduling on safe",
            "\t * locking contexts. But then we need irq work to raise its own",
            "\t * interrupts to avoid circular dependency on the tick",
            "\t */",
            "\tif (!arch_irq_work_has_interrupt()) {",
            "\t\tpr_warn(\"NO_HZ: Can't run full dynticks because arch doesn't support irq work self-IPIs\\n\");",
            "\t\tcpumask_clear(tick_nohz_full_mask);",
            "\t\ttick_nohz_full_running = false;",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_PM_SLEEP_SMP) &&",
            "\t\t\t!IS_ENABLED(CONFIG_PM_SLEEP_SMP_NONZERO_CPU)) {",
            "\t\tcpu = smp_processor_id();",
            "",
            "\t\tif (cpumask_test_cpu(cpu, tick_nohz_full_mask)) {",
            "\t\t\tpr_warn(\"NO_HZ: Clearing %d from nohz_full range \"",
            "\t\t\t\t\"for timekeeping\\n\", cpu);",
            "\t\t\tcpumask_clear_cpu(cpu, tick_nohz_full_mask);",
            "\t\t}",
            "\t}",
            "",
            "\tfor_each_cpu(cpu, tick_nohz_full_mask)",
            "\t\tct_cpu_track_user(cpu);",
            "",
            "\tret = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,",
            "\t\t\t\t\t\"kernel/nohz:predown\", NULL,",
            "\t\t\t\t\ttick_nohz_cpu_down);",
            "\tWARN_ON(ret < 0);",
            "\tpr_info(\"NO_HZ: Full dynticks CPUs: %*pbl.\\n\",",
            "\t\tcpumask_pr_args(tick_nohz_full_mask));",
            "}"
          ],
          "function_name": "tick_nohz_dep_set, tick_nohz_dep_clear, tick_nohz_dep_set_cpu, tick_nohz_dep_clear_cpu, tick_nohz_dep_set_task, tick_nohz_dep_clear_task, tick_nohz_dep_set_signal, tick_nohz_dep_clear_signal, __tick_nohz_task_switch, tick_nohz_full_setup, tick_nohz_cpu_hotpluggable, tick_nohz_cpu_down, tick_nohz_init",
          "description": "提供tick依赖项的增删改查接口，初始化nohz_full模式所需的数据结构和CPU掩码，处理动态tick模式的热插拔和资源分配",
          "similarity": 0.5484088659286499
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5803831815719604,
      "chunks": [
        {
          "chunk_id": 11,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2012,
          "end_line": 2249,
          "content": [
            "static void",
            "rcu_report_qs_rdp(struct rcu_data *rdp)",
            "{",
            "\tunsigned long flags;",
            "\tunsigned long mask;",
            "\tbool needacc = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tWARN_ON_ONCE(rdp->cpu != smp_processor_id());",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\tif (rdp->cpu_no_qs.b.norm || rdp->gp_seq != rnp->gp_seq ||",
            "\t    rdp->gpwrap) {",
            "",
            "\t\t/*",
            "\t\t * The grace period in which this quiescent state was",
            "\t\t * recorded has ended, so don't report it upwards.",
            "\t\t * We will instead need a new quiescent state that lies",
            "\t\t * within the current grace period.",
            "\t\t */",
            "\t\trdp->cpu_no_qs.b.norm = true;\t/* need qs for new gp. */",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;",
            "\t}",
            "\tmask = rdp->grpmask;",
            "\trdp->core_needs_qs = false;",
            "\tif ((rnp->qsmask & mask) == 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t} else {",
            "\t\t/*",
            "\t\t * This GP can't end until cpu checks in, so all of our",
            "\t\t * callbacks can be processed during the next GP.",
            "\t\t *",
            "\t\t * NOCB kthreads have their own way to deal with that...",
            "\t\t */",
            "\t\tif (!rcu_rdp_is_offloaded(rdp)) {",
            "\t\t\t/*",
            "\t\t\t * The current GP has not yet ended, so it",
            "\t\t\t * should not be possible for rcu_accelerate_cbs()",
            "\t\t\t * to return true.  So complain, but don't awaken.",
            "\t\t\t */",
            "\t\t\tWARN_ON_ONCE(rcu_accelerate_cbs(rnp, rdp));",
            "\t\t} else if (!rcu_segcblist_completely_offloaded(&rdp->cblist)) {",
            "\t\t\t/*",
            "\t\t\t * ...but NOCB kthreads may miss or delay callbacks acceleration",
            "\t\t\t * if in the middle of a (de-)offloading process.",
            "\t\t\t */",
            "\t\t\tneedacc = true;",
            "\t\t}",
            "",
            "\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\t/* ^^^ Released rnp->lock */",
            "",
            "\t\tif (needacc) {",
            "\t\t\trcu_nocb_lock_irqsave(rdp, flags);",
            "\t\t\trcu_accelerate_cbs_unlocked(rnp, rdp);",
            "\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "\t\t}",
            "\t}",
            "}",
            "static void",
            "rcu_check_quiescent_state(struct rcu_data *rdp)",
            "{",
            "\t/* Check for grace-period ends and beginnings. */",
            "\tnote_gp_changes(rdp);",
            "",
            "\t/*",
            "\t * Does this CPU still need to do its part for current grace period?",
            "\t * If no, return and let the other CPUs do their part as well.",
            "\t */",
            "\tif (!rdp->core_needs_qs)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Was there a quiescent state since the beginning of the grace",
            "\t * period? If no, then exit and wait for the next call.",
            "\t */",
            "\tif (rdp->cpu_no_qs.b.norm)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Tell RCU we are done (but rcu_report_qs_rdp() will be the",
            "\t * judge of that).",
            "\t */",
            "\trcu_report_qs_rdp(rdp);",
            "}",
            "static bool rcu_do_batch_check_time(long count, long tlimit,",
            "\t\t\t\t    bool jlimit_check, unsigned long jlimit)",
            "{",
            "\t// Invoke local_clock() only once per 32 consecutive callbacks.",
            "\treturn unlikely(tlimit) &&",
            "\t       (!likely(count & 31) ||",
            "\t\t(IS_ENABLED(CONFIG_RCU_DOUBLE_CHECK_CB_TIME) &&",
            "\t\t jlimit_check && time_after(jiffies, jlimit))) &&",
            "\t       local_clock() >= tlimit;",
            "}",
            "static void rcu_do_batch(struct rcu_data *rdp)",
            "{",
            "\tlong bl;",
            "\tlong count = 0;",
            "\tint div;",
            "\tbool __maybe_unused empty;",
            "\tunsigned long flags;",
            "\tunsigned long jlimit;",
            "\tbool jlimit_check = false;",
            "\tlong pending;",
            "\tstruct rcu_cblist rcl = RCU_CBLIST_INITIALIZER(rcl);",
            "\tstruct rcu_head *rhp;",
            "\tlong tlimit = 0;",
            "",
            "\t/* If no callbacks are ready, just return. */",
            "\tif (!rcu_segcblist_ready_cbs(&rdp->cblist)) {",
            "\t\ttrace_rcu_batch_start(rcu_state.name,",
            "\t\t\t\t      rcu_segcblist_n_cbs(&rdp->cblist), 0);",
            "\t\ttrace_rcu_batch_end(rcu_state.name, 0,",
            "\t\t\t\t    !rcu_segcblist_empty(&rdp->cblist),",
            "\t\t\t\t    need_resched(), is_idle_task(current),",
            "\t\t\t\t    rcu_is_callbacks_kthread(rdp));",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Extract the list of ready callbacks, disabling IRQs to prevent",
            "\t * races with call_rcu() from interrupt handlers.  Leave the",
            "\t * callback counts, as rcu_barrier() needs to be conservative.",
            "\t */",
            "\trcu_nocb_lock_irqsave(rdp, flags);",
            "\tWARN_ON_ONCE(cpu_is_offline(smp_processor_id()));",
            "\tpending = rcu_segcblist_get_seglen(&rdp->cblist, RCU_DONE_TAIL);",
            "\tdiv = READ_ONCE(rcu_divisor);",
            "\tdiv = div < 0 ? 7 : div > sizeof(long) * 8 - 2 ? sizeof(long) * 8 - 2 : div;",
            "\tbl = max(rdp->blimit, pending >> div);",
            "\tif ((in_serving_softirq() || rdp->rcu_cpu_kthread_status == RCU_KTHREAD_RUNNING) &&",
            "\t    (IS_ENABLED(CONFIG_RCU_DOUBLE_CHECK_CB_TIME) || unlikely(bl > 100))) {",
            "\t\tconst long npj = NSEC_PER_SEC / HZ;",
            "\t\tlong rrn = READ_ONCE(rcu_resched_ns);",
            "",
            "\t\trrn = rrn < NSEC_PER_MSEC ? NSEC_PER_MSEC : rrn > NSEC_PER_SEC ? NSEC_PER_SEC : rrn;",
            "\t\ttlimit = local_clock() + rrn;",
            "\t\tjlimit = jiffies + (rrn + npj + 1) / npj;",
            "\t\tjlimit_check = true;",
            "\t}",
            "\ttrace_rcu_batch_start(rcu_state.name,",
            "\t\t\t      rcu_segcblist_n_cbs(&rdp->cblist), bl);",
            "\trcu_segcblist_extract_done_cbs(&rdp->cblist, &rcl);",
            "\tif (rcu_rdp_is_offloaded(rdp))",
            "\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCbDequeued\"));",
            "\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "",
            "\t/* Invoke callbacks. */",
            "\ttick_dep_set_task(current, TICK_DEP_BIT_RCU);",
            "\trhp = rcu_cblist_dequeue(&rcl);",
            "",
            "\tfor (; rhp; rhp = rcu_cblist_dequeue(&rcl)) {",
            "\t\trcu_callback_t f;",
            "",
            "\t\tcount++;",
            "\t\tdebug_rcu_head_unqueue(rhp);",
            "",
            "\t\trcu_lock_acquire(&rcu_callback_map);",
            "\t\ttrace_rcu_invoke_callback(rcu_state.name, rhp);",
            "",
            "\t\tf = rhp->func;",
            "\t\tdebug_rcu_head_callback(rhp);",
            "\t\tWRITE_ONCE(rhp->func, (rcu_callback_t)0L);",
            "\t\tf(rhp);",
            "",
            "\t\trcu_lock_release(&rcu_callback_map);",
            "",
            "\t\t/*",
            "\t\t * Stop only if limit reached and CPU has something to do.",
            "\t\t */",
            "\t\tif (in_serving_softirq()) {",
            "\t\t\tif (count >= bl && (need_resched() || !is_idle_task(current)))",
            "\t\t\t\tbreak;",
            "\t\t\t/*",
            "\t\t\t * Make sure we don't spend too much time here and deprive other",
            "\t\t\t * softirq vectors of CPU cycles.",
            "\t\t\t */",
            "\t\t\tif (rcu_do_batch_check_time(count, tlimit, jlimit_check, jlimit))",
            "\t\t\t\tbreak;",
            "\t\t} else {",
            "\t\t\t// In rcuc/rcuoc context, so no worries about",
            "\t\t\t// depriving other softirq vectors of CPU cycles.",
            "\t\t\tlocal_bh_enable();",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\t\tlocal_bh_disable();",
            "\t\t\t// But rcuc kthreads can delay quiescent-state",
            "\t\t\t// reporting, so check time limits for them.",
            "\t\t\tif (rdp->rcu_cpu_kthread_status == RCU_KTHREAD_RUNNING &&",
            "\t\t\t    rcu_do_batch_check_time(count, tlimit, jlimit_check, jlimit)) {",
            "\t\t\t\trdp->rcu_cpu_has_work = 1;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\trcu_nocb_lock_irqsave(rdp, flags);",
            "\trdp->n_cbs_invoked += count;",
            "\ttrace_rcu_batch_end(rcu_state.name, count, !!rcl.head, need_resched(),",
            "\t\t\t    is_idle_task(current), rcu_is_callbacks_kthread(rdp));",
            "",
            "\t/* Update counts and requeue any remaining callbacks. */",
            "\trcu_segcblist_insert_done_cbs(&rdp->cblist, &rcl);",
            "\trcu_segcblist_add_len(&rdp->cblist, -count);",
            "",
            "\t/* Reinstate batch limit if we have worked down the excess. */",
            "\tcount = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\tif (rdp->blimit >= DEFAULT_MAX_RCU_BLIMIT && count <= qlowmark)",
            "\t\trdp->blimit = blimit;",
            "",
            "\t/* Reset ->qlen_last_fqs_check trigger if enough CBs have drained. */",
            "\tif (count == 0 && rdp->qlen_last_fqs_check != 0) {",
            "\t\trdp->qlen_last_fqs_check = 0;",
            "\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t} else if (count < rdp->qlen_last_fqs_check - qhimark)",
            "\t\trdp->qlen_last_fqs_check = count;",
            "",
            "\t/*",
            "\t * The following usually indicates a double call_rcu().  To track",
            "\t * this down, try building with CONFIG_DEBUG_OBJECTS_RCU_HEAD=y.",
            "\t */",
            "\tempty = rcu_segcblist_empty(&rdp->cblist);",
            "\tWARN_ON_ONCE(count == 0 && !empty);",
            "\tWARN_ON_ONCE(!IS_ENABLED(CONFIG_RCU_NOCB_CPU) &&",
            "\t\t     count != 0 && empty);",
            "\tWARN_ON_ONCE(count == 0 && rcu_segcblist_n_segment_cbs(&rdp->cblist) != 0);",
            "\tWARN_ON_ONCE(!empty && rcu_segcblist_n_segment_cbs(&rdp->cblist) == 0);",
            "",
            "\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "",
            "\ttick_dep_clear_task(current, TICK_DEP_BIT_RCU);",
            "}"
          ],
          "function_name": "rcu_report_qs_rdp, rcu_check_quiescent_state, rcu_do_batch_check_time, rcu_do_batch",
          "description": "提供CPU级quiescent状态检测与回调处理机制，包含quiescent状态上报、回调批量处理及性能监控功能，支持动态调整批处理参数",
          "similarity": 0.6293238997459412
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 658,
          "end_line": 845,
          "content": [
            "int rcu_needs_cpu(void)",
            "{",
            "\treturn !rcu_segcblist_empty(&this_cpu_ptr(&rcu_data)->cblist) &&",
            "\t\t!rcu_rdp_is_offloaded(this_cpu_ptr(&rcu_data));",
            "}",
            "static void rcu_disable_urgency_upon_qs(struct rcu_data *rdp)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rdp->mynode);",
            "\tWRITE_ONCE(rdp->rcu_urgent_qs, false);",
            "\tWRITE_ONCE(rdp->rcu_need_heavy_qs, false);",
            "\tif (tick_nohz_full_cpu(rdp->cpu) && rdp->rcu_forced_tick) {",
            "\t\ttick_dep_clear_cpu(rdp->cpu, TICK_DEP_BIT_RCU);",
            "\t\tWRITE_ONCE(rdp->rcu_forced_tick, false);",
            "\t}",
            "}",
            "notrace bool rcu_is_watching(void)",
            "{",
            "\tbool ret;",
            "",
            "\tpreempt_disable_notrace();",
            "\tret = !rcu_dynticks_curr_cpu_in_eqs();",
            "\tpreempt_enable_notrace();",
            "\treturn ret;",
            "}",
            "void rcu_request_urgent_qs_task(struct task_struct *t)",
            "{",
            "\tint cpu;",
            "",
            "\tbarrier();",
            "\tcpu = task_cpu(t);",
            "\tif (!task_curr(t))",
            "\t\treturn; /* This task is not running on that CPU. */",
            "\tsmp_store_release(per_cpu_ptr(&rcu_data.rcu_urgent_qs, cpu), true);",
            "}",
            "static void rcu_gpnum_ovf(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (ULONG_CMP_LT(rcu_seq_current(&rdp->gp_seq) + ULONG_MAX / 4,",
            "\t\t\t rnp->gp_seq))",
            "\t\tWRITE_ONCE(rdp->gpwrap, true);",
            "\tif (ULONG_CMP_LT(rdp->rcu_iw_gp_seq + ULONG_MAX / 4, rnp->gp_seq))",
            "\t\trdp->rcu_iw_gp_seq = rnp->gp_seq + ULONG_MAX / 4;",
            "}",
            "static int dyntick_save_progress_counter(struct rcu_data *rdp)",
            "{",
            "\trdp->dynticks_snap = rcu_dynticks_snap(rdp->cpu);",
            "\tif (rcu_dynticks_in_eqs(rdp->dynticks_snap)) {",
            "\t\ttrace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS(\"dti\"));",
            "\t\trcu_gpnum_ovf(rdp->mynode, rdp);",
            "\t\treturn 1;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int rcu_implicit_dynticks_qs(struct rcu_data *rdp)",
            "{",
            "\tunsigned long jtsq;",
            "\tint ret = 0;",
            "\tstruct rcu_node *rnp = rdp->mynode;",
            "",
            "\t/*",
            "\t * If the CPU passed through or entered a dynticks idle phase with",
            "\t * no active irq/NMI handlers, then we can safely pretend that the CPU",
            "\t * already acknowledged the request to pass through a quiescent",
            "\t * state.  Either way, that CPU cannot possibly be in an RCU",
            "\t * read-side critical section that started before the beginning",
            "\t * of the current RCU grace period.",
            "\t */",
            "\tif (rcu_dynticks_in_eqs_since(rdp, rdp->dynticks_snap)) {",
            "\t\ttrace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS(\"dti\"));",
            "\t\trcu_gpnum_ovf(rnp, rdp);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\t/*",
            "\t * Complain if a CPU that is considered to be offline from RCU's",
            "\t * perspective has not yet reported a quiescent state.  After all,",
            "\t * the offline CPU should have reported a quiescent state during",
            "\t * the CPU-offline process, or, failing that, by rcu_gp_init()",
            "\t * if it ran concurrently with either the CPU going offline or the",
            "\t * last task on a leaf rcu_node structure exiting its RCU read-side",
            "\t * critical section while all CPUs corresponding to that structure",
            "\t * are offline.  This added warning detects bugs in any of these",
            "\t * code paths.",
            "\t *",
            "\t * The rcu_node structure's ->lock is held here, which excludes",
            "\t * the relevant portions the CPU-hotplug code, the grace-period",
            "\t * initialization code, and the rcu_read_unlock() code paths.",
            "\t *",
            "\t * For more detail, please refer to the \"Hotplug CPU\" section",
            "\t * of RCU's Requirements documentation.",
            "\t */",
            "\tif (WARN_ON_ONCE(!rcu_rdp_cpu_online(rdp))) {",
            "\t\tstruct rcu_node *rnp1;",
            "",
            "\t\tpr_info(\"%s: grp: %d-%d level: %d ->gp_seq %ld ->completedqs %ld\\n\",",
            "\t\t\t__func__, rnp->grplo, rnp->grphi, rnp->level,",
            "\t\t\t(long)rnp->gp_seq, (long)rnp->completedqs);",
            "\t\tfor (rnp1 = rnp; rnp1; rnp1 = rnp1->parent)",
            "\t\t\tpr_info(\"%s: %d:%d ->qsmask %#lx ->qsmaskinit %#lx ->qsmaskinitnext %#lx ->rcu_gp_init_mask %#lx\\n\",",
            "\t\t\t\t__func__, rnp1->grplo, rnp1->grphi, rnp1->qsmask, rnp1->qsmaskinit, rnp1->qsmaskinitnext, rnp1->rcu_gp_init_mask);",
            "\t\tpr_info(\"%s %d: %c online: %ld(%d) offline: %ld(%d)\\n\",",
            "\t\t\t__func__, rdp->cpu, \".o\"[rcu_rdp_cpu_online(rdp)],",
            "\t\t\t(long)rdp->rcu_onl_gp_seq, rdp->rcu_onl_gp_flags,",
            "\t\t\t(long)rdp->rcu_ofl_gp_seq, rdp->rcu_ofl_gp_flags);",
            "\t\treturn 1; /* Break things loose after complaining. */",
            "\t}",
            "",
            "\t/*",
            "\t * A CPU running for an extended time within the kernel can",
            "\t * delay RCU grace periods: (1) At age jiffies_to_sched_qs,",
            "\t * set .rcu_urgent_qs, (2) At age 2*jiffies_to_sched_qs, set",
            "\t * both .rcu_need_heavy_qs and .rcu_urgent_qs.  Note that the",
            "\t * unsynchronized assignments to the per-CPU rcu_need_heavy_qs",
            "\t * variable are safe because the assignments are repeated if this",
            "\t * CPU failed to pass through a quiescent state.  This code",
            "\t * also checks .jiffies_resched in case jiffies_to_sched_qs",
            "\t * is set way high.",
            "\t */",
            "\tjtsq = READ_ONCE(jiffies_to_sched_qs);",
            "\tif (!READ_ONCE(rdp->rcu_need_heavy_qs) &&",
            "\t    (time_after(jiffies, rcu_state.gp_start + jtsq * 2) ||",
            "\t     time_after(jiffies, rcu_state.jiffies_resched) ||",
            "\t     rcu_state.cbovld)) {",
            "\t\tWRITE_ONCE(rdp->rcu_need_heavy_qs, true);",
            "\t\t/* Store rcu_need_heavy_qs before rcu_urgent_qs. */",
            "\t\tsmp_store_release(&rdp->rcu_urgent_qs, true);",
            "\t} else if (time_after(jiffies, rcu_state.gp_start + jtsq)) {",
            "\t\tWRITE_ONCE(rdp->rcu_urgent_qs, true);",
            "\t}",
            "",
            "\t/*",
            "\t * NO_HZ_FULL CPUs can run in-kernel without rcu_sched_clock_irq!",
            "\t * The above code handles this, but only for straight cond_resched().",
            "\t * And some in-kernel loops check need_resched() before calling",
            "\t * cond_resched(), which defeats the above code for CPUs that are",
            "\t * running in-kernel with scheduling-clock interrupts disabled.",
            "\t * So hit them over the head with the resched_cpu() hammer!",
            "\t */",
            "\tif (tick_nohz_full_cpu(rdp->cpu) &&",
            "\t    (time_after(jiffies, READ_ONCE(rdp->last_fqs_resched) + jtsq * 3) ||",
            "\t     rcu_state.cbovld)) {",
            "\t\tWRITE_ONCE(rdp->rcu_urgent_qs, true);",
            "\t\tWRITE_ONCE(rdp->last_fqs_resched, jiffies);",
            "\t\tret = -1;",
            "\t}",
            "",
            "\t/*",
            "\t * If more than halfway to RCU CPU stall-warning time, invoke",
            "\t * resched_cpu() more frequently to try to loosen things up a bit.",
            "\t * Also check to see if the CPU is getting hammered with interrupts,",
            "\t * but only once per grace period, just to keep the IPIs down to",
            "\t * a dull roar.",
            "\t */",
            "\tif (time_after(jiffies, rcu_state.jiffies_resched)) {",
            "\t\tif (time_after(jiffies,",
            "\t\t\t       READ_ONCE(rdp->last_fqs_resched) + jtsq)) {",
            "\t\t\tWRITE_ONCE(rdp->last_fqs_resched, jiffies);",
            "\t\t\tret = -1;",
            "\t\t}",
            "\t\tif (IS_ENABLED(CONFIG_IRQ_WORK) &&",
            "\t\t    !rdp->rcu_iw_pending && rdp->rcu_iw_gp_seq != rnp->gp_seq &&",
            "\t\t    (rnp->ffmask & rdp->grpmask)) {",
            "\t\t\trdp->rcu_iw_pending = true;",
            "\t\t\trdp->rcu_iw_gp_seq = rnp->gp_seq;",
            "\t\t\tirq_work_queue_on(&rdp->rcu_iw, rdp->cpu);",
            "\t\t}",
            "",
            "\t\tif (rcu_cpu_stall_cputime && rdp->snap_record.gp_seq != rdp->gp_seq) {",
            "\t\t\tint cpu = rdp->cpu;",
            "\t\t\tstruct rcu_snap_record *rsrp;",
            "\t\t\tstruct kernel_cpustat *kcsp;",
            "",
            "\t\t\tkcsp = &kcpustat_cpu(cpu);",
            "",
            "\t\t\trsrp = &rdp->snap_record;",
            "\t\t\trsrp->cputime_irq     = kcpustat_field(kcsp, CPUTIME_IRQ, cpu);",
            "\t\t\trsrp->cputime_softirq = kcpustat_field(kcsp, CPUTIME_SOFTIRQ, cpu);",
            "\t\t\trsrp->cputime_system  = kcpustat_field(kcsp, CPUTIME_SYSTEM, cpu);",
            "\t\t\trsrp->nr_hardirqs = kstat_cpu_irqs_sum(cpu) + arch_irq_stat_cpu(cpu);",
            "\t\t\trsrp->nr_softirqs = kstat_cpu_softirqs_sum(cpu);",
            "\t\t\trsrp->nr_csw = nr_context_switches_cpu(cpu);",
            "\t\t\trsrp->jiffies = jiffies;",
            "\t\t\trsrp->gp_seq = rdp->gp_seq;",
            "\t\t}",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "rcu_needs_cpu, rcu_disable_urgency_upon_qs, rcu_is_watching, rcu_request_urgent_qs_task, rcu_gpnum_ovf, dyntick_save_progress_counter, rcu_implicit_dynticks_qs",
          "description": "处理RCU紧迫性需求判定和隐式动态tick quiescent状态检测，通过时间阈值触发CPU唤醒以避免RCU阻塞。",
          "similarity": 0.6061598062515259
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2273,
          "end_line": 2388,
          "content": [
            "void rcu_sched_clock_irq(int user)",
            "{",
            "\tunsigned long j;",
            "",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU)) {",
            "\t\tj = jiffies;",
            "\t\tWARN_ON_ONCE(time_before(j, __this_cpu_read(rcu_data.last_sched_clock)));",
            "\t\t__this_cpu_write(rcu_data.last_sched_clock, j);",
            "\t}",
            "\ttrace_rcu_utilization(TPS(\"Start scheduler-tick\"));",
            "\tlockdep_assert_irqs_disabled();",
            "\traw_cpu_inc(rcu_data.ticks_this_gp);",
            "\t/* The load-acquire pairs with the store-release setting to true. */",
            "\tif (smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs))) {",
            "\t\t/* Idle and userspace execution already are quiescent states. */",
            "\t\tif (!rcu_is_cpu_rrupt_from_idle() && !user) {",
            "\t\t\tset_tsk_need_resched(current);",
            "\t\t\tset_preempt_need_resched();",
            "\t\t}",
            "\t\t__this_cpu_write(rcu_data.rcu_urgent_qs, false);",
            "\t}",
            "\trcu_flavor_sched_clock_irq(user);",
            "\tif (rcu_pending(user))",
            "\t\tinvoke_rcu_core();",
            "\tif (user || rcu_is_cpu_rrupt_from_idle())",
            "\t\trcu_note_voluntary_context_switch(current);",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\ttrace_rcu_utilization(TPS(\"End scheduler-tick\"));",
            "}",
            "static void force_qs_rnp(int (*f)(struct rcu_data *rdp))",
            "{",
            "\tint cpu;",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trcu_state.cbovld = rcu_state.cbovldnext;",
            "\trcu_state.cbovldnext = false;",
            "\trcu_for_each_leaf_node(rnp) {",
            "\t\tunsigned long mask = 0;",
            "\t\tunsigned long rsmask = 0;",
            "",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\trcu_state.cbovldnext |= !!rnp->cbovldmask;",
            "\t\tif (rnp->qsmask == 0) {",
            "\t\t\tif (rcu_preempt_blocked_readers_cgp(rnp)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No point in scanning bits because they",
            "\t\t\t\t * are all zero.  But we might need to",
            "\t\t\t\t * priority-boost blocked readers.",
            "\t\t\t\t */",
            "\t\t\t\trcu_initiate_boost(rnp, flags);",
            "\t\t\t\t/* rcu_initiate_boost() releases rnp->lock */",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rnp->qsmask) {",
            "\t\t\tstruct rcu_data *rdp;",
            "\t\t\tint ret;",
            "",
            "\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\t\t\tret = f(rdp);",
            "\t\t\tif (ret > 0) {",
            "\t\t\t\tmask |= rdp->grpmask;",
            "\t\t\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\t\t}",
            "\t\t\tif (ret < 0)",
            "\t\t\t\trsmask |= rdp->grpmask;",
            "\t\t}",
            "\t\tif (mask != 0) {",
            "\t\t\t/* Idle/offline CPUs, report (releases rnp->lock). */",
            "\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\t} else {",
            "\t\t\t/* Nothing to do here, so just drop the lock. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t}",
            "",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rsmask)",
            "\t\t\tresched_cpu(cpu);",
            "\t}",
            "}",
            "void rcu_force_quiescent_state(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool ret;",
            "\tstruct rcu_node *rnp;",
            "\tstruct rcu_node *rnp_old = NULL;",
            "",
            "\t/* Funnel through hierarchy to reduce memory contention. */",
            "\trnp = raw_cpu_read(rcu_data.mynode);",
            "\tfor (; rnp != NULL; rnp = rnp->parent) {",
            "\t\tret = (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) ||",
            "\t\t       !raw_spin_trylock(&rnp->fqslock);",
            "\t\tif (rnp_old != NULL)",
            "\t\t\traw_spin_unlock(&rnp_old->fqslock);",
            "\t\tif (ret)",
            "\t\t\treturn;",
            "\t\trnp_old = rnp;",
            "\t}",
            "\t/* rnp_old == rcu_get_root(), rnp == NULL. */",
            "",
            "\t/* Reached the root of the rcu_node tree, acquire lock. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp_old, flags);",
            "\traw_spin_unlock(&rnp_old->fqslock);",
            "\tif (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\t\treturn;  /* Someone beat us to it. */",
            "\t}",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\trcu_gp_kthread_wake();",
            "}"
          ],
          "function_name": "rcu_sched_clock_irq, force_qs_rnp, rcu_force_quiescent_state",
          "description": "实现强制quiescent状态触发与调度器中断处理，包含优先级提升、回调加速及grace period推进等关键控制流，维护RCU状态一致性",
          "similarity": 0.5821460485458374
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1221,
          "end_line": 1330,
          "content": [
            "static bool __note_gp_changes(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\tbool ret = false;",
            "\tbool need_qs;",
            "\tconst bool offloaded = rcu_rdp_is_offloaded(rdp);",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\tif (rdp->gp_seq == rnp->gp_seq)",
            "\t\treturn false; /* Nothing to do. */",
            "",
            "\t/* Handle the ends of any preceding grace periods first. */",
            "\tif (rcu_seq_completed_gp(rdp->gp_seq, rnp->gp_seq) ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) {",
            "\t\tif (!offloaded)",
            "\t\t\tret = rcu_advance_cbs(rnp, rdp); /* Advance CBs. */",
            "\t\trdp->core_needs_qs = false;",
            "\t\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuend\"));",
            "\t} else {",
            "\t\tif (!offloaded)",
            "\t\t\tret = rcu_accelerate_cbs(rnp, rdp); /* Recent CBs. */",
            "\t\tif (rdp->core_needs_qs)",
            "\t\t\trdp->core_needs_qs = !!(rnp->qsmask & rdp->grpmask);",
            "\t}",
            "",
            "\t/* Now handle the beginnings of any new-to-this-CPU grace periods. */",
            "\tif (rcu_seq_new_gp(rdp->gp_seq, rnp->gp_seq) ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) {",
            "\t\t/*",
            "\t\t * If the current grace period is waiting for this CPU,",
            "\t\t * set up to detect a quiescent state, otherwise don't",
            "\t\t * go looking for one.",
            "\t\t */",
            "\t\ttrace_rcu_grace_period(rcu_state.name, rnp->gp_seq, TPS(\"cpustart\"));",
            "\t\tneed_qs = !!(rnp->qsmask & rdp->grpmask);",
            "\t\trdp->cpu_no_qs.b.norm = need_qs;",
            "\t\trdp->core_needs_qs = need_qs;",
            "\t\tzero_cpu_stall_ticks(rdp);",
            "\t}",
            "\trdp->gp_seq = rnp->gp_seq;  /* Remember new grace-period state. */",
            "\tif (ULONG_CMP_LT(rdp->gp_seq_needed, rnp->gp_seq_needed) || rdp->gpwrap)",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU) && READ_ONCE(rdp->gpwrap))",
            "\t\tWRITE_ONCE(rdp->last_sched_clock, jiffies);",
            "\tWRITE_ONCE(rdp->gpwrap, false);",
            "\trcu_gpnum_ovf(rnp, rdp);",
            "\treturn ret;",
            "}",
            "static void note_gp_changes(struct rcu_data *rdp)",
            "{",
            "\tunsigned long flags;",
            "\tbool needwake;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tlocal_irq_save(flags);",
            "\trnp = rdp->mynode;",
            "\tif ((rdp->gp_seq == rcu_seq_current(&rnp->gp_seq) &&",
            "\t     !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */",
            "\t    !raw_spin_trylock_rcu_node(rnp)) { /* irqs already off, so later. */",
            "\t\tlocal_irq_restore(flags);",
            "\t\treturn;",
            "\t}",
            "\tneedwake = __note_gp_changes(rnp, rdp);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\trcu_strict_gp_check_qs();",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "}",
            "void rcu_gp_slow_register(atomic_t *rgssp)",
            "{",
            "\tWARN_ON_ONCE(rcu_gp_slow_suppress);",
            "",
            "\tWRITE_ONCE(rcu_gp_slow_suppress, rgssp);",
            "}",
            "void rcu_gp_slow_unregister(atomic_t *rgssp)",
            "{",
            "\tWARN_ON_ONCE(rgssp && rgssp != rcu_gp_slow_suppress && rcu_gp_slow_suppress != NULL);",
            "",
            "\tWRITE_ONCE(rcu_gp_slow_suppress, NULL);",
            "}",
            "static bool rcu_gp_slow_is_suppressed(void)",
            "{",
            "\tatomic_t *rgssp = READ_ONCE(rcu_gp_slow_suppress);",
            "",
            "\treturn rgssp && atomic_read(rgssp);",
            "}",
            "static void rcu_gp_slow(int delay)",
            "{",
            "\tif (!rcu_gp_slow_is_suppressed() && delay > 0 &&",
            "\t    !(rcu_seq_ctr(rcu_state.gp_seq) % (rcu_num_nodes * PER_RCU_NODE_PERIOD * delay)))",
            "\t\tschedule_timeout_idle(delay);",
            "}",
            "void rcu_gp_set_torture_wait(int duration)",
            "{",
            "\tif (IS_ENABLED(CONFIG_RCU_TORTURE_TEST) && duration > 0)",
            "\t\tWRITE_ONCE(sleep_duration, duration);",
            "}",
            "static void rcu_gp_torture_wait(void)",
            "{",
            "\tunsigned long duration;",
            "",
            "\tif (!IS_ENABLED(CONFIG_RCU_TORTURE_TEST))",
            "\t\treturn;",
            "\tduration = xchg(&sleep_duration, 0UL);",
            "\tif (duration > 0) {",
            "\t\tpr_alert(\"%s: Waiting %lu jiffies\\n\", __func__, duration);",
            "\t\tschedule_timeout_idle(duration);",
            "\t\tpr_alert(\"%s: Wait complete\\n\", __func__);",
            "\t}",
            "}"
          ],
          "function_name": "__note_gp_changes, note_gp_changes, rcu_gp_slow_register, rcu_gp_slow_unregister, rcu_gp_slow_is_suppressed, rcu_gp_slow, rcu_gp_set_torture_wait, rcu_gp_torture_wait",
          "description": "提供慢速模式下的grace period注册/注销接口，通过原子变量控制抑制状态，并实现基于延迟的调度控制逻辑，支持严格模式下的边界检查。",
          "similarity": 0.559307336807251
        },
        {
          "chunk_id": 25,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4602,
          "end_line": 4729,
          "content": [
            "void rcu_cpu_starting(unsigned int cpu)",
            "{",
            "\tunsigned long mask;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "\tbool newcpu;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tif (rdp->cpu_started)",
            "\t\treturn;",
            "\trdp->cpu_started = true;",
            "",
            "\trnp = rdp->mynode;",
            "\tmask = rdp->grpmask;",
            "\tarch_spin_lock(&rcu_state.ofl_lock);",
            "\trcu_dynticks_eqs_online();",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\traw_spin_lock_rcu_node(rnp);",
            "\tWRITE_ONCE(rnp->qsmaskinitnext, rnp->qsmaskinitnext | mask);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "\tnewcpu = !(rnp->expmaskinitnext & mask);",
            "\trnp->expmaskinitnext |= mask;",
            "\t/* Allow lockless access for expedited grace periods. */",
            "\tsmp_store_release(&rcu_state.ncpus, rcu_state.ncpus + newcpu); /* ^^^ */",
            "\tASSERT_EXCLUSIVE_WRITER(rcu_state.ncpus);",
            "\trcu_gpnum_ovf(rnp, rdp); /* Offline-induced counter wrap? */",
            "\trdp->rcu_onl_gp_seq = READ_ONCE(rcu_state.gp_seq);",
            "\trdp->rcu_onl_gp_flags = READ_ONCE(rcu_state.gp_flags);",
            "",
            "\t/* An incoming CPU should never be blocking a grace period. */",
            "\tif (WARN_ON_ONCE(rnp->qsmask & mask)) { /* RCU waiting on incoming CPU? */",
            "\t\t/* rcu_report_qs_rnp() *really* wants some flags to restore */",
            "\t\tunsigned long flags;",
            "",
            "\t\tlocal_irq_save(flags);",
            "\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\t/* Report QS -after- changing ->qsmaskinitnext! */",
            "\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t} else {",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\t}",
            "\tarch_spin_unlock(&rcu_state.ofl_lock);",
            "\tsmp_store_release(&rdp->beenonline, true);",
            "\tsmp_mb(); /* Ensure RCU read-side usage follows above initialization. */",
            "}",
            "void rcu_report_dead(unsigned int cpu)",
            "{",
            "\tunsigned long flags, seq_flags;",
            "\tunsigned long mask;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tstruct rcu_node *rnp = rdp->mynode;  /* Outgoing CPU's rdp & rnp. */",
            "",
            "\t// Do any dangling deferred wakeups.",
            "\tdo_nocb_deferred_wakeup(rdp);",
            "",
            "\trcu_preempt_deferred_qs(current);",
            "",
            "\t/* Remove outgoing CPU from mask in the leaf rcu_node structure. */",
            "\tmask = rdp->grpmask;",
            "\tlocal_irq_save(seq_flags);",
            "\tarch_spin_lock(&rcu_state.ofl_lock);",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags); /* Enforce GP memory-order guarantee. */",
            "\trdp->rcu_ofl_gp_seq = READ_ONCE(rcu_state.gp_seq);",
            "\trdp->rcu_ofl_gp_flags = READ_ONCE(rcu_state.gp_flags);",
            "\tif (rnp->qsmask & mask) { /* RCU waiting on outgoing CPU? */",
            "\t\t/* Report quiescent state -before- changing ->qsmaskinitnext! */",
            "\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t}",
            "\tWRITE_ONCE(rnp->qsmaskinitnext, rnp->qsmaskinitnext & ~mask);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tarch_spin_unlock(&rcu_state.ofl_lock);",
            "\tlocal_irq_restore(seq_flags);",
            "",
            "\trdp->cpu_started = false;",
            "}",
            "void rcutree_migrate_callbacks(int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *my_rdp;",
            "\tstruct rcu_node *my_rnp;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tbool needwake;",
            "",
            "\tif (rcu_rdp_is_offloaded(rdp))",
            "\t\treturn;",
            "",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\tif (rcu_segcblist_empty(&rdp->cblist)) {",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\treturn;  /* No callbacks to migrate. */",
            "\t}",
            "",
            "\tWARN_ON_ONCE(rcu_rdp_cpu_online(rdp));",
            "\trcu_barrier_entrain(rdp);",
            "\tmy_rdp = this_cpu_ptr(&rcu_data);",
            "\tmy_rnp = my_rdp->mynode;",
            "\trcu_nocb_lock(my_rdp); /* irqs already disabled. */",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(my_rdp, NULL, jiffies, false));",
            "\traw_spin_lock_rcu_node(my_rnp); /* irqs already disabled. */",
            "\t/* Leverage recent GPs and set GP for new callbacks. */",
            "\tneedwake = rcu_advance_cbs(my_rnp, rdp) ||",
            "\t\t   rcu_advance_cbs(my_rnp, my_rdp);",
            "\trcu_segcblist_merge(&my_rdp->cblist, &rdp->cblist);",
            "\traw_spin_unlock(&rcu_state.barrier_lock); /* irqs remain disabled. */",
            "\tneedwake = needwake || rcu_advance_cbs(my_rnp, my_rdp);",
            "\trcu_segcblist_disable(&rdp->cblist);",
            "\tWARN_ON_ONCE(rcu_segcblist_empty(&my_rdp->cblist) != !rcu_segcblist_n_cbs(&my_rdp->cblist));",
            "\tcheck_cb_ovld_locked(my_rdp, my_rnp);",
            "\tif (rcu_rdp_is_offloaded(my_rdp)) {",
            "\t\traw_spin_unlock_rcu_node(my_rnp); /* irqs remain disabled. */",
            "\t\t__call_rcu_nocb_wake(my_rdp, true, flags);",
            "\t} else {",
            "\t\trcu_nocb_unlock(my_rdp); /* irqs remain disabled. */",
            "\t\traw_spin_unlock_rcu_node(my_rnp); /* irqs remain disabled. */",
            "\t}",
            "\tlocal_irq_restore(flags);",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "\tlockdep_assert_irqs_enabled();",
            "\tWARN_ONCE(rcu_segcblist_n_cbs(&rdp->cblist) != 0 ||",
            "\t\t  !rcu_segcblist_empty(&rdp->cblist),",
            "\t\t  \"rcu_cleanup_dead_cpu: Callbacks on offline CPU %d: qlen=%lu, 1stCB=%p\\n\",",
            "\t\t  cpu, rcu_segcblist_n_cbs(&rdp->cblist),",
            "\t\t  rcu_segcblist_first_cb(&rdp->cblist));",
            "}"
          ],
          "function_name": "rcu_cpu_starting, rcu_report_dead, rcutree_migrate_callbacks",
          "description": "处理CPU上线时的RCU状态更新，通过修改rcu_node结构体中的掩码和序列号，确保并发访问的安全性，并在必要时触发静默状态报告。",
          "similarity": 0.5538699626922607
        }
      ]
    },
    {
      "source_file": "kernel/time/timer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:57:06\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `time\\timer.c`\n\n---\n\n# `time/timer.c` 技术文档\n\n## 1. 文件概述\n\n`time/timer.c` 是 Linux 内核中实现**内核定时器子系统**的核心文件，负责管理基于**定时器轮（timer wheel）** 的动态定时器机制。该文件提供了高效、可扩展的定时器调度框架，支持高精度超时处理、SMP（对称多处理）环境下的 per-CPU 定时器管理，以及与 NO_HZ（动态 tick）节能机制的集成。其设计目标是在保证大多数超时场景（如网络、I/O 超时）性能的同时，通过多级粒度结构避免传统定时器轮中频繁的级联（cascading）操作，从而提升系统可扩展性。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`jiffies_64`**：全局 64 位 jiffies 计数器，记录自系统启动以来的时钟滴答数，对齐缓存行以优化 SMP 访问。\n- **多级定时器轮（Timer Wheel）结构**：\n  - 由 `LVL_DEPTH` 层（通常为 8 或 9）组成，每层包含 `LVL_SIZE`（64）个桶（buckets）。\n  - 每层具有不同的时间粒度（granularity），随层级升高而增大。\n- **定时器基础（Timer Bases）**：\n  - `BASE_STD`：标准定时器基础，用于普通定时器。\n  - `BASE_DEF`：可延迟定时器基础（仅当 `CONFIG_NO_HZ_COMMON` 启用时存在），用于在 CPU 空闲时可推迟执行的定时器。\n\n### 关键宏定义\n- `LVL_CLK_SHIFT` / `LVL_CLK_DIV`：定义层级间的时间粒度缩放因子（默认为 8 倍）。\n- `LVL_GRAN(n)`：第 `n` 层的时间粒度（单位：jiffies）。\n- `LVL_START(n)`：第 `n` 层的起始偏移时间，用于计算定时器应插入的层级。\n- `WHEEL_TIMEOUT_CUTOFF` / `WHEEL_TIMEOUT_MAX`：定时器轮的最大支持超时时间（约 12 天 @ HZ=1000）。\n\n### 主要功能\n- 定时器的注册（`add_timer`）、删除（`del_timer`）和修改（`mod_timer`）。\n- 定时器到期处理（软中断上下文执行）。\n- 与 tick 管理子系统（`tick.h`）和 NO_HZ 模式协同工作。\n- 提供 `sys_sysinfo` 系统调用的底层支持。\n\n## 3. 关键实现\n\n### 多级定时器轮算法\n- **层级设计**：定时器根据其到期时间的远近被分配到不同层级。近到期定时器放入低层（高精度），远到期放入高层（低精度）。\n- **无级联机制**：与经典定时器轮不同，本实现**不进行定时器的级联迁移**。高层定时器到期时直接触发，牺牲少量精度换取显著性能提升。\n- **隐式批处理**：高层的粗粒度天然实现超时事件的批处理，减少中断和软中断开销。\n- **超时截断**：超过 `WHEEL_TIMEOUT_MAX` 的定时器会被强制设为最大支持超时值，实测表明实际使用中超时极少超过 5 天。\n\n### 粒度与范围（以 HZ=1000 为例）\n| 层级 | 偏移 | 粒度 | 范围 |\n|------|------|------|------|\n| 0 | 0 | 1 ms | 0 – 63 ms |\n| 1 | 64 | 8 ms | 64 – 511 ms |\n| ... | ... | ... | ... |\n| 8 | 512 | ~4 小时 | ~1 天 – ~12 天 |\n\n### NO_HZ 支持\n- 当启用 `CONFIG_NO_HZ_COMMON` 时，系统维护**两个独立的定时器轮**：\n  - `BASE_STD`：标准定时器，必须准时触发。\n  - `BASE_DEF`：可延迟定时器，在 CPU 进入空闲状态时可推迟执行，用于节能。\n\n### SMP 优化\n- 定时器默认绑定到注册时的 CPU，利用 per-CPU 数据结构减少锁竞争。\n- `jiffies_64` 使用 `__cacheline_aligned_in_smp` 对齐，避免 false sharing。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **时间子系统**：`<linux/time.h>`, `<linux/jiffies.h>`, `<asm/timex.h>`\n- **调度与中断**：`<linux/interrupt.h>`, `<linux/irq_work.h>`, `<linux/sched/*.h>`\n- **内存管理**：`<linux/slab.h>`, `<linux/mm.h>`\n- **系统调用**：`<linux/syscalls.h>`, `<linux/uaccess.h>`\n- **内部模块**：`\"tick-internal.h\"`（tick 管理）、`<trace/events/timer.h>`（跟踪点）\n\n### 内核子系统交互\n- **Tick 管理**：通过 `tick.h` 接口获取时钟事件，驱动定时器轮推进。\n- **软中断**：定时器到期回调在 `TIMER_SOFTIRQ` 软中断上下文中执行。\n- **POSIX 定时器**：为 `<linux/posix-timers.h>` 提供底层支持。\n- **CPU 热插拔**：通过 `cpu.h` 处理 CPU 上下线时的定时器迁移。\n- **电源管理**：与 `NO_HZ` 和 `sched/nohz.h` 协同实现动态 tick。\n\n## 5. 使用场景\n\n- **内核超时机制**：网络协议栈（TCP 重传、连接超时）、块设备 I/O 超时、文件系统缓存回收等。\n- **延迟执行任务**：通过 `mod_timer` 实现延迟工作队列（如 `delayed_work`）。\n- **系统时间维护**：为 `jiffies` 和 `get_jiffies_64()` 提供原子更新。\n- **用户空间接口**：支撑 `sysinfo` 系统调用返回 uptime、负载等信息。\n- **高精度定时需求**：短超时（<64ms @ HZ=1000）可获得毫秒级精度，满足实时性要求。\n- **低功耗系统**：在 `NO_HZ_IDLE` 或 `NO_HZ_FULL` 模式下，通过 `BASE_DEF` 减少不必要的 tick 中断。",
      "similarity": 0.5798580646514893,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/time/timer.c",
          "start_line": 231,
          "end_line": 333,
          "content": [
            "static void timers_update_migration(void)",
            "{",
            "\tif (sysctl_timer_migration && tick_nohz_active)",
            "\t\tstatic_branch_enable(&timers_migration_enabled);",
            "\telse",
            "\t\tstatic_branch_disable(&timers_migration_enabled);",
            "}",
            "static int timer_migration_handler(struct ctl_table *table, int write,",
            "\t\t\t    void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint ret;",
            "",
            "\tmutex_lock(&timer_keys_mutex);",
            "\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (!ret && write)",
            "\t\ttimers_update_migration();",
            "\tmutex_unlock(&timer_keys_mutex);",
            "\treturn ret;",
            "}",
            "static int __init timer_sysctl_init(void)",
            "{",
            "\tregister_sysctl(\"kernel\", timer_sysctl);",
            "\treturn 0;",
            "}",
            "static inline void timers_update_migration(void) { }",
            "static void timer_update_keys(struct work_struct *work)",
            "{",
            "\tmutex_lock(&timer_keys_mutex);",
            "\ttimers_update_migration();",
            "\tstatic_branch_enable(&timers_nohz_active);",
            "\tmutex_unlock(&timer_keys_mutex);",
            "}",
            "void timers_update_nohz(void)",
            "{",
            "\tschedule_work(&timer_update_work);",
            "}",
            "static inline bool is_timers_nohz_active(void)",
            "{",
            "\treturn static_branch_unlikely(&timers_nohz_active);",
            "}",
            "static inline bool is_timers_nohz_active(void) { return false; }",
            "static unsigned long round_jiffies_common(unsigned long j, int cpu,",
            "\t\tbool force_up)",
            "{",
            "\tint rem;",
            "\tunsigned long original = j;",
            "",
            "\t/*",
            "\t * We don't want all cpus firing their timers at once hitting the",
            "\t * same lock or cachelines, so we skew each extra cpu with an extra",
            "\t * 3 jiffies. This 3 jiffies came originally from the mm/ code which",
            "\t * already did this.",
            "\t * The skew is done by adding 3*cpunr, then round, then subtract this",
            "\t * extra offset again.",
            "\t */",
            "\tj += cpu * 3;",
            "",
            "\trem = j % HZ;",
            "",
            "\t/*",
            "\t * If the target jiffie is just after a whole second (which can happen",
            "\t * due to delays of the timer irq, long irq off times etc etc) then",
            "\t * we should round down to the whole second, not up. Use 1/4th second",
            "\t * as cutoff for this rounding as an extreme upper bound for this.",
            "\t * But never round down if @force_up is set.",
            "\t */",
            "\tif (rem < HZ/4 && !force_up) /* round down */",
            "\t\tj = j - rem;",
            "\telse /* round up */",
            "\t\tj = j - rem + HZ;",
            "",
            "\t/* now that we have rounded, subtract the extra skew again */",
            "\tj -= cpu * 3;",
            "",
            "\t/*",
            "\t * Make sure j is still in the future. Otherwise return the",
            "\t * unmodified value.",
            "\t */",
            "\treturn time_is_after_jiffies(j) ? j : original;",
            "}",
            "unsigned long __round_jiffies(unsigned long j, int cpu)",
            "{",
            "\treturn round_jiffies_common(j, cpu, false);",
            "}",
            "unsigned long __round_jiffies_relative(unsigned long j, int cpu)",
            "{",
            "\tunsigned long j0 = jiffies;",
            "",
            "\t/* Use j0 because jiffies might change while we run */",
            "\treturn round_jiffies_common(j + j0, cpu, false) - j0;",
            "}",
            "unsigned long round_jiffies(unsigned long j)",
            "{",
            "\treturn round_jiffies_common(j, raw_smp_processor_id(), false);",
            "}",
            "unsigned long round_jiffies_relative(unsigned long j)",
            "{",
            "\treturn __round_jiffies_relative(j, raw_smp_processor_id());",
            "}",
            "unsigned long __round_jiffies_up(unsigned long j, int cpu)",
            "{",
            "\treturn round_jiffies_common(j, cpu, true);",
            "}"
          ],
          "function_name": "timers_update_migration, timer_migration_handler, timer_sysctl_init, timers_update_migration, timer_update_keys, timers_update_nohz, is_timers_nohz_active, is_timers_nohz_active, round_jiffies_common, __round_jiffies, __round_jiffies_relative, round_jiffies, round_jiffies_relative, __round_jiffies_up",
          "description": "提供定时器迁移策略控制、Jiffies值调整逻辑及NOHZ相关功能，包含迁移开关配置、定时器分布优化算法和基于CPU负载的超时时间调整方法。",
          "similarity": 0.6069836616516113
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/time/timer.c",
          "start_line": 1,
          "end_line": 230,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " *  Kernel internal timers",
            " *",
            " *  Copyright (C) 1991, 1992  Linus Torvalds",
            " *",
            " *  1997-01-28  Modified by Finn Arne Gangstad to make timers scale better.",
            " *",
            " *  1997-09-10  Updated NTP code according to technical memorandum Jan '96",
            " *              \"A Kernel Model for Precision Timekeeping\" by Dave Mills",
            " *  1998-12-24  Fixed a xtime SMP race (we need the xtime_lock rw spinlock to",
            " *              serialize accesses to xtime/lost_ticks).",
            " *                              Copyright (C) 1998  Andrea Arcangeli",
            " *  1999-03-10  Improved NTP compatibility by Ulrich Windl",
            " *  2002-05-31\tMove sys_sysinfo here and make its locking sane, Robert Love",
            " *  2000-10-05  Implemented scalable SMP per-CPU timer handling.",
            " *                              Copyright (C) 2000, 2001, 2002  Ingo Molnar",
            " *              Designed by David S. Miller, Alexey Kuznetsov and Ingo Molnar",
            " */",
            "",
            "#include <linux/kernel_stat.h>",
            "#include <linux/export.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/percpu.h>",
            "#include <linux/init.h>",
            "#include <linux/mm.h>",
            "#include <linux/swap.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/notifier.h>",
            "#include <linux/thread_info.h>",
            "#include <linux/time.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/cpu.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/delay.h>",
            "#include <linux/tick.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/slab.h>",
            "#include <linux/compat.h>",
            "#include <linux/random.h>",
            "#include <linux/sysctl.h>",
            "",
            "#include <linux/uaccess.h>",
            "#include <asm/unistd.h>",
            "#include <asm/div64.h>",
            "#include <asm/timex.h>",
            "#include <asm/io.h>",
            "",
            "#include \"tick-internal.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/timer.h>",
            "",
            "__visible u64 jiffies_64 __cacheline_aligned_in_smp = INITIAL_JIFFIES;",
            "",
            "EXPORT_SYMBOL(jiffies_64);",
            "",
            "/*",
            " * The timer wheel has LVL_DEPTH array levels. Each level provides an array of",
            " * LVL_SIZE buckets. Each level is driven by its own clock and therefor each",
            " * level has a different granularity.",
            " *",
            " * The level granularity is:\t\tLVL_CLK_DIV ^ lvl",
            " * The level clock frequency is:\tHZ / (LVL_CLK_DIV ^ level)",
            " *",
            " * The array level of a newly armed timer depends on the relative expiry",
            " * time. The farther the expiry time is away the higher the array level and",
            " * therefor the granularity becomes.",
            " *",
            " * Contrary to the original timer wheel implementation, which aims for 'exact'",
            " * expiry of the timers, this implementation removes the need for recascading",
            " * the timers into the lower array levels. The previous 'classic' timer wheel",
            " * implementation of the kernel already violated the 'exact' expiry by adding",
            " * slack to the expiry time to provide batched expiration. The granularity",
            " * levels provide implicit batching.",
            " *",
            " * This is an optimization of the original timer wheel implementation for the",
            " * majority of the timer wheel use cases: timeouts. The vast majority of",
            " * timeout timers (networking, disk I/O ...) are canceled before expiry. If",
            " * the timeout expires it indicates that normal operation is disturbed, so it",
            " * does not matter much whether the timeout comes with a slight delay.",
            " *",
            " * The only exception to this are networking timers with a small expiry",
            " * time. They rely on the granularity. Those fit into the first wheel level,",
            " * which has HZ granularity.",
            " *",
            " * We don't have cascading anymore. timers with a expiry time above the",
            " * capacity of the last wheel level are force expired at the maximum timeout",
            " * value of the last wheel level. From data sampling we know that the maximum",
            " * value observed is 5 days (network connection tracking), so this should not",
            " * be an issue.",
            " *",
            " * The currently chosen array constants values are a good compromise between",
            " * array size and granularity.",
            " *",
            " * This results in the following granularity and range levels:",
            " *",
            " * HZ 1000 steps",
            " * Level Offset  Granularity            Range",
            " *  0      0         1 ms                0 ms -         63 ms",
            " *  1     64         8 ms               64 ms -        511 ms",
            " *  2    128        64 ms              512 ms -       4095 ms (512ms - ~4s)",
            " *  3    192       512 ms             4096 ms -      32767 ms (~4s - ~32s)",
            " *  4    256      4096 ms (~4s)      32768 ms -     262143 ms (~32s - ~4m)",
            " *  5    320     32768 ms (~32s)    262144 ms -    2097151 ms (~4m - ~34m)",
            " *  6    384    262144 ms (~4m)    2097152 ms -   16777215 ms (~34m - ~4h)",
            " *  7    448   2097152 ms (~34m)  16777216 ms -  134217727 ms (~4h - ~1d)",
            " *  8    512  16777216 ms (~4h)  134217728 ms - 1073741822 ms (~1d - ~12d)",
            " *",
            " * HZ  300",
            " * Level Offset  Granularity            Range",
            " *  0\t   0         3 ms                0 ms -        210 ms",
            " *  1\t  64        26 ms              213 ms -       1703 ms (213ms - ~1s)",
            " *  2\t 128       213 ms             1706 ms -      13650 ms (~1s - ~13s)",
            " *  3\t 192      1706 ms (~1s)      13653 ms -     109223 ms (~13s - ~1m)",
            " *  4\t 256     13653 ms (~13s)    109226 ms -     873810 ms (~1m - ~14m)",
            " *  5\t 320    109226 ms (~1m)     873813 ms -    6990503 ms (~14m - ~1h)",
            " *  6\t 384    873813 ms (~14m)   6990506 ms -   55924050 ms (~1h - ~15h)",
            " *  7\t 448   6990506 ms (~1h)   55924053 ms -  447392423 ms (~15h - ~5d)",
            " *  8    512  55924053 ms (~15h) 447392426 ms - 3579139406 ms (~5d - ~41d)",
            " *",
            " * HZ  250",
            " * Level Offset  Granularity            Range",
            " *  0\t   0         4 ms                0 ms -        255 ms",
            " *  1\t  64        32 ms              256 ms -       2047 ms (256ms - ~2s)",
            " *  2\t 128       256 ms             2048 ms -      16383 ms (~2s - ~16s)",
            " *  3\t 192      2048 ms (~2s)      16384 ms -     131071 ms (~16s - ~2m)",
            " *  4\t 256     16384 ms (~16s)    131072 ms -    1048575 ms (~2m - ~17m)",
            " *  5\t 320    131072 ms (~2m)    1048576 ms -    8388607 ms (~17m - ~2h)",
            " *  6\t 384   1048576 ms (~17m)   8388608 ms -   67108863 ms (~2h - ~18h)",
            " *  7\t 448   8388608 ms (~2h)   67108864 ms -  536870911 ms (~18h - ~6d)",
            " *  8    512  67108864 ms (~18h) 536870912 ms - 4294967288 ms (~6d - ~49d)",
            " *",
            " * HZ  100",
            " * Level Offset  Granularity            Range",
            " *  0\t   0         10 ms               0 ms -        630 ms",
            " *  1\t  64         80 ms             640 ms -       5110 ms (640ms - ~5s)",
            " *  2\t 128        640 ms            5120 ms -      40950 ms (~5s - ~40s)",
            " *  3\t 192       5120 ms (~5s)     40960 ms -     327670 ms (~40s - ~5m)",
            " *  4\t 256      40960 ms (~40s)   327680 ms -    2621430 ms (~5m - ~43m)",
            " *  5\t 320     327680 ms (~5m)   2621440 ms -   20971510 ms (~43m - ~5h)",
            " *  6\t 384    2621440 ms (~43m) 20971520 ms -  167772150 ms (~5h - ~1d)",
            " *  7\t 448   20971520 ms (~5h) 167772160 ms - 1342177270 ms (~1d - ~15d)",
            " */",
            "",
            "/* Clock divisor for the next level */",
            "#define LVL_CLK_SHIFT\t3",
            "#define LVL_CLK_DIV\t(1UL << LVL_CLK_SHIFT)",
            "#define LVL_CLK_MASK\t(LVL_CLK_DIV - 1)",
            "#define LVL_SHIFT(n)\t((n) * LVL_CLK_SHIFT)",
            "#define LVL_GRAN(n)\t(1UL << LVL_SHIFT(n))",
            "",
            "/*",
            " * The time start value for each level to select the bucket at enqueue",
            " * time. We start from the last possible delta of the previous level",
            " * so that we can later add an extra LVL_GRAN(n) to n (see calc_index()).",
            " */",
            "#define LVL_START(n)\t((LVL_SIZE - 1) << (((n) - 1) * LVL_CLK_SHIFT))",
            "",
            "/* Size of each clock level */",
            "#define LVL_BITS\t6",
            "#define LVL_SIZE\t(1UL << LVL_BITS)",
            "#define LVL_MASK\t(LVL_SIZE - 1)",
            "#define LVL_OFFS(n)\t((n) * LVL_SIZE)",
            "",
            "/* Level depth */",
            "#if HZ > 100",
            "# define LVL_DEPTH\t9",
            "# else",
            "# define LVL_DEPTH\t8",
            "#endif",
            "",
            "/* The cutoff (max. capacity of the wheel) */",
            "#define WHEEL_TIMEOUT_CUTOFF\t(LVL_START(LVL_DEPTH))",
            "#define WHEEL_TIMEOUT_MAX\t(WHEEL_TIMEOUT_CUTOFF - LVL_GRAN(LVL_DEPTH - 1))",
            "",
            "/*",
            " * The resulting wheel size. If NOHZ is configured we allocate two",
            " * wheels so we have a separate storage for the deferrable timers.",
            " */",
            "#define WHEEL_SIZE\t(LVL_SIZE * LVL_DEPTH)",
            "",
            "#ifdef CONFIG_NO_HZ_COMMON",
            "# define NR_BASES\t2",
            "# define BASE_STD\t0",
            "# define BASE_DEF\t1",
            "#else",
            "# define NR_BASES\t1",
            "# define BASE_STD\t0",
            "# define BASE_DEF\t0",
            "#endif",
            "",
            "struct timer_base {",
            "\traw_spinlock_t\t\tlock;",
            "\tstruct timer_list\t*running_timer;",
            "#ifdef CONFIG_PREEMPT_RT",
            "\tspinlock_t\t\texpiry_lock;",
            "\tatomic_t\t\ttimer_waiters;",
            "#endif",
            "\tunsigned long\t\tclk;",
            "\tunsigned long\t\tnext_expiry;",
            "\tunsigned int\t\tcpu;",
            "\tbool\t\t\tnext_expiry_recalc;",
            "\tbool\t\t\tis_idle;",
            "\tbool\t\t\ttimers_pending;",
            "\tDECLARE_BITMAP(pending_map, WHEEL_SIZE);",
            "\tstruct hlist_head\tvectors[WHEEL_SIZE];",
            "} ____cacheline_aligned;",
            "",
            "static DEFINE_PER_CPU(struct timer_base, timer_bases[NR_BASES]);",
            "",
            "#ifdef CONFIG_NO_HZ_COMMON",
            "",
            "static DEFINE_STATIC_KEY_FALSE(timers_nohz_active);",
            "static DEFINE_MUTEX(timer_keys_mutex);",
            "",
            "static void timer_update_keys(struct work_struct *work);",
            "static DECLARE_WORK(timer_update_work, timer_update_keys);",
            "",
            "#ifdef CONFIG_SMP",
            "static unsigned int sysctl_timer_migration = 1;",
            "",
            "DEFINE_STATIC_KEY_FALSE(timers_migration_enabled);",
            ""
          ],
          "function_name": null,
          "description": "定义并实现了内核定时器轮（timer wheel）的数据结构和宏观布局，通过多层级桶结构管理定时器，支持不同粒度的超时处理，包含对NOHZ模式的支持及动态调整机制。",
          "similarity": 0.5847524404525757
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/time/timer.c",
          "start_line": 2030,
          "end_line": 2130,
          "content": [
            "static __latent_entropy void run_timer_softirq(struct softirq_action *h)",
            "{",
            "\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);",
            "",
            "\t__run_timers(base);",
            "\tif (IS_ENABLED(CONFIG_NO_HZ_COMMON))",
            "\t\t__run_timers(this_cpu_ptr(&timer_bases[BASE_DEF]));",
            "}",
            "static void run_local_timers(void)",
            "{",
            "\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);",
            "",
            "\thrtimer_run_queues();",
            "\t/* Raise the softirq only if required. */",
            "\tif (time_before(jiffies, base->next_expiry)) {",
            "\t\tif (!IS_ENABLED(CONFIG_NO_HZ_COMMON))",
            "\t\t\treturn;",
            "\t\t/* CPU is awake, so check the deferrable base. */",
            "\t\tbase++;",
            "\t\tif (time_before(jiffies, base->next_expiry))",
            "\t\t\treturn;",
            "\t}",
            "\traise_timer_softirq(TIMER_SOFTIRQ);",
            "}",
            "void update_process_times(int user_tick)",
            "{",
            "\tstruct task_struct *p = current;",
            "",
            "\t/* Note: this timer irq context must be accounted for as well. */",
            "\taccount_process_tick(p, user_tick);",
            "\trun_local_timers();",
            "\trcu_sched_clock_irq(user_tick);",
            "#ifdef CONFIG_IRQ_WORK",
            "\tif (in_irq())",
            "\t\tirq_work_tick();",
            "#endif",
            "\tsched_tick();",
            "\tif (IS_ENABLED(CONFIG_POSIX_TIMERS))",
            "\t\trun_posix_cpu_timers();",
            "}",
            "static void process_timeout(struct timer_list *t)",
            "{",
            "\tstruct process_timer *timeout = from_timer(timeout, t, timer);",
            "",
            "\twake_up_process(timeout->task);",
            "}",
            "signed long __sched schedule_timeout(signed long timeout)",
            "{",
            "\tstruct process_timer timer;",
            "\tunsigned long expire;",
            "",
            "\tswitch (timeout)",
            "\t{",
            "\tcase MAX_SCHEDULE_TIMEOUT:",
            "\t\t/*",
            "\t\t * These two special cases are useful to be comfortable",
            "\t\t * in the caller. Nothing more. We could take",
            "\t\t * MAX_SCHEDULE_TIMEOUT from one of the negative value",
            "\t\t * but I' d like to return a valid offset (>=0) to allow",
            "\t\t * the caller to do everything it want with the retval.",
            "\t\t */",
            "\t\tschedule();",
            "\t\tgoto out;",
            "\tdefault:",
            "\t\t/*",
            "\t\t * Another bit of PARANOID. Note that the retval will be",
            "\t\t * 0 since no piece of kernel is supposed to do a check",
            "\t\t * for a negative retval of schedule_timeout() (since it",
            "\t\t * should never happens anyway). You just have the printk()",
            "\t\t * that will tell you if something is gone wrong and where.",
            "\t\t */",
            "\t\tif (timeout < 0) {",
            "\t\t\tprintk(KERN_ERR \"schedule_timeout: wrong timeout \"",
            "\t\t\t\t\"value %lx\\n\", timeout);",
            "\t\t\tdump_stack();",
            "\t\t\t__set_current_state(TASK_RUNNING);",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t}",
            "",
            "\texpire = timeout + jiffies;",
            "",
            "\ttimer.task = current;",
            "\ttimer_setup_on_stack(&timer.timer, process_timeout, 0);",
            "\t__mod_timer(&timer.timer, expire, MOD_TIMER_NOTPENDING);",
            "\tschedule();",
            "\tdel_timer_sync(&timer.timer);",
            "",
            "\t/* Remove the timer from the object tracker */",
            "\tdestroy_timer_on_stack(&timer.timer);",
            "",
            "\ttimeout = expire - jiffies;",
            "",
            " out:",
            "\treturn timeout < 0 ? 0 : timeout;",
            "}",
            "signed long __sched schedule_timeout_interruptible(signed long timeout)",
            "{",
            "\t__set_current_state(TASK_INTERRUPTIBLE);",
            "\treturn schedule_timeout(timeout);",
            "}"
          ],
          "function_name": "run_timer_softirq, run_local_timers, update_process_times, process_timeout, schedule_timeout, schedule_timeout_interruptible",
          "description": "该代码段核心功能是处理定时器相关操作，涵盖软中断处理、本地定时器管理、进程时间更新及休眠超时控制。  \n`run_timer_softirq`和`run_local_timers`分别用于处理软中断中的定时器队列和本地定时器检查，`update_process_times`更新进程时间并触发本地定时器，`schedule_timeout`系列通过定时器实现进程休眠与超时唤醒。  \n上下文不完整：部分关键函数（如`__run_timers`、`hrtimer_run_queues`）的实现未展示，依赖外部知识理解其行为。",
          "similarity": 0.559785008430481
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/time/timer.c",
          "start_line": 779,
          "end_line": 890,
          "content": [
            "static inline void debug_timer_activate(struct timer_list *timer)",
            "{",
            "\tdebug_object_activate(timer, &timer_debug_descr);",
            "}",
            "static inline void debug_timer_deactivate(struct timer_list *timer)",
            "{",
            "\tdebug_object_deactivate(timer, &timer_debug_descr);",
            "}",
            "static inline void debug_timer_assert_init(struct timer_list *timer)",
            "{",
            "\tdebug_object_assert_init(timer, &timer_debug_descr);",
            "}",
            "void init_timer_on_stack_key(struct timer_list *timer,",
            "\t\t\t     void (*func)(struct timer_list *),",
            "\t\t\t     unsigned int flags,",
            "\t\t\t     const char *name, struct lock_class_key *key)",
            "{",
            "\tdebug_object_init_on_stack(timer, &timer_debug_descr);",
            "\tdo_init_timer(timer, func, flags, name, key);",
            "}",
            "void destroy_timer_on_stack(struct timer_list *timer)",
            "{",
            "\tdebug_object_free(timer, &timer_debug_descr);",
            "}",
            "static inline void debug_timer_init(struct timer_list *timer) { }",
            "static inline void debug_timer_activate(struct timer_list *timer) { }",
            "static inline void debug_timer_deactivate(struct timer_list *timer) { }",
            "static inline void debug_timer_assert_init(struct timer_list *timer) { }",
            "static inline void debug_init(struct timer_list *timer)",
            "{",
            "\tdebug_timer_init(timer);",
            "\ttrace_timer_init(timer);",
            "}",
            "static inline void debug_deactivate(struct timer_list *timer)",
            "{",
            "\tdebug_timer_deactivate(timer);",
            "\ttrace_timer_cancel(timer);",
            "}",
            "static inline void debug_assert_init(struct timer_list *timer)",
            "{",
            "\tdebug_timer_assert_init(timer);",
            "}",
            "static void do_init_timer(struct timer_list *timer,",
            "\t\t\t  void (*func)(struct timer_list *),",
            "\t\t\t  unsigned int flags,",
            "\t\t\t  const char *name, struct lock_class_key *key)",
            "{",
            "\ttimer->entry.pprev = NULL;",
            "\ttimer->function = func;",
            "\tif (WARN_ON_ONCE(flags & ~TIMER_INIT_FLAGS))",
            "\t\tflags &= TIMER_INIT_FLAGS;",
            "\ttimer->flags = flags | raw_smp_processor_id();",
            "\tlockdep_init_map(&timer->lockdep_map, name, key, 0);",
            "}",
            "void init_timer_key(struct timer_list *timer,",
            "\t\t    void (*func)(struct timer_list *), unsigned int flags,",
            "\t\t    const char *name, struct lock_class_key *key)",
            "{",
            "\tdebug_init(timer);",
            "\tdo_init_timer(timer, func, flags, name, key);",
            "}",
            "static inline void detach_timer(struct timer_list *timer, bool clear_pending)",
            "{",
            "\tstruct hlist_node *entry = &timer->entry;",
            "",
            "\tdebug_deactivate(timer);",
            "",
            "\t__hlist_del(entry);",
            "\tif (clear_pending)",
            "\t\tentry->pprev = NULL;",
            "\tentry->next = LIST_POISON2;",
            "}",
            "static int detach_if_pending(struct timer_list *timer, struct timer_base *base,",
            "\t\t\t     bool clear_pending)",
            "{",
            "\tunsigned idx = timer_get_idx(timer);",
            "",
            "\tif (!timer_pending(timer))",
            "\t\treturn 0;",
            "",
            "\tif (hlist_is_singular_node(&timer->entry, base->vectors + idx)) {",
            "\t\t__clear_bit(idx, base->pending_map);",
            "\t\tbase->next_expiry_recalc = true;",
            "\t}",
            "",
            "\tdetach_timer(timer, clear_pending);",
            "\treturn 1;",
            "}",
            "static inline void forward_timer_base(struct timer_base *base)",
            "{",
            "\tunsigned long jnow = READ_ONCE(jiffies);",
            "",
            "\t/*",
            "\t * No need to forward if we are close enough below jiffies.",
            "\t * Also while executing timers, base->clk is 1 offset ahead",
            "\t * of jiffies to avoid endless requeuing to current jiffies.",
            "\t */",
            "\tif ((long)(jnow - base->clk) < 1)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If the next expiry value is > jiffies, then we fast forward to",
            "\t * jiffies otherwise we forward to the next expiry value.",
            "\t */",
            "\tif (time_after(base->next_expiry, jnow)) {",
            "\t\tbase->clk = jnow;",
            "\t} else {",
            "\t\tif (WARN_ON_ONCE(time_before(base->next_expiry, base->clk)))",
            "\t\t\treturn;",
            "\t\tbase->clk = base->next_expiry;",
            "\t}",
            "}"
          ],
          "function_name": "debug_timer_activate, debug_timer_deactivate, debug_timer_assert_init, init_timer_on_stack_key, destroy_timer_on_stack, debug_timer_init, debug_timer_activate, debug_timer_deactivate, debug_timer_assert_init, debug_init, debug_deactivate, debug_assert_init, do_init_timer, init_timer_key, detach_timer, detach_if_pending, forward_timer_base",
          "description": "实现定时器初始化、注销及状态转换管理，包含锁类初始化、调试断言检查和定时器基础属性设置，提供完整的定时器生命周期控制接口。",
          "similarity": 0.5561680197715759
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/time/timer.c",
          "start_line": 460,
          "end_line": 560,
          "content": [
            "unsigned long __round_jiffies_up_relative(unsigned long j, int cpu)",
            "{",
            "\tunsigned long j0 = jiffies;",
            "",
            "\t/* Use j0 because jiffies might change while we run */",
            "\treturn round_jiffies_common(j + j0, cpu, true) - j0;",
            "}",
            "unsigned long round_jiffies_up(unsigned long j)",
            "{",
            "\treturn round_jiffies_common(j, raw_smp_processor_id(), true);",
            "}",
            "unsigned long round_jiffies_up_relative(unsigned long j)",
            "{",
            "\treturn __round_jiffies_up_relative(j, raw_smp_processor_id());",
            "}",
            "static inline unsigned int timer_get_idx(struct timer_list *timer)",
            "{",
            "\treturn (timer->flags & TIMER_ARRAYMASK) >> TIMER_ARRAYSHIFT;",
            "}",
            "static inline void timer_set_idx(struct timer_list *timer, unsigned int idx)",
            "{",
            "\ttimer->flags = (timer->flags & ~TIMER_ARRAYMASK) |",
            "\t\t\tidx << TIMER_ARRAYSHIFT;",
            "}",
            "static inline unsigned calc_index(unsigned long expires, unsigned lvl,",
            "\t\t\t\t  unsigned long *bucket_expiry)",
            "{",
            "",
            "\t/*",
            "\t * The timer wheel has to guarantee that a timer does not fire",
            "\t * early. Early expiry can happen due to:",
            "\t * - Timer is armed at the edge of a tick",
            "\t * - Truncation of the expiry time in the outer wheel levels",
            "\t *",
            "\t * Round up with level granularity to prevent this.",
            "\t */",
            "\texpires = (expires >> LVL_SHIFT(lvl)) + 1;",
            "\t*bucket_expiry = expires << LVL_SHIFT(lvl);",
            "\treturn LVL_OFFS(lvl) + (expires & LVL_MASK);",
            "}",
            "static int calc_wheel_index(unsigned long expires, unsigned long clk,",
            "\t\t\t    unsigned long *bucket_expiry)",
            "{",
            "\tunsigned long delta = expires - clk;",
            "\tunsigned int idx;",
            "",
            "\tif (delta < LVL_START(1)) {",
            "\t\tidx = calc_index(expires, 0, bucket_expiry);",
            "\t} else if (delta < LVL_START(2)) {",
            "\t\tidx = calc_index(expires, 1, bucket_expiry);",
            "\t} else if (delta < LVL_START(3)) {",
            "\t\tidx = calc_index(expires, 2, bucket_expiry);",
            "\t} else if (delta < LVL_START(4)) {",
            "\t\tidx = calc_index(expires, 3, bucket_expiry);",
            "\t} else if (delta < LVL_START(5)) {",
            "\t\tidx = calc_index(expires, 4, bucket_expiry);",
            "\t} else if (delta < LVL_START(6)) {",
            "\t\tidx = calc_index(expires, 5, bucket_expiry);",
            "\t} else if (delta < LVL_START(7)) {",
            "\t\tidx = calc_index(expires, 6, bucket_expiry);",
            "\t} else if (LVL_DEPTH > 8 && delta < LVL_START(8)) {",
            "\t\tidx = calc_index(expires, 7, bucket_expiry);",
            "\t} else if ((long) delta < 0) {",
            "\t\tidx = clk & LVL_MASK;",
            "\t\t*bucket_expiry = clk;",
            "\t} else {",
            "\t\t/*",
            "\t\t * Force expire obscene large timeouts to expire at the",
            "\t\t * capacity limit of the wheel.",
            "\t\t */",
            "\t\tif (delta >= WHEEL_TIMEOUT_CUTOFF)",
            "\t\t\texpires = clk + WHEEL_TIMEOUT_MAX;",
            "",
            "\t\tidx = calc_index(expires, LVL_DEPTH - 1, bucket_expiry);",
            "\t}",
            "\treturn idx;",
            "}",
            "static void",
            "trigger_dyntick_cpu(struct timer_base *base, struct timer_list *timer)",
            "{",
            "\tif (!is_timers_nohz_active())",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * TODO: This wants some optimizing similar to the code below, but we",
            "\t * will do that when we switch from push to pull for deferrable timers.",
            "\t */",
            "\tif (timer->flags & TIMER_DEFERRABLE) {",
            "\t\tif (tick_nohz_full_cpu(base->cpu))",
            "\t\t\twake_up_nohz_cpu(base->cpu);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * We might have to IPI the remote CPU if the base is idle and the",
            "\t * timer is not deferrable. If the other CPU is on the way to idle",
            "\t * then it can't set base->is_idle as we hold the base lock:",
            "\t */",
            "\tif (base->is_idle)",
            "\t\twake_up_nohz_cpu(base->cpu);",
            "}"
          ],
          "function_name": "__round_jiffies_up_relative, round_jiffies_up, round_jiffies_up_relative, timer_get_idx, timer_set_idx, calc_index, calc_wheel_index, trigger_dyntick_cpu",
          "description": "实现定时器层级索引计算逻辑和动态tick触发机制，通过层级间转换规则确定定时器存储位置，处理非活动CPU上的定时器唤醒需求。",
          "similarity": 0.5509114265441895
        }
      ]
    }
  ]
}