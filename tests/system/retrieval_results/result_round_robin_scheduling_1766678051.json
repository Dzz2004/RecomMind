{
  "query": "round-robin scheduling",
  "timestamp": "2025-12-25 23:54:11",
  "retrieved_files": [
    {
      "source_file": "kernel/trace/ring_buffer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:07:21\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `trace\\ring_buffer.c`\n\n---\n\n# `trace/ring_buffer.c` 技术文档\n\n## 1. 文件概述\n\n`trace/ring_buffer.c` 实现了 Linux 内核中通用的高性能环形缓冲区（ring buffer）机制，主要用于跟踪（tracing）子系统。该缓冲区支持多 CPU 并发写入、单读者或多读者无锁读取，并通过时间戳压缩、事件类型编码和页面交换等技术优化内存使用和性能。该实现是 ftrace、perf 和其他内核跟踪工具的核心基础设施。\n\n## 2. 核心功能\n\n### 主要函数\n- `ring_buffer_print_entry_header()`：输出环形缓冲区条目头部格式说明，用于调试或用户空间解析。\n- `ring_buffer_event_length()`：返回事件有效载荷（payload）的长度，对 TIME_EXTEND 类型自动跳过扩展头。\n- `rb_event_data()`（内联）：返回指向事件实际数据的指针，处理 TIME_EXTEND 和不同长度编码。\n- `rb_event_length()`：返回完整事件结构（含头部）的字节长度。\n- `rb_event_ts_length()`：返回 TIME_EXTEND 事件及其后续数据事件的总长度。\n- `rb_event_data_length()`：计算数据类型事件的总长度（含头部）。\n- `rb_null_event()` / `rb_event_set_padding()`：判断或设置空/填充事件。\n\n### 关键数据结构（隐含或引用）\n- `struct ring_buffer_event`：环形缓冲区中每个事件的通用头部结构。\n- `struct buffer_data_page`：每个 CPU 缓冲区页面的封装，包含数据和元数据。\n- 每 CPU 页面链表：每个 CPU 拥有独立的环形页面链，写者仅写本地 CPU 缓冲区。\n\n### 核心常量与宏\n- `RINGBUF_TYPE_PADDING`、`RINGBUF_TYPE_TIME_EXTEND`、`RINGBUF_TYPE_TIME_STAMP`、`RINGBUF_TYPE_DATA`：事件类型标识。\n- `RB_ALIGNMENT` / `RB_ARCH_ALIGNMENT`：数据对齐策略，根据架构是否支持 64 位对齐访问调整。\n- `RB_MAX_SMALL_DATA`：小数据事件的最大长度（基于 4 字节对齐和类型长度上限）。\n- `TS_MSB` / `ABS_TS_MASK`：用于处理 59 位时间戳的高位截断与恢复。\n\n## 3. 关键实现\n\n### 无锁读写架构\n- **写者**：每个 CPU 只能写入其对应的 per-CPU 缓冲区，通过原子操作和内存屏障保证写入一致性，无需全局锁。\n- **读者**：每个 per-CPU 缓冲区维护一个独立的“reader page”。当 reader page 被读完后，通过原子交换（未来使用 `cmpxchg`）将其与环形缓冲区中的一个页面互换。交换后，原 reader page 不再被写者访问，读者可安全地将其用于 splice、复制或释放。\n\n### 事件编码与压缩\n- 事件头部使用紧凑位域编码：\n  - `type_len`（5 位）：事件类型或小数据长度（≤31）。\n  - `time_delta`（27 位）：相对于前一事件的时间增量。\n  - `array`（32 位）：用于存储大长度值或事件数据。\n- **TIME_EXTEND 事件**：当时间增量超出 27 位或需要绝对时间戳时，插入一个 8 字节的 TIME_EXTEND 事件，后跟实际数据事件。\n- **数据长度编码**：\n  - 若 `type_len > 0` 且 ≤ `RINGBUF_TYPE_DATA_TYPE_LEN_MAX`，则数据长度 = `type_len * RB_ALIGNMENT`，数据从 `array[0]` 开始。\n  - 否则，数据长度存储在 `array[0]`，实际数据从 `array[1]` 开始。\n\n### 时间戳处理\n- 绝对时间戳仅保留低 59 位（`ABS_TS_MASK`），高 5 位（`TS_MSB`）若非零需单独保存并在读取时恢复，以支持长时间运行的跟踪。\n\n### 内存对齐优化\n- 在支持 64 位对齐访问的架构上（`CONFIG_HAVE_64BIT_ALIGNED_ACCESS`），强制 8 字节对齐（`RB_FORCE_8BYTE_ALIGNMENT`），提升访问性能；否则使用 4 字节对齐。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/ring_buffer.h>`：定义公共 API 和数据结构。\n  - `<linux/trace_clock.h>`、`<linux/sched/clock.h>`：提供高精度时间戳源。\n  - `<linux/percpu.h>`：支持 per-CPU 缓冲区分配。\n  - `<linux/spinlock.h>`、`<asm/local.h>`：提供底层原子操作和锁原语。\n  - `<linux/trace_recursion.h>`：防止跟踪递归。\n- **子系统依赖**：\n  - **ftrace**：主要消费者，用于函数跟踪、事件跟踪等。\n  - **perf**：通过 ring buffer 获取性能事件数据。\n  - **Security Module**：通过 `<linux/security.h>` 集成 LSM 钩子（如 trace 访问控制）。\n- **架构依赖**：依赖 `CONFIG_HAVE_64BIT_ALIGNED_ACCESS` 配置项优化对齐策略。\n\n## 5. 使用场景\n\n- **内核跟踪（ftrace）**：记录函数调用、上下文切换、中断等事件，数据写入 per-CPU ring buffer，用户通过 `tracefs` 读取。\n- **性能分析（perf）**：perf 工具通过 ring buffer 接收内核采样事件（如 PMU 中断、软件事件）。\n- **实时监控与调试**：开发者或运维人员通过读取 ring buffer 内容分析系统行为、延迟或错误。\n- **自测试（selftest）**：文件包含自测试逻辑（依赖 `<linux/kthread.h>`），用于验证 ring buffer 功能正确性。\n- **低开销事件记录**：由于其无锁设计和压缩编码，适用于高频事件记录场景（如每秒百万级事件）。",
      "similarity": 0.5204417109489441,
      "chunks": [
        {
          "chunk_id": 20,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 4388,
          "end_line": 4493,
          "content": [
            "static void rb_iter_reset(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;",
            "",
            "\t/* Iterator usage is expected to have record disabled */",
            "\titer->head_page = cpu_buffer->reader_page;",
            "\titer->head = cpu_buffer->reader_page->read;",
            "\titer->next_event = iter->head;",
            "",
            "\titer->cache_reader_page = iter->head_page;",
            "\titer->cache_read = cpu_buffer->read;",
            "\titer->cache_pages_removed = cpu_buffer->pages_removed;",
            "",
            "\tif (iter->head) {",
            "\t\titer->read_stamp = cpu_buffer->read_stamp;",
            "\t\titer->page_stamp = cpu_buffer->reader_page->page->time_stamp;",
            "\t} else {",
            "\t\titer->read_stamp = iter->head_page->page->time_stamp;",
            "\t\titer->page_stamp = iter->read_stamp;",
            "\t}",
            "}",
            "void ring_buffer_iter_reset(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tunsigned long flags;",
            "",
            "\tif (!iter)",
            "\t\treturn;",
            "",
            "\tcpu_buffer = iter->cpu_buffer;",
            "",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\trb_iter_reset(iter);",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "}",
            "int ring_buffer_iter_empty(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct buffer_page *reader;",
            "\tstruct buffer_page *head_page;",
            "\tstruct buffer_page *commit_page;",
            "\tstruct buffer_page *curr_commit_page;",
            "\tunsigned commit;",
            "\tu64 curr_commit_ts;",
            "\tu64 commit_ts;",
            "",
            "\tcpu_buffer = iter->cpu_buffer;",
            "\treader = cpu_buffer->reader_page;",
            "\thead_page = cpu_buffer->head_page;",
            "\tcommit_page = READ_ONCE(cpu_buffer->commit_page);",
            "\tcommit_ts = commit_page->page->time_stamp;",
            "",
            "\t/*",
            "\t * When the writer goes across pages, it issues a cmpxchg which",
            "\t * is a mb(), which will synchronize with the rmb here.",
            "\t * (see rb_tail_page_update())",
            "\t */",
            "\tsmp_rmb();",
            "\tcommit = rb_page_commit(commit_page);",
            "\t/* We want to make sure that the commit page doesn't change */",
            "\tsmp_rmb();",
            "",
            "\t/* Make sure commit page didn't change */",
            "\tcurr_commit_page = READ_ONCE(cpu_buffer->commit_page);",
            "\tcurr_commit_ts = READ_ONCE(curr_commit_page->page->time_stamp);",
            "",
            "\t/* If the commit page changed, then there's more data */",
            "\tif (curr_commit_page != commit_page ||",
            "\t    curr_commit_ts != commit_ts)",
            "\t\treturn 0;",
            "",
            "\t/* Still racy, as it may return a false positive, but that's OK */",
            "\treturn ((iter->head_page == commit_page && iter->head >= commit) ||",
            "\t\t(iter->head_page == reader && commit_page == head_page &&",
            "\t\t head_page->read == commit &&",
            "\t\t iter->head == rb_page_commit(cpu_buffer->reader_page)));",
            "}",
            "static void",
            "rb_update_read_stamp(struct ring_buffer_per_cpu *cpu_buffer,",
            "\t\t     struct ring_buffer_event *event)",
            "{",
            "\tu64 delta;",
            "",
            "\tswitch (event->type_len) {",
            "\tcase RINGBUF_TYPE_PADDING:",
            "\t\treturn;",
            "",
            "\tcase RINGBUF_TYPE_TIME_EXTEND:",
            "\t\tdelta = rb_event_time_stamp(event);",
            "\t\tcpu_buffer->read_stamp += delta;",
            "\t\treturn;",
            "",
            "\tcase RINGBUF_TYPE_TIME_STAMP:",
            "\t\tdelta = rb_event_time_stamp(event);",
            "\t\tdelta = rb_fix_abs_ts(delta, cpu_buffer->read_stamp);",
            "\t\tcpu_buffer->read_stamp = delta;",
            "\t\treturn;",
            "",
            "\tcase RINGBUF_TYPE_DATA:",
            "\t\tcpu_buffer->read_stamp += event->time_delta;",
            "\t\treturn;",
            "",
            "\tdefault:",
            "\t\tRB_WARN_ON(cpu_buffer, 1);",
            "\t}",
            "}"
          ],
          "function_name": "rb_iter_reset, ring_buffer_iter_reset, ring_buffer_iter_empty, rb_update_read_stamp",
          "description": "该代码段主要管理环形缓冲区迭代器状态及时间戳更新机制。  \n`rb_iter_reset` 和 `ring_buffer_iter_reset` 用于安全重置迭代器状态，后者通过锁保护确保多线程环境下的一致性；`ring_buffer_iter_empty` 检测迭代器当前位置是否已到达缓冲区末尾；`rb_update_read_stamp` 根据事件类型动态调整读取时间戳以支持时间戳校准。  \n所有函数协同保障了在高并发场景下对环形缓冲区事件的精确追踪与状态同步。",
          "similarity": 0.556618332862854
        },
        {
          "chunk_id": 23,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 5310,
          "end_line": 5415,
          "content": [
            "static void reset_disabled_cpu_buffer(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "",
            "\tif (RB_WARN_ON(cpu_buffer, local_read(&cpu_buffer->committing)))",
            "\t\tgoto out;",
            "",
            "\tarch_spin_lock(&cpu_buffer->lock);",
            "",
            "\trb_reset_cpu(cpu_buffer);",
            "",
            "\tarch_spin_unlock(&cpu_buffer->lock);",
            "",
            " out:",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "}",
            "void ring_buffer_reset_cpu(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];",
            "",
            "\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\treturn;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tatomic_inc(&cpu_buffer->resize_disabled);",
            "\tatomic_inc(&cpu_buffer->record_disabled);",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\tatomic_dec(&cpu_buffer->record_disabled);",
            "\tatomic_dec(&cpu_buffer->resize_disabled);",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}",
            "void ring_buffer_reset_online_cpus(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tfor_each_online_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\tatomic_add(RESET_BIT, &cpu_buffer->resize_disabled);",
            "\t\tatomic_inc(&cpu_buffer->record_disabled);",
            "\t}",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\t/*",
            "\t\t * If a CPU came online during the synchronize_rcu(), then",
            "\t\t * ignore it.",
            "\t\t */",
            "\t\tif (!(atomic_read(&cpu_buffer->resize_disabled) & RESET_BIT))",
            "\t\t\tcontinue;",
            "",
            "\t\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\t\tatomic_dec(&cpu_buffer->record_disabled);",
            "\t\tatomic_sub(RESET_BIT, &cpu_buffer->resize_disabled);",
            "\t}",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}",
            "void ring_buffer_reset(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\tatomic_inc(&cpu_buffer->resize_disabled);",
            "\t\tatomic_inc(&cpu_buffer->record_disabled);",
            "\t}",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\t\tatomic_dec(&cpu_buffer->record_disabled);",
            "\t\tatomic_dec(&cpu_buffer->resize_disabled);",
            "\t}",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}"
          ],
          "function_name": "reset_disabled_cpu_buffer, ring_buffer_reset_cpu, ring_buffer_reset_online_cpus, ring_buffer_reset",
          "description": "该代码段实现了对跟踪环形缓冲区的重置机制，通过原子操作与RCU同步确保多线程安全。  \n`reset_disabled_cpu_buffer`负责安全地重置指定CPU的缓冲区，`ring_buffer_reset_cpu`、`ring_buffer_reset_online_cpus`和`ring_buffer_reset`分别针对单个CPU、在线CPU及全系统CPU执行重置，均通过原子计数器控制访问权限并阻塞数据提交。  \n由于`rb_reset_cpu`未在当前代码片段中定义，故其具体行为依赖上下文信息。",
          "similarity": 0.5524704456329346
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 993,
          "end_line": 1149,
          "content": [
            "static inline bool",
            "rb_wait_cond(struct rb_irq_work *rbwork, struct trace_buffer *buffer,",
            "\t     int cpu, int full, ring_buffer_cond_fn cond, void *data)",
            "{",
            "\tif (rb_watermark_hit(buffer, cpu, full))",
            "\t\treturn true;",
            "",
            "\tif (cond(data))",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * The events can happen in critical sections where",
            "\t * checking a work queue can cause deadlocks.",
            "\t * After adding a task to the queue, this flag is set",
            "\t * only to notify events to try to wake up the queue",
            "\t * using irq_work.",
            "\t *",
            "\t * We don't clear it even if the buffer is no longer",
            "\t * empty. The flag only causes the next event to run",
            "\t * irq_work to do the work queue wake up. The worse",
            "\t * that can happen if we race with !trace_empty() is that",
            "\t * an event will cause an irq_work to try to wake up",
            "\t * an empty queue.",
            "\t *",
            "\t * There's no reason to protect this flag either, as",
            "\t * the work queue and irq_work logic will do the necessary",
            "\t * synchronization for the wake ups. The only thing",
            "\t * that is necessary is that the wake up happens after",
            "\t * a task has been queued. It's OK for spurious wake ups.",
            "\t */",
            "\tif (full)",
            "\t\trbwork->full_waiters_pending = true;",
            "\telse",
            "\t\trbwork->waiters_pending = true;",
            "",
            "\treturn false;",
            "}",
            "static bool rb_wait_once(void *data)",
            "{",
            "\tlong *once = data;",
            "",
            "\t/* wait_event() actually calls this twice before scheduling*/",
            "\tif (*once > 1)",
            "\t\treturn true;",
            "",
            "\t(*once)++;",
            "\treturn false;",
            "}",
            "int ring_buffer_wait(struct trace_buffer *buffer, int cpu, int full,",
            "\t\t     ring_buffer_cond_fn cond, void *data)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct wait_queue_head *waitq;",
            "\tstruct rb_irq_work *rbwork;",
            "\tlong once = 0;",
            "\tint ret = 0;",
            "",
            "\tif (!cond) {",
            "\t\tcond = rb_wait_once;",
            "\t\tdata = &once;",
            "\t}",
            "",
            "\t/*",
            "\t * Depending on what the caller is waiting for, either any",
            "\t * data in any cpu buffer, or a specific buffer, put the",
            "\t * caller on the appropriate wait queue.",
            "\t */",
            "\tif (cpu == RING_BUFFER_ALL_CPUS) {",
            "\t\trbwork = &buffer->irq_work;",
            "\t\t/* Full only makes sense on per cpu reads */",
            "\t\tfull = 0;",
            "\t} else {",
            "\t\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\t\treturn -ENODEV;",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\trbwork = &cpu_buffer->irq_work;",
            "\t}",
            "",
            "\tif (full)",
            "\t\twaitq = &rbwork->full_waiters;",
            "\telse",
            "\t\twaitq = &rbwork->waiters;",
            "",
            "\tret = wait_event_interruptible((*waitq),",
            "\t\t\t\trb_wait_cond(rbwork, buffer, cpu, full, cond, data));",
            "",
            "\treturn ret;",
            "}",
            "__poll_t ring_buffer_poll_wait(struct trace_buffer *buffer, int cpu,",
            "\t\t\t  struct file *filp, poll_table *poll_table, int full)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct rb_irq_work *rbwork;",
            "",
            "\tif (cpu == RING_BUFFER_ALL_CPUS) {",
            "\t\trbwork = &buffer->irq_work;",
            "\t\tfull = 0;",
            "\t} else {",
            "\t\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\t\treturn EPOLLERR;",
            "",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\trbwork = &cpu_buffer->irq_work;",
            "\t}",
            "",
            "\tif (full) {",
            "\t\tunsigned long flags;",
            "",
            "\t\tpoll_wait(filp, &rbwork->full_waiters, poll_table);",
            "",
            "\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t\tif (!cpu_buffer->shortest_full ||",
            "\t\t    cpu_buffer->shortest_full > full)",
            "\t\t\tcpu_buffer->shortest_full = full;",
            "\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "\t\tif (full_hit(buffer, cpu, full))",
            "\t\t\treturn EPOLLIN | EPOLLRDNORM;",
            "\t\t/*",
            "\t\t * Only allow full_waiters_pending update to be seen after",
            "\t\t * the shortest_full is set. If the writer sees the",
            "\t\t * full_waiters_pending flag set, it will compare the",
            "\t\t * amount in the ring buffer to shortest_full. If the amount",
            "\t\t * in the ring buffer is greater than the shortest_full",
            "\t\t * percent, it will call the irq_work handler to wake up",
            "\t\t * this list. The irq_handler will reset shortest_full",
            "\t\t * back to zero. That's done under the reader_lock, but",
            "\t\t * the below smp_mb() makes sure that the update to",
            "\t\t * full_waiters_pending doesn't leak up into the above.",
            "\t\t */",
            "\t\tsmp_mb();",
            "\t\trbwork->full_waiters_pending = true;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tpoll_wait(filp, &rbwork->waiters, poll_table);",
            "\trbwork->waiters_pending = true;",
            "",
            "\t/*",
            "\t * There's a tight race between setting the waiters_pending and",
            "\t * checking if the ring buffer is empty.  Once the waiters_pending bit",
            "\t * is set, the next event will wake the task up, but we can get stuck",
            "\t * if there's only a single event in.",
            "\t *",
            "\t * FIXME: Ideally, we need a memory barrier on the writer side as well,",
            "\t * but adding a memory barrier to all events will cause too much of a",
            "\t * performance hit in the fast path.  We only need a memory barrier when",
            "\t * the buffer goes from empty to having content.  But as this race is",
            "\t * extremely small, and it's not a problem if another event comes in, we",
            "\t * will fix it later.",
            "\t */",
            "\tsmp_mb();",
            "",
            "\tif ((cpu == RING_BUFFER_ALL_CPUS && !ring_buffer_empty(buffer)) ||",
            "\t    (cpu != RING_BUFFER_ALL_CPUS && !ring_buffer_empty_cpu(buffer, cpu)))",
            "\t\treturn EPOLLIN | EPOLLRDNORM;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rb_wait_cond, rb_wait_once, ring_buffer_wait, ring_buffer_poll_wait",
          "description": "实现环形缓冲区的等待逻辑，通过rb_wait_cond判断条件是否满足，ring_buffer_wait将调用者加入等待队列并阻塞直至条件成立或被中断，ring_buffer_poll_wait支持poll风格的等待并返回EPOLLIN标志。",
          "similarity": 0.5432546138763428
        },
        {
          "chunk_id": 21,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 4511,
          "end_line": 4617,
          "content": [
            "static void",
            "rb_update_iter_read_stamp(struct ring_buffer_iter *iter,",
            "\t\t\t  struct ring_buffer_event *event)",
            "{",
            "\tu64 delta;",
            "",
            "\tswitch (event->type_len) {",
            "\tcase RINGBUF_TYPE_PADDING:",
            "\t\treturn;",
            "",
            "\tcase RINGBUF_TYPE_TIME_EXTEND:",
            "\t\tdelta = rb_event_time_stamp(event);",
            "\t\titer->read_stamp += delta;",
            "\t\treturn;",
            "",
            "\tcase RINGBUF_TYPE_TIME_STAMP:",
            "\t\tdelta = rb_event_time_stamp(event);",
            "\t\tdelta = rb_fix_abs_ts(delta, iter->read_stamp);",
            "\t\titer->read_stamp = delta;",
            "\t\treturn;",
            "",
            "\tcase RINGBUF_TYPE_DATA:",
            "\t\titer->read_stamp += event->time_delta;",
            "\t\treturn;",
            "",
            "\tdefault:",
            "\t\tRB_WARN_ON(iter->cpu_buffer, 1);",
            "\t}",
            "}",
            "static void rb_advance_reader(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tstruct ring_buffer_event *event;",
            "\tstruct buffer_page *reader;",
            "\tunsigned length;",
            "",
            "\treader = rb_get_reader_page(cpu_buffer);",
            "",
            "\t/* This function should not be called when buffer is empty */",
            "\tif (RB_WARN_ON(cpu_buffer, !reader))",
            "\t\treturn;",
            "",
            "\tevent = rb_reader_event(cpu_buffer);",
            "",
            "\tif (event->type_len <= RINGBUF_TYPE_DATA_TYPE_LEN_MAX)",
            "\t\tcpu_buffer->read++;",
            "",
            "\trb_update_read_stamp(cpu_buffer, event);",
            "",
            "\tlength = rb_event_length(event);",
            "\tcpu_buffer->reader_page->read += length;",
            "\tcpu_buffer->read_bytes += length;",
            "}",
            "static void rb_advance_iter(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "",
            "\tcpu_buffer = iter->cpu_buffer;",
            "",
            "\t/* If head == next_event then we need to jump to the next event */",
            "\tif (iter->head == iter->next_event) {",
            "\t\t/* If the event gets overwritten again, there's nothing to do */",
            "\t\tif (rb_iter_head_event(iter) == NULL)",
            "\t\t\treturn;",
            "\t}",
            "",
            "\titer->head = iter->next_event;",
            "",
            "\t/*",
            "\t * Check if we are at the end of the buffer.",
            "\t */",
            "\tif (iter->next_event >= rb_page_size(iter->head_page)) {",
            "\t\t/* discarded commits can make the page empty */",
            "\t\tif (iter->head_page == cpu_buffer->commit_page)",
            "\t\t\treturn;",
            "\t\trb_inc_iter(iter);",
            "\t\treturn;",
            "\t}",
            "",
            "\trb_update_iter_read_stamp(iter, iter->event);",
            "}",
            "static int rb_lost_events(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\treturn cpu_buffer->lost_events;",
            "}",
            "static inline bool rb_reader_lock(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tif (likely(!in_nmi())) {",
            "\t\traw_spin_lock(&cpu_buffer->reader_lock);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/*",
            "\t * If an NMI die dumps out the content of the ring buffer",
            "\t * trylock must be used to prevent a deadlock if the NMI",
            "\t * preempted a task that holds the ring buffer locks. If",
            "\t * we get the lock then all is fine, if not, then continue",
            "\t * to do the read, but this can corrupt the ring buffer,",
            "\t * so it must be permanently disabled from future writes.",
            "\t * Reading from NMI is a oneshot deal.",
            "\t */",
            "\tif (raw_spin_trylock(&cpu_buffer->reader_lock))",
            "\t\treturn true;",
            "",
            "\t/* Continue without locking, but disable the ring buffer */",
            "\tatomic_inc(&cpu_buffer->record_disabled);",
            "\treturn false;",
            "}"
          ],
          "function_name": "rb_update_iter_read_stamp, rb_advance_reader, rb_advance_iter, rb_lost_events, rb_reader_lock",
          "description": "该代码段实现了环形缓冲区（ring buffer）的读取逻辑，包含事件时间戳更新、读指针推进、迭代器状态管理和锁控制等功能。  \n`rb_update_iter_read_stamp` 根据事件类型动态调整迭代器的读取时间戳，确保时间序列一致性；`rb_advance_reader` 和 `rb_advance_iter` 共同推进读取位置并维护事件追踪状态。  \n`rb_lost_events` 提供丢失事件计数接口，`rb_reader_lock` 在 NMI 场景下通过尝试加锁避免死锁，保障多线程环境下的数据安全。",
          "similarity": 0.5347195267677307
        },
        {
          "chunk_id": 15,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 3182,
          "end_line": 3293,
          "content": [
            "static void rb_commit(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tlocal_inc(&cpu_buffer->entries);",
            "\trb_end_commit(cpu_buffer);",
            "}",
            "static __always_inline void",
            "rb_wakeups(struct trace_buffer *buffer, struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tif (buffer->irq_work.waiters_pending) {",
            "\t\tbuffer->irq_work.waiters_pending = false;",
            "\t\t/* irq_work_queue() supplies it's own memory barriers */",
            "\t\tirq_work_queue(&buffer->irq_work.work);",
            "\t}",
            "",
            "\tif (cpu_buffer->irq_work.waiters_pending) {",
            "\t\tcpu_buffer->irq_work.waiters_pending = false;",
            "\t\t/* irq_work_queue() supplies it's own memory barriers */",
            "\t\tirq_work_queue(&cpu_buffer->irq_work.work);",
            "\t}",
            "",
            "\tif (cpu_buffer->last_pages_touch == local_read(&cpu_buffer->pages_touched))",
            "\t\treturn;",
            "",
            "\tif (cpu_buffer->reader_page == cpu_buffer->commit_page)",
            "\t\treturn;",
            "",
            "\tif (!cpu_buffer->irq_work.full_waiters_pending)",
            "\t\treturn;",
            "",
            "\tcpu_buffer->last_pages_touch = local_read(&cpu_buffer->pages_touched);",
            "",
            "\tif (!full_hit(buffer, cpu_buffer->cpu, cpu_buffer->shortest_full))",
            "\t\treturn;",
            "",
            "\tcpu_buffer->irq_work.wakeup_full = true;",
            "\tcpu_buffer->irq_work.full_waiters_pending = false;",
            "\t/* irq_work_queue() supplies it's own memory barriers */",
            "\tirq_work_queue(&cpu_buffer->irq_work.work);",
            "}",
            "static __always_inline bool",
            "trace_recursive_lock(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tunsigned int val = cpu_buffer->current_context;",
            "\tint bit = interrupt_context_level();",
            "",
            "\tbit = RB_CTX_NORMAL - bit;",
            "",
            "\tif (unlikely(val & (1 << (bit + cpu_buffer->nest)))) {",
            "\t\t/*",
            "\t\t * It is possible that this was called by transitioning",
            "\t\t * between interrupt context, and preempt_count() has not",
            "\t\t * been updated yet. In this case, use the TRANSITION bit.",
            "\t\t */",
            "\t\tbit = RB_CTX_TRANSITION;",
            "\t\tif (val & (1 << (bit + cpu_buffer->nest))) {",
            "\t\t\tdo_ring_buffer_record_recursion();",
            "\t\t\treturn true;",
            "\t\t}",
            "\t}",
            "",
            "\tval |= (1 << (bit + cpu_buffer->nest));",
            "\tcpu_buffer->current_context = val;",
            "",
            "\treturn false;",
            "}",
            "static __always_inline void",
            "trace_recursive_unlock(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tcpu_buffer->current_context &=",
            "\t\tcpu_buffer->current_context - (1 << cpu_buffer->nest);",
            "}",
            "void ring_buffer_nest_start(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu;",
            "",
            "\t/* Enabled by ring_buffer_nest_end() */",
            "\tpreempt_disable_notrace();",
            "\tcpu = raw_smp_processor_id();",
            "\tcpu_buffer = buffer->buffers[cpu];",
            "\t/* This is the shift value for the above recursive locking */",
            "\tcpu_buffer->nest += NESTED_BITS;",
            "}",
            "void ring_buffer_nest_end(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu;",
            "",
            "\t/* disabled by ring_buffer_nest_start() */",
            "\tcpu = raw_smp_processor_id();",
            "\tcpu_buffer = buffer->buffers[cpu];",
            "\t/* This is the shift value for the above recursive locking */",
            "\tcpu_buffer->nest -= NESTED_BITS;",
            "\tpreempt_enable_notrace();",
            "}",
            "int ring_buffer_unlock_commit(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu = raw_smp_processor_id();",
            "",
            "\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\trb_commit(cpu_buffer);",
            "",
            "\trb_wakeups(buffer, cpu_buffer);",
            "",
            "\ttrace_recursive_unlock(cpu_buffer);",
            "",
            "\tpreempt_enable_notrace();",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rb_commit, rb_wakeups, trace_recursive_lock, trace_recursive_unlock, ring_buffer_nest_start, ring_buffer_nest_end, ring_buffer_unlock_commit",
          "description": "rb_commit 增加当前CPU缓冲区条目计数并完成提交流程；rb_wakeups 检查并唤醒等待缓冲区空间的线程；trace_recursive_lock/unlock 管理递归中断上下文锁；ring_buffer_nest_start/end 调整嵌套层级；ring_buffer_unlock_commit 提交事件并释放抢占",
          "similarity": 0.5300144553184509
        }
      ]
    },
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.51891028881073,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/rt.c",
          "start_line": 57,
          "end_line": 159,
          "content": [
            "static int __init sched_rt_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", sched_rt_sysctls);",
            "\treturn 0;",
            "}",
            "void init_rt_rq(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_prio_array *array;",
            "\tint i;",
            "",
            "\tarray = &rt_rq->active;",
            "\tfor (i = 0; i < MAX_RT_PRIO; i++) {",
            "\t\tINIT_LIST_HEAD(array->queue + i);",
            "\t\t__clear_bit(i, array->bitmap);",
            "\t}",
            "\t/* delimiter for bitsearch: */",
            "\t__set_bit(MAX_RT_PRIO, array->bitmap);",
            "",
            "#if defined CONFIG_SMP",
            "\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\trt_rq->highest_prio.next = MAX_RT_PRIO-1;",
            "\trt_rq->overloaded = 0;",
            "\tplist_head_init(&rt_rq->pushable_tasks);",
            "#endif /* CONFIG_SMP */",
            "\t/* We start is dequeued state, because no RT tasks are queued */",
            "\trt_rq->rt_queued = 0;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\trt_rq->rt_time = 0;",
            "\trt_rq->rt_throttled = 0;",
            "\trt_rq->rt_runtime = 0;",
            "\traw_spin_lock_init(&rt_rq->rt_runtime_lock);",
            "#endif",
            "}",
            "static enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)",
            "{",
            "\tstruct rt_bandwidth *rt_b =",
            "\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);",
            "\tint idle = 0;",
            "\tint overrun;",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tfor (;;) {",
            "\t\toverrun = hrtimer_forward_now(timer, rt_b->rt_period);",
            "\t\tif (!overrun)",
            "\t\t\tbreak;",
            "",
            "\t\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "\t\tidle = do_sched_rt_period_timer(rt_b, overrun);",
            "\t\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\t}",
            "\tif (idle)",
            "\t\trt_b->rt_period_active = 0;",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "",
            "\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;",
            "}",
            "void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)",
            "{",
            "\trt_b->rt_period = ns_to_ktime(period);",
            "\trt_b->rt_runtime = runtime;",
            "",
            "\traw_spin_lock_init(&rt_b->rt_runtime_lock);",
            "",
            "\thrtimer_init(&rt_b->rt_period_timer, CLOCK_MONOTONIC,",
            "\t\t     HRTIMER_MODE_REL_HARD);",
            "\trt_b->rt_period_timer.function = sched_rt_period_timer;",
            "}",
            "static inline void do_start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tif (!rt_b->rt_period_active) {",
            "\t\trt_b->rt_period_active = 1;",
            "\t\t/*",
            "\t\t * SCHED_DEADLINE updates the bandwidth, as a run away",
            "\t\t * RT task with a DL task could hog a CPU. But DL does",
            "\t\t * not reset the period. If a deadline task was running",
            "\t\t * without an RT task running, it can cause RT tasks to",
            "\t\t * throttle when they start up. Kick the timer right away",
            "\t\t * to update the period.",
            "\t\t */",
            "\t\thrtimer_forward_now(&rt_b->rt_period_timer, ns_to_ktime(0));",
            "\t\thrtimer_start_expires(&rt_b->rt_period_timer,",
            "\t\t\t\t      HRTIMER_MODE_ABS_PINNED_HARD);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}",
            "static void start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)",
            "\t\treturn;",
            "",
            "\tdo_start_rt_bandwidth(rt_b);",
            "}",
            "static void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\thrtimer_cancel(&rt_b->rt_period_timer);",
            "}",
            "void unregister_rt_sched_group(struct task_group *tg)",
            "{",
            "\tif (tg->rt_se)",
            "\t\tdestroy_rt_bandwidth(&tg->rt_bandwidth);",
            "}"
          ],
          "function_name": "sched_rt_sysctl_init, init_rt_rq, sched_rt_period_timer, init_rt_bandwidth, do_start_rt_bandwidth, start_rt_bandwidth, destroy_rt_bandwidth, unregister_rt_sched_group",
          "description": "初始化实时调度相关数据结构，管理实时任务周期定时器，控制实时带宽分配与回收，实现基于时间片轮转的调度策略。",
          "similarity": 0.5818134546279907
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/rt.c",
          "start_line": 776,
          "end_line": 913,
          "content": [
            "static void balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tif (!sched_feat(RT_RUNTIME_SHARE))",
            "\t\treturn;",
            "",
            "\tif (rt_rq->rt_time > rt_rq->rt_runtime) {",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tdo_balance_runtime(rt_rq);",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t}",
            "}",
            "static inline void balance_runtime(struct rt_rq *rt_rq) {}",
            "static int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun)",
            "{",
            "\tint i, idle = 1, throttled = 0;",
            "\tconst struct cpumask *span;",
            "",
            "\tspan = sched_rt_period_mask();",
            "",
            "\t/*",
            "\t * FIXME: isolated CPUs should really leave the root task group,",
            "\t * whether they are isolcpus or were isolated via cpusets, lest",
            "\t * the timer run on a CPU which does not service all runqueues,",
            "\t * potentially leaving other CPUs indefinitely throttled.  If",
            "\t * isolation is really required, the user will turn the throttle",
            "\t * off to kill the perturbations it causes anyway.  Meanwhile,",
            "\t * this maintains functionality for boot and/or troubleshooting.",
            "\t */",
            "\tif (rt_b == &root_task_group.rt_bandwidth)",
            "\t\tspan = cpu_online_mask;",
            "",
            "\tfor_each_cpu(i, span) {",
            "\t\tint enqueue = 0;",
            "\t\tstruct rt_rq *rt_rq = sched_rt_period_rt_rq(rt_b, i);",
            "\t\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\t\tstruct rq_flags rf;",
            "\t\tint skip;",
            "",
            "\t\t/*",
            "\t\t * When span == cpu_online_mask, taking each rq->lock",
            "\t\t * can be time-consuming. Try to avoid it when possible.",
            "\t\t */",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\tif (!sched_feat(RT_RUNTIME_SHARE) && rt_rq->rt_runtime != RUNTIME_INF)",
            "\t\t\trt_rq->rt_runtime = rt_b->rt_runtime;",
            "\t\tskip = !rt_rq->rt_time && !rt_rq->rt_nr_running;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tif (skip)",
            "\t\t\tcontinue;",
            "",
            "\t\trq_lock(rq, &rf);",
            "\t\tupdate_rq_clock(rq);",
            "",
            "\t\tif (rt_rq->rt_time) {",
            "\t\t\tu64 runtime;",
            "",
            "\t\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t\tif (rt_rq->rt_throttled)",
            "\t\t\t\tbalance_runtime(rt_rq);",
            "\t\t\truntime = rt_rq->rt_runtime;",
            "\t\t\trt_rq->rt_time -= min(rt_rq->rt_time, overrun*runtime);",
            "\t\t\tif (rt_rq->rt_throttled && rt_rq->rt_time < runtime) {",
            "\t\t\t\trt_rq->rt_throttled = 0;",
            "\t\t\t\tenqueue = 1;",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * When we're idle and a woken (rt) task is",
            "\t\t\t\t * throttled wakeup_preempt() will set",
            "\t\t\t\t * skip_update and the time between the wakeup",
            "\t\t\t\t * and this unthrottle will get accounted as",
            "\t\t\t\t * 'runtime'.",
            "\t\t\t\t */",
            "\t\t\t\tif (rt_rq->rt_nr_running && rq->curr == rq->idle)",
            "\t\t\t\t\trq_clock_cancel_skipupdate(rq);",
            "\t\t\t}",
            "\t\t\tif (rt_rq->rt_time || rt_rq->rt_nr_running)",
            "\t\t\t\tidle = 0;",
            "\t\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\t} else if (rt_rq->rt_nr_running) {",
            "\t\t\tidle = 0;",
            "\t\t\tif (!rt_rq_throttled(rt_rq))",
            "\t\t\t\tenqueue = 1;",
            "\t\t}",
            "\t\tif (rt_rq->rt_throttled)",
            "\t\t\tthrottled = 1;",
            "",
            "\t\tif (enqueue)",
            "\t\t\tsched_rt_rq_enqueue(rt_rq);",
            "\t\trq_unlock(rq, &rf);",
            "\t}",
            "",
            "\tif (!throttled && (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF))",
            "\t\treturn 1;",
            "",
            "\treturn idle;",
            "}",
            "static int sched_rt_runtime_exceeded(struct rt_rq *rt_rq)",
            "{",
            "\tu64 runtime = sched_rt_runtime(rt_rq);",
            "",
            "\tif (rt_rq->rt_throttled)",
            "\t\treturn rt_rq_throttled(rt_rq);",
            "",
            "\tif (runtime >= sched_rt_period(rt_rq))",
            "\t\treturn 0;",
            "",
            "\tbalance_runtime(rt_rq);",
            "\truntime = sched_rt_runtime(rt_rq);",
            "\tif (runtime == RUNTIME_INF)",
            "\t\treturn 0;",
            "",
            "\tif (rt_rq->rt_time > runtime) {",
            "\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\t\t/*",
            "\t\t * Don't actually throttle groups that have no runtime assigned",
            "\t\t * but accrue some time due to boosting.",
            "\t\t */",
            "\t\tif (likely(rt_b->rt_runtime)) {",
            "\t\t\trt_rq->rt_throttled = 1;",
            "\t\t\tprintk_deferred_once(\"sched: RT throttling activated\\n\");",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * In case we did anyway, make it go away,",
            "\t\t\t * replenishment is a joke, since it will replenish us",
            "\t\t\t * with exactly 0 ns.",
            "\t\t\t */",
            "\t\t\trt_rq->rt_time = 0;",
            "\t\t}",
            "",
            "\t\tif (rt_rq_throttled(rt_rq)) {",
            "\t\t\tsched_rt_rq_dequeue(rt_rq);",
            "\t\t\treturn 1;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "balance_runtime, balance_runtime, do_sched_rt_period_timer, sched_rt_runtime_exceeded",
          "description": "`balance_runtime`在超时时触发重新平衡，`do_sched_rt_period_timer`周期性调整运行时并检查节流状态，`sched_rt_runtime_exceeded`判断是否超出运行时限制并标记节流",
          "similarity": 0.5632336139678955
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/rt.c",
          "start_line": 529,
          "end_line": 633,
          "content": [
            "static void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\tstruct sched_rt_entity *rt_se;",
            "",
            "\tint cpu = cpu_of(rq);",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tif (!rt_se)",
            "\t\t\tenqueue_top_rt_rq(rt_rq);",
            "\t\telse if (!on_rt_rq(rt_se))",
            "\t\t\tenqueue_rt_entity(rt_se, 0);",
            "",
            "\t\tif (rt_rq->highest_prio.curr < curr->prio)",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "}",
            "static void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (!rt_se) {",
            "\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "\t\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);",
            "\t}",
            "\telse if (on_rt_rq(rt_se))",
            "\t\tdequeue_rt_entity(rt_se, 0);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;",
            "}",
            "static int rt_se_boosted(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "\tstruct task_struct *p;",
            "",
            "\tif (rt_rq)",
            "\t\treturn !!rt_rq->rt_nr_boosted;",
            "",
            "\tp = rt_task_of(rt_se);",
            "\treturn p->prio != p->normal_prio;",
            "}",
            "bool sched_rt_bandwidth_account(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\treturn (hrtimer_active(&rt_b->rt_period_timer) ||",
            "\t\trt_rq->rt_time < rt_b->rt_runtime);",
            "}",
            "static void do_balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;",
            "\tint i, weight;",
            "\tu64 rt_period;",
            "",
            "\tweight = cpumask_weight(rd->span);",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\trt_period = ktime_to_ns(rt_b->rt_period);",
            "\tfor_each_cpu(i, rd->span) {",
            "\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\ts64 diff;",
            "",
            "\t\tif (iter == rt_rq)",
            "\t\t\tcontinue;",
            "",
            "\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either all rqs have inf runtime and there's nothing to steal",
            "\t\t * or __disable_runtime() below sets a specific rq to inf to",
            "\t\t * indicate its been disabled and disallow stealing.",
            "\t\t */",
            "\t\tif (iter->rt_runtime == RUNTIME_INF)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * From runqueues with spare time, take 1/n part of their",
            "\t\t * spare time, but no more than our period.",
            "\t\t */",
            "\t\tdiff = iter->rt_runtime - iter->rt_time;",
            "\t\tif (diff > 0) {",
            "\t\t\tdiff = div_u64((u64)diff, weight);",
            "\t\t\tif (rt_rq->rt_runtime + diff > rt_period)",
            "\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;",
            "\t\t\titer->rt_runtime -= diff;",
            "\t\t\trt_rq->rt_runtime += diff;",
            "\t\t\tif (rt_rq->rt_runtime == rt_period) {",
            "\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "next:",
            "\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, rt_se_boosted, sched_rt_bandwidth_account, do_balance_runtime",
          "description": "实现实时任务队列的插入/移除逻辑，跟踪运行时间消耗，通过跨CPU运行时间平衡算法实现带宽公平分配。",
          "similarity": 0.5371553897857666
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/sched/rt.c",
          "start_line": 670,
          "end_line": 773,
          "content": [
            "static void __disable_runtime(struct rq *rq)",
            "{",
            "\tstruct root_domain *rd = rq->rd;",
            "\trt_rq_iter_t iter;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tif (unlikely(!scheduler_running))",
            "\t\treturn;",
            "",
            "\tfor_each_rt_rq(rt_rq, iter, rq) {",
            "\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\t\ts64 want;",
            "\t\tint i;",
            "",
            "\t\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either we're all inf and nobody needs to borrow, or we're",
            "\t\t * already disabled and thus have nothing to do, or we have",
            "\t\t * exactly the right amount of runtime to take out.",
            "\t\t */",
            "\t\tif (rt_rq->rt_runtime == RUNTIME_INF ||",
            "\t\t\t\trt_rq->rt_runtime == rt_b->rt_runtime)",
            "\t\t\tgoto balanced;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "",
            "\t\t/*",
            "\t\t * Calculate the difference between what we started out with",
            "\t\t * and what we current have, that's the amount of runtime",
            "\t\t * we lend and now have to reclaim.",
            "\t\t */",
            "\t\twant = rt_b->rt_runtime - rt_rq->rt_runtime;",
            "",
            "\t\t/*",
            "\t\t * Greedy reclaim, take back as much as we can.",
            "\t\t */",
            "\t\tfor_each_cpu(i, rd->span) {",
            "\t\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\t\ts64 diff;",
            "",
            "\t\t\t/*",
            "\t\t\t * Can't reclaim from ourselves or disabled runqueues.",
            "\t\t\t */",
            "\t\t\tif (iter == rt_rq || iter->rt_runtime == RUNTIME_INF)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t\tif (want > 0) {",
            "\t\t\t\tdiff = min_t(s64, iter->rt_runtime, want);",
            "\t\t\t\titer->rt_runtime -= diff;",
            "\t\t\t\twant -= diff;",
            "\t\t\t} else {",
            "\t\t\t\titer->rt_runtime -= want;",
            "\t\t\t\twant -= want;",
            "\t\t\t}",
            "\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "",
            "\t\t\tif (!want)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * We cannot be left wanting - that would mean some runtime",
            "\t\t * leaked out of the system.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(want);",
            "balanced:",
            "\t\t/*",
            "\t\t * Disable all the borrow logic by pretending we have inf",
            "\t\t * runtime - in which case borrowing doesn't make sense.",
            "\t\t */",
            "\t\trt_rq->rt_runtime = RUNTIME_INF;",
            "\t\trt_rq->rt_throttled = 0;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "",
            "\t\t/* Make rt_rq available for pick_next_task() */",
            "\t\tsched_rt_rq_enqueue(rt_rq);",
            "\t}",
            "}",
            "static void __enable_runtime(struct rq *rq)",
            "{",
            "\trt_rq_iter_t iter;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tif (unlikely(!scheduler_running))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Reset each runqueue's bandwidth settings",
            "\t */",
            "\tfor_each_rt_rq(rt_rq, iter, rq) {",
            "\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\t\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\trt_rq->rt_runtime = rt_b->rt_runtime;",
            "\t\trt_rq->rt_time = 0;",
            "\t\trt_rq->rt_throttled = 0;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "\t}",
            "}"
          ],
          "function_name": "__disable_runtime, __enable_runtime",
          "description": "`__disable_runtime`遍历实时运行队列，将每个队列的运行时设为无穷大以禁用借用逻辑，`__enable_runtime`重置各队列的运行时限制",
          "similarity": 0.5344434380531311
        },
        {
          "chunk_id": 17,
          "file_path": "kernel/sched/rt.c",
          "start_line": 2533,
          "end_line": 2686,
          "content": [
            "static void watchdog(struct rq *rq, struct task_struct *p)",
            "{",
            "\tunsigned long soft, hard;",
            "",
            "\t/* max may change after cur was read, this will be fixed next tick */",
            "\tsoft = task_rlimit(p, RLIMIT_RTTIME);",
            "\thard = task_rlimit_max(p, RLIMIT_RTTIME);",
            "",
            "\tif (soft != RLIM_INFINITY) {",
            "\t\tunsigned long next;",
            "",
            "\t\tif (p->rt.watchdog_stamp != jiffies) {",
            "\t\t\tp->rt.timeout++;",
            "\t\t\tp->rt.watchdog_stamp = jiffies;",
            "\t\t}",
            "",
            "\t\tnext = DIV_ROUND_UP(min(soft, hard), USEC_PER_SEC/HZ);",
            "\t\tif (p->rt.timeout > next) {",
            "\t\t\tposix_cputimers_rt_watchdog(&p->posix_cputimers,",
            "\t\t\t\t\t\t    p->se.sum_exec_runtime);",
            "\t\t}",
            "\t}",
            "}",
            "static inline void watchdog(struct rq *rq, struct task_struct *p) { }",
            "static void task_tick_rt(struct rq *rq, struct task_struct *p, int queued)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tupdate_rt_rq_load_avg(rq_clock_pelt(rq), rq, 1);",
            "",
            "\twatchdog(rq, p);",
            "",
            "\t/*",
            "\t * RR tasks need a special form of timeslice management.",
            "\t * FIFO tasks have no timeslices.",
            "\t */",
            "\tif (p->policy != SCHED_RR)",
            "\t\treturn;",
            "",
            "\tif (--p->rt.time_slice)",
            "\t\treturn;",
            "",
            "\tp->rt.time_slice = sched_rr_timeslice;",
            "",
            "\t/*",
            "\t * Requeue to the end of queue if we (and all of our ancestors) are not",
            "\t * the only element on the queue",
            "\t */",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tif (rt_se->run_list.prev != rt_se->run_list.next) {",
            "\t\t\trequeue_task_rt(rq, p, 0);",
            "\t\t\tresched_curr(rq);",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "}",
            "static unsigned int get_rr_interval_rt(struct rq *rq, struct task_struct *task)",
            "{",
            "\t/*",
            "\t * Time slice is 0 for SCHED_FIFO tasks",
            "\t */",
            "\tif (task->policy == SCHED_RR)",
            "\t\treturn sched_rr_timeslice;",
            "\telse",
            "\t\treturn 0;",
            "}",
            "static int task_is_throttled_rt(struct task_struct *p, int cpu)",
            "{",
            "\tstruct rt_rq *rt_rq;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\trt_rq = task_group(p)->rt_rq[cpu];",
            "#else",
            "\trt_rq = &cpu_rq(cpu)->rt;",
            "#endif",
            "",
            "\treturn rt_rq_throttled(rt_rq);",
            "}",
            "static inline int tg_has_rt_tasks(struct task_group *tg)",
            "{",
            "\tstruct task_struct *task;",
            "\tstruct css_task_iter it;",
            "\tint ret = 0;",
            "",
            "\t/*",
            "\t * Autogroups do not have RT tasks; see autogroup_create().",
            "\t */",
            "\tif (task_group_is_autogroup(tg))",
            "\t\treturn 0;",
            "",
            "\tcss_task_iter_start(&tg->css, 0, &it);",
            "\twhile (!ret && (task = css_task_iter_next(&it)))",
            "\t\tret |= rt_task(task);",
            "\tcss_task_iter_end(&it);",
            "",
            "\treturn ret;",
            "}",
            "static int tg_rt_schedulable(struct task_group *tg, void *data)",
            "{",
            "\tstruct rt_schedulable_data *d = data;",
            "\tstruct task_group *child;",
            "\tunsigned long total, sum = 0;",
            "\tu64 period, runtime;",
            "",
            "\tperiod = ktime_to_ns(tg->rt_bandwidth.rt_period);",
            "\truntime = tg->rt_bandwidth.rt_runtime;",
            "",
            "\tif (tg == d->tg) {",
            "\t\tperiod = d->rt_period;",
            "\t\truntime = d->rt_runtime;",
            "\t}",
            "",
            "\t/*",
            "\t * Cannot have more runtime than the period.",
            "\t */",
            "\tif (runtime > period && runtime != RUNTIME_INF)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Ensure we don't starve existing RT tasks if runtime turns zero.",
            "\t */",
            "\tif (rt_bandwidth_enabled() && !runtime &&",
            "\t    tg->rt_bandwidth.rt_runtime && tg_has_rt_tasks(tg))",
            "\t\treturn -EBUSY;",
            "",
            "\ttotal = to_ratio(period, runtime);",
            "",
            "\t/*",
            "\t * Nobody can have more than the global setting allows.",
            "\t */",
            "\tif (total > to_ratio(global_rt_period(), global_rt_runtime()))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * The sum of our children's runtime should not exceed our own.",
            "\t */",
            "\tlist_for_each_entry_rcu(child, &tg->children, siblings) {",
            "\t\tperiod = ktime_to_ns(child->rt_bandwidth.rt_period);",
            "\t\truntime = child->rt_bandwidth.rt_runtime;",
            "",
            "\t\tif (child == d->tg) {",
            "\t\t\tperiod = d->rt_period;",
            "\t\t\truntime = d->rt_runtime;",
            "\t\t}",
            "",
            "\t\tsum += to_ratio(period, runtime);",
            "\t}",
            "",
            "\tif (sum > total)",
            "\t\treturn -EINVAL;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "watchdog, watchdog, task_tick_rt, get_rr_interval_rt, task_is_throttled_rt, tg_has_rt_tasks, tg_rt_schedulable",
          "description": "实现实时任务的超时监控、时间片管理、任务组资源配额验证及实时任务运行时间统计功能，支持轮转调度下的时间片重置和任务重排。",
          "similarity": 0.5159932971000671
        }
      ]
    },
    {
      "source_file": "kernel/sched/cpufreq_schedutil.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:03:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\cpufreq_schedutil.c`\n\n---\n\n# `sched/cpufreq_schedutil.c` 技术文档\n\n## 1. 文件概述\n\n`sched/cpufreq_schedutil.c` 实现了 Linux 内核中基于调度器提供的 CPU 利用率数据的 **schedutil CPUFreq 调速器（governor）**。该调速器通过实时获取调度器计算的 CPU 利用率（包括 CFS、RT、DL 任务以及 I/O 等待状态），动态调整 CPU 频率，以在性能与能效之间取得平衡。其核心优势在于直接利用调度器的 `util` 信息，避免传统调速器依赖采样机制带来的延迟和不准确性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct sugov_tunables`**  \n  调速器可调参数，包含：\n  - `rate_limit_us`：频率更新的最小时间间隔（微秒），防止过于频繁的频率切换。\n\n- **`struct sugov_policy`**  \n  每个 `cpufreq_policy` 对应的 schedutil 策略实例，包含：\n  - `policy`：关联的 CPUFreq 策略。\n  - `update_lock`：保护频率更新的自旋锁。\n  - `last_freq_update_time` / `freq_update_delay_ns`：控制频率更新速率。\n  - `next_freq` / `cached_raw_freq`：目标频率与原始计算频率缓存。\n  - `irq_work` / `worker` / `thread`：用于慢速切换平台（非 fast-switch）的异步工作队列机制。\n  - `limits_changed` / `need_freq_update`：标志策略限制（如 min/max freq）是否变更。\n\n- **`struct sugov_cpu`**  \n  每个 CPU 的 schedutil 状态，包含：\n  - `update_util`：注册到调度器的回调接口（`update_util_data`）。\n  - `util` / `bw_min`：当前有效利用率及带宽最小值。\n  - `iowait_boost` / `iowait_boost_pending`：I/O 等待唤醒时的频率提升机制。\n  - `last_update`：上次更新时间戳。\n\n### 主要函数\n\n- **`sugov_should_update_freq()`**  \n  判断是否应执行频率更新，考虑硬件是否支持本 CPU 更新、策略限制变更、以及频率更新间隔限制。\n\n- **`sugov_update_next_freq()`**  \n  更新目标频率，处理策略限制变更场景，避免不必要的驱动回调。\n\n- **`get_next_freq()`**  \n  核心频率计算函数，根据 CPU 利用率、最大容量和参考频率，计算目标频率，并通过 `cpufreq_driver_resolve_freq()` 映射到驱动支持的频率。\n\n- **`sugov_get_util()`**  \n  获取当前 CPU 的综合利用率，整合 CFS/RT/DL 任务利用率、boost 值，并调用 `sugov_effective_cpu_perf()` 计算有效性能目标。\n\n- **`sugov_effective_cpu_perf()`**  \n  计算最终的有效性能目标，确保不低于最小性能要求，并限制不超过实际需求。\n\n- **`sugov_iowait_reset()` / `sugov_iowait_boost()`**  \n  实现 I/O 等待唤醒时的动态频率提升机制：短时间内连续 I/O 唤醒会逐步提升 boost 值（从 `IOWAIT_BOOST_MIN` 到最大 OPP），超过一个 tick 无 I/O 唤醒则重置。\n\n- **`get_capacity_ref_freq()`**  \n  获取用于计算 CPU 容量的参考频率，优先使用架构特定的 `arch_scale_freq_ref()`，其次为最大频率或当前频率。\n\n- **`sugov_deferred_update()`**  \n  在不支持 fast-switch 的平台上，通过 `irq_work` 触发异步频率更新。\n\n## 3. 关键实现\n\n### 频率计算算法\n- **频率不变性支持**：若系统支持频率不变调度（`arch_scale_freq_invariant()`），则直接使用调度器提供的频率不变利用率 `util`，按比例计算目标频率：  \n  `next_freq = C * max_freq * util / max`  \n  其中常数 `C = 1.25`，使在 `util/max = 0.8` 时达到 `max_freq`，提供性能余量。\n- **非频率不变性**：使用原始利用率 `util_raw` 乘以 `(curr_freq / max_freq)` 近似频率不变利用率，再计算目标频率。\n\n### I/O 等待 Boost 机制\n- 当任务因 I/O 完成而唤醒时，标记 `SCHED_CPUFREQ_IOWAIT`。\n- 若在 **一个 tick 内** 多次发生 I/O 唤醒，则 `iowait_boost` 值倍增（上限为最大 OPP 对应的利用率）。\n- 若超过一个 tick 无 I/O 唤醒，则重置 boost 值为 `IOWAIT_BOOST_MIN`（`SCHED_CAPACITY_SCALE / 8`），避免对偶发 I/O 过度响应，提升能效。\n\n### 快速切换（Fast-Switch）与异步更新\n- **Fast-Switch 平台**：支持在调度上下文中直接调用 `cpufreq_driver_fast_switch()` 更新频率，延迟最低。\n- **非 Fast-Switch 平台**：通过 `irq_work` 触发内核线程（`kthread_worker`）异步执行频率更新，避免在中断上下文或持有 rq 锁时调用可能阻塞的驱动接口。\n\n### 策略限制变更处理\n- 当用户空间修改 policy 的 min/max 频率时，`sugov_limits()` 设置 `limits_changed` 标志。\n- 下次更新时，强制重新计算频率，并通过内存屏障（`smp_mb()`）确保读取到最新的策略限制。\n\n## 4. 依赖关系\n\n- **调度器子系统**：\n  - 依赖 `update_util_data` 回调机制（通过 `cpufreq_add_update_util_hook()` 注册）。\n  - 调用 `cpu_util_cfs_boost()`、`effective_cpu_util()` 等函数获取综合利用率。\n  - 使用 `scx_cpuperf_target()`（若启用了 SCHED_CLASS_EXT）。\n- **CPUFreq 核心**：\n  - 依赖 `cpufreq_policy`、`cpufreq_driver_resolve_freq()`、`cpufreq_driver_fast_switch()` 等接口。\n  - 使用 `cpufreq_this_cpu_can_update()` 判断硬件更新能力。\n- **架构相关支持**：\n  - 依赖 `arch_scale_freq_ref()` 和 `arch_scale_freq_invariant()` 提供频率不变性信息。\n- **内核基础设施**：\n  - 使用 `irq_work`、`kthread_worker` 实现异步更新。\n  - 依赖 `TICK_NSEC` 定义 tick 时间。\n\n## 5. 使用场景\n\n- **默认高性能能效平衡场景**：现代 Linux 发行版通常将 `schedutil` 作为默认 CPUFreq 调速器，适用于大多数桌面、服务器和移动设备。\n- **实时性要求较高的系统**：由于其低延迟特性（尤其在 fast-switch 平台上），适合对响应时间敏感的应用。\n- **能效敏感设备**：通过 I/O boost 机制和精确的利用率跟踪，在保证交互性能的同时降低空闲功耗。\n- **异构多核系统（如 big.LITTLE）**：结合调度器的 CPU capacity 信息，为不同性能核提供差异化频率调整。",
      "similarity": 0.502391517162323,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 62,
          "end_line": 168,
          "content": [
            "static bool sugov_should_update_freq(struct sugov_policy *sg_policy, u64 time)",
            "{",
            "\ts64 delta_ns;",
            "",
            "\t/*",
            "\t * Since cpufreq_update_util() is called with rq->lock held for",
            "\t * the @target_cpu, our per-CPU data is fully serialized.",
            "\t *",
            "\t * However, drivers cannot in general deal with cross-CPU",
            "\t * requests, so while get_next_freq() will work, our",
            "\t * sugov_update_commit() call may not for the fast switching platforms.",
            "\t *",
            "\t * Hence stop here for remote requests if they aren't supported",
            "\t * by the hardware, as calculating the frequency is pointless if",
            "\t * we cannot in fact act on it.",
            "\t *",
            "\t * This is needed on the slow switching platforms too to prevent CPUs",
            "\t * going offline from leaving stale IRQ work items behind.",
            "\t */",
            "\tif (!cpufreq_this_cpu_can_update(sg_policy->policy))",
            "\t\treturn false;",
            "",
            "\tif (unlikely(READ_ONCE(sg_policy->limits_changed))) {",
            "\t\tWRITE_ONCE(sg_policy->limits_changed, false);",
            "\t\tsg_policy->need_freq_update = true;",
            "",
            "\t\t/*",
            "\t\t * The above limits_changed update must occur before the reads",
            "\t\t * of policy limits in cpufreq_driver_resolve_freq() or a policy",
            "\t\t * limits update might be missed, so use a memory barrier to",
            "\t\t * ensure it.",
            "\t\t *",
            "\t\t * This pairs with the write memory barrier in sugov_limits().",
            "\t\t */",
            "\t\tsmp_mb();",
            "",
            "\t\treturn true;",
            "\t}",
            "",
            "\tdelta_ns = time - sg_policy->last_freq_update_time;",
            "",
            "\treturn delta_ns >= sg_policy->freq_update_delay_ns;",
            "}",
            "static bool sugov_update_next_freq(struct sugov_policy *sg_policy, u64 time,",
            "\t\t\t\t   unsigned int next_freq)",
            "{",
            "\tif (sg_policy->need_freq_update) {",
            "\t\tsg_policy->need_freq_update = false;",
            "\t\t/*",
            "\t\t * The policy limits have changed, but if the return value of",
            "\t\t * cpufreq_driver_resolve_freq() after applying the new limits",
            "\t\t * is still equal to the previously selected frequency, the",
            "\t\t * driver callback need not be invoked unless the driver",
            "\t\t * specifically wants that to happen on every update of the",
            "\t\t * policy limits.",
            "\t\t */",
            "\t\tif (sg_policy->next_freq == next_freq &&",
            "\t\t    !cpufreq_driver_test_flags(CPUFREQ_NEED_UPDATE_LIMITS))",
            "\t\t\treturn false;",
            "\t} else if (sg_policy->next_freq == next_freq) {",
            "\t\treturn false;",
            "\t}",
            "",
            "\tsg_policy->next_freq = next_freq;",
            "\tsg_policy->last_freq_update_time = time;",
            "",
            "\treturn true;",
            "}",
            "static void sugov_deferred_update(struct sugov_policy *sg_policy)",
            "{",
            "\tif (!sg_policy->work_in_progress) {",
            "\t\tsg_policy->work_in_progress = true;",
            "\t\tirq_work_queue(&sg_policy->irq_work);",
            "\t}",
            "}",
            "static __always_inline",
            "unsigned long get_capacity_ref_freq(struct cpufreq_policy *policy)",
            "{",
            "\tunsigned int freq = arch_scale_freq_ref(policy->cpu);",
            "",
            "\tif (freq)",
            "\t\treturn freq;",
            "",
            "\tif (arch_scale_freq_invariant())",
            "\t\treturn policy->cpuinfo.max_freq;",
            "",
            "\t/*",
            "\t * Apply a 25% margin so that we select a higher frequency than",
            "\t * the current one before the CPU is fully busy:",
            "\t */",
            "\treturn policy->cur + (policy->cur >> 2);",
            "}",
            "static unsigned int get_next_freq(struct sugov_policy *sg_policy,",
            "\t\t\t\t  unsigned long util, unsigned long max)",
            "{",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tunsigned int freq;",
            "",
            "\tfreq = get_capacity_ref_freq(policy);",
            "\tfreq = map_util_freq(util, freq, max);",
            "",
            "\tif (freq == sg_policy->cached_raw_freq && !sg_policy->need_freq_update)",
            "\t\treturn sg_policy->next_freq;",
            "",
            "\tsg_policy->cached_raw_freq = freq;",
            "\treturn cpufreq_driver_resolve_freq(policy, freq);",
            "}"
          ],
          "function_name": "sugov_should_update_freq, sugov_update_next_freq, sugov_deferred_update, get_capacity_ref_freq, get_next_freq",
          "description": "实现了频率更新核心逻辑，sugov_should_update_freq判断是否需要更新频率，sugov_update_next_freq计算并记录目标频率，sugov_deferred_update触发异步更新，get_capacity_ref_freq获取基准频率，get_next_freq结合利用率计算最终目标频率。",
          "similarity": 0.5204793810844421
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 508,
          "end_line": 651,
          "content": [
            "static void",
            "sugov_update_shared(struct update_util_data *hook, u64 time, unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tunsigned int next_f;",
            "",
            "\traw_spin_lock(&sg_policy->update_lock);",
            "",
            "\tsugov_iowait_boost(sg_cpu, time, flags);",
            "\tsg_cpu->last_update = time;",
            "",
            "\tignore_dl_rate_limit(sg_cpu);",
            "",
            "\tif (sugov_should_update_freq(sg_policy, time)) {",
            "\t\tnext_f = sugov_next_freq_shared(sg_cpu, time);",
            "",
            "\t\tif (!sugov_update_next_freq(sg_policy, time, next_f))",
            "\t\t\tgoto unlock;",
            "",
            "\t\tif (sg_policy->policy->fast_switch_enabled)",
            "\t\t\tcpufreq_driver_fast_switch(sg_policy->policy, next_f);",
            "\t\telse",
            "\t\t\tsugov_deferred_update(sg_policy);",
            "\t}",
            "unlock:",
            "\traw_spin_unlock(&sg_policy->update_lock);",
            "}",
            "static void sugov_work(struct kthread_work *work)",
            "{",
            "\tstruct sugov_policy *sg_policy = container_of(work, struct sugov_policy, work);",
            "\tunsigned int freq;",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * Hold sg_policy->update_lock shortly to handle the case where:",
            "\t * in case sg_policy->next_freq is read here, and then updated by",
            "\t * sugov_deferred_update() just before work_in_progress is set to false",
            "\t * here, we may miss queueing the new update.",
            "\t *",
            "\t * Note: If a work was queued after the update_lock is released,",
            "\t * sugov_work() will just be called again by kthread_work code; and the",
            "\t * request will be proceed before the sugov thread sleeps.",
            "\t */",
            "\traw_spin_lock_irqsave(&sg_policy->update_lock, flags);",
            "\tfreq = sg_policy->next_freq;",
            "\tsg_policy->work_in_progress = false;",
            "\traw_spin_unlock_irqrestore(&sg_policy->update_lock, flags);",
            "",
            "\tmutex_lock(&sg_policy->work_lock);",
            "\t__cpufreq_driver_target(sg_policy->policy, freq, CPUFREQ_RELATION_L);",
            "\tmutex_unlock(&sg_policy->work_lock);",
            "}",
            "static void sugov_irq_work(struct irq_work *irq_work)",
            "{",
            "\tstruct sugov_policy *sg_policy;",
            "",
            "\tsg_policy = container_of(irq_work, struct sugov_policy, irq_work);",
            "",
            "\tkthread_queue_work(&sg_policy->worker, &sg_policy->work);",
            "}",
            "static ssize_t rate_limit_us_show(struct gov_attr_set *attr_set, char *buf)",
            "{",
            "\tstruct sugov_tunables *tunables = to_sugov_tunables(attr_set);",
            "",
            "\treturn sprintf(buf, \"%u\\n\", tunables->rate_limit_us);",
            "}",
            "static ssize_t",
            "rate_limit_us_store(struct gov_attr_set *attr_set, const char *buf, size_t count)",
            "{",
            "\tstruct sugov_tunables *tunables = to_sugov_tunables(attr_set);",
            "\tstruct sugov_policy *sg_policy;",
            "\tunsigned int rate_limit_us;",
            "",
            "\tif (kstrtouint(buf, 10, &rate_limit_us))",
            "\t\treturn -EINVAL;",
            "",
            "\ttunables->rate_limit_us = rate_limit_us;",
            "",
            "\tlist_for_each_entry(sg_policy, &attr_set->policy_list, tunables_hook)",
            "\t\tsg_policy->freq_update_delay_ns = rate_limit_us * NSEC_PER_USEC;",
            "",
            "\treturn count;",
            "}",
            "static void sugov_tunables_free(struct kobject *kobj)",
            "{",
            "\tstruct gov_attr_set *attr_set = to_gov_attr_set(kobj);",
            "",
            "\tkfree(to_sugov_tunables(attr_set));",
            "}",
            "static void sugov_policy_free(struct sugov_policy *sg_policy)",
            "{",
            "\tkfree(sg_policy);",
            "}",
            "static int sugov_kthread_create(struct sugov_policy *sg_policy)",
            "{",
            "\tstruct task_struct *thread;",
            "\tstruct sched_attr attr = {",
            "\t\t.size\t\t= sizeof(struct sched_attr),",
            "\t\t.sched_policy\t= SCHED_DEADLINE,",
            "\t\t.sched_flags\t= SCHED_FLAG_SUGOV,",
            "\t\t.sched_nice\t= 0,",
            "\t\t.sched_priority\t= 0,",
            "\t\t/*",
            "\t\t * Fake (unused) bandwidth; workaround to \"fix\"",
            "\t\t * priority inheritance.",
            "\t\t */",
            "\t\t.sched_runtime\t=  1000000,",
            "\t\t.sched_deadline = 10000000,",
            "\t\t.sched_period\t= 10000000,",
            "\t};",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tint ret;",
            "",
            "\t/* kthread only required for slow path */",
            "\tif (policy->fast_switch_enabled)",
            "\t\treturn 0;",
            "",
            "\tkthread_init_work(&sg_policy->work, sugov_work);",
            "\tkthread_init_worker(&sg_policy->worker);",
            "\tthread = kthread_create(kthread_worker_fn, &sg_policy->worker,",
            "\t\t\t\t\"sugov:%d\",",
            "\t\t\t\tcpumask_first(policy->related_cpus));",
            "\tif (IS_ERR(thread)) {",
            "\t\tpr_err(\"failed to create sugov thread: %ld\\n\", PTR_ERR(thread));",
            "\t\treturn PTR_ERR(thread);",
            "\t}",
            "",
            "\tret = sched_setattr_nocheck(thread, &attr);",
            "\tif (ret) {",
            "\t\tkthread_stop(thread);",
            "\t\tpr_warn(\"%s: failed to set SCHED_DEADLINE\\n\", __func__);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tsg_policy->thread = thread;",
            "\tkthread_bind_mask(thread, policy->related_cpus);",
            "\tinit_irq_work(&sg_policy->irq_work, sugov_irq_work);",
            "\tmutex_init(&sg_policy->work_lock);",
            "",
            "\twake_up_process(thread);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sugov_update_shared, sugov_work, sugov_irq_work, rate_limit_us_show, rate_limit_us_store, sugov_tunables_free, sugov_policy_free, sugov_kthread_create",
          "description": "管理频率调节的工作线程和参数配置，sugov_kthread_create创建慢速切换场景的后台线程，rate_limit_us_*/提供速率限制配置接口，sugov_work/sugov_irq_work处理异步频率更新任务。",
          "similarity": 0.5168067812919617
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 381,
          "end_line": 496,
          "content": [
            "static inline bool sugov_hold_freq(struct sugov_cpu *sg_cpu) { return false; }",
            "static inline void ignore_dl_rate_limit(struct sugov_cpu *sg_cpu)",
            "{",
            "\tif (cpu_bw_dl(cpu_rq(sg_cpu->cpu)) > sg_cpu->bw_min)",
            "\t\tWRITE_ONCE(sg_cpu->sg_policy->limits_changed, true);",
            "}",
            "static inline bool sugov_update_single_common(struct sugov_cpu *sg_cpu,",
            "\t\t\t\t\t      u64 time, unsigned long max_cap,",
            "\t\t\t\t\t      unsigned int flags)",
            "{",
            "\tunsigned long boost;",
            "",
            "\tsugov_iowait_boost(sg_cpu, time, flags);",
            "\tsg_cpu->last_update = time;",
            "",
            "\tignore_dl_rate_limit(sg_cpu);",
            "",
            "\tif (!sugov_should_update_freq(sg_cpu->sg_policy, time))",
            "\t\treturn false;",
            "",
            "\tboost = sugov_iowait_apply(sg_cpu, time, max_cap);",
            "\tsugov_get_util(sg_cpu, boost);",
            "",
            "\treturn true;",
            "}",
            "static void sugov_update_single_freq(struct update_util_data *hook, u64 time,",
            "\t\t\t\t     unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tunsigned int cached_freq = sg_policy->cached_raw_freq;",
            "\tunsigned long max_cap;",
            "\tunsigned int next_f;",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tif (!sugov_update_single_common(sg_cpu, time, max_cap, flags))",
            "\t\treturn;",
            "",
            "\tnext_f = get_next_freq(sg_policy, sg_cpu->util, max_cap);",
            "",
            "\tif (sugov_hold_freq(sg_cpu) && next_f < sg_policy->next_freq &&",
            "\t    !sg_policy->need_freq_update) {",
            "\t\tnext_f = sg_policy->next_freq;",
            "",
            "\t\t/* Restore cached freq as next_freq has changed */",
            "\t\tsg_policy->cached_raw_freq = cached_freq;",
            "\t}",
            "",
            "\tif (!sugov_update_next_freq(sg_policy, time, next_f))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This code runs under rq->lock for the target CPU, so it won't run",
            "\t * concurrently on two different CPUs for the same target and it is not",
            "\t * necessary to acquire the lock in the fast switch case.",
            "\t */",
            "\tif (sg_policy->policy->fast_switch_enabled) {",
            "\t\tcpufreq_driver_fast_switch(sg_policy->policy, next_f);",
            "\t} else {",
            "\t\traw_spin_lock(&sg_policy->update_lock);",
            "\t\tsugov_deferred_update(sg_policy);",
            "\t\traw_spin_unlock(&sg_policy->update_lock);",
            "\t}",
            "}",
            "static void sugov_update_single_perf(struct update_util_data *hook, u64 time,",
            "\t\t\t\t     unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tunsigned long prev_util = sg_cpu->util;",
            "\tunsigned long max_cap;",
            "",
            "\t/*",
            "\t * Fall back to the \"frequency\" path if frequency invariance is not",
            "\t * supported, because the direct mapping between the utilization and",
            "\t * the performance levels depends on the frequency invariance.",
            "\t */",
            "\tif (!arch_scale_freq_invariant()) {",
            "\t\tsugov_update_single_freq(hook, time, flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tif (!sugov_update_single_common(sg_cpu, time, max_cap, flags))",
            "\t\treturn;",
            "",
            "\tif (sugov_hold_freq(sg_cpu) && sg_cpu->util < prev_util)",
            "\t\tsg_cpu->util = prev_util;",
            "",
            "\tcpufreq_driver_adjust_perf(sg_cpu->cpu, sg_cpu->bw_min,",
            "\t\t\t\t   sg_cpu->util, max_cap);",
            "",
            "\tsg_cpu->sg_policy->last_freq_update_time = time;",
            "}",
            "static unsigned int sugov_next_freq_shared(struct sugov_cpu *sg_cpu, u64 time)",
            "{",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tunsigned long util = 0, max_cap;",
            "\tunsigned int j;",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tfor_each_cpu(j, policy->cpus) {",
            "\t\tstruct sugov_cpu *j_sg_cpu = &per_cpu(sugov_cpu, j);",
            "\t\tunsigned long boost;",
            "",
            "\t\tboost = sugov_iowait_apply(j_sg_cpu, time, max_cap);",
            "\t\tsugov_get_util(j_sg_cpu, boost);",
            "",
            "\t\tutil = max(j_sg_cpu->util, util);",
            "\t}",
            "",
            "\treturn get_next_freq(sg_policy, util, max_cap);",
            "}"
          ],
          "function_name": "sugov_hold_freq, ignore_dl_rate_limit, sugov_update_single_common, sugov_update_single_freq, sugov_update_single_perf, sugov_next_freq_shared",
          "description": "实现单核/多核频率调整逻辑，sugov_update_single_freq处理单核频率更新，sugov_update_single_perf处理性能调优路径，sugov_next_freq_shared计算多核共享场景下的全局目标频率。",
          "similarity": 0.5106566548347473
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 827,
          "end_line": 916,
          "content": [
            "static int sugov_start(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "\tvoid (*uu)(struct update_util_data *data, u64 time, unsigned int flags);",
            "\tunsigned int cpu;",
            "",
            "\tsg_policy->freq_update_delay_ns\t= sg_policy->tunables->rate_limit_us * NSEC_PER_USEC;",
            "\tsg_policy->last_freq_update_time\t= 0;",
            "\tsg_policy->next_freq\t\t\t= 0;",
            "\tsg_policy->work_in_progress\t\t= false;",
            "\tsg_policy->limits_changed\t\t= false;",
            "\tsg_policy->cached_raw_freq\t\t= 0;",
            "",
            "\tsg_policy->need_freq_update = cpufreq_driver_test_flags(CPUFREQ_NEED_UPDATE_LIMITS);",
            "",
            "\tfor_each_cpu(cpu, policy->cpus) {",
            "\t\tstruct sugov_cpu *sg_cpu = &per_cpu(sugov_cpu, cpu);",
            "",
            "\t\tmemset(sg_cpu, 0, sizeof(*sg_cpu));",
            "\t\tsg_cpu->cpu\t\t\t= cpu;",
            "\t\tsg_cpu->sg_policy\t\t= sg_policy;",
            "\t}",
            "",
            "\tif (policy_is_shared(policy))",
            "\t\tuu = sugov_update_shared;",
            "\telse if (policy->fast_switch_enabled && cpufreq_driver_has_adjust_perf())",
            "\t\tuu = sugov_update_single_perf;",
            "\telse",
            "\t\tuu = sugov_update_single_freq;",
            "",
            "\tfor_each_cpu(cpu, policy->cpus) {",
            "\t\tstruct sugov_cpu *sg_cpu = &per_cpu(sugov_cpu, cpu);",
            "",
            "\t\tcpufreq_add_update_util_hook(cpu, &sg_cpu->update_util, uu);",
            "\t}",
            "\treturn 0;",
            "}",
            "static void sugov_stop(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "\tunsigned int cpu;",
            "",
            "\tfor_each_cpu(cpu, policy->cpus)",
            "\t\tcpufreq_remove_update_util_hook(cpu);",
            "",
            "\tsynchronize_rcu();",
            "",
            "\tif (!policy->fast_switch_enabled) {",
            "\t\tirq_work_sync(&sg_policy->irq_work);",
            "\t\tkthread_cancel_work_sync(&sg_policy->work);",
            "\t}",
            "}",
            "static void sugov_limits(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "",
            "\tif (!policy->fast_switch_enabled) {",
            "\t\tmutex_lock(&sg_policy->work_lock);",
            "\t\tcpufreq_policy_apply_limits(policy);",
            "\t\tmutex_unlock(&sg_policy->work_lock);",
            "\t}",
            "",
            "\t/*",
            "\t * The limits_changed update below must take place before the updates",
            "\t * of policy limits in cpufreq_set_policy() or a policy limits update",
            "\t * might be missed, so use a memory barrier to ensure it.",
            "\t *",
            "\t * This pairs with the memory barrier in sugov_should_update_freq().",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\tWRITE_ONCE(sg_policy->limits_changed, true);",
            "}",
            "static void rebuild_sd_workfn(struct work_struct *work)",
            "{",
            "\trebuild_sched_domains_energy();",
            "}",
            "void sched_cpufreq_governor_change(struct cpufreq_policy *policy,",
            "\t\t\t\t  struct cpufreq_governor *old_gov)",
            "{",
            "\tif (old_gov == &schedutil_gov || policy->governor == &schedutil_gov) {",
            "\t\t/*",
            "\t\t * When called from the cpufreq_register_driver() path, the",
            "\t\t * cpu_hotplug_lock is already held, so use a work item to",
            "\t\t * avoid nested locking in rebuild_sched_domains().",
            "\t\t */",
            "\t\tschedule_work(&rebuild_sd_work);",
            "\t}",
            "",
            "}"
          ],
          "function_name": "sugov_start, sugov_stop, sugov_limits, rebuild_sd_workfn, sched_cpufreq_governor_change",
          "description": "sugov_start 注册CPU利用率更新钩子函数并初始化频率更新参数；sugov_stop 移除所有CPU的更新钩子并同步RCU状态；sugov_limits 应用频率限制并标记策略变更；rebuild_sd_workfn 触发调度域能量重新构建；sched_cpufreq_governor_change 在策略切换时安排调度域重建工作",
          "similarity": 0.49798935651779175
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 701,
          "end_line": 809,
          "content": [
            "static void sugov_kthread_stop(struct sugov_policy *sg_policy)",
            "{",
            "\t/* kthread only required for slow path */",
            "\tif (sg_policy->policy->fast_switch_enabled)",
            "\t\treturn;",
            "",
            "\tkthread_flush_worker(&sg_policy->worker);",
            "\tkthread_stop(sg_policy->thread);",
            "\tmutex_destroy(&sg_policy->work_lock);",
            "}",
            "static void sugov_clear_global_tunables(void)",
            "{",
            "\tif (!have_governor_per_policy())",
            "\t\tglobal_tunables = NULL;",
            "}",
            "static int sugov_init(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy;",
            "\tstruct sugov_tunables *tunables;",
            "\tint ret = 0;",
            "",
            "\t/* State should be equivalent to EXIT */",
            "\tif (policy->governor_data)",
            "\t\treturn -EBUSY;",
            "",
            "\tcpufreq_enable_fast_switch(policy);",
            "",
            "\tsg_policy = sugov_policy_alloc(policy);",
            "\tif (!sg_policy) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto disable_fast_switch;",
            "\t}",
            "",
            "\tret = sugov_kthread_create(sg_policy);",
            "\tif (ret)",
            "\t\tgoto free_sg_policy;",
            "",
            "\tmutex_lock(&global_tunables_lock);",
            "",
            "\tif (global_tunables) {",
            "\t\tif (WARN_ON(have_governor_per_policy())) {",
            "\t\t\tret = -EINVAL;",
            "\t\t\tgoto stop_kthread;",
            "\t\t}",
            "\t\tpolicy->governor_data = sg_policy;",
            "\t\tsg_policy->tunables = global_tunables;",
            "",
            "\t\tgov_attr_set_get(&global_tunables->attr_set, &sg_policy->tunables_hook);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttunables = sugov_tunables_alloc(sg_policy);",
            "\tif (!tunables) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto stop_kthread;",
            "\t}",
            "",
            "\ttunables->rate_limit_us = cpufreq_policy_transition_delay_us(policy);",
            "",
            "\tpolicy->governor_data = sg_policy;",
            "\tsg_policy->tunables = tunables;",
            "",
            "\tret = kobject_init_and_add(&tunables->attr_set.kobj, &sugov_tunables_ktype,",
            "\t\t\t\t   get_governor_parent_kobj(policy), \"%s\",",
            "\t\t\t\t   schedutil_gov.name);",
            "\tif (ret)",
            "\t\tgoto fail;",
            "",
            "out:",
            "\tmutex_unlock(&global_tunables_lock);",
            "\treturn 0;",
            "",
            "fail:",
            "\tkobject_put(&tunables->attr_set.kobj);",
            "\tpolicy->governor_data = NULL;",
            "\tsugov_clear_global_tunables();",
            "",
            "stop_kthread:",
            "\tsugov_kthread_stop(sg_policy);",
            "\tmutex_unlock(&global_tunables_lock);",
            "",
            "free_sg_policy:",
            "\tsugov_policy_free(sg_policy);",
            "",
            "disable_fast_switch:",
            "\tcpufreq_disable_fast_switch(policy);",
            "",
            "\tpr_err(\"initialization failed (error %d)\\n\", ret);",
            "\treturn ret;",
            "}",
            "static void sugov_exit(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "\tstruct sugov_tunables *tunables = sg_policy->tunables;",
            "\tunsigned int count;",
            "",
            "\tmutex_lock(&global_tunables_lock);",
            "",
            "\tcount = gov_attr_set_put(&tunables->attr_set, &sg_policy->tunables_hook);",
            "\tpolicy->governor_data = NULL;",
            "\tif (!count)",
            "\t\tsugov_clear_global_tunables();",
            "",
            "\tmutex_unlock(&global_tunables_lock);",
            "",
            "\tsugov_kthread_stop(sg_policy);",
            "\tsugov_policy_free(sg_policy);",
            "\tcpufreq_disable_fast_switch(policy);",
            "}"
          ],
          "function_name": "sugov_kthread_stop, sugov_clear_global_tunables, sugov_init, sugov_exit",
          "description": "sugov_kthread_stop 停止慢速路径相关内核线程并释放锁资源；sugov_clear_global_tunables 清除全局调谐参数指针；sugov_init 初始化CPU频率策略模块，分配策略结构体并创建内核线程；sugov_exit 释放策略资源，停止线程并禁用快速切换功能",
          "similarity": 0.4765297770500183
        }
      ]
    }
  ]
}