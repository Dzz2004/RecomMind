{
  "query": "RAID块设备与文件系统一致性",
  "timestamp": "2025-12-26 00:26:28",
  "retrieved_files": [
    {
      "source_file": "kernel/dma/coherent.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:10:06\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\coherent.c`\n\n---\n\n# `dma/coherent.c` 技术文档\n\n## 1. 文件概述\n\n`dma/coherent.c` 实现了 Linux 内核中**设备专属一致性 DMA 内存池**（per-device coherent DMA memory pool）的管理机制。该机制允许平台代码（如设备树解析代码）为特定设备预分配一块物理连续、CPU 与设备均可高效访问的内存区域，并通过标准的 `dma_alloc_coherent()` 接口从该内存池中分配内存，从而避免通用 DMA 分配器的开销或满足硬件对 DMA 地址范围的特殊要求。\n\n## 2. 核心功能\n\n### 数据结构\n\n- **`struct dma_coherent_mem`**  \n  描述一个设备专属的一致性 DMA 内存池：\n  - `virt_base`：CPU 虚拟地址（通过 `memremap()` 映射）\n  - `device_base`：设备视角的起始 DMA 地址\n  - `pfn_base`：物理内存起始页帧号（PFN）\n  - `size`：内存池大小（以页为单位）\n  - `bitmap`：位图，用于跟踪已分配/空闲页面\n  - `spinlock`：保护位图操作的自旋锁\n  - `use_dev_dma_pfn_offset`：是否使用设备特定的 PFN 偏移转换 DMA 地址\n\n### 主要函数\n\n- **`dma_declare_coherent_memory()`**  \n  由平台代码调用，为设备注册一个一致性 DMA 内存池。\n  \n- **`dma_release_coherent_memory()`**  \n  释放设备关联的一致性内存池资源。\n\n- **`dma_alloc_from_dev_coherent()`**  \n  供架构相关 `dma_alloc_coherent()` 实现调用，尝试从设备专属内存池分配内存。\n\n- **`dma_release_from_dev_coherent()`**  \n  供架构相关 `dma_free_coherent()` 实现调用，尝试释放内存到设备专属内存池。\n\n- **`dma_mmap_from_dev_coherent()`**  \n  供 `dma_mmap_coherent()` 实现调用，将设备专属内存映射到用户空间。\n\n- **内部辅助函数**  \n  - `dma_init_coherent_memory()`：初始化内存池结构\n  - `_dma_release_coherent_memory()`：释放内存池资源\n  - `dma_assign_coherent_memory()`：将内存池绑定到设备\n  - `__dma_alloc_from_coherent()` / `__dma_release_from_coherent()` / `__dma_mmap_from_coherent()`：核心分配/释放/映射逻辑\n\n## 3. 关键实现\n\n### 内存池初始化\n- 使用 `memremap(phys_addr, size, MEMREMAP_WC)` 将指定物理地址映射为 CPU 可访问的虚拟地址（通常使用写合并 WC 属性）。\n- 通过 `bitmap_zalloc()` 分配位图，用于管理页级别的内存分配。\n- 支持两种 DMA 地址计算方式：\n  - 直接使用传入的 `device_addr`\n  - 通过 `phys_to_dma()` 转换物理地址（当 `use_dev_dma_pfn_offset=true` 时）\n\n### 内存分配算法\n- 使用 `bitmap_find_free_region()` 在位图中查找连续的空闲页面块（按 2 的幂对齐）。\n- 分配成功后返回虚拟地址和对应的设备 DMA 地址。\n- 分配的内存会被 `memset()` 清零。\n\n### 线程安全\n- 所有位图操作（分配/释放）均通过 `spinlock` 保护，确保 SMP 环境下的安全性。\n- 使用 `spin_lock_irqsave()`/`spin_unlock_irqrestore()` 禁用本地中断，防止死锁。\n\n### 地址验证\n- 释放和 mmap 操作前会验证虚拟地址是否落在设备内存池范围内，确保操作的安全性。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/io.h>`：提供 `memremap()`/`memunmap()`\n  - `<linux/slab.h>`：提供 `kzalloc()`/`kfree()`\n  - `<linux/dma-direct.h>`：提供 `phys_to_dma()`\n  - `<linux/dma-map-ops.h>`：DMA 映射操作接口\n  - `<linux/bitmap.h>`：位图操作（隐式包含）\n\n- **架构依赖**：\n  - 依赖架构特定的 `dma_alloc_coherent()` 实现调用 `dma_alloc_from_dev_coherent()` 等接口。\n  - 依赖 `phys_to_dma()` 的正确实现（在 `dma-direct.c` 中定义）。\n\n- **配置依赖**：\n  - `CONFIG_DMA_GLOBAL_POOL`：文件末尾有未完成的条件编译代码（可能用于全局 DMA 池扩展）。\n\n## 5. 使用场景\n\n1. **嵌入式/SoC 平台**  \n   当设备（如 GPU、DSP、网络控制器）要求 DMA 内存位于特定物理地址范围（如 DDR 的保留区域）时，平台代码通过设备树解析调用 `dma_declare_coherent_memory()` 预注册内存池。\n\n2. **性能敏感场景**  \n   避免通用 DMA 分配器（如 CMA）的运行时开销，为高频 DMA 操作提供快速、确定性的内存分配。\n\n3. **IOMMU 旁路场景**  \n   在无 IOMMU 或 IOMMU 被绕过的系统中，确保设备能直接访问物理连续内存。\n\n4. **用户空间映射**  \n   通过 `dma_mmap_from_dev_coherent()` 支持将设备专属 DMA 缓冲区直接映射到用户空间（如 V4L2、DRM 驱动）。",
      "similarity": 0.5571116805076599,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/coherent.c",
          "start_line": 30,
          "end_line": 132,
          "content": [
            "static inline dma_addr_t dma_get_device_base(struct device *dev,",
            "\t\t\t\t\t     struct dma_coherent_mem * mem)",
            "{",
            "\tif (mem->use_dev_dma_pfn_offset)",
            "\t\treturn phys_to_dma(dev, PFN_PHYS(mem->pfn_base));",
            "\treturn mem->device_base;",
            "}",
            "static void _dma_release_coherent_memory(struct dma_coherent_mem *mem)",
            "{",
            "\tif (!mem)",
            "\t\treturn;",
            "",
            "\tmemunmap(mem->virt_base);",
            "\tbitmap_free(mem->bitmap);",
            "\tkfree(mem);",
            "}",
            "static int dma_assign_coherent_memory(struct device *dev,",
            "\t\t\t\t      struct dma_coherent_mem *mem)",
            "{",
            "\tif (!dev)",
            "\t\treturn -ENODEV;",
            "",
            "\tif (dev->dma_mem)",
            "\t\treturn -EBUSY;",
            "",
            "\tdev->dma_mem = mem;",
            "\treturn 0;",
            "}",
            "int dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,",
            "\t\t\t\tdma_addr_t device_addr, size_t size)",
            "{",
            "\tstruct dma_coherent_mem *mem;",
            "\tint ret;",
            "",
            "\tmem = dma_init_coherent_memory(phys_addr, device_addr, size, false);",
            "\tif (IS_ERR(mem))",
            "\t\treturn PTR_ERR(mem);",
            "",
            "\tret = dma_assign_coherent_memory(dev, mem);",
            "\tif (ret)",
            "\t\t_dma_release_coherent_memory(mem);",
            "\treturn ret;",
            "}",
            "void dma_release_coherent_memory(struct device *dev)",
            "{",
            "\tif (dev) {",
            "\t\t_dma_release_coherent_memory(dev->dma_mem);",
            "\t\tdev->dma_mem = NULL;",
            "\t}",
            "}",
            "int dma_alloc_from_dev_coherent(struct device *dev, ssize_t size,",
            "\t\tdma_addr_t *dma_handle, void **ret)",
            "{",
            "\tstruct dma_coherent_mem *mem = dev_get_coherent_memory(dev);",
            "",
            "\tif (!mem)",
            "\t\treturn 0;",
            "",
            "\t*ret = __dma_alloc_from_coherent(dev, mem, size, dma_handle);",
            "\treturn 1;",
            "}",
            "static int __dma_release_from_coherent(struct dma_coherent_mem *mem,",
            "\t\t\t\t       int order, void *vaddr)",
            "{",
            "\tif (mem && vaddr >= mem->virt_base && vaddr <",
            "\t\t   (mem->virt_base + ((dma_addr_t)mem->size << PAGE_SHIFT))) {",
            "\t\tint page = (vaddr - mem->virt_base) >> PAGE_SHIFT;",
            "\t\tunsigned long flags;",
            "",
            "\t\tspin_lock_irqsave(&mem->spinlock, flags);",
            "\t\tbitmap_release_region(mem->bitmap, page, order);",
            "\t\tspin_unlock_irqrestore(&mem->spinlock, flags);",
            "\t\treturn 1;",
            "\t}",
            "\treturn 0;",
            "}",
            "int dma_release_from_dev_coherent(struct device *dev, int order, void *vaddr)",
            "{",
            "\tstruct dma_coherent_mem *mem = dev_get_coherent_memory(dev);",
            "",
            "\treturn __dma_release_from_coherent(mem, order, vaddr);",
            "}",
            "static int __dma_mmap_from_coherent(struct dma_coherent_mem *mem,",
            "\t\tstruct vm_area_struct *vma, void *vaddr, size_t size, int *ret)",
            "{",
            "\tif (mem && vaddr >= mem->virt_base && vaddr + size <=",
            "\t\t   (mem->virt_base + ((dma_addr_t)mem->size << PAGE_SHIFT))) {",
            "\t\tunsigned long off = vma->vm_pgoff;",
            "\t\tint start = (vaddr - mem->virt_base) >> PAGE_SHIFT;",
            "\t\tunsigned long user_count = vma_pages(vma);",
            "\t\tint count = PAGE_ALIGN(size) >> PAGE_SHIFT;",
            "",
            "\t\t*ret = -ENXIO;",
            "\t\tif (off < count && user_count <= count - off) {",
            "\t\t\tunsigned long pfn = mem->pfn_base + start + off;",
            "\t\t\t*ret = remap_pfn_range(vma, vma->vm_start, pfn,",
            "\t\t\t\t\t       user_count << PAGE_SHIFT,",
            "\t\t\t\t\t       vma->vm_page_prot);",
            "\t\t}",
            "\t\treturn 1;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "dma_get_device_base, _dma_release_coherent_memory, dma_assign_coherent_memory, dma_declare_coherent_memory, dma_release_coherent_memory, dma_alloc_from_dev_coherent, __dma_release_from_coherent, dma_release_from_dev_coherent, __dma_mmap_from_coherent",
          "description": "实现了DMA一致性内存的分配/释放逻辑，包括声明设备特定DMA区域、分配/释放内存块、处理页表映射及释放操作，通过位图管理内存使用状态并处理不同架构下的DMA地址转换。",
          "similarity": 0.54146409034729
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/coherent.c",
          "start_line": 1,
          "end_line": 29,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Coherent per-device memory handling.",
            " * Borrowed from i386",
            " */",
            "#include <linux/io.h>",
            "#include <linux/slab.h>",
            "#include <linux/kernel.h>",
            "#include <linux/module.h>",
            "#include <linux/dma-direct.h>",
            "#include <linux/dma-map-ops.h>",
            "",
            "struct dma_coherent_mem {",
            "\tvoid\t\t*virt_base;",
            "\tdma_addr_t\tdevice_base;",
            "\tunsigned long\tpfn_base;",
            "\tint\t\tsize;",
            "\tunsigned long\t*bitmap;",
            "\tspinlock_t\tspinlock;",
            "\tbool\t\tuse_dev_dma_pfn_offset;",
            "};",
            "",
            "static inline struct dma_coherent_mem *dev_get_coherent_memory(struct device *dev)",
            "{",
            "\tif (dev && dev->dma_mem)",
            "\t\treturn dev->dma_mem;",
            "\treturn NULL;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了dma_coherent_mem结构体，用于管理设备DMA一致性内存区域，包含虚拟地址基址、设备物理地址基址、PFN基址、大小及位图锁等字段，提供获取设备DMA内存的辅助函数。",
          "similarity": 0.4840003252029419
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/dma/coherent.c",
          "start_line": 271,
          "end_line": 359,
          "content": [
            "int dma_mmap_from_dev_coherent(struct device *dev, struct vm_area_struct *vma,",
            "\t\t\t   void *vaddr, size_t size, int *ret)",
            "{",
            "\tstruct dma_coherent_mem *mem = dev_get_coherent_memory(dev);",
            "",
            "\treturn __dma_mmap_from_coherent(mem, vma, vaddr, size, ret);",
            "}",
            "int dma_release_from_global_coherent(int order, void *vaddr)",
            "{",
            "\tif (!dma_coherent_default_memory)",
            "\t\treturn 0;",
            "",
            "\treturn __dma_release_from_coherent(dma_coherent_default_memory, order,",
            "\t\t\tvaddr);",
            "}",
            "int dma_mmap_from_global_coherent(struct vm_area_struct *vma, void *vaddr,",
            "\t\t\t\t   size_t size, int *ret)",
            "{",
            "\tif (!dma_coherent_default_memory)",
            "\t\treturn 0;",
            "",
            "\treturn __dma_mmap_from_coherent(dma_coherent_default_memory, vma,",
            "\t\t\t\t\tvaddr, size, ret);",
            "}",
            "int dma_init_global_coherent(phys_addr_t phys_addr, size_t size)",
            "{",
            "\tstruct dma_coherent_mem *mem;",
            "",
            "\tmem = dma_init_coherent_memory(phys_addr, phys_addr, size, true);",
            "\tif (IS_ERR(mem))",
            "\t\treturn PTR_ERR(mem);",
            "\tdma_coherent_default_memory = mem;",
            "\tpr_info(\"DMA: default coherent area is set\\n\");",
            "\treturn 0;",
            "}",
            "static int rmem_dma_device_init(struct reserved_mem *rmem, struct device *dev)",
            "{",
            "\tif (!rmem->priv) {",
            "\t\tstruct dma_coherent_mem *mem;",
            "",
            "\t\tmem = dma_init_coherent_memory(rmem->base, rmem->base,",
            "\t\t\t\t\t       rmem->size, true);",
            "\t\tif (IS_ERR(mem))",
            "\t\t\treturn PTR_ERR(mem);",
            "\t\trmem->priv = mem;",
            "\t}",
            "\tdma_assign_coherent_memory(dev, rmem->priv);",
            "\treturn 0;",
            "}",
            "static void rmem_dma_device_release(struct reserved_mem *rmem,",
            "\t\t\t\t    struct device *dev)",
            "{",
            "\tif (dev)",
            "\t\tdev->dma_mem = NULL;",
            "}",
            "static int __init rmem_dma_setup(struct reserved_mem *rmem)",
            "{",
            "\tunsigned long node = rmem->fdt_node;",
            "",
            "\tif (of_get_flat_dt_prop(node, \"reusable\", NULL))",
            "\t\treturn -EINVAL;",
            "",
            "#ifdef CONFIG_ARM",
            "\tif (!of_get_flat_dt_prop(node, \"no-map\", NULL)) {",
            "\t\tpr_err(\"Reserved memory: regions without no-map are not yet supported\\n\");",
            "\t\treturn -EINVAL;",
            "\t}",
            "#endif",
            "",
            "#ifdef CONFIG_DMA_GLOBAL_POOL",
            "\tif (of_get_flat_dt_prop(node, \"linux,dma-default\", NULL)) {",
            "\t\tWARN(dma_reserved_default_memory,",
            "\t\t     \"Reserved memory: region for default DMA coherent area is redefined\\n\");",
            "\t\tdma_reserved_default_memory = rmem;",
            "\t}",
            "#endif",
            "",
            "\trmem->ops = &rmem_dma_ops;",
            "\tpr_info(\"Reserved memory: created DMA memory pool at %pa, size %ld MiB\\n\",",
            "\t\t&rmem->base, (unsigned long)rmem->size / SZ_1M);",
            "\treturn 0;",
            "}",
            "static int __init dma_init_reserved_memory(void)",
            "{",
            "\tif (!dma_reserved_default_memory)",
            "\t\treturn -ENOMEM;",
            "\treturn dma_init_global_coherent(dma_reserved_default_memory->base,",
            "\t\t\t\t\tdma_reserved_default_memory->size);",
            "}"
          ],
          "function_name": "dma_mmap_from_dev_coherent, dma_release_from_global_coherent, dma_mmap_from_global_coherent, dma_init_global_coherent, rmem_dma_device_init, rmem_dma_device_release, rmem_dma_setup, dma_init_reserved_memory",
          "description": "该代码段实现了DMA相干内存的全局与设备级管理，核心功能包括：  \n1. 提供`dma_init_global_coherent`等函数初始化全局默认的DMA相干内存区域，并通过`dma_mmap_from_global_coherent`/`dma_release_from_global_coherent`管理其虚拟映射与释放；  \n2. `rmem_dma_setup`等函数通过保留内存机制（reserved memory）动态分配并注册DMA相干池，支持设备级内存绑定；  \n3. 上下文包含未完全展示的辅助函数（如`dma_init_coherent_memory`）及`rmem_dma_ops`操作符定义，需结合完整代码进一步验证。",
          "similarity": 0.4760311245918274
        }
      ]
    },
    {
      "source_file": "kernel/pid.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:16:06\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `pid.c`\n\n---\n\n# `pid.c` 技术文档\n\n## 1. 文件概述\n\n`pid.c` 是 Linux 内核中实现进程标识符（PID）管理和分配机制的核心文件。它提供了可扩展、时间有界的 PID 分配器，支持 PID 哈希表（pidhash）以及 PID 命名空间（pid namespace）功能。该文件负责 PID 的分配、释放、引用计数管理，并确保在多处理器（SMP）环境下的线程安全性。其设计目标是在高并发场景下高效、无锁地分配和回收 PID，同时支持容器化环境中的 PID 隔离。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct pid`**  \n  表示一个 PID 实例，包含：\n  - 引用计数（`count`）\n  - 多种任务类型链表（`tasks[PIDTYPE_MAX]`），用于关联不同类型的进程（如线程组、会话等）\n  - PID 层级（`level`），用于命名空间嵌套\n  - `numbers[]` 数组：每个命名空间层级对应的 `struct upid`（包含实际 PID 编号 `nr` 和所属命名空间 `ns`）\n  - `rcu` 字段：用于 RCU 安全释放\n  - `wait_pidfd`：用于 pidfd 机制的等待队列\n  - `inodes`：关联的 pidfs inode 列表\n\n- **`struct pid_namespace`**  \n  PID 命名空间结构，包含：\n  - IDR（整数 ID 映射）结构 `idr`，用于高效 PID 分配\n  - `pid_allocated`：当前已分配 PID 数量（含特殊状态如 `PIDNS_ADDING`）\n  - `child_reaper`：命名空间中的 init 进程（子进程回收者）\n  - `level`：命名空间嵌套层级\n  - `pid_cachep`：用于分配 `struct pid` 的 slab 缓存\n\n- **全局变量**\n  - `init_struct_pid`：初始 PID 结构（PID 0，用于 idle 进程）\n  - `init_pid_ns`：初始 PID 命名空间\n  - `pid_max` / `pid_max_min` / `pid_max_max`：PID 分配上限控制\n  - `pidfs_ino`：pidfs 文件系统的 inode 编号起始值\n  - `pidmap_lock`：保护 IDR 和 `pid_allocated` 的自旋锁（SMP 对齐）\n\n### 主要函数\n\n- **`alloc_pid(struct pid_namespace *ns, pid_t *set_tid, size_t set_tid_size)`**  \n  在指定 PID 命名空间中分配一个新的 PID。支持通过 `set_tid` 数组在嵌套命名空间中预设 PID（用于容器恢复等场景）。\n\n- **`free_pid(struct pid *pid)`**  \n  释放 PID 资源，从所有嵌套命名空间的 IDR 中移除，并减少 `pid_allocated` 计数。若命名空间中仅剩 reaper 进程，则唤醒它。\n\n- **`put_pid(struct pid *pid)`**  \n  减少 PID 引用计数，若引用归零则释放内存并减少命名空间引用。\n\n- **`delayed_put_pid(struct rcu_head *rhp)`**  \n  RCU 回调函数，用于安全释放 PID 结构。\n\n## 3. 关键实现\n\n### PID 分配机制\n- 使用 **IDR（Integer ID Allocator）** 替代传统的位图（bitmap），实现 O(1) 分配与释放。\n- 默认采用**循环分配策略**（`idr_alloc_cyclic`），从 `RESERVED_PIDS`（通常为 300）开始，避免低编号 PID 被耗尽。\n- 支持**预设 PID 分配**：通过 `set_tid` 参数在创建进程时指定特定 PID（需具备 `CAP_CHECKPOINT_RESTORE` 权限），用于容器快照恢复。\n\n### 命名空间支持\n- 每个 PID 在嵌套的命名空间中拥有不同的编号（`upid->nr`），通过 `pid->numbers[]` 数组维护层级关系。\n- `pid->level` 表示该 PID 所属的最深命名空间层级。\n- 分配时从最深层命名空间向上遍历至根命名空间，逐层分配 PID。\n\n### 并发与同步\n- **`pidmap_lock`**：保护 IDR 操作和 `pid_allocated` 计数器，使用 `spin_lock_irqsave` 禁用本地中断，防止与 `tasklist_lock` 的死锁。\n- **RCU 释放**：`free_pid` 通过 `call_rcu` 延迟释放 PID 结构，避免在持有锁时执行内存释放。\n- **引用计数**：`struct pid` 使用 `refcount_t` 管理生命周期，确保多任务共享 PID 时的安全释放。\n\n### 特殊状态处理\n- **`PIDNS_ADDING`**：标记命名空间正在添加新进程，防止在 fork 失败时错误减少计数。\n- **Reaper 唤醒**：当命名空间中 PID 数量降至 1 或 2 时，唤醒 `child_reaper`（通常为 init 进程），用于处理命名空间退出（`zap_pid_ns_processes`）。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/pid_namespace.h>`：PID 命名空间定义\n  - `<linux/idr.h>`：IDR 分配器实现\n  - `<linux/rculist.h>`：RCU 安全链表操作\n  - `<linux/sched/task.h>`、`<linux/sched/signal.h>`：任务调度与信号处理\n  - `<linux/pidfs.h>`、`<uapi/linux/pidfd.h>`：pidfd 和 pidfs 支持\n  - `<linux/refcount.h>`：引用计数机制\n\n- **内核模块交互**：\n  - **进程管理子系统**：与 `fork`/`clone` 系统调用集成，分配 PID 并关联到 `task_struct`\n  - **命名空间子系统**：与 `pidns_operations` 协同实现 PID 隔离\n  - **VFS 子系统**：通过 `pidfs_ino` 为 `/proc/[pid]` 提供 inode 编号\n  - **内存管理**：使用 slab 分配器（`kmem_cache_alloc`）管理 `struct pid` 内存\n\n## 5. 使用场景\n\n- **进程创建**：在 `copy_process` 中调用 `alloc_pid` 为新进程分配唯一 PID。\n- **容器运行时**：通过 `clone(CLONE_NEWPID)` 创建 PID 命名空间，实现容器内 PID 隔离。\n- **检查点/恢复（CRIU）**：使用 `set_tid` 参数在恢复进程时精确还原原始 PID。\n- **pidfd 机制**：`pid->wait_pidfd` 支持通过文件描述符等待进程退出（`pidfd_send_signal` 等系统调用）。\n- **命名空间清理**：当容器退出时，`free_pid` 触发 reaper 唤醒，确保孤儿进程被正确回收。",
      "similarity": 0.5511466264724731,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/pid.c",
          "start_line": 109,
          "end_line": 221,
          "content": [
            "void put_pid(struct pid *pid)",
            "{",
            "\tstruct pid_namespace *ns;",
            "",
            "\tif (!pid)",
            "\t\treturn;",
            "",
            "\tns = pid->numbers[pid->level].ns;",
            "\tif (refcount_dec_and_test(&pid->count)) {",
            "\t\tkmem_cache_free(ns->pid_cachep, pid);",
            "\t\tput_pid_ns(ns);",
            "\t}",
            "}",
            "static void delayed_put_pid(struct rcu_head *rhp)",
            "{",
            "\tstruct pid *pid = container_of(rhp, struct pid, rcu);",
            "\tput_pid(pid);",
            "}",
            "void free_pid(struct pid *pid)",
            "{",
            "\t/* We can be called with write_lock_irq(&tasklist_lock) held */",
            "\tint i;",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&pidmap_lock, flags);",
            "\tfor (i = 0; i <= pid->level; i++) {",
            "\t\tstruct upid *upid = pid->numbers + i;",
            "\t\tstruct pid_namespace *ns = upid->ns;",
            "\t\tswitch (--ns->pid_allocated) {",
            "\t\tcase 2:",
            "\t\tcase 1:",
            "\t\t\t/* When all that is left in the pid namespace",
            "\t\t\t * is the reaper wake up the reaper.  The reaper",
            "\t\t\t * may be sleeping in zap_pid_ns_processes().",
            "\t\t\t */",
            "\t\t\twake_up_process(ns->child_reaper);",
            "\t\t\tbreak;",
            "\t\tcase PIDNS_ADDING:",
            "\t\t\t/* Handle a fork failure of the first process */",
            "\t\t\tWARN_ON(ns->child_reaper);",
            "\t\t\tns->pid_allocated = 0;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tidr_remove(&ns->idr, upid->nr);",
            "\t}",
            "\tpidfs_remove_pid(pid);",
            "\tspin_unlock_irqrestore(&pidmap_lock, flags);",
            "",
            "\tcall_rcu(&pid->rcu, delayed_put_pid);",
            "}",
            "void disable_pid_allocation(struct pid_namespace *ns)",
            "{",
            "\tspin_lock_irq(&pidmap_lock);",
            "\tns->pid_allocated &= ~PIDNS_ADDING;",
            "\tspin_unlock_irq(&pidmap_lock);",
            "}",
            "void attach_pid(struct task_struct *task, enum pid_type type)",
            "{",
            "\tstruct pid *pid = *task_pid_ptr(task, type);",
            "\thlist_add_head_rcu(&task->pid_links[type], &pid->tasks[type]);",
            "}",
            "static void __change_pid(struct task_struct *task, enum pid_type type,",
            "\t\t\tstruct pid *new)",
            "{",
            "\tstruct pid **pid_ptr = task_pid_ptr(task, type);",
            "\tstruct pid *pid;",
            "\tint tmp;",
            "",
            "\tpid = *pid_ptr;",
            "",
            "\thlist_del_rcu(&task->pid_links[type]);",
            "\t*pid_ptr = new;",
            "",
            "\tif (type == PIDTYPE_PID) {",
            "\t\tWARN_ON_ONCE(pid_has_task(pid, PIDTYPE_PID));",
            "\t\twake_up_all(&pid->wait_pidfd);",
            "\t}",
            "",
            "\tfor (tmp = PIDTYPE_MAX; --tmp >= 0; )",
            "\t\tif (pid_has_task(pid, tmp))",
            "\t\t\treturn;",
            "",
            "\tfree_pid(pid);",
            "}",
            "void detach_pid(struct task_struct *task, enum pid_type type)",
            "{",
            "\t__change_pid(task, type, NULL);",
            "}",
            "void change_pid(struct task_struct *task, enum pid_type type,",
            "\t\tstruct pid *pid)",
            "{",
            "\t__change_pid(task, type, pid);",
            "\tattach_pid(task, type);",
            "}",
            "void exchange_tids(struct task_struct *left, struct task_struct *right)",
            "{",
            "\tstruct pid *pid1 = left->thread_pid;",
            "\tstruct pid *pid2 = right->thread_pid;",
            "\tstruct hlist_head *head1 = &pid1->tasks[PIDTYPE_PID];",
            "\tstruct hlist_head *head2 = &pid2->tasks[PIDTYPE_PID];",
            "",
            "\t/* Swap the single entry tid lists */",
            "\thlists_swap_heads_rcu(head1, head2);",
            "",
            "\t/* Swap the per task_struct pid */",
            "\trcu_assign_pointer(left->thread_pid, pid2);",
            "\trcu_assign_pointer(right->thread_pid, pid1);",
            "",
            "\t/* Swap the cached value */",
            "\tWRITE_ONCE(left->pid, pid_nr(pid2));",
            "\tWRITE_ONCE(right->pid, pid_nr(pid1));",
            "}"
          ],
          "function_name": "put_pid, delayed_put_pid, free_pid, disable_pid_allocation, attach_pid, __change_pid, detach_pid, change_pid, exchange_tids",
          "description": "实现PID引用计数管理、释放逻辑及任务PID绑定操作，通过锁保护PID分配状态变更，利用RCU机制延迟释放内存，并处理进程ID类型切换和线程ID交换。",
          "similarity": 0.50773686170578
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/pid.c",
          "start_line": 1,
          "end_line": 108,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Generic pidhash and scalable, time-bounded PID allocator",
            " *",
            " * (C) 2002-2003 Nadia Yvette Chambers, IBM",
            " * (C) 2004 Nadia Yvette Chambers, Oracle",
            " * (C) 2002-2004 Ingo Molnar, Red Hat",
            " *",
            " * pid-structures are backing objects for tasks sharing a given ID to chain",
            " * against. There is very little to them aside from hashing them and",
            " * parking tasks using given ID's on a list.",
            " *",
            " * The hash is always changed with the tasklist_lock write-acquired,",
            " * and the hash is only accessed with the tasklist_lock at least",
            " * read-acquired, so there's no additional SMP locking needed here.",
            " *",
            " * We have a list of bitmap pages, which bitmaps represent the PID space.",
            " * Allocating and freeing PIDs is completely lockless. The worst-case",
            " * allocation scenario when all but one out of 1 million PIDs possible are",
            " * allocated already: the scanning of 32 list entries and at most PAGE_SIZE",
            " * bytes. The typical fastpath is a single successful setbit. Freeing is O(1).",
            " *",
            " * Pid namespaces:",
            " *    (C) 2007 Pavel Emelyanov <xemul@openvz.org>, OpenVZ, SWsoft Inc.",
            " *    (C) 2007 Sukadev Bhattiprolu <sukadev@us.ibm.com>, IBM",
            " *     Many thanks to Oleg Nesterov for comments and help",
            " *",
            " */",
            "",
            "#include <linux/mm.h>",
            "#include <linux/export.h>",
            "#include <linux/slab.h>",
            "#include <linux/init.h>",
            "#include <linux/rculist.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/init_task.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/refcount.h>",
            "#include <linux/anon_inodes.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/idr.h>",
            "#include <linux/pidfs.h>",
            "#include <linux/seqlock.h>",
            "#include <net/sock.h>",
            "#include <uapi/linux/pidfd.h>",
            "",
            "struct pid init_struct_pid = {",
            "\t.count\t\t= REFCOUNT_INIT(1),",
            "\t.tasks\t\t= {",
            "\t\t{ .first = NULL },",
            "\t\t{ .first = NULL },",
            "\t\t{ .first = NULL },",
            "\t},",
            "\t.level\t\t= 0,",
            "\t.numbers\t= { {",
            "\t\t.nr\t\t= 0,",
            "\t\t.ns\t\t= &init_pid_ns,",
            "\t}, }",
            "};",
            "",
            "int pid_max = PID_MAX_DEFAULT;",
            "",
            "int pid_max_min = RESERVED_PIDS + 1;",
            "int pid_max_max = PID_MAX_LIMIT;",
            "",
            "/*",
            " * PID-map pages start out as NULL, they get allocated upon",
            " * first use and are never deallocated. This way a low pid_max",
            " * value does not cause lots of bitmaps to be allocated, but",
            " * the scheme scales to up to 4 million PIDs, runtime.",
            " */",
            "struct pid_namespace init_pid_ns = {",
            "\t.ns.count = REFCOUNT_INIT(2),",
            "\t.idr = IDR_INIT(init_pid_ns.idr),",
            "\t.pid_allocated = PIDNS_ADDING,",
            "\t.level = 0,",
            "\t.child_reaper = &init_task,",
            "\t.user_ns = &init_user_ns,",
            "\t.ns.inum = PROC_PID_INIT_INO,",
            "#ifdef CONFIG_PID_NS",
            "\t.ns.ops = &pidns_operations,",
            "#endif",
            "#if defined(CONFIG_SYSCTL) && defined(CONFIG_MEMFD_CREATE)",
            "\t.memfd_noexec_scope = MEMFD_NOEXEC_SCOPE_EXEC,",
            "#endif",
            "};",
            "EXPORT_SYMBOL_GPL(init_pid_ns);",
            "",
            "/*",
            " * Note: disable interrupts while the pidmap_lock is held as an",
            " * interrupt might come in and do read_lock(&tasklist_lock).",
            " *",
            " * If we don't disable interrupts there is a nasty deadlock between",
            " * detach_pid()->free_pid() and another cpu that does",
            " * spin_lock(&pidmap_lock) followed by an interrupt routine that does",
            " * read_lock(&tasklist_lock);",
            " *",
            " * After we clean up the tasklist_lock and know there are no",
            " * irq handlers that take it we can leave the interrupts enabled.",
            " * For now it is easier to be safe than to prove it can't happen.",
            " */",
            "",
            "static  __cacheline_aligned_in_smp DEFINE_SPINLOCK(pidmap_lock);",
            "seqcount_spinlock_t pidmap_lock_seq = SEQCNT_SPINLOCK_ZERO(pidmap_lock_seq, &pidmap_lock);",
            ""
          ],
          "function_name": null,
          "description": "定义了PID命名空间和PID结构体的初始状态，包括全局PID最大值限制、PID映射锁及序列化机制，用于支持多层级PID分配和命名空间隔离。",
          "similarity": 0.4740302562713623
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/pid.c",
          "start_line": 402,
          "end_line": 488,
          "content": [
            "void transfer_pid(struct task_struct *old, struct task_struct *new,",
            "\t\t\t   enum pid_type type)",
            "{",
            "\tWARN_ON_ONCE(type == PIDTYPE_PID);",
            "\thlist_replace_rcu(&old->pid_links[type], &new->pid_links[type]);",
            "}",
            "pid_t pid_nr_ns(struct pid *pid, struct pid_namespace *ns)",
            "{",
            "\tstruct upid *upid;",
            "\tpid_t nr = 0;",
            "",
            "\tif (pid && ns->level <= pid->level) {",
            "\t\tupid = &pid->numbers[ns->level];",
            "\t\tif (upid->ns == ns)",
            "\t\t\tnr = upid->nr;",
            "\t}",
            "\treturn nr;",
            "}",
            "pid_t pid_vnr(struct pid *pid)",
            "{",
            "\treturn pid_nr_ns(pid, task_active_pid_ns(current));",
            "}",
            "pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,",
            "\t\t\tstruct pid_namespace *ns)",
            "{",
            "\tpid_t nr = 0;",
            "",
            "\trcu_read_lock();",
            "\tif (!ns)",
            "\t\tns = task_active_pid_ns(current);",
            "\tnr = pid_nr_ns(rcu_dereference(*task_pid_ptr(task, type)), ns);",
            "\trcu_read_unlock();",
            "",
            "\treturn nr;",
            "}",
            "static int pidfd_create(struct pid *pid, unsigned int flags)",
            "{",
            "\tint pidfd;",
            "\tstruct file *pidfd_file;",
            "",
            "\tpidfd = pidfd_prepare(pid, flags, &pidfd_file);",
            "\tif (pidfd < 0)",
            "\t\treturn pidfd;",
            "",
            "\tfd_install(pidfd, pidfd_file);",
            "\treturn pidfd;",
            "}",
            "void __init pid_idr_init(void)",
            "{",
            "\t/* Verify no one has done anything silly: */",
            "\tBUILD_BUG_ON(PID_MAX_LIMIT >= PIDNS_ADDING);",
            "",
            "\t/* bump default and minimum pid_max based on number of cpus */",
            "\tpid_max = min(pid_max_max, max_t(int, pid_max,",
            "\t\t\t\tPIDS_PER_CPU_DEFAULT * num_possible_cpus()));",
            "\tpid_max_min = max_t(int, pid_max_min,",
            "\t\t\t\tPIDS_PER_CPU_MIN * num_possible_cpus());",
            "\tpr_info(\"pid_max: default: %u minimum: %u\\n\", pid_max, pid_max_min);",
            "",
            "\tidr_init(&init_pid_ns.idr);",
            "",
            "\tinit_pid_ns.pid_cachep = kmem_cache_create(\"pid\",",
            "\t\t\tstruct_size_t(struct pid, numbers, 1),",
            "\t\t\t__alignof__(struct pid),",
            "\t\t\tSLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT,",
            "\t\t\tNULL);",
            "}",
            "static int pidfd_getfd(struct pid *pid, int fd)",
            "{",
            "\tstruct task_struct *task;",
            "\tstruct file *file;",
            "\tint ret;",
            "",
            "\ttask = get_pid_task(pid, PIDTYPE_PID);",
            "\tif (!task)",
            "\t\treturn -ESRCH;",
            "",
            "\tfile = __pidfd_fget(task, fd);",
            "\tput_task_struct(task);",
            "\tif (IS_ERR(file))",
            "\t\treturn PTR_ERR(file);",
            "",
            "\tret = receive_fd(file, O_CLOEXEC);",
            "\tfput(file);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "transfer_pid, pid_nr_ns, pid_vnr, __task_pid_nr_ns, pidfd_create, pid_idr_init, pidfd_getfd",
          "description": "提供跨命名空间PID查询接口及文件描述符创建功能，初始化ID分配器并配置PID命名空间层级关系，支持基于IDR的高效PID索引管理。",
          "similarity": 0.47312137484550476
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5493189692497253,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 908,
          "end_line": 1026,
          "content": [
            "static void trace_rcu_this_gp(struct rcu_node *rnp, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req, const char *s)",
            "{",
            "\ttrace_rcu_future_grace_period(rcu_state.name, READ_ONCE(rnp->gp_seq),",
            "\t\t\t\t      gp_seq_req, rnp->level,",
            "\t\t\t\t      rnp->grplo, rnp->grphi, s);",
            "}",
            "static bool rcu_start_this_gp(struct rcu_node *rnp_start, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req)",
            "{",
            "\tbool ret = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\t/*",
            "\t * Use funnel locking to either acquire the root rcu_node",
            "\t * structure's lock or bail out if the need for this grace period",
            "\t * has already been recorded -- or if that grace period has in",
            "\t * fact already started.  If there is already a grace period in",
            "\t * progress in a non-leaf node, no recording is needed because the",
            "\t * end of the grace period will scan the leaf rcu_node structures.",
            "\t * Note that rnp_start->lock must not be released.",
            "\t */",
            "\traw_lockdep_assert_held_rcu_node(rnp_start);",
            "\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req, TPS(\"Startleaf\"));",
            "\tfor (rnp = rnp_start; 1; rnp = rnp->parent) {",
            "\t\tif (rnp != rnp_start)",
            "\t\t\traw_spin_lock_rcu_node(rnp);",
            "\t\tif (ULONG_CMP_GE(rnp->gp_seq_needed, gp_seq_req) ||",
            "\t\t    rcu_seq_started(&rnp->gp_seq, gp_seq_req) ||",
            "\t\t    (rnp != rnp_start &&",
            "\t\t     rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))) {",
            "\t\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Prestarted\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tWRITE_ONCE(rnp->gp_seq_needed, gp_seq_req);",
            "\t\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq))) {",
            "\t\t\t/*",
            "\t\t\t * We just marked the leaf or internal node, and a",
            "\t\t\t * grace period is in progress, which means that",
            "\t\t\t * rcu_gp_cleanup() will see the marking.  Bail to",
            "\t\t\t * reduce contention.",
            "\t\t\t */",
            "\t\t\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Startedleaf\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tif (rnp != rnp_start && rnp->parent != NULL)",
            "\t\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\tif (!rnp->parent)",
            "\t\t\tbreak;  /* At root, and perhaps also leaf. */",
            "\t}",
            "",
            "\t/* If GP already in progress, just leave, otherwise start one. */",
            "\tif (rcu_gp_in_progress()) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedleafroot\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedroot\"));",
            "\tWRITE_ONCE(rcu_state.gp_flags, rcu_state.gp_flags | RCU_GP_FLAG_INIT);",
            "\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);",
            "\tif (!READ_ONCE(rcu_state.gp_kthread)) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"NoGPkthread\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_grace_period(rcu_state.name, data_race(rcu_state.gp_seq), TPS(\"newreq\"));",
            "\tret = true;  /* Caller must wake GP kthread. */",
            "unlock_out:",
            "\t/* Push furthest requested GP to leaf node and rcu_data structure. */",
            "\tif (ULONG_CMP_LT(gp_seq_req, rnp->gp_seq_needed)) {",
            "\t\tWRITE_ONCE(rnp_start->gp_seq_needed, rnp->gp_seq_needed);",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\t}",
            "\tif (rnp != rnp_start)",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\treturn ret;",
            "}",
            "static bool rcu_future_gp_cleanup(struct rcu_node *rnp)",
            "{",
            "\tbool needmore;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tneedmore = ULONG_CMP_LT(rnp->gp_seq, rnp->gp_seq_needed);",
            "\tif (!needmore)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq; /* Avoid counter wrap. */",
            "\ttrace_rcu_this_gp(rnp, rdp, rnp->gp_seq,",
            "\t\t\t  needmore ? TPS(\"CleanupMore\") : TPS(\"Cleanup\"));",
            "\treturn needmore;",
            "}",
            "static void swake_up_one_online_ipi(void *arg)",
            "{",
            "\tstruct swait_queue_head *wqh = arg;",
            "",
            "\tswake_up_one(wqh);",
            "}",
            "static void swake_up_one_online(struct swait_queue_head *wqh)",
            "{",
            "\tint cpu = get_cpu();",
            "",
            "\t/*",
            "\t * If called from rcutree_report_cpu_starting(), wake up",
            "\t * is dangerous that late in the CPU-down hotplug process. The",
            "\t * scheduler might queue an ignored hrtimer. Defer the wake up",
            "\t * to an online CPU instead.",
            "\t */",
            "\tif (unlikely(cpu_is_offline(cpu))) {",
            "\t\tint target;",
            "",
            "\t\ttarget = cpumask_any_and(housekeeping_cpumask(HK_TYPE_RCU),",
            "\t\t\t\t\t cpu_online_mask);",
            "",
            "\t\tsmp_call_function_single(target, swake_up_one_online_ipi,",
            "\t\t\t\t\t wqh, 0);",
            "\t\tput_cpu();",
            "\t} else {",
            "\t\tput_cpu();",
            "\t\tswake_up_one(wqh);",
            "\t}",
            "}"
          ],
          "function_name": "trace_rcu_this_gp, rcu_start_this_gp, rcu_future_gp_cleanup, swake_up_one_online_ipi, swake_up_one_online",
          "description": "实现RCU grace period事件追踪、新grace period启动逻辑及未来grace period清理机制，支持跨层级节点的同步状态传播。",
          "similarity": 0.5180003643035889
        },
        {
          "chunk_id": 20,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3589,
          "end_line": 3697,
          "content": [
            "static unsigned long",
            "kfree_rcu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)",
            "{",
            "\tint cpu;",
            "\tunsigned long count = 0;",
            "",
            "\t/* Snapshot count of all CPUs */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tcount += krc_count(krcp);",
            "\t\tcount += READ_ONCE(krcp->nr_bkv_objs);",
            "\t\tatomic_set(&krcp->backoff_page_cache_fill, 1);",
            "\t}",
            "",
            "\treturn count == 0 ? SHRINK_EMPTY : count;",
            "}",
            "static unsigned long",
            "kfree_rcu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)",
            "{",
            "\tint cpu, freed = 0;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tint count;",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tcount = krc_count(krcp);",
            "\t\tcount += drain_page_cache(krcp);",
            "\t\tkfree_rcu_monitor(&krcp->monitor_work.work);",
            "",
            "\t\tsc->nr_to_scan -= count;",
            "\t\tfreed += count;",
            "",
            "\t\tif (sc->nr_to_scan <= 0)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn freed == 0 ? SHRINK_STOP : freed;",
            "}",
            "void __init kfree_rcu_scheduler_running(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tif (need_offload_krc(krcp))",
            "\t\t\tschedule_delayed_monitor_work(krcp);",
            "\t}",
            "}",
            "static int rcu_blocking_is_gp(void)",
            "{",
            "\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE) {",
            "\t\tmight_sleep();",
            "\t\treturn false;",
            "\t}",
            "\treturn true;",
            "}",
            "void synchronize_rcu(void)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_sched_lock_map),",
            "\t\t\t \"Illegal synchronize_rcu() in RCU read-side critical section\");",
            "\tif (!rcu_blocking_is_gp()) {",
            "\t\tif (rcu_gp_is_expedited())",
            "\t\t\tsynchronize_rcu_expedited();",
            "\t\telse",
            "\t\t\twait_rcu_gp(call_rcu_hurry);",
            "\t\treturn;",
            "\t}",
            "",
            "\t// Context allows vacuous grace periods.",
            "\t// Note well that this code runs with !PREEMPT && !SMP.",
            "\t// In addition, all code that advances grace periods runs at",
            "\t// process level.  Therefore, this normal GP overlaps with other",
            "\t// normal GPs only by being fully nested within them, which allows",
            "\t// reuse of ->gp_seq_polled_snap.",
            "\trcu_poll_gp_seq_start_unlocked(&rcu_state.gp_seq_polled_snap);",
            "\trcu_poll_gp_seq_end_unlocked(&rcu_state.gp_seq_polled_snap);",
            "",
            "\t// Update the normal grace-period counters to record",
            "\t// this grace period, but only those used by the boot CPU.",
            "\t// The rcu_scheduler_starting() will take care of the rest of",
            "\t// these counters.",
            "\tlocal_irq_save(flags);",
            "\tWARN_ON_ONCE(num_online_cpus() > 1);",
            "\trcu_state.gp_seq += (1 << RCU_SEQ_CTR_SHIFT);",
            "\tfor (rnp = this_cpu_ptr(&rcu_data)->mynode; rnp; rnp = rnp->parent)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq = rcu_state.gp_seq;",
            "\tlocal_irq_restore(flags);",
            "}",
            "void get_completed_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\trgosp->rgos_norm = RCU_GET_STATE_COMPLETED;",
            "\trgosp->rgos_exp = RCU_GET_STATE_COMPLETED;",
            "}",
            "unsigned long get_state_synchronize_rcu(void)",
            "{",
            "\t/*",
            "\t * Any prior manipulation of RCU-protected data must happen",
            "\t * before the load from ->gp_seq.",
            "\t */",
            "\tsmp_mb();  /* ^^^ */",
            "\treturn rcu_seq_snap(&rcu_state.gp_seq_polled);",
            "}"
          ],
          "function_name": "kfree_rcu_shrink_count, kfree_rcu_shrink_scan, kfree_rcu_scheduler_running, rcu_blocking_is_gp, synchronize_rcu, get_completed_synchronize_rcu_full, get_state_synchronize_rcu",
          "description": "实现RCU内存回收的shrinker接口，统计并扫描等待回收的RCU对象，调度监控工作，处理同步屏障逻辑，通过锁竞争检测确保安全访问",
          "similarity": 0.5117650628089905
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2523,
          "end_line": 2628,
          "content": [
            "static void rcu_cpu_kthread_park(unsigned int cpu)",
            "{",
            "\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;",
            "}",
            "static int rcu_cpu_kthread_should_run(unsigned int cpu)",
            "{",
            "\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);",
            "}",
            "static void rcu_cpu_kthread(unsigned int cpu)",
            "{",
            "\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);",
            "\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);",
            "\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);",
            "\tint spincnt;",
            "",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));",
            "\tfor (spincnt = 0; spincnt < 10; spincnt++) {",
            "\t\tWRITE_ONCE(*j, jiffies);",
            "\t\tlocal_bh_disable();",
            "\t\t*statusp = RCU_KTHREAD_RUNNING;",
            "\t\tlocal_irq_disable();",
            "\t\twork = *workp;",
            "\t\tWRITE_ONCE(*workp, 0);",
            "\t\tlocal_irq_enable();",
            "\t\tif (work)",
            "\t\t\trcu_core();",
            "\t\tlocal_bh_enable();",
            "\t\tif (!READ_ONCE(*workp)) {",
            "\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));",
            "\t\t\t*statusp = RCU_KTHREAD_WAITING;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "\t*statusp = RCU_KTHREAD_YIELDING;",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));",
            "\tschedule_timeout_idle(2);",
            "\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));",
            "\t*statusp = RCU_KTHREAD_WAITING;",
            "\tWRITE_ONCE(*j, jiffies);",
            "}",
            "static int __init rcu_spawn_core_kthreads(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;",
            "\tif (use_softirq)",
            "\t\treturn 0;",
            "\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),",
            "\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);",
            "\treturn 0;",
            "}",
            "static void rcutree_enqueue(struct rcu_data *rdp, struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\trcu_segcblist_enqueue(&rdp->cblist, head);",
            "\tif (__is_kvfree_rcu_offset((unsigned long)func))",
            "\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,",
            "\t\t\t\t\t (unsigned long)func,",
            "\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));",
            "\telse",
            "\t\ttrace_rcu_callback(rcu_state.name, head,",
            "\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));",
            "}",
            "static void call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,",
            "\t\t\t  rcu_callback_t func, unsigned long flags)",
            "{",
            "\trcutree_enqueue(rdp, head, func);",
            "\t/*",
            "\t * If called from an extended quiescent state, invoke the RCU",
            "\t * core in order to force a re-evaluation of RCU's idleness.",
            "\t */",
            "\tif (!rcu_is_watching())",
            "\t\tinvoke_rcu_core();",
            "",
            "\t/* If interrupts were disabled or CPU offline, don't invoke RCU core. */",
            "\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Force the grace period if too many callbacks or too long waiting.",
            "\t * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()",
            "\t * if some other CPU has recently done so.  Also, don't bother",
            "\t * invoking rcu_force_quiescent_state() if the newly enqueued callback",
            "\t * is the only one waiting for a grace period to complete.",
            "\t */",
            "\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >",
            "\t\t     rdp->qlen_last_fqs_check + qhimark)) {",
            "",
            "\t\t/* Are we ignoring a completed grace period? */",
            "\t\tnote_gp_changes(rdp);",
            "",
            "\t\t/* Start a new grace period if one not already started. */",
            "\t\tif (!rcu_gp_in_progress()) {",
            "\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);",
            "\t\t} else {",
            "\t\t\t/* Give the grace period a kick. */",
            "\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;",
            "\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&",
            "\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)",
            "\t\t\t\trcu_force_quiescent_state();",
            "\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "rcu_cpu_kthread_park, rcu_cpu_kthread_should_run, rcu_cpu_kthread, rcu_spawn_core_kthreads, rcutree_enqueue, call_rcu_core",
          "description": "实现RCU k线程管理与回调分发基础设施，包含线程启动、回调入队及触发条件判断逻辑，提供跨CPU的异步处理能力",
          "similarity": 0.4952259361743927
        },
        {
          "chunk_id": 21,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3787,
          "end_line": 3910,
          "content": [
            "void get_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\t/*",
            "\t * Any prior manipulation of RCU-protected data must happen",
            "\t * before the loads from ->gp_seq and ->expedited_sequence.",
            "\t */",
            "\tsmp_mb();  /* ^^^ */",
            "\trgosp->rgos_norm = rcu_seq_snap(&rnp->gp_seq);",
            "\trgosp->rgos_exp = rcu_seq_snap(&rcu_state.expedited_sequence);",
            "}",
            "static void start_poll_synchronize_rcu_common(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool needwake;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tlockdep_assert_irqs_enabled();",
            "\tlocal_irq_save(flags);",
            "\trdp = this_cpu_ptr(&rcu_data);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_rcu_node(rnp); // irqs already disabled.",
            "\t// Note it is possible for a grace period to have elapsed between",
            "\t// the above call to get_state_synchronize_rcu() and the below call",
            "\t// to rcu_seq_snap.  This is OK, the worst that happens is that we",
            "\t// get a grace period that no one needed.  These accesses are ordered",
            "\t// by smp_mb(), and we are accessing them in the opposite order",
            "\t// from which they are updated at grace-period start, as required.",
            "\tneedwake = rcu_start_this_gp(rnp, rdp, rcu_seq_snap(&rcu_state.gp_seq));",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "}",
            "unsigned long start_poll_synchronize_rcu(void)",
            "{",
            "\tunsigned long gp_seq = get_state_synchronize_rcu();",
            "",
            "\tstart_poll_synchronize_rcu_common();",
            "\treturn gp_seq;",
            "}",
            "void start_poll_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tget_state_synchronize_rcu_full(rgosp);",
            "",
            "\tstart_poll_synchronize_rcu_common();",
            "}",
            "bool poll_state_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\tif (oldstate == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rcu_state.gp_seq_polled, oldstate)) {",
            "\t\tsmp_mb(); /* Ensure GP ends before subsequent accesses. */",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "bool poll_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tsmp_mb(); // Order against root rcu_node structure grace-period cleanup.",
            "\tif (rgosp->rgos_norm == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rnp->gp_seq, rgosp->rgos_norm) ||",
            "\t    rgosp->rgos_exp == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rcu_state.expedited_sequence, rgosp->rgos_exp)) {",
            "\t\tsmp_mb(); /* Ensure GP ends before subsequent accesses. */",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "void cond_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\tif (!poll_state_synchronize_rcu(oldstate))",
            "\t\tsynchronize_rcu();",
            "}",
            "void cond_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tif (!poll_state_synchronize_rcu_full(rgosp))",
            "\t\tsynchronize_rcu();",
            "}",
            "static int rcu_pending(int user)",
            "{",
            "\tbool gp_in_progress;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "\tstruct rcu_node *rnp = rdp->mynode;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\t/* Check for CPU stalls, if enabled. */",
            "\tcheck_cpu_stall(rdp);",
            "",
            "\t/* Does this CPU need a deferred NOCB wakeup? */",
            "\tif (rcu_nocb_need_deferred_wakeup(rdp, RCU_NOCB_WAKE))",
            "\t\treturn 1;",
            "",
            "\t/* Is this a nohz_full CPU in userspace or idle?  (Ignore RCU if so.) */",
            "\tif ((user || rcu_is_cpu_rrupt_from_idle()) && rcu_nohz_full_cpu())",
            "\t\treturn 0;",
            "",
            "\t/* Is the RCU core waiting for a quiescent state from this CPU? */",
            "\tgp_in_progress = rcu_gp_in_progress();",
            "\tif (rdp->core_needs_qs && !rdp->cpu_no_qs.b.norm && gp_in_progress)",
            "\t\treturn 1;",
            "",
            "\t/* Does this CPU have callbacks ready to invoke? */",
            "\tif (!rcu_rdp_is_offloaded(rdp) &&",
            "\t    rcu_segcblist_ready_cbs(&rdp->cblist))",
            "\t\treturn 1;",
            "",
            "\t/* Has RCU gone idle with this CPU needing another grace period? */",
            "\tif (!gp_in_progress && rcu_segcblist_is_enabled(&rdp->cblist) &&",
            "\t    !rcu_rdp_is_offloaded(rdp) &&",
            "\t    !rcu_segcblist_restempty(&rdp->cblist, RCU_NEXT_READY_TAIL))",
            "\t\treturn 1;",
            "",
            "\t/* Have RCU grace period completed or started?  */",
            "\tif (rcu_seq_current(&rnp->gp_seq) != rdp->gp_seq ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) /* outside lock */",
            "\t\treturn 1;",
            "",
            "\t/* nothing to do */",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_state_synchronize_rcu_full, start_poll_synchronize_rcu_common, start_poll_synchronize_rcu, start_poll_synchronize_rcu_full, poll_state_synchronize_rcu, poll_state_synchronize_rcu_full, cond_synchronize_rcu, cond_synchronize_rcu_full, rcu_pending",
          "description": "提供RCU宽限期状态查询和触发机制，通过序列号比对判断是否需要启动新的宽限期，处理回调队列唤醒逻辑，实现条件同步检查",
          "similarity": 0.49431729316711426
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1840,
          "end_line": 1973,
          "content": [
            "static int __noreturn rcu_gp_kthread(void *unused)",
            "{",
            "\trcu_bind_gp_kthread();",
            "\tfor (;;) {",
            "",
            "\t\t/* Handle grace-period start. */",
            "\t\tfor (;;) {",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwait\"));",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_WAIT_GPS);",
            "\t\t\tswait_event_idle_exclusive(rcu_state.gp_wq,",
            "\t\t\t\t\t READ_ONCE(rcu_state.gp_flags) &",
            "\t\t\t\t\t RCU_GP_FLAG_INIT);",
            "\t\t\trcu_gp_torture_wait();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_DONE_GPS);",
            "\t\t\t/* Locking provides needed memory barrier. */",
            "\t\t\tif (rcu_gp_init())",
            "\t\t\t\tbreak;",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\t\t\tWARN_ON(signal_pending(current));",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwaitsig\"));",
            "\t\t}",
            "",
            "\t\t/* Handle quiescent-state forcing. */",
            "\t\trcu_gp_fqs_loop();",
            "",
            "\t\t/* Handle grace-period end. */",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANUP);",
            "\t\trcu_gp_cleanup();",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANED);",
            "\t}",
            "}",
            "static void rcu_report_qs_rsp(unsigned long flags)",
            "\t__releases(rcu_get_root()->lock)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rcu_get_root());",
            "\tWARN_ON_ONCE(!rcu_gp_in_progress());",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rcu_get_root(), flags);",
            "\trcu_gp_kthread_wake();",
            "}",
            "static void rcu_report_qs_rnp(unsigned long mask, struct rcu_node *rnp,",
            "\t\t\t      unsigned long gps, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long oldmask = 0;",
            "\tstruct rcu_node *rnp_c;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* Walk up the rcu_node hierarchy. */",
            "\tfor (;;) {",
            "\t\tif ((!(rnp->qsmask & mask) && mask) || rnp->gp_seq != gps) {",
            "",
            "\t\t\t/*",
            "\t\t\t * Our bit has already been cleared, or the",
            "\t\t\t * relevant grace period is already over, so done.",
            "\t\t\t */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */",
            "\t\tWARN_ON_ONCE(!rcu_is_leaf_node(rnp) &&",
            "\t\t\t     rcu_preempt_blocked_readers_cgp(rnp));",
            "\t\tWRITE_ONCE(rnp->qsmask, rnp->qsmask & ~mask);",
            "\t\ttrace_rcu_quiescent_state_report(rcu_state.name, rnp->gp_seq,",
            "\t\t\t\t\t\t mask, rnp->qsmask, rnp->level,",
            "\t\t\t\t\t\t rnp->grplo, rnp->grphi,",
            "\t\t\t\t\t\t !!rnp->gp_tasks);",
            "\t\tif (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {",
            "",
            "\t\t\t/* Other bits still set at this level, so done. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\trnp->completedqs = rnp->gp_seq;",
            "\t\tmask = rnp->grpmask;",
            "\t\tif (rnp->parent == NULL) {",
            "",
            "\t\t\t/* No more levels.  Exit loop holding root lock. */",
            "",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\trnp_c = rnp;",
            "\t\trnp = rnp->parent;",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\toldmask = READ_ONCE(rnp_c->qsmask);",
            "\t}",
            "",
            "\t/*",
            "\t * Get here if we are the last CPU to pass through a quiescent",
            "\t * state for this grace period.  Invoke rcu_report_qs_rsp()",
            "\t * to clean up and start the next grace period if one is needed.",
            "\t */",
            "\trcu_report_qs_rsp(flags); /* releases rnp->lock. */",
            "}",
            "static void __maybe_unused",
            "rcu_report_unblock_qs_rnp(struct rcu_node *rnp, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long gps;",
            "\tunsigned long mask;",
            "\tstruct rcu_node *rnp_p;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (WARN_ON_ONCE(!IS_ENABLED(CONFIG_PREEMPT_RCU)) ||",
            "\t    WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)) ||",
            "\t    rnp->qsmask != 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;  /* Still need more quiescent states! */",
            "\t}",
            "",
            "\trnp->completedqs = rnp->gp_seq;",
            "\trnp_p = rnp->parent;",
            "\tif (rnp_p == NULL) {",
            "\t\t/*",
            "\t\t * Only one rcu_node structure in the tree, so don't",
            "\t\t * try to report up to its nonexistent parent!",
            "\t\t */",
            "\t\trcu_report_qs_rsp(flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Report up the rest of the hierarchy, tracking current ->gp_seq. */",
            "\tgps = rnp->gp_seq;",
            "\tmask = rnp->grpmask;",
            "\traw_spin_unlock_rcu_node(rnp);\t/* irqs remain disabled. */",
            "\traw_spin_lock_rcu_node(rnp_p);\t/* irqs already disabled. */",
            "\trcu_report_qs_rnp(mask, rnp_p, gps, flags);",
            "}"
          ],
          "function_name": "rcu_gp_kthread, rcu_report_qs_rsp, rcu_report_qs_rnp, rcu_report_unblock_qs_rnp",
          "description": "实现RCU grace period的主线程循环，处理grace period启动、强制quiescent状态报告及结束逻辑，通过锁和状态标志协调各子系统同步",
          "similarity": 0.49119025468826294
        }
      ]
    }
  ]
}