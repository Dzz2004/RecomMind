{
  "query": "Buffer Cache在文件读写中的作用",
  "timestamp": "2025-12-25 23:27:31",
  "retrieved_files": [
    {
      "source_file": "mm/readahead.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:14:54\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `readahead.c`\n\n---\n\n# readahead.c 技术文档\n\n## 1. 文件概述\n\n`readahead.c` 是 Linux 内核内存管理子系统（MM）中的核心文件，负责实现 **地址空间级别（address_space-level）的文件预读（readahead）机制**。该机制通过在应用程序显式请求之前将数据提前读入页缓存（page cache），从而提升顺序读取性能。文件实现了预读状态管理、预读触发逻辑、预读大小计算以及与文件系统交互的接口。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct file_ra_state`：每个打开文件关联的预读状态结构，记录历史访问信息、当前预读窗口大小等。\n- `struct readahead_control`（`rac`）：预读操作控制块，封装了本次预读请求的上下文（如映射地址空间、文件指针、预读页范围等）。\n\n### 主要函数\n- `file_ra_state_init()`：初始化文件的预读状态结构。\n- `read_pages()`：根据预读控制块，调用文件系统的 `->readahead()` 或 `->read_folio()` 方法发起实际 I/O。\n- `page_cache_ra_unbounded()`：启动无边界检查的预读（用于特殊场景，如超出 `i_size` 的读取）。\n- （注：代码片段未完整包含 `page_cache_async_readahead()` 和 `page_cache_sync_readahead()`，但文档说明中提及它们是主要入口）\n\n## 3. 关键实现\n\n### 预读触发机制\n- 当应用访问的页 **不在页缓存中**，或 **在页缓存中但设置了 `PG_readahead` 标志** 时，触发预读。\n- `PG_readahead` 标志标记了上一次预读窗口中“异步尾部”的第一页，其被访问表明应启动下一轮预读。\n\n### 预读窗口构成\n- 每次预读请求包含 **同步部分**（必须满足当前请求）和 **异步部分**（纯预读）。\n- `struct file_ra_state` 中：\n  - `size`：总预读页数。\n  - `async_size`：异步部分页数。\n- 异步部分的第一页会被设置 `PG_readahead` 标志，用于触发后续预读。\n\n### 预读大小计算策略\n- **基于历史**：若能确定上一次预读大小（通过 `file_ra_state` 或页缓存状态），则按比例（通常翻倍）扩展。\n- **基于上下文**：若无法确定历史，则估算页缓存中连续已存在页数作为参考（需大于当前请求且仅在文件开头可放大）。\n- **文件起始加速**：对文件开头的读取采用更激进的预读策略，因常为顺序访问。\n\n### 与文件系统交互\n- 通过地址空间操作 `->readahead()` 发起批量预读（推荐方式），典型实现为 `mpage_readahead()`。\n- 若文件系统未实现 `->readahead()`，则回退到逐页调用 `->read_folio()`。\n- `readahead_folio()` 用于从预读控制块中逐个获取待读页。\n- **错误处理**：\n  - 同步部分页必须成功读取（或等待资源），不可因拥塞失败。\n  - 异步部分页可因资源不足跳过，此时应调用 `filemap_remove_folio()` 从页缓存移除，以便后续重试；若留在缓存中，将导致低效的单页 `->read_folio()` 回退。\n\n### 资源管理\n- 使用 `blk_plug` 机制合并 I/O 请求以提升效率。\n- 通过 PSI（Pressure Stall Information）跟踪内存压力。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/pagemap.h>`、`<linux/mm_inline.h>` 管理页缓存和页状态。\n- **块设备层**：通过 `<linux/blkdev.h>`、`<linux/blk-cgroup.h>` 与块 I/O 子系统交互。\n- **文件系统接口**：依赖 `struct address_space_operations` 中的 `readahead`/`read_folio` 方法。\n- **其他子系统**：\n  - PSI（`<linux/psi.h>`）用于内存压力监控。\n  - cgroup blkio（`<linux/blk-cgroup.h>`）支持 I/O 控制。\n  - DAX（`<linux/dax.h>`）支持直接访问持久内存。\n  - 任务 I/O 记账（`<linux/task_io_accounting_ops.h>`）。\n\n## 5. 使用场景\n\n- **顺序文件读取**：当应用顺序读取大文件时，内核自动扩展预读窗口，减少 I/O 次数。\n- **随机读取后的顺序检测**：若随机读取后出现连续访问，预读机制可快速切换到顺序模式。\n- **文件系统实现**：文件系统通过实现 `->readahead()` 方法高效处理批量预读请求（如 ext4、XFS）。\n- **特殊 I/O 模式**：通过 `posix_fadvise(POSIX_FADV_SEQUENTIAL)` 等系统调用提示内核启用更强预读。\n- **大页（Huge Page）支持**：预读逻辑适配 folio（大页抽象），提升大页场景性能。",
      "similarity": 0.601207971572876,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "mm/readahead.c",
          "start_line": 710,
          "end_line": 856,
          "content": [
            "void page_cache_sync_ra(struct readahead_control *ractl,",
            "\t\tunsigned long req_count)",
            "{",
            "\tbool do_forced_ra = ractl->file && (ractl->file->f_mode & FMODE_RANDOM);",
            "",
            "\t/*",
            "\t * Even if readahead is disabled, issue this request as readahead",
            "\t * as we'll need it to satisfy the requested range. The forced",
            "\t * readahead will do the right thing and limit the read to just the",
            "\t * requested range, which we'll set to 1 page for this case.",
            "\t */",
            "\tif (!ractl->ra->ra_pages || blk_cgroup_congested()) {",
            "\t\tif (!ractl->file)",
            "\t\t\treturn;",
            "\t\treq_count = 1;",
            "\t\tdo_forced_ra = true;",
            "\t}",
            "",
            "\t/* be dumb */",
            "\tif (do_forced_ra) {",
            "\t\tforce_page_cache_ra(ractl, req_count);",
            "\t\treturn;",
            "\t}",
            "",
            "\tondemand_readahead(ractl, NULL, req_count);",
            "}",
            "void page_cache_async_ra(struct readahead_control *ractl,",
            "\t\tstruct folio *folio, unsigned long req_count)",
            "{",
            "\t/* no readahead */",
            "\tif (!ractl->ra->ra_pages)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Same bit is used for PG_readahead and PG_reclaim.",
            "\t */",
            "\tif (folio_test_writeback(folio))",
            "\t\treturn;",
            "",
            "\tfolio_clear_readahead(folio);",
            "",
            "\tif (blk_cgroup_congested())",
            "\t\treturn;",
            "",
            "\tondemand_readahead(ractl, folio, req_count);",
            "}",
            "ssize_t ksys_readahead(int fd, loff_t offset, size_t count)",
            "{",
            "\tssize_t ret;",
            "\tstruct fd f;",
            "",
            "\tret = -EBADF;",
            "\tf = fdget(fd);",
            "\tif (!f.file || !(f.file->f_mode & FMODE_READ))",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * The readahead() syscall is intended to run only on files",
            "\t * that can execute readahead. If readahead is not possible",
            "\t * on this file, then we must return -EINVAL.",
            "\t */",
            "\tret = -EINVAL;",
            "\tif (!f.file->f_mapping || !f.file->f_mapping->a_ops ||",
            "\t    (!S_ISREG(file_inode(f.file)->i_mode) &&",
            "\t    !S_ISBLK(file_inode(f.file)->i_mode)))",
            "\t\tgoto out;",
            "",
            "\tret = vfs_fadvise(f.file, offset, count, POSIX_FADV_WILLNEED);",
            "out:",
            "\tfdput(f);",
            "\treturn ret;",
            "}",
            "void readahead_expand(struct readahead_control *ractl,",
            "\t\t      loff_t new_start, size_t new_len)",
            "{",
            "\tstruct address_space *mapping = ractl->mapping;",
            "\tstruct file_ra_state *ra = ractl->ra;",
            "\tpgoff_t new_index, new_nr_pages;",
            "\tgfp_t gfp_mask = readahead_gfp_mask(mapping);",
            "\tunsigned long min_nrpages = mapping_min_folio_nrpages(mapping);",
            "\tunsigned int min_order = mapping_min_folio_order(mapping);",
            "",
            "\tnew_index = new_start / PAGE_SIZE;",
            "\t/*",
            "\t * Readahead code should have aligned the ractl->_index to",
            "\t * min_nrpages before calling readahead aops.",
            "\t */",
            "\tVM_BUG_ON(!IS_ALIGNED(ractl->_index, min_nrpages));",
            "",
            "\t/* Expand the leading edge downwards */",
            "\twhile (ractl->_index > new_index) {",
            "\t\tunsigned long index = ractl->_index - 1;",
            "\t\tstruct folio *folio = xa_load(&mapping->i_pages, index);",
            "",
            "\t\tif (folio && !xa_is_value(folio))",
            "\t\t\treturn; /* Folio apparently present */",
            "",
            "\t\tfolio = filemap_alloc_folio(gfp_mask, min_order);",
            "\t\tif (!folio)",
            "\t\t\treturn;",
            "",
            "\t\tindex = mapping_align_index(mapping, index);",
            "\t\tif (filemap_add_folio(mapping, folio, index, gfp_mask) < 0) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tif (unlikely(folio_test_workingset(folio)) &&",
            "\t\t\t\t!ractl->_workingset) {",
            "\t\t\tractl->_workingset = true;",
            "\t\t\tpsi_memstall_enter(&ractl->_pflags);",
            "\t\t}",
            "\t\tractl->_nr_pages += min_nrpages;",
            "\t\tractl->_index = folio->index;",
            "\t}",
            "",
            "\tnew_len += new_start - readahead_pos(ractl);",
            "\tnew_nr_pages = DIV_ROUND_UP(new_len, PAGE_SIZE);",
            "",
            "\t/* Expand the trailing edge upwards */",
            "\twhile (ractl->_nr_pages < new_nr_pages) {",
            "\t\tunsigned long index = ractl->_index + ractl->_nr_pages;",
            "\t\tstruct folio *folio = xa_load(&mapping->i_pages, index);",
            "",
            "\t\tif (folio && !xa_is_value(folio))",
            "\t\t\treturn; /* Folio apparently present */",
            "",
            "\t\tfolio = filemap_alloc_folio(gfp_mask, min_order);",
            "\t\tif (!folio)",
            "\t\t\treturn;",
            "",
            "\t\tindex = mapping_align_index(mapping, index);",
            "\t\tif (filemap_add_folio(mapping, folio, index, gfp_mask) < 0) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tif (unlikely(folio_test_workingset(folio)) &&",
            "\t\t\t\t!ractl->_workingset) {",
            "\t\t\tractl->_workingset = true;",
            "\t\t\tpsi_memstall_enter(&ractl->_pflags);",
            "\t\t}",
            "\t\tractl->_nr_pages += min_nrpages;",
            "\t\tif (ra) {",
            "\t\t\tra->size += min_nrpages;",
            "\t\t\tra->async_size += min_nrpages;",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "page_cache_sync_ra, page_cache_async_ra, ksys_readahead, readahead_expand",
          "description": "该代码块实现了Linux内核的文件读取预读机制，包含同步/异步预读逻辑及读取范围扩展功能。  \n`page_cache_sync_ra` 和 `page_cache_async_ra` 分别处理同步/异步预读请求，通过判断文件属性和I/O压力决定是否触发强制读取或按需读取。  \n`readahead_expand` 动态扩展文件读取范围，分配并插入新页帧以满足超出发起位置的读取需求。  \n注：`ondemand_readahead` 等辅助函数未完全展示，上下文存在缺失。",
          "similarity": 0.5756922960281372
        },
        {
          "chunk_id": 1,
          "file_path": "mm/readahead.c",
          "start_line": 138,
          "end_line": 271,
          "content": [
            "void",
            "file_ra_state_init(struct file_ra_state *ra, struct address_space *mapping)",
            "{",
            "\tra->ra_pages = inode_to_bdi(mapping->host)->ra_pages;",
            "\tra->prev_pos = -1;",
            "}",
            "static void read_pages(struct readahead_control *rac)",
            "{",
            "\tconst struct address_space_operations *aops = rac->mapping->a_ops;",
            "\tstruct folio *folio;",
            "\tstruct blk_plug plug;",
            "",
            "\tif (!readahead_count(rac))",
            "\t\treturn;",
            "",
            "\tif (unlikely(rac->_workingset))",
            "\t\tpsi_memstall_enter(&rac->_pflags);",
            "\tblk_start_plug(&plug);",
            "",
            "\tif (aops->readahead) {",
            "\t\taops->readahead(rac);",
            "\t\t/* Clean up the remaining folios. */",
            "\t\twhile ((folio = readahead_folio(rac)) != NULL) {",
            "\t\t\tfolio_get(folio);",
            "\t\t\tfilemap_remove_folio(folio);",
            "\t\t\tfolio_unlock(folio);",
            "\t\t\tfolio_put(folio);",
            "\t\t}",
            "\t} else {",
            "\t\twhile ((folio = readahead_folio(rac)) != NULL)",
            "\t\t\taops->read_folio(rac->file, folio);",
            "\t}",
            "",
            "\tblk_finish_plug(&plug);",
            "\tif (unlikely(rac->_workingset))",
            "\t\tpsi_memstall_leave(&rac->_pflags);",
            "\trac->_workingset = false;",
            "",
            "\tBUG_ON(readahead_count(rac));",
            "}",
            "void page_cache_ra_unbounded(struct readahead_control *ractl,",
            "\t\tunsigned long nr_to_read, unsigned long lookahead_size)",
            "{",
            "\tstruct address_space *mapping = ractl->mapping;",
            "\tunsigned long index = readahead_index(ractl);",
            "\tgfp_t gfp_mask = readahead_gfp_mask(mapping);",
            "\tunsigned long mark = ULONG_MAX, i = 0;",
            "\tunsigned int min_nrpages = mapping_min_folio_nrpages(mapping);",
            "",
            "\t/*",
            "\t * Partway through the readahead operation, we will have added",
            "\t * locked pages to the page cache, but will not yet have submitted",
            "\t * them for I/O.  Adding another page may need to allocate memory,",
            "\t * which can trigger memory reclaim.  Telling the VM we're in",
            "\t * the middle of a filesystem operation will cause it to not",
            "\t * touch file-backed pages, preventing a deadlock.  Most (all?)",
            "\t * filesystems already specify __GFP_NOFS in their mapping's",
            "\t * gfp_mask, but let's be explicit here.",
            "\t */",
            "\tunsigned int nofs = memalloc_nofs_save();",
            "",
            "\tfilemap_invalidate_lock_shared(mapping);",
            "\tindex = mapping_align_index(mapping, index);",
            "",
            "\t/*",
            "\t * As iterator `i` is aligned to min_nrpages, round_up the",
            "\t * difference between nr_to_read and lookahead_size to mark the",
            "\t * index that only has lookahead or \"async_region\" to set the",
            "\t * readahead flag.",
            "\t */",
            "\tif (lookahead_size <= nr_to_read) {",
            "\t\tunsigned long ra_folio_index;",
            "",
            "\t\tra_folio_index = round_up(readahead_index(ractl) +",
            "\t\t\t\t\t  nr_to_read - lookahead_size,",
            "\t\t\t\t\t  min_nrpages);",
            "\t\tmark = ra_folio_index - index;",
            "\t}",
            "\tnr_to_read += readahead_index(ractl) - index;",
            "\tractl->_index = index;",
            "",
            "\t/*",
            "\t * Preallocate as many pages as we will need.",
            "\t */",
            "\twhile (i < nr_to_read) {",
            "\t\tstruct folio *folio = xa_load(&mapping->i_pages, index + i);",
            "\t\tint ret;",
            "",
            "\t\tif (folio && !xa_is_value(folio)) {",
            "\t\t\t/*",
            "\t\t\t * Page already present?  Kick off the current batch",
            "\t\t\t * of contiguous pages before continuing with the",
            "\t\t\t * next batch.  This page may be the one we would",
            "\t\t\t * have intended to mark as Readahead, but we don't",
            "\t\t\t * have a stable reference to this page, and it's",
            "\t\t\t * not worth getting one just for that.",
            "\t\t\t */",
            "\t\t\tread_pages(ractl);",
            "\t\t\tractl->_index += min_nrpages;",
            "\t\t\ti = ractl->_index + ractl->_nr_pages - index;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tfolio = filemap_alloc_folio(gfp_mask,",
            "\t\t\t\t\t    mapping_min_folio_order(mapping));",
            "\t\tif (!folio)",
            "\t\t\tbreak;",
            "",
            "\t\tret = filemap_add_folio(mapping, folio, index + i, gfp_mask);",
            "\t\tif (ret < 0) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\tif (ret == -ENOMEM)",
            "\t\t\t\tbreak;",
            "\t\t\tread_pages(ractl);",
            "\t\t\tractl->_index += min_nrpages;",
            "\t\t\ti = ractl->_index + ractl->_nr_pages - index;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (i == mark)",
            "\t\t\tfolio_set_readahead(folio);",
            "\t\tractl->_workingset |= folio_test_workingset(folio);",
            "\t\tractl->_nr_pages += min_nrpages;",
            "\t\ti += min_nrpages;",
            "\t}",
            "",
            "\t/*",
            "\t * Now start the IO.  We ignore I/O errors - if the folio is not",
            "\t * uptodate then the caller will launch read_folio again, and",
            "\t * will then handle the error.",
            "\t */",
            "\tread_pages(ractl);",
            "\tfilemap_invalidate_unlock_shared(mapping);",
            "\tmemalloc_nofs_restore(nofs);",
            "}"
          ],
          "function_name": "file_ra_state_init, read_pages, page_cache_ra_unbounded",
          "description": "初始化文件读取状态，执行读取操作并清理未读取的folios，处理无界预读逻辑",
          "similarity": 0.5335376262664795
        },
        {
          "chunk_id": 2,
          "file_path": "mm/readahead.c",
          "start_line": 297,
          "end_line": 411,
          "content": [
            "static void do_page_cache_ra(struct readahead_control *ractl,",
            "\t\tunsigned long nr_to_read, unsigned long lookahead_size)",
            "{",
            "\tstruct inode *inode = ractl->mapping->host;",
            "\tunsigned long index = readahead_index(ractl);",
            "\tloff_t isize = i_size_read(inode);",
            "\tpgoff_t end_index;\t/* The last page we want to read */",
            "",
            "\tif (isize == 0)",
            "\t\treturn;",
            "",
            "\tend_index = (isize - 1) >> PAGE_SHIFT;",
            "\tif (index > end_index)",
            "\t\treturn;",
            "\t/* Don't read past the page containing the last byte of the file */",
            "\tif (nr_to_read > end_index - index)",
            "\t\tnr_to_read = end_index - index + 1;",
            "",
            "\tpage_cache_ra_unbounded(ractl, nr_to_read, lookahead_size);",
            "}",
            "void force_page_cache_ra(struct readahead_control *ractl,",
            "\t\tunsigned long nr_to_read)",
            "{",
            "\tstruct address_space *mapping = ractl->mapping;",
            "\tstruct file_ra_state *ra = ractl->ra;",
            "\tstruct backing_dev_info *bdi = inode_to_bdi(mapping->host);",
            "\tunsigned long max_pages, index;",
            "",
            "\tif (unlikely(!mapping->a_ops->read_folio && !mapping->a_ops->readahead))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If the request exceeds the readahead window, allow the read to",
            "\t * be up to the optimal hardware IO size",
            "\t */",
            "\tindex = readahead_index(ractl);",
            "\tmax_pages = max_t(unsigned long, bdi->io_pages, ra->ra_pages);",
            "\tnr_to_read = min_t(unsigned long, nr_to_read, max_pages);",
            "\twhile (nr_to_read) {",
            "\t\tunsigned long this_chunk = (2 * 1024 * 1024) / PAGE_SIZE;",
            "",
            "\t\tif (this_chunk > nr_to_read)",
            "\t\t\tthis_chunk = nr_to_read;",
            "\t\tractl->_index = index;",
            "\t\tdo_page_cache_ra(ractl, this_chunk, 0);",
            "",
            "\t\tindex += this_chunk;",
            "\t\tnr_to_read -= this_chunk;",
            "\t}",
            "}",
            "static unsigned long get_init_ra_size(unsigned long size, unsigned long max)",
            "{",
            "\tunsigned long newsize = roundup_pow_of_two(size);",
            "",
            "\tif (newsize <= max / 32)",
            "\t\tnewsize = newsize * 4;",
            "\telse if (newsize <= max / 4)",
            "\t\tnewsize = newsize * 2;",
            "\telse",
            "\t\tnewsize = max;",
            "",
            "\treturn newsize;",
            "}",
            "static unsigned long get_next_ra_size(struct file_ra_state *ra,",
            "\t\t\t\t      unsigned long max)",
            "{",
            "\tunsigned long cur = ra->size;",
            "",
            "\tif (cur < max / 16)",
            "\t\treturn 4 * cur;",
            "\tif (cur <= max / 2)",
            "\t\treturn 2 * cur;",
            "\treturn max;",
            "}",
            "static pgoff_t count_history_pages(struct address_space *mapping,",
            "\t\t\t\t   pgoff_t index, unsigned long max)",
            "{",
            "\tpgoff_t head;",
            "",
            "\trcu_read_lock();",
            "\thead = page_cache_prev_miss(mapping, index - 1, max);",
            "\trcu_read_unlock();",
            "",
            "\treturn index - 1 - head;",
            "}",
            "static int try_context_readahead(struct address_space *mapping,",
            "\t\t\t\t struct file_ra_state *ra,",
            "\t\t\t\t pgoff_t index,",
            "\t\t\t\t unsigned long req_size,",
            "\t\t\t\t unsigned long max)",
            "{",
            "\tpgoff_t size;",
            "",
            "\tsize = count_history_pages(mapping, index, max);",
            "",
            "\t/*",
            "\t * not enough history pages:",
            "\t * it could be a random read",
            "\t */",
            "\tif (size <= req_size)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * starts from beginning of file:",
            "\t * it is a strong indication of long-run stream (or whole-file-read)",
            "\t */",
            "\tif (size >= index)",
            "\t\tsize *= 2;",
            "",
            "\tra->start = index;",
            "\tra->size = min(size + req_size, max);",
            "\tra->async_size = 1;",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "do_page_cache_ra, force_page_cache_ra, get_init_ra_size, get_next_ra_size, count_history_pages, try_context_readahead",
          "description": "执行分页预读，强制预读逻辑，计算初始和后续预读大小，统计历史页数及上下文预读判断",
          "similarity": 0.48867136240005493
        },
        {
          "chunk_id": 4,
          "file_path": "mm/readahead.c",
          "start_line": 587,
          "end_line": 708,
          "content": [
            "static void ondemand_readahead(struct readahead_control *ractl,",
            "\t\tstruct folio *folio, unsigned long req_size)",
            "{",
            "\tstruct backing_dev_info *bdi = inode_to_bdi(ractl->mapping->host);",
            "\tstruct file_ra_state *ra = ractl->ra;",
            "\tunsigned long max_pages = ra->ra_pages;",
            "\tunsigned long add_pages;",
            "\tpgoff_t index = readahead_index(ractl);",
            "\tpgoff_t expected, prev_index;",
            "\tunsigned int order = folio ? folio_order(folio) : 0;",
            "",
            "\t/*",
            "\t * If the request exceeds the readahead window, allow the read to",
            "\t * be up to the optimal hardware IO size",
            "\t */",
            "\tif (req_size > max_pages && bdi->io_pages > max_pages)",
            "\t\tmax_pages = min(req_size, bdi->io_pages);",
            "",
            "\t/*",
            "\t * start of file",
            "\t */",
            "\tif (!index)",
            "\t\tgoto initial_readahead;",
            "",
            "\t/*",
            "\t * It's the expected callback index, assume sequential access.",
            "\t * Ramp up sizes, and push forward the readahead window.",
            "\t */",
            "\texpected = round_down(ra->start + ra->size - ra->async_size,",
            "\t\t\t1UL << order);",
            "\tif (folio && index == expected) {",
            "\t\tra->start += ra->size;",
            "\t\t/*",
            "\t\t * In the case of MADV_HUGEPAGE, the actual size might exceed",
            "\t\t * the readahead window.",
            "\t\t */",
            "\t\tra->size = max(ra->size, get_next_ra_size(ra, max_pages));",
            "\t\tra->async_size = ra->size;",
            "\t\tgoto readit;",
            "\t}",
            "",
            "\t/*",
            "\t * Hit a marked folio without valid readahead state.",
            "\t * E.g. interleaved reads.",
            "\t * Query the pagecache for async_size, which normally equals to",
            "\t * readahead size. Ramp it up and use it as the new readahead size.",
            "\t */",
            "\tif (folio) {",
            "\t\tpgoff_t start;",
            "",
            "\t\trcu_read_lock();",
            "\t\tstart = page_cache_next_miss(ractl->mapping, index + 1,",
            "\t\t\t\tmax_pages);",
            "\t\trcu_read_unlock();",
            "",
            "\t\tif (!start || start - index > max_pages)",
            "\t\t\treturn;",
            "",
            "\t\tra->start = start;",
            "\t\tra->size = start - index;\t/* old async_size */",
            "\t\tra->size += req_size;",
            "\t\tra->size = get_next_ra_size(ra, max_pages);",
            "\t\tra->async_size = ra->size;",
            "\t\tgoto readit;",
            "\t}",
            "",
            "\t/*",
            "\t * oversize read",
            "\t */",
            "\tif (req_size > max_pages)",
            "\t\tgoto initial_readahead;",
            "",
            "\t/*",
            "\t * sequential cache miss",
            "\t * trivial case: (index - prev_index) == 1",
            "\t * unaligned reads: (index - prev_index) == 0",
            "\t */",
            "\tprev_index = (unsigned long long)ra->prev_pos >> PAGE_SHIFT;",
            "\tif (index - prev_index <= 1UL)",
            "\t\tgoto initial_readahead;",
            "",
            "\t/*",
            "\t * Query the page cache and look for the traces(cached history pages)",
            "\t * that a sequential stream would leave behind.",
            "\t */",
            "\tif (try_context_readahead(ractl->mapping, ra, index, req_size,",
            "\t\t\tmax_pages))",
            "\t\tgoto readit;",
            "",
            "\t/*",
            "\t * standalone, small random read",
            "\t * Read as is, and do not pollute the readahead state.",
            "\t */",
            "\tdo_page_cache_ra(ractl, req_size, 0);",
            "\treturn;",
            "",
            "initial_readahead:",
            "\tra->start = index;",
            "\tra->size = get_init_ra_size(req_size, max_pages);",
            "\tra->async_size = ra->size > req_size ? ra->size - req_size : ra->size;",
            "",
            "readit:",
            "\t/*",
            "\t * Will this read hit the readahead marker made by itself?",
            "\t * If so, trigger the readahead marker hit now, and merge",
            "\t * the resulted next readahead window into the current one.",
            "\t * Take care of maximum IO pages as above.",
            "\t */",
            "\tif (index == ra->start && ra->size == ra->async_size) {",
            "\t\tadd_pages = get_next_ra_size(ra, max_pages);",
            "\t\tif (ra->size + add_pages <= max_pages) {",
            "\t\t\tra->async_size = add_pages;",
            "\t\t\tra->size += add_pages;",
            "\t\t} else {",
            "\t\t\tra->size = max_pages;",
            "\t\t\tra->async_size = max_pages >> 1;",
            "\t\t}",
            "\t}",
            "",
            "\tractl->_index = ra->start;",
            "\tpage_cache_ra_order(ractl, ra, order);",
            "}"
          ],
          "function_name": "ondemand_readahead",
          "description": "按需预读逻辑，根据请求大小和历史记录动态调整预读窗口及大小",
          "similarity": 0.48337671160697937
        },
        {
          "chunk_id": 3,
          "file_path": "mm/readahead.c",
          "start_line": 480,
          "end_line": 581,
          "content": [
            "static inline int ra_alloc_folio(struct readahead_control *ractl, pgoff_t index,",
            "\t\tpgoff_t mark, unsigned int order, gfp_t gfp)",
            "{",
            "\tint err;",
            "\tstruct folio *folio = filemap_alloc_folio(gfp, order);",
            "",
            "\tif (!folio)",
            "\t\treturn -ENOMEM;",
            "\tmark = round_down(mark, 1UL << order);",
            "\tif (index == mark)",
            "\t\tfolio_set_readahead(folio);",
            "\terr = filemap_add_folio(ractl->mapping, folio, index, gfp);",
            "\tif (err) {",
            "\t\tfolio_put(folio);",
            "\t\treturn err;",
            "\t}",
            "",
            "\tractl->_nr_pages += 1UL << order;",
            "\tractl->_workingset |= folio_test_workingset(folio);",
            "\treturn 0;",
            "}",
            "void page_cache_ra_order(struct readahead_control *ractl,",
            "\t\tstruct file_ra_state *ra, unsigned int new_order)",
            "{",
            "\tstruct address_space *mapping = ractl->mapping;",
            "\tpgoff_t start = readahead_index(ractl);",
            "\tpgoff_t index = start;",
            "\tunsigned int min_order = mapping_min_folio_order(mapping);",
            "\tpgoff_t limit = (i_size_read(mapping->host) - 1) >> PAGE_SHIFT;",
            "\tpgoff_t mark = index + ra->size - ra->async_size;",
            "\tunsigned int nofs;",
            "\tint err = 0;",
            "\tgfp_t gfp = readahead_gfp_mask(mapping);",
            "\tunsigned int min_ra_size = max(4, mapping_min_folio_nrpages(mapping));",
            "",
            "\t/*",
            "\t * Fallback when size < min_nrpages as each folio should be",
            "\t * at least min_nrpages anyway.",
            "\t */",
            "\tif (!mapping_large_folio_support(mapping) || ra->size < min_ra_size)",
            "\t\tgoto fallback;",
            "",
            "\tlimit = min(limit, index + ra->size - 1);",
            "",
            "\tif (new_order < mapping_max_folio_order(mapping))",
            "\t\tnew_order += 2;",
            "",
            "\tnew_order = min(mapping_max_folio_order(mapping), new_order);",
            "\tnew_order = min_t(unsigned int, new_order, ilog2(ra->size));",
            "\tnew_order = max(new_order, min_order);",
            "",
            "\t/* See comment in page_cache_ra_unbounded() */",
            "\tnofs = memalloc_nofs_save();",
            "\tfilemap_invalidate_lock_shared(mapping);",
            "\t/*",
            "\t * If the new_order is greater than min_order and index is",
            "\t * already aligned to new_order, then this will be noop as index",
            "\t * aligned to new_order should also be aligned to min_order.",
            "\t */",
            "\tractl->_index = mapping_align_index(mapping, index);",
            "\tindex = readahead_index(ractl);",
            "",
            "\twhile (index <= limit) {",
            "\t\tunsigned int order = new_order;",
            "",
            "\t\t/* Align with smaller pages if needed */",
            "\t\tif (index & ((1UL << order) - 1))",
            "\t\t\torder = __ffs(index);",
            "\t\t/* Don't allocate pages past EOF */",
            "\t\twhile (order > min_order && index + (1UL << order) - 1 > limit)",
            "\t\t\torder--;",
            "\t\terr = ra_alloc_folio(ractl, index, mark, order, gfp);",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t\tindex += 1UL << order;",
            "\t}",
            "",
            "\tif (index > limit) {",
            "\t\tra->size += index - limit - 1;",
            "\t\tra->async_size += index - limit - 1;",
            "\t}",
            "",
            "\tread_pages(ractl);",
            "\tfilemap_invalidate_unlock_shared(mapping);",
            "\tmemalloc_nofs_restore(nofs);",
            "",
            "\t/*",
            "\t * If there were already pages in the page cache, then we may have",
            "\t * left some gaps.  Let the regular readahead code take care of this",
            "\t * situation below.",
            "\t */",
            "\tif (!err)",
            "\t\treturn;",
            "fallback:",
            "\t/*",
            "\t * ->readahead() may have updated readahead window size so we have to",
            "\t * check there's still something to read.",
            "\t */",
            "\tif (ra->size > index - start)",
            "\t\tdo_page_cache_ra(ractl, ra->size - (index - start),",
            "\t\t\t\t ra->async_size);",
            "}"
          ],
          "function_name": "ra_alloc_folio, page_cache_ra_order",
          "description": "分配folios并设置标记，根据新订单调整预读顺序和范围",
          "similarity": 0.42621299624443054
        }
      ]
    },
    {
      "source_file": "kernel/trace/ring_buffer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:07:21\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `trace\\ring_buffer.c`\n\n---\n\n# `trace/ring_buffer.c` 技术文档\n\n## 1. 文件概述\n\n`trace/ring_buffer.c` 实现了 Linux 内核中通用的高性能环形缓冲区（ring buffer）机制，主要用于跟踪（tracing）子系统。该缓冲区支持多 CPU 并发写入、单读者或多读者无锁读取，并通过时间戳压缩、事件类型编码和页面交换等技术优化内存使用和性能。该实现是 ftrace、perf 和其他内核跟踪工具的核心基础设施。\n\n## 2. 核心功能\n\n### 主要函数\n- `ring_buffer_print_entry_header()`：输出环形缓冲区条目头部格式说明，用于调试或用户空间解析。\n- `ring_buffer_event_length()`：返回事件有效载荷（payload）的长度，对 TIME_EXTEND 类型自动跳过扩展头。\n- `rb_event_data()`（内联）：返回指向事件实际数据的指针，处理 TIME_EXTEND 和不同长度编码。\n- `rb_event_length()`：返回完整事件结构（含头部）的字节长度。\n- `rb_event_ts_length()`：返回 TIME_EXTEND 事件及其后续数据事件的总长度。\n- `rb_event_data_length()`：计算数据类型事件的总长度（含头部）。\n- `rb_null_event()` / `rb_event_set_padding()`：判断或设置空/填充事件。\n\n### 关键数据结构（隐含或引用）\n- `struct ring_buffer_event`：环形缓冲区中每个事件的通用头部结构。\n- `struct buffer_data_page`：每个 CPU 缓冲区页面的封装，包含数据和元数据。\n- 每 CPU 页面链表：每个 CPU 拥有独立的环形页面链，写者仅写本地 CPU 缓冲区。\n\n### 核心常量与宏\n- `RINGBUF_TYPE_PADDING`、`RINGBUF_TYPE_TIME_EXTEND`、`RINGBUF_TYPE_TIME_STAMP`、`RINGBUF_TYPE_DATA`：事件类型标识。\n- `RB_ALIGNMENT` / `RB_ARCH_ALIGNMENT`：数据对齐策略，根据架构是否支持 64 位对齐访问调整。\n- `RB_MAX_SMALL_DATA`：小数据事件的最大长度（基于 4 字节对齐和类型长度上限）。\n- `TS_MSB` / `ABS_TS_MASK`：用于处理 59 位时间戳的高位截断与恢复。\n\n## 3. 关键实现\n\n### 无锁读写架构\n- **写者**：每个 CPU 只能写入其对应的 per-CPU 缓冲区，通过原子操作和内存屏障保证写入一致性，无需全局锁。\n- **读者**：每个 per-CPU 缓冲区维护一个独立的“reader page”。当 reader page 被读完后，通过原子交换（未来使用 `cmpxchg`）将其与环形缓冲区中的一个页面互换。交换后，原 reader page 不再被写者访问，读者可安全地将其用于 splice、复制或释放。\n\n### 事件编码与压缩\n- 事件头部使用紧凑位域编码：\n  - `type_len`（5 位）：事件类型或小数据长度（≤31）。\n  - `time_delta`（27 位）：相对于前一事件的时间增量。\n  - `array`（32 位）：用于存储大长度值或事件数据。\n- **TIME_EXTEND 事件**：当时间增量超出 27 位或需要绝对时间戳时，插入一个 8 字节的 TIME_EXTEND 事件，后跟实际数据事件。\n- **数据长度编码**：\n  - 若 `type_len > 0` 且 ≤ `RINGBUF_TYPE_DATA_TYPE_LEN_MAX`，则数据长度 = `type_len * RB_ALIGNMENT`，数据从 `array[0]` 开始。\n  - 否则，数据长度存储在 `array[0]`，实际数据从 `array[1]` 开始。\n\n### 时间戳处理\n- 绝对时间戳仅保留低 59 位（`ABS_TS_MASK`），高 5 位（`TS_MSB`）若非零需单独保存并在读取时恢复，以支持长时间运行的跟踪。\n\n### 内存对齐优化\n- 在支持 64 位对齐访问的架构上（`CONFIG_HAVE_64BIT_ALIGNED_ACCESS`），强制 8 字节对齐（`RB_FORCE_8BYTE_ALIGNMENT`），提升访问性能；否则使用 4 字节对齐。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/ring_buffer.h>`：定义公共 API 和数据结构。\n  - `<linux/trace_clock.h>`、`<linux/sched/clock.h>`：提供高精度时间戳源。\n  - `<linux/percpu.h>`：支持 per-CPU 缓冲区分配。\n  - `<linux/spinlock.h>`、`<asm/local.h>`：提供底层原子操作和锁原语。\n  - `<linux/trace_recursion.h>`：防止跟踪递归。\n- **子系统依赖**：\n  - **ftrace**：主要消费者，用于函数跟踪、事件跟踪等。\n  - **perf**：通过 ring buffer 获取性能事件数据。\n  - **Security Module**：通过 `<linux/security.h>` 集成 LSM 钩子（如 trace 访问控制）。\n- **架构依赖**：依赖 `CONFIG_HAVE_64BIT_ALIGNED_ACCESS` 配置项优化对齐策略。\n\n## 5. 使用场景\n\n- **内核跟踪（ftrace）**：记录函数调用、上下文切换、中断等事件，数据写入 per-CPU ring buffer，用户通过 `tracefs` 读取。\n- **性能分析（perf）**：perf 工具通过 ring buffer 接收内核采样事件（如 PMU 中断、软件事件）。\n- **实时监控与调试**：开发者或运维人员通过读取 ring buffer 内容分析系统行为、延迟或错误。\n- **自测试（selftest）**：文件包含自测试逻辑（依赖 `<linux/kthread.h>`），用于验证 ring buffer 功能正确性。\n- **低开销事件记录**：由于其无锁设计和压缩编码，适用于高频事件记录场景（如每秒百万级事件）。",
      "similarity": 0.6002131700515747,
      "chunks": [
        {
          "chunk_id": 16,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 3406,
          "end_line": 3536,
          "content": [
            "static void dump_buffer_page(struct buffer_data_page *bpage,",
            "\t\t\t     struct rb_event_info *info,",
            "\t\t\t     unsigned long tail)",
            "{",
            "\tstruct ring_buffer_event *event;",
            "\tu64 ts, delta;",
            "\tint e;",
            "",
            "\tts = bpage->time_stamp;",
            "\tpr_warn(\"  [%lld] PAGE TIME STAMP\\n\", ts);",
            "",
            "\tfor (e = 0; e < tail; e += rb_event_length(event)) {",
            "",
            "\t\tevent = (struct ring_buffer_event *)(bpage->data + e);",
            "",
            "\t\tswitch (event->type_len) {",
            "",
            "\t\tcase RINGBUF_TYPE_TIME_EXTEND:",
            "\t\t\tdelta = rb_event_time_stamp(event);",
            "\t\t\tts += delta;",
            "\t\t\tpr_warn(\"  [%lld] delta:%lld TIME EXTEND\\n\", ts, delta);",
            "\t\t\tbreak;",
            "",
            "\t\tcase RINGBUF_TYPE_TIME_STAMP:",
            "\t\t\tdelta = rb_event_time_stamp(event);",
            "\t\t\tts = rb_fix_abs_ts(delta, ts);",
            "\t\t\tpr_warn(\"  [%lld] absolute:%lld TIME STAMP\\n\", ts, delta);",
            "\t\t\tbreak;",
            "",
            "\t\tcase RINGBUF_TYPE_PADDING:",
            "\t\t\tts += event->time_delta;",
            "\t\t\tpr_warn(\"  [%lld] delta:%d PADDING\\n\", ts, event->time_delta);",
            "\t\t\tbreak;",
            "",
            "\t\tcase RINGBUF_TYPE_DATA:",
            "\t\t\tts += event->time_delta;",
            "\t\t\tpr_warn(\"  [%lld] delta:%d\\n\", ts, event->time_delta);",
            "\t\t\tbreak;",
            "",
            "\t\tdefault:",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "}",
            "static void check_buffer(struct ring_buffer_per_cpu *cpu_buffer,",
            "\t\t\t struct rb_event_info *info,",
            "\t\t\t unsigned long tail)",
            "{",
            "\tstruct ring_buffer_event *event;",
            "\tstruct buffer_data_page *bpage;",
            "\tu64 ts, delta;",
            "\tbool full = false;",
            "\tint e;",
            "",
            "\tbpage = info->tail_page->page;",
            "",
            "\tif (tail == CHECK_FULL_PAGE) {",
            "\t\tfull = true;",
            "\t\ttail = local_read(&bpage->commit);",
            "\t} else if (info->add_timestamp &",
            "\t\t   (RB_ADD_STAMP_FORCE | RB_ADD_STAMP_ABSOLUTE)) {",
            "\t\t/* Ignore events with absolute time stamps */",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Do not check the first event (skip possible extends too).",
            "\t * Also do not check if previous events have not been committed.",
            "\t */",
            "\tif (tail <= 8 || tail > local_read(&bpage->commit))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If this interrupted another event,",
            "\t */",
            "\tif (atomic_inc_return(this_cpu_ptr(&checking)) != 1)",
            "\t\tgoto out;",
            "",
            "\tts = bpage->time_stamp;",
            "",
            "\tfor (e = 0; e < tail; e += rb_event_length(event)) {",
            "",
            "\t\tevent = (struct ring_buffer_event *)(bpage->data + e);",
            "",
            "\t\tswitch (event->type_len) {",
            "",
            "\t\tcase RINGBUF_TYPE_TIME_EXTEND:",
            "\t\t\tdelta = rb_event_time_stamp(event);",
            "\t\t\tts += delta;",
            "\t\t\tbreak;",
            "",
            "\t\tcase RINGBUF_TYPE_TIME_STAMP:",
            "\t\t\tdelta = rb_event_time_stamp(event);",
            "\t\t\tts = rb_fix_abs_ts(delta, ts);",
            "\t\t\tbreak;",
            "",
            "\t\tcase RINGBUF_TYPE_PADDING:",
            "\t\t\tif (event->time_delta == 1)",
            "\t\t\t\tbreak;",
            "\t\t\tfallthrough;",
            "\t\tcase RINGBUF_TYPE_DATA:",
            "\t\t\tts += event->time_delta;",
            "\t\t\tbreak;",
            "",
            "\t\tdefault:",
            "\t\t\tRB_WARN_ON(cpu_buffer, 1);",
            "\t\t}",
            "\t}",
            "\tif ((full && ts > info->ts) ||",
            "\t    (!full && ts + info->delta != info->ts)) {",
            "\t\t/* If another report is happening, ignore this one */",
            "\t\tif (atomic_inc_return(&ts_dump) != 1) {",
            "\t\t\tatomic_dec(&ts_dump);",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tatomic_inc(&cpu_buffer->record_disabled);",
            "\t\t/* There's some cases in boot up that this can happen */",
            "\t\tWARN_ON_ONCE(system_state != SYSTEM_BOOTING);",
            "\t\tpr_warn(\"[CPU: %d]TIME DOES NOT MATCH expected:%lld actual:%lld delta:%lld before:%lld after:%lld%s\\n\",",
            "\t\t\tcpu_buffer->cpu,",
            "\t\t\tts + info->delta, info->ts, info->delta,",
            "\t\t\tinfo->before, info->after,",
            "\t\t\tfull ? \" (full)\" : \"\");",
            "\t\tdump_buffer_page(bpage, info, tail);",
            "\t\tatomic_dec(&ts_dump);",
            "\t\t/* Do not re-enable checking */",
            "\t\treturn;",
            "\t}",
            "out:",
            "\tatomic_dec(this_cpu_ptr(&checking));",
            "}"
          ],
          "function_name": "dump_buffer_page, check_buffer",
          "description": "dump_buffer_page 遍历缓冲页打印时间戳信息；check_buffer 核对时间戳一致性，检测数据异常并触发警告",
          "similarity": 0.5732863545417786
        },
        {
          "chunk_id": 25,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 5641,
          "end_line": 5840,
          "content": [
            "void ring_buffer_free_read_page(struct trace_buffer *buffer, int cpu, void *data)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct buffer_data_page *bpage = data;",
            "\tstruct page *page = virt_to_page(bpage);",
            "\tunsigned long flags;",
            "",
            "\tif (!buffer || !buffer->buffers || !buffer->buffers[cpu])",
            "\t\treturn;",
            "",
            "\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t/* If the page is still in use someplace else, we can't reuse it */",
            "\tif (page_ref_count(page) > 1)",
            "\t\tgoto out;",
            "",
            "\tlocal_irq_save(flags);",
            "\tarch_spin_lock(&cpu_buffer->lock);",
            "",
            "\tif (!cpu_buffer->free_page) {",
            "\t\tcpu_buffer->free_page = bpage;",
            "\t\tbpage = NULL;",
            "\t}",
            "",
            "\tarch_spin_unlock(&cpu_buffer->lock);",
            "\tlocal_irq_restore(flags);",
            "",
            " out:",
            "\tfree_page((unsigned long)bpage);",
            "}",
            "int ring_buffer_read_page(struct trace_buffer *buffer,",
            "\t\t\t  void **data_page, size_t len, int cpu, int full)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];",
            "\tstruct ring_buffer_event *event;",
            "\tstruct buffer_data_page *bpage;",
            "\tstruct buffer_page *reader;",
            "\tunsigned long missed_events;",
            "\tunsigned long flags;",
            "\tunsigned int commit;",
            "\tunsigned int read;",
            "\tu64 save_timestamp;",
            "\tint ret = -1;",
            "",
            "\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * If len is not big enough to hold the page header, then",
            "\t * we can not copy anything.",
            "\t */",
            "\tif (len <= BUF_PAGE_HDR_SIZE)",
            "\t\tgoto out;",
            "",
            "\tlen -= BUF_PAGE_HDR_SIZE;",
            "",
            "\tif (!data_page)",
            "\t\tgoto out;",
            "",
            "\tbpage = *data_page;",
            "\tif (!bpage)",
            "\t\tgoto out;",
            "",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "",
            "\treader = rb_get_reader_page(cpu_buffer);",
            "\tif (!reader)",
            "\t\tgoto out_unlock;",
            "",
            "\tevent = rb_reader_event(cpu_buffer);",
            "",
            "\tread = reader->read;",
            "\tcommit = rb_page_commit(reader);",
            "",
            "\t/* Check if any events were dropped */",
            "\tmissed_events = cpu_buffer->lost_events;",
            "",
            "\t/*",
            "\t * If this page has been partially read or",
            "\t * if len is not big enough to read the rest of the page or",
            "\t * a writer is still on the page, then",
            "\t * we must copy the data from the page to the buffer.",
            "\t * Otherwise, we can simply swap the page with the one passed in.",
            "\t */",
            "\tif (read || (len < (commit - read)) ||",
            "\t    cpu_buffer->reader_page == cpu_buffer->commit_page) {",
            "\t\tstruct buffer_data_page *rpage = cpu_buffer->reader_page->page;",
            "\t\tunsigned int rpos = read;",
            "\t\tunsigned int pos = 0;",
            "\t\tunsigned int size;",
            "",
            "\t\t/*",
            "\t\t * If a full page is expected, this can still be returned",
            "\t\t * if there's been a previous partial read and the",
            "\t\t * rest of the page can be read and the commit page is off",
            "\t\t * the reader page.",
            "\t\t */",
            "\t\tif (full &&",
            "\t\t    (!read || (len < (commit - read)) ||",
            "\t\t     cpu_buffer->reader_page == cpu_buffer->commit_page))",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\tif (len > (commit - read))",
            "\t\t\tlen = (commit - read);",
            "",
            "\t\t/* Always keep the time extend and data together */",
            "\t\tsize = rb_event_ts_length(event);",
            "",
            "\t\tif (len < size)",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\t/* save the current timestamp, since the user will need it */",
            "\t\tsave_timestamp = cpu_buffer->read_stamp;",
            "",
            "\t\t/* Need to copy one event at a time */",
            "\t\tdo {",
            "\t\t\t/* We need the size of one event, because",
            "\t\t\t * rb_advance_reader only advances by one event,",
            "\t\t\t * whereas rb_event_ts_length may include the size of",
            "\t\t\t * one or two events.",
            "\t\t\t * We have already ensured there's enough space if this",
            "\t\t\t * is a time extend. */",
            "\t\t\tsize = rb_event_length(event);",
            "\t\t\tmemcpy(bpage->data + pos, rpage->data + rpos, size);",
            "",
            "\t\t\tlen -= size;",
            "",
            "\t\t\trb_advance_reader(cpu_buffer);",
            "\t\t\trpos = reader->read;",
            "\t\t\tpos += size;",
            "",
            "\t\t\tif (rpos >= commit)",
            "\t\t\t\tbreak;",
            "",
            "\t\t\tevent = rb_reader_event(cpu_buffer);",
            "\t\t\t/* Always keep the time extend and data together */",
            "\t\t\tsize = rb_event_ts_length(event);",
            "\t\t} while (len >= size);",
            "",
            "\t\t/* update bpage */",
            "\t\tlocal_set(&bpage->commit, pos);",
            "\t\tbpage->time_stamp = save_timestamp;",
            "",
            "\t\t/* we copied everything to the beginning */",
            "\t\tread = 0;",
            "\t} else {",
            "\t\t/* update the entry counter */",
            "\t\tcpu_buffer->read += rb_page_entries(reader);",
            "\t\tcpu_buffer->read_bytes += rb_page_commit(reader);",
            "",
            "\t\t/* swap the pages */",
            "\t\trb_init_page(bpage);",
            "\t\tbpage = reader->page;",
            "\t\treader->page = *data_page;",
            "\t\tlocal_set(&reader->write, 0);",
            "\t\tlocal_set(&reader->entries, 0);",
            "\t\treader->read = 0;",
            "\t\t*data_page = bpage;",
            "",
            "\t\t/*",
            "\t\t * Use the real_end for the data size,",
            "\t\t * This gives us a chance to store the lost events",
            "\t\t * on the page.",
            "\t\t */",
            "\t\tif (reader->real_end)",
            "\t\t\tlocal_set(&bpage->commit, reader->real_end);",
            "\t}",
            "\tret = read;",
            "",
            "\tcpu_buffer->lost_events = 0;",
            "",
            "\tcommit = local_read(&bpage->commit);",
            "\t/*",
            "\t * Set a flag in the commit field if we lost events",
            "\t */",
            "\tif (missed_events) {",
            "\t\t/* If there is room at the end of the page to save the",
            "\t\t * missed events, then record it there.",
            "\t\t */",
            "\t\tif (BUF_PAGE_SIZE - commit >= sizeof(missed_events)) {",
            "\t\t\tmemcpy(&bpage->data[commit], &missed_events,",
            "\t\t\t       sizeof(missed_events));",
            "\t\t\tlocal_add(RB_MISSED_STORED, &bpage->commit);",
            "\t\t\tcommit += sizeof(missed_events);",
            "\t\t}",
            "\t\tlocal_add(RB_MISSED_EVENTS, &bpage->commit);",
            "\t}",
            "",
            "\t/*",
            "\t * This page may be off to user land. Zero it out here.",
            "\t */",
            "\tif (commit < BUF_PAGE_SIZE)",
            "\t\tmemset(&bpage->data[commit], 0, BUF_PAGE_SIZE - commit);",
            "",
            " out_unlock:",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "",
            " out:",
            "\treturn ret;",
            "}"
          ],
          "function_name": "ring_buffer_free_read_page, ring_buffer_read_page",
          "description": "ring_buffer_free_read_page 函数用于释放可复用的读取页面，检查页面引用计数后若可复用则将其加入cpu_buffer的free_page链表并释放内存；ring_buffer_read_page 函数负责从环形缓冲区读取事件数据到指定页面，处理部分读取场景，复制事件内容并更新提交指针，同时记录丢失的事件信息",
          "similarity": 0.5694997906684875
        },
        {
          "chunk_id": 22,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 4955,
          "end_line": 5087,
          "content": [
            "static inline void",
            "rb_reader_unlock(struct ring_buffer_per_cpu *cpu_buffer, bool locked)",
            "{",
            "\tif (likely(locked))",
            "\t\traw_spin_unlock(&cpu_buffer->reader_lock);",
            "}",
            "bool ring_buffer_iter_dropped(struct ring_buffer_iter *iter)",
            "{",
            "\tbool ret = iter->missed_events != 0;",
            "",
            "\titer->missed_events = 0;",
            "\treturn ret;",
            "}",
            "void",
            "ring_buffer_read_prepare_sync(void)",
            "{",
            "\tsynchronize_rcu();",
            "}",
            "void",
            "ring_buffer_read_start(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tunsigned long flags;",
            "",
            "\tif (!iter)",
            "\t\treturn;",
            "",
            "\tcpu_buffer = iter->cpu_buffer;",
            "",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\tarch_spin_lock(&cpu_buffer->lock);",
            "\trb_iter_reset(iter);",
            "\tarch_spin_unlock(&cpu_buffer->lock);",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "}",
            "void",
            "ring_buffer_read_finish(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * Ring buffer is disabled from recording, here's a good place",
            "\t * to check the integrity of the ring buffer.",
            "\t * Must prevent readers from trying to read, as the check",
            "\t * clears the HEAD page and readers require it.",
            "\t */",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\trb_check_pages(cpu_buffer);",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "",
            "\tatomic_dec(&cpu_buffer->resize_disabled);",
            "\tkfree(iter->event);",
            "\tkfree(iter);",
            "}",
            "void ring_buffer_iter_advance(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "",
            "\trb_advance_iter(iter);",
            "",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "}",
            "unsigned long ring_buffer_size(struct trace_buffer *buffer, int cpu)",
            "{",
            "\t/*",
            "\t * Earlier, this method returned",
            "\t *\tBUF_PAGE_SIZE * buffer->nr_pages",
            "\t * Since the nr_pages field is now removed, we have converted this to",
            "\t * return the per cpu buffer value.",
            "\t */",
            "\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\treturn 0;",
            "",
            "\treturn BUF_PAGE_SIZE * buffer->buffers[cpu]->nr_pages;",
            "}",
            "static void rb_clear_buffer_page(struct buffer_page *page)",
            "{",
            "\tlocal_set(&page->write, 0);",
            "\tlocal_set(&page->entries, 0);",
            "\trb_init_page(page->page);",
            "\tpage->read = 0;",
            "}",
            "static void",
            "rb_reset_cpu(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tstruct buffer_page *page;",
            "",
            "\trb_head_page_deactivate(cpu_buffer);",
            "",
            "\tcpu_buffer->head_page",
            "\t\t= list_entry(cpu_buffer->pages, struct buffer_page, list);",
            "\trb_clear_buffer_page(cpu_buffer->head_page);",
            "\tlist_for_each_entry(page, cpu_buffer->pages, list) {",
            "\t\trb_clear_buffer_page(page);",
            "\t}",
            "",
            "\tcpu_buffer->tail_page = cpu_buffer->head_page;",
            "\tcpu_buffer->commit_page = cpu_buffer->head_page;",
            "",
            "\tINIT_LIST_HEAD(&cpu_buffer->reader_page->list);",
            "\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\trb_clear_buffer_page(cpu_buffer->reader_page);",
            "",
            "\tlocal_set(&cpu_buffer->entries_bytes, 0);",
            "\tlocal_set(&cpu_buffer->overrun, 0);",
            "\tlocal_set(&cpu_buffer->commit_overrun, 0);",
            "\tlocal_set(&cpu_buffer->dropped_events, 0);",
            "\tlocal_set(&cpu_buffer->entries, 0);",
            "\tlocal_set(&cpu_buffer->committing, 0);",
            "\tlocal_set(&cpu_buffer->commits, 0);",
            "\tlocal_set(&cpu_buffer->pages_touched, 0);",
            "\tlocal_set(&cpu_buffer->pages_lost, 0);",
            "\tlocal_set(&cpu_buffer->pages_read, 0);",
            "\tcpu_buffer->last_pages_touch = 0;",
            "\tcpu_buffer->shortest_full = 0;",
            "\tcpu_buffer->read = 0;",
            "\tcpu_buffer->read_bytes = 0;",
            "",
            "\trb_time_set(&cpu_buffer->write_stamp, 0);",
            "\trb_time_set(&cpu_buffer->before_stamp, 0);",
            "",
            "\tmemset(cpu_buffer->event_stamp, 0, sizeof(cpu_buffer->event_stamp));",
            "",
            "\tcpu_buffer->lost_events = 0;",
            "\tcpu_buffer->last_overrun = 0;",
            "",
            "\trb_head_page_activate(cpu_buffer);",
            "\tcpu_buffer->pages_removed = 0;",
            "}"
          ],
          "function_name": "rb_reader_unlock, ring_buffer_iter_dropped, ring_buffer_read_prepare_sync, ring_buffer_read_start, ring_buffer_read_finish, ring_buffer_iter_advance, ring_buffer_size, rb_clear_buffer_page, rb_reset_cpu",
          "description": "该代码段实现环形缓冲区（ring buffer）的读取控制与状态管理。  \n其中 `rb_reader_unlock` 管理读锁释放，`ring_buffer_read_start` 和 `ring_buffer_read_finish` 负责读取前的锁同步与资源清理，`rb_reset_cpu` 用于重置指定 CPU 的缓冲区状态。  \n其他函数如 `ring_buffer_iter_advance` 推进迭代器位置，`ring_buffer_size` 计算缓冲区容量，`rb_clear_buffer_page` 清空页面数据，共同支撑事件追踪的并发安全与状态维护。",
          "similarity": 0.5658835172653198
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 2052,
          "end_line": 2363,
          "content": [
            "static bool",
            "rb_insert_pages(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tstruct list_head *pages = &cpu_buffer->new_pages;",
            "\tunsigned long flags;",
            "\tbool success;",
            "\tint retries;",
            "",
            "\t/* Can be called at early boot up, where interrupts must not been enabled */",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t/*",
            "\t * We are holding the reader lock, so the reader page won't be swapped",
            "\t * in the ring buffer. Now we are racing with the writer trying to",
            "\t * move head page and the tail page.",
            "\t * We are going to adapt the reader page update process where:",
            "\t * 1. We first splice the start and end of list of new pages between",
            "\t *    the head page and its previous page.",
            "\t * 2. We cmpxchg the prev_page->next to point from head page to the",
            "\t *    start of new pages list.",
            "\t * 3. Finally, we update the head->prev to the end of new list.",
            "\t *",
            "\t * We will try this process 10 times, to make sure that we don't keep",
            "\t * spinning.",
            "\t */",
            "\tretries = 10;",
            "\tsuccess = false;",
            "\twhile (retries--) {",
            "\t\tstruct list_head *head_page, *prev_page, *r;",
            "\t\tstruct list_head *last_page, *first_page;",
            "\t\tstruct list_head *head_page_with_bit;",
            "\t\tstruct buffer_page *hpage = rb_set_head_page(cpu_buffer);",
            "",
            "\t\tif (!hpage)",
            "\t\t\tbreak;",
            "\t\thead_page = &hpage->list;",
            "\t\tprev_page = head_page->prev;",
            "",
            "\t\tfirst_page = pages->next;",
            "\t\tlast_page  = pages->prev;",
            "",
            "\t\thead_page_with_bit = (struct list_head *)",
            "\t\t\t\t     ((unsigned long)head_page | RB_PAGE_HEAD);",
            "",
            "\t\tlast_page->next = head_page_with_bit;",
            "\t\tfirst_page->prev = prev_page;",
            "",
            "\t\tr = cmpxchg(&prev_page->next, head_page_with_bit, first_page);",
            "",
            "\t\tif (r == head_page_with_bit) {",
            "\t\t\t/*",
            "\t\t\t * yay, we replaced the page pointer to our new list,",
            "\t\t\t * now, we just have to update to head page's prev",
            "\t\t\t * pointer to point to end of list",
            "\t\t\t */",
            "\t\t\thead_page->prev = last_page;",
            "\t\t\tsuccess = true;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tif (success)",
            "\t\tINIT_LIST_HEAD(pages);",
            "\t/*",
            "\t * If we weren't successful in adding in new pages, warn and stop",
            "\t * tracing",
            "\t */",
            "\tRB_WARN_ON(cpu_buffer, !success);",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "",
            "\t/* free pages if they weren't inserted */",
            "\tif (!success) {",
            "\t\tstruct buffer_page *bpage, *tmp;",
            "\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,",
            "\t\t\t\t\t list) {",
            "\t\t\tlist_del_init(&bpage->list);",
            "\t\t\tfree_buffer_page(bpage);",
            "\t\t}",
            "\t}",
            "\treturn success;",
            "}",
            "static void rb_update_pages(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tbool success;",
            "",
            "\tif (cpu_buffer->nr_pages_to_update > 0)",
            "\t\tsuccess = rb_insert_pages(cpu_buffer);",
            "\telse",
            "\t\tsuccess = rb_remove_pages(cpu_buffer,",
            "\t\t\t\t\t-cpu_buffer->nr_pages_to_update);",
            "",
            "\tif (success)",
            "\t\tcpu_buffer->nr_pages += cpu_buffer->nr_pages_to_update;",
            "}",
            "static void update_pages_handler(struct work_struct *work)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = container_of(work,",
            "\t\t\tstruct ring_buffer_per_cpu, update_pages_work);",
            "\trb_update_pages(cpu_buffer);",
            "\tcomplete(&cpu_buffer->update_done);",
            "}",
            "int ring_buffer_resize(struct trace_buffer *buffer, unsigned long size,",
            "\t\t\tint cpu_id)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tunsigned long nr_pages;",
            "\tint cpu, err;",
            "",
            "\t/*",
            "\t * Always succeed at resizing a non-existent buffer:",
            "\t */",
            "\tif (!buffer)",
            "\t\treturn 0;",
            "",
            "\t/* Make sure the requested buffer exists */",
            "\tif (cpu_id != RING_BUFFER_ALL_CPUS &&",
            "\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))",
            "\t\treturn 0;",
            "",
            "\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);",
            "",
            "\t/* we need a minimum of two pages */",
            "\tif (nr_pages < 2)",
            "\t\tnr_pages = 2;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "\tatomic_inc(&buffer->resizing);",
            "",
            "\tif (cpu_id == RING_BUFFER_ALL_CPUS) {",
            "\t\t/*",
            "\t\t * Don't succeed if resizing is disabled, as a reader might be",
            "\t\t * manipulating the ring buffer and is expecting a sane state while",
            "\t\t * this is true.",
            "\t\t */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {",
            "\t\t\t\terr = -EBUSY;",
            "\t\t\t\tgoto out_err_unlock;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* calculate the pages to update */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -",
            "\t\t\t\t\t\t\tcpu_buffer->nr_pages;",
            "\t\t\t/*",
            "\t\t\t * nothing more to do for removing pages or no update",
            "\t\t\t */",
            "\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)",
            "\t\t\t\tcontinue;",
            "\t\t\t/*",
            "\t\t\t * to add pages, make sure all new pages can be",
            "\t\t\t * allocated without receiving ENOMEM",
            "\t\t\t */",
            "\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\t\t\tif (__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,",
            "\t\t\t\t\t\t&cpu_buffer->new_pages)) {",
            "\t\t\t\t/* not enough memory for new pages */",
            "\t\t\t\terr = -ENOMEM;",
            "\t\t\t\tgoto out_err;",
            "\t\t\t}",
            "",
            "\t\t\tcond_resched();",
            "\t\t}",
            "",
            "\t\tcpus_read_lock();",
            "\t\t/*",
            "\t\t * Fire off all the required work handlers",
            "\t\t * We can't schedule on offline CPUs, but it's not necessary",
            "\t\t * since we can change their buffer sizes without any race.",
            "\t\t */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (!cpu_buffer->nr_pages_to_update)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\t/* Can't run something on an offline CPU. */",
            "\t\t\tif (!cpu_online(cpu)) {",
            "\t\t\t\trb_update_pages(cpu_buffer);",
            "\t\t\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\t\t} else {",
            "\t\t\t\t/* Run directly if possible. */",
            "\t\t\t\tmigrate_disable();",
            "\t\t\t\tif (cpu != smp_processor_id()) {",
            "\t\t\t\t\tmigrate_enable();",
            "\t\t\t\t\tschedule_work_on(cpu,",
            "\t\t\t\t\t\t\t &cpu_buffer->update_pages_work);",
            "\t\t\t\t} else {",
            "\t\t\t\t\tupdate_pages_handler(&cpu_buffer->update_pages_work);",
            "\t\t\t\t\tmigrate_enable();",
            "\t\t\t\t}",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* wait for all the updates to complete */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (!cpu_buffer->nr_pages_to_update)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tif (cpu_online(cpu))",
            "\t\t\t\twait_for_completion(&cpu_buffer->update_done);",
            "\t\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\t}",
            "",
            "\t\tcpus_read_unlock();",
            "\t} else {",
            "\t\tcpu_buffer = buffer->buffers[cpu_id];",
            "",
            "\t\tif (nr_pages == cpu_buffer->nr_pages)",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * Don't succeed if resizing is disabled, as a reader might be",
            "\t\t * manipulating the ring buffer and is expecting a sane state while",
            "\t\t * this is true.",
            "\t\t */",
            "\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {",
            "\t\t\terr = -EBUSY;",
            "\t\t\tgoto out_err_unlock;",
            "\t\t}",
            "",
            "\t\tcpu_buffer->nr_pages_to_update = nr_pages -",
            "\t\t\t\t\t\tcpu_buffer->nr_pages;",
            "",
            "\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\t\tif (cpu_buffer->nr_pages_to_update > 0 &&",
            "\t\t\t__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,",
            "\t\t\t\t\t    &cpu_buffer->new_pages)) {",
            "\t\t\terr = -ENOMEM;",
            "\t\t\tgoto out_err;",
            "\t\t}",
            "",
            "\t\tcpus_read_lock();",
            "",
            "\t\t/* Can't run something on an offline CPU. */",
            "\t\tif (!cpu_online(cpu_id))",
            "\t\t\trb_update_pages(cpu_buffer);",
            "\t\telse {",
            "\t\t\t/* Run directly if possible. */",
            "\t\t\tmigrate_disable();",
            "\t\t\tif (cpu_id == smp_processor_id()) {",
            "\t\t\t\trb_update_pages(cpu_buffer);",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t} else {",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t\tschedule_work_on(cpu_id,",
            "\t\t\t\t\t\t &cpu_buffer->update_pages_work);",
            "\t\t\t\twait_for_completion(&cpu_buffer->update_done);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\tcpus_read_unlock();",
            "\t}",
            "",
            " out:",
            "\t/*",
            "\t * The ring buffer resize can happen with the ring buffer",
            "\t * enabled, so that the update disturbs the tracing as little",
            "\t * as possible. But if the buffer is disabled, we do not need",
            "\t * to worry about that, and we can take the time to verify",
            "\t * that the buffer is not corrupt.",
            "\t */",
            "\tif (atomic_read(&buffer->record_disabled)) {",
            "\t\tatomic_inc(&buffer->record_disabled);",
            "\t\t/*",
            "\t\t * Even though the buffer was disabled, we must make sure",
            "\t\t * that it is truly disabled before calling rb_check_pages.",
            "\t\t * There could have been a race between checking",
            "\t\t * record_disable and incrementing it.",
            "\t\t */",
            "\t\tsynchronize_rcu();",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tunsigned long flags;",
            "",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t\t\trb_check_pages(cpu_buffer);",
            "\t\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "\t\t}",
            "\t\tatomic_dec(&buffer->record_disabled);",
            "\t}",
            "",
            "\tatomic_dec(&buffer->resizing);",
            "\tmutex_unlock(&buffer->mutex);",
            "\treturn 0;",
            "",
            " out_err:",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tstruct buffer_page *bpage, *tmp;",
            "",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\tcpu_buffer->nr_pages_to_update = 0;",
            "",
            "\t\tif (list_empty(&cpu_buffer->new_pages))",
            "\t\t\tcontinue;",
            "",
            "\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,",
            "\t\t\t\t\tlist) {",
            "\t\t\tlist_del_init(&bpage->list);",
            "\t\t\tfree_buffer_page(bpage);",
            "\t\t}",
            "\t}",
            " out_err_unlock:",
            "\tatomic_dec(&buffer->resizing);",
            "\tmutex_unlock(&buffer->mutex);",
            "\treturn err;",
            "}"
          ],
          "function_name": "rb_insert_pages, rb_update_pages, update_pages_handler, ring_buffer_resize",
          "description": "实现将新分配的缓冲页插入到环形缓冲区的头部，通过CAS操作确保线程安全地更新链表结构，若失败则释放内存资源。包含调整缓冲区大小的核心逻辑，协调多CPU上的页面分配与更新操作。",
          "similarity": 0.5340591669082642
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 357,
          "end_line": 461,
          "content": [
            "static __always_inline unsigned int rb_page_commit(struct buffer_page *bpage)",
            "{",
            "\treturn local_read(&bpage->page->commit);",
            "}",
            "static void free_buffer_page(struct buffer_page *bpage)",
            "{",
            "\tfree_page((unsigned long)bpage->page);",
            "\tkfree(bpage);",
            "}",
            "static inline bool test_time_stamp(u64 delta)",
            "{",
            "\treturn !!(delta & TS_DELTA_TEST);",
            "}",
            "int ring_buffer_print_page_header(struct trace_seq *s)",
            "{",
            "\tstruct buffer_data_page field;",
            "",
            "\ttrace_seq_printf(s, \"\\tfield: u64 timestamp;\\t\"",
            "\t\t\t \"offset:0;\\tsize:%u;\\tsigned:%u;\\n\",",
            "\t\t\t (unsigned int)sizeof(field.time_stamp),",
            "\t\t\t (unsigned int)is_signed_type(u64));",
            "",
            "\ttrace_seq_printf(s, \"\\tfield: local_t commit;\\t\"",
            "\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",",
            "\t\t\t (unsigned int)offsetof(typeof(field), commit),",
            "\t\t\t (unsigned int)sizeof(field.commit),",
            "\t\t\t (unsigned int)is_signed_type(long));",
            "",
            "\ttrace_seq_printf(s, \"\\tfield: int overwrite;\\t\"",
            "\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",",
            "\t\t\t (unsigned int)offsetof(typeof(field), commit),",
            "\t\t\t 1,",
            "\t\t\t (unsigned int)is_signed_type(long));",
            "",
            "\ttrace_seq_printf(s, \"\\tfield: char data;\\t\"",
            "\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",",
            "\t\t\t (unsigned int)offsetof(typeof(field), data),",
            "\t\t\t (unsigned int)BUF_PAGE_SIZE,",
            "\t\t\t (unsigned int)is_signed_type(char));",
            "",
            "\treturn !trace_seq_has_overflowed(s);",
            "}",
            "static inline int rb_time_cnt(unsigned long val)",
            "{",
            "\treturn (val >> RB_TIME_SHIFT) & 3;",
            "}",
            "static inline u64 rb_time_val(unsigned long top, unsigned long bottom)",
            "{",
            "\tu64 val;",
            "",
            "\tval = top & RB_TIME_VAL_MASK;",
            "\tval <<= RB_TIME_SHIFT;",
            "\tval |= bottom & RB_TIME_VAL_MASK;",
            "",
            "\treturn val;",
            "}",
            "static inline bool __rb_time_read(rb_time_t *t, u64 *ret, unsigned long *cnt)",
            "{",
            "\tunsigned long top, bottom, msb;",
            "\tunsigned long c;",
            "",
            "\t/*",
            "\t * If the read is interrupted by a write, then the cnt will",
            "\t * be different. Loop until both top and bottom have been read",
            "\t * without interruption.",
            "\t */",
            "\tdo {",
            "\t\tc = local_read(&t->cnt);",
            "\t\ttop = local_read(&t->top);",
            "\t\tbottom = local_read(&t->bottom);",
            "\t\tmsb = local_read(&t->msb);",
            "\t} while (c != local_read(&t->cnt));",
            "",
            "\t*cnt = rb_time_cnt(top);",
            "",
            "\t/* If top, msb or bottom counts don't match, this interrupted a write */",
            "\tif (*cnt != rb_time_cnt(msb) || *cnt != rb_time_cnt(bottom))",
            "\t\treturn false;",
            "",
            "\t/* The shift to msb will lose its cnt bits */",
            "\t*ret = rb_time_val(top, bottom) | ((u64)msb << RB_TIME_MSB_SHIFT);",
            "\treturn true;",
            "}",
            "static bool rb_time_read(rb_time_t *t, u64 *ret)",
            "{",
            "\tunsigned long cnt;",
            "",
            "\treturn __rb_time_read(t, ret, &cnt);",
            "}",
            "static inline unsigned long rb_time_val_cnt(unsigned long val, unsigned long cnt)",
            "{",
            "\treturn (val & RB_TIME_VAL_MASK) | ((cnt & 3) << RB_TIME_SHIFT);",
            "}",
            "static inline void rb_time_split(u64 val, unsigned long *top, unsigned long *bottom,",
            "\t\t\t\t unsigned long *msb)",
            "{",
            "\t*top = (unsigned long)((val >> RB_TIME_SHIFT) & RB_TIME_VAL_MASK);",
            "\t*bottom = (unsigned long)(val & RB_TIME_VAL_MASK);",
            "\t*msb = (unsigned long)(val >> RB_TIME_MSB_SHIFT);",
            "}",
            "static inline void rb_time_val_set(local_t *t, unsigned long val, unsigned long cnt)",
            "{",
            "\tval = rb_time_val_cnt(val, cnt);",
            "\tlocal_set(t, val);",
            "}"
          ],
          "function_name": "rb_page_commit, free_buffer_page, test_time_stamp, ring_buffer_print_page_header, rb_time_cnt, rb_time_val, __rb_time_read, rb_time_read, rb_time_val_cnt, rb_time_split, rb_time_val_set",
          "description": "实现缓冲页提交计数器读取、页面释放、时间戳字段解析与组合操作，包含原子操作辅助函数用于安全的时间戳读写，支持多处理器环境下时间戳的精确捕获",
          "similarity": 0.5290120840072632
        }
      ]
    },
    {
      "source_file": "kernel/bpf/core.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:05:43\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\core.c`\n\n---\n\n# `bpf/core.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/core.c` 是 Linux 内核中 BPF（Berkeley Packet Filter）子系统的核心实现文件之一，主要负责 BPF 程序的内存分配、生命周期管理、辅助函数支持以及与 JIT（Just-In-Time）编译器的交互。该文件为 eBPF（extended BPF）程序提供基础运行时支持，包括程序结构体的分配与释放、统计信息管理、调试信息（如行号信息 linfo）填充、程序标签（tag）计算等关键功能。其设计融合了经典 BPF 的兼容性与现代 eBPF 的扩展能力，是连接用户空间 BPF 程序加载与内核执行环境的重要桥梁。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct bpf_prog`：eBPF 程序的核心结构体，包含指令数组、辅助信息（`aux`）、JIT 编译后的函数指针等。\n- `struct bpf_prog_aux`：BPF 程序的辅助数据结构，用于存储映射（maps）、外部函数（kfuncs）、调试信息（linfo）、引用计数等。\n- `struct bpf_prog_stats`：每 CPU 的 BPF 程序执行统计信息（如执行次数、运行时间）。\n- `bpf_global_ma`：全局 BPF 内存分配器实例，用于内存控制组（memcg）感知的内存分配。\n\n### 主要函数\n- `bpf_internal_load_pointer_neg_helper()`：为经典 BPF 程序提供负偏移量的数据包指针加载辅助函数。\n- `bpf_prog_alloc_no_stats()`：分配不包含统计信息的 BPF 程序结构体。\n- `bpf_prog_alloc()`：分配包含每 CPU 统计信息的完整 BPF 程序结构体（导出符号，供模块使用）。\n- `bpf_prog_alloc_jited_linfo()`：为 JIT 编译分配行号信息（linfo）映射数组。\n- `bpf_prog_jit_attempt_done()`：清理 JIT 编译尝试后的临时资源（如未使用的 linfo 或 kfunc 表）。\n- `bpf_prog_fill_jited_linfo()`：根据 JIT 指令偏移映射填充调试用的行号信息。\n- `bpf_prog_realloc()`：重新分配更大的 BPF 程序内存空间（用于程序转换或扩展）。\n- `__bpf_prog_free()`：释放 BPF 程序及其所有关联资源。\n- `bpf_prog_calc_tag()`：计算 BPF 程序的 SHA1 哈希标签（用于唯一标识程序内容，排除不稳定字段如 map fd）。\n\n## 3. 关键实现\n\n### 内存分配与管理\n- BPF 程序主体（`struct bpf_prog`）使用 `__vmalloc()` 分配，按页对齐（`round_up(size, PAGE_SIZE)`），支持大程序。\n- 辅助结构（`aux`）、每 CPU 统计（`stats`）和活跃状态（`active`）使用 `kzalloc()` 和 `alloc_percpu_gfp()` 分配，并集成内存控制组（memcg）支持（通过 `bpf_memcg_flags()`）。\n- 全局内存分配器 `bpf_global_ma` 用于统一管理 BPF 相关内存，支持 memcg 隔离。\n\n### JIT 调试信息支持\n- `bpf_prog_fill_jited_linfo()` 实现了从 BPF 指令偏移到 JIT 机器码偏移的映射，用于将源码行号信息（`linfo`）关联到 JIT 编译后的代码地址，便于调试和性能分析。\n- 该函数依赖 JIT 引擎提供的 `insn_to_jit_off` 数组，确保调试信息与实际执行代码对齐。\n\n### 程序标签计算\n- `bpf_prog_calc_tag()` 在计算 SHA1 哈希前，会复制程序指令并**移除不稳定的字段**（如 `BPF_LD_MAP_FD` 指令中的 map 文件描述符），确保相同逻辑的程序生成相同的标签，用于程序去重和验证缓存。\n\n### 安全与兼容性\n- `bpf_internal_load_pointer_neg_helper()` 处理经典 BPF 的负偏移访问（如 `SKF_NET_OFF`、`SKF_LL_OFF`），严格检查指针是否在 `sk_buff` 数据范围内，防止越界访问。\n- 所有内存分配均使用 `__GFP_ZERO` 初始化，避免信息泄露。\n- 支持细粒度锁（如 `used_maps_mutex`、`ext_mutex`）保护共享资源。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - BPF 核心接口：`<linux/bpf.h>`, `<linux/filter.h>`\n  - 验证器：`<linux/bpf_verifier.h>`\n  - 内存管理：`<linux/vmalloc.h>`, `<linux/memcontrol.h>`, `<linux/bpf_mem_alloc.h>`\n  - 网络子系统：`<linux/skbuff.h>`（用于经典 BPF 辅助函数）\n  - 调试与符号：`<linux/kallsyms.h>`, `<linux/perf_event.h>`\n  - 架构相关：`<asm/barrier.h>`, `<asm/unaligned.h>`\n- **模块依赖**：\n  - BPF JIT 编译器（通过 `ebpf_jit_enabled()` 和 `bpf_jit_blinding_enabled()` 交互）\n  - BPF 验证器（`bpf_check_classic()` 在其他文件中，但与此文件协同工作）\n  - cgroup BPF（`CONFIG_CGROUP_BPF` 条件编译）\n  - 内存控制组（memcg）子系统\n\n## 5. 使用场景\n\n- **BPF 程序加载**：当用户空间通过 `bpf(BPF_PROG_LOAD, ...)` 系统调用加载程序时，内核调用 `bpf_prog_alloc()` 分配程序结构。\n- **JIT 编译流程**：JIT 编译器在编译前后调用 `bpf_prog_alloc_jited_linfo()` 和 `bpf_prog_fill_jited_linfo()` 管理调试信息；编译失败时调用 `bpf_prog_jit_attempt_done()` 清理资源。\n- **程序转换与优化**：验证器或 JIT 在优化过程中可能需要更大的内存空间，调用 `bpf_prog_realloc()` 扩展程序存储。\n- **程序卸载**：当 BPF 程序引用计数归零时，调用 `__bpf_prog_free()` 释放所有资源。\n- **程序标识与缓存**：`bpf_prog_calc_tag()` 生成的标签用于内核内部缓存（如 verifier log 缓存）和用户空间工具（如 bpftool）识别程序。\n- **经典 BPF 兼容**：网络抓包工具（如 tcpdump）使用经典 BPF 时，内核通过 `bpf_internal_load_pointer_neg_helper()` 安全访问数据包头。",
      "similarity": 0.5952862501144409,
      "chunks": [
        {
          "chunk_id": 9,
          "file_path": "kernel/bpf/core.c",
          "start_line": 2443,
          "end_line": 2555,
          "content": [
            "static void bpf_prog_select_func(struct bpf_prog *fp)",
            "{",
            "#ifndef CONFIG_BPF_JIT_ALWAYS_ON",
            "\tu32 stack_depth = max_t(u32, fp->aux->stack_depth, 1);",
            "\tu32 idx = (round_up(stack_depth, 32) / 32) - 1;",
            "",
            "\t/* may_goto may cause stack size > 512, leading to idx out-of-bounds.",
            "\t * But for non-JITed programs, we don't need bpf_func, so no bounds",
            "\t * check needed.",
            "\t */",
            "\tif (!fp->jit_requested &&",
            "\t    !WARN_ON_ONCE(idx >= ARRAY_SIZE(interpreters))) {",
            "\t\tfp->bpf_func = interpreters[idx];",
            "\t} else {",
            "\t\tfp->bpf_func = __bpf_prog_ret0_warn;",
            "\t}",
            "#else",
            "\tfp->bpf_func = __bpf_prog_ret0_warn;",
            "#endif",
            "}",
            "static unsigned int __bpf_prog_ret1(const void *ctx,",
            "\t\t\t\t    const struct bpf_insn *insn)",
            "{",
            "\treturn 1;",
            "}",
            "void bpf_prog_array_free(struct bpf_prog_array *progs)",
            "{",
            "\tif (!progs || progs == &bpf_empty_prog_array.hdr)",
            "\t\treturn;",
            "\tkfree_rcu(progs, rcu);",
            "}",
            "static void __bpf_prog_array_free_sleepable_cb(struct rcu_head *rcu)",
            "{",
            "\tstruct bpf_prog_array *progs;",
            "",
            "\t/* If RCU Tasks Trace grace period implies RCU grace period, there is",
            "\t * no need to call kfree_rcu(), just call kfree() directly.",
            "\t */",
            "\tprogs = container_of(rcu, struct bpf_prog_array, rcu);",
            "\tif (rcu_trace_implies_rcu_gp())",
            "\t\tkfree(progs);",
            "\telse",
            "\t\tkfree_rcu(progs, rcu);",
            "}",
            "void bpf_prog_array_free_sleepable(struct bpf_prog_array *progs)",
            "{",
            "\tif (!progs || progs == &bpf_empty_prog_array.hdr)",
            "\t\treturn;",
            "\tcall_rcu_tasks_trace(&progs->rcu, __bpf_prog_array_free_sleepable_cb);",
            "}",
            "int bpf_prog_array_length(struct bpf_prog_array *array)",
            "{",
            "\tstruct bpf_prog_array_item *item;",
            "\tu32 cnt = 0;",
            "",
            "\tfor (item = array->items; item->prog; item++)",
            "\t\tif (item->prog != &dummy_bpf_prog.prog)",
            "\t\t\tcnt++;",
            "\treturn cnt;",
            "}",
            "bool bpf_prog_array_is_empty(struct bpf_prog_array *array)",
            "{",
            "\tstruct bpf_prog_array_item *item;",
            "",
            "\tfor (item = array->items; item->prog; item++)",
            "\t\tif (item->prog != &dummy_bpf_prog.prog)",
            "\t\t\treturn false;",
            "\treturn true;",
            "}",
            "static bool bpf_prog_array_copy_core(struct bpf_prog_array *array,",
            "\t\t\t\t     u32 *prog_ids,",
            "\t\t\t\t     u32 request_cnt)",
            "{",
            "\tstruct bpf_prog_array_item *item;",
            "\tint i = 0;",
            "",
            "\tfor (item = array->items; item->prog; item++) {",
            "\t\tif (item->prog == &dummy_bpf_prog.prog)",
            "\t\t\tcontinue;",
            "\t\tprog_ids[i] = item->prog->aux->id;",
            "\t\tif (++i == request_cnt) {",
            "\t\t\titem++;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\treturn !!(item->prog);",
            "}",
            "int bpf_prog_array_copy_to_user(struct bpf_prog_array *array,",
            "\t\t\t\t__u32 __user *prog_ids, u32 cnt)",
            "{",
            "\tunsigned long err = 0;",
            "\tbool nospc;",
            "\tu32 *ids;",
            "",
            "\t/* users of this function are doing:",
            "\t * cnt = bpf_prog_array_length();",
            "\t * if (cnt > 0)",
            "\t *     bpf_prog_array_copy_to_user(..., cnt);",
            "\t * so below kcalloc doesn't need extra cnt > 0 check.",
            "\t */",
            "\tids = kcalloc(cnt, sizeof(u32), GFP_USER | __GFP_NOWARN);",
            "\tif (!ids)",
            "\t\treturn -ENOMEM;",
            "\tnospc = bpf_prog_array_copy_core(array, ids, cnt);",
            "\terr = copy_to_user(prog_ids, ids, cnt * sizeof(u32));",
            "\tkfree(ids);",
            "\tif (err)",
            "\t\treturn -EFAULT;",
            "\tif (nospc)",
            "\t\treturn -ENOSPC;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "bpf_prog_select_func, __bpf_prog_ret1, bpf_prog_array_free, __bpf_prog_array_free_sleepable_cb, bpf_prog_array_free_sleepable, bpf_prog_array_length, bpf_prog_array_is_empty, bpf_prog_array_copy_core, bpf_prog_array_copy_to_user",
          "description": "提供BPF程序选择执行路径、数组操作及用户态数据复制功能，包含RCU安全释放机制和程序ID拷贝接口",
          "similarity": 0.5275813937187195
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/bpf/core.c",
          "start_line": 1434,
          "end_line": 1643,
          "content": [
            "static void bpf_prog_clone_free(struct bpf_prog *fp)",
            "{",
            "\t/* aux was stolen by the other clone, so we cannot free",
            "\t * it from this path! It will be freed eventually by the",
            "\t * other program on release.",
            "\t *",
            "\t * At this point, we don't need a deferred release since",
            "\t * clone is guaranteed to not be locked.",
            "\t */",
            "\tfp->aux = NULL;",
            "\tfp->stats = NULL;",
            "\tfp->active = NULL;",
            "\t__bpf_prog_free(fp);",
            "}",
            "void bpf_jit_prog_release_other(struct bpf_prog *fp, struct bpf_prog *fp_other)",
            "{",
            "\t/* We have to repoint aux->prog to self, as we don't",
            "\t * know whether fp here is the clone or the original.",
            "\t */",
            "\tfp->aux->prog = fp;",
            "\tbpf_prog_clone_free(fp_other);",
            "}",
            "noinline u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)",
            "{",
            "\treturn 0;",
            "}",
            "bool bpf_opcode_in_insntable(u8 code)",
            "{",
            "#define BPF_INSN_2_TBL(x, y)    [BPF_##x | BPF_##y] = true",
            "#define BPF_INSN_3_TBL(x, y, z) [BPF_##x | BPF_##y | BPF_##z] = true",
            "\tstatic const bool public_insntable[256] = {",
            "\t\t[0 ... 255] = false,",
            "\t\t/* Now overwrite non-defaults ... */",
            "\t\tBPF_INSN_MAP(BPF_INSN_2_TBL, BPF_INSN_3_TBL),",
            "\t\t/* UAPI exposed, but rewritten opcodes. cBPF carry-over. */",
            "\t\t[BPF_LD | BPF_ABS | BPF_B] = true,",
            "\t\t[BPF_LD | BPF_ABS | BPF_H] = true,",
            "\t\t[BPF_LD | BPF_ABS | BPF_W] = true,",
            "\t\t[BPF_LD | BPF_IND | BPF_B] = true,",
            "\t\t[BPF_LD | BPF_IND | BPF_H] = true,",
            "\t\t[BPF_LD | BPF_IND | BPF_W] = true,",
            "\t\t[BPF_JMP | BPF_JCOND] = true,",
            "\t};",
            "#undef BPF_INSN_3_TBL",
            "#undef BPF_INSN_2_TBL",
            "\treturn public_insntable[code];",
            "}",
            "static u64 ___bpf_prog_run(u64 *regs, const struct bpf_insn *insn)",
            "{",
            "#define BPF_INSN_2_LBL(x, y)    [BPF_##x | BPF_##y] = &&x##_##y",
            "#define BPF_INSN_3_LBL(x, y, z) [BPF_##x | BPF_##y | BPF_##z] = &&x##_##y##_##z",
            "\tstatic const void * const jumptable[256] __annotate_jump_table = {",
            "\t\t[0 ... 255] = &&default_label,",
            "\t\t/* Now overwrite non-defaults ... */",
            "\t\tBPF_INSN_MAP(BPF_INSN_2_LBL, BPF_INSN_3_LBL),",
            "\t\t/* Non-UAPI available opcodes. */",
            "\t\t[BPF_JMP | BPF_CALL_ARGS] = &&JMP_CALL_ARGS,",
            "\t\t[BPF_JMP | BPF_TAIL_CALL] = &&JMP_TAIL_CALL,",
            "\t\t[BPF_ST  | BPF_NOSPEC] = &&ST_NOSPEC,",
            "\t\t[BPF_LDX | BPF_PROBE_MEM | BPF_B] = &&LDX_PROBE_MEM_B,",
            "\t\t[BPF_LDX | BPF_PROBE_MEM | BPF_H] = &&LDX_PROBE_MEM_H,",
            "\t\t[BPF_LDX | BPF_PROBE_MEM | BPF_W] = &&LDX_PROBE_MEM_W,",
            "\t\t[BPF_LDX | BPF_PROBE_MEM | BPF_DW] = &&LDX_PROBE_MEM_DW,",
            "\t\t[BPF_LDX | BPF_PROBE_MEMSX | BPF_B] = &&LDX_PROBE_MEMSX_B,",
            "\t\t[BPF_LDX | BPF_PROBE_MEMSX | BPF_H] = &&LDX_PROBE_MEMSX_H,",
            "\t\t[BPF_LDX | BPF_PROBE_MEMSX | BPF_W] = &&LDX_PROBE_MEMSX_W,",
            "\t};",
            "#undef BPF_INSN_3_LBL",
            "#undef BPF_INSN_2_LBL",
            "\tu32 tail_call_cnt = 0;",
            "",
            "#define CONT\t ({ insn++; goto select_insn; })",
            "#define CONT_JMP ({ insn++; goto select_insn; })",
            "",
            "select_insn:",
            "\tgoto *jumptable[insn->code];",
            "",
            "\t/* Explicitly mask the register-based shift amounts with 63 or 31",
            "\t * to avoid undefined behavior. Normally this won't affect the",
            "\t * generated code, for example, in case of native 64 bit archs such",
            "\t * as x86-64 or arm64, the compiler is optimizing the AND away for",
            "\t * the interpreter. In case of JITs, each of the JIT backends compiles",
            "\t * the BPF shift operations to machine instructions which produce",
            "\t * implementation-defined results in such a case; the resulting",
            "\t * contents of the register may be arbitrary, but program behaviour",
            "\t * as a whole remains defined. In other words, in case of JIT backends,",
            "\t * the AND must /not/ be added to the emitted LSH/RSH/ARSH translation.",
            "\t */",
            "\t/* ALU (shifts) */",
            "#define SHT(OPCODE, OP)\t\t\t\t\t\\",
            "\tALU64_##OPCODE##_X:\t\t\t\t\\",
            "\t\tDST = DST OP (SRC & 63);\t\t\\",
            "\t\tCONT;\t\t\t\t\t\\",
            "\tALU_##OPCODE##_X:\t\t\t\t\\",
            "\t\tDST = (u32) DST OP ((u32) SRC & 31);\t\\",
            "\t\tCONT;\t\t\t\t\t\\",
            "\tALU64_##OPCODE##_K:\t\t\t\t\\",
            "\t\tDST = DST OP IMM;\t\t\t\\",
            "\t\tCONT;\t\t\t\t\t\\",
            "\tALU_##OPCODE##_K:\t\t\t\t\\",
            "\t\tDST = (u32) DST OP (u32) IMM;\t\t\\",
            "\t\tCONT;",
            "\t/* ALU (rest) */",
            "#define ALU(OPCODE, OP)\t\t\t\t\t\\",
            "\tALU64_##OPCODE##_X:\t\t\t\t\\",
            "\t\tDST = DST OP SRC;\t\t\t\\",
            "\t\tCONT;\t\t\t\t\t\\",
            "\tALU_##OPCODE##_X:\t\t\t\t\\",
            "\t\tDST = (u32) DST OP (u32) SRC;\t\t\\",
            "\t\tCONT;\t\t\t\t\t\\",
            "\tALU64_##OPCODE##_K:\t\t\t\t\\",
            "\t\tDST = DST OP IMM;\t\t\t\\",
            "\t\tCONT;\t\t\t\t\t\\",
            "\tALU_##OPCODE##_K:\t\t\t\t\\",
            "\t\tDST = (u32) DST OP (u32) IMM;\t\t\\",
            "\t\tCONT;",
            "\tALU(ADD,  +)",
            "\tALU(SUB,  -)",
            "\tALU(AND,  &)",
            "\tALU(OR,   |)",
            "\tALU(XOR,  ^)",
            "\tALU(MUL,  *)",
            "\tSHT(LSH, <<)",
            "\tSHT(RSH, >>)",
            "#undef SHT",
            "#undef ALU",
            "\tALU_NEG:",
            "\t\tDST = (u32) -DST;",
            "\t\tCONT;",
            "\tALU64_NEG:",
            "\t\tDST = -DST;",
            "\t\tCONT;",
            "\tALU_MOV_X:",
            "\t\tswitch (OFF) {",
            "\t\tcase 0:",
            "\t\t\tDST = (u32) SRC;",
            "\t\t\tbreak;",
            "\t\tcase 8:",
            "\t\t\tDST = (u32)(s8) SRC;",
            "\t\t\tbreak;",
            "\t\tcase 16:",
            "\t\t\tDST = (u32)(s16) SRC;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tCONT;",
            "\tALU_MOV_K:",
            "\t\tDST = (u32) IMM;",
            "\t\tCONT;",
            "\tALU64_MOV_X:",
            "\t\tswitch (OFF) {",
            "\t\tcase 0:",
            "\t\t\tDST = SRC;",
            "\t\t\tbreak;",
            "\t\tcase 8:",
            "\t\t\tDST = (s8) SRC;",
            "\t\t\tbreak;",
            "\t\tcase 16:",
            "\t\t\tDST = (s16) SRC;",
            "\t\t\tbreak;",
            "\t\tcase 32:",
            "\t\t\tDST = (s32) SRC;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tCONT;",
            "\tALU64_MOV_K:",
            "\t\tDST = IMM;",
            "\t\tCONT;",
            "\tLD_IMM_DW:",
            "\t\tDST = (u64) (u32) insn[0].imm | ((u64) (u32) insn[1].imm) << 32;",
            "\t\tinsn++;",
            "\t\tCONT;",
            "\tALU_ARSH_X:",
            "\t\tDST = (u64) (u32) (((s32) DST) >> (SRC & 31));",
            "\t\tCONT;",
            "\tALU_ARSH_K:",
            "\t\tDST = (u64) (u32) (((s32) DST) >> IMM);",
            "\t\tCONT;",
            "\tALU64_ARSH_X:",
            "\t\t(*(s64 *) &DST) >>= (SRC & 63);",
            "\t\tCONT;",
            "\tALU64_ARSH_K:",
            "\t\t(*(s64 *) &DST) >>= IMM;",
            "\t\tCONT;",
            "\tALU64_MOD_X:",
            "\t\tswitch (OFF) {",
            "\t\tcase 0:",
            "\t\t\tdiv64_u64_rem(DST, SRC, &AX);",
            "\t\t\tDST = AX;",
            "\t\t\tbreak;",
            "\t\tcase 1:",
            "\t\t\tAX = div64_s64(DST, SRC);",
            "\t\t\tDST = DST - AX * SRC;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tCONT;",
            "\tALU_MOD_X:",
            "\t\tswitch (OFF) {",
            "\t\tcase 0:",
            "\t\t\tAX = (u32) DST;",
            "\t\t\tDST = do_div(AX, (u32) SRC);",
            "\t\t\tbreak;",
            "\t\tcase 1:",
            "\t\t\tAX = abs((s32)DST);",
            "\t\t\tAX = do_div(AX, abs((s32)SRC));",
            "\t\t\tif ((s32)DST < 0)",
            "\t\t\t\tDST = (u32)-AX;",
            "\t\t\telse",
            "\t\t\t\tDST = (u32)AX;",
            "\t\t\tbreak;",
            "\t\t}"
          ],
          "function_name": "bpf_prog_clone_free, bpf_jit_prog_release_other, __bpf_call_base, bpf_opcode_in_insntable, ___bpf_prog_run",
          "description": "实现BPF程序克隆释放机制，维护调用基址标识，构建指令跳转表并处理算术运算，包含寄存器移位优化及异常处理逻辑",
          "similarity": 0.4977734684944153
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/bpf/core.c",
          "start_line": 2653,
          "end_line": 2756,
          "content": [
            "void bpf_prog_array_delete_safe(struct bpf_prog_array *array,",
            "\t\t\t\tstruct bpf_prog *old_prog)",
            "{",
            "\tstruct bpf_prog_array_item *item;",
            "",
            "\tfor (item = array->items; item->prog; item++)",
            "\t\tif (item->prog == old_prog) {",
            "\t\t\tWRITE_ONCE(item->prog, &dummy_bpf_prog.prog);",
            "\t\t\tbreak;",
            "\t\t}",
            "}",
            "int bpf_prog_array_delete_safe_at(struct bpf_prog_array *array, int index)",
            "{",
            "\treturn bpf_prog_array_update_at(array, index, &dummy_bpf_prog.prog);",
            "}",
            "int bpf_prog_array_update_at(struct bpf_prog_array *array, int index,",
            "\t\t\t     struct bpf_prog *prog)",
            "{",
            "\tstruct bpf_prog_array_item *item;",
            "",
            "\tif (unlikely(index < 0))",
            "\t\treturn -EINVAL;",
            "",
            "\tfor (item = array->items; item->prog; item++) {",
            "\t\tif (item->prog == &dummy_bpf_prog.prog)",
            "\t\t\tcontinue;",
            "\t\tif (!index) {",
            "\t\t\tWRITE_ONCE(item->prog, prog);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t\tindex--;",
            "\t}",
            "\treturn -ENOENT;",
            "}",
            "int bpf_prog_array_copy(struct bpf_prog_array *old_array,",
            "\t\t\tstruct bpf_prog *exclude_prog,",
            "\t\t\tstruct bpf_prog *include_prog,",
            "\t\t\tu64 bpf_cookie,",
            "\t\t\tstruct bpf_prog_array **new_array)",
            "{",
            "\tint new_prog_cnt, carry_prog_cnt = 0;",
            "\tstruct bpf_prog_array_item *existing, *new;",
            "\tstruct bpf_prog_array *array;",
            "\tbool found_exclude = false;",
            "",
            "\t/* Figure out how many existing progs we need to carry over to",
            "\t * the new array.",
            "\t */",
            "\tif (old_array) {",
            "\t\texisting = old_array->items;",
            "\t\tfor (; existing->prog; existing++) {",
            "\t\t\tif (existing->prog == exclude_prog) {",
            "\t\t\t\tfound_exclude = true;",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\tif (existing->prog != &dummy_bpf_prog.prog)",
            "\t\t\t\tcarry_prog_cnt++;",
            "\t\t\tif (existing->prog == include_prog)",
            "\t\t\t\treturn -EEXIST;",
            "\t\t}",
            "\t}",
            "",
            "\tif (exclude_prog && !found_exclude)",
            "\t\treturn -ENOENT;",
            "",
            "\t/* How many progs (not NULL) will be in the new array? */",
            "\tnew_prog_cnt = carry_prog_cnt;",
            "\tif (include_prog)",
            "\t\tnew_prog_cnt += 1;",
            "",
            "\t/* Do we have any prog (not NULL) in the new array? */",
            "\tif (!new_prog_cnt) {",
            "\t\t*new_array = NULL;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/* +1 as the end of prog_array is marked with NULL */",
            "\tarray = bpf_prog_array_alloc(new_prog_cnt + 1, GFP_KERNEL);",
            "\tif (!array)",
            "\t\treturn -ENOMEM;",
            "\tnew = array->items;",
            "",
            "\t/* Fill in the new prog array */",
            "\tif (carry_prog_cnt) {",
            "\t\texisting = old_array->items;",
            "\t\tfor (; existing->prog; existing++) {",
            "\t\t\tif (existing->prog == exclude_prog ||",
            "\t\t\t    existing->prog == &dummy_bpf_prog.prog)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tnew->prog = existing->prog;",
            "\t\t\tnew->bpf_cookie = existing->bpf_cookie;",
            "\t\t\tnew++;",
            "\t\t}",
            "\t}",
            "\tif (include_prog) {",
            "\t\tnew->prog = include_prog;",
            "\t\tnew->bpf_cookie = bpf_cookie;",
            "\t\tnew++;",
            "\t}",
            "\tnew->prog = NULL;",
            "\t*new_array = array;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "bpf_prog_array_delete_safe, bpf_prog_array_delete_safe_at, bpf_prog_array_update_at, bpf_prog_array_copy",
          "description": "实现对 BPF 程序数组的安全删除和更新操作，通过遍历数组项查找目标程序并替换为占位程序或直接修改指定索引项，支持数组复制时排除特定程序并保留其他条目",
          "similarity": 0.49658679962158203
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/core.c",
          "start_line": 1,
          "end_line": 166,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Linux Socket Filter - Kernel level socket filtering",
            " *",
            " * Based on the design of the Berkeley Packet Filter. The new",
            " * internal format has been designed by PLUMgrid:",
            " *",
            " *\tCopyright (c) 2011 - 2014 PLUMgrid, http://plumgrid.com",
            " *",
            " * Authors:",
            " *",
            " *\tJay Schulist <jschlst@samba.org>",
            " *\tAlexei Starovoitov <ast@plumgrid.com>",
            " *\tDaniel Borkmann <dborkman@redhat.com>",
            " *",
            " * Andi Kleen - Fix a few bad bugs and races.",
            " * Kris Katterjohn - Added many additional checks in bpf_check_classic()",
            " */",
            "",
            "#include <uapi/linux/btf.h>",
            "#include <linux/filter.h>",
            "#include <linux/skbuff.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/random.h>",
            "#include <linux/moduleloader.h>",
            "#include <linux/bpf.h>",
            "#include <linux/btf.h>",
            "#include <linux/objtool.h>",
            "#include <linux/rbtree_latch.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/extable.h>",
            "#include <linux/log2.h>",
            "#include <linux/bpf_verifier.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/nospec.h>",
            "#include <linux/bpf_mem_alloc.h>",
            "#include <linux/memcontrol.h>",
            "",
            "#include <asm/barrier.h>",
            "#include <asm/unaligned.h>",
            "",
            "/* Registers */",
            "#define BPF_R0\tregs[BPF_REG_0]",
            "#define BPF_R1\tregs[BPF_REG_1]",
            "#define BPF_R2\tregs[BPF_REG_2]",
            "#define BPF_R3\tregs[BPF_REG_3]",
            "#define BPF_R4\tregs[BPF_REG_4]",
            "#define BPF_R5\tregs[BPF_REG_5]",
            "#define BPF_R6\tregs[BPF_REG_6]",
            "#define BPF_R7\tregs[BPF_REG_7]",
            "#define BPF_R8\tregs[BPF_REG_8]",
            "#define BPF_R9\tregs[BPF_REG_9]",
            "#define BPF_R10\tregs[BPF_REG_10]",
            "",
            "/* Named registers */",
            "#define DST\tregs[insn->dst_reg]",
            "#define SRC\tregs[insn->src_reg]",
            "#define FP\tregs[BPF_REG_FP]",
            "#define AX\tregs[BPF_REG_AX]",
            "#define ARG1\tregs[BPF_REG_ARG1]",
            "#define CTX\tregs[BPF_REG_CTX]",
            "#define OFF\tinsn->off",
            "#define IMM\tinsn->imm",
            "",
            "struct bpf_mem_alloc bpf_global_ma;",
            "bool bpf_global_ma_set;",
            "",
            "/* No hurry in this branch",
            " *",
            " * Exported for the bpf jit load helper.",
            " */",
            "void *bpf_internal_load_pointer_neg_helper(const struct sk_buff *skb, int k, unsigned int size)",
            "{",
            "\tu8 *ptr = NULL;",
            "",
            "\tif (k >= SKF_NET_OFF) {",
            "\t\tptr = skb_network_header(skb) + k - SKF_NET_OFF;",
            "\t} else if (k >= SKF_LL_OFF) {",
            "\t\tif (unlikely(!skb_mac_header_was_set(skb)))",
            "\t\t\treturn NULL;",
            "\t\tptr = skb_mac_header(skb) + k - SKF_LL_OFF;",
            "\t}",
            "\tif (ptr >= skb->head && ptr + size <= skb_tail_pointer(skb))",
            "\t\treturn ptr;",
            "",
            "\treturn NULL;",
            "}",
            "",
            "/* tell bpf programs that include vmlinux.h kernel's PAGE_SIZE */",
            "enum page_size_enum {",
            "\t__PAGE_SIZE = PAGE_SIZE",
            "};",
            "",
            "struct bpf_prog *bpf_prog_alloc_no_stats(unsigned int size, gfp_t gfp_extra_flags)",
            "{",
            "\tgfp_t gfp_flags = bpf_memcg_flags(GFP_KERNEL | __GFP_ZERO | gfp_extra_flags);",
            "\tstruct bpf_prog_aux *aux;",
            "\tstruct bpf_prog *fp;",
            "",
            "\tsize = round_up(size, __PAGE_SIZE);",
            "\tfp = __vmalloc(size, gfp_flags);",
            "\tif (fp == NULL)",
            "\t\treturn NULL;",
            "",
            "\taux = kzalloc(sizeof(*aux), bpf_memcg_flags(GFP_KERNEL | gfp_extra_flags));",
            "\tif (aux == NULL) {",
            "\t\tvfree(fp);",
            "\t\treturn NULL;",
            "\t}",
            "\tfp->active = alloc_percpu_gfp(int, bpf_memcg_flags(GFP_KERNEL | gfp_extra_flags));",
            "\tif (!fp->active) {",
            "\t\tvfree(fp);",
            "\t\tkfree(aux);",
            "\t\treturn NULL;",
            "\t}",
            "",
            "\tfp->pages = size / PAGE_SIZE;",
            "\tfp->aux = aux;",
            "\tfp->aux->prog = fp;",
            "\tfp->jit_requested = ebpf_jit_enabled();",
            "\tfp->blinding_requested = bpf_jit_blinding_enabled(fp);",
            "#ifdef CONFIG_CGROUP_BPF",
            "\taux->cgroup_atype = CGROUP_BPF_ATTACH_TYPE_INVALID;",
            "#endif",
            "",
            "\tINIT_LIST_HEAD_RCU(&fp->aux->ksym.lnode);",
            "#ifdef CONFIG_FINEIBT",
            "\tINIT_LIST_HEAD_RCU(&fp->aux->ksym_prefix.lnode);",
            "#endif",
            "\tmutex_init(&fp->aux->used_maps_mutex);",
            "\tmutex_init(&fp->aux->ext_mutex);",
            "\tmutex_init(&fp->aux->dst_mutex);",
            "",
            "\treturn fp;",
            "}",
            "",
            "struct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags)",
            "{",
            "\tgfp_t gfp_flags = bpf_memcg_flags(GFP_KERNEL | __GFP_ZERO | gfp_extra_flags);",
            "\tstruct bpf_prog *prog;",
            "\tint cpu;",
            "",
            "\tprog = bpf_prog_alloc_no_stats(size, gfp_extra_flags);",
            "\tif (!prog)",
            "\t\treturn NULL;",
            "",
            "\tprog->stats = alloc_percpu_gfp(struct bpf_prog_stats, gfp_flags);",
            "\tif (!prog->stats) {",
            "\t\tfree_percpu(prog->active);",
            "\t\tkfree(prog->aux);",
            "\t\tvfree(prog);",
            "\t\treturn NULL;",
            "\t}",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct bpf_prog_stats *pstats;",
            "",
            "\t\tpstats = per_cpu_ptr(prog->stats, cpu);",
            "\t\tu64_stats_init(&pstats->syncp);",
            "\t}",
            "\treturn prog;",
            "}",
            "EXPORT_SYMBOL_GPL(bpf_prog_alloc);",
            ""
          ],
          "function_name": null,
          "description": "定义BPF核心功能和宏，提供BPF程序分配函数bpf_prog_alloc及其辅助结构初始化，包含内存分配、锁初始化及辅助数据结构设置。",
          "similarity": 0.4962555468082428
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/bpf/core.c",
          "start_line": 2791,
          "end_line": 2899,
          "content": [
            "int bpf_prog_array_copy_info(struct bpf_prog_array *array,",
            "\t\t\t     u32 *prog_ids, u32 request_cnt,",
            "\t\t\t     u32 *prog_cnt)",
            "{",
            "\tu32 cnt = 0;",
            "",
            "\tif (array)",
            "\t\tcnt = bpf_prog_array_length(array);",
            "",
            "\t*prog_cnt = cnt;",
            "",
            "\t/* return early if user requested only program count or nothing to copy */",
            "\tif (!request_cnt || !cnt)",
            "\t\treturn 0;",
            "",
            "\t/* this function is called under trace/bpf_trace.c: bpf_event_mutex */",
            "\treturn bpf_prog_array_copy_core(array, prog_ids, request_cnt) ? -ENOSPC",
            "\t\t\t\t\t\t\t\t     : 0;",
            "}",
            "void __bpf_free_used_maps(struct bpf_prog_aux *aux,",
            "\t\t\t  struct bpf_map **used_maps, u32 len)",
            "{",
            "\tstruct bpf_map *map;",
            "\tbool sleepable;",
            "\tu32 i;",
            "",
            "\tsleepable = aux->prog->sleepable;",
            "\tfor (i = 0; i < len; i++) {",
            "\t\tmap = used_maps[i];",
            "\t\tif (map->ops->map_poke_untrack)",
            "\t\t\tmap->ops->map_poke_untrack(map, aux);",
            "\t\tif (sleepable)",
            "\t\t\tatomic64_dec(&map->sleepable_refcnt);",
            "\t\tbpf_map_put(map);",
            "\t}",
            "}",
            "static void bpf_free_used_maps(struct bpf_prog_aux *aux)",
            "{",
            "\t__bpf_free_used_maps(aux, aux->used_maps, aux->used_map_cnt);",
            "\tkfree(aux->used_maps);",
            "}",
            "void __bpf_free_used_btfs(struct bpf_prog_aux *aux,",
            "\t\t\t  struct btf_mod_pair *used_btfs, u32 len)",
            "{",
            "#ifdef CONFIG_BPF_SYSCALL",
            "\tstruct btf_mod_pair *btf_mod;",
            "\tu32 i;",
            "",
            "\tfor (i = 0; i < len; i++) {",
            "\t\tbtf_mod = &used_btfs[i];",
            "\t\tif (btf_mod->module)",
            "\t\t\tmodule_put(btf_mod->module);",
            "\t\tbtf_put(btf_mod->btf);",
            "\t}",
            "#endif",
            "}",
            "static void bpf_free_used_btfs(struct bpf_prog_aux *aux)",
            "{",
            "\t__bpf_free_used_btfs(aux, aux->used_btfs, aux->used_btf_cnt);",
            "\tkfree(aux->used_btfs);",
            "}",
            "static void bpf_prog_free_deferred(struct work_struct *work)",
            "{",
            "\tstruct bpf_prog_aux *aux;",
            "\tint i;",
            "",
            "\taux = container_of(work, struct bpf_prog_aux, work);",
            "#ifdef CONFIG_BPF_SYSCALL",
            "\tbpf_free_kfunc_btf_tab(aux->kfunc_btf_tab);",
            "#endif",
            "#ifdef CONFIG_CGROUP_BPF",
            "\tif (aux->cgroup_atype != CGROUP_BPF_ATTACH_TYPE_INVALID)",
            "\t\tbpf_cgroup_atype_put(aux->cgroup_atype);",
            "#endif",
            "\tbpf_free_used_maps(aux);",
            "\tbpf_free_used_btfs(aux);",
            "\tif (bpf_prog_is_dev_bound(aux))",
            "\t\tbpf_prog_dev_bound_destroy(aux->prog);",
            "#ifdef CONFIG_PERF_EVENTS",
            "\tif (aux->prog->has_callchain_buf)",
            "\t\tput_callchain_buffers();",
            "#endif",
            "\tif (aux->dst_trampoline)",
            "\t\tbpf_trampoline_put(aux->dst_trampoline);",
            "\tfor (i = 0; i < aux->real_func_cnt; i++) {",
            "\t\t/* We can just unlink the subprog poke descriptor table as",
            "\t\t * it was originally linked to the main program and is also",
            "\t\t * released along with it.",
            "\t\t */",
            "\t\taux->func[i]->aux->poke_tab = NULL;",
            "\t\tbpf_jit_free(aux->func[i]);",
            "\t}",
            "\tif (aux->real_func_cnt) {",
            "\t\tkfree(aux->func);",
            "\t\tbpf_prog_unlock_free(aux->prog);",
            "\t} else {",
            "\t\tbpf_jit_free(aux->prog);",
            "\t}",
            "}",
            "void bpf_prog_free(struct bpf_prog *fp)",
            "{",
            "\tstruct bpf_prog_aux *aux = fp->aux;",
            "",
            "\tif (aux->dst_prog)",
            "\t\tbpf_prog_put(aux->dst_prog);",
            "\tbpf_token_put(aux->token);",
            "\tINIT_WORK(&aux->work, bpf_prog_free_deferred);",
            "\tschedule_work(&aux->work);",
            "}"
          ],
          "function_name": "bpf_prog_array_copy_info, __bpf_free_used_maps, bpf_free_used_maps, __bpf_free_used_btfs, bpf_free_used_btfs, bpf_prog_free_deferred, bpf_prog_free",
          "description": "提供 BPF 程序辅助资源释放逻辑，包括释放关联 maps、BTFS 类型引用，处理延迟释放工作队列，以及清理 JIT 编译产生的子程序和堆栈相关资源",
          "similarity": 0.49562302231788635
        }
      ]
    }
  ]
}