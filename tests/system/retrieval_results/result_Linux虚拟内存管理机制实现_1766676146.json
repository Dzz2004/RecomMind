{
  "query": "Linux虚拟内存管理机制实现",
  "timestamp": "2025-12-25 23:22:26",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.6309643983840942,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.6330062747001648
        },
        {
          "chunk_id": 11,
          "file_path": "mm/mempolicy.c",
          "start_line": 1855,
          "end_line": 1971,
          "content": [
            "static int kernel_get_mempolicy(int __user *policy,",
            "\t\t\t\tunsigned long __user *nmask,",
            "\t\t\t\tunsigned long maxnode,",
            "\t\t\t\tunsigned long addr,",
            "\t\t\t\tunsigned long flags)",
            "{",
            "\tint err;",
            "\tint pval;",
            "\tnodemask_t nodes;",
            "",
            "\tif (nmask != NULL && maxnode < nr_node_ids)",
            "\t\treturn -EINVAL;",
            "",
            "\taddr = untagged_addr(addr);",
            "",
            "\terr = do_get_mempolicy(&pval, &nodes, addr, flags);",
            "",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (policy && put_user(pval, policy))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (nmask)",
            "\t\terr = copy_nodes_to_user(nmask, maxnode, &nodes);",
            "",
            "\treturn err;",
            "}",
            "bool vma_migratable(struct vm_area_struct *vma)",
            "{",
            "\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * DAX device mappings require predictable access latency, so avoid",
            "\t * incurring periodic faults.",
            "\t */",
            "\tif (vma_is_dax(vma))",
            "\t\treturn false;",
            "",
            "\tif (is_vm_hugetlb_page(vma) &&",
            "\t\t!hugepage_migration_supported(hstate_vma(vma)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Migration allocates pages in the highest zone. If we cannot",
            "\t * do so then migration (at least from node to node) is not",
            "\t * possible.",
            "\t */",
            "\tif (vma->vm_file &&",
            "\t\tgfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))",
            "\t\t\t< policy_zone)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "bool vma_policy_mof(struct vm_area_struct *vma)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->get_policy) {",
            "\t\tbool ret = false;",
            "\t\tpgoff_t ilx;\t\t/* ignored here */",
            "",
            "\t\tpol = vma->vm_ops->get_policy(vma, vma->vm_start, &ilx);",
            "\t\tif (pol && (pol->flags & MPOL_F_MOF))",
            "\t\t\tret = true;",
            "\t\tmpol_cond_put(pol);",
            "",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tpol = vma->vm_policy;",
            "\tif (!pol)",
            "\t\tpol = get_task_policy(current);",
            "",
            "\treturn pol->flags & MPOL_F_MOF;",
            "}",
            "bool apply_policy_zone(struct mempolicy *policy, enum zone_type zone)",
            "{",
            "\tenum zone_type dynamic_policy_zone = policy_zone;",
            "",
            "\tBUG_ON(dynamic_policy_zone == ZONE_MOVABLE);",
            "",
            "\t/*",
            "\t * if policy->nodes has movable memory only,",
            "\t * we apply policy when gfp_zone(gfp) = ZONE_MOVABLE only.",
            "\t *",
            "\t * policy->nodes is intersect with node_states[N_MEMORY].",
            "\t * so if the following test fails, it implies",
            "\t * policy->nodes has movable memory only.",
            "\t */",
            "\tif (!nodes_intersects(policy->nodes, node_states[N_HIGH_MEMORY]))",
            "\t\tdynamic_policy_zone = ZONE_MOVABLE;",
            "",
            "\treturn zone >= dynamic_policy_zone;",
            "}",
            "static unsigned int weighted_interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int node;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "retry:",
            "\t/* to prevent miscount use tsk->mems_allowed_seq to detect rebind */",
            "\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\tnode = current->il_prev;",
            "\tif (!current->il_weight || !node_isset(node, policy->nodes)) {",
            "\t\tnode = next_node_in(node, policy->nodes);",
            "\t\tif (read_mems_allowed_retry(cpuset_mems_cookie))",
            "\t\t\tgoto retry;",
            "\t\tif (node == MAX_NUMNODES)",
            "\t\t\treturn node;",
            "\t\tcurrent->il_prev = node;",
            "\t\tcurrent->il_weight = get_il_weight(node);",
            "\t}",
            "\tcurrent->il_weight--;",
            "\treturn node;",
            "}"
          ],
          "function_name": "kernel_get_mempolicy, vma_migratable, vma_policy_mof, apply_policy_zone, weighted_interleave_nodes",
          "description": "kernel_get_mempolicy 获取当前内存策略参数并复制到用户空间；vma_migratable 判断虚拟内存区域是否支持迁移；vma_policy_mof 检查VMA是否启用了MOF（Migration On Fault）策略；apply_policy_zone 确定当前zone是否满足策略要求；weighted_interleave_nodes 计算加权交错分配的目标节点。",
          "similarity": 0.6307358741760254
        },
        {
          "chunk_id": 6,
          "file_path": "mm/mempolicy.c",
          "start_line": 1012,
          "end_line": 1144,
          "content": [
            "static void get_policy_nodemask(struct mempolicy *pol, nodemask_t *nodes)",
            "{",
            "\tnodes_clear(*nodes);",
            "\tif (pol == &default_policy)",
            "\t\treturn;",
            "",
            "\tswitch (pol->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*nodes = pol->nodes;",
            "\t\tbreak;",
            "\tcase MPOL_LOCAL:",
            "\t\t/* return empty node mask for local allocation */",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static int lookup_node(struct mm_struct *mm, unsigned long addr)",
            "{",
            "\tstruct page *p = NULL;",
            "\tint ret;",
            "",
            "\tret = get_user_pages_fast(addr & PAGE_MASK, 1, 0, &p);",
            "\tif (ret > 0) {",
            "\t\tret = page_to_nid(p);",
            "\t\tput_page(p);",
            "\t}",
            "\treturn ret;",
            "}",
            "static long do_get_mempolicy(int *policy, nodemask_t *nmask,",
            "\t\t\t     unsigned long addr, unsigned long flags)",
            "{",
            "\tint err;",
            "\tstruct mm_struct *mm = current->mm;",
            "\tstruct vm_area_struct *vma = NULL;",
            "\tstruct mempolicy *pol = current->mempolicy, *pol_refcount = NULL;",
            "",
            "\tif (flags &",
            "\t\t~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (flags & MPOL_F_MEMS_ALLOWED) {",
            "\t\tif (flags & (MPOL_F_NODE|MPOL_F_ADDR))",
            "\t\t\treturn -EINVAL;",
            "\t\t*policy = 0;\t/* just so it's initialized */",
            "\t\ttask_lock(current);",
            "\t\t*nmask  = cpuset_current_mems_allowed;",
            "\t\ttask_unlock(current);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (flags & MPOL_F_ADDR) {",
            "\t\tpgoff_t ilx;\t\t/* ignored here */",
            "\t\t/*",
            "\t\t * Do NOT fall back to task policy if the",
            "\t\t * vma/shared policy at addr is NULL.  We",
            "\t\t * want to return MPOL_DEFAULT in this case.",
            "\t\t */",
            "\t\tmmap_read_lock(mm);",
            "\t\tvma = vma_lookup(mm, addr);",
            "\t\tif (!vma) {",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\treturn -EFAULT;",
            "\t\t}",
            "\t\tpol = __get_vma_policy(vma, addr, &ilx);",
            "\t} else if (addr)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!pol)",
            "\t\tpol = &default_policy;\t/* indicates default behavior */",
            "",
            "\tif (flags & MPOL_F_NODE) {",
            "\t\tif (flags & MPOL_F_ADDR) {",
            "\t\t\t/*",
            "\t\t\t * Take a refcount on the mpol, because we are about to",
            "\t\t\t * drop the mmap_lock, after which only \"pol\" remains",
            "\t\t\t * valid, \"vma\" is stale.",
            "\t\t\t */",
            "\t\t\tpol_refcount = pol;",
            "\t\t\tvma = NULL;",
            "\t\t\tmpol_get(pol);",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\terr = lookup_node(mm, addr);",
            "\t\t\tif (err < 0)",
            "\t\t\t\tgoto out;",
            "\t\t\t*policy = err;",
            "\t\t} else if (pol == current->mempolicy &&",
            "\t\t\t\tpol->mode == MPOL_INTERLEAVE) {",
            "\t\t\t*policy = next_node_in(current->il_prev, pol->nodes);",
            "\t\t} else if (pol == current->mempolicy &&",
            "\t\t\t\tpol->mode == MPOL_WEIGHTED_INTERLEAVE) {",
            "\t\t\tif (current->il_weight)",
            "\t\t\t\t*policy = current->il_prev;",
            "\t\t\telse",
            "\t\t\t\t*policy = next_node_in(current->il_prev,",
            "\t\t\t\t\t\t       pol->nodes);",
            "\t\t} else {",
            "\t\t\terr = -EINVAL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t} else {",
            "\t\t*policy = pol == &default_policy ? MPOL_DEFAULT :",
            "\t\t\t\t\t\tpol->mode;",
            "\t\t/*",
            "\t\t * Internal mempolicy flags must be masked off before exposing",
            "\t\t * the policy to userspace.",
            "\t\t */",
            "\t\t*policy |= (pol->flags & MPOL_MODE_FLAGS);",
            "\t}",
            "",
            "\terr = 0;",
            "\tif (nmask) {",
            "\t\tif (mpol_store_user_nodemask(pol)) {",
            "\t\t\t*nmask = pol->w.user_nodemask;",
            "\t\t} else {",
            "\t\t\ttask_lock(current);",
            "\t\t\tget_policy_nodemask(pol, nmask);",
            "\t\t\ttask_unlock(current);",
            "\t\t}",
            "\t}",
            "",
            " out:",
            "\tmpol_cond_put(pol);",
            "\tif (vma)",
            "\t\tmmap_read_unlock(mm);",
            "\tif (pol_refcount)",
            "\t\tmpol_put(pol_refcount);",
            "\treturn err;",
            "}"
          ],
          "function_name": "get_policy_nodemask, lookup_node, do_get_mempolicy",
          "description": "获取内存策略节点掩码，lookup_node查询页面所属节点，do_get_mempolicy根据地址或标志获取当前/VA内存策略并返回节点掩码",
          "similarity": 0.5838149189949036
        },
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.5775474309921265
        },
        {
          "chunk_id": 10,
          "file_path": "mm/mempolicy.c",
          "start_line": 1735,
          "end_line": 1838,
          "content": [
            "static long kernel_set_mempolicy(int mode, const unsigned long __user *nmask,",
            "\t\t\t\t unsigned long maxnode)",
            "{",
            "\tunsigned short mode_flags;",
            "\tnodemask_t nodes;",
            "\tint lmode = mode;",
            "\tint err;",
            "",
            "\terr = sanitize_mpol_flags(&lmode, &mode_flags);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\terr = get_nodes(&nodes, nmask, maxnode);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\treturn do_set_mempolicy(lmode, mode_flags, &nodes);",
            "}",
            "static int kernel_migrate_pages(pid_t pid, unsigned long maxnode,",
            "\t\t\t\tconst unsigned long __user *old_nodes,",
            "\t\t\t\tconst unsigned long __user *new_nodes)",
            "{",
            "\tstruct mm_struct *mm = NULL;",
            "\tstruct task_struct *task;",
            "\tnodemask_t task_nodes;",
            "\tint err;",
            "\tnodemask_t *old;",
            "\tnodemask_t *new;",
            "\tNODEMASK_SCRATCH(scratch);",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\told = &scratch->mask1;",
            "\tnew = &scratch->mask2;",
            "",
            "\terr = get_nodes(old, old_nodes, maxnode);",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\terr = get_nodes(new, new_nodes, maxnode);",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\t/* Find the mm_struct */",
            "\trcu_read_lock();",
            "\ttask = pid ? find_task_by_vpid(pid) : current;",
            "\tif (!task) {",
            "\t\trcu_read_unlock();",
            "\t\terr = -ESRCH;",
            "\t\tgoto out;",
            "\t}",
            "\tget_task_struct(task);",
            "",
            "\terr = -EINVAL;",
            "",
            "\t/*",
            "\t * Check if this process has the right to modify the specified process.",
            "\t * Use the regular \"ptrace_may_access()\" checks.",
            "\t */",
            "\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {",
            "\t\trcu_read_unlock();",
            "\t\terr = -EPERM;",
            "\t\tgoto out_put;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\ttask_nodes = cpuset_mems_allowed(task);",
            "\t/* Is the user allowed to access the target nodes? */",
            "\tif (!nodes_subset(*new, task_nodes) && !capable(CAP_SYS_NICE)) {",
            "\t\terr = -EPERM;",
            "\t\tgoto out_put;",
            "\t}",
            "",
            "\ttask_nodes = cpuset_mems_allowed(current);",
            "\tnodes_and(*new, *new, task_nodes);",
            "\tif (nodes_empty(*new))",
            "\t\tgoto out_put;",
            "",
            "\terr = security_task_movememory(task);",
            "\tif (err)",
            "\t\tgoto out_put;",
            "",
            "\tmm = get_task_mm(task);",
            "\tput_task_struct(task);",
            "",
            "\tif (!mm) {",
            "\t\terr = -EINVAL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\terr = do_migrate_pages(mm, old, new,",
            "\t\tcapable(CAP_SYS_NICE) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE);",
            "",
            "\tmmput(mm);",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "",
            "\treturn err;",
            "",
            "out_put:",
            "\tput_task_struct(task);",
            "\tgoto out;",
            "}"
          ],
          "function_name": "kernel_set_mempolicy, kernel_migrate_pages",
          "description": "kernel_set_mempolicy 设置进程的内存放置策略，通过sanitize_mpol_flags验证模式标志并调用do_set_mempolicy应用策略；kernel_migrate_pages 实现页面迁移，检查目标进程权限，限制迁移节点范围，并调用do_migrate_pages进行实际迁移操作。",
          "similarity": 0.5676689147949219
        }
      ]
    },
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.6290985941886902,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.6483751535415649
        },
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.6295607686042786
        },
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.5423334836959839
        }
      ]
    },
    {
      "source_file": "mm/vmpressure.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:33:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmpressure.c`\n\n---\n\n# vmpressure.c 技术文档\n\n## 1. 文件概述\n\n`vmpressure.c` 实现了 Linux 内核中的虚拟内存压力（VM pressure）监控机制。该机制通过跟踪页面扫描（scanned）与回收（reclaimed）的比率，评估系统或特定内存控制组（memcg）所面临的内存压力程度，并在达到预设阈值时向用户空间发送通知。此功能主要用于支持 cgroup v2 的 memory.pressure 接口，使用户空间程序（如容器运行时）能够感知内存紧张状况并作出响应（如释放缓存、限制内存使用等），从而避免系统进入 OOM（Out-Of-Memory）状态。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct vmpressure`**  \n  表示一个内存控制组的 VM 压力状态，包含：\n  - `tree_scanned` / `tree_reclaimed`：累积的扫描和回收页数（用于子树模式）\n  - `sr_lock`：保护上述计数器的自旋锁\n  - `events_lock`：保护事件监听列表的互斥锁\n  - `events`：注册的事件监听器链表（`struct vmpressure_event`）\n  - `work`：延迟处理工作项（`struct work_struct`）\n\n- **`struct vmpressure_event`**  \n  表示一个用户空间注册的压力事件监听器，包含：\n  - `efd`：关联的 eventfd 上下文，用于通知\n  - `level`：触发通知的最低压力等级（low/medium/critical）\n  - `mode`：通知模式（default/hierarchy/local）\n  - `node`：链表节点\n\n### 主要函数\n\n- **`vmpressure()`**  \n  核心接口函数，由内存回收路径（vmscan）调用，传入当前扫描和回收的页数，更新压力统计并可能调度异步处理。\n\n- **`vmpressure_work_fn()`**  \n  工作队列回调函数，负责计算压力等级、触发事件通知，并向上遍历内存控制组层级（支持层次化通知）。\n\n- **`vmpressure_calc_level()`**  \n  根据 `scanned` 和 `reclaimed` 计算压力百分比，并映射到离散的压力等级（low/medium/critical）。\n\n- **`vmpressure_event()`**  \n  遍历当前 memcg 注册的所有事件监听器，根据压力等级、通知模式和层级关系决定是否触发 eventfd 信号。\n\n- **辅助函数**  \n  - `vmpressure_parent()`：获取父级 memcg 对应的 `vmpressure` 结构\n  - `vmpressure_level()`：将压力百分比映射为枚举等级\n  - `work_to_vmpressure()`：从 work_struct 转换为 vmpressure 指针\n\n### 关键常量\n\n- **`vmpressure_win`**：压力计算窗口大小（512 页），用于速率限制和平均\n- **`vmpressure_level_med`**（60）和 **`vmpressure_level_critical`**（95）：中等和严重压力的百分比阈值\n- **`vmpressure_level_critical_prio`**：基于扫描优先级判断严重压力的备用机制\n\n## 3. 关键实现\n\n### 压力等级计算\n压力通过公式 `pressure = (scanned - reclaimed) * 100 / scanned` 计算（实际实现考虑了 `reclaimed > scanned` 的边界情况）。该值反映回收效率：值越高表示回收越困难，内存压力越大。\n\n### 异步处理机制\n`vmpressure()` 函数仅累加计数器并调度 `vmpressure_work_fn` 工作项，避免在内存回收关键路径上执行复杂逻辑。工作项在后台执行压力计算和通知。\n\n### 层级化通知（Hierarchy）\n支持三种通知模式：\n- **`local`**：仅当前 memcg 触发\n- **`hierarchy`**：当前及所有祖先 memcg 均可触发\n- **`default`（no passthrough）**：当前 memcg 触发后，阻止向祖先传递（避免重复通知）\n\n### 窗口与速率限制\n使用固定窗口（`vmpressure_win`）累积扫描/回收页数，确保压力评估具有时间局部性，同时防止高频通知。\n\n### 与 vmscan 集成\n直接接收 vmscan 传递的 `scanned` 和 `reclaimed` 参数，紧密耦合内存回收行为，提供实时压力反馈。\n\n## 4. 依赖关系\n\n- **内存控制组（memcg）**：通过 `mem_cgroup` 结构关联 `vmpressure` 实例，依赖 cgroup 子系统（`memory_cgrp_subsys`）\n- **内存管理核心（mm）**：依赖 `vmscan` 回收路径调用 `vmpressure()`，使用 `SWAP_CLUSTER_MAX` 常量\n- **事件通知机制**：使用 `eventfd` 向用户空间发送信号\n- **内核同步原语**：使用 `spinlock`（`sr_lock`）和 `mutex`（`events_lock`）保护数据\n- **通用内核组件**：依赖 `workqueue`（延迟处理）、`slab`（内存分配）、`printk`（调试）\n\n## 5. 使用场景\n\n1. **容器内存管理**  \n   容器运行时（如 Docker、systemd-nspawn）通过监听 cgroup v2 的 `memory.pressure` 文件，在内存压力升高时主动释放缓存或限制应用内存使用，避免被 OOM killer 终止。\n\n2. **系统级内存优化**  \n   用户空间守护进程（如 earlyoom、nohang）利用压力事件提前干预，例如在 `critical` 压力下终止低优先级进程。\n\n3. **内核子系统集成**  \n   其他内核模块可通过注册 `vmpressure` 事件监听器，在内存紧张时调整自身行为（如降低缓存占用）。\n\n4. **传统 cgroup v1 支持**  \n   通过 `tree` 参数兼容旧版 subtree 压力报告模式（尽管主要面向 cgroup v2 设计）。",
      "similarity": 0.6272993087768555,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/vmpressure.c",
          "start_line": 111,
          "end_line": 290,
          "content": [
            "static enum vmpressure_levels vmpressure_level(unsigned long pressure)",
            "{",
            "\tif (pressure >= vmpressure_level_critical)",
            "\t\treturn VMPRESSURE_CRITICAL;",
            "\telse if (pressure >= vmpressure_level_med)",
            "\t\treturn VMPRESSURE_MEDIUM;",
            "\treturn VMPRESSURE_LOW;",
            "}",
            "static enum vmpressure_levels vmpressure_calc_level(unsigned long scanned,",
            "\t\t\t\t\t\t    unsigned long reclaimed)",
            "{",
            "\tunsigned long scale = scanned + reclaimed;",
            "\tunsigned long pressure = 0;",
            "",
            "\t/*",
            "\t * reclaimed can be greater than scanned for things such as reclaimed",
            "\t * slab pages. shrink_node() just adds reclaimed pages without a",
            "\t * related increment to scanned pages.",
            "\t */",
            "\tif (reclaimed >= scanned)",
            "\t\tgoto out;",
            "\t/*",
            "\t * We calculate the ratio (in percents) of how many pages were",
            "\t * scanned vs. reclaimed in a given time frame (window). Note that",
            "\t * time is in VM reclaimer's \"ticks\", i.e. number of pages",
            "\t * scanned. This makes it possible to set desired reaction time",
            "\t * and serves as a ratelimit.",
            "\t */",
            "\tpressure = scale - (reclaimed * scale / scanned);",
            "\tpressure = pressure * 100 / scale;",
            "",
            "out:",
            "\tpr_debug(\"%s: %3lu  (s: %lu  r: %lu)\\n\", __func__, pressure,",
            "\t\t scanned, reclaimed);",
            "",
            "\treturn vmpressure_level(pressure);",
            "}",
            "static bool vmpressure_event(struct vmpressure *vmpr,",
            "\t\t\t     const enum vmpressure_levels level,",
            "\t\t\t     bool ancestor, bool signalled)",
            "{",
            "\tstruct vmpressure_event *ev;",
            "\tbool ret = false;",
            "",
            "\tmutex_lock(&vmpr->events_lock);",
            "\tlist_for_each_entry(ev, &vmpr->events, node) {",
            "\t\tif (ancestor && ev->mode == VMPRESSURE_LOCAL)",
            "\t\t\tcontinue;",
            "\t\tif (signalled && ev->mode == VMPRESSURE_NO_PASSTHROUGH)",
            "\t\t\tcontinue;",
            "\t\tif (level < ev->level)",
            "\t\t\tcontinue;",
            "\t\teventfd_signal(ev->efd);",
            "\t\tret = true;",
            "\t}",
            "\tmutex_unlock(&vmpr->events_lock);",
            "",
            "\treturn ret;",
            "}",
            "static void vmpressure_work_fn(struct work_struct *work)",
            "{",
            "\tstruct vmpressure *vmpr = work_to_vmpressure(work);",
            "\tunsigned long scanned;",
            "\tunsigned long reclaimed;",
            "\tenum vmpressure_levels level;",
            "\tbool ancestor = false;",
            "\tbool signalled = false;",
            "",
            "\tspin_lock(&vmpr->sr_lock);",
            "\t/*",
            "\t * Several contexts might be calling vmpressure(), so it is",
            "\t * possible that the work was rescheduled again before the old",
            "\t * work context cleared the counters. In that case we will run",
            "\t * just after the old work returns, but then scanned might be zero",
            "\t * here. No need for any locks here since we don't care if",
            "\t * vmpr->reclaimed is in sync.",
            "\t */",
            "\tscanned = vmpr->tree_scanned;",
            "\tif (!scanned) {",
            "\t\tspin_unlock(&vmpr->sr_lock);",
            "\t\treturn;",
            "\t}",
            "",
            "\treclaimed = vmpr->tree_reclaimed;",
            "\tvmpr->tree_scanned = 0;",
            "\tvmpr->tree_reclaimed = 0;",
            "\tspin_unlock(&vmpr->sr_lock);",
            "",
            "\tlevel = vmpressure_calc_level(scanned, reclaimed);",
            "",
            "\tdo {",
            "\t\tif (vmpressure_event(vmpr, level, ancestor, signalled))",
            "\t\t\tsignalled = true;",
            "\t\tancestor = true;",
            "\t} while ((vmpr = vmpressure_parent(vmpr)));",
            "}",
            "void vmpressure(gfp_t gfp, struct mem_cgroup *memcg, bool tree,",
            "\t\tunsigned long scanned, unsigned long reclaimed)",
            "{",
            "\tstruct vmpressure *vmpr;",
            "",
            "\tif (mem_cgroup_disabled())",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * The in-kernel users only care about the reclaim efficiency",
            "\t * for this @memcg rather than the whole subtree, and there",
            "\t * isn't and won't be any in-kernel user in a legacy cgroup.",
            "\t */",
            "\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys) && !tree)",
            "\t\treturn;",
            "",
            "\tvmpr = memcg_to_vmpressure(memcg);",
            "",
            "\t/*",
            "\t * Here we only want to account pressure that userland is able to",
            "\t * help us with. For example, suppose that DMA zone is under",
            "\t * pressure; if we notify userland about that kind of pressure,",
            "\t * then it will be mostly a waste as it will trigger unnecessary",
            "\t * freeing of memory by userland (since userland is more likely to",
            "\t * have HIGHMEM/MOVABLE pages instead of the DMA fallback). That",
            "\t * is why we include only movable, highmem and FS/IO pages.",
            "\t * Indirect reclaim (kswapd) sets sc->gfp_mask to GFP_KERNEL, so",
            "\t * we account it too.",
            "\t */",
            "\tif (!(gfp & (__GFP_HIGHMEM | __GFP_MOVABLE | __GFP_IO | __GFP_FS)))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If we got here with no pages scanned, then that is an indicator",
            "\t * that reclaimer was unable to find any shrinkable LRUs at the",
            "\t * current scanning depth. But it does not mean that we should",
            "\t * report the critical pressure, yet. If the scanning priority",
            "\t * (scanning depth) goes too high (deep), we will be notified",
            "\t * through vmpressure_prio(). But so far, keep calm.",
            "\t */",
            "\tif (!scanned)",
            "\t\treturn;",
            "",
            "\tif (tree) {",
            "\t\tspin_lock(&vmpr->sr_lock);",
            "\t\tscanned = vmpr->tree_scanned += scanned;",
            "\t\tvmpr->tree_reclaimed += reclaimed;",
            "\t\tspin_unlock(&vmpr->sr_lock);",
            "",
            "\t\tif (scanned < vmpressure_win)",
            "\t\t\treturn;",
            "\t\tschedule_work(&vmpr->work);",
            "\t} else {",
            "\t\tenum vmpressure_levels level;",
            "",
            "\t\t/* For now, no users for root-level efficiency */",
            "\t\tif (!memcg || mem_cgroup_is_root(memcg))",
            "\t\t\treturn;",
            "",
            "\t\tspin_lock(&vmpr->sr_lock);",
            "\t\tscanned = vmpr->scanned += scanned;",
            "\t\treclaimed = vmpr->reclaimed += reclaimed;",
            "\t\tif (scanned < vmpressure_win) {",
            "\t\t\tspin_unlock(&vmpr->sr_lock);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tvmpr->scanned = vmpr->reclaimed = 0;",
            "\t\tspin_unlock(&vmpr->sr_lock);",
            "",
            "\t\tlevel = vmpressure_calc_level(scanned, reclaimed);",
            "",
            "\t\tif (level > VMPRESSURE_LOW) {",
            "\t\t\t/*",
            "\t\t\t * Let the socket buffer allocator know that",
            "\t\t\t * we are having trouble reclaiming LRU pages.",
            "\t\t\t *",
            "\t\t\t * For hysteresis keep the pressure state",
            "\t\t\t * asserted for a second in which subsequent",
            "\t\t\t * pressure events can occur.",
            "\t\t\t */",
            "\t\t\tWRITE_ONCE(memcg->socket_pressure, jiffies + HZ);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "vmpressure_level, vmpressure_calc_level, vmpressure_event, vmpressure_work_fn, vmpressure",
          "description": "实现内存压力计算和事件触发逻辑，vmpressure_level计算压力等级，vmpressure_calc_level基于扫描与回收比确定压力值，vmpressure_event处理事件通知，vmpressure_work_fn执行压力分析并触发相应操作",
          "similarity": 0.5413584113121033
        },
        {
          "chunk_id": 2,
          "file_path": "mm/vmpressure.c",
          "start_line": 335,
          "end_line": 432,
          "content": [
            "void vmpressure_prio(gfp_t gfp, struct mem_cgroup *memcg, int prio)",
            "{",
            "\t/*",
            "\t * We only use prio for accounting critical level. For more info",
            "\t * see comment for vmpressure_level_critical_prio variable above.",
            "\t */",
            "\tif (prio > vmpressure_level_critical_prio)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * OK, the prio is below the threshold, updating vmpressure",
            "\t * information before shrinker dives into long shrinking of long",
            "\t * range vmscan. Passing scanned = vmpressure_win, reclaimed = 0",
            "\t * to the vmpressure() basically means that we signal 'critical'",
            "\t * level.",
            "\t */",
            "\tvmpressure(gfp, memcg, true, vmpressure_win, 0);",
            "}",
            "int vmpressure_register_event(struct mem_cgroup *memcg,",
            "\t\t\t      struct eventfd_ctx *eventfd, const char *args)",
            "{",
            "\tstruct vmpressure *vmpr = memcg_to_vmpressure(memcg);",
            "\tstruct vmpressure_event *ev;",
            "\tenum vmpressure_modes mode = VMPRESSURE_NO_PASSTHROUGH;",
            "\tenum vmpressure_levels level;",
            "\tchar *spec, *spec_orig;",
            "\tchar *token;",
            "\tint ret = 0;",
            "",
            "\tspec_orig = spec = kstrndup(args, MAX_VMPRESSURE_ARGS_LEN, GFP_KERNEL);",
            "\tif (!spec)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/* Find required level */",
            "\ttoken = strsep(&spec, \",\");",
            "\tret = match_string(vmpressure_str_levels, VMPRESSURE_NUM_LEVELS, token);",
            "\tif (ret < 0)",
            "\t\tgoto out;",
            "\tlevel = ret;",
            "",
            "\t/* Find optional mode */",
            "\ttoken = strsep(&spec, \",\");",
            "\tif (token) {",
            "\t\tret = match_string(vmpressure_str_modes, VMPRESSURE_NUM_MODES, token);",
            "\t\tif (ret < 0)",
            "\t\t\tgoto out;",
            "\t\tmode = ret;",
            "\t}",
            "",
            "\tev = kzalloc(sizeof(*ev), GFP_KERNEL);",
            "\tif (!ev) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tev->efd = eventfd;",
            "\tev->level = level;",
            "\tev->mode = mode;",
            "",
            "\tmutex_lock(&vmpr->events_lock);",
            "\tlist_add(&ev->node, &vmpr->events);",
            "\tmutex_unlock(&vmpr->events_lock);",
            "\tret = 0;",
            "out:",
            "\tkfree(spec_orig);",
            "\treturn ret;",
            "}",
            "void vmpressure_unregister_event(struct mem_cgroup *memcg,",
            "\t\t\t\t struct eventfd_ctx *eventfd)",
            "{",
            "\tstruct vmpressure *vmpr = memcg_to_vmpressure(memcg);",
            "\tstruct vmpressure_event *ev;",
            "",
            "\tmutex_lock(&vmpr->events_lock);",
            "\tlist_for_each_entry(ev, &vmpr->events, node) {",
            "\t\tif (ev->efd != eventfd)",
            "\t\t\tcontinue;",
            "\t\tlist_del(&ev->node);",
            "\t\tkfree(ev);",
            "\t\tbreak;",
            "\t}",
            "\tmutex_unlock(&vmpr->events_lock);",
            "}",
            "void vmpressure_init(struct vmpressure *vmpr)",
            "{",
            "\tspin_lock_init(&vmpr->sr_lock);",
            "\tmutex_init(&vmpr->events_lock);",
            "\tINIT_LIST_HEAD(&vmpr->events);",
            "\tINIT_WORK(&vmpr->work, vmpressure_work_fn);",
            "}",
            "void vmpressure_cleanup(struct vmpressure *vmpr)",
            "{",
            "\t/*",
            "\t * Make sure there is no pending work before eventfd infrastructure",
            "\t * goes away.",
            "\t */",
            "\tflush_work(&vmpr->work);",
            "}"
          ],
          "function_name": "vmpressure_prio, vmpressure_register_event, vmpressure_unregister_event, vmpressure_init, vmpressure_cleanup",
          "description": "提供压力优先级处理、事件注册注销及资源初始化清理功能，vmpressure_prio根据优先级触发压力检测，注册事件接口用于订阅压力状态变化，初始化函数配置锁和工作队列，清理函数确保资源正确释放",
          "similarity": 0.5138068795204163
        },
        {
          "chunk_id": 0,
          "file_path": "mm/vmpressure.c",
          "start_line": 1,
          "end_line": 110,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Linux VM pressure",
            " *",
            " * Copyright 2012 Linaro Ltd.",
            " *\t\t  Anton Vorontsov <anton.vorontsov@linaro.org>",
            " *",
            " * Based on ideas from Andrew Morton, David Rientjes, KOSAKI Motohiro,",
            " * Leonid Moiseichuk, Mel Gorman, Minchan Kim and Pekka Enberg.",
            " */",
            "",
            "#include <linux/cgroup.h>",
            "#include <linux/fs.h>",
            "#include <linux/log2.h>",
            "#include <linux/sched.h>",
            "#include <linux/mm.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/eventfd.h>",
            "#include <linux/slab.h>",
            "#include <linux/swap.h>",
            "#include <linux/printk.h>",
            "#include <linux/vmpressure.h>",
            "",
            "/*",
            " * The window size (vmpressure_win) is the number of scanned pages before",
            " * we try to analyze scanned/reclaimed ratio. So the window is used as a",
            " * rate-limit tunable for the \"low\" level notification, and also for",
            " * averaging the ratio for medium/critical levels. Using small window",
            " * sizes can cause lot of false positives, but too big window size will",
            " * delay the notifications.",
            " *",
            " * As the vmscan reclaimer logic works with chunks which are multiple of",
            " * SWAP_CLUSTER_MAX, it makes sense to use it for the window size as well.",
            " *",
            " * TODO: Make the window size depend on machine size, as we do for vmstat",
            " * thresholds. Currently we set it to 512 pages (2MB for 4KB pages).",
            " */",
            "static const unsigned long vmpressure_win = SWAP_CLUSTER_MAX * 16;",
            "",
            "/*",
            " * These thresholds are used when we account memory pressure through",
            " * scanned/reclaimed ratio. The current values were chosen empirically. In",
            " * essence, they are percents: the higher the value, the more number",
            " * unsuccessful reclaims there were.",
            " */",
            "static const unsigned int vmpressure_level_med = 60;",
            "static const unsigned int vmpressure_level_critical = 95;",
            "",
            "/*",
            " * When there are too little pages left to scan, vmpressure() may miss the",
            " * critical pressure as number of pages will be less than \"window size\".",
            " * However, in that case the vmscan priority will raise fast as the",
            " * reclaimer will try to scan LRUs more deeply.",
            " *",
            " * The vmscan logic considers these special priorities:",
            " *",
            " * prio == DEF_PRIORITY (12): reclaimer starts with that value",
            " * prio <= DEF_PRIORITY - 2 : kswapd becomes somewhat overwhelmed",
            " * prio == 0                : close to OOM, kernel scans every page in an lru",
            " *",
            " * Any value in this range is acceptable for this tunable (i.e. from 12 to",
            " * 0). Current value for the vmpressure_level_critical_prio is chosen",
            " * empirically, but the number, in essence, means that we consider",
            " * critical level when scanning depth is ~10% of the lru size (vmscan",
            " * scans 'lru_size >> prio' pages, so it is actually 12.5%, or one",
            " * eights).",
            " */",
            "static const unsigned int vmpressure_level_critical_prio = ilog2(100 / 10);",
            "",
            "static struct vmpressure *work_to_vmpressure(struct work_struct *work)",
            "{",
            "\treturn container_of(work, struct vmpressure, work);",
            "}",
            "",
            "static struct vmpressure *vmpressure_parent(struct vmpressure *vmpr)",
            "{",
            "\tstruct mem_cgroup *memcg = vmpressure_to_memcg(vmpr);",
            "",
            "\tmemcg = parent_mem_cgroup(memcg);",
            "\tif (!memcg)",
            "\t\treturn NULL;",
            "\treturn memcg_to_vmpressure(memcg);",
            "}",
            "",
            "enum vmpressure_levels {",
            "\tVMPRESSURE_LOW = 0,",
            "\tVMPRESSURE_MEDIUM,",
            "\tVMPRESSURE_CRITICAL,",
            "\tVMPRESSURE_NUM_LEVELS,",
            "};",
            "",
            "enum vmpressure_modes {",
            "\tVMPRESSURE_NO_PASSTHROUGH = 0,",
            "\tVMPRESSURE_HIERARCHY,",
            "\tVMPRESSURE_LOCAL,",
            "\tVMPRESSURE_NUM_MODES,",
            "};",
            "",
            "static const char * const vmpressure_str_levels[] = {",
            "\t[VMPRESSURE_LOW] = \"low\",",
            "\t[VMPRESSURE_MEDIUM] = \"medium\",",
            "\t[VMPRESSURE_CRITICAL] = \"critical\",",
            "};",
            "",
            "static const char * const vmpressure_str_modes[] = {",
            "\t[VMPRESSURE_NO_PASSTHROUGH] = \"default\",",
            "\t[VMPRESSURE_HIERARCHY] = \"hierarchy\",",
            "\t[VMPRESSURE_LOCAL] = \"local\",",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义内存压力监控的相关常量和枚举类型，其中vmpressure_win设置扫描窗口大小，vmpressure_level_med和vmpressure_level_critical定义压力阈值，枚举类型表示压力等级和模式，用于后续压力检测逻辑",
          "similarity": 0.4754849970340729
        }
      ]
    }
  ]
}