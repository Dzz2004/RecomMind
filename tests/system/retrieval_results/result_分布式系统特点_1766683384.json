{
  "query": "分布式系统特点",
  "timestamp": "2025-12-26 01:23:04",
  "retrieved_files": [
    {
      "source_file": "mm/swap_slots.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:27:07\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `swap_slots.c`\n\n---\n\n# swap_slots.c 技术文档\n\n## 1. 文件概述\n\n`swap_slots.c` 实现了 Linux 内核中用于管理交换槽（swap slots）的本地 CPU 缓存机制。该机制通过为每个 CPU 维护一个交换槽缓存，避免在每次分配或释放交换槽时频繁获取全局 `swap_info` 锁，从而提升性能。同时，它支持将回收的交换槽批量归还到全局池中，以减少内存碎片。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct swap_slots_cache`：每个 CPU 的交换槽缓存结构，包含两个数组：\n  - `slots`：用于分配的交换槽缓存\n  - `slots_ret`：用于暂存待回收的交换槽\n- 全局 per-CPU 变量 `swp_slots`：存储各 CPU 的交换槽缓存实例\n\n### 主要函数\n- `alloc_swap_slot_cache()`：为指定 CPU 分配交换槽缓存内存\n- `free_slot_cache()`：释放指定 CPU 的交换槽缓存\n- `refill_swap_slots_cache()`：从全局交换池填充本地缓存\n- `free_swap_slot()`：将交换槽返回到本地缓存或直接释放\n- `__drain_swap_slots_cache()`：将所有在线 CPU 的缓存中的交换槽归还到全局池\n- `drain_slots_cache_cpu()`：清空指定 CPU 的交换槽缓存\n- `enable_swap_slots_cache()`：启用交换槽缓存机制\n- `disable_swap_slots_cache_lock()` / `reenable_swap_slots_cache_unlock()`：禁用/重新启用缓存\n- `check_cache_active()`：根据系统交换页数量动态激活/停用缓存\n\n### 全局变量\n- `swap_slot_cache_active`：指示缓存是否当前处于活跃状态\n- `swap_slot_cache_enabled`：指示缓存功能是否已启用\n- `swap_slot_cache_initialized`：指示缓存子系统是否已完成初始化\n- `swap_slots_cache_mutex`：保护缓存操作的互斥锁\n- `swap_slots_cache_enable_mutex`：序列化缓存启用/禁用操作的互斥锁\n\n## 3. 关键实现\n\n### 本地缓存设计\n- 每个 CPU 拥有两个交换槽数组：\n  - `slots`：用于快速分配交换槽（受 `alloc_lock` 互斥锁保护）\n  - `slots_ret`：用于暂存待释放的交换槽（受 `free_lock` 自旋锁保护）\n- 分配时优先从本地 `slots` 数组获取；若为空，则批量从全局池获取 `SWAP_SLOTS_CACHE_SIZE` 个槽位填充\n- 释放时先放入 `slots_ret`，当其满时才批量归还到全局池，有助于减少锁竞争和内存碎片\n\n### 动态启停机制\n- 通过 `check_cache_active()` 根据可用交换页数量动态控制缓存活跃状态：\n  - 当可用交换页 > `num_online_cpus() * THRESHOLD_ACTIVATE_SWAP_SLOTS_CACHE` 时激活缓存\n  - 当可用交换页 < `num_online_cpus() * THRESHOLD_DEACTIVATE_SWAP_SLOTS_CACHE` 时停用缓存\n- 停用时会立即清空所有 CPU 缓存中的交换槽并归还到全局池\n\n### CPU 热插拔支持\n- 使用 `cpuhp_setup_state()` 注册 CPU 热插拔回调：\n  - CPU 上线时调用 `alloc_swap_slot_cache()` 分配缓存\n  - CPU 下线时调用 `free_slot_cache()` 释放缓存\n- 在清空缓存时使用 `for_each_online_cpu()` 遍历，避免与 CPU 热插拔操作死锁\n\n### 锁设计\n- 使用互斥锁（mutex）而非自旋锁，因为分配交换槽可能触发内存回收而睡眠\n- 分离分配锁（`alloc_lock`）和释放锁（`free_lock`），提高并发性\n- 全局操作使用 `swap_slots_cache_mutex` 保护，启用/禁用操作使用独立的 `swap_slots_cache_enable_mutex`\n\n### 安全标记\n- 从全局池分配的交换槽会被标记 `SWAP_HAS_CACHE`，防止被重复分配\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/swap_slots.h>`：定义交换槽缓存接口和数据结构\n  - `<linux/cpu.h>`、`<linux/cpumask.h>`：CPU 热插拔和掩码操作\n  - `<linux/slab.h>`、`<linux/vmalloc.h>`：内存分配\n  - `<linux/mutex.h>`、`<linux/spinlock.h>`：同步原语\n  - `<linux/mm.h>`：内存管理相关函数\n\n- **外部函数依赖**：\n  - `get_swap_pages()`：从全局交换池批量获取交换槽\n  - `swapcache_free_entries()`：将交换槽归还到全局池\n  - `has_usable_swap()`：检查是否存在可用交换设备\n  - `get_nr_swap_pages()`：获取当前可用交换页数量\n  - `zswap_invalidate()`：通知 zswap 无效化交换条目\n\n- **被调用方**：\n  - `folio_alloc_swap()`：在页面分配交换槽时使用此缓存机制\n  - 交换子系统其他组件通过 `free_swap_slot()` 释放交换槽\n\n## 5. 使用场景\n\n1. **页面交换分配**：当内核需要为匿名页分配交换槽时，优先从本地 CPU 缓存获取，避免全局锁竞争\n2. **页面交换释放**：当交换页被换入内存后，其交换槽通过 `free_swap_slot()` 返回到本地缓存\n3. **内存压力场景**：在低内存情况下，系统可能停用交换槽缓存以释放更多交换空间\n4. **交换设备管理**：\n   - `swapon` 时通过 `enable_swap_slots_cache()` 启用缓存\n   - `swapoff` 时通过 `__drain_swap_slots_cache()` 确保所有缓存槽位归还\n5. **CPU 热插拔**：动态为新上线 CPU 分配缓存，为下线 CPU 释放缓存资源\n6. **系统休眠/恢复**：在休眠前确保交换槽缓存被正确清空，避免状态不一致",
      "similarity": 0.4978281855583191,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "mm/swap_slots.c",
          "start_line": 305,
          "end_line": 353,
          "content": [
            "swp_entry_t folio_alloc_swap(struct folio *folio)",
            "{",
            "\tswp_entry_t entry;",
            "\tstruct swap_slots_cache *cache;",
            "",
            "\tentry.val = 0;",
            "",
            "\tif (folio_test_large(folio)) {",
            "\t\tif (IS_ENABLED(CONFIG_THP_SWAP))",
            "\t\t\tget_swap_pages(1, &entry, folio_order(folio));",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/*",
            "\t * Preemption is allowed here, because we may sleep",
            "\t * in refill_swap_slots_cache().  But it is safe, because",
            "\t * accesses to the per-CPU data structure are protected by the",
            "\t * mutex cache->alloc_lock.",
            "\t *",
            "\t * The alloc path here does not touch cache->slots_ret",
            "\t * so cache->free_lock is not taken.",
            "\t */",
            "\tcache = raw_cpu_ptr(&swp_slots);",
            "",
            "\tif (likely(check_cache_active() && cache->slots)) {",
            "\t\tmutex_lock(&cache->alloc_lock);",
            "\t\tif (cache->slots) {",
            "repeat:",
            "\t\t\tif (cache->nr) {",
            "\t\t\t\tentry = cache->slots[cache->cur];",
            "\t\t\t\tcache->slots[cache->cur++].val = 0;",
            "\t\t\t\tcache->nr--;",
            "\t\t\t} else if (refill_swap_slots_cache(cache)) {",
            "\t\t\t\tgoto repeat;",
            "\t\t\t}",
            "\t\t}",
            "\t\tmutex_unlock(&cache->alloc_lock);",
            "\t\tif (entry.val)",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tget_swap_pages(1, &entry, 0);",
            "out:",
            "\tif (mem_cgroup_try_charge_swap(folio, entry)) {",
            "\t\tput_swap_folio(folio, entry);",
            "\t\tentry.val = 0;",
            "\t}",
            "\treturn entry;",
            "}"
          ],
          "function_name": "folio_alloc_swap",
          "description": "从swap slots缓存或直接全局池分配交换槽，支持并发场景下的缓存预取与资源回收协作",
          "similarity": 0.44386690855026245
        },
        {
          "chunk_id": 0,
          "file_path": "mm/swap_slots.c",
          "start_line": 1,
          "end_line": 51,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Manage cache of swap slots to be used for and returned from",
            " * swap.",
            " *",
            " * Copyright(c) 2016 Intel Corporation.",
            " *",
            " * Author: Tim Chen <tim.c.chen@linux.intel.com>",
            " *",
            " * We allocate the swap slots from the global pool and put",
            " * it into local per cpu caches.  This has the advantage",
            " * of no needing to acquire the swap_info lock every time",
            " * we need a new slot.",
            " *",
            " * There is also opportunity to simply return the slot",
            " * to local caches without needing to acquire swap_info",
            " * lock.  We do not reuse the returned slots directly but",
            " * move them back to the global pool in a batch.  This",
            " * allows the slots to coalesce and reduce fragmentation.",
            " *",
            " * The swap entry allocated is marked with SWAP_HAS_CACHE",
            " * flag in map_count that prevents it from being allocated",
            " * again from the global pool.",
            " *",
            " * The swap slots cache is protected by a mutex instead of",
            " * a spin lock as when we search for slots with scan_swap_map,",
            " * we can possibly sleep.",
            " */",
            "",
            "#include <linux/swap_slots.h>",
            "#include <linux/cpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/slab.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/mutex.h>",
            "#include <linux/mm.h>",
            "",
            "static DEFINE_PER_CPU(struct swap_slots_cache, swp_slots);",
            "static bool\tswap_slot_cache_active;",
            "bool\tswap_slot_cache_enabled;",
            "static bool\tswap_slot_cache_initialized;",
            "static DEFINE_MUTEX(swap_slots_cache_mutex);",
            "/* Serialize swap slots cache enable/disable operations */",
            "static DEFINE_MUTEX(swap_slots_cache_enable_mutex);",
            "",
            "static void __drain_swap_slots_cache(unsigned int type);",
            "",
            "#define use_swap_slot_cache (swap_slot_cache_active && swap_slot_cache_enabled)",
            "#define SLOTS_CACHE 0x1",
            "#define SLOTS_CACHE_RET 0x2",
            ""
          ],
          "function_name": null,
          "description": "声明swap slots缓存相关的全局变量和宏，用于管理交换槽的本地CPU缓存与全局池切换逻辑",
          "similarity": 0.43477538228034973
        },
        {
          "chunk_id": 1,
          "file_path": "mm/swap_slots.c",
          "start_line": 52,
          "end_line": 159,
          "content": [
            "static void deactivate_swap_slots_cache(void)",
            "{",
            "\tmutex_lock(&swap_slots_cache_mutex);",
            "\tswap_slot_cache_active = false;",
            "\t__drain_swap_slots_cache(SLOTS_CACHE|SLOTS_CACHE_RET);",
            "\tmutex_unlock(&swap_slots_cache_mutex);",
            "}",
            "static void reactivate_swap_slots_cache(void)",
            "{",
            "\tmutex_lock(&swap_slots_cache_mutex);",
            "\tswap_slot_cache_active = true;",
            "\tmutex_unlock(&swap_slots_cache_mutex);",
            "}",
            "void disable_swap_slots_cache_lock(void)",
            "{",
            "\tmutex_lock(&swap_slots_cache_enable_mutex);",
            "\tswap_slot_cache_enabled = false;",
            "\tif (swap_slot_cache_initialized) {",
            "\t\t/* serialize with cpu hotplug operations */",
            "\t\tcpus_read_lock();",
            "\t\t__drain_swap_slots_cache(SLOTS_CACHE|SLOTS_CACHE_RET);",
            "\t\tcpus_read_unlock();",
            "\t}",
            "}",
            "static void __reenable_swap_slots_cache(void)",
            "{",
            "\tswap_slot_cache_enabled = has_usable_swap();",
            "}",
            "void reenable_swap_slots_cache_unlock(void)",
            "{",
            "\t__reenable_swap_slots_cache();",
            "\tmutex_unlock(&swap_slots_cache_enable_mutex);",
            "}",
            "static bool check_cache_active(void)",
            "{",
            "\tlong pages;",
            "",
            "\tif (!swap_slot_cache_enabled)",
            "\t\treturn false;",
            "",
            "\tpages = get_nr_swap_pages();",
            "\tif (!swap_slot_cache_active) {",
            "\t\tif (pages > num_online_cpus() *",
            "\t\t    THRESHOLD_ACTIVATE_SWAP_SLOTS_CACHE)",
            "\t\t\treactivate_swap_slots_cache();",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* if global pool of slot caches too low, deactivate cache */",
            "\tif (pages < num_online_cpus() * THRESHOLD_DEACTIVATE_SWAP_SLOTS_CACHE)",
            "\t\tdeactivate_swap_slots_cache();",
            "out:",
            "\treturn swap_slot_cache_active;",
            "}",
            "static int alloc_swap_slot_cache(unsigned int cpu)",
            "{",
            "\tstruct swap_slots_cache *cache;",
            "\tswp_entry_t *slots, *slots_ret;",
            "",
            "\t/*",
            "\t * Do allocation outside swap_slots_cache_mutex",
            "\t * as kvzalloc could trigger reclaim and folio_alloc_swap,",
            "\t * which can lock swap_slots_cache_mutex.",
            "\t */",
            "\tslots = kvcalloc(SWAP_SLOTS_CACHE_SIZE, sizeof(swp_entry_t),",
            "\t\t\t GFP_KERNEL);",
            "\tif (!slots)",
            "\t\treturn -ENOMEM;",
            "",
            "\tslots_ret = kvcalloc(SWAP_SLOTS_CACHE_SIZE, sizeof(swp_entry_t),",
            "\t\t\t     GFP_KERNEL);",
            "\tif (!slots_ret) {",
            "\t\tkvfree(slots);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\tmutex_lock(&swap_slots_cache_mutex);",
            "\tcache = &per_cpu(swp_slots, cpu);",
            "\tif (cache->slots || cache->slots_ret) {",
            "\t\t/* cache already allocated */",
            "\t\tmutex_unlock(&swap_slots_cache_mutex);",
            "",
            "\t\tkvfree(slots);",
            "\t\tkvfree(slots_ret);",
            "",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (!cache->lock_initialized) {",
            "\t\tmutex_init(&cache->alloc_lock);",
            "\t\tspin_lock_init(&cache->free_lock);",
            "\t\tcache->lock_initialized = true;",
            "\t}",
            "\tcache->nr = 0;",
            "\tcache->cur = 0;",
            "\tcache->n_ret = 0;",
            "\t/*",
            "\t * We initialized alloc_lock and free_lock earlier.  We use",
            "\t * !cache->slots or !cache->slots_ret to know if it is safe to acquire",
            "\t * the corresponding lock and use the cache.  Memory barrier below",
            "\t * ensures the assumption.",
            "\t */",
            "\tmb();",
            "\tcache->slots = slots;",
            "\tcache->slots_ret = slots_ret;",
            "\tmutex_unlock(&swap_slots_cache_mutex);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "deactivate_swap_slots_cache, reactivate_swap_slots_cache, disable_swap_slots_cache_lock, __reenable_swap_slots_cache, reenable_swap_slots_cache_unlock, check_cache_active, alloc_swap_slot_cache",
          "description": "提供swap slots缓存的激活/停用控制接口，通过互斥锁同步状态变更并协调内存分配与回收流程",
          "similarity": 0.42368069291114807
        },
        {
          "chunk_id": 2,
          "file_path": "mm/swap_slots.c",
          "start_line": 168,
          "end_line": 297,
          "content": [
            "static void drain_slots_cache_cpu(unsigned int cpu, unsigned int type,",
            "\t\t\t\t  bool free_slots)",
            "{",
            "\tstruct swap_slots_cache *cache;",
            "\tswp_entry_t *slots = NULL;",
            "",
            "\tcache = &per_cpu(swp_slots, cpu);",
            "\tif ((type & SLOTS_CACHE) && cache->slots) {",
            "\t\tmutex_lock(&cache->alloc_lock);",
            "\t\tswapcache_free_entries(cache->slots + cache->cur, cache->nr);",
            "\t\tcache->cur = 0;",
            "\t\tcache->nr = 0;",
            "\t\tif (free_slots && cache->slots) {",
            "\t\t\tkvfree(cache->slots);",
            "\t\t\tcache->slots = NULL;",
            "\t\t}",
            "\t\tmutex_unlock(&cache->alloc_lock);",
            "\t}",
            "\tif ((type & SLOTS_CACHE_RET) && cache->slots_ret) {",
            "\t\tspin_lock_irq(&cache->free_lock);",
            "\t\tswapcache_free_entries(cache->slots_ret, cache->n_ret);",
            "\t\tcache->n_ret = 0;",
            "\t\tif (free_slots && cache->slots_ret) {",
            "\t\t\tslots = cache->slots_ret;",
            "\t\t\tcache->slots_ret = NULL;",
            "\t\t}",
            "\t\tspin_unlock_irq(&cache->free_lock);",
            "\t\tkvfree(slots);",
            "\t}",
            "}",
            "static void __drain_swap_slots_cache(unsigned int type)",
            "{",
            "\tunsigned int cpu;",
            "",
            "\t/*",
            "\t * This function is called during",
            "\t *\t1) swapoff, when we have to make sure no",
            "\t *\t   left over slots are in cache when we remove",
            "\t *\t   a swap device;",
            "\t *      2) disabling of swap slot cache, when we run low",
            "\t *\t   on swap slots when allocating memory and need",
            "\t *\t   to return swap slots to global pool.",
            "\t *",
            "\t * We cannot acquire cpu hot plug lock here as",
            "\t * this function can be invoked in the cpu",
            "\t * hot plug path:",
            "\t * cpu_up -> lock cpu_hotplug -> cpu hotplug state callback",
            "\t *   -> memory allocation -> direct reclaim -> folio_alloc_swap",
            "\t *   -> drain_swap_slots_cache",
            "\t *",
            "\t * Hence the loop over current online cpu below could miss cpu that",
            "\t * is being brought online but not yet marked as online.",
            "\t * That is okay as we do not schedule and run anything on a",
            "\t * cpu before it has been marked online. Hence, we will not",
            "\t * fill any swap slots in slots cache of such cpu.",
            "\t * There are no slots on such cpu that need to be drained.",
            "\t */",
            "\tfor_each_online_cpu(cpu)",
            "\t\tdrain_slots_cache_cpu(cpu, type, false);",
            "}",
            "static int free_slot_cache(unsigned int cpu)",
            "{",
            "\tmutex_lock(&swap_slots_cache_mutex);",
            "\tdrain_slots_cache_cpu(cpu, SLOTS_CACHE | SLOTS_CACHE_RET, true);",
            "\tmutex_unlock(&swap_slots_cache_mutex);",
            "\treturn 0;",
            "}",
            "void enable_swap_slots_cache(void)",
            "{",
            "\tmutex_lock(&swap_slots_cache_enable_mutex);",
            "\tif (!swap_slot_cache_initialized) {",
            "\t\tint ret;",
            "",
            "\t\tret = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN, \"swap_slots_cache\",",
            "\t\t\t\t\talloc_swap_slot_cache, free_slot_cache);",
            "\t\tif (WARN_ONCE(ret < 0, \"Cache allocation failed (%s), operating \"",
            "\t\t\t\t       \"without swap slots cache.\\n\", __func__))",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\tswap_slot_cache_initialized = true;",
            "\t}",
            "",
            "\t__reenable_swap_slots_cache();",
            "out_unlock:",
            "\tmutex_unlock(&swap_slots_cache_enable_mutex);",
            "}",
            "static int refill_swap_slots_cache(struct swap_slots_cache *cache)",
            "{",
            "\tif (!use_swap_slot_cache)",
            "\t\treturn 0;",
            "",
            "\tcache->cur = 0;",
            "\tif (swap_slot_cache_active)",
            "\t\tcache->nr = get_swap_pages(SWAP_SLOTS_CACHE_SIZE,",
            "\t\t\t\t\t   cache->slots, 0);",
            "",
            "\treturn cache->nr;",
            "}",
            "void free_swap_slot(swp_entry_t entry)",
            "{",
            "\tstruct swap_slots_cache *cache;",
            "",
            "\t/* Large folio swap slot is not covered. */",
            "\tzswap_invalidate(entry);",
            "",
            "\tcache = raw_cpu_ptr(&swp_slots);",
            "\tif (likely(use_swap_slot_cache && cache->slots_ret)) {",
            "\t\tspin_lock_irq(&cache->free_lock);",
            "\t\t/* Swap slots cache may be deactivated before acquiring lock */",
            "\t\tif (!use_swap_slot_cache || !cache->slots_ret) {",
            "\t\t\tspin_unlock_irq(&cache->free_lock);",
            "\t\t\tgoto direct_free;",
            "\t\t}",
            "\t\tif (cache->n_ret >= SWAP_SLOTS_CACHE_SIZE) {",
            "\t\t\t/*",
            "\t\t\t * Return slots to global pool.",
            "\t\t\t * The current swap_map value is SWAP_HAS_CACHE.",
            "\t\t\t * Set it to 0 to indicate it is available for",
            "\t\t\t * allocation in global pool",
            "\t\t\t */",
            "\t\t\tswapcache_free_entries(cache->slots_ret, cache->n_ret);",
            "\t\t\tcache->n_ret = 0;",
            "\t\t}",
            "\t\tcache->slots_ret[cache->n_ret++] = entry;",
            "\t\tspin_unlock_irq(&cache->free_lock);",
            "\t} else {",
            "direct_free:",
            "\t\tswapcache_free_entries(&entry, 1);",
            "\t}",
            "}"
          ],
          "function_name": "drain_slots_cache_cpu, __drain_swap_slots_cache, free_slot_cache, enable_swap_slots_cache, refill_swap_slots_cache, free_swap_slot",
          "description": "实现swap slots缓存的清理机制，包括CPU本地缓存数据迁移、内存释放及全局池归还操作",
          "similarity": 0.419680118560791
        }
      ]
    },
    {
      "source_file": "mm/percpu.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:11:13\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `percpu.c`\n\n---\n\n# percpu.c 技术文档\n\n## 1. 文件概述\n\n`percpu.c` 是 Linux 内核中实现每 CPU（per-CPU）内存分配器的核心文件。该分配器用于管理静态和动态的 per-CPU 内存区域，支持在多处理器系统中为每个 CPU 分配独立但布局一致的内存块。其设计兼顾了 NUMA 架构下的性能优化、内存控制组（memcg）感知能力以及对原子上下文分配的支持。该分配器通过“块（chunk）-单元（unit）”模型组织内存，并特别处理包含内核静态 per-CPU 变量的首个内存块。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct pcpu_chunk`：表示一个 per-CPU 内存块，包含多个单元（每个 CPU 一个），管理分配位图、空闲信息等。\n- `pcpu_chunk_lists`：按最大连续空闲区域大小分槽（slot）组织的块链表数组，用于快速查找合适大小的空闲块。\n- `pcpu_first_chunk`：指向包含内核静态 per-CPU 变量的首个特殊内存块。\n- `pcpu_reserved_chunk`：可选的保留内存块，用于服务特定保留区域的分配（如内核模块的静态 per-CPU 变量）。\n\n### 关键全局变量\n- `pcpu_base_addr`：per-CPU 区域的基地址，用于地址与 per-CPU 指针之间的转换。\n- `pcpu_unit_map` / `pcpu_unit_offsets`：CPU 到其对应内存单元的映射及偏移。\n- `pcpu_nr_groups` / `pcpu_group_offsets` / `pcpu_group_sizes`：基于 NUMA 节点分组的信息，用于虚拟内存分配。\n- `pcpu_lock`：保护所有内部数据结构的自旋锁。\n- `pcpu_alloc_mutex`：保护块创建/销毁、填充/释放等重型操作的互斥锁。\n\n### 核心机制\n- **地址转换宏**：`__addr_to_pcpu_ptr()` 和 `__pcpu_ptr_to_addr()`，用于普通虚拟地址与 per-CPU 指针之间的相互转换。\n- **异步平衡工作**：`pcpu_balance_workfn()` 工作队列函数，用于异步地填充或释放内存页以维持空闲页数量在合理范围（`PCPU_EMPTY_POP_PAGES_LOW/HIGH`）。\n- **内存槽管理**：使用位图（bitmap）跟踪块内分配状态，通过分槽（slot）机制加速分配查找。\n\n## 3. 关键实现\n\n### 内存组织模型\n- **Chunk-Unit 模型**：内存被划分为多个 Chunk，每个 Chunk 包含 `pcpu_nr_units` 个 Unit（通常每个可能的 CPU 对应一个 Unit）。所有 Unit 在各自 Chunk 内具有相同的内存布局。\n- **首个 Chunk 特殊处理**：首个 Chunk 的布局为 `<Static | [Reserved] | Dynamic>`。静态部分包含编译时确定的内核 per-CPU 变量；保留部分（可选）用于内核模块；动态部分用于运行时分配。\n- **NUMA 感知**：Units 根据底层机器的 NUMA 属性进行分组，以优化内存访问局部性。\n\n### 分配策略\n- **分槽（Slot）管理**：空闲 Chunk 按其最大连续空闲区域大小放入不同 Slot。分配时优先从最满（即最大空闲区域最小但足够）的 Chunk 开始尝试，以提高内存利用率。\n- **位图管理**：每个 Chunk 使用两个位图：\n  - **分配位图（alloc map）**：记录每个最小分配单元（`PCPU_MIN_ALLOC_SIZE`）的占用状态，在每次分配/释放时更新。\n  - **边界位图（boundary map）**：仅在分配时更新，用于快速合并相邻空闲块。\n- **Memcg 感知**：通过 `__GFP_ACCOUNT` 标志区分是否为内存控制组感知的分配。Memcg 感知分配和非感知（或 root cgroup）分配使用两套独立的 Chunk 集合。\n\n### 内存管理与优化\n- **惰性填充（Lazy Population）**：Chunk 的物理页按需填充，且同一 Chunk 的所有 Unit 同步增长。\n- **异步平衡**：当原子分配失败或空闲页数量超出阈值时，调度 `pcpu_balance_work` 工作项，异步地填充新页或释放完全空闲的页，避免在关键路径上执行耗时操作。\n- **页到 Chunk 的反向映射**：利用 `struct page` 的 `index` 字段存储指向所属 Chunk 的指针，便于快速定位。\n\n### 地址转换\n- 在 SMP 系统中，per-CPU 指针是相对于 `__per_cpu_start` 的偏移量。通过 `pcpu_base_addr` 进行重定位，实现从 per-CPU 指针到实际虚拟地址的转换。\n- 在 UP（单处理器）系统中，per-CPU 指针直接等同于虚拟地址。\n\n## 4. 依赖关系\n\n- **架构相关代码**：依赖 `asm/percpu.h` 提供的 `__addr_to_pcpu_ptr` 和 `__pcpu_ptr_to_addr` 宏（若架构有特殊需求），以及 `asm/cacheflush.h`、`asm/tlbflush.h` 进行缓存和 TLB 管理。\n- **内存管理子系统**：重度依赖 `mm.h`、`vmalloc.h`、`memblock.h` 进行底层内存分配和管理。\n- **调度与同步**：使用 `spinlock.h`、`mutex.h`、`workqueue.h` 实现并发控制和异步操作。\n- **其他核心子系统**：\n  - `cpumask.h`：处理 CPU 掩码和在线 CPU 信息。\n  - `memcontrol.h`：集成内存控制组（cgroup）功能。\n  - `kmemleak.h`：支持内存泄漏检测。\n  - `trace/events/percpu.h`：提供跟踪点用于调试和性能分析。\n- **内部头文件**：包含 `percpu-internal.h`，定义了分配器内部使用的数据结构和辅助函数。\n\n## 5. 使用场景\n\n- **内核初始化**：在系统启动早期，通过 `pcpu_setup_first_chunk()` 初始化首个 Chunk，将链接器生成的静态 per-CPU 变量（位于 `__per_cpu_start` 到 `__per_cpu_end`）映射到 per-CPU 内存区域。\n- **内核模块加载**：为内核模块中的静态 per-CPU 变量分配空间（通常使用保留区域或动态分配）。\n- **运行时动态分配**：内核代码通过 `alloc_percpu()`、`__alloc_percpu()` 等 API 在运行时申请 per-CPU 内存，用于存储每个 CPU 独立的数据结构（如统计计数器、缓冲区等）。\n- **高性能路径**：由于 per-CPU 内存访问无需加锁，常用于中断处理、软中断、RCU 回调等对性能敏感的上下文中。\n- **内存资源控制**：当进程属于非 root 内存 cgroup 且分配请求带有 `__GFP_ACCOUNT` 标志时，分配的 per-CPU 内存会计入该 cgroup 的内存使用量。",
      "similarity": 0.4912741184234619,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/percpu.c",
          "start_line": 1,
          "end_line": 200,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * mm/percpu.c - percpu memory allocator",
            " *",
            " * Copyright (C) 2009\t\tSUSE Linux Products GmbH",
            " * Copyright (C) 2009\t\tTejun Heo <tj@kernel.org>",
            " *",
            " * Copyright (C) 2017\t\tFacebook Inc.",
            " * Copyright (C) 2017\t\tDennis Zhou <dennis@kernel.org>",
            " *",
            " * The percpu allocator handles both static and dynamic areas.  Percpu",
            " * areas are allocated in chunks which are divided into units.  There is",
            " * a 1-to-1 mapping for units to possible cpus.  These units are grouped",
            " * based on NUMA properties of the machine.",
            " *",
            " *  c0                           c1                         c2",
            " *  -------------------          -------------------        ------------",
            " * | u0 | u1 | u2 | u3 |        | u0 | u1 | u2 | u3 |      | u0 | u1 | u",
            " *  -------------------  ......  -------------------  ....  ------------",
            " *",
            " * Allocation is done by offsets into a unit's address space.  Ie., an",
            " * area of 512 bytes at 6k in c1 occupies 512 bytes at 6k in c1:u0,",
            " * c1:u1, c1:u2, etc.  On NUMA machines, the mapping may be non-linear",
            " * and even sparse.  Access is handled by configuring percpu base",
            " * registers according to the cpu to unit mappings and offsetting the",
            " * base address using pcpu_unit_size.",
            " *",
            " * There is special consideration for the first chunk which must handle",
            " * the static percpu variables in the kernel image as allocation services",
            " * are not online yet.  In short, the first chunk is structured like so:",
            " *",
            " *                  <Static | [Reserved] | Dynamic>",
            " *",
            " * The static data is copied from the original section managed by the",
            " * linker.  The reserved section, if non-zero, primarily manages static",
            " * percpu variables from kernel modules.  Finally, the dynamic section",
            " * takes care of normal allocations.",
            " *",
            " * The allocator organizes chunks into lists according to free size and",
            " * memcg-awareness.  To make a percpu allocation memcg-aware the __GFP_ACCOUNT",
            " * flag should be passed.  All memcg-aware allocations are sharing one set",
            " * of chunks and all unaccounted allocations and allocations performed",
            " * by processes belonging to the root memory cgroup are using the second set.",
            " *",
            " * The allocator tries to allocate from the fullest chunk first. Each chunk",
            " * is managed by a bitmap with metadata blocks.  The allocation map is updated",
            " * on every allocation and free to reflect the current state while the boundary",
            " * map is only updated on allocation.  Each metadata block contains",
            " * information to help mitigate the need to iterate over large portions",
            " * of the bitmap.  The reverse mapping from page to chunk is stored in",
            " * the page's index.  Lastly, units are lazily backed and grow in unison.",
            " *",
            " * There is a unique conversion that goes on here between bytes and bits.",
            " * Each bit represents a fragment of size PCPU_MIN_ALLOC_SIZE.  The chunk",
            " * tracks the number of pages it is responsible for in nr_pages.  Helper",
            " * functions are used to convert from between the bytes, bits, and blocks.",
            " * All hints are managed in bits unless explicitly stated.",
            " *",
            " * To use this allocator, arch code should do the following:",
            " *",
            " * - define __addr_to_pcpu_ptr() and __pcpu_ptr_to_addr() to translate",
            " *   regular address to percpu pointer and back if they need to be",
            " *   different from the default",
            " *",
            " * - use pcpu_setup_first_chunk() during percpu area initialization to",
            " *   setup the first chunk containing the kernel static percpu area",
            " */",
            "",
            "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt",
            "",
            "#include <linux/bitmap.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/memblock.h>",
            "#include <linux/err.h>",
            "#include <linux/list.h>",
            "#include <linux/log2.h>",
            "#include <linux/mm.h>",
            "#include <linux/module.h>",
            "#include <linux/mutex.h>",
            "#include <linux/percpu.h>",
            "#include <linux/pfn.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/memcontrol.h>",
            "",
            "#include <asm/cacheflush.h>",
            "#include <asm/sections.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/io.h>",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/percpu.h>",
            "",
            "#include \"percpu-internal.h\"",
            "",
            "/*",
            " * The slots are sorted by the size of the biggest continuous free area.",
            " * 1-31 bytes share the same slot.",
            " */",
            "#define PCPU_SLOT_BASE_SHIFT\t\t5",
            "/* chunks in slots below this are subject to being sidelined on failed alloc */",
            "#define PCPU_SLOT_FAIL_THRESHOLD\t3",
            "",
            "#define PCPU_EMPTY_POP_PAGES_LOW\t2",
            "#define PCPU_EMPTY_POP_PAGES_HIGH\t4",
            "",
            "#ifdef CONFIG_SMP",
            "/* default addr <-> pcpu_ptr mapping, override in asm/percpu.h if necessary */",
            "#ifndef __addr_to_pcpu_ptr",
            "#define __addr_to_pcpu_ptr(addr)\t\t\t\t\t\\",
            "\t(void __percpu *)((unsigned long)(addr) -\t\t\t\\",
            "\t\t\t  (unsigned long)pcpu_base_addr\t+\t\t\\",
            "\t\t\t  (unsigned long)__per_cpu_start)",
            "#endif",
            "#ifndef __pcpu_ptr_to_addr",
            "#define __pcpu_ptr_to_addr(ptr)\t\t\t\t\t\t\\",
            "\t(void __force *)((unsigned long)(ptr) +\t\t\t\t\\",
            "\t\t\t (unsigned long)pcpu_base_addr -\t\t\\",
            "\t\t\t (unsigned long)__per_cpu_start)",
            "#endif",
            "#else\t/* CONFIG_SMP */",
            "/* on UP, it's always identity mapped */",
            "#define __addr_to_pcpu_ptr(addr)\t(void __percpu *)(addr)",
            "#define __pcpu_ptr_to_addr(ptr)\t\t(void __force *)(ptr)",
            "#endif\t/* CONFIG_SMP */",
            "",
            "static int pcpu_unit_pages __ro_after_init;",
            "static int pcpu_unit_size __ro_after_init;",
            "static int pcpu_nr_units __ro_after_init;",
            "static int pcpu_atom_size __ro_after_init;",
            "int pcpu_nr_slots __ro_after_init;",
            "static int pcpu_free_slot __ro_after_init;",
            "int pcpu_sidelined_slot __ro_after_init;",
            "int pcpu_to_depopulate_slot __ro_after_init;",
            "static size_t pcpu_chunk_struct_size __ro_after_init;",
            "",
            "/* cpus with the lowest and highest unit addresses */",
            "static unsigned int pcpu_low_unit_cpu __ro_after_init;",
            "static unsigned int pcpu_high_unit_cpu __ro_after_init;",
            "",
            "/* the address of the first chunk which starts with the kernel static area */",
            "void *pcpu_base_addr __ro_after_init;",
            "",
            "static const int *pcpu_unit_map __ro_after_init;\t\t/* cpu -> unit */",
            "const unsigned long *pcpu_unit_offsets __ro_after_init;\t/* cpu -> unit offset */",
            "",
            "/* group information, used for vm allocation */",
            "static int pcpu_nr_groups __ro_after_init;",
            "static const unsigned long *pcpu_group_offsets __ro_after_init;",
            "static const size_t *pcpu_group_sizes __ro_after_init;",
            "",
            "/*",
            " * The first chunk which always exists.  Note that unlike other",
            " * chunks, this one can be allocated and mapped in several different",
            " * ways and thus often doesn't live in the vmalloc area.",
            " */",
            "struct pcpu_chunk *pcpu_first_chunk __ro_after_init;",
            "",
            "/*",
            " * Optional reserved chunk.  This chunk reserves part of the first",
            " * chunk and serves it for reserved allocations.  When the reserved",
            " * region doesn't exist, the following variable is NULL.",
            " */",
            "struct pcpu_chunk *pcpu_reserved_chunk __ro_after_init;",
            "",
            "DEFINE_SPINLOCK(pcpu_lock);\t/* all internal data structures */",
            "static DEFINE_MUTEX(pcpu_alloc_mutex);\t/* chunk create/destroy, [de]pop, map ext */",
            "",
            "struct list_head *pcpu_chunk_lists __ro_after_init; /* chunk list slots */",
            "",
            "/*",
            " * The number of empty populated pages, protected by pcpu_lock.",
            " * The reserved chunk doesn't contribute to the count.",
            " */",
            "int pcpu_nr_empty_pop_pages;",
            "",
            "/*",
            " * The number of populated pages in use by the allocator, protected by",
            " * pcpu_lock.  This number is kept per a unit per chunk (i.e. when a page gets",
            " * allocated/deallocated, it is allocated/deallocated in all units of a chunk",
            " * and increments/decrements this count by 1).",
            " */",
            "static unsigned long pcpu_nr_populated;",
            "",
            "/*",
            " * Balance work is used to populate or destroy chunks asynchronously.  We",
            " * try to keep the number of populated free pages between",
            " * PCPU_EMPTY_POP_PAGES_LOW and HIGH for atomic allocations and at most one",
            " * empty chunk.",
            " */",
            "static void pcpu_balance_workfn(struct work_struct *work);",
            "static DECLARE_WORK(pcpu_balance_work, pcpu_balance_workfn);",
            "static bool pcpu_async_enabled __read_mostly;",
            "static bool pcpu_atomic_alloc_failed;",
            ""
          ],
          "function_name": null,
          "description": "定义了percpu内存分配器的全局变量和基础架构，包括单元映射、槽位管理、块元数据及chunk列表。核心功能是管理多CPU下的静态/动态内存分配，通过分块和位图跟踪页状态，支持NUMA拓扑下的非线性地址映射。",
          "similarity": 0.5019586086273193
        },
        {
          "chunk_id": 2,
          "file_path": "mm/percpu.c",
          "start_line": 420,
          "end_line": 524,
          "content": [
            "static void pcpu_next_fit_region(struct pcpu_chunk *chunk, int alloc_bits,",
            "\t\t\t\t int align, int *bit_off, int *bits)",
            "{",
            "\tint i = pcpu_off_to_block_index(*bit_off);",
            "\tint block_off = pcpu_off_to_block_off(*bit_off);",
            "\tstruct pcpu_block_md *block;",
            "",
            "\t*bits = 0;",
            "\tfor (block = chunk->md_blocks + i; i < pcpu_chunk_nr_blocks(chunk);",
            "\t     block++, i++) {",
            "\t\t/* handles contig area across blocks */",
            "\t\tif (*bits) {",
            "\t\t\t*bits += block->left_free;",
            "\t\t\tif (*bits >= alloc_bits)",
            "\t\t\t\treturn;",
            "\t\t\tif (block->left_free == PCPU_BITMAP_BLOCK_BITS)",
            "\t\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/* check block->contig_hint */",
            "\t\t*bits = ALIGN(block->contig_hint_start, align) -",
            "\t\t\tblock->contig_hint_start;",
            "\t\t/*",
            "\t\t * This uses the block offset to determine if this has been",
            "\t\t * checked in the prior iteration.",
            "\t\t */",
            "\t\tif (block->contig_hint &&",
            "\t\t    block->contig_hint_start >= block_off &&",
            "\t\t    block->contig_hint >= *bits + alloc_bits) {",
            "\t\t\tint start = pcpu_next_hint(block, alloc_bits);",
            "",
            "\t\t\t*bits += alloc_bits + block->contig_hint_start -",
            "\t\t\t\t start;",
            "\t\t\t*bit_off = pcpu_block_off_to_off(i, start);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\t/* reset to satisfy the second predicate above */",
            "\t\tblock_off = 0;",
            "",
            "\t\t*bit_off = ALIGN(PCPU_BITMAP_BLOCK_BITS - block->right_free,",
            "\t\t\t\t align);",
            "\t\t*bits = PCPU_BITMAP_BLOCK_BITS - *bit_off;",
            "\t\t*bit_off = pcpu_block_off_to_off(i, *bit_off);",
            "\t\tif (*bits >= alloc_bits)",
            "\t\t\treturn;",
            "\t}",
            "",
            "\t/* no valid offsets were found - fail condition */",
            "\t*bit_off = pcpu_chunk_map_bits(chunk);",
            "}",
            "static void pcpu_mem_free(void *ptr)",
            "{",
            "\tkvfree(ptr);",
            "}",
            "static void __pcpu_chunk_move(struct pcpu_chunk *chunk, int slot,",
            "\t\t\t      bool move_front)",
            "{",
            "\tif (chunk != pcpu_reserved_chunk) {",
            "\t\tif (move_front)",
            "\t\t\tlist_move(&chunk->list, &pcpu_chunk_lists[slot]);",
            "\t\telse",
            "\t\t\tlist_move_tail(&chunk->list, &pcpu_chunk_lists[slot]);",
            "\t}",
            "}",
            "static void pcpu_chunk_move(struct pcpu_chunk *chunk, int slot)",
            "{",
            "\t__pcpu_chunk_move(chunk, slot, true);",
            "}",
            "static void pcpu_chunk_relocate(struct pcpu_chunk *chunk, int oslot)",
            "{",
            "\tint nslot = pcpu_chunk_slot(chunk);",
            "",
            "\t/* leave isolated chunks in-place */",
            "\tif (chunk->isolated)",
            "\t\treturn;",
            "",
            "\tif (oslot != nslot)",
            "\t\t__pcpu_chunk_move(chunk, nslot, oslot < nslot);",
            "}",
            "static void pcpu_isolate_chunk(struct pcpu_chunk *chunk)",
            "{",
            "\tlockdep_assert_held(&pcpu_lock);",
            "",
            "\tif (!chunk->isolated) {",
            "\t\tchunk->isolated = true;",
            "\t\tpcpu_nr_empty_pop_pages -= chunk->nr_empty_pop_pages;",
            "\t}",
            "\tlist_move(&chunk->list, &pcpu_chunk_lists[pcpu_to_depopulate_slot]);",
            "}",
            "static void pcpu_reintegrate_chunk(struct pcpu_chunk *chunk)",
            "{",
            "\tlockdep_assert_held(&pcpu_lock);",
            "",
            "\tif (chunk->isolated) {",
            "\t\tchunk->isolated = false;",
            "\t\tpcpu_nr_empty_pop_pages += chunk->nr_empty_pop_pages;",
            "\t\tpcpu_chunk_relocate(chunk, -1);",
            "\t}",
            "}",
            "static inline void pcpu_update_empty_pages(struct pcpu_chunk *chunk, int nr)",
            "{",
            "\tchunk->nr_empty_pop_pages += nr;",
            "\tif (chunk != pcpu_reserved_chunk && !chunk->isolated)",
            "\t\tpcpu_nr_empty_pop_pages += nr;",
            "}"
          ],
          "function_name": "pcpu_next_fit_region, pcpu_mem_free, __pcpu_chunk_move, pcpu_chunk_move, pcpu_chunk_relocate, pcpu_isolate_chunk, pcpu_reintegrate_chunk, pcpu_update_empty_pages",
          "description": "实现chunk的动态迁移与状态管理，包含槽位重定位、隔离/重构逻辑及空闲页统计更新。核心功能是根据分配负载调整chunk分布，通过隔离机制控制内存碎片，维持系统级空闲页池平衡。",
          "similarity": 0.4864017367362976
        },
        {
          "chunk_id": 3,
          "file_path": "mm/percpu.c",
          "start_line": 615,
          "end_line": 723,
          "content": [
            "static inline bool pcpu_region_overlap(int a, int b, int x, int y)",
            "{",
            "\treturn (a < y) && (x < b);",
            "}",
            "static void pcpu_block_update(struct pcpu_block_md *block, int start, int end)",
            "{",
            "\tint contig = end - start;",
            "",
            "\tblock->first_free = min(block->first_free, start);",
            "\tif (start == 0)",
            "\t\tblock->left_free = contig;",
            "",
            "\tif (end == block->nr_bits)",
            "\t\tblock->right_free = contig;",
            "",
            "\tif (contig > block->contig_hint) {",
            "\t\t/* promote the old contig_hint to be the new scan_hint */",
            "\t\tif (start > block->contig_hint_start) {",
            "\t\t\tif (block->contig_hint > block->scan_hint) {",
            "\t\t\t\tblock->scan_hint_start =",
            "\t\t\t\t\tblock->contig_hint_start;",
            "\t\t\t\tblock->scan_hint = block->contig_hint;",
            "\t\t\t} else if (start < block->scan_hint_start) {",
            "\t\t\t\t/*",
            "\t\t\t\t * The old contig_hint == scan_hint.  But, the",
            "\t\t\t\t * new contig is larger so hold the invariant",
            "\t\t\t\t * scan_hint_start < contig_hint_start.",
            "\t\t\t\t */",
            "\t\t\t\tblock->scan_hint = 0;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tblock->scan_hint = 0;",
            "\t\t}",
            "\t\tblock->contig_hint_start = start;",
            "\t\tblock->contig_hint = contig;",
            "\t} else if (contig == block->contig_hint) {",
            "\t\tif (block->contig_hint_start &&",
            "\t\t    (!start ||",
            "\t\t     __ffs(start) > __ffs(block->contig_hint_start))) {",
            "\t\t\t/* start has a better alignment so use it */",
            "\t\t\tblock->contig_hint_start = start;",
            "\t\t\tif (start < block->scan_hint_start &&",
            "\t\t\t    block->contig_hint > block->scan_hint)",
            "\t\t\t\tblock->scan_hint = 0;",
            "\t\t} else if (start > block->scan_hint_start ||",
            "\t\t\t   block->contig_hint > block->scan_hint) {",
            "\t\t\t/*",
            "\t\t\t * Knowing contig == contig_hint, update the scan_hint",
            "\t\t\t * if it is farther than or larger than the current",
            "\t\t\t * scan_hint.",
            "\t\t\t */",
            "\t\t\tblock->scan_hint_start = start;",
            "\t\t\tblock->scan_hint = contig;",
            "\t\t}",
            "\t} else {",
            "\t\t/*",
            "\t\t * The region is smaller than the contig_hint.  So only update",
            "\t\t * the scan_hint if it is larger than or equal and farther than",
            "\t\t * the current scan_hint.",
            "\t\t */",
            "\t\tif ((start < block->contig_hint_start &&",
            "\t\t     (contig > block->scan_hint ||",
            "\t\t      (contig == block->scan_hint &&",
            "\t\t       start > block->scan_hint_start)))) {",
            "\t\t\tblock->scan_hint_start = start;",
            "\t\t\tblock->scan_hint = contig;",
            "\t\t}",
            "\t}",
            "}",
            "static void pcpu_block_update_scan(struct pcpu_chunk *chunk, int bit_off,",
            "\t\t\t\t   int bits)",
            "{",
            "\tint s_off = pcpu_off_to_block_off(bit_off);",
            "\tint e_off = s_off + bits;",
            "\tint s_index, l_bit;",
            "\tstruct pcpu_block_md *block;",
            "",
            "\tif (e_off > PCPU_BITMAP_BLOCK_BITS)",
            "\t\treturn;",
            "",
            "\ts_index = pcpu_off_to_block_index(bit_off);",
            "\tblock = chunk->md_blocks + s_index;",
            "",
            "\t/* scan backwards in case of alignment skipping free bits */",
            "\tl_bit = find_last_bit(pcpu_index_alloc_map(chunk, s_index), s_off);",
            "\ts_off = (s_off == l_bit) ? 0 : l_bit + 1;",
            "",
            "\tpcpu_block_update(block, s_off, e_off);",
            "}",
            "static void pcpu_chunk_refresh_hint(struct pcpu_chunk *chunk, bool full_scan)",
            "{",
            "\tstruct pcpu_block_md *chunk_md = &chunk->chunk_md;",
            "\tint bit_off, bits;",
            "",
            "\t/* promote scan_hint to contig_hint */",
            "\tif (!full_scan && chunk_md->scan_hint) {",
            "\t\tbit_off = chunk_md->scan_hint_start + chunk_md->scan_hint;",
            "\t\tchunk_md->contig_hint_start = chunk_md->scan_hint_start;",
            "\t\tchunk_md->contig_hint = chunk_md->scan_hint;",
            "\t\tchunk_md->scan_hint = 0;",
            "\t} else {",
            "\t\tbit_off = chunk_md->first_free;",
            "\t\tchunk_md->contig_hint = 0;",
            "\t}",
            "",
            "\tbits = 0;",
            "\tpcpu_for_each_md_free_region(chunk, bit_off, bits)",
            "\t\tpcpu_block_update(chunk_md, bit_off, bit_off + bits);",
            "}"
          ],
          "function_name": "pcpu_region_overlap, pcpu_block_update, pcpu_block_update_scan, pcpu_chunk_refresh_hint",
          "description": "负责块级元数据更新与hint刷新，检测分配区域覆盖关系并更新连续空闲记录。核心功能是维护块内空闲区的contig_hint和scan_hint，通过比对新旧分配范围自动调整元数据，提升后续分配效率。",
          "similarity": 0.4847109913825989
        },
        {
          "chunk_id": 4,
          "file_path": "mm/percpu.c",
          "start_line": 774,
          "end_line": 932,
          "content": [
            "static void pcpu_block_refresh_hint(struct pcpu_chunk *chunk, int index)",
            "{",
            "\tstruct pcpu_block_md *block = chunk->md_blocks + index;",
            "\tunsigned long *alloc_map = pcpu_index_alloc_map(chunk, index);",
            "\tunsigned int start, end;\t/* region start, region end */",
            "",
            "\t/* promote scan_hint to contig_hint */",
            "\tif (block->scan_hint) {",
            "\t\tstart = block->scan_hint_start + block->scan_hint;",
            "\t\tblock->contig_hint_start = block->scan_hint_start;",
            "\t\tblock->contig_hint = block->scan_hint;",
            "\t\tblock->scan_hint = 0;",
            "\t} else {",
            "\t\tstart = block->first_free;",
            "\t\tblock->contig_hint = 0;",
            "\t}",
            "",
            "\tblock->right_free = 0;",
            "",
            "\t/* iterate over free areas and update the contig hints */",
            "\tfor_each_clear_bitrange_from(start, end, alloc_map, PCPU_BITMAP_BLOCK_BITS)",
            "\t\tpcpu_block_update(block, start, end);",
            "}",
            "static void pcpu_block_update_hint_alloc(struct pcpu_chunk *chunk, int bit_off,",
            "\t\t\t\t\t int bits)",
            "{",
            "\tstruct pcpu_block_md *chunk_md = &chunk->chunk_md;",
            "\tint nr_empty_pages = 0;",
            "\tstruct pcpu_block_md *s_block, *e_block, *block;",
            "\tint s_index, e_index;\t/* block indexes of the freed allocation */",
            "\tint s_off, e_off;\t/* block offsets of the freed allocation */",
            "",
            "\t/*",
            "\t * Calculate per block offsets.",
            "\t * The calculation uses an inclusive range, but the resulting offsets",
            "\t * are [start, end).  e_index always points to the last block in the",
            "\t * range.",
            "\t */",
            "\ts_index = pcpu_off_to_block_index(bit_off);",
            "\te_index = pcpu_off_to_block_index(bit_off + bits - 1);",
            "\ts_off = pcpu_off_to_block_off(bit_off);",
            "\te_off = pcpu_off_to_block_off(bit_off + bits - 1) + 1;",
            "",
            "\ts_block = chunk->md_blocks + s_index;",
            "\te_block = chunk->md_blocks + e_index;",
            "",
            "\t/*",
            "\t * Update s_block.",
            "\t */",
            "\tif (s_block->contig_hint == PCPU_BITMAP_BLOCK_BITS)",
            "\t\tnr_empty_pages++;",
            "",
            "\t/*",
            "\t * block->first_free must be updated if the allocation takes its place.",
            "\t * If the allocation breaks the contig_hint, a scan is required to",
            "\t * restore this hint.",
            "\t */",
            "\tif (s_off == s_block->first_free)",
            "\t\ts_block->first_free = find_next_zero_bit(",
            "\t\t\t\t\tpcpu_index_alloc_map(chunk, s_index),",
            "\t\t\t\t\tPCPU_BITMAP_BLOCK_BITS,",
            "\t\t\t\t\ts_off + bits);",
            "",
            "\tif (pcpu_region_overlap(s_block->scan_hint_start,",
            "\t\t\t\ts_block->scan_hint_start + s_block->scan_hint,",
            "\t\t\t\ts_off,",
            "\t\t\t\ts_off + bits))",
            "\t\ts_block->scan_hint = 0;",
            "",
            "\tif (pcpu_region_overlap(s_block->contig_hint_start,",
            "\t\t\t\ts_block->contig_hint_start +",
            "\t\t\t\ts_block->contig_hint,",
            "\t\t\t\ts_off,",
            "\t\t\t\ts_off + bits)) {",
            "\t\t/* block contig hint is broken - scan to fix it */",
            "\t\tif (!s_off)",
            "\t\t\ts_block->left_free = 0;",
            "\t\tpcpu_block_refresh_hint(chunk, s_index);",
            "\t} else {",
            "\t\t/* update left and right contig manually */",
            "\t\ts_block->left_free = min(s_block->left_free, s_off);",
            "\t\tif (s_index == e_index)",
            "\t\t\ts_block->right_free = min_t(int, s_block->right_free,",
            "\t\t\t\t\tPCPU_BITMAP_BLOCK_BITS - e_off);",
            "\t\telse",
            "\t\t\ts_block->right_free = 0;",
            "\t}",
            "",
            "\t/*",
            "\t * Update e_block.",
            "\t */",
            "\tif (s_index != e_index) {",
            "\t\tif (e_block->contig_hint == PCPU_BITMAP_BLOCK_BITS)",
            "\t\t\tnr_empty_pages++;",
            "",
            "\t\t/*",
            "\t\t * When the allocation is across blocks, the end is along",
            "\t\t * the left part of the e_block.",
            "\t\t */",
            "\t\te_block->first_free = find_next_zero_bit(",
            "\t\t\t\tpcpu_index_alloc_map(chunk, e_index),",
            "\t\t\t\tPCPU_BITMAP_BLOCK_BITS, e_off);",
            "",
            "\t\tif (e_off == PCPU_BITMAP_BLOCK_BITS) {",
            "\t\t\t/* reset the block */",
            "\t\t\te_block++;",
            "\t\t} else {",
            "\t\t\tif (e_off > e_block->scan_hint_start)",
            "\t\t\t\te_block->scan_hint = 0;",
            "",
            "\t\t\te_block->left_free = 0;",
            "\t\t\tif (e_off > e_block->contig_hint_start) {",
            "\t\t\t\t/* contig hint is broken - scan to fix it */",
            "\t\t\t\tpcpu_block_refresh_hint(chunk, e_index);",
            "\t\t\t} else {",
            "\t\t\t\te_block->right_free =",
            "\t\t\t\t\tmin_t(int, e_block->right_free,",
            "\t\t\t\t\t      PCPU_BITMAP_BLOCK_BITS - e_off);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* update in-between md_blocks */",
            "\t\tnr_empty_pages += (e_index - s_index - 1);",
            "\t\tfor (block = s_block + 1; block < e_block; block++) {",
            "\t\t\tblock->scan_hint = 0;",
            "\t\t\tblock->contig_hint = 0;",
            "\t\t\tblock->left_free = 0;",
            "\t\t\tblock->right_free = 0;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * If the allocation is not atomic, some blocks may not be",
            "\t * populated with pages, while we account it here.  The number",
            "\t * of pages will be added back with pcpu_chunk_populated()",
            "\t * when populating pages.",
            "\t */",
            "\tif (nr_empty_pages)",
            "\t\tpcpu_update_empty_pages(chunk, -nr_empty_pages);",
            "",
            "\tif (pcpu_region_overlap(chunk_md->scan_hint_start,",
            "\t\t\t\tchunk_md->scan_hint_start +",
            "\t\t\t\tchunk_md->scan_hint,",
            "\t\t\t\tbit_off,",
            "\t\t\t\tbit_off + bits))",
            "\t\tchunk_md->scan_hint = 0;",
            "",
            "\t/*",
            "\t * The only time a full chunk scan is required is if the chunk",
            "\t * contig hint is broken.  Otherwise, it means a smaller space",
            "\t * was used and therefore the chunk contig hint is still correct.",
            "\t */",
            "\tif (pcpu_region_overlap(chunk_md->contig_hint_start,",
            "\t\t\t\tchunk_md->contig_hint_start +",
            "\t\t\t\tchunk_md->contig_hint,",
            "\t\t\t\tbit_off,",
            "\t\t\t\tbit_off + bits))",
            "\t\tpcpu_chunk_refresh_hint(chunk, false);",
            "}"
          ],
          "function_name": "pcpu_block_refresh_hint, pcpu_block_update_hint_alloc",
          "description": "深度处理块级元数据更新细节，精确追踪分配操作对hint的影响。核心功能是在跨块分配时修复碎片化记录，通过遍历受影响块并重置相关元数据字段，确保hint系统能正确反映当前空闲格局。",
          "similarity": 0.46648162603378296
        },
        {
          "chunk_id": 6,
          "file_path": "mm/percpu.c",
          "start_line": 1110,
          "end_line": 1222,
          "content": [
            "static int pcpu_find_block_fit(struct pcpu_chunk *chunk, int alloc_bits,",
            "\t\t\t       size_t align, bool pop_only)",
            "{",
            "\tstruct pcpu_block_md *chunk_md = &chunk->chunk_md;",
            "\tint bit_off, bits, next_off;",
            "",
            "\t/*",
            "\t * This is an optimization to prevent scanning by assuming if the",
            "\t * allocation cannot fit in the global hint, there is memory pressure",
            "\t * and creating a new chunk would happen soon.",
            "\t */",
            "\tif (!pcpu_check_block_hint(chunk_md, alloc_bits, align))",
            "\t\treturn -1;",
            "",
            "\tbit_off = pcpu_next_hint(chunk_md, alloc_bits);",
            "\tbits = 0;",
            "\tpcpu_for_each_fit_region(chunk, alloc_bits, align, bit_off, bits) {",
            "\t\tif (!pop_only || pcpu_is_populated(chunk, bit_off, bits,",
            "\t\t\t\t\t\t   &next_off))",
            "\t\t\tbreak;",
            "",
            "\t\tbit_off = next_off;",
            "\t\tbits = 0;",
            "\t}",
            "",
            "\tif (bit_off == pcpu_chunk_map_bits(chunk))",
            "\t\treturn -1;",
            "",
            "\treturn bit_off;",
            "}",
            "static unsigned long pcpu_find_zero_area(unsigned long *map,",
            "\t\t\t\t\t unsigned long size,",
            "\t\t\t\t\t unsigned long start,",
            "\t\t\t\t\t unsigned long nr,",
            "\t\t\t\t\t unsigned long align_mask,",
            "\t\t\t\t\t unsigned long *largest_off,",
            "\t\t\t\t\t unsigned long *largest_bits)",
            "{",
            "\tunsigned long index, end, i, area_off, area_bits;",
            "again:",
            "\tindex = find_next_zero_bit(map, size, start);",
            "",
            "\t/* Align allocation */",
            "\tindex = __ALIGN_MASK(index, align_mask);",
            "\tarea_off = index;",
            "",
            "\tend = index + nr;",
            "\tif (end > size)",
            "\t\treturn end;",
            "\ti = find_next_bit(map, end, index);",
            "\tif (i < end) {",
            "\t\tarea_bits = i - area_off;",
            "\t\t/* remember largest unused area with best alignment */",
            "\t\tif (area_bits > *largest_bits ||",
            "\t\t    (area_bits == *largest_bits && *largest_off &&",
            "\t\t     (!area_off || __ffs(area_off) > __ffs(*largest_off)))) {",
            "\t\t\t*largest_off = area_off;",
            "\t\t\t*largest_bits = area_bits;",
            "\t\t}",
            "",
            "\t\tstart = i + 1;",
            "\t\tgoto again;",
            "\t}",
            "\treturn index;",
            "}",
            "static int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,",
            "\t\t\t   size_t align, int start)",
            "{",
            "\tstruct pcpu_block_md *chunk_md = &chunk->chunk_md;",
            "\tsize_t align_mask = (align) ? (align - 1) : 0;",
            "\tunsigned long area_off = 0, area_bits = 0;",
            "\tint bit_off, end, oslot;",
            "",
            "\tlockdep_assert_held(&pcpu_lock);",
            "",
            "\toslot = pcpu_chunk_slot(chunk);",
            "",
            "\t/*",
            "\t * Search to find a fit.",
            "\t */",
            "\tend = min_t(int, start + alloc_bits + PCPU_BITMAP_BLOCK_BITS,",
            "\t\t    pcpu_chunk_map_bits(chunk));",
            "\tbit_off = pcpu_find_zero_area(chunk->alloc_map, end, start, alloc_bits,",
            "\t\t\t\t      align_mask, &area_off, &area_bits);",
            "\tif (bit_off >= end)",
            "\t\treturn -1;",
            "",
            "\tif (area_bits)",
            "\t\tpcpu_block_update_scan(chunk, area_off, area_bits);",
            "",
            "\t/* update alloc map */",
            "\tbitmap_set(chunk->alloc_map, bit_off, alloc_bits);",
            "",
            "\t/* update boundary map */",
            "\tset_bit(bit_off, chunk->bound_map);",
            "\tbitmap_clear(chunk->bound_map, bit_off + 1, alloc_bits - 1);",
            "\tset_bit(bit_off + alloc_bits, chunk->bound_map);",
            "",
            "\tchunk->free_bytes -= alloc_bits * PCPU_MIN_ALLOC_SIZE;",
            "",
            "\t/* update first free bit */",
            "\tif (bit_off == chunk_md->first_free)",
            "\t\tchunk_md->first_free = find_next_zero_bit(",
            "\t\t\t\t\tchunk->alloc_map,",
            "\t\t\t\t\tpcpu_chunk_map_bits(chunk),",
            "\t\t\t\t\tbit_off + alloc_bits);",
            "",
            "\tpcpu_block_update_hint_alloc(chunk, bit_off, alloc_bits);",
            "",
            "\tpcpu_chunk_relocate(chunk, oslot);",
            "",
            "\treturn bit_off * PCPU_MIN_ALLOC_SIZE;",
            "}"
          ],
          "function_name": "pcpu_find_block_fit, pcpu_find_zero_area, pcpu_alloc_area",
          "description": "该代码段实现了 PER-CPU 内存块的动态分配逻辑，包含查找适配区域、零区搜索和实际分配三个阶段。  \npcpu_find_block_fit 通过优化策略筛选可用块位置，pcpu_find_zero_area 寻找对齐的连续空闲区域，pcpu_alloc_area 执行实际分配并维护分配/边界映射表。  \n代码存在上下文不完整问题，关键辅助函数如 pcpu_check_block_hint 的具体实现未在片段中展现。",
          "similarity": 0.44713276624679565
        }
      ]
    },
    {
      "source_file": "kernel/sched/psi.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:11\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\psi.c`\n\n---\n\n# `sched/psi.c` 技术文档\n\n## 1. 文件概述\n\n`sched/psi.c` 实现了 **压力失速信息**（Pressure Stall Information, PSI）机制，用于监控和量化系统在 CPU、内存和 I/O 资源上的争用压力。该机制通过测量任务因资源不足而延迟执行的时间比例，提供两类关键指标：\n\n- **SOME**：表示至少有一个任务因资源争用而延迟，反映工作负载性能下降。\n- **FULL**：表示所有非空闲任务均被阻塞，导致 CPU 完全无法推进工作，反映资源利用率损失。\n\nPSI 为系统管理员和容器运行时（如 cgroup）提供细粒度的资源压力感知能力，用于负载调度、自动扩缩容和性能调优。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- `struct psi_group`  \n  表示一个 PSI 监控组（如系统级或 cgroup 级），包含：\n  - 每 CPU 的状态统计（`pcpu`）\n  - 压力平均值（10s/60s/300s）\n  - 触发器列表（用于通知用户空间）\n  - 定时器与工作队列（用于周期性更新）\n\n- `struct psi_group_cpu`  \n  每 CPU 的 PSI 状态快照，记录：\n  - 延迟任务数（`nr_delayed`）\n  - 有效任务数（`nr_productive`）\n  - 非空闲任务数（`nr_nonidle`）\n  - 各状态的累计时间（`tstamp`、`state_start` 等）\n\n- `psi_system`  \n  全局系统级 PSI 组实例。\n\n### 主要函数与机制\n\n- `psi_write_begin()` / `psi_write_end()`  \n  使用 `seqcount_t` 保护每 CPU PSI 状态写入，确保读取一致性。\n\n- `psi_read_begin()` / `psi_read_retry()`  \n  提供无锁读取接口，配合 seqcount 实现安全的跨 CPU 状态聚合。\n\n- `group_init()`  \n  初始化 PSI 组，包括锁、触发器链表、延迟工作队列等。\n\n- `psi_avgs_work()`  \n  延迟工作函数，用于计算并更新滑动窗口下的压力平均值（10s/60s/300s）。\n\n- `poll_timer_fn()`  \n  轮询定时器回调，支持用户空间通过轮询方式获取 PSI 更新。\n\n- `setup_psi()`  \n  内核启动参数 `psi=` 的解析函数，用于动态启用/禁用 PSI。\n\n### 全局变量\n\n- `psi_disabled` / `psi_cgroups_enabled`  \n  静态分支预测键，用于在编译时或运行时优化 PSI 路径。\n\n- `psi_enable`  \n  控制 PSI 是否默认启用（受 `CONFIG_PSI_DEFAULT_DISABLED` 影响）。\n\n- `psi_period`  \n  PSI 采样周期（单位：纳秒），默认为 2 秒（`PSI_FREQ = 2*HZ+1`）。\n\n- `EXP_10s` / `EXP_60s` / `EXP_300s`  \n  指数加权移动平均（EWMA）的衰减系数，用于计算不同时间窗口的压力均值。\n\n## 3. 关键实现\n\n### 压力模型\n\nPSI 基于 **执行潜力损失** 模型：\n- **SOME** = `min(nr_delayed / threads, 1)`  \n- **FULL** = `(threads - min(nr_productive, threads)) / threads`  \n其中 `threads = min(nr_nonidle_tasks, nr_cpus)`，反映系统实际可并行执行的线程数。\n\n### 多 CPU 聚合策略\n\n为避免全局锁开销，PSI 采用 **每 CPU 局部统计 + 周期性聚合** 策略：\n- 每个 runqueue 独立记录 `tSOME[cpu]`、`tFULL[cpu]` 和 `tNONIDLE[cpu]`\n- 聚合时加权平均：  \n  `tSOME = Σ(tSOME[i] * tNONIDLE[i]) / Σ(tNONIDLE[i])`  \n  该方法在低开销下逼近真实全局压力。\n\n### 无锁读取与一致性\n\n使用 `seqcount_t` 机制：\n- 写入时调用 `write_seqcount_begin/end()`\n- 读取时通过 `read_seqcount_begin/retry()` 检测写冲突，必要时重试\n- 避免读写锁，提升高并发下的性能\n\n### 平均值计算\n\n采用 **指数加权移动平均**（EWMA）计算 10s/60s/300s 压力均值：\n- 每 2 秒更新一次（`PSI_FREQ`）\n- 衰减因子预计算为定点整数（如 `EXP_10s = 1677` 对应 `1/exp(2/10)`）\n\n### 触发器支持\n\n支持两类触发器：\n- **平均值触发器**（`avg_triggers`）：当某窗口平均压力超过阈值时通知\n- **实时轮询触发器**（`rtpoll_triggers`）：支持用户空间高效轮询\n\n## 4. 依赖关系\n\n- **调度器核心**（`kernel/sched/`）  \n  PSI 深度集成于 CFS 调度器，在任务入队/出队、睡眠/唤醒等路径调用 PSI 接口更新状态。\n\n- **cgroup 子系统**  \n  PSI 为每个支持的 cgroup（如 cpu、memory）创建独立的 `psi_group`，实现容器级资源压力监控。\n\n- **时间子系统**  \n  依赖 `sched_clock()` 获取高精度时间戳，用于状态持续时间计算。\n\n- **工作队列与定时器**  \n  使用 `delayed_work` 和 `timer_list` 实现异步平均值更新和轮询通知。\n\n- **配置选项**  \n  由 `CONFIG_PSI` 控制编译，`CONFIG_PSI_DEFAULT_DISABLED` 控制默认启用状态。\n\n## 5. 使用场景\n\n- **系统监控工具**（如 `pressure` 文件）  \n  用户可通过 `/proc/pressure/{cpu,memory,io}` 读取系统级 PSI 数据。\n\n- **容器运行时**（如 Docker、Kubernetes）  \n  通过 cgroup v2 的 `memory.pressure`、`cpu.pressure` 等接口获取容器内压力指标，用于弹性伸缩或驱逐决策。\n\n- **内核自适应调度**  \n  PSI 数据可被其他子系统（如内存回收、负载均衡）用作反馈信号，优化资源分配。\n\n- **性能分析与调优**  \n  开发者利用 PSI 识别资源瓶颈（如内存回收导致的 FULL stall），定位性能下降根因。",
      "similarity": 0.48560601472854614,
      "chunks": [
        {
          "chunk_id": 10,
          "file_path": "kernel/sched/psi.c",
          "start_line": 1506,
          "end_line": 1607,
          "content": [
            "static int psi_io_show(struct seq_file *m, void *v)",
            "{",
            "\treturn psi_show(m, &psi_system, PSI_IO);",
            "}",
            "static int psi_memory_show(struct seq_file *m, void *v)",
            "{",
            "\treturn psi_show(m, &psi_system, PSI_MEM);",
            "}",
            "static int psi_cpu_show(struct seq_file *m, void *v)",
            "{",
            "\treturn psi_show(m, &psi_system, PSI_CPU);",
            "}",
            "static int psi_io_open(struct inode *inode, struct file *file)",
            "{",
            "\treturn single_open(file, psi_io_show, NULL);",
            "}",
            "static int psi_memory_open(struct inode *inode, struct file *file)",
            "{",
            "\treturn single_open(file, psi_memory_show, NULL);",
            "}",
            "static int psi_cpu_open(struct inode *inode, struct file *file)",
            "{",
            "\treturn single_open(file, psi_cpu_show, NULL);",
            "}",
            "static ssize_t psi_write(struct file *file, const char __user *user_buf,",
            "\t\t\t size_t nbytes, enum psi_res res)",
            "{",
            "\tchar buf[32];",
            "\tsize_t buf_size;",
            "\tstruct seq_file *seq;",
            "\tstruct psi_trigger *new;",
            "",
            "\tif (static_branch_likely(&psi_disabled))",
            "\t\treturn -EOPNOTSUPP;",
            "",
            "\tif (!nbytes)",
            "\t\treturn -EINVAL;",
            "",
            "\tbuf_size = min(nbytes, sizeof(buf));",
            "\tif (copy_from_user(buf, user_buf, buf_size))",
            "\t\treturn -EFAULT;",
            "",
            "\tbuf[buf_size - 1] = '\\0';",
            "",
            "\tseq = file->private_data;",
            "",
            "\t/* Take seq->lock to protect seq->private from concurrent writes */",
            "\tmutex_lock(&seq->lock);",
            "",
            "\t/* Allow only one trigger per file descriptor */",
            "\tif (seq->private) {",
            "\t\tmutex_unlock(&seq->lock);",
            "\t\treturn -EBUSY;",
            "\t}",
            "",
            "\tnew = psi_trigger_create(&psi_system, buf, res, file, NULL);",
            "\tif (IS_ERR(new)) {",
            "\t\tmutex_unlock(&seq->lock);",
            "\t\treturn PTR_ERR(new);",
            "\t}",
            "",
            "\tsmp_store_release(&seq->private, new);",
            "\tmutex_unlock(&seq->lock);",
            "",
            "\treturn nbytes;",
            "}",
            "static ssize_t psi_io_write(struct file *file, const char __user *user_buf,",
            "\t\t\t    size_t nbytes, loff_t *ppos)",
            "{",
            "\treturn psi_write(file, user_buf, nbytes, PSI_IO);",
            "}",
            "static ssize_t psi_memory_write(struct file *file, const char __user *user_buf,",
            "\t\t\t\tsize_t nbytes, loff_t *ppos)",
            "{",
            "\treturn psi_write(file, user_buf, nbytes, PSI_MEM);",
            "}",
            "static ssize_t psi_cpu_write(struct file *file, const char __user *user_buf,",
            "\t\t\t     size_t nbytes, loff_t *ppos)",
            "{",
            "\treturn psi_write(file, user_buf, nbytes, PSI_CPU);",
            "}",
            "static __poll_t psi_fop_poll(struct file *file, poll_table *wait)",
            "{",
            "\tstruct seq_file *seq = file->private_data;",
            "",
            "\treturn psi_trigger_poll(&seq->private, file, wait);",
            "}",
            "static int psi_fop_release(struct inode *inode, struct file *file)",
            "{",
            "\tstruct seq_file *seq = file->private_data;",
            "",
            "\tpsi_trigger_destroy(seq->private);",
            "\treturn single_release(inode, file);",
            "}",
            "static int psi_irq_show(struct seq_file *m, void *v)",
            "{",
            "\treturn psi_show(m, &psi_system, PSI_IRQ);",
            "}",
            "static int psi_irq_open(struct inode *inode, struct file *file)",
            "{",
            "\treturn single_open(file, psi_irq_show, NULL);",
            "}"
          ],
          "function_name": "psi_io_show, psi_memory_show, psi_cpu_show, psi_io_open, psi_memory_open, psi_cpu_open, psi_write, psi_io_write, psi_memory_write, psi_cpu_write, psi_fop_poll, psi_fop_release, psi_irq_show, psi_irq_open",
          "description": "该代码段实现了PSI（Pressure Stall Information）子系统的文件操作接口，用于暴露和操控不同维度（IO、Memory、CPU、IRQ）的压力状态。各_show函数通过psi_show生成对应类型的统计信息，_open函数注册了单次打开逻辑以支持序列化读取，_write系列函数接收用户输入并创建psi_trigger对象以触发电压事件。由于缺少psi_show及psi_trigger相关实现，上下文存在缺失。",
          "similarity": 0.43281078338623047
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/psi.c",
          "start_line": 338,
          "end_line": 442,
          "content": [
            "static void calc_avgs(unsigned long avg[3], int missed_periods,",
            "\t\t      u64 time, u64 period)",
            "{",
            "\tunsigned long pct;",
            "",
            "\t/* Fill in zeroes for periods of no activity */",
            "\tif (missed_periods) {",
            "\t\tavg[0] = calc_load_n(avg[0], EXP_10s, 0, missed_periods);",
            "\t\tavg[1] = calc_load_n(avg[1], EXP_60s, 0, missed_periods);",
            "\t\tavg[2] = calc_load_n(avg[2], EXP_300s, 0, missed_periods);",
            "\t}",
            "",
            "\t/* Sample the most recent active period */",
            "\tpct = div_u64(time * 100, period);",
            "\tpct *= FIXED_1;",
            "\tavg[0] = calc_load(avg[0], EXP_10s, pct);",
            "\tavg[1] = calc_load(avg[1], EXP_60s, pct);",
            "\tavg[2] = calc_load(avg[2], EXP_300s, pct);",
            "}",
            "static void collect_percpu_times(struct psi_group *group,",
            "\t\t\t\t enum psi_aggregators aggregator,",
            "\t\t\t\t u32 *pchanged_states)",
            "{",
            "\tu64 deltas[NR_PSI_STATES - 1] = { 0, };",
            "\tunsigned long nonidle_total = 0;",
            "\tu32 changed_states = 0;",
            "\tint cpu;",
            "\tint s;",
            "",
            "\t/*",
            "\t * Collect the per-cpu time buckets and average them into a",
            "\t * single time sample that is normalized to wallclock time.",
            "\t *",
            "\t * For averaging, each CPU is weighted by its non-idle time in",
            "\t * the sampling period. This eliminates artifacts from uneven",
            "\t * loading, or even entirely idle CPUs.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tu32 times[NR_PSI_STATES];",
            "\t\tu32 nonidle;",
            "\t\tu32 cpu_changed_states;",
            "",
            "\t\tget_recent_times(group, cpu, aggregator, times,",
            "\t\t\t\t&cpu_changed_states);",
            "\t\tchanged_states |= cpu_changed_states;",
            "",
            "\t\tnonidle = nsecs_to_jiffies(times[PSI_NONIDLE]);",
            "\t\tnonidle_total += nonidle;",
            "",
            "\t\tfor (s = 0; s < PSI_NONIDLE; s++)",
            "\t\t\tdeltas[s] += (u64)times[s] * nonidle;",
            "\t}",
            "",
            "\t/*",
            "\t * Integrate the sample into the running statistics that are",
            "\t * reported to userspace: the cumulative stall times and the",
            "\t * decaying averages.",
            "\t *",
            "\t * Pressure percentages are sampled at PSI_FREQ. We might be",
            "\t * called more often when the user polls more frequently than",
            "\t * that; we might be called less often when there is no task",
            "\t * activity, thus no data, and clock ticks are sporadic. The",
            "\t * below handles both.",
            "\t */",
            "",
            "\t/* total= */",
            "\tfor (s = 0; s < NR_PSI_STATES - 1; s++)",
            "\t\tgroup->total[aggregator][s] +=",
            "\t\t\t\tdiv_u64(deltas[s], max(nonidle_total, 1UL));",
            "",
            "\tif (pchanged_states)",
            "\t\t*pchanged_states = changed_states;",
            "}",
            "static void window_reset(struct psi_window *win, u64 now, u64 value,",
            "\t\t\t u64 prev_growth)",
            "{",
            "\twin->start_time = now;",
            "\twin->start_value = value;",
            "\twin->prev_growth = prev_growth;",
            "}",
            "static u64 window_update(struct psi_window *win, u64 now, u64 value)",
            "{",
            "\tu64 elapsed;",
            "\tu64 growth;",
            "",
            "\telapsed = now - win->start_time;",
            "\tgrowth = value - win->start_value;",
            "\t/*",
            "\t * After each tracking window passes win->start_value and",
            "\t * win->start_time get reset and win->prev_growth stores",
            "\t * the average per-window growth of the previous window.",
            "\t * win->prev_growth is then used to interpolate additional",
            "\t * growth from the previous window assuming it was linear.",
            "\t */",
            "\tif (elapsed > win->size)",
            "\t\twindow_reset(win, now, value, growth);",
            "\telse {",
            "\t\tu32 remaining;",
            "",
            "\t\tremaining = win->size - elapsed;",
            "\t\tgrowth += div64_u64(win->prev_growth * remaining, win->size);",
            "\t}",
            "",
            "\treturn growth;",
            "}"
          ],
          "function_name": "calc_avgs, collect_percpu_times, window_reset, window_update",
          "description": "计算加权平均压力值，汇总各CPU的统计时间并归一化到总时间，维护时间窗口增长记录以防止数据溢出。",
          "similarity": 0.42650049924850464
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/psi.c",
          "start_line": 1,
          "end_line": 149,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Pressure stall information for CPU, memory and IO",
            " *",
            " * Copyright (c) 2018 Facebook, Inc.",
            " * Author: Johannes Weiner <hannes@cmpxchg.org>",
            " *",
            " * Polling support by Suren Baghdasaryan <surenb@google.com>",
            " * Copyright (c) 2018 Google, Inc.",
            " *",
            " * When CPU, memory and IO are contended, tasks experience delays that",
            " * reduce throughput and introduce latencies into the workload. Memory",
            " * and IO contention, in addition, can cause a full loss of forward",
            " * progress in which the CPU goes idle.",
            " *",
            " * This code aggregates individual task delays into resource pressure",
            " * metrics that indicate problems with both workload health and",
            " * resource utilization.",
            " *",
            " *\t\t\tModel",
            " *",
            " * The time in which a task can execute on a CPU is our baseline for",
            " * productivity. Pressure expresses the amount of time in which this",
            " * potential cannot be realized due to resource contention.",
            " *",
            " * This concept of productivity has two components: the workload and",
            " * the CPU. To measure the impact of pressure on both, we define two",
            " * contention states for a resource: SOME and FULL.",
            " *",
            " * In the SOME state of a given resource, one or more tasks are",
            " * delayed on that resource. This affects the workload's ability to",
            " * perform work, but the CPU may still be executing other tasks.",
            " *",
            " * In the FULL state of a given resource, all non-idle tasks are",
            " * delayed on that resource such that nobody is advancing and the CPU",
            " * goes idle. This leaves both workload and CPU unproductive.",
            " *",
            " *\tSOME = nr_delayed_tasks != 0",
            " *\tFULL = nr_delayed_tasks != 0 && nr_productive_tasks == 0",
            " *",
            " * What it means for a task to be productive is defined differently",
            " * for each resource. For IO, productive means a running task. For",
            " * memory, productive means a running task that isn't a reclaimer. For",
            " * CPU, productive means an oncpu task.",
            " *",
            " * Naturally, the FULL state doesn't exist for the CPU resource at the",
            " * system level, but exist at the cgroup level. At the cgroup level,",
            " * FULL means all non-idle tasks in the cgroup are delayed on the CPU",
            " * resource which is being used by others outside of the cgroup or",
            " * throttled by the cgroup cpu.max configuration.",
            " *",
            " * The percentage of wallclock time spent in those compound stall",
            " * states gives pressure numbers between 0 and 100 for each resource,",
            " * where the SOME percentage indicates workload slowdowns and the FULL",
            " * percentage indicates reduced CPU utilization:",
            " *",
            " *\t%SOME = time(SOME) / period",
            " *\t%FULL = time(FULL) / period",
            " *",
            " *\t\t\tMultiple CPUs",
            " *",
            " * The more tasks and available CPUs there are, the more work can be",
            " * performed concurrently. This means that the potential that can go",
            " * unrealized due to resource contention *also* scales with non-idle",
            " * tasks and CPUs.",
            " *",
            " * Consider a scenario where 257 number crunching tasks are trying to",
            " * run concurrently on 256 CPUs. If we simply aggregated the task",
            " * states, we would have to conclude a CPU SOME pressure number of",
            " * 100%, since *somebody* is waiting on a runqueue at all",
            " * times. However, that is clearly not the amount of contention the",
            " * workload is experiencing: only one out of 256 possible execution",
            " * threads will be contended at any given time, or about 0.4%.",
            " *",
            " * Conversely, consider a scenario of 4 tasks and 4 CPUs where at any",
            " * given time *one* of the tasks is delayed due to a lack of memory.",
            " * Again, looking purely at the task state would yield a memory FULL",
            " * pressure number of 0%, since *somebody* is always making forward",
            " * progress. But again this wouldn't capture the amount of execution",
            " * potential lost, which is 1 out of 4 CPUs, or 25%.",
            " *",
            " * To calculate wasted potential (pressure) with multiple processors,",
            " * we have to base our calculation on the number of non-idle tasks in",
            " * conjunction with the number of available CPUs, which is the number",
            " * of potential execution threads. SOME becomes then the proportion of",
            " * delayed tasks to possible threads, and FULL is the share of possible",
            " * threads that are unproductive due to delays:",
            " *",
            " *\tthreads = min(nr_nonidle_tasks, nr_cpus)",
            " *\t   SOME = min(nr_delayed_tasks / threads, 1)",
            " *\t   FULL = (threads - min(nr_productive_tasks, threads)) / threads",
            " *",
            " * For the 257 number crunchers on 256 CPUs, this yields:",
            " *",
            " *\tthreads = min(257, 256)",
            " *\t   SOME = min(1 / 256, 1)             = 0.4%",
            " *\t   FULL = (256 - min(256, 256)) / 256 = 0%",
            " *",
            " * For the 1 out of 4 memory-delayed tasks, this yields:",
            " *",
            " *\tthreads = min(4, 4)",
            " *\t   SOME = min(1 / 4, 1)               = 25%",
            " *\t   FULL = (4 - min(3, 4)) / 4         = 25%",
            " *",
            " * [ Substitute nr_cpus with 1, and you can see that it's a natural",
            " *   extension of the single-CPU model. ]",
            " *",
            " *\t\t\tImplementation",
            " *",
            " * To assess the precise time spent in each such state, we would have",
            " * to freeze the system on task changes and start/stop the state",
            " * clocks accordingly. Obviously that doesn't scale in practice.",
            " *",
            " * Because the scheduler aims to distribute the compute load evenly",
            " * among the available CPUs, we can track task state locally to each",
            " * CPU and, at much lower frequency, extrapolate the global state for",
            " * the cumulative stall times and the running averages.",
            " *",
            " * For each runqueue, we track:",
            " *",
            " *\t   tSOME[cpu] = time(nr_delayed_tasks[cpu] != 0)",
            " *\t   tFULL[cpu] = time(nr_delayed_tasks[cpu] && !nr_productive_tasks[cpu])",
            " *\ttNONIDLE[cpu] = time(nr_nonidle_tasks[cpu] != 0)",
            " *",
            " * and then periodically aggregate:",
            " *",
            " *\ttNONIDLE = sum(tNONIDLE[i])",
            " *",
            " *\t   tSOME = sum(tSOME[i] * tNONIDLE[i]) / tNONIDLE",
            " *\t   tFULL = sum(tFULL[i] * tNONIDLE[i]) / tNONIDLE",
            " *",
            " *\t   %SOME = tSOME / period",
            " *\t   %FULL = tFULL / period",
            " *",
            " * This gives us an approximation of pressure that is practical",
            " * cost-wise, yet way more sensitive and accurate than periodic",
            " * sampling of the aggregate task states would be.",
            " */",
            "",
            "static int psi_bug __read_mostly;",
            "",
            "DEFINE_STATIC_KEY_FALSE(psi_disabled);",
            "static DEFINE_STATIC_KEY_TRUE(psi_cgroups_enabled);",
            "",
            "#ifdef CONFIG_PSI_DEFAULT_DISABLED",
            "static bool psi_enable;",
            "#else",
            "static bool psi_enable = true;",
            "#endif"
          ],
          "function_name": null,
          "description": "定义PSI模块的基本配置和启用状态，声明静态键控开关及全局变量，用于控制PSI功能的启用与禁用。",
          "similarity": 0.4177449941635132
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/psi.c",
          "start_line": 150,
          "end_line": 302,
          "content": [
            "static int __init setup_psi(char *str)",
            "{",
            "\treturn kstrtobool(str, &psi_enable) == 0;",
            "}",
            "static inline void psi_write_begin(int cpu)",
            "{",
            "\twrite_seqcount_begin(per_cpu_ptr(&psi_seq, cpu));",
            "}",
            "static inline void psi_write_end(int cpu)",
            "{",
            "\twrite_seqcount_end(per_cpu_ptr(&psi_seq, cpu));",
            "}",
            "static inline u32 psi_read_begin(int cpu)",
            "{",
            "\treturn read_seqcount_begin(per_cpu_ptr(&psi_seq, cpu));",
            "}",
            "static inline bool psi_read_retry(int cpu, u32 seq)",
            "{",
            "\treturn read_seqcount_retry(per_cpu_ptr(&psi_seq, cpu), seq);",
            "}",
            "static void group_init(struct psi_group *group)",
            "{",
            "\tgroup->enabled = true;",
            "\tgroup->avg_last_update = sched_clock();",
            "\tgroup->avg_next_update = group->avg_last_update + psi_period;",
            "\tmutex_init(&group->avgs_lock);",
            "",
            "\t/* Init avg trigger-related members */",
            "\tINIT_LIST_HEAD(&group->avg_triggers);",
            "\tmemset(group->avg_nr_triggers, 0, sizeof(group->avg_nr_triggers));",
            "\tINIT_DELAYED_WORK(&group->avgs_work, psi_avgs_work);",
            "",
            "\t/* Init rtpoll trigger-related members */",
            "\tatomic_set(&group->rtpoll_scheduled, 0);",
            "\tmutex_init(&group->rtpoll_trigger_lock);",
            "\tINIT_LIST_HEAD(&group->rtpoll_triggers);",
            "\tgroup->rtpoll_min_period = U32_MAX;",
            "\tgroup->rtpoll_next_update = ULLONG_MAX;",
            "\tinit_waitqueue_head(&group->rtpoll_wait);",
            "\ttimer_setup(&group->rtpoll_timer, poll_timer_fn, 0);",
            "\trcu_assign_pointer(group->rtpoll_task, NULL);",
            "}",
            "void __init psi_init(void)",
            "{",
            "\tif (!psi_enable) {",
            "\t\tstatic_branch_enable(&psi_disabled);",
            "\t\tstatic_branch_disable(&psi_cgroups_enabled);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!cgroup_psi_enabled())",
            "\t\tstatic_branch_disable(&psi_cgroups_enabled);",
            "",
            "\tpsi_period = jiffies_to_nsecs(PSI_FREQ);",
            "\tgroup_init(&psi_system);",
            "}",
            "static u32 test_states(unsigned int *tasks, u32 state_mask)",
            "{",
            "\tconst bool oncpu = state_mask & PSI_ONCPU;",
            "",
            "\tif (tasks[NR_IOWAIT]) {",
            "\t\tstate_mask |= BIT(PSI_IO_SOME);",
            "\t\tif (!tasks[NR_RUNNING])",
            "\t\t\tstate_mask |= BIT(PSI_IO_FULL);",
            "\t}",
            "",
            "\tif (tasks[NR_MEMSTALL]) {",
            "\t\tstate_mask |= BIT(PSI_MEM_SOME);",
            "\t\tif (tasks[NR_RUNNING] == tasks[NR_MEMSTALL_RUNNING])",
            "\t\t\tstate_mask |= BIT(PSI_MEM_FULL);",
            "\t}",
            "",
            "\tif (tasks[NR_RUNNING] > oncpu)",
            "\t\tstate_mask |= BIT(PSI_CPU_SOME);",
            "",
            "\tif (tasks[NR_RUNNING] && !oncpu)",
            "\t\tstate_mask |= BIT(PSI_CPU_FULL);",
            "",
            "\tif (tasks[NR_IOWAIT] || tasks[NR_MEMSTALL] || tasks[NR_RUNNING])",
            "\t\tstate_mask |= BIT(PSI_NONIDLE);",
            "",
            "\treturn state_mask;",
            "}",
            "static void get_recent_times(struct psi_group *group, int cpu,",
            "\t\t\t     enum psi_aggregators aggregator, u32 *times,",
            "\t\t\t     u32 *pchanged_states)",
            "{",
            "\tstruct psi_group_cpu *groupc = per_cpu_ptr(group->pcpu, cpu);",
            "\tint current_cpu = raw_smp_processor_id();",
            "\tunsigned int tasks[NR_PSI_TASK_COUNTS];",
            "\tu64 now, state_start;",
            "\tenum psi_states s;",
            "\tunsigned int seq;",
            "\tu32 state_mask;",
            "",
            "\t*pchanged_states = 0;",
            "",
            "\t/* Snapshot a coherent view of the CPU state */",
            "\tdo {",
            "\t\tseq = psi_read_begin(cpu);",
            "\t\tnow = cpu_clock(cpu);",
            "\t\tmemcpy(times, groupc->times, sizeof(groupc->times));",
            "\t\tstate_mask = groupc->state_mask;",
            "\t\tstate_start = groupc->state_start;",
            "\t\tif (cpu == current_cpu)",
            "\t\t\tmemcpy(tasks, groupc->tasks, sizeof(groupc->tasks));",
            "\t} while (psi_read_retry(cpu, seq));",
            "",
            "\t/* Calculate state time deltas against the previous snapshot */",
            "\tfor (s = 0; s < NR_PSI_STATES; s++) {",
            "\t\tu32 delta;",
            "\t\t/*",
            "\t\t * In addition to already concluded states, we also",
            "\t\t * incorporate currently active states on the CPU,",
            "\t\t * since states may last for many sampling periods.",
            "\t\t *",
            "\t\t * This way we keep our delta sampling buckets small",
            "\t\t * (u32) and our reported pressure close to what's",
            "\t\t * actually happening.",
            "\t\t */",
            "\t\tif (state_mask & (1 << s))",
            "\t\t\ttimes[s] += now - state_start;",
            "",
            "\t\tdelta = times[s] - groupc->times_prev[aggregator][s];",
            "\t\tgroupc->times_prev[aggregator][s] = times[s];",
            "",
            "\t\ttimes[s] = delta;",
            "\t\tif (delta)",
            "\t\t\t*pchanged_states |= (1 << s);",
            "\t}",
            "",
            "\t/*",
            "\t * When collect_percpu_times() from the avgs_work, we don't want to",
            "\t * re-arm avgs_work when all CPUs are IDLE. But the current CPU running",
            "\t * this avgs_work is never IDLE, cause avgs_work can't be shut off.",
            "\t * So for the current CPU, we need to re-arm avgs_work only when",
            "\t * (NR_RUNNING > 1 || NR_IOWAIT > 0 || NR_MEMSTALL > 0), for other CPUs",
            "\t * we can just check PSI_NONIDLE delta.",
            "\t */",
            "\tif (current_work() == &group->avgs_work.work) {",
            "\t\tbool reschedule;",
            "",
            "\t\tif (cpu == current_cpu)",
            "\t\t\treschedule = tasks[NR_RUNNING] +",
            "\t\t\t\t     tasks[NR_IOWAIT] +",
            "\t\t\t\t     tasks[NR_MEMSTALL] > 1;",
            "\t\telse",
            "\t\t\treschedule = *pchanged_states & (1 << PSI_NONIDLE);",
            "",
            "\t\tif (reschedule)",
            "\t\t\t*pchanged_states |= PSI_STATE_RESCHEDULE;",
            "\t}",
            "}"
          ],
          "function_name": "setup_psi, psi_write_begin, psi_write_end, psi_read_begin, psi_read_retry, group_init, psi_init, test_states, get_recent_times",
          "description": "初始化PSI组结构体，注册回调函数处理压力状态变化，通过遍历任务状态计算压力掩码并收集各CPU的压力时间数据。",
          "similarity": 0.4168485701084137
        },
        {
          "chunk_id": 8,
          "file_path": "kernel/sched/psi.c",
          "start_line": 1135,
          "end_line": 1276,
          "content": [
            "void psi_cgroup_free(struct cgroup *cgroup)",
            "{",
            "\tif (!static_branch_likely(&psi_cgroups_enabled))",
            "\t\treturn;",
            "",
            "\tcancel_delayed_work_sync(&cgroup->psi->avgs_work);",
            "\tfree_percpu(cgroup->psi->pcpu);",
            "\t/* All triggers must be removed by now */",
            "\tWARN_ONCE(cgroup->psi->rtpoll_states, \"psi: trigger leak\\n\");",
            "\tkfree(cgroup->psi);",
            "}",
            "void cgroup_move_task(struct task_struct *task, struct css_set *to)",
            "{",
            "\tunsigned int task_flags;",
            "\tstruct rq_flags rf;",
            "\tstruct rq *rq;",
            "",
            "\tif (!static_branch_likely(&psi_cgroups_enabled)) {",
            "\t\t/*",
            "\t\t * Lame to do this here, but the scheduler cannot be locked",
            "\t\t * from the outside, so we move cgroups from inside sched/.",
            "\t\t */",
            "\t\trcu_assign_pointer(task->cgroups, to);",
            "\t\treturn;",
            "\t}",
            "",
            "\trq = task_rq_lock(task, &rf);",
            "",
            "\t/*",
            "\t * We may race with schedule() dropping the rq lock between",
            "\t * deactivating prev and switching to next. Because the psi",
            "\t * updates from the deactivation are deferred to the switch",
            "\t * callback to save cgroup tree updates, the task's scheduling",
            "\t * state here is not coherent with its psi state:",
            "\t *",
            "\t * schedule()                   cgroup_move_task()",
            "\t *   rq_lock()",
            "\t *   deactivate_task()",
            "\t *     p->on_rq = 0",
            "\t *     psi_dequeue() // defers TSK_RUNNING & TSK_IOWAIT updates",
            "\t *   pick_next_task()",
            "\t *     rq_unlock()",
            "\t *                                rq_lock()",
            "\t *                                psi_task_change() // old cgroup",
            "\t *                                task->cgroups = to",
            "\t *                                psi_task_change() // new cgroup",
            "\t *                                rq_unlock()",
            "\t *     rq_lock()",
            "\t *   psi_sched_switch() // does deferred updates in new cgroup",
            "\t *",
            "\t * Don't rely on the scheduling state. Use psi_flags instead.",
            "\t */",
            "\ttask_flags = task->psi_flags;",
            "",
            "\tif (task_flags)",
            "\t\tpsi_task_change(task, task_flags, 0);",
            "",
            "\t/* See comment above */",
            "\trcu_assign_pointer(task->cgroups, to);",
            "",
            "\tif (task_flags)",
            "\t\tpsi_task_change(task, 0, task_flags);",
            "",
            "\ttask_rq_unlock(rq, task, &rf);",
            "}",
            "void psi_cgroup_restart(struct psi_group *group)",
            "{",
            "\tint cpu;",
            "",
            "\t/*",
            "\t * After we disable psi_group->enabled, we don't actually",
            "\t * stop percpu tasks accounting in each psi_group_cpu,",
            "\t * instead only stop test_states() loop, record_times()",
            "\t * and averaging worker, see psi_group_change() for details.",
            "\t *",
            "\t * When disable cgroup PSI, this function has nothing to sync",
            "\t * since cgroup pressure files are hidden and percpu psi_group_cpu",
            "\t * would see !psi_group->enabled and only do task accounting.",
            "\t *",
            "\t * When re-enable cgroup PSI, this function use psi_group_change()",
            "\t * to get correct state mask from test_states() loop on tasks[],",
            "\t * and restart groupc->state_start from now, use .clear = .set = 0",
            "\t * here since no task status really changed.",
            "\t */",
            "\tif (!group->enabled)",
            "\t\treturn;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tu64 now;",
            "",
            "\t\tguard(rq_lock_irq)(cpu_rq(cpu));",
            "",
            "\t\tpsi_write_begin(cpu);",
            "\t\tnow = cpu_clock(cpu);",
            "\t\tpsi_group_change(group, cpu, 0, 0, now, true);",
            "\t\tpsi_write_end(cpu);",
            "\t}",
            "}",
            "int psi_show(struct seq_file *m, struct psi_group *group, enum psi_res res)",
            "{",
            "\tbool only_full = false;",
            "\tint full;",
            "\tu64 now;",
            "",
            "\tif (static_branch_likely(&psi_disabled))",
            "\t\treturn -EOPNOTSUPP;",
            "",
            "\t/* Update averages before reporting them */",
            "\tmutex_lock(&group->avgs_lock);",
            "\tnow = sched_clock();",
            "\tcollect_percpu_times(group, PSI_AVGS, NULL);",
            "\tif (now >= group->avg_next_update)",
            "\t\tgroup->avg_next_update = update_averages(group, now);",
            "\tmutex_unlock(&group->avgs_lock);",
            "",
            "#ifdef CONFIG_IRQ_TIME_ACCOUNTING",
            "\tonly_full = res == PSI_IRQ;",
            "#endif",
            "",
            "\tfor (full = 0; full < 2 - only_full; full++) {",
            "\t\tunsigned long avg[3] = { 0, };",
            "\t\tu64 total = 0;",
            "\t\tint w;",
            "",
            "\t\t/* CPU FULL is undefined at the system level */",
            "\t\tif (!(group == &psi_system && res == PSI_CPU && full)) {",
            "\t\t\tfor (w = 0; w < 3; w++)",
            "\t\t\t\tavg[w] = group->avg[res * 2 + full][w];",
            "\t\t\ttotal = div_u64(group->total[PSI_AVGS][res * 2 + full],",
            "\t\t\t\t\tNSEC_PER_USEC);",
            "\t\t}",
            "",
            "\t\tseq_printf(m, \"%s avg10=%lu.%02lu avg60=%lu.%02lu avg300=%lu.%02lu total=%llu\\n\",",
            "\t\t\t   full || only_full ? \"full\" : \"some\",",
            "\t\t\t   LOAD_INT(avg[0]), LOAD_FRAC(avg[0]),",
            "\t\t\t   LOAD_INT(avg[1]), LOAD_FRAC(avg[1]),",
            "\t\t\t   LOAD_INT(avg[2]), LOAD_FRAC(avg[2]),",
            "\t\t\t   total);",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "psi_cgroup_free, cgroup_move_task, psi_cgroup_restart, psi_show",
          "description": "psi_cgroup_free 释放 PSI 资源并验证无残留触发器；cgroup_move_task 协调任务迁移时的 PSI 状态同步；psi_cgroup_restart 重置 PSI 组状态；psi_show 输出 PSI 平均负载统计信息",
          "similarity": 0.41495054960250854
        }
      ]
    }
  ]
}