{
  "query": "optimization strategies",
  "timestamp": "2025-12-26 01:39:32",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.5071029663085938,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/mempolicy.c",
          "start_line": 168,
          "end_line": 268,
          "content": [
            "static u8 get_il_weight(int node)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tu8 weight = 1;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state)",
            "\t\tweight = state->iw_table[node];",
            "\trcu_read_unlock();",
            "\treturn weight;",
            "}",
            "static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)",
            "{",
            "\tu64 sum_bw = 0;",
            "\tunsigned int cast_sum_bw, scaling_factor = 1, iw_gcd = 0;",
            "\tint nid;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tsum_bw += bw[nid];",
            "",
            "\t/* Scale bandwidths to whole numbers in the range [1, weightiness] */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t/*",
            "\t\t * Try not to perform 64-bit division.",
            "\t\t * If sum_bw < scaling_factor, then sum_bw < U32_MAX.",
            "\t\t * If sum_bw > scaling_factor, then round the weight up to 1.",
            "\t\t */",
            "\t\tscaling_factor = weightiness * bw[nid];",
            "\t\tif (bw[nid] && sum_bw < scaling_factor) {",
            "\t\t\tcast_sum_bw = (unsigned int)sum_bw;",
            "\t\t\tnew_iw[nid] = scaling_factor / cast_sum_bw;",
            "\t\t} else {",
            "\t\t\tnew_iw[nid] = 1;",
            "\t\t}",
            "\t\tif (!iw_gcd)",
            "\t\t\tiw_gcd = new_iw[nid];",
            "\t\tiw_gcd = gcd(iw_gcd, new_iw[nid]);",
            "\t}",
            "",
            "\t/* 1:2 is strictly better than 16:32. Reduce by the weights' GCD. */",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tnew_iw[nid] /= iw_gcd;",
            "}",
            "int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tunsigned int *old_bw, *new_bw;",
            "\tunsigned int bw_val;",
            "\tint i;",
            "",
            "\tbw_val = min(coords->read_bandwidth, coords->write_bandwidth);",
            "\tnew_bw = kcalloc(nr_node_ids, sizeof(unsigned int), GFP_KERNEL);",
            "\tif (!new_bw)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew_wi_state = kmalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state) {",
            "\t\tkfree(new_bw);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tnew_wi_state->mode_auto = true;",
            "\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\tnew_wi_state->iw_table[i] = 1;",
            "",
            "\t/*",
            "\t * Update bandwidth info, even in manual mode. That way, when switching",
            "\t * to auto mode in the future, iw_table can be overwritten using",
            "\t * accurate bw data.",
            "\t */",
            "\tmutex_lock(&wi_state_lock);",
            "",
            "\told_bw = node_bw_table;",
            "\tif (old_bw)",
            "\t\tmemcpy(new_bw, old_bw, nr_node_ids * sizeof(*old_bw));",
            "\tnew_bw[node] = bw_val;",
            "\tnode_bw_table = new_bw;",
            "",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state && !old_wi_state->mode_auto) {",
            "\t\t/* Manual mode; skip reducing weights and updating wi_state */",
            "\t\tmutex_unlock(&wi_state_lock);",
            "\t\tkfree(new_wi_state);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* NULL wi_state assumes auto=true; reduce weights and update wi_state*/",
            "\treduce_interleave_weights(new_bw, new_wi_state->iw_table);",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "out:",
            "\tkfree(old_bw);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_il_weight, reduce_interleave_weights, mempolicy_set_node_perf",
          "description": "实现带权交错策略的权重计算与调整逻辑，通过获取节点带宽数据动态修改权重比例，支持根据性能参数更新节点间内存分配优先级。",
          "similarity": 0.5436671376228333
        },
        {
          "chunk_id": 17,
          "file_path": "mm/mempolicy.c",
          "start_line": 2969,
          "end_line": 3093,
          "content": [
            "void mpol_put_task_policy(struct task_struct *task)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\ttask_lock(task);",
            "\tpol = task->mempolicy;",
            "\ttask->mempolicy = NULL;",
            "\ttask_unlock(task);",
            "\tmpol_put(pol);",
            "}",
            "static void sp_delete(struct shared_policy *sp, struct sp_node *n)",
            "{",
            "\trb_erase(&n->nd, &sp->root);",
            "\tsp_free(n);",
            "}",
            "static void sp_node_init(struct sp_node *node, unsigned long start,",
            "\t\t\tunsigned long end, struct mempolicy *pol)",
            "{",
            "\tnode->start = start;",
            "\tnode->end = end;",
            "\tnode->policy = pol;",
            "}",
            "static int shared_policy_replace(struct shared_policy *sp, pgoff_t start,",
            "\t\t\t\t pgoff_t end, struct sp_node *new)",
            "{",
            "\tstruct sp_node *n;",
            "\tstruct sp_node *n_new = NULL;",
            "\tstruct mempolicy *mpol_new = NULL;",
            "\tint ret = 0;",
            "",
            "restart:",
            "\twrite_lock(&sp->lock);",
            "\tn = sp_lookup(sp, start, end);",
            "\t/* Take care of old policies in the same range. */",
            "\twhile (n && n->start < end) {",
            "\t\tstruct rb_node *next = rb_next(&n->nd);",
            "\t\tif (n->start >= start) {",
            "\t\t\tif (n->end <= end)",
            "\t\t\t\tsp_delete(sp, n);",
            "\t\t\telse",
            "\t\t\t\tn->start = end;",
            "\t\t} else {",
            "\t\t\t/* Old policy spanning whole new range. */",
            "\t\t\tif (n->end > end) {",
            "\t\t\t\tif (!n_new)",
            "\t\t\t\t\tgoto alloc_new;",
            "",
            "\t\t\t\t*mpol_new = *n->policy;",
            "\t\t\t\tatomic_set(&mpol_new->refcnt, 1);",
            "\t\t\t\tsp_node_init(n_new, end, n->end, mpol_new);",
            "\t\t\t\tn->end = start;",
            "\t\t\t\tsp_insert(sp, n_new);",
            "\t\t\t\tn_new = NULL;",
            "\t\t\t\tmpol_new = NULL;",
            "\t\t\t\tbreak;",
            "\t\t\t} else",
            "\t\t\t\tn->end = start;",
            "\t\t}",
            "\t\tif (!next)",
            "\t\t\tbreak;",
            "\t\tn = rb_entry(next, struct sp_node, nd);",
            "\t}",
            "\tif (new)",
            "\t\tsp_insert(sp, new);",
            "\twrite_unlock(&sp->lock);",
            "\tret = 0;",
            "",
            "err_out:",
            "\tif (mpol_new)",
            "\t\tmpol_put(mpol_new);",
            "\tif (n_new)",
            "\t\tkmem_cache_free(sn_cache, n_new);",
            "",
            "\treturn ret;",
            "",
            "alloc_new:",
            "\twrite_unlock(&sp->lock);",
            "\tret = -ENOMEM;",
            "\tn_new = kmem_cache_alloc(sn_cache, GFP_KERNEL);",
            "\tif (!n_new)",
            "\t\tgoto err_out;",
            "\tmpol_new = kmem_cache_alloc(policy_cache, GFP_KERNEL);",
            "\tif (!mpol_new)",
            "\t\tgoto err_out;",
            "\tatomic_set(&mpol_new->refcnt, 1);",
            "\tgoto restart;",
            "}",
            "void mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol)",
            "{",
            "\tint ret;",
            "",
            "\tsp->root = RB_ROOT;\t\t/* empty tree == default mempolicy */",
            "\trwlock_init(&sp->lock);",
            "",
            "\tif (mpol) {",
            "\t\tstruct sp_node *sn;",
            "\t\tstruct mempolicy *npol;",
            "\t\tNODEMASK_SCRATCH(scratch);",
            "",
            "\t\tif (!scratch)",
            "\t\t\tgoto put_mpol;",
            "",
            "\t\t/* contextualize the tmpfs mount point mempolicy to this file */",
            "\t\tnpol = mpol_new(mpol->mode, mpol->flags, &mpol->w.user_nodemask);",
            "\t\tif (IS_ERR(npol))",
            "\t\t\tgoto free_scratch; /* no valid nodemask intersection */",
            "",
            "\t\ttask_lock(current);",
            "\t\tret = mpol_set_nodemask(npol, &mpol->w.user_nodemask, scratch);",
            "\t\ttask_unlock(current);",
            "\t\tif (ret)",
            "\t\t\tgoto put_npol;",
            "",
            "\t\t/* alloc node covering entire file; adds ref to file's npol */",
            "\t\tsn = sp_alloc(0, MAX_LFS_FILESIZE >> PAGE_SHIFT, npol);",
            "\t\tif (sn)",
            "\t\t\tsp_insert(sp, sn);",
            "put_npol:",
            "\t\tmpol_put(npol);\t/* drop initial ref on file's npol */",
            "free_scratch:",
            "\t\tNODEMASK_SCRATCH_FREE(scratch);",
            "put_mpol:",
            "\t\tmpol_put(mpol);\t/* drop our incoming ref on sb mpol */",
            "\t}",
            "}"
          ],
          "function_name": "mpol_put_task_policy, sp_delete, sp_node_init, shared_policy_replace, mpol_shared_policy_init",
          "description": "mpol_put_task_policy释放任务级内存策略引用，sp_delete从RB树删除节点并回收资源，sp_node_init初始化共享策略节点，shared_policy_replace替换共享策略区间并处理节点分裂，mpol_shared_policy_init初始化共享策略结构体并设置初始策略。",
          "similarity": 0.5410330295562744
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.5410139560699463
        },
        {
          "chunk_id": 13,
          "file_path": "mm/mempolicy.c",
          "start_line": 2149,
          "end_line": 2255,
          "content": [
            "static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nnodes;",
            "\tint i;",
            "\tint nid;",
            "",
            "\tnnodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nnodes)",
            "\t\treturn numa_node_id();",
            "\ttarget = ilx % nnodes;",
            "\tnid = first_node(nodemask);",
            "\tfor (i = 0; i < target; i++)",
            "\t\tnid = next_node(nid, nodemask);",
            "\treturn nid;",
            "}",
            "int huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,",
            "\t\tstruct mempolicy **mpol, nodemask_t **nodemask)",
            "{",
            "\tpgoff_t ilx;",
            "\tint nid;",
            "",
            "\tnid = numa_node_id();",
            "\t*mpol = get_vma_policy(vma, addr, hstate_vma(vma)->order, &ilx);",
            "\t*nodemask = policy_nodemask(gfp_flags, *mpol, ilx, &nid);",
            "\treturn nid;",
            "}",
            "bool init_nodemask_of_mempolicy(nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "",
            "\tif (!(mask && current->mempolicy))",
            "\t\treturn false;",
            "",
            "\ttask_lock(current);",
            "\tmempolicy = current->mempolicy;",
            "\tswitch (mempolicy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*mask = mempolicy->nodes;",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tinit_nodemask_of_node(mask, numa_node_id());",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "\ttask_unlock(current);",
            "",
            "\treturn true;",
            "}",
            "bool mempolicy_in_oom_domain(struct task_struct *tsk,",
            "\t\t\t\t\tconst nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "\tbool ret = true;",
            "",
            "\tif (!mask)",
            "\t\treturn ret;",
            "",
            "\ttask_lock(tsk);",
            "\tmempolicy = tsk->mempolicy;",
            "\tif (mempolicy && mempolicy->mode == MPOL_BIND)",
            "\t\tret = nodes_intersects(mempolicy->nodes, *mask);",
            "\ttask_unlock(tsk);",
            "",
            "\treturn ret;",
            "}",
            "static unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tint nodes;",
            "\tunsigned long nr_pages_per_node;",
            "\tint delta;",
            "\tint i;",
            "\tunsigned long nr_allocated;",
            "\tunsigned long total_allocated = 0;",
            "",
            "\tnodes = nodes_weight(pol->nodes);",
            "\tnr_pages_per_node = nr_pages / nodes;",
            "\tdelta = nr_pages - nodes * nr_pages_per_node;",
            "",
            "\tfor (i = 0; i < nodes; i++) {",
            "\t\tif (delta) {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node + 1, NULL,",
            "\t\t\t\t\tpage_array);",
            "\t\t\tdelta--;",
            "\t\t} else {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node, NULL, page_array);",
            "\t\t}",
            "",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t}",
            "",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "interleave_nid, huge_node, init_nodemask_of_mempolicy, mempolicy_in_oom_domain, alloc_pages_bulk_array_interleave",
          "description": "interleave_nid 计算简单交错分配的目标节点；huge_node 结合HugeTLB策略确定大页分配节点；init_nodemask_of_mempolicy 初始化当前进程的内存策略节点掩码；mempolicy_in_oom_domain 检查策略节点是否与OOM域重叠；alloc_pages_bulk_array_interleave 执行批量交错分配。",
          "similarity": 0.5355331301689148
        },
        {
          "chunk_id": 12,
          "file_path": "mm/mempolicy.c",
          "start_line": 2024,
          "end_line": 2135,
          "content": [
            "static unsigned int interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int nid;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "\t/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnid = next_node_in(current->il_prev, policy->nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\tif (nid < MAX_NUMNODES)",
            "\t\tcurrent->il_prev = nid;",
            "\treturn nid;",
            "}",
            "unsigned int mempolicy_slab_node(void)",
            "{",
            "\tstruct mempolicy *policy;",
            "\tint node = numa_mem_id();",
            "",
            "\tif (!in_task())",
            "\t\treturn node;",
            "",
            "\tpolicy = current->mempolicy;",
            "\tif (!policy)",
            "\t\treturn node;",
            "",
            "\tswitch (policy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\treturn first_node(policy->nodes);",
            "",
            "\tcase MPOL_INTERLEAVE:",
            "\t\treturn interleave_nodes(policy);",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn weighted_interleave_nodes(policy);",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t{",
            "\t\tstruct zoneref *z;",
            "",
            "\t\t/*",
            "\t\t * Follow bind policy behavior and start allocation at the",
            "\t\t * first node.",
            "\t\t */",
            "\t\tstruct zonelist *zonelist;",
            "\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);",
            "\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];",
            "\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,",
            "\t\t\t\t\t\t\t&policy->nodes);",
            "\t\treturn z->zone ? zone_to_nid(z->zone) : node;",
            "\t}",
            "\tcase MPOL_LOCAL:",
            "\t\treturn node;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static unsigned int read_once_policy_nodemask(struct mempolicy *pol,",
            "\t\t\t\t\t      nodemask_t *mask)",
            "{",
            "\t/*",
            "\t * barrier stabilizes the nodemask locally so that it can be iterated",
            "\t * over safely without concern for changes. Allocators validate node",
            "\t * selection does not violate mems_allowed, so this is safe.",
            "\t */",
            "\tbarrier();",
            "\tmemcpy(mask, &pol->nodes, sizeof(nodemask_t));",
            "\tbarrier();",
            "\treturn nodes_weight(*mask);",
            "}",
            "static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nr_nodes;",
            "\tu8 *table = NULL;",
            "\tunsigned int weight_total = 0;",
            "\tu8 weight;",
            "\tint nid = 0;",
            "",
            "\tnr_nodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nr_nodes)",
            "\t\treturn numa_node_id();",
            "",
            "\trcu_read_lock();",
            "",
            "\tstate = rcu_dereference(wi_state);",
            "\t/* Uninitialized wi_state means we should assume all weights are 1 */",
            "\tif (state)",
            "\t\ttable = state->iw_table;",
            "",
            "\t/* calculate the total weight */",
            "\tfor_each_node_mask(nid, nodemask)",
            "\t\tweight_total += table ? table[nid] : 1;",
            "",
            "\t/* Calculate the node offset based on totals */",
            "\ttarget = ilx % weight_total;",
            "\tnid = first_node(nodemask);",
            "\twhile (target) {",
            "\t\t/* detect system default usage */",
            "\t\tweight = table ? table[nid] : 1;",
            "\t\tif (target < weight)",
            "\t\t\tbreak;",
            "\t\ttarget -= weight;",
            "\t\tnid = next_node_in(nid, nodemask);",
            "\t}",
            "\trcu_read_unlock();",
            "\treturn nid;",
            "}"
          ],
          "function_name": "interleave_nodes, mempolicy_slab_node, read_once_policy_nodemask, weighted_interleave_nid",
          "description": "interleave_nodes 计算交错分配的下一个节点；mempolicy_slab_node 根据内存策略返回Slab分配的节点；read_once_policy_nodemask 安全读取策略节点掩码；weighted_interleave_nid 基于权重计算加权交错分配的目标节点。",
          "similarity": 0.5263170599937439
        }
      ]
    },
    {
      "source_file": "kernel/sched/build_policy.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:56:50\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\build_policy.c`\n\n---\n\n# `sched/build_policy.c` 技术文档\n\n## 1. 文件概述\n\n`build_policy.c` 是 Linux 内核调度子系统中的一个构建辅助文件，其主要作用是将多个与调度策略相关的源代码模块（如实时调度、截止时间调度、CPU 时间统计等）合并到一个编译单元中进行编译。这种设计并非用于实现具体调度逻辑，而是出于**构建效率优化**的目的：通过减少重复包含头文件的开销、平衡各编译单元的大小，从而缩短整体内核编译时间。该文件本身不包含任何函数或数据结构定义，仅通过 `#include` 指令聚合其他 `.c` 文件。\n\n## 2. 核心功能\n\n该文件本身**不定义任何函数或数据结构**，其“功能”体现在所包含的源文件模块中，主要包括：\n\n- **调度策略实现模块**：\n  - `idle.c`：空闲任务（idle task）的调度逻辑\n  - `rt.c`：实时调度类（SCHED_FIFO / SCHED_RR）的实现\n  - `deadline.c`：截止时间调度类（SCHED_DEADLINE）的实现\n  - `cpudeadline.c`（仅在 `CONFIG_SMP` 下）：SMP 架构下截止时间调度的 CPU 负载管理\n  - `ext.c`（仅在 `CONFIG_SCHED_CLASS_EXT` 下）：可扩展调度类支持\n\n- **辅助功能模块**：\n  - `cputime.c`：CPU 时间统计与账户管理\n  - `pelt.c`（仅在 `CONFIG_SMP` 下）：Per-Entity Load Tracking（PELT）负载跟踪机制\n  - `syscalls.c`：调度相关的系统调用（如 `sched_setattr`, `sched_getattr` 等）\n\n## 3. 关键实现\n\n- **单一编译单元聚合**：  \n  通过在一个 `.c` 文件中包含多个功能相关的 `.c` 文件，将原本分散的调度策略代码合并为一个较大的编译单元。这减少了每个源文件单独包含大量公共头文件（如 `sched.h`, `linux/sched/*.h` 等）所带来的重复解析开销。\n\n- **条件编译控制**：  \n  使用 `#ifdef CONFIG_SMP` 和 `#ifdef CONFIG_SCHED_CLASS_EXT` 等宏，确保仅在对应内核配置启用时才包含特定功能模块（如 `cpudeadline.c`、`pelt.c`、`ext.c`），保证构建的灵活性和配置适应性。\n\n- **构建时间平衡**：  \n  注释中明确指出，此编译单元的大小与 `core.c`（调度核心）和 `fair.c`（CFS 完全公平调度器）相当，有助于在并行编译时更均匀地分配工作负载，避免某些编译任务过重而拖慢整体构建速度。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 调度子系统内部头文件：`\"sched.h\"`, `\"smp.h\"`, `\"autogroup.h\"`, `\"stats.h\"`, `\"pelt.h\"`\n  - 内核通用子系统：`<linux/sched/*.h>`, `<linux/cpuidle.h>`, `<linux/psi.h>`, `<linux/rhashtable.h>` 等\n  - 用户态接口：`<uapi/linux/sched/types.h>`\n\n- **模块依赖**：\n  - 依赖 `core.c` 和 `fair.c` 提供的调度核心框架和 CFS 调度器（但这两者被单独编译）\n  - 所包含的模块（如 `rt.c`, `deadline.c`）依赖调度类注册机制、运行队列管理、负载均衡等核心调度基础设施\n  - `pelt.c` 依赖 SMP 架构下的负载跟踪和迁移逻辑\n\n## 5. 使用场景\n\n- **内核构建阶段**：  \n  该文件仅在内核编译过程中被使用，用于高效地编译调度策略相关代码。最终生成的内核镜像中不包含此文件的独立实体。\n\n- **调度策略运行时**：  \n  虽然 `build_policy.c` 本身不参与运行时逻辑，但它所聚合的模块（如实时调度、截止时间调度、CPU 时间统计等）在以下场景中被激活：\n  - 用户进程使用 `SCHED_FIFO`、`SCHED_RR` 或 `SCHED_DEADLINE` 策略\n  - 系统调用如 `sched_setattr()` 被调用以配置高级调度参数\n  - 内核进行 CPU 负载跟踪（PELT）、空闲 CPU 管理、CPU 热插拔时的调度状态迁移\n  - 能耗管理（如 cpuidle、suspend）与调度器协同工作时\n\n该文件是 Linux 内核构建系统优化的一个典型示例，体现了在大型项目中通过源码组织方式提升编译效率的设计思想。",
      "similarity": 0.5046800374984741,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/build_policy.c",
          "start_line": 1,
          "end_line": 66,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * These are the scheduling policy related scheduler files, built",
            " * in a single compilation unit for build efficiency reasons.",
            " *",
            " * ( Incidentally, the size of the compilation unit is roughly",
            " *   comparable to core.c and fair.c, the other two big",
            " *   compilation units. This helps balance build time, while",
            " *   coalescing source files to amortize header inclusion",
            " *   cost. )",
            " *",
            " * core.c and fair.c are built separately.",
            " */",
            "",
            "/* Headers: */",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/posix-timers.h>",
            "#include <linux/sched/rt.h>",
            "",
            "#include <linux/cpuidle.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/kobject.h>",
            "#include <linux/livepatch.h>",
            "#include <linux/pm.h>",
            "#include <linux/psi.h>",
            "#include <linux/rhashtable.h>",
            "#include <linux/seq_buf.h>",
            "#include <linux/seqlock_api.h>",
            "#include <linux/slab.h>",
            "#include <linux/suspend.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/vtime.h>",
            "#include <linux/sysrq.h>",
            "#include <linux/percpu-rwsem.h>",
            "",
            "#include <uapi/linux/sched/types.h>",
            "",
            "#include \"sched.h\"",
            "#include \"smp.h\"",
            "",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include \"pelt.h\"",
            "",
            "/* Source code modules: */",
            "",
            "#include \"idle.c\"",
            "",
            "#include \"rt.c\"",
            "",
            "#ifdef CONFIG_SMP",
            "# include \"cpudeadline.c\"",
            "# include \"pelt.c\"",
            "#endif",
            "",
            "#include \"cputime.c\"",
            "#include \"deadline.c\"",
            "",
            "#ifdef CONFIG_SCHED_CLASS_EXT",
            "# include \"ext.c\"",
            "#endif",
            "",
            "#include \"syscalls.c\""
          ],
          "function_name": null,
          "description": "此代码段是Linux内核调度器中与调度策略相关的编译单元，核心功能是整合多个调度类（如实时调度、截止时间调度等）的实现，通过条件编译包含不同模块（如`idle.c`、`rt.c`、`deadline.c`等）以提升构建效率。该文件作为调度策略逻辑的聚合点，协调各子模块的实现并管理其依赖关系。",
          "similarity": 0.4892071485519409
        }
      ]
    },
    {
      "source_file": "mm/compaction.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:44:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `compaction.c`\n\n---\n\n# compaction.c 技术文档\n\n## 1. 文件概述\n\n`compaction.c` 是 Linux 内核内存管理子系统中实现**内存压缩（Memory Compaction）**功能的核心文件。其主要目标是**减少外部碎片（external fragmentation）**，通过迁移可移动页面，将物理内存中的空闲页聚集形成连续的大块内存区域，从而满足高阶（high-order）内存分配请求（如透明大页 THP 或 HugeTLB）。该机制高度依赖页面迁移（page migration）来完成实际的页面移动工作。\n\n## 2. 核心功能\n\n### 主要函数\n- **`PageMovable()` / `__SetPageMovable()` / `__ClearPageMovable()`**: 管理页面的可移动性标记，用于标识页面是否可通过注册的 `movable_operations` 进行迁移。\n- **`defer_compaction()` / `compaction_deferred()` / `compaction_defer_reset()`**: 实现**压缩延迟（deferred compaction）**机制，避免在压缩失败后频繁重试，提升系统性能。\n- **`compaction_restarting()`**: 判断是否因多次失败后重新启动压缩流程。\n- **`isolation_suitable()`**: 检查指定 pageblock 是否适合进行页面隔离（用于迁移）。\n- **`reset_cached_positions()`**: 重置压缩控制结构中缓存的扫描位置（迁移源和空闲目标页的起始 PFN）。\n- **`release_free_list()`**: 将临时空闲页面列表中的页面释放回伙伴系统。\n- **`skip_offline_sections()` / `skip_offline_sections_reverse()`**: 在 SPARSEMEM 内存模型下跳过离线内存段。\n\n### 关键数据结构与宏\n- **`struct compact_control`**: 压缩操作的控制上下文（定义在 `internal.h` 中），包含扫描范围、迁移/空闲页面列表等。\n- **`COMPACTION_HPAGE_ORDER`**: 用于计算节点/区域“碎片分数（fragmentation score）”的参考阶数，通常为透明大页或 HugeTLB 的阶数。\n- **`COMPACT_MAX_DEFER_SHIFT`**: 压缩延迟的最大次数（64 次）。\n- **`zone->compact_*` 字段**: 包括 `compact_considered`, `compact_defer_shift`, `compact_order_failed`, `compact_cached_*_pfn` 等，用于跟踪压缩状态和优化扫描。\n\n## 3. 关键实现\n\n### 内存压缩流程\n1. **扫描阶段**：从区域两端向中间扫描：\n   - **迁移源扫描器（migrate scanner）**：从低地址向高地址扫描，寻找可迁移的页面。\n   - **空闲目标扫描器（free scanner）**：从高地址向低地址扫描，寻找空闲页面块。\n2. **页面隔离**：将找到的可迁移页面加入迁移列表，空闲页面加入空闲列表。\n3. **页面迁移**：调用 `migrate_pages()` 将迁移列表中的页面移动到空闲列表提供的目标位置。\n4. **结果处理**：迁移成功后，原位置变为空闲，可能形成更大的连续空闲块；失败则回滚。\n\n### 压缩延迟机制（Deferred Compaction）\n- 当压缩未能成功分配指定 `order` 的页面时，调用 `defer_compaction()` 增加延迟计数器 `compact_defer_shift`。\n- 后续分配请求会通过 `compaction_deferred()` 检查是否应跳过压缩（基于 `compact_considered` 计数和 `defer_limit = 1 << compact_defer_shift`）。\n- 若分配成功或预期成功，则通过 `compaction_defer_reset()` 重置延迟状态。\n- 当延迟达到最大值（`COMPACT_MAX_DEFER_SHIFT`）且考虑次数足够多时，`compaction_restarting()` 返回 true，强制重启完整压缩流程。\n\n### 碎片分数与主动压缩\n- 通过 `COMPACTION_HPAGE_ORDER` 定义的阶数评估区域的外部碎片程度。\n- 主动压缩（proactive compaction）定期（`HPAGE_FRAG_CHECK_INTERVAL_MSEC = 500ms`）检查碎片分数，并在需要时触发后台压缩。\n- `is_via_compact_memory()` 用于识别通过 `/proc/sys/vm/compact_memory` 等接口发起的全量压缩请求（`order = -1`）。\n\n### 页面可移动性管理\n- `__SetPageMovable()` 将 `movable_operations` 指针编码到 `page->mapping` 中，并设置 `PAGE_MAPPING_MOVABLE` 标志。\n- `PageMovable()` 验证页面是否被正确标记为可移动，并确保其操作集有效。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`, `internal.h`, `mm_inline.h` 提供的伙伴系统、页面分配/释放、zone 管理等基础功能。\n- **页面迁移**：紧密集成 `<linux/migrate.h>`，压缩的核心操作由 `migrate_pages()` 完成。\n- **内存模型**：针对 `CONFIG_SPARSEMEM` 提供离线内存段跳过逻辑。\n- **大页支持**：根据 `CONFIG_TRANSPARENT_HUGEPAGE` 或 `CONFIG_HUGETLBFS` 确定 `COMPACTION_HPAGE_ORDER`。\n- **调试与追踪**：使用 `kasan.h`, `page_owner.h` 进行调试；通过 `trace/events/compaction.h` 提供 ftrace 事件。\n- **系统调用与接口**：通过 sysctl (`/proc/sys/vm/`) 和 sysfs (`/sys/devices/system/node/`) 暴露用户空间控制接口。\n- **进程与调度**：使用 `kthread.h`, `freezer.h` 管理后台压缩线程；`psi.h` 用于压力状态监控。\n\n## 5. 使用场景\n\n1. **高阶内存分配失败时**：当 `alloc_pages()` 请求高阶页面（如 order ≥ 3）失败时，内核会尝试同步内存压缩以满足请求。\n2. **透明大页（THP）分配**：THP 需要连续的 2MB 物理内存，压缩是满足此类请求的关键机制。\n3. **主动后台压缩**：通过 `vm.compaction_proactiveness` sysctl 参数配置，内核定期评估内存碎片并主动压缩，预防分配延迟。\n4. **用户空间触发**：管理员可通过写入 `/proc/sys/vm/compact_memory` 或特定 NUMA 节点的 `compact` sysfs 文件，手动触发全量内存压缩。\n5. **CMA（Contiguous Memory Allocator）**：在 CMA 区域分配大块连续内存前，使用压缩机制迁移占用页面以腾出空间（需 `CONFIG_CMA`）。",
      "similarity": 0.4940439462661743,
      "chunks": [
        {
          "chunk_id": 15,
          "file_path": "mm/compaction.c",
          "start_line": 2743,
          "end_line": 2857,
          "content": [
            "static enum compact_result compact_zone_order(struct zone *zone, int order,",
            "\t\tgfp_t gfp_mask, enum compact_priority prio,",
            "\t\tunsigned int alloc_flags, int highest_zoneidx,",
            "\t\tstruct page **capture)",
            "{",
            "\tenum compact_result ret;",
            "\tstruct compact_control cc = {",
            "\t\t.order = order,",
            "\t\t.search_order = order,",
            "\t\t.gfp_mask = gfp_mask,",
            "\t\t.zone = zone,",
            "\t\t.mode = (prio == COMPACT_PRIO_ASYNC) ?",
            "\t\t\t\t\tMIGRATE_ASYNC :\tMIGRATE_SYNC_LIGHT,",
            "\t\t.alloc_flags = alloc_flags,",
            "\t\t.highest_zoneidx = highest_zoneidx,",
            "\t\t.direct_compaction = true,",
            "\t\t.whole_zone = (prio == MIN_COMPACT_PRIORITY),",
            "\t\t.ignore_skip_hint = (prio == MIN_COMPACT_PRIORITY),",
            "\t\t.ignore_block_suitable = (prio == MIN_COMPACT_PRIORITY)",
            "\t};",
            "\tstruct capture_control capc = {",
            "\t\t.cc = &cc,",
            "\t\t.page = NULL,",
            "\t};",
            "",
            "\t/*",
            "\t * Make sure the structs are really initialized before we expose the",
            "\t * capture control, in case we are interrupted and the interrupt handler",
            "\t * frees a page.",
            "\t */",
            "\tbarrier();",
            "\tWRITE_ONCE(current->capture_control, &capc);",
            "",
            "\tret = compact_zone(&cc, &capc);",
            "",
            "\t/*",
            "\t * Make sure we hide capture control first before we read the captured",
            "\t * page pointer, otherwise an interrupt could free and capture a page",
            "\t * and we would leak it.",
            "\t */",
            "\tWRITE_ONCE(current->capture_control, NULL);",
            "\t*capture = READ_ONCE(capc.page);",
            "\t/*",
            "\t * Technically, it is also possible that compaction is skipped but",
            "\t * the page is still captured out of luck(IRQ came and freed the page).",
            "\t * Returning COMPACT_SUCCESS in such cases helps in properly accounting",
            "\t * the COMPACT[STALL|FAIL] when compaction is skipped.",
            "\t */",
            "\tif (*capture)",
            "\t\tret = COMPACT_SUCCESS;",
            "",
            "\treturn ret;",
            "}",
            "enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,",
            "\t\tunsigned int alloc_flags, const struct alloc_context *ac,",
            "\t\tenum compact_priority prio, struct page **capture)",
            "{",
            "\tstruct zoneref *z;",
            "\tstruct zone *zone;",
            "\tenum compact_result rc = COMPACT_SKIPPED;",
            "",
            "\tif (!gfp_compaction_allowed(gfp_mask))",
            "\t\treturn COMPACT_SKIPPED;",
            "",
            "\ttrace_mm_compaction_try_to_compact_pages(order, gfp_mask, prio);",
            "",
            "\t/* Compact each zone in the list */",
            "\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist,",
            "\t\t\t\t\tac->highest_zoneidx, ac->nodemask) {",
            "\t\tenum compact_result status;",
            "",
            "\t\tif (prio > MIN_COMPACT_PRIORITY",
            "\t\t\t\t\t&& compaction_deferred(zone, order)) {",
            "\t\t\trc = max_t(enum compact_result, COMPACT_DEFERRED, rc);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tstatus = compact_zone_order(zone, order, gfp_mask, prio,",
            "\t\t\t\talloc_flags, ac->highest_zoneidx, capture);",
            "\t\trc = max(status, rc);",
            "",
            "\t\t/* The allocation should succeed, stop compacting */",
            "\t\tif (status == COMPACT_SUCCESS) {",
            "\t\t\t/*",
            "\t\t\t * We think the allocation will succeed in this zone,",
            "\t\t\t * but it is not certain, hence the false. The caller",
            "\t\t\t * will repeat this with true if allocation indeed",
            "\t\t\t * succeeds in this zone.",
            "\t\t\t */",
            "\t\t\tcompaction_defer_reset(zone, order, false);",
            "",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tif (prio != COMPACT_PRIO_ASYNC && (status == COMPACT_COMPLETE ||",
            "\t\t\t\t\tstatus == COMPACT_PARTIAL_SKIPPED))",
            "\t\t\t/*",
            "\t\t\t * We think that allocation won't succeed in this zone",
            "\t\t\t * so we defer compaction there. If it ends up",
            "\t\t\t * succeeding after all, it will be reset.",
            "\t\t\t */",
            "\t\t\tdefer_compaction(zone, order);",
            "",
            "\t\t/*",
            "\t\t * We might have stopped compacting due to need_resched() in",
            "\t\t * async compaction, or due to a fatal signal detected. In that",
            "\t\t * case do not try further zones",
            "\t\t */",
            "\t\tif ((prio == COMPACT_PRIO_ASYNC && need_resched())",
            "\t\t\t\t\t|| fatal_signal_pending(current))",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn rc;",
            "}"
          ],
          "function_name": "compact_zone_order, try_to_compact_pages",
          "description": "实现compact_zone_order函数，用于在指定zone中执行内存紧缩操作，根据优先级设置不同模式，并通过捕获页指针处理异常情况。try_to_compact_pages函数遍历所有zone尝试紧缩，根据优先级和状态决定是否跳过或延迟紧缩操作。",
          "similarity": 0.536849856376648
        },
        {
          "chunk_id": 12,
          "file_path": "mm/compaction.c",
          "start_line": 2200,
          "end_line": 2344,
          "content": [
            "static unsigned int fragmentation_score_zone(struct zone *zone)",
            "{",
            "\treturn extfrag_for_order(zone, COMPACTION_HPAGE_ORDER);",
            "}",
            "static unsigned int fragmentation_score_zone_weighted(struct zone *zone)",
            "{",
            "\tunsigned long score;",
            "",
            "\tscore = zone->present_pages * fragmentation_score_zone(zone);",
            "\treturn div64_ul(score, zone->zone_pgdat->node_present_pages + 1);",
            "}",
            "static unsigned int fragmentation_score_node(pg_data_t *pgdat)",
            "{",
            "\tunsigned int score = 0;",
            "\tint zoneid;",
            "",
            "\tfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {",
            "\t\tstruct zone *zone;",
            "",
            "\t\tzone = &pgdat->node_zones[zoneid];",
            "\t\tif (!populated_zone(zone))",
            "\t\t\tcontinue;",
            "\t\tscore += fragmentation_score_zone_weighted(zone);",
            "\t}",
            "",
            "\treturn score;",
            "}",
            "static unsigned int fragmentation_score_wmark(bool low)",
            "{",
            "\tunsigned int wmark_low;",
            "",
            "\t/*",
            "\t * Cap the low watermark to avoid excessive compaction",
            "\t * activity in case a user sets the proactiveness tunable",
            "\t * close to 100 (maximum).",
            "\t */",
            "\twmark_low = max(100U - sysctl_compaction_proactiveness, 5U);",
            "\treturn low ? wmark_low : min(wmark_low + 10, 100U);",
            "}",
            "static bool should_proactive_compact_node(pg_data_t *pgdat)",
            "{",
            "\tint wmark_high;",
            "",
            "\tif (!sysctl_compaction_proactiveness || kswapd_is_running(pgdat))",
            "\t\treturn false;",
            "",
            "\twmark_high = fragmentation_score_wmark(false);",
            "\treturn fragmentation_score_node(pgdat) > wmark_high;",
            "}",
            "static enum compact_result __compact_finished(struct compact_control *cc)",
            "{",
            "\tunsigned int order;",
            "\tconst int migratetype = cc->migratetype;",
            "\tint ret;",
            "",
            "\t/* Compaction run completes if the migrate and free scanner meet */",
            "\tif (compact_scanners_met(cc)) {",
            "\t\t/* Let the next compaction start anew. */",
            "\t\treset_cached_positions(cc->zone);",
            "",
            "\t\t/*",
            "\t\t * Mark that the PG_migrate_skip information should be cleared",
            "\t\t * by kswapd when it goes to sleep. kcompactd does not set the",
            "\t\t * flag itself as the decision to be clear should be directly",
            "\t\t * based on an allocation request.",
            "\t\t */",
            "\t\tif (cc->direct_compaction)",
            "\t\t\tcc->zone->compact_blockskip_flush = true;",
            "",
            "\t\tif (cc->whole_zone)",
            "\t\t\treturn COMPACT_COMPLETE;",
            "\t\telse",
            "\t\t\treturn COMPACT_PARTIAL_SKIPPED;",
            "\t}",
            "",
            "\tif (cc->proactive_compaction) {",
            "\t\tint score, wmark_low;",
            "\t\tpg_data_t *pgdat;",
            "",
            "\t\tpgdat = cc->zone->zone_pgdat;",
            "\t\tif (kswapd_is_running(pgdat))",
            "\t\t\treturn COMPACT_PARTIAL_SKIPPED;",
            "",
            "\t\tscore = fragmentation_score_zone(cc->zone);",
            "\t\twmark_low = fragmentation_score_wmark(true);",
            "",
            "\t\tif (score > wmark_low)",
            "\t\t\tret = COMPACT_CONTINUE;",
            "\t\telse",
            "\t\t\tret = COMPACT_SUCCESS;",
            "",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tif (is_via_compact_memory(cc->order))",
            "\t\treturn COMPACT_CONTINUE;",
            "",
            "\t/*",
            "\t * Always finish scanning a pageblock to reduce the possibility of",
            "\t * fallbacks in the future. This is particularly important when",
            "\t * migration source is unmovable/reclaimable but it's not worth",
            "\t * special casing.",
            "\t */",
            "\tif (!pageblock_aligned(cc->migrate_pfn))",
            "\t\treturn COMPACT_CONTINUE;",
            "",
            "\t/* Direct compactor: Is a suitable page free? */",
            "\tret = COMPACT_NO_SUITABLE_PAGE;",
            "\tfor (order = cc->order; order < NR_PAGE_ORDERS; order++) {",
            "\t\tstruct free_area *area = &cc->zone->free_area[order];",
            "\t\tbool can_steal;",
            "",
            "\t\t/* Job done if page is free of the right migratetype */",
            "\t\tif (!free_area_empty(area, migratetype))",
            "\t\t\treturn COMPACT_SUCCESS;",
            "",
            "#ifdef CONFIG_CMA",
            "\t\t/* MIGRATE_MOVABLE can fallback on MIGRATE_CMA */",
            "\t\tif (migratetype == MIGRATE_MOVABLE &&",
            "\t\t\t!free_area_empty(area, MIGRATE_CMA))",
            "\t\t\treturn COMPACT_SUCCESS;",
            "#endif",
            "\t\t/*",
            "\t\t * Job done if allocation would steal freepages from",
            "\t\t * other migratetype buddy lists.",
            "\t\t */",
            "\t\tif (find_suitable_fallback(area, order, migratetype,",
            "\t\t\t\t\t\ttrue, &can_steal) != -1)",
            "\t\t\t/*",
            "\t\t\t * Movable pages are OK in any pageblock. If we are",
            "\t\t\t * stealing for a non-movable allocation, make sure",
            "\t\t\t * we finish compacting the current pageblock first",
            "\t\t\t * (which is assured by the above migrate_pfn align",
            "\t\t\t * check) so it is as free as possible and we won't",
            "\t\t\t * have to steal another one soon.",
            "\t\t\t */",
            "\t\t\treturn COMPACT_SUCCESS;",
            "\t}",
            "",
            "out:",
            "\tif (cc->contended || fatal_signal_pending(current))",
            "\t\tret = COMPACT_CONTENDED;",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "fragmentation_score_zone, fragmentation_score_zone_weighted, fragmentation_score_node, fragmentation_score_wmark, should_proactive_compact_node, __compact_finished",
          "description": "fragmentation_score_zone计算区碎片评分，fragmentation_score_zone_weighted加权计算节点总分，fragmentation_score_wmark设置低水位阈值，should_proactive_compact_node根据评分判断是否需要主动压缩",
          "similarity": 0.535163164138794
        },
        {
          "chunk_id": 14,
          "file_path": "mm/compaction.c",
          "start_line": 2489,
          "end_line": 2740,
          "content": [
            "static enum compact_result",
            "compaction_suit_allocation_order(struct zone *zone, unsigned int order,",
            "\t\t\t\t int highest_zoneidx, unsigned int alloc_flags)",
            "{",
            "\tunsigned long watermark;",
            "",
            "\twatermark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK);",
            "\tif (zone_watermark_ok(zone, order, watermark, highest_zoneidx,",
            "\t\t\t      alloc_flags))",
            "\t\treturn COMPACT_SUCCESS;",
            "",
            "\tif (!compaction_suitable(zone, order, highest_zoneidx))",
            "\t\treturn COMPACT_SKIPPED;",
            "",
            "\treturn COMPACT_CONTINUE;",
            "}",
            "static enum compact_result",
            "compact_zone(struct compact_control *cc, struct capture_control *capc)",
            "{",
            "\tenum compact_result ret;",
            "\tunsigned long start_pfn = cc->zone->zone_start_pfn;",
            "\tunsigned long end_pfn = zone_end_pfn(cc->zone);",
            "\tunsigned long last_migrated_pfn;",
            "\tconst bool sync = cc->mode != MIGRATE_ASYNC;",
            "\tbool update_cached;",
            "\tunsigned int nr_succeeded = 0, nr_migratepages;",
            "\tint order;",
            "",
            "\t/*",
            "\t * These counters track activities during zone compaction.  Initialize",
            "\t * them before compacting a new zone.",
            "\t */",
            "\tcc->total_migrate_scanned = 0;",
            "\tcc->total_free_scanned = 0;",
            "\tcc->nr_migratepages = 0;",
            "\tcc->nr_freepages = 0;",
            "\tfor (order = 0; order < NR_PAGE_ORDERS; order++)",
            "\t\tINIT_LIST_HEAD(&cc->freepages[order]);",
            "\tINIT_LIST_HEAD(&cc->migratepages);",
            "",
            "\tcc->migratetype = gfp_migratetype(cc->gfp_mask);",
            "",
            "\tif (!is_via_compact_memory(cc->order)) {",
            "\t\tret = compaction_suit_allocation_order(cc->zone, cc->order,",
            "\t\t\t\t\t\t       cc->highest_zoneidx,",
            "\t\t\t\t\t\t       cc->alloc_flags);",
            "\t\tif (ret != COMPACT_CONTINUE)",
            "\t\t\treturn ret;",
            "\t}",
            "",
            "\t/*",
            "\t * Clear pageblock skip if there were failures recently and compaction",
            "\t * is about to be retried after being deferred.",
            "\t */",
            "\tif (compaction_restarting(cc->zone, cc->order))",
            "\t\t__reset_isolation_suitable(cc->zone);",
            "",
            "\t/*",
            "\t * Setup to move all movable pages to the end of the zone. Used cached",
            "\t * information on where the scanners should start (unless we explicitly",
            "\t * want to compact the whole zone), but check that it is initialised",
            "\t * by ensuring the values are within zone boundaries.",
            "\t */",
            "\tcc->fast_start_pfn = 0;",
            "\tif (cc->whole_zone) {",
            "\t\tcc->migrate_pfn = start_pfn;",
            "\t\tcc->free_pfn = pageblock_start_pfn(end_pfn - 1);",
            "\t} else {",
            "\t\tcc->migrate_pfn = cc->zone->compact_cached_migrate_pfn[sync];",
            "\t\tcc->free_pfn = cc->zone->compact_cached_free_pfn;",
            "\t\tif (cc->free_pfn < start_pfn || cc->free_pfn >= end_pfn) {",
            "\t\t\tcc->free_pfn = pageblock_start_pfn(end_pfn - 1);",
            "\t\t\tcc->zone->compact_cached_free_pfn = cc->free_pfn;",
            "\t\t}",
            "\t\tif (cc->migrate_pfn < start_pfn || cc->migrate_pfn >= end_pfn) {",
            "\t\t\tcc->migrate_pfn = start_pfn;",
            "\t\t\tcc->zone->compact_cached_migrate_pfn[0] = cc->migrate_pfn;",
            "\t\t\tcc->zone->compact_cached_migrate_pfn[1] = cc->migrate_pfn;",
            "\t\t}",
            "",
            "\t\tif (cc->migrate_pfn <= cc->zone->compact_init_migrate_pfn)",
            "\t\t\tcc->whole_zone = true;",
            "\t}",
            "",
            "\tlast_migrated_pfn = 0;",
            "",
            "\t/*",
            "\t * Migrate has separate cached PFNs for ASYNC and SYNC* migration on",
            "\t * the basis that some migrations will fail in ASYNC mode. However,",
            "\t * if the cached PFNs match and pageblocks are skipped due to having",
            "\t * no isolation candidates, then the sync state does not matter.",
            "\t * Until a pageblock with isolation candidates is found, keep the",
            "\t * cached PFNs in sync to avoid revisiting the same blocks.",
            "\t */",
            "\tupdate_cached = !sync &&",
            "\t\tcc->zone->compact_cached_migrate_pfn[0] == cc->zone->compact_cached_migrate_pfn[1];",
            "",
            "\ttrace_mm_compaction_begin(cc, start_pfn, end_pfn, sync);",
            "",
            "\t/* lru_add_drain_all could be expensive with involving other CPUs */",
            "\tlru_add_drain();",
            "",
            "\twhile ((ret = compact_finished(cc)) == COMPACT_CONTINUE) {",
            "\t\tint err;",
            "\t\tunsigned long iteration_start_pfn = cc->migrate_pfn;",
            "",
            "\t\t/*",
            "\t\t * Avoid multiple rescans of the same pageblock which can",
            "\t\t * happen if a page cannot be isolated (dirty/writeback in",
            "\t\t * async mode) or if the migrated pages are being allocated",
            "\t\t * before the pageblock is cleared.  The first rescan will",
            "\t\t * capture the entire pageblock for migration. If it fails,",
            "\t\t * it'll be marked skip and scanning will proceed as normal.",
            "\t\t */",
            "\t\tcc->finish_pageblock = false;",
            "\t\tif (pageblock_start_pfn(last_migrated_pfn) ==",
            "\t\t    pageblock_start_pfn(iteration_start_pfn)) {",
            "\t\t\tcc->finish_pageblock = true;",
            "\t\t}",
            "",
            "rescan:",
            "\t\tswitch (isolate_migratepages(cc)) {",
            "\t\tcase ISOLATE_ABORT:",
            "\t\t\tret = COMPACT_CONTENDED;",
            "\t\t\tputback_movable_pages(&cc->migratepages);",
            "\t\t\tcc->nr_migratepages = 0;",
            "\t\t\tgoto out;",
            "\t\tcase ISOLATE_NONE:",
            "\t\t\tif (update_cached) {",
            "\t\t\t\tcc->zone->compact_cached_migrate_pfn[1] =",
            "\t\t\t\t\tcc->zone->compact_cached_migrate_pfn[0];",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * We haven't isolated and migrated anything, but",
            "\t\t\t * there might still be unflushed migrations from",
            "\t\t\t * previous cc->order aligned block.",
            "\t\t\t */",
            "\t\t\tgoto check_drain;",
            "\t\tcase ISOLATE_SUCCESS:",
            "\t\t\tupdate_cached = false;",
            "\t\t\tlast_migrated_pfn = max(cc->zone->zone_start_pfn,",
            "\t\t\t\tpageblock_start_pfn(cc->migrate_pfn - 1));",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Record the number of pages to migrate since the",
            "\t\t * compaction_alloc/free() will update cc->nr_migratepages",
            "\t\t * properly.",
            "\t\t */",
            "\t\tnr_migratepages = cc->nr_migratepages;",
            "\t\terr = migrate_pages(&cc->migratepages, compaction_alloc,",
            "\t\t\t\tcompaction_free, (unsigned long)cc, cc->mode,",
            "\t\t\t\tMR_COMPACTION, &nr_succeeded);",
            "",
            "\t\ttrace_mm_compaction_migratepages(nr_migratepages, nr_succeeded);",
            "",
            "\t\t/* All pages were either migrated or will be released */",
            "\t\tcc->nr_migratepages = 0;",
            "\t\tif (err) {",
            "\t\t\tputback_movable_pages(&cc->migratepages);",
            "\t\t\t/*",
            "\t\t\t * migrate_pages() may return -ENOMEM when scanners meet",
            "\t\t\t * and we want compact_finished() to detect it",
            "\t\t\t */",
            "\t\t\tif (err == -ENOMEM && !compact_scanners_met(cc)) {",
            "\t\t\t\tret = COMPACT_CONTENDED;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t\t/*",
            "\t\t\t * If an ASYNC or SYNC_LIGHT fails to migrate a page",
            "\t\t\t * within the pageblock_order-aligned block and",
            "\t\t\t * fast_find_migrateblock may be used then scan the",
            "\t\t\t * remainder of the pageblock. This will mark the",
            "\t\t\t * pageblock \"skip\" to avoid rescanning in the near",
            "\t\t\t * future. This will isolate more pages than necessary",
            "\t\t\t * for the request but avoid loops due to",
            "\t\t\t * fast_find_migrateblock revisiting blocks that were",
            "\t\t\t * recently partially scanned.",
            "\t\t\t */",
            "\t\t\tif (!pageblock_aligned(cc->migrate_pfn) &&",
            "\t\t\t    !cc->ignore_skip_hint && !cc->finish_pageblock &&",
            "\t\t\t    (cc->mode < MIGRATE_SYNC)) {",
            "\t\t\t\tcc->finish_pageblock = true;",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * Draining pcplists does not help THP if",
            "\t\t\t\t * any page failed to migrate. Even after",
            "\t\t\t\t * drain, the pageblock will not be free.",
            "\t\t\t\t */",
            "\t\t\t\tif (cc->order == COMPACTION_HPAGE_ORDER)",
            "\t\t\t\t\tlast_migrated_pfn = 0;",
            "",
            "\t\t\t\tgoto rescan;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* Stop if a page has been captured */",
            "\t\tif (capc && capc->page) {",
            "\t\t\tret = COMPACT_SUCCESS;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "check_drain:",
            "\t\t/*",
            "\t\t * Has the migration scanner moved away from the previous",
            "\t\t * cc->order aligned block where we migrated from? If yes,",
            "\t\t * flush the pages that were freed, so that they can merge and",
            "\t\t * compact_finished() can detect immediately if allocation",
            "\t\t * would succeed.",
            "\t\t */",
            "\t\tif (cc->order > 0 && last_migrated_pfn) {",
            "\t\t\tunsigned long current_block_start =",
            "\t\t\t\tblock_start_pfn(cc->migrate_pfn, cc->order);",
            "",
            "\t\t\tif (last_migrated_pfn < current_block_start) {",
            "\t\t\t\tlru_add_drain_cpu_zone(cc->zone);",
            "\t\t\t\t/* No more flushing until we migrate again */",
            "\t\t\t\tlast_migrated_pfn = 0;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "out:",
            "\t/*",
            "\t * Release free pages and update where the free scanner should restart,",
            "\t * so we don't leave any returned pages behind in the next attempt.",
            "\t */",
            "\tif (cc->nr_freepages > 0) {",
            "\t\tunsigned long free_pfn = release_free_list(cc->freepages);",
            "",
            "\t\tcc->nr_freepages = 0;",
            "\t\tVM_BUG_ON(free_pfn == 0);",
            "\t\t/* The cached pfn is always the first in a pageblock */",
            "\t\tfree_pfn = pageblock_start_pfn(free_pfn);",
            "\t\t/*",
            "\t\t * Only go back, not forward. The cached pfn might have been",
            "\t\t * already reset to zone end in compact_finished()",
            "\t\t */",
            "\t\tif (free_pfn > cc->zone->compact_cached_free_pfn)",
            "\t\t\tcc->zone->compact_cached_free_pfn = free_pfn;",
            "\t}",
            "",
            "\tcount_compact_events(COMPACTMIGRATE_SCANNED, cc->total_migrate_scanned);",
            "\tcount_compact_events(COMPACTFREE_SCANNED, cc->total_free_scanned);",
            "",
            "\ttrace_mm_compaction_end(cc, start_pfn, end_pfn, sync, ret);",
            "",
            "\tVM_BUG_ON(!list_empty(&cc->migratepages));",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "compaction_suit_allocation_order, compact_zone",
          "description": "compaction_suit_allocation_order检查分配顺序兼容性，compact_zone执行核心压缩流程，迁移页面并调整缓存扫描起点，处理页块扫描和迁移结果",
          "similarity": 0.5248655676841736
        },
        {
          "chunk_id": 10,
          "file_path": "mm/compaction.c",
          "start_line": 1888,
          "end_line": 2042,
          "content": [
            "static void compaction_free(struct folio *dst, unsigned long data)",
            "{",
            "\tstruct compact_control *cc = (struct compact_control *)data;",
            "\tint order = folio_order(dst);",
            "\tstruct page *page = &dst->page;",
            "",
            "\tif (folio_put_testzero(dst)) {",
            "\t\tfree_pages_prepare(page, order);",
            "\t\tlist_add(&dst->lru, &cc->freepages[order]);",
            "\t\tcc->nr_freepages += 1 << order;",
            "\t}",
            "\tcc->nr_migratepages += 1 << order;",
            "\t/*",
            "\t * someone else has referenced the page, we cannot take it back to our",
            "\t * free list.",
            "\t */",
            "}",
            "static inline void",
            "update_fast_start_pfn(struct compact_control *cc, unsigned long pfn)",
            "{",
            "\tif (cc->fast_start_pfn == ULONG_MAX)",
            "\t\treturn;",
            "",
            "\tif (!cc->fast_start_pfn)",
            "\t\tcc->fast_start_pfn = pfn;",
            "",
            "\tcc->fast_start_pfn = min(cc->fast_start_pfn, pfn);",
            "}",
            "static inline unsigned long",
            "reinit_migrate_pfn(struct compact_control *cc)",
            "{",
            "\tif (!cc->fast_start_pfn || cc->fast_start_pfn == ULONG_MAX)",
            "\t\treturn cc->migrate_pfn;",
            "",
            "\tcc->migrate_pfn = cc->fast_start_pfn;",
            "\tcc->fast_start_pfn = ULONG_MAX;",
            "",
            "\treturn cc->migrate_pfn;",
            "}",
            "static unsigned long fast_find_migrateblock(struct compact_control *cc)",
            "{",
            "\tunsigned int limit = freelist_scan_limit(cc);",
            "\tunsigned int nr_scanned = 0;",
            "\tunsigned long distance;",
            "\tunsigned long pfn = cc->migrate_pfn;",
            "\tunsigned long high_pfn;",
            "\tint order;",
            "\tbool found_block = false;",
            "",
            "\t/* Skip hints are relied on to avoid repeats on the fast search */",
            "\tif (cc->ignore_skip_hint)",
            "\t\treturn pfn;",
            "",
            "\t/*",
            "\t * If the pageblock should be finished then do not select a different",
            "\t * pageblock.",
            "\t */",
            "\tif (cc->finish_pageblock)",
            "\t\treturn pfn;",
            "",
            "\t/*",
            "\t * If the migrate_pfn is not at the start of a zone or the start",
            "\t * of a pageblock then assume this is a continuation of a previous",
            "\t * scan restarted due to COMPACT_CLUSTER_MAX.",
            "\t */",
            "\tif (pfn != cc->zone->zone_start_pfn && pfn != pageblock_start_pfn(pfn))",
            "\t\treturn pfn;",
            "",
            "\t/*",
            "\t * For smaller orders, just linearly scan as the number of pages",
            "\t * to migrate should be relatively small and does not necessarily",
            "\t * justify freeing up a large block for a small allocation.",
            "\t */",
            "\tif (cc->order <= PAGE_ALLOC_COSTLY_ORDER)",
            "\t\treturn pfn;",
            "",
            "\t/*",
            "\t * Only allow kcompactd and direct requests for movable pages to",
            "\t * quickly clear out a MOVABLE pageblock for allocation. This",
            "\t * reduces the risk that a large movable pageblock is freed for",
            "\t * an unmovable/reclaimable small allocation.",
            "\t */",
            "\tif (cc->direct_compaction && cc->migratetype != MIGRATE_MOVABLE)",
            "\t\treturn pfn;",
            "",
            "\t/*",
            "\t * When starting the migration scanner, pick any pageblock within the",
            "\t * first half of the search space. Otherwise try and pick a pageblock",
            "\t * within the first eighth to reduce the chances that a migration",
            "\t * target later becomes a source.",
            "\t */",
            "\tdistance = (cc->free_pfn - cc->migrate_pfn) >> 1;",
            "\tif (cc->migrate_pfn != cc->zone->zone_start_pfn)",
            "\t\tdistance >>= 2;",
            "\thigh_pfn = pageblock_start_pfn(cc->migrate_pfn + distance);",
            "",
            "\tfor (order = cc->order - 1;",
            "\t     order >= PAGE_ALLOC_COSTLY_ORDER && !found_block && nr_scanned < limit;",
            "\t     order--) {",
            "\t\tstruct free_area *area = &cc->zone->free_area[order];",
            "\t\tstruct list_head *freelist;",
            "\t\tunsigned long flags;",
            "\t\tstruct page *freepage;",
            "",
            "\t\tif (!area->nr_free)",
            "\t\t\tcontinue;",
            "",
            "\t\tspin_lock_irqsave(&cc->zone->lock, flags);",
            "\t\tfreelist = &area->free_list[MIGRATE_MOVABLE];",
            "\t\tlist_for_each_entry(freepage, freelist, buddy_list) {",
            "\t\t\tunsigned long free_pfn;",
            "",
            "\t\t\tif (nr_scanned++ >= limit) {",
            "\t\t\t\tmove_freelist_tail(freelist, freepage);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "",
            "\t\t\tfree_pfn = page_to_pfn(freepage);",
            "\t\t\tif (free_pfn < high_pfn) {",
            "\t\t\t\t/*",
            "\t\t\t\t * Avoid if skipped recently. Ideally it would",
            "\t\t\t\t * move to the tail but even safe iteration of",
            "\t\t\t\t * the list assumes an entry is deleted, not",
            "\t\t\t\t * reordered.",
            "\t\t\t\t */",
            "\t\t\t\tif (get_pageblock_skip(freepage))",
            "\t\t\t\t\tcontinue;",
            "",
            "\t\t\t\t/* Reorder to so a future search skips recent pages */",
            "\t\t\t\tmove_freelist_tail(freelist, freepage);",
            "",
            "\t\t\t\tupdate_fast_start_pfn(cc, free_pfn);",
            "\t\t\t\tpfn = pageblock_start_pfn(free_pfn);",
            "\t\t\t\tif (pfn < cc->zone->zone_start_pfn)",
            "\t\t\t\t\tpfn = cc->zone->zone_start_pfn;",
            "\t\t\t\tcc->fast_search_fail = 0;",
            "\t\t\t\tfound_block = true;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);",
            "\t}",
            "",
            "\tcc->total_migrate_scanned += nr_scanned;",
            "",
            "\t/*",
            "\t * If fast scanning failed then use a cached entry for a page block",
            "\t * that had free pages as the basis for starting a linear scan.",
            "\t */",
            "\tif (!found_block) {",
            "\t\tcc->fast_search_fail++;",
            "\t\tpfn = reinit_migrate_pfn(cc);",
            "\t}",
            "\treturn pfn;",
            "}"
          ],
          "function_name": "compaction_free, update_fast_start_pfn, reinit_migrate_pfn, fast_find_migrateblock",
          "description": "实现内存碎片整理中页面释放与迁移扫描逻辑，compaction_free处理页面回收，update_fast_start_pfn维护快速扫描起点，reinit_migrate_pfn重置迁移扫描位置，fast_find_migrateblock寻找适合迁移的页块，优先考虑可移动类型且未被跳过的页块",
          "similarity": 0.4922640025615692
        },
        {
          "chunk_id": 5,
          "file_path": "mm/compaction.c",
          "start_line": 725,
          "end_line": 837,
          "content": [
            "unsigned long",
            "isolate_freepages_range(struct compact_control *cc,",
            "\t\t\tunsigned long start_pfn, unsigned long end_pfn)",
            "{",
            "\tunsigned long isolated, pfn, block_start_pfn, block_end_pfn;",
            "\tint order;",
            "",
            "\tfor (order = 0; order < NR_PAGE_ORDERS; order++)",
            "\t\tINIT_LIST_HEAD(&cc->freepages[order]);",
            "",
            "\tpfn = start_pfn;",
            "\tblock_start_pfn = pageblock_start_pfn(pfn);",
            "\tif (block_start_pfn < cc->zone->zone_start_pfn)",
            "\t\tblock_start_pfn = cc->zone->zone_start_pfn;",
            "\tblock_end_pfn = pageblock_end_pfn(pfn);",
            "",
            "\tfor (; pfn < end_pfn; pfn += isolated,",
            "\t\t\t\tblock_start_pfn = block_end_pfn,",
            "\t\t\t\tblock_end_pfn += pageblock_nr_pages) {",
            "\t\t/* Protect pfn from changing by isolate_freepages_block */",
            "\t\tunsigned long isolate_start_pfn = pfn;",
            "",
            "\t\t/*",
            "\t\t * pfn could pass the block_end_pfn if isolated freepage",
            "\t\t * is more than pageblock order. In this case, we adjust",
            "\t\t * scanning range to right one.",
            "\t\t */",
            "\t\tif (pfn >= block_end_pfn) {",
            "\t\t\tblock_start_pfn = pageblock_start_pfn(pfn);",
            "\t\t\tblock_end_pfn = pageblock_end_pfn(pfn);",
            "\t\t}",
            "",
            "\t\tblock_end_pfn = min(block_end_pfn, end_pfn);",
            "",
            "\t\tif (!pageblock_pfn_to_page(block_start_pfn,",
            "\t\t\t\t\tblock_end_pfn, cc->zone))",
            "\t\t\tbreak;",
            "",
            "\t\tisolated = isolate_freepages_block(cc, &isolate_start_pfn,",
            "\t\t\t\t\tblock_end_pfn, cc->freepages, 0, true);",
            "",
            "\t\t/*",
            "\t\t * In strict mode, isolate_freepages_block() returns 0 if",
            "\t\t * there are any holes in the block (ie. invalid PFNs or",
            "\t\t * non-free pages).",
            "\t\t */",
            "\t\tif (!isolated)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * If we managed to isolate pages, it is always (1 << n) *",
            "\t\t * pageblock_nr_pages for some non-negative n.  (Max order",
            "\t\t * page may span two pageblocks).",
            "\t\t */",
            "\t}",
            "",
            "\tif (pfn < end_pfn) {",
            "\t\t/* Loop terminated early, cleanup. */",
            "\t\trelease_free_list(cc->freepages);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/* We don't use freelists for anything. */",
            "\treturn pfn;",
            "}",
            "static bool too_many_isolated(struct compact_control *cc)",
            "{",
            "\tpg_data_t *pgdat = cc->zone->zone_pgdat;",
            "\tbool too_many;",
            "",
            "\tunsigned long active, inactive, isolated;",
            "",
            "\tinactive = node_page_state(pgdat, NR_INACTIVE_FILE) +",
            "\t\t\tnode_page_state(pgdat, NR_INACTIVE_ANON);",
            "\tactive = node_page_state(pgdat, NR_ACTIVE_FILE) +",
            "\t\t\tnode_page_state(pgdat, NR_ACTIVE_ANON);",
            "\tisolated = node_page_state(pgdat, NR_ISOLATED_FILE) +",
            "\t\t\tnode_page_state(pgdat, NR_ISOLATED_ANON);",
            "",
            "\t/*",
            "\t * Allow GFP_NOFS to isolate past the limit set for regular",
            "\t * compaction runs. This prevents an ABBA deadlock when other",
            "\t * compactors have already isolated to the limit, but are",
            "\t * blocked on filesystem locks held by the GFP_NOFS thread.",
            "\t */",
            "\tif (cc->gfp_mask & __GFP_FS) {",
            "\t\tinactive >>= 3;",
            "\t\tactive >>= 3;",
            "\t}",
            "",
            "\ttoo_many = isolated > (inactive + active) / 2;",
            "\tif (!too_many)",
            "\t\twake_throttle_isolated(pgdat);",
            "",
            "\treturn too_many;",
            "}",
            "static bool skip_isolation_on_order(int order, int target_order)",
            "{",
            "\t/*",
            "\t * Unless we are performing global compaction (i.e.,",
            "\t * is_via_compact_memory), skip any folios that are larger than the",
            "\t * target order: we wouldn't be here if we'd have a free folio with",
            "\t * the desired target_order, so migrating this folio would likely fail",
            "\t * later.",
            "\t */",
            "\tif (!is_via_compact_memory(target_order) && order >= target_order)",
            "\t\treturn true;",
            "\t/*",
            "\t * We limit memory compaction to pageblocks and won't try",
            "\t * creating free blocks of memory that are larger than that.",
            "\t */",
            "\treturn order >= pageblock_order;",
            "}"
          ],
          "function_name": "isolate_freepages_range, too_many_isolated, skip_isolation_on_order",
          "description": "isolate_freepages_range扫描指定PFN范围内的页面块，隔离空闲页至freepages数组；too_many_isolated检测当前系统是否已隔离过多页面以避免死锁；skip_isolation_on_order判断是否跳过大于目标顺序的页面",
          "similarity": 0.4864872097969055
        }
      ]
    }
  ]
}