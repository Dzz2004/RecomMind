{
  "query": "Linux内核读写锁实现原理",
  "timestamp": "2025-12-26 01:02:09",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/spinlock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:53:30\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\spinlock.c`\n\n---\n\n# `locking/spinlock.c` 技术文档\n\n## 1. 文件概述\n\n`locking/spinlock.c` 是 Linux 内核中实现自旋锁（spinlock）和读写锁（rwlock）通用逻辑的核心源文件。该文件主要为对称多处理器（SMP）系统以及启用了锁调试（如 `DEBUG_SPINLOCK` 或 `DEBUG_LOCK_ALLOC`）的配置提供锁操作的通用实现。在单处理器（UP）且未启用调试的配置下，相关操作通常以内联函数形式直接展开，不依赖此文件。\n\n该文件通过宏生成机制构建了多种锁操作函数，包括普通加锁、中断屏蔽加锁、软中断屏蔽加锁等变体，并支持可抢占内核（`CONFIG_PREEMPT`）下的友好调度行为。此外，文件还处理了与内存映射 I/O 写屏障（`CONFIG_MMIOWB`）相关的每 CPU 状态管理。\n\n## 2. 核心功能\n\n### 数据结构\n- `struct mmiowb_state`（条件定义）：用于跟踪每 CPU 的内存映射 I/O 写屏障状态，仅在 `CONFIG_MMIOWB` 启用且架构未提供自有实现时定义。\n  - 全局符号：`DEFINE_PER_CPU(struct mmiowb_state, __mmiowb_state)`，并通过 `EXPORT_PER_CPU_SYMBOL` 导出。\n\n### 主要函数（通过宏生成或显式定义）\n- **自旋锁（spinlock）操作**：\n  - `_raw_spin_lock` / `__raw_spin_lock`\n  - `_raw_spin_lock_irq` / `__raw_spin_lock_irq`\n  - `_raw_spin_lock_irqsave` / `__raw_spin_lock_irqsave`\n  - `_raw_spin_lock_bh` / `__raw_spin_lock_bh`\n  - `_raw_spin_trylock` / `__raw_spin_trylock`\n  - 对应的解锁函数（如 `_raw_spin_unlock` 等）\n\n- **读写锁（rwlock）操作**（非 `PREEMPT_RT` 配置下）：\n  - `_raw_read_lock` / `__raw_read_lock` 等读操作\n  - `_raw_write_lock` / `__raw_write_lock` 等写操作\n  - `_raw_write_lock_nested`：支持锁类嵌套的写锁获取\n\n- **架构相关松弛函数（默认回退）**：\n  - `arch_read_relax`, `arch_write_relax`, `arch_spin_relax`：默认定义为 `cpu_relax()`，允许架构提供特定优化。\n\n## 3. 关键实现\n\n### 锁操作的通用构建机制\n- 使用 `BUILD_LOCK_OPS(op, locktype)` 宏统一生成加锁函数族（`_lock`, `_lock_irqsave`, `_lock_irq`, `_lock_bh`）。\n- 每个加锁函数采用 **“尝试-失败-松弛-重试”** 循环：\n  1. 禁用抢占（`preempt_disable()`）\n  2. 尝试原子获取锁（调用 `do_raw_##op##_trylock`）\n  3. 若成功则退出；否则恢复抢占（`preempt_enable()`）\n  4. 调用架构特定的 `arch_##op##_relax()`（默认为 `cpu_relax()`）以降低 CPU 占用\n- 在 `_irqsave` 和 `_bh` 变体中，正确处理中断和软中断的屏蔽与恢复。\n\n### 可抢占性与调试兼容性\n- 当启用 `CONFIG_DEBUG_LOCK_ALLOC` 或未定义 `CONFIG_GENERIC_LOCKBREAK` 时，**不使用**上述通用构建逻辑，而是依赖头文件（`spinlock_api_smp.h` / `rwlock_api_smp.h`）中的内联实现，以满足锁依赖验证器（lockdep）对中断状态的假设。\n- 在通用构建路径中，循环内显式启用/禁用抢占，使得长时间自旋时当前 CPU 可被抢占，提升系统响应性。\n\n### 函数导出与内联控制\n- 所有 `_raw_*` 函数均通过条件编译（如 `#ifndef CONFIG_INLINE_SPIN_LOCK`）决定是否以内联或 `noinline` 形式定义。\n- 非内联版本使用 `EXPORT_SYMBOL` 导出，供模块或其他编译单元调用。\n- 解锁函数同样受 `CONFIG_UNINLINE_SPIN_UNLOCK` 等配置项控制。\n\n### 嵌套写锁支持\n- `_raw_write_lock_nested` 函数在非调试模式下退化为普通写锁；在调试模式下保留子类参数以支持锁类验证。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/spinlock.h>`：核心锁类型和 API 声明\n  - `<linux/preempt.h>`：抢占控制原语（`preempt_disable/enable`）\n  - `<linux/interrupt.h>`：中断控制（`local_irq_save/restore`, `local_bh_disable`）\n  - `<linux/debug_locks.h>`：调试锁相关宏\n  - `<linux/export.h>`：符号导出宏\n  - `<linux/linkage.h>`：链接属性定义\n\n- **架构依赖**：\n  - 依赖架构层提供底层原子操作（如 `do_raw_spin_trylock` 的实际实现通常在 `arch/*/include/asm/spinlock.h` 中）\n  - 架构可覆盖 `arch_*_relax` 宏以优化自旋行为\n  - 某些架构的性能分析工具（如 `profile_pc`）依赖此文件中函数的栈帧结构稳定性\n\n- **配置依赖**：\n  - `CONFIG_SMP`：SMP 支持是此文件生效的前提\n  - `CONFIG_PREEMPT` / `CONFIG_PREEMPT_RT`：影响锁实现路径选择\n  - `CONFIG_DEBUG_LOCK_ALLOC`：决定是否使用通用构建逻辑\n  - `CONFIG_MMIOWB`：控制每 CPU `mmiowb_state` 的定义\n\n## 5. 使用场景\n\n- **内核同步原语实现**：作为自旋锁和读写锁的通用后端，被内核各子系统（如内存管理、文件系统、设备驱动、网络栈等）广泛用于保护临界区。\n- **中断上下文同步**：通过 `_irq` / `_irqsave` 变体，在中断处理程序与进程上下文之间提供同步。\n- **软中断同步**：通过 `_bh` 变体，防止软中断与进程上下文同时访问共享数据。\n- **实时内核适配**：在 `PREEMPT_RT` 补丁集下，读写锁实现被替换，但自旋锁仍由此文件提供（部分路径被绕过）。\n- **锁调试与验证**：配合 `lockdep` 子系统，在开发和调试阶段检测死锁、锁顺序违规等问题。\n- **性能关键路径**：通过可配置的内联/非内联策略，在代码大小与性能之间取得平衡。",
      "similarity": 0.6656622290611267,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/spinlock.c",
          "start_line": 136,
          "end_line": 239,
          "content": [
            "noinline int __lockfunc _raw_spin_trylock(raw_spinlock_t *lock)",
            "{",
            "\treturn __raw_spin_trylock(lock);",
            "}",
            "noinline int __lockfunc _raw_spin_trylock_bh(raw_spinlock_t *lock)",
            "{",
            "\treturn __raw_spin_trylock_bh(lock);",
            "}",
            "noinline void __lockfunc _raw_spin_lock(raw_spinlock_t *lock)",
            "{",
            "\t__raw_spin_lock(lock);",
            "}",
            "noinline unsigned long __lockfunc _raw_spin_lock_irqsave(raw_spinlock_t *lock)",
            "{",
            "\treturn __raw_spin_lock_irqsave(lock);",
            "}",
            "noinline void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)",
            "{",
            "\t__raw_spin_lock_irq(lock);",
            "}",
            "noinline void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)",
            "{",
            "\t__raw_spin_lock_bh(lock);",
            "}",
            "noinline void __lockfunc _raw_spin_unlock(raw_spinlock_t *lock)",
            "{",
            "\t__raw_spin_unlock(lock);",
            "}",
            "noinline void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)",
            "{",
            "\t__raw_spin_unlock_irqrestore(lock, flags);",
            "}",
            "noinline void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)",
            "{",
            "\t__raw_spin_unlock_irq(lock);",
            "}",
            "noinline void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)",
            "{",
            "\t__raw_spin_unlock_bh(lock);",
            "}",
            "noinline int __lockfunc _raw_read_trylock(rwlock_t *lock)",
            "{",
            "\treturn __raw_read_trylock(lock);",
            "}",
            "noinline void __lockfunc _raw_read_lock(rwlock_t *lock)",
            "{",
            "\t__raw_read_lock(lock);",
            "}",
            "noinline unsigned long __lockfunc _raw_read_lock_irqsave(rwlock_t *lock)",
            "{",
            "\treturn __raw_read_lock_irqsave(lock);",
            "}",
            "noinline void __lockfunc _raw_read_lock_irq(rwlock_t *lock)",
            "{",
            "\t__raw_read_lock_irq(lock);",
            "}",
            "noinline void __lockfunc _raw_read_lock_bh(rwlock_t *lock)",
            "{",
            "\t__raw_read_lock_bh(lock);",
            "}",
            "noinline void __lockfunc _raw_read_unlock(rwlock_t *lock)",
            "{",
            "\t__raw_read_unlock(lock);",
            "}",
            "noinline void __lockfunc _raw_read_unlock_irqrestore(rwlock_t *lock, unsigned long flags)",
            "{",
            "\t__raw_read_unlock_irqrestore(lock, flags);",
            "}",
            "noinline void __lockfunc _raw_read_unlock_irq(rwlock_t *lock)",
            "{",
            "\t__raw_read_unlock_irq(lock);",
            "}",
            "noinline void __lockfunc _raw_read_unlock_bh(rwlock_t *lock)",
            "{",
            "\t__raw_read_unlock_bh(lock);",
            "}",
            "noinline int __lockfunc _raw_write_trylock(rwlock_t *lock)",
            "{",
            "\treturn __raw_write_trylock(lock);",
            "}",
            "noinline void __lockfunc _raw_write_lock(rwlock_t *lock)",
            "{",
            "\t__raw_write_lock(lock);",
            "}",
            "void __lockfunc _raw_write_lock_nested(rwlock_t *lock, int subclass)",
            "{",
            "\t__raw_write_lock_nested(lock, subclass);",
            "}",
            "noinline unsigned long __lockfunc _raw_write_lock_irqsave(rwlock_t *lock)",
            "{",
            "\treturn __raw_write_lock_irqsave(lock);",
            "}",
            "noinline void __lockfunc _raw_write_lock_irq(rwlock_t *lock)",
            "{",
            "\t__raw_write_lock_irq(lock);",
            "}",
            "noinline void __lockfunc _raw_write_lock_bh(rwlock_t *lock)",
            "{",
            "\t__raw_write_lock_bh(lock);",
            "}",
            "noinline void __lockfunc _raw_write_unlock(rwlock_t *lock)",
            "{",
            "\t__raw_write_unlock(lock);",
            "}"
          ],
          "function_name": "_raw_spin_trylock, _raw_spin_trylock_bh, _raw_spin_lock, _raw_spin_lock_irqsave, _raw_spin_lock_irq, _raw_spin_lock_bh, _raw_spin_unlock, _raw_spin_unlock_irqrestore, _raw_spin_unlock_irq, _raw_spin_unlock_bh, _raw_read_trylock, _raw_read_lock, _raw_read_lock_irqsave, _raw_read_lock_irq, _raw_read_lock_bh, _raw_read_unlock, _raw_read_unlock_irqrestore, _raw_read_unlock_irq, _raw_read_unlock_bh, _raw_write_trylock, _raw_write_lock, _raw_write_lock_nested, _raw_write_lock_irqsave, _raw_write_lock_irq, _raw_write_lock_bh, _raw_write_unlock",
          "description": "实现底层自旋锁和读写锁的具体操作函数，包括尝试加锁、加锁、解锁及其中断/软中断变种，所有函数均调用相应的__raw_*系列内核函数执行实际锁状态变更。",
          "similarity": 0.6337029933929443
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/spinlock.c",
          "start_line": 1,
          "end_line": 135,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (2004) Linus Torvalds",
            " *",
            " * Author: Zwane Mwaikambo <zwane@fsmlabs.com>",
            " *",
            " * Copyright (2004, 2005) Ingo Molnar",
            " *",
            " * This file contains the spinlock/rwlock implementations for the",
            " * SMP and the DEBUG_SPINLOCK cases. (UP-nondebug inlines them)",
            " *",
            " * Note that some architectures have special knowledge about the",
            " * stack frames of these functions in their profile_pc. If you",
            " * change anything significant here that could change the stack",
            " * frame contact the architecture maintainers.",
            " */",
            "",
            "#include <linux/linkage.h>",
            "#include <linux/preempt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/export.h>",
            "",
            "#ifdef CONFIG_MMIOWB",
            "#ifndef arch_mmiowb_state",
            "DEFINE_PER_CPU(struct mmiowb_state, __mmiowb_state);",
            "EXPORT_PER_CPU_SYMBOL(__mmiowb_state);",
            "#endif",
            "#endif",
            "",
            "/*",
            " * If lockdep is enabled then we use the non-preemption spin-ops",
            " * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are",
            " * not re-enabled during lock-acquire (which the preempt-spin-ops do):",
            " */",
            "#if !defined(CONFIG_GENERIC_LOCKBREAK) || defined(CONFIG_DEBUG_LOCK_ALLOC)",
            "/*",
            " * The __lock_function inlines are taken from",
            " * spinlock : include/linux/spinlock_api_smp.h",
            " * rwlock   : include/linux/rwlock_api_smp.h",
            " */",
            "#else",
            "",
            "/*",
            " * Some architectures can relax in favour of the CPU owning the lock.",
            " */",
            "#ifndef arch_read_relax",
            "# define arch_read_relax(l)\tcpu_relax()",
            "#endif",
            "#ifndef arch_write_relax",
            "# define arch_write_relax(l)\tcpu_relax()",
            "#endif",
            "#ifndef arch_spin_relax",
            "# define arch_spin_relax(l)\tcpu_relax()",
            "#endif",
            "",
            "/*",
            " * We build the __lock_function inlines here. They are too large for",
            " * inlining all over the place, but here is only one user per function",
            " * which embeds them into the calling _lock_function below.",
            " *",
            " * This could be a long-held lock. We both prepare to spin for a long",
            " * time (making _this_ CPU preemptible if possible), and we also signal",
            " * towards that other CPU that it should break the lock ASAP.",
            " */",
            "#define BUILD_LOCK_OPS(op, locktype)\t\t\t\t\t\\",
            "void __lockfunc __raw_##op##_lock(locktype##_t *lock)\t\t\t\\",
            "{\t\t\t\t\t\t\t\t\t\\",
            "\tfor (;;) {\t\t\t\t\t\t\t\\",
            "\t\tpreempt_disable();\t\t\t\t\t\\",
            "\t\tif (likely(do_raw_##op##_trylock(lock)))\t\t\\",
            "\t\t\tbreak;\t\t\t\t\t\t\\",
            "\t\tpreempt_enable();\t\t\t\t\t\\",
            "\t\t\t\t\t\t\t\t\t\\",
            "\t\tarch_##op##_relax(&lock->raw_lock);\t\t\t\\",
            "\t}\t\t\t\t\t\t\t\t\\",
            "}\t\t\t\t\t\t\t\t\t\\",
            "\t\t\t\t\t\t\t\t\t\\",
            "unsigned long __lockfunc __raw_##op##_lock_irqsave(locktype##_t *lock)\t\\",
            "{\t\t\t\t\t\t\t\t\t\\",
            "\tunsigned long flags;\t\t\t\t\t\t\\",
            "\t\t\t\t\t\t\t\t\t\\",
            "\tfor (;;) {\t\t\t\t\t\t\t\\",
            "\t\tpreempt_disable();\t\t\t\t\t\\",
            "\t\tlocal_irq_save(flags);\t\t\t\t\t\\",
            "\t\tif (likely(do_raw_##op##_trylock(lock)))\t\t\\",
            "\t\t\tbreak;\t\t\t\t\t\t\\",
            "\t\tlocal_irq_restore(flags);\t\t\t\t\\",
            "\t\tpreempt_enable();\t\t\t\t\t\\",
            "\t\t\t\t\t\t\t\t\t\\",
            "\t\tarch_##op##_relax(&lock->raw_lock);\t\t\t\\",
            "\t}\t\t\t\t\t\t\t\t\\",
            "\t\t\t\t\t\t\t\t\t\\",
            "\treturn flags;\t\t\t\t\t\t\t\\",
            "}\t\t\t\t\t\t\t\t\t\\",
            "\t\t\t\t\t\t\t\t\t\\",
            "void __lockfunc __raw_##op##_lock_irq(locktype##_t *lock)\t\t\\",
            "{\t\t\t\t\t\t\t\t\t\\",
            "\t_raw_##op##_lock_irqsave(lock);\t\t\t\t\t\\",
            "}\t\t\t\t\t\t\t\t\t\\",
            "\t\t\t\t\t\t\t\t\t\\",
            "void __lockfunc __raw_##op##_lock_bh(locktype##_t *lock)\t\t\\",
            "{\t\t\t\t\t\t\t\t\t\\",
            "\tunsigned long flags;\t\t\t\t\t\t\\",
            "\t\t\t\t\t\t\t\t\t\\",
            "\t/*\t\t\t\t\t\t\t*/\t\\",
            "\t/* Careful: we must exclude softirqs too, hence the\t*/\t\\",
            "\t/* irq-disabling. We use the generic preemption-aware\t*/\t\\",
            "\t/* function:\t\t\t\t\t\t*/\t\\",
            "\t/**/\t\t\t\t\t\t\t\t\\",
            "\tflags = _raw_##op##_lock_irqsave(lock);\t\t\t\t\\",
            "\tlocal_bh_disable();\t\t\t\t\t\t\\",
            "\tlocal_irq_restore(flags);\t\t\t\t\t\\",
            "}\t\t\t\t\t\t\t\t\t\\",
            "",
            "/*",
            " * Build preemption-friendly versions of the following",
            " * lock-spinning functions:",
            " *",
            " *         __[spin|read|write]_lock()",
            " *         __[spin|read|write]_lock_irq()",
            " *         __[spin|read|write]_lock_irqsave()",
            " *         __[spin|read|write]_lock_bh()",
            " */",
            "BUILD_LOCK_OPS(spin, raw_spinlock);",
            "",
            "#ifndef CONFIG_PREEMPT_RT",
            "BUILD_LOCK_OPS(read, rwlock);",
            "BUILD_LOCK_OPS(write, rwlock);",
            "#endif",
            "",
            "#endif",
            "",
            "#ifndef CONFIG_INLINE_SPIN_TRYLOCK"
          ],
          "function_name": null,
          "description": "定义并生成针对SMP和DEBUG_SPINLOCK的自旋锁及读写锁操作函数，通过宏展开创建多种锁操作接口，支持中断和软中断处理，包含架构特定的放松函数（如cpu_relax）和MMIOWB状态定义。",
          "similarity": 0.5650541186332703
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/spinlock.c",
          "start_line": 348,
          "end_line": 395,
          "content": [
            "noinline void __lockfunc _raw_write_unlock_irqrestore(rwlock_t *lock, unsigned long flags)",
            "{",
            "\t__raw_write_unlock_irqrestore(lock, flags);",
            "}",
            "noinline void __lockfunc _raw_write_unlock_irq(rwlock_t *lock)",
            "{",
            "\t__raw_write_unlock_irq(lock);",
            "}",
            "noinline void __lockfunc _raw_write_unlock_bh(rwlock_t *lock)",
            "{",
            "\t__raw_write_unlock_bh(lock);",
            "}",
            "void __lockfunc _raw_spin_lock_nested(raw_spinlock_t *lock, int subclass)",
            "{",
            "\tpreempt_disable();",
            "\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);",
            "\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);",
            "}",
            "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,",
            "\t\t\t\t\t\t   int subclass)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\tpreempt_disable();",
            "\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);",
            "\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);",
            "\treturn flags;",
            "}",
            "void __lockfunc _raw_spin_lock_nest_lock(raw_spinlock_t *lock,",
            "\t\t\t\t     struct lockdep_map *nest_lock)",
            "{",
            "\tpreempt_disable();",
            "\tspin_acquire_nest(&lock->dep_map, 0, 0, nest_lock, _RET_IP_);",
            "\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);",
            "}",
            "notrace int in_lock_functions(unsigned long addr)",
            "{",
            "\t/* Linker adds these: start and end of __lockfunc functions */",
            "\textern char __lock_text_start[], __lock_text_end[];",
            "",
            "\treturn addr >= (unsigned long)__lock_text_start",
            "\t&& addr < (unsigned long)__lock_text_end;",
            "}",
            "void notrace lockdep_assert_in_softirq_func(void)",
            "{",
            "\tlockdep_assert_in_softirq();",
            "}"
          ],
          "function_name": "_raw_write_unlock_irqrestore, _raw_write_unlock_irq, _raw_write_unlock_bh, _raw_spin_lock_nested, _raw_spin_lock_irqsave_nested, _raw_spin_lock_nest_lock, in_lock_functions, lockdep_assert_in_softirq_func",
          "description": "提供嵌套锁操作和锁跟踪辅助函数，包含带子类别的锁获取、锁范围检测函数in_lock_functions，以及确保在软中断上下文的安全性检查函数lockdep_assert_in_softirq_func。",
          "similarity": 0.5220186710357666
        }
      ]
    },
    {
      "source_file": "kernel/locking/rwsem.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:51:36\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\rwsem.c`\n\n---\n\n# `locking/rwsem.c` 技术文档\n\n## 1. 文件概述\n\n`locking/rwsem.c` 是 Linux 内核中读写信号量（Read-Write Semaphore, rwsem）的核心实现文件，提供了对共享资源进行并发访问控制的机制。该机制允许多个读者并发访问资源，但写者必须独占访问。文件实现了 rwsem 的底层原子操作、锁获取/释放逻辑、乐观自旋（optimistic spinning）、写者锁抢占（lock-stealing）以及调试支持等功能，适用于高并发场景下的同步需求。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct rw_semaphore`：读写信号量的核心结构体，包含：\n  - `count`：原子长整型，编码了写者锁状态、等待者标志、移交标志、读者计数等信息。\n  - `owner`：记录当前锁持有者（写者任务指针或带标志的读者信息）。\n  - `wait_list`：等待队列，用于管理阻塞的读者和写者。\n  - `wait_lock`：保护等待队列的自旋锁。\n\n### 关键宏定义\n- **Owner 字段标志位**：\n  - `RWSEM_READER_OWNED`（bit 0）：表示当前由读者持有。\n  - `RWSEM_NONSPINNABLE`（bit 1）：禁止乐观自旋。\n- **Count 字段位布局**（64 位架构）：\n  - bit 0：`RWSEM_WRITER_LOCKED`（写者已加锁）\n  - bit 1：`RWSEM_FLAG_WAITERS`（存在等待者）\n  - bit 2：`RWSEM_FLAG_HANDOFF`（锁移交标志）\n  - bits 8–62：55 位读者计数\n  - bit 63：`RWSEM_FLAG_READFAIL`（读取失败标志，用于未来扩展）\n\n### 核心内联函数\n- `rwsem_set_owner()` / `rwsem_clear_owner()`：设置/清除写者所有者。\n- `rwsem_set_reader_owned()` / `rwsem_clear_reader_owned()`：标记/清除读者所有者（带调试支持）。\n- `is_rwsem_reader_owned()`：判断是否由读者持有。\n- `rwsem_set_nonspinnable()`：在读者持有时设置不可自旋标志。\n- `rwsem_test_oflags()`：测试 owner 字段中的标志位。\n\n## 3. 关键实现\n\n### 位域编码设计\n`count` 字段采用紧凑的位域编码，将写者锁状态、等待者存在标志、锁移交标志和读者计数集成在一个 `atomic_long_t` 中。这种设计使得 fast-path（快速路径）操作（如读者加锁）可通过单一原子加法完成，极大提升性能。\n\n### 乐观自旋（Optimistic Spinning）\n当写者尝试获取锁失败时，若满足条件（如锁由写者刚释放、无移交请求），会进入乐观自旋状态，避免立即进入睡眠。若自旋超时且锁仍为读者持有，则设置 `RWSEM_NONSPINNABLE` 标志，禁止后续写者自旋，防止 CPU 资源浪费。\n\n### 写者锁抢占（Writer Lock-Stealing）\n在特定条件下（如锁刚由写者释放、无等待者、无移交标志），新来的写者可直接抢占锁，无需排队，减少延迟。\n\n### 所有者追踪机制\n- **写者**：`owner` 字段直接存储 `task_struct*` 指针。\n- **读者**：`owner` 字段存储当前读者任务指针并置 `RWSEM_READER_OWNED` 位。由于性能考虑，仅记录**最后一个获取锁的读者**，而非所有读者。\n- 调试模式（`CONFIG_DEBUG_RWSEMS`）下，`rwsem_clear_reader_owned()` 确保只有真正的持有者才能清除其所有者记录。\n\n### 原子操作策略\n- **读者加锁**：使用 `atomic_long_fetch_add()` 原子增加读者计数。\n- **写者加锁**：使用 `atomic_long_cmpxchg()` 进行条件交换，确保互斥。\n\n### 锁移交（Handoff）机制\n当写者被唤醒时，若其位于等待队列头部，可设置 `RWSEM_FLAG_HANDOFF` 标志，确保该写者优先获得锁，避免“写者饥饿”。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/rwsem.h>`：定义 `rw_semaphore` 结构和公共 API。\n  - `<linux/atomic.h>`：提供原子操作原语。\n  - `<linux/sched/*.h>`：任务调度、唤醒队列、实时调度支持。\n  - `<trace/events/lock.h>`：锁事件追踪。\n- **配置依赖**：\n  - `CONFIG_PREEMPT_RT`：若启用，部分实现（如乐观自旋）被禁用。\n  - `CONFIG_DEBUG_RWSEMS`：启用所有者一致性检查和警告。\n- **内部依赖**：\n  - `lock_events.h`：统计锁事件（仅在非 `PREEMPT_RT` 下）。\n  - 内核调度器：用于任务阻塞/唤醒。\n\n## 5. 使用场景\n\n- **文件系统**：如 ext4、XFS 使用 rwsem 保护 inode 或目录结构，允许多读者并发访问元数据。\n- **内存管理**：`mm_struct` 的 `mmap_lock` 采用 rwsem，支持并发读（如页表遍历）与独占写（如内存映射修改）。\n- **模块加载**：内核模块的引用计数和符号表访问通过 rwsem 同步。\n- **RCU 替代场景**：在需要严格写者优先或不能使用 RCU 的上下文中，rwsem 提供强一致性保证。\n- **调试与死锁检测**：结合 `lockdep` 和 `DEBUG_RWSEMS`，用于检测读者/写者死锁或非法嵌套。",
      "similarity": 0.6529009938240051,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/rwsem.c",
          "start_line": 369,
          "end_line": 544,
          "content": [
            "static inline void",
            "rwsem_add_waiter(struct rw_semaphore *sem, struct rwsem_waiter *waiter)",
            "{",
            "\tlockdep_assert_held(&sem->wait_lock);",
            "\tlist_add_tail(&waiter->list, &sem->wait_list);",
            "\t/* caller will set RWSEM_FLAG_WAITERS */",
            "}",
            "static inline bool",
            "rwsem_del_waiter(struct rw_semaphore *sem, struct rwsem_waiter *waiter)",
            "{",
            "\tlockdep_assert_held(&sem->wait_lock);",
            "\tlist_del(&waiter->list);",
            "\tif (likely(!list_empty(&sem->wait_list)))",
            "\t\treturn true;",
            "",
            "\tatomic_long_andnot(RWSEM_FLAG_HANDOFF | RWSEM_FLAG_WAITERS, &sem->count);",
            "\treturn false;",
            "}",
            "static void rwsem_mark_wake(struct rw_semaphore *sem,",
            "\t\t\t    enum rwsem_wake_type wake_type,",
            "\t\t\t    struct wake_q_head *wake_q)",
            "{",
            "\tstruct rwsem_waiter *waiter, *tmp;",
            "\tlong oldcount, woken = 0, adjustment = 0;",
            "\tstruct list_head wlist;",
            "",
            "\tlockdep_assert_held(&sem->wait_lock);",
            "",
            "\t/*",
            "\t * Take a peek at the queue head waiter such that we can determine",
            "\t * the wakeup(s) to perform.",
            "\t */",
            "\twaiter = rwsem_first_waiter(sem);",
            "",
            "\tif (waiter->type == RWSEM_WAITING_FOR_WRITE) {",
            "\t\tif (wake_type == RWSEM_WAKE_ANY) {",
            "\t\t\t/*",
            "\t\t\t * Mark writer at the front of the queue for wakeup.",
            "\t\t\t * Until the task is actually later awoken later by",
            "\t\t\t * the caller, other writers are able to steal it.",
            "\t\t\t * Readers, on the other hand, will block as they",
            "\t\t\t * will notice the queued writer.",
            "\t\t\t */",
            "\t\t\twake_q_add(wake_q, waiter->task);",
            "\t\t\tlockevent_inc(rwsem_wake_writer);",
            "\t\t}",
            "",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * No reader wakeup if there are too many of them already.",
            "\t */",
            "\tif (unlikely(atomic_long_read(&sem->count) < 0))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Writers might steal the lock before we grant it to the next reader.",
            "\t * We prefer to do the first reader grant before counting readers",
            "\t * so we can bail out early if a writer stole the lock.",
            "\t */",
            "\tif (wake_type != RWSEM_WAKE_READ_OWNED) {",
            "\t\tstruct task_struct *owner;",
            "",
            "\t\tadjustment = RWSEM_READER_BIAS;",
            "\t\toldcount = atomic_long_fetch_add(adjustment, &sem->count);",
            "\t\tif (unlikely(oldcount & RWSEM_WRITER_MASK)) {",
            "\t\t\t/*",
            "\t\t\t * When we've been waiting \"too\" long (for writers",
            "\t\t\t * to give up the lock), request a HANDOFF to",
            "\t\t\t * force the issue.",
            "\t\t\t */",
            "\t\t\tif (time_after(jiffies, waiter->timeout)) {",
            "\t\t\t\tif (!(oldcount & RWSEM_FLAG_HANDOFF)) {",
            "\t\t\t\t\tadjustment -= RWSEM_FLAG_HANDOFF;",
            "\t\t\t\t\tlockevent_inc(rwsem_rlock_handoff);",
            "\t\t\t\t}",
            "\t\t\t\twaiter->handoff_set = true;",
            "\t\t\t}",
            "",
            "\t\t\tatomic_long_add(-adjustment, &sem->count);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\t/*",
            "\t\t * Set it to reader-owned to give spinners an early",
            "\t\t * indication that readers now have the lock.",
            "\t\t * The reader nonspinnable bit seen at slowpath entry of",
            "\t\t * the reader is copied over.",
            "\t\t */",
            "\t\towner = waiter->task;",
            "\t\t__rwsem_set_reader_owned(sem, owner);",
            "\t}",
            "",
            "\t/*",
            "\t * Grant up to MAX_READERS_WAKEUP read locks to all the readers in the",
            "\t * queue. We know that the woken will be at least 1 as we accounted",
            "\t * for above. Note we increment the 'active part' of the count by the",
            "\t * number of readers before waking any processes up.",
            "\t *",
            "\t * This is an adaptation of the phase-fair R/W locks where at the",
            "\t * reader phase (first waiter is a reader), all readers are eligible",
            "\t * to acquire the lock at the same time irrespective of their order",
            "\t * in the queue. The writers acquire the lock according to their",
            "\t * order in the queue.",
            "\t *",
            "\t * We have to do wakeup in 2 passes to prevent the possibility that",
            "\t * the reader count may be decremented before it is incremented. It",
            "\t * is because the to-be-woken waiter may not have slept yet. So it",
            "\t * may see waiter->task got cleared, finish its critical section and",
            "\t * do an unlock before the reader count increment.",
            "\t *",
            "\t * 1) Collect the read-waiters in a separate list, count them and",
            "\t *    fully increment the reader count in rwsem.",
            "\t * 2) For each waiters in the new list, clear waiter->task and",
            "\t *    put them into wake_q to be woken up later.",
            "\t */",
            "\tINIT_LIST_HEAD(&wlist);",
            "\tlist_for_each_entry_safe(waiter, tmp, &sem->wait_list, list) {",
            "\t\tif (waiter->type == RWSEM_WAITING_FOR_WRITE)",
            "\t\t\tcontinue;",
            "",
            "\t\twoken++;",
            "\t\tlist_move_tail(&waiter->list, &wlist);",
            "",
            "\t\t/*",
            "\t\t * Limit # of readers that can be woken up per wakeup call.",
            "\t\t */",
            "\t\tif (unlikely(woken >= MAX_READERS_WAKEUP))",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tadjustment = woken * RWSEM_READER_BIAS - adjustment;",
            "\tlockevent_cond_inc(rwsem_wake_reader, woken);",
            "",
            "\toldcount = atomic_long_read(&sem->count);",
            "\tif (list_empty(&sem->wait_list)) {",
            "\t\t/*",
            "\t\t * Combined with list_move_tail() above, this implies",
            "\t\t * rwsem_del_waiter().",
            "\t\t */",
            "\t\tadjustment -= RWSEM_FLAG_WAITERS;",
            "\t\tif (oldcount & RWSEM_FLAG_HANDOFF)",
            "\t\t\tadjustment -= RWSEM_FLAG_HANDOFF;",
            "\t} else if (woken) {",
            "\t\t/*",
            "\t\t * When we've woken a reader, we no longer need to force",
            "\t\t * writers to give up the lock and we can clear HANDOFF.",
            "\t\t */",
            "\t\tif (oldcount & RWSEM_FLAG_HANDOFF)",
            "\t\t\tadjustment -= RWSEM_FLAG_HANDOFF;",
            "\t}",
            "",
            "\tif (adjustment)",
            "\t\tatomic_long_add(adjustment, &sem->count);",
            "",
            "\t/* 2nd pass */",
            "\tlist_for_each_entry_safe(waiter, tmp, &wlist, list) {",
            "\t\tstruct task_struct *tsk;",
            "",
            "\t\ttsk = waiter->task;",
            "\t\tget_task_struct(tsk);",
            "",
            "\t\t/*",
            "\t\t * Ensure calling get_task_struct() before setting the reader",
            "\t\t * waiter to nil such that rwsem_down_read_slowpath() cannot",
            "\t\t * race with do_exit() by always holding a reference count",
            "\t\t * to the task to wakeup.",
            "\t\t */",
            "\t\tsmp_store_release(&waiter->task, NULL);",
            "\t\t/*",
            "\t\t * Ensure issuing the wakeup (either by us or someone else)",
            "\t\t * after setting the reader waiter to nil.",
            "\t\t */",
            "\t\twake_q_add_safe(wake_q, tsk);",
            "\t}",
            "}"
          ],
          "function_name": "rwsem_add_waiter, rwsem_del_waiter, rwsem_mark_wake",
          "description": "实现等待队列的维护逻辑，支持唤醒等待线程并处理读写锁的优先级唤醒策略",
          "similarity": 0.597815990447998
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/locking/rwsem.c",
          "start_line": 576,
          "end_line": 698,
          "content": [
            "static inline void",
            "rwsem_del_wake_waiter(struct rw_semaphore *sem, struct rwsem_waiter *waiter,",
            "\t\t      struct wake_q_head *wake_q)",
            "\t\t      __releases(&sem->wait_lock)",
            "{",
            "\tbool first = rwsem_first_waiter(sem) == waiter;",
            "",
            "\twake_q_init(wake_q);",
            "",
            "\t/*",
            "\t * If the wait_list isn't empty and the waiter to be deleted is",
            "\t * the first waiter, we wake up the remaining waiters as they may",
            "\t * be eligible to acquire or spin on the lock.",
            "\t */",
            "\tif (rwsem_del_waiter(sem, waiter) && first)",
            "\t\trwsem_mark_wake(sem, RWSEM_WAKE_ANY, wake_q);",
            "\traw_spin_unlock_irq(&sem->wait_lock);",
            "\tif (!wake_q_empty(wake_q))",
            "\t\twake_up_q(wake_q);",
            "}",
            "static inline bool rwsem_try_write_lock(struct rw_semaphore *sem,",
            "\t\t\t\t\tstruct rwsem_waiter *waiter)",
            "{",
            "\tstruct rwsem_waiter *first = rwsem_first_waiter(sem);",
            "\tlong count, new;",
            "",
            "\tlockdep_assert_held(&sem->wait_lock);",
            "",
            "\tcount = atomic_long_read(&sem->count);",
            "\tdo {",
            "\t\tbool has_handoff = !!(count & RWSEM_FLAG_HANDOFF);",
            "",
            "\t\tif (has_handoff) {",
            "\t\t\t/*",
            "\t\t\t * Honor handoff bit and yield only when the first",
            "\t\t\t * waiter is the one that set it. Otherwisee, we",
            "\t\t\t * still try to acquire the rwsem.",
            "\t\t\t */",
            "\t\t\tif (first->handoff_set && (waiter != first))",
            "\t\t\t\treturn false;",
            "\t\t}",
            "",
            "\t\tnew = count;",
            "",
            "\t\tif (count & RWSEM_LOCK_MASK) {",
            "\t\t\t/*",
            "\t\t\t * A waiter (first or not) can set the handoff bit",
            "\t\t\t * if it is an RT task or wait in the wait queue",
            "\t\t\t * for too long.",
            "\t\t\t */",
            "\t\t\tif (has_handoff || (!rt_or_dl_task(waiter->task) &&",
            "\t\t\t\t\t    !time_after(jiffies, waiter->timeout)))",
            "\t\t\t\treturn false;",
            "",
            "\t\t\tnew |= RWSEM_FLAG_HANDOFF;",
            "\t\t} else {",
            "\t\t\tnew |= RWSEM_WRITER_LOCKED;",
            "\t\t\tnew &= ~RWSEM_FLAG_HANDOFF;",
            "",
            "\t\t\tif (list_is_singular(&sem->wait_list))",
            "\t\t\t\tnew &= ~RWSEM_FLAG_WAITERS;",
            "\t\t}",
            "\t} while (!atomic_long_try_cmpxchg_acquire(&sem->count, &count, new));",
            "",
            "\t/*",
            "\t * We have either acquired the lock with handoff bit cleared or set",
            "\t * the handoff bit. Only the first waiter can have its handoff_set",
            "\t * set here to enable optimistic spinning in slowpath loop.",
            "\t */",
            "\tif (new & RWSEM_FLAG_HANDOFF) {",
            "\t\tfirst->handoff_set = true;",
            "\t\tlockevent_inc(rwsem_wlock_handoff);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/*",
            "\t * Have rwsem_try_write_lock() fully imply rwsem_del_waiter() on",
            "\t * success.",
            "\t */",
            "\tlist_del(&waiter->list);",
            "\trwsem_set_owner(sem);",
            "\treturn true;",
            "}",
            "static inline bool rwsem_try_write_lock_unqueued(struct rw_semaphore *sem)",
            "{",
            "\tlong count = atomic_long_read(&sem->count);",
            "",
            "\twhile (!(count & (RWSEM_LOCK_MASK|RWSEM_FLAG_HANDOFF))) {",
            "\t\tif (atomic_long_try_cmpxchg_acquire(&sem->count, &count,",
            "\t\t\t\t\tcount | RWSEM_WRITER_LOCKED)) {",
            "\t\t\trwsem_set_owner(sem);",
            "\t\t\tlockevent_inc(rwsem_opt_lock);",
            "\t\t\treturn true;",
            "\t\t}",
            "\t}",
            "\treturn false;",
            "}",
            "static inline bool rwsem_can_spin_on_owner(struct rw_semaphore *sem)",
            "{",
            "\tstruct task_struct *owner;",
            "\tunsigned long flags;",
            "\tbool ret = true;",
            "",
            "\tif (need_resched()) {",
            "\t\tlockevent_inc(rwsem_opt_fail);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/*",
            "\t * Disable preemption is equal to the RCU read-side crital section,",
            "\t * thus the task_strcut structure won't go away.",
            "\t */",
            "\towner = rwsem_owner_flags(sem, &flags);",
            "\t/*",
            "\t * Don't check the read-owner as the entry may be stale.",
            "\t */",
            "\tif ((flags & RWSEM_NONSPINNABLE) ||",
            "\t    (owner && !(flags & RWSEM_READER_OWNED) && !owner_on_cpu(owner)))",
            "\t\tret = false;",
            "",
            "\tlockevent_cond_inc(rwsem_opt_fail, !ret);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "rwsem_del_wake_waiter, rwsem_try_write_lock, rwsem_try_write_lock_unqueued, rwsem_can_spin_on_owner",
          "description": "处理写锁的尝试获取逻辑，包含手柄标志判定、等待队列操作及自旋兼容性检查",
          "similarity": 0.5896966457366943
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/locking/rwsem.c",
          "start_line": 1483,
          "end_line": 1585,
          "content": [
            "static inline int __sched __down_write_killable(struct rw_semaphore *sem)",
            "{",
            "\treturn rwbase_write_lock(&sem->rwbase, TASK_KILLABLE);",
            "}",
            "static inline int __down_write_trylock(struct rw_semaphore *sem)",
            "{",
            "\treturn rwbase_write_trylock(&sem->rwbase);",
            "}",
            "static inline void __up_write(struct rw_semaphore *sem)",
            "{",
            "\trwbase_write_unlock(&sem->rwbase);",
            "}",
            "static inline void __downgrade_write(struct rw_semaphore *sem)",
            "{",
            "\trwbase_write_downgrade(&sem->rwbase);",
            "}",
            "static inline void __rwsem_set_reader_owned(struct rw_semaphore *sem,",
            "\t\t\t\t\t    struct task_struct *owner)",
            "{",
            "}",
            "static inline bool is_rwsem_reader_owned(struct rw_semaphore *sem)",
            "{",
            "\tint count = atomic_read(&sem->rwbase.readers);",
            "",
            "\treturn count < 0 && count != READER_BIAS;",
            "}",
            "void __sched down_read(struct rw_semaphore *sem)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire_read(&sem->dep_map, 0, 0, _RET_IP_);",
            "",
            "\tLOCK_CONTENDED(sem, __down_read_trylock, __down_read);",
            "}",
            "int __sched down_read_interruptible(struct rw_semaphore *sem)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire_read(&sem->dep_map, 0, 0, _RET_IP_);",
            "",
            "\tif (LOCK_CONTENDED_RETURN(sem, __down_read_trylock, __down_read_interruptible)) {",
            "\t\trwsem_release(&sem->dep_map, _RET_IP_);",
            "\t\treturn -EINTR;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __sched down_read_killable(struct rw_semaphore *sem)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire_read(&sem->dep_map, 0, 0, _RET_IP_);",
            "",
            "\tif (LOCK_CONTENDED_RETURN(sem, __down_read_trylock, __down_read_killable)) {",
            "\t\trwsem_release(&sem->dep_map, _RET_IP_);",
            "\t\treturn -EINTR;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int down_read_trylock(struct rw_semaphore *sem)",
            "{",
            "\tint ret = __down_read_trylock(sem);",
            "",
            "\tif (ret == 1)",
            "\t\trwsem_acquire_read(&sem->dep_map, 0, 1, _RET_IP_);",
            "\treturn ret;",
            "}",
            "void __sched down_write(struct rw_semaphore *sem)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire(&sem->dep_map, 0, 0, _RET_IP_);",
            "\tLOCK_CONTENDED(sem, __down_write_trylock, __down_write);",
            "}",
            "int __sched down_write_killable(struct rw_semaphore *sem)",
            "{",
            "\tmight_sleep();",
            "\trwsem_acquire(&sem->dep_map, 0, 0, _RET_IP_);",
            "",
            "\tif (LOCK_CONTENDED_RETURN(sem, __down_write_trylock,",
            "\t\t\t\t  __down_write_killable)) {",
            "\t\trwsem_release(&sem->dep_map, _RET_IP_);",
            "\t\treturn -EINTR;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int down_write_trylock(struct rw_semaphore *sem)",
            "{",
            "\tint ret = __down_write_trylock(sem);",
            "",
            "\tif (ret == 1)",
            "\t\trwsem_acquire(&sem->dep_map, 0, 1, _RET_IP_);",
            "",
            "\treturn ret;",
            "}",
            "void up_read(struct rw_semaphore *sem)",
            "{",
            "\trwsem_release(&sem->dep_map, _RET_IP_);",
            "\t__up_read(sem);",
            "}",
            "void up_write(struct rw_semaphore *sem)",
            "{",
            "\trwsem_release(&sem->dep_map, _RET_IP_);",
            "\t__up_write(sem);",
            "}"
          ],
          "function_name": "__down_write_killable, __down_write_trylock, __up_write, __downgrade_write, __rwsem_set_reader_owned, is_rwsem_reader_owned, down_read, down_read_interruptible, down_read_killable, down_read_trylock, down_write, down_write_killable, down_write_trylock, up_read, up_write",
          "description": "提供标准的读写信号量API接口，包含down_read/down_write等公共函数，通过LOCK_CONTENDED宏协调自旋尝试与阻塞等待，整合锁的获取/释放追踪机制（如rwsem_acquire/read_unlock），并处理中断可睡眠的锁获取场景。",
          "similarity": 0.5896247625350952
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/locking/rwsem.c",
          "start_line": 941,
          "end_line": 1044,
          "content": [
            "static inline void clear_nonspinnable(struct rw_semaphore *sem)",
            "{",
            "\tif (unlikely(rwsem_test_oflags(sem, RWSEM_NONSPINNABLE)))",
            "\t\tatomic_long_andnot(RWSEM_NONSPINNABLE, &sem->owner);",
            "}",
            "static inline bool rwsem_can_spin_on_owner(struct rw_semaphore *sem)",
            "{",
            "\treturn false;",
            "}",
            "static inline bool rwsem_optimistic_spin(struct rw_semaphore *sem)",
            "{",
            "\treturn false;",
            "}",
            "static inline void clear_nonspinnable(struct rw_semaphore *sem) { }",
            "static inline enum owner_state",
            "rwsem_spin_on_owner(struct rw_semaphore *sem)",
            "{",
            "\treturn OWNER_NONSPINNABLE;",
            "}",
            "static inline void rwsem_cond_wake_waiter(struct rw_semaphore *sem, long count,",
            "\t\t\t\t\t  struct wake_q_head *wake_q)",
            "{",
            "\tenum rwsem_wake_type wake_type;",
            "",
            "\tif (count & RWSEM_WRITER_MASK)",
            "\t\treturn;",
            "",
            "\tif (count & RWSEM_READER_MASK) {",
            "\t\twake_type = RWSEM_WAKE_READERS;",
            "\t} else {",
            "\t\twake_type = RWSEM_WAKE_ANY;",
            "\t\tclear_nonspinnable(sem);",
            "\t}",
            "\trwsem_mark_wake(sem, wake_type, wake_q);",
            "}",
            "static __always_inline int __down_read_common(struct rw_semaphore *sem, int state)",
            "{",
            "\tint ret = 0;",
            "\tlong count;",
            "",
            "\tpreempt_disable();",
            "\tif (!rwsem_read_trylock(sem, &count)) {",
            "\t\tif (IS_ERR(rwsem_down_read_slowpath(sem, count, state))) {",
            "\t\t\tret = -EINTR;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tDEBUG_RWSEMS_WARN_ON(!is_rwsem_reader_owned(sem), sem);",
            "\t}",
            "out:",
            "\tpreempt_enable();",
            "\treturn ret;",
            "}",
            "static __always_inline void __down_read(struct rw_semaphore *sem)",
            "{",
            "\t__down_read_common(sem, TASK_UNINTERRUPTIBLE);",
            "}",
            "static __always_inline int __down_read_interruptible(struct rw_semaphore *sem)",
            "{",
            "\treturn __down_read_common(sem, TASK_INTERRUPTIBLE);",
            "}",
            "static __always_inline int __down_read_killable(struct rw_semaphore *sem)",
            "{",
            "\treturn __down_read_common(sem, TASK_KILLABLE);",
            "}",
            "static inline int __down_read_trylock(struct rw_semaphore *sem)",
            "{",
            "\tint ret = 0;",
            "\tlong tmp;",
            "",
            "\tDEBUG_RWSEMS_WARN_ON(sem->magic != sem, sem);",
            "",
            "\tpreempt_disable();",
            "\ttmp = atomic_long_read(&sem->count);",
            "\twhile (!(tmp & RWSEM_READ_FAILED_MASK)) {",
            "\t\tif (atomic_long_try_cmpxchg_acquire(&sem->count, &tmp,",
            "\t\t\t\t\t\t    tmp + RWSEM_READER_BIAS)) {",
            "\t\t\trwsem_set_reader_owned(sem);",
            "\t\t\tret = 1;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tpreempt_enable();",
            "\treturn ret;",
            "}",
            "static __always_inline int __down_write_common(struct rw_semaphore *sem, int state)",
            "{",
            "\tint ret = 0;",
            "",
            "\tpreempt_disable();",
            "\tif (unlikely(!rwsem_write_trylock(sem))) {",
            "\t\tif (IS_ERR(rwsem_down_write_slowpath(sem, state)))",
            "\t\t\tret = -EINTR;",
            "\t}",
            "\tpreempt_enable();",
            "\treturn ret;",
            "}",
            "static __always_inline void __down_write(struct rw_semaphore *sem)",
            "{",
            "\t__down_write_common(sem, TASK_UNINTERRUPTIBLE);",
            "}",
            "static __always_inline int __down_write_killable(struct rw_semaphore *sem)",
            "{",
            "\treturn __down_write_common(sem, TASK_KILLABLE);",
            "}"
          ],
          "function_name": "clear_nonspinnable, rwsem_can_spin_on_owner, rwsem_optimistic_spin, clear_nonspinnable, rwsem_spin_on_owner, rwsem_cond_wake_waiter, __down_read_common, __down_read, __down_read_interruptible, __down_read_killable, __down_read_trylock, __down_write_common, __down_write, __down_write_killable",
          "description": "定义与读写信号量状态管理相关的一组内联函数，包括清除非自旋标志位、判断能否自旋等待、处理唤醒逻辑及读写锁的获取流程。其中__down_read_common和__down_write_common负责通用的锁获取逻辑，通过preempt_disable/enable保护临界区，并根据尝试获取结果决定是否进入慢速路径。",
          "similarity": 0.5701603889465332
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/rwsem.c",
          "start_line": 140,
          "end_line": 249,
          "content": [
            "static inline void rwsem_set_owner(struct rw_semaphore *sem)",
            "{",
            "\tlockdep_assert_preemption_disabled();",
            "\tatomic_long_set(&sem->owner, (long)current);",
            "}",
            "static inline void rwsem_clear_owner(struct rw_semaphore *sem)",
            "{",
            "\tlockdep_assert_preemption_disabled();",
            "\tatomic_long_set(&sem->owner, 0);",
            "}",
            "static inline bool rwsem_test_oflags(struct rw_semaphore *sem, long flags)",
            "{",
            "\treturn atomic_long_read(&sem->owner) & flags;",
            "}",
            "static inline void __rwsem_set_reader_owned(struct rw_semaphore *sem,",
            "\t\t\t\t\t    struct task_struct *owner)",
            "{",
            "\tunsigned long val = (unsigned long)owner | RWSEM_READER_OWNED |",
            "\t\t(atomic_long_read(&sem->owner) & RWSEM_NONSPINNABLE);",
            "",
            "\tatomic_long_set(&sem->owner, val);",
            "}",
            "static inline void rwsem_set_reader_owned(struct rw_semaphore *sem)",
            "{",
            "\t__rwsem_set_reader_owned(sem, current);",
            "}",
            "static inline bool is_rwsem_reader_owned(struct rw_semaphore *sem)",
            "{",
            "#ifdef CONFIG_DEBUG_RWSEMS",
            "\t/*",
            "\t * Check the count to see if it is write-locked.",
            "\t */",
            "\tlong count = atomic_long_read(&sem->count);",
            "",
            "\tif (count & RWSEM_WRITER_MASK)",
            "\t\treturn false;",
            "#endif",
            "\treturn rwsem_test_oflags(sem, RWSEM_READER_OWNED);",
            "}",
            "static inline void rwsem_clear_reader_owned(struct rw_semaphore *sem)",
            "{",
            "\tunsigned long val = atomic_long_read(&sem->owner);",
            "",
            "\twhile ((val & ~RWSEM_OWNER_FLAGS_MASK) == (unsigned long)current) {",
            "\t\tif (atomic_long_try_cmpxchg(&sem->owner, &val,",
            "\t\t\t\t\t    val & RWSEM_OWNER_FLAGS_MASK))",
            "\t\t\treturn;",
            "\t}",
            "}",
            "static inline void rwsem_clear_reader_owned(struct rw_semaphore *sem)",
            "{",
            "}",
            "static inline void rwsem_set_nonspinnable(struct rw_semaphore *sem)",
            "{",
            "\tunsigned long owner = atomic_long_read(&sem->owner);",
            "",
            "\tdo {",
            "\t\tif (!(owner & RWSEM_READER_OWNED))",
            "\t\t\tbreak;",
            "\t\tif (owner & RWSEM_NONSPINNABLE)",
            "\t\t\tbreak;",
            "\t} while (!atomic_long_try_cmpxchg(&sem->owner, &owner,",
            "\t\t\t\t\t  owner | RWSEM_NONSPINNABLE));",
            "}",
            "static inline bool rwsem_read_trylock(struct rw_semaphore *sem, long *cntp)",
            "{",
            "\t*cntp = atomic_long_add_return_acquire(RWSEM_READER_BIAS, &sem->count);",
            "",
            "\tif (WARN_ON_ONCE(*cntp < 0))",
            "\t\trwsem_set_nonspinnable(sem);",
            "",
            "\tif (!(*cntp & RWSEM_READ_FAILED_MASK)) {",
            "\t\trwsem_set_reader_owned(sem);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}",
            "static inline bool rwsem_write_trylock(struct rw_semaphore *sem)",
            "{",
            "\tlong tmp = RWSEM_UNLOCKED_VALUE;",
            "",
            "\tif (atomic_long_try_cmpxchg_acquire(&sem->count, &tmp, RWSEM_WRITER_LOCKED)) {",
            "\t\trwsem_set_owner(sem);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}",
            "void __init_rwsem(struct rw_semaphore *sem, const char *name,",
            "\t\t  struct lock_class_key *key)",
            "{",
            "#ifdef CONFIG_DEBUG_LOCK_ALLOC",
            "\t/*",
            "\t * Make sure we are not reinitializing a held semaphore:",
            "\t */",
            "\tdebug_check_no_locks_freed((void *)sem, sizeof(*sem));",
            "\tlockdep_init_map_wait(&sem->dep_map, name, key, 0, LD_WAIT_SLEEP);",
            "#endif",
            "#ifdef CONFIG_DEBUG_RWSEMS",
            "\tsem->magic = sem;",
            "#endif",
            "\tatomic_long_set(&sem->count, RWSEM_UNLOCKED_VALUE);",
            "\traw_spin_lock_init(&sem->wait_lock);",
            "\tINIT_LIST_HEAD(&sem->wait_list);",
            "\tatomic_long_set(&sem->owner, 0L);",
            "#ifdef CONFIG_RWSEM_SPIN_ON_OWNER",
            "\tosq_lock_init(&sem->osq);",
            "#endif",
            "}"
          ],
          "function_name": "rwsem_set_owner, rwsem_clear_owner, rwsem_test_oflags, __rwsem_set_reader_owned, rwsem_set_reader_owned, is_rwsem_reader_owned, rwsem_clear_reader_owned, rwsem_clear_reader_owned, rwsem_set_nonspinnable, rwsem_read_trylock, rwsem_write_trylock, __init_rwsem",
          "description": "提供读写信号量的核心操作函数，包括设置/清除所有者、尝试加锁、初始化信号量结构体及其状态管理",
          "similarity": 0.5574722290039062
        }
      ]
    },
    {
      "source_file": "kernel/locking/lockdep.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:37:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\lockdep.c`\n\n---\n\n# `locking/lockdep.c` 技术文档\n\n## 1. 文件概述\n\n`lockdep.c` 是 Linux 内核中 **运行时锁依赖验证器（Lock Dependency Validator）** 的核心实现文件。该模块用于在内核运行期间动态追踪所有锁的获取顺序和依赖关系，旨在提前检测潜在的死锁风险和锁使用错误，即使当前执行路径并未实际触发死锁。其主要目标是发现以下三类并发编程错误：\n\n- **锁顺序反转（Lock Inversion）**：两个任务以相反顺序获取同一组锁。\n- **循环锁依赖（Circular Lock Dependencies）**：形成 A→B→C→A 的依赖环。\n- **中断上下文锁安全违规**：在硬中断（hardirq）或软中断（softirq）上下文中不安全地使用锁（如在中断禁用区域持有非 IRQ-safe 锁）。\n\n该机制由 Ingo Molnar 和 Peter Zijlstra 设计实现，基于 Arjan van de Ven 提出的运行时锁依赖映射思想。\n\n## 2. 核心功能\n\n### 主要全局变量与数据结构\n- `lockdep_recursion`（per-CPU）：递归计数器，防止 lockdep 自身递归调用。\n- `__lock`：原始自旋锁（`arch_spinlock_t`），保护 lockdep 内部全局数据结构。\n- `lock_classes[]`：存储所有已识别的锁类（`struct lock_class`），最大数量由 `MAX_LOCKDEP_KEYS` 限定。\n- `list_entries[]`：存储锁依赖边（`struct lock_list`），用于构建依赖图。\n- `lock_keys_hash[]`：哈希表，用于快速查找锁类。\n- `cpu_lock_stats`（per-CPU，仅当 `CONFIG_LOCK_STAT` 启用）：锁性能统计信息。\n\n### 关键函数\n- `lockdep_enabled()`：判断当前是否启用 lockdep 验证。\n- `lockdep_lock()` / `lockdep_unlock()`：获取/释放 lockdep 内部保护锁。\n- `graph_lock()` / `graph_unlock()`：安全地获取 lockdep 依赖图锁。\n- `hlock_class()`：根据 `held_lock` 获取对应的 `lock_class`。\n- `lock_stats()` / `clear_lock_stats()`（仅当 `CONFIG_LOCK_STAT` 启用）：聚合/清除锁统计信息。\n\n### 配置参数（可通过 sysfs 或 module_param 调整）\n- `prove_locking`（`CONFIG_PROVE_LOCKING`）：启用/禁用锁正确性验证。\n- `lock_stat`（`CONFIG_LOCK_STAT`）：启用/禁用锁性能统计。\n\n## 3. 关键实现\n\n### 锁依赖图构建\n- 每个锁实例在首次使用时被归类到一个 **锁类（lock class）**，相同类型的锁（如多个 `struct mutex` 实例）共享同一类。\n- 每次成功获取锁时，lockdep 记录当前任务持有的锁栈，并为新锁与栈中每个已有锁建立 **依赖边（A → B 表示先持 A 再持 B）**。\n- 所有依赖边存储在 `list_entries[]` 中，通过位图 `list_entries_in_use` 管理分配。\n\n### 死锁检测算法\n- 使用 **深度优先搜索（DFS）** 遍历依赖图，检测是否存在从新锁指向当前锁栈中某锁的反向路径（即环）。\n- 支持跨任务、跨时间的依赖检查：只要历史上存在过相反的锁序，即报告潜在死锁。\n\n### 中断上下文安全检查\n- 跟踪每个锁类在不同上下文（进程、softirq、hardirq）中的使用情况。\n- 若某锁在中断上下文中被持有，但未标记为 IRQ-safe，则在进程上下文获取该锁时会触发警告。\n\n### 递归防护机制\n- 通过 per-CPU 变量 `lockdep_recursion` 和任务结构中的 `lockdep_recursion` 字段，防止 lockdep 自身在验证过程中因嵌套锁操作而递归崩溃。\n- 内部保护锁 `__lock` 使用 **原始自旋锁（raw spinlock）**，避免其自身调用路径触发 lockdep 检查。\n\n### 性能统计（`CONFIG_LOCK_STAT`）\n- 每个 CPU 维护独立的锁统计结构，记录：\n  - 等待时间（`read_waittime`/`write_waittime`）\n  - 持有时间（`read_holdtime`/`write_holdtime`）\n  - 争用点（`contention_point`）和竞争源（`contending_point`）\n- 通过 `lock_stats()` 聚合所有 CPU 的统计数据供调试使用。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- **调度子系统**：`<linux/sched.h>`、`<linux/sched/clock.h>` 等，用于获取当前任务和时间戳。\n- **中断管理**：`<linux/interrupt.h>`、`<linux/irqflags.h>`，用于上下文判断。\n- **内存管理**：`<linux/gfp.h>`、`<linux/mm.h>`，用于动态分配（部分功能）。\n- **调试支持**：`<linux/debug_locks.h>`、`<linux/stacktrace.h>`、`<linux/kallsyms.h>`，用于错误报告和符号解析。\n- **同步原语**：`<linux/mutex.h>`、`<linux/spinlock.h>`，定义锁类型。\n- **跟踪系统**：`<trace/events/lock.h>`，集成 ftrace 锁事件。\n\n### 内部依赖\n- 依赖 `lockdep_internals.h` 中定义的内部数据结构和辅助宏。\n- 与 `kernel/locking/` 目录下的其他锁实现（如 `mutex.c`、`spinlock.c`）通过 `lock_acquire()`/`lock_release()` 等钩子函数交互。\n\n## 5. 使用场景\n\n- **内核开发与调试**：在 `CONFIG_PROVE_LOCKING` 启用时，自动检测驱动或子系统中的潜在死锁，是内核开发的重要调试工具。\n- **运行时验证**：在测试或生产环境中（通常仅限调试版本）持续监控锁行为，提前暴露并发问题。\n- **性能分析**：通过 `CONFIG_LOCK_STAT` 收集锁争用和延迟数据，优化高并发路径。\n- **中断安全审计**：确保中断处理程序中使用的锁符合 IRQ-safe 要求，防止系统挂死。\n- **自检机制**：内核启动时执行 lockdep 自检（`lockdep_selftest_task_struct` 相关逻辑，虽未在片段中完整显示），验证 lockdep 自身功能正常。",
      "similarity": 0.6479048728942871,
      "chunks": [
        {
          "chunk_id": 25,
          "file_path": "kernel/locking/lockdep.c",
          "start_line": 4409,
          "end_line": 4543,
          "content": [
            "void noinstr lockdep_hardirqs_on(unsigned long ip)",
            "{",
            "\tstruct irqtrace_events *trace = &current->irqtrace;",
            "",
            "\tif (unlikely(!debug_locks))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * NMIs can happen in the middle of local_irq_{en,dis}able() where the",
            "\t * tracking state and hardware state are out of sync.",
            "\t *",
            "\t * NMIs must save lockdep_hardirqs_enabled() to restore IRQ state from,",
            "\t * and not rely on hardware state like normal interrupts.",
            "\t */",
            "\tif (unlikely(in_nmi())) {",
            "\t\tif (!IS_ENABLED(CONFIG_TRACE_IRQFLAGS_NMI))",
            "\t\t\treturn;",
            "",
            "\t\t/*",
            "\t\t * Skip:",
            "\t\t *  - recursion check, because NMI can hit lockdep;",
            "\t\t *  - hardware state check, because above;",
            "\t\t *  - chain_key check, see lockdep_hardirqs_on_prepare().",
            "\t\t */",
            "\t\tgoto skip_checks;",
            "\t}",
            "",
            "\tif (unlikely(this_cpu_read(lockdep_recursion)))",
            "\t\treturn;",
            "",
            "\tif (lockdep_hardirqs_enabled()) {",
            "\t\t/*",
            "\t\t * Neither irq nor preemption are disabled here",
            "\t\t * so this is racy by nature but losing one hit",
            "\t\t * in a stat is not a big deal.",
            "\t\t */",
            "\t\t__debug_atomic_inc(redundant_hardirqs_on);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * We're enabling irqs and according to our state above irqs weren't",
            "\t * already enabled, yet we find the hardware thinks they are in fact",
            "\t * enabled.. someone messed up their IRQ state tracing.",
            "\t */",
            "\tif (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Ensure the lock stack remained unchanged between",
            "\t * lockdep_hardirqs_on_prepare() and lockdep_hardirqs_on().",
            "\t */",
            "\tDEBUG_LOCKS_WARN_ON(current->hardirq_chain_key !=",
            "\t\t\t    current->curr_chain_key);",
            "",
            "skip_checks:",
            "\t/* we'll do an OFF -> ON transition: */",
            "\t__this_cpu_write(hardirqs_enabled, 1);",
            "\ttrace->hardirq_enable_ip = ip;",
            "\ttrace->hardirq_enable_event = ++trace->irq_events;",
            "\tdebug_atomic_inc(hardirqs_on_events);",
            "}",
            "void noinstr lockdep_hardirqs_off(unsigned long ip)",
            "{",
            "\tif (unlikely(!debug_locks))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Matching lockdep_hardirqs_on(), allow NMIs in the middle of lockdep;",
            "\t * they will restore the software state. This ensures the software",
            "\t * state is consistent inside NMIs as well.",
            "\t */",
            "\tif (in_nmi()) {",
            "\t\tif (!IS_ENABLED(CONFIG_TRACE_IRQFLAGS_NMI))",
            "\t\t\treturn;",
            "\t} else if (__this_cpu_read(lockdep_recursion))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * So we're supposed to get called after you mask local IRQs, but for",
            "\t * some reason the hardware doesn't quite think you did a proper job.",
            "\t */",
            "\tif (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))",
            "\t\treturn;",
            "",
            "\tif (lockdep_hardirqs_enabled()) {",
            "\t\tstruct irqtrace_events *trace = &current->irqtrace;",
            "",
            "\t\t/*",
            "\t\t * We have done an ON -> OFF transition:",
            "\t\t */",
            "\t\t__this_cpu_write(hardirqs_enabled, 0);",
            "\t\ttrace->hardirq_disable_ip = ip;",
            "\t\ttrace->hardirq_disable_event = ++trace->irq_events;",
            "\t\tdebug_atomic_inc(hardirqs_off_events);",
            "\t} else {",
            "\t\tdebug_atomic_inc(redundant_hardirqs_off);",
            "\t}",
            "}",
            "void lockdep_softirqs_on(unsigned long ip)",
            "{",
            "\tstruct irqtrace_events *trace = &current->irqtrace;",
            "",
            "\tif (unlikely(!lockdep_enabled()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * We fancy IRQs being disabled here, see softirq.c, avoids",
            "\t * funny state and nesting things.",
            "\t */",
            "\tif (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))",
            "\t\treturn;",
            "",
            "\tif (current->softirqs_enabled) {",
            "\t\tdebug_atomic_inc(redundant_softirqs_on);",
            "\t\treturn;",
            "\t}",
            "",
            "\tlockdep_recursion_inc();",
            "\t/*",
            "\t * We'll do an OFF -> ON transition:",
            "\t */",
            "\tcurrent->softirqs_enabled = 1;",
            "\ttrace->softirq_enable_ip = ip;",
            "\ttrace->softirq_enable_event = ++trace->irq_events;",
            "\tdebug_atomic_inc(softirqs_on_events);",
            "\t/*",
            "\t * We are going to turn softirqs on, so set the",
            "\t * usage bit for all held locks, if hardirqs are",
            "\t * enabled too:",
            "\t */",
            "\tif (lockdep_hardirqs_enabled())",
            "\t\tmark_held_locks(current, LOCK_ENABLED_SOFTIRQ);",
            "\tlockdep_recursion_finish();",
            "}"
          ],
          "function_name": "lockdep_hardirqs_on, lockdep_hardirqs_off, lockdep_softirqs_on",
          "description": "实现硬中断使能/禁止的锁跟踪逻辑，通过更新hardirqs_enabled标志并记录跟踪事件，确保中断状态与硬件状态同步，处理NMI场景下的特殊规则。",
          "similarity": 0.593895673751831
        },
        {
          "chunk_id": 38,
          "file_path": "kernel/locking/lockdep.c",
          "start_line": 6403,
          "end_line": 6508,
          "content": [
            "static void lockdep_free_key_range_imm(void *start, unsigned long size)",
            "{",
            "\tstruct pending_free *pf = delayed_free.pf;",
            "\tunsigned long flags;",
            "",
            "\tinit_data_structures_once();",
            "",
            "\traw_local_irq_save(flags);",
            "\tlockdep_lock();",
            "\t__lockdep_free_key_range(pf, start, size);",
            "\t__free_zapped_classes(pf);",
            "\tlockdep_unlock();",
            "\traw_local_irq_restore(flags);",
            "}",
            "void lockdep_free_key_range(void *start, unsigned long size)",
            "{",
            "\tinit_data_structures_once();",
            "",
            "\tif (inside_selftest())",
            "\t\tlockdep_free_key_range_imm(start, size);",
            "\telse",
            "\t\tlockdep_free_key_range_reg(start, size);",
            "}",
            "static bool lock_class_cache_is_registered(struct lockdep_map *lock)",
            "{",
            "\tstruct lock_class *class;",
            "\tstruct hlist_head *head;",
            "\tint i, j;",
            "",
            "\tfor (i = 0; i < CLASSHASH_SIZE; i++) {",
            "\t\thead = classhash_table + i;",
            "\t\thlist_for_each_entry_rcu(class, head, hash_entry) {",
            "\t\t\tfor (j = 0; j < NR_LOCKDEP_CACHING_CLASSES; j++)",
            "\t\t\t\tif (lock->class_cache[j] == class)",
            "\t\t\t\t\treturn true;",
            "\t\t}",
            "\t}",
            "\treturn false;",
            "}",
            "static void __lockdep_reset_lock(struct pending_free *pf,",
            "\t\t\t\t struct lockdep_map *lock)",
            "{",
            "\tstruct lock_class *class;",
            "\tint j;",
            "",
            "\t/*",
            "\t * Remove all classes this lock might have:",
            "\t */",
            "\tfor (j = 0; j < MAX_LOCKDEP_SUBCLASSES; j++) {",
            "\t\t/*",
            "\t\t * If the class exists we look it up and zap it:",
            "\t\t */",
            "\t\tclass = look_up_lock_class(lock, j);",
            "\t\tif (class)",
            "\t\t\tzap_class(pf, class);",
            "\t}",
            "\t/*",
            "\t * Debug check: in the end all mapped classes should",
            "\t * be gone.",
            "\t */",
            "\tif (WARN_ON_ONCE(lock_class_cache_is_registered(lock)))",
            "\t\tdebug_locks_off();",
            "}",
            "static void lockdep_reset_lock_reg(struct lockdep_map *lock)",
            "{",
            "\tstruct pending_free *pf;",
            "\tunsigned long flags;",
            "\tint locked;",
            "\tbool need_callback = false;",
            "",
            "\traw_local_irq_save(flags);",
            "\tlocked = graph_lock();",
            "\tif (!locked)",
            "\t\tgoto out_irq;",
            "",
            "\tpf = get_pending_free();",
            "\t__lockdep_reset_lock(pf, lock);",
            "\tneed_callback = prepare_call_rcu_zapped(pf);",
            "",
            "\tgraph_unlock();",
            "out_irq:",
            "\traw_local_irq_restore(flags);",
            "\tif (need_callback)",
            "\t\tcall_rcu(&delayed_free.rcu_head, free_zapped_rcu);",
            "}",
            "static void lockdep_reset_lock_imm(struct lockdep_map *lock)",
            "{",
            "\tstruct pending_free *pf = delayed_free.pf;",
            "\tunsigned long flags;",
            "",
            "\traw_local_irq_save(flags);",
            "\tlockdep_lock();",
            "\t__lockdep_reset_lock(pf, lock);",
            "\t__free_zapped_classes(pf);",
            "\tlockdep_unlock();",
            "\traw_local_irq_restore(flags);",
            "}",
            "void lockdep_reset_lock(struct lockdep_map *lock)",
            "{",
            "\tinit_data_structures_once();",
            "",
            "\tif (inside_selftest())",
            "\t\tlockdep_reset_lock_imm(lock);",
            "\telse",
            "\t\tlockdep_reset_lock_reg(lock);",
            "}"
          ],
          "function_name": "lockdep_free_key_range_imm, lockdep_free_key_range, lock_class_cache_is_registered, __lockdep_reset_lock, lockdep_reset_lock_reg, lockdep_reset_lock_imm, lockdep_reset_lock",
          "description": "实现锁键范围释放和重置操作，区分立即/延迟模式处理锁依赖缓存的注册与清除",
          "similarity": 0.5836359262466431
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/locking/lockdep.c",
          "start_line": 2132,
          "end_line": 2251,
          "content": [
            "unsigned long lockdep_count_backward_deps(struct lock_class *class)",
            "{",
            "\tunsigned long ret, flags;",
            "\tstruct lock_list this;",
            "",
            "\t__bfs_init_root(&this, class);",
            "",
            "\traw_local_irq_save(flags);",
            "\tlockdep_lock();",
            "\tret = __lockdep_count_backward_deps(&this);",
            "\tlockdep_unlock();",
            "\traw_local_irq_restore(flags);",
            "",
            "\treturn ret;",
            "}",
            "static noinline enum bfs_result",
            "check_path(struct held_lock *target, struct lock_list *src_entry,",
            "\t   bool (*match)(struct lock_list *entry, void *data),",
            "\t   bool (*skip)(struct lock_list *entry, void *data),",
            "\t   struct lock_list **target_entry)",
            "{",
            "\tenum bfs_result ret;",
            "",
            "\tret = __bfs_forwards(src_entry, target, match, skip, target_entry);",
            "",
            "\tif (unlikely(bfs_error(ret)))",
            "\t\tprint_bfs_bug(ret);",
            "",
            "\treturn ret;",
            "}",
            "static noinline enum bfs_result",
            "check_noncircular(struct held_lock *src, struct held_lock *target,",
            "\t\t  struct lock_trace **const trace)",
            "{",
            "\tenum bfs_result ret;",
            "\tstruct lock_list *target_entry;",
            "\tstruct lock_list src_entry;",
            "",
            "\tbfs_init_root(&src_entry, src);",
            "",
            "\tdebug_atomic_inc(nr_cyclic_checks);",
            "",
            "\tret = check_path(target, &src_entry, hlock_conflict, NULL, &target_entry);",
            "",
            "\tif (unlikely(ret == BFS_RMATCH)) {",
            "\t\tif (!*trace) {",
            "\t\t\t/*",
            "\t\t\t * If save_trace fails here, the printing might",
            "\t\t\t * trigger a WARN but because of the !nr_entries it",
            "\t\t\t * should not do bad things.",
            "\t\t\t */",
            "\t\t\t*trace = save_trace();",
            "\t\t}",
            "",
            "\t\tif (src->class_idx == target->class_idx)",
            "\t\t\tprint_deadlock_bug(current, src, target);",
            "\t\telse",
            "\t\t\tprint_circular_bug(&src_entry, target_entry, src, target);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "static inline bool usage_accumulate(struct lock_list *entry, void *mask)",
            "{",
            "\tif (!entry->only_xr)",
            "\t\t*(unsigned long *)mask |= entry->class->usage_mask;",
            "\telse /* Mask out _READ usage bits */",
            "\t\t*(unsigned long *)mask |= (entry->class->usage_mask & LOCKF_IRQ);",
            "",
            "\treturn false;",
            "}",
            "static inline bool usage_match(struct lock_list *entry, void *mask)",
            "{",
            "\tif (!entry->only_xr)",
            "\t\treturn !!(entry->class->usage_mask & *(unsigned long *)mask);",
            "\telse /* Mask out _READ usage bits */",
            "\t\treturn !!((entry->class->usage_mask & LOCKF_IRQ) & *(unsigned long *)mask);",
            "}",
            "static inline bool usage_skip(struct lock_list *entry, void *mask)",
            "{",
            "\tif (entry->class->lock_type == LD_LOCK_NORMAL)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Skip local_lock() for irq inversion detection.",
            "\t *",
            "\t * For !RT, local_lock() is not a real lock, so it won't carry any",
            "\t * dependency.",
            "\t *",
            "\t * For RT, an irq inversion happens when we have lock A and B, and on",
            "\t * some CPU we can have:",
            "\t *",
            "\t *\tlock(A);",
            "\t *\t<interrupted>",
            "\t *\t  lock(B);",
            "\t *",
            "\t * where lock(B) cannot sleep, and we have a dependency B -> ... -> A.",
            "\t *",
            "\t * Now we prove local_lock() cannot exist in that dependency. First we",
            "\t * have the observation for any lock chain L1 -> ... -> Ln, for any",
            "\t * 1 <= i <= n, Li.inner_wait_type <= L1.inner_wait_type, otherwise",
            "\t * wait context check will complain. And since B is not a sleep lock,",
            "\t * therefore B.inner_wait_type >= 2, and since the inner_wait_type of",
            "\t * local_lock() is 3, which is greater than 2, therefore there is no",
            "\t * way the local_lock() exists in the dependency B -> ... -> A.",
            "\t *",
            "\t * As a result, we will skip local_lock(), when we search for irq",
            "\t * inversion bugs.",
            "\t */",
            "\tif (entry->class->lock_type == LD_LOCK_PERCPU &&",
            "\t    DEBUG_LOCKS_WARN_ON(entry->class->wait_type_inner < LD_WAIT_CONFIG))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Skip WAIT_OVERRIDE for irq inversion detection -- it's not actually",
            "\t * a lock and only used to override the wait_type.",
            "\t */",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "lockdep_count_backward_deps, check_path, check_noncircular, usage_accumulate, usage_match, usage_skip",
          "description": "实现非循环路径检查及锁使用掩码的累积、匹配和跳过逻辑，用于追踪锁的持有状态和中断安全依赖关系。",
          "similarity": 0.5716590881347656
        },
        {
          "chunk_id": 39,
          "file_path": "kernel/locking/lockdep.c",
          "start_line": 6537,
          "end_line": 6639,
          "content": [
            "void lockdep_unregister_key(struct lock_class_key *key)",
            "{",
            "\tstruct hlist_head *hash_head = keyhashentry(key);",
            "\tstruct lock_class_key *k;",
            "\tstruct pending_free *pf;",
            "\tunsigned long flags;",
            "\tbool found = false;",
            "\tbool need_callback = false;",
            "",
            "\tmight_sleep();",
            "",
            "\tif (WARN_ON_ONCE(static_obj(key)))",
            "\t\treturn;",
            "",
            "\traw_local_irq_save(flags);",
            "\tlockdep_lock();",
            "",
            "\thlist_for_each_entry_rcu(k, hash_head, hash_entry) {",
            "\t\tif (k == key) {",
            "\t\t\thlist_del_rcu(&k->hash_entry);",
            "\t\t\tfound = true;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tWARN_ON_ONCE(!found && debug_locks);",
            "\tif (found) {",
            "\t\tpf = get_pending_free();",
            "\t\t__lockdep_free_key_range(pf, key, 1);",
            "\t\tneed_callback = prepare_call_rcu_zapped(pf);",
            "\t}",
            "\tlockdep_unlock();",
            "\traw_local_irq_restore(flags);",
            "",
            "\tif (need_callback)",
            "\t\tcall_rcu(&delayed_free.rcu_head, free_zapped_rcu);",
            "",
            "\t/* Wait until is_dynamic_key() has finished accessing k->hash_entry. */",
            "\tsynchronize_rcu();",
            "}",
            "void __init lockdep_init(void)",
            "{",
            "\tprintk(\"Lock dependency validator: Copyright (c) 2006 Red Hat, Inc., Ingo Molnar\\n\");",
            "",
            "\tprintk(\"... MAX_LOCKDEP_SUBCLASSES:  %lu\\n\", MAX_LOCKDEP_SUBCLASSES);",
            "\tprintk(\"... MAX_LOCK_DEPTH:          %lu\\n\", MAX_LOCK_DEPTH);",
            "\tprintk(\"... MAX_LOCKDEP_KEYS:        %lu\\n\", MAX_LOCKDEP_KEYS);",
            "\tprintk(\"... CLASSHASH_SIZE:          %lu\\n\", CLASSHASH_SIZE);",
            "\tprintk(\"... MAX_LOCKDEP_ENTRIES:     %lu\\n\", MAX_LOCKDEP_ENTRIES);",
            "\tprintk(\"... MAX_LOCKDEP_CHAINS:      %lu\\n\", MAX_LOCKDEP_CHAINS);",
            "\tprintk(\"... CHAINHASH_SIZE:          %lu\\n\", CHAINHASH_SIZE);",
            "",
            "\tprintk(\" memory used by lock dependency info: %zu kB\\n\",",
            "\t       (sizeof(lock_classes) +",
            "\t\tsizeof(lock_classes_in_use) +",
            "\t\tsizeof(classhash_table) +",
            "\t\tsizeof(list_entries) +",
            "\t\tsizeof(list_entries_in_use) +",
            "\t\tsizeof(chainhash_table) +",
            "\t\tsizeof(delayed_free)",
            "#ifdef CONFIG_PROVE_LOCKING",
            "\t\t+ sizeof(lock_cq)",
            "\t\t+ sizeof(lock_chains)",
            "\t\t+ sizeof(lock_chains_in_use)",
            "\t\t+ sizeof(chain_hlocks)",
            "#endif",
            "\t\t) / 1024",
            "\t\t);",
            "",
            "#if defined(CONFIG_TRACE_IRQFLAGS) && defined(CONFIG_PROVE_LOCKING)",
            "\tprintk(\" memory used for stack traces: %zu kB\\n\",",
            "\t       (sizeof(stack_trace) + sizeof(stack_trace_hash)) / 1024",
            "\t       );",
            "#endif",
            "",
            "\tprintk(\" per task-struct memory footprint: %zu bytes\\n\",",
            "\t       sizeof(((struct task_struct *)NULL)->held_locks));",
            "}",
            "static void",
            "print_freed_lock_bug(struct task_struct *curr, const void *mem_from,",
            "\t\t     const void *mem_to, struct held_lock *hlock)",
            "{",
            "\tif (!debug_locks_off())",
            "\t\treturn;",
            "\tif (debug_locks_silent)",
            "\t\treturn;",
            "",
            "\tnbcon_cpu_emergency_enter();",
            "",
            "\tpr_warn(\"\\n\");",
            "\tpr_warn(\"=========================\\n\");",
            "\tpr_warn(\"WARNING: held lock freed!\\n\");",
            "\tprint_kernel_ident();",
            "\tpr_warn(\"-------------------------\\n\");",
            "\tpr_warn(\"%s/%d is freeing memory %px-%px, with a lock still held there!\\n\",",
            "\t\tcurr->comm, task_pid_nr(curr), mem_from, mem_to-1);",
            "\tprint_lock(hlock);",
            "\tlockdep_print_held_locks(curr);",
            "",
            "\tpr_warn(\"\\nstack backtrace:\\n\");",
            "\tdump_stack();",
            "",
            "\tnbcon_cpu_emergency_exit();",
            "}"
          ],
          "function_name": "lockdep_unregister_key, lockdep_init, print_freed_lock_bug",
          "description": "初始化锁依赖验证系统并注册锁键，检测内存释放时仍持锁的情况并触发警告",
          "similarity": 0.5651252269744873
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/lockdep.c",
          "start_line": 104,
          "end_line": 215,
          "content": [
            "static __init int kernel_lockdep_sysctls_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", kern_lockdep_table);",
            "\treturn 0;",
            "}",
            "static __always_inline bool lockdep_enabled(void)",
            "{",
            "\tif (!debug_locks)",
            "\t\treturn false;",
            "",
            "\tif (this_cpu_read(lockdep_recursion))",
            "\t\treturn false;",
            "",
            "\tif (current->lockdep_recursion)",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static inline void lockdep_lock(void)",
            "{",
            "\tDEBUG_LOCKS_WARN_ON(!irqs_disabled());",
            "",
            "\t__this_cpu_inc(lockdep_recursion);",
            "\tarch_spin_lock(&__lock);",
            "\t__owner = current;",
            "}",
            "static inline void lockdep_unlock(void)",
            "{",
            "\tDEBUG_LOCKS_WARN_ON(!irqs_disabled());",
            "",
            "\tif (debug_locks && DEBUG_LOCKS_WARN_ON(__owner != current))",
            "\t\treturn;",
            "",
            "\t__owner = NULL;",
            "\tarch_spin_unlock(&__lock);",
            "\t__this_cpu_dec(lockdep_recursion);",
            "}",
            "static inline bool lockdep_assert_locked(void)",
            "{",
            "\treturn DEBUG_LOCKS_WARN_ON(__owner != current);",
            "}",
            "static int graph_lock(void)",
            "{",
            "\tlockdep_lock();",
            "\t/*",
            "\t * Make sure that if another CPU detected a bug while",
            "\t * walking the graph we dont change it (while the other",
            "\t * CPU is busy printing out stuff with the graph lock",
            "\t * dropped already)",
            "\t */",
            "\tif (!debug_locks) {",
            "\t\tlockdep_unlock();",
            "\t\treturn 0;",
            "\t}",
            "\treturn 1;",
            "}",
            "static inline void graph_unlock(void)",
            "{",
            "\tlockdep_unlock();",
            "}",
            "static inline int debug_locks_off_graph_unlock(void)",
            "{",
            "\tint ret = debug_locks_off();",
            "",
            "\tlockdep_unlock();",
            "",
            "\treturn ret;",
            "}",
            "static inline u64 lockstat_clock(void)",
            "{",
            "\treturn local_clock();",
            "}",
            "static int lock_point(unsigned long points[], unsigned long ip)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < LOCKSTAT_POINTS; i++) {",
            "\t\tif (points[i] == 0) {",
            "\t\t\tpoints[i] = ip;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tif (points[i] == ip)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn i;",
            "}",
            "static void lock_time_inc(struct lock_time *lt, u64 time)",
            "{",
            "\tif (time > lt->max)",
            "\t\tlt->max = time;",
            "",
            "\tif (time < lt->min || !lt->nr)",
            "\t\tlt->min = time;",
            "",
            "\tlt->total += time;",
            "\tlt->nr++;",
            "}",
            "static inline void lock_time_add(struct lock_time *src, struct lock_time *dst)",
            "{",
            "\tif (!src->nr)",
            "\t\treturn;",
            "",
            "\tif (src->max > dst->max)",
            "\t\tdst->max = src->max;",
            "",
            "\tif (src->min < dst->min || !dst->nr)",
            "\t\tdst->min = src->min;",
            "",
            "\tdst->total += src->total;",
            "\tdst->nr += src->nr;",
            "}"
          ],
          "function_name": "kernel_lockdep_sysctls_init, lockdep_enabled, lockdep_lock, lockdep_unlock, lockdep_assert_locked, graph_lock, graph_unlock, debug_locks_off_graph_unlock, lockstat_clock, lock_point, lock_time_inc, lock_time_add",
          "description": "实现锁依赖验证模块的核心同步机制，包括启用检查标志位、递归计数器管理、图遍历锁及时间戳记录功能。",
          "similarity": 0.5639966130256653
        }
      ]
    }
  ]
}