{
  "query": "分布式系统节点故障自动恢复实现",
  "timestamp": "2025-12-26 01:26:21",
  "retrieved_files": [
    {
      "source_file": "kernel/trace/rethook.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:06:37\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `trace\\rethook.c`\n\n---\n\n# `trace/rethook.c` 技术文档\n\n## 1. 文件概述\n\n`rethook.c` 实现了 Linux 内核中的 **rethook（Return Hook）** 机制，这是一种通用的函数返回拦截框架，用于在函数返回时执行回调处理。该机制为 kretprobes、ftrace 等动态追踪工具提供底层支持，允许在函数返回点安全地插入处理逻辑，同时管理返回地址的重写与恢复。rethook 使用 per-task 的无锁链表（LLIST）作为“影子栈”来跟踪活跃的 hook 节点，并结合 RCU 和引用计数实现内存安全的生命周期管理。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct rethook`：rethook 实例，包含回调函数指针、私有数据、节点池（freelist）和引用计数。\n- `struct rethook_node`：单个 hook 节点，嵌入在用户数据结构中，包含返回地址、帧指针、所属 rethook 指针及链表/空闲链表节点。\n\n### 主要函数\n| 函数 | 功能描述 |\n|------|--------|\n| `rethook_alloc()` | 分配并初始化一个新的 `rethook` 实例 |\n| `rethook_free()` / `rethook_stop()` | 停止并异步释放 `rethook` 实例 |\n| `rethook_add_node()` | 向 rethook 添加预分配的节点 |\n| `rethook_try_get()` | 从 rethook 的空闲池中获取一个未使用的节点（需禁用抢占） |\n| `rethook_hook()` | 在函数入口处注册返回 hook，将节点加入当前任务的 rethook 链表 |\n| `rethook_recycle()` | 回收 hook 节点：若 rethook 有效则归还到空闲池，否则异步释放 |\n| `rethook_find_ret_addr()` | 在指定任务的 rethook 链表中查找与给定帧指针对应的真实返回地址 |\n| `rethook_flush_task()` | 在任务退出时清理其所有未返回的 rethook 节点 |\n\n## 3. 关键实现\n\n### 影子栈与任务绑定\n- 每个 `task_struct` 包含一个 `rethooks` 字段（`struct llist_head`），用于存储该任务当前活跃的所有 `rethook_node`。\n- 使用无锁链表（LLIST）实现高效、并发安全的插入（`__llist_add`）和批量删除（`__llist_del_all`）。\n\n### 内存与生命周期管理\n- **引用计数**：`rethook` 的 `ref` 字段初始为 1，每添加一个节点加 1；节点回收时减 1。当引用归零且节点池清空后，`rethook` 本体被释放。\n- **RCU 安全**：通过 `rcu_assign_pointer()` 和 `rcu_dereference_check()` 管理 `handler` 指针的读写，确保在 RCU 读侧临界区内安全访问。\n- **延迟释放**：`rethook_free()` 和无效节点的回收均通过 `call_rcu()` 异步执行，避免在中断或原子上下文中释放内存。\n\n### 上下文感知的 Hook 注入\n- `rethook_hook()` 接收 `mcount` 参数区分调用上下文（ftrace vs kprobe），由架构相关代码（`arch_rethook_prepare()`）决定如何修改返回地址（例如插入 trampoline）。\n- 要求调用者处于 RCU 可用上下文（`rcu_is_watching()`），确保后续的 RCU 回调能正确执行。\n\n### 返回地址恢复\n- `rethook_find_ret_addr()` 遍历任务的 rethook 链表，跳过 trampoline 地址，返回与指定栈帧匹配的真实返回地址，用于栈回溯修正。\n\n## 4. 依赖关系\n\n- **架构依赖**：依赖 `arch_rethook_prepare()` 和 `arch_rethook_trampoline`（由各架构实现），用于实际修改返回地址和提供 trampoline 函数。\n- **内核子系统**：\n  - `<linux/rcu.h>`：RCU 同步机制\n  - `<linux/slab.h>`：动态内存分配\n  - `<linux/preempt.h>`：抢占控制\n  - `<linux/kprobes.h>`：与 kretprobes 集成\n  - `<linux/freelist.h>`：无锁空闲链表实现\n- **任务管理**：依赖 `task_struct::rethooks` 字段和 `delayed_put_task_struct()` 回调。\n\n## 5. 使用场景\n\n- **kretprobes 实现**：作为 kretprobe 的底层机制，拦截函数返回并执行用户定义的处理函数。\n- **ftrace 动态追踪**：在 function graph tracer 等场景中，用于捕获函数返回事件。\n- **内核栈回溯修正**：当函数返回地址被 trampoline 覆盖时，通过 `rethook_find_ret_addr()` 恢复原始调用栈。\n- **安全监控与性能分析**：第三方模块可基于 rethook 框架实现函数级的返回行为监控或统计。",
      "similarity": 0.5776281356811523,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/trace/rethook.c",
          "start_line": 267,
          "end_line": 334,
          "content": [
            "void __weak arch_rethook_fixup_return(struct pt_regs *regs,",
            "\t\t\t\t      unsigned long correct_ret_addr)",
            "{",
            "\t/*",
            "\t * Do nothing by default. If the architecture which uses a",
            "\t * frame pointer to record real return address on the stack,",
            "\t * it should fill this function to fixup the return address",
            "\t * so that stacktrace works from the rethook handler.",
            "\t */",
            "}",
            "unsigned long rethook_trampoline_handler(struct pt_regs *regs,",
            "\t\t\t\t\t unsigned long frame)",
            "{",
            "\tstruct llist_node *first, *node = NULL;",
            "\tunsigned long correct_ret_addr;",
            "\trethook_handler_t handler;",
            "\tstruct rethook_node *rhn;",
            "",
            "\tcorrect_ret_addr = __rethook_find_ret_addr(current, &node);",
            "\tif (!correct_ret_addr) {",
            "\t\tpr_err(\"rethook: Return address not found! Maybe there is a bug in the kernel\\n\");",
            "\t\tBUG_ON(1);",
            "\t}",
            "",
            "\tinstruction_pointer_set(regs, correct_ret_addr);",
            "",
            "\t/*",
            "\t * These loops must be protected from rethook_free_rcu() because those",
            "\t * are accessing 'rhn->rethook'.",
            "\t */",
            "\tpreempt_disable_notrace();",
            "",
            "\t/*",
            "\t * Run the handler on the shadow stack. Do not unlink the list here because",
            "\t * stackdump inside the handlers needs to decode it.",
            "\t */",
            "\tfirst = current->rethooks.first;",
            "\twhile (first) {",
            "\t\trhn = container_of(first, struct rethook_node, llist);",
            "\t\tif (WARN_ON_ONCE(rhn->frame != frame))",
            "\t\t\tbreak;",
            "\t\thandler = rethook_get_handler(rhn->rethook);",
            "\t\tif (handler)",
            "\t\t\thandler(rhn, rhn->rethook->data,",
            "\t\t\t\tcorrect_ret_addr, regs);",
            "",
            "\t\tif (first == node)",
            "\t\t\tbreak;",
            "\t\tfirst = first->next;",
            "\t}",
            "",
            "\t/* Fixup registers for returning to correct address. */",
            "\tarch_rethook_fixup_return(regs, correct_ret_addr);",
            "",
            "\t/* Unlink used shadow stack */",
            "\tfirst = current->rethooks.first;",
            "\tcurrent->rethooks.first = node->next;",
            "\tnode->next = NULL;",
            "",
            "\twhile (first) {",
            "\t\trhn = container_of(first, struct rethook_node, llist);",
            "\t\tfirst = first->next;",
            "\t\trethook_recycle(rhn);",
            "\t}",
            "\tpreempt_enable_notrace();",
            "",
            "\treturn correct_ret_addr;",
            "}"
          ],
          "function_name": "arch_rethook_fixup_return, rethook_trampoline_handler",
          "description": "提供架构无关的返回地址修复接口与trampoline处理函数，协调影子堆栈遍历、处理程序执行及寄存器状态恢复流程",
          "similarity": 0.6239532232284546
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/trace/rethook.c",
          "start_line": 22,
          "end_line": 131,
          "content": [
            "void rethook_flush_task(struct task_struct *tk)",
            "{",
            "\tstruct rethook_node *rhn;",
            "\tstruct llist_node *node;",
            "",
            "\tnode = __llist_del_all(&tk->rethooks);",
            "\twhile (node) {",
            "\t\trhn = container_of(node, struct rethook_node, llist);",
            "\t\tnode = node->next;",
            "\t\tpreempt_disable();",
            "\t\trethook_recycle(rhn);",
            "\t\tpreempt_enable();",
            "\t}",
            "}",
            "static void rethook_free_rcu(struct rcu_head *head)",
            "{",
            "\tstruct rethook *rh = container_of(head, struct rethook, rcu);",
            "\tobjpool_fini(&rh->pool);",
            "}",
            "void rethook_stop(struct rethook *rh)",
            "{",
            "\trcu_assign_pointer(rh->handler, NULL);",
            "}",
            "void rethook_free(struct rethook *rh)",
            "{",
            "\trethook_stop(rh);",
            "",
            "\tcall_rcu(&rh->rcu, rethook_free_rcu);",
            "}",
            "static int rethook_init_node(void *nod, void *context)",
            "{",
            "\tstruct rethook_node *node = nod;",
            "",
            "\tnode->rethook = context;",
            "\treturn 0;",
            "}",
            "static int rethook_fini_pool(struct objpool_head *head, void *context)",
            "{",
            "\tkfree(context);",
            "\treturn 0;",
            "}",
            "static inline rethook_handler_t rethook_get_handler(struct rethook *rh)",
            "{",
            "\treturn (rethook_handler_t)rcu_dereference_check(rh->handler,",
            "\t\t\t\t\t\t\trcu_read_lock_any_held());",
            "}",
            "static void free_rethook_node_rcu(struct rcu_head *head)",
            "{",
            "\tstruct rethook_node *node = container_of(head, struct rethook_node, rcu);",
            "\tstruct rethook *rh = node->rethook;",
            "",
            "\tobjpool_drop(node, &rh->pool);",
            "}",
            "void rethook_recycle(struct rethook_node *node)",
            "{",
            "\trethook_handler_t handler;",
            "",
            "\thandler = rethook_get_handler(node->rethook);",
            "\tif (likely(handler))",
            "\t\tobjpool_push(node, &node->rethook->pool);",
            "\telse",
            "\t\tcall_rcu(&node->rcu, free_rethook_node_rcu);",
            "}",
            "void rethook_hook(struct rethook_node *node, struct pt_regs *regs, bool mcount)",
            "{",
            "\tarch_rethook_prepare(node, regs, mcount);",
            "\t__llist_add(&node->llist, &current->rethooks);",
            "}",
            "static unsigned long __rethook_find_ret_addr(struct task_struct *tsk,",
            "\t\t\t\t\t     struct llist_node **cur)",
            "{",
            "\tstruct rethook_node *rh = NULL;",
            "\tstruct llist_node *node = *cur;",
            "",
            "\tif (!node)",
            "\t\tnode = tsk->rethooks.first;",
            "\telse",
            "\t\tnode = node->next;",
            "",
            "\twhile (node) {",
            "\t\trh = container_of(node, struct rethook_node, llist);",
            "\t\tif (rh->ret_addr != (unsigned long)arch_rethook_trampoline) {",
            "\t\t\t*cur = node;",
            "\t\t\treturn rh->ret_addr;",
            "\t\t}",
            "\t\tnode = node->next;",
            "\t}",
            "\treturn 0;",
            "}",
            "unsigned long rethook_find_ret_addr(struct task_struct *tsk, unsigned long frame,",
            "\t\t\t\t    struct llist_node **cur)",
            "{",
            "\tstruct rethook_node *rhn = NULL;",
            "\tunsigned long ret;",
            "",
            "\tif (WARN_ON_ONCE(!cur))",
            "\t\treturn 0;",
            "",
            "\tif (WARN_ON_ONCE(tsk != current && task_is_running(tsk)))",
            "\t\treturn 0;",
            "",
            "\tdo {",
            "\t\tret = __rethook_find_ret_addr(tsk, cur);",
            "\t\tif (!ret)",
            "\t\t\tbreak;",
            "\t\trhn = container_of(*cur, struct rethook_node, llist);",
            "\t} while (rhn->frame != frame);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "rethook_flush_task, rethook_free_rcu, rethook_stop, rethook_free, rethook_init_node, rethook_fini_pool, rethook_get_handler, free_rethook_node_rcu, rethook_recycle, rethook_hook, __rethook_find_ret_addr, rethook_find_ret_addr",
          "description": "实现rethook节点的生命周期管理，包含RCU安全释放、池化内存管理、影子堆栈操作及异常返回地址查找逻辑",
          "similarity": 0.6130930185317993
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/trace/rethook.c",
          "start_line": 1,
          "end_line": 21,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "",
            "#define pr_fmt(fmt) \"rethook: \" fmt",
            "",
            "#include <linux/bug.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/preempt.h>",
            "#include <linux/rethook.h>",
            "#include <linux/slab.h>",
            "#include <linux/sort.h>",
            "#include <linux/smp.h>",
            "",
            "/* Return hook list (shadow stack by list) */",
            "",
            "/*",
            " * This function is called from delayed_put_task_struct() when a task is",
            " * dead and cleaned up to recycle any kretprobe instances associated with",
            " * this task. These left over instances represent probed functions that",
            " * have been called but will never return.",
            " */"
          ],
          "function_name": null,
          "description": "定义rethook模块的基础结构及函数声明，用于在任务终止时回收关联的kretprobe实例，防止僵尸探针残留",
          "similarity": 0.5110319256782532
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5719545483589172,
      "chunks": [
        {
          "chunk_id": 28,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 5098,
          "end_line": 5206,
          "content": [
            "static void __init rcu_dump_rcu_node_tree(void)",
            "{",
            "\tint level = 0;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tpr_info(\"rcu_node tree layout dump\\n\");",
            "\tpr_info(\" \");",
            "\trcu_for_each_node_breadth_first(rnp) {",
            "\t\tif (rnp->level != level) {",
            "\t\t\tpr_cont(\"\\n\");",
            "\t\t\tpr_info(\" \");",
            "\t\t\tlevel = rnp->level;",
            "\t\t}",
            "\t\tpr_cont(\"%d:%d ^%d  \", rnp->grplo, rnp->grphi, rnp->grpnum);",
            "\t}",
            "\tpr_cont(\"\\n\");",
            "}",
            "static void __init kfree_rcu_batch_init(void)",
            "{",
            "\tint cpu;",
            "\tint i, j;",
            "\tstruct shrinker *kfree_rcu_shrinker;",
            "",
            "\t/* Clamp it to [0:100] seconds interval. */",
            "\tif (rcu_delay_page_cache_fill_msec < 0 ||",
            "\t\trcu_delay_page_cache_fill_msec > 100 * MSEC_PER_SEC) {",
            "",
            "\t\trcu_delay_page_cache_fill_msec =",
            "\t\t\tclamp(rcu_delay_page_cache_fill_msec, 0,",
            "\t\t\t\t(int) (100 * MSEC_PER_SEC));",
            "",
            "\t\tpr_info(\"Adjusting rcutree.rcu_delay_page_cache_fill_msec to %d ms.\\n\",",
            "\t\t\trcu_delay_page_cache_fill_msec);",
            "\t}",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\t\tINIT_RCU_WORK(&krcp->krw_arr[i].rcu_work, kfree_rcu_work);",
            "\t\t\tkrcp->krw_arr[i].krcp = krcp;",
            "",
            "\t\t\tfor (j = 0; j < FREE_N_CHANNELS; j++)",
            "\t\t\t\tINIT_LIST_HEAD(&krcp->krw_arr[i].bulk_head_free[j]);",
            "\t\t}",
            "",
            "\t\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\t\tINIT_LIST_HEAD(&krcp->bulk_head[i]);",
            "",
            "\t\tINIT_DELAYED_WORK(&krcp->monitor_work, kfree_rcu_monitor);",
            "\t\tINIT_DELAYED_WORK(&krcp->page_cache_work, fill_page_cache_func);",
            "\t\tkrcp->initialized = true;",
            "\t}",
            "",
            "\tkfree_rcu_shrinker = shrinker_alloc(0, \"rcu-kfree\");",
            "\tif (!kfree_rcu_shrinker) {",
            "\t\tpr_err(\"Failed to allocate kfree_rcu() shrinker!\\n\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tkfree_rcu_shrinker->count_objects = kfree_rcu_shrink_count;",
            "\tkfree_rcu_shrinker->scan_objects = kfree_rcu_shrink_scan;",
            "",
            "\tshrinker_register(kfree_rcu_shrinker);",
            "}",
            "void __init rcu_init(void)",
            "{",
            "\tint cpu = smp_processor_id();",
            "",
            "\trcu_early_boot_tests();",
            "",
            "\tkfree_rcu_batch_init();",
            "\trcu_bootup_announce();",
            "\tsanitize_kthread_prio();",
            "\trcu_init_geometry();",
            "\trcu_init_one();",
            "\tif (dump_tree)",
            "\t\trcu_dump_rcu_node_tree();",
            "\tif (use_softirq)",
            "\t\topen_softirq(RCU_SOFTIRQ, rcu_core_si);",
            "",
            "\t/*",
            "\t * We don't need protection against CPU-hotplug here because",
            "\t * this is called early in boot, before either interrupts",
            "\t * or the scheduler are operational.",
            "\t */",
            "\tpm_notifier(rcu_pm_notify, 0);",
            "\tWARN_ON(num_online_cpus() > 1); // Only one CPU this early in boot.",
            "\trcutree_prepare_cpu(cpu);",
            "\trcu_cpu_starting(cpu);",
            "\trcutree_online_cpu(cpu);",
            "",
            "\t/* Create workqueue for Tree SRCU and for expedited GPs. */",
            "\trcu_gp_wq = alloc_workqueue(\"rcu_gp\", WQ_MEM_RECLAIM, 0);",
            "\tWARN_ON(!rcu_gp_wq);",
            "\trcu_alloc_par_gp_wq();",
            "",
            "\t/* Fill in default value for rcutree.qovld boot parameter. */",
            "\t/* -After- the rcu_node ->lock fields are initialized! */",
            "\tif (qovld < 0)",
            "\t\tqovld_calc = DEFAULT_RCU_QOVLD_MULT * qhimark;",
            "\telse",
            "\t\tqovld_calc = qovld;",
            "",
            "\t// Kick-start in case any polled grace periods started early.",
            "\t(void)start_poll_synchronize_rcu_expedited();",
            "",
            "\trcu_test_sync_prims();",
            "}"
          ],
          "function_name": "rcu_dump_rcu_node_tree, kfree_rcu_batch_init, rcu_init",
          "description": "执行RCU节点树的调试输出，初始化内存回收批量机制，完成RCU子系统的整体初始化流程，包括资源分配、事件注册及核心组件启动。",
          "similarity": 0.5994814038276672
        },
        {
          "chunk_id": 24,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4444,
          "end_line": 4559,
          "content": [
            "static void __init",
            "rcu_boot_init_percpu_data(int cpu)",
            "{",
            "\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t/* Set up local state, ensuring consistent view of global state. */",
            "\trdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);",
            "\tINIT_WORK(&rdp->strict_work, strict_work_handler);",
            "\tWARN_ON_ONCE(ct->dynticks_nesting != 1);",
            "\tWARN_ON_ONCE(rcu_dynticks_in_eqs(rcu_dynticks_snap(cpu)));",
            "\trdp->barrier_seq_snap = rcu_state.barrier_sequence;",
            "\trdp->rcu_ofl_gp_seq = rcu_state.gp_seq;",
            "\trdp->rcu_ofl_gp_flags = RCU_GP_CLEANED;",
            "\trdp->rcu_onl_gp_seq = rcu_state.gp_seq;",
            "\trdp->rcu_onl_gp_flags = RCU_GP_CLEANED;",
            "\trdp->last_sched_clock = jiffies;",
            "\trdp->cpu = cpu;",
            "\trcu_boot_init_nocb_percpu_data(rdp);",
            "}",
            "int rcutree_prepare_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct context_tracking *ct = per_cpu_ptr(&context_tracking, cpu);",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\t/* Set up local state, ensuring consistent view of global state. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trdp->qlen_last_fqs_check = 0;",
            "\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\trdp->blimit = blimit;",
            "\tct->dynticks_nesting = 1;\t/* CPU not up, no tearing. */",
            "\traw_spin_unlock_rcu_node(rnp);\t\t/* irqs remain disabled. */",
            "",
            "\t/*",
            "\t * Only non-NOCB CPUs that didn't have early-boot callbacks need to be",
            "\t * (re-)initialized.",
            "\t */",
            "\tif (!rcu_segcblist_is_enabled(&rdp->cblist))",
            "\t\trcu_segcblist_init(&rdp->cblist);  /* Re-enable callbacks. */",
            "",
            "\t/*",
            "\t * Add CPU to leaf rcu_node pending-online bitmask.  Any needed",
            "\t * propagation up the rcu_node tree will happen at the beginning",
            "\t * of the next grace period.",
            "\t */",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_rcu_node(rnp);\t\t/* irqs already disabled. */",
            "\trdp->gp_seq = READ_ONCE(rnp->gp_seq);",
            "\trdp->gp_seq_needed = rdp->gp_seq;",
            "\trdp->cpu_no_qs.b.norm = true;",
            "\trdp->core_needs_qs = false;",
            "\trdp->rcu_iw_pending = false;",
            "\trdp->rcu_iw = IRQ_WORK_INIT_HARD(rcu_iw_handler);",
            "\trdp->rcu_iw_gp_seq = rdp->gp_seq - 1;",
            "\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuonl\"));",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "",
            "\trcu_preempt_deferred_qs_init(rdp);",
            "\trcu_spawn_one_boost_kthread(rnp);",
            "\trcu_spawn_cpu_nocb_kthread(cpu);",
            "\tWRITE_ONCE(rcu_state.n_online_cpus, rcu_state.n_online_cpus + 1);",
            "",
            "\treturn 0;",
            "}",
            "static void rcutree_affinity_setting(unsigned int cpu, int outgoing)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\trcu_boost_kthread_setaffinity(rdp->mynode, outgoing);",
            "}",
            "bool rcu_cpu_beenfullyonline(int cpu)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\treturn smp_load_acquire(&rdp->beenonline);",
            "}",
            "int rcutree_online_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trnp->ffmask |= rdp->grpmask;",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_INACTIVE)",
            "\t\treturn 0; /* Too early in boot for scheduler work. */",
            "\tsync_sched_exp_online_cleanup(cpu);",
            "\trcutree_affinity_setting(cpu, -1);",
            "",
            "\t// Stop-machine done, so allow nohz_full to disable tick.",
            "\ttick_dep_clear(TICK_DEP_BIT_RCU);",
            "\treturn 0;",
            "}",
            "int rcutree_offline_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trnp->ffmask &= ~rdp->grpmask;",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "",
            "\trcutree_affinity_setting(cpu, cpu);",
            "",
            "\t// nohz_full CPUs need the tick for stop-machine to work quickly",
            "\ttick_dep_set(TICK_DEP_BIT_RCU);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rcu_boot_init_percpu_data, rcutree_prepare_cpu, rcutree_affinity_setting, rcu_cpu_beenfullyonline, rcutree_online_cpu, rcutree_offline_cpu",
          "description": "初始化每个CPU的RCU私有数据结构，处理CPU上线/下线时的RCU状态同步，配置中断亲和性，更新全局在线CPU计数器",
          "similarity": 0.5859391093254089
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2012,
          "end_line": 2249,
          "content": [
            "static void",
            "rcu_report_qs_rdp(struct rcu_data *rdp)",
            "{",
            "\tunsigned long flags;",
            "\tunsigned long mask;",
            "\tbool needacc = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tWARN_ON_ONCE(rdp->cpu != smp_processor_id());",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\tif (rdp->cpu_no_qs.b.norm || rdp->gp_seq != rnp->gp_seq ||",
            "\t    rdp->gpwrap) {",
            "",
            "\t\t/*",
            "\t\t * The grace period in which this quiescent state was",
            "\t\t * recorded has ended, so don't report it upwards.",
            "\t\t * We will instead need a new quiescent state that lies",
            "\t\t * within the current grace period.",
            "\t\t */",
            "\t\trdp->cpu_no_qs.b.norm = true;\t/* need qs for new gp. */",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;",
            "\t}",
            "\tmask = rdp->grpmask;",
            "\trdp->core_needs_qs = false;",
            "\tif ((rnp->qsmask & mask) == 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t} else {",
            "\t\t/*",
            "\t\t * This GP can't end until cpu checks in, so all of our",
            "\t\t * callbacks can be processed during the next GP.",
            "\t\t *",
            "\t\t * NOCB kthreads have their own way to deal with that...",
            "\t\t */",
            "\t\tif (!rcu_rdp_is_offloaded(rdp)) {",
            "\t\t\t/*",
            "\t\t\t * The current GP has not yet ended, so it",
            "\t\t\t * should not be possible for rcu_accelerate_cbs()",
            "\t\t\t * to return true.  So complain, but don't awaken.",
            "\t\t\t */",
            "\t\t\tWARN_ON_ONCE(rcu_accelerate_cbs(rnp, rdp));",
            "\t\t} else if (!rcu_segcblist_completely_offloaded(&rdp->cblist)) {",
            "\t\t\t/*",
            "\t\t\t * ...but NOCB kthreads may miss or delay callbacks acceleration",
            "\t\t\t * if in the middle of a (de-)offloading process.",
            "\t\t\t */",
            "\t\t\tneedacc = true;",
            "\t\t}",
            "",
            "\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\t/* ^^^ Released rnp->lock */",
            "",
            "\t\tif (needacc) {",
            "\t\t\trcu_nocb_lock_irqsave(rdp, flags);",
            "\t\t\trcu_accelerate_cbs_unlocked(rnp, rdp);",
            "\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "\t\t}",
            "\t}",
            "}",
            "static void",
            "rcu_check_quiescent_state(struct rcu_data *rdp)",
            "{",
            "\t/* Check for grace-period ends and beginnings. */",
            "\tnote_gp_changes(rdp);",
            "",
            "\t/*",
            "\t * Does this CPU still need to do its part for current grace period?",
            "\t * If no, return and let the other CPUs do their part as well.",
            "\t */",
            "\tif (!rdp->core_needs_qs)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Was there a quiescent state since the beginning of the grace",
            "\t * period? If no, then exit and wait for the next call.",
            "\t */",
            "\tif (rdp->cpu_no_qs.b.norm)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Tell RCU we are done (but rcu_report_qs_rdp() will be the",
            "\t * judge of that).",
            "\t */",
            "\trcu_report_qs_rdp(rdp);",
            "}",
            "static bool rcu_do_batch_check_time(long count, long tlimit,",
            "\t\t\t\t    bool jlimit_check, unsigned long jlimit)",
            "{",
            "\t// Invoke local_clock() only once per 32 consecutive callbacks.",
            "\treturn unlikely(tlimit) &&",
            "\t       (!likely(count & 31) ||",
            "\t\t(IS_ENABLED(CONFIG_RCU_DOUBLE_CHECK_CB_TIME) &&",
            "\t\t jlimit_check && time_after(jiffies, jlimit))) &&",
            "\t       local_clock() >= tlimit;",
            "}",
            "static void rcu_do_batch(struct rcu_data *rdp)",
            "{",
            "\tlong bl;",
            "\tlong count = 0;",
            "\tint div;",
            "\tbool __maybe_unused empty;",
            "\tunsigned long flags;",
            "\tunsigned long jlimit;",
            "\tbool jlimit_check = false;",
            "\tlong pending;",
            "\tstruct rcu_cblist rcl = RCU_CBLIST_INITIALIZER(rcl);",
            "\tstruct rcu_head *rhp;",
            "\tlong tlimit = 0;",
            "",
            "\t/* If no callbacks are ready, just return. */",
            "\tif (!rcu_segcblist_ready_cbs(&rdp->cblist)) {",
            "\t\ttrace_rcu_batch_start(rcu_state.name,",
            "\t\t\t\t      rcu_segcblist_n_cbs(&rdp->cblist), 0);",
            "\t\ttrace_rcu_batch_end(rcu_state.name, 0,",
            "\t\t\t\t    !rcu_segcblist_empty(&rdp->cblist),",
            "\t\t\t\t    need_resched(), is_idle_task(current),",
            "\t\t\t\t    rcu_is_callbacks_kthread(rdp));",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Extract the list of ready callbacks, disabling IRQs to prevent",
            "\t * races with call_rcu() from interrupt handlers.  Leave the",
            "\t * callback counts, as rcu_barrier() needs to be conservative.",
            "\t */",
            "\trcu_nocb_lock_irqsave(rdp, flags);",
            "\tWARN_ON_ONCE(cpu_is_offline(smp_processor_id()));",
            "\tpending = rcu_segcblist_get_seglen(&rdp->cblist, RCU_DONE_TAIL);",
            "\tdiv = READ_ONCE(rcu_divisor);",
            "\tdiv = div < 0 ? 7 : div > sizeof(long) * 8 - 2 ? sizeof(long) * 8 - 2 : div;",
            "\tbl = max(rdp->blimit, pending >> div);",
            "\tif ((in_serving_softirq() || rdp->rcu_cpu_kthread_status == RCU_KTHREAD_RUNNING) &&",
            "\t    (IS_ENABLED(CONFIG_RCU_DOUBLE_CHECK_CB_TIME) || unlikely(bl > 100))) {",
            "\t\tconst long npj = NSEC_PER_SEC / HZ;",
            "\t\tlong rrn = READ_ONCE(rcu_resched_ns);",
            "",
            "\t\trrn = rrn < NSEC_PER_MSEC ? NSEC_PER_MSEC : rrn > NSEC_PER_SEC ? NSEC_PER_SEC : rrn;",
            "\t\ttlimit = local_clock() + rrn;",
            "\t\tjlimit = jiffies + (rrn + npj + 1) / npj;",
            "\t\tjlimit_check = true;",
            "\t}",
            "\ttrace_rcu_batch_start(rcu_state.name,",
            "\t\t\t      rcu_segcblist_n_cbs(&rdp->cblist), bl);",
            "\trcu_segcblist_extract_done_cbs(&rdp->cblist, &rcl);",
            "\tif (rcu_rdp_is_offloaded(rdp))",
            "\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCbDequeued\"));",
            "\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "",
            "\t/* Invoke callbacks. */",
            "\ttick_dep_set_task(current, TICK_DEP_BIT_RCU);",
            "\trhp = rcu_cblist_dequeue(&rcl);",
            "",
            "\tfor (; rhp; rhp = rcu_cblist_dequeue(&rcl)) {",
            "\t\trcu_callback_t f;",
            "",
            "\t\tcount++;",
            "\t\tdebug_rcu_head_unqueue(rhp);",
            "",
            "\t\trcu_lock_acquire(&rcu_callback_map);",
            "\t\ttrace_rcu_invoke_callback(rcu_state.name, rhp);",
            "",
            "\t\tf = rhp->func;",
            "\t\tdebug_rcu_head_callback(rhp);",
            "\t\tWRITE_ONCE(rhp->func, (rcu_callback_t)0L);",
            "\t\tf(rhp);",
            "",
            "\t\trcu_lock_release(&rcu_callback_map);",
            "",
            "\t\t/*",
            "\t\t * Stop only if limit reached and CPU has something to do.",
            "\t\t */",
            "\t\tif (in_serving_softirq()) {",
            "\t\t\tif (count >= bl && (need_resched() || !is_idle_task(current)))",
            "\t\t\t\tbreak;",
            "\t\t\t/*",
            "\t\t\t * Make sure we don't spend too much time here and deprive other",
            "\t\t\t * softirq vectors of CPU cycles.",
            "\t\t\t */",
            "\t\t\tif (rcu_do_batch_check_time(count, tlimit, jlimit_check, jlimit))",
            "\t\t\t\tbreak;",
            "\t\t} else {",
            "\t\t\t// In rcuc/rcuoc context, so no worries about",
            "\t\t\t// depriving other softirq vectors of CPU cycles.",
            "\t\t\tlocal_bh_enable();",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\t\tlocal_bh_disable();",
            "\t\t\t// But rcuc kthreads can delay quiescent-state",
            "\t\t\t// reporting, so check time limits for them.",
            "\t\t\tif (rdp->rcu_cpu_kthread_status == RCU_KTHREAD_RUNNING &&",
            "\t\t\t    rcu_do_batch_check_time(count, tlimit, jlimit_check, jlimit)) {",
            "\t\t\t\trdp->rcu_cpu_has_work = 1;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\trcu_nocb_lock_irqsave(rdp, flags);",
            "\trdp->n_cbs_invoked += count;",
            "\ttrace_rcu_batch_end(rcu_state.name, count, !!rcl.head, need_resched(),",
            "\t\t\t    is_idle_task(current), rcu_is_callbacks_kthread(rdp));",
            "",
            "\t/* Update counts and requeue any remaining callbacks. */",
            "\trcu_segcblist_insert_done_cbs(&rdp->cblist, &rcl);",
            "\trcu_segcblist_add_len(&rdp->cblist, -count);",
            "",
            "\t/* Reinstate batch limit if we have worked down the excess. */",
            "\tcount = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\tif (rdp->blimit >= DEFAULT_MAX_RCU_BLIMIT && count <= qlowmark)",
            "\t\trdp->blimit = blimit;",
            "",
            "\t/* Reset ->qlen_last_fqs_check trigger if enough CBs have drained. */",
            "\tif (count == 0 && rdp->qlen_last_fqs_check != 0) {",
            "\t\trdp->qlen_last_fqs_check = 0;",
            "\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t} else if (count < rdp->qlen_last_fqs_check - qhimark)",
            "\t\trdp->qlen_last_fqs_check = count;",
            "",
            "\t/*",
            "\t * The following usually indicates a double call_rcu().  To track",
            "\t * this down, try building with CONFIG_DEBUG_OBJECTS_RCU_HEAD=y.",
            "\t */",
            "\tempty = rcu_segcblist_empty(&rdp->cblist);",
            "\tWARN_ON_ONCE(count == 0 && !empty);",
            "\tWARN_ON_ONCE(!IS_ENABLED(CONFIG_RCU_NOCB_CPU) &&",
            "\t\t     count != 0 && empty);",
            "\tWARN_ON_ONCE(count == 0 && rcu_segcblist_n_segment_cbs(&rdp->cblist) != 0);",
            "\tWARN_ON_ONCE(!empty && rcu_segcblist_n_segment_cbs(&rdp->cblist) == 0);",
            "",
            "\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "",
            "\ttick_dep_clear_task(current, TICK_DEP_BIT_RCU);",
            "}"
          ],
          "function_name": "rcu_report_qs_rdp, rcu_check_quiescent_state, rcu_do_batch_check_time, rcu_do_batch",
          "description": "提供CPU级quiescent状态检测与回调处理机制，包含quiescent状态上报、回调批量处理及性能监控功能，支持动态调整批处理参数",
          "similarity": 0.5719659328460693
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 438,
          "end_line": 544,
          "content": [
            "static int param_set_first_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : j);",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "static int param_set_next_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : (j ?: 1));",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "unsigned long rcu_get_gp_seq(void)",
            "{",
            "\treturn READ_ONCE(rcu_state.gp_seq);",
            "}",
            "unsigned long rcu_exp_batches_completed(void)",
            "{",
            "\treturn rcu_state.expedited_sequence;",
            "}",
            "void rcutorture_get_gp_data(enum rcutorture_type test_type, int *flags,",
            "\t\t\t    unsigned long *gp_seq)",
            "{",
            "\tswitch (test_type) {",
            "\tcase RCU_FLAVOR:",
            "\t\t*flags = READ_ONCE(rcu_state.gp_flags);",
            "\t\t*gp_seq = rcu_seq_current(&rcu_state.gp_seq);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "}",
            "static void late_wakeup_func(struct irq_work *work)",
            "{",
            "}",
            "noinstr void rcu_irq_work_resched(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tif (IS_ENABLED(CONFIG_GENERIC_ENTRY) && !(current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tif (IS_ENABLED(CONFIG_KVM_XFER_TO_GUEST_WORK) && (current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tinstrumentation_begin();",
            "\tif (do_nocb_deferred_wakeup(rdp) && need_resched()) {",
            "\t\tirq_work_queue(this_cpu_ptr(&late_wakeup_work));",
            "\t}",
            "\tinstrumentation_end();",
            "}",
            "void rcu_irq_exit_check_preempt(void)",
            "{",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nesting() <= 0,",
            "\t\t\t \"RCU dynticks_nesting counter underflow/zero!\");",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nmi_nesting() !=",
            "\t\t\t DYNTICK_IRQ_NONIDLE,",
            "\t\t\t \"Bad RCU  dynticks_nmi_nesting counter\\n\");",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"RCU in extended quiescent state!\");",
            "}",
            "void __rcu_irq_enter_check_tick(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\t// If we're here from NMI there's nothing to do.",
            "\tif (in_nmi())",
            "\t\treturn;",
            "",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"Illegal rcu_irq_enter_check_tick() from extended quiescent state\");",
            "",
            "\tif (!tick_nohz_full_cpu(rdp->cpu) ||",
            "\t    !READ_ONCE(rdp->rcu_urgent_qs) ||",
            "\t    READ_ONCE(rdp->rcu_forced_tick)) {",
            "\t\t// RCU doesn't need nohz_full help from this CPU, or it is",
            "\t\t// already getting that help.",
            "\t\treturn;",
            "\t}",
            "",
            "\t// We get here only when not in an extended quiescent state and",
            "\t// from interrupts (as opposed to NMIs).  Therefore, (1) RCU is",
            "\t// already watching and (2) The fact that we are in an interrupt",
            "\t// handler and that the rcu_node lock is an irq-disabled lock",
            "\t// prevents self-deadlock.  So we can safely recheck under the lock.",
            "\t// Note that the nohz_full state currently cannot change.",
            "\traw_spin_lock_rcu_node(rdp->mynode);",
            "\tif (READ_ONCE(rdp->rcu_urgent_qs) && !rdp->rcu_forced_tick) {",
            "\t\t// A nohz_full CPU is in the kernel and RCU needs a",
            "\t\t// quiescent state.  Turn on the tick!",
            "\t\tWRITE_ONCE(rdp->rcu_forced_tick, true);",
            "\t\ttick_dep_set_cpu(rdp->cpu, TICK_DEP_BIT_RCU);",
            "\t}",
            "\traw_spin_unlock_rcu_node(rdp->mynode);",
            "}"
          ],
          "function_name": "param_set_first_fqs_jiffies, param_set_next_fqs_jiffies, rcu_get_gp_seq, rcu_exp_batches_completed, rcutorture_get_gp_data, late_wakeup_func, rcu_irq_work_resched, rcu_irq_exit_check_preempt, __rcu_irq_enter_check_tick",
          "description": "实现参数配置回调函数和中断上下文RCU工作重排逻辑，管理grace period序列号读取及nohz_full模式下的tick依赖关系。",
          "similarity": 0.5686286091804504
        },
        {
          "chunk_id": 27,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4895,
          "end_line": 5082,
          "content": [
            "static void __init rcu_init_one(void)",
            "{",
            "\tstatic const char * const buf[] = RCU_NODE_NAME_INIT;",
            "\tstatic const char * const fqs[] = RCU_FQS_NAME_INIT;",
            "\tstatic struct lock_class_key rcu_node_class[RCU_NUM_LVLS];",
            "\tstatic struct lock_class_key rcu_fqs_class[RCU_NUM_LVLS];",
            "",
            "\tint levelspread[RCU_NUM_LVLS];\t\t/* kids/node in each level. */",
            "\tint cpustride = 1;",
            "\tint i;",
            "\tint j;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tBUILD_BUG_ON(RCU_NUM_LVLS > ARRAY_SIZE(buf));  /* Fix buf[] init! */",
            "",
            "\t/* Silence gcc 4.8 false positive about array index out of range. */",
            "\tif (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)",
            "\t\tpanic(\"rcu_init_one: rcu_num_lvls out of range\");",
            "",
            "\t/* Initialize the level-tracking arrays. */",
            "",
            "\tfor (i = 1; i < rcu_num_lvls; i++)",
            "\t\trcu_state.level[i] =",
            "\t\t\trcu_state.level[i - 1] + num_rcu_lvl[i - 1];",
            "\trcu_init_levelspread(levelspread, num_rcu_lvl);",
            "",
            "\t/* Initialize the elements themselves, starting from the leaves. */",
            "",
            "\tfor (i = rcu_num_lvls - 1; i >= 0; i--) {",
            "\t\tcpustride *= levelspread[i];",
            "\t\trnp = rcu_state.level[i];",
            "\t\tfor (j = 0; j < num_rcu_lvl[i]; j++, rnp++) {",
            "\t\t\traw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));",
            "\t\t\tlockdep_set_class_and_name(&ACCESS_PRIVATE(rnp, lock),",
            "\t\t\t\t\t\t   &rcu_node_class[i], buf[i]);",
            "\t\t\traw_spin_lock_init(&rnp->fqslock);",
            "\t\t\tlockdep_set_class_and_name(&rnp->fqslock,",
            "\t\t\t\t\t\t   &rcu_fqs_class[i], fqs[i]);",
            "\t\t\trnp->gp_seq = rcu_state.gp_seq;",
            "\t\t\trnp->gp_seq_needed = rcu_state.gp_seq;",
            "\t\t\trnp->completedqs = rcu_state.gp_seq;",
            "\t\t\trnp->qsmask = 0;",
            "\t\t\trnp->qsmaskinit = 0;",
            "\t\t\trnp->grplo = j * cpustride;",
            "\t\t\trnp->grphi = (j + 1) * cpustride - 1;",
            "\t\t\tif (rnp->grphi >= nr_cpu_ids)",
            "\t\t\t\trnp->grphi = nr_cpu_ids - 1;",
            "\t\t\tif (i == 0) {",
            "\t\t\t\trnp->grpnum = 0;",
            "\t\t\t\trnp->grpmask = 0;",
            "\t\t\t\trnp->parent = NULL;",
            "\t\t\t} else {",
            "\t\t\t\trnp->grpnum = j % levelspread[i - 1];",
            "\t\t\t\trnp->grpmask = BIT(rnp->grpnum);",
            "\t\t\t\trnp->parent = rcu_state.level[i - 1] +",
            "\t\t\t\t\t      j / levelspread[i - 1];",
            "\t\t\t}",
            "\t\t\trnp->level = i;",
            "\t\t\tINIT_LIST_HEAD(&rnp->blkd_tasks);",
            "\t\t\trcu_init_one_nocb(rnp);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[0]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[1]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[2]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[3]);",
            "\t\t\tspin_lock_init(&rnp->exp_lock);",
            "\t\t\tmutex_init(&rnp->boost_kthread_mutex);",
            "\t\t\traw_spin_lock_init(&rnp->exp_poll_lock);",
            "\t\t\trnp->exp_seq_poll_rq = RCU_GET_STATE_COMPLETED;",
            "\t\t\tINIT_WORK(&rnp->exp_poll_wq, sync_rcu_do_polled_gp);",
            "\t\t}",
            "\t}",
            "",
            "\tinit_swait_queue_head(&rcu_state.gp_wq);",
            "\tinit_swait_queue_head(&rcu_state.expedited_wq);",
            "\trnp = rcu_first_leaf_node();",
            "\tfor_each_possible_cpu(i) {",
            "\t\twhile (i > rnp->grphi)",
            "\t\t\trnp++;",
            "\t\tper_cpu_ptr(&rcu_data, i)->mynode = rnp;",
            "\t\trcu_boot_init_percpu_data(i);",
            "\t}",
            "}",
            "static void __init sanitize_kthread_prio(void)",
            "{",
            "\tint kthread_prio_in = kthread_prio;",
            "",
            "\tif (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 2",
            "\t    && IS_BUILTIN(CONFIG_RCU_TORTURE_TEST))",
            "\t\tkthread_prio = 2;",
            "\telse if (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 1)",
            "\t\tkthread_prio = 1;",
            "\telse if (kthread_prio < 0)",
            "\t\tkthread_prio = 0;",
            "\telse if (kthread_prio > 99)",
            "\t\tkthread_prio = 99;",
            "",
            "\tif (kthread_prio != kthread_prio_in)",
            "\t\tpr_alert(\"%s: Limited prio to %d from %d\\n\",",
            "\t\t\t __func__, kthread_prio, kthread_prio_in);",
            "}",
            "void rcu_init_geometry(void)",
            "{",
            "\tulong d;",
            "\tint i;",
            "\tstatic unsigned long old_nr_cpu_ids;",
            "\tint rcu_capacity[RCU_NUM_LVLS];",
            "\tstatic bool initialized;",
            "",
            "\tif (initialized) {",
            "\t\t/*",
            "\t\t * Warn if setup_nr_cpu_ids() had not yet been invoked,",
            "\t\t * unless nr_cpus_ids == NR_CPUS, in which case who cares?",
            "\t\t */",
            "\t\tWARN_ON_ONCE(old_nr_cpu_ids != nr_cpu_ids);",
            "\t\treturn;",
            "\t}",
            "",
            "\told_nr_cpu_ids = nr_cpu_ids;",
            "\tinitialized = true;",
            "",
            "\t/*",
            "\t * Initialize any unspecified boot parameters.",
            "\t * The default values of jiffies_till_first_fqs and",
            "\t * jiffies_till_next_fqs are set to the RCU_JIFFIES_TILL_FORCE_QS",
            "\t * value, which is a function of HZ, then adding one for each",
            "\t * RCU_JIFFIES_FQS_DIV CPUs that might be on the system.",
            "\t */",
            "\td = RCU_JIFFIES_TILL_FORCE_QS + nr_cpu_ids / RCU_JIFFIES_FQS_DIV;",
            "\tif (jiffies_till_first_fqs == ULONG_MAX)",
            "\t\tjiffies_till_first_fqs = d;",
            "\tif (jiffies_till_next_fqs == ULONG_MAX)",
            "\t\tjiffies_till_next_fqs = d;",
            "\tadjust_jiffies_till_sched_qs();",
            "",
            "\t/* If the compile-time values are accurate, just leave. */",
            "\tif (rcu_fanout_leaf == RCU_FANOUT_LEAF &&",
            "\t    nr_cpu_ids == NR_CPUS)",
            "\t\treturn;",
            "\tpr_info(\"Adjusting geometry for rcu_fanout_leaf=%d, nr_cpu_ids=%u\\n\",",
            "\t\trcu_fanout_leaf, nr_cpu_ids);",
            "",
            "\t/*",
            "\t * The boot-time rcu_fanout_leaf parameter must be at least two",
            "\t * and cannot exceed the number of bits in the rcu_node masks.",
            "\t * Complain and fall back to the compile-time values if this",
            "\t * limit is exceeded.",
            "\t */",
            "\tif (rcu_fanout_leaf < 2 ||",
            "\t    rcu_fanout_leaf > sizeof(unsigned long) * 8) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Compute number of nodes that can be handled an rcu_node tree",
            "\t * with the given number of levels.",
            "\t */",
            "\trcu_capacity[0] = rcu_fanout_leaf;",
            "\tfor (i = 1; i < RCU_NUM_LVLS; i++)",
            "\t\trcu_capacity[i] = rcu_capacity[i - 1] * RCU_FANOUT;",
            "",
            "\t/*",
            "\t * The tree must be able to accommodate the configured number of CPUs.",
            "\t * If this limit is exceeded, fall back to the compile-time values.",
            "\t */",
            "\tif (nr_cpu_ids > rcu_capacity[RCU_NUM_LVLS - 1]) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Calculate the number of levels in the tree. */",
            "\tfor (i = 0; nr_cpu_ids > rcu_capacity[i]; i++) {",
            "\t}",
            "\trcu_num_lvls = i + 1;",
            "",
            "\t/* Calculate the number of rcu_nodes at each level of the tree. */",
            "\tfor (i = 0; i < rcu_num_lvls; i++) {",
            "\t\tint cap = rcu_capacity[(rcu_num_lvls - 1) - i];",
            "\t\tnum_rcu_lvl[i] = DIV_ROUND_UP(nr_cpu_ids, cap);",
            "\t}",
            "",
            "\t/* Calculate the total number of rcu_node structures. */",
            "\trcu_num_nodes = 0;",
            "\tfor (i = 0; i < rcu_num_lvls; i++)",
            "\t\trcu_num_nodes += num_rcu_lvl[i];",
            "}"
          ],
          "function_name": "rcu_init_one, sanitize_kthread_prio, rcu_init_geometry",
          "description": "构建多级RCU节点树结构，初始化各层级的锁类和节点属性，动态调整RCU树的几何形态以适配当前CPU数量和层级分布需求。",
          "similarity": 0.5686240196228027
        }
      ]
    },
    {
      "source_file": "kernel/irq/spurious.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:09:47\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\spurious.c`\n\n---\n\n# `irq/spurious.c` 技术文档\n\n## 1. 文件概述\n\n`irq/spurious.c` 是 Linux 内核中断子系统中的一个关键组件，负责处理**伪中断**（spurious interrupts）和**错误路由中断**（misrouted interrupts）。当硬件中断未被任何中断处理程序正确处理（返回 `IRQ_NONE`）时，内核会怀疑该中断是伪中断或被错误路由到当前 IRQ 线。该文件实现了检测、诊断和恢复机制，包括：\n\n- 统计未处理中断次数并判断是否为“卡住”的 IRQ\n- 在启用 `irqfixup` 选项时尝试在其他 IRQ 线上查找真正的中断源（中断错位恢复）\n- 定期轮询被禁用的伪中断线以尝试恢复共享中断设备\n- 提供诊断信息（如调用栈和注册的处理函数列表）\n\n该机制对于提高系统在硬件或固件存在缺陷时的鲁棒性至关重要。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `irqfixup`：模块参数，控制伪中断修复行为（0=禁用，1=仅对未处理中断尝试修复，2=对标记为 `IRQF_IRQPOLL` 的中断也尝试修复）\n- `poll_spurious_irq_timer`：定时器，用于定期轮询被标记为 `IRQS_SPURIOUS_DISABLED` 的中断线\n- `irq_poll_cpu`：记录当前正在执行轮询任务的 CPU ID\n- `irq_poll_active`：原子变量，防止多个 CPU 同时执行轮询\n\n### 主要函数\n- `irq_wait_for_poll(struct irq_desc *desc)`  \n  等待轮询操作完成，避免与轮询线程竞争。在 SMP 系统中自旋等待 `IRQS_POLL_INPROGRESS` 标志清除。\n  \n- `try_one_irq(struct irq_desc *desc, bool force)`  \n  尝试在指定中断描述符上执行中断处理。跳过 PER_CPU、嵌套线程和显式标记为轮询的中断。若中断被禁用，则仅在 `force=true` 时处理。支持共享中断的 `IRQS_PENDING` 重试机制。\n\n- `misrouted_irq(int irq)`  \n  遍历所有 IRQ（除 0 和当前 IRQ），调用 `try_one_irq()` 尝试在其他线上找到真正的中断源。用于中断错位恢复。\n\n- `poll_spurious_irqs(struct timer_list *unused)`  \n  定时器回调函数，轮询所有被标记为 `IRQS_SPURIOUS_DISABLED` 的中断线，强制尝试处理（`force=true`）。\n\n- `__report_bad_irq()` / `report_bad_irq()`  \n  打印伪中断诊断信息，包括中断号、错误返回值、调用栈及所有注册的处理函数。\n\n- `try_misrouted_irq()`  \n  根据 `irqfixup` 级别判断是否应尝试中断错位恢复。\n\n- `note_interrupt(struct irq_desc *desc, irqreturn_t action_ret)`  \n  中断处理结果分析入口。统计未处理中断，触发伪中断检测、诊断和恢复逻辑。\n\n## 3. 关键实现\n\n### 伪中断检测机制\n- 当 `note_interrupt()` 收到 `IRQ_NONE` 时，会递增中断描述符的未处理计数。\n- 若在 100,000 次中断中有 99,900 次未处理，则判定该 IRQ “卡住”，打印诊断信息并建议使用 `irqpoll` 启动参数。\n- 诊断信息包含所有注册的处理函数地址及符号名，便于调试。\n\n### 中断错位恢复（Misrouted IRQ Recovery）\n- 通过 `irqfixup` 内核参数启用（启动时传入 `irqfixup=1` 或 `2`）。\n- 当当前 IRQ 未被处理时，遍历其他所有 IRQ 线，尝试调用其处理函数（`try_one_irq()`）。\n- 仅适用于共享中断（`IRQF_SHARED`）且非 PER_CPU/嵌套线程类型。\n- 使用 `IRQS_POLL_INPROGRESS` 标志防止与正常中断处理冲突。\n\n### 轮询恢复机制\n- 被判定为伪中断的 IRQ 会被标记 `IRQS_SPURIOUS_DISABLED` 并禁用。\n- 启用 `irqfixup` 时，启动定时器 `poll_spurious_irq_timer`（间隔 100ms）。\n- 定时器回调 `poll_spurious_irqs()` 遍历所有 `IRQS_SPURIOUS_DISABLED` 的 IRQ，强制尝试处理（即使已禁用）。\n- 通过 `local_irq_disable/enable()` 保证轮询期间本地中断关闭，避免嵌套。\n\n### SMP 安全性\n- 使用 `irq_poll_active` 原子变量确保同一时间仅一个 CPU 执行轮询。\n- `irq_wait_for_poll()` 在 SMP 下自旋等待轮询完成，防止死锁。\n- 所有关键操作均在 `desc->lock` 保护下进行。\n\n### 线程化中断处理支持\n- 若主处理函数返回 `IRQ_WAKE_THREAD`，则延迟伪中断判断至下一次硬件中断，以等待线程处理结果。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/irq.h>`、`<linux/interrupt.h>`：中断核心数据结构和 API\n  - `<linux/timer.h>`：定时器支持（用于轮询）\n  - `\"internals.h\"`：中断子系统内部接口\n\n- **内核配置依赖**：\n  - `CONFIG_SMP`：影响 `irq_wait_for_poll()` 的实现\n  - `irqfixup` 模块参数：控制恢复行为\n\n- **与其他模块交互**：\n  - 被通用中断处理流程（如 `handle_irq_event()`）调用\n  - 与中断描述符管理（`irq_desc`）紧密集成\n  - 依赖内核打印和栈回溯机制（`dump_stack()`）\n\n## 5. 使用场景\n\n1. **硬件/固件缺陷处理**：  \n   当 BIOS 或硬件错误地将设备中断路由到错误的 IRQ 线时，通过 `irqfixup` 机制尝试在其他线上找到真正的处理函数。\n\n2. **共享中断线故障恢复**：  \n   在多个设备共享同一 IRQ 线时，若其中一个设备故障产生持续中断但无处理函数响应，内核可禁用该线并定期轮询，避免系统被中断风暴拖垮。\n\n3. **系统调试与诊断**：  \n   当出现“nobody cared”中断错误时，自动打印详细的处理函数列表和调用栈，帮助开发者定位问题设备或驱动。\n\n4. **高可用性系统**：  \n   在无法立即修复硬件问题的生产环境中，通过 `irqpoll` 启动参数启用轮询机制，维持系统基本运行。\n\n5. **传统 PC 兼容性**：  \n   特别处理 IRQ 0（系统定时器），因其在传统 PC 架构中的特殊地位，即使在 `irqfixup=2` 模式下也始终尝试恢复。",
      "similarity": 0.5694302916526794,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 272,
          "end_line": 432,
          "content": [
            "void note_interrupt(struct irq_desc *desc, irqreturn_t action_ret)",
            "{",
            "\tunsigned int irq;",
            "",
            "\tif (desc->istate & IRQS_POLL_INPROGRESS ||",
            "\t    irq_settings_is_polled(desc))",
            "\t\treturn;",
            "",
            "\tif (bad_action_ret(action_ret)) {",
            "\t\treport_bad_irq(desc, action_ret);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * We cannot call note_interrupt from the threaded handler",
            "\t * because we need to look at the compound of all handlers",
            "\t * (primary and threaded). Aside of that in the threaded",
            "\t * shared case we have no serialization against an incoming",
            "\t * hardware interrupt while we are dealing with a threaded",
            "\t * result.",
            "\t *",
            "\t * So in case a thread is woken, we just note the fact and",
            "\t * defer the analysis to the next hardware interrupt.",
            "\t *",
            "\t * The threaded handlers store whether they successfully",
            "\t * handled an interrupt and we check whether that number",
            "\t * changed versus the last invocation.",
            "\t *",
            "\t * We could handle all interrupts with the delayed by one",
            "\t * mechanism, but for the non forced threaded case we'd just",
            "\t * add pointless overhead to the straight hardirq interrupts",
            "\t * for the sake of a few lines less code.",
            "\t */",
            "\tif (action_ret & IRQ_WAKE_THREAD) {",
            "\t\t/*",
            "\t\t * There is a thread woken. Check whether one of the",
            "\t\t * shared primary handlers returned IRQ_HANDLED. If",
            "\t\t * not we defer the spurious detection to the next",
            "\t\t * interrupt.",
            "\t\t */",
            "\t\tif (action_ret == IRQ_WAKE_THREAD) {",
            "\t\t\tint handled;",
            "\t\t\t/*",
            "\t\t\t * We use bit 31 of thread_handled_last to",
            "\t\t\t * denote the deferred spurious detection",
            "\t\t\t * active. No locking necessary as",
            "\t\t\t * thread_handled_last is only accessed here",
            "\t\t\t * and we have the guarantee that hard",
            "\t\t\t * interrupts are not reentrant.",
            "\t\t\t */",
            "\t\t\tif (!(desc->threads_handled_last & SPURIOUS_DEFERRED)) {",
            "\t\t\t\tdesc->threads_handled_last |= SPURIOUS_DEFERRED;",
            "\t\t\t\treturn;",
            "\t\t\t}",
            "\t\t\t/*",
            "\t\t\t * Check whether one of the threaded handlers",
            "\t\t\t * returned IRQ_HANDLED since the last",
            "\t\t\t * interrupt happened.",
            "\t\t\t *",
            "\t\t\t * For simplicity we just set bit 31, as it is",
            "\t\t\t * set in threads_handled_last as well. So we",
            "\t\t\t * avoid extra masking. And we really do not",
            "\t\t\t * care about the high bits of the handled",
            "\t\t\t * count. We just care about the count being",
            "\t\t\t * different than the one we saw before.",
            "\t\t\t */",
            "\t\t\thandled = atomic_read(&desc->threads_handled);",
            "\t\t\thandled |= SPURIOUS_DEFERRED;",
            "\t\t\tif (handled != desc->threads_handled_last) {",
            "\t\t\t\taction_ret = IRQ_HANDLED;",
            "\t\t\t\t/*",
            "\t\t\t\t * Note: We keep the SPURIOUS_DEFERRED",
            "\t\t\t\t * bit set. We are handling the",
            "\t\t\t\t * previous invocation right now.",
            "\t\t\t\t * Keep it for the current one, so the",
            "\t\t\t\t * next hardware interrupt will",
            "\t\t\t\t * account for it.",
            "\t\t\t\t */",
            "\t\t\t\tdesc->threads_handled_last = handled;",
            "\t\t\t} else {",
            "\t\t\t\t/*",
            "\t\t\t\t * None of the threaded handlers felt",
            "\t\t\t\t * responsible for the last interrupt",
            "\t\t\t\t *",
            "\t\t\t\t * We keep the SPURIOUS_DEFERRED bit",
            "\t\t\t\t * set in threads_handled_last as we",
            "\t\t\t\t * need to account for the current",
            "\t\t\t\t * interrupt as well.",
            "\t\t\t\t */",
            "\t\t\t\taction_ret = IRQ_NONE;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * One of the primary handlers returned",
            "\t\t\t * IRQ_HANDLED. So we don't care about the",
            "\t\t\t * threaded handlers on the same line. Clear",
            "\t\t\t * the deferred detection bit.",
            "\t\t\t *",
            "\t\t\t * In theory we could/should check whether the",
            "\t\t\t * deferred bit is set and take the result of",
            "\t\t\t * the previous run into account here as",
            "\t\t\t * well. But it's really not worth the",
            "\t\t\t * trouble. If every other interrupt is",
            "\t\t\t * handled we never trigger the spurious",
            "\t\t\t * detector. And if this is just the one out",
            "\t\t\t * of 100k unhandled ones which is handled",
            "\t\t\t * then we merily delay the spurious detection",
            "\t\t\t * by one hard interrupt. Not a real problem.",
            "\t\t\t */",
            "\t\t\tdesc->threads_handled_last &= ~SPURIOUS_DEFERRED;",
            "\t\t}",
            "\t}",
            "",
            "\tif (unlikely(action_ret == IRQ_NONE)) {",
            "\t\t/*",
            "\t\t * If we are seeing only the odd spurious IRQ caused by",
            "\t\t * bus asynchronicity then don't eventually trigger an error,",
            "\t\t * otherwise the counter becomes a doomsday timer for otherwise",
            "\t\t * working systems",
            "\t\t */",
            "\t\tif (time_after(jiffies, desc->last_unhandled + HZ/10))",
            "\t\t\tdesc->irqs_unhandled = 1;",
            "\t\telse",
            "\t\t\tdesc->irqs_unhandled++;",
            "\t\tdesc->last_unhandled = jiffies;",
            "\t}",
            "",
            "\tirq = irq_desc_get_irq(desc);",
            "\tif (unlikely(try_misrouted_irq(irq, desc, action_ret))) {",
            "\t\tint ok = misrouted_irq(irq);",
            "\t\tif (action_ret == IRQ_NONE)",
            "\t\t\tdesc->irqs_unhandled -= ok;",
            "\t}",
            "",
            "\tif (likely(!desc->irqs_unhandled))",
            "\t\treturn;",
            "",
            "\t/* Now getting into unhandled irq detection */",
            "\tdesc->irq_count++;",
            "\tif (likely(desc->irq_count < 100000))",
            "\t\treturn;",
            "",
            "\tdesc->irq_count = 0;",
            "\tif (unlikely(desc->irqs_unhandled > 99900)) {",
            "\t\t/*",
            "\t\t * The interrupt is stuck",
            "\t\t */",
            "\t\t__report_bad_irq(desc, action_ret);",
            "\t\t/*",
            "\t\t * Now kill the IRQ",
            "\t\t */",
            "\t\tprintk(KERN_EMERG \"Disabling IRQ #%d\\n\", irq);",
            "\t\tdesc->istate |= IRQS_SPURIOUS_DISABLED;",
            "\t\tdesc->depth++;",
            "\t\tirq_disable(desc);",
            "",
            "\t\tmod_timer(&poll_spurious_irq_timer,",
            "\t\t\t  jiffies + POLL_SPURIOUS_IRQ_INTERVAL);",
            "\t}",
            "\tdesc->irqs_unhandled = 0;",
            "}"
          ],
          "function_name": "note_interrupt",
          "description": "note_interrupt记录中断事件，检测未处理中断并触发报告，处理线程唤醒场景下的特殊逻辑。",
          "similarity": 0.5390856266021729
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 144,
          "end_line": 256,
          "content": [
            "static void poll_spurious_irqs(struct timer_list *unused)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tint i;",
            "",
            "\tif (atomic_inc_return(&irq_poll_active) != 1)",
            "\t\tgoto out;",
            "\tirq_poll_cpu = smp_processor_id();",
            "",
            "\tfor_each_irq_desc(i, desc) {",
            "\t\tunsigned int state;",
            "",
            "\t\tif (!i)",
            "\t\t\t continue;",
            "",
            "\t\t/* Racy but it doesn't matter */",
            "\t\tstate = desc->istate;",
            "\t\tbarrier();",
            "\t\tif (!(state & IRQS_SPURIOUS_DISABLED))",
            "\t\t\tcontinue;",
            "",
            "\t\tlocal_irq_disable();",
            "\t\ttry_one_irq(desc, true);",
            "\t\tlocal_irq_enable();",
            "\t}",
            "out:",
            "\tatomic_dec(&irq_poll_active);",
            "\tmod_timer(&poll_spurious_irq_timer,",
            "\t\t  jiffies + POLL_SPURIOUS_IRQ_INTERVAL);",
            "}",
            "static inline int bad_action_ret(irqreturn_t action_ret)",
            "{",
            "\tunsigned int r = action_ret;",
            "",
            "\tif (likely(r <= (IRQ_HANDLED | IRQ_WAKE_THREAD)))",
            "\t\treturn 0;",
            "\treturn 1;",
            "}",
            "static void __report_bad_irq(struct irq_desc *desc, irqreturn_t action_ret)",
            "{",
            "\tunsigned int irq = irq_desc_get_irq(desc);",
            "\tstruct irqaction *action;",
            "\tunsigned long flags;",
            "",
            "\tif (bad_action_ret(action_ret)) {",
            "\t\tprintk(KERN_ERR \"irq event %d: bogus return value %x\\n\",",
            "\t\t\t\tirq, action_ret);",
            "\t} else {",
            "\t\tprintk(KERN_ERR \"irq %d: nobody cared (try booting with \"",
            "\t\t\t\t\"the \\\"irqpoll\\\" option)\\n\", irq);",
            "\t}",
            "\tdump_stack();",
            "\tprintk(KERN_ERR \"handlers:\\n\");",
            "",
            "\t/*",
            "\t * We need to take desc->lock here. note_interrupt() is called",
            "\t * w/o desc->lock held, but IRQ_PROGRESS set. We might race",
            "\t * with something else removing an action. It's ok to take",
            "\t * desc->lock here. See synchronize_irq().",
            "\t */",
            "\traw_spin_lock_irqsave(&desc->lock, flags);",
            "\tfor_each_action_of_desc(desc, action) {",
            "\t\tprintk(KERN_ERR \"[<%p>] %ps\", action->handler, action->handler);",
            "\t\tif (action->thread_fn)",
            "\t\t\tprintk(KERN_CONT \" threaded [<%p>] %ps\",",
            "\t\t\t\t\taction->thread_fn, action->thread_fn);",
            "\t\tprintk(KERN_CONT \"\\n\");",
            "\t}",
            "\traw_spin_unlock_irqrestore(&desc->lock, flags);",
            "}",
            "static void report_bad_irq(struct irq_desc *desc, irqreturn_t action_ret)",
            "{",
            "\tstatic int count = 100;",
            "",
            "\tif (count > 0) {",
            "\t\tcount--;",
            "\t\t__report_bad_irq(desc, action_ret);",
            "\t}",
            "}",
            "static inline int",
            "try_misrouted_irq(unsigned int irq, struct irq_desc *desc,",
            "\t\t  irqreturn_t action_ret)",
            "{",
            "\tstruct irqaction *action;",
            "",
            "\tif (!irqfixup)",
            "\t\treturn 0;",
            "",
            "\t/* We didn't actually handle the IRQ - see if it was misrouted? */",
            "\tif (action_ret == IRQ_NONE)",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * But for 'irqfixup == 2' we also do it for handled interrupts if",
            "\t * they are marked as IRQF_IRQPOLL (or for irq zero, which is the",
            "\t * traditional PC timer interrupt.. Legacy)",
            "\t */",
            "\tif (irqfixup < 2)",
            "\t\treturn 0;",
            "",
            "\tif (!irq)",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * Since we don't get the descriptor lock, \"action\" can",
            "\t * change under us.  We don't really care, but we don't",
            "\t * want to follow a NULL pointer. So tell the compiler to",
            "\t * just load it once by using a barrier.",
            "\t */",
            "\taction = desc->action;",
            "\tbarrier();",
            "\treturn action && (action->flags & IRQF_IRQPOLL);",
            "}"
          ],
          "function_name": "poll_spurious_irqs, bad_action_ret, __report_bad_irq, report_bad_irq, try_misrouted_irq",
          "description": "poll_spurious_irqs定时扫描中断描述符并处理疑似虚假中断，包含错误报告辅助函数和误路由检测逻辑。",
          "similarity": 0.5190109610557556
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 1,
          "end_line": 35,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 1992, 1998-2004 Linus Torvalds, Ingo Molnar",
            " *",
            " * This file contains spurious interrupt handling.",
            " */",
            "",
            "#include <linux/jiffies.h>",
            "#include <linux/irq.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/timer.h>",
            "",
            "#include \"internals.h\"",
            "",
            "static int irqfixup __read_mostly;",
            "",
            "#define POLL_SPURIOUS_IRQ_INTERVAL (HZ/10)",
            "static void poll_spurious_irqs(struct timer_list *unused);",
            "static DEFINE_TIMER(poll_spurious_irq_timer, poll_spurious_irqs);",
            "static int irq_poll_cpu;",
            "static atomic_t irq_poll_active;",
            "",
            "/*",
            " * We wait here for a poller to finish.",
            " *",
            " * If the poll runs on this CPU, then we yell loudly and return",
            " * false. That will leave the interrupt line disabled in the worst",
            " * case, but it should never happen.",
            " *",
            " * We wait until the poller is done and then recheck disabled and",
            " * action (about to be disabled). Only if it's still active, we return",
            " * true and let the handler run.",
            " */"
          ],
          "function_name": null,
          "description": "定义了处理虚假中断的相关变量和定时器，用于周期性地扫描和处理可能存在的虚假中断。",
          "similarity": 0.4979487359523773
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 436,
          "end_line": 467,
          "content": [
            "int noirqdebug_setup(char *str)",
            "{",
            "\tnoirqdebug = 1;",
            "\tprintk(KERN_INFO \"IRQ lockup detection disabled\\n\");",
            "",
            "\treturn 1;",
            "}",
            "static int __init irqfixup_setup(char *str)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {",
            "\t\tpr_warn(\"irqfixup boot option not supported with PREEMPT_RT\\n\");",
            "\t\treturn 1;",
            "\t}",
            "\tirqfixup = 1;",
            "\tprintk(KERN_WARNING \"Misrouted IRQ fixup support enabled.\\n\");",
            "\tprintk(KERN_WARNING \"This may impact system performance.\\n\");",
            "",
            "\treturn 1;",
            "}",
            "static int __init irqpoll_setup(char *str)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {",
            "\t\tpr_warn(\"irqpoll boot option not supported with PREEMPT_RT\\n\");",
            "\t\treturn 1;",
            "\t}",
            "\tirqfixup = 2;",
            "\tprintk(KERN_WARNING \"Misrouted IRQ fixup and polling support \"",
            "\t\t\t\t\"enabled\\n\");",
            "\tprintk(KERN_WARNING \"This may significantly impact system \"",
            "\t\t\t\t\"performance\\n\");",
            "\treturn 1;",
            "}"
          ],
          "function_name": "noirqdebug_setup, irqfixup_setup, irqpoll_setup",
          "description": "提供启动参数配置接口，用于启用或禁用irqfixup和irqpoll功能，并输出相应警告信息。",
          "similarity": 0.4922015070915222
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 36,
          "end_line": 136,
          "content": [
            "bool irq_wait_for_poll(struct irq_desc *desc)",
            "\t__must_hold(&desc->lock)",
            "{",
            "\tif (WARN_ONCE(irq_poll_cpu == smp_processor_id(),",
            "\t\t      \"irq poll in progress on cpu %d for irq %d\\n\",",
            "\t\t      smp_processor_id(), desc->irq_data.irq))",
            "\t\treturn false;",
            "",
            "#ifdef CONFIG_SMP",
            "\tdo {",
            "\t\traw_spin_unlock(&desc->lock);",
            "\t\twhile (irqd_irq_inprogress(&desc->irq_data))",
            "\t\t\tcpu_relax();",
            "\t\traw_spin_lock(&desc->lock);",
            "\t} while (irqd_irq_inprogress(&desc->irq_data));",
            "\t/* Might have been disabled in meantime */",
            "\treturn !irqd_irq_disabled(&desc->irq_data) && desc->action;",
            "#else",
            "\treturn false;",
            "#endif",
            "}",
            "static int try_one_irq(struct irq_desc *desc, bool force)",
            "{",
            "\tirqreturn_t ret = IRQ_NONE;",
            "\tstruct irqaction *action;",
            "",
            "\traw_spin_lock(&desc->lock);",
            "",
            "\t/*",
            "\t * PER_CPU, nested thread interrupts and interrupts explicitly",
            "\t * marked polled are excluded from polling.",
            "\t */",
            "\tif (irq_settings_is_per_cpu(desc) ||",
            "\t    irq_settings_is_nested_thread(desc) ||",
            "\t    irq_settings_is_polled(desc))",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * Do not poll disabled interrupts unless the spurious",
            "\t * disabled poller asks explicitly.",
            "\t */",
            "\tif (irqd_irq_disabled(&desc->irq_data) && !force)",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * All handlers must agree on IRQF_SHARED, so we test just the",
            "\t * first.",
            "\t */",
            "\taction = desc->action;",
            "\tif (!action || !(action->flags & IRQF_SHARED) ||",
            "\t    (action->flags & __IRQF_TIMER))",
            "\t\tgoto out;",
            "",
            "\t/* Already running on another processor */",
            "\tif (irqd_irq_inprogress(&desc->irq_data)) {",
            "\t\t/*",
            "\t\t * Already running: If it is shared get the other",
            "\t\t * CPU to go looking for our mystery interrupt too",
            "\t\t */",
            "\t\tdesc->istate |= IRQS_PENDING;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* Mark it poll in progress */",
            "\tdesc->istate |= IRQS_POLL_INPROGRESS;",
            "\tdo {",
            "\t\tif (handle_irq_event(desc) == IRQ_HANDLED)",
            "\t\t\tret = IRQ_HANDLED;",
            "\t\t/* Make sure that there is still a valid action */",
            "\t\taction = desc->action;",
            "\t} while ((desc->istate & IRQS_PENDING) && action);",
            "\tdesc->istate &= ~IRQS_POLL_INPROGRESS;",
            "out:",
            "\traw_spin_unlock(&desc->lock);",
            "\treturn ret == IRQ_HANDLED;",
            "}",
            "static int misrouted_irq(int irq)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tint i, ok = 0;",
            "",
            "\tif (atomic_inc_return(&irq_poll_active) != 1)",
            "\t\tgoto out;",
            "",
            "\tirq_poll_cpu = smp_processor_id();",
            "",
            "\tfor_each_irq_desc(i, desc) {",
            "\t\tif (!i)",
            "\t\t\t continue;",
            "",
            "\t\tif (i == irq)\t/* Already tried */",
            "\t\t\tcontinue;",
            "",
            "\t\tif (try_one_irq(desc, false))",
            "\t\t\tok = 1;",
            "\t}",
            "out:",
            "\tatomic_dec(&irq_poll_active);",
            "\t/* So the caller can adjust the irq error counts */",
            "\treturn ok;",
            "}"
          ],
          "function_name": "irq_wait_for_poll, try_one_irq, misrouted_irq",
          "description": "实现了irq_wait_for_poll用于等待轮询完成，try_one_irq尝试处理单个中断，misrouted_irq尝试修复误路由中断。",
          "similarity": 0.47202950716018677
        }
      ]
    }
  ]
}