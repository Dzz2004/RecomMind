{
  "query": "TLB如何加速地址转换",
  "timestamp": "2025-12-26 00:14:26",
  "retrieved_files": [
    {
      "source_file": "mm/hugetlb_vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:07:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `hugetlb_vmemmap.c`\n\n---\n\n# hugetlb_vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`hugetlb_vmemmap.c` 实现了 **HugeTLB Vmemmap Optimization (HVO)** 功能，旨在优化与 HugeTLB 页面关联的 `vmemmap`（虚拟内存映射）结构所占用的物理内存。在 Linux 内核中，每个物理页都对应一个 `struct page` 结构，这些结构通过 `vmemmap` 虚拟地址空间进行线性映射。当使用大页（如 2MB 或 1GB HugeTLB 页面）时，为整个大页区域分配完整的 `struct page` 数组会造成大量内存浪费（因为大部分尾部页面不会被单独使用）。  \n\n本文件通过 **重映射（remap）** 技术，将大页对应的多个 `vmemmap` 页面中的尾部页面重新映射到同一个物理页（通常是头部页面），从而显著减少 `vmemmap` 所需的物理内存开销，同时保持内核对 `struct page` 的访问语义正确。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct vmemmap_remap_walk`**  \n  用于遍历和操作 `vmemmap` 页表的上下文结构：\n  - `remap_pte`: 回调函数，处理每个 PTE 条目\n  - `nr_walked`: 已遍历的 PTE 数量\n  - `reuse_page`: 用于重用的物理页（通常是头部页）\n  - `reuse_addr`: `reuse_page` 对应的虚拟地址\n  - `vmemmap_pages`: 可释放的 `vmemmap` 页面链表\n  - `flags`: 控制 TLB 刷新行为的标志位（`VMEMMAP_SPLIT_NO_TLB_FLUSH`, `VMEMMAP_REMAP_NO_TLB_FLUSH`）\n\n### 主要函数\n\n- **`vmemmap_split_pmd()`**  \n  将一个 PMD（Page Middle Directory）级别的大页映射拆分为 PTE 级别的细粒度映射，为后续重映射做准备。\n\n- **`vmemmap_pmd_entry()`**  \n  `mm_walk` 回调函数，在遍历到 PMD 条目时触发，负责检查是否需要拆分 PMD 并执行拆分操作。\n\n- **`vmemmap_pte_entry()`**  \n  `mm_walk` 回调函数，在遍历到 PTE 条目时触发，用于识别重用页并执行重映射逻辑。\n\n- **`vmemmap_remap_range()`**  \n  驱动整个重映射流程，使用 `walk_page_range_novma()` 遍历指定的 `vmemmap` 虚拟地址范围。\n\n- **`vmemmap_remap_pte()`**  \n  实际执行 PTE 重映射的核心函数：将尾部 `vmemmap` 页面的 PTE 指向 `reuse_page`，并设置为只读以防止非法写入。\n\n- **`vmemmap_restore_pte()`**  \n  用于恢复原始映射（例如在取消优化时），从可释放列表中取出原页面并恢复其内容。\n\n- **`free_vmemmap_page()` / `free_vmemmap_page_list()`**  \n  安全释放 `vmemmap` 页面，区分来自 `memblock`（启动内存）或 `buddy` 分配器的页面。\n\n- **`reset_struct_pages()`**  \n  重置 `struct page` 结构的关键字段，避免因重映射导致的元数据不一致问题（如“corrupted mapping in tail page”警告）。\n\n## 3. 关键实现\n\n### 重映射机制\n1. **PMD 拆分**：首先将覆盖目标 `vmemmap` 范围的 PMD 大页映射拆分为 PTE 映射，确保可以独立修改每个 `struct page` 对应的物理页。\n2. **重用页识别**：在遍历 PTE 时，第一个遇到的页面被选为 `reuse_page`（即头部页）。\n3. **尾页重映射**：后续所有 PTE 条目均被修改为指向 `reuse_page`，并设置为只读（`PAGE_KERNEL_RO`），防止对尾部 `struct page` 的意外写入。\n4. **元数据清理**：由于尾部 `struct page` 与头部共享物理内存，其元数据（如 `flags`、`mapping`）可能无效。通过 `reset_struct_pages()` 复制有效数据到尾部结构，避免内核校验失败。\n5. **安全释放**：被替换的原始尾部页面被加入 `vmemmap_pages` 链表，可在后续安全释放。\n\n### 自托管检测\n在内存热插拔场景下（`memmap_on_memory`），`vmemmap` 结构可能位于待优化的内存区域内（即“自托管”）。代码通过检查首个 `vmemmap` 页面的 `PageVmemmapSelfHosted()` 标志，若为真则拒绝优化（返回 `-ENOTSUPP`），防止破坏关键元数据。\n\n### 内存屏障与 TLB 刷新\n- 使用 `smp_wmb()` 确保页面内容更新在 PTE 修改前完成。\n- 在 PMD 拆分和 PTE 重映射后，默认执行 `flush_tlb_kernel_range()` 刷新 TLB，可通过标志位跳过以提升性能。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `pgtable.h`、`pagewalk.h`、`mmdebug.h` 等核心 MM 头文件。\n- **架构相关代码**：使用 `asm/pgalloc.h` 和 `asm/tlbflush.h` 提供的页表分配与 TLB 刷新接口。\n- **HugeTLB 子系统**：与 `hugetlb.h` 协同工作，优化 HugeTLB 页面的 `vmemmap` 开销。\n- **内存热插拔**：处理 `memmap_on_memory` 场景下的自托管 `vmemmap` 限制。\n- **启动内存管理**：通过 `bootmem_info.h` 区分 `memblock` 与 `buddy` 分配的页面。\n\n## 5. 使用场景\n\n- **HugeTLB 内存优化**：在系统配置大量 HugeTLB 页面时，显著减少 `vmemmap` 的物理内存占用（例如，2MB HugeTLB 页面可节省约 87.5% 的 `vmemmap` 内存）。\n- **内存受限环境**：在内存资源紧张的系统（如容器、嵌入式设备）中降低内核内存开销。\n- **内存热插拔**：在支持 `memmap_on_memory` 的热插拔场景中，安全地优化新插入内存区域的 `vmemmap`。\n- **内核调试与维护**：通过只读保护捕获对尾部 `struct page` 的非法写入，提升系统稳定性。",
      "similarity": 0.5307711362838745,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 450,
          "end_line": 579,
          "content": [
            "static int __hugetlb_vmemmap_restore_folio(const struct hstate *h,",
            "\t\t\t\t\t   struct folio *folio, unsigned long flags)",
            "{",
            "\tint ret;",
            "\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;",
            "\tunsigned long vmemmap_reuse;",
            "",
            "\tVM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(folio), folio);",
            "\tVM_WARN_ON_ONCE_FOLIO(folio_ref_count(folio), folio);",
            "",
            "\tif (!folio_test_hugetlb_vmemmap_optimized(folio))",
            "\t\treturn 0;",
            "",
            "\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);",
            "\tvmemmap_reuse\t= vmemmap_start;",
            "\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;",
            "",
            "\t/*",
            "\t * The pages which the vmemmap virtual address range [@vmemmap_start,",
            "\t * @vmemmap_end) are mapped to are freed to the buddy allocator, and",
            "\t * the range is mapped to the page which @vmemmap_reuse is mapped to.",
            "\t * When a HugeTLB page is freed to the buddy allocator, previously",
            "\t * discarded vmemmap pages must be allocated and remapping.",
            "\t */",
            "\tret = vmemmap_remap_alloc(vmemmap_start, vmemmap_end, vmemmap_reuse, flags);",
            "\tif (!ret) {",
            "\t\tfolio_clear_hugetlb_vmemmap_optimized(folio);",
            "\t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "int hugetlb_vmemmap_restore_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\t/* avoid writes from page_ref_add_unless() while unfolding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\treturn __hugetlb_vmemmap_restore_folio(h, folio, 0);",
            "}",
            "long hugetlb_vmemmap_restore_folios(const struct hstate *h,",
            "\t\t\t\t\tstruct list_head *folio_list,",
            "\t\t\t\t\tstruct list_head *non_hvo_folios)",
            "{",
            "\tstruct folio *folio, *t_folio;",
            "\tlong restored = 0;",
            "\tlong ret = 0;",
            "",
            "\t/* avoid writes from page_ref_add_unless() while unfolding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\tlist_for_each_entry_safe(folio, t_folio, folio_list, lru) {",
            "\t\tif (folio_test_hugetlb_vmemmap_optimized(folio)) {",
            "\t\t\tret = __hugetlb_vmemmap_restore_folio(h, folio,",
            "\t\t\t\t\t\t\t      VMEMMAP_REMAP_NO_TLB_FLUSH);",
            "\t\t\tif (ret)",
            "\t\t\t\tbreak;",
            "\t\t\trestored++;",
            "\t\t}",
            "",
            "\t\t/* Add non-optimized folios to output list */",
            "\t\tlist_move(&folio->lru, non_hvo_folios);",
            "\t}",
            "",
            "\tif (restored)",
            "\t\tflush_tlb_all();",
            "\tif (!ret)",
            "\t\tret = restored;",
            "\treturn ret;",
            "}",
            "static bool vmemmap_should_optimize_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\tif (folio_test_hugetlb_vmemmap_optimized(folio))",
            "\t\treturn false;",
            "",
            "\tif (!READ_ONCE(vmemmap_optimize_enabled))",
            "\t\treturn false;",
            "",
            "\tif (!hugetlb_vmemmap_optimizable(h))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,",
            "\t\t\t\t\t    struct folio *folio,",
            "\t\t\t\t\t    struct list_head *vmemmap_pages,",
            "\t\t\t\t\t    unsigned long flags)",
            "{",
            "\tint ret = 0;",
            "\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;",
            "\tunsigned long vmemmap_reuse;",
            "",
            "\tVM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(folio), folio);",
            "\tVM_WARN_ON_ONCE_FOLIO(folio_ref_count(folio), folio);",
            "",
            "\tif (!vmemmap_should_optimize_folio(h, folio))",
            "\t\treturn ret;",
            "",
            "\tstatic_branch_inc(&hugetlb_optimize_vmemmap_key);",
            "\t/*",
            "\t * Very Subtle",
            "\t * If VMEMMAP_REMAP_NO_TLB_FLUSH is set, TLB flushing is not performed",
            "\t * immediately after remapping.  As a result, subsequent accesses",
            "\t * and modifications to struct pages associated with the hugetlb",
            "\t * page could be to the OLD struct pages.  Set the vmemmap optimized",
            "\t * flag here so that it is copied to the new head page.  This keeps",
            "\t * the old and new struct pages in sync.",
            "\t * If there is an error during optimization, we will immediately FLUSH",
            "\t * the TLB and clear the flag below.",
            "\t */",
            "\tfolio_set_hugetlb_vmemmap_optimized(folio);",
            "",
            "\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);",
            "\tvmemmap_reuse\t= vmemmap_start;",
            "\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;",
            "",
            "\t/*",
            "\t * Remap the vmemmap virtual address range [@vmemmap_start, @vmemmap_end)",
            "\t * to the page which @vmemmap_reuse is mapped to.  Add pages previously",
            "\t * mapping the range to vmemmap_pages list so that they can be freed by",
            "\t * the caller.",
            "\t */",
            "\tret = vmemmap_remap_free(vmemmap_start, vmemmap_end, vmemmap_reuse,",
            "\t\t\t\t vmemmap_pages, flags);",
            "\tif (ret) {",
            "\t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);",
            "\t\tfolio_clear_hugetlb_vmemmap_optimized(folio);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__hugetlb_vmemmap_restore_folio, hugetlb_vmemmap_restore_folio, hugetlb_vmemmap_restore_folios, vmemmap_should_optimize_folio, __hugetlb_vmemmap_optimize_folio",
          "description": "实现hugeTLB页的vmemmap优化控制逻辑，包含__hugetlb_vmemmap_restore_folio页回收恢复、hugetlb_vmemmap_restore_folios批量处理、vmemmap_should_optimize_folio优化判定及__hugetlb_vmemmap_optimize_folio优化执行路径。",
          "similarity": 0.49201130867004395
        },
        {
          "chunk_id": 5,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 619,
          "end_line": 710,
          "content": [
            "void hugetlb_vmemmap_optimize_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\tLIST_HEAD(vmemmap_pages);",
            "",
            "\t/* avoid writes from page_ref_add_unless() while folding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\t__hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages, 0);",
            "\tfree_vmemmap_page_list(&vmemmap_pages);",
            "}",
            "static int hugetlb_vmemmap_split_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;",
            "\tunsigned long vmemmap_reuse;",
            "",
            "\tif (!vmemmap_should_optimize_folio(h, folio))",
            "\t\treturn 0;",
            "",
            "\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);",
            "\tvmemmap_reuse\t= vmemmap_start;",
            "\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;",
            "",
            "\t/*",
            "\t * Split PMDs on the vmemmap virtual address range [@vmemmap_start,",
            "\t * @vmemmap_end]",
            "\t */",
            "\treturn vmemmap_remap_split(vmemmap_start, vmemmap_end, vmemmap_reuse);",
            "}",
            "void hugetlb_vmemmap_optimize_folios(struct hstate *h, struct list_head *folio_list)",
            "{",
            "\tstruct folio *folio;",
            "\tLIST_HEAD(vmemmap_pages);",
            "",
            "\tlist_for_each_entry(folio, folio_list, lru) {",
            "\t\tint ret = hugetlb_vmemmap_split_folio(h, folio);",
            "",
            "\t\t/*",
            "\t\t * Spliting the PMD requires allocating a page, thus lets fail",
            "\t\t * early once we encounter the first OOM. No point in retrying",
            "\t\t * as it can be dynamically done on remap with the memory",
            "\t\t * we get back from the vmemmap deduplication.",
            "\t\t */",
            "\t\tif (ret == -ENOMEM)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tflush_tlb_all();",
            "",
            "\t/* avoid writes from page_ref_add_unless() while folding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\tlist_for_each_entry(folio, folio_list, lru) {",
            "\t\tint ret;",
            "",
            "\t\tret = __hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages,",
            "\t\t\t\t\t\t       VMEMMAP_REMAP_NO_TLB_FLUSH);",
            "",
            "\t\t/*",
            "\t\t * Pages to be freed may have been accumulated.  If we",
            "\t\t * encounter an ENOMEM,  free what we have and try again.",
            "\t\t * This can occur in the case that both spliting fails",
            "\t\t * halfway and head page allocation also failed. In this",
            "\t\t * case __hugetlb_vmemmap_optimize_folio() would free memory",
            "\t\t * allowing more vmemmap remaps to occur.",
            "\t\t */",
            "\t\tif (ret == -ENOMEM && !list_empty(&vmemmap_pages)) {",
            "\t\t\tflush_tlb_all();",
            "\t\t\tfree_vmemmap_page_list(&vmemmap_pages);",
            "\t\t\tINIT_LIST_HEAD(&vmemmap_pages);",
            "\t\t\t__hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages,",
            "\t\t\t\t\t\t\t VMEMMAP_REMAP_NO_TLB_FLUSH);",
            "\t\t}",
            "\t}",
            "",
            "\tflush_tlb_all();",
            "\tfree_vmemmap_page_list(&vmemmap_pages);",
            "}",
            "static int __init hugetlb_vmemmap_init(void)",
            "{",
            "\tconst struct hstate *h;",
            "",
            "\t/* HUGETLB_VMEMMAP_RESERVE_SIZE should cover all used struct pages */",
            "\tBUILD_BUG_ON(__NR_USED_SUBPAGE > HUGETLB_VMEMMAP_RESERVE_PAGES);",
            "",
            "\tfor_each_hstate(h) {",
            "\t\tif (hugetlb_vmemmap_optimizable(h)) {",
            "\t\t\tregister_sysctl_init(\"vm\", hugetlb_vmemmap_sysctls);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "hugetlb_vmemmap_optimize_folio, hugetlb_vmemmap_split_folio, hugetlb_vmemmap_optimize_folios, hugetlb_vmemmap_init",
          "description": "该代码段实现了HugeTLB大页面的虚拟内存映射优化机制。  \n`hugetlb_vmemmap_split_folio`负责分割PMDs以优化VMEMMAP区域空间，`hugetlb_vmemmap_optimize_folios`遍历folio列表执行拆分与内存回收操作，`hugetlb_vmemmap_init`注册系统控制接口用于动态调整优化策略。由于`__hugetlb_vmemmap_optimize_folio`等关键函数未完整展示，需结合上下文进一步验证实现细节。",
          "similarity": 0.4915611147880554
        },
        {
          "chunk_id": 0,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 1,
          "end_line": 48,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * HugeTLB Vmemmap Optimization (HVO)",
            " *",
            " * Copyright (c) 2020, ByteDance. All rights reserved.",
            " *",
            " *     Author: Muchun Song <songmuchun@bytedance.com>",
            " *",
            " * See Documentation/mm/vmemmap_dedup.rst",
            " */",
            "#define pr_fmt(fmt)\t\"HugeTLB: \" fmt",
            "",
            "#include <linux/pgtable.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/bootmem_info.h>",
            "#include <linux/mmdebug.h>",
            "#include <linux/pagewalk.h>",
            "#include <asm/pgalloc.h>",
            "#include <asm/tlbflush.h>",
            "#include \"hugetlb_vmemmap.h\"",
            "",
            "/**",
            " * struct vmemmap_remap_walk - walk vmemmap page table",
            " *",
            " * @remap_pte:\t\tcalled for each lowest-level entry (PTE).",
            " * @nr_walked:\t\tthe number of walked pte.",
            " * @reuse_page:\t\tthe page which is reused for the tail vmemmap pages.",
            " * @reuse_addr:\t\tthe virtual address of the @reuse_page page.",
            " * @vmemmap_pages:\tthe list head of the vmemmap pages that can be freed",
            " *\t\t\tor is mapped from.",
            " * @flags:\t\tused to modify behavior in vmemmap page table walking",
            " *\t\t\toperations.",
            " */",
            "struct vmemmap_remap_walk {",
            "\tvoid\t\t\t(*remap_pte)(pte_t *pte, unsigned long addr,",
            "\t\t\t\t\t     struct vmemmap_remap_walk *walk);",
            "\tunsigned long\t\tnr_walked;",
            "\tstruct page\t\t*reuse_page;",
            "\tunsigned long\t\treuse_addr;",
            "\tstruct list_head\t*vmemmap_pages;",
            "",
            "/* Skip the TLB flush when we split the PMD */",
            "#define VMEMMAP_SPLIT_NO_TLB_FLUSH\tBIT(0)",
            "/* Skip the TLB flush when we remap the PTE */",
            "#define VMEMMAP_REMAP_NO_TLB_FLUSH\tBIT(1)",
            "\tunsigned long\t\tflags;",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义了用于HugeTLB vmemmap优化的结构体vmemmap_remap_walk，包含回调函数指针、统计计数器、复用页面信息及控制标志位，用于跟踪页表遍历过程中的重映射操作。",
          "similarity": 0.4769335389137268
        },
        {
          "chunk_id": 3,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 316,
          "end_line": 427,
          "content": [
            "static int vmemmap_remap_free(unsigned long start, unsigned long end,",
            "\t\t\t      unsigned long reuse,",
            "\t\t\t      struct list_head *vmemmap_pages,",
            "\t\t\t      unsigned long flags)",
            "{",
            "\tint ret;",
            "\tstruct vmemmap_remap_walk walk = {",
            "\t\t.remap_pte\t= vmemmap_remap_pte,",
            "\t\t.reuse_addr\t= reuse,",
            "\t\t.vmemmap_pages\t= vmemmap_pages,",
            "\t\t.flags\t\t= flags,",
            "\t};",
            "\tint nid = page_to_nid((struct page *)reuse);",
            "\tgfp_t gfp_mask = GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN;",
            "",
            "\t/*",
            "\t * Allocate a new head vmemmap page to avoid breaking a contiguous",
            "\t * block of struct page memory when freeing it back to page allocator",
            "\t * in free_vmemmap_page_list(). This will allow the likely contiguous",
            "\t * struct page backing memory to be kept contiguous and allowing for",
            "\t * more allocations of hugepages. Fallback to the currently",
            "\t * mapped head page in case should it fail to allocate.",
            "\t */",
            "\twalk.reuse_page = alloc_pages_node(nid, gfp_mask, 0);",
            "\tif (walk.reuse_page) {",
            "\t\tcopy_page(page_to_virt(walk.reuse_page),",
            "\t\t\t  (void *)walk.reuse_addr);",
            "\t\tlist_add(&walk.reuse_page->lru, vmemmap_pages);",
            "\t}",
            "",
            "\t/*",
            "\t * In order to make remapping routine most efficient for the huge pages,",
            "\t * the routine of vmemmap page table walking has the following rules",
            "\t * (see more details from the vmemmap_pte_range()):",
            "\t *",
            "\t * - The range [@start, @end) and the range [@reuse, @reuse + PAGE_SIZE)",
            "\t *   should be continuous.",
            "\t * - The @reuse address is part of the range [@reuse, @end) that we are",
            "\t *   walking which is passed to vmemmap_remap_range().",
            "\t * - The @reuse address is the first in the complete range.",
            "\t *",
            "\t * So we need to make sure that @start and @reuse meet the above rules.",
            "\t */",
            "\tBUG_ON(start - reuse != PAGE_SIZE);",
            "",
            "\tmmap_read_lock(&init_mm);",
            "\tret = vmemmap_remap_range(reuse, end, &walk);",
            "\tif (ret && walk.nr_walked) {",
            "\t\tend = reuse + walk.nr_walked * PAGE_SIZE;",
            "\t\t/*",
            "\t\t * vmemmap_pages contains pages from the previous",
            "\t\t * vmemmap_remap_range call which failed.  These",
            "\t\t * are pages which were removed from the vmemmap.",
            "\t\t * They will be restored in the following call.",
            "\t\t */",
            "\t\twalk = (struct vmemmap_remap_walk) {",
            "\t\t\t.remap_pte\t= vmemmap_restore_pte,",
            "\t\t\t.reuse_addr\t= reuse,",
            "\t\t\t.vmemmap_pages\t= vmemmap_pages,",
            "\t\t\t.flags\t\t= 0,",
            "\t\t};",
            "",
            "\t\tvmemmap_remap_range(reuse, end, &walk);",
            "\t}",
            "\tmmap_read_unlock(&init_mm);",
            "",
            "\treturn ret;",
            "}",
            "static int alloc_vmemmap_page_list(unsigned long start, unsigned long end,",
            "\t\t\t\t   struct list_head *list)",
            "{",
            "\tgfp_t gfp_mask = GFP_KERNEL | __GFP_RETRY_MAYFAIL;",
            "\tunsigned long nr_pages = (end - start) >> PAGE_SHIFT;",
            "\tint nid = page_to_nid((struct page *)start);",
            "\tstruct page *page, *next;",
            "",
            "\twhile (nr_pages--) {",
            "\t\tpage = alloc_pages_node(nid, gfp_mask, 0);",
            "\t\tif (!page)",
            "\t\t\tgoto out;",
            "\t\tlist_add(&page->lru, list);",
            "\t}",
            "",
            "\treturn 0;",
            "out:",
            "\tlist_for_each_entry_safe(page, next, list, lru)",
            "\t\t__free_page(page);",
            "\treturn -ENOMEM;",
            "}",
            "static int vmemmap_remap_alloc(unsigned long start, unsigned long end,",
            "\t\t\t       unsigned long reuse, unsigned long flags)",
            "{",
            "\tLIST_HEAD(vmemmap_pages);",
            "\tstruct vmemmap_remap_walk walk = {",
            "\t\t.remap_pte\t= vmemmap_restore_pte,",
            "\t\t.reuse_addr\t= reuse,",
            "\t\t.vmemmap_pages\t= &vmemmap_pages,",
            "\t\t.flags\t\t= flags,",
            "\t};",
            "",
            "\t/* See the comment in the vmemmap_remap_free(). */",
            "\tBUG_ON(start - reuse != PAGE_SIZE);",
            "",
            "\tif (alloc_vmemmap_page_list(start, end, &vmemmap_pages))",
            "\t\treturn -ENOMEM;",
            "",
            "\tmmap_read_lock(&init_mm);",
            "\tvmemmap_remap_range(reuse, end, &walk);",
            "\tmmap_read_unlock(&init_mm);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmemmap_remap_free, alloc_vmemmap_page_list, vmemmap_remap_alloc",
          "description": "包含vmemmap_remap_free释放大页时的映射回收逻辑、alloc_vmemmap_page_list页面列表分配函数及vmemmap_remap_alloc预分配页面的接口，实现高效的资源管理。",
          "similarity": 0.45590123534202576
        },
        {
          "chunk_id": 2,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 159,
          "end_line": 264,
          "content": [
            "static int vmemmap_remap_range(unsigned long start, unsigned long end,",
            "\t\t\t       struct vmemmap_remap_walk *walk)",
            "{",
            "\tint ret;",
            "",
            "\tVM_BUG_ON(!PAGE_ALIGNED(start | end));",
            "",
            "\tret = walk_page_range_novma(&init_mm, start, end, &vmemmap_remap_ops,",
            "\t\t\t\t    NULL, walk);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tif (walk->remap_pte && !(walk->flags & VMEMMAP_REMAP_NO_TLB_FLUSH))",
            "\t\tflush_tlb_kernel_range(start, end);",
            "",
            "\treturn 0;",
            "}",
            "static inline void free_vmemmap_page(struct page *page)",
            "{",
            "\tif (PageReserved(page))",
            "\t\tfree_bootmem_page(page);",
            "\telse",
            "\t\t__free_page(page);",
            "}",
            "static void free_vmemmap_page_list(struct list_head *list)",
            "{",
            "\tstruct page *page, *next;",
            "",
            "\tlist_for_each_entry_safe(page, next, list, lru)",
            "\t\tfree_vmemmap_page(page);",
            "}",
            "static void vmemmap_remap_pte(pte_t *pte, unsigned long addr,",
            "\t\t\t      struct vmemmap_remap_walk *walk)",
            "{",
            "\t/*",
            "\t * Remap the tail pages as read-only to catch illegal write operation",
            "\t * to the tail pages.",
            "\t */",
            "\tpgprot_t pgprot = PAGE_KERNEL_RO;",
            "\tstruct page *page = pte_page(ptep_get(pte));",
            "\tpte_t entry;",
            "",
            "\t/* Remapping the head page requires r/w */",
            "\tif (unlikely(addr == walk->reuse_addr)) {",
            "\t\tpgprot = PAGE_KERNEL;",
            "\t\tlist_del(&walk->reuse_page->lru);",
            "",
            "\t\t/*",
            "\t\t * Makes sure that preceding stores to the page contents from",
            "\t\t * vmemmap_remap_free() become visible before the set_pte_at()",
            "\t\t * write.",
            "\t\t */",
            "\t\tsmp_wmb();",
            "\t}",
            "",
            "\tentry = mk_pte(walk->reuse_page, pgprot);",
            "\tlist_add(&page->lru, walk->vmemmap_pages);",
            "\tset_pte_at(&init_mm, addr, pte, entry);",
            "}",
            "static inline void reset_struct_pages(struct page *start)",
            "{",
            "\tstruct page *from = start + NR_RESET_STRUCT_PAGE;",
            "",
            "\tBUILD_BUG_ON(NR_RESET_STRUCT_PAGE * 2 > PAGE_SIZE / sizeof(struct page));",
            "\tmemcpy(start, from, sizeof(*from) * NR_RESET_STRUCT_PAGE);",
            "}",
            "static void vmemmap_restore_pte(pte_t *pte, unsigned long addr,",
            "\t\t\t\tstruct vmemmap_remap_walk *walk)",
            "{",
            "\tpgprot_t pgprot = PAGE_KERNEL;",
            "\tstruct page *page;",
            "\tvoid *to;",
            "",
            "\tBUG_ON(pte_page(ptep_get(pte)) != walk->reuse_page);",
            "",
            "\tpage = list_first_entry(walk->vmemmap_pages, struct page, lru);",
            "\tlist_del(&page->lru);",
            "\tto = page_to_virt(page);",
            "\tcopy_page(to, (void *)walk->reuse_addr);",
            "\treset_struct_pages(to);",
            "",
            "\t/*",
            "\t * Makes sure that preceding stores to the page contents become visible",
            "\t * before the set_pte_at() write.",
            "\t */",
            "\tsmp_wmb();",
            "\tset_pte_at(&init_mm, addr, pte, mk_pte(page, pgprot));",
            "}",
            "static int vmemmap_remap_split(unsigned long start, unsigned long end,",
            "\t\t\t       unsigned long reuse)",
            "{",
            "\tint ret;",
            "\tstruct vmemmap_remap_walk walk = {",
            "\t\t.remap_pte\t= NULL,",
            "\t\t.flags\t\t= VMEMMAP_SPLIT_NO_TLB_FLUSH,",
            "\t};",
            "",
            "\t/* See the comment in the vmemmap_remap_free(). */",
            "\tBUG_ON(start - reuse != PAGE_SIZE);",
            "",
            "\tmmap_read_lock(&init_mm);",
            "\tret = vmemmap_remap_range(reuse, end, &walk);",
            "\tmmap_read_unlock(&init_mm);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "vmemmap_remap_range, free_vmemmap_page, free_vmemmap_page_list, vmemmap_remap_pte, reset_struct_pages, vmemmap_restore_pte, vmemmap_remap_split",
          "description": "提供vmemmap_remap_range范围重映射接口、free_vmemmap_page页面释放函数、reset_struct_pages结构页重置方法及remap_pte/restore_pte的映射更新逻辑，支持安全写保护和数据恢复。",
          "similarity": 0.4224509298801422
        }
      ]
    },
    {
      "source_file": "kernel/bpf/lpm_trie.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:16:29\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\lpm_trie.c`\n\n---\n\n# bpf/lpm_trie.c 技术文档\n\n## 1. 文件概述\n\n`bpf/lpm_trie.c` 实现了基于最长前缀匹配（Longest Prefix Match, LPM）算法的 BPF 映射（map）类型，主要用于高效匹配 IP 地址前缀。该数据结构特别适用于网络路由、访问控制列表（ACL）等需要根据 IP 地址前缀进行快速查找的场景。该实现支持 IPv4 和 IPv6 地址，并通过前缀树（Trie）结构实现高效的插入、查找和删除操作。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct lpm_trie_node`**  \n  表示前缀树中的一个节点，包含：\n  - `rcu`：用于 RCU（Read-Copy-Update）内存回收机制\n  - `child[2]`：指向左右子节点的指针（0 和 1 分支）\n  - `prefixlen`：该节点表示的前缀长度（位数）\n  - `flags`：节点标志，`LPM_TREE_NODE_FLAG_IM` 表示中间节点（无用户数据）\n  - `data[]`：变长数组，存储前缀数据（大端序）\n\n- **`struct lpm_trie`**  \n  表示整个 LPM Trie 映射，包含：\n  - `map`：嵌入的 `bpf_map` 基类\n  - `root`：指向根节点的 RCU 指针\n  - `n_entries`：当前条目数量\n  - `max_prefixlen`：最大前缀长度（如 32 表示 IPv4，128 表示 IPv6）\n  - `data_size`：前缀数据字节数（如 4 字节对应 IPv4）\n  - `lock`：自旋锁，用于同步更新操作\n\n### 主要函数\n\n- **`extract_bit()`**  \n  从字节数组中提取指定索引位置的比特位（大端序）。\n\n- **`longest_prefix_match()`**  \n  计算给定节点与查询键之间的最长匹配前缀长度，利用字对齐和字长优化（32/64 位）加速比较。\n\n- **`trie_lookup_elem()`**  \n  实现 BPF map 的查找接口，从根节点开始遍历 Trie，返回匹配路径中最深的非中间节点（即具有用户数据的节点）。\n\n## 3. 关键实现\n\n### 前缀树结构设计\n\n- 所有前缀数据以**大端序**存储，`data[0]` 为最高有效字节。\n- 节点分为两类：\n  - **真实节点**：包含用户通过 `bpf_map_update_elem()` 设置的值。\n  - **中间节点（Intermediate Node）**：仅用于路径分叉，无用户数据，由 `LPM_TREE_NODE_FLAG_IM` 标识。\n- 插入新前缀时，若路径上已有更短前缀，则沿比特路径向下查找；若需分叉但子节点已存在，则插入新的中间节点以扩展路径。\n\n### 高效前缀匹配算法\n\n- `longest_prefix_match()` 函数通过逐字（word-wise）比较加速前缀匹配：\n  - 在支持高效非对齐访问的 64 位平台上，优先使用 64 位比较。\n  - 否则依次尝试 32 位、16 位和 8 位比较。\n  - 利用 `fls()`/`fls64()`（Find Last Set）快速定位第一个不同比特位。\n\n### 并发与内存管理\n\n- **读操作（lookup）**：使用 RCU 机制，无需加锁，支持高并发。\n- **写操作（update/delete）**：通过自旋锁 `trie->lock` 保证原子性。\n- 节点内存通过 `kmalloc`/`vmalloc` 分配，并通过 RCU 回收。\n\n### 查找逻辑\n\n- 从根节点开始，逐层向下遍历。\n- 每次根据当前节点前缀长度之后的下一位比特值（0 或 1）选择子节点。\n- 记录遍历路径中最后一个**非中间节点**作为候选结果。\n- 当无法继续向下匹配时，返回该候选节点的值。\n\n## 4. 依赖关系\n\n- **内核头文件依赖**：\n  - `<linux/bpf.h>`：BPF 核心框架\n  - `<linux/rcupdate.h>`（通过 `<linux/bpf.h>` 间接包含）：RCU 机制\n  - `<linux/slab.h>` / `<linux/vmalloc.h>`：内存分配\n  - `<net/ipv6.h>`：IPv6 相关辅助函数（如 `ipv6_addr_prefix()`）\n  - `<linux/btf.h>`：BPF 类型格式（BTF）支持\n- **BPF 子系统**：作为 `BPF_MAP_TYPE_LPM_TRIE` 类型的后端实现，与 BPF verifier、map 操作接口紧密集成。\n- **网络子系统**：主要用于 IP 路由和策略匹配，但本身不直接依赖网络协议栈。\n\n## 5. 使用场景\n\n- **eBPF 程序中的 IP 路由查找**：例如在 XDP 或 TC 程序中根据目标 IP 查找下一跳或策略。\n- **访问控制**：匹配 IP 前缀以决定是否允许流量（如防火墙规则）。\n- **负载均衡**：根据客户端 IP 前缀进行会话保持或区域路由。\n- **网络监控**：按 IP 段聚合流量统计。\n- **用户空间工具**：通过 `bpf()` 系统调用管理 LPM Trie 映射，实现动态策略更新。",
      "similarity": 0.5290656685829163,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/lpm_trie.c",
          "start_line": 152,
          "end_line": 359,
          "content": [
            "static inline int extract_bit(const u8 *data, size_t index)",
            "{",
            "\treturn !!(data[index / 8] & (1 << (7 - (index % 8))));",
            "}",
            "static size_t longest_prefix_match(const struct lpm_trie *trie,",
            "\t\t\t\t   const struct lpm_trie_node *node,",
            "\t\t\t\t   const struct bpf_lpm_trie_key_u8 *key)",
            "{",
            "\tu32 limit = min(node->prefixlen, key->prefixlen);",
            "\tu32 prefixlen = 0, i = 0;",
            "",
            "\tBUILD_BUG_ON(offsetof(struct lpm_trie_node, data) % sizeof(u32));",
            "\tBUILD_BUG_ON(offsetof(struct bpf_lpm_trie_key_u8, data) % sizeof(u32));",
            "",
            "#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && defined(CONFIG_64BIT)",
            "",
            "\t/* data_size >= 16 has very small probability.",
            "\t * We do not use a loop for optimal code generation.",
            "\t */",
            "\tif (trie->data_size >= 8) {",
            "\t\tu64 diff = be64_to_cpu(*(__be64 *)node->data ^",
            "\t\t\t\t       *(__be64 *)key->data);",
            "",
            "\t\tprefixlen = 64 - fls64(diff);",
            "\t\tif (prefixlen >= limit)",
            "\t\t\treturn limit;",
            "\t\tif (diff)",
            "\t\t\treturn prefixlen;",
            "\t\ti = 8;",
            "\t}",
            "#endif",
            "",
            "\twhile (trie->data_size >= i + 4) {",
            "\t\tu32 diff = be32_to_cpu(*(__be32 *)&node->data[i] ^",
            "\t\t\t\t       *(__be32 *)&key->data[i]);",
            "",
            "\t\tprefixlen += 32 - fls(diff);",
            "\t\tif (prefixlen >= limit)",
            "\t\t\treturn limit;",
            "\t\tif (diff)",
            "\t\t\treturn prefixlen;",
            "\t\ti += 4;",
            "\t}",
            "",
            "\tif (trie->data_size >= i + 2) {",
            "\t\tu16 diff = be16_to_cpu(*(__be16 *)&node->data[i] ^",
            "\t\t\t\t       *(__be16 *)&key->data[i]);",
            "",
            "\t\tprefixlen += 16 - fls(diff);",
            "\t\tif (prefixlen >= limit)",
            "\t\t\treturn limit;",
            "\t\tif (diff)",
            "\t\t\treturn prefixlen;",
            "\t\ti += 2;",
            "\t}",
            "",
            "\tif (trie->data_size >= i + 1) {",
            "\t\tprefixlen += 8 - fls(node->data[i] ^ key->data[i]);",
            "",
            "\t\tif (prefixlen >= limit)",
            "\t\t\treturn limit;",
            "\t}",
            "",
            "\treturn prefixlen;",
            "}",
            "static int trie_check_add_elem(struct lpm_trie *trie, u64 flags)",
            "{",
            "\tif (flags == BPF_EXIST)",
            "\t\treturn -ENOENT;",
            "\tif (trie->n_entries == trie->map.max_entries)",
            "\t\treturn -ENOSPC;",
            "\ttrie->n_entries++;",
            "\treturn 0;",
            "}",
            "static long trie_update_elem(struct bpf_map *map,",
            "\t\t\t     void *_key, void *value, u64 flags)",
            "{",
            "\tstruct lpm_trie *trie = container_of(map, struct lpm_trie, map);",
            "\tstruct lpm_trie_node *node, *im_node, *new_node = NULL;",
            "\tstruct lpm_trie_node *free_node = NULL;",
            "\tstruct lpm_trie_node __rcu **slot;",
            "\tstruct bpf_lpm_trie_key_u8 *key = _key;",
            "\tunsigned long irq_flags;",
            "\tunsigned int next_bit;",
            "\tsize_t matchlen = 0;",
            "\tint ret = 0;",
            "",
            "\tif (unlikely(flags > BPF_EXIST))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (key->prefixlen > trie->max_prefixlen)",
            "\t\treturn -EINVAL;",
            "",
            "\tspin_lock_irqsave(&trie->lock, irq_flags);",
            "",
            "\t/* Allocate and fill a new node */",
            "\tnew_node = lpm_trie_node_alloc(trie, value);",
            "\tif (!new_node) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tnew_node->prefixlen = key->prefixlen;",
            "\tRCU_INIT_POINTER(new_node->child[0], NULL);",
            "\tRCU_INIT_POINTER(new_node->child[1], NULL);",
            "\tmemcpy(new_node->data, key->data, trie->data_size);",
            "",
            "\t/* Now find a slot to attach the new node. To do that, walk the tree",
            "\t * from the root and match as many bits as possible for each node until",
            "\t * we either find an empty slot or a slot that needs to be replaced by",
            "\t * an intermediate node.",
            "\t */",
            "\tslot = &trie->root;",
            "",
            "\twhile ((node = rcu_dereference_protected(*slot,",
            "\t\t\t\t\tlockdep_is_held(&trie->lock)))) {",
            "\t\tmatchlen = longest_prefix_match(trie, node, key);",
            "",
            "\t\tif (node->prefixlen != matchlen ||",
            "\t\t    node->prefixlen == key->prefixlen ||",
            "\t\t    node->prefixlen == trie->max_prefixlen)",
            "\t\t\tbreak;",
            "",
            "\t\tnext_bit = extract_bit(key->data, node->prefixlen);",
            "\t\tslot = &node->child[next_bit];",
            "\t}",
            "",
            "\t/* If the slot is empty (a free child pointer or an empty root),",
            "\t * simply assign the @new_node to that slot and be done.",
            "\t */",
            "\tif (!node) {",
            "\t\tret = trie_check_add_elem(trie, flags);",
            "\t\tif (ret)",
            "\t\t\tgoto out;",
            "",
            "\t\trcu_assign_pointer(*slot, new_node);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* If the slot we picked already exists, replace it with @new_node",
            "\t * which already has the correct data array set.",
            "\t */",
            "\tif (node->prefixlen == matchlen) {",
            "\t\tif (!(node->flags & LPM_TREE_NODE_FLAG_IM)) {",
            "\t\t\tif (flags == BPF_NOEXIST) {",
            "\t\t\t\tret = -EEXIST;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tret = trie_check_add_elem(trie, flags);",
            "\t\t\tif (ret)",
            "\t\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tnew_node->child[0] = node->child[0];",
            "\t\tnew_node->child[1] = node->child[1];",
            "",
            "\t\trcu_assign_pointer(*slot, new_node);",
            "\t\tfree_node = node;",
            "",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tret = trie_check_add_elem(trie, flags);",
            "\tif (ret)",
            "\t\tgoto out;",
            "",
            "\t/* If the new node matches the prefix completely, it must be inserted",
            "\t * as an ancestor. Simply insert it between @node and *@slot.",
            "\t */",
            "\tif (matchlen == key->prefixlen) {",
            "\t\tnext_bit = extract_bit(node->data, matchlen);",
            "\t\trcu_assign_pointer(new_node->child[next_bit], node);",
            "\t\trcu_assign_pointer(*slot, new_node);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tim_node = lpm_trie_node_alloc(trie, NULL);",
            "\tif (!im_node) {",
            "\t\ttrie->n_entries--;",
            "\t\tret = -ENOMEM;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tim_node->prefixlen = matchlen;",
            "\tim_node->flags |= LPM_TREE_NODE_FLAG_IM;",
            "\tmemcpy(im_node->data, node->data, trie->data_size);",
            "",
            "\t/* Now determine which child to install in which slot */",
            "\tif (extract_bit(key->data, matchlen)) {",
            "\t\trcu_assign_pointer(im_node->child[0], node);",
            "\t\trcu_assign_pointer(im_node->child[1], new_node);",
            "\t} else {",
            "\t\trcu_assign_pointer(im_node->child[0], new_node);",
            "\t\trcu_assign_pointer(im_node->child[1], node);",
            "\t}",
            "",
            "\t/* Finally, assign the intermediate node to the determined slot */",
            "\trcu_assign_pointer(*slot, im_node);",
            "",
            "out:",
            "\tif (ret)",
            "\t\tkfree(new_node);",
            "\tspin_unlock_irqrestore(&trie->lock, irq_flags);",
            "\tkfree_rcu(free_node, rcu);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "extract_bit, longest_prefix_match, trie_check_add_elem, trie_update_elem",
          "description": "实现了最长前缀匹配核心逻辑（longest_prefix_match），提取位操作（extract_bit），以及添加/更新元素的处理（trie_check_add_elem, trie_update_elem）。其中trie_update_elem负责根据匹配结果决定插入新节点、替换现有节点或创建中间节点以维持树结构。",
          "similarity": 0.5247877240180969
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/lpm_trie.c",
          "start_line": 1,
          "end_line": 151,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Longest prefix match list implementation",
            " *",
            " * Copyright (c) 2016,2017 Daniel Mack",
            " * Copyright (c) 2016 David Herrmann",
            " */",
            "",
            "#include <linux/bpf.h>",
            "#include <linux/btf.h>",
            "#include <linux/err.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <net/ipv6.h>",
            "#include <uapi/linux/btf.h>",
            "#include <linux/btf_ids.h>",
            "",
            "/* Intermediate node */",
            "#define LPM_TREE_NODE_FLAG_IM BIT(0)",
            "",
            "struct lpm_trie_node;",
            "",
            "struct lpm_trie_node {",
            "\tstruct rcu_head rcu;",
            "\tstruct lpm_trie_node __rcu\t*child[2];",
            "\tu32\t\t\t\tprefixlen;",
            "\tu32\t\t\t\tflags;",
            "\tu8\t\t\t\tdata[];",
            "};",
            "",
            "struct lpm_trie {",
            "\tstruct bpf_map\t\t\tmap;",
            "\tstruct lpm_trie_node __rcu\t*root;",
            "\tsize_t\t\t\t\tn_entries;",
            "\tsize_t\t\t\t\tmax_prefixlen;",
            "\tsize_t\t\t\t\tdata_size;",
            "\tspinlock_t\t\t\tlock;",
            "};",
            "",
            "/* This trie implements a longest prefix match algorithm that can be used to",
            " * match IP addresses to a stored set of ranges.",
            " *",
            " * Data stored in @data of struct bpf_lpm_key and struct lpm_trie_node is",
            " * interpreted as big endian, so data[0] stores the most significant byte.",
            " *",
            " * Match ranges are internally stored in instances of struct lpm_trie_node",
            " * which each contain their prefix length as well as two pointers that may",
            " * lead to more nodes containing more specific matches. Each node also stores",
            " * a value that is defined by and returned to userspace via the update_elem",
            " * and lookup functions.",
            " *",
            " * For instance, let's start with a trie that was created with a prefix length",
            " * of 32, so it can be used for IPv4 addresses, and one single element that",
            " * matches 192.168.0.0/16. The data array would hence contain",
            " * [0xc0, 0xa8, 0x00, 0x00] in big-endian notation. This documentation will",
            " * stick to IP-address notation for readability though.",
            " *",
            " * As the trie is empty initially, the new node (1) will be places as root",
            " * node, denoted as (R) in the example below. As there are no other node, both",
            " * child pointers are %NULL.",
            " *",
            " *              +----------------+",
            " *              |       (1)  (R) |",
            " *              | 192.168.0.0/16 |",
            " *              |    value: 1    |",
            " *              |   [0]    [1]   |",
            " *              +----------------+",
            " *",
            " * Next, let's add a new node (2) matching 192.168.0.0/24. As there is already",
            " * a node with the same data and a smaller prefix (ie, a less specific one),",
            " * node (2) will become a child of (1). In child index depends on the next bit",
            " * that is outside of what (1) matches, and that bit is 0, so (2) will be",
            " * child[0] of (1):",
            " *",
            " *              +----------------+",
            " *              |       (1)  (R) |",
            " *              | 192.168.0.0/16 |",
            " *              |    value: 1    |",
            " *              |   [0]    [1]   |",
            " *              +----------------+",
            " *                   |",
            " *    +----------------+",
            " *    |       (2)      |",
            " *    | 192.168.0.0/24 |",
            " *    |    value: 2    |",
            " *    |   [0]    [1]   |",
            " *    +----------------+",
            " *",
            " * The child[1] slot of (1) could be filled with another node which has bit #17",
            " * (the next bit after the ones that (1) matches on) set to 1. For instance,",
            " * 192.168.128.0/24:",
            " *",
            " *              +----------------+",
            " *              |       (1)  (R) |",
            " *              | 192.168.0.0/16 |",
            " *              |    value: 1    |",
            " *              |   [0]    [1]   |",
            " *              +----------------+",
            " *                   |      |",
            " *    +----------------+  +------------------+",
            " *    |       (2)      |  |        (3)       |",
            " *    | 192.168.0.0/24 |  | 192.168.128.0/24 |",
            " *    |    value: 2    |  |     value: 3     |",
            " *    |   [0]    [1]   |  |    [0]    [1]    |",
            " *    +----------------+  +------------------+",
            " *",
            " * Let's add another node (4) to the game for 192.168.1.0/24. In order to place",
            " * it, node (1) is looked at first, and because (4) of the semantics laid out",
            " * above (bit #17 is 0), it would normally be attached to (1) as child[0].",
            " * However, that slot is already allocated, so a new node is needed in between.",
            " * That node does not have a value attached to it and it will never be",
            " * returned to users as result of a lookup. It is only there to differentiate",
            " * the traversal further. It will get a prefix as wide as necessary to",
            " * distinguish its two children:",
            " *",
            " *                      +----------------+",
            " *                      |       (1)  (R) |",
            " *                      | 192.168.0.0/16 |",
            " *                      |    value: 1    |",
            " *                      |   [0]    [1]   |",
            " *                      +----------------+",
            " *                           |      |",
            " *            +----------------+  +------------------+",
            " *            |       (4)  (I) |  |        (3)       |",
            " *            | 192.168.0.0/23 |  | 192.168.128.0/24 |",
            " *            |    value: ---  |  |     value: 3     |",
            " *            |   [0]    [1]   |  |    [0]    [1]    |",
            " *            +----------------+  +------------------+",
            " *                 |      |",
            " *  +----------------+  +----------------+",
            " *  |       (2)      |  |       (5)      |",
            " *  | 192.168.0.0/24 |  | 192.168.1.0/24 |",
            " *  |    value: 2    |  |     value: 5   |",
            " *  |   [0]    [1]   |  |   [0]    [1]   |",
            " *  +----------------+  +----------------+",
            " *",
            " * 192.168.1.1/32 would be a child of (5) etc.",
            " *",
            " * An intermediate node will be turned into a 'real' node on demand. In the",
            " * example above, (4) would be re-used if 192.168.0.0/23 is added to the trie.",
            " *",
            " * A fully populated trie would have a height of 32 nodes, as the trie was",
            " * created with a prefix length of 32.",
            " *",
            " * The lookup starts at the root node. If the current node matches and if there",
            " * is a child that can be used to become more specific, the trie is traversed",
            " * downwards. The last node in the traversal that is a non-intermediate one is",
            " * returned.",
            " */",
            ""
          ],
          "function_name": null,
          "description": "定义了LPM Trie的数据结构，包含节点（带子节点指针、前缀长度和标志位）和树结构（根节点、最大前缀长度、锁等）。该结构用于实现最长前缀匹配算法，支持IP地址范围查找，节点数据按大端存储，中间节点用于区分不同分支。",
          "similarity": 0.4950280785560608
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/lpm_trie.c",
          "start_line": 633,
          "end_line": 738,
          "content": [
            "static int trie_get_next_key(struct bpf_map *map, void *_key, void *_next_key)",
            "{",
            "\tstruct lpm_trie_node *node, *next_node = NULL, *parent, *search_root;",
            "\tstruct lpm_trie *trie = container_of(map, struct lpm_trie, map);",
            "\tstruct bpf_lpm_trie_key_u8 *key = _key, *next_key = _next_key;",
            "\tstruct lpm_trie_node **node_stack = NULL;",
            "\tint err = 0, stack_ptr = -1;",
            "\tunsigned int next_bit;",
            "\tsize_t matchlen = 0;",
            "",
            "\t/* The get_next_key follows postorder. For the 4 node example in",
            "\t * the top of this file, the trie_get_next_key() returns the following",
            "\t * one after another:",
            "\t *   192.168.0.0/24",
            "\t *   192.168.1.0/24",
            "\t *   192.168.128.0/24",
            "\t *   192.168.0.0/16",
            "\t *",
            "\t * The idea is to return more specific keys before less specific ones.",
            "\t */",
            "",
            "\t/* Empty trie */",
            "\tsearch_root = rcu_dereference(trie->root);",
            "\tif (!search_root)",
            "\t\treturn -ENOENT;",
            "",
            "\t/* For invalid key, find the leftmost node in the trie */",
            "\tif (!key || key->prefixlen > trie->max_prefixlen)",
            "\t\tgoto find_leftmost;",
            "",
            "\tnode_stack = kmalloc_array(trie->max_prefixlen + 1,",
            "\t\t\t\t   sizeof(struct lpm_trie_node *),",
            "\t\t\t\t   GFP_ATOMIC | __GFP_NOWARN);",
            "\tif (!node_stack)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/* Try to find the exact node for the given key */",
            "\tfor (node = search_root; node;) {",
            "\t\tnode_stack[++stack_ptr] = node;",
            "\t\tmatchlen = longest_prefix_match(trie, node, key);",
            "\t\tif (node->prefixlen != matchlen ||",
            "\t\t    node->prefixlen == key->prefixlen)",
            "\t\t\tbreak;",
            "",
            "\t\tnext_bit = extract_bit(key->data, node->prefixlen);",
            "\t\tnode = rcu_dereference(node->child[next_bit]);",
            "\t}",
            "\tif (!node || node->prefixlen != matchlen ||",
            "\t    (node->flags & LPM_TREE_NODE_FLAG_IM))",
            "\t\tgoto find_leftmost;",
            "",
            "\t/* The node with the exactly-matching key has been found,",
            "\t * find the first node in postorder after the matched node.",
            "\t */",
            "\tnode = node_stack[stack_ptr];",
            "\twhile (stack_ptr > 0) {",
            "\t\tparent = node_stack[stack_ptr - 1];",
            "\t\tif (rcu_dereference(parent->child[0]) == node) {",
            "\t\t\tsearch_root = rcu_dereference(parent->child[1]);",
            "\t\t\tif (search_root)",
            "\t\t\t\tgoto find_leftmost;",
            "\t\t}",
            "\t\tif (!(parent->flags & LPM_TREE_NODE_FLAG_IM)) {",
            "\t\t\tnext_node = parent;",
            "\t\t\tgoto do_copy;",
            "\t\t}",
            "",
            "\t\tnode = parent;",
            "\t\tstack_ptr--;",
            "\t}",
            "",
            "\t/* did not find anything */",
            "\terr = -ENOENT;",
            "\tgoto free_stack;",
            "",
            "find_leftmost:",
            "\t/* Find the leftmost non-intermediate node, all intermediate nodes",
            "\t * have exact two children, so this function will never return NULL.",
            "\t */",
            "\tfor (node = search_root; node;) {",
            "\t\tif (node->flags & LPM_TREE_NODE_FLAG_IM) {",
            "\t\t\tnode = rcu_dereference(node->child[0]);",
            "\t\t} else {",
            "\t\t\tnext_node = node;",
            "\t\t\tnode = rcu_dereference(node->child[0]);",
            "\t\t\tif (!node)",
            "\t\t\t\tnode = rcu_dereference(next_node->child[1]);",
            "\t\t}",
            "\t}",
            "do_copy:",
            "\tnext_key->prefixlen = next_node->prefixlen;",
            "\tmemcpy((void *)next_key + offsetof(struct bpf_lpm_trie_key_u8, data),",
            "\t       next_node->data, trie->data_size);",
            "free_stack:",
            "\tkfree(node_stack);",
            "\treturn err;",
            "}",
            "static int trie_check_btf(const struct bpf_map *map,",
            "\t\t\t  const struct btf *btf,",
            "\t\t\t  const struct btf_type *key_type,",
            "\t\t\t  const struct btf_type *value_type)",
            "{",
            "\t/* Keys must have struct bpf_lpm_trie_key_u8 embedded. */",
            "\treturn BTF_INFO_KIND(key_type->info) != BTF_KIND_STRUCT ?",
            "\t       -EINVAL : 0;",
            "}"
          ],
          "function_name": "trie_get_next_key, trie_check_btf",
          "description": "实现了获取下一个键（trie_get_next_key）的后序遍历逻辑，以及BTF类型校验（trie_check_btf）。get_next_key遵循特定顺序返回更具体的键，check_btf确保键类型符合预定义结构。",
          "similarity": 0.4924299120903015
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/lpm_trie.c",
          "start_line": 452,
          "end_line": 586,
          "content": [
            "static long trie_delete_elem(struct bpf_map *map, void *_key)",
            "{",
            "\tstruct lpm_trie *trie = container_of(map, struct lpm_trie, map);",
            "\tstruct lpm_trie_node *free_node = NULL, *free_parent = NULL;",
            "\tstruct bpf_lpm_trie_key_u8 *key = _key;",
            "\tstruct lpm_trie_node __rcu **trim, **trim2;",
            "\tstruct lpm_trie_node *node, *parent;",
            "\tunsigned long irq_flags;",
            "\tunsigned int next_bit;",
            "\tsize_t matchlen = 0;",
            "\tint ret = 0;",
            "",
            "\tif (key->prefixlen > trie->max_prefixlen)",
            "\t\treturn -EINVAL;",
            "",
            "\tspin_lock_irqsave(&trie->lock, irq_flags);",
            "",
            "\t/* Walk the tree looking for an exact key/length match and keeping",
            "\t * track of the path we traverse.  We will need to know the node",
            "\t * we wish to delete, and the slot that points to the node we want",
            "\t * to delete.  We may also need to know the nodes parent and the",
            "\t * slot that contains it.",
            "\t */",
            "\ttrim = &trie->root;",
            "\ttrim2 = trim;",
            "\tparent = NULL;",
            "\twhile ((node = rcu_dereference_protected(",
            "\t\t       *trim, lockdep_is_held(&trie->lock)))) {",
            "\t\tmatchlen = longest_prefix_match(trie, node, key);",
            "",
            "\t\tif (node->prefixlen != matchlen ||",
            "\t\t    node->prefixlen == key->prefixlen)",
            "\t\t\tbreak;",
            "",
            "\t\tparent = node;",
            "\t\ttrim2 = trim;",
            "\t\tnext_bit = extract_bit(key->data, node->prefixlen);",
            "\t\ttrim = &node->child[next_bit];",
            "\t}",
            "",
            "\tif (!node || node->prefixlen != key->prefixlen ||",
            "\t    node->prefixlen != matchlen ||",
            "\t    (node->flags & LPM_TREE_NODE_FLAG_IM)) {",
            "\t\tret = -ENOENT;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttrie->n_entries--;",
            "",
            "\t/* If the node we are removing has two children, simply mark it",
            "\t * as intermediate and we are done.",
            "\t */",
            "\tif (rcu_access_pointer(node->child[0]) &&",
            "\t    rcu_access_pointer(node->child[1])) {",
            "\t\tnode->flags |= LPM_TREE_NODE_FLAG_IM;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* If the parent of the node we are about to delete is an intermediate",
            "\t * node, and the deleted node doesn't have any children, we can delete",
            "\t * the intermediate parent as well and promote its other child",
            "\t * up the tree.  Doing this maintains the invariant that all",
            "\t * intermediate nodes have exactly 2 children and that there are no",
            "\t * unnecessary intermediate nodes in the tree.",
            "\t */",
            "\tif (parent && (parent->flags & LPM_TREE_NODE_FLAG_IM) &&",
            "\t    !node->child[0] && !node->child[1]) {",
            "\t\tif (node == rcu_access_pointer(parent->child[0]))",
            "\t\t\trcu_assign_pointer(",
            "\t\t\t\t*trim2, rcu_access_pointer(parent->child[1]));",
            "\t\telse",
            "\t\t\trcu_assign_pointer(",
            "\t\t\t\t*trim2, rcu_access_pointer(parent->child[0]));",
            "\t\tfree_parent = parent;",
            "\t\tfree_node = node;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* The node we are removing has either zero or one child. If there",
            "\t * is a child, move it into the removed node's slot then delete",
            "\t * the node.  Otherwise just clear the slot and delete the node.",
            "\t */",
            "\tif (node->child[0])",
            "\t\trcu_assign_pointer(*trim, rcu_access_pointer(node->child[0]));",
            "\telse if (node->child[1])",
            "\t\trcu_assign_pointer(*trim, rcu_access_pointer(node->child[1]));",
            "\telse",
            "\t\tRCU_INIT_POINTER(*trim, NULL);",
            "\tfree_node = node;",
            "",
            "out:",
            "\tspin_unlock_irqrestore(&trie->lock, irq_flags);",
            "\tkfree_rcu(free_parent, rcu);",
            "\tkfree_rcu(free_node, rcu);",
            "",
            "\treturn ret;",
            "}",
            "static void trie_free(struct bpf_map *map)",
            "{",
            "\tstruct lpm_trie *trie = container_of(map, struct lpm_trie, map);",
            "\tstruct lpm_trie_node __rcu **slot;",
            "\tstruct lpm_trie_node *node;",
            "",
            "\t/* Always start at the root and walk down to a node that has no",
            "\t * children. Then free that node, nullify its reference in the parent",
            "\t * and start over.",
            "\t */",
            "",
            "\tfor (;;) {",
            "\t\tslot = &trie->root;",
            "",
            "\t\tfor (;;) {",
            "\t\t\tnode = rcu_dereference_protected(*slot, 1);",
            "\t\t\tif (!node)",
            "\t\t\t\tgoto out;",
            "",
            "\t\t\tif (rcu_access_pointer(node->child[0])) {",
            "\t\t\t\tslot = &node->child[0];",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tif (rcu_access_pointer(node->child[1])) {",
            "\t\t\t\tslot = &node->child[1];",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tkfree(node);",
            "\t\t\tRCU_INIT_POINTER(*slot, NULL);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "out:",
            "\tbpf_map_area_free(trie);",
            "}"
          ],
          "function_name": "trie_delete_elem, trie_free",
          "description": "提供了删除元素（trie_delete_elem）和销毁树（trie_free）的功能。删除时会判断节点是否为中间节点并进行相应调整，销毁时通过RCU机制安全地遍历并释放所有节点内存。",
          "similarity": 0.43567976355552673
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/bpf/lpm_trie.c",
          "start_line": 741,
          "end_line": 749,
          "content": [
            "static u64 trie_mem_usage(const struct bpf_map *map)",
            "{",
            "\tstruct lpm_trie *trie = container_of(map, struct lpm_trie, map);",
            "\tu64 elem_size;",
            "",
            "\telem_size = sizeof(struct lpm_trie_node) + trie->data_size +",
            "\t\t\t    trie->map.value_size;",
            "\treturn elem_size * READ_ONCE(trie->n_entries);",
            "}"
          ],
          "function_name": "trie_mem_usage",
          "description": "计算并返回当前Trie占用的内存量（trie_mem_usage），基于节点大小（含动态数据区域）与元素数量的乘积，用于监控内存使用情况。",
          "similarity": 0.4098608195781708
        }
      ]
    },
    {
      "source_file": "kernel/livepatch/transition.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:34:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `livepatch\\transition.c`\n\n---\n\n# livepatch/transition.c 技术文档\n\n## 1. 文件概述\n\n`livepatch/transition.c` 是 Linux 内核实时补丁（Kernel Live Patching）子系统的核心组件之一，负责管理补丁状态转换过程。该文件实现了从旧代码到新补丁代码（或反向）的安全过渡机制，确保所有正在运行的任务（包括内核线程、用户态进程和 idle 线程）都能安全地切换到目标补丁状态，避免在函数栈中仍存在待替换函数时进行切换，从而防止系统崩溃或行为异常。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `klp_transition_patch`：指向当前正在进行状态转换的补丁对象。\n- `klp_target_state`：目标补丁状态（`KLP_PATCHED` 或 `KLP_UNPATCHED`），初始为 `KLP_UNDEFINED`。\n- `klp_signals_cnt`：用于统计信号处理相关计数（当前未在代码片段中完整使用）。\n- `klp_stack_entries`：每 CPU 栈追踪缓冲区，用于保存任务调用栈。\n\n### 主要函数\n- `klp_transition_work_fn()`：延迟工作队列回调，用于重试未能完成转换的“滞留”任务。\n- `klp_synchronize_transition()`：强制在所有 CPU 上执行调度同步，确保 RCU 不可见区域也能完成同步。\n- `klp_complete_transition()`：完成整个补丁状态转换，清理数据结构并调用回调。\n- `klp_cancel_transition()`：在转换开始前取消补丁操作。\n- `klp_update_patch_state()`：更新指定任务的补丁状态。\n- `klp_check_stack_func()`：检查给定函数是否出现在栈追踪中。\n- `klp_check_stack()`：检查任务栈中是否存在待替换/待移除的函数（代码片段中被截断）。\n\n### 静态键与调度集成\n- 在支持 `CONFIG_PREEMPT_DYNAMIC` 的系统上，通过 `sched_dynamic_klp_enable/disable()` 启用/禁用 cond_resched 中的栈检查。\n- 否则使用静态键 `klp_sched_try_switch_key` 控制是否在 `cond_resched()` 中进行补丁栈检查，以帮助 CPU 密集型内核线程完成补丁切换。\n\n## 3. 关键实现\n\n### 补丁状态转换流程\n1. **初始化阶段**：设置 `klp_transition_patch` 和 `klp_target_state`。\n2. **任务状态更新**：通过 `TIF_PATCH_PENDING` 标志标记需要更新状态的任务。\n3. **栈安全检查**：使用 `stack_trace_save_tsk_reliable()` 获取可靠栈追踪，检查是否存在待替换函数。\n4. **同步机制**：\n   - 使用 `klp_synchronize_transition()` 调用 `schedule_on_each_cpu(klp_sync)`，强制所有 CPU（包括 idle 和用户态）参与同步。\n   - 此机制绕过标准 RCU，适用于 RCU 不活跃的上下文（如 `user_exit()` 前）。\n5. **完成清理**：\n   - 清除所有任务的 `patch_state` 为 `KLP_UNDEFINED`。\n   - 调用对象级的 `post_patch` 或 `post_unpatch` 回调。\n   - 重置全局状态变量。\n\n### 栈检查逻辑\n- **打补丁时（KLP_PATCHED）**：检查栈中是否包含**旧函数**（原始函数或上一个补丁版本的函数）。\n- **卸补丁时（KLP_UNPATCHED）**：检查栈中是否包含**新函数**（当前补丁中的函数）。\n- 若发现相关函数在栈中，则返回 `-EAGAIN`，推迟该任务的状态切换。\n\n### 内存屏障与并发控制\n- `test_and_clear_tsk_thread_flag()` 不仅清除 `TIF_PATCH_PENDING`，还充当读屏障（`smp_rmb`），确保：\n  1. `klp_target_state` 的读取顺序正确。\n  2. 后续 `klp_ftrace_handler()` 能看到一致的 `func->transition` 状态。\n\n### 滞留任务处理\n- 通过 `DECLARE_DELAYED_WORK(klp_transition_work, ...)` 定期重试未能完成转换的任务，提高转换成功率。\n\n## 4. 依赖关系\n\n- **内部依赖**：\n  - `core.h`：提供 `klp_mutex`、`klp_for_each_object/func` 等核心宏和函数。\n  - `patch.h`：定义 `klp_func`、`klp_object`、`klp_patch` 等数据结构及操作函数（如 `klp_unpatch_objects`）。\n  - `transition.h`：声明本文件导出的接口（如 `klp_cancel_transition`）。\n- **内核子系统**：\n  - **RCU**：用于常规同步，但在 RCU 不活跃区域使用自定义同步。\n  - **调度器**：通过 `cond_resched()` 集成补丁检查，依赖 `CONFIG_PREEMPT_DYNAMIC` 或静态键。\n  - **栈追踪**：使用 `stack_trace_save_tsk_reliable()` 获取可靠调用栈。\n  - **CPU 热插拔**：通过 `for_each_possible_cpu` 处理所有可能的 CPU（包括离线 CPU 的 idle 任务）。\n\n## 5. 使用场景\n\n- **应用实时补丁**：当管理员通过 sysfs 启用一个 livepatch 模块时，内核调用此文件中的函数将所有任务从旧代码切换到新补丁代码。\n- **卸载实时补丁**：当禁用补丁时，安全地将所有任务切换回旧函数，并清理补丁数据结构。\n- **处理滞留任务**：对于因长时间运行或处于不可中断状态而未能及时切换的任务，通过延迟工作队列周期性重试。\n- **支持特殊上下文**：确保在 RCU 不活跃的上下文（如系统调用入口/出口、idle 循环）中也能安全完成补丁切换。\n- **错误恢复**：在补丁初始化后、实际切换前发生错误时，调用 `klp_cancel_transition()` 安全回滚。",
      "similarity": 0.5280008316040039,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "kernel/livepatch/transition.c",
          "start_line": 648,
          "end_line": 740,
          "content": [
            "void klp_reverse_transition(void)",
            "{",
            "\tunsigned int cpu;",
            "\tstruct task_struct *g, *task;",
            "",
            "\tpr_debug(\"'%s': reversing transition from %s\\n\",",
            "\t\t klp_transition_patch->mod->name,",
            "\t\t klp_target_state == KLP_PATCHED ? \"patching to unpatching\" :",
            "\t\t\t\t\t\t   \"unpatching to patching\");",
            "",
            "\t/*",
            "\t * Clear all TIF_PATCH_PENDING flags to prevent races caused by",
            "\t * klp_update_patch_state() or __klp_sched_try_switch() running in",
            "\t * parallel with the reverse transition.",
            "\t */",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task)",
            "\t\tclear_tsk_thread_flag(task, TIF_PATCH_PENDING);",
            "\tread_unlock(&tasklist_lock);",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tclear_tsk_thread_flag(idle_task(cpu), TIF_PATCH_PENDING);",
            "",
            "\t/*",
            "\t * Make sure all existing invocations of klp_update_patch_state() and",
            "\t * __klp_sched_try_switch() see the cleared TIF_PATCH_PENDING before",
            "\t * starting the reverse transition.",
            "\t */",
            "\tklp_synchronize_transition();",
            "",
            "\t/*",
            "\t * All patching has stopped, now re-initialize the global variables to",
            "\t * prepare for the reverse transition.",
            "\t */",
            "\tklp_transition_patch->enabled = !klp_transition_patch->enabled;",
            "\tklp_target_state = !klp_target_state;",
            "",
            "\t/*",
            "\t * Enforce the order of the klp_target_state write and the",
            "\t * TIF_PATCH_PENDING writes in klp_start_transition() to ensure",
            "\t * klp_update_patch_state() and __klp_sched_try_switch() don't set",
            "\t * task->patch_state to the wrong value.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\tklp_start_transition();",
            "}",
            "void klp_copy_process(struct task_struct *child)",
            "{",
            "",
            "\t/*",
            "\t * The parent process may have gone through a KLP transition since",
            "\t * the thread flag was copied in setup_thread_stack earlier. Bring",
            "\t * the task flag up to date with the parent here.",
            "\t *",
            "\t * The operation is serialized against all klp_*_transition()",
            "\t * operations by the tasklist_lock. The only exceptions are",
            "\t * klp_update_patch_state(current) and __klp_sched_try_switch(), but we",
            "\t * cannot race with them because we are current.",
            "\t */",
            "\tif (test_tsk_thread_flag(current, TIF_PATCH_PENDING))",
            "\t\tset_tsk_thread_flag(child, TIF_PATCH_PENDING);",
            "\telse",
            "\t\tclear_tsk_thread_flag(child, TIF_PATCH_PENDING);",
            "",
            "\tchild->patch_state = current->patch_state;",
            "}",
            "void klp_force_transition(void)",
            "{",
            "\tstruct klp_patch *patch;",
            "\tstruct task_struct *g, *task;",
            "\tunsigned int cpu;",
            "",
            "\tpr_warn(\"forcing remaining tasks to the patched state\\n\");",
            "",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task)",
            "\t\tklp_update_patch_state(task);",
            "\tread_unlock(&tasklist_lock);",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tklp_update_patch_state(idle_task(cpu));",
            "",
            "\t/* Set forced flag for patches being removed. */",
            "\tif (klp_target_state == KLP_UNPATCHED)",
            "\t\tklp_transition_patch->forced = true;",
            "\telse if (klp_transition_patch->replace) {",
            "\t\tklp_for_each_patch(patch) {",
            "\t\t\tif (patch != klp_transition_patch)",
            "\t\t\t\tpatch->forced = true;",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "klp_reverse_transition, klp_copy_process, klp_force_transition",
          "description": "该代码段实现Live Patching框架中的状态转换控制逻辑。  \n`klp_reverse_transition`负责反向转换补丁状态，清除所有任务的TIF_PATCH_PENDING标志并切换全局状态后启动反向迁移；`klp_copy_process`在进程复制时同步父进程的补丁状态标志；`klp_force_transition`强制将剩余任务设为目标状态，并标记待移除补丁的强制属性。  \n\n注：代码依赖`klp_transition_patch`、`klp_target_state`等全局变量及`tasklist_lock`等上下文，此处仅展示部分实现。",
          "similarity": 0.5532863140106201
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/livepatch/transition.c",
          "start_line": 214,
          "end_line": 352,
          "content": [
            "static int klp_check_stack_func(struct klp_func *func, unsigned long *entries,",
            "\t\t\t\tunsigned int nr_entries)",
            "{",
            "\tunsigned long func_addr, func_size, address;",
            "\tstruct klp_ops *ops;",
            "\tint i;",
            "",
            "\tif (klp_target_state == KLP_UNPATCHED) {",
            "\t\t /*",
            "\t\t  * Check for the to-be-unpatched function",
            "\t\t  * (the func itself).",
            "\t\t  */",
            "\t\tfunc_addr = (unsigned long)func->new_func;",
            "\t\tfunc_size = func->new_size;",
            "\t} else {",
            "\t\t/*",
            "\t\t * Check for the to-be-patched function",
            "\t\t * (the previous func).",
            "\t\t */",
            "\t\tops = klp_find_ops(func->old_func);",
            "",
            "\t\tif (list_is_singular(&ops->func_stack)) {",
            "\t\t\t/* original function */",
            "\t\t\tfunc_addr = (unsigned long)func->old_func;",
            "\t\t\tfunc_size = func->old_size;",
            "\t\t} else {",
            "\t\t\t/* previously patched function */",
            "\t\t\tstruct klp_func *prev;",
            "",
            "\t\t\tprev = list_next_entry(func, stack_node);",
            "\t\t\tfunc_addr = (unsigned long)prev->new_func;",
            "\t\t\tfunc_size = prev->new_size;",
            "\t\t}",
            "\t}",
            "",
            "\tfor (i = 0; i < nr_entries; i++) {",
            "\t\taddress = entries[i];",
            "",
            "\t\tif (address >= func_addr && address < func_addr + func_size)",
            "\t\t\treturn -EAGAIN;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int klp_check_stack(struct task_struct *task, const char **oldname)",
            "{",
            "\tunsigned long *entries = this_cpu_ptr(klp_stack_entries);",
            "\tstruct klp_object *obj;",
            "\tstruct klp_func *func;",
            "\tint ret, nr_entries;",
            "",
            "\t/* Protect 'klp_stack_entries' */",
            "\tlockdep_assert_preemption_disabled();",
            "",
            "\tret = stack_trace_save_tsk_reliable(task, entries, MAX_STACK_ENTRIES);",
            "\tif (ret < 0)",
            "\t\treturn -EINVAL;",
            "\tnr_entries = ret;",
            "",
            "\tklp_for_each_object(klp_transition_patch, obj) {",
            "\t\tif (!obj->patched)",
            "\t\t\tcontinue;",
            "\t\tklp_for_each_func(obj, func) {",
            "\t\t\tret = klp_check_stack_func(func, entries, nr_entries);",
            "\t\t\tif (ret) {",
            "\t\t\t\t*oldname = func->old_name;",
            "\t\t\t\treturn -EADDRINUSE;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int klp_check_and_switch_task(struct task_struct *task, void *arg)",
            "{",
            "\tint ret;",
            "",
            "\tif (task_curr(task) && task != current)",
            "\t\treturn -EBUSY;",
            "",
            "\tret = klp_check_stack(task, arg);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tclear_tsk_thread_flag(task, TIF_PATCH_PENDING);",
            "\ttask->patch_state = klp_target_state;",
            "\treturn 0;",
            "}",
            "static bool klp_try_switch_task(struct task_struct *task)",
            "{",
            "\tconst char *old_name;",
            "\tint ret;",
            "",
            "\t/* check if this task has already switched over */",
            "\tif (task->patch_state == klp_target_state)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * For arches which don't have reliable stack traces, we have to rely",
            "\t * on other methods (e.g., switching tasks at kernel exit).",
            "\t */",
            "\tif (!klp_have_reliable_stack())",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Now try to check the stack for any to-be-patched or to-be-unpatched",
            "\t * functions.  If all goes well, switch the task to the target patch",
            "\t * state.",
            "\t */",
            "\tif (task == current)",
            "\t\tret = klp_check_and_switch_task(current, &old_name);",
            "\telse",
            "\t\tret = task_call_func(task, klp_check_and_switch_task, &old_name);",
            "",
            "\tswitch (ret) {",
            "\tcase 0:\t\t/* success */",
            "\t\tbreak;",
            "",
            "\tcase -EBUSY:\t/* klp_check_and_switch_task() */",
            "\t\tpr_debug(\"%s: %s:%d is running\\n\",",
            "\t\t\t __func__, task->comm, task->pid);",
            "\t\tbreak;",
            "\tcase -EINVAL:\t/* klp_check_and_switch_task() */",
            "\t\tpr_debug(\"%s: %s:%d has an unreliable stack\\n\",",
            "\t\t\t __func__, task->comm, task->pid);",
            "\t\tbreak;",
            "\tcase -EADDRINUSE: /* klp_check_and_switch_task() */",
            "\t\tpr_debug(\"%s: %s:%d is sleeping on function %s\\n\",",
            "\t\t\t __func__, task->comm, task->pid, old_name);",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tpr_debug(\"%s: Unknown error code (%d) when trying to switch %s:%d\\n\",",
            "\t\t\t __func__, ret, task->comm, task->pid);",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn !ret;",
            "}"
          ],
          "function_name": "klp_check_stack_func, klp_check_stack, klp_check_and_switch_task, klp_try_switch_task",
          "description": "提供堆栈检查与任务状态切换机制，验证当前线程堆栈中是否包含待修改函数地址，确保安全切换到目标补丁状态",
          "similarity": 0.5488104820251465
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/livepatch/transition.c",
          "start_line": 530,
          "end_line": 634,
          "content": [
            "void klp_start_transition(void)",
            "{",
            "\tstruct task_struct *g, *task;",
            "\tunsigned int cpu;",
            "",
            "\tWARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);",
            "",
            "\tpr_notice(\"'%s': starting %s transition\\n\",",
            "\t\t  klp_transition_patch->mod->name,",
            "\t\t  klp_target_state == KLP_PATCHED ? \"patching\" : \"unpatching\");",
            "",
            "\t/*",
            "\t * Mark all normal tasks as needing a patch state update.  They'll",
            "\t * switch either in klp_try_complete_transition() or as they exit the",
            "\t * kernel.",
            "\t */",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task)",
            "\t\tif (task->patch_state != klp_target_state)",
            "\t\t\tset_tsk_thread_flag(task, TIF_PATCH_PENDING);",
            "\tread_unlock(&tasklist_lock);",
            "",
            "\t/*",
            "\t * Mark all idle tasks as needing a patch state update.  They'll switch",
            "\t * either in klp_try_complete_transition() or at the idle loop switch",
            "\t * point.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\ttask = idle_task(cpu);",
            "\t\tif (task->patch_state != klp_target_state)",
            "\t\t\tset_tsk_thread_flag(task, TIF_PATCH_PENDING);",
            "\t}",
            "",
            "\tklp_cond_resched_enable();",
            "",
            "\tklp_signals_cnt = 0;",
            "}",
            "void klp_init_transition(struct klp_patch *patch, int state)",
            "{",
            "\tstruct task_struct *g, *task;",
            "\tunsigned int cpu;",
            "\tstruct klp_object *obj;",
            "\tstruct klp_func *func;",
            "\tint initial_state = !state;",
            "",
            "\tWARN_ON_ONCE(klp_target_state != KLP_UNDEFINED);",
            "",
            "\tklp_transition_patch = patch;",
            "",
            "\t/*",
            "\t * Set the global target patch state which tasks will switch to.  This",
            "\t * has no effect until the TIF_PATCH_PENDING flags get set later.",
            "\t */",
            "\tklp_target_state = state;",
            "",
            "\tpr_debug(\"'%s': initializing %s transition\\n\", patch->mod->name,",
            "\t\t klp_target_state == KLP_PATCHED ? \"patching\" : \"unpatching\");",
            "",
            "\t/*",
            "\t * Initialize all tasks to the initial patch state to prepare them for",
            "\t * switching to the target state.",
            "\t */",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task) {",
            "\t\tWARN_ON_ONCE(task->patch_state != KLP_UNDEFINED);",
            "\t\ttask->patch_state = initial_state;",
            "\t}",
            "\tread_unlock(&tasklist_lock);",
            "",
            "\t/*",
            "\t * Ditto for the idle \"swapper\" tasks.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\ttask = idle_task(cpu);",
            "\t\tWARN_ON_ONCE(task->patch_state != KLP_UNDEFINED);",
            "\t\ttask->patch_state = initial_state;",
            "\t}",
            "",
            "\t/*",
            "\t * Enforce the order of the task->patch_state initializations and the",
            "\t * func->transition updates to ensure that klp_ftrace_handler() doesn't",
            "\t * see a func in transition with a task->patch_state of KLP_UNDEFINED.",
            "\t *",
            "\t * Also enforce the order of the klp_target_state write and future",
            "\t * TIF_PATCH_PENDING writes to ensure klp_update_patch_state() and",
            "\t * __klp_sched_try_switch() don't set a task->patch_state to",
            "\t * KLP_UNDEFINED.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\t/*",
            "\t * Set the func transition states so klp_ftrace_handler() will know to",
            "\t * switch to the transition logic.",
            "\t *",
            "\t * When patching, the funcs aren't yet in the func_stack and will be",
            "\t * made visible to the ftrace handler shortly by the calls to",
            "\t * klp_patch_object().",
            "\t *",
            "\t * When unpatching, the funcs are already in the func_stack and so are",
            "\t * already visible to the ftrace handler.",
            "\t */",
            "\tklp_for_each_object(patch, obj)",
            "\t\tklp_for_each_func(obj, func)",
            "\t\t\tfunc->transition = true;",
            "}"
          ],
          "function_name": "klp_start_transition, klp_init_transition",
          "description": "初始化热补丁过渡阶段，设置全局目标状态并批量标记所有任务需更新补丁状态，通过内存屏障保证状态更新顺序性",
          "similarity": 0.4971836507320404
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/livepatch/transition.c",
          "start_line": 366,
          "end_line": 509,
          "content": [
            "void __klp_sched_try_switch(void)",
            "{",
            "\tif (likely(!klp_patch_pending(current)))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This function is called from cond_resched() which is called in many",
            "\t * places throughout the kernel.  Using the klp_mutex here might",
            "\t * deadlock.",
            "\t *",
            "\t * Instead, disable preemption to prevent racing with other callers of",
            "\t * klp_try_switch_task().  Thanks to task_call_func() they won't be",
            "\t * able to switch this task while it's running.",
            "\t */",
            "\tpreempt_disable();",
            "",
            "\t/*",
            "\t * Make sure current didn't get patched between the above check and",
            "\t * preempt_disable().",
            "\t */",
            "\tif (unlikely(!klp_patch_pending(current)))",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * Enforce the order of the TIF_PATCH_PENDING read above and the",
            "\t * klp_target_state read in klp_try_switch_task().  The corresponding",
            "\t * write barriers are in klp_init_transition() and",
            "\t * klp_reverse_transition().",
            "\t */",
            "\tsmp_rmb();",
            "",
            "\tklp_try_switch_task(current);",
            "",
            "out:",
            "\tpreempt_enable();",
            "}",
            "static void klp_send_signals(void)",
            "{",
            "\tstruct task_struct *g, *task;",
            "",
            "\tif (klp_signals_cnt == SIGNALS_TIMEOUT)",
            "\t\tpr_notice(\"signaling remaining tasks\\n\");",
            "",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task) {",
            "\t\tif (!klp_patch_pending(task))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * There is a small race here. We could see TIF_PATCH_PENDING",
            "\t\t * set and decide to wake up a kthread or send a fake signal.",
            "\t\t * Meanwhile the task could migrate itself and the action",
            "\t\t * would be meaningless. It is not serious though.",
            "\t\t */",
            "\t\tif (task->flags & PF_KTHREAD) {",
            "\t\t\t/*",
            "\t\t\t * Wake up a kthread which sleeps interruptedly and",
            "\t\t\t * still has not been migrated.",
            "\t\t\t */",
            "\t\t\twake_up_state(task, TASK_INTERRUPTIBLE);",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * Send fake signal to all non-kthread tasks which are",
            "\t\t\t * still not migrated.",
            "\t\t\t */",
            "\t\t\tset_notify_signal(task);",
            "\t\t}",
            "\t}",
            "\tread_unlock(&tasklist_lock);",
            "}",
            "void klp_try_complete_transition(void)",
            "{",
            "\tunsigned int cpu;",
            "\tstruct task_struct *g, *task;",
            "\tstruct klp_patch *patch;",
            "\tbool complete = true;",
            "",
            "\tWARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);",
            "",
            "\t/*",
            "\t * Try to switch the tasks to the target patch state by walking their",
            "\t * stacks and looking for any to-be-patched or to-be-unpatched",
            "\t * functions.  If such functions are found on a stack, or if the stack",
            "\t * is deemed unreliable, the task can't be switched yet.",
            "\t *",
            "\t * Usually this will transition most (or all) of the tasks on a system",
            "\t * unless the patch includes changes to a very common function.",
            "\t */",
            "\tread_lock(&tasklist_lock);",
            "\tfor_each_process_thread(g, task)",
            "\t\tif (!klp_try_switch_task(task))",
            "\t\t\tcomplete = false;",
            "\tread_unlock(&tasklist_lock);",
            "",
            "\t/*",
            "\t * Ditto for the idle \"swapper\" tasks.",
            "\t */",
            "\tcpus_read_lock();",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\ttask = idle_task(cpu);",
            "\t\tif (cpu_online(cpu)) {",
            "\t\t\tif (!klp_try_switch_task(task)) {",
            "\t\t\t\tcomplete = false;",
            "\t\t\t\t/* Make idle task go through the main loop. */",
            "\t\t\t\twake_up_if_idle(cpu);",
            "\t\t\t}",
            "\t\t} else if (task->patch_state != klp_target_state) {",
            "\t\t\t/* offline idle tasks can be switched immediately */",
            "\t\t\tclear_tsk_thread_flag(task, TIF_PATCH_PENDING);",
            "\t\t\ttask->patch_state = klp_target_state;",
            "\t\t}",
            "\t}",
            "\tcpus_read_unlock();",
            "",
            "\tif (!complete) {",
            "\t\tif (klp_signals_cnt && !(klp_signals_cnt % SIGNALS_TIMEOUT))",
            "\t\t\tklp_send_signals();",
            "\t\tklp_signals_cnt++;",
            "",
            "\t\t/*",
            "\t\t * Some tasks weren't able to be switched over.  Try again",
            "\t\t * later and/or wait for other methods like kernel exit",
            "\t\t * switching.",
            "\t\t */",
            "\t\tschedule_delayed_work(&klp_transition_work,",
            "\t\t\t\t      round_jiffies_relative(HZ));",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Done!  Now cleanup the data structures. */",
            "\tklp_cond_resched_disable();",
            "\tpatch = klp_transition_patch;",
            "\tklp_complete_transition();",
            "",
            "\t/*",
            "\t * It would make more sense to free the unused patches in",
            "\t * klp_complete_transition() but it is called also",
            "\t * from klp_cancel_transition().",
            "\t */",
            "\tif (!patch->enabled)",
            "\t\tklp_free_patch_async(patch);",
            "\telse if (patch->replace)",
            "\t\tklp_free_replaced_patches_async(patch);",
            "}"
          ],
          "function_name": "__klp_sched_try_switch, klp_send_signals, klp_try_complete_transition",
          "description": "调度器层面的过渡辅助函数，通过禁止抢占防止竞态条件，并向未迁移任务发送信号触发上下文切换以完成补丁应用",
          "similarity": 0.4894309341907501
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/livepatch/transition.c",
          "start_line": 1,
          "end_line": 52,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * transition.c - Kernel Live Patching transition functions",
            " *",
            " * Copyright (C) 2015-2016 Josh Poimboeuf <jpoimboe@redhat.com>",
            " */",
            "",
            "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt",
            "",
            "#include <linux/cpu.h>",
            "#include <linux/stacktrace.h>",
            "#include <linux/static_call.h>",
            "#include \"core.h\"",
            "#include \"patch.h\"",
            "#include \"transition.h\"",
            "",
            "#define MAX_STACK_ENTRIES  100",
            "static DEFINE_PER_CPU(unsigned long[MAX_STACK_ENTRIES], klp_stack_entries);",
            "",
            "#define STACK_ERR_BUF_SIZE 128",
            "",
            "#define SIGNALS_TIMEOUT 15",
            "",
            "struct klp_patch *klp_transition_patch;",
            "",
            "static int klp_target_state = KLP_UNDEFINED;",
            "",
            "static unsigned int klp_signals_cnt;",
            "",
            "/*",
            " * When a livepatch is in progress, enable klp stack checking in",
            " * cond_resched().  This helps CPU-bound kthreads get patched.",
            " */",
            "#if defined(CONFIG_PREEMPT_DYNAMIC) && defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)",
            "",
            "#define klp_cond_resched_enable() sched_dynamic_klp_enable()",
            "#define klp_cond_resched_disable() sched_dynamic_klp_disable()",
            "",
            "#else /* !CONFIG_PREEMPT_DYNAMIC || !CONFIG_HAVE_PREEMPT_DYNAMIC_CALL */",
            "",
            "DEFINE_STATIC_KEY_FALSE(klp_sched_try_switch_key);",
            "EXPORT_SYMBOL(klp_sched_try_switch_key);",
            "",
            "#define klp_cond_resched_enable() static_branch_enable(&klp_sched_try_switch_key)",
            "#define klp_cond_resched_disable() static_branch_disable(&klp_sched_try_switch_key)",
            "",
            "#endif /* CONFIG_PREEMPT_DYNAMIC && CONFIG_HAVE_PREEMPT_DYNAMIC_CALL */",
            "",
            "/*",
            " * This work can be performed periodically to finish patching or unpatching any",
            " * \"straggler\" tasks which failed to transition in the first attempt.",
            " */"
          ],
          "function_name": null,
          "description": "定义内核热补丁过渡相关全局变量和辅助宏，启用调度器动态KLPL检查逻辑，用于在补丁应用期间检测函数调用堆栈中的潜在冲突",
          "similarity": 0.47812455892562866
        }
      ]
    }
  ]
}