{
  "query": "RAID的优缺点 DMA的作用",
  "timestamp": "2025-12-26 00:31:17",
  "retrieved_files": [
    {
      "source_file": "kernel/dma/mapping.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:14:22\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\mapping.c`\n\n---\n\n# `dma/mapping.c` 技术文档\n\n## 1. 文件概述\n\n`dma/mapping.c` 是 Linux 内核中与架构无关的 DMA（Direct Memory Access）映射核心实现文件。该文件提供了统一的、可管理的 DMA 内存分配与映射接口，屏蔽了底层硬件（如 IOMMU、直接映射等）的差异，为驱动开发者提供一致的 DMA 操作抽象。同时，它支持“资源管理”（Managed DMA）机制，确保在设备卸载时自动释放 DMA 资源，防止内存泄漏。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct dma_devres`**  \n  用于实现“Managed DMA”资源管理的私有结构体，包含：\n  - `size`：分配的内存大小\n  - `vaddr`：内核虚拟地址\n  - `dma_handle`：设备可见的 DMA 地址\n  - `attrs`：DMA 属性标志（如 `DMA_ATTR_*`）\n\n### 主要函数\n- **Managed DMA 接口**\n  - `dmam_alloc_attrs()`：分配受管理的 DMA 内存（自动释放）\n  - `dmam_free_coherent()`：显式释放受管理的 coherent DMA 内存（通常由 devres 自动调用）\n\n- **DMA 映射/解映射接口**\n  - `dma_map_page_attrs()`：将单个页面映射为 DMA 地址\n  - `dma_unmap_page_attrs()`：解映射单个页面的 DMA 地址\n  - `dma_map_sg_attrs()`：映射 scatterlist 缓冲区（返回成功映射的条目数）\n  - `dma_map_sgtable()`：映射 `sg_table` 结构描述的缓冲区（返回错误码）\n\n- **内部辅助函数**\n  - `dma_go_direct()` / `dma_alloc_direct()` / `dma_map_direct()`：判断是否可绕过 IOMMU 使用直接映射\n  - `__dma_map_sg_attrs()`：`dma_map_sg_attrs()` 和 `dma_map_sgtable()` 的公共实现\n\n## 3. 关键实现\n\n### Managed DMA 资源管理机制\n- 使用 `devres`（Device Resource Management）框架实现自动资源回收。\n- `dmam_alloc_attrs()` 在分配 DMA 内存后，将 `dma_devres` 结构注册到设备的资源链表中。\n- 设备卸载时，`devres` 框架自动调用 `dmam_release()`，进而调用 `dma_free_attrs()` 释放内存。\n- `dmam_match()` 用于在显式释放时匹配资源条目，确保一致性。\n\n### 直接映射（Direct Mapping）优化\n- 通过 `dma_go_direct()` 判断是否可跳过 IOMMU 操作：\n  - 若设备无自定义 `dma_map_ops`，默认使用直接映射。\n  - 若启用 `CONFIG_DMA_OPS_BYPASS` 且设备设置 `dma_ops_bypass`，且 DMA 掩码足够大（覆盖物理地址空间），则使用直接映射。\n- 直接映射路径调用 `dma_direct_*` 系列函数（定义在 `direct.h` 中），避免 IOMMU 开销。\n\n### 错误处理与调试支持\n- 所有映射函数均校验 DMA 方向（`valid_dma_direction`）和设备 DMA 掩码。\n- 集成 `KMSAN`（Kernel Memory Sanitizer）支持，通过 `kmsan_handle_dma*` 标记 DMA 访问区域。\n- 启用 `DMA_API_DEBUG` 时，调用 `debug_dma_*` 函数记录映射操作，用于检测错误使用（如未映射即访问）。\n\n### Scatterlist 映射语义\n- `dma_map_sg_attrs()` 返回实际映射成功的条目数（可能 ≤ `nents`），但**解映射必须使用原始 `nents`**。\n- `dma_map_sgtable()` 返回标准错误码（如 `-ENOMEM`, `-EINVAL`），便于错误分类和重试逻辑。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/dma-map-ops.h>`：DMA 操作集抽象（`dma_map_ops`）\n  - `<linux/devres.h>`（隐式）：设备资源管理框架\n  - `\"direct.h\"`：直接映射实现（`dma_direct_map_page` 等）\n  - `\"debug.h\"`：DMA 调试接口\n- **配置依赖**：\n  - `CONFIG_ARCH_HAS_SYNC_DMA_*`：决定是否定义 `dma_default_coherent`\n  - `CONFIG_DMA_OPS_BYPASS`：启用 DMA 操作绕过优化\n  - `CONFIG_ARCH_DMA_DEFAULT_COHERENT`：设置默认一致性属性\n- **底层依赖**：\n  - 架构特定的 `arch_dma_*_direct()` 函数（用于判断直接映射可行性）\n  - IOMMU 驱动提供的 `dma_map_ops` 实现（当不使用直接映射时）\n\n## 5. 使用场景\n\n- **驱动开发**：\n  - 使用 `dmam_alloc_attrs()` 分配 DMA 缓冲区，避免手动释放。\n  - 使用 `dma_map_page_attrs()` 或 `dma_map_sg_attrs()` 映射数据缓冲区供设备访问。\n- **IOMMU 子系统**：\n  - IOMMU 驱动通过注册 `dma_map_ops` 拦截映射请求，实现地址转换和权限控制。\n  - 当设备 DMA 能力足够时（如 64 位 DMA 掩码），自动切换至直接映射以提升性能。\n- **内存调试**：\n  - 与 KMSAN 集成，在 DMA 操作前后标记内存状态，检测 CPU/DMA 访问冲突。\n  - 通过 `DMA_API_DEBUG` 捕获驱动错误（如重复映射、方向错误等）。",
      "similarity": 0.5755074620246887,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/mapping.c",
          "start_line": 36,
          "end_line": 145,
          "content": [
            "static void dmam_release(struct device *dev, void *res)",
            "{",
            "\tstruct dma_devres *this = res;",
            "",
            "\tdma_free_attrs(dev, this->size, this->vaddr, this->dma_handle,",
            "\t\t\tthis->attrs);",
            "}",
            "static int dmam_match(struct device *dev, void *res, void *match_data)",
            "{",
            "\tstruct dma_devres *this = res, *match = match_data;",
            "",
            "\tif (this->vaddr == match->vaddr) {",
            "\t\tWARN_ON(this->size != match->size ||",
            "\t\t\tthis->dma_handle != match->dma_handle);",
            "\t\treturn 1;",
            "\t}",
            "\treturn 0;",
            "}",
            "void dmam_free_coherent(struct device *dev, size_t size, void *vaddr,",
            "\t\t\tdma_addr_t dma_handle)",
            "{",
            "\tstruct dma_devres match_data = { size, vaddr, dma_handle };",
            "",
            "\tWARN_ON(devres_destroy(dev, dmam_release, dmam_match, &match_data));",
            "\tdma_free_coherent(dev, size, vaddr, dma_handle);",
            "}",
            "static bool dma_go_direct(struct device *dev, dma_addr_t mask,",
            "\t\tconst struct dma_map_ops *ops)",
            "{",
            "\tif (likely(!ops))",
            "\t\treturn true;",
            "#ifdef CONFIG_DMA_OPS_BYPASS",
            "\tif (dev->dma_ops_bypass)",
            "\t\treturn min_not_zero(mask, dev->bus_dma_limit) >=",
            "\t\t\t    dma_direct_get_required_mask(dev);",
            "#endif",
            "\treturn false;",
            "}",
            "static inline bool dma_alloc_direct(struct device *dev,",
            "\t\tconst struct dma_map_ops *ops)",
            "{",
            "\treturn dma_go_direct(dev, dev->coherent_dma_mask, ops);",
            "}",
            "static inline bool dma_map_direct(struct device *dev,",
            "\t\tconst struct dma_map_ops *ops)",
            "{",
            "\treturn dma_go_direct(dev, *dev->dma_mask, ops);",
            "}",
            "dma_addr_t dma_map_page_attrs(struct device *dev, struct page *page,",
            "\t\tsize_t offset, size_t size, enum dma_data_direction dir,",
            "\t\tunsigned long attrs)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "\tdma_addr_t addr;",
            "",
            "\tBUG_ON(!valid_dma_direction(dir));",
            "",
            "\tif (WARN_ON_ONCE(!dev->dma_mask))",
            "\t\treturn DMA_MAPPING_ERROR;",
            "",
            "\tif (dma_map_direct(dev, ops) ||",
            "\t    arch_dma_map_page_direct(dev, page_to_phys(page) + offset + size))",
            "\t\taddr = dma_direct_map_page(dev, page, offset, size, dir, attrs);",
            "\telse",
            "\t\taddr = ops->map_page(dev, page, offset, size, dir, attrs);",
            "\tkmsan_handle_dma(page, offset, size, dir);",
            "\tdebug_dma_map_page(dev, page, offset, size, dir, addr, attrs);",
            "",
            "\treturn addr;",
            "}",
            "void dma_unmap_page_attrs(struct device *dev, dma_addr_t addr, size_t size,",
            "\t\tenum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tBUG_ON(!valid_dma_direction(dir));",
            "\tif (dma_map_direct(dev, ops) ||",
            "\t    arch_dma_unmap_page_direct(dev, addr + size))",
            "\t\tdma_direct_unmap_page(dev, addr, size, dir, attrs);",
            "\telse if (ops->unmap_page)",
            "\t\tops->unmap_page(dev, addr, size, dir, attrs);",
            "\tdebug_dma_unmap_page(dev, addr, size, dir);",
            "}",
            "static int __dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,",
            "\t int nents, enum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "\tint ents;",
            "",
            "\tBUG_ON(!valid_dma_direction(dir));",
            "",
            "\tif (WARN_ON_ONCE(!dev->dma_mask))",
            "\t\treturn 0;",
            "",
            "\tif (dma_map_direct(dev, ops) ||",
            "\t    arch_dma_map_sg_direct(dev, sg, nents))",
            "\t\tents = dma_direct_map_sg(dev, sg, nents, dir, attrs);",
            "\telse",
            "\t\tents = ops->map_sg(dev, sg, nents, dir, attrs);",
            "",
            "\tif (ents > 0) {",
            "\t\tkmsan_handle_dma_sg(sg, nents, dir);",
            "\t\tdebug_dma_map_sg(dev, sg, nents, ents, dir, attrs);",
            "\t} else if (WARN_ON_ONCE(ents != -EINVAL && ents != -ENOMEM &&",
            "\t\t\t\tents != -EIO && ents != -EREMOTEIO)) {",
            "\t\treturn -EIO;",
            "\t}",
            "",
            "\treturn ents;",
            "}"
          ],
          "function_name": "dmam_release, dmam_match, dmam_free_coherent, dma_go_direct, dma_alloc_direct, dma_map_direct, dma_map_page_attrs, dma_unmap_page_attrs, __dma_map_sg_attrs",
          "description": "实现DMA资源释放与匹配逻辑，通过dmam_release/Match管理设备资源生命周期；定义dma_go_direct/dma_alloc_direct等辅助函数判断是否启用直接映射路径；实现dma_map_page_attrs等核心接口，根据设备能力选择直接映射或通用DMA操作路径。",
          "similarity": 0.5476925373077393
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/mapping.c",
          "start_line": 1,
          "end_line": 35,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * arch-independent dma-mapping routines",
            " *",
            " * Copyright (c) 2006  SUSE Linux Products GmbH",
            " * Copyright (c) 2006  Tejun Heo <teheo@suse.de>",
            " */",
            "#include <linux/memblock.h> /* for max_pfn */",
            "#include <linux/acpi.h>",
            "#include <linux/dma-map-ops.h>",
            "#include <linux/export.h>",
            "#include <linux/gfp.h>",
            "#include <linux/kmsan.h>",
            "#include <linux/of_device.h>",
            "#include <linux/slab.h>",
            "#include <linux/vmalloc.h>",
            "#include \"debug.h\"",
            "#include \"direct.h\"",
            "",
            "#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \\",
            "\tdefined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \\",
            "\tdefined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL)",
            "bool dma_default_coherent = IS_ENABLED(CONFIG_ARCH_DMA_DEFAULT_COHERENT);",
            "#endif",
            "",
            "/*",
            " * Managed DMA API",
            " */",
            "struct dma_devres {",
            "\tsize_t\t\tsize;",
            "\tvoid\t\t*vaddr;",
            "\tdma_addr_t\tdma_handle;",
            "\tunsigned long\tattrs;",
            "};",
            ""
          ],
          "function_name": null,
          "description": "声明DMA设备资源结构体dma_devres，用于跟踪分配的DMA缓冲区大小、虚拟地址、DMA句柄及属性，并定义ARCH_HAS_SYNC_DMA系列配置选项，为后续DMA操作提供基础支持。",
          "similarity": 0.5402394533157349
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/dma/mapping.c",
          "start_line": 609,
          "end_line": 719,
          "content": [
            "int dma_mmap_pages(struct device *dev, struct vm_area_struct *vma,",
            "\t\tsize_t size, struct page *page)",
            "{",
            "\tunsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;",
            "",
            "\tif (vma->vm_pgoff >= count || vma_pages(vma) > count - vma->vm_pgoff)",
            "\t\treturn -ENXIO;",
            "\treturn remap_pfn_range(vma, vma->vm_start,",
            "\t\t\t       page_to_pfn(page) + vma->vm_pgoff,",
            "\t\t\t       vma_pages(vma) << PAGE_SHIFT, vma->vm_page_prot);",
            "}",
            "static void free_single_sgt(struct device *dev, size_t size,",
            "\t\tstruct sg_table *sgt, enum dma_data_direction dir)",
            "{",
            "\t__dma_free_pages(dev, size, sg_page(sgt->sgl), sgt->sgl->dma_address,",
            "\t\t\t dir);",
            "\tsg_free_table(sgt);",
            "\tkfree(sgt);",
            "}",
            "void dma_free_noncontiguous(struct device *dev, size_t size,",
            "\t\tstruct sg_table *sgt, enum dma_data_direction dir)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tdebug_dma_unmap_sg(dev, sgt->sgl, sgt->orig_nents, dir);",
            "\tif (ops && ops->free_noncontiguous)",
            "\t\tops->free_noncontiguous(dev, size, sgt, dir);",
            "\telse",
            "\t\tfree_single_sgt(dev, size, sgt, dir);",
            "}",
            "void dma_vunmap_noncontiguous(struct device *dev, void *vaddr)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tif (ops && ops->alloc_noncontiguous)",
            "\t\tvunmap(vaddr);",
            "}",
            "int dma_mmap_noncontiguous(struct device *dev, struct vm_area_struct *vma,",
            "\t\tsize_t size, struct sg_table *sgt)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tif (ops && ops->alloc_noncontiguous) {",
            "\t\tunsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;",
            "",
            "\t\tif (vma->vm_pgoff >= count ||",
            "\t\t    vma_pages(vma) > count - vma->vm_pgoff)",
            "\t\t\treturn -ENXIO;",
            "\t\treturn vm_map_pages(vma, sgt_handle(sgt)->pages, count);",
            "\t}",
            "\treturn dma_mmap_pages(dev, vma, size, sg_page(sgt->sgl));",
            "}",
            "static int dma_supported(struct device *dev, u64 mask)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\t/*",
            "\t * ->dma_supported sets the bypass flag, so we must always call",
            "\t * into the method here unless the device is truly direct mapped.",
            "\t */",
            "\tif (!ops)",
            "\t\treturn dma_direct_supported(dev, mask);",
            "\tif (!ops->dma_supported)",
            "\t\treturn 1;",
            "\treturn ops->dma_supported(dev, mask);",
            "}",
            "bool dma_pci_p2pdma_supported(struct device *dev)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\t/* if ops is not set, dma direct will be used which supports P2PDMA */",
            "\tif (!ops)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Note: dma_ops_bypass is not checked here because P2PDMA should",
            "\t * not be used with dma mapping ops that do not have support even",
            "\t * if the specific device is bypassing them.",
            "\t */",
            "",
            "\treturn ops->flags & DMA_F_PCI_P2PDMA_SUPPORTED;",
            "}",
            "int dma_set_mask(struct device *dev, u64 mask)",
            "{",
            "\t/*",
            "\t * Truncate the mask to the actually supported dma_addr_t width to",
            "\t * avoid generating unsupportable addresses.",
            "\t */",
            "\tmask = (dma_addr_t)mask;",
            "",
            "\tif (!dev->dma_mask || !dma_supported(dev, mask))",
            "\t\treturn -EIO;",
            "",
            "\tarch_dma_set_mask(dev, mask);",
            "\t*dev->dma_mask = mask;",
            "\treturn 0;",
            "}",
            "int dma_set_coherent_mask(struct device *dev, u64 mask)",
            "{",
            "\t/*",
            "\t * Truncate the mask to the actually supported dma_addr_t width to",
            "\t * avoid generating unsupportable addresses.",
            "\t */",
            "\tmask = (dma_addr_t)mask;",
            "",
            "\tif (!dma_supported(dev, mask))",
            "\t\treturn -EIO;",
            "",
            "\tdev->coherent_dma_mask = mask;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "dma_mmap_pages, free_single_sgt, dma_free_noncontiguous, dma_vunmap_noncontiguous, dma_mmap_noncontiguous, dma_supported, dma_pci_p2pdma_supported, dma_set_mask, dma_set_coherent_mask",
          "description": "处理非连续物理内存的映射与释放，dma_mmap_noncontiguous通过SG表实现页框映射；dma_set_mask/coherent_mask配置DMA地址掩码；dma_pci_p2pdma_supported检测设备对P2PDMA的支持状态。",
          "similarity": 0.5260905027389526
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/dma/mapping.c",
          "start_line": 399,
          "end_line": 503,
          "content": [
            "int dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tif (dma_alloc_direct(dev, ops))",
            "\t\treturn dma_direct_get_sgtable(dev, sgt, cpu_addr, dma_addr,",
            "\t\t\t\tsize, attrs);",
            "\tif (!ops->get_sgtable)",
            "\t\treturn -ENXIO;",
            "\treturn ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size, attrs);",
            "}",
            "pgprot_t dma_pgprot(struct device *dev, pgprot_t prot, unsigned long attrs)",
            "{",
            "\tif (dev_is_dma_coherent(dev))",
            "\t\treturn prot;",
            "#ifdef CONFIG_ARCH_HAS_DMA_WRITE_COMBINE",
            "\tif (attrs & DMA_ATTR_WRITE_COMBINE)",
            "\t\treturn pgprot_writecombine(prot);",
            "#endif",
            "\treturn pgprot_dmacoherent(prot);",
            "}",
            "bool dma_can_mmap(struct device *dev)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tif (dma_alloc_direct(dev, ops))",
            "\t\treturn dma_direct_can_mmap(dev);",
            "\treturn ops->mmap != NULL;",
            "}",
            "int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tif (dma_alloc_direct(dev, ops))",
            "\t\treturn dma_direct_mmap(dev, vma, cpu_addr, dma_addr, size,",
            "\t\t\t\tattrs);",
            "\tif (!ops->mmap)",
            "\t\treturn -ENXIO;",
            "\treturn ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);",
            "}",
            "u64 dma_get_required_mask(struct device *dev)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tif (dma_alloc_direct(dev, ops))",
            "\t\treturn dma_direct_get_required_mask(dev);",
            "\tif (ops->get_required_mask)",
            "\t\treturn ops->get_required_mask(dev);",
            "",
            "\t/*",
            "\t * We require every DMA ops implementation to at least support a 32-bit",
            "\t * DMA mask (and use bounce buffering if that isn't supported in",
            "\t * hardware).  As the direct mapping code has its own routine to",
            "\t * actually report an optimal mask we default to 32-bit here as that",
            "\t * is the right thing for most IOMMUs, and at least not actively",
            "\t * harmful in general.",
            "\t */",
            "\treturn DMA_BIT_MASK(32);",
            "}",
            "void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,",
            "\t\tdma_addr_t dma_handle, unsigned long attrs)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tif (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))",
            "\t\treturn;",
            "\t/*",
            "\t * On non-coherent platforms which implement DMA-coherent buffers via",
            "\t * non-cacheable remaps, ops->free() may call vunmap(). Thus getting",
            "\t * this far in IRQ context is a) at risk of a BUG_ON() or trying to",
            "\t * sleep on some machines, and b) an indication that the driver is",
            "\t * probably misusing the coherent API anyway.",
            "\t */",
            "\tWARN_ON(irqs_disabled());",
            "",
            "\tif (!cpu_addr)",
            "\t\treturn;",
            "",
            "\tdebug_dma_free_coherent(dev, size, cpu_addr, dma_handle);",
            "\tif (dma_alloc_direct(dev, ops))",
            "\t\tdma_direct_free(dev, size, cpu_addr, dma_handle, attrs);",
            "\telse if (ops->free)",
            "\t\tops->free(dev, size, cpu_addr, dma_handle, attrs);",
            "}",
            "static void __dma_free_pages(struct device *dev, size_t size, struct page *page,",
            "\t\tdma_addr_t dma_handle, enum dma_data_direction dir)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tsize = PAGE_ALIGN(size);",
            "\tif (dma_alloc_direct(dev, ops))",
            "\t\tdma_direct_free_pages(dev, size, page, dma_handle, dir);",
            "\telse if (ops->free_pages)",
            "\t\tops->free_pages(dev, size, page, dma_handle, dir);",
            "}",
            "void dma_free_pages(struct device *dev, size_t size, struct page *page,",
            "\t\tdma_addr_t dma_handle, enum dma_data_direction dir)",
            "{",
            "\tdebug_dma_unmap_page(dev, dma_handle, size, dir);",
            "\t__dma_free_pages(dev, size, page, dma_handle, dir);",
            "}"
          ],
          "function_name": "dma_get_sgtable_attrs, dma_pgprot, dma_can_mmap, dma_mmap_attrs, dma_get_required_mask, dma_free_attrs, __dma_free_pages, dma_free_pages",
          "description": "实现DMA地址转换相关功能，dma_get_sgtable_attrs生成SG表并设置页面保护属性；dma_get_required_mask确定设备所需DMA掩码；dma_free_attrs释放带属性的DMA缓冲区，区分直接映射与通用操作路径。",
          "similarity": 0.517382025718689
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/dma/mapping.c",
          "start_line": 796,
          "end_line": 834,
          "content": [
            "size_t dma_max_mapping_size(struct device *dev)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "\tsize_t size = SIZE_MAX;",
            "",
            "\tif (dma_map_direct(dev, ops))",
            "\t\tsize = dma_direct_max_mapping_size(dev);",
            "\telse if (ops && ops->max_mapping_size)",
            "\t\tsize = ops->max_mapping_size(dev);",
            "",
            "\treturn size;",
            "}",
            "size_t dma_opt_mapping_size(struct device *dev)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "\tsize_t size = SIZE_MAX;",
            "",
            "\tif (ops && ops->opt_mapping_size)",
            "\t\tsize = ops->opt_mapping_size();",
            "",
            "\treturn min(dma_max_mapping_size(dev), size);",
            "}",
            "bool dma_need_sync(struct device *dev, dma_addr_t dma_addr)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tif (dma_map_direct(dev, ops))",
            "\t\treturn dma_direct_need_sync(dev, dma_addr);",
            "\treturn ops->sync_single_for_cpu || ops->sync_single_for_device;",
            "}",
            "unsigned long dma_get_merge_boundary(struct device *dev)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tif (!ops || !ops->get_merge_boundary)",
            "\t\treturn 0;\t/* can't merge */",
            "",
            "\treturn ops->get_merge_boundary(dev);",
            "}"
          ],
          "function_name": "dma_max_mapping_size, dma_opt_mapping_size, dma_need_sync, dma_get_merge_boundary",
          "description": "这段代码实现了DMA映射相关的参数查询功能，包含四个关键函数：  \n1. `dma_max_mapping_size` 获取设备最大映射尺寸，优先采用直接映射逻辑或DMA操作符的定制实现；  \n2. `dma_opt_mapping_size` 返回最优映射尺寸与最大尺寸的最小值，依赖于DMA操作符的回调接口；  \n3. `dma_need_sync` 判断DMA地址是否需同步，通过直接映射逻辑或操作符的同步标志位决定；  \n4. `dma_get_merge_boundary` 查询DMA合并边界，若操作符未实现则返回0。  \n\n注：代码片段未包含`dma_map_direct`等辅助函数定义，部分条件分支依赖上下文完整的DMA操作符结构体实现。",
          "similarity": 0.49226829409599304
        }
      ]
    },
    {
      "source_file": "kernel/dma/direct.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:12:42\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\direct.c`\n\n---\n\n# `dma/direct.c` 技术文档\n\n## 1. 文件概述\n\n`dma/direct.c` 实现了 Linux 内核中 **DMA 直接映射操作（DMA direct mapping）** 的核心逻辑。该文件提供了一套不依赖 IOMMU 的 DMA 内存分配与映射机制，适用于物理地址可直接被设备访问的平台（如 x86、ARM64 等无 IOMMU 或 IOMMU 被禁用的场景）。其核心思想是将物理内存地址直接转换为设备可见的 DMA 地址，避免复杂的地址转换开销。\n\n该实现支持多种内存分配策略，包括：\n- 连续物理内存分配（CMA 或 buddy allocator）\n- SWIOTLB 回退机制（用于处理高地址设备无法访问的情况）\n- 原子池分配（用于不可阻塞上下文）\n- 高端内存重映射\n- 内存加密/解密（如 AMD SEV、Intel TDX 等安全特性）\n\n## 2. 核心功能\n\n### 全局变量\n- `zone_dma_bits`：定义 ZONE_DMA 的地址位宽（默认 24 位，即 16MB），可由架构代码覆盖。\n\n### 主要函数\n| 函数名 | 功能描述 |\n|--------|--------|\n| `phys_to_dma_direct()` | 将物理地址转换为 DMA 地址，支持强制解密场景 |\n| `dma_direct_to_page()` | 通过 DMA 地址反查对应的 `struct page` |\n| `dma_direct_get_required_mask()` | 计算设备所需的 DMA 地址掩码（基于系统最大物理地址）|\n| `dma_coherent_ok()` | 检查给定物理地址范围是否在设备的 DMA 地址能力范围内 |\n| `dma_direct_alloc()` | **主入口函数**：为设备分配 DMA 内存，支持多种属性和回退策略 |\n| `__dma_direct_alloc_pages()` | 底层页面分配函数，尝试最优内存区域并回退到低地址区域 |\n| `dma_direct_alloc_from_pool()` | 从原子池分配不可阻塞的 DMA 内存 |\n| `dma_direct_alloc_no_mapping()` | 分配无内核虚拟映射的 DMA 内存（返回 `struct page*`）|\n| `dma_set_decrypted()` / `dma_set_encrypted()` | 设置内存页为解密/加密状态（用于安全虚拟化）|\n\n### 辅助函数\n- `dma_direct_optimal_gfp_mask()`：根据设备 DMA 限制选择最优的 GFP 标志（GFP_DMA / GFP_DMA32）\n- `__dma_direct_free_pages()`：释放通过直接分配获得的页面（优先尝试 SWIOTLB 释放）\n\n## 3. 关键实现\n\n### 3.1 DMA 地址空间约束处理\n- 使用 `dev->coherent_dma_mask` 和 `dev->bus_dma_limit` 确定设备可寻址的物理地址上限。\n- 通过 `dma_coherent_ok()` 验证分配的物理内存是否在设备可访问范围内。\n- 若分配的内存超出范围，则回退到更低地址区域（先尝试 GFP_DMA32，再尝试 GFP_DMA）。\n\n### 3.2 多层次内存分配策略\n1. **首选 CMA 连续内存**：通过 `dma_alloc_contiguous()` 分配。\n2. **回退到 buddy allocator**：使用 `alloc_pages_node()`。\n3. **SWIOTLB 支持**：当设备无法访问高地址时，通过 `swiotlb_alloc()` 分配 bounce buffer。\n4. **原子上下文支持**：在不可阻塞场景下使用 `dma_direct_alloc_from_pool()` 从预分配池中获取内存。\n\n### 3.3 非一致性缓存与内存属性处理\n- 对于非一致性缓存架构（`!dev_is_dma_coherent()`）：\n  - 优先使用全局一致性内存池（`CONFIG_DMA_GLOBAL_POOL`）\n  - 或启用重映射（`CONFIG_DMA_DIRECT_REMAP`）创建 uncached 映射\n  - 或调用架构特定的 `arch_dma_alloc()`\n- 调用 `arch_dma_prep_coherent()` 清理内核别名的脏缓存行。\n\n### 3.4 安全内存处理（加密/解密）\n- 当 `force_dma_unencrypted(dev)` 为真（如 SEV 环境），分配的内存需标记为解密。\n- 使用 `set_memory_decrypted()` / `set_memory_encrypted()` 修改页表属性。\n- 解密操作可能阻塞，因此在原子上下文中需使用内存池。\n\n### 3.5 高端内存（HighMem）处理\n- 若分配的页面位于高端内存（`PageHighMem`），则必须通过 `dma_common_contiguous_remap()` 创建内核虚拟地址映射。\n- 重映射时应用设备特定的页保护属性（`dma_pgprot()`）并处理解密需求。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- `<linux/memblock.h>`：获取 `max_pfn` 系统最大页帧号\n- `<linux/dma-map-ops.h>`：DMA 映射操作抽象接口\n- `<linux/scatterlist.h>`：SG 表支持（间接依赖）\n- `<linux/set_memory.h>`：内存加密/解密操作（`set_memory_decrypted` 等）\n- `<linux/vmalloc.h>`：高端内存重映射支持\n- `\"direct.h\"`：本地 DMA direct 实现的私有头文件\n\n### 配置选项依赖\n- `CONFIG_SWIOTLB`：SWIOTLB bounce buffer 支持\n- `CONFIG_DMA_COHERENT_POOL`：原子上下文 DMA 内存池\n- `CONFIG_DMA_GLOBAL_POOL`：全局一致性 DMA 内存池\n- `CONFIG_DMA_DIRECT_REMAP`：非一致性设备的重映射支持\n- `CONFIG_ZONE_DMA` / `CONFIG_ZONE_DMA32`：低地址内存区域支持\n- `CONFIG_ARCH_HAS_DMA_SET_UNCACHED`：架构特定 uncached 映射支持\n\n### 架构相关依赖\n- `phys_to_dma()` / `dma_to_phys()`：架构提供的物理地址与 DMA 地址转换函数\n- `arch_dma_prep_coherent()`：架构特定的缓存一致性准备\n- `arch_dma_alloc()`：架构特定的 DMA 分配回退路径\n\n## 5. 使用场景\n\n### 5.1 设备驱动 DMA 分配\n- 驱动调用 `dma_alloc_coherent()` 或 `dma_alloc_attrs()` 时，若系统未启用 IOMMU，则最终由 `dma_direct_alloc()` 处理。\n- 适用于大多数无 IOMMU 的嵌入式系统、传统 PC 平台或 IOMMU 被显式禁用的场景。\n\n### 5.2 安全虚拟化环境\n- 在 AMD SEV 或 Intel TDX 等机密计算环境中，DMA 内存需标记为“解密”，该文件通过 `force_dma_unencrypted()` 机制实现。\n\n### 5.3 资源受限或实时系统\n- 通过 `DMA_ATTR_NO_KERNEL_MAPPING` 属性分配无内核映射的 DMA 内存，减少 TLB 压力。\n- 在中断上下文等不可阻塞场景中，自动使用原子内存池分配。\n\n### 5.4 老旧设备兼容\n- 对仅支持 32 位或 24 位 DMA 地址的设备，自动分配低地址内存（通过 GFP_DMA32/GFP_DMA）并验证地址范围。\n\n### 5.5 高端内存平台\n- 在 32 位系统或内存大于直接映射区域的平台上，自动处理高端内存的重映射，确保返回有效的内核虚拟地址。",
      "similarity": 0.5754282474517822,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/direct.c",
          "start_line": 25,
          "end_line": 140,
          "content": [
            "static inline dma_addr_t phys_to_dma_direct(struct device *dev,",
            "\t\tphys_addr_t phys)",
            "{",
            "\tif (force_dma_unencrypted(dev))",
            "\t\treturn phys_to_dma_unencrypted(dev, phys);",
            "\treturn phys_to_dma(dev, phys);",
            "}",
            "u64 dma_direct_get_required_mask(struct device *dev)",
            "{",
            "\tphys_addr_t phys = (phys_addr_t)(max_pfn - 1) << PAGE_SHIFT;",
            "\tu64 max_dma = phys_to_dma_direct(dev, phys);",
            "",
            "\treturn (1ULL << (fls64(max_dma) - 1)) * 2 - 1;",
            "}",
            "static gfp_t dma_direct_optimal_gfp_mask(struct device *dev, u64 *phys_limit)",
            "{",
            "\tu64 dma_limit = min_not_zero(",
            "\t\tdev->coherent_dma_mask,",
            "\t\tdev->bus_dma_limit);",
            "",
            "\t/*",
            "\t * Optimistically try the zone that the physical address mask falls",
            "\t * into first.  If that returns memory that isn't actually addressable",
            "\t * we will fallback to the next lower zone and try again.",
            "\t *",
            "\t * Note that GFP_DMA32 and GFP_DMA are no ops without the corresponding",
            "\t * zones.",
            "\t */",
            "\t*phys_limit = dma_to_phys(dev, dma_limit);",
            "\tif (*phys_limit <= DMA_BIT_MASK(zone_dma_bits))",
            "\t\treturn GFP_DMA;",
            "\tif (*phys_limit <= DMA_BIT_MASK(32))",
            "\t\treturn GFP_DMA32;",
            "\treturn 0;",
            "}",
            "bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)",
            "{",
            "\tdma_addr_t dma_addr = phys_to_dma_direct(dev, phys);",
            "",
            "\tif (dma_addr == DMA_MAPPING_ERROR)",
            "\t\treturn false;",
            "\treturn dma_addr + size - 1 <=",
            "\t\tmin_not_zero(dev->coherent_dma_mask, dev->bus_dma_limit);",
            "}",
            "static int dma_set_decrypted(struct device *dev, void *vaddr, size_t size)",
            "{",
            "\tif (!force_dma_unencrypted(dev))",
            "\t\treturn 0;",
            "\treturn set_memory_decrypted((unsigned long)vaddr, PFN_UP(size));",
            "}",
            "static int dma_set_encrypted(struct device *dev, void *vaddr, size_t size)",
            "{",
            "\tint ret;",
            "",
            "\tif (!force_dma_unencrypted(dev))",
            "\t\treturn 0;",
            "\tret = set_memory_encrypted((unsigned long)vaddr, PFN_UP(size));",
            "\tif (ret)",
            "\t\tpr_warn_ratelimited(\"leaking DMA memory that can't be re-encrypted\\n\");",
            "\treturn ret;",
            "}",
            "static void __dma_direct_free_pages(struct device *dev, struct page *page,",
            "\t\t\t\t    size_t size)",
            "{",
            "\tif (swiotlb_free(dev, page, size))",
            "\t\treturn;",
            "\tdma_free_contiguous(dev, page, size);",
            "}",
            "static bool dma_direct_use_pool(struct device *dev, gfp_t gfp)",
            "{",
            "\treturn !gfpflags_allow_blocking(gfp) && !is_swiotlb_for_alloc(dev);",
            "}",
            "void dma_direct_free(struct device *dev, size_t size,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, unsigned long attrs)",
            "{",
            "\tunsigned int page_order = get_order(size);",
            "",
            "\tif ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) &&",
            "\t    !force_dma_unencrypted(dev) && !is_swiotlb_for_alloc(dev)) {",
            "\t\t/* cpu_addr is a struct page cookie, not a kernel address */",
            "\t\tdma_free_contiguous(dev, cpu_addr, size);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_SET_UNCACHED) &&",
            "\t    !IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&",
            "\t    !IS_ENABLED(CONFIG_DMA_GLOBAL_POOL) &&",
            "\t    !dev_is_dma_coherent(dev) &&",
            "\t    !is_swiotlb_for_alloc(dev)) {",
            "\t\tarch_dma_free(dev, size, cpu_addr, dma_addr, attrs);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_DMA_GLOBAL_POOL) &&",
            "\t    !dev_is_dma_coherent(dev)) {",
            "\t\tif (!dma_release_from_global_coherent(page_order, cpu_addr))",
            "\t\t\tWARN_ON_ONCE(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */",
            "\tif (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&",
            "\t    dma_free_from_pool(dev, cpu_addr, PAGE_ALIGN(size)))",
            "\t\treturn;",
            "",
            "\tif (is_vmalloc_addr(cpu_addr)) {",
            "\t\tvunmap(cpu_addr);",
            "\t} else {",
            "\t\tif (IS_ENABLED(CONFIG_ARCH_HAS_DMA_CLEAR_UNCACHED))",
            "\t\t\tarch_dma_clear_uncached(cpu_addr, size);",
            "\t\tif (dma_set_encrypted(dev, cpu_addr, size))",
            "\t\t\treturn;",
            "\t}",
            "",
            "\t__dma_direct_free_pages(dev, dma_direct_to_page(dev, dma_addr), size);",
            "}"
          ],
          "function_name": "phys_to_dma_direct, dma_direct_get_required_mask, dma_direct_optimal_gfp_mask, dma_coherent_ok, dma_set_decrypted, dma_set_encrypted, __dma_direct_free_pages, dma_direct_use_pool, dma_direct_free",
          "description": "实现了DMA直接映射的核心辅助函数，包括物理地址转DMA地址、计算DMA掩码、优化内存分配策略、检查DMA一致性及加密内存设置等功能。dma_direct_free处理不同条件下的内存释放逻辑，涉及SWIOTLB、原子池和架构特定的释放路径。",
          "similarity": 0.5682024955749512
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/dma/direct.c",
          "start_line": 520,
          "end_line": 633,
          "content": [
            "dma_addr_t dma_direct_map_resource(struct device *dev, phys_addr_t paddr,",
            "\t\tsize_t size, enum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tdma_addr_t dma_addr = paddr;",
            "",
            "\tif (unlikely(!dma_capable(dev, dma_addr, size, false))) {",
            "\t\tdev_err_once(dev,",
            "\t\t\t     \"DMA addr %pad+%zu overflow (mask %llx, bus limit %llx).\\n\",",
            "\t\t\t     &dma_addr, size, *dev->dma_mask, dev->bus_dma_limit);",
            "\t\tWARN_ON_ONCE(1);",
            "\t\treturn DMA_MAPPING_ERROR;",
            "\t}",
            "",
            "\treturn dma_addr;",
            "}",
            "int dma_direct_get_sgtable(struct device *dev, struct sg_table *sgt,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "\tstruct page *page = dma_direct_to_page(dev, dma_addr);",
            "\tint ret;",
            "",
            "\tret = sg_alloc_table(sgt, 1, GFP_KERNEL);",
            "\tif (!ret)",
            "\t\tsg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);",
            "\treturn ret;",
            "}",
            "bool dma_direct_can_mmap(struct device *dev)",
            "{",
            "\treturn dev_is_dma_coherent(dev) ||",
            "\t\tIS_ENABLED(CONFIG_DMA_NONCOHERENT_MMAP);",
            "}",
            "int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "\tunsigned long user_count = vma_pages(vma);",
            "\tunsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;",
            "\tunsigned long pfn = PHYS_PFN(dma_to_phys(dev, dma_addr));",
            "\tint ret = -ENXIO;",
            "",
            "\tvma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);",
            "\tif (force_dma_unencrypted(dev))",
            "\t\tvma->vm_page_prot = pgprot_decrypted(vma->vm_page_prot);",
            "",
            "\tif (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))",
            "\t\treturn ret;",
            "\tif (dma_mmap_from_global_coherent(vma, cpu_addr, size, &ret))",
            "\t\treturn ret;",
            "",
            "\tif (vma->vm_pgoff >= count || user_count > count - vma->vm_pgoff)",
            "\t\treturn -ENXIO;",
            "\treturn remap_pfn_range(vma, vma->vm_start, pfn + vma->vm_pgoff,",
            "\t\t\tuser_count << PAGE_SHIFT, vma->vm_page_prot);",
            "}",
            "int dma_direct_supported(struct device *dev, u64 mask)",
            "{",
            "\tu64 min_mask = (max_pfn - 1) << PAGE_SHIFT;",
            "",
            "\t/*",
            "\t * Because 32-bit DMA masks are so common we expect every architecture",
            "\t * to be able to satisfy them - either by not supporting more physical",
            "\t * memory, or by providing a ZONE_DMA32.  If neither is the case, the",
            "\t * architecture needs to use an IOMMU instead of the direct mapping.",
            "\t */",
            "\tif (mask >= DMA_BIT_MASK(32))",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * This check needs to be against the actual bit mask value, so use",
            "\t * phys_to_dma_unencrypted() here so that the SME encryption mask isn't",
            "\t * part of the check.",
            "\t */",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA))",
            "\t\tmin_mask = min_t(u64, min_mask, DMA_BIT_MASK(zone_dma_bits));",
            "\treturn mask >= phys_to_dma_unencrypted(dev, min_mask);",
            "}",
            "size_t dma_direct_max_mapping_size(struct device *dev)",
            "{",
            "\t/* If SWIOTLB is active, use its maximum mapping size */",
            "\tif (is_swiotlb_active(dev) &&",
            "\t    (dma_addressing_limited(dev) || is_swiotlb_force_bounce(dev)))",
            "\t\treturn swiotlb_max_mapping_size(dev);",
            "\treturn SIZE_MAX;",
            "}",
            "bool dma_direct_need_sync(struct device *dev, dma_addr_t dma_addr)",
            "{",
            "\treturn !dev_is_dma_coherent(dev) ||",
            "\t       is_swiotlb_buffer(dev, dma_to_phys(dev, dma_addr));",
            "}",
            "int dma_direct_set_offset(struct device *dev, phys_addr_t cpu_start,",
            "\t\t\t dma_addr_t dma_start, u64 size)",
            "{",
            "\tstruct bus_dma_region *map;",
            "\tu64 offset = (u64)cpu_start - (u64)dma_start;",
            "",
            "\tif (dev->dma_range_map) {",
            "\t\tdev_err(dev, \"attempt to add DMA range to existing map\\n\");",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tif (!offset)",
            "\t\treturn 0;",
            "",
            "\tmap = kcalloc(2, sizeof(*map), GFP_KERNEL);",
            "\tif (!map)",
            "\t\treturn -ENOMEM;",
            "\tmap[0].cpu_start = cpu_start;",
            "\tmap[0].dma_start = dma_start;",
            "\tmap[0].offset = offset;",
            "\tmap[0].size = size;",
            "\tdev->dma_range_map = map;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "dma_direct_map_resource, dma_direct_get_sgtable, dma_direct_can_mmap, dma_direct_mmap, dma_direct_supported, dma_direct_max_mapping_size, dma_direct_need_sync, dma_direct_set_offset",
          "description": "包含DMA直接映射的资源映射、SG表构建、内存映射支持性检测及最大映射尺寸计算等功能。dma_direct_mmap实现设备内存的VMA映射，dma_direct_supported验证DMA掩码兼容性，dma_direct_set_offset用于配置DMA地址偏移量。",
          "similarity": 0.560196578502655
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/direct.c",
          "start_line": 1,
          "end_line": 24,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 2018-2020 Christoph Hellwig.",
            " *",
            " * DMA operations that map physical memory directly without using an IOMMU.",
            " */",
            "#include <linux/memblock.h> /* for max_pfn */",
            "#include <linux/export.h>",
            "#include <linux/mm.h>",
            "#include <linux/dma-map-ops.h>",
            "#include <linux/scatterlist.h>",
            "#include <linux/pfn.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/slab.h>",
            "#include \"direct.h\"",
            "",
            "/*",
            " * Most architectures use ZONE_DMA for the first 16 Megabytes, but some use",
            " * it for entirely different regions. In that case the arch code needs to",
            " * override the variable below for dma-direct to work properly.",
            " */",
            "unsigned int zone_dma_bits __ro_after_init = 24;",
            ""
          ],
          "function_name": null,
          "description": "定义了用于DMA直接映射的全局变量zone_dma_bits，默认值为24位，表示DMA地址空间的位宽。该变量用于控制DMA操作的物理地址范围，架构代码可通过覆盖此变量调整DMA直接映射的行为。",
          "similarity": 0.5098603963851929
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/dma/direct.c",
          "start_line": 391,
          "end_line": 503,
          "content": [
            "void dma_direct_free_pages(struct device *dev, size_t size,",
            "\t\tstruct page *page, dma_addr_t dma_addr,",
            "\t\tenum dma_data_direction dir)",
            "{",
            "\tvoid *vaddr = page_address(page);",
            "",
            "\t/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */",
            "\tif (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&",
            "\t    dma_free_from_pool(dev, vaddr, size))",
            "\t\treturn;",
            "",
            "\tif (dma_set_encrypted(dev, vaddr, size))",
            "\t\treturn;",
            "\t__dma_direct_free_pages(dev, page, size);",
            "}",
            "void dma_direct_sync_sg_for_device(struct device *dev,",
            "\t\tstruct scatterlist *sgl, int nents, enum dma_data_direction dir)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tphys_addr_t paddr = dma_to_phys(dev, sg_dma_address(sg));",
            "",
            "\t\tif (unlikely(is_swiotlb_buffer(dev, paddr)))",
            "\t\t\tswiotlb_sync_single_for_device(dev, paddr, sg->length,",
            "\t\t\t\t\t\t       dir);",
            "",
            "\t\tif (!dev_is_dma_coherent(dev))",
            "\t\t\tarch_sync_dma_for_device(paddr, sg->length,",
            "\t\t\t\t\tdir);",
            "\t}",
            "}",
            "void dma_direct_sync_sg_for_cpu(struct device *dev,",
            "\t\tstruct scatterlist *sgl, int nents, enum dma_data_direction dir)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tphys_addr_t paddr = dma_to_phys(dev, sg_dma_address(sg));",
            "",
            "\t\tif (!dev_is_dma_coherent(dev))",
            "\t\t\tarch_sync_dma_for_cpu(paddr, sg->length, dir);",
            "",
            "\t\tif (unlikely(is_swiotlb_buffer(dev, paddr)))",
            "\t\t\tswiotlb_sync_single_for_cpu(dev, paddr, sg->length,",
            "\t\t\t\t\t\t    dir);",
            "",
            "\t\tif (dir == DMA_FROM_DEVICE)",
            "\t\t\tarch_dma_mark_clean(paddr, sg->length);",
            "\t}",
            "",
            "\tif (!dev_is_dma_coherent(dev))",
            "\t\tarch_sync_dma_for_cpu_all();",
            "}",
            "void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,",
            "\t\tint nents, enum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl,  sg, nents, i) {",
            "\t\tif (sg_dma_is_bus_address(sg))",
            "\t\t\tsg_dma_unmark_bus_address(sg);",
            "\t\telse",
            "\t\t\tdma_direct_unmap_page(dev, sg->dma_address,",
            "\t\t\t\t\t      sg_dma_len(sg), dir, attrs);",
            "\t}",
            "}",
            "int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,",
            "\t\tenum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tstruct pci_p2pdma_map_state p2pdma_state = {};",
            "\tenum pci_p2pdma_map_type map;",
            "\tstruct scatterlist *sg;",
            "\tint i, ret;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tif (is_pci_p2pdma_page(sg_page(sg))) {",
            "\t\t\tmap = pci_p2pdma_map_segment(&p2pdma_state, dev, sg);",
            "\t\t\tswitch (map) {",
            "\t\t\tcase PCI_P2PDMA_MAP_BUS_ADDR:",
            "\t\t\t\tcontinue;",
            "\t\t\tcase PCI_P2PDMA_MAP_THRU_HOST_BRIDGE:",
            "\t\t\t\t/*",
            "\t\t\t\t * Any P2P mapping that traverses the PCI",
            "\t\t\t\t * host bridge must be mapped with CPU physical",
            "\t\t\t\t * address and not PCI bus addresses. This is",
            "\t\t\t\t * done with dma_direct_map_page() below.",
            "\t\t\t\t */",
            "\t\t\t\tbreak;",
            "\t\t\tdefault:",
            "\t\t\t\tret = -EREMOTEIO;",
            "\t\t\t\tgoto out_unmap;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tsg->dma_address = dma_direct_map_page(dev, sg_page(sg),",
            "\t\t\t\tsg->offset, sg->length, dir, attrs);",
            "\t\tif (sg->dma_address == DMA_MAPPING_ERROR) {",
            "\t\t\tret = -EIO;",
            "\t\t\tgoto out_unmap;",
            "\t\t}",
            "\t\tsg_dma_len(sg) = sg->length;",
            "\t}",
            "",
            "\treturn nents;",
            "",
            "out_unmap:",
            "\tdma_direct_unmap_sg(dev, sgl, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "dma_direct_free_pages, dma_direct_sync_sg_for_device, dma_direct_sync_sg_for_cpu, dma_direct_unmap_sg, dma_direct_map_sg",
          "description": "提供了SCATTERLIST的同步、解映射和映射操作实现，包括对非一致内存的架构同步、PCI P2P DMA的特殊处理以及SG表的构建。sync_sg系列函数负责设备与CPU之间的数据缓存一致性维护。",
          "similarity": 0.4304473400115967
        }
      ]
    },
    {
      "source_file": "kernel/dma/remap.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:16:22\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\remap.c`\n\n---\n\n# `dma/remap.c` 技术文档\n\n## 1. 文件概述\n\n`dma/remap.c` 是 Linux 内核中用于 DMA（Direct Memory Access）一致性内存管理的辅助实现文件。该文件提供了一组通用函数，用于将物理页面（`struct page`）重新映射到内核虚拟地址空间中，并标记为 DMA 一致性映射区域（`VM_DMA_COHERENT`）。这些函数主要用于支持架构无关的 DMA 映射操作，特别是在需要将非连续物理页或连续物理内存块映射为连续虚拟地址的场景中。\n\n## 2. 核心功能\n\n### 主要函数：\n\n- **`dma_common_find_pages(void *cpu_addr)`**  \n  根据给定的内核虚拟地址 `cpu_addr`，查找其对应的 `struct page` 数组。该地址必须是由 `dma_common_*_remap` 系列函数创建的、标记为 `VM_DMA_COHERENT` 的 vmalloc 区域。\n\n- **`dma_common_pages_remap(struct page **pages, size_t size, pgprot_t prot, const void *caller)`**  \n  将一组非连续的物理页面（由 `pages` 数组指定）重新映射为一个连续的内核虚拟地址区域，并标记为 `VM_DMA_COHERENT`。该函数不可在原子上下文（如中断处理程序）中调用。\n\n- **`dma_common_contiguous_remap(struct page *page, size_t size, pgprot_t prot, const void *caller)`**  \n  将一段物理上连续的内存区域（起始于 `page`，长度为 `size`）重新映射为连续的内核虚拟地址，并标记为 `VM_DMA_COHERENT`。内部会临时分配一个 `struct page *` 数组来描述每一页。\n\n- **`dma_common_free_remap(void *cpu_addr, size_t size)`**  \n  释放由上述 `remap` 函数创建的虚拟映射区域。会验证该区域是否为有效的 `VM_DMA_COHERENT` 类型，若无效则触发警告。\n\n### 数据结构：\n- 无显式定义新数据结构，但依赖于内核已有的：\n  - `struct page`\n  - `struct vm_struct`\n  - `pgprot_t`\n\n## 3. 关键实现\n\n- **VM 区域标识**：所有通过 `dma_common_*_remap` 创建的映射区域均使用 `VM_DMA_COHERENT` 标志，以便后续可通过 `find_vm_area()` 识别其为 DMA 一致性映射区域。\n  \n- **页面数组管理**：\n  - `dma_common_pages_remap` 直接使用传入的 `pages` 数组，并在成功 `vmap` 后将其保存到 `vm_struct->pages` 字段中，供 `dma_common_find_pages` 查询。\n  - `dma_common_contiguous_remap` 针对连续物理内存，动态构建 `pages` 数组（使用 `kvmalloc_array`），调用 `vmap` 后立即释放该临时数组，但 `vmap` 内部会复制页面指针。\n\n- **内存分配与映射**：\n  - 使用 `vmap()` 将物理页面映射到 vmalloc 区域，确保返回的虚拟地址在内核空间连续。\n  - 使用 `kvmalloc_array`/`kvfree` 进行临时内存分配，兼顾大内存分配的可靠性（可回退到 vmalloc）。\n\n- **错误处理与调试**：\n  - `dma_common_free_remap` 中包含 `WARN(1, ...)`，用于检测非法释放操作，提升调试能力。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/dma-map-ops.h>`：提供 DMA 映射操作相关的类型和接口。\n  - `<linux/slab.h>`：提供 `kvmalloc_array`/`kvfree` 等内存分配接口。\n  - `<linux/vmalloc.h>`：提供 `vmap`、`vunmap`、`find_vm_area` 等 vmalloc 区域管理函数。\n\n- **内核子系统依赖**：\n  - **VMALLOC 子系统**：依赖 `vmap`/`vunmap` 实现虚拟地址映射。\n  - **内存管理子系统（MM）**：依赖 `struct page` 和页面操作函数（如 `nth_page`）。\n  - **DMA 子系统**：作为 `dma_map_ops` 的底层支持，被架构特定的 DMA 实现（如 ARM、ARM64）调用。\n\n## 5. 使用场景\n\n- **DMA 一致性内存分配**：当设备驱动需要分配大块 DMA 一致性内存，且底层无法直接提供连续虚拟地址时，可通过此模块将物理页重新映射为连续虚拟地址。\n  \n- **IOMMU 或非一致性缓存架构支持**：在缓存不一致的系统（如某些 ARM 平台）上，为保证 CPU 与设备对内存视图一致，需使用特殊页表属性（`pgprot_t`）进行映射，本模块提供通用封装。\n\n- **通用 DMA 映射框架后端**：作为 `dma_map_ops` 中 `alloc`/`free` 等操作的辅助实现，被 `dma-direct.c`、`arm_dma_alloc.c` 等架构相关代码调用。\n\n- **调试与验证**：通过 `VM_DMA_COHERENT` 标志和 `WARN` 机制，帮助检测非法的 DMA 内存释放操作，提升系统稳定性。",
      "similarity": 0.5722994208335876,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/remap.c",
          "start_line": 60,
          "end_line": 70,
          "content": [
            "void dma_common_free_remap(void *cpu_addr, size_t size)",
            "{",
            "\tstruct vm_struct *area = find_vm_area(cpu_addr);",
            "",
            "\tif (!area || area->flags != VM_DMA_COHERENT) {",
            "\t\tWARN(1, \"trying to free invalid coherent area: %p\\n\", cpu_addr);",
            "\t\treturn;",
            "\t}",
            "",
            "\tvunmap(cpu_addr);",
            "}"
          ],
          "function_name": "dma_common_free_remap",
          "description": "实现dma_common_free_remap函数，验证虚拟地址所属的vm_area结构体标志后，调用vunmap释放对应DMA一致性区域的映射",
          "similarity": 0.4664214849472046
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/remap.c",
          "start_line": 1,
          "end_line": 59,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (c) 2014 The Linux Foundation",
            " */",
            "#include <linux/dma-map-ops.h>",
            "#include <linux/slab.h>",
            "#include <linux/vmalloc.h>",
            "",
            "struct page **dma_common_find_pages(void *cpu_addr)",
            "{",
            "\tstruct vm_struct *area = find_vm_area(cpu_addr);",
            "",
            "\tif (!area || area->flags != VM_DMA_COHERENT)",
            "\t\treturn NULL;",
            "\treturn area->pages;",
            "}",
            "",
            "/*",
            " * Remaps an array of PAGE_SIZE pages into another vm_area.",
            " * Cannot be used in non-sleeping contexts",
            " */",
            "void *dma_common_pages_remap(struct page **pages, size_t size,",
            "\t\t\t pgprot_t prot, const void *caller)",
            "{",
            "\tvoid *vaddr;",
            "",
            "\tvaddr = vmap(pages, PAGE_ALIGN(size) >> PAGE_SHIFT,",
            "\t\t     VM_DMA_COHERENT, prot);",
            "\tif (vaddr)",
            "\t\tfind_vm_area(vaddr)->pages = pages;",
            "\treturn vaddr;",
            "}",
            "",
            "/*",
            " * Remaps an allocated contiguous region into another vm_area.",
            " * Cannot be used in non-sleeping contexts",
            " */",
            "void *dma_common_contiguous_remap(struct page *page, size_t size,",
            "\t\t\tpgprot_t prot, const void *caller)",
            "{",
            "\tint count = PAGE_ALIGN(size) >> PAGE_SHIFT;",
            "\tstruct page **pages;",
            "\tvoid *vaddr;",
            "\tint i;",
            "",
            "\tpages = kvmalloc_array(count, sizeof(struct page *), GFP_KERNEL);",
            "\tif (!pages)",
            "\t\treturn NULL;",
            "\tfor (i = 0; i < count; i++)",
            "\t\tpages[i] = nth_page(page, i);",
            "\tvaddr = vmap(pages, count, VM_DMA_COHERENT, prot);",
            "\tkvfree(pages);",
            "",
            "\treturn vaddr;",
            "}",
            "",
            "/*",
            " * Unmaps a range previously mapped by dma_common_*_remap",
            " */"
          ],
          "function_name": null,
          "description": "定义dma_common_find_pages函数，通过查找VM_DMA_COHERENT标记的vm_area结构体，返回对应页面数组指针",
          "similarity": 0.44094526767730713
        }
      ]
    }
  ]
}