{
  "query": "根据负载计算目标频率 预取策略",
  "timestamp": "2025-12-26 01:44:42",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/cpufreq_schedutil.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:03:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\cpufreq_schedutil.c`\n\n---\n\n# `sched/cpufreq_schedutil.c` 技术文档\n\n## 1. 文件概述\n\n`sched/cpufreq_schedutil.c` 实现了 Linux 内核中基于调度器提供的 CPU 利用率数据的 **schedutil CPUFreq 调速器（governor）**。该调速器通过实时获取调度器计算的 CPU 利用率（包括 CFS、RT、DL 任务以及 I/O 等待状态），动态调整 CPU 频率，以在性能与能效之间取得平衡。其核心优势在于直接利用调度器的 `util` 信息，避免传统调速器依赖采样机制带来的延迟和不准确性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct sugov_tunables`**  \n  调速器可调参数，包含：\n  - `rate_limit_us`：频率更新的最小时间间隔（微秒），防止过于频繁的频率切换。\n\n- **`struct sugov_policy`**  \n  每个 `cpufreq_policy` 对应的 schedutil 策略实例，包含：\n  - `policy`：关联的 CPUFreq 策略。\n  - `update_lock`：保护频率更新的自旋锁。\n  - `last_freq_update_time` / `freq_update_delay_ns`：控制频率更新速率。\n  - `next_freq` / `cached_raw_freq`：目标频率与原始计算频率缓存。\n  - `irq_work` / `worker` / `thread`：用于慢速切换平台（非 fast-switch）的异步工作队列机制。\n  - `limits_changed` / `need_freq_update`：标志策略限制（如 min/max freq）是否变更。\n\n- **`struct sugov_cpu`**  \n  每个 CPU 的 schedutil 状态，包含：\n  - `update_util`：注册到调度器的回调接口（`update_util_data`）。\n  - `util` / `bw_min`：当前有效利用率及带宽最小值。\n  - `iowait_boost` / `iowait_boost_pending`：I/O 等待唤醒时的频率提升机制。\n  - `last_update`：上次更新时间戳。\n\n### 主要函数\n\n- **`sugov_should_update_freq()`**  \n  判断是否应执行频率更新，考虑硬件是否支持本 CPU 更新、策略限制变更、以及频率更新间隔限制。\n\n- **`sugov_update_next_freq()`**  \n  更新目标频率，处理策略限制变更场景，避免不必要的驱动回调。\n\n- **`get_next_freq()`**  \n  核心频率计算函数，根据 CPU 利用率、最大容量和参考频率，计算目标频率，并通过 `cpufreq_driver_resolve_freq()` 映射到驱动支持的频率。\n\n- **`sugov_get_util()`**  \n  获取当前 CPU 的综合利用率，整合 CFS/RT/DL 任务利用率、boost 值，并调用 `sugov_effective_cpu_perf()` 计算有效性能目标。\n\n- **`sugov_effective_cpu_perf()`**  \n  计算最终的有效性能目标，确保不低于最小性能要求，并限制不超过实际需求。\n\n- **`sugov_iowait_reset()` / `sugov_iowait_boost()`**  \n  实现 I/O 等待唤醒时的动态频率提升机制：短时间内连续 I/O 唤醒会逐步提升 boost 值（从 `IOWAIT_BOOST_MIN` 到最大 OPP），超过一个 tick 无 I/O 唤醒则重置。\n\n- **`get_capacity_ref_freq()`**  \n  获取用于计算 CPU 容量的参考频率，优先使用架构特定的 `arch_scale_freq_ref()`，其次为最大频率或当前频率。\n\n- **`sugov_deferred_update()`**  \n  在不支持 fast-switch 的平台上，通过 `irq_work` 触发异步频率更新。\n\n## 3. 关键实现\n\n### 频率计算算法\n- **频率不变性支持**：若系统支持频率不变调度（`arch_scale_freq_invariant()`），则直接使用调度器提供的频率不变利用率 `util`，按比例计算目标频率：  \n  `next_freq = C * max_freq * util / max`  \n  其中常数 `C = 1.25`，使在 `util/max = 0.8` 时达到 `max_freq`，提供性能余量。\n- **非频率不变性**：使用原始利用率 `util_raw` 乘以 `(curr_freq / max_freq)` 近似频率不变利用率，再计算目标频率。\n\n### I/O 等待 Boost 机制\n- 当任务因 I/O 完成而唤醒时，标记 `SCHED_CPUFREQ_IOWAIT`。\n- 若在 **一个 tick 内** 多次发生 I/O 唤醒，则 `iowait_boost` 值倍增（上限为最大 OPP 对应的利用率）。\n- 若超过一个 tick 无 I/O 唤醒，则重置 boost 值为 `IOWAIT_BOOST_MIN`（`SCHED_CAPACITY_SCALE / 8`），避免对偶发 I/O 过度响应，提升能效。\n\n### 快速切换（Fast-Switch）与异步更新\n- **Fast-Switch 平台**：支持在调度上下文中直接调用 `cpufreq_driver_fast_switch()` 更新频率，延迟最低。\n- **非 Fast-Switch 平台**：通过 `irq_work` 触发内核线程（`kthread_worker`）异步执行频率更新，避免在中断上下文或持有 rq 锁时调用可能阻塞的驱动接口。\n\n### 策略限制变更处理\n- 当用户空间修改 policy 的 min/max 频率时，`sugov_limits()` 设置 `limits_changed` 标志。\n- 下次更新时，强制重新计算频率，并通过内存屏障（`smp_mb()`）确保读取到最新的策略限制。\n\n## 4. 依赖关系\n\n- **调度器子系统**：\n  - 依赖 `update_util_data` 回调机制（通过 `cpufreq_add_update_util_hook()` 注册）。\n  - 调用 `cpu_util_cfs_boost()`、`effective_cpu_util()` 等函数获取综合利用率。\n  - 使用 `scx_cpuperf_target()`（若启用了 SCHED_CLASS_EXT）。\n- **CPUFreq 核心**：\n  - 依赖 `cpufreq_policy`、`cpufreq_driver_resolve_freq()`、`cpufreq_driver_fast_switch()` 等接口。\n  - 使用 `cpufreq_this_cpu_can_update()` 判断硬件更新能力。\n- **架构相关支持**：\n  - 依赖 `arch_scale_freq_ref()` 和 `arch_scale_freq_invariant()` 提供频率不变性信息。\n- **内核基础设施**：\n  - 使用 `irq_work`、`kthread_worker` 实现异步更新。\n  - 依赖 `TICK_NSEC` 定义 tick 时间。\n\n## 5. 使用场景\n\n- **默认高性能能效平衡场景**：现代 Linux 发行版通常将 `schedutil` 作为默认 CPUFreq 调速器，适用于大多数桌面、服务器和移动设备。\n- **实时性要求较高的系统**：由于其低延迟特性（尤其在 fast-switch 平台上），适合对响应时间敏感的应用。\n- **能效敏感设备**：通过 I/O boost 机制和精确的利用率跟踪，在保证交互性能的同时降低空闲功耗。\n- **异构多核系统（如 big.LITTLE）**：结合调度器的 CPU capacity 信息，为不同性能核提供差异化频率调整。",
      "similarity": 0.5837129354476929,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 62,
          "end_line": 168,
          "content": [
            "static bool sugov_should_update_freq(struct sugov_policy *sg_policy, u64 time)",
            "{",
            "\ts64 delta_ns;",
            "",
            "\t/*",
            "\t * Since cpufreq_update_util() is called with rq->lock held for",
            "\t * the @target_cpu, our per-CPU data is fully serialized.",
            "\t *",
            "\t * However, drivers cannot in general deal with cross-CPU",
            "\t * requests, so while get_next_freq() will work, our",
            "\t * sugov_update_commit() call may not for the fast switching platforms.",
            "\t *",
            "\t * Hence stop here for remote requests if they aren't supported",
            "\t * by the hardware, as calculating the frequency is pointless if",
            "\t * we cannot in fact act on it.",
            "\t *",
            "\t * This is needed on the slow switching platforms too to prevent CPUs",
            "\t * going offline from leaving stale IRQ work items behind.",
            "\t */",
            "\tif (!cpufreq_this_cpu_can_update(sg_policy->policy))",
            "\t\treturn false;",
            "",
            "\tif (unlikely(READ_ONCE(sg_policy->limits_changed))) {",
            "\t\tWRITE_ONCE(sg_policy->limits_changed, false);",
            "\t\tsg_policy->need_freq_update = true;",
            "",
            "\t\t/*",
            "\t\t * The above limits_changed update must occur before the reads",
            "\t\t * of policy limits in cpufreq_driver_resolve_freq() or a policy",
            "\t\t * limits update might be missed, so use a memory barrier to",
            "\t\t * ensure it.",
            "\t\t *",
            "\t\t * This pairs with the write memory barrier in sugov_limits().",
            "\t\t */",
            "\t\tsmp_mb();",
            "",
            "\t\treturn true;",
            "\t}",
            "",
            "\tdelta_ns = time - sg_policy->last_freq_update_time;",
            "",
            "\treturn delta_ns >= sg_policy->freq_update_delay_ns;",
            "}",
            "static bool sugov_update_next_freq(struct sugov_policy *sg_policy, u64 time,",
            "\t\t\t\t   unsigned int next_freq)",
            "{",
            "\tif (sg_policy->need_freq_update) {",
            "\t\tsg_policy->need_freq_update = false;",
            "\t\t/*",
            "\t\t * The policy limits have changed, but if the return value of",
            "\t\t * cpufreq_driver_resolve_freq() after applying the new limits",
            "\t\t * is still equal to the previously selected frequency, the",
            "\t\t * driver callback need not be invoked unless the driver",
            "\t\t * specifically wants that to happen on every update of the",
            "\t\t * policy limits.",
            "\t\t */",
            "\t\tif (sg_policy->next_freq == next_freq &&",
            "\t\t    !cpufreq_driver_test_flags(CPUFREQ_NEED_UPDATE_LIMITS))",
            "\t\t\treturn false;",
            "\t} else if (sg_policy->next_freq == next_freq) {",
            "\t\treturn false;",
            "\t}",
            "",
            "\tsg_policy->next_freq = next_freq;",
            "\tsg_policy->last_freq_update_time = time;",
            "",
            "\treturn true;",
            "}",
            "static void sugov_deferred_update(struct sugov_policy *sg_policy)",
            "{",
            "\tif (!sg_policy->work_in_progress) {",
            "\t\tsg_policy->work_in_progress = true;",
            "\t\tirq_work_queue(&sg_policy->irq_work);",
            "\t}",
            "}",
            "static __always_inline",
            "unsigned long get_capacity_ref_freq(struct cpufreq_policy *policy)",
            "{",
            "\tunsigned int freq = arch_scale_freq_ref(policy->cpu);",
            "",
            "\tif (freq)",
            "\t\treturn freq;",
            "",
            "\tif (arch_scale_freq_invariant())",
            "\t\treturn policy->cpuinfo.max_freq;",
            "",
            "\t/*",
            "\t * Apply a 25% margin so that we select a higher frequency than",
            "\t * the current one before the CPU is fully busy:",
            "\t */",
            "\treturn policy->cur + (policy->cur >> 2);",
            "}",
            "static unsigned int get_next_freq(struct sugov_policy *sg_policy,",
            "\t\t\t\t  unsigned long util, unsigned long max)",
            "{",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tunsigned int freq;",
            "",
            "\tfreq = get_capacity_ref_freq(policy);",
            "\tfreq = map_util_freq(util, freq, max);",
            "",
            "\tif (freq == sg_policy->cached_raw_freq && !sg_policy->need_freq_update)",
            "\t\treturn sg_policy->next_freq;",
            "",
            "\tsg_policy->cached_raw_freq = freq;",
            "\treturn cpufreq_driver_resolve_freq(policy, freq);",
            "}"
          ],
          "function_name": "sugov_should_update_freq, sugov_update_next_freq, sugov_deferred_update, get_capacity_ref_freq, get_next_freq",
          "description": "实现了频率更新核心逻辑，sugov_should_update_freq判断是否需要更新频率，sugov_update_next_freq计算并记录目标频率，sugov_deferred_update触发异步更新，get_capacity_ref_freq获取基准频率，get_next_freq结合利用率计算最终目标频率。",
          "similarity": 0.6447414755821228
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 381,
          "end_line": 496,
          "content": [
            "static inline bool sugov_hold_freq(struct sugov_cpu *sg_cpu) { return false; }",
            "static inline void ignore_dl_rate_limit(struct sugov_cpu *sg_cpu)",
            "{",
            "\tif (cpu_bw_dl(cpu_rq(sg_cpu->cpu)) > sg_cpu->bw_min)",
            "\t\tWRITE_ONCE(sg_cpu->sg_policy->limits_changed, true);",
            "}",
            "static inline bool sugov_update_single_common(struct sugov_cpu *sg_cpu,",
            "\t\t\t\t\t      u64 time, unsigned long max_cap,",
            "\t\t\t\t\t      unsigned int flags)",
            "{",
            "\tunsigned long boost;",
            "",
            "\tsugov_iowait_boost(sg_cpu, time, flags);",
            "\tsg_cpu->last_update = time;",
            "",
            "\tignore_dl_rate_limit(sg_cpu);",
            "",
            "\tif (!sugov_should_update_freq(sg_cpu->sg_policy, time))",
            "\t\treturn false;",
            "",
            "\tboost = sugov_iowait_apply(sg_cpu, time, max_cap);",
            "\tsugov_get_util(sg_cpu, boost);",
            "",
            "\treturn true;",
            "}",
            "static void sugov_update_single_freq(struct update_util_data *hook, u64 time,",
            "\t\t\t\t     unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tunsigned int cached_freq = sg_policy->cached_raw_freq;",
            "\tunsigned long max_cap;",
            "\tunsigned int next_f;",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tif (!sugov_update_single_common(sg_cpu, time, max_cap, flags))",
            "\t\treturn;",
            "",
            "\tnext_f = get_next_freq(sg_policy, sg_cpu->util, max_cap);",
            "",
            "\tif (sugov_hold_freq(sg_cpu) && next_f < sg_policy->next_freq &&",
            "\t    !sg_policy->need_freq_update) {",
            "\t\tnext_f = sg_policy->next_freq;",
            "",
            "\t\t/* Restore cached freq as next_freq has changed */",
            "\t\tsg_policy->cached_raw_freq = cached_freq;",
            "\t}",
            "",
            "\tif (!sugov_update_next_freq(sg_policy, time, next_f))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This code runs under rq->lock for the target CPU, so it won't run",
            "\t * concurrently on two different CPUs for the same target and it is not",
            "\t * necessary to acquire the lock in the fast switch case.",
            "\t */",
            "\tif (sg_policy->policy->fast_switch_enabled) {",
            "\t\tcpufreq_driver_fast_switch(sg_policy->policy, next_f);",
            "\t} else {",
            "\t\traw_spin_lock(&sg_policy->update_lock);",
            "\t\tsugov_deferred_update(sg_policy);",
            "\t\traw_spin_unlock(&sg_policy->update_lock);",
            "\t}",
            "}",
            "static void sugov_update_single_perf(struct update_util_data *hook, u64 time,",
            "\t\t\t\t     unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tunsigned long prev_util = sg_cpu->util;",
            "\tunsigned long max_cap;",
            "",
            "\t/*",
            "\t * Fall back to the \"frequency\" path if frequency invariance is not",
            "\t * supported, because the direct mapping between the utilization and",
            "\t * the performance levels depends on the frequency invariance.",
            "\t */",
            "\tif (!arch_scale_freq_invariant()) {",
            "\t\tsugov_update_single_freq(hook, time, flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tif (!sugov_update_single_common(sg_cpu, time, max_cap, flags))",
            "\t\treturn;",
            "",
            "\tif (sugov_hold_freq(sg_cpu) && sg_cpu->util < prev_util)",
            "\t\tsg_cpu->util = prev_util;",
            "",
            "\tcpufreq_driver_adjust_perf(sg_cpu->cpu, sg_cpu->bw_min,",
            "\t\t\t\t   sg_cpu->util, max_cap);",
            "",
            "\tsg_cpu->sg_policy->last_freq_update_time = time;",
            "}",
            "static unsigned int sugov_next_freq_shared(struct sugov_cpu *sg_cpu, u64 time)",
            "{",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tunsigned long util = 0, max_cap;",
            "\tunsigned int j;",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tfor_each_cpu(j, policy->cpus) {",
            "\t\tstruct sugov_cpu *j_sg_cpu = &per_cpu(sugov_cpu, j);",
            "\t\tunsigned long boost;",
            "",
            "\t\tboost = sugov_iowait_apply(j_sg_cpu, time, max_cap);",
            "\t\tsugov_get_util(j_sg_cpu, boost);",
            "",
            "\t\tutil = max(j_sg_cpu->util, util);",
            "\t}",
            "",
            "\treturn get_next_freq(sg_policy, util, max_cap);",
            "}"
          ],
          "function_name": "sugov_hold_freq, ignore_dl_rate_limit, sugov_update_single_common, sugov_update_single_freq, sugov_update_single_perf, sugov_next_freq_shared",
          "description": "实现单核/多核频率调整逻辑，sugov_update_single_freq处理单核频率更新，sugov_update_single_perf处理性能调优路径，sugov_next_freq_shared计算多核共享场景下的全局目标频率。",
          "similarity": 0.6025582551956177
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 508,
          "end_line": 651,
          "content": [
            "static void",
            "sugov_update_shared(struct update_util_data *hook, u64 time, unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tunsigned int next_f;",
            "",
            "\traw_spin_lock(&sg_policy->update_lock);",
            "",
            "\tsugov_iowait_boost(sg_cpu, time, flags);",
            "\tsg_cpu->last_update = time;",
            "",
            "\tignore_dl_rate_limit(sg_cpu);",
            "",
            "\tif (sugov_should_update_freq(sg_policy, time)) {",
            "\t\tnext_f = sugov_next_freq_shared(sg_cpu, time);",
            "",
            "\t\tif (!sugov_update_next_freq(sg_policy, time, next_f))",
            "\t\t\tgoto unlock;",
            "",
            "\t\tif (sg_policy->policy->fast_switch_enabled)",
            "\t\t\tcpufreq_driver_fast_switch(sg_policy->policy, next_f);",
            "\t\telse",
            "\t\t\tsugov_deferred_update(sg_policy);",
            "\t}",
            "unlock:",
            "\traw_spin_unlock(&sg_policy->update_lock);",
            "}",
            "static void sugov_work(struct kthread_work *work)",
            "{",
            "\tstruct sugov_policy *sg_policy = container_of(work, struct sugov_policy, work);",
            "\tunsigned int freq;",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * Hold sg_policy->update_lock shortly to handle the case where:",
            "\t * in case sg_policy->next_freq is read here, and then updated by",
            "\t * sugov_deferred_update() just before work_in_progress is set to false",
            "\t * here, we may miss queueing the new update.",
            "\t *",
            "\t * Note: If a work was queued after the update_lock is released,",
            "\t * sugov_work() will just be called again by kthread_work code; and the",
            "\t * request will be proceed before the sugov thread sleeps.",
            "\t */",
            "\traw_spin_lock_irqsave(&sg_policy->update_lock, flags);",
            "\tfreq = sg_policy->next_freq;",
            "\tsg_policy->work_in_progress = false;",
            "\traw_spin_unlock_irqrestore(&sg_policy->update_lock, flags);",
            "",
            "\tmutex_lock(&sg_policy->work_lock);",
            "\t__cpufreq_driver_target(sg_policy->policy, freq, CPUFREQ_RELATION_L);",
            "\tmutex_unlock(&sg_policy->work_lock);",
            "}",
            "static void sugov_irq_work(struct irq_work *irq_work)",
            "{",
            "\tstruct sugov_policy *sg_policy;",
            "",
            "\tsg_policy = container_of(irq_work, struct sugov_policy, irq_work);",
            "",
            "\tkthread_queue_work(&sg_policy->worker, &sg_policy->work);",
            "}",
            "static ssize_t rate_limit_us_show(struct gov_attr_set *attr_set, char *buf)",
            "{",
            "\tstruct sugov_tunables *tunables = to_sugov_tunables(attr_set);",
            "",
            "\treturn sprintf(buf, \"%u\\n\", tunables->rate_limit_us);",
            "}",
            "static ssize_t",
            "rate_limit_us_store(struct gov_attr_set *attr_set, const char *buf, size_t count)",
            "{",
            "\tstruct sugov_tunables *tunables = to_sugov_tunables(attr_set);",
            "\tstruct sugov_policy *sg_policy;",
            "\tunsigned int rate_limit_us;",
            "",
            "\tif (kstrtouint(buf, 10, &rate_limit_us))",
            "\t\treturn -EINVAL;",
            "",
            "\ttunables->rate_limit_us = rate_limit_us;",
            "",
            "\tlist_for_each_entry(sg_policy, &attr_set->policy_list, tunables_hook)",
            "\t\tsg_policy->freq_update_delay_ns = rate_limit_us * NSEC_PER_USEC;",
            "",
            "\treturn count;",
            "}",
            "static void sugov_tunables_free(struct kobject *kobj)",
            "{",
            "\tstruct gov_attr_set *attr_set = to_gov_attr_set(kobj);",
            "",
            "\tkfree(to_sugov_tunables(attr_set));",
            "}",
            "static void sugov_policy_free(struct sugov_policy *sg_policy)",
            "{",
            "\tkfree(sg_policy);",
            "}",
            "static int sugov_kthread_create(struct sugov_policy *sg_policy)",
            "{",
            "\tstruct task_struct *thread;",
            "\tstruct sched_attr attr = {",
            "\t\t.size\t\t= sizeof(struct sched_attr),",
            "\t\t.sched_policy\t= SCHED_DEADLINE,",
            "\t\t.sched_flags\t= SCHED_FLAG_SUGOV,",
            "\t\t.sched_nice\t= 0,",
            "\t\t.sched_priority\t= 0,",
            "\t\t/*",
            "\t\t * Fake (unused) bandwidth; workaround to \"fix\"",
            "\t\t * priority inheritance.",
            "\t\t */",
            "\t\t.sched_runtime\t=  1000000,",
            "\t\t.sched_deadline = 10000000,",
            "\t\t.sched_period\t= 10000000,",
            "\t};",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tint ret;",
            "",
            "\t/* kthread only required for slow path */",
            "\tif (policy->fast_switch_enabled)",
            "\t\treturn 0;",
            "",
            "\tkthread_init_work(&sg_policy->work, sugov_work);",
            "\tkthread_init_worker(&sg_policy->worker);",
            "\tthread = kthread_create(kthread_worker_fn, &sg_policy->worker,",
            "\t\t\t\t\"sugov:%d\",",
            "\t\t\t\tcpumask_first(policy->related_cpus));",
            "\tif (IS_ERR(thread)) {",
            "\t\tpr_err(\"failed to create sugov thread: %ld\\n\", PTR_ERR(thread));",
            "\t\treturn PTR_ERR(thread);",
            "\t}",
            "",
            "\tret = sched_setattr_nocheck(thread, &attr);",
            "\tif (ret) {",
            "\t\tkthread_stop(thread);",
            "\t\tpr_warn(\"%s: failed to set SCHED_DEADLINE\\n\", __func__);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tsg_policy->thread = thread;",
            "\tkthread_bind_mask(thread, policy->related_cpus);",
            "\tinit_irq_work(&sg_policy->irq_work, sugov_irq_work);",
            "\tmutex_init(&sg_policy->work_lock);",
            "",
            "\twake_up_process(thread);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sugov_update_shared, sugov_work, sugov_irq_work, rate_limit_us_show, rate_limit_us_store, sugov_tunables_free, sugov_policy_free, sugov_kthread_create",
          "description": "管理频率调节的工作线程和参数配置，sugov_kthread_create创建慢速切换场景的后台线程，rate_limit_us_*/提供速率限制配置接口，sugov_work/sugov_irq_work处理异步频率更新任务。",
          "similarity": 0.5817341804504395
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 204,
          "end_line": 330,
          "content": [
            "unsigned long sugov_effective_cpu_perf(int cpu, unsigned long actual,",
            "\t\t\t\t unsigned long min,",
            "\t\t\t\t unsigned long max)",
            "{",
            "\t/* Add dvfs headroom to actual utilization */",
            "\tactual = map_util_perf(actual);",
            "\t/* Actually we don't need to target the max performance */",
            "\tif (actual < max)",
            "\t\tmax = actual;",
            "",
            "\t/*",
            "\t * Ensure at least minimum performance while providing more compute",
            "\t * capacity when possible.",
            "\t */",
            "\treturn max(min, max);",
            "}",
            "static void sugov_get_util(struct sugov_cpu *sg_cpu, unsigned long boost)",
            "{",
            "\tunsigned long min, max, util = scx_cpuperf_target(sg_cpu->cpu);",
            "",
            "\tif (!scx_switched_all())",
            "\t\tutil += cpu_util_cfs_boost(sg_cpu->cpu);",
            "\tutil = effective_cpu_util(sg_cpu->cpu, util, &min, &max);",
            "\tutil = max(util, boost);",
            "\tsg_cpu->bw_min = min;",
            "\tsg_cpu->util = sugov_effective_cpu_perf(sg_cpu->cpu, util, min, max);",
            "}",
            "static bool sugov_iowait_reset(struct sugov_cpu *sg_cpu, u64 time,",
            "\t\t\t       bool set_iowait_boost)",
            "{",
            "\ts64 delta_ns = time - sg_cpu->last_update;",
            "",
            "\t/* Reset boost only if a tick has elapsed since last request */",
            "\tif (delta_ns <= TICK_NSEC)",
            "\t\treturn false;",
            "",
            "\tsg_cpu->iowait_boost = set_iowait_boost ? IOWAIT_BOOST_MIN : 0;",
            "\tsg_cpu->iowait_boost_pending = set_iowait_boost;",
            "",
            "\treturn true;",
            "}",
            "static void sugov_iowait_boost(struct sugov_cpu *sg_cpu, u64 time,",
            "\t\t\t       unsigned int flags)",
            "{",
            "\tbool set_iowait_boost = flags & SCHED_CPUFREQ_IOWAIT;",
            "",
            "\t/* Reset boost if the CPU appears to have been idle enough */",
            "\tif (sg_cpu->iowait_boost &&",
            "\t    sugov_iowait_reset(sg_cpu, time, set_iowait_boost))",
            "\t\treturn;",
            "",
            "\t/* Boost only tasks waking up after IO */",
            "\tif (!set_iowait_boost)",
            "\t\treturn;",
            "",
            "\t/* Ensure boost doubles only one time at each request */",
            "\tif (sg_cpu->iowait_boost_pending)",
            "\t\treturn;",
            "\tsg_cpu->iowait_boost_pending = true;",
            "",
            "\t/* Double the boost at each request */",
            "\tif (sg_cpu->iowait_boost) {",
            "\t\tsg_cpu->iowait_boost =",
            "\t\t\tmin_t(unsigned int, sg_cpu->iowait_boost << 1, SCHED_CAPACITY_SCALE);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* First wakeup after IO: start with minimum boost */",
            "\tsg_cpu->iowait_boost = IOWAIT_BOOST_MIN;",
            "}",
            "static unsigned long sugov_iowait_apply(struct sugov_cpu *sg_cpu, u64 time,",
            "\t\t\t       unsigned long max_cap)",
            "{",
            "\t/* No boost currently required */",
            "\tif (!sg_cpu->iowait_boost)",
            "\t\treturn 0;",
            "",
            "\t/* Reset boost if the CPU appears to have been idle enough */",
            "\tif (sugov_iowait_reset(sg_cpu, time, false))",
            "\t\treturn 0;",
            "",
            "\tif (!sg_cpu->iowait_boost_pending) {",
            "\t\t/*",
            "\t\t * No boost pending; reduce the boost value.",
            "\t\t */",
            "\t\tsg_cpu->iowait_boost >>= 1;",
            "\t\tif (sg_cpu->iowait_boost < IOWAIT_BOOST_MIN) {",
            "\t\t\tsg_cpu->iowait_boost = 0;",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            "\tsg_cpu->iowait_boost_pending = false;",
            "",
            "\t/*",
            "\t * sg_cpu->util is already in capacity scale; convert iowait_boost",
            "\t * into the same scale so we can compare.",
            "\t */",
            "\treturn (sg_cpu->iowait_boost * max_cap) >> SCHED_CAPACITY_SHIFT;",
            "}",
            "static bool sugov_hold_freq(struct sugov_cpu *sg_cpu)",
            "{",
            "\tunsigned long idle_calls;",
            "\tbool ret;",
            "",
            "\t/*",
            "\t * The heuristics in this function is for the fair class. For SCX, the",
            "\t * performance target comes directly from the BPF scheduler. Let's just",
            "\t * follow it.",
            "\t */",
            "\tif (scx_switched_all())",
            "\t\treturn false;",
            "",
            "\t/* if capped by uclamp_max, always update to be in compliance */",
            "\tif (uclamp_rq_is_capped(cpu_rq(sg_cpu->cpu)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Maintain the frequency if the CPU has not been idle recently, as",
            "\t * reduction is likely to be premature.",
            "\t */",
            "\tidle_calls = tick_nohz_get_idle_calls_cpu(sg_cpu->cpu);",
            "\tret = idle_calls == sg_cpu->saved_idle_calls;",
            "",
            "\tsg_cpu->saved_idle_calls = idle_calls;",
            "\treturn ret;",
            "}"
          ],
          "function_name": "sugov_effective_cpu_perf, sugov_get_util, sugov_iowait_reset, sugov_iowait_boost, sugov_iowait_apply, sugov_hold_freq",
          "description": "处理利用率计算和I/O等待优化，sugov_effective_cpu_perf计算有效性能需求，sugov_get_util获取考虑boost后的利用率，sugov_iowait_*系列函数管理I/O等待场景下的频率提升机制。",
          "similarity": 0.5673856139183044
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 827,
          "end_line": 916,
          "content": [
            "static int sugov_start(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "\tvoid (*uu)(struct update_util_data *data, u64 time, unsigned int flags);",
            "\tunsigned int cpu;",
            "",
            "\tsg_policy->freq_update_delay_ns\t= sg_policy->tunables->rate_limit_us * NSEC_PER_USEC;",
            "\tsg_policy->last_freq_update_time\t= 0;",
            "\tsg_policy->next_freq\t\t\t= 0;",
            "\tsg_policy->work_in_progress\t\t= false;",
            "\tsg_policy->limits_changed\t\t= false;",
            "\tsg_policy->cached_raw_freq\t\t= 0;",
            "",
            "\tsg_policy->need_freq_update = cpufreq_driver_test_flags(CPUFREQ_NEED_UPDATE_LIMITS);",
            "",
            "\tfor_each_cpu(cpu, policy->cpus) {",
            "\t\tstruct sugov_cpu *sg_cpu = &per_cpu(sugov_cpu, cpu);",
            "",
            "\t\tmemset(sg_cpu, 0, sizeof(*sg_cpu));",
            "\t\tsg_cpu->cpu\t\t\t= cpu;",
            "\t\tsg_cpu->sg_policy\t\t= sg_policy;",
            "\t}",
            "",
            "\tif (policy_is_shared(policy))",
            "\t\tuu = sugov_update_shared;",
            "\telse if (policy->fast_switch_enabled && cpufreq_driver_has_adjust_perf())",
            "\t\tuu = sugov_update_single_perf;",
            "\telse",
            "\t\tuu = sugov_update_single_freq;",
            "",
            "\tfor_each_cpu(cpu, policy->cpus) {",
            "\t\tstruct sugov_cpu *sg_cpu = &per_cpu(sugov_cpu, cpu);",
            "",
            "\t\tcpufreq_add_update_util_hook(cpu, &sg_cpu->update_util, uu);",
            "\t}",
            "\treturn 0;",
            "}",
            "static void sugov_stop(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "\tunsigned int cpu;",
            "",
            "\tfor_each_cpu(cpu, policy->cpus)",
            "\t\tcpufreq_remove_update_util_hook(cpu);",
            "",
            "\tsynchronize_rcu();",
            "",
            "\tif (!policy->fast_switch_enabled) {",
            "\t\tirq_work_sync(&sg_policy->irq_work);",
            "\t\tkthread_cancel_work_sync(&sg_policy->work);",
            "\t}",
            "}",
            "static void sugov_limits(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "",
            "\tif (!policy->fast_switch_enabled) {",
            "\t\tmutex_lock(&sg_policy->work_lock);",
            "\t\tcpufreq_policy_apply_limits(policy);",
            "\t\tmutex_unlock(&sg_policy->work_lock);",
            "\t}",
            "",
            "\t/*",
            "\t * The limits_changed update below must take place before the updates",
            "\t * of policy limits in cpufreq_set_policy() or a policy limits update",
            "\t * might be missed, so use a memory barrier to ensure it.",
            "\t *",
            "\t * This pairs with the memory barrier in sugov_should_update_freq().",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\tWRITE_ONCE(sg_policy->limits_changed, true);",
            "}",
            "static void rebuild_sd_workfn(struct work_struct *work)",
            "{",
            "\trebuild_sched_domains_energy();",
            "}",
            "void sched_cpufreq_governor_change(struct cpufreq_policy *policy,",
            "\t\t\t\t  struct cpufreq_governor *old_gov)",
            "{",
            "\tif (old_gov == &schedutil_gov || policy->governor == &schedutil_gov) {",
            "\t\t/*",
            "\t\t * When called from the cpufreq_register_driver() path, the",
            "\t\t * cpu_hotplug_lock is already held, so use a work item to",
            "\t\t * avoid nested locking in rebuild_sched_domains().",
            "\t\t */",
            "\t\tschedule_work(&rebuild_sd_work);",
            "\t}",
            "",
            "}"
          ],
          "function_name": "sugov_start, sugov_stop, sugov_limits, rebuild_sd_workfn, sched_cpufreq_governor_change",
          "description": "sugov_start 注册CPU利用率更新钩子函数并初始化频率更新参数；sugov_stop 移除所有CPU的更新钩子并同步RCU状态；sugov_limits 应用频率限制并标记策略变更；rebuild_sd_workfn 触发调度域能量重新构建；sched_cpufreq_governor_change 在策略切换时安排调度域重建工作",
          "similarity": 0.5531589984893799
        }
      ]
    },
    {
      "source_file": "kernel/sched/pelt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:13:26\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\pelt.c`\n\n---\n\n# `sched/pelt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/pelt.c` 实现了 **Per-Entity Load Tracking（PELT）** 机制，这是 Linux 内核 CFS（Completely Fair Scheduler）调度器中用于精确跟踪每个调度实体（如任务或任务组）负载、可运行性和 CPU 利用率的核心算法。  \nPELT 将时间划分为约 1ms（1024ns）的周期段，使用指数衰减的几何级数对历史负载进行加权求和，使得近期负载权重更高，远期负载影响逐渐衰减。该机制为负载均衡、能效调度（如 EAS）和 CPU 频率调节等子系统提供关键的负载指标。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `decay_load(u64 val, u64 n)`  \n  计算负载值 `val` 经过 `n` 个时间单位后的衰减值，利用预计算的衰减系数表和位移优化实现高效指数衰减。\n\n- `__accumulate_pelt_segments(u64 periods, u32 d1, u32 d3)`  \n  计算跨越多个完整周期时，负载贡献的三部分之和：上一周期剩余部分（d1）、中间完整周期总和（d2）、当前周期已过部分（d3）。\n\n- `accumulate_sum(u64 delta, struct sched_avg *sa, unsigned long load, unsigned long runnable, int running)`  \n  核心累加函数，根据时间增量 `delta` 更新 `load_sum`、`runnable_sum` 和 `util_sum`，处理跨周期衰减与新贡献累加。\n\n- `___update_load_sum(u64 now, struct sched_avg *sa, unsigned long load, unsigned long runnable, int running)`  \n  入口函数，计算自上次更新以来的时间差，调用 `accumulate_sum` 更新负载总和，并处理时间回退等异常情况。\n\n- `___update_load_avg(struct sched_avg *sa, unsigned long load)`  \n  根据当前 `*_sum` 值和动态除数（divider）计算并更新 `load_avg`、`runnable_avg` 和 `util_avg`。\n\n### 关键数据结构\n\n- `struct sched_avg`  \n  存储 PELT 相关状态，包括：\n  - `load_sum` / `runnable_sum` / `util_sum`：衰减加权后的负载总和\n  - `load_avg` / `runnable_avg` / `util_avg`：归一化后的平均负载值\n  - `last_update_time`：上次更新时间戳\n  - `period_contrib`：当前周期内已累积的时间（<1024ns）\n\n## 3. 关键实现\n\n### 时间分段与衰减模型\n- 时间以 **1024ns（≈1μs）** 为基本单位，每 **1024 单位（≈1ms）** 构成一个 PELT 周期。\n- 衰减因子 `y` 满足 `y^32 ≈ 0.5`，即约 32ms 前的负载贡献衰减至当前的一半。\n- 负载历史表示为几何级数：`u₀ + u₁·y + u₂·y² + ...`，其中 `uᵢ` 是第 `i` 个周期内的可运行比例。\n\n### 高效衰减计算\n- `decay_load()` 利用 `y^32 = 1/2` 的特性，将 `y^n` 拆分为 `1/2^(n/32) * y^(n%32)`。\n- 通过右移操作快速计算 `1/2^k` 部分，再查表 `runnable_avg_yN_inv[]` 获取 `y^(n%32)` 的倒数，结合 `mul_u64_u32_shr` 完成乘法。\n\n### 负载累加三段式\n当时间增量跨越多个周期时，负载贡献分为：\n1. **d1**：上一周期未完成部分（`1024 - period_contrib`）\n2. **d2**：中间完整周期的理论最大贡献（`LOAD_AVG_MAX - decay_load(LOAD_AVG_MAX, periods) - 1024`）\n3. **d3**：当前周期已过部分（`delta % 1024`）\n\n### 动态归一化\n- 使用 `get_pelt_divider()` 获取当前周期位置对应的归一化除数，避免因周期未结束导致的平均值震荡。\n- 除数公式：`LOAD_AVG_MAX - 1024 + period_contrib`，确保最大负载值在 `[1002, 1024)` 区间稳定。\n\n### 状态一致性保障\n- 若 `load == 0`，强制 `runnable = running = 0`，避免已出队实体产生无效贡献。\n- 时间回退（如 TSC 切换）时直接重置 `last_update_time`，防止负时间差导致异常。\n\n## 4. 依赖关系\n\n- **头文件依赖**：  \n  依赖 `kernel/sched/sched.h` 中定义的 `struct sched_avg`、`SCHED_CAPACITY_SHIFT`、`LOAD_AVG_*` 常量及 `get_pelt_divider()` 等辅助函数。\n- **预计算表**：  \n  使用外部定义的 `runnable_avg_yN_inv[32]` 衰减系数表（通常在 `fair.c` 或 `pelt.h` 中初始化）。\n- **调度器集成**：  \n  被 `fair.c` 中的 CFS 调度实体（`sched_entity`）和 CFS 运行队列（`cfs_rq`）调用，用于更新任务/任务组的负载状态。\n- **能效调度**：  \n  为 Energy Aware Scheduling (EAS) 提供 `util_avg` 作为 CPU 需求预测依据。\n\n## 5. 使用场景\n\n- **任务负载跟踪**：  \n  每个 `task_struct` 的 `sched_entity` 通过 PELT 实时更新其 `load_avg` 和 `util_avg`，反映任务对 CPU 的历史需求。\n- **任务组调度**：  \n  CFS 任务组（`task_group`）的 `cfs_rq` 使用 PELT 聚合子任务的负载，实现层级化负载均衡。\n- **负载均衡决策**：  \n  `load_balance()` 等函数依据 `runnable_avg` 判断 CPU 间负载差异，触发任务迁移。\n- **CPU 频率调节**：  \n  CPUFreq 的 `schedutil` 调速器使用 `util_avg` 动态调整 CPU 频率，平衡性能与功耗。\n- **空闲负载处理**：  \n  在 `idle_balance()` 等场景中，即使任务已出队，仍需通过 PELT 正确衰减其历史负载贡献。",
      "similarity": 0.5719118714332581,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/pelt.c",
          "start_line": 256,
          "end_line": 384,
          "content": [
            "static __always_inline void",
            "___update_load_avg(struct sched_avg *sa, unsigned long load)",
            "{",
            "\tu32 divider = get_pelt_divider(sa);",
            "",
            "\t/*",
            "\t * Step 2: update *_avg.",
            "\t */",
            "\tsa->load_avg = div_u64(load * sa->load_sum, divider);",
            "\tsa->runnable_avg = div_u64(sa->runnable_sum, divider);",
            "\tWRITE_ONCE(sa->util_avg, sa->util_sum / divider);",
            "}",
            "int __update_load_avg_blocked_se(u64 now, struct sched_entity *se)",
            "{",
            "\tif (___update_load_sum(now, &se->avg, 0, 0, 0)) {",
            "\t\t___update_load_avg(&se->avg, se_weight(se));",
            "\t\ttrace_pelt_se_tp(se);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __update_load_avg_se(u64 now, struct cfs_rq *cfs_rq, struct sched_entity *se)",
            "{",
            "\tif (___update_load_sum(now, &se->avg, !!se->on_rq, se_runnable(se),",
            "\t\t\t\tcfs_rq->curr == se)) {",
            "",
            "\t\t___update_load_avg(&se->avg, se_weight(se));",
            "\t\tcfs_se_util_change(&se->avg);",
            "\t\ttrace_pelt_se_tp(se);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __update_load_avg_cfs_rq(u64 now, struct cfs_rq *cfs_rq)",
            "{",
            "\tif (___update_load_sum(now, &cfs_rq->avg,",
            "\t\t\t\tscale_load_down(cfs_rq->load.weight),",
            "\t\t\t\tcfs_rq->h_nr_running,",
            "\t\t\t\tcfs_rq->curr != NULL)) {",
            "",
            "\t\t___update_load_avg(&cfs_rq->avg, 1);",
            "\t\ttrace_pelt_cfs_tp(cfs_rq);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int update_rt_rq_load_avg(u64 now, struct rq *rq, int running)",
            "{",
            "\tif (___update_load_sum(now, &rq->avg_rt,",
            "\t\t\t\trunning,",
            "\t\t\t\trunning,",
            "\t\t\t\trunning)) {",
            "",
            "\t\t___update_load_avg(&rq->avg_rt, 1);",
            "\t\ttrace_pelt_rt_tp(rq);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int update_dl_rq_load_avg(u64 now, struct rq *rq, int running)",
            "{",
            "\tif (___update_load_sum(now, &rq->avg_dl,",
            "\t\t\t\trunning,",
            "\t\t\t\trunning,",
            "\t\t\t\trunning)) {",
            "",
            "\t\t___update_load_avg(&rq->avg_dl, 1);",
            "\t\ttrace_pelt_dl_tp(rq);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int update_hw_load_avg(u64 now, struct rq *rq, u64 capacity)",
            "{",
            "\tif (___update_load_sum(now, &rq->avg_hw,",
            "\t\t\t       capacity,",
            "\t\t\t       capacity,",
            "\t\t\t       capacity)) {",
            "\t\t___update_load_avg(&rq->avg_hw, 1);",
            "\t\ttrace_pelt_hw_tp(rq);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int update_irq_load_avg(struct rq *rq, u64 running)",
            "{",
            "\tint ret = 0;",
            "",
            "\t/*",
            "\t * We can't use clock_pelt because irq time is not accounted in",
            "\t * clock_task. Instead we directly scale the running time to",
            "\t * reflect the real amount of computation",
            "\t */",
            "\trunning = cap_scale(running, arch_scale_freq_capacity(cpu_of(rq)));",
            "\trunning = cap_scale(running, arch_scale_cpu_capacity(cpu_of(rq)));",
            "",
            "\t/*",
            "\t * We know the time that has been used by interrupt since last update",
            "\t * but we don't when. Let be pessimistic and assume that interrupt has",
            "\t * happened just before the update. This is not so far from reality",
            "\t * because interrupt will most probably wake up task and trig an update",
            "\t * of rq clock during which the metric is updated.",
            "\t * We start to decay with normal context time and then we add the",
            "\t * interrupt context time.",
            "\t * We can safely remove running from rq->clock because",
            "\t * rq->clock += delta with delta >= running",
            "\t */",
            "\tret = ___update_load_sum(rq->clock - running, &rq->avg_irq,",
            "\t\t\t\t0,",
            "\t\t\t\t0,",
            "\t\t\t\t0);",
            "\tret += ___update_load_sum(rq->clock, &rq->avg_irq,",
            "\t\t\t\t1,",
            "\t\t\t\t1,",
            "\t\t\t\t1);",
            "",
            "\tif (ret) {",
            "\t\t___update_load_avg(&rq->avg_irq, 1);",
            "\t\ttrace_pelt_irq_tp(rq);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "___update_load_avg, __update_load_avg_blocked_se, __update_load_avg_se, __update_load_avg_cfs_rq, update_rt_rq_load_avg, update_dl_rq_load_avg, update_hw_load_avg, update_irq_load_avg",
          "description": "提供多场景下的负载平均值更新接口，包含六个函数：___update_load_avg计算负载平均值；__update_load_avg_blocked_se更新阻塞任务实体；__update_load_avg_se处理CFS队列中任务实体；__update_load_avg_cfs_rq更新CFS队列负载；update_rt_rq_load_avg/update_dl_rq_load_avg分别处理实时/延迟调度队列；update_hw_load_avg和update_irq_load_avg分别更新硬件资源及中断负载统计",
          "similarity": 0.5786542892456055
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/pelt.c",
          "start_line": 31,
          "end_line": 178,
          "content": [
            "static u64 decay_load(u64 val, u64 n)",
            "{",
            "\tunsigned int local_n;",
            "",
            "\tif (unlikely(n > LOAD_AVG_PERIOD * 63))",
            "\t\treturn 0;",
            "",
            "\t/* after bounds checking we can collapse to 32-bit */",
            "\tlocal_n = n;",
            "",
            "\t/*",
            "\t * As y^PERIOD = 1/2, we can combine",
            "\t *    y^n = 1/2^(n/PERIOD) * y^(n%PERIOD)",
            "\t * With a look-up table which covers y^n (n<PERIOD)",
            "\t *",
            "\t * To achieve constant time decay_load.",
            "\t */",
            "\tif (unlikely(local_n >= LOAD_AVG_PERIOD)) {",
            "\t\tval >>= local_n / LOAD_AVG_PERIOD;",
            "\t\tlocal_n %= LOAD_AVG_PERIOD;",
            "\t}",
            "",
            "\tval = mul_u64_u32_shr(val, runnable_avg_yN_inv[local_n], 32);",
            "\treturn val;",
            "}",
            "static u32 __accumulate_pelt_segments(u64 periods, u32 d1, u32 d3)",
            "{",
            "\tu32 c1, c2, c3 = d3; /* y^0 == 1 */",
            "",
            "\t/*",
            "\t * c1 = d1 y^p",
            "\t */",
            "\tc1 = decay_load((u64)d1, periods);",
            "",
            "\t/*",
            "\t *            p-1",
            "\t * c2 = 1024 \\Sum y^n",
            "\t *            n=1",
            "\t *",
            "\t *              inf        inf",
            "\t *    = 1024 ( \\Sum y^n - \\Sum y^n - y^0 )",
            "\t *              n=0        n=p",
            "\t */",
            "\tc2 = LOAD_AVG_MAX - decay_load(LOAD_AVG_MAX, periods) - 1024;",
            "",
            "\treturn c1 + c2 + c3;",
            "}",
            "static __always_inline u32",
            "accumulate_sum(u64 delta, struct sched_avg *sa,",
            "\t       unsigned long load, unsigned long runnable, int running)",
            "{",
            "\tu32 contrib = (u32)delta; /* p == 0 -> delta < 1024 */",
            "\tu64 periods;",
            "",
            "\tdelta += sa->period_contrib;",
            "\tperiods = delta / 1024; /* A period is 1024us (~1ms) */",
            "",
            "\t/*",
            "\t * Step 1: decay old *_sum if we crossed period boundaries.",
            "\t */",
            "\tif (periods) {",
            "\t\tsa->load_sum = decay_load(sa->load_sum, periods);",
            "\t\tsa->runnable_sum =",
            "\t\t\tdecay_load(sa->runnable_sum, periods);",
            "\t\tsa->util_sum = decay_load((u64)(sa->util_sum), periods);",
            "",
            "\t\t/*",
            "\t\t * Step 2",
            "\t\t */",
            "\t\tdelta %= 1024;",
            "\t\tif (load) {",
            "\t\t\t/*",
            "\t\t\t * This relies on the:",
            "\t\t\t *",
            "\t\t\t * if (!load)",
            "\t\t\t *\trunnable = running = 0;",
            "\t\t\t *",
            "\t\t\t * clause from ___update_load_sum(); this results in",
            "\t\t\t * the below usage of @contrib to disappear entirely,",
            "\t\t\t * so no point in calculating it.",
            "\t\t\t */",
            "\t\t\tcontrib = __accumulate_pelt_segments(periods,",
            "\t\t\t\t\t1024 - sa->period_contrib, delta);",
            "\t\t}",
            "\t}",
            "\tsa->period_contrib = delta;",
            "",
            "\tif (load)",
            "\t\tsa->load_sum += load * contrib;",
            "\tif (runnable)",
            "\t\tsa->runnable_sum += runnable * contrib << SCHED_CAPACITY_SHIFT;",
            "\tif (running)",
            "\t\tsa->util_sum += contrib << SCHED_CAPACITY_SHIFT;",
            "",
            "\treturn periods;",
            "}",
            "static __always_inline int",
            "___update_load_sum(u64 now, struct sched_avg *sa,",
            "\t\t  unsigned long load, unsigned long runnable, int running)",
            "{",
            "\tu64 delta;",
            "",
            "\tdelta = now - sa->last_update_time;",
            "\t/*",
            "\t * This should only happen when time goes backwards, which it",
            "\t * unfortunately does during sched clock init when we swap over to TSC.",
            "\t */",
            "\tif ((s64)delta < 0) {",
            "\t\tsa->last_update_time = now;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/*",
            "\t * Use 1024ns as the unit of measurement since it's a reasonable",
            "\t * approximation of 1us and fast to compute.",
            "\t */",
            "\tdelta >>= 10;",
            "\tif (!delta)",
            "\t\treturn 0;",
            "",
            "\tsa->last_update_time += delta << 10;",
            "",
            "\t/*",
            "\t * running is a subset of runnable (weight) so running can't be set if",
            "\t * runnable is clear. But there are some corner cases where the current",
            "\t * se has been already dequeued but cfs_rq->curr still points to it.",
            "\t * This means that weight will be 0 but not running for a sched_entity",
            "\t * but also for a cfs_rq if the latter becomes idle. As an example,",
            "\t * this happens during idle_balance() which calls",
            "\t * sched_balance_update_blocked_averages().",
            "\t *",
            "\t * Also see the comment in accumulate_sum().",
            "\t */",
            "\tif (!load)",
            "\t\trunnable = running = 0;",
            "",
            "\t/*",
            "\t * Now we know we crossed measurement unit boundaries. The *_avg",
            "\t * accrues by two steps:",
            "\t *",
            "\t * Step 1: accumulate *_sum since last_update_time. If we haven't",
            "\t * crossed period boundaries, finish.",
            "\t */",
            "\tif (!accumulate_sum(delta, sa, load, runnable, running))",
            "\t\treturn 0;",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "decay_load, __accumulate_pelt_segments, accumulate_sum, ___update_load_sum",
          "description": "实现PELT核心算法，包含四个关键函数：decay_load通过位移运算模拟指数衰减；__accumulate_pelt_segments计算周期性负载贡献；accumulate_sum根据时间差更新负载、运行时和利用率的加权总和；___update_load_sum处理时间边界穿越时的衰减逻辑并触发更新。所有函数共同维护调度实体的动态负载统计",
          "similarity": 0.5770823955535889
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/pelt.c",
          "start_line": 1,
          "end_line": 30,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Per Entity Load Tracking",
            " *",
            " *  Copyright (C) 2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>",
            " *",
            " *  Interactivity improvements by Mike Galbraith",
            " *  (C) 2007 Mike Galbraith <efault@gmx.de>",
            " *",
            " *  Various enhancements by Dmitry Adamushko.",
            " *  (C) 2007 Dmitry Adamushko <dmitry.adamushko@gmail.com>",
            " *",
            " *  Group scheduling enhancements by Srivatsa Vaddagiri",
            " *  Copyright IBM Corporation, 2007",
            " *  Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>",
            " *",
            " *  Scaled math optimizations by Thomas Gleixner",
            " *  Copyright (C) 2007, Thomas Gleixner <tglx@linutronix.de>",
            " *",
            " *  Adaptive scheduling granularity, math enhancements by Peter Zijlstra",
            " *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra",
            " *",
            " *  Move PELT related code from fair.c into this pelt.c file",
            " *  Author: Vincent Guittot <vincent.guittot@linaro.org>",
            " */",
            "",
            "/*",
            " * Approximate:",
            " *   val * y^n,    where y^32 ~= 0.5 (~1 scheduling period)",
            " */"
          ],
          "function_name": null,
          "description": "此代码块为PELT（Per-entity Load Tracking）模块的头部注释，声明了该模块的版权信息、作者及主要贡献者，并概述了PELT算法的目标，即通过时间衰减模型精确跟踪调度实体的负载变化，支持交互性优化、分组调度等功能。上下文不完整",
          "similarity": 0.5091543197631836
        }
      ]
    },
    {
      "source_file": "kernel/sched/sched-pelt.h",
      "md_summary": "> 自动生成时间: 2025-10-25 16:15:27\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\sched-pelt.h`\n\n---\n\n# `sched/sched-pelt.h` 技术文档\n\n## 1. 文件概述\n\n`sched/sched-pelt.h` 是 Linux 内核调度器中用于实现 **PELT（Per-Entity Load Tracking，每实体负载跟踪）** 机制的头文件。该文件定义了 PELT 算法所需的关键常量和预计算的衰减系数表，用于高效计算任务和运行队列的负载贡献。PELT 是 CFS（完全公平调度器）中用于准确跟踪 CPU 负载和利用率的核心机制，支持负载均衡、能效调度（如 EAS）等高级调度功能。\n\n## 2. 核心功能\n\n本文件不包含函数定义，主要提供以下数据结构和宏定义：\n\n- **`runnable_avg_yN_inv[]`**：一个预计算的 32 位无符号整型数组，存储 PELT 算法中用于指数衰减计算的倒数系数。\n- **`LOAD_AVG_PERIOD`**：定义 PELT 负载计算的基本周期长度（单位为调度周期数），值为 32。\n- **`LOAD_AVG_MAX`**：表示 PELT 负载值的理论最大值（约为 47742），用于归一化和防止溢出。\n\n## 3. 关键实现\n\n### PELT 衰减模型\nPELT 将时间划分为 1ms 的调度周期（`SCHED_CAPACITY_SCALE` 相关），并假设负载在每个周期内呈指数衰减。负载贡献按如下方式累积：\n\n$$\nL(t) = L_0 \\cdot y + L_1 \\cdot y^2 + L_2 \\cdot y^3 + \\cdots\n$$\n\n其中 $ y = e^{-\\Delta t / T} $，$ T $ 为时间常数（通常为 32ms），$ \\Delta t $ 为周期长度（1ms）。因此 $ y \\approx 0.96875 $。\n\n### 预计算系数表\n`runnable_avg_yN_inv[]` 数组存储的是 $ 1 / y^n $ 的 32 位定点数近似值（Q31 格式），用于在运行时通过乘法代替除法，加速衰减计算。例如：\n- `runnable_avg_yN_inv[0] = 0xffffffff` 对应 $ 1 / y^0 = 1 $\n- 后续项依次对应 $ 1 / y^1, 1 / y^2, \\dots, 1 / y^{31} $\n\n该表由脚本 `Documentation/scheduler/sched-pelt` 自动生成，确保精度与性能平衡。\n\n### 负载归一化\n- `LOAD_AVG_PERIOD = 32` 表示每 32 个调度周期（约 32ms）构成一个完整的衰减窗口。\n- `LOAD_AVG_MAX = 47742` 是当实体持续 100% 可运行时，PELT 累积负载的稳态最大值，计算公式为：\n\n$$\n\\text{LOAD\\_AVG\\_MAX} = \\sum_{i=0}^{\\infty} y^i \\cdot \\text{period\\_contrib} \\approx \\frac{1024 - 1024 \\cdot y^{32}}{1 - y}\n$$\n\n该值用于将原始负载值映射到 [0, 1024] 或 [0, SCHED_CAPACITY_SCALE] 的标准化范围。\n\n## 4. 依赖关系\n\n- **调度核心模块**：被 `kernel/sched/pelt.c` 和 `kernel/sched/fair.c` 包含，用于实现 `___update_load_avg()` 等负载更新函数。\n- **调度类**：CFS 调度类（`struct sched_class` 的 `fair_sched_class`）依赖此文件进行任务和运行队列的负载跟踪。\n- **能效调度（EAS）**：EAS 使用 PELT 提供的利用率信号进行 CPU 频率选择和任务放置。\n- **负载均衡器**：`load_balance()` 等函数利用 PELT 负载值评估 CPU 间负载差异。\n\n## 5. 使用场景\n\n- **任务负载跟踪**：每个 `struct sched_entity` 使用 PELT 计算其对 CPU 的负载贡献。\n- **运行队列负载聚合**：`struct cfs_rq` 聚合其下所有任务的 PELT 负载，用于负载均衡决策。\n- **CPU 利用率估计**：为 CPUFreq 的 schedutil 调频策略提供实时利用率输入。\n- **能效感知调度**：在异构多核系统（如 big.LITTLE）中，基于 PELT 利用率进行任务迁移以优化能效。\n- **热插拔与 CPU 疲劳管理**：系统根据 PELT 负载动态启停 CPU 核心。",
      "similarity": 0.5528355240821838,
      "chunks": []
    }
  ]
}