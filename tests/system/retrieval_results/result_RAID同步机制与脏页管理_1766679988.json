{
  "query": "RAID同步机制与脏页管理",
  "timestamp": "2025-12-26 00:26:28",
  "retrieved_files": [
    {
      "source_file": "mm/page-writeback.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:09\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page-writeback.c`\n\n---\n\n# page-writeback.c 技术文档\n\n## 1. 文件概述\n\n`page-writeback.c` 是 Linux 内核内存管理子系统（MM）中的核心文件，负责实现**脏页回写（dirty page writeback）机制**。该机制用于控制和协调将修改过的页面（即“脏页”）从内存写回到持久化存储（如磁盘）的过程，以确保数据一致性、防止内存耗尽，并在系统负载与 I/O 带宽之间取得平衡。\n\n该文件主要提供以下功能：\n- 脏页数量的全局与每 BDI（Backing Device Info）级别的阈值管理\n- 脏页生成速率的动态限流（throttling）\n- 后台回写线程（如 `writeback` 线程）的触发逻辑\n- 支持基于 cgroup 的内存回写控制（当启用 `CONFIG_CGROUP_WRITEBACK` 时）\n- 与 `/proc/sys/vm` 中可调参数的交互接口\n\n## 2. 核心功能\n\n### 主要全局变量（可通过 sysctl 调整）\n| 变量名 | 默认值 | 说明 |\n|--------|--------|------|\n| `dirty_background_ratio` | 10 | 当脏页占可用内存比例达到此值时，启动后台回写 |\n| `vm_dirty_ratio` | 20 | 脏页比例硬上限，超过则阻塞写进程进行同步回写 |\n| `dirty_background_bytes` | 0 | 以字节为单位指定后台回写阈值（优先级高于 ratio） |\n| `vm_dirty_bytes` | 0 | 以字节为单位指定脏页硬上限（优先级高于 ratio） |\n| `dirty_writeback_interval` | 500 (5秒) | 后台回写线程的唤醒间隔（单位：厘秒） |\n| `dirty_expire_interval` | 3000 (30秒) | 脏页最大存活时间，超时强制回写 |\n| `laptop_mode` | 0 | 笔记本模式开关，减少磁盘活动以省电 |\n| `ratelimit_pages` | 32 | 每 CPU 脏页速率限制阈值 |\n\n### 关键数据结构\n- **`struct wb_domain`**  \n  回写域（writeback domain），用于聚合多个 BDI 的回写状态，支持全局或 per-memcg 的回写控制。\n  \n- **`struct dirty_throttle_control` (dtc)**  \n  脏页限流控制上下文，包含：\n  - `avail`：当前可脏化的内存总量\n  - `dirty`：当前脏页数量\n  - `thresh` / `bg_thresh`：硬/软回写阈值\n  - `wb_dirty` / `wb_thresh` / `wb_bg_thresh`：per-BDI 级别的对应值\n  - `pos_ratio`：用于计算回写速率的比例因子\n\n- **条件编译支持**  \n  通过 `CONFIG_CGROUP_WRITEBACK` 区分是否支持 memcg 级别的回写控制，提供 `GDTC_INIT`、`MDTC_INIT` 等宏及辅助函数（如 `mdtc_valid()`、`wb_min_max_ratio()`）。\n\n### 核心辅助函数（部分在截断代码中未完整显示）\n- `node_dirtyable_memory()`：计算指定 NUMA 节点中可用于脏页缓存的内存总量（包括空闲页 + 文件缓存页 - 保留页）。\n- `balance_dirty_pages()`：主限流函数，在进程写入时被调用，根据当前脏页水位决定是否休眠或触发回写。\n- `balance_dirty_pages_ratelimited()`：带速率限制的脏页平衡入口，避免频繁调用开销。\n\n## 3. 关键实现\n\n### 脏页阈值计算逻辑\n- 脏页上限基于 **“dirtyable memory”** 计算，即 `(free pages + file cache pages - kernel reserves)`。\n- 支持两种配置方式：**百分比（ratio）** 或 **绝对字节数（bytes）**，后者优先。\n- 当启用 `vm_highmem_is_dirtyable` 时，highmem 区域的空闲页也计入 dirtyable memory。\n\n### 动态限流机制\n- 使用 **`MAX_PAUSE`（最大 200ms）** 限制单次 `balance_dirty_pages()` 的休眠时间。\n- 引入 **`DIRTY_POLL_THRESH`（128KB）** 作为调用间隔优化阈值：若脏页增长过快，则提升休眠时间至最大值。\n- 通过 **`BANDWIDTH_INTERVAL`（200ms）** 动态估算存储设备的写入带宽，用于调整回写速率。\n\n### cgroup writeback 支持\n- 在 `CONFIG_CGROUP_WRITEBACK` 启用时：\n  - 每个 memcg 有独立的 `wb_domain`\n  - `dirty_throttle_control` 可关联全局（gdtc）或 memcg（mdtc）上下文\n  - BDI 的 min/max_ratio 根据其实际带宽动态缩放，实现公平分配\n\n### 老化与完成计数\n- 使用 `fprop_local_percpu` 结构跟踪每个 BDI 的回写完成情况。\n- `VM_COMPLETIONS_PERIOD_LEN`（3 秒）定义了回写完成率的老化周期，影响带宽估算的响应速度。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/swap.h>`、`<linux/pagevec.h>` 等，与页分配、回收机制紧密集成。\n- **VFS 层**：通过 `<linux/fs.h>`、`<linux/pagemap.h>` 与 address_space 和 inode 交互。\n- **块设备层**：通过 `<linux/blkdev.h>`、`<linux/backing-dev.h>` 获取 BDI 信息和 I/O 能力。\n- **调度与同步**：使用 `<linux/sched.h>`、`<linux/spinlock.h>`、`<linux/timer.h>` 实现休眠、锁和定时器。\n- **追踪系统**：集成 `<trace/events/writeback.h>` 提供回写事件追踪点。\n- **内部头文件**：包含 `\"internal.h\"` 获取 MM 子系统内部接口。\n\n## 5. 使用场景\n\n1. **用户空间写入文件**  \n   当进程通过 `write()` 修改文件页时，页被标记为脏，随后调用 `balance_dirty_pages_ratelimited()` 触发脏页控制。\n\n2. **内存压力下的页面回收**  \n   kswapd 或直接回收路径在需要释放内存时，可能调用回写逻辑清理脏页。\n\n3. **定期后台回写**  \n   `writeback` 内核线程按 `dirty_writeback_interval` 周期唤醒，检查并回写超过 `dirty_expire_interval` 的脏页。\n\n4. **系统关闭或 sync 调用**  \n   虽然主要同步逻辑在其他文件，但本文件提供的阈值和状态是决策基础。\n\n5. **容器环境中的资源隔离**  \n   启用 cgroup writeback 后，不同 memcg 的脏页回写相互隔离，避免一个容器的大量写入影响其他容器性能。\n\n6. **笔记本省电模式**  \n   当 `laptop_mode` 启用时，延迟回写以减少磁盘旋转时间，延长电池寿命。",
      "similarity": 0.6298643350601196,
      "chunks": [
        {
          "chunk_id": 9,
          "file_path": "mm/page-writeback.c",
          "start_line": 1662,
          "end_line": 1988,
          "content": [
            "static inline void wb_dirty_limits(struct dirty_throttle_control *dtc)",
            "{",
            "\tstruct bdi_writeback *wb = dtc->wb;",
            "\tunsigned long wb_reclaimable;",
            "",
            "\t/*",
            "\t * wb_thresh is not treated as some limiting factor as",
            "\t * dirty_thresh, due to reasons",
            "\t * - in JBOD setup, wb_thresh can fluctuate a lot",
            "\t * - in a system with HDD and USB key, the USB key may somehow",
            "\t *   go into state (wb_dirty >> wb_thresh) either because",
            "\t *   wb_dirty starts high, or because wb_thresh drops low.",
            "\t *   In this case we don't want to hard throttle the USB key",
            "\t *   dirtiers for 100 seconds until wb_dirty drops under",
            "\t *   wb_thresh. Instead the auxiliary wb control line in",
            "\t *   wb_position_ratio() will let the dirtier task progress",
            "\t *   at some rate <= (write_bw / 2) for bringing down wb_dirty.",
            "\t */",
            "\tdtc->wb_thresh = __wb_calc_thresh(dtc);",
            "\tdtc->wb_bg_thresh = dtc->thresh ?",
            "\t\tdiv_u64((u64)dtc->wb_thresh * dtc->bg_thresh, dtc->thresh) : 0;",
            "",
            "\t/*",
            "\t * In order to avoid the stacked BDI deadlock we need",
            "\t * to ensure we accurately count the 'dirty' pages when",
            "\t * the threshold is low.",
            "\t *",
            "\t * Otherwise it would be possible to get thresh+n pages",
            "\t * reported dirty, even though there are thresh-m pages",
            "\t * actually dirty; with m+n sitting in the percpu",
            "\t * deltas.",
            "\t */",
            "\tif (dtc->wb_thresh < 2 * wb_stat_error()) {",
            "\t\twb_reclaimable = wb_stat_sum(wb, WB_RECLAIMABLE);",
            "\t\tdtc->wb_dirty = wb_reclaimable + wb_stat_sum(wb, WB_WRITEBACK);",
            "\t} else {",
            "\t\twb_reclaimable = wb_stat(wb, WB_RECLAIMABLE);",
            "\t\tdtc->wb_dirty = wb_reclaimable + wb_stat(wb, WB_WRITEBACK);",
            "\t}",
            "}",
            "static int balance_dirty_pages(struct bdi_writeback *wb,",
            "\t\t\t       unsigned long pages_dirtied, unsigned int flags)",
            "{",
            "\tstruct dirty_throttle_control gdtc_stor = { GDTC_INIT(wb) };",
            "\tstruct dirty_throttle_control mdtc_stor = { MDTC_INIT(wb, &gdtc_stor) };",
            "\tstruct dirty_throttle_control * const gdtc = &gdtc_stor;",
            "\tstruct dirty_throttle_control * const mdtc = mdtc_valid(&mdtc_stor) ?",
            "\t\t\t\t\t\t     &mdtc_stor : NULL;",
            "\tstruct dirty_throttle_control *sdtc;",
            "\tunsigned long nr_dirty;",
            "\tlong period;",
            "\tlong pause;",
            "\tlong max_pause;",
            "\tlong min_pause;",
            "\tint nr_dirtied_pause;",
            "\tbool dirty_exceeded = false;",
            "\tunsigned long task_ratelimit;",
            "\tunsigned long dirty_ratelimit;",
            "\tstruct backing_dev_info *bdi = wb->bdi;",
            "\tbool strictlimit = bdi->capabilities & BDI_CAP_STRICTLIMIT;",
            "\tunsigned long start_time = jiffies;",
            "\tint ret = 0;",
            "",
            "\tfor (;;) {",
            "\t\tunsigned long now = jiffies;",
            "\t\tunsigned long dirty, thresh, bg_thresh;",
            "\t\tunsigned long m_dirty = 0;\t/* stop bogus uninit warnings */",
            "\t\tunsigned long m_thresh = 0;",
            "\t\tunsigned long m_bg_thresh = 0;",
            "",
            "\t\tnr_dirty = global_node_page_state(NR_FILE_DIRTY);",
            "\t\tgdtc->avail = global_dirtyable_memory();",
            "\t\tgdtc->dirty = nr_dirty + global_node_page_state(NR_WRITEBACK);",
            "",
            "\t\tdomain_dirty_limits(gdtc);",
            "",
            "\t\tif (unlikely(strictlimit)) {",
            "\t\t\twb_dirty_limits(gdtc);",
            "",
            "\t\t\tdirty = gdtc->wb_dirty;",
            "\t\t\tthresh = gdtc->wb_thresh;",
            "\t\t\tbg_thresh = gdtc->wb_bg_thresh;",
            "\t\t} else {",
            "\t\t\tdirty = gdtc->dirty;",
            "\t\t\tthresh = gdtc->thresh;",
            "\t\t\tbg_thresh = gdtc->bg_thresh;",
            "\t\t}",
            "",
            "\t\tif (mdtc) {",
            "\t\t\tunsigned long filepages, headroom, writeback;",
            "",
            "\t\t\t/*",
            "\t\t\t * If @wb belongs to !root memcg, repeat the same",
            "\t\t\t * basic calculations for the memcg domain.",
            "\t\t\t */",
            "\t\t\tmem_cgroup_wb_stats(wb, &filepages, &headroom,",
            "\t\t\t\t\t    &mdtc->dirty, &writeback);",
            "\t\t\tmdtc->dirty += writeback;",
            "\t\t\tmdtc_calc_avail(mdtc, filepages, headroom);",
            "",
            "\t\t\tdomain_dirty_limits(mdtc);",
            "",
            "\t\t\tif (unlikely(strictlimit)) {",
            "\t\t\t\twb_dirty_limits(mdtc);",
            "\t\t\t\tm_dirty = mdtc->wb_dirty;",
            "\t\t\t\tm_thresh = mdtc->wb_thresh;",
            "\t\t\t\tm_bg_thresh = mdtc->wb_bg_thresh;",
            "\t\t\t} else {",
            "\t\t\t\tm_dirty = mdtc->dirty;",
            "\t\t\t\tm_thresh = mdtc->thresh;",
            "\t\t\t\tm_bg_thresh = mdtc->bg_thresh;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * In laptop mode, we wait until hitting the higher threshold",
            "\t\t * before starting background writeout, and then write out all",
            "\t\t * the way down to the lower threshold.  So slow writers cause",
            "\t\t * minimal disk activity.",
            "\t\t *",
            "\t\t * In normal mode, we start background writeout at the lower",
            "\t\t * background_thresh, to keep the amount of dirty memory low.",
            "\t\t */",
            "\t\tif (!laptop_mode && nr_dirty > gdtc->bg_thresh &&",
            "\t\t    !writeback_in_progress(wb))",
            "\t\t\twb_start_background_writeback(wb);",
            "",
            "\t\t/*",
            "\t\t * Throttle it only when the background writeback cannot",
            "\t\t * catch-up. This avoids (excessively) small writeouts",
            "\t\t * when the wb limits are ramping up in case of !strictlimit.",
            "\t\t *",
            "\t\t * In strictlimit case make decision based on the wb counters",
            "\t\t * and limits. Small writeouts when the wb limits are ramping",
            "\t\t * up are the price we consciously pay for strictlimit-ing.",
            "\t\t *",
            "\t\t * If memcg domain is in effect, @dirty should be under",
            "\t\t * both global and memcg freerun ceilings.",
            "\t\t */",
            "\t\tif (dirty <= dirty_freerun_ceiling(thresh, bg_thresh) &&",
            "\t\t    (!mdtc ||",
            "\t\t     m_dirty <= dirty_freerun_ceiling(m_thresh, m_bg_thresh))) {",
            "\t\t\tunsigned long intv;",
            "\t\t\tunsigned long m_intv;",
            "",
            "free_running:",
            "\t\t\tintv = dirty_poll_interval(dirty, thresh);",
            "\t\t\tm_intv = ULONG_MAX;",
            "",
            "\t\t\tcurrent->dirty_paused_when = now;",
            "\t\t\tcurrent->nr_dirtied = 0;",
            "\t\t\tif (mdtc)",
            "\t\t\t\tm_intv = dirty_poll_interval(m_dirty, m_thresh);",
            "\t\t\tcurrent->nr_dirtied_pause = min(intv, m_intv);",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t/* Start writeback even when in laptop mode */",
            "\t\tif (unlikely(!writeback_in_progress(wb)))",
            "\t\t\twb_start_background_writeback(wb);",
            "",
            "\t\tmem_cgroup_flush_foreign(wb);",
            "",
            "\t\t/*",
            "\t\t * Calculate global domain's pos_ratio and select the",
            "\t\t * global dtc by default.",
            "\t\t */",
            "\t\tif (!strictlimit) {",
            "\t\t\twb_dirty_limits(gdtc);",
            "",
            "\t\t\tif ((current->flags & PF_LOCAL_THROTTLE) &&",
            "\t\t\t    gdtc->wb_dirty <",
            "\t\t\t    dirty_freerun_ceiling(gdtc->wb_thresh,",
            "\t\t\t\t\t\t  gdtc->wb_bg_thresh))",
            "\t\t\t\t/*",
            "\t\t\t\t * LOCAL_THROTTLE tasks must not be throttled",
            "\t\t\t\t * when below the per-wb freerun ceiling.",
            "\t\t\t\t */",
            "\t\t\t\tgoto free_running;",
            "\t\t}",
            "",
            "\t\tdirty_exceeded = (gdtc->wb_dirty > gdtc->wb_thresh) &&",
            "\t\t\t((gdtc->dirty > gdtc->thresh) || strictlimit);",
            "",
            "\t\twb_position_ratio(gdtc);",
            "\t\tsdtc = gdtc;",
            "",
            "\t\tif (mdtc) {",
            "\t\t\t/*",
            "\t\t\t * If memcg domain is in effect, calculate its",
            "\t\t\t * pos_ratio.  @wb should satisfy constraints from",
            "\t\t\t * both global and memcg domains.  Choose the one",
            "\t\t\t * w/ lower pos_ratio.",
            "\t\t\t */",
            "\t\t\tif (!strictlimit) {",
            "\t\t\t\twb_dirty_limits(mdtc);",
            "",
            "\t\t\t\tif ((current->flags & PF_LOCAL_THROTTLE) &&",
            "\t\t\t\t    mdtc->wb_dirty <",
            "\t\t\t\t    dirty_freerun_ceiling(mdtc->wb_thresh,",
            "\t\t\t\t\t\t\t  mdtc->wb_bg_thresh))",
            "\t\t\t\t\t/*",
            "\t\t\t\t\t * LOCAL_THROTTLE tasks must not be",
            "\t\t\t\t\t * throttled when below the per-wb",
            "\t\t\t\t\t * freerun ceiling.",
            "\t\t\t\t\t */",
            "\t\t\t\t\tgoto free_running;",
            "\t\t\t}",
            "\t\t\tdirty_exceeded |= (mdtc->wb_dirty > mdtc->wb_thresh) &&",
            "\t\t\t\t((mdtc->dirty > mdtc->thresh) || strictlimit);",
            "",
            "\t\t\twb_position_ratio(mdtc);",
            "\t\t\tif (mdtc->pos_ratio < gdtc->pos_ratio)",
            "\t\t\t\tsdtc = mdtc;",
            "\t\t}",
            "",
            "\t\tif (dirty_exceeded != wb->dirty_exceeded)",
            "\t\t\twb->dirty_exceeded = dirty_exceeded;",
            "",
            "\t\tif (time_is_before_jiffies(READ_ONCE(wb->bw_time_stamp) +",
            "\t\t\t\t\t   BANDWIDTH_INTERVAL))",
            "\t\t\t__wb_update_bandwidth(gdtc, mdtc, true);",
            "",
            "\t\t/* throttle according to the chosen dtc */",
            "\t\tdirty_ratelimit = READ_ONCE(wb->dirty_ratelimit);",
            "\t\ttask_ratelimit = ((u64)dirty_ratelimit * sdtc->pos_ratio) >>",
            "\t\t\t\t\t\t\tRATELIMIT_CALC_SHIFT;",
            "\t\tmax_pause = wb_max_pause(wb, sdtc->wb_dirty);",
            "\t\tmin_pause = wb_min_pause(wb, max_pause,",
            "\t\t\t\t\t task_ratelimit, dirty_ratelimit,",
            "\t\t\t\t\t &nr_dirtied_pause);",
            "",
            "\t\tif (unlikely(task_ratelimit == 0)) {",
            "\t\t\tperiod = max_pause;",
            "\t\t\tpause = max_pause;",
            "\t\t\tgoto pause;",
            "\t\t}",
            "\t\tperiod = HZ * pages_dirtied / task_ratelimit;",
            "\t\tpause = period;",
            "\t\tif (current->dirty_paused_when)",
            "\t\t\tpause -= now - current->dirty_paused_when;",
            "\t\t/*",
            "\t\t * For less than 1s think time (ext3/4 may block the dirtier",
            "\t\t * for up to 800ms from time to time on 1-HDD; so does xfs,",
            "\t\t * however at much less frequency), try to compensate it in",
            "\t\t * future periods by updating the virtual time; otherwise just",
            "\t\t * do a reset, as it may be a light dirtier.",
            "\t\t */",
            "\t\tif (pause < min_pause) {",
            "\t\t\ttrace_balance_dirty_pages(wb,",
            "\t\t\t\t\t\t  sdtc->thresh,",
            "\t\t\t\t\t\t  sdtc->bg_thresh,",
            "\t\t\t\t\t\t  sdtc->dirty,",
            "\t\t\t\t\t\t  sdtc->wb_thresh,",
            "\t\t\t\t\t\t  sdtc->wb_dirty,",
            "\t\t\t\t\t\t  dirty_ratelimit,",
            "\t\t\t\t\t\t  task_ratelimit,",
            "\t\t\t\t\t\t  pages_dirtied,",
            "\t\t\t\t\t\t  period,",
            "\t\t\t\t\t\t  min(pause, 0L),",
            "\t\t\t\t\t\t  start_time);",
            "\t\t\tif (pause < -HZ) {",
            "\t\t\t\tcurrent->dirty_paused_when = now;",
            "\t\t\t\tcurrent->nr_dirtied = 0;",
            "\t\t\t} else if (period) {",
            "\t\t\t\tcurrent->dirty_paused_when += period;",
            "\t\t\t\tcurrent->nr_dirtied = 0;",
            "\t\t\t} else if (current->nr_dirtied_pause <= pages_dirtied)",
            "\t\t\t\tcurrent->nr_dirtied_pause += pages_dirtied;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tif (unlikely(pause > max_pause)) {",
            "\t\t\t/* for occasional dropped task_ratelimit */",
            "\t\t\tnow += min(pause - max_pause, max_pause);",
            "\t\t\tpause = max_pause;",
            "\t\t}",
            "",
            "pause:",
            "\t\ttrace_balance_dirty_pages(wb,",
            "\t\t\t\t\t  sdtc->thresh,",
            "\t\t\t\t\t  sdtc->bg_thresh,",
            "\t\t\t\t\t  sdtc->dirty,",
            "\t\t\t\t\t  sdtc->wb_thresh,",
            "\t\t\t\t\t  sdtc->wb_dirty,",
            "\t\t\t\t\t  dirty_ratelimit,",
            "\t\t\t\t\t  task_ratelimit,",
            "\t\t\t\t\t  pages_dirtied,",
            "\t\t\t\t\t  period,",
            "\t\t\t\t\t  pause,",
            "\t\t\t\t\t  start_time);",
            "\t\tif (flags & BDP_ASYNC) {",
            "\t\t\tret = -EAGAIN;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\t__set_current_state(TASK_KILLABLE);",
            "\t\tbdi->last_bdp_sleep = jiffies;",
            "\t\tio_schedule_timeout(pause);",
            "",
            "\t\tcurrent->dirty_paused_when = now + pause;",
            "\t\tcurrent->nr_dirtied = 0;",
            "\t\tcurrent->nr_dirtied_pause = nr_dirtied_pause;",
            "",
            "\t\t/*",
            "\t\t * This is typically equal to (dirty < thresh) and can also",
            "\t\t * keep \"1000+ dd on a slow USB stick\" under control.",
            "\t\t */",
            "\t\tif (task_ratelimit)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * In the case of an unresponsive NFS server and the NFS dirty",
            "\t\t * pages exceeds dirty_thresh, give the other good wb's a pipe",
            "\t\t * to go through, so that tasks on them still remain responsive.",
            "\t\t *",
            "\t\t * In theory 1 page is enough to keep the consumer-producer",
            "\t\t * pipe going: the flusher cleans 1 page => the task dirties 1",
            "\t\t * more page. However wb_dirty has accounting errors.  So use",
            "\t\t * the larger and more IO friendly wb_stat_error.",
            "\t\t */",
            "\t\tif (sdtc->wb_dirty <= wb_stat_error())",
            "\t\t\tbreak;",
            "",
            "\t\tif (fatal_signal_pending(current))",
            "\t\t\tbreak;",
            "\t}",
            "\treturn ret;",
            "}"
          ],
          "function_name": "wb_dirty_limits, balance_dirty_pages",
          "description": "实现脏页管理核心逻辑，包含确定当前脏页限制的计算函数和脏页平衡主函数，通过多级阈值检测、动态速率限制、暂停时间控制等机制，在保证系统响应性的同时防止内存过载，支持严格限制模式下的特殊处理。",
          "similarity": 0.7004736065864563
        },
        {
          "chunk_id": 7,
          "file_path": "mm/page-writeback.c",
          "start_line": 1221,
          "end_line": 1472,
          "content": [
            "static void wb_update_write_bandwidth(struct bdi_writeback *wb,",
            "\t\t\t\t      unsigned long elapsed,",
            "\t\t\t\t      unsigned long written)",
            "{",
            "\tconst unsigned long period = roundup_pow_of_two(3 * HZ);",
            "\tunsigned long avg = wb->avg_write_bandwidth;",
            "\tunsigned long old = wb->write_bandwidth;",
            "\tu64 bw;",
            "",
            "\t/*",
            "\t * bw = written * HZ / elapsed",
            "\t *",
            "\t *                   bw * elapsed + write_bandwidth * (period - elapsed)",
            "\t * write_bandwidth = ---------------------------------------------------",
            "\t *                                          period",
            "\t *",
            "\t * @written may have decreased due to folio_redirty_for_writepage().",
            "\t * Avoid underflowing @bw calculation.",
            "\t */",
            "\tbw = written - min(written, wb->written_stamp);",
            "\tbw *= HZ;",
            "\tif (unlikely(elapsed > period)) {",
            "\t\tbw = div64_ul(bw, elapsed);",
            "\t\tavg = bw;",
            "\t\tgoto out;",
            "\t}",
            "\tbw += (u64)wb->write_bandwidth * (period - elapsed);",
            "\tbw >>= ilog2(period);",
            "",
            "\t/*",
            "\t * one more level of smoothing, for filtering out sudden spikes",
            "\t */",
            "\tif (avg > old && old >= (unsigned long)bw)",
            "\t\tavg -= (avg - old) >> 3;",
            "",
            "\tif (avg < old && old <= (unsigned long)bw)",
            "\t\tavg += (old - avg) >> 3;",
            "",
            "out:",
            "\t/* keep avg > 0 to guarantee that tot > 0 if there are dirty wbs */",
            "\tavg = max(avg, 1LU);",
            "\tif (wb_has_dirty_io(wb)) {",
            "\t\tlong delta = avg - wb->avg_write_bandwidth;",
            "\t\tWARN_ON_ONCE(atomic_long_add_return(delta,",
            "\t\t\t\t\t&wb->bdi->tot_write_bandwidth) <= 0);",
            "\t}",
            "\twb->write_bandwidth = bw;",
            "\tWRITE_ONCE(wb->avg_write_bandwidth, avg);",
            "}",
            "static void update_dirty_limit(struct dirty_throttle_control *dtc)",
            "{",
            "\tstruct wb_domain *dom = dtc_dom(dtc);",
            "\tunsigned long thresh = dtc->thresh;",
            "\tunsigned long limit = dom->dirty_limit;",
            "",
            "\t/*",
            "\t * Follow up in one step.",
            "\t */",
            "\tif (limit < thresh) {",
            "\t\tlimit = thresh;",
            "\t\tgoto update;",
            "\t}",
            "",
            "\t/*",
            "\t * Follow down slowly. Use the higher one as the target, because thresh",
            "\t * may drop below dirty. This is exactly the reason to introduce",
            "\t * dom->dirty_limit which is guaranteed to lie above the dirty pages.",
            "\t */",
            "\tthresh = max(thresh, dtc->dirty);",
            "\tif (limit > thresh) {",
            "\t\tlimit -= (limit - thresh) >> 5;",
            "\t\tgoto update;",
            "\t}",
            "\treturn;",
            "update:",
            "\tdom->dirty_limit = limit;",
            "}",
            "static void domain_update_dirty_limit(struct dirty_throttle_control *dtc,",
            "\t\t\t\t      unsigned long now)",
            "{",
            "\tstruct wb_domain *dom = dtc_dom(dtc);",
            "",
            "\t/*",
            "\t * check locklessly first to optimize away locking for the most time",
            "\t */",
            "\tif (time_before(now, dom->dirty_limit_tstamp + BANDWIDTH_INTERVAL))",
            "\t\treturn;",
            "",
            "\tspin_lock(&dom->lock);",
            "\tif (time_after_eq(now, dom->dirty_limit_tstamp + BANDWIDTH_INTERVAL)) {",
            "\t\tupdate_dirty_limit(dtc);",
            "\t\tdom->dirty_limit_tstamp = now;",
            "\t}",
            "\tspin_unlock(&dom->lock);",
            "}",
            "static void wb_update_dirty_ratelimit(struct dirty_throttle_control *dtc,",
            "\t\t\t\t      unsigned long dirtied,",
            "\t\t\t\t      unsigned long elapsed)",
            "{",
            "\tstruct bdi_writeback *wb = dtc->wb;",
            "\tunsigned long dirty = dtc->dirty;",
            "\tunsigned long freerun = dirty_freerun_ceiling(dtc->thresh, dtc->bg_thresh);",
            "\tunsigned long limit = hard_dirty_limit(dtc_dom(dtc), dtc->thresh);",
            "\tunsigned long setpoint = (freerun + limit) / 2;",
            "\tunsigned long write_bw = wb->avg_write_bandwidth;",
            "\tunsigned long dirty_ratelimit = wb->dirty_ratelimit;",
            "\tunsigned long dirty_rate;",
            "\tunsigned long task_ratelimit;",
            "\tunsigned long balanced_dirty_ratelimit;",
            "\tunsigned long step;",
            "\tunsigned long x;",
            "\tunsigned long shift;",
            "",
            "\t/*",
            "\t * The dirty rate will match the writeout rate in long term, except",
            "\t * when dirty pages are truncated by userspace or re-dirtied by FS.",
            "\t */",
            "\tdirty_rate = (dirtied - wb->dirtied_stamp) * HZ / elapsed;",
            "",
            "\t/*",
            "\t * task_ratelimit reflects each dd's dirty rate for the past 200ms.",
            "\t */",
            "\ttask_ratelimit = (u64)dirty_ratelimit *",
            "\t\t\t\t\tdtc->pos_ratio >> RATELIMIT_CALC_SHIFT;",
            "\ttask_ratelimit++; /* it helps rampup dirty_ratelimit from tiny values */",
            "",
            "\t/*",
            "\t * A linear estimation of the \"balanced\" throttle rate. The theory is,",
            "\t * if there are N dd tasks, each throttled at task_ratelimit, the wb's",
            "\t * dirty_rate will be measured to be (N * task_ratelimit). So the below",
            "\t * formula will yield the balanced rate limit (write_bw / N).",
            "\t *",
            "\t * Note that the expanded form is not a pure rate feedback:",
            "\t *\trate_(i+1) = rate_(i) * (write_bw / dirty_rate)\t\t     (1)",
            "\t * but also takes pos_ratio into account:",
            "\t *\trate_(i+1) = rate_(i) * (write_bw / dirty_rate) * pos_ratio  (2)",
            "\t *",
            "\t * (1) is not realistic because pos_ratio also takes part in balancing",
            "\t * the dirty rate.  Consider the state",
            "\t *\tpos_ratio = 0.5\t\t\t\t\t\t     (3)",
            "\t *\trate = 2 * (write_bw / N)\t\t\t\t     (4)",
            "\t * If (1) is used, it will stuck in that state! Because each dd will",
            "\t * be throttled at",
            "\t *\ttask_ratelimit = pos_ratio * rate = (write_bw / N)\t     (5)",
            "\t * yielding",
            "\t *\tdirty_rate = N * task_ratelimit = write_bw\t\t     (6)",
            "\t * put (6) into (1) we get",
            "\t *\trate_(i+1) = rate_(i)\t\t\t\t\t     (7)",
            "\t *",
            "\t * So we end up using (2) to always keep",
            "\t *\trate_(i+1) ~= (write_bw / N)\t\t\t\t     (8)",
            "\t * regardless of the value of pos_ratio. As long as (8) is satisfied,",
            "\t * pos_ratio is able to drive itself to 1.0, which is not only where",
            "\t * the dirty count meet the setpoint, but also where the slope of",
            "\t * pos_ratio is most flat and hence task_ratelimit is least fluctuated.",
            "\t */",
            "\tbalanced_dirty_ratelimit = div_u64((u64)task_ratelimit * write_bw,",
            "\t\t\t\t\t   dirty_rate | 1);",
            "\t/*",
            "\t * balanced_dirty_ratelimit ~= (write_bw / N) <= write_bw",
            "\t */",
            "\tif (unlikely(balanced_dirty_ratelimit > write_bw))",
            "\t\tbalanced_dirty_ratelimit = write_bw;",
            "",
            "\t/*",
            "\t * We could safely do this and return immediately:",
            "\t *",
            "\t *\twb->dirty_ratelimit = balanced_dirty_ratelimit;",
            "\t *",
            "\t * However to get a more stable dirty_ratelimit, the below elaborated",
            "\t * code makes use of task_ratelimit to filter out singular points and",
            "\t * limit the step size.",
            "\t *",
            "\t * The below code essentially only uses the relative value of",
            "\t *",
            "\t *\ttask_ratelimit - dirty_ratelimit",
            "\t *\t= (pos_ratio - 1) * dirty_ratelimit",
            "\t *",
            "\t * which reflects the direction and size of dirty position error.",
            "\t */",
            "",
            "\t/*",
            "\t * dirty_ratelimit will follow balanced_dirty_ratelimit iff",
            "\t * task_ratelimit is on the same side of dirty_ratelimit, too.",
            "\t * For example, when",
            "\t * - dirty_ratelimit > balanced_dirty_ratelimit",
            "\t * - dirty_ratelimit > task_ratelimit (dirty pages are above setpoint)",
            "\t * lowering dirty_ratelimit will help meet both the position and rate",
            "\t * control targets. Otherwise, don't update dirty_ratelimit if it will",
            "\t * only help meet the rate target. After all, what the users ultimately",
            "\t * feel and care are stable dirty rate and small position error.",
            "\t *",
            "\t * |task_ratelimit - dirty_ratelimit| is used to limit the step size",
            "\t * and filter out the singular points of balanced_dirty_ratelimit. Which",
            "\t * keeps jumping around randomly and can even leap far away at times",
            "\t * due to the small 200ms estimation period of dirty_rate (we want to",
            "\t * keep that period small to reduce time lags).",
            "\t */",
            "\tstep = 0;",
            "",
            "\t/*",
            "\t * For strictlimit case, calculations above were based on wb counters",
            "\t * and limits (starting from pos_ratio = wb_position_ratio() and up to",
            "\t * balanced_dirty_ratelimit = task_ratelimit * write_bw / dirty_rate).",
            "\t * Hence, to calculate \"step\" properly, we have to use wb_dirty as",
            "\t * \"dirty\" and wb_setpoint as \"setpoint\".",
            "\t *",
            "\t * We rampup dirty_ratelimit forcibly if wb_dirty is low because",
            "\t * it's possible that wb_thresh is close to zero due to inactivity",
            "\t * of backing device.",
            "\t */",
            "\tif (unlikely(wb->bdi->capabilities & BDI_CAP_STRICTLIMIT)) {",
            "\t\tdirty = dtc->wb_dirty;",
            "\t\tif (dtc->wb_dirty < 8)",
            "\t\t\tsetpoint = dtc->wb_dirty + 1;",
            "\t\telse",
            "\t\t\tsetpoint = (dtc->wb_thresh + dtc->wb_bg_thresh) / 2;",
            "\t}",
            "",
            "\tif (dirty < setpoint) {",
            "\t\tx = min3(wb->balanced_dirty_ratelimit,",
            "\t\t\t balanced_dirty_ratelimit, task_ratelimit);",
            "\t\tif (dirty_ratelimit < x)",
            "\t\t\tstep = x - dirty_ratelimit;",
            "\t} else {",
            "\t\tx = max3(wb->balanced_dirty_ratelimit,",
            "\t\t\t balanced_dirty_ratelimit, task_ratelimit);",
            "\t\tif (dirty_ratelimit > x)",
            "\t\t\tstep = dirty_ratelimit - x;",
            "\t}",
            "",
            "\t/*",
            "\t * Don't pursue 100% rate matching. It's impossible since the balanced",
            "\t * rate itself is constantly fluctuating. So decrease the track speed",
            "\t * when it gets close to the target. Helps eliminate pointless tremors.",
            "\t */",
            "\tshift = dirty_ratelimit / (2 * step + 1);",
            "\tif (shift < BITS_PER_LONG)",
            "\t\tstep = DIV_ROUND_UP(step >> shift, 8);",
            "\telse",
            "\t\tstep = 0;",
            "",
            "\tif (dirty_ratelimit < balanced_dirty_ratelimit)",
            "\t\tdirty_ratelimit += step;",
            "\telse",
            "\t\tdirty_ratelimit -= step;",
            "",
            "\tWRITE_ONCE(wb->dirty_ratelimit, max(dirty_ratelimit, 1UL));",
            "\twb->balanced_dirty_ratelimit = balanced_dirty_ratelimit;",
            "",
            "\ttrace_bdi_dirty_ratelimit(wb, dirty_rate, task_ratelimit);",
            "}"
          ],
          "function_name": "wb_update_write_bandwidth, update_dirty_limit, domain_update_dirty_limit, wb_update_dirty_ratelimit",
          "description": "实现写带宽统计更新逻辑，包含平滑写入速率变化的算法，更新脏页限制的动态调整机制，以及根据当前脏页速率与写入能力计算目标脏页产生速率的控制逻辑，通过分层限速策略维持系统稳定。",
          "similarity": 0.6388963460922241
        },
        {
          "chunk_id": 6,
          "file_path": "mm/page-writeback.c",
          "start_line": 883,
          "end_line": 1126,
          "content": [
            "static unsigned long __wb_calc_thresh(struct dirty_throttle_control *dtc)",
            "{",
            "\tstruct wb_domain *dom = dtc_dom(dtc);",
            "\tunsigned long thresh = dtc->thresh;",
            "\tu64 wb_thresh;",
            "\tunsigned long numerator, denominator;",
            "\tunsigned long wb_min_ratio, wb_max_ratio;",
            "",
            "\t/*",
            "\t * Calculate this BDI's share of the thresh ratio.",
            "\t */",
            "\tfprop_fraction_percpu(&dom->completions, dtc->wb_completions,",
            "\t\t\t      &numerator, &denominator);",
            "",
            "\twb_thresh = (thresh * (100 * BDI_RATIO_SCALE - bdi_min_ratio)) / (100 * BDI_RATIO_SCALE);",
            "\twb_thresh *= numerator;",
            "\twb_thresh = div64_ul(wb_thresh, denominator);",
            "",
            "\twb_min_max_ratio(dtc->wb, &wb_min_ratio, &wb_max_ratio);",
            "",
            "\twb_thresh += (thresh * wb_min_ratio) / (100 * BDI_RATIO_SCALE);",
            "\tif (wb_thresh > (thresh * wb_max_ratio) / (100 * BDI_RATIO_SCALE))",
            "\t\twb_thresh = thresh * wb_max_ratio / (100 * BDI_RATIO_SCALE);",
            "",
            "\treturn wb_thresh;",
            "}",
            "unsigned long wb_calc_thresh(struct bdi_writeback *wb, unsigned long thresh)",
            "{",
            "\tstruct dirty_throttle_control gdtc = { GDTC_INIT(wb),",
            "\t\t\t\t\t       .thresh = thresh };",
            "\treturn __wb_calc_thresh(&gdtc);",
            "}",
            "unsigned long cgwb_calc_thresh(struct bdi_writeback *wb)",
            "{",
            "\tstruct dirty_throttle_control gdtc = { GDTC_INIT_NO_WB };",
            "\tstruct dirty_throttle_control mdtc = { MDTC_INIT(wb, &gdtc) };",
            "\tunsigned long filepages = 0, headroom = 0, writeback = 0;",
            "",
            "\tgdtc.avail = global_dirtyable_memory();",
            "\tgdtc.dirty = global_node_page_state(NR_FILE_DIRTY) +",
            "\t\t     global_node_page_state(NR_WRITEBACK);",
            "",
            "\tmem_cgroup_wb_stats(wb, &filepages, &headroom,",
            "\t\t\t    &mdtc.dirty, &writeback);",
            "\tmdtc.dirty += writeback;",
            "\tmdtc_calc_avail(&mdtc, filepages, headroom);",
            "\tdomain_dirty_limits(&mdtc);",
            "",
            "\treturn __wb_calc_thresh(&mdtc);",
            "}",
            "static long long pos_ratio_polynom(unsigned long setpoint,",
            "\t\t\t\t\t  unsigned long dirty,",
            "\t\t\t\t\t  unsigned long limit)",
            "{",
            "\tlong long pos_ratio;",
            "\tlong x;",
            "",
            "\tx = div64_s64(((s64)setpoint - (s64)dirty) << RATELIMIT_CALC_SHIFT,",
            "\t\t      (limit - setpoint) | 1);",
            "\tpos_ratio = x;",
            "\tpos_ratio = pos_ratio * x >> RATELIMIT_CALC_SHIFT;",
            "\tpos_ratio = pos_ratio * x >> RATELIMIT_CALC_SHIFT;",
            "\tpos_ratio += 1 << RATELIMIT_CALC_SHIFT;",
            "",
            "\treturn clamp(pos_ratio, 0LL, 2LL << RATELIMIT_CALC_SHIFT);",
            "}",
            "static void wb_position_ratio(struct dirty_throttle_control *dtc)",
            "{",
            "\tstruct bdi_writeback *wb = dtc->wb;",
            "\tunsigned long write_bw = READ_ONCE(wb->avg_write_bandwidth);",
            "\tunsigned long freerun = dirty_freerun_ceiling(dtc->thresh, dtc->bg_thresh);",
            "\tunsigned long limit = hard_dirty_limit(dtc_dom(dtc), dtc->thresh);",
            "\tunsigned long wb_thresh = dtc->wb_thresh;",
            "\tunsigned long x_intercept;",
            "\tunsigned long setpoint;\t\t/* dirty pages' target balance point */",
            "\tunsigned long wb_setpoint;",
            "\tunsigned long span;",
            "\tlong long pos_ratio;\t\t/* for scaling up/down the rate limit */",
            "\tlong x;",
            "",
            "\tdtc->pos_ratio = 0;",
            "",
            "\tif (unlikely(dtc->dirty >= limit))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * global setpoint",
            "\t *",
            "\t * See comment for pos_ratio_polynom().",
            "\t */",
            "\tsetpoint = (freerun + limit) / 2;",
            "\tpos_ratio = pos_ratio_polynom(setpoint, dtc->dirty, limit);",
            "",
            "\t/*",
            "\t * The strictlimit feature is a tool preventing mistrusted filesystems",
            "\t * from growing a large number of dirty pages before throttling. For",
            "\t * such filesystems balance_dirty_pages always checks wb counters",
            "\t * against wb limits. Even if global \"nr_dirty\" is under \"freerun\".",
            "\t * This is especially important for fuse which sets bdi->max_ratio to",
            "\t * 1% by default. Without strictlimit feature, fuse writeback may",
            "\t * consume arbitrary amount of RAM because it is accounted in",
            "\t * NR_WRITEBACK_TEMP which is not involved in calculating \"nr_dirty\".",
            "\t *",
            "\t * Here, in wb_position_ratio(), we calculate pos_ratio based on",
            "\t * two values: wb_dirty and wb_thresh. Let's consider an example:",
            "\t * total amount of RAM is 16GB, bdi->max_ratio is equal to 1%, global",
            "\t * limits are set by default to 10% and 20% (background and throttle).",
            "\t * Then wb_thresh is 1% of 20% of 16GB. This amounts to ~8K pages.",
            "\t * wb_calc_thresh(wb, bg_thresh) is about ~4K pages. wb_setpoint is",
            "\t * about ~6K pages (as the average of background and throttle wb",
            "\t * limits). The 3rd order polynomial will provide positive feedback if",
            "\t * wb_dirty is under wb_setpoint and vice versa.",
            "\t *",
            "\t * Note, that we cannot use global counters in these calculations",
            "\t * because we want to throttle process writing to a strictlimit wb",
            "\t * much earlier than global \"freerun\" is reached (~23MB vs. ~2.3GB",
            "\t * in the example above).",
            "\t */",
            "\tif (unlikely(wb->bdi->capabilities & BDI_CAP_STRICTLIMIT)) {",
            "\t\tlong long wb_pos_ratio;",
            "",
            "\t\tif (dtc->wb_dirty < 8) {",
            "\t\t\tdtc->pos_ratio = min_t(long long, pos_ratio * 2,",
            "\t\t\t\t\t   2 << RATELIMIT_CALC_SHIFT);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tif (dtc->wb_dirty >= wb_thresh)",
            "\t\t\treturn;",
            "",
            "\t\twb_setpoint = dirty_freerun_ceiling(wb_thresh,",
            "\t\t\t\t\t\t    dtc->wb_bg_thresh);",
            "",
            "\t\tif (wb_setpoint == 0 || wb_setpoint == wb_thresh)",
            "\t\t\treturn;",
            "",
            "\t\twb_pos_ratio = pos_ratio_polynom(wb_setpoint, dtc->wb_dirty,",
            "\t\t\t\t\t\t wb_thresh);",
            "",
            "\t\t/*",
            "\t\t * Typically, for strictlimit case, wb_setpoint << setpoint",
            "\t\t * and pos_ratio >> wb_pos_ratio. In the other words global",
            "\t\t * state (\"dirty\") is not limiting factor and we have to",
            "\t\t * make decision based on wb counters. But there is an",
            "\t\t * important case when global pos_ratio should get precedence:",
            "\t\t * global limits are exceeded (e.g. due to activities on other",
            "\t\t * wb's) while given strictlimit wb is below limit.",
            "\t\t *",
            "\t\t * \"pos_ratio * wb_pos_ratio\" would work for the case above,",
            "\t\t * but it would look too non-natural for the case of all",
            "\t\t * activity in the system coming from a single strictlimit wb",
            "\t\t * with bdi->max_ratio == 100%.",
            "\t\t *",
            "\t\t * Note that min() below somewhat changes the dynamics of the",
            "\t\t * control system. Normally, pos_ratio value can be well over 3",
            "\t\t * (when globally we are at freerun and wb is well below wb",
            "\t\t * setpoint). Now the maximum pos_ratio in the same situation",
            "\t\t * is 2. We might want to tweak this if we observe the control",
            "\t\t * system is too slow to adapt.",
            "\t\t */",
            "\t\tdtc->pos_ratio = min(pos_ratio, wb_pos_ratio);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * We have computed basic pos_ratio above based on global situation. If",
            "\t * the wb is over/under its share of dirty pages, we want to scale",
            "\t * pos_ratio further down/up. That is done by the following mechanism.",
            "\t */",
            "",
            "\t/*",
            "\t * wb setpoint",
            "\t *",
            "\t *        f(wb_dirty) := 1.0 + k * (wb_dirty - wb_setpoint)",
            "\t *",
            "\t *                        x_intercept - wb_dirty",
            "\t *                     := --------------------------",
            "\t *                        x_intercept - wb_setpoint",
            "\t *",
            "\t * The main wb control line is a linear function that subjects to",
            "\t *",
            "\t * (1) f(wb_setpoint) = 1.0",
            "\t * (2) k = - 1 / (8 * write_bw)  (in single wb case)",
            "\t *     or equally: x_intercept = wb_setpoint + 8 * write_bw",
            "\t *",
            "\t * For single wb case, the dirty pages are observed to fluctuate",
            "\t * regularly within range",
            "\t *        [wb_setpoint - write_bw/2, wb_setpoint + write_bw/2]",
            "\t * for various filesystems, where (2) can yield in a reasonable 12.5%",
            "\t * fluctuation range for pos_ratio.",
            "\t *",
            "\t * For JBOD case, wb_thresh (not wb_dirty!) could fluctuate up to its",
            "\t * own size, so move the slope over accordingly and choose a slope that",
            "\t * yields 100% pos_ratio fluctuation on suddenly doubled wb_thresh.",
            "\t */",
            "\tif (unlikely(wb_thresh > dtc->thresh))",
            "\t\twb_thresh = dtc->thresh;",
            "\t/*",
            "\t * It's very possible that wb_thresh is close to 0 not because the",
            "\t * device is slow, but that it has remained inactive for long time.",
            "\t * Honour such devices a reasonable good (hopefully IO efficient)",
            "\t * threshold, so that the occasional writes won't be blocked and active",
            "\t * writes can rampup the threshold quickly.",
            "\t */",
            "\twb_thresh = max(wb_thresh, (limit - dtc->dirty) / 8);",
            "\t/*",
            "\t * scale global setpoint to wb's:",
            "\t *\twb_setpoint = setpoint * wb_thresh / thresh",
            "\t */",
            "\tx = div_u64((u64)wb_thresh << 16, dtc->thresh | 1);",
            "\twb_setpoint = setpoint * (u64)x >> 16;",
            "\t/*",
            "\t * Use span=(8*write_bw) in single wb case as indicated by",
            "\t * (thresh - wb_thresh ~= 0) and transit to wb_thresh in JBOD case.",
            "\t *",
            "\t *        wb_thresh                    thresh - wb_thresh",
            "\t * span = --------- * (8 * write_bw) + ------------------ * wb_thresh",
            "\t *         thresh                           thresh",
            "\t */",
            "\tspan = (dtc->thresh - wb_thresh + 8 * write_bw) * (u64)x >> 16;",
            "\tx_intercept = wb_setpoint + span;",
            "",
            "\tif (dtc->wb_dirty < x_intercept - span / 4) {",
            "\t\tpos_ratio = div64_u64(pos_ratio * (x_intercept - dtc->wb_dirty),",
            "\t\t\t\t      (x_intercept - wb_setpoint) | 1);",
            "\t} else",
            "\t\tpos_ratio /= 4;",
            "",
            "\t/*",
            "\t * wb reserve area, safeguard against dirty pool underrun and disk idle",
            "\t * It may push the desired control point of global dirty pages higher",
            "\t * than setpoint.",
            "\t */",
            "\tx_intercept = wb_thresh / 2;",
            "\tif (dtc->wb_dirty < x_intercept) {",
            "\t\tif (dtc->wb_dirty > x_intercept / 8)",
            "\t\t\tpos_ratio = div_u64(pos_ratio * x_intercept,",
            "\t\t\t\t\t    dtc->wb_dirty);",
            "\t\telse",
            "\t\t\tpos_ratio *= 8;",
            "\t}",
            "",
            "\tdtc->pos_ratio = pos_ratio;",
            "}"
          ],
          "function_name": "__wb_calc_thresh, wb_calc_thresh, cgwb_calc_thresh, pos_ratio_polynom, wb_position_ratio",
          "description": "实现基于脏页阈值的动态调节算法，包含计算全局和内存控制组的脏页阈值、基于多项式的位置比值计算、根据写入带宽和当前脏页状态调整控制参数等功能，通过多级阈值和反馈机制平衡系统负载。",
          "similarity": 0.6161462664604187
        },
        {
          "chunk_id": 1,
          "file_path": "mm/page-writeback.c",
          "start_line": 164,
          "end_line": 268,
          "content": [
            "static bool mdtc_valid(struct dirty_throttle_control *dtc)",
            "{",
            "\treturn dtc->dom;",
            "}",
            "static void wb_min_max_ratio(struct bdi_writeback *wb,",
            "\t\t\t     unsigned long *minp, unsigned long *maxp)",
            "{",
            "\tunsigned long this_bw = READ_ONCE(wb->avg_write_bandwidth);",
            "\tunsigned long tot_bw = atomic_long_read(&wb->bdi->tot_write_bandwidth);",
            "\tunsigned long long min = wb->bdi->min_ratio;",
            "\tunsigned long long max = wb->bdi->max_ratio;",
            "",
            "\t/*",
            "\t * @wb may already be clean by the time control reaches here and",
            "\t * the total may not include its bw.",
            "\t */",
            "\tif (this_bw < tot_bw) {",
            "\t\tif (min) {",
            "\t\t\tmin *= this_bw;",
            "\t\t\tmin = div64_ul(min, tot_bw);",
            "\t\t}",
            "\t\tif (max < 100 * BDI_RATIO_SCALE) {",
            "\t\t\tmax *= this_bw;",
            "\t\t\tmax = div64_ul(max, tot_bw);",
            "\t\t}",
            "\t}",
            "",
            "\t*minp = min;",
            "\t*maxp = max;",
            "}",
            "static bool mdtc_valid(struct dirty_throttle_control *dtc)",
            "{",
            "\treturn false;",
            "}",
            "static void wb_min_max_ratio(struct bdi_writeback *wb,",
            "\t\t\t     unsigned long *minp, unsigned long *maxp)",
            "{",
            "\t*minp = wb->bdi->min_ratio;",
            "\t*maxp = wb->bdi->max_ratio;",
            "}",
            "static unsigned long node_dirtyable_memory(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long nr_pages = 0;",
            "\tint z;",
            "",
            "\tfor (z = 0; z < MAX_NR_ZONES; z++) {",
            "\t\tstruct zone *zone = pgdat->node_zones + z;",
            "",
            "\t\tif (!populated_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tnr_pages += zone_page_state(zone, NR_FREE_PAGES);",
            "\t}",
            "",
            "\t/*",
            "\t * Pages reserved for the kernel should not be considered",
            "\t * dirtyable, to prevent a situation where reclaim has to",
            "\t * clean pages in order to balance the zones.",
            "\t */",
            "\tnr_pages -= min(nr_pages, pgdat->totalreserve_pages);",
            "",
            "\tnr_pages += node_page_state(pgdat, NR_INACTIVE_FILE);",
            "\tnr_pages += node_page_state(pgdat, NR_ACTIVE_FILE);",
            "",
            "\treturn nr_pages;",
            "}",
            "static unsigned long highmem_dirtyable_memory(unsigned long total)",
            "{",
            "#ifdef CONFIG_HIGHMEM",
            "\tint node;",
            "\tunsigned long x = 0;",
            "\tint i;",
            "",
            "\tfor_each_node_state(node, N_HIGH_MEMORY) {",
            "\t\tfor (i = ZONE_NORMAL + 1; i < MAX_NR_ZONES; i++) {",
            "\t\t\tstruct zone *z;",
            "\t\t\tunsigned long nr_pages;",
            "",
            "\t\t\tif (!is_highmem_idx(i))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tz = &NODE_DATA(node)->node_zones[i];",
            "\t\t\tif (!populated_zone(z))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tnr_pages = zone_page_state(z, NR_FREE_PAGES);",
            "\t\t\t/* watch for underflows */",
            "\t\t\tnr_pages -= min(nr_pages, high_wmark_pages(z));",
            "\t\t\tnr_pages += zone_page_state(z, NR_ZONE_INACTIVE_FILE);",
            "\t\t\tnr_pages += zone_page_state(z, NR_ZONE_ACTIVE_FILE);",
            "\t\t\tx += nr_pages;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Make sure that the number of highmem pages is never larger",
            "\t * than the number of the total dirtyable memory. This can only",
            "\t * occur in very strange VM situations but we want to make sure",
            "\t * that this does not occur.",
            "\t */",
            "\treturn min(x, total);",
            "#else",
            "\treturn 0;",
            "#endif",
            "}"
          ],
          "function_name": "mdtc_valid, wb_min_max_ratio, mdtc_valid, wb_min_max_ratio, node_dirtyable_memory, highmem_dirtyable_memory",
          "description": "计算各节点可脏化内存大小，并基于当前写入带宽调整脏页限制的最小和最大比率，用于后续脏页阈值计算。",
          "similarity": 0.6113120317459106
        },
        {
          "chunk_id": 3,
          "file_path": "mm/page-writeback.c",
          "start_line": 495,
          "end_line": 600,
          "content": [
            "bool node_dirty_ok(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long limit = node_dirty_limit(pgdat);",
            "\tunsigned long nr_pages = 0;",
            "",
            "\tnr_pages += node_page_state(pgdat, NR_FILE_DIRTY);",
            "\tnr_pages += node_page_state(pgdat, NR_WRITEBACK);",
            "",
            "\treturn nr_pages <= limit;",
            "}",
            "static int dirty_background_ratio_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint ret;",
            "",
            "\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write)",
            "\t\tdirty_background_bytes = 0;",
            "\treturn ret;",
            "}",
            "static int dirty_background_bytes_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint ret;",
            "\tunsigned long old_bytes = dirty_background_bytes;",
            "",
            "\tret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write) {",
            "\t\tif (DIV_ROUND_UP(dirty_background_bytes, PAGE_SIZE) >",
            "\t\t\t\t\t\t\t\tUINT_MAX) {",
            "\t\t\tdirty_background_bytes = old_bytes;",
            "\t\t\treturn -ERANGE;",
            "\t\t}",
            "\t\tdirty_background_ratio = 0;",
            "\t}",
            "\treturn ret;",
            "}",
            "static int dirty_ratio_handler(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos)",
            "{",
            "\tint old_ratio = vm_dirty_ratio;",
            "\tint ret;",
            "",
            "\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write && vm_dirty_ratio != old_ratio) {",
            "\t\tvm_dirty_bytes = 0;",
            "\t\twriteback_set_ratelimit();",
            "\t}",
            "\treturn ret;",
            "}",
            "static int dirty_bytes_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tunsigned long old_bytes = vm_dirty_bytes;",
            "\tint ret;",
            "",
            "\tret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write && vm_dirty_bytes != old_bytes) {",
            "\t\tif (DIV_ROUND_UP(vm_dirty_bytes, PAGE_SIZE) > UINT_MAX) {",
            "\t\t\tvm_dirty_bytes = old_bytes;",
            "\t\t\treturn -ERANGE;",
            "\t\t}",
            "\t\twriteback_set_ratelimit();",
            "\t\tvm_dirty_ratio = 0;",
            "\t}",
            "\treturn ret;",
            "}",
            "static unsigned long wp_next_time(unsigned long cur_time)",
            "{",
            "\tcur_time += VM_COMPLETIONS_PERIOD_LEN;",
            "\t/* 0 has a special meaning... */",
            "\tif (!cur_time)",
            "\t\treturn 1;",
            "\treturn cur_time;",
            "}",
            "static void wb_domain_writeout_add(struct wb_domain *dom,",
            "\t\t\t\t   struct fprop_local_percpu *completions,",
            "\t\t\t\t   unsigned int max_prop_frac, long nr)",
            "{",
            "\t__fprop_add_percpu_max(&dom->completions, completions,",
            "\t\t\t       max_prop_frac, nr);",
            "\t/* First event after period switching was turned off? */",
            "\tif (unlikely(!dom->period_time)) {",
            "\t\t/*",
            "\t\t * We can race with other __bdi_writeout_inc calls here but",
            "\t\t * it does not cause any harm since the resulting time when",
            "\t\t * timer will fire and what is in writeout_period_time will be",
            "\t\t * roughly the same.",
            "\t\t */",
            "\t\tdom->period_time = wp_next_time(jiffies);",
            "\t\tmod_timer(&dom->period_timer, dom->period_time);",
            "\t}",
            "}",
            "static inline void __wb_writeout_add(struct bdi_writeback *wb, long nr)",
            "{",
            "\tstruct wb_domain *cgdom;",
            "",
            "\twb_stat_mod(wb, WB_WRITTEN, nr);",
            "\twb_domain_writeout_add(&global_wb_domain, &wb->completions,",
            "\t\t\t       wb->bdi->max_prop_frac, nr);",
            "",
            "\tcgdom = mem_cgroup_wb_domain(wb);",
            "\tif (cgdom)",
            "\t\twb_domain_writeout_add(cgdom, wb_memcg_completions(wb),",
            "\t\t\t\t       wb->bdi->max_prop_frac, nr);",
            "}"
          ],
          "function_name": "node_dirty_ok, dirty_background_ratio_handler, dirty_background_bytes_handler, dirty_ratio_handler, dirty_bytes_handler, wp_next_time, wb_domain_writeout_add, __wb_writeout_add",
          "description": "通过sysctl接口动态调整脏页写回参数，维护写回统计信息并周期性触发写回检查，确保系统内存使用符合预设策略。",
          "similarity": 0.6072782278060913
        }
      ]
    },
    {
      "source_file": "mm/page_reporting.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:05:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_reporting.c`\n\n---\n\n# page_reporting.c 技术文档\n\n## 1. 文件概述\n\n`page_reporting.c` 是 Linux 内核中实现**空闲页报告（Page Reporting）**机制的核心模块。该机制允许内核将大块连续的空闲物理内存信息异步上报给注册的设备驱动（如 virtio-balloon、内存热插拔管理器等），以便这些设备可以回收或迁移这些内存，从而提升系统整体内存利用效率。本文件负责协调从伙伴系统（buddy allocator）中提取符合要求的空闲页、构建散列表（scatterlist）、调用设备驱动的报告回调，并在报告完成后将页面安全地归还到伙伴系统。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `page_reporting_order`: 可通过内核启动参数或 sysfs 配置的全局变量，指定待报告空闲页块的最小阶数（order）。默认值为 `-1`（表示未设置），有效范围为 `[0, MAX_PAGE_ORDER]`。导出为 GPL 符号供其他驱动访问。\n- `pr_dev_info`: 指向当前注册的 `page_reporting_dev_info` 结构的 RCU 保护指针，代表提供页报告服务的设备。\n\n### 主要函数\n- `__page_reporting_notify(void)`: 通知已注册的页报告设备有新的空闲页可供报告。这是内核其他部分（如内存释放路径）触发报告流程的入口点。\n- `__page_reporting_request(struct page_reporting_dev_info *prdev)`: 内部函数，用于向指定设备请求启动页报告工作。包含状态机管理和延迟调度逻辑。\n- `page_reporting_drain(...)`: 在设备完成页报告（无论成功与否）后，将散列表中的页面重新放回伙伴系统的对应 free_area 和 migratetype 列表中，并根据报告结果设置 `PG_reported` 标志位。\n- `page_reporting_cycle(...)`: 核心处理循环，遍历指定 zone、order 和 migratetype 的 free_list，隔离未报告的页面到散列表中，达到容量后触发设备报告回调。\n- `page_reporting_process_zone(...)`: 处理单个内存区域（zone）的入口函数，负责水位线检查并按 order 从高到低调用 `page_reporting_cycle`。\n\n### 关键数据结构（外部定义）\n- `struct page_reporting_dev_info`: 由设备驱动提供，包含报告回调函数 `report()`、工作队列 `work` 和状态原子变量 `state` 等。\n\n## 3. 关键实现\n\n### 状态机与延迟调度\n- 使用原子变量 `prdev->state` 管理三种状态：`IDLE`（空闲）、`REQUESTED`（已请求）、`ACTIVE`（活跃中）。\n- 当收到报告请求时，若当前为空闲状态，则调度一个延迟为 `2 * HZ`（约 2 秒）的 `delayed_work`。此延迟旨在累积足够多的空闲页再进行批量报告，减少频繁调用设备驱动的开销。\n\n### 页面隔离与归还\n- **隔离**: 在持有 zone 自旋锁期间，使用 `__isolate_free_page()` 将符合条件的 Buddy 页面从 free_list 中移除。\n- **归还**: 报告完成后，在 `page_reporting_drain()` 中调用 `__putback_isolated_page()` 将页面放回原 free_area 和 migratetype 列表。\n- **报告标记**: 仅当页面在报告后仍保持 Buddy 状态且其阶数未变时，才设置 `PG_reported` 标志，避免对已合并的大页面重复报告。\n\n### 散列表（Scatterlist）管理\n- 使用固定大小（`PAGE_REPORTING_CAPACITY`）的散列表作为设备驱动和内核之间的传输缓冲区。\n- 采用“填满即报”的策略：当散列表填满或遍历完当前 free_list 后，立即调用设备驱动的 `report()` 回调。\n- 报告完成后重置散列表（`sg_init_table`）以供下次使用。\n\n### 遍历策略与预算控制\n- **遍历顺序**: 按内存区域（zone）、迁移类型（migratetype）、页面阶数（order，从高到低）进行嵌套遍历。\n- **预算限制**: 对每个 `(zone, order, mt)` 组合设置处理预算（`budget`），防止单次处理耗时过长影响系统响应。预算基于该 free_area 中空闲页数量动态计算。\n- **列表旋转**: 在中断遍历时，将下一个待处理页面旋转到 free_list 头部（`list_rotate_to_front`），确保下次从断点继续，避免饥饿。\n\n### 水位线保护\n- 在 `page_reporting_process_zone()` 中检查 zone 的空闲页是否高于 `low_wmark + (capacity << reporting_order)`，防止因报告操作导致内存水位过低而引发分配失败或 OOM。\n\n## 4. 依赖关系\n\n- **内部依赖**:\n  - `mm/internal.h`: 提供 `__putback_isolated_page()`、`__isolate_free_page()` 等伙伴系统内部操作函数。\n  - `page_reporting.h` (本地): 定义本地辅助函数和常量（如 `PAGE_REPORTING_CAPACITY`）。\n- **外部依赖**:\n  - `<linux/mm.h>`, `<linux/mmzone.h>`: 内存管理核心头文件，提供 `struct zone`、`free_area`、页面操作宏等。\n  - `<linux/page_reporting.h>`: 定义公共接口 `struct page_reporting_dev_info` 和注册/注销 API。\n  - `<linux/scatterlist.h>`: 提供散列表操作函数（`sg_set_page`, `sg_next` 等）。\n  - 设备驱动: 必须实现 `page_reporting_dev_info.report` 回调函数，并通过 `page_reporting_register()` 注册。\n\n## 5. 使用场景\n\n- **虚拟化环境**: Virtio-balloon 驱动利用此机制向宿主机报告客户机中大块空闲内存，宿主机可将其回收用于其他虚拟机，提高物理内存利用率。\n- **内存热插拔/卸载**: 在移除内存前，通过页报告机制确保目标内存区域尽可能空闲，减少迁移成本。\n- **透明大页（THP）优化**: 协助识别和释放可用于 THP 分配的大块连续空闲内存。\n- **通用内存回收**: 任何需要感知系统大块空闲内存布局的子系统（如 CMA、HMM）均可注册为页报告设备，实现定制化内存管理策略。",
      "similarity": 0.6168655157089233,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "mm/page_reporting.c",
          "start_line": 401,
          "end_line": 416,
          "content": [
            "void page_reporting_unregister(struct page_reporting_dev_info *prdev)",
            "{",
            "\tmutex_lock(&page_reporting_mutex);",
            "",
            "\tif (prdev == rcu_dereference_protected(pr_dev_info,",
            "\t\t\t\tlockdep_is_held(&page_reporting_mutex))) {",
            "\t\t/* Disable page reporting notification */",
            "\t\tRCU_INIT_POINTER(pr_dev_info, NULL);",
            "\t\tsynchronize_rcu();",
            "",
            "\t\t/* Flush any existing work, and lock it out */",
            "\t\tcancel_delayed_work_sync(&prdev->work);",
            "\t}",
            "",
            "\tmutex_unlock(&page_reporting_mutex);",
            "}"
          ],
          "function_name": "page_reporting_unregister",
          "description": "实现页面报告设备注销操作，包含RCU安全指针替换、延迟任务取消及互斥锁保护的资源清理逻辑。",
          "similarity": 0.5674667358398438
        },
        {
          "chunk_id": 1,
          "file_path": "mm/page_reporting.c",
          "start_line": 17,
          "end_line": 213,
          "content": [
            "static int page_order_update_notify(const char *val, const struct kernel_param *kp)",
            "{",
            "\t/*",
            "\t * If param is set beyond this limit, order is set to default",
            "\t * pageblock_order value",
            "\t */",
            "\treturn  param_set_uint_minmax(val, kp, 0, MAX_PAGE_ORDER);",
            "}",
            "static void",
            "__page_reporting_request(struct page_reporting_dev_info *prdev)",
            "{",
            "\tunsigned int state;",
            "",
            "\t/* Check to see if we are in desired state */",
            "\tstate = atomic_read(&prdev->state);",
            "\tif (state == PAGE_REPORTING_REQUESTED)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If reporting is already active there is nothing we need to do.",
            "\t * Test against 0 as that represents PAGE_REPORTING_IDLE.",
            "\t */",
            "\tstate = atomic_xchg(&prdev->state, PAGE_REPORTING_REQUESTED);",
            "\tif (state != PAGE_REPORTING_IDLE)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Delay the start of work to allow a sizable queue to build. For",
            "\t * now we are limiting this to running no more than once every",
            "\t * couple of seconds.",
            "\t */",
            "\tschedule_delayed_work(&prdev->work, PAGE_REPORTING_DELAY);",
            "}",
            "void __page_reporting_notify(void)",
            "{",
            "\tstruct page_reporting_dev_info *prdev;",
            "",
            "\t/*",
            "\t * We use RCU to protect the pr_dev_info pointer. In almost all",
            "\t * cases this should be present, however in the unlikely case of",
            "\t * a shutdown this will be NULL and we should exit.",
            "\t */",
            "\trcu_read_lock();",
            "\tprdev = rcu_dereference(pr_dev_info);",
            "\tif (likely(prdev))",
            "\t\t__page_reporting_request(prdev);",
            "",
            "\trcu_read_unlock();",
            "}",
            "static void",
            "page_reporting_drain(struct page_reporting_dev_info *prdev,",
            "\t\t     struct scatterlist *sgl, unsigned int nents, bool reported)",
            "{",
            "\tstruct scatterlist *sg = sgl;",
            "",
            "\t/*",
            "\t * Drain the now reported pages back into their respective",
            "\t * free lists/areas. We assume at least one page is populated.",
            "\t */",
            "\tdo {",
            "\t\tstruct page *page = sg_page(sg);",
            "\t\tint mt = get_pageblock_migratetype(page);",
            "\t\tunsigned int order = get_order(sg->length);",
            "",
            "\t\t__putback_isolated_page(page, order, mt);",
            "",
            "\t\t/* If the pages were not reported due to error skip flagging */",
            "\t\tif (!reported)",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * If page was not comingled with another page we can",
            "\t\t * consider the result to be \"reported\" since the page",
            "\t\t * hasn't been modified, otherwise we will need to",
            "\t\t * report on the new larger page when we make our way",
            "\t\t * up to that higher order.",
            "\t\t */",
            "\t\tif (PageBuddy(page) && buddy_order(page) == order)",
            "\t\t\t__SetPageReported(page);",
            "\t} while ((sg = sg_next(sg)));",
            "",
            "\t/* reinitialize scatterlist now that it is empty */",
            "\tsg_init_table(sgl, nents);",
            "}",
            "static int",
            "page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,",
            "\t\t     unsigned int order, unsigned int mt,",
            "\t\t     struct scatterlist *sgl, unsigned int *offset)",
            "{",
            "\tstruct free_area *area = &zone->free_area[order];",
            "\tstruct list_head *list = &area->free_list[mt];",
            "\tunsigned int page_len = PAGE_SIZE << order;",
            "\tstruct page *page, *next;",
            "\tlong budget;",
            "\tint err = 0;",
            "",
            "\t/*",
            "\t * Perform early check, if free area is empty there is",
            "\t * nothing to process so we can skip this free_list.",
            "\t */",
            "\tif (list_empty(list))",
            "\t\treturn err;",
            "",
            "\tspin_lock_irq(&zone->lock);",
            "",
            "\t/*",
            "\t * Limit how many calls we will be making to the page reporting",
            "\t * device for this list. By doing this we avoid processing any",
            "\t * given list for too long.",
            "\t *",
            "\t * The current value used allows us enough calls to process over a",
            "\t * sixteenth of the current list plus one additional call to handle",
            "\t * any pages that may have already been present from the previous",
            "\t * list processed. This should result in us reporting all pages on",
            "\t * an idle system in about 30 seconds.",
            "\t *",
            "\t * The division here should be cheap since PAGE_REPORTING_CAPACITY",
            "\t * should always be a power of 2.",
            "\t */",
            "\tbudget = DIV_ROUND_UP(area->nr_free, PAGE_REPORTING_CAPACITY * 16);",
            "",
            "\t/* loop through free list adding unreported pages to sg list */",
            "\tlist_for_each_entry_safe(page, next, list, lru) {",
            "\t\t/* We are going to skip over the reported pages. */",
            "\t\tif (PageReported(page))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * If we fully consumed our budget then update our",
            "\t\t * state to indicate that we are requesting additional",
            "\t\t * processing and exit this list.",
            "\t\t */",
            "\t\tif (budget < 0) {",
            "\t\t\tatomic_set(&prdev->state, PAGE_REPORTING_REQUESTED);",
            "\t\t\tnext = page;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t/* Attempt to pull page from list and place in scatterlist */",
            "\t\tif (*offset) {",
            "\t\t\tif (!__isolate_free_page(page, order)) {",
            "\t\t\t\tnext = page;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "",
            "\t\t\t/* Add page to scatter list */",
            "\t\t\t--(*offset);",
            "\t\t\tsg_set_page(&sgl[*offset], page, page_len, 0);",
            "",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Make the first non-reported page in the free list",
            "\t\t * the new head of the free list before we release the",
            "\t\t * zone lock.",
            "\t\t */",
            "\t\tif (!list_is_first(&page->lru, list))",
            "\t\t\tlist_rotate_to_front(&page->lru, list);",
            "",
            "\t\t/* release lock before waiting on report processing */",
            "\t\tspin_unlock_irq(&zone->lock);",
            "",
            "\t\t/* begin processing pages in local list */",
            "\t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);",
            "",
            "\t\t/* reset offset since the full list was reported */",
            "\t\t*offset = PAGE_REPORTING_CAPACITY;",
            "",
            "\t\t/* update budget to reflect call to report function */",
            "\t\tbudget--;",
            "",
            "\t\t/* reacquire zone lock and resume processing */",
            "\t\tspin_lock_irq(&zone->lock);",
            "",
            "\t\t/* flush reported pages from the sg list */",
            "\t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);",
            "",
            "\t\t/*",
            "\t\t * Reset next to first entry, the old next isn't valid",
            "\t\t * since we dropped the lock to report the pages",
            "\t\t */",
            "\t\tnext = list_first_entry(list, struct page, lru);",
            "",
            "\t\t/* exit on error */",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\t/* Rotate any leftover pages to the head of the freelist */",
            "\tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))",
            "\t\tlist_rotate_to_front(&next->lru, list);",
            "",
            "\tspin_unlock_irq(&zone->lock);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "page_order_update_notify, __page_reporting_request, __page_reporting_notify, page_reporting_drain, page_reporting_cycle",
          "description": "实现页面报告参数更新、请求处理、状态通知及周期性处理逻辑，包含延迟工作队列调度、内存区域遍历和页面报告流程控制。",
          "similarity": 0.5334898233413696
        },
        {
          "chunk_id": 2,
          "file_path": "mm/page_reporting.c",
          "start_line": 259,
          "end_line": 393,
          "content": [
            "static int",
            "page_reporting_process_zone(struct page_reporting_dev_info *prdev,",
            "\t\t\t    struct scatterlist *sgl, struct zone *zone)",
            "{",
            "\tunsigned int order, mt, leftover, offset = PAGE_REPORTING_CAPACITY;",
            "\tunsigned long watermark;",
            "\tint err = 0;",
            "",
            "\t/* Generate minimum watermark to be able to guarantee progress */",
            "\twatermark = low_wmark_pages(zone) +",
            "\t\t    (PAGE_REPORTING_CAPACITY << page_reporting_order);",
            "",
            "\t/*",
            "\t * Cancel request if insufficient free memory or if we failed",
            "\t * to allocate page reporting statistics for the zone.",
            "\t */",
            "\tif (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))",
            "\t\treturn err;",
            "",
            "\t/* Process each free list starting from lowest order/mt */",
            "\tfor (order = page_reporting_order; order < NR_PAGE_ORDERS; order++) {",
            "\t\tfor (mt = 0; mt < MIGRATE_TYPES; mt++) {",
            "\t\t\t/* We do not pull pages from the isolate free list */",
            "\t\t\tif (is_migrate_isolate(mt))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\terr = page_reporting_cycle(prdev, zone, order, mt,",
            "\t\t\t\t\t\t   sgl, &offset);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "\t}",
            "",
            "\t/* report the leftover pages before going idle */",
            "\tleftover = PAGE_REPORTING_CAPACITY - offset;",
            "\tif (leftover) {",
            "\t\tsgl = &sgl[offset];",
            "\t\terr = prdev->report(prdev, sgl, leftover);",
            "",
            "\t\t/* flush any remaining pages out from the last report */",
            "\t\tspin_lock_irq(&zone->lock);",
            "\t\tpage_reporting_drain(prdev, sgl, leftover, !err);",
            "\t\tspin_unlock_irq(&zone->lock);",
            "\t}",
            "",
            "\treturn err;",
            "}",
            "static void page_reporting_process(struct work_struct *work)",
            "{",
            "\tstruct delayed_work *d_work = to_delayed_work(work);",
            "\tstruct page_reporting_dev_info *prdev =",
            "\t\tcontainer_of(d_work, struct page_reporting_dev_info, work);",
            "\tint err = 0, state = PAGE_REPORTING_ACTIVE;",
            "\tstruct scatterlist *sgl;",
            "\tstruct zone *zone;",
            "",
            "\t/*",
            "\t * Change the state to \"Active\" so that we can track if there is",
            "\t * anyone requests page reporting after we complete our pass. If",
            "\t * the state is not altered by the end of the pass we will switch",
            "\t * to idle and quit scheduling reporting runs.",
            "\t */",
            "\tatomic_set(&prdev->state, state);",
            "",
            "\t/* allocate scatterlist to store pages being reported on */",
            "\tsgl = kmalloc_array(PAGE_REPORTING_CAPACITY, sizeof(*sgl), GFP_KERNEL);",
            "\tif (!sgl)",
            "\t\tgoto err_out;",
            "",
            "\tsg_init_table(sgl, PAGE_REPORTING_CAPACITY);",
            "",
            "\tfor_each_zone(zone) {",
            "\t\terr = page_reporting_process_zone(prdev, sgl, zone);",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tkfree(sgl);",
            "err_out:",
            "\t/*",
            "\t * If the state has reverted back to requested then there may be",
            "\t * additional pages to be processed. We will defer for 2s to allow",
            "\t * more pages to accumulate.",
            "\t */",
            "\tstate = atomic_cmpxchg(&prdev->state, state, PAGE_REPORTING_IDLE);",
            "\tif (state == PAGE_REPORTING_REQUESTED)",
            "\t\tschedule_delayed_work(&prdev->work, PAGE_REPORTING_DELAY);",
            "}",
            "int page_reporting_register(struct page_reporting_dev_info *prdev)",
            "{",
            "\tint err = 0;",
            "",
            "\tmutex_lock(&page_reporting_mutex);",
            "",
            "\t/* nothing to do if already in use */",
            "\tif (rcu_dereference_protected(pr_dev_info,",
            "\t\t\t\tlockdep_is_held(&page_reporting_mutex))) {",
            "\t\terr = -EBUSY;",
            "\t\tgoto err_out;",
            "\t}",
            "",
            "\t/*",
            "\t * If the page_reporting_order value is not set, we check if",
            "\t * an order is provided from the driver that is performing the",
            "\t * registration. If that is not provided either, we default to",
            "\t * pageblock_order.",
            "\t */",
            "",
            "\tif (page_reporting_order == -1) {",
            "\t\tif (prdev->order > 0 && prdev->order <= MAX_PAGE_ORDER)",
            "\t\t\tpage_reporting_order = prdev->order;",
            "\t\telse",
            "\t\t\tpage_reporting_order = pageblock_order;",
            "\t}",
            "",
            "\t/* initialize state and work structures */",
            "\tatomic_set(&prdev->state, PAGE_REPORTING_IDLE);",
            "\tINIT_DELAYED_WORK(&prdev->work, &page_reporting_process);",
            "",
            "\t/* Begin initial flush of zones */",
            "\t__page_reporting_request(prdev);",
            "",
            "\t/* Assign device to allow notifications */",
            "\trcu_assign_pointer(pr_dev_info, prdev);",
            "",
            "\t/* enable page reporting notification */",
            "\tif (!static_key_enabled(&page_reporting_enabled)) {",
            "\t\tstatic_branch_enable(&page_reporting_enabled);",
            "\t\tpr_info(\"Free page reporting enabled\\n\");",
            "\t}",
            "err_out:",
            "\tmutex_unlock(&page_reporting_mutex);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "page_reporting_process_zone, page_reporting_process, page_reporting_register",
          "description": "提供内存区级页面报告处理函数，包含主处理循环、工作线程执行路径及设备注册时的初始配置与状态设置。",
          "similarity": 0.5312143564224243
        },
        {
          "chunk_id": 0,
          "file_path": "mm/page_reporting.c",
          "start_line": 1,
          "end_line": 16,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/page_reporting.h>",
            "#include <linux/gfp.h>",
            "#include <linux/export.h>",
            "#include <linux/module.h>",
            "#include <linux/delay.h>",
            "#include <linux/scatterlist.h>",
            "",
            "#include \"page_reporting.h\"",
            "#include \"internal.h\"",
            "",
            "/* Initialize to an unsupported value */",
            "unsigned int page_reporting_order = -1;",
            ""
          ],
          "function_name": null,
          "description": "声明全局变量page_reporting_order用于存储页面报告的订单值，并包含相关内核模块头文件。",
          "similarity": 0.44832149147987366
        }
      ]
    },
    {
      "source_file": "mm/swap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:25:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `swap.c`\n\n---\n\n# swap.c 技术文档\n\n## 1. 文件概述\n\n`swap.c` 是 Linux 内核内存管理子系统（MM）中的核心文件之一，主要负责页面回收（page reclaim）、LRU（Least Recently Used）链表管理、页面释放以及与交换（swap）机制相关的底层支持逻辑。尽管文件名为 `swap.c`，但其功能不仅限于交换，而是涵盖了通用的页面生命周期管理、LRU 链表操作、页面引用计数释放、可回收性判断等关键内存管理任务。该文件为页面缓存（page cache）、匿名页（anonymous pages）和大页（huge pages）提供统一的释放与 LRU 管理接口。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `page_cluster`：控制一次 I/O 操作中尝试换入/换出的页面数量（以 2 的幂表示），默认值由系统配置决定。\n- `page_cluster_max`：`page_cluster` 的最大允许值（31，即最多 2^31 页，实际受架构限制）。\n\n### 主要数据结构\n- `struct lru_rotate`：每个 CPU 私有的结构，用于在中断禁用上下文中批量处理需移至 LRU 链表尾部的页面（如 `folio_rotate_reclaimable` 场景）。\n- `struct cpu_fbatches`：每个 CPU 私有的 folio 批处理结构，包含多个 folio_batch，用于高效地向 LRU 链表添加、停用或激活页面，避免频繁获取 LRU 锁。\n\n### 主要函数\n- `__folio_put()`：释放一个 folio 的核心函数，根据 folio 类型（设备内存、大页、普通页）调用相应的释放路径。\n- `put_pages_list()`：批量释放通过 `lru` 字段链接的页面列表，常用于网络子系统或 compound page 释放。\n- `lru_add_fn()`：将 folio 添加到对应 LRU 链表的回调函数，处理可回收性（evictable/unevictable）状态转换和统计计数。\n- `folio_batch_move_lru()`：批量执行 LRU 操作（如添加、移动），在持有 LRU 锁期间完成所有 folio 的处理。\n- `folio_rotate_reclaimable()`：在写回完成后，若页面仍可回收，则将其移至 inactive LRU 链表尾部，以延迟其被回收的时间。\n- `lru_note_cost()`：记录 LRU 扫描过程中的 I/O 和旋转（rotation）成本，用于后续调整 anon/file LRU 的扫描比例。\n\n## 3. 关键实现\n\n### LRU 批处理机制\n为减少 LRU 锁竞争，内核采用 per-CPU 批处理（`folio_batch`）方式暂存待处理的 folio。当批处理满或遇到大页（`folio_test_large`）时，才批量获取 LRU 锁并执行操作（如 `lru_add_fn`）。这显著提升了高并发场景下的性能。\n\n### 可回收性管理\n页面是否可回收由 `folio_evictable()` 判断，主要依据是否被 mlock 锁定。在添加到 LRU 时：\n- 若页面变为可回收（原为 unevictable），则增加 `UNEVICTABLE_PGRESCUED` 统计；\n- 若页面不可回收，则清除 active 标志，设置 unevictable 标志，并重置 `mlock_count`，同时增加 `UNEVICTABLE_PGCULLED` 统计。\n\n### 页面释放路径\n`__folio_put()` 是 folio 引用计数归零后的释放入口：\n1. 设备内存 folio 调用 `free_zone_device_folio()`\n2. 大页 folio 调用 `free_huge_folio()`\n3. 普通 folio 先从 LRU 移除（若在 LRU 上），然后解绑内存控制组（memcg），最后调用 `free_unref_page()` 释放到伙伴系统。\n\n### LRU 旋转优化\n`folio_rotate_reclaimable()` 在写回结束时，若页面干净且未锁定，则将其移至 inactive LRU 尾部。此操作通过 per-CPU 的 `lru_rotate` 批处理完成，仅在必要时获取 LRU 锁，避免影响写回关键路径性能。\n\n### 成本跟踪\n`lru_note_cost()` 通过累加 `nr_io * SWAP_CLUSTER_MAX + nr_rotated` 来量化扫描成本，用于动态调整匿名页与文件页 LRU 的扫描比例，优化内存回收效率。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/pagevec.h>`、`\"internal.h\"` 等，使用伙伴系统、LRU 框架、内存控制组（memcg）等基础组件。\n- **交换子系统**：虽不直接实现 swap read/write，但为 `vmscan.c` 中的页面回收提供 LRU 操作接口，是 swap 机制的支撑模块。\n- **大页支持**：通过 `hugetlb.h` 与大页子系统交互，特殊处理大页释放。\n- **设备内存**：通过 `memremap.h` 支持持久内存（pmem）等 zone device 页面的释放。\n- **跟踪与统计**：使用 tracepoint（`trace/events/pagemap.h`）和 VM 统计（`kernel_stat.h`）进行性能分析。\n- **SMP 支持**：大量使用 per-CPU 变量（`DEFINE_PER_CPU`）和本地锁（`local_lock_t`）优化多核性能。\n\n## 5. 使用场景\n\n- **页面回收（Reclaim）**：当内存压力触发 kswapd 或 direct reclaim 时，`vmscan.c` 调用本文件的 LRU 操作函数来隔离、释放页面。\n- **页面缓存释放**：文件系统或网络子系统在释放 page cache 页面时，通过 `__folio_put()` 或 `put_pages_list()` 触发 LRU 移除和内存释放。\n- **写回完成处理**：块设备或文件系统在完成脏页写回后，调用 `folio_rotate_reclaimable()` 更新页面在 LRU 中的位置。\n- **内存控制组（cgroup）**：memcg 回收内存时，复用本文件的 LRU 批处理和 folio 释放逻辑。\n- **大页与设备内存管理**：透明大页（THP）或持久内存应用释放页面时，通过统一的 `__folio_put()` 接口分发到专用释放函数。\n- **系统调优**：管理员通过 `/proc/sys/vm/page-cluster` 调整 `page_cluster` 值，影响 swap 和 page cache 的 I/O 批量大小。",
      "similarity": 0.6109069585800171,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/swap.c",
          "start_line": 77,
          "end_line": 190,
          "content": [
            "static void __page_cache_release(struct folio *folio, struct lruvec **lruvecp,",
            "\t\tunsigned long *flagsp)",
            "{",
            "\tif (folio_test_lru(folio)) {",
            "\t\tfolio_lruvec_relock_irqsave(folio, lruvecp, flagsp);",
            "\t\tlruvec_del_folio(*lruvecp, folio);",
            "\t\t__folio_clear_lru_flags(folio);",
            "\t}",
            "}",
            "static void page_cache_release(struct folio *folio)",
            "{",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags;",
            "",
            "\t__page_cache_release(folio, &lruvec, &flags);",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "}",
            "void __folio_put(struct folio *folio)",
            "{",
            "\tif (unlikely(folio_is_zone_device(folio))) {",
            "\t\tfree_zone_device_folio(folio);",
            "\t\treturn;",
            "\t} else if (folio_test_hugetlb(folio)) {",
            "\t\tfree_huge_folio(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tpage_cache_release(folio);",
            "\tfolio_unqueue_deferred_split(folio);",
            "\tmem_cgroup_uncharge(folio);",
            "\tfree_unref_page(&folio->page, folio_order(folio));",
            "}",
            "void put_pages_list(struct list_head *pages)",
            "{",
            "\tstruct folio_batch fbatch;",
            "\tstruct folio *folio, *next;",
            "",
            "\tfolio_batch_init(&fbatch);",
            "\tlist_for_each_entry_safe(folio, next, pages, lru) {",
            "\t\tif (!folio_put_testzero(folio))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_hugetlb(folio)) {",
            "\t\t\tfree_huge_folio(folio);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\t/* LRU flag must be clear because it's passed using the lru */",
            "\t\tif (folio_batch_add(&fbatch, folio) > 0)",
            "\t\t\tcontinue;",
            "\t\tfree_unref_folios(&fbatch);",
            "\t}",
            "",
            "\tif (fbatch.nr)",
            "\t\tfree_unref_folios(&fbatch);",
            "\tINIT_LIST_HEAD(pages);",
            "}",
            "static void lru_add_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tint was_unevictable = folio_test_clear_unevictable(folio);",
            "\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\t/*",
            "\t * Is an smp_mb__after_atomic() still required here, before",
            "\t * folio_evictable() tests the mlocked flag, to rule out the possibility",
            "\t * of stranding an evictable folio on an unevictable LRU?  I think",
            "\t * not, because __munlock_folio() only clears the mlocked flag",
            "\t * while the LRU lock is held.",
            "\t *",
            "\t * (That is not true of __page_cache_release(), and not necessarily",
            "\t * true of folios_put(): but those only clear the mlocked flag after",
            "\t * folio_put_testzero() has excluded any other users of the folio.)",
            "\t */",
            "\tif (folio_evictable(folio)) {",
            "\t\tif (was_unevictable)",
            "\t\t\t__count_vm_events(UNEVICTABLE_PGRESCUED, nr_pages);",
            "\t} else {",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_set_unevictable(folio);",
            "\t\t/*",
            "\t\t * folio->mlock_count = !!folio_test_mlocked(folio)?",
            "\t\t * But that leaves __mlock_folio() in doubt whether another",
            "\t\t * actor has already counted the mlock or not.  Err on the",
            "\t\t * safe side, underestimate, let page reclaim fix it, rather",
            "\t\t * than leaving a page on the unevictable LRU indefinitely.",
            "\t\t */",
            "\t\tfolio->mlock_count = 0;",
            "\t\tif (!was_unevictable)",
            "\t\t\t__count_vm_events(UNEVICTABLE_PGCULLED, nr_pages);",
            "\t}",
            "",
            "\tlruvec_add_folio(lruvec, folio);",
            "\ttrace_mm_lru_insertion(folio);",
            "}",
            "static void folio_batch_move_lru(struct folio_batch *fbatch, move_fn_t move_fn)",
            "{",
            "\tint i;",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags = 0;",
            "",
            "\tfor (i = 0; i < folio_batch_count(fbatch); i++) {",
            "\t\tstruct folio *folio = fbatch->folios[i];",
            "",
            "\t\tfolio_lruvec_relock_irqsave(folio, &lruvec, &flags);",
            "\t\tmove_fn(lruvec, folio);",
            "",
            "\t\tfolio_set_lru(folio);",
            "\t}",
            "",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\tfolios_put(fbatch);",
            "}"
          ],
          "function_name": "__page_cache_release, page_cache_release, __folio_put, put_pages_list, lru_add_fn, folio_batch_move_lru",
          "description": "实现了页面缓存释放和LRU列表维护逻辑，包含__page_cache_release用于从LRU列表移除页面，page_cache_release处理普通页面释放流程，__folio_put负责释放非设备映射和大页，put_pages_list批量处理页面释放，lru_add_fn将页面添加到LRU列表并根据是否可交换设置相应标志。",
          "similarity": 0.5906191468238831
        },
        {
          "chunk_id": 7,
          "file_path": "mm/swap.c",
          "start_line": 912,
          "end_line": 1023,
          "content": [
            "void lru_add_drain_all(void)",
            "{",
            "\t__lru_add_drain_all(false);",
            "}",
            "void lru_add_drain_all(void)",
            "{",
            "\tlru_add_drain();",
            "}",
            "void lru_cache_disable(void)",
            "{",
            "\tatomic_inc(&lru_disable_count);",
            "\t/*",
            "\t * Readers of lru_disable_count are protected by either disabling",
            "\t * preemption or rcu_read_lock:",
            "\t *",
            "\t * preempt_disable, local_irq_disable  [bh_lru_lock()]",
            "\t * rcu_read_lock\t\t       [rt_spin_lock CONFIG_PREEMPT_RT]",
            "\t * preempt_disable\t\t       [local_lock !CONFIG_PREEMPT_RT]",
            "\t *",
            "\t * Since v5.1 kernel, synchronize_rcu() is guaranteed to wait on",
            "\t * preempt_disable() regions of code. So any CPU which sees",
            "\t * lru_disable_count = 0 will have exited the critical",
            "\t * section when synchronize_rcu() returns.",
            "\t */",
            "\tsynchronize_rcu_expedited();",
            "#ifdef CONFIG_SMP",
            "\t__lru_add_drain_all(true);",
            "#else",
            "\tlru_add_and_bh_lrus_drain();",
            "#endif",
            "}",
            "void folios_put_refs(struct folio_batch *folios, unsigned int *refs)",
            "{",
            "\tint i, j;",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags = 0;",
            "",
            "\tfor (i = 0, j = 0; i < folios->nr; i++) {",
            "\t\tstruct folio *folio = folios->folios[i];",
            "\t\tunsigned int nr_refs = refs ? refs[i] : 1;",
            "",
            "\t\tif (is_huge_zero_folio(folio))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (folio_is_zone_device(folio)) {",
            "\t\t\tif (lruvec) {",
            "\t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\t\t\t\tlruvec = NULL;",
            "\t\t\t}",
            "\t\t\tif (put_devmap_managed_folio_refs(folio, nr_refs))",
            "\t\t\t\tcontinue;",
            "\t\t\tif (folio_ref_sub_and_test(folio, nr_refs))",
            "\t\t\t\tfree_zone_device_folio(folio);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tif (!folio_ref_sub_and_test(folio, nr_refs))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* hugetlb has its own memcg */",
            "\t\tif (folio_test_hugetlb(folio)) {",
            "\t\t\tif (lruvec) {",
            "\t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\t\t\t\tlruvec = NULL;",
            "\t\t\t}",
            "\t\t\tfree_huge_folio(folio);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfolio_unqueue_deferred_split(folio);",
            "\t\t__page_cache_release(folio, &lruvec, &flags);",
            "",
            "\t\tif (j != i)",
            "\t\t\tfolios->folios[j] = folio;",
            "\t\tj++;",
            "\t}",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\tif (!j) {",
            "\t\tfolio_batch_reinit(folios);",
            "\t\treturn;",
            "\t}",
            "",
            "\tfolios->nr = j;",
            "\tmem_cgroup_uncharge_folios(folios);",
            "\tfree_unref_folios(folios);",
            "}",
            "void release_pages(release_pages_arg arg, int nr)",
            "{",
            "\tstruct folio_batch fbatch;",
            "\tint refs[PAGEVEC_SIZE];",
            "\tstruct encoded_page **encoded = arg.encoded_pages;",
            "\tint i;",
            "",
            "\tfolio_batch_init(&fbatch);",
            "\tfor (i = 0; i < nr; i++) {",
            "\t\t/* Turn any of the argument types into a folio */",
            "\t\tstruct folio *folio = page_folio(encoded_page_ptr(encoded[i]));",
            "",
            "\t\t/* Is our next entry actually \"nr_pages\" -> \"nr_refs\" ? */",
            "\t\trefs[fbatch.nr] = 1;",
            "\t\tif (unlikely(encoded_page_flags(encoded[i]) &",
            "\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\trefs[fbatch.nr] = encoded_nr_pages(encoded[++i]);",
            "",
            "\t\tif (folio_batch_add(&fbatch, folio) > 0)",
            "\t\t\tcontinue;",
            "\t\tfolios_put_refs(&fbatch, refs);",
            "\t}",
            "",
            "\tif (fbatch.nr)",
            "\t\tfolios_put_refs(&fbatch, refs);",
            "}"
          ],
          "function_name": "lru_add_drain_all, lru_add_drain_all, lru_cache_disable, folios_put_refs, release_pages",
          "description": "lru_add_drain_all 和 lru_cache_disable 控制全局LRU缓存状态切换，通过synchronize_rcu_expedited强制同步后，根据配置选择全CPU或单CPU模式执行页面释放；release_pages 批量释放页面引用并处理内存组计费。",
          "similarity": 0.5570687055587769
        },
        {
          "chunk_id": 6,
          "file_path": "mm/swap.c",
          "start_line": 781,
          "end_line": 896,
          "content": [
            "void lru_add_drain_cpu_zone(struct zone *zone)",
            "{",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tlru_add_drain_cpu(smp_processor_id());",
            "\tdrain_local_pages(zone);",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "\tmlock_drain_local();",
            "}",
            "static void lru_add_drain_per_cpu(struct work_struct *dummy)",
            "{",
            "\tlru_add_and_bh_lrus_drain();",
            "}",
            "static bool cpu_needs_drain(unsigned int cpu)",
            "{",
            "\tstruct cpu_fbatches *fbatches = &per_cpu(cpu_fbatches, cpu);",
            "",
            "\t/* Check these in order of likelihood that they're not zero */",
            "\treturn folio_batch_count(&fbatches->lru_add) ||",
            "\t\tdata_race(folio_batch_count(&per_cpu(lru_rotate.fbatch, cpu))) ||",
            "\t\tfolio_batch_count(&fbatches->lru_deactivate_file) ||",
            "\t\tfolio_batch_count(&fbatches->lru_deactivate) ||",
            "\t\tfolio_batch_count(&fbatches->lru_lazyfree) ||",
            "\t\tfolio_batch_count(&fbatches->activate) ||",
            "\t\tneed_mlock_drain(cpu) ||",
            "\t\thas_bh_in_lru(cpu, NULL);",
            "}",
            "static inline void __lru_add_drain_all(bool force_all_cpus)",
            "{",
            "\t/*",
            "\t * lru_drain_gen - Global pages generation number",
            "\t *",
            "\t * (A) Definition: global lru_drain_gen = x implies that all generations",
            "\t *     0 < n <= x are already *scheduled* for draining.",
            "\t *",
            "\t * This is an optimization for the highly-contended use case where a",
            "\t * user space workload keeps constantly generating a flow of pages for",
            "\t * each CPU.",
            "\t */",
            "\tstatic unsigned int lru_drain_gen;",
            "\tstatic struct cpumask has_work;",
            "\tstatic DEFINE_MUTEX(lock);",
            "\tunsigned cpu, this_gen;",
            "",
            "\t/*",
            "\t * Make sure nobody triggers this path before mm_percpu_wq is fully",
            "\t * initialized.",
            "\t */",
            "\tif (WARN_ON(!mm_percpu_wq))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Guarantee folio_batch counter stores visible by this CPU",
            "\t * are visible to other CPUs before loading the current drain",
            "\t * generation.",
            "\t */",
            "\tsmp_mb();",
            "",
            "\t/*",
            "\t * (B) Locally cache global LRU draining generation number",
            "\t *",
            "\t * The read barrier ensures that the counter is loaded before the mutex",
            "\t * is taken. It pairs with smp_mb() inside the mutex critical section",
            "\t * at (D).",
            "\t */",
            "\tthis_gen = smp_load_acquire(&lru_drain_gen);",
            "",
            "\tmutex_lock(&lock);",
            "",
            "\t/*",
            "\t * (C) Exit the draining operation if a newer generation, from another",
            "\t * lru_add_drain_all(), was already scheduled for draining. Check (A).",
            "\t */",
            "\tif (unlikely(this_gen != lru_drain_gen && !force_all_cpus))",
            "\t\tgoto done;",
            "",
            "\t/*",
            "\t * (D) Increment global generation number",
            "\t *",
            "\t * Pairs with smp_load_acquire() at (B), outside of the critical",
            "\t * section. Use a full memory barrier to guarantee that the",
            "\t * new global drain generation number is stored before loading",
            "\t * folio_batch counters.",
            "\t *",
            "\t * This pairing must be done here, before the for_each_online_cpu loop",
            "\t * below which drains the page vectors.",
            "\t *",
            "\t * Let x, y, and z represent some system CPU numbers, where x < y < z.",
            "\t * Assume CPU #z is in the middle of the for_each_online_cpu loop",
            "\t * below and has already reached CPU #y's per-cpu data. CPU #x comes",
            "\t * along, adds some pages to its per-cpu vectors, then calls",
            "\t * lru_add_drain_all().",
            "\t *",
            "\t * If the paired barrier is done at any later step, e.g. after the",
            "\t * loop, CPU #x will just exit at (C) and miss flushing out all of its",
            "\t * added pages.",
            "\t */",
            "\tWRITE_ONCE(lru_drain_gen, lru_drain_gen + 1);",
            "\tsmp_mb();",
            "",
            "\tcpumask_clear(&has_work);",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct work_struct *work = &per_cpu(lru_add_drain_work, cpu);",
            "",
            "\t\tif (cpu_needs_drain(cpu)) {",
            "\t\t\tINIT_WORK(work, lru_add_drain_per_cpu);",
            "\t\t\tqueue_work_on(cpu, mm_percpu_wq, work);",
            "\t\t\t__cpumask_set_cpu(cpu, &has_work);",
            "\t\t}",
            "\t}",
            "",
            "\tfor_each_cpu(cpu, &has_work)",
            "\t\tflush_work(&per_cpu(lru_add_drain_work, cpu));",
            "",
            "done:",
            "\tmutex_unlock(&lock);",
            "}"
          ],
          "function_name": "lru_add_drain_cpu_zone, lru_add_drain_per_cpu, cpu_needs_drain, __lru_add_drain_all",
          "description": "__lru_add_drain_all 管理全局LRU排水过程，通过generation-based机制避免重复处理，利用互斥锁和smp_mb屏障确保跨CPU数据可见性，并调度工作队列处理各CPU的待处理页面批次。",
          "similarity": 0.5525270700454712
        },
        {
          "chunk_id": 5,
          "file_path": "mm/swap.c",
          "start_line": 641,
          "end_line": 743,
          "content": [
            "void lru_add_drain_cpu(int cpu)",
            "{",
            "\tstruct cpu_fbatches *fbatches = &per_cpu(cpu_fbatches, cpu);",
            "\tstruct folio_batch *fbatch = &fbatches->lru_add;",
            "",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, lru_add_fn);",
            "",
            "\tfbatch = &per_cpu(lru_rotate.fbatch, cpu);",
            "\t/* Disabling interrupts below acts as a compiler barrier. */",
            "\tif (data_race(folio_batch_count(fbatch))) {",
            "\t\tunsigned long flags;",
            "",
            "\t\t/* No harm done if a racing interrupt already did this */",
            "\t\tlocal_lock_irqsave(&lru_rotate.lock, flags);",
            "\t\tfolio_batch_move_lru(fbatch, lru_move_tail_fn);",
            "\t\tlocal_unlock_irqrestore(&lru_rotate.lock, flags);",
            "\t}",
            "",
            "\tfbatch = &fbatches->lru_deactivate_file;",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, lru_deactivate_file_fn);",
            "",
            "\tfbatch = &fbatches->lru_deactivate;",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, lru_deactivate_fn);",
            "",
            "\tfbatch = &fbatches->lru_lazyfree;",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, lru_lazyfree_fn);",
            "",
            "\tfolio_activate_drain(cpu);",
            "}",
            "void deactivate_file_folio(struct folio *folio)",
            "{",
            "\tstruct folio_batch *fbatch;",
            "",
            "\t/* Deactivating an unevictable folio will not accelerate reclaim */",
            "\tif (folio_test_unevictable(folio))",
            "\t\treturn;",
            "",
            "\tfolio_get(folio);",
            "\tif (!folio_test_clear_lru(folio)) {",
            "\t\tfolio_put(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_deactivate_file);",
            "\tfolio_batch_add_and_move(fbatch, folio, lru_deactivate_file_fn);",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "}",
            "void folio_deactivate(struct folio *folio)",
            "{",
            "\tif (!folio_test_unevictable(folio) && (folio_test_active(folio) ||",
            "\t    lru_gen_enabled())) {",
            "\t\tstruct folio_batch *fbatch;",
            "",
            "\t\tfolio_get(folio);",
            "\t\tif (!folio_test_clear_lru(folio)) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tlocal_lock(&cpu_fbatches.lock);",
            "\t\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_deactivate);",
            "\t\tfolio_batch_add_and_move(fbatch, folio, lru_deactivate_fn);",
            "\t\tlocal_unlock(&cpu_fbatches.lock);",
            "\t}",
            "}",
            "void folio_mark_lazyfree(struct folio *folio)",
            "{",
            "\tif (folio_test_anon(folio) && folio_test_swapbacked(folio) &&",
            "\t    !folio_test_swapcache(folio) && !folio_test_unevictable(folio)) {",
            "\t\tstruct folio_batch *fbatch;",
            "",
            "\t\tfolio_get(folio);",
            "\t\tif (!folio_test_clear_lru(folio)) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tlocal_lock(&cpu_fbatches.lock);",
            "\t\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_lazyfree);",
            "\t\tfolio_batch_add_and_move(fbatch, folio, lru_lazyfree_fn);",
            "\t\tlocal_unlock(&cpu_fbatches.lock);",
            "\t}",
            "}",
            "void lru_add_drain(void)",
            "{",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tlru_add_drain_cpu(smp_processor_id());",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "\tmlock_drain_local();",
            "}",
            "static void lru_add_and_bh_lrus_drain(void)",
            "{",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tlru_add_drain_cpu(smp_processor_id());",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "\tinvalidate_bh_lrus_cpu();",
            "\tmlock_drain_local();",
            "}"
          ],
          "function_name": "lru_add_drain_cpu, deactivate_file_folio, folio_deactivate, folio_mark_lazyfree, lru_add_drain, lru_add_and_bh_lrus_drain",
          "description": "lru_add_drain_cpu 处理CPU本地LRU批次的移动，将不同类型的页面（如lru_add、deactivate、lazyfree等）通过folio_batch_move_lru分发至相应LRU队列，同时协调中断禁用和锁保护以确保原子性。",
          "similarity": 0.5435304045677185
        },
        {
          "chunk_id": 4,
          "file_path": "mm/swap.c",
          "start_line": 496,
          "end_line": 600,
          "content": [
            "void folio_add_lru(struct folio *folio)",
            "{",
            "\tstruct folio_batch *fbatch;",
            "",
            "\tVM_BUG_ON_FOLIO(folio_test_active(folio) &&",
            "\t\t\tfolio_test_unevictable(folio), folio);",
            "\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\t/* see the comment in lru_gen_add_folio() */",
            "\tif (lru_gen_enabled() && !folio_test_unevictable(folio) &&",
            "\t    lru_gen_in_fault() && !(current->flags & PF_MEMALLOC))",
            "\t\tfolio_set_active(folio);",
            "",
            "\tfolio_get(folio);",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_add);",
            "\tfolio_batch_add_and_move(fbatch, folio, lru_add_fn);",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "}",
            "void folio_add_lru_vma(struct folio *folio, struct vm_area_struct *vma)",
            "{",
            "\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\tif (unlikely((vma->vm_flags & (VM_LOCKED | VM_SPECIAL)) == VM_LOCKED))",
            "\t\tmlock_new_folio(folio);",
            "\telse",
            "\t\tfolio_add_lru(folio);",
            "}",
            "static void lru_deactivate_file_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tbool active = folio_test_active(folio);",
            "\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\tif (folio_test_unevictable(folio))",
            "\t\treturn;",
            "",
            "\t/* Some processes are using the folio */",
            "\tif (folio_mapped(folio))",
            "\t\treturn;",
            "",
            "\tlruvec_del_folio(lruvec, folio);",
            "\tfolio_clear_active(folio);",
            "\tfolio_clear_referenced(folio);",
            "",
            "\tif (folio_test_writeback(folio) || folio_test_dirty(folio)) {",
            "\t\t/*",
            "\t\t * Setting the reclaim flag could race with",
            "\t\t * folio_end_writeback() and confuse readahead.  But the",
            "\t\t * race window is _really_ small and  it's not a critical",
            "\t\t * problem.",
            "\t\t */",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "\t\tfolio_set_reclaim(folio);",
            "\t} else {",
            "\t\t/*",
            "\t\t * The folio's writeback ended while it was in the batch.",
            "\t\t * We move that folio to the tail of the inactive list.",
            "\t\t */",
            "\t\tlruvec_add_folio_tail(lruvec, folio);",
            "\t\t__count_vm_events(PGROTATED, nr_pages);",
            "\t}",
            "",
            "\tif (active) {",
            "\t\t__count_vm_events(PGDEACTIVATE, nr_pages);",
            "\t\t__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE,",
            "\t\t\t\t     nr_pages);",
            "\t}",
            "}",
            "static void lru_deactivate_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tif (!folio_test_unevictable(folio) && (folio_test_active(folio) || lru_gen_enabled())) {",
            "\t\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_clear_referenced(folio);",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "",
            "\t\t__count_vm_events(PGDEACTIVATE, nr_pages);",
            "\t\t__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE,",
            "\t\t\t\t     nr_pages);",
            "\t}",
            "}",
            "static void lru_lazyfree_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tif (folio_test_anon(folio) && folio_test_swapbacked(folio) &&",
            "\t    !folio_test_swapcache(folio) && !folio_test_unevictable(folio)) {",
            "\t\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_clear_referenced(folio);",
            "\t\t/*",
            "\t\t * Lazyfree folios are clean anonymous folios.  They have",
            "\t\t * the swapbacked flag cleared, to distinguish them from normal",
            "\t\t * anonymous folios",
            "\t\t */",
            "\t\tfolio_clear_swapbacked(folio);",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "",
            "\t\t__count_vm_events(PGLAZYFREE, nr_pages);",
            "\t\t__count_memcg_events(lruvec_memcg(lruvec), PGLAZYFREE,",
            "\t\t\t\t     nr_pages);",
            "\t}",
            "}"
          ],
          "function_name": "folio_add_lru, folio_add_lru_vma, lru_deactivate_file_fn, lru_deactivate_fn, lru_lazyfree_fn",
          "description": "包含页面LRU列表插入和状态转换逻辑，folio_add_lru将页面加入LRU列表，folio_add_lru_vma处理VMA特定的页面添加，lru_deactivate_file_fn和lru_deactivate_fn处理文件页去激活操作，lru_lazyfree_fn处理延迟释放的匿名页面，均通过统一接口修改页面状态并触发统计事件。",
          "similarity": 0.5258580446243286
        }
      ]
    }
  ]
}