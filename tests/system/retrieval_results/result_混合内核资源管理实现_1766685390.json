{
  "query": "混合内核资源管理实现",
  "timestamp": "2025-12-26 01:56:30",
  "retrieved_files": [
    {
      "source_file": "mm/list_lru.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:35:23\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `list_lru.c`\n\n---\n\n# list_lru.c 技术文档\n\n## 1. 文件概述\n\n`list_lru.c` 实现了 Linux 内核中通用的 **List-based LRU（Least Recently Used）基础设施**，用于管理可回收对象的双向链表。该机制支持按 NUMA 节点（node）和内存控制组（memcg）进行细粒度组织，便于内存压力下的高效回收。主要服务于 slab 分配器等子系统，作为 shrinker 框架的一部分，在内存紧张时协助释放非活跃对象。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct list_lru`：顶层 LRU 管理结构，包含 per-node 的 `list_lru_node`\n- `struct list_lru_node`：每个 NUMA 节点对应的 LRU 节点，含自旋锁和总项数\n- `struct list_lru_one`：实际存储对象链表和计数的单元（per-memcg per-node）\n- `struct list_lru_memcg`：当启用 `CONFIG_MEMCG` 时，为每个 memcg 存储 per-node 的 `list_lru_one`\n\n### 主要导出函数\n- `list_lru_add()` / `list_lru_add_obj()`：向 LRU 添加对象\n- `list_lru_del()` / `list_lru_del_obj()`：从 LRU 删除对象\n- `list_lru_isolate()` / `list_lru_isolate_move()`：在回收过程中隔离对象\n- `list_lru_count_one()` / `list_lru_count_node()`：查询 LRU 中对象数量\n- `list_lru_walk_one()` / `list_lru_walk_node()`：遍历并处理 LRU 中的对象（用于 shrinker 回调）\n\n### 内部辅助函数\n- `list_lru_from_memcg_idx()`：根据 memcg ID 获取对应的 `list_lru_one`\n- `__list_lru_walk_one()`：带锁的 LRU 遍历核心逻辑\n- `list_lru_register()` / `list_lru_unregister()`：注册/注销 memcg-aware 的 LRU（用于全局追踪）\n\n## 3. 关键实现\n\n### 内存控制组（memcg）支持\n- 通过 `CONFIG_MEMCG` 条件编译控制 memcg 相关逻辑\n- 使用 XArray (`lru->xa`) 动态存储每个 memcg 对应的 `list_lru_memcg` 结构\n- 每个 memcg 在每个 NUMA 节点上拥有独立的 `list_lru_one`，实现资源隔离\n- 全局 `memcg_list_lrus` 链表和 `list_lrus_mutex` 用于跟踪所有 memcg-aware 的 LRU 实例\n\n### 并发控制\n- 每个 NUMA 节点 (`list_lru_node`) 拥有独立的自旋锁 (`nlru->lock`)\n- 所有对 LRU 链表的操作（增、删、遍历）均在对应节点锁保护下进行\n- 提供 `_irq` 版本的遍历函数（`list_lru_walk_one_irq`）用于中断上下文\n\n### 回收遍历机制\n- `list_lru_walk_*` 函数接受回调函数 `isolate`，由调用者定义回收策略\n- 回调返回值控制遍历行为：\n  - `LRU_REMOVED`：成功移除\n  - `LRU_REMOVED_RETRY`：移除后需重新开始遍历（锁曾被释放）\n  - `LRU_RETRY`：未移除但需重新开始遍历\n  - `LRU_ROTATE`：将对象移到链表尾部（标记为最近使用）\n  - `LRU_SKIP`：跳过当前对象\n  - `LRU_STOP`：立即停止遍历\n- 通过 `nr_to_walk` 限制单次遍历的最大对象数，防止长时间持锁\n\n### Shrinker 集成\n- 当向空的 `list_lru_one` 添加首个对象时，调用 `set_shrinker_bit()` 标记该 memcg/node 需要被 shrinker 处理\n- `lru_shrinker_id()` 返回关联的 shrinker ID，用于通知内存回收子系统\n\n### 对象归属识别\n- `list_lru_add_obj()` / `list_lru_del_obj()` 通过 `mem_cgroup_from_slab_obj()` 自动获取对象所属的 memcg\n- 使用 `page_to_nid(virt_to_page(item))` 确定对象所在的 NUMA 节点\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/list_lru.h>`：定义核心数据结构和 API\n  - `<linux/memcontrol.h>`：memcg 相关接口（如 `memcg_kmem_id`）\n  - `\"slab.h\"` 和 `\"internal.h\"`：slab 分配器内部接口（如 `mem_cgroup_from_slab_obj`）\n- **配置依赖**：\n  - `CONFIG_MEMCG`：决定是否编译 memcg 相关代码\n  - `CONFIG_NUMA`：影响 per-node 数据结构的大小（通过 `nr_node_ids`）\n- **子系统依赖**：\n  - Slab 分配器：作为主要使用者，管理可回收 slab 对象\n  - Memory Control Group (memcg)：提供内存隔离和记账\n  - Shrinker 框架：通过 shrinker 回调触发 LRU 遍历回收\n\n## 5. 使用场景\n\n- **Slab 对象回收**：当系统内存压力大时，shrinker 通过 `list_lru_walk_*` 遍历 inactive slab 对象链表，释放可回收对象\n- **Per-memcg 内存限制**：在 cgroup 内存超限时，仅遍历该 memcg 对应的 LRU 部分，实现精确回收\n- **NUMA 感知管理**：按 NUMA 节点分离 LRU 链表，减少远程内存访问，提升性能\n- **通用 LRU 容器**：任何需要按 LRU 策略管理可回收对象的内核子系统均可使用此基础设施（如 dentry、inode 缓存等）",
      "similarity": 0.5834872722625732,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/list_lru.c",
          "start_line": 22,
          "end_line": 129,
          "content": [
            "static inline bool list_lru_memcg_aware(struct list_lru *lru)",
            "{",
            "\treturn lru->memcg_aware;",
            "}",
            "static void list_lru_register(struct list_lru *lru)",
            "{",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_add(&lru->list, &memcg_list_lrus);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static void list_lru_unregister(struct list_lru *lru)",
            "{",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_del(&lru->list);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static int lru_shrinker_id(struct list_lru *lru)",
            "{",
            "\treturn lru->shrinker_id;",
            "}",
            "static void list_lru_register(struct list_lru *lru)",
            "{",
            "}",
            "static void list_lru_unregister(struct list_lru *lru)",
            "{",
            "}",
            "static int lru_shrinker_id(struct list_lru *lru)",
            "{",
            "\treturn -1;",
            "}",
            "static inline bool list_lru_memcg_aware(struct list_lru *lru)",
            "{",
            "\treturn false;",
            "}",
            "bool list_lru_add(struct list_lru *lru, struct list_head *item, int nid,",
            "\t\t    struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tif (list_empty(item)) {",
            "\t\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\t\tlist_add_tail(item, &l->list);",
            "\t\t/* Set shrinker bit if the first element was added */",
            "\t\tif (!l->nr_items++)",
            "\t\t\tset_shrinker_bit(memcg, nid, lru_shrinker_id(lru));",
            "\t\tnlru->nr_items++;",
            "\t\tspin_unlock(&nlru->lock);",
            "\t\treturn true;",
            "\t}",
            "\tspin_unlock(&nlru->lock);",
            "\treturn false;",
            "}",
            "bool list_lru_add_obj(struct list_lru *lru, struct list_head *item)",
            "{",
            "\tbool ret;",
            "\tint nid = page_to_nid(virt_to_page(item));",
            "",
            "\tif (list_lru_memcg_aware(lru)) {",
            "\t\trcu_read_lock();",
            "\t\tret = list_lru_add(lru, item, nid, mem_cgroup_from_slab_obj(item));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tret = list_lru_add(lru, item, nid, NULL);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "bool list_lru_del(struct list_lru *lru, struct list_head *item, int nid,",
            "\t\t    struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tif (!list_empty(item)) {",
            "\t\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\t\tlist_del_init(item);",
            "\t\tl->nr_items--;",
            "\t\tnlru->nr_items--;",
            "\t\tspin_unlock(&nlru->lock);",
            "\t\treturn true;",
            "\t}",
            "\tspin_unlock(&nlru->lock);",
            "\treturn false;",
            "}",
            "bool list_lru_del_obj(struct list_lru *lru, struct list_head *item)",
            "{",
            "\tbool ret;",
            "\tint nid = page_to_nid(virt_to_page(item));",
            "",
            "\tif (list_lru_memcg_aware(lru)) {",
            "\t\trcu_read_lock();",
            "\t\tret = list_lru_del(lru, item, nid, mem_cgroup_from_slab_obj(item));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tret = list_lru_del(lru, item, nid, NULL);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "list_lru_memcg_aware, list_lru_register, list_lru_unregister, lru_shrinker_id, list_lru_register, list_lru_unregister, lru_shrinker_id, list_lru_memcg_aware, list_lru_add, list_lru_add_obj, list_lru_del, list_lru_del_obj",
          "description": "实现了LRU列表的添加/删除操作，支持MemCG感知的节点和内存组粒度管理，包含处理多核、内存组切换及RCU安全访问的逻辑。",
          "similarity": 0.577884316444397
        },
        {
          "chunk_id": 4,
          "file_path": "mm/list_lru.c",
          "start_line": 425,
          "end_line": 551,
          "content": [
            "static void memcg_reparent_list_lru(struct list_lru *lru,",
            "\t\t\t\t    int src_idx, struct mem_cgroup *dst_memcg)",
            "{",
            "\tint i;",
            "",
            "\tfor_each_node(i)",
            "\t\tmemcg_reparent_list_lru_node(lru, i, src_idx, dst_memcg);",
            "",
            "\tmemcg_list_lru_free(lru, src_idx);",
            "}",
            "void memcg_reparent_list_lrus(struct mem_cgroup *memcg, struct mem_cgroup *parent)",
            "{",
            "\tstruct cgroup_subsys_state *css;",
            "\tstruct list_lru *lru;",
            "\tint src_idx = memcg->kmemcg_id;",
            "",
            "\t/*",
            "\t * Change kmemcg_id of this cgroup and all its descendants to the",
            "\t * parent's id, and then move all entries from this cgroup's list_lrus",
            "\t * to ones of the parent.",
            "\t *",
            "\t * After we have finished, all list_lrus corresponding to this cgroup",
            "\t * are guaranteed to remain empty. So we can safely free this cgroup's",
            "\t * list lrus in memcg_list_lru_free().",
            "\t *",
            "\t * Changing ->kmemcg_id to the parent can prevent memcg_list_lru_alloc()",
            "\t * from allocating list lrus for this cgroup after memcg_list_lru_free()",
            "\t * call.",
            "\t */",
            "\trcu_read_lock();",
            "\tcss_for_each_descendant_pre(css, &memcg->css) {",
            "\t\tstruct mem_cgroup *child;",
            "",
            "\t\tchild = mem_cgroup_from_css(css);",
            "\t\tWRITE_ONCE(child->kmemcg_id, parent->kmemcg_id);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_for_each_entry(lru, &memcg_list_lrus, list)",
            "\t\tmemcg_reparent_list_lru(lru, src_idx, parent);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static inline bool memcg_list_lru_allocated(struct mem_cgroup *memcg,",
            "\t\t\t\t\t    struct list_lru *lru)",
            "{",
            "\tint idx = memcg->kmemcg_id;",
            "",
            "\treturn idx < 0 || xa_load(&lru->xa, idx);",
            "}",
            "int memcg_list_lru_alloc(struct mem_cgroup *memcg, struct list_lru *lru,",
            "\t\t\t gfp_t gfp)",
            "{",
            "\tint i;",
            "\tunsigned long flags;",
            "\tstruct list_lru_memcg_table {",
            "\t\tstruct list_lru_memcg *mlru;",
            "\t\tstruct mem_cgroup *memcg;",
            "\t} *table;",
            "\tXA_STATE(xas, &lru->xa, 0);",
            "",
            "\tif (!list_lru_memcg_aware(lru) || memcg_list_lru_allocated(memcg, lru))",
            "\t\treturn 0;",
            "",
            "\tgfp &= GFP_RECLAIM_MASK;",
            "\ttable = kmalloc_array(memcg->css.cgroup->level, sizeof(*table), gfp);",
            "\tif (!table)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * Because the list_lru can be reparented to the parent cgroup's",
            "\t * list_lru, we should make sure that this cgroup and all its",
            "\t * ancestors have allocated list_lru_memcg.",
            "\t */",
            "\tfor (i = 0; memcg; memcg = parent_mem_cgroup(memcg), i++) {",
            "\t\tif (memcg_list_lru_allocated(memcg, lru))",
            "\t\t\tbreak;",
            "",
            "\t\ttable[i].memcg = memcg;",
            "\t\ttable[i].mlru = memcg_init_list_lru_one(gfp);",
            "\t\tif (!table[i].mlru) {",
            "\t\t\twhile (i--)",
            "\t\t\t\tkfree(table[i].mlru);",
            "\t\t\tkfree(table);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "\t}",
            "",
            "\txas_lock_irqsave(&xas, flags);",
            "\twhile (i--) {",
            "\t\tint index = READ_ONCE(table[i].memcg->kmemcg_id);",
            "\t\tstruct list_lru_memcg *mlru = table[i].mlru;",
            "",
            "\t\txas_set(&xas, index);",
            "retry:",
            "\t\tif (unlikely(index < 0 || xas_error(&xas) || xas_load(&xas))) {",
            "\t\t\tkfree(mlru);",
            "\t\t} else {",
            "\t\t\txas_store(&xas, mlru);",
            "\t\t\tif (xas_error(&xas) == -ENOMEM) {",
            "\t\t\t\txas_unlock_irqrestore(&xas, flags);",
            "\t\t\t\tif (xas_nomem(&xas, gfp))",
            "\t\t\t\t\txas_set_err(&xas, 0);",
            "\t\t\t\txas_lock_irqsave(&xas, flags);",
            "\t\t\t\t/*",
            "\t\t\t\t * The xas lock has been released, this memcg",
            "\t\t\t\t * can be reparented before us. So reload",
            "\t\t\t\t * memcg id. More details see the comments",
            "\t\t\t\t * in memcg_reparent_list_lrus().",
            "\t\t\t\t */",
            "\t\t\t\tindex = READ_ONCE(table[i].memcg->kmemcg_id);",
            "\t\t\t\tif (index < 0)",
            "\t\t\t\t\txas_set_err(&xas, 0);",
            "\t\t\t\telse if (!xas_error(&xas) && index != xas.xa_index)",
            "\t\t\t\t\txas_set(&xas, index);",
            "\t\t\t\tgoto retry;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\t/* xas_nomem() is used to free memory instead of memory allocation. */",
            "\tif (xas.xa_alloc)",
            "\t\txas_nomem(&xas, gfp);",
            "\txas_unlock_irqrestore(&xas, flags);",
            "\tkfree(table);",
            "",
            "\treturn xas_error(&xas);",
            "}"
          ],
          "function_name": "memcg_reparent_list_lru, memcg_reparent_list_lrus, memcg_list_lru_allocated, memcg_list_lru_alloc",
          "description": "实现内存组层级间的LRU列表迁移与分配机制，包含递归子组处理、动态分配/释放LRU结构体及冲突解决逻辑。",
          "similarity": 0.5771186947822571
        },
        {
          "chunk_id": 5,
          "file_path": "mm/list_lru.c",
          "start_line": 556,
          "end_line": 605,
          "content": [
            "static inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)",
            "{",
            "}",
            "static void memcg_destroy_list_lru(struct list_lru *lru)",
            "{",
            "}",
            "int __list_lru_init(struct list_lru *lru, bool memcg_aware,",
            "\t\t    struct lock_class_key *key, struct shrinker *shrinker)",
            "{",
            "\tint i;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tif (shrinker)",
            "\t\tlru->shrinker_id = shrinker->id;",
            "\telse",
            "\t\tlru->shrinker_id = -1;",
            "#endif",
            "",
            "\tlru->node = kcalloc(nr_node_ids, sizeof(*lru->node), GFP_KERNEL);",
            "\tif (!lru->node)",
            "\t\treturn -ENOMEM;",
            "",
            "\tfor_each_node(i) {",
            "\t\tspin_lock_init(&lru->node[i].lock);",
            "\t\tif (key)",
            "\t\t\tlockdep_set_class(&lru->node[i].lock, key);",
            "\t\tinit_one_lru(&lru->node[i].lru);",
            "\t}",
            "",
            "\tmemcg_init_list_lru(lru, memcg_aware);",
            "\tlist_lru_register(lru);",
            "",
            "\treturn 0;",
            "}",
            "void list_lru_destroy(struct list_lru *lru)",
            "{",
            "\t/* Already destroyed or not yet initialized? */",
            "\tif (!lru->node)",
            "\t\treturn;",
            "",
            "\tlist_lru_unregister(lru);",
            "",
            "\tmemcg_destroy_list_lru(lru);",
            "\tkfree(lru->node);",
            "\tlru->node = NULL;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tlru->shrinker_id = -1;",
            "#endif",
            "}"
          ],
          "function_name": "memcg_init_list_lru, memcg_destroy_list_lru, __list_lru_init, list_lru_destroy",
          "description": "该代码段实现了基于内存控制组（MEMCG）的LRU列表管理功能。  \n`__list_lru_init` 初始化 `list_lru` 结构体并注册到系统，其中包含 MEMCG 相关的 shrinker ID 设置及节点锁初始化；`list_lru_destroy` 反向清理资源，但 `memcg_init_list_lru` 和 `memcg_destroy_list_lru` 的具体实现缺失，上下文不完整。",
          "similarity": 0.5752817392349243
        },
        {
          "chunk_id": 3,
          "file_path": "mm/list_lru.c",
          "start_line": 289,
          "end_line": 400,
          "content": [
            "unsigned long",
            "list_lru_walk_one_irq(struct list_lru *lru, int nid, struct mem_cgroup *memcg,",
            "\t\t      list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t      unsigned long *nr_to_walk)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tunsigned long ret;",
            "",
            "\tspin_lock_irq(&nlru->lock);",
            "\tret = __list_lru_walk_one(lru, nid, memcg_kmem_id(memcg), isolate,",
            "\t\t\t\t  cb_arg, nr_to_walk);",
            "\tspin_unlock_irq(&nlru->lock);",
            "\treturn ret;",
            "}",
            "unsigned long list_lru_walk_node(struct list_lru *lru, int nid,",
            "\t\t\t\t list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t\t\t unsigned long *nr_to_walk)",
            "{",
            "\tlong isolated = 0;",
            "",
            "\tisolated += list_lru_walk_one(lru, nid, NULL, isolate, cb_arg,",
            "\t\t\t\t      nr_to_walk);",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tif (*nr_to_walk > 0 && list_lru_memcg_aware(lru)) {",
            "\t\tstruct list_lru_memcg *mlru;",
            "\t\tunsigned long index;",
            "",
            "\t\txa_for_each(&lru->xa, index, mlru) {",
            "\t\t\tstruct list_lru_node *nlru = &lru->node[nid];",
            "",
            "\t\t\tspin_lock(&nlru->lock);",
            "\t\t\tisolated += __list_lru_walk_one(lru, nid, index,",
            "\t\t\t\t\t\t\tisolate, cb_arg,",
            "\t\t\t\t\t\t\tnr_to_walk);",
            "\t\t\tspin_unlock(&nlru->lock);",
            "",
            "\t\t\tif (*nr_to_walk <= 0)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "#endif",
            "",
            "\treturn isolated;",
            "}",
            "static void init_one_lru(struct list_lru_one *l)",
            "{",
            "\tINIT_LIST_HEAD(&l->list);",
            "\tl->nr_items = 0;",
            "}",
            "static void memcg_list_lru_free(struct list_lru *lru, int src_idx)",
            "{",
            "\tstruct list_lru_memcg *mlru = xa_erase_irq(&lru->xa, src_idx);",
            "",
            "\t/*",
            "\t * The __list_lru_walk_one() can walk the list of this node.",
            "\t * We need kvfree_rcu() here. And the walking of the list",
            "\t * is under lru->node[nid]->lock, which can serve as a RCU",
            "\t * read-side critical section.",
            "\t */",
            "\tif (mlru)",
            "\t\tkvfree_rcu(mlru, rcu);",
            "}",
            "static inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)",
            "{",
            "\tif (memcg_aware)",
            "\t\txa_init_flags(&lru->xa, XA_FLAGS_LOCK_IRQ);",
            "\tlru->memcg_aware = memcg_aware;",
            "}",
            "static void memcg_destroy_list_lru(struct list_lru *lru)",
            "{",
            "\tXA_STATE(xas, &lru->xa, 0);",
            "\tstruct list_lru_memcg *mlru;",
            "",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\txas_lock_irq(&xas);",
            "\txas_for_each(&xas, mlru, ULONG_MAX) {",
            "\t\tkfree(mlru);",
            "\t\txas_store(&xas, NULL);",
            "\t}",
            "\txas_unlock_irq(&xas);",
            "}",
            "static void memcg_reparent_list_lru_node(struct list_lru *lru, int nid,",
            "\t\t\t\t\t int src_idx, struct mem_cgroup *dst_memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tint dst_idx = dst_memcg->kmemcg_id;",
            "\tstruct list_lru_one *src, *dst;",
            "",
            "\t/*",
            "\t * Since list_lru_{add,del} may be called under an IRQ-safe lock,",
            "\t * we have to use IRQ-safe primitives here to avoid deadlock.",
            "\t */",
            "\tspin_lock_irq(&nlru->lock);",
            "",
            "\tsrc = list_lru_from_memcg_idx(lru, nid, src_idx);",
            "\tif (!src)",
            "\t\tgoto out;",
            "\tdst = list_lru_from_memcg_idx(lru, nid, dst_idx);",
            "",
            "\tlist_splice_init(&src->list, &dst->list);",
            "",
            "\tif (src->nr_items) {",
            "\t\tdst->nr_items += src->nr_items;",
            "\t\tset_shrinker_bit(dst_memcg, nid, lru_shrinker_id(lru));",
            "\t\tsrc->nr_items = 0;",
            "\t}",
            "out:",
            "\tspin_unlock_irq(&nlru->lock);",
            "}"
          ],
          "function_name": "list_lru_walk_one_irq, list_lru_walk_node, init_one_lru, memcg_list_lru_free, memcg_init_list_lru, memcg_destroy_list_lru, memcg_reparent_list_lru_node",
          "description": "包含LRU节点初始化、内存组间列表迁移、资源释放等高级操作，涉及XA表管理、中断安全锁操作及内存组重新归属处理。",
          "similarity": 0.5543912649154663
        },
        {
          "chunk_id": 0,
          "file_path": "mm/list_lru.c",
          "start_line": 1,
          "end_line": 21,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2013 Red Hat, Inc. and Parallels Inc. All rights reserved.",
            " * Authors: David Chinner and Glauber Costa",
            " *",
            " * Generic LRU infrastructure",
            " */",
            "#include <linux/kernel.h>",
            "#include <linux/module.h>",
            "#include <linux/mm.h>",
            "#include <linux/list_lru.h>",
            "#include <linux/slab.h>",
            "#include <linux/mutex.h>",
            "#include <linux/memcontrol.h>",
            "#include \"slab.h\"",
            "#include \"internal.h\"",
            "",
            "#ifdef CONFIG_MEMCG",
            "static LIST_HEAD(memcg_list_lrus);",
            "static DEFINE_MUTEX(list_lrus_mutex);",
            ""
          ],
          "function_name": null,
          "description": "定义了支持内存控制组（MemCG）的LRU基础设施，声明了全局链表头memcg_list_lrus和互斥锁list_lrus_mutex，用于管理MemCG环境下的LRU列表注册与注销操作。",
          "similarity": 0.5348333120346069
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5792161226272583,
      "chunks": [
        {
          "chunk_id": 14,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2523,
          "end_line": 2628,
          "content": [
            "static void rcu_cpu_kthread_park(unsigned int cpu)",
            "{",
            "\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;",
            "}",
            "static int rcu_cpu_kthread_should_run(unsigned int cpu)",
            "{",
            "\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);",
            "}",
            "static void rcu_cpu_kthread(unsigned int cpu)",
            "{",
            "\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);",
            "\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);",
            "\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);",
            "\tint spincnt;",
            "",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));",
            "\tfor (spincnt = 0; spincnt < 10; spincnt++) {",
            "\t\tWRITE_ONCE(*j, jiffies);",
            "\t\tlocal_bh_disable();",
            "\t\t*statusp = RCU_KTHREAD_RUNNING;",
            "\t\tlocal_irq_disable();",
            "\t\twork = *workp;",
            "\t\tWRITE_ONCE(*workp, 0);",
            "\t\tlocal_irq_enable();",
            "\t\tif (work)",
            "\t\t\trcu_core();",
            "\t\tlocal_bh_enable();",
            "\t\tif (!READ_ONCE(*workp)) {",
            "\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));",
            "\t\t\t*statusp = RCU_KTHREAD_WAITING;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "\t*statusp = RCU_KTHREAD_YIELDING;",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));",
            "\tschedule_timeout_idle(2);",
            "\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));",
            "\t*statusp = RCU_KTHREAD_WAITING;",
            "\tWRITE_ONCE(*j, jiffies);",
            "}",
            "static int __init rcu_spawn_core_kthreads(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;",
            "\tif (use_softirq)",
            "\t\treturn 0;",
            "\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),",
            "\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);",
            "\treturn 0;",
            "}",
            "static void rcutree_enqueue(struct rcu_data *rdp, struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\trcu_segcblist_enqueue(&rdp->cblist, head);",
            "\tif (__is_kvfree_rcu_offset((unsigned long)func))",
            "\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,",
            "\t\t\t\t\t (unsigned long)func,",
            "\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));",
            "\telse",
            "\t\ttrace_rcu_callback(rcu_state.name, head,",
            "\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));",
            "}",
            "static void call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,",
            "\t\t\t  rcu_callback_t func, unsigned long flags)",
            "{",
            "\trcutree_enqueue(rdp, head, func);",
            "\t/*",
            "\t * If called from an extended quiescent state, invoke the RCU",
            "\t * core in order to force a re-evaluation of RCU's idleness.",
            "\t */",
            "\tif (!rcu_is_watching())",
            "\t\tinvoke_rcu_core();",
            "",
            "\t/* If interrupts were disabled or CPU offline, don't invoke RCU core. */",
            "\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Force the grace period if too many callbacks or too long waiting.",
            "\t * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()",
            "\t * if some other CPU has recently done so.  Also, don't bother",
            "\t * invoking rcu_force_quiescent_state() if the newly enqueued callback",
            "\t * is the only one waiting for a grace period to complete.",
            "\t */",
            "\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >",
            "\t\t     rdp->qlen_last_fqs_check + qhimark)) {",
            "",
            "\t\t/* Are we ignoring a completed grace period? */",
            "\t\tnote_gp_changes(rdp);",
            "",
            "\t\t/* Start a new grace period if one not already started. */",
            "\t\tif (!rcu_gp_in_progress()) {",
            "\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);",
            "\t\t} else {",
            "\t\t\t/* Give the grace period a kick. */",
            "\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;",
            "\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&",
            "\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)",
            "\t\t\t\trcu_force_quiescent_state();",
            "\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "rcu_cpu_kthread_park, rcu_cpu_kthread_should_run, rcu_cpu_kthread, rcu_spawn_core_kthreads, rcutree_enqueue, call_rcu_core",
          "description": "实现RCU k线程管理与回调分发基础设施，包含线程启动、回调入队及触发条件判断逻辑，提供跨CPU的异步处理能力",
          "similarity": 0.585922122001648
        },
        {
          "chunk_id": 20,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3589,
          "end_line": 3697,
          "content": [
            "static unsigned long",
            "kfree_rcu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)",
            "{",
            "\tint cpu;",
            "\tunsigned long count = 0;",
            "",
            "\t/* Snapshot count of all CPUs */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tcount += krc_count(krcp);",
            "\t\tcount += READ_ONCE(krcp->nr_bkv_objs);",
            "\t\tatomic_set(&krcp->backoff_page_cache_fill, 1);",
            "\t}",
            "",
            "\treturn count == 0 ? SHRINK_EMPTY : count;",
            "}",
            "static unsigned long",
            "kfree_rcu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)",
            "{",
            "\tint cpu, freed = 0;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tint count;",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tcount = krc_count(krcp);",
            "\t\tcount += drain_page_cache(krcp);",
            "\t\tkfree_rcu_monitor(&krcp->monitor_work.work);",
            "",
            "\t\tsc->nr_to_scan -= count;",
            "\t\tfreed += count;",
            "",
            "\t\tif (sc->nr_to_scan <= 0)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn freed == 0 ? SHRINK_STOP : freed;",
            "}",
            "void __init kfree_rcu_scheduler_running(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tif (need_offload_krc(krcp))",
            "\t\t\tschedule_delayed_monitor_work(krcp);",
            "\t}",
            "}",
            "static int rcu_blocking_is_gp(void)",
            "{",
            "\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE) {",
            "\t\tmight_sleep();",
            "\t\treturn false;",
            "\t}",
            "\treturn true;",
            "}",
            "void synchronize_rcu(void)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_sched_lock_map),",
            "\t\t\t \"Illegal synchronize_rcu() in RCU read-side critical section\");",
            "\tif (!rcu_blocking_is_gp()) {",
            "\t\tif (rcu_gp_is_expedited())",
            "\t\t\tsynchronize_rcu_expedited();",
            "\t\telse",
            "\t\t\twait_rcu_gp(call_rcu_hurry);",
            "\t\treturn;",
            "\t}",
            "",
            "\t// Context allows vacuous grace periods.",
            "\t// Note well that this code runs with !PREEMPT && !SMP.",
            "\t// In addition, all code that advances grace periods runs at",
            "\t// process level.  Therefore, this normal GP overlaps with other",
            "\t// normal GPs only by being fully nested within them, which allows",
            "\t// reuse of ->gp_seq_polled_snap.",
            "\trcu_poll_gp_seq_start_unlocked(&rcu_state.gp_seq_polled_snap);",
            "\trcu_poll_gp_seq_end_unlocked(&rcu_state.gp_seq_polled_snap);",
            "",
            "\t// Update the normal grace-period counters to record",
            "\t// this grace period, but only those used by the boot CPU.",
            "\t// The rcu_scheduler_starting() will take care of the rest of",
            "\t// these counters.",
            "\tlocal_irq_save(flags);",
            "\tWARN_ON_ONCE(num_online_cpus() > 1);",
            "\trcu_state.gp_seq += (1 << RCU_SEQ_CTR_SHIFT);",
            "\tfor (rnp = this_cpu_ptr(&rcu_data)->mynode; rnp; rnp = rnp->parent)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq = rcu_state.gp_seq;",
            "\tlocal_irq_restore(flags);",
            "}",
            "void get_completed_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\trgosp->rgos_norm = RCU_GET_STATE_COMPLETED;",
            "\trgosp->rgos_exp = RCU_GET_STATE_COMPLETED;",
            "}",
            "unsigned long get_state_synchronize_rcu(void)",
            "{",
            "\t/*",
            "\t * Any prior manipulation of RCU-protected data must happen",
            "\t * before the load from ->gp_seq.",
            "\t */",
            "\tsmp_mb();  /* ^^^ */",
            "\treturn rcu_seq_snap(&rcu_state.gp_seq_polled);",
            "}"
          ],
          "function_name": "kfree_rcu_shrink_count, kfree_rcu_shrink_scan, kfree_rcu_scheduler_running, rcu_blocking_is_gp, synchronize_rcu, get_completed_synchronize_rcu_full, get_state_synchronize_rcu",
          "description": "实现RCU内存回收的shrinker接口，统计并扫描等待回收的RCU对象，调度监控工作，处理同步屏障逻辑，通过锁竞争检测确保安全访问",
          "similarity": 0.5583767890930176
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 438,
          "end_line": 544,
          "content": [
            "static int param_set_first_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : j);",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "static int param_set_next_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : (j ?: 1));",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "unsigned long rcu_get_gp_seq(void)",
            "{",
            "\treturn READ_ONCE(rcu_state.gp_seq);",
            "}",
            "unsigned long rcu_exp_batches_completed(void)",
            "{",
            "\treturn rcu_state.expedited_sequence;",
            "}",
            "void rcutorture_get_gp_data(enum rcutorture_type test_type, int *flags,",
            "\t\t\t    unsigned long *gp_seq)",
            "{",
            "\tswitch (test_type) {",
            "\tcase RCU_FLAVOR:",
            "\t\t*flags = READ_ONCE(rcu_state.gp_flags);",
            "\t\t*gp_seq = rcu_seq_current(&rcu_state.gp_seq);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "}",
            "static void late_wakeup_func(struct irq_work *work)",
            "{",
            "}",
            "noinstr void rcu_irq_work_resched(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tif (IS_ENABLED(CONFIG_GENERIC_ENTRY) && !(current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tif (IS_ENABLED(CONFIG_KVM_XFER_TO_GUEST_WORK) && (current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tinstrumentation_begin();",
            "\tif (do_nocb_deferred_wakeup(rdp) && need_resched()) {",
            "\t\tirq_work_queue(this_cpu_ptr(&late_wakeup_work));",
            "\t}",
            "\tinstrumentation_end();",
            "}",
            "void rcu_irq_exit_check_preempt(void)",
            "{",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nesting() <= 0,",
            "\t\t\t \"RCU dynticks_nesting counter underflow/zero!\");",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nmi_nesting() !=",
            "\t\t\t DYNTICK_IRQ_NONIDLE,",
            "\t\t\t \"Bad RCU  dynticks_nmi_nesting counter\\n\");",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"RCU in extended quiescent state!\");",
            "}",
            "void __rcu_irq_enter_check_tick(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\t// If we're here from NMI there's nothing to do.",
            "\tif (in_nmi())",
            "\t\treturn;",
            "",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"Illegal rcu_irq_enter_check_tick() from extended quiescent state\");",
            "",
            "\tif (!tick_nohz_full_cpu(rdp->cpu) ||",
            "\t    !READ_ONCE(rdp->rcu_urgent_qs) ||",
            "\t    READ_ONCE(rdp->rcu_forced_tick)) {",
            "\t\t// RCU doesn't need nohz_full help from this CPU, or it is",
            "\t\t// already getting that help.",
            "\t\treturn;",
            "\t}",
            "",
            "\t// We get here only when not in an extended quiescent state and",
            "\t// from interrupts (as opposed to NMIs).  Therefore, (1) RCU is",
            "\t// already watching and (2) The fact that we are in an interrupt",
            "\t// handler and that the rcu_node lock is an irq-disabled lock",
            "\t// prevents self-deadlock.  So we can safely recheck under the lock.",
            "\t// Note that the nohz_full state currently cannot change.",
            "\traw_spin_lock_rcu_node(rdp->mynode);",
            "\tif (READ_ONCE(rdp->rcu_urgent_qs) && !rdp->rcu_forced_tick) {",
            "\t\t// A nohz_full CPU is in the kernel and RCU needs a",
            "\t\t// quiescent state.  Turn on the tick!",
            "\t\tWRITE_ONCE(rdp->rcu_forced_tick, true);",
            "\t\ttick_dep_set_cpu(rdp->cpu, TICK_DEP_BIT_RCU);",
            "\t}",
            "\traw_spin_unlock_rcu_node(rdp->mynode);",
            "}"
          ],
          "function_name": "param_set_first_fqs_jiffies, param_set_next_fqs_jiffies, rcu_get_gp_seq, rcu_exp_batches_completed, rcutorture_get_gp_data, late_wakeup_func, rcu_irq_work_resched, rcu_irq_exit_check_preempt, __rcu_irq_enter_check_tick",
          "description": "实现参数配置回调函数和中断上下文RCU工作重排逻辑，管理grace period序列号读取及nohz_full模式下的tick依赖关系。",
          "similarity": 0.5532681345939636
        },
        {
          "chunk_id": 22,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4072,
          "end_line": 4226,
          "content": [
            "static void rcu_barrier_trace(const char *s, int cpu, unsigned long done)",
            "{",
            "\ttrace_rcu_barrier(rcu_state.name, s, cpu,",
            "\t\t\t  atomic_read(&rcu_state.barrier_cpu_count), done);",
            "}",
            "static void rcu_barrier_callback(struct rcu_head *rhp)",
            "{",
            "\tunsigned long __maybe_unused s = rcu_state.barrier_sequence;",
            "",
            "\tif (atomic_dec_and_test(&rcu_state.barrier_cpu_count)) {",
            "\t\trcu_barrier_trace(TPS(\"LastCB\"), -1, s);",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "\t} else {",
            "\t\trcu_barrier_trace(TPS(\"CB\"), -1, s);",
            "\t}",
            "}",
            "static void rcu_barrier_entrain(struct rcu_data *rdp)",
            "{",
            "\tunsigned long gseq = READ_ONCE(rcu_state.barrier_sequence);",
            "\tunsigned long lseq = READ_ONCE(rdp->barrier_seq_snap);",
            "\tbool wake_nocb = false;",
            "\tbool was_alldone = false;",
            "",
            "\tlockdep_assert_held(&rcu_state.barrier_lock);",
            "\tif (rcu_seq_state(lseq) || !rcu_seq_state(gseq) || rcu_seq_ctr(lseq) != rcu_seq_ctr(gseq))",
            "\t\treturn;",
            "\trcu_barrier_trace(TPS(\"IRQ\"), -1, rcu_state.barrier_sequence);",
            "\trdp->barrier_head.func = rcu_barrier_callback;",
            "\tdebug_rcu_head_queue(&rdp->barrier_head);",
            "\trcu_nocb_lock(rdp);",
            "\t/*",
            "\t * Flush bypass and wakeup rcuog if we add callbacks to an empty regular",
            "\t * queue. This way we don't wait for bypass timer that can reach seconds",
            "\t * if it's fully lazy.",
            "\t */",
            "\twas_alldone = rcu_rdp_is_offloaded(rdp) && !rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));",
            "\twake_nocb = was_alldone && rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tif (rcu_segcblist_entrain(&rdp->cblist, &rdp->barrier_head)) {",
            "\t\tatomic_inc(&rcu_state.barrier_cpu_count);",
            "\t} else {",
            "\t\tdebug_rcu_head_unqueue(&rdp->barrier_head);",
            "\t\trcu_barrier_trace(TPS(\"IRQNQ\"), -1, rcu_state.barrier_sequence);",
            "\t}",
            "\trcu_nocb_unlock(rdp);",
            "\tif (wake_nocb)",
            "\t\twake_nocb_gp(rdp, false);",
            "\tsmp_store_release(&rdp->barrier_seq_snap, gseq);",
            "}",
            "static void rcu_barrier_handler(void *cpu_in)",
            "{",
            "\tuintptr_t cpu = (uintptr_t)cpu_in;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\tWARN_ON_ONCE(cpu != rdp->cpu);",
            "\tWARN_ON_ONCE(cpu != smp_processor_id());",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\trcu_barrier_entrain(rdp);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "}",
            "void rcu_barrier(void)",
            "{",
            "\tuintptr_t cpu;",
            "\tunsigned long flags;",
            "\tunsigned long gseq;",
            "\tstruct rcu_data *rdp;",
            "\tunsigned long s = rcu_seq_snap(&rcu_state.barrier_sequence);",
            "",
            "\trcu_barrier_trace(TPS(\"Begin\"), -1, s);",
            "",
            "\t/* Take mutex to serialize concurrent rcu_barrier() requests. */",
            "\tmutex_lock(&rcu_state.barrier_mutex);",
            "",
            "\t/* Did someone else do our work for us? */",
            "\tif (rcu_seq_done(&rcu_state.barrier_sequence, s)) {",
            "\t\trcu_barrier_trace(TPS(\"EarlyExit\"), -1, rcu_state.barrier_sequence);",
            "\t\tsmp_mb(); /* caller's subsequent code after above check. */",
            "\t\tmutex_unlock(&rcu_state.barrier_mutex);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Mark the start of the barrier operation. */",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\trcu_seq_start(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\trcu_barrier_trace(TPS(\"Inc1\"), -1, rcu_state.barrier_sequence);",
            "",
            "\t/*",
            "\t * Initialize the count to two rather than to zero in order",
            "\t * to avoid a too-soon return to zero in case of an immediate",
            "\t * invocation of the just-enqueued callback (or preemption of",
            "\t * this task).  Exclude CPU-hotplug operations to ensure that no",
            "\t * offline non-offloaded CPU has callbacks queued.",
            "\t */",
            "\tinit_completion(&rcu_state.barrier_completion);",
            "\tatomic_set(&rcu_state.barrier_cpu_count, 2);",
            "\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "",
            "\t/*",
            "\t * Force each CPU with callbacks to register a new callback.",
            "\t * When that callback is invoked, we will know that all of the",
            "\t * corresponding CPU's preceding callbacks have been invoked.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "retry:",
            "\t\tif (smp_load_acquire(&rdp->barrier_seq_snap) == gseq)",
            "\t\t\tcontinue;",
            "\t\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\t\tif (!rcu_segcblist_n_cbs(&rdp->cblist)) {",
            "\t\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"NQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (!rcu_rdp_cpu_online(rdp)) {",
            "\t\t\trcu_barrier_entrain(rdp);",
            "\t\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"OfflineNoCBQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\tif (smp_call_function_single(cpu, rcu_barrier_handler, (void *)cpu, 1)) {",
            "\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\tgoto retry;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\trcu_barrier_trace(TPS(\"OnlineQ\"), cpu, rcu_state.barrier_sequence);",
            "\t}",
            "",
            "\t/*",
            "\t * Now that we have an rcu_barrier_callback() callback on each",
            "\t * CPU, and thus each counted, remove the initial count.",
            "\t */",
            "\tif (atomic_sub_and_test(2, &rcu_state.barrier_cpu_count))",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "",
            "\t/* Wait for all rcu_barrier_callback() callbacks to be invoked. */",
            "\twait_for_completion(&rcu_state.barrier_completion);",
            "",
            "\t/* Mark the end of the barrier operation. */",
            "\trcu_barrier_trace(TPS(\"Inc2\"), -1, rcu_state.barrier_sequence);",
            "\trcu_seq_end(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t}",
            "",
            "\t/* Other rcu_barrier() invocations can now safely proceed. */",
            "\tmutex_unlock(&rcu_state.barrier_mutex);",
            "}"
          ],
          "function_name": "rcu_barrier_trace, rcu_barrier_callback, rcu_barrier_entrain, rcu_barrier_handler, rcu_barrier",
          "description": "实现RCU屏障功能，通过分发回调函数强制所有CPU完成当前RCU操作，使用原子计数器跟踪完成状态，通过completion等待所有回调完成",
          "similarity": 0.5505958199501038
        },
        {
          "chunk_id": 16,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2977,
          "end_line": 3111,
          "content": [
            "static inline bool",
            "put_cached_bnode(struct kfree_rcu_cpu *krcp,",
            "\tstruct kvfree_rcu_bulk_data *bnode)",
            "{",
            "\t// Check the limit.",
            "\tif (krcp->nr_bkv_objs >= rcu_min_cached_objs)",
            "\t\treturn false;",
            "",
            "\tllist_add((struct llist_node *) bnode, &krcp->bkvcache);",
            "\tWRITE_ONCE(krcp->nr_bkv_objs, krcp->nr_bkv_objs + 1);",
            "\treturn true;",
            "}",
            "static int",
            "drain_page_cache(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tunsigned long flags;",
            "\tstruct llist_node *page_list, *pos, *n;",
            "\tint freed = 0;",
            "",
            "\tif (!rcu_min_cached_objs)",
            "\t\treturn 0;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\tpage_list = llist_del_all(&krcp->bkvcache);",
            "\tWRITE_ONCE(krcp->nr_bkv_objs, 0);",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "",
            "\tllist_for_each_safe(pos, n, page_list) {",
            "\t\tfree_page((unsigned long)pos);",
            "\t\tfreed++;",
            "\t}",
            "",
            "\treturn freed;",
            "}",
            "static void",
            "kvfree_rcu_bulk(struct kfree_rcu_cpu *krcp,",
            "\tstruct kvfree_rcu_bulk_data *bnode, int idx)",
            "{",
            "\tunsigned long flags;",
            "\tint i;",
            "",
            "\tif (!WARN_ON_ONCE(!poll_state_synchronize_rcu_full(&bnode->gp_snap))) {",
            "\t\tdebug_rcu_bhead_unqueue(bnode);",
            "\t\trcu_lock_acquire(&rcu_callback_map);",
            "\t\tif (idx == 0) { // kmalloc() / kfree().",
            "\t\t\ttrace_rcu_invoke_kfree_bulk_callback(",
            "\t\t\t\trcu_state.name, bnode->nr_records,",
            "\t\t\t\tbnode->records);",
            "",
            "\t\t\tkfree_bulk(bnode->nr_records, bnode->records);",
            "\t\t} else { // vmalloc() / vfree().",
            "\t\t\tfor (i = 0; i < bnode->nr_records; i++) {",
            "\t\t\t\ttrace_rcu_invoke_kvfree_callback(",
            "\t\t\t\t\trcu_state.name, bnode->records[i], 0);",
            "",
            "\t\t\t\tvfree(bnode->records[i]);",
            "\t\t\t}",
            "\t\t}",
            "\t\trcu_lock_release(&rcu_callback_map);",
            "\t}",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\tif (put_cached_bnode(krcp, bnode))",
            "\t\tbnode = NULL;",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "",
            "\tif (bnode)",
            "\t\tfree_page((unsigned long) bnode);",
            "",
            "\tcond_resched_tasks_rcu_qs();",
            "}",
            "static void",
            "kvfree_rcu_list(struct rcu_head *head)",
            "{",
            "\tstruct rcu_head *next;",
            "",
            "\tfor (; head; head = next) {",
            "\t\tvoid *ptr = (void *) head->func;",
            "\t\tunsigned long offset = (void *) head - ptr;",
            "",
            "\t\tnext = head->next;",
            "\t\tdebug_rcu_head_unqueue((struct rcu_head *)ptr);",
            "\t\trcu_lock_acquire(&rcu_callback_map);",
            "\t\ttrace_rcu_invoke_kvfree_callback(rcu_state.name, head, offset);",
            "",
            "\t\tif (!WARN_ON_ONCE(!__is_kvfree_rcu_offset(offset)))",
            "\t\t\tkvfree(ptr);",
            "",
            "\t\trcu_lock_release(&rcu_callback_map);",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t}",
            "}",
            "static void kfree_rcu_work(struct work_struct *work)",
            "{",
            "\tunsigned long flags;",
            "\tstruct kvfree_rcu_bulk_data *bnode, *n;",
            "\tstruct list_head bulk_head[FREE_N_CHANNELS];",
            "\tstruct rcu_head *head;",
            "\tstruct kfree_rcu_cpu *krcp;",
            "\tstruct kfree_rcu_cpu_work *krwp;",
            "\tstruct rcu_gp_oldstate head_gp_snap;",
            "\tint i;",
            "",
            "\tkrwp = container_of(to_rcu_work(work),",
            "\t\tstruct kfree_rcu_cpu_work, rcu_work);",
            "\tkrcp = krwp->krcp;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\t// Channels 1 and 2.",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\tlist_replace_init(&krwp->bulk_head_free[i], &bulk_head[i]);",
            "",
            "\t// Channel 3.",
            "\thead = krwp->head_free;",
            "\tkrwp->head_free = NULL;",
            "\thead_gp_snap = krwp->head_free_gp_snap;",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "",
            "\t// Handle the first two channels.",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++) {",
            "\t\t// Start from the tail page, so a GP is likely passed for it.",
            "\t\tlist_for_each_entry_safe(bnode, n, &bulk_head[i], list)",
            "\t\t\tkvfree_rcu_bulk(krcp, bnode, i);",
            "\t}",
            "",
            "\t/*",
            "\t * This is used when the \"bulk\" path can not be used for the",
            "\t * double-argument of kvfree_rcu().  This happens when the",
            "\t * page-cache is empty, which means that objects are instead",
            "\t * queued on a linked list through their rcu_head structures.",
            "\t * This list is named \"Channel 3\".",
            "\t */",
            "\tif (head && !WARN_ON_ONCE(!poll_state_synchronize_rcu_full(&head_gp_snap)))",
            "\t\tkvfree_rcu_list(head);",
            "}"
          ],
          "function_name": "put_cached_bnode, drain_page_cache, kvfree_rcu_bulk, kvfree_rcu_list, kfree_rcu_work",
          "description": "处理批量内存释放，通过缓存节点管理、页面缓存填充及多通道分类释放机制实现高效内存回收。",
          "similarity": 0.5492886900901794
        }
      ]
    },
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.5789965987205505,
      "chunks": [
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.5839776992797852
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mempolicy.c",
          "start_line": 168,
          "end_line": 268,
          "content": [
            "static u8 get_il_weight(int node)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tu8 weight = 1;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state)",
            "\t\tweight = state->iw_table[node];",
            "\trcu_read_unlock();",
            "\treturn weight;",
            "}",
            "static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)",
            "{",
            "\tu64 sum_bw = 0;",
            "\tunsigned int cast_sum_bw, scaling_factor = 1, iw_gcd = 0;",
            "\tint nid;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tsum_bw += bw[nid];",
            "",
            "\t/* Scale bandwidths to whole numbers in the range [1, weightiness] */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t/*",
            "\t\t * Try not to perform 64-bit division.",
            "\t\t * If sum_bw < scaling_factor, then sum_bw < U32_MAX.",
            "\t\t * If sum_bw > scaling_factor, then round the weight up to 1.",
            "\t\t */",
            "\t\tscaling_factor = weightiness * bw[nid];",
            "\t\tif (bw[nid] && sum_bw < scaling_factor) {",
            "\t\t\tcast_sum_bw = (unsigned int)sum_bw;",
            "\t\t\tnew_iw[nid] = scaling_factor / cast_sum_bw;",
            "\t\t} else {",
            "\t\t\tnew_iw[nid] = 1;",
            "\t\t}",
            "\t\tif (!iw_gcd)",
            "\t\t\tiw_gcd = new_iw[nid];",
            "\t\tiw_gcd = gcd(iw_gcd, new_iw[nid]);",
            "\t}",
            "",
            "\t/* 1:2 is strictly better than 16:32. Reduce by the weights' GCD. */",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tnew_iw[nid] /= iw_gcd;",
            "}",
            "int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tunsigned int *old_bw, *new_bw;",
            "\tunsigned int bw_val;",
            "\tint i;",
            "",
            "\tbw_val = min(coords->read_bandwidth, coords->write_bandwidth);",
            "\tnew_bw = kcalloc(nr_node_ids, sizeof(unsigned int), GFP_KERNEL);",
            "\tif (!new_bw)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew_wi_state = kmalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state) {",
            "\t\tkfree(new_bw);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tnew_wi_state->mode_auto = true;",
            "\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\tnew_wi_state->iw_table[i] = 1;",
            "",
            "\t/*",
            "\t * Update bandwidth info, even in manual mode. That way, when switching",
            "\t * to auto mode in the future, iw_table can be overwritten using",
            "\t * accurate bw data.",
            "\t */",
            "\tmutex_lock(&wi_state_lock);",
            "",
            "\told_bw = node_bw_table;",
            "\tif (old_bw)",
            "\t\tmemcpy(new_bw, old_bw, nr_node_ids * sizeof(*old_bw));",
            "\tnew_bw[node] = bw_val;",
            "\tnode_bw_table = new_bw;",
            "",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state && !old_wi_state->mode_auto) {",
            "\t\t/* Manual mode; skip reducing weights and updating wi_state */",
            "\t\tmutex_unlock(&wi_state_lock);",
            "\t\tkfree(new_wi_state);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* NULL wi_state assumes auto=true; reduce weights and update wi_state*/",
            "\treduce_interleave_weights(new_bw, new_wi_state->iw_table);",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "out:",
            "\tkfree(old_bw);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_il_weight, reduce_interleave_weights, mempolicy_set_node_perf",
          "description": "实现带权交错策略的权重计算与调整逻辑，通过获取节点带宽数据动态修改权重比例，支持根据性能参数更新节点间内存分配优先级。",
          "similarity": 0.5651564002037048
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.5629547834396362
        },
        {
          "chunk_id": 17,
          "file_path": "mm/mempolicy.c",
          "start_line": 2969,
          "end_line": 3093,
          "content": [
            "void mpol_put_task_policy(struct task_struct *task)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\ttask_lock(task);",
            "\tpol = task->mempolicy;",
            "\ttask->mempolicy = NULL;",
            "\ttask_unlock(task);",
            "\tmpol_put(pol);",
            "}",
            "static void sp_delete(struct shared_policy *sp, struct sp_node *n)",
            "{",
            "\trb_erase(&n->nd, &sp->root);",
            "\tsp_free(n);",
            "}",
            "static void sp_node_init(struct sp_node *node, unsigned long start,",
            "\t\t\tunsigned long end, struct mempolicy *pol)",
            "{",
            "\tnode->start = start;",
            "\tnode->end = end;",
            "\tnode->policy = pol;",
            "}",
            "static int shared_policy_replace(struct shared_policy *sp, pgoff_t start,",
            "\t\t\t\t pgoff_t end, struct sp_node *new)",
            "{",
            "\tstruct sp_node *n;",
            "\tstruct sp_node *n_new = NULL;",
            "\tstruct mempolicy *mpol_new = NULL;",
            "\tint ret = 0;",
            "",
            "restart:",
            "\twrite_lock(&sp->lock);",
            "\tn = sp_lookup(sp, start, end);",
            "\t/* Take care of old policies in the same range. */",
            "\twhile (n && n->start < end) {",
            "\t\tstruct rb_node *next = rb_next(&n->nd);",
            "\t\tif (n->start >= start) {",
            "\t\t\tif (n->end <= end)",
            "\t\t\t\tsp_delete(sp, n);",
            "\t\t\telse",
            "\t\t\t\tn->start = end;",
            "\t\t} else {",
            "\t\t\t/* Old policy spanning whole new range. */",
            "\t\t\tif (n->end > end) {",
            "\t\t\t\tif (!n_new)",
            "\t\t\t\t\tgoto alloc_new;",
            "",
            "\t\t\t\t*mpol_new = *n->policy;",
            "\t\t\t\tatomic_set(&mpol_new->refcnt, 1);",
            "\t\t\t\tsp_node_init(n_new, end, n->end, mpol_new);",
            "\t\t\t\tn->end = start;",
            "\t\t\t\tsp_insert(sp, n_new);",
            "\t\t\t\tn_new = NULL;",
            "\t\t\t\tmpol_new = NULL;",
            "\t\t\t\tbreak;",
            "\t\t\t} else",
            "\t\t\t\tn->end = start;",
            "\t\t}",
            "\t\tif (!next)",
            "\t\t\tbreak;",
            "\t\tn = rb_entry(next, struct sp_node, nd);",
            "\t}",
            "\tif (new)",
            "\t\tsp_insert(sp, new);",
            "\twrite_unlock(&sp->lock);",
            "\tret = 0;",
            "",
            "err_out:",
            "\tif (mpol_new)",
            "\t\tmpol_put(mpol_new);",
            "\tif (n_new)",
            "\t\tkmem_cache_free(sn_cache, n_new);",
            "",
            "\treturn ret;",
            "",
            "alloc_new:",
            "\twrite_unlock(&sp->lock);",
            "\tret = -ENOMEM;",
            "\tn_new = kmem_cache_alloc(sn_cache, GFP_KERNEL);",
            "\tif (!n_new)",
            "\t\tgoto err_out;",
            "\tmpol_new = kmem_cache_alloc(policy_cache, GFP_KERNEL);",
            "\tif (!mpol_new)",
            "\t\tgoto err_out;",
            "\tatomic_set(&mpol_new->refcnt, 1);",
            "\tgoto restart;",
            "}",
            "void mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol)",
            "{",
            "\tint ret;",
            "",
            "\tsp->root = RB_ROOT;\t\t/* empty tree == default mempolicy */",
            "\trwlock_init(&sp->lock);",
            "",
            "\tif (mpol) {",
            "\t\tstruct sp_node *sn;",
            "\t\tstruct mempolicy *npol;",
            "\t\tNODEMASK_SCRATCH(scratch);",
            "",
            "\t\tif (!scratch)",
            "\t\t\tgoto put_mpol;",
            "",
            "\t\t/* contextualize the tmpfs mount point mempolicy to this file */",
            "\t\tnpol = mpol_new(mpol->mode, mpol->flags, &mpol->w.user_nodemask);",
            "\t\tif (IS_ERR(npol))",
            "\t\t\tgoto free_scratch; /* no valid nodemask intersection */",
            "",
            "\t\ttask_lock(current);",
            "\t\tret = mpol_set_nodemask(npol, &mpol->w.user_nodemask, scratch);",
            "\t\ttask_unlock(current);",
            "\t\tif (ret)",
            "\t\t\tgoto put_npol;",
            "",
            "\t\t/* alloc node covering entire file; adds ref to file's npol */",
            "\t\tsn = sp_alloc(0, MAX_LFS_FILESIZE >> PAGE_SHIFT, npol);",
            "\t\tif (sn)",
            "\t\t\tsp_insert(sp, sn);",
            "put_npol:",
            "\t\tmpol_put(npol);\t/* drop initial ref on file's npol */",
            "free_scratch:",
            "\t\tNODEMASK_SCRATCH_FREE(scratch);",
            "put_mpol:",
            "\t\tmpol_put(mpol);\t/* drop our incoming ref on sb mpol */",
            "\t}",
            "}"
          ],
          "function_name": "mpol_put_task_policy, sp_delete, sp_node_init, shared_policy_replace, mpol_shared_policy_init",
          "description": "mpol_put_task_policy释放任务级内存策略引用，sp_delete从RB树删除节点并回收资源，sp_node_init初始化共享策略节点，shared_policy_replace替换共享策略区间并处理节点分裂，mpol_shared_policy_init初始化共享策略结构体并设置初始策略。",
          "similarity": 0.5534802675247192
        },
        {
          "chunk_id": 16,
          "file_path": "mm/mempolicy.c",
          "start_line": 2846,
          "end_line": 2946,
          "content": [
            "static void sp_free(struct sp_node *n)",
            "{",
            "\tmpol_put(n->policy);",
            "\tkmem_cache_free(sn_cache, n);",
            "}",
            "int mpol_misplaced(struct folio *folio, struct vm_fault *vmf,",
            "\t\t   unsigned long addr)",
            "{",
            "\tstruct mempolicy *pol;",
            "\tpgoff_t ilx;",
            "\tstruct zoneref *z;",
            "\tint curnid = folio_nid(folio);",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tint thiscpu = raw_smp_processor_id();",
            "\tint thisnid = numa_node_id();",
            "\tint polnid = NUMA_NO_NODE;",
            "\tint ret = NUMA_NO_NODE;",
            "",
            "\t/*",
            "\t * Make sure ptl is held so that we don't preempt and we",
            "\t * have a stable smp processor id",
            "\t */",
            "\tlockdep_assert_held(vmf->ptl);",
            "\tpol = get_vma_policy(vma, addr, folio_order(folio), &ilx);",
            "\tif (!(pol->flags & MPOL_F_MOF))",
            "\t\tgoto out;",
            "",
            "\tswitch (pol->mode) {",
            "\tcase MPOL_INTERLEAVE:",
            "\t\tpolnid = interleave_nid(pol, ilx);",
            "\t\tbreak;",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\tpolnid = weighted_interleave_nid(pol, ilx);",
            "\t\tbreak;",
            "",
            "\tcase MPOL_PREFERRED:",
            "\t\tif (node_isset(curnid, pol->nodes))",
            "\t\t\tgoto out;",
            "\t\tpolnid = first_node(pol->nodes);",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tpolnid = numa_node_id();",
            "\t\tbreak;",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t\t/*",
            "\t\t * Even though MPOL_PREFERRED_MANY can allocate pages outside",
            "\t\t * policy nodemask we don't allow numa migration to nodes",
            "\t\t * outside policy nodemask for now. This is done so that if we",
            "\t\t * want demotion to slow memory to happen, before allocating",
            "\t\t * from some DRAM node say 'x', we will end up using a",
            "\t\t * MPOL_PREFERRED_MANY mask excluding node 'x'. In such scenario",
            "\t\t * we should not promote to node 'x' from slow memory node.",
            "\t\t */",
            "\t\tif (pol->flags & MPOL_F_MORON) {",
            "\t\t\t/*",
            "\t\t\t * Optimize placement among multiple nodes",
            "\t\t\t * via NUMA balancing",
            "\t\t\t */",
            "\t\t\tif (node_isset(thisnid, pol->nodes))",
            "\t\t\t\tbreak;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * use current page if in policy nodemask,",
            "\t\t * else select nearest allowed node, if any.",
            "\t\t * If no allowed nodes, use current [!misplaced].",
            "\t\t */",
            "\t\tif (node_isset(curnid, pol->nodes))",
            "\t\t\tgoto out;",
            "\t\tz = first_zones_zonelist(",
            "\t\t\t\tnode_zonelist(thisnid, GFP_HIGHUSER),",
            "\t\t\t\tgfp_zone(GFP_HIGHUSER),",
            "\t\t\t\t&pol->nodes);",
            "\t\tpolnid = zone_to_nid(z->zone);",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "",
            "\t/* Migrate the folio towards the node whose CPU is referencing it */",
            "\tif (pol->flags & MPOL_F_MORON) {",
            "\t\tpolnid = thisnid;",
            "",
            "\t\tif (!should_numa_migrate_memory(current, folio, curnid,",
            "\t\t\t\t\t\tthiscpu))",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tif (curnid != polnid)",
            "\t\tret = polnid;",
            "out:",
            "\tmpol_cond_put(pol);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "sp_free, mpol_misplaced",
          "description": "sp_free释放共享策略节点资源，mpol_misplaced判断页面是否符合内存策略节点要求，若不符合则计算应迁移的目标节点，支持多种策略模式下的节点选择逻辑。",
          "similarity": 0.547890305519104
        }
      ]
    }
  ]
}