{
  "query": "操作系统中管程的代码结构示例",
  "timestamp": "2025-12-25 23:45:43",
  "retrieved_files": [
    {
      "source_file": "kernel/rseq.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:54:26\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rseq.c`\n\n---\n\n# rseq.c 技术文档\n\n## 文件概述\n\n`rseq.c` 实现了 Linux 内核对 **Restartable Sequences（可重启序列）** 系统调用的支持。该机制允许用户空间程序在不使用重量级原子操作的前提下，高效地执行与调度器抢占、信号投递和 CPU 迁移相关的**伪原子操作**，特别适用于高性能的每 CPU（per-CPU）数据结构操作。该文件负责管理用户空间注册的 `struct rseq` TLS（线程局部存储）区域，并在任务被抢占、迁移或收到信号时，安全地中止并重定向用户空间执行流。\n\n## 核心功能\n\n### 主要数据结构\n- `struct rseq`：用户空间注册的 TLS 结构，包含 `cpu_id_start`、`cpu_id`、`node_id`、`mm_cid` 和 `rseq_cs`（critical section 描述符指针）等字段。\n- `struct rseq_cs`：用户空间关键区（critical section）的描述结构，包含起始地址、提交地址、中止地址和标志位。\n\n### 主要函数\n- `rseq_update_cpu_node_id(struct task_struct *t)`  \n  更新任务的 `rseq` TLS 区域中的 CPU ID、NUMA 节点 ID 和内存上下文 ID（mm_cid），用于反映当前执行上下文。\n  \n- `rseq_reset_rseq_cpu_node_id(struct task_struct *t)`  \n  将任务的 `rseq` TLS 区域重置为初始状态（`cpu_id` 设为 `RSEQ_CPU_ID_UNINITIALIZED`）。\n\n- `rseq_get_rseq_cs_ptr_val(struct rseq __user *rseq, u64 *rseq_cs)`  \n  从用户空间 `rseq` 结构中安全读取 `rseq_cs` 指针值。\n\n- `rseq_get_rseq_cs(struct task_struct *t, struct rseq_cs *rseq_cs)`  \n  若 `rseq_cs` 指针有效，则从用户空间复制并验证 `struct rseq_cs` 内容（代码片段未完整展示）。\n\n- `rseq_validate_ro_fields(struct task_struct *t)`（仅在 `CONFIG_DEBUG_RSEQ` 下启用）  \n  验证用户空间 `rseq` 结构中应为只读的字段是否与内核副本一致，防止用户空间篡改。\n\n### 宏定义\n- `rseq_unsafe_put_user()`：在写入用户空间 `rseq` 字段的同时，同步更新内核中的副本（调试模式下），确保状态一致性。\n- `RSEQ_CS_NO_RESTART_FLAGS`：定义关键区中禁止因抢占、信号或迁移而重启的标志组合。\n\n## 关键实现\n\n### 可重启序列执行模型\n用户空间关键区执行流程如下：\n1. 将关键区描述符地址写入 TLS 的 `rseq->rseq_cs`；\n2. 比较 `cpu_id_start` 与当前 `cpu_id`，不一致则跳转至 `abort_ip`；\n3. 执行关键区操作；\n4. 成功提交后继续正常执行。\n\n若在步骤 1–3 之间发生**抢占、CPU 迁移或信号投递**，内核会：\n- 清空 `rseq->rseq_cs`（设为 NULL）；\n- 将用户空间返回地址设置为 `abort_ip`；\n- 恢复执行时跳转至中止处理逻辑。\n\n### 安全访问与调试支持\n- 使用 `user_read_access_begin/end()` 和 `user_write_access_begin/end()` 确保对用户空间内存的安全访问。\n- 在 `CONFIG_DEBUG_RSEQ` 模式下，内核维护 `rseq` 字段的内核副本，并在每次更新前后校验用户空间只读字段的一致性，防止恶意或错误的用户空间修改。\n- 通过 `trace_rseq_update()` 提供跟踪点，便于性能分析和调试。\n\n### 兼容性处理\n- 原始 `rseq` 结构大小为 32 字节（`ORIG_RSEQ_SIZE`）；\n- 对于扩展字段（如 `mm_cid`），仅在 `t->rseq_len > ORIG_RSEQ_SIZE` 时才进行更新或重置，确保向后兼容。\n\n## 依赖关系\n\n- **调度子系统**：依赖 `raw_smp_processor_id()` 获取当前 CPU，`task_mm_cid()` 获取内存上下文 ID。\n- **内存管理**：使用 `cpu_to_node()` 获取 NUMA 节点信息。\n- **用户空间访问**：依赖 `uaccess.h` 提供的安全用户空间读写原语（如 `unsafe_get_user`/`unsafe_put_user`）。\n- **跟踪系统**：通过 `trace/events/rseq.h` 集成内核跟踪基础设施。\n- **架构支持**：依赖 `asm/ptrace.h` 处理信号/抢占后的用户空间返回地址重定向（完整实现位于架构相关代码中）。\n\n## 使用场景\n\n- **高性能 per-CPU 操作**：如无锁计数器、每 CPU 队列等，避免传统原子操作或锁的开销。\n- **实时/低延迟应用**：减少因内核同步原语引入的延迟抖动。\n- **用户空间调度器/运行时**：如 Go、Java 虚拟机等，用于实现高效的线程本地状态管理。\n- **系统调用 `sys_rseq()`**：由用户空间通过 `rseq(2)` 系统调用注册或注销 `rseq` TLS 区域，本文件提供内核侧支持逻辑（注册/注销时调用 `rseq_update_cpu_node_id` 或 `rseq_reset_rseq_cpu_node_id`）。",
      "similarity": 0.5674456357955933,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/rseq.c",
          "start_line": 1,
          "end_line": 34,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0+",
            "/*",
            " * Restartable sequences system call",
            " *",
            " * Copyright (C) 2015, Google, Inc.,",
            " * Paul Turner <pjt@google.com> and Andrew Hunter <ahh@google.com>",
            " * Copyright (C) 2015-2018, EfficiOS Inc.,",
            " * Mathieu Desnoyers <mathieu.desnoyers@efficios.com>",
            " */",
            "",
            "#include <linux/sched.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/rseq.h>",
            "#include <linux/types.h>",
            "#include <linux/ratelimit.h>",
            "#include <asm/ptrace.h>",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/rseq.h>",
            "",
            "/* The original rseq structure size (including padding) is 32 bytes. */",
            "#define ORIG_RSEQ_SIZE\t\t32",
            "",
            "#define RSEQ_CS_NO_RESTART_FLAGS (RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT | \\",
            "\t\t\t\t  RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL | \\",
            "\t\t\t\t  RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE)",
            "",
            "#ifdef CONFIG_DEBUG_RSEQ",
            "static struct rseq *rseq_kernel_fields(struct task_struct *t)",
            "{",
            "\treturn (struct rseq *) t->rseq_fields;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义rseq内核字段访问函数，用于获取当前任务的rseq结构体指针，供后续验证和操作使用。",
          "similarity": 0.49875426292419434
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/rseq.c",
          "start_line": 242,
          "end_line": 346,
          "content": [
            "static int rseq_get_rseq_cs_ptr_val(struct rseq __user *rseq, u64 *rseq_cs)",
            "{",
            "\tif (!rseq_cs)",
            "\t\treturn -EFAULT;",
            "",
            "#ifdef CONFIG_64BIT",
            "\tif (get_user(*rseq_cs, &rseq->rseq_cs))",
            "\t\treturn -EFAULT;",
            "#else",
            "\tif (copy_from_user(rseq_cs, &rseq->rseq_cs, sizeof(*rseq_cs)))",
            "\t\treturn -EFAULT;",
            "#endif",
            "",
            "\treturn 0;",
            "}",
            "static int rseq_get_rseq_cs(struct task_struct *t, struct rseq_cs *rseq_cs)",
            "{",
            "\tstruct rseq_cs __user *urseq_cs;",
            "\tu64 ptr;",
            "\tu32 __user *usig;",
            "\tu32 sig;",
            "\tint ret;",
            "",
            "\tret = rseq_get_rseq_cs_ptr_val(t->rseq, &ptr);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\t/* If the rseq_cs pointer is NULL, return a cleared struct rseq_cs. */",
            "\tif (!ptr) {",
            "\t\tmemset(rseq_cs, 0, sizeof(*rseq_cs));",
            "\t\treturn 0;",
            "\t}",
            "\t/* Check that the pointer value fits in the user-space process space. */",
            "\tif (ptr >= TASK_SIZE)",
            "\t\treturn -EINVAL;",
            "\turseq_cs = (struct rseq_cs __user *)(unsigned long)ptr;",
            "\tif (copy_from_user(rseq_cs, urseq_cs, sizeof(*rseq_cs)))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (rseq_cs->start_ip >= TASK_SIZE ||",
            "\t    rseq_cs->start_ip + rseq_cs->post_commit_offset >= TASK_SIZE ||",
            "\t    rseq_cs->abort_ip >= TASK_SIZE ||",
            "\t    rseq_cs->version > 0)",
            "\t\treturn -EINVAL;",
            "\t/* Check for overflow. */",
            "\tif (rseq_cs->start_ip + rseq_cs->post_commit_offset < rseq_cs->start_ip)",
            "\t\treturn -EINVAL;",
            "\t/* Ensure that abort_ip is not in the critical section. */",
            "\tif (rseq_cs->abort_ip - rseq_cs->start_ip < rseq_cs->post_commit_offset)",
            "\t\treturn -EINVAL;",
            "",
            "\tusig = (u32 __user *)(unsigned long)(rseq_cs->abort_ip - sizeof(u32));",
            "\tret = get_user(sig, usig);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tif (current->rseq_sig != sig) {",
            "\t\tprintk_ratelimited(KERN_WARNING",
            "\t\t\t\"Possible attack attempt. Unexpected rseq signature 0x%x, expecting 0x%x (pid=%d, addr=%p).\\n\",",
            "\t\t\tsig, current->rseq_sig, current->pid, usig);",
            "\t\treturn -EINVAL;",
            "\t}",
            "\treturn 0;",
            "}",
            "static bool rseq_warn_flags(const char *str, u32 flags)",
            "{",
            "\tu32 test_flags;",
            "",
            "\tif (!flags)",
            "\t\treturn false;",
            "\ttest_flags = flags & RSEQ_CS_NO_RESTART_FLAGS;",
            "\tif (test_flags)",
            "\t\tpr_warn_once(\"Deprecated flags (%u) in %s ABI structure\", test_flags, str);",
            "\ttest_flags = flags & ~RSEQ_CS_NO_RESTART_FLAGS;",
            "\tif (test_flags)",
            "\t\tpr_warn_once(\"Unknown flags (%u) in %s ABI structure\", test_flags, str);",
            "\treturn true;",
            "}",
            "static int rseq_need_restart(struct task_struct *t, u32 cs_flags)",
            "{",
            "\tu32 flags, event_mask;",
            "\tint ret;",
            "",
            "\tif (rseq_warn_flags(\"rseq_cs\", cs_flags))",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Get thread flags. */",
            "\tret = get_user(flags, &t->rseq->flags);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tif (rseq_warn_flags(\"rseq\", flags))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Load and clear event mask atomically with respect to",
            "\t * scheduler preemption.",
            "\t */",
            "\tpreempt_disable();",
            "\tevent_mask = t->rseq_event_mask;",
            "\tt->rseq_event_mask = 0;",
            "\tpreempt_enable();",
            "",
            "\treturn !!event_mask;",
            "}"
          ],
          "function_name": "rseq_get_rseq_cs_ptr_val, rseq_get_rseq_cs, rseq_warn_flags, rseq_need_restart",
          "description": "提供rseq_cs指针解析、结构体验证、标志位检查及是否需要重启的判定逻辑，保障安全性和兼容性。",
          "similarity": 0.4857812821865082
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/rseq.c",
          "start_line": 35,
          "end_line": 159,
          "content": [
            "static int rseq_validate_ro_fields(struct task_struct *t)",
            "{",
            "\tstatic DEFINE_RATELIMIT_STATE(_rs,",
            "\t\t\t\t      DEFAULT_RATELIMIT_INTERVAL,",
            "\t\t\t\t      DEFAULT_RATELIMIT_BURST);",
            "\tu32 cpu_id_start, cpu_id, node_id, mm_cid;",
            "\tstruct rseq __user *rseq = t->rseq;",
            "",
            "\t/*",
            "\t * Validate fields which are required to be read-only by",
            "\t * user-space.",
            "\t */",
            "\tif (!user_read_access_begin(rseq, t->rseq_len))",
            "\t\tgoto efault;",
            "\tunsafe_get_user(cpu_id_start, &rseq->cpu_id_start, efault_end);",
            "\tunsafe_get_user(cpu_id, &rseq->cpu_id, efault_end);",
            "\tunsafe_get_user(node_id, &rseq->node_id, efault_end);",
            "\tunsafe_get_user(mm_cid, &rseq->mm_cid, efault_end);",
            "\tuser_read_access_end();",
            "",
            "\tif ((cpu_id_start != rseq_kernel_fields(t)->cpu_id_start ||",
            "\t    cpu_id != rseq_kernel_fields(t)->cpu_id ||",
            "\t    node_id != rseq_kernel_fields(t)->node_id ||",
            "\t    mm_cid != rseq_kernel_fields(t)->mm_cid) && __ratelimit(&_rs)) {",
            "",
            "\t\tpr_warn(\"Detected rseq corruption for pid: %d, name: %s\\n\"",
            "\t\t\t\"\\tcpu_id_start: %u ?= %u\\n\"",
            "\t\t\t\"\\tcpu_id:       %u ?= %u\\n\"",
            "\t\t\t\"\\tnode_id:      %u ?= %u\\n\"",
            "\t\t\t\"\\tmm_cid:       %u ?= %u\\n\",",
            "\t\t\tt->pid, t->comm,",
            "\t\t\tcpu_id_start, rseq_kernel_fields(t)->cpu_id_start,",
            "\t\t\tcpu_id, rseq_kernel_fields(t)->cpu_id,",
            "\t\t\tnode_id, rseq_kernel_fields(t)->node_id,",
            "\t\t\tmm_cid, rseq_kernel_fields(t)->mm_cid);",
            "\t}",
            "",
            "\t/* For now, only print a console warning on mismatch. */",
            "\treturn 0;",
            "",
            "efault_end:",
            "\tuser_read_access_end();",
            "efault:",
            "\treturn -EFAULT;",
            "}",
            "static int rseq_validate_ro_fields(struct task_struct *t)",
            "{",
            "\treturn 0;",
            "}",
            "static int rseq_update_cpu_node_id(struct task_struct *t)",
            "{",
            "\tstruct rseq __user *rseq = t->rseq;",
            "\tu32 cpu_id = raw_smp_processor_id();",
            "\tu32 node_id = cpu_to_node(cpu_id);",
            "\tu32 mm_cid = task_mm_cid(t);",
            "",
            "\t/*",
            "\t * Validate read-only rseq fields.",
            "\t */",
            "\tif (rseq_validate_ro_fields(t))",
            "\t\tgoto efault;",
            "\tWARN_ON_ONCE((int) mm_cid < 0);",
            "\tif (!user_write_access_begin(rseq, t->rseq_len))",
            "\t\tgoto efault;",
            "",
            "\trseq_unsafe_put_user(t, cpu_id, cpu_id_start, efault_end);",
            "\trseq_unsafe_put_user(t, cpu_id, cpu_id, efault_end);",
            "\trseq_unsafe_put_user(t, node_id, node_id, efault_end);",
            "\trseq_unsafe_put_user(t, mm_cid, mm_cid, efault_end);",
            "",
            "\t/*",
            "\t * Additional feature fields added after ORIG_RSEQ_SIZE",
            "\t * need to be conditionally updated only if",
            "\t * t->rseq_len != ORIG_RSEQ_SIZE.",
            "\t */",
            "\tuser_write_access_end();",
            "\ttrace_rseq_update(t);",
            "\treturn 0;",
            "",
            "efault_end:",
            "\tuser_write_access_end();",
            "efault:",
            "\treturn -EFAULT;",
            "}",
            "static int rseq_reset_rseq_cpu_node_id(struct task_struct *t)",
            "{",
            "\tstruct rseq __user *rseq = t->rseq;",
            "\tu32 cpu_id_start = 0, cpu_id = RSEQ_CPU_ID_UNINITIALIZED, node_id = 0,",
            "\t    mm_cid = 0;",
            "",
            "\t/*",
            "\t * Validate read-only rseq fields.",
            "\t */",
            "\tif (rseq_validate_ro_fields(t))",
            "\t\tgoto efault;",
            "",
            "\tif (!user_write_access_begin(rseq, t->rseq_len))",
            "\t\tgoto efault;",
            "",
            "\t/*",
            "\t * Reset all fields to their initial state.",
            "\t *",
            "\t * All fields have an initial state of 0 except cpu_id which is set to",
            "\t * RSEQ_CPU_ID_UNINITIALIZED, so that any user coming in after",
            "\t * unregistration can figure out that rseq needs to be registered",
            "\t * again.",
            "\t */",
            "\trseq_unsafe_put_user(t, cpu_id_start, cpu_id_start, efault_end);",
            "\trseq_unsafe_put_user(t, cpu_id, cpu_id, efault_end);",
            "\trseq_unsafe_put_user(t, node_id, node_id, efault_end);",
            "\trseq_unsafe_put_user(t, mm_cid, mm_cid, efault_end);",
            "",
            "\t/*",
            "\t * Additional feature fields added after ORIG_RSEQ_SIZE",
            "\t * need to be conditionally reset only if",
            "\t * t->rseq_len != ORIG_RSEQ_SIZE.",
            "\t */",
            "\tuser_write_access_end();",
            "\treturn 0;",
            "",
            "efault_end:",
            "\tuser_write_access_end();",
            "efault:",
            "\treturn -EFAULT;",
            "}"
          ],
          "function_name": "rseq_validate_ro_fields, rseq_validate_ro_fields, rseq_update_cpu_node_id, rseq_reset_rseq_cpu_node_id",
          "description": "实现rseq只读字段校验、CPU/节点ID更新及重置功能，检测并防止用户态对只读字段的篡改。",
          "similarity": 0.4763776659965515
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/rseq.c",
          "start_line": 355,
          "end_line": 442,
          "content": [
            "static int clear_rseq_cs(struct rseq __user *rseq)",
            "{",
            "\t/*",
            "\t * The rseq_cs field is set to NULL on preemption or signal",
            "\t * delivery on top of rseq assembly block, as well as on top",
            "\t * of code outside of the rseq assembly block. This performs",
            "\t * a lazy clear of the rseq_cs field.",
            "\t *",
            "\t * Set rseq_cs to NULL.",
            "\t */",
            "#ifdef CONFIG_64BIT",
            "\treturn put_user(0UL, &rseq->rseq_cs);",
            "#else",
            "\tif (clear_user(&rseq->rseq_cs, sizeof(rseq->rseq_cs)))",
            "\t\treturn -EFAULT;",
            "\treturn 0;",
            "#endif",
            "}",
            "static bool in_rseq_cs(unsigned long ip, struct rseq_cs *rseq_cs)",
            "{",
            "\treturn ip - rseq_cs->start_ip < rseq_cs->post_commit_offset;",
            "}",
            "static int rseq_ip_fixup(struct pt_regs *regs)",
            "{",
            "\tunsigned long ip = instruction_pointer(regs);",
            "\tstruct task_struct *t = current;",
            "\tstruct rseq_cs rseq_cs;",
            "\tint ret;",
            "",
            "\tret = rseq_get_rseq_cs(t, &rseq_cs);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\t/*",
            "\t * Handle potentially not being within a critical section.",
            "\t * If not nested over a rseq critical section, restart is useless.",
            "\t * Clear the rseq_cs pointer and return.",
            "\t */",
            "\tif (!in_rseq_cs(ip, &rseq_cs))",
            "\t\treturn clear_rseq_cs(t->rseq);",
            "\tret = rseq_need_restart(t, rseq_cs.flags);",
            "\tif (ret <= 0)",
            "\t\treturn ret;",
            "\tret = clear_rseq_cs(t->rseq);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\ttrace_rseq_ip_fixup(ip, rseq_cs.start_ip, rseq_cs.post_commit_offset,",
            "\t\t\t    rseq_cs.abort_ip);",
            "\tinstruction_pointer_set(regs, (unsigned long)rseq_cs.abort_ip);",
            "\treturn 0;",
            "}",
            "void __rseq_handle_notify_resume(struct ksignal *ksig, struct pt_regs *regs)",
            "{",
            "\tstruct task_struct *t = current;",
            "\tint ret, sig;",
            "",
            "\tif (unlikely(t->flags & PF_EXITING))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * regs is NULL if and only if the caller is in a syscall path.  Skip",
            "\t * fixup and leave rseq_cs as is so that rseq_sycall() will detect and",
            "\t * kill a misbehaving userspace on debug kernels.",
            "\t */",
            "\tif (regs) {",
            "\t\tret = rseq_ip_fixup(regs);",
            "\t\tif (unlikely(ret < 0))",
            "\t\t\tgoto error;",
            "\t}",
            "\tif (unlikely(rseq_update_cpu_node_id(t)))",
            "\t\tgoto error;",
            "\treturn;",
            "",
            "error:",
            "\tsig = ksig ? ksig->sig : 0;",
            "\tforce_sigsegv(sig);",
            "}",
            "void rseq_syscall(struct pt_regs *regs)",
            "{",
            "\tunsigned long ip = instruction_pointer(regs);",
            "\tstruct task_struct *t = current;",
            "\tstruct rseq_cs rseq_cs;",
            "",
            "\tif (!t->rseq)",
            "\t\treturn;",
            "\tif (rseq_get_rseq_cs(t, &rseq_cs) || in_rseq_cs(ip, &rseq_cs))",
            "\t\tforce_sig(SIGSEGV);",
            "}"
          ],
          "function_name": "clear_rseq_cs, in_rseq_cs, rseq_ip_fixup, __rseq_handle_notify_resume, rseq_syscall",
          "description": "实现rseq异常处理流程，包括临界区IP修复、通知恢复处理和系统调用保护，确保rseq执行的安全边界。",
          "similarity": 0.46279335021972656
        }
      ]
    },
    {
      "source_file": "kernel/sched/membarrier.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:12:44\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\membarrier.c`\n\n---\n\n# `sched/membarrier.c` 技术文档\n\n## 1. 文件概述\n\n`sched/membarrier.c` 实现了 Linux 内核中的 `membarrier` 系统调用，该调用为用户空间程序提供了一种高效的全局内存屏障机制。与传统的在每个线程中显式插入内存屏障相比，`membarrier` 允许一个线程通过一次系统调用，强制所有运行在系统上的线程（或特定进程组内的线程）执行内存屏障操作，从而简化用户空间并发同步逻辑并提升性能。\n\n该文件的核心目标是在多核系统中确保内存操作的全局可见性顺序，尤其适用于需要跨线程强内存顺序保证的用户空间同步原语（如 RCU、无锁数据结构等）。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`ipi_mb(void *info)`**  \n  IPI（处理器间中断）处理函数，执行 `smp_mb()` 内存屏障，用于基础的全局内存屏障命令。\n\n- **`ipi_sync_core(void *info)`**  \n  用于 `MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE` 命令的 IPI 处理函数，在执行内存屏障后调用 `sync_core_before_usermode()`，确保 CPU 核心状态同步（如指令缓存一致性）。\n\n- **`ipi_rseq(void *info)`**  \n  用于 `MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ` 命令的 IPI 处理函数，在内存屏障后调用 `rseq_preempt(current)`，以支持 restartable sequences（rseq）机制的正确性。\n\n- **`ipi_sync_rq_state(void *info)`**  \n  用于同步 per-CPU runqueue 的 `membarrier_state` 字段，使其与指定 `mm_struct` 的状态一致，确保后续 `membarrier` 调用能正确识别注册状态。\n\n- **`membarrier_exec_mmap(struct mm_struct *mm)`**  \n  在进程执行 `exec` 系统调用时被调用，重置该内存描述符（`mm_struct`）的 `membarrier_state` 为 0，并同步 per-CPU runqueue 状态，防止 exec 后残留旧的注册状态。\n\n### 数据结构与宏\n\n- **`MEMBARRIER_CMD_BITMASK`**  \n  定义所有支持的 `membarrier` 命令的位掩码（不含 `QUERY`），用于命令合法性校验。\n\n- **`membarrier_ipi_mutex`**  \n  互斥锁，用于序列化 IPI 发送过程，防止多个 `membarrier` 调用并发执行导致 IPI 风暴或状态不一致。\n\n- **`SERIALIZE_IPI()`**  \n  宏封装，使用 `membarrier_ipi_mutex` 实现 IPI 发送的串行化。\n\n## 3. 关键实现\n\n### 内存屏障语义保证\n\n文件顶部的注释详细描述了五种关键内存顺序场景（A–E），说明为何在 `membarrier()` 调用前后必须插入 `smp_mb()`：\n\n- **场景 A**：确保调用者 CPU 在 `membarrier()` 之前的写操作，在其他 CPU 收到 IPI 并执行屏障后对其可见。\n- **场景 B**：确保其他 CPU 在 IPI 屏障前的写操作，在调用者 CPU 执行 `membarrier()` 后对其可见。\n- **场景 C–E**：处理线程切换、`exit_mm`、kthread 使用/释放 mm 等边界情况，确保 `membarrier` 能正确识别用户态上下文并施加屏障。\n\n这些场景共同要求 `membarrier()` 实现必须在发送 IPI **前**和**后**各执行一次 `smp_mb()`，以建立完整的全局内存顺序。\n\n### IPI 分发机制\n\n- 根据不同的 `membarrier` 命令类型（如全局、私有、带 rseq 或 sync_core），选择对应的 IPI 处理函数。\n- 使用 `mutex` 保护 IPI 发送过程，避免并发调用导致性能下降或状态竞争。\n- 对于私有命令（如 `PRIVATE_EXPEDITED`），仅向共享同一 `mm_struct` 的 CPU 发送 IPI。\n\n### 状态管理\n\n- 每个 `mm_struct` 包含一个 `membarrier_state` 原子变量，记录该地址空间已注册的 `membarrier` 命令类型。\n- 每个 per-CPU runqueue 也缓存一份 `membarrier_state`，通过 `ipi_sync_rq_state` 保持与 `mm_struct` 同步，加速后续命令的判断。\n- `exec` 时调用 `membarrier_exec_mmap` 重置状态，防止子进程继承父进程的注册状态。\n\n### 条件编译支持\n\n- `CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE`：启用 `SYNC_CORE` 相关命令。\n- `CONFIG_RSEQ`：启用 `RSEQ` 相关命令及 `rseq_preempt` 调用。\n\n## 4. 依赖关系\n\n- **调度子系统（sched）**：依赖 runqueue（`rq`）结构和 CPU 上下文切换逻辑，用于判断当前是否处于用户态及 mm 匹配。\n- **内存管理（mm）**：依赖 `mm_struct` 及其生命周期管理（如 `exec_mmap`、`exit_mm`）。\n- **RSEQ 子系统**：当启用 `CONFIG_RSEQ` 时，调用 `rseq_preempt()` 以维护 restartable sequences 的一致性。\n- **SMP 原语**：依赖 `smp_mb()`、`smp_call_function_many()` 等 SMP 内存屏障和 IPI 接口。\n- **架构支持**：部分命令（如 `SYNC_CORE`）依赖特定架构实现 `sync_core_before_usermode()`。\n\n## 5. 使用场景\n\n- **用户空间无锁编程**：应用程序使用 `membarrier(SYS_MEMBARRIER_CMD_GLOBAL_EXPEDITED)` 替代在每个读线程中插入 `smp_load_acquire()`，简化代码并提升性能。\n- **RSEQ（Restartable Sequences）**：配合 `membarrier(SYS_MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ)` 确保在抢占或迁移后 rseq 区域的原子性。\n- **实时或低延迟系统**：通过私有命令（`PRIVATE_EXPEDITED`）仅对特定进程组施加屏障，减少系统范围开销。\n- **动态代码生成/热更新**：使用 `SYNC_CORE` 命令确保指令缓存一致性，适用于 JIT 编译器等场景。\n- **进程生命周期管理**：在 `exec` 时自动清理 `membarrier` 注册状态，保证新程序映像的干净执行环境。",
      "similarity": 0.565230131149292,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 555,
          "end_line": 587,
          "content": [
            "static int membarrier_get_registrations(void)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint registrations_mask = 0, membarrier_state, i;",
            "\tstatic const int states[] = {",
            "\t\tMEMBARRIER_STATE_GLOBAL_EXPEDITED |",
            "\t\t\tMEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY",
            "\t};",
            "\tstatic const int registration_cmds[] = {",
            "\t\tMEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ",
            "\t};",
            "\tBUILD_BUG_ON(ARRAY_SIZE(states) != ARRAY_SIZE(registration_cmds));",
            "",
            "\tmembarrier_state = atomic_read(&mm->membarrier_state);",
            "\tfor (i = 0; i < ARRAY_SIZE(states); ++i) {",
            "\t\tif (membarrier_state & states[i]) {",
            "\t\t\tregistrations_mask |= registration_cmds[i];",
            "\t\t\tmembarrier_state &= ~states[i];",
            "\t\t}",
            "\t}",
            "\tWARN_ON_ONCE(membarrier_state != 0);",
            "\treturn registrations_mask;",
            "}"
          ],
          "function_name": "membarrier_get_registrations",
          "description": "解析当前进程的内存屏障注册状态，将有效状态转换为对应的注册命令掩码。",
          "similarity": 0.5123358964920044
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 168,
          "end_line": 306,
          "content": [
            "static void ipi_mb(void *info)",
            "{",
            "\tsmp_mb();\t/* IPIs should be serializing but paranoid. */",
            "}",
            "static void ipi_sync_core(void *info)",
            "{",
            "\t/*",
            "\t * The smp_mb() in membarrier after all the IPIs is supposed to",
            "\t * ensure that memory on remote CPUs that occur before the IPI",
            "\t * become visible to membarrier()'s caller -- see scenario B in",
            "\t * the big comment at the top of this file.",
            "\t *",
            "\t * A sync_core() would provide this guarantee, but",
            "\t * sync_core_before_usermode() might end up being deferred until",
            "\t * after membarrier()'s smp_mb().",
            "\t */",
            "\tsmp_mb();\t/* IPIs should be serializing but paranoid. */",
            "",
            "\tsync_core_before_usermode();",
            "}",
            "static void ipi_rseq(void *info)",
            "{",
            "\t/*",
            "\t * Ensure that all stores done by the calling thread are visible",
            "\t * to the current task before the current task resumes.  We could",
            "\t * probably optimize this away on most architectures, but by the",
            "\t * time we've already sent an IPI, the cost of the extra smp_mb()",
            "\t * is negligible.",
            "\t */",
            "\tsmp_mb();",
            "\trseq_preempt(current);",
            "}",
            "static void ipi_sync_rq_state(void *info)",
            "{",
            "\tstruct mm_struct *mm = (struct mm_struct *) info;",
            "",
            "\tif (current->mm != mm)",
            "\t\treturn;",
            "\tthis_cpu_write(runqueues.membarrier_state,",
            "\t\t       atomic_read(&mm->membarrier_state));",
            "\t/*",
            "\t * Issue a memory barrier after setting",
            "\t * MEMBARRIER_STATE_GLOBAL_EXPEDITED in the current runqueue to",
            "\t * guarantee that no memory access following registration is reordered",
            "\t * before registration.",
            "\t */",
            "\tsmp_mb();",
            "}",
            "void membarrier_exec_mmap(struct mm_struct *mm)",
            "{",
            "\t/*",
            "\t * Issue a memory barrier before clearing membarrier_state to",
            "\t * guarantee that no memory access prior to exec is reordered after",
            "\t * clearing this state.",
            "\t */",
            "\tsmp_mb();",
            "\tatomic_set(&mm->membarrier_state, 0);",
            "\t/*",
            "\t * Keep the runqueue membarrier_state in sync with this mm",
            "\t * membarrier_state.",
            "\t */",
            "\tthis_cpu_write(runqueues.membarrier_state, 0);",
            "}",
            "void membarrier_update_current_mm(struct mm_struct *next_mm)",
            "{",
            "\tstruct rq *rq = this_rq();",
            "\tint membarrier_state = 0;",
            "",
            "\tif (next_mm)",
            "\t\tmembarrier_state = atomic_read(&next_mm->membarrier_state);",
            "\tif (READ_ONCE(rq->membarrier_state) == membarrier_state)",
            "\t\treturn;",
            "\tWRITE_ONCE(rq->membarrier_state, membarrier_state);",
            "}",
            "static int membarrier_global_expedited(void)",
            "{",
            "\tint cpu;",
            "\tcpumask_var_t tmpmask;",
            "",
            "\tif (num_online_cpus() == 1)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Matches memory barriers after rq->curr modification in",
            "\t * scheduler.",
            "\t */",
            "\tsmp_mb();\t/* system call entry is not a mb. */",
            "",
            "\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "\trcu_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\t/*",
            "\t\t * Skipping the current CPU is OK even through we can be",
            "\t\t * migrated at any point. The current CPU, at the point",
            "\t\t * where we read raw_smp_processor_id(), is ensured to",
            "\t\t * be in program order with respect to the caller",
            "\t\t * thread. Therefore, we can skip this CPU from the",
            "\t\t * iteration.",
            "\t\t */",
            "\t\tif (cpu == raw_smp_processor_id())",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!(READ_ONCE(cpu_rq(cpu)->membarrier_state) &",
            "\t\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * Skip the CPU if it runs a kernel thread which is not using",
            "\t\t * a task mm.",
            "\t\t */",
            "\t\tp = rcu_dereference(cpu_rq(cpu)->curr);",
            "\t\tif (!p->mm)",
            "\t\t\tcontinue;",
            "",
            "\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tpreempt_disable();",
            "\tsmp_call_function_many(tmpmask, ipi_mb, NULL, 1);",
            "\tpreempt_enable();",
            "",
            "\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\t/*",
            "\t * Memory barrier on the caller thread _after_ we finished",
            "\t * waiting for the last IPI. Matches memory barriers before",
            "\t * rq->curr modification in scheduler.",
            "\t */",
            "\tsmp_mb();\t/* exit from system call is not a mb */",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ipi_mb, ipi_sync_core, ipi_rseq, ipi_sync_rq_state, membarrier_exec_mmap, membarrier_update_current_mm, membarrier_global_expedited",
          "description": "实现多种IPI处理函数及全局快速内存屏障逻辑，通过跨CPU调用来确保内存访问顺序一致性。",
          "similarity": 0.48647406697273254
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 1,
          "end_line": 167,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Copyright (C) 2010-2017 Mathieu Desnoyers <mathieu.desnoyers@efficios.com>",
            " *",
            " * membarrier system call",
            " */",
            "",
            "/*",
            " * For documentation purposes, here are some membarrier ordering",
            " * scenarios to keep in mind:",
            " *",
            " * A) Userspace thread execution after IPI vs membarrier's memory",
            " *    barrier before sending the IPI",
            " *",
            " * Userspace variables:",
            " *",
            " * int x = 0, y = 0;",
            " *",
            " * The memory barrier at the start of membarrier() on CPU0 is necessary in",
            " * order to enforce the guarantee that any writes occurring on CPU0 before",
            " * the membarrier() is executed will be visible to any code executing on",
            " * CPU1 after the IPI-induced memory barrier:",
            " *",
            " *         CPU0                              CPU1",
            " *",
            " *         x = 1",
            " *         membarrier():",
            " *           a: smp_mb()",
            " *           b: send IPI                       IPI-induced mb",
            " *           c: smp_mb()",
            " *         r2 = y",
            " *                                           y = 1",
            " *                                           barrier()",
            " *                                           r1 = x",
            " *",
            " *                     BUG_ON(r1 == 0 && r2 == 0)",
            " *",
            " * The write to y and load from x by CPU1 are unordered by the hardware,",
            " * so it's possible to have \"r1 = x\" reordered before \"y = 1\" at any",
            " * point after (b).  If the memory barrier at (a) is omitted, then \"x = 1\"",
            " * can be reordered after (a) (although not after (c)), so we get r1 == 0",
            " * and r2 == 0.  This violates the guarantee that membarrier() is",
            " * supposed by provide.",
            " *",
            " * The timing of the memory barrier at (a) has to ensure that it executes",
            " * before the IPI-induced memory barrier on CPU1.",
            " *",
            " * B) Userspace thread execution before IPI vs membarrier's memory",
            " *    barrier after completing the IPI",
            " *",
            " * Userspace variables:",
            " *",
            " * int x = 0, y = 0;",
            " *",
            " * The memory barrier at the end of membarrier() on CPU0 is necessary in",
            " * order to enforce the guarantee that any writes occurring on CPU1 before",
            " * the membarrier() is executed will be visible to any code executing on",
            " * CPU0 after the membarrier():",
            " *",
            " *         CPU0                              CPU1",
            " *",
            " *                                           x = 1",
            " *                                           barrier()",
            " *                                           y = 1",
            " *         r2 = y",
            " *         membarrier():",
            " *           a: smp_mb()",
            " *           b: send IPI                       IPI-induced mb",
            " *           c: smp_mb()",
            " *         r1 = x",
            " *         BUG_ON(r1 == 0 && r2 == 1)",
            " *",
            " * The writes to x and y are unordered by the hardware, so it's possible to",
            " * have \"r2 = 1\" even though the write to x doesn't execute until (b).  If",
            " * the memory barrier at (c) is omitted then \"r1 = x\" can be reordered",
            " * before (b) (although not before (a)), so we get \"r1 = 0\".  This violates",
            " * the guarantee that membarrier() is supposed to provide.",
            " *",
            " * The timing of the memory barrier at (c) has to ensure that it executes",
            " * after the IPI-induced memory barrier on CPU1.",
            " *",
            " * C) Scheduling userspace thread -> kthread -> userspace thread vs membarrier",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *           a: smp_mb()",
            " *                                           d: switch to kthread (includes mb)",
            " *           b: read rq->curr->mm == NULL",
            " *                                           e: switch to user (includes mb)",
            " *           c: smp_mb()",
            " *",
            " * Using the scenario from (A), we can show that (a) needs to be paired",
            " * with (e). Using the scenario from (B), we can show that (c) needs to",
            " * be paired with (d).",
            " *",
            " * D) exit_mm vs membarrier",
            " *",
            " * Two thread groups are created, A and B.  Thread group B is created by",
            " * issuing clone from group A with flag CLONE_VM set, but not CLONE_THREAD.",
            " * Let's assume we have a single thread within each thread group (Thread A",
            " * and Thread B).  Thread A runs on CPU0, Thread B runs on CPU1.",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *             a: smp_mb()",
            " *                                           exit_mm():",
            " *                                             d: smp_mb()",
            " *                                             e: current->mm = NULL",
            " *             b: read rq->curr->mm == NULL",
            " *             c: smp_mb()",
            " *",
            " * Using scenario (B), we can show that (c) needs to be paired with (d).",
            " *",
            " * E) kthread_{use,unuse}_mm vs membarrier",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *           a: smp_mb()",
            " *                                           kthread_unuse_mm()",
            " *                                             d: smp_mb()",
            " *                                             e: current->mm = NULL",
            " *           b: read rq->curr->mm == NULL",
            " *                                           kthread_use_mm()",
            " *                                             f: current->mm = mm",
            " *                                             g: smp_mb()",
            " *           c: smp_mb()",
            " *",
            " * Using the scenario from (A), we can show that (a) needs to be paired",
            " * with (g). Using the scenario from (B), we can show that (c) needs to",
            " * be paired with (d).",
            " */",
            "",
            "/*",
            " * Bitmask made from a \"or\" of all commands within enum membarrier_cmd,",
            " * except MEMBARRIER_CMD_QUERY.",
            " */",
            "#ifdef CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t\t\t\\",
            "\t(MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE)",
            "#else",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t0",
            "#endif",
            "",
            "#ifdef CONFIG_RSEQ",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t\t\\",
            "\t(MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ)",
            "#else",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t0",
            "#endif",
            "",
            "#define MEMBARRIER_CMD_BITMASK\t\t\t\t\t\t\\",
            "\t(MEMBARRIER_CMD_GLOBAL | MEMBARRIER_CMD_GLOBAL_EXPEDITED\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED\t\t\t\\",
            "\t| MEMBARRIER_CMD_PRIVATE_EXPEDITED\t\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED\t\t\t\\",
            "\t| MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t\t\\",
            "\t| MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t\t\t\\",
            "\t| MEMBARRIER_CMD_GET_REGISTRATIONS)",
            "",
            "static DEFINE_MUTEX(membarrier_ipi_mutex);",
            "#define SERIALIZE_IPI() guard(mutex)(&membarrier_ipi_mutex)",
            ""
          ],
          "function_name": null,
          "description": "定义内存屏障命令位掩码和互斥锁，用于协调多处理器间内存顺序保证。",
          "similarity": 0.4732052981853485
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 436,
          "end_line": 551,
          "content": [
            "static int sync_runqueues_membarrier_state(struct mm_struct *mm)",
            "{",
            "\tint membarrier_state = atomic_read(&mm->membarrier_state);",
            "\tcpumask_var_t tmpmask;",
            "\tint cpu;",
            "",
            "\tif (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1) {",
            "\t\tthis_cpu_write(runqueues.membarrier_state, membarrier_state);",
            "",
            "\t\t/*",
            "\t\t * For single mm user, we can simply issue a memory barrier",
            "\t\t * after setting MEMBARRIER_STATE_GLOBAL_EXPEDITED in the",
            "\t\t * mm and in the current runqueue to guarantee that no memory",
            "\t\t * access following registration is reordered before",
            "\t\t * registration.",
            "\t\t */",
            "\t\tsmp_mb();",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * For mm with multiple users, we need to ensure all future",
            "\t * scheduler executions will observe @mm's new membarrier",
            "\t * state.",
            "\t */",
            "\tsynchronize_rcu();",
            "",
            "\t/*",
            "\t * For each cpu runqueue, if the task's mm match @mm, ensure that all",
            "\t * @mm's membarrier state set bits are also set in the runqueue's",
            "\t * membarrier state. This ensures that a runqueue scheduling",
            "\t * between threads which are users of @mm has its membarrier state",
            "\t * updated.",
            "\t */",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "\trcu_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct rq *rq = cpu_rq(cpu);",
            "\t\tstruct task_struct *p;",
            "",
            "\t\tp = rcu_dereference(rq->curr);",
            "\t\tif (p && p->mm == mm)",
            "\t\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\ton_each_cpu_mask(tmpmask, ipi_sync_rq_state, mm, true);",
            "",
            "\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\treturn 0;",
            "}",
            "static int membarrier_register_global_expedited(void)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint ret;",
            "",
            "\tif (atomic_read(&mm->membarrier_state) &",
            "\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)",
            "\t\treturn 0;",
            "\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);",
            "\tret = sync_runqueues_membarrier_state(mm);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,",
            "\t\t  &mm->membarrier_state);",
            "",
            "\treturn 0;",
            "}",
            "static int membarrier_register_private_expedited(int flags)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint ready_state = MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY,",
            "\t    set_state = MEMBARRIER_STATE_PRIVATE_EXPEDITED,",
            "\t    ret;",
            "",
            "\tif (flags == MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))",
            "\t\t\treturn -EINVAL;",
            "\t\tready_state =",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY;",
            "\t} else if (flags == MEMBARRIER_FLAG_RSEQ) {",
            "\t\tif (!IS_ENABLED(CONFIG_RSEQ))",
            "\t\t\treturn -EINVAL;",
            "\t\tready_state =",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY;",
            "\t} else {",
            "\t\tWARN_ON_ONCE(flags);",
            "\t}",
            "",
            "\t/*",
            "\t * We need to consider threads belonging to different thread",
            "\t * groups, which use the same mm. (CLONE_VM but not",
            "\t * CLONE_THREAD).",
            "\t */",
            "\tif ((atomic_read(&mm->membarrier_state) & ready_state) == ready_state)",
            "\t\treturn 0;",
            "\tif (flags & MEMBARRIER_FLAG_SYNC_CORE)",
            "\t\tset_state |= MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE;",
            "\tif (flags & MEMBARRIER_FLAG_RSEQ)",
            "\t\tset_state |= MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ;",
            "\tatomic_or(set_state, &mm->membarrier_state);",
            "\tret = sync_runqueues_membarrier_state(mm);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tatomic_or(ready_state, &mm->membarrier_state);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sync_runqueues_membarrier_state, membarrier_register_global_expedited, membarrier_register_private_expedited",
          "description": "同步运行队列内存屏障状态，通过RCU和IPI确保多用户场景下状态传播的正确性。",
          "similarity": 0.4617185890674591
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 314,
          "end_line": 434,
          "content": [
            "static int membarrier_private_expedited(int flags, int cpu_id)",
            "{",
            "\tcpumask_var_t tmpmask;",
            "\tstruct mm_struct *mm = current->mm;",
            "\tsmp_call_func_t ipi_func = ipi_mb;",
            "",
            "\tif (flags == MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY))",
            "\t\t\treturn -EPERM;",
            "\t\tipi_func = ipi_sync_core;",
            "\t\tprepare_sync_core_cmd(mm);",
            "\t} else if (flags == MEMBARRIER_FLAG_RSEQ) {",
            "\t\tif (!IS_ENABLED(CONFIG_RSEQ))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY))",
            "\t\t\treturn -EPERM;",
            "\t\tipi_func = ipi_rseq;",
            "\t} else {",
            "\t\tWARN_ON_ONCE(flags);",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY))",
            "\t\t\treturn -EPERM;",
            "\t}",
            "",
            "\tif (flags != MEMBARRIER_FLAG_SYNC_CORE &&",
            "\t    (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Matches memory barriers after rq->curr modification in",
            "\t * scheduler.",
            "\t *",
            "\t * On RISC-V, this barrier pairing is also needed for the",
            "\t * SYNC_CORE command when switching between processes, cf.",
            "\t * the inline comments in membarrier_arch_switch_mm().",
            "\t */",
            "\tsmp_mb();\t/* system call entry is not a mb. */",
            "",
            "\tif (cpu_id < 0 && !zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "",
            "\tif (cpu_id >= 0) {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\tif (cpu_id >= nr_cpu_ids || !cpu_online(cpu_id))",
            "\t\t\tgoto out;",
            "\t\trcu_read_lock();",
            "\t\tp = rcu_dereference(cpu_rq(cpu_id)->curr);",
            "\t\tif (!p || p->mm != mm) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tint cpu;",
            "",
            "\t\trcu_read_lock();",
            "\t\tfor_each_online_cpu(cpu) {",
            "\t\t\tstruct task_struct *p;",
            "",
            "\t\t\tp = rcu_dereference(cpu_rq(cpu)->curr);",
            "\t\t\tif (p && p->mm == mm)",
            "\t\t\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t}",
            "",
            "\tif (cpu_id >= 0) {",
            "\t\t/*",
            "\t\t * smp_call_function_single() will call ipi_func() if cpu_id",
            "\t\t * is the calling CPU.",
            "\t\t */",
            "\t\tsmp_call_function_single(cpu_id, ipi_func, NULL, 1);",
            "\t} else {",
            "\t\t/*",
            "\t\t * For regular membarrier, we can save a few cycles by",
            "\t\t * skipping the current cpu -- we're about to do smp_mb()",
            "\t\t * below, and if we migrate to a different cpu, this cpu",
            "\t\t * and the new cpu will execute a full barrier in the",
            "\t\t * scheduler.",
            "\t\t *",
            "\t\t * For SYNC_CORE, we do need a barrier on the current cpu --",
            "\t\t * otherwise, if we are migrated and replaced by a different",
            "\t\t * task in the same mm just before, during, or after",
            "\t\t * membarrier, we will end up with some thread in the mm",
            "\t\t * running without a core sync.",
            "\t\t *",
            "\t\t * For RSEQ, don't rseq_preempt() the caller.  User code",
            "\t\t * is not supposed to issue syscalls at all from inside an",
            "\t\t * rseq critical section.",
            "\t\t */",
            "\t\tif (flags != MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\t\tpreempt_disable();",
            "\t\t\tsmp_call_function_many(tmpmask, ipi_func, NULL, true);",
            "\t\t\tpreempt_enable();",
            "\t\t} else {",
            "\t\t\ton_each_cpu_mask(tmpmask, ipi_func, NULL, true);",
            "\t\t}",
            "\t}",
            "",
            "out:",
            "\tif (cpu_id < 0)",
            "\t\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\t/*",
            "\t * Memory barrier on the caller thread _after_ we finished",
            "\t * waiting for the last IPI. Matches memory barriers before",
            "\t * rq->curr modification in scheduler.",
            "\t */",
            "\tsmp_mb();\t/* exit from system call is not a mb */",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "membarrier_private_expedited",
          "description": "处理私有快速内存屏障，根据配置标志选择不同同步方式并验证状态有效性。",
          "similarity": 0.44377535581588745
        }
      ]
    },
    {
      "source_file": "kernel/workqueue_internal.h",
      "md_summary": "> 自动生成时间: 2025-10-25 17:54:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workqueue_internal.h`\n\n---\n\n# `workqueue_internal.h` 技术文档\n\n## 1. 文件概述\n\n`workqueue_internal.h` 是 Linux 内核工作队列（workqueue）子系统的内部头文件，仅限工作队列核心代码及内核关键子系统（如 `async` 和调度器）包含使用。该文件定义了工作队列内部使用的 `struct worker` 数据结构，并声明了调度器与工作队列交互所需的钩子函数。其主要作用是封装工作线程（worker）的内部状态和行为，为并发管理型工作队列（Concurrency Managed Workqueue, CMWQ）提供底层支持。\n\n## 2. 核心功能\n\n### 数据结构\n\n- **`struct worker`**  \n  表示一个工作队列的工作线程（worker），包含其运行状态、当前处理的工作项、所属线程池、调度信息等。关键字段包括：\n  - `entry` / `hentry`：联合体，用于在空闲时挂入空闲链表，繁忙时挂入哈希表。\n  - `current_work` / `current_func`：当前正在执行的工作项及其回调函数。\n  - `current_pwq`：当前工作项所属的 `pool_workqueue`。\n  - `sleeping`：标识该 worker 是否处于睡眠状态。\n  - `scheduled`：已调度但尚未执行的工作项链表。\n  - `task`：对应的内核线程（kthread）任务结构。\n  - `pool`：所属的 `worker_pool`。\n  - `flags` / `id`：worker 的标志位和唯一标识。\n  - `desc`：用于调试的描述字符串（通过 `work_set_desc()` 设置）。\n  - `rescue_wq`：仅用于 rescuer worker，指向需要被救援的工作队列。\n\n- **内联函数**\n  - **`current_wq_worker()`**：判断当前执行上下文是否为工作队列 worker 线程。若是，则返回对应的 `struct worker` 指针；否则返回 `NULL`。通过检查 `current->flags & PF_WQ_WORKER` 并调用 `kthread_data()` 实现。\n\n### 函数声明（调度器钩子）\n\n- **`wq_worker_running(struct task_struct *task)`**  \n  通知工作队列子系统：指定 worker 线程已开始运行。\n\n- **`wq_worker_sleeping(struct task_struct *task)`**  \n  通知工作队列子系统：指定 worker 线程即将进入睡眠状态。\n\n- **`wq_worker_tick(struct task_struct *task)`**  \n  由调度器周期性调用，用于更新 worker 的运行时统计信息（如 CPU 时间）。\n\n- **`wq_worker_last_func(struct task_struct *task)`**  \n  返回指定 worker 线程最近执行的工作函数指针，供调度器或调试使用。\n\n## 3. 关键实现\n\n- **Worker 状态管理**  \n  `struct worker` 使用联合体 `entry/hentry` 实现状态复用：空闲时通过 `entry` 挂入 `worker_pool` 的空闲链表；执行工作时通过 `hentry` 挂入 busy 哈希表，便于快速查找和管理。\n\n- **并发管理支持**  \n  通过 `sleeping` 字段和调度器钩子函数（如 `wq_worker_sleeping`/`wq_worker_running`），工作队列子系统可精确跟踪 worker 的运行状态，从而动态调整线程池大小，实现高效的并发控制。\n\n- **调试支持**  \n  `desc` 字段允许通过 `work_set_desc()` 为工作项设置可读描述，在内核崩溃（WARN/BUG/panic）或 SysRq 调试时输出，便于定位问题。\n\n- **Rescuer 机制**  \n  `rescue_wq` 字段专用于 rescuer worker（用于处理内存压力下无法创建新 worker 的紧急情况），指向需要被“救援”的工作队列。\n\n- **锁注释约定**  \n  结构体字段注释中的字母（如 `L`, `K`, `I`, `A`, `S`）表示访问该字段所需的锁或上下文，具体含义需参考 `workqueue.c` 中的说明（例如 `L` 表示 pool->lock，`K` 表示需要关闭内核抢占等）。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/workqueue.h>`：提供工作队列公共接口和基础类型（如 `work_struct`、`work_func_t`）。\n  - `<linux/kthread.h>`：提供内核线程相关功能（如 `kthread_data()`）。\n  - `<linux/preempt.h>`：用于内核抢占控制。\n\n- **模块依赖**：\n  - **`workqueue.c`**：工作队列核心实现，定义了 `struct worker` 的操作逻辑。\n  - **`kernel/async.c`**：异步初始化框架，使用内部 worker 结构。\n  - **`kernel/sched/`**：调度器子系统，调用 `wq_worker_*` 钩子函数以集成工作队列状态管理。\n\n## 5. 使用场景\n\n- **工作队列执行路径**  \n  当工作项被调度执行时，内核从 `worker_pool` 中唤醒或创建 `worker`，通过 `current_wq_worker()` 获取当前上下文的 worker 结构，并更新其状态字段（如 `current_work`、`last_func`）。\n\n- **调度器集成**  \n  调度器在 worker 线程状态切换（运行/睡眠）或时钟滴答（tick）时调用相应钩子，使工作队列子系统能动态管理线程池并发度。\n\n- **内存压力恢复**  \n  在内存紧张无法创建新 worker 时，rescuer worker 被激活，通过 `rescue_wq` 字段处理阻塞的工作队列。\n\n- **内核调试与诊断**  \n  在系统崩溃或通过 SysRq 触发任务转储时，`desc` 字段提供工作项的语义信息，辅助开发者分析死锁或性能问题。",
      "similarity": 0.5652053952217102,
      "chunks": []
    }
  ]
}