{
  "query": "环形缓冲区实现原理",
  "timestamp": "2025-12-25 23:48:56",
  "retrieved_files": [
    {
      "source_file": "kernel/bpf/ringbuf.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:29:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\ringbuf.c`\n\n---\n\n# `bpf/ringbuf.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/ringbuf.c` 实现了 BPF（Berkeley Packet Filter）子系统中的**环形缓冲区（Ring Buffer）**机制，用于在内核与用户空间之间高效、安全地传递数据。该机制支持两种生产者模式：**内核生产者**（如 BPF 程序）和**用户空间生产者**，并提供内存映射（`mmap`）、等待队列通知、并发控制等核心功能，是 BPF 数据输出（如 perf event 替代方案）的关键基础设施。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_ringbuf`**  \n  环形缓冲区的核心结构体，包含：\n  - `waitq`：等待队列，用于通知用户空间有新数据\n  - `work`：IRQ 工作项，用于异步唤醒等待队列\n  - `mask`：环形缓冲区大小掩码（`data_sz - 1`），用于快速取模\n  - `pages` / `nr_pages`：物理页数组，支持双映射\n  - `spinlock`：用于内核生产者的自旋锁（SMP 对齐）\n  - `busy`：原子变量，用于用户空间生产者的互斥访问（避免持有自旋锁过久）\n  - `consumer_pos` / `producer_pos` / `pending_pos`：消费者、生产者和待提交位置（各自独占一页，支持不同 mmap 权限）\n  - `data[]`：实际数据存储区域（页对齐）\n\n- **`struct bpf_ringbuf_map`**  \n  封装标准 `bpf_map`，关联一个 `bpf_ringbuf` 实例。\n\n- **`struct bpf_ringbuf_hdr`**  \n  8 字节记录头，包含：\n  - `len`：记录有效载荷长度\n  - `pg_off`：记录在页内的偏移（用于跨页处理）\n\n### 主要函数\n\n- **`bpf_ringbuf_area_alloc()`**  \n  分配并初始化环形缓冲区的虚拟内存区域，采用**双映射数据页**技术简化环绕处理。\n\n- **`bpf_ringbuf_alloc()`**  \n  初始化 `bpf_ringbuf` 结构体，设置锁、等待队列、IRQ 工作项及初始位置。\n\n- **`bpf_ringbuf_free()`**  \n  释放环形缓冲区占用的虚拟内存和物理页。\n\n- **`ringbuf_map_alloc()`**  \n  BPF map 分配器回调，验证参数并创建 `bpf_ringbuf_map`。\n\n- **`ringbuf_map_free()`**  \n  BPF map 释放器回调，清理资源。\n\n- **`ringbuf_map_*_elem()` / `ringbuf_map_get_next_key()`**  \n  禁用标准 map 操作（返回 `-ENOTSUPP`），因为 ringbuf 不支持键值操作。\n\n- **`bpf_ringbuf_notify()`**  \n  IRQ 工作回调，唤醒所有等待数据的用户进程。\n\n## 3. 关键实现\n\n### 双映射数据页（Double-Mapped Data Pages）\n\n为简化环形缓冲区**环绕（wrap-around）**时的数据读取逻辑，数据页被**连续映射两次**：\n```\n[meta pages][data pages][data pages (same as first copy)]\n```\n当读取跨越缓冲区末尾时，可直接线性读取第二份映射，无需特殊处理。此设计同时适用于内核和用户空间 `mmap`。\n\n### 权限隔离与安全\n\n- **`consumer_pos` 和 `producer_pos` 各占独立页**，允许通过 `mmap` 设置不同权限：\n  - **内核生产者模式**：`producer_pos` 和数据页对用户空间为**只读**，防止篡改。\n  - **用户空间生产者模式**：仅 `consumer_pos` 对用户空间为**只读**，内核需严格验证用户提交的记录。\n\n### 并发控制策略\n\n- **内核生产者**：使用 `raw_spinlock_t` 保证多生产者安全。\n- **用户空间生产者**：使用 `atomic_t busy` 原子变量，避免在 BPF 程序回调期间长期持有 IRQ 自旋锁（可能导致死锁或延迟）。若 `busy` 被占用，`__bpf_user_ringbuf_peek()` 返回 `-EBUSY`。\n\n### 内存布局与对齐\n\n- 非 `mmap` 部分（`waitq` 到 `pending_pos`）大小由 `RINGBUF_PGOFF` 定义。\n- `consumer_pos`、`producer_pos` 和 `data` 均按 `PAGE_SIZE` 对齐，确保可独立映射。\n- 总元数据页数：`RINGBUF_NR_META_PAGES = RINGBUF_PGOFF + 2`（含 consumer/producer 页）。\n\n### 大小限制\n\n- 最大记录大小：`RINGBUF_MAX_RECORD_SZ = UINT_MAX / 4`（约 1GB）。\n- 最大缓冲区大小受 `bpf_ringbuf_hdr.pg_off`（32 位页偏移）限制，理论最大约 **64GB**。\n\n## 4. 依赖关系\n\n- **BPF 子系统**：依赖 `bpf_map` 基础设施（`bpf_map_area_alloc/free`、`bpf_map_init_from_attr`）。\n- **内存管理**：使用 `alloc_pages_node`、`vmap`/`vunmap`、`__free_page` 管理物理页和虚拟映射。\n- **同步机制**：依赖 `wait_queue`、`irq_work`、`raw_spinlock` 和 `atomic_t`。\n- **BTF（BPF Type Format）**：包含 BTF 相关头文件，可能用于未来类型验证（当前未直接使用）。\n- **用户 API**：与 `uapi/linux/bpf.h` 中的 `BPF_F_NUMA_NODE` 等标志交互。\n\n## 5. 使用场景\n\n- **BPF 程序输出数据**：替代 `bpf_perf_event_output()`，提供更低开销、更高吞吐的内核到用户空间数据通道。\n- **用户空间主动提交数据**：允许用户程序通过 ringbuf 向内核提交样本（需内核验证）。\n- **实时监控与追踪**：用于 eBPF 监控工具（如 `bpftrace`、`libbpf` 应用）高效收集内核事件。\n- **NUMA 感知分配**：支持通过 `BPF_F_NUMA_NODE` 标志在指定 NUMA 节点分配内存，优化性能。",
      "similarity": 0.5951293110847473,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 150,
          "end_line": 258,
          "content": [
            "static void bpf_ringbuf_notify(struct irq_work *work)",
            "{",
            "\tstruct bpf_ringbuf *rb = container_of(work, struct bpf_ringbuf, work);",
            "",
            "\twake_up_all(&rb->waitq);",
            "}",
            "static void bpf_ringbuf_free(struct bpf_ringbuf *rb)",
            "{",
            "\t/* copy pages pointer and nr_pages to local variable, as we are going",
            "\t * to unmap rb itself with vunmap() below",
            "\t */",
            "\tstruct page **pages = rb->pages;",
            "\tint i, nr_pages = rb->nr_pages;",
            "",
            "\tvunmap(rb);",
            "\tfor (i = 0; i < nr_pages; i++)",
            "\t\t__free_page(pages[i]);",
            "\tbpf_map_area_free(pages);",
            "}",
            "static void ringbuf_map_free(struct bpf_map *map)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tbpf_ringbuf_free(rb_map->rb);",
            "\tbpf_map_area_free(rb_map);",
            "}",
            "static long ringbuf_map_update_elem(struct bpf_map *map, void *key, void *value,",
            "\t\t\t\t    u64 flags)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static long ringbuf_map_delete_elem(struct bpf_map *map, void *key)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static int ringbuf_map_get_next_key(struct bpf_map *map, void *key,",
            "\t\t\t\t    void *next_key)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static int ringbuf_map_mmap_kern(struct bpf_map *map, struct vm_area_struct *vma)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "",
            "\tif (vma->vm_flags & VM_WRITE) {",
            "\t\t/* allow writable mapping for the consumer_pos only */",
            "\t\tif (vma->vm_pgoff != 0 || vma->vm_end - vma->vm_start != PAGE_SIZE)",
            "\t\t\treturn -EPERM;",
            "\t}",
            "\t/* remap_vmalloc_range() checks size and offset constraints */",
            "\treturn remap_vmalloc_range(vma, rb_map->rb,",
            "\t\t\t\t   vma->vm_pgoff + RINGBUF_PGOFF);",
            "}",
            "static int ringbuf_map_mmap_user(struct bpf_map *map, struct vm_area_struct *vma)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "",
            "\tif (vma->vm_flags & VM_WRITE) {",
            "\t\tif (vma->vm_pgoff == 0)",
            "\t\t\t/* Disallow writable mappings to the consumer pointer,",
            "\t\t\t * and allow writable mappings to both the producer",
            "\t\t\t * position, and the ring buffer data itself.",
            "\t\t\t */",
            "\t\t\treturn -EPERM;",
            "\t}",
            "\t/* remap_vmalloc_range() checks size and offset constraints */",
            "\treturn remap_vmalloc_range(vma, rb_map->rb, vma->vm_pgoff + RINGBUF_PGOFF);",
            "}",
            "static unsigned long ringbuf_avail_data_sz(struct bpf_ringbuf *rb)",
            "{",
            "\tunsigned long cons_pos, prod_pos;",
            "",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos);",
            "\tprod_pos = smp_load_acquire(&rb->producer_pos);",
            "\treturn prod_pos - cons_pos;",
            "}",
            "static u32 ringbuf_total_data_sz(const struct bpf_ringbuf *rb)",
            "{",
            "\treturn rb->mask + 1;",
            "}",
            "static __poll_t ringbuf_map_poll_kern(struct bpf_map *map, struct file *filp,",
            "\t\t\t\t      struct poll_table_struct *pts)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tpoll_wait(filp, &rb_map->rb->waitq, pts);",
            "",
            "\tif (ringbuf_avail_data_sz(rb_map->rb))",
            "\t\treturn EPOLLIN | EPOLLRDNORM;",
            "\treturn 0;",
            "}",
            "static __poll_t ringbuf_map_poll_user(struct bpf_map *map, struct file *filp,",
            "\t\t\t\t      struct poll_table_struct *pts)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tpoll_wait(filp, &rb_map->rb->waitq, pts);",
            "",
            "\tif (ringbuf_avail_data_sz(rb_map->rb) < ringbuf_total_data_sz(rb_map->rb))",
            "\t\treturn EPOLLOUT | EPOLLWRNORM;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "bpf_ringbuf_notify, bpf_ringbuf_free, ringbuf_map_free, ringbuf_map_update_elem, ringbuf_map_delete_elem, ringbuf_map_get_next_key, ringbuf_map_mmap_kern, ringbuf_map_mmap_user, ringbuf_avail_data_sz, ringbuf_total_data_sz, ringbuf_map_poll_kern, ringbuf_map_poll_user",
          "description": "实现了环形缓冲区的事件通知、资源释放、内存映射控制及I/O监控功能。包含针对用户态和内核态的差异化mmap处理逻辑，通过spinlock和atomic_t实现并发控制，提供poll接口检测缓冲区可用数据状态。",
          "similarity": 0.6460150480270386
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 335,
          "end_line": 447,
          "content": [
            "static u64 ringbuf_map_mem_usage(const struct bpf_map *map)",
            "{",
            "\tstruct bpf_ringbuf *rb;",
            "\tint nr_data_pages;",
            "\tint nr_meta_pages;",
            "\tu64 usage = sizeof(struct bpf_ringbuf_map);",
            "",
            "\trb = container_of(map, struct bpf_ringbuf_map, map)->rb;",
            "\tusage += (u64)rb->nr_pages << PAGE_SHIFT;",
            "\tnr_meta_pages = RINGBUF_NR_META_PAGES;",
            "\tnr_data_pages = map->max_entries >> PAGE_SHIFT;",
            "\tusage += (nr_meta_pages + 2 * nr_data_pages) * sizeof(struct page *);",
            "\treturn usage;",
            "}",
            "static size_t bpf_ringbuf_rec_pg_off(struct bpf_ringbuf *rb,",
            "\t\t\t\t     struct bpf_ringbuf_hdr *hdr)",
            "{",
            "\treturn ((void *)hdr - (void *)rb) >> PAGE_SHIFT;",
            "}",
            "static void bpf_ringbuf_commit(void *sample, u64 flags, bool discard)",
            "{",
            "\tunsigned long rec_pos, cons_pos;",
            "\tstruct bpf_ringbuf_hdr *hdr;",
            "\tstruct bpf_ringbuf *rb;",
            "\tu32 new_len;",
            "",
            "\thdr = sample - BPF_RINGBUF_HDR_SZ;",
            "\trb = bpf_ringbuf_restore_from_rec(hdr);",
            "\tnew_len = hdr->len ^ BPF_RINGBUF_BUSY_BIT;",
            "\tif (discard)",
            "\t\tnew_len |= BPF_RINGBUF_DISCARD_BIT;",
            "",
            "\t/* update record header with correct final size prefix */",
            "\txchg(&hdr->len, new_len);",
            "",
            "\t/* if consumer caught up and is waiting for our record, notify about",
            "\t * new data availability",
            "\t */",
            "\trec_pos = (void *)hdr - (void *)rb->data;",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos) & rb->mask;",
            "",
            "\tif (flags & BPF_RB_FORCE_WAKEUP)",
            "\t\tirq_work_queue(&rb->work);",
            "\telse if (cons_pos == rec_pos && !(flags & BPF_RB_NO_WAKEUP))",
            "\t\tirq_work_queue(&rb->work);",
            "}",
            "static int __bpf_user_ringbuf_peek(struct bpf_ringbuf *rb, void **sample, u32 *size)",
            "{",
            "\tint err;",
            "\tu32 hdr_len, sample_len, total_len, flags, *hdr;",
            "\tu64 cons_pos, prod_pos;",
            "",
            "\t/* Synchronizes with smp_store_release() in user-space producer. */",
            "\tprod_pos = smp_load_acquire(&rb->producer_pos);",
            "\tif (prod_pos % 8)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Synchronizes with smp_store_release() in __bpf_user_ringbuf_sample_release() */",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos);",
            "\tif (cons_pos >= prod_pos)",
            "\t\treturn -ENODATA;",
            "",
            "\thdr = (u32 *)((uintptr_t)rb->data + (uintptr_t)(cons_pos & rb->mask));",
            "\t/* Synchronizes with smp_store_release() in user-space producer. */",
            "\thdr_len = smp_load_acquire(hdr);",
            "\tflags = hdr_len & (BPF_RINGBUF_BUSY_BIT | BPF_RINGBUF_DISCARD_BIT);",
            "\tsample_len = hdr_len & ~flags;",
            "\ttotal_len = round_up(sample_len + BPF_RINGBUF_HDR_SZ, 8);",
            "",
            "\t/* The sample must fit within the region advertised by the producer position. */",
            "\tif (total_len > prod_pos - cons_pos)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* The sample must fit within the data region of the ring buffer. */",
            "\tif (total_len > ringbuf_total_data_sz(rb))",
            "\t\treturn -E2BIG;",
            "",
            "\t/* The sample must fit into a struct bpf_dynptr. */",
            "\terr = bpf_dynptr_check_size(sample_len);",
            "\tif (err)",
            "\t\treturn -E2BIG;",
            "",
            "\tif (flags & BPF_RINGBUF_DISCARD_BIT) {",
            "\t\t/* If the discard bit is set, the sample should be skipped.",
            "\t\t *",
            "\t\t * Update the consumer pos, and return -EAGAIN so the caller",
            "\t\t * knows to skip this sample and try to read the next one.",
            "\t\t */",
            "\t\tsmp_store_release(&rb->consumer_pos, cons_pos + total_len);",
            "\t\treturn -EAGAIN;",
            "\t}",
            "",
            "\tif (flags & BPF_RINGBUF_BUSY_BIT)",
            "\t\treturn -ENODATA;",
            "",
            "\t*sample = (void *)((uintptr_t)rb->data +",
            "\t\t\t   (uintptr_t)((cons_pos + BPF_RINGBUF_HDR_SZ) & rb->mask));",
            "\t*size = sample_len;",
            "\treturn 0;",
            "}",
            "static void __bpf_user_ringbuf_sample_release(struct bpf_ringbuf *rb, size_t size, u64 flags)",
            "{",
            "\tu64 consumer_pos;",
            "\tu32 rounded_size = round_up(size + BPF_RINGBUF_HDR_SZ, 8);",
            "",
            "\t/* Using smp_load_acquire() is unnecessary here, as the busy-bit",
            "\t * prevents another task from writing to consumer_pos after it was read",
            "\t * by this task with smp_load_acquire() in __bpf_user_ringbuf_peek().",
            "\t */",
            "\tconsumer_pos = rb->consumer_pos;",
            "\t /* Synchronizes with smp_load_acquire() in user-space producer. */",
            "\tsmp_store_release(&rb->consumer_pos, consumer_pos + rounded_size);",
            "}"
          ],
          "function_name": "ringbuf_map_mem_usage, bpf_ringbuf_rec_pg_off, bpf_ringbuf_commit, __bpf_user_ringbuf_peek, __bpf_user_ringbuf_sample_release",
          "description": "提供了环形缓冲区的内存占用统计、记录位置转换、样本提交及消费操作。包含用户态生产者与消费者的同步机制，通过忙位防止竞态条件，确保样本数据完整性校验和消费进度更新的有序性。",
          "similarity": 0.6176662445068359
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 1,
          "end_line": 149,
          "content": [
            "#include <linux/bpf.h>",
            "#include <linux/btf.h>",
            "#include <linux/err.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/slab.h>",
            "#include <linux/filter.h>",
            "#include <linux/mm.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/wait.h>",
            "#include <linux/poll.h>",
            "#include <linux/kmemleak.h>",
            "#include <uapi/linux/btf.h>",
            "#include <linux/btf_ids.h>",
            "",
            "#define RINGBUF_CREATE_FLAG_MASK (BPF_F_NUMA_NODE)",
            "",
            "/* non-mmap()'able part of bpf_ringbuf (everything up to consumer page) */",
            "#define RINGBUF_PGOFF \\",
            "\t(offsetof(struct bpf_ringbuf, consumer_pos) >> PAGE_SHIFT)",
            "/* consumer page and producer page */",
            "#define RINGBUF_POS_PAGES 2",
            "#define RINGBUF_NR_META_PAGES (RINGBUF_PGOFF + RINGBUF_POS_PAGES)",
            "",
            "#define RINGBUF_MAX_RECORD_SZ (UINT_MAX/4)",
            "",
            "struct bpf_ringbuf {",
            "\twait_queue_head_t waitq;",
            "\tstruct irq_work work;",
            "\tu64 mask;",
            "\tstruct page **pages;",
            "\tint nr_pages;",
            "\traw_spinlock_t spinlock ____cacheline_aligned_in_smp;",
            "\t/* For user-space producer ring buffers, an atomic_t busy bit is used",
            "\t * to synchronize access to the ring buffers in the kernel, rather than",
            "\t * the spinlock that is used for kernel-producer ring buffers. This is",
            "\t * done because the ring buffer must hold a lock across a BPF program's",
            "\t * callback:",
            "\t *",
            "\t *    __bpf_user_ringbuf_peek() // lock acquired",
            "\t * -> program callback_fn()",
            "\t * -> __bpf_user_ringbuf_sample_release() // lock released",
            "\t *",
            "\t * It is unsafe and incorrect to hold an IRQ spinlock across what could",
            "\t * be a long execution window, so we instead simply disallow concurrent",
            "\t * access to the ring buffer by kernel consumers, and return -EBUSY from",
            "\t * __bpf_user_ringbuf_peek() if the busy bit is held by another task.",
            "\t */",
            "\tatomic_t busy ____cacheline_aligned_in_smp;",
            "\t/* Consumer and producer counters are put into separate pages to",
            "\t * allow each position to be mapped with different permissions.",
            "\t * This prevents a user-space application from modifying the",
            "\t * position and ruining in-kernel tracking. The permissions of the",
            "\t * pages depend on who is producing samples: user-space or the",
            "\t * kernel. Note that the pending counter is placed in the same",
            "\t * page as the producer, so that it shares the same cache line.",
            "\t *",
            "\t * Kernel-producer",
            "\t * ---------------",
            "\t * The producer position and data pages are mapped as r/o in",
            "\t * userspace. For this approach, bits in the header of samples are",
            "\t * used to signal to user-space, and to other producers, whether a",
            "\t * sample is currently being written.",
            "\t *",
            "\t * User-space producer",
            "\t * -------------------",
            "\t * Only the page containing the consumer position is mapped r/o in",
            "\t * user-space. User-space producers also use bits of the header to",
            "\t * communicate to the kernel, but the kernel must carefully check and",
            "\t * validate each sample to ensure that they're correctly formatted, and",
            "\t * fully contained within the ring buffer.",
            "\t */",
            "\tunsigned long consumer_pos __aligned(PAGE_SIZE);",
            "\tunsigned long producer_pos __aligned(PAGE_SIZE);",
            "\tunsigned long pending_pos;",
            "\tchar data[] __aligned(PAGE_SIZE);",
            "};",
            "",
            "struct bpf_ringbuf_map {",
            "\tstruct bpf_map map;",
            "\tstruct bpf_ringbuf *rb;",
            "};",
            "",
            "/* 8-byte ring buffer record header structure */",
            "struct bpf_ringbuf_hdr {",
            "\tu32 len;",
            "\tu32 pg_off;",
            "};",
            "",
            "static struct bpf_ringbuf *bpf_ringbuf_area_alloc(size_t data_sz, int numa_node)",
            "{",
            "\tconst gfp_t flags = GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL |",
            "\t\t\t    __GFP_NOWARN | __GFP_ZERO;",
            "\tint nr_meta_pages = RINGBUF_NR_META_PAGES;",
            "\tint nr_data_pages = data_sz >> PAGE_SHIFT;",
            "\tint nr_pages = nr_meta_pages + nr_data_pages;",
            "\tstruct page **pages, *page;",
            "\tstruct bpf_ringbuf *rb;",
            "\tsize_t array_size;",
            "\tint i;",
            "",
            "\t/* Each data page is mapped twice to allow \"virtual\"",
            "\t * continuous read of samples wrapping around the end of ring",
            "\t * buffer area:",
            "\t * ------------------------------------------------------",
            "\t * | meta pages |  real data pages  |  same data pages  |",
            "\t * ------------------------------------------------------",
            "\t * |            | 1 2 3 4 5 6 7 8 9 | 1 2 3 4 5 6 7 8 9 |",
            "\t * ------------------------------------------------------",
            "\t * |            | TA             DA | TA             DA |",
            "\t * ------------------------------------------------------",
            "\t *                               ^^^^^^^",
            "\t *                                  |",
            "\t * Here, no need to worry about special handling of wrapped-around",
            "\t * data due to double-mapped data pages. This works both in kernel and",
            "\t * when mmap()'ed in user-space, simplifying both kernel and",
            "\t * user-space implementations significantly.",
            "\t */",
            "\tarray_size = (nr_meta_pages + 2 * nr_data_pages) * sizeof(*pages);",
            "\tpages = bpf_map_area_alloc(array_size, numa_node);",
            "\tif (!pages)",
            "\t\treturn NULL;",
            "",
            "\tfor (i = 0; i < nr_pages; i++) {",
            "\t\tpage = alloc_pages_node(numa_node, flags, 0);",
            "\t\tif (!page) {",
            "\t\t\tnr_pages = i;",
            "\t\t\tgoto err_free_pages;",
            "\t\t}",
            "\t\tpages[i] = page;",
            "\t\tif (i >= nr_meta_pages)",
            "\t\t\tpages[nr_data_pages + i] = page;",
            "\t}",
            "",
            "\trb = vmap(pages, nr_meta_pages + 2 * nr_data_pages,",
            "\t\t  VM_MAP | VM_USERMAP, PAGE_KERNEL);",
            "\tif (rb) {",
            "\t\tkmemleak_not_leak(pages);",
            "\t\trb->pages = pages;",
            "\t\trb->nr_pages = nr_pages;",
            "\t\treturn rb;",
            "\t}",
            "",
            "err_free_pages:",
            "\tfor (i = 0; i < nr_pages; i++)",
            "\t\t__free_page(pages[i]);",
            "\tbpf_map_area_free(pages);",
            "\treturn NULL;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了bpf_ringbuf结构体及其相关宏，用于管理BPF环形缓冲区的元数据和数据区域。通过页面数组实现环形缓冲区的虚拟连续读取，支持用户态和内核态生产者的差异化权限控制，其中包含消费者/生产者位置指针、忙位原子变量及锁保护的元数据。",
          "similarity": 0.5582902431488037
        }
      ]
    },
    {
      "source_file": "kernel/trace/ring_buffer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:07:21\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `trace\\ring_buffer.c`\n\n---\n\n# `trace/ring_buffer.c` 技术文档\n\n## 1. 文件概述\n\n`trace/ring_buffer.c` 实现了 Linux 内核中通用的高性能环形缓冲区（ring buffer）机制，主要用于跟踪（tracing）子系统。该缓冲区支持多 CPU 并发写入、单读者或多读者无锁读取，并通过时间戳压缩、事件类型编码和页面交换等技术优化内存使用和性能。该实现是 ftrace、perf 和其他内核跟踪工具的核心基础设施。\n\n## 2. 核心功能\n\n### 主要函数\n- `ring_buffer_print_entry_header()`：输出环形缓冲区条目头部格式说明，用于调试或用户空间解析。\n- `ring_buffer_event_length()`：返回事件有效载荷（payload）的长度，对 TIME_EXTEND 类型自动跳过扩展头。\n- `rb_event_data()`（内联）：返回指向事件实际数据的指针，处理 TIME_EXTEND 和不同长度编码。\n- `rb_event_length()`：返回完整事件结构（含头部）的字节长度。\n- `rb_event_ts_length()`：返回 TIME_EXTEND 事件及其后续数据事件的总长度。\n- `rb_event_data_length()`：计算数据类型事件的总长度（含头部）。\n- `rb_null_event()` / `rb_event_set_padding()`：判断或设置空/填充事件。\n\n### 关键数据结构（隐含或引用）\n- `struct ring_buffer_event`：环形缓冲区中每个事件的通用头部结构。\n- `struct buffer_data_page`：每个 CPU 缓冲区页面的封装，包含数据和元数据。\n- 每 CPU 页面链表：每个 CPU 拥有独立的环形页面链，写者仅写本地 CPU 缓冲区。\n\n### 核心常量与宏\n- `RINGBUF_TYPE_PADDING`、`RINGBUF_TYPE_TIME_EXTEND`、`RINGBUF_TYPE_TIME_STAMP`、`RINGBUF_TYPE_DATA`：事件类型标识。\n- `RB_ALIGNMENT` / `RB_ARCH_ALIGNMENT`：数据对齐策略，根据架构是否支持 64 位对齐访问调整。\n- `RB_MAX_SMALL_DATA`：小数据事件的最大长度（基于 4 字节对齐和类型长度上限）。\n- `TS_MSB` / `ABS_TS_MASK`：用于处理 59 位时间戳的高位截断与恢复。\n\n## 3. 关键实现\n\n### 无锁读写架构\n- **写者**：每个 CPU 只能写入其对应的 per-CPU 缓冲区，通过原子操作和内存屏障保证写入一致性，无需全局锁。\n- **读者**：每个 per-CPU 缓冲区维护一个独立的“reader page”。当 reader page 被读完后，通过原子交换（未来使用 `cmpxchg`）将其与环形缓冲区中的一个页面互换。交换后，原 reader page 不再被写者访问，读者可安全地将其用于 splice、复制或释放。\n\n### 事件编码与压缩\n- 事件头部使用紧凑位域编码：\n  - `type_len`（5 位）：事件类型或小数据长度（≤31）。\n  - `time_delta`（27 位）：相对于前一事件的时间增量。\n  - `array`（32 位）：用于存储大长度值或事件数据。\n- **TIME_EXTEND 事件**：当时间增量超出 27 位或需要绝对时间戳时，插入一个 8 字节的 TIME_EXTEND 事件，后跟实际数据事件。\n- **数据长度编码**：\n  - 若 `type_len > 0` 且 ≤ `RINGBUF_TYPE_DATA_TYPE_LEN_MAX`，则数据长度 = `type_len * RB_ALIGNMENT`，数据从 `array[0]` 开始。\n  - 否则，数据长度存储在 `array[0]`，实际数据从 `array[1]` 开始。\n\n### 时间戳处理\n- 绝对时间戳仅保留低 59 位（`ABS_TS_MASK`），高 5 位（`TS_MSB`）若非零需单独保存并在读取时恢复，以支持长时间运行的跟踪。\n\n### 内存对齐优化\n- 在支持 64 位对齐访问的架构上（`CONFIG_HAVE_64BIT_ALIGNED_ACCESS`），强制 8 字节对齐（`RB_FORCE_8BYTE_ALIGNMENT`），提升访问性能；否则使用 4 字节对齐。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/ring_buffer.h>`：定义公共 API 和数据结构。\n  - `<linux/trace_clock.h>`、`<linux/sched/clock.h>`：提供高精度时间戳源。\n  - `<linux/percpu.h>`：支持 per-CPU 缓冲区分配。\n  - `<linux/spinlock.h>`、`<asm/local.h>`：提供底层原子操作和锁原语。\n  - `<linux/trace_recursion.h>`：防止跟踪递归。\n- **子系统依赖**：\n  - **ftrace**：主要消费者，用于函数跟踪、事件跟踪等。\n  - **perf**：通过 ring buffer 获取性能事件数据。\n  - **Security Module**：通过 `<linux/security.h>` 集成 LSM 钩子（如 trace 访问控制）。\n- **架构依赖**：依赖 `CONFIG_HAVE_64BIT_ALIGNED_ACCESS` 配置项优化对齐策略。\n\n## 5. 使用场景\n\n- **内核跟踪（ftrace）**：记录函数调用、上下文切换、中断等事件，数据写入 per-CPU ring buffer，用户通过 `tracefs` 读取。\n- **性能分析（perf）**：perf 工具通过 ring buffer 接收内核采样事件（如 PMU 中断、软件事件）。\n- **实时监控与调试**：开发者或运维人员通过读取 ring buffer 内容分析系统行为、延迟或错误。\n- **自测试（selftest）**：文件包含自测试逻辑（依赖 `<linux/kthread.h>`），用于验证 ring buffer 功能正确性。\n- **低开销事件记录**：由于其无锁设计和压缩编码，适用于高频事件记录场景（如每秒百万级事件）。",
      "similarity": 0.5893055200576782,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 45,
          "end_line": 148,
          "content": [
            "int ring_buffer_print_entry_header(struct trace_seq *s)",
            "{",
            "\ttrace_seq_puts(s, \"# compressed entry header\\n\");",
            "\ttrace_seq_puts(s, \"\\ttype_len    :    5 bits\\n\");",
            "\ttrace_seq_puts(s, \"\\ttime_delta  :   27 bits\\n\");",
            "\ttrace_seq_puts(s, \"\\tarray       :   32 bits\\n\");",
            "\ttrace_seq_putc(s, '\\n');",
            "\ttrace_seq_printf(s, \"\\tpadding     : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_PADDING);",
            "\ttrace_seq_printf(s, \"\\ttime_extend : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_TIME_EXTEND);",
            "\ttrace_seq_printf(s, \"\\ttime_stamp : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_TIME_STAMP);",
            "\ttrace_seq_printf(s, \"\\tdata max type_len  == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_DATA_TYPE_LEN_MAX);",
            "",
            "\treturn !trace_seq_has_overflowed(s);",
            "}",
            "static inline bool rb_null_event(struct ring_buffer_event *event)",
            "{",
            "\treturn event->type_len == RINGBUF_TYPE_PADDING && !event->time_delta;",
            "}",
            "static void rb_event_set_padding(struct ring_buffer_event *event)",
            "{",
            "\t/* padding has a NULL time_delta */",
            "\tevent->type_len = RINGBUF_TYPE_PADDING;",
            "\tevent->time_delta = 0;",
            "}",
            "static unsigned",
            "rb_event_data_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned length;",
            "",
            "\tif (event->type_len)",
            "\t\tlength = event->type_len * RB_ALIGNMENT;",
            "\telse",
            "\t\tlength = event->array[0];",
            "\treturn length + RB_EVNT_HDR_SIZE;",
            "}",
            "static inline unsigned",
            "rb_event_length(struct ring_buffer_event *event)",
            "{",
            "\tswitch (event->type_len) {",
            "\tcase RINGBUF_TYPE_PADDING:",
            "\t\tif (rb_null_event(event))",
            "\t\t\t/* undefined */",
            "\t\t\treturn -1;",
            "\t\treturn  event->array[0] + RB_EVNT_HDR_SIZE;",
            "",
            "\tcase RINGBUF_TYPE_TIME_EXTEND:",
            "\t\treturn RB_LEN_TIME_EXTEND;",
            "",
            "\tcase RINGBUF_TYPE_TIME_STAMP:",
            "\t\treturn RB_LEN_TIME_STAMP;",
            "",
            "\tcase RINGBUF_TYPE_DATA:",
            "\t\treturn rb_event_data_length(event);",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(1);",
            "\t}",
            "\t/* not hit */",
            "\treturn 0;",
            "}",
            "static inline unsigned",
            "rb_event_ts_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned len = 0;",
            "",
            "\tif (extended_time(event)) {",
            "\t\t/* time extends include the data event after it */",
            "\t\tlen = RB_LEN_TIME_EXTEND;",
            "\t\tevent = skip_time_extend(event);",
            "\t}",
            "\treturn len + rb_event_length(event);",
            "}",
            "unsigned ring_buffer_event_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned length;",
            "",
            "\tif (extended_time(event))",
            "\t\tevent = skip_time_extend(event);",
            "",
            "\tlength = rb_event_length(event);",
            "\tif (event->type_len > RINGBUF_TYPE_DATA_TYPE_LEN_MAX)",
            "\t\treturn length;",
            "\tlength -= RB_EVNT_HDR_SIZE;",
            "\tif (length > RB_MAX_SMALL_DATA + sizeof(event->array[0]))",
            "                length -= sizeof(event->array[0]);",
            "\treturn length;",
            "}",
            "static u64 rb_event_time_stamp(struct ring_buffer_event *event)",
            "{",
            "\tu64 ts;",
            "",
            "\tts = event->array[0];",
            "\tts <<= TS_SHIFT;",
            "\tts += event->time_delta;",
            "",
            "\treturn ts;",
            "}",
            "static void rb_init_page(struct buffer_data_page *bpage)",
            "{",
            "\tlocal_set(&bpage->commit, 0);",
            "}"
          ],
          "function_name": "ring_buffer_print_entry_header, rb_null_event, rb_event_set_padding, rb_event_data_length, rb_event_length, rb_event_ts_length, ring_buffer_event_length, rb_event_time_stamp, rb_init_page",
          "description": "实现环形缓冲区事件解析功能，包括打印事件头信息、识别空事件、设置填充事件、计算不同事件类型的数据长度及时间戳，提供事件长度和时间戳读取接口",
          "similarity": 0.6801183223724365
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 2052,
          "end_line": 2363,
          "content": [
            "static bool",
            "rb_insert_pages(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tstruct list_head *pages = &cpu_buffer->new_pages;",
            "\tunsigned long flags;",
            "\tbool success;",
            "\tint retries;",
            "",
            "\t/* Can be called at early boot up, where interrupts must not been enabled */",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t/*",
            "\t * We are holding the reader lock, so the reader page won't be swapped",
            "\t * in the ring buffer. Now we are racing with the writer trying to",
            "\t * move head page and the tail page.",
            "\t * We are going to adapt the reader page update process where:",
            "\t * 1. We first splice the start and end of list of new pages between",
            "\t *    the head page and its previous page.",
            "\t * 2. We cmpxchg the prev_page->next to point from head page to the",
            "\t *    start of new pages list.",
            "\t * 3. Finally, we update the head->prev to the end of new list.",
            "\t *",
            "\t * We will try this process 10 times, to make sure that we don't keep",
            "\t * spinning.",
            "\t */",
            "\tretries = 10;",
            "\tsuccess = false;",
            "\twhile (retries--) {",
            "\t\tstruct list_head *head_page, *prev_page, *r;",
            "\t\tstruct list_head *last_page, *first_page;",
            "\t\tstruct list_head *head_page_with_bit;",
            "\t\tstruct buffer_page *hpage = rb_set_head_page(cpu_buffer);",
            "",
            "\t\tif (!hpage)",
            "\t\t\tbreak;",
            "\t\thead_page = &hpage->list;",
            "\t\tprev_page = head_page->prev;",
            "",
            "\t\tfirst_page = pages->next;",
            "\t\tlast_page  = pages->prev;",
            "",
            "\t\thead_page_with_bit = (struct list_head *)",
            "\t\t\t\t     ((unsigned long)head_page | RB_PAGE_HEAD);",
            "",
            "\t\tlast_page->next = head_page_with_bit;",
            "\t\tfirst_page->prev = prev_page;",
            "",
            "\t\tr = cmpxchg(&prev_page->next, head_page_with_bit, first_page);",
            "",
            "\t\tif (r == head_page_with_bit) {",
            "\t\t\t/*",
            "\t\t\t * yay, we replaced the page pointer to our new list,",
            "\t\t\t * now, we just have to update to head page's prev",
            "\t\t\t * pointer to point to end of list",
            "\t\t\t */",
            "\t\t\thead_page->prev = last_page;",
            "\t\t\tsuccess = true;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tif (success)",
            "\t\tINIT_LIST_HEAD(pages);",
            "\t/*",
            "\t * If we weren't successful in adding in new pages, warn and stop",
            "\t * tracing",
            "\t */",
            "\tRB_WARN_ON(cpu_buffer, !success);",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "",
            "\t/* free pages if they weren't inserted */",
            "\tif (!success) {",
            "\t\tstruct buffer_page *bpage, *tmp;",
            "\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,",
            "\t\t\t\t\t list) {",
            "\t\t\tlist_del_init(&bpage->list);",
            "\t\t\tfree_buffer_page(bpage);",
            "\t\t}",
            "\t}",
            "\treturn success;",
            "}",
            "static void rb_update_pages(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tbool success;",
            "",
            "\tif (cpu_buffer->nr_pages_to_update > 0)",
            "\t\tsuccess = rb_insert_pages(cpu_buffer);",
            "\telse",
            "\t\tsuccess = rb_remove_pages(cpu_buffer,",
            "\t\t\t\t\t-cpu_buffer->nr_pages_to_update);",
            "",
            "\tif (success)",
            "\t\tcpu_buffer->nr_pages += cpu_buffer->nr_pages_to_update;",
            "}",
            "static void update_pages_handler(struct work_struct *work)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = container_of(work,",
            "\t\t\tstruct ring_buffer_per_cpu, update_pages_work);",
            "\trb_update_pages(cpu_buffer);",
            "\tcomplete(&cpu_buffer->update_done);",
            "}",
            "int ring_buffer_resize(struct trace_buffer *buffer, unsigned long size,",
            "\t\t\tint cpu_id)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tunsigned long nr_pages;",
            "\tint cpu, err;",
            "",
            "\t/*",
            "\t * Always succeed at resizing a non-existent buffer:",
            "\t */",
            "\tif (!buffer)",
            "\t\treturn 0;",
            "",
            "\t/* Make sure the requested buffer exists */",
            "\tif (cpu_id != RING_BUFFER_ALL_CPUS &&",
            "\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))",
            "\t\treturn 0;",
            "",
            "\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);",
            "",
            "\t/* we need a minimum of two pages */",
            "\tif (nr_pages < 2)",
            "\t\tnr_pages = 2;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "\tatomic_inc(&buffer->resizing);",
            "",
            "\tif (cpu_id == RING_BUFFER_ALL_CPUS) {",
            "\t\t/*",
            "\t\t * Don't succeed if resizing is disabled, as a reader might be",
            "\t\t * manipulating the ring buffer and is expecting a sane state while",
            "\t\t * this is true.",
            "\t\t */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {",
            "\t\t\t\terr = -EBUSY;",
            "\t\t\t\tgoto out_err_unlock;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* calculate the pages to update */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -",
            "\t\t\t\t\t\t\tcpu_buffer->nr_pages;",
            "\t\t\t/*",
            "\t\t\t * nothing more to do for removing pages or no update",
            "\t\t\t */",
            "\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)",
            "\t\t\t\tcontinue;",
            "\t\t\t/*",
            "\t\t\t * to add pages, make sure all new pages can be",
            "\t\t\t * allocated without receiving ENOMEM",
            "\t\t\t */",
            "\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\t\t\tif (__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,",
            "\t\t\t\t\t\t&cpu_buffer->new_pages)) {",
            "\t\t\t\t/* not enough memory for new pages */",
            "\t\t\t\terr = -ENOMEM;",
            "\t\t\t\tgoto out_err;",
            "\t\t\t}",
            "",
            "\t\t\tcond_resched();",
            "\t\t}",
            "",
            "\t\tcpus_read_lock();",
            "\t\t/*",
            "\t\t * Fire off all the required work handlers",
            "\t\t * We can't schedule on offline CPUs, but it's not necessary",
            "\t\t * since we can change their buffer sizes without any race.",
            "\t\t */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (!cpu_buffer->nr_pages_to_update)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\t/* Can't run something on an offline CPU. */",
            "\t\t\tif (!cpu_online(cpu)) {",
            "\t\t\t\trb_update_pages(cpu_buffer);",
            "\t\t\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\t\t} else {",
            "\t\t\t\t/* Run directly if possible. */",
            "\t\t\t\tmigrate_disable();",
            "\t\t\t\tif (cpu != smp_processor_id()) {",
            "\t\t\t\t\tmigrate_enable();",
            "\t\t\t\t\tschedule_work_on(cpu,",
            "\t\t\t\t\t\t\t &cpu_buffer->update_pages_work);",
            "\t\t\t\t} else {",
            "\t\t\t\t\tupdate_pages_handler(&cpu_buffer->update_pages_work);",
            "\t\t\t\t\tmigrate_enable();",
            "\t\t\t\t}",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* wait for all the updates to complete */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (!cpu_buffer->nr_pages_to_update)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tif (cpu_online(cpu))",
            "\t\t\t\twait_for_completion(&cpu_buffer->update_done);",
            "\t\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\t}",
            "",
            "\t\tcpus_read_unlock();",
            "\t} else {",
            "\t\tcpu_buffer = buffer->buffers[cpu_id];",
            "",
            "\t\tif (nr_pages == cpu_buffer->nr_pages)",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * Don't succeed if resizing is disabled, as a reader might be",
            "\t\t * manipulating the ring buffer and is expecting a sane state while",
            "\t\t * this is true.",
            "\t\t */",
            "\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {",
            "\t\t\terr = -EBUSY;",
            "\t\t\tgoto out_err_unlock;",
            "\t\t}",
            "",
            "\t\tcpu_buffer->nr_pages_to_update = nr_pages -",
            "\t\t\t\t\t\tcpu_buffer->nr_pages;",
            "",
            "\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\t\tif (cpu_buffer->nr_pages_to_update > 0 &&",
            "\t\t\t__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,",
            "\t\t\t\t\t    &cpu_buffer->new_pages)) {",
            "\t\t\terr = -ENOMEM;",
            "\t\t\tgoto out_err;",
            "\t\t}",
            "",
            "\t\tcpus_read_lock();",
            "",
            "\t\t/* Can't run something on an offline CPU. */",
            "\t\tif (!cpu_online(cpu_id))",
            "\t\t\trb_update_pages(cpu_buffer);",
            "\t\telse {",
            "\t\t\t/* Run directly if possible. */",
            "\t\t\tmigrate_disable();",
            "\t\t\tif (cpu_id == smp_processor_id()) {",
            "\t\t\t\trb_update_pages(cpu_buffer);",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t} else {",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t\tschedule_work_on(cpu_id,",
            "\t\t\t\t\t\t &cpu_buffer->update_pages_work);",
            "\t\t\t\twait_for_completion(&cpu_buffer->update_done);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\tcpus_read_unlock();",
            "\t}",
            "",
            " out:",
            "\t/*",
            "\t * The ring buffer resize can happen with the ring buffer",
            "\t * enabled, so that the update disturbs the tracing as little",
            "\t * as possible. But if the buffer is disabled, we do not need",
            "\t * to worry about that, and we can take the time to verify",
            "\t * that the buffer is not corrupt.",
            "\t */",
            "\tif (atomic_read(&buffer->record_disabled)) {",
            "\t\tatomic_inc(&buffer->record_disabled);",
            "\t\t/*",
            "\t\t * Even though the buffer was disabled, we must make sure",
            "\t\t * that it is truly disabled before calling rb_check_pages.",
            "\t\t * There could have been a race between checking",
            "\t\t * record_disable and incrementing it.",
            "\t\t */",
            "\t\tsynchronize_rcu();",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tunsigned long flags;",
            "",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t\t\trb_check_pages(cpu_buffer);",
            "\t\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "\t\t}",
            "\t\tatomic_dec(&buffer->record_disabled);",
            "\t}",
            "",
            "\tatomic_dec(&buffer->resizing);",
            "\tmutex_unlock(&buffer->mutex);",
            "\treturn 0;",
            "",
            " out_err:",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tstruct buffer_page *bpage, *tmp;",
            "",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\tcpu_buffer->nr_pages_to_update = 0;",
            "",
            "\t\tif (list_empty(&cpu_buffer->new_pages))",
            "\t\t\tcontinue;",
            "",
            "\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,",
            "\t\t\t\t\tlist) {",
            "\t\t\tlist_del_init(&bpage->list);",
            "\t\t\tfree_buffer_page(bpage);",
            "\t\t}",
            "\t}",
            " out_err_unlock:",
            "\tatomic_dec(&buffer->resizing);",
            "\tmutex_unlock(&buffer->mutex);",
            "\treturn err;",
            "}"
          ],
          "function_name": "rb_insert_pages, rb_update_pages, update_pages_handler, ring_buffer_resize",
          "description": "实现将新分配的缓冲页插入到环形缓冲区的头部，通过CAS操作确保线程安全地更新链表结构，若失败则释放内存资源。包含调整缓冲区大小的核心逻辑，协调多CPU上的页面分配与更新操作。",
          "similarity": 0.65456223487854
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 993,
          "end_line": 1149,
          "content": [
            "static inline bool",
            "rb_wait_cond(struct rb_irq_work *rbwork, struct trace_buffer *buffer,",
            "\t     int cpu, int full, ring_buffer_cond_fn cond, void *data)",
            "{",
            "\tif (rb_watermark_hit(buffer, cpu, full))",
            "\t\treturn true;",
            "",
            "\tif (cond(data))",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * The events can happen in critical sections where",
            "\t * checking a work queue can cause deadlocks.",
            "\t * After adding a task to the queue, this flag is set",
            "\t * only to notify events to try to wake up the queue",
            "\t * using irq_work.",
            "\t *",
            "\t * We don't clear it even if the buffer is no longer",
            "\t * empty. The flag only causes the next event to run",
            "\t * irq_work to do the work queue wake up. The worse",
            "\t * that can happen if we race with !trace_empty() is that",
            "\t * an event will cause an irq_work to try to wake up",
            "\t * an empty queue.",
            "\t *",
            "\t * There's no reason to protect this flag either, as",
            "\t * the work queue and irq_work logic will do the necessary",
            "\t * synchronization for the wake ups. The only thing",
            "\t * that is necessary is that the wake up happens after",
            "\t * a task has been queued. It's OK for spurious wake ups.",
            "\t */",
            "\tif (full)",
            "\t\trbwork->full_waiters_pending = true;",
            "\telse",
            "\t\trbwork->waiters_pending = true;",
            "",
            "\treturn false;",
            "}",
            "static bool rb_wait_once(void *data)",
            "{",
            "\tlong *once = data;",
            "",
            "\t/* wait_event() actually calls this twice before scheduling*/",
            "\tif (*once > 1)",
            "\t\treturn true;",
            "",
            "\t(*once)++;",
            "\treturn false;",
            "}",
            "int ring_buffer_wait(struct trace_buffer *buffer, int cpu, int full,",
            "\t\t     ring_buffer_cond_fn cond, void *data)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct wait_queue_head *waitq;",
            "\tstruct rb_irq_work *rbwork;",
            "\tlong once = 0;",
            "\tint ret = 0;",
            "",
            "\tif (!cond) {",
            "\t\tcond = rb_wait_once;",
            "\t\tdata = &once;",
            "\t}",
            "",
            "\t/*",
            "\t * Depending on what the caller is waiting for, either any",
            "\t * data in any cpu buffer, or a specific buffer, put the",
            "\t * caller on the appropriate wait queue.",
            "\t */",
            "\tif (cpu == RING_BUFFER_ALL_CPUS) {",
            "\t\trbwork = &buffer->irq_work;",
            "\t\t/* Full only makes sense on per cpu reads */",
            "\t\tfull = 0;",
            "\t} else {",
            "\t\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\t\treturn -ENODEV;",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\trbwork = &cpu_buffer->irq_work;",
            "\t}",
            "",
            "\tif (full)",
            "\t\twaitq = &rbwork->full_waiters;",
            "\telse",
            "\t\twaitq = &rbwork->waiters;",
            "",
            "\tret = wait_event_interruptible((*waitq),",
            "\t\t\t\trb_wait_cond(rbwork, buffer, cpu, full, cond, data));",
            "",
            "\treturn ret;",
            "}",
            "__poll_t ring_buffer_poll_wait(struct trace_buffer *buffer, int cpu,",
            "\t\t\t  struct file *filp, poll_table *poll_table, int full)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct rb_irq_work *rbwork;",
            "",
            "\tif (cpu == RING_BUFFER_ALL_CPUS) {",
            "\t\trbwork = &buffer->irq_work;",
            "\t\tfull = 0;",
            "\t} else {",
            "\t\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\t\treturn EPOLLERR;",
            "",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\trbwork = &cpu_buffer->irq_work;",
            "\t}",
            "",
            "\tif (full) {",
            "\t\tunsigned long flags;",
            "",
            "\t\tpoll_wait(filp, &rbwork->full_waiters, poll_table);",
            "",
            "\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t\tif (!cpu_buffer->shortest_full ||",
            "\t\t    cpu_buffer->shortest_full > full)",
            "\t\t\tcpu_buffer->shortest_full = full;",
            "\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "\t\tif (full_hit(buffer, cpu, full))",
            "\t\t\treturn EPOLLIN | EPOLLRDNORM;",
            "\t\t/*",
            "\t\t * Only allow full_waiters_pending update to be seen after",
            "\t\t * the shortest_full is set. If the writer sees the",
            "\t\t * full_waiters_pending flag set, it will compare the",
            "\t\t * amount in the ring buffer to shortest_full. If the amount",
            "\t\t * in the ring buffer is greater than the shortest_full",
            "\t\t * percent, it will call the irq_work handler to wake up",
            "\t\t * this list. The irq_handler will reset shortest_full",
            "\t\t * back to zero. That's done under the reader_lock, but",
            "\t\t * the below smp_mb() makes sure that the update to",
            "\t\t * full_waiters_pending doesn't leak up into the above.",
            "\t\t */",
            "\t\tsmp_mb();",
            "\t\trbwork->full_waiters_pending = true;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tpoll_wait(filp, &rbwork->waiters, poll_table);",
            "\trbwork->waiters_pending = true;",
            "",
            "\t/*",
            "\t * There's a tight race between setting the waiters_pending and",
            "\t * checking if the ring buffer is empty.  Once the waiters_pending bit",
            "\t * is set, the next event will wake the task up, but we can get stuck",
            "\t * if there's only a single event in.",
            "\t *",
            "\t * FIXME: Ideally, we need a memory barrier on the writer side as well,",
            "\t * but adding a memory barrier to all events will cause too much of a",
            "\t * performance hit in the fast path.  We only need a memory barrier when",
            "\t * the buffer goes from empty to having content.  But as this race is",
            "\t * extremely small, and it's not a problem if another event comes in, we",
            "\t * will fix it later.",
            "\t */",
            "\tsmp_mb();",
            "",
            "\tif ((cpu == RING_BUFFER_ALL_CPUS && !ring_buffer_empty(buffer)) ||",
            "\t    (cpu != RING_BUFFER_ALL_CPUS && !ring_buffer_empty_cpu(buffer, cpu)))",
            "\t\treturn EPOLLIN | EPOLLRDNORM;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rb_wait_cond, rb_wait_once, ring_buffer_wait, ring_buffer_poll_wait",
          "description": "实现环形缓冲区的等待逻辑，通过rb_wait_cond判断条件是否满足，ring_buffer_wait将调用者加入等待队列并阻塞直至条件成立或被中断，ring_buffer_poll_wait支持poll风格的等待并返回EPOLLIN标志。",
          "similarity": 0.6293971538543701
        },
        {
          "chunk_id": 22,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 4955,
          "end_line": 5087,
          "content": [
            "static inline void",
            "rb_reader_unlock(struct ring_buffer_per_cpu *cpu_buffer, bool locked)",
            "{",
            "\tif (likely(locked))",
            "\t\traw_spin_unlock(&cpu_buffer->reader_lock);",
            "}",
            "bool ring_buffer_iter_dropped(struct ring_buffer_iter *iter)",
            "{",
            "\tbool ret = iter->missed_events != 0;",
            "",
            "\titer->missed_events = 0;",
            "\treturn ret;",
            "}",
            "void",
            "ring_buffer_read_prepare_sync(void)",
            "{",
            "\tsynchronize_rcu();",
            "}",
            "void",
            "ring_buffer_read_start(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tunsigned long flags;",
            "",
            "\tif (!iter)",
            "\t\treturn;",
            "",
            "\tcpu_buffer = iter->cpu_buffer;",
            "",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\tarch_spin_lock(&cpu_buffer->lock);",
            "\trb_iter_reset(iter);",
            "\tarch_spin_unlock(&cpu_buffer->lock);",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "}",
            "void",
            "ring_buffer_read_finish(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * Ring buffer is disabled from recording, here's a good place",
            "\t * to check the integrity of the ring buffer.",
            "\t * Must prevent readers from trying to read, as the check",
            "\t * clears the HEAD page and readers require it.",
            "\t */",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\trb_check_pages(cpu_buffer);",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "",
            "\tatomic_dec(&cpu_buffer->resize_disabled);",
            "\tkfree(iter->event);",
            "\tkfree(iter);",
            "}",
            "void ring_buffer_iter_advance(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "",
            "\trb_advance_iter(iter);",
            "",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "}",
            "unsigned long ring_buffer_size(struct trace_buffer *buffer, int cpu)",
            "{",
            "\t/*",
            "\t * Earlier, this method returned",
            "\t *\tBUF_PAGE_SIZE * buffer->nr_pages",
            "\t * Since the nr_pages field is now removed, we have converted this to",
            "\t * return the per cpu buffer value.",
            "\t */",
            "\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\treturn 0;",
            "",
            "\treturn BUF_PAGE_SIZE * buffer->buffers[cpu]->nr_pages;",
            "}",
            "static void rb_clear_buffer_page(struct buffer_page *page)",
            "{",
            "\tlocal_set(&page->write, 0);",
            "\tlocal_set(&page->entries, 0);",
            "\trb_init_page(page->page);",
            "\tpage->read = 0;",
            "}",
            "static void",
            "rb_reset_cpu(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tstruct buffer_page *page;",
            "",
            "\trb_head_page_deactivate(cpu_buffer);",
            "",
            "\tcpu_buffer->head_page",
            "\t\t= list_entry(cpu_buffer->pages, struct buffer_page, list);",
            "\trb_clear_buffer_page(cpu_buffer->head_page);",
            "\tlist_for_each_entry(page, cpu_buffer->pages, list) {",
            "\t\trb_clear_buffer_page(page);",
            "\t}",
            "",
            "\tcpu_buffer->tail_page = cpu_buffer->head_page;",
            "\tcpu_buffer->commit_page = cpu_buffer->head_page;",
            "",
            "\tINIT_LIST_HEAD(&cpu_buffer->reader_page->list);",
            "\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\trb_clear_buffer_page(cpu_buffer->reader_page);",
            "",
            "\tlocal_set(&cpu_buffer->entries_bytes, 0);",
            "\tlocal_set(&cpu_buffer->overrun, 0);",
            "\tlocal_set(&cpu_buffer->commit_overrun, 0);",
            "\tlocal_set(&cpu_buffer->dropped_events, 0);",
            "\tlocal_set(&cpu_buffer->entries, 0);",
            "\tlocal_set(&cpu_buffer->committing, 0);",
            "\tlocal_set(&cpu_buffer->commits, 0);",
            "\tlocal_set(&cpu_buffer->pages_touched, 0);",
            "\tlocal_set(&cpu_buffer->pages_lost, 0);",
            "\tlocal_set(&cpu_buffer->pages_read, 0);",
            "\tcpu_buffer->last_pages_touch = 0;",
            "\tcpu_buffer->shortest_full = 0;",
            "\tcpu_buffer->read = 0;",
            "\tcpu_buffer->read_bytes = 0;",
            "",
            "\trb_time_set(&cpu_buffer->write_stamp, 0);",
            "\trb_time_set(&cpu_buffer->before_stamp, 0);",
            "",
            "\tmemset(cpu_buffer->event_stamp, 0, sizeof(cpu_buffer->event_stamp));",
            "",
            "\tcpu_buffer->lost_events = 0;",
            "\tcpu_buffer->last_overrun = 0;",
            "",
            "\trb_head_page_activate(cpu_buffer);",
            "\tcpu_buffer->pages_removed = 0;",
            "}"
          ],
          "function_name": "rb_reader_unlock, ring_buffer_iter_dropped, ring_buffer_read_prepare_sync, ring_buffer_read_start, ring_buffer_read_finish, ring_buffer_iter_advance, ring_buffer_size, rb_clear_buffer_page, rb_reset_cpu",
          "description": "该代码段实现环形缓冲区（ring buffer）的读取控制与状态管理。  \n其中 `rb_reader_unlock` 管理读锁释放，`ring_buffer_read_start` 和 `ring_buffer_read_finish` 负责读取前的锁同步与资源清理，`rb_reset_cpu` 用于重置指定 CPU 的缓冲区状态。  \n其他函数如 `ring_buffer_iter_advance` 推进迭代器位置，`ring_buffer_size` 计算缓冲区容量，`rb_clear_buffer_page` 清空页面数据，共同支撑事件追踪的并发安全与状态维护。",
          "similarity": 0.6165286898612976
        },
        {
          "chunk_id": 23,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 5310,
          "end_line": 5415,
          "content": [
            "static void reset_disabled_cpu_buffer(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "",
            "\tif (RB_WARN_ON(cpu_buffer, local_read(&cpu_buffer->committing)))",
            "\t\tgoto out;",
            "",
            "\tarch_spin_lock(&cpu_buffer->lock);",
            "",
            "\trb_reset_cpu(cpu_buffer);",
            "",
            "\tarch_spin_unlock(&cpu_buffer->lock);",
            "",
            " out:",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "}",
            "void ring_buffer_reset_cpu(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];",
            "",
            "\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\treturn;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tatomic_inc(&cpu_buffer->resize_disabled);",
            "\tatomic_inc(&cpu_buffer->record_disabled);",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\tatomic_dec(&cpu_buffer->record_disabled);",
            "\tatomic_dec(&cpu_buffer->resize_disabled);",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}",
            "void ring_buffer_reset_online_cpus(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tfor_each_online_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\tatomic_add(RESET_BIT, &cpu_buffer->resize_disabled);",
            "\t\tatomic_inc(&cpu_buffer->record_disabled);",
            "\t}",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\t/*",
            "\t\t * If a CPU came online during the synchronize_rcu(), then",
            "\t\t * ignore it.",
            "\t\t */",
            "\t\tif (!(atomic_read(&cpu_buffer->resize_disabled) & RESET_BIT))",
            "\t\t\tcontinue;",
            "",
            "\t\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\t\tatomic_dec(&cpu_buffer->record_disabled);",
            "\t\tatomic_sub(RESET_BIT, &cpu_buffer->resize_disabled);",
            "\t}",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}",
            "void ring_buffer_reset(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\tatomic_inc(&cpu_buffer->resize_disabled);",
            "\t\tatomic_inc(&cpu_buffer->record_disabled);",
            "\t}",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\t\tatomic_dec(&cpu_buffer->record_disabled);",
            "\t\tatomic_dec(&cpu_buffer->resize_disabled);",
            "\t}",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}"
          ],
          "function_name": "reset_disabled_cpu_buffer, ring_buffer_reset_cpu, ring_buffer_reset_online_cpus, ring_buffer_reset",
          "description": "该代码段实现了对跟踪环形缓冲区的重置机制，通过原子操作与RCU同步确保多线程安全。  \n`reset_disabled_cpu_buffer`负责安全地重置指定CPU的缓冲区，`ring_buffer_reset_cpu`、`ring_buffer_reset_online_cpus`和`ring_buffer_reset`分别针对单个CPU、在线CPU及全系统CPU执行重置，均通过原子计数器控制访问权限并阻塞数据提交。  \n由于`rb_reset_cpu`未在当前代码片段中定义，故其具体行为依赖上下文信息。",
          "similarity": 0.6146184802055359
        }
      ]
    },
    {
      "source_file": "kernel/printk/printk_ringbuffer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:34:17\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `printk\\printk_ringbuffer.c`\n\n---\n\n# printk_ringbuffer.c 技术文档\n\n## 文件概述\n\n`printk_ringbuffer.c` 实现了 Linux 内核中用于日志记录的无锁环形缓冲区（printk ringbuffer）核心逻辑。该缓冲区用于高效、并发安全地存储内核日志消息（printk 输出），支持多写者-多读者模型，无需使用传统锁机制，从而在高并发或中断上下文中也能安全使用。该实现是现代 printk 子系统的基础组件，用于替代旧的 log_buf。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`printk_ringbuffer`**：顶层环形缓冲区结构，包含三个内部环形缓冲区：\n  - **`desc_ring`**：描述符环，存储每条日志记录的元数据（序列号、时间戳、日志级别、状态等）及指向文本数据的逻辑位置。\n  - **`text_data_ring`**：文本数据环，以字节为单位存储日志文本内容，每个数据块以描述符 ID 开头，后接实际文本。\n  - **`info` 数组**：与描述符一一对应的 `printk_info` 结构数组，存储日志记录的详细元数据。\n\n- **描述符状态（`state_var`）**：\n  - `reserved`：写者正在修改记录。\n  - `committed`：记录已提交，数据一致，但可被原写者重新打开修改。\n  - `finalized`：记录已最终确定，对读者可见，不可再修改。\n  - `reusable`：记录可被回收复用。\n  - `miss`（伪状态）：查询时发现描述符 ID 不匹配。\n\n- **`blk_lpos`**：逻辑位置结构，用于在数据环中定位数据块的起始和结束位置。\n\n### 主要函数（接口）\n\n- `prb_reserve()`：为新日志记录预留空间，返回保留条目。\n- `prb_commit()`：提交当前记录（可后续重新打开）。\n- `prb_final_commit()`：提交并最终确定记录，使其对读者可见。\n- `prb_read_valid()` / `prb_read_valid_info()`：安全读取指定序列号的日志记录及其元数据。\n- `prb_first_valid_seq()` / `prb_next_seq()`：获取有效日志序列范围。\n\n## 关键实现\n\n### 无锁同步机制\n\n通过原子操作更新描述符的 `state_var` 字段（将 ID 与状态位打包），实现写者与读者之间的无锁同步。状态转换遵循严格顺序：`reserved → committed → finalized → reusable`。\n\n### 描述符生命周期管理\n\n- **预留（Reserve）**：分配新描述符，状态设为 `reserved`。\n- **提交（Commit）**：写入完成后设为 `committed`，数据一致但可重入。\n- **最终确定（Finalize）**：在以下任一情况下自动或显式触发：\n  1. 调用 `prb_final_commit()`；\n  2. 下一条记录被预留且当前记录已 `committed`；\n  3. 提交一条记录时已有更新记录存在。\n- **回收（Reuse）**：缓冲区满时，将最旧的 `finalized` 或 `reusable` 记录状态转为 `reusable`，并推进 `tail_id`。\n\n### 数据环的环绕处理\n\n当日志文本跨越缓冲区末尾时，仅在末尾存储描述符 ID，完整数据块（ID + 文本）从缓冲区起始位置存储。`blk_lpos` 正确指向环绕前的 ID 位置，保证逻辑连续性。\n\n### 尾部推进安全约束\n\n`tail_id` 和 `tail_lpos` 仅在对应记录处于 `committed` 或 `reusable` 状态时才可推进，确保始终保留至少一条有效日志的序列号，避免读者读取到无效数据。\n\n### 元数据一致性保障\n\n读取 `printk_info` 时，需在读取前后两次检查对应描述符状态，确保元数据未在读取过程中被覆盖或修改（ABA 问题防护）。\n\n## 依赖关系\n\n- **内部依赖**：\n  - `printk_ringbuffer.h`：定义核心数据结构和 API。\n  - `internal.h`：包含 printk 子系统内部辅助函数和定义。\n- **内核头文件**：\n  - `<linux/kernel.h>`、`<linux/irqflags.h>`、`<linux/string.h>`、`<linux/bug.h>`：提供基础内核功能、原子操作、内存操作及调试支持。\n- **被 printk.c 调用**：作为 printk 日志后端，由 `printk.c` 中的 `vprintk_store()` 等函数调用其预留/提交接口。\n\n## 使用场景\n\n- **内核日志记录**：所有 `printk()` 调用最终通过此环形缓冲区存储日志消息。\n- **高并发环境**：在中断上下文、NMI、SMP 系统中安全记录日志，无需睡眠或持有自旋锁。\n- **日志读取**：`/dev/kmsg`、`dmesg` 命令及内核日志守护进程通过此缓冲区读取日志。\n- **崩溃转储**：在系统崩溃（如 panic）时，确保关键日志能被可靠记录和后续分析。\n- **动态日志扩展**：支持在提交后、最终确定前扩展日志内容（如追加堆栈信息），适用于延迟格式化场景。",
      "similarity": 0.5629606246948242,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 1006,
          "end_line": 1134,
          "content": [
            "static unsigned long get_next_lpos(struct prb_data_ring *data_ring,",
            "\t\t\t\t   unsigned long lpos, unsigned int size)",
            "{",
            "\tunsigned long begin_lpos;",
            "\tunsigned long next_lpos;",
            "",
            "\tbegin_lpos = lpos;",
            "\tnext_lpos = lpos + size;",
            "",
            "\t/* First check if the data block does not wrap. */",
            "\tif (DATA_WRAPS(data_ring, begin_lpos) == DATA_WRAPS(data_ring, next_lpos))",
            "\t\treturn next_lpos;",
            "",
            "\t/* Wrapping data blocks store their data at the beginning. */",
            "\treturn (DATA_THIS_WRAP_START_LPOS(data_ring, next_lpos) + size);",
            "}",
            "static unsigned int space_used(struct prb_data_ring *data_ring,",
            "\t\t\t       struct prb_data_blk_lpos *blk_lpos)",
            "{",
            "\t/* Data-less blocks take no space. */",
            "\tif (BLK_DATALESS(blk_lpos))",
            "\t\treturn 0;",
            "",
            "\tif (DATA_WRAPS(data_ring, blk_lpos->begin) == DATA_WRAPS(data_ring, blk_lpos->next)) {",
            "\t\t/* Data block does not wrap. */",
            "\t\treturn (DATA_INDEX(data_ring, blk_lpos->next) -",
            "\t\t\tDATA_INDEX(data_ring, blk_lpos->begin));",
            "\t}",
            "",
            "\t/*",
            "\t * For wrapping data blocks, the trailing (wasted) space is",
            "\t * also counted.",
            "\t */",
            "\treturn (DATA_INDEX(data_ring, blk_lpos->next) +",
            "\t\tDATA_SIZE(data_ring) - DATA_INDEX(data_ring, blk_lpos->begin));",
            "}",
            "bool prb_reserve_in_last(struct prb_reserved_entry *e, struct printk_ringbuffer *rb,",
            "\t\t\t struct printk_record *r, u32 caller_id, unsigned int max_size)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tstruct printk_info *info;",
            "\tunsigned int data_size;",
            "\tstruct prb_desc *d;",
            "\tunsigned long id;",
            "",
            "\tlocal_irq_save(e->irqflags);",
            "",
            "\t/* Transition the newest descriptor back to the reserved state. */",
            "\td = desc_reopen_last(desc_ring, caller_id, &id);",
            "\tif (!d) {",
            "\t\tlocal_irq_restore(e->irqflags);",
            "\t\tgoto fail_reopen;",
            "\t}",
            "",
            "\t/* Now the writer has exclusive access: LMM(prb_reserve_in_last:A) */",
            "",
            "\tinfo = to_info(desc_ring, id);",
            "",
            "\t/*",
            "\t * Set the @e fields here so that prb_commit() can be used if",
            "\t * anything fails from now on.",
            "\t */",
            "\te->rb = rb;",
            "\te->id = id;",
            "",
            "\t/*",
            "\t * desc_reopen_last() checked the caller_id, but there was no",
            "\t * exclusive access at that point. The descriptor may have",
            "\t * changed since then.",
            "\t */",
            "\tif (caller_id != info->caller_id)",
            "\t\tgoto fail;",
            "",
            "\tif (BLK_DATALESS(&d->text_blk_lpos)) {",
            "\t\tif (WARN_ON_ONCE(info->text_len != 0)) {",
            "\t\t\tpr_warn_once(\"wrong text_len value (%hu, expecting 0)\\n\",",
            "\t\t\t\t     info->text_len);",
            "\t\t\tinfo->text_len = 0;",
            "\t\t}",
            "",
            "\t\tif (!data_check_size(&rb->text_data_ring, r->text_buf_size))",
            "\t\t\tgoto fail;",
            "",
            "\t\tif (r->text_buf_size > max_size)",
            "\t\t\tgoto fail;",
            "",
            "\t\tr->text_buf = data_alloc(rb, r->text_buf_size,",
            "\t\t\t\t\t &d->text_blk_lpos, id);",
            "\t} else {",
            "\t\tif (!get_data(&rb->text_data_ring, &d->text_blk_lpos, &data_size))",
            "\t\t\tgoto fail;",
            "",
            "\t\t/*",
            "\t\t * Increase the buffer size to include the original size. If",
            "\t\t * the meta data (@text_len) is not sane, use the full data",
            "\t\t * block size.",
            "\t\t */",
            "\t\tif (WARN_ON_ONCE(info->text_len > data_size)) {",
            "\t\t\tpr_warn_once(\"wrong text_len value (%hu, expecting <=%u)\\n\",",
            "\t\t\t\t     info->text_len, data_size);",
            "\t\t\tinfo->text_len = data_size;",
            "\t\t}",
            "\t\tr->text_buf_size += info->text_len;",
            "",
            "\t\tif (!data_check_size(&rb->text_data_ring, r->text_buf_size))",
            "\t\t\tgoto fail;",
            "",
            "\t\tif (r->text_buf_size > max_size)",
            "\t\t\tgoto fail;",
            "",
            "\t\tr->text_buf = data_realloc(rb, r->text_buf_size,",
            "\t\t\t\t\t   &d->text_blk_lpos, id);",
            "\t}",
            "\tif (r->text_buf_size && !r->text_buf)",
            "\t\tgoto fail;",
            "",
            "\tr->info = info;",
            "",
            "\te->text_space = space_used(&rb->text_data_ring, &d->text_blk_lpos);",
            "",
            "\treturn true;",
            "fail:",
            "\tprb_commit(e);",
            "\t/* prb_commit() re-enabled interrupts. */",
            "fail_reopen:",
            "\t/* Make it clear to the caller that the re-reserve failed. */",
            "\tmemset(r, 0, sizeof(*r));",
            "\treturn false;",
            "}"
          ],
          "function_name": "get_next_lpos, space_used, prb_reserve_in_last",
          "description": "实现用于计算数据环形缓冲区中下一个逻辑位置、已用空间及预留数据块的操作。get_next_lpos确定数据块结束位置，space_used计算数据块占用空间，prb_reserve_in_last为记录预留数据空间并处理分配失败情况。",
          "similarity": 0.603223979473114
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 2179,
          "end_line": 2259,
          "content": [
            "bool prb_read_valid(struct printk_ringbuffer *rb, u64 seq,",
            "\t\t    struct printk_record *r)",
            "{",
            "\treturn _prb_read_valid(rb, &seq, r, NULL);",
            "}",
            "bool prb_read_valid_info(struct printk_ringbuffer *rb, u64 seq,",
            "\t\t\t struct printk_info *info, unsigned int *line_count)",
            "{",
            "\tstruct printk_record r;",
            "",
            "\tprb_rec_init_rd(&r, info, NULL, 0);",
            "",
            "\treturn _prb_read_valid(rb, &seq, &r, line_count);",
            "}",
            "u64 prb_first_valid_seq(struct printk_ringbuffer *rb)",
            "{",
            "\tu64 seq = 0;",
            "",
            "\tif (!_prb_read_valid(rb, &seq, NULL, NULL))",
            "\t\treturn 0;",
            "",
            "\treturn seq;",
            "}",
            "u64 prb_next_seq(struct printk_ringbuffer *rb)",
            "{",
            "\tu64 seq;",
            "",
            "\tseq = desc_last_finalized_seq(rb);",
            "",
            "\t/*",
            "\t * Begin searching after the last finalized record.",
            "\t *",
            "\t * On 0, the search must begin at 0 because of hack#2",
            "\t * of the bootstrapping phase it is not known if a",
            "\t * record at index 0 exists.",
            "\t */",
            "\tif (seq != 0)",
            "\t\tseq++;",
            "",
            "\t/*",
            "\t * The information about the last finalized @seq might be inaccurate.",
            "\t * Search forward to find the current one.",
            "\t */",
            "\twhile (_prb_read_valid(rb, &seq, NULL, NULL))",
            "\t\tseq++;",
            "",
            "\treturn seq;",
            "}",
            "void prb_init(struct printk_ringbuffer *rb,",
            "\t      char *text_buf, unsigned int textbits,",
            "\t      struct prb_desc *descs, unsigned int descbits,",
            "\t      struct printk_info *infos)",
            "{",
            "\tmemset(descs, 0, _DESCS_COUNT(descbits) * sizeof(descs[0]));",
            "\tmemset(infos, 0, _DESCS_COUNT(descbits) * sizeof(infos[0]));",
            "",
            "\trb->desc_ring.count_bits = descbits;",
            "\trb->desc_ring.descs = descs;",
            "\trb->desc_ring.infos = infos;",
            "\tatomic_long_set(&rb->desc_ring.head_id, DESC0_ID(descbits));",
            "\tatomic_long_set(&rb->desc_ring.tail_id, DESC0_ID(descbits));",
            "\tatomic_long_set(&rb->desc_ring.last_finalized_seq, 0);",
            "",
            "\trb->text_data_ring.size_bits = textbits;",
            "\trb->text_data_ring.data = text_buf;",
            "\tatomic_long_set(&rb->text_data_ring.head_lpos, BLK0_LPOS(textbits));",
            "\tatomic_long_set(&rb->text_data_ring.tail_lpos, BLK0_LPOS(textbits));",
            "",
            "\tatomic_long_set(&rb->fail, 0);",
            "",
            "\tatomic_long_set(&(descs[_DESCS_COUNT(descbits) - 1].state_var), DESC0_SV(descbits));",
            "\tdescs[_DESCS_COUNT(descbits) - 1].text_blk_lpos.begin = FAILED_LPOS;",
            "\tdescs[_DESCS_COUNT(descbits) - 1].text_blk_lpos.next = FAILED_LPOS;",
            "",
            "\tinfos[0].seq = -(u64)_DESCS_COUNT(descbits);",
            "\tinfos[_DESCS_COUNT(descbits) - 1].seq = 0;",
            "}",
            "unsigned int prb_record_text_space(struct prb_reserved_entry *e)",
            "{",
            "\treturn e->text_space;",
            "}"
          ],
          "function_name": "prb_read_valid, prb_read_valid_info, prb_first_valid_seq, prb_next_seq, prb_init, prb_record_text_space",
          "description": "该代码段实现了 printk 环形缓冲区的读写控制逻辑，主要包含以下内容：  \n1. `prb_read_valid` 系列函数通过 `_prb_read_valid` 检查序列号有效性并填充记录信息，用于日志读取校验；  \n2. `prb_first_valid_seq` 和 `prb_next_seq` 用于定位首个有效序列号与后续序列号，解决缓冲区初始状态不确定的问题；  \n3. `prb_init` 初始化缓冲区结构，配置描述符环和文本数据环的元数据，`prb_record_text_space` 返回记录文本空间大小。注：`_prb_read_valid` 实现未完整展示，需结合上下文理解其核心逻辑。",
          "similarity": 0.5834264755249023
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 771,
          "end_line": 876,
          "content": [
            "static bool desc_push_tail(struct printk_ringbuffer *rb,",
            "\t\t\t   unsigned long tail_id)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tenum desc_state d_state;",
            "\tstruct prb_desc desc;",
            "",
            "\td_state = desc_read(desc_ring, tail_id, &desc, NULL, NULL);",
            "",
            "\tswitch (d_state) {",
            "\tcase desc_miss:",
            "\t\t/*",
            "\t\t * If the ID is exactly 1 wrap behind the expected, it is",
            "\t\t * in the process of being reserved by another writer and",
            "\t\t * must be considered reserved.",
            "\t\t */",
            "\t\tif (DESC_ID(atomic_long_read(&desc.state_var)) ==",
            "\t\t    DESC_ID_PREV_WRAP(desc_ring, tail_id)) {",
            "\t\t\treturn false;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * The ID has changed. Another writer must have pushed the",
            "\t\t * tail and recycled the descriptor already. Success is",
            "\t\t * returned because the caller is only interested in the",
            "\t\t * specified tail being pushed, which it was.",
            "\t\t */",
            "\t\treturn true;",
            "\tcase desc_reserved:",
            "\tcase desc_committed:",
            "\t\treturn false;",
            "\tcase desc_finalized:",
            "\t\tdesc_make_reusable(desc_ring, tail_id);",
            "\t\tbreak;",
            "\tcase desc_reusable:",
            "\t\tbreak;",
            "\t}",
            "",
            "\t/*",
            "\t * Data blocks must be invalidated before their associated",
            "\t * descriptor can be made available for recycling. Invalidating",
            "\t * them later is not possible because there is no way to trust",
            "\t * data blocks once their associated descriptor is gone.",
            "\t */",
            "",
            "\tif (!data_push_tail(rb, desc.text_blk_lpos.next))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Check the next descriptor after @tail_id before pushing the tail",
            "\t * to it because the tail must always be in a finalized or reusable",
            "\t * state. The implementation of prb_first_seq() relies on this.",
            "\t *",
            "\t * A successful read implies that the next descriptor is less than or",
            "\t * equal to @head_id so there is no risk of pushing the tail past the",
            "\t * head.",
            "\t */",
            "\td_state = desc_read(desc_ring, DESC_ID(tail_id + 1), &desc,",
            "\t\t\t    NULL, NULL); /* LMM(desc_push_tail:A) */",
            "",
            "\tif (d_state == desc_finalized || d_state == desc_reusable) {",
            "\t\t/*",
            "\t\t * Guarantee any descriptor states that have transitioned to",
            "\t\t * reusable are stored before pushing the tail ID. This allows",
            "\t\t * verifying the recycled descriptor state. A full memory",
            "\t\t * barrier is needed since other CPUs may have made the",
            "\t\t * descriptor states reusable. This pairs with desc_reserve:D.",
            "\t\t */",
            "\t\tatomic_long_cmpxchg(&desc_ring->tail_id, tail_id,",
            "\t\t\t\t    DESC_ID(tail_id + 1)); /* LMM(desc_push_tail:B) */",
            "\t} else {",
            "\t\t/*",
            "\t\t * Guarantee the last state load from desc_read() is before",
            "\t\t * reloading @tail_id in order to see a new tail ID in the",
            "\t\t * case that the descriptor has been recycled. This pairs",
            "\t\t * with desc_reserve:D.",
            "\t\t *",
            "\t\t * Memory barrier involvement:",
            "\t\t *",
            "\t\t * If desc_push_tail:A reads from desc_reserve:F, then",
            "\t\t * desc_push_tail:D reads from desc_push_tail:B.",
            "\t\t *",
            "\t\t * Relies on:",
            "\t\t *",
            "\t\t * MB from desc_push_tail:B to desc_reserve:F",
            "\t\t *    matching",
            "\t\t * RMB from desc_push_tail:A to desc_push_tail:D",
            "\t\t *",
            "\t\t * Note: desc_push_tail:B and desc_reserve:F can be different",
            "\t\t *       CPUs. However, the desc_reserve:F CPU (which performs",
            "\t\t *       the full memory barrier) must have previously seen",
            "\t\t *       desc_push_tail:B.",
            "\t\t */",
            "\t\tsmp_rmb(); /* LMM(desc_push_tail:C) */",
            "",
            "\t\t/*",
            "\t\t * Re-check the tail ID. The descriptor following @tail_id is",
            "\t\t * not in an allowed tail state. But if the tail has since",
            "\t\t * been moved by another CPU, then it does not matter.",
            "\t\t */",
            "\t\tif (atomic_long_read(&desc_ring->tail_id) == tail_id) /* LMM(desc_push_tail:D) */",
            "\t\t\treturn false;",
            "\t}",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "desc_push_tail",
          "description": "推进描述符环尾指针，检查后续描述符状态合法性，通过内存屏障保障状态变更顺序以避免非法尾指针推进",
          "similarity": 0.5478522181510925
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 383,
          "end_line": 521,
          "content": [
            "static unsigned int to_blk_size(unsigned int size)",
            "{",
            "\tstruct prb_data_block *db = NULL;",
            "",
            "\tsize += sizeof(*db);",
            "\tsize = ALIGN(size, sizeof(db->id));",
            "\treturn size;",
            "}",
            "static bool data_check_size(struct prb_data_ring *data_ring, unsigned int size)",
            "{",
            "\tstruct prb_data_block *db = NULL;",
            "",
            "\tif (size == 0)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Ensure the alignment padded size could possibly fit in the data",
            "\t * array. The largest possible data block must still leave room for",
            "\t * at least the ID of the next block.",
            "\t */",
            "\tsize = to_blk_size(size);",
            "\tif (size > DATA_SIZE(data_ring) - sizeof(db->id))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static enum desc_state get_desc_state(unsigned long id,",
            "\t\t\t\t      unsigned long state_val)",
            "{",
            "\tif (id != DESC_ID(state_val))",
            "\t\treturn desc_miss;",
            "",
            "\treturn DESC_STATE(state_val);",
            "}",
            "static enum desc_state desc_read(struct prb_desc_ring *desc_ring,",
            "\t\t\t\t unsigned long id, struct prb_desc *desc_out,",
            "\t\t\t\t u64 *seq_out, u32 *caller_id_out)",
            "{",
            "\tstruct printk_info *info = to_info(desc_ring, id);",
            "\tstruct prb_desc *desc = to_desc(desc_ring, id);",
            "\tatomic_long_t *state_var = &desc->state_var;",
            "\tenum desc_state d_state;",
            "\tunsigned long state_val;",
            "",
            "\t/* Check the descriptor state. */",
            "\tstate_val = atomic_long_read(state_var); /* LMM(desc_read:A) */",
            "\td_state = get_desc_state(id, state_val);",
            "\tif (d_state == desc_miss || d_state == desc_reserved) {",
            "\t\t/*",
            "\t\t * The descriptor is in an inconsistent state. Set at least",
            "\t\t * @state_var so that the caller can see the details of",
            "\t\t * the inconsistent state.",
            "\t\t */",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/*",
            "\t * Guarantee the state is loaded before copying the descriptor",
            "\t * content. This avoids copying obsolete descriptor content that might",
            "\t * not apply to the descriptor state. This pairs with _prb_commit:B.",
            "\t *",
            "\t * Memory barrier involvement:",
            "\t *",
            "\t * If desc_read:A reads from _prb_commit:B, then desc_read:C reads",
            "\t * from _prb_commit:A.",
            "\t *",
            "\t * Relies on:",
            "\t *",
            "\t * WMB from _prb_commit:A to _prb_commit:B",
            "\t *    matching",
            "\t * RMB from desc_read:A to desc_read:C",
            "\t */",
            "\tsmp_rmb(); /* LMM(desc_read:B) */",
            "",
            "\t/*",
            "\t * Copy the descriptor data. The data is not valid until the",
            "\t * state has been re-checked. A memcpy() for all of @desc",
            "\t * cannot be used because of the atomic_t @state_var field.",
            "\t */",
            "\tif (desc_out) {",
            "\t\tmemcpy(&desc_out->text_blk_lpos, &desc->text_blk_lpos,",
            "\t\t       sizeof(desc_out->text_blk_lpos)); /* LMM(desc_read:C) */",
            "\t}",
            "\tif (seq_out)",
            "\t\t*seq_out = info->seq; /* also part of desc_read:C */",
            "\tif (caller_id_out)",
            "\t\t*caller_id_out = info->caller_id; /* also part of desc_read:C */",
            "",
            "\t/*",
            "\t * 1. Guarantee the descriptor content is loaded before re-checking",
            "\t *    the state. This avoids reading an obsolete descriptor state",
            "\t *    that may not apply to the copied content. This pairs with",
            "\t *    desc_reserve:F.",
            "\t *",
            "\t *    Memory barrier involvement:",
            "\t *",
            "\t *    If desc_read:C reads from desc_reserve:G, then desc_read:E",
            "\t *    reads from desc_reserve:F.",
            "\t *",
            "\t *    Relies on:",
            "\t *",
            "\t *    WMB from desc_reserve:F to desc_reserve:G",
            "\t *       matching",
            "\t *    RMB from desc_read:C to desc_read:E",
            "\t *",
            "\t * 2. Guarantee the record data is loaded before re-checking the",
            "\t *    state. This avoids reading an obsolete descriptor state that may",
            "\t *    not apply to the copied data. This pairs with data_alloc:A and",
            "\t *    data_realloc:A.",
            "\t *",
            "\t *    Memory barrier involvement:",
            "\t *",
            "\t *    If copy_data:A reads from data_alloc:B, then desc_read:E",
            "\t *    reads from desc_make_reusable:A.",
            "\t *",
            "\t *    Relies on:",
            "\t *",
            "\t *    MB from desc_make_reusable:A to data_alloc:B",
            "\t *       matching",
            "\t *    RMB from desc_read:C to desc_read:E",
            "\t *",
            "\t *    Note: desc_make_reusable:A and data_alloc:B can be different",
            "\t *          CPUs. However, the data_alloc:B CPU (which performs the",
            "\t *          full memory barrier) must have previously seen",
            "\t *          desc_make_reusable:A.",
            "\t */",
            "\tsmp_rmb(); /* LMM(desc_read:D) */",
            "",
            "\t/*",
            "\t * The data has been copied. Return the current descriptor state,",
            "\t * which may have changed since the load above.",
            "\t */",
            "\tstate_val = atomic_long_read(state_var); /* LMM(desc_read:E) */",
            "\td_state = get_desc_state(id, state_val);",
            "out:",
            "\tif (desc_out)",
            "\t\tatomic_long_set(&desc_out->state_var, state_val);",
            "\treturn d_state;",
            "}"
          ],
          "function_name": "to_blk_size, data_check_size, get_desc_state, desc_read",
          "description": "实现数据块大小计算、数据大小验证、描述符状态获取及描述符读取逻辑，含多处内存屏障保障状态一致性",
          "similarity": 0.5367937088012695
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 1,
          "end_line": 382,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "",
            "#include <linux/kernel.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/string.h>",
            "#include <linux/errno.h>",
            "#include <linux/bug.h>",
            "#include \"printk_ringbuffer.h\"",
            "#include \"internal.h\"",
            "",
            "/**",
            " * DOC: printk_ringbuffer overview",
            " *",
            " * Data Structure",
            " * --------------",
            " * The printk_ringbuffer is made up of 3 internal ringbuffers:",
            " *",
            " *   desc_ring",
            " *     A ring of descriptors and their meta data (such as sequence number,",
            " *     timestamp, loglevel, etc.) as well as internal state information about",
            " *     the record and logical positions specifying where in the other",
            " *     ringbuffer the text strings are located.",
            " *",
            " *   text_data_ring",
            " *     A ring of data blocks. A data block consists of an unsigned long",
            " *     integer (ID) that maps to a desc_ring index followed by the text",
            " *     string of the record.",
            " *",
            " * The internal state information of a descriptor is the key element to allow",
            " * readers and writers to locklessly synchronize access to the data.",
            " *",
            " * Implementation",
            " * --------------",
            " *",
            " * Descriptor Ring",
            " * ~~~~~~~~~~~~~~~",
            " * The descriptor ring is an array of descriptors. A descriptor contains",
            " * essential meta data to track the data of a printk record using",
            " * blk_lpos structs pointing to associated text data blocks (see",
            " * \"Data Rings\" below). Each descriptor is assigned an ID that maps",
            " * directly to index values of the descriptor array and has a state. The ID",
            " * and the state are bitwise combined into a single descriptor field named",
            " * @state_var, allowing ID and state to be synchronously and atomically",
            " * updated.",
            " *",
            " * Descriptors have four states:",
            " *",
            " *   reserved",
            " *     A writer is modifying the record.",
            " *",
            " *   committed",
            " *     The record and all its data are written. A writer can reopen the",
            " *     descriptor (transitioning it back to reserved), but in the committed",
            " *     state the data is consistent.",
            " *",
            " *   finalized",
            " *     The record and all its data are complete and available for reading. A",
            " *     writer cannot reopen the descriptor.",
            " *",
            " *   reusable",
            " *     The record exists, but its text and/or meta data may no longer be",
            " *     available.",
            " *",
            " * Querying the @state_var of a record requires providing the ID of the",
            " * descriptor to query. This can yield a possible fifth (pseudo) state:",
            " *",
            " *   miss",
            " *     The descriptor being queried has an unexpected ID.",
            " *",
            " * The descriptor ring has a @tail_id that contains the ID of the oldest",
            " * descriptor and @head_id that contains the ID of the newest descriptor.",
            " *",
            " * When a new descriptor should be created (and the ring is full), the tail",
            " * descriptor is invalidated by first transitioning to the reusable state and",
            " * then invalidating all tail data blocks up to and including the data blocks",
            " * associated with the tail descriptor (for the text ring). Then",
            " * @tail_id is advanced, followed by advancing @head_id. And finally the",
            " * @state_var of the new descriptor is initialized to the new ID and reserved",
            " * state.",
            " *",
            " * The @tail_id can only be advanced if the new @tail_id would be in the",
            " * committed or reusable queried state. This makes it possible that a valid",
            " * sequence number of the tail is always available.",
            " *",
            " * Descriptor Finalization",
            " * ~~~~~~~~~~~~~~~~~~~~~~~",
            " * When a writer calls the commit function prb_commit(), record data is",
            " * fully stored and is consistent within the ringbuffer. However, a writer can",
            " * reopen that record, claiming exclusive access (as with prb_reserve()), and",
            " * modify that record. When finished, the writer must again commit the record.",
            " *",
            " * In order for a record to be made available to readers (and also become",
            " * recyclable for writers), it must be finalized. A finalized record cannot be",
            " * reopened and can never become \"unfinalized\". Record finalization can occur",
            " * in three different scenarios:",
            " *",
            " *   1) A writer can simultaneously commit and finalize its record by calling",
            " *      prb_final_commit() instead of prb_commit().",
            " *",
            " *   2) When a new record is reserved and the previous record has been",
            " *      committed via prb_commit(), that previous record is automatically",
            " *      finalized.",
            " *",
            " *   3) When a record is committed via prb_commit() and a newer record",
            " *      already exists, the record being committed is automatically finalized.",
            " *",
            " * Data Ring",
            " * ~~~~~~~~~",
            " * The text data ring is a byte array composed of data blocks. Data blocks are",
            " * referenced by blk_lpos structs that point to the logical position of the",
            " * beginning of a data block and the beginning of the next adjacent data",
            " * block. Logical positions are mapped directly to index values of the byte",
            " * array ringbuffer.",
            " *",
            " * Each data block consists of an ID followed by the writer data. The ID is",
            " * the identifier of a descriptor that is associated with the data block. A",
            " * given data block is considered valid if all of the following conditions",
            " * are met:",
            " *",
            " *   1) The descriptor associated with the data block is in the committed",
            " *      or finalized queried state.",
            " *",
            " *   2) The blk_lpos struct within the descriptor associated with the data",
            " *      block references back to the same data block.",
            " *",
            " *   3) The data block is within the head/tail logical position range.",
            " *",
            " * If the writer data of a data block would extend beyond the end of the",
            " * byte array, only the ID of the data block is stored at the logical",
            " * position and the full data block (ID and writer data) is stored at the",
            " * beginning of the byte array. The referencing blk_lpos will point to the",
            " * ID before the wrap and the next data block will be at the logical",
            " * position adjacent the full data block after the wrap.",
            " *",
            " * Data rings have a @tail_lpos that points to the beginning of the oldest",
            " * data block and a @head_lpos that points to the logical position of the",
            " * next (not yet existing) data block.",
            " *",
            " * When a new data block should be created (and the ring is full), tail data",
            " * blocks will first be invalidated by putting their associated descriptors",
            " * into the reusable state and then pushing the @tail_lpos forward beyond",
            " * them. Then the @head_lpos is pushed forward and is associated with a new",
            " * descriptor. If a data block is not valid, the @tail_lpos cannot be",
            " * advanced beyond it.",
            " *",
            " * Info Array",
            " * ~~~~~~~~~~",
            " * The general meta data of printk records are stored in printk_info structs,",
            " * stored in an array with the same number of elements as the descriptor ring.",
            " * Each info corresponds to the descriptor of the same index in the",
            " * descriptor ring. Info validity is confirmed by evaluating the corresponding",
            " * descriptor before and after loading the info.",
            " *",
            " * Usage",
            " * -----",
            " * Here are some simple examples demonstrating writers and readers. For the",
            " * examples a global ringbuffer (test_rb) is available (which is not the",
            " * actual ringbuffer used by printk)::",
            " *",
            " *\tDEFINE_PRINTKRB(test_rb, 15, 5);",
            " *",
            " * This ringbuffer allows up to 32768 records (2 ^ 15) and has a size of",
            " * 1 MiB (2 ^ (15 + 5)) for text data.",
            " *",
            " * Sample writer code::",
            " *",
            " *\tconst char *textstr = \"message text\";",
            " *\tstruct prb_reserved_entry e;",
            " *\tstruct printk_record r;",
            " *",
            " *\t// specify how much to allocate",
            " *\tprb_rec_init_wr(&r, strlen(textstr) + 1);",
            " *",
            " *\tif (prb_reserve(&e, &test_rb, &r)) {",
            " *\t\tsnprintf(r.text_buf, r.text_buf_size, \"%s\", textstr);",
            " *",
            " *\t\tr.info->text_len = strlen(textstr);",
            " *\t\tr.info->ts_nsec = local_clock();",
            " *\t\tr.info->caller_id = printk_caller_id();",
            " *",
            " *\t\t// commit and finalize the record",
            " *\t\tprb_final_commit(&e);",
            " *\t}",
            " *",
            " * Note that additional writer functions are available to extend a record",
            " * after it has been committed but not yet finalized. This can be done as",
            " * long as no new records have been reserved and the caller is the same.",
            " *",
            " * Sample writer code (record extending)::",
            " *",
            " *\t\t// alternate rest of previous example",
            " *",
            " *\t\tr.info->text_len = strlen(textstr);",
            " *\t\tr.info->ts_nsec = local_clock();",
            " *\t\tr.info->caller_id = printk_caller_id();",
            " *",
            " *\t\t// commit the record (but do not finalize yet)",
            " *\t\tprb_commit(&e);",
            " *\t}",
            " *",
            " *\t...",
            " *",
            " *\t// specify additional 5 bytes text space to extend",
            " *\tprb_rec_init_wr(&r, 5);",
            " *",
            " *\t// try to extend, but only if it does not exceed 32 bytes",
            " *\tif (prb_reserve_in_last(&e, &test_rb, &r, printk_caller_id(), 32)) {",
            " *\t\tsnprintf(&r.text_buf[r.info->text_len],",
            " *\t\t\t r.text_buf_size - r.info->text_len, \"hello\");",
            " *",
            " *\t\tr.info->text_len += 5;",
            " *",
            " *\t\t// commit and finalize the record",
            " *\t\tprb_final_commit(&e);",
            " *\t}",
            " *",
            " * Sample reader code::",
            " *",
            " *\tstruct printk_info info;",
            " *\tstruct printk_record r;",
            " *\tchar text_buf[32];",
            " *\tu64 seq;",
            " *",
            " *\tprb_rec_init_rd(&r, &info, &text_buf[0], sizeof(text_buf));",
            " *",
            " *\tprb_for_each_record(0, &test_rb, &seq, &r) {",
            " *\t\tif (info.seq != seq)",
            " *\t\t\tpr_warn(\"lost %llu records\\n\", info.seq - seq);",
            " *",
            " *\t\tif (info.text_len > r.text_buf_size) {",
            " *\t\t\tpr_warn(\"record %llu text truncated\\n\", info.seq);",
            " *\t\t\ttext_buf[r.text_buf_size - 1] = 0;",
            " *\t\t}",
            " *",
            " *\t\tpr_info(\"%llu: %llu: %s\\n\", info.seq, info.ts_nsec,",
            " *\t\t\t&text_buf[0]);",
            " *\t}",
            " *",
            " * Note that additional less convenient reader functions are available to",
            " * allow complex record access.",
            " *",
            " * ABA Issues",
            " * ~~~~~~~~~~",
            " * To help avoid ABA issues, descriptors are referenced by IDs (array index",
            " * values combined with tagged bits counting array wraps) and data blocks are",
            " * referenced by logical positions (array index values combined with tagged",
            " * bits counting array wraps). However, on 32-bit systems the number of",
            " * tagged bits is relatively small such that an ABA incident is (at least",
            " * theoretically) possible. For example, if 4 million maximally sized (1KiB)",
            " * printk messages were to occur in NMI context on a 32-bit system, the",
            " * interrupted context would not be able to recognize that the 32-bit integer",
            " * completely wrapped and thus represents a different data block than the one",
            " * the interrupted context expects.",
            " *",
            " * To help combat this possibility, additional state checking is performed",
            " * (such as using cmpxchg() even though set() would suffice). These extra",
            " * checks are commented as such and will hopefully catch any ABA issue that",
            " * a 32-bit system might experience.",
            " *",
            " * Memory Barriers",
            " * ~~~~~~~~~~~~~~~",
            " * Multiple memory barriers are used. To simplify proving correctness and",
            " * generating litmus tests, lines of code related to memory barriers",
            " * (loads, stores, and the associated memory barriers) are labeled::",
            " *",
            " *\tLMM(function:letter)",
            " *",
            " * Comments reference the labels using only the \"function:letter\" part.",
            " *",
            " * The memory barrier pairs and their ordering are:",
            " *",
            " *   desc_reserve:D / desc_reserve:B",
            " *     push descriptor tail (id), then push descriptor head (id)",
            " *",
            " *   desc_reserve:D / data_push_tail:B",
            " *     push data tail (lpos), then set new descriptor reserved (state)",
            " *",
            " *   desc_reserve:D / desc_push_tail:C",
            " *     push descriptor tail (id), then set new descriptor reserved (state)",
            " *",
            " *   desc_reserve:D / prb_first_seq:C",
            " *     push descriptor tail (id), then set new descriptor reserved (state)",
            " *",
            " *   desc_reserve:F / desc_read:D",
            " *     set new descriptor id and reserved (state), then allow writer changes",
            " *",
            " *   data_alloc:A (or data_realloc:A) / desc_read:D",
            " *     set old descriptor reusable (state), then modify new data block area",
            " *",
            " *   data_alloc:A (or data_realloc:A) / data_push_tail:B",
            " *     push data tail (lpos), then modify new data block area",
            " *",
            " *   _prb_commit:B / desc_read:B",
            " *     store writer changes, then set new descriptor committed (state)",
            " *",
            " *   desc_reopen_last:A / _prb_commit:B",
            " *     set descriptor reserved (state), then read descriptor data",
            " *",
            " *   _prb_commit:B / desc_reserve:D",
            " *     set new descriptor committed (state), then check descriptor head (id)",
            " *",
            " *   data_push_tail:D / data_push_tail:A",
            " *     set descriptor reusable (state), then push data tail (lpos)",
            " *",
            " *   desc_push_tail:B / desc_reserve:D",
            " *     set descriptor reusable (state), then push descriptor tail (id)",
            " *",
            " *   desc_update_last_finalized:A / desc_last_finalized_seq:A",
            " *     store finalized record, then set new highest finalized sequence number",
            " */",
            "",
            "#define DATA_SIZE(data_ring)\t\t_DATA_SIZE((data_ring)->size_bits)",
            "#define DATA_SIZE_MASK(data_ring)\t(DATA_SIZE(data_ring) - 1)",
            "",
            "#define DESCS_COUNT(desc_ring)\t\t_DESCS_COUNT((desc_ring)->count_bits)",
            "#define DESCS_COUNT_MASK(desc_ring)\t(DESCS_COUNT(desc_ring) - 1)",
            "",
            "/* Determine the data array index from a logical position. */",
            "#define DATA_INDEX(data_ring, lpos)\t((lpos) & DATA_SIZE_MASK(data_ring))",
            "",
            "/* Determine the desc array index from an ID or sequence number. */",
            "#define DESC_INDEX(desc_ring, n)\t((n) & DESCS_COUNT_MASK(desc_ring))",
            "",
            "/* Determine how many times the data array has wrapped. */",
            "#define DATA_WRAPS(data_ring, lpos)\t((lpos) >> (data_ring)->size_bits)",
            "",
            "/* Determine if a logical position refers to a data-less block. */",
            "#define LPOS_DATALESS(lpos)\t\t((lpos) & 1UL)",
            "#define BLK_DATALESS(blk)\t\t(LPOS_DATALESS((blk)->begin) && \\",
            "\t\t\t\t\t LPOS_DATALESS((blk)->next))",
            "",
            "/* Get the logical position at index 0 of the current wrap. */",
            "#define DATA_THIS_WRAP_START_LPOS(data_ring, lpos) \\",
            "((lpos) & ~DATA_SIZE_MASK(data_ring))",
            "",
            "/* Get the ID for the same index of the previous wrap as the given ID. */",
            "#define DESC_ID_PREV_WRAP(desc_ring, id) \\",
            "DESC_ID((id) - DESCS_COUNT(desc_ring))",
            "",
            "/*",
            " * A data block: mapped directly to the beginning of the data block area",
            " * specified as a logical position within the data ring.",
            " *",
            " * @id:   the ID of the associated descriptor",
            " * @data: the writer data",
            " *",
            " * Note that the size of a data block is only known by its associated",
            " * descriptor.",
            " */",
            "struct prb_data_block {",
            "\tunsigned long\tid;",
            "\tchar\t\tdata[];",
            "};",
            "",
            "/*",
            " * Return the descriptor associated with @n. @n can be either a",
            " * descriptor ID or a sequence number.",
            " */",
            "static struct prb_desc *to_desc(struct prb_desc_ring *desc_ring, u64 n)",
            "{",
            "\treturn &desc_ring->descs[DESC_INDEX(desc_ring, n)];",
            "}",
            "",
            "/*",
            " * Return the printk_info associated with @n. @n can be either a",
            " * descriptor ID or a sequence number.",
            " */",
            "static struct printk_info *to_info(struct prb_desc_ring *desc_ring, u64 n)",
            "{",
            "\treturn &desc_ring->infos[DESC_INDEX(desc_ring, n)];",
            "}",
            "",
            "static struct prb_data_block *to_block(struct prb_data_ring *data_ring,",
            "\t\t\t\t       unsigned long begin_lpos)",
            "{",
            "\treturn (void *)&data_ring->data[DATA_INDEX(data_ring, begin_lpos)];",
            "}",
            "",
            "/*",
            " * Increase the data size to account for data block meta data plus any",
            " * padding so that the adjacent data block is aligned on the ID size.",
            " */"
          ],
          "function_name": null,
          "description": "定义printk_ringbuffer的数据结构和宏，用于管理描述符环与文本数据环，包含内存屏障标注和ABAA问题处理说明",
          "similarity": 0.5205179452896118
        }
      ]
    }
  ]
}