{
  "query": "虚拟内存地址转换过程",
  "timestamp": "2025-12-25 23:25:51",
  "retrieved_files": [
    {
      "source_file": "kernel/dma/direct.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:12:42\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\direct.c`\n\n---\n\n# `dma/direct.c` 技术文档\n\n## 1. 文件概述\n\n`dma/direct.c` 实现了 Linux 内核中 **DMA 直接映射操作（DMA direct mapping）** 的核心逻辑。该文件提供了一套不依赖 IOMMU 的 DMA 内存分配与映射机制，适用于物理地址可直接被设备访问的平台（如 x86、ARM64 等无 IOMMU 或 IOMMU 被禁用的场景）。其核心思想是将物理内存地址直接转换为设备可见的 DMA 地址，避免复杂的地址转换开销。\n\n该实现支持多种内存分配策略，包括：\n- 连续物理内存分配（CMA 或 buddy allocator）\n- SWIOTLB 回退机制（用于处理高地址设备无法访问的情况）\n- 原子池分配（用于不可阻塞上下文）\n- 高端内存重映射\n- 内存加密/解密（如 AMD SEV、Intel TDX 等安全特性）\n\n## 2. 核心功能\n\n### 全局变量\n- `zone_dma_bits`：定义 ZONE_DMA 的地址位宽（默认 24 位，即 16MB），可由架构代码覆盖。\n\n### 主要函数\n| 函数名 | 功能描述 |\n|--------|--------|\n| `phys_to_dma_direct()` | 将物理地址转换为 DMA 地址，支持强制解密场景 |\n| `dma_direct_to_page()` | 通过 DMA 地址反查对应的 `struct page` |\n| `dma_direct_get_required_mask()` | 计算设备所需的 DMA 地址掩码（基于系统最大物理地址）|\n| `dma_coherent_ok()` | 检查给定物理地址范围是否在设备的 DMA 地址能力范围内 |\n| `dma_direct_alloc()` | **主入口函数**：为设备分配 DMA 内存，支持多种属性和回退策略 |\n| `__dma_direct_alloc_pages()` | 底层页面分配函数，尝试最优内存区域并回退到低地址区域 |\n| `dma_direct_alloc_from_pool()` | 从原子池分配不可阻塞的 DMA 内存 |\n| `dma_direct_alloc_no_mapping()` | 分配无内核虚拟映射的 DMA 内存（返回 `struct page*`）|\n| `dma_set_decrypted()` / `dma_set_encrypted()` | 设置内存页为解密/加密状态（用于安全虚拟化）|\n\n### 辅助函数\n- `dma_direct_optimal_gfp_mask()`：根据设备 DMA 限制选择最优的 GFP 标志（GFP_DMA / GFP_DMA32）\n- `__dma_direct_free_pages()`：释放通过直接分配获得的页面（优先尝试 SWIOTLB 释放）\n\n## 3. 关键实现\n\n### 3.1 DMA 地址空间约束处理\n- 使用 `dev->coherent_dma_mask` 和 `dev->bus_dma_limit` 确定设备可寻址的物理地址上限。\n- 通过 `dma_coherent_ok()` 验证分配的物理内存是否在设备可访问范围内。\n- 若分配的内存超出范围，则回退到更低地址区域（先尝试 GFP_DMA32，再尝试 GFP_DMA）。\n\n### 3.2 多层次内存分配策略\n1. **首选 CMA 连续内存**：通过 `dma_alloc_contiguous()` 分配。\n2. **回退到 buddy allocator**：使用 `alloc_pages_node()`。\n3. **SWIOTLB 支持**：当设备无法访问高地址时，通过 `swiotlb_alloc()` 分配 bounce buffer。\n4. **原子上下文支持**：在不可阻塞场景下使用 `dma_direct_alloc_from_pool()` 从预分配池中获取内存。\n\n### 3.3 非一致性缓存与内存属性处理\n- 对于非一致性缓存架构（`!dev_is_dma_coherent()`）：\n  - 优先使用全局一致性内存池（`CONFIG_DMA_GLOBAL_POOL`）\n  - 或启用重映射（`CONFIG_DMA_DIRECT_REMAP`）创建 uncached 映射\n  - 或调用架构特定的 `arch_dma_alloc()`\n- 调用 `arch_dma_prep_coherent()` 清理内核别名的脏缓存行。\n\n### 3.4 安全内存处理（加密/解密）\n- 当 `force_dma_unencrypted(dev)` 为真（如 SEV 环境），分配的内存需标记为解密。\n- 使用 `set_memory_decrypted()` / `set_memory_encrypted()` 修改页表属性。\n- 解密操作可能阻塞，因此在原子上下文中需使用内存池。\n\n### 3.5 高端内存（HighMem）处理\n- 若分配的页面位于高端内存（`PageHighMem`），则必须通过 `dma_common_contiguous_remap()` 创建内核虚拟地址映射。\n- 重映射时应用设备特定的页保护属性（`dma_pgprot()`）并处理解密需求。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- `<linux/memblock.h>`：获取 `max_pfn` 系统最大页帧号\n- `<linux/dma-map-ops.h>`：DMA 映射操作抽象接口\n- `<linux/scatterlist.h>`：SG 表支持（间接依赖）\n- `<linux/set_memory.h>`：内存加密/解密操作（`set_memory_decrypted` 等）\n- `<linux/vmalloc.h>`：高端内存重映射支持\n- `\"direct.h\"`：本地 DMA direct 实现的私有头文件\n\n### 配置选项依赖\n- `CONFIG_SWIOTLB`：SWIOTLB bounce buffer 支持\n- `CONFIG_DMA_COHERENT_POOL`：原子上下文 DMA 内存池\n- `CONFIG_DMA_GLOBAL_POOL`：全局一致性 DMA 内存池\n- `CONFIG_DMA_DIRECT_REMAP`：非一致性设备的重映射支持\n- `CONFIG_ZONE_DMA` / `CONFIG_ZONE_DMA32`：低地址内存区域支持\n- `CONFIG_ARCH_HAS_DMA_SET_UNCACHED`：架构特定 uncached 映射支持\n\n### 架构相关依赖\n- `phys_to_dma()` / `dma_to_phys()`：架构提供的物理地址与 DMA 地址转换函数\n- `arch_dma_prep_coherent()`：架构特定的缓存一致性准备\n- `arch_dma_alloc()`：架构特定的 DMA 分配回退路径\n\n## 5. 使用场景\n\n### 5.1 设备驱动 DMA 分配\n- 驱动调用 `dma_alloc_coherent()` 或 `dma_alloc_attrs()` 时，若系统未启用 IOMMU，则最终由 `dma_direct_alloc()` 处理。\n- 适用于大多数无 IOMMU 的嵌入式系统、传统 PC 平台或 IOMMU 被显式禁用的场景。\n\n### 5.2 安全虚拟化环境\n- 在 AMD SEV 或 Intel TDX 等机密计算环境中，DMA 内存需标记为“解密”，该文件通过 `force_dma_unencrypted()` 机制实现。\n\n### 5.3 资源受限或实时系统\n- 通过 `DMA_ATTR_NO_KERNEL_MAPPING` 属性分配无内核映射的 DMA 内存，减少 TLB 压力。\n- 在中断上下文等不可阻塞场景中，自动使用原子内存池分配。\n\n### 5.4 老旧设备兼容\n- 对仅支持 32 位或 24 位 DMA 地址的设备，自动分配低地址内存（通过 GFP_DMA32/GFP_DMA）并验证地址范围。\n\n### 5.5 高端内存平台\n- 在 32 位系统或内存大于直接映射区域的平台上，自动处理高端内存的重映射，确保返回有效的内核虚拟地址。",
      "similarity": 0.5830478668212891,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/direct.c",
          "start_line": 25,
          "end_line": 140,
          "content": [
            "static inline dma_addr_t phys_to_dma_direct(struct device *dev,",
            "\t\tphys_addr_t phys)",
            "{",
            "\tif (force_dma_unencrypted(dev))",
            "\t\treturn phys_to_dma_unencrypted(dev, phys);",
            "\treturn phys_to_dma(dev, phys);",
            "}",
            "u64 dma_direct_get_required_mask(struct device *dev)",
            "{",
            "\tphys_addr_t phys = (phys_addr_t)(max_pfn - 1) << PAGE_SHIFT;",
            "\tu64 max_dma = phys_to_dma_direct(dev, phys);",
            "",
            "\treturn (1ULL << (fls64(max_dma) - 1)) * 2 - 1;",
            "}",
            "static gfp_t dma_direct_optimal_gfp_mask(struct device *dev, u64 *phys_limit)",
            "{",
            "\tu64 dma_limit = min_not_zero(",
            "\t\tdev->coherent_dma_mask,",
            "\t\tdev->bus_dma_limit);",
            "",
            "\t/*",
            "\t * Optimistically try the zone that the physical address mask falls",
            "\t * into first.  If that returns memory that isn't actually addressable",
            "\t * we will fallback to the next lower zone and try again.",
            "\t *",
            "\t * Note that GFP_DMA32 and GFP_DMA are no ops without the corresponding",
            "\t * zones.",
            "\t */",
            "\t*phys_limit = dma_to_phys(dev, dma_limit);",
            "\tif (*phys_limit <= DMA_BIT_MASK(zone_dma_bits))",
            "\t\treturn GFP_DMA;",
            "\tif (*phys_limit <= DMA_BIT_MASK(32))",
            "\t\treturn GFP_DMA32;",
            "\treturn 0;",
            "}",
            "bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)",
            "{",
            "\tdma_addr_t dma_addr = phys_to_dma_direct(dev, phys);",
            "",
            "\tif (dma_addr == DMA_MAPPING_ERROR)",
            "\t\treturn false;",
            "\treturn dma_addr + size - 1 <=",
            "\t\tmin_not_zero(dev->coherent_dma_mask, dev->bus_dma_limit);",
            "}",
            "static int dma_set_decrypted(struct device *dev, void *vaddr, size_t size)",
            "{",
            "\tif (!force_dma_unencrypted(dev))",
            "\t\treturn 0;",
            "\treturn set_memory_decrypted((unsigned long)vaddr, PFN_UP(size));",
            "}",
            "static int dma_set_encrypted(struct device *dev, void *vaddr, size_t size)",
            "{",
            "\tint ret;",
            "",
            "\tif (!force_dma_unencrypted(dev))",
            "\t\treturn 0;",
            "\tret = set_memory_encrypted((unsigned long)vaddr, PFN_UP(size));",
            "\tif (ret)",
            "\t\tpr_warn_ratelimited(\"leaking DMA memory that can't be re-encrypted\\n\");",
            "\treturn ret;",
            "}",
            "static void __dma_direct_free_pages(struct device *dev, struct page *page,",
            "\t\t\t\t    size_t size)",
            "{",
            "\tif (swiotlb_free(dev, page, size))",
            "\t\treturn;",
            "\tdma_free_contiguous(dev, page, size);",
            "}",
            "static bool dma_direct_use_pool(struct device *dev, gfp_t gfp)",
            "{",
            "\treturn !gfpflags_allow_blocking(gfp) && !is_swiotlb_for_alloc(dev);",
            "}",
            "void dma_direct_free(struct device *dev, size_t size,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, unsigned long attrs)",
            "{",
            "\tunsigned int page_order = get_order(size);",
            "",
            "\tif ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) &&",
            "\t    !force_dma_unencrypted(dev) && !is_swiotlb_for_alloc(dev)) {",
            "\t\t/* cpu_addr is a struct page cookie, not a kernel address */",
            "\t\tdma_free_contiguous(dev, cpu_addr, size);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_SET_UNCACHED) &&",
            "\t    !IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&",
            "\t    !IS_ENABLED(CONFIG_DMA_GLOBAL_POOL) &&",
            "\t    !dev_is_dma_coherent(dev) &&",
            "\t    !is_swiotlb_for_alloc(dev)) {",
            "\t\tarch_dma_free(dev, size, cpu_addr, dma_addr, attrs);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_DMA_GLOBAL_POOL) &&",
            "\t    !dev_is_dma_coherent(dev)) {",
            "\t\tif (!dma_release_from_global_coherent(page_order, cpu_addr))",
            "\t\t\tWARN_ON_ONCE(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */",
            "\tif (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&",
            "\t    dma_free_from_pool(dev, cpu_addr, PAGE_ALIGN(size)))",
            "\t\treturn;",
            "",
            "\tif (is_vmalloc_addr(cpu_addr)) {",
            "\t\tvunmap(cpu_addr);",
            "\t} else {",
            "\t\tif (IS_ENABLED(CONFIG_ARCH_HAS_DMA_CLEAR_UNCACHED))",
            "\t\t\tarch_dma_clear_uncached(cpu_addr, size);",
            "\t\tif (dma_set_encrypted(dev, cpu_addr, size))",
            "\t\t\treturn;",
            "\t}",
            "",
            "\t__dma_direct_free_pages(dev, dma_direct_to_page(dev, dma_addr), size);",
            "}"
          ],
          "function_name": "phys_to_dma_direct, dma_direct_get_required_mask, dma_direct_optimal_gfp_mask, dma_coherent_ok, dma_set_decrypted, dma_set_encrypted, __dma_direct_free_pages, dma_direct_use_pool, dma_direct_free",
          "description": "实现了DMA直接映射的核心辅助函数，包括物理地址转DMA地址、计算DMA掩码、优化内存分配策略、检查DMA一致性及加密内存设置等功能。dma_direct_free处理不同条件下的内存释放逻辑，涉及SWIOTLB、原子池和架构特定的释放路径。",
          "similarity": 0.529992401599884
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/direct.c",
          "start_line": 1,
          "end_line": 24,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 2018-2020 Christoph Hellwig.",
            " *",
            " * DMA operations that map physical memory directly without using an IOMMU.",
            " */",
            "#include <linux/memblock.h> /* for max_pfn */",
            "#include <linux/export.h>",
            "#include <linux/mm.h>",
            "#include <linux/dma-map-ops.h>",
            "#include <linux/scatterlist.h>",
            "#include <linux/pfn.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/slab.h>",
            "#include \"direct.h\"",
            "",
            "/*",
            " * Most architectures use ZONE_DMA for the first 16 Megabytes, but some use",
            " * it for entirely different regions. In that case the arch code needs to",
            " * override the variable below for dma-direct to work properly.",
            " */",
            "unsigned int zone_dma_bits __ro_after_init = 24;",
            ""
          ],
          "function_name": null,
          "description": "定义了用于DMA直接映射的全局变量zone_dma_bits，默认值为24位，表示DMA地址空间的位宽。该变量用于控制DMA操作的物理地址范围，架构代码可通过覆盖此变量调整DMA直接映射的行为。",
          "similarity": 0.49567633867263794
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/dma/direct.c",
          "start_line": 520,
          "end_line": 633,
          "content": [
            "dma_addr_t dma_direct_map_resource(struct device *dev, phys_addr_t paddr,",
            "\t\tsize_t size, enum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tdma_addr_t dma_addr = paddr;",
            "",
            "\tif (unlikely(!dma_capable(dev, dma_addr, size, false))) {",
            "\t\tdev_err_once(dev,",
            "\t\t\t     \"DMA addr %pad+%zu overflow (mask %llx, bus limit %llx).\\n\",",
            "\t\t\t     &dma_addr, size, *dev->dma_mask, dev->bus_dma_limit);",
            "\t\tWARN_ON_ONCE(1);",
            "\t\treturn DMA_MAPPING_ERROR;",
            "\t}",
            "",
            "\treturn dma_addr;",
            "}",
            "int dma_direct_get_sgtable(struct device *dev, struct sg_table *sgt,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "\tstruct page *page = dma_direct_to_page(dev, dma_addr);",
            "\tint ret;",
            "",
            "\tret = sg_alloc_table(sgt, 1, GFP_KERNEL);",
            "\tif (!ret)",
            "\t\tsg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);",
            "\treturn ret;",
            "}",
            "bool dma_direct_can_mmap(struct device *dev)",
            "{",
            "\treturn dev_is_dma_coherent(dev) ||",
            "\t\tIS_ENABLED(CONFIG_DMA_NONCOHERENT_MMAP);",
            "}",
            "int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "\tunsigned long user_count = vma_pages(vma);",
            "\tunsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;",
            "\tunsigned long pfn = PHYS_PFN(dma_to_phys(dev, dma_addr));",
            "\tint ret = -ENXIO;",
            "",
            "\tvma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);",
            "\tif (force_dma_unencrypted(dev))",
            "\t\tvma->vm_page_prot = pgprot_decrypted(vma->vm_page_prot);",
            "",
            "\tif (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))",
            "\t\treturn ret;",
            "\tif (dma_mmap_from_global_coherent(vma, cpu_addr, size, &ret))",
            "\t\treturn ret;",
            "",
            "\tif (vma->vm_pgoff >= count || user_count > count - vma->vm_pgoff)",
            "\t\treturn -ENXIO;",
            "\treturn remap_pfn_range(vma, vma->vm_start, pfn + vma->vm_pgoff,",
            "\t\t\tuser_count << PAGE_SHIFT, vma->vm_page_prot);",
            "}",
            "int dma_direct_supported(struct device *dev, u64 mask)",
            "{",
            "\tu64 min_mask = (max_pfn - 1) << PAGE_SHIFT;",
            "",
            "\t/*",
            "\t * Because 32-bit DMA masks are so common we expect every architecture",
            "\t * to be able to satisfy them - either by not supporting more physical",
            "\t * memory, or by providing a ZONE_DMA32.  If neither is the case, the",
            "\t * architecture needs to use an IOMMU instead of the direct mapping.",
            "\t */",
            "\tif (mask >= DMA_BIT_MASK(32))",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * This check needs to be against the actual bit mask value, so use",
            "\t * phys_to_dma_unencrypted() here so that the SME encryption mask isn't",
            "\t * part of the check.",
            "\t */",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA))",
            "\t\tmin_mask = min_t(u64, min_mask, DMA_BIT_MASK(zone_dma_bits));",
            "\treturn mask >= phys_to_dma_unencrypted(dev, min_mask);",
            "}",
            "size_t dma_direct_max_mapping_size(struct device *dev)",
            "{",
            "\t/* If SWIOTLB is active, use its maximum mapping size */",
            "\tif (is_swiotlb_active(dev) &&",
            "\t    (dma_addressing_limited(dev) || is_swiotlb_force_bounce(dev)))",
            "\t\treturn swiotlb_max_mapping_size(dev);",
            "\treturn SIZE_MAX;",
            "}",
            "bool dma_direct_need_sync(struct device *dev, dma_addr_t dma_addr)",
            "{",
            "\treturn !dev_is_dma_coherent(dev) ||",
            "\t       is_swiotlb_buffer(dev, dma_to_phys(dev, dma_addr));",
            "}",
            "int dma_direct_set_offset(struct device *dev, phys_addr_t cpu_start,",
            "\t\t\t dma_addr_t dma_start, u64 size)",
            "{",
            "\tstruct bus_dma_region *map;",
            "\tu64 offset = (u64)cpu_start - (u64)dma_start;",
            "",
            "\tif (dev->dma_range_map) {",
            "\t\tdev_err(dev, \"attempt to add DMA range to existing map\\n\");",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tif (!offset)",
            "\t\treturn 0;",
            "",
            "\tmap = kcalloc(2, sizeof(*map), GFP_KERNEL);",
            "\tif (!map)",
            "\t\treturn -ENOMEM;",
            "\tmap[0].cpu_start = cpu_start;",
            "\tmap[0].dma_start = dma_start;",
            "\tmap[0].offset = offset;",
            "\tmap[0].size = size;",
            "\tdev->dma_range_map = map;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "dma_direct_map_resource, dma_direct_get_sgtable, dma_direct_can_mmap, dma_direct_mmap, dma_direct_supported, dma_direct_max_mapping_size, dma_direct_need_sync, dma_direct_set_offset",
          "description": "包含DMA直接映射的资源映射、SG表构建、内存映射支持性检测及最大映射尺寸计算等功能。dma_direct_mmap实现设备内存的VMA映射，dma_direct_supported验证DMA掩码兼容性，dma_direct_set_offset用于配置DMA地址偏移量。",
          "similarity": 0.4806588292121887
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/dma/direct.c",
          "start_line": 391,
          "end_line": 503,
          "content": [
            "void dma_direct_free_pages(struct device *dev, size_t size,",
            "\t\tstruct page *page, dma_addr_t dma_addr,",
            "\t\tenum dma_data_direction dir)",
            "{",
            "\tvoid *vaddr = page_address(page);",
            "",
            "\t/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */",
            "\tif (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&",
            "\t    dma_free_from_pool(dev, vaddr, size))",
            "\t\treturn;",
            "",
            "\tif (dma_set_encrypted(dev, vaddr, size))",
            "\t\treturn;",
            "\t__dma_direct_free_pages(dev, page, size);",
            "}",
            "void dma_direct_sync_sg_for_device(struct device *dev,",
            "\t\tstruct scatterlist *sgl, int nents, enum dma_data_direction dir)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tphys_addr_t paddr = dma_to_phys(dev, sg_dma_address(sg));",
            "",
            "\t\tif (unlikely(is_swiotlb_buffer(dev, paddr)))",
            "\t\t\tswiotlb_sync_single_for_device(dev, paddr, sg->length,",
            "\t\t\t\t\t\t       dir);",
            "",
            "\t\tif (!dev_is_dma_coherent(dev))",
            "\t\t\tarch_sync_dma_for_device(paddr, sg->length,",
            "\t\t\t\t\tdir);",
            "\t}",
            "}",
            "void dma_direct_sync_sg_for_cpu(struct device *dev,",
            "\t\tstruct scatterlist *sgl, int nents, enum dma_data_direction dir)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tphys_addr_t paddr = dma_to_phys(dev, sg_dma_address(sg));",
            "",
            "\t\tif (!dev_is_dma_coherent(dev))",
            "\t\t\tarch_sync_dma_for_cpu(paddr, sg->length, dir);",
            "",
            "\t\tif (unlikely(is_swiotlb_buffer(dev, paddr)))",
            "\t\t\tswiotlb_sync_single_for_cpu(dev, paddr, sg->length,",
            "\t\t\t\t\t\t    dir);",
            "",
            "\t\tif (dir == DMA_FROM_DEVICE)",
            "\t\t\tarch_dma_mark_clean(paddr, sg->length);",
            "\t}",
            "",
            "\tif (!dev_is_dma_coherent(dev))",
            "\t\tarch_sync_dma_for_cpu_all();",
            "}",
            "void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,",
            "\t\tint nents, enum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl,  sg, nents, i) {",
            "\t\tif (sg_dma_is_bus_address(sg))",
            "\t\t\tsg_dma_unmark_bus_address(sg);",
            "\t\telse",
            "\t\t\tdma_direct_unmap_page(dev, sg->dma_address,",
            "\t\t\t\t\t      sg_dma_len(sg), dir, attrs);",
            "\t}",
            "}",
            "int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,",
            "\t\tenum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tstruct pci_p2pdma_map_state p2pdma_state = {};",
            "\tenum pci_p2pdma_map_type map;",
            "\tstruct scatterlist *sg;",
            "\tint i, ret;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tif (is_pci_p2pdma_page(sg_page(sg))) {",
            "\t\t\tmap = pci_p2pdma_map_segment(&p2pdma_state, dev, sg);",
            "\t\t\tswitch (map) {",
            "\t\t\tcase PCI_P2PDMA_MAP_BUS_ADDR:",
            "\t\t\t\tcontinue;",
            "\t\t\tcase PCI_P2PDMA_MAP_THRU_HOST_BRIDGE:",
            "\t\t\t\t/*",
            "\t\t\t\t * Any P2P mapping that traverses the PCI",
            "\t\t\t\t * host bridge must be mapped with CPU physical",
            "\t\t\t\t * address and not PCI bus addresses. This is",
            "\t\t\t\t * done with dma_direct_map_page() below.",
            "\t\t\t\t */",
            "\t\t\t\tbreak;",
            "\t\t\tdefault:",
            "\t\t\t\tret = -EREMOTEIO;",
            "\t\t\t\tgoto out_unmap;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tsg->dma_address = dma_direct_map_page(dev, sg_page(sg),",
            "\t\t\t\tsg->offset, sg->length, dir, attrs);",
            "\t\tif (sg->dma_address == DMA_MAPPING_ERROR) {",
            "\t\t\tret = -EIO;",
            "\t\t\tgoto out_unmap;",
            "\t\t}",
            "\t\tsg_dma_len(sg) = sg->length;",
            "\t}",
            "",
            "\treturn nents;",
            "",
            "out_unmap:",
            "\tdma_direct_unmap_sg(dev, sgl, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "dma_direct_free_pages, dma_direct_sync_sg_for_device, dma_direct_sync_sg_for_cpu, dma_direct_unmap_sg, dma_direct_map_sg",
          "description": "提供了SCATTERLIST的同步、解映射和映射操作实现，包括对非一致内存的架构同步、PCI P2P DMA的特殊处理以及SG表的构建。sync_sg系列函数负责设备与CPU之间的数据缓存一致性维护。",
          "similarity": 0.41505923867225647
        }
      ]
    },
    {
      "source_file": "mm/process_vm_access.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:13:36\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `process_vm_access.c`\n\n---\n\n# `process_vm_access.c` 技术文档\n\n## 1. 文件概述\n\n`process_vm_access.c` 是 Linux 内核中实现跨进程虚拟内存读写功能的核心文件，提供了系统调用 `process_vm_readv` 和 `process_vm_writev` 的底层支持。该机制允许一个进程在无需目标进程协作的情况下，安全地从另一个进程中读取或向其写入数据，常用于调试器、容器运行时、性能分析工具等需要跨进程内存访问的场景。其实现基于内核的页表管理和用户页锁定（`pin_user_pages_remote`）机制，在保证安全性的同时避免了传统 `ptrace` 方式的上下文切换开销。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`process_vm_rw_pages`**  \n  执行实际的页面级内存拷贝操作，根据 `vm_write` 标志决定是将本地数据写入目标页（`copy_page_from_iter`）还是从目标页读取到本地（`copy_page_to_iter`）。\n\n- **`process_vm_rw_single_vec`**  \n  处理单个内存区域（由起始地址和长度定义）的读写操作。负责计算所需页数、通过 `pin_user_pages_remote` 锁定目标进程的物理页，并调用 `process_vm_rw_pages` 执行拷贝。\n\n- **`process_vm_rw_core`**  \n  核心调度函数，遍历远程进程的 iovec 数组（`rvec`），对每个内存段调用 `process_vm_rw_single_vec`。管理页指针数组的分配（栈上或动态）、目标进程查找（`find_get_task_by_vpid`）及内存描述符访问（`mm_access`）。\n\n- **`process_vm_rw`**  \n  系统调用入口的封装层，负责验证并导入用户态传入的本地（`lvec`）和远程（`rvec`）iovec 数组，初始化 `iov_iter` 迭代器，并调用 `process_vm_rw_core`。\n\n- **`SYSCALL_DEFINE6(process_vm_readv, ...)`**  \n  `process_vm_readv` 系统调用的定义（代码片段截断，但完整实现会在此处调用 `process_vm_rw` 并设置 `vm_write=0`）。\n\n- **`SYSCALL_DEFINE6(process_vm_writev, ...)`**  \n  （隐含存在）`process_vm_writev` 系统调用的定义，调用 `process_vm_rw` 并设置 `vm_write=1`。\n\n### 关键数据结构与常量\n\n- **`PVM_MAX_PP_ARRAY_COUNT`** (`16`)  \n  栈上预分配的 `struct page*` 数组的最大元素数量，用于存储待操作页的指针，避免小规模操作时的动态分配。\n\n- **`PVM_MAX_KMALLOC_PAGES`** (`PAGE_SIZE * 2`)  \n  动态分配 `struct page*` 数组时的最大内存限制（以字节计），确保 `kmalloc` 调用的可靠性。\n\n- **`iov_iter`**  \n  内核通用的 I/O 迭代器，用于高效遍历本地缓冲区（`lvec`）。\n\n## 3. 关键实现\n\n- **分页处理与批量锁定**：  \n  函数 `process_vm_rw_single_vec` 将大块内存访问拆分为多个页面批次处理。每批次最多处理 `PVM_MAX_KMALLOC_PAGES / sizeof(struct page*)` 个页，通过 `pin_user_pages_remote` 在目标进程的 `mm_struct` 上下文中锁定物理页，确保在拷贝期间页不会被换出或释放。\n\n- **内存安全与权限检查**：  \n  使用 `mm_access(task, PTRACE_MODE_ATTACH_REALCREDS)` 检查调用者是否有权访问目标进程的内存，该模式要求调用者具有 `CAP_SYS_PTRACE` 能力或满足 ptrace 附加条件。若返回 `-EACCES`，则转换为更合适的 `-EPERM` 错误码。\n\n- **资源管理与错误处理**：  \n  - 页指针数组优先使用栈空间（`pp_stack`），超出 `PVM_MAX_PP_ARRAY_COUNT` 时才动态分配。\n  - 拷贝过程中若发生部分成功（`total_len > 0`），即使后续出错也返回已成功传输的字节数。\n  - 使用 `unpin_user_pages_dirty_lock` 释放锁定的页，若为写操作（`vm_write=1`）则标记页为脏（`dirty`），确保修改能回写。\n\n- **I/O 向量化支持**：  \n  通过 `import_iovec` 和 `iovec_from_user` 处理用户态传入的分散/聚集 I/O 向量（iovec），支持非连续内存区域的高效批量传输。\n\n## 4. 依赖关系\n\n- **内存管理子系统 (`<linux/mm.h>`, `<linux/highmem.h>`)**：  \n  依赖 `pin_user_pages_remote`、`unpin_user_pages_dirty_lock`、`copy_page_to/from_iter` 等核心内存操作函数。\n  \n- **进程管理 (`<linux/sched.h>`, `<linux/sched/mm.h>`)**：  \n  使用 `find_get_task_by_vpid` 查找目标进程，`mm_access` 获取并验证其内存描述符。\n\n- **I/O 子系统 (`<linux/uio.h>`)**：  \n  基于 `iov_iter` 框架实现高效的 I/O 向量处理。\n\n- **系统调用接口 (`<linux/syscalls.h>`)**：  \n  通过 `SYSCALL_DEFINE6` 定义用户态可调用的系统调用入口。\n\n- **兼容层 (`<linux/compat.h>`)**：  \n  支持 32 位用户程序在 64 位内核上的调用（`in_compat_syscall()`）。\n\n- **安全框架 (`<linux/ptrace.h>`)**：  \n  复用 ptrace 的权限检查模型（`PTRACE_MODE_ATTACH_REALCREDS`）确保内存访问安全。\n\n## 5. 使用场景\n\n- **调试与监控工具**：  \n  如 `gdb`、`strace` 等工具通过 `process_vm_readv` 直接读取被调试进程的内存状态，避免频繁的 `ptrace` 陷入内核。\n\n- **容器与沙箱技术**：  \n  容器运行时（如 Docker、Kata Containers）利用此接口在不侵入容器内部的情况下，从宿主机读取或注入容器进程的内存数据。\n\n- **高性能进程间通信 (IPC)**：  \n  在特定场景下替代传统的管道、共享内存等 IPC 机制，实现零拷贝或低开销的跨进程数据交换。\n\n- **内核自检与故障注入**：  \n  内核测试框架可通过此接口模拟内存错误或验证进程内存布局。\n\n- **安全审计工具**：  \n  如 `auditd` 或自定义 LSM 模块，用于监控敏感进程的内存活动。",
      "similarity": 0.5800683498382568,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/process_vm_access.c",
          "start_line": 253,
          "end_line": 289,
          "content": [
            "static ssize_t process_vm_rw(pid_t pid,",
            "\t\t\t     const struct iovec __user *lvec,",
            "\t\t\t     unsigned long liovcnt,",
            "\t\t\t     const struct iovec __user *rvec,",
            "\t\t\t     unsigned long riovcnt,",
            "\t\t\t     unsigned long flags, int vm_write)",
            "{",
            "\tstruct iovec iovstack_l[UIO_FASTIOV];",
            "\tstruct iovec iovstack_r[UIO_FASTIOV];",
            "\tstruct iovec *iov_l = iovstack_l;",
            "\tstruct iovec *iov_r;",
            "\tstruct iov_iter iter;",
            "\tssize_t rc;",
            "\tint dir = vm_write ? ITER_SOURCE : ITER_DEST;",
            "",
            "\tif (flags != 0)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Check iovecs */",
            "\trc = import_iovec(dir, lvec, liovcnt, UIO_FASTIOV, &iov_l, &iter);",
            "\tif (rc < 0)",
            "\t\treturn rc;",
            "\tif (!iov_iter_count(&iter))",
            "\t\tgoto free_iov_l;",
            "\tiov_r = iovec_from_user(rvec, riovcnt, UIO_FASTIOV, iovstack_r,",
            "\t\t\t\tin_compat_syscall());",
            "\tif (IS_ERR(iov_r)) {",
            "\t\trc = PTR_ERR(iov_r);",
            "\t\tgoto free_iov_l;",
            "\t}",
            "\trc = process_vm_rw_core(pid, &iter, iov_r, riovcnt, flags, vm_write);",
            "\tif (iov_r != iovstack_r)",
            "\t\tkfree(iov_r);",
            "free_iov_l:",
            "\tkfree(iov_l);",
            "\treturn rc;",
            "}"
          ],
          "function_name": "process_vm_rw",
          "description": "实现process_vm_rw函数，整合用户态iovec数据到内核迭代器，通过import_iovec解析输入向量，调用核心处理流程完成跨进程内存读写操作",
          "similarity": 0.5902774333953857
        },
        {
          "chunk_id": 1,
          "file_path": "mm/process_vm_access.c",
          "start_line": 27,
          "end_line": 203,
          "content": [
            "static int process_vm_rw_pages(struct page **pages,",
            "\t\t\t       unsigned offset,",
            "\t\t\t       size_t len,",
            "\t\t\t       struct iov_iter *iter,",
            "\t\t\t       int vm_write)",
            "{",
            "\t/* Do the copy for each page */",
            "\twhile (len && iov_iter_count(iter)) {",
            "\t\tstruct page *page = *pages++;",
            "\t\tsize_t copy = PAGE_SIZE - offset;",
            "\t\tsize_t copied;",
            "",
            "\t\tif (copy > len)",
            "\t\t\tcopy = len;",
            "",
            "\t\tif (vm_write)",
            "\t\t\tcopied = copy_page_from_iter(page, offset, copy, iter);",
            "\t\telse",
            "\t\t\tcopied = copy_page_to_iter(page, offset, copy, iter);",
            "",
            "\t\tlen -= copied;",
            "\t\tif (copied < copy && iov_iter_count(iter))",
            "\t\t\treturn -EFAULT;",
            "\t\toffset = 0;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int process_vm_rw_single_vec(unsigned long addr,",
            "\t\t\t\t    unsigned long len,",
            "\t\t\t\t    struct iov_iter *iter,",
            "\t\t\t\t    struct page **process_pages,",
            "\t\t\t\t    struct mm_struct *mm,",
            "\t\t\t\t    struct task_struct *task,",
            "\t\t\t\t    int vm_write)",
            "{",
            "\tunsigned long pa = addr & PAGE_MASK;",
            "\tunsigned long start_offset = addr - pa;",
            "\tunsigned long nr_pages;",
            "\tssize_t rc = 0;",
            "\tunsigned long max_pages_per_loop = PVM_MAX_KMALLOC_PAGES",
            "\t\t/ sizeof(struct pages *);",
            "\tunsigned int flags = 0;",
            "",
            "\t/* Work out address and page range required */",
            "\tif (len == 0)",
            "\t\treturn 0;",
            "\tnr_pages = (addr + len - 1) / PAGE_SIZE - addr / PAGE_SIZE + 1;",
            "",
            "\tif (vm_write)",
            "\t\tflags |= FOLL_WRITE;",
            "",
            "\twhile (!rc && nr_pages && iov_iter_count(iter)) {",
            "\t\tint pinned_pages = min(nr_pages, max_pages_per_loop);",
            "\t\tint locked = 1;",
            "\t\tsize_t bytes;",
            "",
            "\t\t/*",
            "\t\t * Get the pages we're interested in.  We must",
            "\t\t * access remotely because task/mm might not",
            "\t\t * current/current->mm",
            "\t\t */",
            "\t\tmmap_read_lock(mm);",
            "\t\tpinned_pages = pin_user_pages_remote(mm, pa, pinned_pages,",
            "\t\t\t\t\t\t     flags, process_pages,",
            "\t\t\t\t\t\t     &locked);",
            "\t\tif (locked)",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\tif (pinned_pages <= 0)",
            "\t\t\treturn -EFAULT;",
            "",
            "\t\tbytes = pinned_pages * PAGE_SIZE - start_offset;",
            "\t\tif (bytes > len)",
            "\t\t\tbytes = len;",
            "",
            "\t\trc = process_vm_rw_pages(process_pages,",
            "\t\t\t\t\t start_offset, bytes, iter,",
            "\t\t\t\t\t vm_write);",
            "\t\tlen -= bytes;",
            "\t\tstart_offset = 0;",
            "\t\tnr_pages -= pinned_pages;",
            "\t\tpa += pinned_pages * PAGE_SIZE;",
            "",
            "\t\t/* If vm_write is set, the pages need to be made dirty: */",
            "\t\tunpin_user_pages_dirty_lock(process_pages, pinned_pages,",
            "\t\t\t\t\t    vm_write);",
            "\t}",
            "",
            "\treturn rc;",
            "}",
            "static ssize_t process_vm_rw_core(pid_t pid, struct iov_iter *iter,",
            "\t\t\t\t  const struct iovec *rvec,",
            "\t\t\t\t  unsigned long riovcnt,",
            "\t\t\t\t  unsigned long flags, int vm_write)",
            "{",
            "\tstruct task_struct *task;",
            "\tstruct page *pp_stack[PVM_MAX_PP_ARRAY_COUNT];",
            "\tstruct page **process_pages = pp_stack;",
            "\tstruct mm_struct *mm;",
            "\tunsigned long i;",
            "\tssize_t rc = 0;",
            "\tunsigned long nr_pages = 0;",
            "\tunsigned long nr_pages_iov;",
            "\tssize_t iov_len;",
            "\tsize_t total_len = iov_iter_count(iter);",
            "",
            "\t/*",
            "\t * Work out how many pages of struct pages we're going to need",
            "\t * when eventually calling get_user_pages",
            "\t */",
            "\tfor (i = 0; i < riovcnt; i++) {",
            "\t\tiov_len = rvec[i].iov_len;",
            "\t\tif (iov_len > 0) {",
            "\t\t\tnr_pages_iov = ((unsigned long)rvec[i].iov_base",
            "\t\t\t\t\t+ iov_len)",
            "\t\t\t\t/ PAGE_SIZE - (unsigned long)rvec[i].iov_base",
            "\t\t\t\t/ PAGE_SIZE + 1;",
            "\t\t\tnr_pages = max(nr_pages, nr_pages_iov);",
            "\t\t}",
            "\t}",
            "",
            "\tif (nr_pages == 0)",
            "\t\treturn 0;",
            "",
            "\tif (nr_pages > PVM_MAX_PP_ARRAY_COUNT) {",
            "\t\t/* For reliability don't try to kmalloc more than",
            "\t\t   2 pages worth */",
            "\t\tprocess_pages = kmalloc(min_t(size_t, PVM_MAX_KMALLOC_PAGES,",
            "\t\t\t\t\t      sizeof(struct pages *)*nr_pages),",
            "\t\t\t\t\tGFP_KERNEL);",
            "",
            "\t\tif (!process_pages)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\t/* Get process information */",
            "\ttask = find_get_task_by_vpid(pid);",
            "\tif (!task) {",
            "\t\trc = -ESRCH;",
            "\t\tgoto free_proc_pages;",
            "\t}",
            "",
            "\tmm = mm_access(task, PTRACE_MODE_ATTACH_REALCREDS);",
            "\tif (!mm || IS_ERR(mm)) {",
            "\t\trc = IS_ERR(mm) ? PTR_ERR(mm) : -ESRCH;",
            "\t\t/*",
            "\t\t * Explicitly map EACCES to EPERM as EPERM is a more",
            "\t\t * appropriate error code for process_vw_readv/writev",
            "\t\t */",
            "\t\tif (rc == -EACCES)",
            "\t\t\trc = -EPERM;",
            "\t\tgoto put_task_struct;",
            "\t}",
            "",
            "\tfor (i = 0; i < riovcnt && iov_iter_count(iter) && !rc; i++)",
            "\t\trc = process_vm_rw_single_vec(",
            "\t\t\t(unsigned long)rvec[i].iov_base, rvec[i].iov_len,",
            "\t\t\titer, process_pages, mm, task, vm_write);",
            "",
            "\t/* copied = space before - space after */",
            "\ttotal_len -= iov_iter_count(iter);",
            "",
            "\t/* If we have managed to copy any data at all then",
            "\t   we return the number of bytes copied. Otherwise",
            "\t   we return the error code */",
            "\tif (total_len)",
            "\t\trc = total_len;",
            "",
            "\tmmput(mm);",
            "",
            "put_task_struct:",
            "\tput_task_struct(task);",
            "",
            "free_proc_pages:",
            "\tif (process_pages != pp_stack)",
            "\t\tkfree(process_pages);",
            "\treturn rc;",
            "}"
          ],
          "function_name": "process_vm_rw_pages, process_vm_rw_single_vec, process_vm_rw_core",
          "description": "实现process_vm_rw_pages通过循环处理页面数据拷贝，process_vm_rw_single_vec获取远程页面并调用核心处理逻辑，process_vm_rw_core协调多进程地址范围映射与页面锁定操作",
          "similarity": 0.5058541297912598
        },
        {
          "chunk_id": 0,
          "file_path": "mm/process_vm_access.c",
          "start_line": 1,
          "end_line": 26,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * linux/mm/process_vm_access.c",
            " *",
            " * Copyright (C) 2010-2011 Christopher Yeoh <cyeoh@au1.ibm.com>, IBM Corp.",
            " */",
            "",
            "#include <linux/compat.h>",
            "#include <linux/mm.h>",
            "#include <linux/uio.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/highmem.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/slab.h>",
            "#include <linux/syscalls.h>",
            "",
            "/**",
            " * process_vm_rw_pages - read/write pages from task specified",
            " * @pages: array of pointers to pages we want to copy",
            " * @offset: offset in page to start copying from/to",
            " * @len: number of bytes to copy",
            " * @iter: where to copy to/from locally",
            " * @vm_write: 0 means copy from, 1 means copy to",
            " * Returns 0 on success, error code otherwise",
            " */"
          ],
          "function_name": null,
          "description": "声明process_vm_rw_pages函数，用于从指定任务的页面进行读写操作，接受页面指针数组、偏移量、长度、迭代器及写标志，返回操作结果状态码",
          "similarity": 0.4454103112220764
        }
      ]
    },
    {
      "source_file": "mm/nommu.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:57:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `nommu.c`\n\n---\n\n# nommu.c 技术文档\n\n## 1. 文件概述\n\n`nommu.c` 是 Linux 内核中为不支持内存管理单元（MMU）的 CPU 架构提供的内存管理替代实现。在无 MMU 的系统（如某些嵌入式处理器）中，无法使用虚拟内存机制，因此该文件提供了一套简化但功能完整的内存管理接口，以兼容标准内核 API。其核心目标是模拟 `mm/` 子系统中与虚拟内存相关的函数行为，同时避免依赖页表、地址转换等 MMU 特性。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `high_memory`：指向高内存区域起始地址（在 NOMMU 系统中通常为 NULL 或物理内存末尾）\n- `mem_map`：物理页描述符数组的起始地址\n- `max_mapnr`：系统中最大页帧号（PFN）\n- `highest_memmap_pfn`：`mem_map` 中最高有效 PFN\n- `sysctl_nr_trim_pages`：初始内存修剪阈值（用于释放未使用的内存块）\n- `heap_stack_gap`：堆与栈之间的最小间隙（NOMMU 下通常为 0）\n- `mmap_pages_allocated`：通过 mmap 分配的总页数（原子计数器）\n- `nommu_region_tree`：红黑树，用于管理已映射的共享内存区域\n- `nommu_region_sem`：读写信号量，保护 `nommu_region_tree` 的并发访问\n- `vm_region_jar`：slab 缓存，用于分配 `vm_region` 结构\n\n### 主要函数\n- `kobjsize(const void *objp)`：估算给定指针所占内存大小（支持 kmalloc、VMA 区域或普通页）\n- `follow_pfn(struct vm_area_struct *vma, unsigned long address, unsigned long *pfn)`：从用户虚拟地址获取物理页帧号（仅支持 IO 或 PFN 映射）\n- `vfree(const void *addr)`：释放由 vmalloc 分配的内存（实际调用 kfree）\n- `__vmalloc_noprof()` 及相关变体（`vmalloc`, `vzalloc`, `vmalloc_user` 等）：提供 vmalloc 系列函数的 NOMMU 实现（底层使用 kmalloc）\n- `vmalloc_to_page()` / `vmalloc_to_pfn()`：将 vmalloc 地址转换为 page 或 PFN（直接使用 `virt_to_page`）\n- `vread_iter()`：将内核地址内容拷贝到 iov_iter（用于 `/proc/vmallocinfo` 等）\n\n### 操作结构体\n- `generic_file_vm_ops`：空的 `vm_operations_struct`，作为 NOMMU 下文件映射的默认操作集\n\n## 3. 关键实现\n\n### 内存分配模型\n- **无虚拟地址空间**：所有“虚拟”地址实为物理地址的线性映射，`vmalloc` 系列函数退化为 `kmalloc` 调用。\n- **GFP 标志处理**：自动清除 `__GFP_HIGHMEM`（因 kmalloc 无法返回高端内存逻辑地址），并添加 `__GFP_COMP` 以支持复合页。\n- **零填充支持**：`vzalloc` 等函数通过 `__GFP_ZERO` 标志实现。\n\n### 内存区域管理\n- **共享区域跟踪**：使用红黑树 `nommu_region_tree` 和 slab 缓存 `vm_region_jar` 管理可共享的映射区域（如文件映射），确保多个进程可共享同一物理内存。\n- **VMA 标记**：`vmalloc_user` 分配的内存会标记 `VM_USERMAP`，允许后续通过 `remap_vmalloc_range` 映射到用户空间。\n\n### 地址转换简化\n- **PFN 获取**：`follow_pfn` 直接通过地址右移 `PAGE_SHIFT` 计算 PFN（因无虚拟地址转换）。\n- **vmalloc 地址转换**：`vmalloc_to_page` 直接调用 `virt_to_page`（因 vmalloc 地址即物理地址的线性映射）。\n\n### 安全与兼容性\n- **kobjsize 安全检查**：先验证地址有效性（`virt_addr_valid`），再根据页类型（Slab/Compound/VMA）返回不同大小。\n- **用户空间映射**：`vmalloc_user` 在分配后查找对应 VMA 并设置 `VM_USERMAP`，确保安全暴露给用户态。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心内存管理**：`<linux/mm.h>`, `<linux/mman.h>`, `\"internal.h\"`\n- **内存分配**：`<linux/slab.h>`, `<linux/vmalloc.h>`\n- **页与地址转换**：`<linux/highmem.h>`, `<linux/pagemap.h>`, `<asm/tlb.h>`\n- **进程与安全**：`<linux/sched/mm.h>`, `<linux/security.h>`, `<linux/audit.h>`\n- **I/O 与文件**：`<linux/file.h>`, `<linux/uio.h>`, `<linux/backing-dev.h>`\n\n### 符号导出\n- 导出关键符号供其他模块使用：`high_memory`, `max_mapnr`, `mem_map`, `follow_pfn`, `vfree`, `vmalloc` 系列函数等。\n\n### 架构依赖\n- 依赖架构特定头文件（如 `asm/tlb.h`, `asm/mmu_context.h`），但 NOMMU 架构下这些通常为空实现。\n\n## 5. 使用场景\n\n- **无 MMU 嵌入式系统**：运行于 uClinux 等无虚拟内存系统的设备（如早期 ARM7、Blackfin、m68knommu）。\n- **内核子系统兼容**：为依赖 `vmalloc`、`vfree`、`follow_pfn` 等接口的驱动或子系统（如 GPU 驱动、DMA 映射）提供 NOMMU 兼容层。\n- **用户空间内存映射**：支持 `mmap` 系统调用对文件或设备的映射（通过 `nommu_region_tree` 管理共享区域）。\n- **调试与监控**：`vread_iter` 支持 `/proc/vmallocinfo` 等接口读取内核内存布局。\n- **安全内存分配**：`vmalloc_user` 提供可安全映射到用户空间的零初始化内存（用于 IPC 或共享缓冲区）。",
      "similarity": 0.5736543536186218,
      "chunks": [
        {
          "chunk_id": 8,
          "file_path": "mm/nommu.c",
          "start_line": 1612,
          "end_line": 1742,
          "content": [
            "int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,",
            "\t\tunsigned long pfn, unsigned long size, pgprot_t prot)",
            "{",
            "\tif (addr != (pfn << PAGE_SHIFT))",
            "\t\treturn -EINVAL;",
            "",
            "\tvm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);",
            "\treturn 0;",
            "}",
            "int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)",
            "{",
            "\tunsigned long pfn = start >> PAGE_SHIFT;",
            "\tunsigned long vm_len = vma->vm_end - vma->vm_start;",
            "",
            "\tpfn += vma->vm_pgoff;",
            "\treturn io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);",
            "}",
            "int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,",
            "\t\t\tunsigned long pgoff)",
            "{",
            "\tunsigned int size = vma->vm_end - vma->vm_start;",
            "",
            "\tif (!(vma->vm_flags & VM_USERMAP))",
            "\t\treturn -EINVAL;",
            "",
            "\tvma->vm_start = (unsigned long)(addr + (pgoff << PAGE_SHIFT));",
            "\tvma->vm_end = vma->vm_start + size;",
            "",
            "\treturn 0;",
            "}",
            "vm_fault_t filemap_fault(struct vm_fault *vmf)",
            "{",
            "\tBUG();",
            "\treturn 0;",
            "}",
            "vm_fault_t filemap_map_pages(struct vm_fault *vmf,",
            "\t\tpgoff_t start_pgoff, pgoff_t end_pgoff)",
            "{",
            "\tBUG();",
            "\treturn 0;",
            "}",
            "int __access_remote_vm(struct mm_struct *mm, unsigned long addr, void *buf,",
            "\t\t       int len, unsigned int gup_flags)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tint write = gup_flags & FOLL_WRITE;",
            "",
            "\tif (mmap_read_lock_killable(mm))",
            "\t\treturn 0;",
            "",
            "\t/* the access must start within one of the target process's mappings */",
            "\tvma = find_vma(mm, addr);",
            "\tif (vma) {",
            "\t\t/* don't overrun this mapping */",
            "\t\tif (addr + len >= vma->vm_end)",
            "\t\t\tlen = vma->vm_end - addr;",
            "",
            "\t\t/* only read or write mappings where it is permitted */",
            "\t\tif (write && vma->vm_flags & VM_MAYWRITE)",
            "\t\t\tcopy_to_user_page(vma, NULL, addr,",
            "\t\t\t\t\t (void *) addr, buf, len);",
            "\t\telse if (!write && vma->vm_flags & VM_MAYREAD)",
            "\t\t\tcopy_from_user_page(vma, NULL, addr,",
            "\t\t\t\t\t    buf, (void *) addr, len);",
            "\t\telse",
            "\t\t\tlen = 0;",
            "\t} else {",
            "\t\tlen = 0;",
            "\t}",
            "",
            "\tmmap_read_unlock(mm);",
            "",
            "\treturn len;",
            "}",
            "int access_remote_vm(struct mm_struct *mm, unsigned long addr,",
            "\t\tvoid *buf, int len, unsigned int gup_flags)",
            "{",
            "\treturn __access_remote_vm(mm, addr, buf, len, gup_flags);",
            "}",
            "int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len,",
            "\t\tunsigned int gup_flags)",
            "{",
            "\tstruct mm_struct *mm;",
            "",
            "\tif (addr + len < addr)",
            "\t\treturn 0;",
            "",
            "\tmm = get_task_mm(tsk);",
            "\tif (!mm)",
            "\t\treturn 0;",
            "",
            "\tlen = __access_remote_vm(mm, addr, buf, len, gup_flags);",
            "",
            "\tmmput(mm);",
            "\treturn len;",
            "}",
            "static int __copy_remote_vm_str(struct mm_struct *mm, unsigned long addr,",
            "\t\t\t\tvoid *buf, int len)",
            "{",
            "\tunsigned long addr_end;",
            "\tstruct vm_area_struct *vma;",
            "\tint ret = -EFAULT;",
            "",
            "\t*(char *)buf = '\\0';",
            "",
            "\tif (mmap_read_lock_killable(mm))",
            "\t\treturn ret;",
            "",
            "\t/* the access must start within one of the target process's mappings */",
            "\tvma = find_vma(mm, addr);",
            "\tif (!vma)",
            "\t\tgoto out;",
            "",
            "\tif (check_add_overflow(addr, len, &addr_end))",
            "\t\tgoto out;",
            "",
            "\t/* don't overrun this mapping */",
            "\tif (addr_end > vma->vm_end)",
            "\t\tlen = vma->vm_end - addr;",
            "",
            "\t/* only read mappings where it is permitted */",
            "\tif (vma->vm_flags & VM_MAYREAD) {",
            "\t\tret = strscpy(buf, (char *)addr, len);",
            "\t\tif (ret < 0)",
            "\t\t\tret = len - 1;",
            "\t}",
            "",
            "out:",
            "\tmmap_read_unlock(mm);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "remap_pfn_range, vm_iomap_memory, remap_vmalloc_range, filemap_fault, filemap_map_pages, __access_remote_vm, access_remote_vm, access_process_vm, __copy_remote_vm_str",
          "description": "该代码块主要处理非MMU环境下的物理地址到虚拟地址的映射及远程内存访问控制。  \n`remap_pfn_range`/`vm_iomap_memory`/`remap_vmalloc_range`实现物理页框或虚拟内存区域的映射配置，`__access_remote_vm`系列函数提供对目标进程虚拟内存的受限读写访问。  \n部分函数依赖未展示的辅助实现（如`io_remap_pfn_range`），且`filemap_fault`等函数通过`BUG()`表明仅在特定条件下触发。",
          "similarity": 0.6426947712898254
        },
        {
          "chunk_id": 1,
          "file_path": "mm/nommu.c",
          "start_line": 72,
          "end_line": 179,
          "content": [
            "unsigned int kobjsize(const void *objp)",
            "{",
            "\tstruct page *page;",
            "",
            "\t/*",
            "\t * If the object we have should not have ksize performed on it,",
            "\t * return size of 0",
            "\t */",
            "\tif (!objp || !virt_addr_valid(objp))",
            "\t\treturn 0;",
            "",
            "\tpage = virt_to_head_page(objp);",
            "",
            "\t/*",
            "\t * If the allocator sets PageSlab, we know the pointer came from",
            "\t * kmalloc().",
            "\t */",
            "\tif (PageSlab(page))",
            "\t\treturn ksize(objp);",
            "",
            "\t/*",
            "\t * If it's not a compound page, see if we have a matching VMA",
            "\t * region. This test is intentionally done in reverse order,",
            "\t * so if there's no VMA, we still fall through and hand back",
            "\t * PAGE_SIZE for 0-order pages.",
            "\t */",
            "\tif (!PageCompound(page)) {",
            "\t\tstruct vm_area_struct *vma;",
            "",
            "\t\tvma = find_vma(current->mm, (unsigned long)objp);",
            "\t\tif (vma)",
            "\t\t\treturn vma->vm_end - vma->vm_start;",
            "\t}",
            "",
            "\t/*",
            "\t * The ksize() function is only guaranteed to work for pointers",
            "\t * returned by kmalloc(). So handle arbitrary pointers here.",
            "\t */",
            "\treturn page_size(page);",
            "}",
            "int follow_pfn(struct vm_area_struct *vma, unsigned long address,",
            "\tunsigned long *pfn)",
            "{",
            "\tif (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))",
            "\t\treturn -EINVAL;",
            "",
            "\t*pfn = address >> PAGE_SHIFT;",
            "\treturn 0;",
            "}",
            "void vfree(const void *addr)",
            "{",
            "\tkfree(addr);",
            "}",
            "unsigned long vmalloc_to_pfn(const void *addr)",
            "{",
            "\treturn page_to_pfn(virt_to_page(addr));",
            "}",
            "long vread_iter(struct iov_iter *iter, const char *addr, size_t count)",
            "{",
            "\t/* Don't allow overflow */",
            "\tif ((unsigned long) addr + count < count)",
            "\t\tcount = -(unsigned long) addr;",
            "",
            "\treturn copy_to_iter(addr, count, iter);",
            "}",
            "void vunmap(const void *addr)",
            "{",
            "\tBUG();",
            "}",
            "void vm_unmap_ram(const void *mem, unsigned int count)",
            "{",
            "\tBUG();",
            "}",
            "void vm_unmap_aliases(void)",
            "{",
            "}",
            "void free_vm_area(struct vm_struct *area)",
            "{",
            "\tBUG();",
            "}",
            "int vm_insert_page(struct vm_area_struct *vma, unsigned long addr,",
            "\t\t   struct page *page)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "int vm_insert_pages(struct vm_area_struct *vma, unsigned long addr,",
            "\t\t\tstruct page **pages, unsigned long *num)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "int vm_map_pages(struct vm_area_struct *vma, struct page **pages,",
            "\t\t\tunsigned long num)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "int vm_map_pages_zero(struct vm_area_struct *vma, struct page **pages,",
            "\t\t\t\tunsigned long num)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "void __init mmap_init(void)",
            "{",
            "\tint ret;",
            "",
            "\tret = percpu_counter_init(&vm_committed_as, 0, GFP_KERNEL);",
            "\tVM_BUG_ON(ret);",
            "\tvm_region_jar = KMEM_CACHE(vm_region, SLAB_PANIC|SLAB_ACCOUNT);",
            "}"
          ],
          "function_name": "kobjsize, follow_pfn, vfree, vmalloc_to_pfn, vread_iter, vunmap, vm_unmap_ram, vm_unmap_aliases, free_vm_area, vm_insert_page, vm_insert_pages, vm_map_pages, vm_map_pages_zero, mmap_init",
          "description": "提供非MMU环境下的虚拟内存操作替代函数，如kobjsize计算对象大小、follow_pfn获取物理帧号、vfree释放内存等。部分函数因缺乏MMU支持而直接返回错误或触发BUG。包含mmap_init初始化相关资源。",
          "similarity": 0.5881164073944092
        },
        {
          "chunk_id": 3,
          "file_path": "mm/nommu.c",
          "start_line": 573,
          "end_line": 768,
          "content": [
            "static void cleanup_vma_from_mm(struct vm_area_struct *vma)",
            "{",
            "\tvma->vm_mm->map_count--;",
            "\t/* remove the VMA from the mapping */",
            "\tif (vma->vm_file) {",
            "\t\tstruct address_space *mapping;",
            "\t\tmapping = vma->vm_file->f_mapping;",
            "",
            "\t\ti_mmap_lock_write(mapping);",
            "\t\tflush_dcache_mmap_lock(mapping);",
            "\t\tvma_interval_tree_remove(vma, &mapping->i_mmap);",
            "\t\tflush_dcache_mmap_unlock(mapping);",
            "\t\ti_mmap_unlock_write(mapping);",
            "\t}",
            "}",
            "static int delete_vma_from_mm(struct vm_area_struct *vma)",
            "{",
            "\tVMA_ITERATOR(vmi, vma->vm_mm, vma->vm_start);",
            "",
            "\tvma_iter_config(&vmi, vma->vm_start, vma->vm_end);",
            "\tif (vma_iter_prealloc(&vmi, NULL)) {",
            "\t\tpr_warn(\"Allocation of vma tree for process %d failed\\n\",",
            "\t\t       current->pid);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tcleanup_vma_from_mm(vma);",
            "",
            "\t/* remove from the MM's tree and list */",
            "\tvma_iter_clear(&vmi);",
            "\treturn 0;",
            "}",
            "static void delete_vma(struct mm_struct *mm, struct vm_area_struct *vma)",
            "{",
            "\tvma_close(vma);",
            "\tif (vma->vm_file)",
            "\t\tfput(vma->vm_file);",
            "\tput_nommu_region(vma->vm_region);",
            "\tvm_area_free(vma);",
            "}",
            "int expand_stack_locked(struct vm_area_struct *vma, unsigned long addr)",
            "{",
            "\treturn -ENOMEM;",
            "}",
            "static int validate_mmap_request(struct file *file,",
            "\t\t\t\t unsigned long addr,",
            "\t\t\t\t unsigned long len,",
            "\t\t\t\t unsigned long prot,",
            "\t\t\t\t unsigned long flags,",
            "\t\t\t\t unsigned long pgoff,",
            "\t\t\t\t unsigned long *_capabilities)",
            "{",
            "\tunsigned long capabilities, rlen;",
            "\tint ret;",
            "",
            "\t/* do the simple checks first */",
            "\tif (flags & MAP_FIXED)",
            "\t\treturn -EINVAL;",
            "",
            "\tif ((flags & MAP_TYPE) != MAP_PRIVATE &&",
            "\t    (flags & MAP_TYPE) != MAP_SHARED)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!len)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Careful about overflows.. */",
            "\trlen = PAGE_ALIGN(len);",
            "\tif (!rlen || rlen > TASK_SIZE)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/* offset overflow? */",
            "\tif ((pgoff + (rlen >> PAGE_SHIFT)) < pgoff)",
            "\t\treturn -EOVERFLOW;",
            "",
            "\tif (file) {",
            "\t\t/* files must support mmap */",
            "\t\tif (!file->f_op->mmap)",
            "\t\t\treturn -ENODEV;",
            "",
            "\t\t/* work out if what we've got could possibly be shared",
            "\t\t * - we support chardevs that provide their own \"memory\"",
            "\t\t * - we support files/blockdevs that are memory backed",
            "\t\t */",
            "\t\tif (file->f_op->mmap_capabilities) {",
            "\t\t\tcapabilities = file->f_op->mmap_capabilities(file);",
            "\t\t} else {",
            "\t\t\t/* no explicit capabilities set, so assume some",
            "\t\t\t * defaults */",
            "\t\t\tswitch (file_inode(file)->i_mode & S_IFMT) {",
            "\t\t\tcase S_IFREG:",
            "\t\t\tcase S_IFBLK:",
            "\t\t\t\tcapabilities = NOMMU_MAP_COPY;",
            "\t\t\t\tbreak;",
            "",
            "\t\t\tcase S_IFCHR:",
            "\t\t\t\tcapabilities =",
            "\t\t\t\t\tNOMMU_MAP_DIRECT |",
            "\t\t\t\t\tNOMMU_MAP_READ |",
            "\t\t\t\t\tNOMMU_MAP_WRITE;",
            "\t\t\t\tbreak;",
            "",
            "\t\t\tdefault:",
            "\t\t\t\treturn -EINVAL;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* eliminate any capabilities that we can't support on this",
            "\t\t * device */",
            "\t\tif (!file->f_op->get_unmapped_area)",
            "\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;",
            "\t\tif (!(file->f_mode & FMODE_CAN_READ))",
            "\t\t\tcapabilities &= ~NOMMU_MAP_COPY;",
            "",
            "\t\t/* The file shall have been opened with read permission. */",
            "\t\tif (!(file->f_mode & FMODE_READ))",
            "\t\t\treturn -EACCES;",
            "",
            "\t\tif (flags & MAP_SHARED) {",
            "\t\t\t/* do checks for writing, appending and locking */",
            "\t\t\tif ((prot & PROT_WRITE) &&",
            "\t\t\t    !(file->f_mode & FMODE_WRITE))",
            "\t\t\t\treturn -EACCES;",
            "",
            "\t\t\tif (IS_APPEND(file_inode(file)) &&",
            "\t\t\t    (file->f_mode & FMODE_WRITE))",
            "\t\t\t\treturn -EACCES;",
            "",
            "\t\t\tif (!(capabilities & NOMMU_MAP_DIRECT))",
            "\t\t\t\treturn -ENODEV;",
            "",
            "\t\t\t/* we mustn't privatise shared mappings */",
            "\t\t\tcapabilities &= ~NOMMU_MAP_COPY;",
            "\t\t} else {",
            "\t\t\t/* we're going to read the file into private memory we",
            "\t\t\t * allocate */",
            "\t\t\tif (!(capabilities & NOMMU_MAP_COPY))",
            "\t\t\t\treturn -ENODEV;",
            "",
            "\t\t\t/* we don't permit a private writable mapping to be",
            "\t\t\t * shared with the backing device */",
            "\t\t\tif (prot & PROT_WRITE)",
            "\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;",
            "\t\t}",
            "",
            "\t\tif (capabilities & NOMMU_MAP_DIRECT) {",
            "\t\t\tif (((prot & PROT_READ)  && !(capabilities & NOMMU_MAP_READ))  ||",
            "\t\t\t    ((prot & PROT_WRITE) && !(capabilities & NOMMU_MAP_WRITE)) ||",
            "\t\t\t    ((prot & PROT_EXEC)  && !(capabilities & NOMMU_MAP_EXEC))",
            "\t\t\t    ) {",
            "\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;",
            "\t\t\t\tif (flags & MAP_SHARED) {",
            "\t\t\t\t\tpr_warn(\"MAP_SHARED not completely supported on !MMU\\n\");",
            "\t\t\t\t\treturn -EINVAL;",
            "\t\t\t\t}",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* handle executable mappings and implied executable",
            "\t\t * mappings */",
            "\t\tif (path_noexec(&file->f_path)) {",
            "\t\t\tif (prot & PROT_EXEC)",
            "\t\t\t\treturn -EPERM;",
            "\t\t} else if ((prot & PROT_READ) && !(prot & PROT_EXEC)) {",
            "\t\t\t/* handle implication of PROT_EXEC by PROT_READ */",
            "\t\t\tif (current->personality & READ_IMPLIES_EXEC) {",
            "\t\t\t\tif (capabilities & NOMMU_MAP_EXEC)",
            "\t\t\t\t\tprot |= PROT_EXEC;",
            "\t\t\t}",
            "\t\t} else if ((prot & PROT_READ) &&",
            "\t\t\t (prot & PROT_EXEC) &&",
            "\t\t\t !(capabilities & NOMMU_MAP_EXEC)",
            "\t\t\t ) {",
            "\t\t\t/* backing file is not executable, try to copy */",
            "\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;",
            "\t\t}",
            "\t} else {",
            "\t\t/* anonymous mappings are always memory backed and can be",
            "\t\t * privately mapped",
            "\t\t */",
            "\t\tcapabilities = NOMMU_MAP_COPY;",
            "",
            "\t\t/* handle PROT_EXEC implication by PROT_READ */",
            "\t\tif ((prot & PROT_READ) &&",
            "\t\t    (current->personality & READ_IMPLIES_EXEC))",
            "\t\t\tprot |= PROT_EXEC;",
            "\t}",
            "",
            "\t/* allow the security API to have its say */",
            "\tret = security_mmap_addr(addr);",
            "\tif (ret < 0)",
            "\t\treturn ret;",
            "",
            "\t/* looks okay */",
            "\t*_capabilities = capabilities;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "cleanup_vma_from_mm, delete_vma_from_mm, delete_vma, expand_stack_locked, validate_mmap_request",
          "description": "处理虚拟内存区域的验证与清理操作，validate_mmap_request检查mmap请求合法性并设置权限标志。包含删除VMA、扩展堆栈等辅助函数，确保内存管理一致性。",
          "similarity": 0.5858302712440491
        },
        {
          "chunk_id": 6,
          "file_path": "mm/nommu.c",
          "start_line": 1260,
          "end_line": 1389,
          "content": [
            "unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,",
            "\t\t\t      unsigned long prot, unsigned long flags,",
            "\t\t\t      unsigned long fd, unsigned long pgoff)",
            "{",
            "\tstruct file *file = NULL;",
            "\tunsigned long retval = -EBADF;",
            "",
            "\taudit_mmap_fd(fd, flags);",
            "\tif (!(flags & MAP_ANONYMOUS)) {",
            "\t\tfile = fget(fd);",
            "\t\tif (!file)",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);",
            "",
            "\tif (file)",
            "\t\tfput(file);",
            "out:",
            "\treturn retval;",
            "}",
            "static int split_vma(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\t     unsigned long addr, int new_below)",
            "{",
            "\tstruct vm_area_struct *new;",
            "\tstruct vm_region *region;",
            "\tunsigned long npages;",
            "\tstruct mm_struct *mm;",
            "",
            "\t/* we're only permitted to split anonymous regions (these should have",
            "\t * only a single usage on the region) */",
            "\tif (vma->vm_file)",
            "\t\treturn -ENOMEM;",
            "",
            "\tmm = vma->vm_mm;",
            "\tif (mm->map_count >= sysctl_max_map_count)",
            "\t\treturn -ENOMEM;",
            "",
            "\tregion = kmem_cache_alloc(vm_region_jar, GFP_KERNEL);",
            "\tif (!region)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = vm_area_dup(vma);",
            "\tif (!new)",
            "\t\tgoto err_vma_dup;",
            "",
            "\t/* most fields are the same, copy all, and then fixup */",
            "\t*region = *vma->vm_region;",
            "\tnew->vm_region = region;",
            "",
            "\tnpages = (addr - vma->vm_start) >> PAGE_SHIFT;",
            "",
            "\tif (new_below) {",
            "\t\tregion->vm_top = region->vm_end = new->vm_end = addr;",
            "\t} else {",
            "\t\tregion->vm_start = new->vm_start = addr;",
            "\t\tregion->vm_pgoff = new->vm_pgoff += npages;",
            "\t}",
            "",
            "\tvma_iter_config(vmi, new->vm_start, new->vm_end);",
            "\tif (vma_iter_prealloc(vmi, vma)) {",
            "\t\tpr_warn(\"Allocation of vma tree for process %d failed\\n\",",
            "\t\t\tcurrent->pid);",
            "\t\tgoto err_vmi_preallocate;",
            "\t}",
            "",
            "\tif (new->vm_ops && new->vm_ops->open)",
            "\t\tnew->vm_ops->open(new);",
            "",
            "\tdown_write(&nommu_region_sem);",
            "\tdelete_nommu_region(vma->vm_region);",
            "\tif (new_below) {",
            "\t\tvma->vm_region->vm_start = vma->vm_start = addr;",
            "\t\tvma->vm_region->vm_pgoff = vma->vm_pgoff += npages;",
            "\t} else {",
            "\t\tvma->vm_region->vm_end = vma->vm_end = addr;",
            "\t\tvma->vm_region->vm_top = addr;",
            "\t}",
            "\tadd_nommu_region(vma->vm_region);",
            "\tadd_nommu_region(new->vm_region);",
            "\tup_write(&nommu_region_sem);",
            "",
            "\tsetup_vma_to_mm(vma, mm);",
            "\tsetup_vma_to_mm(new, mm);",
            "\tvma_iter_store(vmi, new);",
            "\tmm->map_count++;",
            "\treturn 0;",
            "",
            "err_vmi_preallocate:",
            "\tvm_area_free(new);",
            "err_vma_dup:",
            "\tkmem_cache_free(vm_region_jar, region);",
            "\treturn -ENOMEM;",
            "}",
            "static int vmi_shrink_vma(struct vma_iterator *vmi,",
            "\t\t      struct vm_area_struct *vma,",
            "\t\t      unsigned long from, unsigned long to)",
            "{",
            "\tstruct vm_region *region;",
            "",
            "\t/* adjust the VMA's pointers, which may reposition it in the MM's tree",
            "\t * and list */",
            "\tif (from > vma->vm_start) {",
            "\t\tif (vma_iter_clear_gfp(vmi, from, vma->vm_end, GFP_KERNEL))",
            "\t\t\treturn -ENOMEM;",
            "\t\tvma->vm_end = from;",
            "\t} else {",
            "\t\tif (vma_iter_clear_gfp(vmi, vma->vm_start, to, GFP_KERNEL))",
            "\t\t\treturn -ENOMEM;",
            "\t\tvma->vm_start = to;",
            "\t}",
            "",
            "\t/* cut the backing region down to size */",
            "\tregion = vma->vm_region;",
            "\tBUG_ON(region->vm_usage != 1);",
            "",
            "\tdown_write(&nommu_region_sem);",
            "\tdelete_nommu_region(region);",
            "\tif (from > region->vm_start) {",
            "\t\tto = region->vm_top;",
            "\t\tregion->vm_top = region->vm_end = from;",
            "\t} else {",
            "\t\tregion->vm_start = to;",
            "\t}",
            "\tadd_nommu_region(region);",
            "\tup_write(&nommu_region_sem);",
            "",
            "\tfree_page_series(from, to);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ksys_mmap_pgoff, split_vma, vmi_shrink_vma",
          "description": "该代码段实现了非MMU架构下虚拟内存区域（VMA）的管理功能，包含三个关键组件：\n\n1. `ksys_mmap_pgoff`作为内存映射入口，负责根据文件描述符或匿名标志调用`vm_mmap_pgoff`完成地址空间分配，并处理文件引用计数；\n2. `split_vma`用于分割现有匿名VMA区域，通过复制VMA结构并调整起始/终止地址，实现内存区域的拆分操作；\n3. `vmi_shrink_vma`用于缩减VMA范围，通过修改VMA边界及对应的region结构体，配合页面回收完成内存收缩。\n\n注：代码上下文完整，所有实现均基于非MMU架构特有的`vm_region`管理机制。",
          "similarity": 0.5833563804626465
        },
        {
          "chunk_id": 9,
          "file_path": "mm/nommu.c",
          "start_line": 1791,
          "end_line": 1879,
          "content": [
            "int copy_remote_vm_str(struct task_struct *tsk, unsigned long addr,",
            "\t\t       void *buf, int len, unsigned int gup_flags)",
            "{",
            "\tstruct mm_struct *mm;",
            "\tint ret;",
            "",
            "\tif (unlikely(len == 0))",
            "\t\treturn 0;",
            "",
            "\tmm = get_task_mm(tsk);",
            "\tif (!mm) {",
            "\t\t*(char *)buf = '\\0';",
            "\t\treturn -EFAULT;",
            "\t}",
            "",
            "\tret = __copy_remote_vm_str(mm, addr, buf, len);",
            "",
            "\tmmput(mm);",
            "",
            "\treturn ret;",
            "}",
            "int nommu_shrink_inode_mappings(struct inode *inode, size_t size,",
            "\t\t\t\tsize_t newsize)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tstruct vm_region *region;",
            "\tpgoff_t low, high;",
            "\tsize_t r_size, r_top;",
            "",
            "\tlow = newsize >> PAGE_SHIFT;",
            "\thigh = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;",
            "",
            "\tdown_write(&nommu_region_sem);",
            "\ti_mmap_lock_read(inode->i_mapping);",
            "",
            "\t/* search for VMAs that fall within the dead zone */",
            "\tvma_interval_tree_foreach(vma, &inode->i_mapping->i_mmap, low, high) {",
            "\t\t/* found one - only interested if it's shared out of the page",
            "\t\t * cache */",
            "\t\tif (vma->vm_flags & VM_SHARED) {",
            "\t\t\ti_mmap_unlock_read(inode->i_mapping);",
            "\t\t\tup_write(&nommu_region_sem);",
            "\t\t\treturn -ETXTBSY; /* not quite true, but near enough */",
            "\t\t}",
            "\t}",
            "",
            "\t/* reduce any regions that overlap the dead zone - if in existence,",
            "\t * these will be pointed to by VMAs that don't overlap the dead zone",
            "\t *",
            "\t * we don't check for any regions that start beyond the EOF as there",
            "\t * shouldn't be any",
            "\t */",
            "\tvma_interval_tree_foreach(vma, &inode->i_mapping->i_mmap, 0, ULONG_MAX) {",
            "\t\tif (!(vma->vm_flags & VM_SHARED))",
            "\t\t\tcontinue;",
            "",
            "\t\tregion = vma->vm_region;",
            "\t\tr_size = region->vm_top - region->vm_start;",
            "\t\tr_top = (region->vm_pgoff << PAGE_SHIFT) + r_size;",
            "",
            "\t\tif (r_top > newsize) {",
            "\t\t\tregion->vm_top -= r_top - newsize;",
            "\t\t\tif (region->vm_end > region->vm_top)",
            "\t\t\t\tregion->vm_end = region->vm_top;",
            "\t\t}",
            "\t}",
            "",
            "\ti_mmap_unlock_read(inode->i_mapping);",
            "\tup_write(&nommu_region_sem);",
            "\treturn 0;",
            "}",
            "static int __meminit init_user_reserve(void)",
            "{",
            "\tunsigned long free_kbytes;",
            "",
            "\tfree_kbytes = K(global_zone_page_state(NR_FREE_PAGES));",
            "",
            "\tsysctl_user_reserve_kbytes = min(free_kbytes / 32, 1UL << 17);",
            "\treturn 0;",
            "}",
            "static int __meminit init_admin_reserve(void)",
            "{",
            "\tunsigned long free_kbytes;",
            "",
            "\tfree_kbytes = K(global_zone_page_state(NR_FREE_PAGES));",
            "",
            "\tsysctl_admin_reserve_kbytes = min(free_kbytes / 32, 1UL << 13);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "copy_remote_vm_str, nommu_shrink_inode_mappings, init_user_reserve, init_admin_reserve",
          "description": "该代码段实现了NOMMU环境下的虚拟内存管理辅助功能。  \n`copy_remote_vm_str`通过获取目标进程的mm_struct完成远程字符串拷贝，`nommu_shrink_inode_mappings`遍历并裁剪文件映射区域以适配新大小，`init_user_reserve`与`init_admin_reserve`分别初始化用户级和管理员级内存保留阈值。  \n所有API均基于内核现有机制，未引入虚构组件。",
          "similarity": 0.5724107623100281
        }
      ]
    }
  ]
}