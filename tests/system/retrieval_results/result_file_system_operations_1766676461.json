{
  "query": "file system operations",
  "timestamp": "2025-12-25 23:27:41",
  "retrieved_files": [
    {
      "source_file": "kernel/bpf/bpf_task_storage.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:02:38\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\bpf_task_storage.c`\n\n---\n\n# bpf_task_storage.c 技术文档\n\n## 文件概述\n\n`bpf_task_storage.c` 实现了 BPF（Berkeley Packet Filter）任务本地存储（task-local storage）机制，允许 BPF 程序为内核中的 `task_struct`（即进程/线程）关联自定义的私有数据。该机制基于 BPF 本地存储（`bpf_local_storage`）框架，为每个任务提供键值对形式的存储能力，支持通过 `pidfd` 或直接通过 `task_struct` 指针进行数据的查找、更新和删除操作。该功能主要用于 eBPF 程序在追踪、监控或安全策略中为特定任务附加元数据。\n\n## 核心功能\n\n### 主要数据结构\n- `DEFINE_BPF_STORAGE_CACHE(task_cache)`：为任务存储定义专用的内存缓存，用于高效分配/释放存储节点。\n- `bpf_task_storage_busy`（per-CPU 变量）：用于实现轻量级的 per-CPU 自旋锁，防止递归或并发访问导致的死锁。\n\n### 主要函数\n\n#### 存储访问控制\n- `bpf_task_storage_lock()` / `bpf_task_storage_unlock()`：获取/释放任务存储的 per-CPU 锁，禁止 CPU 迁移以保证原子性。\n- `bpf_task_storage_trylock()`：尝试获取锁，若当前 CPU 已持有锁则失败，用于支持递归调用场景。\n\n#### 存储操作接口\n- `task_storage_ptr()`：返回 `task_struct` 中 `bpf_storage` 字段的地址，供通用本地存储框架使用。\n- `task_storage_lookup()`：在指定任务中查找与给定 BPF map 关联的存储数据。\n- `bpf_task_storage_free()`：在任务销毁时释放其所有 BPF 本地存储数据。\n\n#### BPF Map 操作方法（通过 pidfd）\n- `bpf_pid_task_storage_lookup_elem()`：通过 `pidfd`（文件描述符形式的进程 ID）查找任务存储数据。\n- `bpf_pid_task_storage_update_elem()`：通过 `pidfd` 更新任务存储数据。\n- `bpf_pid_task_storage_delete_elem()`：通过 `pidfd` 删除任务存储数据。\n\n#### BPF 辅助函数（Helper Functions）\n- `bpf_task_storage_get()` / `bpf_task_storage_get_recur()`：BPF 程序调用的辅助函数，用于获取或创建任务存储数据；`_recur` 版本支持在已持有锁的上下文中安全调用。\n- `bpf_task_storage_delete()` / `bpf_task_storage_delete_recur()`：BPF 程序调用的辅助函数，用于删除任务存储数据；同样提供递归安全版本。\n\n#### BPF Map 操作结构\n- `task_storage_map_alloc()` / `task_storage_map_free()`：分配和释放任务存储类型的 BPF map。\n- `task_storage_map_ops`：定义该类型 BPF map 的操作方法集合，包括分配、释放、查找等。\n\n## 关键实现\n\n### 1. 任务存储的组织结构\n每个 `task_struct` 包含一个 `bpf_storage` 字段（类型为 `struct bpf_local_storage __rcu *`），指向一个通用的本地存储结构。该结构内部维护一个哈希表，将 BPF map 与对应的存储数据（`bpf_local_storage_data`）关联起来。\n\n### 2. 并发控制机制\n- 使用 per-CPU 计数器 `bpf_task_storage_busy` 实现轻量级锁，避免传统自旋锁开销。\n- `migrate_disable()` / `migrate_enable()` 禁用 CPU 迁移，确保临界区在同一个 CPU 上执行。\n- 提供“尝试锁”机制（`trylock`），用于支持 BPF 辅助函数在可能已持有锁的上下文（如 tracepoint 回调）中安全调用，避免死锁。\n\n### 3. 生命周期管理\n- 任务销毁时（`bpf_task_storage_free`），在 RCU 保护下安全释放所有关联的 BPF 存储数据。\n- 存储数据的分配使用 `GFP_ATOMIC` 标志，确保在原子上下文（如中断、软中断）中安全分配内存。\n\n### 4. pidfd 支持\n通过 `pidfd_get_pid()` 将用户空间传入的 `pidfd`（文件描述符）转换为内核 `pid` 结构，再通过 `pid_task()` 获取对应的 `task_struct`，从而实现基于进程 ID 的跨进程存储访问。\n\n### 5. 递归安全设计\n提供两套辅助函数（普通版和 `_recur` 版），普通版总是加锁，而 `_recur` 版先尝试加锁，若失败则以“非忙”（`nobusy`）模式操作，避免在已持有锁的上下文中死锁。\n\n## 依赖关系\n\n- **核心依赖**：\n  - `linux/bpf_local_storage.h`：提供通用的 BPF 本地存储框架（`bpf_local_storage_*` 系列函数）。\n  - `linux/pid.h` / `linux/sched.h`：提供 `pidfd_get_pid()`、`pid_task()` 和 `task_struct` 相关操作。\n  - `linux/rcupdate_trace.h`：提供 `bpf_rcu_lock_held()` 用于验证 RCU 上下文。\n  - `linux/bpf.h` / `linux/filter.h`：BPF 核心基础设施和辅助函数注册机制。\n\n- **内存管理**：\n  - 使用 `DEFINE_BPF_STORAGE_CACHE` 宏创建专用 SLAB 缓存，提升内存分配效率。\n  - 依赖 RCU 机制实现无锁读取和安全延迟释放。\n\n- **BPF 子系统**：\n  - 通过 `BPF_CALL_*` 宏注册 BPF 辅助函数，供 eBPF 字节码调用。\n  - 实现 `bpf_map_ops` 接口，使任务存储 map 可通过 BPF map 系统调用操作。\n\n## 使用场景\n\n1. **进程追踪与监控**：eBPF 程序可在进程创建/执行/退出时为其附加自定义元数据（如安全标签、资源使用统计），并通过 `bpf_task_storage_get()` 在后续事件中快速访问。\n\n2. **安全策略实施**：LSM（Linux Security Module）或自定义安全模块可通过 BPF 为任务关联策略数据，在系统调用或资源访问时进行策略检查。\n\n3. **性能分析**：性能分析工具（如 BCC、bpftrace）可利用该机制为每个线程存储调用栈、延迟信息等上下文数据。\n\n4. **跨进程数据共享**：通过 `pidfd` 机制，一个进程可安全地访问另一个进程的 BPF 存储数据（需具备相应权限），适用于调试器、监控代理等场景。\n\n5. **内核子系统扩展**：其他内核模块可通过注册 BPF 程序，在任务生命周期中动态注入和查询数据，而无需修改核心调度器或进程管理代码。",
      "similarity": 0.5923619270324707,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/bpf_task_storage.c",
          "start_line": 26,
          "end_line": 145,
          "content": [
            "static void bpf_task_storage_lock(void)",
            "{",
            "\tmigrate_disable();",
            "\tthis_cpu_inc(bpf_task_storage_busy);",
            "}",
            "static void bpf_task_storage_unlock(void)",
            "{",
            "\tthis_cpu_dec(bpf_task_storage_busy);",
            "\tmigrate_enable();",
            "}",
            "static bool bpf_task_storage_trylock(void)",
            "{",
            "\tmigrate_disable();",
            "\tif (unlikely(this_cpu_inc_return(bpf_task_storage_busy) != 1)) {",
            "\t\tthis_cpu_dec(bpf_task_storage_busy);",
            "\t\tmigrate_enable();",
            "\t\treturn false;",
            "\t}",
            "\treturn true;",
            "}",
            "void bpf_task_storage_free(struct task_struct *task)",
            "{",
            "\tstruct bpf_local_storage *local_storage;",
            "",
            "\trcu_read_lock();",
            "",
            "\tlocal_storage = rcu_dereference(task->bpf_storage);",
            "\tif (!local_storage) {",
            "\t\trcu_read_unlock();",
            "\t\treturn;",
            "\t}",
            "",
            "\tbpf_task_storage_lock();",
            "\tbpf_local_storage_destroy(local_storage);",
            "\tbpf_task_storage_unlock();",
            "\trcu_read_unlock();",
            "}",
            "static long bpf_pid_task_storage_update_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\t     void *value, u64 map_flags)",
            "{",
            "\tstruct bpf_local_storage_data *sdata;",
            "\tstruct task_struct *task;",
            "\tunsigned int f_flags;",
            "\tstruct pid *pid;",
            "\tint fd, err;",
            "",
            "\tif ((map_flags & BPF_F_LOCK) && btf_record_has_field(map->record, BPF_UPTR))",
            "\t\treturn -EOPNOTSUPP;",
            "",
            "\tfd = *(int *)key;",
            "\tpid = pidfd_get_pid(fd, &f_flags);",
            "\tif (IS_ERR(pid))",
            "\t\treturn PTR_ERR(pid);",
            "",
            "\t/* We should be in an RCU read side critical section, it should be safe",
            "\t * to call pid_task.",
            "\t */",
            "\tWARN_ON_ONCE(!rcu_read_lock_held());",
            "\ttask = pid_task(pid, PIDTYPE_PID);",
            "\tif (!task) {",
            "\t\terr = -ENOENT;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tbpf_task_storage_lock();",
            "\tsdata = bpf_local_storage_update(",
            "\t\ttask, (struct bpf_local_storage_map *)map, value, map_flags,",
            "\t\ttrue, GFP_ATOMIC);",
            "\tbpf_task_storage_unlock();",
            "",
            "\terr = PTR_ERR_OR_ZERO(sdata);",
            "out:",
            "\tput_pid(pid);",
            "\treturn err;",
            "}",
            "static int task_storage_delete(struct task_struct *task, struct bpf_map *map,",
            "\t\t\t       bool nobusy)",
            "{",
            "\tstruct bpf_local_storage_data *sdata;",
            "",
            "\tsdata = task_storage_lookup(task, map, false);",
            "\tif (!sdata)",
            "\t\treturn -ENOENT;",
            "",
            "\tif (!nobusy)",
            "\t\treturn -EBUSY;",
            "",
            "\tbpf_selem_unlink(SELEM(sdata), false);",
            "",
            "\treturn 0;",
            "}",
            "static long bpf_pid_task_storage_delete_elem(struct bpf_map *map, void *key)",
            "{",
            "\tstruct task_struct *task;",
            "\tunsigned int f_flags;",
            "\tstruct pid *pid;",
            "\tint fd, err;",
            "",
            "\tfd = *(int *)key;",
            "\tpid = pidfd_get_pid(fd, &f_flags);",
            "\tif (IS_ERR(pid))",
            "\t\treturn PTR_ERR(pid);",
            "",
            "\t/* We should be in an RCU read side critical section, it should be safe",
            "\t * to call pid_task.",
            "\t */",
            "\tWARN_ON_ONCE(!rcu_read_lock_held());",
            "\ttask = pid_task(pid, PIDTYPE_PID);",
            "\tif (!task) {",
            "\t\terr = -ENOENT;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tbpf_task_storage_lock();",
            "\terr = task_storage_delete(task, map, true);",
            "\tbpf_task_storage_unlock();",
            "out:",
            "\tput_pid(pid);",
            "\treturn err;",
            "}"
          ],
          "function_name": "bpf_task_storage_lock, bpf_task_storage_unlock, bpf_task_storage_trylock, bpf_task_storage_free, bpf_pid_task_storage_update_elem, task_storage_delete, bpf_pid_task_storage_delete_elem",
          "description": "实现任务存储的并发控制机制，包含加锁/解锁接口及原子操作，提供基于PID的存储更新与删除功能，通过RCU保护数据结构并处理任务引用计数",
          "similarity": 0.5425782799720764
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/bpf_task_storage.c",
          "start_line": 1,
          "end_line": 25,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (c) 2020 Facebook",
            " * Copyright 2020 Google LLC.",
            " */",
            "",
            "#include <linux/pid.h>",
            "#include <linux/sched.h>",
            "#include <linux/rculist.h>",
            "#include <linux/list.h>",
            "#include <linux/hash.h>",
            "#include <linux/types.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/bpf.h>",
            "#include <linux/bpf_local_storage.h>",
            "#include <linux/filter.h>",
            "#include <uapi/linux/btf.h>",
            "#include <linux/btf_ids.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/rcupdate_trace.h>",
            "",
            "DEFINE_BPF_STORAGE_CACHE(task_cache);",
            "",
            "static DEFINE_PER_CPU(int, bpf_task_storage_busy);",
            ""
          ],
          "function_name": null,
          "description": "定义任务存储全局缓存task_cache用于管理BPF本地存储对象，声明per-CPU变量bpf_task_storage_busy用于记录当前CPU的任务存储操作繁忙状态",
          "similarity": 0.5368750095367432
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/bpf_task_storage.c",
          "start_line": 308,
          "end_line": 315,
          "content": [
            "static int notsupp_get_next_key(struct bpf_map *map, void *key, void *next_key)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static void task_storage_map_free(struct bpf_map *map)",
            "{",
            "\tbpf_local_storage_map_free(map, &task_cache, &bpf_task_storage_busy);",
            "}"
          ],
          "function_name": "notsupp_get_next_key, task_storage_map_free",
          "description": "声明不支持的遍历键接口并注册任务存储映射释放回调，通过通用函数释放关联的本地存储对象并清理缓存资源",
          "similarity": 0.49595603346824646
        }
      ]
    },
    {
      "source_file": "kernel/pid_sysctl.h",
      "md_summary": "> 自动生成时间: 2025-10-25 15:17:36\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `pid_sysctl.h`\n\n---\n\n# `pid_sysctl.h` 技术文档\n\n## 1. 文件概述\n\n`pid_sysctl.h` 是 Linux 内核中用于定义与 PID 命名空间（`pid_namespace`）相关的系统控制（sysctl）接口的头文件。其核心功能是提供对 `memfd_noexec` 系统策略的运行时配置支持，该策略用于控制通过 `memfd_create()` 创建的内存文件是否允许执行代码。该配置具有层级继承语义：子 PID 命名空间的策略不能比其父命名空间更宽松，以确保安全策略的向下兼容性和强制性。\n\n## 2. 核心功能\n\n### 函数\n\n- **`pid_mfd_noexec_dointvec_minmax`**  \n  自定义的 sysctl 处理函数，用于读写 `memfd_noexec` 策略值。在写入时执行权限检查和策略继承约束验证。\n\n- **`register_pid_ns_sysctl_table_vm`**  \n  内联函数，用于向内核 sysctl 子系统注册 `vm.memfd_noexec` 控制项（仅在 `CONFIG_SYSCTL` 和 `CONFIG_MEMFD_CREATE` 同时启用时有效）。\n\n### 数据结构\n\n- **`pid_ns_ctl_table_vm`**  \n  `ctl_table` 类型的静态数组，定义了 `vm.memfd_noexec` sysctl 条目，包括其名称、数据指针、访问权限、处理函数及取值范围（0 到 2）。\n\n## 3. 关键实现\n\n- **权限控制**：  \n  在写入 `memfd_noexec` 值时，调用 `ns_capable(ns->user_ns, CAP_SYS_ADMIN)` 检查当前任务是否在对应用户命名空间中拥有 `CAP_SYS_ADMIN` 能力，防止非特权用户修改安全策略。\n\n- **策略继承约束**：  \n  通过 `pidns_memfd_noexec_scope(ns->parent)` 获取父 PID 命名空间的策略值 `parent_scope`，并确保当前命名空间的策略值 `scope` 不小于父策略（即不能更宽松）。实际写入前使用 `max(READ_ONCE(ns->memfd_noexec_scope), parent_scope)` 保证该约束。\n\n- **原子读写**：  \n  使用 `READ_ONCE()` 和 `WRITE_ONCE()` 对 `ns->memfd_noexec_scope` 进行访问，确保在并发环境下内存访问的可见性和顺序性。\n\n- **sysctl 注册**：  \n  通过 `register_sysctl(\"vm\", pid_ns_ctl_table_vm)` 将控制项注册到 `/proc/sys/vm/memfd_noexec` 路径下，供用户空间通过标准 sysctl 接口访问。\n\n- **条件编译**：  \n  整个功能仅在 `CONFIG_SYSCTL`（启用 sysctl 支持）和 `CONFIG_MEMFD_CREATE`（启用 memfd_create 系统调用）同时配置时编译，否则 `register_pid_ns_sysctl_table_vm` 为空内联函数，避免代码膨胀。\n\n## 4. 依赖关系\n\n- **`<linux/pid_namespace.h>`**：  \n  提供 `struct pid_namespace` 定义及辅助函数如 `task_active_pid_ns()` 和 `pidns_memfd_noexec_scope()`。\n\n- **`CONFIG_SYSCTL`**：  \n  内核配置选项，启用 sysctl 框架支持，提供 `register_sysctl`、`proc_dointvec_minmax` 等接口。\n\n- **`CONFIG_MEMFD_CREATE`**：  \n  内核配置选项，启用 `memfd_create()` 系统调用及相关功能（如 `memfd_noexec_scope` 字段）。\n\n- **能力子系统（Capabilities）**：  \n  依赖 `ns_capable()` 进行命名空间感知的权限检查。\n\n## 5. 使用场景\n\n- **安全策略配置**：  \n  系统管理员或容器运行时可通过写入 `/proc/sys/vm/memfd_noexec` 设置当前 PID 命名空间中 `memfd` 文件的执行限制级别（0=允许执行，1=禁止执行但可覆盖，2=严格禁止执行），用于防御基于内存文件的代码注入攻击。\n\n- **容器隔离**：  \n  在容器化环境中，不同容器运行在独立的 PID 命名空间中。父命名空间（如宿主机）可设置较严格的 `memfd_noexec` 策略，子容器无法降低该策略级别，从而实现自上而下的安全策略强制。\n\n- **运行时动态调整**：  \n  允许在系统运行期间动态调整 `memfd` 执行策略，无需重启或重新加载内核模块，提升系统灵活性与安全性。",
      "similarity": 0.5706998705863953,
      "chunks": []
    },
    {
      "source_file": "kernel/bpf/hashtab.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:10:56\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\hashtab.c`\n\n---\n\n# bpf/hashtab.c 技术文档\n\n## 1. 文件概述\n\n`bpf/hashtab.c` 是 Linux 内核中 BPF（Berkeley Packet Filter）子系统的核心实现文件之一，负责提供基于哈希表（hash table）的 BPF map 类型支持。该文件实现了多种 BPF map 类型，包括普通哈希表（`BPF_MAP_TYPE_HASH`）、LRU 哈希表（`BPF_MAP_TYPE_LRU_HASH`）、每 CPU 哈希表（`BPF_MAP_TYPE_PERCPU_HASH`）及其 LRU 变体。它支持预分配（pre-allocated）和动态分配（non-preallocated）两种内存管理模式，并集成了 BPF 内存分配器（`bpf_mem_alloc`）、LRU 驱逐机制、每 CPU 自由列表（percpu freelist）等高级特性，以满足高性能、低延迟的 BPF 程序需求。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bucket`**  \n  哈希桶结构，包含一个 `hlist_nulls_head` 链表头和一个 `raw_spinlock_t` 原始自旋锁，用于保护桶内元素的并发访问。\n\n- **`struct bpf_htab`**  \n  BPF 哈希表的主控制结构，继承自 `struct bpf_map`，包含：\n  - 桶数组指针 `buckets`\n  - 元素存储区 `elems`\n  - 内存分配器 `ma`（主）和 `pcpu_ma`（每 CPU）\n  - LRU 或 percpu_freelist 联合体\n  - 元素计数器（`pcount` 或 `count`）\n  - 哈希种子 `hashrnd`\n  - 锁依赖类键 `lockdep_key`\n  - 每 CPU 锁状态数组 `map_locked`（用于防止递归）\n\n- **`struct htab_elem`**  \n  哈希表元素结构，包含：\n  - 哈希链表节点 `hash_node`\n  - LRU 节点或自由列表节点\n  - 指向每 CPU 指针的指针（用于 per-CPU map）\n  - 哈希值 `hash`\n  - 可变长键 `key[]`（后接值或 per-CPU 指针）\n\n### 关键辅助函数\n\n- `htab_is_prealloc()`：判断是否为预分配模式\n- `htab_is_lru()` / `htab_is_percpu()`：判断 map 类型是否为 LRU 或 per-CPU\n- `htab_init_buckets()`：初始化所有哈希桶\n- `htab_lock_bucket()` / `htab_unlock_bucket()`：带递归保护的桶锁操作\n- `htab_elem_set_ptr()` / `htab_elem_get_ptr()`：操作 per-CPU 指针\n- `get_htab_elem()`：从预分配区域获取第 i 个元素\n- `htab_has_extra_elems()`：判断是否包含额外元素（用于 per-CPU 扩展）\n- `htab_free_prealloced_timers_and_wq()`：释放预分配元素中的 BPF 定时器和工作队列资源\n\n### 批量操作宏\n\n- `BATCH_OPS(_name)`：定义批量操作函数指针，如 `map_lookup_batch`、`map_update_batch` 等。\n\n## 3. 关键实现\n\n### 并发控制与死锁预防\n\n- 使用 **原始自旋锁（`raw_spinlock_t`）** 保护每个哈希桶，确保在任意上下文（如 kprobe、perf、tracepoint）中安全使用。\n- 引入 **每 CPU 递归计数器 `map_locked[]`**，防止 BPF 程序在持有桶锁时再次进入（例如通过 `sys_bpf()` 或嵌套 BPF 调用），避免死锁。\n- 在 `PREEMPT_RT` 实时内核上，由于普通自旋锁可能睡眠，必须使用 `raw_spinlock` 以保证原子性；结合 `bpf_mem_alloc` 后，即使非预分配模式也可安全使用原始锁。\n\n### 内存管理\n\n- **预分配模式（`BPF_F_NO_PREALLOC` 未设置）**：启动时一次性分配所有元素，使用 `pcpu_freelist` 管理空闲元素。\n- **非预分配模式**：按需通过 `bpf_mem_alloc` 动态分配元素，支持 NUMA 感知和内存回收。\n- **Per-CPU 支持**：对于 `PERCPU_HASH` 类型，每个键对应一个 per-CPU 值数组，通过 `htab_elem_get_ptr()` 访问。\n\n### LRU 驱逐机制\n\n- 当 map 类型为 `LRU_HASH` 或 `LRU_PERCPU_HASH` 时，使用 `bpf_lru` 子系统管理元素生命周期，自动驱逐最近最少使用的条目以维持 `max_entries` 限制。\n\n### 扩展字段支持\n\n- 支持 BTF（BPF Type Format）描述的复杂值类型，如 `BPF_TIMER` 和 `BPF_WORKQUEUE`，在销毁 map 时自动释放相关资源（见 `htab_free_prealloced_timers_and_wq`）。\n\n### 哈希与对齐\n\n- 使用 `jhash` 算法计算键的哈希值，并通过 `hashrnd` 引入随机种子防止哈希碰撞攻击。\n- 键和值之间按 8 字节对齐（`__aligned(8)`），确保 per-CPU 指针正确对齐。\n\n## 4. 依赖关系\n\n- **内核头文件**：\n  - `<linux/bpf.h>`、`<linux/btf.h>`：BPF 和 BTF 核心接口\n  - `<linux/jhash.h>`：哈希函数\n  - `<linux/rculist_nulls.h>`：RCU 安全的空指针链表\n  - `<linux/percpu_freelist.h>`、`<linux/bpf_lru_list.h>`：内存管理子系统\n  - `<linux/bpf_mem_alloc.h>`：BPF 专用内存分配器\n\n- **内部模块**：\n  - `map_in_map.h`：支持 map-in-map 功能\n  - `bpf_lru_list.c`：LRU 驱逐实现\n  - `percpu_freelist.c`：每 CPU 自由列表管理\n\n- **BPF 子系统**：\n  - 与 `bpf_map` 通用框架集成，通过 `bpf_map_ops` 注册操作函数\n  - 依赖 `bpf_prog_active` 机制防止 BPF 递归\n\n## 5. 使用场景\n\n- **网络数据包过滤与监控**：eBPF 程序使用 `BPF_MAP_TYPE_HASH` 存储连接状态、统计信息等。\n- **性能分析**：通过 `PERCPU_HASH` 收集每 CPU 的性能计数器，避免锁竞争。\n- **资源限制与缓存**：`LRU_HASH` 用于实现有界缓存（如 DNS 缓存、会话表），自动淘汰旧条目。\n- **内核跟踪**：kprobe、tracepoint 等 attach 的 BPF 程序频繁读写哈希表，要求低延迟和高并发。\n- **用户空间交互**：通过 `bpf(2)` 系统调用进行 map 的创建、更新、查询和删除，支持批量操作提升效率。\n- **高级 BPF 功能**：支持包含定时器（`bpf_timer`）或工作队列（`bpf_workqueue`）的复杂 map 值类型，用于异步任务调度。",
      "similarity": 0.5673032402992249,
      "chunks": [
        {
          "chunk_id": 11,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 1701,
          "end_line": 1938,
          "content": [
            "static int htab_lru_percpu_map_lookup_and_delete_elem(struct bpf_map *map,",
            "\t\t\t\t\t\t      void *key, void *value,",
            "\t\t\t\t\t\t      u64 flags)",
            "{",
            "\treturn __htab_map_lookup_and_delete_elem(map, key, value, true, true,",
            "\t\t\t\t\t\t flags);",
            "}",
            "static int",
            "__htab_map_lookup_and_delete_batch(struct bpf_map *map,",
            "\t\t\t\t   const union bpf_attr *attr,",
            "\t\t\t\t   union bpf_attr __user *uattr,",
            "\t\t\t\t   bool do_delete, bool is_lru_map,",
            "\t\t\t\t   bool is_percpu)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tu32 bucket_cnt, total, key_size, value_size, roundup_key_size;",
            "\tvoid *keys = NULL, *values = NULL, *value, *dst_key, *dst_val;",
            "\tvoid __user *uvalues = u64_to_user_ptr(attr->batch.values);",
            "\tvoid __user *ukeys = u64_to_user_ptr(attr->batch.keys);",
            "\tvoid __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);",
            "\tu32 batch, max_count, size, bucket_size, map_id;",
            "\tstruct htab_elem *node_to_free = NULL;",
            "\tu64 elem_map_flags, map_flags;",
            "\tstruct hlist_nulls_head *head;",
            "\tstruct hlist_nulls_node *n;",
            "\tunsigned long flags = 0;",
            "\tbool locked = false;",
            "\tstruct htab_elem *l;",
            "\tstruct bucket *b;",
            "\tint ret = 0;",
            "",
            "\telem_map_flags = attr->batch.elem_flags;",
            "\tif ((elem_map_flags & ~BPF_F_LOCK) ||",
            "\t    ((elem_map_flags & BPF_F_LOCK) && !btf_record_has_field(map->record, BPF_SPIN_LOCK)))",
            "\t\treturn -EINVAL;",
            "",
            "\tmap_flags = attr->batch.flags;",
            "\tif (map_flags)",
            "\t\treturn -EINVAL;",
            "",
            "\tmax_count = attr->batch.count;",
            "\tif (!max_count)",
            "\t\treturn 0;",
            "",
            "\tif (put_user(0, &uattr->batch.count))",
            "\t\treturn -EFAULT;",
            "",
            "\tbatch = 0;",
            "\tif (ubatch && copy_from_user(&batch, ubatch, sizeof(batch)))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (batch >= htab->n_buckets)",
            "\t\treturn -ENOENT;",
            "",
            "\tkey_size = htab->map.key_size;",
            "\troundup_key_size = round_up(htab->map.key_size, 8);",
            "\tvalue_size = htab->map.value_size;",
            "\tsize = round_up(value_size, 8);",
            "\tif (is_percpu)",
            "\t\tvalue_size = size * num_possible_cpus();",
            "\ttotal = 0;",
            "\t/* while experimenting with hash tables with sizes ranging from 10 to",
            "\t * 1000, it was observed that a bucket can have up to 5 entries.",
            "\t */",
            "\tbucket_size = 5;",
            "",
            "alloc:",
            "\t/* We cannot do copy_from_user or copy_to_user inside",
            "\t * the rcu_read_lock. Allocate enough space here.",
            "\t */",
            "\tkeys = kvmalloc_array(key_size, bucket_size, GFP_USER | __GFP_NOWARN);",
            "\tvalues = kvmalloc_array(value_size, bucket_size, GFP_USER | __GFP_NOWARN);",
            "\tif (!keys || !values) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto after_loop;",
            "\t}",
            "",
            "again:",
            "\tbpf_disable_instrumentation();",
            "\trcu_read_lock();",
            "again_nocopy:",
            "\tdst_key = keys;",
            "\tdst_val = values;",
            "\tb = &htab->buckets[batch];",
            "\thead = &b->head;",
            "\t/* do not grab the lock unless need it (bucket_cnt > 0). */",
            "\tif (locked) {",
            "\t\tret = htab_lock_bucket(htab, b, batch, &flags);",
            "\t\tif (ret) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\tbpf_enable_instrumentation();",
            "\t\t\tgoto after_loop;",
            "\t\t}",
            "\t}",
            "",
            "\tbucket_cnt = 0;",
            "\thlist_nulls_for_each_entry_rcu(l, n, head, hash_node)",
            "\t\tbucket_cnt++;",
            "",
            "\tif (bucket_cnt && !locked) {",
            "\t\tlocked = true;",
            "\t\tgoto again_nocopy;",
            "\t}",
            "",
            "\tif (bucket_cnt > (max_count - total)) {",
            "\t\tif (total == 0)",
            "\t\t\tret = -ENOSPC;",
            "\t\t/* Note that since bucket_cnt > 0 here, it is implicit",
            "\t\t * that the locked was grabbed, so release it.",
            "\t\t */",
            "\t\thtab_unlock_bucket(htab, b, batch, flags);",
            "\t\trcu_read_unlock();",
            "\t\tbpf_enable_instrumentation();",
            "\t\tgoto after_loop;",
            "\t}",
            "",
            "\tif (bucket_cnt > bucket_size) {",
            "\t\tbucket_size = bucket_cnt;",
            "\t\t/* Note that since bucket_cnt > 0 here, it is implicit",
            "\t\t * that the locked was grabbed, so release it.",
            "\t\t */",
            "\t\thtab_unlock_bucket(htab, b, batch, flags);",
            "\t\trcu_read_unlock();",
            "\t\tbpf_enable_instrumentation();",
            "\t\tkvfree(keys);",
            "\t\tkvfree(values);",
            "\t\tgoto alloc;",
            "\t}",
            "",
            "\t/* Next block is only safe to run if you have grabbed the lock */",
            "\tif (!locked)",
            "\t\tgoto next_batch;",
            "",
            "\thlist_nulls_for_each_entry_safe(l, n, head, hash_node) {",
            "\t\tmemcpy(dst_key, l->key, key_size);",
            "",
            "\t\tif (is_percpu) {",
            "\t\t\tint off = 0, cpu;",
            "\t\t\tvoid __percpu *pptr;",
            "",
            "\t\t\tpptr = htab_elem_get_ptr(l, map->key_size);",
            "\t\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\t\tcopy_map_value_long(&htab->map, dst_val + off, per_cpu_ptr(pptr, cpu));",
            "\t\t\t\tcheck_and_init_map_value(&htab->map, dst_val + off);",
            "\t\t\t\toff += size;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tvalue = l->key + roundup_key_size;",
            "\t\t\tif (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {",
            "\t\t\t\tstruct bpf_map **inner_map = value;",
            "",
            "\t\t\t\t /* Actual value is the id of the inner map */",
            "\t\t\t\tmap_id = map->ops->map_fd_sys_lookup_elem(*inner_map);",
            "\t\t\t\tvalue = &map_id;",
            "\t\t\t}",
            "",
            "\t\t\tif (elem_map_flags & BPF_F_LOCK)",
            "\t\t\t\tcopy_map_value_locked(map, dst_val, value,",
            "\t\t\t\t\t\t      true);",
            "\t\t\telse",
            "\t\t\t\tcopy_map_value(map, dst_val, value);",
            "\t\t\t/* Zeroing special fields in the temp buffer */",
            "\t\t\tcheck_and_init_map_value(map, dst_val);",
            "\t\t}",
            "\t\tif (do_delete) {",
            "\t\t\thlist_nulls_del_rcu(&l->hash_node);",
            "",
            "\t\t\t/* bpf_lru_push_free() will acquire lru_lock, which",
            "\t\t\t * may cause deadlock. See comments in function",
            "\t\t\t * prealloc_lru_pop(). Let us do bpf_lru_push_free()",
            "\t\t\t * after releasing the bucket lock.",
            "\t\t\t *",
            "\t\t\t * For htab of maps, htab_put_fd_value() in",
            "\t\t\t * free_htab_elem() may acquire a spinlock with bucket",
            "\t\t\t * lock being held and it violates the lock rule, so",
            "\t\t\t * invoke free_htab_elem() after unlock as well.",
            "\t\t\t */",
            "\t\t\tl->batch_flink = node_to_free;",
            "\t\t\tnode_to_free = l;",
            "\t\t}",
            "\t\tdst_key += key_size;",
            "\t\tdst_val += value_size;",
            "\t}",
            "",
            "\thtab_unlock_bucket(htab, b, batch, flags);",
            "\tlocked = false;",
            "",
            "\twhile (node_to_free) {",
            "\t\tl = node_to_free;",
            "\t\tnode_to_free = node_to_free->batch_flink;",
            "\t\tif (is_lru_map)",
            "\t\t\thtab_lru_push_free(htab, l);",
            "\t\telse",
            "\t\t\tfree_htab_elem(htab, l);",
            "\t}",
            "",
            "next_batch:",
            "\t/* If we are not copying data, we can go to next bucket and avoid",
            "\t * unlocking the rcu.",
            "\t */",
            "\tif (!bucket_cnt && (batch + 1 < htab->n_buckets)) {",
            "\t\tbatch++;",
            "\t\tgoto again_nocopy;",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "\tbpf_enable_instrumentation();",
            "\tif (bucket_cnt && (copy_to_user(ukeys + total * key_size, keys,",
            "\t    key_size * bucket_cnt) ||",
            "\t    copy_to_user(uvalues + total * value_size, values,",
            "\t    value_size * bucket_cnt))) {",
            "\t\tret = -EFAULT;",
            "\t\tgoto after_loop;",
            "\t}",
            "",
            "\ttotal += bucket_cnt;",
            "\tbatch++;",
            "\tif (batch >= htab->n_buckets) {",
            "\t\tret = -ENOENT;",
            "\t\tgoto after_loop;",
            "\t}",
            "\tgoto again;",
            "",
            "after_loop:",
            "\tif (ret == -EFAULT)",
            "\t\tgoto out;",
            "",
            "\t/* copy # of entries and next batch */",
            "\tubatch = u64_to_user_ptr(attr->batch.out_batch);",
            "\tif (copy_to_user(ubatch, &batch, sizeof(batch)) ||",
            "\t    put_user(total, &uattr->batch.count))",
            "\t\tret = -EFAULT;",
            "",
            "out:",
            "\tkvfree(keys);",
            "\tkvfree(values);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "htab_lru_percpu_map_lookup_and_delete_elem, __htab_map_lookup_and_delete_batch",
          "description": "处理批量查找删除操作，通过RCU读锁遍历指定桶内元素，支持普通/PERCPU/LRU类型。动态分配缓冲区复制键值，处理锁竞争和内存溢出情况，返回操作结果及统计信息。",
          "similarity": 0.5431249141693115
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 1941,
          "end_line": 2043,
          "content": [
            "static int",
            "htab_percpu_map_lookup_batch(struct bpf_map *map, const union bpf_attr *attr,",
            "\t\t\t     union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, false,",
            "\t\t\t\t\t\t  false, true);",
            "}",
            "static int",
            "htab_percpu_map_lookup_and_delete_batch(struct bpf_map *map,",
            "\t\t\t\t\tconst union bpf_attr *attr,",
            "\t\t\t\t\tunion bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, true,",
            "\t\t\t\t\t\t  false, true);",
            "}",
            "static int",
            "htab_map_lookup_batch(struct bpf_map *map, const union bpf_attr *attr,",
            "\t\t      union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, false,",
            "\t\t\t\t\t\t  false, false);",
            "}",
            "static int",
            "htab_map_lookup_and_delete_batch(struct bpf_map *map,",
            "\t\t\t\t const union bpf_attr *attr,",
            "\t\t\t\t union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, true,",
            "\t\t\t\t\t\t  false, false);",
            "}",
            "static int",
            "htab_lru_percpu_map_lookup_batch(struct bpf_map *map,",
            "\t\t\t\t const union bpf_attr *attr,",
            "\t\t\t\t union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, false,",
            "\t\t\t\t\t\t  true, true);",
            "}",
            "static int",
            "htab_lru_percpu_map_lookup_and_delete_batch(struct bpf_map *map,",
            "\t\t\t\t\t    const union bpf_attr *attr,",
            "\t\t\t\t\t    union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, true,",
            "\t\t\t\t\t\t  true, true);",
            "}",
            "static int",
            "htab_lru_map_lookup_batch(struct bpf_map *map, const union bpf_attr *attr,",
            "\t\t\t  union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, false,",
            "\t\t\t\t\t\t  true, false);",
            "}",
            "static int",
            "htab_lru_map_lookup_and_delete_batch(struct bpf_map *map,",
            "\t\t\t\t     const union bpf_attr *attr,",
            "\t\t\t\t     union bpf_attr __user *uattr)",
            "{",
            "\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, true,",
            "\t\t\t\t\t\t  true, false);",
            "}",
            "static int __bpf_hash_map_seq_show(struct seq_file *seq, struct htab_elem *elem)",
            "{",
            "\tstruct bpf_iter_seq_hash_map_info *info = seq->private;",
            "\tu32 roundup_key_size, roundup_value_size;",
            "\tstruct bpf_iter__bpf_map_elem ctx = {};",
            "\tstruct bpf_map *map = info->map;",
            "\tstruct bpf_iter_meta meta;",
            "\tint ret = 0, off = 0, cpu;",
            "\tstruct bpf_prog *prog;",
            "\tvoid __percpu *pptr;",
            "",
            "\tmeta.seq = seq;",
            "\tprog = bpf_iter_get_info(&meta, elem == NULL);",
            "\tif (prog) {",
            "\t\tctx.meta = &meta;",
            "\t\tctx.map = info->map;",
            "\t\tif (elem) {",
            "\t\t\troundup_key_size = round_up(map->key_size, 8);",
            "\t\t\tctx.key = elem->key;",
            "\t\t\tif (!info->percpu_value_buf) {",
            "\t\t\t\tctx.value = elem->key + roundup_key_size;",
            "\t\t\t} else {",
            "\t\t\t\troundup_value_size = round_up(map->value_size, 8);",
            "\t\t\t\tpptr = htab_elem_get_ptr(elem, map->key_size);",
            "\t\t\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\t\t\tcopy_map_value_long(map, info->percpu_value_buf + off,",
            "\t\t\t\t\t\t\t    per_cpu_ptr(pptr, cpu));",
            "\t\t\t\t\tcheck_and_init_map_value(map, info->percpu_value_buf + off);",
            "\t\t\t\t\toff += roundup_value_size;",
            "\t\t\t\t}",
            "\t\t\t\tctx.value = info->percpu_value_buf;",
            "\t\t\t}",
            "\t\t}",
            "\t\tret = bpf_iter_run_prog(prog, &ctx);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "static int bpf_hash_map_seq_show(struct seq_file *seq, void *v)",
            "{",
            "\treturn __bpf_hash_map_seq_show(seq, v);",
            "}"
          ],
          "function_name": "htab_percpu_map_lookup_batch, htab_percpu_map_lookup_and_delete_batch, htab_map_lookup_batch, htab_map_lookup_and_delete_batch, htab_lru_percpu_map_lookup_batch, htab_lru_percpu_map_lookup_and_delete_batch, htab_lru_map_lookup_batch, htab_lru_map_lookup_and_delete_batch, __bpf_hash_map_seq_show, bpf_hash_map_seq_show",
          "description": "提供多种批量操作接口封装，统一调用__htab_map_lookup_and_delete_batch实现。包含序列化展示函数，处理PERCPU值的特殊复制逻辑，支持迭代器上下文管理。",
          "similarity": 0.5384365916252136
        },
        {
          "chunk_id": 15,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 2555,
          "end_line": 2611,
          "content": [
            "int bpf_fd_htab_map_lookup_elem(struct bpf_map *map, void *key, u32 *value)",
            "{",
            "\tvoid **ptr;",
            "\tint ret = 0;",
            "",
            "\tif (!map->ops->map_fd_sys_lookup_elem)",
            "\t\treturn -ENOTSUPP;",
            "",
            "\trcu_read_lock();",
            "\tptr = htab_map_lookup_elem(map, key);",
            "\tif (ptr)",
            "\t\t*value = map->ops->map_fd_sys_lookup_elem(READ_ONCE(*ptr));",
            "\telse",
            "\t\tret = -ENOENT;",
            "\trcu_read_unlock();",
            "",
            "\treturn ret;",
            "}",
            "int bpf_fd_htab_map_update_elem(struct bpf_map *map, struct file *map_file,",
            "\t\t\t\tvoid *key, void *value, u64 map_flags)",
            "{",
            "\tvoid *ptr;",
            "\tint ret;",
            "\tu32 ufd = *(u32 *)value;",
            "",
            "\tptr = map->ops->map_fd_get_ptr(map, map_file, ufd);",
            "\tif (IS_ERR(ptr))",
            "\t\treturn PTR_ERR(ptr);",
            "",
            "\tret = htab_map_update_elem(map, key, &ptr, map_flags);",
            "\tif (ret)",
            "\t\tmap->ops->map_fd_put_ptr(map, ptr, false);",
            "",
            "\treturn ret;",
            "}",
            "static int htab_of_map_gen_lookup(struct bpf_map *map,",
            "\t\t\t\t  struct bpf_insn *insn_buf)",
            "{",
            "\tstruct bpf_insn *insn = insn_buf;",
            "\tconst int ret = BPF_REG_0;",
            "",
            "\tBUILD_BUG_ON(!__same_type(&__htab_map_lookup_elem,",
            "\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));",
            "\t*insn++ = BPF_EMIT_CALL(__htab_map_lookup_elem);",
            "\t*insn++ = BPF_JMP_IMM(BPF_JEQ, ret, 0, 2);",
            "\t*insn++ = BPF_ALU64_IMM(BPF_ADD, ret,",
            "\t\t\t\toffsetof(struct htab_elem, key) +",
            "\t\t\t\tround_up(map->key_size, 8));",
            "\t*insn++ = BPF_LDX_MEM(BPF_DW, ret, ret, 0);",
            "",
            "\treturn insn - insn_buf;",
            "}",
            "static void htab_of_map_free(struct bpf_map *map)",
            "{",
            "\tbpf_map_meta_free(map->inner_map_meta);",
            "\tfd_htab_map_free(map);",
            "}"
          ],
          "function_name": "bpf_fd_htab_map_lookup_elem, bpf_fd_htab_map_update_elem, htab_of_map_gen_lookup, htab_of_map_free",
          "description": "该代码段实现了基于文件描述符的哈希表操作，包含查找、更新及释放逻辑。  \n`bpf_fd_htab_map_lookup_elem` 和 `bpf_fd_htab_map_update_elem` 分别用于通过文件描述符键查找和更新哈希表项，依赖于 `map->ops` 中的回调函数。  \n`htab_of_map_gen_lookup` 生成 eBPF 指令以调用哈希表查找逻辑，`htab_of_map_free` 释放哈希表相关元数据；部分底层函数（如 `htab_map_lookup_elem`）未展示，上下文不完整。",
          "similarity": 0.5258850455284119
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 896,
          "end_line": 996,
          "content": [
            "static void htab_elem_free(struct bpf_htab *htab, struct htab_elem *l)",
            "{",
            "\tcheck_and_free_fields(htab, l);",
            "",
            "\tmigrate_disable();",
            "\tif (htab->map.map_type == BPF_MAP_TYPE_PERCPU_HASH)",
            "\t\tbpf_mem_cache_free(&htab->pcpu_ma, l->ptr_to_pptr);",
            "\tbpf_mem_cache_free(&htab->ma, l);",
            "\tmigrate_enable();",
            "}",
            "static void htab_put_fd_value(struct bpf_htab *htab, struct htab_elem *l)",
            "{",
            "\tstruct bpf_map *map = &htab->map;",
            "\tvoid *ptr;",
            "",
            "\tif (map->ops->map_fd_put_ptr) {",
            "\t\tptr = fd_htab_map_get_ptr(map, l);",
            "\t\tmap->ops->map_fd_put_ptr(map, ptr, true);",
            "\t}",
            "}",
            "static bool is_map_full(struct bpf_htab *htab)",
            "{",
            "\tif (htab->use_percpu_counter)",
            "\t\treturn __percpu_counter_compare(&htab->pcount, htab->map.max_entries,",
            "\t\t\t\t\t\tPERCPU_COUNTER_BATCH) >= 0;",
            "\treturn atomic_read(&htab->count) >= htab->map.max_entries;",
            "}",
            "static void inc_elem_count(struct bpf_htab *htab)",
            "{",
            "\tbpf_map_inc_elem_count(&htab->map);",
            "",
            "\tif (htab->use_percpu_counter)",
            "\t\tpercpu_counter_add_batch(&htab->pcount, 1, PERCPU_COUNTER_BATCH);",
            "\telse",
            "\t\tatomic_inc(&htab->count);",
            "}",
            "static void dec_elem_count(struct bpf_htab *htab)",
            "{",
            "\tbpf_map_dec_elem_count(&htab->map);",
            "",
            "\tif (htab->use_percpu_counter)",
            "\t\tpercpu_counter_add_batch(&htab->pcount, -1, PERCPU_COUNTER_BATCH);",
            "\telse",
            "\t\tatomic_dec(&htab->count);",
            "}",
            "static void free_htab_elem(struct bpf_htab *htab, struct htab_elem *l)",
            "{",
            "\thtab_put_fd_value(htab, l);",
            "",
            "\tif (htab_is_prealloc(htab)) {",
            "\t\tbpf_map_dec_elem_count(&htab->map);",
            "\t\tcheck_and_free_fields(htab, l);",
            "\t\tpcpu_freelist_push(&htab->freelist, &l->fnode);",
            "\t} else {",
            "\t\tdec_elem_count(htab);",
            "\t\thtab_elem_free(htab, l);",
            "\t}",
            "}",
            "static void pcpu_copy_value(struct bpf_htab *htab, void __percpu *pptr,",
            "\t\t\t    void *value, bool onallcpus)",
            "{",
            "\tif (!onallcpus) {",
            "\t\t/* copy true value_size bytes */",
            "\t\tcopy_map_value(&htab->map, this_cpu_ptr(pptr), value);",
            "\t} else {",
            "\t\tu32 size = round_up(htab->map.value_size, 8);",
            "\t\tint off = 0, cpu;",
            "",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tcopy_map_value_long(&htab->map, per_cpu_ptr(pptr, cpu), value + off);",
            "\t\t\toff += size;",
            "\t\t}",
            "\t}",
            "}",
            "static void pcpu_init_value(struct bpf_htab *htab, void __percpu *pptr,",
            "\t\t\t    void *value, bool onallcpus)",
            "{",
            "\t/* When not setting the initial value on all cpus, zero-fill element",
            "\t * values for other cpus. Otherwise, bpf program has no way to ensure",
            "\t * known initial values for cpus other than current one",
            "\t * (onallcpus=false always when coming from bpf prog).",
            "\t */",
            "\tif (!onallcpus) {",
            "\t\tint current_cpu = raw_smp_processor_id();",
            "\t\tint cpu;",
            "",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tif (cpu == current_cpu)",
            "\t\t\t\tcopy_map_value_long(&htab->map, per_cpu_ptr(pptr, cpu), value);",
            "\t\t\telse /* Since elem is preallocated, we cannot touch special fields */",
            "\t\t\t\tzero_map_value(&htab->map, per_cpu_ptr(pptr, cpu));",
            "\t\t}",
            "\t} else {",
            "\t\tpcpu_copy_value(htab, pptr, value, onallcpus);",
            "\t}",
            "}",
            "static bool fd_htab_map_needs_adjust(const struct bpf_htab *htab)",
            "{",
            "\treturn htab->map.map_type == BPF_MAP_TYPE_HASH_OF_MAPS &&",
            "\t       BITS_PER_LONG == 64;",
            "}"
          ],
          "function_name": "htab_elem_free, htab_put_fd_value, is_map_full, inc_elem_count, dec_elem_count, free_htab_elem, pcpu_copy_value, pcpu_init_value, fd_htab_map_needs_adjust",
          "description": "实现哈希表元素的内存释放与计数管理，包含元素字段释放、文件描述符值处理、满状态检测、元素计数增减及Per-CPU内存缓存操作",
          "similarity": 0.5093764066696167
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 2345,
          "end_line": 2464,
          "content": [
            "static int htab_percpu_map_gen_lookup(struct bpf_map *map, struct bpf_insn *insn_buf)",
            "{",
            "\tstruct bpf_insn *insn = insn_buf;",
            "",
            "\tif (!bpf_jit_supports_percpu_insn())",
            "\t\treturn -EOPNOTSUPP;",
            "",
            "\tBUILD_BUG_ON(!__same_type(&__htab_map_lookup_elem,",
            "\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));",
            "\t*insn++ = BPF_EMIT_CALL(__htab_map_lookup_elem);",
            "\t*insn++ = BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 3);",
            "\t*insn++ = BPF_ALU64_IMM(BPF_ADD, BPF_REG_0,",
            "\t\t\t\toffsetof(struct htab_elem, key) + roundup(map->key_size, 8));",
            "\t*insn++ = BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_0, 0);",
            "\t*insn++ = BPF_MOV64_PERCPU_REG(BPF_REG_0, BPF_REG_0);",
            "",
            "\treturn insn - insn_buf;",
            "}",
            "int bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value)",
            "{",
            "\tstruct htab_elem *l;",
            "\tvoid __percpu *pptr;",
            "\tint ret = -ENOENT;",
            "\tint cpu, off = 0;",
            "\tu32 size;",
            "",
            "\t/* per_cpu areas are zero-filled and bpf programs can only",
            "\t * access 'value_size' of them, so copying rounded areas",
            "\t * will not leak any kernel data",
            "\t */",
            "\tsize = round_up(map->value_size, 8);",
            "\trcu_read_lock();",
            "\tl = __htab_map_lookup_elem(map, key);",
            "\tif (!l)",
            "\t\tgoto out;",
            "\t/* We do not mark LRU map element here in order to not mess up",
            "\t * eviction heuristics when user space does a map walk.",
            "\t */",
            "\tpptr = htab_elem_get_ptr(l, map->key_size);",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tcopy_map_value_long(map, value + off, per_cpu_ptr(pptr, cpu));",
            "\t\tcheck_and_init_map_value(map, value + off);",
            "\t\toff += size;",
            "\t}",
            "\tret = 0;",
            "out:",
            "\trcu_read_unlock();",
            "\treturn ret;",
            "}",
            "int bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,",
            "\t\t\t   u64 map_flags)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tint ret;",
            "",
            "\trcu_read_lock();",
            "\tif (htab_is_lru(htab))",
            "\t\tret = __htab_lru_percpu_map_update_elem(map, key, value,",
            "\t\t\t\t\t\t\tmap_flags, true);",
            "\telse",
            "\t\tret = __htab_percpu_map_update_elem(map, key, value, map_flags,",
            "\t\t\t\t\t\t    true);",
            "\trcu_read_unlock();",
            "",
            "\treturn ret;",
            "}",
            "static void htab_percpu_map_seq_show_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\t  struct seq_file *m)",
            "{",
            "\tstruct htab_elem *l;",
            "\tvoid __percpu *pptr;",
            "\tint cpu;",
            "",
            "\trcu_read_lock();",
            "",
            "\tl = __htab_map_lookup_elem(map, key);",
            "\tif (!l) {",
            "\t\trcu_read_unlock();",
            "\t\treturn;",
            "\t}",
            "",
            "\tbtf_type_seq_show(map->btf, map->btf_key_type_id, key, m);",
            "\tseq_puts(m, \": {\\n\");",
            "\tpptr = htab_elem_get_ptr(l, map->key_size);",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tseq_printf(m, \"\\tcpu%d: \", cpu);",
            "\t\tbtf_type_seq_show(map->btf, map->btf_value_type_id,",
            "\t\t\t\t  per_cpu_ptr(pptr, cpu), m);",
            "\t\tseq_puts(m, \"\\n\");",
            "\t}",
            "\tseq_puts(m, \"}\\n\");",
            "",
            "\trcu_read_unlock();",
            "}",
            "static int fd_htab_map_alloc_check(union bpf_attr *attr)",
            "{",
            "\tif (attr->value_size != sizeof(u32))",
            "\t\treturn -EINVAL;",
            "\treturn htab_map_alloc_check(attr);",
            "}",
            "static void fd_htab_map_free(struct bpf_map *map)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tstruct hlist_nulls_node *n;",
            "\tstruct hlist_nulls_head *head;",
            "\tstruct htab_elem *l;",
            "\tint i;",
            "",
            "\tfor (i = 0; i < htab->n_buckets; i++) {",
            "\t\thead = select_bucket(htab, i);",
            "",
            "\t\thlist_nulls_for_each_entry_safe(l, n, head, hash_node) {",
            "\t\t\tvoid *ptr = fd_htab_map_get_ptr(map, l);",
            "",
            "\t\t\tmap->ops->map_fd_put_ptr(map, ptr, false);",
            "\t\t}",
            "\t}",
            "",
            "\thtab_map_free(map);",
            "}"
          ],
          "function_name": "htab_percpu_map_gen_lookup, bpf_percpu_hash_copy, bpf_percpu_hash_update, htab_percpu_map_seq_show_elem, fd_htab_map_alloc_check, fd_htab_map_free",
          "description": "生成PERCPU哈希表查找指令，实现值复制/更新操作，支持序列化展示多CPU值。包含资源分配校验和释放函数，确保PERCPU映射生命周期管理及内存安全。",
          "similarity": 0.5033966302871704
        }
      ]
    }
  ]
}