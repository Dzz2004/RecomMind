{
  "query": "Slab Allocator 碎片管理实现细节",
  "timestamp": "2025-12-26 00:08:34",
  "retrieved_files": [
    {
      "source_file": "mm/slab.h",
      "md_summary": "> 自动生成时间: 2025-12-07 17:22:03\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `slab.h`\n\n---\n\n# `slab.h` 技术文档\n\n## 1. 文件概述\n\n`slab.h` 是 Linux 内核内存管理子系统中 SLAB/SLUB 分配器的核心内部头文件，定义了 slab 分配器所使用的底层数据结构（如 `struct slab` 和 `struct kmem_cache`）、关键宏和辅助函数。该文件主要用于在页（`struct page`）与 slab 表示之间进行安全转换，并提供对 slab 元数据的原子访问机制，以支持高性能、可扩展的对象缓存分配。\n\n此头文件专供内核内存管理内部使用，不对外暴露给模块开发者，是实现 SLUB（默认）或 SLAB 分配器的关键基础设施。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`freelist_aba_t`**  \n  联合体，将空闲对象指针（`freelist`）与计数器（`counter`）打包为一个原子单元，用于避免 ABA 问题（Compare-and-Swap 中因值重复导致的逻辑错误）。\n\n- **`struct slab`**  \n  slab 的内部表示，复用 `struct page` 的内存布局。包含：\n  - 所属的 `kmem_cache`\n  - 空闲对象链表（`freelist`）\n  - 对象使用计数（`inuse`）、总对象数（`objects`）\n  - 冻结状态（`frozen`，用于调试）\n  - RCU 回收头（`rcu_head`）\n  - 引用计数（`__page_refcount`）\n  - 可选的 per-object 扩展数据（`obj_exts`）\n\n- **`struct kmem_cache_order_objects`**  \n  封装 slab 阶数（order）与对象数量的复合值，支持原子读写。\n\n- **`struct kmem_cache`**  \n  slab 缓存描述符，包含：\n  - 每 CPU 缓存（`cpu_slab`）\n  - 对象大小（`size`, `object_size`）\n  - 构造函数（`ctor`）\n  - 对齐要求（`align`）\n  - 分配标志（`allocflags`）\n  - NUMA 相关参数（如 `remote_node_defrag_ratio`）\n  - 安全特性（如 `random` 用于 freelist 加固）\n\n### 主要宏与辅助函数\n\n- **类型安全转换宏**：\n  - `folio_slab()` / `slab_folio()`：在 `folio` 与 `slab` 之间安全转换\n  - `page_slab()` / `slab_page()`：兼容旧代码，在 `page` 与 `slab` 之间转换\n\n- **slab 属性访问函数**：\n  - `slab_address()`：获取 slab 起始虚拟地址\n  - `slab_nid()` / `slab_pgdat()`：获取所属 NUMA 节点和内存域\n  - `slab_order()` / `slab_size()`：获取分配阶数和总字节数\n\n- **pfmemalloc 标志操作**：\n  - `slab_test_pfmemalloc()` / `slab_set_pfmemalloc()` 等：标记 slab 是否来自紧急内存预留区（用于网络交换等场景）\n\n- **每 CPU partial slab 支持（`CONFIG_SLUB_CPU_PARTIAL`）**：\n  - `slub_percpu_partial()` 等宏：管理每 CPU 的 partial slab 链表\n\n## 3. 关键实现\n\n### 内存布局复用与静态断言\n\n- `struct slab` 并非独立分配，而是直接复用 `struct page` 的内存空间。通过 `static_assert` 确保关键字段偏移一致（如 `flags` ↔ `__page_flags`），保证类型转换安全。\n- 整个 `struct slab` 大小不超过 `struct page`，确保无越界访问。\n\n### ABA 问题防护\n\n- 在支持 `cmpxchg128`（64 位）或 `cmpxchg64`（32 位）的架构上，启用 `freelist_aba_t` 结构，将 `freelist` 指针与递增计数器打包为单个原子单元。\n- 使用 `try_cmpxchg_freelist` 进行原子更新，防止因指针值循环重用导致的 ABA 错误。\n- 若系统不支持对齐的 `struct page`（`!CONFIG_HAVE_ALIGNED_STRUCT_PAGE`），则禁用此优化。\n\n### 类型安全转换\n\n- 使用 C11 `_Generic` 实现类型安全的 `folio`/`slab`/`page` 转换，避免强制类型转换带来的风险，并为未来重构（如完全迁移到 folio）预留接口。\n\n### pfmemalloc 标志复用\n\n- 利用 `folio` 的 `PG_active` 位存储 `pfmemalloc` 标志，指示该 slab 是否从紧急内存池分配，用于网络子系统在内存压力下仍能分配 skb 等关键结构。\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/page.h>` / `<linux/folio.h>`：通过 `folio_*` 系列函数操作底层内存\n  - `<linux/reciprocal_div.h>`：用于快速除法（计算对象索引）\n  - `<linux/rcupdate.h>`：通过 `rcu_head` 支持 RCU 安全的 slab 回收\n\n- **可选依赖（由 Kconfig 控制）**：\n  - `CONFIG_SLUB_CPU_PARTIAL`：每 CPU partial slab 优化\n  - `CONFIG_SLAB_OBJ_EXT`：per-object 扩展元数据\n  - `CONFIG_SLAB_FREELIST_HARDENED`：freelist 指针随机化加固\n  - `CONFIG_NUMA`：NUMA 感知分配与碎片整理\n  - `CONFIG_KASAN` / `CONFIG_KFENCE`：内存错误检测集成\n\n- **与内存控制器集成**：\n  - 通过 `memcg_data` 字段（复用 `obj_exts`）支持 memcg 内存统计\n\n## 5. 使用场景\n\n- **SLUB 分配器内部**：作为 `slub.c` 的核心数据结构定义，用于管理 slab 生命周期、对象分配/释放。\n- **内存回收路径**：在 direct reclaim 或 kswapd 中，通过 `slab_folio` 获取 folio 信息以决策回收策略。\n- **调试与监控**：sysfs (`kobj`)、KASAN/KFENCE 集成依赖此结构获取 slab 元数据。\n- **网络子系统**：通过 `pfmemalloc` 标志识别紧急内存分配，确保高优先级数据包处理不被阻塞。\n- **NUMA 优化**：在远程节点分配时使用 `remote_node_defrag_ratio` 参数控制跨节点分配行为。\n- **安全加固**：`SLAB_FREELIST_HARDENED` 利用 `random` 字段混淆 freelist 指针，防止堆利用攻击。",
      "similarity": 0.6035460233688354,
      "chunks": []
    },
    {
      "source_file": "mm/slub.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:23:28\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `slub.c`\n\n---\n\n# slub.c 技术文档\n\n## 1. 文件概述\n\n`slub.c` 是 Linux 内核中 SLUB（Simple Low-overhead Unqueued Allocator）内存分配器的核心实现文件。SLUB 是一种高效的 slab 分配器，旨在减少缓存行使用并避免在每个 CPU 和节点上维护复杂的对象队列。它通过 per-slab 锁或原子操作进行同步，仅在管理部分填充的 slab 池时使用集中式锁。该分配器优化了常见路径的性能，同时支持调试、内存检测和热插拔等高级功能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `kmem_cache`: slab 缓存描述符，包含对象大小、对齐方式、构造函数等元数据\n- `kmem_cache_cpu`: 每 CPU 的 slab 管理结构，包含当前 CPU 的活跃 slab 和局部 freelist\n- `slab`: slab 描述符（通常嵌入在 page 结构中），包含 freelist、inuse 计数、objects 总数和 frozen 状态\n\n### 关键机制\n- **CPU slab**: 每个 CPU 分配专用的 slab 进行快速分配\n- **Partial slab 列表**: 节点级别的部分填充 slab 列表\n- **CPU partial slab**: CPU 本地的部分填充 slab 缓存，用于加速释放操作\n- **Frozen slab**: 冻结状态的 slab，免于全局列表管理\n\n### 锁机制层次\n1. `slab_mutex` - 全局互斥锁，保护所有 slab 列表和元数据变更\n2. `node->list_lock` - 自旋锁，保护节点的 partial/full 列表\n3. `kmem_cache->cpu_slab->lock` - 本地锁，保护慢路径的 per-CPU 字段\n4. `slab_lock(slab)` - slab 锁（仅在不支持 `cmpxchg_double` 的架构上使用）\n5. `object_map_lock` - 调试用途的对象映射锁\n\n## 3. 关键实现\n\n### 锁无关快速路径\n- 分配 (`slab_alloc_node()`) 和释放 (`do_slab_free()`) 操作在满足条件时完全无锁\n- 使用事务 ID (tid) 字段检测抢占或 CPU 迁移\n- 在支持 `cmpxchg_double` 的架构上避免使用 slab_lock\n\n### Slab 状态管理\n- **Node partial slab**: `PG_Workingset && !frozen`\n- **CPU partial slab**: `!PG_Workingset && !frozen`\n- **CPU slab**: `!PG_Workingset && frozen`\n- **Full slab**: `!PG_Workingset && !frozen`\n\n### PREEMPT_RT 支持\n- 在 RT 内核中禁用锁无关快速路径\n- 使用 `migrate_disable()/enable()` 替代 `preempt_disable()/enable()`\n- 本地锁始终被获取以确保 RT 安全性\n\n### 内存管理优化\n- 最小化 slab 设置/拆卸开销，依赖页分配器的 per-CPU 缓存\n- 空 slab 直接释放回页分配器\n- CPU partial slab 机制加速批量释放操作\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **内存管理**: `<linux/mm.h>`, `<linux/swap.h>`, `<linux/memory.h>`\n- **同步原语**: `<linux/bit_spinlock.h>`, `<linux/interrupt.h>`\n- **调试支持**: `<linux/kasan.h>`, `<linux/kmsan.h>`, `<linux/kfence.h>`, `<linux/debugobjects.h>`\n- **系统设施**: `<linux/module.h>`, `<linux/proc_fs.h>`, `<linux/debugfs.h>`\n- **内存控制**: `<linux/memcontrol.h>`, `<linux/cpuset.h>`, `<linux/mempolicy.h>`\n- **测试框架**: `<kunit/test.h>`\n\n### 内部依赖\n- `\"slab.h\"` - slab 分配器通用接口\n- `\"internal.h\"` - 内存管理内部实现\n\n### 子系统交互\n- **页分配器**: 作为底层内存来源\n- **内存热插拔**: 通过 `slab_mutex` 同步回调\n- **内存控制器**: 集成 memcg 功能\n- **跟踪系统**: 通过 `trace/events/kmem.h` 提供分配事件跟踪\n\n## 5. 使用场景\n\n### 内核内存分配\n- 为内核对象（如 task_struct、inode、dentry 等）提供高效的小内存分配\n- 作为 `kmalloc()` 系列函数的底层实现\n- 支持不同大小类别的内存请求（8 字节到几 KB）\n\n### 性能关键路径\n- 中断上下文中的内存分配（通过适当的锁机制保证安全）\n- 高频分配/释放场景（利用 per-CPU slab 和 lockless 快速路径）\n- 批量分配操作（通过 CPU partial slab 优化）\n\n### 调试和监控\n- 内存错误检测（KASAN、KMSAN、KFENCE 集成）\n- 内存泄漏检测（kmemleak 集成）\n- 性能分析（通过 `/proc/slabinfo` 和 debugfs 接口）\n- 故障注入测试（fault-inject 支持）\n\n### 特殊环境支持\n- 实时系统（PREEMPT_RT 配置）\n- 内存受限系统（CONFIG_SLUB_TINY 优化）\n- NUMA 系统（节点感知分配）\n- 内存热插拔环境",
      "similarity": 0.567449688911438,
      "chunks": [
        {
          "chunk_id": 32,
          "file_path": "mm/slub.c",
          "start_line": 6074,
          "end_line": 6180,
          "content": [
            "static int add_location(struct loc_track *t, struct kmem_cache *s,",
            "\t\t\t\tconst struct track *track,",
            "\t\t\t\tunsigned int orig_size)",
            "{",
            "\tlong start, end, pos;",
            "\tstruct location *l;",
            "\tunsigned long caddr, chandle, cwaste;",
            "\tunsigned long age = jiffies - track->when;",
            "\tdepot_stack_handle_t handle = 0;",
            "\tunsigned int waste = s->object_size - orig_size;",
            "",
            "#ifdef CONFIG_STACKDEPOT",
            "\thandle = READ_ONCE(track->handle);",
            "#endif",
            "\tstart = -1;",
            "\tend = t->count;",
            "",
            "\tfor ( ; ; ) {",
            "\t\tpos = start + (end - start + 1) / 2;",
            "",
            "\t\t/*",
            "\t\t * There is nothing at \"end\". If we end up there",
            "\t\t * we need to add something to before end.",
            "\t\t */",
            "\t\tif (pos == end)",
            "\t\t\tbreak;",
            "",
            "\t\tl = &t->loc[pos];",
            "\t\tcaddr = l->addr;",
            "\t\tchandle = l->handle;",
            "\t\tcwaste = l->waste;",
            "\t\tif ((track->addr == caddr) && (handle == chandle) &&",
            "\t\t\t(waste == cwaste)) {",
            "",
            "\t\t\tl->count++;",
            "\t\t\tif (track->when) {",
            "\t\t\t\tl->sum_time += age;",
            "\t\t\t\tif (age < l->min_time)",
            "\t\t\t\t\tl->min_time = age;",
            "\t\t\t\tif (age > l->max_time)",
            "\t\t\t\t\tl->max_time = age;",
            "",
            "\t\t\t\tif (track->pid < l->min_pid)",
            "\t\t\t\t\tl->min_pid = track->pid;",
            "\t\t\t\tif (track->pid > l->max_pid)",
            "\t\t\t\t\tl->max_pid = track->pid;",
            "",
            "\t\t\t\tcpumask_set_cpu(track->cpu,",
            "\t\t\t\t\t\tto_cpumask(l->cpus));",
            "\t\t\t}",
            "\t\t\tnode_set(page_to_nid(virt_to_page(track)), l->nodes);",
            "\t\t\treturn 1;",
            "\t\t}",
            "",
            "\t\tif (track->addr < caddr)",
            "\t\t\tend = pos;",
            "\t\telse if (track->addr == caddr && handle < chandle)",
            "\t\t\tend = pos;",
            "\t\telse if (track->addr == caddr && handle == chandle &&",
            "\t\t\t\twaste < cwaste)",
            "\t\t\tend = pos;",
            "\t\telse",
            "\t\t\tstart = pos;",
            "\t}",
            "",
            "\t/*",
            "\t * Not found. Insert new tracking element.",
            "\t */",
            "\tif (t->count >= t->max && !alloc_loc_track(t, 2 * t->max, GFP_ATOMIC))",
            "\t\treturn 0;",
            "",
            "\tl = t->loc + pos;",
            "\tif (pos < t->count)",
            "\t\tmemmove(l + 1, l,",
            "\t\t\t(t->count - pos) * sizeof(struct location));",
            "\tt->count++;",
            "\tl->count = 1;",
            "\tl->addr = track->addr;",
            "\tl->sum_time = age;",
            "\tl->min_time = age;",
            "\tl->max_time = age;",
            "\tl->min_pid = track->pid;",
            "\tl->max_pid = track->pid;",
            "\tl->handle = handle;",
            "\tl->waste = waste;",
            "\tcpumask_clear(to_cpumask(l->cpus));",
            "\tcpumask_set_cpu(track->cpu, to_cpumask(l->cpus));",
            "\tnodes_clear(l->nodes);",
            "\tnode_set(page_to_nid(virt_to_page(track)), l->nodes);",
            "\treturn 1;",
            "}",
            "static void process_slab(struct loc_track *t, struct kmem_cache *s,",
            "\t\tstruct slab *slab, enum track_item alloc,",
            "\t\tunsigned long *obj_map)",
            "{",
            "\tvoid *addr = slab_address(slab);",
            "\tbool is_alloc = (alloc == TRACK_ALLOC);",
            "\tvoid *p;",
            "",
            "\t__fill_map(obj_map, s, slab);",
            "",
            "\tfor_each_object(p, s, addr, slab->objects)",
            "\t\tif (!test_bit(__obj_to_index(s, addr, p), obj_map))",
            "\t\t\tadd_location(t, s, get_track(s, p, alloc),",
            "\t\t\t\t     is_alloc ? get_orig_size(s, p) :",
            "\t\t\t\t\t\ts->object_size);",
            "}"
          ],
          "function_name": "add_location, process_slab",
          "description": "记录对象分配位置信息，遍历slab对象并更新追踪数据，支持调试分析内存分配路径和来源。",
          "similarity": 0.6420583724975586
        },
        {
          "chunk_id": 33,
          "file_path": "mm/slub.c",
          "start_line": 6200,
          "end_line": 6310,
          "content": [
            "static ssize_t show_slab_objects(struct kmem_cache *s,",
            "\t\t\t\t char *buf, unsigned long flags)",
            "{",
            "\tunsigned long total = 0;",
            "\tint node;",
            "\tint x;",
            "\tunsigned long *nodes;",
            "\tint len = 0;",
            "",
            "\tnodes = kcalloc(nr_node_ids, sizeof(unsigned long), GFP_KERNEL);",
            "\tif (!nodes)",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (flags & SO_CPU) {",
            "\t\tint cpu;",
            "",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tstruct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab,",
            "\t\t\t\t\t\t\t       cpu);",
            "\t\t\tint node;",
            "\t\t\tstruct slab *slab;",
            "",
            "\t\t\tslab = READ_ONCE(c->slab);",
            "\t\t\tif (!slab)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tnode = slab_nid(slab);",
            "\t\t\tif (flags & SO_TOTAL)",
            "\t\t\t\tx = slab->objects;",
            "\t\t\telse if (flags & SO_OBJECTS)",
            "\t\t\t\tx = slab->inuse;",
            "\t\t\telse",
            "\t\t\t\tx = 1;",
            "",
            "\t\t\ttotal += x;",
            "\t\t\tnodes[node] += x;",
            "",
            "#ifdef CONFIG_SLUB_CPU_PARTIAL",
            "\t\t\tslab = slub_percpu_partial_read_once(c);",
            "\t\t\tif (slab) {",
            "\t\t\t\tnode = slab_nid(slab);",
            "\t\t\t\tif (flags & SO_TOTAL)",
            "\t\t\t\t\tWARN_ON_ONCE(1);",
            "\t\t\t\telse if (flags & SO_OBJECTS)",
            "\t\t\t\t\tWARN_ON_ONCE(1);",
            "\t\t\t\telse",
            "\t\t\t\t\tx = data_race(slab->slabs);",
            "\t\t\t\ttotal += x;",
            "\t\t\t\tnodes[node] += x;",
            "\t\t\t}",
            "#endif",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * It is impossible to take \"mem_hotplug_lock\" here with \"kernfs_mutex\"",
            "\t * already held which will conflict with an existing lock order:",
            "\t *",
            "\t * mem_hotplug_lock->slab_mutex->kernfs_mutex",
            "\t *",
            "\t * We don't really need mem_hotplug_lock (to hold off",
            "\t * slab_mem_going_offline_callback) here because slab's memory hot",
            "\t * unplug code doesn't destroy the kmem_cache->node[] data.",
            "\t */",
            "",
            "#ifdef CONFIG_SLUB_DEBUG",
            "\tif (flags & SO_ALL) {",
            "\t\tstruct kmem_cache_node *n;",
            "",
            "\t\tfor_each_kmem_cache_node(s, node, n) {",
            "",
            "\t\t\tif (flags & SO_TOTAL)",
            "\t\t\t\tx = node_nr_objs(n);",
            "\t\t\telse if (flags & SO_OBJECTS)",
            "\t\t\t\tx = node_nr_objs(n) - count_partial(n, count_free);",
            "\t\t\telse",
            "\t\t\t\tx = node_nr_slabs(n);",
            "\t\t\ttotal += x;",
            "\t\t\tnodes[node] += x;",
            "\t\t}",
            "",
            "\t} else",
            "#endif",
            "\tif (flags & SO_PARTIAL) {",
            "\t\tstruct kmem_cache_node *n;",
            "",
            "\t\tfor_each_kmem_cache_node(s, node, n) {",
            "\t\t\tif (flags & SO_TOTAL)",
            "\t\t\t\tx = count_partial(n, count_total);",
            "\t\t\telse if (flags & SO_OBJECTS)",
            "\t\t\t\tx = count_partial(n, count_inuse);",
            "\t\t\telse",
            "\t\t\t\tx = n->nr_partial;",
            "\t\t\ttotal += x;",
            "\t\t\tnodes[node] += x;",
            "\t\t}",
            "\t}",
            "",
            "\tlen += sysfs_emit_at(buf, len, \"%lu\", total);",
            "#ifdef CONFIG_NUMA",
            "\tfor (node = 0; node < nr_node_ids; node++) {",
            "\t\tif (nodes[node])",
            "\t\t\tlen += sysfs_emit_at(buf, len, \" N%d=%lu\",",
            "\t\t\t\t\t     node, nodes[node]);",
            "\t}",
            "#endif",
            "\tlen += sysfs_emit_at(buf, len, \"\\n\");",
            "\tkfree(nodes);",
            "",
            "\treturn len;",
            "}"
          ],
          "function_name": "show_slab_objects",
          "description": "统计并展示各节点上slab对象数量，根据标志位区分总对象数、部分列表对象数及CPU局部缓存对象分布。",
          "similarity": 0.6129804849624634
        },
        {
          "chunk_id": 14,
          "file_path": "mm/slub.c",
          "start_line": 2555,
          "end_line": 2657,
          "content": [
            "static void rcu_free_slab(struct rcu_head *h)",
            "{",
            "\tstruct slab *slab = container_of(h, struct slab, rcu_head);",
            "",
            "\t__free_slab(slab->slab_cache, slab);",
            "}",
            "static void free_slab(struct kmem_cache *s, struct slab *slab)",
            "{",
            "\tif (kmem_cache_debug_flags(s, SLAB_CONSISTENCY_CHECKS)) {",
            "\t\tvoid *p;",
            "",
            "\t\tslab_pad_check(s, slab);",
            "\t\tfor_each_object(p, s, slab_address(slab), slab->objects)",
            "\t\t\tcheck_object(s, slab, p, SLUB_RED_INACTIVE);",
            "\t}",
            "",
            "\tif (unlikely(s->flags & SLAB_TYPESAFE_BY_RCU))",
            "\t\tcall_rcu(&slab->rcu_head, rcu_free_slab);",
            "\telse",
            "\t\t__free_slab(s, slab);",
            "}",
            "static void discard_slab(struct kmem_cache *s, struct slab *slab)",
            "{",
            "\tdec_slabs_node(s, slab_nid(slab), slab->objects);",
            "\tfree_slab(s, slab);",
            "}",
            "static inline bool slab_test_node_partial(const struct slab *slab)",
            "{",
            "\treturn folio_test_workingset((struct folio *)slab_folio(slab));",
            "}",
            "static inline void slab_set_node_partial(struct slab *slab)",
            "{",
            "\tset_bit(PG_workingset, folio_flags(slab_folio(slab), 0));",
            "}",
            "static inline void slab_clear_node_partial(struct slab *slab)",
            "{",
            "\tclear_bit(PG_workingset, folio_flags(slab_folio(slab), 0));",
            "}",
            "static inline void",
            "__add_partial(struct kmem_cache_node *n, struct slab *slab, int tail)",
            "{",
            "\tn->nr_partial++;",
            "\tif (tail == DEACTIVATE_TO_TAIL)",
            "\t\tlist_add_tail(&slab->slab_list, &n->partial);",
            "\telse",
            "\t\tlist_add(&slab->slab_list, &n->partial);",
            "\tslab_set_node_partial(slab);",
            "}",
            "static inline void add_partial(struct kmem_cache_node *n,",
            "\t\t\t\tstruct slab *slab, int tail)",
            "{",
            "\tlockdep_assert_held(&n->list_lock);",
            "\t__add_partial(n, slab, tail);",
            "}",
            "static inline void remove_partial(struct kmem_cache_node *n,",
            "\t\t\t\t\tstruct slab *slab)",
            "{",
            "\tlockdep_assert_held(&n->list_lock);",
            "\tlist_del(&slab->slab_list);",
            "\tslab_clear_node_partial(slab);",
            "\tn->nr_partial--;",
            "}",
            "static inline void put_cpu_partial(struct kmem_cache *s, struct slab *slab,",
            "\t\t\t\t   int drain) { }",
            "static inline unsigned long next_tid(unsigned long tid)",
            "{",
            "\treturn tid + TID_STEP;",
            "}",
            "static inline unsigned int tid_to_cpu(unsigned long tid)",
            "{",
            "\treturn tid % TID_STEP;",
            "}",
            "static inline unsigned long tid_to_event(unsigned long tid)",
            "{",
            "\treturn tid / TID_STEP;",
            "}",
            "static inline unsigned int init_tid(int cpu)",
            "{",
            "\treturn cpu;",
            "}",
            "static inline void note_cmpxchg_failure(const char *n,",
            "\t\tconst struct kmem_cache *s, unsigned long tid)",
            "{",
            "#ifdef SLUB_DEBUG_CMPXCHG",
            "\tunsigned long actual_tid = __this_cpu_read(s->cpu_slab->tid);",
            "",
            "\tpr_info(\"%s %s: cmpxchg redo \", n, s->name);",
            "",
            "#ifdef CONFIG_PREEMPTION",
            "\tif (tid_to_cpu(tid) != tid_to_cpu(actual_tid))",
            "\t\tpr_warn(\"due to cpu change %d -> %d\\n\",",
            "\t\t\ttid_to_cpu(tid), tid_to_cpu(actual_tid));",
            "\telse",
            "#endif",
            "\tif (tid_to_event(tid) != tid_to_event(actual_tid))",
            "\t\tpr_warn(\"due to cpu running other code. Event %ld->%ld\\n\",",
            "\t\t\ttid_to_event(tid), tid_to_event(actual_tid));",
            "\telse",
            "\t\tpr_warn(\"for unknown reason: actual=%lx was=%lx target=%lx\\n\",",
            "\t\t\tactual_tid, tid, next_tid(tid));",
            "#endif",
            "\tstat(s, CMPXCHG_DOUBLE_CPU_FAIL);",
            "}"
          ],
          "function_name": "rcu_free_slab, free_slab, discard_slab, slab_test_node_partial, slab_set_node_partial, slab_clear_node_partial, __add_partial, add_partial, remove_partial, put_cpu_partial, next_tid, tid_to_cpu, tid_to_event, init_tid, note_cmpxchg_failure",
          "description": "实现基于RCU的slab延迟释放机制，管理部分slab的缓存队列（partial list），包含CPU亲和性跟踪（TID）相关操作及并发控制辅助函数。",
          "similarity": 0.6120237112045288
        },
        {
          "chunk_id": 20,
          "file_path": "mm/slub.c",
          "start_line": 4277,
          "end_line": 4387,
          "content": [
            "static void __slab_free(struct kmem_cache *s, struct slab *slab,",
            "\t\t\tvoid *head, void *tail, int cnt,",
            "\t\t\tunsigned long addr)",
            "",
            "{",
            "\tvoid *prior;",
            "\tint was_frozen;",
            "\tstruct slab new;",
            "\tunsigned long counters;",
            "\tstruct kmem_cache_node *n = NULL;",
            "\tunsigned long flags;",
            "\tbool on_node_partial;",
            "",
            "\tstat(s, FREE_SLOWPATH);",
            "",
            "\tif (IS_ENABLED(CONFIG_SLUB_TINY) || kmem_cache_debug(s)) {",
            "\t\tfree_to_partial_list(s, slab, head, tail, cnt, addr);",
            "\t\treturn;",
            "\t}",
            "",
            "\tdo {",
            "\t\tif (unlikely(n)) {",
            "\t\t\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "\t\t\tn = NULL;",
            "\t\t}",
            "\t\tprior = slab->freelist;",
            "\t\tcounters = slab->counters;",
            "\t\tset_freepointer(s, tail, prior);",
            "\t\tnew.counters = counters;",
            "\t\twas_frozen = new.frozen;",
            "\t\tnew.inuse -= cnt;",
            "\t\tif ((!new.inuse || !prior) && !was_frozen) {",
            "\t\t\t/* Needs to be taken off a list */",
            "\t\t\tif (!kmem_cache_has_cpu_partial(s) || prior) {",
            "",
            "\t\t\t\tn = get_node(s, slab_nid(slab));",
            "\t\t\t\t/*",
            "\t\t\t\t * Speculatively acquire the list_lock.",
            "\t\t\t\t * If the cmpxchg does not succeed then we may",
            "\t\t\t\t * drop the list_lock without any processing.",
            "\t\t\t\t *",
            "\t\t\t\t * Otherwise the list_lock will synchronize with",
            "\t\t\t\t * other processors updating the list of slabs.",
            "\t\t\t\t */",
            "\t\t\t\tspin_lock_irqsave(&n->list_lock, flags);",
            "",
            "\t\t\t\ton_node_partial = slab_test_node_partial(slab);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t} while (!slab_update_freelist(s, slab,",
            "\t\tprior, counters,",
            "\t\thead, new.counters,",
            "\t\t\"__slab_free\"));",
            "",
            "\tif (likely(!n)) {",
            "",
            "\t\tif (likely(was_frozen)) {",
            "\t\t\t/*",
            "\t\t\t * The list lock was not taken therefore no list",
            "\t\t\t * activity can be necessary.",
            "\t\t\t */",
            "\t\t\tstat(s, FREE_FROZEN);",
            "\t\t} else if (kmem_cache_has_cpu_partial(s) && !prior) {",
            "\t\t\t/*",
            "\t\t\t * If we started with a full slab then put it onto the",
            "\t\t\t * per cpu partial list.",
            "\t\t\t */",
            "\t\t\tput_cpu_partial(s, slab, 1);",
            "\t\t\tstat(s, CPU_PARTIAL_FREE);",
            "\t\t}",
            "",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * This slab was partially empty but not on the per-node partial list,",
            "\t * in which case we shouldn't manipulate its list, just return.",
            "\t */",
            "\tif (prior && !on_node_partial) {",
            "\t\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (unlikely(!new.inuse && n->nr_partial >= s->min_partial))",
            "\t\tgoto slab_empty;",
            "",
            "\t/*",
            "\t * Objects left in the slab. If it was not on the partial list before",
            "\t * then add it.",
            "\t */",
            "\tif (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {",
            "\t\tadd_partial(n, slab, DEACTIVATE_TO_TAIL);",
            "\t\tstat(s, FREE_ADD_PARTIAL);",
            "\t}",
            "\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "\treturn;",
            "",
            "slab_empty:",
            "\tif (prior) {",
            "\t\t/*",
            "\t\t * Slab on the partial list.",
            "\t\t */",
            "\t\tremove_partial(n, slab);",
            "\t\tstat(s, FREE_REMOVE_PARTIAL);",
            "\t}",
            "",
            "\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "\tstat(s, FREE_SLAB);",
            "\tdiscard_slab(s, slab);",
            "}"
          ],
          "function_name": "__slab_free",
          "description": "实现SLUB分配器的慢速路径释放逻辑，负责更新slab的空闲指针、计数器及管理部分空闲slab的节点挂载，处理并发场景下的slab状态变更和列表迁移。",
          "similarity": 0.6094101667404175
        },
        {
          "chunk_id": 19,
          "file_path": "mm/slub.c",
          "start_line": 3927,
          "end_line": 4047,
          "content": [
            "static __always_inline void maybe_wipe_obj_freeptr(struct kmem_cache *s,",
            "\t\t\t\t\t\t   void *obj)",
            "{",
            "\tif (unlikely(slab_want_init_on_free(s)) && obj &&",
            "\t    !freeptr_outside_object(s))",
            "\t\tmemset((void *)((char *)kasan_reset_tag(obj) + s->offset),",
            "\t\t\t0, sizeof(void *));",
            "}",
            "static __fastpath_inline",
            "bool slab_post_alloc_hook(struct kmem_cache *s, struct list_lru *lru,",
            "\t\t\t  gfp_t flags, size_t size, void **p, bool init,",
            "\t\t\t  unsigned int orig_size)",
            "{",
            "\tunsigned int zero_size = s->object_size;",
            "\tbool kasan_init = init;",
            "\tsize_t i;",
            "\tgfp_t init_flags = flags & gfp_allowed_mask;",
            "",
            "\t/*",
            "\t * For kmalloc object, the allocated memory size(object_size) is likely",
            "\t * larger than the requested size(orig_size). If redzone check is",
            "\t * enabled for the extra space, don't zero it, as it will be redzoned",
            "\t * soon. The redzone operation for this extra space could be seen as a",
            "\t * replacement of current poisoning under certain debug option, and",
            "\t * won't break other sanity checks.",
            "\t */",
            "\tif (kmem_cache_debug_flags(s, SLAB_STORE_USER | SLAB_RED_ZONE) &&",
            "\t    (s->flags & SLAB_KMALLOC))",
            "\t\tzero_size = orig_size;",
            "",
            "\t/*",
            "\t * When slab_debug is enabled, avoid memory initialization integrated",
            "\t * into KASAN and instead zero out the memory via the memset below with",
            "\t * the proper size. Otherwise, KASAN might overwrite SLUB redzones and",
            "\t * cause false-positive reports. This does not lead to a performance",
            "\t * penalty on production builds, as slab_debug is not intended to be",
            "\t * enabled there.",
            "\t */",
            "\tif (__slub_debug_enabled())",
            "\t\tkasan_init = false;",
            "",
            "\t/*",
            "\t * As memory initialization might be integrated into KASAN,",
            "\t * kasan_slab_alloc and initialization memset must be",
            "\t * kept together to avoid discrepancies in behavior.",
            "\t *",
            "\t * As p[i] might get tagged, memset and kmemleak hook come after KASAN.",
            "\t */",
            "\tfor (i = 0; i < size; i++) {",
            "\t\tp[i] = kasan_slab_alloc(s, p[i], init_flags, kasan_init);",
            "\t\tif (p[i] && init && (!kasan_init ||",
            "\t\t\t\t     !kasan_has_integrated_init()))",
            "\t\t\tmemset(p[i], 0, zero_size);",
            "\t\tkmemleak_alloc_recursive(p[i], s->object_size, 1,",
            "\t\t\t\t\t s->flags, init_flags);",
            "\t\tkmsan_slab_alloc(s, p[i], init_flags);",
            "\t\talloc_tagging_slab_alloc_hook(s, p[i], flags);",
            "\t}",
            "",
            "\treturn memcg_slab_post_alloc_hook(s, lru, flags, size, p);",
            "}",
            "static noinline void free_to_partial_list(",
            "\tstruct kmem_cache *s, struct slab *slab,",
            "\tvoid *head, void *tail, int bulk_cnt,",
            "\tunsigned long addr)",
            "{",
            "\tstruct kmem_cache_node *n = get_node(s, slab_nid(slab));",
            "\tstruct slab *slab_free = NULL;",
            "\tint cnt = bulk_cnt;",
            "\tunsigned long flags;",
            "\tdepot_stack_handle_t handle = 0;",
            "",
            "\tif (s->flags & SLAB_STORE_USER)",
            "\t\thandle = set_track_prepare();",
            "",
            "\tspin_lock_irqsave(&n->list_lock, flags);",
            "",
            "\tif (free_debug_processing(s, slab, head, tail, &cnt, addr, handle)) {",
            "\t\tvoid *prior = slab->freelist;",
            "",
            "\t\t/* Perform the actual freeing while we still hold the locks */",
            "\t\tslab->inuse -= cnt;",
            "\t\tset_freepointer(s, tail, prior);",
            "\t\tslab->freelist = head;",
            "",
            "\t\t/*",
            "\t\t * If the slab is empty, and node's partial list is full,",
            "\t\t * it should be discarded anyway no matter it's on full or",
            "\t\t * partial list.",
            "\t\t */",
            "\t\tif (slab->inuse == 0 && n->nr_partial >= s->min_partial)",
            "\t\t\tslab_free = slab;",
            "",
            "\t\tif (!prior) {",
            "\t\t\t/* was on full list */",
            "\t\t\tremove_full(s, n, slab);",
            "\t\t\tif (!slab_free) {",
            "\t\t\t\tadd_partial(n, slab, DEACTIVATE_TO_TAIL);",
            "\t\t\t\tstat(s, FREE_ADD_PARTIAL);",
            "\t\t\t}",
            "\t\t} else if (slab_free) {",
            "\t\t\tremove_partial(n, slab);",
            "\t\t\tstat(s, FREE_REMOVE_PARTIAL);",
            "\t\t}",
            "\t}",
            "",
            "\tif (slab_free) {",
            "\t\t/*",
            "\t\t * Update the counters while still holding n->list_lock to",
            "\t\t * prevent spurious validation warnings",
            "\t\t */",
            "\t\tdec_slabs_node(s, slab_nid(slab_free), slab_free->objects);",
            "\t}",
            "",
            "\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "",
            "\tif (slab_free) {",
            "\t\tstat(s, FREE_SLAB);",
            "\t\tfree_slab(s, slab_free);",
            "\t}",
            "}"
          ],
          "function_name": "maybe_wipe_obj_freeptr, slab_post_alloc_hook, free_to_partial_list",
          "description": "该代码段涉及SLUB内存管理器的关键逻辑：  \n1. `maybe_wipe_obj_freeptr` 用于在对象分配后清除freeptr指针以防止误用，仅在启用调试模式且对象未超出边界时执行。  \n2. `slab_post_alloc_hook` 处理分配后的安全初始化、红区标记及KASAN集成，通过条件判断控制是否对额外空间进行零填充以兼容调试检查。  \n3. `free_to_partial_list` 管理slab的回收策略，将空闲对象移至partial链表或直接释放，根据slab使用情况动态调整full/partial列表状态。  \n\n注：代码上下文完整，未引入未声明的API或机制。",
          "similarity": 0.6019941568374634
        }
      ]
    },
    {
      "source_file": "mm/list_lru.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:35:23\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `list_lru.c`\n\n---\n\n# list_lru.c 技术文档\n\n## 1. 文件概述\n\n`list_lru.c` 实现了 Linux 内核中通用的 **List-based LRU（Least Recently Used）基础设施**，用于管理可回收对象的双向链表。该机制支持按 NUMA 节点（node）和内存控制组（memcg）进行细粒度组织，便于内存压力下的高效回收。主要服务于 slab 分配器等子系统，作为 shrinker 框架的一部分，在内存紧张时协助释放非活跃对象。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct list_lru`：顶层 LRU 管理结构，包含 per-node 的 `list_lru_node`\n- `struct list_lru_node`：每个 NUMA 节点对应的 LRU 节点，含自旋锁和总项数\n- `struct list_lru_one`：实际存储对象链表和计数的单元（per-memcg per-node）\n- `struct list_lru_memcg`：当启用 `CONFIG_MEMCG` 时，为每个 memcg 存储 per-node 的 `list_lru_one`\n\n### 主要导出函数\n- `list_lru_add()` / `list_lru_add_obj()`：向 LRU 添加对象\n- `list_lru_del()` / `list_lru_del_obj()`：从 LRU 删除对象\n- `list_lru_isolate()` / `list_lru_isolate_move()`：在回收过程中隔离对象\n- `list_lru_count_one()` / `list_lru_count_node()`：查询 LRU 中对象数量\n- `list_lru_walk_one()` / `list_lru_walk_node()`：遍历并处理 LRU 中的对象（用于 shrinker 回调）\n\n### 内部辅助函数\n- `list_lru_from_memcg_idx()`：根据 memcg ID 获取对应的 `list_lru_one`\n- `__list_lru_walk_one()`：带锁的 LRU 遍历核心逻辑\n- `list_lru_register()` / `list_lru_unregister()`：注册/注销 memcg-aware 的 LRU（用于全局追踪）\n\n## 3. 关键实现\n\n### 内存控制组（memcg）支持\n- 通过 `CONFIG_MEMCG` 条件编译控制 memcg 相关逻辑\n- 使用 XArray (`lru->xa`) 动态存储每个 memcg 对应的 `list_lru_memcg` 结构\n- 每个 memcg 在每个 NUMA 节点上拥有独立的 `list_lru_one`，实现资源隔离\n- 全局 `memcg_list_lrus` 链表和 `list_lrus_mutex` 用于跟踪所有 memcg-aware 的 LRU 实例\n\n### 并发控制\n- 每个 NUMA 节点 (`list_lru_node`) 拥有独立的自旋锁 (`nlru->lock`)\n- 所有对 LRU 链表的操作（增、删、遍历）均在对应节点锁保护下进行\n- 提供 `_irq` 版本的遍历函数（`list_lru_walk_one_irq`）用于中断上下文\n\n### 回收遍历机制\n- `list_lru_walk_*` 函数接受回调函数 `isolate`，由调用者定义回收策略\n- 回调返回值控制遍历行为：\n  - `LRU_REMOVED`：成功移除\n  - `LRU_REMOVED_RETRY`：移除后需重新开始遍历（锁曾被释放）\n  - `LRU_RETRY`：未移除但需重新开始遍历\n  - `LRU_ROTATE`：将对象移到链表尾部（标记为最近使用）\n  - `LRU_SKIP`：跳过当前对象\n  - `LRU_STOP`：立即停止遍历\n- 通过 `nr_to_walk` 限制单次遍历的最大对象数，防止长时间持锁\n\n### Shrinker 集成\n- 当向空的 `list_lru_one` 添加首个对象时，调用 `set_shrinker_bit()` 标记该 memcg/node 需要被 shrinker 处理\n- `lru_shrinker_id()` 返回关联的 shrinker ID，用于通知内存回收子系统\n\n### 对象归属识别\n- `list_lru_add_obj()` / `list_lru_del_obj()` 通过 `mem_cgroup_from_slab_obj()` 自动获取对象所属的 memcg\n- 使用 `page_to_nid(virt_to_page(item))` 确定对象所在的 NUMA 节点\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/list_lru.h>`：定义核心数据结构和 API\n  - `<linux/memcontrol.h>`：memcg 相关接口（如 `memcg_kmem_id`）\n  - `\"slab.h\"` 和 `\"internal.h\"`：slab 分配器内部接口（如 `mem_cgroup_from_slab_obj`）\n- **配置依赖**：\n  - `CONFIG_MEMCG`：决定是否编译 memcg 相关代码\n  - `CONFIG_NUMA`：影响 per-node 数据结构的大小（通过 `nr_node_ids`）\n- **子系统依赖**：\n  - Slab 分配器：作为主要使用者，管理可回收 slab 对象\n  - Memory Control Group (memcg)：提供内存隔离和记账\n  - Shrinker 框架：通过 shrinker 回调触发 LRU 遍历回收\n\n## 5. 使用场景\n\n- **Slab 对象回收**：当系统内存压力大时，shrinker 通过 `list_lru_walk_*` 遍历 inactive slab 对象链表，释放可回收对象\n- **Per-memcg 内存限制**：在 cgroup 内存超限时，仅遍历该 memcg 对应的 LRU 部分，实现精确回收\n- **NUMA 感知管理**：按 NUMA 节点分离 LRU 链表，减少远程内存访问，提升性能\n- **通用 LRU 容器**：任何需要按 LRU 策略管理可回收对象的内核子系统均可使用此基础设施（如 dentry、inode 缓存等）",
      "similarity": 0.5673943161964417,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/list_lru.c",
          "start_line": 22,
          "end_line": 129,
          "content": [
            "static inline bool list_lru_memcg_aware(struct list_lru *lru)",
            "{",
            "\treturn lru->memcg_aware;",
            "}",
            "static void list_lru_register(struct list_lru *lru)",
            "{",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_add(&lru->list, &memcg_list_lrus);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static void list_lru_unregister(struct list_lru *lru)",
            "{",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_del(&lru->list);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static int lru_shrinker_id(struct list_lru *lru)",
            "{",
            "\treturn lru->shrinker_id;",
            "}",
            "static void list_lru_register(struct list_lru *lru)",
            "{",
            "}",
            "static void list_lru_unregister(struct list_lru *lru)",
            "{",
            "}",
            "static int lru_shrinker_id(struct list_lru *lru)",
            "{",
            "\treturn -1;",
            "}",
            "static inline bool list_lru_memcg_aware(struct list_lru *lru)",
            "{",
            "\treturn false;",
            "}",
            "bool list_lru_add(struct list_lru *lru, struct list_head *item, int nid,",
            "\t\t    struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tif (list_empty(item)) {",
            "\t\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\t\tlist_add_tail(item, &l->list);",
            "\t\t/* Set shrinker bit if the first element was added */",
            "\t\tif (!l->nr_items++)",
            "\t\t\tset_shrinker_bit(memcg, nid, lru_shrinker_id(lru));",
            "\t\tnlru->nr_items++;",
            "\t\tspin_unlock(&nlru->lock);",
            "\t\treturn true;",
            "\t}",
            "\tspin_unlock(&nlru->lock);",
            "\treturn false;",
            "}",
            "bool list_lru_add_obj(struct list_lru *lru, struct list_head *item)",
            "{",
            "\tbool ret;",
            "\tint nid = page_to_nid(virt_to_page(item));",
            "",
            "\tif (list_lru_memcg_aware(lru)) {",
            "\t\trcu_read_lock();",
            "\t\tret = list_lru_add(lru, item, nid, mem_cgroup_from_slab_obj(item));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tret = list_lru_add(lru, item, nid, NULL);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "bool list_lru_del(struct list_lru *lru, struct list_head *item, int nid,",
            "\t\t    struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tif (!list_empty(item)) {",
            "\t\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\t\tlist_del_init(item);",
            "\t\tl->nr_items--;",
            "\t\tnlru->nr_items--;",
            "\t\tspin_unlock(&nlru->lock);",
            "\t\treturn true;",
            "\t}",
            "\tspin_unlock(&nlru->lock);",
            "\treturn false;",
            "}",
            "bool list_lru_del_obj(struct list_lru *lru, struct list_head *item)",
            "{",
            "\tbool ret;",
            "\tint nid = page_to_nid(virt_to_page(item));",
            "",
            "\tif (list_lru_memcg_aware(lru)) {",
            "\t\trcu_read_lock();",
            "\t\tret = list_lru_del(lru, item, nid, mem_cgroup_from_slab_obj(item));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tret = list_lru_del(lru, item, nid, NULL);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "list_lru_memcg_aware, list_lru_register, list_lru_unregister, lru_shrinker_id, list_lru_register, list_lru_unregister, lru_shrinker_id, list_lru_memcg_aware, list_lru_add, list_lru_add_obj, list_lru_del, list_lru_del_obj",
          "description": "实现了LRU列表的添加/删除操作，支持MemCG感知的节点和内存组粒度管理，包含处理多核、内存组切换及RCU安全访问的逻辑。",
          "similarity": 0.5434736013412476
        },
        {
          "chunk_id": 5,
          "file_path": "mm/list_lru.c",
          "start_line": 556,
          "end_line": 605,
          "content": [
            "static inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)",
            "{",
            "}",
            "static void memcg_destroy_list_lru(struct list_lru *lru)",
            "{",
            "}",
            "int __list_lru_init(struct list_lru *lru, bool memcg_aware,",
            "\t\t    struct lock_class_key *key, struct shrinker *shrinker)",
            "{",
            "\tint i;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tif (shrinker)",
            "\t\tlru->shrinker_id = shrinker->id;",
            "\telse",
            "\t\tlru->shrinker_id = -1;",
            "#endif",
            "",
            "\tlru->node = kcalloc(nr_node_ids, sizeof(*lru->node), GFP_KERNEL);",
            "\tif (!lru->node)",
            "\t\treturn -ENOMEM;",
            "",
            "\tfor_each_node(i) {",
            "\t\tspin_lock_init(&lru->node[i].lock);",
            "\t\tif (key)",
            "\t\t\tlockdep_set_class(&lru->node[i].lock, key);",
            "\t\tinit_one_lru(&lru->node[i].lru);",
            "\t}",
            "",
            "\tmemcg_init_list_lru(lru, memcg_aware);",
            "\tlist_lru_register(lru);",
            "",
            "\treturn 0;",
            "}",
            "void list_lru_destroy(struct list_lru *lru)",
            "{",
            "\t/* Already destroyed or not yet initialized? */",
            "\tif (!lru->node)",
            "\t\treturn;",
            "",
            "\tlist_lru_unregister(lru);",
            "",
            "\tmemcg_destroy_list_lru(lru);",
            "\tkfree(lru->node);",
            "\tlru->node = NULL;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tlru->shrinker_id = -1;",
            "#endif",
            "}"
          ],
          "function_name": "memcg_init_list_lru, memcg_destroy_list_lru, __list_lru_init, list_lru_destroy",
          "description": "该代码段实现了基于内存控制组（MEMCG）的LRU列表管理功能。  \n`__list_lru_init` 初始化 `list_lru` 结构体并注册到系统，其中包含 MEMCG 相关的 shrinker ID 设置及节点锁初始化；`list_lru_destroy` 反向清理资源，但 `memcg_init_list_lru` 和 `memcg_destroy_list_lru` 的具体实现缺失，上下文不完整。",
          "similarity": 0.5340109467506409
        },
        {
          "chunk_id": 4,
          "file_path": "mm/list_lru.c",
          "start_line": 425,
          "end_line": 551,
          "content": [
            "static void memcg_reparent_list_lru(struct list_lru *lru,",
            "\t\t\t\t    int src_idx, struct mem_cgroup *dst_memcg)",
            "{",
            "\tint i;",
            "",
            "\tfor_each_node(i)",
            "\t\tmemcg_reparent_list_lru_node(lru, i, src_idx, dst_memcg);",
            "",
            "\tmemcg_list_lru_free(lru, src_idx);",
            "}",
            "void memcg_reparent_list_lrus(struct mem_cgroup *memcg, struct mem_cgroup *parent)",
            "{",
            "\tstruct cgroup_subsys_state *css;",
            "\tstruct list_lru *lru;",
            "\tint src_idx = memcg->kmemcg_id;",
            "",
            "\t/*",
            "\t * Change kmemcg_id of this cgroup and all its descendants to the",
            "\t * parent's id, and then move all entries from this cgroup's list_lrus",
            "\t * to ones of the parent.",
            "\t *",
            "\t * After we have finished, all list_lrus corresponding to this cgroup",
            "\t * are guaranteed to remain empty. So we can safely free this cgroup's",
            "\t * list lrus in memcg_list_lru_free().",
            "\t *",
            "\t * Changing ->kmemcg_id to the parent can prevent memcg_list_lru_alloc()",
            "\t * from allocating list lrus for this cgroup after memcg_list_lru_free()",
            "\t * call.",
            "\t */",
            "\trcu_read_lock();",
            "\tcss_for_each_descendant_pre(css, &memcg->css) {",
            "\t\tstruct mem_cgroup *child;",
            "",
            "\t\tchild = mem_cgroup_from_css(css);",
            "\t\tWRITE_ONCE(child->kmemcg_id, parent->kmemcg_id);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_for_each_entry(lru, &memcg_list_lrus, list)",
            "\t\tmemcg_reparent_list_lru(lru, src_idx, parent);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static inline bool memcg_list_lru_allocated(struct mem_cgroup *memcg,",
            "\t\t\t\t\t    struct list_lru *lru)",
            "{",
            "\tint idx = memcg->kmemcg_id;",
            "",
            "\treturn idx < 0 || xa_load(&lru->xa, idx);",
            "}",
            "int memcg_list_lru_alloc(struct mem_cgroup *memcg, struct list_lru *lru,",
            "\t\t\t gfp_t gfp)",
            "{",
            "\tint i;",
            "\tunsigned long flags;",
            "\tstruct list_lru_memcg_table {",
            "\t\tstruct list_lru_memcg *mlru;",
            "\t\tstruct mem_cgroup *memcg;",
            "\t} *table;",
            "\tXA_STATE(xas, &lru->xa, 0);",
            "",
            "\tif (!list_lru_memcg_aware(lru) || memcg_list_lru_allocated(memcg, lru))",
            "\t\treturn 0;",
            "",
            "\tgfp &= GFP_RECLAIM_MASK;",
            "\ttable = kmalloc_array(memcg->css.cgroup->level, sizeof(*table), gfp);",
            "\tif (!table)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * Because the list_lru can be reparented to the parent cgroup's",
            "\t * list_lru, we should make sure that this cgroup and all its",
            "\t * ancestors have allocated list_lru_memcg.",
            "\t */",
            "\tfor (i = 0; memcg; memcg = parent_mem_cgroup(memcg), i++) {",
            "\t\tif (memcg_list_lru_allocated(memcg, lru))",
            "\t\t\tbreak;",
            "",
            "\t\ttable[i].memcg = memcg;",
            "\t\ttable[i].mlru = memcg_init_list_lru_one(gfp);",
            "\t\tif (!table[i].mlru) {",
            "\t\t\twhile (i--)",
            "\t\t\t\tkfree(table[i].mlru);",
            "\t\t\tkfree(table);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "\t}",
            "",
            "\txas_lock_irqsave(&xas, flags);",
            "\twhile (i--) {",
            "\t\tint index = READ_ONCE(table[i].memcg->kmemcg_id);",
            "\t\tstruct list_lru_memcg *mlru = table[i].mlru;",
            "",
            "\t\txas_set(&xas, index);",
            "retry:",
            "\t\tif (unlikely(index < 0 || xas_error(&xas) || xas_load(&xas))) {",
            "\t\t\tkfree(mlru);",
            "\t\t} else {",
            "\t\t\txas_store(&xas, mlru);",
            "\t\t\tif (xas_error(&xas) == -ENOMEM) {",
            "\t\t\t\txas_unlock_irqrestore(&xas, flags);",
            "\t\t\t\tif (xas_nomem(&xas, gfp))",
            "\t\t\t\t\txas_set_err(&xas, 0);",
            "\t\t\t\txas_lock_irqsave(&xas, flags);",
            "\t\t\t\t/*",
            "\t\t\t\t * The xas lock has been released, this memcg",
            "\t\t\t\t * can be reparented before us. So reload",
            "\t\t\t\t * memcg id. More details see the comments",
            "\t\t\t\t * in memcg_reparent_list_lrus().",
            "\t\t\t\t */",
            "\t\t\t\tindex = READ_ONCE(table[i].memcg->kmemcg_id);",
            "\t\t\t\tif (index < 0)",
            "\t\t\t\t\txas_set_err(&xas, 0);",
            "\t\t\t\telse if (!xas_error(&xas) && index != xas.xa_index)",
            "\t\t\t\t\txas_set(&xas, index);",
            "\t\t\t\tgoto retry;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\t/* xas_nomem() is used to free memory instead of memory allocation. */",
            "\tif (xas.xa_alloc)",
            "\t\txas_nomem(&xas, gfp);",
            "\txas_unlock_irqrestore(&xas, flags);",
            "\tkfree(table);",
            "",
            "\treturn xas_error(&xas);",
            "}"
          ],
          "function_name": "memcg_reparent_list_lru, memcg_reparent_list_lrus, memcg_list_lru_allocated, memcg_list_lru_alloc",
          "description": "实现内存组层级间的LRU列表迁移与分配机制，包含递归子组处理、动态分配/释放LRU结构体及冲突解决逻辑。",
          "similarity": 0.5247762203216553
        },
        {
          "chunk_id": 3,
          "file_path": "mm/list_lru.c",
          "start_line": 289,
          "end_line": 400,
          "content": [
            "unsigned long",
            "list_lru_walk_one_irq(struct list_lru *lru, int nid, struct mem_cgroup *memcg,",
            "\t\t      list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t      unsigned long *nr_to_walk)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tunsigned long ret;",
            "",
            "\tspin_lock_irq(&nlru->lock);",
            "\tret = __list_lru_walk_one(lru, nid, memcg_kmem_id(memcg), isolate,",
            "\t\t\t\t  cb_arg, nr_to_walk);",
            "\tspin_unlock_irq(&nlru->lock);",
            "\treturn ret;",
            "}",
            "unsigned long list_lru_walk_node(struct list_lru *lru, int nid,",
            "\t\t\t\t list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t\t\t unsigned long *nr_to_walk)",
            "{",
            "\tlong isolated = 0;",
            "",
            "\tisolated += list_lru_walk_one(lru, nid, NULL, isolate, cb_arg,",
            "\t\t\t\t      nr_to_walk);",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tif (*nr_to_walk > 0 && list_lru_memcg_aware(lru)) {",
            "\t\tstruct list_lru_memcg *mlru;",
            "\t\tunsigned long index;",
            "",
            "\t\txa_for_each(&lru->xa, index, mlru) {",
            "\t\t\tstruct list_lru_node *nlru = &lru->node[nid];",
            "",
            "\t\t\tspin_lock(&nlru->lock);",
            "\t\t\tisolated += __list_lru_walk_one(lru, nid, index,",
            "\t\t\t\t\t\t\tisolate, cb_arg,",
            "\t\t\t\t\t\t\tnr_to_walk);",
            "\t\t\tspin_unlock(&nlru->lock);",
            "",
            "\t\t\tif (*nr_to_walk <= 0)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "#endif",
            "",
            "\treturn isolated;",
            "}",
            "static void init_one_lru(struct list_lru_one *l)",
            "{",
            "\tINIT_LIST_HEAD(&l->list);",
            "\tl->nr_items = 0;",
            "}",
            "static void memcg_list_lru_free(struct list_lru *lru, int src_idx)",
            "{",
            "\tstruct list_lru_memcg *mlru = xa_erase_irq(&lru->xa, src_idx);",
            "",
            "\t/*",
            "\t * The __list_lru_walk_one() can walk the list of this node.",
            "\t * We need kvfree_rcu() here. And the walking of the list",
            "\t * is under lru->node[nid]->lock, which can serve as a RCU",
            "\t * read-side critical section.",
            "\t */",
            "\tif (mlru)",
            "\t\tkvfree_rcu(mlru, rcu);",
            "}",
            "static inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)",
            "{",
            "\tif (memcg_aware)",
            "\t\txa_init_flags(&lru->xa, XA_FLAGS_LOCK_IRQ);",
            "\tlru->memcg_aware = memcg_aware;",
            "}",
            "static void memcg_destroy_list_lru(struct list_lru *lru)",
            "{",
            "\tXA_STATE(xas, &lru->xa, 0);",
            "\tstruct list_lru_memcg *mlru;",
            "",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\txas_lock_irq(&xas);",
            "\txas_for_each(&xas, mlru, ULONG_MAX) {",
            "\t\tkfree(mlru);",
            "\t\txas_store(&xas, NULL);",
            "\t}",
            "\txas_unlock_irq(&xas);",
            "}",
            "static void memcg_reparent_list_lru_node(struct list_lru *lru, int nid,",
            "\t\t\t\t\t int src_idx, struct mem_cgroup *dst_memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tint dst_idx = dst_memcg->kmemcg_id;",
            "\tstruct list_lru_one *src, *dst;",
            "",
            "\t/*",
            "\t * Since list_lru_{add,del} may be called under an IRQ-safe lock,",
            "\t * we have to use IRQ-safe primitives here to avoid deadlock.",
            "\t */",
            "\tspin_lock_irq(&nlru->lock);",
            "",
            "\tsrc = list_lru_from_memcg_idx(lru, nid, src_idx);",
            "\tif (!src)",
            "\t\tgoto out;",
            "\tdst = list_lru_from_memcg_idx(lru, nid, dst_idx);",
            "",
            "\tlist_splice_init(&src->list, &dst->list);",
            "",
            "\tif (src->nr_items) {",
            "\t\tdst->nr_items += src->nr_items;",
            "\t\tset_shrinker_bit(dst_memcg, nid, lru_shrinker_id(lru));",
            "\t\tsrc->nr_items = 0;",
            "\t}",
            "out:",
            "\tspin_unlock_irq(&nlru->lock);",
            "}"
          ],
          "function_name": "list_lru_walk_one_irq, list_lru_walk_node, init_one_lru, memcg_list_lru_free, memcg_init_list_lru, memcg_destroy_list_lru, memcg_reparent_list_lru_node",
          "description": "包含LRU节点初始化、内存组间列表迁移、资源释放等高级操作，涉及XA表管理、中断安全锁操作及内存组重新归属处理。",
          "similarity": 0.515612006187439
        },
        {
          "chunk_id": 0,
          "file_path": "mm/list_lru.c",
          "start_line": 1,
          "end_line": 21,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2013 Red Hat, Inc. and Parallels Inc. All rights reserved.",
            " * Authors: David Chinner and Glauber Costa",
            " *",
            " * Generic LRU infrastructure",
            " */",
            "#include <linux/kernel.h>",
            "#include <linux/module.h>",
            "#include <linux/mm.h>",
            "#include <linux/list_lru.h>",
            "#include <linux/slab.h>",
            "#include <linux/mutex.h>",
            "#include <linux/memcontrol.h>",
            "#include \"slab.h\"",
            "#include \"internal.h\"",
            "",
            "#ifdef CONFIG_MEMCG",
            "static LIST_HEAD(memcg_list_lrus);",
            "static DEFINE_MUTEX(list_lrus_mutex);",
            ""
          ],
          "function_name": null,
          "description": "定义了支持内存控制组（MemCG）的LRU基础设施，声明了全局链表头memcg_list_lrus和互斥锁list_lrus_mutex，用于管理MemCG环境下的LRU列表注册与注销操作。",
          "similarity": 0.4889673590660095
        }
      ]
    }
  ]
}