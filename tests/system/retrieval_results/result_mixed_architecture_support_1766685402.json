{
  "query": "mixed architecture support",
  "timestamp": "2025-12-26 01:56:42",
  "retrieved_files": [
    {
      "source_file": "kernel/dma/ops_helpers.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:14:54\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\ops_helpers.c`\n\n---\n\n# `dma/ops_helpers.c` 技术文档\n\n## 1. 文件概述\n\n`dma/ops_helpers.c` 是 Linux 内核中为 DMA（Direct Memory Access）操作提供通用辅助功能的实现文件。该文件封装了多个通用的 DMA 操作辅助函数，用于简化不同架构或设备驱动中 DMA 映射、内存分配、用户空间映射及 scatter-gather 表构建等常见任务。这些函数假设所分配的内存位于内核直接映射区域（normal pages in the direct kernel mapping），并依赖底层 `dma_map_ops` 操作集完成实际的硬件相关操作。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `dma_common_vaddr_to_page(void *cpu_addr)`  \n  将内核虚拟地址转换为对应的 `struct page *`，支持 `vmalloc` 区域和直接映射区域。\n\n- `dma_common_get_sgtable(struct device *dev, struct sg_table *sgt, void *cpu_addr, dma_addr_t dma_addr, size_t size, unsigned long attrs)`  \n  为已分配的 DMA 缓冲区创建单页 scatter-gather 表（`sg_table`）。\n\n- `dma_common_mmap(struct device *dev, struct vm_area_struct *vma, void *cpu_addr, dma_addr_t dma_addr, size_t size, unsigned long attrs)`  \n  为 DMA 一致性内存创建用户空间 mmap 映射。\n\n- `dma_common_alloc_pages(struct device *dev, size_t size, dma_addr_t *dma_handle, enum dma_data_direction dir, gfp_t gfp)`  \n  分配物理连续（或通过 CMA）的页面，并通过 DMA 映射操作获取设备可访问的总线地址。\n\n- `dma_common_free_pages(struct device *dev, size_t size, struct page *page, dma_addr_t dma_handle, enum dma_data_direction dir)`  \n  释放由 `dma_common_alloc_pages` 分配的页面，并取消 DMA 映射。\n\n### 数据结构\n\n- 无定义新的数据结构，主要使用内核通用结构：\n  - `struct page`\n  - `struct sg_table`\n  - `struct vm_area_struct`\n  - `struct device`\n\n## 3. 关键实现\n\n### 地址到页面转换\n`dma_common_vaddr_to_page` 函数首先判断传入的 CPU 虚拟地址是否属于 `vmalloc` 区域。若是，则调用 `vmalloc_to_page`；否则使用 `virt_to_page`。这确保了对内核不同内存区域的兼容性。\n\n### Scatter-Gather 表构建\n`dma_common_get_sgtable` 假设 DMA 缓冲区由单个物理页面（或连续页面）组成，因此只分配一个 scatterlist 条目，并通过 `sg_set_page` 设置页面、长度（按页对齐）和偏移（0）。\n\n### 用户空间 mmap 支持\n`dma_common_mmap` 函数：\n- 仅在 `CONFIG_MMU` 配置下有效；\n- 首先尝试通过 `dma_mmap_from_dev_coherent` 处理设备特定的一致性内存映射；\n- 若失败，则使用通用路径：将内核页面的 PFN（页帧号）加上 `vma->vm_pgoff`，通过 `remap_pfn_range` 映射到用户空间；\n- 映射前进行边界检查，防止越界访问；\n- 使用 `dma_pgprot` 根据设备属性调整页保护标志。\n\n### 页面分配与释放\n- `dma_common_alloc_pages` 优先尝试通过 CMA（Contiguous Memory Allocator）分配连续物理内存（`dma_alloc_contiguous`），失败后回退到 `alloc_pages_node`；\n- 分配成功后，调用设备的 `map_page` 操作获取 DMA 地址，并跳过 CPU 缓存同步（`DMA_ATTR_SKIP_CPU_SYNC`）；\n- 分配的内存会被清零；\n- 释放时先调用 `unmap_page`（若存在），再通过 `dma_free_contiguous` 释放物理页面。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/dma-map-ops.h>`：提供 `dma_map_ops`、`get_dma_ops`、`dma_alloc_contiguous` 等核心 DMA 操作接口。\n- **内核子系统依赖**：\n  - **内存管理子系统**：依赖 `vmalloc`、`page`、`pfn`、`remap_pfn_range` 等 MMU 相关机制；\n  - **CMA（Contiguous Memory Allocator）**：用于分配大块连续物理内存；\n  - **设备模型**：通过 `struct device` 获取 NUMA 节点（`dev_to_node`）和 DMA 操作集；\n  - **DMA 映射框架**：依赖各架构或平台实现的 `dma_map_ops`（如 `map_page`/`unmap_page`）。\n\n## 5. 使用场景\n\n- **设备驱动开发**：当驱动需要实现自定义的 `dma_map_ops` 时，可复用本文件中的通用函数，避免重复实现 scatterlist 构建、mmap 或页面分配逻辑。\n- **一致性 DMA 内存管理**：适用于需要分配一致性（coherent）DMA 内存并映射到用户空间的场景（如音视频、网络设备驱动）。\n- **简化 DMA 编程模型**：为不支持复杂 IOMMU 或 scatter-gather 的简单设备提供轻量级 DMA 操作封装。\n- **跨架构兼容性**：通过抽象底层差异，使驱动代码在不同架构（如 ARM、x86）上保持一致行为。",
      "similarity": 0.5100351572036743,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/ops_helpers.c",
          "start_line": 18,
          "end_line": 65,
          "content": [
            "int dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,",
            "\t\t void *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\t unsigned long attrs)",
            "{",
            "\tstruct page *page = dma_common_vaddr_to_page(cpu_addr);",
            "\tint ret;",
            "",
            "\tret = sg_alloc_table(sgt, 1, GFP_KERNEL);",
            "\tif (!ret)",
            "\t\tsg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);",
            "\treturn ret;",
            "}",
            "int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "#ifdef CONFIG_MMU",
            "\tunsigned long user_count = vma_pages(vma);",
            "\tunsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;",
            "\tunsigned long off = vma->vm_pgoff;",
            "\tstruct page *page = dma_common_vaddr_to_page(cpu_addr);",
            "\tint ret = -ENXIO;",
            "",
            "\tvma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);",
            "",
            "\tif (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))",
            "\t\treturn ret;",
            "",
            "\tif (off >= count || user_count > count - off)",
            "\t\treturn -ENXIO;",
            "",
            "\treturn remap_pfn_range(vma, vma->vm_start,",
            "\t\t\tpage_to_pfn(page) + vma->vm_pgoff,",
            "\t\t\tuser_count << PAGE_SHIFT, vma->vm_page_prot);",
            "#else",
            "\treturn -ENXIO;",
            "#endif /* CONFIG_MMU */",
            "}",
            "void dma_common_free_pages(struct device *dev, size_t size, struct page *page,",
            "\t\tdma_addr_t dma_handle, enum dma_data_direction dir)",
            "{",
            "\tconst struct dma_map_ops *ops = get_dma_ops(dev);",
            "",
            "\tif (ops->unmap_page)",
            "\t\tops->unmap_page(dev, dma_handle, size, dir,",
            "\t\t\t\tDMA_ATTR_SKIP_CPU_SYNC);",
            "\tdma_free_contiguous(dev, page, size);",
            "}"
          ],
          "function_name": "dma_common_get_sgtable, dma_common_mmap, dma_common_free_pages",
          "description": "该代码块实现了三个DMA操作辅助函数，dma_common_get_sgtable构建单页SG表用于DMA传输，dma_common_mmap处理设备内存映射并支持MMU配置，dma_common_free_pages负责解除页面映射并释放连续内存资源",
          "similarity": 0.5247459411621094
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/ops_helpers.c",
          "start_line": 1,
          "end_line": 17,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Helpers for DMA ops implementations.  These generally rely on the fact that",
            " * the allocated memory contains normal pages in the direct kernel mapping.",
            " */",
            "#include <linux/dma-map-ops.h>",
            "",
            "static struct page *dma_common_vaddr_to_page(void *cpu_addr)",
            "{",
            "\tif (is_vmalloc_addr(cpu_addr))",
            "\t\treturn vmalloc_to_page(cpu_addr);",
            "\treturn virt_to_page(cpu_addr);",
            "}",
            "",
            "/*",
            " * Create scatter-list for the already allocated DMA buffer.",
            " */"
          ],
          "function_name": null,
          "description": "该代码块定义了dma_common_vaddr_to_page函数，用于将CPU虚拟地址转换为对应的物理页面结构，通过判断地址是否属于vmalloc区域选择不同的转换方式。后续未展示的代码可能包含与DMA操作相关的辅助函数声明，当前上下文不完整",
          "similarity": 0.4382959008216858
        }
      ]
    },
    {
      "source_file": "kernel/dma/swiotlb.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:17:23\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\swiotlb.c`\n\n---\n\n# `dma/swiotlb.c` 技术文档\n\n## 1. 文件概述\n\n`swiotlb.c` 实现了 **Software I/O Translation Lookaside Buffer (SWIOTLB)**，即软件 I/O TLB 机制，作为硬件 I/O TLB（如 IOMMU）不可用时的 **DMA 映射回退方案**。该机制通过在低地址空间（通常为 32 位可寻址区域）预分配一块连续的“反弹缓冲区”（bounce buffer），用于在设备无法直接访问高地址内存时，中转 DMA 数据传输，从而确保 DMA 操作的正确性和兼容性。\n\n该文件最初由 Intel 和 HP 开发，现已演变为 Linux 内核通用的软件 DMA 映射基础设施，支持动态分配、高内存（highmem）、加密内存等高级特性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct io_tlb_slot`**  \n  描述 SWIOTLB 池中的一个槽位（slot）：\n  - `orig_addr`：原始物理地址（DMA 映射前的地址）\n  - `alloc_size`：分配的实际缓冲区大小\n  - `list`：用于空闲链表管理，表示从此索引开始的连续空闲槽位数\n  - `pad_slots`：前置填充槽位数量（仅在首个非填充槽有效）\n\n- **`struct io_tlb_area`**  \n  描述 SWIOTLB 内存池中的一个区域（area），用于细粒度并发控制：\n  - `used`：已使用的槽位数\n  - `index`：下一次分配的起始搜索索引\n  - `lock`：保护该区域数据结构的自旋锁\n\n- **`struct io_tlb_mem`**  \n  SWIOTLB 内存池的顶层结构（定义在头文件中），包含多个 `io_tlb_pool`，支持动态扩展（`CONFIG_SWIOTLB_DYNAMIC`）\n\n- **`struct io_tlb_pool`**  \n  单个 SWIOTLB 内存池，包含物理地址范围、槽位数组、区域数组等\n\n### 主要函数与接口\n\n- **`setup_io_tlb_npages(char *str)`**  \n  解析内核启动参数 `swiotlb=`，设置缓冲区大小、区域数量及强制启用/禁用策略。\n\n- **`swiotlb_size_or_default(void)`**  \n  返回当前配置的 SWIOTLB 缓冲区总大小（字节）。\n\n- **`swiotlb_adjust_size(unsigned long size)`**  \n  允许架构代码（如支持内存加密的 AMD SEV）在未显式指定 `swiotlb` 参数时调整缓冲区大小。\n\n- **`swiotlb_print_info(void)`**  \n  打印 SWIOTLB 缓冲区的映射信息（物理地址范围和大小）。\n\n- **`swiotlb_update_mem_attributes(void)`**  \n  在早期分配后，由架构代码调用以更新内存属性（如将内存标记为“解密”，用于 AMD SEV 等场景）。\n\n- **`swiotlb_init_io_tlb_pool(...)`**  \n  初始化一个 SWIOTLB 内存池，设置其物理/虚拟地址、槽位数量、区域划分等。\n\n- **`round_up_default_nslabs()` / `swiotlb_adjust_nareas()` / `limit_nareas()`**  \n  辅助函数，用于根据配置调整槽位数量和区域数量，确保对齐和性能优化。\n\n### 全局变量\n\n- `swiotlb_force_bounce`：强制所有 DMA 使用 SWIOTLB（即使设备支持高地址）\n- `swiotlb_force_disable`：禁用 SWIOTLB（即使设备需要）\n- `io_tlb_default_mem`：默认的 SWIOTLB 内存池实例\n- `default_nslabs` / `default_nareas`：默认槽位数和区域数\n\n## 3. 关键实现\n\n### 内存池布局与区域划分\n- SWIOTLB 缓冲区被划分为多个 **区域（area）**，每个区域有独立的自旋锁，以减少多 CPU 并发访问时的锁竞争。\n- 每个区域包含若干 **槽位（slot）**，每个槽位大小为 `IO_TLB_SIZE`（通常为 128 字节）。\n- 区域数量 `nareas` 必须是 2 的幂，且总槽位数 `nslabs` 会向上对齐到 `IO_TLB_SEGSIZE`（段大小，通常为 128 个槽）的倍数，再进一步对齐到 2 的幂，以优化分配算法。\n\n### 启动参数解析\n- 通过 `early_param(\"swiotlb\", ...)` 注册解析函数。\n- 支持格式：`swiotlb=<size>[,<nareas>][,force|noforce]`\n  - `<size>`：缓冲区大小（单位：页或字节），自动对齐到段边界\n  - `<nareas>`：区域数量，自动向上取整为 2 的幂\n  - `force`：强制启用 SWIOTLB\n  - `noforce`：禁用 SWIOTLB\n\n### 动态分配支持（`CONFIG_SWIOTLB_DYNAMIC`）\n- 默认内存池 `io_tlb_default_mem` 包含一个工作队列项 `dyn_alloc`，用于在运行时动态扩展缓冲区。\n- 通过链表 `pools` 管理多个内存池实例。\n\n### 内存属性更新\n- 在支持内存加密（如 AMD SEV）的平台上，早期分配的内存可能需要后续标记为“解密”，以便设备可访问。`swiotlb_update_mem_attributes()` 提供此钩子。\n\n### 对齐与大小约束\n- 最小缓冲区大小为 `IO_TLB_MIN_SLABS`（1MB）\n- 槽位数量必须是 `IO_TLB_SEGSIZE` 的倍数，避免段跨越区域边界，简化连续空闲槽位追踪。\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/dma-direct.h>`：提供直接 DMA 映射操作\n  - `<linux/dma-map-ops.h>`：DMA 映射操作集接口\n  - `<linux/swiotlb.h>`：SWIOTLB 公共头文件，定义核心结构和 API\n  - `<linux/scatterlist.h>`：支持 scatterlist DMA 映射\n\n- **内存管理**：\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/highmem.h>`：高内存支持（`CONFIG_HIGHMEM`）\n  - `<linux/set_memory.h>`：内存属性设置（如加密/解密）\n\n- **平台特性**：\n  - `<linux/cc_platform.h>`：机密计算平台支持（如 SEV）\n  - `CONFIG_DMA_RESTRICTED_POOL`：支持从设备树预留内存分配 SWIOTLB\n\n- **调试与跟踪**：\n  - `<linux/debugfs.h>`：调试接口\n  - `<trace/events/swiotlb.h>`：跟踪点定义\n\n## 5. 使用场景\n\n1. **无 IOMMU 的 64 位系统**：  \n   当设备仅支持 32 位 DMA 地址，但系统内存超过 4GB 时，SWIOTLB 作为透明回退机制，自动使用低地址反弹缓冲区。\n\n2. **内存加密环境（如 AMD SEV）**：  \n   加密内存对设备不可见，SWIOTLB 提供解密的 bounce buffer 用于 DMA 传输。\n\n3. **调试与强制测试**：  \n   通过 `swiotlb=force` 强制所有 DMA 走 SWIOTLB 路径，用于测试驱动兼容性或调试 DMA 问题。\n\n4. **嵌入式或资源受限系统**：  \n   在无硬件 IOMMU 的嵌入式平台，SWIOTLB 是确保 DMA 正确性的关键组件。\n\n5. **高内存（Highmem）系统**：  \n   支持将高内存页映射到 SWIOTLB 缓冲区进行 DMA 操作。\n\n6. **动态内存压力场景**：  \n   启用 `CONFIG_SWIOTLB_DYNAMIC` 后，可在运行时扩展缓冲区以应对突发 DMA 需求。",
      "similarity": 0.5025721192359924,
      "chunks": [
        {
          "chunk_id": 6,
          "file_path": "kernel/dma/swiotlb.c",
          "start_line": 1118,
          "end_line": 1226,
          "content": [
            "static int swiotlb_pool_find_slots(struct device *dev, struct io_tlb_pool *pool,",
            "\t\tphys_addr_t orig_addr, size_t alloc_size,",
            "\t\tunsigned int alloc_align_mask)",
            "{",
            "\tint start = raw_smp_processor_id() & (pool->nareas - 1);",
            "\tint i = start, index;",
            "",
            "\tdo {",
            "\t\tindex = swiotlb_area_find_slots(dev, pool, i, orig_addr,",
            "\t\t\t\t\t\talloc_size, alloc_align_mask);",
            "\t\tif (index >= 0)",
            "\t\t\treturn index;",
            "\t\tif (++i >= pool->nareas)",
            "\t\t\ti = 0;",
            "\t} while (i != start);",
            "",
            "\treturn -1;",
            "}",
            "static int swiotlb_find_slots(struct device *dev, phys_addr_t orig_addr,",
            "\t\tsize_t alloc_size, unsigned int alloc_align_mask,",
            "\t\tstruct io_tlb_pool **retpool)",
            "{",
            "\tstruct io_tlb_mem *mem = dev->dma_io_tlb_mem;",
            "\tstruct io_tlb_pool *pool;",
            "\tunsigned long nslabs;",
            "\tunsigned long flags;",
            "\tu64 phys_limit;",
            "\tint index;",
            "",
            "\trcu_read_lock();",
            "\tlist_for_each_entry_rcu(pool, &mem->pools, node) {",
            "\t\tindex = swiotlb_pool_find_slots(dev, pool, orig_addr,",
            "\t\t\t\t\t\talloc_size, alloc_align_mask);",
            "\t\tif (index >= 0) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\tgoto found;",
            "\t\t}",
            "\t}",
            "\trcu_read_unlock();",
            "\tif (!mem->can_grow)",
            "\t\treturn -1;",
            "",
            "\tschedule_work(&mem->dyn_alloc);",
            "",
            "\tnslabs = nr_slots(alloc_size);",
            "\tphys_limit = min_not_zero(*dev->dma_mask, dev->bus_dma_limit);",
            "\tpool = swiotlb_alloc_pool(dev, nslabs, nslabs, 1, phys_limit,",
            "\t\t\t\t  GFP_NOWAIT | __GFP_NOWARN);",
            "\tif (!pool)",
            "\t\treturn -1;",
            "",
            "\tindex = swiotlb_pool_find_slots(dev, pool, orig_addr,",
            "\t\t\t\t\talloc_size, alloc_align_mask);",
            "\tif (index < 0) {",
            "\t\tswiotlb_dyn_free(&pool->rcu);",
            "\t\treturn -1;",
            "\t}",
            "",
            "\tpool->transient = true;",
            "\tspin_lock_irqsave(&dev->dma_io_tlb_lock, flags);",
            "\tlist_add_rcu(&pool->node, &dev->dma_io_tlb_pools);",
            "\tspin_unlock_irqrestore(&dev->dma_io_tlb_lock, flags);",
            "",
            "found:",
            "\tWRITE_ONCE(dev->dma_uses_io_tlb, true);",
            "",
            "\t/*",
            "\t * The general barrier orders reads and writes against a presumed store",
            "\t * of the SWIOTLB buffer address by a device driver (to a driver private",
            "\t * data structure). It serves two purposes.",
            "\t *",
            "\t * First, the store to dev->dma_uses_io_tlb must be ordered before the",
            "\t * presumed store. This guarantees that the returned buffer address",
            "\t * cannot be passed to another CPU before updating dev->dma_uses_io_tlb.",
            "\t *",
            "\t * Second, the load from mem->pools must be ordered before the same",
            "\t * presumed store. This guarantees that the returned buffer address",
            "\t * cannot be observed by another CPU before an update of the RCU list",
            "\t * that was made by swiotlb_dyn_alloc() on a third CPU (cf. multicopy",
            "\t * atomicity).",
            "\t *",
            "\t * See also the comment in is_swiotlb_buffer().",
            "\t */",
            "\tsmp_mb();",
            "",
            "\t*retpool = pool;",
            "\treturn index;",
            "}",
            "static int swiotlb_find_slots(struct device *dev, phys_addr_t orig_addr,",
            "\t\tsize_t alloc_size, unsigned int alloc_align_mask,",
            "\t\tstruct io_tlb_pool **retpool)",
            "{",
            "\t*retpool = &dev->dma_io_tlb_mem->defpool;",
            "\treturn swiotlb_pool_find_slots(dev, *retpool,",
            "\t\t\t\t       orig_addr, alloc_size, alloc_align_mask);",
            "}",
            "static unsigned long mem_used(struct io_tlb_mem *mem)",
            "{",
            "\treturn atomic_long_read(&mem->total_used);",
            "}",
            "static unsigned long mem_pool_used(struct io_tlb_pool *pool)",
            "{",
            "\tint i;",
            "\tunsigned long used = 0;",
            "",
            "\tfor (i = 0; i < pool->nareas; i++)",
            "\t\tused += pool->areas[i].used;",
            "\treturn used;",
            "}"
          ],
          "function_name": "swiotlb_pool_find_slots, swiotlb_find_slots, swiotlb_find_slots, mem_used, mem_pool_used",
          "description": "实现多池槽位查找算法，支持动态扩展内存池并在RCU读锁下遍历池列表，维护槽使用统计信息，提供槽分配失败警告和动态分配工作队列集成。",
          "similarity": 0.5396469831466675
        },
        {
          "chunk_id": 8,
          "file_path": "kernel/dma/swiotlb.c",
          "start_line": 1429,
          "end_line": 1529,
          "content": [
            "static bool swiotlb_del_transient(struct device *dev, phys_addr_t tlb_addr)",
            "{",
            "\tstruct io_tlb_pool *pool;",
            "",
            "\tpool = swiotlb_find_pool(dev, tlb_addr);",
            "\tif (!pool->transient)",
            "\t\treturn false;",
            "",
            "\tdec_used(dev->dma_io_tlb_mem, pool->nslabs);",
            "\tswiotlb_del_pool(dev, pool);",
            "\treturn true;",
            "}",
            "static inline bool swiotlb_del_transient(struct device *dev,",
            "\t\t\t\t\t phys_addr_t tlb_addr)",
            "{",
            "\treturn false;",
            "}",
            "void swiotlb_tbl_unmap_single(struct device *dev, phys_addr_t tlb_addr,",
            "\t\t\t      size_t mapping_size, enum dma_data_direction dir,",
            "\t\t\t      unsigned long attrs)",
            "{",
            "\t/*",
            "\t * First, sync the memory before unmapping the entry",
            "\t */",
            "\tif (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&",
            "\t    (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL))",
            "\t\tswiotlb_bounce(dev, tlb_addr, mapping_size, DMA_FROM_DEVICE);",
            "",
            "\tif (swiotlb_del_transient(dev, tlb_addr))",
            "\t\treturn;",
            "\tswiotlb_release_slots(dev, tlb_addr);",
            "}",
            "void swiotlb_sync_single_for_device(struct device *dev, phys_addr_t tlb_addr,",
            "\t\tsize_t size, enum dma_data_direction dir)",
            "{",
            "\tif (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)",
            "\t\tswiotlb_bounce(dev, tlb_addr, size, DMA_TO_DEVICE);",
            "\telse",
            "\t\tBUG_ON(dir != DMA_FROM_DEVICE);",
            "}",
            "void swiotlb_sync_single_for_cpu(struct device *dev, phys_addr_t tlb_addr,",
            "\t\tsize_t size, enum dma_data_direction dir)",
            "{",
            "\tif (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL)",
            "\t\tswiotlb_bounce(dev, tlb_addr, size, DMA_FROM_DEVICE);",
            "\telse",
            "\t\tBUG_ON(dir != DMA_TO_DEVICE);",
            "}",
            "dma_addr_t swiotlb_map(struct device *dev, phys_addr_t paddr, size_t size,",
            "\t\tenum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tphys_addr_t swiotlb_addr;",
            "\tdma_addr_t dma_addr;",
            "",
            "\ttrace_swiotlb_bounced(dev, phys_to_dma(dev, paddr), size);",
            "",
            "\tswiotlb_addr = swiotlb_tbl_map_single(dev, paddr, size, size, 0, dir,",
            "\t\t\tattrs);",
            "\tif (swiotlb_addr == (phys_addr_t)DMA_MAPPING_ERROR)",
            "\t\treturn DMA_MAPPING_ERROR;",
            "",
            "\t/* Ensure that the address returned is DMA'ble */",
            "\tdma_addr = phys_to_dma_unencrypted(dev, swiotlb_addr);",
            "\tif (unlikely(!dma_capable(dev, dma_addr, size, true))) {",
            "\t\tswiotlb_tbl_unmap_single(dev, swiotlb_addr, size, dir,",
            "\t\t\tattrs | DMA_ATTR_SKIP_CPU_SYNC);",
            "\t\tdev_WARN_ONCE(dev, 1,",
            "\t\t\t\"swiotlb addr %pad+%zu overflow (mask %llx, bus limit %llx).\\n\",",
            "\t\t\t&dma_addr, size, *dev->dma_mask, dev->bus_dma_limit);",
            "\t\treturn DMA_MAPPING_ERROR;",
            "\t}",
            "",
            "\tif (!dev_is_dma_coherent(dev) && !(attrs & DMA_ATTR_SKIP_CPU_SYNC))",
            "\t\tarch_sync_dma_for_device(swiotlb_addr, size, dir);",
            "\treturn dma_addr;",
            "}",
            "size_t swiotlb_max_mapping_size(struct device *dev)",
            "{",
            "\tint min_align_mask = dma_get_min_align_mask(dev);",
            "\tint min_align = 0;",
            "",
            "\t/*",
            "\t * swiotlb_find_slots() skips slots according to",
            "\t * min align mask. This affects max mapping size.",
            "\t * Take it into acount here.",
            "\t */",
            "\tif (min_align_mask)",
            "\t\tmin_align = roundup(min_align_mask, IO_TLB_SIZE);",
            "",
            "\treturn ((size_t)IO_TLB_SIZE) * IO_TLB_SEGSIZE - min_align;",
            "}",
            "bool is_swiotlb_allocated(void)",
            "{",
            "\treturn io_tlb_default_mem.nslabs;",
            "}",
            "bool is_swiotlb_active(struct device *dev)",
            "{",
            "\tstruct io_tlb_mem *mem = dev->dma_io_tlb_mem;",
            "",
            "\treturn mem && mem->nslabs;",
            "}"
          ],
          "function_name": "swiotlb_del_transient, swiotlb_del_transient, swiotlb_tbl_unmap_single, swiotlb_sync_single_for_device, swiotlb_sync_single_for_cpu, swiotlb_map, swiotlb_max_mapping_size, is_swiotlb_allocated, is_swiotlb_active",
          "description": "实现临时池清理逻辑和DMA同步接口，提供设备方向同步及地址有效性验证，计算最大映射尺寸并暴露SWIOTLB运行状态检测函数。",
          "similarity": 0.5193281769752502
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/dma/swiotlb.c",
          "start_line": 269,
          "end_line": 375,
          "content": [
            "static void swiotlb_init_io_tlb_pool(struct io_tlb_pool *mem, phys_addr_t start,",
            "\t\tunsigned long nslabs, bool late_alloc, unsigned int nareas)",
            "{",
            "\tvoid *vaddr = phys_to_virt(start);",
            "\tunsigned long bytes = nslabs << IO_TLB_SHIFT, i;",
            "",
            "\tmem->nslabs = nslabs;",
            "\tmem->start = start;",
            "\tmem->end = mem->start + bytes;",
            "\tmem->late_alloc = late_alloc;",
            "\tmem->nareas = nareas;",
            "\tmem->area_nslabs = nslabs / mem->nareas;",
            "",
            "\tfor (i = 0; i < mem->nareas; i++) {",
            "\t\tspin_lock_init(&mem->areas[i].lock);",
            "\t\tmem->areas[i].index = 0;",
            "\t\tmem->areas[i].used = 0;",
            "\t}",
            "",
            "\tfor (i = 0; i < mem->nslabs; i++) {",
            "\t\tmem->slots[i].list = min(IO_TLB_SEGSIZE - io_tlb_offset(i),",
            "\t\t\t\t\t mem->nslabs - i);",
            "\t\tmem->slots[i].orig_addr = INVALID_PHYS_ADDR;",
            "\t\tmem->slots[i].alloc_size = 0;",
            "\t\tmem->slots[i].pad_slots = 0;",
            "\t}",
            "",
            "\tmemset(vaddr, 0, bytes);",
            "\tmem->vaddr = vaddr;",
            "\treturn;",
            "}",
            "static void add_mem_pool(struct io_tlb_mem *mem, struct io_tlb_pool *pool)",
            "{",
            "#ifdef CONFIG_SWIOTLB_DYNAMIC",
            "\tspin_lock(&mem->lock);",
            "\tlist_add_rcu(&pool->node, &mem->pools);",
            "\tmem->nslabs += pool->nslabs;",
            "\tspin_unlock(&mem->lock);",
            "#else",
            "\tmem->nslabs = pool->nslabs;",
            "#endif",
            "}",
            "void __init swiotlb_init_remap(bool addressing_limit, unsigned int flags,",
            "\t\tint (*remap)(void *tlb, unsigned long nslabs))",
            "{",
            "\tstruct io_tlb_pool *mem = &io_tlb_default_mem.defpool;",
            "\tunsigned long nslabs;",
            "\tunsigned int nareas;",
            "\tsize_t alloc_size;",
            "\tvoid *tlb;",
            "",
            "\tif (!addressing_limit && !swiotlb_force_bounce)",
            "\t\treturn;",
            "\tif (swiotlb_force_disable)",
            "\t\treturn;",
            "",
            "\tio_tlb_default_mem.force_bounce =",
            "\t\tswiotlb_force_bounce || (flags & SWIOTLB_FORCE);",
            "",
            "#ifdef CONFIG_SWIOTLB_DYNAMIC",
            "\tif (!remap)",
            "\t\tio_tlb_default_mem.can_grow = true;",
            "\tif (flags & SWIOTLB_ANY)",
            "\t\tio_tlb_default_mem.phys_limit = virt_to_phys(high_memory - 1);",
            "\telse",
            "\t\tio_tlb_default_mem.phys_limit = ARCH_LOW_ADDRESS_LIMIT;",
            "#endif",
            "",
            "\tif (!default_nareas)",
            "\t\tswiotlb_adjust_nareas(num_possible_cpus());",
            "",
            "\tnslabs = default_nslabs;",
            "\tnareas = limit_nareas(default_nareas, nslabs);",
            "\twhile ((tlb = swiotlb_memblock_alloc(nslabs, flags, remap)) == NULL) {",
            "\t\tif (nslabs <= IO_TLB_MIN_SLABS)",
            "\t\t\treturn;",
            "\t\tnslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);",
            "\t\tnareas = limit_nareas(nareas, nslabs);",
            "\t}",
            "",
            "\tif (default_nslabs != nslabs) {",
            "\t\tpr_info(\"SWIOTLB bounce buffer size adjusted %lu -> %lu slabs\",",
            "\t\t\tdefault_nslabs, nslabs);",
            "\t\tdefault_nslabs = nslabs;",
            "\t}",
            "",
            "\talloc_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), nslabs));",
            "\tmem->slots = memblock_alloc(alloc_size, PAGE_SIZE);",
            "\tif (!mem->slots) {",
            "\t\tpr_warn(\"%s: Failed to allocate %zu bytes align=0x%lx\\n\",",
            "\t\t\t__func__, alloc_size, PAGE_SIZE);",
            "\t\treturn;",
            "\t}",
            "",
            "\tmem->areas = memblock_alloc(array_size(sizeof(struct io_tlb_area),",
            "\t\tnareas), SMP_CACHE_BYTES);",
            "\tif (!mem->areas) {",
            "\t\tpr_warn(\"%s: Failed to allocate mem->areas.\\n\", __func__);",
            "\t\treturn;",
            "\t}",
            "",
            "\tswiotlb_init_io_tlb_pool(mem, __pa(tlb), nslabs, false, nareas);",
            "\tadd_mem_pool(&io_tlb_default_mem, mem);",
            "",
            "\tif (flags & SWIOTLB_VERBOSE)",
            "\t\tswiotlb_print_info();",
            "}"
          ],
          "function_name": "swiotlb_init_io_tlb_pool, add_mem_pool, swiotlb_init_remap",
          "description": "初始化IO TLB内存池结构，分配虚拟地址空间，设置槽位列表和区域锁，通过动态分配或预分配方式建立物理-虚拟地址映射关系。",
          "similarity": 0.49640804529190063
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/dma/swiotlb.c",
          "start_line": 657,
          "end_line": 785,
          "content": [
            "static void swiotlb_free_tlb(void *vaddr, size_t bytes)",
            "{",
            "\tif (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&",
            "\t    dma_free_from_pool(NULL, vaddr, bytes))",
            "\t\treturn;",
            "",
            "\t/* Intentional leak if pages cannot be encrypted again. */",
            "\tif (!set_memory_encrypted((unsigned long)vaddr, PFN_UP(bytes)))",
            "\t\t__free_pages(virt_to_page(vaddr), get_order(bytes));",
            "}",
            "static void swiotlb_dyn_alloc(struct work_struct *work)",
            "{",
            "\tstruct io_tlb_mem *mem =",
            "\t\tcontainer_of(work, struct io_tlb_mem, dyn_alloc);",
            "\tstruct io_tlb_pool *pool;",
            "",
            "\tpool = swiotlb_alloc_pool(NULL, IO_TLB_MIN_SLABS, default_nslabs,",
            "\t\t\t\t  default_nareas, mem->phys_limit, GFP_KERNEL);",
            "\tif (!pool) {",
            "\t\tpr_warn_ratelimited(\"Failed to allocate new pool\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tadd_mem_pool(mem, pool);",
            "}",
            "static void swiotlb_dyn_free(struct rcu_head *rcu)",
            "{",
            "\tstruct io_tlb_pool *pool = container_of(rcu, struct io_tlb_pool, rcu);",
            "\tsize_t slots_size = array_size(sizeof(*pool->slots), pool->nslabs);",
            "\tsize_t tlb_size = pool->end - pool->start;",
            "",
            "\tfree_pages((unsigned long)pool->slots, get_order(slots_size));",
            "\tswiotlb_free_tlb(pool->vaddr, tlb_size);",
            "\tkfree(pool);",
            "}",
            "static void swiotlb_del_pool(struct device *dev, struct io_tlb_pool *pool)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&dev->dma_io_tlb_lock, flags);",
            "\tlist_del_rcu(&pool->node);",
            "\tspin_unlock_irqrestore(&dev->dma_io_tlb_lock, flags);",
            "",
            "\tcall_rcu(&pool->rcu, swiotlb_dyn_free);",
            "}",
            "void swiotlb_dev_init(struct device *dev)",
            "{",
            "\tdev->dma_io_tlb_mem = &io_tlb_default_mem;",
            "#ifdef CONFIG_SWIOTLB_DYNAMIC",
            "\tINIT_LIST_HEAD(&dev->dma_io_tlb_pools);",
            "\tspin_lock_init(&dev->dma_io_tlb_lock);",
            "\tdev->dma_uses_io_tlb = false;",
            "#endif",
            "}",
            "static unsigned int swiotlb_align_offset(struct device *dev,",
            "\t\t\t\t\t unsigned int align_mask, u64 addr)",
            "{",
            "\treturn addr & dma_get_min_align_mask(dev) &",
            "\t\t(align_mask | (IO_TLB_SIZE - 1));",
            "}",
            "static void swiotlb_bounce(struct device *dev, phys_addr_t tlb_addr, size_t size,",
            "\t\t\t   enum dma_data_direction dir)",
            "{",
            "\tstruct io_tlb_pool *mem = swiotlb_find_pool(dev, tlb_addr);",
            "\tint index = (tlb_addr - mem->start) >> IO_TLB_SHIFT;",
            "\tphys_addr_t orig_addr = mem->slots[index].orig_addr;",
            "\tsize_t alloc_size = mem->slots[index].alloc_size;",
            "\tunsigned long pfn = PFN_DOWN(orig_addr);",
            "\tunsigned char *vaddr = mem->vaddr + tlb_addr - mem->start;",
            "\tunsigned int tlb_offset, orig_addr_offset;",
            "",
            "\tif (orig_addr == INVALID_PHYS_ADDR)",
            "\t\treturn;",
            "",
            "\ttlb_offset = tlb_addr & (IO_TLB_SIZE - 1);",
            "\torig_addr_offset = swiotlb_align_offset(dev, 0, orig_addr);",
            "\tif (tlb_offset < orig_addr_offset) {",
            "\t\tdev_WARN_ONCE(dev, 1,",
            "\t\t\t\"Access before mapping start detected. orig offset %u, requested offset %u.\\n\",",
            "\t\t\torig_addr_offset, tlb_offset);",
            "\t\treturn;",
            "\t}",
            "",
            "\ttlb_offset -= orig_addr_offset;",
            "\tif (tlb_offset > alloc_size) {",
            "\t\tdev_WARN_ONCE(dev, 1,",
            "\t\t\t\"Buffer overflow detected. Allocation size: %zu. Mapping size: %zu+%u.\\n\",",
            "\t\t\talloc_size, size, tlb_offset);",
            "\t\treturn;",
            "\t}",
            "",
            "\torig_addr += tlb_offset;",
            "\talloc_size -= tlb_offset;",
            "",
            "\tif (size > alloc_size) {",
            "\t\tdev_WARN_ONCE(dev, 1,",
            "\t\t\t\"Buffer overflow detected. Allocation size: %zu. Mapping size: %zu.\\n\",",
            "\t\t\talloc_size, size);",
            "\t\tsize = alloc_size;",
            "\t}",
            "",
            "\tif (PageHighMem(pfn_to_page(pfn))) {",
            "\t\tunsigned int offset = orig_addr & ~PAGE_MASK;",
            "\t\tstruct page *page;",
            "\t\tunsigned int sz = 0;",
            "\t\tunsigned long flags;",
            "",
            "\t\twhile (size) {",
            "\t\t\tsz = min_t(size_t, PAGE_SIZE - offset, size);",
            "",
            "\t\t\tlocal_irq_save(flags);",
            "\t\t\tpage = pfn_to_page(pfn);",
            "\t\t\tif (dir == DMA_TO_DEVICE)",
            "\t\t\t\tmemcpy_from_page(vaddr, page, offset, sz);",
            "\t\t\telse",
            "\t\t\t\tmemcpy_to_page(page, offset, vaddr, sz);",
            "\t\t\tlocal_irq_restore(flags);",
            "",
            "\t\t\tsize -= sz;",
            "\t\t\tpfn++;",
            "\t\t\tvaddr += sz;",
            "\t\t\toffset = 0;",
            "\t\t}",
            "\t} else if (dir == DMA_TO_DEVICE) {",
            "\t\tmemcpy(vaddr, phys_to_virt(orig_addr), size);",
            "\t} else {",
            "\t\tmemcpy(phys_to_virt(orig_addr), vaddr, size);",
            "\t}",
            "}"
          ],
          "function_name": "swiotlb_free_tlb, swiotlb_dyn_alloc, swiotlb_dyn_free, swiotlb_del_pool, swiotlb_dev_init, swiotlb_align_offset, swiotlb_bounce",
          "description": "实现动态内存池管理功能，包含缓冲区溢出检测、数据方向匹配的内存拷贝操作，支持高内存页面处理及RCU回调机制的安全释放。",
          "similarity": 0.4859447479248047
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/dma/swiotlb.c",
          "start_line": 1282,
          "end_line": 1411,
          "content": [
            "static unsigned long mem_used(struct io_tlb_mem *mem)",
            "{",
            "#ifdef CONFIG_SWIOTLB_DYNAMIC",
            "\tstruct io_tlb_pool *pool;",
            "\tunsigned long used = 0;",
            "",
            "\trcu_read_lock();",
            "\tlist_for_each_entry_rcu(pool, &mem->pools, node)",
            "\t\tused += mem_pool_used(pool);",
            "\trcu_read_unlock();",
            "",
            "\treturn used;",
            "#else",
            "\treturn mem_pool_used(&mem->defpool);",
            "#endif",
            "}",
            "phys_addr_t swiotlb_tbl_map_single(struct device *dev, phys_addr_t orig_addr,",
            "\t\tsize_t mapping_size, size_t alloc_size,",
            "\t\tunsigned int alloc_align_mask, enum dma_data_direction dir,",
            "\t\tunsigned long attrs)",
            "{",
            "\tstruct io_tlb_mem *mem = dev->dma_io_tlb_mem;",
            "\tunsigned int offset;",
            "\tstruct io_tlb_pool *pool;",
            "\tunsigned int i;",
            "\tint index;",
            "\tphys_addr_t tlb_addr;",
            "\tunsigned short pad_slots;",
            "",
            "\tif (!mem || !mem->nslabs) {",
            "\t\tdev_warn_ratelimited(dev,",
            "\t\t\t\"Can not allocate SWIOTLB buffer earlier and can't now provide you with the DMA bounce buffer\");",
            "\t\treturn (phys_addr_t)DMA_MAPPING_ERROR;",
            "\t}",
            "",
            "\tif (cc_platform_has(CC_ATTR_MEM_ENCRYPT))",
            "\t\tpr_warn_once(\"Memory encryption is active and system is using DMA bounce buffers\\n\");",
            "",
            "\tif (mapping_size > alloc_size) {",
            "\t\tdev_warn_once(dev, \"Invalid sizes (mapping: %zd bytes, alloc: %zd bytes)\",",
            "\t\t\t      mapping_size, alloc_size);",
            "\t\treturn (phys_addr_t)DMA_MAPPING_ERROR;",
            "\t}",
            "",
            "\toffset = swiotlb_align_offset(dev, alloc_align_mask, orig_addr);",
            "\tindex = swiotlb_find_slots(dev, orig_addr,",
            "\t\t\t\t   alloc_size + offset, alloc_align_mask, &pool);",
            "\tif (index == -1) {",
            "\t\tif (!(attrs & DMA_ATTR_NO_WARN))",
            "\t\t\tdev_warn_ratelimited(dev,",
            "\t\"swiotlb buffer is full (sz: %zd bytes), total %lu (slots), used %lu (slots)\\n\",",
            "\t\t\t\t alloc_size, mem->nslabs, mem_used(mem));",
            "\t\treturn (phys_addr_t)DMA_MAPPING_ERROR;",
            "\t}",
            "",
            "\t/*",
            "\t * Save away the mapping from the original address to the DMA address.",
            "\t * This is needed when we sync the memory.  Then we sync the buffer if",
            "\t * needed.",
            "\t */",
            "\tpad_slots = offset >> IO_TLB_SHIFT;",
            "\toffset &= (IO_TLB_SIZE - 1);",
            "\tindex += pad_slots;",
            "\tpool->slots[index].pad_slots = pad_slots;",
            "\tfor (i = 0; i < nr_slots(alloc_size + offset); i++)",
            "\t\tpool->slots[index + i].orig_addr = slot_addr(orig_addr, i);",
            "\ttlb_addr = slot_addr(pool->start, index) + offset;",
            "\t/*",
            "\t * When dir == DMA_FROM_DEVICE we could omit the copy from the orig",
            "\t * to the tlb buffer, if we knew for sure the device will",
            "\t * overwrite the entire current content. But we don't. Thus",
            "\t * unconditional bounce may prevent leaking swiotlb content (i.e.",
            "\t * kernel memory) to user-space.",
            "\t */",
            "\tswiotlb_bounce(dev, tlb_addr, mapping_size, DMA_TO_DEVICE);",
            "\treturn tlb_addr;",
            "}",
            "static void swiotlb_release_slots(struct device *dev, phys_addr_t tlb_addr)",
            "{",
            "\tstruct io_tlb_pool *mem = swiotlb_find_pool(dev, tlb_addr);",
            "\tunsigned long flags;",
            "\tunsigned int offset = swiotlb_align_offset(dev, 0, tlb_addr);",
            "\tint index, nslots, aindex;",
            "\tstruct io_tlb_area *area;",
            "\tint count, i;",
            "",
            "\tindex = (tlb_addr - offset - mem->start) >> IO_TLB_SHIFT;",
            "\tindex -= mem->slots[index].pad_slots;",
            "\tnslots = nr_slots(mem->slots[index].alloc_size + offset);",
            "\taindex = index / mem->area_nslabs;",
            "\tarea = &mem->areas[aindex];",
            "",
            "\t/*",
            "\t * Return the buffer to the free list by setting the corresponding",
            "\t * entries to indicate the number of contiguous entries available.",
            "\t * While returning the entries to the free list, we merge the entries",
            "\t * with slots below and above the pool being returned.",
            "\t */",
            "\tBUG_ON(aindex >= mem->nareas);",
            "",
            "\tspin_lock_irqsave(&area->lock, flags);",
            "\tif (index + nslots < ALIGN(index + 1, IO_TLB_SEGSIZE))",
            "\t\tcount = mem->slots[index + nslots].list;",
            "\telse",
            "\t\tcount = 0;",
            "",
            "\t/*",
            "\t * Step 1: return the slots to the free list, merging the slots with",
            "\t * superceeding slots",
            "\t */",
            "\tfor (i = index + nslots - 1; i >= index; i--) {",
            "\t\tmem->slots[i].list = ++count;",
            "\t\tmem->slots[i].orig_addr = INVALID_PHYS_ADDR;",
            "\t\tmem->slots[i].alloc_size = 0;",
            "\t\tmem->slots[i].pad_slots = 0;",
            "\t}",
            "",
            "\t/*",
            "\t * Step 2: merge the returned slots with the preceding slots, if",
            "\t * available (non zero)",
            "\t */",
            "\tfor (i = index - 1;",
            "\t     io_tlb_offset(i) != IO_TLB_SEGSIZE - 1 && mem->slots[i].list;",
            "\t     i--)",
            "\t\tmem->slots[i].list = ++count;",
            "\tarea->used -= nslots;",
            "\tspin_unlock_irqrestore(&area->lock, flags);",
            "",
            "\tdec_used(dev->dma_io_tlb_mem, nslots);",
            "}"
          ],
          "function_name": "mem_used, swiotlb_tbl_map_single, swiotlb_release_slots",
          "description": "完成DMA缓冲区映射与槽位绑定操作，通过偏移量调整索引并记录原始地址，释放槽位时执行内存合并操作，维护槽状态表并更新使用计数器。",
          "similarity": 0.47726741433143616
        }
      ]
    },
    {
      "source_file": "kernel/irq/matrix.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:03:08\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\matrix.c`\n\n---\n\n# `irq/matrix.c` 技术文档\n\n## 1. 文件概述\n\n`irq/matrix.c` 实现了一个通用的中断位图（IRQ matrix）管理机制，用于在多 CPU 系统中高效地分配和管理中断向量（或中断位）。该机制支持两类中断分配：\n\n- **普通分配（allocated）**：由设备驱动等动态申请的中断。\n- **托管分配（managed）**：由内核子系统（如 MSI/MSI-X）预先保留、按需激活的中断。\n\n该文件通过 per-CPU 的位图结构，结合全局状态跟踪，实现了跨 CPU 的中断资源分配、预留、释放和在线/离线管理，特别适用于中断向量数量有限（如 x86 的 256 个向量）的架构。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct cpumap`**：每个 CPU 的本地中断位图状态\n  - `available`：当前 CPU 可用的中断数量\n  - `allocated`：已分配的普通中断数量\n  - `managed` / `managed_allocated`：预留和已激活的托管中断数量\n  - `alloc_map[]`：记录已分配的普通中断位\n  - `managed_map[]`：记录预留的托管中断位\n  - `initialized` / `online`：CPU 初始化和在线状态\n\n- **`struct irq_matrix`**：全局中断矩阵控制结构\n  - `matrix_bits`：总位图大小（≤ `IRQ_MATRIX_BITS`）\n  - `alloc_start` / `alloc_end`：可分配范围\n  - `global_available`：全局可用中断总数\n  - `system_map[]`：系统保留位（如 APIC 自身使用的向量）\n  - `maps`：指向 per-CPU `cpumap` 的指针\n  - `scratch_map[]`：临时位图，用于分配时的合并计算\n\n### 主要函数\n\n| 函数 | 功能 |\n|------|------|\n| `irq_alloc_matrix()` | 分配并初始化一个 `irq_matrix` 结构 |\n| `irq_matrix_online()` / `irq_matrix_offline()` | 将本地 CPU 的中断矩阵置为在线/离线状态 |\n| `irq_matrix_assign_system()` | 在矩阵中保留系统级中断位（如 APIC 向量） |\n| `irq_matrix_reserve_managed()` | 在指定 CPU 掩码上为托管中断预留位 |\n| `irq_matrix_remove_managed()` | 移除托管中断的预留位 |\n| `irq_matrix_alloc_managed()` | 从预留的托管中断中分配一个实际使用的中断 |\n| `matrix_alloc_area()` | 内部辅助函数：在合并位图中查找连续空闲区域 |\n| `matrix_find_best_cpu()` / `matrix_find_best_cpu_managed()` | 选择最优 CPU（基于可用数或托管分配数最少） |\n\n## 3. 关键实现\n\n### 位图合并分配策略\n- 在分配中断时，`matrix_alloc_area()` 会临时合并三个位图：\n  1. 当前 CPU 的 `managed_map`（托管预留）\n  2. 全局 `system_map`（系统保留）\n  3. 当前 CPU 的 `alloc_map`（已分配）\n- 使用 `bitmap_find_next_zero_area()` 在合并后的位图中查找连续空闲区域，确保不会重复分配。\n\n### 托管中断（Managed IRQ）机制\n- **两阶段分配**：\n  1. **预留（reserve）**：调用 `irq_matrix_reserve_managed()` 在多个 CPU 上各预留一个位（不一定对齐）。\n  2. **激活（alloc）**：调用 `irq_matrix_alloc_managed()` 从预留位中选择一个未使用的位进行实际分配。\n- **动态 CPU 选择**：`matrix_find_best_cpu_managed()` 优先选择 `managed_allocated` 最少的 CPU，实现负载均衡。\n\n### 系统中断保留\n- `irq_matrix_assign_system()` 用于保留如 x86 的 `IRQ0_VECTOR`（时钟中断）等关键系统向量。\n- 通过 `BUG_ON()` 强制保证：系统中断只能在单 CPU 初始化阶段分配，防止运行时冲突。\n\n### 在线/离线管理\n- CPU 上线时，将其 `available` 计数加入 `global_available`。\n- CPU 离线时，从全局计数中减去，但保留其位图数据（支持重新上线）。\n\n### 跟踪与调试\n- 集成 `trace/events/irq_matrix.h`，提供分配、预留、系统保留等关键操作的 tracepoint，便于调试中断分配问题。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/bitmap.h>`：位图操作（`bitmap_set`, `bitmap_find_next_zero_area` 等）\n  - `<linux/percpu.h>`：Per-CPU 变量支持\n  - `<linux/cpu.h>`：CPU 在线/离线状态\n  - `<linux/irq.h>`：中断子系统基础定义\n  - `<trace/events/irq_matrix.h>`：自定义 tracepoint\n\n- **内核子系统**：\n  - **中断子系统**：作为底层分配器，被 `irqdomain`、MSI/MSI-X 驱动等使用。\n  - **x86 APIC 驱动**：典型使用者，用于管理 256 个中断向量的分配（如 `kernel/irq/vector.c`）。\n\n## 5. 使用场景\n\n- **x86 中断向量管理**：在 `CONFIG_X86_IO_APIC` 或 `CONFIG_X86_LOCAL_APIC` 下，用于分配 IRQ 向量（0-255），区分系统向量、普通设备中断和 MSI 中断。\n- **MSI/MSI-X 中断分配**：PCIe 设备的 MSI 中断通过托管机制预留和分配，确保每个设备在多个 CPU 上有可用向量。\n- **CPU 热插拔**：支持 CPU 动态上线/下线时的中断资源重新平衡。\n- **中断负载均衡**：通过 `matrix_find_best_cpu*` 函数，在多 CPU 间均匀分配中断，避免单 CPU 向量耗尽。",
      "similarity": 0.4958556890487671,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 78,
          "end_line": 205,
          "content": [
            "void irq_matrix_online(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tBUG_ON(cm->online);",
            "",
            "\tif (!cm->initialized) {",
            "\t\tcm->available = m->alloc_size;",
            "\t\tcm->available -= cm->managed + m->systembits_inalloc;",
            "\t\tcm->initialized = true;",
            "\t}",
            "\tm->global_available += cm->available;",
            "\tcm->online = true;",
            "\tm->online_maps++;",
            "\ttrace_irq_matrix_online(m);",
            "}",
            "void irq_matrix_offline(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\t/* Update the global available size */",
            "\tm->global_available -= cm->available;",
            "\tcm->online = false;",
            "\tm->online_maps--;",
            "\ttrace_irq_matrix_offline(m);",
            "}",
            "static unsigned int matrix_alloc_area(struct irq_matrix *m, struct cpumap *cm,",
            "\t\t\t\t      unsigned int num, bool managed)",
            "{",
            "\tunsigned int area, start = m->alloc_start;",
            "\tunsigned int end = m->alloc_end;",
            "",
            "\tbitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);",
            "\tbitmap_or(m->scratch_map, m->scratch_map, cm->alloc_map, end);",
            "\tarea = bitmap_find_next_zero_area(m->scratch_map, end, start, num, 0);",
            "\tif (area >= end)",
            "\t\treturn area;",
            "\tif (managed)",
            "\t\tbitmap_set(cm->managed_map, area, num);",
            "\telse",
            "\t\tbitmap_set(cm->alloc_map, area, num);",
            "\treturn area;",
            "}",
            "static unsigned int matrix_find_best_cpu(struct irq_matrix *m,",
            "\t\t\t\t\tconst struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, best_cpu, maxavl = 0;",
            "\tstruct cpumap *cm;",
            "",
            "\tbest_cpu = UINT_MAX;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tcm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tif (!cm->online || cm->available <= maxavl)",
            "\t\t\tcontinue;",
            "",
            "\t\tbest_cpu = cpu;",
            "\t\tmaxavl = cm->available;",
            "\t}",
            "\treturn best_cpu;",
            "}",
            "static unsigned int matrix_find_best_cpu_managed(struct irq_matrix *m,",
            "\t\t\t\t\t\tconst struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, best_cpu, allocated = UINT_MAX;",
            "\tstruct cpumap *cm;",
            "",
            "\tbest_cpu = UINT_MAX;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tcm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tif (!cm->online || cm->managed_allocated > allocated)",
            "\t\t\tcontinue;",
            "",
            "\t\tbest_cpu = cpu;",
            "\t\tallocated = cm->managed_allocated;",
            "\t}",
            "\treturn best_cpu;",
            "}",
            "void irq_matrix_assign_system(struct irq_matrix *m, unsigned int bit,",
            "\t\t\t      bool replace)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tBUG_ON(bit > m->matrix_bits);",
            "\tBUG_ON(m->online_maps > 1 || (m->online_maps && !replace));",
            "",
            "\tset_bit(bit, m->system_map);",
            "\tif (replace) {",
            "\t\tBUG_ON(!test_and_clear_bit(bit, cm->alloc_map));",
            "\t\tcm->allocated--;",
            "\t\tm->total_allocated--;",
            "\t}",
            "\tif (bit >= m->alloc_start && bit < m->alloc_end)",
            "\t\tm->systembits_inalloc++;",
            "",
            "\ttrace_irq_matrix_assign_system(bit, m);",
            "}",
            "int irq_matrix_reserve_managed(struct irq_matrix *m, const struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, failed_cpu;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "\t\tunsigned int bit;",
            "",
            "\t\tbit = matrix_alloc_area(m, cm, 1, true);",
            "\t\tif (bit >= m->alloc_end)",
            "\t\t\tgoto cleanup;",
            "\t\tcm->managed++;",
            "\t\tif (cm->online) {",
            "\t\t\tcm->available--;",
            "\t\t\tm->global_available--;",
            "\t\t}",
            "\t\ttrace_irq_matrix_reserve_managed(bit, cpu, m, cm);",
            "\t}",
            "\treturn 0;",
            "cleanup:",
            "\tfailed_cpu = cpu;",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tif (cpu == failed_cpu)",
            "\t\t\tbreak;",
            "\t\tirq_matrix_remove_managed(m, cpumask_of(cpu));",
            "\t}",
            "\treturn -ENOSPC;",
            "}"
          ],
          "function_name": "irq_matrix_online, irq_matrix_offline, matrix_alloc_area, matrix_find_best_cpu, matrix_find_best_cpu_managed, irq_matrix_assign_system, irq_matrix_reserve_managed",
          "description": "实现CPU矩阵的上线/下线操作，通过bitmap操作实现中断位的分配策略，包含寻找最佳CPU的逻辑，支持系统位管理和保留区域的分配与追踪",
          "similarity": 0.5146487951278687
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 418,
          "end_line": 483,
          "content": [
            "void irq_matrix_free(struct irq_matrix *m, unsigned int cpu,",
            "\t\t     unsigned int bit, bool managed)",
            "{",
            "\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\tif (WARN_ON_ONCE(bit < m->alloc_start || bit >= m->alloc_end))",
            "\t\treturn;",
            "",
            "\tif (WARN_ON_ONCE(!test_and_clear_bit(bit, cm->alloc_map)))",
            "\t\treturn;",
            "",
            "\tcm->allocated--;",
            "\tif(managed)",
            "\t\tcm->managed_allocated--;",
            "",
            "\tif (cm->online)",
            "\t\tm->total_allocated--;",
            "",
            "\tif (!managed) {",
            "\t\tcm->available++;",
            "\t\tif (cm->online)",
            "\t\t\tm->global_available++;",
            "\t}",
            "\ttrace_irq_matrix_free(bit, cpu, m, cm);",
            "}",
            "unsigned int irq_matrix_available(struct irq_matrix *m, bool cpudown)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tif (!cpudown)",
            "\t\treturn m->global_available;",
            "\treturn m->global_available - cm->available;",
            "}",
            "unsigned int irq_matrix_reserved(struct irq_matrix *m)",
            "{",
            "\treturn m->global_reserved;",
            "}",
            "unsigned int irq_matrix_allocated(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\treturn cm->allocated - cm->managed_allocated;",
            "}",
            "void irq_matrix_debug_show(struct seq_file *sf, struct irq_matrix *m, int ind)",
            "{",
            "\tunsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);",
            "\tint cpu;",
            "",
            "\tseq_printf(sf, \"Online bitmaps:   %6u\\n\", m->online_maps);",
            "\tseq_printf(sf, \"Global available: %6u\\n\", m->global_available);",
            "\tseq_printf(sf, \"Global reserved:  %6u\\n\", m->global_reserved);",
            "\tseq_printf(sf, \"Total allocated:  %6u\\n\", m->total_allocated);",
            "\tseq_printf(sf, \"System: %u: %*pbl\\n\", nsys, m->matrix_bits,",
            "\t\t   m->system_map);",
            "\tseq_printf(sf, \"%*s| CPU | avl | man | mac | act | vectors\\n\", ind, \" \");",
            "\tcpus_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tseq_printf(sf, \"%*s %4d  %4u  %4u  %4u %4u  %*pbl\\n\", ind, \" \",",
            "\t\t\t   cpu, cm->available, cm->managed,",
            "\t\t\t   cm->managed_allocated, cm->allocated,",
            "\t\t\t   m->matrix_bits, cm->alloc_map);",
            "\t}",
            "\tcpus_read_unlock();",
            "}"
          ],
          "function_name": "irq_matrix_free, irq_matrix_available, irq_matrix_reserved, irq_matrix_allocated, irq_matrix_debug_show",
          "description": "提供中断资源的释放接口，实现全局和CPU级的资源使用统计查询，包含调试信息展示功能，通过位图操作维护系统中断位的使用状态",
          "similarity": 0.49951034784317017
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 251,
          "end_line": 365,
          "content": [
            "void irq_matrix_remove_managed(struct irq_matrix *m, const struct cpumask *msk)",
            "{",
            "\tunsigned int cpu;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "\t\tunsigned int bit, end = m->alloc_end;",
            "",
            "\t\tif (WARN_ON_ONCE(!cm->managed))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Get managed bit which are not allocated */",
            "\t\tbitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);",
            "",
            "\t\tbit = find_first_bit(m->scratch_map, end);",
            "\t\tif (WARN_ON_ONCE(bit >= end))",
            "\t\t\tcontinue;",
            "",
            "\t\tclear_bit(bit, cm->managed_map);",
            "",
            "\t\tcm->managed--;",
            "\t\tif (cm->online) {",
            "\t\t\tcm->available++;",
            "\t\t\tm->global_available++;",
            "\t\t}",
            "\t\ttrace_irq_matrix_remove_managed(bit, cpu, m, cm);",
            "\t}",
            "}",
            "int irq_matrix_alloc_managed(struct irq_matrix *m, const struct cpumask *msk,",
            "\t\t\t     unsigned int *mapped_cpu)",
            "{",
            "\tunsigned int bit, cpu, end;",
            "\tstruct cpumap *cm;",
            "",
            "\tif (cpumask_empty(msk))",
            "\t\treturn -EINVAL;",
            "",
            "\tcpu = matrix_find_best_cpu_managed(m, msk);",
            "\tif (cpu == UINT_MAX)",
            "\t\treturn -ENOSPC;",
            "",
            "\tcm = per_cpu_ptr(m->maps, cpu);",
            "\tend = m->alloc_end;",
            "\t/* Get managed bit which are not allocated */",
            "\tbitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);",
            "\tbit = find_first_bit(m->scratch_map, end);",
            "\tif (bit >= end)",
            "\t\treturn -ENOSPC;",
            "\tset_bit(bit, cm->alloc_map);",
            "\tcm->allocated++;",
            "\tcm->managed_allocated++;",
            "\tm->total_allocated++;",
            "\t*mapped_cpu = cpu;",
            "\ttrace_irq_matrix_alloc_managed(bit, cpu, m, cm);",
            "\treturn bit;",
            "}",
            "void irq_matrix_assign(struct irq_matrix *m, unsigned int bit)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tif (WARN_ON_ONCE(bit < m->alloc_start || bit >= m->alloc_end))",
            "\t\treturn;",
            "\tif (WARN_ON_ONCE(test_and_set_bit(bit, cm->alloc_map)))",
            "\t\treturn;",
            "\tcm->allocated++;",
            "\tm->total_allocated++;",
            "\tcm->available--;",
            "\tm->global_available--;",
            "\ttrace_irq_matrix_assign(bit, smp_processor_id(), m, cm);",
            "}",
            "void irq_matrix_reserve(struct irq_matrix *m)",
            "{",
            "\tif (m->global_reserved == m->global_available)",
            "\t\tpr_warn(\"Interrupt reservation exceeds available resources\\n\");",
            "",
            "\tm->global_reserved++;",
            "\ttrace_irq_matrix_reserve(m);",
            "}",
            "void irq_matrix_remove_reserved(struct irq_matrix *m)",
            "{",
            "\tm->global_reserved--;",
            "\ttrace_irq_matrix_remove_reserved(m);",
            "}",
            "int irq_matrix_alloc(struct irq_matrix *m, const struct cpumask *msk,",
            "\t\t     bool reserved, unsigned int *mapped_cpu)",
            "{",
            "\tunsigned int cpu, bit;",
            "\tstruct cpumap *cm;",
            "",
            "\t/*",
            "\t * Not required in theory, but matrix_find_best_cpu() uses",
            "\t * for_each_cpu() which ignores the cpumask on UP .",
            "\t */",
            "\tif (cpumask_empty(msk))",
            "\t\treturn -EINVAL;",
            "",
            "\tcpu = matrix_find_best_cpu(m, msk);",
            "\tif (cpu == UINT_MAX)",
            "\t\treturn -ENOSPC;",
            "",
            "\tcm = per_cpu_ptr(m->maps, cpu);",
            "\tbit = matrix_alloc_area(m, cm, 1, false);",
            "\tif (bit >= m->alloc_end)",
            "\t\treturn -ENOSPC;",
            "\tcm->allocated++;",
            "\tcm->available--;",
            "\tm->total_allocated++;",
            "\tm->global_available--;",
            "\tif (reserved)",
            "\t\tm->global_reserved--;",
            "\t*mapped_cpu = cpu;",
            "\ttrace_irq_matrix_alloc(bit, cpu, m, cm);",
            "\treturn bit;",
            "",
            "}"
          ],
          "function_name": "irq_matrix_remove_managed, irq_matrix_alloc_managed, irq_matrix_assign, irq_matrix_reserve, irq_matrix_remove_reserved, irq_matrix_alloc",
          "description": "实现中断位的分配/回收机制，包含保留中断位的管理、跨CPU的中断分配逻辑，以及根据预留状态进行资源分配的控制流程",
          "similarity": 0.494850754737854
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 1,
          "end_line": 77,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "// Copyright (C) 2017 Thomas Gleixner <tglx@linutronix.de>",
            "",
            "#include <linux/spinlock.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/bitmap.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpu.h>",
            "#include <linux/irq.h>",
            "",
            "#define IRQ_MATRIX_SIZE\t(BITS_TO_LONGS(IRQ_MATRIX_BITS))",
            "",
            "struct cpumap {",
            "\tunsigned int\t\tavailable;",
            "\tunsigned int\t\tallocated;",
            "\tunsigned int\t\tmanaged;",
            "\tunsigned int\t\tmanaged_allocated;",
            "\tbool\t\t\tinitialized;",
            "\tbool\t\t\tonline;",
            "\tunsigned long\t\talloc_map[IRQ_MATRIX_SIZE];",
            "\tunsigned long\t\tmanaged_map[IRQ_MATRIX_SIZE];",
            "};",
            "",
            "struct irq_matrix {",
            "\tunsigned int\t\tmatrix_bits;",
            "\tunsigned int\t\talloc_start;",
            "\tunsigned int\t\talloc_end;",
            "\tunsigned int\t\talloc_size;",
            "\tunsigned int\t\tglobal_available;",
            "\tunsigned int\t\tglobal_reserved;",
            "\tunsigned int\t\tsystembits_inalloc;",
            "\tunsigned int\t\ttotal_allocated;",
            "\tunsigned int\t\tonline_maps;",
            "\tstruct cpumap __percpu\t*maps;",
            "\tunsigned long\t\tscratch_map[IRQ_MATRIX_SIZE];",
            "\tunsigned long\t\tsystem_map[IRQ_MATRIX_SIZE];",
            "};",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/irq_matrix.h>",
            "",
            "/**",
            " * irq_alloc_matrix - Allocate a irq_matrix structure and initialize it",
            " * @matrix_bits:\tNumber of matrix bits must be <= IRQ_MATRIX_BITS",
            " * @alloc_start:\tFrom which bit the allocation search starts",
            " * @alloc_end:\t\tAt which bit the allocation search ends, i.e first",
            " *\t\t\tinvalid bit",
            " */",
            "__init struct irq_matrix *irq_alloc_matrix(unsigned int matrix_bits,",
            "\t\t\t\t\t   unsigned int alloc_start,",
            "\t\t\t\t\t   unsigned int alloc_end)",
            "{",
            "\tstruct irq_matrix *m;",
            "",
            "\tif (matrix_bits > IRQ_MATRIX_BITS)",
            "\t\treturn NULL;",
            "",
            "\tm = kzalloc(sizeof(*m), GFP_KERNEL);",
            "\tif (!m)",
            "\t\treturn NULL;",
            "",
            "\tm->matrix_bits = matrix_bits;",
            "\tm->alloc_start = alloc_start;",
            "\tm->alloc_end = alloc_end;",
            "\tm->alloc_size = alloc_end - alloc_start;",
            "\tm->maps = alloc_percpu(*m->maps);",
            "\tif (!m->maps) {",
            "\t\tkfree(m);",
            "\t\treturn NULL;",
            "\t}",
            "\treturn m;",
            "}",
            "",
            "/**",
            " * irq_matrix_online - Bring the local CPU matrix online",
            " * @m:\t\tMatrix pointer",
            " */"
          ],
          "function_name": null,
          "description": "定义irq_matrix结构体和相关辅助数据结构，提供irq_alloc_matrix函数用于初始化并分配irq_matrix实例，设置矩阵大小、起始结束位置等参数，并分配per-CPU的cpumap数组",
          "similarity": 0.48792701959609985
        }
      ]
    }
  ]
}