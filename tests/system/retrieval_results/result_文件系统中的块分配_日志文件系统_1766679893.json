{
  "query": "文件系统中的块分配 日志文件系统",
  "timestamp": "2025-12-26 00:24:53",
  "retrieved_files": [
    {
      "source_file": "mm/dmapool.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:56:43\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dmapool.c`\n\n---\n\n# dmapool.c 技术文档\n\n## 1. 文件概述\n\n`dmapool.c` 实现了 Linux 内核中的 **DMA 池（DMA Pool）分配器**，用于为设备驱动程序提供小块、一致（coherent）且可 DMA 访问的内存。该分配器基于 `dma_alloc_coherent()` 分配整页内存，并将其划分为固定大小的块，以满足频繁分配/释放小块 DMA 内存的需求，避免直接使用页级分配造成的内存浪费。此机制特别适用于需要大量相同大小 DMA 缓冲区的设备驱动（如 USB、网络、存储控制器等）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct dma_pool`**  \n  表示一个 DMA 池，包含：\n  - `page_list`：已分配物理页的双向链表\n  - `lock`：自旋锁，保护池内操作\n  - `next_block`：空闲块的单向链表头指针\n  - `nr_blocks` / `nr_active` / `nr_pages`：统计信息（总块数、活跃块数、页数）\n  - `dev`：关联的设备\n  - `size` / `allocation` / `boundary`：块大小、页分配大小、边界对齐限制\n  - `name`：池名称（用于调试）\n  - `pools`：挂载到设备 `dma_pools` 列表的节点\n\n- **`struct dma_page`**  \n  表示从 `dma_alloc_coherent()` 分配的一个物理页，包含虚拟地址 `vaddr` 和 DMA 地址 `dma`。\n\n- **`struct dma_block`**  \n  嵌入在每个 DMA 块起始位置的元数据结构，仅包含指向下一个空闲块的指针 `next_block` 和该块的 DMA 地址 `dma`。\n\n### 主要函数\n\n- **`dma_pool_create()`**  \n  创建一个新的 DMA 池，指定名称、设备、块大小、对齐要求和边界限制。\n\n- **`pool_block_pop()` / `pool_block_push()`**  \n  从空闲链表中分配/归还一个 DMA 块。\n\n- **`pool_check_block()` / `pool_block_err()` / `pool_init_page()`**  \n  调试辅助函数（在 `DMAPOOL_DEBUG` 启用时），用于检测内存越界、重复释放等错误，并进行内存毒化（poisoning）。\n\n- **`pools_show()`**  \n  sysfs 接口回调，显示设备下所有 DMA 池的统计信息。\n\n## 3. 关键实现\n\n- **内存组织**：  \n  每次调用 `dma_alloc_coherent()` 分配至少一页（`PAGE_SIZE`）的连续物理内存（`allocation` 字段）。该页被划分为多个 `size` 字节的块。每个块的起始处嵌入 `struct dma_block` 元数据。\n\n- **空闲管理**：  \n  所有空闲块通过 `next_block` 指针组成一个**全局单向链表**（由 `dma_pool.next_block` 指向头节点）。分配时从链表头部弹出，释放时压入头部。**已分配块不被显式跟踪**，仅通过 `nr_active` 计数。\n\n- **边界对齐处理**：  \n  若指定了 `boundary`（如 4KB），则确保单个 DMA 块不会跨越该边界。实现上通过限制每页实际可用区域或调整分配策略（代码片段未完整展示具体划分逻辑）。\n\n- **调试支持（DMAPOOL_DEBUG）**：  \n  在 SLUB 调试开启时启用：\n  - 分配时用 `POOL_POISON_ALLOCATED` 填充用户区域（若未启用 init-on-alloc）\n  - 释放时用 `POOL_POISON_FREED` 填充，并检查是否已被释放（防 double-free）\n  - 提供 `pool_find_page()` 辅助验证 DMA 地址有效性\n\n- **Sysfs 集成**：  \n  首次为设备创建 DMA 池时，自动注册 `pools` sysfs 属性文件，可通过 `/sys/devices/.../pools` 查看池状态（名称、活跃块数、总块数、块大小、页数）。\n\n- **并发控制**：  \n  - `pools_lock`：保护设备 `dma_pools` 列表的增删\n  - `pools_reg_lock`：防止 `dma_pool_create()` 与 `dma_pool_destroy()` 之间的竞争\n  - `dma_pool.lock`：保护池内部的空闲链表和计数器\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/dma-mapping.h>`：提供 `dma_alloc_coherent()` / `dma_free_coherent()` 等底层 DMA 映射接口\n  - `<linux/device.h>`：设备模型及 sysfs 支持\n  - `<linux/slab.h>`：用于分配 `struct dma_pool` 结构体内存\n\n- **调试依赖**：\n  - `CONFIG_SLUB_DEBUG_ON`：启用内存毒化和错误检查\n  - `<linux/poison.h>`：提供 `POOL_POISON_*` 常量\n\n- **同步原语**：\n  - `<linux/mutex.h>` / `<linux/spinlock.h>`：提供互斥锁和自旋锁\n\n## 5. 使用场景\n\n- **设备驱动开发**：  \n  当驱动需要频繁分配/释放**固定大小**的小块（通常小于一页）DMA 缓冲区时，使用 DMA 池可显著提升性能并减少内存碎片。典型场景包括：\n  - USB 主机控制器的传输描述符（TD）池\n  - 网络设备的接收/发送描述符环\n  - 存储控制器的命令/状态块\n\n- **替代方案**：  \n  相比直接调用 `dma_alloc_coherent()` 分配整页内存，DMA 池避免了小块分配的内存浪费；相比通用 slab 分配器（如 kmalloc），它保证了返回内存的 DMA 一致性（无需手动缓存维护）。\n\n- **限制条件**：  \n  - 仅适用于**一致性 DMA 映射**（coherent DMA）\n  - 所有块大小在池创建时固定\n  - 不适用于大块（接近或超过一页）内存分配",
      "similarity": 0.5549072027206421,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/dmapool.c",
          "start_line": 1,
          "end_line": 71,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * DMA Pool allocator",
            " *",
            " * Copyright 2001 David Brownell",
            " * Copyright 2007 Intel Corporation",
            " *   Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This allocator returns small blocks of a given size which are DMA-able by",
            " * the given device.  It uses the dma_alloc_coherent page allocator to get",
            " * new pages, then splits them up into blocks of the required size.",
            " * Many older drivers still have their own code to do this.",
            " *",
            " * The current design of this allocator is fairly simple.  The pool is",
            " * represented by the 'struct dma_pool' which keeps a doubly-linked list of",
            " * allocated pages.  Each page in the page_list is split into blocks of at",
            " * least 'size' bytes.  Free blocks are tracked in an unsorted singly-linked",
            " * list of free blocks across all pages.  Used blocks aren't tracked, but we",
            " * keep a count of how many are currently allocated from each page.",
            " */",
            "",
            "#include <linux/device.h>",
            "#include <linux/dma-mapping.h>",
            "#include <linux/dmapool.h>",
            "#include <linux/kernel.h>",
            "#include <linux/list.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/poison.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/slab.h>",
            "#include <linux/stat.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/string.h>",
            "#include <linux/types.h>",
            "#include <linux/wait.h>",
            "",
            "#ifdef CONFIG_SLUB_DEBUG_ON",
            "#define DMAPOOL_DEBUG 1",
            "#endif",
            "",
            "struct dma_block {",
            "\tstruct dma_block *next_block;",
            "\tdma_addr_t dma;",
            "};",
            "",
            "struct dma_pool {\t\t/* the pool */",
            "\tstruct list_head page_list;",
            "\tspinlock_t lock;",
            "\tstruct dma_block *next_block;",
            "\tsize_t nr_blocks;",
            "\tsize_t nr_active;",
            "\tsize_t nr_pages;",
            "\tstruct device *dev;",
            "\tunsigned int size;",
            "\tunsigned int allocation;",
            "\tunsigned int boundary;",
            "\tchar name[32];",
            "\tstruct list_head pools;",
            "};",
            "",
            "struct dma_page {\t\t/* cacheable header for 'allocation' bytes */",
            "\tstruct list_head page_list;",
            "\tvoid *vaddr;",
            "\tdma_addr_t dma;",
            "};",
            "",
            "static DEFINE_MUTEX(pools_lock);",
            "static DEFINE_MUTEX(pools_reg_lock);",
            ""
          ],
          "function_name": null,
          "description": "定义DMA池相关结构体及全局锁，用于管理可DMA访问的小块内存分配，包含页面链表、块链表、设备指针及参数配置。",
          "similarity": 0.5130660533905029
        },
        {
          "chunk_id": 1,
          "file_path": "mm/dmapool.c",
          "start_line": 72,
          "end_line": 196,
          "content": [
            "static ssize_t pools_show(struct device *dev, struct device_attribute *attr, char *buf)",
            "{",
            "\tstruct dma_pool *pool;",
            "\tunsigned size;",
            "",
            "\tsize = sysfs_emit(buf, \"poolinfo - 0.1\\n\");",
            "",
            "\tmutex_lock(&pools_lock);",
            "\tlist_for_each_entry(pool, &dev->dma_pools, pools) {",
            "\t\t/* per-pool info, no real statistics yet */",
            "\t\tsize += sysfs_emit_at(buf, size, \"%-16s %4zu %4zu %4u %2zu\\n\",",
            "\t\t\t\t      pool->name, pool->nr_active,",
            "\t\t\t\t      pool->nr_blocks, pool->size,",
            "\t\t\t\t      pool->nr_pages);",
            "\t}",
            "\tmutex_unlock(&pools_lock);",
            "",
            "\treturn size;",
            "}",
            "static void pool_check_block(struct dma_pool *pool, struct dma_block *block,",
            "\t\t\t     gfp_t mem_flags)",
            "{",
            "\tu8 *data = (void *)block;",
            "\tint i;",
            "",
            "\tfor (i = sizeof(struct dma_block); i < pool->size; i++) {",
            "\t\tif (data[i] == POOL_POISON_FREED)",
            "\t\t\tcontinue;",
            "\t\tdev_err(pool->dev, \"%s %s, %p (corrupted)\\n\", __func__,",
            "\t\t\tpool->name, block);",
            "",
            "\t\t/*",
            "\t\t * Dump the first 4 bytes even if they are not",
            "\t\t * POOL_POISON_FREED",
            "\t\t */",
            "\t\tprint_hex_dump(KERN_ERR, \"\", DUMP_PREFIX_OFFSET, 16, 1,",
            "\t\t\t\tdata, pool->size, 1);",
            "\t\tbreak;",
            "\t}",
            "",
            "\tif (!want_init_on_alloc(mem_flags))",
            "\t\tmemset(block, POOL_POISON_ALLOCATED, pool->size);",
            "}",
            "static bool pool_block_err(struct dma_pool *pool, void *vaddr, dma_addr_t dma)",
            "{",
            "\tstruct dma_block *block = pool->next_block;",
            "\tstruct dma_page *page;",
            "",
            "\tpage = pool_find_page(pool, dma);",
            "\tif (!page) {",
            "\t\tdev_err(pool->dev, \"%s %s, %p/%pad (bad dma)\\n\",",
            "\t\t\t__func__, pool->name, vaddr, &dma);",
            "\t\treturn true;",
            "\t}",
            "",
            "\twhile (block) {",
            "\t\tif (block != vaddr) {",
            "\t\t\tblock = block->next_block;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tdev_err(pool->dev, \"%s %s, dma %pad already free\\n\",",
            "\t\t\t__func__, pool->name, &dma);",
            "\t\treturn true;",
            "\t}",
            "",
            "\tmemset(vaddr, POOL_POISON_FREED, pool->size);",
            "\treturn false;",
            "}",
            "static void pool_init_page(struct dma_pool *pool, struct dma_page *page)",
            "{",
            "\tmemset(page->vaddr, POOL_POISON_FREED, pool->allocation);",
            "}",
            "static void pool_check_block(struct dma_pool *pool, struct dma_block *block,",
            "\t\t\t     gfp_t mem_flags)",
            "{",
            "}",
            "static bool pool_block_err(struct dma_pool *pool, void *vaddr, dma_addr_t dma)",
            "{",
            "\tif (want_init_on_free())",
            "\t\tmemset(vaddr, 0, pool->size);",
            "\treturn false;",
            "}",
            "static void pool_init_page(struct dma_pool *pool, struct dma_page *page)",
            "{",
            "}",
            "static void pool_block_push(struct dma_pool *pool, struct dma_block *block,",
            "\t\t\t    dma_addr_t dma)",
            "{",
            "\tblock->dma = dma;",
            "\tblock->next_block = pool->next_block;",
            "\tpool->next_block = block;",
            "}",
            "static void pool_initialise_page(struct dma_pool *pool, struct dma_page *page)",
            "{",
            "\tunsigned int next_boundary = pool->boundary, offset = 0;",
            "\tstruct dma_block *block, *first = NULL, *last = NULL;",
            "",
            "\tpool_init_page(pool, page);",
            "\twhile (offset + pool->size <= pool->allocation) {",
            "\t\tif (offset + pool->size > next_boundary) {",
            "\t\t\toffset = next_boundary;",
            "\t\t\tnext_boundary += pool->boundary;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tblock = page->vaddr + offset;",
            "\t\tblock->dma = page->dma + offset;",
            "\t\tblock->next_block = NULL;",
            "",
            "\t\tif (last)",
            "\t\t\tlast->next_block = block;",
            "\t\telse",
            "\t\t\tfirst = block;",
            "\t\tlast = block;",
            "",
            "\t\toffset += pool->size;",
            "\t\tpool->nr_blocks++;",
            "\t}",
            "",
            "\tlast->next_block = pool->next_block;",
            "\tpool->next_block = first;",
            "",
            "\tlist_add(&page->page_list, &pool->page_list);",
            "\tpool->nr_pages++;",
            "}"
          ],
          "function_name": "pools_show, pool_check_block, pool_block_err, pool_init_page, pool_check_block, pool_block_err, pool_init_page, pool_block_push, pool_initialise_page",
          "description": "提供DMA池调试显示、块校验、错误检测及页面初始化等功能，部分函数存在重复声明或未完成实现，上下文不完整。",
          "similarity": 0.4736142158508301
        },
        {
          "chunk_id": 2,
          "file_path": "mm/dmapool.c",
          "start_line": 360,
          "end_line": 419,
          "content": [
            "void dma_pool_destroy(struct dma_pool *pool)",
            "{",
            "\tstruct dma_page *page, *tmp;",
            "\tbool empty, busy = false;",
            "",
            "\tif (unlikely(!pool))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&pools_reg_lock);",
            "\tmutex_lock(&pools_lock);",
            "\tlist_del(&pool->pools);",
            "\tempty = list_empty(&pool->dev->dma_pools);",
            "\tmutex_unlock(&pools_lock);",
            "\tif (empty)",
            "\t\tdevice_remove_file(pool->dev, &dev_attr_pools);",
            "\tmutex_unlock(&pools_reg_lock);",
            "",
            "\tif (pool->nr_active) {",
            "\t\tdev_err(pool->dev, \"%s %s busy\\n\", __func__, pool->name);",
            "\t\tbusy = true;",
            "\t}",
            "",
            "\tlist_for_each_entry_safe(page, tmp, &pool->page_list, page_list) {",
            "\t\tif (!busy)",
            "\t\t\tdma_free_coherent(pool->dev, pool->allocation,",
            "\t\t\t\t\t  page->vaddr, page->dma);",
            "\t\tlist_del(&page->page_list);",
            "\t\tkfree(page);",
            "\t}",
            "",
            "\tkfree(pool);",
            "}",
            "void dma_pool_free(struct dma_pool *pool, void *vaddr, dma_addr_t dma)",
            "{",
            "\tstruct dma_block *block = vaddr;",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&pool->lock, flags);",
            "\tif (!pool_block_err(pool, vaddr, dma)) {",
            "\t\tpool_block_push(pool, block, dma);",
            "\t\tpool->nr_active--;",
            "\t}",
            "\tspin_unlock_irqrestore(&pool->lock, flags);",
            "}",
            "static void dmam_pool_release(struct device *dev, void *res)",
            "{",
            "\tstruct dma_pool *pool = *(struct dma_pool **)res;",
            "",
            "\tdma_pool_destroy(pool);",
            "}",
            "static int dmam_pool_match(struct device *dev, void *res, void *match_data)",
            "{",
            "\treturn *(struct dma_pool **)res == match_data;",
            "}",
            "void dmam_pool_destroy(struct dma_pool *pool)",
            "{",
            "\tstruct device *dev = pool->dev;",
            "",
            "\tWARN_ON(devres_release(dev, dmam_pool_release, dmam_pool_match, pool));",
            "}"
          ],
          "function_name": "dma_pool_destroy, dma_pool_free, dmam_pool_release, dmam_pool_match, dmam_pool_destroy",
          "description": "实现DMA池销毁逻辑，包括资源回收、活跃块计数管理及页面内存释放，通过互斥锁保证线程安全，支持设备资源释放回调。",
          "similarity": 0.45427969098091125
        }
      ]
    },
    {
      "source_file": "mm/cma_debug.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:43:28\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `cma_debug.c`\n\n---\n\n# cma_debug.c 技术文档\n\n## 1. 文件概述\n\n`cma_debug.c` 是 Linux 内核中用于提供 CMA（Contiguous Memory Allocator，连续内存分配器）调试信息的 DebugFS 接口实现。该文件通过 DebugFS 文件系统暴露 CMA 区域的关键运行时状态（如已用内存、最大连续空闲块等），并支持通过写入操作动态分配或释放 CMA 内存，便于开发和调试阶段对 CMA 行为进行观察与控制。\n\n## 2. 核心功能\n\n### 数据结构\n- `struct cma_mem`：用于跟踪通过 DebugFS 分配的 CMA 内存块。\n  - `node`：哈希链表节点，用于将内存块挂入 CMA 实例的 `mem_head` 链表。\n  - `p`：指向分配的起始页结构体（`struct page *`）。\n  - `n`：分配的页数。\n\n### 主要函数\n- `cma_debugfs_get()`：通用读取函数，用于导出 CMA 结构中的简单数值字段。\n- `cma_used_get()`：计算并返回 CMA 区域中已使用的页数。\n- `cma_maxchunk_get()`：扫描 CMA 位图，找出最大的连续空闲内存块（以页为单位）。\n- `cma_alloc_write()` / `cma_alloc_mem()`：通过 DebugFS 写入触发 CMA 内存分配，并记录到链表。\n- `cma_free_write()` / `cma_free_mem()`：通过 DebugFS 写入触发 CMA 内存释放，支持部分释放（仅当 `order_per_bit == 0` 时）。\n- `cma_add_to_cma_mem_list()` / `cma_get_entry_from_list()`：线程安全地管理 CMA 内存块链表。\n- `cma_debugfs_add_one()`：为单个 CMA 区域创建对应的 DebugFS 目录及文件。\n- `cma_debugfs_init()`：模块初始化函数，遍历所有 CMA 区域并注册 DebugFS 接口。\n\n### DebugFS 文件接口\n- `alloc`（写）：分配指定页数的 CMA 内存。\n- `free`（写）：释放指定页数的 CMA 内存。\n- `base_pfn`（读）：CMA 区域起始物理页帧号。\n- `count`（读）：CMA 区域总页数。\n- `order_per_bit`（读）：位图中每个比特所代表的内存阶数（即 `2^order_per_bit` 页）。\n- `used`（读）：当前已分配的页数。\n- `maxchunk`（读）：当前最大连续空闲块的页数。\n- `bitmap`（读）：CMA 位图的原始二进制数据（以 u32 数组形式导出）。\n\n## 3. 关键实现\n\n- **位图解析**：\n  - `cma_used_get()` 使用 `bitmap_weight()` 统计位图中已置位（已分配）的比特数，再乘以每比特对应的页数（`1 << cma->order_per_bit`）得到实际已用页数。\n  - `cma_maxchunk_get()` 通过交替调用 `find_next_zero_bit()` 和 `find_next_bit()` 遍历位图，计算最长连续零序列（即最大空闲块），结果同样按 `order_per_bit` 转换为页数。\n\n- **内存块管理**：\n  - 所有通过 DebugFS 分配的内存块被封装为 `struct cma_mem` 并加入对应 CMA 实例的哈希链表（`mem_head`），由自旋锁 `mem_head_lock` 保护。\n  - 释放时支持“部分释放”：若请求释放页数小于当前块大小且 `order_per_bit == 0`（即位图粒度为单页），则拆分块并重新入链；否则拒绝部分释放（因高阶分配无法保证内部连续性）。\n\n- **DebugFS 注册**：\n  - 使用 `DEFINE_DEBUGFS_ATTRIBUTE` 宏定义只读或只写属性文件的操作函数。\n  - `debugfs_create_u32_array()` 用于高效导出大块位图数据。\n\n- **初始化时机**：\n  - 通过 `late_initcall()` 注册，在内核启动后期（CMA 区域已初始化完毕）创建 DebugFS 节点。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/debugfs.h>`：提供 DebugFS 接口。\n  - `<linux/cma.h>` 和本地 `\"cma.h\"`：定义 CMA 核心数据结构（如 `struct cma`）和操作函数（`cma_alloc()`/`cma_release()`）。\n  - `<linux/list.h>`、`<linux/slab.h>`、`<linux/mm_types.h>`：提供链表、内存分配和页描述符支持。\n\n- **内核模块依赖**：\n  - 依赖 CMA 子系统（`mm/cma.c`）提供的全局变量 `cma_areas[]` 和 `cma_area_count`。\n  - 依赖 DebugFS 子系统（`fs/debugfs/`）提供虚拟文件系统支持。\n\n## 5. 使用场景\n\n- **内核开发与调试**：\n  - 开发者可通过 `/sys/kernel/debug/cma/<area>/` 下的文件实时监控 CMA 区域使用情况（如碎片化程度、剩余空间）。\n  - 通过向 `alloc`/`free` 写入页数，可手动触发 CMA 分配/释放行为，用于复现或验证内存分配问题。\n\n- **系统集成测试**：\n  - 在嵌入式或多媒体系统中，用于验证 CMA 配置是否满足大块连续内存需求（如摄像头、GPU 缓冲区）。\n  - 通过 `maxchunk` 文件评估内存碎片对应用性能的影响。\n\n- **故障诊断**：\n  - 当 CMA 分配失败时，检查 `used` 和 `maxchunk` 可快速判断是空间不足还是碎片化严重。\n  - `bitmap` 文件可用于离线分析位图状态，辅助定位分配算法问题。",
      "similarity": 0.5416932702064514,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/cma_debug.c",
          "start_line": 1,
          "end_line": 23,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * CMA DebugFS Interface",
            " *",
            " * Copyright (c) 2015 Sasha Levin <sasha.levin@oracle.com>",
            " */",
            "",
            "",
            "#include <linux/debugfs.h>",
            "#include <linux/cma.h>",
            "#include <linux/list.h>",
            "#include <linux/kernel.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm_types.h>",
            "",
            "#include \"cma.h\"",
            "",
            "struct cma_mem {",
            "\tstruct hlist_node node;",
            "\tstruct page *p;",
            "\tunsigned long n;",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义用于跟踪CMA区域内存分配的结构体cma_mem，包含链表节点、页指针及分配数量字段",
          "similarity": 0.5219841003417969
        },
        {
          "chunk_id": 1,
          "file_path": "mm/cma_debug.c",
          "start_line": 24,
          "end_line": 128,
          "content": [
            "static int cma_debugfs_get(void *data, u64 *val)",
            "{",
            "\tunsigned long *p = data;",
            "",
            "\t*val = *p;",
            "",
            "\treturn 0;",
            "}",
            "static int cma_used_get(void *data, u64 *val)",
            "{",
            "\tstruct cma *cma = data;",
            "\tunsigned long used;",
            "",
            "\tspin_lock_irq(&cma->lock);",
            "\t/* pages counter is smaller than sizeof(int) */",
            "\tused = bitmap_weight(cma->bitmap, (int)cma_bitmap_maxno(cma));",
            "\tspin_unlock_irq(&cma->lock);",
            "\t*val = (u64)used << cma->order_per_bit;",
            "",
            "\treturn 0;",
            "}",
            "static int cma_maxchunk_get(void *data, u64 *val)",
            "{",
            "\tstruct cma *cma = data;",
            "\tunsigned long maxchunk = 0;",
            "\tunsigned long start, end = 0;",
            "\tunsigned long bitmap_maxno = cma_bitmap_maxno(cma);",
            "",
            "\tspin_lock_irq(&cma->lock);",
            "\tfor (;;) {",
            "\t\tstart = find_next_zero_bit(cma->bitmap, bitmap_maxno, end);",
            "\t\tif (start >= bitmap_maxno)",
            "\t\t\tbreak;",
            "\t\tend = find_next_bit(cma->bitmap, bitmap_maxno, start);",
            "\t\tmaxchunk = max(end - start, maxchunk);",
            "\t}",
            "\tspin_unlock_irq(&cma->lock);",
            "\t*val = (u64)maxchunk << cma->order_per_bit;",
            "",
            "\treturn 0;",
            "}",
            "static void cma_add_to_cma_mem_list(struct cma *cma, struct cma_mem *mem)",
            "{",
            "\tspin_lock(&cma->mem_head_lock);",
            "\thlist_add_head(&mem->node, &cma->mem_head);",
            "\tspin_unlock(&cma->mem_head_lock);",
            "}",
            "static int cma_free_mem(struct cma *cma, int count)",
            "{",
            "\tstruct cma_mem *mem = NULL;",
            "",
            "\twhile (count) {",
            "\t\tmem = cma_get_entry_from_list(cma);",
            "\t\tif (mem == NULL)",
            "\t\t\treturn 0;",
            "",
            "\t\tif (mem->n <= count) {",
            "\t\t\tcma_release(cma, mem->p, mem->n);",
            "\t\t\tcount -= mem->n;",
            "\t\t\tkfree(mem);",
            "\t\t} else if (cma->order_per_bit == 0) {",
            "\t\t\tcma_release(cma, mem->p, count);",
            "\t\t\tmem->p += count;",
            "\t\t\tmem->n -= count;",
            "\t\t\tcount = 0;",
            "\t\t\tcma_add_to_cma_mem_list(cma, mem);",
            "\t\t} else {",
            "\t\t\tpr_debug(\"cma: cannot release partial block when order_per_bit != 0\\n\");",
            "\t\t\tcma_add_to_cma_mem_list(cma, mem);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "",
            "}",
            "static int cma_free_write(void *data, u64 val)",
            "{",
            "\tint pages = val;",
            "\tstruct cma *cma = data;",
            "",
            "\treturn cma_free_mem(cma, pages);",
            "}",
            "static int cma_alloc_mem(struct cma *cma, int count)",
            "{",
            "\tstruct cma_mem *mem;",
            "\tstruct page *p;",
            "",
            "\tmem = kzalloc(sizeof(*mem), GFP_KERNEL);",
            "\tif (!mem)",
            "\t\treturn -ENOMEM;",
            "",
            "\tp = cma_alloc(cma, count, 0, false);",
            "\tif (!p) {",
            "\t\tkfree(mem);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\tmem->p = p;",
            "\tmem->n = count;",
            "",
            "\tcma_add_to_cma_mem_list(cma, mem);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "cma_debugfs_get, cma_used_get, cma_maxchunk_get, cma_add_to_cma_mem_list, cma_free_mem, cma_free_write, cma_alloc_mem",
          "description": "实现CMA调试接口函数，包括读取调试信息、统计使用量、计算最大连续空闲块、管理内存列表及分配/释放操作",
          "similarity": 0.5010492205619812
        },
        {
          "chunk_id": 2,
          "file_path": "mm/cma_debug.c",
          "start_line": 154,
          "end_line": 193,
          "content": [
            "static int cma_alloc_write(void *data, u64 val)",
            "{",
            "\tint pages = val;",
            "\tstruct cma *cma = data;",
            "",
            "\treturn cma_alloc_mem(cma, pages);",
            "}",
            "static void cma_debugfs_add_one(struct cma *cma, struct dentry *root_dentry)",
            "{",
            "\tstruct dentry *tmp;",
            "",
            "\ttmp = debugfs_create_dir(cma->name, root_dentry);",
            "",
            "\tdebugfs_create_file(\"alloc\", 0200, tmp, cma, &cma_alloc_fops);",
            "\tdebugfs_create_file(\"free\", 0200, tmp, cma, &cma_free_fops);",
            "\tdebugfs_create_file(\"base_pfn\", 0444, tmp,",
            "\t\t\t    &cma->base_pfn, &cma_debugfs_fops);",
            "\tdebugfs_create_file(\"count\", 0444, tmp, &cma->count, &cma_debugfs_fops);",
            "\tdebugfs_create_file(\"order_per_bit\", 0444, tmp,",
            "\t\t\t    &cma->order_per_bit, &cma_debugfs_fops);",
            "\tdebugfs_create_file(\"used\", 0444, tmp, cma, &cma_used_fops);",
            "\tdebugfs_create_file(\"maxchunk\", 0444, tmp, cma, &cma_maxchunk_fops);",
            "",
            "\tcma->dfs_bitmap.array = (u32 *)cma->bitmap;",
            "\tcma->dfs_bitmap.n_elements = DIV_ROUND_UP(cma_bitmap_maxno(cma),",
            "\t\t\t\t\t\t  BITS_PER_BYTE * sizeof(u32));",
            "\tdebugfs_create_u32_array(\"bitmap\", 0444, tmp, &cma->dfs_bitmap);",
            "}",
            "static int __init cma_debugfs_init(void)",
            "{",
            "\tstruct dentry *cma_debugfs_root;",
            "\tint i;",
            "",
            "\tcma_debugfs_root = debugfs_create_dir(\"cma\", NULL);",
            "",
            "\tfor (i = 0; i < cma_area_count; i++)",
            "\t\tcma_debugfs_add_one(&cma_areas[i], cma_debugfs_root);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "cma_alloc_write, cma_debugfs_add_one, cma_debugfs_init",
          "description": "初始化CMA调试文件系统入口点，注册调试接口文件并为每个CMA区域创建调试目录及其属性文件",
          "similarity": 0.47898727655410767
        }
      ]
    },
    {
      "source_file": "mm/percpu-stats.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:09:48\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `percpu-stats.c`\n\n---\n\n# percpu-stats.c 技术文档\n\n## 1. 文件概述\n\n`percpu-stats.c` 是 Linux 内核中用于收集和展示 per-CPU（每 CPU）内存分配器运行时统计信息的调试模块。该文件通过 debugfs 接口 `/sys/kernel/debug/percpu_stats` 向用户空间暴露详细的 per-CPU 内存分配状态，包括全局统计、分配参数以及每个内存块（chunk）的碎片、分配大小分布等信息，主要用于性能分析和内存调试。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct percpu_stats pcpu_stats`：全局 per-CPU 分配器统计信息，记录分配/释放次数、当前/最大分配数、最小/最大分配大小等。\n- `struct pcpu_alloc_info pcpu_stats_ai`：per-CPU 内存布局配置信息，如单元大小、静态区大小、动态区大小等。\n- `int *buffer`：临时缓冲区，用于在 `chunk_map_stats()` 中存储每个 chunk 的分配/空闲片段大小。\n\n### 主要函数\n- `find_max_nr_alloc(void)`：遍历所有 per-CPU 内存块（chunks），找出具有最多分配项（`nr_alloc`）的块，用于预分配临时缓冲区。\n- `chunk_map_stats(struct seq_file *m, struct pcpu_chunk *chunk, int *buffer)`：分析并打印单个 chunk 的详细状态，包括碎片情况、分配大小分布（最小、中位数、最大）等。\n- `percpu_stats_show(struct seq_file *m, void *v)`：debugfs 文件的读取回调函数，汇总并输出全局统计、分配配置及所有 chunk 的详细信息。\n- `init_percpu_stats_debugfs(void)`：模块初始化函数，在 debugfs 中创建 `percpu_stats` 文件。\n\n## 3. 关键实现\n\n### Chunk 状态分析算法\n`chunk_map_stats()` 函数通过遍历 `alloc_map` 和 `bound_map` 位图来重建 chunk 的内存布局：\n- `alloc_map` 标记已分配的最小单位（`PCPU_MIN_ALLOC_SIZE`）；\n- `bound_map` 标记每个分配区域的结束边界；\n- 通过交替查找 `alloc_map` 和 `bound_map` 中的下一个置位位，将连续区域划分为“已分配”或“空闲”片段；\n- 已分配片段记录为正数，空闲片段记录为负数，并统一转换为字节数；\n- 对片段数组排序后，负值（空闲）在前且按绝对值降序排列，便于计算总碎片（`sum_frag`）和最大连续空闲块（`max_frag`）；\n- 从第一个非负值开始，提取当前最小、中位数和最大分配大小。\n\n### 并发安全与重试机制\n由于 per-CPU 分配器状态可能在缓冲区分配前后发生变化，`percpu_stats_show()` 采用以下策略确保一致性：\n1. 先在 `pcpu_lock` 保护下获取当前最大 `nr_alloc`；\n2. 按此值分配足够大的临时缓冲区；\n3. 再次加锁检查 `nr_alloc` 是否增长，若增长则释放缓冲区并重试；\n4. 整个统计过程在 `pcpu_lock` 临界区内完成，避免并发修改导致的数据不一致。\n\n### 输出格式\n使用宏 `P(X, Y)`、`PL(X)`、`PU(X)` 统一格式化输出，字段对齐，数值右对齐，便于阅读和脚本解析。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/debugfs.h>`：提供 debugfs 文件系统接口；\n  - `<linux/percpu.h>` 和 `\"percpu-internal.h\"`：访问 per-CPU 分配器内部数据结构（如 `pcpu_chunk`、`pcpu_chunk_lists`、`pcpu_lock` 等）；\n  - `<linux/seq_file.h>`：支持顺序文件输出；\n  - `<linux/sort.h>`：用于对分配/空闲片段数组排序；\n  - `<linux/vmalloc.h>`：使用 `vmalloc_array()` 分配大块临时内存。\n\n- **内核子系统**：\n  - 依赖 per-CPU 内存管理子系统（`mm/percpu.c`）提供的全局变量和内部结构；\n  - 通过 `late_initcall` 在内核初始化后期注册 debugfs 接口。\n\n## 5. 使用场景\n\n- **内核开发者调试**：当怀疑 per-CPU 内存分配存在碎片化、性能下降或内存泄漏时，可通过读取 `/sys/kernel/debug/percpu_stats` 获取详细分配状态。\n- **系统性能分析**：运维人员或性能工程师可监控 `nr_cur_alloc`、`free_bytes`、`sum_frag` 等指标，评估 per-CPU 内存使用效率。\n- **内存优化参考**：通过观察 `cur_min_alloc`、`cur_med_alloc`、`cur_max_alloc` 等分配大小分布，指导内核模块调整 per-CPU 变量的大小或对齐方式。\n- **验证内存布局**：检查 `unit_size`、`static_size`、`dyn_size` 等配置是否符合预期，特别是在自定义内核配置或架构移植时。",
      "similarity": 0.5413811206817627,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/percpu-stats.c",
          "start_line": 25,
          "end_line": 212,
          "content": [
            "static int cmpint(const void *a, const void *b)",
            "{",
            "\treturn *(int *)a - *(int *)b;",
            "}",
            "static int find_max_nr_alloc(void)",
            "{",
            "\tstruct pcpu_chunk *chunk;",
            "\tint slot, max_nr_alloc;",
            "",
            "\tmax_nr_alloc = 0;",
            "\tfor (slot = 0; slot < pcpu_nr_slots; slot++)",
            "\t\tlist_for_each_entry(chunk, &pcpu_chunk_lists[slot], list)",
            "\t\t\tmax_nr_alloc = max(max_nr_alloc, chunk->nr_alloc);",
            "",
            "\treturn max_nr_alloc;",
            "}",
            "static void chunk_map_stats(struct seq_file *m, struct pcpu_chunk *chunk,",
            "\t\t\t    int *buffer)",
            "{",
            "\tstruct pcpu_block_md *chunk_md = &chunk->chunk_md;",
            "\tint i, last_alloc, as_len, start, end;",
            "\tint *alloc_sizes, *p;",
            "\t/* statistics */",
            "\tint sum_frag = 0, max_frag = 0;",
            "\tint cur_min_alloc = 0, cur_med_alloc = 0, cur_max_alloc = 0;",
            "",
            "\talloc_sizes = buffer;",
            "",
            "\t/*",
            "\t * find_last_bit returns the start value if nothing found.",
            "\t * Therefore, we must determine if it is a failure of find_last_bit",
            "\t * and set the appropriate value.",
            "\t */",
            "\tlast_alloc = find_last_bit(chunk->alloc_map,",
            "\t\t\t\t   pcpu_chunk_map_bits(chunk) -",
            "\t\t\t\t   chunk->end_offset / PCPU_MIN_ALLOC_SIZE - 1);",
            "\tlast_alloc = test_bit(last_alloc, chunk->alloc_map) ?",
            "\t\t     last_alloc + 1 : 0;",
            "",
            "\tas_len = 0;",
            "\tstart = chunk->start_offset / PCPU_MIN_ALLOC_SIZE;",
            "",
            "\t/*",
            "\t * If a bit is set in the allocation map, the bound_map identifies",
            "\t * where the allocation ends.  If the allocation is not set, the",
            "\t * bound_map does not identify free areas as it is only kept accurate",
            "\t * on allocation, not free.",
            "\t *",
            "\t * Positive values are allocations and negative values are free",
            "\t * fragments.",
            "\t */",
            "\twhile (start < last_alloc) {",
            "\t\tif (test_bit(start, chunk->alloc_map)) {",
            "\t\t\tend = find_next_bit(chunk->bound_map, last_alloc,",
            "\t\t\t\t\t    start + 1);",
            "\t\t\talloc_sizes[as_len] = 1;",
            "\t\t} else {",
            "\t\t\tend = find_next_bit(chunk->alloc_map, last_alloc,",
            "\t\t\t\t\t    start + 1);",
            "\t\t\talloc_sizes[as_len] = -1;",
            "\t\t}",
            "",
            "\t\talloc_sizes[as_len++] *= (end - start) * PCPU_MIN_ALLOC_SIZE;",
            "",
            "\t\tstart = end;",
            "\t}",
            "",
            "\t/*",
            "\t * The negative values are free fragments and thus sorting gives the",
            "\t * free fragments at the beginning in largest first order.",
            "\t */",
            "\tif (as_len > 0) {",
            "\t\tsort(alloc_sizes, as_len, sizeof(int), cmpint, NULL);",
            "",
            "\t\t/* iterate through the unallocated fragments */",
            "\t\tfor (i = 0, p = alloc_sizes; *p < 0 && i < as_len; i++, p++) {",
            "\t\t\tsum_frag -= *p;",
            "\t\t\tmax_frag = max(max_frag, -1 * (*p));",
            "\t\t}",
            "",
            "\t\tcur_min_alloc = alloc_sizes[i];",
            "\t\tcur_med_alloc = alloc_sizes[(i + as_len - 1) / 2];",
            "\t\tcur_max_alloc = alloc_sizes[as_len - 1];",
            "\t}",
            "",
            "\tP(\"nr_alloc\", chunk->nr_alloc);",
            "\tP(\"max_alloc_size\", chunk->max_alloc_size);",
            "\tP(\"empty_pop_pages\", chunk->nr_empty_pop_pages);",
            "\tP(\"first_bit\", chunk_md->first_free);",
            "\tP(\"free_bytes\", chunk->free_bytes);",
            "\tP(\"contig_bytes\", chunk_md->contig_hint * PCPU_MIN_ALLOC_SIZE);",
            "\tP(\"sum_frag\", sum_frag);",
            "\tP(\"max_frag\", max_frag);",
            "\tP(\"cur_min_alloc\", cur_min_alloc);",
            "\tP(\"cur_med_alloc\", cur_med_alloc);",
            "\tP(\"cur_max_alloc\", cur_max_alloc);",
            "\tseq_putc(m, '\\n');",
            "}",
            "static int percpu_stats_show(struct seq_file *m, void *v)",
            "{",
            "\tstruct pcpu_chunk *chunk;",
            "\tint slot, max_nr_alloc;",
            "\tint *buffer;",
            "",
            "alloc_buffer:",
            "\tspin_lock_irq(&pcpu_lock);",
            "\tmax_nr_alloc = find_max_nr_alloc();",
            "\tspin_unlock_irq(&pcpu_lock);",
            "",
            "\t/* there can be at most this many free and allocated fragments */",
            "\tbuffer = vmalloc_array(2 * max_nr_alloc + 1, sizeof(int));",
            "\tif (!buffer)",
            "\t\treturn -ENOMEM;",
            "",
            "\tspin_lock_irq(&pcpu_lock);",
            "",
            "\t/* if the buffer allocated earlier is too small */",
            "\tif (max_nr_alloc < find_max_nr_alloc()) {",
            "\t\tspin_unlock_irq(&pcpu_lock);",
            "\t\tvfree(buffer);",
            "\t\tgoto alloc_buffer;",
            "\t}",
            "",
            "#define PL(X)\t\t\t\t\t\t\t\t\\",
            "\tseq_printf(m, \"  %-20s: %12lld\\n\", #X, (long long int)pcpu_stats_ai.X)",
            "",
            "\tseq_printf(m,",
            "\t\t\t\"Percpu Memory Statistics\\n\"",
            "\t\t\t\"Allocation Info:\\n\"",
            "\t\t\t\"----------------------------------------\\n\");",
            "\tPL(unit_size);",
            "\tPL(static_size);",
            "\tPL(reserved_size);",
            "\tPL(dyn_size);",
            "\tPL(atom_size);",
            "\tPL(alloc_size);",
            "\tseq_putc(m, '\\n');",
            "",
            "#undef PL",
            "",
            "#define PU(X) \\",
            "\tseq_printf(m, \"  %-20s: %12llu\\n\", #X, (unsigned long long)pcpu_stats.X)",
            "",
            "\tseq_printf(m,",
            "\t\t\t\"Global Stats:\\n\"",
            "\t\t\t\"----------------------------------------\\n\");",
            "\tPU(nr_alloc);",
            "\tPU(nr_dealloc);",
            "\tPU(nr_cur_alloc);",
            "\tPU(nr_max_alloc);",
            "\tPU(nr_chunks);",
            "\tPU(nr_max_chunks);",
            "\tPU(min_alloc_size);",
            "\tPU(max_alloc_size);",
            "\tP(\"empty_pop_pages\", pcpu_nr_empty_pop_pages);",
            "\tseq_putc(m, '\\n');",
            "",
            "#undef PU",
            "",
            "\tseq_printf(m,",
            "\t\t\t\"Per Chunk Stats:\\n\"",
            "\t\t\t\"----------------------------------------\\n\");",
            "",
            "\tif (pcpu_reserved_chunk) {",
            "\t\tseq_puts(m, \"Chunk: <- Reserved Chunk\\n\");",
            "\t\tchunk_map_stats(m, pcpu_reserved_chunk, buffer);",
            "\t}",
            "",
            "\tfor (slot = 0; slot < pcpu_nr_slots; slot++) {",
            "\t\tlist_for_each_entry(chunk, &pcpu_chunk_lists[slot], list) {",
            "\t\t\tif (chunk == pcpu_first_chunk)",
            "\t\t\t\tseq_puts(m, \"Chunk: <- First Chunk\\n\");",
            "\t\t\telse if (slot == pcpu_to_depopulate_slot)",
            "\t\t\t\tseq_puts(m, \"Chunk (to_depopulate)\\n\");",
            "\t\t\telse if (slot == pcpu_sidelined_slot)",
            "\t\t\t\tseq_puts(m, \"Chunk (sidelined):\\n\");",
            "\t\t\telse",
            "\t\t\t\tseq_puts(m, \"Chunk:\\n\");",
            "\t\t\tchunk_map_stats(m, chunk, buffer);",
            "\t\t}",
            "\t}",
            "",
            "\tspin_unlock_irq(&pcpu_lock);",
            "",
            "\tvfree(buffer);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "cmpint, find_max_nr_alloc, chunk_map_stats, percpu_stats_show",
          "description": "实现percpu内存分配统计的核心逻辑，包含比较函数、最大分配计算、块级统计分析及总体统计展示函数，通过遍历所有内存块生成详细的分配状态报告。",
          "similarity": 0.621342658996582
        },
        {
          "chunk_id": 2,
          "file_path": "mm/percpu-stats.c",
          "start_line": 227,
          "end_line": 233,
          "content": [
            "static int __init init_percpu_stats_debugfs(void)",
            "{",
            "\tdebugfs_create_file(\"percpu_stats\", 0444, NULL, NULL,",
            "\t\t\t&percpu_stats_fops);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "init_percpu_stats_debugfs",
          "description": "注册debugfs文件系统接口，创建名为percpu_stats的调试文件节点，用于暴露percpu内存分配的统计信息给用户空间工具。",
          "similarity": 0.5926830768585205
        },
        {
          "chunk_id": 0,
          "file_path": "mm/percpu-stats.c",
          "start_line": 1,
          "end_line": 24,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * mm/percpu-debug.c",
            " *",
            " * Copyright (C) 2017\t\tFacebook Inc.",
            " * Copyright (C) 2017\t\tDennis Zhou <dennis@kernel.org>",
            " *",
            " * Prints statistics about the percpu allocator and backing chunks.",
            " */",
            "#include <linux/debugfs.h>",
            "#include <linux/list.h>",
            "#include <linux/percpu.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/sort.h>",
            "#include <linux/vmalloc.h>",
            "",
            "#include \"percpu-internal.h\"",
            "",
            "#define P(X, Y) \\",
            "\tseq_printf(m, \"  %-20s: %12lld\\n\", X, (long long int)Y)",
            "",
            "struct percpu_stats pcpu_stats;",
            "struct pcpu_alloc_info pcpu_stats_ai;",
            ""
          ],
          "function_name": null,
          "description": "定义用于存储percpu分配器统计信息的全局结构体pcpu_stats和pcpu_stats_ai，为后续统计信息收集和调试输出提供数据容器。",
          "similarity": 0.52788245677948
        }
      ]
    }
  ]
}