{
  "query": "虚拟内存原理",
  "timestamp": "2025-12-25 23:25:59",
  "retrieved_files": [
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.6249868273735046,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.6743072271347046
        },
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.6520488262176514
        },
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.5862747430801392
        }
      ]
    },
    {
      "source_file": "mm/memory_hotplug.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:43:14\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memory_hotplug.c`\n\n---\n\n# memory_hotplug.c 技术文档\n\n## 1. 文件概述\n\n`memory_hotplug.c` 是 Linux 内核中实现内存热插拔（Memory Hotplug）功能的核心源文件，位于 `mm/` 子系统目录下。该文件提供了在系统运行时动态添加或移除物理内存区域的能力，包括内存资源注册、页表映射管理、内存上线策略控制、以及与 NUMA 架构的协同支持。它主要处理热添加内存时的初始化、内存块（memory block）管理、vmemmap 映射优化、以及在线策略配置等关键逻辑。\n\n## 2. 核心功能\n\n### 主要全局变量与参数\n- `memmap_mode`：控制是否启用“内存上的 memmap”（memmap on memory）特性，支持 `disable`、`enable` 和 `force` 三种模式。\n- `online_policy`：定义内存上线时的默认区域分配策略，可选 `contig-zones`（保持区域连续）或 `auto-movable`（自动分配到 ZONE_MOVABLE）。\n- `auto_movable_ratio`：在 `auto-movable` 策略下，系统允许的 MOVABLE 与 KERNEL 内存的最大百分比比例（默认 301%，即约 3:1）。\n- `auto_movable_numa_aware`（仅 CONFIG_NUMA）：是否在 `auto-movable` 策略中考虑 NUMA 节点级别的内存统计。\n- `mhp_default_online_type`：内存热插拔时的默认上线类型（如 `MMOP_ONLINE_KERNEL`、`MMOP_ONLINE_MOVABLE` 等）。\n- `movable_node_enabled`：标志是否启用了可移动节点（movable node）功能。\n- `max_mem_size`：系统允许的最大内存大小上限（默认为 `U64_MAX`）。\n\n### 主要函数与接口\n- `get_online_mems()` / `put_online_mems()`：获取/释放内存热插拔读锁，用于保护内存上线/下线操作。\n- `mem_hotplug_begin()` / `mem_hotplug_done()`：执行内存热插拔写操作前后的同步原语，同时持有 CPU 热插拔读锁和内存热插拔写锁。\n- `mhp_get_default_online_type()` / `mhp_set_default_online_type()`：获取或设置内存热插拔的默认上线类型。\n- `register_memory_resource()`：将新添加的内存区域注册为 I/O 资源（`System RAM` 类型），并检查是否超出 `max_mem_size` 限制。\n- `mhp_memmap_on_memory()`：判断当前是否启用了 memmap on memory 特性。\n- `memory_block_memmap_on_memory_pages()`：计算在 memmap on memory 模式下，每个内存块所需的额外页数（可能因对齐而浪费内存）。\n\n### 回调机制\n- `online_page_callback`：指向当前用于上线单个页面的回调函数，默认为 `generic_online_page`。\n- `set_online_page_callback()` / `restore_online_page_callback()`（声明未在片段中，但有注释说明）：用于动态替换或恢复页面上线回调。\n\n### 内核参数（module_param）\n- `memmap_on_memory`：启用 memmap on memory 功能（Y/N/force）。\n- `online_policy`：设置默认上线策略。\n- `auto_movable_ratio`：设置 MOVABLE/KERNEL 内存比例上限。\n- `auto_movable_numa_aware`：是否在 NUMA 感知下应用 auto-movable 策略。\n- 启动参数 `memhp_default_state=`：通过内核命令行设置默认上线状态。\n\n## 3. 关键实现\n\n### Memmap on Memory 机制\n当启用 `CONFIG_MHP_MEMMAP_ON_MEMORY` 时，内核尝试将描述物理页的 `struct page` 数组（即 vmemmap）直接放置在待热插拔的内存区域内，而非依赖预先保留的虚拟地址空间。这减少了对固定 vmemmap 区域的依赖，提升灵活性：\n- **ENABLE 模式**：仅当 vmemmap 大小能被页块（pageblock）整除时才启用。\n- **FORCE 模式**：强制对齐到页块边界，即使造成内存浪费（通过 `pageblock_align()` 实现），确保总能使用该内存区域存放 memmap。\n\n### 内存上线策略\n- **contig-zones（默认）**：将新内存添加到现有内存区域末尾，保持 ZONE_NORMAL 等区域的物理连续性。\n- **auto-movable**：根据全局（及 NUMA 节点）的 KERNEL 与 MOVABLE 内存比例，智能决定是否将新内存加入 ZONE_MOVABLE，以提高内存可迁移性和碎片整理效率。比例由 `auto_movable_ratio` 控制。\n\n### 并发控制\n使用 `percpu_rwsem mem_hotplug_lock` 作为内存热插拔操作的主同步机制：\n- 读操作（如内存访问路径）调用 `get/put_online_mems()` 获取读锁。\n- 写操作（如 add_memory）调用 `mem_hotplug_begin/done()` 获取写锁，并同时持有 `cpus_read_lock()` 防止 CPU 热插拔干扰。\n\n### 资源与大小限制\n- 通过 `mhp_range_allowed()` 检查待添加内存是否超出 `max_mem_size`。\n- 使用 `register_memory_resource()` 将内存注册为 `IORESOURCE_SYSTEM_RAM` 资源，若资源名非 \"System RAM\" 则标记为驱动管理（`IORESOURCE_SYSRAM_DRIVER_MANAGED`）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `mm.h`、`page-isolation.h`、`migrate.h`、`compaction.h` 等，用于页面分配、隔离、迁移和压缩。\n- **体系结构相关**：包含 `asm/tlbflush.h` 用于 TLB 刷新；依赖 `pfn.h`、`memblock.h` 处理物理页帧和启动内存布局。\n- **设备模型与 sysfs**：通过 `memory.h` 与用户空间交互（如 `/sys/devices/system/memory/`）。\n- **NUMA 支持**：在 `CONFIG_NUMA` 下使用节点感知策略。\n- **虚拟内存**：依赖 `vmalloc.h` 和 `memremap.h` 管理 vmemmap 映射。\n- **电源管理**：包含 `suspend.h`，可能与休眠/唤醒流程协调。\n- **固件接口**：使用 `firmware-map.h` 与平台固件交互内存布局信息。\n\n## 5. 使用场景\n\n- **物理内存热添加**：在支持内存热插拔的服务器（如 IBM Power、x86 ACPI 系统）上，动态增加 DIMM 或内存模块后，内核通过此文件完成内存初始化和上线。\n- **虚拟化环境**：KVM、Xen 等 hypervisor 向客户机热添加内存时，客户机内核调用此模块处理新增内存。\n- **内存故障恢复**：在某些 RAS（Reliability, Availability, Serviceability）场景中，隔离坏页后重新上线备用内存。\n- **测试与开发**：通过 sysfs 接口（如 `echo online > /sys/devices/system/memory/memoryX/state`）手动上线内存块，配合 `online_policy` 和 `memmap_on_memory` 参数进行功能验证。\n- **容器与云平台**：支持弹性内存扩展，按需分配物理内存资源。",
      "similarity": 0.6164743900299072,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 52,
          "end_line": 153,
          "content": [
            "static inline unsigned long memory_block_memmap_size(void)",
            "{",
            "\treturn PHYS_PFN(memory_block_size_bytes()) * sizeof(struct page);",
            "}",
            "static inline unsigned long memory_block_memmap_on_memory_pages(void)",
            "{",
            "\tunsigned long nr_pages = PFN_UP(memory_block_memmap_size());",
            "",
            "\t/*",
            "\t * In \"forced\" memmap_on_memory mode, we add extra pages to align the",
            "\t * vmemmap size to cover full pageblocks. That way, we can add memory",
            "\t * even if the vmemmap size is not properly aligned, however, we might waste",
            "\t * memory.",
            "\t */",
            "\tif (memmap_mode == MEMMAP_ON_MEMORY_FORCE)",
            "\t\treturn pageblock_align(nr_pages);",
            "\treturn nr_pages;",
            "}",
            "static int set_memmap_mode(const char *val, const struct kernel_param *kp)",
            "{",
            "\tint ret, mode;",
            "\tbool enabled;",
            "",
            "\tif (sysfs_streq(val, \"force\") ||  sysfs_streq(val, \"FORCE\")) {",
            "\t\tmode = MEMMAP_ON_MEMORY_FORCE;",
            "\t} else {",
            "\t\tret = kstrtobool(val, &enabled);",
            "\t\tif (ret < 0)",
            "\t\t\treturn ret;",
            "\t\tif (enabled)",
            "\t\t\tmode = MEMMAP_ON_MEMORY_ENABLE;",
            "\t\telse",
            "\t\t\tmode = MEMMAP_ON_MEMORY_DISABLE;",
            "\t}",
            "\t*((int *)kp->arg) = mode;",
            "\tif (mode == MEMMAP_ON_MEMORY_FORCE) {",
            "\t\tunsigned long memmap_pages = memory_block_memmap_on_memory_pages();",
            "",
            "\t\tpr_info_once(\"Memory hotplug will waste %ld pages in each memory block\\n\",",
            "\t\t\t     memmap_pages - PFN_UP(memory_block_memmap_size()));",
            "\t}",
            "\treturn 0;",
            "}",
            "static int get_memmap_mode(char *buffer, const struct kernel_param *kp)",
            "{",
            "\tint mode = *((int *)kp->arg);",
            "",
            "\tif (mode == MEMMAP_ON_MEMORY_FORCE)",
            "\t\treturn sprintf(buffer, \"force\\n\");",
            "\treturn sprintf(buffer, \"%c\\n\", mode ? 'Y' : 'N');",
            "}",
            "static inline bool mhp_memmap_on_memory(void)",
            "{",
            "\treturn memmap_mode != MEMMAP_ON_MEMORY_DISABLE;",
            "}",
            "static inline bool mhp_memmap_on_memory(void)",
            "{",
            "\treturn false;",
            "}",
            "static int set_online_policy(const char *val, const struct kernel_param *kp)",
            "{",
            "\tint ret = sysfs_match_string(online_policy_to_str, val);",
            "",
            "\tif (ret < 0)",
            "\t\treturn ret;",
            "\t*((int *)kp->arg) = ret;",
            "\treturn 0;",
            "}",
            "static int get_online_policy(char *buffer, const struct kernel_param *kp)",
            "{",
            "\treturn sprintf(buffer, \"%s\\n\", online_policy_to_str[*((int *)kp->arg)]);",
            "}",
            "void get_online_mems(void)",
            "{",
            "\tpercpu_down_read(&mem_hotplug_lock);",
            "}",
            "void put_online_mems(void)",
            "{",
            "\tpercpu_up_read(&mem_hotplug_lock);",
            "}",
            "int mhp_get_default_online_type(void)",
            "{",
            "\tif (mhp_default_online_type >= 0)",
            "\t\treturn mhp_default_online_type;",
            "",
            "\tif (IS_ENABLED(CONFIG_MHP_DEFAULT_ONLINE_TYPE_OFFLINE))",
            "\t\tmhp_default_online_type = MMOP_OFFLINE;",
            "\telse if (IS_ENABLED(CONFIG_MHP_DEFAULT_ONLINE_TYPE_ONLINE_AUTO))",
            "\t\tmhp_default_online_type = MMOP_ONLINE;",
            "\telse if (IS_ENABLED(CONFIG_MHP_DEFAULT_ONLINE_TYPE_ONLINE_KERNEL))",
            "\t\tmhp_default_online_type = MMOP_ONLINE_KERNEL;",
            "\telse if (IS_ENABLED(CONFIG_MHP_DEFAULT_ONLINE_TYPE_ONLINE_MOVABLE))",
            "\t\tmhp_default_online_type = MMOP_ONLINE_MOVABLE;",
            "\telse",
            "\t\tmhp_default_online_type = MMOP_OFFLINE;",
            "",
            "\treturn mhp_default_online_type;",
            "}",
            "void mhp_set_default_online_type(int online_type)",
            "{",
            "\tmhp_default_online_type = online_type;",
            "}"
          ],
          "function_name": "memory_block_memmap_size, memory_block_memmap_on_memory_pages, set_memmap_mode, get_memmap_mode, mhp_memmap_on_memory, mhp_memmap_on_memory, set_online_policy, get_online_policy, get_online_mems, put_online_mems, mhp_get_default_online_type, mhp_set_default_online_type",
          "description": "提供内存映射模式配置接口(set/get)，实现memmap_on_memory策略判断逻辑，包含在线策略设置与获取函数及默认类型处理逻辑。",
          "similarity": 0.5603604316711426
        },
        {
          "chunk_id": 13,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 2233,
          "end_line": 2352,
          "content": [
            "void __remove_memory(u64 start, u64 size)",
            "{",
            "",
            "\t/*",
            "\t * trigger BUG() if some memory is not offlined prior to calling this",
            "\t * function",
            "\t */",
            "\tif (try_remove_memory(start, size))",
            "\t\tBUG();",
            "}",
            "int remove_memory(u64 start, u64 size)",
            "{",
            "\tint rc;",
            "",
            "\tlock_device_hotplug();",
            "\trc = try_remove_memory(start, size);",
            "\tunlock_device_hotplug();",
            "",
            "\treturn rc;",
            "}",
            "static int try_offline_memory_block(struct memory_block *mem, void *arg)",
            "{",
            "\tuint8_t online_type = MMOP_ONLINE_KERNEL;",
            "\tuint8_t **online_types = arg;",
            "\tstruct page *page;",
            "\tint rc;",
            "",
            "\t/*",
            "\t * Sense the online_type via the zone of the memory block. Offlining",
            "\t * with multiple zones within one memory block will be rejected",
            "\t * by offlining code ... so we don't care about that.",
            "\t */",
            "\tpage = pfn_to_online_page(section_nr_to_pfn(mem->start_section_nr));",
            "\tif (page && zone_idx(page_zone(page)) == ZONE_MOVABLE)",
            "\t\tonline_type = MMOP_ONLINE_MOVABLE;",
            "",
            "\trc = device_offline(&mem->dev);",
            "\t/*",
            "\t * Default is MMOP_OFFLINE - change it only if offlining succeeded,",
            "\t * so try_reonline_memory_block() can do the right thing.",
            "\t */",
            "\tif (!rc)",
            "\t\t**online_types = online_type;",
            "",
            "\t(*online_types)++;",
            "\t/* Ignore if already offline. */",
            "\treturn rc < 0 ? rc : 0;",
            "}",
            "static int try_reonline_memory_block(struct memory_block *mem, void *arg)",
            "{",
            "\tuint8_t **online_types = arg;",
            "\tint rc;",
            "",
            "\tif (**online_types != MMOP_OFFLINE) {",
            "\t\tmem->online_type = **online_types;",
            "\t\trc = device_online(&mem->dev);",
            "\t\tif (rc < 0)",
            "\t\t\tpr_warn(\"%s: Failed to re-online memory: %d\",",
            "\t\t\t\t__func__, rc);",
            "\t}",
            "",
            "\t/* Continue processing all remaining memory blocks. */",
            "\t(*online_types)++;",
            "\treturn 0;",
            "}",
            "int offline_and_remove_memory(u64 start, u64 size)",
            "{",
            "\tconst unsigned long mb_count = size / memory_block_size_bytes();",
            "\tuint8_t *online_types, *tmp;",
            "\tint rc;",
            "",
            "\tif (!IS_ALIGNED(start, memory_block_size_bytes()) ||",
            "\t    !IS_ALIGNED(size, memory_block_size_bytes()) || !size)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * We'll remember the old online type of each memory block, so we can",
            "\t * try to revert whatever we did when offlining one memory block fails",
            "\t * after offlining some others succeeded.",
            "\t */",
            "\tonline_types = kmalloc_array(mb_count, sizeof(*online_types),",
            "\t\t\t\t     GFP_KERNEL);",
            "\tif (!online_types)",
            "\t\treturn -ENOMEM;",
            "\t/*",
            "\t * Initialize all states to MMOP_OFFLINE, so when we abort processing in",
            "\t * try_offline_memory_block(), we'll skip all unprocessed blocks in",
            "\t * try_reonline_memory_block().",
            "\t */",
            "\tmemset(online_types, MMOP_OFFLINE, mb_count);",
            "",
            "\tlock_device_hotplug();",
            "",
            "\ttmp = online_types;",
            "\trc = walk_memory_blocks(start, size, &tmp, try_offline_memory_block);",
            "",
            "\t/*",
            "\t * In case we succeeded to offline all memory, remove it.",
            "\t * This cannot fail as it cannot get onlined in the meantime.",
            "\t */",
            "\tif (!rc) {",
            "\t\trc = try_remove_memory(start, size);",
            "\t\tif (rc)",
            "\t\t\tpr_err(\"%s: Failed to remove memory: %d\", __func__, rc);",
            "\t}",
            "",
            "\t/*",
            "\t * Rollback what we did. While memory onlining might theoretically fail",
            "\t * (nacked by a notifier), it barely ever happens.",
            "\t */",
            "\tif (rc) {",
            "\t\ttmp = online_types;",
            "\t\twalk_memory_blocks(start, size, &tmp,",
            "\t\t\t\t   try_reonline_memory_block);",
            "\t}",
            "\tunlock_device_hotplug();",
            "",
            "\tkfree(online_types);",
            "\treturn rc;",
            "}"
          ],
          "function_name": "__remove_memory, remove_memory, try_offline_memory_block, try_reonline_memory_block, offline_and_remove_memory",
          "description": "offline_and_remove_memory 管理内存块的离线与移除流程，记录各内存块原始在线类型，在部分失败时回滚操作；try_offline_memory_block 和 try_reonline_memory_block 分别用于设置内存块离线状态及恢复在线状态",
          "similarity": 0.5595325231552124
        },
        {
          "chunk_id": 9,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 1529,
          "end_line": 1660,
          "content": [
            "int __ref __add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags)",
            "{",
            "\tstruct resource *res;",
            "\tint ret;",
            "",
            "\tres = register_memory_resource(start, size, \"System RAM\");",
            "\tif (IS_ERR(res))",
            "\t\treturn PTR_ERR(res);",
            "",
            "\tret = add_memory_resource(nid, res, mhp_flags);",
            "\tif (ret < 0)",
            "\t\trelease_memory_resource(res);",
            "\treturn ret;",
            "}",
            "int add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags)",
            "{",
            "\tint rc;",
            "",
            "\tlock_device_hotplug();",
            "\trc = __add_memory(nid, start, size, mhp_flags);",
            "\tunlock_device_hotplug();",
            "",
            "\treturn rc;",
            "}",
            "int add_memory_driver_managed(int nid, u64 start, u64 size,",
            "\t\t\t      const char *resource_name, mhp_t mhp_flags)",
            "{",
            "\tstruct resource *res;",
            "\tint rc;",
            "",
            "\tif (!resource_name ||",
            "\t    strstr(resource_name, \"System RAM (\") != resource_name ||",
            "\t    resource_name[strlen(resource_name) - 1] != ')')",
            "\t\treturn -EINVAL;",
            "",
            "\tlock_device_hotplug();",
            "",
            "\tres = register_memory_resource(start, size, resource_name);",
            "\tif (IS_ERR(res)) {",
            "\t\trc = PTR_ERR(res);",
            "\t\tgoto out_unlock;",
            "\t}",
            "",
            "\trc = add_memory_resource(nid, res, mhp_flags);",
            "\tif (rc < 0)",
            "\t\trelease_memory_resource(res);",
            "",
            "out_unlock:",
            "\tunlock_device_hotplug();",
            "\treturn rc;",
            "}",
            "struct range __weak arch_get_mappable_range(void)",
            "{",
            "\tstruct range mhp_range = {",
            "\t\t.start = 0UL,",
            "\t\t.end = -1ULL,",
            "\t};",
            "\treturn mhp_range;",
            "}",
            "struct range mhp_get_pluggable_range(bool need_mapping)",
            "{",
            "\tconst u64 max_phys = PHYSMEM_END;",
            "\tstruct range mhp_range;",
            "",
            "\tif (need_mapping) {",
            "\t\tmhp_range = arch_get_mappable_range();",
            "\t\tif (mhp_range.start > max_phys) {",
            "\t\t\tmhp_range.start = 0;",
            "\t\t\tmhp_range.end = 0;",
            "\t\t}",
            "\t\tmhp_range.end = min_t(u64, mhp_range.end, max_phys);",
            "\t} else {",
            "\t\tmhp_range.start = 0;",
            "\t\tmhp_range.end = max_phys;",
            "\t}",
            "\treturn mhp_range;",
            "}",
            "bool mhp_range_allowed(u64 start, u64 size, bool need_mapping)",
            "{",
            "\tstruct range mhp_range = mhp_get_pluggable_range(need_mapping);",
            "\tu64 end = start + size;",
            "",
            "\tif (start < end && start >= mhp_range.start && (end - 1) <= mhp_range.end)",
            "\t\treturn true;",
            "",
            "\tpr_warn(\"Hotplug memory [%#llx-%#llx] exceeds maximum addressable range [%#llx-%#llx]\\n\",",
            "\t\tstart, end, mhp_range.start, mhp_range.end);",
            "\treturn false;",
            "}",
            "static int scan_movable_pages(unsigned long start, unsigned long end,",
            "\t\t\t      unsigned long *movable_pfn)",
            "{",
            "\tunsigned long pfn;",
            "",
            "\tfor_each_valid_pfn(pfn, start, end) {",
            "\t\tstruct page *page;",
            "\t\tstruct folio *folio;",
            "",
            "\t\tpage = pfn_to_page(pfn);",
            "\t\tif (PageLRU(page))",
            "\t\t\tgoto found;",
            "\t\tif (__PageMovable(page))",
            "\t\t\tgoto found;",
            "",
            "\t\t/*",
            "\t\t * PageOffline() pages that are not marked __PageMovable() and",
            "\t\t * have a reference count > 0 (after MEM_GOING_OFFLINE) are",
            "\t\t * definitely unmovable. If their reference count would be 0,",
            "\t\t * they could at least be skipped when offlining memory.",
            "\t\t */",
            "\t\tif (PageOffline(page) && page_count(page))",
            "\t\t\treturn -EBUSY;",
            "",
            "\t\tif (!PageHuge(page))",
            "\t\t\tcontinue;",
            "\t\tfolio = page_folio(page);",
            "\t\t/*",
            "\t\t * This test is racy as we hold no reference or lock.  The",
            "\t\t * hugetlb page could have been free'ed and head is no longer",
            "\t\t * a hugetlb page before the following check.  In such unlikely",
            "\t\t * cases false positives and negatives are possible.  Calling",
            "\t\t * code must deal with these scenarios.",
            "\t\t */",
            "\t\tif (folio_test_hugetlb_migratable(folio))",
            "\t\t\tgoto found;",
            "\t\tpfn |= folio_nr_pages(folio) - 1;",
            "\t}",
            "\treturn -ENOENT;",
            "found:",
            "\t*movable_pfn = pfn;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "__add_memory, add_memory, add_memory_driver_managed, arch_get_mappable_range, mhp_get_pluggable_range, mhp_range_allowed, scan_movable_pages",
          "description": "__add_memory 和 add_memory 添加内存资源；add_memory_driver_managed 处理驱动管理内存资源；arch_get_mappable_range 获取可映射范围；mhp_get_pluggable_range 确定可插入内存范围；mhp_range_allowed 检查地址有效性；scan_movable_pages 扫描可移动页面",
          "similarity": 0.5585607886314392
        },
        {
          "chunk_id": 2,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 247,
          "end_line": 355,
          "content": [
            "static int __init setup_memhp_default_state(char *str)",
            "{",
            "\tconst int online_type = mhp_online_type_from_str(str);",
            "",
            "\tif (online_type >= 0)",
            "\t\tmhp_default_online_type = online_type;",
            "",
            "\treturn 1;",
            "}",
            "void mem_hotplug_begin(void)",
            "{",
            "\tcpus_read_lock();",
            "\tpercpu_down_write(&mem_hotplug_lock);",
            "}",
            "void mem_hotplug_done(void)",
            "{",
            "\tpercpu_up_write(&mem_hotplug_lock);",
            "\tcpus_read_unlock();",
            "}",
            "static void release_memory_resource(struct resource *res)",
            "{",
            "\tif (!res)",
            "\t\treturn;",
            "\trelease_resource(res);",
            "\tkfree(res);",
            "}",
            "static int check_pfn_span(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\t/*",
            "\t * Disallow all operations smaller than a sub-section and only",
            "\t * allow operations smaller than a section for",
            "\t * SPARSEMEM_VMEMMAP. Note that check_hotplug_memory_range()",
            "\t * enforces a larger memory_block_size_bytes() granularity for",
            "\t * memory that will be marked online, so this check should only",
            "\t * fire for direct arch_{add,remove}_memory() users outside of",
            "\t * add_memory_resource().",
            "\t */",
            "\tunsigned long min_align;",
            "",
            "\tif (IS_ENABLED(CONFIG_SPARSEMEM_VMEMMAP))",
            "\t\tmin_align = PAGES_PER_SUBSECTION;",
            "\telse",
            "\t\tmin_align = PAGES_PER_SECTION;",
            "\tif (!IS_ALIGNED(pfn | nr_pages, min_align))",
            "\t\treturn -EINVAL;",
            "\treturn 0;",
            "}",
            "int __ref __add_pages(int nid, unsigned long pfn, unsigned long nr_pages,",
            "\t\tstruct mhp_params *params)",
            "{",
            "\tconst unsigned long end_pfn = pfn + nr_pages;",
            "\tunsigned long cur_nr_pages;",
            "\tint err;",
            "\tstruct vmem_altmap *altmap = params->altmap;",
            "",
            "\tif (WARN_ON_ONCE(!pgprot_val(params->pgprot)))",
            "\t\treturn -EINVAL;",
            "",
            "\tVM_BUG_ON(!mhp_range_allowed(PFN_PHYS(pfn), nr_pages * PAGE_SIZE, false));",
            "",
            "\tif (altmap) {",
            "\t\t/*",
            "\t\t * Validate altmap is within bounds of the total request",
            "\t\t */",
            "\t\tif (altmap->base_pfn != pfn",
            "\t\t\t\t|| vmem_altmap_offset(altmap) > nr_pages) {",
            "\t\t\tpr_warn_once(\"memory add fail, invalid altmap\\n\");",
            "\t\t\treturn -EINVAL;",
            "\t\t}",
            "\t\taltmap->alloc = 0;",
            "\t}",
            "",
            "\tif (check_pfn_span(pfn, nr_pages)) {",
            "\t\tWARN(1, \"Misaligned %s start: %#lx end: %#lx\\n\", __func__, pfn, pfn + nr_pages - 1);",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tfor (; pfn < end_pfn; pfn += cur_nr_pages) {",
            "\t\t/* Select all remaining pages up to the next section boundary */",
            "\t\tcur_nr_pages = min(end_pfn - pfn,",
            "\t\t\t\t   SECTION_ALIGN_UP(pfn + 1) - pfn);",
            "\t\terr = sparse_add_section(nid, pfn, cur_nr_pages, altmap,",
            "\t\t\t\t\t params->pgmap);",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t\tcond_resched();",
            "\t}",
            "\tvmemmap_populate_print_last();",
            "\treturn err;",
            "}",
            "static unsigned long find_smallest_section_pfn(int nid, struct zone *zone,",
            "\t\t\t\t     unsigned long start_pfn,",
            "\t\t\t\t     unsigned long end_pfn)",
            "{",
            "\tfor (; start_pfn < end_pfn; start_pfn += PAGES_PER_SUBSECTION) {",
            "\t\tif (unlikely(!pfn_to_online_page(start_pfn)))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (unlikely(pfn_to_nid(start_pfn) != nid))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (zone != page_zone(pfn_to_page(start_pfn)))",
            "\t\t\tcontinue;",
            "",
            "\t\treturn start_pfn;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "setup_memhp_default_state, mem_hotplug_begin, mem_hotplug_done, release_memory_resource, check_pfn_span, __add_pages, find_smallest_section_pfn",
          "description": "初始化内存热插拔默认状态，实现内存热插拔锁操作，验证PFN对齐有效性，执行内存添加操作并处理替代映射信息。",
          "similarity": 0.5575071573257446
        },
        {
          "chunk_id": 4,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 592,
          "end_line": 697,
          "content": [
            "void __remove_pages(unsigned long pfn, unsigned long nr_pages,",
            "\t\t    struct vmem_altmap *altmap)",
            "{",
            "\tconst unsigned long end_pfn = pfn + nr_pages;",
            "\tunsigned long cur_nr_pages;",
            "",
            "\tif (check_pfn_span(pfn, nr_pages)) {",
            "\t\tWARN(1, \"Misaligned %s start: %#lx end: %#lx\\n\", __func__, pfn, pfn + nr_pages - 1);",
            "\t\treturn;",
            "\t}",
            "",
            "\tfor (; pfn < end_pfn; pfn += cur_nr_pages) {",
            "\t\tcond_resched();",
            "\t\t/* Select all remaining pages up to the next section boundary */",
            "\t\tcur_nr_pages = min(end_pfn - pfn,",
            "\t\t\t\t   SECTION_ALIGN_UP(pfn + 1) - pfn);",
            "\t\tsparse_remove_section(pfn, cur_nr_pages, altmap);",
            "\t}",
            "}",
            "int set_online_page_callback(online_page_callback_t callback)",
            "{",
            "\tint rc = -EINVAL;",
            "",
            "\tget_online_mems();",
            "\tmutex_lock(&online_page_callback_lock);",
            "",
            "\tif (online_page_callback == generic_online_page) {",
            "\t\tonline_page_callback = callback;",
            "\t\trc = 0;",
            "\t}",
            "",
            "\tmutex_unlock(&online_page_callback_lock);",
            "\tput_online_mems();",
            "",
            "\treturn rc;",
            "}",
            "int restore_online_page_callback(online_page_callback_t callback)",
            "{",
            "\tint rc = -EINVAL;",
            "",
            "\tget_online_mems();",
            "\tmutex_lock(&online_page_callback_lock);",
            "",
            "\tif (online_page_callback == callback) {",
            "\t\tonline_page_callback = generic_online_page;",
            "\t\trc = 0;",
            "\t}",
            "",
            "\tmutex_unlock(&online_page_callback_lock);",
            "\tput_online_mems();",
            "",
            "\treturn rc;",
            "}",
            "void generic_online_page(struct page *page, unsigned int order)",
            "{",
            "\t__free_pages_core(page, order, MEMINIT_HOTPLUG);",
            "}",
            "static void online_pages_range(unsigned long start_pfn, unsigned long nr_pages)",
            "{",
            "\tconst unsigned long end_pfn = start_pfn + nr_pages;",
            "\tunsigned long pfn;",
            "",
            "\t/*",
            "\t * Online the pages in MAX_PAGE_ORDER aligned chunks. The callback might",
            "\t * decide to not expose all pages to the buddy (e.g., expose them",
            "\t * later). We account all pages as being online and belonging to this",
            "\t * zone (\"present\").",
            "\t * When using memmap_on_memory, the range might not be aligned to",
            "\t * MAX_ORDER_NR_PAGES - 1, but pageblock aligned. __ffs() will detect",
            "\t * this and the first chunk to online will be pageblock_nr_pages.",
            "\t */",
            "\tfor (pfn = start_pfn; pfn < end_pfn;) {",
            "\t\tint order;",
            "",
            "\t\t/*",
            "\t\t * Free to online pages in the largest chunks alignment allows.",
            "\t\t *",
            "\t\t * __ffs() behaviour is undefined for 0. start == 0 is",
            "\t\t * MAX_PAGE_ORDER-aligned, Set order to MAX_PAGE_ORDER for",
            "\t\t * the case.",
            "\t\t */",
            "\t\tif (pfn)",
            "\t\t\torder = min_t(int, MAX_PAGE_ORDER, __ffs(pfn));",
            "\t\telse",
            "\t\t\torder = MAX_PAGE_ORDER;",
            "",
            "\t\t(*online_page_callback)(pfn_to_page(pfn), order);",
            "\t\tpfn += (1UL << order);",
            "\t}",
            "",
            "\t/* mark all involved sections as online */",
            "\tonline_mem_sections(start_pfn, end_pfn);",
            "}",
            "static void node_states_check_changes_online(unsigned long nr_pages,",
            "\tstruct zone *zone, struct memory_notify *arg)",
            "{",
            "\tint nid = zone_to_nid(zone);",
            "",
            "\targ->status_change_nid = NUMA_NO_NODE;",
            "\targ->status_change_nid_normal = NUMA_NO_NODE;",
            "",
            "\tif (!node_state(nid, N_MEMORY))",
            "\t\targ->status_change_nid = nid;",
            "\tif (zone_idx(zone) <= ZONE_NORMAL && !node_state(nid, N_NORMAL_MEMORY))",
            "\t\targ->status_change_nid_normal = nid;",
            "}"
          ],
          "function_name": "__remove_pages, set_online_page_callback, restore_online_page_callback, generic_online_page, online_pages_range, node_states_check_changes_online",
          "description": "实现内存页面移除操作，提供在线回调函数注册/恢复接口，分块执行页面上线处理，同步更新内存section在线状态并通知节点状态变化。",
          "similarity": 0.5471016764640808
        }
      ]
    },
    {
      "source_file": "mm/vmalloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:32:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmalloc.c`\n\n---\n\n# vmalloc.c 技术文档\n\n## 1. 文件概述\n\n`vmalloc.c` 是 Linux 内核中实现虚拟内存分配（vmalloc）机制的核心源文件。该文件提供了在内核虚拟地址空间中非连续物理页映射为连续虚拟地址的功能，主要用于分配大块内存、I/O 映射（如 `ioremap`）以及需要页表特殊属性（如不可执行、缓存控制等）的场景。与 `kmalloc` 不同，`vmalloc` 分配的内存物理上不连续，但虚拟地址连续，适用于大内存分配或硬件寄存器映射。\n\n## 2. 核心功能\n\n### 主要函数\n- `is_vmalloc_addr(const void *x)`：判断给定地址是否位于 vmalloc 区域。\n- `vmap_page_range(unsigned long addr, unsigned long end, phys_addr_t phys_addr, pgprot_t prot)`：将指定物理地址范围映射到内核虚拟地址空间，支持普通页和大页。\n- `ioremap_page_range(...)`：用于 I/O 内存重映射（代码片段未完整展示）。\n- `vmap_range_noflush(...)`：执行实际的页表填充操作，不触发 TLB 刷新。\n- `vmap_pte_range`, `vmap_pmd_range`, `vmap_pud_range`, `vmap_p4d_range`：逐级填充页表项的辅助函数。\n- `vmap_try_huge_*` 系列函数（如 `vmap_try_huge_pmd`）：尝试使用大页（huge page）进行映射以提升性能。\n\n### 主要数据结构\n- `struct vfree_deferred`：用于延迟释放 vmalloc 内存的 per-CPU 工作队列结构。\n- `ioremap_max_page_shift`：控制 I/O 映射时允许的最大页面大小（受 `nohugeiomap` 启动参数影响）。\n- `vmap_allow_huge`：控制 vmalloc 是否允许使用大页（受 `nohugevmalloc` 启动参数影响）。\n\n## 3. 关键实现\n\n### 大页（Huge Page）支持\n- 通过 `CONFIG_HAVE_ARCH_HUGE_VMAP` 和 `CONFIG_HAVE_ARCH_HUGE_VMALLOC` 配置选项启用架构相关的大页映射能力。\n- 在页表填充过程中（如 `vmap_pmd_range`），优先尝试使用 PMD/PUD/P4D 级别的大页映射（通过 `vmap_try_huge_pmd` 等函数），前提是：\n  - 地址和物理地址对齐；\n  - 请求区域大小等于对应层级的大页尺寸；\n  - 架构支持该级别的大页（通过 `arch_vmap_*_supported` 判断）；\n  - 当前页表项未被占用或可安全释放下级页表。\n- 启动参数 `nohugeiomap` 和 `nohugevmalloc` 可分别禁用 I/O 映射和通用 vmalloc 的大页功能。\n\n### 页表操作与跟踪\n- 使用 `_track` 后缀的页表分配函数（如 `pte_alloc_kernel_track`）配合 `pgtbl_mod_mask` 标记修改的页表层级，便于后续同步（如 `arch_sync_kernel_mappings`）。\n- 映射完成后调用 `flush_cache_vmap` 确保缓存一致性，并集成 KMSAN（Kernel Memory Sanitizer）支持。\n\n### 安全与调试\n- 使用 `kasan_reset_tag` 处理 KASAN 的内存标记，确保地址比较正确。\n- 通过 `BUG_ON` 检查页表项是否为空，防止覆盖已有映射。\n- 支持 `kmemleak` 内存泄漏检测。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/highmem.h>`、`<linux/pfn.h>` 等提供基础内存操作。\n- **体系结构相关接口**：通过 `asm/tlbflush.h`、`asm/shmparam.h` 及 `arch_*` 函数（如 `arch_make_huge_pte`）适配不同 CPU 架构。\n- **内核基础设施**：\n  - RCU（`<linux/rcupdate.h>`）用于安全遍历；\n  - Per-CPU 变量（`DEFINE_PER_CPU`）实现无锁延迟释放；\n  - 工作队列（`work_struct`）处理异步释放；\n  - 调试工具（`debugobjects`、`kallsyms`、`trace/events/vmalloc.h`）。\n- **安全特性**：集成 KASAN、KMSAN、`set_memory.h`（页属性设置）等。\n\n## 5. 使用场景\n\n- **内核模块加载**：模块的代码和数据通常通过 `vmalloc` 分配。\n- **大内存分配**：当所需内存超过 `kmalloc` 的限制（通常几 MB）时使用。\n- **设备 I/O 映射**：通过 `ioremap` 将设备寄存器映射到内核地址空间，底层调用 `ioremap_page_range`。\n- **动态内核数据结构**：如网络协议栈的某些缓冲区、文件系统元数据缓存等。\n- **安全隔离**：为敏感数据分配具有特殊页属性（如不可执行 NX）的内存区域。",
      "similarity": 0.613590657711029,
      "chunks": [
        {
          "chunk_id": 9,
          "file_path": "mm/vmalloc.c",
          "start_line": 1728,
          "end_line": 1843,
          "content": [
            "static unsigned long",
            "va_alloc(struct vmap_area *va,",
            "\t\tstruct rb_root *root, struct list_head *head,",
            "\t\tunsigned long size, unsigned long align,",
            "\t\tunsigned long vstart, unsigned long vend)",
            "{",
            "\tunsigned long nva_start_addr;",
            "\tint ret;",
            "",
            "\tif (va->va_start > vstart)",
            "\t\tnva_start_addr = ALIGN(va->va_start, align);",
            "\telse",
            "\t\tnva_start_addr = ALIGN(vstart, align);",
            "",
            "\t/* Check the \"vend\" restriction. */",
            "\tif (nva_start_addr + size > vend)",
            "\t\treturn vend;",
            "",
            "\t/* Update the free vmap_area. */",
            "\tret = va_clip(root, head, va, nva_start_addr, size);",
            "\tif (WARN_ON_ONCE(ret))",
            "\t\treturn vend;",
            "",
            "\treturn nva_start_addr;",
            "}",
            "static __always_inline unsigned long",
            "__alloc_vmap_area(struct rb_root *root, struct list_head *head,",
            "\tunsigned long size, unsigned long align,",
            "\tunsigned long vstart, unsigned long vend)",
            "{",
            "\tbool adjust_search_size = true;",
            "\tunsigned long nva_start_addr;",
            "\tstruct vmap_area *va;",
            "",
            "\t/*",
            "\t * Do not adjust when:",
            "\t *   a) align <= PAGE_SIZE, because it does not make any sense.",
            "\t *      All blocks(their start addresses) are at least PAGE_SIZE",
            "\t *      aligned anyway;",
            "\t *   b) a short range where a requested size corresponds to exactly",
            "\t *      specified [vstart:vend] interval and an alignment > PAGE_SIZE.",
            "\t *      With adjusted search length an allocation would not succeed.",
            "\t */",
            "\tif (align <= PAGE_SIZE || (align > PAGE_SIZE && (vend - vstart) == size))",
            "\t\tadjust_search_size = false;",
            "",
            "\tva = find_vmap_lowest_match(root, size, align, vstart, adjust_search_size);",
            "\tif (unlikely(!va))",
            "\t\treturn vend;",
            "",
            "\tnva_start_addr = va_alloc(va, root, head, size, align, vstart, vend);",
            "\tif (nva_start_addr == vend)",
            "\t\treturn vend;",
            "",
            "#if DEBUG_AUGMENT_LOWEST_MATCH_CHECK",
            "\tfind_vmap_lowest_match_check(root, head, size, align);",
            "#endif",
            "",
            "\treturn nva_start_addr;",
            "}",
            "static void free_vmap_area(struct vmap_area *va)",
            "{",
            "\tstruct vmap_node *vn = addr_to_node(va->va_start);",
            "",
            "\t/*",
            "\t * Remove from the busy tree/list.",
            "\t */",
            "\tspin_lock(&vn->busy.lock);",
            "\tunlink_va(va, &vn->busy.root);",
            "\tspin_unlock(&vn->busy.lock);",
            "",
            "\t/*",
            "\t * Insert/Merge it back to the free tree/list.",
            "\t */",
            "\tspin_lock(&free_vmap_area_lock);",
            "\tmerge_or_add_vmap_area_augment(va, &free_vmap_area_root, &free_vmap_area_list);",
            "\tspin_unlock(&free_vmap_area_lock);",
            "}",
            "static inline void",
            "preload_this_cpu_lock(spinlock_t *lock, gfp_t gfp_mask, int node)",
            "{",
            "\tstruct vmap_area *va = NULL;",
            "",
            "\t/*",
            "\t * Preload this CPU with one extra vmap_area object. It is used",
            "\t * when fit type of free area is NE_FIT_TYPE. It guarantees that",
            "\t * a CPU that does an allocation is preloaded.",
            "\t *",
            "\t * We do it in non-atomic context, thus it allows us to use more",
            "\t * permissive allocation masks to be more stable under low memory",
            "\t * condition and high memory pressure.",
            "\t */",
            "\tif (!this_cpu_read(ne_fit_preload_node))",
            "\t\tva = kmem_cache_alloc_node(vmap_area_cachep, gfp_mask, node);",
            "",
            "\tspin_lock(lock);",
            "",
            "\tif (va && __this_cpu_cmpxchg(ne_fit_preload_node, NULL, va))",
            "\t\tkmem_cache_free(vmap_area_cachep, va);",
            "}",
            "static bool",
            "node_pool_add_va(struct vmap_node *n, struct vmap_area *va)",
            "{",
            "\tstruct vmap_pool *vp;",
            "",
            "\tvp = size_to_va_pool(n, va_size(va));",
            "\tif (!vp)",
            "\t\treturn false;",
            "",
            "\tspin_lock(&n->pool_lock);",
            "\tlist_add(&va->list, &vp->head);",
            "\tWRITE_ONCE(vp->len, vp->len + 1);",
            "\tspin_unlock(&n->pool_lock);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "va_alloc, __alloc_vmap_area, free_vmap_area, preload_this_cpu_lock, node_pool_add_va",
          "description": "实现虚拟内存区域的分配/回收核心逻辑，包含智能搜索算法、节点池管理及预加载机制，确保高并发场景下的内存分配稳定性",
          "similarity": 0.6990139484405518
        },
        {
          "chunk_id": 8,
          "file_path": "mm/vmalloc.c",
          "start_line": 1465,
          "end_line": 1624,
          "content": [
            "static __always_inline bool",
            "is_within_this_va(struct vmap_area *va, unsigned long size,",
            "\tunsigned long align, unsigned long vstart)",
            "{",
            "\tunsigned long nva_start_addr;",
            "",
            "\tif (va->va_start > vstart)",
            "\t\tnva_start_addr = ALIGN(va->va_start, align);",
            "\telse",
            "\t\tnva_start_addr = ALIGN(vstart, align);",
            "",
            "\t/* Can be overflowed due to big size or alignment. */",
            "\tif (nva_start_addr + size < nva_start_addr ||",
            "\t\t\tnva_start_addr < vstart)",
            "\t\treturn false;",
            "",
            "\treturn (nva_start_addr + size <= va->va_end);",
            "}",
            "static void",
            "find_vmap_lowest_match_check(struct rb_root *root, struct list_head *head,",
            "\t\t\t     unsigned long size, unsigned long align)",
            "{",
            "\tstruct vmap_area *va_1, *va_2;",
            "\tunsigned long vstart;",
            "\tunsigned int rnd;",
            "",
            "\tget_random_bytes(&rnd, sizeof(rnd));",
            "\tvstart = VMALLOC_START + rnd;",
            "",
            "\tva_1 = find_vmap_lowest_match(root, size, align, vstart, false);",
            "\tva_2 = find_vmap_lowest_linear_match(head, size, align, vstart);",
            "",
            "\tif (va_1 != va_2)",
            "\t\tpr_emerg(\"not lowest: t: 0x%p, l: 0x%p, v: 0x%lx\\n\",",
            "\t\t\tva_1, va_2, vstart);",
            "}",
            "static __always_inline enum fit_type",
            "classify_va_fit_type(struct vmap_area *va,",
            "\tunsigned long nva_start_addr, unsigned long size)",
            "{",
            "\tenum fit_type type;",
            "",
            "\t/* Check if it is within VA. */",
            "\tif (nva_start_addr < va->va_start ||",
            "\t\t\tnva_start_addr + size > va->va_end)",
            "\t\treturn NOTHING_FIT;",
            "",
            "\t/* Now classify. */",
            "\tif (va->va_start == nva_start_addr) {",
            "\t\tif (va->va_end == nva_start_addr + size)",
            "\t\t\ttype = FL_FIT_TYPE;",
            "\t\telse",
            "\t\t\ttype = LE_FIT_TYPE;",
            "\t} else if (va->va_end == nva_start_addr + size) {",
            "\t\ttype = RE_FIT_TYPE;",
            "\t} else {",
            "\t\ttype = NE_FIT_TYPE;",
            "\t}",
            "",
            "\treturn type;",
            "}",
            "static __always_inline int",
            "va_clip(struct rb_root *root, struct list_head *head,",
            "\t\tstruct vmap_area *va, unsigned long nva_start_addr,",
            "\t\tunsigned long size)",
            "{",
            "\tstruct vmap_area *lva = NULL;",
            "\tenum fit_type type = classify_va_fit_type(va, nva_start_addr, size);",
            "",
            "\tif (type == FL_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * No need to split VA, it fully fits.",
            "\t\t *",
            "\t\t * |               |",
            "\t\t * V      NVA      V",
            "\t\t * |---------------|",
            "\t\t */",
            "\t\tunlink_va_augment(va, root);",
            "\t\tkmem_cache_free(vmap_area_cachep, va);",
            "\t} else if (type == LE_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * Split left edge of fit VA.",
            "\t\t *",
            "\t\t * |       |",
            "\t\t * V  NVA  V   R",
            "\t\t * |-------|-------|",
            "\t\t */",
            "\t\tva->va_start += size;",
            "\t} else if (type == RE_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * Split right edge of fit VA.",
            "\t\t *",
            "\t\t *         |       |",
            "\t\t *     L   V  NVA  V",
            "\t\t * |-------|-------|",
            "\t\t */",
            "\t\tva->va_end = nva_start_addr;",
            "\t} else if (type == NE_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * Split no edge of fit VA.",
            "\t\t *",
            "\t\t *     |       |",
            "\t\t *   L V  NVA  V R",
            "\t\t * |---|-------|---|",
            "\t\t */",
            "\t\tlva = __this_cpu_xchg(ne_fit_preload_node, NULL);",
            "\t\tif (unlikely(!lva)) {",
            "\t\t\t/*",
            "\t\t\t * For percpu allocator we do not do any pre-allocation",
            "\t\t\t * and leave it as it is. The reason is it most likely",
            "\t\t\t * never ends up with NE_FIT_TYPE splitting. In case of",
            "\t\t\t * percpu allocations offsets and sizes are aligned to",
            "\t\t\t * fixed align request, i.e. RE_FIT_TYPE and FL_FIT_TYPE",
            "\t\t\t * are its main fitting cases.",
            "\t\t\t *",
            "\t\t\t * There are a few exceptions though, as an example it is",
            "\t\t\t * a first allocation (early boot up) when we have \"one\"",
            "\t\t\t * big free space that has to be split.",
            "\t\t\t *",
            "\t\t\t * Also we can hit this path in case of regular \"vmap\"",
            "\t\t\t * allocations, if \"this\" current CPU was not preloaded.",
            "\t\t\t * See the comment in alloc_vmap_area() why. If so, then",
            "\t\t\t * GFP_NOWAIT is used instead to get an extra object for",
            "\t\t\t * split purpose. That is rare and most time does not",
            "\t\t\t * occur.",
            "\t\t\t *",
            "\t\t\t * What happens if an allocation gets failed. Basically,",
            "\t\t\t * an \"overflow\" path is triggered to purge lazily freed",
            "\t\t\t * areas to free some memory, then, the \"retry\" path is",
            "\t\t\t * triggered to repeat one more time. See more details",
            "\t\t\t * in alloc_vmap_area() function.",
            "\t\t\t */",
            "\t\t\tlva = kmem_cache_alloc(vmap_area_cachep, GFP_NOWAIT);",
            "\t\t\tif (!lva)",
            "\t\t\t\treturn -1;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Build the remainder.",
            "\t\t */",
            "\t\tlva->va_start = va->va_start;",
            "\t\tlva->va_end = nva_start_addr;",
            "",
            "\t\t/*",
            "\t\t * Shrink this VA to remaining size.",
            "\t\t */",
            "\t\tva->va_start = nva_start_addr + size;",
            "\t} else {",
            "\t\treturn -1;",
            "\t}",
            "",
            "\tif (type != FL_FIT_TYPE) {",
            "\t\taugment_tree_propagate_from(va);",
            "",
            "\t\tif (lva)\t/* type == NE_FIT_TYPE */",
            "\t\t\tinsert_vmap_area_augment(lva, &va->rb_node, root, head);",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "is_within_this_va, find_vmap_lowest_match_check, classify_va_fit_type, va_clip",
          "description": "实现虚拟内存区域匹配算法，通过分类判断（FL/LE/RE/NE）进行地址裁剪和区域分割，支持不同场景下的内存分配策略选择",
          "similarity": 0.6932514309883118
        },
        {
          "chunk_id": 18,
          "file_path": "mm/vmalloc.c",
          "start_line": 4366,
          "end_line": 4510,
          "content": [
            "long vread_iter(struct iov_iter *iter, const char *addr, size_t count)",
            "{",
            "\tstruct vmap_node *vn;",
            "\tstruct vmap_area *va;",
            "\tstruct vm_struct *vm;",
            "\tchar *vaddr;",
            "\tsize_t n, size, flags, remains;",
            "\tunsigned long next;",
            "",
            "\taddr = kasan_reset_tag(addr);",
            "",
            "\t/* Don't allow overflow */",
            "\tif ((unsigned long) addr + count < count)",
            "\t\tcount = -(unsigned long) addr;",
            "",
            "\tremains = count;",
            "",
            "\tvn = find_vmap_area_exceed_addr_lock((unsigned long) addr, &va);",
            "\tif (!vn)",
            "\t\tgoto finished_zero;",
            "",
            "\t/* no intersects with alive vmap_area */",
            "\tif ((unsigned long)addr + remains <= va->va_start)",
            "\t\tgoto finished_zero;",
            "",
            "\tdo {",
            "\t\tsize_t copied;",
            "",
            "\t\tif (remains == 0)",
            "\t\t\tgoto finished;",
            "",
            "\t\tvm = va->vm;",
            "\t\tflags = va->flags & VMAP_FLAGS_MASK;",
            "\t\t/*",
            "\t\t * VMAP_BLOCK indicates a sub-type of vm_map_ram area, need",
            "\t\t * be set together with VMAP_RAM.",
            "\t\t */",
            "\t\tWARN_ON(flags == VMAP_BLOCK);",
            "",
            "\t\tif (!vm && !flags)",
            "\t\t\tgoto next_va;",
            "",
            "\t\tif (vm && (vm->flags & VM_UNINITIALIZED))",
            "\t\t\tgoto next_va;",
            "",
            "\t\t/* Pair with smp_wmb() in clear_vm_uninitialized_flag() */",
            "\t\tsmp_rmb();",
            "",
            "\t\tvaddr = (char *) va->va_start;",
            "\t\tsize = vm ? get_vm_area_size(vm) : va_size(va);",
            "",
            "\t\tif (addr >= vaddr + size)",
            "\t\t\tgoto next_va;",
            "",
            "\t\tif (addr < vaddr) {",
            "\t\t\tsize_t to_zero = min_t(size_t, vaddr - addr, remains);",
            "\t\t\tsize_t zeroed = zero_iter(iter, to_zero);",
            "",
            "\t\t\taddr += zeroed;",
            "\t\t\tremains -= zeroed;",
            "",
            "\t\t\tif (remains == 0 || zeroed != to_zero)",
            "\t\t\t\tgoto finished;",
            "\t\t}",
            "",
            "\t\tn = vaddr + size - addr;",
            "\t\tif (n > remains)",
            "\t\t\tn = remains;",
            "",
            "\t\tif (flags & VMAP_RAM)",
            "\t\t\tcopied = vmap_ram_vread_iter(iter, addr, n, flags);",
            "\t\telse if (!(vm && (vm->flags & (VM_IOREMAP | VM_SPARSE))))",
            "\t\t\tcopied = aligned_vread_iter(iter, addr, n);",
            "\t\telse /* IOREMAP | SPARSE area is treated as memory hole */",
            "\t\t\tcopied = zero_iter(iter, n);",
            "",
            "\t\taddr += copied;",
            "\t\tremains -= copied;",
            "",
            "\t\tif (copied != n)",
            "\t\t\tgoto finished;",
            "",
            "\tnext_va:",
            "\t\tnext = va->va_end;",
            "\t\tspin_unlock(&vn->busy.lock);",
            "\t} while ((vn = find_vmap_area_exceed_addr_lock(next, &va)));",
            "",
            "finished_zero:",
            "\tif (vn)",
            "\t\tspin_unlock(&vn->busy.lock);",
            "",
            "\t/* zero-fill memory holes */",
            "\treturn count - remains + zero_iter(iter, remains);",
            "finished:",
            "\t/* Nothing remains, or We couldn't copy/zero everything. */",
            "\tif (vn)",
            "\t\tspin_unlock(&vn->busy.lock);",
            "",
            "\treturn count - remains;",
            "}",
            "int remap_vmalloc_range_partial(struct vm_area_struct *vma, unsigned long uaddr,",
            "\t\t\t\tvoid *kaddr, unsigned long pgoff,",
            "\t\t\t\tunsigned long size)",
            "{",
            "\tstruct vm_struct *area;",
            "\tunsigned long off;",
            "\tunsigned long end_index;",
            "",
            "\tif (check_shl_overflow(pgoff, PAGE_SHIFT, &off))",
            "\t\treturn -EINVAL;",
            "",
            "\tsize = PAGE_ALIGN(size);",
            "",
            "\tif (!PAGE_ALIGNED(uaddr) || !PAGE_ALIGNED(kaddr))",
            "\t\treturn -EINVAL;",
            "",
            "\tarea = find_vm_area(kaddr);",
            "\tif (!area)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!(area->flags & (VM_USERMAP | VM_DMA_COHERENT)))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (check_add_overflow(size, off, &end_index) ||",
            "\t    end_index > get_vm_area_size(area))",
            "\t\treturn -EINVAL;",
            "\tkaddr += off;",
            "",
            "\tdo {",
            "\t\tstruct page *page = vmalloc_to_page(kaddr);",
            "\t\tint ret;",
            "",
            "\t\tret = vm_insert_page(vma, uaddr, page);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "",
            "\t\tuaddr += PAGE_SIZE;",
            "\t\tkaddr += PAGE_SIZE;",
            "\t\tsize -= PAGE_SIZE;",
            "\t} while (size > 0);",
            "",
            "\tvm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vread_iter, remap_vmalloc_range_partial",
          "description": "vread_iter 实现虚拟内存区域的数据读取，处理零填充、对齐读取和RAM区域特殊读取逻辑；remap_vmalloc_range_partial 部分重新映射虚拟内存区域，将内核空间页面插入用户虚拟地址空间",
          "similarity": 0.6664834022521973
        },
        {
          "chunk_id": 7,
          "file_path": "mm/vmalloc.c",
          "start_line": 1208,
          "end_line": 1309,
          "content": [
            "static __always_inline void",
            "link_va(struct vmap_area *va, struct rb_root *root,",
            "\tstruct rb_node *parent, struct rb_node **link,",
            "\tstruct list_head *head)",
            "{",
            "\t__link_va(va, root, parent, link, head, false);",
            "}",
            "static __always_inline void",
            "link_va_augment(struct vmap_area *va, struct rb_root *root,",
            "\tstruct rb_node *parent, struct rb_node **link,",
            "\tstruct list_head *head)",
            "{",
            "\t__link_va(va, root, parent, link, head, true);",
            "}",
            "static __always_inline void",
            "__unlink_va(struct vmap_area *va, struct rb_root *root, bool augment)",
            "{",
            "\tif (WARN_ON(RB_EMPTY_NODE(&va->rb_node)))",
            "\t\treturn;",
            "",
            "\tif (augment)",
            "\t\trb_erase_augmented(&va->rb_node,",
            "\t\t\troot, &free_vmap_area_rb_augment_cb);",
            "\telse",
            "\t\trb_erase(&va->rb_node, root);",
            "",
            "\tlist_del_init(&va->list);",
            "\tRB_CLEAR_NODE(&va->rb_node);",
            "}",
            "static __always_inline void",
            "unlink_va(struct vmap_area *va, struct rb_root *root)",
            "{",
            "\t__unlink_va(va, root, false);",
            "}",
            "static __always_inline void",
            "unlink_va_augment(struct vmap_area *va, struct rb_root *root)",
            "{",
            "\t__unlink_va(va, root, true);",
            "}",
            "static __always_inline unsigned long",
            "compute_subtree_max_size(struct vmap_area *va)",
            "{",
            "\treturn max3(va_size(va),",
            "\t\tget_subtree_max_size(va->rb_node.rb_left),",
            "\t\tget_subtree_max_size(va->rb_node.rb_right));",
            "}",
            "static void",
            "augment_tree_propagate_check(void)",
            "{",
            "\tstruct vmap_area *va;",
            "\tunsigned long computed_size;",
            "",
            "\tlist_for_each_entry(va, &free_vmap_area_list, list) {",
            "\t\tcomputed_size = compute_subtree_max_size(va);",
            "\t\tif (computed_size != va->subtree_max_size)",
            "\t\t\tpr_emerg(\"tree is corrupted: %lu, %lu\\n\",",
            "\t\t\t\tva_size(va), va->subtree_max_size);",
            "\t}",
            "}",
            "static __always_inline void",
            "augment_tree_propagate_from(struct vmap_area *va)",
            "{",
            "\t/*",
            "\t * Populate the tree from bottom towards the root until",
            "\t * the calculated maximum available size of checked node",
            "\t * is equal to its current one.",
            "\t */",
            "\tfree_vmap_area_rb_augment_cb_propagate(&va->rb_node, NULL);",
            "",
            "#if DEBUG_AUGMENT_PROPAGATE_CHECK",
            "\taugment_tree_propagate_check();",
            "#endif",
            "}",
            "static void",
            "insert_vmap_area(struct vmap_area *va,",
            "\tstruct rb_root *root, struct list_head *head)",
            "{",
            "\tstruct rb_node **link;",
            "\tstruct rb_node *parent;",
            "",
            "\tlink = find_va_links(va, root, NULL, &parent);",
            "\tif (link)",
            "\t\tlink_va(va, root, parent, link, head);",
            "}",
            "static void",
            "insert_vmap_area_augment(struct vmap_area *va,",
            "\tstruct rb_node *from, struct rb_root *root,",
            "\tstruct list_head *head)",
            "{",
            "\tstruct rb_node **link;",
            "\tstruct rb_node *parent;",
            "",
            "\tif (from)",
            "\t\tlink = find_va_links(va, NULL, from, &parent);",
            "\telse",
            "\t\tlink = find_va_links(va, root, NULL, &parent);",
            "",
            "\tif (link) {",
            "\t\tlink_va_augment(va, root, parent, link, head);",
            "\t\taugment_tree_propagate_from(va);",
            "\t}",
            "}"
          ],
          "function_name": "link_va, link_va_augment, __unlink_va, unlink_va, unlink_va_augment, compute_subtree_max_size, augment_tree_propagate_check, augment_tree_propagate_from, insert_vmap_area, insert_vmap_area_augment",
          "description": "实现基于红黑树的虚拟内存区域动态管理，包含节点插入/删除、子树最大尺寸计算及传播机制，确保树结构平衡性与查询效率",
          "similarity": 0.6649792194366455
        },
        {
          "chunk_id": 4,
          "file_path": "mm/vmalloc.c",
          "start_line": 434,
          "end_line": 537,
          "content": [
            "void __vunmap_range_noflush(unsigned long start, unsigned long end)",
            "{",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tunsigned long addr = start;",
            "\tpgtbl_mod_mask mask = 0;",
            "",
            "\tBUG_ON(addr >= end);",
            "\tpgd = pgd_offset_k(addr);",
            "\tdo {",
            "\t\tnext = pgd_addr_end(addr, end);",
            "\t\tif (pgd_bad(*pgd))",
            "\t\t\tmask |= PGTBL_PGD_MODIFIED;",
            "\t\tif (pgd_none_or_clear_bad(pgd))",
            "\t\t\tcontinue;",
            "\t\tvunmap_p4d_range(pgd, addr, next, &mask);",
            "\t} while (pgd++, addr = next, addr != end);",
            "",
            "\tif (mask & ARCH_PAGE_TABLE_SYNC_MASK)",
            "\t\tarch_sync_kernel_mappings(start, end);",
            "}",
            "void vunmap_range_noflush(unsigned long start, unsigned long end)",
            "{",
            "\tkmsan_vunmap_range_noflush(start, end);",
            "\t__vunmap_range_noflush(start, end);",
            "}",
            "void vunmap_range(unsigned long addr, unsigned long end)",
            "{",
            "\tflush_cache_vunmap(addr, end);",
            "\tvunmap_range_noflush(addr, end);",
            "\tflush_tlb_kernel_range(addr, end);",
            "}",
            "static int vmap_pages_pte_range(pmd_t *pmd, unsigned long addr,",
            "\t\tunsigned long end, pgprot_t prot, struct page **pages, int *nr,",
            "\t\tpgtbl_mod_mask *mask)",
            "{",
            "\tint err = 0;",
            "\tpte_t *pte;",
            "",
            "\t/*",
            "\t * nr is a running index into the array which helps higher level",
            "\t * callers keep track of where we're up to.",
            "\t */",
            "",
            "\tpte = pte_alloc_kernel_track(pmd, addr, mask);",
            "\tif (!pte)",
            "\t\treturn -ENOMEM;",
            "\tdo {",
            "\t\tstruct page *page = pages[*nr];",
            "",
            "\t\tif (WARN_ON(!pte_none(ptep_get(pte)))) {",
            "\t\t\terr = -EBUSY;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tif (WARN_ON(!page)) {",
            "\t\t\terr = -ENOMEM;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tif (WARN_ON(!pfn_valid(page_to_pfn(page)))) {",
            "\t\t\terr = -EINVAL;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tset_pte_at(&init_mm, addr, pte, mk_pte(page, prot));",
            "\t\t(*nr)++;",
            "\t} while (pte++, addr += PAGE_SIZE, addr != end);",
            "\t*mask |= PGTBL_PTE_MODIFIED;",
            "",
            "\treturn err;",
            "}",
            "static int vmap_pages_pmd_range(pud_t *pud, unsigned long addr,",
            "\t\tunsigned long end, pgprot_t prot, struct page **pages, int *nr,",
            "\t\tpgtbl_mod_mask *mask)",
            "{",
            "\tpmd_t *pmd;",
            "\tunsigned long next;",
            "",
            "\tpmd = pmd_alloc_track(&init_mm, pud, addr, mask);",
            "\tif (!pmd)",
            "\t\treturn -ENOMEM;",
            "\tdo {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "\t\tif (vmap_pages_pte_range(pmd, addr, next, prot, pages, nr, mask))",
            "\t\t\treturn -ENOMEM;",
            "\t} while (pmd++, addr = next, addr != end);",
            "\treturn 0;",
            "}",
            "static int vmap_pages_pud_range(p4d_t *p4d, unsigned long addr,",
            "\t\tunsigned long end, pgprot_t prot, struct page **pages, int *nr,",
            "\t\tpgtbl_mod_mask *mask)",
            "{",
            "\tpud_t *pud;",
            "\tunsigned long next;",
            "",
            "\tpud = pud_alloc_track(&init_mm, p4d, addr, mask);",
            "\tif (!pud)",
            "\t\treturn -ENOMEM;",
            "\tdo {",
            "\t\tnext = pud_addr_end(addr, end);",
            "\t\tif (vmap_pages_pmd_range(pud, addr, next, prot, pages, nr, mask))",
            "\t\t\treturn -ENOMEM;",
            "\t} while (pud++, addr = next, addr != end);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "__vunmap_range_noflush, vunmap_range_noflush, vunmap_range, vmap_pages_pte_range, vmap_pages_pmd_range, vmap_pages_pud_range",
          "description": "提供了基于页面对象数组的虚拟内存映射实现，包含页表项填充与清除逻辑，支持通过struct page直接映射物理页面并维护页表修改标记。",
          "similarity": 0.6520783305168152
        }
      ]
    }
  ]
}