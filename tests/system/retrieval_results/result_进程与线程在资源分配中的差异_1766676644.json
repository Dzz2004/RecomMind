{
  "query": "进程与线程在资源分配中的差异",
  "timestamp": "2025-12-25 23:30:44",
  "retrieved_files": [
    {
      "source_file": "kernel/irq/matrix.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:03:08\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\matrix.c`\n\n---\n\n# `irq/matrix.c` 技术文档\n\n## 1. 文件概述\n\n`irq/matrix.c` 实现了一个通用的中断位图（IRQ matrix）管理机制，用于在多 CPU 系统中高效地分配和管理中断向量（或中断位）。该机制支持两类中断分配：\n\n- **普通分配（allocated）**：由设备驱动等动态申请的中断。\n- **托管分配（managed）**：由内核子系统（如 MSI/MSI-X）预先保留、按需激活的中断。\n\n该文件通过 per-CPU 的位图结构，结合全局状态跟踪，实现了跨 CPU 的中断资源分配、预留、释放和在线/离线管理，特别适用于中断向量数量有限（如 x86 的 256 个向量）的架构。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct cpumap`**：每个 CPU 的本地中断位图状态\n  - `available`：当前 CPU 可用的中断数量\n  - `allocated`：已分配的普通中断数量\n  - `managed` / `managed_allocated`：预留和已激活的托管中断数量\n  - `alloc_map[]`：记录已分配的普通中断位\n  - `managed_map[]`：记录预留的托管中断位\n  - `initialized` / `online`：CPU 初始化和在线状态\n\n- **`struct irq_matrix`**：全局中断矩阵控制结构\n  - `matrix_bits`：总位图大小（≤ `IRQ_MATRIX_BITS`）\n  - `alloc_start` / `alloc_end`：可分配范围\n  - `global_available`：全局可用中断总数\n  - `system_map[]`：系统保留位（如 APIC 自身使用的向量）\n  - `maps`：指向 per-CPU `cpumap` 的指针\n  - `scratch_map[]`：临时位图，用于分配时的合并计算\n\n### 主要函数\n\n| 函数 | 功能 |\n|------|------|\n| `irq_alloc_matrix()` | 分配并初始化一个 `irq_matrix` 结构 |\n| `irq_matrix_online()` / `irq_matrix_offline()` | 将本地 CPU 的中断矩阵置为在线/离线状态 |\n| `irq_matrix_assign_system()` | 在矩阵中保留系统级中断位（如 APIC 向量） |\n| `irq_matrix_reserve_managed()` | 在指定 CPU 掩码上为托管中断预留位 |\n| `irq_matrix_remove_managed()` | 移除托管中断的预留位 |\n| `irq_matrix_alloc_managed()` | 从预留的托管中断中分配一个实际使用的中断 |\n| `matrix_alloc_area()` | 内部辅助函数：在合并位图中查找连续空闲区域 |\n| `matrix_find_best_cpu()` / `matrix_find_best_cpu_managed()` | 选择最优 CPU（基于可用数或托管分配数最少） |\n\n## 3. 关键实现\n\n### 位图合并分配策略\n- 在分配中断时，`matrix_alloc_area()` 会临时合并三个位图：\n  1. 当前 CPU 的 `managed_map`（托管预留）\n  2. 全局 `system_map`（系统保留）\n  3. 当前 CPU 的 `alloc_map`（已分配）\n- 使用 `bitmap_find_next_zero_area()` 在合并后的位图中查找连续空闲区域，确保不会重复分配。\n\n### 托管中断（Managed IRQ）机制\n- **两阶段分配**：\n  1. **预留（reserve）**：调用 `irq_matrix_reserve_managed()` 在多个 CPU 上各预留一个位（不一定对齐）。\n  2. **激活（alloc）**：调用 `irq_matrix_alloc_managed()` 从预留位中选择一个未使用的位进行实际分配。\n- **动态 CPU 选择**：`matrix_find_best_cpu_managed()` 优先选择 `managed_allocated` 最少的 CPU，实现负载均衡。\n\n### 系统中断保留\n- `irq_matrix_assign_system()` 用于保留如 x86 的 `IRQ0_VECTOR`（时钟中断）等关键系统向量。\n- 通过 `BUG_ON()` 强制保证：系统中断只能在单 CPU 初始化阶段分配，防止运行时冲突。\n\n### 在线/离线管理\n- CPU 上线时，将其 `available` 计数加入 `global_available`。\n- CPU 离线时，从全局计数中减去，但保留其位图数据（支持重新上线）。\n\n### 跟踪与调试\n- 集成 `trace/events/irq_matrix.h`，提供分配、预留、系统保留等关键操作的 tracepoint，便于调试中断分配问题。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/bitmap.h>`：位图操作（`bitmap_set`, `bitmap_find_next_zero_area` 等）\n  - `<linux/percpu.h>`：Per-CPU 变量支持\n  - `<linux/cpu.h>`：CPU 在线/离线状态\n  - `<linux/irq.h>`：中断子系统基础定义\n  - `<trace/events/irq_matrix.h>`：自定义 tracepoint\n\n- **内核子系统**：\n  - **中断子系统**：作为底层分配器，被 `irqdomain`、MSI/MSI-X 驱动等使用。\n  - **x86 APIC 驱动**：典型使用者，用于管理 256 个中断向量的分配（如 `kernel/irq/vector.c`）。\n\n## 5. 使用场景\n\n- **x86 中断向量管理**：在 `CONFIG_X86_IO_APIC` 或 `CONFIG_X86_LOCAL_APIC` 下，用于分配 IRQ 向量（0-255），区分系统向量、普通设备中断和 MSI 中断。\n- **MSI/MSI-X 中断分配**：PCIe 设备的 MSI 中断通过托管机制预留和分配，确保每个设备在多个 CPU 上有可用向量。\n- **CPU 热插拔**：支持 CPU 动态上线/下线时的中断资源重新平衡。\n- **中断负载均衡**：通过 `matrix_find_best_cpu*` 函数，在多 CPU 间均匀分配中断，避免单 CPU 向量耗尽。",
      "similarity": 0.559389054775238,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 78,
          "end_line": 205,
          "content": [
            "void irq_matrix_online(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tBUG_ON(cm->online);",
            "",
            "\tif (!cm->initialized) {",
            "\t\tcm->available = m->alloc_size;",
            "\t\tcm->available -= cm->managed + m->systembits_inalloc;",
            "\t\tcm->initialized = true;",
            "\t}",
            "\tm->global_available += cm->available;",
            "\tcm->online = true;",
            "\tm->online_maps++;",
            "\ttrace_irq_matrix_online(m);",
            "}",
            "void irq_matrix_offline(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\t/* Update the global available size */",
            "\tm->global_available -= cm->available;",
            "\tcm->online = false;",
            "\tm->online_maps--;",
            "\ttrace_irq_matrix_offline(m);",
            "}",
            "static unsigned int matrix_alloc_area(struct irq_matrix *m, struct cpumap *cm,",
            "\t\t\t\t      unsigned int num, bool managed)",
            "{",
            "\tunsigned int area, start = m->alloc_start;",
            "\tunsigned int end = m->alloc_end;",
            "",
            "\tbitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);",
            "\tbitmap_or(m->scratch_map, m->scratch_map, cm->alloc_map, end);",
            "\tarea = bitmap_find_next_zero_area(m->scratch_map, end, start, num, 0);",
            "\tif (area >= end)",
            "\t\treturn area;",
            "\tif (managed)",
            "\t\tbitmap_set(cm->managed_map, area, num);",
            "\telse",
            "\t\tbitmap_set(cm->alloc_map, area, num);",
            "\treturn area;",
            "}",
            "static unsigned int matrix_find_best_cpu(struct irq_matrix *m,",
            "\t\t\t\t\tconst struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, best_cpu, maxavl = 0;",
            "\tstruct cpumap *cm;",
            "",
            "\tbest_cpu = UINT_MAX;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tcm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tif (!cm->online || cm->available <= maxavl)",
            "\t\t\tcontinue;",
            "",
            "\t\tbest_cpu = cpu;",
            "\t\tmaxavl = cm->available;",
            "\t}",
            "\treturn best_cpu;",
            "}",
            "static unsigned int matrix_find_best_cpu_managed(struct irq_matrix *m,",
            "\t\t\t\t\t\tconst struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, best_cpu, allocated = UINT_MAX;",
            "\tstruct cpumap *cm;",
            "",
            "\tbest_cpu = UINT_MAX;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tcm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tif (!cm->online || cm->managed_allocated > allocated)",
            "\t\t\tcontinue;",
            "",
            "\t\tbest_cpu = cpu;",
            "\t\tallocated = cm->managed_allocated;",
            "\t}",
            "\treturn best_cpu;",
            "}",
            "void irq_matrix_assign_system(struct irq_matrix *m, unsigned int bit,",
            "\t\t\t      bool replace)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tBUG_ON(bit > m->matrix_bits);",
            "\tBUG_ON(m->online_maps > 1 || (m->online_maps && !replace));",
            "",
            "\tset_bit(bit, m->system_map);",
            "\tif (replace) {",
            "\t\tBUG_ON(!test_and_clear_bit(bit, cm->alloc_map));",
            "\t\tcm->allocated--;",
            "\t\tm->total_allocated--;",
            "\t}",
            "\tif (bit >= m->alloc_start && bit < m->alloc_end)",
            "\t\tm->systembits_inalloc++;",
            "",
            "\ttrace_irq_matrix_assign_system(bit, m);",
            "}",
            "int irq_matrix_reserve_managed(struct irq_matrix *m, const struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, failed_cpu;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "\t\tunsigned int bit;",
            "",
            "\t\tbit = matrix_alloc_area(m, cm, 1, true);",
            "\t\tif (bit >= m->alloc_end)",
            "\t\t\tgoto cleanup;",
            "\t\tcm->managed++;",
            "\t\tif (cm->online) {",
            "\t\t\tcm->available--;",
            "\t\t\tm->global_available--;",
            "\t\t}",
            "\t\ttrace_irq_matrix_reserve_managed(bit, cpu, m, cm);",
            "\t}",
            "\treturn 0;",
            "cleanup:",
            "\tfailed_cpu = cpu;",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tif (cpu == failed_cpu)",
            "\t\t\tbreak;",
            "\t\tirq_matrix_remove_managed(m, cpumask_of(cpu));",
            "\t}",
            "\treturn -ENOSPC;",
            "}"
          ],
          "function_name": "irq_matrix_online, irq_matrix_offline, matrix_alloc_area, matrix_find_best_cpu, matrix_find_best_cpu_managed, irq_matrix_assign_system, irq_matrix_reserve_managed",
          "description": "实现CPU矩阵的上线/下线操作，通过bitmap操作实现中断位的分配策略，包含寻找最佳CPU的逻辑，支持系统位管理和保留区域的分配与追踪",
          "similarity": 0.571663498878479
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 251,
          "end_line": 365,
          "content": [
            "void irq_matrix_remove_managed(struct irq_matrix *m, const struct cpumask *msk)",
            "{",
            "\tunsigned int cpu;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "\t\tunsigned int bit, end = m->alloc_end;",
            "",
            "\t\tif (WARN_ON_ONCE(!cm->managed))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Get managed bit which are not allocated */",
            "\t\tbitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);",
            "",
            "\t\tbit = find_first_bit(m->scratch_map, end);",
            "\t\tif (WARN_ON_ONCE(bit >= end))",
            "\t\t\tcontinue;",
            "",
            "\t\tclear_bit(bit, cm->managed_map);",
            "",
            "\t\tcm->managed--;",
            "\t\tif (cm->online) {",
            "\t\t\tcm->available++;",
            "\t\t\tm->global_available++;",
            "\t\t}",
            "\t\ttrace_irq_matrix_remove_managed(bit, cpu, m, cm);",
            "\t}",
            "}",
            "int irq_matrix_alloc_managed(struct irq_matrix *m, const struct cpumask *msk,",
            "\t\t\t     unsigned int *mapped_cpu)",
            "{",
            "\tunsigned int bit, cpu, end;",
            "\tstruct cpumap *cm;",
            "",
            "\tif (cpumask_empty(msk))",
            "\t\treturn -EINVAL;",
            "",
            "\tcpu = matrix_find_best_cpu_managed(m, msk);",
            "\tif (cpu == UINT_MAX)",
            "\t\treturn -ENOSPC;",
            "",
            "\tcm = per_cpu_ptr(m->maps, cpu);",
            "\tend = m->alloc_end;",
            "\t/* Get managed bit which are not allocated */",
            "\tbitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);",
            "\tbit = find_first_bit(m->scratch_map, end);",
            "\tif (bit >= end)",
            "\t\treturn -ENOSPC;",
            "\tset_bit(bit, cm->alloc_map);",
            "\tcm->allocated++;",
            "\tcm->managed_allocated++;",
            "\tm->total_allocated++;",
            "\t*mapped_cpu = cpu;",
            "\ttrace_irq_matrix_alloc_managed(bit, cpu, m, cm);",
            "\treturn bit;",
            "}",
            "void irq_matrix_assign(struct irq_matrix *m, unsigned int bit)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tif (WARN_ON_ONCE(bit < m->alloc_start || bit >= m->alloc_end))",
            "\t\treturn;",
            "\tif (WARN_ON_ONCE(test_and_set_bit(bit, cm->alloc_map)))",
            "\t\treturn;",
            "\tcm->allocated++;",
            "\tm->total_allocated++;",
            "\tcm->available--;",
            "\tm->global_available--;",
            "\ttrace_irq_matrix_assign(bit, smp_processor_id(), m, cm);",
            "}",
            "void irq_matrix_reserve(struct irq_matrix *m)",
            "{",
            "\tif (m->global_reserved == m->global_available)",
            "\t\tpr_warn(\"Interrupt reservation exceeds available resources\\n\");",
            "",
            "\tm->global_reserved++;",
            "\ttrace_irq_matrix_reserve(m);",
            "}",
            "void irq_matrix_remove_reserved(struct irq_matrix *m)",
            "{",
            "\tm->global_reserved--;",
            "\ttrace_irq_matrix_remove_reserved(m);",
            "}",
            "int irq_matrix_alloc(struct irq_matrix *m, const struct cpumask *msk,",
            "\t\t     bool reserved, unsigned int *mapped_cpu)",
            "{",
            "\tunsigned int cpu, bit;",
            "\tstruct cpumap *cm;",
            "",
            "\t/*",
            "\t * Not required in theory, but matrix_find_best_cpu() uses",
            "\t * for_each_cpu() which ignores the cpumask on UP .",
            "\t */",
            "\tif (cpumask_empty(msk))",
            "\t\treturn -EINVAL;",
            "",
            "\tcpu = matrix_find_best_cpu(m, msk);",
            "\tif (cpu == UINT_MAX)",
            "\t\treturn -ENOSPC;",
            "",
            "\tcm = per_cpu_ptr(m->maps, cpu);",
            "\tbit = matrix_alloc_area(m, cm, 1, false);",
            "\tif (bit >= m->alloc_end)",
            "\t\treturn -ENOSPC;",
            "\tcm->allocated++;",
            "\tcm->available--;",
            "\tm->total_allocated++;",
            "\tm->global_available--;",
            "\tif (reserved)",
            "\t\tm->global_reserved--;",
            "\t*mapped_cpu = cpu;",
            "\ttrace_irq_matrix_alloc(bit, cpu, m, cm);",
            "\treturn bit;",
            "",
            "}"
          ],
          "function_name": "irq_matrix_remove_managed, irq_matrix_alloc_managed, irq_matrix_assign, irq_matrix_reserve, irq_matrix_remove_reserved, irq_matrix_alloc",
          "description": "实现中断位的分配/回收机制，包含保留中断位的管理、跨CPU的中断分配逻辑，以及根据预留状态进行资源分配的控制流程",
          "similarity": 0.5528470277786255
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 418,
          "end_line": 483,
          "content": [
            "void irq_matrix_free(struct irq_matrix *m, unsigned int cpu,",
            "\t\t     unsigned int bit, bool managed)",
            "{",
            "\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\tif (WARN_ON_ONCE(bit < m->alloc_start || bit >= m->alloc_end))",
            "\t\treturn;",
            "",
            "\tif (WARN_ON_ONCE(!test_and_clear_bit(bit, cm->alloc_map)))",
            "\t\treturn;",
            "",
            "\tcm->allocated--;",
            "\tif(managed)",
            "\t\tcm->managed_allocated--;",
            "",
            "\tif (cm->online)",
            "\t\tm->total_allocated--;",
            "",
            "\tif (!managed) {",
            "\t\tcm->available++;",
            "\t\tif (cm->online)",
            "\t\t\tm->global_available++;",
            "\t}",
            "\ttrace_irq_matrix_free(bit, cpu, m, cm);",
            "}",
            "unsigned int irq_matrix_available(struct irq_matrix *m, bool cpudown)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tif (!cpudown)",
            "\t\treturn m->global_available;",
            "\treturn m->global_available - cm->available;",
            "}",
            "unsigned int irq_matrix_reserved(struct irq_matrix *m)",
            "{",
            "\treturn m->global_reserved;",
            "}",
            "unsigned int irq_matrix_allocated(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\treturn cm->allocated - cm->managed_allocated;",
            "}",
            "void irq_matrix_debug_show(struct seq_file *sf, struct irq_matrix *m, int ind)",
            "{",
            "\tunsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);",
            "\tint cpu;",
            "",
            "\tseq_printf(sf, \"Online bitmaps:   %6u\\n\", m->online_maps);",
            "\tseq_printf(sf, \"Global available: %6u\\n\", m->global_available);",
            "\tseq_printf(sf, \"Global reserved:  %6u\\n\", m->global_reserved);",
            "\tseq_printf(sf, \"Total allocated:  %6u\\n\", m->total_allocated);",
            "\tseq_printf(sf, \"System: %u: %*pbl\\n\", nsys, m->matrix_bits,",
            "\t\t   m->system_map);",
            "\tseq_printf(sf, \"%*s| CPU | avl | man | mac | act | vectors\\n\", ind, \" \");",
            "\tcpus_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tseq_printf(sf, \"%*s %4d  %4u  %4u  %4u %4u  %*pbl\\n\", ind, \" \",",
            "\t\t\t   cpu, cm->available, cm->managed,",
            "\t\t\t   cm->managed_allocated, cm->allocated,",
            "\t\t\t   m->matrix_bits, cm->alloc_map);",
            "\t}",
            "\tcpus_read_unlock();",
            "}"
          ],
          "function_name": "irq_matrix_free, irq_matrix_available, irq_matrix_reserved, irq_matrix_allocated, irq_matrix_debug_show",
          "description": "提供中断资源的释放接口，实现全局和CPU级的资源使用统计查询，包含调试信息展示功能，通过位图操作维护系统中断位的使用状态",
          "similarity": 0.495979368686676
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 1,
          "end_line": 77,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "// Copyright (C) 2017 Thomas Gleixner <tglx@linutronix.de>",
            "",
            "#include <linux/spinlock.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/bitmap.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpu.h>",
            "#include <linux/irq.h>",
            "",
            "#define IRQ_MATRIX_SIZE\t(BITS_TO_LONGS(IRQ_MATRIX_BITS))",
            "",
            "struct cpumap {",
            "\tunsigned int\t\tavailable;",
            "\tunsigned int\t\tallocated;",
            "\tunsigned int\t\tmanaged;",
            "\tunsigned int\t\tmanaged_allocated;",
            "\tbool\t\t\tinitialized;",
            "\tbool\t\t\tonline;",
            "\tunsigned long\t\talloc_map[IRQ_MATRIX_SIZE];",
            "\tunsigned long\t\tmanaged_map[IRQ_MATRIX_SIZE];",
            "};",
            "",
            "struct irq_matrix {",
            "\tunsigned int\t\tmatrix_bits;",
            "\tunsigned int\t\talloc_start;",
            "\tunsigned int\t\talloc_end;",
            "\tunsigned int\t\talloc_size;",
            "\tunsigned int\t\tglobal_available;",
            "\tunsigned int\t\tglobal_reserved;",
            "\tunsigned int\t\tsystembits_inalloc;",
            "\tunsigned int\t\ttotal_allocated;",
            "\tunsigned int\t\tonline_maps;",
            "\tstruct cpumap __percpu\t*maps;",
            "\tunsigned long\t\tscratch_map[IRQ_MATRIX_SIZE];",
            "\tunsigned long\t\tsystem_map[IRQ_MATRIX_SIZE];",
            "};",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/irq_matrix.h>",
            "",
            "/**",
            " * irq_alloc_matrix - Allocate a irq_matrix structure and initialize it",
            " * @matrix_bits:\tNumber of matrix bits must be <= IRQ_MATRIX_BITS",
            " * @alloc_start:\tFrom which bit the allocation search starts",
            " * @alloc_end:\t\tAt which bit the allocation search ends, i.e first",
            " *\t\t\tinvalid bit",
            " */",
            "__init struct irq_matrix *irq_alloc_matrix(unsigned int matrix_bits,",
            "\t\t\t\t\t   unsigned int alloc_start,",
            "\t\t\t\t\t   unsigned int alloc_end)",
            "{",
            "\tstruct irq_matrix *m;",
            "",
            "\tif (matrix_bits > IRQ_MATRIX_BITS)",
            "\t\treturn NULL;",
            "",
            "\tm = kzalloc(sizeof(*m), GFP_KERNEL);",
            "\tif (!m)",
            "\t\treturn NULL;",
            "",
            "\tm->matrix_bits = matrix_bits;",
            "\tm->alloc_start = alloc_start;",
            "\tm->alloc_end = alloc_end;",
            "\tm->alloc_size = alloc_end - alloc_start;",
            "\tm->maps = alloc_percpu(*m->maps);",
            "\tif (!m->maps) {",
            "\t\tkfree(m);",
            "\t\treturn NULL;",
            "\t}",
            "\treturn m;",
            "}",
            "",
            "/**",
            " * irq_matrix_online - Bring the local CPU matrix online",
            " * @m:\t\tMatrix pointer",
            " */"
          ],
          "function_name": null,
          "description": "定义irq_matrix结构体和相关辅助数据结构，提供irq_alloc_matrix函数用于初始化并分配irq_matrix实例，设置矩阵大小、起始结束位置等参数，并分配per-CPU的cpumap数组",
          "similarity": 0.4457840323448181
        }
      ]
    },
    {
      "source_file": "kernel/workqueue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:53:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workqueue.c`\n\n---\n\n# workqueue.c 技术文档\n\n## 1. 文件概述\n\n`workqueue.c` 是 Linux 内核中实现通用异步执行机制的核心文件，提供基于共享工作线程池（worker pool）的延迟任务调度功能。工作项（work items）在进程上下文中执行，支持 CPU 绑定和非绑定两种模式。每个 CPU 默认拥有两个标准工作池（普通优先级和高优先级），同时支持动态创建非绑定工作池以满足不同工作队列的需求。该机制替代了早期的 taskqueue/keventd 实现，具有更高的可扩展性和资源利用率。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct worker_pool`**  \n  工作线程池结构体，管理一组工作线程（workers），包含：\n  - `lock`：保护池状态的自旋锁\n  - `cpu` / `node`：关联的 CPU 和 NUMA 节点（绑定池）\n  - `worklist`：待处理工作项队列\n  - `idle_list` / `busy_hash`：空闲和忙碌工作线程的管理结构\n  - `nr_workers` / `nr_idle`：工作线程数量统计\n  - `attrs`：工作线程属性（如优先级、CPU 亲和性）\n  - `mayday_timer`：紧急情况下的救援请求定时器\n\n- **`struct pool_workqueue`**  \n  工作队列与工作池之间的关联结构，每个工作队列在每个池中都有一个对应的 `pool_workqueue` 实例，用于：\n  - 管理工作项的入队和执行\n  - 实现 `max_active` 限制（控制并发执行数）\n  - 支持 flush 操作（等待所有工作完成）\n  - 统计性能指标（如启动/完成次数、CPU 时间等）\n\n- **`struct worker`**（定义在 `workqueue_internal.h`）  \n  工作线程的运行时上下文，包含状态标志（如 `WORKER_IDLE`, `WORKER_UNBOUND`）、当前执行的工作项等。\n\n### 关键枚举与常量\n\n- **池/工作线程标志**：\n  - `POOL_DISASSOCIATED`：CPU 离线时池进入非绑定状态\n  - `WORKER_UNBOUND`：工作线程可在任意 CPU 上运行\n  - `WORKER_CPU_INTENSIVE`：标记 CPU 密集型任务，影响并发控制\n\n- **配置参数**：\n  - `NR_STD_WORKER_POOLS = 2`：每 CPU 标准池数量（普通 + 高优先级）\n  - `IDLE_WORKER_TIMEOUT = 300 * HZ`：空闲线程保留时间（5 分钟）\n  - `MAYDAY_INITIAL_TIMEOUT`：工作积压时触发救援的延迟（10ms）\n\n- **统计指标**（`pool_workqueue_stats`）：\n  - `PWQ_STAT_STARTED` / `PWQ_STAT_COMPLETED`：工作项执行统计\n  - `PWQ_STAT_MAYDAY` / `PWQ_STAT_RESCUED`：紧急救援事件计数\n\n## 3. 关键实现\n\n### 工作池管理\n- **绑定池（Bound Pool）**：与特定 CPU 关联，工作线程默认绑定到该 CPU。当 CPU 离线时，池进入 `DISASSOCIATED` 状态，工作线程转为非绑定模式。\n- **非绑定池（Unbound Pool）**：动态创建，通过哈希表（`unbound_pool_hash`）按属性（`workqueue_attrs`）去重，支持跨 CPU 调度。\n- **并发控制**：通过 `nr_running` 计数器和 `max_active` 限制，防止工作项过度并发执行。\n\n### 工作线程生命周期\n- **空闲管理**：空闲线程加入 `idle_list`，超时（`IDLE_WORKER_TIMEOUT`）后被回收。\n- **动态伸缩**：当工作积压时，通过 `mayday_timer` 触发新线程创建；若创建失败，向全局救援线程（rescuer）求助。\n- **状态标志**：使用位标志（如 `WORKER_IDLE`, `WORKER_PREP`）高效管理线程状态，避免锁竞争。\n\n### 内存与同步\n- **RCU 保护**：工作池销毁通过 RCU 延迟释放，确保 `get_work_pool()` 等读取路径无锁安全。\n- **锁分层**：\n  - `pool->lock`（自旋锁）：保护池内部状态\n  - `wq_pool_mutex`：全局池管理互斥锁\n  - `wq_pool_attach_mutex`：防止 CPU 绑定状态变更冲突\n\n### 工作项调度\n- **数据指针复用**：`work_struct->data` 的高有效位存储 `pool_workqueue` 指针，低有效位用于标志位（如 `WORK_STRUCT_INACTIVE`）。\n- **优先级支持**：高优先级工作池使用 `HIGHPRI_NICE_LEVEL = MIN_NICE` 提升调度优先级。\n\n## 4. 依赖关系\n\n- **内核子系统**：\n  - **调度器**（`<linux/sched.h>`）：创建工作线程（kworker），管理 CPU 亲和性\n  - **内存管理**（`<linux/slab.h>`）：分配工作池、工作队列等结构\n  - **CPU 热插拔**（`<linux/cpu.h>`）：处理 CPU 上下线时的池绑定状态切换\n  - **RCU**（`<linux/rculist.h>`）：实现无锁读取路径\n  - **定时器**（`<linux/timer.h>`）：实现空闲超时和救援机制\n\n- **内部依赖**：\n  - `workqueue_internal.h`：定义 `struct worker` 等内部结构\n  - `Documentation/core-api/workqueue.rst`：详细设计文档\n\n## 5. 使用场景\n\n- **驱动程序延迟操作**：硬件中断后调度下半部处理（如网络包处理、磁盘 I/O 完成回调）。\n- **内核子系统异步任务**：文件系统元数据更新、内存回收、电源管理状态切换。\n- **高优先级任务**：使用 `WQ_HIGHPRI` 标志创建工作队列，确保关键任务及时执行（如死锁恢复）。\n- **CPU 密集型任务**：标记 `WQ_CPU_INTENSIVE` 避免占用过多并发槽位，提升系统响应性。\n- **NUMA 感知调度**：非绑定工作队列可指定 NUMA 节点，优化内存访问延迟。",
      "similarity": 0.5583642721176147,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "kernel/workqueue.c",
          "start_line": 895,
          "end_line": 1037,
          "content": [
            "static inline void worker_clr_flags(struct worker *worker, unsigned int flags)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "\tunsigned int oflags = worker->flags;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\tworker->flags &= ~flags;",
            "",
            "\t/*",
            "\t * If transitioning out of NOT_RUNNING, increment nr_running.  Note",
            "\t * that the nested NOT_RUNNING is not a noop.  NOT_RUNNING is mask",
            "\t * of multiple flags, not a single flag.",
            "\t */",
            "\tif ((flags & WORKER_NOT_RUNNING) && (oflags & WORKER_NOT_RUNNING))",
            "\t\tif (!(worker->flags & WORKER_NOT_RUNNING))",
            "\t\t\tpool->nr_running++;",
            "}",
            "static void worker_enter_idle(struct worker *worker)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tif (WARN_ON_ONCE(worker->flags & WORKER_IDLE) ||",
            "\t    WARN_ON_ONCE(!list_empty(&worker->entry) &&",
            "\t\t\t (worker->hentry.next || worker->hentry.pprev)))",
            "\t\treturn;",
            "",
            "\t/* can't use worker_set_flags(), also called from create_worker() */",
            "\tworker->flags |= WORKER_IDLE;",
            "\tpool->nr_idle++;",
            "\tworker->last_active = jiffies;",
            "",
            "\t/* idle_list is LIFO */",
            "\tlist_add(&worker->entry, &pool->idle_list);",
            "",
            "\tif (too_many_workers(pool) && !timer_pending(&pool->idle_timer))",
            "\t\tmod_timer(&pool->idle_timer, jiffies + IDLE_WORKER_TIMEOUT);",
            "",
            "\t/* Sanity check nr_running. */",
            "\tWARN_ON_ONCE(pool->nr_workers == pool->nr_idle && pool->nr_running);",
            "}",
            "static void worker_leave_idle(struct worker *worker)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tif (WARN_ON_ONCE(!(worker->flags & WORKER_IDLE)))",
            "\t\treturn;",
            "\tworker_clr_flags(worker, WORKER_IDLE);",
            "\tpool->nr_idle--;",
            "\tlist_del_init(&worker->entry);",
            "}",
            "static void move_linked_works(struct work_struct *work, struct list_head *head,",
            "\t\t\t      struct work_struct **nextp)",
            "{",
            "\tstruct work_struct *n;",
            "",
            "\t/*",
            "\t * Linked worklist will always end before the end of the list,",
            "\t * use NULL for list head.",
            "\t */",
            "\tlist_for_each_entry_safe_from(work, n, NULL, entry) {",
            "\t\tlist_move_tail(&work->entry, head);",
            "\t\tif (!(*work_data_bits(work) & WORK_STRUCT_LINKED))",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\t/*",
            "\t * If we're already inside safe list traversal and have moved",
            "\t * multiple works to the scheduled queue, the next position",
            "\t * needs to be updated.",
            "\t */",
            "\tif (nextp)",
            "\t\t*nextp = n;",
            "}",
            "static bool assign_work(struct work_struct *work, struct worker *worker,",
            "\t\t\tstruct work_struct **nextp)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "\tstruct worker *collision;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\t/*",
            "\t * A single work shouldn't be executed concurrently by multiple workers.",
            "\t * __queue_work() ensures that @work doesn't jump to a different pool",
            "\t * while still running in the previous pool. Here, we should ensure that",
            "\t * @work is not executed concurrently by multiple workers from the same",
            "\t * pool. Check whether anyone is already processing the work. If so,",
            "\t * defer the work to the currently executing one.",
            "\t */",
            "\tcollision = find_worker_executing_work(pool, work);",
            "\tif (unlikely(collision)) {",
            "\t\tmove_linked_works(work, &collision->scheduled, nextp);",
            "\t\treturn false;",
            "\t}",
            "",
            "\tmove_linked_works(work, &worker->scheduled, nextp);",
            "\treturn true;",
            "}",
            "static bool kick_pool(struct worker_pool *pool)",
            "{",
            "\tstruct worker *worker = first_idle_worker(pool);",
            "\tstruct task_struct *p;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\tif (!need_more_worker(pool) || !worker)",
            "\t\treturn false;",
            "",
            "\tp = worker->task;",
            "",
            "#ifdef CONFIG_SMP",
            "\t/*",
            "\t * Idle @worker is about to execute @work and waking up provides an",
            "\t * opportunity to migrate @worker at a lower cost by setting the task's",
            "\t * wake_cpu field. Let's see if we want to move @worker to improve",
            "\t * execution locality.",
            "\t *",
            "\t * We're waking the worker that went idle the latest and there's some",
            "\t * chance that @worker is marked idle but hasn't gone off CPU yet. If",
            "\t * so, setting the wake_cpu won't do anything. As this is a best-effort",
            "\t * optimization and the race window is narrow, let's leave as-is for",
            "\t * now. If this becomes pronounced, we can skip over workers which are",
            "\t * still on cpu when picking an idle worker.",
            "\t *",
            "\t * If @pool has non-strict affinity, @worker might have ended up outside",
            "\t * its affinity scope. Repatriate.",
            "\t */",
            "\tif (!pool->attrs->affn_strict &&",
            "\t    !cpumask_test_cpu(p->wake_cpu, pool->attrs->__pod_cpumask)) {",
            "\t\tstruct work_struct *work = list_first_entry(&pool->worklist,",
            "\t\t\t\t\t\tstruct work_struct, entry);",
            "\t\tint wake_cpu = cpumask_any_and_distribute(pool->attrs->__pod_cpumask,",
            "\t\t\t\t\t\t\t  cpu_online_mask);",
            "\t\tif (wake_cpu < nr_cpu_ids) {",
            "\t\t\tp->wake_cpu = wake_cpu;",
            "\t\t\tget_work_pwq(work)->stats[PWQ_STAT_REPATRIATED]++;",
            "\t\t}",
            "\t}",
            "#endif",
            "\twake_up_process(p);",
            "\treturn true;",
            "}"
          ],
          "function_name": "worker_clr_flags, worker_enter_idle, worker_leave_idle, move_linked_works, assign_work, kick_pool",
          "description": "实现工作者线程空闲状态切换和工作项迁移机制，包含空闲工作者列表管理、链接工作项批量转移功能，以及唤醒工作者线程的调度逻辑，支持跨CPU亲和性迁移优化，确保工作项正确分发到可用工作者线程。",
          "similarity": 0.548772931098938
        },
        {
          "chunk_id": 21,
          "file_path": "kernel/workqueue.c",
          "start_line": 4591,
          "end_line": 4693,
          "content": [
            "static int alloc_and_link_pwqs(struct workqueue_struct *wq)",
            "{",
            "\tbool highpri = wq->flags & WQ_HIGHPRI;",
            "\tint cpu, ret;",
            "",
            "\twq->cpu_pwq = alloc_percpu(struct pool_workqueue *);",
            "\tif (!wq->cpu_pwq)",
            "\t\tgoto enomem;",
            "",
            "\tif (!(wq->flags & WQ_UNBOUND)) {",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tstruct pool_workqueue **pwq_p =",
            "\t\t\t\tper_cpu_ptr(wq->cpu_pwq, cpu);",
            "\t\t\tstruct worker_pool *pool =",
            "\t\t\t\t&(per_cpu_ptr(cpu_worker_pools, cpu)[highpri]);",
            "",
            "\t\t\t*pwq_p = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL,",
            "\t\t\t\t\t\t       pool->node);",
            "\t\t\tif (!*pwq_p)",
            "\t\t\t\tgoto enomem;",
            "",
            "\t\t\tinit_pwq(*pwq_p, wq, pool);",
            "",
            "\t\t\tmutex_lock(&wq->mutex);",
            "\t\t\tlink_pwq(*pwq_p);",
            "\t\t\tmutex_unlock(&wq->mutex);",
            "\t\t}",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tcpus_read_lock();",
            "\tif (wq->flags & __WQ_ORDERED) {",
            "\t\tret = apply_workqueue_attrs(wq, ordered_wq_attrs[highpri]);",
            "\t\t/* there should only be single pwq for ordering guarantee */",
            "\t\tWARN(!ret && (wq->pwqs.next != &wq->dfl_pwq->pwqs_node ||",
            "\t\t\t      wq->pwqs.prev != &wq->dfl_pwq->pwqs_node),",
            "\t\t     \"ordering guarantee broken for workqueue %s\\n\", wq->name);",
            "\t} else {",
            "\t\tret = apply_workqueue_attrs(wq, unbound_std_wq_attrs[highpri]);",
            "\t}",
            "\tcpus_read_unlock();",
            "",
            "\t/* for unbound pwq, flush the pwq_release_worker ensures that the",
            "\t * pwq_release_workfn() completes before calling kfree(wq).",
            "\t */",
            "\tif (ret)",
            "\t\tkthread_flush_worker(pwq_release_worker);",
            "",
            "\treturn ret;",
            "",
            "enomem:",
            "\tif (wq->cpu_pwq) {",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tstruct pool_workqueue *pwq = *per_cpu_ptr(wq->cpu_pwq, cpu);",
            "",
            "\t\t\tif (pwq)",
            "\t\t\t\tkmem_cache_free(pwq_cache, pwq);",
            "\t\t}",
            "\t\tfree_percpu(wq->cpu_pwq);",
            "\t\twq->cpu_pwq = NULL;",
            "\t}",
            "\treturn -ENOMEM;",
            "}",
            "static int wq_clamp_max_active(int max_active, unsigned int flags,",
            "\t\t\t       const char *name)",
            "{",
            "\tif (max_active < 1 || max_active > WQ_MAX_ACTIVE)",
            "\t\tpr_warn(\"workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\\n\",",
            "\t\t\tmax_active, name, 1, WQ_MAX_ACTIVE);",
            "",
            "\treturn clamp_val(max_active, 1, WQ_MAX_ACTIVE);",
            "}",
            "static int init_rescuer(struct workqueue_struct *wq)",
            "{",
            "\tstruct worker *rescuer;",
            "\tint ret;",
            "",
            "\tif (!(wq->flags & WQ_MEM_RECLAIM))",
            "\t\treturn 0;",
            "",
            "\trescuer = alloc_worker(NUMA_NO_NODE);",
            "\tif (!rescuer) {",
            "\t\tpr_err(\"workqueue: Failed to allocate a rescuer for wq \\\"%s\\\"\\n\",",
            "\t\t       wq->name);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\trescuer->rescue_wq = wq;",
            "\trescuer->task = kthread_create(rescuer_thread, rescuer, \"kworker/R-%s\", wq->name);",
            "\tif (IS_ERR(rescuer->task)) {",
            "\t\tret = PTR_ERR(rescuer->task);",
            "\t\tpr_err(\"workqueue: Failed to create a rescuer kthread for wq \\\"%s\\\": %pe\",",
            "\t\t       wq->name, ERR_PTR(ret));",
            "\t\tkfree(rescuer);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\twq->rescuer = rescuer;",
            "\tkthread_bind_mask(rescuer->task, cpu_possible_mask);",
            "\twake_up_process(rescuer->task);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "alloc_and_link_pwqs, wq_clamp_max_active, init_rescuer",
          "description": "包含分配并链接池工作队列的alloc_and_link_pwqs函数，wq_clamp_max_active用于限制最大活跃任务数范围，init_rescuer初始化救援线程以处理内存回收场景下的异常情况。",
          "similarity": 0.5382066369056702
        },
        {
          "chunk_id": 27,
          "file_path": "kernel/workqueue.c",
          "start_line": 5563,
          "end_line": 5664,
          "content": [
            "int workqueue_prepare_cpu(unsigned int cpu)",
            "{",
            "\tstruct worker_pool *pool;",
            "",
            "\tfor_each_cpu_worker_pool(pool, cpu) {",
            "\t\tif (pool->nr_workers)",
            "\t\t\tcontinue;",
            "\t\tif (!create_worker(pool))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}",
            "int workqueue_online_cpu(unsigned int cpu)",
            "{",
            "\tstruct worker_pool *pool;",
            "\tstruct workqueue_struct *wq;",
            "\tint pi;",
            "",
            "\tmutex_lock(&wq_pool_mutex);",
            "",
            "\tfor_each_pool(pool, pi) {",
            "\t\tmutex_lock(&wq_pool_attach_mutex);",
            "",
            "\t\tif (pool->cpu == cpu)",
            "\t\t\trebind_workers(pool);",
            "\t\telse if (pool->cpu < 0)",
            "\t\t\trestore_unbound_workers_cpumask(pool, cpu);",
            "",
            "\t\tmutex_unlock(&wq_pool_attach_mutex);",
            "\t}",
            "",
            "\t/* update pod affinity of unbound workqueues */",
            "\tlist_for_each_entry(wq, &workqueues, list) {",
            "\t\tstruct workqueue_attrs *attrs = wq->unbound_attrs;",
            "",
            "\t\tif (attrs) {",
            "\t\t\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);",
            "\t\t\tint tcpu;",
            "",
            "\t\t\tfor_each_cpu(tcpu, pt->pod_cpus[pt->cpu_pod[cpu]])",
            "\t\t\t\twq_update_pod(wq, tcpu, cpu, true);",
            "\t\t}",
            "\t}",
            "",
            "\tmutex_unlock(&wq_pool_mutex);",
            "\treturn 0;",
            "}",
            "int workqueue_offline_cpu(unsigned int cpu)",
            "{",
            "\tstruct workqueue_struct *wq;",
            "",
            "\t/* unbinding per-cpu workers should happen on the local CPU */",
            "\tif (WARN_ON(cpu != smp_processor_id()))",
            "\t\treturn -1;",
            "",
            "\tunbind_workers(cpu);",
            "",
            "\t/* update pod affinity of unbound workqueues */",
            "\tmutex_lock(&wq_pool_mutex);",
            "\tlist_for_each_entry(wq, &workqueues, list) {",
            "\t\tstruct workqueue_attrs *attrs = wq->unbound_attrs;",
            "",
            "\t\tif (attrs) {",
            "\t\t\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);",
            "\t\t\tint tcpu;",
            "",
            "\t\t\tfor_each_cpu(tcpu, pt->pod_cpus[pt->cpu_pod[cpu]])",
            "\t\t\t\twq_update_pod(wq, tcpu, cpu, false);",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&wq_pool_mutex);",
            "",
            "\treturn 0;",
            "}",
            "static void work_for_cpu_fn(struct work_struct *work)",
            "{",
            "\tstruct work_for_cpu *wfc = container_of(work, struct work_for_cpu, work);",
            "",
            "\twfc->ret = wfc->fn(wfc->arg);",
            "}",
            "long work_on_cpu_key(int cpu, long (*fn)(void *),",
            "\t\t     void *arg, struct lock_class_key *key)",
            "{",
            "\tstruct work_for_cpu wfc = { .fn = fn, .arg = arg };",
            "",
            "\tINIT_WORK_ONSTACK_KEY(&wfc.work, work_for_cpu_fn, key);",
            "\tschedule_work_on(cpu, &wfc.work);",
            "\tflush_work(&wfc.work);",
            "\tdestroy_work_on_stack(&wfc.work);",
            "\treturn wfc.ret;",
            "}",
            "long work_on_cpu_safe_key(int cpu, long (*fn)(void *),",
            "\t\t\t  void *arg, struct lock_class_key *key)",
            "{",
            "\tlong ret = -ENODEV;",
            "",
            "\tcpus_read_lock();",
            "\tif (cpu_online(cpu))",
            "\t\tret = work_on_cpu_key(cpu, fn, arg, key);",
            "\tcpus_read_unlock();",
            "\treturn ret;",
            "}"
          ],
          "function_name": "workqueue_prepare_cpu, workqueue_online_cpu, workqueue_offline_cpu, work_for_cpu_fn, work_on_cpu_key, work_on_cpu_safe_key",
          "description": "处理CPU事件生命周期，准备工作者、更新工作者池状态，并提供跨CPU任务执行接口以保证调度一致性",
          "similarity": 0.5268574953079224
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/workqueue.c",
          "start_line": 2706,
          "end_line": 2914,
          "content": [
            "static void process_scheduled_works(struct worker *worker)",
            "{",
            "\tstruct work_struct *work;",
            "\tbool first = true;",
            "",
            "\twhile ((work = list_first_entry_or_null(&worker->scheduled,",
            "\t\t\t\t\t\tstruct work_struct, entry))) {",
            "\t\tif (first) {",
            "\t\t\tworker->pool->watchdog_ts = jiffies;",
            "\t\t\tfirst = false;",
            "\t\t}",
            "\t\tprocess_one_work(worker, work);",
            "\t}",
            "}",
            "static void set_pf_worker(bool val)",
            "{",
            "\tmutex_lock(&wq_pool_attach_mutex);",
            "\tif (val)",
            "\t\tcurrent->flags |= PF_WQ_WORKER;",
            "\telse",
            "\t\tcurrent->flags &= ~PF_WQ_WORKER;",
            "\tmutex_unlock(&wq_pool_attach_mutex);",
            "}",
            "static int worker_thread(void *__worker)",
            "{",
            "\tstruct worker *worker = __worker;",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\t/* tell the scheduler that this is a workqueue worker */",
            "\tset_pf_worker(true);",
            "woke_up:",
            "\traw_spin_lock_irq(&pool->lock);",
            "",
            "\t/* am I supposed to die? */",
            "\tif (unlikely(worker->flags & WORKER_DIE)) {",
            "\t\traw_spin_unlock_irq(&pool->lock);",
            "\t\tset_pf_worker(false);",
            "",
            "\t\tset_task_comm(worker->task, \"kworker/dying\");",
            "\t\tida_free(&pool->worker_ida, worker->id);",
            "\t\tworker_detach_from_pool(worker);",
            "\t\tWARN_ON_ONCE(!list_empty(&worker->entry));",
            "\t\tkfree(worker);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tworker_leave_idle(worker);",
            "recheck:",
            "\t/* no more worker necessary? */",
            "\tif (!need_more_worker(pool))",
            "\t\tgoto sleep;",
            "",
            "\t/* do we need to manage? */",
            "\tif (unlikely(!may_start_working(pool)) && manage_workers(worker))",
            "\t\tgoto recheck;",
            "",
            "\t/*",
            "\t * ->scheduled list can only be filled while a worker is",
            "\t * preparing to process a work or actually processing it.",
            "\t * Make sure nobody diddled with it while I was sleeping.",
            "\t */",
            "\tWARN_ON_ONCE(!list_empty(&worker->scheduled));",
            "",
            "\t/*",
            "\t * Finish PREP stage.  We're guaranteed to have at least one idle",
            "\t * worker or that someone else has already assumed the manager",
            "\t * role.  This is where @worker starts participating in concurrency",
            "\t * management if applicable and concurrency management is restored",
            "\t * after being rebound.  See rebind_workers() for details.",
            "\t */",
            "\tworker_clr_flags(worker, WORKER_PREP | WORKER_REBOUND);",
            "",
            "\tdo {",
            "\t\tstruct work_struct *work =",
            "\t\t\tlist_first_entry(&pool->worklist,",
            "\t\t\t\t\t struct work_struct, entry);",
            "",
            "\t\tif (assign_work(work, worker, NULL))",
            "\t\t\tprocess_scheduled_works(worker);",
            "\t} while (keep_working(pool));",
            "",
            "\tworker_set_flags(worker, WORKER_PREP);",
            "sleep:",
            "\t/*",
            "\t * pool->lock is held and there's no work to process and no need to",
            "\t * manage, sleep.  Workers are woken up only while holding",
            "\t * pool->lock or from local cpu, so setting the current state",
            "\t * before releasing pool->lock is enough to prevent losing any",
            "\t * event.",
            "\t */",
            "\tworker_enter_idle(worker);",
            "\t__set_current_state(TASK_IDLE);",
            "\traw_spin_unlock_irq(&pool->lock);",
            "\tschedule();",
            "\tgoto woke_up;",
            "}",
            "static int rescuer_thread(void *__rescuer)",
            "{",
            "\tstruct worker *rescuer = __rescuer;",
            "\tstruct workqueue_struct *wq = rescuer->rescue_wq;",
            "\tbool should_stop;",
            "",
            "\tset_user_nice(current, RESCUER_NICE_LEVEL);",
            "",
            "\t/*",
            "\t * Mark rescuer as worker too.  As WORKER_PREP is never cleared, it",
            "\t * doesn't participate in concurrency management.",
            "\t */",
            "\tset_pf_worker(true);",
            "repeat:",
            "\tset_current_state(TASK_IDLE);",
            "",
            "\t/*",
            "\t * By the time the rescuer is requested to stop, the workqueue",
            "\t * shouldn't have any work pending, but @wq->maydays may still have",
            "\t * pwq(s) queued.  This can happen by non-rescuer workers consuming",
            "\t * all the work items before the rescuer got to them.  Go through",
            "\t * @wq->maydays processing before acting on should_stop so that the",
            "\t * list is always empty on exit.",
            "\t */",
            "\tshould_stop = kthread_should_stop();",
            "",
            "\t/* see whether any pwq is asking for help */",
            "\traw_spin_lock_irq(&wq_mayday_lock);",
            "",
            "\twhile (!list_empty(&wq->maydays)) {",
            "\t\tstruct pool_workqueue *pwq = list_first_entry(&wq->maydays,",
            "\t\t\t\t\tstruct pool_workqueue, mayday_node);",
            "\t\tstruct worker_pool *pool = pwq->pool;",
            "\t\tstruct work_struct *work, *n;",
            "",
            "\t\t__set_current_state(TASK_RUNNING);",
            "\t\tlist_del_init(&pwq->mayday_node);",
            "",
            "\t\traw_spin_unlock_irq(&wq_mayday_lock);",
            "",
            "\t\tworker_attach_to_pool(rescuer, pool);",
            "",
            "\t\traw_spin_lock_irq(&pool->lock);",
            "",
            "\t\t/*",
            "\t\t * Slurp in all works issued via this workqueue and",
            "\t\t * process'em.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(!list_empty(&rescuer->scheduled));",
            "\t\tlist_for_each_entry_safe(work, n, &pool->worklist, entry) {",
            "\t\t\tif (get_work_pwq(work) == pwq &&",
            "\t\t\t    assign_work(work, rescuer, &n))",
            "\t\t\t\tpwq->stats[PWQ_STAT_RESCUED]++;",
            "\t\t}",
            "",
            "\t\tif (!list_empty(&rescuer->scheduled)) {",
            "\t\t\tprocess_scheduled_works(rescuer);",
            "",
            "\t\t\t/*",
            "\t\t\t * The above execution of rescued work items could",
            "\t\t\t * have created more to rescue through",
            "\t\t\t * pwq_activate_first_inactive() or chained",
            "\t\t\t * queueing.  Let's put @pwq back on mayday list so",
            "\t\t\t * that such back-to-back work items, which may be",
            "\t\t\t * being used to relieve memory pressure, don't",
            "\t\t\t * incur MAYDAY_INTERVAL delay inbetween.",
            "\t\t\t */",
            "\t\t\tif (pwq->nr_active && need_to_create_worker(pool)) {",
            "\t\t\t\traw_spin_lock(&wq_mayday_lock);",
            "\t\t\t\t/*",
            "\t\t\t\t * Queue iff we aren't racing destruction",
            "\t\t\t\t * and somebody else hasn't queued it already.",
            "\t\t\t\t */",
            "\t\t\t\tif (wq->rescuer && list_empty(&pwq->mayday_node)) {",
            "\t\t\t\t\tget_pwq(pwq);",
            "\t\t\t\t\tlist_add_tail(&pwq->mayday_node, &wq->maydays);",
            "\t\t\t\t}",
            "\t\t\t\traw_spin_unlock(&wq_mayday_lock);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Leave this pool. Notify regular workers; otherwise, we end up",
            "\t\t * with 0 concurrency and stalling the execution.",
            "\t\t */",
            "\t\tkick_pool(pool);",
            "",
            "\t\traw_spin_unlock_irq(&pool->lock);",
            "",
            "\t\tworker_detach_from_pool(rescuer);",
            "",
            "\t\t/*",
            "\t\t * Put the reference grabbed by send_mayday().  @pool might",
            "\t\t * go away any time after it.",
            "\t\t */",
            "\t\tput_pwq_unlocked(pwq);",
            "",
            "\t\traw_spin_lock_irq(&wq_mayday_lock);",
            "\t}",
            "",
            "\traw_spin_unlock_irq(&wq_mayday_lock);",
            "",
            "\tif (should_stop) {",
            "\t\t__set_current_state(TASK_RUNNING);",
            "\t\tset_pf_worker(false);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/* rescuers should never participate in concurrency management */",
            "\tWARN_ON_ONCE(!(rescuer->flags & WORKER_NOT_RUNNING));",
            "\tschedule();",
            "\tgoto repeat;",
            "}"
          ],
          "function_name": "process_scheduled_works, set_pf_worker, worker_thread, rescuer_thread",
          "description": "管理工作者线程主循环与救援线程逻辑，包含工作项调度、睡眠/唤醒机制及跨池协作的并发控制",
          "similarity": 0.5250226259231567
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/workqueue.c",
          "start_line": 692,
          "end_line": 797,
          "content": [
            "static void set_work_pool_and_clear_pending(struct work_struct *work,",
            "\t\t\t\t\t    int pool_id)",
            "{",
            "\t/*",
            "\t * The following wmb is paired with the implied mb in",
            "\t * test_and_set_bit(PENDING) and ensures all updates to @work made",
            "\t * here are visible to and precede any updates by the next PENDING",
            "\t * owner.",
            "\t */",
            "\tsmp_wmb();",
            "\tset_work_data(work, (unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT, 0);",
            "\t/*",
            "\t * The following mb guarantees that previous clear of a PENDING bit",
            "\t * will not be reordered with any speculative LOADS or STORES from",
            "\t * work->current_func, which is executed afterwards.  This possible",
            "\t * reordering can lead to a missed execution on attempt to queue",
            "\t * the same @work.  E.g. consider this case:",
            "\t *",
            "\t *   CPU#0                         CPU#1",
            "\t *   ----------------------------  --------------------------------",
            "\t *",
            "\t * 1  STORE event_indicated",
            "\t * 2  queue_work_on() {",
            "\t * 3    test_and_set_bit(PENDING)",
            "\t * 4 }                             set_..._and_clear_pending() {",
            "\t * 5                                 set_work_data() # clear bit",
            "\t * 6                                 smp_mb()",
            "\t * 7                               work->current_func() {",
            "\t * 8\t\t\t\t      LOAD event_indicated",
            "\t *\t\t\t\t   }",
            "\t *",
            "\t * Without an explicit full barrier speculative LOAD on line 8 can",
            "\t * be executed before CPU#0 does STORE on line 1.  If that happens,",
            "\t * CPU#0 observes the PENDING bit is still set and new execution of",
            "\t * a @work is not queued in a hope, that CPU#1 will eventually",
            "\t * finish the queued @work.  Meanwhile CPU#1 does not see",
            "\t * event_indicated is set, because speculative LOAD was executed",
            "\t * before actual STORE.",
            "\t */",
            "\tsmp_mb();",
            "}",
            "static void clear_work_data(struct work_struct *work)",
            "{",
            "\tsmp_wmb();\t/* see set_work_pool_and_clear_pending() */",
            "\tset_work_data(work, WORK_STRUCT_NO_POOL, 0);",
            "}",
            "static int get_work_pool_id(struct work_struct *work)",
            "{",
            "\tunsigned long data = atomic_long_read(&work->data);",
            "",
            "\tif (data & WORK_STRUCT_PWQ)",
            "\t\treturn work_struct_pwq(data)->pool->id;",
            "",
            "\treturn data >> WORK_OFFQ_POOL_SHIFT;",
            "}",
            "static void mark_work_canceling(struct work_struct *work)",
            "{",
            "\tunsigned long pool_id = get_work_pool_id(work);",
            "",
            "\tpool_id <<= WORK_OFFQ_POOL_SHIFT;",
            "\tset_work_data(work, pool_id | WORK_OFFQ_CANCELING, WORK_STRUCT_PENDING);",
            "}",
            "static bool work_is_canceling(struct work_struct *work)",
            "{",
            "\tunsigned long data = atomic_long_read(&work->data);",
            "",
            "\treturn !(data & WORK_STRUCT_PWQ) && (data & WORK_OFFQ_CANCELING);",
            "}",
            "static bool need_more_worker(struct worker_pool *pool)",
            "{",
            "\treturn !list_empty(&pool->worklist) && !pool->nr_running;",
            "}",
            "static bool may_start_working(struct worker_pool *pool)",
            "{",
            "\treturn pool->nr_idle;",
            "}",
            "static bool keep_working(struct worker_pool *pool)",
            "{",
            "\treturn !list_empty(&pool->worklist) && (pool->nr_running <= 1);",
            "}",
            "static bool need_to_create_worker(struct worker_pool *pool)",
            "{",
            "\treturn need_more_worker(pool) && !may_start_working(pool);",
            "}",
            "static bool too_many_workers(struct worker_pool *pool)",
            "{",
            "\tbool managing = pool->flags & POOL_MANAGER_ACTIVE;",
            "\tint nr_idle = pool->nr_idle + managing; /* manager is considered idle */",
            "\tint nr_busy = pool->nr_workers - nr_idle;",
            "",
            "\treturn nr_idle > 2 && (nr_idle - 2) * MAX_IDLE_WORKERS_RATIO >= nr_busy;",
            "}",
            "static inline void worker_set_flags(struct worker *worker, unsigned int flags)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\t/* If transitioning into NOT_RUNNING, adjust nr_running. */",
            "\tif ((flags & WORKER_NOT_RUNNING) &&",
            "\t    !(worker->flags & WORKER_NOT_RUNNING)) {",
            "\t\tpool->nr_running--;",
            "\t}",
            "",
            "\tworker->flags |= flags;",
            "}"
          ],
          "function_name": "set_work_pool_and_clear_pending, clear_work_data, get_work_pool_id, mark_work_canceling, work_is_canceling, need_more_worker, may_start_working, keep_working, need_to_create_worker, too_many_workers, worker_set_flags",
          "description": "实现工作者线程池状态控制逻辑，包含需要创建新工作者的判断条件、工作者空闲状态管理、工作项冲突检测及任务分配函数，通过锁保护保证池状态一致性，维护工作者线程与待处理工作项的匹配关系。",
          "similarity": 0.5201948285102844
        }
      ]
    },
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.5573455691337585,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/rt.c",
          "start_line": 529,
          "end_line": 633,
          "content": [
            "static void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\tstruct sched_rt_entity *rt_se;",
            "",
            "\tint cpu = cpu_of(rq);",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tif (!rt_se)",
            "\t\t\tenqueue_top_rt_rq(rt_rq);",
            "\t\telse if (!on_rt_rq(rt_se))",
            "\t\t\tenqueue_rt_entity(rt_se, 0);",
            "",
            "\t\tif (rt_rq->highest_prio.curr < curr->prio)",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "}",
            "static void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (!rt_se) {",
            "\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "\t\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);",
            "\t}",
            "\telse if (on_rt_rq(rt_se))",
            "\t\tdequeue_rt_entity(rt_se, 0);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;",
            "}",
            "static int rt_se_boosted(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "\tstruct task_struct *p;",
            "",
            "\tif (rt_rq)",
            "\t\treturn !!rt_rq->rt_nr_boosted;",
            "",
            "\tp = rt_task_of(rt_se);",
            "\treturn p->prio != p->normal_prio;",
            "}",
            "bool sched_rt_bandwidth_account(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\treturn (hrtimer_active(&rt_b->rt_period_timer) ||",
            "\t\trt_rq->rt_time < rt_b->rt_runtime);",
            "}",
            "static void do_balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;",
            "\tint i, weight;",
            "\tu64 rt_period;",
            "",
            "\tweight = cpumask_weight(rd->span);",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\trt_period = ktime_to_ns(rt_b->rt_period);",
            "\tfor_each_cpu(i, rd->span) {",
            "\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\ts64 diff;",
            "",
            "\t\tif (iter == rt_rq)",
            "\t\t\tcontinue;",
            "",
            "\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either all rqs have inf runtime and there's nothing to steal",
            "\t\t * or __disable_runtime() below sets a specific rq to inf to",
            "\t\t * indicate its been disabled and disallow stealing.",
            "\t\t */",
            "\t\tif (iter->rt_runtime == RUNTIME_INF)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * From runqueues with spare time, take 1/n part of their",
            "\t\t * spare time, but no more than our period.",
            "\t\t */",
            "\t\tdiff = iter->rt_runtime - iter->rt_time;",
            "\t\tif (diff > 0) {",
            "\t\t\tdiff = div_u64((u64)diff, weight);",
            "\t\t\tif (rt_rq->rt_runtime + diff > rt_period)",
            "\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;",
            "\t\t\titer->rt_runtime -= diff;",
            "\t\t\trt_rq->rt_runtime += diff;",
            "\t\t\tif (rt_rq->rt_runtime == rt_period) {",
            "\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "next:",
            "\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, rt_se_boosted, sched_rt_bandwidth_account, do_balance_runtime",
          "description": "实现实时任务队列的插入/移除逻辑，跟踪运行时间消耗，通过跨CPU运行时间平衡算法实现带宽公平分配。",
          "similarity": 0.588947057723999
        },
        {
          "chunk_id": 8,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1066,
          "end_line": 1170,
          "content": [
            "static void",
            "inc_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\t/*",
            "\t * Change rq's cpupri only if rt_rq is the top queue.",
            "\t */",
            "\tif (&rq->rt != rt_rq)",
            "\t\treturn;",
            "#endif",
            "\tif (rq->online && prio < prev_prio)",
            "\t\tcpupri_set(&rq->rd->cpupri, rq->cpu, prio);",
            "}",
            "static void",
            "dec_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\t/*",
            "\t * Change rq's cpupri only if rt_rq is the top queue.",
            "\t */",
            "\tif (&rq->rt != rt_rq)",
            "\t\treturn;",
            "#endif",
            "\tif (rq->online && rt_rq->highest_prio.curr != prev_prio)",
            "\t\tcpupri_set(&rq->rd->cpupri, rq->cpu, rt_rq->highest_prio.curr);",
            "}",
            "static inline",
            "void inc_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio) {}",
            "static inline",
            "void dec_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio) {}",
            "static void",
            "inc_rt_prio(struct rt_rq *rt_rq, int prio)",
            "{",
            "\tint prev_prio = rt_rq->highest_prio.curr;",
            "",
            "\tif (prio < prev_prio)",
            "\t\trt_rq->highest_prio.curr = prio;",
            "",
            "\tinc_rt_prio_smp(rt_rq, prio, prev_prio);",
            "}",
            "static void",
            "dec_rt_prio(struct rt_rq *rt_rq, int prio)",
            "{",
            "\tint prev_prio = rt_rq->highest_prio.curr;",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "",
            "\t\tWARN_ON(prio < prev_prio);",
            "",
            "\t\t/*",
            "\t\t * This may have been our highest task, and therefore",
            "\t\t * we may have some recomputation to do",
            "\t\t */",
            "\t\tif (prio == prev_prio) {",
            "\t\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "",
            "\t\t\trt_rq->highest_prio.curr =",
            "\t\t\t\tsched_find_first_bit(array->bitmap);",
            "\t\t}",
            "",
            "\t} else {",
            "\t\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\t}",
            "",
            "\tdec_rt_prio_smp(rt_rq, prio, prev_prio);",
            "}",
            "static inline void inc_rt_prio(struct rt_rq *rt_rq, int prio) {}",
            "static inline void dec_rt_prio(struct rt_rq *rt_rq, int prio) {}",
            "static void",
            "inc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)",
            "{",
            "\tif (rt_se_boosted(rt_se))",
            "\t\trt_rq->rt_nr_boosted++;",
            "",
            "\tif (rt_rq->tg)",
            "\t\tstart_rt_bandwidth(&rt_rq->tg->rt_bandwidth);",
            "}",
            "static void",
            "dec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)",
            "{",
            "\tif (rt_se_boosted(rt_se))",
            "\t\trt_rq->rt_nr_boosted--;",
            "",
            "\tWARN_ON(!rt_rq->rt_nr_running && rt_rq->rt_nr_boosted);",
            "}",
            "static void",
            "inc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)",
            "{",
            "}",
            "static inline",
            "void dec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq) {}",
            "static inline",
            "unsigned int rt_se_nr_running(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *group_rq = group_rt_rq(rt_se);",
            "",
            "\tif (group_rq)",
            "\t\treturn group_rq->rt_nr_running;",
            "\telse",
            "\t\treturn 1;",
            "}"
          ],
          "function_name": "inc_rt_prio_smp, dec_rt_prio_smp, inc_rt_prio_smp, dec_rt_prio_smp, inc_rt_prio, dec_rt_prio, inc_rt_prio, dec_rt_prio, inc_rt_group, dec_rt_group, inc_rt_group, dec_rt_group, rt_se_nr_running",
          "description": "`inc_rt_prio/dec_rt_prio`维护实时队列最高优先级，`inc_rt_group/dec_rt_group`处理调度组的资源计数，`rt_se_nr_running`查询任务所属组的运行数量",
          "similarity": 0.5761231184005737
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/rt.c",
          "start_line": 201,
          "end_line": 311,
          "content": [
            "void free_rt_sched_group(struct task_group *tg)",
            "{",
            "\tint i;",
            "",
            "\tfor_each_possible_cpu(i) {",
            "\t\tif (tg->rt_rq)",
            "\t\t\tkfree(tg->rt_rq[i]);",
            "\t\tif (tg->rt_se)",
            "\t\t\tkfree(tg->rt_se[i]);",
            "\t}",
            "",
            "\tkfree(tg->rt_rq);",
            "\tkfree(tg->rt_se);",
            "}",
            "void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,",
            "\t\tstruct sched_rt_entity *rt_se, int cpu,",
            "\t\tstruct sched_rt_entity *parent)",
            "{",
            "\tstruct rq *rq = cpu_rq(cpu);",
            "",
            "\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\trt_rq->rt_nr_boosted = 0;",
            "\trt_rq->rq = rq;",
            "\trt_rq->tg = tg;",
            "",
            "\ttg->rt_rq[cpu] = rt_rq;",
            "\ttg->rt_se[cpu] = rt_se;",
            "",
            "\tif (!rt_se)",
            "\t\treturn;",
            "",
            "\tif (!parent)",
            "\t\trt_se->rt_rq = &rq->rt;",
            "\telse",
            "\t\trt_se->rt_rq = parent->my_q;",
            "",
            "\trt_se->my_q = rt_rq;",
            "\trt_se->parent = parent;",
            "\tINIT_LIST_HEAD(&rt_se->run_list);",
            "}",
            "int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)",
            "{",
            "\tstruct rt_rq *rt_rq;",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint i;",
            "",
            "\ttg->rt_rq = kcalloc(nr_cpu_ids, sizeof(rt_rq), GFP_KERNEL);",
            "\tif (!tg->rt_rq)",
            "\t\tgoto err;",
            "\ttg->rt_se = kcalloc(nr_cpu_ids, sizeof(rt_se), GFP_KERNEL);",
            "\tif (!tg->rt_se)",
            "\t\tgoto err;",
            "",
            "\tinit_rt_bandwidth(&tg->rt_bandwidth, ktime_to_ns(global_rt_period()), 0);",
            "",
            "\tfor_each_possible_cpu(i) {",
            "\t\trt_rq = kzalloc_node(sizeof(struct rt_rq),",
            "\t\t\t\t     GFP_KERNEL, cpu_to_node(i));",
            "\t\tif (!rt_rq)",
            "\t\t\tgoto err;",
            "",
            "\t\trt_se = kzalloc_node(sizeof(struct sched_rt_entity),",
            "\t\t\t\t     GFP_KERNEL, cpu_to_node(i));",
            "\t\tif (!rt_se)",
            "\t\t\tgoto err_free_rq;",
            "",
            "\t\tinit_rt_rq(rt_rq);",
            "\t\trt_rq->rt_runtime = tg->rt_bandwidth.rt_runtime;",
            "\t\tinit_tg_rt_entry(tg, rt_rq, rt_se, i, parent->rt_se[i]);",
            "\t}",
            "",
            "\treturn 1;",
            "",
            "err_free_rq:",
            "\tkfree(rt_rq);",
            "err:",
            "\treturn 0;",
            "}",
            "void unregister_rt_sched_group(struct task_group *tg) { }",
            "void free_rt_sched_group(struct task_group *tg) { }",
            "int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)",
            "{",
            "\treturn 1;",
            "}",
            "static inline bool need_pull_rt_task(struct rq *rq, struct task_struct *prev)",
            "{",
            "\t/* Try to pull RT tasks here if we lower this rq's prio */",
            "\treturn rq->online && rq->rt.highest_prio.curr > prev->prio;",
            "}",
            "static inline int rt_overloaded(struct rq *rq)",
            "{",
            "\treturn atomic_read(&rq->rd->rto_count);",
            "}",
            "static inline void rt_set_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tcpumask_set_cpu(rq->cpu, rq->rd->rto_mask);",
            "\t/*",
            "\t * Make sure the mask is visible before we set",
            "\t * the overload count. That is checked to determine",
            "\t * if we should look at the mask. It would be a shame",
            "\t * if we looked at the mask, but the mask was not",
            "\t * updated yet.",
            "\t *",
            "\t * Matched by the barrier in pull_rt_task().",
            "\t */",
            "\tsmp_wmb();",
            "\tatomic_inc(&rq->rd->rto_count);",
            "}"
          ],
          "function_name": "free_rt_sched_group, init_tg_rt_entry, alloc_rt_sched_group, unregister_rt_sched_group, free_rt_sched_group, alloc_rt_sched_group, need_pull_rt_task, rt_overloaded, rt_set_overload",
          "description": "分配/释放实时调度组资源，初始化任务组内的运行队列和实体结构，处理实时任务提升（boost）状态的标记与管理。",
          "similarity": 0.5625879764556885
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1449,
          "end_line": 1589,
          "content": [
            "static void dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rq *rq = rq_of_rt_se(rt_se);",
            "",
            "\tupdate_stats_dequeue_rt(rt_rq_of_se(rt_se), rt_se, flags);",
            "",
            "\tdequeue_rt_stack(rt_se, flags);",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\t\tif (rt_rq && rt_rq->rt_nr_running)",
            "\t\t\t__enqueue_rt_entity(rt_se, flags);",
            "\t}",
            "\tenqueue_top_rt_rq(&rq->rt);",
            "}",
            "static void",
            "enqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tif (flags & ENQUEUE_WAKEUP)",
            "\t\trt_se->timeout = 0;",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_rt(rt_rq_of_se(rt_se), rt_se);",
            "",
            "\tenqueue_rt_entity(rt_se, flags);",
            "",
            "\tif (!task_current(rq, p) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_task(rq, p);",
            "}",
            "static bool dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tdequeue_rt_entity(rt_se, flags);",
            "",
            "\tdequeue_pushable_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void",
            "requeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)",
            "{",
            "\tif (on_rt_rq(rt_se)) {",
            "\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "\t\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);",
            "",
            "\t\tif (head)",
            "\t\t\tlist_move(&rt_se->run_list, queue);",
            "\t\telse",
            "\t\t\tlist_move_tail(&rt_se->run_list, queue);",
            "\t}",
            "}",
            "static void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\trt_rq = rt_rq_of_se(rt_se);",
            "\t\trequeue_rt_entity(rt_rq, rt_se, head);",
            "\t}",
            "}",
            "static void yield_task_rt(struct rq *rq)",
            "{",
            "\trequeue_task_rt(rq, rq->curr, 0);",
            "}",
            "static int",
            "select_task_rq_rt(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tstruct rq *rq;",
            "\tbool test;",
            "",
            "\t/* For anything but wake ups, just return the task_cpu */",
            "\tif (!(flags & (WF_TTWU | WF_FORK)))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If the current task on @p's runqueue is an RT task, then",
            "\t * try to see if we can wake this RT task up on another",
            "\t * runqueue. Otherwise simply start this RT task",
            "\t * on its current runqueue.",
            "\t *",
            "\t * We want to avoid overloading runqueues. If the woken",
            "\t * task is a higher priority, then it will stay on this CPU",
            "\t * and the lower prio task should be moved to another CPU.",
            "\t * Even though this will probably make the lower prio task",
            "\t * lose its cache, we do not want to bounce a higher task",
            "\t * around just because it gave up its CPU, perhaps for a",
            "\t * lock?",
            "\t *",
            "\t * For equal prio tasks, we just let the scheduler sort it out.",
            "\t *",
            "\t * Otherwise, just let it ride on the affined RQ and the",
            "\t * post-schedule router will push the preempted task away",
            "\t *",
            "\t * This test is optimistic, if we get it wrong the load-balancer",
            "\t * will have to sort it out.",
            "\t *",
            "\t * We take into account the capacity of the CPU to ensure it fits the",
            "\t * requirement of the task - which is only important on heterogeneous",
            "\t * systems like big.LITTLE.",
            "\t */",
            "\ttest = curr &&",
            "\t       unlikely(rt_task(curr)) &&",
            "\t       (curr->nr_cpus_allowed < 2 || curr->prio <= p->prio);",
            "",
            "\tif (test || !rt_task_fits_capacity(p, cpu)) {",
            "\t\tint target = find_lowest_rq(p);",
            "",
            "\t\t/*",
            "\t\t * Bail out if we were forcing a migration to find a better",
            "\t\t * fitting CPU but our search failed.",
            "\t\t */",
            "\t\tif (!test && target != -1 && !rt_task_fits_capacity(p, target))",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\t/*",
            "\t\t * Don't bother moving it if the destination CPU is",
            "\t\t * not running a lower priority task.",
            "\t\t */",
            "\t\tif (target != -1 &&",
            "\t\t    p->prio < cpu_rq(target)->rt.highest_prio.curr)",
            "\t\t\tcpu = target;",
            "\t}",
            "",
            "out_unlock:",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "dequeue_rt_entity, enqueue_task_rt, dequeue_task_rt, requeue_rt_entity, requeue_task_rt, yield_task_rt, select_task_rq_rt",
          "description": "实现实时任务的出队逻辑、唤醒和迁移策略，提供CPU亲和性选择及负载均衡支持，维护优先级队列的动态调整。",
          "similarity": 0.5457853674888611
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/sched/rt.c",
          "start_line": 934,
          "end_line": 1036,
          "content": [
            "static inline void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "\tif (!rt_rq->rt_nr_running)",
            "\t\treturn;",
            "",
            "\tenqueue_top_rt_rq(rt_rq);",
            "\tresched_curr(rq);",
            "}",
            "static inline void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn false;",
            "}",
            "static void __enable_runtime(struct rq *rq) { }",
            "static void __disable_runtime(struct rq *rq) { }",
            "static inline int rt_se_prio(struct sched_rt_entity *rt_se)",
            "{",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\tif (rt_rq)",
            "\t\treturn rt_rq->highest_prio.curr;",
            "#endif",
            "",
            "\treturn rt_task_of(rt_se)->prio;",
            "}",
            "static void update_curr_rt(struct rq *rq)",
            "{",
            "\tstruct task_struct *curr = rq->curr;",
            "\ts64 delta_exec;",
            "",
            "\tif (curr->sched_class != &rt_sched_class)",
            "\t\treturn;",
            "",
            "\tdelta_exec = update_curr_common(rq);",
            "\tif (unlikely(delta_exec <= 0))",
            "\t\treturn;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\tstruct sched_rt_entity *rt_se = &curr->rt;",
            "",
            "\tif (!rt_bandwidth_enabled())",
            "\t\treturn;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);",
            "\t\tint exceeded;",
            "",
            "\t\tif (sched_rt_runtime(rt_rq) != RUNTIME_INF) {",
            "\t\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t\trt_rq->rt_time += delta_exec;",
            "\t\t\texceeded = sched_rt_runtime_exceeded(rt_rq);",
            "\t\t\tif (exceeded)",
            "\t\t\t\tresched_curr(rq);",
            "\t\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\t\tif (exceeded)",
            "\t\t\t\tdo_start_rt_bandwidth(sched_rt_bandwidth(rt_rq));",
            "\t\t}",
            "\t}",
            "#endif",
            "}",
            "static void",
            "dequeue_top_rt_rq(struct rt_rq *rt_rq, unsigned int count)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "\tBUG_ON(&rq->rt != rt_rq);",
            "",
            "\tif (!rt_rq->rt_queued)",
            "\t\treturn;",
            "",
            "\tBUG_ON(!rq->nr_running);",
            "",
            "\tsub_nr_running(rq, count);",
            "\trt_rq->rt_queued = 0;",
            "",
            "}",
            "static void",
            "enqueue_top_rt_rq(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "\tBUG_ON(&rq->rt != rt_rq);",
            "",
            "\tif (rt_rq->rt_queued)",
            "\t\treturn;",
            "",
            "\tif (rt_rq_throttled(rt_rq))",
            "\t\treturn;",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tadd_nr_running(rq, rt_rq->rt_nr_running);",
            "\t\trt_rq->rt_queued = 1;",
            "\t}",
            "",
            "\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\tcpufreq_update_util(rq, 0);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, __enable_runtime, __disable_runtime, rt_se_prio, update_curr_rt, dequeue_top_rt_rq, enqueue_top_rt_rq",
          "description": "`sched_rt_rq_enqueue/dequeue`管理实时队列的挂载/卸载，`rt_rq_throttled`判断是否节流，`update_curr_rt`更新当前任务的运行时间并检查节流策略",
          "similarity": 0.5401912927627563
        }
      ]
    }
  ]
}