{
  "query": "硬实时系统内存分配策略实现原理",
  "timestamp": "2025-12-26 00:02:21",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.6643315553665161,
      "chunks": [
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.6527016162872314
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mempolicy.c",
          "start_line": 168,
          "end_line": 268,
          "content": [
            "static u8 get_il_weight(int node)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tu8 weight = 1;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state)",
            "\t\tweight = state->iw_table[node];",
            "\trcu_read_unlock();",
            "\treturn weight;",
            "}",
            "static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)",
            "{",
            "\tu64 sum_bw = 0;",
            "\tunsigned int cast_sum_bw, scaling_factor = 1, iw_gcd = 0;",
            "\tint nid;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tsum_bw += bw[nid];",
            "",
            "\t/* Scale bandwidths to whole numbers in the range [1, weightiness] */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t/*",
            "\t\t * Try not to perform 64-bit division.",
            "\t\t * If sum_bw < scaling_factor, then sum_bw < U32_MAX.",
            "\t\t * If sum_bw > scaling_factor, then round the weight up to 1.",
            "\t\t */",
            "\t\tscaling_factor = weightiness * bw[nid];",
            "\t\tif (bw[nid] && sum_bw < scaling_factor) {",
            "\t\t\tcast_sum_bw = (unsigned int)sum_bw;",
            "\t\t\tnew_iw[nid] = scaling_factor / cast_sum_bw;",
            "\t\t} else {",
            "\t\t\tnew_iw[nid] = 1;",
            "\t\t}",
            "\t\tif (!iw_gcd)",
            "\t\t\tiw_gcd = new_iw[nid];",
            "\t\tiw_gcd = gcd(iw_gcd, new_iw[nid]);",
            "\t}",
            "",
            "\t/* 1:2 is strictly better than 16:32. Reduce by the weights' GCD. */",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tnew_iw[nid] /= iw_gcd;",
            "}",
            "int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tunsigned int *old_bw, *new_bw;",
            "\tunsigned int bw_val;",
            "\tint i;",
            "",
            "\tbw_val = min(coords->read_bandwidth, coords->write_bandwidth);",
            "\tnew_bw = kcalloc(nr_node_ids, sizeof(unsigned int), GFP_KERNEL);",
            "\tif (!new_bw)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew_wi_state = kmalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state) {",
            "\t\tkfree(new_bw);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tnew_wi_state->mode_auto = true;",
            "\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\tnew_wi_state->iw_table[i] = 1;",
            "",
            "\t/*",
            "\t * Update bandwidth info, even in manual mode. That way, when switching",
            "\t * to auto mode in the future, iw_table can be overwritten using",
            "\t * accurate bw data.",
            "\t */",
            "\tmutex_lock(&wi_state_lock);",
            "",
            "\told_bw = node_bw_table;",
            "\tif (old_bw)",
            "\t\tmemcpy(new_bw, old_bw, nr_node_ids * sizeof(*old_bw));",
            "\tnew_bw[node] = bw_val;",
            "\tnode_bw_table = new_bw;",
            "",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state && !old_wi_state->mode_auto) {",
            "\t\t/* Manual mode; skip reducing weights and updating wi_state */",
            "\t\tmutex_unlock(&wi_state_lock);",
            "\t\tkfree(new_wi_state);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* NULL wi_state assumes auto=true; reduce weights and update wi_state*/",
            "\treduce_interleave_weights(new_bw, new_wi_state->iw_table);",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "out:",
            "\tkfree(old_bw);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_il_weight, reduce_interleave_weights, mempolicy_set_node_perf",
          "description": "实现带权交错策略的权重计算与调整逻辑，通过获取节点带宽数据动态修改权重比例，支持根据性能参数更新节点间内存分配优先级。",
          "similarity": 0.6435925960540771
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.6170463562011719
        },
        {
          "chunk_id": 13,
          "file_path": "mm/mempolicy.c",
          "start_line": 2149,
          "end_line": 2255,
          "content": [
            "static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nnodes;",
            "\tint i;",
            "\tint nid;",
            "",
            "\tnnodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nnodes)",
            "\t\treturn numa_node_id();",
            "\ttarget = ilx % nnodes;",
            "\tnid = first_node(nodemask);",
            "\tfor (i = 0; i < target; i++)",
            "\t\tnid = next_node(nid, nodemask);",
            "\treturn nid;",
            "}",
            "int huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,",
            "\t\tstruct mempolicy **mpol, nodemask_t **nodemask)",
            "{",
            "\tpgoff_t ilx;",
            "\tint nid;",
            "",
            "\tnid = numa_node_id();",
            "\t*mpol = get_vma_policy(vma, addr, hstate_vma(vma)->order, &ilx);",
            "\t*nodemask = policy_nodemask(gfp_flags, *mpol, ilx, &nid);",
            "\treturn nid;",
            "}",
            "bool init_nodemask_of_mempolicy(nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "",
            "\tif (!(mask && current->mempolicy))",
            "\t\treturn false;",
            "",
            "\ttask_lock(current);",
            "\tmempolicy = current->mempolicy;",
            "\tswitch (mempolicy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*mask = mempolicy->nodes;",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tinit_nodemask_of_node(mask, numa_node_id());",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "\ttask_unlock(current);",
            "",
            "\treturn true;",
            "}",
            "bool mempolicy_in_oom_domain(struct task_struct *tsk,",
            "\t\t\t\t\tconst nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "\tbool ret = true;",
            "",
            "\tif (!mask)",
            "\t\treturn ret;",
            "",
            "\ttask_lock(tsk);",
            "\tmempolicy = tsk->mempolicy;",
            "\tif (mempolicy && mempolicy->mode == MPOL_BIND)",
            "\t\tret = nodes_intersects(mempolicy->nodes, *mask);",
            "\ttask_unlock(tsk);",
            "",
            "\treturn ret;",
            "}",
            "static unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tint nodes;",
            "\tunsigned long nr_pages_per_node;",
            "\tint delta;",
            "\tint i;",
            "\tunsigned long nr_allocated;",
            "\tunsigned long total_allocated = 0;",
            "",
            "\tnodes = nodes_weight(pol->nodes);",
            "\tnr_pages_per_node = nr_pages / nodes;",
            "\tdelta = nr_pages - nodes * nr_pages_per_node;",
            "",
            "\tfor (i = 0; i < nodes; i++) {",
            "\t\tif (delta) {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node + 1, NULL,",
            "\t\t\t\t\tpage_array);",
            "\t\t\tdelta--;",
            "\t\t} else {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node, NULL, page_array);",
            "\t\t}",
            "",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t}",
            "",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "interleave_nid, huge_node, init_nodemask_of_mempolicy, mempolicy_in_oom_domain, alloc_pages_bulk_array_interleave",
          "description": "interleave_nid 计算简单交错分配的目标节点；huge_node 结合HugeTLB策略确定大页分配节点；init_nodemask_of_mempolicy 初始化当前进程的内存策略节点掩码；mempolicy_in_oom_domain 检查策略节点是否与OOM域重叠；alloc_pages_bulk_array_interleave 执行批量交错分配。",
          "similarity": 0.6150586009025574
        },
        {
          "chunk_id": 12,
          "file_path": "mm/mempolicy.c",
          "start_line": 2024,
          "end_line": 2135,
          "content": [
            "static unsigned int interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int nid;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "\t/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnid = next_node_in(current->il_prev, policy->nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\tif (nid < MAX_NUMNODES)",
            "\t\tcurrent->il_prev = nid;",
            "\treturn nid;",
            "}",
            "unsigned int mempolicy_slab_node(void)",
            "{",
            "\tstruct mempolicy *policy;",
            "\tint node = numa_mem_id();",
            "",
            "\tif (!in_task())",
            "\t\treturn node;",
            "",
            "\tpolicy = current->mempolicy;",
            "\tif (!policy)",
            "\t\treturn node;",
            "",
            "\tswitch (policy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\treturn first_node(policy->nodes);",
            "",
            "\tcase MPOL_INTERLEAVE:",
            "\t\treturn interleave_nodes(policy);",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn weighted_interleave_nodes(policy);",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t{",
            "\t\tstruct zoneref *z;",
            "",
            "\t\t/*",
            "\t\t * Follow bind policy behavior and start allocation at the",
            "\t\t * first node.",
            "\t\t */",
            "\t\tstruct zonelist *zonelist;",
            "\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);",
            "\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];",
            "\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,",
            "\t\t\t\t\t\t\t&policy->nodes);",
            "\t\treturn z->zone ? zone_to_nid(z->zone) : node;",
            "\t}",
            "\tcase MPOL_LOCAL:",
            "\t\treturn node;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static unsigned int read_once_policy_nodemask(struct mempolicy *pol,",
            "\t\t\t\t\t      nodemask_t *mask)",
            "{",
            "\t/*",
            "\t * barrier stabilizes the nodemask locally so that it can be iterated",
            "\t * over safely without concern for changes. Allocators validate node",
            "\t * selection does not violate mems_allowed, so this is safe.",
            "\t */",
            "\tbarrier();",
            "\tmemcpy(mask, &pol->nodes, sizeof(nodemask_t));",
            "\tbarrier();",
            "\treturn nodes_weight(*mask);",
            "}",
            "static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nr_nodes;",
            "\tu8 *table = NULL;",
            "\tunsigned int weight_total = 0;",
            "\tu8 weight;",
            "\tint nid = 0;",
            "",
            "\tnr_nodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nr_nodes)",
            "\t\treturn numa_node_id();",
            "",
            "\trcu_read_lock();",
            "",
            "\tstate = rcu_dereference(wi_state);",
            "\t/* Uninitialized wi_state means we should assume all weights are 1 */",
            "\tif (state)",
            "\t\ttable = state->iw_table;",
            "",
            "\t/* calculate the total weight */",
            "\tfor_each_node_mask(nid, nodemask)",
            "\t\tweight_total += table ? table[nid] : 1;",
            "",
            "\t/* Calculate the node offset based on totals */",
            "\ttarget = ilx % weight_total;",
            "\tnid = first_node(nodemask);",
            "\twhile (target) {",
            "\t\t/* detect system default usage */",
            "\t\tweight = table ? table[nid] : 1;",
            "\t\tif (target < weight)",
            "\t\t\tbreak;",
            "\t\ttarget -= weight;",
            "\t\tnid = next_node_in(nid, nodemask);",
            "\t}",
            "\trcu_read_unlock();",
            "\treturn nid;",
            "}"
          ],
          "function_name": "interleave_nodes, mempolicy_slab_node, read_once_policy_nodemask, weighted_interleave_nid",
          "description": "interleave_nodes 计算交错分配的下一个节点；mempolicy_slab_node 根据内存策略返回Slab分配的节点；read_once_policy_nodemask 安全读取策略节点掩码；weighted_interleave_nid 基于权重计算加权交错分配的目标节点。",
          "similarity": 0.6126917600631714
        }
      ]
    },
    {
      "source_file": "kernel/sched/fair.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:09:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\fair.c`\n\n---\n\n# `sched/fair.c` 技术文档\n\n## 1. 文件概述\n\n`sched/fair.c` 是 Linux 内核中 **完全公平调度器**（Completely Fair Scheduler, CFS）的核心实现文件，负责实现 `SCHED_NORMAL` 和 `SCHED_BATCH` 调度策略。CFS 旨在通过红黑树（RB-tree）维护可运行任务的虚拟运行时间（vruntime），以实现 CPU 时间的公平分配。该文件实现了任务调度、负载跟踪、时间片计算、组调度（group scheduling）、NUMA 负载均衡、带宽控制等关键机制，是 Linux 通用调度子系统的核心组成部分。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_entity`：调度实体，代表一个可调度单元（任务或任务组）\n- `struct cfs_rq`：CFS 运行队列，管理一组调度实体\n- `struct load_weight`：负载权重结构，用于计算任务对系统负载的贡献\n\n### 关键函数与宏\n- `__calc_delta()` / `calc_delta_fair()`：计算基于权重的调度时间增量\n- `update_load_add()` / `update_load_sub()` / `update_load_set()`：更新负载权重\n- `__update_inv_weight()`：预计算权重的倒数以优化除法运算\n- `get_update_sysctl_factor()`：根据在线 CPU 数量动态调整调度参数\n- `update_sysctl()` / `sched_init_granularity()`：初始化和更新调度粒度参数\n- `for_each_sched_entity()`：遍历调度实体层级结构（用于组调度）\n\n### 可调参数（sysctl）\n- `sysctl_sched_base_slice`：基础时间片（默认 700,000 纳秒）\n- `sysctl_sched_tunable_scaling`：调度参数缩放策略（NONE/LOG/LINEAR）\n- `sysctl_sched_migration_cost`：任务迁移成本阈值（500 微秒）\n- `sysctl_sched_cfs_bandwidth_slice_us`（CFS 带宽控制切片，默认 5 毫秒）\n- `sysctl_numa_balancing_promote_rate_limit_MBps`（NUMA 页迁移速率限制）\n\n## 3. 关键实现\n\n### 虚拟时间与公平性\nCFS 使用 **虚拟运行时间**（vruntime）衡量任务已使用的 CPU 时间，并通过 `calc_delta_fair()` 将实际执行时间按任务权重归一化。权重由任务的 nice 值决定（`NICE_0_LOAD = 1024` 为基准）。调度器总是选择 vruntime 最小的任务运行，确保高优先级（高权重）任务获得更多 CPU 时间。\n\n### 高效除法优化\n为避免频繁除法运算，CFS 预计算 `inv_weight = WMULT_CONST / weight`（`WMULT_CONST = ~0U`），将除法转换为乘法和右移操作（`mul_u64_u32_shr`）。`__calc_delta()` 通过动态调整移位位数（`shift`）保证计算精度，适用于 32/64 位架构。\n\n### 动态粒度调整\n基础时间片 `sched_base_slice` 根据在线 CPU 数量动态缩放：\n- `SCHED_TUNABLESCALING_NONE`：固定值\n- `SCHED_TUNABLESCALING_LINEAR`：线性缩放（×ncpus）\n- `SCHED_TUNABLESCALING_LOG`（默认）：对数缩放（×(1 + ilog2(ncpus))）  \n此设计确保在多核系统中保持合理的调度延迟和交互性。\n\n### 组调度支持\n通过 `for_each_sched_entity()` 宏遍历任务所属的调度实体层级（任务 → 任务组 → 父任务组），实现 CPU 带宽在任务组间的公平分配。每个 `cfs_rq` 独立维护其子实体的红黑树。\n\n### SMP 相关优化\n- **非对称 CPU 优先级**：`arch_asym_cpu_priority()` 允许架构定义 CPU 能力差异（如大小核）\n- **容量比较宏**：`fits_capacity()`（20% 容差）和 `capacity_greater()`（5% 容差）用于负载均衡决策\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- 调度核心：`\"sched.h\"`、`\"stats.h\"`、`\"autogroup.h\"`\n- 系统服务：`<linux/sched/clock.h>`、`<linux/sched/nohz.h>`、`<linux/psi.h>`\n- 内存管理：`<linux/mem_policy.h>`、`<linux/energy_model.h>`\n- SMP 支持：`<linux/topology.h>`、`<linux/cpumask_api.h>`\n- 数据结构：`<linux/rbtree_augmented.h>`\n\n### 条件编译特性\n- `CONFIG_SMP`：多处理器调度优化\n- `CONFIG_CFS_BANDWIDTH`：CPU 带宽限制（cgroup v1/v2）\n- `CONFIG_NUMA_BALANCING`：NUMA 自动迁移\n- `CONFIG_FAIR_GROUP_SCHED`：CFS 组调度（cgroup 支持）\n\n## 5. 使用场景\n\n- **通用任务调度**：所有使用 `SCHED_NORMAL` 或 `SCHED_BATCH` 策略的用户态进程\n- **cgroup CPU 资源控制**：通过 `cpu.cfs_quota_us` 和 `cpu.cfs_period_us` 限制任务组带宽\n- **NUMA 优化**：自动迁移内存页以减少远程访问（`numa_balancing`）\n- **节能调度**：结合 `energy_model` 在满足性能前提下选择低功耗 CPU\n- **实时性保障**：通过 `cond_resched()` 在长循环中主动让出 CPU，避免内核抢占延迟过高\n- **系统调优**：管理员通过 `/proc/sys/kernel/` 下的 sysctl 参数动态调整调度行为",
      "similarity": 0.6340739130973816,
      "chunks": [
        {
          "chunk_id": 73,
          "file_path": "kernel/sched/fair.c",
          "start_line": 12860,
          "end_line": 12960,
          "content": [
            "bool cfs_prio_less(const struct task_struct *a, const struct task_struct *b,",
            "\t\t\tbool in_fi)",
            "{",
            "\tstruct rq *rq = task_rq(a);",
            "\tconst struct sched_entity *sea = &a->se;",
            "\tconst struct sched_entity *seb = &b->se;",
            "\tstruct cfs_rq *cfs_rqa;",
            "\tstruct cfs_rq *cfs_rqb;",
            "\ts64 delta;",
            "",
            "\tSCHED_WARN_ON(task_rq(b)->core != rq->core);",
            "",
            "#ifdef CONFIG_FAIR_GROUP_SCHED",
            "\t/*",
            "\t * Find an se in the hierarchy for tasks a and b, such that the se's",
            "\t * are immediate siblings.",
            "\t */",
            "\twhile (sea->cfs_rq->tg != seb->cfs_rq->tg) {",
            "\t\tint sea_depth = sea->depth;",
            "\t\tint seb_depth = seb->depth;",
            "",
            "\t\tif (sea_depth >= seb_depth)",
            "\t\t\tsea = parent_entity(sea);",
            "\t\tif (sea_depth <= seb_depth)",
            "\t\t\tseb = parent_entity(seb);",
            "\t}",
            "",
            "\tse_fi_update(sea, rq->core->core_forceidle_seq, in_fi);",
            "\tse_fi_update(seb, rq->core->core_forceidle_seq, in_fi);",
            "",
            "\tcfs_rqa = sea->cfs_rq;",
            "\tcfs_rqb = seb->cfs_rq;",
            "#else",
            "\tcfs_rqa = &task_rq(a)->cfs;",
            "\tcfs_rqb = &task_rq(b)->cfs;",
            "#endif",
            "",
            "\t/*",
            "\t * Find delta after normalizing se's vruntime with its cfs_rq's",
            "\t * min_vruntime_fi, which would have been updated in prior calls",
            "\t * to se_fi_update().",
            "\t */",
            "\tdelta = (s64)(sea->vruntime - seb->vruntime) +",
            "\t\t(s64)(cfs_rqb->min_vruntime_fi - cfs_rqa->min_vruntime_fi);",
            "",
            "\treturn delta > 0;",
            "}",
            "static int task_is_throttled_fair(struct task_struct *p, int cpu)",
            "{",
            "\tstruct cfs_rq *cfs_rq;",
            "",
            "#ifdef CONFIG_FAIR_GROUP_SCHED",
            "\tcfs_rq = task_group(p)->cfs_rq[cpu];",
            "#else",
            "\tcfs_rq = &cpu_rq(cpu)->cfs;",
            "#endif",
            "\treturn throttled_hierarchy(cfs_rq);",
            "}",
            "static inline void task_tick_core(struct rq *rq, struct task_struct *curr) {}",
            "static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)",
            "{",
            "\tstruct cfs_rq *cfs_rq;",
            "\tstruct sched_entity *se = &curr->se;",
            "",
            "\tfor_each_sched_entity(se) {",
            "\t\tcfs_rq = cfs_rq_of(se);",
            "\t\tentity_tick(cfs_rq, se, queued);",
            "\t}",
            "",
            "\tif (static_branch_unlikely(&sched_numa_balancing))",
            "\t\ttask_tick_numa(rq, curr);",
            "",
            "\tupdate_misfit_status(curr, rq);",
            "\tcheck_update_overutilized_status(task_rq(curr));",
            "",
            "\ttask_tick_core(rq, curr);",
            "}",
            "static void task_fork_fair(struct task_struct *p)",
            "{",
            "\tset_task_max_allowed_capacity(p);",
            "}",
            "static void",
            "prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)",
            "{",
            "\tif (!task_on_rq_queued(p))",
            "\t\treturn;",
            "",
            "\tif (rq->cfs.nr_running == 1)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Reschedule if we are currently running on this runqueue and",
            "\t * our priority decreased, or if we are not currently running on",
            "\t * this runqueue and our priority is higher than the current's",
            "\t */",
            "\tif (task_current(rq, p)) {",
            "\t\tif (p->prio > oldprio)",
            "\t\t\tresched_curr(rq);",
            "\t} else",
            "\t\twakeup_preempt(rq, p, 0);",
            "}"
          ],
          "function_name": "cfs_prio_less, task_is_throttled_fair, task_tick_core, task_tick_fair, task_fork_fair, prio_changed_fair",
          "description": "实现基于CFS的优先级比较和任务调度规则，处理任务优先级变更时的重调度需求，校验任务是否受限制，并维护强制空闲态下的虚拟运行时间计算逻辑。",
          "similarity": 0.6106491684913635
        },
        {
          "chunk_id": 65,
          "file_path": "kernel/sched/fair.c",
          "start_line": 11564,
          "end_line": 11837,
          "content": [
            "static int sched_balance_rq(int this_cpu, struct rq *this_rq,",
            "\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,",
            "\t\t\tint *continue_balancing)",
            "{",
            "\tint ld_moved, cur_ld_moved, active_balance = 0;",
            "\tstruct sched_domain *sd_parent = sd->parent;",
            "\tstruct sched_group *group;",
            "\tstruct rq *busiest;",
            "\tstruct rq_flags rf;",
            "\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);",
            "\tstruct lb_env env = {",
            "\t\t.sd\t\t= sd,",
            "\t\t.dst_cpu\t= this_cpu,",
            "\t\t.dst_rq\t\t= this_rq,",
            "\t\t.dst_grpmask    = group_balance_mask(sd->groups),",
            "\t\t.idle\t\t= idle,",
            "\t\t.loop_break\t= SCHED_NR_MIGRATE_BREAK,",
            "\t\t.cpus\t\t= cpus,",
            "\t\t.fbq_type\t= all,",
            "\t\t.tasks\t\t= LIST_HEAD_INIT(env.tasks),",
            "\t};",
            "",
            "\tcpumask_and(cpus, sched_domain_span(sd), cpu_active_mask);",
            "",
            "\tschedstat_inc(sd->lb_count[idle]);",
            "",
            "redo:",
            "\tif (!should_we_balance(&env)) {",
            "\t\t*continue_balancing = 0;",
            "\t\tgoto out_balanced;",
            "\t}",
            "",
            "\tgroup = sched_balance_find_src_group(&env);",
            "\tif (!group) {",
            "\t\tschedstat_inc(sd->lb_nobusyg[idle]);",
            "\t\tgoto out_balanced;",
            "\t}",
            "",
            "\tbusiest = find_busiest_queue(&env, group);",
            "\tif (!busiest) {",
            "\t\tschedstat_inc(sd->lb_nobusyq[idle]);",
            "\t\tgoto out_balanced;",
            "\t}",
            "",
            "\tWARN_ON_ONCE(busiest == env.dst_rq);",
            "",
            "\tschedstat_add(sd->lb_imbalance[idle], env.imbalance);",
            "",
            "\tenv.src_cpu = busiest->cpu;",
            "\tenv.src_rq = busiest;",
            "",
            "\tld_moved = 0;",
            "\t/* Clear this flag as soon as we find a pullable task */",
            "\tenv.flags |= LBF_ALL_PINNED;",
            "\tif (busiest->nr_running > 1) {",
            "\t\t/*",
            "\t\t * Attempt to move tasks. If sched_balance_find_src_group has found",
            "\t\t * an imbalance but busiest->nr_running <= 1, the group is",
            "\t\t * still unbalanced. ld_moved simply stays zero, so it is",
            "\t\t * correctly treated as an imbalance.",
            "\t\t */",
            "\t\tenv.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);",
            "",
            "more_balance:",
            "\t\trq_lock_irqsave(busiest, &rf);",
            "\t\tupdate_rq_clock(busiest);",
            "",
            "\t\t/*",
            "\t\t * cur_ld_moved - load moved in current iteration",
            "\t\t * ld_moved     - cumulative load moved across iterations",
            "\t\t */",
            "\t\tcur_ld_moved = detach_tasks(&env);",
            "",
            "\t\t/*",
            "\t\t * We've detached some tasks from busiest_rq. Every",
            "\t\t * task is masked \"TASK_ON_RQ_MIGRATING\", so we can safely",
            "\t\t * unlock busiest->lock, and we are able to be sure",
            "\t\t * that nobody can manipulate the tasks in parallel.",
            "\t\t * See task_rq_lock() family for the details.",
            "\t\t */",
            "",
            "\t\trq_unlock(busiest, &rf);",
            "",
            "\t\tif (cur_ld_moved) {",
            "\t\t\tattach_tasks(&env);",
            "\t\t\tld_moved += cur_ld_moved;",
            "\t\t}",
            "",
            "\t\tlocal_irq_restore(rf.flags);",
            "",
            "\t\tif (env.flags & LBF_NEED_BREAK) {",
            "\t\t\tenv.flags &= ~LBF_NEED_BREAK;",
            "\t\t\tgoto more_balance;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Revisit (affine) tasks on src_cpu that couldn't be moved to",
            "\t\t * us and move them to an alternate dst_cpu in our sched_group",
            "\t\t * where they can run. The upper limit on how many times we",
            "\t\t * iterate on same src_cpu is dependent on number of CPUs in our",
            "\t\t * sched_group.",
            "\t\t *",
            "\t\t * This changes load balance semantics a bit on who can move",
            "\t\t * load to a given_cpu. In addition to the given_cpu itself",
            "\t\t * (or a ilb_cpu acting on its behalf where given_cpu is",
            "\t\t * nohz-idle), we now have balance_cpu in a position to move",
            "\t\t * load to given_cpu. In rare situations, this may cause",
            "\t\t * conflicts (balance_cpu and given_cpu/ilb_cpu deciding",
            "\t\t * _independently_ and at _same_ time to move some load to",
            "\t\t * given_cpu) causing excess load to be moved to given_cpu.",
            "\t\t * This however should not happen so much in practice and",
            "\t\t * moreover subsequent load balance cycles should correct the",
            "\t\t * excess load moved.",
            "\t\t */",
            "\t\tif ((env.flags & LBF_DST_PINNED) && env.imbalance > 0) {",
            "",
            "\t\t\t/* Prevent to re-select dst_cpu via env's CPUs */",
            "\t\t\t__cpumask_clear_cpu(env.dst_cpu, env.cpus);",
            "",
            "\t\t\tenv.dst_rq\t = cpu_rq(env.new_dst_cpu);",
            "\t\t\tenv.dst_cpu\t = env.new_dst_cpu;",
            "\t\t\tenv.flags\t&= ~LBF_DST_PINNED;",
            "\t\t\tenv.loop\t = 0;",
            "\t\t\tenv.loop_break\t = SCHED_NR_MIGRATE_BREAK;",
            "",
            "\t\t\t/*",
            "\t\t\t * Go back to \"more_balance\" rather than \"redo\" since we",
            "\t\t\t * need to continue with same src_cpu.",
            "\t\t\t */",
            "\t\t\tgoto more_balance;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * We failed to reach balance because of affinity.",
            "\t\t */",
            "\t\tif (sd_parent) {",
            "\t\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;",
            "",
            "\t\t\tif ((env.flags & LBF_SOME_PINNED) && env.imbalance > 0)",
            "\t\t\t\t*group_imbalance = 1;",
            "\t\t}",
            "",
            "\t\t/* All tasks on this runqueue were pinned by CPU affinity */",
            "\t\tif (unlikely(env.flags & LBF_ALL_PINNED)) {",
            "\t\t\t__cpumask_clear_cpu(cpu_of(busiest), cpus);",
            "\t\t\t/*",
            "\t\t\t * Attempting to continue load balancing at the current",
            "\t\t\t * sched_domain level only makes sense if there are",
            "\t\t\t * active CPUs remaining as possible busiest CPUs to",
            "\t\t\t * pull load from which are not contained within the",
            "\t\t\t * destination group that is receiving any migrated",
            "\t\t\t * load.",
            "\t\t\t */",
            "\t\t\tif (!cpumask_subset(cpus, env.dst_grpmask)) {",
            "\t\t\t\tenv.loop = 0;",
            "\t\t\t\tenv.loop_break = SCHED_NR_MIGRATE_BREAK;",
            "\t\t\t\tgoto redo;",
            "\t\t\t}",
            "\t\t\tgoto out_all_pinned;",
            "\t\t}",
            "\t}",
            "",
            "\tif (!ld_moved) {",
            "\t\tschedstat_inc(sd->lb_failed[idle]);",
            "\t\t/*",
            "\t\t * Increment the failure counter only on periodic balance.",
            "\t\t * We do not want newidle balance, which can be very",
            "\t\t * frequent, pollute the failure counter causing",
            "\t\t * excessive cache_hot migrations and active balances.",
            "\t\t *",
            "\t\t * Similarly for migration_misfit which is not related to",
            "\t\t * load/util migration, don't pollute nr_balance_failed.",
            "\t\t */",
            "\t\tif (idle != CPU_NEWLY_IDLE &&",
            "\t\t    env.migration_type != migrate_misfit)",
            "\t\t\tsd->nr_balance_failed++;",
            "",
            "\t\tif (need_active_balance(&env)) {",
            "\t\t\tunsigned long flags;",
            "",
            "\t\t\traw_spin_rq_lock_irqsave(busiest, flags);",
            "",
            "\t\t\t/*",
            "\t\t\t * Don't kick the active_load_balance_cpu_stop,",
            "\t\t\t * if the curr task on busiest CPU can't be",
            "\t\t\t * moved to this_cpu:",
            "\t\t\t */",
            "\t\t\tif (!cpumask_test_cpu(this_cpu, busiest->curr->cpus_ptr)) {",
            "\t\t\t\traw_spin_rq_unlock_irqrestore(busiest, flags);",
            "\t\t\t\tgoto out_one_pinned;",
            "\t\t\t}",
            "",
            "\t\t\t/* Record that we found at least one task that could run on this_cpu */",
            "\t\t\tenv.flags &= ~LBF_ALL_PINNED;",
            "",
            "\t\t\t/*",
            "\t\t\t * ->active_balance synchronizes accesses to",
            "\t\t\t * ->active_balance_work.  Once set, it's cleared",
            "\t\t\t * only after active load balance is finished.",
            "\t\t\t */",
            "\t\t\tif (!busiest->active_balance) {",
            "\t\t\t\tbusiest->active_balance = 1;",
            "\t\t\t\tbusiest->push_cpu = this_cpu;",
            "\t\t\t\tactive_balance = 1;",
            "\t\t\t}",
            "",
            "\t\t\tpreempt_disable();",
            "\t\t\traw_spin_rq_unlock_irqrestore(busiest, flags);",
            "\t\t\tif (active_balance) {",
            "\t\t\t\tstop_one_cpu_nowait(cpu_of(busiest),",
            "\t\t\t\t\tactive_load_balance_cpu_stop, busiest,",
            "\t\t\t\t\t&busiest->active_balance_work);",
            "\t\t\t}",
            "\t\t\tpreempt_enable();",
            "\t\t}",
            "\t} else {",
            "\t\tsd->nr_balance_failed = 0;",
            "\t}",
            "",
            "\tif (likely(!active_balance) || need_active_balance(&env)) {",
            "\t\t/* We were unbalanced, so reset the balancing interval */",
            "\t\tsd->balance_interval = sd->min_interval;",
            "\t}",
            "",
            "\tgoto out;",
            "",
            "out_balanced:",
            "\t/*",
            "\t * We reach balance although we may have faced some affinity",
            "\t * constraints. Clear the imbalance flag only if other tasks got",
            "\t * a chance to move and fix the imbalance.",
            "\t */",
            "\tif (sd_parent && !(env.flags & LBF_ALL_PINNED)) {",
            "\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;",
            "",
            "\t\tif (*group_imbalance)",
            "\t\t\t*group_imbalance = 0;",
            "\t}",
            "",
            "out_all_pinned:",
            "\t/*",
            "\t * We reach balance because all tasks are pinned at this level so",
            "\t * we can't migrate them. Let the imbalance flag set so parent level",
            "\t * can try to migrate them.",
            "\t */",
            "\tschedstat_inc(sd->lb_balanced[idle]);",
            "",
            "\tsd->nr_balance_failed = 0;",
            "",
            "out_one_pinned:",
            "\tld_moved = 0;",
            "",
            "\t/*",
            "\t * sched_balance_newidle() disregards balance intervals, so we could",
            "\t * repeatedly reach this code, which would lead to balance_interval",
            "\t * skyrocketing in a short amount of time. Skip the balance_interval",
            "\t * increase logic to avoid that.",
            "\t *",
            "\t * Similarly misfit migration which is not necessarily an indication of",
            "\t * the system being busy and requires lb to backoff to let it settle",
            "\t * down.",
            "\t */",
            "\tif (env.idle == CPU_NEWLY_IDLE ||",
            "\t    env.migration_type == migrate_misfit)",
            "\t\tgoto out;",
            "",
            "\t/* tune up the balancing interval */",
            "\tif ((env.flags & LBF_ALL_PINNED &&",
            "\t     sd->balance_interval < MAX_PINNED_INTERVAL) ||",
            "\t    sd->balance_interval < sd->max_interval)",
            "\t\tsd->balance_interval *= 2;",
            "out:",
            "\treturn ld_moved;",
            "}"
          ],
          "function_name": "sched_balance_rq",
          "description": "实现负载平衡逻辑，尝试从繁忙运行队列迁移任务到当前CPU，处理迁移失败场景，更新统计信息并调整平衡间隔",
          "similarity": 0.6012468934059143
        },
        {
          "chunk_id": 72,
          "file_path": "kernel/sched/fair.c",
          "start_line": 12739,
          "end_line": 12841,
          "content": [
            "static __latent_entropy void sched_balance_softirq(struct softirq_action *h)",
            "{",
            "\tstruct rq *this_rq = this_rq();",
            "\tenum cpu_idle_type idle = this_rq->idle_balance ?",
            "\t\t\t\t\t\tCPU_IDLE : CPU_NOT_IDLE;",
            "",
            "\t/*",
            "\t * If this CPU has a pending nohz_balance_kick, then do the",
            "\t * balancing on behalf of the other idle CPUs whose ticks are",
            "\t * stopped. Do nohz_idle_balance *before* sched_balance_domains to",
            "\t * give the idle CPUs a chance to load balance. Else we may",
            "\t * load balance only within the local sched_domain hierarchy",
            "\t * and abort nohz_idle_balance altogether if we pull some load.",
            "\t */",
            "\tif (nohz_idle_balance(this_rq, idle))",
            "\t\treturn;",
            "",
            "\t/* normal load balance */",
            "\tsched_balance_update_blocked_averages(this_rq->cpu);",
            "\tsched_balance_domains(this_rq, idle);",
            "}",
            "void sched_balance_trigger(struct rq *rq)",
            "{",
            "\t/*",
            "\t * Don't need to rebalance while attached to NULL domain or",
            "\t * runqueue CPU is not active",
            "\t */",
            "\tif (unlikely(on_null_domain(rq) || !cpu_active(cpu_of(rq))))",
            "\t\treturn;",
            "",
            "\tif (time_after_eq(jiffies, rq->next_balance))",
            "\t\traise_softirq(SCHED_SOFTIRQ);",
            "",
            "\tnohz_balancer_kick(rq);",
            "}",
            "static void rq_online_fair(struct rq *rq)",
            "{",
            "\tupdate_sysctl();",
            "",
            "\tupdate_runtime_enabled(rq);",
            "}",
            "static void rq_offline_fair(struct rq *rq)",
            "{",
            "\tupdate_sysctl();",
            "",
            "\t/* Ensure any throttled groups are reachable by pick_next_task */",
            "\tunthrottle_offline_cfs_rqs(rq);",
            "}",
            "static inline bool",
            "__entity_slice_used(struct sched_entity *se, int min_nr_tasks)",
            "{",
            "\tu64 rtime = se->sum_exec_runtime - se->prev_sum_exec_runtime;",
            "\tu64 slice = se->slice;",
            "",
            "\treturn (rtime * min_nr_tasks > slice);",
            "}",
            "static inline void task_tick_core(struct rq *rq, struct task_struct *curr)",
            "{",
            "\tif (!sched_core_enabled(rq))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If runqueue has only one task which used up its slice and",
            "\t * if the sibling is forced idle, then trigger schedule to",
            "\t * give forced idle task a chance.",
            "\t *",
            "\t * sched_slice() considers only this active rq and it gets the",
            "\t * whole slice. But during force idle, we have siblings acting",
            "\t * like a single runqueue and hence we need to consider runnable",
            "\t * tasks on this CPU and the forced idle CPU. Ideally, we should",
            "\t * go through the forced idle rq, but that would be a perf hit.",
            "\t * We can assume that the forced idle CPU has at least",
            "\t * MIN_NR_TASKS_DURING_FORCEIDLE - 1 tasks and use that to check",
            "\t * if we need to give up the CPU.",
            "\t */",
            "\tif (rq->core->core_forceidle_count && rq->cfs.nr_running == 1 &&",
            "\t    __entity_slice_used(&curr->se, MIN_NR_TASKS_DURING_FORCEIDLE))",
            "\t\tresched_curr(rq);",
            "}",
            "static void se_fi_update(const struct sched_entity *se, unsigned int fi_seq,",
            "\t\t\t bool forceidle)",
            "{",
            "\tfor_each_sched_entity(se) {",
            "\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);",
            "",
            "\t\tif (forceidle) {",
            "\t\t\tif (cfs_rq->forceidle_seq == fi_seq)",
            "\t\t\t\tbreak;",
            "\t\t\tcfs_rq->forceidle_seq = fi_seq;",
            "\t\t}",
            "",
            "\t\tcfs_rq->min_vruntime_fi = cfs_rq->min_vruntime;",
            "\t}",
            "}",
            "void task_vruntime_update(struct rq *rq, struct task_struct *p, bool in_fi)",
            "{",
            "\tstruct sched_entity *se = &p->se;",
            "",
            "\tif (p->sched_class != &fair_sched_class)",
            "\t\treturn;",
            "",
            "\tse_fi_update(se, rq->core->core_forceidle_seq, in_fi);",
            "}"
          ],
          "function_name": "sched_balance_softirq, sched_balance_trigger, rq_online_fair, rq_offline_fair, __entity_slice_used, task_tick_core, se_fi_update, task_vruntime_update",
          "description": "管理运行队列的负载平衡触发机制，在软中断中执行跨CPU的负载均衡，维护运行队列状态并在CPU上线/下线时更新相关参数，支持强制空闲态的任务调度决策。",
          "similarity": 0.5931034088134766
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/sched/fair.c",
          "start_line": 1911,
          "end_line": 2012,
          "content": [
            "bool should_numa_migrate_memory(struct task_struct *p, struct folio *folio,",
            "\t\t\t\tint src_nid, int dst_cpu)",
            "{",
            "\tstruct numa_group *ng = deref_curr_numa_group(p);",
            "\tint dst_nid = cpu_to_node(dst_cpu);",
            "\tint last_cpupid, this_cpupid;",
            "",
            "\t/*",
            "\t * The pages in slow memory node should be migrated according",
            "\t * to hot/cold instead of private/shared.",
            "\t */",
            "\tif (folio_use_access_time(folio)) {",
            "\t\tstruct pglist_data *pgdat;",
            "\t\tunsigned long rate_limit;",
            "\t\tunsigned int latency, th, def_th;",
            "",
            "\t\tpgdat = NODE_DATA(dst_nid);",
            "\t\tif (pgdat_free_space_enough(pgdat)) {",
            "\t\t\t/* workload changed, reset hot threshold */",
            "\t\t\tpgdat->nbp_threshold = 0;",
            "\t\t\treturn true;",
            "\t\t}",
            "",
            "\t\tdef_th = sysctl_numa_balancing_hot_threshold;",
            "\t\trate_limit = sysctl_numa_balancing_promote_rate_limit << \\",
            "\t\t\t(20 - PAGE_SHIFT);",
            "\t\tnuma_promotion_adjust_threshold(pgdat, rate_limit, def_th);",
            "",
            "\t\tth = pgdat->nbp_threshold ? : def_th;",
            "\t\tlatency = numa_hint_fault_latency(folio);",
            "\t\tif (latency >= th)",
            "\t\t\treturn false;",
            "",
            "\t\treturn !numa_promotion_rate_limit(pgdat, rate_limit,",
            "\t\t\t\t\t\t  folio_nr_pages(folio));",
            "\t}",
            "",
            "\tthis_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);",
            "\tlast_cpupid = folio_xchg_last_cpupid(folio, this_cpupid);",
            "",
            "\tif (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&",
            "\t    !node_is_toptier(src_nid) && !cpupid_valid(last_cpupid))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Allow first faults or private faults to migrate immediately early in",
            "\t * the lifetime of a task. The magic number 4 is based on waiting for",
            "\t * two full passes of the \"multi-stage node selection\" test that is",
            "\t * executed below.",
            "\t */",
            "\tif ((p->numa_preferred_nid == NUMA_NO_NODE || p->numa_scan_seq <= 4) &&",
            "\t    (cpupid_pid_unset(last_cpupid) || cpupid_match_pid(p, last_cpupid)))",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Multi-stage node selection is used in conjunction with a periodic",
            "\t * migration fault to build a temporal task<->page relation. By using",
            "\t * a two-stage filter we remove short/unlikely relations.",
            "\t *",
            "\t * Using P(p) ~ n_p / n_t as per frequentist probability, we can equate",
            "\t * a task's usage of a particular page (n_p) per total usage of this",
            "\t * page (n_t) (in a given time-span) to a probability.",
            "\t *",
            "\t * Our periodic faults will sample this probability and getting the",
            "\t * same result twice in a row, given these samples are fully",
            "\t * independent, is then given by P(n)^2, provided our sample period",
            "\t * is sufficiently short compared to the usage pattern.",
            "\t *",
            "\t * This quadric squishes small probabilities, making it less likely we",
            "\t * act on an unlikely task<->page relation.",
            "\t */",
            "\tif (!cpupid_pid_unset(last_cpupid) &&",
            "\t\t\t\tcpupid_to_nid(last_cpupid) != dst_nid)",
            "\t\treturn false;",
            "",
            "\t/* Always allow migrate on private faults */",
            "\tif (cpupid_match_pid(p, last_cpupid))",
            "\t\treturn true;",
            "",
            "\t/* A shared fault, but p->numa_group has not been set up yet. */",
            "\tif (!ng)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Destination node is much more heavily used than the source",
            "\t * node? Allow migration.",
            "\t */",
            "\tif (group_faults_cpu(ng, dst_nid) > group_faults_cpu(ng, src_nid) *",
            "\t\t\t\t\tACTIVE_NODE_FRACTION)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Distribute memory according to CPU & memory use on each node,",
            "\t * with 3/4 hysteresis to avoid unnecessary memory migrations:",
            "\t *",
            "\t * faults_cpu(dst)   3   faults_cpu(src)",
            "\t * --------------- * - > ---------------",
            "\t * faults_mem(dst)   4   faults_mem(src)",
            "\t */",
            "\treturn group_faults_cpu(ng, dst_nid) * group_faults(p, src_nid) * 3 >",
            "\t       group_faults_cpu(ng, src_nid) * group_faults(p, dst_nid) * 4;",
            "}"
          ],
          "function_name": "should_numa_migrate_memory",
          "description": "综合任务访问延迟、迁移速率限制、节点负载及CPU亲和性，决定是否进行内存迁移；优先考虑首次故障、私有页迁移及节点间负载差异",
          "similarity": 0.5847090482711792
        },
        {
          "chunk_id": 52,
          "file_path": "kernel/sched/fair.c",
          "start_line": 8581,
          "end_line": 8709,
          "content": [
            "static void set_task_max_allowed_capacity(struct task_struct *p)",
            "{",
            "\tstruct asym_cap_data *entry;",
            "",
            "\tif (!sched_asym_cpucap_active())",
            "\t\treturn;",
            "",
            "\trcu_read_lock();",
            "\tlist_for_each_entry_rcu(entry, &asym_cap_list, link) {",
            "\t\tcpumask_t *cpumask;",
            "",
            "\t\tcpumask = cpu_capacity_span(entry);",
            "\t\tif (!cpumask_intersects(p->cpus_ptr, cpumask))",
            "\t\t\tcontinue;",
            "",
            "\t\tp->max_allowed_capacity = entry->capacity;",
            "\t\tbreak;",
            "\t}",
            "\trcu_read_unlock();",
            "}",
            "static void set_cpus_allowed_fair(struct task_struct *p, struct affinity_context *ctx)",
            "{",
            "\tset_cpus_allowed_common(p, ctx);",
            "\tset_task_max_allowed_capacity(p);",
            "}",
            "static int",
            "balance_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)",
            "{",
            "\tif (sched_fair_runnable(rq))",
            "\t\treturn 1;",
            "",
            "\treturn sched_balance_newidle(rq, rf) != 0;",
            "}",
            "static inline void set_task_max_allowed_capacity(struct task_struct *p) {}",
            "static void set_next_buddy(struct sched_entity *se)",
            "{",
            "\tfor_each_sched_entity(se) {",
            "\t\tif (SCHED_WARN_ON(!se->on_rq))",
            "\t\t\treturn;",
            "\t\tif (se_is_idle(se))",
            "\t\t\treturn;",
            "\t\tcfs_rq_of(se)->next = se;",
            "\t}",
            "}",
            "static void check_preempt_wakeup_fair(struct rq *rq, struct task_struct *p, int wake_flags)",
            "{",
            "\tstruct task_struct *curr = rq->curr;",
            "\tstruct sched_entity *se = &curr->se, *pse = &p->se;",
            "\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);",
            "\tint next_buddy_marked = 0;",
            "\tint cse_is_idle, pse_is_idle;",
            "",
            "\tif (unlikely(se == pse))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This is possible from callers such as attach_tasks(), in which we",
            "\t * unconditionally wakeup_preempt() after an enqueue (which may have",
            "\t * lead to a throttle).  This both saves work and prevents false",
            "\t * next-buddy nomination below.",
            "\t */",
            "\tif (unlikely(throttled_hierarchy(cfs_rq_of(pse))))",
            "\t\treturn;",
            "",
            "\tif (sched_feat(NEXT_BUDDY) && !(wake_flags & WF_FORK) && !pse->sched_delayed) {",
            "\t\tset_next_buddy(pse);",
            "\t\tnext_buddy_marked = 1;",
            "\t}",
            "",
            "\t/*",
            "\t * We can come here with TIF_NEED_RESCHED already set from new task",
            "\t * wake up path.",
            "\t *",
            "\t * Note: this also catches the edge-case of curr being in a throttled",
            "\t * group (e.g. via set_curr_task), since update_curr() (in the",
            "\t * enqueue of curr) will have resulted in resched being set.  This",
            "\t * prevents us from potentially nominating it as a false LAST_BUDDY",
            "\t * below.",
            "\t */",
            "\tif (test_tsk_need_resched(curr))",
            "\t\treturn;",
            "",
            "\tif (!sched_feat(WAKEUP_PREEMPTION))",
            "\t\treturn;",
            "",
            "\tfind_matching_se(&se, &pse);",
            "\tWARN_ON_ONCE(!pse);",
            "",
            "\tcse_is_idle = se_is_idle(se);",
            "\tpse_is_idle = se_is_idle(pse);",
            "",
            "\t/*",
            "\t * Preempt an idle entity in favor of a non-idle entity (and don't preempt",
            "\t * in the inverse case).",
            "\t */",
            "\tif (cse_is_idle && !pse_is_idle)",
            "\t\tgoto preempt;",
            "\tif (cse_is_idle != pse_is_idle)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * BATCH and IDLE tasks do not preempt others.",
            "\t */",
            "\tif (unlikely(!normal_policy(p->policy)))",
            "\t\treturn;",
            "",
            "\tcfs_rq = cfs_rq_of(se);",
            "\tupdate_curr(cfs_rq);",
            "\t/*",
            "\t * If @p has a shorter slice than current and @p is eligible, override",
            "\t * current's slice protection in order to allow preemption.",
            "\t *",
            "\t * Note that even if @p does not turn out to be the most eligible",
            "\t * task at this moment, current's slice protection will be lost.",
            "\t */",
            "\tif (do_preempt_short(cfs_rq, pse, se) && se->vlag == se->deadline)",
            "\t\tse->vlag = se->deadline + 1;",
            "",
            "\t/*",
            "\t * If @p has become the most eligible task, force preemption.",
            "\t */",
            "\tif (pick_eevdf(cfs_rq) == pse)",
            "\t\tgoto preempt;",
            "",
            "\treturn;",
            "",
            "preempt:",
            "\tresched_curr(rq);",
            "}"
          ],
          "function_name": "set_task_max_allowed_capacity, set_cpus_allowed_fair, balance_fair, set_task_max_allowed_capacity, set_next_buddy, check_preempt_wakeup_fair",
          "description": "设置任务最大允许容量，实现负载均衡判断逻辑，管理调度实体间的抢占关系和运行队列状态更新。",
          "similarity": 0.5828168392181396
        }
      ]
    },
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.6093047857284546,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.6634036302566528
        },
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.5657525062561035
        },
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.5356571078300476
        }
      ]
    }
  ]
}