{
  "query": "缓冲区在操作系统中的作用",
  "timestamp": "2025-12-25 23:48:55",
  "retrieved_files": [
    {
      "source_file": "mm/page_alloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_alloc.c`\n\n---\n\n# page_alloc.c 技术文档\n\n## 1. 文件概述\n\n`page_alloc.c` 是 Linux 内核内存管理子系统的核心文件之一，负责物理页面的分配与释放。该文件实现了基于区域（zone）和迁移类型（migratetype）的伙伴系统（Buddy System）内存分配器，管理系统的空闲页链表，并提供高效的页面分配/回收机制。它不处理小对象分配（由 slab/slub/slob 子系统负责），而是专注于以页为单位的大块物理内存管理。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct per_cpu_pages`**：每个 CPU 的每区（per-zone）页面缓存，用于减少锁竞争，提升分配性能。\n- **`node_states[NR_NODE_STATES]`**：全局节点状态掩码数组，跟踪各 NUMA 节点的状态（如在线、有内存等）。\n- **`sysctl_lowmem_reserve_ratio[MAX_NR_ZONES]`**：各内存区域的低内存保留比例，防止高优先级区域耗尽低优先级区域的内存。\n- **`zone_names[]` 和 `migratetype_names[]`**：内存区域和页面迁移类型的名称字符串，用于调试和日志。\n- **`gfp_allowed_mask`**：全局 GFP（Get Free Page）标志掩码，控制启动早期可使用的分配标志。\n\n### 主要函数（部分声明）\n- **`__free_pages_ok()`**：内部页面释放函数，执行实际的伙伴系统合并与链表插入逻辑。\n- 各种页面分配函数（如 `alloc_pages()`、`__alloc_pages()` 等，定义在其他位置但在此文件中实现核心逻辑）。\n- 每 CPU 页面列表操作辅助宏（如 `pcp_spin_lock()`、`pcp_spin_trylock()`）。\n\n### 关键常量与标志\n- **`fpi_t` 类型及标志**：\n  - `FPI_NONE`：无特殊要求。\n  - `FPI_SKIP_REPORT_NOTIFY`：跳过空闲页报告通知。\n  - `FPI_TO_TAIL`：将页面放回空闲链表尾部（用于优化场景如内存热插拔）。\n- **`min_free_kbytes`**：系统保留的最小空闲内存（KB），影响水位线计算。\n\n## 3. 关键实现\n\n### 每 CPU 页面缓存（Per-CPU Page Caching）\n- 通过 `struct per_cpu_pages` 为每个 CPU 维护热/冷页列表，避免频繁访问全局 zone 锁。\n- 使用 `pcpu_spin_lock` 宏族安全地访问每 CPU 数据，结合 `preempt_disable()`（非 RT）或 `migrate_disable()`（RT）防止任务迁移导致访问错误 CPU 的数据。\n- 在 UP 系统上，使用 IRQ 关闭防止重入；在 SMP/RT 系统上依赖自旋锁语义。\n\n### 内存区域（Zone）与 NUMA 支持\n- 支持多种内存区域（DMA、DMA32、Normal、HighMem、Movable、Device），通过 `zone_names` 标识。\n- 实现 `lowmem_reserve_ratio` 机制，确保高区域分配不会耗尽低区域的保留内存（如 ZONE_DMA 为设备保留）。\n- 通过 `node_states` 和 per-CPU 变量（如 `numa_node`、`_numa_mem_`）支持 NUMA 和无内存节点架构。\n\n### 空闲页管理优化\n- **`FPI_TO_TAIL` 标志**：允许将页面放回空闲链表尾部，配合内存打乱（shuffle）或热插拔时批量初始化。\n- **`FPI_SKIP_REPORT_NOTIFY` 标志**：在临时取出并归还页面时不触发空闲页报告机制，减少开销。\n- **水位线与保留内存**：`min_free_kbytes` 控制最低水位，影响 OOM（Out-Of-Memory）决策和内存回收行为。\n\n### 实时内核（PREEMPT_RT）适配\n- 在 RT 内核中使用 `migrate_disable()` 替代 `preempt_disable()`，避免干扰 RT 自旋锁的优先级继承机制。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心内存管理**：`<linux/mm.h>`, `<linux/highmem.h>`, `\"internal.h\"`\n- **同步机制**：`<linux/spinlock.h>`（隐含）、`<linux/mutex.h>`\n- **NUMA 与拓扑**：`<linux/topology.h>`, `<linux/nodemask.h>`\n- **调试与追踪**：`<linux/kasan.h>`, `<trace/events/kmem.h>`, `<linux/page_owner.h>`\n- **高级特性**：`<linux/compaction.h>`, `<linux/migrate.h>`, `<linux/memcontrol.h>`\n\n### 子系统交互\n- **Slab 分配器**：本文件不处理 kmalloc，由 `slab.c` 等负责。\n- **内存回收**：与 `vmscan.c` 协同，通过水位线触发 reclaim。\n- **内存热插拔**：通过 `memory_hotplug.h` 接口管理动态内存。\n- **OOM Killer**：通过 `oom.h` 和水位线机制触发 OOM。\n- **透明大页（THP）**：与 `khugepaged` 协同进行大页分配。\n\n## 5. 使用场景\n\n- **内核内存分配**：所有以页为单位的内核内存请求（如 `alloc_pages()`）最终由本文件处理。\n- **用户空间缺页处理**：匿名页、文件页的物理页分配。\n- **内存映射（mmap）**：大块物理内存的分配与管理。\n- **内存回收与迁移**：页面回收、压缩（compaction）、迁移（migration）过程中涉及的页面释放与重新分配。\n- **系统启动与热插拔**：初始化内存区域、处理动态添加/移除内存。\n- **实时系统**：在 PREEMPT_RT 内核中提供低延迟的页面分配路径。\n- **调试与监控**：通过 page owner、KASAN、tracepoint 等机制提供内存使用追踪。",
      "similarity": 0.6635134220123291,
      "chunks": [
        {
          "chunk_id": 30,
          "file_path": "mm/page_alloc.c",
          "start_line": 5824,
          "end_line": 5948,
          "content": [
            "unsigned long free_reserved_area(void *start, void *end, int poison, const char *s)",
            "{",
            "\tvoid *pos;",
            "\tunsigned long pages = 0;",
            "",
            "\tstart = (void *)PAGE_ALIGN((unsigned long)start);",
            "\tend = (void *)((unsigned long)end & PAGE_MASK);",
            "\tfor (pos = start; pos < end; pos += PAGE_SIZE, pages++) {",
            "\t\tstruct page *page = virt_to_page(pos);",
            "\t\tvoid *direct_map_addr;",
            "",
            "\t\t/*",
            "\t\t * 'direct_map_addr' might be different from 'pos'",
            "\t\t * because some architectures' virt_to_page()",
            "\t\t * work with aliases.  Getting the direct map",
            "\t\t * address ensures that we get a _writeable_",
            "\t\t * alias for the memset().",
            "\t\t */",
            "\t\tdirect_map_addr = page_address(page);",
            "\t\t/*",
            "\t\t * Perform a kasan-unchecked memset() since this memory",
            "\t\t * has not been initialized.",
            "\t\t */",
            "\t\tdirect_map_addr = kasan_reset_tag(direct_map_addr);",
            "\t\tif ((unsigned int)poison <= 0xFF)",
            "\t\t\tmemset(direct_map_addr, poison, PAGE_SIZE);",
            "",
            "\t\tfree_reserved_page(page);",
            "\t}",
            "",
            "\tif (pages && s)",
            "\t\tpr_info(\"Freeing %s memory: %ldK\\n\", s, K(pages));",
            "",
            "\treturn pages;",
            "}",
            "void free_reserved_page(struct page *page)",
            "{",
            "\tclear_page_tag_ref(page);",
            "\tClearPageReserved(page);",
            "\tinit_page_count(page);",
            "\t__free_page(page);",
            "\tadjust_managed_page_count(page, 1);",
            "}",
            "static int page_alloc_cpu_dead(unsigned int cpu)",
            "{",
            "\tstruct zone *zone;",
            "",
            "\tlru_add_drain_cpu(cpu);",
            "\tmlock_drain_remote(cpu);",
            "\tdrain_pages(cpu);",
            "",
            "\t/*",
            "\t * Spill the event counters of the dead processor",
            "\t * into the current processors event counters.",
            "\t * This artificially elevates the count of the current",
            "\t * processor.",
            "\t */",
            "\tvm_events_fold_cpu(cpu);",
            "",
            "\t/*",
            "\t * Zero the differential counters of the dead processor",
            "\t * so that the vm statistics are consistent.",
            "\t *",
            "\t * This is only okay since the processor is dead and cannot",
            "\t * race with what we are doing.",
            "\t */",
            "\tcpu_vm_stats_fold(cpu);",
            "",
            "\tfor_each_populated_zone(zone)",
            "\t\tzone_pcp_update(zone, 0);",
            "",
            "\treturn 0;",
            "}",
            "static int page_alloc_cpu_online(unsigned int cpu)",
            "{",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_populated_zone(zone)",
            "\t\tzone_pcp_update(zone, 1);",
            "\treturn 0;",
            "}",
            "void __init page_alloc_init_cpuhp(void)",
            "{",
            "\tint ret;",
            "",
            "\tret = cpuhp_setup_state_nocalls(CPUHP_PAGE_ALLOC,",
            "\t\t\t\t\t\"mm/page_alloc:pcp\",",
            "\t\t\t\t\tpage_alloc_cpu_online,",
            "\t\t\t\t\tpage_alloc_cpu_dead);",
            "\tWARN_ON(ret < 0);",
            "}",
            "static void calculate_totalreserve_pages(void)",
            "{",
            "\tstruct pglist_data *pgdat;",
            "\tunsigned long reserve_pages = 0;",
            "\tenum zone_type i, j;",
            "",
            "\tfor_each_online_pgdat(pgdat) {",
            "",
            "\t\tpgdat->totalreserve_pages = 0;",
            "",
            "\t\tfor (i = 0; i < MAX_NR_ZONES; i++) {",
            "\t\t\tstruct zone *zone = pgdat->node_zones + i;",
            "\t\t\tlong max = 0;",
            "\t\t\tunsigned long managed_pages = zone_managed_pages(zone);",
            "",
            "\t\t\t/* Find valid and maximum lowmem_reserve in the zone */",
            "\t\t\tfor (j = i; j < MAX_NR_ZONES; j++) {",
            "\t\t\t\tif (zone->lowmem_reserve[j] > max)",
            "\t\t\t\t\tmax = zone->lowmem_reserve[j];",
            "\t\t\t}",
            "",
            "\t\t\t/* we treat the high watermark as reserved pages. */",
            "\t\t\tmax += high_wmark_pages(zone);",
            "",
            "\t\t\tif (max > managed_pages)",
            "\t\t\t\tmax = managed_pages;",
            "",
            "\t\t\tpgdat->totalreserve_pages += max;",
            "",
            "\t\t\treserve_pages += max;",
            "\t\t}",
            "\t}",
            "\ttotalreserve_pages = reserve_pages;",
            "}"
          ],
          "function_name": "free_reserved_area, free_reserved_page, page_alloc_cpu_dead, page_alloc_cpu_online, page_alloc_init_cpuhp, calculate_totalreserve_pages",
          "description": "实现释放保留内存区域，遍历指定范围内的页面进行初始化和释放操作，并维护统计信息。包含释放保留页面、CPU在线/离线回调及计算总保留页数等功能。",
          "similarity": 0.6379296183586121
        },
        {
          "chunk_id": 34,
          "file_path": "mm/page_alloc.c",
          "start_line": 6473,
          "end_line": 6610,
          "content": [
            "static void split_free_pages(struct list_head *list)",
            "{",
            "\tint order;",
            "",
            "\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {",
            "\t\tstruct page *page, *next;",
            "\t\tint nr_pages = 1 << order;",
            "",
            "\t\tlist_for_each_entry_safe(page, next, &list[order], lru) {",
            "\t\t\tint i;",
            "",
            "\t\t\tpost_alloc_hook(page, order, __GFP_MOVABLE);",
            "\t\t\tif (!order)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tsplit_page(page, order);",
            "",
            "\t\t\t/* Add all subpages to the order-0 head, in sequence. */",
            "\t\t\tlist_del(&page->lru);",
            "\t\t\tfor (i = 0; i < nr_pages; i++)",
            "\t\t\t\tlist_add_tail(&page[i].lru, &list[0]);",
            "\t\t}",
            "\t}",
            "}",
            "int alloc_contig_range_noprof(unsigned long start, unsigned long end,",
            "\t\t       unsigned migratetype, gfp_t gfp_mask)",
            "{",
            "\tunsigned long outer_start, outer_end;",
            "\tint ret = 0;",
            "",
            "\tstruct compact_control cc = {",
            "\t\t.nr_migratepages = 0,",
            "\t\t.order = -1,",
            "\t\t.zone = page_zone(pfn_to_page(start)),",
            "\t\t.mode = MIGRATE_SYNC,",
            "\t\t.ignore_skip_hint = true,",
            "\t\t.no_set_skip_hint = true,",
            "\t\t.gfp_mask = current_gfp_context(gfp_mask),",
            "\t\t.alloc_contig = true,",
            "\t};",
            "\tINIT_LIST_HEAD(&cc.migratepages);",
            "",
            "\t/*",
            "\t * What we do here is we mark all pageblocks in range as",
            "\t * MIGRATE_ISOLATE.  Because pageblock and max order pages may",
            "\t * have different sizes, and due to the way page allocator",
            "\t * work, start_isolate_page_range() has special handlings for this.",
            "\t *",
            "\t * Once the pageblocks are marked as MIGRATE_ISOLATE, we",
            "\t * migrate the pages from an unaligned range (ie. pages that",
            "\t * we are interested in). This will put all the pages in",
            "\t * range back to page allocator as MIGRATE_ISOLATE.",
            "\t *",
            "\t * When this is done, we take the pages in range from page",
            "\t * allocator removing them from the buddy system.  This way",
            "\t * page allocator will never consider using them.",
            "\t *",
            "\t * This lets us mark the pageblocks back as",
            "\t * MIGRATE_CMA/MIGRATE_MOVABLE so that free pages in the",
            "\t * aligned range but not in the unaligned, original range are",
            "\t * put back to page allocator so that buddy can use them.",
            "\t */",
            "",
            "\tret = start_isolate_page_range(start, end, migratetype, 0, gfp_mask);",
            "\tif (ret)",
            "\t\tgoto done;",
            "",
            "\tdrain_all_pages(cc.zone);",
            "",
            "\t/*",
            "\t * In case of -EBUSY, we'd like to know which page causes problem.",
            "\t * So, just fall through. test_pages_isolated() has a tracepoint",
            "\t * which will report the busy page.",
            "\t *",
            "\t * It is possible that busy pages could become available before",
            "\t * the call to test_pages_isolated, and the range will actually be",
            "\t * allocated.  So, if we fall through be sure to clear ret so that",
            "\t * -EBUSY is not accidentally used or returned to caller.",
            "\t */",
            "\tret = __alloc_contig_migrate_range(&cc, start, end, migratetype);",
            "\tif (ret && ret != -EBUSY)",
            "\t\tgoto done;",
            "\tret = 0;",
            "",
            "\t/*",
            "\t * Pages from [start, end) are within a pageblock_nr_pages",
            "\t * aligned blocks that are marked as MIGRATE_ISOLATE.  What's",
            "\t * more, all pages in [start, end) are free in page allocator.",
            "\t * What we are going to do is to allocate all pages from",
            "\t * [start, end) (that is remove them from page allocator).",
            "\t *",
            "\t * The only problem is that pages at the beginning and at the",
            "\t * end of interesting range may be not aligned with pages that",
            "\t * page allocator holds, ie. they can be part of higher order",
            "\t * pages.  Because of this, we reserve the bigger range and",
            "\t * once this is done free the pages we are not interested in.",
            "\t *",
            "\t * We don't have to hold zone->lock here because the pages are",
            "\t * isolated thus they won't get removed from buddy.",
            "\t */",
            "\touter_start = find_large_buddy(start);",
            "",
            "\t/* Make sure the range is really isolated. */",
            "\tif (test_pages_isolated(outer_start, end, 0)) {",
            "\t\tret = -EBUSY;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\t/* Grab isolated pages from freelists. */",
            "\touter_end = isolate_freepages_range(&cc, outer_start, end);",
            "\tif (!outer_end) {",
            "\t\tret = -EBUSY;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\tif (!(gfp_mask & __GFP_COMP)) {",
            "\t\tsplit_free_pages(cc.freepages);",
            "",
            "\t\t/* Free head and tail (if any) */",
            "\t\tif (start != outer_start)",
            "\t\t\tfree_contig_range(outer_start, start - outer_start);",
            "\t\tif (end != outer_end)",
            "\t\t\tfree_contig_range(end, outer_end - end);",
            "\t} else if (start == outer_start && end == outer_end && is_power_of_2(end - start)) {",
            "\t\tstruct page *head = pfn_to_page(start);",
            "\t\tint order = ilog2(end - start);",
            "",
            "\t\tcheck_new_pages(head, order);",
            "\t\tprep_new_page(head, order, gfp_mask, 0);",
            "\t} else {",
            "\t\tret = -EINVAL;",
            "\t\tWARN(true, \"PFN range: requested [%lu, %lu), allocated [%lu, %lu)\\n\",",
            "\t\t     start, end, outer_start, outer_end);",
            "\t}",
            "done:",
            "\tundo_isolate_page_range(start, end, migratetype);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "split_free_pages, alloc_contig_range_noprof",
          "description": "实现连续物理内存范围分配功能，通过隔离页面块、迁移页面和调整区域状态，确保目标范围内内存可被系统使用并处理分配异常情况。",
          "similarity": 0.6201975345611572
        },
        {
          "chunk_id": 32,
          "file_path": "mm/page_alloc.c",
          "start_line": 6106,
          "end_line": 6209,
          "content": [
            "void calculate_min_free_kbytes(void)",
            "{",
            "\tunsigned long lowmem_kbytes;",
            "\tint new_min_free_kbytes;",
            "",
            "\tlowmem_kbytes = nr_free_buffer_pages() * (PAGE_SIZE >> 10);",
            "\tnew_min_free_kbytes = int_sqrt(lowmem_kbytes * 16);",
            "",
            "\tif (new_min_free_kbytes > user_min_free_kbytes)",
            "\t\tmin_free_kbytes = clamp(new_min_free_kbytes, 128, 262144);",
            "\telse",
            "\t\tpr_warn(\"min_free_kbytes is not updated to %d because user defined value %d is preferred\\n\",",
            "\t\t\t\tnew_min_free_kbytes, user_min_free_kbytes);",
            "",
            "}",
            "int __meminit init_per_zone_wmark_min(void)",
            "{",
            "\tcalculate_min_free_kbytes();",
            "\tsetup_per_zone_wmarks();",
            "\trefresh_zone_stat_thresholds();",
            "\tsetup_per_zone_lowmem_reserve();",
            "",
            "#ifdef CONFIG_NUMA",
            "\tsetup_min_unmapped_ratio();",
            "\tsetup_min_slab_ratio();",
            "#endif",
            "",
            "\tkhugepaged_min_free_kbytes_update();",
            "",
            "\treturn 0;",
            "}",
            "postcore_initcall(init_per_zone_wmark_min)",
            "",
            "/*",
            " * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so",
            " *\tthat we can call two helper functions whenever min_free_kbytes",
            " *\tchanges.",
            " */",
            "static int min_free_kbytes_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tif (write) {",
            "\t\tuser_min_free_kbytes = min_free_kbytes;",
            "\t\tsetup_per_zone_wmarks();",
            "\t}",
            "\treturn 0;",
            "}",
            "static int watermark_scale_factor_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tif (write)",
            "\t\tsetup_per_zone_wmarks();",
            "",
            "\treturn 0;",
            "}",
            "static void setup_min_unmapped_ratio(void)",
            "{",
            "\tpg_data_t *pgdat;",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_online_pgdat(pgdat)",
            "\t\tpgdat->min_unmapped_pages = 0;",
            "",
            "\tfor_each_zone(zone)",
            "\t\tzone->zone_pgdat->min_unmapped_pages += (zone_managed_pages(zone) *",
            "\t\t\t\t\t\t         sysctl_min_unmapped_ratio) / 100;",
            "}",
            "static int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tsetup_min_unmapped_ratio();",
            "",
            "\treturn 0;",
            "}",
            "static void setup_min_slab_ratio(void)",
            "{",
            "\tpg_data_t *pgdat;",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_online_pgdat(pgdat)",
            "\t\tpgdat->min_slab_pages = 0;",
            "",
            "\tfor_each_zone(zone)",
            "\t\tzone->zone_pgdat->min_slab_pages += (zone_managed_pages(zone) *",
            "\t\t\t\t\t\t     sysctl_min_slab_ratio) / 100;",
            "}"
          ],
          "function_name": "calculate_min_free_kbytes, init_per_zone_wmark_min, min_free_kbytes_sysctl_handler, watermark_scale_factor_sysctl_handler, setup_min_unmapped_ratio, sysctl_min_unmapped_ratio_sysctl_handler, setup_min_slab_ratio",
          "description": "计算并更新最小空闲内存阈值，初始化区域水印参数，处理sysctl接口变更，动态调整内存管理策略参数。",
          "similarity": 0.6037539839744568
        },
        {
          "chunk_id": 31,
          "file_path": "mm/page_alloc.c",
          "start_line": 5966,
          "end_line": 6071,
          "content": [
            "static void setup_per_zone_lowmem_reserve(void)",
            "{",
            "\tstruct pglist_data *pgdat;",
            "\tenum zone_type i, j;",
            "",
            "\tfor_each_online_pgdat(pgdat) {",
            "\t\tfor (i = 0; i < MAX_NR_ZONES - 1; i++) {",
            "\t\t\tstruct zone *zone = &pgdat->node_zones[i];",
            "\t\t\tint ratio = sysctl_lowmem_reserve_ratio[i];",
            "\t\t\tbool clear = !ratio || !zone_managed_pages(zone);",
            "\t\t\tunsigned long managed_pages = 0;",
            "",
            "\t\t\tfor (j = i + 1; j < MAX_NR_ZONES; j++) {",
            "\t\t\t\tstruct zone *upper_zone = &pgdat->node_zones[j];",
            "",
            "\t\t\t\tmanaged_pages += zone_managed_pages(upper_zone);",
            "",
            "\t\t\t\tif (clear)",
            "\t\t\t\t\tzone->lowmem_reserve[j] = 0;",
            "\t\t\t\telse",
            "\t\t\t\t\tzone->lowmem_reserve[j] = managed_pages / ratio;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\t/* update totalreserve_pages */",
            "\tcalculate_totalreserve_pages();",
            "}",
            "static void __setup_per_zone_wmarks(void)",
            "{",
            "\tunsigned long pages_min = min_free_kbytes >> (PAGE_SHIFT - 10);",
            "\tunsigned long lowmem_pages = 0;",
            "\tstruct zone *zone;",
            "\tunsigned long flags;",
            "",
            "\t/* Calculate total number of !ZONE_HIGHMEM and !ZONE_MOVABLE pages */",
            "\tfor_each_zone(zone) {",
            "\t\tif (!is_highmem(zone) && zone_idx(zone) != ZONE_MOVABLE)",
            "\t\t\tlowmem_pages += zone_managed_pages(zone);",
            "\t}",
            "",
            "\tfor_each_zone(zone) {",
            "\t\tu64 tmp;",
            "",
            "\t\tspin_lock_irqsave(&zone->lock, flags);",
            "\t\ttmp = (u64)pages_min * zone_managed_pages(zone);",
            "\t\tdo_div(tmp, lowmem_pages);",
            "\t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {",
            "\t\t\t/*",
            "\t\t\t * __GFP_HIGH and PF_MEMALLOC allocations usually don't",
            "\t\t\t * need highmem and movable zones pages, so cap pages_min",
            "\t\t\t * to a small  value here.",
            "\t\t\t *",
            "\t\t\t * The WMARK_HIGH-WMARK_LOW and (WMARK_LOW-WMARK_MIN)",
            "\t\t\t * deltas control async page reclaim, and so should",
            "\t\t\t * not be capped for highmem and movable zones.",
            "\t\t\t */",
            "\t\t\tunsigned long min_pages;",
            "",
            "\t\t\tmin_pages = zone_managed_pages(zone) / 1024;",
            "\t\t\tmin_pages = clamp(min_pages, SWAP_CLUSTER_MAX, 128UL);",
            "\t\t\tzone->_watermark[WMARK_MIN] = min_pages;",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * If it's a lowmem zone, reserve a number of pages",
            "\t\t\t * proportionate to the zone's size.",
            "\t\t\t */",
            "\t\t\tzone->_watermark[WMARK_MIN] = tmp;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Set the kswapd watermarks distance according to the",
            "\t\t * scale factor in proportion to available memory, but",
            "\t\t * ensure a minimum size on small systems.",
            "\t\t */",
            "\t\ttmp = max_t(u64, tmp >> 2,",
            "\t\t\t    mult_frac(zone_managed_pages(zone),",
            "\t\t\t\t      watermark_scale_factor, 10000));",
            "",
            "\t\tzone->watermark_boost = 0;",
            "\t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;",
            "\t\tzone->_watermark[WMARK_HIGH] = low_wmark_pages(zone) + tmp;",
            "\t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;",
            "",
            "\t\tspin_unlock_irqrestore(&zone->lock, flags);",
            "\t}",
            "",
            "\t/* update totalreserve_pages */",
            "\tcalculate_totalreserve_pages();",
            "}",
            "void setup_per_zone_wmarks(void)",
            "{",
            "\tstruct zone *zone;",
            "\tstatic DEFINE_SPINLOCK(lock);",
            "",
            "\tspin_lock(&lock);",
            "\t__setup_per_zone_wmarks();",
            "\tspin_unlock(&lock);",
            "",
            "\t/*",
            "\t * The watermark size have changed so update the pcpu batch",
            "\t * and high limits or the limits may be inappropriate.",
            "\t */",
            "\tfor_each_zone(zone)",
            "\t\tzone_pcp_update(zone, 0);",
            "}"
          ],
          "function_name": "setup_per_zone_lowmem_reserve, __setup_per_zone_wmarks, setup_per_zone_wmarks",
          "description": "设置各内存区域的低内存保留值和水印标记，根据系统配置调整内存管理参数，确保内存分配策略适应不同场景需求。",
          "similarity": 0.5941185355186462
        },
        {
          "chunk_id": 28,
          "file_path": "mm/page_alloc.c",
          "start_line": 5556,
          "end_line": 5674,
          "content": [
            "static int zone_highsize(struct zone *zone, int batch, int cpu_online,",
            "\t\t\t int high_fraction)",
            "{",
            "#ifdef CONFIG_MMU",
            "\tint high;",
            "\tint nr_split_cpus;",
            "\tunsigned long total_pages;",
            "",
            "\tif (!high_fraction) {",
            "\t\t/*",
            "\t\t * By default, the high value of the pcp is based on the zone",
            "\t\t * low watermark so that if they are full then background",
            "\t\t * reclaim will not be started prematurely.",
            "\t\t */",
            "\t\ttotal_pages = low_wmark_pages(zone);",
            "\t} else {",
            "\t\t/*",
            "\t\t * If percpu_pagelist_high_fraction is configured, the high",
            "\t\t * value is based on a fraction of the managed pages in the",
            "\t\t * zone.",
            "\t\t */",
            "\t\ttotal_pages = zone_managed_pages(zone) / high_fraction;",
            "\t}",
            "",
            "\t/*",
            "\t * Split the high value across all online CPUs local to the zone. Note",
            "\t * that early in boot that CPUs may not be online yet and that during",
            "\t * CPU hotplug that the cpumask is not yet updated when a CPU is being",
            "\t * onlined. For memory nodes that have no CPUs, split the high value",
            "\t * across all online CPUs to mitigate the risk that reclaim is triggered",
            "\t * prematurely due to pages stored on pcp lists.",
            "\t */",
            "\tnr_split_cpus = cpumask_weight(cpumask_of_node(zone_to_nid(zone))) + cpu_online;",
            "\tif (!nr_split_cpus)",
            "\t\tnr_split_cpus = num_online_cpus();",
            "\thigh = total_pages / nr_split_cpus;",
            "",
            "\t/*",
            "\t * Ensure high is at least batch*4. The multiple is based on the",
            "\t * historical relationship between high and batch.",
            "\t */",
            "\thigh = max(high, batch << 2);",
            "",
            "\treturn high;",
            "#else",
            "\treturn 0;",
            "#endif",
            "}",
            "static void pageset_update(struct per_cpu_pages *pcp, unsigned long high_min,",
            "\t\t\t   unsigned long high_max, unsigned long batch)",
            "{",
            "\tWRITE_ONCE(pcp->batch, batch);",
            "\tWRITE_ONCE(pcp->high_min, high_min);",
            "\tWRITE_ONCE(pcp->high_max, high_max);",
            "}",
            "static void per_cpu_pages_init(struct per_cpu_pages *pcp, struct per_cpu_zonestat *pzstats)",
            "{",
            "\tint pindex;",
            "",
            "\tmemset(pcp, 0, sizeof(*pcp));",
            "\tmemset(pzstats, 0, sizeof(*pzstats));",
            "",
            "\tspin_lock_init(&pcp->lock);",
            "\tfor (pindex = 0; pindex < NR_PCP_LISTS; pindex++)",
            "\t\tINIT_LIST_HEAD(&pcp->lists[pindex]);",
            "",
            "\t/*",
            "\t * Set batch and high values safe for a boot pageset. A true percpu",
            "\t * pageset's initialization will update them subsequently. Here we don't",
            "\t * need to be as careful as pageset_update() as nobody can access the",
            "\t * pageset yet.",
            "\t */",
            "\tpcp->high_min = BOOT_PAGESET_HIGH;",
            "\tpcp->high_max = BOOT_PAGESET_HIGH;",
            "\tpcp->batch = BOOT_PAGESET_BATCH;",
            "\tpcp->free_count = 0;",
            "}",
            "static void __zone_set_pageset_high_and_batch(struct zone *zone, unsigned long high_min,",
            "\t\t\t\t\t      unsigned long high_max, unsigned long batch)",
            "{",
            "\tstruct per_cpu_pages *pcp;",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tpcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);",
            "\t\tpageset_update(pcp, high_min, high_max, batch);",
            "\t}",
            "}",
            "static void zone_set_pageset_high_and_batch(struct zone *zone, int cpu_online)",
            "{",
            "\tint new_high_min, new_high_max, new_batch;",
            "",
            "\tnew_batch = max(1, zone_batchsize(zone));",
            "\tif (percpu_pagelist_high_fraction) {",
            "\t\tnew_high_min = zone_highsize(zone, new_batch, cpu_online,",
            "\t\t\t\t\t     percpu_pagelist_high_fraction);",
            "\t\t/*",
            "\t\t * PCP high is tuned manually, disable auto-tuning via",
            "\t\t * setting high_min and high_max to the manual value.",
            "\t\t */",
            "\t\tnew_high_max = new_high_min;",
            "\t} else {",
            "\t\tnew_high_min = zone_highsize(zone, new_batch, cpu_online, 0);",
            "\t\tnew_high_max = zone_highsize(zone, new_batch, cpu_online,",
            "\t\t\t\t\t     MIN_PERCPU_PAGELIST_HIGH_FRACTION);",
            "\t}",
            "",
            "\tif (zone->pageset_high_min == new_high_min &&",
            "\t    zone->pageset_high_max == new_high_max &&",
            "\t    zone->pageset_batch == new_batch)",
            "\t\treturn;",
            "",
            "\tzone->pageset_high_min = new_high_min;",
            "\tzone->pageset_high_max = new_high_max;",
            "\tzone->pageset_batch = new_batch;",
            "",
            "\t__zone_set_pageset_high_and_batch(zone, new_high_min, new_high_max,",
            "\t\t\t\t\t  new_batch);",
            "}"
          ],
          "function_name": "zone_highsize, pageset_update, per_cpu_pages_init, __zone_set_pageset_high_and_batch, zone_set_pageset_high_and_batch",
          "description": "计算并设置各CPU的页集高水位和批量参数，根据缓存数据片大小调整批量标志位，控制PCP列表的行为特性。",
          "similarity": 0.5846865773200989
        }
      ]
    },
    {
      "source_file": "kernel/sched/membarrier.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:12:44\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\membarrier.c`\n\n---\n\n# `sched/membarrier.c` 技术文档\n\n## 1. 文件概述\n\n`sched/membarrier.c` 实现了 Linux 内核中的 `membarrier` 系统调用，该调用为用户空间程序提供了一种高效的全局内存屏障机制。与传统的在每个线程中显式插入内存屏障相比，`membarrier` 允许一个线程通过一次系统调用，强制所有运行在系统上的线程（或特定进程组内的线程）执行内存屏障操作，从而简化用户空间并发同步逻辑并提升性能。\n\n该文件的核心目标是在多核系统中确保内存操作的全局可见性顺序，尤其适用于需要跨线程强内存顺序保证的用户空间同步原语（如 RCU、无锁数据结构等）。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`ipi_mb(void *info)`**  \n  IPI（处理器间中断）处理函数，执行 `smp_mb()` 内存屏障，用于基础的全局内存屏障命令。\n\n- **`ipi_sync_core(void *info)`**  \n  用于 `MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE` 命令的 IPI 处理函数，在执行内存屏障后调用 `sync_core_before_usermode()`，确保 CPU 核心状态同步（如指令缓存一致性）。\n\n- **`ipi_rseq(void *info)`**  \n  用于 `MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ` 命令的 IPI 处理函数，在内存屏障后调用 `rseq_preempt(current)`，以支持 restartable sequences（rseq）机制的正确性。\n\n- **`ipi_sync_rq_state(void *info)`**  \n  用于同步 per-CPU runqueue 的 `membarrier_state` 字段，使其与指定 `mm_struct` 的状态一致，确保后续 `membarrier` 调用能正确识别注册状态。\n\n- **`membarrier_exec_mmap(struct mm_struct *mm)`**  \n  在进程执行 `exec` 系统调用时被调用，重置该内存描述符（`mm_struct`）的 `membarrier_state` 为 0，并同步 per-CPU runqueue 状态，防止 exec 后残留旧的注册状态。\n\n### 数据结构与宏\n\n- **`MEMBARRIER_CMD_BITMASK`**  \n  定义所有支持的 `membarrier` 命令的位掩码（不含 `QUERY`），用于命令合法性校验。\n\n- **`membarrier_ipi_mutex`**  \n  互斥锁，用于序列化 IPI 发送过程，防止多个 `membarrier` 调用并发执行导致 IPI 风暴或状态不一致。\n\n- **`SERIALIZE_IPI()`**  \n  宏封装，使用 `membarrier_ipi_mutex` 实现 IPI 发送的串行化。\n\n## 3. 关键实现\n\n### 内存屏障语义保证\n\n文件顶部的注释详细描述了五种关键内存顺序场景（A–E），说明为何在 `membarrier()` 调用前后必须插入 `smp_mb()`：\n\n- **场景 A**：确保调用者 CPU 在 `membarrier()` 之前的写操作，在其他 CPU 收到 IPI 并执行屏障后对其可见。\n- **场景 B**：确保其他 CPU 在 IPI 屏障前的写操作，在调用者 CPU 执行 `membarrier()` 后对其可见。\n- **场景 C–E**：处理线程切换、`exit_mm`、kthread 使用/释放 mm 等边界情况，确保 `membarrier` 能正确识别用户态上下文并施加屏障。\n\n这些场景共同要求 `membarrier()` 实现必须在发送 IPI **前**和**后**各执行一次 `smp_mb()`，以建立完整的全局内存顺序。\n\n### IPI 分发机制\n\n- 根据不同的 `membarrier` 命令类型（如全局、私有、带 rseq 或 sync_core），选择对应的 IPI 处理函数。\n- 使用 `mutex` 保护 IPI 发送过程，避免并发调用导致性能下降或状态竞争。\n- 对于私有命令（如 `PRIVATE_EXPEDITED`），仅向共享同一 `mm_struct` 的 CPU 发送 IPI。\n\n### 状态管理\n\n- 每个 `mm_struct` 包含一个 `membarrier_state` 原子变量，记录该地址空间已注册的 `membarrier` 命令类型。\n- 每个 per-CPU runqueue 也缓存一份 `membarrier_state`，通过 `ipi_sync_rq_state` 保持与 `mm_struct` 同步，加速后续命令的判断。\n- `exec` 时调用 `membarrier_exec_mmap` 重置状态，防止子进程继承父进程的注册状态。\n\n### 条件编译支持\n\n- `CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE`：启用 `SYNC_CORE` 相关命令。\n- `CONFIG_RSEQ`：启用 `RSEQ` 相关命令及 `rseq_preempt` 调用。\n\n## 4. 依赖关系\n\n- **调度子系统（sched）**：依赖 runqueue（`rq`）结构和 CPU 上下文切换逻辑，用于判断当前是否处于用户态及 mm 匹配。\n- **内存管理（mm）**：依赖 `mm_struct` 及其生命周期管理（如 `exec_mmap`、`exit_mm`）。\n- **RSEQ 子系统**：当启用 `CONFIG_RSEQ` 时，调用 `rseq_preempt()` 以维护 restartable sequences 的一致性。\n- **SMP 原语**：依赖 `smp_mb()`、`smp_call_function_many()` 等 SMP 内存屏障和 IPI 接口。\n- **架构支持**：部分命令（如 `SYNC_CORE`）依赖特定架构实现 `sync_core_before_usermode()`。\n\n## 5. 使用场景\n\n- **用户空间无锁编程**：应用程序使用 `membarrier(SYS_MEMBARRIER_CMD_GLOBAL_EXPEDITED)` 替代在每个读线程中插入 `smp_load_acquire()`，简化代码并提升性能。\n- **RSEQ（Restartable Sequences）**：配合 `membarrier(SYS_MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ)` 确保在抢占或迁移后 rseq 区域的原子性。\n- **实时或低延迟系统**：通过私有命令（`PRIVATE_EXPEDITED`）仅对特定进程组施加屏障，减少系统范围开销。\n- **动态代码生成/热更新**：使用 `SYNC_CORE` 命令确保指令缓存一致性，适用于 JIT 编译器等场景。\n- **进程生命周期管理**：在 `exec` 时自动清理 `membarrier` 注册状态，保证新程序映像的干净执行环境。",
      "similarity": 0.6464890837669373,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 1,
          "end_line": 167,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Copyright (C) 2010-2017 Mathieu Desnoyers <mathieu.desnoyers@efficios.com>",
            " *",
            " * membarrier system call",
            " */",
            "",
            "/*",
            " * For documentation purposes, here are some membarrier ordering",
            " * scenarios to keep in mind:",
            " *",
            " * A) Userspace thread execution after IPI vs membarrier's memory",
            " *    barrier before sending the IPI",
            " *",
            " * Userspace variables:",
            " *",
            " * int x = 0, y = 0;",
            " *",
            " * The memory barrier at the start of membarrier() on CPU0 is necessary in",
            " * order to enforce the guarantee that any writes occurring on CPU0 before",
            " * the membarrier() is executed will be visible to any code executing on",
            " * CPU1 after the IPI-induced memory barrier:",
            " *",
            " *         CPU0                              CPU1",
            " *",
            " *         x = 1",
            " *         membarrier():",
            " *           a: smp_mb()",
            " *           b: send IPI                       IPI-induced mb",
            " *           c: smp_mb()",
            " *         r2 = y",
            " *                                           y = 1",
            " *                                           barrier()",
            " *                                           r1 = x",
            " *",
            " *                     BUG_ON(r1 == 0 && r2 == 0)",
            " *",
            " * The write to y and load from x by CPU1 are unordered by the hardware,",
            " * so it's possible to have \"r1 = x\" reordered before \"y = 1\" at any",
            " * point after (b).  If the memory barrier at (a) is omitted, then \"x = 1\"",
            " * can be reordered after (a) (although not after (c)), so we get r1 == 0",
            " * and r2 == 0.  This violates the guarantee that membarrier() is",
            " * supposed by provide.",
            " *",
            " * The timing of the memory barrier at (a) has to ensure that it executes",
            " * before the IPI-induced memory barrier on CPU1.",
            " *",
            " * B) Userspace thread execution before IPI vs membarrier's memory",
            " *    barrier after completing the IPI",
            " *",
            " * Userspace variables:",
            " *",
            " * int x = 0, y = 0;",
            " *",
            " * The memory barrier at the end of membarrier() on CPU0 is necessary in",
            " * order to enforce the guarantee that any writes occurring on CPU1 before",
            " * the membarrier() is executed will be visible to any code executing on",
            " * CPU0 after the membarrier():",
            " *",
            " *         CPU0                              CPU1",
            " *",
            " *                                           x = 1",
            " *                                           barrier()",
            " *                                           y = 1",
            " *         r2 = y",
            " *         membarrier():",
            " *           a: smp_mb()",
            " *           b: send IPI                       IPI-induced mb",
            " *           c: smp_mb()",
            " *         r1 = x",
            " *         BUG_ON(r1 == 0 && r2 == 1)",
            " *",
            " * The writes to x and y are unordered by the hardware, so it's possible to",
            " * have \"r2 = 1\" even though the write to x doesn't execute until (b).  If",
            " * the memory barrier at (c) is omitted then \"r1 = x\" can be reordered",
            " * before (b) (although not before (a)), so we get \"r1 = 0\".  This violates",
            " * the guarantee that membarrier() is supposed to provide.",
            " *",
            " * The timing of the memory barrier at (c) has to ensure that it executes",
            " * after the IPI-induced memory barrier on CPU1.",
            " *",
            " * C) Scheduling userspace thread -> kthread -> userspace thread vs membarrier",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *           a: smp_mb()",
            " *                                           d: switch to kthread (includes mb)",
            " *           b: read rq->curr->mm == NULL",
            " *                                           e: switch to user (includes mb)",
            " *           c: smp_mb()",
            " *",
            " * Using the scenario from (A), we can show that (a) needs to be paired",
            " * with (e). Using the scenario from (B), we can show that (c) needs to",
            " * be paired with (d).",
            " *",
            " * D) exit_mm vs membarrier",
            " *",
            " * Two thread groups are created, A and B.  Thread group B is created by",
            " * issuing clone from group A with flag CLONE_VM set, but not CLONE_THREAD.",
            " * Let's assume we have a single thread within each thread group (Thread A",
            " * and Thread B).  Thread A runs on CPU0, Thread B runs on CPU1.",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *             a: smp_mb()",
            " *                                           exit_mm():",
            " *                                             d: smp_mb()",
            " *                                             e: current->mm = NULL",
            " *             b: read rq->curr->mm == NULL",
            " *             c: smp_mb()",
            " *",
            " * Using scenario (B), we can show that (c) needs to be paired with (d).",
            " *",
            " * E) kthread_{use,unuse}_mm vs membarrier",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *           a: smp_mb()",
            " *                                           kthread_unuse_mm()",
            " *                                             d: smp_mb()",
            " *                                             e: current->mm = NULL",
            " *           b: read rq->curr->mm == NULL",
            " *                                           kthread_use_mm()",
            " *                                             f: current->mm = mm",
            " *                                             g: smp_mb()",
            " *           c: smp_mb()",
            " *",
            " * Using the scenario from (A), we can show that (a) needs to be paired",
            " * with (g). Using the scenario from (B), we can show that (c) needs to",
            " * be paired with (d).",
            " */",
            "",
            "/*",
            " * Bitmask made from a \"or\" of all commands within enum membarrier_cmd,",
            " * except MEMBARRIER_CMD_QUERY.",
            " */",
            "#ifdef CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t\t\t\\",
            "\t(MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE)",
            "#else",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t0",
            "#endif",
            "",
            "#ifdef CONFIG_RSEQ",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t\t\\",
            "\t(MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ)",
            "#else",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t0",
            "#endif",
            "",
            "#define MEMBARRIER_CMD_BITMASK\t\t\t\t\t\t\\",
            "\t(MEMBARRIER_CMD_GLOBAL | MEMBARRIER_CMD_GLOBAL_EXPEDITED\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED\t\t\t\\",
            "\t| MEMBARRIER_CMD_PRIVATE_EXPEDITED\t\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED\t\t\t\\",
            "\t| MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t\t\\",
            "\t| MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t\t\t\\",
            "\t| MEMBARRIER_CMD_GET_REGISTRATIONS)",
            "",
            "static DEFINE_MUTEX(membarrier_ipi_mutex);",
            "#define SERIALIZE_IPI() guard(mutex)(&membarrier_ipi_mutex)",
            ""
          ],
          "function_name": null,
          "description": "定义内存屏障命令位掩码和互斥锁，用于协调多处理器间内存顺序保证。",
          "similarity": 0.6283371448516846
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 168,
          "end_line": 306,
          "content": [
            "static void ipi_mb(void *info)",
            "{",
            "\tsmp_mb();\t/* IPIs should be serializing but paranoid. */",
            "}",
            "static void ipi_sync_core(void *info)",
            "{",
            "\t/*",
            "\t * The smp_mb() in membarrier after all the IPIs is supposed to",
            "\t * ensure that memory on remote CPUs that occur before the IPI",
            "\t * become visible to membarrier()'s caller -- see scenario B in",
            "\t * the big comment at the top of this file.",
            "\t *",
            "\t * A sync_core() would provide this guarantee, but",
            "\t * sync_core_before_usermode() might end up being deferred until",
            "\t * after membarrier()'s smp_mb().",
            "\t */",
            "\tsmp_mb();\t/* IPIs should be serializing but paranoid. */",
            "",
            "\tsync_core_before_usermode();",
            "}",
            "static void ipi_rseq(void *info)",
            "{",
            "\t/*",
            "\t * Ensure that all stores done by the calling thread are visible",
            "\t * to the current task before the current task resumes.  We could",
            "\t * probably optimize this away on most architectures, but by the",
            "\t * time we've already sent an IPI, the cost of the extra smp_mb()",
            "\t * is negligible.",
            "\t */",
            "\tsmp_mb();",
            "\trseq_preempt(current);",
            "}",
            "static void ipi_sync_rq_state(void *info)",
            "{",
            "\tstruct mm_struct *mm = (struct mm_struct *) info;",
            "",
            "\tif (current->mm != mm)",
            "\t\treturn;",
            "\tthis_cpu_write(runqueues.membarrier_state,",
            "\t\t       atomic_read(&mm->membarrier_state));",
            "\t/*",
            "\t * Issue a memory barrier after setting",
            "\t * MEMBARRIER_STATE_GLOBAL_EXPEDITED in the current runqueue to",
            "\t * guarantee that no memory access following registration is reordered",
            "\t * before registration.",
            "\t */",
            "\tsmp_mb();",
            "}",
            "void membarrier_exec_mmap(struct mm_struct *mm)",
            "{",
            "\t/*",
            "\t * Issue a memory barrier before clearing membarrier_state to",
            "\t * guarantee that no memory access prior to exec is reordered after",
            "\t * clearing this state.",
            "\t */",
            "\tsmp_mb();",
            "\tatomic_set(&mm->membarrier_state, 0);",
            "\t/*",
            "\t * Keep the runqueue membarrier_state in sync with this mm",
            "\t * membarrier_state.",
            "\t */",
            "\tthis_cpu_write(runqueues.membarrier_state, 0);",
            "}",
            "void membarrier_update_current_mm(struct mm_struct *next_mm)",
            "{",
            "\tstruct rq *rq = this_rq();",
            "\tint membarrier_state = 0;",
            "",
            "\tif (next_mm)",
            "\t\tmembarrier_state = atomic_read(&next_mm->membarrier_state);",
            "\tif (READ_ONCE(rq->membarrier_state) == membarrier_state)",
            "\t\treturn;",
            "\tWRITE_ONCE(rq->membarrier_state, membarrier_state);",
            "}",
            "static int membarrier_global_expedited(void)",
            "{",
            "\tint cpu;",
            "\tcpumask_var_t tmpmask;",
            "",
            "\tif (num_online_cpus() == 1)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Matches memory barriers after rq->curr modification in",
            "\t * scheduler.",
            "\t */",
            "\tsmp_mb();\t/* system call entry is not a mb. */",
            "",
            "\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "\trcu_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\t/*",
            "\t\t * Skipping the current CPU is OK even through we can be",
            "\t\t * migrated at any point. The current CPU, at the point",
            "\t\t * where we read raw_smp_processor_id(), is ensured to",
            "\t\t * be in program order with respect to the caller",
            "\t\t * thread. Therefore, we can skip this CPU from the",
            "\t\t * iteration.",
            "\t\t */",
            "\t\tif (cpu == raw_smp_processor_id())",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!(READ_ONCE(cpu_rq(cpu)->membarrier_state) &",
            "\t\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * Skip the CPU if it runs a kernel thread which is not using",
            "\t\t * a task mm.",
            "\t\t */",
            "\t\tp = rcu_dereference(cpu_rq(cpu)->curr);",
            "\t\tif (!p->mm)",
            "\t\t\tcontinue;",
            "",
            "\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tpreempt_disable();",
            "\tsmp_call_function_many(tmpmask, ipi_mb, NULL, 1);",
            "\tpreempt_enable();",
            "",
            "\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\t/*",
            "\t * Memory barrier on the caller thread _after_ we finished",
            "\t * waiting for the last IPI. Matches memory barriers before",
            "\t * rq->curr modification in scheduler.",
            "\t */",
            "\tsmp_mb();\t/* exit from system call is not a mb */",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ipi_mb, ipi_sync_core, ipi_rseq, ipi_sync_rq_state, membarrier_exec_mmap, membarrier_update_current_mm, membarrier_global_expedited",
          "description": "实现多种IPI处理函数及全局快速内存屏障逻辑，通过跨CPU调用来确保内存访问顺序一致性。",
          "similarity": 0.5493664741516113
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 555,
          "end_line": 587,
          "content": [
            "static int membarrier_get_registrations(void)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint registrations_mask = 0, membarrier_state, i;",
            "\tstatic const int states[] = {",
            "\t\tMEMBARRIER_STATE_GLOBAL_EXPEDITED |",
            "\t\t\tMEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY",
            "\t};",
            "\tstatic const int registration_cmds[] = {",
            "\t\tMEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ",
            "\t};",
            "\tBUILD_BUG_ON(ARRAY_SIZE(states) != ARRAY_SIZE(registration_cmds));",
            "",
            "\tmembarrier_state = atomic_read(&mm->membarrier_state);",
            "\tfor (i = 0; i < ARRAY_SIZE(states); ++i) {",
            "\t\tif (membarrier_state & states[i]) {",
            "\t\t\tregistrations_mask |= registration_cmds[i];",
            "\t\t\tmembarrier_state &= ~states[i];",
            "\t\t}",
            "\t}",
            "\tWARN_ON_ONCE(membarrier_state != 0);",
            "\treturn registrations_mask;",
            "}"
          ],
          "function_name": "membarrier_get_registrations",
          "description": "解析当前进程的内存屏障注册状态，将有效状态转换为对应的注册命令掩码。",
          "similarity": 0.5392320156097412
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 436,
          "end_line": 551,
          "content": [
            "static int sync_runqueues_membarrier_state(struct mm_struct *mm)",
            "{",
            "\tint membarrier_state = atomic_read(&mm->membarrier_state);",
            "\tcpumask_var_t tmpmask;",
            "\tint cpu;",
            "",
            "\tif (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1) {",
            "\t\tthis_cpu_write(runqueues.membarrier_state, membarrier_state);",
            "",
            "\t\t/*",
            "\t\t * For single mm user, we can simply issue a memory barrier",
            "\t\t * after setting MEMBARRIER_STATE_GLOBAL_EXPEDITED in the",
            "\t\t * mm and in the current runqueue to guarantee that no memory",
            "\t\t * access following registration is reordered before",
            "\t\t * registration.",
            "\t\t */",
            "\t\tsmp_mb();",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * For mm with multiple users, we need to ensure all future",
            "\t * scheduler executions will observe @mm's new membarrier",
            "\t * state.",
            "\t */",
            "\tsynchronize_rcu();",
            "",
            "\t/*",
            "\t * For each cpu runqueue, if the task's mm match @mm, ensure that all",
            "\t * @mm's membarrier state set bits are also set in the runqueue's",
            "\t * membarrier state. This ensures that a runqueue scheduling",
            "\t * between threads which are users of @mm has its membarrier state",
            "\t * updated.",
            "\t */",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "\trcu_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct rq *rq = cpu_rq(cpu);",
            "\t\tstruct task_struct *p;",
            "",
            "\t\tp = rcu_dereference(rq->curr);",
            "\t\tif (p && p->mm == mm)",
            "\t\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\ton_each_cpu_mask(tmpmask, ipi_sync_rq_state, mm, true);",
            "",
            "\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\treturn 0;",
            "}",
            "static int membarrier_register_global_expedited(void)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint ret;",
            "",
            "\tif (atomic_read(&mm->membarrier_state) &",
            "\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)",
            "\t\treturn 0;",
            "\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);",
            "\tret = sync_runqueues_membarrier_state(mm);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,",
            "\t\t  &mm->membarrier_state);",
            "",
            "\treturn 0;",
            "}",
            "static int membarrier_register_private_expedited(int flags)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint ready_state = MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY,",
            "\t    set_state = MEMBARRIER_STATE_PRIVATE_EXPEDITED,",
            "\t    ret;",
            "",
            "\tif (flags == MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))",
            "\t\t\treturn -EINVAL;",
            "\t\tready_state =",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY;",
            "\t} else if (flags == MEMBARRIER_FLAG_RSEQ) {",
            "\t\tif (!IS_ENABLED(CONFIG_RSEQ))",
            "\t\t\treturn -EINVAL;",
            "\t\tready_state =",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY;",
            "\t} else {",
            "\t\tWARN_ON_ONCE(flags);",
            "\t}",
            "",
            "\t/*",
            "\t * We need to consider threads belonging to different thread",
            "\t * groups, which use the same mm. (CLONE_VM but not",
            "\t * CLONE_THREAD).",
            "\t */",
            "\tif ((atomic_read(&mm->membarrier_state) & ready_state) == ready_state)",
            "\t\treturn 0;",
            "\tif (flags & MEMBARRIER_FLAG_SYNC_CORE)",
            "\t\tset_state |= MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE;",
            "\tif (flags & MEMBARRIER_FLAG_RSEQ)",
            "\t\tset_state |= MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ;",
            "\tatomic_or(set_state, &mm->membarrier_state);",
            "\tret = sync_runqueues_membarrier_state(mm);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tatomic_or(ready_state, &mm->membarrier_state);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sync_runqueues_membarrier_state, membarrier_register_global_expedited, membarrier_register_private_expedited",
          "description": "同步运行队列内存屏障状态，通过RCU和IPI确保多用户场景下状态传播的正确性。",
          "similarity": 0.5374110341072083
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 314,
          "end_line": 434,
          "content": [
            "static int membarrier_private_expedited(int flags, int cpu_id)",
            "{",
            "\tcpumask_var_t tmpmask;",
            "\tstruct mm_struct *mm = current->mm;",
            "\tsmp_call_func_t ipi_func = ipi_mb;",
            "",
            "\tif (flags == MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY))",
            "\t\t\treturn -EPERM;",
            "\t\tipi_func = ipi_sync_core;",
            "\t\tprepare_sync_core_cmd(mm);",
            "\t} else if (flags == MEMBARRIER_FLAG_RSEQ) {",
            "\t\tif (!IS_ENABLED(CONFIG_RSEQ))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY))",
            "\t\t\treturn -EPERM;",
            "\t\tipi_func = ipi_rseq;",
            "\t} else {",
            "\t\tWARN_ON_ONCE(flags);",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY))",
            "\t\t\treturn -EPERM;",
            "\t}",
            "",
            "\tif (flags != MEMBARRIER_FLAG_SYNC_CORE &&",
            "\t    (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Matches memory barriers after rq->curr modification in",
            "\t * scheduler.",
            "\t *",
            "\t * On RISC-V, this barrier pairing is also needed for the",
            "\t * SYNC_CORE command when switching between processes, cf.",
            "\t * the inline comments in membarrier_arch_switch_mm().",
            "\t */",
            "\tsmp_mb();\t/* system call entry is not a mb. */",
            "",
            "\tif (cpu_id < 0 && !zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "",
            "\tif (cpu_id >= 0) {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\tif (cpu_id >= nr_cpu_ids || !cpu_online(cpu_id))",
            "\t\t\tgoto out;",
            "\t\trcu_read_lock();",
            "\t\tp = rcu_dereference(cpu_rq(cpu_id)->curr);",
            "\t\tif (!p || p->mm != mm) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tint cpu;",
            "",
            "\t\trcu_read_lock();",
            "\t\tfor_each_online_cpu(cpu) {",
            "\t\t\tstruct task_struct *p;",
            "",
            "\t\t\tp = rcu_dereference(cpu_rq(cpu)->curr);",
            "\t\t\tif (p && p->mm == mm)",
            "\t\t\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t}",
            "",
            "\tif (cpu_id >= 0) {",
            "\t\t/*",
            "\t\t * smp_call_function_single() will call ipi_func() if cpu_id",
            "\t\t * is the calling CPU.",
            "\t\t */",
            "\t\tsmp_call_function_single(cpu_id, ipi_func, NULL, 1);",
            "\t} else {",
            "\t\t/*",
            "\t\t * For regular membarrier, we can save a few cycles by",
            "\t\t * skipping the current cpu -- we're about to do smp_mb()",
            "\t\t * below, and if we migrate to a different cpu, this cpu",
            "\t\t * and the new cpu will execute a full barrier in the",
            "\t\t * scheduler.",
            "\t\t *",
            "\t\t * For SYNC_CORE, we do need a barrier on the current cpu --",
            "\t\t * otherwise, if we are migrated and replaced by a different",
            "\t\t * task in the same mm just before, during, or after",
            "\t\t * membarrier, we will end up with some thread in the mm",
            "\t\t * running without a core sync.",
            "\t\t *",
            "\t\t * For RSEQ, don't rseq_preempt() the caller.  User code",
            "\t\t * is not supposed to issue syscalls at all from inside an",
            "\t\t * rseq critical section.",
            "\t\t */",
            "\t\tif (flags != MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\t\tpreempt_disable();",
            "\t\t\tsmp_call_function_many(tmpmask, ipi_func, NULL, true);",
            "\t\t\tpreempt_enable();",
            "\t\t} else {",
            "\t\t\ton_each_cpu_mask(tmpmask, ipi_func, NULL, true);",
            "\t\t}",
            "\t}",
            "",
            "out:",
            "\tif (cpu_id < 0)",
            "\t\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\t/*",
            "\t * Memory barrier on the caller thread _after_ we finished",
            "\t * waiting for the last IPI. Matches memory barriers before",
            "\t * rq->curr modification in scheduler.",
            "\t */",
            "\tsmp_mb();\t/* exit from system call is not a mb */",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "membarrier_private_expedited",
          "description": "处理私有快速内存屏障，根据配置标志选择不同同步方式并验证状态有效性。",
          "similarity": 0.5364813804626465
        }
      ]
    },
    {
      "source_file": "kernel/vmcore_info.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:49:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmcore_info.c`\n\n---\n\n# vmcore_info.c 技术文档\n\n## 1. 文件概述\n\n`vmcore_info.c` 是 Linux 内核中用于支持内核崩溃转储（crash dump）机制的核心文件之一。该文件负责在系统崩溃前收集并保存关键的内核元数据（称为 **vmcoreinfo**），这些信息在后续使用 `crash` 或 `gdb` 等工具分析 vmcore 文件时至关重要。vmcoreinfo 包含了内核符号地址、数据结构布局、编译时配置、页表信息等，使得用户空间工具能够正确解析崩溃内存镜像。\n\n## 2. 核心功能\n\n### 全局变量\n- `vmcoreinfo_data`：指向动态分配的缓冲区，用于存储 vmcoreinfo 的文本内容。\n- `vmcoreinfo_size`：当前 `vmcoreinfo_data` 中已写入的数据长度。\n- `vmcoreinfo_note`：指向 ELF note 结构的内存区域，用于将 vmcoreinfo 嵌入到 ELF 格式的 crash dump 中。\n- `vmcoreinfo_data_safecopy`：指向位于“崩溃安全内存”中的 vmcoreinfo 数据副本，确保在系统崩溃后仍可访问。\n\n### 主要函数\n- `append_elf_note()`：将指定名称、类型和数据追加为一个 ELF note 条目。\n- `final_note()`：在 ELF note 缓冲区末尾写入终止符（空 note）。\n- `update_vmcoreinfo_note()`：使用当前 `vmcoreinfo_data` 重新构建 `vmcoreinfo_note`。\n- `crash_update_vmcoreinfo_safecopy()`：将 vmcoreinfo 数据复制到安全内存区域，并更新 `vmcoreinfo_data_safecopy` 指针。\n- `crash_save_vmcoreinfo()`：在系统即将崩溃前调用，更新崩溃时间戳并刷新 vmcoreinfo note。\n- `vmcoreinfo_append_str()`：格式化字符串并追加到 `vmcoreinfo_data` 缓冲区，防止溢出。\n- `arch_crash_save_vmcoreinfo()`（弱符号）：架构特定的 vmcoreinfo 扩展点，可由各架构实现。\n- `paddr_vmcoreinfo_note()`（弱符号）：返回 `vmcoreinfo_note` 的物理地址，供 kexec 使用。\n- `crash_save_vmcoreinfo_init()`：模块初始化函数，分配内存并填充初始 vmcoreinfo 内容。\n\n### 宏辅助\n文件大量使用 `VMCOREINFO_*` 宏（定义在其他头文件中），用于便捷地向 `vmcoreinfo_data` 添加：\n- 符号地址（`VMCOREINFO_SYMBOL`）\n- 结构体偏移量（`VMCOREINFO_OFFSET`）\n- 结构体/类型大小（`VMCOREINFO_STRUCT_SIZE`, `VMCOREINFO_SIZE`）\n- 数组长度（`VMCOREINFO_LENGTH`）\n- 常量数值（`VMCOREINFO_NUMBER`）\n- 字符串值（如 `VMCOREINFO_OSRELEASE`）\n\n## 3. 关键实现\n\n### ELF Note 构建\n- `vmcoreinfo_note` 被格式化为标准的 **ELF note**（`PT_NOTE` 类型），名称为 `\"VMCOREINFO\"`。\n- `append_elf_note()` 按照 ELF 规范对齐 name 和 desc 字段（以 `Elf_Word` 为单位向上取整）。\n- `final_note()` 写入一个空 note 作为终止标记，符合 ELF note 列表惯例。\n\n### 安全副本机制\n- 为防止系统崩溃时常规内存损坏，`crash_update_vmcoreinfo_safecopy()` 允许将 vmcoreinfo 复制到预先保留的“崩溃安全内存”中。\n- 在 `crash_save_vmcoreinfo()` 中，若存在安全副本，则临时切换 `vmcoreinfo_data` 指针指向该副本，确保生成的 note 基于可靠数据。\n\n### 初始化内容\n- `crash_save_vmcoreinfo_init()` 在内核初始化早期（`subsys_initcall` 阶段）执行：\n  - 分配一页内存（`get_zeroed_page`）作为 `vmcoreinfo_data`。\n  - 分配精确大小的内存（`alloc_pages_exact`）作为 `vmcoreinfo_note`。\n  - 填充大量内核元数据，包括：\n    - 内核版本（`init_uts_ns.name.release`）\n    - Build ID（用于匹配调试符号）\n    - 页大小（`PAGE_SIZE`）\n    - 关键符号地址（如 `_stext`, `mem_map`, `swapper_pg_dir` 等）\n    - 内存管理结构布局（`page`, `pglist_data`, `zone` 等的偏移和大小）\n    - 位图和标志常量（如 `PG_lru`, `PG_head_mask` 等）\n    - KASLR 相关符号（若启用 `CONFIG_KALLSYMS`）\n    - 架构特定信息（通过 `arch_crash_save_vmcoreinfo()`）\n\n### 溢出保护\n- `vmcoreinfo_append_str()` 使用 `vscnprintf` 格式化字符串，并通过 `min()` 限制写入长度，确保不超过 `VMCOREINFO_BYTES`（通常为一页）。\n- 若缓冲区满，会触发 `WARN_ONCE` 提示截断。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/kexec.h>`：kexec 和 crash dump 核心接口。\n  - `<linux/buildid.h>`：获取内核 Build ID。\n  - `<linux/utsname.h>`：获取内核版本信息。\n  - `<asm/sections.h>`：访问内核链接段符号（如 `_stext`）。\n  - `\"kallsyms_internal.h\"`：访问 kallsyms 内部符号表。\n  - `\"kexec_internal.h\"`：kexec 内部辅助函数和宏定义（如 `VMCOREINFO_*`）。\n- **架构依赖**：\n  - 通过弱符号 `arch_crash_save_vmcoreinfo()` 和 `paddr_vmcoreinfo_note()` 允许架构代码覆盖默认行为。\n  - 条件编译依赖多个内核配置选项（如 `CONFIG_NUMA`, `CONFIG_SPARSEMEM`, `CONFIG_KALLSYMS` 等）。\n- **内存管理**：\n  - 依赖 `memblock` 和 `vmalloc` 相关接口进行内存分配。\n  - 与 `log_buf_vmcoreinfo_setup()`（来自 printk 子系统）集成以导出日志缓冲区信息。\n\n## 5. 使用场景\n\n- **内核崩溃转储（kdump/kexec）**：\n  - 当系统发生严重错误（如 panic）并配置了 kdump 时，`crash_save_vmcoreinfo()` 被调用，更新崩溃时间并确保 vmcoreinfo note 最新。\n  - 第二内核（capture kernel）通过 `paddr_vmcoreinfo_note()` 获取第一内核的 vmcoreinfo 物理地址，并将其嵌入到生成的 vmcore 文件中。\n- **离线内存分析**：\n  - 用户空间工具（如 `crash` utility）加载 vmcore 文件时，解析其中的 VMCOREINFO note，利用其中的元数据正确解读内存布局、符号地址和数据结构，从而进行有效的故障诊断。\n- **热补丁与调试**：\n  - vmcoreinfo 提供的结构体偏移和符号信息也可用于内核热补丁（livepatch）或高级调试场景，确保运行时与编译时视图一致。",
      "similarity": 0.6439744234085083,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/vmcore_info.c",
          "start_line": 51,
          "end_line": 219,
          "content": [
            "void final_note(Elf_Word *buf)",
            "{",
            "\tmemset(buf, 0, sizeof(struct elf_note));",
            "}",
            "static void update_vmcoreinfo_note(void)",
            "{",
            "\tu32 *buf = vmcoreinfo_note;",
            "",
            "\tif (!vmcoreinfo_size)",
            "\t\treturn;",
            "\tbuf = append_elf_note(buf, VMCOREINFO_NOTE_NAME, 0, vmcoreinfo_data,",
            "\t\t\t      vmcoreinfo_size);",
            "\tfinal_note(buf);",
            "}",
            "void crash_update_vmcoreinfo_safecopy(void *ptr)",
            "{",
            "\tif (ptr)",
            "\t\tmemcpy(ptr, vmcoreinfo_data, vmcoreinfo_size);",
            "",
            "\tvmcoreinfo_data_safecopy = ptr;",
            "}",
            "void crash_save_vmcoreinfo(void)",
            "{",
            "\tif (!vmcoreinfo_note)",
            "\t\treturn;",
            "",
            "\t/* Use the safe copy to generate vmcoreinfo note if have */",
            "\tif (vmcoreinfo_data_safecopy)",
            "\t\tvmcoreinfo_data = vmcoreinfo_data_safecopy;",
            "",
            "\tvmcoreinfo_append_str(\"CRASHTIME=%lld\\n\", ktime_get_real_seconds());",
            "\tupdate_vmcoreinfo_note();",
            "}",
            "void vmcoreinfo_append_str(const char *fmt, ...)",
            "{",
            "\tva_list args;",
            "\tchar buf[0x50];",
            "\tsize_t r;",
            "",
            "\tva_start(args, fmt);",
            "\tr = vscnprintf(buf, sizeof(buf), fmt, args);",
            "\tva_end(args);",
            "",
            "\tr = min(r, (size_t)VMCOREINFO_BYTES - vmcoreinfo_size);",
            "",
            "\tmemcpy(&vmcoreinfo_data[vmcoreinfo_size], buf, r);",
            "",
            "\tvmcoreinfo_size += r;",
            "",
            "\tWARN_ONCE(vmcoreinfo_size == VMCOREINFO_BYTES,",
            "\t\t  \"vmcoreinfo data exceeds allocated size, truncating\");",
            "}",
            "void __weak arch_crash_save_vmcoreinfo(void)",
            "{}",
            "phys_addr_t __weak paddr_vmcoreinfo_note(void)",
            "{",
            "\treturn __pa(vmcoreinfo_note);",
            "}",
            "static int __init crash_save_vmcoreinfo_init(void)",
            "{",
            "\tvmcoreinfo_data = (unsigned char *)get_zeroed_page(GFP_KERNEL);",
            "\tif (!vmcoreinfo_data) {",
            "\t\tpr_warn(\"Memory allocation for vmcoreinfo_data failed\\n\");",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\tvmcoreinfo_note = alloc_pages_exact(VMCOREINFO_NOTE_SIZE,",
            "\t\t\t\t\t\tGFP_KERNEL | __GFP_ZERO);",
            "\tif (!vmcoreinfo_note) {",
            "\t\tfree_page((unsigned long)vmcoreinfo_data);",
            "\t\tvmcoreinfo_data = NULL;",
            "\t\tpr_warn(\"Memory allocation for vmcoreinfo_note failed\\n\");",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\tVMCOREINFO_OSRELEASE(init_uts_ns.name.release);",
            "\tVMCOREINFO_BUILD_ID();",
            "\tVMCOREINFO_PAGESIZE(PAGE_SIZE);",
            "",
            "\tVMCOREINFO_SYMBOL(init_uts_ns);",
            "\tVMCOREINFO_OFFSET(uts_namespace, name);",
            "\tVMCOREINFO_SYMBOL(node_online_map);",
            "#ifdef CONFIG_MMU",
            "\tVMCOREINFO_SYMBOL_ARRAY(swapper_pg_dir);",
            "#endif",
            "\tVMCOREINFO_SYMBOL(_stext);",
            "\tvmcoreinfo_append_str(\"NUMBER(VMALLOC_START)=0x%lx\\n\", (unsigned long) VMALLOC_START);",
            "",
            "#ifndef CONFIG_NUMA",
            "\tVMCOREINFO_SYMBOL(mem_map);",
            "\tVMCOREINFO_SYMBOL(contig_page_data);",
            "#endif",
            "#ifdef CONFIG_SPARSEMEM_VMEMMAP",
            "\tVMCOREINFO_SYMBOL_ARRAY(vmemmap);",
            "#endif",
            "#ifdef CONFIG_SPARSEMEM",
            "\tVMCOREINFO_SYMBOL_ARRAY(mem_section);",
            "\tVMCOREINFO_LENGTH(mem_section, NR_SECTION_ROOTS);",
            "\tVMCOREINFO_STRUCT_SIZE(mem_section);",
            "\tVMCOREINFO_OFFSET(mem_section, section_mem_map);",
            "\tVMCOREINFO_NUMBER(SECTION_SIZE_BITS);",
            "\tVMCOREINFO_NUMBER(MAX_PHYSMEM_BITS);",
            "#endif",
            "\tVMCOREINFO_STRUCT_SIZE(page);",
            "\tVMCOREINFO_STRUCT_SIZE(pglist_data);",
            "\tVMCOREINFO_STRUCT_SIZE(zone);",
            "\tVMCOREINFO_STRUCT_SIZE(free_area);",
            "\tVMCOREINFO_STRUCT_SIZE(list_head);",
            "\tVMCOREINFO_SIZE(nodemask_t);",
            "\tVMCOREINFO_OFFSET(page, flags);",
            "\tVMCOREINFO_OFFSET(page, _refcount);",
            "\tVMCOREINFO_OFFSET(page, mapping);",
            "\tVMCOREINFO_OFFSET(page, lru);",
            "\tVMCOREINFO_OFFSET(page, _mapcount);",
            "\tVMCOREINFO_OFFSET(page, private);",
            "\tVMCOREINFO_OFFSET(page, compound_head);",
            "\tVMCOREINFO_OFFSET(pglist_data, node_zones);",
            "\tVMCOREINFO_OFFSET(pglist_data, nr_zones);",
            "#ifdef CONFIG_FLATMEM",
            "\tVMCOREINFO_OFFSET(pglist_data, node_mem_map);",
            "#endif",
            "\tVMCOREINFO_OFFSET(pglist_data, node_start_pfn);",
            "\tVMCOREINFO_OFFSET(pglist_data, node_spanned_pages);",
            "\tVMCOREINFO_OFFSET(pglist_data, node_id);",
            "\tVMCOREINFO_OFFSET(zone, free_area);",
            "\tVMCOREINFO_OFFSET(zone, vm_stat);",
            "\tVMCOREINFO_OFFSET(zone, spanned_pages);",
            "\tVMCOREINFO_OFFSET(free_area, free_list);",
            "\tVMCOREINFO_OFFSET(list_head, next);",
            "\tVMCOREINFO_OFFSET(list_head, prev);",
            "\tVMCOREINFO_LENGTH(zone.free_area, NR_PAGE_ORDERS);",
            "\tlog_buf_vmcoreinfo_setup();",
            "\tVMCOREINFO_LENGTH(free_area.free_list, MIGRATE_TYPES);",
            "\tVMCOREINFO_NUMBER(NR_FREE_PAGES);",
            "\tVMCOREINFO_NUMBER(PG_lru);",
            "\tVMCOREINFO_NUMBER(PG_private);",
            "\tVMCOREINFO_NUMBER(PG_swapcache);",
            "\tVMCOREINFO_NUMBER(PG_swapbacked);",
            "#define PAGE_SLAB_MAPCOUNT_VALUE\t(~PG_slab)",
            "\tVMCOREINFO_NUMBER(PAGE_SLAB_MAPCOUNT_VALUE);",
            "#ifdef CONFIG_MEMORY_FAILURE",
            "\tVMCOREINFO_NUMBER(PG_hwpoison);",
            "#endif",
            "\tVMCOREINFO_NUMBER(PG_head_mask);",
            "#define PAGE_BUDDY_MAPCOUNT_VALUE\t(~PG_buddy)",
            "\tVMCOREINFO_NUMBER(PAGE_BUDDY_MAPCOUNT_VALUE);",
            "#define PAGE_HUGETLB_MAPCOUNT_VALUE\t(~PG_hugetlb)",
            "\tVMCOREINFO_NUMBER(PAGE_HUGETLB_MAPCOUNT_VALUE);",
            "#define PAGE_OFFLINE_MAPCOUNT_VALUE\t(~PG_offline)",
            "\tVMCOREINFO_NUMBER(PAGE_OFFLINE_MAPCOUNT_VALUE);",
            "",
            "#ifdef CONFIG_KALLSYMS",
            "\tVMCOREINFO_SYMBOL(kallsyms_names);",
            "\tVMCOREINFO_SYMBOL(kallsyms_num_syms);",
            "\tVMCOREINFO_SYMBOL(kallsyms_token_table);",
            "\tVMCOREINFO_SYMBOL(kallsyms_token_index);",
            "#ifdef CONFIG_KALLSYMS_BASE_RELATIVE",
            "\tVMCOREINFO_SYMBOL(kallsyms_offsets);",
            "\tVMCOREINFO_SYMBOL(kallsyms_relative_base);",
            "#else",
            "\tVMCOREINFO_SYMBOL(kallsyms_addresses);",
            "#endif /* CONFIG_KALLSYMS_BASE_RELATIVE */",
            "#endif /* CONFIG_KALLSYMS */",
            "",
            "\tarch_crash_save_vmcoreinfo();",
            "\tupdate_vmcoreinfo_note();",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "final_note, update_vmcoreinfo_note, crash_update_vmcoreinfo_safecopy, crash_save_vmcoreinfo, vmcoreinfo_append_str, arch_crash_save_vmcoreinfo, paddr_vmcoreinfo_note, crash_save_vmcoreinfo_init",
          "description": "实现了崩溃时核心信息收集的核心逻辑。包含初始化内存分配、符号记录、偏移量设置、字符串追加等功能，通过vmcoreinfo_append_str动态扩展数据区域，最终调用架构相关的arch_crash_save_vmcoreinfo完成特殊符号注册，构建可用于用户空间解析的ELF note结构。",
          "similarity": 0.5788100361824036
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/vmcore_info.c",
          "start_line": 1,
          "end_line": 50,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * crash.c - kernel crash support code.",
            " * Copyright (C) 2002-2004 Eric Biederman  <ebiederm@xmission.com>",
            " */",
            "",
            "#include <linux/buildid.h>",
            "#include <linux/init.h>",
            "#include <linux/utsname.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sizes.h>",
            "#include <linux/kexec.h>",
            "#include <linux/memory.h>",
            "#include <linux/cpuhotplug.h>",
            "#include <linux/memblock.h>",
            "#include <linux/kmemleak.h>",
            "",
            "#include <asm/page.h>",
            "#include <asm/sections.h>",
            "",
            "#include <crypto/sha1.h>",
            "",
            "#include \"kallsyms_internal.h\"",
            "#include \"kexec_internal.h\"",
            "",
            "/* vmcoreinfo stuff */",
            "unsigned char *vmcoreinfo_data;",
            "size_t vmcoreinfo_size;",
            "u32 *vmcoreinfo_note;",
            "",
            "/* trusted vmcoreinfo, e.g. we can make a copy in the crash memory */",
            "static unsigned char *vmcoreinfo_data_safecopy;",
            "",
            "Elf_Word *append_elf_note(Elf_Word *buf, char *name, unsigned int type,",
            "\t\t\t  void *data, size_t data_len)",
            "{",
            "\tstruct elf_note *note = (struct elf_note *)buf;",
            "",
            "\tnote->n_namesz = strlen(name) + 1;",
            "\tnote->n_descsz = data_len;",
            "\tnote->n_type   = type;",
            "\tbuf += DIV_ROUND_UP(sizeof(*note), sizeof(Elf_Word));",
            "\tmemcpy(buf, name, note->n_namesz);",
            "\tbuf += DIV_ROUND_UP(note->n_namesz, sizeof(Elf_Word));",
            "\tmemcpy(buf, data, data_len);",
            "\tbuf += DIV_ROUND_UP(data_len, sizeof(Elf_Word));",
            "",
            "\treturn buf;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于生成核心转储（vmcore）信息的辅助函数和全局变量。append_elf_note函数用于构建ELF note结构体，将名称、类型和数据依次追加到缓冲区，通过计算对齐后的位置进行内存拷贝，实现ELF note的序列化操作。",
          "similarity": 0.5436031818389893
        }
      ]
    }
  ]
}