{
  "query": "mdadm配置与RAID级别选择",
  "timestamp": "2025-12-26 00:26:28",
  "retrieved_files": [
    {
      "source_file": "mm/damon/vaddr.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:53:40\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `damon\\vaddr.c`\n\n---\n\n# `damon/vaddr.c` 技术文档\n\n## 1. 文件概述\n\n`damon/vaddr.c` 是 Linux 内核中 DAMON（Data Access MONitor）子系统的一部分，专门用于在**虚拟地址空间**（Virtual Address Space）上实现监控原语。该文件提供了针对进程虚拟内存布局的区域初始化、内存映射分析以及与页表和 VMA（Virtual Memory Area）交互的核心逻辑，旨在高效地将复杂的虚拟地址空间抽象为少量可监控的区域，从而降低监控开销并提升适应性。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`damon_get_task_struct()`**  \n  根据 `damon_target` 中保存的 `pid` 获取对应的 `task_struct`，并增加其引用计数。\n\n- **`damon_get_mm()`**  \n  获取目标进程的 `mm_struct`（内存描述符），调用者需在使用后调用 `mmput()` 释放。\n\n- **`damon_va_evenly_split_region()`**  \n  将一个 DAMON 监控区域均匀分割为指定数量的小区域，每个小区域大小对齐到 `DAMON_MIN_REGION`。\n\n- **`__damon_va_three_regions()`**  \n  在给定的 `mm_struct` 中扫描 VMA，找出两个最大的未映射间隙（unmapped gaps），并据此划分出三个覆盖所有已映射区域的地址范围。\n\n- **`damon_va_three_regions()`**  \n  封装 `__damon_va_three_regions()`，负责获取目标进程的内存上下文并加读锁后调用。\n\n- **`__damon_va_init_regions()`**  \n  为指定的监控目标（进程）初始化三个初始监控区域，并根据配置进一步细分为多个子区域。\n\n- **`damon_va_init()`**  \n  （代码截断，但意图明确）遍历 DAMON 上下文中的所有目标，为每个目标调用 `__damon_va_init_regions()` 进行初始化。\n\n### 关键数据结构\n\n- **`struct damon_target`**  \n  表示一个被监控的目标（通常是一个进程），包含 `pid` 指针等信息。\n\n- **`struct damon_region`**  \n  DAMON 监控的基本单位，表示一段连续的虚拟地址区间（`ar.start` 到 `ar.end`）。\n\n- **`struct damon_addr_range`**  \n  简单的地址范围结构，用于临时存储起止地址。\n\n## 3. 关键实现\n\n### 三区域划分算法（Three-Region Heuristic）\n\n该文件的核心思想是：**避免直接监控整个虚拟地址空间**（含大量未映射区域）。为此，采用启发式方法：\n\n1. 遍历进程的 VMA 链表（通过 `VMA_ITERATOR` 和 RCU 读锁安全访问）。\n2. 记录所有相邻 VMA 之间的间隙（`gap = vma->vm_start - prev->vm_end`）。\n3. 找出**两个最大的间隙**（`first_gap` 和 `second_gap`）。\n4. 将整个已映射地址空间划分为三个区域：\n   - 区域0：从第一个 VMA 起始地址到第一个大间隙的开始\n   - 区域1：从第一个大间隙结束到第二个大间隙开始\n   - 区域2：从第二个大间隙结束到最后一个 VMA 结束地址\n5. 所有边界对齐到 `DAMON_MIN_REGION`（通常为页大小或更大）。\n\n此方法有效跳过了堆与 mmap 区之间、mmap 区与栈之间的巨大空洞，显著减少无效监控区域。\n\n### 区域细分策略\n\n初始化的三个大区域会根据 DAMON 上下文配置的 `min_nr_regions` 进一步细分：\n- 计算平均区域大小：`总监控大小 / min_nr_regions`\n- 若计算结果小于 `DAMON_MIN_REGION`，则使用后者作为最小粒度\n- 调用 `damon_va_evenly_split_region()` 将每个大区域均匀切分为若干子区域\n\n这确保了初始监控粒度既不过粗（丢失细节），也不过细（开销过大）。\n\n### 内存安全与同步\n\n- 使用 `mmap_read_lock()`/`mmap_read_unlock()` 保护 VMA 遍历，兼容并发内存映射变更。\n- 通过 `get_task_mm()` 安全获取 `mm_struct`，防止进程退出导致悬空指针。\n- 所有 `mm_struct` 和 `task_struct` 的引用均正确配对（`get`/`put`）。\n\n## 4. 依赖关系\n\n- **内核头文件依赖**：\n  - `<linux/mm.h>` 相关：`hugetlb.h`, `highmem.h`, `page_idle.h`, `pagewalk.h`, `sched/mm.h`\n  - `<linux/mmu_notifier.h>`：用于内存映射变更通知（虽未直接使用，但为 DAMON 整体架构所需）\n  - `<asm-generic/mman-common.h>`：内存管理常量\n- **DAMON 内部依赖**：\n  - `\"ops-common.h\"`：提供 `damon_new_region()`, `damon_add_region()` 等通用操作\n  - 依赖 DAMON 核心框架的 `damon_ctx`, `damon_target`, `damon_region` 等结构定义\n- **KUnit 测试支持**：\n  - `CONFIG_DAMON_VADDR_KUNIT_TEST` 宏用于测试时调整 `DAMON_MIN_REGION` 为 1，便于验证逻辑\n\n## 5. 使用场景\n\n- **DAMON 虚拟地址监控模式初始化**：当用户通过 DAMON 接口（如 debugfs 或 tracepoint）启动对一组进程的内存访问模式监控时，DAMON 核心调用 `damon_va_init()` 为每个目标进程构建初始监控区域。\n- **内存优化工具基础**：为 `damo`（DAMON 用户空间工具）等提供底层支持，用于识别冷热内存、指导内存回收（如 `reclaim`）、透明大页（THP）优化等。\n- **低开销内存行为分析**：适用于需要长期、低性能影响地监控进程内存访问模式的场景，如云环境中的资源调度、性能剖析等。\n- **自适应内存监控起点**：所生成的初始区域将作为 DAMON 自适应区域调整机制（合并/分裂）的起点，在后续监控周期中动态优化区域划分。",
      "similarity": 0.5754895210266113,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "mm/damon/vaddr.c",
          "start_line": 632,
          "end_line": 723,
          "content": [
            "static unsigned long damos_madvise(struct damon_target *target,",
            "\t\tstruct damon_region *r, int behavior)",
            "{",
            "\tstruct mm_struct *mm;",
            "\tunsigned long start = PAGE_ALIGN(r->ar.start);",
            "\tunsigned long len = PAGE_ALIGN(damon_sz_region(r));",
            "\tunsigned long applied;",
            "",
            "\tmm = damon_get_mm(target);",
            "\tif (!mm)",
            "\t\treturn 0;",
            "",
            "\tapplied = do_madvise(mm, start, len, behavior) ? 0 : len;",
            "\tmmput(mm);",
            "",
            "\treturn applied;",
            "}",
            "static unsigned long damon_va_apply_scheme(struct damon_ctx *ctx,",
            "\t\tstruct damon_target *t, struct damon_region *r,",
            "\t\tstruct damos *scheme)",
            "{",
            "\tint madv_action;",
            "",
            "\tswitch (scheme->action) {",
            "\tcase DAMOS_WILLNEED:",
            "\t\tmadv_action = MADV_WILLNEED;",
            "\t\tbreak;",
            "\tcase DAMOS_COLD:",
            "\t\tmadv_action = MADV_COLD;",
            "\t\tbreak;",
            "\tcase DAMOS_PAGEOUT:",
            "\t\tmadv_action = MADV_PAGEOUT;",
            "\t\tbreak;",
            "\tcase DAMOS_HUGEPAGE:",
            "\t\tmadv_action = MADV_HUGEPAGE;",
            "\t\tbreak;",
            "\tcase DAMOS_NOHUGEPAGE:",
            "\t\tmadv_action = MADV_NOHUGEPAGE;",
            "\t\tbreak;",
            "\tcase DAMOS_STAT:",
            "\t\treturn 0;",
            "\tdefault:",
            "\t\t/*",
            "\t\t * DAMOS actions that are not yet supported by 'vaddr'.",
            "\t\t */",
            "\t\treturn 0;",
            "\t}",
            "",
            "\treturn damos_madvise(t, r, madv_action);",
            "}",
            "static int damon_va_scheme_score(struct damon_ctx *context,",
            "\t\tstruct damon_target *t, struct damon_region *r,",
            "\t\tstruct damos *scheme)",
            "{",
            "",
            "\tswitch (scheme->action) {",
            "\tcase DAMOS_PAGEOUT:",
            "\t\treturn damon_cold_score(context, r, scheme);",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn DAMOS_MAX_SCORE;",
            "}",
            "static int __init damon_va_initcall(void)",
            "{",
            "\tstruct damon_operations ops = {",
            "\t\t.id = DAMON_OPS_VADDR,",
            "\t\t.init = damon_va_init,",
            "\t\t.update = damon_va_update,",
            "\t\t.prepare_access_checks = damon_va_prepare_access_checks,",
            "\t\t.check_accesses = damon_va_check_accesses,",
            "\t\t.reset_aggregated = NULL,",
            "\t\t.target_valid = damon_va_target_valid,",
            "\t\t.cleanup = NULL,",
            "\t\t.apply_scheme = damon_va_apply_scheme,",
            "\t\t.get_scheme_score = damon_va_scheme_score,",
            "\t};",
            "\t/* ops for fixed virtual address ranges */",
            "\tstruct damon_operations ops_fvaddr = ops;",
            "\tint err;",
            "",
            "\t/* Don't set the monitoring target regions for the entire mapping */",
            "\tops_fvaddr.id = DAMON_OPS_FVADDR;",
            "\tops_fvaddr.init = NULL;",
            "\tops_fvaddr.update = NULL;",
            "",
            "\terr = damon_register_ops(&ops);",
            "\tif (err)",
            "\t\treturn err;",
            "\treturn damon_register_ops(&ops_fvaddr);",
            "};"
          ],
          "function_name": "damos_madvise, damon_va_apply_scheme, damon_va_scheme_score, damon_va_initcall",
          "description": "该代码实现基于虚拟地址的内存优化策略管理，主要功能包括：  \n1. `damos_madvise` 和 `damon_va_apply_scheme` 通过 `madvise` 系统调用对内存区域应用特定行为（如预读、冷页迁移等），`damon_va_scheme_score` 根据策略动态计算区域评分；  \n2. `damon_va_initcall` 注册虚拟地址范围监控操作集，支持两种模式（普通/固定范围），关联策略应用与评分逻辑；  \n3. 代码上下文不完整，依赖外部未展示的 `do_madvise`、`damon_cold_score` 等函数及 `damon_operations` 操作接口。",
          "similarity": 0.5295529961585999
        },
        {
          "chunk_id": 0,
          "file_path": "mm/damon/vaddr.c",
          "start_line": 1,
          "end_line": 63,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * DAMON Primitives for Virtual Address Spaces",
            " *",
            " * Author: SeongJae Park <sjpark@amazon.de>",
            " */",
            "",
            "#define pr_fmt(fmt) \"damon-va: \" fmt",
            "",
            "#include <asm-generic/mman-common.h>",
            "#include <linux/highmem.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/page_idle.h>",
            "#include <linux/pagewalk.h>",
            "#include <linux/sched/mm.h>",
            "",
            "#include \"ops-common.h\"",
            "",
            "#ifdef CONFIG_DAMON_VADDR_KUNIT_TEST",
            "#undef DAMON_MIN_REGION",
            "#define DAMON_MIN_REGION 1",
            "#endif",
            "",
            "/*",
            " * 't->pid' should be the pointer to the relevant 'struct pid' having reference",
            " * count.  Caller must put the returned task, unless it is NULL.",
            " */",
            "static inline struct task_struct *damon_get_task_struct(struct damon_target *t)",
            "{",
            "\treturn get_pid_task(t->pid, PIDTYPE_PID);",
            "}",
            "",
            "/*",
            " * Get the mm_struct of the given target",
            " *",
            " * Caller _must_ put the mm_struct after use, unless it is NULL.",
            " *",
            " * Returns the mm_struct of the target on success, NULL on failure",
            " */",
            "static struct mm_struct *damon_get_mm(struct damon_target *t)",
            "{",
            "\tstruct task_struct *task;",
            "\tstruct mm_struct *mm;",
            "",
            "\ttask = damon_get_task_struct(t);",
            "\tif (!task)",
            "\t\treturn NULL;",
            "",
            "\tmm = get_task_mm(task);",
            "\tput_task_struct(task);",
            "\treturn mm;",
            "}",
            "",
            "/*",
            " * Functions for the initial monitoring target regions construction",
            " */",
            "",
            "/*",
            " * Size-evenly split a region into 'nr_pieces' small regions",
            " *",
            " * Returns 0 on success, or negative error code otherwise.",
            " */"
          ],
          "function_name": null,
          "description": "定义获取进程任务结构体和MM结构体的辅助函数，并声明用于初始监控区域构造的相关函数，核心功能是提供虚拟地址空间监控的基本支持。",
          "similarity": 0.46905937790870667
        },
        {
          "chunk_id": 2,
          "file_path": "mm/damon/vaddr.c",
          "start_line": 235,
          "end_line": 358,
          "content": [
            "static void __damon_va_init_regions(struct damon_ctx *ctx,",
            "\t\t\t\t     struct damon_target *t)",
            "{",
            "\tstruct damon_target *ti;",
            "\tstruct damon_region *r;",
            "\tstruct damon_addr_range regions[3];",
            "\tunsigned long sz = 0, nr_pieces;",
            "\tint i, tidx = 0;",
            "",
            "\tif (damon_va_three_regions(t, regions)) {",
            "\t\tdamon_for_each_target(ti, ctx) {",
            "\t\t\tif (ti == t)",
            "\t\t\t\tbreak;",
            "\t\t\ttidx++;",
            "\t\t}",
            "\t\tpr_debug(\"Failed to get three regions of %dth target\\n\", tidx);",
            "\t\treturn;",
            "\t}",
            "",
            "\tfor (i = 0; i < 3; i++)",
            "\t\tsz += regions[i].end - regions[i].start;",
            "\tif (ctx->attrs.min_nr_regions)",
            "\t\tsz /= ctx->attrs.min_nr_regions;",
            "\tif (sz < DAMON_MIN_REGION)",
            "\t\tsz = DAMON_MIN_REGION;",
            "",
            "\t/* Set the initial three regions of the target */",
            "\tfor (i = 0; i < 3; i++) {",
            "\t\tr = damon_new_region(regions[i].start, regions[i].end);",
            "\t\tif (!r) {",
            "\t\t\tpr_err(\"%d'th init region creation failed\\n\", i);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tdamon_add_region(r, t);",
            "",
            "\t\tnr_pieces = (regions[i].end - regions[i].start) / sz;",
            "\t\tdamon_va_evenly_split_region(t, r, nr_pieces);",
            "\t}",
            "}",
            "static void damon_va_init(struct damon_ctx *ctx)",
            "{",
            "\tstruct damon_target *t;",
            "",
            "\tdamon_for_each_target(t, ctx) {",
            "\t\t/* the user may set the target regions as they want */",
            "\t\tif (!damon_nr_regions(t))",
            "\t\t\t__damon_va_init_regions(ctx, t);",
            "\t}",
            "}",
            "static void damon_va_update(struct damon_ctx *ctx)",
            "{",
            "\tstruct damon_addr_range three_regions[3];",
            "\tstruct damon_target *t;",
            "",
            "\tdamon_for_each_target(t, ctx) {",
            "\t\tif (damon_va_three_regions(t, three_regions))",
            "\t\t\tcontinue;",
            "\t\tdamon_set_regions(t, three_regions, 3);",
            "\t}",
            "}",
            "static int damon_mkold_pmd_entry(pmd_t *pmd, unsigned long addr,",
            "\t\tunsigned long next, struct mm_walk *walk)",
            "{",
            "\tpte_t *pte;",
            "\tpmd_t pmde;",
            "\tspinlock_t *ptl;",
            "",
            "\tif (pmd_trans_huge(pmdp_get(pmd))) {",
            "\t\tptl = pmd_lock(walk->mm, pmd);",
            "\t\tpmde = pmdp_get(pmd);",
            "",
            "\t\tif (!pmd_present(pmde)) {",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\treturn 0;",
            "\t\t}",
            "",
            "\t\tif (pmd_trans_huge(pmde)) {",
            "\t\t\tdamon_pmdp_mkold(pmd, walk->vma, addr);",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t\tspin_unlock(ptl);",
            "\t}",
            "",
            "\tpte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);",
            "\tif (!pte) {",
            "\t\twalk->action = ACTION_AGAIN;",
            "\t\treturn 0;",
            "\t}",
            "\tif (!pte_present(ptep_get(pte)))",
            "\t\tgoto out;",
            "\tdamon_ptep_mkold(pte, walk->vma, addr);",
            "out:",
            "\tpte_unmap_unlock(pte, ptl);",
            "\treturn 0;",
            "}",
            "static void damon_hugetlb_mkold(pte_t *pte, struct mm_struct *mm,",
            "\t\t\t\tstruct vm_area_struct *vma, unsigned long addr)",
            "{",
            "\tbool referenced = false;",
            "\tpte_t entry = huge_ptep_get(pte);",
            "\tstruct folio *folio = pfn_folio(pte_pfn(entry));",
            "\tunsigned long psize = huge_page_size(hstate_vma(vma));",
            "",
            "\tfolio_get(folio);",
            "",
            "\tif (pte_young(entry)) {",
            "\t\treferenced = true;",
            "\t\tentry = pte_mkold(entry);",
            "\t\tset_huge_pte_at(mm, addr, pte, entry, psize);",
            "\t}",
            "",
            "#ifdef CONFIG_MMU_NOTIFIER",
            "\tif (mmu_notifier_clear_young(mm, addr,",
            "\t\t\t\t     addr + huge_page_size(hstate_vma(vma))))",
            "\t\treferenced = true;",
            "#endif /* CONFIG_MMU_NOTIFIER */",
            "",
            "\tif (referenced)",
            "\t\tfolio_set_young(folio);",
            "",
            "\tfolio_set_idle(folio);",
            "\tfolio_put(folio);",
            "}"
          ],
          "function_name": "__damon_va_init_regions, damon_va_init, damon_va_update, damon_mkold_pmd_entry, damon_hugetlb_mkold",
          "description": "初始化和更新监控区域的函数，以及处理大页表项的mkold操作，核心功能是构建初始监控区域并维护页面年轻状态标记。",
          "similarity": 0.4530596137046814
        },
        {
          "chunk_id": 1,
          "file_path": "mm/damon/vaddr.c",
          "start_line": 64,
          "end_line": 170,
          "content": [
            "static int damon_va_evenly_split_region(struct damon_target *t,",
            "\t\tstruct damon_region *r, unsigned int nr_pieces)",
            "{",
            "\tunsigned long sz_orig, sz_piece, orig_end;",
            "\tstruct damon_region *n = NULL, *next;",
            "\tunsigned long start;",
            "\tunsigned int i;",
            "",
            "\tif (!r || !nr_pieces)",
            "\t\treturn -EINVAL;",
            "",
            "\torig_end = r->ar.end;",
            "\tsz_orig = damon_sz_region(r);",
            "\tsz_piece = ALIGN_DOWN(sz_orig / nr_pieces, DAMON_MIN_REGION);",
            "",
            "\tif (!sz_piece)",
            "\t\treturn -EINVAL;",
            "",
            "\tr->ar.end = r->ar.start + sz_piece;",
            "\tnext = damon_next_region(r);",
            "\tfor (start = r->ar.end, i = 1; i < nr_pieces; start += sz_piece, i++) {",
            "\t\tn = damon_new_region(start, start + sz_piece);",
            "\t\tif (!n)",
            "\t\t\treturn -ENOMEM;",
            "\t\tdamon_insert_region(n, r, next, t);",
            "\t\tr = n;",
            "\t}",
            "\t/* complement last region for possible rounding error */",
            "\tif (n)",
            "\t\tn->ar.end = orig_end;",
            "",
            "\treturn 0;",
            "}",
            "static unsigned long sz_range(struct damon_addr_range *r)",
            "{",
            "\treturn r->end - r->start;",
            "}",
            "static int __damon_va_three_regions(struct mm_struct *mm,",
            "\t\t\t\t       struct damon_addr_range regions[3])",
            "{",
            "\tstruct damon_addr_range first_gap = {0}, second_gap = {0};",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "\tstruct vm_area_struct *vma, *prev = NULL;",
            "\tunsigned long start;",
            "",
            "\t/*",
            "\t * Find the two biggest gaps so that first_gap > second_gap > others.",
            "\t * If this is too slow, it can be optimised to examine the maple",
            "\t * tree gaps.",
            "\t */",
            "\trcu_read_lock();",
            "\tfor_each_vma(vmi, vma) {",
            "\t\tunsigned long gap;",
            "",
            "\t\tif (!prev) {",
            "\t\t\tstart = vma->vm_start;",
            "\t\t\tgoto next;",
            "\t\t}",
            "\t\tgap = vma->vm_start - prev->vm_end;",
            "",
            "\t\tif (gap > sz_range(&first_gap)) {",
            "\t\t\tsecond_gap = first_gap;",
            "\t\t\tfirst_gap.start = prev->vm_end;",
            "\t\t\tfirst_gap.end = vma->vm_start;",
            "\t\t} else if (gap > sz_range(&second_gap)) {",
            "\t\t\tsecond_gap.start = prev->vm_end;",
            "\t\t\tsecond_gap.end = vma->vm_start;",
            "\t\t}",
            "next:",
            "\t\tprev = vma;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tif (!sz_range(&second_gap) || !sz_range(&first_gap))",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Sort the two biggest gaps by address */",
            "\tif (first_gap.start > second_gap.start)",
            "\t\tswap(first_gap, second_gap);",
            "",
            "\t/* Store the result */",
            "\tregions[0].start = ALIGN(start, DAMON_MIN_REGION);",
            "\tregions[0].end = ALIGN(first_gap.start, DAMON_MIN_REGION);",
            "\tregions[1].start = ALIGN(first_gap.end, DAMON_MIN_REGION);",
            "\tregions[1].end = ALIGN(second_gap.start, DAMON_MIN_REGION);",
            "\tregions[2].start = ALIGN(second_gap.end, DAMON_MIN_REGION);",
            "\tregions[2].end = ALIGN(prev->vm_end, DAMON_MIN_REGION);",
            "",
            "\treturn 0;",
            "}",
            "static int damon_va_three_regions(struct damon_target *t,",
            "\t\t\t\tstruct damon_addr_range regions[3])",
            "{",
            "\tstruct mm_struct *mm;",
            "\tint rc;",
            "",
            "\tmm = damon_get_mm(t);",
            "\tif (!mm)",
            "\t\treturn -EINVAL;",
            "",
            "\tmmap_read_lock(mm);",
            "\trc = __damon_va_three_regions(mm, regions);",
            "\tmmap_read_unlock(mm);",
            "",
            "\tmmput(mm);",
            "\treturn rc;",
            "}"
          ],
          "function_name": "damon_va_evenly_split_region, sz_range, __damon_va_three_regions, damon_va_three_regions",
          "description": "实现将监控区域均分、计算范围大小及寻找最大空闲间隙的函数，核心功能是通过遍历VMA找到两个最大空闲区间用于后续监控区域划分。",
          "similarity": 0.44845306873321533
        },
        {
          "chunk_id": 4,
          "file_path": "mm/damon/vaddr.c",
          "start_line": 503,
          "end_line": 605,
          "content": [
            "static int damon_young_hugetlb_entry(pte_t *pte, unsigned long hmask,",
            "\t\t\t\t     unsigned long addr, unsigned long end,",
            "\t\t\t\t     struct mm_walk *walk)",
            "{",
            "\tstruct damon_young_walk_private *priv = walk->private;",
            "\tstruct hstate *h = hstate_vma(walk->vma);",
            "\tstruct folio *folio;",
            "\tspinlock_t *ptl;",
            "\tpte_t entry;",
            "",
            "\tptl = huge_pte_lock(h, walk->mm, pte);",
            "\tentry = huge_ptep_get(pte);",
            "\tif (!pte_present(entry))",
            "\t\tgoto out;",
            "",
            "\tfolio = pfn_folio(pte_pfn(entry));",
            "\tfolio_get(folio);",
            "",
            "\tif (pte_young(entry) || !folio_test_idle(folio) ||",
            "\t    mmu_notifier_test_young(walk->mm, addr))",
            "\t\tpriv->young = true;",
            "\t*priv->folio_sz = huge_page_size(h);",
            "",
            "\tfolio_put(folio);",
            "",
            "out:",
            "\tspin_unlock(ptl);",
            "\treturn 0;",
            "}",
            "static bool damon_va_young(struct mm_struct *mm, unsigned long addr,",
            "\t\tunsigned long *folio_sz)",
            "{",
            "\tstruct damon_young_walk_private arg = {",
            "\t\t.folio_sz = folio_sz,",
            "\t\t.young = false,",
            "\t};",
            "",
            "\tmmap_read_lock(mm);",
            "\twalk_page_range(mm, addr, addr + 1, &damon_young_ops, &arg);",
            "\tmmap_read_unlock(mm);",
            "\treturn arg.young;",
            "}",
            "static void __damon_va_check_access(struct mm_struct *mm,",
            "\t\t\t\tstruct damon_region *r, bool same_target)",
            "{",
            "\tstatic unsigned long last_addr;",
            "\tstatic unsigned long last_folio_sz = PAGE_SIZE;",
            "\tstatic bool last_accessed;",
            "",
            "\t/* If the region is in the last checked page, reuse the result */",
            "\tif (same_target && (ALIGN_DOWN(last_addr, last_folio_sz) ==",
            "\t\t\t\tALIGN_DOWN(r->sampling_addr, last_folio_sz))) {",
            "\t\tif (last_accessed)",
            "\t\t\tr->nr_accesses++;",
            "\t\treturn;",
            "\t}",
            "",
            "\tlast_accessed = damon_va_young(mm, r->sampling_addr, &last_folio_sz);",
            "\tif (last_accessed)",
            "\t\tr->nr_accesses++;",
            "",
            "\tlast_addr = r->sampling_addr;",
            "}",
            "static unsigned int damon_va_check_accesses(struct damon_ctx *ctx)",
            "{",
            "\tstruct damon_target *t;",
            "\tstruct mm_struct *mm;",
            "\tstruct damon_region *r;",
            "\tunsigned int max_nr_accesses = 0;",
            "\tbool same_target;",
            "",
            "\tdamon_for_each_target(t, ctx) {",
            "\t\tmm = damon_get_mm(t);",
            "\t\tif (!mm)",
            "\t\t\tcontinue;",
            "\t\tsame_target = false;",
            "\t\tdamon_for_each_region(r, t) {",
            "\t\t\t__damon_va_check_access(mm, r, same_target);",
            "\t\t\tmax_nr_accesses = max(r->nr_accesses, max_nr_accesses);",
            "\t\t\tsame_target = true;",
            "\t\t}",
            "\t\tmmput(mm);",
            "\t}",
            "",
            "\treturn max_nr_accesses;",
            "}",
            "static bool damon_va_target_valid(struct damon_target *t)",
            "{",
            "\tstruct task_struct *task;",
            "",
            "\ttask = damon_get_task_struct(t);",
            "\tif (task) {",
            "\t\tput_task_struct(task);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}",
            "static unsigned long damos_madvise(struct damon_target *target,",
            "\t\tstruct damon_region *r, int behavior)",
            "{",
            "\treturn 0;",
            "}"
          ],
          "function_name": "damon_young_hugetlb_entry, damon_va_young, __damon_va_check_access, damon_va_check_accesses, damon_va_target_valid, damos_madvise",
          "description": "检查页面访问状态及统计访问次数的函数，核心功能是通过遍历内存区域判断页面是否被访问并更新监控数据。",
          "similarity": 0.3747153878211975
        }
      ]
    },
    {
      "source_file": "kernel/dma/pool.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:15:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\pool.c`\n\n---\n\n# `dma/pool.c` 技术文档\n\n## 1. 文件概述\n\n`dma/pool.c` 实现了 Linux 内核中的 **DMA 原子内存池（atomic DMA pools）** 机制，用于在无法睡眠的上下文（如中断处理、原子上下文）中分配一致性（coherent）DMA 内存。该机制通过预分配多个按内存区域（ZONE_DMA、ZONE_DMA32、普通内核内存）划分的通用内存池（`gen_pool`），并在池空间不足时通过工作队列异步扩展，从而支持在 GFP_ATOMIC 等限制性分配标志下安全地分配 DMA 内存。\n\n该文件主要用于支持 `dma-direct` 子系统中的原子 DMA 分配路径，确保即使在内存压力大或无法睡眠的场景下，设备驱动仍能获得满足地址限制（如 32 位或 24 位寻址）的一致性 DMA 缓冲区。\n\n## 2. 核心功能\n\n### 全局变量\n- `atomic_pool_dma` / `pool_size_dma`：用于 `GFP_DMA` 区域的原子 DMA 池及其已分配大小。\n- `atomic_pool_dma32` / `pool_size_dma32`：用于 `GFP_DMA32` 区域的原子 DMA 池及其已分配大小。\n- `atomic_pool_kernel` / `pool_size_kernel`：用于普通内核区域（无特殊 DMA 限制）的原子 DMA 池及其已分配大小。\n- `atomic_pool_size`：每个池的初始目标大小，可通过内核命令行参数 `coherent_pool=` 设置。\n- `atomic_pool_work`：用于后台动态扩展内存池的工作项。\n\n### 主要函数\n- `early_coherent_pool()`：解析内核命令行参数 `coherent_pool`，设置 `atomic_pool_size`。\n- `dma_atomic_pool_init()`：初始化所有原子 DMA 池（postcore 阶段调用）。\n- `__dma_atomic_pool_init()`：创建并填充指定 GFP 标志的原子池。\n- `atomic_pool_expand()`：向指定池中添加一块连续物理内存。\n- `atomic_pool_resize()` / `atomic_pool_work_fn()`：检查池剩余空间，若不足则触发扩展。\n- `dma_alloc_from_pool()`：从合适的原子池中分配指定大小的 DMA 内存。\n- `dma_free_from_pool()`：将内存归还到对应的原子池。\n- `dma_guess_pool()`：根据 GFP 标志和尝试顺序选择合适的内存池。\n- `cma_in_zone()`：判断 CMA 区域是否位于指定 DMA 区域内，以决定是否优先从 CMA 分配。\n- `dma_atomic_pool_debugfs_init()`：在 debugfs 中导出各池的当前大小。\n\n## 3. 关键实现\n\n### 内存池初始化策略\n- 若未通过 `coherent_pool=` 指定大小，则默认按 **每 1GB 物理内存分配 128KB** 原子池，最小 128KB，最大不超过 `MAX_ORDER_NR_PAGES` 对应的内存。\n- 每个池使用 `gen_pool` 管理，分配算法为 `gen_pool_first_fit_order_align`，保证分配地址按页对齐。\n- 初始化时调用 `atomic_pool_expand()` 预分配内存。\n\n### 内存分配来源\n- 优先尝试从 **CMA（Contiguous Memory Allocator）** 区域分配（若 CMA 区域位于目标 DMA zone 内）。\n- 若 CMA 不可用或不在目标 zone，则回退到 `alloc_pages()`。\n- 分配的内存块大小不超过 `MAX_PAGE_ORDER`，通过降序尝试（从大到小）提高分配成功率。\n\n### 内存属性处理\n- 调用 `arch_dma_prep_coherent()` 通知架构层准备一致性内存。\n- 在支持内存加密（如 AMD SEV、Intel TDX）的系统上，显式调用 `set_memory_decrypted()` 确保 DMA 内存为 **未加密状态**，因为设备无法访问加密内存。\n- 若启用了 `CONFIG_DMA_DIRECT_REMAP`，则通过 `dma_common_contiguous_remap()` 建立非缓存或设备专用的页表映射。\n\n### 动态扩展机制\n- 每次从池中分配内存后，检查剩余空间是否小于 `atomic_pool_size`。\n- 若不足，则调度 `atomic_pool_work` 工作项，在进程上下文中异步扩展对应池。\n- 扩展时尝试分配与当前池总大小相当的新内存块，避免频繁小量扩展。\n\n### 多池选择逻辑\n- `dma_guess_pool()` 实现池选择策略：\n  1. 首选与 GFP 标志匹配的池（DMA32 > DMA > 普通内核）。\n  2. 若首次分配失败，按 `kernel → dma32 → dma` 顺序尝试其他池（fallback 机制）。\n- 释放时遍历所有池，通过 `gen_pool_has_addr()` 确定内存归属。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - 依赖 `genalloc`（`gen_pool`）实现内存池管理。\n  - 使用 `alloc_pages()`、`__free_pages()` 进行底层页分配。\n  - 依赖 CMA 接口（`dma_alloc_from_contiguous()`）获取大块连续内存。\n- **DMA 子系统**：\n  - 与 `dma-direct.c` 紧密集成，为其提供 `___dma_direct_alloc_pages()` 中的原子分配路径。\n  - 使用 `dma-map-ops.h` 和 `dma-direct.h` 中的辅助函数。\n- **架构相关支持**：\n  - 调用 `arch_dma_prep_coherent()`（架构可选实现）。\n  - 使用 `set_memory_decrypted()`/`set_memory_encrypted()`（x86/ARM64 等支持内存加密的架构）。\n  - 依赖 `DMA_BIT_MASK()` 和 `zone_dma_bits` 判断 DMA 地址范围。\n- **其他**：\n  - 使用 `debugfs` 导出调试信息。\n  - 依赖 `workqueue` 实现异步扩展。\n  - 使用 `slab.h` 中的内存分配器（间接）。\n\n## 5. 使用场景\n\n- **原子上下文 DMA 分配**：当设备驱动在中断处理程序、自旋锁保护区域或使用 `GFP_ATOMIC` 标志调用 `dma_alloc_coherent()` 时，若常规页分配器无法满足（如内存碎片），内核会回退到从原子池分配。\n- **满足地址限制的 DMA 缓冲区**：对于需要 24 位（ISA 设备）或 32 位（旧 PCIe 设备）寻址能力的设备，驱动使用 `DMA_BIT_MASK(24)` 或 `DMA_BIT_MASK(32)` 限制 DMA 地址范围，原子池确保分配的内存物理地址符合要求。\n- **一致性内存需求**：适用于需要 CPU 与设备之间缓存一致性的场景（如网络数据包缓冲区、音频流缓冲区），原子池分配的内存经过 `arch_dma_prep_coherent()` 处理，保证一致性。\n- **内存加密环境**：在启用内存加密的系统中，确保分配给设备的 DMA 内存处于解密状态，使设备能正常访问。",
      "similarity": 0.5695434212684631,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/pool.c",
          "start_line": 29,
          "end_line": 138,
          "content": [
            "static int __init early_coherent_pool(char *p)",
            "{",
            "\tatomic_pool_size = memparse(p, &p);",
            "\treturn 0;",
            "}",
            "static void __init dma_atomic_pool_debugfs_init(void)",
            "{",
            "\tstruct dentry *root;",
            "",
            "\troot = debugfs_create_dir(\"dma_pools\", NULL);",
            "\tdebugfs_create_ulong(\"pool_size_dma\", 0400, root, &pool_size_dma);",
            "\tdebugfs_create_ulong(\"pool_size_dma32\", 0400, root, &pool_size_dma32);",
            "\tdebugfs_create_ulong(\"pool_size_kernel\", 0400, root, &pool_size_kernel);",
            "}",
            "static void dma_atomic_pool_size_add(gfp_t gfp, size_t size)",
            "{",
            "\tif (gfp & __GFP_DMA)",
            "\t\tpool_size_dma += size;",
            "\telse if (gfp & __GFP_DMA32)",
            "\t\tpool_size_dma32 += size;",
            "\telse",
            "\t\tpool_size_kernel += size;",
            "}",
            "static bool cma_in_zone(gfp_t gfp)",
            "{",
            "\tunsigned long size;",
            "\tphys_addr_t end;",
            "\tstruct cma *cma;",
            "",
            "\tcma = dev_get_cma_area(NULL);",
            "\tif (!cma)",
            "\t\treturn false;",
            "",
            "\tsize = cma_get_size(cma);",
            "\tif (!size)",
            "\t\treturn false;",
            "",
            "\t/* CMA can't cross zone boundaries, see cma_activate_area() */",
            "\tend = cma_get_base(cma) + size - 1;",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA) && (gfp & GFP_DMA))",
            "\t\treturn end <= DMA_BIT_MASK(zone_dma_bits);",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA32) && (gfp & GFP_DMA32))",
            "\t\treturn end <= DMA_BIT_MASK(32);",
            "\treturn true;",
            "}",
            "static int atomic_pool_expand(struct gen_pool *pool, size_t pool_size,",
            "\t\t\t      gfp_t gfp)",
            "{",
            "\tunsigned int order;",
            "\tstruct page *page = NULL;",
            "\tvoid *addr;",
            "\tint ret = -ENOMEM;",
            "",
            "\t/* Cannot allocate larger than MAX_PAGE_ORDER */",
            "\torder = min(get_order(pool_size), MAX_PAGE_ORDER);",
            "",
            "\tdo {",
            "\t\tpool_size = 1 << (PAGE_SHIFT + order);",
            "\t\tif (cma_in_zone(gfp))",
            "\t\t\tpage = dma_alloc_from_contiguous(NULL, 1 << order,",
            "\t\t\t\t\t\t\t order, false);",
            "\t\tif (!page)",
            "\t\t\tpage = alloc_pages(gfp, order);",
            "\t} while (!page && order-- > 0);",
            "\tif (!page)",
            "\t\tgoto out;",
            "",
            "\tarch_dma_prep_coherent(page, pool_size);",
            "",
            "#ifdef CONFIG_DMA_DIRECT_REMAP",
            "\taddr = dma_common_contiguous_remap(page, pool_size,",
            "\t\t\tpgprot_decrypted(pgprot_dmacoherent(PAGE_KERNEL)),",
            "\t\t\t__builtin_return_address(0));",
            "\tif (!addr)",
            "\t\tgoto free_page;",
            "#else",
            "\taddr = page_to_virt(page);",
            "#endif",
            "\t/*",
            "\t * Memory in the atomic DMA pools must be unencrypted, the pools do not",
            "\t * shrink so no re-encryption occurs in dma_direct_free().",
            "\t */",
            "\tret = set_memory_decrypted((unsigned long)page_to_virt(page),",
            "\t\t\t\t   1 << order);",
            "\tif (ret)",
            "\t\tgoto remove_mapping;",
            "\tret = gen_pool_add_virt(pool, (unsigned long)addr, page_to_phys(page),",
            "\t\t\t\tpool_size, NUMA_NO_NODE);",
            "\tif (ret)",
            "\t\tgoto encrypt_mapping;",
            "",
            "\tdma_atomic_pool_size_add(gfp, pool_size);",
            "\treturn 0;",
            "",
            "encrypt_mapping:",
            "\tret = set_memory_encrypted((unsigned long)page_to_virt(page),",
            "\t\t\t\t   1 << order);",
            "\tif (WARN_ON_ONCE(ret)) {",
            "\t\t/* Decrypt succeeded but encrypt failed, purposely leak */",
            "\t\tgoto out;",
            "\t}",
            "remove_mapping:",
            "#ifdef CONFIG_DMA_DIRECT_REMAP",
            "\tdma_common_free_remap(addr, pool_size);",
            "free_page:",
            "\t__free_pages(page, order);",
            "#endif",
            "out:",
            "\treturn ret;",
            "}"
          ],
          "function_name": "early_coherent_pool, dma_atomic_pool_debugfs_init, dma_atomic_pool_size_add, cma_in_zone, atomic_pool_expand",
          "description": "实现DMA内存池的动态扩展逻辑，包含解析命令行参数、调试接口注册、内存分配策略选择、CMA区域有效性检测及池扩容操作，支持加密/解密内存映射管理。",
          "similarity": 0.5324661731719971
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/pool.c",
          "start_line": 1,
          "end_line": 28,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 2012 ARM Ltd.",
            " * Copyright (C) 2020 Google LLC",
            " */",
            "#include <linux/cma.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/dma-map-ops.h>",
            "#include <linux/dma-direct.h>",
            "#include <linux/init.h>",
            "#include <linux/genalloc.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/slab.h>",
            "#include <linux/workqueue.h>",
            "",
            "static struct gen_pool *atomic_pool_dma __ro_after_init;",
            "static unsigned long pool_size_dma;",
            "static struct gen_pool *atomic_pool_dma32 __ro_after_init;",
            "static unsigned long pool_size_dma32;",
            "static struct gen_pool *atomic_pool_kernel __ro_after_init;",
            "static unsigned long pool_size_kernel;",
            "",
            "/* Size can be defined by the coherent_pool command line */",
            "static size_t atomic_pool_size;",
            "",
            "/* Dynamic background expansion when the atomic pool is near capacity */",
            "static struct work_struct atomic_pool_work;",
            ""
          ],
          "function_name": null,
          "description": "定义并初始化用于管理DMA内存池的全局变量，包括针对不同架构（DMA/DMA32/内核）的通用池指针、尺寸参数及动态扩展的工作队列。",
          "similarity": 0.5193501114845276
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/dma/pool.c",
          "start_line": 145,
          "end_line": 207,
          "content": [
            "static void atomic_pool_resize(struct gen_pool *pool, gfp_t gfp)",
            "{",
            "\tif (pool && gen_pool_avail(pool) < atomic_pool_size)",
            "\t\tatomic_pool_expand(pool, gen_pool_size(pool), gfp);",
            "}",
            "static void atomic_pool_work_fn(struct work_struct *work)",
            "{",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA))",
            "\t\tatomic_pool_resize(atomic_pool_dma,",
            "\t\t\t\t   GFP_KERNEL | GFP_DMA);",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA32))",
            "\t\tatomic_pool_resize(atomic_pool_dma32,",
            "\t\t\t\t   GFP_KERNEL | GFP_DMA32);",
            "\tatomic_pool_resize(atomic_pool_kernel, GFP_KERNEL);",
            "}",
            "static int __init dma_atomic_pool_init(void)",
            "{",
            "\tint ret = 0;",
            "",
            "\t/*",
            "\t * If coherent_pool was not used on the command line, default the pool",
            "\t * sizes to 128KB per 1GB of memory, min 128KB, max MAX_PAGE_ORDER.",
            "\t */",
            "\tif (!atomic_pool_size) {",
            "\t\tunsigned long pages = totalram_pages() / (SZ_1G / SZ_128K);",
            "\t\tpages = min_t(unsigned long, pages, MAX_ORDER_NR_PAGES);",
            "\t\tatomic_pool_size = max_t(size_t, pages << PAGE_SHIFT, SZ_128K);",
            "\t}",
            "\tINIT_WORK(&atomic_pool_work, atomic_pool_work_fn);",
            "",
            "\tatomic_pool_kernel = __dma_atomic_pool_init(atomic_pool_size,",
            "\t\t\t\t\t\t    GFP_KERNEL);",
            "\tif (!atomic_pool_kernel)",
            "\t\tret = -ENOMEM;",
            "\tif (has_managed_dma()) {",
            "\t\tatomic_pool_dma = __dma_atomic_pool_init(atomic_pool_size,",
            "\t\t\t\t\t\tGFP_KERNEL | GFP_DMA);",
            "\t\tif (!atomic_pool_dma)",
            "\t\t\tret = -ENOMEM;",
            "\t}",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA32)) {",
            "\t\tatomic_pool_dma32 = __dma_atomic_pool_init(atomic_pool_size,",
            "\t\t\t\t\t\tGFP_KERNEL | GFP_DMA32);",
            "\t\tif (!atomic_pool_dma32)",
            "\t\t\tret = -ENOMEM;",
            "\t}",
            "",
            "\tdma_atomic_pool_debugfs_init();",
            "\treturn ret;",
            "}",
            "bool dma_free_from_pool(struct device *dev, void *start, size_t size)",
            "{",
            "\tstruct gen_pool *pool = NULL;",
            "",
            "\twhile ((pool = dma_guess_pool(pool, 0))) {",
            "\t\tif (!gen_pool_has_addr(pool, (unsigned long)start, size))",
            "\t\t\tcontinue;",
            "\t\tgen_pool_free(pool, (unsigned long)start, size);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "atomic_pool_resize, atomic_pool_work_fn, dma_atomic_pool_init, dma_free_from_pool",
          "description": "实现内存池的初始化与维护机制，包含池大小自动调节逻辑、后台扩展任务调度、默认尺寸计算及内存释放查找功能，提供设备内存池的统一管理接口。",
          "similarity": 0.4842296838760376
        }
      ]
    },
    {
      "source_file": "kernel/dma/direct.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:12:42\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\direct.c`\n\n---\n\n# `dma/direct.c` 技术文档\n\n## 1. 文件概述\n\n`dma/direct.c` 实现了 Linux 内核中 **DMA 直接映射操作（DMA direct mapping）** 的核心逻辑。该文件提供了一套不依赖 IOMMU 的 DMA 内存分配与映射机制，适用于物理地址可直接被设备访问的平台（如 x86、ARM64 等无 IOMMU 或 IOMMU 被禁用的场景）。其核心思想是将物理内存地址直接转换为设备可见的 DMA 地址，避免复杂的地址转换开销。\n\n该实现支持多种内存分配策略，包括：\n- 连续物理内存分配（CMA 或 buddy allocator）\n- SWIOTLB 回退机制（用于处理高地址设备无法访问的情况）\n- 原子池分配（用于不可阻塞上下文）\n- 高端内存重映射\n- 内存加密/解密（如 AMD SEV、Intel TDX 等安全特性）\n\n## 2. 核心功能\n\n### 全局变量\n- `zone_dma_bits`：定义 ZONE_DMA 的地址位宽（默认 24 位，即 16MB），可由架构代码覆盖。\n\n### 主要函数\n| 函数名 | 功能描述 |\n|--------|--------|\n| `phys_to_dma_direct()` | 将物理地址转换为 DMA 地址，支持强制解密场景 |\n| `dma_direct_to_page()` | 通过 DMA 地址反查对应的 `struct page` |\n| `dma_direct_get_required_mask()` | 计算设备所需的 DMA 地址掩码（基于系统最大物理地址）|\n| `dma_coherent_ok()` | 检查给定物理地址范围是否在设备的 DMA 地址能力范围内 |\n| `dma_direct_alloc()` | **主入口函数**：为设备分配 DMA 内存，支持多种属性和回退策略 |\n| `__dma_direct_alloc_pages()` | 底层页面分配函数，尝试最优内存区域并回退到低地址区域 |\n| `dma_direct_alloc_from_pool()` | 从原子池分配不可阻塞的 DMA 内存 |\n| `dma_direct_alloc_no_mapping()` | 分配无内核虚拟映射的 DMA 内存（返回 `struct page*`）|\n| `dma_set_decrypted()` / `dma_set_encrypted()` | 设置内存页为解密/加密状态（用于安全虚拟化）|\n\n### 辅助函数\n- `dma_direct_optimal_gfp_mask()`：根据设备 DMA 限制选择最优的 GFP 标志（GFP_DMA / GFP_DMA32）\n- `__dma_direct_free_pages()`：释放通过直接分配获得的页面（优先尝试 SWIOTLB 释放）\n\n## 3. 关键实现\n\n### 3.1 DMA 地址空间约束处理\n- 使用 `dev->coherent_dma_mask` 和 `dev->bus_dma_limit` 确定设备可寻址的物理地址上限。\n- 通过 `dma_coherent_ok()` 验证分配的物理内存是否在设备可访问范围内。\n- 若分配的内存超出范围，则回退到更低地址区域（先尝试 GFP_DMA32，再尝试 GFP_DMA）。\n\n### 3.2 多层次内存分配策略\n1. **首选 CMA 连续内存**：通过 `dma_alloc_contiguous()` 分配。\n2. **回退到 buddy allocator**：使用 `alloc_pages_node()`。\n3. **SWIOTLB 支持**：当设备无法访问高地址时，通过 `swiotlb_alloc()` 分配 bounce buffer。\n4. **原子上下文支持**：在不可阻塞场景下使用 `dma_direct_alloc_from_pool()` 从预分配池中获取内存。\n\n### 3.3 非一致性缓存与内存属性处理\n- 对于非一致性缓存架构（`!dev_is_dma_coherent()`）：\n  - 优先使用全局一致性内存池（`CONFIG_DMA_GLOBAL_POOL`）\n  - 或启用重映射（`CONFIG_DMA_DIRECT_REMAP`）创建 uncached 映射\n  - 或调用架构特定的 `arch_dma_alloc()`\n- 调用 `arch_dma_prep_coherent()` 清理内核别名的脏缓存行。\n\n### 3.4 安全内存处理（加密/解密）\n- 当 `force_dma_unencrypted(dev)` 为真（如 SEV 环境），分配的内存需标记为解密。\n- 使用 `set_memory_decrypted()` / `set_memory_encrypted()` 修改页表属性。\n- 解密操作可能阻塞，因此在原子上下文中需使用内存池。\n\n### 3.5 高端内存（HighMem）处理\n- 若分配的页面位于高端内存（`PageHighMem`），则必须通过 `dma_common_contiguous_remap()` 创建内核虚拟地址映射。\n- 重映射时应用设备特定的页保护属性（`dma_pgprot()`）并处理解密需求。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- `<linux/memblock.h>`：获取 `max_pfn` 系统最大页帧号\n- `<linux/dma-map-ops.h>`：DMA 映射操作抽象接口\n- `<linux/scatterlist.h>`：SG 表支持（间接依赖）\n- `<linux/set_memory.h>`：内存加密/解密操作（`set_memory_decrypted` 等）\n- `<linux/vmalloc.h>`：高端内存重映射支持\n- `\"direct.h\"`：本地 DMA direct 实现的私有头文件\n\n### 配置选项依赖\n- `CONFIG_SWIOTLB`：SWIOTLB bounce buffer 支持\n- `CONFIG_DMA_COHERENT_POOL`：原子上下文 DMA 内存池\n- `CONFIG_DMA_GLOBAL_POOL`：全局一致性 DMA 内存池\n- `CONFIG_DMA_DIRECT_REMAP`：非一致性设备的重映射支持\n- `CONFIG_ZONE_DMA` / `CONFIG_ZONE_DMA32`：低地址内存区域支持\n- `CONFIG_ARCH_HAS_DMA_SET_UNCACHED`：架构特定 uncached 映射支持\n\n### 架构相关依赖\n- `phys_to_dma()` / `dma_to_phys()`：架构提供的物理地址与 DMA 地址转换函数\n- `arch_dma_prep_coherent()`：架构特定的缓存一致性准备\n- `arch_dma_alloc()`：架构特定的 DMA 分配回退路径\n\n## 5. 使用场景\n\n### 5.1 设备驱动 DMA 分配\n- 驱动调用 `dma_alloc_coherent()` 或 `dma_alloc_attrs()` 时，若系统未启用 IOMMU，则最终由 `dma_direct_alloc()` 处理。\n- 适用于大多数无 IOMMU 的嵌入式系统、传统 PC 平台或 IOMMU 被显式禁用的场景。\n\n### 5.2 安全虚拟化环境\n- 在 AMD SEV 或 Intel TDX 等机密计算环境中，DMA 内存需标记为“解密”，该文件通过 `force_dma_unencrypted()` 机制实现。\n\n### 5.3 资源受限或实时系统\n- 通过 `DMA_ATTR_NO_KERNEL_MAPPING` 属性分配无内核映射的 DMA 内存，减少 TLB 压力。\n- 在中断上下文等不可阻塞场景中，自动使用原子内存池分配。\n\n### 5.4 老旧设备兼容\n- 对仅支持 32 位或 24 位 DMA 地址的设备，自动分配低地址内存（通过 GFP_DMA32/GFP_DMA）并验证地址范围。\n\n### 5.5 高端内存平台\n- 在 32 位系统或内存大于直接映射区域的平台上，自动处理高端内存的重映射，确保返回有效的内核虚拟地址。",
      "similarity": 0.5585893392562866,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "kernel/dma/direct.c",
          "start_line": 520,
          "end_line": 633,
          "content": [
            "dma_addr_t dma_direct_map_resource(struct device *dev, phys_addr_t paddr,",
            "\t\tsize_t size, enum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tdma_addr_t dma_addr = paddr;",
            "",
            "\tif (unlikely(!dma_capable(dev, dma_addr, size, false))) {",
            "\t\tdev_err_once(dev,",
            "\t\t\t     \"DMA addr %pad+%zu overflow (mask %llx, bus limit %llx).\\n\",",
            "\t\t\t     &dma_addr, size, *dev->dma_mask, dev->bus_dma_limit);",
            "\t\tWARN_ON_ONCE(1);",
            "\t\treturn DMA_MAPPING_ERROR;",
            "\t}",
            "",
            "\treturn dma_addr;",
            "}",
            "int dma_direct_get_sgtable(struct device *dev, struct sg_table *sgt,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "\tstruct page *page = dma_direct_to_page(dev, dma_addr);",
            "\tint ret;",
            "",
            "\tret = sg_alloc_table(sgt, 1, GFP_KERNEL);",
            "\tif (!ret)",
            "\t\tsg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);",
            "\treturn ret;",
            "}",
            "bool dma_direct_can_mmap(struct device *dev)",
            "{",
            "\treturn dev_is_dma_coherent(dev) ||",
            "\t\tIS_ENABLED(CONFIG_DMA_NONCOHERENT_MMAP);",
            "}",
            "int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,",
            "\t\tunsigned long attrs)",
            "{",
            "\tunsigned long user_count = vma_pages(vma);",
            "\tunsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;",
            "\tunsigned long pfn = PHYS_PFN(dma_to_phys(dev, dma_addr));",
            "\tint ret = -ENXIO;",
            "",
            "\tvma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);",
            "\tif (force_dma_unencrypted(dev))",
            "\t\tvma->vm_page_prot = pgprot_decrypted(vma->vm_page_prot);",
            "",
            "\tif (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))",
            "\t\treturn ret;",
            "\tif (dma_mmap_from_global_coherent(vma, cpu_addr, size, &ret))",
            "\t\treturn ret;",
            "",
            "\tif (vma->vm_pgoff >= count || user_count > count - vma->vm_pgoff)",
            "\t\treturn -ENXIO;",
            "\treturn remap_pfn_range(vma, vma->vm_start, pfn + vma->vm_pgoff,",
            "\t\t\tuser_count << PAGE_SHIFT, vma->vm_page_prot);",
            "}",
            "int dma_direct_supported(struct device *dev, u64 mask)",
            "{",
            "\tu64 min_mask = (max_pfn - 1) << PAGE_SHIFT;",
            "",
            "\t/*",
            "\t * Because 32-bit DMA masks are so common we expect every architecture",
            "\t * to be able to satisfy them - either by not supporting more physical",
            "\t * memory, or by providing a ZONE_DMA32.  If neither is the case, the",
            "\t * architecture needs to use an IOMMU instead of the direct mapping.",
            "\t */",
            "\tif (mask >= DMA_BIT_MASK(32))",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * This check needs to be against the actual bit mask value, so use",
            "\t * phys_to_dma_unencrypted() here so that the SME encryption mask isn't",
            "\t * part of the check.",
            "\t */",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA))",
            "\t\tmin_mask = min_t(u64, min_mask, DMA_BIT_MASK(zone_dma_bits));",
            "\treturn mask >= phys_to_dma_unencrypted(dev, min_mask);",
            "}",
            "size_t dma_direct_max_mapping_size(struct device *dev)",
            "{",
            "\t/* If SWIOTLB is active, use its maximum mapping size */",
            "\tif (is_swiotlb_active(dev) &&",
            "\t    (dma_addressing_limited(dev) || is_swiotlb_force_bounce(dev)))",
            "\t\treturn swiotlb_max_mapping_size(dev);",
            "\treturn SIZE_MAX;",
            "}",
            "bool dma_direct_need_sync(struct device *dev, dma_addr_t dma_addr)",
            "{",
            "\treturn !dev_is_dma_coherent(dev) ||",
            "\t       is_swiotlb_buffer(dev, dma_to_phys(dev, dma_addr));",
            "}",
            "int dma_direct_set_offset(struct device *dev, phys_addr_t cpu_start,",
            "\t\t\t dma_addr_t dma_start, u64 size)",
            "{",
            "\tstruct bus_dma_region *map;",
            "\tu64 offset = (u64)cpu_start - (u64)dma_start;",
            "",
            "\tif (dev->dma_range_map) {",
            "\t\tdev_err(dev, \"attempt to add DMA range to existing map\\n\");",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tif (!offset)",
            "\t\treturn 0;",
            "",
            "\tmap = kcalloc(2, sizeof(*map), GFP_KERNEL);",
            "\tif (!map)",
            "\t\treturn -ENOMEM;",
            "\tmap[0].cpu_start = cpu_start;",
            "\tmap[0].dma_start = dma_start;",
            "\tmap[0].offset = offset;",
            "\tmap[0].size = size;",
            "\tdev->dma_range_map = map;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "dma_direct_map_resource, dma_direct_get_sgtable, dma_direct_can_mmap, dma_direct_mmap, dma_direct_supported, dma_direct_max_mapping_size, dma_direct_need_sync, dma_direct_set_offset",
          "description": "包含DMA直接映射的资源映射、SG表构建、内存映射支持性检测及最大映射尺寸计算等功能。dma_direct_mmap实现设备内存的VMA映射，dma_direct_supported验证DMA掩码兼容性，dma_direct_set_offset用于配置DMA地址偏移量。",
          "similarity": 0.5169765949249268
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/direct.c",
          "start_line": 25,
          "end_line": 140,
          "content": [
            "static inline dma_addr_t phys_to_dma_direct(struct device *dev,",
            "\t\tphys_addr_t phys)",
            "{",
            "\tif (force_dma_unencrypted(dev))",
            "\t\treturn phys_to_dma_unencrypted(dev, phys);",
            "\treturn phys_to_dma(dev, phys);",
            "}",
            "u64 dma_direct_get_required_mask(struct device *dev)",
            "{",
            "\tphys_addr_t phys = (phys_addr_t)(max_pfn - 1) << PAGE_SHIFT;",
            "\tu64 max_dma = phys_to_dma_direct(dev, phys);",
            "",
            "\treturn (1ULL << (fls64(max_dma) - 1)) * 2 - 1;",
            "}",
            "static gfp_t dma_direct_optimal_gfp_mask(struct device *dev, u64 *phys_limit)",
            "{",
            "\tu64 dma_limit = min_not_zero(",
            "\t\tdev->coherent_dma_mask,",
            "\t\tdev->bus_dma_limit);",
            "",
            "\t/*",
            "\t * Optimistically try the zone that the physical address mask falls",
            "\t * into first.  If that returns memory that isn't actually addressable",
            "\t * we will fallback to the next lower zone and try again.",
            "\t *",
            "\t * Note that GFP_DMA32 and GFP_DMA are no ops without the corresponding",
            "\t * zones.",
            "\t */",
            "\t*phys_limit = dma_to_phys(dev, dma_limit);",
            "\tif (*phys_limit <= DMA_BIT_MASK(zone_dma_bits))",
            "\t\treturn GFP_DMA;",
            "\tif (*phys_limit <= DMA_BIT_MASK(32))",
            "\t\treturn GFP_DMA32;",
            "\treturn 0;",
            "}",
            "bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)",
            "{",
            "\tdma_addr_t dma_addr = phys_to_dma_direct(dev, phys);",
            "",
            "\tif (dma_addr == DMA_MAPPING_ERROR)",
            "\t\treturn false;",
            "\treturn dma_addr + size - 1 <=",
            "\t\tmin_not_zero(dev->coherent_dma_mask, dev->bus_dma_limit);",
            "}",
            "static int dma_set_decrypted(struct device *dev, void *vaddr, size_t size)",
            "{",
            "\tif (!force_dma_unencrypted(dev))",
            "\t\treturn 0;",
            "\treturn set_memory_decrypted((unsigned long)vaddr, PFN_UP(size));",
            "}",
            "static int dma_set_encrypted(struct device *dev, void *vaddr, size_t size)",
            "{",
            "\tint ret;",
            "",
            "\tif (!force_dma_unencrypted(dev))",
            "\t\treturn 0;",
            "\tret = set_memory_encrypted((unsigned long)vaddr, PFN_UP(size));",
            "\tif (ret)",
            "\t\tpr_warn_ratelimited(\"leaking DMA memory that can't be re-encrypted\\n\");",
            "\treturn ret;",
            "}",
            "static void __dma_direct_free_pages(struct device *dev, struct page *page,",
            "\t\t\t\t    size_t size)",
            "{",
            "\tif (swiotlb_free(dev, page, size))",
            "\t\treturn;",
            "\tdma_free_contiguous(dev, page, size);",
            "}",
            "static bool dma_direct_use_pool(struct device *dev, gfp_t gfp)",
            "{",
            "\treturn !gfpflags_allow_blocking(gfp) && !is_swiotlb_for_alloc(dev);",
            "}",
            "void dma_direct_free(struct device *dev, size_t size,",
            "\t\tvoid *cpu_addr, dma_addr_t dma_addr, unsigned long attrs)",
            "{",
            "\tunsigned int page_order = get_order(size);",
            "",
            "\tif ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) &&",
            "\t    !force_dma_unencrypted(dev) && !is_swiotlb_for_alloc(dev)) {",
            "\t\t/* cpu_addr is a struct page cookie, not a kernel address */",
            "\t\tdma_free_contiguous(dev, cpu_addr, size);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_SET_UNCACHED) &&",
            "\t    !IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&",
            "\t    !IS_ENABLED(CONFIG_DMA_GLOBAL_POOL) &&",
            "\t    !dev_is_dma_coherent(dev) &&",
            "\t    !is_swiotlb_for_alloc(dev)) {",
            "\t\tarch_dma_free(dev, size, cpu_addr, dma_addr, attrs);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_DMA_GLOBAL_POOL) &&",
            "\t    !dev_is_dma_coherent(dev)) {",
            "\t\tif (!dma_release_from_global_coherent(page_order, cpu_addr))",
            "\t\t\tWARN_ON_ONCE(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */",
            "\tif (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&",
            "\t    dma_free_from_pool(dev, cpu_addr, PAGE_ALIGN(size)))",
            "\t\treturn;",
            "",
            "\tif (is_vmalloc_addr(cpu_addr)) {",
            "\t\tvunmap(cpu_addr);",
            "\t} else {",
            "\t\tif (IS_ENABLED(CONFIG_ARCH_HAS_DMA_CLEAR_UNCACHED))",
            "\t\t\tarch_dma_clear_uncached(cpu_addr, size);",
            "\t\tif (dma_set_encrypted(dev, cpu_addr, size))",
            "\t\t\treturn;",
            "\t}",
            "",
            "\t__dma_direct_free_pages(dev, dma_direct_to_page(dev, dma_addr), size);",
            "}"
          ],
          "function_name": "phys_to_dma_direct, dma_direct_get_required_mask, dma_direct_optimal_gfp_mask, dma_coherent_ok, dma_set_decrypted, dma_set_encrypted, __dma_direct_free_pages, dma_direct_use_pool, dma_direct_free",
          "description": "实现了DMA直接映射的核心辅助函数，包括物理地址转DMA地址、计算DMA掩码、优化内存分配策略、检查DMA一致性及加密内存设置等功能。dma_direct_free处理不同条件下的内存释放逻辑，涉及SWIOTLB、原子池和架构特定的释放路径。",
          "similarity": 0.49311214685440063
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/direct.c",
          "start_line": 1,
          "end_line": 24,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 2018-2020 Christoph Hellwig.",
            " *",
            " * DMA operations that map physical memory directly without using an IOMMU.",
            " */",
            "#include <linux/memblock.h> /* for max_pfn */",
            "#include <linux/export.h>",
            "#include <linux/mm.h>",
            "#include <linux/dma-map-ops.h>",
            "#include <linux/scatterlist.h>",
            "#include <linux/pfn.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/slab.h>",
            "#include \"direct.h\"",
            "",
            "/*",
            " * Most architectures use ZONE_DMA for the first 16 Megabytes, but some use",
            " * it for entirely different regions. In that case the arch code needs to",
            " * override the variable below for dma-direct to work properly.",
            " */",
            "unsigned int zone_dma_bits __ro_after_init = 24;",
            ""
          ],
          "function_name": null,
          "description": "定义了用于DMA直接映射的全局变量zone_dma_bits，默认值为24位，表示DMA地址空间的位宽。该变量用于控制DMA操作的物理地址范围，架构代码可通过覆盖此变量调整DMA直接映射的行为。",
          "similarity": 0.4643070697784424
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/dma/direct.c",
          "start_line": 391,
          "end_line": 503,
          "content": [
            "void dma_direct_free_pages(struct device *dev, size_t size,",
            "\t\tstruct page *page, dma_addr_t dma_addr,",
            "\t\tenum dma_data_direction dir)",
            "{",
            "\tvoid *vaddr = page_address(page);",
            "",
            "\t/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */",
            "\tif (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&",
            "\t    dma_free_from_pool(dev, vaddr, size))",
            "\t\treturn;",
            "",
            "\tif (dma_set_encrypted(dev, vaddr, size))",
            "\t\treturn;",
            "\t__dma_direct_free_pages(dev, page, size);",
            "}",
            "void dma_direct_sync_sg_for_device(struct device *dev,",
            "\t\tstruct scatterlist *sgl, int nents, enum dma_data_direction dir)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tphys_addr_t paddr = dma_to_phys(dev, sg_dma_address(sg));",
            "",
            "\t\tif (unlikely(is_swiotlb_buffer(dev, paddr)))",
            "\t\t\tswiotlb_sync_single_for_device(dev, paddr, sg->length,",
            "\t\t\t\t\t\t       dir);",
            "",
            "\t\tif (!dev_is_dma_coherent(dev))",
            "\t\t\tarch_sync_dma_for_device(paddr, sg->length,",
            "\t\t\t\t\tdir);",
            "\t}",
            "}",
            "void dma_direct_sync_sg_for_cpu(struct device *dev,",
            "\t\tstruct scatterlist *sgl, int nents, enum dma_data_direction dir)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tphys_addr_t paddr = dma_to_phys(dev, sg_dma_address(sg));",
            "",
            "\t\tif (!dev_is_dma_coherent(dev))",
            "\t\t\tarch_sync_dma_for_cpu(paddr, sg->length, dir);",
            "",
            "\t\tif (unlikely(is_swiotlb_buffer(dev, paddr)))",
            "\t\t\tswiotlb_sync_single_for_cpu(dev, paddr, sg->length,",
            "\t\t\t\t\t\t    dir);",
            "",
            "\t\tif (dir == DMA_FROM_DEVICE)",
            "\t\t\tarch_dma_mark_clean(paddr, sg->length);",
            "\t}",
            "",
            "\tif (!dev_is_dma_coherent(dev))",
            "\t\tarch_sync_dma_for_cpu_all();",
            "}",
            "void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,",
            "\t\tint nents, enum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tstruct scatterlist *sg;",
            "\tint i;",
            "",
            "\tfor_each_sg(sgl,  sg, nents, i) {",
            "\t\tif (sg_dma_is_bus_address(sg))",
            "\t\t\tsg_dma_unmark_bus_address(sg);",
            "\t\telse",
            "\t\t\tdma_direct_unmap_page(dev, sg->dma_address,",
            "\t\t\t\t\t      sg_dma_len(sg), dir, attrs);",
            "\t}",
            "}",
            "int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,",
            "\t\tenum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tstruct pci_p2pdma_map_state p2pdma_state = {};",
            "\tenum pci_p2pdma_map_type map;",
            "\tstruct scatterlist *sg;",
            "\tint i, ret;",
            "",
            "\tfor_each_sg(sgl, sg, nents, i) {",
            "\t\tif (is_pci_p2pdma_page(sg_page(sg))) {",
            "\t\t\tmap = pci_p2pdma_map_segment(&p2pdma_state, dev, sg);",
            "\t\t\tswitch (map) {",
            "\t\t\tcase PCI_P2PDMA_MAP_BUS_ADDR:",
            "\t\t\t\tcontinue;",
            "\t\t\tcase PCI_P2PDMA_MAP_THRU_HOST_BRIDGE:",
            "\t\t\t\t/*",
            "\t\t\t\t * Any P2P mapping that traverses the PCI",
            "\t\t\t\t * host bridge must be mapped with CPU physical",
            "\t\t\t\t * address and not PCI bus addresses. This is",
            "\t\t\t\t * done with dma_direct_map_page() below.",
            "\t\t\t\t */",
            "\t\t\t\tbreak;",
            "\t\t\tdefault:",
            "\t\t\t\tret = -EREMOTEIO;",
            "\t\t\t\tgoto out_unmap;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tsg->dma_address = dma_direct_map_page(dev, sg_page(sg),",
            "\t\t\t\tsg->offset, sg->length, dir, attrs);",
            "\t\tif (sg->dma_address == DMA_MAPPING_ERROR) {",
            "\t\t\tret = -EIO;",
            "\t\t\tgoto out_unmap;",
            "\t\t}",
            "\t\tsg_dma_len(sg) = sg->length;",
            "\t}",
            "",
            "\treturn nents;",
            "",
            "out_unmap:",
            "\tdma_direct_unmap_sg(dev, sgl, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "dma_direct_free_pages, dma_direct_sync_sg_for_device, dma_direct_sync_sg_for_cpu, dma_direct_unmap_sg, dma_direct_map_sg",
          "description": "提供了SCATTERLIST的同步、解映射和映射操作实现，包括对非一致内存的架构同步、PCI P2P DMA的特殊处理以及SG表的构建。sync_sg系列函数负责设备与CPU之间的数据缓存一致性维护。",
          "similarity": 0.4504104256629944
        }
      ]
    }
  ]
}