{
  "query": "页表结构在Linux内核中的作用",
  "timestamp": "2025-12-25 23:25:51",
  "retrieved_files": [
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.6638950109481812,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.6102898120880127
        },
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.520865261554718
        },
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.505458652973175
        }
      ]
    },
    {
      "source_file": "mm/memory.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:42:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memory.c`\n\n---\n\n# memory.c 技术文档\n\n## 1. 文件概述\n\n`memory.c` 是 Linux 内核内存管理子系统（MM）的核心实现文件之一，位于 `mm/` 目录下。该文件主要负责虚拟内存到物理内存的映射管理、缺页异常（page fault）处理、页表结构的分配与释放、以及与用户空间内存操作相关的底层机制。它实现了按需加载（demand-loading）、共享页面、交换（swapping）等关键虚拟内存功能，并为多级页表架构（如 x86-64 的四级页表）提供通用支持。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `high_memory`：指向直接映射区域（ZONE_NORMAL）的上界，用于区分低端内存和高端内存。\n- `randomize_va_space`：控制地址空间布局随机化（ASLR）策略的级别（0=关闭，1=部分启用，2=完全启用）。\n- `zero_pfn`：指向全零物理页的页帧号（PFN），用于高效实现只读零页映射。\n- `max_mapnr` 和 `mem_map`（非 NUMA 配置下）：分别表示最大页帧号和全局页描述符数组。\n- `highest_memmap_pfn`：记录系统中最高的已注册页帧号。\n\n### 关键函数\n- `free_pgd_range()`：释放指定虚拟地址范围内的用户级页表结构（从 PGD 到 PTE）。\n- `free_p4d_range()`, `free_pud_range()`, `free_pmd_range()`, `free_pte_range()`：递归释放各级页表项及其对应的页表页。\n- `do_fault()`：处理基于文件映射的缺页异常。\n- `do_anonymous_page()`：处理匿名映射（如堆、栈）的缺页异常。\n- `vmf_orig_pte_uffd_wp()`：判断原始 PTE 是否为 userfaultfd 写保护标记。\n- `init_zero_pfn()`：早期初始化 `zero_pfn`。\n- `mm_trace_rss_stat()`：触发 RSS（Resident Set Size）统计的跟踪事件。\n\n### 内联辅助函数\n- `arch_wants_old_prefaulted_pte()`：允许架构层决定预取页表项是否应标记为“old”以优化访问位更新开销。\n\n## 3. 关键实现\n\n### 页表释放机制\n- 采用自顶向下（PGD → P4D → PUD → PMD → PTE）的递归方式释放页表。\n- 每级释放函数（如 `free_pmd_range`）遍历地址范围内的页表项：\n  - 跳过空或无效项（`pmd_none_or_clear_bad`）。\n  - 递归释放下一级页表。\n  - 在满足对齐和边界条件（`floor`/`ceiling`）时，释放当前级页表页并更新 MMU gather 结构中的计数器（如 `mm_dec_nr_ptes`）。\n- 使用 `mmu_gather` 机制批量延迟 TLB 刷新和页表页释放，提升性能。\n\n### 缺页处理框架\n- 提供 `do_fault` 和 `do_anonymous_page` 作为缺页处理的核心入口，分别处理文件映射和匿名映射。\n- 支持 `userfaultfd` 写保护机制，通过 `vmf_orig_pte_uffd_wp` 检测特殊 PTE 标记。\n\n### 地址空间随机化（ASLR）\n- 通过 `randomize_va_space` 控制栈、mmap 区域、brk 等的随机化行为。\n- 支持内核启动参数 `norandmaps` 完全禁用 ASLR。\n- 兼容旧版 libc5 二进制（`CONFIG_COMPAT_BRK`），此时 brk 区域不参与随机化。\n\n### 零页优化\n- `zero_pfn` 指向一个全局只读的全零物理页，用于高效实现对未初始化数据段（如 `.bss`）或显式映射 `/dev/zero` 的只读访问，避免每次分配新页。\n\n### 架构适配\n- 通过 `arch_wants_old_prefaulted_pte` 允许特定架构优化页表项的“young/old”状态设置。\n- 依赖 `asm/mmu_context.h`、`asm/pgalloc.h`、`asm/tlb.h` 等架构相关头文件实现底层操作。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- **内存管理核心**：`<linux/mm.h>`, `<linux/mman.h>`, `<linux/swap.h>`, `<linux/pagemap.h>`, `<linux/memcontrol.h>`\n- **进程与调度**：`<linux/sched/mm.h>`, `<linux/sched/task.h>`, `<linux/delayacct.h>`\n- **NUMA 与迁移**：`<linux/numa.h>`, `<linux/migrate.h>`, `<linux/sched/numa_balancing.h>`\n- **特殊内存类型**：`<linux/hugetlb.h>`, `<linux/highmem.h>`, `<linux/dax.h>`, `<linux/zswap.h>`\n- **调试与跟踪**：`<trace/events/kmem.h>`, `<linux/debugfs.h>`, `<linux/oom.h>`\n- **架构相关**：`<asm/io.h>`, `<asm/mmu_context.h>`, `<asm/pgalloc.h>`, `<asm/tlbflush.h>`\n\n### 内部模块依赖\n- `internal.h`：包含 MM 子系统内部通用定义。\n- `swap.h`：交换子系统接口。\n- `pgalloc-track.h`：页表分配跟踪（用于调试）。\n\n## 5. 使用场景\n\n- **进程创建与退出**：在 `fork()` 和进程终止时，通过 `free_pgd_range` 释放整个地址空间的页表。\n- **内存映射操作**：`mmap()`、`munmap()`、`mremap()` 等系统调用触发页表的建立或释放。\n- **缺页异常处理**：当 CPU 访问未映射或换出的虚拟地址时，由体系结构相关的缺页处理程序调用 `do_fault` 或 `do_anonymous_page`。\n- **内存回收**：在内存压力下，kswapd 或直接回收路径可能触发页表清理。\n- **用户态内存监控**：`userfaultfd` 机制利用 `vmf_orig_pte_uffd_wp` 实现用户空间对缺页事件的精细控制。\n- **内核初始化**：早期调用 `init_zero_pfn` 设置零页，`paging_init()`（架构相关）初始化 `high_memory` 和 `ZERO_PAGE`。",
      "similarity": 0.6539894938468933,
      "chunks": [
        {
          "chunk_id": 33,
          "file_path": "mm/memory.c",
          "start_line": 5775,
          "end_line": 5921,
          "content": [
            "static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,",
            "\t\tunsigned long address, unsigned int flags)",
            "{",
            "\tstruct vm_fault vmf = {",
            "\t\t.vma = vma,",
            "\t\t.address = address & PAGE_MASK,",
            "\t\t.real_address = address,",
            "\t\t.flags = flags,",
            "\t\t.pgoff = linear_page_index(vma, address),",
            "\t\t.gfp_mask = __get_fault_gfp_mask(vma),",
            "\t};",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tunsigned long vm_flags = vma->vm_flags;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tvm_fault_t ret;",
            "",
            "\tpgd = pgd_offset(mm, address);",
            "\tp4d = p4d_alloc(mm, pgd, address);",
            "\tif (!p4d)",
            "\t\treturn VM_FAULT_OOM;",
            "",
            "\tvmf.pud = pud_alloc(mm, p4d, address);",
            "\tif (!vmf.pud)",
            "\t\treturn VM_FAULT_OOM;",
            "retry_pud:",
            "\tif (pud_none(*vmf.pud) &&",
            "\t    thp_vma_allowable_order(vma, vm_flags,",
            "\t\t\t\tTVA_IN_PF | TVA_ENFORCE_SYSFS, PUD_ORDER)) {",
            "\t\tret = create_huge_pud(&vmf);",
            "\t\tif (!(ret & VM_FAULT_FALLBACK))",
            "\t\t\treturn ret;",
            "\t} else {",
            "\t\tpud_t orig_pud = *vmf.pud;",
            "",
            "\t\tbarrier();",
            "\t\tif (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {",
            "",
            "\t\t\t/*",
            "\t\t\t * TODO once we support anonymous PUDs: NUMA case and",
            "\t\t\t * FAULT_FLAG_UNSHARE handling.",
            "\t\t\t */",
            "\t\t\tif ((flags & FAULT_FLAG_WRITE) && !pud_write(orig_pud)) {",
            "\t\t\t\tret = wp_huge_pud(&vmf, orig_pud);",
            "\t\t\t\tif (!(ret & VM_FAULT_FALLBACK))",
            "\t\t\t\t\treturn ret;",
            "\t\t\t} else {",
            "\t\t\t\thuge_pud_set_accessed(&vmf, orig_pud);",
            "\t\t\t\treturn 0;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\tvmf.pmd = pmd_alloc(mm, vmf.pud, address);",
            "\tif (!vmf.pmd)",
            "\t\treturn VM_FAULT_OOM;",
            "",
            "\t/* Huge pud page fault raced with pmd_alloc? */",
            "\tif (pud_trans_unstable(vmf.pud))",
            "\t\tgoto retry_pud;",
            "",
            "\tif (pmd_none(*vmf.pmd) &&",
            "\t    thp_vma_allowable_order(vma, vm_flags,",
            "\t\t\t\tTVA_IN_PF | TVA_ENFORCE_SYSFS, PMD_ORDER)) {",
            "\t\tret = create_huge_pmd(&vmf);",
            "\t\tif (!(ret & VM_FAULT_FALLBACK))",
            "\t\t\treturn ret;",
            "\t} else {",
            "\t\tvmf.orig_pmd = pmdp_get_lockless(vmf.pmd);",
            "",
            "\t\tif (unlikely(is_swap_pmd(vmf.orig_pmd))) {",
            "\t\t\tVM_BUG_ON(thp_migration_supported() &&",
            "\t\t\t\t\t  !is_pmd_migration_entry(vmf.orig_pmd));",
            "\t\t\tif (is_pmd_migration_entry(vmf.orig_pmd))",
            "\t\t\t\tpmd_migration_entry_wait(mm, vmf.pmd);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t\tif (pmd_trans_huge(vmf.orig_pmd) || pmd_devmap(vmf.orig_pmd)) {",
            "\t\t\tif (pmd_protnone(vmf.orig_pmd) && vma_is_accessible(vma))",
            "\t\t\t\treturn do_huge_pmd_numa_page(&vmf);",
            "",
            "\t\t\tif ((flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) &&",
            "\t\t\t    !pmd_write(vmf.orig_pmd)) {",
            "\t\t\t\tret = wp_huge_pmd(&vmf);",
            "\t\t\t\tif (!(ret & VM_FAULT_FALLBACK))",
            "\t\t\t\t\treturn ret;",
            "\t\t\t} else {",
            "\t\t\t\thuge_pmd_set_accessed(&vmf);",
            "\t\t\t\treturn 0;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\treturn handle_pte_fault(&vmf);",
            "}",
            "static inline void mm_account_fault(struct mm_struct *mm, struct pt_regs *regs,",
            "\t\t\t\t    unsigned long address, unsigned int flags,",
            "\t\t\t\t    vm_fault_t ret)",
            "{",
            "\tbool major;",
            "",
            "\t/* Incomplete faults will be accounted upon completion. */",
            "\tif (ret & VM_FAULT_RETRY)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * To preserve the behavior of older kernels, PGFAULT counters record",
            "\t * both successful and failed faults, as opposed to perf counters,",
            "\t * which ignore failed cases.",
            "\t */",
            "\tcount_vm_event(PGFAULT);",
            "\tcount_memcg_event_mm(mm, PGFAULT);",
            "",
            "\t/*",
            "\t * Do not account for unsuccessful faults (e.g. when the address wasn't",
            "\t * valid).  That includes arch_vma_access_permitted() failing before",
            "\t * reaching here. So this is not a \"this many hardware page faults\"",
            "\t * counter.  We should use the hw profiling for that.",
            "\t */",
            "\tif (ret & VM_FAULT_ERROR)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * We define the fault as a major fault when the final successful fault",
            "\t * is VM_FAULT_MAJOR, or if it retried (which implies that we couldn't",
            "\t * handle it immediately previously).",
            "\t */",
            "\tmajor = (ret & VM_FAULT_MAJOR) || (flags & FAULT_FLAG_TRIED);",
            "",
            "\tif (major)",
            "\t\tcurrent->maj_flt++;",
            "\telse",
            "\t\tcurrent->min_flt++;",
            "",
            "\t/*",
            "\t * If the fault is done for GUP, regs will be NULL.  We only do the",
            "\t * accounting for the per thread fault counters who triggered the",
            "\t * fault, and we skip the perf event updates.",
            "\t */",
            "\tif (!regs)",
            "\t\treturn;",
            "",
            "\tif (major)",
            "\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);",
            "\telse",
            "\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);",
            "}"
          ],
          "function_name": "__handle_mm_fault, mm_account_fault",
          "description": "核心内存故障处理函数，负责页表分配、大页检查及异常处理，最终调用handle_pte_fault完成页面故障处理并记录统计信息",
          "similarity": 0.6375927925109863
        },
        {
          "chunk_id": 40,
          "file_path": "mm/memory.c",
          "start_line": 6963,
          "end_line": 6991,
          "content": [
            "void __init ptlock_cache_init(void)",
            "{",
            "\tpage_ptl_cachep = kmem_cache_create(\"page->ptl\", sizeof(spinlock_t), 0,",
            "\t\t\tSLAB_PANIC, NULL);",
            "}",
            "bool ptlock_alloc(struct ptdesc *ptdesc)",
            "{",
            "\tspinlock_t *ptl;",
            "",
            "\tptl = kmem_cache_alloc(page_ptl_cachep, GFP_KERNEL);",
            "\tif (!ptl)",
            "\t\treturn false;",
            "\tptdesc->ptl = ptl;",
            "\treturn true;",
            "}",
            "void ptlock_free(struct ptdesc *ptdesc)",
            "{",
            "\tkmem_cache_free(page_ptl_cachep, ptdesc->ptl);",
            "}",
            "void vma_pgtable_walk_begin(struct vm_area_struct *vma)",
            "{",
            "\tif (is_vm_hugetlb_page(vma))",
            "\t\thugetlb_vma_lock_read(vma);",
            "}",
            "void vma_pgtable_walk_end(struct vm_area_struct *vma)",
            "{",
            "\tif (is_vm_hugetlb_page(vma))",
            "\t\thugetlb_vma_unlock_read(vma);",
            "}"
          ],
          "function_name": "ptlock_cache_init, ptlock_alloc, ptlock_free, vma_pgtable_walk_begin, vma_pgtable_walk_end",
          "description": "该代码段主要实现页表锁定资源的管理及HUGETLB页面的读锁控制。  \n`ptlock_cache_init` 初始化用于分配 `spinlock_t` 的 slab 缓存；`ptlock_alloc/free` 分配和释放页表锁资源；`vma_pgtable_walk_*` 对 HUGETLB 页面进行读锁保护。  \n上下文不完整：缺少页表遍历具体调用链及锁使用的完整逻辑。",
          "similarity": 0.6229854822158813
        },
        {
          "chunk_id": 2,
          "file_path": "mm/memory.c",
          "start_line": 267,
          "end_line": 414,
          "content": [
            "static inline void free_p4d_range(struct mmu_gather *tlb, pgd_t *pgd,",
            "\t\t\t\tunsigned long addr, unsigned long end,",
            "\t\t\t\tunsigned long floor, unsigned long ceiling)",
            "{",
            "\tp4d_t *p4d;",
            "\tunsigned long next;",
            "\tunsigned long start;",
            "",
            "\tstart = addr;",
            "\tp4d = p4d_offset(pgd, addr);",
            "\tdo {",
            "\t\tnext = p4d_addr_end(addr, end);",
            "\t\tif (p4d_none_or_clear_bad(p4d))",
            "\t\t\tcontinue;",
            "\t\tfree_pud_range(tlb, p4d, addr, next, floor, ceiling);",
            "\t} while (p4d++, addr = next, addr != end);",
            "",
            "\tstart &= PGDIR_MASK;",
            "\tif (start < floor)",
            "\t\treturn;",
            "\tif (ceiling) {",
            "\t\tceiling &= PGDIR_MASK;",
            "\t\tif (!ceiling)",
            "\t\t\treturn;",
            "\t}",
            "\tif (end - 1 > ceiling - 1)",
            "\t\treturn;",
            "",
            "\tp4d = p4d_offset(pgd, start);",
            "\tpgd_clear(pgd);",
            "\tp4d_free_tlb(tlb, p4d, start);",
            "}",
            "void free_pgd_range(struct mmu_gather *tlb,",
            "\t\t\tunsigned long addr, unsigned long end,",
            "\t\t\tunsigned long floor, unsigned long ceiling)",
            "{",
            "\tpgd_t *pgd;",
            "\tunsigned long next;",
            "",
            "\t/*",
            "\t * The next few lines have given us lots of grief...",
            "\t *",
            "\t * Why are we testing PMD* at this top level?  Because often",
            "\t * there will be no work to do at all, and we'd prefer not to",
            "\t * go all the way down to the bottom just to discover that.",
            "\t *",
            "\t * Why all these \"- 1\"s?  Because 0 represents both the bottom",
            "\t * of the address space and the top of it (using -1 for the",
            "\t * top wouldn't help much: the masks would do the wrong thing).",
            "\t * The rule is that addr 0 and floor 0 refer to the bottom of",
            "\t * the address space, but end 0 and ceiling 0 refer to the top",
            "\t * Comparisons need to use \"end - 1\" and \"ceiling - 1\" (though",
            "\t * that end 0 case should be mythical).",
            "\t *",
            "\t * Wherever addr is brought up or ceiling brought down, we must",
            "\t * be careful to reject \"the opposite 0\" before it confuses the",
            "\t * subsequent tests.  But what about where end is brought down",
            "\t * by PMD_SIZE below? no, end can't go down to 0 there.",
            "\t *",
            "\t * Whereas we round start (addr) and ceiling down, by different",
            "\t * masks at different levels, in order to test whether a table",
            "\t * now has no other vmas using it, so can be freed, we don't",
            "\t * bother to round floor or end up - the tests don't need that.",
            "\t */",
            "",
            "\taddr &= PMD_MASK;",
            "\tif (addr < floor) {",
            "\t\taddr += PMD_SIZE;",
            "\t\tif (!addr)",
            "\t\t\treturn;",
            "\t}",
            "\tif (ceiling) {",
            "\t\tceiling &= PMD_MASK;",
            "\t\tif (!ceiling)",
            "\t\t\treturn;",
            "\t}",
            "\tif (end - 1 > ceiling - 1)",
            "\t\tend -= PMD_SIZE;",
            "\tif (addr > end - 1)",
            "\t\treturn;",
            "\t/*",
            "\t * We add page table cache pages with PAGE_SIZE,",
            "\t * (see pte_free_tlb()), flush the tlb if we need",
            "\t */",
            "\ttlb_change_page_size(tlb, PAGE_SIZE);",
            "\tpgd = pgd_offset(tlb->mm, addr);",
            "\tdo {",
            "\t\tnext = pgd_addr_end(addr, end);",
            "\t\tif (pgd_none_or_clear_bad(pgd))",
            "\t\t\tcontinue;",
            "\t\tfree_p4d_range(tlb, pgd, addr, next, floor, ceiling);",
            "\t} while (pgd++, addr = next, addr != end);",
            "}",
            "void free_pgtables(struct mmu_gather *tlb, struct ma_state *mas,",
            "\t\t   struct vm_area_struct *vma, unsigned long floor,",
            "\t\t   unsigned long ceiling, bool mm_wr_locked)",
            "{",
            "\tstruct unlink_vma_file_batch vb;",
            "",
            "\tdo {",
            "\t\tunsigned long addr = vma->vm_start;",
            "\t\tstruct vm_area_struct *next;",
            "",
            "\t\t/*",
            "\t\t * Note: USER_PGTABLES_CEILING may be passed as ceiling and may",
            "\t\t * be 0.  This will underflow and is okay.",
            "\t\t */",
            "\t\tnext = mas_find(mas, ceiling - 1);",
            "\t\tif (unlikely(xa_is_zero(next)))",
            "\t\t\tnext = NULL;",
            "",
            "\t\t/*",
            "\t\t * Hide vma from rmap and truncate_pagecache before freeing",
            "\t\t * pgtables",
            "\t\t */",
            "\t\tif (mm_wr_locked)",
            "\t\t\tvma_start_write(vma);",
            "\t\tunlink_anon_vmas(vma);",
            "",
            "\t\tif (is_vm_hugetlb_page(vma)) {",
            "\t\t\tunlink_file_vma(vma);",
            "\t\t\thugetlb_free_pgd_range(tlb, addr, vma->vm_end,",
            "\t\t\t\tfloor, next ? next->vm_start : ceiling);",
            "\t\t} else {",
            "\t\t\tunlink_file_vma_batch_init(&vb);",
            "\t\t\tunlink_file_vma_batch_add(&vb, vma);",
            "",
            "\t\t\t/*",
            "\t\t\t * Optimization: gather nearby vmas into one call down",
            "\t\t\t */",
            "\t\t\twhile (next && next->vm_start <= vma->vm_end + PMD_SIZE",
            "\t\t\t       && !is_vm_hugetlb_page(next)) {",
            "\t\t\t\tvma = next;",
            "\t\t\t\tnext = mas_find(mas, ceiling - 1);",
            "\t\t\t\tif (unlikely(xa_is_zero(next)))",
            "\t\t\t\t\tnext = NULL;",
            "\t\t\t\tif (mm_wr_locked)",
            "\t\t\t\t\tvma_start_write(vma);",
            "\t\t\t\tunlink_anon_vmas(vma);",
            "\t\t\t\tunlink_file_vma_batch_add(&vb, vma);",
            "\t\t\t}",
            "\t\t\tunlink_file_vma_batch_final(&vb);",
            "\t\t\tfree_pgd_range(tlb, addr, vma->vm_end,",
            "\t\t\t\tfloor, next ? next->vm_start : ceiling);",
            "\t\t}",
            "\t\tvma = next;",
            "\t} while (vma);",
            "}"
          ],
          "function_name": "free_p4d_range, free_pgd_range, free_pgtables",
          "description": "实现多级页表（PGD/PMD/PUD/P4D）的释放逻辑，通过遍历地址范围清理无效页表项并减少相应的页表计数",
          "similarity": 0.6020397543907166
        },
        {
          "chunk_id": 31,
          "file_path": "mm/memory.c",
          "start_line": 5451,
          "end_line": 5592,
          "content": [
            "static void numa_rebuild_single_mapping(struct vm_fault *vmf, struct vm_area_struct *vma,",
            "\t\t\t\t\tunsigned long fault_addr, pte_t *fault_pte,",
            "\t\t\t\t\tbool writable)",
            "{",
            "\tpte_t pte, old_pte;",
            "",
            "\told_pte = ptep_modify_prot_start(vma, fault_addr, fault_pte);",
            "\tpte = pte_modify(old_pte, vma->vm_page_prot);",
            "\tpte = pte_mkyoung(pte);",
            "\tif (writable)",
            "\t\tpte = pte_mkwrite(pte, vma);",
            "\tptep_modify_prot_commit(vma, fault_addr, fault_pte, old_pte, pte);",
            "\tupdate_mmu_cache_range(vmf, vma, fault_addr, fault_pte, 1);",
            "}",
            "static void numa_rebuild_large_mapping(struct vm_fault *vmf, struct vm_area_struct *vma,",
            "\t\t\t\t       struct folio *folio, pte_t fault_pte,",
            "\t\t\t\t       bool ignore_writable, bool pte_write_upgrade)",
            "{",
            "\tint nr = pte_pfn(fault_pte) - folio_pfn(folio);",
            "\tunsigned long start, end, addr = vmf->address;",
            "\tunsigned long addr_start = addr - (nr << PAGE_SHIFT);",
            "\tunsigned long pt_start = ALIGN_DOWN(addr, PMD_SIZE);",
            "\tpte_t *start_ptep;",
            "",
            "\t/* Stay within the VMA and within the page table. */",
            "\tstart = max3(addr_start, pt_start, vma->vm_start);",
            "\tend = min3(addr_start + folio_size(folio), pt_start + PMD_SIZE,",
            "\t\t   vma->vm_end);",
            "\tstart_ptep = vmf->pte - ((addr - start) >> PAGE_SHIFT);",
            "",
            "\t/* Restore all PTEs' mapping of the large folio */",
            "\tfor (addr = start; addr != end; start_ptep++, addr += PAGE_SIZE) {",
            "\t\tpte_t ptent = ptep_get(start_ptep);",
            "\t\tbool writable = false;",
            "",
            "\t\tif (!pte_present(ptent) || !pte_protnone(ptent))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (pfn_folio(pte_pfn(ptent)) != folio)",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!ignore_writable) {",
            "\t\t\tptent = pte_modify(ptent, vma->vm_page_prot);",
            "\t\t\twritable = pte_write(ptent);",
            "\t\t\tif (!writable && pte_write_upgrade &&",
            "\t\t\t    can_change_pte_writable(vma, addr, ptent))",
            "\t\t\t\twritable = true;",
            "\t\t}",
            "",
            "\t\tnuma_rebuild_single_mapping(vmf, vma, addr, start_ptep, writable);",
            "\t}",
            "}",
            "static vm_fault_t do_numa_page(struct vm_fault *vmf)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tstruct folio *folio = NULL;",
            "\tint nid = NUMA_NO_NODE;",
            "\tbool writable = false, ignore_writable = false;",
            "\tbool pte_write_upgrade = vma_wants_manual_pte_write_upgrade(vma);",
            "\tint last_cpupid;",
            "\tint target_nid;",
            "\tpte_t pte, old_pte;",
            "\tint flags = 0, nr_pages;",
            "",
            "\t/*",
            "\t * The pte cannot be used safely until we verify, while holding the page",
            "\t * table lock, that its contents have not changed during fault handling.",
            "\t */",
            "\tspin_lock(vmf->ptl);",
            "\t/* Read the live PTE from the page tables: */",
            "\told_pte = ptep_get(vmf->pte);",
            "",
            "\tif (unlikely(!pte_same(old_pte, vmf->orig_pte))) {",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tpte = pte_modify(old_pte, vma->vm_page_prot);",
            "",
            "\t/*",
            "\t * Detect now whether the PTE could be writable; this information",
            "\t * is only valid while holding the PT lock.",
            "\t */",
            "\twritable = pte_write(pte);",
            "\tif (!writable && pte_write_upgrade &&",
            "\t    can_change_pte_writable(vma, vmf->address, pte))",
            "\t\twritable = true;",
            "",
            "\tfolio = vm_normal_folio(vma, vmf->address, pte);",
            "\tif (!folio || folio_is_zone_device(folio))",
            "\t\tgoto out_map;",
            "",
            "\tnid = folio_nid(folio);",
            "\tnr_pages = folio_nr_pages(folio);",
            "",
            "\ttarget_nid = numa_migrate_check(folio, vmf, vmf->address, &flags,",
            "\t\t\t\t\twritable, &last_cpupid);",
            "\tif (target_nid == NUMA_NO_NODE)",
            "\t\tgoto out_map;",
            "\tif (migrate_misplaced_folio_prepare(folio, vma, target_nid)) {",
            "\t\tflags |= TNF_MIGRATE_FAIL;",
            "\t\tgoto out_map;",
            "\t}",
            "\t/* The folio is isolated and isolation code holds a folio reference. */",
            "\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\twritable = false;",
            "\tignore_writable = true;",
            "",
            "\t/* Migrate to the requested node */",
            "\tif (!migrate_misplaced_folio(folio, vma, target_nid)) {",
            "\t\tnid = target_nid;",
            "\t\tflags |= TNF_MIGRATED;",
            "\t\ttask_numa_fault(last_cpupid, nid, nr_pages, flags);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tflags |= TNF_MIGRATE_FAIL;",
            "\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t       vmf->address, &vmf->ptl);",
            "\tif (unlikely(!vmf->pte))",
            "\t\treturn 0;",
            "\tif (unlikely(!pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\t\treturn 0;",
            "\t}",
            "out_map:",
            "\t/*",
            "\t * Make it present again, depending on how arch implements",
            "\t * non-accessible ptes, some can allow access by kernel mode.",
            "\t */",
            "\tif (folio && folio_test_large(folio))",
            "\t\tnuma_rebuild_large_mapping(vmf, vma, folio, pte, ignore_writable,",
            "\t\t\t\t\t   pte_write_upgrade);",
            "\telse",
            "\t\tnuma_rebuild_single_mapping(vmf, vma, vmf->address, vmf->pte,",
            "\t\t\t\t\t    writable);",
            "\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "",
            "\tif (nid != NUMA_NO_NODE)",
            "\t\ttask_numa_fault(last_cpupid, nid, nr_pages, flags);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "numa_rebuild_single_mapping, numa_rebuild_large_mapping, do_numa_page",
          "description": "处理NUMA架构下大页（Huge Page）的重建逻辑，通过遍历页表项恢复大页映射并更新访问属性",
          "similarity": 0.5923862457275391
        },
        {
          "chunk_id": 25,
          "file_path": "mm/memory.c",
          "start_line": 4037,
          "end_line": 4496,
          "content": [
            "static inline unsigned long thp_swap_suitable_orders(pgoff_t swp_offset,",
            "\t\t\t\t\t\t     unsigned long addr,",
            "\t\t\t\t\t\t     unsigned long orders)",
            "{",
            "\tint order, nr;",
            "",
            "\torder = highest_order(orders);",
            "",
            "\t/*",
            "\t * To swap in a THP with nr pages, we require that its first swap_offset",
            "\t * is aligned with that number, as it was when the THP was swapped out.",
            "\t * This helps filter out most invalid entries.",
            "\t */",
            "\twhile (orders) {",
            "\t\tnr = 1 << order;",
            "\t\tif ((addr >> PAGE_SHIFT) % nr == swp_offset % nr)",
            "\t\t\tbreak;",
            "\t\torder = next_order(&orders, order);",
            "\t}",
            "",
            "\treturn orders;",
            "}",
            "vm_fault_t do_swap_page(struct vm_fault *vmf)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tstruct folio *swapcache, *folio = NULL;",
            "\tDECLARE_WAITQUEUE(wait, current);",
            "\tstruct page *page;",
            "\tstruct swap_info_struct *si = NULL;",
            "\trmap_t rmap_flags = RMAP_NONE;",
            "\tbool need_clear_cache = false;",
            "\tbool exclusive = false;",
            "\tswp_entry_t entry;",
            "\tpte_t pte;",
            "\tvm_fault_t ret = 0;",
            "\tvoid *shadow = NULL;",
            "\tint nr_pages;",
            "\tunsigned long page_idx;",
            "\tunsigned long address;",
            "\tpte_t *ptep;",
            "",
            "\tif (!pte_unmap_same(vmf))",
            "\t\tgoto out;",
            "",
            "\tentry = pte_to_swp_entry(vmf->orig_pte);",
            "\tif (unlikely(non_swap_entry(entry))) {",
            "\t\tif (is_migration_entry(entry)) {",
            "\t\t\tmigration_entry_wait(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\t     vmf->address);",
            "\t\t} else if (is_device_exclusive_entry(entry)) {",
            "\t\t\tvmf->page = pfn_swap_entry_to_page(entry);",
            "\t\t\tret = remove_device_exclusive_entry(vmf);",
            "\t\t} else if (is_device_private_entry(entry)) {",
            "\t\t\tif (vmf->flags & FAULT_FLAG_VMA_LOCK) {",
            "\t\t\t\t/*",
            "\t\t\t\t * migrate_to_ram is not yet ready to operate",
            "\t\t\t\t * under VMA lock.",
            "\t\t\t\t */",
            "\t\t\t\tvma_end_read(vma);",
            "\t\t\t\tret = VM_FAULT_RETRY;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "",
            "\t\t\tvmf->page = pfn_swap_entry_to_page(entry);",
            "\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\tvmf->address, &vmf->ptl);",
            "\t\t\tif (unlikely(!vmf->pte ||",
            "\t\t\t\t     !pte_same(ptep_get(vmf->pte),",
            "\t\t\t\t\t\t\tvmf->orig_pte)))",
            "\t\t\t\tgoto unlock;",
            "",
            "\t\t\t/*",
            "\t\t\t * Get a page reference while we know the page can't be",
            "\t\t\t * freed.",
            "\t\t\t */",
            "\t\t\tget_page(vmf->page);",
            "\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\t\t\tret = vmf->page->pgmap->ops->migrate_to_ram(vmf);",
            "\t\t\tput_page(vmf->page);",
            "\t\t} else if (is_hwpoison_entry(entry)) {",
            "\t\t\tret = VM_FAULT_HWPOISON;",
            "\t\t} else if (is_pte_marker_entry(entry)) {",
            "\t\t\tret = handle_pte_marker(vmf);",
            "\t\t} else {",
            "\t\t\tprint_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);",
            "\t\t\tret = VM_FAULT_SIGBUS;",
            "\t\t}",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* Prevent swapoff from happening to us. */",
            "\tsi = get_swap_device(entry);",
            "\tif (unlikely(!si))",
            "\t\tgoto out;",
            "",
            "\tfolio = swap_cache_get_folio(entry, vma, vmf->address);",
            "\tif (folio)",
            "\t\tpage = folio_file_page(folio, swp_offset(entry));",
            "\tswapcache = folio;",
            "",
            "\tif (!folio) {",
            "\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO) &&",
            "\t\t    __swap_count(entry) == 1) {",
            "\t\t\t/* skip swapcache */",
            "\t\t\tfolio = alloc_swap_folio(vmf);",
            "\t\t\tpage = &folio->page;",
            "\t\t\tif (folio) {",
            "\t\t\t\t__folio_set_locked(folio);",
            "\t\t\t\t__folio_set_swapbacked(folio);",
            "",
            "\t\t\t\tnr_pages = folio_nr_pages(folio);",
            "\t\t\t\tif (folio_test_large(folio))",
            "\t\t\t\t\tentry.val = ALIGN_DOWN(entry.val, nr_pages);",
            "\t\t\t\t/*",
            "\t\t\t\t * Prevent parallel swapin from proceeding with",
            "\t\t\t\t * the cache flag. Otherwise, another thread",
            "\t\t\t\t * may finish swapin first, free the entry, and",
            "\t\t\t\t * swapout reusing the same entry. It's",
            "\t\t\t\t * undetectable as pte_same() returns true due",
            "\t\t\t\t * to entry reuse.",
            "\t\t\t\t */",
            "\t\t\t\tif (swapcache_prepare(entry, nr_pages)) {",
            "\t\t\t\t\t/*",
            "\t\t\t\t\t * Relax a bit to prevent rapid",
            "\t\t\t\t\t * repeated page faults.",
            "\t\t\t\t\t */",
            "\t\t\t\t\tadd_wait_queue(&swapcache_wq, &wait);",
            "\t\t\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\t\t\tremove_wait_queue(&swapcache_wq, &wait);",
            "\t\t\t\t\tgoto out_page;",
            "\t\t\t\t}",
            "\t\t\t\tneed_clear_cache = true;",
            "",
            "\t\t\t\tmem_cgroup_swapin_uncharge_swap(entry, nr_pages);",
            "",
            "\t\t\t\tshadow = get_shadow_from_swap_cache(entry);",
            "\t\t\t\tif (shadow)",
            "\t\t\t\t\tworkingset_refault(folio, shadow);",
            "",
            "\t\t\t\tfolio_add_lru(folio);",
            "",
            "\t\t\t\t/* To provide entry to swap_read_folio() */",
            "\t\t\t\tfolio->swap = entry;",
            "\t\t\t\tswap_read_folio(folio, NULL);",
            "\t\t\t\tfolio->private = NULL;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tpage = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,",
            "\t\t\t\t\t\tvmf);",
            "\t\t\tif (page)",
            "\t\t\t\tfolio = page_folio(page);",
            "\t\t\tswapcache = folio;",
            "\t\t}",
            "",
            "\t\tif (!folio) {",
            "\t\t\t/*",
            "\t\t\t * Back out if somebody else faulted in this pte",
            "\t\t\t * while we released the pte lock.",
            "\t\t\t */",
            "\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\tvmf->address, &vmf->ptl);",
            "\t\t\tif (likely(vmf->pte &&",
            "\t\t\t\t   pte_same(ptep_get(vmf->pte), vmf->orig_pte)))",
            "\t\t\t\tret = VM_FAULT_OOM;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "",
            "\t\t/* Had to read the page from swap area: Major fault */",
            "\t\tret = VM_FAULT_MAJOR;",
            "\t\tcount_vm_event(PGMAJFAULT);",
            "\t\tcount_memcg_event_mm(vma->vm_mm, PGMAJFAULT);",
            "\t} else if (PageHWPoison(page)) {",
            "\t\t/*",
            "\t\t * hwpoisoned dirty swapcache pages are kept for killing",
            "\t\t * owner processes (which may be unknown at hwpoison time)",
            "\t\t */",
            "\t\tret = VM_FAULT_HWPOISON;",
            "\t\tgoto out_release;",
            "\t}",
            "",
            "\tret |= folio_lock_or_retry(folio, vmf);",
            "\tif (ret & VM_FAULT_RETRY)",
            "\t\tgoto out_release;",
            "",
            "\tif (swapcache) {",
            "\t\t/*",
            "\t\t * Make sure folio_free_swap() or swapoff did not release the",
            "\t\t * swapcache from under us.  The page pin, and pte_same test",
            "\t\t * below, are not enough to exclude that.  Even if it is still",
            "\t\t * swapcache, we need to check that the page's swap has not",
            "\t\t * changed.",
            "\t\t */",
            "\t\tif (unlikely(!folio_test_swapcache(folio) ||",
            "\t\t\t     page_swap_entry(page).val != entry.val))",
            "\t\t\tgoto out_page;",
            "",
            "\t\t/*",
            "\t\t * KSM sometimes has to copy on read faults, for example, if",
            "\t\t * page->index of !PageKSM() pages would be nonlinear inside the",
            "\t\t * anon VMA -- PageKSM() is lost on actual swapout.",
            "\t\t */",
            "\t\tfolio = ksm_might_need_to_copy(folio, vma, vmf->address);",
            "\t\tif (unlikely(!folio)) {",
            "\t\t\tret = VM_FAULT_OOM;",
            "\t\t\tfolio = swapcache;",
            "\t\t\tgoto out_page;",
            "\t\t} else if (unlikely(folio == ERR_PTR(-EHWPOISON))) {",
            "\t\t\tret = VM_FAULT_HWPOISON;",
            "\t\t\tfolio = swapcache;",
            "\t\t\tgoto out_page;",
            "\t\t}",
            "\t\tif (folio != swapcache)",
            "\t\t\tpage = folio_page(folio, 0);",
            "",
            "\t\t/*",
            "\t\t * If we want to map a page that's in the swapcache writable, we",
            "\t\t * have to detect via the refcount if we're really the exclusive",
            "\t\t * owner. Try removing the extra reference from the local LRU",
            "\t\t * caches if required.",
            "\t\t */",
            "\t\tif ((vmf->flags & FAULT_FLAG_WRITE) && folio == swapcache &&",
            "\t\t    !folio_test_ksm(folio) && !folio_test_lru(folio))",
            "\t\t\tlru_add_drain();",
            "\t}",
            "",
            "\tfolio_throttle_swaprate(folio, GFP_KERNEL);",
            "",
            "\t/*",
            "\t * Back out if somebody else already faulted in this pte.",
            "\t */",
            "\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,",
            "\t\t\t&vmf->ptl);",
            "\tif (unlikely(!vmf->pte || !pte_same(ptep_get(vmf->pte), vmf->orig_pte)))",
            "\t\tgoto out_nomap;",
            "",
            "\tif (unlikely(!folio_test_uptodate(folio))) {",
            "\t\tret = VM_FAULT_SIGBUS;",
            "\t\tgoto out_nomap;",
            "\t}",
            "",
            "\t/* allocated large folios for SWP_SYNCHRONOUS_IO */",
            "\tif (folio_test_large(folio) && !folio_test_swapcache(folio)) {",
            "\t\tunsigned long nr = folio_nr_pages(folio);",
            "\t\tunsigned long folio_start = ALIGN_DOWN(vmf->address, nr * PAGE_SIZE);",
            "\t\tunsigned long idx = (vmf->address - folio_start) / PAGE_SIZE;",
            "\t\tpte_t *folio_ptep = vmf->pte - idx;",
            "\t\tpte_t folio_pte = ptep_get(folio_ptep);",
            "",
            "\t\tif (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||",
            "\t\t    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)",
            "\t\t\tgoto out_nomap;",
            "",
            "\t\tpage_idx = idx;",
            "\t\taddress = folio_start;",
            "\t\tptep = folio_ptep;",
            "\t\tgoto check_folio;",
            "\t}",
            "",
            "\tnr_pages = 1;",
            "\tpage_idx = 0;",
            "\taddress = vmf->address;",
            "\tptep = vmf->pte;",
            "\tif (folio_test_large(folio) && folio_test_swapcache(folio)) {",
            "\t\tint nr = folio_nr_pages(folio);",
            "\t\tunsigned long idx = folio_page_idx(folio, page);",
            "\t\tunsigned long folio_start = address - idx * PAGE_SIZE;",
            "\t\tunsigned long folio_end = folio_start + nr * PAGE_SIZE;",
            "\t\tpte_t *folio_ptep;",
            "\t\tpte_t folio_pte;",
            "",
            "\t\tif (unlikely(folio_start < max(address & PMD_MASK, vma->vm_start)))",
            "\t\t\tgoto check_folio;",
            "\t\tif (unlikely(folio_end > pmd_addr_end(address, vma->vm_end)))",
            "\t\t\tgoto check_folio;",
            "",
            "\t\tfolio_ptep = vmf->pte - idx;",
            "\t\tfolio_pte = ptep_get(folio_ptep);",
            "\t\tif (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||",
            "\t\t    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)",
            "\t\t\tgoto check_folio;",
            "",
            "\t\tpage_idx = idx;",
            "\t\taddress = folio_start;",
            "\t\tptep = folio_ptep;",
            "\t\tnr_pages = nr;",
            "\t\tentry = folio->swap;",
            "\t\tpage = &folio->page;",
            "\t}",
            "",
            "check_folio:",
            "\t/*",
            "\t * PG_anon_exclusive reuses PG_mappedtodisk for anon pages. A swap pte",
            "\t * must never point at an anonymous page in the swapcache that is",
            "\t * PG_anon_exclusive. Sanity check that this holds and especially, that",
            "\t * no filesystem set PG_mappedtodisk on a page in the swapcache. Sanity",
            "\t * check after taking the PT lock and making sure that nobody",
            "\t * concurrently faulted in this page and set PG_anon_exclusive.",
            "\t */",
            "\tBUG_ON(!folio_test_anon(folio) && folio_test_mappedtodisk(folio));",
            "\tBUG_ON(folio_test_anon(folio) && PageAnonExclusive(page));",
            "",
            "\t/*",
            "\t * Check under PT lock (to protect against concurrent fork() sharing",
            "\t * the swap entry concurrently) for certainly exclusive pages.",
            "\t */",
            "\tif (!folio_test_ksm(folio)) {",
            "\t\texclusive = pte_swp_exclusive(vmf->orig_pte);",
            "\t\tif (folio != swapcache) {",
            "\t\t\t/*",
            "\t\t\t * We have a fresh page that is not exposed to the",
            "\t\t\t * swapcache -> certainly exclusive.",
            "\t\t\t */",
            "\t\t\texclusive = true;",
            "\t\t} else if (exclusive && folio_test_writeback(folio) &&",
            "\t\t\t  data_race(si->flags & SWP_STABLE_WRITES)) {",
            "\t\t\t/*",
            "\t\t\t * This is tricky: not all swap backends support",
            "\t\t\t * concurrent page modifications while under writeback.",
            "\t\t\t *",
            "\t\t\t * So if we stumble over such a page in the swapcache",
            "\t\t\t * we must not set the page exclusive, otherwise we can",
            "\t\t\t * map it writable without further checks and modify it",
            "\t\t\t * while still under writeback.",
            "\t\t\t *",
            "\t\t\t * For these problematic swap backends, simply drop the",
            "\t\t\t * exclusive marker: this is perfectly fine as we start",
            "\t\t\t * writeback only if we fully unmapped the page and",
            "\t\t\t * there are no unexpected references on the page after",
            "\t\t\t * unmapping succeeded. After fully unmapped, no",
            "\t\t\t * further GUP references (FOLL_GET and FOLL_PIN) can",
            "\t\t\t * appear, so dropping the exclusive marker and mapping",
            "\t\t\t * it only R/O is fine.",
            "\t\t\t */",
            "\t\t\texclusive = false;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Some architectures may have to restore extra metadata to the page",
            "\t * when reading from swap. This metadata may be indexed by swap entry",
            "\t * so this must be called before swap_free().",
            "\t */",
            "\tarch_swap_restore(folio_swap(entry, folio), folio);",
            "",
            "\t/*",
            "\t * Remove the swap entry and conditionally try to free up the swapcache.",
            "\t * We're already holding a reference on the page but haven't mapped it",
            "\t * yet.",
            "\t */",
            "\tswap_free_nr(entry, nr_pages);",
            "\tif (should_try_to_free_swap(folio, vma, vmf->flags))",
            "\t\tfolio_free_swap(folio);",
            "",
            "\tadd_mm_counter(vma->vm_mm, MM_ANONPAGES, nr_pages);",
            "\tadd_mm_counter(vma->vm_mm, MM_SWAPENTS, -nr_pages);",
            "\tpte = mk_pte(page, vma->vm_page_prot);",
            "",
            "\t/*",
            "\t * Same logic as in do_wp_page(); however, optimize for pages that are",
            "\t * certainly not shared either because we just allocated them without",
            "\t * exposing them to the swapcache or because the swap entry indicates",
            "\t * exclusivity.",
            "\t */",
            "\tif (!folio_test_ksm(folio) &&",
            "\t    (exclusive || folio_ref_count(folio) == 1)) {",
            "\t\tif (vmf->flags & FAULT_FLAG_WRITE) {",
            "\t\t\tpte = maybe_mkwrite(pte_mkdirty(pte), vma);",
            "\t\t\tvmf->flags &= ~FAULT_FLAG_WRITE;",
            "\t\t}",
            "\t\trmap_flags |= RMAP_EXCLUSIVE;",
            "\t}",
            "\tfolio_ref_add(folio, nr_pages - 1);",
            "\tflush_icache_pages(vma, page, nr_pages);",
            "\tif (pte_swp_soft_dirty(vmf->orig_pte))",
            "\t\tpte = pte_mksoft_dirty(pte);",
            "\tif (pte_swp_uffd_wp(vmf->orig_pte))",
            "\t\tpte = pte_mkuffd_wp(pte);",
            "\tvmf->orig_pte = pte_advance_pfn(pte, page_idx);",
            "",
            "\t/* ksm created a completely new copy */",
            "\tif (unlikely(folio != swapcache && swapcache)) {",
            "\t\tfolio_add_new_anon_rmap(folio, vma, address, RMAP_EXCLUSIVE);",
            "\t\tfolio_add_lru_vma(folio, vma);",
            "\t} else if (!folio_test_anon(folio)) {",
            "\t\t/*",
            "\t\t * We currently only expect small !anon folios which are either",
            "\t\t * fully exclusive or fully shared, or new allocated large",
            "\t\t * folios which are fully exclusive. If we ever get large",
            "\t\t * folios within swapcache here, we have to be careful.",
            "\t\t */",
            "\t\tVM_WARN_ON_ONCE(folio_test_large(folio) && folio_test_swapcache(folio));",
            "\t\tVM_WARN_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\t\tfolio_add_new_anon_rmap(folio, vma, address, rmap_flags);",
            "\t} else {",
            "\t\tfolio_add_anon_rmap_ptes(folio, page, nr_pages, vma, address,",
            "\t\t\t\t\trmap_flags);",
            "\t}",
            "",
            "\tVM_BUG_ON(!folio_test_anon(folio) ||",
            "\t\t\t(pte_write(pte) && !PageAnonExclusive(page)));",
            "\tset_ptes(vma->vm_mm, address, ptep, pte, nr_pages);",
            "\tarch_do_swap_page_nr(vma->vm_mm, vma, address,",
            "\t\t\tpte, pte, nr_pages);",
            "",
            "\tfolio_unlock(folio);",
            "\tif (folio != swapcache && swapcache) {",
            "\t\t/*",
            "\t\t * Hold the lock to avoid the swap entry to be reused",
            "\t\t * until we take the PT lock for the pte_same() check",
            "\t\t * (to avoid false positives from pte_same). For",
            "\t\t * further safety release the lock after the swap_free",
            "\t\t * so that the swap count won't change under a",
            "\t\t * parallel locked swapcache.",
            "\t\t */",
            "\t\tfolio_unlock(swapcache);",
            "\t\tfolio_put(swapcache);",
            "\t}",
            "",
            "\tif (vmf->flags & FAULT_FLAG_WRITE) {",
            "\t\tret |= do_wp_page(vmf);",
            "\t\tif (ret & VM_FAULT_ERROR)",
            "\t\t\tret &= VM_FAULT_ERROR;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* No need to invalidate - it was non-present before */",
            "\tupdate_mmu_cache_range(vmf, vma, address, ptep, nr_pages);",
            "unlock:",
            "\tif (vmf->pte)",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "out:",
            "\t/* Clear the swap cache pin for direct swapin after PTL unlock */",
            "\tif (need_clear_cache) {",
            "\t\tswapcache_clear(si, entry, nr_pages);",
            "\t\tif (waitqueue_active(&swapcache_wq))",
            "\t\t\twake_up(&swapcache_wq);",
            "\t}",
            "\tif (si)",
            "\t\tput_swap_device(si);",
            "\treturn ret;",
            "out_nomap:",
            "\tif (vmf->pte)",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "out_page:",
            "\tfolio_unlock(folio);",
            "out_release:",
            "\tfolio_put(folio);",
            "\tif (folio != swapcache && swapcache) {",
            "\t\tfolio_unlock(swapcache);",
            "\t\tfolio_put(swapcache);",
            "\t}",
            "\tif (need_clear_cache) {",
            "\t\tswapcache_clear(si, entry, nr_pages);",
            "\t\tif (waitqueue_active(&swapcache_wq))",
            "\t\t\twake_up(&swapcache_wq);",
            "\t}",
            "\tif (si)",
            "\t\tput_swap_device(si);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "thp_swap_suitable_orders, do_swap_page",
          "description": "处理页面交换时的多种情况，如迁移条目、设备私有条目、硬件故障等，根据不同的条目类型执行相应逻辑，最终将交换页面映射到内存中。",
          "similarity": 0.5874451398849487
        }
      ]
    },
    {
      "source_file": "mm/pgalloc-track.h",
      "md_summary": "> 自动生成时间: 2025-12-07 17:11:54\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `pgalloc-track.h`\n\n---\n\n# pgalloc-track.h 技术文档\n\n## 1. 文件概述\n\n`pgalloc-track.h` 是 Linux 内核中用于页表分配与修改追踪的辅助头文件。该文件提供了一组带“追踪”（track）功能的页表分配接口，能够在分配缺失的页表层级时，自动记录哪些页表层级被修改，以便后续进行 TLB 刷新、内存同步或其他页表一致性维护操作。这些接口主要用于支持内核在处理缺页异常或动态映射时高效地构建多级页表结构，并精确标记被修改的页表层级。\n\n## 2. 核心功能\n\n### 函数列表\n\n- `p4d_alloc_track(struct mm_struct *mm, pgd_t *pgd, unsigned long address, pgtbl_mod_mask *mod_mask)`  \n  分配并返回指定地址对应的 P4D（Page 4-level Directory）项指针，若原 PGD 项为空则分配新 P4D 表，并设置 `PGTBL_PGD_MODIFIED` 标志。\n\n- `pud_alloc_track(struct mm_struct *mm, p4d_t *p4d, unsigned long address, pgtbl_mod_mask *mod_mask)`  \n  分配并返回 PUD（Page Upper Directory）项指针，若原 P4D 项为空则分配新 PUD 表，并设置 `PGTBL_P4D_MODIFIED` 标志。\n\n- `pmd_alloc_track(struct mm_struct *mm, pud_t *pud, unsigned long address, pgtbl_mod_mask *mod_mask)`  \n  分配并返回 PMD（Page Middle Directory）项指针，若原 PUD 项为空则分配新 PMD 表，并设置 `PGTBL_PUD_MODIFIED` 标志。\n\n- `pte_alloc_kernel_track(pmd, address, mask)`（宏）  \n  为内核地址空间分配 PTE（Page Table Entry）页表，若原 PMD 项为空则调用 `__pte_alloc_kernel` 分配，并设置 `PGTBL_PMD_MODIFIED` 标志。\n\n### 数据结构依赖\n\n- `struct mm_struct`：进程内存描述符。\n- `pgd_t`, `p4d_t`, `pud_t`, `pmd_t`：各级页表项类型。\n- `pgtbl_mod_mask`：位掩码类型，用于记录哪些页表层级被修改（如 `PGTBL_PGD_MODIFIED` 等常量）。\n\n> 注：上述函数仅在 `CONFIG_MMU` 配置启用时定义，即仅适用于支持 MMU 的架构。\n\n## 3. 关键实现\n\n- **条件分配机制**：所有 `_alloc_track` 函数均采用“按需分配”策略。仅当上级页表项为 `none`（即未分配）时，才调用底层分配函数（如 `__p4d_alloc`）创建下一级页表。\n  \n- **修改标记追踪**：每次成功分配新的页表层级后，通过位或操作（`|=`）将对应的修改标志（如 `PGTBL_P4D_MODIFIED`）写入传入的 `mod_mask` 指针所指向的掩码变量中。这使得调用者能够精确知道在本次页表遍历过程中哪些层级发生了变更。\n\n- **内联与宏优化**：所有函数均为 `static inline`，以减少函数调用开销；`pte_alloc_kernel_track` 使用宏实现，结合三元运算符和语句表达式（`({...})`）在单行中完成条件判断、分配、标记和返回。\n\n- **错误处理**：若底层分配函数（如 `__pud_alloc`）失败，函数直接返回 `NULL`，由上层调用者处理错误。\n\n## 4. 依赖关系\n\n- **配置依赖**：依赖 `CONFIG_MMU` 内核配置选项，仅在支持虚拟内存管理单元（MMU）的系统上编译相关函数。\n- **头文件依赖**：隐式依赖以下内核头文件（虽未显式包含，但使用其定义）：\n  - `<linux/mm_types.h>`：定义 `struct mm_struct` 和页表项类型。\n  - `<asm/pgtable.h>`：提供 `pgd_none`、`p4d_offset` 等页表操作宏及 `__p4d_alloc` 等分配函数。\n  - `<linux/pgtable.h>`：可能定义 `pgtbl_mod_mask` 及相关修改标志（如 `PGTBL_PGD_MODIFIED`）。\n- **函数依赖**：依赖底层页表分配函数 `__p4d_alloc`、`__pud_alloc`、`__pmd_alloc` 和 `__pte_alloc_kernel`，这些通常由架构相关代码或通用内存管理模块提供。\n\n## 5. 使用场景\n\n- **缺页异常处理**：在 `handle_mm_fault()` 或类似路径中，当需要为用户或内核地址建立完整页表映射时，逐级调用这些 `_alloc_track` 函数构建页表，并收集修改掩码用于后续 TLB 批量刷新。\n  \n- **内核动态映射**：在内核需要动态映射物理内存（如 `ioremap`、`vmalloc` 等）时，使用 `pte_alloc_kernel_track` 安全地分配内核 PTE 页表并记录 PMD 修改状态。\n\n- **页表预分配或扩展**：在内存管理子系统预分配页表或扩展现有 VMA 映射范围时，利用该接口确保页表结构完整性并精确追踪变更。\n\n- **性能敏感路径**：由于采用内联和轻量级检查，适用于对性能要求较高的内存管理关键路径，同时保证修改信息的准确性以支持高效的 TLB 管理。",
      "similarity": 0.6519107222557068,
      "chunks": []
    }
  ]
}