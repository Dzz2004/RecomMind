{
  "query": "进程退出时的资源释放机制",
  "timestamp": "2025-12-26 00:40:45",
  "retrieved_files": [
    {
      "source_file": "kernel/exit.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:27:18\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `exit.c`\n\n---\n\n# `exit.c` 技术文档\n\n## 1. 文件概述\n\n`exit.c` 是 Linux 内核中负责进程退出（termination）核心逻辑的关键源文件，位于 `kernel/` 目录下。该文件实现了进程终止时的资源回收、信号处理、线程组清理、引用计数释放以及与用户空间和内核其他子系统的协调机制。其主要职责包括：\n\n- 安全地释放进程占用的内核资源（如内存、文件描述符、信号处理结构等）\n- 更新进程组和会话的统计信息\n- 通知父进程子进程已退出（通过 `SIGCHLD` 信号）\n- 管理僵尸进程（zombie）的生命周期\n- 支持线程组（thread group）的协同退出\n- 提供与 oops（内核异常）相关的计数和限制机制\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|---------|\n| `__unhash_process()` | 从内核的进程哈希表和链表中移除进程，减少线程计数 |\n| `__exit_signal()` | 清理进程的信号相关资源，累加 CPU 时间和 I/O 统计到 `signal_struct` |\n| `delayed_put_task_struct()` | RCU 回调函数，延迟释放 `task_struct` 及其关联资源 |\n| `put_task_struct_rcu_user()` | 安全地减少 `task_struct` 的 RCU 用户引用计数，并在为零时调度延迟释放 |\n| `release_thread()` | 架构相关的线程资源释放钩子（弱符号，可由架构代码覆盖） |\n| `release_task()` | 主进程释放入口函数，协调整个退出流程，包括通知父进程、释放资源等 |\n| `rcuwait_wake_up()` | 唤醒等待在 `rcuwait` 上的任务（代码片段未完整） |\n\n### 关键数据结构与变量\n\n| 名称 | 类型/说明 |\n|------|----------|\n| `oops_limit` | `unsigned int`，限制内核 oops 发生次数的阈值（默认 10000） |\n| `oops_count` | `atomic_t`，原子计数器，记录系统发生 oops 的总次数 |\n| `kern_exit_table` | `ctl_table`，用于 `/proc/sys/kernel/oops_limit` 的 sysctl 接口 |\n| `oops_count_attr` | `kobj_attribute`，用于 `/sys/kernel/oops_count` 的 sysfs 接口 |\n\n## 3. 关键实现\n\n### 进程退出流程\n\n1. **资源统计聚合**：  \n   在 `__exit_signal()` 中，将退出线程的 CPU 时间（`utime`/`stime`）、I/O 操作、上下文切换次数等统计信息累加到所属线程组的 `signal_struct` 中，确保即使线程组 leader 尚未退出，也能被 `wait4()` 等系统调用正确获取。\n\n2. **线程组协同退出**：  \n   - 若当前退出的是线程组 leader（`group_dead == true`），则清理整个线程组的 PID 类型（TGID、PGID、SID），并从全局任务链表中移除。\n   - 若非 leader，则仅减少线程组计数，并可能更新 `curr_target`（用于信号投递）。\n\n3. **僵尸进程处理**：  \n   在 `release_task()` 中，检查线程组 leader 是否已变为僵尸状态。若是且当前线程是最后一个成员，则调用 `do_notify_parent()` 通知其父进程。若父进程忽略 `SIGCHLD`，则直接将 leader 状态置为 `EXIT_DEAD` 并递归释放。\n\n4. **延迟释放机制**：  \n   通过 RCU（Read-Copy-Update）机制安全释放 `task_struct`。`put_task_struct_rcu_user()` 在引用计数归零时调用 `call_rcu()`，由 `delayed_put_task_struct()` 在 RCU 宽限期后执行实际释放，确保并发读取安全。\n\n5. **Oops 计数与限制**：  \n   提供 `oops_count`（只读）和 `oops_limit`（可调）两个接口，用于监控和限制内核异常次数，防止因频繁崩溃导致资源耗尽或引用计数溢出。\n\n### 锁与同步\n\n- **`tasklist_lock`**：写锁保护进程链表和 PID 哈希表的修改。\n- **`sighand->siglock`**：自旋锁保护信号处理结构。\n- **`signal->stats_lock`**：顺序锁（seqlock）保护线程组统计信息的聚合。\n- **RCU**：用于安全地延迟释放 `task_struct`，避免在遍历任务链表时访问已释放内存。\n\n## 4. 依赖关系\n\n`exit.c` 与内核多个子系统紧密耦合，主要依赖包括：\n\n- **调度器（SCHED）**：`<linux/sched/*.h>`，用于任务状态管理、CPU 时间统计、任务链表操作。\n- **内存管理（MM）**：`<linux/mm.h>`、`<linux/slab.h>`，用于内存释放和 slab 分配器交互。\n- **文件系统（VFS）**：`<linux/file.h>`、`<linux/fdtable.h>`、`<linux/fs_struct.h>`，用于关闭文件描述符和释放文件系统上下文。\n- **进程间通信（IPC）**：`<linux/shm.h>`、`<linux/posix-timers.h>`，用于清理共享内存和定时器资源。\n- **安全与审计**：`<linux/audit.h>`、`<linux/seccomp.h>`（通过 `seccomp_filter_release`），用于释放安全策略和审计上下文。\n- **cgroup 与资源控制**：`<linux/cgroup.h>`、`<linux/resource.h>`，用于资源计数释放和限制检查。\n- **跟踪与性能**：`<linux/perf_event.h>`、`<trace/events/sched.h>`，用于性能事件清理和调度跟踪点。\n- **架构相关代码**：`<asm/mmu_context.h>`、`release_thread()` 弱符号，允许架构层定制线程释放逻辑。\n\n## 5. 使用场景\n\n- **进程正常退出**：当用户程序调用 `exit()` 或 `exit_group()` 系统调用时，内核通过此文件执行清理。\n- **进程被信号终止**：如收到 `SIGKILL` 或 `SIGTERM` 后，内核调度退出路径。\n- **线程退出**：POSIX 线程（通过 `pthread_exit()` 或线程函数返回）触发 `release_task()` 清理单个线程。\n- **内核 Oops/panic 处理**：每次内核异常会递增 `oops_count`，用于监控系统稳定性。\n- **僵尸进程回收**：父进程调用 `wait()` 系列系统调用后，内核最终通过 `release_task()` 释放僵尸进程的内核结构。\n- **容器/命名空间退出**：在 PID 命名空间或 cgroup 中进程退出时，协调资源释放和通知机制。",
      "similarity": 0.7238982319831848,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/exit.c",
          "start_line": 546,
          "end_line": 686,
          "content": [
            "static void exit_mm(void)",
            "{",
            "\tstruct mm_struct *mm = current->mm;",
            "",
            "\texit_mm_release(current, mm);",
            "\tif (!mm)",
            "\t\treturn;",
            "\tsync_mm_rss(mm);",
            "\tmmap_read_lock(mm);",
            "\tmmgrab_lazy_tlb(mm);",
            "\tBUG_ON(mm != current->active_mm);",
            "\t/* more a memory barrier than a real lock */",
            "\ttask_lock(current);",
            "\t/*",
            "\t * When a thread stops operating on an address space, the loop",
            "\t * in membarrier_private_expedited() may not observe that",
            "\t * tsk->mm, and the loop in membarrier_global_expedited() may",
            "\t * not observe a MEMBARRIER_STATE_GLOBAL_EXPEDITED",
            "\t * rq->membarrier_state, so those would not issue an IPI.",
            "\t * Membarrier requires a memory barrier after accessing",
            "\t * user-space memory, before clearing tsk->mm or the",
            "\t * rq->membarrier_state.",
            "\t */",
            "\tsmp_mb__after_spinlock();",
            "\tlocal_irq_disable();",
            "\tcurrent->mm = NULL;",
            "\t#ifdef CONFIG_IEE",
            "\tiee_set_token_pgd(current, NULL);",
            "\t#endif",
            "\tmembarrier_update_current_mm(NULL);",
            "\tenter_lazy_tlb(mm, current);",
            "\tlocal_irq_enable();",
            "\ttask_unlock(current);",
            "\tmmap_read_unlock(mm);",
            "\tmm_update_next_owner(mm);",
            "\tmmput(mm);",
            "\tif (test_thread_flag(TIF_MEMDIE))",
            "\t\texit_oom_victim();",
            "}",
            "static void reparent_leader(struct task_struct *father, struct task_struct *p,",
            "\t\t\t\tstruct list_head *dead)",
            "{",
            "\tif (unlikely(p->exit_state == EXIT_DEAD))",
            "\t\treturn;",
            "",
            "\t/* We don't want people slaying init. */",
            "\tp->exit_signal = SIGCHLD;",
            "",
            "\t/* If it has exited notify the new parent about this child's death. */",
            "\tif (!p->ptrace &&",
            "\t    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {",
            "\t\tif (do_notify_parent(p, p->exit_signal)) {",
            "\t\t\tp->exit_state = EXIT_DEAD;",
            "\t\t\tlist_add(&p->ptrace_entry, dead);",
            "\t\t}",
            "\t}",
            "",
            "\tkill_orphaned_pgrp(p, father);",
            "}",
            "static void forget_original_parent(struct task_struct *father,",
            "\t\t\t\t\tstruct list_head *dead)",
            "{",
            "\tstruct task_struct *p, *t, *reaper;",
            "",
            "\tif (unlikely(!list_empty(&father->ptraced)))",
            "\t\texit_ptrace(father, dead);",
            "",
            "\t/* Can drop and reacquire tasklist_lock */",
            "\treaper = find_child_reaper(father, dead);",
            "\tif (list_empty(&father->children))",
            "\t\treturn;",
            "",
            "\treaper = find_new_reaper(father, reaper);",
            "\tlist_for_each_entry(p, &father->children, sibling) {",
            "\t\tfor_each_thread(p, t) {",
            "\t\t\tRCU_INIT_POINTER(t->real_parent, reaper);",
            "\t\t\tBUG_ON((!t->ptrace) != (rcu_access_pointer(t->parent) == father));",
            "\t\t\tif (likely(!t->ptrace))",
            "\t\t\t\tt->parent = t->real_parent;",
            "\t\t\tif (t->pdeath_signal)",
            "\t\t\t\tgroup_send_sig_info(t->pdeath_signal,",
            "\t\t\t\t\t\t    SEND_SIG_NOINFO, t,",
            "\t\t\t\t\t\t    PIDTYPE_TGID);",
            "\t\t}",
            "\t\t/*",
            "\t\t * If this is a threaded reparent there is no need to",
            "\t\t * notify anyone anything has happened.",
            "\t\t */",
            "\t\tif (!same_thread_group(reaper, father))",
            "\t\t\treparent_leader(father, p, dead);",
            "\t}",
            "\tlist_splice_tail_init(&father->children, &reaper->children);",
            "}",
            "static void exit_notify(struct task_struct *tsk, int group_dead)",
            "{",
            "\tbool autoreap;",
            "\tstruct task_struct *p, *n;",
            "\tLIST_HEAD(dead);",
            "",
            "\twrite_lock_irq(&tasklist_lock);",
            "\tforget_original_parent(tsk, &dead);",
            "",
            "\tif (group_dead)",
            "\t\tkill_orphaned_pgrp(tsk->group_leader, NULL);",
            "",
            "\ttsk->exit_state = EXIT_ZOMBIE;",
            "\t/*",
            "\t * sub-thread or delay_group_leader(), wake up the",
            "\t * PIDFD_THREAD waiters.",
            "\t */",
            "\tif (!thread_group_empty(tsk))",
            "\t\tdo_notify_pidfd(tsk);",
            "",
            "\tif (unlikely(tsk->ptrace)) {",
            "\t\tint sig = thread_group_leader(tsk) &&",
            "\t\t\t\tthread_group_empty(tsk) &&",
            "\t\t\t\t!ptrace_reparented(tsk) ?",
            "\t\t\ttsk->exit_signal : SIGCHLD;",
            "\t\tautoreap = do_notify_parent(tsk, sig);",
            "\t} else if (thread_group_leader(tsk)) {",
            "\t\tautoreap = thread_group_empty(tsk) &&",
            "\t\t\tdo_notify_parent(tsk, tsk->exit_signal);",
            "\t} else {",
            "\t\tautoreap = true;",
            "\t}",
            "",
            "\tif (autoreap) {",
            "\t\ttsk->exit_state = EXIT_DEAD;",
            "\t\tlist_add(&tsk->ptrace_entry, &dead);",
            "\t}",
            "",
            "\t/* mt-exec, de_thread() is waiting for group leader */",
            "\tif (unlikely(tsk->signal->notify_count < 0))",
            "\t\twake_up_process(tsk->signal->group_exec_task);",
            "\twrite_unlock_irq(&tasklist_lock);",
            "",
            "\tlist_for_each_entry_safe(p, n, &dead, ptrace_entry) {",
            "\t\tlist_del_init(&p->ptrace_entry);",
            "\t\trelease_task(p);",
            "\t}",
            "}"
          ],
          "function_name": "exit_mm, reparent_leader, forget_original_parent, exit_notify",
          "description": "完成内存映射释放、父进程重定位、原始父进程解除关联及进程退出状态通知流程。",
          "similarity": 0.7031585574150085
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/exit.c",
          "start_line": 791,
          "end_line": 942,
          "content": [
            "static void check_stack_usage(void)",
            "{",
            "\tstatic DEFINE_SPINLOCK(low_water_lock);",
            "\tstatic int lowest_to_date = THREAD_SIZE;",
            "\tunsigned long free;",
            "",
            "\tfree = stack_not_used(current);",
            "",
            "\tif (free >= lowest_to_date)",
            "\t\treturn;",
            "",
            "\tspin_lock(&low_water_lock);",
            "\tif (free < lowest_to_date) {",
            "\t\tpr_info(\"%s (%d) used greatest stack depth: %lu bytes left\\n\",",
            "\t\t\tcurrent->comm, task_pid_nr(current), free);",
            "\t\tlowest_to_date = free;",
            "\t}",
            "\tspin_unlock(&low_water_lock);",
            "}",
            "static inline void check_stack_usage(void) {}",
            "static void synchronize_group_exit(struct task_struct *tsk, long code)",
            "{",
            "\tstruct sighand_struct *sighand = tsk->sighand;",
            "\tstruct signal_struct *signal = tsk->signal;",
            "",
            "\tspin_lock_irq(&sighand->siglock);",
            "\tsignal->quick_threads--;",
            "\tif ((signal->quick_threads == 0) &&",
            "\t    !(signal->flags & SIGNAL_GROUP_EXIT)) {",
            "\t\tsignal->flags = SIGNAL_GROUP_EXIT;",
            "\t\tsignal->group_exit_code = code;",
            "\t\tsignal->group_stop_count = 0;",
            "\t}",
            "\tspin_unlock_irq(&sighand->siglock);",
            "}",
            "void __noreturn do_exit(long code)",
            "{",
            "\tstruct task_struct *tsk = current;",
            "\tint group_dead;",
            "",
            "\tWARN_ON(irqs_disabled());",
            "",
            "\tsynchronize_group_exit(tsk, code);",
            "",
            "\tWARN_ON(tsk->plug);",
            "",
            "\tkcov_task_exit(tsk);",
            "\tkmsan_task_exit(tsk);",
            "",
            "\tcoredump_task_exit(tsk);",
            "\tptrace_event(PTRACE_EVENT_EXIT, code);",
            "\tuser_events_exit(tsk);",
            "",
            "\tio_uring_files_cancel();",
            "\texit_signals(tsk);  /* sets PF_EXITING */",
            "",
            "\t/* sync mm's RSS info before statistics gathering */",
            "\tif (tsk->mm)",
            "\t\tsync_mm_rss(tsk->mm);",
            "\tacct_update_integrals(tsk);",
            "\tgroup_dead = atomic_dec_and_test(&tsk->signal->live);",
            "\tif (group_dead) {",
            "\t\t/*",
            "\t\t * If the last thread of global init has exited, panic",
            "\t\t * immediately to get a useable coredump.",
            "\t\t */",
            "\t\tif (unlikely(is_global_init(tsk)))",
            "\t\t\tpanic(\"Attempted to kill init! exitcode=0x%08x\\n\",",
            "\t\t\t\ttsk->signal->group_exit_code ?: (int)code);",
            "",
            "#ifdef CONFIG_POSIX_TIMERS",
            "\t\thrtimer_cancel(&tsk->signal->real_timer);",
            "\t\texit_itimers(tsk);",
            "#endif",
            "\t\tif (tsk->mm)",
            "\t\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);",
            "\t}",
            "\tacct_collect(code, group_dead);",
            "\tif (group_dead)",
            "\t\ttty_audit_exit();",
            "\taudit_free(tsk);",
            "",
            "\ttsk->exit_code = code;",
            "\ttaskstats_exit(tsk, group_dead);",
            "",
            "\t/*",
            "\t * Since sampling can touch ->mm, make sure to stop everything before we",
            "\t * tear it down.",
            "\t *",
            "\t * Also flushes inherited counters to the parent - before the parent",
            "\t * gets woken up by child-exit notifications.",
            "\t */",
            "\tperf_event_exit_task(tsk);",
            "",
            "\texit_mm();",
            "",
            "\tif (group_dead)",
            "\t\tacct_process();",
            "\ttrace_sched_process_exit(tsk);",
            "",
            "\texit_sem(tsk);",
            "\texit_shm(tsk);",
            "\texit_files(tsk);",
            "\texit_fs(tsk);",
            "\tif (group_dead)",
            "\t\tdisassociate_ctty(1);",
            "\texit_task_namespaces(tsk);",
            "\texit_task_work(tsk);",
            "\texit_thread(tsk);",
            "",
            "\tsched_autogroup_exit_task(tsk);",
            "\tcgroup_exit(tsk);",
            "",
            "\t/*",
            "\t * FIXME: do that only when needed, using sched_exit tracepoint",
            "\t */",
            "\tflush_ptrace_hw_breakpoint(tsk);",
            "",
            "\texit_tasks_rcu_start();",
            "\texit_notify(tsk, group_dead);",
            "\tproc_exit_connector(tsk);",
            "\tmpol_put_task_policy(tsk);",
            "#ifdef CONFIG_FUTEX",
            "\tif (unlikely(current->pi_state_cache))",
            "\t\tkfree(current->pi_state_cache);",
            "#endif",
            "\t/*",
            "\t * Make sure we are holding no locks:",
            "\t */",
            "\tdebug_check_no_locks_held();",
            "",
            "\tif (tsk->io_context)",
            "\t\texit_io_context(tsk);",
            "",
            "\tif (tsk->splice_pipe)",
            "\t\tfree_pipe_info(tsk->splice_pipe);",
            "",
            "\tif (tsk->task_frag.page)",
            "\t\tput_page(tsk->task_frag.page);",
            "",
            "\texit_task_stack_account(tsk);",
            "",
            "\tcheck_stack_usage();",
            "\tpreempt_disable();",
            "\tif (tsk->nr_dirtied)",
            "\t\t__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);",
            "\texit_rcu();",
            "\texit_tasks_rcu_finish();",
            "",
            "\tlockdep_free_task(tsk);",
            "\tdo_task_dead();",
            "}"
          ],
          "function_name": "check_stack_usage, check_stack_usage, synchronize_group_exit, do_exit",
          "description": "do_exit函数负责处理进程退出流程，包括同步线程组退出、释放资源、更新统计信息、清理内存映射、解除命名空间关联等操作。其中synchronize_group_exit用于减少信号量计数并标记线程组退出状态，check_stack_usage监控最大堆栈使用量。",
          "similarity": 0.6887727975845337
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/exit.c",
          "start_line": 228,
          "end_line": 339,
          "content": [
            "void put_task_struct_rcu_user(struct task_struct *task)",
            "{",
            "\tif (refcount_dec_and_test(&task->rcu_users))",
            "\t\tcall_rcu(&task->rcu, delayed_put_task_struct);",
            "}",
            "void __weak release_thread(struct task_struct *dead_task)",
            "{",
            "}",
            "void release_task(struct task_struct *p)",
            "{",
            "\tstruct task_struct *leader;",
            "\tstruct pid *thread_pid;",
            "\tint zap_leader;",
            "repeat:",
            "\t/* don't need to get the RCU readlock here - the process is dead and",
            "\t * can't be modifying its own credentials. But shut RCU-lockdep up */",
            "\trcu_read_lock();",
            "\tdec_rlimit_ucounts(task_ucounts(p), UCOUNT_RLIMIT_NPROC, 1);",
            "\trcu_read_unlock();",
            "",
            "\tcgroup_release(p);",
            "",
            "\twrite_lock_irq(&tasklist_lock);",
            "\tptrace_release_task(p);",
            "\tthread_pid = get_pid(p->thread_pid);",
            "\t__exit_signal(p);",
            "",
            "\t/*",
            "\t * If we are the last non-leader member of the thread",
            "\t * group, and the leader is zombie, then notify the",
            "\t * group leader's parent process. (if it wants notification.)",
            "\t */",
            "\tzap_leader = 0;",
            "\tleader = p->group_leader;",
            "\tif (leader != p && thread_group_empty(leader)",
            "\t\t\t&& leader->exit_state == EXIT_ZOMBIE) {",
            "\t\t/*",
            "\t\t * If we were the last child thread and the leader has",
            "\t\t * exited already, and the leader's parent ignores SIGCHLD,",
            "\t\t * then we are the one who should release the leader.",
            "\t\t */",
            "\t\tzap_leader = do_notify_parent(leader, leader->exit_signal);",
            "\t\tif (zap_leader)",
            "\t\t\tleader->exit_state = EXIT_DEAD;",
            "\t}",
            "",
            "\twrite_unlock_irq(&tasklist_lock);",
            "\tseccomp_filter_release(p);",
            "\tproc_flush_pid(thread_pid);",
            "\tput_pid(thread_pid);",
            "\trelease_thread(p);",
            "\t/*",
            "\t * This task was already removed from the process/thread/pid lists",
            "\t * and lock_task_sighand(p) can't succeed. Nobody else can touch",
            "\t * ->pending or, if group dead, signal->shared_pending. We can call",
            "\t * flush_sigqueue() lockless.",
            "\t */",
            "\tflush_sigqueue(&p->pending);",
            "\tif (thread_group_leader(p))",
            "\t\tflush_sigqueue(&p->signal->shared_pending);",
            "",
            "\tput_task_struct_rcu_user(p);",
            "",
            "\tp = leader;",
            "\tif (unlikely(zap_leader))",
            "\t\tgoto repeat;",
            "}",
            "int rcuwait_wake_up(struct rcuwait *w)",
            "{",
            "\tint ret = 0;",
            "\tstruct task_struct *task;",
            "",
            "\trcu_read_lock();",
            "",
            "\t/*",
            "\t * Order condition vs @task, such that everything prior to the load",
            "\t * of @task is visible. This is the condition as to why the user called",
            "\t * rcuwait_wake() in the first place. Pairs with set_current_state()",
            "\t * barrier (A) in rcuwait_wait_event().",
            "\t *",
            "\t *    WAIT                WAKE",
            "\t *    [S] tsk = current\t  [S] cond = true",
            "\t *        MB (A)\t      MB (B)",
            "\t *    [L] cond\t\t  [L] tsk",
            "\t */",
            "\tsmp_mb(); /* (B) */",
            "",
            "\ttask = rcu_dereference(w->task);",
            "\tif (task)",
            "\t\tret = wake_up_process(task);",
            "\trcu_read_unlock();",
            "",
            "\treturn ret;",
            "}",
            "static int will_become_orphaned_pgrp(struct pid *pgrp,",
            "\t\t\t\t\tstruct task_struct *ignored_task)",
            "{",
            "\tstruct task_struct *p;",
            "",
            "\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {",
            "\t\tif ((p == ignored_task) ||",
            "\t\t    (p->exit_state && thread_group_empty(p)) ||",
            "\t\t    is_global_init(p->real_parent))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (task_pgrp(p->real_parent) != pgrp &&",
            "\t\t    task_session(p->real_parent) == task_session(p))",
            "\t\t\treturn 0;",
            "\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "put_task_struct_rcu_user, release_thread, release_task, rcuwait_wake_up, will_become_orphaned_pgrp",
          "description": "提供RCU安全的任务结构释放机制，处理线程组解关联、会话管理及条件唤醒操作。",
          "similarity": 0.6440460681915283
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/exit.c",
          "start_line": 1098,
          "end_line": 1226,
          "content": [
            "static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)",
            "{",
            "\tint state, status;",
            "\tpid_t pid = task_pid_vnr(p);",
            "\tuid_t uid = from_kuid_munged(current_user_ns(), task_uid(p));",
            "\tstruct waitid_info *infop;",
            "",
            "\tif (!likely(wo->wo_flags & WEXITED))",
            "\t\treturn 0;",
            "",
            "\tif (unlikely(wo->wo_flags & WNOWAIT)) {",
            "\t\tstatus = (p->signal->flags & SIGNAL_GROUP_EXIT)",
            "\t\t\t? p->signal->group_exit_code : p->exit_code;",
            "\t\tget_task_struct(p);",
            "\t\tread_unlock(&tasklist_lock);",
            "\t\tsched_annotate_sleep();",
            "\t\tif (wo->wo_rusage)",
            "\t\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);",
            "\t\tput_task_struct(p);",
            "\t\tgoto out_info;",
            "\t}",
            "\t/*",
            "\t * Move the task's state to DEAD/TRACE, only one thread can do this.",
            "\t */",
            "\tstate = (ptrace_reparented(p) && thread_group_leader(p)) ?",
            "\t\tEXIT_TRACE : EXIT_DEAD;",
            "\tif (cmpxchg(&p->exit_state, EXIT_ZOMBIE, state) != EXIT_ZOMBIE)",
            "\t\treturn 0;",
            "\t/*",
            "\t * We own this thread, nobody else can reap it.",
            "\t */",
            "\tread_unlock(&tasklist_lock);",
            "\tsched_annotate_sleep();",
            "",
            "\t/*",
            "\t * Check thread_group_leader() to exclude the traced sub-threads.",
            "\t */",
            "\tif (state == EXIT_DEAD && thread_group_leader(p)) {",
            "\t\tstruct signal_struct *sig = p->signal;",
            "\t\tstruct signal_struct *psig = current->signal;",
            "\t\tunsigned long maxrss;",
            "\t\tu64 tgutime, tgstime;",
            "",
            "\t\t/*",
            "\t\t * The resource counters for the group leader are in its",
            "\t\t * own task_struct.  Those for dead threads in the group",
            "\t\t * are in its signal_struct, as are those for the child",
            "\t\t * processes it has previously reaped.  All these",
            "\t\t * accumulate in the parent's signal_struct c* fields.",
            "\t\t *",
            "\t\t * We don't bother to take a lock here to protect these",
            "\t\t * p->signal fields because the whole thread group is dead",
            "\t\t * and nobody can change them.",
            "\t\t *",
            "\t\t * psig->stats_lock also protects us from our sub-threads",
            "\t\t * which can reap other children at the same time. Until",
            "\t\t * we change k_getrusage()-like users to rely on this lock",
            "\t\t * we have to take ->siglock as well.",
            "\t\t *",
            "\t\t * We use thread_group_cputime_adjusted() to get times for",
            "\t\t * the thread group, which consolidates times for all threads",
            "\t\t * in the group including the group leader.",
            "\t\t */",
            "\t\tthread_group_cputime_adjusted(p, &tgutime, &tgstime);",
            "\t\tspin_lock_irq(&current->sighand->siglock);",
            "\t\twrite_seqlock(&psig->stats_lock);",
            "\t\tpsig->cutime += tgutime + sig->cutime;",
            "\t\tpsig->cstime += tgstime + sig->cstime;",
            "\t\tpsig->cgtime += task_gtime(p) + sig->gtime + sig->cgtime;",
            "\t\tpsig->cmin_flt +=",
            "\t\t\tp->min_flt + sig->min_flt + sig->cmin_flt;",
            "\t\tpsig->cmaj_flt +=",
            "\t\t\tp->maj_flt + sig->maj_flt + sig->cmaj_flt;",
            "\t\tpsig->cnvcsw +=",
            "\t\t\tp->nvcsw + sig->nvcsw + sig->cnvcsw;",
            "\t\tpsig->cnivcsw +=",
            "\t\t\tp->nivcsw + sig->nivcsw + sig->cnivcsw;",
            "\t\tpsig->cinblock +=",
            "\t\t\ttask_io_get_inblock(p) +",
            "\t\t\tsig->inblock + sig->cinblock;",
            "\t\tpsig->coublock +=",
            "\t\t\ttask_io_get_oublock(p) +",
            "\t\t\tsig->oublock + sig->coublock;",
            "\t\tmaxrss = max(sig->maxrss, sig->cmaxrss);",
            "\t\tif (psig->cmaxrss < maxrss)",
            "\t\t\tpsig->cmaxrss = maxrss;",
            "\t\ttask_io_accounting_add(&psig->ioac, &p->ioac);",
            "\t\ttask_io_accounting_add(&psig->ioac, &sig->ioac);",
            "\t\twrite_sequnlock(&psig->stats_lock);",
            "\t\tspin_unlock_irq(&current->sighand->siglock);",
            "\t}",
            "",
            "\tif (wo->wo_rusage)",
            "\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);",
            "\tstatus = (p->signal->flags & SIGNAL_GROUP_EXIT)",
            "\t\t? p->signal->group_exit_code : p->exit_code;",
            "\two->wo_stat = status;",
            "",
            "\tif (state == EXIT_TRACE) {",
            "\t\twrite_lock_irq(&tasklist_lock);",
            "\t\t/* We dropped tasklist, ptracer could die and untrace */",
            "\t\tptrace_unlink(p);",
            "",
            "\t\t/* If parent wants a zombie, don't release it now */",
            "\t\tstate = EXIT_ZOMBIE;",
            "\t\tif (do_notify_parent(p, p->exit_signal))",
            "\t\t\tstate = EXIT_DEAD;",
            "\t\tp->exit_state = state;",
            "\t\twrite_unlock_irq(&tasklist_lock);",
            "\t}",
            "\tif (state == EXIT_DEAD)",
            "\t\trelease_task(p);",
            "",
            "out_info:",
            "\tinfop = wo->wo_info;",
            "\tif (infop) {",
            "\t\tif ((status & 0x7f) == 0) {",
            "\t\t\tinfop->cause = CLD_EXITED;",
            "\t\t\tinfop->status = status >> 8;",
            "\t\t} else {",
            "\t\t\tinfop->cause = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;",
            "\t\t\tinfop->status = status & 0x7f;",
            "\t\t}",
            "\t\tinfop->pid = pid;",
            "\t\tinfop->uid = uid;",
            "\t}",
            "",
            "\treturn pid;",
            "}"
          ],
          "function_name": "wait_task_zombie",
          "description": "wait_task_zombie处理僵尸进程回收，当进程处于EXIT_ZOMBIE状态时，更新父进程的资源统计信息，设置退出状态，并根据等待选项填充waitid_info结构体数据，返回进程ID和退出状态。",
          "similarity": 0.6364007592201233
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/exit.c",
          "start_line": 948,
          "end_line": 1066,
          "content": [
            "void __noreturn make_task_dead(int signr)",
            "{",
            "\t/*",
            "\t * Take the task off the cpu after something catastrophic has",
            "\t * happened.",
            "\t *",
            "\t * We can get here from a kernel oops, sometimes with preemption off.",
            "\t * Start by checking for critical errors.",
            "\t * Then fix up important state like USER_DS and preemption.",
            "\t * Then do everything else.",
            "\t */",
            "\tstruct task_struct *tsk = current;",
            "\tunsigned int limit;",
            "",
            "\tif (unlikely(in_interrupt()))",
            "\t\tpanic(\"Aiee, killing interrupt handler!\");",
            "\tif (unlikely(!tsk->pid))",
            "\t\tpanic(\"Attempted to kill the idle task!\");",
            "",
            "\tif (unlikely(irqs_disabled())) {",
            "\t\tpr_info(\"note: %s[%d] exited with irqs disabled\\n\",",
            "\t\t\tcurrent->comm, task_pid_nr(current));",
            "\t\tlocal_irq_enable();",
            "\t}",
            "\tif (unlikely(in_atomic())) {",
            "\t\tpr_info(\"note: %s[%d] exited with preempt_count %d\\n\",",
            "\t\t\tcurrent->comm, task_pid_nr(current),",
            "\t\t\tpreempt_count());",
            "\t\tpreempt_count_set(PREEMPT_ENABLED);",
            "\t}",
            "",
            "\t/*",
            "\t * Every time the system oopses, if the oops happens while a reference",
            "\t * to an object was held, the reference leaks.",
            "\t * If the oops doesn't also leak memory, repeated oopsing can cause",
            "\t * reference counters to wrap around (if they're not using refcount_t).",
            "\t * This means that repeated oopsing can make unexploitable-looking bugs",
            "\t * exploitable through repeated oopsing.",
            "\t * To make sure this can't happen, place an upper bound on how often the",
            "\t * kernel may oops without panic().",
            "\t */",
            "\tlimit = READ_ONCE(oops_limit);",
            "\tif (atomic_inc_return(&oops_count) >= limit && limit)",
            "\t\tpanic(\"Oopsed too often (kernel.oops_limit is %d)\", limit);",
            "",
            "\t/*",
            "\t * We're taking recursive faults here in make_task_dead. Safest is to just",
            "\t * leave this task alone and wait for reboot.",
            "\t */",
            "\tif (unlikely(tsk->flags & PF_EXITING)) {",
            "\t\tpr_alert(\"Fixing recursive fault but reboot is needed!\\n\");",
            "\t\tfutex_exit_recursive(tsk);",
            "\t\ttsk->exit_state = EXIT_DEAD;",
            "\t\trefcount_inc(&tsk->rcu_users);",
            "\t\tdo_task_dead();",
            "\t}",
            "",
            "\tdo_exit(signr);",
            "}",
            "void __noreturn",
            "do_group_exit(int exit_code)",
            "{",
            "\tstruct signal_struct *sig = current->signal;",
            "",
            "\tif (sig->flags & SIGNAL_GROUP_EXIT)",
            "\t\texit_code = sig->group_exit_code;",
            "\telse if (sig->group_exec_task)",
            "\t\texit_code = 0;",
            "\telse {",
            "\t\tstruct sighand_struct *const sighand = current->sighand;",
            "",
            "\t\tspin_lock_irq(&sighand->siglock);",
            "\t\tif (sig->flags & SIGNAL_GROUP_EXIT)",
            "\t\t\t/* Another thread got here before we took the lock.  */",
            "\t\t\texit_code = sig->group_exit_code;",
            "\t\telse if (sig->group_exec_task)",
            "\t\t\texit_code = 0;",
            "\t\telse {",
            "\t\t\tsig->group_exit_code = exit_code;",
            "\t\t\tsig->flags = SIGNAL_GROUP_EXIT;",
            "\t\t\tzap_other_threads(current);",
            "\t\t}",
            "\t\tspin_unlock_irq(&sighand->siglock);",
            "\t}",
            "",
            "\tdo_exit(exit_code);",
            "\t/* NOTREACHED */",
            "}",
            "static int eligible_pid(struct wait_opts *wo, struct task_struct *p)",
            "{",
            "\treturn\two->wo_type == PIDTYPE_MAX ||",
            "\t\ttask_pid_type(p, wo->wo_type) == wo->wo_pid;",
            "}",
            "static int",
            "eligible_child(struct wait_opts *wo, bool ptrace, struct task_struct *p)",
            "{",
            "\tif (!eligible_pid(wo, p))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Wait for all children (clone and not) if __WALL is set or",
            "\t * if it is traced by us.",
            "\t */",
            "\tif (ptrace || (wo->wo_flags & __WALL))",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * Otherwise, wait for clone children *only* if __WCLONE is set;",
            "\t * otherwise, wait for non-clone children *only*.",
            "\t *",
            "\t * Note: a \"clone\" child here is one that reports to its parent",
            "\t * using a signal other than SIGCHLD, or a non-leader thread which",
            "\t * we can only see if it is traced by us.",
            "\t */",
            "\tif ((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))",
            "\t\treturn 0;",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "make_task_dead, do_group_exit, eligible_pid, eligible_child",
          "description": "make_task_dead处理致命错误导致的进程终止，通过do_exit完成退出流程；do_group_exit用于线程组统一退出，设置退出码并触发do_exit；eligible_pid和eligible_child用于过滤符合等待条件的子进程，根据PID类型和跟踪标志进行匹配。",
          "similarity": 0.6209113597869873
        }
      ]
    },
    {
      "source_file": "kernel/sched/ext.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:08:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\ext.c`\n\n---\n\n# `sched/ext.c` 技术文档\n\n## 文件概述\n\n`sched/ext.c` 是 Linux 内核中 **BPF 可扩展调度器（sched_ext）** 的核心实现文件之一，定义了调度器与 BPF 程序交互所需的数据结构、常量和操作接口。该文件为用户空间通过 BPF 实现自定义调度策略提供了内核侧的框架支持，允许将任务调度逻辑完全委托给加载的 BPF 程序，同时保留与内核调度子系统的安全集成。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`struct sched_ext_ops`**  \n  BPF 调度器的操作函数表，包含调度器必须或可选实现的回调函数，如 `select_cpu`、`enqueue`、`dequeue`、`dispatch` 等，用于控制任务的 CPU 选择、入队、出队和分发逻辑。\n\n- **`struct scx_exit_info`**  \n  描述 BPF 调度器退出原因的结构体，包含退出类型（`kind`）、退出码（`exit_code`）、错误信息（`reason`、`msg`）、回溯栈（`bt`）和调试转储（`dump`）。\n\n- **`struct scx_init_task_args` / `scx_exit_task_args`**  \n  分别用于 `ops.init_task()` 和 `ops.exit_task()` 回调的参数容器，传递任务初始化/退出上下文（如是否由 fork 触发、所属 cgroup 等）。\n\n- **`struct scx_cpu_acquire_args` / `scx_cpu_release_args`**  \n  用于 CPU 获取/释放回调的参数结构，其中 `cpu_release` 包含抢占原因（如 RT/DL 任务抢占）和即将运行的任务。\n\n- **`struct scx_dump_ctx`**  \n  为调度器转储（dump）操作提供上下文信息，包括退出类型、时间戳等。\n\n### 关键枚举与常量\n\n- **`enum scx_exit_kind`**  \n  定义调度器退出的类别，如正常退出（`SCX_EXIT_DONE`）、用户/BPF/内核主动注销（`SCX_EXIT_UNREG*`）、系统请求（`SCX_EXIT_SYSRQ`）或运行时错误（`SCX_EXIT_ERROR*`）。\n\n- **`enum scx_exit_code`**  \n  定义 64 位退出码的位域格式，支持系统原因（如 `SCX_ECODE_RSN_HOTPLUG`）和系统动作（如 `SCX_ECODE_ACT_RESTART`），允许用户自定义退出上下文。\n\n- **`enum scx_ops_flags`**  \n  调度器操作标志，控制调度行为：\n  - `SCX_OPS_KEEP_BUILTIN_IDLE`：保留内建空闲跟踪\n  - `SCX_OPS_ENQ_LAST`：切片到期后仍无任务时重新入队\n  - `SCX_OPS_ENQ_EXITING`：由 BPF 处理退出中任务\n  - `SCX_OPS_SWITCH_PARTIAL`：仅调度 `SCHED_EXT` 策略任务\n  - `SCX_OPS_HAS_CGROUP_WEIGHT`：支持 cgroup cpu.weight\n\n- **调度器常量**  \n  如 `SCX_DSP_DFL_MAX_BATCH`（默认分发批大小）、`SCX_WATCHDOG_MAX_TIMEOUT`（看门狗超时）、`SCX_OPS_TASK_ITER_BATCH`（任务迭代锁释放批次）等，用于控制调度器内部行为。\n\n## 关键实现\n\n- **BPF 调度器生命周期管理**  \n  通过 `scx_exit_info` 和退出码机制，支持多种退出路径（用户、BPF、内核、SysRq、错误），并提供详细的诊断信息（回溯、消息、转储）。\n\n- **任务入队优化**  \n  在 `select_cpu` 中允许直接插入 DSQ（如本地 DSQ），跳过后续 `enqueue` 调用，减少调度开销；同时通过 `SCX_OPS_ENQ_EXITING` 标志处理退出中任务的调度问题，避免 RCU 停顿。\n\n- **CPU 抢占通知**  \n  通过 `scx_cpu_release_args` 向 BPF 调度器传递 CPU 被高优先级调度类（RT/DL/Stop）抢占的原因，便于调度器做出相应调整。\n\n- **cgroup 集成**  \n  支持 cgroup 调度（`CONFIG_EXT_GROUP_SCHED`），在任务加入 cgroup 时传递权重信息（`scx_cgroup_init_args`），并通过 `SCX_OPS_HAS_CGROUP_WEIGHT` 标志启用。\n\n- **安全与鲁棒性**  \n  内核侧跟踪 BPF 是否拥有任务，可忽略无效分发；任务迭代时定期释放锁（`SCX_OPS_TASK_ITER_BATCH`），防止 RCU/CSD 停顿；看门狗机制（`SCX_WATCHDOG_MAX_TIMEOUT`）检测任务卡死。\n\n## 依赖关系\n\n- **BPF 子系统**：通过 `#include <linux/bpf.h>` 依赖 BPF 基础设施，用于加载和验证调度器 BPF 程序。\n- **调度核心**：与 `kernel/sched/` 下的核心调度代码（如 `core.c`、`rt.c`、`dl.c`）交互，处理任务入队、CPU 选择和抢占。\n- **cgroup 子系统**：当启用 `CONFIG_EXT_GROUP_SCHED` 时，依赖 cgroup CPU 控制器获取任务权重和层级信息。\n- **RCU 与锁机制**：使用 `scx_tasks_lock` 保护任务迭代，需与 RCU 同步机制协调。\n\n## 使用场景\n\n- **自定义调度策略开发**：用户通过 BPF 实现特定工作负载的调度逻辑（如延迟敏感型、批处理优化、NUMA 感知等），并注册到 `sched_ext`。\n- **系统调试与监控**：利用 `ops.dump()` 和退出信息结构体，在调度器异常退出时收集诊断数据。\n- **混合调度部署**：通过 `SCX_OPS_SWITCH_PARTIAL` 标志，仅对部分任务（`SCHED_EXT`）启用 BPF 调度，其余任务仍由 CFS 处理。\n- **资源隔离与 QoS**：结合 cgroup 支持，为不同 cgroup 配置不同的调度行为和资源权重。\n- **内核调度实验平台**：作为安全的沙箱环境，测试新型调度算法而无需修改核心调度代码。",
      "similarity": 0.655738353729248,
      "chunks": [
        {
          "chunk_id": 22,
          "file_path": "kernel/sched/ext.c",
          "start_line": 4366,
          "end_line": 4528,
          "content": [
            "static void free_exit_info(struct scx_exit_info *ei)",
            "{",
            "\tkfree(ei->dump);",
            "\tkfree(ei->msg);",
            "\tkfree(ei->bt);",
            "\tkfree(ei);",
            "}",
            "static void scx_ops_disable_workfn(struct kthread_work *work)",
            "{",
            "\tstruct scx_exit_info *ei = scx_exit_info;",
            "\tstruct scx_task_iter sti;",
            "\tstruct task_struct *p;",
            "\tstruct rhashtable_iter rht_iter;",
            "\tstruct scx_dispatch_q *dsq;",
            "\tint i, kind;",
            "",
            "\tkind = atomic_read(&scx_exit_kind);",
            "\twhile (true) {",
            "\t\t/*",
            "\t\t * NONE indicates that a new scx_ops has been registered since",
            "\t\t * disable was scheduled - don't kill the new ops. DONE",
            "\t\t * indicates that the ops has already been disabled.",
            "\t\t */",
            "\t\tif (kind == SCX_EXIT_NONE || kind == SCX_EXIT_DONE)",
            "\t\t\treturn;",
            "\t\tif (atomic_try_cmpxchg(&scx_exit_kind, &kind, SCX_EXIT_DONE))",
            "\t\t\tbreak;",
            "\t}",
            "\tei->kind = kind;",
            "\tei->reason = scx_exit_reason(ei->kind);",
            "",
            "\t/* guarantee forward progress by bypassing scx_ops */",
            "\tscx_ops_bypass(true);",
            "",
            "\tswitch (scx_ops_set_enable_state(SCX_OPS_DISABLING)) {",
            "\tcase SCX_OPS_DISABLING:",
            "\t\tWARN_ONCE(true, \"sched_ext: duplicate disabling instance?\");",
            "\t\tbreak;",
            "\tcase SCX_OPS_DISABLED:",
            "\t\tpr_warn(\"sched_ext: ops error detected without ops (%s)\\n\",",
            "\t\t\tscx_exit_info->msg);",
            "\t\tWARN_ON_ONCE(scx_ops_set_enable_state(SCX_OPS_DISABLED) !=",
            "\t\t\t     SCX_OPS_DISABLING);",
            "\t\tgoto done;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "",
            "\t/*",
            "\t * Here, every runnable task is guaranteed to make forward progress and",
            "\t * we can safely use blocking synchronization constructs. Actually",
            "\t * disable ops.",
            "\t */",
            "\tmutex_lock(&scx_ops_enable_mutex);",
            "",
            "\tstatic_branch_disable(&__scx_switched_all);",
            "\tWRITE_ONCE(scx_switching_all, false);",
            "",
            "\t/*",
            "\t * Shut down cgroup support before tasks so that the cgroup attach path",
            "\t * doesn't race against scx_ops_exit_task().",
            "\t */",
            "\tscx_cgroup_lock();",
            "\tscx_cgroup_exit();",
            "\tscx_cgroup_unlock();",
            "",
            "\t/*",
            "\t * The BPF scheduler is going away. All tasks including %TASK_DEAD ones",
            "\t * must be switched out and exited synchronously.",
            "\t */",
            "\tpercpu_down_write(&scx_fork_rwsem);",
            "",
            "\tscx_ops_init_task_enabled = false;",
            "",
            "\tscx_task_iter_start(&sti);",
            "\twhile ((p = scx_task_iter_next_locked(&sti))) {",
            "\t\tconst struct sched_class *old_class = p->sched_class;",
            "\t\tconst struct sched_class *new_class =",
            "\t\t\t__setscheduler_class(p->policy, p->prio);",
            "\t\tstruct sched_enq_and_set_ctx ctx;",
            "",
            "\t\tif (old_class != new_class && p->se.sched_delayed)",
            "\t\t\tdequeue_task(task_rq(p), p, DEQUEUE_SLEEP | DEQUEUE_DELAYED);",
            "",
            "\t\tsched_deq_and_put_task(p, DEQUEUE_SAVE | DEQUEUE_MOVE, &ctx);",
            "",
            "\t\tp->sched_class = new_class;",
            "\t\tcheck_class_changing(task_rq(p), p, old_class);",
            "",
            "\t\tsched_enq_and_set_task(&ctx);",
            "",
            "\t\tcheck_class_changed(task_rq(p), p, old_class, p->prio);",
            "\t\tscx_ops_exit_task(p);",
            "\t}",
            "\tscx_task_iter_stop(&sti);",
            "\tpercpu_up_write(&scx_fork_rwsem);",
            "",
            "\t/* no task is on scx, turn off all the switches and flush in-progress calls */",
            "\tstatic_branch_disable(&__scx_ops_enabled);",
            "\tfor (i = SCX_OPI_BEGIN; i < SCX_OPI_END; i++)",
            "\t\tstatic_branch_disable(&scx_has_op[i]);",
            "\tstatic_branch_disable(&scx_ops_enq_last);",
            "\tstatic_branch_disable(&scx_ops_enq_exiting);",
            "\tstatic_branch_disable(&scx_ops_cpu_preempt);",
            "\tstatic_branch_disable(&scx_builtin_idle_enabled);",
            "\tsynchronize_rcu();",
            "",
            "\tif (ei->kind >= SCX_EXIT_ERROR) {",
            "\t\tpr_err(\"sched_ext: BPF scheduler \\\"%s\\\" disabled (%s)\\n\",",
            "\t\t       scx_ops.name, ei->reason);",
            "",
            "\t\tif (ei->msg[0] != '\\0')",
            "\t\t\tpr_err(\"sched_ext: %s: %s\\n\", scx_ops.name, ei->msg);",
            "#ifdef CONFIG_STACKTRACE",
            "\t\tstack_trace_print(ei->bt, ei->bt_len, 2);",
            "#endif",
            "\t} else {",
            "\t\tpr_info(\"sched_ext: BPF scheduler \\\"%s\\\" disabled (%s)\\n\",",
            "\t\t\tscx_ops.name, ei->reason);",
            "\t}",
            "",
            "\tif (scx_ops.exit)",
            "\t\tSCX_CALL_OP(SCX_KF_UNLOCKED, exit, ei);",
            "",
            "\tcancel_delayed_work_sync(&scx_watchdog_work);",
            "",
            "\t/*",
            "\t * Delete the kobject from the hierarchy eagerly in addition to just",
            "\t * dropping a reference. Otherwise, if the object is deleted",
            "\t * asynchronously, sysfs could observe an object of the same name still",
            "\t * in the hierarchy when another scheduler is loaded.",
            "\t */",
            "\tkobject_del(scx_root_kobj);",
            "\tkobject_put(scx_root_kobj);",
            "\tscx_root_kobj = NULL;",
            "",
            "\tmemset(&scx_ops, 0, sizeof(scx_ops));",
            "",
            "\trhashtable_walk_enter(&dsq_hash, &rht_iter);",
            "\tdo {",
            "\t\trhashtable_walk_start(&rht_iter);",
            "",
            "\t\twhile ((dsq = rhashtable_walk_next(&rht_iter)) && !IS_ERR(dsq))",
            "\t\t\tdestroy_dsq(dsq->id);",
            "",
            "\t\trhashtable_walk_stop(&rht_iter);",
            "\t} while (dsq == ERR_PTR(-EAGAIN));",
            "\trhashtable_walk_exit(&rht_iter);",
            "",
            "\tfree_percpu(scx_dsp_ctx);",
            "\tscx_dsp_ctx = NULL;",
            "\tscx_dsp_max_batch = 0;",
            "",
            "\tfree_exit_info(scx_exit_info);",
            "\tscx_exit_info = NULL;",
            "",
            "\tmutex_unlock(&scx_ops_enable_mutex);",
            "",
            "\tWARN_ON_ONCE(scx_ops_set_enable_state(SCX_OPS_DISABLED) !=",
            "\t\t     SCX_OPS_DISABLING);",
            "done:",
            "\tscx_ops_bypass(false);",
            "}"
          ],
          "function_name": "free_exit_info, scx_ops_disable_workfn",
          "description": "该代码段包含两个函数：  \n1. `free_exit_info` 用于释放 `scx_exit_info` 结构体关联的动态内存（`dump`/`msg`/`bt`）及自身；  \n2. `scx_ops_disable_workfn` 核心功能为安全禁用 SCX 操作，通过原子操作标记状态、强制切换所有任务至新调度类、清理 DSQ 和 kobject 等资源，并最终释放 `scx_exit_info`。  \n\n`scx_ops_disable_workfn` 实现了 SCX 操作的有序停用逻辑，确保任务迁移、状态同步及资源回收的完整性，同时处理异常情况下的错误日志输出。",
          "similarity": 0.6203750967979431
        },
        {
          "chunk_id": 16,
          "file_path": "kernel/sched/ext.c",
          "start_line": 3513,
          "end_line": 3625,
          "content": [
            "static void scx_ops_disable_task(struct task_struct *p)",
            "{",
            "\tlockdep_assert_rq_held(task_rq(p));",
            "\tWARN_ON_ONCE(scx_get_task_state(p) != SCX_TASK_ENABLED);",
            "",
            "\tif (SCX_HAS_OP(disable))",
            "\t\tSCX_CALL_OP(SCX_KF_REST, disable, p);",
            "\tscx_set_task_state(p, SCX_TASK_READY);",
            "}",
            "static void scx_ops_exit_task(struct task_struct *p)",
            "{",
            "\tstruct scx_exit_task_args args = {",
            "\t\t.cancelled = false,",
            "\t};",
            "",
            "\tlockdep_assert_rq_held(task_rq(p));",
            "",
            "\tswitch (scx_get_task_state(p)) {",
            "\tcase SCX_TASK_NONE:",
            "\t\treturn;",
            "\tcase SCX_TASK_INIT:",
            "\t\targs.cancelled = true;",
            "\t\tbreak;",
            "\tcase SCX_TASK_READY:",
            "\t\tbreak;",
            "\tcase SCX_TASK_ENABLED:",
            "\t\tscx_ops_disable_task(p);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(true);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (SCX_HAS_OP(exit_task))",
            "\t\tSCX_CALL_OP(SCX_KF_REST, exit_task, p, &args);",
            "\tscx_set_task_state(p, SCX_TASK_NONE);",
            "}",
            "void init_scx_entity(struct sched_ext_entity *scx)",
            "{",
            "\t/*",
            "\t * init_idle() calls this function again after fork sequence is",
            "\t * complete. Don't touch ->tasks_node as it's already linked.",
            "\t */",
            "\tmemset(scx, 0, offsetof(struct sched_ext_entity, tasks_node));",
            "",
            "\tINIT_LIST_HEAD(&scx->dsq_list.node);",
            "\tRB_CLEAR_NODE(&scx->dsq_priq);",
            "\tscx->sticky_cpu = -1;",
            "\tscx->holding_cpu = -1;",
            "\tINIT_LIST_HEAD(&scx->runnable_node);",
            "\tscx->runnable_at = jiffies;",
            "\tscx->ddsp_dsq_id = SCX_DSQ_INVALID;",
            "\tscx->slice = SCX_SLICE_DFL;",
            "}",
            "void scx_pre_fork(struct task_struct *p)",
            "{",
            "\t/*",
            "\t * BPF scheduler enable/disable paths want to be able to iterate and",
            "\t * update all tasks which can become complex when racing forks. As",
            "\t * enable/disable are very cold paths, let's use a percpu_rwsem to",
            "\t * exclude forks.",
            "\t */",
            "\tpercpu_down_read(&scx_fork_rwsem);",
            "}",
            "int scx_fork(struct task_struct *p)",
            "{",
            "\tpercpu_rwsem_assert_held(&scx_fork_rwsem);",
            "",
            "\tif (scx_ops_init_task_enabled)",
            "\t\treturn scx_ops_init_task(p, task_group(p), true);",
            "\telse",
            "\t\treturn 0;",
            "}",
            "void scx_post_fork(struct task_struct *p)",
            "{",
            "\tif (scx_ops_init_task_enabled) {",
            "\t\tscx_set_task_state(p, SCX_TASK_READY);",
            "",
            "\t\t/*",
            "\t\t * Enable the task immediately if it's running on sched_ext.",
            "\t\t * Otherwise, it'll be enabled in switching_to_scx() if and",
            "\t\t * when it's ever configured to run with a SCHED_EXT policy.",
            "\t\t */",
            "\t\tif (p->sched_class == &ext_sched_class) {",
            "\t\t\tstruct rq_flags rf;",
            "\t\t\tstruct rq *rq;",
            "",
            "\t\t\trq = task_rq_lock(p, &rf);",
            "\t\t\tscx_ops_enable_task(p);",
            "\t\t\ttask_rq_unlock(rq, p, &rf);",
            "\t\t}",
            "\t}",
            "",
            "\tspin_lock_irq(&scx_tasks_lock);",
            "\tlist_add_tail(&p->scx.tasks_node, &scx_tasks);",
            "\tspin_unlock_irq(&scx_tasks_lock);",
            "",
            "\tpercpu_up_read(&scx_fork_rwsem);",
            "}",
            "void scx_cancel_fork(struct task_struct *p)",
            "{",
            "\tif (scx_enabled()) {",
            "\t\tstruct rq *rq;",
            "\t\tstruct rq_flags rf;",
            "",
            "\t\trq = task_rq_lock(p, &rf);",
            "\t\tWARN_ON_ONCE(scx_get_task_state(p) >= SCX_TASK_READY);",
            "\t\tscx_ops_exit_task(p);",
            "\t\ttask_rq_unlock(rq, p, &rf);",
            "\t}",
            "",
            "\tpercpu_up_read(&scx_fork_rwsem);",
            "}"
          ],
          "function_name": "scx_ops_disable_task, scx_ops_exit_task, init_scx_entity, scx_pre_fork, scx_fork, scx_post_fork, scx_cancel_fork",
          "description": "提供任务禁用退出逻辑和fork流程控制，通过读锁保护并发访问，在分叉前后调整任务状态并维护全局任务列表。",
          "similarity": 0.6202893257141113
        },
        {
          "chunk_id": 35,
          "file_path": "kernel/sched/ext.c",
          "start_line": 6810,
          "end_line": 6910,
          "content": [
            "static s32 bstr_format(struct scx_bstr_buf *buf,",
            "\t\t       char *fmt, unsigned long long *data, u32 data__sz)",
            "{",
            "\treturn __bstr_format(buf->data, buf->line, sizeof(buf->line),",
            "\t\t\t     fmt, data, data__sz);",
            "}",
            "__bpf_kfunc void scx_bpf_exit_bstr(s64 exit_code, char *fmt,",
            "\t\t\t\t   unsigned long long *data, u32 data__sz)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&scx_exit_bstr_buf_lock, flags);",
            "\tif (bstr_format(&scx_exit_bstr_buf, fmt, data, data__sz) >= 0)",
            "\t\tscx_ops_exit_kind(SCX_EXIT_UNREG_BPF, exit_code, \"%s\",",
            "\t\t\t\t  scx_exit_bstr_buf.line);",
            "\traw_spin_unlock_irqrestore(&scx_exit_bstr_buf_lock, flags);",
            "}",
            "__bpf_kfunc void scx_bpf_error_bstr(char *fmt, unsigned long long *data,",
            "\t\t\t\t    u32 data__sz)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&scx_exit_bstr_buf_lock, flags);",
            "\tif (bstr_format(&scx_exit_bstr_buf, fmt, data, data__sz) >= 0)",
            "\t\tscx_ops_exit_kind(SCX_EXIT_ERROR_BPF, 0, \"%s\",",
            "\t\t\t\t  scx_exit_bstr_buf.line);",
            "\traw_spin_unlock_irqrestore(&scx_exit_bstr_buf_lock, flags);",
            "}",
            "__bpf_kfunc void scx_bpf_dump_bstr(char *fmt, unsigned long long *data,",
            "\t\t\t\t   u32 data__sz)",
            "{",
            "\tstruct scx_dump_data *dd = &scx_dump_data;",
            "\tstruct scx_bstr_buf *buf = &dd->buf;",
            "\ts32 ret;",
            "",
            "\tif (raw_smp_processor_id() != dd->cpu) {",
            "\t\tscx_ops_error(\"scx_bpf_dump() must only be called from ops.dump() and friends\");",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* append the formatted string to the line buf */",
            "\tret = __bstr_format(buf->data, buf->line + dd->cursor,",
            "\t\t\t    sizeof(buf->line) - dd->cursor, fmt, data, data__sz);",
            "\tif (ret < 0) {",
            "\t\tdump_line(dd->s, \"%s[!] (\\\"%s\\\", %p, %u) failed to format (%d)\",",
            "\t\t\t  dd->prefix, fmt, data, data__sz, ret);",
            "\t\treturn;",
            "\t}",
            "",
            "\tdd->cursor += ret;",
            "\tdd->cursor = min_t(s32, dd->cursor, sizeof(buf->line));",
            "",
            "\tif (!dd->cursor)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If the line buf overflowed or ends in a newline, flush it into the",
            "\t * dump. This is to allow the caller to generate a single line over",
            "\t * multiple calls. As ops_dump_flush() can also handle multiple lines in",
            "\t * the line buf, the only case which can lead to an unexpected",
            "\t * truncation is when the caller keeps generating newlines in the middle",
            "\t * instead of the end consecutively. Don't do that.",
            "\t */",
            "\tif (dd->cursor >= sizeof(buf->line) || buf->line[dd->cursor - 1] == '\\n')",
            "\t\tops_dump_flush();",
            "}",
            "__bpf_kfunc u32 scx_bpf_cpuperf_cap(s32 cpu)",
            "{",
            "\tif (ops_cpu_valid(cpu, NULL))",
            "\t\treturn arch_scale_cpu_capacity(cpu);",
            "\telse",
            "\t\treturn SCX_CPUPERF_ONE;",
            "}",
            "__bpf_kfunc u32 scx_bpf_cpuperf_cur(s32 cpu)",
            "{",
            "\tif (ops_cpu_valid(cpu, NULL))",
            "\t\treturn arch_scale_freq_capacity(cpu);",
            "\telse",
            "\t\treturn SCX_CPUPERF_ONE;",
            "}",
            "__bpf_kfunc void scx_bpf_cpuperf_set(s32 cpu, u32 perf)",
            "{",
            "\tif (unlikely(perf > SCX_CPUPERF_ONE)) {",
            "\t\tscx_ops_error(\"Invalid cpuperf target %u for CPU %d\", perf, cpu);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (ops_cpu_valid(cpu, NULL)) {",
            "\t\tstruct rq *rq = cpu_rq(cpu);",
            "",
            "\t\trq->scx.cpuperf_target = perf;",
            "",
            "\t\trcu_read_lock_sched_notrace();",
            "\t\tcpufreq_update_util(cpu_rq(cpu), 0);",
            "\t\trcu_read_unlock_sched_notrace();",
            "\t}",
            "}",
            "__bpf_kfunc u32 scx_bpf_nr_cpu_ids(void)",
            "{",
            "\treturn nr_cpu_ids;",
            "}"
          ],
          "function_name": "bstr_format, scx_bpf_exit_bstr, scx_bpf_error_bstr, scx_bpf_dump_bstr, scx_bpf_cpuperf_cap, scx_bpf_cpuperf_cur, scx_bpf_cpuperf_set, scx_bpf_nr_cpu_ids",
          "description": "实现字符串格式化辅助函数及错误/退出信息记录功能，通过锁保护缓冲区并调用scx_ops_exit_kind或scx_ops_error接口上报格式化后的错误信息或退出码。包含CPU性能监控相关接口，提供CPU容量比例查询和设置接口，以及获取CPU数量的功能。",
          "similarity": 0.5984071493148804
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/ext.c",
          "start_line": 2075,
          "end_line": 2208,
          "content": [
            "static void clr_task_runnable(struct task_struct *p, bool reset_runnable_at)",
            "{",
            "\tlist_del_init(&p->scx.runnable_node);",
            "\tif (reset_runnable_at)",
            "\t\tp->scx.flags |= SCX_TASK_RESET_RUNNABLE_AT;",
            "}",
            "static void enqueue_task_scx(struct rq *rq, struct task_struct *p, int enq_flags)",
            "{",
            "\tint sticky_cpu = p->scx.sticky_cpu;",
            "",
            "\tif (enq_flags & ENQUEUE_WAKEUP)",
            "\t\trq->scx.flags |= SCX_RQ_IN_WAKEUP;",
            "",
            "\tenq_flags |= rq->scx.extra_enq_flags;",
            "",
            "\tif (sticky_cpu >= 0)",
            "\t\tp->scx.sticky_cpu = -1;",
            "",
            "\t/*",
            "\t * Restoring a running task will be immediately followed by",
            "\t * set_next_task_scx() which expects the task to not be on the BPF",
            "\t * scheduler as tasks can only start running through local DSQs. Force",
            "\t * direct-dispatch into the local DSQ by setting the sticky_cpu.",
            "\t */",
            "\tif (unlikely(enq_flags & ENQUEUE_RESTORE) && task_current(rq, p))",
            "\t\tsticky_cpu = cpu_of(rq);",
            "",
            "\tif (p->scx.flags & SCX_TASK_QUEUED) {",
            "\t\tWARN_ON_ONCE(!task_runnable(p));",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tset_task_runnable(rq, p);",
            "\tp->scx.flags |= SCX_TASK_QUEUED;",
            "\trq->scx.nr_running++;",
            "\tadd_nr_running(rq, 1);",
            "",
            "\tif (SCX_HAS_OP(runnable) && !task_on_rq_migrating(p))",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, runnable, p, enq_flags);",
            "",
            "\tif (enq_flags & SCX_ENQ_WAKEUP)",
            "\t\ttouch_core_sched(rq, p);",
            "",
            "\tdo_enqueue_task(rq, p, enq_flags, sticky_cpu);",
            "out:",
            "\trq->scx.flags &= ~SCX_RQ_IN_WAKEUP;",
            "}",
            "static void ops_dequeue(struct task_struct *p, u64 deq_flags)",
            "{",
            "\tunsigned long opss;",
            "",
            "\t/* dequeue is always temporary, don't reset runnable_at */",
            "\tclr_task_runnable(p, false);",
            "",
            "\t/* acquire ensures that we see the preceding updates on QUEUED */",
            "\topss = atomic_long_read_acquire(&p->scx.ops_state);",
            "",
            "\tswitch (opss & SCX_OPSS_STATE_MASK) {",
            "\tcase SCX_OPSS_NONE:",
            "\t\tbreak;",
            "\tcase SCX_OPSS_QUEUEING:",
            "\t\t/*",
            "\t\t * QUEUEING is started and finished while holding @p's rq lock.",
            "\t\t * As we're holding the rq lock now, we shouldn't see QUEUEING.",
            "\t\t */",
            "\t\tBUG();",
            "\tcase SCX_OPSS_QUEUED:",
            "\t\tif (SCX_HAS_OP(dequeue))",
            "\t\t\tSCX_CALL_OP_TASK(SCX_KF_REST, dequeue, p, deq_flags);",
            "",
            "\t\tif (atomic_long_try_cmpxchg(&p->scx.ops_state, &opss,",
            "\t\t\t\t\t    SCX_OPSS_NONE))",
            "\t\t\tbreak;",
            "\t\tfallthrough;",
            "\tcase SCX_OPSS_DISPATCHING:",
            "\t\t/*",
            "\t\t * If @p is being dispatched from the BPF scheduler to a DSQ,",
            "\t\t * wait for the transfer to complete so that @p doesn't get",
            "\t\t * added to its DSQ after dequeueing is complete.",
            "\t\t *",
            "\t\t * As we're waiting on DISPATCHING with the rq locked, the",
            "\t\t * dispatching side shouldn't try to lock the rq while",
            "\t\t * DISPATCHING is set. See dispatch_to_local_dsq().",
            "\t\t *",
            "\t\t * DISPATCHING shouldn't have qseq set and control can reach",
            "\t\t * here with NONE @opss from the above QUEUED case block.",
            "\t\t * Explicitly wait on %SCX_OPSS_DISPATCHING instead of @opss.",
            "\t\t */",
            "\t\twait_ops_state(p, SCX_OPSS_DISPATCHING);",
            "\t\tBUG_ON(atomic_long_read(&p->scx.ops_state) != SCX_OPSS_NONE);",
            "\t\tbreak;",
            "\t}",
            "}",
            "static bool dequeue_task_scx(struct rq *rq, struct task_struct *p, int deq_flags)",
            "{",
            "\tif (!(p->scx.flags & SCX_TASK_QUEUED)) {",
            "\t\tWARN_ON_ONCE(task_runnable(p));",
            "\t\treturn true;",
            "\t}",
            "",
            "\tops_dequeue(p, deq_flags);",
            "",
            "\t/*",
            "\t * A currently running task which is going off @rq first gets dequeued",
            "\t * and then stops running. As we want running <-> stopping transitions",
            "\t * to be contained within runnable <-> quiescent transitions, trigger",
            "\t * ->stopping() early here instead of in put_prev_task_scx().",
            "\t *",
            "\t * @p may go through multiple stopping <-> running transitions between",
            "\t * here and put_prev_task_scx() if task attribute changes occur while",
            "\t * balance_scx() leaves @rq unlocked. However, they don't contain any",
            "\t * information meaningful to the BPF scheduler and can be suppressed by",
            "\t * skipping the callbacks if the task is !QUEUED.",
            "\t */",
            "\tif (SCX_HAS_OP(stopping) && task_current(rq, p)) {",
            "\t\tupdate_curr_scx(rq);",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, stopping, p, false);",
            "\t}",
            "",
            "\tif (SCX_HAS_OP(quiescent) && !task_on_rq_migrating(p))",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, quiescent, p, deq_flags);",
            "",
            "\tif (deq_flags & SCX_DEQ_SLEEP)",
            "\t\tp->scx.flags |= SCX_TASK_DEQD_FOR_SLEEP;",
            "\telse",
            "\t\tp->scx.flags &= ~SCX_TASK_DEQD_FOR_SLEEP;",
            "",
            "\tp->scx.flags &= ~SCX_TASK_QUEUED;",
            "\trq->scx.nr_running--;",
            "\tsub_nr_running(rq, 1);",
            "",
            "\tdispatch_dequeue(rq, p);",
            "\treturn true;",
            "}"
          ],
          "function_name": "clr_task_runnable, enqueue_task_scx, ops_dequeue, dequeue_task_scx",
          "description": "处理任务入队和出队逻辑，enqueue_task_scx标记任务为排队状态并触发BPF操作，ops_dequeue根据状态执行解队操作，dequeue_task_scx移除任务并通知BPF调度器任务离开运行队列",
          "similarity": 0.5922950506210327
        },
        {
          "chunk_id": 9,
          "file_path": "kernel/sched/ext.c",
          "start_line": 2554,
          "end_line": 2736,
          "content": [
            "static void finish_dispatch(struct rq *rq, struct task_struct *p,",
            "\t\t\t    unsigned long qseq_at_dispatch,",
            "\t\t\t    u64 dsq_id, u64 enq_flags)",
            "{",
            "\tstruct scx_dispatch_q *dsq;",
            "\tunsigned long opss;",
            "",
            "\ttouch_core_sched_dispatch(rq, p);",
            "retry:",
            "\t/*",
            "\t * No need for _acquire here. @p is accessed only after a successful",
            "\t * try_cmpxchg to DISPATCHING.",
            "\t */",
            "\topss = atomic_long_read(&p->scx.ops_state);",
            "",
            "\tswitch (opss & SCX_OPSS_STATE_MASK) {",
            "\tcase SCX_OPSS_DISPATCHING:",
            "\tcase SCX_OPSS_NONE:",
            "\t\t/* someone else already got to it */",
            "\t\treturn;",
            "\tcase SCX_OPSS_QUEUED:",
            "\t\t/*",
            "\t\t * If qseq doesn't match, @p has gone through at least one",
            "\t\t * dispatch/dequeue and re-enqueue cycle between",
            "\t\t * scx_bpf_dsq_insert() and here and we have no claim on it.",
            "\t\t */",
            "\t\tif ((opss & SCX_OPSS_QSEQ_MASK) != qseq_at_dispatch)",
            "\t\t\treturn;",
            "",
            "\t\t/*",
            "\t\t * While we know @p is accessible, we don't yet have a claim on",
            "\t\t * it - the BPF scheduler is allowed to dispatch tasks",
            "\t\t * spuriously and there can be a racing dequeue attempt. Let's",
            "\t\t * claim @p by atomically transitioning it from QUEUED to",
            "\t\t * DISPATCHING.",
            "\t\t */",
            "\t\tif (likely(atomic_long_try_cmpxchg(&p->scx.ops_state, &opss,",
            "\t\t\t\t\t\t   SCX_OPSS_DISPATCHING)))",
            "\t\t\tbreak;",
            "\t\tgoto retry;",
            "\tcase SCX_OPSS_QUEUEING:",
            "\t\t/*",
            "\t\t * do_enqueue_task() is in the process of transferring the task",
            "\t\t * to the BPF scheduler while holding @p's rq lock. As we aren't",
            "\t\t * holding any kernel or BPF resource that the enqueue path may",
            "\t\t * depend upon, it's safe to wait.",
            "\t\t */",
            "\t\twait_ops_state(p, opss);",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tBUG_ON(!(p->scx.flags & SCX_TASK_QUEUED));",
            "",
            "\tdsq = find_dsq_for_dispatch(this_rq(), dsq_id, p);",
            "",
            "\tif (dsq->id == SCX_DSQ_LOCAL)",
            "\t\tdispatch_to_local_dsq(rq, dsq, p, enq_flags);",
            "\telse",
            "\t\tdispatch_enqueue(dsq, p, enq_flags | SCX_ENQ_CLEAR_OPSS);",
            "}",
            "static void flush_dispatch_buf(struct rq *rq)",
            "{",
            "\tstruct scx_dsp_ctx *dspc = this_cpu_ptr(scx_dsp_ctx);",
            "\tu32 u;",
            "",
            "\tfor (u = 0; u < dspc->cursor; u++) {",
            "\t\tstruct scx_dsp_buf_ent *ent = &dspc->buf[u];",
            "",
            "\t\tfinish_dispatch(rq, ent->task, ent->qseq, ent->dsq_id,",
            "\t\t\t\tent->enq_flags);",
            "\t}",
            "",
            "\tdspc->nr_tasks += dspc->cursor;",
            "\tdspc->cursor = 0;",
            "}",
            "static int balance_one(struct rq *rq, struct task_struct *prev)",
            "{",
            "\tstruct scx_dsp_ctx *dspc = this_cpu_ptr(scx_dsp_ctx);",
            "\tbool prev_on_scx = prev->sched_class == &ext_sched_class;",
            "\tint nr_loops = SCX_DSP_MAX_LOOPS;",
            "",
            "\tlockdep_assert_rq_held(rq);",
            "\trq->scx.flags |= SCX_RQ_IN_BALANCE;",
            "\trq->scx.flags &= ~SCX_RQ_BAL_KEEP;",
            "",
            "\tif (static_branch_unlikely(&scx_ops_cpu_preempt) &&",
            "\t    unlikely(rq->scx.cpu_released)) {",
            "\t\t/*",
            "\t\t * If the previous sched_class for the current CPU was not SCX,",
            "\t\t * notify the BPF scheduler that it again has control of the",
            "\t\t * core. This callback complements ->cpu_release(), which is",
            "\t\t * emitted in switch_class().",
            "\t\t */",
            "\t\tif (SCX_HAS_OP(cpu_acquire))",
            "\t\t\tSCX_CALL_OP(0, cpu_acquire, cpu_of(rq), NULL);",
            "\t\trq->scx.cpu_released = false;",
            "\t}",
            "",
            "\tif (prev_on_scx) {",
            "\t\tupdate_curr_scx(rq);",
            "",
            "\t\t/*",
            "\t\t * If @prev is runnable & has slice left, it has priority and",
            "\t\t * fetching more just increases latency for the fetched tasks.",
            "\t\t * Tell pick_task_scx() to keep running @prev. If the BPF",
            "\t\t * scheduler wants to handle this explicitly, it should",
            "\t\t * implement ->cpu_release().",
            "\t\t *",
            "\t\t * See scx_ops_disable_workfn() for the explanation on the",
            "\t\t * bypassing test.",
            "\t\t */",
            "\t\tif ((prev->scx.flags & SCX_TASK_QUEUED) &&",
            "\t\t    prev->scx.slice && !scx_rq_bypassing(rq)) {",
            "\t\t\trq->scx.flags |= SCX_RQ_BAL_KEEP;",
            "\t\t\tgoto has_tasks;",
            "\t\t}",
            "\t}",
            "",
            "\t/* if there already are tasks to run, nothing to do */",
            "\tif (rq->scx.local_dsq.nr)",
            "\t\tgoto has_tasks;",
            "",
            "\tif (consume_global_dsq(rq))",
            "\t\tgoto has_tasks;",
            "",
            "\tif (!SCX_HAS_OP(dispatch) || scx_rq_bypassing(rq) || !scx_rq_online(rq))",
            "\t\tgoto no_tasks;",
            "",
            "\tdspc->rq = rq;",
            "",
            "\t/*",
            "\t * The dispatch loop. Because flush_dispatch_buf() may drop the rq lock,",
            "\t * the local DSQ might still end up empty after a successful",
            "\t * ops.dispatch(). If the local DSQ is empty even after ops.dispatch()",
            "\t * produced some tasks, retry. The BPF scheduler may depend on this",
            "\t * looping behavior to simplify its implementation.",
            "\t */",
            "\tdo {",
            "\t\tdspc->nr_tasks = 0;",
            "",
            "\t\tSCX_CALL_OP(SCX_KF_DISPATCH, dispatch, cpu_of(rq),",
            "\t\t\t    prev_on_scx ? prev : NULL);",
            "",
            "\t\tflush_dispatch_buf(rq);",
            "",
            "\t\tif (rq->scx.local_dsq.nr)",
            "\t\t\tgoto has_tasks;",
            "\t\tif (consume_global_dsq(rq))",
            "\t\t\tgoto has_tasks;",
            "",
            "\t\t/*",
            "\t\t * ops.dispatch() can trap us in this loop by repeatedly",
            "\t\t * dispatching ineligible tasks. Break out once in a while to",
            "\t\t * allow the watchdog to run. As IRQ can't be enabled in",
            "\t\t * balance(), we want to complete this scheduling cycle and then",
            "\t\t * start a new one. IOW, we want to call resched_curr() on the",
            "\t\t * next, most likely idle, task, not the current one. Use",
            "\t\t * scx_bpf_kick_cpu() for deferred kicking.",
            "\t\t */",
            "\t\tif (unlikely(!--nr_loops)) {",
            "\t\t\tscx_bpf_kick_cpu(cpu_of(rq), 0);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t} while (dspc->nr_tasks);",
            "",
            "no_tasks:",
            "\t/*",
            "\t * Didn't find another task to run. Keep running @prev unless",
            "\t * %SCX_OPS_ENQ_LAST is in effect.",
            "\t */",
            "\tif ((prev->scx.flags & SCX_TASK_QUEUED) &&",
            "\t    (!static_branch_unlikely(&scx_ops_enq_last) ||",
            "\t     scx_rq_bypassing(rq))) {",
            "\t\trq->scx.flags |= SCX_RQ_BAL_KEEP;",
            "\t\tgoto has_tasks;",
            "\t}",
            "\trq->scx.flags &= ~SCX_RQ_IN_BALANCE;",
            "\treturn false;",
            "",
            "has_tasks:",
            "\trq->scx.flags &= ~SCX_RQ_IN_BALANCE;",
            "\treturn true;",
            "}"
          ],
          "function_name": "finish_dispatch, flush_dispatch_buf, balance_one",
          "description": "调度平衡逻辑，balance_one通过调用BPF的dispatch接口获取任务，flush_dispatch_buf处理缓冲区任务分发，循环尝试直到找到可运行任务或达到最大循环次数",
          "similarity": 0.5704703330993652
        }
      ]
    },
    {
      "source_file": "kernel/irq/devres.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:52:25\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\devres.c`\n\n---\n\n# `irq/devres.c` 技术文档\n\n## 1. 文件概述\n\n`irq/devres.c` 是 Linux 内核中用于实现**设备资源管理（Device Resource Management, devres）感知的中断（IRQ）申请与释放机制**的核心文件。该文件封装了标准 IRQ 操作（如 `request_irq`、`free_irq` 等）为“可自动释放”的资源管理版本，确保在设备驱动卸载或设备移除时，已申请的中断资源能被自动、安全地释放，避免资源泄漏。\n\n该机制基于内核的 `devres`（Device Resource）框架，将 IRQ 资源与 `struct device` 生命周期绑定，极大简化了驱动开发中的资源管理逻辑。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct irq_devres`**  \n  用于跟踪通过 `devm_*` 接口申请的中断资源，包含：\n  - `irq`：中断号\n  - `dev_id`：传递给中断处理函数的设备标识（用于共享中断的区分）\n\n- **`struct irq_desc_devres`**  \n  用于跟踪通过 `__devm_irq_alloc_descs` 分配的中断描述符范围，包含：\n  - `from`：分配的起始中断号\n  - `cnt`：分配的中断数量\n\n- **`struct irq_generic_chip_devres`**  \n  用于跟踪通过 `devm_irq_setup_generic_chip` 设置的通用中断芯片资源，包含：\n  - `gc`：指向 `irq_chip_generic` 结构的指针\n  - `msk`、`clr`、`set`：用于在释放时还原中断状态的参数\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `devm_request_threaded_irq` | 为设备申请带线程化处理的中断，自动管理生命周期 |\n| `devm_request_any_context_irq` | 为设备申请可在任意上下文（硬中断或线程）处理的中断 |\n| `devm_free_irq` | 手动释放由 `devm_*` 接口申请的中断（通常不需要调用） |\n| `__devm_irq_alloc_descs` | 为设备分配并管理一组中断描述符（IRQ descriptors） |\n| `devm_irq_alloc_generic_chip` | 为设备分配并初始化一个通用中断芯片结构（`irq_chip_generic`） |\n| `devm_irq_setup_generic_chip` | 为设备设置通用中断芯片的中断范围，并注册资源释放回调 |\n\n## 3. 关键实现\n\n### 资源自动释放机制\n- 所有 `devm_*` 接口在成功申请资源后，会通过 `devres_alloc()` 分配一个资源描述结构（如 `irq_devres`），并注册对应的释放函数（如 `devm_irq_release`）。\n- 该资源结构通过 `devres_add()` 绑定到 `struct device`。\n- 当设备被移除（`device_del`）或驱动卸载时，内核自动调用所有注册的 `devres` 释放函数，确保 `free_irq()` 或 `irq_free_descs()` 被正确调用。\n\n### 中断匹配逻辑\n- `devm_free_irq()` 使用 `devm_irq_match` 函数通过 `irq` 和 `dev_id` 精确匹配要释放的资源，确保不会误删其他中断。\n\n### 通用中断芯片支持\n- `devm_irq_alloc_generic_chip` 使用 `devm_kzalloc` 分配内存，确保芯片结构随设备生命周期自动释放。\n- `devm_irq_setup_generic_chip` 在设置芯片后注册 `devm_irq_remove_generic_chip` 回调，在设备移除时自动调用 `irq_remove_generic_chip` 清理中断配置。\n\n### 错误处理\n- 所有分配操作（如 `devres_alloc`）失败时返回 `-ENOMEM`。\n- 底层 IRQ 申请失败时，会释放已分配的 `devres` 结构，避免内存泄漏。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/device.h>`：提供 `devres` 框架接口（`devres_alloc`, `devres_add`, `devres_destroy` 等）\n  - `<linux/interrupt.h>`：提供标准 IRQ 接口（`request_threaded_irq`, `free_irq` 等）\n  - `<linux/irq.h>`：提供中断描述符管理接口（`__irq_alloc_descs`, `irq_free_descs`）\n  - `\"internals.h\"`：包含 IRQ 子系统内部实现细节\n\n- **内核配置依赖**：\n  - `CONFIG_GENERIC_IRQ_CHIP`：启用通用中断芯片支持（影响 `devm_irq_alloc_generic_chip` 和 `devm_irq_setup_generic_chip` 的编译）\n\n- **模块导出**：\n  - `devm_request_threaded_irq`、`devm_request_any_context_irq`、`devm_free_irq` 通过 `EXPORT_SYMBOL` 导出，供其他模块使用。\n  - 中断描述符和通用芯片相关函数通过 `EXPORT_SYMBOL_GPL` 导出，仅限 GPL 兼容模块使用。\n\n## 5. 使用场景\n\n- **驱动开发**：设备驱动在 `probe` 函数中使用 `devm_request_threaded_irq()` 申请中断，无需在 `remove` 函数中显式调用 `free_irq()`，简化代码并避免遗漏。\n- **虚拟中断分配**：平台驱动或中断控制器驱动使用 `__devm_irq_alloc_descs()` 为虚拟设备分配中断号范围，确保在设备移除时自动释放描述符。\n- **通用中断控制器**：使用 `devm_irq_alloc_generic_chip()` 和 `devm_irq_setup_generic_chip()` 管理基于 `irq_chip_generic` 的中断控制器，适用于 GPIO、I2C、SPI 等子系统中的中断复用场景。\n- **资源安全释放**：在驱动异常退出或设备热插拔场景下，内核自动释放 IRQ 资源，防止中断悬挂或资源冲突。",
      "similarity": 0.6503486037254333,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/devres.c",
          "start_line": 18,
          "end_line": 122,
          "content": [
            "static void devm_irq_release(struct device *dev, void *res)",
            "{",
            "\tstruct irq_devres *this = res;",
            "",
            "\tfree_irq(this->irq, this->dev_id);",
            "}",
            "static int devm_irq_match(struct device *dev, void *res, void *data)",
            "{",
            "\tstruct irq_devres *this = res, *match = data;",
            "",
            "\treturn this->irq == match->irq && this->dev_id == match->dev_id;",
            "}",
            "int devm_request_threaded_irq(struct device *dev, unsigned int irq,",
            "\t\t\t      irq_handler_t handler, irq_handler_t thread_fn,",
            "\t\t\t      unsigned long irqflags, const char *devname,",
            "\t\t\t      void *dev_id)",
            "{",
            "\tstruct irq_devres *dr;",
            "\tint rc;",
            "",
            "\tdr = devres_alloc(devm_irq_release, sizeof(struct irq_devres),",
            "\t\t\t  GFP_KERNEL);",
            "\tif (!dr)",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (!devname)",
            "\t\tdevname = dev_name(dev);",
            "",
            "\trc = request_threaded_irq(irq, handler, thread_fn, irqflags, devname,",
            "\t\t\t\t  dev_id);",
            "\tif (rc) {",
            "\t\tdevres_free(dr);",
            "\t\treturn rc;",
            "\t}",
            "",
            "\tdr->irq = irq;",
            "\tdr->dev_id = dev_id;",
            "\tdevres_add(dev, dr);",
            "",
            "\treturn 0;",
            "}",
            "int devm_request_any_context_irq(struct device *dev, unsigned int irq,",
            "\t\t\t      irq_handler_t handler, unsigned long irqflags,",
            "\t\t\t      const char *devname, void *dev_id)",
            "{",
            "\tstruct irq_devres *dr;",
            "\tint rc;",
            "",
            "\tdr = devres_alloc(devm_irq_release, sizeof(struct irq_devres),",
            "\t\t\t  GFP_KERNEL);",
            "\tif (!dr)",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (!devname)",
            "\t\tdevname = dev_name(dev);",
            "",
            "\trc = request_any_context_irq(irq, handler, irqflags, devname, dev_id);",
            "\tif (rc < 0) {",
            "\t\tdevres_free(dr);",
            "\t\treturn rc;",
            "\t}",
            "",
            "\tdr->irq = irq;",
            "\tdr->dev_id = dev_id;",
            "\tdevres_add(dev, dr);",
            "",
            "\treturn rc;",
            "}",
            "void devm_free_irq(struct device *dev, unsigned int irq, void *dev_id)",
            "{",
            "\tstruct irq_devres match_data = { irq, dev_id };",
            "",
            "\tWARN_ON(devres_destroy(dev, devm_irq_release, devm_irq_match,",
            "\t\t\t       &match_data));",
            "\tfree_irq(irq, dev_id);",
            "}",
            "static void devm_irq_desc_release(struct device *dev, void *res)",
            "{",
            "\tstruct irq_desc_devres *this = res;",
            "",
            "\tirq_free_descs(this->from, this->cnt);",
            "}",
            "int __devm_irq_alloc_descs(struct device *dev, int irq, unsigned int from,",
            "\t\t\t   unsigned int cnt, int node, struct module *owner,",
            "\t\t\t   const struct irq_affinity_desc *affinity)",
            "{",
            "\tstruct irq_desc_devres *dr;",
            "\tint base;",
            "",
            "\tdr = devres_alloc(devm_irq_desc_release, sizeof(*dr), GFP_KERNEL);",
            "\tif (!dr)",
            "\t\treturn -ENOMEM;",
            "",
            "\tbase = __irq_alloc_descs(irq, from, cnt, node, owner, affinity);",
            "\tif (base < 0) {",
            "\t\tdevres_free(dr);",
            "\t\treturn base;",
            "\t}",
            "",
            "\tdr->from = base;",
            "\tdr->cnt = cnt;",
            "\tdevres_add(dev, dr);",
            "",
            "\treturn base;",
            "}"
          ],
          "function_name": "devm_irq_release, devm_irq_match, devm_request_threaded_irq, devm_request_any_context_irq, devm_free_irq, devm_irq_desc_release, __devm_irq_alloc_descs",
          "description": "实现设备资源管理的中断申请与释放逻辑，包含中断线程处理注册、任意上下文中断申请、中断释放及中断描述符分配等功能，通过devres框架实现资源自动管理。",
          "similarity": 0.6251298189163208
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq/devres.c",
          "start_line": 240,
          "end_line": 266,
          "content": [
            "static void devm_irq_remove_generic_chip(struct device *dev, void *res)",
            "{",
            "\tstruct irq_generic_chip_devres *this = res;",
            "",
            "\tirq_remove_generic_chip(this->gc, this->msk, this->clr, this->set);",
            "}",
            "int devm_irq_setup_generic_chip(struct device *dev, struct irq_chip_generic *gc,",
            "\t\t\t\tu32 msk, enum irq_gc_flags flags,",
            "\t\t\t\tunsigned int clr, unsigned int set)",
            "{",
            "\tstruct irq_generic_chip_devres *dr;",
            "",
            "\tdr = devres_alloc(devm_irq_remove_generic_chip,",
            "\t\t\t  sizeof(*dr), GFP_KERNEL);",
            "\tif (!dr)",
            "\t\treturn -ENOMEM;",
            "",
            "\tirq_setup_generic_chip(gc, msk, flags, clr, set);",
            "",
            "\tdr->gc = gc;",
            "\tdr->msk = msk;",
            "\tdr->clr = clr;",
            "\tdr->set = set;",
            "\tdevres_add(dev, dr);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "devm_irq_remove_generic_chip, devm_irq_setup_generic_chip",
          "description": "提供通用中断芯片的配置与清理接口，用于在设备初始化时设置中断控制器参数并在设备移除时安全地移除通用中断芯片配置。",
          "similarity": 0.5545509457588196
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/devres.c",
          "start_line": 1,
          "end_line": 17,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/device.h>",
            "#include <linux/gfp.h>",
            "#include <linux/irq.h>",
            "",
            "#include \"internals.h\"",
            "",
            "/*",
            " * Device resource management aware IRQ request/free implementation.",
            " */",
            "struct irq_devres {",
            "\tunsigned int irq;",
            "\tvoid *dev_id;",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义irq_devres结构体，用于存储中断号和设备ID，作为设备资源管理中断请求的辅助数据结构。",
          "similarity": 0.46091869473457336
        }
      ]
    }
  ]
}