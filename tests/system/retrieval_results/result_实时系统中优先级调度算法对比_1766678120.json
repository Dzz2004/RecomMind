{
  "query": "实时系统中优先级调度算法对比",
  "timestamp": "2025-12-25 23:55:20",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/cpupri.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:04:45\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\cpupri.c`\n\n---\n\n# `sched/cpupri.c` 技术文档\n\n## 1. 文件概述\n\n`sched/cpupri.c` 实现了 **CPU 优先级管理（CPU Priority Management）** 机制，用于实时任务（RT tasks）的全局负载均衡和迁移决策。该机制通过维护一个二维位图结构，快速追踪每个 CPU 当前运行任务的最高优先级，从而在 O(1) 时间复杂度内为新唤醒或迁移的实时任务找到合适的 CPU 目标。该机制特别优化了无 CPU 亲和性限制的任务调度路径，同时支持带亲和性约束的场景。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `convert_prio(int prio)` | 将任务的调度优先级（`p->prio`）转换为内部 `cpupri` 优先级值（范围：-1 到 100） |\n| `__cpupri_find(struct cpupri *cp, struct task_struct *p, struct cpumask *lowest_mask, int idx)` | 在指定优先级层级 `idx` 中查找满足任务 `p` 的 CPU（考虑亲和性） |\n| `cpupri_find(struct cpupri *cp, struct task_struct *p, struct cpumask *lowest_mask)` | 查找系统中优先级 **低于或等于** 任务 `p` 的 CPU（即任务可运行的 CPU） |\n| `cpupri_find_fitness(...)` | 增强版查找函数，支持通过 `fitness_fn` 自定义 CPU 适配条件（如容量感知） |\n| `cpupri_set(struct cpupri *cp, int cpu, int newpri)` | 更新指定 CPU 的当前最高优先级状态 |\n| `cpupri_init(struct cpupri *cp)` | 初始化 `cpupri` 数据结构（声明但未在片段中实现） |\n\n### 关键数据结构（隐含）\n\n- `struct cpupri`：全局 CPU 优先级管理上下文\n  - `cpu_to_pri[]`：每个 CPU 当前的 `cpupri` 优先级\n  - `pri_to_cpu[]`：每个优先级对应的 `struct cpupri_vec`\n- `struct cpupri_vec`：\n  - `mask`：该优先级下所有 CPU 的位图\n  - `count`：该优先级下活跃 CPU 的数量（原子计数）\n\n### 优先级映射关系\n\n| 任务 `p->prio` | `cpupri` 值 | 含义 |\n|---------------|------------|------|\n| -1 | -1 (`CPUPRI_INVALID`) | 无效状态（CPU 不可调度） |\n| 0–98 | 99–1 | 实时优先级（数值越大，任务优先级越高，`cpupri` 值越小） |\n| 99 (`MAX_RT_PRIO-1`) | 0 (`CPUPRI_NORMAL`) | 普通（非实时）任务 |\n| 100 (`MAX_RT_PRIO`) | 100 (`CPUPRI_HIGHER`) | 高于所有 RT 任务的特殊优先级 |\n\n> **注意**：`cpupri` 值越小，表示 CPU 当前负载的优先级 **越高**。\n\n## 3. 关键实现\n\n### 优先级转换逻辑\n- `convert_prio()` 实现了任务调度优先级到 `cpupri` 内部表示的映射，确保实时任务（`p->prio` ∈ [0, 98]）被正确映射到 `cpupri` ∈ [1, 99]，且高优先级任务对应更小的 `cpupri` 值。\n\n### 快速查找算法\n- 使用 **二维位图**：第一维为优先级（0–100），第二维为 CPU 位图。\n- `cpupri_find_fitness()` 从最低优先级（`idx = 0`）开始遍历，找到第一个存在可用 CPU 的优先级层级。\n- 对于每个层级，通过 `cpumask_any_and()` 快速判断任务亲和性掩码与该优先级 CPU 掩码是否有交集。\n- 若提供 `fitness_fn`（如容量检查），会过滤掉不满足条件的 CPU；若过滤后无 CPU 可用，则继续搜索更高优先级层级。\n\n### 容错与回退策略\n- 如果启用了 `fitness_fn` 但未找到满足条件的 CPU，函数会 **忽略 fitness 条件重新搜索**，确保高优先级任务总能找到运行 CPU（优先保证实时性，而非最优资源匹配）。\n\n### 并发安全更新\n- `cpupri_set()` 使用 **内存屏障（`smp_mb__before/after_atomic()`）** 确保 CPU 位图和计数器的更新顺序：\n  1. **添加 CPU**：先设置位图 → 内存屏障 → 增加计数器\n  2. **移除 CPU**：先减少计数器 → 内存屏障 → 清除位图\n- 此顺序防止 `cpupri_find` 在并发读取时看到不一致状态（如计数器为 0 但位图仍置位）。\n\n### 亲和性处理\n- 所有查找操作均与任务的 `p->cpus_mask`（CPU 亲和性）和 `cpu_active_mask`（活跃 CPU）进行交集运算，确保只返回合法 CPU。\n\n## 4. 依赖关系\n\n- **调度器核心**：依赖 `task_struct`、`p->prio`、`p->cpus_mask` 等调度器基本结构。\n- **实时调度类（`rt.c`）**：`cpupri` 主要服务于 `SCHED_FIFO`/`SCHED_RR` 任务的负载均衡。\n- **CPU 掩码操作**：使用 `cpumask_*` 系列函数（如 `cpumask_and`, `cpumask_clear_cpu`）。\n- **内存屏障原语**：依赖 `smp_rmb()`、`smp_mb__before_atomic()` 等 SMP 同步机制。\n- **原子操作**：使用 `atomic_read/inc/dec` 管理优先级层级的 CPU 计数。\n\n## 5. 使用场景\n\n- **实时任务唤醒/迁移**：当高优先级 RT 任务被唤醒或需要迁移时，调用 `cpupri_find()` 快速定位可运行的最低优先级 CPU（减少抢占开销）。\n- **全局负载均衡**：RT 调度器的 `push_rt_task()` 和 `pull_rt_task()` 机制利用 `cpupri` 决定任务推送/拉取的目标 CPU。\n- **容量感知调度（Capacity Awareness）**：通过 `cpupri_find_fitness()` 的 `fitness_fn` 参数，集成 CPU 性能/能效信息（如 ARM big.LITTLE 架构），在满足优先级前提下选择合适 CPU。\n- **CPU 热插拔**：CPU 上下线时通过 `cpupri_set()` 更新其优先级状态（设为 `CPUPRI_INVALID` 或恢复）。",
      "similarity": 0.605638861656189,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 42,
          "end_line": 178,
          "content": [
            "static int convert_prio(int prio)",
            "{",
            "\tint cpupri;",
            "",
            "\tswitch (prio) {",
            "\tcase CPUPRI_INVALID:",
            "\t\tcpupri = CPUPRI_INVALID;\t/* -1 */",
            "\t\tbreak;",
            "",
            "\tcase 0 ... 98:",
            "\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */",
            "\t\tbreak;",
            "",
            "\tcase MAX_RT_PRIO-1:",
            "\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */",
            "\t\tbreak;",
            "",
            "\tcase MAX_RT_PRIO:",
            "\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn cpupri;",
            "}",
            "static inline int __cpupri_find(struct cpupri *cp, struct task_struct *p,",
            "\t\t\t\tstruct cpumask *lowest_mask, int idx)",
            "{",
            "\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[idx];",
            "\tint skip = 0;",
            "",
            "\tif (!atomic_read(&(vec)->count))",
            "\t\tskip = 1;",
            "\t/*",
            "\t * When looking at the vector, we need to read the counter,",
            "\t * do a memory barrier, then read the mask.",
            "\t *",
            "\t * Note: This is still all racy, but we can deal with it.",
            "\t *  Ideally, we only want to look at masks that are set.",
            "\t *",
            "\t *  If a mask is not set, then the only thing wrong is that we",
            "\t *  did a little more work than necessary.",
            "\t *",
            "\t *  If we read a zero count but the mask is set, because of the",
            "\t *  memory barriers, that can only happen when the highest prio",
            "\t *  task for a run queue has left the run queue, in which case,",
            "\t *  it will be followed by a pull. If the task we are processing",
            "\t *  fails to find a proper place to go, that pull request will",
            "\t *  pull this task if the run queue is running at a lower",
            "\t *  priority.",
            "\t */",
            "\tsmp_rmb();",
            "",
            "\t/* Need to do the rmb for every iteration */",
            "\tif (skip)",
            "\t\treturn 0;",
            "",
            "\tif (cpumask_any_and(&p->cpus_mask, vec->mask) >= nr_cpu_ids)",
            "\t\treturn 0;",
            "",
            "\tif (lowest_mask) {",
            "\t\tcpumask_and(lowest_mask, &p->cpus_mask, vec->mask);",
            "\t\tcpumask_and(lowest_mask, lowest_mask, cpu_active_mask);",
            "",
            "\t\t/*",
            "\t\t * We have to ensure that we have at least one bit",
            "\t\t * still set in the array, since the map could have",
            "\t\t * been concurrently emptied between the first and",
            "\t\t * second reads of vec->mask.  If we hit this",
            "\t\t * condition, simply act as though we never hit this",
            "\t\t * priority level and continue on.",
            "\t\t */",
            "\t\tif (cpumask_empty(lowest_mask))",
            "\t\t\treturn 0;",
            "\t}",
            "",
            "\treturn 1;",
            "}",
            "int cpupri_find(struct cpupri *cp, struct task_struct *p,",
            "\t\tstruct cpumask *lowest_mask)",
            "{",
            "\treturn cpupri_find_fitness(cp, p, lowest_mask, NULL);",
            "}",
            "int cpupri_find_fitness(struct cpupri *cp, struct task_struct *p,",
            "\t\tstruct cpumask *lowest_mask,",
            "\t\tbool (*fitness_fn)(struct task_struct *p, int cpu))",
            "{",
            "\tint task_pri = convert_prio(p->prio);",
            "\tint idx, cpu;",
            "",
            "\tWARN_ON_ONCE(task_pri >= CPUPRI_NR_PRIORITIES);",
            "",
            "\tfor (idx = 0; idx < task_pri; idx++) {",
            "",
            "\t\tif (!__cpupri_find(cp, p, lowest_mask, idx))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!lowest_mask || !fitness_fn)",
            "\t\t\treturn 1;",
            "",
            "\t\t/* Ensure the capacity of the CPUs fit the task */",
            "\t\tfor_each_cpu(cpu, lowest_mask) {",
            "\t\t\tif (!fitness_fn(p, cpu))",
            "\t\t\t\tcpumask_clear_cpu(cpu, lowest_mask);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If no CPU at the current priority can fit the task",
            "\t\t * continue looking",
            "\t\t */",
            "\t\tif (cpumask_empty(lowest_mask))",
            "\t\t\tcontinue;",
            "",
            "\t\treturn 1;",
            "\t}",
            "",
            "\t/*",
            "\t * If we failed to find a fitting lowest_mask, kick off a new search",
            "\t * but without taking into account any fitness criteria this time.",
            "\t *",
            "\t * This rule favours honouring priority over fitting the task in the",
            "\t * correct CPU (Capacity Awareness being the only user now).",
            "\t * The idea is that if a higher priority task can run, then it should",
            "\t * run even if this ends up being on unfitting CPU.",
            "\t *",
            "\t * The cost of this trade-off is not entirely clear and will probably",
            "\t * be good for some workloads and bad for others.",
            "\t *",
            "\t * The main idea here is that if some CPUs were over-committed, we try",
            "\t * to spread which is what the scheduler traditionally did. Sys admins",
            "\t * must do proper RT planning to avoid overloading the system if they",
            "\t * really care.",
            "\t */",
            "\tif (fitness_fn)",
            "\t\treturn cpupri_find(cp, p, lowest_mask);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "convert_prio, __cpupri_find, cpupri_find, cpupri_find_fitness",
          "description": "convert_prio将任务优先级映射为CPU优先级数值；__cpupri_find检查特定优先级下是否存在可用CPU；cpupri_find_fitness遍历优先级层级寻找适配CPU，结合适应性判断与优先级策略决定最终选择",
          "similarity": 0.6087123155593872
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 1,
          "end_line": 41,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  kernel/sched/cpupri.c",
            " *",
            " *  CPU priority management",
            " *",
            " *  Copyright (C) 2007-2008 Novell",
            " *",
            " *  Author: Gregory Haskins <ghaskins@novell.com>",
            " *",
            " *  This code tracks the priority of each CPU so that global migration",
            " *  decisions are easy to calculate.  Each CPU can be in a state as follows:",
            " *",
            " *                 (INVALID), NORMAL, RT1, ... RT99, HIGHER",
            " *",
            " *  going from the lowest priority to the highest.  CPUs in the INVALID state",
            " *  are not eligible for routing.  The system maintains this state with",
            " *  a 2 dimensional bitmap (the first for priority class, the second for CPUs",
            " *  in that class).  Therefore a typical application without affinity",
            " *  restrictions can find a suitable CPU with O(1) complexity (e.g. two bit",
            " *  searches).  For tasks with affinity restrictions, the algorithm has a",
            " *  worst case complexity of O(min(101, nr_domcpus)), though the scenario that",
            " *  yields the worst case search is fairly contrived.",
            " */",
            "",
            "/*",
            " * p->rt_priority   p->prio   newpri   cpupri",
            " *",
            " *\t\t\t\t  -1       -1 (CPUPRI_INVALID)",
            " *",
            " *\t\t\t\t  99        0 (CPUPRI_NORMAL)",
            " *",
            " *\t\t1        98       98        1",
            " *\t      ...",
            " *\t       49        50       50       49",
            " *\t       50        49       49       50",
            " *\t      ...",
            " *\t       99         0        0       99",
            " *",
            " *\t\t\t\t 100\t  100 (CPUPRI_HIGHER)",
            " */"
          ],
          "function_name": null,
          "description": "定义CPU优先级管理模块，通过二维位图跟踪各CPU优先级状态，支持NORMAL、RT1至RT99及HIGHER五种优先级分类，INVALID状态表示CPU不可用，用于全局任务调度时快速计算迁移决策",
          "similarity": 0.5838420391082764
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 210,
          "end_line": 304,
          "content": [
            "void cpupri_set(struct cpupri *cp, int cpu, int newpri)",
            "{",
            "\tint *currpri = &cp->cpu_to_pri[cpu];",
            "\tint oldpri = *currpri;",
            "\tint do_mb = 0;",
            "",
            "\tnewpri = convert_prio(newpri);",
            "",
            "\tBUG_ON(newpri >= CPUPRI_NR_PRIORITIES);",
            "",
            "\tif (newpri == oldpri)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If the CPU was currently mapped to a different value, we",
            "\t * need to map it to the new value then remove the old value.",
            "\t * Note, we must add the new value first, otherwise we risk the",
            "\t * cpu being missed by the priority loop in cpupri_find.",
            "\t */",
            "\tif (likely(newpri != CPUPRI_INVALID)) {",
            "\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[newpri];",
            "",
            "\t\tcpumask_set_cpu(cpu, vec->mask);",
            "\t\t/*",
            "\t\t * When adding a new vector, we update the mask first,",
            "\t\t * do a write memory barrier, and then update the count, to",
            "\t\t * make sure the vector is visible when count is set.",
            "\t\t */",
            "\t\tsmp_mb__before_atomic();",
            "\t\tatomic_inc(&(vec)->count);",
            "\t\tdo_mb = 1;",
            "\t}",
            "\tif (likely(oldpri != CPUPRI_INVALID)) {",
            "\t\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[oldpri];",
            "",
            "\t\t/*",
            "\t\t * Because the order of modification of the vec->count",
            "\t\t * is important, we must make sure that the update",
            "\t\t * of the new prio is seen before we decrement the",
            "\t\t * old prio. This makes sure that the loop sees",
            "\t\t * one or the other when we raise the priority of",
            "\t\t * the run queue. We don't care about when we lower the",
            "\t\t * priority, as that will trigger an rt pull anyway.",
            "\t\t *",
            "\t\t * We only need to do a memory barrier if we updated",
            "\t\t * the new priority vec.",
            "\t\t */",
            "\t\tif (do_mb)",
            "\t\t\tsmp_mb__after_atomic();",
            "",
            "\t\t/*",
            "\t\t * When removing from the vector, we decrement the counter first",
            "\t\t * do a memory barrier and then clear the mask.",
            "\t\t */",
            "\t\tatomic_dec(&(vec)->count);",
            "\t\tsmp_mb__after_atomic();",
            "\t\tcpumask_clear_cpu(cpu, vec->mask);",
            "\t}",
            "",
            "\t*currpri = newpri;",
            "}",
            "int cpupri_init(struct cpupri *cp)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++) {",
            "\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[i];",
            "",
            "\t\tatomic_set(&vec->count, 0);",
            "\t\tif (!zalloc_cpumask_var(&vec->mask, GFP_KERNEL))",
            "\t\t\tgoto cleanup;",
            "\t}",
            "",
            "\tcp->cpu_to_pri = kcalloc(nr_cpu_ids, sizeof(int), GFP_KERNEL);",
            "\tif (!cp->cpu_to_pri)",
            "\t\tgoto cleanup;",
            "",
            "\tfor_each_possible_cpu(i)",
            "\t\tcp->cpu_to_pri[i] = CPUPRI_INVALID;",
            "",
            "\treturn 0;",
            "",
            "cleanup:",
            "\tfor (i--; i >= 0; i--)",
            "\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);",
            "\treturn -ENOMEM;",
            "}",
            "void cpupri_cleanup(struct cpupri *cp)",
            "{",
            "\tint i;",
            "",
            "\tkfree(cp->cpu_to_pri);",
            "\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++)",
            "\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);",
            "}"
          ],
          "function_name": "cpupri_set, cpupri_init, cpupri_cleanup",
          "description": "cpupri_set更新CPU优先级状态，通过原子操作同步位图与计数器；cpupri_init初始化优先级到CPU的映射表与CPU到优先级的数组；cpupri_cleanup释放所有动态分配的位图资源与优先级数组",
          "similarity": 0.5564659833908081
        }
      ]
    },
    {
      "source_file": "kernel/sched/syscalls.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:19:13\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\syscalls.c`\n\n---\n\n# `sched/syscalls.c` 技术文档\n\n## 1. 文件概述\n\n`sched/syscalls.c` 是 Linux 内核调度子系统的核心源文件之一，主要负责实现与调度相关的系统调用接口和优先级管理逻辑。该文件封装了任务优先级计算、nice 值设置、CPU 空闲状态判断等关键功能，为用户空间提供 `nice()` 等系统调用的内核支持，并为调度器内部模块提供优先级操作原语。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `__normal_prio()`：根据调度策略（SCHED_NORMAL/SCHED_BATCH/SCHED_IDLE、SCHED_FIFO/SCHED_RR、SCHED_DEADLINE）计算任务的“正常”优先级。\n- `normal_prio()`：基于任务当前策略、实时优先级和静态 nice 值计算其正常优先级。\n- `effective_prio()`：计算任务当前实际生效的调度优先级，考虑 RT 继承或提升。\n- `set_user_nice()`：安全地修改指定任务的 nice 值，更新其静态优先级和调度权重，并触发调度器重评估。\n- `is_nice_reduction()` / `can_nice()`：检查任务是否具备降低 nice 值（即提高优先级）的权限。\n- `sys_nice()`：实现 `nice(2)` 系统调用，允许当前进程调整自身优先级。\n- `task_prio()`：返回任务在 `/proc` 中对外暴露的用户可见优先级值。\n- `idle_cpu()` / `available_idle_cpu()`：判断指定 CPU 是否处于空闲状态。\n- `idle_task()`：获取指定 CPU 的 idle 任务结构体。\n- `update_other_load_avgs()`（SMP）：更新除 CFS 外其他调度类（RT、DL、IRQ）的负载平均值。\n- `effective_cpu_util()`（SMP）：计算 CPU 的有效利用率，用于频率调节（如 CPUFreq）。\n\n### 关键数据结构\n- 无独立定义的数据结构，主要操作 `struct task_struct` 和 `struct rq`（运行队列）。\n\n## 3. 关键实现\n\n### 优先级计算模型\n- **优先级映射**：\n  - 用户态 nice 值范围 `[-20, 19]` 映射到内核静态优先级 `[100, 139]`（通过 `NICE_TO_PRIO`）。\n  - 实时任务（RT/DL）使用 `[0, 99]` 的高优先级范围（`MAX_RT_PRIO = 100`）。\n  - `task_prio()` 返回值将内核优先级转换为用户可见格式：普通任务为 `[0,39]`，RT 任务为 `[-2,-100]`，DL 任务为 `-101`。\n- **有效优先级**：`effective_prio()` 区分“正常优先级”与“被提升的优先级”。若任务当前优先级为 RT/DL（即 `rt_or_dl_prio(p->prio)` 为真），则保留提升后的值；否则使用 `normal_prio`。\n\n### Nice 值修改安全机制\n- `set_user_nice()` 在修改 nice 值前：\n  1. 获取任务所在 CPU 的运行队列锁（`task_rq_lock`），防止并发调度。\n  2. 对 RT/DL 任务仅更新 `static_prio`（不影响调度行为）。\n  3. 对普通任务，先从运行队列中移除（若已入队或正在运行），更新 `static_prio` 和负载权重（`set_load_weight`），重新计算 `prio`，再重新入队。\n  4. 调用调度类的 `prio_changed` 回调，通知调度器优先级变更。\n\n### 权限控制\n- `can_nice()` 结合资源限制（`RLIMIT_NICE`）和特权（`CAP_SYS_NICE`）判断是否允许降低 nice 值（提高优先级）。\n- `nice_to_rlimit()` 将 nice 值 `[19,-20]` 转换为 rlimit 格式 `[1,40]` 以匹配 `RLIMIT_NICE` 的语义。\n\n### CPU 空闲判断\n- `idle_cpu()` 检查：\n  - 当前运行任务是否为 idle 任务。\n  - 运行队列中无其他可运行任务（`nr_running == 0`）。\n  - （SMP）无待处理的远程唤醒（`ttwu_pending == 0`）。\n- `available_idle_cpu()` 额外检查虚拟化场景下 CPU 是否被抢占（`vcpu_is_preempted`）。\n\n### 负载与利用率计算（SMP）\n- `update_other_load_avgs()` 周期性更新 RT、DL、IRQ 和硬件压力的负载平均值。\n- `effective_cpu_util()` 聚合 CFS、RT、DL、IRQ 的利用率，并考虑 DL 带宽预留，输出用于 CPU 频率调节的有效利用率。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/sched.h>`：核心调度数据结构和 API。\n  - `<linux/cpuset.h>`：CPU 亲和性相关（间接影响调度）。\n  - `\"sched.h\"`（本地）：调度器内部实现细节。\n  - `\"autogroup.h\"`：自动任务分组支持。\n- **调度类依赖**：\n  - 调用各调度类（CFS、RT、DL）的回调函数（如 `prio_changed`、`enqueue_task` 等）。\n- **安全模块**：调用 LSM 钩子 `security_task_setnice()`。\n- **架构相关**：\n  - `arch_scale_cpu_capacity()` / `arch_scale_hw_pressure()`：架构特定的 CPU 容量和硬件压力缩放。\n  - `__ARCH_WANT_SYS_NICE`：控制 `sys_nice` 是否编译进内核。\n\n## 5. 使用场景\n\n- **系统调用处理**：为 `nice(2)` 系统调用提供内核实现，允许用户进程动态调整自身优先级。\n- **调度器内部操作**：\n  - 在 `fork()`、`sched_setscheduler()` 等操作中计算任务优先级。\n  - 调度类在任务入队/出队时更新优先级和负载。\n- **资源监控与管理**：\n  - `/proc/[pid]/stat` 中的优先级字段通过 `task_prio()` 获取。\n  - 负载均衡器和 CPUFreq 驱动使用 `effective_cpu_util()` 获取 CPU 利用率。\n- **空闲检测**：\n  - 负载均衡、任务迁移、节能策略（如 cpuidle）依赖 `idle_cpu()` 和 `available_idle_cpu()` 判断 CPU 状态。\n- **权限控制**：在设置优先级时执行安全检查，防止非特权进程提升调度优先级。",
      "similarity": 0.5861879587173462,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/syscalls.c",
          "start_line": 19,
          "end_line": 130,
          "content": [
            "static inline int __normal_prio(int policy, int rt_prio, int nice)",
            "{",
            "\tint prio;",
            "",
            "\tif (dl_policy(policy))",
            "\t\tprio = MAX_DL_PRIO - 1;",
            "\telse if (rt_policy(policy))",
            "\t\tprio = MAX_RT_PRIO - 1 - rt_prio;",
            "\telse",
            "\t\tprio = NICE_TO_PRIO(nice);",
            "",
            "\treturn prio;",
            "}",
            "static inline int normal_prio(struct task_struct *p)",
            "{",
            "\treturn __normal_prio(p->policy, p->rt_priority, PRIO_TO_NICE(p->static_prio));",
            "}",
            "static int effective_prio(struct task_struct *p)",
            "{",
            "\tp->normal_prio = normal_prio(p);",
            "\t/*",
            "\t * If we are RT tasks or we were boosted to RT priority,",
            "\t * keep the priority unchanged. Otherwise, update priority",
            "\t * to the normal priority:",
            "\t */",
            "\tif (!rt_or_dl_prio(p->prio))",
            "\t\treturn p->normal_prio;",
            "\treturn p->prio;",
            "}",
            "void set_user_nice(struct task_struct *p, long nice)",
            "{",
            "\tbool queued, running;",
            "\tstruct rq *rq;",
            "\tint old_prio;",
            "",
            "\tif (task_nice(p) == nice || nice < MIN_NICE || nice > MAX_NICE)",
            "\t\treturn;",
            "\t/*",
            "\t * We have to be careful, if called from sys_setpriority(),",
            "\t * the task might be in the middle of scheduling on another CPU.",
            "\t */",
            "\tCLASS(task_rq_lock, rq_guard)(p);",
            "\trq = rq_guard.rq;",
            "",
            "\tupdate_rq_clock(rq);",
            "",
            "\t/*",
            "\t * The RT priorities are set via sched_setscheduler(), but we still",
            "\t * allow the 'normal' nice value to be set - but as expected",
            "\t * it won't have any effect on scheduling until the task is",
            "\t * SCHED_DEADLINE, SCHED_FIFO or SCHED_RR:",
            "\t */",
            "\tif (task_has_dl_policy(p) || task_has_rt_policy(p)) {",
            "\t\tp->static_prio = NICE_TO_PRIO(nice);",
            "\t\treturn;",
            "\t}",
            "",
            "\tqueued = task_on_rq_queued(p);",
            "\trunning = task_current(rq, p);",
            "\tif (queued)",
            "\t\tdequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);",
            "\tif (running)",
            "\t\tput_prev_task(rq, p);",
            "",
            "\tp->static_prio = NICE_TO_PRIO(nice);",
            "\tset_load_weight(p, true);",
            "\told_prio = p->prio;",
            "\tp->prio = effective_prio(p);",
            "",
            "\tif (queued)",
            "\t\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);",
            "\tif (running)",
            "\t\tset_next_task(rq, p);",
            "",
            "\t/*",
            "\t * If the task increased its priority or is running and",
            "\t * lowered its priority, then reschedule its CPU:",
            "\t */",
            "\tp->sched_class->prio_changed(rq, p, old_prio);",
            "}",
            "static bool is_nice_reduction(const struct task_struct *p, const int nice)",
            "{",
            "\t/* Convert nice value [19,-20] to rlimit style value [1,40]: */",
            "\tint nice_rlim = nice_to_rlimit(nice);",
            "",
            "\treturn (nice_rlim <= task_rlimit(p, RLIMIT_NICE));",
            "}",
            "int can_nice(const struct task_struct *p, const int nice)",
            "{",
            "\treturn is_nice_reduction(p, nice) || capable(CAP_SYS_NICE);",
            "}",
            "int task_prio(const struct task_struct *p)",
            "{",
            "\treturn p->prio - MAX_RT_PRIO;",
            "}",
            "int idle_cpu(int cpu)",
            "{",
            "\tstruct rq *rq = cpu_rq(cpu);",
            "",
            "\tif (rq->curr != rq->idle)",
            "\t\treturn 0;",
            "",
            "\tif (rq->nr_running)",
            "\t\treturn 0;",
            "",
            "#ifdef CONFIG_SMP",
            "\tif (rq->ttwu_pending)",
            "\t\treturn 0;",
            "#endif",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "__normal_prio, normal_prio, effective_prio, set_user_nice, is_nice_reduction, can_nice, task_prio, idle_cpu",
          "description": "实现优先级计算与调整逻辑，包含正常优先级计算、有效优先级判定、用户nice值修改、优先级变化检测及实时任务优先级处理等功能",
          "similarity": 0.6391683220863342
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/syscalls.c",
          "start_line": 531,
          "end_line": 634,
          "content": [
            "static void __setscheduler_uclamp(struct task_struct *p,",
            "\t\t\t\t  const struct sched_attr *attr)",
            "{",
            "\tenum uclamp_id clamp_id;",
            "",
            "\tfor_each_clamp_id(clamp_id) {",
            "\t\tstruct uclamp_se *uc_se = &p->uclamp_req[clamp_id];",
            "\t\tunsigned int value;",
            "",
            "\t\tif (!uclamp_reset(attr, clamp_id, uc_se))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * RT by default have a 100% boost value that could be modified",
            "\t\t * at runtime.",
            "\t\t */",
            "\t\tif (unlikely(rt_task(p) && clamp_id == UCLAMP_MIN))",
            "\t\t\tvalue = sysctl_sched_uclamp_util_min_rt_default;",
            "\t\telse",
            "\t\t\tvalue = uclamp_none(clamp_id);",
            "",
            "\t\tuclamp_se_set(uc_se, value, false);",
            "",
            "\t}",
            "",
            "\tif (likely(!(attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)))",
            "\t\treturn;",
            "",
            "\tif (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN &&",
            "\t    attr->sched_util_min != -1) {",
            "\t\tuclamp_se_set(&p->uclamp_req[UCLAMP_MIN],",
            "\t\t\t      attr->sched_util_min, true);",
            "\t}",
            "",
            "\tif (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX &&",
            "\t    attr->sched_util_max != -1) {",
            "\t\tuclamp_se_set(&p->uclamp_req[UCLAMP_MAX],",
            "\t\t\t      attr->sched_util_max, true);",
            "\t}",
            "}",
            "static inline int uclamp_validate(struct task_struct *p,",
            "\t\t\t\t  const struct sched_attr *attr)",
            "{",
            "\treturn -EOPNOTSUPP;",
            "}",
            "static void __setscheduler_uclamp(struct task_struct *p,",
            "\t\t\t\t  const struct sched_attr *attr) { }",
            "static int user_check_sched_setscheduler(struct task_struct *p,",
            "\t\t\t\t\t const struct sched_attr *attr,",
            "\t\t\t\t\t int policy, int reset_on_fork)",
            "{",
            "\tif (fair_policy(policy)) {",
            "\t\tif (attr->sched_nice < task_nice(p) &&",
            "\t\t    !is_nice_reduction(p, attr->sched_nice))",
            "\t\t\tgoto req_priv;",
            "\t}",
            "",
            "\tif (rt_policy(policy)) {",
            "\t\tunsigned long rlim_rtprio = task_rlimit(p, RLIMIT_RTPRIO);",
            "",
            "\t\t/* Can't set/change the rt policy: */",
            "\t\tif (policy != p->policy && !rlim_rtprio)",
            "\t\t\tgoto req_priv;",
            "",
            "\t\t/* Can't increase priority: */",
            "\t\tif (attr->sched_priority > p->rt_priority &&",
            "\t\t    attr->sched_priority > rlim_rtprio)",
            "\t\t\tgoto req_priv;",
            "\t}",
            "",
            "\t/*",
            "\t * Can't set/change SCHED_DEADLINE policy at all for now",
            "\t * (safest behavior); in the future we would like to allow",
            "\t * unprivileged DL tasks to increase their relative deadline",
            "\t * or reduce their runtime (both ways reducing utilization)",
            "\t */",
            "\tif (dl_policy(policy))",
            "\t\tgoto req_priv;",
            "",
            "\t/*",
            "\t * Treat SCHED_IDLE as nice 20. Only allow a switch to",
            "\t * SCHED_NORMAL if the RLIMIT_NICE would normally permit it.",
            "\t */",
            "\tif (task_has_idle_policy(p) && !idle_policy(policy)) {",
            "\t\tif (!is_nice_reduction(p, task_nice(p)))",
            "\t\t\tgoto req_priv;",
            "\t}",
            "",
            "\t/* Can't change other user's priorities: */",
            "\tif (!check_same_owner(p))",
            "\t\tgoto req_priv;",
            "",
            "\t/* Normal users shall not reset the sched_reset_on_fork flag: */",
            "\tif (p->sched_reset_on_fork && !reset_on_fork)",
            "\t\tgoto req_priv;",
            "",
            "\treturn 0;",
            "",
            "req_priv:",
            "\tif (!capable(CAP_SYS_NICE))",
            "\t\treturn -EPERM;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "__setscheduler_uclamp, uclamp_validate, __setscheduler_uclamp, user_check_sched_setscheduler",
          "description": "管理uclamp参数设置与验证，实现调度策略变更权限检查，包含实时任务优先级限制、SCHED_DEADLINE策略限制及跨用户优先级修改控制",
          "similarity": 0.5473164319992065
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/sched/syscalls.c",
          "start_line": 1065,
          "end_line": 1186,
          "content": [
            "static void get_params(struct task_struct *p, struct sched_attr *attr)",
            "{",
            "\tif (task_has_dl_policy(p)) {",
            "\t\t__getparam_dl(p, attr);",
            "\t} else if (task_has_rt_policy(p)) {",
            "\t\tattr->sched_priority = p->rt_priority;",
            "\t} else {",
            "\t\tattr->sched_nice = task_nice(p);",
            "\t\tattr->sched_runtime = p->se.slice;",
            "\t}",
            "}",
            "static int",
            "sched_attr_copy_to_user(struct sched_attr __user *uattr,",
            "\t\t\tstruct sched_attr *kattr,",
            "\t\t\tunsigned int usize)",
            "{",
            "\tunsigned int ksize = sizeof(*kattr);",
            "",
            "\tif (!access_ok(uattr, usize))",
            "\t\treturn -EFAULT;",
            "",
            "\t/*",
            "\t * sched_getattr() ABI forwards and backwards compatibility:",
            "\t *",
            "\t * If usize == ksize then we just copy everything to user-space and all is good.",
            "\t *",
            "\t * If usize < ksize then we only copy as much as user-space has space for,",
            "\t * this keeps ABI compatibility as well. We skip the rest.",
            "\t *",
            "\t * If usize > ksize then user-space is using a newer version of the ABI,",
            "\t * which part the kernel doesn't know about. Just ignore it - tooling can",
            "\t * detect the kernel's knowledge of attributes from the attr->size value",
            "\t * which is set to ksize in this case.",
            "\t */",
            "\tkattr->size = min(usize, ksize);",
            "",
            "\tif (copy_to_user(uattr, kattr, kattr->size))",
            "\t\treturn -EFAULT;",
            "",
            "\treturn 0;",
            "}",
            "int dl_task_check_affinity(struct task_struct *p, const struct cpumask *mask)",
            "{",
            "\t/*",
            "\t * If the task isn't a deadline task or admission control is",
            "\t * disabled then we don't care about affinity changes.",
            "\t */",
            "\tif (!task_has_dl_policy(p) || !dl_bandwidth_enabled())",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Since bandwidth control happens on root_domain basis,",
            "\t * if admission test is enabled, we only admit -deadline",
            "\t * tasks allowed to run on all the CPUs in the task's",
            "\t * root_domain.",
            "\t */",
            "\tguard(rcu)();",
            "\tif (!cpumask_subset(task_rq(p)->rd->span, mask))",
            "\t\treturn -EBUSY;",
            "",
            "\treturn 0;",
            "}",
            "int __sched_setaffinity(struct task_struct *p, struct affinity_context *ctx)",
            "{",
            "\tint retval;",
            "\tcpumask_var_t cpus_allowed, new_mask;",
            "",
            "\tif (!alloc_cpumask_var(&cpus_allowed, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL)) {",
            "\t\tretval = -ENOMEM;",
            "\t\tgoto out_free_cpus_allowed;",
            "\t}",
            "",
            "\tcpuset_cpus_allowed(p, cpus_allowed);",
            "\tcpumask_and(new_mask, ctx->new_mask, cpus_allowed);",
            "",
            "\tctx->new_mask = new_mask;",
            "\tctx->flags |= SCA_CHECK;",
            "",
            "\tretval = dl_task_check_affinity(p, new_mask);",
            "\tif (retval)",
            "\t\tgoto out_free_new_mask;",
            "",
            "\tretval = __set_cpus_allowed_ptr(p, ctx);",
            "\tif (retval)",
            "\t\tgoto out_free_new_mask;",
            "",
            "\tcpuset_cpus_allowed(p, cpus_allowed);",
            "\tif (!cpumask_subset(new_mask, cpus_allowed)) {",
            "\t\t/*",
            "\t\t * We must have raced with a concurrent cpuset update.",
            "\t\t * Just reset the cpumask to the cpuset's cpus_allowed.",
            "\t\t */",
            "\t\tcpumask_copy(new_mask, cpus_allowed);",
            "",
            "\t\t/*",
            "\t\t * If SCA_USER is set, a 2nd call to __set_cpus_allowed_ptr()",
            "\t\t * will restore the previous user_cpus_ptr value.",
            "\t\t *",
            "\t\t * In the unlikely event a previous user_cpus_ptr exists,",
            "\t\t * we need to further restrict the mask to what is allowed",
            "\t\t * by that old user_cpus_ptr.",
            "\t\t */",
            "\t\tif (unlikely((ctx->flags & SCA_USER) && ctx->user_mask)) {",
            "\t\t\tbool empty = !cpumask_and(new_mask, new_mask,",
            "\t\t\t\t\t\t  ctx->user_mask);",
            "",
            "\t\t\tif (empty)",
            "\t\t\t\tcpumask_copy(new_mask, cpus_allowed);",
            "\t\t}",
            "\t\t__set_cpus_allowed_ptr(p, ctx);",
            "\t\tretval = -EINVAL;",
            "\t}",
            "",
            "out_free_new_mask:",
            "\tfree_cpumask_var(new_mask);",
            "out_free_cpus_allowed:",
            "\tfree_cpumask_var(cpus_allowed);",
            "\treturn retval;",
            "}"
          ],
          "function_name": "get_params, sched_attr_copy_to_user, dl_task_check_affinity, __sched_setaffinity",
          "description": "实现调度参数获取与亲和性检查功能。get_params提取任务调度参数，sched_attr_copy_to_user完成内核参数向用户空间复制。dl_task_check_affinity验证截止时间任务的CPU亲和性有效性，__sched_setaffinity处理任务CPU亲和性设置及约束检查。",
          "similarity": 0.5281924605369568
        },
        {
          "chunk_id": 9,
          "file_path": "kernel/sched/syscalls.c",
          "start_line": 1693,
          "end_line": 1719,
          "content": [
            "static int sched_rr_get_interval(pid_t pid, struct timespec64 *t)",
            "{",
            "\tunsigned int time_slice = 0;",
            "\tint retval;",
            "",
            "\tif (pid < 0)",
            "\t\treturn -EINVAL;",
            "",
            "\tscoped_guard (rcu) {",
            "\t\tstruct task_struct *p = find_process_by_pid(pid);",
            "\t\tif (!p)",
            "\t\t\treturn -ESRCH;",
            "",
            "\t\tretval = security_task_getscheduler(p);",
            "\t\tif (retval)",
            "\t\t\treturn retval;",
            "",
            "\t\tscoped_guard (task_rq_lock, p) {",
            "\t\t\tstruct rq *rq = scope.rq;",
            "\t\t\tif (p->sched_class->get_rr_interval)",
            "\t\t\t\ttime_slice = p->sched_class->get_rr_interval(rq, p);",
            "\t\t}",
            "\t}",
            "",
            "\tjiffies_to_timespec64(time_slice, t);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sched_rr_get_interval",
          "description": "实现实时轮转调度时间片查询功能。通过获取目标进程的调度类，调用其get_rr_interval方法获得时间片值，并将其转换为timespec64结构返回给用户空间。",
          "similarity": 0.5224179029464722
        },
        {
          "chunk_id": 8,
          "file_path": "kernel/sched/syscalls.c",
          "start_line": 1375,
          "end_line": 1517,
          "content": [
            "long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)",
            "{",
            "\tstruct affinity_context ac;",
            "\tstruct cpumask *user_mask;",
            "\tint retval;",
            "",
            "\tCLASS(find_get_task, p)(pid);",
            "\tif (!p)",
            "\t\treturn -ESRCH;",
            "",
            "\tif (p->flags & PF_NO_SETAFFINITY)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!check_same_owner(p)) {",
            "\t\tguard(rcu)();",
            "\t\tif (!ns_capable(__task_cred(p)->user_ns, CAP_SYS_NICE))",
            "\t\t\treturn -EPERM;",
            "\t}",
            "",
            "\tretval = security_task_setscheduler(p);",
            "\tif (retval)",
            "\t\treturn retval;",
            "",
            "\t/*",
            "\t * With non-SMP configs, user_cpus_ptr/user_mask isn't used and",
            "\t * alloc_user_cpus_ptr() returns NULL.",
            "\t */",
            "\tuser_mask = alloc_user_cpus_ptr(NUMA_NO_NODE);",
            "\tif (user_mask) {",
            "\t\tcpumask_copy(user_mask, in_mask);",
            "\t} else if (IS_ENABLED(CONFIG_SMP)) {",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\tac = (struct affinity_context){",
            "\t\t.new_mask  = in_mask,",
            "\t\t.user_mask = user_mask,",
            "\t\t.flags     = SCA_USER,",
            "\t};",
            "",
            "\tretval = __sched_setaffinity(p, &ac);",
            "\tkfree(ac.user_mask);",
            "",
            "\treturn retval;",
            "}",
            "static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,",
            "\t\t\t     struct cpumask *new_mask)",
            "{",
            "\tif (len < cpumask_size())",
            "\t\tcpumask_clear(new_mask);",
            "\telse if (len > cpumask_size())",
            "\t\tlen = cpumask_size();",
            "",
            "\treturn copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;",
            "}",
            "long sched_getaffinity(pid_t pid, struct cpumask *mask)",
            "{",
            "\tstruct task_struct *p;",
            "\tint retval;",
            "",
            "\tguard(rcu)();",
            "\tp = find_process_by_pid(pid);",
            "\tif (!p)",
            "\t\treturn -ESRCH;",
            "",
            "\tretval = security_task_getscheduler(p);",
            "\tif (retval)",
            "\t\treturn retval;",
            "",
            "\tguard(raw_spinlock_irqsave)(&p->pi_lock);",
            "\tcpumask_and(mask, &p->cpus_mask, cpu_active_mask);",
            "",
            "\treturn 0;",
            "}",
            "static void do_sched_yield(void)",
            "{",
            "\tstruct rq_flags rf;",
            "\tstruct rq *rq;",
            "",
            "\trq = this_rq_lock_irq(&rf);",
            "",
            "\tschedstat_inc(rq->yld_count);",
            "\tcurrent->sched_class->yield_task(rq);",
            "",
            "\tpreempt_disable();",
            "\trq_unlock_irq(rq, &rf);",
            "\tsched_preempt_enable_no_resched();",
            "",
            "\tschedule();",
            "}",
            "void __sched yield(void)",
            "{",
            "\tset_current_state(TASK_RUNNING);",
            "\tdo_sched_yield();",
            "}",
            "int __sched yield_to(struct task_struct *p, bool preempt)",
            "{",
            "\tstruct task_struct *curr = current;",
            "\tstruct rq *rq, *p_rq;",
            "\tint yielded = 0;",
            "",
            "\tscoped_guard (irqsave) {",
            "\t\trq = this_rq();",
            "",
            "again:",
            "\t\tp_rq = task_rq(p);",
            "\t\t/*",
            "\t\t * If we're the only runnable task on the rq and target rq also",
            "\t\t * has only one task, there's absolutely no point in yielding.",
            "\t\t */",
            "\t\tif (rq->nr_running == 1 && p_rq->nr_running == 1)",
            "\t\t\treturn -ESRCH;",
            "",
            "\t\tguard(double_rq_lock)(rq, p_rq);",
            "\t\tif (task_rq(p) != p_rq)",
            "\t\t\tgoto again;",
            "",
            "\t\tif (!curr->sched_class->yield_to_task)",
            "\t\t\treturn 0;",
            "",
            "\t\tif (curr->sched_class != p->sched_class)",
            "\t\t\treturn 0;",
            "",
            "\t\tif (task_on_cpu(p_rq, p) || !task_is_running(p))",
            "\t\t\treturn 0;",
            "",
            "\t\tyielded = curr->sched_class->yield_to_task(rq, p);",
            "\t\tif (yielded) {",
            "\t\t\tschedstat_inc(rq->yld_count);",
            "\t\t\t/*",
            "\t\t\t * Make p's CPU reschedule; pick_next_entity",
            "\t\t\t * takes care of fairness.",
            "\t\t\t */",
            "\t\t\tif (preempt && rq != p_rq)",
            "\t\t\t\tresched_curr(p_rq);",
            "\t\t}",
            "\t}",
            "",
            "\tif (yielded)",
            "\t\tschedule();",
            "",
            "\treturn yielded;",
            "}"
          ],
          "function_name": "sched_setaffinity, get_user_cpu_mask, sched_getaffinity, do_sched_yield, yield, yield_to",
          "description": "实现CPU亲和性设置/获取及调度让步功能。sched_setaffinity设置进程CPU亲和性掩码并进行权限检查，sched_getaffinity获取当前亲和性掩码。yield/yield_to实现调度器让步操作，do_sched_yield触发当前进程主动让出CPU。",
          "similarity": 0.5162132978439331
        }
      ]
    },
    {
      "source_file": "kernel/sched/fair.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:09:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\fair.c`\n\n---\n\n# `sched/fair.c` 技术文档\n\n## 1. 文件概述\n\n`sched/fair.c` 是 Linux 内核中 **完全公平调度器**（Completely Fair Scheduler, CFS）的核心实现文件，负责实现 `SCHED_NORMAL` 和 `SCHED_BATCH` 调度策略。CFS 旨在通过红黑树（RB-tree）维护可运行任务的虚拟运行时间（vruntime），以实现 CPU 时间的公平分配。该文件实现了任务调度、负载跟踪、时间片计算、组调度（group scheduling）、NUMA 负载均衡、带宽控制等关键机制，是 Linux 通用调度子系统的核心组成部分。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_entity`：调度实体，代表一个可调度单元（任务或任务组）\n- `struct cfs_rq`：CFS 运行队列，管理一组调度实体\n- `struct load_weight`：负载权重结构，用于计算任务对系统负载的贡献\n\n### 关键函数与宏\n- `__calc_delta()` / `calc_delta_fair()`：计算基于权重的调度时间增量\n- `update_load_add()` / `update_load_sub()` / `update_load_set()`：更新负载权重\n- `__update_inv_weight()`：预计算权重的倒数以优化除法运算\n- `get_update_sysctl_factor()`：根据在线 CPU 数量动态调整调度参数\n- `update_sysctl()` / `sched_init_granularity()`：初始化和更新调度粒度参数\n- `for_each_sched_entity()`：遍历调度实体层级结构（用于组调度）\n\n### 可调参数（sysctl）\n- `sysctl_sched_base_slice`：基础时间片（默认 700,000 纳秒）\n- `sysctl_sched_tunable_scaling`：调度参数缩放策略（NONE/LOG/LINEAR）\n- `sysctl_sched_migration_cost`：任务迁移成本阈值（500 微秒）\n- `sysctl_sched_cfs_bandwidth_slice_us`（CFS 带宽控制切片，默认 5 毫秒）\n- `sysctl_numa_balancing_promote_rate_limit_MBps`（NUMA 页迁移速率限制）\n\n## 3. 关键实现\n\n### 虚拟时间与公平性\nCFS 使用 **虚拟运行时间**（vruntime）衡量任务已使用的 CPU 时间，并通过 `calc_delta_fair()` 将实际执行时间按任务权重归一化。权重由任务的 nice 值决定（`NICE_0_LOAD = 1024` 为基准）。调度器总是选择 vruntime 最小的任务运行，确保高优先级（高权重）任务获得更多 CPU 时间。\n\n### 高效除法优化\n为避免频繁除法运算，CFS 预计算 `inv_weight = WMULT_CONST / weight`（`WMULT_CONST = ~0U`），将除法转换为乘法和右移操作（`mul_u64_u32_shr`）。`__calc_delta()` 通过动态调整移位位数（`shift`）保证计算精度，适用于 32/64 位架构。\n\n### 动态粒度调整\n基础时间片 `sched_base_slice` 根据在线 CPU 数量动态缩放：\n- `SCHED_TUNABLESCALING_NONE`：固定值\n- `SCHED_TUNABLESCALING_LINEAR`：线性缩放（×ncpus）\n- `SCHED_TUNABLESCALING_LOG`（默认）：对数缩放（×(1 + ilog2(ncpus))）  \n此设计确保在多核系统中保持合理的调度延迟和交互性。\n\n### 组调度支持\n通过 `for_each_sched_entity()` 宏遍历任务所属的调度实体层级（任务 → 任务组 → 父任务组），实现 CPU 带宽在任务组间的公平分配。每个 `cfs_rq` 独立维护其子实体的红黑树。\n\n### SMP 相关优化\n- **非对称 CPU 优先级**：`arch_asym_cpu_priority()` 允许架构定义 CPU 能力差异（如大小核）\n- **容量比较宏**：`fits_capacity()`（20% 容差）和 `capacity_greater()`（5% 容差）用于负载均衡决策\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- 调度核心：`\"sched.h\"`、`\"stats.h\"`、`\"autogroup.h\"`\n- 系统服务：`<linux/sched/clock.h>`、`<linux/sched/nohz.h>`、`<linux/psi.h>`\n- 内存管理：`<linux/mem_policy.h>`、`<linux/energy_model.h>`\n- SMP 支持：`<linux/topology.h>`、`<linux/cpumask_api.h>`\n- 数据结构：`<linux/rbtree_augmented.h>`\n\n### 条件编译特性\n- `CONFIG_SMP`：多处理器调度优化\n- `CONFIG_CFS_BANDWIDTH`：CPU 带宽限制（cgroup v1/v2）\n- `CONFIG_NUMA_BALANCING`：NUMA 自动迁移\n- `CONFIG_FAIR_GROUP_SCHED`：CFS 组调度（cgroup 支持）\n\n## 5. 使用场景\n\n- **通用任务调度**：所有使用 `SCHED_NORMAL` 或 `SCHED_BATCH` 策略的用户态进程\n- **cgroup CPU 资源控制**：通过 `cpu.cfs_quota_us` 和 `cpu.cfs_period_us` 限制任务组带宽\n- **NUMA 优化**：自动迁移内存页以减少远程访问（`numa_balancing`）\n- **节能调度**：结合 `energy_model` 在满足性能前提下选择低功耗 CPU\n- **实时性保障**：通过 `cond_resched()` 在长循环中主动让出 CPU，避免内核抢占延迟过高\n- **系统调优**：管理员通过 `/proc/sys/kernel/` 下的 sysctl 参数动态调整调度行为",
      "similarity": 0.575517475605011,
      "chunks": [
        {
          "chunk_id": 73,
          "file_path": "kernel/sched/fair.c",
          "start_line": 12860,
          "end_line": 12960,
          "content": [
            "bool cfs_prio_less(const struct task_struct *a, const struct task_struct *b,",
            "\t\t\tbool in_fi)",
            "{",
            "\tstruct rq *rq = task_rq(a);",
            "\tconst struct sched_entity *sea = &a->se;",
            "\tconst struct sched_entity *seb = &b->se;",
            "\tstruct cfs_rq *cfs_rqa;",
            "\tstruct cfs_rq *cfs_rqb;",
            "\ts64 delta;",
            "",
            "\tSCHED_WARN_ON(task_rq(b)->core != rq->core);",
            "",
            "#ifdef CONFIG_FAIR_GROUP_SCHED",
            "\t/*",
            "\t * Find an se in the hierarchy for tasks a and b, such that the se's",
            "\t * are immediate siblings.",
            "\t */",
            "\twhile (sea->cfs_rq->tg != seb->cfs_rq->tg) {",
            "\t\tint sea_depth = sea->depth;",
            "\t\tint seb_depth = seb->depth;",
            "",
            "\t\tif (sea_depth >= seb_depth)",
            "\t\t\tsea = parent_entity(sea);",
            "\t\tif (sea_depth <= seb_depth)",
            "\t\t\tseb = parent_entity(seb);",
            "\t}",
            "",
            "\tse_fi_update(sea, rq->core->core_forceidle_seq, in_fi);",
            "\tse_fi_update(seb, rq->core->core_forceidle_seq, in_fi);",
            "",
            "\tcfs_rqa = sea->cfs_rq;",
            "\tcfs_rqb = seb->cfs_rq;",
            "#else",
            "\tcfs_rqa = &task_rq(a)->cfs;",
            "\tcfs_rqb = &task_rq(b)->cfs;",
            "#endif",
            "",
            "\t/*",
            "\t * Find delta after normalizing se's vruntime with its cfs_rq's",
            "\t * min_vruntime_fi, which would have been updated in prior calls",
            "\t * to se_fi_update().",
            "\t */",
            "\tdelta = (s64)(sea->vruntime - seb->vruntime) +",
            "\t\t(s64)(cfs_rqb->min_vruntime_fi - cfs_rqa->min_vruntime_fi);",
            "",
            "\treturn delta > 0;",
            "}",
            "static int task_is_throttled_fair(struct task_struct *p, int cpu)",
            "{",
            "\tstruct cfs_rq *cfs_rq;",
            "",
            "#ifdef CONFIG_FAIR_GROUP_SCHED",
            "\tcfs_rq = task_group(p)->cfs_rq[cpu];",
            "#else",
            "\tcfs_rq = &cpu_rq(cpu)->cfs;",
            "#endif",
            "\treturn throttled_hierarchy(cfs_rq);",
            "}",
            "static inline void task_tick_core(struct rq *rq, struct task_struct *curr) {}",
            "static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)",
            "{",
            "\tstruct cfs_rq *cfs_rq;",
            "\tstruct sched_entity *se = &curr->se;",
            "",
            "\tfor_each_sched_entity(se) {",
            "\t\tcfs_rq = cfs_rq_of(se);",
            "\t\tentity_tick(cfs_rq, se, queued);",
            "\t}",
            "",
            "\tif (static_branch_unlikely(&sched_numa_balancing))",
            "\t\ttask_tick_numa(rq, curr);",
            "",
            "\tupdate_misfit_status(curr, rq);",
            "\tcheck_update_overutilized_status(task_rq(curr));",
            "",
            "\ttask_tick_core(rq, curr);",
            "}",
            "static void task_fork_fair(struct task_struct *p)",
            "{",
            "\tset_task_max_allowed_capacity(p);",
            "}",
            "static void",
            "prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)",
            "{",
            "\tif (!task_on_rq_queued(p))",
            "\t\treturn;",
            "",
            "\tif (rq->cfs.nr_running == 1)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Reschedule if we are currently running on this runqueue and",
            "\t * our priority decreased, or if we are not currently running on",
            "\t * this runqueue and our priority is higher than the current's",
            "\t */",
            "\tif (task_current(rq, p)) {",
            "\t\tif (p->prio > oldprio)",
            "\t\t\tresched_curr(rq);",
            "\t} else",
            "\t\twakeup_preempt(rq, p, 0);",
            "}"
          ],
          "function_name": "cfs_prio_less, task_is_throttled_fair, task_tick_core, task_tick_fair, task_fork_fair, prio_changed_fair",
          "description": "实现基于CFS的优先级比较和任务调度规则，处理任务优先级变更时的重调度需求，校验任务是否受限制，并维护强制空闲态下的虚拟运行时间计算逻辑。",
          "similarity": 0.6626434922218323
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/fair.c",
          "start_line": 2200,
          "end_line": 2414,
          "content": [
            "static bool load_too_imbalanced(long src_load, long dst_load,",
            "\t\t\t\tstruct task_numa_env *env)",
            "{",
            "\tlong imb, old_imb;",
            "\tlong orig_src_load, orig_dst_load;",
            "\tlong src_capacity, dst_capacity;",
            "",
            "\t/*",
            "\t * The load is corrected for the CPU capacity available on each node.",
            "\t *",
            "\t * src_load        dst_load",
            "\t * ------------ vs ---------",
            "\t * src_capacity    dst_capacity",
            "\t */",
            "\tsrc_capacity = env->src_stats.compute_capacity;",
            "\tdst_capacity = env->dst_stats.compute_capacity;",
            "",
            "\timb = abs(dst_load * src_capacity - src_load * dst_capacity);",
            "",
            "\torig_src_load = env->src_stats.load;",
            "\torig_dst_load = env->dst_stats.load;",
            "",
            "\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);",
            "",
            "\t/* Would this change make things worse? */",
            "\treturn (imb > old_imb);",
            "}",
            "static bool task_numa_compare(struct task_numa_env *env,",
            "\t\t\t      long taskimp, long groupimp, bool maymove)",
            "{",
            "\tstruct numa_group *cur_ng, *p_ng = deref_curr_numa_group(env->p);",
            "\tstruct rq *dst_rq = cpu_rq(env->dst_cpu);",
            "\tlong imp = p_ng ? groupimp : taskimp;",
            "\tstruct task_struct *cur;",
            "\tlong src_load, dst_load;",
            "\tint dist = env->dist;",
            "\tlong moveimp = imp;",
            "\tlong load;",
            "\tbool stopsearch = false;",
            "",
            "\tif (READ_ONCE(dst_rq->numa_migrate_on))",
            "\t\treturn false;",
            "",
            "\trcu_read_lock();",
            "\tcur = rcu_dereference(dst_rq->curr);",
            "\tif (cur && ((cur->flags & (PF_EXITING | PF_KTHREAD)) ||",
            "\t\t    !cur->mm))",
            "\t\tcur = NULL;",
            "",
            "\t/*",
            "\t * Because we have preemption enabled we can get migrated around and",
            "\t * end try selecting ourselves (current == env->p) as a swap candidate.",
            "\t */",
            "\tif (cur == env->p) {",
            "\t\tstopsearch = true;",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\tif (!cur) {",
            "\t\tif (maymove && moveimp >= env->best_imp)",
            "\t\t\tgoto assign;",
            "\t\telse",
            "\t\t\tgoto unlock;",
            "\t}",
            "",
            "\t/* Skip this swap candidate if cannot move to the source cpu. */",
            "\tif (!cpumask_test_cpu(env->src_cpu, cur->cpus_ptr))",
            "\t\tgoto unlock;",
            "",
            "\t/*",
            "\t * Skip this swap candidate if it is not moving to its preferred",
            "\t * node and the best task is.",
            "\t */",
            "\tif (env->best_task &&",
            "\t    env->best_task->numa_preferred_nid == env->src_nid &&",
            "\t    cur->numa_preferred_nid != env->src_nid) {",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\t/*",
            "\t * \"imp\" is the fault differential for the source task between the",
            "\t * source and destination node. Calculate the total differential for",
            "\t * the source task and potential destination task. The more negative",
            "\t * the value is, the more remote accesses that would be expected to",
            "\t * be incurred if the tasks were swapped.",
            "\t *",
            "\t * If dst and source tasks are in the same NUMA group, or not",
            "\t * in any group then look only at task weights.",
            "\t */",
            "\tcur_ng = rcu_dereference(cur->numa_group);",
            "\tif (cur_ng == p_ng) {",
            "\t\t/*",
            "\t\t * Do not swap within a group or between tasks that have",
            "\t\t * no group if there is spare capacity. Swapping does",
            "\t\t * not address the load imbalance and helps one task at",
            "\t\t * the cost of punishing another.",
            "\t\t */",
            "\t\tif (env->dst_stats.node_type == node_has_spare)",
            "\t\t\tgoto unlock;",
            "",
            "\t\timp = taskimp + task_weight(cur, env->src_nid, dist) -",
            "\t\t      task_weight(cur, env->dst_nid, dist);",
            "\t\t/*",
            "\t\t * Add some hysteresis to prevent swapping the",
            "\t\t * tasks within a group over tiny differences.",
            "\t\t */",
            "\t\tif (cur_ng)",
            "\t\t\timp -= imp / 16;",
            "\t} else {",
            "\t\t/*",
            "\t\t * Compare the group weights. If a task is all by itself",
            "\t\t * (not part of a group), use the task weight instead.",
            "\t\t */",
            "\t\tif (cur_ng && p_ng)",
            "\t\t\timp += group_weight(cur, env->src_nid, dist) -",
            "\t\t\t       group_weight(cur, env->dst_nid, dist);",
            "\t\telse",
            "\t\t\timp += task_weight(cur, env->src_nid, dist) -",
            "\t\t\t       task_weight(cur, env->dst_nid, dist);",
            "\t}",
            "",
            "\t/* Discourage picking a task already on its preferred node */",
            "\tif (cur->numa_preferred_nid == env->dst_nid)",
            "\t\timp -= imp / 16;",
            "",
            "\t/*",
            "\t * Encourage picking a task that moves to its preferred node.",
            "\t * This potentially makes imp larger than it's maximum of",
            "\t * 1998 (see SMALLIMP and task_weight for why) but in this",
            "\t * case, it does not matter.",
            "\t */",
            "\tif (cur->numa_preferred_nid == env->src_nid)",
            "\t\timp += imp / 8;",
            "",
            "\tif (maymove && moveimp > imp && moveimp > env->best_imp) {",
            "\t\timp = moveimp;",
            "\t\tcur = NULL;",
            "\t\tgoto assign;",
            "\t}",
            "",
            "\t/*",
            "\t * Prefer swapping with a task moving to its preferred node over a",
            "\t * task that is not.",
            "\t */",
            "\tif (env->best_task && cur->numa_preferred_nid == env->src_nid &&",
            "\t    env->best_task->numa_preferred_nid != env->src_nid) {",
            "\t\tgoto assign;",
            "\t}",
            "",
            "\t/*",
            "\t * If the NUMA importance is less than SMALLIMP,",
            "\t * task migration might only result in ping pong",
            "\t * of tasks and also hurt performance due to cache",
            "\t * misses.",
            "\t */",
            "\tif (imp < SMALLIMP || imp <= env->best_imp + SMALLIMP / 2)",
            "\t\tgoto unlock;",
            "",
            "\t/*",
            "\t * In the overloaded case, try and keep the load balanced.",
            "\t */",
            "\tload = task_h_load(env->p) - task_h_load(cur);",
            "\tif (!load)",
            "\t\tgoto assign;",
            "",
            "\tdst_load = env->dst_stats.load + load;",
            "\tsrc_load = env->src_stats.load - load;",
            "",
            "\tif (load_too_imbalanced(src_load, dst_load, env))",
            "\t\tgoto unlock;",
            "",
            "assign:",
            "\t/* Evaluate an idle CPU for a task numa move. */",
            "\tif (!cur) {",
            "\t\tint cpu = env->dst_stats.idle_cpu;",
            "",
            "\t\t/* Nothing cached so current CPU went idle since the search. */",
            "\t\tif (cpu < 0)",
            "\t\t\tcpu = env->dst_cpu;",
            "",
            "\t\t/*",
            "\t\t * If the CPU is no longer truly idle and the previous best CPU",
            "\t\t * is, keep using it.",
            "\t\t */",
            "\t\tif (!idle_cpu(cpu) && env->best_cpu >= 0 &&",
            "\t\t    idle_cpu(env->best_cpu)) {",
            "\t\t\tcpu = env->best_cpu;",
            "\t\t}",
            "",
            "\t\tenv->dst_cpu = cpu;",
            "\t}",
            "",
            "\ttask_numa_assign(env, cur, imp);",
            "",
            "\t/*",
            "\t * If a move to idle is allowed because there is capacity or load",
            "\t * balance improves then stop the search. While a better swap",
            "\t * candidate may exist, a search is not free.",
            "\t */",
            "\tif (maymove && !cur && env->best_cpu >= 0 && idle_cpu(env->best_cpu))",
            "\t\tstopsearch = true;",
            "",
            "\t/*",
            "\t * If a swap candidate must be identified and the current best task",
            "\t * moves its preferred node then stop the search.",
            "\t */",
            "\tif (!maymove && env->best_task &&",
            "\t    env->best_task->numa_preferred_nid == env->src_nid) {",
            "\t\tstopsearch = true;",
            "\t}",
            "unlock:",
            "\trcu_read_unlock();",
            "",
            "\treturn stopsearch;",
            "}"
          ],
          "function_name": "load_too_imbalanced, task_numa_compare",
          "description": "评估负载不平衡程度，通过任务权重差值对比候选任务，优先选择能改善负载且降低远程访问的迁移目标",
          "similarity": 0.598063588142395
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/fair.c",
          "start_line": 694,
          "end_line": 797,
          "content": [
            "static s64 entity_lag(u64 avruntime, struct sched_entity *se)",
            "{",
            "\ts64 vlag, limit;",
            "",
            "\tvlag = avruntime - se->vruntime;",
            "\tlimit = calc_delta_fair(max_t(u64, 2*se->slice, TICK_NSEC), se);",
            "",
            "\treturn clamp(vlag, -limit, limit);",
            "}",
            "static void update_entity_lag(struct cfs_rq *cfs_rq, struct sched_entity *se)",
            "{",
            "\tSCHED_WARN_ON(!se->on_rq);",
            "",
            "\tse->vlag = entity_lag(avg_vruntime(cfs_rq), se);",
            "}",
            "static int vruntime_eligible(struct cfs_rq *cfs_rq, u64 vruntime)",
            "{",
            "\tstruct sched_entity *curr = cfs_rq->curr;",
            "\ts64 avg = cfs_rq->avg_vruntime;",
            "\tlong load = cfs_rq->avg_load;",
            "",
            "\tif (curr && curr->on_rq) {",
            "\t\tunsigned long weight = scale_load_down(curr->load.weight);",
            "",
            "\t\tavg += entity_key(cfs_rq, curr) * weight;",
            "\t\tload += weight;",
            "\t}",
            "",
            "\treturn avg >= (s64)(vruntime - cfs_rq->min_vruntime) * load;",
            "}",
            "int entity_eligible(struct cfs_rq *cfs_rq, struct sched_entity *se)",
            "{",
            "\treturn vruntime_eligible(cfs_rq, se->vruntime);",
            "}",
            "static u64 __update_min_vruntime(struct cfs_rq *cfs_rq, u64 vruntime)",
            "{",
            "\tu64 min_vruntime = cfs_rq->min_vruntime;",
            "\t/*",
            "\t * open coded max_vruntime() to allow updating avg_vruntime",
            "\t */",
            "\ts64 delta = (s64)(vruntime - min_vruntime);",
            "\tif (delta > 0) {",
            "\t\tavg_vruntime_update(cfs_rq, delta);",
            "\t\tmin_vruntime = vruntime;",
            "\t}",
            "\treturn min_vruntime;",
            "}",
            "static void update_min_vruntime(struct cfs_rq *cfs_rq)",
            "{",
            "\tstruct sched_entity *se = __pick_root_entity(cfs_rq);",
            "\tstruct sched_entity *curr = cfs_rq->curr;",
            "\tu64 vruntime = cfs_rq->min_vruntime;",
            "",
            "\tif (curr) {",
            "\t\tif (curr->on_rq)",
            "\t\t\tvruntime = curr->vruntime;",
            "\t\telse",
            "\t\t\tcurr = NULL;",
            "\t}",
            "",
            "\tif (se) {",
            "\t\tif (!curr)",
            "\t\t\tvruntime = se->min_vruntime;",
            "\t\telse",
            "\t\t\tvruntime = min_vruntime(vruntime, se->min_vruntime);",
            "\t}",
            "",
            "\t/* ensure we never gain time by being placed backwards. */",
            "\tcfs_rq->min_vruntime = __update_min_vruntime(cfs_rq, vruntime);",
            "}",
            "static inline u64 cfs_rq_min_slice(struct cfs_rq *cfs_rq)",
            "{",
            "\tstruct sched_entity *root = __pick_root_entity(cfs_rq);",
            "\tstruct sched_entity *curr = cfs_rq->curr;",
            "\tu64 min_slice = ~0ULL;",
            "",
            "\tif (curr && curr->on_rq)",
            "\t\tmin_slice = curr->slice;",
            "",
            "\tif (root)",
            "\t\tmin_slice = min(min_slice, root->min_slice);",
            "",
            "\treturn min_slice;",
            "}",
            "static inline bool __entity_less(struct rb_node *a, const struct rb_node *b)",
            "{",
            "\treturn entity_before(__node_2_se(a), __node_2_se(b));",
            "}",
            "static inline void __min_vruntime_update(struct sched_entity *se, struct rb_node *node)",
            "{",
            "\tif (node) {",
            "\t\tstruct sched_entity *rse = __node_2_se(node);",
            "\t\tif (vruntime_gt(min_vruntime, se, rse))",
            "\t\t\tse->min_vruntime = rse->min_vruntime;",
            "\t}",
            "}",
            "static inline void __min_slice_update(struct sched_entity *se, struct rb_node *node)",
            "{",
            "\tif (node) {",
            "\t\tstruct sched_entity *rse = __node_2_se(node);",
            "\t\tif (rse->min_slice < se->min_slice)",
            "\t\t\tse->min_slice = rse->min_slice;",
            "\t}",
            "}"
          ],
          "function_name": "entity_lag, update_entity_lag, vruntime_eligible, entity_eligible, __update_min_vruntime, update_min_vruntime, cfs_rq_min_slice, __entity_less, __min_vruntime_update, __min_slice_update",
          "description": "处理实体延迟计算与最小运行时间更新，通过虚拟时间差异评估任务调度合理性并动态调整时间片基准值。",
          "similarity": 0.5897257924079895
        },
        {
          "chunk_id": 52,
          "file_path": "kernel/sched/fair.c",
          "start_line": 8581,
          "end_line": 8709,
          "content": [
            "static void set_task_max_allowed_capacity(struct task_struct *p)",
            "{",
            "\tstruct asym_cap_data *entry;",
            "",
            "\tif (!sched_asym_cpucap_active())",
            "\t\treturn;",
            "",
            "\trcu_read_lock();",
            "\tlist_for_each_entry_rcu(entry, &asym_cap_list, link) {",
            "\t\tcpumask_t *cpumask;",
            "",
            "\t\tcpumask = cpu_capacity_span(entry);",
            "\t\tif (!cpumask_intersects(p->cpus_ptr, cpumask))",
            "\t\t\tcontinue;",
            "",
            "\t\tp->max_allowed_capacity = entry->capacity;",
            "\t\tbreak;",
            "\t}",
            "\trcu_read_unlock();",
            "}",
            "static void set_cpus_allowed_fair(struct task_struct *p, struct affinity_context *ctx)",
            "{",
            "\tset_cpus_allowed_common(p, ctx);",
            "\tset_task_max_allowed_capacity(p);",
            "}",
            "static int",
            "balance_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)",
            "{",
            "\tif (sched_fair_runnable(rq))",
            "\t\treturn 1;",
            "",
            "\treturn sched_balance_newidle(rq, rf) != 0;",
            "}",
            "static inline void set_task_max_allowed_capacity(struct task_struct *p) {}",
            "static void set_next_buddy(struct sched_entity *se)",
            "{",
            "\tfor_each_sched_entity(se) {",
            "\t\tif (SCHED_WARN_ON(!se->on_rq))",
            "\t\t\treturn;",
            "\t\tif (se_is_idle(se))",
            "\t\t\treturn;",
            "\t\tcfs_rq_of(se)->next = se;",
            "\t}",
            "}",
            "static void check_preempt_wakeup_fair(struct rq *rq, struct task_struct *p, int wake_flags)",
            "{",
            "\tstruct task_struct *curr = rq->curr;",
            "\tstruct sched_entity *se = &curr->se, *pse = &p->se;",
            "\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);",
            "\tint next_buddy_marked = 0;",
            "\tint cse_is_idle, pse_is_idle;",
            "",
            "\tif (unlikely(se == pse))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This is possible from callers such as attach_tasks(), in which we",
            "\t * unconditionally wakeup_preempt() after an enqueue (which may have",
            "\t * lead to a throttle).  This both saves work and prevents false",
            "\t * next-buddy nomination below.",
            "\t */",
            "\tif (unlikely(throttled_hierarchy(cfs_rq_of(pse))))",
            "\t\treturn;",
            "",
            "\tif (sched_feat(NEXT_BUDDY) && !(wake_flags & WF_FORK) && !pse->sched_delayed) {",
            "\t\tset_next_buddy(pse);",
            "\t\tnext_buddy_marked = 1;",
            "\t}",
            "",
            "\t/*",
            "\t * We can come here with TIF_NEED_RESCHED already set from new task",
            "\t * wake up path.",
            "\t *",
            "\t * Note: this also catches the edge-case of curr being in a throttled",
            "\t * group (e.g. via set_curr_task), since update_curr() (in the",
            "\t * enqueue of curr) will have resulted in resched being set.  This",
            "\t * prevents us from potentially nominating it as a false LAST_BUDDY",
            "\t * below.",
            "\t */",
            "\tif (test_tsk_need_resched(curr))",
            "\t\treturn;",
            "",
            "\tif (!sched_feat(WAKEUP_PREEMPTION))",
            "\t\treturn;",
            "",
            "\tfind_matching_se(&se, &pse);",
            "\tWARN_ON_ONCE(!pse);",
            "",
            "\tcse_is_idle = se_is_idle(se);",
            "\tpse_is_idle = se_is_idle(pse);",
            "",
            "\t/*",
            "\t * Preempt an idle entity in favor of a non-idle entity (and don't preempt",
            "\t * in the inverse case).",
            "\t */",
            "\tif (cse_is_idle && !pse_is_idle)",
            "\t\tgoto preempt;",
            "\tif (cse_is_idle != pse_is_idle)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * BATCH and IDLE tasks do not preempt others.",
            "\t */",
            "\tif (unlikely(!normal_policy(p->policy)))",
            "\t\treturn;",
            "",
            "\tcfs_rq = cfs_rq_of(se);",
            "\tupdate_curr(cfs_rq);",
            "\t/*",
            "\t * If @p has a shorter slice than current and @p is eligible, override",
            "\t * current's slice protection in order to allow preemption.",
            "\t *",
            "\t * Note that even if @p does not turn out to be the most eligible",
            "\t * task at this moment, current's slice protection will be lost.",
            "\t */",
            "\tif (do_preempt_short(cfs_rq, pse, se) && se->vlag == se->deadline)",
            "\t\tse->vlag = se->deadline + 1;",
            "",
            "\t/*",
            "\t * If @p has become the most eligible task, force preemption.",
            "\t */",
            "\tif (pick_eevdf(cfs_rq) == pse)",
            "\t\tgoto preempt;",
            "",
            "\treturn;",
            "",
            "preempt:",
            "\tresched_curr(rq);",
            "}"
          ],
          "function_name": "set_task_max_allowed_capacity, set_cpus_allowed_fair, balance_fair, set_task_max_allowed_capacity, set_next_buddy, check_preempt_wakeup_fair",
          "description": "设置任务最大允许容量，实现负载均衡判断逻辑，管理调度实体间的抢占关系和运行队列状态更新。",
          "similarity": 0.5728209614753723
        },
        {
          "chunk_id": 53,
          "file_path": "kernel/sched/fair.c",
          "start_line": 8842,
          "end_line": 8948,
          "content": [
            "static bool fair_server_has_tasks(struct sched_dl_entity *dl_se)",
            "{",
            "\treturn !!dl_se->rq->cfs.nr_running;",
            "}",
            "void fair_server_init(struct rq *rq)",
            "{",
            "\tstruct sched_dl_entity *dl_se = &rq->fair_server;",
            "",
            "\tinit_dl_entity(dl_se);",
            "",
            "\tdl_server_init(dl_se, rq, fair_server_has_tasks, fair_server_pick_next,",
            "\t\t       fair_server_pick_task);",
            "",
            "}",
            "static void put_prev_task_fair(struct rq *rq, struct task_struct *prev, struct task_struct *next)",
            "{",
            "\tstruct sched_entity *se = &prev->se;",
            "\tstruct cfs_rq *cfs_rq;",
            "",
            "\tfor_each_sched_entity(se) {",
            "\t\tcfs_rq = cfs_rq_of(se);",
            "\t\tput_prev_entity(cfs_rq, se);",
            "\t}",
            "}",
            "static void yield_task_fair(struct rq *rq)",
            "{",
            "\tstruct task_struct *curr = rq->curr;",
            "\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);",
            "\tstruct sched_entity *se = &curr->se;",
            "",
            "\t/*",
            "\t * Are we the only task in the tree?",
            "\t */",
            "\tif (unlikely(rq->nr_running == 1))",
            "\t\treturn;",
            "",
            "\tclear_buddies(cfs_rq, se);",
            "",
            "\tupdate_rq_clock(rq);",
            "\t/*",
            "\t * Update run-time statistics of the 'current'.",
            "\t */",
            "\tupdate_curr(cfs_rq);",
            "\t/*",
            "\t * Tell update_rq_clock() that we've just updated,",
            "\t * so we don't do microscopic update in schedule()",
            "\t * and double the fastpath cost.",
            "\t */",
            "\trq_clock_skip_update(rq);",
            "",
            "\tse->deadline = se->vruntime + calc_delta_fair(se->slice, se);",
            "}",
            "static bool yield_to_task_fair(struct rq *rq, struct task_struct *p)",
            "{",
            "\tstruct sched_entity *se = &p->se;",
            "",
            "\t/* throttled hierarchies are not runnable */",
            "\tif (!se->on_rq || throttled_hierarchy(cfs_rq_of(se)))",
            "\t\treturn false;",
            "",
            "\t/* Tell the scheduler that we'd really like se to run next. */",
            "\tset_next_buddy(se);",
            "",
            "\tyield_task_fair(rq);",
            "",
            "\treturn true;",
            "}",
            "static int task_hot(struct task_struct *p, struct lb_env *env)",
            "{",
            "\ts64 delta;",
            "",
            "\tlockdep_assert_rq_held(env->src_rq);",
            "",
            "\tif (p->sched_class != &fair_sched_class)",
            "\t\treturn 0;",
            "",
            "\tif (unlikely(task_has_idle_policy(p)))",
            "\t\treturn 0;",
            "",
            "\t/* SMT siblings share cache */",
            "\tif (env->sd->flags & SD_SHARE_CPUCAPACITY)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Buddy candidates are cache hot:",
            "\t */",
            "\tif (sched_feat(CACHE_HOT_BUDDY) && env->dst_rq->nr_running &&",
            "\t    (&p->se == cfs_rq_of(&p->se)->next))",
            "\t\treturn 1;",
            "",
            "\tif (sysctl_sched_migration_cost == -1)",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * Don't migrate task if the task's cookie does not match",
            "\t * with the destination CPU's core cookie.",
            "\t */",
            "\tif (!sched_core_cookie_match(cpu_rq(env->dst_cpu), p))",
            "\t\treturn 1;",
            "",
            "\tif (sysctl_sched_migration_cost == 0)",
            "\t\treturn 0;",
            "",
            "\tdelta = rq_clock_task(env->src_rq) - p->se.exec_start;",
            "",
            "\treturn delta < (s64)sysctl_sched_migration_cost;",
            "}"
          ],
          "function_name": "fair_server_has_tasks, fair_server_init, put_prev_task_fair, yield_task_fair, yield_to_task_fair, task_hot",
          "description": "维护公平调度器的服务器实体状态，实现任务切换时的统计更新、主动让出CPU以及基于缓存热度的任务迁移判定。",
          "similarity": 0.5722862482070923
        }
      ]
    }
  ]
}