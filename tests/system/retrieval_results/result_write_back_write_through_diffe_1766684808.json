{
  "query": "write back write through difference",
  "timestamp": "2025-12-26 01:46:48",
  "retrieved_files": [
    {
      "source_file": "mm/page-writeback.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:09\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page-writeback.c`\n\n---\n\n# page-writeback.c 技术文档\n\n## 1. 文件概述\n\n`page-writeback.c` 是 Linux 内核内存管理子系统（MM）中的核心文件，负责实现**脏页回写（dirty page writeback）机制**。该机制用于控制和协调将修改过的页面（即“脏页”）从内存写回到持久化存储（如磁盘）的过程，以确保数据一致性、防止内存耗尽，并在系统负载与 I/O 带宽之间取得平衡。\n\n该文件主要提供以下功能：\n- 脏页数量的全局与每 BDI（Backing Device Info）级别的阈值管理\n- 脏页生成速率的动态限流（throttling）\n- 后台回写线程（如 `writeback` 线程）的触发逻辑\n- 支持基于 cgroup 的内存回写控制（当启用 `CONFIG_CGROUP_WRITEBACK` 时）\n- 与 `/proc/sys/vm` 中可调参数的交互接口\n\n## 2. 核心功能\n\n### 主要全局变量（可通过 sysctl 调整）\n| 变量名 | 默认值 | 说明 |\n|--------|--------|------|\n| `dirty_background_ratio` | 10 | 当脏页占可用内存比例达到此值时，启动后台回写 |\n| `vm_dirty_ratio` | 20 | 脏页比例硬上限，超过则阻塞写进程进行同步回写 |\n| `dirty_background_bytes` | 0 | 以字节为单位指定后台回写阈值（优先级高于 ratio） |\n| `vm_dirty_bytes` | 0 | 以字节为单位指定脏页硬上限（优先级高于 ratio） |\n| `dirty_writeback_interval` | 500 (5秒) | 后台回写线程的唤醒间隔（单位：厘秒） |\n| `dirty_expire_interval` | 3000 (30秒) | 脏页最大存活时间，超时强制回写 |\n| `laptop_mode` | 0 | 笔记本模式开关，减少磁盘活动以省电 |\n| `ratelimit_pages` | 32 | 每 CPU 脏页速率限制阈值 |\n\n### 关键数据结构\n- **`struct wb_domain`**  \n  回写域（writeback domain），用于聚合多个 BDI 的回写状态，支持全局或 per-memcg 的回写控制。\n  \n- **`struct dirty_throttle_control` (dtc)**  \n  脏页限流控制上下文，包含：\n  - `avail`：当前可脏化的内存总量\n  - `dirty`：当前脏页数量\n  - `thresh` / `bg_thresh`：硬/软回写阈值\n  - `wb_dirty` / `wb_thresh` / `wb_bg_thresh`：per-BDI 级别的对应值\n  - `pos_ratio`：用于计算回写速率的比例因子\n\n- **条件编译支持**  \n  通过 `CONFIG_CGROUP_WRITEBACK` 区分是否支持 memcg 级别的回写控制，提供 `GDTC_INIT`、`MDTC_INIT` 等宏及辅助函数（如 `mdtc_valid()`、`wb_min_max_ratio()`）。\n\n### 核心辅助函数（部分在截断代码中未完整显示）\n- `node_dirtyable_memory()`：计算指定 NUMA 节点中可用于脏页缓存的内存总量（包括空闲页 + 文件缓存页 - 保留页）。\n- `balance_dirty_pages()`：主限流函数，在进程写入时被调用，根据当前脏页水位决定是否休眠或触发回写。\n- `balance_dirty_pages_ratelimited()`：带速率限制的脏页平衡入口，避免频繁调用开销。\n\n## 3. 关键实现\n\n### 脏页阈值计算逻辑\n- 脏页上限基于 **“dirtyable memory”** 计算，即 `(free pages + file cache pages - kernel reserves)`。\n- 支持两种配置方式：**百分比（ratio）** 或 **绝对字节数（bytes）**，后者优先。\n- 当启用 `vm_highmem_is_dirtyable` 时，highmem 区域的空闲页也计入 dirtyable memory。\n\n### 动态限流机制\n- 使用 **`MAX_PAUSE`（最大 200ms）** 限制单次 `balance_dirty_pages()` 的休眠时间。\n- 引入 **`DIRTY_POLL_THRESH`（128KB）** 作为调用间隔优化阈值：若脏页增长过快，则提升休眠时间至最大值。\n- 通过 **`BANDWIDTH_INTERVAL`（200ms）** 动态估算存储设备的写入带宽，用于调整回写速率。\n\n### cgroup writeback 支持\n- 在 `CONFIG_CGROUP_WRITEBACK` 启用时：\n  - 每个 memcg 有独立的 `wb_domain`\n  - `dirty_throttle_control` 可关联全局（gdtc）或 memcg（mdtc）上下文\n  - BDI 的 min/max_ratio 根据其实际带宽动态缩放，实现公平分配\n\n### 老化与完成计数\n- 使用 `fprop_local_percpu` 结构跟踪每个 BDI 的回写完成情况。\n- `VM_COMPLETIONS_PERIOD_LEN`（3 秒）定义了回写完成率的老化周期，影响带宽估算的响应速度。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/swap.h>`、`<linux/pagevec.h>` 等，与页分配、回收机制紧密集成。\n- **VFS 层**：通过 `<linux/fs.h>`、`<linux/pagemap.h>` 与 address_space 和 inode 交互。\n- **块设备层**：通过 `<linux/blkdev.h>`、`<linux/backing-dev.h>` 获取 BDI 信息和 I/O 能力。\n- **调度与同步**：使用 `<linux/sched.h>`、`<linux/spinlock.h>`、`<linux/timer.h>` 实现休眠、锁和定时器。\n- **追踪系统**：集成 `<trace/events/writeback.h>` 提供回写事件追踪点。\n- **内部头文件**：包含 `\"internal.h\"` 获取 MM 子系统内部接口。\n\n## 5. 使用场景\n\n1. **用户空间写入文件**  \n   当进程通过 `write()` 修改文件页时，页被标记为脏，随后调用 `balance_dirty_pages_ratelimited()` 触发脏页控制。\n\n2. **内存压力下的页面回收**  \n   kswapd 或直接回收路径在需要释放内存时，可能调用回写逻辑清理脏页。\n\n3. **定期后台回写**  \n   `writeback` 内核线程按 `dirty_writeback_interval` 周期唤醒，检查并回写超过 `dirty_expire_interval` 的脏页。\n\n4. **系统关闭或 sync 调用**  \n   虽然主要同步逻辑在其他文件，但本文件提供的阈值和状态是决策基础。\n\n5. **容器环境中的资源隔离**  \n   启用 cgroup writeback 后，不同 memcg 的脏页回写相互隔离，避免一个容器的大量写入影响其他容器性能。\n\n6. **笔记本省电模式**  \n   当 `laptop_mode` 启用时，延迟回写以减少磁盘旋转时间，延长电池寿命。",
      "similarity": 0.5240751504898071,
      "chunks": [
        {
          "chunk_id": 14,
          "file_path": "mm/page-writeback.c",
          "start_line": 2956,
          "end_line": 3070,
          "content": [
            "static void wb_inode_writeback_start(struct bdi_writeback *wb)",
            "{",
            "\tatomic_inc(&wb->writeback_inodes);",
            "}",
            "static void wb_inode_writeback_end(struct bdi_writeback *wb)",
            "{",
            "\tunsigned long flags;",
            "\tatomic_dec(&wb->writeback_inodes);",
            "\t/*",
            "\t * Make sure estimate of writeback throughput gets updated after",
            "\t * writeback completed. We delay the update by BANDWIDTH_INTERVAL",
            "\t * (which is the interval other bandwidth updates use for batching) so",
            "\t * that if multiple inodes end writeback at a similar time, they get",
            "\t * batched into one bandwidth update.",
            "\t */",
            "\tspin_lock_irqsave(&wb->work_lock, flags);",
            "\tif (test_bit(WB_registered, &wb->state))",
            "\t\tqueue_delayed_work(bdi_wq, &wb->bw_dwork, BANDWIDTH_INTERVAL);",
            "\tspin_unlock_irqrestore(&wb->work_lock, flags);",
            "}",
            "bool __folio_end_writeback(struct folio *folio)",
            "{",
            "\tlong nr = folio_nr_pages(folio);",
            "\tstruct address_space *mapping = folio_mapping(folio);",
            "\tbool ret;",
            "",
            "\tif (mapping && mapping_use_writeback_tags(mapping)) {",
            "\t\tstruct inode *inode = mapping->host;",
            "\t\tstruct backing_dev_info *bdi = inode_to_bdi(inode);",
            "\t\tunsigned long flags;",
            "",
            "\t\txa_lock_irqsave(&mapping->i_pages, flags);",
            "\t\tret = folio_xor_flags_has_waiters(folio, 1 << PG_writeback);",
            "\t\t__xa_clear_mark(&mapping->i_pages, folio_index(folio),",
            "\t\t\t\t\tPAGECACHE_TAG_WRITEBACK);",
            "\t\tif (bdi->capabilities & BDI_CAP_WRITEBACK_ACCT) {",
            "\t\t\tstruct bdi_writeback *wb = inode_to_wb(inode);",
            "",
            "\t\t\twb_stat_mod(wb, WB_WRITEBACK, -nr);",
            "\t\t\t__wb_writeout_add(wb, nr);",
            "\t\t\tif (!mapping_tagged(mapping, PAGECACHE_TAG_WRITEBACK))",
            "\t\t\t\twb_inode_writeback_end(wb);",
            "\t\t}",
            "",
            "\t\tif (mapping->host && !mapping_tagged(mapping,",
            "\t\t\t\t\t\t     PAGECACHE_TAG_WRITEBACK))",
            "\t\t\tsb_clear_inode_writeback(mapping->host);",
            "",
            "\t\txa_unlock_irqrestore(&mapping->i_pages, flags);",
            "\t} else {",
            "\t\tret = folio_xor_flags_has_waiters(folio, 1 << PG_writeback);",
            "\t}",
            "",
            "\tlruvec_stat_mod_folio(folio, NR_WRITEBACK, -nr);",
            "\tzone_stat_mod_folio(folio, NR_ZONE_WRITE_PENDING, -nr);",
            "\tnode_stat_mod_folio(folio, NR_WRITTEN, nr);",
            "",
            "\treturn ret;",
            "}",
            "void __folio_start_writeback(struct folio *folio, bool keep_write)",
            "{",
            "\tlong nr = folio_nr_pages(folio);",
            "\tstruct address_space *mapping = folio_mapping(folio);",
            "\tint access_ret;",
            "",
            "\tVM_BUG_ON_FOLIO(folio_test_writeback(folio), folio);",
            "",
            "\tif (mapping && mapping_use_writeback_tags(mapping)) {",
            "\t\tXA_STATE(xas, &mapping->i_pages, folio_index(folio));",
            "\t\tstruct inode *inode = mapping->host;",
            "\t\tstruct backing_dev_info *bdi = inode_to_bdi(inode);",
            "\t\tunsigned long flags;",
            "\t\tbool on_wblist;",
            "",
            "\t\txas_lock_irqsave(&xas, flags);",
            "\t\txas_load(&xas);",
            "\t\tfolio_test_set_writeback(folio);",
            "",
            "\t\ton_wblist = mapping_tagged(mapping, PAGECACHE_TAG_WRITEBACK);",
            "",
            "\t\txas_set_mark(&xas, PAGECACHE_TAG_WRITEBACK);",
            "\t\tif (bdi->capabilities & BDI_CAP_WRITEBACK_ACCT) {",
            "\t\t\tstruct bdi_writeback *wb = inode_to_wb(inode);",
            "",
            "\t\t\twb_stat_mod(wb, WB_WRITEBACK, nr);",
            "\t\t\tif (!on_wblist)",
            "\t\t\t\twb_inode_writeback_start(wb);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * We can come through here when swapping anonymous",
            "\t\t * folios, so we don't necessarily have an inode to",
            "\t\t * track for sync.",
            "\t\t */",
            "\t\tif (mapping->host && !on_wblist)",
            "\t\t\tsb_mark_inode_writeback(mapping->host);",
            "\t\tif (!folio_test_dirty(folio))",
            "\t\t\txas_clear_mark(&xas, PAGECACHE_TAG_DIRTY);",
            "\t\tif (!keep_write)",
            "\t\t\txas_clear_mark(&xas, PAGECACHE_TAG_TOWRITE);",
            "\t\txas_unlock_irqrestore(&xas, flags);",
            "\t} else {",
            "\t\tfolio_test_set_writeback(folio);",
            "\t}",
            "",
            "\tlruvec_stat_mod_folio(folio, NR_WRITEBACK, nr);",
            "\tzone_stat_mod_folio(folio, NR_ZONE_WRITE_PENDING, nr);",
            "",
            "\taccess_ret = arch_make_folio_accessible(folio);",
            "\t/*",
            "\t * If writeback has been triggered on a page that cannot be made",
            "\t * accessible, it is too late to recover here.",
            "\t */",
            "\tVM_BUG_ON_FOLIO(access_ret != 0, folio);",
            "}"
          ],
          "function_name": "wb_inode_writeback_start, wb_inode_writeback_end, __folio_end_writeback, __folio_start_writeback",
          "description": "wb_inode_writeback_start/end 维护写回中的inode计数。__folio_end_writeback 结束写回并更新统计信息。__folio_start_writeback 开始写回并标记页面状态。所有操作均伴随相应的统计信息更新和状态同步。",
          "similarity": 0.5309727191925049
        },
        {
          "chunk_id": 11,
          "file_path": "mm/page-writeback.c",
          "start_line": 2176,
          "end_line": 2404,
          "content": [
            "static int dirty_writeback_centisecs_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tunsigned int old_interval = dirty_writeback_interval;",
            "\tint ret;",
            "",
            "\tret = proc_dointvec(table, write, buffer, length, ppos);",
            "",
            "\t/*",
            "\t * Writing 0 to dirty_writeback_interval will disable periodic writeback",
            "\t * and a different non-zero value will wakeup the writeback threads.",
            "\t * wb_wakeup_delayed() would be more appropriate, but it's a pain to",
            "\t * iterate over all bdis and wbs.",
            "\t * The reason we do this is to make the change take effect immediately.",
            "\t */",
            "\tif (!ret && write && dirty_writeback_interval &&",
            "\t\tdirty_writeback_interval != old_interval)",
            "\t\twakeup_flusher_threads(WB_REASON_PERIODIC);",
            "",
            "\treturn ret;",
            "}",
            "void laptop_mode_timer_fn(struct timer_list *t)",
            "{",
            "\tstruct backing_dev_info *backing_dev_info =",
            "\t\tfrom_timer(backing_dev_info, t, laptop_mode_wb_timer);",
            "",
            "\twakeup_flusher_threads_bdi(backing_dev_info, WB_REASON_LAPTOP_TIMER);",
            "}",
            "void laptop_io_completion(struct backing_dev_info *info)",
            "{",
            "\tmod_timer(&info->laptop_mode_wb_timer, jiffies + laptop_mode);",
            "}",
            "void laptop_sync_completion(void)",
            "{",
            "\tstruct backing_dev_info *bdi;",
            "",
            "\trcu_read_lock();",
            "",
            "\tlist_for_each_entry_rcu(bdi, &bdi_list, bdi_list)",
            "\t\tdel_timer(&bdi->laptop_mode_wb_timer);",
            "",
            "\trcu_read_unlock();",
            "}",
            "void writeback_set_ratelimit(void)",
            "{",
            "\tstruct wb_domain *dom = &global_wb_domain;",
            "\tunsigned long background_thresh;",
            "\tunsigned long dirty_thresh;",
            "",
            "\tglobal_dirty_limits(&background_thresh, &dirty_thresh);",
            "\tdom->dirty_limit = dirty_thresh;",
            "\tratelimit_pages = dirty_thresh / (num_online_cpus() * 32);",
            "\tif (ratelimit_pages < 16)",
            "\t\tratelimit_pages = 16;",
            "}",
            "static int page_writeback_cpu_online(unsigned int cpu)",
            "{",
            "\twriteback_set_ratelimit();",
            "\treturn 0;",
            "}",
            "void __init page_writeback_init(void)",
            "{",
            "\tBUG_ON(wb_domain_init(&global_wb_domain, GFP_KERNEL));",
            "",
            "\tcpuhp_setup_state(CPUHP_AP_ONLINE_DYN, \"mm/writeback:online\",",
            "\t\t\t  page_writeback_cpu_online, NULL);",
            "\tcpuhp_setup_state(CPUHP_MM_WRITEBACK_DEAD, \"mm/writeback:dead\", NULL,",
            "\t\t\t  page_writeback_cpu_online);",
            "#ifdef CONFIG_SYSCTL",
            "\tregister_sysctl_init(\"vm\", vm_page_writeback_sysctls);",
            "#endif",
            "}",
            "void tag_pages_for_writeback(struct address_space *mapping,",
            "\t\t\t     pgoff_t start, pgoff_t end)",
            "{",
            "\tXA_STATE(xas, &mapping->i_pages, start);",
            "\tunsigned int tagged = 0;",
            "\tvoid *page;",
            "",
            "\txas_lock_irq(&xas);",
            "\txas_for_each_marked(&xas, page, end, PAGECACHE_TAG_DIRTY) {",
            "\t\txas_set_mark(&xas, PAGECACHE_TAG_TOWRITE);",
            "\t\tif (++tagged % XA_CHECK_SCHED)",
            "\t\t\tcontinue;",
            "",
            "\t\txas_pause(&xas);",
            "\t\txas_unlock_irq(&xas);",
            "\t\tcond_resched();",
            "\t\txas_lock_irq(&xas);",
            "\t}",
            "\txas_unlock_irq(&xas);",
            "}",
            "int write_cache_pages(struct address_space *mapping,",
            "\t\t      struct writeback_control *wbc, writepage_t writepage,",
            "\t\t      void *data)",
            "{",
            "\tint ret = 0;",
            "\tint done = 0;",
            "\tint error;",
            "\tstruct folio_batch fbatch;",
            "\tint nr_folios;",
            "\tpgoff_t index;",
            "\tpgoff_t end;\t\t/* Inclusive */",
            "\tpgoff_t done_index;",
            "\tint range_whole = 0;",
            "\txa_mark_t tag;",
            "",
            "\tfolio_batch_init(&fbatch);",
            "\tif (wbc->range_cyclic) {",
            "\t\tindex = mapping->writeback_index; /* prev offset */",
            "\t\tend = -1;",
            "\t} else {",
            "\t\tindex = wbc->range_start >> PAGE_SHIFT;",
            "\t\tend = wbc->range_end >> PAGE_SHIFT;",
            "\t\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)",
            "\t\t\trange_whole = 1;",
            "\t}",
            "\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages) {",
            "\t\ttag_pages_for_writeback(mapping, index, end);",
            "\t\ttag = PAGECACHE_TAG_TOWRITE;",
            "\t} else {",
            "\t\ttag = PAGECACHE_TAG_DIRTY;",
            "\t}",
            "\tdone_index = index;",
            "\twhile (!done && (index <= end)) {",
            "\t\tint i;",
            "",
            "\t\tnr_folios = filemap_get_folios_tag(mapping, &index, end,",
            "\t\t\t\ttag, &fbatch);",
            "",
            "\t\tif (nr_folios == 0)",
            "\t\t\tbreak;",
            "",
            "\t\tfor (i = 0; i < nr_folios; i++) {",
            "\t\t\tstruct folio *folio = fbatch.folios[i];",
            "\t\t\tunsigned long nr;",
            "",
            "\t\t\tdone_index = folio->index;",
            "",
            "\t\t\tfolio_lock(folio);",
            "",
            "\t\t\t/*",
            "\t\t\t * Page truncated or invalidated. We can freely skip it",
            "\t\t\t * then, even for data integrity operations: the page",
            "\t\t\t * has disappeared concurrently, so there could be no",
            "\t\t\t * real expectation of this data integrity operation",
            "\t\t\t * even if there is now a new, dirty page at the same",
            "\t\t\t * pagecache address.",
            "\t\t\t */",
            "\t\t\tif (unlikely(folio->mapping != mapping)) {",
            "continue_unlock:",
            "\t\t\t\tfolio_unlock(folio);",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tif (!folio_test_dirty(folio)) {",
            "\t\t\t\t/* someone wrote it for us */",
            "\t\t\t\tgoto continue_unlock;",
            "\t\t\t}",
            "",
            "\t\t\tif (folio_test_writeback(folio)) {",
            "\t\t\t\tif (wbc->sync_mode != WB_SYNC_NONE)",
            "\t\t\t\t\tfolio_wait_writeback(folio);",
            "\t\t\t\telse",
            "\t\t\t\t\tgoto continue_unlock;",
            "\t\t\t}",
            "",
            "\t\t\tBUG_ON(folio_test_writeback(folio));",
            "\t\t\tif (!folio_clear_dirty_for_io(folio))",
            "\t\t\t\tgoto continue_unlock;",
            "",
            "\t\t\ttrace_wbc_writepage(wbc, inode_to_bdi(mapping->host));",
            "\t\t\terror = writepage(folio, wbc, data);",
            "\t\t\tnr = folio_nr_pages(folio);",
            "\t\t\tif (unlikely(error)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * Handle errors according to the type of",
            "\t\t\t\t * writeback. There's no need to continue for",
            "\t\t\t\t * background writeback. Just push done_index",
            "\t\t\t\t * past this page so media errors won't choke",
            "\t\t\t\t * writeout for the entire file. For integrity",
            "\t\t\t\t * writeback, we must process the entire dirty",
            "\t\t\t\t * set regardless of errors because the fs may",
            "\t\t\t\t * still have state to clear for each page. In",
            "\t\t\t\t * that case we continue processing and return",
            "\t\t\t\t * the first error.",
            "\t\t\t\t */",
            "\t\t\t\tif (error == AOP_WRITEPAGE_ACTIVATE) {",
            "\t\t\t\t\tfolio_unlock(folio);",
            "\t\t\t\t\terror = 0;",
            "\t\t\t\t} else if (wbc->sync_mode != WB_SYNC_ALL) {",
            "\t\t\t\t\tret = error;",
            "\t\t\t\t\tdone_index = folio->index + nr;",
            "\t\t\t\t\tdone = 1;",
            "\t\t\t\t\tbreak;",
            "\t\t\t\t}",
            "\t\t\t\tif (!ret)",
            "\t\t\t\t\tret = error;",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * We stop writing back only if we are not doing",
            "\t\t\t * integrity sync. In case of integrity sync we have to",
            "\t\t\t * keep going until we have written all the pages",
            "\t\t\t * we tagged for writeback prior to entering this loop.",
            "\t\t\t */",
            "\t\t\twbc->nr_to_write -= nr;",
            "\t\t\tif (wbc->nr_to_write <= 0 &&",
            "\t\t\t    wbc->sync_mode == WB_SYNC_NONE) {",
            "\t\t\t\tdone = 1;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t\tfolio_batch_release(&fbatch);",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\t/*",
            "\t * If we hit the last page and there is more work to be done: wrap",
            "\t * back the index back to the start of the file for the next",
            "\t * time we are called.",
            "\t */",
            "\tif (wbc->range_cyclic && !done)",
            "\t\tdone_index = 0;",
            "\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))",
            "\t\tmapping->writeback_index = done_index;",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "dirty_writeback_centisecs_handler, laptop_mode_timer_fn, laptop_io_completion, laptop_sync_completion, writeback_set_ratelimit, page_writeback_cpu_online, page_writeback_init, tag_pages_for_writeback, write_cache_pages",
          "description": "dirty_writeback_centisecs_handler 设置脏页写回间隔，laptop_mode_timer_fn 处理笔记本模式下定时触发写回。writeback_set_ratelimit 根据CPU数量动态调整速率限制参数。tag_pages_for_writeback 标记待写回页面，write_cache_pages 执行实际的缓存页面写回流程。",
          "similarity": 0.5227488279342651
        },
        {
          "chunk_id": 15,
          "file_path": "mm/page-writeback.c",
          "start_line": 3088,
          "end_line": 3109,
          "content": [
            "void folio_wait_writeback(struct folio *folio)",
            "{",
            "\twhile (folio_test_writeback(folio)) {",
            "\t\ttrace_folio_wait_writeback(folio, folio_mapping(folio));",
            "\t\tfolio_wait_bit(folio, PG_writeback);",
            "\t}",
            "}",
            "int folio_wait_writeback_killable(struct folio *folio)",
            "{",
            "\twhile (folio_test_writeback(folio)) {",
            "\t\ttrace_folio_wait_writeback(folio, folio_mapping(folio));",
            "\t\tif (folio_wait_bit_killable(folio, PG_writeback))",
            "\t\t\treturn -EINTR;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "void folio_wait_stable(struct folio *folio)",
            "{",
            "\tif (mapping_stable_writes(folio_mapping(folio)))",
            "\t\tfolio_wait_writeback(folio);",
            "}"
          ],
          "function_name": "folio_wait_writeback, folio_wait_writeback_killable, folio_wait_stable",
          "description": "该代码段实现了与页面写回状态相关的同步机制。`folio_wait_writeback`持续阻塞直至指定页的writeback标志清除，`folio_wait_writeback_killable`则允许在等待期间被信号中断并返回-EINTR。`folio_wait_stable`通过判断映射稳定性决定是否触发写回等待，确保数据持久化后再执行后续操作。",
          "similarity": 0.5181934237480164
        },
        {
          "chunk_id": 12,
          "file_path": "mm/page-writeback.c",
          "start_line": 2574,
          "end_line": 2681,
          "content": [
            "static int writepage_cb(struct folio *folio, struct writeback_control *wbc,",
            "\t\tvoid *data)",
            "{",
            "\tstruct address_space *mapping = data;",
            "\tint ret = mapping->a_ops->writepage(&folio->page, wbc);",
            "\tmapping_set_error(mapping, ret);",
            "\treturn ret;",
            "}",
            "int do_writepages(struct address_space *mapping, struct writeback_control *wbc)",
            "{",
            "\tint ret;",
            "\tstruct bdi_writeback *wb;",
            "",
            "\tif (wbc->nr_to_write <= 0)",
            "\t\treturn 0;",
            "\twb = inode_to_wb_wbc(mapping->host, wbc);",
            "\twb_bandwidth_estimate_start(wb);",
            "\twhile (1) {",
            "\t\tif (mapping->a_ops->writepages) {",
            "\t\t\tret = mapping->a_ops->writepages(mapping, wbc);",
            "\t\t} else if (mapping->a_ops->writepage) {",
            "\t\t\tstruct blk_plug plug;",
            "",
            "\t\t\tblk_start_plug(&plug);",
            "\t\t\tret = write_cache_pages(mapping, wbc, writepage_cb,",
            "\t\t\t\t\t\tmapping);",
            "\t\t\tblk_finish_plug(&plug);",
            "\t\t} else {",
            "\t\t\t/* deal with chardevs and other special files */",
            "\t\t\tret = 0;",
            "\t\t}",
            "\t\tif (ret != -ENOMEM || wbc->sync_mode != WB_SYNC_ALL)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * Lacking an allocation context or the locality or writeback",
            "\t\t * state of any of the inode's pages, throttle based on",
            "\t\t * writeback activity on the local node. It's as good a",
            "\t\t * guess as any.",
            "\t\t */",
            "\t\treclaim_throttle(NODE_DATA(numa_node_id()),",
            "\t\t\tVMSCAN_THROTTLE_WRITEBACK);",
            "\t}",
            "\t/*",
            "\t * Usually few pages are written by now from those we've just submitted",
            "\t * but if there's constant writeback being submitted, this makes sure",
            "\t * writeback bandwidth is updated once in a while.",
            "\t */",
            "\tif (time_is_before_jiffies(READ_ONCE(wb->bw_time_stamp) +",
            "\t\t\t\t   BANDWIDTH_INTERVAL))",
            "\t\twb_update_bandwidth(wb);",
            "\treturn ret;",
            "}",
            "bool noop_dirty_folio(struct address_space *mapping, struct folio *folio)",
            "{",
            "\tif (!folio_test_dirty(folio))",
            "\t\treturn !folio_test_set_dirty(folio);",
            "\treturn false;",
            "}",
            "static void folio_account_dirtied(struct folio *folio,",
            "\t\tstruct address_space *mapping)",
            "{",
            "\tstruct inode *inode = mapping->host;",
            "",
            "\ttrace_writeback_dirty_folio(folio, mapping);",
            "",
            "\tif (mapping_can_writeback(mapping)) {",
            "\t\tstruct bdi_writeback *wb;",
            "\t\tlong nr = folio_nr_pages(folio);",
            "",
            "\t\tinode_attach_wb(inode, folio);",
            "\t\twb = inode_to_wb(inode);",
            "",
            "\t\t__lruvec_stat_mod_folio(folio, NR_FILE_DIRTY, nr);",
            "\t\t__zone_stat_mod_folio(folio, NR_ZONE_WRITE_PENDING, nr);",
            "\t\t__node_stat_mod_folio(folio, NR_DIRTIED, nr);",
            "\t\twb_stat_mod(wb, WB_RECLAIMABLE, nr);",
            "\t\twb_stat_mod(wb, WB_DIRTIED, nr);",
            "\t\ttask_io_account_write(nr * PAGE_SIZE);",
            "\t\tcurrent->nr_dirtied += nr;",
            "\t\t__this_cpu_add(bdp_ratelimits, nr);",
            "",
            "\t\tmem_cgroup_track_foreign_dirty(folio, wb);",
            "\t}",
            "}",
            "void folio_account_cleaned(struct folio *folio, struct bdi_writeback *wb)",
            "{",
            "\tlong nr = folio_nr_pages(folio);",
            "",
            "\tlruvec_stat_mod_folio(folio, NR_FILE_DIRTY, -nr);",
            "\tzone_stat_mod_folio(folio, NR_ZONE_WRITE_PENDING, -nr);",
            "\twb_stat_mod(wb, WB_RECLAIMABLE, -nr);",
            "\ttask_io_account_cancelled_write(nr * PAGE_SIZE);",
            "}",
            "void __folio_mark_dirty(struct folio *folio, struct address_space *mapping,",
            "\t\t\t     int warn)",
            "{",
            "\tunsigned long flags;",
            "",
            "\txa_lock_irqsave(&mapping->i_pages, flags);",
            "\tif (folio->mapping) {\t/* Race with truncate? */",
            "\t\tWARN_ON_ONCE(warn && !folio_test_uptodate(folio));",
            "\t\tfolio_account_dirtied(folio, mapping);",
            "\t\t__xa_set_mark(&mapping->i_pages, folio_index(folio),",
            "\t\t\t\tPAGECACHE_TAG_DIRTY);",
            "\t}",
            "\txa_unlock_irqrestore(&mapping->i_pages, flags);",
            "}"
          ],
          "function_name": "writepage_cb, do_writepages, noop_dirty_folio, folio_account_dirtied, folio_account_cleaned, __folio_mark_dirty",
          "description": "writepage_cb 调用文件系统的writepage接口。do_writepages 执行批量写回操作，处理不同写回场景。noop_dirty_folio 默认处理脏页标记。folio_account_dirtied 和 folio_account_cleaned 维护脏页统计信息。__folio_mark_dirty 标记页面为脏状态。",
          "similarity": 0.48920637369155884
        },
        {
          "chunk_id": 10,
          "file_path": "mm/page-writeback.c",
          "start_line": 2033,
          "end_line": 2145,
          "content": [
            "int balance_dirty_pages_ratelimited_flags(struct address_space *mapping,",
            "\t\t\t\t\tunsigned int flags)",
            "{",
            "\tstruct inode *inode = mapping->host;",
            "\tstruct backing_dev_info *bdi = inode_to_bdi(inode);",
            "\tstruct bdi_writeback *wb = NULL;",
            "\tint ratelimit;",
            "\tint ret = 0;",
            "\tint *p;",
            "",
            "\tif (!(bdi->capabilities & BDI_CAP_WRITEBACK))",
            "\t\treturn ret;",
            "",
            "\tif (inode_cgwb_enabled(inode))",
            "\t\twb = wb_get_create_current(bdi, GFP_KERNEL);",
            "\tif (!wb)",
            "\t\twb = &bdi->wb;",
            "",
            "\tratelimit = current->nr_dirtied_pause;",
            "\tif (wb->dirty_exceeded)",
            "\t\tratelimit = min(ratelimit, 32 >> (PAGE_SHIFT - 10));",
            "",
            "\tpreempt_disable();",
            "\t/*",
            "\t * This prevents one CPU to accumulate too many dirtied pages without",
            "\t * calling into balance_dirty_pages(), which can happen when there are",
            "\t * 1000+ tasks, all of them start dirtying pages at exactly the same",
            "\t * time, hence all honoured too large initial task->nr_dirtied_pause.",
            "\t */",
            "\tp =  this_cpu_ptr(&bdp_ratelimits);",
            "\tif (unlikely(current->nr_dirtied >= ratelimit))",
            "\t\t*p = 0;",
            "\telse if (unlikely(*p >= ratelimit_pages)) {",
            "\t\t*p = 0;",
            "\t\tratelimit = 0;",
            "\t}",
            "\t/*",
            "\t * Pick up the dirtied pages by the exited tasks. This avoids lots of",
            "\t * short-lived tasks (eg. gcc invocations in a kernel build) escaping",
            "\t * the dirty throttling and livelock other long-run dirtiers.",
            "\t */",
            "\tp = this_cpu_ptr(&dirty_throttle_leaks);",
            "\tif (*p > 0 && current->nr_dirtied < ratelimit) {",
            "\t\tunsigned long nr_pages_dirtied;",
            "\t\tnr_pages_dirtied = min(*p, ratelimit - current->nr_dirtied);",
            "\t\t*p -= nr_pages_dirtied;",
            "\t\tcurrent->nr_dirtied += nr_pages_dirtied;",
            "\t}",
            "\tpreempt_enable();",
            "",
            "\tif (unlikely(current->nr_dirtied >= ratelimit))",
            "\t\tret = balance_dirty_pages(wb, current->nr_dirtied, flags);",
            "",
            "\twb_put(wb);",
            "\treturn ret;",
            "}",
            "void balance_dirty_pages_ratelimited(struct address_space *mapping)",
            "{",
            "\tbalance_dirty_pages_ratelimited_flags(mapping, 0);",
            "}",
            "bool wb_over_bg_thresh(struct bdi_writeback *wb)",
            "{",
            "\tstruct dirty_throttle_control gdtc_stor = { GDTC_INIT(wb) };",
            "\tstruct dirty_throttle_control mdtc_stor = { MDTC_INIT(wb, &gdtc_stor) };",
            "\tstruct dirty_throttle_control * const gdtc = &gdtc_stor;",
            "\tstruct dirty_throttle_control * const mdtc = mdtc_valid(&mdtc_stor) ?",
            "\t\t\t\t\t\t     &mdtc_stor : NULL;",
            "\tunsigned long reclaimable;",
            "\tunsigned long thresh;",
            "",
            "\t/*",
            "\t * Similar to balance_dirty_pages() but ignores pages being written",
            "\t * as we're trying to decide whether to put more under writeback.",
            "\t */",
            "\tgdtc->avail = global_dirtyable_memory();",
            "\tgdtc->dirty = global_node_page_state(NR_FILE_DIRTY);",
            "\tdomain_dirty_limits(gdtc);",
            "",
            "\tif (gdtc->dirty > gdtc->bg_thresh)",
            "\t\treturn true;",
            "",
            "\tthresh = wb_calc_thresh(gdtc->wb, gdtc->bg_thresh);",
            "\tif (thresh < 2 * wb_stat_error())",
            "\t\treclaimable = wb_stat_sum(wb, WB_RECLAIMABLE);",
            "\telse",
            "\t\treclaimable = wb_stat(wb, WB_RECLAIMABLE);",
            "",
            "\tif (reclaimable > thresh)",
            "\t\treturn true;",
            "",
            "\tif (mdtc) {",
            "\t\tunsigned long filepages, headroom, writeback;",
            "",
            "\t\tmem_cgroup_wb_stats(wb, &filepages, &headroom, &mdtc->dirty,",
            "\t\t\t\t    &writeback);",
            "\t\tmdtc_calc_avail(mdtc, filepages, headroom);",
            "\t\tdomain_dirty_limits(mdtc);\t/* ditto, ignore writeback */",
            "",
            "\t\tif (mdtc->dirty > mdtc->bg_thresh)",
            "\t\t\treturn true;",
            "",
            "\t\tthresh = wb_calc_thresh(mdtc->wb, mdtc->bg_thresh);",
            "\t\tif (thresh < 2 * wb_stat_error())",
            "\t\t\treclaimable = wb_stat_sum(wb, WB_RECLAIMABLE);",
            "\t\telse",
            "\t\t\treclaimable = wb_stat(wb, WB_RECLAIMABLE);",
            "",
            "\t\tif (reclaimable > thresh)",
            "\t\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "balance_dirty_pages_ratelimited_flags, balance_dirty_pages_ratelimited, wb_over_bg_thresh",
          "description": "balance_dirty_pages_ratelimited_flags 函数用于控制脏页的速率限制和写回决策，通过检查当前脏页数量与速率限制阈值比较，决定是否触发writeback。wb_over_bg_thresh 判断是否超过背景写回阈值，用于决定是否允许更多脏页产生。",
          "similarity": 0.4728958010673523
        }
      ]
    },
    {
      "source_file": "kernel/time/timecounter.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:54:23\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `time\\timecounter.c`\n\n---\n\n# `time/timecounter.c` 技术文档\n\n## 1. 文件概述\n\n`time/timecounter.c` 实现了 Linux 内核中的 **timecounter** 机制，用于将底层硬件周期计数器（cycle counter）的原始计数值转换为高精度的纳秒时间戳。该机制基于 `cyclecounter` 抽象，能够处理计数器溢出，并支持将任意周期时间戳转换为对应的纳秒时间，广泛应用于网络时间戳、PTP（精确时间协议）等需要高精度时间同步的场景。\n\n## 2. 核心功能\n\n### 数据结构\n- `struct timecounter`：时间计数器抽象，包含指向底层 `cyclecounter` 的指针、上次读取的周期值、当前纳秒时间戳、掩码和分数部分等字段。\n\n### 主要函数\n- `timecounter_init()`：初始化 `timecounter` 实例。\n- `timecounter_read()`：获取当前纳秒时间戳，并更新内部状态。\n- `timecounter_read_delta()`（静态）：计算自上次调用以来经过的纳秒数。\n- `timecounter_cyc2time()`：将给定的周期计数值转换为对应的纳秒时间戳（支持向前或向后转换）。\n- `cc_cyc2ns_backwards()`（静态）：辅助函数，用于反向（历史时间）的周期到纳秒转换。\n\n## 3. 关键实现\n\n### 初始化 (`timecounter_init`)\n- 将用户提供的 `cyclecounter` 指针保存到 `tc->cc`。\n- 读取当前硬件周期值作为 `cycle_last`。\n- 设置初始纳秒时间戳 `nsec` 为 `start_tstamp`。\n- 计算掩码 `mask = (1ULL << cc->shift) - 1`，用于后续溢出处理。\n- 初始化分数部分 `frac = 0`，用于高精度纳秒转换。\n\n### 时间读取 (`timecounter_read`)\n- 调用 `timecounter_read_delta()` 获取自上次读取以来的纳秒增量。\n- 将增量累加到 `tc->nsec`，并返回更新后的时间戳。\n- **注意**：首次调用返回值未定义，仅用于初始化内部状态。\n\n### 增量计算 (`timecounter_read_delta`)\n- 读取当前周期值 `cycle_now`。\n- 计算与上次值的差值 `cycle_delta`，并通过 `& cc->mask` 处理单次溢出。\n- 使用 `cyclecounter_cyc2ns()` 将周期差值转换为纳秒偏移量（含分数精度补偿）。\n- 更新 `cycle_last` 为当前值。\n\n### 周期转时间 (`timecounter_cyc2time`)\n- 计算目标周期 `cycle_tstamp` 与 `cycle_last` 的差值 `delta`。\n- **智能方向判断**：若 `delta > mask / 2`，说明 `cycle_tstamp` 实际是历史时间（因计数器回绕），则反向计算。\n  - 使用 `cc_cyc2ns_backwards()` 从当前纳秒时间减去对应的历史偏移。\n- 否则视为未来时间，使用标准 `cyclecounter_cyc2ns()` 正向累加。\n- 该设计确保即使在计数器溢出边界附近，也能正确解析时间戳。\n\n### 反向转换 (`cc_cyc2ns_backwards`)\n- 与 `cyclecounter_cyc2ns()` 类似，但先减去分数部分再右移，适用于历史时间计算。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/timecounter.h>`：定义 `struct timecounter` 和相关函数原型。\n  - `<linux/export.h>`：提供 `EXPORT_SYMBOL_GPL` 宏，用于导出符号供其他模块使用。\n- **功能依赖**：\n  - 依赖 `cyclecounter` 子系统（定义在 `include/linux/cyclecounter.h`），特别是 `cyclecounter_cyc2ns()` 函数。\n  - 依赖底层硬件驱动提供符合 `cyclecounter` 接口的周期计数器（如 `read()` 函数和 `mask`/`mult`/`shift` 参数）。\n\n## 5. 使用场景\n\n- **网络时间戳**：在网络驱动中，硬件捕获数据包到达/发送时的周期计数值，通过 `timecounter_cyc2time()` 转换为精确的纳秒时间戳，用于 PTP（IEEE 1588）等协议。\n- **高精度定时**：在需要比 `jiffies` 或 `ktime` 更高分辨率的场景中，结合硬件计数器使用。\n- **跨溢出时间计算**：当底层计数器位宽有限（如 32 位）且频率较高时，频繁溢出，`timecounter` 能透明处理单次溢出，保证时间连续性。\n- **时间同步子系统**：作为 PTP 硬件时钟（PHC）实现的基础组件，将硬件寄存器值映射到系统时间域。",
      "similarity": 0.506420910358429,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/time/timecounter.c",
          "start_line": 8,
          "end_line": 77,
          "content": [
            "void timecounter_init(struct timecounter *tc,",
            "\t\t      const struct cyclecounter *cc,",
            "\t\t      u64 start_tstamp)",
            "{",
            "\ttc->cc = cc;",
            "\ttc->cycle_last = cc->read(cc);",
            "\ttc->nsec = start_tstamp;",
            "\ttc->mask = (1ULL << cc->shift) - 1;",
            "\ttc->frac = 0;",
            "}",
            "static u64 timecounter_read_delta(struct timecounter *tc)",
            "{",
            "\tu64 cycle_now, cycle_delta;",
            "\tu64 ns_offset;",
            "",
            "\t/* read cycle counter: */",
            "\tcycle_now = tc->cc->read(tc->cc);",
            "",
            "\t/* calculate the delta since the last timecounter_read_delta(): */",
            "\tcycle_delta = (cycle_now - tc->cycle_last) & tc->cc->mask;",
            "",
            "\t/* convert to nanoseconds: */",
            "\tns_offset = cyclecounter_cyc2ns(tc->cc, cycle_delta,",
            "\t\t\t\t\ttc->mask, &tc->frac);",
            "",
            "\t/* update time stamp of timecounter_read_delta() call: */",
            "\ttc->cycle_last = cycle_now;",
            "",
            "\treturn ns_offset;",
            "}",
            "u64 timecounter_read(struct timecounter *tc)",
            "{",
            "\tu64 nsec;",
            "",
            "\t/* increment time by nanoseconds since last call */",
            "\tnsec = timecounter_read_delta(tc);",
            "\tnsec += tc->nsec;",
            "\ttc->nsec = nsec;",
            "",
            "\treturn nsec;",
            "}",
            "static u64 cc_cyc2ns_backwards(const struct cyclecounter *cc,",
            "\t\t\t       u64 cycles, u64 mask, u64 frac)",
            "{",
            "\tu64 ns = (u64) cycles;",
            "",
            "\tns = ((ns * cc->mult) - frac) >> cc->shift;",
            "",
            "\treturn ns;",
            "}",
            "u64 timecounter_cyc2time(const struct timecounter *tc,",
            "\t\t\t u64 cycle_tstamp)",
            "{",
            "\tu64 delta = (cycle_tstamp - tc->cycle_last) & tc->cc->mask;",
            "\tu64 nsec = tc->nsec, frac = tc->frac;",
            "",
            "\t/*",
            "\t * Instead of always treating cycle_tstamp as more recent",
            "\t * than tc->cycle_last, detect when it is too far in the",
            "\t * future and treat it as old time stamp instead.",
            "\t */",
            "\tif (delta > tc->cc->mask / 2) {",
            "\t\tdelta = (tc->cycle_last - cycle_tstamp) & tc->cc->mask;",
            "\t\tnsec -= cc_cyc2ns_backwards(tc->cc, delta, tc->mask, frac);",
            "\t} else {",
            "\t\tnsec += cyclecounter_cyc2ns(tc->cc, delta, tc->mask, &frac);",
            "\t}",
            "",
            "\treturn nsec;",
            "}"
          ],
          "function_name": "timecounter_init, timecounter_read_delta, timecounter_read, cc_cyc2ns_backwards, timecounter_cyc2time",
          "description": "实现时间计数器初始化与时间读取逻辑，包含周期计数转纳秒计算、时间差获取及时间戳更新功能",
          "similarity": 0.4411469101905823
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/time/timecounter.c",
          "start_line": 1,
          "end_line": 7,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0+",
            "/*",
            " * Based on clocksource code. See commit 74d23cc704d1",
            " */",
            "#include <linux/export.h>",
            "#include <linux/timecounter.h>",
            ""
          ],
          "function_name": null,
          "description": "声明时间计数器模块的许可证及包含必要头文件，为后续时间计数器实现提供基础",
          "similarity": 0.36841297149658203
        }
      ]
    },
    {
      "source_file": "kernel/printk/printk_ringbuffer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:34:17\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `printk\\printk_ringbuffer.c`\n\n---\n\n# printk_ringbuffer.c 技术文档\n\n## 文件概述\n\n`printk_ringbuffer.c` 实现了 Linux 内核中用于日志记录的无锁环形缓冲区（printk ringbuffer）核心逻辑。该缓冲区用于高效、并发安全地存储内核日志消息（printk 输出），支持多写者-多读者模型，无需使用传统锁机制，从而在高并发或中断上下文中也能安全使用。该实现是现代 printk 子系统的基础组件，用于替代旧的 log_buf。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`printk_ringbuffer`**：顶层环形缓冲区结构，包含三个内部环形缓冲区：\n  - **`desc_ring`**：描述符环，存储每条日志记录的元数据（序列号、时间戳、日志级别、状态等）及指向文本数据的逻辑位置。\n  - **`text_data_ring`**：文本数据环，以字节为单位存储日志文本内容，每个数据块以描述符 ID 开头，后接实际文本。\n  - **`info` 数组**：与描述符一一对应的 `printk_info` 结构数组，存储日志记录的详细元数据。\n\n- **描述符状态（`state_var`）**：\n  - `reserved`：写者正在修改记录。\n  - `committed`：记录已提交，数据一致，但可被原写者重新打开修改。\n  - `finalized`：记录已最终确定，对读者可见，不可再修改。\n  - `reusable`：记录可被回收复用。\n  - `miss`（伪状态）：查询时发现描述符 ID 不匹配。\n\n- **`blk_lpos`**：逻辑位置结构，用于在数据环中定位数据块的起始和结束位置。\n\n### 主要函数（接口）\n\n- `prb_reserve()`：为新日志记录预留空间，返回保留条目。\n- `prb_commit()`：提交当前记录（可后续重新打开）。\n- `prb_final_commit()`：提交并最终确定记录，使其对读者可见。\n- `prb_read_valid()` / `prb_read_valid_info()`：安全读取指定序列号的日志记录及其元数据。\n- `prb_first_valid_seq()` / `prb_next_seq()`：获取有效日志序列范围。\n\n## 关键实现\n\n### 无锁同步机制\n\n通过原子操作更新描述符的 `state_var` 字段（将 ID 与状态位打包），实现写者与读者之间的无锁同步。状态转换遵循严格顺序：`reserved → committed → finalized → reusable`。\n\n### 描述符生命周期管理\n\n- **预留（Reserve）**：分配新描述符，状态设为 `reserved`。\n- **提交（Commit）**：写入完成后设为 `committed`，数据一致但可重入。\n- **最终确定（Finalize）**：在以下任一情况下自动或显式触发：\n  1. 调用 `prb_final_commit()`；\n  2. 下一条记录被预留且当前记录已 `committed`；\n  3. 提交一条记录时已有更新记录存在。\n- **回收（Reuse）**：缓冲区满时，将最旧的 `finalized` 或 `reusable` 记录状态转为 `reusable`，并推进 `tail_id`。\n\n### 数据环的环绕处理\n\n当日志文本跨越缓冲区末尾时，仅在末尾存储描述符 ID，完整数据块（ID + 文本）从缓冲区起始位置存储。`blk_lpos` 正确指向环绕前的 ID 位置，保证逻辑连续性。\n\n### 尾部推进安全约束\n\n`tail_id` 和 `tail_lpos` 仅在对应记录处于 `committed` 或 `reusable` 状态时才可推进，确保始终保留至少一条有效日志的序列号，避免读者读取到无效数据。\n\n### 元数据一致性保障\n\n读取 `printk_info` 时，需在读取前后两次检查对应描述符状态，确保元数据未在读取过程中被覆盖或修改（ABA 问题防护）。\n\n## 依赖关系\n\n- **内部依赖**：\n  - `printk_ringbuffer.h`：定义核心数据结构和 API。\n  - `internal.h`：包含 printk 子系统内部辅助函数和定义。\n- **内核头文件**：\n  - `<linux/kernel.h>`、`<linux/irqflags.h>`、`<linux/string.h>`、`<linux/bug.h>`：提供基础内核功能、原子操作、内存操作及调试支持。\n- **被 printk.c 调用**：作为 printk 日志后端，由 `printk.c` 中的 `vprintk_store()` 等函数调用其预留/提交接口。\n\n## 使用场景\n\n- **内核日志记录**：所有 `printk()` 调用最终通过此环形缓冲区存储日志消息。\n- **高并发环境**：在中断上下文、NMI、SMP 系统中安全记录日志，无需睡眠或持有自旋锁。\n- **日志读取**：`/dev/kmsg`、`dmesg` 命令及内核日志守护进程通过此缓冲区读取日志。\n- **崩溃转储**：在系统崩溃（如 panic）时，确保关键日志能被可靠记录和后续分析。\n- **动态日志扩展**：支持在提交后、最终确定前扩展日志内容（如追加堆栈信息），适用于延迟格式化场景。",
      "similarity": 0.4965234398841858,
      "chunks": [
        {
          "chunk_id": 9,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 2005,
          "end_line": 2140,
          "content": [
            "u64 prb_next_reserve_seq(struct printk_ringbuffer *rb)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tunsigned long last_finalized_id;",
            "\tatomic_long_t *state_var;",
            "\tu64 last_finalized_seq;",
            "\tunsigned long head_id;",
            "\tstruct prb_desc desc;",
            "\tunsigned long diff;",
            "\tstruct prb_desc *d;",
            "\tint err;",
            "",
            "\t/*",
            "\t * It may not be possible to read a sequence number for @head_id.",
            "\t * So the ID of @last_finailzed_seq is used to calculate what the",
            "\t * sequence number of @head_id will be.",
            "\t */",
            "",
            "try_again:",
            "\tlast_finalized_seq = desc_last_finalized_seq(rb);",
            "",
            "\t/*",
            "\t * @head_id is loaded after @last_finalized_seq to ensure that",
            "\t * it points to the record with @last_finalized_seq or newer.",
            "\t *",
            "\t * Memory barrier involvement:",
            "\t *",
            "\t * If desc_last_finalized_seq:A reads from",
            "\t * desc_update_last_finalized:A, then",
            "\t * prb_next_reserve_seq:A reads from desc_reserve:D.",
            "\t *",
            "\t * Relies on:",
            "\t *",
            "\t * RELEASE from desc_reserve:D to desc_update_last_finalized:A",
            "\t *    matching",
            "\t * ACQUIRE from desc_last_finalized_seq:A to prb_next_reserve_seq:A",
            "\t *",
            "\t * Note: desc_reserve:D and desc_update_last_finalized:A can be",
            "\t *       different CPUs. However, the desc_update_last_finalized:A CPU",
            "\t *       (which performs the release) must have previously seen",
            "\t *       desc_read:C, which implies desc_reserve:D can be seen.",
            "\t */",
            "\thead_id = atomic_long_read(&desc_ring->head_id); /* LMM(prb_next_reserve_seq:A) */",
            "",
            "\td = to_desc(desc_ring, last_finalized_seq);",
            "\tstate_var = &d->state_var;",
            "",
            "\t/* Extract the ID, used to specify the descriptor to read. */",
            "\tlast_finalized_id = DESC_ID(atomic_long_read(state_var));",
            "",
            "\t/* Ensure @last_finalized_id is correct. */",
            "\terr = desc_read_finalized_seq(desc_ring, last_finalized_id, last_finalized_seq, &desc);",
            "",
            "\tif (err == -EINVAL) {",
            "\t\tif (last_finalized_seq == 0) {",
            "\t\t\t/*",
            "\t\t\t * No record has been finalized or even reserved yet.",
            "\t\t\t *",
            "\t\t\t * The @head_id is initialized such that the first",
            "\t\t\t * increment will yield the first record (seq=0).",
            "\t\t\t * Handle it separately to avoid a negative @diff",
            "\t\t\t * below.",
            "\t\t\t */",
            "\t\t\tif (head_id == DESC0_ID(desc_ring->count_bits))",
            "\t\t\t\treturn 0;",
            "",
            "\t\t\t/*",
            "\t\t\t * One or more descriptors are already reserved. Use",
            "\t\t\t * the descriptor ID of the first one (@seq=0) for",
            "\t\t\t * the @diff below.",
            "\t\t\t */",
            "\t\t\tlast_finalized_id = DESC0_ID(desc_ring->count_bits) + 1;",
            "\t\t} else {",
            "\t\t\t/* Record must have been overwritten. Try again. */",
            "\t\t\tgoto try_again;",
            "\t\t}",
            "\t}",
            "",
            "\t/* Diff of known descriptor IDs to compute related sequence numbers. */",
            "\tdiff = head_id - last_finalized_id;",
            "",
            "\t/*",
            "\t * @head_id points to the most recently reserved record, but this",
            "\t * function returns the sequence number that will be assigned to the",
            "\t * next (not yet reserved) record. Thus +1 is needed.",
            "\t */",
            "\treturn (last_finalized_seq + diff + 1);",
            "}",
            "static bool _prb_read_valid(struct printk_ringbuffer *rb, u64 *seq,",
            "\t\t\t    struct printk_record *r, unsigned int *line_count)",
            "{",
            "\tu64 tail_seq;",
            "\tint err;",
            "",
            "\twhile ((err = prb_read(rb, *seq, r, line_count))) {",
            "\t\ttail_seq = prb_first_seq(rb);",
            "",
            "\t\tif (*seq < tail_seq) {",
            "\t\t\t/*",
            "\t\t\t * Behind the tail. Catch up and try again. This",
            "\t\t\t * can happen for -ENOENT and -EINVAL cases.",
            "\t\t\t */",
            "\t\t\t*seq = tail_seq;",
            "",
            "\t\t} else if (err == -ENOENT) {",
            "\t\t\t/* Record exists, but the data was lost. Skip. */",
            "\t\t\t(*seq)++;",
            "",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * Non-existent/non-finalized record. Must stop.",
            "\t\t\t *",
            "\t\t\t * For panic situations it cannot be expected that",
            "\t\t\t * non-finalized records will become finalized. But",
            "\t\t\t * there may be other finalized records beyond that",
            "\t\t\t * need to be printed for a panic situation. If this",
            "\t\t\t * is the panic CPU, skip this",
            "\t\t\t * non-existent/non-finalized record unless it is",
            "\t\t\t * at or beyond the head, in which case it is not",
            "\t\t\t * possible to continue.",
            "\t\t\t *",
            "\t\t\t * Note that new messages printed on panic CPU are",
            "\t\t\t * finalized when we are here. The only exception",
            "\t\t\t * might be the last message without trailing newline.",
            "\t\t\t * But it would have the sequence number returned",
            "\t\t\t * by \"prb_next_reserve_seq() - 1\".",
            "\t\t\t */",
            "\t\t\tif (this_cpu_in_panic() && ((*seq + 1) < prb_next_reserve_seq(rb)))",
            "\t\t\t\t(*seq)++;",
            "\t\t\telse",
            "\t\t\t\treturn false;",
            "\t\t}",
            "\t}",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "prb_next_reserve_seq, _prb_read_valid",
          "description": "计算下次预留序列号并校验记录有效性。prb_next_reserve_seq根据头指针和最后确认序列号推导下次预留序号，_prb_read_valid处理读取过程中的无效记录跳过及序列号递增逻辑。",
          "similarity": 0.4713224470615387
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 879,
          "end_line": 1003,
          "content": [
            "static bool desc_reserve(struct printk_ringbuffer *rb, unsigned long *id_out)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tunsigned long prev_state_val;",
            "\tunsigned long id_prev_wrap;",
            "\tstruct prb_desc *desc;",
            "\tunsigned long head_id;",
            "\tunsigned long id;",
            "",
            "\thead_id = atomic_long_read(&desc_ring->head_id); /* LMM(desc_reserve:A) */",
            "",
            "\tdo {",
            "\t\tid = DESC_ID(head_id + 1);",
            "\t\tid_prev_wrap = DESC_ID_PREV_WRAP(desc_ring, id);",
            "",
            "\t\t/*",
            "\t\t * Guarantee the head ID is read before reading the tail ID.",
            "\t\t * Since the tail ID is updated before the head ID, this",
            "\t\t * guarantees that @id_prev_wrap is never ahead of the tail",
            "\t\t * ID. This pairs with desc_reserve:D.",
            "\t\t *",
            "\t\t * Memory barrier involvement:",
            "\t\t *",
            "\t\t * If desc_reserve:A reads from desc_reserve:D, then",
            "\t\t * desc_reserve:C reads from desc_push_tail:B.",
            "\t\t *",
            "\t\t * Relies on:",
            "\t\t *",
            "\t\t * MB from desc_push_tail:B to desc_reserve:D",
            "\t\t *    matching",
            "\t\t * RMB from desc_reserve:A to desc_reserve:C",
            "\t\t *",
            "\t\t * Note: desc_push_tail:B and desc_reserve:D can be different",
            "\t\t *       CPUs. However, the desc_reserve:D CPU (which performs",
            "\t\t *       the full memory barrier) must have previously seen",
            "\t\t *       desc_push_tail:B.",
            "\t\t */",
            "\t\tsmp_rmb(); /* LMM(desc_reserve:B) */",
            "",
            "\t\tif (id_prev_wrap == atomic_long_read(&desc_ring->tail_id",
            "\t\t\t\t\t\t    )) { /* LMM(desc_reserve:C) */",
            "\t\t\t/*",
            "\t\t\t * Make space for the new descriptor by",
            "\t\t\t * advancing the tail.",
            "\t\t\t */",
            "\t\t\tif (!desc_push_tail(rb, id_prev_wrap))",
            "\t\t\t\treturn false;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * 1. Guarantee the tail ID is read before validating the",
            "\t\t *    recycled descriptor state. A read memory barrier is",
            "\t\t *    sufficient for this. This pairs with desc_push_tail:B.",
            "\t\t *",
            "\t\t *    Memory barrier involvement:",
            "\t\t *",
            "\t\t *    If desc_reserve:C reads from desc_push_tail:B, then",
            "\t\t *    desc_reserve:E reads from desc_make_reusable:A.",
            "\t\t *",
            "\t\t *    Relies on:",
            "\t\t *",
            "\t\t *    MB from desc_make_reusable:A to desc_push_tail:B",
            "\t\t *       matching",
            "\t\t *    RMB from desc_reserve:C to desc_reserve:E",
            "\t\t *",
            "\t\t *    Note: desc_make_reusable:A and desc_push_tail:B can be",
            "\t\t *          different CPUs. However, the desc_push_tail:B CPU",
            "\t\t *          (which performs the full memory barrier) must have",
            "\t\t *          previously seen desc_make_reusable:A.",
            "\t\t *",
            "\t\t * 2. Guarantee the tail ID is stored before storing the head",
            "\t\t *    ID. This pairs with desc_reserve:B.",
            "\t\t *",
            "\t\t * 3. Guarantee any data ring tail changes are stored before",
            "\t\t *    recycling the descriptor. Data ring tail changes can",
            "\t\t *    happen via desc_push_tail()->data_push_tail(). A full",
            "\t\t *    memory barrier is needed since another CPU may have",
            "\t\t *    pushed the data ring tails. This pairs with",
            "\t\t *    data_push_tail:B.",
            "\t\t *",
            "\t\t * 4. Guarantee a new tail ID is stored before recycling the",
            "\t\t *    descriptor. A full memory barrier is needed since",
            "\t\t *    another CPU may have pushed the tail ID. This pairs",
            "\t\t *    with desc_push_tail:C and this also pairs with",
            "\t\t *    prb_first_seq:C.",
            "\t\t *",
            "\t\t * 5. Guarantee the head ID is stored before trying to",
            "\t\t *    finalize the previous descriptor. This pairs with",
            "\t\t *    _prb_commit:B.",
            "\t\t */",
            "\t} while (!atomic_long_try_cmpxchg(&desc_ring->head_id, &head_id,",
            "\t\t\t\t\t  id)); /* LMM(desc_reserve:D) */",
            "",
            "\tdesc = to_desc(desc_ring, id);",
            "",
            "\t/*",
            "\t * If the descriptor has been recycled, verify the old state val.",
            "\t * See \"ABA Issues\" about why this verification is performed.",
            "\t */",
            "\tprev_state_val = atomic_long_read(&desc->state_var); /* LMM(desc_reserve:E) */",
            "\tif (prev_state_val &&",
            "\t    get_desc_state(id_prev_wrap, prev_state_val) != desc_reusable) {",
            "\t\tWARN_ON_ONCE(1);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/*",
            "\t * Assign the descriptor a new ID and set its state to reserved.",
            "\t * See \"ABA Issues\" about why cmpxchg() instead of set() is used.",
            "\t *",
            "\t * Guarantee the new descriptor ID and state is stored before making",
            "\t * any other changes. A write memory barrier is sufficient for this.",
            "\t * This pairs with desc_read:D.",
            "\t */",
            "\tif (!atomic_long_try_cmpxchg(&desc->state_var, &prev_state_val,",
            "\t\t\tDESC_SV(id, desc_reserved))) { /* LMM(desc_reserve:F) */",
            "\t\tWARN_ON_ONCE(1);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/* Now data in @desc can be modified: LMM(desc_reserve:G) */",
            "",
            "\t*id_out = id;",
            "\treturn true;",
            "}"
          ],
          "function_name": "desc_reserve",
          "description": "实现描述符保留逻辑，通过CAS操作获取新ID并设置保留状态，含多重内存屏障保障状态变更顺序与数据一致性",
          "similarity": 0.4687822163105011
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 771,
          "end_line": 876,
          "content": [
            "static bool desc_push_tail(struct printk_ringbuffer *rb,",
            "\t\t\t   unsigned long tail_id)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tenum desc_state d_state;",
            "\tstruct prb_desc desc;",
            "",
            "\td_state = desc_read(desc_ring, tail_id, &desc, NULL, NULL);",
            "",
            "\tswitch (d_state) {",
            "\tcase desc_miss:",
            "\t\t/*",
            "\t\t * If the ID is exactly 1 wrap behind the expected, it is",
            "\t\t * in the process of being reserved by another writer and",
            "\t\t * must be considered reserved.",
            "\t\t */",
            "\t\tif (DESC_ID(atomic_long_read(&desc.state_var)) ==",
            "\t\t    DESC_ID_PREV_WRAP(desc_ring, tail_id)) {",
            "\t\t\treturn false;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * The ID has changed. Another writer must have pushed the",
            "\t\t * tail and recycled the descriptor already. Success is",
            "\t\t * returned because the caller is only interested in the",
            "\t\t * specified tail being pushed, which it was.",
            "\t\t */",
            "\t\treturn true;",
            "\tcase desc_reserved:",
            "\tcase desc_committed:",
            "\t\treturn false;",
            "\tcase desc_finalized:",
            "\t\tdesc_make_reusable(desc_ring, tail_id);",
            "\t\tbreak;",
            "\tcase desc_reusable:",
            "\t\tbreak;",
            "\t}",
            "",
            "\t/*",
            "\t * Data blocks must be invalidated before their associated",
            "\t * descriptor can be made available for recycling. Invalidating",
            "\t * them later is not possible because there is no way to trust",
            "\t * data blocks once their associated descriptor is gone.",
            "\t */",
            "",
            "\tif (!data_push_tail(rb, desc.text_blk_lpos.next))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Check the next descriptor after @tail_id before pushing the tail",
            "\t * to it because the tail must always be in a finalized or reusable",
            "\t * state. The implementation of prb_first_seq() relies on this.",
            "\t *",
            "\t * A successful read implies that the next descriptor is less than or",
            "\t * equal to @head_id so there is no risk of pushing the tail past the",
            "\t * head.",
            "\t */",
            "\td_state = desc_read(desc_ring, DESC_ID(tail_id + 1), &desc,",
            "\t\t\t    NULL, NULL); /* LMM(desc_push_tail:A) */",
            "",
            "\tif (d_state == desc_finalized || d_state == desc_reusable) {",
            "\t\t/*",
            "\t\t * Guarantee any descriptor states that have transitioned to",
            "\t\t * reusable are stored before pushing the tail ID. This allows",
            "\t\t * verifying the recycled descriptor state. A full memory",
            "\t\t * barrier is needed since other CPUs may have made the",
            "\t\t * descriptor states reusable. This pairs with desc_reserve:D.",
            "\t\t */",
            "\t\tatomic_long_cmpxchg(&desc_ring->tail_id, tail_id,",
            "\t\t\t\t    DESC_ID(tail_id + 1)); /* LMM(desc_push_tail:B) */",
            "\t} else {",
            "\t\t/*",
            "\t\t * Guarantee the last state load from desc_read() is before",
            "\t\t * reloading @tail_id in order to see a new tail ID in the",
            "\t\t * case that the descriptor has been recycled. This pairs",
            "\t\t * with desc_reserve:D.",
            "\t\t *",
            "\t\t * Memory barrier involvement:",
            "\t\t *",
            "\t\t * If desc_push_tail:A reads from desc_reserve:F, then",
            "\t\t * desc_push_tail:D reads from desc_push_tail:B.",
            "\t\t *",
            "\t\t * Relies on:",
            "\t\t *",
            "\t\t * MB from desc_push_tail:B to desc_reserve:F",
            "\t\t *    matching",
            "\t\t * RMB from desc_push_tail:A to desc_push_tail:D",
            "\t\t *",
            "\t\t * Note: desc_push_tail:B and desc_reserve:F can be different",
            "\t\t *       CPUs. However, the desc_reserve:F CPU (which performs",
            "\t\t *       the full memory barrier) must have previously seen",
            "\t\t *       desc_push_tail:B.",
            "\t\t */",
            "\t\tsmp_rmb(); /* LMM(desc_push_tail:C) */",
            "",
            "\t\t/*",
            "\t\t * Re-check the tail ID. The descriptor following @tail_id is",
            "\t\t * not in an allowed tail state. But if the tail has since",
            "\t\t * been moved by another CPU, then it does not matter.",
            "\t\t */",
            "\t\tif (atomic_long_read(&desc_ring->tail_id) == tail_id) /* LMM(desc_push_tail:D) */",
            "\t\t\treturn false;",
            "\t}",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "desc_push_tail",
          "description": "推进描述符环尾指针，检查后续描述符状态合法性，通过内存屏障保障状态变更顺序以避免非法尾指针推进",
          "similarity": 0.460053026676178
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 2179,
          "end_line": 2259,
          "content": [
            "bool prb_read_valid(struct printk_ringbuffer *rb, u64 seq,",
            "\t\t    struct printk_record *r)",
            "{",
            "\treturn _prb_read_valid(rb, &seq, r, NULL);",
            "}",
            "bool prb_read_valid_info(struct printk_ringbuffer *rb, u64 seq,",
            "\t\t\t struct printk_info *info, unsigned int *line_count)",
            "{",
            "\tstruct printk_record r;",
            "",
            "\tprb_rec_init_rd(&r, info, NULL, 0);",
            "",
            "\treturn _prb_read_valid(rb, &seq, &r, line_count);",
            "}",
            "u64 prb_first_valid_seq(struct printk_ringbuffer *rb)",
            "{",
            "\tu64 seq = 0;",
            "",
            "\tif (!_prb_read_valid(rb, &seq, NULL, NULL))",
            "\t\treturn 0;",
            "",
            "\treturn seq;",
            "}",
            "u64 prb_next_seq(struct printk_ringbuffer *rb)",
            "{",
            "\tu64 seq;",
            "",
            "\tseq = desc_last_finalized_seq(rb);",
            "",
            "\t/*",
            "\t * Begin searching after the last finalized record.",
            "\t *",
            "\t * On 0, the search must begin at 0 because of hack#2",
            "\t * of the bootstrapping phase it is not known if a",
            "\t * record at index 0 exists.",
            "\t */",
            "\tif (seq != 0)",
            "\t\tseq++;",
            "",
            "\t/*",
            "\t * The information about the last finalized @seq might be inaccurate.",
            "\t * Search forward to find the current one.",
            "\t */",
            "\twhile (_prb_read_valid(rb, &seq, NULL, NULL))",
            "\t\tseq++;",
            "",
            "\treturn seq;",
            "}",
            "void prb_init(struct printk_ringbuffer *rb,",
            "\t      char *text_buf, unsigned int textbits,",
            "\t      struct prb_desc *descs, unsigned int descbits,",
            "\t      struct printk_info *infos)",
            "{",
            "\tmemset(descs, 0, _DESCS_COUNT(descbits) * sizeof(descs[0]));",
            "\tmemset(infos, 0, _DESCS_COUNT(descbits) * sizeof(infos[0]));",
            "",
            "\trb->desc_ring.count_bits = descbits;",
            "\trb->desc_ring.descs = descs;",
            "\trb->desc_ring.infos = infos;",
            "\tatomic_long_set(&rb->desc_ring.head_id, DESC0_ID(descbits));",
            "\tatomic_long_set(&rb->desc_ring.tail_id, DESC0_ID(descbits));",
            "\tatomic_long_set(&rb->desc_ring.last_finalized_seq, 0);",
            "",
            "\trb->text_data_ring.size_bits = textbits;",
            "\trb->text_data_ring.data = text_buf;",
            "\tatomic_long_set(&rb->text_data_ring.head_lpos, BLK0_LPOS(textbits));",
            "\tatomic_long_set(&rb->text_data_ring.tail_lpos, BLK0_LPOS(textbits));",
            "",
            "\tatomic_long_set(&rb->fail, 0);",
            "",
            "\tatomic_long_set(&(descs[_DESCS_COUNT(descbits) - 1].state_var), DESC0_SV(descbits));",
            "\tdescs[_DESCS_COUNT(descbits) - 1].text_blk_lpos.begin = FAILED_LPOS;",
            "\tdescs[_DESCS_COUNT(descbits) - 1].text_blk_lpos.next = FAILED_LPOS;",
            "",
            "\tinfos[0].seq = -(u64)_DESCS_COUNT(descbits);",
            "\tinfos[_DESCS_COUNT(descbits) - 1].seq = 0;",
            "}",
            "unsigned int prb_record_text_space(struct prb_reserved_entry *e)",
            "{",
            "\treturn e->text_space;",
            "}"
          ],
          "function_name": "prb_read_valid, prb_read_valid_info, prb_first_valid_seq, prb_next_seq, prb_init, prb_record_text_space",
          "description": "该代码段实现了 printk 环形缓冲区的读写控制逻辑，主要包含以下内容：  \n1. `prb_read_valid` 系列函数通过 `_prb_read_valid` 检查序列号有效性并填充记录信息，用于日志读取校验；  \n2. `prb_first_valid_seq` 和 `prb_next_seq` 用于定位首个有效序列号与后续序列号，解决缓冲区初始状态不确定的问题；  \n3. `prb_init` 初始化缓冲区结构，配置描述符环和文本数据环的元数据，`prb_record_text_space` 返回记录文本空间大小。注：`_prb_read_valid` 实现未完整展示，需结合上下文理解其核心逻辑。",
          "similarity": 0.453259140253067
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/printk/printk_ringbuffer.c",
          "start_line": 1477,
          "end_line": 1647,
          "content": [
            "static u64 desc_last_finalized_seq(struct printk_ringbuffer *rb)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tunsigned long ulseq;",
            "",
            "\t/*",
            "\t * Guarantee the sequence number is loaded before loading the",
            "\t * associated record in order to guarantee that the record can be",
            "\t * seen by this CPU. This pairs with desc_update_last_finalized:A.",
            "\t */",
            "\tulseq = atomic_long_read_acquire(&desc_ring->last_finalized_seq",
            "\t\t\t\t\t); /* LMM(desc_last_finalized_seq:A) */",
            "",
            "\treturn __ulseq_to_u64seq(rb, ulseq);",
            "}",
            "static void desc_update_last_finalized(struct printk_ringbuffer *rb)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tu64 old_seq = desc_last_finalized_seq(rb);",
            "\tunsigned long oldval;",
            "\tunsigned long newval;",
            "\tu64 finalized_seq;",
            "\tu64 try_seq;",
            "",
            "try_again:",
            "\tfinalized_seq = old_seq;",
            "\ttry_seq = finalized_seq + 1;",
            "",
            "\t/* Try to find later finalized records. */",
            "\twhile (_prb_read_valid(rb, &try_seq, NULL, NULL)) {",
            "\t\tfinalized_seq = try_seq;",
            "\t\ttry_seq++;",
            "\t}",
            "",
            "\t/* No update needed if no later finalized record was found. */",
            "\tif (finalized_seq == old_seq)",
            "\t\treturn;",
            "",
            "\toldval = __u64seq_to_ulseq(old_seq);",
            "\tnewval = __u64seq_to_ulseq(finalized_seq);",
            "",
            "\t/*",
            "\t * Set the sequence number of a later finalized record that has been",
            "\t * seen.",
            "\t *",
            "\t * Guarantee the record data is visible to other CPUs before storing",
            "\t * its sequence number. This pairs with desc_last_finalized_seq:A.",
            "\t *",
            "\t * Memory barrier involvement:",
            "\t *",
            "\t * If desc_last_finalized_seq:A reads from",
            "\t * desc_update_last_finalized:A, then desc_read:A reads from",
            "\t * _prb_commit:B.",
            "\t *",
            "\t * Relies on:",
            "\t *",
            "\t * RELEASE from _prb_commit:B to desc_update_last_finalized:A",
            "\t *    matching",
            "\t * ACQUIRE from desc_last_finalized_seq:A to desc_read:A",
            "\t *",
            "\t * Note: _prb_commit:B and desc_update_last_finalized:A can be",
            "\t *       different CPUs. However, the desc_update_last_finalized:A",
            "\t *       CPU (which performs the release) must have previously seen",
            "\t *       _prb_commit:B.",
            "\t */",
            "\tif (!atomic_long_try_cmpxchg_release(&desc_ring->last_finalized_seq,",
            "\t\t\t\t&oldval, newval)) { /* LMM(desc_update_last_finalized:A) */",
            "\t\told_seq = __ulseq_to_u64seq(rb, oldval);",
            "\t\tgoto try_again;",
            "\t}",
            "}",
            "static void desc_make_final(struct printk_ringbuffer *rb, unsigned long id)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tunsigned long prev_state_val = DESC_SV(id, desc_committed);",
            "\tstruct prb_desc *d = to_desc(desc_ring, id);",
            "",
            "\tif (atomic_long_try_cmpxchg_relaxed(&d->state_var, &prev_state_val,",
            "\t\t\tDESC_SV(id, desc_finalized))) { /* LMM(desc_make_final:A) */",
            "\t\tdesc_update_last_finalized(rb);",
            "\t}",
            "}",
            "bool prb_reserve(struct prb_reserved_entry *e, struct printk_ringbuffer *rb,",
            "\t\t struct printk_record *r)",
            "{",
            "\tstruct prb_desc_ring *desc_ring = &rb->desc_ring;",
            "\tstruct printk_info *info;",
            "\tstruct prb_desc *d;",
            "\tunsigned long id;",
            "\tu64 seq;",
            "",
            "\tif (!data_check_size(&rb->text_data_ring, r->text_buf_size))",
            "\t\tgoto fail;",
            "",
            "\t/*",
            "\t * Descriptors in the reserved state act as blockers to all further",
            "\t * reservations once the desc_ring has fully wrapped. Disable",
            "\t * interrupts during the reserve/commit window in order to minimize",
            "\t * the likelihood of this happening.",
            "\t */",
            "\tlocal_irq_save(e->irqflags);",
            "",
            "\tif (!desc_reserve(rb, &id)) {",
            "\t\t/* Descriptor reservation failures are tracked. */",
            "\t\tatomic_long_inc(&rb->fail);",
            "\t\tlocal_irq_restore(e->irqflags);",
            "\t\tgoto fail;",
            "\t}",
            "",
            "\td = to_desc(desc_ring, id);",
            "\tinfo = to_info(desc_ring, id);",
            "",
            "\t/*",
            "\t * All @info fields (except @seq) are cleared and must be filled in",
            "\t * by the writer. Save @seq before clearing because it is used to",
            "\t * determine the new sequence number.",
            "\t */",
            "\tseq = info->seq;",
            "\tmemset(info, 0, sizeof(*info));",
            "",
            "\t/*",
            "\t * Set the @e fields here so that prb_commit() can be used if",
            "\t * text data allocation fails.",
            "\t */",
            "\te->rb = rb;",
            "\te->id = id;",
            "",
            "\t/*",
            "\t * Initialize the sequence number if it has \"never been set\".",
            "\t * Otherwise just increment it by a full wrap.",
            "\t *",
            "\t * @seq is considered \"never been set\" if it has a value of 0,",
            "\t * _except_ for @infos[0], which was specially setup by the ringbuffer",
            "\t * initializer and therefore is always considered as set.",
            "\t *",
            "\t * See the \"Bootstrap\" comment block in printk_ringbuffer.h for",
            "\t * details about how the initializer bootstraps the descriptors.",
            "\t */",
            "\tif (seq == 0 && DESC_INDEX(desc_ring, id) != 0)",
            "\t\tinfo->seq = DESC_INDEX(desc_ring, id);",
            "\telse",
            "\t\tinfo->seq = seq + DESCS_COUNT(desc_ring);",
            "",
            "\t/*",
            "\t * New data is about to be reserved. Once that happens, previous",
            "\t * descriptors are no longer able to be extended. Finalize the",
            "\t * previous descriptor now so that it can be made available to",
            "\t * readers. (For seq==0 there is no previous descriptor.)",
            "\t */",
            "\tif (info->seq > 0)",
            "\t\tdesc_make_final(rb, DESC_ID(id - 1));",
            "",
            "\tr->text_buf = data_alloc(rb, r->text_buf_size, &d->text_blk_lpos, id);",
            "\t/* If text data allocation fails, a data-less record is committed. */",
            "\tif (r->text_buf_size && !r->text_buf) {",
            "\t\tprb_commit(e);",
            "\t\t/* prb_commit() re-enabled interrupts. */",
            "\t\tgoto fail;",
            "\t}",
            "",
            "\tr->info = info;",
            "",
            "\t/* Record full text space used by record. */",
            "\te->text_space = space_used(&rb->text_data_ring, &d->text_blk_lpos);",
            "",
            "\treturn true;",
            "fail:",
            "\t/* Make it clear to the caller that the reserve failed. */",
            "\tmemset(r, 0, sizeof(*r));",
            "\treturn false;",
            "}"
          ],
          "function_name": "desc_last_finalized_seq, desc_update_last_finalized, desc_make_final, prb_reserve",
          "description": "管理描述符状态转换与序列号同步。desc_last_finalized_seq获取最后确认的序列号，desc_update_last_finalized更新该序列号以保证内存顺序，prb_reserve预留新记录并初始化描述符字段。",
          "similarity": 0.4462321400642395
        }
      ]
    }
  ]
}