{
  "query": "减少系统调用开销的底层实现",
  "timestamp": "2025-12-26 02:10:12",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.5911431908607483,
      "chunks": [
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1776,
          "end_line": 1992,
          "content": [
            "static int pick_rt_task(struct rq *rq, struct task_struct *p, int cpu)",
            "{",
            "\tif (!task_on_cpu(rq, p) &&",
            "\t    cpumask_test_cpu(cpu, &p->cpus_mask))",
            "\t\treturn 1;",
            "",
            "\treturn 0;",
            "}",
            "static int find_lowest_rq(struct task_struct *task)",
            "{",
            "\tstruct sched_domain *sd;",
            "\tstruct cpumask *lowest_mask = this_cpu_cpumask_var_ptr(local_cpu_mask);",
            "\tint this_cpu = smp_processor_id();",
            "\tint cpu      = task_cpu(task);",
            "\tint ret;",
            "",
            "\t/* Make sure the mask is initialized first */",
            "\tif (unlikely(!lowest_mask))",
            "\t\treturn -1;",
            "",
            "\tif (task->nr_cpus_allowed == 1)",
            "\t\treturn -1; /* No other targets possible */",
            "",
            "\t/*",
            "\t * If we're on asym system ensure we consider the different capacities",
            "\t * of the CPUs when searching for the lowest_mask.",
            "\t */",
            "\tif (sched_asym_cpucap_active()) {",
            "",
            "\t\tret = cpupri_find_fitness(&task_rq(task)->rd->cpupri,",
            "\t\t\t\t\t  task, lowest_mask,",
            "\t\t\t\t\t  rt_task_fits_capacity);",
            "\t} else {",
            "",
            "\t\tret = cpupri_find(&task_rq(task)->rd->cpupri,",
            "\t\t\t\t  task, lowest_mask);",
            "\t}",
            "",
            "\tif (!ret)",
            "\t\treturn -1; /* No targets found */",
            "",
            "\t/*",
            "\t * At this point we have built a mask of CPUs representing the",
            "\t * lowest priority tasks in the system.  Now we want to elect",
            "\t * the best one based on our affinity and topology.",
            "\t *",
            "\t * We prioritize the last CPU that the task executed on since",
            "\t * it is most likely cache-hot in that location.",
            "\t */",
            "\tif (cpumask_test_cpu(cpu, lowest_mask))",
            "\t\treturn cpu;",
            "",
            "\t/*",
            "\t * Otherwise, we consult the sched_domains span maps to figure",
            "\t * out which CPU is logically closest to our hot cache data.",
            "\t */",
            "\tif (!cpumask_test_cpu(this_cpu, lowest_mask))",
            "\t\tthis_cpu = -1; /* Skip this_cpu opt if not among lowest */",
            "",
            "\trcu_read_lock();",
            "\tfor_each_domain(cpu, sd) {",
            "\t\tif (sd->flags & SD_WAKE_AFFINE) {",
            "\t\t\tint best_cpu;",
            "",
            "\t\t\t/*",
            "\t\t\t * \"this_cpu\" is cheaper to preempt than a",
            "\t\t\t * remote processor.",
            "\t\t\t */",
            "\t\t\tif (this_cpu != -1 &&",
            "\t\t\t    cpumask_test_cpu(this_cpu, sched_domain_span(sd))) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn this_cpu;",
            "\t\t\t}",
            "",
            "\t\t\tbest_cpu = cpumask_any_and_distribute(lowest_mask,",
            "\t\t\t\t\t\t\t      sched_domain_span(sd));",
            "\t\t\tif (best_cpu < nr_cpu_ids) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn best_cpu;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * And finally, if there were no matches within the domains",
            "\t * just give the caller *something* to work with from the compatible",
            "\t * locations.",
            "\t */",
            "\tif (this_cpu != -1)",
            "\t\treturn this_cpu;",
            "",
            "\tcpu = cpumask_any_distribute(lowest_mask);",
            "\tif (cpu < nr_cpu_ids)",
            "\t\treturn cpu;",
            "",
            "\treturn -1;",
            "}",
            "static int push_rt_task(struct rq *rq, bool pull)",
            "{",
            "\tstruct task_struct *next_task;",
            "\tstruct rq *lowest_rq;",
            "\tint ret = 0;",
            "",
            "\tif (!rq->rt.overloaded)",
            "\t\treturn 0;",
            "",
            "\tnext_task = pick_next_pushable_task(rq);",
            "\tif (!next_task)",
            "\t\treturn 0;",
            "",
            "retry:",
            "\t/*",
            "\t * It's possible that the next_task slipped in of",
            "\t * higher priority than current. If that's the case",
            "\t * just reschedule current.",
            "\t */",
            "\tif (unlikely(next_task->prio < rq->curr->prio)) {",
            "\t\tresched_curr(rq);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (is_migration_disabled(next_task)) {",
            "\t\tstruct task_struct *push_task = NULL;",
            "\t\tint cpu;",
            "",
            "\t\tif (!pull || rq->push_busy)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * Invoking find_lowest_rq() on anything but an RT task doesn't",
            "\t\t * make sense. Per the above priority check, curr has to",
            "\t\t * be of higher priority than next_task, so no need to",
            "\t\t * reschedule when bailing out.",
            "\t\t *",
            "\t\t * Note that the stoppers are masqueraded as SCHED_FIFO",
            "\t\t * (cf. sched_set_stop_task()), so we can't rely on rt_task().",
            "\t\t */",
            "\t\tif (rq->curr->sched_class != &rt_sched_class)",
            "\t\t\treturn 0;",
            "",
            "\t\tcpu = find_lowest_rq(rq->curr);",
            "\t\tif (cpu == -1 || cpu == rq->cpu)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * Given we found a CPU with lower priority than @next_task,",
            "\t\t * therefore it should be running. However we cannot migrate it",
            "\t\t * to this other CPU, instead attempt to push the current",
            "\t\t * running task on this CPU away.",
            "\t\t */",
            "\t\tpush_task = get_push_task(rq);",
            "\t\tif (push_task) {",
            "\t\t\tpreempt_disable();",
            "\t\t\traw_spin_rq_unlock(rq);",
            "\t\t\tstop_one_cpu_nowait(rq->cpu, push_cpu_stop,",
            "\t\t\t\t\t    push_task, &rq->push_work);",
            "\t\t\tpreempt_enable();",
            "\t\t\traw_spin_rq_lock(rq);",
            "\t\t}",
            "",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (WARN_ON(next_task == rq->curr))",
            "\t\treturn 0;",
            "",
            "\t/* We might release rq lock */",
            "\tget_task_struct(next_task);",
            "",
            "\t/* find_lock_lowest_rq locks the rq if found */",
            "\tlowest_rq = find_lock_lowest_rq(next_task, rq);",
            "\tif (!lowest_rq) {",
            "\t\tstruct task_struct *task;",
            "\t\t/*",
            "\t\t * find_lock_lowest_rq releases rq->lock",
            "\t\t * so it is possible that next_task has migrated.",
            "\t\t *",
            "\t\t * We need to make sure that the task is still on the same",
            "\t\t * run-queue and is also still the next task eligible for",
            "\t\t * pushing.",
            "\t\t */",
            "\t\ttask = pick_next_pushable_task(rq);",
            "\t\tif (task == next_task) {",
            "\t\t\t/*",
            "\t\t\t * The task hasn't migrated, and is still the next",
            "\t\t\t * eligible task, but we failed to find a run-queue",
            "\t\t\t * to push it to.  Do not retry in this case, since",
            "\t\t\t * other CPUs will pull from us when ready.",
            "\t\t\t */",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tif (!task)",
            "\t\t\t/* No more tasks, just exit */",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * Something has shifted, try again.",
            "\t\t */",
            "\t\tput_task_struct(next_task);",
            "\t\tnext_task = task;",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tdeactivate_task(rq, next_task, 0);",
            "\tset_task_cpu(next_task, lowest_rq->cpu);",
            "\tactivate_task(lowest_rq, next_task, 0);",
            "\tresched_curr(lowest_rq);",
            "\tret = 1;",
            "",
            "\tdouble_unlock_balance(rq, lowest_rq);",
            "out:",
            "\tput_task_struct(next_task);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "pick_rt_task, find_lowest_rq, push_rt_task",
          "description": "实现实时任务选择算法、低优先级CPU搜索及强制迁移逻辑，支持异构系统下的能效优化和拓扑感知调度。",
          "similarity": 0.6557859182357788
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/rt.c",
          "start_line": 529,
          "end_line": 633,
          "content": [
            "static void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\tstruct sched_rt_entity *rt_se;",
            "",
            "\tint cpu = cpu_of(rq);",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tif (!rt_se)",
            "\t\t\tenqueue_top_rt_rq(rt_rq);",
            "\t\telse if (!on_rt_rq(rt_se))",
            "\t\t\tenqueue_rt_entity(rt_se, 0);",
            "",
            "\t\tif (rt_rq->highest_prio.curr < curr->prio)",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "}",
            "static void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (!rt_se) {",
            "\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "\t\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);",
            "\t}",
            "\telse if (on_rt_rq(rt_se))",
            "\t\tdequeue_rt_entity(rt_se, 0);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;",
            "}",
            "static int rt_se_boosted(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "\tstruct task_struct *p;",
            "",
            "\tif (rt_rq)",
            "\t\treturn !!rt_rq->rt_nr_boosted;",
            "",
            "\tp = rt_task_of(rt_se);",
            "\treturn p->prio != p->normal_prio;",
            "}",
            "bool sched_rt_bandwidth_account(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\treturn (hrtimer_active(&rt_b->rt_period_timer) ||",
            "\t\trt_rq->rt_time < rt_b->rt_runtime);",
            "}",
            "static void do_balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;",
            "\tint i, weight;",
            "\tu64 rt_period;",
            "",
            "\tweight = cpumask_weight(rd->span);",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\trt_period = ktime_to_ns(rt_b->rt_period);",
            "\tfor_each_cpu(i, rd->span) {",
            "\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\ts64 diff;",
            "",
            "\t\tif (iter == rt_rq)",
            "\t\t\tcontinue;",
            "",
            "\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either all rqs have inf runtime and there's nothing to steal",
            "\t\t * or __disable_runtime() below sets a specific rq to inf to",
            "\t\t * indicate its been disabled and disallow stealing.",
            "\t\t */",
            "\t\tif (iter->rt_runtime == RUNTIME_INF)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * From runqueues with spare time, take 1/n part of their",
            "\t\t * spare time, but no more than our period.",
            "\t\t */",
            "\t\tdiff = iter->rt_runtime - iter->rt_time;",
            "\t\tif (diff > 0) {",
            "\t\t\tdiff = div_u64((u64)diff, weight);",
            "\t\t\tif (rt_rq->rt_runtime + diff > rt_period)",
            "\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;",
            "\t\t\titer->rt_runtime -= diff;",
            "\t\t\trt_rq->rt_runtime += diff;",
            "\t\t\tif (rt_rq->rt_runtime == rt_period) {",
            "\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "next:",
            "\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, rt_se_boosted, sched_rt_bandwidth_account, do_balance_runtime",
          "description": "实现实时任务队列的插入/移除逻辑，跟踪运行时间消耗，通过跨CPU运行时间平衡算法实现带宽公平分配。",
          "similarity": 0.6174466013908386
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1607,
          "end_line": 1716,
          "content": [
            "static void check_preempt_equal_prio(struct rq *rq, struct task_struct *p)",
            "{",
            "\t/*",
            "\t * Current can't be migrated, useless to reschedule,",
            "\t * let's hope p can move out.",
            "\t */",
            "\tif (rq->curr->nr_cpus_allowed == 1 ||",
            "\t    !cpupri_find(&rq->rd->cpupri, rq->curr, NULL))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * p is migratable, so let's not schedule it and",
            "\t * see if it is pushed or pulled somewhere else.",
            "\t */",
            "\tif (p->nr_cpus_allowed != 1 &&",
            "\t    cpupri_find(&rq->rd->cpupri, p, NULL))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * There appear to be other CPUs that can accept",
            "\t * the current task but none can run 'p', so lets reschedule",
            "\t * to try and push the current task away:",
            "\t */",
            "\trequeue_task_rt(rq, p, 1);",
            "\tresched_curr(rq);",
            "}",
            "static int balance_rt(struct rq *rq, struct task_struct *p, struct rq_flags *rf)",
            "{",
            "\tif (!on_rt_rq(&p->rt) && need_pull_rt_task(rq, p)) {",
            "\t\t/*",
            "\t\t * This is OK, because current is on_cpu, which avoids it being",
            "\t\t * picked for load-balance and preemption/IRQs are still",
            "\t\t * disabled avoiding further scheduler activity on it and we've",
            "\t\t * not yet started the picking loop.",
            "\t\t */",
            "\t\trq_unpin_lock(rq, rf);",
            "\t\tpull_rt_task(rq);",
            "\t\trq_repin_lock(rq, rf);",
            "\t}",
            "",
            "\treturn sched_stop_runnable(rq) || sched_dl_runnable(rq) || sched_rt_runnable(rq);",
            "}",
            "static void wakeup_preempt_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tif (p->prio < rq->curr->prio) {",
            "\t\tresched_curr(rq);",
            "\t\treturn;",
            "\t}",
            "",
            "#ifdef CONFIG_SMP",
            "\t/*",
            "\t * If:",
            "\t *",
            "\t * - the newly woken task is of equal priority to the current task",
            "\t * - the newly woken task is non-migratable while current is migratable",
            "\t * - current will be preempted on the next reschedule",
            "\t *",
            "\t * we should check to see if current can readily move to a different",
            "\t * cpu.  If so, we will reschedule to allow the push logic to try",
            "\t * to move current somewhere else, making room for our non-migratable",
            "\t * task.",
            "\t */",
            "\tif (p->prio == rq->curr->prio && !test_tsk_need_resched(rq->curr))",
            "\t\tcheck_preempt_equal_prio(rq, p);",
            "#endif",
            "}",
            "static inline void set_next_task_rt(struct rq *rq, struct task_struct *p, bool first)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq = &rq->rt;",
            "",
            "\tp->se.exec_start = rq_clock_task(rq);",
            "\tif (on_rt_rq(&p->rt))",
            "\t\tupdate_stats_wait_end_rt(rt_rq, rt_se);",
            "",
            "\t/* The running task is never eligible for pushing */",
            "\tdequeue_pushable_task(rq, p);",
            "",
            "\tif (!first)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If prev task was rt, put_prev_task() has already updated the",
            "\t * utilization. We only care of the case where we start to schedule a",
            "\t * rt task",
            "\t */",
            "\tif (rq->curr->sched_class != &rt_sched_class)",
            "\t\tupdate_rt_rq_load_avg(rq_clock_pelt(rq), rq, 0);",
            "",
            "\trt_queue_push_tasks(rq);",
            "}",
            "static void put_prev_task_rt(struct rq *rq, struct task_struct *p, struct task_struct *next)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq = &rq->rt;",
            "",
            "\tif (on_rt_rq(&p->rt))",
            "\t\tupdate_stats_wait_start_rt(rt_rq, rt_se);",
            "",
            "\tupdate_curr_rt(rq);",
            "",
            "\tupdate_rt_rq_load_avg(rq_clock_pelt(rq), rq, 1);",
            "",
            "\t/*",
            "\t * The previous task needs to be made eligible for pushing",
            "\t * if it is still active",
            "\t */",
            "\tif (on_rt_rq(&p->rt) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_task(rq, p);",
            "}"
          ],
          "function_name": "check_preempt_equal_prio, balance_rt, wakeup_preempt_rt, set_next_task_rt, put_prev_task_rt",
          "description": "实现同优先级抢占检测、实时任务负载平衡、唤醒抢占触发机制以及调度器状态更新与负载计算。",
          "similarity": 0.6136938333511353
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/rt.c",
          "start_line": 57,
          "end_line": 159,
          "content": [
            "static int __init sched_rt_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", sched_rt_sysctls);",
            "\treturn 0;",
            "}",
            "void init_rt_rq(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_prio_array *array;",
            "\tint i;",
            "",
            "\tarray = &rt_rq->active;",
            "\tfor (i = 0; i < MAX_RT_PRIO; i++) {",
            "\t\tINIT_LIST_HEAD(array->queue + i);",
            "\t\t__clear_bit(i, array->bitmap);",
            "\t}",
            "\t/* delimiter for bitsearch: */",
            "\t__set_bit(MAX_RT_PRIO, array->bitmap);",
            "",
            "#if defined CONFIG_SMP",
            "\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\trt_rq->highest_prio.next = MAX_RT_PRIO-1;",
            "\trt_rq->overloaded = 0;",
            "\tplist_head_init(&rt_rq->pushable_tasks);",
            "#endif /* CONFIG_SMP */",
            "\t/* We start is dequeued state, because no RT tasks are queued */",
            "\trt_rq->rt_queued = 0;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\trt_rq->rt_time = 0;",
            "\trt_rq->rt_throttled = 0;",
            "\trt_rq->rt_runtime = 0;",
            "\traw_spin_lock_init(&rt_rq->rt_runtime_lock);",
            "#endif",
            "}",
            "static enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)",
            "{",
            "\tstruct rt_bandwidth *rt_b =",
            "\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);",
            "\tint idle = 0;",
            "\tint overrun;",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tfor (;;) {",
            "\t\toverrun = hrtimer_forward_now(timer, rt_b->rt_period);",
            "\t\tif (!overrun)",
            "\t\t\tbreak;",
            "",
            "\t\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "\t\tidle = do_sched_rt_period_timer(rt_b, overrun);",
            "\t\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\t}",
            "\tif (idle)",
            "\t\trt_b->rt_period_active = 0;",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "",
            "\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;",
            "}",
            "void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)",
            "{",
            "\trt_b->rt_period = ns_to_ktime(period);",
            "\trt_b->rt_runtime = runtime;",
            "",
            "\traw_spin_lock_init(&rt_b->rt_runtime_lock);",
            "",
            "\thrtimer_init(&rt_b->rt_period_timer, CLOCK_MONOTONIC,",
            "\t\t     HRTIMER_MODE_REL_HARD);",
            "\trt_b->rt_period_timer.function = sched_rt_period_timer;",
            "}",
            "static inline void do_start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tif (!rt_b->rt_period_active) {",
            "\t\trt_b->rt_period_active = 1;",
            "\t\t/*",
            "\t\t * SCHED_DEADLINE updates the bandwidth, as a run away",
            "\t\t * RT task with a DL task could hog a CPU. But DL does",
            "\t\t * not reset the period. If a deadline task was running",
            "\t\t * without an RT task running, it can cause RT tasks to",
            "\t\t * throttle when they start up. Kick the timer right away",
            "\t\t * to update the period.",
            "\t\t */",
            "\t\thrtimer_forward_now(&rt_b->rt_period_timer, ns_to_ktime(0));",
            "\t\thrtimer_start_expires(&rt_b->rt_period_timer,",
            "\t\t\t\t      HRTIMER_MODE_ABS_PINNED_HARD);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}",
            "static void start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)",
            "\t\treturn;",
            "",
            "\tdo_start_rt_bandwidth(rt_b);",
            "}",
            "static void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\thrtimer_cancel(&rt_b->rt_period_timer);",
            "}",
            "void unregister_rt_sched_group(struct task_group *tg)",
            "{",
            "\tif (tg->rt_se)",
            "\t\tdestroy_rt_bandwidth(&tg->rt_bandwidth);",
            "}"
          ],
          "function_name": "sched_rt_sysctl_init, init_rt_rq, sched_rt_period_timer, init_rt_bandwidth, do_start_rt_bandwidth, start_rt_bandwidth, destroy_rt_bandwidth, unregister_rt_sched_group",
          "description": "初始化实时调度相关数据结构，管理实时任务周期定时器，控制实时带宽分配与回收，实现基于时间片轮转的调度策略。",
          "similarity": 0.6092846393585205
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1449,
          "end_line": 1589,
          "content": [
            "static void dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rq *rq = rq_of_rt_se(rt_se);",
            "",
            "\tupdate_stats_dequeue_rt(rt_rq_of_se(rt_se), rt_se, flags);",
            "",
            "\tdequeue_rt_stack(rt_se, flags);",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\t\tif (rt_rq && rt_rq->rt_nr_running)",
            "\t\t\t__enqueue_rt_entity(rt_se, flags);",
            "\t}",
            "\tenqueue_top_rt_rq(&rq->rt);",
            "}",
            "static void",
            "enqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tif (flags & ENQUEUE_WAKEUP)",
            "\t\trt_se->timeout = 0;",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_rt(rt_rq_of_se(rt_se), rt_se);",
            "",
            "\tenqueue_rt_entity(rt_se, flags);",
            "",
            "\tif (!task_current(rq, p) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_task(rq, p);",
            "}",
            "static bool dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tdequeue_rt_entity(rt_se, flags);",
            "",
            "\tdequeue_pushable_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void",
            "requeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)",
            "{",
            "\tif (on_rt_rq(rt_se)) {",
            "\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "\t\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);",
            "",
            "\t\tif (head)",
            "\t\t\tlist_move(&rt_se->run_list, queue);",
            "\t\telse",
            "\t\t\tlist_move_tail(&rt_se->run_list, queue);",
            "\t}",
            "}",
            "static void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\trt_rq = rt_rq_of_se(rt_se);",
            "\t\trequeue_rt_entity(rt_rq, rt_se, head);",
            "\t}",
            "}",
            "static void yield_task_rt(struct rq *rq)",
            "{",
            "\trequeue_task_rt(rq, rq->curr, 0);",
            "}",
            "static int",
            "select_task_rq_rt(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tstruct rq *rq;",
            "\tbool test;",
            "",
            "\t/* For anything but wake ups, just return the task_cpu */",
            "\tif (!(flags & (WF_TTWU | WF_FORK)))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If the current task on @p's runqueue is an RT task, then",
            "\t * try to see if we can wake this RT task up on another",
            "\t * runqueue. Otherwise simply start this RT task",
            "\t * on its current runqueue.",
            "\t *",
            "\t * We want to avoid overloading runqueues. If the woken",
            "\t * task is a higher priority, then it will stay on this CPU",
            "\t * and the lower prio task should be moved to another CPU.",
            "\t * Even though this will probably make the lower prio task",
            "\t * lose its cache, we do not want to bounce a higher task",
            "\t * around just because it gave up its CPU, perhaps for a",
            "\t * lock?",
            "\t *",
            "\t * For equal prio tasks, we just let the scheduler sort it out.",
            "\t *",
            "\t * Otherwise, just let it ride on the affined RQ and the",
            "\t * post-schedule router will push the preempted task away",
            "\t *",
            "\t * This test is optimistic, if we get it wrong the load-balancer",
            "\t * will have to sort it out.",
            "\t *",
            "\t * We take into account the capacity of the CPU to ensure it fits the",
            "\t * requirement of the task - which is only important on heterogeneous",
            "\t * systems like big.LITTLE.",
            "\t */",
            "\ttest = curr &&",
            "\t       unlikely(rt_task(curr)) &&",
            "\t       (curr->nr_cpus_allowed < 2 || curr->prio <= p->prio);",
            "",
            "\tif (test || !rt_task_fits_capacity(p, cpu)) {",
            "\t\tint target = find_lowest_rq(p);",
            "",
            "\t\t/*",
            "\t\t * Bail out if we were forcing a migration to find a better",
            "\t\t * fitting CPU but our search failed.",
            "\t\t */",
            "\t\tif (!test && target != -1 && !rt_task_fits_capacity(p, target))",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\t/*",
            "\t\t * Don't bother moving it if the destination CPU is",
            "\t\t * not running a lower priority task.",
            "\t\t */",
            "\t\tif (target != -1 &&",
            "\t\t    p->prio < cpu_rq(target)->rt.highest_prio.curr)",
            "\t\t\tcpu = target;",
            "\t}",
            "",
            "out_unlock:",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "dequeue_rt_entity, enqueue_task_rt, dequeue_task_rt, requeue_rt_entity, requeue_task_rt, yield_task_rt, select_task_rq_rt",
          "description": "实现实时任务的出队逻辑、唤醒和迁移策略，提供CPU亲和性选择及负载均衡支持，维护优先级队列的动态调整。",
          "similarity": 0.6060090065002441
        }
      ]
    },
    {
      "source_file": "kernel/static_call_inline.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:29:06\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `static_call_inline.c`\n\n---\n\n# static_call_inline.c 技术文档\n\n## 1. 文件概述\n\n`static_call_inline.c` 是 Linux 内核中实现 **静态调用（Static Call）** 机制的核心文件之一。静态调用是一种运行时可动态更新的函数调用优化技术，它在编译时将函数调用点内联为对跳板（trampoline）的直接跳转，而在运行时可通过 `__static_call_update()` 动态修改所有调用点，使其跳转到新的目标函数，从而避免传统函数指针调用的间接开销。该机制常用于性能敏感路径（如调度、RCU、tracepoint 等），同时支持模块热插拔和初始化阶段的特殊处理。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `static_call_force_reinit(void)`  \n  强制重新初始化静态调用机制，用于调试或特殊场景，必须在 `early_initcall()` 之前调用。\n\n- `__static_call_update(struct static_call_key *key, void *tramp, void *func)`  \n  核心更新函数：将指定 `key` 对应的所有静态调用点更新为调用 `func`，并更新跳板 `tramp`。支持内核和模块中的调用点。\n\n- `__static_call_init(struct module *mod, struct static_call_site *start, struct static_call_site *stop)`  \n  初始化静态调用站点，对站点按 `key` 排序，并建立 `key` 到站点的映射关系，同时执行首次 `arch_static_call_transform`。\n\n- `__static_call_text_reserved(...)`  \n  检查指定代码区间是否与活跃的静态调用站点冲突，用于内存热插拔或代码修改前的安全校验。\n\n### 主要数据结构\n\n- `struct static_call_site`  \n  描述一个静态调用点的位置（`addr`）和关联的 `key`（带标志位）。\n\n- `struct static_call_key`  \n  静态调用的“键”，用于将多个调用点分组。包含当前函数指针 `func` 和类型/模块信息。\n\n- `struct static_call_mod`  \n  用于模块场景下，将模块与该模块中属于某 `key` 的调用点列表关联。\n\n- 全局符号：\n  - `__start_static_call_sites[]` / `__stop_static_call_sites[]`：内核镜像中所有静态调用点的链接器生成数组。\n  - `__start_static_call_tramp_key[]` / `__stop_static_call_tramp_key[]`：跳板与 key 的映射。\n\n### 辅助函数与宏\n\n- `static_call_addr(site)`：计算调用点的实际地址（处理重定位）。\n- `static_call_key(site)`：从站点中提取 `static_call_key*`（忽略标志位）。\n- `static_call_is_init(site)` / `static_call_is_tail(site)`：检查站点是否位于 `__init` 段或是否为尾调用。\n- `static_call_sort_entries()`：对站点按 `key` 排序，便于批量处理。\n- `static_call_key_has_mods()` / `static_call_key_sites()`：判断 key 是否关联模块或直接站点。\n\n## 3. 关键实现\n\n### 地址重定位处理\n由于静态调用站点在编译时使用相对地址存储，`static_call_addr()` 和 `__static_call_key()` 通过 `(long)field + (long)&field` 的方式计算出运行时绝对地址，这是处理位置无关代码（PIC）和内核重定位的关键技巧。\n\n### 站点组织与模块支持\n- **内核（vmlinux）场景**：为节省内存和避免早期内存分配，将首个站点指针直接编码到 `key->type` 的低有效位中（通过 `| 1` 标记）。\n- **模块场景**：使用 `static_call_mod` 链表管理不同模块中属于同一 `key` 的站点，支持模块加载/卸载时的动态注册。\n\n### 初始化与更新流程\n1. **初始化**（`__static_call_init`）：\n   - 对站点按 `key` 排序。\n   - 标记位于 `__init` 段的站点（后续更新可跳过）。\n   - 建立 `key` 到站点的映射。\n   - 调用架构相关 `arch_static_call_transform` 执行首次转换（通常设为跳板）。\n\n2. **更新**（`__static_call_update`）：\n   - 更新 `key->func`。\n   - 更新跳板 `tramp` 指向新函数。\n   - 遍历所有关联站点（包括模块），调用 `arch_static_call_transform` 修改调用点指令（如 x86 的 `jmp` 目标）。\n   - 跳过 `__init` 段中已初始化的站点（因不会被执行）。\n\n### 安全与并发控制\n- 使用 `cpus_read_lock()` 防止 CPU 热插拔期间的并发问题。\n- 使用 `static_call_mutex` 保护 `key` 和站点数据结构的修改。\n- 通过 `kernel_text_address()` 验证调用点是否在可执行内核文本段，避免修改无效地址。\n\n## 4. 依赖关系\n\n- **架构依赖**：依赖 `asm/sections.h` 和 `arch_static_call_transform()`（由各架构实现，如 x86、ARM64）。\n- **内核子系统**：\n  - `linux/module.h`：模块加载/卸载时的静态调用站点管理。\n  - `linux/cpu.h` / `linux/smp.h`：CPU 热插拔和并发控制。\n  - `linux/sort.h`：站点排序。\n  - `linux/slab.h`：模块场景下的动态内存分配。\n- **链接器脚本**：依赖链接器生成的 `__start/stop_static_call_sites` 等符号，这些在 `vmlinux.lds` 中定义。\n\n## 5. 使用场景\n\n- **内核核心优化**：在调度器、RCU、中断处理等高频路径中替代函数指针，减少间接调用开销。\n- **动态追踪（ftrace）**：作为 tracepoint 或 kprobe 的底层机制，实现零开销探针。\n- **模块热插拔**：模块加载时注册其静态调用站点，卸载时自动清理，确保调用点始终有效。\n- **初始化优化**：`__init` 段的调用点在初始化完成后可被安全忽略，减少运行时开销。\n- **安全代码修改**：在 livepatch 或内核热补丁中，安全地替换函数实现而不影响运行中的调用。",
      "similarity": 0.5905970335006714,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/static_call_inline.c",
          "start_line": 23,
          "end_line": 176,
          "content": [
            "void static_call_force_reinit(void)",
            "{",
            "\tif (WARN_ON_ONCE(!static_call_initialized))",
            "\t\treturn;",
            "",
            "\tstatic_call_initialized++;",
            "}",
            "static void static_call_lock(void)",
            "{",
            "\tmutex_lock(&static_call_mutex);",
            "}",
            "static void static_call_unlock(void)",
            "{",
            "\tmutex_unlock(&static_call_mutex);",
            "}",
            "static inline unsigned long __static_call_key(const struct static_call_site *site)",
            "{",
            "\treturn (long)site->key + (long)&site->key;",
            "}",
            "static inline bool static_call_is_init(struct static_call_site *site)",
            "{",
            "\treturn __static_call_key(site) & STATIC_CALL_SITE_INIT;",
            "}",
            "static inline bool static_call_is_tail(struct static_call_site *site)",
            "{",
            "\treturn __static_call_key(site) & STATIC_CALL_SITE_TAIL;",
            "}",
            "static inline void static_call_set_init(struct static_call_site *site)",
            "{",
            "\tsite->key = (__static_call_key(site) | STATIC_CALL_SITE_INIT) -",
            "\t\t    (long)&site->key;",
            "}",
            "static int static_call_site_cmp(const void *_a, const void *_b)",
            "{",
            "\tconst struct static_call_site *a = _a;",
            "\tconst struct static_call_site *b = _b;",
            "\tconst struct static_call_key *key_a = static_call_key(a);",
            "\tconst struct static_call_key *key_b = static_call_key(b);",
            "",
            "\tif (key_a < key_b)",
            "\t\treturn -1;",
            "",
            "\tif (key_a > key_b)",
            "\t\treturn 1;",
            "",
            "\treturn 0;",
            "}",
            "static void static_call_site_swap(void *_a, void *_b, int size)",
            "{",
            "\tlong delta = (unsigned long)_a - (unsigned long)_b;",
            "\tstruct static_call_site *a = _a;",
            "\tstruct static_call_site *b = _b;",
            "\tstruct static_call_site tmp = *a;",
            "",
            "\ta->addr = b->addr  - delta;",
            "\ta->key  = b->key   - delta;",
            "",
            "\tb->addr = tmp.addr + delta;",
            "\tb->key  = tmp.key  + delta;",
            "}",
            "static inline void static_call_sort_entries(struct static_call_site *start,",
            "\t\t\t\t\t    struct static_call_site *stop)",
            "{",
            "\tsort(start, stop - start, sizeof(struct static_call_site),",
            "\t     static_call_site_cmp, static_call_site_swap);",
            "}",
            "static inline bool static_call_key_has_mods(struct static_call_key *key)",
            "{",
            "\treturn !(key->type & 1);",
            "}",
            "void __static_call_update(struct static_call_key *key, void *tramp, void *func)",
            "{",
            "\tstruct static_call_site *site, *stop;",
            "\tstruct static_call_mod *site_mod, first;",
            "",
            "\tcpus_read_lock();",
            "\tstatic_call_lock();",
            "",
            "\tif (key->func == func)",
            "\t\tgoto done;",
            "",
            "\tkey->func = func;",
            "",
            "\tarch_static_call_transform(NULL, tramp, func, false);",
            "",
            "\t/*",
            "\t * If uninitialized, we'll not update the callsites, but they still",
            "\t * point to the trampoline and we just patched that.",
            "\t */",
            "\tif (WARN_ON_ONCE(!static_call_initialized))",
            "\t\tgoto done;",
            "",
            "\tfirst = (struct static_call_mod){",
            "\t\t.next = static_call_key_next(key),",
            "\t\t.mod = NULL,",
            "\t\t.sites = static_call_key_sites(key),",
            "\t};",
            "",
            "\tfor (site_mod = &first; site_mod; site_mod = site_mod->next) {",
            "\t\tbool init = system_state < SYSTEM_RUNNING;",
            "\t\tstruct module *mod = site_mod->mod;",
            "",
            "\t\tif (!site_mod->sites) {",
            "\t\t\t/*",
            "\t\t\t * This can happen if the static call key is defined in",
            "\t\t\t * a module which doesn't use it.",
            "\t\t\t *",
            "\t\t\t * It also happens in the has_mods case, where the",
            "\t\t\t * 'first' entry has no sites associated with it.",
            "\t\t\t */",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tstop = __stop_static_call_sites;",
            "",
            "\t\tif (mod) {",
            "#ifdef CONFIG_MODULES",
            "\t\t\tstop = mod->static_call_sites +",
            "\t\t\t       mod->num_static_call_sites;",
            "\t\t\tinit = mod->state == MODULE_STATE_COMING;",
            "#endif",
            "\t\t}",
            "",
            "\t\tfor (site = site_mod->sites;",
            "\t\t     site < stop && static_call_key(site) == key; site++) {",
            "\t\t\tvoid *site_addr = static_call_addr(site);",
            "",
            "\t\t\tif (!init && static_call_is_init(site))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tif (!kernel_text_address((unsigned long)site_addr)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * This skips patching built-in __exit, which",
            "\t\t\t\t * is part of init_section_contains() but is",
            "\t\t\t\t * not part of kernel_text_address().",
            "\t\t\t\t *",
            "\t\t\t\t * Skipping built-in __exit is fine since it",
            "\t\t\t\t * will never be executed.",
            "\t\t\t\t */",
            "\t\t\t\tWARN_ONCE(!static_call_is_init(site),",
            "\t\t\t\t\t  \"can't patch static call site at %pS\",",
            "\t\t\t\t\t  site_addr);",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tarch_static_call_transform(site_addr, NULL, func,",
            "\t\t\t\t\t\t   static_call_is_tail(site));",
            "\t\t}",
            "\t}",
            "",
            "done:",
            "\tstatic_call_unlock();",
            "\tcpus_read_unlock();",
            "}"
          ],
          "function_name": "static_call_force_reinit, static_call_lock, static_call_unlock, __static_call_key, static_call_is_init, static_call_is_tail, static_call_set_init, static_call_site_cmp, static_call_site_swap, static_call_sort_entries, static_call_key_has_mods, __static_call_update",
          "description": "实现静态调用的互斥锁控制、键值计算、站点排序及更新逻辑，包含地址冲突检测和模块间调用关系维护功能。",
          "similarity": 0.6380609273910522
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/static_call_inline.c",
          "start_line": 220,
          "end_line": 340,
          "content": [
            "static int __static_call_init(struct module *mod,",
            "\t\t\t      struct static_call_site *start,",
            "\t\t\t      struct static_call_site *stop)",
            "{",
            "\tstruct static_call_site *site;",
            "\tstruct static_call_key *key, *prev_key = NULL;",
            "\tstruct static_call_mod *site_mod;",
            "",
            "\tif (start == stop)",
            "\t\treturn 0;",
            "",
            "\tstatic_call_sort_entries(start, stop);",
            "",
            "\tfor (site = start; site < stop; site++) {",
            "\t\tvoid *site_addr = static_call_addr(site);",
            "",
            "\t\tif ((mod && within_module_init((unsigned long)site_addr, mod)) ||",
            "\t\t    (!mod && init_section_contains(site_addr, 1)))",
            "\t\t\tstatic_call_set_init(site);",
            "",
            "\t\tkey = static_call_key(site);",
            "\t\tif (key != prev_key) {",
            "\t\t\tprev_key = key;",
            "",
            "\t\t\t/*",
            "\t\t\t * For vmlinux (!mod) avoid the allocation by storing",
            "\t\t\t * the sites pointer in the key itself. Also see",
            "\t\t\t * __static_call_update()'s @first.",
            "\t\t\t *",
            "\t\t\t * This allows architectures (eg. x86) to call",
            "\t\t\t * static_call_init() before memory allocation works.",
            "\t\t\t */",
            "\t\t\tif (!mod) {",
            "\t\t\t\tkey->sites = site;",
            "\t\t\t\tkey->type |= 1;",
            "\t\t\t\tgoto do_transform;",
            "\t\t\t}",
            "",
            "\t\t\tsite_mod = kzalloc(sizeof(*site_mod), GFP_KERNEL);",
            "\t\t\tif (!site_mod)",
            "\t\t\t\treturn -ENOMEM;",
            "",
            "\t\t\t/*",
            "\t\t\t * When the key has a direct sites pointer, extract",
            "\t\t\t * that into an explicit struct static_call_mod, so we",
            "\t\t\t * can have a list of modules.",
            "\t\t\t */",
            "\t\t\tif (static_call_key_sites(key)) {",
            "\t\t\t\tsite_mod->mod = NULL;",
            "\t\t\t\tsite_mod->next = NULL;",
            "\t\t\t\tsite_mod->sites = static_call_key_sites(key);",
            "",
            "\t\t\t\tkey->mods = site_mod;",
            "",
            "\t\t\t\tsite_mod = kzalloc(sizeof(*site_mod), GFP_KERNEL);",
            "\t\t\t\tif (!site_mod)",
            "\t\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "",
            "\t\t\tsite_mod->mod = mod;",
            "\t\t\tsite_mod->sites = site;",
            "\t\t\tsite_mod->next = static_call_key_next(key);",
            "\t\t\tkey->mods = site_mod;",
            "\t\t}",
            "",
            "do_transform:",
            "\t\tarch_static_call_transform(site_addr, NULL, key->func,",
            "\t\t\t\tstatic_call_is_tail(site));",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int addr_conflict(struct static_call_site *site, void *start, void *end)",
            "{",
            "\tunsigned long addr = (unsigned long)static_call_addr(site);",
            "",
            "\tif (addr <= (unsigned long)end &&",
            "\t    addr + CALL_INSN_SIZE > (unsigned long)start)",
            "\t\treturn 1;",
            "",
            "\treturn 0;",
            "}",
            "static int __static_call_text_reserved(struct static_call_site *iter_start,",
            "\t\t\t\t       struct static_call_site *iter_stop,",
            "\t\t\t\t       void *start, void *end, bool init)",
            "{",
            "\tstruct static_call_site *iter = iter_start;",
            "",
            "\twhile (iter < iter_stop) {",
            "\t\tif (init || !static_call_is_init(iter)) {",
            "\t\t\tif (addr_conflict(iter, start, end))",
            "\t\t\t\treturn 1;",
            "\t\t}",
            "\t\titer++;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int __static_call_mod_text_reserved(void *start, void *end)",
            "{",
            "\tstruct module *mod;",
            "\tint ret;",
            "",
            "\tpreempt_disable();",
            "\tmod = __module_text_address((unsigned long)start);",
            "\tWARN_ON_ONCE(__module_text_address((unsigned long)end) != mod);",
            "\tif (!try_module_get(mod))",
            "\t\tmod = NULL;",
            "\tpreempt_enable();",
            "",
            "\tif (!mod)",
            "\t\treturn 0;",
            "",
            "\tret = __static_call_text_reserved(mod->static_call_sites,",
            "\t\t\tmod->static_call_sites + mod->num_static_call_sites,",
            "\t\t\tstart, end, mod->state == MODULE_STATE_COMING);",
            "",
            "\tmodule_put(mod);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__static_call_init, addr_conflict, __static_call_text_reserved, __static_call_mod_text_reserved",
          "description": "执行静态调用初始化流程，分配模块关联结构体并进行地址转换，实现文本区域预留检查以避免内存覆盖。",
          "similarity": 0.6235028505325317
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/static_call_inline.c",
          "start_line": 347,
          "end_line": 449,
          "content": [
            "static unsigned long tramp_key_lookup(unsigned long addr)",
            "{",
            "\tstruct static_call_tramp_key *start = __start_static_call_tramp_key;",
            "\tstruct static_call_tramp_key *stop = __stop_static_call_tramp_key;",
            "\tstruct static_call_tramp_key *tramp_key;",
            "",
            "\tfor (tramp_key = start; tramp_key != stop; tramp_key++) {",
            "\t\tunsigned long tramp;",
            "",
            "\t\ttramp = (long)tramp_key->tramp + (long)&tramp_key->tramp;",
            "\t\tif (tramp == addr)",
            "\t\t\treturn (long)tramp_key->key + (long)&tramp_key->key;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int static_call_add_module(struct module *mod)",
            "{",
            "\tstruct static_call_site *start = mod->static_call_sites;",
            "\tstruct static_call_site *stop = start + mod->num_static_call_sites;",
            "\tstruct static_call_site *site;",
            "",
            "#ifdef CONFIG_LIVEPATCH_WO_FTRACE",
            "\tif (unlikely(!mod_klp_rel_completed(mod)))",
            "\t\treturn 0;",
            "#endif",
            "",
            "\tfor (site = start; site != stop; site++) {",
            "\t\tunsigned long s_key = __static_call_key(site);",
            "\t\tunsigned long addr = s_key & ~STATIC_CALL_SITE_FLAGS;",
            "\t\tunsigned long key;",
            "",
            "\t\t/*",
            "\t\t * Is the key is exported, 'addr' points to the key, which",
            "\t\t * means modules are allowed to call static_call_update() on",
            "\t\t * it.",
            "\t\t *",
            "\t\t * Otherwise, the key isn't exported, and 'addr' points to the",
            "\t\t * trampoline so we need to lookup the key.",
            "\t\t *",
            "\t\t * We go through this dance to prevent crazy modules from",
            "\t\t * abusing sensitive static calls.",
            "\t\t */",
            "\t\tif (!kernel_text_address(addr))",
            "\t\t\tcontinue;",
            "",
            "\t\tkey = tramp_key_lookup(addr);",
            "\t\tif (!key) {",
            "\t\t\tpr_warn(\"Failed to fixup __raw_static_call() usage at: %ps\\n\",",
            "\t\t\t\tstatic_call_addr(site));",
            "\t\t\treturn -EINVAL;",
            "\t\t}",
            "",
            "\t\tkey |= s_key & STATIC_CALL_SITE_FLAGS;",
            "\t\tsite->key = key - (long)&site->key;",
            "\t}",
            "",
            "\treturn __static_call_init(mod, start, stop);",
            "}",
            "static void static_call_del_module(struct module *mod)",
            "{",
            "\tstruct static_call_site *start = mod->static_call_sites;",
            "\tstruct static_call_site *stop = mod->static_call_sites +",
            "\t\t\t\t\tmod->num_static_call_sites;",
            "\tstruct static_call_key *key, *prev_key = NULL;",
            "\tstruct static_call_mod *site_mod, **prev;",
            "\tstruct static_call_site *site;",
            "",
            "#ifdef CONFIG_LIVEPATCH_WO_FTRACE",
            "\tif (unlikely(!mod_klp_rel_completed(mod)))",
            "\t\treturn;",
            "#endif",
            "",
            "\tfor (site = start; site < stop; site++) {",
            "\t\tkey = static_call_key(site);",
            "",
            "\t\t/*",
            "\t\t * If the key was not updated due to a memory allocation",
            "\t\t * failure in __static_call_init() then treating key::sites",
            "\t\t * as key::mods in the code below would cause random memory",
            "\t\t * access and #GP. In that case all subsequent sites have",
            "\t\t * not been touched either, so stop iterating.",
            "\t\t */",
            "\t\tif (!static_call_key_has_mods(key))",
            "\t\t\tbreak;",
            "",
            "\t\tif (key == prev_key)",
            "\t\t\tcontinue;",
            "",
            "\t\tprev_key = key;",
            "",
            "\t\tfor (prev = &key->mods, site_mod = key->mods;",
            "\t\t     site_mod && site_mod->mod != mod;",
            "\t\t     prev = &site_mod->next, site_mod = site_mod->next)",
            "\t\t\t;",
            "",
            "\t\tif (!site_mod)",
            "\t\t\tcontinue;",
            "",
            "\t\t*prev = site_mod->next;",
            "\t\tkfree(site_mod);",
            "\t}",
            "}"
          ],
          "function_name": "tramp_key_lookup, static_call_add_module, static_call_del_module",
          "description": "处理模块动态加载/卸载时的静态调用更新，通过键查找机制确保跨模块调用正确性，并维护静态调用站点的模块绑定关系。",
          "similarity": 0.6011219024658203
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/static_call_inline.c",
          "start_line": 453,
          "end_line": 552,
          "content": [
            "static int static_call_module_notify(struct notifier_block *nb,",
            "\t\t\t\t     unsigned long val, void *data)",
            "{",
            "\tstruct module *mod = data;",
            "\tint ret = 0;",
            "",
            "\tcpus_read_lock();",
            "\tstatic_call_lock();",
            "",
            "\tswitch (val) {",
            "\tcase MODULE_STATE_COMING:",
            "\t\tret = static_call_add_module(mod);",
            "\t\tif (ret) {",
            "\t\t\tpr_warn(\"Failed to allocate memory for static calls\\n\");",
            "\t\t\tstatic_call_del_module(mod);",
            "\t\t}",
            "\t\tbreak;",
            "\tcase MODULE_STATE_GOING:",
            "\t\tstatic_call_del_module(mod);",
            "\t\tbreak;",
            "\t}",
            "",
            "\tstatic_call_unlock();",
            "\tcpus_read_unlock();",
            "",
            "\treturn notifier_from_errno(ret);",
            "}",
            "int klp_static_call_register(struct module *mod)",
            "{",
            "\tint ret;",
            "",
            "\tret = static_call_module_notify(&static_call_module_nb, MODULE_STATE_COMING, mod);",
            "\treturn notifier_to_errno(ret);",
            "}",
            "static inline int __static_call_mod_text_reserved(void *start, void *end)",
            "{",
            "\treturn 0;",
            "}",
            "int static_call_text_reserved(void *start, void *end)",
            "{",
            "\tbool init = system_state < SYSTEM_RUNNING;",
            "\tint ret = __static_call_text_reserved(__start_static_call_sites,",
            "\t\t\t__stop_static_call_sites, start, end, init);",
            "",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\treturn __static_call_mod_text_reserved(start, end);",
            "}",
            "int __init static_call_init(void)",
            "{",
            "\tint ret;",
            "",
            "\t/* See static_call_force_reinit(). */",
            "\tif (static_call_initialized == 1)",
            "\t\treturn 0;",
            "",
            "\tcpus_read_lock();",
            "\tstatic_call_lock();",
            "\tret = __static_call_init(NULL, __start_static_call_sites,",
            "\t\t\t\t __stop_static_call_sites);",
            "\tstatic_call_unlock();",
            "\tcpus_read_unlock();",
            "",
            "\tif (ret) {",
            "\t\tpr_err(\"Failed to allocate memory for static_call!\\n\");",
            "\t\tBUG();",
            "\t}",
            "",
            "#ifdef CONFIG_MODULES",
            "\tif (!static_call_initialized)",
            "\t\tregister_module_notifier(&static_call_module_nb);",
            "#endif",
            "",
            "\tstatic_call_initialized = 1;",
            "\treturn 0;",
            "}",
            "static int func_a(int x)",
            "{",
            "\treturn x+1;",
            "}",
            "static int func_b(int x)",
            "{",
            "\treturn x+2;",
            "}",
            "static int __init test_static_call_init(void)",
            "{",
            "      int i;",
            "",
            "      for (i = 0; i < ARRAY_SIZE(static_call_data); i++ ) {",
            "\t      struct static_call_data *scd = &static_call_data[i];",
            "",
            "              if (scd->func)",
            "                      static_call_update(sc_selftest, scd->func);",
            "",
            "              WARN_ON(static_call(sc_selftest)(scd->val) != scd->expect);",
            "      }",
            "",
            "      return 0;",
            "}"
          ],
          "function_name": "static_call_module_notify, klp_static_call_register, __static_call_mod_text_reserved, static_call_text_reserved, static_call_init, func_a, func_b, test_static_call_init",
          "description": "实现模块状态变更通知机制，完成静态调用系统的初始化注册，包含测试函数用于验证静态调用逻辑的正确性。",
          "similarity": 0.5999325513839722
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/static_call_inline.c",
          "start_line": 1,
          "end_line": 22,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/init.h>",
            "#include <linux/static_call.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>",
            "#include <linux/sort.h>",
            "#include <linux/slab.h>",
            "#include <linux/module.h>",
            "#include <linux/cpu.h>",
            "#include <linux/processor.h>",
            "#include <asm/sections.h>",
            "",
            "extern struct static_call_site __start_static_call_sites[],",
            "\t\t\t       __stop_static_call_sites[];",
            "extern struct static_call_tramp_key __start_static_call_tramp_key[],",
            "\t\t\t\t    __stop_static_call_tramp_key[];",
            "",
            "int static_call_initialized;",
            "",
            "/*",
            " * Must be called before early_initcall() to be effective.",
            " */"
          ],
          "function_name": null,
          "description": "声明静态调用相关的全局变量和外部符号，定义静态调用初始化标志位，为后续静态调用站点管理和地址转换提供基础结构。",
          "similarity": 0.5571154356002808
        }
      ]
    },
    {
      "source_file": "kernel/sched/fair.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:09:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\fair.c`\n\n---\n\n# `sched/fair.c` 技术文档\n\n## 1. 文件概述\n\n`sched/fair.c` 是 Linux 内核中 **完全公平调度器**（Completely Fair Scheduler, CFS）的核心实现文件，负责实现 `SCHED_NORMAL` 和 `SCHED_BATCH` 调度策略。CFS 旨在通过红黑树（RB-tree）维护可运行任务的虚拟运行时间（vruntime），以实现 CPU 时间的公平分配。该文件实现了任务调度、负载跟踪、时间片计算、组调度（group scheduling）、NUMA 负载均衡、带宽控制等关键机制，是 Linux 通用调度子系统的核心组成部分。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_entity`：调度实体，代表一个可调度单元（任务或任务组）\n- `struct cfs_rq`：CFS 运行队列，管理一组调度实体\n- `struct load_weight`：负载权重结构，用于计算任务对系统负载的贡献\n\n### 关键函数与宏\n- `__calc_delta()` / `calc_delta_fair()`：计算基于权重的调度时间增量\n- `update_load_add()` / `update_load_sub()` / `update_load_set()`：更新负载权重\n- `__update_inv_weight()`：预计算权重的倒数以优化除法运算\n- `get_update_sysctl_factor()`：根据在线 CPU 数量动态调整调度参数\n- `update_sysctl()` / `sched_init_granularity()`：初始化和更新调度粒度参数\n- `for_each_sched_entity()`：遍历调度实体层级结构（用于组调度）\n\n### 可调参数（sysctl）\n- `sysctl_sched_base_slice`：基础时间片（默认 700,000 纳秒）\n- `sysctl_sched_tunable_scaling`：调度参数缩放策略（NONE/LOG/LINEAR）\n- `sysctl_sched_migration_cost`：任务迁移成本阈值（500 微秒）\n- `sysctl_sched_cfs_bandwidth_slice_us`（CFS 带宽控制切片，默认 5 毫秒）\n- `sysctl_numa_balancing_promote_rate_limit_MBps`（NUMA 页迁移速率限制）\n\n## 3. 关键实现\n\n### 虚拟时间与公平性\nCFS 使用 **虚拟运行时间**（vruntime）衡量任务已使用的 CPU 时间，并通过 `calc_delta_fair()` 将实际执行时间按任务权重归一化。权重由任务的 nice 值决定（`NICE_0_LOAD = 1024` 为基准）。调度器总是选择 vruntime 最小的任务运行，确保高优先级（高权重）任务获得更多 CPU 时间。\n\n### 高效除法优化\n为避免频繁除法运算，CFS 预计算 `inv_weight = WMULT_CONST / weight`（`WMULT_CONST = ~0U`），将除法转换为乘法和右移操作（`mul_u64_u32_shr`）。`__calc_delta()` 通过动态调整移位位数（`shift`）保证计算精度，适用于 32/64 位架构。\n\n### 动态粒度调整\n基础时间片 `sched_base_slice` 根据在线 CPU 数量动态缩放：\n- `SCHED_TUNABLESCALING_NONE`：固定值\n- `SCHED_TUNABLESCALING_LINEAR`：线性缩放（×ncpus）\n- `SCHED_TUNABLESCALING_LOG`（默认）：对数缩放（×(1 + ilog2(ncpus))）  \n此设计确保在多核系统中保持合理的调度延迟和交互性。\n\n### 组调度支持\n通过 `for_each_sched_entity()` 宏遍历任务所属的调度实体层级（任务 → 任务组 → 父任务组），实现 CPU 带宽在任务组间的公平分配。每个 `cfs_rq` 独立维护其子实体的红黑树。\n\n### SMP 相关优化\n- **非对称 CPU 优先级**：`arch_asym_cpu_priority()` 允许架构定义 CPU 能力差异（如大小核）\n- **容量比较宏**：`fits_capacity()`（20% 容差）和 `capacity_greater()`（5% 容差）用于负载均衡决策\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- 调度核心：`\"sched.h\"`、`\"stats.h\"`、`\"autogroup.h\"`\n- 系统服务：`<linux/sched/clock.h>`、`<linux/sched/nohz.h>`、`<linux/psi.h>`\n- 内存管理：`<linux/mem_policy.h>`、`<linux/energy_model.h>`\n- SMP 支持：`<linux/topology.h>`、`<linux/cpumask_api.h>`\n- 数据结构：`<linux/rbtree_augmented.h>`\n\n### 条件编译特性\n- `CONFIG_SMP`：多处理器调度优化\n- `CONFIG_CFS_BANDWIDTH`：CPU 带宽限制（cgroup v1/v2）\n- `CONFIG_NUMA_BALANCING`：NUMA 自动迁移\n- `CONFIG_FAIR_GROUP_SCHED`：CFS 组调度（cgroup 支持）\n\n## 5. 使用场景\n\n- **通用任务调度**：所有使用 `SCHED_NORMAL` 或 `SCHED_BATCH` 策略的用户态进程\n- **cgroup CPU 资源控制**：通过 `cpu.cfs_quota_us` 和 `cpu.cfs_period_us` 限制任务组带宽\n- **NUMA 优化**：自动迁移内存页以减少远程访问（`numa_balancing`）\n- **节能调度**：结合 `energy_model` 在满足性能前提下选择低功耗 CPU\n- **实时性保障**：通过 `cond_resched()` 在长循环中主动让出 CPU，避免内核抢占延迟过高\n- **系统调优**：管理员通过 `/proc/sys/kernel/` 下的 sysctl 参数动态调整调度行为",
      "similarity": 0.5763738751411438,
      "chunks": [
        {
          "chunk_id": 43,
          "file_path": "kernel/sched/fair.c",
          "start_line": 7054,
          "end_line": 7159,
          "content": [
            "static int dequeue_entities(struct rq *rq, struct sched_entity *se, int flags)",
            "{",
            "\tbool was_sched_idle = sched_idle_rq(rq);",
            "\tint rq_h_nr_running = rq->cfs.h_nr_running;",
            "\tbool task_sleep = flags & DEQUEUE_SLEEP;",
            "\tbool task_delayed = flags & DEQUEUE_DELAYED;",
            "\tstruct task_struct *p = NULL;",
            "\tint idle_h_nr_running = 0;",
            "\tint h_nr_running = 0;",
            "\tstruct cfs_rq *cfs_rq;",
            "\tu64 slice = 0;",
            "",
            "\tif (entity_is_task(se)) {",
            "\t\tp = task_of(se);",
            "\t\th_nr_running = 1;",
            "\t\tidle_h_nr_running = task_has_idle_policy(p);",
            "\t}",
            "",
            "\tfor_each_sched_entity(se) {",
            "\t\tcfs_rq = cfs_rq_of(se);",
            "",
            "\t\tif (!dequeue_entity(cfs_rq, se, flags)) {",
            "\t\t\tif (p && &p->se == se)",
            "\t\t\t\treturn -1;",
            "",
            "\t\t\tslice = cfs_rq_min_slice(cfs_rq);",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tcfs_rq->h_nr_running -= h_nr_running;",
            "\t\tcfs_rq->idle_h_nr_running -= idle_h_nr_running;",
            "",
            "\t\tif (cfs_rq_is_idle(cfs_rq))",
            "\t\t\tidle_h_nr_running = h_nr_running;",
            "",
            "\t\t/* end evaluation on encountering a throttled cfs_rq */",
            "\t\tif (cfs_rq_throttled(cfs_rq))",
            "\t\t\treturn 0;",
            "",
            "\t\t/* Don't dequeue parent if it has other entities besides us */",
            "\t\tif (cfs_rq->load.weight) {",
            "\t\t\tslice = cfs_rq_min_slice(cfs_rq);",
            "",
            "\t\t\t/* Avoid re-evaluating load for this entity: */",
            "\t\t\tse = parent_entity(se);",
            "\t\t\t/*",
            "\t\t\t * Bias pick_next to pick a task from this cfs_rq, as",
            "\t\t\t * p is sleeping when it is within its sched_slice.",
            "\t\t\t */",
            "\t\t\tif (task_sleep && se && !throttled_hierarchy(cfs_rq))",
            "\t\t\t\tset_next_buddy(se);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tflags |= DEQUEUE_SLEEP;",
            "\t\tflags &= ~(DEQUEUE_DELAYED | DEQUEUE_SPECIAL);",
            "\t}",
            "",
            "\tfor_each_sched_entity(se) {",
            "\t\tcfs_rq = cfs_rq_of(se);",
            "",
            "\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);",
            "\t\tse_update_runnable(se);",
            "\t\tupdate_cfs_group(se);",
            "",
            "\t\tse->slice = slice;",
            "\t\tif (se != cfs_rq->curr)",
            "\t\t\tmin_vruntime_cb_propagate(&se->run_node, NULL);",
            "\t\tslice = cfs_rq_min_slice(cfs_rq);",
            "",
            "\t\tcfs_rq->h_nr_running -= h_nr_running;",
            "\t\tcfs_rq->idle_h_nr_running -= idle_h_nr_running;",
            "",
            "\t\tif (cfs_rq_is_idle(cfs_rq))",
            "\t\t\tidle_h_nr_running = h_nr_running;",
            "",
            "\t\t/* end evaluation on encountering a throttled cfs_rq */",
            "\t\tif (cfs_rq_throttled(cfs_rq))",
            "\t\t\treturn 0;",
            "\t}",
            "",
            "\tsub_nr_running(rq, h_nr_running);",
            "",
            "\tif (rq_h_nr_running && !rq->cfs.h_nr_running)",
            "\t\tdl_server_stop(&rq->fair_server);",
            "",
            "\t/* balance early to pull high priority tasks */",
            "\tif (unlikely(!was_sched_idle && sched_idle_rq(rq)))",
            "\t\trq->next_balance = jiffies;",
            "",
            "\tif (p && task_delayed) {",
            "\t\tSCHED_WARN_ON(!task_sleep);",
            "\t\tSCHED_WARN_ON(p->on_rq != 1);",
            "",
            "\t\t/* Fix-up what dequeue_task_fair() skipped */",
            "\t\thrtick_update(rq);",
            "",
            "\t\t/*",
            "\t\t * Fix-up what block_task() skipped.",
            "\t\t *",
            "\t\t * Must be last, @p might not be valid after this.",
            "\t\t */",
            "\t\t__block_task(rq, p);",
            "\t}",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "dequeue_entities",
          "description": "从运行队列中移除调度实体，处理节流队列检查、负载更新及运行队列统计信息维护，支持延迟实体的特殊处理。",
          "similarity": 0.631867527961731
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/sched/fair.c",
          "start_line": 830,
          "end_line": 948,
          "content": [
            "static inline bool min_vruntime_update(struct sched_entity *se, bool exit)",
            "{",
            "\tu64 old_min_vruntime = se->min_vruntime;",
            "\tu64 old_min_slice = se->min_slice;",
            "\tstruct rb_node *node = &se->run_node;",
            "",
            "\tse->min_vruntime = se->vruntime;",
            "\t__min_vruntime_update(se, node->rb_right);",
            "\t__min_vruntime_update(se, node->rb_left);",
            "",
            "\tse->min_slice = se->slice;",
            "\t__min_slice_update(se, node->rb_right);",
            "\t__min_slice_update(se, node->rb_left);",
            "",
            "\treturn se->min_vruntime == old_min_vruntime &&",
            "\t       se->min_slice == old_min_slice;",
            "}",
            "static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)",
            "{",
            "\tavg_vruntime_add(cfs_rq, se);",
            "\tse->min_vruntime = se->vruntime;",
            "\tse->min_slice = se->slice;",
            "\trb_add_augmented_cached(&se->run_node, &cfs_rq->tasks_timeline,",
            "\t\t\t\t__entity_less, &min_vruntime_cb);",
            "}",
            "static void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)",
            "{",
            "\trb_erase_augmented_cached(&se->run_node, &cfs_rq->tasks_timeline,",
            "\t\t\t\t  &min_vruntime_cb);",
            "\tavg_vruntime_sub(cfs_rq, se);",
            "}",
            "int sched_update_scaling(void)",
            "{",
            "\tunsigned int factor = get_update_sysctl_factor();",
            "",
            "#define WRT_SYSCTL(name) \\",
            "\t(normalized_sysctl_##name = sysctl_##name / (factor))",
            "\tWRT_SYSCTL(sched_base_slice);",
            "#undef WRT_SYSCTL",
            "",
            "\treturn 0;",
            "}",
            "static bool update_deadline(struct cfs_rq *cfs_rq, struct sched_entity *se)",
            "{",
            "\tif ((s64)(se->vruntime - se->deadline) < 0)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * For EEVDF the virtual time slope is determined by w_i (iow.",
            "\t * nice) while the request time r_i is determined by",
            "\t * sysctl_sched_base_slice.",
            "\t */",
            "\tif (!se->custom_slice)",
            "\t\tse->slice = sysctl_sched_base_slice;",
            "",
            "\t/*",
            "\t * EEVDF: vd_i = ve_i + r_i / w_i",
            "\t */",
            "\tse->deadline = se->vruntime + calc_delta_fair(se->slice, se);",
            "",
            "\t/*",
            "\t * The task has consumed its request, reschedule.",
            "\t */",
            "\treturn true;",
            "}",
            "void init_entity_runnable_average(struct sched_entity *se)",
            "{",
            "\tstruct sched_avg *sa = &se->avg;",
            "",
            "\tmemset(sa, 0, sizeof(*sa));",
            "",
            "\t/*",
            "\t * Tasks are initialized with full load to be seen as heavy tasks until",
            "\t * they get a chance to stabilize to their real load level.",
            "\t * Group entities are initialized with zero load to reflect the fact that",
            "\t * nothing has been attached to the task group yet.",
            "\t */",
            "\tif (entity_is_task(se))",
            "\t\tsa->load_avg = scale_load_down(se->load.weight);",
            "",
            "\t/* when this task is enqueued, it will contribute to its cfs_rq's load_avg */",
            "}",
            "void post_init_entity_util_avg(struct task_struct *p)",
            "{",
            "\tstruct sched_entity *se = &p->se;",
            "\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);",
            "\tstruct sched_avg *sa = &se->avg;",
            "\tlong cpu_scale = arch_scale_cpu_capacity(cpu_of(rq_of(cfs_rq)));",
            "\tlong cap = (long)(cpu_scale - cfs_rq->avg.util_avg) / 2;",
            "",
            "\tif (p->sched_class != &fair_sched_class) {",
            "\t\t/*",
            "\t\t * For !fair tasks do:",
            "\t\t *",
            "\t\tupdate_cfs_rq_load_avg(now, cfs_rq);",
            "\t\tattach_entity_load_avg(cfs_rq, se);",
            "\t\tswitched_from_fair(rq, p);",
            "\t\t *",
            "\t\t * such that the next switched_to_fair() has the",
            "\t\t * expected state.",
            "\t\t */",
            "\t\tse->avg.last_update_time = cfs_rq_clock_pelt(cfs_rq);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (cap > 0) {",
            "\t\tif (cfs_rq->avg.util_avg != 0) {",
            "\t\t\tsa->util_avg  = cfs_rq->avg.util_avg * se->load.weight;",
            "\t\t\tsa->util_avg /= (cfs_rq->avg.load_avg + 1);",
            "",
            "\t\t\tif (sa->util_avg > cap)",
            "\t\t\t\tsa->util_avg = cap;",
            "\t\t} else {",
            "\t\t\tsa->util_avg = cap;",
            "\t\t}",
            "\t}",
            "",
            "\tsa->runnable_avg = sa->util_avg;",
            "}"
          ],
          "function_name": "min_vruntime_update, __enqueue_entity, __dequeue_entity, sched_update_scaling, update_deadline, init_entity_runnable_average, post_init_entity_util_avg",
          "description": "min_vruntime_update 更新调度实体的最小虚拟运行时间及切片值，通过红黑树操作同步子节点数据。__enqueueEntity 将实体插入CFS队列红黑树并更新平均运行时间。__dequeueEntity 从红黑树移除实体并减少平均运行时间。sched_update_scaling 调整调度基线切片因子。update_deadline 计算截止时间用于EDF算法。init_entity_runnable_average 初始化实体可运行平均负载。post_init_entity_util_avg 设置实体利用率平均值。",
          "similarity": 0.6173977851867676
        },
        {
          "chunk_id": 52,
          "file_path": "kernel/sched/fair.c",
          "start_line": 8581,
          "end_line": 8709,
          "content": [
            "static void set_task_max_allowed_capacity(struct task_struct *p)",
            "{",
            "\tstruct asym_cap_data *entry;",
            "",
            "\tif (!sched_asym_cpucap_active())",
            "\t\treturn;",
            "",
            "\trcu_read_lock();",
            "\tlist_for_each_entry_rcu(entry, &asym_cap_list, link) {",
            "\t\tcpumask_t *cpumask;",
            "",
            "\t\tcpumask = cpu_capacity_span(entry);",
            "\t\tif (!cpumask_intersects(p->cpus_ptr, cpumask))",
            "\t\t\tcontinue;",
            "",
            "\t\tp->max_allowed_capacity = entry->capacity;",
            "\t\tbreak;",
            "\t}",
            "\trcu_read_unlock();",
            "}",
            "static void set_cpus_allowed_fair(struct task_struct *p, struct affinity_context *ctx)",
            "{",
            "\tset_cpus_allowed_common(p, ctx);",
            "\tset_task_max_allowed_capacity(p);",
            "}",
            "static int",
            "balance_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)",
            "{",
            "\tif (sched_fair_runnable(rq))",
            "\t\treturn 1;",
            "",
            "\treturn sched_balance_newidle(rq, rf) != 0;",
            "}",
            "static inline void set_task_max_allowed_capacity(struct task_struct *p) {}",
            "static void set_next_buddy(struct sched_entity *se)",
            "{",
            "\tfor_each_sched_entity(se) {",
            "\t\tif (SCHED_WARN_ON(!se->on_rq))",
            "\t\t\treturn;",
            "\t\tif (se_is_idle(se))",
            "\t\t\treturn;",
            "\t\tcfs_rq_of(se)->next = se;",
            "\t}",
            "}",
            "static void check_preempt_wakeup_fair(struct rq *rq, struct task_struct *p, int wake_flags)",
            "{",
            "\tstruct task_struct *curr = rq->curr;",
            "\tstruct sched_entity *se = &curr->se, *pse = &p->se;",
            "\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);",
            "\tint next_buddy_marked = 0;",
            "\tint cse_is_idle, pse_is_idle;",
            "",
            "\tif (unlikely(se == pse))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This is possible from callers such as attach_tasks(), in which we",
            "\t * unconditionally wakeup_preempt() after an enqueue (which may have",
            "\t * lead to a throttle).  This both saves work and prevents false",
            "\t * next-buddy nomination below.",
            "\t */",
            "\tif (unlikely(throttled_hierarchy(cfs_rq_of(pse))))",
            "\t\treturn;",
            "",
            "\tif (sched_feat(NEXT_BUDDY) && !(wake_flags & WF_FORK) && !pse->sched_delayed) {",
            "\t\tset_next_buddy(pse);",
            "\t\tnext_buddy_marked = 1;",
            "\t}",
            "",
            "\t/*",
            "\t * We can come here with TIF_NEED_RESCHED already set from new task",
            "\t * wake up path.",
            "\t *",
            "\t * Note: this also catches the edge-case of curr being in a throttled",
            "\t * group (e.g. via set_curr_task), since update_curr() (in the",
            "\t * enqueue of curr) will have resulted in resched being set.  This",
            "\t * prevents us from potentially nominating it as a false LAST_BUDDY",
            "\t * below.",
            "\t */",
            "\tif (test_tsk_need_resched(curr))",
            "\t\treturn;",
            "",
            "\tif (!sched_feat(WAKEUP_PREEMPTION))",
            "\t\treturn;",
            "",
            "\tfind_matching_se(&se, &pse);",
            "\tWARN_ON_ONCE(!pse);",
            "",
            "\tcse_is_idle = se_is_idle(se);",
            "\tpse_is_idle = se_is_idle(pse);",
            "",
            "\t/*",
            "\t * Preempt an idle entity in favor of a non-idle entity (and don't preempt",
            "\t * in the inverse case).",
            "\t */",
            "\tif (cse_is_idle && !pse_is_idle)",
            "\t\tgoto preempt;",
            "\tif (cse_is_idle != pse_is_idle)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * BATCH and IDLE tasks do not preempt others.",
            "\t */",
            "\tif (unlikely(!normal_policy(p->policy)))",
            "\t\treturn;",
            "",
            "\tcfs_rq = cfs_rq_of(se);",
            "\tupdate_curr(cfs_rq);",
            "\t/*",
            "\t * If @p has a shorter slice than current and @p is eligible, override",
            "\t * current's slice protection in order to allow preemption.",
            "\t *",
            "\t * Note that even if @p does not turn out to be the most eligible",
            "\t * task at this moment, current's slice protection will be lost.",
            "\t */",
            "\tif (do_preempt_short(cfs_rq, pse, se) && se->vlag == se->deadline)",
            "\t\tse->vlag = se->deadline + 1;",
            "",
            "\t/*",
            "\t * If @p has become the most eligible task, force preemption.",
            "\t */",
            "\tif (pick_eevdf(cfs_rq) == pse)",
            "\t\tgoto preempt;",
            "",
            "\treturn;",
            "",
            "preempt:",
            "\tresched_curr(rq);",
            "}"
          ],
          "function_name": "set_task_max_allowed_capacity, set_cpus_allowed_fair, balance_fair, set_task_max_allowed_capacity, set_next_buddy, check_preempt_wakeup_fair",
          "description": "设置任务最大允许容量，实现负载均衡判断逻辑，管理调度实体间的抢占关系和运行队列状态更新。",
          "similarity": 0.6075229644775391
        },
        {
          "chunk_id": 73,
          "file_path": "kernel/sched/fair.c",
          "start_line": 12860,
          "end_line": 12960,
          "content": [
            "bool cfs_prio_less(const struct task_struct *a, const struct task_struct *b,",
            "\t\t\tbool in_fi)",
            "{",
            "\tstruct rq *rq = task_rq(a);",
            "\tconst struct sched_entity *sea = &a->se;",
            "\tconst struct sched_entity *seb = &b->se;",
            "\tstruct cfs_rq *cfs_rqa;",
            "\tstruct cfs_rq *cfs_rqb;",
            "\ts64 delta;",
            "",
            "\tSCHED_WARN_ON(task_rq(b)->core != rq->core);",
            "",
            "#ifdef CONFIG_FAIR_GROUP_SCHED",
            "\t/*",
            "\t * Find an se in the hierarchy for tasks a and b, such that the se's",
            "\t * are immediate siblings.",
            "\t */",
            "\twhile (sea->cfs_rq->tg != seb->cfs_rq->tg) {",
            "\t\tint sea_depth = sea->depth;",
            "\t\tint seb_depth = seb->depth;",
            "",
            "\t\tif (sea_depth >= seb_depth)",
            "\t\t\tsea = parent_entity(sea);",
            "\t\tif (sea_depth <= seb_depth)",
            "\t\t\tseb = parent_entity(seb);",
            "\t}",
            "",
            "\tse_fi_update(sea, rq->core->core_forceidle_seq, in_fi);",
            "\tse_fi_update(seb, rq->core->core_forceidle_seq, in_fi);",
            "",
            "\tcfs_rqa = sea->cfs_rq;",
            "\tcfs_rqb = seb->cfs_rq;",
            "#else",
            "\tcfs_rqa = &task_rq(a)->cfs;",
            "\tcfs_rqb = &task_rq(b)->cfs;",
            "#endif",
            "",
            "\t/*",
            "\t * Find delta after normalizing se's vruntime with its cfs_rq's",
            "\t * min_vruntime_fi, which would have been updated in prior calls",
            "\t * to se_fi_update().",
            "\t */",
            "\tdelta = (s64)(sea->vruntime - seb->vruntime) +",
            "\t\t(s64)(cfs_rqb->min_vruntime_fi - cfs_rqa->min_vruntime_fi);",
            "",
            "\treturn delta > 0;",
            "}",
            "static int task_is_throttled_fair(struct task_struct *p, int cpu)",
            "{",
            "\tstruct cfs_rq *cfs_rq;",
            "",
            "#ifdef CONFIG_FAIR_GROUP_SCHED",
            "\tcfs_rq = task_group(p)->cfs_rq[cpu];",
            "#else",
            "\tcfs_rq = &cpu_rq(cpu)->cfs;",
            "#endif",
            "\treturn throttled_hierarchy(cfs_rq);",
            "}",
            "static inline void task_tick_core(struct rq *rq, struct task_struct *curr) {}",
            "static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)",
            "{",
            "\tstruct cfs_rq *cfs_rq;",
            "\tstruct sched_entity *se = &curr->se;",
            "",
            "\tfor_each_sched_entity(se) {",
            "\t\tcfs_rq = cfs_rq_of(se);",
            "\t\tentity_tick(cfs_rq, se, queued);",
            "\t}",
            "",
            "\tif (static_branch_unlikely(&sched_numa_balancing))",
            "\t\ttask_tick_numa(rq, curr);",
            "",
            "\tupdate_misfit_status(curr, rq);",
            "\tcheck_update_overutilized_status(task_rq(curr));",
            "",
            "\ttask_tick_core(rq, curr);",
            "}",
            "static void task_fork_fair(struct task_struct *p)",
            "{",
            "\tset_task_max_allowed_capacity(p);",
            "}",
            "static void",
            "prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)",
            "{",
            "\tif (!task_on_rq_queued(p))",
            "\t\treturn;",
            "",
            "\tif (rq->cfs.nr_running == 1)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Reschedule if we are currently running on this runqueue and",
            "\t * our priority decreased, or if we are not currently running on",
            "\t * this runqueue and our priority is higher than the current's",
            "\t */",
            "\tif (task_current(rq, p)) {",
            "\t\tif (p->prio > oldprio)",
            "\t\t\tresched_curr(rq);",
            "\t} else",
            "\t\twakeup_preempt(rq, p, 0);",
            "}"
          ],
          "function_name": "cfs_prio_less, task_is_throttled_fair, task_tick_core, task_tick_fair, task_fork_fair, prio_changed_fair",
          "description": "实现基于CFS的优先级比较和任务调度规则，处理任务优先级变更时的重调度需求，校验任务是否受限制，并维护强制空闲态下的虚拟运行时间计算逻辑。",
          "similarity": 0.5970910787582397
        },
        {
          "chunk_id": 42,
          "file_path": "kernel/sched/fair.c",
          "start_line": 6828,
          "end_line": 7021,
          "content": [
            "static inline bool cpu_overutilized(int cpu)",
            "{",
            "\tunsigned long  rq_util_min, rq_util_max;",
            "",
            "\tif (!sched_energy_enabled())",
            "\t\treturn false;",
            "",
            "\trq_util_min = uclamp_rq_get(cpu_rq(cpu), UCLAMP_MIN);",
            "\trq_util_max = uclamp_rq_get(cpu_rq(cpu), UCLAMP_MAX);",
            "",
            "\t/* Return true only if the utilization doesn't fit CPU's capacity */",
            "\treturn !util_fits_cpu(cpu_util_cfs(cpu), rq_util_min, rq_util_max, cpu);",
            "}",
            "static inline bool is_rd_overutilized(struct root_domain *rd)",
            "{",
            "\treturn !sched_energy_enabled() || READ_ONCE(rd->overutilized);",
            "}",
            "static inline void set_rd_overutilized(struct root_domain *rd, bool flag)",
            "{",
            "\tif (!sched_energy_enabled())",
            "\t\treturn;",
            "",
            "\tWRITE_ONCE(rd->overutilized, flag);",
            "\ttrace_sched_overutilized_tp(rd, flag);",
            "}",
            "static inline void check_update_overutilized_status(struct rq *rq)",
            "{",
            "\t/*",
            "\t * overutilized field is used for load balancing decisions only",
            "\t * if energy aware scheduler is being used",
            "\t */",
            "",
            "\tif (!is_rd_overutilized(rq->rd) && cpu_overutilized(rq->cpu))",
            "\t\tset_rd_overutilized(rq->rd, 1);",
            "}",
            "static inline void check_update_overutilized_status(struct rq *rq) { }",
            "static int sched_idle_rq(struct rq *rq)",
            "{",
            "\treturn unlikely(rq->nr_running == rq->cfs.idle_h_nr_running &&",
            "\t\t\trq->nr_running);",
            "}",
            "static int sched_idle_cpu(int cpu)",
            "{",
            "\treturn sched_idle_rq(cpu_rq(cpu));",
            "}",
            "static void",
            "requeue_delayed_entity(struct sched_entity *se)",
            "{",
            "\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);",
            "",
            "\t/*",
            "\t * se->sched_delayed should imply: se->on_rq == 1.",
            "\t * Because a delayed entity is one that is still on",
            "\t * the runqueue competing until elegibility.",
            "\t */",
            "\tSCHED_WARN_ON(!se->sched_delayed);",
            "\tSCHED_WARN_ON(!se->on_rq);",
            "",
            "\tif (sched_feat(DELAY_ZERO)) {",
            "\t\tupdate_entity_lag(cfs_rq, se);",
            "\t\tif (se->vlag > 0) {",
            "\t\t\tcfs_rq->nr_running--;",
            "\t\t\tif (se != cfs_rq->curr)",
            "\t\t\t\t__dequeue_entity(cfs_rq, se);",
            "\t\t\tse->vlag = 0;",
            "\t\t\tplace_entity(cfs_rq, se, 0);",
            "\t\t\tif (se != cfs_rq->curr)",
            "\t\t\t\t__enqueue_entity(cfs_rq, se);",
            "\t\t\tcfs_rq->nr_running++;",
            "\t\t}",
            "\t}",
            "",
            "\tupdate_load_avg(cfs_rq, se, 0);",
            "\tse->sched_delayed = 0;",
            "}",
            "static void",
            "enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct cfs_rq *cfs_rq;",
            "\tstruct sched_entity *se = &p->se;",
            "\tint idle_h_nr_running = task_has_idle_policy(p);",
            "\tint task_new = !(flags & ENQUEUE_WAKEUP);",
            "\tint rq_h_nr_running = rq->cfs.h_nr_running;",
            "\tu64 slice = 0;",
            "",
            "\tif (flags & ENQUEUE_DELAYED) {",
            "\t\trequeue_delayed_entity(se);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * The code below (indirectly) updates schedutil which looks at",
            "\t * the cfs_rq utilization to select a frequency.",
            "\t * Let's add the task's estimated utilization to the cfs_rq's",
            "\t * estimated utilization, before we update schedutil.",
            "\t */",
            "\tutil_est_enqueue(&rq->cfs, p);",
            "",
            "\t/*",
            "\t * If in_iowait is set, the code below may not trigger any cpufreq",
            "\t * utilization updates, so do it here explicitly with the IOWAIT flag",
            "\t * passed.",
            "\t */",
            "\tif (p->in_iowait)",
            "\t\tcpufreq_update_util(rq, SCHED_CPUFREQ_IOWAIT);",
            "",
            "\tfor_each_sched_entity(se) {",
            "\t\tif (se->on_rq) {",
            "\t\t\tif (se->sched_delayed)",
            "\t\t\t\trequeue_delayed_entity(se);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tcfs_rq = cfs_rq_of(se);",
            "",
            "\t\t/*",
            "\t\t * Basically set the slice of group entries to the min_slice of",
            "\t\t * their respective cfs_rq. This ensures the group can service",
            "\t\t * its entities in the desired time-frame.",
            "\t\t */",
            "\t\tif (slice) {",
            "\t\t\tse->slice = slice;",
            "\t\t\tse->custom_slice = 1;",
            "\t\t}",
            "\t\tenqueue_entity(cfs_rq, se, flags);",
            "\t\tslice = cfs_rq_min_slice(cfs_rq);",
            "",
            "\t\tcfs_rq->h_nr_running++;",
            "\t\tcfs_rq->idle_h_nr_running += idle_h_nr_running;",
            "",
            "\t\tif (cfs_rq_is_idle(cfs_rq))",
            "\t\t\tidle_h_nr_running = 1;",
            "",
            "\t\t/* end evaluation on encountering a throttled cfs_rq */",
            "\t\tif (cfs_rq_throttled(cfs_rq))",
            "\t\t\tgoto enqueue_throttle;",
            "",
            "\t\tflags = ENQUEUE_WAKEUP;",
            "\t}",
            "",
            "\tfor_each_sched_entity(se) {",
            "\t\tcfs_rq = cfs_rq_of(se);",
            "",
            "\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);",
            "\t\tse_update_runnable(se);",
            "\t\tupdate_cfs_group(se);",
            "",
            "\t\tse->slice = slice;",
            "\t\tif (se != cfs_rq->curr)",
            "\t\t\tmin_vruntime_cb_propagate(&se->run_node, NULL);",
            "\t\tslice = cfs_rq_min_slice(cfs_rq);",
            "",
            "\t\tcfs_rq->h_nr_running++;",
            "\t\tcfs_rq->idle_h_nr_running += idle_h_nr_running;",
            "",
            "\t\tif (cfs_rq_is_idle(cfs_rq))",
            "\t\t\tidle_h_nr_running = 1;",
            "",
            "\t\t/* end evaluation on encountering a throttled cfs_rq */",
            "\t\tif (cfs_rq_throttled(cfs_rq))",
            "\t\t\tgoto enqueue_throttle;",
            "\t}",
            "",
            "\tif (!rq_h_nr_running && rq->cfs.h_nr_running) {",
            "\t\t/* Account for idle runtime */",
            "\t\tif (!rq->nr_running)",
            "\t\t\tdl_server_update_idle_time(rq, rq->curr);",
            "\t\tdl_server_start(&rq->fair_server);",
            "\t}",
            "",
            "\t/* At this point se is NULL and we are at root level*/",
            "\tadd_nr_running(rq, 1);",
            "",
            "\t/*",
            "\t * Since new tasks are assigned an initial util_avg equal to",
            "\t * half of the spare capacity of their CPU, tiny tasks have the",
            "\t * ability to cross the overutilized threshold, which will",
            "\t * result in the load balancer ruining all the task placement",
            "\t * done by EAS. As a way to mitigate that effect, do not account",
            "\t * for the first enqueue operation of new tasks during the",
            "\t * overutilized flag detection.",
            "\t *",
            "\t * A better way of solving this problem would be to wait for",
            "\t * the PELT signals of tasks to converge before taking them",
            "\t * into account, but that is not straightforward to implement,",
            "\t * and the following generally works well enough in practice.",
            "\t */",
            "\tif (!task_new)",
            "\t\tcheck_update_overutilized_status(rq);",
            "",
            "enqueue_throttle:",
            "\tassert_list_leaf_cfs_rq(rq);",
            "",
            "\thrtick_update(rq);",
            "}"
          ],
          "function_name": "cpu_overutilized, is_rd_overutilized, set_rd_overutilized, check_update_overutilized_status, check_update_overutilized_status, sched_idle_rq, sched_idle_cpu, requeue_delayed_entity, enqueue_task_fair",
          "description": "实现CPU利用率检测逻辑，包括过载判定、根域过载标志设置及更新，处理空闲运行队列和延迟实体重排的辅助函数。",
          "similarity": 0.5966098308563232
        }
      ]
    }
  ]
}