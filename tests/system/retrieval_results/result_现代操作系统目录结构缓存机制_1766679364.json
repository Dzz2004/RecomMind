{
  "query": "现代操作系统目录结构缓存机制",
  "timestamp": "2025-12-26 00:16:04",
  "retrieved_files": [
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.5934939384460449,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.5359940528869629
        },
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.5356693863868713
        },
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.5124369263648987
        }
      ]
    },
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.5905103087425232,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.5627347230911255
        },
        {
          "chunk_id": 2,
          "file_path": "mm/mempolicy.c",
          "start_line": 285,
          "end_line": 392,
          "content": [
            "int numa_nearest_node(int node, unsigned int state)",
            "{",
            "\tint min_dist = INT_MAX, dist, n, min_node;",
            "",
            "\tif (state >= NR_NODE_STATES)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (node == NUMA_NO_NODE || node_state(node, state))",
            "\t\treturn node;",
            "",
            "\tmin_node = node;",
            "\tfor_each_node_state(n, state) {",
            "\t\tdist = node_distance(node, n);",
            "\t\tif (dist < min_dist) {",
            "\t\t\tmin_dist = dist;",
            "\t\t\tmin_node = n;",
            "\t\t}",
            "\t}",
            "",
            "\treturn min_node;",
            "}",
            "static inline int mpol_store_user_nodemask(const struct mempolicy *pol)",
            "{",
            "\treturn pol->flags & MPOL_MODE_FLAGS;",
            "}",
            "static void mpol_relative_nodemask(nodemask_t *ret, const nodemask_t *orig,",
            "\t\t\t\t   const nodemask_t *rel)",
            "{",
            "\tnodemask_t tmp;",
            "\tnodes_fold(tmp, *orig, nodes_weight(*rel));",
            "\tnodes_onto(*ret, tmp, *rel);",
            "}",
            "static int mpol_new_nodemask(struct mempolicy *pol, const nodemask_t *nodes)",
            "{",
            "\tif (nodes_empty(*nodes))",
            "\t\treturn -EINVAL;",
            "\tpol->nodes = *nodes;",
            "\treturn 0;",
            "}",
            "static int mpol_new_preferred(struct mempolicy *pol, const nodemask_t *nodes)",
            "{",
            "\tif (nodes_empty(*nodes))",
            "\t\treturn -EINVAL;",
            "",
            "\tnodes_clear(pol->nodes);",
            "\tnode_set(first_node(*nodes), pol->nodes);",
            "\treturn 0;",
            "}",
            "static int mpol_set_nodemask(struct mempolicy *pol,",
            "\t\t     const nodemask_t *nodes, struct nodemask_scratch *nsc)",
            "{",
            "\tint ret;",
            "",
            "\t/*",
            "\t * Default (pol==NULL) resp. local memory policies are not a",
            "\t * subject of any remapping. They also do not need any special",
            "\t * constructor.",
            "\t */",
            "\tif (!pol || pol->mode == MPOL_LOCAL)",
            "\t\treturn 0;",
            "",
            "\t/* Check N_MEMORY */",
            "\tnodes_and(nsc->mask1,",
            "\t\t  cpuset_current_mems_allowed, node_states[N_MEMORY]);",
            "",
            "\tVM_BUG_ON(!nodes);",
            "",
            "\tif (pol->flags & MPOL_F_RELATIVE_NODES)",
            "\t\tmpol_relative_nodemask(&nsc->mask2, nodes, &nsc->mask1);",
            "\telse",
            "\t\tnodes_and(nsc->mask2, *nodes, nsc->mask1);",
            "",
            "\tif (mpol_store_user_nodemask(pol))",
            "\t\tpol->w.user_nodemask = *nodes;",
            "\telse",
            "\t\tpol->w.cpuset_mems_allowed = cpuset_current_mems_allowed;",
            "",
            "\tret = mpol_ops[pol->mode].create(pol, &nsc->mask2);",
            "\treturn ret;",
            "}",
            "void __mpol_put(struct mempolicy *pol)",
            "{",
            "\tif (!atomic_dec_and_test(&pol->refcnt))",
            "\t\treturn;",
            "\tkmem_cache_free(policy_cache, pol);",
            "}",
            "static void mpol_rebind_default(struct mempolicy *pol, const nodemask_t *nodes)",
            "{",
            "}",
            "static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)",
            "{",
            "\tnodemask_t tmp;",
            "",
            "\tif (pol->flags & MPOL_F_STATIC_NODES)",
            "\t\tnodes_and(tmp, pol->w.user_nodemask, *nodes);",
            "\telse if (pol->flags & MPOL_F_RELATIVE_NODES)",
            "\t\tmpol_relative_nodemask(&tmp, &pol->w.user_nodemask, nodes);",
            "\telse {",
            "\t\tnodes_remap(tmp, pol->nodes, pol->w.cpuset_mems_allowed,",
            "\t\t\t\t\t\t\t\t*nodes);",
            "\t\tpol->w.cpuset_mems_allowed = *nodes;",
            "\t}",
            "",
            "\tif (nodes_empty(tmp))",
            "\t\ttmp = *nodes;",
            "",
            "\tpol->nodes = tmp;",
            "}"
          ],
          "function_name": "numa_nearest_node, mpol_store_user_nodemask, mpol_relative_nodemask, mpol_new_nodemask, mpol_new_preferred, mpol_set_nodemask, __mpol_put, mpol_rebind_default, mpol_rebind_nodemask",
          "description": "提供节点掩码操作辅助函数，包括最近节点查找、相对掩码转换、节点掩码设置及策略绑定重置，用于维护内存策略的节点约束条件。",
          "similarity": 0.5511499047279358
        },
        {
          "chunk_id": 10,
          "file_path": "mm/mempolicy.c",
          "start_line": 1735,
          "end_line": 1838,
          "content": [
            "static long kernel_set_mempolicy(int mode, const unsigned long __user *nmask,",
            "\t\t\t\t unsigned long maxnode)",
            "{",
            "\tunsigned short mode_flags;",
            "\tnodemask_t nodes;",
            "\tint lmode = mode;",
            "\tint err;",
            "",
            "\terr = sanitize_mpol_flags(&lmode, &mode_flags);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\terr = get_nodes(&nodes, nmask, maxnode);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\treturn do_set_mempolicy(lmode, mode_flags, &nodes);",
            "}",
            "static int kernel_migrate_pages(pid_t pid, unsigned long maxnode,",
            "\t\t\t\tconst unsigned long __user *old_nodes,",
            "\t\t\t\tconst unsigned long __user *new_nodes)",
            "{",
            "\tstruct mm_struct *mm = NULL;",
            "\tstruct task_struct *task;",
            "\tnodemask_t task_nodes;",
            "\tint err;",
            "\tnodemask_t *old;",
            "\tnodemask_t *new;",
            "\tNODEMASK_SCRATCH(scratch);",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\told = &scratch->mask1;",
            "\tnew = &scratch->mask2;",
            "",
            "\terr = get_nodes(old, old_nodes, maxnode);",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\terr = get_nodes(new, new_nodes, maxnode);",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\t/* Find the mm_struct */",
            "\trcu_read_lock();",
            "\ttask = pid ? find_task_by_vpid(pid) : current;",
            "\tif (!task) {",
            "\t\trcu_read_unlock();",
            "\t\terr = -ESRCH;",
            "\t\tgoto out;",
            "\t}",
            "\tget_task_struct(task);",
            "",
            "\terr = -EINVAL;",
            "",
            "\t/*",
            "\t * Check if this process has the right to modify the specified process.",
            "\t * Use the regular \"ptrace_may_access()\" checks.",
            "\t */",
            "\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {",
            "\t\trcu_read_unlock();",
            "\t\terr = -EPERM;",
            "\t\tgoto out_put;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\ttask_nodes = cpuset_mems_allowed(task);",
            "\t/* Is the user allowed to access the target nodes? */",
            "\tif (!nodes_subset(*new, task_nodes) && !capable(CAP_SYS_NICE)) {",
            "\t\terr = -EPERM;",
            "\t\tgoto out_put;",
            "\t}",
            "",
            "\ttask_nodes = cpuset_mems_allowed(current);",
            "\tnodes_and(*new, *new, task_nodes);",
            "\tif (nodes_empty(*new))",
            "\t\tgoto out_put;",
            "",
            "\terr = security_task_movememory(task);",
            "\tif (err)",
            "\t\tgoto out_put;",
            "",
            "\tmm = get_task_mm(task);",
            "\tput_task_struct(task);",
            "",
            "\tif (!mm) {",
            "\t\terr = -EINVAL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\terr = do_migrate_pages(mm, old, new,",
            "\t\tcapable(CAP_SYS_NICE) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE);",
            "",
            "\tmmput(mm);",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "",
            "\treturn err;",
            "",
            "out_put:",
            "\tput_task_struct(task);",
            "\tgoto out;",
            "}"
          ],
          "function_name": "kernel_set_mempolicy, kernel_migrate_pages",
          "description": "kernel_set_mempolicy 设置进程的内存放置策略，通过sanitize_mpol_flags验证模式标志并调用do_set_mempolicy应用策略；kernel_migrate_pages 实现页面迁移，检查目标进程权限，限制迁移节点范围，并调用do_migrate_pages进行实际迁移操作。",
          "similarity": 0.5413869619369507
        },
        {
          "chunk_id": 3,
          "file_path": "mm/mempolicy.c",
          "start_line": 484,
          "end_line": 639,
          "content": [
            "static void mpol_rebind_preferred(struct mempolicy *pol,",
            "\t\t\t\t\t\tconst nodemask_t *nodes)",
            "{",
            "\tpol->w.cpuset_mems_allowed = *nodes;",
            "}",
            "static void mpol_rebind_policy(struct mempolicy *pol, const nodemask_t *newmask)",
            "{",
            "\tif (!pol || pol->mode == MPOL_LOCAL)",
            "\t\treturn;",
            "\tif (!mpol_store_user_nodemask(pol) &&",
            "\t    nodes_equal(pol->w.cpuset_mems_allowed, *newmask))",
            "\t\treturn;",
            "",
            "\tmpol_ops[pol->mode].rebind(pol, newmask);",
            "}",
            "void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new)",
            "{",
            "\tmpol_rebind_policy(tsk->mempolicy, new);",
            "}",
            "void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "",
            "\tmmap_write_lock(mm);",
            "\tfor_each_vma(vmi, vma) {",
            "\t\tvma_start_write(vma);",
            "\t\tmpol_rebind_policy(vma->vm_policy, new);",
            "\t}",
            "\tmmap_write_unlock(mm);",
            "}",
            "static bool strictly_unmovable(unsigned long flags)",
            "{",
            "\t/*",
            "\t * STRICT without MOVE flags lets do_mbind() fail immediately with -EIO",
            "\t * if any misplaced page is found.",
            "\t */",
            "\treturn (flags & (MPOL_MF_STRICT | MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ==",
            "\t\t\t MPOL_MF_STRICT;",
            "}",
            "static inline bool queue_folio_required(struct folio *folio,",
            "\t\t\t\t\tstruct queue_pages *qp)",
            "{",
            "\tint nid = folio_nid(folio);",
            "\tunsigned long flags = qp->flags;",
            "",
            "\treturn node_isset(nid, *qp->nmask) == !(flags & MPOL_MF_INVERT);",
            "}",
            "static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)",
            "{",
            "\tstruct folio *folio;",
            "\tstruct queue_pages *qp = walk->private;",
            "",
            "\tif (unlikely(is_pmd_migration_entry(*pmd))) {",
            "\t\tqp->nr_failed++;",
            "\t\treturn;",
            "\t}",
            "\tfolio = pmd_folio(*pmd);",
            "\tif (is_huge_zero_folio(folio)) {",
            "\t\twalk->action = ACTION_CONTINUE;",
            "\t\treturn;",
            "\t}",
            "\tif (!queue_folio_required(folio, qp))",
            "\t\treturn;",
            "\tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||",
            "\t    !vma_migratable(walk->vma) ||",
            "\t    !migrate_folio_add(folio, qp->pagelist, qp->flags))",
            "\t\tqp->nr_failed++;",
            "}",
            "static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,",
            "\t\t\tunsigned long end, struct mm_walk *walk)",
            "{",
            "\tconst fpb_t fpb_flags = FPB_IGNORE_DIRTY | FPB_IGNORE_SOFT_DIRTY;",
            "\tstruct vm_area_struct *vma = walk->vma;",
            "\tstruct folio *folio;",
            "\tstruct queue_pages *qp = walk->private;",
            "\tunsigned long flags = qp->flags;",
            "\tpte_t *pte, *mapped_pte;",
            "\tpte_t ptent;",
            "\tspinlock_t *ptl;",
            "\tint max_nr, nr;",
            "",
            "\tptl = pmd_trans_huge_lock(pmd, vma);",
            "\tif (ptl) {",
            "\t\tqueue_folios_pmd(pmd, walk);",
            "\t\tspin_unlock(ptl);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tmapped_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);",
            "\tif (!pte) {",
            "\t\twalk->action = ACTION_AGAIN;",
            "\t\treturn 0;",
            "\t}",
            "\tfor (; addr != end; pte += nr, addr += nr * PAGE_SIZE) {",
            "\t\tmax_nr = (end - addr) >> PAGE_SHIFT;",
            "\t\tnr = 1;",
            "\t\tptent = ptep_get(pte);",
            "\t\tif (pte_none(ptent))",
            "\t\t\tcontinue;",
            "\t\tif (!pte_present(ptent)) {",
            "\t\t\tif (is_migration_entry(pte_to_swp_entry(ptent)))",
            "\t\t\t\tqp->nr_failed++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfolio = vm_normal_folio(vma, addr, ptent);",
            "\t\tif (!folio || folio_is_zone_device(folio))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_large(folio) && max_nr != 1)",
            "\t\t\tnr = folio_pte_batch(folio, addr, pte, ptent,",
            "\t\t\t\t\t     max_nr, fpb_flags,",
            "\t\t\t\t\t     NULL, NULL, NULL);",
            "\t\t/*",
            "\t\t * vm_normal_folio() filters out zero pages, but there might",
            "\t\t * still be reserved folios to skip, perhaps in a VDSO.",
            "\t\t */",
            "\t\tif (folio_test_reserved(folio))",
            "\t\t\tcontinue;",
            "\t\tif (!queue_folio_required(folio, qp))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_large(folio)) {",
            "\t\t\t/*",
            "\t\t\t * A large folio can only be isolated from LRU once,",
            "\t\t\t * but may be mapped by many PTEs (and Copy-On-Write may",
            "\t\t\t * intersperse PTEs of other, order 0, folios).  This is",
            "\t\t\t * a common case, so don't mistake it for failure (but",
            "\t\t\t * there can be other cases of multi-mapped pages which",
            "\t\t\t * this quick check does not help to filter out - and a",
            "\t\t\t * search of the pagelist might grow to be prohibitive).",
            "\t\t\t *",
            "\t\t\t * migrate_pages(&pagelist) returns nr_failed folios, so",
            "\t\t\t * check \"large\" now so that queue_pages_range() returns",
            "\t\t\t * a comparable nr_failed folios.  This does imply that",
            "\t\t\t * if folio could not be isolated for some racy reason",
            "\t\t\t * at its first PTE, later PTEs will not give it another",
            "\t\t\t * chance of isolation; but keeps the accounting simple.",
            "\t\t\t */",
            "\t\t\tif (folio == qp->large)",
            "\t\t\t\tcontinue;",
            "\t\t\tqp->large = folio;",
            "\t\t}",
            "\t\tif (!(flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||",
            "\t\t    !vma_migratable(vma) ||",
            "\t\t    !migrate_folio_add(folio, qp->pagelist, flags)) {",
            "\t\t\tqp->nr_failed += nr;",
            "\t\t\tif (strictly_unmovable(flags))",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tpte_unmap_unlock(mapped_pte, ptl);",
            "\tcond_resched();",
            "out:",
            "\tif (qp->nr_failed && strictly_unmovable(flags))",
            "\t\treturn -EIO;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "mpol_rebind_preferred, mpol_rebind_policy, mpol_rebind_task, mpol_rebind_mm, strictly_unmovable, queue_folio_required, queue_folios_pmd, queue_folios_pte_range",
          "description": "处理页表项遍历与迁移操作，包含页帧队列检测、迁移标志验证及大页迁移逻辑，确保内存分配符合NUMA策略要求。",
          "similarity": 0.5380010008811951
        },
        {
          "chunk_id": 6,
          "file_path": "mm/mempolicy.c",
          "start_line": 1012,
          "end_line": 1144,
          "content": [
            "static void get_policy_nodemask(struct mempolicy *pol, nodemask_t *nodes)",
            "{",
            "\tnodes_clear(*nodes);",
            "\tif (pol == &default_policy)",
            "\t\treturn;",
            "",
            "\tswitch (pol->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*nodes = pol->nodes;",
            "\t\tbreak;",
            "\tcase MPOL_LOCAL:",
            "\t\t/* return empty node mask for local allocation */",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static int lookup_node(struct mm_struct *mm, unsigned long addr)",
            "{",
            "\tstruct page *p = NULL;",
            "\tint ret;",
            "",
            "\tret = get_user_pages_fast(addr & PAGE_MASK, 1, 0, &p);",
            "\tif (ret > 0) {",
            "\t\tret = page_to_nid(p);",
            "\t\tput_page(p);",
            "\t}",
            "\treturn ret;",
            "}",
            "static long do_get_mempolicy(int *policy, nodemask_t *nmask,",
            "\t\t\t     unsigned long addr, unsigned long flags)",
            "{",
            "\tint err;",
            "\tstruct mm_struct *mm = current->mm;",
            "\tstruct vm_area_struct *vma = NULL;",
            "\tstruct mempolicy *pol = current->mempolicy, *pol_refcount = NULL;",
            "",
            "\tif (flags &",
            "\t\t~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (flags & MPOL_F_MEMS_ALLOWED) {",
            "\t\tif (flags & (MPOL_F_NODE|MPOL_F_ADDR))",
            "\t\t\treturn -EINVAL;",
            "\t\t*policy = 0;\t/* just so it's initialized */",
            "\t\ttask_lock(current);",
            "\t\t*nmask  = cpuset_current_mems_allowed;",
            "\t\ttask_unlock(current);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (flags & MPOL_F_ADDR) {",
            "\t\tpgoff_t ilx;\t\t/* ignored here */",
            "\t\t/*",
            "\t\t * Do NOT fall back to task policy if the",
            "\t\t * vma/shared policy at addr is NULL.  We",
            "\t\t * want to return MPOL_DEFAULT in this case.",
            "\t\t */",
            "\t\tmmap_read_lock(mm);",
            "\t\tvma = vma_lookup(mm, addr);",
            "\t\tif (!vma) {",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\treturn -EFAULT;",
            "\t\t}",
            "\t\tpol = __get_vma_policy(vma, addr, &ilx);",
            "\t} else if (addr)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!pol)",
            "\t\tpol = &default_policy;\t/* indicates default behavior */",
            "",
            "\tif (flags & MPOL_F_NODE) {",
            "\t\tif (flags & MPOL_F_ADDR) {",
            "\t\t\t/*",
            "\t\t\t * Take a refcount on the mpol, because we are about to",
            "\t\t\t * drop the mmap_lock, after which only \"pol\" remains",
            "\t\t\t * valid, \"vma\" is stale.",
            "\t\t\t */",
            "\t\t\tpol_refcount = pol;",
            "\t\t\tvma = NULL;",
            "\t\t\tmpol_get(pol);",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\terr = lookup_node(mm, addr);",
            "\t\t\tif (err < 0)",
            "\t\t\t\tgoto out;",
            "\t\t\t*policy = err;",
            "\t\t} else if (pol == current->mempolicy &&",
            "\t\t\t\tpol->mode == MPOL_INTERLEAVE) {",
            "\t\t\t*policy = next_node_in(current->il_prev, pol->nodes);",
            "\t\t} else if (pol == current->mempolicy &&",
            "\t\t\t\tpol->mode == MPOL_WEIGHTED_INTERLEAVE) {",
            "\t\t\tif (current->il_weight)",
            "\t\t\t\t*policy = current->il_prev;",
            "\t\t\telse",
            "\t\t\t\t*policy = next_node_in(current->il_prev,",
            "\t\t\t\t\t\t       pol->nodes);",
            "\t\t} else {",
            "\t\t\terr = -EINVAL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t} else {",
            "\t\t*policy = pol == &default_policy ? MPOL_DEFAULT :",
            "\t\t\t\t\t\tpol->mode;",
            "\t\t/*",
            "\t\t * Internal mempolicy flags must be masked off before exposing",
            "\t\t * the policy to userspace.",
            "\t\t */",
            "\t\t*policy |= (pol->flags & MPOL_MODE_FLAGS);",
            "\t}",
            "",
            "\terr = 0;",
            "\tif (nmask) {",
            "\t\tif (mpol_store_user_nodemask(pol)) {",
            "\t\t\t*nmask = pol->w.user_nodemask;",
            "\t\t} else {",
            "\t\t\ttask_lock(current);",
            "\t\t\tget_policy_nodemask(pol, nmask);",
            "\t\t\ttask_unlock(current);",
            "\t\t}",
            "\t}",
            "",
            " out:",
            "\tmpol_cond_put(pol);",
            "\tif (vma)",
            "\t\tmmap_read_unlock(mm);",
            "\tif (pol_refcount)",
            "\t\tmpol_put(pol_refcount);",
            "\treturn err;",
            "}"
          ],
          "function_name": "get_policy_nodemask, lookup_node, do_get_mempolicy",
          "description": "获取内存策略节点掩码，lookup_node查询页面所属节点，do_get_mempolicy根据地址或标志获取当前/VA内存策略并返回节点掩码",
          "similarity": 0.5376051664352417
        }
      ]
    },
    {
      "source_file": "mm/sparse.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:25:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse.c`\n\n---\n\n# sparse.c 技术文档\n\n## 1. 文件概述\n\n`sparse.c` 是 Linux 内核中实现 **SPARSEMEM（稀疏内存模型）** 的核心文件，用于管理物理内存的稀疏映射。该模型将整个物理地址空间划分为固定大小的“内存段”（memory sections），仅对实际存在的内存段分配 `mem_map`（页描述符数组），从而在支持大物理地址空间的同时节省内存开销。此文件负责内存段的初始化、节点关联、存在性标记以及与内存热插拔和 vmemmap 相关的功能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`mem_section`**: 全局内存段数组，每个元素代表一个内存段，存储该段的 `mem_map` 指针及其他元数据。\n  - 在 `CONFIG_SPARSEMEM_EXTREME` 下为二级指针（动态分配根数组）\n  - 否则为静态二维数组 `[NR_SECTION_ROOTS][SECTIONS_PER_ROOT]`\n- **`section_to_node_table`**: （仅当 `NODE_NOT_IN_PAGE_FLAGS` 时）用于通过内存段号查找所属 NUMA 节点的查找表。\n- **`__highest_present_section_nr`**: 记录当前系统中编号最大的已存在内存段，用于优化遍历。\n\n### 主要函数\n- **`memory_present()`**: 标记指定 PFN 范围内的内存段为“存在”，并关联到指定 NUMA 节点。\n- **`memblocks_present()`**: 遍历所有 memblock 内存区域，调用 `memory_present()` 标记所有系统内存。\n- **`sparse_index_init()`**: （仅 `CONFIG_SPARSEMEM_EXTREME`）为指定内存段分配其所在的根数组项。\n- **`sparse_encode_mem_map()` / `sparse_decode_mem_map()`**: 编码/解码 `mem_map` 指针，使其能通过段内偏移计算出实际 PFN。\n- **`subsection_map_init()`**: （仅 `CONFIG_SPARSEMEM_VMEMMAP`）初始化子段（subsection）位图，用于更细粒度的内存管理。\n- **`page_to_nid()`**: （仅 `NODE_NOT_IN_PAGE_FLAGS`）通过页结构获取其所属 NUMA 节点。\n- **`mminit_validate_memmodel_limits()`**: 验证传入的 PFN 范围是否超出 SPARSEMEM 模型支持的最大地址。\n\n### 辅助宏与内联函数\n- **`for_each_present_section_nr()`**: 高效遍历所有已存在的内存段。\n- **`first_present_section_nr()`**: 获取第一个存在的内存段编号。\n- **`sparse_encode_early_nid()` / `sparse_early_nid()`**: 在早期启动阶段利用 `section_mem_map` 字段临时存储 NUMA 节点 ID。\n\n## 3. 关键实现\n\n### 内存段管理\n- 物理内存被划分为 `PAGES_PER_SECTION` 大小的段（通常 128MB）。\n- `mem_section` 数组索引即为段号（section number），通过 `__nr_to_section()` 宏访问。\n- 段的存在性通过 `SECTION_MARKED_PRESENT` 位标记，并维护 `__highest_present_section_nr` 以加速遍历。\n\n### NUMA 节点关联\n- 若页结构体（`struct page`）中未直接存储节点 ID（`NODE_NOT_IN_PAGE_FLAGS`），则使用 `section_to_node_table` 查找。\n- 在 `memory_present()` 中通过 `set_section_nid()` 建立段到节点的映射。\n\n### 动态内存段分配（SPARSEMEM_EXTREME）\n- 为减少静态内存占用，`mem_section` 采用二级结构：\n  - 一级：`mem_section[]` 指向多个二级数组\n  - 二级：每个 `mem_section[root]` 指向 `SECTIONS_PER_ROOT` 个 `struct mem_section`\n- `sparse_index_init()` 在需要时动态分配二级数组（使用 `kzalloc_node` 或 `memblock_alloc_node`）。\n\n### 早期启动阶段的优化\n- 在 `mem_map` 分配前，复用 `section_mem_map` 字段的高位存储 NUMA 节点 ID（`sparse_encode_early_nid()`）。\n- 此信息在分配真实 `mem_map` 前被清除。\n\n### vmemmap 子段支持\n- 当启用 `CONFIG_SPARSEMEM_VMEMMAP` 时，每个内存段进一步划分为子段（subsections）。\n- `subsection_map_init()` 初始化位图，标记哪些子段包含有效内存，支持更灵活的内存热插拔。\n\n### 地址空间验证\n- `mminit_validate_memmodel_limits()` 确保传入的 PFN 范围不超过 `PHYSMEM_END`（SPARSEMEM 模型最大支持地址），防止越界。\n\n## 4. 依赖关系\n\n- **头文件依赖**:\n  - `<linux/mm.h>`, `<linux/mmzone.h>`: 内存管理核心定义\n  - `<linux/memblock.h>`: 早期内存分配器\n  - `<linux/vmalloc.h>`: 用于 vmemmap 映射\n  - `<asm/dma.h>`: 架构相关 DMA 定义\n  - `\"internal.h\"`: MM 子系统内部头文件\n- **配置选项依赖**:\n  - `CONFIG_SPARSEMEM`: 基础稀疏内存模型\n  - `CONFIG_SPARSEMEM_EXTREME`: 动态内存段分配\n  - `CONFIG_SPARSEMEM_VMEMMAP`: 使用虚拟映射的 mem_map\n  - `CONFIG_MEMORY_HOTPLUG`: 内存热插拔支持\n  - `NODE_NOT_IN_PAGE_FLAGS`: 页结构体不包含节点 ID\n- **与其他模块交互**:\n  - **Memory Block (memblock)**: 通过 `for_each_mem_pfn_range()` 获取初始内存布局\n  - **Page Allocator**: 提供 `struct page` 数组（mem_map）\n  - **NUMA Subsystem**: 通过节点 ID 关联内存与 CPU 拓扑\n  - **Memory Hotplug**: 依赖本文件提供的段管理接口进行内存增删\n\n## 5. 使用场景\n\n- **系统启动初始化**:\n  - `memblocks_present()` 在 `mm_init()` 阶段被调用，标记所有固件报告的内存区域为“存在”。\n- **内存热插拔**:\n  - 热添加内存时，调用 `memory_present()` 标记新段；热移除时清理对应段。\n  - `sparse_index_init()` 支持动态扩展 `mem_section` 数组。\n- **页到节点转换**:\n  - 当 `page_to_nid()` 被调用时（如页面迁移、NUMA 调度），通过段查找节点。\n- **vmemmap 优化**:\n  - 在支持 `SPARSEMEM_VMEMMAP` 的架构（如 x86_64, ARM64）上，`subsection_map_init()` 使内核能按子段粒度映射 `struct page`，减少虚拟地址空间占用。\n- **调试与验证**:\n  - `mminit_validate_memmodel_limits()` 在开发阶段捕获内存模型配置错误。",
      "similarity": 0.5864207744598389,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/sparse.c",
          "start_line": 219,
          "end_line": 339,
          "content": [
            "void __init subsection_map_init(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "}",
            "static void __init memory_present(int nid, unsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn;",
            "",
            "#ifdef CONFIG_SPARSEMEM_EXTREME",
            "\tif (unlikely(!mem_section)) {",
            "\t\tunsigned long size, align;",
            "",
            "\t\tsize = sizeof(struct mem_section *) * NR_SECTION_ROOTS;",
            "\t\talign = 1 << (INTERNODE_CACHE_SHIFT);",
            "\t\tmem_section = memblock_alloc(size, align);",
            "\t\tif (!mem_section)",
            "\t\t\tpanic(\"%s: Failed to allocate %lu bytes align=0x%lx\\n\",",
            "\t\t\t      __func__, size, align);",
            "\t}",
            "#endif",
            "",
            "\tstart &= PAGE_SECTION_MASK;",
            "\tmminit_validate_memmodel_limits(&start, &end);",
            "\tfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION) {",
            "\t\tunsigned long section = pfn_to_section_nr(pfn);",
            "\t\tstruct mem_section *ms;",
            "",
            "\t\tsparse_index_init(section, nid);",
            "\t\tset_section_nid(section, nid);",
            "",
            "\t\tms = __nr_to_section(section);",
            "\t\tif (!ms->section_mem_map) {",
            "\t\t\tms->section_mem_map = sparse_encode_early_nid(nid) |",
            "\t\t\t\t\t\t\tSECTION_IS_ONLINE;",
            "\t\t\t__section_mark_present(ms, section);",
            "\t\t}",
            "\t}",
            "}",
            "static void __init memblocks_present(void)",
            "{",
            "\tunsigned long start, end;",
            "\tint i, nid;",
            "",
            "\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, &nid)",
            "\t\tmemory_present(nid, start, end);",
            "}",
            "static unsigned long sparse_encode_mem_map(struct page *mem_map, unsigned long pnum)",
            "{",
            "\tunsigned long coded_mem_map =",
            "\t\t(unsigned long)(mem_map - (section_nr_to_pfn(pnum)));",
            "\tBUILD_BUG_ON(SECTION_MAP_LAST_BIT > PFN_SECTION_SHIFT);",
            "\tBUG_ON(coded_mem_map & ~SECTION_MAP_MASK);",
            "\treturn coded_mem_map;",
            "}",
            "static void __meminit sparse_init_one_section(struct mem_section *ms,",
            "\t\tunsigned long pnum, struct page *mem_map,",
            "\t\tstruct mem_section_usage *usage, unsigned long flags)",
            "{",
            "\tms->section_mem_map &= ~SECTION_MAP_MASK;",
            "\tms->section_mem_map |= sparse_encode_mem_map(mem_map, pnum)",
            "\t\t| SECTION_HAS_MEM_MAP | flags;",
            "\tms->usage = usage;",
            "}",
            "static unsigned long usemap_size(void)",
            "{",
            "\treturn BITS_TO_LONGS(SECTION_BLOCKFLAGS_BITS) * sizeof(unsigned long);",
            "}",
            "size_t mem_section_usage_size(void)",
            "{",
            "\treturn sizeof(struct mem_section_usage) + usemap_size();",
            "}",
            "static inline phys_addr_t pgdat_to_phys(struct pglist_data *pgdat)",
            "{",
            "#ifndef CONFIG_NUMA",
            "\tVM_BUG_ON(pgdat != &contig_page_data);",
            "\treturn __pa_symbol(&contig_page_data);",
            "#else",
            "\treturn __pa(pgdat);",
            "#endif",
            "}",
            "static void __init check_usemap_section_nr(int nid,",
            "\t\tstruct mem_section_usage *usage)",
            "{",
            "\tunsigned long usemap_snr, pgdat_snr;",
            "\tstatic unsigned long old_usemap_snr;",
            "\tstatic unsigned long old_pgdat_snr;",
            "\tstruct pglist_data *pgdat = NODE_DATA(nid);",
            "\tint usemap_nid;",
            "",
            "\t/* First call */",
            "\tif (!old_usemap_snr) {",
            "\t\told_usemap_snr = NR_MEM_SECTIONS;",
            "\t\told_pgdat_snr = NR_MEM_SECTIONS;",
            "\t}",
            "",
            "\tusemap_snr = pfn_to_section_nr(__pa(usage) >> PAGE_SHIFT);",
            "\tpgdat_snr = pfn_to_section_nr(pgdat_to_phys(pgdat) >> PAGE_SHIFT);",
            "\tif (usemap_snr == pgdat_snr)",
            "\t\treturn;",
            "",
            "\tif (old_usemap_snr == usemap_snr && old_pgdat_snr == pgdat_snr)",
            "\t\t/* skip redundant message */",
            "\t\treturn;",
            "",
            "\told_usemap_snr = usemap_snr;",
            "\told_pgdat_snr = pgdat_snr;",
            "",
            "\tusemap_nid = sparse_early_nid(__nr_to_section(usemap_snr));",
            "\tif (usemap_nid != nid) {",
            "\t\tpr_info(\"node %d must be removed before remove section %ld\\n\",",
            "\t\t\tnid, usemap_snr);",
            "\t\treturn;",
            "\t}",
            "\t/*",
            "\t * There is a circular dependency.",
            "\t * Some platforms allow un-removable section because they will just",
            "\t * gather other removable sections for dynamic partitioning.",
            "\t * Just notify un-removable section's number here.",
            "\t */",
            "\tpr_info(\"Section %ld and %ld (node %d) have a circular dependency on usemap and pgdat allocations\\n\",",
            "\t\tusemap_snr, pgdat_snr, nid);",
            "}"
          ],
          "function_name": "subsection_map_init, memory_present, memblocks_present, sparse_encode_mem_map, sparse_init_one_section, usemap_size, mem_section_usage_size, pgdat_to_phys, check_usemap_section_nr",
          "description": "实现memory_present标记内存区段为有效，memblocks_present遍历所有内存块执行此操作。提供sparse_encode_mem_map编码内存图，sparse_init_one_section初始化区段结构体。包含使用图大小计算、PGDAT物理地址转换及使用图与PGDAT的依赖检查功能。",
          "similarity": 0.5470656752586365
        },
        {
          "chunk_id": 4,
          "file_path": "mm/sparse.c",
          "start_line": 592,
          "end_line": 692,
          "content": [
            "void online_mem_sections(unsigned long start_pfn, unsigned long end_pfn)",
            "{",
            "\tunsigned long pfn;",
            "",
            "\tfor (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {",
            "\t\tunsigned long section_nr = pfn_to_section_nr(pfn);",
            "\t\tstruct mem_section *ms;",
            "",
            "\t\t/* onlining code should never touch invalid ranges */",
            "\t\tif (WARN_ON(!valid_section_nr(section_nr)))",
            "\t\t\tcontinue;",
            "",
            "\t\tms = __nr_to_section(section_nr);",
            "\t\tms->section_mem_map |= SECTION_IS_ONLINE;",
            "\t}",
            "}",
            "void offline_mem_sections(unsigned long start_pfn, unsigned long end_pfn)",
            "{",
            "\tunsigned long pfn;",
            "",
            "\tfor (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {",
            "\t\tunsigned long section_nr = pfn_to_section_nr(pfn);",
            "\t\tstruct mem_section *ms;",
            "",
            "\t\t/*",
            "\t\t * TODO this needs some double checking. Offlining code makes",
            "\t\t * sure to check pfn_valid but those checks might be just bogus",
            "\t\t */",
            "\t\tif (WARN_ON(!valid_section_nr(section_nr)))",
            "\t\t\tcontinue;",
            "",
            "\t\tms = __nr_to_section(section_nr);",
            "\t\tms->section_mem_map &= ~SECTION_IS_ONLINE;",
            "\t}",
            "}",
            "static void depopulate_section_memmap(unsigned long pfn, unsigned long nr_pages,",
            "\t\tstruct vmem_altmap *altmap)",
            "{",
            "\tunsigned long start = (unsigned long) pfn_to_page(pfn);",
            "\tunsigned long end = start + nr_pages * sizeof(struct page);",
            "",
            "\tvmemmap_free(start, end, altmap);",
            "}",
            "static void free_map_bootmem(struct page *memmap)",
            "{",
            "\tunsigned long start = (unsigned long)memmap;",
            "\tunsigned long end = (unsigned long)(memmap + PAGES_PER_SECTION);",
            "",
            "\tvmemmap_free(start, end, NULL);",
            "}",
            "static int clear_subsection_map(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\tDECLARE_BITMAP(map, SUBSECTIONS_PER_SECTION) = { 0 };",
            "\tDECLARE_BITMAP(tmp, SUBSECTIONS_PER_SECTION) = { 0 };",
            "\tstruct mem_section *ms = __pfn_to_section(pfn);",
            "\tunsigned long *subsection_map = ms->usage",
            "\t\t? &ms->usage->subsection_map[0] : NULL;",
            "",
            "\tsubsection_mask_set(map, pfn, nr_pages);",
            "\tif (subsection_map)",
            "\t\tbitmap_and(tmp, map, subsection_map, SUBSECTIONS_PER_SECTION);",
            "",
            "\tif (WARN(!subsection_map || !bitmap_equal(tmp, map, SUBSECTIONS_PER_SECTION),",
            "\t\t\t\t\"section already deactivated (%#lx + %ld)\\n\",",
            "\t\t\t\tpfn, nr_pages))",
            "\t\treturn -EINVAL;",
            "",
            "\tbitmap_xor(subsection_map, map, subsection_map, SUBSECTIONS_PER_SECTION);",
            "\treturn 0;",
            "}",
            "static bool is_subsection_map_empty(struct mem_section *ms)",
            "{",
            "\treturn bitmap_empty(&ms->usage->subsection_map[0],",
            "\t\t\t    SUBSECTIONS_PER_SECTION);",
            "}",
            "static int fill_subsection_map(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\tstruct mem_section *ms = __pfn_to_section(pfn);",
            "\tDECLARE_BITMAP(map, SUBSECTIONS_PER_SECTION) = { 0 };",
            "\tunsigned long *subsection_map;",
            "\tint rc = 0;",
            "",
            "\tsubsection_mask_set(map, pfn, nr_pages);",
            "",
            "\tsubsection_map = &ms->usage->subsection_map[0];",
            "",
            "\tif (bitmap_empty(map, SUBSECTIONS_PER_SECTION))",
            "\t\trc = -EINVAL;",
            "\telse if (bitmap_intersects(map, subsection_map, SUBSECTIONS_PER_SECTION))",
            "\t\trc = -EEXIST;",
            "\telse",
            "\t\tbitmap_or(subsection_map, map, subsection_map,",
            "\t\t\t\tSUBSECTIONS_PER_SECTION);",
            "",
            "\treturn rc;",
            "}",
            "static void depopulate_section_memmap(unsigned long pfn, unsigned long nr_pages,",
            "\t\tstruct vmem_altmap *altmap)",
            "{",
            "\tkvfree(pfn_to_page(pfn));",
            "}"
          ],
          "function_name": "online_mem_sections, offline_mem_sections, depopulate_section_memmap, free_map_bootmem, clear_subsection_map, is_subsection_map_empty, fill_subsection_map, depopulate_section_memmap",
          "description": "提供区段在线/离线操作接口，修改区段在线状态标志。实现depopulate_section_memmap释放内存映射，free_map_bootmem释放启动内存。包含子部分掩码操作函数，用于跟踪和验证内存区域的有效性。",
          "similarity": 0.5406969785690308
        },
        {
          "chunk_id": 0,
          "file_path": "mm/sparse.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * sparse memory mappings.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/slab.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/compiler.h>",
            "#include <linux/highmem.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/swap.h>",
            "#include <linux/swapops.h>",
            "#include <linux/bootmem_info.h>",
            "",
            "#include \"internal.h\"",
            "#include <asm/dma.h>",
            "",
            "/*",
            " * Permanent SPARSEMEM data:",
            " *",
            " * 1) mem_section\t- memory sections, mem_map's for valid memory",
            " */",
            "#ifdef CONFIG_SPARSEMEM_EXTREME",
            "struct mem_section **mem_section;",
            "#else",
            "struct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT]",
            "\t____cacheline_internodealigned_in_smp;",
            "#endif",
            "EXPORT_SYMBOL(mem_section);",
            "",
            "#ifdef NODE_NOT_IN_PAGE_FLAGS",
            "/*",
            " * If we did not store the node number in the page then we have to",
            " * do a lookup in the section_to_node_table in order to find which",
            " * node the page belongs to.",
            " */",
            "#if MAX_NUMNODES <= 256",
            "static u8 section_to_node_table[NR_MEM_SECTIONS] __cacheline_aligned;",
            "#else",
            "static u16 section_to_node_table[NR_MEM_SECTIONS] __cacheline_aligned;",
            "#endif",
            ""
          ],
          "function_name": null,
          "description": "定义SPARSEMEM永久数据结构mem_section，用于存储内存区段信息。根据CONFIG_SPARSEMEM_EXTREME配置选择数组形式或指针数组。声明section_to_node_table用于节点号查询，当未在页结构中存储节点号时使用。",
          "similarity": 0.5240885019302368
        },
        {
          "chunk_id": 3,
          "file_path": "mm/sparse.c",
          "start_line": 410,
          "end_line": 527,
          "content": [
            "static void __init check_usemap_section_nr(int nid,",
            "\t\tstruct mem_section_usage *usage)",
            "{",
            "}",
            "static unsigned long __init section_map_size(void)",
            "{",
            "\treturn ALIGN(sizeof(struct page) * PAGES_PER_SECTION, PMD_SIZE);",
            "}",
            "static unsigned long __init section_map_size(void)",
            "{",
            "\treturn PAGE_ALIGN(sizeof(struct page) * PAGES_PER_SECTION);",
            "}",
            "static inline void __meminit sparse_buffer_free(unsigned long size)",
            "{",
            "\tWARN_ON(!sparsemap_buf || size == 0);",
            "\tmemblock_free(sparsemap_buf, size);",
            "}",
            "static void __init sparse_buffer_init(unsigned long size, int nid)",
            "{",
            "\tphys_addr_t addr = __pa(MAX_DMA_ADDRESS);",
            "\tWARN_ON(sparsemap_buf);\t/* forgot to call sparse_buffer_fini()? */",
            "\t/*",
            "\t * Pre-allocated buffer is mainly used by __populate_section_memmap",
            "\t * and we want it to be properly aligned to the section size - this is",
            "\t * especially the case for VMEMMAP which maps memmap to PMDs",
            "\t */",
            "\tsparsemap_buf = memmap_alloc(size, section_map_size(), addr, nid, true);",
            "\tsparsemap_buf_end = sparsemap_buf + size;",
            "}",
            "static void __init sparse_buffer_fini(void)",
            "{",
            "\tunsigned long size = sparsemap_buf_end - sparsemap_buf;",
            "",
            "\tif (sparsemap_buf && size > 0)",
            "\t\tsparse_buffer_free(size);",
            "\tsparsemap_buf = NULL;",
            "}",
            "void __weak __meminit vmemmap_populate_print_last(void)",
            "{",
            "}",
            "static void __init sparse_init_nid(int nid, unsigned long pnum_begin,",
            "\t\t\t\t   unsigned long pnum_end,",
            "\t\t\t\t   unsigned long map_count)",
            "{",
            "\tstruct mem_section_usage *usage;",
            "\tunsigned long pnum;",
            "\tstruct page *map;",
            "",
            "\tusage = sparse_early_usemaps_alloc_pgdat_section(NODE_DATA(nid),",
            "\t\t\tmem_section_usage_size() * map_count);",
            "\tif (!usage) {",
            "\t\tpr_err(\"%s: node[%d] usemap allocation failed\", __func__, nid);",
            "\t\tgoto failed;",
            "\t}",
            "\tsparse_buffer_init(map_count * section_map_size(), nid);",
            "\tfor_each_present_section_nr(pnum_begin, pnum) {",
            "\t\tunsigned long pfn = section_nr_to_pfn(pnum);",
            "",
            "\t\tif (pnum >= pnum_end)",
            "\t\t\tbreak;",
            "",
            "\t\tmap = __populate_section_memmap(pfn, PAGES_PER_SECTION,",
            "\t\t\t\tnid, NULL, NULL);",
            "\t\tif (!map) {",
            "\t\t\tpr_err(\"%s: node[%d] memory map backing failed. Some memory will not be available.\",",
            "\t\t\t       __func__, nid);",
            "\t\t\tpnum_begin = pnum;",
            "\t\t\tsparse_buffer_fini();",
            "\t\t\tgoto failed;",
            "\t\t}",
            "\t\tcheck_usemap_section_nr(nid, usage);",
            "\t\tsparse_init_one_section(__nr_to_section(pnum), pnum, map, usage,",
            "\t\t\t\tSECTION_IS_EARLY);",
            "\t\tusage = (void *) usage + mem_section_usage_size();",
            "\t}",
            "\tsparse_buffer_fini();",
            "\treturn;",
            "failed:",
            "\t/* We failed to allocate, mark all the following pnums as not present */",
            "\tfor_each_present_section_nr(pnum_begin, pnum) {",
            "\t\tstruct mem_section *ms;",
            "",
            "\t\tif (pnum >= pnum_end)",
            "\t\t\tbreak;",
            "\t\tms = __nr_to_section(pnum);",
            "\t\tms->section_mem_map = 0;",
            "\t}",
            "}",
            "void __init sparse_init(void)",
            "{",
            "\tunsigned long pnum_end, pnum_begin, map_count = 1;",
            "\tint nid_begin;",
            "",
            "\tmemblocks_present();",
            "",
            "\tpnum_begin = first_present_section_nr();",
            "\tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));",
            "",
            "\t/* Setup pageblock_order for HUGETLB_PAGE_SIZE_VARIABLE */",
            "\tset_pageblock_order();",
            "",
            "\tfor_each_present_section_nr(pnum_begin + 1, pnum_end) {",
            "\t\tint nid = sparse_early_nid(__nr_to_section(pnum_end));",
            "",
            "\t\tif (nid == nid_begin) {",
            "\t\t\tmap_count++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\t/* Init node with sections in range [pnum_begin, pnum_end) */",
            "\t\tsparse_init_nid(nid_begin, pnum_begin, pnum_end, map_count);",
            "\t\tnid_begin = nid;",
            "\t\tpnum_begin = pnum_end;",
            "\t\tmap_count = 1;",
            "\t}",
            "\t/* cover the last node */",
            "\tsparse_init_nid(nid_begin, pnum_begin, pnum_end, map_count);",
            "\tvmemmap_populate_print_last();",
            "}"
          ],
          "function_name": "check_usemap_section_nr, section_map_size, section_map_size, sparse_buffer_free, sparse_buffer_init, sparse_buffer_fini, vmemmap_populate_print_last, sparse_init_nid, sparse_init",
          "description": "定义section_map_size计算区段映射大小，管理sparse_buffer缓冲区的分配释放。实现sparse_init_nid初始化节点区段，通过__populate_section_memmap填充内存图。包含错误处理逻辑，失败时清除已分配区段标识。",
          "similarity": 0.5207201838493347
        },
        {
          "chunk_id": 5,
          "file_path": "mm/sparse.c",
          "start_line": 717,
          "end_line": 839,
          "content": [
            "static void free_map_bootmem(struct page *memmap)",
            "{",
            "\tunsigned long maps_section_nr, removing_section_nr, i;",
            "\tunsigned long magic, nr_pages;",
            "\tstruct page *page = virt_to_page(memmap);",
            "",
            "\tnr_pages = PAGE_ALIGN(PAGES_PER_SECTION * sizeof(struct page))",
            "\t\t>> PAGE_SHIFT;",
            "",
            "\tfor (i = 0; i < nr_pages; i++, page++) {",
            "\t\tmagic = page->index;",
            "",
            "\t\tBUG_ON(magic == NODE_INFO);",
            "",
            "\t\tmaps_section_nr = pfn_to_section_nr(page_to_pfn(page));",
            "\t\tremoving_section_nr = page_private(page);",
            "",
            "\t\t/*",
            "\t\t * When this function is called, the removing section is",
            "\t\t * logical offlined state. This means all pages are isolated",
            "\t\t * from page allocator. If removing section's memmap is placed",
            "\t\t * on the same section, it must not be freed.",
            "\t\t * If it is freed, page allocator may allocate it which will",
            "\t\t * be removed physically soon.",
            "\t\t */",
            "\t\tif (maps_section_nr != removing_section_nr)",
            "\t\t\tput_page_bootmem(page);",
            "\t}",
            "}",
            "static int clear_subsection_map(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\treturn 0;",
            "}",
            "static bool is_subsection_map_empty(struct mem_section *ms)",
            "{",
            "\treturn true;",
            "}",
            "static int fill_subsection_map(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\treturn 0;",
            "}",
            "static void section_deactivate(unsigned long pfn, unsigned long nr_pages,",
            "\t\tstruct vmem_altmap *altmap)",
            "{",
            "\tstruct mem_section *ms = __pfn_to_section(pfn);",
            "\tbool section_is_early = early_section(ms);",
            "\tstruct page *memmap = NULL;",
            "\tbool empty;",
            "",
            "\tif (clear_subsection_map(pfn, nr_pages))",
            "\t\treturn;",
            "",
            "\tempty = is_subsection_map_empty(ms);",
            "\tif (empty) {",
            "\t\tunsigned long section_nr = pfn_to_section_nr(pfn);",
            "",
            "\t\t/*",
            "\t\t * Mark the section invalid so that valid_section()",
            "\t\t * return false. This prevents code from dereferencing",
            "\t\t * ms->usage array.",
            "\t\t */",
            "\t\tms->section_mem_map &= ~SECTION_HAS_MEM_MAP;",
            "",
            "\t\t/*",
            "\t\t * When removing an early section, the usage map is kept (as the",
            "\t\t * usage maps of other sections fall into the same page). It",
            "\t\t * will be re-used when re-adding the section - which is then no",
            "\t\t * longer an early section. If the usage map is PageReserved, it",
            "\t\t * was allocated during boot.",
            "\t\t */",
            "\t\tif (!PageReserved(virt_to_page(ms->usage))) {",
            "\t\t\tkfree_rcu(ms->usage, rcu);",
            "\t\t\tWRITE_ONCE(ms->usage, NULL);",
            "\t\t}",
            "\t\tmemmap = sparse_decode_mem_map(ms->section_mem_map, section_nr);",
            "\t}",
            "",
            "\t/*",
            "\t * The memmap of early sections is always fully populated. See",
            "\t * section_activate() and pfn_valid() .",
            "\t */",
            "\tif (!section_is_early)",
            "\t\tdepopulate_section_memmap(pfn, nr_pages, altmap);",
            "\telse if (memmap)",
            "\t\tfree_map_bootmem(memmap);",
            "",
            "\tif (empty)",
            "\t\tms->section_mem_map = (unsigned long)NULL;",
            "}",
            "int __meminit sparse_add_section(int nid, unsigned long start_pfn,",
            "\t\tunsigned long nr_pages, struct vmem_altmap *altmap,",
            "\t\tstruct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long section_nr = pfn_to_section_nr(start_pfn);",
            "\tstruct mem_section *ms;",
            "\tstruct page *memmap;",
            "\tint ret;",
            "",
            "\tret = sparse_index_init(section_nr, nid);",
            "\tif (ret < 0)",
            "\t\treturn ret;",
            "",
            "\tmemmap = section_activate(nid, start_pfn, nr_pages, altmap, pgmap);",
            "\tif (IS_ERR(memmap))",
            "\t\treturn PTR_ERR(memmap);",
            "",
            "\t/*",
            "\t * Poison uninitialized struct pages in order to catch invalid flags",
            "\t * combinations.",
            "\t */",
            "\tpage_init_poison(memmap, sizeof(struct page) * nr_pages);",
            "",
            "\tms = __nr_to_section(section_nr);",
            "\tset_section_nid(section_nr, nid);",
            "\t__section_mark_present(ms, section_nr);",
            "",
            "\t/* Align memmap to section boundary in the subsection case */",
            "\tif (section_nr_to_pfn(section_nr) != start_pfn)",
            "\t\tmemmap = pfn_to_page(section_nr_to_pfn(section_nr));",
            "\tsparse_init_one_section(ms, section_nr, memmap, ms->usage, 0);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "free_map_bootmem, clear_subsection_map, is_subsection_map_empty, fill_subsection_map, section_deactivate, sparse_add_section",
          "description": "free_map_bootmem 函数遍历指定内存映射区域的 page 结构，跳过与当前移除节（section）相同的物理节号的页面，其余页面通过 put_page_bootmem 释放。clear_subsection_map、is_subsection_map_empty、fill_subsection_map 均为空实现。section_deactivate 处理内存节停用流程，清除子节映射标志，释放早期节的 usage 数据并调用 depopulate_section_memmap 或 free_map_bootmem。sparse_add_section 初始化新内存节，设置节点 ID，标记节为有效，并调整 memmap 地址对齐。",
          "similarity": 0.4988657236099243
        }
      ]
    }
  ]
}