{
  "query": "slab allocator",
  "timestamp": "2025-12-25 23:22:36",
  "retrieved_files": [
    {
      "source_file": "mm/slab.h",
      "md_summary": "> 自动生成时间: 2025-12-07 17:22:03\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `slab.h`\n\n---\n\n# `slab.h` 技术文档\n\n## 1. 文件概述\n\n`slab.h` 是 Linux 内核内存管理子系统中 SLAB/SLUB 分配器的核心内部头文件，定义了 slab 分配器所使用的底层数据结构（如 `struct slab` 和 `struct kmem_cache`）、关键宏和辅助函数。该文件主要用于在页（`struct page`）与 slab 表示之间进行安全转换，并提供对 slab 元数据的原子访问机制，以支持高性能、可扩展的对象缓存分配。\n\n此头文件专供内核内存管理内部使用，不对外暴露给模块开发者，是实现 SLUB（默认）或 SLAB 分配器的关键基础设施。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`freelist_aba_t`**  \n  联合体，将空闲对象指针（`freelist`）与计数器（`counter`）打包为一个原子单元，用于避免 ABA 问题（Compare-and-Swap 中因值重复导致的逻辑错误）。\n\n- **`struct slab`**  \n  slab 的内部表示，复用 `struct page` 的内存布局。包含：\n  - 所属的 `kmem_cache`\n  - 空闲对象链表（`freelist`）\n  - 对象使用计数（`inuse`）、总对象数（`objects`）\n  - 冻结状态（`frozen`，用于调试）\n  - RCU 回收头（`rcu_head`）\n  - 引用计数（`__page_refcount`）\n  - 可选的 per-object 扩展数据（`obj_exts`）\n\n- **`struct kmem_cache_order_objects`**  \n  封装 slab 阶数（order）与对象数量的复合值，支持原子读写。\n\n- **`struct kmem_cache`**  \n  slab 缓存描述符，包含：\n  - 每 CPU 缓存（`cpu_slab`）\n  - 对象大小（`size`, `object_size`）\n  - 构造函数（`ctor`）\n  - 对齐要求（`align`）\n  - 分配标志（`allocflags`）\n  - NUMA 相关参数（如 `remote_node_defrag_ratio`）\n  - 安全特性（如 `random` 用于 freelist 加固）\n\n### 主要宏与辅助函数\n\n- **类型安全转换宏**：\n  - `folio_slab()` / `slab_folio()`：在 `folio` 与 `slab` 之间安全转换\n  - `page_slab()` / `slab_page()`：兼容旧代码，在 `page` 与 `slab` 之间转换\n\n- **slab 属性访问函数**：\n  - `slab_address()`：获取 slab 起始虚拟地址\n  - `slab_nid()` / `slab_pgdat()`：获取所属 NUMA 节点和内存域\n  - `slab_order()` / `slab_size()`：获取分配阶数和总字节数\n\n- **pfmemalloc 标志操作**：\n  - `slab_test_pfmemalloc()` / `slab_set_pfmemalloc()` 等：标记 slab 是否来自紧急内存预留区（用于网络交换等场景）\n\n- **每 CPU partial slab 支持（`CONFIG_SLUB_CPU_PARTIAL`）**：\n  - `slub_percpu_partial()` 等宏：管理每 CPU 的 partial slab 链表\n\n## 3. 关键实现\n\n### 内存布局复用与静态断言\n\n- `struct slab` 并非独立分配，而是直接复用 `struct page` 的内存空间。通过 `static_assert` 确保关键字段偏移一致（如 `flags` ↔ `__page_flags`），保证类型转换安全。\n- 整个 `struct slab` 大小不超过 `struct page`，确保无越界访问。\n\n### ABA 问题防护\n\n- 在支持 `cmpxchg128`（64 位）或 `cmpxchg64`（32 位）的架构上，启用 `freelist_aba_t` 结构，将 `freelist` 指针与递增计数器打包为单个原子单元。\n- 使用 `try_cmpxchg_freelist` 进行原子更新，防止因指针值循环重用导致的 ABA 错误。\n- 若系统不支持对齐的 `struct page`（`!CONFIG_HAVE_ALIGNED_STRUCT_PAGE`），则禁用此优化。\n\n### 类型安全转换\n\n- 使用 C11 `_Generic` 实现类型安全的 `folio`/`slab`/`page` 转换，避免强制类型转换带来的风险，并为未来重构（如完全迁移到 folio）预留接口。\n\n### pfmemalloc 标志复用\n\n- 利用 `folio` 的 `PG_active` 位存储 `pfmemalloc` 标志，指示该 slab 是否从紧急内存池分配，用于网络子系统在内存压力下仍能分配 skb 等关键结构。\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/page.h>` / `<linux/folio.h>`：通过 `folio_*` 系列函数操作底层内存\n  - `<linux/reciprocal_div.h>`：用于快速除法（计算对象索引）\n  - `<linux/rcupdate.h>`：通过 `rcu_head` 支持 RCU 安全的 slab 回收\n\n- **可选依赖（由 Kconfig 控制）**：\n  - `CONFIG_SLUB_CPU_PARTIAL`：每 CPU partial slab 优化\n  - `CONFIG_SLAB_OBJ_EXT`：per-object 扩展元数据\n  - `CONFIG_SLAB_FREELIST_HARDENED`：freelist 指针随机化加固\n  - `CONFIG_NUMA`：NUMA 感知分配与碎片整理\n  - `CONFIG_KASAN` / `CONFIG_KFENCE`：内存错误检测集成\n\n- **与内存控制器集成**：\n  - 通过 `memcg_data` 字段（复用 `obj_exts`）支持 memcg 内存统计\n\n## 5. 使用场景\n\n- **SLUB 分配器内部**：作为 `slub.c` 的核心数据结构定义，用于管理 slab 生命周期、对象分配/释放。\n- **内存回收路径**：在 direct reclaim 或 kswapd 中，通过 `slab_folio` 获取 folio 信息以决策回收策略。\n- **调试与监控**：sysfs (`kobj`)、KASAN/KFENCE 集成依赖此结构获取 slab 元数据。\n- **网络子系统**：通过 `pfmemalloc` 标志识别紧急内存分配，确保高优先级数据包处理不被阻塞。\n- **NUMA 优化**：在远程节点分配时使用 `remote_node_defrag_ratio` 参数控制跨节点分配行为。\n- **安全加固**：`SLAB_FREELIST_HARDENED` 利用 `random` 字段混淆 freelist 指针，防止堆利用攻击。",
      "similarity": 0.48680710792541504,
      "chunks": []
    },
    {
      "source_file": "mm/slub.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:23:28\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `slub.c`\n\n---\n\n# slub.c 技术文档\n\n## 1. 文件概述\n\n`slub.c` 是 Linux 内核中 SLUB（Simple Low-overhead Unqueued Allocator）内存分配器的核心实现文件。SLUB 是一种高效的 slab 分配器，旨在减少缓存行使用并避免在每个 CPU 和节点上维护复杂的对象队列。它通过 per-slab 锁或原子操作进行同步，仅在管理部分填充的 slab 池时使用集中式锁。该分配器优化了常见路径的性能，同时支持调试、内存检测和热插拔等高级功能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `kmem_cache`: slab 缓存描述符，包含对象大小、对齐方式、构造函数等元数据\n- `kmem_cache_cpu`: 每 CPU 的 slab 管理结构，包含当前 CPU 的活跃 slab 和局部 freelist\n- `slab`: slab 描述符（通常嵌入在 page 结构中），包含 freelist、inuse 计数、objects 总数和 frozen 状态\n\n### 关键机制\n- **CPU slab**: 每个 CPU 分配专用的 slab 进行快速分配\n- **Partial slab 列表**: 节点级别的部分填充 slab 列表\n- **CPU partial slab**: CPU 本地的部分填充 slab 缓存，用于加速释放操作\n- **Frozen slab**: 冻结状态的 slab，免于全局列表管理\n\n### 锁机制层次\n1. `slab_mutex` - 全局互斥锁，保护所有 slab 列表和元数据变更\n2. `node->list_lock` - 自旋锁，保护节点的 partial/full 列表\n3. `kmem_cache->cpu_slab->lock` - 本地锁，保护慢路径的 per-CPU 字段\n4. `slab_lock(slab)` - slab 锁（仅在不支持 `cmpxchg_double` 的架构上使用）\n5. `object_map_lock` - 调试用途的对象映射锁\n\n## 3. 关键实现\n\n### 锁无关快速路径\n- 分配 (`slab_alloc_node()`) 和释放 (`do_slab_free()`) 操作在满足条件时完全无锁\n- 使用事务 ID (tid) 字段检测抢占或 CPU 迁移\n- 在支持 `cmpxchg_double` 的架构上避免使用 slab_lock\n\n### Slab 状态管理\n- **Node partial slab**: `PG_Workingset && !frozen`\n- **CPU partial slab**: `!PG_Workingset && !frozen`\n- **CPU slab**: `!PG_Workingset && frozen`\n- **Full slab**: `!PG_Workingset && !frozen`\n\n### PREEMPT_RT 支持\n- 在 RT 内核中禁用锁无关快速路径\n- 使用 `migrate_disable()/enable()` 替代 `preempt_disable()/enable()`\n- 本地锁始终被获取以确保 RT 安全性\n\n### 内存管理优化\n- 最小化 slab 设置/拆卸开销，依赖页分配器的 per-CPU 缓存\n- 空 slab 直接释放回页分配器\n- CPU partial slab 机制加速批量释放操作\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **内存管理**: `<linux/mm.h>`, `<linux/swap.h>`, `<linux/memory.h>`\n- **同步原语**: `<linux/bit_spinlock.h>`, `<linux/interrupt.h>`\n- **调试支持**: `<linux/kasan.h>`, `<linux/kmsan.h>`, `<linux/kfence.h>`, `<linux/debugobjects.h>`\n- **系统设施**: `<linux/module.h>`, `<linux/proc_fs.h>`, `<linux/debugfs.h>`\n- **内存控制**: `<linux/memcontrol.h>`, `<linux/cpuset.h>`, `<linux/mempolicy.h>`\n- **测试框架**: `<kunit/test.h>`\n\n### 内部依赖\n- `\"slab.h\"` - slab 分配器通用接口\n- `\"internal.h\"` - 内存管理内部实现\n\n### 子系统交互\n- **页分配器**: 作为底层内存来源\n- **内存热插拔**: 通过 `slab_mutex` 同步回调\n- **内存控制器**: 集成 memcg 功能\n- **跟踪系统**: 通过 `trace/events/kmem.h` 提供分配事件跟踪\n\n## 5. 使用场景\n\n### 内核内存分配\n- 为内核对象（如 task_struct、inode、dentry 等）提供高效的小内存分配\n- 作为 `kmalloc()` 系列函数的底层实现\n- 支持不同大小类别的内存请求（8 字节到几 KB）\n\n### 性能关键路径\n- 中断上下文中的内存分配（通过适当的锁机制保证安全）\n- 高频分配/释放场景（利用 per-CPU slab 和 lockless 快速路径）\n- 批量分配操作（通过 CPU partial slab 优化）\n\n### 调试和监控\n- 内存错误检测（KASAN、KMSAN、KFENCE 集成）\n- 内存泄漏检测（kmemleak 集成）\n- 性能分析（通过 `/proc/slabinfo` 和 debugfs 接口）\n- 故障注入测试（fault-inject 支持）\n\n### 特殊环境支持\n- 实时系统（PREEMPT_RT 配置）\n- 内存受限系统（CONFIG_SLUB_TINY 优化）\n- NUMA 系统（节点感知分配）\n- 内存热插拔环境",
      "similarity": 0.4763641357421875,
      "chunks": [
        {
          "chunk_id": 20,
          "file_path": "mm/slub.c",
          "start_line": 4277,
          "end_line": 4387,
          "content": [
            "static void __slab_free(struct kmem_cache *s, struct slab *slab,",
            "\t\t\tvoid *head, void *tail, int cnt,",
            "\t\t\tunsigned long addr)",
            "",
            "{",
            "\tvoid *prior;",
            "\tint was_frozen;",
            "\tstruct slab new;",
            "\tunsigned long counters;",
            "\tstruct kmem_cache_node *n = NULL;",
            "\tunsigned long flags;",
            "\tbool on_node_partial;",
            "",
            "\tstat(s, FREE_SLOWPATH);",
            "",
            "\tif (IS_ENABLED(CONFIG_SLUB_TINY) || kmem_cache_debug(s)) {",
            "\t\tfree_to_partial_list(s, slab, head, tail, cnt, addr);",
            "\t\treturn;",
            "\t}",
            "",
            "\tdo {",
            "\t\tif (unlikely(n)) {",
            "\t\t\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "\t\t\tn = NULL;",
            "\t\t}",
            "\t\tprior = slab->freelist;",
            "\t\tcounters = slab->counters;",
            "\t\tset_freepointer(s, tail, prior);",
            "\t\tnew.counters = counters;",
            "\t\twas_frozen = new.frozen;",
            "\t\tnew.inuse -= cnt;",
            "\t\tif ((!new.inuse || !prior) && !was_frozen) {",
            "\t\t\t/* Needs to be taken off a list */",
            "\t\t\tif (!kmem_cache_has_cpu_partial(s) || prior) {",
            "",
            "\t\t\t\tn = get_node(s, slab_nid(slab));",
            "\t\t\t\t/*",
            "\t\t\t\t * Speculatively acquire the list_lock.",
            "\t\t\t\t * If the cmpxchg does not succeed then we may",
            "\t\t\t\t * drop the list_lock without any processing.",
            "\t\t\t\t *",
            "\t\t\t\t * Otherwise the list_lock will synchronize with",
            "\t\t\t\t * other processors updating the list of slabs.",
            "\t\t\t\t */",
            "\t\t\t\tspin_lock_irqsave(&n->list_lock, flags);",
            "",
            "\t\t\t\ton_node_partial = slab_test_node_partial(slab);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t} while (!slab_update_freelist(s, slab,",
            "\t\tprior, counters,",
            "\t\thead, new.counters,",
            "\t\t\"__slab_free\"));",
            "",
            "\tif (likely(!n)) {",
            "",
            "\t\tif (likely(was_frozen)) {",
            "\t\t\t/*",
            "\t\t\t * The list lock was not taken therefore no list",
            "\t\t\t * activity can be necessary.",
            "\t\t\t */",
            "\t\t\tstat(s, FREE_FROZEN);",
            "\t\t} else if (kmem_cache_has_cpu_partial(s) && !prior) {",
            "\t\t\t/*",
            "\t\t\t * If we started with a full slab then put it onto the",
            "\t\t\t * per cpu partial list.",
            "\t\t\t */",
            "\t\t\tput_cpu_partial(s, slab, 1);",
            "\t\t\tstat(s, CPU_PARTIAL_FREE);",
            "\t\t}",
            "",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * This slab was partially empty but not on the per-node partial list,",
            "\t * in which case we shouldn't manipulate its list, just return.",
            "\t */",
            "\tif (prior && !on_node_partial) {",
            "\t\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (unlikely(!new.inuse && n->nr_partial >= s->min_partial))",
            "\t\tgoto slab_empty;",
            "",
            "\t/*",
            "\t * Objects left in the slab. If it was not on the partial list before",
            "\t * then add it.",
            "\t */",
            "\tif (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {",
            "\t\tadd_partial(n, slab, DEACTIVATE_TO_TAIL);",
            "\t\tstat(s, FREE_ADD_PARTIAL);",
            "\t}",
            "\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "\treturn;",
            "",
            "slab_empty:",
            "\tif (prior) {",
            "\t\t/*",
            "\t\t * Slab on the partial list.",
            "\t\t */",
            "\t\tremove_partial(n, slab);",
            "\t\tstat(s, FREE_REMOVE_PARTIAL);",
            "\t}",
            "",
            "\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "\tstat(s, FREE_SLAB);",
            "\tdiscard_slab(s, slab);",
            "}"
          ],
          "function_name": "__slab_free",
          "description": "实现SLUB分配器的慢速路径释放逻辑，负责更新slab的空闲指针、计数器及管理部分空闲slab的节点挂载，处理并发场景下的slab状态变更和列表迁移。",
          "similarity": 0.5078893899917603
        },
        {
          "chunk_id": 15,
          "file_path": "mm/slub.c",
          "start_line": 2916,
          "end_line": 3045,
          "content": [
            "static void init_kmem_cache_cpus(struct kmem_cache *s)",
            "{",
            "\tint cpu;",
            "\tstruct kmem_cache_cpu *c;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tc = per_cpu_ptr(s->cpu_slab, cpu);",
            "\t\tlocal_lock_init(&c->lock);",
            "\t\tc->tid = init_tid(cpu);",
            "\t}",
            "}",
            "static void deactivate_slab(struct kmem_cache *s, struct slab *slab,",
            "\t\t\t    void *freelist)",
            "{",
            "\tstruct kmem_cache_node *n = get_node(s, slab_nid(slab));",
            "\tint free_delta = 0;",
            "\tvoid *nextfree, *freelist_iter, *freelist_tail;",
            "\tint tail = DEACTIVATE_TO_HEAD;",
            "\tunsigned long flags = 0;",
            "\tstruct slab new;",
            "\tstruct slab old;",
            "",
            "\tif (READ_ONCE(slab->freelist)) {",
            "\t\tstat(s, DEACTIVATE_REMOTE_FREES);",
            "\t\ttail = DEACTIVATE_TO_TAIL;",
            "\t}",
            "",
            "\t/*",
            "\t * Stage one: Count the objects on cpu's freelist as free_delta and",
            "\t * remember the last object in freelist_tail for later splicing.",
            "\t */",
            "\tfreelist_tail = NULL;",
            "\tfreelist_iter = freelist;",
            "\twhile (freelist_iter) {",
            "\t\tnextfree = get_freepointer(s, freelist_iter);",
            "",
            "\t\t/*",
            "\t\t * If 'nextfree' is invalid, it is possible that the object at",
            "\t\t * 'freelist_iter' is already corrupted.  So isolate all objects",
            "\t\t * starting at 'freelist_iter' by skipping them.",
            "\t\t */",
            "\t\tif (freelist_corrupted(s, slab, &freelist_iter, nextfree))",
            "\t\t\tbreak;",
            "",
            "\t\tfreelist_tail = freelist_iter;",
            "\t\tfree_delta++;",
            "",
            "\t\tfreelist_iter = nextfree;",
            "\t}",
            "",
            "\t/*",
            "\t * Stage two: Unfreeze the slab while splicing the per-cpu",
            "\t * freelist to the head of slab's freelist.",
            "\t */",
            "\tdo {",
            "\t\told.freelist = READ_ONCE(slab->freelist);",
            "\t\told.counters = READ_ONCE(slab->counters);",
            "\t\tVM_BUG_ON(!old.frozen);",
            "",
            "\t\t/* Determine target state of the slab */",
            "\t\tnew.counters = old.counters;",
            "\t\tnew.frozen = 0;",
            "\t\tif (freelist_tail) {",
            "\t\t\tnew.inuse -= free_delta;",
            "\t\t\tset_freepointer(s, freelist_tail, old.freelist);",
            "\t\t\tnew.freelist = freelist;",
            "\t\t} else {",
            "\t\t\tnew.freelist = old.freelist;",
            "\t\t}",
            "\t} while (!slab_update_freelist(s, slab,",
            "\t\told.freelist, old.counters,",
            "\t\tnew.freelist, new.counters,",
            "\t\t\"unfreezing slab\"));",
            "",
            "\t/*",
            "\t * Stage three: Manipulate the slab list based on the updated state.",
            "\t */",
            "\tif (!new.inuse && n->nr_partial >= s->min_partial) {",
            "\t\tstat(s, DEACTIVATE_EMPTY);",
            "\t\tdiscard_slab(s, slab);",
            "\t\tstat(s, FREE_SLAB);",
            "\t} else if (new.freelist) {",
            "\t\tspin_lock_irqsave(&n->list_lock, flags);",
            "\t\tadd_partial(n, slab, tail);",
            "\t\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "\t\tstat(s, tail);",
            "\t} else {",
            "\t\tstat(s, DEACTIVATE_FULL);",
            "\t}",
            "}",
            "static void __put_partials(struct kmem_cache *s, struct slab *partial_slab)",
            "{",
            "\tstruct kmem_cache_node *n = NULL, *n2 = NULL;",
            "\tstruct slab *slab, *slab_to_discard = NULL;",
            "\tunsigned long flags = 0;",
            "",
            "\twhile (partial_slab) {",
            "\t\tslab = partial_slab;",
            "\t\tpartial_slab = slab->next;",
            "",
            "\t\tn2 = get_node(s, slab_nid(slab));",
            "\t\tif (n != n2) {",
            "\t\t\tif (n)",
            "\t\t\t\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "",
            "\t\t\tn = n2;",
            "\t\t\tspin_lock_irqsave(&n->list_lock, flags);",
            "\t\t}",
            "",
            "\t\tif (unlikely(!slab->inuse && n->nr_partial >= s->min_partial)) {",
            "\t\t\tslab->next = slab_to_discard;",
            "\t\t\tslab_to_discard = slab;",
            "\t\t} else {",
            "\t\t\tadd_partial(n, slab, DEACTIVATE_TO_TAIL);",
            "\t\t\tstat(s, FREE_ADD_PARTIAL);",
            "\t\t}",
            "\t}",
            "",
            "\tif (n)",
            "\t\tspin_unlock_irqrestore(&n->list_lock, flags);",
            "",
            "\twhile (slab_to_discard) {",
            "\t\tslab = slab_to_discard;",
            "\t\tslab_to_discard = slab_to_discard->next;",
            "",
            "\t\tstat(s, DEACTIVATE_EMPTY);",
            "\t\tdiscard_slab(s, slab);",
            "\t\tstat(s, FREE_SLAB);",
            "\t}",
            "}"
          ],
          "function_name": "init_kmem_cache_cpus, deactivate_slab, __put_partials",
          "description": "该代码段实现SLUB内存分配器中多CPU协作与内存回收机制。  \n`init_kmem_cache_cpus` 初始化每个CPU的本地slab结构并设置tid；`deactivate_slab` 解冻slab并调整其freelist与计数器，决定是否丢弃或迁移至部分链表；`__put_partials` 管理部分填充slab的回收，判断是否满足空闲条件后进行丢弃。  \n上下文完整，涵盖SLUB分配器中CPU本地缓存同步、slab状态转换及部分对象回收逻辑。",
          "similarity": 0.48020273447036743
        },
        {
          "chunk_id": 32,
          "file_path": "mm/slub.c",
          "start_line": 6074,
          "end_line": 6180,
          "content": [
            "static int add_location(struct loc_track *t, struct kmem_cache *s,",
            "\t\t\t\tconst struct track *track,",
            "\t\t\t\tunsigned int orig_size)",
            "{",
            "\tlong start, end, pos;",
            "\tstruct location *l;",
            "\tunsigned long caddr, chandle, cwaste;",
            "\tunsigned long age = jiffies - track->when;",
            "\tdepot_stack_handle_t handle = 0;",
            "\tunsigned int waste = s->object_size - orig_size;",
            "",
            "#ifdef CONFIG_STACKDEPOT",
            "\thandle = READ_ONCE(track->handle);",
            "#endif",
            "\tstart = -1;",
            "\tend = t->count;",
            "",
            "\tfor ( ; ; ) {",
            "\t\tpos = start + (end - start + 1) / 2;",
            "",
            "\t\t/*",
            "\t\t * There is nothing at \"end\". If we end up there",
            "\t\t * we need to add something to before end.",
            "\t\t */",
            "\t\tif (pos == end)",
            "\t\t\tbreak;",
            "",
            "\t\tl = &t->loc[pos];",
            "\t\tcaddr = l->addr;",
            "\t\tchandle = l->handle;",
            "\t\tcwaste = l->waste;",
            "\t\tif ((track->addr == caddr) && (handle == chandle) &&",
            "\t\t\t(waste == cwaste)) {",
            "",
            "\t\t\tl->count++;",
            "\t\t\tif (track->when) {",
            "\t\t\t\tl->sum_time += age;",
            "\t\t\t\tif (age < l->min_time)",
            "\t\t\t\t\tl->min_time = age;",
            "\t\t\t\tif (age > l->max_time)",
            "\t\t\t\t\tl->max_time = age;",
            "",
            "\t\t\t\tif (track->pid < l->min_pid)",
            "\t\t\t\t\tl->min_pid = track->pid;",
            "\t\t\t\tif (track->pid > l->max_pid)",
            "\t\t\t\t\tl->max_pid = track->pid;",
            "",
            "\t\t\t\tcpumask_set_cpu(track->cpu,",
            "\t\t\t\t\t\tto_cpumask(l->cpus));",
            "\t\t\t}",
            "\t\t\tnode_set(page_to_nid(virt_to_page(track)), l->nodes);",
            "\t\t\treturn 1;",
            "\t\t}",
            "",
            "\t\tif (track->addr < caddr)",
            "\t\t\tend = pos;",
            "\t\telse if (track->addr == caddr && handle < chandle)",
            "\t\t\tend = pos;",
            "\t\telse if (track->addr == caddr && handle == chandle &&",
            "\t\t\t\twaste < cwaste)",
            "\t\t\tend = pos;",
            "\t\telse",
            "\t\t\tstart = pos;",
            "\t}",
            "",
            "\t/*",
            "\t * Not found. Insert new tracking element.",
            "\t */",
            "\tif (t->count >= t->max && !alloc_loc_track(t, 2 * t->max, GFP_ATOMIC))",
            "\t\treturn 0;",
            "",
            "\tl = t->loc + pos;",
            "\tif (pos < t->count)",
            "\t\tmemmove(l + 1, l,",
            "\t\t\t(t->count - pos) * sizeof(struct location));",
            "\tt->count++;",
            "\tl->count = 1;",
            "\tl->addr = track->addr;",
            "\tl->sum_time = age;",
            "\tl->min_time = age;",
            "\tl->max_time = age;",
            "\tl->min_pid = track->pid;",
            "\tl->max_pid = track->pid;",
            "\tl->handle = handle;",
            "\tl->waste = waste;",
            "\tcpumask_clear(to_cpumask(l->cpus));",
            "\tcpumask_set_cpu(track->cpu, to_cpumask(l->cpus));",
            "\tnodes_clear(l->nodes);",
            "\tnode_set(page_to_nid(virt_to_page(track)), l->nodes);",
            "\treturn 1;",
            "}",
            "static void process_slab(struct loc_track *t, struct kmem_cache *s,",
            "\t\tstruct slab *slab, enum track_item alloc,",
            "\t\tunsigned long *obj_map)",
            "{",
            "\tvoid *addr = slab_address(slab);",
            "\tbool is_alloc = (alloc == TRACK_ALLOC);",
            "\tvoid *p;",
            "",
            "\t__fill_map(obj_map, s, slab);",
            "",
            "\tfor_each_object(p, s, addr, slab->objects)",
            "\t\tif (!test_bit(__obj_to_index(s, addr, p), obj_map))",
            "\t\t\tadd_location(t, s, get_track(s, p, alloc),",
            "\t\t\t\t     is_alloc ? get_orig_size(s, p) :",
            "\t\t\t\t\t\ts->object_size);",
            "}"
          ],
          "function_name": "add_location, process_slab",
          "description": "记录对象分配位置信息，遍历slab对象并更新追踪数据，支持调试分析内存分配路径和来源。",
          "similarity": 0.47719210386276245
        },
        {
          "chunk_id": 33,
          "file_path": "mm/slub.c",
          "start_line": 6200,
          "end_line": 6310,
          "content": [
            "static ssize_t show_slab_objects(struct kmem_cache *s,",
            "\t\t\t\t char *buf, unsigned long flags)",
            "{",
            "\tunsigned long total = 0;",
            "\tint node;",
            "\tint x;",
            "\tunsigned long *nodes;",
            "\tint len = 0;",
            "",
            "\tnodes = kcalloc(nr_node_ids, sizeof(unsigned long), GFP_KERNEL);",
            "\tif (!nodes)",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (flags & SO_CPU) {",
            "\t\tint cpu;",
            "",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tstruct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab,",
            "\t\t\t\t\t\t\t       cpu);",
            "\t\t\tint node;",
            "\t\t\tstruct slab *slab;",
            "",
            "\t\t\tslab = READ_ONCE(c->slab);",
            "\t\t\tif (!slab)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tnode = slab_nid(slab);",
            "\t\t\tif (flags & SO_TOTAL)",
            "\t\t\t\tx = slab->objects;",
            "\t\t\telse if (flags & SO_OBJECTS)",
            "\t\t\t\tx = slab->inuse;",
            "\t\t\telse",
            "\t\t\t\tx = 1;",
            "",
            "\t\t\ttotal += x;",
            "\t\t\tnodes[node] += x;",
            "",
            "#ifdef CONFIG_SLUB_CPU_PARTIAL",
            "\t\t\tslab = slub_percpu_partial_read_once(c);",
            "\t\t\tif (slab) {",
            "\t\t\t\tnode = slab_nid(slab);",
            "\t\t\t\tif (flags & SO_TOTAL)",
            "\t\t\t\t\tWARN_ON_ONCE(1);",
            "\t\t\t\telse if (flags & SO_OBJECTS)",
            "\t\t\t\t\tWARN_ON_ONCE(1);",
            "\t\t\t\telse",
            "\t\t\t\t\tx = data_race(slab->slabs);",
            "\t\t\t\ttotal += x;",
            "\t\t\t\tnodes[node] += x;",
            "\t\t\t}",
            "#endif",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * It is impossible to take \"mem_hotplug_lock\" here with \"kernfs_mutex\"",
            "\t * already held which will conflict with an existing lock order:",
            "\t *",
            "\t * mem_hotplug_lock->slab_mutex->kernfs_mutex",
            "\t *",
            "\t * We don't really need mem_hotplug_lock (to hold off",
            "\t * slab_mem_going_offline_callback) here because slab's memory hot",
            "\t * unplug code doesn't destroy the kmem_cache->node[] data.",
            "\t */",
            "",
            "#ifdef CONFIG_SLUB_DEBUG",
            "\tif (flags & SO_ALL) {",
            "\t\tstruct kmem_cache_node *n;",
            "",
            "\t\tfor_each_kmem_cache_node(s, node, n) {",
            "",
            "\t\t\tif (flags & SO_TOTAL)",
            "\t\t\t\tx = node_nr_objs(n);",
            "\t\t\telse if (flags & SO_OBJECTS)",
            "\t\t\t\tx = node_nr_objs(n) - count_partial(n, count_free);",
            "\t\t\telse",
            "\t\t\t\tx = node_nr_slabs(n);",
            "\t\t\ttotal += x;",
            "\t\t\tnodes[node] += x;",
            "\t\t}",
            "",
            "\t} else",
            "#endif",
            "\tif (flags & SO_PARTIAL) {",
            "\t\tstruct kmem_cache_node *n;",
            "",
            "\t\tfor_each_kmem_cache_node(s, node, n) {",
            "\t\t\tif (flags & SO_TOTAL)",
            "\t\t\t\tx = count_partial(n, count_total);",
            "\t\t\telse if (flags & SO_OBJECTS)",
            "\t\t\t\tx = count_partial(n, count_inuse);",
            "\t\t\telse",
            "\t\t\t\tx = n->nr_partial;",
            "\t\t\ttotal += x;",
            "\t\t\tnodes[node] += x;",
            "\t\t}",
            "\t}",
            "",
            "\tlen += sysfs_emit_at(buf, len, \"%lu\", total);",
            "#ifdef CONFIG_NUMA",
            "\tfor (node = 0; node < nr_node_ids; node++) {",
            "\t\tif (nodes[node])",
            "\t\t\tlen += sysfs_emit_at(buf, len, \" N%d=%lu\",",
            "\t\t\t\t\t     node, nodes[node]);",
            "\t}",
            "#endif",
            "\tlen += sysfs_emit_at(buf, len, \"\\n\");",
            "\tkfree(nodes);",
            "",
            "\treturn len;",
            "}"
          ],
          "function_name": "show_slab_objects",
          "description": "统计并展示各节点上slab对象数量，根据标志位区分总对象数、部分列表对象数及CPU局部缓存对象分布。",
          "similarity": 0.47547733783721924
        },
        {
          "chunk_id": 24,
          "file_path": "mm/slub.c",
          "start_line": 4824,
          "end_line": 4929,
          "content": [
            "int kmem_cache_alloc_bulk_noprof(struct kmem_cache *s, gfp_t flags, size_t size,",
            "\t\t\t\t void **p)",
            "{",
            "\tint i;",
            "",
            "\tif (!size)",
            "\t\treturn 0;",
            "",
            "\ts = slab_pre_alloc_hook(s, flags);",
            "\tif (unlikely(!s))",
            "\t\treturn 0;",
            "",
            "\ti = __kmem_cache_alloc_bulk(s, flags, size, p);",
            "\tif (unlikely(i == 0))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * memcg and kmem_cache debug support and memory initialization.",
            "\t * Done outside of the IRQ disabled fastpath loop.",
            "\t */",
            "\tif (unlikely(!slab_post_alloc_hook(s, NULL, flags, size, p,",
            "\t\t    slab_want_init_on_alloc(flags, s), s->object_size))) {",
            "\t\treturn 0;",
            "\t}",
            "\treturn i;",
            "}",
            "static inline unsigned int calc_slab_order(unsigned int size,",
            "\t\tunsigned int min_objects, unsigned int max_order,",
            "\t\tunsigned int fract_leftover)",
            "{",
            "\tunsigned int min_order = slub_min_order;",
            "\tunsigned int order;",
            "",
            "\tif (order_objects(min_order, size) > MAX_OBJS_PER_PAGE)",
            "\t\treturn get_order(size * MAX_OBJS_PER_PAGE) - 1;",
            "",
            "\tfor (order = max(min_order, (unsigned int)get_order(min_objects * size));",
            "\t\t\torder <= max_order; order++) {",
            "",
            "\t\tunsigned int slab_size = (unsigned int)PAGE_SIZE << order;",
            "\t\tunsigned int rem;",
            "",
            "\t\trem = slab_size % size;",
            "",
            "\t\tif (rem <= slab_size / fract_leftover)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn order;",
            "}",
            "static inline int calculate_order(unsigned int size)",
            "{",
            "\tunsigned int order;",
            "\tunsigned int min_objects;",
            "\tunsigned int max_objects;",
            "\tunsigned int nr_cpus;",
            "",
            "\tmin_objects = slub_min_objects;",
            "\tif (!min_objects) {",
            "\t\t/*",
            "\t\t * Some architectures will only update present cpus when",
            "\t\t * onlining them, so don't trust the number if it's just 1. But",
            "\t\t * we also don't want to use nr_cpu_ids always, as on some other",
            "\t\t * architectures, there can be many possible cpus, but never",
            "\t\t * onlined. Here we compromise between trying to avoid too high",
            "\t\t * order on systems that appear larger than they are, and too",
            "\t\t * low order on systems that appear smaller than they are.",
            "\t\t */",
            "\t\tnr_cpus = num_present_cpus();",
            "\t\tif (nr_cpus <= 1)",
            "\t\t\tnr_cpus = nr_cpu_ids;",
            "\t\tmin_objects = 4 * (fls(nr_cpus) + 1);",
            "\t}",
            "\tmax_objects = order_objects(slub_max_order, size);",
            "\tmin_objects = min(min_objects, max_objects);",
            "",
            "\t/*",
            "\t * Attempt to find best configuration for a slab. This works by first",
            "\t * attempting to generate a layout with the best possible configuration",
            "\t * and backing off gradually.",
            "\t *",
            "\t * We start with accepting at most 1/16 waste and try to find the",
            "\t * smallest order from min_objects-derived/slab_min_order up to",
            "\t * slab_max_order that will satisfy the constraint. Note that increasing",
            "\t * the order can only result in same or less fractional waste, not more.",
            "\t *",
            "\t * If that fails, we increase the acceptable fraction of waste and try",
            "\t * again. The last iteration with fraction of 1/2 would effectively",
            "\t * accept any waste and give us the order determined by min_objects, as",
            "\t * long as at least single object fits within slab_max_order.",
            "\t */",
            "\tfor (unsigned int fraction = 16; fraction > 1; fraction /= 2) {",
            "\t\torder = calc_slab_order(size, min_objects, slub_max_order,",
            "\t\t\t\t\tfraction);",
            "\t\tif (order <= slub_max_order)",
            "\t\t\treturn order;",
            "\t}",
            "",
            "\t/*",
            "\t * Doh this slab cannot be placed using slab_max_order.",
            "\t */",
            "\torder = get_order(size);",
            "\tif (order <= MAX_PAGE_ORDER)",
            "\t\treturn order;",
            "\treturn -ENOSYS;",
            "}"
          ],
          "function_name": "kmem_cache_alloc_bulk_noprof, calc_slab_order, calculate_order",
          "description": "计算slab页面顺序的辅助函数，基于系统配置动态调整slab大小，通过分层遍历策略寻找最优页面顺序以最小化内存浪费并满足对象分配需求。",
          "similarity": 0.4710855484008789
        }
      ]
    },
    {
      "source_file": "mm/slab_common.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:22:45\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `slab_common.c`\n\n---\n\n# slab_common.c 技术文档\n\n## 1. 文件概述\n\n`slab_common.c` 是 Linux 内核中 Slab 分配器的通用实现文件，包含与具体分配策略（如 SLAB、SLUB、SLOB）无关的公共函数和基础设施。该文件负责管理 Slab 缓存的创建、合并、销毁等核心逻辑，并提供统一的接口供上层使用。它实现了缓存注册、命名、对齐计算、调试支持以及与内存子系统交互的基础功能。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `slab_state`：表示 Slab 子系统的初始化状态（如 DOWN、PARTIAL、UP 等）\n- `slab_caches`：全局链表，维护所有已注册的 `kmem_cache` 实例\n- `slab_mutex`：保护 `slab_caches` 链表的互斥锁\n- `kmem_cache`：指向用于分配 `kmem_cache` 结构本身的缓存对象\n- `slab_caches_to_rcu_destroy`：待通过 RCU 安全方式销毁的缓存链表\n- `slab_nomerge`：控制是否禁止 Slab 缓存合并的布尔标志\n\n### 主要函数\n- `kmem_cache_size()`：返回指定缓存中对象的实际大小\n- `calculate_alignment()`：根据标志、用户对齐要求和对象大小计算最终对齐值\n- `slab_unmergeable()`：判断给定缓存是否可被合并\n- `find_mergeable()`：在现有缓存中查找可合并的目标缓存\n- `create_cache()`：创建新的 `kmem_cache` 实例\n- `__kmem_cache_create_args()`：带参数的缓存创建主入口函数\n\n### 关键宏定义\n- `SLAB_NEVER_MERGE`：包含禁止缓存合并的标志集合（如 RED_ZONE、POISON 等）\n- `SLAB_MERGE_SAME`：合并时必须相同的标志集合（如 DMA 相关、ACCOUNT 等）\n\n## 3. 关键实现\n\n### 缓存合并机制\n文件实现了智能的缓存合并策略：\n1. 通过 `slab_nomerge` 全局开关或内核启动参数（`slab_nomerge`/`slab_merge`）控制是否启用合并\n2. 使用 `SLAB_NEVER_MERGE` 排除带有调试或特殊语义标志的缓存\n3. 在 `find_mergeable()` 中遍历 `slab_caches` 链表，检查：\n   - 对象大小兼容性（新缓存 ≤ 现有缓存）\n   - 必须相同的标志位一致\n   - 对齐兼容性（现有缓存大小是新对齐的整数倍）\n   - 内存浪费不超过一个指针大小\n\n### 对齐计算\n`calculate_alignment()` 实现了分层对齐策略：\n- 若设置 `SLAB_HWCACHE_ALIGN`，则基于缓存行大小动态调整（对象越小，对齐粒度越细）\n- 始终满足架构最小对齐要求（`arch_slab_minalign()`）\n- 最终对齐值向上对齐到指针大小的倍数\n\n### 安全与调试支持\n- **完整性检查**：`kmem_cache_sanity_check()` 在 `CONFIG_DEBUG_VM` 下验证缓存名和大小合法性\n- **用户复制硬化**：集成 `CONFIG_HARDENED_USERCOPY` 检查用户区域偏移/大小有效性\n- **调试标志处理**：自动启用 `slub_debug_enabled` 静态分支和 `stack_depot` 初始化\n- **KFENCE/KASAN 集成**：通过头文件包含支持内存错误检测框架\n\n### RCU 安全销毁\n通过工作队列 `slab_caches_to_rcu_destroy_work` 延迟销毁缓存，确保在 RCU 宽限期结束后释放内存，避免并发访问已释放结构。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心子系统**：`<linux/slab.h>`, `<linux/mm.h>`, `<linux/memory.h>`\n- **调试设施**：`<linux/kasan.h>`, `<linux/kfence.h>`, `<linux/kmemleak.h>`\n- **架构相关**：`<asm/cacheflush.h>`, `<asm/page.h>`\n- **内部实现**：`\"internal.h\"`, `\"slab.h\"`（包含分配器特定接口）\n\n### 功能依赖\n- **内存管理**：依赖页分配器（`alloc_pages`）和内存控制组（`memcontrol`）\n- **同步机制**：使用 mutex、RCU 和 workqueue 实现并发控制\n- **调试框架**：与 KASAN、KFENCE、SLUB_DEBUG 等调试子系统深度集成\n- **DMA 支持**：通过 `SLAB_CACHE_DMA`/`SLAB_CACHE_DMA32` 标志与 DMA 映射子系统交互\n\n## 5. 使用场景\n\n### 内核初始化\n- 在 `start_kernel()` 早期阶段初始化 `kmem_cache` 自身（bootstrap 过程）\n- 通过 `__setup_param` 处理内核命令行参数（如 `slab_nomerge`）\n\n### 动态缓存创建\n- 当驱动或子系统调用 `kmem_cache_create()` 时，经由此文件的 `__kmem_cache_create_args()` 创建新缓存\n- 自动尝试合并相似缓存以减少内存碎片（除非显式禁用）\n\n### 调试与监控\n- `/proc/slabinfo` 和 debugfs 接口通过此文件获取缓存列表信息\n- 内存错误检测工具（KASAN/KFENCE）利用此文件的钩子注入检测逻辑\n\n### 特殊内存分配\n- 支持 DMA 缓存（`SLAB_CACHE_DMA`）、RCU 安全释放（`SLAB_TYPESAFE_BY_RCU`）等特殊场景\n- 为 hardened usercopy 提供对象边界验证所需元数据（`useroffset`/`usersize`）",
      "similarity": 0.43294602632522583,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/slab_common.c",
          "start_line": 1,
          "end_line": 62,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Slab allocator functions that are independent of the allocator strategy",
            " *",
            " * (C) 2012 Christoph Lameter <cl@linux.com>",
            " */",
            "#include <linux/slab.h>",
            "",
            "#include <linux/mm.h>",
            "#include <linux/poison.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/memory.h>",
            "#include <linux/cache.h>",
            "#include <linux/compiler.h>",
            "#include <linux/kfence.h>",
            "#include <linux/module.h>",
            "#include <linux/cpu.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/dma-mapping.h>",
            "#include <linux/swiotlb.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/kasan.h>",
            "#include <asm/cacheflush.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/page.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/stackdepot.h>",
            "",
            "#include \"internal.h\"",
            "#include \"slab.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/kmem.h>",
            "",
            "enum slab_state slab_state;",
            "LIST_HEAD(slab_caches);",
            "DEFINE_MUTEX(slab_mutex);",
            "struct kmem_cache *kmem_cache;",
            "",
            "static LIST_HEAD(slab_caches_to_rcu_destroy);",
            "static void slab_caches_to_rcu_destroy_workfn(struct work_struct *work);",
            "static DECLARE_WORK(slab_caches_to_rcu_destroy_work,",
            "\t\t    slab_caches_to_rcu_destroy_workfn);",
            "",
            "/*",
            " * Set of flags that will prevent slab merging",
            " */",
            "#define SLAB_NEVER_MERGE (SLAB_RED_ZONE | SLAB_POISON | SLAB_STORE_USER | \\",
            "\t\tSLAB_TRACE | SLAB_TYPESAFE_BY_RCU | SLAB_NOLEAKTRACE | \\",
            "\t\tSLAB_FAILSLAB | SLAB_NO_MERGE)",
            "",
            "#define SLAB_MERGE_SAME (SLAB_RECLAIM_ACCOUNT | SLAB_CACHE_DMA | \\",
            "\t\t\t SLAB_CACHE_DMA32 | SLAB_ACCOUNT)",
            "",
            "/*",
            " * Merge control. If this is set then no merging of slab caches will occur.",
            " */",
            "static bool slab_nomerge = !IS_ENABLED(CONFIG_SLAB_MERGE_DEFAULT);",
            ""
          ],
          "function_name": null,
          "description": "定义Slab分配器的基础结构，包括全局变量slab_state、slab_caches链表、互斥锁slab_mutex及kmem_cache指针，声明SLAB_NEVER_MERGE等合并控制相关标志位，用于后续Slab缓存合并策略的配置。",
          "similarity": 0.4896188974380493
        },
        {
          "chunk_id": 2,
          "file_path": "mm/slab_common.c",
          "start_line": 497,
          "end_line": 617,
          "content": [
            "static int shutdown_cache(struct kmem_cache *s)",
            "{",
            "\t/* free asan quarantined objects */",
            "\tkasan_cache_shutdown(s);",
            "",
            "\tif (__kmem_cache_shutdown(s) != 0)",
            "\t\treturn -EBUSY;",
            "",
            "\tlist_del(&s->list);",
            "",
            "\tif (s->flags & SLAB_TYPESAFE_BY_RCU) {",
            "\t\tlist_add_tail(&s->list, &slab_caches_to_rcu_destroy);",
            "\t\tschedule_work(&slab_caches_to_rcu_destroy_work);",
            "\t} else {",
            "\t\tkfence_shutdown_cache(s);",
            "\t\tdebugfs_slab_release(s);",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "void slab_kmem_cache_release(struct kmem_cache *s)",
            "{",
            "\t__kmem_cache_release(s);",
            "\tkfree_const(s->name);",
            "\tkmem_cache_free(kmem_cache, s);",
            "}",
            "void kmem_cache_destroy(struct kmem_cache *s)",
            "{",
            "\tint err = -EBUSY;",
            "\tbool rcu_set;",
            "",
            "\tif (unlikely(!s) || !kasan_check_byte(s))",
            "\t\treturn;",
            "",
            "\tcpus_read_lock();",
            "\tmutex_lock(&slab_mutex);",
            "",
            "\trcu_set = s->flags & SLAB_TYPESAFE_BY_RCU;",
            "",
            "\ts->refcount--;",
            "\tif (s->refcount)",
            "\t\tgoto out_unlock;",
            "",
            "\terr = shutdown_cache(s);",
            "\tWARN(err, \"%s %s: Slab cache still has objects when called from %pS\",",
            "\t     __func__, s->name, (void *)_RET_IP_);",
            "out_unlock:",
            "\tmutex_unlock(&slab_mutex);",
            "\tcpus_read_unlock();",
            "\tif (!err && !rcu_set)",
            "\t\tkmem_cache_release(s);",
            "}",
            "int kmem_cache_shrink(struct kmem_cache *cachep)",
            "{",
            "\tkasan_cache_shrink(cachep);",
            "",
            "\treturn __kmem_cache_shrink(cachep);",
            "}",
            "bool slab_is_available(void)",
            "{",
            "\treturn slab_state >= UP;",
            "}",
            "static void kmem_obj_info(struct kmem_obj_info *kpp, void *object, struct slab *slab)",
            "{",
            "\tif (__kfence_obj_info(kpp, object, slab))",
            "\t\treturn;",
            "\t__kmem_obj_info(kpp, object, slab);",
            "}",
            "bool kmem_dump_obj(void *object)",
            "{",
            "\tchar *cp = IS_ENABLED(CONFIG_MMU) ? \"\" : \"/vmalloc\";",
            "\tint i;",
            "\tstruct slab *slab;",
            "\tunsigned long ptroffset;",
            "\tstruct kmem_obj_info kp = { };",
            "",
            "\t/* Some arches consider ZERO_SIZE_PTR to be a valid address. */",
            "\tif (object < (void *)PAGE_SIZE || !virt_addr_valid(object))",
            "\t\treturn false;",
            "\tslab = virt_to_slab(object);",
            "\tif (!slab)",
            "\t\treturn false;",
            "",
            "\tkmem_obj_info(&kp, object, slab);",
            "\tif (kp.kp_slab_cache)",
            "\t\tpr_cont(\" slab%s %s\", cp, kp.kp_slab_cache->name);",
            "\telse",
            "\t\tpr_cont(\" slab%s\", cp);",
            "\tif (is_kfence_address(object))",
            "\t\tpr_cont(\" (kfence)\");",
            "\tif (kp.kp_objp)",
            "\t\tpr_cont(\" start %px\", kp.kp_objp);",
            "\tif (kp.kp_data_offset)",
            "\t\tpr_cont(\" data offset %lu\", kp.kp_data_offset);",
            "\tif (kp.kp_objp) {",
            "\t\tptroffset = ((char *)object - (char *)kp.kp_objp) - kp.kp_data_offset;",
            "\t\tpr_cont(\" pointer offset %lu\", ptroffset);",
            "\t}",
            "\tif (kp.kp_slab_cache && kp.kp_slab_cache->object_size)",
            "\t\tpr_cont(\" size %u\", kp.kp_slab_cache->object_size);",
            "\tif (kp.kp_ret)",
            "\t\tpr_cont(\" allocated at %pS\\n\", kp.kp_ret);",
            "\telse",
            "\t\tpr_cont(\"\\n\");",
            "\tfor (i = 0; i < ARRAY_SIZE(kp.kp_stack); i++) {",
            "\t\tif (!kp.kp_stack[i])",
            "\t\t\tbreak;",
            "\t\tpr_info(\"    %pS\\n\", kp.kp_stack[i]);",
            "\t}",
            "",
            "\tif (kp.kp_free_stack[0])",
            "\t\tpr_cont(\" Free path:\\n\");",
            "",
            "\tfor (i = 0; i < ARRAY_SIZE(kp.kp_free_stack); i++) {",
            "\t\tif (!kp.kp_free_stack[i])",
            "\t\t\tbreak;",
            "\t\tpr_info(\"    %pS\\n\", kp.kp_free_stack[i]);",
            "\t}",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "shutdown_cache, slab_kmem_cache_release, kmem_cache_destroy, kmem_cache_shrink, slab_is_available, kmem_obj_info, kmem_dump_obj",
          "description": "提供Slab缓存生命周期管理接口，shutdown_cache执行缓存关闭操作，slab_kmem_cache_release释放缓存资源，kmem_cache_destroy实现缓存销毁并校验引用计数，kmem_dump_obj输出对象详细调试信息。",
          "similarity": 0.45778822898864746
        },
        {
          "chunk_id": 1,
          "file_path": "mm/slab_common.c",
          "start_line": 63,
          "end_line": 176,
          "content": [
            "static int __init setup_slab_nomerge(char *str)",
            "{",
            "\tslab_nomerge = true;",
            "\treturn 1;",
            "}",
            "static int __init setup_slab_merge(char *str)",
            "{",
            "\tslab_nomerge = false;",
            "\treturn 1;",
            "}",
            "unsigned int kmem_cache_size(struct kmem_cache *s)",
            "{",
            "\treturn s->object_size;",
            "}",
            "static int kmem_cache_sanity_check(const char *name, unsigned int size)",
            "{",
            "\tif (!name || in_interrupt() || size > KMALLOC_MAX_SIZE) {",
            "\t\tpr_err(\"kmem_cache_create(%s) integrity check failed\\n\", name);",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tWARN_ON(strchr(name, ' '));\t/* It confuses parsers */",
            "\treturn 0;",
            "}",
            "static inline int kmem_cache_sanity_check(const char *name, unsigned int size)",
            "{",
            "\treturn 0;",
            "}",
            "static unsigned int calculate_alignment(slab_flags_t flags,",
            "\t\tunsigned int align, unsigned int size)",
            "{",
            "\t/*",
            "\t * If the user wants hardware cache aligned objects then follow that",
            "\t * suggestion if the object is sufficiently large.",
            "\t *",
            "\t * The hardware cache alignment cannot override the specified",
            "\t * alignment though. If that is greater then use it.",
            "\t */",
            "\tif (flags & SLAB_HWCACHE_ALIGN) {",
            "\t\tunsigned int ralign;",
            "",
            "\t\tralign = cache_line_size();",
            "\t\twhile (size <= ralign / 2)",
            "\t\t\tralign /= 2;",
            "\t\talign = max(align, ralign);",
            "\t}",
            "",
            "\talign = max(align, arch_slab_minalign());",
            "",
            "\treturn ALIGN(align, sizeof(void *));",
            "}",
            "int slab_unmergeable(struct kmem_cache *s)",
            "{",
            "\tif (slab_nomerge || (s->flags & SLAB_NEVER_MERGE))",
            "\t\treturn 1;",
            "",
            "\tif (s->ctor)",
            "\t\treturn 1;",
            "",
            "#ifdef CONFIG_HARDENED_USERCOPY",
            "\tif (s->usersize)",
            "\t\treturn 1;",
            "#endif",
            "",
            "\t/*",
            "\t * We may have set a slab to be unmergeable during bootstrap.",
            "\t */",
            "\tif (s->refcount < 0)",
            "\t\treturn 1;",
            "",
            "\treturn 0;",
            "}",
            "static void kmem_cache_release(struct kmem_cache *s)",
            "{",
            "\tif (slab_state >= FULL) {",
            "\t\tsysfs_slab_unlink(s);",
            "\t\tsysfs_slab_release(s);",
            "\t} else {",
            "\t\tslab_kmem_cache_release(s);",
            "\t}",
            "}",
            "static void kmem_cache_release(struct kmem_cache *s)",
            "{",
            "\tslab_kmem_cache_release(s);",
            "}",
            "static void slab_caches_to_rcu_destroy_workfn(struct work_struct *work)",
            "{",
            "\tLIST_HEAD(to_destroy);",
            "\tstruct kmem_cache *s, *s2;",
            "",
            "\t/*",
            "\t * On destruction, SLAB_TYPESAFE_BY_RCU kmem_caches are put on the",
            "\t * @slab_caches_to_rcu_destroy list.  The slab pages are freed",
            "\t * through RCU and the associated kmem_cache are dereferenced",
            "\t * while freeing the pages, so the kmem_caches should be freed only",
            "\t * after the pending RCU operations are finished.  As rcu_barrier()",
            "\t * is a pretty slow operation, we batch all pending destructions",
            "\t * asynchronously.",
            "\t */",
            "\tmutex_lock(&slab_mutex);",
            "\tlist_splice_init(&slab_caches_to_rcu_destroy, &to_destroy);",
            "\tmutex_unlock(&slab_mutex);",
            "",
            "\tif (list_empty(&to_destroy))",
            "\t\treturn;",
            "",
            "\trcu_barrier();",
            "",
            "\tlist_for_each_entry_safe(s, s2, &to_destroy, list) {",
            "\t\tdebugfs_slab_release(s);",
            "\t\tkfence_shutdown_cache(s);",
            "\t\tkmem_cache_release(s);",
            "\t}",
            "}"
          ],
          "function_name": "setup_slab_nomerge, setup_slab_merge, kmem_cache_size, kmem_cache_sanity_check, kmem_cache_sanity_check, calculate_alignment, slab_unmergeable, kmem_cache_release, kmem_cache_release, slab_caches_to_rcu_destroy_workfn",
          "description": "实现Slab缓存合并控制逻辑，通过setup_slab_nomerge/setup_slab_merge设置合并禁用标志，calculate_alignment计算对齐需求，slab_unmergeable判定缓存是否可合并，kmem_cache_release处理缓存释放并关联RCU安全销毁工作队列。",
          "similarity": 0.45010459423065186
        },
        {
          "chunk_id": 5,
          "file_path": "mm/slab_common.c",
          "start_line": 1080,
          "end_line": 1198,
          "content": [
            "static void print_slabinfo_header(struct seq_file *m)",
            "{",
            "\t/*",
            "\t * Output format version, so at least we can change it",
            "\t * without _too_ many complaints.",
            "\t */",
            "\tseq_puts(m, \"slabinfo - version: 2.1\\n\");",
            "\tseq_puts(m, \"# name            <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab>\");",
            "\tseq_puts(m, \" : tunables <limit> <batchcount> <sharedfactor>\");",
            "\tseq_puts(m, \" : slabdata <active_slabs> <num_slabs> <sharedavail>\");",
            "\tseq_putc(m, '\\n');",
            "}",
            "static void slab_stop(struct seq_file *m, void *p)",
            "{",
            "\tmutex_unlock(&slab_mutex);",
            "}",
            "static void cache_show(struct kmem_cache *s, struct seq_file *m)",
            "{",
            "\tstruct slabinfo sinfo;",
            "",
            "\tmemset(&sinfo, 0, sizeof(sinfo));",
            "\tget_slabinfo(s, &sinfo);",
            "",
            "\tseq_printf(m, \"%-17s %6lu %6lu %6u %4u %4d\",",
            "\t\t   s->name, sinfo.active_objs, sinfo.num_objs, s->size,",
            "\t\t   sinfo.objects_per_slab, (1 << sinfo.cache_order));",
            "",
            "\tseq_printf(m, \" : tunables %4u %4u %4u\",",
            "\t\t   sinfo.limit, sinfo.batchcount, sinfo.shared);",
            "\tseq_printf(m, \" : slabdata %6lu %6lu %6lu\",",
            "\t\t   sinfo.active_slabs, sinfo.num_slabs, sinfo.shared_avail);",
            "\tslabinfo_show_stats(m, s);",
            "\tseq_putc(m, '\\n');",
            "}",
            "static int slab_show(struct seq_file *m, void *p)",
            "{",
            "\tstruct kmem_cache *s = list_entry(p, struct kmem_cache, list);",
            "",
            "\tif (p == slab_caches.next)",
            "\t\tprint_slabinfo_header(m);",
            "\tcache_show(s, m);",
            "\treturn 0;",
            "}",
            "void dump_unreclaimable_slab(void)",
            "{",
            "\tstruct kmem_cache *s;",
            "\tstruct slabinfo sinfo;",
            "",
            "\t/*",
            "\t * Here acquiring slab_mutex is risky since we don't prefer to get",
            "\t * sleep in oom path. But, without mutex hold, it may introduce a",
            "\t * risk of crash.",
            "\t * Use mutex_trylock to protect the list traverse, dump nothing",
            "\t * without acquiring the mutex.",
            "\t */",
            "\tif (!mutex_trylock(&slab_mutex)) {",
            "\t\tpr_warn(\"excessive unreclaimable slab but cannot dump stats\\n\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tpr_info(\"Unreclaimable slab info:\\n\");",
            "\tpr_info(\"Name                      Used          Total\\n\");",
            "",
            "\tlist_for_each_entry(s, &slab_caches, list) {",
            "\t\tif (s->flags & SLAB_RECLAIM_ACCOUNT)",
            "\t\t\tcontinue;",
            "",
            "\t\tget_slabinfo(s, &sinfo);",
            "",
            "\t\tif (sinfo.num_objs > 0)",
            "\t\t\tpr_info(\"%-17s %10luKB %10luKB\\n\", s->name,",
            "\t\t\t\t(sinfo.active_objs * s->size) / 1024,",
            "\t\t\t\t(sinfo.num_objs * s->size) / 1024);",
            "\t}",
            "\tmutex_unlock(&slab_mutex);",
            "}",
            "static int slabinfo_open(struct inode *inode, struct file *file)",
            "{",
            "\treturn seq_open(file, &slabinfo_op);",
            "}",
            "static int __init slab_proc_init(void)",
            "{",
            "\tproc_create(\"slabinfo\", SLABINFO_RIGHTS, NULL, &slabinfo_proc_ops);",
            "\treturn 0;",
            "}",
            "void kfree_sensitive(const void *p)",
            "{",
            "\tsize_t ks;",
            "\tvoid *mem = (void *)p;",
            "",
            "\tks = ksize(mem);",
            "\tif (ks) {",
            "\t\tkasan_unpoison_range(mem, ks);",
            "\t\tmemzero_explicit(mem, ks);",
            "\t}",
            "\tkfree(mem);",
            "}",
            "size_t ksize(const void *objp)",
            "{",
            "\t/*",
            "\t * We need to first check that the pointer to the object is valid.",
            "\t * The KASAN report printed from ksize() is more useful, then when",
            "\t * it's printed later when the behaviour could be undefined due to",
            "\t * a potential use-after-free or double-free.",
            "\t *",
            "\t * We use kasan_check_byte(), which is supported for the hardware",
            "\t * tag-based KASAN mode, unlike kasan_check_read/write().",
            "\t *",
            "\t * If the pointed to memory is invalid, we return 0 to avoid users of",
            "\t * ksize() writing to and potentially corrupting the memory region.",
            "\t *",
            "\t * We want to perform the check before __ksize(), to avoid potentially",
            "\t * crashing in __ksize() due to accessing invalid metadata.",
            "\t */",
            "\tif (unlikely(ZERO_OR_NULL_PTR(objp)) || !kasan_check_byte(objp))",
            "\t\treturn 0;",
            "",
            "\treturn kfence_ksize(objp) ?: __ksize(objp);",
            "}"
          ],
          "function_name": "print_slabinfo_header, slab_stop, cache_show, slab_show, dump_unreclaimable_slab, slabinfo_open, slab_proc_init, kfree_sensitive, ksize",
          "description": "该代码段实现Linux内核slab分配器的信息导出与调试功能，包含slab状态统计、/proc接口注册及内存安全检查模块。其中`print_slabinfo_header`定义输出格式，`cache_show`和`slab_show`负责遍历并打印各slab缓存的详细统计信息，`dump_unreclaimable_slab`用于安全地导出不可回收slab的数据。`kfree_sensitive`和`ksize`协同实现对已释放内存的跟踪与安全清除机制。",
          "similarity": 0.4122334122657776
        },
        {
          "chunk_id": 3,
          "file_path": "mm/slab_common.c",
          "start_line": 655,
          "end_line": 793,
          "content": [
            "void __init create_boot_cache(struct kmem_cache *s, const char *name,",
            "\t\tunsigned int size, slab_flags_t flags,",
            "\t\tunsigned int useroffset, unsigned int usersize)",
            "{",
            "\tint err;",
            "\tunsigned int align = ARCH_KMALLOC_MINALIGN;",
            "\tstruct kmem_cache_args kmem_args = {};",
            "",
            "\t/*",
            "\t * For power of two sizes, guarantee natural alignment for kmalloc",
            "\t * caches, regardless of SL*B debugging options.",
            "\t */",
            "\tif (is_power_of_2(size))",
            "\t\talign = max(align, size);",
            "\tkmem_args.align = calculate_alignment(flags, align, size);",
            "",
            "#ifdef CONFIG_HARDENED_USERCOPY",
            "\tkmem_args.useroffset = useroffset;",
            "\tkmem_args.usersize = usersize;",
            "#endif",
            "",
            "\terr = do_kmem_cache_create(s, name, size, &kmem_args, flags);",
            "",
            "\tif (err)",
            "\t\tpanic(\"Creation of kmalloc slab %s size=%u failed. Reason %d\\n\",",
            "\t\t\t\t\tname, size, err);",
            "",
            "\ts->refcount = -1;\t/* Exempt from merging for now */",
            "}",
            "size_t kmalloc_size_roundup(size_t size)",
            "{",
            "\tif (size && size <= KMALLOC_MAX_CACHE_SIZE) {",
            "\t\t/*",
            "\t\t * The flags don't matter since size_index is common to all.",
            "\t\t * Neither does the caller for just getting ->object_size.",
            "\t\t */",
            "\t\treturn kmalloc_slab(size, NULL, GFP_KERNEL, 0)->object_size;",
            "\t}",
            "",
            "\t/* Above the smaller buckets, size is a multiple of page size. */",
            "\tif (size && size <= KMALLOC_MAX_SIZE)",
            "\t\treturn PAGE_SIZE << get_order(size);",
            "",
            "\t/*",
            "\t * Return 'size' for 0 - kmalloc() returns ZERO_SIZE_PTR",
            "\t * and very large size - kmalloc() may fail.",
            "\t */",
            "\treturn size;",
            "",
            "}",
            "void __init setup_kmalloc_cache_index_table(void)",
            "{",
            "\tunsigned int i;",
            "",
            "\tBUILD_BUG_ON(KMALLOC_MIN_SIZE > 256 ||",
            "\t\t!is_power_of_2(KMALLOC_MIN_SIZE));",
            "",
            "\tfor (i = 8; i < KMALLOC_MIN_SIZE; i += 8) {",
            "\t\tunsigned int elem = size_index_elem(i);",
            "",
            "\t\tif (elem >= ARRAY_SIZE(kmalloc_size_index))",
            "\t\t\tbreak;",
            "\t\tkmalloc_size_index[elem] = KMALLOC_SHIFT_LOW;",
            "\t}",
            "",
            "\tif (KMALLOC_MIN_SIZE >= 64) {",
            "\t\t/*",
            "\t\t * The 96 byte sized cache is not used if the alignment",
            "\t\t * is 64 byte.",
            "\t\t */",
            "\t\tfor (i = 64 + 8; i <= 96; i += 8)",
            "\t\t\tkmalloc_size_index[size_index_elem(i)] = 7;",
            "",
            "\t}",
            "",
            "\tif (KMALLOC_MIN_SIZE >= 128) {",
            "\t\t/*",
            "\t\t * The 192 byte sized cache is not used if the alignment",
            "\t\t * is 128 byte. Redirect kmalloc to use the 256 byte cache",
            "\t\t * instead.",
            "\t\t */",
            "\t\tfor (i = 128 + 8; i <= 192; i += 8)",
            "\t\t\tkmalloc_size_index[size_index_elem(i)] = 8;",
            "\t}",
            "}",
            "static unsigned int __kmalloc_minalign(void)",
            "{",
            "\tunsigned int minalign = dma_get_cache_alignment();",
            "",
            "\tif (IS_ENABLED(CONFIG_DMA_BOUNCE_UNALIGNED_KMALLOC) &&",
            "\t    is_swiotlb_allocated())",
            "\t\tminalign = ARCH_KMALLOC_MINALIGN;",
            "",
            "\treturn max(minalign, arch_slab_minalign());",
            "}",
            "static void __init",
            "new_kmalloc_cache(int idx, enum kmalloc_cache_type type)",
            "{",
            "\tslab_flags_t flags = 0;",
            "\tunsigned int minalign = __kmalloc_minalign();",
            "\tunsigned int aligned_size = kmalloc_info[idx].size;",
            "\tint aligned_idx = idx;",
            "",
            "\tif ((KMALLOC_RECLAIM != KMALLOC_NORMAL) && (type == KMALLOC_RECLAIM)) {",
            "\t\tflags |= SLAB_RECLAIM_ACCOUNT;",
            "\t} else if (IS_ENABLED(CONFIG_MEMCG) && (type == KMALLOC_CGROUP)) {",
            "\t\tif (mem_cgroup_kmem_disabled()) {",
            "\t\t\tkmalloc_caches[type][idx] = kmalloc_caches[KMALLOC_NORMAL][idx];",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tflags |= SLAB_ACCOUNT;",
            "\t} else if (IS_ENABLED(CONFIG_ZONE_DMA) && (type == KMALLOC_DMA)) {",
            "\t\tflags |= SLAB_CACHE_DMA;",
            "\t}",
            "",
            "#ifdef CONFIG_RANDOM_KMALLOC_CACHES",
            "\tif (type >= KMALLOC_RANDOM_START && type <= KMALLOC_RANDOM_END)",
            "\t\tflags |= SLAB_NO_MERGE;",
            "#endif",
            "",
            "\t/*",
            "\t * If CONFIG_MEMCG is enabled, disable cache merging for",
            "\t * KMALLOC_NORMAL caches.",
            "\t */",
            "\tif (IS_ENABLED(CONFIG_MEMCG) && (type == KMALLOC_NORMAL))",
            "\t\tflags |= SLAB_NO_MERGE;",
            "",
            "\tif (minalign > ARCH_KMALLOC_MINALIGN) {",
            "\t\taligned_size = ALIGN(aligned_size, minalign);",
            "\t\taligned_idx = __kmalloc_index(aligned_size, false);",
            "\t}",
            "",
            "\tif (!kmalloc_caches[type][aligned_idx])",
            "\t\tkmalloc_caches[type][aligned_idx] = create_kmalloc_cache(",
            "\t\t\t\t\tkmalloc_info[aligned_idx].name[type],",
            "\t\t\t\t\taligned_size, flags);",
            "\tif (idx != aligned_idx)",
            "\t\tkmalloc_caches[type][idx] = kmalloc_caches[type][aligned_idx];",
            "}"
          ],
          "function_name": "create_boot_cache, kmalloc_size_roundup, setup_kmalloc_cache_index_table, __kmalloc_minalign, new_kmalloc_cache",
          "description": "初始化kmalloc专用Slab缓存，create_boot_cache创建启动时基础缓存，setup_kmalloc_cache_index_table构建大小到缓存索引的映射表，new_kmalloc_cache根据类型创建不同属性的缓存实例。",
          "similarity": 0.3891250491142273
        }
      ]
    }
  ]
}