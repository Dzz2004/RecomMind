{
  "query": "操作系统中线程调度与进程调度的实现原理",
  "timestamp": "2025-12-25 23:30:44",
  "retrieved_files": [
    {
      "source_file": "kernel/workqueue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:53:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workqueue.c`\n\n---\n\n# workqueue.c 技术文档\n\n## 1. 文件概述\n\n`workqueue.c` 是 Linux 内核中实现通用异步执行机制的核心文件，提供基于共享工作线程池（worker pool）的延迟任务调度功能。工作项（work items）在进程上下文中执行，支持 CPU 绑定和非绑定两种模式。每个 CPU 默认拥有两个标准工作池（普通优先级和高优先级），同时支持动态创建非绑定工作池以满足不同工作队列的需求。该机制替代了早期的 taskqueue/keventd 实现，具有更高的可扩展性和资源利用率。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct worker_pool`**  \n  工作线程池结构体，管理一组工作线程（workers），包含：\n  - `lock`：保护池状态的自旋锁\n  - `cpu` / `node`：关联的 CPU 和 NUMA 节点（绑定池）\n  - `worklist`：待处理工作项队列\n  - `idle_list` / `busy_hash`：空闲和忙碌工作线程的管理结构\n  - `nr_workers` / `nr_idle`：工作线程数量统计\n  - `attrs`：工作线程属性（如优先级、CPU 亲和性）\n  - `mayday_timer`：紧急情况下的救援请求定时器\n\n- **`struct pool_workqueue`**  \n  工作队列与工作池之间的关联结构，每个工作队列在每个池中都有一个对应的 `pool_workqueue` 实例，用于：\n  - 管理工作项的入队和执行\n  - 实现 `max_active` 限制（控制并发执行数）\n  - 支持 flush 操作（等待所有工作完成）\n  - 统计性能指标（如启动/完成次数、CPU 时间等）\n\n- **`struct worker`**（定义在 `workqueue_internal.h`）  \n  工作线程的运行时上下文，包含状态标志（如 `WORKER_IDLE`, `WORKER_UNBOUND`）、当前执行的工作项等。\n\n### 关键枚举与常量\n\n- **池/工作线程标志**：\n  - `POOL_DISASSOCIATED`：CPU 离线时池进入非绑定状态\n  - `WORKER_UNBOUND`：工作线程可在任意 CPU 上运行\n  - `WORKER_CPU_INTENSIVE`：标记 CPU 密集型任务，影响并发控制\n\n- **配置参数**：\n  - `NR_STD_WORKER_POOLS = 2`：每 CPU 标准池数量（普通 + 高优先级）\n  - `IDLE_WORKER_TIMEOUT = 300 * HZ`：空闲线程保留时间（5 分钟）\n  - `MAYDAY_INITIAL_TIMEOUT`：工作积压时触发救援的延迟（10ms）\n\n- **统计指标**（`pool_workqueue_stats`）：\n  - `PWQ_STAT_STARTED` / `PWQ_STAT_COMPLETED`：工作项执行统计\n  - `PWQ_STAT_MAYDAY` / `PWQ_STAT_RESCUED`：紧急救援事件计数\n\n## 3. 关键实现\n\n### 工作池管理\n- **绑定池（Bound Pool）**：与特定 CPU 关联，工作线程默认绑定到该 CPU。当 CPU 离线时，池进入 `DISASSOCIATED` 状态，工作线程转为非绑定模式。\n- **非绑定池（Unbound Pool）**：动态创建，通过哈希表（`unbound_pool_hash`）按属性（`workqueue_attrs`）去重，支持跨 CPU 调度。\n- **并发控制**：通过 `nr_running` 计数器和 `max_active` 限制，防止工作项过度并发执行。\n\n### 工作线程生命周期\n- **空闲管理**：空闲线程加入 `idle_list`，超时（`IDLE_WORKER_TIMEOUT`）后被回收。\n- **动态伸缩**：当工作积压时，通过 `mayday_timer` 触发新线程创建；若创建失败，向全局救援线程（rescuer）求助。\n- **状态标志**：使用位标志（如 `WORKER_IDLE`, `WORKER_PREP`）高效管理线程状态，避免锁竞争。\n\n### 内存与同步\n- **RCU 保护**：工作池销毁通过 RCU 延迟释放，确保 `get_work_pool()` 等读取路径无锁安全。\n- **锁分层**：\n  - `pool->lock`（自旋锁）：保护池内部状态\n  - `wq_pool_mutex`：全局池管理互斥锁\n  - `wq_pool_attach_mutex`：防止 CPU 绑定状态变更冲突\n\n### 工作项调度\n- **数据指针复用**：`work_struct->data` 的高有效位存储 `pool_workqueue` 指针，低有效位用于标志位（如 `WORK_STRUCT_INACTIVE`）。\n- **优先级支持**：高优先级工作池使用 `HIGHPRI_NICE_LEVEL = MIN_NICE` 提升调度优先级。\n\n## 4. 依赖关系\n\n- **内核子系统**：\n  - **调度器**（`<linux/sched.h>`）：创建工作线程（kworker），管理 CPU 亲和性\n  - **内存管理**（`<linux/slab.h>`）：分配工作池、工作队列等结构\n  - **CPU 热插拔**（`<linux/cpu.h>`）：处理 CPU 上下线时的池绑定状态切换\n  - **RCU**（`<linux/rculist.h>`）：实现无锁读取路径\n  - **定时器**（`<linux/timer.h>`）：实现空闲超时和救援机制\n\n- **内部依赖**：\n  - `workqueue_internal.h`：定义 `struct worker` 等内部结构\n  - `Documentation/core-api/workqueue.rst`：详细设计文档\n\n## 5. 使用场景\n\n- **驱动程序延迟操作**：硬件中断后调度下半部处理（如网络包处理、磁盘 I/O 完成回调）。\n- **内核子系统异步任务**：文件系统元数据更新、内存回收、电源管理状态切换。\n- **高优先级任务**：使用 `WQ_HIGHPRI` 标志创建工作队列，确保关键任务及时执行（如死锁恢复）。\n- **CPU 密集型任务**：标记 `WQ_CPU_INTENSIVE` 避免占用过多并发槽位，提升系统响应性。\n- **NUMA 感知调度**：非绑定工作队列可指定 NUMA 节点，优化内存访问延迟。",
      "similarity": 0.6421319842338562,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/workqueue.c",
          "start_line": 692,
          "end_line": 797,
          "content": [
            "static void set_work_pool_and_clear_pending(struct work_struct *work,",
            "\t\t\t\t\t    int pool_id)",
            "{",
            "\t/*",
            "\t * The following wmb is paired with the implied mb in",
            "\t * test_and_set_bit(PENDING) and ensures all updates to @work made",
            "\t * here are visible to and precede any updates by the next PENDING",
            "\t * owner.",
            "\t */",
            "\tsmp_wmb();",
            "\tset_work_data(work, (unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT, 0);",
            "\t/*",
            "\t * The following mb guarantees that previous clear of a PENDING bit",
            "\t * will not be reordered with any speculative LOADS or STORES from",
            "\t * work->current_func, which is executed afterwards.  This possible",
            "\t * reordering can lead to a missed execution on attempt to queue",
            "\t * the same @work.  E.g. consider this case:",
            "\t *",
            "\t *   CPU#0                         CPU#1",
            "\t *   ----------------------------  --------------------------------",
            "\t *",
            "\t * 1  STORE event_indicated",
            "\t * 2  queue_work_on() {",
            "\t * 3    test_and_set_bit(PENDING)",
            "\t * 4 }                             set_..._and_clear_pending() {",
            "\t * 5                                 set_work_data() # clear bit",
            "\t * 6                                 smp_mb()",
            "\t * 7                               work->current_func() {",
            "\t * 8\t\t\t\t      LOAD event_indicated",
            "\t *\t\t\t\t   }",
            "\t *",
            "\t * Without an explicit full barrier speculative LOAD on line 8 can",
            "\t * be executed before CPU#0 does STORE on line 1.  If that happens,",
            "\t * CPU#0 observes the PENDING bit is still set and new execution of",
            "\t * a @work is not queued in a hope, that CPU#1 will eventually",
            "\t * finish the queued @work.  Meanwhile CPU#1 does not see",
            "\t * event_indicated is set, because speculative LOAD was executed",
            "\t * before actual STORE.",
            "\t */",
            "\tsmp_mb();",
            "}",
            "static void clear_work_data(struct work_struct *work)",
            "{",
            "\tsmp_wmb();\t/* see set_work_pool_and_clear_pending() */",
            "\tset_work_data(work, WORK_STRUCT_NO_POOL, 0);",
            "}",
            "static int get_work_pool_id(struct work_struct *work)",
            "{",
            "\tunsigned long data = atomic_long_read(&work->data);",
            "",
            "\tif (data & WORK_STRUCT_PWQ)",
            "\t\treturn work_struct_pwq(data)->pool->id;",
            "",
            "\treturn data >> WORK_OFFQ_POOL_SHIFT;",
            "}",
            "static void mark_work_canceling(struct work_struct *work)",
            "{",
            "\tunsigned long pool_id = get_work_pool_id(work);",
            "",
            "\tpool_id <<= WORK_OFFQ_POOL_SHIFT;",
            "\tset_work_data(work, pool_id | WORK_OFFQ_CANCELING, WORK_STRUCT_PENDING);",
            "}",
            "static bool work_is_canceling(struct work_struct *work)",
            "{",
            "\tunsigned long data = atomic_long_read(&work->data);",
            "",
            "\treturn !(data & WORK_STRUCT_PWQ) && (data & WORK_OFFQ_CANCELING);",
            "}",
            "static bool need_more_worker(struct worker_pool *pool)",
            "{",
            "\treturn !list_empty(&pool->worklist) && !pool->nr_running;",
            "}",
            "static bool may_start_working(struct worker_pool *pool)",
            "{",
            "\treturn pool->nr_idle;",
            "}",
            "static bool keep_working(struct worker_pool *pool)",
            "{",
            "\treturn !list_empty(&pool->worklist) && (pool->nr_running <= 1);",
            "}",
            "static bool need_to_create_worker(struct worker_pool *pool)",
            "{",
            "\treturn need_more_worker(pool) && !may_start_working(pool);",
            "}",
            "static bool too_many_workers(struct worker_pool *pool)",
            "{",
            "\tbool managing = pool->flags & POOL_MANAGER_ACTIVE;",
            "\tint nr_idle = pool->nr_idle + managing; /* manager is considered idle */",
            "\tint nr_busy = pool->nr_workers - nr_idle;",
            "",
            "\treturn nr_idle > 2 && (nr_idle - 2) * MAX_IDLE_WORKERS_RATIO >= nr_busy;",
            "}",
            "static inline void worker_set_flags(struct worker *worker, unsigned int flags)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\t/* If transitioning into NOT_RUNNING, adjust nr_running. */",
            "\tif ((flags & WORKER_NOT_RUNNING) &&",
            "\t    !(worker->flags & WORKER_NOT_RUNNING)) {",
            "\t\tpool->nr_running--;",
            "\t}",
            "",
            "\tworker->flags |= flags;",
            "}"
          ],
          "function_name": "set_work_pool_and_clear_pending, clear_work_data, get_work_pool_id, mark_work_canceling, work_is_canceling, need_more_worker, may_start_working, keep_working, need_to_create_worker, too_many_workers, worker_set_flags",
          "description": "实现工作者线程池状态控制逻辑，包含需要创建新工作者的判断条件、工作者空闲状态管理、工作项冲突检测及任务分配函数，通过锁保护保证池状态一致性，维护工作者线程与待处理工作项的匹配关系。",
          "similarity": 0.6269057989120483
        },
        {
          "chunk_id": 27,
          "file_path": "kernel/workqueue.c",
          "start_line": 5563,
          "end_line": 5664,
          "content": [
            "int workqueue_prepare_cpu(unsigned int cpu)",
            "{",
            "\tstruct worker_pool *pool;",
            "",
            "\tfor_each_cpu_worker_pool(pool, cpu) {",
            "\t\tif (pool->nr_workers)",
            "\t\t\tcontinue;",
            "\t\tif (!create_worker(pool))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}",
            "int workqueue_online_cpu(unsigned int cpu)",
            "{",
            "\tstruct worker_pool *pool;",
            "\tstruct workqueue_struct *wq;",
            "\tint pi;",
            "",
            "\tmutex_lock(&wq_pool_mutex);",
            "",
            "\tfor_each_pool(pool, pi) {",
            "\t\tmutex_lock(&wq_pool_attach_mutex);",
            "",
            "\t\tif (pool->cpu == cpu)",
            "\t\t\trebind_workers(pool);",
            "\t\telse if (pool->cpu < 0)",
            "\t\t\trestore_unbound_workers_cpumask(pool, cpu);",
            "",
            "\t\tmutex_unlock(&wq_pool_attach_mutex);",
            "\t}",
            "",
            "\t/* update pod affinity of unbound workqueues */",
            "\tlist_for_each_entry(wq, &workqueues, list) {",
            "\t\tstruct workqueue_attrs *attrs = wq->unbound_attrs;",
            "",
            "\t\tif (attrs) {",
            "\t\t\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);",
            "\t\t\tint tcpu;",
            "",
            "\t\t\tfor_each_cpu(tcpu, pt->pod_cpus[pt->cpu_pod[cpu]])",
            "\t\t\t\twq_update_pod(wq, tcpu, cpu, true);",
            "\t\t}",
            "\t}",
            "",
            "\tmutex_unlock(&wq_pool_mutex);",
            "\treturn 0;",
            "}",
            "int workqueue_offline_cpu(unsigned int cpu)",
            "{",
            "\tstruct workqueue_struct *wq;",
            "",
            "\t/* unbinding per-cpu workers should happen on the local CPU */",
            "\tif (WARN_ON(cpu != smp_processor_id()))",
            "\t\treturn -1;",
            "",
            "\tunbind_workers(cpu);",
            "",
            "\t/* update pod affinity of unbound workqueues */",
            "\tmutex_lock(&wq_pool_mutex);",
            "\tlist_for_each_entry(wq, &workqueues, list) {",
            "\t\tstruct workqueue_attrs *attrs = wq->unbound_attrs;",
            "",
            "\t\tif (attrs) {",
            "\t\t\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);",
            "\t\t\tint tcpu;",
            "",
            "\t\t\tfor_each_cpu(tcpu, pt->pod_cpus[pt->cpu_pod[cpu]])",
            "\t\t\t\twq_update_pod(wq, tcpu, cpu, false);",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&wq_pool_mutex);",
            "",
            "\treturn 0;",
            "}",
            "static void work_for_cpu_fn(struct work_struct *work)",
            "{",
            "\tstruct work_for_cpu *wfc = container_of(work, struct work_for_cpu, work);",
            "",
            "\twfc->ret = wfc->fn(wfc->arg);",
            "}",
            "long work_on_cpu_key(int cpu, long (*fn)(void *),",
            "\t\t     void *arg, struct lock_class_key *key)",
            "{",
            "\tstruct work_for_cpu wfc = { .fn = fn, .arg = arg };",
            "",
            "\tINIT_WORK_ONSTACK_KEY(&wfc.work, work_for_cpu_fn, key);",
            "\tschedule_work_on(cpu, &wfc.work);",
            "\tflush_work(&wfc.work);",
            "\tdestroy_work_on_stack(&wfc.work);",
            "\treturn wfc.ret;",
            "}",
            "long work_on_cpu_safe_key(int cpu, long (*fn)(void *),",
            "\t\t\t  void *arg, struct lock_class_key *key)",
            "{",
            "\tlong ret = -ENODEV;",
            "",
            "\tcpus_read_lock();",
            "\tif (cpu_online(cpu))",
            "\t\tret = work_on_cpu_key(cpu, fn, arg, key);",
            "\tcpus_read_unlock();",
            "\treturn ret;",
            "}"
          ],
          "function_name": "workqueue_prepare_cpu, workqueue_online_cpu, workqueue_offline_cpu, work_for_cpu_fn, work_on_cpu_key, work_on_cpu_safe_key",
          "description": "处理CPU事件生命周期，准备工作者、更新工作者池状态，并提供跨CPU任务执行接口以保证调度一致性",
          "similarity": 0.6097320318222046
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/workqueue.c",
          "start_line": 895,
          "end_line": 1037,
          "content": [
            "static inline void worker_clr_flags(struct worker *worker, unsigned int flags)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "\tunsigned int oflags = worker->flags;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\tworker->flags &= ~flags;",
            "",
            "\t/*",
            "\t * If transitioning out of NOT_RUNNING, increment nr_running.  Note",
            "\t * that the nested NOT_RUNNING is not a noop.  NOT_RUNNING is mask",
            "\t * of multiple flags, not a single flag.",
            "\t */",
            "\tif ((flags & WORKER_NOT_RUNNING) && (oflags & WORKER_NOT_RUNNING))",
            "\t\tif (!(worker->flags & WORKER_NOT_RUNNING))",
            "\t\t\tpool->nr_running++;",
            "}",
            "static void worker_enter_idle(struct worker *worker)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tif (WARN_ON_ONCE(worker->flags & WORKER_IDLE) ||",
            "\t    WARN_ON_ONCE(!list_empty(&worker->entry) &&",
            "\t\t\t (worker->hentry.next || worker->hentry.pprev)))",
            "\t\treturn;",
            "",
            "\t/* can't use worker_set_flags(), also called from create_worker() */",
            "\tworker->flags |= WORKER_IDLE;",
            "\tpool->nr_idle++;",
            "\tworker->last_active = jiffies;",
            "",
            "\t/* idle_list is LIFO */",
            "\tlist_add(&worker->entry, &pool->idle_list);",
            "",
            "\tif (too_many_workers(pool) && !timer_pending(&pool->idle_timer))",
            "\t\tmod_timer(&pool->idle_timer, jiffies + IDLE_WORKER_TIMEOUT);",
            "",
            "\t/* Sanity check nr_running. */",
            "\tWARN_ON_ONCE(pool->nr_workers == pool->nr_idle && pool->nr_running);",
            "}",
            "static void worker_leave_idle(struct worker *worker)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tif (WARN_ON_ONCE(!(worker->flags & WORKER_IDLE)))",
            "\t\treturn;",
            "\tworker_clr_flags(worker, WORKER_IDLE);",
            "\tpool->nr_idle--;",
            "\tlist_del_init(&worker->entry);",
            "}",
            "static void move_linked_works(struct work_struct *work, struct list_head *head,",
            "\t\t\t      struct work_struct **nextp)",
            "{",
            "\tstruct work_struct *n;",
            "",
            "\t/*",
            "\t * Linked worklist will always end before the end of the list,",
            "\t * use NULL for list head.",
            "\t */",
            "\tlist_for_each_entry_safe_from(work, n, NULL, entry) {",
            "\t\tlist_move_tail(&work->entry, head);",
            "\t\tif (!(*work_data_bits(work) & WORK_STRUCT_LINKED))",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\t/*",
            "\t * If we're already inside safe list traversal and have moved",
            "\t * multiple works to the scheduled queue, the next position",
            "\t * needs to be updated.",
            "\t */",
            "\tif (nextp)",
            "\t\t*nextp = n;",
            "}",
            "static bool assign_work(struct work_struct *work, struct worker *worker,",
            "\t\t\tstruct work_struct **nextp)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "\tstruct worker *collision;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\t/*",
            "\t * A single work shouldn't be executed concurrently by multiple workers.",
            "\t * __queue_work() ensures that @work doesn't jump to a different pool",
            "\t * while still running in the previous pool. Here, we should ensure that",
            "\t * @work is not executed concurrently by multiple workers from the same",
            "\t * pool. Check whether anyone is already processing the work. If so,",
            "\t * defer the work to the currently executing one.",
            "\t */",
            "\tcollision = find_worker_executing_work(pool, work);",
            "\tif (unlikely(collision)) {",
            "\t\tmove_linked_works(work, &collision->scheduled, nextp);",
            "\t\treturn false;",
            "\t}",
            "",
            "\tmove_linked_works(work, &worker->scheduled, nextp);",
            "\treturn true;",
            "}",
            "static bool kick_pool(struct worker_pool *pool)",
            "{",
            "\tstruct worker *worker = first_idle_worker(pool);",
            "\tstruct task_struct *p;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\tif (!need_more_worker(pool) || !worker)",
            "\t\treturn false;",
            "",
            "\tp = worker->task;",
            "",
            "#ifdef CONFIG_SMP",
            "\t/*",
            "\t * Idle @worker is about to execute @work and waking up provides an",
            "\t * opportunity to migrate @worker at a lower cost by setting the task's",
            "\t * wake_cpu field. Let's see if we want to move @worker to improve",
            "\t * execution locality.",
            "\t *",
            "\t * We're waking the worker that went idle the latest and there's some",
            "\t * chance that @worker is marked idle but hasn't gone off CPU yet. If",
            "\t * so, setting the wake_cpu won't do anything. As this is a best-effort",
            "\t * optimization and the race window is narrow, let's leave as-is for",
            "\t * now. If this becomes pronounced, we can skip over workers which are",
            "\t * still on cpu when picking an idle worker.",
            "\t *",
            "\t * If @pool has non-strict affinity, @worker might have ended up outside",
            "\t * its affinity scope. Repatriate.",
            "\t */",
            "\tif (!pool->attrs->affn_strict &&",
            "\t    !cpumask_test_cpu(p->wake_cpu, pool->attrs->__pod_cpumask)) {",
            "\t\tstruct work_struct *work = list_first_entry(&pool->worklist,",
            "\t\t\t\t\t\tstruct work_struct, entry);",
            "\t\tint wake_cpu = cpumask_any_and_distribute(pool->attrs->__pod_cpumask,",
            "\t\t\t\t\t\t\t  cpu_online_mask);",
            "\t\tif (wake_cpu < nr_cpu_ids) {",
            "\t\t\tp->wake_cpu = wake_cpu;",
            "\t\t\tget_work_pwq(work)->stats[PWQ_STAT_REPATRIATED]++;",
            "\t\t}",
            "\t}",
            "#endif",
            "\twake_up_process(p);",
            "\treturn true;",
            "}"
          ],
          "function_name": "worker_clr_flags, worker_enter_idle, worker_leave_idle, move_linked_works, assign_work, kick_pool",
          "description": "实现工作者线程空闲状态切换和工作项迁移机制，包含空闲工作者列表管理、链接工作项批量转移功能，以及唤醒工作者线程的调度逻辑，支持跨CPU亲和性迁移优化，确保工作项正确分发到可用工作者线程。",
          "similarity": 0.5974740982055664
        },
        {
          "chunk_id": 26,
          "file_path": "kernel/workqueue.c",
          "start_line": 5431,
          "end_line": 5543,
          "content": [
            "static void unbind_workers(int cpu)",
            "{",
            "\tstruct worker_pool *pool;",
            "\tstruct worker *worker;",
            "",
            "\tfor_each_cpu_worker_pool(pool, cpu) {",
            "\t\tmutex_lock(&wq_pool_attach_mutex);",
            "\t\traw_spin_lock_irq(&pool->lock);",
            "",
            "\t\t/*",
            "\t\t * We've blocked all attach/detach operations. Make all workers",
            "\t\t * unbound and set DISASSOCIATED.  Before this, all workers",
            "\t\t * must be on the cpu.  After this, they may become diasporas.",
            "\t\t * And the preemption disabled section in their sched callbacks",
            "\t\t * are guaranteed to see WORKER_UNBOUND since the code here",
            "\t\t * is on the same cpu.",
            "\t\t */",
            "\t\tfor_each_pool_worker(worker, pool)",
            "\t\t\tworker->flags |= WORKER_UNBOUND;",
            "",
            "\t\tpool->flags |= POOL_DISASSOCIATED;",
            "",
            "\t\t/*",
            "\t\t * The handling of nr_running in sched callbacks are disabled",
            "\t\t * now.  Zap nr_running.  After this, nr_running stays zero and",
            "\t\t * need_more_worker() and keep_working() are always true as",
            "\t\t * long as the worklist is not empty.  This pool now behaves as",
            "\t\t * an unbound (in terms of concurrency management) pool which",
            "\t\t * are served by workers tied to the pool.",
            "\t\t */",
            "\t\tpool->nr_running = 0;",
            "",
            "\t\t/*",
            "\t\t * With concurrency management just turned off, a busy",
            "\t\t * worker blocking could lead to lengthy stalls.  Kick off",
            "\t\t * unbound chain execution of currently pending work items.",
            "\t\t */",
            "\t\tkick_pool(pool);",
            "",
            "\t\traw_spin_unlock_irq(&pool->lock);",
            "",
            "\t\tfor_each_pool_worker(worker, pool)",
            "\t\t\tunbind_worker(worker);",
            "",
            "\t\tmutex_unlock(&wq_pool_attach_mutex);",
            "\t}",
            "}",
            "static void rebind_workers(struct worker_pool *pool)",
            "{",
            "\tstruct worker *worker;",
            "",
            "\tlockdep_assert_held(&wq_pool_attach_mutex);",
            "",
            "\t/*",
            "\t * Restore CPU affinity of all workers.  As all idle workers should",
            "\t * be on the run-queue of the associated CPU before any local",
            "\t * wake-ups for concurrency management happen, restore CPU affinity",
            "\t * of all workers first and then clear UNBOUND.  As we're called",
            "\t * from CPU_ONLINE, the following shouldn't fail.",
            "\t */",
            "\tfor_each_pool_worker(worker, pool) {",
            "\t\tkthread_set_per_cpu(worker->task, pool->cpu);",
            "\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task,",
            "\t\t\t\t\t\t  pool_allowed_cpus(pool)) < 0);",
            "\t}",
            "",
            "\traw_spin_lock_irq(&pool->lock);",
            "",
            "\tpool->flags &= ~POOL_DISASSOCIATED;",
            "",
            "\tfor_each_pool_worker(worker, pool) {",
            "\t\tunsigned int worker_flags = worker->flags;",
            "",
            "\t\t/*",
            "\t\t * We want to clear UNBOUND but can't directly call",
            "\t\t * worker_clr_flags() or adjust nr_running.  Atomically",
            "\t\t * replace UNBOUND with another NOT_RUNNING flag REBOUND.",
            "\t\t * @worker will clear REBOUND using worker_clr_flags() when",
            "\t\t * it initiates the next execution cycle thus restoring",
            "\t\t * concurrency management.  Note that when or whether",
            "\t\t * @worker clears REBOUND doesn't affect correctness.",
            "\t\t *",
            "\t\t * WRITE_ONCE() is necessary because @worker->flags may be",
            "\t\t * tested without holding any lock in",
            "\t\t * wq_worker_running().  Without it, NOT_RUNNING test may",
            "\t\t * fail incorrectly leading to premature concurrency",
            "\t\t * management operations.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(!(worker_flags & WORKER_UNBOUND));",
            "\t\tworker_flags |= WORKER_REBOUND;",
            "\t\tworker_flags &= ~WORKER_UNBOUND;",
            "\t\tWRITE_ONCE(worker->flags, worker_flags);",
            "\t}",
            "",
            "\traw_spin_unlock_irq(&pool->lock);",
            "}",
            "static void restore_unbound_workers_cpumask(struct worker_pool *pool, int cpu)",
            "{",
            "\tstatic cpumask_t cpumask;",
            "\tstruct worker *worker;",
            "",
            "\tlockdep_assert_held(&wq_pool_attach_mutex);",
            "",
            "\t/* is @cpu allowed for @pool? */",
            "\tif (!cpumask_test_cpu(cpu, pool->attrs->cpumask))",
            "\t\treturn;",
            "",
            "\tcpumask_and(&cpumask, pool->attrs->cpumask, cpu_online_mask);",
            "",
            "\t/* as we're called from CPU_ONLINE, the following shouldn't fail */",
            "\tfor_each_pool_worker(worker, pool)",
            "\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, &cpumask) < 0);",
            "}"
          ],
          "function_name": "unbind_workers, rebind_workers, restore_unbound_workers_cpumask",
          "description": "管理工作者与CPU的绑定状态，通过解除/重新绑定调整工作者运行环境，确保CPU上下线时并发控制正确性",
          "similarity": 0.5888622999191284
        },
        {
          "chunk_id": 33,
          "file_path": "kernel/workqueue.c",
          "start_line": 6492,
          "end_line": 6592,
          "content": [
            "static void panic_on_wq_watchdog(void)",
            "{",
            "\tstatic unsigned int wq_stall;",
            "",
            "\tif (wq_panic_on_stall) {",
            "\t\twq_stall++;",
            "\t\tBUG_ON(wq_stall >= wq_panic_on_stall);",
            "\t}",
            "}",
            "static void wq_watchdog_reset_touched(void)",
            "{",
            "\tint cpu;",
            "",
            "\twq_watchdog_touched = jiffies;",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(wq_watchdog_touched_cpu, cpu) = jiffies;",
            "}",
            "static void wq_watchdog_timer_fn(struct timer_list *unused)",
            "{",
            "\tunsigned long thresh = READ_ONCE(wq_watchdog_thresh) * HZ;",
            "\tbool lockup_detected = false;",
            "\tbool cpu_pool_stall = false;",
            "\tunsigned long now = jiffies;",
            "\tstruct worker_pool *pool;",
            "\tint pi;",
            "",
            "\tif (!thresh)",
            "\t\treturn;",
            "",
            "\trcu_read_lock();",
            "",
            "\tfor_each_pool(pool, pi) {",
            "\t\tunsigned long pool_ts, touched, ts;",
            "",
            "\t\tpool->cpu_stall = false;",
            "\t\tif (list_empty(&pool->worklist))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * If a virtual machine is stopped by the host it can look to",
            "\t\t * the watchdog like a stall.",
            "\t\t */",
            "\t\tkvm_check_and_clear_guest_paused();",
            "",
            "\t\t/* get the latest of pool and touched timestamps */",
            "\t\tif (pool->cpu >= 0)",
            "\t\t\ttouched = READ_ONCE(per_cpu(wq_watchdog_touched_cpu, pool->cpu));",
            "\t\telse",
            "\t\t\ttouched = READ_ONCE(wq_watchdog_touched);",
            "\t\tpool_ts = READ_ONCE(pool->watchdog_ts);",
            "",
            "\t\tif (time_after(pool_ts, touched))",
            "\t\t\tts = pool_ts;",
            "\t\telse",
            "\t\t\tts = touched;",
            "",
            "\t\t/* did we stall? */",
            "\t\tif (time_after(now, ts + thresh)) {",
            "\t\t\tlockup_detected = true;",
            "\t\t\tif (pool->cpu >= 0) {",
            "\t\t\t\tpool->cpu_stall = true;",
            "\t\t\t\tcpu_pool_stall = true;",
            "\t\t\t}",
            "\t\t\tpr_emerg(\"BUG: workqueue lockup - pool\");",
            "\t\t\tpr_cont_pool_info(pool);",
            "\t\t\tpr_cont(\" stuck for %us!\\n\",",
            "\t\t\t\tjiffies_to_msecs(now - pool_ts) / 1000);",
            "\t\t}",
            "",
            "",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "",
            "\tif (lockup_detected)",
            "\t\tshow_all_workqueues();",
            "",
            "\tif (cpu_pool_stall)",
            "\t\tshow_cpu_pools_hogs();",
            "",
            "\tif (lockup_detected)",
            "\t\tpanic_on_wq_watchdog();",
            "",
            "\twq_watchdog_reset_touched();",
            "\tmod_timer(&wq_watchdog_timer, jiffies + thresh);",
            "}",
            "notrace void wq_watchdog_touch(int cpu)",
            "{",
            "\tunsigned long thresh = READ_ONCE(wq_watchdog_thresh) * HZ;",
            "\tunsigned long touch_ts = READ_ONCE(wq_watchdog_touched);",
            "\tunsigned long now = jiffies;",
            "",
            "\tif (cpu >= 0)",
            "\t\tper_cpu(wq_watchdog_touched_cpu, cpu) = now;",
            "\telse",
            "\t\tWARN_ONCE(1, \"%s should be called with valid CPU\", __func__);",
            "",
            "\t/* Don't unnecessarily store to global cacheline */",
            "\tif (time_after(now, touch_ts + thresh / 4))",
            "\t\tWRITE_ONCE(wq_watchdog_touched, jiffies);",
            "}"
          ],
          "function_name": "panic_on_wq_watchdog, wq_watchdog_reset_touched, wq_watchdog_timer_fn, wq_watchdog_touch",
          "description": "实现工作队列看门狗机制，通过定时器周期性检测任务阻塞状态，当检测到CPU池超时时触发警告日志和panic，包含超时阈值管理、时间戳更新及阻塞状态标识逻辑。",
          "similarity": 0.5832298994064331
        }
      ]
    },
    {
      "source_file": "kernel/kthread.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:30:24\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `kthread.c`\n\n---\n\n# kthread.c 技术文档\n\n## 文件概述\n\n`kthread.c` 是 Linux 内核中实现内核线程（kernel thread, kthread）管理机制的核心文件。它提供了创建、控制、同步和销毁内核线程的基础设施，确保内核线程在干净、受控的环境中运行，即使是从用户空间（如 modprobe、CPU 热插拔等）触发创建也能保证一致性。该文件实现了 kthread 的生命周期管理、状态控制（如停止、暂停）、数据访问接口以及与调度器、cgroup、freezer 等子系统的集成。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`struct kthread_create_info`**  \n  用于在 `kthread_create()` 和后台守护线程 `kthreadd` 之间传递创建参数和结果，包含线程函数、数据、节点信息、任务结构体指针和完成量。\n\n- **`struct kthread`**  \n  内核线程的私有控制块，挂载在 `task_struct->worker_private` 上，包含：\n  - 状态标志位（`KTHREAD_IS_PER_CPU`, `KTHREAD_SHOULD_STOP`, `KTHREAD_SHOULD_PARK`）\n  - CPU 绑定信息\n  - 线程函数指针和用户数据\n  - 用于同步的 `parked` 和 `exited` 完成量\n  - 完整线程名（当 `task->comm` 被截断时使用）\n  - （可选）块设备 cgroup 上下文（`blkcg_css`）\n\n- **全局变量**\n  - `kthread_create_lock`：保护 `kthread_create_list` 的自旋锁\n  - `kthread_create_list`：待创建内核线程的请求队列\n  - `kthreadd_task`：负责实际创建内核线程的守护进程任务结构体\n\n### 主要函数\n\n- **状态查询函数**\n  - `kthread_should_stop()`：检查是否应停止线程（由 `kthread_stop()` 触发）\n  - `kthread_should_park()`：检查是否应暂停线程（由 `kthread_park()` 触发）\n  - `kthread_should_stop_or_park()`：同时检查停止或暂停请求\n  - `kthread_freezable_should_stop()`：支持冻结的 kthread 停止检查，集成 freezer 机制\n\n- **数据访问函数**\n  - `kthread_func()`：获取线程创建时指定的函数指针\n  - `kthread_data()`：获取线程创建时传入的私有数据\n  - `kthread_probe_data()`：安全地探测可能的 kthread 数据（使用 `copy_from_kernel_nofault` 避免崩溃）\n  - `get_kthread_comm()`：获取完整的线程名称（优先使用 `full_name`）\n\n- **生命周期管理**\n  - `set_kthread_struct()`：为新任务分配并初始化 `struct kthread`\n  - `free_kthread_struct()`：释放 `struct kthread` 及其资源\n  - `kthread_parkme()`：将当前线程置于 `TASK_PARKED` 状态并等待唤醒\n  - `kthread_exit()`：终止当前 kthread 并返回结果（未在代码片段中完整显示）\n\n- **辅助函数**\n  - `to_kthread()` / `__to_kthread()`：从 `task_struct` 安全转换为 `struct kthread`，后者不假设任务一定是 kthread\n\n## 关键实现\n\n### kthread 私有数据管理\n- 每个 kthread 通过 `task_struct->worker_private` 指向其 `struct kthread` 实例。\n- `to_kthread()` 在访问前验证 `PF_KTHREAD` 标志，确保类型安全。\n- `__to_kthread()` 更加保守，仅在同时满足 `worker_private != NULL` 且 `PF_KTHREAD` 时才返回有效指针，以应对 `kernel_thread()` 可能执行 `exec()` 导致标志失效的情况。\n\n### 线程暂停机制（Parking）\n- 使用 `TASK_PARKED` 特殊任务状态，避免与常规调度状态冲突。\n- 在设置状态和检查标志之间使用原子操作，防止唤醒丢失。\n- 调用 `schedule_preempt_disabled()` 禁用抢占，确保 `kthread_park()` 调用者能可靠检测到线程已暂停。\n\n### 安全数据访问\n- `kthread_probe_data()` 使用 `copy_from_kernel_nofault()` 安全读取数据指针，即使目标内存无效也不会导致内核 oops，适用于调试或不确定上下文。\n\n### 冻结集成\n- `kthread_freezable_should_stop()` 在检查停止标志前先处理冻结请求，调用 `__refrigerator()` 进入冻结状态，避免 freezer 与 kthread_stop 死锁。\n\n### 名称管理\n- 当线程名超过 `TASK_COMM_LEN` 时，原始名称存储在 `kthread->full_name` 中，`get_kthread_comm()` 优先返回完整名称。\n\n## 依赖关系\n\n- **调度子系统**：依赖 `sched.h` 提供任务状态管理、调度原语（`schedule()`）、CPU 隔离等。\n- **内存管理**：使用 `slab.h` 分配 `kthread` 结构，`mm.h` 处理内存上下文。\n- **同步机制**：依赖 `completion.h` 实现线程创建和状态同步。\n- **cgroup 子系统**：条件编译支持 `CONFIG_BLK_CGROUP`，集成块设备 cgroup 控制。\n- **冻结子系统**：通过 `freezer.h` 与系统 suspend/hibernate 机制协作。\n- **追踪系统**：集成 `trace/events/sched.h` 提供调度事件追踪。\n- **用户空间接口**：通过 `uaccess.h` 支持安全内核空间访问（用于 `kthread_probe_data`）。\n\n## 使用场景\n\n- **内核模块加载**：`modprobe` 触发的模块可能创建 kthread，需通过 `kthreadd` 确保干净环境。\n- **设备驱动**：驱动程序使用 `kthread_run()` 创建工作线程处理中断下半部或轮询任务。\n- **系统服务线程**：如 `kswapd`（内存回收）、`kcompactd`（内存压缩）等核心内核线程。\n- **CPU 热插拔**：在 CPU 上下线时创建或迁移 per-CPU kthread。\n- **电源管理**：通过 `kthread_freezable_should_stop()` 支持系统 suspend 时冻结 kthread。\n- **动态资源管理**：使用 `kthread_park/unpark` 暂停/恢复线程以节省资源（如空闲时暂停工作线程）。\n- **调试与监控**：工具通过 `kthread_func()` 和 `kthread_data()` 获取线程上下文信息。",
      "similarity": 0.6339151859283447,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/kthread.c",
          "start_line": 299,
          "end_line": 413,
          "content": [
            "void kthread_parkme(void)",
            "{",
            "\t__kthread_parkme(to_kthread(current));",
            "}",
            "void __noreturn kthread_exit(long result)",
            "{",
            "\tstruct kthread *kthread = to_kthread(current);",
            "\tkthread->result = result;",
            "\tdo_exit(0);",
            "}",
            "void __noreturn kthread_complete_and_exit(struct completion *comp, long code)",
            "{",
            "\tif (comp)",
            "\t\tcomplete(comp);",
            "",
            "\tkthread_exit(code);",
            "}",
            "static int kthread(void *_create)",
            "{",
            "\tstatic const struct sched_param param = { .sched_priority = 0 };",
            "\t/* Copy data: it's on kthread's stack */",
            "\tstruct kthread_create_info *create = _create;",
            "\tint (*threadfn)(void *data) = create->threadfn;",
            "\tvoid *data = create->data;",
            "\tstruct completion *done;",
            "\tstruct kthread *self;",
            "\tint ret;",
            "",
            "\tself = to_kthread(current);",
            "",
            "\t/* Release the structure when caller killed by a fatal signal. */",
            "\tdone = xchg(&create->done, NULL);",
            "\tif (!done) {",
            "\t\tkfree(create->full_name);",
            "\t\tkfree(create);",
            "\t\tkthread_exit(-EINTR);",
            "\t}",
            "",
            "\tself->full_name = create->full_name;",
            "\tself->threadfn = threadfn;",
            "\tself->data = data;",
            "",
            "\t/*",
            "\t * The new thread inherited kthreadd's priority and CPU mask. Reset",
            "\t * back to default in case they have been changed.",
            "\t */",
            "\tsched_setscheduler_nocheck(current, SCHED_NORMAL, &param);",
            "\tset_cpus_allowed_ptr(current, housekeeping_cpumask(HK_TYPE_KTHREAD));",
            "",
            "\t/* OK, tell user we're spawned, wait for stop or wakeup */",
            "\t__set_current_state(TASK_UNINTERRUPTIBLE);",
            "\tcreate->result = current;",
            "\t/*",
            "\t * Thread is going to call schedule(), do not preempt it,",
            "\t * or the creator may spend more time in wait_task_inactive().",
            "\t */",
            "\tpreempt_disable();",
            "\tcomplete(done);",
            "\tschedule_preempt_disabled();",
            "\tpreempt_enable();",
            "",
            "\tret = -EINTR;",
            "\tif (!test_bit(KTHREAD_SHOULD_STOP, &self->flags)) {",
            "\t\tcgroup_kthread_ready();",
            "\t\t__kthread_parkme(self);",
            "\t\tret = threadfn(data);",
            "\t}",
            "\tkthread_exit(ret);",
            "}",
            "int tsk_fork_get_node(struct task_struct *tsk)",
            "{",
            "#ifdef CONFIG_NUMA",
            "\tif (tsk == kthreadd_task)",
            "\t\treturn tsk->pref_node_fork;",
            "#endif",
            "\treturn NUMA_NO_NODE;",
            "}",
            "static void create_kthread(struct kthread_create_info *create)",
            "{",
            "\tint pid;",
            "",
            "#ifdef CONFIG_NUMA",
            "\tcurrent->pref_node_fork = create->node;",
            "#endif",
            "\t/* We want our own signal handler (we take no signals by default). */",
            "\tpid = kernel_thread(kthread, create, create->full_name,",
            "\t\t\t    CLONE_FS | CLONE_FILES | SIGCHLD);",
            "\tif (pid < 0) {",
            "\t\t/* Release the structure when caller killed by a fatal signal. */",
            "\t\tstruct completion *done = xchg(&create->done, NULL);",
            "",
            "\t\tkfree(create->full_name);",
            "\t\tif (!done) {",
            "\t\t\tkfree(create);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tcreate->result = ERR_PTR(pid);",
            "\t\tcomplete(done);",
            "\t}",
            "}",
            "static void __kthread_bind_mask(struct task_struct *p, const struct cpumask *mask, unsigned int state)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tif (!wait_task_inactive(p, state)) {",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* It's safe because the task is inactive. */",
            "\traw_spin_lock_irqsave(&p->pi_lock, flags);",
            "\tdo_set_cpus_allowed(p, mask);",
            "\tp->flags |= PF_NO_SETAFFINITY;",
            "\traw_spin_unlock_irqrestore(&p->pi_lock, flags);",
            "}"
          ],
          "function_name": "kthread_parkme, kthread_exit, kthread_complete_and_exit, kthread, tsk_fork_get_node, create_kthread, __kthread_bind_mask",
          "description": "处理线程执行流程、节点绑定及异常退出，kthread作为内核线程入口执行指定函数，create_kthread创建新线程并绑定CPU，__kthread_bind_mask调整线程CPU亲和性。",
          "similarity": 0.6415051817893982
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/kthread.c",
          "start_line": 982,
          "end_line": 1095,
          "content": [
            "static inline bool queuing_blocked(struct kthread_worker *worker,",
            "\t\t\t\t   struct kthread_work *work)",
            "{",
            "\tlockdep_assert_held(&worker->lock);",
            "",
            "\treturn !list_empty(&work->node) || work->canceling;",
            "}",
            "static void kthread_insert_work_sanity_check(struct kthread_worker *worker,",
            "\t\t\t\t\t     struct kthread_work *work)",
            "{",
            "\tlockdep_assert_held(&worker->lock);",
            "\tWARN_ON_ONCE(!list_empty(&work->node));",
            "\t/* Do not use a work with >1 worker, see kthread_queue_work() */",
            "\tWARN_ON_ONCE(work->worker && work->worker != worker);",
            "}",
            "static void kthread_insert_work(struct kthread_worker *worker,",
            "\t\t\t\tstruct kthread_work *work,",
            "\t\t\t\tstruct list_head *pos)",
            "{",
            "\tkthread_insert_work_sanity_check(worker, work);",
            "",
            "\ttrace_sched_kthread_work_queue_work(worker, work);",
            "",
            "\tlist_add_tail(&work->node, pos);",
            "\twork->worker = worker;",
            "\tif (!worker->current_work && likely(worker->task))",
            "\t\twake_up_process(worker->task);",
            "}",
            "bool kthread_queue_work(struct kthread_worker *worker,",
            "\t\t\tstruct kthread_work *work)",
            "{",
            "\tbool ret = false;",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&worker->lock, flags);",
            "\tif (!queuing_blocked(worker, work)) {",
            "\t\tkthread_insert_work(worker, work, &worker->work_list);",
            "\t\tret = true;",
            "\t}",
            "\traw_spin_unlock_irqrestore(&worker->lock, flags);",
            "\treturn ret;",
            "}",
            "void kthread_delayed_work_timer_fn(struct timer_list *t)",
            "{",
            "\tstruct kthread_delayed_work *dwork = from_timer(dwork, t, timer);",
            "\tstruct kthread_work *work = &dwork->work;",
            "\tstruct kthread_worker *worker = work->worker;",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * This might happen when a pending work is reinitialized.",
            "\t * It means that it is used a wrong way.",
            "\t */",
            "\tif (WARN_ON_ONCE(!worker))",
            "\t\treturn;",
            "",
            "\traw_spin_lock_irqsave(&worker->lock, flags);",
            "\t/* Work must not be used with >1 worker, see kthread_queue_work(). */",
            "\tWARN_ON_ONCE(work->worker != worker);",
            "",
            "\t/* Move the work from worker->delayed_work_list. */",
            "\tWARN_ON_ONCE(list_empty(&work->node));",
            "\tlist_del_init(&work->node);",
            "\tif (!work->canceling)",
            "\t\tkthread_insert_work(worker, work, &worker->work_list);",
            "",
            "\traw_spin_unlock_irqrestore(&worker->lock, flags);",
            "}",
            "static void __kthread_queue_delayed_work(struct kthread_worker *worker,",
            "\t\t\t\t\t struct kthread_delayed_work *dwork,",
            "\t\t\t\t\t unsigned long delay)",
            "{",
            "\tstruct timer_list *timer = &dwork->timer;",
            "\tstruct kthread_work *work = &dwork->work;",
            "",
            "\tWARN_ON_ONCE(timer->function != kthread_delayed_work_timer_fn);",
            "",
            "\t/*",
            "\t * If @delay is 0, queue @dwork->work immediately.  This is for",
            "\t * both optimization and correctness.  The earliest @timer can",
            "\t * expire is on the closest next tick and delayed_work users depend",
            "\t * on that there's no such delay when @delay is 0.",
            "\t */",
            "\tif (!delay) {",
            "\t\tkthread_insert_work(worker, work, &worker->work_list);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Be paranoid and try to detect possible races already now. */",
            "\tkthread_insert_work_sanity_check(worker, work);",
            "",
            "\tlist_add(&work->node, &worker->delayed_work_list);",
            "\twork->worker = worker;",
            "\ttimer->expires = jiffies + delay;",
            "\tadd_timer(timer);",
            "}",
            "bool kthread_queue_delayed_work(struct kthread_worker *worker,",
            "\t\t\t\tstruct kthread_delayed_work *dwork,",
            "\t\t\t\tunsigned long delay)",
            "{",
            "\tstruct kthread_work *work = &dwork->work;",
            "\tunsigned long flags;",
            "\tbool ret = false;",
            "",
            "\traw_spin_lock_irqsave(&worker->lock, flags);",
            "",
            "\tif (!queuing_blocked(worker, work)) {",
            "\t\t__kthread_queue_delayed_work(worker, dwork, delay);",
            "\t\tret = true;",
            "\t}",
            "",
            "\traw_spin_unlock_irqrestore(&worker->lock, flags);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queuing_blocked, kthread_insert_work_sanity_check, kthread_insert_work, kthread_queue_work, kthread_delayed_work_timer_fn, __kthread_queue_delayed_work, kthread_queue_delayed_work",
          "description": "实现kthread_worker与kthread_work的队列管理，包含插入/延迟插入逻辑、锁保护及任务唤醒机制，处理工作项状态校验和延迟定时器回调",
          "similarity": 0.6164517402648926
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/kthread.c",
          "start_line": 1316,
          "end_line": 1426,
          "content": [
            "static bool __kthread_cancel_work_sync(struct kthread_work *work, bool is_dwork)",
            "{",
            "\tstruct kthread_worker *worker = work->worker;",
            "\tunsigned long flags;",
            "\tint ret = false;",
            "",
            "\tif (!worker)",
            "\t\tgoto out;",
            "",
            "\traw_spin_lock_irqsave(&worker->lock, flags);",
            "\t/* Work must not be used with >1 worker, see kthread_queue_work(). */",
            "\tWARN_ON_ONCE(work->worker != worker);",
            "",
            "\tif (is_dwork)",
            "\t\tkthread_cancel_delayed_work_timer(work, &flags);",
            "",
            "\tret = __kthread_cancel_work(work);",
            "",
            "\tif (worker->current_work != work)",
            "\t\tgoto out_fast;",
            "",
            "\t/*",
            "\t * The work is in progress and we need to wait with the lock released.",
            "\t * In the meantime, block any queuing by setting the canceling counter.",
            "\t */",
            "\twork->canceling++;",
            "\traw_spin_unlock_irqrestore(&worker->lock, flags);",
            "\tkthread_flush_work(work);",
            "\traw_spin_lock_irqsave(&worker->lock, flags);",
            "\twork->canceling--;",
            "",
            "out_fast:",
            "\traw_spin_unlock_irqrestore(&worker->lock, flags);",
            "out:",
            "\treturn ret;",
            "}",
            "bool kthread_cancel_work_sync(struct kthread_work *work)",
            "{",
            "\treturn __kthread_cancel_work_sync(work, false);",
            "}",
            "bool kthread_cancel_delayed_work_sync(struct kthread_delayed_work *dwork)",
            "{",
            "\treturn __kthread_cancel_work_sync(&dwork->work, true);",
            "}",
            "void kthread_flush_worker(struct kthread_worker *worker)",
            "{",
            "\tstruct kthread_flush_work fwork = {",
            "\t\tKTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),",
            "\t\tCOMPLETION_INITIALIZER_ONSTACK(fwork.done),",
            "\t};",
            "",
            "\tkthread_queue_work(worker, &fwork.work);",
            "\twait_for_completion(&fwork.done);",
            "}",
            "void kthread_destroy_worker(struct kthread_worker *worker)",
            "{",
            "\tstruct task_struct *task;",
            "",
            "\ttask = worker->task;",
            "\tif (WARN_ON(!task))",
            "\t\treturn;",
            "",
            "\tkthread_flush_worker(worker);",
            "\tkthread_stop(task);",
            "\tWARN_ON(!list_empty(&worker->delayed_work_list));",
            "\tWARN_ON(!list_empty(&worker->work_list));",
            "\tkfree(worker);",
            "}",
            "void kthread_use_mm(struct mm_struct *mm)",
            "{",
            "\tstruct mm_struct *active_mm;",
            "\tstruct task_struct *tsk = current;",
            "",
            "\tWARN_ON_ONCE(!(tsk->flags & PF_KTHREAD));",
            "\tWARN_ON_ONCE(tsk->mm);",
            "",
            "\t/*",
            "\t * It is possible for mm to be the same as tsk->active_mm, but",
            "\t * we must still mmgrab(mm) and mmdrop_lazy_tlb(active_mm),",
            "\t * because these references are not equivalent.",
            "\t */",
            "\tmmgrab(mm);",
            "",
            "\ttask_lock(tsk);",
            "\t/* Hold off tlb flush IPIs while switching mm's */",
            "\tlocal_irq_disable();",
            "\tactive_mm = tsk->active_mm;",
            "\ttsk->active_mm = mm;",
            "\ttsk->mm = mm;",
            "\tmembarrier_update_current_mm(mm);",
            "\t#ifdef CONFIG_IEE",
            "\tiee_set_token_pgd(tsk, mm->pgd);",
            "\t#endif",
            "\tswitch_mm_irqs_off(active_mm, mm, tsk);",
            "\tlocal_irq_enable();",
            "\ttask_unlock(tsk);",
            "#ifdef finish_arch_post_lock_switch",
            "\tfinish_arch_post_lock_switch();",
            "#endif",
            "",
            "\t/*",
            "\t * When a kthread starts operating on an address space, the loop",
            "\t * in membarrier_{private,global}_expedited() may not observe",
            "\t * that tsk->mm, and not issue an IPI. Membarrier requires a",
            "\t * memory barrier after storing to tsk->mm, before accessing",
            "\t * user-space memory. A full memory barrier for membarrier",
            "\t * {PRIVATE,GLOBAL}_EXPEDITED is implicitly provided by",
            "\t * mmdrop_lazy_tlb().",
            "\t */",
            "\tmmdrop_lazy_tlb(active_mm);",
            "}"
          ],
          "function_name": "__kthread_cancel_work_sync, kthread_cancel_work_sync, kthread_cancel_delayed_work_sync, kthread_flush_worker, kthread_destroy_worker, kthread_use_mm",
          "description": "实现同步取消工作项及延迟工作功能，强制刷新工作者队列并销毁工作者线程，包含内存管理切换和块控制组关联逻辑",
          "similarity": 0.6020618677139282
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/kthread.c",
          "start_line": 731,
          "end_line": 846,
          "content": [
            "int kthread_stop_put(struct task_struct *k)",
            "{",
            "\tint ret;",
            "",
            "\tret = kthread_stop(k);",
            "\tput_task_struct(k);",
            "\treturn ret;",
            "}",
            "int kthreadd(void *unused)",
            "{",
            "\tstruct task_struct *tsk = current;",
            "",
            "\t/* Setup a clean context for our children to inherit. */",
            "\tset_task_comm(tsk, \"kthreadd\");",
            "\tignore_signals(tsk);",
            "\tset_cpus_allowed_ptr(tsk, housekeeping_cpumask(HK_TYPE_KTHREAD));",
            "\tset_mems_allowed(node_states[N_MEMORY]);",
            "",
            "\tcurrent->flags |= PF_NOFREEZE;",
            "\tcgroup_init_kthreadd();",
            "",
            "\tfor (;;) {",
            "\t\tset_current_state(TASK_INTERRUPTIBLE);",
            "\t\tif (list_empty(&kthread_create_list))",
            "\t\t\tschedule();",
            "\t\t__set_current_state(TASK_RUNNING);",
            "",
            "\t\tspin_lock(&kthread_create_lock);",
            "\t\twhile (!list_empty(&kthread_create_list)) {",
            "\t\t\tstruct kthread_create_info *create;",
            "",
            "\t\t\tcreate = list_entry(kthread_create_list.next,",
            "\t\t\t\t\t    struct kthread_create_info, list);",
            "\t\t\tlist_del_init(&create->list);",
            "\t\t\tspin_unlock(&kthread_create_lock);",
            "",
            "\t\t\tcreate_kthread(create);",
            "",
            "\t\t\tspin_lock(&kthread_create_lock);",
            "\t\t}",
            "\t\tspin_unlock(&kthread_create_lock);",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "void __kthread_init_worker(struct kthread_worker *worker,",
            "\t\t\t\tconst char *name,",
            "\t\t\t\tstruct lock_class_key *key)",
            "{",
            "\tmemset(worker, 0, sizeof(struct kthread_worker));",
            "\traw_spin_lock_init(&worker->lock);",
            "\tlockdep_set_class_and_name(&worker->lock, key, name);",
            "\tINIT_LIST_HEAD(&worker->work_list);",
            "\tINIT_LIST_HEAD(&worker->delayed_work_list);",
            "}",
            "int kthread_worker_fn(void *worker_ptr)",
            "{",
            "\tstruct kthread_worker *worker = worker_ptr;",
            "\tstruct kthread_work *work;",
            "",
            "\t/*",
            "\t * FIXME: Update the check and remove the assignment when all kthread",
            "\t * worker users are created using kthread_create_worker*() functions.",
            "\t */",
            "\tWARN_ON(worker->task && worker->task != current);",
            "\tworker->task = current;",
            "",
            "\tif (worker->flags & KTW_FREEZABLE)",
            "\t\tset_freezable();",
            "",
            "repeat:",
            "\tset_current_state(TASK_INTERRUPTIBLE);\t/* mb paired w/ kthread_stop */",
            "",
            "\tif (kthread_should_stop()) {",
            "\t\t__set_current_state(TASK_RUNNING);",
            "\t\traw_spin_lock_irq(&worker->lock);",
            "\t\tworker->task = NULL;",
            "\t\traw_spin_unlock_irq(&worker->lock);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\twork = NULL;",
            "\traw_spin_lock_irq(&worker->lock);",
            "\tif (!list_empty(&worker->work_list)) {",
            "\t\twork = list_first_entry(&worker->work_list,",
            "\t\t\t\t\tstruct kthread_work, node);",
            "\t\tlist_del_init(&work->node);",
            "\t}",
            "\tworker->current_work = work;",
            "\traw_spin_unlock_irq(&worker->lock);",
            "",
            "\tif (work) {",
            "\t\tkthread_work_func_t func = work->func;",
            "\t\t__set_current_state(TASK_RUNNING);",
            "\t\ttrace_sched_kthread_work_execute_start(work);",
            "\t\twork->func(work);",
            "\t\t/*",
            "\t\t * Avoid dereferencing work after this point.  The trace",
            "\t\t * event only cares about the address.",
            "\t\t */",
            "\t\ttrace_sched_kthread_work_execute_end(work, func);",
            "\t} else if (!freezing(current)) {",
            "\t\tschedule();",
            "\t} else {",
            "\t\t/*",
            "\t\t * Handle the case where the current remains",
            "\t\t * TASK_INTERRUPTIBLE. try_to_freeze() expects",
            "\t\t * the current to be TASK_RUNNING.",
            "\t\t */",
            "\t\t__set_current_state(TASK_RUNNING);",
            "\t}",
            "",
            "\ttry_to_freeze();",
            "\tcond_resched();",
            "\tgoto repeat;",
            "}"
          ],
          "function_name": "kthread_stop_put, kthreadd, __kthread_init_worker, kthread_worker_fn",
          "description": "实现kthreadd主线程逻辑及工作队列管理，kthreadd持续处理线程创建请求，kthread_worker_fn作为工作队列执行入口，支持可冻结状态下的任务调度。",
          "similarity": 0.6018428802490234
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/kthread.c",
          "start_line": 102,
          "end_line": 211,
          "content": [
            "void get_kthread_comm(char *buf, size_t buf_size, struct task_struct *tsk)",
            "{",
            "\tstruct kthread *kthread = to_kthread(tsk);",
            "",
            "\tif (!kthread || !kthread->full_name) {",
            "\t\t__get_task_comm(buf, buf_size, tsk);",
            "\t\treturn;",
            "\t}",
            "",
            "\tstrscpy_pad(buf, kthread->full_name, buf_size);",
            "}",
            "bool set_kthread_struct(struct task_struct *p)",
            "{",
            "\tstruct kthread *kthread;",
            "",
            "\tif (WARN_ON_ONCE(to_kthread(p)))",
            "\t\treturn false;",
            "",
            "\tkthread = kzalloc(sizeof(*kthread), GFP_KERNEL);",
            "\tif (!kthread)",
            "\t\treturn false;",
            "",
            "\tinit_completion(&kthread->exited);",
            "\tinit_completion(&kthread->parked);",
            "\tp->vfork_done = &kthread->exited;",
            "",
            "\tp->worker_private = kthread;",
            "\treturn true;",
            "}",
            "void free_kthread_struct(struct task_struct *k)",
            "{",
            "\tstruct kthread *kthread;",
            "",
            "\t/*",
            "\t * Can be NULL if kmalloc() in set_kthread_struct() failed.",
            "\t */",
            "\tkthread = to_kthread(k);",
            "\tif (!kthread)",
            "\t\treturn;",
            "",
            "#ifdef CONFIG_BLK_CGROUP",
            "\tWARN_ON_ONCE(kthread->blkcg_css);",
            "#endif",
            "\tk->worker_private = NULL;",
            "\tkfree(kthread->full_name);",
            "\tkfree(kthread);",
            "}",
            "bool kthread_should_stop(void)",
            "{",
            "\treturn test_bit(KTHREAD_SHOULD_STOP, &to_kthread(current)->flags);",
            "}",
            "static bool __kthread_should_park(struct task_struct *k)",
            "{",
            "\treturn test_bit(KTHREAD_SHOULD_PARK, &to_kthread(k)->flags);",
            "}",
            "bool kthread_should_park(void)",
            "{",
            "\treturn __kthread_should_park(current);",
            "}",
            "bool kthread_should_stop_or_park(void)",
            "{",
            "\tstruct kthread *kthread = __to_kthread(current);",
            "",
            "\tif (!kthread)",
            "\t\treturn false;",
            "",
            "\treturn kthread->flags & (BIT(KTHREAD_SHOULD_STOP) | BIT(KTHREAD_SHOULD_PARK));",
            "}",
            "bool kthread_freezable_should_stop(bool *was_frozen)",
            "{",
            "\tbool frozen = false;",
            "",
            "\tmight_sleep();",
            "",
            "\tif (unlikely(freezing(current)))",
            "\t\tfrozen = __refrigerator(true);",
            "",
            "\tif (was_frozen)",
            "\t\t*was_frozen = frozen;",
            "",
            "\treturn kthread_should_stop();",
            "}",
            "static void __kthread_parkme(struct kthread *self)",
            "{",
            "\tfor (;;) {",
            "\t\t/*",
            "\t\t * TASK_PARKED is a special state; we must serialize against",
            "\t\t * possible pending wakeups to avoid store-store collisions on",
            "\t\t * task->state.",
            "\t\t *",
            "\t\t * Such a collision might possibly result in the task state",
            "\t\t * changin from TASK_PARKED and us failing the",
            "\t\t * wait_task_inactive() in kthread_park().",
            "\t\t */",
            "\t\tset_special_state(TASK_PARKED);",
            "\t\tif (!test_bit(KTHREAD_SHOULD_PARK, &self->flags))",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * Thread is going to call schedule(), do not preempt it,",
            "\t\t * or the caller of kthread_park() may spend more time in",
            "\t\t * wait_task_inactive().",
            "\t\t */",
            "\t\tpreempt_disable();",
            "\t\tcomplete(&self->parked);",
            "\t\tschedule_preempt_disabled();",
            "\t\tpreempt_enable();",
            "\t}",
            "\t__set_current_state(TASK_RUNNING);",
            "}"
          ],
          "function_name": "get_kthread_comm, set_kthread_struct, free_kthread_struct, kthread_should_stop, __kthread_should_park, kthread_should_park, kthread_should_stop_or_park, kthread_freezable_should_stop, __kthread_parkme",
          "description": "实现内核线程的名称获取、结构体分配与释放、状态检测等功能，set_kthread_struct分配并初始化线程结构体，kthread_should_stop系列函数检测线程终止或停放标志。",
          "similarity": 0.5896944999694824
        }
      ]
    },
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.6331566572189331,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/rt.c",
          "start_line": 529,
          "end_line": 633,
          "content": [
            "static void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\tstruct sched_rt_entity *rt_se;",
            "",
            "\tint cpu = cpu_of(rq);",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tif (!rt_se)",
            "\t\t\tenqueue_top_rt_rq(rt_rq);",
            "\t\telse if (!on_rt_rq(rt_se))",
            "\t\t\tenqueue_rt_entity(rt_se, 0);",
            "",
            "\t\tif (rt_rq->highest_prio.curr < curr->prio)",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "}",
            "static void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (!rt_se) {",
            "\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "\t\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);",
            "\t}",
            "\telse if (on_rt_rq(rt_se))",
            "\t\tdequeue_rt_entity(rt_se, 0);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;",
            "}",
            "static int rt_se_boosted(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "\tstruct task_struct *p;",
            "",
            "\tif (rt_rq)",
            "\t\treturn !!rt_rq->rt_nr_boosted;",
            "",
            "\tp = rt_task_of(rt_se);",
            "\treturn p->prio != p->normal_prio;",
            "}",
            "bool sched_rt_bandwidth_account(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\treturn (hrtimer_active(&rt_b->rt_period_timer) ||",
            "\t\trt_rq->rt_time < rt_b->rt_runtime);",
            "}",
            "static void do_balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;",
            "\tint i, weight;",
            "\tu64 rt_period;",
            "",
            "\tweight = cpumask_weight(rd->span);",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\trt_period = ktime_to_ns(rt_b->rt_period);",
            "\tfor_each_cpu(i, rd->span) {",
            "\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\ts64 diff;",
            "",
            "\t\tif (iter == rt_rq)",
            "\t\t\tcontinue;",
            "",
            "\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either all rqs have inf runtime and there's nothing to steal",
            "\t\t * or __disable_runtime() below sets a specific rq to inf to",
            "\t\t * indicate its been disabled and disallow stealing.",
            "\t\t */",
            "\t\tif (iter->rt_runtime == RUNTIME_INF)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * From runqueues with spare time, take 1/n part of their",
            "\t\t * spare time, but no more than our period.",
            "\t\t */",
            "\t\tdiff = iter->rt_runtime - iter->rt_time;",
            "\t\tif (diff > 0) {",
            "\t\t\tdiff = div_u64((u64)diff, weight);",
            "\t\t\tif (rt_rq->rt_runtime + diff > rt_period)",
            "\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;",
            "\t\t\titer->rt_runtime -= diff;",
            "\t\t\trt_rq->rt_runtime += diff;",
            "\t\t\tif (rt_rq->rt_runtime == rt_period) {",
            "\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "next:",
            "\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, rt_se_boosted, sched_rt_bandwidth_account, do_balance_runtime",
          "description": "实现实时任务队列的插入/移除逻辑，跟踪运行时间消耗，通过跨CPU运行时间平衡算法实现带宽公平分配。",
          "similarity": 0.6417088508605957
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1339,
          "end_line": 1439,
          "content": [
            "static inline void",
            "update_stats_dequeue_rt(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se,",
            "\t\t\tint flags)",
            "{",
            "\tstruct task_struct *p = NULL;",
            "",
            "\tif (!schedstat_enabled())",
            "\t\treturn;",
            "",
            "\tif (rt_entity_is_task(rt_se))",
            "\t\tp = rt_task_of(rt_se);",
            "",
            "\tif ((flags & DEQUEUE_SLEEP) && p) {",
            "\t\tunsigned int state;",
            "",
            "\t\tstate = READ_ONCE(p->__state);",
            "\t\tif (state & TASK_INTERRUPTIBLE)",
            "\t\t\t__schedstat_set(p->stats.sleep_start,",
            "\t\t\t\t\trq_clock(rq_of_rt_rq(rt_rq)));",
            "",
            "\t\tif (state & TASK_UNINTERRUPTIBLE)",
            "\t\t\t__schedstat_set(p->stats.block_start,",
            "\t\t\t\t\trq_clock(rq_of_rt_rq(rt_rq)));",
            "\t}",
            "}",
            "static void __enqueue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);",
            "\tstruct rt_prio_array *array = &rt_rq->active;",
            "\tstruct rt_rq *group_rq = group_rt_rq(rt_se);",
            "\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);",
            "",
            "\t/*",
            "\t * Don't enqueue the group if its throttled, or when empty.",
            "\t * The latter is a consequence of the former when a child group",
            "\t * get throttled and the current group doesn't have any other",
            "\t * active members.",
            "\t */",
            "\tif (group_rq && (rt_rq_throttled(group_rq) || !group_rq->rt_nr_running)) {",
            "\t\tif (rt_se->on_list)",
            "\t\t\t__delist_rt_entity(rt_se, array);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (move_entity(flags)) {",
            "\t\tWARN_ON_ONCE(rt_se->on_list);",
            "\t\tif (flags & ENQUEUE_HEAD)",
            "\t\t\tlist_add(&rt_se->run_list, queue);",
            "\t\telse",
            "\t\t\tlist_add_tail(&rt_se->run_list, queue);",
            "",
            "\t\t__set_bit(rt_se_prio(rt_se), array->bitmap);",
            "\t\trt_se->on_list = 1;",
            "\t}",
            "\trt_se->on_rq = 1;",
            "",
            "\tinc_rt_tasks(rt_se, rt_rq);",
            "}",
            "static void __dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);",
            "\tstruct rt_prio_array *array = &rt_rq->active;",
            "",
            "\tif (move_entity(flags)) {",
            "\t\tWARN_ON_ONCE(!rt_se->on_list);",
            "\t\t__delist_rt_entity(rt_se, array);",
            "\t}",
            "\trt_se->on_rq = 0;",
            "",
            "\tdec_rt_tasks(rt_se, rt_rq);",
            "}",
            "static void dequeue_rt_stack(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct sched_rt_entity *back = NULL;",
            "\tunsigned int rt_nr_running;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\trt_se->back = back;",
            "\t\tback = rt_se;",
            "\t}",
            "",
            "\trt_nr_running = rt_rq_of_se(back)->rt_nr_running;",
            "",
            "\tfor (rt_se = back; rt_se; rt_se = rt_se->back) {",
            "\t\tif (on_rt_rq(rt_se))",
            "\t\t\t__dequeue_rt_entity(rt_se, flags);",
            "\t}",
            "",
            "\tdequeue_top_rt_rq(rt_rq_of_se(back), rt_nr_running);",
            "}",
            "static void enqueue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rq *rq = rq_of_rt_se(rt_se);",
            "",
            "\tupdate_stats_enqueue_rt(rt_rq_of_se(rt_se), rt_se, flags);",
            "",
            "\tdequeue_rt_stack(rt_se, flags);",
            "\tfor_each_sched_rt_entity(rt_se)",
            "\t\t__enqueue_rt_entity(rt_se, flags);",
            "\tenqueue_top_rt_rq(&rq->rt);",
            "}"
          ],
          "function_name": "update_stats_dequeue_rt, __enqueue_rt_entity, __dequeue_rt_entity, dequeue_rt_stack, enqueue_rt_entity",
          "description": "处理实时任务的统计更新、入队和出队操作，管理优先级队列和位图，协调多层级调度实体的栈式处理。",
          "similarity": 0.6333726644515991
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1449,
          "end_line": 1589,
          "content": [
            "static void dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rq *rq = rq_of_rt_se(rt_se);",
            "",
            "\tupdate_stats_dequeue_rt(rt_rq_of_se(rt_se), rt_se, flags);",
            "",
            "\tdequeue_rt_stack(rt_se, flags);",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\t\tif (rt_rq && rt_rq->rt_nr_running)",
            "\t\t\t__enqueue_rt_entity(rt_se, flags);",
            "\t}",
            "\tenqueue_top_rt_rq(&rq->rt);",
            "}",
            "static void",
            "enqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tif (flags & ENQUEUE_WAKEUP)",
            "\t\trt_se->timeout = 0;",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_rt(rt_rq_of_se(rt_se), rt_se);",
            "",
            "\tenqueue_rt_entity(rt_se, flags);",
            "",
            "\tif (!task_current(rq, p) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_task(rq, p);",
            "}",
            "static bool dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tdequeue_rt_entity(rt_se, flags);",
            "",
            "\tdequeue_pushable_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void",
            "requeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)",
            "{",
            "\tif (on_rt_rq(rt_se)) {",
            "\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "\t\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);",
            "",
            "\t\tif (head)",
            "\t\t\tlist_move(&rt_se->run_list, queue);",
            "\t\telse",
            "\t\t\tlist_move_tail(&rt_se->run_list, queue);",
            "\t}",
            "}",
            "static void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\trt_rq = rt_rq_of_se(rt_se);",
            "\t\trequeue_rt_entity(rt_rq, rt_se, head);",
            "\t}",
            "}",
            "static void yield_task_rt(struct rq *rq)",
            "{",
            "\trequeue_task_rt(rq, rq->curr, 0);",
            "}",
            "static int",
            "select_task_rq_rt(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tstruct rq *rq;",
            "\tbool test;",
            "",
            "\t/* For anything but wake ups, just return the task_cpu */",
            "\tif (!(flags & (WF_TTWU | WF_FORK)))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If the current task on @p's runqueue is an RT task, then",
            "\t * try to see if we can wake this RT task up on another",
            "\t * runqueue. Otherwise simply start this RT task",
            "\t * on its current runqueue.",
            "\t *",
            "\t * We want to avoid overloading runqueues. If the woken",
            "\t * task is a higher priority, then it will stay on this CPU",
            "\t * and the lower prio task should be moved to another CPU.",
            "\t * Even though this will probably make the lower prio task",
            "\t * lose its cache, we do not want to bounce a higher task",
            "\t * around just because it gave up its CPU, perhaps for a",
            "\t * lock?",
            "\t *",
            "\t * For equal prio tasks, we just let the scheduler sort it out.",
            "\t *",
            "\t * Otherwise, just let it ride on the affined RQ and the",
            "\t * post-schedule router will push the preempted task away",
            "\t *",
            "\t * This test is optimistic, if we get it wrong the load-balancer",
            "\t * will have to sort it out.",
            "\t *",
            "\t * We take into account the capacity of the CPU to ensure it fits the",
            "\t * requirement of the task - which is only important on heterogeneous",
            "\t * systems like big.LITTLE.",
            "\t */",
            "\ttest = curr &&",
            "\t       unlikely(rt_task(curr)) &&",
            "\t       (curr->nr_cpus_allowed < 2 || curr->prio <= p->prio);",
            "",
            "\tif (test || !rt_task_fits_capacity(p, cpu)) {",
            "\t\tint target = find_lowest_rq(p);",
            "",
            "\t\t/*",
            "\t\t * Bail out if we were forcing a migration to find a better",
            "\t\t * fitting CPU but our search failed.",
            "\t\t */",
            "\t\tif (!test && target != -1 && !rt_task_fits_capacity(p, target))",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\t/*",
            "\t\t * Don't bother moving it if the destination CPU is",
            "\t\t * not running a lower priority task.",
            "\t\t */",
            "\t\tif (target != -1 &&",
            "\t\t    p->prio < cpu_rq(target)->rt.highest_prio.curr)",
            "\t\t\tcpu = target;",
            "\t}",
            "",
            "out_unlock:",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "dequeue_rt_entity, enqueue_task_rt, dequeue_task_rt, requeue_rt_entity, requeue_task_rt, yield_task_rt, select_task_rq_rt",
          "description": "实现实时任务的出队逻辑、唤醒和迁移策略，提供CPU亲和性选择及负载均衡支持，维护优先级队列的动态调整。",
          "similarity": 0.6278740763664246
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1,
          "end_line": 56,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Real-Time Scheduling Class (mapped to the SCHED_FIFO and SCHED_RR",
            " * policies)",
            " */",
            "",
            "int sched_rr_timeslice = RR_TIMESLICE;",
            "/* More than 4 hours if BW_SHIFT equals 20. */",
            "static const u64 max_rt_runtime = MAX_BW;",
            "",
            "/*",
            " * period over which we measure -rt task CPU usage in us.",
            " * default: 1s",
            " */",
            "int sysctl_sched_rt_period = 1000000;",
            "",
            "/*",
            " * part of the period that we allow rt tasks to run in us.",
            " * default: 0.95s",
            " */",
            "int sysctl_sched_rt_runtime = 950000;",
            "",
            "#ifdef CONFIG_SYSCTL",
            "static int sysctl_sched_rr_timeslice = (MSEC_PER_SEC * RR_TIMESLICE) / HZ;",
            "static int sched_rt_handler(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos);",
            "static int sched_rr_handler(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos);",
            "static struct ctl_table sched_rt_sysctls[] = {",
            "\t{",
            "\t\t.procname       = \"sched_rt_period_us\",",
            "\t\t.data           = &sysctl_sched_rt_period,",
            "\t\t.maxlen         = sizeof(int),",
            "\t\t.mode           = 0644,",
            "\t\t.proc_handler   = sched_rt_handler,",
            "\t\t.extra1         = SYSCTL_ONE,",
            "\t\t.extra2         = SYSCTL_INT_MAX,",
            "\t},",
            "\t{",
            "\t\t.procname       = \"sched_rt_runtime_us\",",
            "\t\t.data           = &sysctl_sched_rt_runtime,",
            "\t\t.maxlen         = sizeof(int),",
            "\t\t.mode           = 0644,",
            "\t\t.proc_handler   = sched_rt_handler,",
            "\t\t.extra1         = SYSCTL_NEG_ONE,",
            "\t\t.extra2         = (void *)&sysctl_sched_rt_period,",
            "\t},",
            "\t{",
            "\t\t.procname       = \"sched_rr_timeslice_ms\",",
            "\t\t.data           = &sysctl_sched_rr_timeslice,",
            "\t\t.maxlen         = sizeof(int),",
            "\t\t.mode           = 0644,",
            "\t\t.proc_handler   = sched_rr_handler,",
            "\t},",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义实时调度类的配置参数及sysctl接口，设置默认的实时任务周期和运行时间，并注册相应的proc_handler以允许动态调整。",
          "similarity": 0.6142527461051941
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1776,
          "end_line": 1992,
          "content": [
            "static int pick_rt_task(struct rq *rq, struct task_struct *p, int cpu)",
            "{",
            "\tif (!task_on_cpu(rq, p) &&",
            "\t    cpumask_test_cpu(cpu, &p->cpus_mask))",
            "\t\treturn 1;",
            "",
            "\treturn 0;",
            "}",
            "static int find_lowest_rq(struct task_struct *task)",
            "{",
            "\tstruct sched_domain *sd;",
            "\tstruct cpumask *lowest_mask = this_cpu_cpumask_var_ptr(local_cpu_mask);",
            "\tint this_cpu = smp_processor_id();",
            "\tint cpu      = task_cpu(task);",
            "\tint ret;",
            "",
            "\t/* Make sure the mask is initialized first */",
            "\tif (unlikely(!lowest_mask))",
            "\t\treturn -1;",
            "",
            "\tif (task->nr_cpus_allowed == 1)",
            "\t\treturn -1; /* No other targets possible */",
            "",
            "\t/*",
            "\t * If we're on asym system ensure we consider the different capacities",
            "\t * of the CPUs when searching for the lowest_mask.",
            "\t */",
            "\tif (sched_asym_cpucap_active()) {",
            "",
            "\t\tret = cpupri_find_fitness(&task_rq(task)->rd->cpupri,",
            "\t\t\t\t\t  task, lowest_mask,",
            "\t\t\t\t\t  rt_task_fits_capacity);",
            "\t} else {",
            "",
            "\t\tret = cpupri_find(&task_rq(task)->rd->cpupri,",
            "\t\t\t\t  task, lowest_mask);",
            "\t}",
            "",
            "\tif (!ret)",
            "\t\treturn -1; /* No targets found */",
            "",
            "\t/*",
            "\t * At this point we have built a mask of CPUs representing the",
            "\t * lowest priority tasks in the system.  Now we want to elect",
            "\t * the best one based on our affinity and topology.",
            "\t *",
            "\t * We prioritize the last CPU that the task executed on since",
            "\t * it is most likely cache-hot in that location.",
            "\t */",
            "\tif (cpumask_test_cpu(cpu, lowest_mask))",
            "\t\treturn cpu;",
            "",
            "\t/*",
            "\t * Otherwise, we consult the sched_domains span maps to figure",
            "\t * out which CPU is logically closest to our hot cache data.",
            "\t */",
            "\tif (!cpumask_test_cpu(this_cpu, lowest_mask))",
            "\t\tthis_cpu = -1; /* Skip this_cpu opt if not among lowest */",
            "",
            "\trcu_read_lock();",
            "\tfor_each_domain(cpu, sd) {",
            "\t\tif (sd->flags & SD_WAKE_AFFINE) {",
            "\t\t\tint best_cpu;",
            "",
            "\t\t\t/*",
            "\t\t\t * \"this_cpu\" is cheaper to preempt than a",
            "\t\t\t * remote processor.",
            "\t\t\t */",
            "\t\t\tif (this_cpu != -1 &&",
            "\t\t\t    cpumask_test_cpu(this_cpu, sched_domain_span(sd))) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn this_cpu;",
            "\t\t\t}",
            "",
            "\t\t\tbest_cpu = cpumask_any_and_distribute(lowest_mask,",
            "\t\t\t\t\t\t\t      sched_domain_span(sd));",
            "\t\t\tif (best_cpu < nr_cpu_ids) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn best_cpu;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * And finally, if there were no matches within the domains",
            "\t * just give the caller *something* to work with from the compatible",
            "\t * locations.",
            "\t */",
            "\tif (this_cpu != -1)",
            "\t\treturn this_cpu;",
            "",
            "\tcpu = cpumask_any_distribute(lowest_mask);",
            "\tif (cpu < nr_cpu_ids)",
            "\t\treturn cpu;",
            "",
            "\treturn -1;",
            "}",
            "static int push_rt_task(struct rq *rq, bool pull)",
            "{",
            "\tstruct task_struct *next_task;",
            "\tstruct rq *lowest_rq;",
            "\tint ret = 0;",
            "",
            "\tif (!rq->rt.overloaded)",
            "\t\treturn 0;",
            "",
            "\tnext_task = pick_next_pushable_task(rq);",
            "\tif (!next_task)",
            "\t\treturn 0;",
            "",
            "retry:",
            "\t/*",
            "\t * It's possible that the next_task slipped in of",
            "\t * higher priority than current. If that's the case",
            "\t * just reschedule current.",
            "\t */",
            "\tif (unlikely(next_task->prio < rq->curr->prio)) {",
            "\t\tresched_curr(rq);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (is_migration_disabled(next_task)) {",
            "\t\tstruct task_struct *push_task = NULL;",
            "\t\tint cpu;",
            "",
            "\t\tif (!pull || rq->push_busy)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * Invoking find_lowest_rq() on anything but an RT task doesn't",
            "\t\t * make sense. Per the above priority check, curr has to",
            "\t\t * be of higher priority than next_task, so no need to",
            "\t\t * reschedule when bailing out.",
            "\t\t *",
            "\t\t * Note that the stoppers are masqueraded as SCHED_FIFO",
            "\t\t * (cf. sched_set_stop_task()), so we can't rely on rt_task().",
            "\t\t */",
            "\t\tif (rq->curr->sched_class != &rt_sched_class)",
            "\t\t\treturn 0;",
            "",
            "\t\tcpu = find_lowest_rq(rq->curr);",
            "\t\tif (cpu == -1 || cpu == rq->cpu)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * Given we found a CPU with lower priority than @next_task,",
            "\t\t * therefore it should be running. However we cannot migrate it",
            "\t\t * to this other CPU, instead attempt to push the current",
            "\t\t * running task on this CPU away.",
            "\t\t */",
            "\t\tpush_task = get_push_task(rq);",
            "\t\tif (push_task) {",
            "\t\t\tpreempt_disable();",
            "\t\t\traw_spin_rq_unlock(rq);",
            "\t\t\tstop_one_cpu_nowait(rq->cpu, push_cpu_stop,",
            "\t\t\t\t\t    push_task, &rq->push_work);",
            "\t\t\tpreempt_enable();",
            "\t\t\traw_spin_rq_lock(rq);",
            "\t\t}",
            "",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (WARN_ON(next_task == rq->curr))",
            "\t\treturn 0;",
            "",
            "\t/* We might release rq lock */",
            "\tget_task_struct(next_task);",
            "",
            "\t/* find_lock_lowest_rq locks the rq if found */",
            "\tlowest_rq = find_lock_lowest_rq(next_task, rq);",
            "\tif (!lowest_rq) {",
            "\t\tstruct task_struct *task;",
            "\t\t/*",
            "\t\t * find_lock_lowest_rq releases rq->lock",
            "\t\t * so it is possible that next_task has migrated.",
            "\t\t *",
            "\t\t * We need to make sure that the task is still on the same",
            "\t\t * run-queue and is also still the next task eligible for",
            "\t\t * pushing.",
            "\t\t */",
            "\t\ttask = pick_next_pushable_task(rq);",
            "\t\tif (task == next_task) {",
            "\t\t\t/*",
            "\t\t\t * The task hasn't migrated, and is still the next",
            "\t\t\t * eligible task, but we failed to find a run-queue",
            "\t\t\t * to push it to.  Do not retry in this case, since",
            "\t\t\t * other CPUs will pull from us when ready.",
            "\t\t\t */",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tif (!task)",
            "\t\t\t/* No more tasks, just exit */",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * Something has shifted, try again.",
            "\t\t */",
            "\t\tput_task_struct(next_task);",
            "\t\tnext_task = task;",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tdeactivate_task(rq, next_task, 0);",
            "\tset_task_cpu(next_task, lowest_rq->cpu);",
            "\tactivate_task(lowest_rq, next_task, 0);",
            "\tresched_curr(lowest_rq);",
            "\tret = 1;",
            "",
            "\tdouble_unlock_balance(rq, lowest_rq);",
            "out:",
            "\tput_task_struct(next_task);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "pick_rt_task, find_lowest_rq, push_rt_task",
          "description": "实现实时任务选择算法、低优先级CPU搜索及强制迁移逻辑，支持异构系统下的能效优化和拓扑感知调度。",
          "similarity": 0.6127806305885315
        }
      ]
    }
  ]
}