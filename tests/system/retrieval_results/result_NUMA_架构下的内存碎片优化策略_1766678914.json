{
  "query": "NUMA 架构下的内存碎片优化策略",
  "timestamp": "2025-12-26 00:08:34",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.6753936409950256,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/mempolicy.c",
          "start_line": 1,
          "end_line": 167,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Simple NUMA memory policy for the Linux kernel.",
            " *",
            " * Copyright 2003,2004 Andi Kleen, SuSE Labs.",
            " * (C) Copyright 2005 Christoph Lameter, Silicon Graphics, Inc.",
            " *",
            " * NUMA policy allows the user to give hints in which node(s) memory should",
            " * be allocated.",
            " *",
            " * Support four policies per VMA and per process:",
            " *",
            " * The VMA policy has priority over the process policy for a page fault.",
            " *",
            " * interleave     Allocate memory interleaved over a set of nodes,",
            " *                with normal fallback if it fails.",
            " *                For VMA based allocations this interleaves based on the",
            " *                offset into the backing object or offset into the mapping",
            " *                for anonymous memory. For process policy an process counter",
            " *                is used.",
            " *",
            " * weighted interleave",
            " *                Allocate memory interleaved over a set of nodes based on",
            " *                a set of weights (per-node), with normal fallback if it",
            " *                fails.  Otherwise operates the same as interleave.",
            " *                Example: nodeset(0,1) & weights (2,1) - 2 pages allocated",
            " *                on node 0 for every 1 page allocated on node 1.",
            " *",
            " * bind           Only allocate memory on a specific set of nodes,",
            " *                no fallback.",
            " *                FIXME: memory is allocated starting with the first node",
            " *                to the last. It would be better if bind would truly restrict",
            " *                the allocation to memory nodes instead",
            " *",
            " * preferred      Try a specific node first before normal fallback.",
            " *                As a special case NUMA_NO_NODE here means do the allocation",
            " *                on the local CPU. This is normally identical to default,",
            " *                but useful to set in a VMA when you have a non default",
            " *                process policy.",
            " *",
            " * preferred many Try a set of nodes first before normal fallback. This is",
            " *                similar to preferred without the special case.",
            " *",
            " * default        Allocate on the local node first, or when on a VMA",
            " *                use the process policy. This is what Linux always did",
            " *\t\t  in a NUMA aware kernel and still does by, ahem, default.",
            " *",
            " * The process policy is applied for most non interrupt memory allocations",
            " * in that process' context. Interrupts ignore the policies and always",
            " * try to allocate on the local CPU. The VMA policy is only applied for memory",
            " * allocations for a VMA in the VM.",
            " *",
            " * Currently there are a few corner cases in swapping where the policy",
            " * is not applied, but the majority should be handled. When process policy",
            " * is used it is not remembered over swap outs/swap ins.",
            " *",
            " * Only the highest zone in the zone hierarchy gets policied. Allocations",
            " * requesting a lower zone just use default policy. This implies that",
            " * on systems with highmem kernel lowmem allocation don't get policied.",
            " * Same with GFP_DMA allocations.",
            " *",
            " * For shmem/tmpfs shared memory the policy is shared between",
            " * all users and remembered even when nobody has memory mapped.",
            " */",
            "",
            "/* Notebook:",
            "   fix mmap readahead to honour policy and enable policy for any page cache",
            "   object",
            "   statistics for bigpages",
            "   global policy for page cache? currently it uses process policy. Requires",
            "   first item above.",
            "   handle mremap for shared memory (currently ignored for the policy)",
            "   grows down?",
            "   make bind policy root only? It can trigger oom much faster and the",
            "   kernel is not always grateful with that.",
            "*/",
            "",
            "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt",
            "",
            "#include <linux/mempolicy.h>",
            "#include <linux/pagewalk.h>",
            "#include <linux/highmem.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/kernel.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/slab.h>",
            "#include <linux/string.h>",
            "#include <linux/export.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/init.h>",
            "#include <linux/compat.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/swap.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/migrate.h>",
            "#include <linux/ksm.h>",
            "#include <linux/rmap.h>",
            "#include <linux/security.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/ctype.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/printk.h>",
            "#include <linux/swapops.h>",
            "#include <linux/gcd.h>",
            "",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/memory.h>",
            "",
            "#include \"internal.h\"",
            "",
            "/* Internal flags */",
            "#define MPOL_MF_DISCONTIG_OK (MPOL_MF_INTERNAL << 0)\t/* Skip checks for continuous vmas */",
            "#define MPOL_MF_INVERT       (MPOL_MF_INTERNAL << 1)\t/* Invert check for nodemask */",
            "#define MPOL_MF_WRLOCK       (MPOL_MF_INTERNAL << 2)\t/* Write-lock walked vmas */",
            "",
            "static struct kmem_cache *policy_cache;",
            "static struct kmem_cache *sn_cache;",
            "",
            "/* Highest zone. An specific allocation for a zone below that is not",
            "   policied. */",
            "enum zone_type policy_zone = 0;",
            "",
            "/*",
            " * run-time system-wide default policy => local allocation",
            " */",
            "static struct mempolicy default_policy = {",
            "\t.refcnt = ATOMIC_INIT(1), /* never free it */",
            "\t.mode = MPOL_LOCAL,",
            "};",
            "",
            "static struct mempolicy preferred_node_policy[MAX_NUMNODES];",
            "",
            "/*",
            " * weightiness balances the tradeoff between small weights (cycles through nodes",
            " * faster, more fair/even distribution) and large weights (smaller errors",
            " * between actual bandwidth ratios and weight ratios). 32 is a number that has",
            " * been found to perform at a reasonable compromise between the two goals.",
            " */",
            "static const int weightiness = 32;",
            "",
            "/*",
            " * A null weighted_interleave_state is interpreted as having .mode=\"auto\",",
            " * and .iw_table is interpreted as an array of 1s with length nr_node_ids.",
            " */",
            "struct weighted_interleave_state {",
            "\tbool mode_auto;",
            "\tu8 iw_table[];",
            "};",
            "static struct weighted_interleave_state __rcu *wi_state;",
            "static unsigned int *node_bw_table;",
            "",
            "/*",
            " * wi_state_lock protects both wi_state and node_bw_table.",
            " * node_bw_table is only used by writers to update wi_state.",
            " */",
            "static DEFINE_MUTEX(wi_state_lock);",
            ""
          ],
          "function_name": null,
          "description": "定义NUMA内存策略相关结构体和全局变量，包括默认策略default_policy、节点带宽表node_bw_table及加权交错策略状态wi_state，用于管理多节点内存分配策略。",
          "similarity": 0.6620398759841919
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.6459200382232666
        },
        {
          "chunk_id": 3,
          "file_path": "mm/mempolicy.c",
          "start_line": 484,
          "end_line": 639,
          "content": [
            "static void mpol_rebind_preferred(struct mempolicy *pol,",
            "\t\t\t\t\t\tconst nodemask_t *nodes)",
            "{",
            "\tpol->w.cpuset_mems_allowed = *nodes;",
            "}",
            "static void mpol_rebind_policy(struct mempolicy *pol, const nodemask_t *newmask)",
            "{",
            "\tif (!pol || pol->mode == MPOL_LOCAL)",
            "\t\treturn;",
            "\tif (!mpol_store_user_nodemask(pol) &&",
            "\t    nodes_equal(pol->w.cpuset_mems_allowed, *newmask))",
            "\t\treturn;",
            "",
            "\tmpol_ops[pol->mode].rebind(pol, newmask);",
            "}",
            "void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new)",
            "{",
            "\tmpol_rebind_policy(tsk->mempolicy, new);",
            "}",
            "void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "",
            "\tmmap_write_lock(mm);",
            "\tfor_each_vma(vmi, vma) {",
            "\t\tvma_start_write(vma);",
            "\t\tmpol_rebind_policy(vma->vm_policy, new);",
            "\t}",
            "\tmmap_write_unlock(mm);",
            "}",
            "static bool strictly_unmovable(unsigned long flags)",
            "{",
            "\t/*",
            "\t * STRICT without MOVE flags lets do_mbind() fail immediately with -EIO",
            "\t * if any misplaced page is found.",
            "\t */",
            "\treturn (flags & (MPOL_MF_STRICT | MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ==",
            "\t\t\t MPOL_MF_STRICT;",
            "}",
            "static inline bool queue_folio_required(struct folio *folio,",
            "\t\t\t\t\tstruct queue_pages *qp)",
            "{",
            "\tint nid = folio_nid(folio);",
            "\tunsigned long flags = qp->flags;",
            "",
            "\treturn node_isset(nid, *qp->nmask) == !(flags & MPOL_MF_INVERT);",
            "}",
            "static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)",
            "{",
            "\tstruct folio *folio;",
            "\tstruct queue_pages *qp = walk->private;",
            "",
            "\tif (unlikely(is_pmd_migration_entry(*pmd))) {",
            "\t\tqp->nr_failed++;",
            "\t\treturn;",
            "\t}",
            "\tfolio = pmd_folio(*pmd);",
            "\tif (is_huge_zero_folio(folio)) {",
            "\t\twalk->action = ACTION_CONTINUE;",
            "\t\treturn;",
            "\t}",
            "\tif (!queue_folio_required(folio, qp))",
            "\t\treturn;",
            "\tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||",
            "\t    !vma_migratable(walk->vma) ||",
            "\t    !migrate_folio_add(folio, qp->pagelist, qp->flags))",
            "\t\tqp->nr_failed++;",
            "}",
            "static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,",
            "\t\t\tunsigned long end, struct mm_walk *walk)",
            "{",
            "\tconst fpb_t fpb_flags = FPB_IGNORE_DIRTY | FPB_IGNORE_SOFT_DIRTY;",
            "\tstruct vm_area_struct *vma = walk->vma;",
            "\tstruct folio *folio;",
            "\tstruct queue_pages *qp = walk->private;",
            "\tunsigned long flags = qp->flags;",
            "\tpte_t *pte, *mapped_pte;",
            "\tpte_t ptent;",
            "\tspinlock_t *ptl;",
            "\tint max_nr, nr;",
            "",
            "\tptl = pmd_trans_huge_lock(pmd, vma);",
            "\tif (ptl) {",
            "\t\tqueue_folios_pmd(pmd, walk);",
            "\t\tspin_unlock(ptl);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tmapped_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);",
            "\tif (!pte) {",
            "\t\twalk->action = ACTION_AGAIN;",
            "\t\treturn 0;",
            "\t}",
            "\tfor (; addr != end; pte += nr, addr += nr * PAGE_SIZE) {",
            "\t\tmax_nr = (end - addr) >> PAGE_SHIFT;",
            "\t\tnr = 1;",
            "\t\tptent = ptep_get(pte);",
            "\t\tif (pte_none(ptent))",
            "\t\t\tcontinue;",
            "\t\tif (!pte_present(ptent)) {",
            "\t\t\tif (is_migration_entry(pte_to_swp_entry(ptent)))",
            "\t\t\t\tqp->nr_failed++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfolio = vm_normal_folio(vma, addr, ptent);",
            "\t\tif (!folio || folio_is_zone_device(folio))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_large(folio) && max_nr != 1)",
            "\t\t\tnr = folio_pte_batch(folio, addr, pte, ptent,",
            "\t\t\t\t\t     max_nr, fpb_flags,",
            "\t\t\t\t\t     NULL, NULL, NULL);",
            "\t\t/*",
            "\t\t * vm_normal_folio() filters out zero pages, but there might",
            "\t\t * still be reserved folios to skip, perhaps in a VDSO.",
            "\t\t */",
            "\t\tif (folio_test_reserved(folio))",
            "\t\t\tcontinue;",
            "\t\tif (!queue_folio_required(folio, qp))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_large(folio)) {",
            "\t\t\t/*",
            "\t\t\t * A large folio can only be isolated from LRU once,",
            "\t\t\t * but may be mapped by many PTEs (and Copy-On-Write may",
            "\t\t\t * intersperse PTEs of other, order 0, folios).  This is",
            "\t\t\t * a common case, so don't mistake it for failure (but",
            "\t\t\t * there can be other cases of multi-mapped pages which",
            "\t\t\t * this quick check does not help to filter out - and a",
            "\t\t\t * search of the pagelist might grow to be prohibitive).",
            "\t\t\t *",
            "\t\t\t * migrate_pages(&pagelist) returns nr_failed folios, so",
            "\t\t\t * check \"large\" now so that queue_pages_range() returns",
            "\t\t\t * a comparable nr_failed folios.  This does imply that",
            "\t\t\t * if folio could not be isolated for some racy reason",
            "\t\t\t * at its first PTE, later PTEs will not give it another",
            "\t\t\t * chance of isolation; but keeps the accounting simple.",
            "\t\t\t */",
            "\t\t\tif (folio == qp->large)",
            "\t\t\t\tcontinue;",
            "\t\t\tqp->large = folio;",
            "\t\t}",
            "\t\tif (!(flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||",
            "\t\t    !vma_migratable(vma) ||",
            "\t\t    !migrate_folio_add(folio, qp->pagelist, flags)) {",
            "\t\t\tqp->nr_failed += nr;",
            "\t\t\tif (strictly_unmovable(flags))",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tpte_unmap_unlock(mapped_pte, ptl);",
            "\tcond_resched();",
            "out:",
            "\tif (qp->nr_failed && strictly_unmovable(flags))",
            "\t\treturn -EIO;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "mpol_rebind_preferred, mpol_rebind_policy, mpol_rebind_task, mpol_rebind_mm, strictly_unmovable, queue_folio_required, queue_folios_pmd, queue_folios_pte_range",
          "description": "处理页表项遍历与迁移操作，包含页帧队列检测、迁移标志验证及大页迁移逻辑，确保内存分配符合NUMA策略要求。",
          "similarity": 0.6358882188796997
        },
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.6239728927612305
        },
        {
          "chunk_id": 13,
          "file_path": "mm/mempolicy.c",
          "start_line": 2149,
          "end_line": 2255,
          "content": [
            "static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nnodes;",
            "\tint i;",
            "\tint nid;",
            "",
            "\tnnodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nnodes)",
            "\t\treturn numa_node_id();",
            "\ttarget = ilx % nnodes;",
            "\tnid = first_node(nodemask);",
            "\tfor (i = 0; i < target; i++)",
            "\t\tnid = next_node(nid, nodemask);",
            "\treturn nid;",
            "}",
            "int huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,",
            "\t\tstruct mempolicy **mpol, nodemask_t **nodemask)",
            "{",
            "\tpgoff_t ilx;",
            "\tint nid;",
            "",
            "\tnid = numa_node_id();",
            "\t*mpol = get_vma_policy(vma, addr, hstate_vma(vma)->order, &ilx);",
            "\t*nodemask = policy_nodemask(gfp_flags, *mpol, ilx, &nid);",
            "\treturn nid;",
            "}",
            "bool init_nodemask_of_mempolicy(nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "",
            "\tif (!(mask && current->mempolicy))",
            "\t\treturn false;",
            "",
            "\ttask_lock(current);",
            "\tmempolicy = current->mempolicy;",
            "\tswitch (mempolicy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*mask = mempolicy->nodes;",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tinit_nodemask_of_node(mask, numa_node_id());",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "\ttask_unlock(current);",
            "",
            "\treturn true;",
            "}",
            "bool mempolicy_in_oom_domain(struct task_struct *tsk,",
            "\t\t\t\t\tconst nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "\tbool ret = true;",
            "",
            "\tif (!mask)",
            "\t\treturn ret;",
            "",
            "\ttask_lock(tsk);",
            "\tmempolicy = tsk->mempolicy;",
            "\tif (mempolicy && mempolicy->mode == MPOL_BIND)",
            "\t\tret = nodes_intersects(mempolicy->nodes, *mask);",
            "\ttask_unlock(tsk);",
            "",
            "\treturn ret;",
            "}",
            "static unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tint nodes;",
            "\tunsigned long nr_pages_per_node;",
            "\tint delta;",
            "\tint i;",
            "\tunsigned long nr_allocated;",
            "\tunsigned long total_allocated = 0;",
            "",
            "\tnodes = nodes_weight(pol->nodes);",
            "\tnr_pages_per_node = nr_pages / nodes;",
            "\tdelta = nr_pages - nodes * nr_pages_per_node;",
            "",
            "\tfor (i = 0; i < nodes; i++) {",
            "\t\tif (delta) {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node + 1, NULL,",
            "\t\t\t\t\tpage_array);",
            "\t\t\tdelta--;",
            "\t\t} else {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node, NULL, page_array);",
            "\t\t}",
            "",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t}",
            "",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "interleave_nid, huge_node, init_nodemask_of_mempolicy, mempolicy_in_oom_domain, alloc_pages_bulk_array_interleave",
          "description": "interleave_nid 计算简单交错分配的目标节点；huge_node 结合HugeTLB策略确定大页分配节点；init_nodemask_of_mempolicy 初始化当前进程的内存策略节点掩码；mempolicy_in_oom_domain 检查策略节点是否与OOM域重叠；alloc_pages_bulk_array_interleave 执行批量交错分配。",
          "similarity": 0.5981055498123169
        }
      ]
    },
    {
      "source_file": "mm/oom_kill.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:58:11\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `oom_kill.c`\n\n---\n\n# oom_kill.c 技术文档\n\n## 1. 文件概述\n\n`oom_kill.c` 是 Linux 内核内存管理子系统中的关键组件，负责在系统严重内存不足（Out-Of-Memory, OOM）时选择并终止一个或多个进程，以释放内存资源、防止系统崩溃。该文件实现了 OOM Killer 的核心逻辑，包括候选进程的选择策略、内存压力评估、以及与内存控制组（memcg）、NUMA 策略、cpuset 等子系统的集成。OOM Killer 通常由 `__alloc_pages()` 在无法满足内存分配请求时触发。\n\n## 2. 核心功能\n\n### 主要函数\n- **`out_of_memory()`**：OOM Killer 的主入口函数（虽未在片段中完整显示，但为本文件核心）\n- **`oom_badness()`**：计算进程“坏度”（badness）分数的核心启发式函数，用于决定哪个进程最应被杀死\n- **`find_lock_task_mm()`**：在进程及其线程组中查找具有有效内存描述符（`mm_struct`）的可杀任务，并加锁\n- **`oom_unkillable_task()`**：判断某任务是否不可被 OOM Killer 杀死（如 init 进程、内核线程）\n- **`constrained_alloc()`**：确定当前内存分配所受的约束类型（如 memcg、cpuset、mempolicy）\n- **`oom_cpuset_eligible()`**（仅 CONFIG_NUMA）：在 NUMA 系统中检查任务是否符合 cpuset 或 mempolicy 的 OOM 杀死条件\n- **`should_dump_unreclaim_slab()`**：判断是否因不可回收 slab 内存过多而触发 OOM，用于辅助诊断\n\n### 关键数据结构\n- **`struct oom_control`**：封装 OOM 事件上下文，包括分配标志（`gfp_mask`）、节点掩码（`nodemask`）、内存控制组（`memcg`）、分配阶数（`order`）等\n- **`enum oom_constraint`**：表示内存分配受限的类型（`CONSTRAINT_NONE`、`CONSTRAINT_CPUSET`、`CONSTRAINT_MEMORY_POLICY`、`CONSTRAINT_MEMCG`）\n\n### 全局变量\n- **`sysctl_panic_on_oom`**：控制 OOM 时是否直接 panic\n- **`sysctl_oom_kill_allocating_task`**：若置位，则优先杀死触发 OOM 的进程\n- **`sysctl_oom_dump_tasks`**：控制 OOM 时是否打印所有任务的内存使用信息\n- **`oom_lock`**：互斥锁，序列化 OOM Killer 调用，防止并发过度杀进程\n- **`oom_adj_mutex`**：互斥锁，保护 `oom_score_adj` 和 `oom_score_adj_min` 的更新\n\n## 3. 关键实现\n\n### OOM 坏度评分算法 (`oom_badness`)\n- **基础分值**：基于进程的 RSS（Resident Set Size）、交换页数量（`MM_SWAPENTS`）和页表占用内存（`mm_pgtables_bytes`），单位为页数。\n- **调整因子**：通过 `oom_score_adj`（范围 [-1000, 1000]）进行线性调整。调整量 = `oom_score_adj * totalpages / 1000`，其中 `totalpages` 为当前 OOM 上下文允许的最大内存页数（全局或 memcg 限制）。\n- **排除规则**：\n  - 全局 init 进程（PID 1）和内核线程（`PF_KTHREAD`）不可杀。\n  - 显式设置 `oom_score_adj = OOM_SCORE_ADJ_MIN (-1000)` 的进程不可杀。\n  - 已被标记跳过（`MMF_OOM_SKIP`）或处于 `vfork` 中间状态的进程不可杀。\n- **返回值**：`LONG_MIN` 表示不可杀；否则返回综合评分，值越大越优先被杀。\n\n### 内存分配约束识别 (`constrained_alloc`)\n- **Memcg OOM**：若 `oc->memcg` 非空，则 `totalpages` 设为 memcg 的内存上限，约束类型为 `CONSTRAINT_MEMCG`。\n- **全局 OOM**：默认 `totalpages = totalram_pages + total_swap_pages`。\n- **NUMA 约束**：\n  - 若分配请求指定 `__GFP_THISNODE`，视为无特殊约束（避免杀死当前进程）。\n  - 若存在非全集的 `nodemask`（来自 mempolicy），则 `totalpages` 仅统计该 nodemask 覆盖节点的内存，约束类型为 `CONSTRAINT_MEMORY_POLICY`。\n  - Cpuset 约束由页面分配器处理，此处不直接计算。\n\n### 多线程与内存描述符处理 (`find_lock_task_mm`)\n- 遍历目标进程的整个线程组（`for_each_thread`），寻找任一仍持有有效 `mm_struct` 的线程。\n- 对找到的线程加 `task_lock` 并返回，确保在检查其内存状态时不会被释放。\n- 适用于主线程已退出但子线程仍在运行的场景。\n\n### NUMA 可杀性检查 (`oom_cpuset_eligible`)\n- 在 NUMA 系统中，仅当候选任务与触发 OOM 的当前任务在内存策略（mempolicy）或 cpuset 允许的节点集上有交集时，才视为可杀。\n- 若 OOM 由 mempolicy 触发（`oc->nodemask` 非空），则仅检查 mempolicy 交集。\n- 否则，检查 cpuset 的 `mems_allowed` 交集。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/gfp.h>`、`<linux/swap.h>` 获取内存状态、分配标志和交换信息。\n- **进程调度与管理**：依赖 `<linux/sched.h>` 及相关头文件访问任务结构、线程组、cpuset 和内存策略。\n- **内存控制组 (cgroup v2)**：通过 `<linux/memcontrol.h>` 集成 memcg，支持容器级 OOM。\n- **安全模块**：通过 `<linux/security.h>` 调用 LSM 钩子（如 `security_oom_kill()`）。\n- **调试与追踪**：使用 ftrace (`<linux/ftrace.h>`) 和自定义 tracepoint (`trace/events/oom.h`) 记录 OOM 事件。\n- **体系结构相关**：包含 `<asm/tlb.h>` 处理 TLB 刷新。\n- **内部 MM 实现**：包含 `\"internal.h\"` 和 `\"slab.h\"` 访问内核私有内存管理接口。\n\n## 5. 使用场景\n\n- **全局内存耗尽**：当系统整体可用内存（含 swap）低于临界阈值，且无法通过页面回收释放足够内存时，由页面分配器调用 `out_of_memory()`。\n- **Memcg 内存超限**：当某个 memory cgroup 的内存使用超过其配额时，触发该 cgroup 内的 OOM Killer。\n- **SysRq 触发**：通过 Magic SysRq 键（`Alt+SysRq+f`）手动触发 OOM Killer，此时 `oc->order = -1`。\n- **诊断辅助**：当不可回收 slab 内存（如内核对象缓存）异常增长导致 OOM 时，`should_dump_unreclaim_slab()` 可触发 slab 信息转储以辅助调试。\n- **策略约束下的 OOM**：在 NUMA 系统中，受 cpuset 或 mempolicy 限制的进程在局部节点内存耗尽时触发针对性 OOM。",
      "similarity": 0.6261012554168701,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/oom_kill.c",
          "start_line": 253,
          "end_line": 359,
          "content": [
            "static enum oom_constraint constrained_alloc(struct oom_control *oc)",
            "{",
            "\tstruct zone *zone;",
            "\tstruct zoneref *z;",
            "\tenum zone_type highest_zoneidx = gfp_zone(oc->gfp_mask);",
            "\tbool cpuset_limited = false;",
            "\tint nid;",
            "",
            "\tif (is_memcg_oom(oc)) {",
            "\t\toc->totalpages = mem_cgroup_get_max(oc->memcg) ?: 1;",
            "\t\treturn CONSTRAINT_MEMCG;",
            "\t}",
            "",
            "\t/* Default to all available memory */",
            "\toc->totalpages = totalram_pages() + total_swap_pages;",
            "",
            "\tif (!IS_ENABLED(CONFIG_NUMA))",
            "\t\treturn CONSTRAINT_NONE;",
            "",
            "\tif (!oc->zonelist)",
            "\t\treturn CONSTRAINT_NONE;",
            "\t/*",
            "\t * Reach here only when __GFP_NOFAIL is used. So, we should avoid",
            "\t * to kill current.We have to random task kill in this case.",
            "\t * Hopefully, CONSTRAINT_THISNODE...but no way to handle it, now.",
            "\t */",
            "\tif (oc->gfp_mask & __GFP_THISNODE)",
            "\t\treturn CONSTRAINT_NONE;",
            "",
            "\t/*",
            "\t * This is not a __GFP_THISNODE allocation, so a truncated nodemask in",
            "\t * the page allocator means a mempolicy is in effect.  Cpuset policy",
            "\t * is enforced in get_page_from_freelist().",
            "\t */",
            "\tif (oc->nodemask &&",
            "\t    !nodes_subset(node_states[N_MEMORY], *oc->nodemask)) {",
            "\t\toc->totalpages = total_swap_pages;",
            "\t\tfor_each_node_mask(nid, *oc->nodemask)",
            "\t\t\toc->totalpages += node_present_pages(nid);",
            "\t\treturn CONSTRAINT_MEMORY_POLICY;",
            "\t}",
            "",
            "\t/* Check this allocation failure is caused by cpuset's wall function */",
            "\tfor_each_zone_zonelist_nodemask(zone, z, oc->zonelist,",
            "\t\t\thighest_zoneidx, oc->nodemask)",
            "\t\tif (!cpuset_zone_allowed(zone, oc->gfp_mask))",
            "\t\t\tcpuset_limited = true;",
            "",
            "\tif (cpuset_limited) {",
            "\t\toc->totalpages = total_swap_pages;",
            "\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)",
            "\t\t\toc->totalpages += node_present_pages(nid);",
            "\t\treturn CONSTRAINT_CPUSET;",
            "\t}",
            "\treturn CONSTRAINT_NONE;",
            "}",
            "static int oom_evaluate_task(struct task_struct *task, void *arg)",
            "{",
            "\tstruct oom_control *oc = arg;",
            "\tlong points;",
            "",
            "\tif (oom_unkillable_task(task))",
            "\t\tgoto next;",
            "",
            "\t/* p may not have freeable memory in nodemask */",
            "\tif (!is_memcg_oom(oc) && !oom_cpuset_eligible(task, oc))",
            "\t\tgoto next;",
            "",
            "\t/*",
            "\t * This task already has access to memory reserves and is being killed.",
            "\t * Don't allow any other task to have access to the reserves unless",
            "\t * the task has MMF_OOM_SKIP because chances that it would release",
            "\t * any memory is quite low.",
            "\t */",
            "\tif (!is_sysrq_oom(oc) && tsk_is_oom_victim(task)) {",
            "\t\tif (test_bit(MMF_OOM_SKIP, &task->signal->oom_mm->flags))",
            "\t\t\tgoto next;",
            "\t\tgoto abort;",
            "\t}",
            "",
            "\t/*",
            "\t * If task is allocating a lot of memory and has been marked to be",
            "\t * killed first if it triggers an oom, then select it.",
            "\t */",
            "\tif (oom_task_origin(task)) {",
            "\t\tpoints = LONG_MAX;",
            "\t\tgoto select;",
            "\t}",
            "",
            "\tpoints = oom_badness(task, oc->totalpages);",
            "\tif (points == LONG_MIN || points < oc->chosen_points)",
            "\t\tgoto next;",
            "",
            "select:",
            "\tif (oc->chosen)",
            "\t\tput_task_struct(oc->chosen);",
            "\tget_task_struct(task);",
            "\toc->chosen = task;",
            "\toc->chosen_points = points;",
            "next:",
            "\treturn 0;",
            "abort:",
            "\tif (oc->chosen)",
            "\t\tput_task_struct(oc->chosen);",
            "\toc->chosen = (void *)-1UL;",
            "\treturn 1;",
            "}"
          ],
          "function_name": "constrained_alloc, oom_evaluate_task",
          "description": "根据内存分配约束条件（如NUMA节点、内存组）动态调整可回收内存总量，并通过评估函数筛选出具有最高OOM不良值的任务作为潜在受害者。",
          "similarity": 0.6904187202453613
        },
        {
          "chunk_id": 3,
          "file_path": "mm/oom_kill.c",
          "start_line": 366,
          "end_line": 466,
          "content": [
            "static void select_bad_process(struct oom_control *oc)",
            "{",
            "\toc->chosen_points = LONG_MIN;",
            "",
            "\tif (is_memcg_oom(oc))",
            "\t\tmem_cgroup_scan_tasks(oc->memcg, oom_evaluate_task, oc);",
            "\telse {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\trcu_read_lock();",
            "\t\tfor_each_process(p)",
            "\t\t\tif (oom_evaluate_task(p, oc))",
            "\t\t\t\tbreak;",
            "\t\trcu_read_unlock();",
            "\t}",
            "}",
            "static int dump_task(struct task_struct *p, void *arg)",
            "{",
            "\tstruct oom_control *oc = arg;",
            "\tstruct task_struct *task;",
            "",
            "\tif (oom_unkillable_task(p))",
            "\t\treturn 0;",
            "",
            "\t/* p may not have freeable memory in nodemask */",
            "\tif (!is_memcg_oom(oc) && !oom_cpuset_eligible(p, oc))",
            "\t\treturn 0;",
            "",
            "\ttask = find_lock_task_mm(p);",
            "\tif (!task) {",
            "\t\t/*",
            "\t\t * All of p's threads have already detached their mm's. There's",
            "\t\t * no need to report them; they can't be oom killed anyway.",
            "\t\t */",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tpr_info(\"[%7d] %5d %5d %8lu %8lu %8lu %8lu %9lu %8ld %8lu         %5hd %s\\n\",",
            "\t\ttask->pid, from_kuid(&init_user_ns, task_uid(task)),",
            "\t\ttask->tgid, task->mm->total_vm, get_mm_rss(task->mm),",
            "\t\tget_mm_counter(task->mm, MM_ANONPAGES), get_mm_counter(task->mm, MM_FILEPAGES),",
            "\t\tget_mm_counter(task->mm, MM_SHMEMPAGES), mm_pgtables_bytes(task->mm),",
            "\t\tget_mm_counter(task->mm, MM_SWAPENTS),",
            "\t\ttask->signal->oom_score_adj, task->comm);",
            "\ttask_unlock(task);",
            "",
            "\treturn 0;",
            "}",
            "static void dump_tasks(struct oom_control *oc)",
            "{",
            "\tpr_info(\"Tasks state (memory values in pages):\\n\");",
            "\tpr_info(\"[  pid  ]   uid  tgid total_vm      rss rss_anon rss_file rss_shmem pgtables_bytes swapents oom_score_adj name\\n\");",
            "",
            "\tif (is_memcg_oom(oc))",
            "\t\tmem_cgroup_scan_tasks(oc->memcg, dump_task, oc);",
            "\telse {",
            "\t\tstruct task_struct *p;",
            "\t\tint i = 0;",
            "",
            "\t\trcu_read_lock();",
            "\t\tfor_each_process(p) {",
            "\t\t\t/* Avoid potential softlockup warning */",
            "\t\t\tif ((++i & 1023) == 0)",
            "\t\t\t\ttouch_softlockup_watchdog();",
            "\t\t\tdump_task(p, oc);",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t}",
            "}",
            "static void dump_oom_summary(struct oom_control *oc, struct task_struct *victim)",
            "{",
            "\t/* one line summary of the oom killer context. */",
            "\tpr_info(\"oom-kill:constraint=%s,nodemask=%*pbl\",",
            "\t\t\toom_constraint_text[oc->constraint],",
            "\t\t\tnodemask_pr_args(oc->nodemask));",
            "\tcpuset_print_current_mems_allowed();",
            "\tmem_cgroup_print_oom_context(oc->memcg, victim);",
            "\tpr_cont(\",task=%s,pid=%d,uid=%d\\n\", victim->comm, victim->pid,",
            "\t\tfrom_kuid(&init_user_ns, task_uid(victim)));",
            "}",
            "static void dump_header(struct oom_control *oc, struct task_struct *p)",
            "{",
            "\tpr_warn(\"%s invoked oom-killer: gfp_mask=%#x(%pGg), order=%d, oom_score_adj=%hd\\n\",",
            "\t\tcurrent->comm, oc->gfp_mask, &oc->gfp_mask, oc->order,",
            "\t\t\tcurrent->signal->oom_score_adj);",
            "\tif (!IS_ENABLED(CONFIG_COMPACTION) && oc->order)",
            "\t\tpr_warn(\"COMPACTION is disabled!!!\\n\");",
            "",
            "\tdump_stack();",
            "\tif (is_memcg_oom(oc))",
            "\t\tmem_cgroup_print_oom_meminfo(oc->memcg);",
            "\telse {",
            "\t\t__show_mem(SHOW_MEM_FILTER_NODES, oc->nodemask, gfp_zone(oc->gfp_mask));",
            "\t\tif (should_dump_unreclaim_slab())",
            "\t\t\tdump_unreclaimable_slab();",
            "\t}",
            "\tif (sysctl_oom_dump_tasks)",
            "\t\tdump_tasks(oc);",
            "\tif (p)",
            "\t\tdump_oom_summary(oc, p);",
            "}"
          ],
          "function_name": "select_bad_process, dump_task, dump_tasks, dump_oom_summary, dump_header",
          "description": "遍历所有进程选择最优OOM目标，输出详细的进程内存状态信息，并生成OOM事件的上下文摘要报告。",
          "similarity": 0.5868927240371704
        },
        {
          "chunk_id": 7,
          "file_path": "mm/oom_kill.c",
          "start_line": 1011,
          "end_line": 1162,
          "content": [
            "static int oom_kill_memcg_member(struct task_struct *task, void *message)",
            "{",
            "\tif (task->signal->oom_score_adj != OOM_SCORE_ADJ_MIN &&",
            "\t    !is_global_init(task)) {",
            "\t\tget_task_struct(task);",
            "\t\t__oom_kill_process(task, message);",
            "\t}",
            "\treturn 0;",
            "}",
            "static void oom_kill_process(struct oom_control *oc, const char *message)",
            "{",
            "\tstruct task_struct *victim = oc->chosen;",
            "\tstruct mem_cgroup *oom_group;",
            "\tstatic DEFINE_RATELIMIT_STATE(oom_rs, DEFAULT_RATELIMIT_INTERVAL,",
            "\t\t\t\t\t      DEFAULT_RATELIMIT_BURST);",
            "",
            "\t/*",
            "\t * If the task is already exiting, don't alarm the sysadmin or kill",
            "\t * its children or threads, just give it access to memory reserves",
            "\t * so it can die quickly",
            "\t */",
            "\ttask_lock(victim);",
            "\tif (task_will_free_mem(victim)) {",
            "\t\tmark_oom_victim(victim);",
            "\t\tqueue_oom_reaper(victim);",
            "\t\ttask_unlock(victim);",
            "\t\tput_task_struct(victim);",
            "\t\treturn;",
            "\t}",
            "\ttask_unlock(victim);",
            "",
            "\tif (__ratelimit(&oom_rs))",
            "\t\tdump_header(oc, victim);",
            "",
            "\t/*",
            "\t * Do we need to kill the entire memory cgroup?",
            "\t * Or even one of the ancestor memory cgroups?",
            "\t * Check this out before killing the victim task.",
            "\t */",
            "\toom_group = mem_cgroup_get_oom_group(victim, oc->memcg);",
            "",
            "\t__oom_kill_process(victim, message);",
            "",
            "\t/*",
            "\t * If necessary, kill all tasks in the selected memory cgroup.",
            "\t */",
            "\tif (oom_group) {",
            "\t\tmemcg_memory_event(oom_group, MEMCG_OOM_GROUP_KILL);",
            "\t\tmem_cgroup_print_oom_group(oom_group);",
            "\t\tmem_cgroup_scan_tasks(oom_group, oom_kill_memcg_member,",
            "\t\t\t\t      (void *)message);",
            "\t\tmem_cgroup_put(oom_group);",
            "\t}",
            "}",
            "static void check_panic_on_oom(struct oom_control *oc)",
            "{",
            "\tif (likely(!sysctl_panic_on_oom))",
            "\t\treturn;",
            "\tif (sysctl_panic_on_oom != 2) {",
            "\t\t/*",
            "\t\t * panic_on_oom == 1 only affects CONSTRAINT_NONE, the kernel",
            "\t\t * does not panic for cpuset, mempolicy, or memcg allocation",
            "\t\t * failures.",
            "\t\t */",
            "\t\tif (oc->constraint != CONSTRAINT_NONE)",
            "\t\t\treturn;",
            "\t}",
            "\t/* Do not panic for oom kills triggered by sysrq */",
            "\tif (is_sysrq_oom(oc))",
            "\t\treturn;",
            "\tdump_header(oc, NULL);",
            "\tpanic(\"Out of memory: %s panic_on_oom is enabled\\n\",",
            "\t\tsysctl_panic_on_oom == 2 ? \"compulsory\" : \"system-wide\");",
            "}",
            "int register_oom_notifier(struct notifier_block *nb)",
            "{",
            "\treturn blocking_notifier_chain_register(&oom_notify_list, nb);",
            "}",
            "int unregister_oom_notifier(struct notifier_block *nb)",
            "{",
            "\treturn blocking_notifier_chain_unregister(&oom_notify_list, nb);",
            "}",
            "bool out_of_memory(struct oom_control *oc)",
            "{",
            "\tunsigned long freed = 0;",
            "",
            "\tif (oom_killer_disabled)",
            "\t\treturn false;",
            "",
            "\tif (!is_memcg_oom(oc)) {",
            "\t\tblocking_notifier_call_chain(&oom_notify_list, 0, &freed);",
            "\t\tif (freed > 0 && !is_sysrq_oom(oc))",
            "\t\t\t/* Got some memory back in the last second. */",
            "\t\t\treturn true;",
            "\t}",
            "",
            "\t/*",
            "\t * If current has a pending SIGKILL or is exiting, then automatically",
            "\t * select it.  The goal is to allow it to allocate so that it may",
            "\t * quickly exit and free its memory.",
            "\t */",
            "\tif (task_will_free_mem(current)) {",
            "\t\tmark_oom_victim(current);",
            "\t\tqueue_oom_reaper(current);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/*",
            "\t * The OOM killer does not compensate for IO-less reclaim.",
            "\t * But mem_cgroup_oom() has to invoke the OOM killer even",
            "\t * if it is a GFP_NOFS allocation.",
            "\t */",
            "\tif (!(oc->gfp_mask & __GFP_FS) && !is_memcg_oom(oc))",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Check if there were limitations on the allocation (only relevant for",
            "\t * NUMA and memcg) that may require different handling.",
            "\t */",
            "\toc->constraint = constrained_alloc(oc);",
            "\tif (oc->constraint != CONSTRAINT_MEMORY_POLICY)",
            "\t\toc->nodemask = NULL;",
            "\tcheck_panic_on_oom(oc);",
            "",
            "\tif (!is_memcg_oom(oc) && sysctl_oom_kill_allocating_task &&",
            "\t    current->mm && !oom_unkillable_task(current) &&",
            "\t    oom_cpuset_eligible(current, oc) &&",
            "\t    current->signal->oom_score_adj != OOM_SCORE_ADJ_MIN) {",
            "\t\tget_task_struct(current);",
            "\t\toc->chosen = current;",
            "\t\toom_kill_process(oc, \"Out of memory (oom_kill_allocating_task)\");",
            "\t\treturn true;",
            "\t}",
            "",
            "\tselect_bad_process(oc);",
            "\t/* Found nothing?!?! */",
            "\tif (!oc->chosen) {",
            "\t\tdump_header(oc, NULL);",
            "\t\tpr_warn(\"Out of memory and no killable processes...\\n\");",
            "\t\t/*",
            "\t\t * If we got here due to an actual allocation at the",
            "\t\t * system level, we cannot survive this and will enter",
            "\t\t * an endless loop in the allocator. Bail out now.",
            "\t\t */",
            "\t\tif (!is_sysrq_oom(oc) && !is_memcg_oom(oc))",
            "\t\t\tpanic(\"System is deadlocked on memory\\n\");",
            "\t}",
            "\tif (oc->chosen && oc->chosen != (void *)-1UL)",
            "\t\toom_kill_process(oc, !is_memcg_oom(oc) ? \"Out of memory\" :",
            "\t\t\t\t \"Memory cgroup out of memory\");",
            "\treturn !!oc->chosen;",
            "}"
          ],
          "function_name": "oom_kill_memcg_member, oom_kill_process, check_panic_on_oom, register_oom_notifier, unregister_oom_notifier, out_of_memory",
          "description": "实现基于内存控制组的OOM处理逻辑，集成OOM通知机制，包含OOM触发判定、进程选择算法、内存组遍历杀进程等功能，支持系统级OOM恐慌检测",
          "similarity": 0.5840144753456116
        },
        {
          "chunk_id": 8,
          "file_path": "mm/oom_kill.c",
          "start_line": 1191,
          "end_line": 1204,
          "content": [
            "void pagefault_out_of_memory(void)",
            "{",
            "\tstatic DEFINE_RATELIMIT_STATE(pfoom_rs, DEFAULT_RATELIMIT_INTERVAL,",
            "\t\t\t\t      DEFAULT_RATELIMIT_BURST);",
            "",
            "\tif (mem_cgroup_oom_synchronize(true))",
            "\t\treturn;",
            "",
            "\tif (fatal_signal_pending(current))",
            "\t\treturn;",
            "",
            "\tif (__ratelimit(&pfoom_rs))",
            "\t\tpr_warn(\"Huh VM_FAULT_OOM leaked out to the #PF handler. Retrying PF\\n\");",
            "}"
          ],
          "function_name": "pagefault_out_of_memory",
          "description": "处理页面故障引发的OOM场景，通过速率限制控制日志输出，同步内存控制组状态以防止无限循环分配，避免对已挂起的进程重复触发OOM处理",
          "similarity": 0.5721372365951538
        },
        {
          "chunk_id": 4,
          "file_path": "mm/oom_kill.c",
          "start_line": 496,
          "end_line": 624,
          "content": [
            "bool process_shares_mm(struct task_struct *p, struct mm_struct *mm)",
            "{",
            "\tstruct task_struct *t;",
            "",
            "\tfor_each_thread(p, t) {",
            "\t\tstruct mm_struct *t_mm = READ_ONCE(t->mm);",
            "\t\tif (t_mm)",
            "\t\t\treturn t_mm == mm;",
            "\t}",
            "\treturn false;",
            "}",
            "static bool __oom_reap_task_mm(struct mm_struct *mm)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tbool ret = true;",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "",
            "\t/*",
            "\t * Tell all users of get_user/copy_from_user etc... that the content",
            "\t * is no longer stable. No barriers really needed because unmapping",
            "\t * should imply barriers already and the reader would hit a page fault",
            "\t * if it stumbled over a reaped memory.",
            "\t */",
            "\tset_bit(MMF_UNSTABLE, &mm->flags);",
            "",
            "\tfor_each_vma(vmi, vma) {",
            "\t\tif (vma->vm_flags & (VM_HUGETLB|VM_PFNMAP))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * Only anonymous pages have a good chance to be dropped",
            "\t\t * without additional steps which we cannot afford as we",
            "\t\t * are OOM already.",
            "\t\t *",
            "\t\t * We do not even care about fs backed pages because all",
            "\t\t * which are reclaimable have already been reclaimed and",
            "\t\t * we do not want to block exit_mmap by keeping mm ref",
            "\t\t * count elevated without a good reason.",
            "\t\t */",
            "\t\tif (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED)) {",
            "\t\t\tstruct mmu_notifier_range range;",
            "\t\t\tstruct mmu_gather tlb;",
            "",
            "\t\t\tmmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0,",
            "\t\t\t\t\t\tmm, vma->vm_start,",
            "\t\t\t\t\t\tvma->vm_end);",
            "\t\t\ttlb_gather_mmu(&tlb, mm);",
            "\t\t\tif (mmu_notifier_invalidate_range_start_nonblock(&range)) {",
            "\t\t\t\ttlb_finish_mmu(&tlb);",
            "\t\t\t\tret = false;",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\tunmap_page_range(&tlb, vma, range.start, range.end, NULL);",
            "\t\t\tmmu_notifier_invalidate_range_end(&range);",
            "\t\t\ttlb_finish_mmu(&tlb);",
            "\t\t}",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "static bool oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)",
            "{",
            "\tbool ret = true;",
            "",
            "\tif (!mmap_read_trylock(mm)) {",
            "\t\ttrace_skip_task_reaping(tsk->pid);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/*",
            "\t * MMF_OOM_SKIP is set by exit_mmap when the OOM reaper can't",
            "\t * work on the mm anymore. The check for MMF_OOM_SKIP must run",
            "\t * under mmap_lock for reading because it serializes against the",
            "\t * mmap_write_lock();mmap_write_unlock() cycle in exit_mmap().",
            "\t */",
            "\tif (test_bit(MMF_OOM_SKIP, &mm->flags)) {",
            "\t\ttrace_skip_task_reaping(tsk->pid);",
            "\t\tgoto out_unlock;",
            "\t}",
            "",
            "\ttrace_start_task_reaping(tsk->pid);",
            "",
            "\t/* failed to reap part of the address space. Try again later */",
            "\tret = __oom_reap_task_mm(mm);",
            "\tif (!ret)",
            "\t\tgoto out_finish;",
            "",
            "\tpr_info(\"oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\\n\",",
            "\t\t\ttask_pid_nr(tsk), tsk->comm,",
            "\t\t\tK(get_mm_counter(mm, MM_ANONPAGES)),",
            "\t\t\tK(get_mm_counter(mm, MM_FILEPAGES)),",
            "\t\t\tK(get_mm_counter(mm, MM_SHMEMPAGES)));",
            "out_finish:",
            "\ttrace_finish_task_reaping(tsk->pid);",
            "out_unlock:",
            "\tmmap_read_unlock(mm);",
            "",
            "\treturn ret;",
            "}",
            "static void oom_reap_task(struct task_struct *tsk)",
            "{",
            "\tint attempts = 0;",
            "\tstruct mm_struct *mm = tsk->signal->oom_mm;",
            "",
            "\t/* Retry the mmap_read_trylock(mm) a few times */",
            "\twhile (attempts++ < MAX_OOM_REAP_RETRIES && !oom_reap_task_mm(tsk, mm))",
            "\t\tschedule_timeout_idle(HZ/10);",
            "",
            "\tif (attempts <= MAX_OOM_REAP_RETRIES ||",
            "\t    test_bit(MMF_OOM_SKIP, &mm->flags))",
            "\t\tgoto done;",
            "",
            "\tpr_info(\"oom_reaper: unable to reap pid:%d (%s)\\n\",",
            "\t\ttask_pid_nr(tsk), tsk->comm);",
            "\tsched_show_task(tsk);",
            "\tdebug_show_all_locks();",
            "",
            "done:",
            "\ttsk->oom_reaper_list = NULL;",
            "",
            "\t/*",
            "\t * Hide this mm from OOM killer because it has been either reaped or",
            "\t * somebody can't call mmap_write_unlock(mm).",
            "\t */",
            "\tset_bit(MMF_OOM_SKIP, &mm->flags);",
            "",
            "\t/* Drop a reference taken by queue_oom_reaper */",
            "\tput_task_struct(tsk);",
            "}"
          ],
          "function_name": "process_shares_mm, __oom_reap_task_mm, oom_reap_task_mm, oom_reap_task",
          "description": "通过强制解除地址空间映射、清除MMF_OOM_SKIP标志等方式尝试回收选定进程的内存资源，最终标记该进程为不可再次被OOM杀手处理。",
          "similarity": 0.5576037168502808
        }
      ]
    },
    {
      "source_file": "kernel/numa.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:12:41\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `numa.c`\n\n---\n\n# numa.c 技术文档\n\n## 1. 文件概述\n\n`numa.c` 是 Linux 内核中用于处理 NUMA（Non-Uniform Memory Access，非统一内存访问）架构下物理内存地址到 NUMA 节点映射的辅助实现文件。该文件提供两个关键函数的**桩函数（stub functions）**实现，用于在架构未原生支持相关功能时提供默认行为：将未知物理地址映射到节点 0，并输出一次性的警告信息。这种设计确保了内核在缺乏特定平台 NUMA 信息支持时仍能正常运行。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`memory_add_physaddr_to_nid(u64 start)`**  \n  将用于内存热插拔（memory hotplug）场景，根据物理起始地址 `start` 返回对应的 NUMA 节点 ID。若平台未实现该函数，则使用本文件中的桩函数。\n\n- **`phys_to_target_node(u64 start)`**  \n  用于查询物理地址 `start` 所属的“目标节点”（target node），通常在内存分配或迁移策略中使用。若平台未提供实现，则回退到本文件的默认桩函数。\n\n> 两个函数均使用 `#ifndef` 条件编译，仅在未由其他架构相关代码定义时才编译本文件中的实现。\n\n## 3. 关键实现\n\n- **条件编译机制**：  \n  使用 `#ifndef` 预处理指令包裹函数定义，确保只有在架构特定代码（如 `arch/*/mm/numa.c`）未提供对应实现时，才使用本通用桩函数。这体现了内核“优先使用平台优化实现，否则回退到通用默认”的设计原则。\n\n- **默认行为**：  \n  两个函数均返回节点 `0`，这是 NUMA 系统中最基础的默认节点，保证了即使在缺乏 NUMA 拓扑信息的情况下，内核仍能将内存分配到一个有效节点。\n\n- **一次性日志提示**：  \n  使用 `pr_info_once()` 宏打印警告信息，仅在首次调用时输出日志，避免重复刷屏。日志内容明确指出“未知节点，假设为节点 0”，便于调试和诊断 NUMA 配置问题。\n\n- **符号导出**：  \n  通过 `EXPORT_SYMBOL_GPL()` 导出两个函数符号，供其他内核模块（如内存管理子系统、热插拔驱动等）在 GPL 许可下使用。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/printk.h>`：提供 `pr_info_once()` 日志宏。\n  - `<linux/numa.h>`：声明 NUMA 相关接口和类型（如 `nid` 类型）。\n\n- **模块依赖**：\n  - 内存热插拔子系统（`mm/memory_hotplug.c`）可能调用 `memory_add_physaddr_to_nid`。\n  - 内存策略或迁移模块（如 `mm/mempolicy.c`、`mm/migrate.c`）可能使用 `phys_to_target_node`。\n  - 架构特定 NUMA 实现（如 x86、ARM64 的 `numa.c`）若已定义这两个函数，则本文件内容不会被编译。\n\n## 5. 使用场景\n\n- **无 NUMA 支持的平台**：在单节点（UMA）或未实现 NUMA 拓扑解析的架构上，作为默认回退实现。\n- **NUMA 信息缺失的热插拔内存**：当系统动态添加内存区域但无法确定其所属 NUMA 节点时，使用此桩函数确保内存仍可被纳入管理。\n- **内核移植初期**：在新架构支持 NUMA 前，可先依赖此通用实现使系统启动，后续再提供平台特定优化。\n- **调试与诊断**：通过 `pr_info_once` 日志帮助开发者识别系统中未正确配置 NUMA 节点映射的内存区域。",
      "similarity": 0.615877628326416,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/numa.c",
          "start_line": 9,
          "end_line": 20,
          "content": [
            "int memory_add_physaddr_to_nid(u64 start)",
            "{",
            "\tpr_info_once(\"Unknown online node for memory at 0x%llx, assuming node 0\\n\",",
            "\t\t\tstart);",
            "\treturn 0;",
            "}",
            "int phys_to_target_node(u64 start)",
            "{",
            "\tpr_info_once(\"Unknown target node for memory at 0x%llx, assuming node 0\\n\",",
            "\t\t\tstart);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "memory_add_physaddr_to_nid, phys_to_target_node",
          "description": "提供了memory_add_physaddr_to_nid和phys_to_target_node两个函数的stub实现，核心功能是在无法确定物理内存所属节点时默认归零，用于NUMA架构中内存地址到节点的映射处理",
          "similarity": 0.6095118522644043
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/numa.c",
          "start_line": 1,
          "end_line": 8,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "",
            "#include <linux/printk.h>",
            "#include <linux/numa.h>",
            "",
            "/* Stub functions: */",
            "",
            "#ifndef memory_add_physaddr_to_nid"
          ],
          "function_name": null,
          "description": "此代码片段展示了一个未定义的宏memory_add_physaddr_to_nid，表明当前上下文不完整，缺少该宏的具体实现或相关NUMA内存注册逻辑",
          "similarity": 0.5651860237121582
        }
      ]
    }
  ]
}