{
  "query": "多线程上下文切换优化方案",
  "timestamp": "2025-12-26 00:00:41",
  "retrieved_files": [
    {
      "source_file": "kernel/irq_work.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:11:23\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq_work.c`\n\n---\n\n# `irq_work.c` 技术文档\n\n## 1. 文件概述\n\n`irq_work.c` 实现了一个轻量级的中断上下文工作队列机制，允许在硬中断（hardirq）或 NMI（不可屏蔽中断）上下文中安全地调度回调函数，并在稍后的硬中断上下文或专用内核线程中执行。该机制的核心目标是提供一种 **NMI 安全** 的方式来延迟执行某些不能在 NMI 或硬中断中直接完成的操作。\n\n该框架特别适用于需要从 NMI 或硬中断中触发后续处理（如 perf 事件、ftrace、RCU 等子系统）但又不能阻塞或执行复杂逻辑的场景。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- `struct irq_work`：表示一个中断工作项，包含回调函数 `func` 和状态标志（如 `IRQ_WORK_PENDING`、`IRQ_WORK_CLAIMED`、`IRQ_WORK_BUSY`、`IRQ_WORK_LAZY`、`IRQ_WORK_HARD_IRQ`）。\n- 每 CPU 变量：\n  - `raised_list`：存放需在硬中断上下文中立即处理的工作项。\n  - `lazy_list`：存放“惰性”工作项，在非硬中断上下文（如 tick 或专用线程）中处理。\n  - `irq_workd`：指向每 CPU 的 `irq_work` 内核线程（仅在 `CONFIG_PREEMPT_RT` 下使用）。\n\n### 主要函数\n\n| 函数 | 功能 |\n|------|------|\n| `irq_work_queue(struct irq_work *work)` | 在当前 CPU 上排队一个 `irq_work`，若未被声明则声明并入队。 |\n| `irq_work_queue_on(struct irq_work *work, int cpu)` | 将 `irq_work` 排队到指定 CPU（支持跨 CPU 调度）。 |\n| `irq_work_run(void)` | 在当前 CPU 上执行所有 `raised_list` 和（非 RT 下的）`lazy_list` 中的工作项。 |\n| `irq_work_tick(void)` | 由时钟 tick 调用，处理未被硬中断处理的 `raised_list` 和 `lazy_list`。 |\n| `irq_work_sync(struct irq_work *work)` | 同步等待指定 `irq_work` 执行完毕。 |\n| `irq_work_single(void *arg)` | 执行单个工作项的回调函数，并清理状态。 |\n| `arch_irq_work_raise(void)` | 架构相关函数，用于触发 IPI 或中断以唤醒处理逻辑（弱符号，默认为空）。 |\n\n## 3. 关键实现\n\n### 状态管理与原子操作\n\n- 每个 `irq_work` 通过 `atomic_t node.a_flags` 管理状态：\n  - `IRQ_WORK_PENDING`：表示工作项已入队但尚未执行。\n  - `IRQ_WORK_CLAIMED`：表示已被声明，防止重复入队。\n  - `IRQ_WORK_BUSY`：表示正在执行中。\n- `irq_work_claim()` 使用 `atomic_fetch_or()` 原子地设置 `CLAIMED` 和 `PENDING` 标志，并检查是否已存在，避免重复入队。\n\n### 双队列设计\n\n- **`raised_list`**：用于需要尽快在硬中断上下文执行的工作（如标记为 `IRQ_WORK_HARD_IRQ` 的项）。\n- **`lazy_list`**：\n  - 在非 RT 内核中，由 `irq_work_tick()` 或 `irq_work_run()` 在软中断或进程上下文中处理。\n  - 在 `CONFIG_PREEMPT_RT` 下，由每 CPU 的 `irq_work/%u` 内核线程处理（以避免在硬中断中执行非硬实时代码）。\n\n### NMI 安全性\n\n- 入队操作（如 `irq_work_queue`）仅使用原子操作和每 CPU 链表（`llist`），不涉及锁或内存分配，因此可在 NMI 上下文中安全调用。\n- 跨 CPU 入队时（`irq_work_queue_on`）会检查 `in_nmi()`，防止在 NMI 中调用非 NMI 安全的 IPI 发送函数。\n\n### PREEMPT_RT 支持\n\n- 在 RT 内核中，非 `IRQ_WORK_HARD_IRQ` 的工作项被放入 `lazy_list`，并通过专用内核线程执行，以避免在硬中断中运行可能阻塞或延迟高的代码。\n- 使用 `rcuwait` 机制实现 `irq_work_sync()` 的睡眠等待。\n\n### IPI 触发机制\n\n- 若架构支持（通过 `arch_irq_work_has_interrupt()`），调用 `arch_irq_work_raise()` 触发本地中断处理。\n- 否则依赖时钟 tick（`irq_work_tick`）或显式调用 `irq_work_run` 来处理队列。\n\n## 4. 依赖关系\n\n- **架构依赖**：\n  - `arch_irq_work_raise()` 和 `arch_irq_work_has_interrupt()` 需由具体架构实现（如 x86 提供）。\n- **内核子系统**：\n  - `llist`（无锁链表）：用于高效、无锁的每 CPU 队列管理。\n  - `smpboot`：用于注册每 CPU 内核线程（RT 模式）。\n  - `rcu`：`rcuwait` 用于同步等待（RT 模式）。\n  - `tick`：`tick_nohz_tick_stopped()` 用于判断是否需要立即触发处理。\n  - `trace_events`：IPI 跟踪点 `trace_ipi_send_cpu`。\n- **配置选项**：\n  - `CONFIG_SMP`：启用跨 CPU 调度和 IPI 支持。\n  - `CONFIG_PREEMPT_RT`：启用 RT 模式下的线程化处理。\n\n## 5. 使用场景\n\n- **性能监控（perf）**：从 NMI 中记录采样后，通过 `irq_work` 安全地将数据传递到常规上下文处理。\n- **ftrace / tracing**：在中断上下文中触发延迟的跟踪事件处理。\n- **RCU**：某些 RCU 实现使用 `irq_work` 来触发宽限期处理。\n- **热插拔 CPU**：在 CPU 离线前通过 `flush_smp_call_function_queue()` 调用 `irq_work_run()` 确保工作项被清空。\n- **中断负载均衡或延迟处理**：将非关键中断处理逻辑延迟到更安全的上下文执行。\n\n该机制为内核提供了一种高效、安全且可扩展的中断后处理框架，尤其适用于实时性和可靠性要求高的子系统。",
      "similarity": 0.5823126435279846,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq_work.c",
          "start_line": 31,
          "end_line": 157,
          "content": [
            "static void wake_irq_workd(void)",
            "{",
            "\tstruct task_struct *tsk = __this_cpu_read(irq_workd);",
            "",
            "\tif (!llist_empty(this_cpu_ptr(&lazy_list)) && tsk)",
            "\t\twake_up_process(tsk);",
            "}",
            "static void irq_work_wake(struct irq_work *entry)",
            "{",
            "\twake_irq_workd();",
            "}",
            "static int irq_workd_should_run(unsigned int cpu)",
            "{",
            "\treturn !llist_empty(this_cpu_ptr(&lazy_list));",
            "}",
            "static bool irq_work_claim(struct irq_work *work)",
            "{",
            "\tint oflags;",
            "",
            "\toflags = atomic_fetch_or(IRQ_WORK_CLAIMED | CSD_TYPE_IRQ_WORK, &work->node.a_flags);",
            "\t/*",
            "\t * If the work is already pending, no need to raise the IPI.",
            "\t * The pairing smp_mb() in irq_work_single() makes sure",
            "\t * everything we did before is visible.",
            "\t */",
            "\tif (oflags & IRQ_WORK_PENDING)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "void __weak arch_irq_work_raise(void)",
            "{",
            "\t/*",
            "\t * Lame architectures will get the timer tick callback",
            "\t */",
            "}",
            "static __always_inline void irq_work_raise(struct irq_work *work)",
            "{",
            "\tif (trace_ipi_send_cpu_enabled() && arch_irq_work_has_interrupt())",
            "\t\ttrace_ipi_send_cpu(smp_processor_id(), _RET_IP_, work->func);",
            "",
            "\tarch_irq_work_raise();",
            "}",
            "static void __irq_work_queue_local(struct irq_work *work)",
            "{",
            "\tstruct llist_head *list;",
            "\tbool rt_lazy_work = false;",
            "\tbool lazy_work = false;",
            "\tint work_flags;",
            "",
            "\twork_flags = atomic_read(&work->node.a_flags);",
            "\tif (work_flags & IRQ_WORK_LAZY)",
            "\t\tlazy_work = true;",
            "\telse if (IS_ENABLED(CONFIG_PREEMPT_RT) &&",
            "\t\t !(work_flags & IRQ_WORK_HARD_IRQ))",
            "\t\trt_lazy_work = true;",
            "",
            "\tif (lazy_work || rt_lazy_work)",
            "\t\tlist = this_cpu_ptr(&lazy_list);",
            "\telse",
            "\t\tlist = this_cpu_ptr(&raised_list);",
            "",
            "\tif (!llist_add(&work->node.llist, list))",
            "\t\treturn;",
            "",
            "\t/* If the work is \"lazy\", handle it from next tick if any */",
            "\tif (!lazy_work || tick_nohz_tick_stopped())",
            "\t\tirq_work_raise(work);",
            "}",
            "bool irq_work_queue(struct irq_work *work)",
            "{",
            "\t/* Only queue if not already pending */",
            "\tif (!irq_work_claim(work))",
            "\t\treturn false;",
            "",
            "\t/* Queue the entry and raise the IPI if needed. */",
            "\tpreempt_disable();",
            "\t__irq_work_queue_local(work);",
            "\tpreempt_enable();",
            "",
            "\treturn true;",
            "}",
            "bool irq_work_queue_on(struct irq_work *work, int cpu)",
            "{",
            "#ifndef CONFIG_SMP",
            "\treturn irq_work_queue(work);",
            "",
            "#else /* CONFIG_SMP: */",
            "\t/* All work should have been flushed before going offline */",
            "\tWARN_ON_ONCE(cpu_is_offline(cpu));",
            "",
            "\t/* Only queue if not already pending */",
            "\tif (!irq_work_claim(work))",
            "\t\treturn false;",
            "",
            "\tkasan_record_aux_stack_noalloc(work);",
            "",
            "\tpreempt_disable();",
            "\tif (cpu != smp_processor_id()) {",
            "\t\t/* Arch remote IPI send/receive backend aren't NMI safe */",
            "\t\tWARN_ON_ONCE(in_nmi());",
            "",
            "\t\t/*",
            "\t\t * On PREEMPT_RT the items which are not marked as",
            "\t\t * IRQ_WORK_HARD_IRQ are added to the lazy list and a HARD work",
            "\t\t * item is used on the remote CPU to wake the thread.",
            "\t\t */",
            "\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) &&",
            "\t\t    !(atomic_read(&work->node.a_flags) & IRQ_WORK_HARD_IRQ)) {",
            "",
            "\t\t\tif (!llist_add(&work->node.llist, &per_cpu(lazy_list, cpu)))",
            "\t\t\t\tgoto out;",
            "",
            "\t\t\twork = &per_cpu(irq_work_wakeup, cpu);",
            "\t\t\tif (!irq_work_claim(work))",
            "\t\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\t__smp_call_single_queue(cpu, &work->node.llist);",
            "\t} else {",
            "\t\t__irq_work_queue_local(work);",
            "\t}",
            "out:",
            "\tpreempt_enable();",
            "",
            "\treturn true;",
            "#endif /* CONFIG_SMP */",
            "}"
          ],
          "function_name": "wake_irq_workd, irq_work_wake, irq_workd_should_run, irq_work_claim, arch_irq_work_raise, irq_work_raise, __irq_work_queue_local, irq_work_queue, irq_work_queue_on",
          "description": "实现了中断工作项的排队逻辑，区分硬中断与延迟工作项，通过IPI或线程唤醒机制确保跨CPU执行，支持PREEMPT_RT配置下的延迟处理",
          "similarity": 0.5337114334106445
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq_work.c",
          "start_line": 1,
          "end_line": 30,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (C) 2010 Red Hat, Inc., Peter Zijlstra",
            " *",
            " * Provides a framework for enqueueing and running callbacks from hardirq",
            " * context. The enqueueing is NMI-safe.",
            " */",
            "",
            "#include <linux/bug.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/percpu.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/sched.h>",
            "#include <linux/tick.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/smp.h>",
            "#include <linux/smpboot.h>",
            "#include <asm/processor.h>",
            "#include <linux/kasan.h>",
            "",
            "#include <trace/events/ipi.h>",
            "",
            "static DEFINE_PER_CPU(struct llist_head, raised_list);",
            "static DEFINE_PER_CPU(struct llist_head, lazy_list);",
            "static DEFINE_PER_CPU(struct task_struct *, irq_workd);",
            ""
          ],
          "function_name": null,
          "description": "定义了用于管理中断工作队列的per-CPU链表（raised_list/lazy_list）和irq_workd线程指针，提供NMI安全的enqueue框架",
          "similarity": 0.5290089845657349
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq_work.c",
          "start_line": 184,
          "end_line": 286,
          "content": [
            "bool irq_work_needs_cpu(void)",
            "{",
            "\tstruct llist_head *raised, *lazy;",
            "",
            "\traised = this_cpu_ptr(&raised_list);",
            "\tlazy = this_cpu_ptr(&lazy_list);",
            "",
            "\tif (llist_empty(raised) || arch_irq_work_has_interrupt())",
            "\t\tif (llist_empty(lazy))",
            "\t\t\treturn false;",
            "",
            "\t/* All work should have been flushed before going offline */",
            "\tWARN_ON_ONCE(cpu_is_offline(smp_processor_id()));",
            "",
            "\treturn true;",
            "}",
            "void irq_work_single(void *arg)",
            "{",
            "\tstruct irq_work *work = arg;",
            "\tint flags;",
            "",
            "\t/*",
            "\t * Clear the PENDING bit, after this point the @work can be re-used.",
            "\t * The PENDING bit acts as a lock, and we own it, so we can clear it",
            "\t * without atomic ops.",
            "\t */",
            "\tflags = atomic_read(&work->node.a_flags);",
            "\tflags &= ~IRQ_WORK_PENDING;",
            "\tatomic_set(&work->node.a_flags, flags);",
            "",
            "\t/*",
            "\t * See irq_work_claim().",
            "\t */",
            "\tsmp_mb();",
            "",
            "\tlockdep_irq_work_enter(flags);",
            "\twork->func(work);",
            "\tlockdep_irq_work_exit(flags);",
            "",
            "\t/*",
            "\t * Clear the BUSY bit, if set, and return to the free state if no-one",
            "\t * else claimed it meanwhile.",
            "\t */",
            "\t(void)atomic_cmpxchg(&work->node.a_flags, flags, flags & ~IRQ_WORK_BUSY);",
            "",
            "\tif ((IS_ENABLED(CONFIG_PREEMPT_RT) && !irq_work_is_hard(work)) ||",
            "\t    !arch_irq_work_has_interrupt())",
            "\t\trcuwait_wake_up(&work->irqwait);",
            "}",
            "static void irq_work_run_list(struct llist_head *list)",
            "{",
            "\tstruct irq_work *work, *tmp;",
            "\tstruct llist_node *llnode;",
            "",
            "\t/*",
            "\t * On PREEMPT_RT IRQ-work which is not marked as HARD will be processed",
            "\t * in a per-CPU thread in preemptible context. Only the items which are",
            "\t * marked as IRQ_WORK_HARD_IRQ will be processed in hardirq context.",
            "\t */",
            "\tBUG_ON(!irqs_disabled() && !IS_ENABLED(CONFIG_PREEMPT_RT));",
            "",
            "\tif (llist_empty(list))",
            "\t\treturn;",
            "",
            "\tllnode = llist_del_all(list);",
            "\tllist_for_each_entry_safe(work, tmp, llnode, node.llist)",
            "\t\tirq_work_single(work);",
            "}",
            "void irq_work_run(void)",
            "{",
            "\tirq_work_run_list(this_cpu_ptr(&raised_list));",
            "\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tirq_work_run_list(this_cpu_ptr(&lazy_list));",
            "\telse",
            "\t\twake_irq_workd();",
            "}",
            "void irq_work_tick(void)",
            "{",
            "\tstruct llist_head *raised = this_cpu_ptr(&raised_list);",
            "",
            "\tif (!llist_empty(raised) && !arch_irq_work_has_interrupt())",
            "\t\tirq_work_run_list(raised);",
            "",
            "\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tirq_work_run_list(this_cpu_ptr(&lazy_list));",
            "\telse",
            "\t\twake_irq_workd();",
            "}",
            "void irq_work_sync(struct irq_work *work)",
            "{",
            "\tlockdep_assert_irqs_enabled();",
            "\tmight_sleep();",
            "",
            "\tif ((IS_ENABLED(CONFIG_PREEMPT_RT) && !irq_work_is_hard(work)) ||",
            "\t    !arch_irq_work_has_interrupt()) {",
            "\t\trcuwait_wait_event(&work->irqwait, !irq_work_is_busy(work),",
            "\t\t\t\t   TASK_UNINTERRUPTIBLE);",
            "\t\treturn;",
            "\t}",
            "",
            "\twhile (irq_work_is_busy(work))",
            "\t\tcpu_relax();",
            "}"
          ],
          "function_name": "irq_work_needs_cpu, irq_work_single, irq_work_run_list, irq_work_run, irq_work_tick, irq_work_sync",
          "description": "处理工作项的实际执行流程，包含单次执行逻辑、链表遍历运行及同步等待机制，区分硬中断上下文与RCU等待状态的处理",
          "similarity": 0.5281553864479065
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/irq_work.c",
          "start_line": 303,
          "end_line": 316,
          "content": [
            "static void run_irq_workd(unsigned int cpu)",
            "{",
            "\tirq_work_run_list(this_cpu_ptr(&lazy_list));",
            "}",
            "static void irq_workd_setup(unsigned int cpu)",
            "{",
            "\tsched_set_fifo_low(current);",
            "}",
            "static __init int irq_work_init_threads(void)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tBUG_ON(smpboot_register_percpu_thread(&irqwork_threads));",
            "\treturn 0;",
            "}"
          ],
          "function_name": "run_irq_workd, irq_workd_setup, irq_work_init_threads",
          "description": "初始化PREEMPT_RT环境下的per-CPU工作线程，注册并启动处理延迟工作项的专用线程，通过smpboot接口创建线程实体",
          "similarity": 0.5109190940856934
        }
      ]
    },
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.5589970350265503,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/rt.c",
          "start_line": 529,
          "end_line": 633,
          "content": [
            "static void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\tstruct sched_rt_entity *rt_se;",
            "",
            "\tint cpu = cpu_of(rq);",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tif (!rt_se)",
            "\t\t\tenqueue_top_rt_rq(rt_rq);",
            "\t\telse if (!on_rt_rq(rt_se))",
            "\t\t\tenqueue_rt_entity(rt_se, 0);",
            "",
            "\t\tif (rt_rq->highest_prio.curr < curr->prio)",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "}",
            "static void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (!rt_se) {",
            "\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "\t\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);",
            "\t}",
            "\telse if (on_rt_rq(rt_se))",
            "\t\tdequeue_rt_entity(rt_se, 0);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;",
            "}",
            "static int rt_se_boosted(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "\tstruct task_struct *p;",
            "",
            "\tif (rt_rq)",
            "\t\treturn !!rt_rq->rt_nr_boosted;",
            "",
            "\tp = rt_task_of(rt_se);",
            "\treturn p->prio != p->normal_prio;",
            "}",
            "bool sched_rt_bandwidth_account(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\treturn (hrtimer_active(&rt_b->rt_period_timer) ||",
            "\t\trt_rq->rt_time < rt_b->rt_runtime);",
            "}",
            "static void do_balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;",
            "\tint i, weight;",
            "\tu64 rt_period;",
            "",
            "\tweight = cpumask_weight(rd->span);",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\trt_period = ktime_to_ns(rt_b->rt_period);",
            "\tfor_each_cpu(i, rd->span) {",
            "\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\ts64 diff;",
            "",
            "\t\tif (iter == rt_rq)",
            "\t\t\tcontinue;",
            "",
            "\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either all rqs have inf runtime and there's nothing to steal",
            "\t\t * or __disable_runtime() below sets a specific rq to inf to",
            "\t\t * indicate its been disabled and disallow stealing.",
            "\t\t */",
            "\t\tif (iter->rt_runtime == RUNTIME_INF)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * From runqueues with spare time, take 1/n part of their",
            "\t\t * spare time, but no more than our period.",
            "\t\t */",
            "\t\tdiff = iter->rt_runtime - iter->rt_time;",
            "\t\tif (diff > 0) {",
            "\t\t\tdiff = div_u64((u64)diff, weight);",
            "\t\t\tif (rt_rq->rt_runtime + diff > rt_period)",
            "\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;",
            "\t\t\titer->rt_runtime -= diff;",
            "\t\t\trt_rq->rt_runtime += diff;",
            "\t\t\tif (rt_rq->rt_runtime == rt_period) {",
            "\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "next:",
            "\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, rt_se_boosted, sched_rt_bandwidth_account, do_balance_runtime",
          "description": "实现实时任务队列的插入/移除逻辑，跟踪运行时间消耗，通过跨CPU运行时间平衡算法实现带宽公平分配。",
          "similarity": 0.5777875781059265
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/sched/rt.c",
          "start_line": 2102,
          "end_line": 2228,
          "content": [
            "static void push_rt_tasks(struct rq *rq)",
            "{",
            "\t/* push_rt_task will return true if it moved an RT */",
            "\twhile (push_rt_task(rq, false))",
            "\t\t;",
            "}",
            "static int rto_next_cpu(struct root_domain *rd)",
            "{",
            "\tint next;",
            "\tint cpu;",
            "",
            "\t/*",
            "\t * When starting the IPI RT pushing, the rto_cpu is set to -1,",
            "\t * rt_next_cpu() will simply return the first CPU found in",
            "\t * the rto_mask.",
            "\t *",
            "\t * If rto_next_cpu() is called with rto_cpu is a valid CPU, it",
            "\t * will return the next CPU found in the rto_mask.",
            "\t *",
            "\t * If there are no more CPUs left in the rto_mask, then a check is made",
            "\t * against rto_loop and rto_loop_next. rto_loop is only updated with",
            "\t * the rto_lock held, but any CPU may increment the rto_loop_next",
            "\t * without any locking.",
            "\t */",
            "\tfor (;;) {",
            "",
            "\t\t/* When rto_cpu is -1 this acts like cpumask_first() */",
            "\t\tcpu = cpumask_next(rd->rto_cpu, rd->rto_mask);",
            "",
            "\t\trd->rto_cpu = cpu;",
            "",
            "\t\tif (cpu < nr_cpu_ids)",
            "\t\t\treturn cpu;",
            "",
            "\t\trd->rto_cpu = -1;",
            "",
            "\t\t/*",
            "\t\t * ACQUIRE ensures we see the @rto_mask changes",
            "\t\t * made prior to the @next value observed.",
            "\t\t *",
            "\t\t * Matches WMB in rt_set_overload().",
            "\t\t */",
            "\t\tnext = atomic_read_acquire(&rd->rto_loop_next);",
            "",
            "\t\tif (rd->rto_loop == next)",
            "\t\t\tbreak;",
            "",
            "\t\trd->rto_loop = next;",
            "\t}",
            "",
            "\treturn -1;",
            "}",
            "static inline bool rto_start_trylock(atomic_t *v)",
            "{",
            "\treturn !atomic_cmpxchg_acquire(v, 0, 1);",
            "}",
            "static inline void rto_start_unlock(atomic_t *v)",
            "{",
            "\tatomic_set_release(v, 0);",
            "}",
            "static void tell_cpu_to_push(struct rq *rq)",
            "{",
            "\tint cpu = -1;",
            "",
            "\t/* Keep the loop going if the IPI is currently active */",
            "\tatomic_inc(&rq->rd->rto_loop_next);",
            "",
            "\t/* Only one CPU can initiate a loop at a time */",
            "\tif (!rto_start_trylock(&rq->rd->rto_loop_start))",
            "\t\treturn;",
            "",
            "\traw_spin_lock(&rq->rd->rto_lock);",
            "",
            "\t/*",
            "\t * The rto_cpu is updated under the lock, if it has a valid CPU",
            "\t * then the IPI is still running and will continue due to the",
            "\t * update to loop_next, and nothing needs to be done here.",
            "\t * Otherwise it is finishing up and an ipi needs to be sent.",
            "\t */",
            "\tif (rq->rd->rto_cpu < 0)",
            "\t\tcpu = rto_next_cpu(rq->rd);",
            "",
            "\traw_spin_unlock(&rq->rd->rto_lock);",
            "",
            "\trto_start_unlock(&rq->rd->rto_loop_start);",
            "",
            "\tif (cpu >= 0) {",
            "\t\t/* Make sure the rd does not get freed while pushing */",
            "\t\tsched_get_rd(rq->rd);",
            "\t\tirq_work_queue_on(&rq->rd->rto_push_work, cpu);",
            "\t}",
            "}",
            "void rto_push_irq_work_func(struct irq_work *work)",
            "{",
            "\tstruct root_domain *rd =",
            "\t\tcontainer_of(work, struct root_domain, rto_push_work);",
            "\tstruct rq *rq;",
            "\tint cpu;",
            "",
            "\trq = this_rq();",
            "",
            "\t/*",
            "\t * We do not need to grab the lock to check for has_pushable_tasks.",
            "\t * When it gets updated, a check is made if a push is possible.",
            "\t */",
            "\tif (has_pushable_tasks(rq)) {",
            "\t\traw_spin_rq_lock(rq);",
            "\t\twhile (push_rt_task(rq, true))",
            "\t\t\t;",
            "\t\traw_spin_rq_unlock(rq);",
            "\t}",
            "",
            "\traw_spin_lock(&rd->rto_lock);",
            "",
            "\t/* Pass the IPI to the next rt overloaded queue */",
            "\tcpu = rto_next_cpu(rd);",
            "",
            "\traw_spin_unlock(&rd->rto_lock);",
            "",
            "\tif (cpu < 0) {",
            "\t\tsched_put_rd(rd);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Try the next RT overloaded CPU */",
            "\tirq_work_queue_on(&rd->rto_push_work, cpu);",
            "}"
          ],
          "function_name": "push_rt_tasks, rto_next_cpu, rto_start_trylock, rto_start_unlock, tell_cpu_to_push, rto_push_irq_work_func",
          "description": "实现实时任务批量迁移、冗余CPU迭代选择、锁竞争控制及中断工作队列驱动的跨CPU负载均衡机制。",
          "similarity": 0.568376898765564
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1449,
          "end_line": 1589,
          "content": [
            "static void dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rq *rq = rq_of_rt_se(rt_se);",
            "",
            "\tupdate_stats_dequeue_rt(rt_rq_of_se(rt_se), rt_se, flags);",
            "",
            "\tdequeue_rt_stack(rt_se, flags);",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\t\tif (rt_rq && rt_rq->rt_nr_running)",
            "\t\t\t__enqueue_rt_entity(rt_se, flags);",
            "\t}",
            "\tenqueue_top_rt_rq(&rq->rt);",
            "}",
            "static void",
            "enqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tif (flags & ENQUEUE_WAKEUP)",
            "\t\trt_se->timeout = 0;",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_rt(rt_rq_of_se(rt_se), rt_se);",
            "",
            "\tenqueue_rt_entity(rt_se, flags);",
            "",
            "\tif (!task_current(rq, p) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_task(rq, p);",
            "}",
            "static bool dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tdequeue_rt_entity(rt_se, flags);",
            "",
            "\tdequeue_pushable_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void",
            "requeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)",
            "{",
            "\tif (on_rt_rq(rt_se)) {",
            "\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "\t\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);",
            "",
            "\t\tif (head)",
            "\t\t\tlist_move(&rt_se->run_list, queue);",
            "\t\telse",
            "\t\t\tlist_move_tail(&rt_se->run_list, queue);",
            "\t}",
            "}",
            "static void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\trt_rq = rt_rq_of_se(rt_se);",
            "\t\trequeue_rt_entity(rt_rq, rt_se, head);",
            "\t}",
            "}",
            "static void yield_task_rt(struct rq *rq)",
            "{",
            "\trequeue_task_rt(rq, rq->curr, 0);",
            "}",
            "static int",
            "select_task_rq_rt(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tstruct rq *rq;",
            "\tbool test;",
            "",
            "\t/* For anything but wake ups, just return the task_cpu */",
            "\tif (!(flags & (WF_TTWU | WF_FORK)))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If the current task on @p's runqueue is an RT task, then",
            "\t * try to see if we can wake this RT task up on another",
            "\t * runqueue. Otherwise simply start this RT task",
            "\t * on its current runqueue.",
            "\t *",
            "\t * We want to avoid overloading runqueues. If the woken",
            "\t * task is a higher priority, then it will stay on this CPU",
            "\t * and the lower prio task should be moved to another CPU.",
            "\t * Even though this will probably make the lower prio task",
            "\t * lose its cache, we do not want to bounce a higher task",
            "\t * around just because it gave up its CPU, perhaps for a",
            "\t * lock?",
            "\t *",
            "\t * For equal prio tasks, we just let the scheduler sort it out.",
            "\t *",
            "\t * Otherwise, just let it ride on the affined RQ and the",
            "\t * post-schedule router will push the preempted task away",
            "\t *",
            "\t * This test is optimistic, if we get it wrong the load-balancer",
            "\t * will have to sort it out.",
            "\t *",
            "\t * We take into account the capacity of the CPU to ensure it fits the",
            "\t * requirement of the task - which is only important on heterogeneous",
            "\t * systems like big.LITTLE.",
            "\t */",
            "\ttest = curr &&",
            "\t       unlikely(rt_task(curr)) &&",
            "\t       (curr->nr_cpus_allowed < 2 || curr->prio <= p->prio);",
            "",
            "\tif (test || !rt_task_fits_capacity(p, cpu)) {",
            "\t\tint target = find_lowest_rq(p);",
            "",
            "\t\t/*",
            "\t\t * Bail out if we were forcing a migration to find a better",
            "\t\t * fitting CPU but our search failed.",
            "\t\t */",
            "\t\tif (!test && target != -1 && !rt_task_fits_capacity(p, target))",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\t/*",
            "\t\t * Don't bother moving it if the destination CPU is",
            "\t\t * not running a lower priority task.",
            "\t\t */",
            "\t\tif (target != -1 &&",
            "\t\t    p->prio < cpu_rq(target)->rt.highest_prio.curr)",
            "\t\t\tcpu = target;",
            "\t}",
            "",
            "out_unlock:",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "dequeue_rt_entity, enqueue_task_rt, dequeue_task_rt, requeue_rt_entity, requeue_task_rt, yield_task_rt, select_task_rq_rt",
          "description": "实现实时任务的出队逻辑、唤醒和迁移策略，提供CPU亲和性选择及负载均衡支持，维护优先级队列的动态调整。",
          "similarity": 0.5514229536056519
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1776,
          "end_line": 1992,
          "content": [
            "static int pick_rt_task(struct rq *rq, struct task_struct *p, int cpu)",
            "{",
            "\tif (!task_on_cpu(rq, p) &&",
            "\t    cpumask_test_cpu(cpu, &p->cpus_mask))",
            "\t\treturn 1;",
            "",
            "\treturn 0;",
            "}",
            "static int find_lowest_rq(struct task_struct *task)",
            "{",
            "\tstruct sched_domain *sd;",
            "\tstruct cpumask *lowest_mask = this_cpu_cpumask_var_ptr(local_cpu_mask);",
            "\tint this_cpu = smp_processor_id();",
            "\tint cpu      = task_cpu(task);",
            "\tint ret;",
            "",
            "\t/* Make sure the mask is initialized first */",
            "\tif (unlikely(!lowest_mask))",
            "\t\treturn -1;",
            "",
            "\tif (task->nr_cpus_allowed == 1)",
            "\t\treturn -1; /* No other targets possible */",
            "",
            "\t/*",
            "\t * If we're on asym system ensure we consider the different capacities",
            "\t * of the CPUs when searching for the lowest_mask.",
            "\t */",
            "\tif (sched_asym_cpucap_active()) {",
            "",
            "\t\tret = cpupri_find_fitness(&task_rq(task)->rd->cpupri,",
            "\t\t\t\t\t  task, lowest_mask,",
            "\t\t\t\t\t  rt_task_fits_capacity);",
            "\t} else {",
            "",
            "\t\tret = cpupri_find(&task_rq(task)->rd->cpupri,",
            "\t\t\t\t  task, lowest_mask);",
            "\t}",
            "",
            "\tif (!ret)",
            "\t\treturn -1; /* No targets found */",
            "",
            "\t/*",
            "\t * At this point we have built a mask of CPUs representing the",
            "\t * lowest priority tasks in the system.  Now we want to elect",
            "\t * the best one based on our affinity and topology.",
            "\t *",
            "\t * We prioritize the last CPU that the task executed on since",
            "\t * it is most likely cache-hot in that location.",
            "\t */",
            "\tif (cpumask_test_cpu(cpu, lowest_mask))",
            "\t\treturn cpu;",
            "",
            "\t/*",
            "\t * Otherwise, we consult the sched_domains span maps to figure",
            "\t * out which CPU is logically closest to our hot cache data.",
            "\t */",
            "\tif (!cpumask_test_cpu(this_cpu, lowest_mask))",
            "\t\tthis_cpu = -1; /* Skip this_cpu opt if not among lowest */",
            "",
            "\trcu_read_lock();",
            "\tfor_each_domain(cpu, sd) {",
            "\t\tif (sd->flags & SD_WAKE_AFFINE) {",
            "\t\t\tint best_cpu;",
            "",
            "\t\t\t/*",
            "\t\t\t * \"this_cpu\" is cheaper to preempt than a",
            "\t\t\t * remote processor.",
            "\t\t\t */",
            "\t\t\tif (this_cpu != -1 &&",
            "\t\t\t    cpumask_test_cpu(this_cpu, sched_domain_span(sd))) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn this_cpu;",
            "\t\t\t}",
            "",
            "\t\t\tbest_cpu = cpumask_any_and_distribute(lowest_mask,",
            "\t\t\t\t\t\t\t      sched_domain_span(sd));",
            "\t\t\tif (best_cpu < nr_cpu_ids) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn best_cpu;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * And finally, if there were no matches within the domains",
            "\t * just give the caller *something* to work with from the compatible",
            "\t * locations.",
            "\t */",
            "\tif (this_cpu != -1)",
            "\t\treturn this_cpu;",
            "",
            "\tcpu = cpumask_any_distribute(lowest_mask);",
            "\tif (cpu < nr_cpu_ids)",
            "\t\treturn cpu;",
            "",
            "\treturn -1;",
            "}",
            "static int push_rt_task(struct rq *rq, bool pull)",
            "{",
            "\tstruct task_struct *next_task;",
            "\tstruct rq *lowest_rq;",
            "\tint ret = 0;",
            "",
            "\tif (!rq->rt.overloaded)",
            "\t\treturn 0;",
            "",
            "\tnext_task = pick_next_pushable_task(rq);",
            "\tif (!next_task)",
            "\t\treturn 0;",
            "",
            "retry:",
            "\t/*",
            "\t * It's possible that the next_task slipped in of",
            "\t * higher priority than current. If that's the case",
            "\t * just reschedule current.",
            "\t */",
            "\tif (unlikely(next_task->prio < rq->curr->prio)) {",
            "\t\tresched_curr(rq);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (is_migration_disabled(next_task)) {",
            "\t\tstruct task_struct *push_task = NULL;",
            "\t\tint cpu;",
            "",
            "\t\tif (!pull || rq->push_busy)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * Invoking find_lowest_rq() on anything but an RT task doesn't",
            "\t\t * make sense. Per the above priority check, curr has to",
            "\t\t * be of higher priority than next_task, so no need to",
            "\t\t * reschedule when bailing out.",
            "\t\t *",
            "\t\t * Note that the stoppers are masqueraded as SCHED_FIFO",
            "\t\t * (cf. sched_set_stop_task()), so we can't rely on rt_task().",
            "\t\t */",
            "\t\tif (rq->curr->sched_class != &rt_sched_class)",
            "\t\t\treturn 0;",
            "",
            "\t\tcpu = find_lowest_rq(rq->curr);",
            "\t\tif (cpu == -1 || cpu == rq->cpu)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * Given we found a CPU with lower priority than @next_task,",
            "\t\t * therefore it should be running. However we cannot migrate it",
            "\t\t * to this other CPU, instead attempt to push the current",
            "\t\t * running task on this CPU away.",
            "\t\t */",
            "\t\tpush_task = get_push_task(rq);",
            "\t\tif (push_task) {",
            "\t\t\tpreempt_disable();",
            "\t\t\traw_spin_rq_unlock(rq);",
            "\t\t\tstop_one_cpu_nowait(rq->cpu, push_cpu_stop,",
            "\t\t\t\t\t    push_task, &rq->push_work);",
            "\t\t\tpreempt_enable();",
            "\t\t\traw_spin_rq_lock(rq);",
            "\t\t}",
            "",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (WARN_ON(next_task == rq->curr))",
            "\t\treturn 0;",
            "",
            "\t/* We might release rq lock */",
            "\tget_task_struct(next_task);",
            "",
            "\t/* find_lock_lowest_rq locks the rq if found */",
            "\tlowest_rq = find_lock_lowest_rq(next_task, rq);",
            "\tif (!lowest_rq) {",
            "\t\tstruct task_struct *task;",
            "\t\t/*",
            "\t\t * find_lock_lowest_rq releases rq->lock",
            "\t\t * so it is possible that next_task has migrated.",
            "\t\t *",
            "\t\t * We need to make sure that the task is still on the same",
            "\t\t * run-queue and is also still the next task eligible for",
            "\t\t * pushing.",
            "\t\t */",
            "\t\ttask = pick_next_pushable_task(rq);",
            "\t\tif (task == next_task) {",
            "\t\t\t/*",
            "\t\t\t * The task hasn't migrated, and is still the next",
            "\t\t\t * eligible task, but we failed to find a run-queue",
            "\t\t\t * to push it to.  Do not retry in this case, since",
            "\t\t\t * other CPUs will pull from us when ready.",
            "\t\t\t */",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tif (!task)",
            "\t\t\t/* No more tasks, just exit */",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * Something has shifted, try again.",
            "\t\t */",
            "\t\tput_task_struct(next_task);",
            "\t\tnext_task = task;",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tdeactivate_task(rq, next_task, 0);",
            "\tset_task_cpu(next_task, lowest_rq->cpu);",
            "\tactivate_task(lowest_rq, next_task, 0);",
            "\tresched_curr(lowest_rq);",
            "\tret = 1;",
            "",
            "\tdouble_unlock_balance(rq, lowest_rq);",
            "out:",
            "\tput_task_struct(next_task);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "pick_rt_task, find_lowest_rq, push_rt_task",
          "description": "实现实时任务选择算法、低优先级CPU搜索及强制迁移逻辑，支持异构系统下的能效优化和拓扑感知调度。",
          "similarity": 0.5497124791145325
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/rt.c",
          "start_line": 352,
          "end_line": 453,
          "content": [
            "static inline void rt_clear_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\t/* the order here really doesn't matter */",
            "\tatomic_dec(&rq->rd->rto_count);",
            "\tcpumask_clear_cpu(rq->cpu, rq->rd->rto_mask);",
            "}",
            "static inline int has_pushable_tasks(struct rq *rq)",
            "{",
            "\treturn !plist_head_empty(&rq->rt.pushable_tasks);",
            "}",
            "static inline void rt_queue_push_tasks(struct rq *rq)",
            "{",
            "\tif (!has_pushable_tasks(rq))",
            "\t\treturn;",
            "",
            "\tqueue_balance_callback(rq, &per_cpu(rt_push_head, rq->cpu), push_rt_tasks);",
            "}",
            "static inline void rt_queue_pull_task(struct rq *rq)",
            "{",
            "\tqueue_balance_callback(rq, &per_cpu(rt_pull_head, rq->cpu), pull_rt_task);",
            "}",
            "static void enqueue_pushable_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tplist_del(&p->pushable_tasks, &rq->rt.pushable_tasks);",
            "\tplist_node_init(&p->pushable_tasks, p->prio);",
            "\tplist_add(&p->pushable_tasks, &rq->rt.pushable_tasks);",
            "",
            "\t/* Update the highest prio pushable task */",
            "\tif (p->prio < rq->rt.highest_prio.next)",
            "\t\trq->rt.highest_prio.next = p->prio;",
            "",
            "\tif (!rq->rt.overloaded) {",
            "\t\trt_set_overload(rq);",
            "\t\trq->rt.overloaded = 1;",
            "\t}",
            "}",
            "static void dequeue_pushable_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tplist_del(&p->pushable_tasks, &rq->rt.pushable_tasks);",
            "",
            "\t/* Update the new highest prio pushable task */",
            "\tif (has_pushable_tasks(rq)) {",
            "\t\tp = plist_first_entry(&rq->rt.pushable_tasks,",
            "\t\t\t\t      struct task_struct, pushable_tasks);",
            "\t\trq->rt.highest_prio.next = p->prio;",
            "\t} else {",
            "\t\trq->rt.highest_prio.next = MAX_RT_PRIO-1;",
            "",
            "\t\tif (rq->rt.overloaded) {",
            "\t\t\trt_clear_overload(rq);",
            "\t\t\trq->rt.overloaded = 0;",
            "\t\t}",
            "\t}",
            "}",
            "static inline void enqueue_pushable_task(struct rq *rq, struct task_struct *p)",
            "{",
            "}",
            "static inline void dequeue_pushable_task(struct rq *rq, struct task_struct *p)",
            "{",
            "}",
            "static inline void rt_queue_push_tasks(struct rq *rq)",
            "{",
            "}",
            "static inline int on_rt_rq(struct sched_rt_entity *rt_se)",
            "{",
            "\treturn rt_se->on_rq;",
            "}",
            "static inline bool rt_task_fits_capacity(struct task_struct *p, int cpu)",
            "{",
            "\tunsigned int min_cap;",
            "\tunsigned int max_cap;",
            "\tunsigned int cpu_cap;",
            "",
            "\t/* Only heterogeneous systems can benefit from this check */",
            "\tif (!sched_asym_cpucap_active())",
            "\t\treturn true;",
            "",
            "\tmin_cap = uclamp_eff_value(p, UCLAMP_MIN);",
            "\tmax_cap = uclamp_eff_value(p, UCLAMP_MAX);",
            "",
            "\tcpu_cap = arch_scale_cpu_capacity(cpu);",
            "",
            "\treturn cpu_cap >= min(min_cap, max_cap);",
            "}",
            "static inline bool rt_task_fits_capacity(struct task_struct *p, int cpu)",
            "{",
            "\treturn true;",
            "}",
            "static inline u64 sched_rt_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tif (!rt_rq->tg)",
            "\t\treturn RUNTIME_INF;",
            "",
            "\treturn rt_rq->rt_runtime;",
            "}",
            "static inline u64 sched_rt_period(struct rt_rq *rt_rq)",
            "{",
            "\treturn ktime_to_ns(rt_rq->tg->rt_bandwidth.rt_period);",
            "}"
          ],
          "function_name": "rt_clear_overload, has_pushable_tasks, rt_queue_push_tasks, rt_queue_pull_task, enqueue_pushable_task, dequeue_pushable_task, enqueue_pushable_task, dequeue_pushable_task, rt_queue_push_tasks, on_rt_rq, rt_task_fits_capacity, rt_task_fits_capacity, sched_rt_runtime, sched_rt_period",
          "description": "维护实时任务的可推送队列，检测并处理过载状态，提供任务兼容性检查及运行时间统计查询接口。",
          "similarity": 0.5447961688041687
        }
      ]
    },
    {
      "source_file": "kernel/irq/matrix.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:03:08\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\matrix.c`\n\n---\n\n# `irq/matrix.c` 技术文档\n\n## 1. 文件概述\n\n`irq/matrix.c` 实现了一个通用的中断位图（IRQ matrix）管理机制，用于在多 CPU 系统中高效地分配和管理中断向量（或中断位）。该机制支持两类中断分配：\n\n- **普通分配（allocated）**：由设备驱动等动态申请的中断。\n- **托管分配（managed）**：由内核子系统（如 MSI/MSI-X）预先保留、按需激活的中断。\n\n该文件通过 per-CPU 的位图结构，结合全局状态跟踪，实现了跨 CPU 的中断资源分配、预留、释放和在线/离线管理，特别适用于中断向量数量有限（如 x86 的 256 个向量）的架构。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct cpumap`**：每个 CPU 的本地中断位图状态\n  - `available`：当前 CPU 可用的中断数量\n  - `allocated`：已分配的普通中断数量\n  - `managed` / `managed_allocated`：预留和已激活的托管中断数量\n  - `alloc_map[]`：记录已分配的普通中断位\n  - `managed_map[]`：记录预留的托管中断位\n  - `initialized` / `online`：CPU 初始化和在线状态\n\n- **`struct irq_matrix`**：全局中断矩阵控制结构\n  - `matrix_bits`：总位图大小（≤ `IRQ_MATRIX_BITS`）\n  - `alloc_start` / `alloc_end`：可分配范围\n  - `global_available`：全局可用中断总数\n  - `system_map[]`：系统保留位（如 APIC 自身使用的向量）\n  - `maps`：指向 per-CPU `cpumap` 的指针\n  - `scratch_map[]`：临时位图，用于分配时的合并计算\n\n### 主要函数\n\n| 函数 | 功能 |\n|------|------|\n| `irq_alloc_matrix()` | 分配并初始化一个 `irq_matrix` 结构 |\n| `irq_matrix_online()` / `irq_matrix_offline()` | 将本地 CPU 的中断矩阵置为在线/离线状态 |\n| `irq_matrix_assign_system()` | 在矩阵中保留系统级中断位（如 APIC 向量） |\n| `irq_matrix_reserve_managed()` | 在指定 CPU 掩码上为托管中断预留位 |\n| `irq_matrix_remove_managed()` | 移除托管中断的预留位 |\n| `irq_matrix_alloc_managed()` | 从预留的托管中断中分配一个实际使用的中断 |\n| `matrix_alloc_area()` | 内部辅助函数：在合并位图中查找连续空闲区域 |\n| `matrix_find_best_cpu()` / `matrix_find_best_cpu_managed()` | 选择最优 CPU（基于可用数或托管分配数最少） |\n\n## 3. 关键实现\n\n### 位图合并分配策略\n- 在分配中断时，`matrix_alloc_area()` 会临时合并三个位图：\n  1. 当前 CPU 的 `managed_map`（托管预留）\n  2. 全局 `system_map`（系统保留）\n  3. 当前 CPU 的 `alloc_map`（已分配）\n- 使用 `bitmap_find_next_zero_area()` 在合并后的位图中查找连续空闲区域，确保不会重复分配。\n\n### 托管中断（Managed IRQ）机制\n- **两阶段分配**：\n  1. **预留（reserve）**：调用 `irq_matrix_reserve_managed()` 在多个 CPU 上各预留一个位（不一定对齐）。\n  2. **激活（alloc）**：调用 `irq_matrix_alloc_managed()` 从预留位中选择一个未使用的位进行实际分配。\n- **动态 CPU 选择**：`matrix_find_best_cpu_managed()` 优先选择 `managed_allocated` 最少的 CPU，实现负载均衡。\n\n### 系统中断保留\n- `irq_matrix_assign_system()` 用于保留如 x86 的 `IRQ0_VECTOR`（时钟中断）等关键系统向量。\n- 通过 `BUG_ON()` 强制保证：系统中断只能在单 CPU 初始化阶段分配，防止运行时冲突。\n\n### 在线/离线管理\n- CPU 上线时，将其 `available` 计数加入 `global_available`。\n- CPU 离线时，从全局计数中减去，但保留其位图数据（支持重新上线）。\n\n### 跟踪与调试\n- 集成 `trace/events/irq_matrix.h`，提供分配、预留、系统保留等关键操作的 tracepoint，便于调试中断分配问题。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/bitmap.h>`：位图操作（`bitmap_set`, `bitmap_find_next_zero_area` 等）\n  - `<linux/percpu.h>`：Per-CPU 变量支持\n  - `<linux/cpu.h>`：CPU 在线/离线状态\n  - `<linux/irq.h>`：中断子系统基础定义\n  - `<trace/events/irq_matrix.h>`：自定义 tracepoint\n\n- **内核子系统**：\n  - **中断子系统**：作为底层分配器，被 `irqdomain`、MSI/MSI-X 驱动等使用。\n  - **x86 APIC 驱动**：典型使用者，用于管理 256 个中断向量的分配（如 `kernel/irq/vector.c`）。\n\n## 5. 使用场景\n\n- **x86 中断向量管理**：在 `CONFIG_X86_IO_APIC` 或 `CONFIG_X86_LOCAL_APIC` 下，用于分配 IRQ 向量（0-255），区分系统向量、普通设备中断和 MSI 中断。\n- **MSI/MSI-X 中断分配**：PCIe 设备的 MSI 中断通过托管机制预留和分配，确保每个设备在多个 CPU 上有可用向量。\n- **CPU 热插拔**：支持 CPU 动态上线/下线时的中断资源重新平衡。\n- **中断负载均衡**：通过 `matrix_find_best_cpu*` 函数，在多 CPU 间均匀分配中断，避免单 CPU 向量耗尽。",
      "similarity": 0.5519288778305054,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 78,
          "end_line": 205,
          "content": [
            "void irq_matrix_online(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tBUG_ON(cm->online);",
            "",
            "\tif (!cm->initialized) {",
            "\t\tcm->available = m->alloc_size;",
            "\t\tcm->available -= cm->managed + m->systembits_inalloc;",
            "\t\tcm->initialized = true;",
            "\t}",
            "\tm->global_available += cm->available;",
            "\tcm->online = true;",
            "\tm->online_maps++;",
            "\ttrace_irq_matrix_online(m);",
            "}",
            "void irq_matrix_offline(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\t/* Update the global available size */",
            "\tm->global_available -= cm->available;",
            "\tcm->online = false;",
            "\tm->online_maps--;",
            "\ttrace_irq_matrix_offline(m);",
            "}",
            "static unsigned int matrix_alloc_area(struct irq_matrix *m, struct cpumap *cm,",
            "\t\t\t\t      unsigned int num, bool managed)",
            "{",
            "\tunsigned int area, start = m->alloc_start;",
            "\tunsigned int end = m->alloc_end;",
            "",
            "\tbitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);",
            "\tbitmap_or(m->scratch_map, m->scratch_map, cm->alloc_map, end);",
            "\tarea = bitmap_find_next_zero_area(m->scratch_map, end, start, num, 0);",
            "\tif (area >= end)",
            "\t\treturn area;",
            "\tif (managed)",
            "\t\tbitmap_set(cm->managed_map, area, num);",
            "\telse",
            "\t\tbitmap_set(cm->alloc_map, area, num);",
            "\treturn area;",
            "}",
            "static unsigned int matrix_find_best_cpu(struct irq_matrix *m,",
            "\t\t\t\t\tconst struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, best_cpu, maxavl = 0;",
            "\tstruct cpumap *cm;",
            "",
            "\tbest_cpu = UINT_MAX;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tcm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tif (!cm->online || cm->available <= maxavl)",
            "\t\t\tcontinue;",
            "",
            "\t\tbest_cpu = cpu;",
            "\t\tmaxavl = cm->available;",
            "\t}",
            "\treturn best_cpu;",
            "}",
            "static unsigned int matrix_find_best_cpu_managed(struct irq_matrix *m,",
            "\t\t\t\t\t\tconst struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, best_cpu, allocated = UINT_MAX;",
            "\tstruct cpumap *cm;",
            "",
            "\tbest_cpu = UINT_MAX;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tcm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tif (!cm->online || cm->managed_allocated > allocated)",
            "\t\t\tcontinue;",
            "",
            "\t\tbest_cpu = cpu;",
            "\t\tallocated = cm->managed_allocated;",
            "\t}",
            "\treturn best_cpu;",
            "}",
            "void irq_matrix_assign_system(struct irq_matrix *m, unsigned int bit,",
            "\t\t\t      bool replace)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tBUG_ON(bit > m->matrix_bits);",
            "\tBUG_ON(m->online_maps > 1 || (m->online_maps && !replace));",
            "",
            "\tset_bit(bit, m->system_map);",
            "\tif (replace) {",
            "\t\tBUG_ON(!test_and_clear_bit(bit, cm->alloc_map));",
            "\t\tcm->allocated--;",
            "\t\tm->total_allocated--;",
            "\t}",
            "\tif (bit >= m->alloc_start && bit < m->alloc_end)",
            "\t\tm->systembits_inalloc++;",
            "",
            "\ttrace_irq_matrix_assign_system(bit, m);",
            "}",
            "int irq_matrix_reserve_managed(struct irq_matrix *m, const struct cpumask *msk)",
            "{",
            "\tunsigned int cpu, failed_cpu;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "\t\tunsigned int bit;",
            "",
            "\t\tbit = matrix_alloc_area(m, cm, 1, true);",
            "\t\tif (bit >= m->alloc_end)",
            "\t\t\tgoto cleanup;",
            "\t\tcm->managed++;",
            "\t\tif (cm->online) {",
            "\t\t\tcm->available--;",
            "\t\t\tm->global_available--;",
            "\t\t}",
            "\t\ttrace_irq_matrix_reserve_managed(bit, cpu, m, cm);",
            "\t}",
            "\treturn 0;",
            "cleanup:",
            "\tfailed_cpu = cpu;",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tif (cpu == failed_cpu)",
            "\t\t\tbreak;",
            "\t\tirq_matrix_remove_managed(m, cpumask_of(cpu));",
            "\t}",
            "\treturn -ENOSPC;",
            "}"
          ],
          "function_name": "irq_matrix_online, irq_matrix_offline, matrix_alloc_area, matrix_find_best_cpu, matrix_find_best_cpu_managed, irq_matrix_assign_system, irq_matrix_reserve_managed",
          "description": "实现CPU矩阵的上线/下线操作，通过bitmap操作实现中断位的分配策略，包含寻找最佳CPU的逻辑，支持系统位管理和保留区域的分配与追踪",
          "similarity": 0.5952922105789185
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 251,
          "end_line": 365,
          "content": [
            "void irq_matrix_remove_managed(struct irq_matrix *m, const struct cpumask *msk)",
            "{",
            "\tunsigned int cpu;",
            "",
            "\tfor_each_cpu(cpu, msk) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "\t\tunsigned int bit, end = m->alloc_end;",
            "",
            "\t\tif (WARN_ON_ONCE(!cm->managed))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Get managed bit which are not allocated */",
            "\t\tbitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);",
            "",
            "\t\tbit = find_first_bit(m->scratch_map, end);",
            "\t\tif (WARN_ON_ONCE(bit >= end))",
            "\t\t\tcontinue;",
            "",
            "\t\tclear_bit(bit, cm->managed_map);",
            "",
            "\t\tcm->managed--;",
            "\t\tif (cm->online) {",
            "\t\t\tcm->available++;",
            "\t\t\tm->global_available++;",
            "\t\t}",
            "\t\ttrace_irq_matrix_remove_managed(bit, cpu, m, cm);",
            "\t}",
            "}",
            "int irq_matrix_alloc_managed(struct irq_matrix *m, const struct cpumask *msk,",
            "\t\t\t     unsigned int *mapped_cpu)",
            "{",
            "\tunsigned int bit, cpu, end;",
            "\tstruct cpumap *cm;",
            "",
            "\tif (cpumask_empty(msk))",
            "\t\treturn -EINVAL;",
            "",
            "\tcpu = matrix_find_best_cpu_managed(m, msk);",
            "\tif (cpu == UINT_MAX)",
            "\t\treturn -ENOSPC;",
            "",
            "\tcm = per_cpu_ptr(m->maps, cpu);",
            "\tend = m->alloc_end;",
            "\t/* Get managed bit which are not allocated */",
            "\tbitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);",
            "\tbit = find_first_bit(m->scratch_map, end);",
            "\tif (bit >= end)",
            "\t\treturn -ENOSPC;",
            "\tset_bit(bit, cm->alloc_map);",
            "\tcm->allocated++;",
            "\tcm->managed_allocated++;",
            "\tm->total_allocated++;",
            "\t*mapped_cpu = cpu;",
            "\ttrace_irq_matrix_alloc_managed(bit, cpu, m, cm);",
            "\treturn bit;",
            "}",
            "void irq_matrix_assign(struct irq_matrix *m, unsigned int bit)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tif (WARN_ON_ONCE(bit < m->alloc_start || bit >= m->alloc_end))",
            "\t\treturn;",
            "\tif (WARN_ON_ONCE(test_and_set_bit(bit, cm->alloc_map)))",
            "\t\treturn;",
            "\tcm->allocated++;",
            "\tm->total_allocated++;",
            "\tcm->available--;",
            "\tm->global_available--;",
            "\ttrace_irq_matrix_assign(bit, smp_processor_id(), m, cm);",
            "}",
            "void irq_matrix_reserve(struct irq_matrix *m)",
            "{",
            "\tif (m->global_reserved == m->global_available)",
            "\t\tpr_warn(\"Interrupt reservation exceeds available resources\\n\");",
            "",
            "\tm->global_reserved++;",
            "\ttrace_irq_matrix_reserve(m);",
            "}",
            "void irq_matrix_remove_reserved(struct irq_matrix *m)",
            "{",
            "\tm->global_reserved--;",
            "\ttrace_irq_matrix_remove_reserved(m);",
            "}",
            "int irq_matrix_alloc(struct irq_matrix *m, const struct cpumask *msk,",
            "\t\t     bool reserved, unsigned int *mapped_cpu)",
            "{",
            "\tunsigned int cpu, bit;",
            "\tstruct cpumap *cm;",
            "",
            "\t/*",
            "\t * Not required in theory, but matrix_find_best_cpu() uses",
            "\t * for_each_cpu() which ignores the cpumask on UP .",
            "\t */",
            "\tif (cpumask_empty(msk))",
            "\t\treturn -EINVAL;",
            "",
            "\tcpu = matrix_find_best_cpu(m, msk);",
            "\tif (cpu == UINT_MAX)",
            "\t\treturn -ENOSPC;",
            "",
            "\tcm = per_cpu_ptr(m->maps, cpu);",
            "\tbit = matrix_alloc_area(m, cm, 1, false);",
            "\tif (bit >= m->alloc_end)",
            "\t\treturn -ENOSPC;",
            "\tcm->allocated++;",
            "\tcm->available--;",
            "\tm->total_allocated++;",
            "\tm->global_available--;",
            "\tif (reserved)",
            "\t\tm->global_reserved--;",
            "\t*mapped_cpu = cpu;",
            "\ttrace_irq_matrix_alloc(bit, cpu, m, cm);",
            "\treturn bit;",
            "",
            "}"
          ],
          "function_name": "irq_matrix_remove_managed, irq_matrix_alloc_managed, irq_matrix_assign, irq_matrix_reserve, irq_matrix_remove_reserved, irq_matrix_alloc",
          "description": "实现中断位的分配/回收机制，包含保留中断位的管理、跨CPU的中断分配逻辑，以及根据预留状态进行资源分配的控制流程",
          "similarity": 0.5233515501022339
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 418,
          "end_line": 483,
          "content": [
            "void irq_matrix_free(struct irq_matrix *m, unsigned int cpu,",
            "\t\t     unsigned int bit, bool managed)",
            "{",
            "\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\tif (WARN_ON_ONCE(bit < m->alloc_start || bit >= m->alloc_end))",
            "\t\treturn;",
            "",
            "\tif (WARN_ON_ONCE(!test_and_clear_bit(bit, cm->alloc_map)))",
            "\t\treturn;",
            "",
            "\tcm->allocated--;",
            "\tif(managed)",
            "\t\tcm->managed_allocated--;",
            "",
            "\tif (cm->online)",
            "\t\tm->total_allocated--;",
            "",
            "\tif (!managed) {",
            "\t\tcm->available++;",
            "\t\tif (cm->online)",
            "\t\t\tm->global_available++;",
            "\t}",
            "\ttrace_irq_matrix_free(bit, cpu, m, cm);",
            "}",
            "unsigned int irq_matrix_available(struct irq_matrix *m, bool cpudown)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\tif (!cpudown)",
            "\t\treturn m->global_available;",
            "\treturn m->global_available - cm->available;",
            "}",
            "unsigned int irq_matrix_reserved(struct irq_matrix *m)",
            "{",
            "\treturn m->global_reserved;",
            "}",
            "unsigned int irq_matrix_allocated(struct irq_matrix *m)",
            "{",
            "\tstruct cpumap *cm = this_cpu_ptr(m->maps);",
            "",
            "\treturn cm->allocated - cm->managed_allocated;",
            "}",
            "void irq_matrix_debug_show(struct seq_file *sf, struct irq_matrix *m, int ind)",
            "{",
            "\tunsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);",
            "\tint cpu;",
            "",
            "\tseq_printf(sf, \"Online bitmaps:   %6u\\n\", m->online_maps);",
            "\tseq_printf(sf, \"Global available: %6u\\n\", m->global_available);",
            "\tseq_printf(sf, \"Global reserved:  %6u\\n\", m->global_reserved);",
            "\tseq_printf(sf, \"Total allocated:  %6u\\n\", m->total_allocated);",
            "\tseq_printf(sf, \"System: %u: %*pbl\\n\", nsys, m->matrix_bits,",
            "\t\t   m->system_map);",
            "\tseq_printf(sf, \"%*s| CPU | avl | man | mac | act | vectors\\n\", ind, \" \");",
            "\tcpus_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct cpumap *cm = per_cpu_ptr(m->maps, cpu);",
            "",
            "\t\tseq_printf(sf, \"%*s %4d  %4u  %4u  %4u %4u  %*pbl\\n\", ind, \" \",",
            "\t\t\t   cpu, cm->available, cm->managed,",
            "\t\t\t   cm->managed_allocated, cm->allocated,",
            "\t\t\t   m->matrix_bits, cm->alloc_map);",
            "\t}",
            "\tcpus_read_unlock();",
            "}"
          ],
          "function_name": "irq_matrix_free, irq_matrix_available, irq_matrix_reserved, irq_matrix_allocated, irq_matrix_debug_show",
          "description": "提供中断资源的释放接口，实现全局和CPU级的资源使用统计查询，包含调试信息展示功能，通过位图操作维护系统中断位的使用状态",
          "similarity": 0.49875664710998535
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/matrix.c",
          "start_line": 1,
          "end_line": 77,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "// Copyright (C) 2017 Thomas Gleixner <tglx@linutronix.de>",
            "",
            "#include <linux/spinlock.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/bitmap.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpu.h>",
            "#include <linux/irq.h>",
            "",
            "#define IRQ_MATRIX_SIZE\t(BITS_TO_LONGS(IRQ_MATRIX_BITS))",
            "",
            "struct cpumap {",
            "\tunsigned int\t\tavailable;",
            "\tunsigned int\t\tallocated;",
            "\tunsigned int\t\tmanaged;",
            "\tunsigned int\t\tmanaged_allocated;",
            "\tbool\t\t\tinitialized;",
            "\tbool\t\t\tonline;",
            "\tunsigned long\t\talloc_map[IRQ_MATRIX_SIZE];",
            "\tunsigned long\t\tmanaged_map[IRQ_MATRIX_SIZE];",
            "};",
            "",
            "struct irq_matrix {",
            "\tunsigned int\t\tmatrix_bits;",
            "\tunsigned int\t\talloc_start;",
            "\tunsigned int\t\talloc_end;",
            "\tunsigned int\t\talloc_size;",
            "\tunsigned int\t\tglobal_available;",
            "\tunsigned int\t\tglobal_reserved;",
            "\tunsigned int\t\tsystembits_inalloc;",
            "\tunsigned int\t\ttotal_allocated;",
            "\tunsigned int\t\tonline_maps;",
            "\tstruct cpumap __percpu\t*maps;",
            "\tunsigned long\t\tscratch_map[IRQ_MATRIX_SIZE];",
            "\tunsigned long\t\tsystem_map[IRQ_MATRIX_SIZE];",
            "};",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/irq_matrix.h>",
            "",
            "/**",
            " * irq_alloc_matrix - Allocate a irq_matrix structure and initialize it",
            " * @matrix_bits:\tNumber of matrix bits must be <= IRQ_MATRIX_BITS",
            " * @alloc_start:\tFrom which bit the allocation search starts",
            " * @alloc_end:\t\tAt which bit the allocation search ends, i.e first",
            " *\t\t\tinvalid bit",
            " */",
            "__init struct irq_matrix *irq_alloc_matrix(unsigned int matrix_bits,",
            "\t\t\t\t\t   unsigned int alloc_start,",
            "\t\t\t\t\t   unsigned int alloc_end)",
            "{",
            "\tstruct irq_matrix *m;",
            "",
            "\tif (matrix_bits > IRQ_MATRIX_BITS)",
            "\t\treturn NULL;",
            "",
            "\tm = kzalloc(sizeof(*m), GFP_KERNEL);",
            "\tif (!m)",
            "\t\treturn NULL;",
            "",
            "\tm->matrix_bits = matrix_bits;",
            "\tm->alloc_start = alloc_start;",
            "\tm->alloc_end = alloc_end;",
            "\tm->alloc_size = alloc_end - alloc_start;",
            "\tm->maps = alloc_percpu(*m->maps);",
            "\tif (!m->maps) {",
            "\t\tkfree(m);",
            "\t\treturn NULL;",
            "\t}",
            "\treturn m;",
            "}",
            "",
            "/**",
            " * irq_matrix_online - Bring the local CPU matrix online",
            " * @m:\t\tMatrix pointer",
            " */"
          ],
          "function_name": null,
          "description": "定义irq_matrix结构体和相关辅助数据结构，提供irq_alloc_matrix函数用于初始化并分配irq_matrix实例，设置矩阵大小、起始结束位置等参数，并分配per-CPU的cpumap数组",
          "similarity": 0.4536626935005188
        }
      ]
    }
  ]
}