{
  "query": "多道程序设计原理与实现",
  "timestamp": "2025-12-25 23:52:25",
  "retrieved_files": [
    {
      "source_file": "kernel/bpf/mprog.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:20:50\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\mprog.c`\n\n---\n\n# bpf/mprog.c 技术文档\n\n## 1. 文件概述\n\n`bpf/mprog.c` 是 Linux 内核中用于管理 **BPF 多程序（multi-program）挂载点** 的核心实现文件。该文件提供了一套机制，允许在同一个挂载点上按顺序组织多个 BPF 程序（或通过 BPF link 关联的程序），并支持在运行时对这些程序进行 **插入、替换、删除** 等原子操作。此机制主要用于支持 **BPF 程序链（program chains）**，例如在 tc（traffic control）、XDP 或 cgroup 等子系统中实现多个 BPF 程序的有序执行。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct bpf_tuple`：封装一个 BPF 程序及其关联的 link（可选），用于统一表示待操作的目标程序。\n- `struct bpf_mprog_entry`：表示一个多程序挂载点的当前状态，包含程序数组、引用计数、版本号等。\n- `struct bpf_mprog_fp` / `struct bpf_mprog_cp`：分别表示程序的“快路径”（fast path）和“控制路径”（control path）数据，用于 RCU 安全的读写分离。\n\n### 主要函数\n\n| 函数 | 功能 |\n|------|------|\n| `bpf_mprog_link()` | 从 ID 或 FD 解析 BPF link，并验证程序类型 |\n| `bpf_mprog_prog()` | 从 ID 或 FD 解析 BPF program，并验证程序类型 |\n| `bpf_mprog_tuple_relative()` | 根据 flags（如 `BPF_F_ID`, `BPF_F_LINK`）统一解析用户传入的 `id_or_fd` 为 `bpf_tuple` |\n| `bpf_mprog_tuple_put()` | 释放 `bpf_tuple` 中持有的 program 或 link 引用 |\n| `bpf_mprog_replace()` | 在指定索引位置替换现有程序 |\n| `bpf_mprog_insert()` | 在指定位置（支持 `BPF_F_BEFORE` / `BPF_F_AFTER`）插入新程序 |\n| `bpf_mprog_delete()` | 删除指定位置的程序（支持首尾删除：`idx = -1` 或 `idx = total`） |\n| `bpf_mprog_pos_exact()` | 查找与给定 tuple 完全匹配的程序位置 |\n| `bpf_mprog_pos_before()` / `bpf_mprog_pos_after()` | 根据相对位置语义计算插入/删除目标索引 |\n| `bpf_mprog_attach()` | **核心入口函数**：根据用户 flags 执行 attach、replace 或 insert 操作 |\n| `bpf_mprog_fetch()` | （未完整实现）用于获取指定索引处的程序信息 |\n\n## 3. 关键实现\n\n### 3.1 程序与 Link 的统一抽象（`bpf_tuple`）\n通过 `bpf_tuple` 结构，将直接使用 BPF program FD/ID 与通过 BPF link 引用程序两种方式统一处理。`BPF_F_LINK` 标志决定是否从 link 解析，`BPF_F_ID` 决定输入是 ID 还是 FD。\n\n### 3.2 RCU 安全的多程序管理\n- 使用 `bpf_mprog_entry` 的 peer 机制实现 **写时复制（Copy-on-Write）**：\n  - 修改操作（insert/replace/delete）先复制当前 entry 到 peer\n  - 在 peer 上修改，最后原子切换指针\n  - 旧 entry 通过 RCU 回收，确保并发读安全\n- `bpf_mprog_read()` / `bpf_mprog_write()` 封装了对 `fp`（fast path）和 `cp`（control path）的访问\n\n### 3.3 相对位置语义支持\n- `BPF_F_BEFORE` / `BPF_F_AFTER` 允许用户指定相对于某个已有程序的位置\n- `bpf_mprog_pos_before()` / `bpf_mprog_pos_after()` 遍历当前程序列表，查找参考程序位置并返回目标索引\n- 特殊情况：当 `id_or_fd = 0` 且无 flags 时，表示在末尾插入（`idx = total`）\n\n### 3.4 原子性与一致性保障\n- `revision` 参数用于防止并发修改冲突（类似乐观锁）\n- `bpf_mprog_exists()` 检查避免重复添加同一程序\n- 所有修改操作最终通过 `*entry_new` 返回新 entry，由调用者负责发布\n\n### 3.5 边界处理\n- 插入到末尾：`idx == total`\n- 删除首元素：`idx = -1` → 转换为 `0`\n- 删除尾元素：`idx = total` → 转换为 `total - 1`\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/bpf.h>`：BPF 核心定义（`bpf_prog`, `bpf_link` 等）\n  - `<linux/bpf_mprog.h>`：多程序管理相关 API 和数据结构声明\n- **内核子系统依赖**：\n  - BPF 核心子系统（程序/链接生命周期管理）\n  - RCU 机制（用于无锁读取）\n  - 内存管理（`kmalloc`/`kfree` 用于 entry 复制）\n- **被调用方**：\n  - BPF 系统调用处理函数（如 `bpf(BPF_PROG_ATTACH, ...)` 的多程序扩展）\n  - 网络子系统（如 tc BPF 多程序支持）\n\n## 5. 使用场景\n\n1. **tc BPF 多程序链**：在同一个网络 qdisc 上挂载多个 BPF 程序，按顺序执行分类/过滤/修改操作\n2. **cgroup BPF 程序链**：在 cgroup 层级上组合多个安全或资源控制策略\n3. **动态策略更新**：运行时替换某个中间策略程序，而不中断整个链的执行\n4. **模块化 BPF 应用**：将复杂逻辑拆分为多个小程序，通过 attach 顺序组合\n5. **调试与热补丁**：临时插入诊断程序或替换有缺陷的程序版本\n\n该机制为 BPF 提供了类似“插件链”或“中间件栈”的能力，增强了 BPF 程序的组合性和动态管理能力。",
      "similarity": 0.5900787711143494,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/mprog.c",
          "start_line": 151,
          "end_line": 284,
          "content": [
            "static int bpf_mprog_delete(struct bpf_mprog_entry *entry,",
            "\t\t\t    struct bpf_mprog_entry **entry_new,",
            "\t\t\t    struct bpf_tuple *dtuple, int idx)",
            "{",
            "\tint total = bpf_mprog_total(entry);",
            "\tstruct bpf_mprog_entry *peer;",
            "",
            "\tpeer = bpf_mprog_peer(entry);",
            "\tbpf_mprog_entry_copy(peer, entry);",
            "\tif (idx == -1)",
            "\t\tidx = 0;",
            "\telse if (idx == total)",
            "\t\tidx = total - 1;",
            "\tbpf_mprog_entry_shrink(peer, idx);",
            "\tbpf_mprog_dec(peer);",
            "\tbpf_mprog_mark_for_release(peer, dtuple);",
            "\t*entry_new = peer;",
            "\treturn 0;",
            "}",
            "static int bpf_mprog_pos_exact(struct bpf_mprog_entry *entry,",
            "\t\t\t       struct bpf_tuple *tuple)",
            "{",
            "\tstruct bpf_mprog_fp *fp;",
            "\tstruct bpf_mprog_cp *cp;",
            "\tint i;",
            "",
            "\tfor (i = 0; i < bpf_mprog_total(entry); i++) {",
            "\t\tbpf_mprog_read(entry, i, &fp, &cp);",
            "\t\tif (tuple->prog == READ_ONCE(fp->prog))",
            "\t\t\treturn tuple->link == cp->link ? i : -EBUSY;",
            "\t}",
            "\treturn -ENOENT;",
            "}",
            "static int bpf_mprog_pos_before(struct bpf_mprog_entry *entry,",
            "\t\t\t\tstruct bpf_tuple *tuple)",
            "{",
            "\tstruct bpf_mprog_fp *fp;",
            "\tstruct bpf_mprog_cp *cp;",
            "\tint i;",
            "",
            "\tfor (i = 0; i < bpf_mprog_total(entry); i++) {",
            "\t\tbpf_mprog_read(entry, i, &fp, &cp);",
            "\t\tif (tuple->prog == READ_ONCE(fp->prog) &&",
            "\t\t    (!tuple->link || tuple->link == cp->link))",
            "\t\t\treturn i - 1;",
            "\t}",
            "\treturn tuple->prog ? -ENOENT : -1;",
            "}",
            "static int bpf_mprog_pos_after(struct bpf_mprog_entry *entry,",
            "\t\t\t       struct bpf_tuple *tuple)",
            "{",
            "\tstruct bpf_mprog_fp *fp;",
            "\tstruct bpf_mprog_cp *cp;",
            "\tint i;",
            "",
            "\tfor (i = 0; i < bpf_mprog_total(entry); i++) {",
            "\t\tbpf_mprog_read(entry, i, &fp, &cp);",
            "\t\tif (tuple->prog == READ_ONCE(fp->prog) &&",
            "\t\t    (!tuple->link || tuple->link == cp->link))",
            "\t\t\treturn i + 1;",
            "\t}",
            "\treturn tuple->prog ? -ENOENT : bpf_mprog_total(entry);",
            "}",
            "int bpf_mprog_attach(struct bpf_mprog_entry *entry,",
            "\t\t     struct bpf_mprog_entry **entry_new,",
            "\t\t     struct bpf_prog *prog_new, struct bpf_link *link,",
            "\t\t     struct bpf_prog *prog_old,",
            "\t\t     u32 flags, u32 id_or_fd, u64 revision)",
            "{",
            "\tstruct bpf_tuple rtuple, ntuple = {",
            "\t\t.prog = prog_new,",
            "\t\t.link = link,",
            "\t}, otuple = {",
            "\t\t.prog = prog_old,",
            "\t\t.link = link,",
            "\t};",
            "\tint ret, idx = -ERANGE, tidx;",
            "",
            "\tif (revision && revision != bpf_mprog_revision(entry))",
            "\t\treturn -ESTALE;",
            "\tif (bpf_mprog_exists(entry, prog_new))",
            "\t\treturn -EEXIST;",
            "\tret = bpf_mprog_tuple_relative(&rtuple, id_or_fd,",
            "\t\t\t\t       flags & ~BPF_F_REPLACE,",
            "\t\t\t\t       prog_new->type);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tif (flags & BPF_F_REPLACE) {",
            "\t\ttidx = bpf_mprog_pos_exact(entry, &otuple);",
            "\t\tif (tidx < 0) {",
            "\t\t\tret = tidx;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tidx = tidx;",
            "\t} else if (bpf_mprog_total(entry) == bpf_mprog_max()) {",
            "\t\tret = -ERANGE;",
            "\t\tgoto out;",
            "\t}",
            "\tif (flags & BPF_F_BEFORE) {",
            "\t\ttidx = bpf_mprog_pos_before(entry, &rtuple);",
            "\t\tif (tidx < -1 || (idx >= -1 && tidx != idx)) {",
            "\t\t\tret = tidx < -1 ? tidx : -ERANGE;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tidx = tidx;",
            "\t}",
            "\tif (flags & BPF_F_AFTER) {",
            "\t\ttidx = bpf_mprog_pos_after(entry, &rtuple);",
            "\t\tif (tidx < -1 || (idx >= -1 && tidx != idx)) {",
            "\t\t\tret = tidx < 0 ? tidx : -ERANGE;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tidx = tidx;",
            "\t}",
            "\tif (idx < -1) {",
            "\t\tif (rtuple.prog || flags) {",
            "\t\t\tret = -EINVAL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tidx = bpf_mprog_total(entry);",
            "\t\tflags = BPF_F_AFTER;",
            "\t}",
            "\tif (idx >= bpf_mprog_max()) {",
            "\t\tret = -ERANGE;",
            "\t\tgoto out;",
            "\t}",
            "\tif (flags & BPF_F_REPLACE)",
            "\t\tret = bpf_mprog_replace(entry, entry_new, &ntuple, idx);",
            "\telse",
            "\t\tret = bpf_mprog_insert(entry, entry_new, &ntuple, idx, flags);",
            "out:",
            "\tbpf_mprog_tuple_put(&rtuple);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "bpf_mprog_delete, bpf_mprog_pos_exact, bpf_mprog_pos_before, bpf_mprog_pos_after, bpf_mprog_attach",
          "description": "实现多程序条目删除、精确匹配位置查找、前后位置确定等功能，支持基于程序和链接的条件匹配与索引计算",
          "similarity": 0.6045150756835938
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/mprog.c",
          "start_line": 7,
          "end_line": 116,
          "content": [
            "static int bpf_mprog_link(struct bpf_tuple *tuple,",
            "\t\t\t  u32 id_or_fd, u32 flags,",
            "\t\t\t  enum bpf_prog_type type)",
            "{",
            "\tstruct bpf_link *link = ERR_PTR(-EINVAL);",
            "\tbool id = flags & BPF_F_ID;",
            "",
            "\tif (id)",
            "\t\tlink = bpf_link_by_id(id_or_fd);",
            "\telse if (id_or_fd)",
            "\t\tlink = bpf_link_get_from_fd(id_or_fd);",
            "\tif (IS_ERR(link))",
            "\t\treturn PTR_ERR(link);",
            "\tif (type && link->prog->type != type) {",
            "\t\tbpf_link_put(link);",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\ttuple->link = link;",
            "\ttuple->prog = link->prog;",
            "\treturn 0;",
            "}",
            "static int bpf_mprog_prog(struct bpf_tuple *tuple,",
            "\t\t\t  u32 id_or_fd, u32 flags,",
            "\t\t\t  enum bpf_prog_type type)",
            "{",
            "\tstruct bpf_prog *prog = ERR_PTR(-EINVAL);",
            "\tbool id = flags & BPF_F_ID;",
            "",
            "\tif (id)",
            "\t\tprog = bpf_prog_by_id(id_or_fd);",
            "\telse if (id_or_fd)",
            "\t\tprog = bpf_prog_get(id_or_fd);",
            "\tif (IS_ERR(prog))",
            "\t\treturn PTR_ERR(prog);",
            "\tif (type && prog->type != type) {",
            "\t\tbpf_prog_put(prog);",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\ttuple->link = NULL;",
            "\ttuple->prog = prog;",
            "\treturn 0;",
            "}",
            "static int bpf_mprog_tuple_relative(struct bpf_tuple *tuple,",
            "\t\t\t\t    u32 id_or_fd, u32 flags,",
            "\t\t\t\t    enum bpf_prog_type type)",
            "{",
            "\tbool link = flags & BPF_F_LINK;",
            "\tbool id = flags & BPF_F_ID;",
            "",
            "\tmemset(tuple, 0, sizeof(*tuple));",
            "\tif (link)",
            "\t\treturn bpf_mprog_link(tuple, id_or_fd, flags, type);",
            "\t/* If no relevant flag is set and no id_or_fd was passed, then",
            "\t * tuple link/prog is just NULLed. This is the case when before/",
            "\t * after selects first/last position without passing fd.",
            "\t */",
            "\tif (!id && !id_or_fd)",
            "\t\treturn 0;",
            "\treturn bpf_mprog_prog(tuple, id_or_fd, flags, type);",
            "}",
            "static void bpf_mprog_tuple_put(struct bpf_tuple *tuple)",
            "{",
            "\tif (tuple->link)",
            "\t\tbpf_link_put(tuple->link);",
            "\telse if (tuple->prog)",
            "\t\tbpf_prog_put(tuple->prog);",
            "}",
            "static int bpf_mprog_replace(struct bpf_mprog_entry *entry,",
            "\t\t\t     struct bpf_mprog_entry **entry_new,",
            "\t\t\t     struct bpf_tuple *ntuple, int idx)",
            "{",
            "\tstruct bpf_mprog_fp *fp;",
            "\tstruct bpf_mprog_cp *cp;",
            "\tstruct bpf_prog *oprog;",
            "",
            "\tbpf_mprog_read(entry, idx, &fp, &cp);",
            "\toprog = READ_ONCE(fp->prog);",
            "\tbpf_mprog_write(fp, cp, ntuple);",
            "\tif (!ntuple->link) {",
            "\t\tWARN_ON_ONCE(cp->link);",
            "\t\tbpf_prog_put(oprog);",
            "\t}",
            "\t*entry_new = entry;",
            "\treturn 0;",
            "}",
            "static int bpf_mprog_insert(struct bpf_mprog_entry *entry,",
            "\t\t\t    struct bpf_mprog_entry **entry_new,",
            "\t\t\t    struct bpf_tuple *ntuple, int idx, u32 flags)",
            "{",
            "\tint total = bpf_mprog_total(entry);",
            "\tstruct bpf_mprog_entry *peer;",
            "\tstruct bpf_mprog_fp *fp;",
            "\tstruct bpf_mprog_cp *cp;",
            "",
            "\tpeer = bpf_mprog_peer(entry);",
            "\tbpf_mprog_entry_copy(peer, entry);",
            "\tif (idx == total)",
            "\t\tgoto insert;",
            "\telse if (flags & BPF_F_BEFORE)",
            "\t\tidx += 1;",
            "\tbpf_mprog_entry_grow(peer, idx);",
            "insert:",
            "\tbpf_mprog_read(peer, idx, &fp, &cp);",
            "\tbpf_mprog_write(fp, cp, ntuple);",
            "\tbpf_mprog_inc(peer);",
            "\t*entry_new = peer;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "bpf_mprog_link, bpf_mprog_prog, bpf_mprog_tuple_relative, bpf_mprog_tuple_put, bpf_mprog_replace, bpf_mprog_insert",
          "description": "实现多程序链接与程序绑定逻辑，通过不同标志位选择性设置tuple中的link或prog字段，进行类型校验和引用计数管理",
          "similarity": 0.6003193855285645
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/mprog.c",
          "start_line": 297,
          "end_line": 450,
          "content": [
            "static int bpf_mprog_fetch(struct bpf_mprog_entry *entry,",
            "\t\t\t   struct bpf_tuple *tuple, int idx)",
            "{",
            "\tint total = bpf_mprog_total(entry);",
            "\tstruct bpf_mprog_cp *cp;",
            "\tstruct bpf_mprog_fp *fp;",
            "\tstruct bpf_prog *prog;",
            "\tstruct bpf_link *link;",
            "",
            "\tif (idx == -1)",
            "\t\tidx = 0;",
            "\telse if (idx == total)",
            "\t\tidx = total - 1;",
            "\tbpf_mprog_read(entry, idx, &fp, &cp);",
            "\tprog = READ_ONCE(fp->prog);",
            "\tlink = cp->link;",
            "\t/* The deletion request can either be without filled tuple in which",
            "\t * case it gets populated here based on idx, or with filled tuple",
            "\t * where the only thing we end up doing is the WARN_ON_ONCE() assert.",
            "\t * If we hit a BPF link at the given index, it must not be removed",
            "\t * from opts path.",
            "\t */",
            "\tif (link && !tuple->link)",
            "\t\treturn -EBUSY;",
            "\tWARN_ON_ONCE(tuple->prog && tuple->prog != prog);",
            "\tWARN_ON_ONCE(tuple->link && tuple->link != link);",
            "\ttuple->prog = prog;",
            "\ttuple->link = link;",
            "\treturn 0;",
            "}",
            "int bpf_mprog_detach(struct bpf_mprog_entry *entry,",
            "\t\t     struct bpf_mprog_entry **entry_new,",
            "\t\t     struct bpf_prog *prog, struct bpf_link *link,",
            "\t\t     u32 flags, u32 id_or_fd, u64 revision)",
            "{",
            "\tstruct bpf_tuple rtuple, dtuple = {",
            "\t\t.prog = prog,",
            "\t\t.link = link,",
            "\t};",
            "\tint ret, idx = -ERANGE, tidx;",
            "",
            "\tif (flags & BPF_F_REPLACE)",
            "\t\treturn -EINVAL;",
            "\tif (revision && revision != bpf_mprog_revision(entry))",
            "\t\treturn -ESTALE;",
            "\tif (!bpf_mprog_total(entry))",
            "\t\treturn -ENOENT;",
            "\tret = bpf_mprog_tuple_relative(&rtuple, id_or_fd, flags,",
            "\t\t\t\t       prog ? prog->type :",
            "\t\t\t\t       BPF_PROG_TYPE_UNSPEC);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tif (dtuple.prog) {",
            "\t\ttidx = bpf_mprog_pos_exact(entry, &dtuple);",
            "\t\tif (tidx < 0) {",
            "\t\t\tret = tidx;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tidx = tidx;",
            "\t}",
            "\tif (flags & BPF_F_BEFORE) {",
            "\t\ttidx = bpf_mprog_pos_before(entry, &rtuple);",
            "\t\tif (tidx < -1 || (idx >= -1 && tidx != idx)) {",
            "\t\t\tret = tidx < -1 ? tidx : -ERANGE;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tidx = tidx;",
            "\t}",
            "\tif (flags & BPF_F_AFTER) {",
            "\t\ttidx = bpf_mprog_pos_after(entry, &rtuple);",
            "\t\tif (tidx < -1 || (idx >= -1 && tidx != idx)) {",
            "\t\t\tret = tidx < 0 ? tidx : -ERANGE;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tidx = tidx;",
            "\t}",
            "\tif (idx < -1) {",
            "\t\tif (rtuple.prog || flags) {",
            "\t\t\tret = -EINVAL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tidx = bpf_mprog_total(entry);",
            "\t\tflags = BPF_F_AFTER;",
            "\t}",
            "\tif (idx >= bpf_mprog_max()) {",
            "\t\tret = -ERANGE;",
            "\t\tgoto out;",
            "\t}",
            "\tret = bpf_mprog_fetch(entry, &dtuple, idx);",
            "\tif (ret)",
            "\t\tgoto out;",
            "\tret = bpf_mprog_delete(entry, entry_new, &dtuple, idx);",
            "out:",
            "\tbpf_mprog_tuple_put(&rtuple);",
            "\treturn ret;",
            "}",
            "int bpf_mprog_query(const union bpf_attr *attr, union bpf_attr __user *uattr,",
            "\t\t    struct bpf_mprog_entry *entry)",
            "{",
            "\tu32 __user *uprog_flags, *ulink_flags;",
            "\tu32 __user *uprog_id, *ulink_id;",
            "\tstruct bpf_mprog_fp *fp;",
            "\tstruct bpf_mprog_cp *cp;",
            "\tstruct bpf_prog *prog;",
            "\tconst u32 flags = 0;",
            "\tu32 id, count = 0;",
            "\tu64 revision = 1;",
            "\tint i, ret = 0;",
            "",
            "\tif (attr->query.query_flags || attr->query.attach_flags)",
            "\t\treturn -EINVAL;",
            "\tif (entry) {",
            "\t\trevision = bpf_mprog_revision(entry);",
            "\t\tcount = bpf_mprog_total(entry);",
            "\t}",
            "\tif (copy_to_user(&uattr->query.attach_flags, &flags, sizeof(flags)))",
            "\t\treturn -EFAULT;",
            "\tif (copy_to_user(&uattr->query.revision, &revision, sizeof(revision)))",
            "\t\treturn -EFAULT;",
            "\tif (copy_to_user(&uattr->query.count, &count, sizeof(count)))",
            "\t\treturn -EFAULT;",
            "\tuprog_id = u64_to_user_ptr(attr->query.prog_ids);",
            "\tuprog_flags = u64_to_user_ptr(attr->query.prog_attach_flags);",
            "\tulink_id = u64_to_user_ptr(attr->query.link_ids);",
            "\tulink_flags = u64_to_user_ptr(attr->query.link_attach_flags);",
            "\tif (attr->query.count == 0 || !uprog_id || !count)",
            "\t\treturn 0;",
            "\tif (attr->query.count < count) {",
            "\t\tcount = attr->query.count;",
            "\t\tret = -ENOSPC;",
            "\t}",
            "\tfor (i = 0; i < bpf_mprog_max(); i++) {",
            "\t\tbpf_mprog_read(entry, i, &fp, &cp);",
            "\t\tprog = READ_ONCE(fp->prog);",
            "\t\tif (!prog)",
            "\t\t\tbreak;",
            "\t\tid = prog->aux->id;",
            "\t\tif (copy_to_user(uprog_id + i, &id, sizeof(id)))",
            "\t\t\treturn -EFAULT;",
            "\t\tif (uprog_flags &&",
            "\t\t    copy_to_user(uprog_flags + i, &flags, sizeof(flags)))",
            "\t\t\treturn -EFAULT;",
            "\t\tid = cp->link ? cp->link->id : 0;",
            "\t\tif (ulink_id &&",
            "\t\t    copy_to_user(ulink_id + i, &id, sizeof(id)))",
            "\t\t\treturn -EFAULT;",
            "\t\tif (ulink_flags &&",
            "\t\t    copy_to_user(ulink_flags + i, &flags, sizeof(flags)))",
            "\t\t\treturn -EFAULT;",
            "\t\tif (i + 1 == count)",
            "\t\t\tbreak;",
            "\t}",
            "\treturn ret;",
            "}"
          ],
          "function_name": "bpf_mprog_fetch, bpf_mprog_detach, bpf_mprog_query",
          "description": "实现多程序条目查询接口，支持根据索引获取运行时状态、执行删除操作，并向用户空间导出程序ID和链接ID等元信息",
          "similarity": 0.5773290395736694
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/mprog.c",
          "start_line": 1,
          "end_line": 6,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/* Copyright (c) 2023 Isovalent */",
            "",
            "#include <linux/bpf.h>",
            "#include <linux/bpf_mprog.h>",
            ""
          ],
          "function_name": null,
          "description": "声明GPL许可证并包含BPF相关头文件，为后续多程序模块实现提供基础",
          "similarity": 0.5554373860359192
        }
      ]
    },
    {
      "source_file": "kernel/locking/qspinlock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:45:55\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\qspinlock.c`\n\n---\n\n# `locking/qspinlock.c` 技术文档\n\n## 1. 文件概述\n\n`qspinlock.c` 实现了 Linux 内核中的 **排队自旋锁（Queued Spinlock）**，这是一种高性能、可扩展的自旋锁机制，旨在替代传统的 ticket spinlock。该实现基于经典的 **MCS 锁（Mellor-Crummey and Scott lock）** 算法，但针对 Linux 内核的 `spinlock_t` 限制（仅 4 字节）进行了高度优化和压缩，同时保留了原有自旋锁的 API 兼容性。其核心目标是在多核系统中减少缓存行争用（cache line bouncing），提升高并发场景下的锁性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct qnode`**  \n  每 CPU 的队列节点结构，封装了 `mcs_spinlock` 节点，并在启用 `CONFIG_PARAVIRT_SPINLOCKS` 时预留额外空间用于半虚拟化支持。每个 CPU 最多维护 `MAX_NODES=4` 个节点，对应最多 4 层嵌套上下文（task、softirq、hardirq、NMI）。\n\n- **`qnodes[MAX_NODES]`**  \n  每 CPU 对齐分配的 `qnode` 数组，确保在 64 位架构上恰好占用一个 64 字节缓存行（半虚拟化模式下占用两个）。\n\n### 关键辅助函数\n\n- **`encode_tail(cpu, idx)`**  \n  将 CPU 编号（+1 以区分无尾状态）和嵌套索引编码为 32 位尾部值，用于表示队列尾节点。\n\n- **`decode_tail(tail)`**  \n  解码尾部值，返回对应的 `mcs_spinlock` 节点指针。\n\n- **`grab_mcs_node(base, idx)`**  \n  从基础 MCS 节点指针偏移获取指定索引的节点。\n\n### 核心锁操作函数（内联）\n\n- **`clear_pending(lock)`**  \n  清除锁的 pending 位（`*,1,* → *,0,*`）。\n\n- **`clear_pending_set_locked(lock)`**  \n  同时清除 pending 位并设置 locked 位，完成锁获取（`*,1,0 → *,0,1`）。\n\n- **`xchg_tail(lock, tail)`**  \n  原子交换锁的尾部字段，返回旧尾部值，用于将当前节点加入等待队列。\n\n- **`queued_fetch_set_pending_acquire(lock)`**  \n  原子获取锁的当前值并设置 pending 位（`*,*,* → *,1,*`），带有获取语义。\n\n- **`set_locked(lock)`**  \n  直接设置 locked 位以获取锁（`*,*,0 → *,0,1`）。\n\n> 注：上述函数根据 `_Q_PENDING_BITS` 是否等于 8 分为两种实现路径，分别优化字节访问和原子位操作。\n\n## 3. 关键实现\n\n### 锁状态压缩设计\n- 传统 MCS 锁需 8 字节尾指针 + 8 字节 next 指针，但 Linux 要求 `spinlock_t` 仅占 4 字节。\n- 本实现将锁状态压缩为 32 位：\n  - **1 字节 locked 字段**：表示锁是否被持有（优化字节写性能）。\n  - **1 字节 pending 字段**：表示是否有第二个竞争者（避免频繁队列操作）。\n  - **2 字节 tail 字段**：编码 `(cpu+1, idx)`，其中 `idx ∈ [0,3]` 表示嵌套层级。\n- 通过 `cpu+1` 编码区分“无尾”（0）和“CPU 0 的尾节点”。\n\n### 快速路径优化\n- **第一个竞争者**：直接自旋在 `locked` 位，无需分配 MCS 节点。\n- **第二个竞争者**：设置 `pending` 位，避免立即进入慢速队列路径。\n- **第三个及以上竞争者**：才真正进入 MCS 队列，通过 `xchg_tail` 原子更新尾指针。\n\n### 嵌套上下文支持\n- 利用每 CPU 的 `qnodes[4]` 数组支持最多 4 层嵌套（task/softirq/hardirq/NMI）。\n- 通过 `idx` 参数在嵌套时选择不同节点，避免递归死锁。\n\n### 架构适配\n- 针对 `_Q_PENDING_BITS == 8`（如 x86）使用字节级原子操作（`WRITE_ONCE`）。\n- 其他架构使用通用原子位操作（`atomic_fetch_or_acquire` 等）。\n- 依赖架构支持 8/16 位原子操作。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/smp.h>`, `<linux/percpu.h>`：SMP 和每 CPU 变量支持。\n  - `<asm/qspinlock.h>`：架构相关的锁布局定义（如 `_Q_*_MASK`）。\n  - `\"mcs_spinlock.h\"`：MCS 锁基础实现。\n  - `\"qspinlock_stat.h\"`：锁统计信息（若启用）。\n- **配置依赖**：\n  - `CONFIG_PARAVIRT_SPINLOCKS`：半虚拟化自旋锁支持（扩展 `qnode` 大小）。\n- **架构要求**：必须支持 8/16 位原子操作（如 x86、ARM64）。\n\n## 5. 使用场景\n\n- **内核通用自旋锁**：作为 `spin_lock()`/`spin_unlock()` 的底层实现，广泛用于内核临界区保护。\n- **高并发场景**：在多核系统中显著优于传统 ticket spinlock，尤其适用于锁竞争激烈的子系统（如内存管理、调度器、文件系统）。\n- **中断上下文**：支持在 hardirq/NMI 等嵌套上下文中安全使用。\n- **半虚拟化环境**：通过 `CONFIG_PARAVIRT_SPINLOCKS` 与 hypervisor 协作减少自旋开销（如 KVM、Xen）。",
      "similarity": 0.5409688353538513,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/qspinlock.c",
          "start_line": 116,
          "end_line": 435,
          "content": [
            "static inline __pure u32 encode_tail(int cpu, int idx)",
            "{",
            "\tu32 tail;",
            "",
            "\ttail  = (cpu + 1) << _Q_TAIL_CPU_OFFSET;",
            "\ttail |= idx << _Q_TAIL_IDX_OFFSET; /* assume < 4 */",
            "",
            "\treturn tail;",
            "}",
            "static __always_inline void clear_pending(struct qspinlock *lock)",
            "{",
            "\tWRITE_ONCE(lock->pending, 0);",
            "}",
            "static __always_inline void clear_pending_set_locked(struct qspinlock *lock)",
            "{",
            "\tWRITE_ONCE(lock->locked_pending, _Q_LOCKED_VAL);",
            "}",
            "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)",
            "{",
            "\t/*",
            "\t * We can use relaxed semantics since the caller ensures that the",
            "\t * MCS node is properly initialized before updating the tail.",
            "\t */",
            "\treturn (u32)xchg_relaxed(&lock->tail,",
            "\t\t\t\t tail >> _Q_TAIL_OFFSET) << _Q_TAIL_OFFSET;",
            "}",
            "static __always_inline void clear_pending(struct qspinlock *lock)",
            "{",
            "\tatomic_andnot(_Q_PENDING_VAL, &lock->val);",
            "}",
            "static __always_inline void clear_pending_set_locked(struct qspinlock *lock)",
            "{",
            "\tatomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);",
            "}",
            "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)",
            "{",
            "\tu32 old, new, val = atomic_read(&lock->val);",
            "",
            "\tfor (;;) {",
            "\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;",
            "\t\t/*",
            "\t\t * We can use relaxed semantics since the caller ensures that",
            "\t\t * the MCS node is properly initialized before updating the",
            "\t\t * tail.",
            "\t\t */",
            "\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);",
            "\t\tif (old == val)",
            "\t\t\tbreak;",
            "",
            "\t\tval = old;",
            "\t}",
            "\treturn old;",
            "}",
            "static __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock)",
            "{",
            "\treturn atomic_fetch_or_acquire(_Q_PENDING_VAL, &lock->val);",
            "}",
            "static __always_inline void set_locked(struct qspinlock *lock)",
            "{",
            "\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);",
            "}",
            "static __always_inline void __pv_init_node(struct mcs_spinlock *node) { }",
            "static __always_inline void __pv_wait_node(struct mcs_spinlock *node,",
            "\t\t\t\t\t   struct mcs_spinlock *prev) { }",
            "static __always_inline void __pv_kick_node(struct qspinlock *lock,",
            "\t\t\t\t\t   struct mcs_spinlock *node) { }",
            "static __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,",
            "\t\t\t\t\t\t   struct mcs_spinlock *node)",
            "\t\t\t\t\t\t   { return 0; }",
            "void __lockfunc queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)",
            "{",
            "\tstruct mcs_spinlock *prev, *next, *node;",
            "\tu32 old, tail;",
            "\tint idx;",
            "",
            "\tBUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));",
            "",
            "\tif (pv_enabled())",
            "\t\tgoto pv_queue;",
            "",
            "\tif (virt_spin_lock(lock))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Wait for in-progress pending->locked hand-overs with a bounded",
            "\t * number of spins so that we guarantee forward progress.",
            "\t *",
            "\t * 0,1,0 -> 0,0,1",
            "\t */",
            "\tif (val == _Q_PENDING_VAL) {",
            "\t\tint cnt = _Q_PENDING_LOOPS;",
            "\t\tval = atomic_cond_read_relaxed(&lock->val,",
            "\t\t\t\t\t       (VAL != _Q_PENDING_VAL) || !cnt--);",
            "\t}",
            "",
            "\t/*",
            "\t * If we observe any contention; queue.",
            "\t */",
            "\tif (val & ~_Q_LOCKED_MASK)",
            "\t\tgoto queue;",
            "",
            "\t/*",
            "\t * trylock || pending",
            "\t *",
            "\t * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock",
            "\t */",
            "\tval = queued_fetch_set_pending_acquire(lock);",
            "",
            "\t/*",
            "\t * If we observe contention, there is a concurrent locker.",
            "\t *",
            "\t * Undo and queue; our setting of PENDING might have made the",
            "\t * n,0,0 -> 0,0,0 transition fail and it will now be waiting",
            "\t * on @next to become !NULL.",
            "\t */",
            "\tif (unlikely(val & ~_Q_LOCKED_MASK)) {",
            "",
            "\t\t/* Undo PENDING if we set it. */",
            "\t\tif (!(val & _Q_PENDING_MASK))",
            "\t\t\tclear_pending(lock);",
            "",
            "\t\tgoto queue;",
            "\t}",
            "",
            "\t/*",
            "\t * We're pending, wait for the owner to go away.",
            "\t *",
            "\t * 0,1,1 -> *,1,0",
            "\t *",
            "\t * this wait loop must be a load-acquire such that we match the",
            "\t * store-release that clears the locked bit and create lock",
            "\t * sequentiality; this is because not all",
            "\t * clear_pending_set_locked() implementations imply full",
            "\t * barriers.",
            "\t */",
            "\tif (val & _Q_LOCKED_MASK)",
            "\t\tsmp_cond_load_acquire(&lock->locked, !VAL);",
            "",
            "\t/*",
            "\t * take ownership and clear the pending bit.",
            "\t *",
            "\t * 0,1,0 -> 0,0,1",
            "\t */",
            "\tclear_pending_set_locked(lock);",
            "\tlockevent_inc(lock_pending);",
            "\treturn;",
            "",
            "\t/*",
            "\t * End of pending bit optimistic spinning and beginning of MCS",
            "\t * queuing.",
            "\t */",
            "queue:",
            "\tlockevent_inc(lock_slowpath);",
            "pv_queue:",
            "\tnode = this_cpu_ptr(&qnodes[0].mcs);",
            "\tidx = node->count++;",
            "\ttail = encode_tail(smp_processor_id(), idx);",
            "",
            "\ttrace_contention_begin(lock, LCB_F_SPIN);",
            "",
            "\t/*",
            "\t * 4 nodes are allocated based on the assumption that there will",
            "\t * not be nested NMIs taking spinlocks. That may not be true in",
            "\t * some architectures even though the chance of needing more than",
            "\t * 4 nodes will still be extremely unlikely. When that happens,",
            "\t * we fall back to spinning on the lock directly without using",
            "\t * any MCS node. This is not the most elegant solution, but is",
            "\t * simple enough.",
            "\t */",
            "\tif (unlikely(idx >= MAX_NODES)) {",
            "\t\tlockevent_inc(lock_no_node);",
            "\t\twhile (!queued_spin_trylock(lock))",
            "\t\t\tcpu_relax();",
            "\t\tgoto release;",
            "\t}",
            "",
            "\tnode = grab_mcs_node(node, idx);",
            "",
            "\t/*",
            "\t * Keep counts of non-zero index values:",
            "\t */",
            "\tlockevent_cond_inc(lock_use_node2 + idx - 1, idx);",
            "",
            "\t/*",
            "\t * Ensure that we increment the head node->count before initialising",
            "\t * the actual node. If the compiler is kind enough to reorder these",
            "\t * stores, then an IRQ could overwrite our assignments.",
            "\t */",
            "\tbarrier();",
            "",
            "\tnode->locked = 0;",
            "\tnode->next = NULL;",
            "\tpv_init_node(node);",
            "",
            "\t/*",
            "\t * We touched a (possibly) cold cacheline in the per-cpu queue node;",
            "\t * attempt the trylock once more in the hope someone let go while we",
            "\t * weren't watching.",
            "\t */",
            "\tif (queued_spin_trylock(lock))",
            "\t\tgoto release;",
            "",
            "\t/*",
            "\t * Ensure that the initialisation of @node is complete before we",
            "\t * publish the updated tail via xchg_tail() and potentially link",
            "\t * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\t/*",
            "\t * Publish the updated tail.",
            "\t * We have already touched the queueing cacheline; don't bother with",
            "\t * pending stuff.",
            "\t *",
            "\t * p,*,* -> n,*,*",
            "\t */",
            "\told = xchg_tail(lock, tail);",
            "\tnext = NULL;",
            "",
            "\t/*",
            "\t * if there was a previous node; link it and wait until reaching the",
            "\t * head of the waitqueue.",
            "\t */",
            "\tif (old & _Q_TAIL_MASK) {",
            "\t\tprev = decode_tail(old);",
            "",
            "\t\t/* Link @node into the waitqueue. */",
            "\t\tWRITE_ONCE(prev->next, node);",
            "",
            "\t\tpv_wait_node(node, prev);",
            "\t\tarch_mcs_spin_lock_contended(&node->locked);",
            "",
            "\t\t/*",
            "\t\t * While waiting for the MCS lock, the next pointer may have",
            "\t\t * been set by another lock waiter. We optimistically load",
            "\t\t * the next pointer & prefetch the cacheline for writing",
            "\t\t * to reduce latency in the upcoming MCS unlock operation.",
            "\t\t */",
            "\t\tnext = READ_ONCE(node->next);",
            "\t\tif (next)",
            "\t\t\tprefetchw(next);",
            "\t}",
            "",
            "\t/*",
            "\t * we're at the head of the waitqueue, wait for the owner & pending to",
            "\t * go away.",
            "\t *",
            "\t * *,x,y -> *,0,0",
            "\t *",
            "\t * this wait loop must use a load-acquire such that we match the",
            "\t * store-release that clears the locked bit and create lock",
            "\t * sequentiality; this is because the set_locked() function below",
            "\t * does not imply a full barrier.",
            "\t *",
            "\t * The PV pv_wait_head_or_lock function, if active, will acquire",
            "\t * the lock and return a non-zero value. So we have to skip the",
            "\t * atomic_cond_read_acquire() call. As the next PV queue head hasn't",
            "\t * been designated yet, there is no way for the locked value to become",
            "\t * _Q_SLOW_VAL. So both the set_locked() and the",
            "\t * atomic_cmpxchg_relaxed() calls will be safe.",
            "\t *",
            "\t * If PV isn't active, 0 will be returned instead.",
            "\t *",
            "\t */",
            "\tif ((val = pv_wait_head_or_lock(lock, node)))",
            "\t\tgoto locked;",
            "",
            "\tval = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));",
            "",
            "locked:",
            "\t/*",
            "\t * claim the lock:",
            "\t *",
            "\t * n,0,0 -> 0,0,1 : lock, uncontended",
            "\t * *,*,0 -> *,*,1 : lock, contended",
            "\t *",
            "\t * If the queue head is the only one in the queue (lock value == tail)",
            "\t * and nobody is pending, clear the tail code and grab the lock.",
            "\t * Otherwise, we only need to grab the lock.",
            "\t */",
            "",
            "\t/*",
            "\t * In the PV case we might already have _Q_LOCKED_VAL set, because",
            "\t * of lock stealing; therefore we must also allow:",
            "\t *",
            "\t * n,0,1 -> 0,0,1",
            "\t *",
            "\t * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the",
            "\t *       above wait condition, therefore any concurrent setting of",
            "\t *       PENDING will make the uncontended transition fail.",
            "\t */",
            "\tif ((val & _Q_TAIL_MASK) == tail) {",
            "\t\tif (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))",
            "\t\t\tgoto release; /* No contention */",
            "\t}",
            "",
            "\t/*",
            "\t * Either somebody is queued behind us or _Q_PENDING_VAL got set",
            "\t * which will then detect the remaining tail and queue behind us",
            "\t * ensuring we'll see a @next.",
            "\t */",
            "\tset_locked(lock);",
            "",
            "\t/*",
            "\t * contended path; wait for next if not observed yet, release.",
            "\t */",
            "\tif (!next)",
            "\t\tnext = smp_cond_load_relaxed(&node->next, (VAL));",
            "",
            "\tarch_mcs_spin_unlock_contended(&next->locked);",
            "\tpv_kick_node(lock, next);",
            "",
            "release:",
            "\ttrace_contention_end(lock, 0);",
            "",
            "\t/*",
            "\t * release the node",
            "\t */",
            "\t__this_cpu_dec(qnodes[0].mcs.count);",
            "}"
          ],
          "function_name": "encode_tail, clear_pending, clear_pending_set_locked, xchg_tail, clear_pending, clear_pending_set_locked, xchg_tail, queued_fetch_set_pending_acquire, set_locked, __pv_init_node, __pv_wait_node, __pv_kick_node, __pv_wait_head_or_lock, queued_spin_lock_slowpath",
          "description": "实现了qspinlock的核心状态转换函数和慢路径获取逻辑，包含尾部编码、挂起状态清除、锁状态设置等原子操作，并通过MCS队列处理锁竞争，支持硬中断、软中断等嵌套场景的递归控制",
          "similarity": 0.5259831547737122
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/qspinlock.c",
          "start_line": 1,
          "end_line": 115,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Queued spinlock",
            " *",
            " * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.",
            " * (C) Copyright 2013-2014,2018 Red Hat, Inc.",
            " * (C) Copyright 2015 Intel Corp.",
            " * (C) Copyright 2015 Hewlett-Packard Enterprise Development LP",
            " *",
            " * Authors: Waiman Long <longman@redhat.com>",
            " *          Peter Zijlstra <peterz@infradead.org>",
            " */",
            "",
            "#ifndef _GEN_PV_LOCK_SLOWPATH",
            "",
            "#include <linux/smp.h>",
            "#include <linux/bug.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/percpu.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/mutex.h>",
            "#include <linux/prefetch.h>",
            "#include <asm/byteorder.h>",
            "#include <asm/qspinlock.h>",
            "#include <trace/events/lock.h>",
            "",
            "/*",
            " * Include queued spinlock statistics code",
            " */",
            "#include \"qspinlock_stat.h\"",
            "",
            "/*",
            " * The basic principle of a queue-based spinlock can best be understood",
            " * by studying a classic queue-based spinlock implementation called the",
            " * MCS lock. A copy of the original MCS lock paper (\"Algorithms for Scalable",
            " * Synchronization on Shared-Memory Multiprocessors by Mellor-Crummey and",
            " * Scott\") is available at",
            " *",
            " * https://bugzilla.kernel.org/show_bug.cgi?id=206115",
            " *",
            " * This queued spinlock implementation is based on the MCS lock, however to",
            " * make it fit the 4 bytes we assume spinlock_t to be, and preserve its",
            " * existing API, we must modify it somehow.",
            " *",
            " * In particular; where the traditional MCS lock consists of a tail pointer",
            " * (8 bytes) and needs the next pointer (another 8 bytes) of its own node to",
            " * unlock the next pending (next->locked), we compress both these: {tail,",
            " * next->locked} into a single u32 value.",
            " *",
            " * Since a spinlock disables recursion of its own context and there is a limit",
            " * to the contexts that can nest; namely: task, softirq, hardirq, nmi. As there",
            " * are at most 4 nesting levels, it can be encoded by a 2-bit number. Now",
            " * we can encode the tail by combining the 2-bit nesting level with the cpu",
            " * number. With one byte for the lock value and 3 bytes for the tail, only a",
            " * 32-bit word is now needed. Even though we only need 1 bit for the lock,",
            " * we extend it to a full byte to achieve better performance for architectures",
            " * that support atomic byte write.",
            " *",
            " * We also change the first spinner to spin on the lock bit instead of its",
            " * node; whereby avoiding the need to carry a node from lock to unlock, and",
            " * preserving existing lock API. This also makes the unlock code simpler and",
            " * faster.",
            " *",
            " * N.B. The current implementation only supports architectures that allow",
            " *      atomic operations on smaller 8-bit and 16-bit data types.",
            " *",
            " */",
            "",
            "#include \"mcs_spinlock.h\"",
            "#define MAX_NODES\t4",
            "",
            "/*",
            " * On 64-bit architectures, the mcs_spinlock structure will be 16 bytes in",
            " * size and four of them will fit nicely in one 64-byte cacheline. For",
            " * pvqspinlock, however, we need more space for extra data. To accommodate",
            " * that, we insert two more long words to pad it up to 32 bytes. IOW, only",
            " * two of them can fit in a cacheline in this case. That is OK as it is rare",
            " * to have more than 2 levels of slowpath nesting in actual use. We don't",
            " * want to penalize pvqspinlocks to optimize for a rare case in native",
            " * qspinlocks.",
            " */",
            "struct qnode {",
            "\tstruct mcs_spinlock mcs;",
            "#ifdef CONFIG_PARAVIRT_SPINLOCKS",
            "\tlong reserved[2];",
            "#endif",
            "};",
            "",
            "/*",
            " * The pending bit spinning loop count.",
            " * This heuristic is used to limit the number of lockword accesses",
            " * made by atomic_cond_read_relaxed when waiting for the lock to",
            " * transition out of the \"== _Q_PENDING_VAL\" state. We don't spin",
            " * indefinitely because there's no guarantee that we'll make forward",
            " * progress.",
            " */",
            "#ifndef _Q_PENDING_LOOPS",
            "#define _Q_PENDING_LOOPS\t1",
            "#endif",
            "",
            "/*",
            " * Per-CPU queue node structures; we can never have more than 4 nested",
            " * contexts: task, softirq, hardirq, nmi.",
            " *",
            " * Exactly fits one 64-byte cacheline on a 64-bit architecture.",
            " *",
            " * PV doubles the storage and uses the second cacheline for PV state.",
            " */",
            "static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);",
            "",
            "/*",
            " * We must be able to distinguish between no-tail and the tail at 0:0,",
            " * therefore increment the cpu number by one.",
            " */",
            ""
          ],
          "function_name": null,
          "description": "定义了qspinlock的队列节点结构体qnode及其per-CPU数组，用于支持paravirtualization的锁机制，通过压缩尾部指针与锁状态到单个32位值，结合MCS锁算法实现可扩展同步",
          "similarity": 0.48971307277679443
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/qspinlock.c",
          "start_line": 590,
          "end_line": 594,
          "content": [
            "static __init int parse_nopvspin(char *arg)",
            "{",
            "\tnopvspin = true;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "parse_nopvspin",
          "description": "解析内核启动参数以禁用paravirtualization锁机制的初始化函数，通过设置nopvspin标志位控制是否启用特定的虚拟化架构优化特性",
          "similarity": 0.4221370816230774
        }
      ]
    },
    {
      "source_file": "kernel/bpf/dispatcher.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:10:06\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\dispatcher.c`\n\n---\n\n# `bpf/dispatcher.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/dispatcher.c` 实现了 BPF（Berkeley Packet Filter）调度器（dispatcher）机制，其核心目标是通过生成多路分支的直接调用代码，避免在启用 retpoline（用于缓解 Spectre v2 攻击的间接跳转防护机制）时因间接调用带来的性能开销。该调度器通过劫持一个 trampoline 函数的 `__fentry__` 入口，动态生成包含多个 BPF 程序直接调用的跳转逻辑，从而将原本的间接调用转换为高效的直接调用。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct bpf_dispatcher`：BPF 调度器主结构体，包含：\n  - `progs[BPF_DISPATCHER_MAX]`：最多支持 `BPF_DISPATCHER_MAX` 个 BPF 程序的注册项\n  - `num_progs`：当前注册的程序数量\n  - `image` 和 `rw_image`：分别指向只读可执行（RO+X）和可读写（RW）的代码页\n  - `image_off`：用于双缓冲机制的偏移量\n  - `mutex`：保护调度器状态的互斥锁\n  - `ksym`：用于内核符号管理的 ksym 结构\n- `struct bpf_dispatcher_prog`：调度器中每个 BPF 程序的注册项，包含：\n  - `prog`：指向注册的 `struct bpf_prog`\n  - `users`：引用计数\n\n### 主要函数\n- `bpf_dispatcher_find_prog()`：在调度器中查找指定 BPF 程序的注册项\n- `bpf_dispatcher_find_free()`：查找空闲的注册槽位\n- `bpf_dispatcher_add_prog()`：向调度器注册一个 BPF 程序（带引用计数）\n- `bpf_dispatcher_remove_prog()`：从调度器注销一个 BPF 程序（引用计数减一，若为零则真正移除）\n- `arch_prepare_bpf_dispatcher()`（弱符号）：架构相关函数，用于生成实际的多路分支机器码\n- `bpf_dispatcher_prepare()`：准备调度器代码镜像，收集所有已注册 BPF 程序的入口地址\n- `bpf_dispatcher_update()`：更新调度器的可执行代码，使用双缓冲机制避免执行时修改代码\n- `bpf_dispatcher_change_prog()`：主入口函数，用于将一个 BPF 程序替换为另一个，并触发调度器代码更新\n\n## 3. 关键实现\n\n### 调度器工作原理\n调度器维护一个最多包含 `BPF_DISPATCHER_MAX` 个 BPF 程序的列表。当有程序注册或注销时，调度器会重新生成一段包含所有有效程序直接调用的机器码（多路分支），并通过 trampoline 机制被调用。\n\n### 双缓冲代码更新机制\n为避免在 CPU 执行调度器代码时修改代码页导致崩溃，采用双缓冲策略：\n- 调度器分配两个半页（共一页）的内存：`image`（RO+X）和 `rw_image`（RW）\n- `image_off` 在 `0` 和 `PAGE_SIZE/2` 之间切换，指示当前活跃的半页\n- 新代码先在 `rw_image` 的非活跃半页中生成，再通过 `bpf_arch_text_copy` 原子复制到 `image` 的对应位置\n- 调用 `synchronize_rcu()` 确保所有 CPU 退出旧代码后再切换活跃半页\n\n### 引用计数管理\n每个注册的 BPF 程序通过 `refcount_t users` 管理引用计数，允许多次注册同一程序（仅增加引用计数），只有当引用归零时才真正从调度器中移除并释放程序。\n\n### 架构无关与相关分离\n- 架构无关逻辑（如程序管理、缓冲区切换）在本文件实现\n- 架构相关代码生成由 `arch_prepare_bpf_dispatcher()` 实现（通常在 `arch/xxx/net/bpf_dispatcher.c` 中），若未实现则返回 `-ENOTSUPP`\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/hash.h>`：哈希辅助（虽未直接使用，但可能为未来扩展预留）\n  - `<linux/bpf.h>` 和 `<linux/filter.h>`：BPF 核心数据结构（`bpf_prog`、`bpf_insn` 等）\n  - `<linux/static_call.h>`：静态调用优化支持（用于 `__BPF_DISPATCHER_UPDATE` 宏）\n- **内核模块依赖**：\n  - BPF JIT 子系统：通过 `bpf_prog_pack_alloc()`、`bpf_jit_alloc_exec()` 分配可执行内存\n  - 内存管理：使用 `bpf_prog_inc()`/`bpf_prog_put()` 管理 BPF 程序生命周期\n  - RCU 机制：通过 `synchronize_rcu()` 实现安全的代码更新\n  - 架构特定代码复制：依赖 `bpf_arch_text_copy()`（通常基于 `text_poke()`）\n\n## 5. 使用场景\n\n- **BPF 程序热替换**：当 attach 到 tracepoint、kprobe、perf event 等的 BPF 程序被替换时，通过 `bpf_dispatcher_change_prog()` 更新调度器，避免间接调用开销。\n- **高性能 BPF 执行路径**：在需要极致性能的场景（如网络数据包处理、系统调用跟踪），调度器可显著提升 BPF 程序调用效率，尤其在启用 retpoline 的系统上。\n- **多程序共享调度器**：多个相同类型的 BPF 程序（如多个 socket filter）可共享同一个调度器实例，统一管理直接调用入口。",
      "similarity": 0.5407694578170776,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/dispatcher.c",
          "start_line": 43,
          "end_line": 166,
          "content": [
            "static bool bpf_dispatcher_add_prog(struct bpf_dispatcher *d,",
            "\t\t\t\t    struct bpf_prog *prog)",
            "{",
            "\tstruct bpf_dispatcher_prog *entry;",
            "",
            "\tif (!prog)",
            "\t\treturn false;",
            "",
            "\tentry = bpf_dispatcher_find_prog(d, prog);",
            "\tif (entry) {",
            "\t\trefcount_inc(&entry->users);",
            "\t\treturn false;",
            "\t}",
            "",
            "\tentry = bpf_dispatcher_find_free(d);",
            "\tif (!entry)",
            "\t\treturn false;",
            "",
            "\tbpf_prog_inc(prog);",
            "\tentry->prog = prog;",
            "\trefcount_set(&entry->users, 1);",
            "\td->num_progs++;",
            "\treturn true;",
            "}",
            "static bool bpf_dispatcher_remove_prog(struct bpf_dispatcher *d,",
            "\t\t\t\t       struct bpf_prog *prog)",
            "{",
            "\tstruct bpf_dispatcher_prog *entry;",
            "",
            "\tif (!prog)",
            "\t\treturn false;",
            "",
            "\tentry = bpf_dispatcher_find_prog(d, prog);",
            "\tif (!entry)",
            "\t\treturn false;",
            "",
            "\tif (refcount_dec_and_test(&entry->users)) {",
            "\t\tentry->prog = NULL;",
            "\t\tbpf_prog_put(prog);",
            "\t\td->num_progs--;",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "int __weak arch_prepare_bpf_dispatcher(void *image, void *buf, s64 *funcs, int num_funcs)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static int bpf_dispatcher_prepare(struct bpf_dispatcher *d, void *image, void *buf)",
            "{",
            "\ts64 ips[BPF_DISPATCHER_MAX] = {}, *ipsp = &ips[0];",
            "\tint i;",
            "",
            "\tfor (i = 0; i < BPF_DISPATCHER_MAX; i++) {",
            "\t\tif (d->progs[i].prog)",
            "\t\t\t*ipsp++ = (s64)(uintptr_t)d->progs[i].prog->bpf_func;",
            "\t}",
            "\treturn arch_prepare_bpf_dispatcher(image, buf, &ips[0], d->num_progs);",
            "}",
            "static void bpf_dispatcher_update(struct bpf_dispatcher *d, int prev_num_progs)",
            "{",
            "\tvoid *new, *tmp;",
            "\tu32 noff = 0;",
            "",
            "\tif (prev_num_progs)",
            "\t\tnoff = d->image_off ^ (PAGE_SIZE / 2);",
            "",
            "\tnew = d->num_progs ? d->image + noff : NULL;",
            "\ttmp = d->num_progs ? d->rw_image + noff : NULL;",
            "\tif (new) {",
            "\t\t/* Prepare the dispatcher in d->rw_image. Then use",
            "\t\t * bpf_arch_text_copy to update d->image, which is RO+X.",
            "\t\t */",
            "\t\tif (bpf_dispatcher_prepare(d, new, tmp))",
            "\t\t\treturn;",
            "\t\tif (IS_ERR(bpf_arch_text_copy(new, tmp, PAGE_SIZE / 2)))",
            "\t\t\treturn;",
            "\t}",
            "",
            "\t__BPF_DISPATCHER_UPDATE(d, new ?: (void *)&bpf_dispatcher_nop_func);",
            "",
            "\t/* Make sure all the callers executing the previous/old half of the",
            "\t * image leave it, so following update call can modify it safely.",
            "\t */",
            "\tsynchronize_rcu();",
            "",
            "\tif (new)",
            "\t\td->image_off = noff;",
            "}",
            "void bpf_dispatcher_change_prog(struct bpf_dispatcher *d, struct bpf_prog *from,",
            "\t\t\t\tstruct bpf_prog *to)",
            "{",
            "\tbool changed = false;",
            "\tint prev_num_progs;",
            "",
            "\tif (from == to)",
            "\t\treturn;",
            "",
            "\tmutex_lock(&d->mutex);",
            "\tif (!d->image) {",
            "\t\td->image = bpf_prog_pack_alloc(PAGE_SIZE, bpf_jit_fill_hole_with_zero);",
            "\t\tif (!d->image)",
            "\t\t\tgoto out;",
            "\t\td->rw_image = bpf_jit_alloc_exec(PAGE_SIZE);",
            "\t\tif (!d->rw_image) {",
            "\t\t\tbpf_prog_pack_free(d->image, PAGE_SIZE);",
            "\t\t\td->image = NULL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tbpf_image_ksym_init(d->image, PAGE_SIZE, &d->ksym);",
            "\t\tbpf_image_ksym_add(&d->ksym);",
            "\t}",
            "",
            "\tprev_num_progs = d->num_progs;",
            "\tchanged |= bpf_dispatcher_remove_prog(d, from);",
            "\tchanged |= bpf_dispatcher_add_prog(d, to);",
            "",
            "\tif (!changed)",
            "\t\tgoto out;",
            "",
            "\tbpf_dispatcher_update(d, prev_num_progs);",
            "out:",
            "\tmutex_unlock(&d->mutex);",
            "}"
          ],
          "function_name": "bpf_dispatcher_add_prog, bpf_dispatcher_remove_prog, arch_prepare_bpf_dispatcher, bpf_dispatcher_prepare, bpf_dispatcher_update, bpf_dispatcher_change_prog",
          "description": "该代码块实现了BPF调度器的增删改逻辑及图像更新。bpf_dispatcher_add_prog/bpf_dispatcher_remove_prog管理程序注册与解注册，arch_prepare_bpf_dispatcher为架构弱符号接口，bpf_dispatcher_prepare收集程序函数地址，bpf_dispatcher_update执行安全的图像更新，bpf_dispatcher_change_prog协调程序替换流程并触发更新操作。",
          "similarity": 0.4904322624206543
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/dispatcher.c",
          "start_line": 1,
          "end_line": 42,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright(c) 2019 Intel Corporation. */",
            "",
            "#include <linux/hash.h>",
            "#include <linux/bpf.h>",
            "#include <linux/filter.h>",
            "#include <linux/static_call.h>",
            "",
            "/* The BPF dispatcher is a multiway branch code generator. The",
            " * dispatcher is a mechanism to avoid the performance penalty of an",
            " * indirect call, which is expensive when retpolines are enabled. A",
            " * dispatch client registers a BPF program into the dispatcher, and if",
            " * there is available room in the dispatcher a direct call to the BPF",
            " * program will be generated. All calls to the BPF programs called via",
            " * the dispatcher will then be a direct call, instead of an",
            " * indirect. The dispatcher hijacks a trampoline function it via the",
            " * __fentry__ of the trampoline. The trampoline function has the",
            " * following signature:",
            " *",
            " * unsigned int trampoline(const void *ctx, const struct bpf_insn *insnsi,",
            " *                         unsigned int (*bpf_func)(const void *,",
            " *                                                  const struct bpf_insn *));",
            " */",
            "",
            "static struct bpf_dispatcher_prog *bpf_dispatcher_find_prog(",
            "\tstruct bpf_dispatcher *d, struct bpf_prog *prog)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < BPF_DISPATCHER_MAX; i++) {",
            "\t\tif (prog == d->progs[i].prog)",
            "\t\t\treturn &d->progs[i];",
            "\t}",
            "\treturn NULL;",
            "}",
            "",
            "static struct bpf_dispatcher_prog *bpf_dispatcher_find_free(",
            "\tstruct bpf_dispatcher *d)",
            "{",
            "\treturn bpf_dispatcher_find_prog(d, NULL);",
            "}",
            ""
          ],
          "function_name": null,
          "description": "该代码块定义了BPF调度器的辅助函数，用于查找已注册的BPF程序或空闲槽位。bpf_dispatcher_find_prog遍历调度器槽位以匹配指定程序，返回对应项指针；bpf_dispatcher_find_free用于定位未占用的槽位，为后续程序注册做准备。",
          "similarity": 0.4786199629306793
        }
      ]
    }
  ]
}