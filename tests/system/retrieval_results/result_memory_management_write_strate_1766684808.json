{
  "query": "memory management write strategies",
  "timestamp": "2025-12-26 01:46:48",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.5929845571517944,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.620250940322876
        },
        {
          "chunk_id": 10,
          "file_path": "mm/mempolicy.c",
          "start_line": 1735,
          "end_line": 1838,
          "content": [
            "static long kernel_set_mempolicy(int mode, const unsigned long __user *nmask,",
            "\t\t\t\t unsigned long maxnode)",
            "{",
            "\tunsigned short mode_flags;",
            "\tnodemask_t nodes;",
            "\tint lmode = mode;",
            "\tint err;",
            "",
            "\terr = sanitize_mpol_flags(&lmode, &mode_flags);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\terr = get_nodes(&nodes, nmask, maxnode);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\treturn do_set_mempolicy(lmode, mode_flags, &nodes);",
            "}",
            "static int kernel_migrate_pages(pid_t pid, unsigned long maxnode,",
            "\t\t\t\tconst unsigned long __user *old_nodes,",
            "\t\t\t\tconst unsigned long __user *new_nodes)",
            "{",
            "\tstruct mm_struct *mm = NULL;",
            "\tstruct task_struct *task;",
            "\tnodemask_t task_nodes;",
            "\tint err;",
            "\tnodemask_t *old;",
            "\tnodemask_t *new;",
            "\tNODEMASK_SCRATCH(scratch);",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\told = &scratch->mask1;",
            "\tnew = &scratch->mask2;",
            "",
            "\terr = get_nodes(old, old_nodes, maxnode);",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\terr = get_nodes(new, new_nodes, maxnode);",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\t/* Find the mm_struct */",
            "\trcu_read_lock();",
            "\ttask = pid ? find_task_by_vpid(pid) : current;",
            "\tif (!task) {",
            "\t\trcu_read_unlock();",
            "\t\terr = -ESRCH;",
            "\t\tgoto out;",
            "\t}",
            "\tget_task_struct(task);",
            "",
            "\terr = -EINVAL;",
            "",
            "\t/*",
            "\t * Check if this process has the right to modify the specified process.",
            "\t * Use the regular \"ptrace_may_access()\" checks.",
            "\t */",
            "\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {",
            "\t\trcu_read_unlock();",
            "\t\terr = -EPERM;",
            "\t\tgoto out_put;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\ttask_nodes = cpuset_mems_allowed(task);",
            "\t/* Is the user allowed to access the target nodes? */",
            "\tif (!nodes_subset(*new, task_nodes) && !capable(CAP_SYS_NICE)) {",
            "\t\terr = -EPERM;",
            "\t\tgoto out_put;",
            "\t}",
            "",
            "\ttask_nodes = cpuset_mems_allowed(current);",
            "\tnodes_and(*new, *new, task_nodes);",
            "\tif (nodes_empty(*new))",
            "\t\tgoto out_put;",
            "",
            "\terr = security_task_movememory(task);",
            "\tif (err)",
            "\t\tgoto out_put;",
            "",
            "\tmm = get_task_mm(task);",
            "\tput_task_struct(task);",
            "",
            "\tif (!mm) {",
            "\t\terr = -EINVAL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\terr = do_migrate_pages(mm, old, new,",
            "\t\tcapable(CAP_SYS_NICE) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE);",
            "",
            "\tmmput(mm);",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "",
            "\treturn err;",
            "",
            "out_put:",
            "\tput_task_struct(task);",
            "\tgoto out;",
            "}"
          ],
          "function_name": "kernel_set_mempolicy, kernel_migrate_pages",
          "description": "kernel_set_mempolicy 设置进程的内存放置策略，通过sanitize_mpol_flags验证模式标志并调用do_set_mempolicy应用策略；kernel_migrate_pages 实现页面迁移，检查目标进程权限，限制迁移节点范围，并调用do_migrate_pages进行实际迁移操作。",
          "similarity": 0.5888751149177551
        },
        {
          "chunk_id": 12,
          "file_path": "mm/mempolicy.c",
          "start_line": 2024,
          "end_line": 2135,
          "content": [
            "static unsigned int interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int nid;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "\t/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnid = next_node_in(current->il_prev, policy->nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\tif (nid < MAX_NUMNODES)",
            "\t\tcurrent->il_prev = nid;",
            "\treturn nid;",
            "}",
            "unsigned int mempolicy_slab_node(void)",
            "{",
            "\tstruct mempolicy *policy;",
            "\tint node = numa_mem_id();",
            "",
            "\tif (!in_task())",
            "\t\treturn node;",
            "",
            "\tpolicy = current->mempolicy;",
            "\tif (!policy)",
            "\t\treturn node;",
            "",
            "\tswitch (policy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\treturn first_node(policy->nodes);",
            "",
            "\tcase MPOL_INTERLEAVE:",
            "\t\treturn interleave_nodes(policy);",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn weighted_interleave_nodes(policy);",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t{",
            "\t\tstruct zoneref *z;",
            "",
            "\t\t/*",
            "\t\t * Follow bind policy behavior and start allocation at the",
            "\t\t * first node.",
            "\t\t */",
            "\t\tstruct zonelist *zonelist;",
            "\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);",
            "\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];",
            "\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,",
            "\t\t\t\t\t\t\t&policy->nodes);",
            "\t\treturn z->zone ? zone_to_nid(z->zone) : node;",
            "\t}",
            "\tcase MPOL_LOCAL:",
            "\t\treturn node;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static unsigned int read_once_policy_nodemask(struct mempolicy *pol,",
            "\t\t\t\t\t      nodemask_t *mask)",
            "{",
            "\t/*",
            "\t * barrier stabilizes the nodemask locally so that it can be iterated",
            "\t * over safely without concern for changes. Allocators validate node",
            "\t * selection does not violate mems_allowed, so this is safe.",
            "\t */",
            "\tbarrier();",
            "\tmemcpy(mask, &pol->nodes, sizeof(nodemask_t));",
            "\tbarrier();",
            "\treturn nodes_weight(*mask);",
            "}",
            "static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nr_nodes;",
            "\tu8 *table = NULL;",
            "\tunsigned int weight_total = 0;",
            "\tu8 weight;",
            "\tint nid = 0;",
            "",
            "\tnr_nodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nr_nodes)",
            "\t\treturn numa_node_id();",
            "",
            "\trcu_read_lock();",
            "",
            "\tstate = rcu_dereference(wi_state);",
            "\t/* Uninitialized wi_state means we should assume all weights are 1 */",
            "\tif (state)",
            "\t\ttable = state->iw_table;",
            "",
            "\t/* calculate the total weight */",
            "\tfor_each_node_mask(nid, nodemask)",
            "\t\tweight_total += table ? table[nid] : 1;",
            "",
            "\t/* Calculate the node offset based on totals */",
            "\ttarget = ilx % weight_total;",
            "\tnid = first_node(nodemask);",
            "\twhile (target) {",
            "\t\t/* detect system default usage */",
            "\t\tweight = table ? table[nid] : 1;",
            "\t\tif (target < weight)",
            "\t\t\tbreak;",
            "\t\ttarget -= weight;",
            "\t\tnid = next_node_in(nid, nodemask);",
            "\t}",
            "\trcu_read_unlock();",
            "\treturn nid;",
            "}"
          ],
          "function_name": "interleave_nodes, mempolicy_slab_node, read_once_policy_nodemask, weighted_interleave_nid",
          "description": "interleave_nodes 计算交错分配的下一个节点；mempolicy_slab_node 根据内存策略返回Slab分配的节点；read_once_policy_nodemask 安全读取策略节点掩码；weighted_interleave_nid 基于权重计算加权交错分配的目标节点。",
          "similarity": 0.58316570520401
        },
        {
          "chunk_id": 19,
          "file_path": "mm/mempolicy.c",
          "start_line": 3268,
          "end_line": 3394,
          "content": [
            "void numa_default_policy(void)",
            "{",
            "\tdo_set_mempolicy(MPOL_DEFAULT, 0, NULL);",
            "}",
            "int mpol_parse_str(char *str, struct mempolicy **mpol)",
            "{",
            "\tstruct mempolicy *new = NULL;",
            "\tunsigned short mode_flags;",
            "\tnodemask_t nodes;",
            "\tchar *nodelist = strchr(str, ':');",
            "\tchar *flags = strchr(str, '=');",
            "\tint err = 1, mode;",
            "",
            "\tif (flags)",
            "\t\t*flags++ = '\\0';\t/* terminate mode string */",
            "",
            "\tif (nodelist) {",
            "\t\t/* NUL-terminate mode or flags string */",
            "\t\t*nodelist++ = '\\0';",
            "\t\tif (nodelist_parse(nodelist, nodes))",
            "\t\t\tgoto out;",
            "\t\tif (!nodes_subset(nodes, node_states[N_MEMORY]))",
            "\t\t\tgoto out;",
            "\t} else",
            "\t\tnodes_clear(nodes);",
            "",
            "\tmode = match_string(policy_modes, MPOL_MAX, str);",
            "\tif (mode < 0)",
            "\t\tgoto out;",
            "",
            "\tswitch (mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\t/*",
            "\t\t * Insist on a nodelist of one node only, although later",
            "\t\t * we use first_node(nodes) to grab a single node, so here",
            "\t\t * nodelist (or nodes) cannot be empty.",
            "\t\t */",
            "\t\tif (nodelist) {",
            "\t\t\tchar *rest = nodelist;",
            "\t\t\twhile (isdigit(*rest))",
            "\t\t\t\trest++;",
            "\t\t\tif (*rest)",
            "\t\t\t\tgoto out;",
            "\t\t\tif (nodes_empty(nodes))",
            "\t\t\t\tgoto out;",
            "\t\t}",
            "\t\tbreak;",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t/*",
            "\t\t * Default to online nodes with memory if no nodelist",
            "\t\t */",
            "\t\tif (!nodelist)",
            "\t\t\tnodes = node_states[N_MEMORY];",
            "\t\tbreak;",
            "\tcase MPOL_LOCAL:",
            "\t\t/*",
            "\t\t * Don't allow a nodelist;  mpol_new() checks flags",
            "\t\t */",
            "\t\tif (nodelist)",
            "\t\t\tgoto out;",
            "\t\tbreak;",
            "\tcase MPOL_DEFAULT:",
            "\t\t/*",
            "\t\t * Insist on a empty nodelist",
            "\t\t */",
            "\t\tif (!nodelist)",
            "\t\t\terr = 0;",
            "\t\tgoto out;",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\t\t/*",
            "\t\t * Insist on a nodelist",
            "\t\t */",
            "\t\tif (!nodelist)",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tmode_flags = 0;",
            "\tif (flags) {",
            "\t\t/*",
            "\t\t * Currently, we only support two mutually exclusive",
            "\t\t * mode flags.",
            "\t\t */",
            "\t\tif (!strcmp(flags, \"static\"))",
            "\t\t\tmode_flags |= MPOL_F_STATIC_NODES;",
            "\t\telse if (!strcmp(flags, \"relative\"))",
            "\t\t\tmode_flags |= MPOL_F_RELATIVE_NODES;",
            "\t\telse",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tnew = mpol_new(mode, mode_flags, &nodes);",
            "\tif (IS_ERR(new))",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * Save nodes for mpol_to_str() to show the tmpfs mount options",
            "\t * for /proc/mounts, /proc/pid/mounts and /proc/pid/mountinfo.",
            "\t */",
            "\tif (mode != MPOL_PREFERRED) {",
            "\t\tnew->nodes = nodes;",
            "\t} else if (nodelist) {",
            "\t\tnodes_clear(new->nodes);",
            "\t\tnode_set(first_node(nodes), new->nodes);",
            "\t} else {",
            "\t\tnew->mode = MPOL_LOCAL;",
            "\t}",
            "",
            "\t/*",
            "\t * Save nodes for contextualization: this will be used to \"clone\"",
            "\t * the mempolicy in a specific context [cpuset] at a later time.",
            "\t */",
            "\tnew->w.user_nodemask = nodes;",
            "",
            "\terr = 0;",
            "",
            "out:",
            "\t/* Restore string for error message */",
            "\tif (nodelist)",
            "\t\t*--nodelist = ':';",
            "\tif (flags)",
            "\t\t*--flags = '=';",
            "\tif (!err)",
            "\t\t*mpol = new;",
            "\treturn err;",
            "}"
          ],
          "function_name": "numa_default_policy, mpol_parse_str",
          "description": "numa_default_policy设置系统默认内存策略为默认模式，mpol_parse_str解析内存策略字符串，根据模式类型处理节点列表和标志位，构建对应的内存策略结构体。",
          "similarity": 0.5826463103294373
        },
        {
          "chunk_id": 17,
          "file_path": "mm/mempolicy.c",
          "start_line": 2969,
          "end_line": 3093,
          "content": [
            "void mpol_put_task_policy(struct task_struct *task)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\ttask_lock(task);",
            "\tpol = task->mempolicy;",
            "\ttask->mempolicy = NULL;",
            "\ttask_unlock(task);",
            "\tmpol_put(pol);",
            "}",
            "static void sp_delete(struct shared_policy *sp, struct sp_node *n)",
            "{",
            "\trb_erase(&n->nd, &sp->root);",
            "\tsp_free(n);",
            "}",
            "static void sp_node_init(struct sp_node *node, unsigned long start,",
            "\t\t\tunsigned long end, struct mempolicy *pol)",
            "{",
            "\tnode->start = start;",
            "\tnode->end = end;",
            "\tnode->policy = pol;",
            "}",
            "static int shared_policy_replace(struct shared_policy *sp, pgoff_t start,",
            "\t\t\t\t pgoff_t end, struct sp_node *new)",
            "{",
            "\tstruct sp_node *n;",
            "\tstruct sp_node *n_new = NULL;",
            "\tstruct mempolicy *mpol_new = NULL;",
            "\tint ret = 0;",
            "",
            "restart:",
            "\twrite_lock(&sp->lock);",
            "\tn = sp_lookup(sp, start, end);",
            "\t/* Take care of old policies in the same range. */",
            "\twhile (n && n->start < end) {",
            "\t\tstruct rb_node *next = rb_next(&n->nd);",
            "\t\tif (n->start >= start) {",
            "\t\t\tif (n->end <= end)",
            "\t\t\t\tsp_delete(sp, n);",
            "\t\t\telse",
            "\t\t\t\tn->start = end;",
            "\t\t} else {",
            "\t\t\t/* Old policy spanning whole new range. */",
            "\t\t\tif (n->end > end) {",
            "\t\t\t\tif (!n_new)",
            "\t\t\t\t\tgoto alloc_new;",
            "",
            "\t\t\t\t*mpol_new = *n->policy;",
            "\t\t\t\tatomic_set(&mpol_new->refcnt, 1);",
            "\t\t\t\tsp_node_init(n_new, end, n->end, mpol_new);",
            "\t\t\t\tn->end = start;",
            "\t\t\t\tsp_insert(sp, n_new);",
            "\t\t\t\tn_new = NULL;",
            "\t\t\t\tmpol_new = NULL;",
            "\t\t\t\tbreak;",
            "\t\t\t} else",
            "\t\t\t\tn->end = start;",
            "\t\t}",
            "\t\tif (!next)",
            "\t\t\tbreak;",
            "\t\tn = rb_entry(next, struct sp_node, nd);",
            "\t}",
            "\tif (new)",
            "\t\tsp_insert(sp, new);",
            "\twrite_unlock(&sp->lock);",
            "\tret = 0;",
            "",
            "err_out:",
            "\tif (mpol_new)",
            "\t\tmpol_put(mpol_new);",
            "\tif (n_new)",
            "\t\tkmem_cache_free(sn_cache, n_new);",
            "",
            "\treturn ret;",
            "",
            "alloc_new:",
            "\twrite_unlock(&sp->lock);",
            "\tret = -ENOMEM;",
            "\tn_new = kmem_cache_alloc(sn_cache, GFP_KERNEL);",
            "\tif (!n_new)",
            "\t\tgoto err_out;",
            "\tmpol_new = kmem_cache_alloc(policy_cache, GFP_KERNEL);",
            "\tif (!mpol_new)",
            "\t\tgoto err_out;",
            "\tatomic_set(&mpol_new->refcnt, 1);",
            "\tgoto restart;",
            "}",
            "void mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol)",
            "{",
            "\tint ret;",
            "",
            "\tsp->root = RB_ROOT;\t\t/* empty tree == default mempolicy */",
            "\trwlock_init(&sp->lock);",
            "",
            "\tif (mpol) {",
            "\t\tstruct sp_node *sn;",
            "\t\tstruct mempolicy *npol;",
            "\t\tNODEMASK_SCRATCH(scratch);",
            "",
            "\t\tif (!scratch)",
            "\t\t\tgoto put_mpol;",
            "",
            "\t\t/* contextualize the tmpfs mount point mempolicy to this file */",
            "\t\tnpol = mpol_new(mpol->mode, mpol->flags, &mpol->w.user_nodemask);",
            "\t\tif (IS_ERR(npol))",
            "\t\t\tgoto free_scratch; /* no valid nodemask intersection */",
            "",
            "\t\ttask_lock(current);",
            "\t\tret = mpol_set_nodemask(npol, &mpol->w.user_nodemask, scratch);",
            "\t\ttask_unlock(current);",
            "\t\tif (ret)",
            "\t\t\tgoto put_npol;",
            "",
            "\t\t/* alloc node covering entire file; adds ref to file's npol */",
            "\t\tsn = sp_alloc(0, MAX_LFS_FILESIZE >> PAGE_SHIFT, npol);",
            "\t\tif (sn)",
            "\t\t\tsp_insert(sp, sn);",
            "put_npol:",
            "\t\tmpol_put(npol);\t/* drop initial ref on file's npol */",
            "free_scratch:",
            "\t\tNODEMASK_SCRATCH_FREE(scratch);",
            "put_mpol:",
            "\t\tmpol_put(mpol);\t/* drop our incoming ref on sb mpol */",
            "\t}",
            "}"
          ],
          "function_name": "mpol_put_task_policy, sp_delete, sp_node_init, shared_policy_replace, mpol_shared_policy_init",
          "description": "mpol_put_task_policy释放任务级内存策略引用，sp_delete从RB树删除节点并回收资源，sp_node_init初始化共享策略节点，shared_policy_replace替换共享策略区间并处理节点分裂，mpol_shared_policy_init初始化共享策略结构体并设置初始策略。",
          "similarity": 0.5737452507019043
        }
      ]
    },
    {
      "source_file": "mm/page-writeback.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:09\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page-writeback.c`\n\n---\n\n# page-writeback.c 技术文档\n\n## 1. 文件概述\n\n`page-writeback.c` 是 Linux 内核内存管理子系统（MM）中的核心文件，负责实现**脏页回写（dirty page writeback）机制**。该机制用于控制和协调将修改过的页面（即“脏页”）从内存写回到持久化存储（如磁盘）的过程，以确保数据一致性、防止内存耗尽，并在系统负载与 I/O 带宽之间取得平衡。\n\n该文件主要提供以下功能：\n- 脏页数量的全局与每 BDI（Backing Device Info）级别的阈值管理\n- 脏页生成速率的动态限流（throttling）\n- 后台回写线程（如 `writeback` 线程）的触发逻辑\n- 支持基于 cgroup 的内存回写控制（当启用 `CONFIG_CGROUP_WRITEBACK` 时）\n- 与 `/proc/sys/vm` 中可调参数的交互接口\n\n## 2. 核心功能\n\n### 主要全局变量（可通过 sysctl 调整）\n| 变量名 | 默认值 | 说明 |\n|--------|--------|------|\n| `dirty_background_ratio` | 10 | 当脏页占可用内存比例达到此值时，启动后台回写 |\n| `vm_dirty_ratio` | 20 | 脏页比例硬上限，超过则阻塞写进程进行同步回写 |\n| `dirty_background_bytes` | 0 | 以字节为单位指定后台回写阈值（优先级高于 ratio） |\n| `vm_dirty_bytes` | 0 | 以字节为单位指定脏页硬上限（优先级高于 ratio） |\n| `dirty_writeback_interval` | 500 (5秒) | 后台回写线程的唤醒间隔（单位：厘秒） |\n| `dirty_expire_interval` | 3000 (30秒) | 脏页最大存活时间，超时强制回写 |\n| `laptop_mode` | 0 | 笔记本模式开关，减少磁盘活动以省电 |\n| `ratelimit_pages` | 32 | 每 CPU 脏页速率限制阈值 |\n\n### 关键数据结构\n- **`struct wb_domain`**  \n  回写域（writeback domain），用于聚合多个 BDI 的回写状态，支持全局或 per-memcg 的回写控制。\n  \n- **`struct dirty_throttle_control` (dtc)**  \n  脏页限流控制上下文，包含：\n  - `avail`：当前可脏化的内存总量\n  - `dirty`：当前脏页数量\n  - `thresh` / `bg_thresh`：硬/软回写阈值\n  - `wb_dirty` / `wb_thresh` / `wb_bg_thresh`：per-BDI 级别的对应值\n  - `pos_ratio`：用于计算回写速率的比例因子\n\n- **条件编译支持**  \n  通过 `CONFIG_CGROUP_WRITEBACK` 区分是否支持 memcg 级别的回写控制，提供 `GDTC_INIT`、`MDTC_INIT` 等宏及辅助函数（如 `mdtc_valid()`、`wb_min_max_ratio()`）。\n\n### 核心辅助函数（部分在截断代码中未完整显示）\n- `node_dirtyable_memory()`：计算指定 NUMA 节点中可用于脏页缓存的内存总量（包括空闲页 + 文件缓存页 - 保留页）。\n- `balance_dirty_pages()`：主限流函数，在进程写入时被调用，根据当前脏页水位决定是否休眠或触发回写。\n- `balance_dirty_pages_ratelimited()`：带速率限制的脏页平衡入口，避免频繁调用开销。\n\n## 3. 关键实现\n\n### 脏页阈值计算逻辑\n- 脏页上限基于 **“dirtyable memory”** 计算，即 `(free pages + file cache pages - kernel reserves)`。\n- 支持两种配置方式：**百分比（ratio）** 或 **绝对字节数（bytes）**，后者优先。\n- 当启用 `vm_highmem_is_dirtyable` 时，highmem 区域的空闲页也计入 dirtyable memory。\n\n### 动态限流机制\n- 使用 **`MAX_PAUSE`（最大 200ms）** 限制单次 `balance_dirty_pages()` 的休眠时间。\n- 引入 **`DIRTY_POLL_THRESH`（128KB）** 作为调用间隔优化阈值：若脏页增长过快，则提升休眠时间至最大值。\n- 通过 **`BANDWIDTH_INTERVAL`（200ms）** 动态估算存储设备的写入带宽，用于调整回写速率。\n\n### cgroup writeback 支持\n- 在 `CONFIG_CGROUP_WRITEBACK` 启用时：\n  - 每个 memcg 有独立的 `wb_domain`\n  - `dirty_throttle_control` 可关联全局（gdtc）或 memcg（mdtc）上下文\n  - BDI 的 min/max_ratio 根据其实际带宽动态缩放，实现公平分配\n\n### 老化与完成计数\n- 使用 `fprop_local_percpu` 结构跟踪每个 BDI 的回写完成情况。\n- `VM_COMPLETIONS_PERIOD_LEN`（3 秒）定义了回写完成率的老化周期，影响带宽估算的响应速度。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/swap.h>`、`<linux/pagevec.h>` 等，与页分配、回收机制紧密集成。\n- **VFS 层**：通过 `<linux/fs.h>`、`<linux/pagemap.h>` 与 address_space 和 inode 交互。\n- **块设备层**：通过 `<linux/blkdev.h>`、`<linux/backing-dev.h>` 获取 BDI 信息和 I/O 能力。\n- **调度与同步**：使用 `<linux/sched.h>`、`<linux/spinlock.h>`、`<linux/timer.h>` 实现休眠、锁和定时器。\n- **追踪系统**：集成 `<trace/events/writeback.h>` 提供回写事件追踪点。\n- **内部头文件**：包含 `\"internal.h\"` 获取 MM 子系统内部接口。\n\n## 5. 使用场景\n\n1. **用户空间写入文件**  \n   当进程通过 `write()` 修改文件页时，页被标记为脏，随后调用 `balance_dirty_pages_ratelimited()` 触发脏页控制。\n\n2. **内存压力下的页面回收**  \n   kswapd 或直接回收路径在需要释放内存时，可能调用回写逻辑清理脏页。\n\n3. **定期后台回写**  \n   `writeback` 内核线程按 `dirty_writeback_interval` 周期唤醒，检查并回写超过 `dirty_expire_interval` 的脏页。\n\n4. **系统关闭或 sync 调用**  \n   虽然主要同步逻辑在其他文件，但本文件提供的阈值和状态是决策基础。\n\n5. **容器环境中的资源隔离**  \n   启用 cgroup writeback 后，不同 memcg 的脏页回写相互隔离，避免一个容器的大量写入影响其他容器性能。\n\n6. **笔记本省电模式**  \n   当 `laptop_mode` 启用时，延迟回写以减少磁盘旋转时间，延长电池寿命。",
      "similarity": 0.5651710629463196,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "mm/page-writeback.c",
          "start_line": 495,
          "end_line": 600,
          "content": [
            "bool node_dirty_ok(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long limit = node_dirty_limit(pgdat);",
            "\tunsigned long nr_pages = 0;",
            "",
            "\tnr_pages += node_page_state(pgdat, NR_FILE_DIRTY);",
            "\tnr_pages += node_page_state(pgdat, NR_WRITEBACK);",
            "",
            "\treturn nr_pages <= limit;",
            "}",
            "static int dirty_background_ratio_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint ret;",
            "",
            "\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write)",
            "\t\tdirty_background_bytes = 0;",
            "\treturn ret;",
            "}",
            "static int dirty_background_bytes_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint ret;",
            "\tunsigned long old_bytes = dirty_background_bytes;",
            "",
            "\tret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write) {",
            "\t\tif (DIV_ROUND_UP(dirty_background_bytes, PAGE_SIZE) >",
            "\t\t\t\t\t\t\t\tUINT_MAX) {",
            "\t\t\tdirty_background_bytes = old_bytes;",
            "\t\t\treturn -ERANGE;",
            "\t\t}",
            "\t\tdirty_background_ratio = 0;",
            "\t}",
            "\treturn ret;",
            "}",
            "static int dirty_ratio_handler(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos)",
            "{",
            "\tint old_ratio = vm_dirty_ratio;",
            "\tint ret;",
            "",
            "\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write && vm_dirty_ratio != old_ratio) {",
            "\t\tvm_dirty_bytes = 0;",
            "\t\twriteback_set_ratelimit();",
            "\t}",
            "\treturn ret;",
            "}",
            "static int dirty_bytes_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tunsigned long old_bytes = vm_dirty_bytes;",
            "\tint ret;",
            "",
            "\tret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write && vm_dirty_bytes != old_bytes) {",
            "\t\tif (DIV_ROUND_UP(vm_dirty_bytes, PAGE_SIZE) > UINT_MAX) {",
            "\t\t\tvm_dirty_bytes = old_bytes;",
            "\t\t\treturn -ERANGE;",
            "\t\t}",
            "\t\twriteback_set_ratelimit();",
            "\t\tvm_dirty_ratio = 0;",
            "\t}",
            "\treturn ret;",
            "}",
            "static unsigned long wp_next_time(unsigned long cur_time)",
            "{",
            "\tcur_time += VM_COMPLETIONS_PERIOD_LEN;",
            "\t/* 0 has a special meaning... */",
            "\tif (!cur_time)",
            "\t\treturn 1;",
            "\treturn cur_time;",
            "}",
            "static void wb_domain_writeout_add(struct wb_domain *dom,",
            "\t\t\t\t   struct fprop_local_percpu *completions,",
            "\t\t\t\t   unsigned int max_prop_frac, long nr)",
            "{",
            "\t__fprop_add_percpu_max(&dom->completions, completions,",
            "\t\t\t       max_prop_frac, nr);",
            "\t/* First event after period switching was turned off? */",
            "\tif (unlikely(!dom->period_time)) {",
            "\t\t/*",
            "\t\t * We can race with other __bdi_writeout_inc calls here but",
            "\t\t * it does not cause any harm since the resulting time when",
            "\t\t * timer will fire and what is in writeout_period_time will be",
            "\t\t * roughly the same.",
            "\t\t */",
            "\t\tdom->period_time = wp_next_time(jiffies);",
            "\t\tmod_timer(&dom->period_timer, dom->period_time);",
            "\t}",
            "}",
            "static inline void __wb_writeout_add(struct bdi_writeback *wb, long nr)",
            "{",
            "\tstruct wb_domain *cgdom;",
            "",
            "\twb_stat_mod(wb, WB_WRITTEN, nr);",
            "\twb_domain_writeout_add(&global_wb_domain, &wb->completions,",
            "\t\t\t       wb->bdi->max_prop_frac, nr);",
            "",
            "\tcgdom = mem_cgroup_wb_domain(wb);",
            "\tif (cgdom)",
            "\t\twb_domain_writeout_add(cgdom, wb_memcg_completions(wb),",
            "\t\t\t\t       wb->bdi->max_prop_frac, nr);",
            "}"
          ],
          "function_name": "node_dirty_ok, dirty_background_ratio_handler, dirty_background_bytes_handler, dirty_ratio_handler, dirty_bytes_handler, wp_next_time, wb_domain_writeout_add, __wb_writeout_add",
          "description": "通过sysctl接口动态调整脏页写回参数，维护写回统计信息并周期性触发写回检查，确保系统内存使用符合预设策略。",
          "similarity": 0.5912494659423828
        },
        {
          "chunk_id": 7,
          "file_path": "mm/page-writeback.c",
          "start_line": 1221,
          "end_line": 1472,
          "content": [
            "static void wb_update_write_bandwidth(struct bdi_writeback *wb,",
            "\t\t\t\t      unsigned long elapsed,",
            "\t\t\t\t      unsigned long written)",
            "{",
            "\tconst unsigned long period = roundup_pow_of_two(3 * HZ);",
            "\tunsigned long avg = wb->avg_write_bandwidth;",
            "\tunsigned long old = wb->write_bandwidth;",
            "\tu64 bw;",
            "",
            "\t/*",
            "\t * bw = written * HZ / elapsed",
            "\t *",
            "\t *                   bw * elapsed + write_bandwidth * (period - elapsed)",
            "\t * write_bandwidth = ---------------------------------------------------",
            "\t *                                          period",
            "\t *",
            "\t * @written may have decreased due to folio_redirty_for_writepage().",
            "\t * Avoid underflowing @bw calculation.",
            "\t */",
            "\tbw = written - min(written, wb->written_stamp);",
            "\tbw *= HZ;",
            "\tif (unlikely(elapsed > period)) {",
            "\t\tbw = div64_ul(bw, elapsed);",
            "\t\tavg = bw;",
            "\t\tgoto out;",
            "\t}",
            "\tbw += (u64)wb->write_bandwidth * (period - elapsed);",
            "\tbw >>= ilog2(period);",
            "",
            "\t/*",
            "\t * one more level of smoothing, for filtering out sudden spikes",
            "\t */",
            "\tif (avg > old && old >= (unsigned long)bw)",
            "\t\tavg -= (avg - old) >> 3;",
            "",
            "\tif (avg < old && old <= (unsigned long)bw)",
            "\t\tavg += (old - avg) >> 3;",
            "",
            "out:",
            "\t/* keep avg > 0 to guarantee that tot > 0 if there are dirty wbs */",
            "\tavg = max(avg, 1LU);",
            "\tif (wb_has_dirty_io(wb)) {",
            "\t\tlong delta = avg - wb->avg_write_bandwidth;",
            "\t\tWARN_ON_ONCE(atomic_long_add_return(delta,",
            "\t\t\t\t\t&wb->bdi->tot_write_bandwidth) <= 0);",
            "\t}",
            "\twb->write_bandwidth = bw;",
            "\tWRITE_ONCE(wb->avg_write_bandwidth, avg);",
            "}",
            "static void update_dirty_limit(struct dirty_throttle_control *dtc)",
            "{",
            "\tstruct wb_domain *dom = dtc_dom(dtc);",
            "\tunsigned long thresh = dtc->thresh;",
            "\tunsigned long limit = dom->dirty_limit;",
            "",
            "\t/*",
            "\t * Follow up in one step.",
            "\t */",
            "\tif (limit < thresh) {",
            "\t\tlimit = thresh;",
            "\t\tgoto update;",
            "\t}",
            "",
            "\t/*",
            "\t * Follow down slowly. Use the higher one as the target, because thresh",
            "\t * may drop below dirty. This is exactly the reason to introduce",
            "\t * dom->dirty_limit which is guaranteed to lie above the dirty pages.",
            "\t */",
            "\tthresh = max(thresh, dtc->dirty);",
            "\tif (limit > thresh) {",
            "\t\tlimit -= (limit - thresh) >> 5;",
            "\t\tgoto update;",
            "\t}",
            "\treturn;",
            "update:",
            "\tdom->dirty_limit = limit;",
            "}",
            "static void domain_update_dirty_limit(struct dirty_throttle_control *dtc,",
            "\t\t\t\t      unsigned long now)",
            "{",
            "\tstruct wb_domain *dom = dtc_dom(dtc);",
            "",
            "\t/*",
            "\t * check locklessly first to optimize away locking for the most time",
            "\t */",
            "\tif (time_before(now, dom->dirty_limit_tstamp + BANDWIDTH_INTERVAL))",
            "\t\treturn;",
            "",
            "\tspin_lock(&dom->lock);",
            "\tif (time_after_eq(now, dom->dirty_limit_tstamp + BANDWIDTH_INTERVAL)) {",
            "\t\tupdate_dirty_limit(dtc);",
            "\t\tdom->dirty_limit_tstamp = now;",
            "\t}",
            "\tspin_unlock(&dom->lock);",
            "}",
            "static void wb_update_dirty_ratelimit(struct dirty_throttle_control *dtc,",
            "\t\t\t\t      unsigned long dirtied,",
            "\t\t\t\t      unsigned long elapsed)",
            "{",
            "\tstruct bdi_writeback *wb = dtc->wb;",
            "\tunsigned long dirty = dtc->dirty;",
            "\tunsigned long freerun = dirty_freerun_ceiling(dtc->thresh, dtc->bg_thresh);",
            "\tunsigned long limit = hard_dirty_limit(dtc_dom(dtc), dtc->thresh);",
            "\tunsigned long setpoint = (freerun + limit) / 2;",
            "\tunsigned long write_bw = wb->avg_write_bandwidth;",
            "\tunsigned long dirty_ratelimit = wb->dirty_ratelimit;",
            "\tunsigned long dirty_rate;",
            "\tunsigned long task_ratelimit;",
            "\tunsigned long balanced_dirty_ratelimit;",
            "\tunsigned long step;",
            "\tunsigned long x;",
            "\tunsigned long shift;",
            "",
            "\t/*",
            "\t * The dirty rate will match the writeout rate in long term, except",
            "\t * when dirty pages are truncated by userspace or re-dirtied by FS.",
            "\t */",
            "\tdirty_rate = (dirtied - wb->dirtied_stamp) * HZ / elapsed;",
            "",
            "\t/*",
            "\t * task_ratelimit reflects each dd's dirty rate for the past 200ms.",
            "\t */",
            "\ttask_ratelimit = (u64)dirty_ratelimit *",
            "\t\t\t\t\tdtc->pos_ratio >> RATELIMIT_CALC_SHIFT;",
            "\ttask_ratelimit++; /* it helps rampup dirty_ratelimit from tiny values */",
            "",
            "\t/*",
            "\t * A linear estimation of the \"balanced\" throttle rate. The theory is,",
            "\t * if there are N dd tasks, each throttled at task_ratelimit, the wb's",
            "\t * dirty_rate will be measured to be (N * task_ratelimit). So the below",
            "\t * formula will yield the balanced rate limit (write_bw / N).",
            "\t *",
            "\t * Note that the expanded form is not a pure rate feedback:",
            "\t *\trate_(i+1) = rate_(i) * (write_bw / dirty_rate)\t\t     (1)",
            "\t * but also takes pos_ratio into account:",
            "\t *\trate_(i+1) = rate_(i) * (write_bw / dirty_rate) * pos_ratio  (2)",
            "\t *",
            "\t * (1) is not realistic because pos_ratio also takes part in balancing",
            "\t * the dirty rate.  Consider the state",
            "\t *\tpos_ratio = 0.5\t\t\t\t\t\t     (3)",
            "\t *\trate = 2 * (write_bw / N)\t\t\t\t     (4)",
            "\t * If (1) is used, it will stuck in that state! Because each dd will",
            "\t * be throttled at",
            "\t *\ttask_ratelimit = pos_ratio * rate = (write_bw / N)\t     (5)",
            "\t * yielding",
            "\t *\tdirty_rate = N * task_ratelimit = write_bw\t\t     (6)",
            "\t * put (6) into (1) we get",
            "\t *\trate_(i+1) = rate_(i)\t\t\t\t\t     (7)",
            "\t *",
            "\t * So we end up using (2) to always keep",
            "\t *\trate_(i+1) ~= (write_bw / N)\t\t\t\t     (8)",
            "\t * regardless of the value of pos_ratio. As long as (8) is satisfied,",
            "\t * pos_ratio is able to drive itself to 1.0, which is not only where",
            "\t * the dirty count meet the setpoint, but also where the slope of",
            "\t * pos_ratio is most flat and hence task_ratelimit is least fluctuated.",
            "\t */",
            "\tbalanced_dirty_ratelimit = div_u64((u64)task_ratelimit * write_bw,",
            "\t\t\t\t\t   dirty_rate | 1);",
            "\t/*",
            "\t * balanced_dirty_ratelimit ~= (write_bw / N) <= write_bw",
            "\t */",
            "\tif (unlikely(balanced_dirty_ratelimit > write_bw))",
            "\t\tbalanced_dirty_ratelimit = write_bw;",
            "",
            "\t/*",
            "\t * We could safely do this and return immediately:",
            "\t *",
            "\t *\twb->dirty_ratelimit = balanced_dirty_ratelimit;",
            "\t *",
            "\t * However to get a more stable dirty_ratelimit, the below elaborated",
            "\t * code makes use of task_ratelimit to filter out singular points and",
            "\t * limit the step size.",
            "\t *",
            "\t * The below code essentially only uses the relative value of",
            "\t *",
            "\t *\ttask_ratelimit - dirty_ratelimit",
            "\t *\t= (pos_ratio - 1) * dirty_ratelimit",
            "\t *",
            "\t * which reflects the direction and size of dirty position error.",
            "\t */",
            "",
            "\t/*",
            "\t * dirty_ratelimit will follow balanced_dirty_ratelimit iff",
            "\t * task_ratelimit is on the same side of dirty_ratelimit, too.",
            "\t * For example, when",
            "\t * - dirty_ratelimit > balanced_dirty_ratelimit",
            "\t * - dirty_ratelimit > task_ratelimit (dirty pages are above setpoint)",
            "\t * lowering dirty_ratelimit will help meet both the position and rate",
            "\t * control targets. Otherwise, don't update dirty_ratelimit if it will",
            "\t * only help meet the rate target. After all, what the users ultimately",
            "\t * feel and care are stable dirty rate and small position error.",
            "\t *",
            "\t * |task_ratelimit - dirty_ratelimit| is used to limit the step size",
            "\t * and filter out the singular points of balanced_dirty_ratelimit. Which",
            "\t * keeps jumping around randomly and can even leap far away at times",
            "\t * due to the small 200ms estimation period of dirty_rate (we want to",
            "\t * keep that period small to reduce time lags).",
            "\t */",
            "\tstep = 0;",
            "",
            "\t/*",
            "\t * For strictlimit case, calculations above were based on wb counters",
            "\t * and limits (starting from pos_ratio = wb_position_ratio() and up to",
            "\t * balanced_dirty_ratelimit = task_ratelimit * write_bw / dirty_rate).",
            "\t * Hence, to calculate \"step\" properly, we have to use wb_dirty as",
            "\t * \"dirty\" and wb_setpoint as \"setpoint\".",
            "\t *",
            "\t * We rampup dirty_ratelimit forcibly if wb_dirty is low because",
            "\t * it's possible that wb_thresh is close to zero due to inactivity",
            "\t * of backing device.",
            "\t */",
            "\tif (unlikely(wb->bdi->capabilities & BDI_CAP_STRICTLIMIT)) {",
            "\t\tdirty = dtc->wb_dirty;",
            "\t\tif (dtc->wb_dirty < 8)",
            "\t\t\tsetpoint = dtc->wb_dirty + 1;",
            "\t\telse",
            "\t\t\tsetpoint = (dtc->wb_thresh + dtc->wb_bg_thresh) / 2;",
            "\t}",
            "",
            "\tif (dirty < setpoint) {",
            "\t\tx = min3(wb->balanced_dirty_ratelimit,",
            "\t\t\t balanced_dirty_ratelimit, task_ratelimit);",
            "\t\tif (dirty_ratelimit < x)",
            "\t\t\tstep = x - dirty_ratelimit;",
            "\t} else {",
            "\t\tx = max3(wb->balanced_dirty_ratelimit,",
            "\t\t\t balanced_dirty_ratelimit, task_ratelimit);",
            "\t\tif (dirty_ratelimit > x)",
            "\t\t\tstep = dirty_ratelimit - x;",
            "\t}",
            "",
            "\t/*",
            "\t * Don't pursue 100% rate matching. It's impossible since the balanced",
            "\t * rate itself is constantly fluctuating. So decrease the track speed",
            "\t * when it gets close to the target. Helps eliminate pointless tremors.",
            "\t */",
            "\tshift = dirty_ratelimit / (2 * step + 1);",
            "\tif (shift < BITS_PER_LONG)",
            "\t\tstep = DIV_ROUND_UP(step >> shift, 8);",
            "\telse",
            "\t\tstep = 0;",
            "",
            "\tif (dirty_ratelimit < balanced_dirty_ratelimit)",
            "\t\tdirty_ratelimit += step;",
            "\telse",
            "\t\tdirty_ratelimit -= step;",
            "",
            "\tWRITE_ONCE(wb->dirty_ratelimit, max(dirty_ratelimit, 1UL));",
            "\twb->balanced_dirty_ratelimit = balanced_dirty_ratelimit;",
            "",
            "\ttrace_bdi_dirty_ratelimit(wb, dirty_rate, task_ratelimit);",
            "}"
          ],
          "function_name": "wb_update_write_bandwidth, update_dirty_limit, domain_update_dirty_limit, wb_update_dirty_ratelimit",
          "description": "实现写带宽统计更新逻辑，包含平滑写入速率变化的算法，更新脏页限制的动态调整机制，以及根据当前脏页速率与写入能力计算目标脏页产生速率的控制逻辑，通过分层限速策略维持系统稳定。",
          "similarity": 0.5727295279502869
        },
        {
          "chunk_id": 4,
          "file_path": "mm/page-writeback.c",
          "start_line": 615,
          "end_line": 719,
          "content": [
            "void wb_writeout_inc(struct bdi_writeback *wb)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\t__wb_writeout_add(wb, 1);",
            "\tlocal_irq_restore(flags);",
            "}",
            "static void writeout_period(struct timer_list *t)",
            "{",
            "\tstruct wb_domain *dom = from_timer(dom, t, period_timer);",
            "\tint miss_periods = (jiffies - dom->period_time) /",
            "\t\t\t\t\t\t VM_COMPLETIONS_PERIOD_LEN;",
            "",
            "\tif (fprop_new_period(&dom->completions, miss_periods + 1)) {",
            "\t\tdom->period_time = wp_next_time(dom->period_time +",
            "\t\t\t\tmiss_periods * VM_COMPLETIONS_PERIOD_LEN);",
            "\t\tmod_timer(&dom->period_timer, dom->period_time);",
            "\t} else {",
            "\t\t/*",
            "\t\t * Aging has zeroed all fractions. Stop wasting CPU on period",
            "\t\t * updates.",
            "\t\t */",
            "\t\tdom->period_time = 0;",
            "\t}",
            "}",
            "int wb_domain_init(struct wb_domain *dom, gfp_t gfp)",
            "{",
            "\tmemset(dom, 0, sizeof(*dom));",
            "",
            "\tspin_lock_init(&dom->lock);",
            "",
            "\ttimer_setup(&dom->period_timer, writeout_period, TIMER_DEFERRABLE);",
            "",
            "\tdom->dirty_limit_tstamp = jiffies;",
            "",
            "\treturn fprop_global_init(&dom->completions, gfp);",
            "}",
            "void wb_domain_exit(struct wb_domain *dom)",
            "{",
            "\tdel_timer_sync(&dom->period_timer);",
            "\tfprop_global_destroy(&dom->completions);",
            "}",
            "static int bdi_check_pages_limit(unsigned long pages)",
            "{",
            "\tunsigned long max_dirty_pages = global_dirtyable_memory();",
            "",
            "\tif (pages > max_dirty_pages)",
            "\t\treturn -EINVAL;",
            "",
            "\treturn 0;",
            "}",
            "static unsigned long bdi_ratio_from_pages(unsigned long pages)",
            "{",
            "\tunsigned long background_thresh;",
            "\tunsigned long dirty_thresh;",
            "\tunsigned long ratio;",
            "",
            "\tglobal_dirty_limits(&background_thresh, &dirty_thresh);",
            "\tratio = div64_u64(pages * 100ULL * BDI_RATIO_SCALE, dirty_thresh);",
            "",
            "\treturn ratio;",
            "}",
            "static u64 bdi_get_bytes(unsigned int ratio)",
            "{",
            "\tunsigned long background_thresh;",
            "\tunsigned long dirty_thresh;",
            "\tu64 bytes;",
            "",
            "\tglobal_dirty_limits(&background_thresh, &dirty_thresh);",
            "\tbytes = (dirty_thresh * PAGE_SIZE * ratio) / BDI_RATIO_SCALE / 100;",
            "",
            "\treturn bytes;",
            "}",
            "static int __bdi_set_min_ratio(struct backing_dev_info *bdi, unsigned int min_ratio)",
            "{",
            "\tunsigned int delta;",
            "\tint ret = 0;",
            "",
            "\tif (min_ratio > 100 * BDI_RATIO_SCALE)",
            "\t\treturn -EINVAL;",
            "\tmin_ratio *= BDI_RATIO_SCALE;",
            "",
            "\tspin_lock_bh(&bdi_lock);",
            "\tif (min_ratio > bdi->max_ratio) {",
            "\t\tret = -EINVAL;",
            "\t} else {",
            "\t\tif (min_ratio < bdi->min_ratio) {",
            "\t\t\tdelta = bdi->min_ratio - min_ratio;",
            "\t\t\tbdi_min_ratio -= delta;",
            "\t\t\tbdi->min_ratio = min_ratio;",
            "\t\t} else {",
            "\t\t\tdelta = min_ratio - bdi->min_ratio;",
            "\t\t\tif (bdi_min_ratio + delta < 100 * BDI_RATIO_SCALE) {",
            "\t\t\t\tbdi_min_ratio += delta;",
            "\t\t\t\tbdi->min_ratio = min_ratio;",
            "\t\t\t} else {",
            "\t\t\t\tret = -EINVAL;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\tspin_unlock_bh(&bdi_lock);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "wb_writeout_inc, writeout_period, wb_domain_init, wb_domain_exit, bdi_check_pages_limit, bdi_ratio_from_pages, bdi_get_bytes, __bdi_set_min_ratio",
          "description": "实现写回统计增量记录、定时器驱动的写回周期校准及内存设备（bdi）的脏页限制验证与参数转换逻辑，支撑动态写回策略调整。",
          "similarity": 0.5572454333305359
        },
        {
          "chunk_id": 11,
          "file_path": "mm/page-writeback.c",
          "start_line": 2176,
          "end_line": 2404,
          "content": [
            "static int dirty_writeback_centisecs_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tunsigned int old_interval = dirty_writeback_interval;",
            "\tint ret;",
            "",
            "\tret = proc_dointvec(table, write, buffer, length, ppos);",
            "",
            "\t/*",
            "\t * Writing 0 to dirty_writeback_interval will disable periodic writeback",
            "\t * and a different non-zero value will wakeup the writeback threads.",
            "\t * wb_wakeup_delayed() would be more appropriate, but it's a pain to",
            "\t * iterate over all bdis and wbs.",
            "\t * The reason we do this is to make the change take effect immediately.",
            "\t */",
            "\tif (!ret && write && dirty_writeback_interval &&",
            "\t\tdirty_writeback_interval != old_interval)",
            "\t\twakeup_flusher_threads(WB_REASON_PERIODIC);",
            "",
            "\treturn ret;",
            "}",
            "void laptop_mode_timer_fn(struct timer_list *t)",
            "{",
            "\tstruct backing_dev_info *backing_dev_info =",
            "\t\tfrom_timer(backing_dev_info, t, laptop_mode_wb_timer);",
            "",
            "\twakeup_flusher_threads_bdi(backing_dev_info, WB_REASON_LAPTOP_TIMER);",
            "}",
            "void laptop_io_completion(struct backing_dev_info *info)",
            "{",
            "\tmod_timer(&info->laptop_mode_wb_timer, jiffies + laptop_mode);",
            "}",
            "void laptop_sync_completion(void)",
            "{",
            "\tstruct backing_dev_info *bdi;",
            "",
            "\trcu_read_lock();",
            "",
            "\tlist_for_each_entry_rcu(bdi, &bdi_list, bdi_list)",
            "\t\tdel_timer(&bdi->laptop_mode_wb_timer);",
            "",
            "\trcu_read_unlock();",
            "}",
            "void writeback_set_ratelimit(void)",
            "{",
            "\tstruct wb_domain *dom = &global_wb_domain;",
            "\tunsigned long background_thresh;",
            "\tunsigned long dirty_thresh;",
            "",
            "\tglobal_dirty_limits(&background_thresh, &dirty_thresh);",
            "\tdom->dirty_limit = dirty_thresh;",
            "\tratelimit_pages = dirty_thresh / (num_online_cpus() * 32);",
            "\tif (ratelimit_pages < 16)",
            "\t\tratelimit_pages = 16;",
            "}",
            "static int page_writeback_cpu_online(unsigned int cpu)",
            "{",
            "\twriteback_set_ratelimit();",
            "\treturn 0;",
            "}",
            "void __init page_writeback_init(void)",
            "{",
            "\tBUG_ON(wb_domain_init(&global_wb_domain, GFP_KERNEL));",
            "",
            "\tcpuhp_setup_state(CPUHP_AP_ONLINE_DYN, \"mm/writeback:online\",",
            "\t\t\t  page_writeback_cpu_online, NULL);",
            "\tcpuhp_setup_state(CPUHP_MM_WRITEBACK_DEAD, \"mm/writeback:dead\", NULL,",
            "\t\t\t  page_writeback_cpu_online);",
            "#ifdef CONFIG_SYSCTL",
            "\tregister_sysctl_init(\"vm\", vm_page_writeback_sysctls);",
            "#endif",
            "}",
            "void tag_pages_for_writeback(struct address_space *mapping,",
            "\t\t\t     pgoff_t start, pgoff_t end)",
            "{",
            "\tXA_STATE(xas, &mapping->i_pages, start);",
            "\tunsigned int tagged = 0;",
            "\tvoid *page;",
            "",
            "\txas_lock_irq(&xas);",
            "\txas_for_each_marked(&xas, page, end, PAGECACHE_TAG_DIRTY) {",
            "\t\txas_set_mark(&xas, PAGECACHE_TAG_TOWRITE);",
            "\t\tif (++tagged % XA_CHECK_SCHED)",
            "\t\t\tcontinue;",
            "",
            "\t\txas_pause(&xas);",
            "\t\txas_unlock_irq(&xas);",
            "\t\tcond_resched();",
            "\t\txas_lock_irq(&xas);",
            "\t}",
            "\txas_unlock_irq(&xas);",
            "}",
            "int write_cache_pages(struct address_space *mapping,",
            "\t\t      struct writeback_control *wbc, writepage_t writepage,",
            "\t\t      void *data)",
            "{",
            "\tint ret = 0;",
            "\tint done = 0;",
            "\tint error;",
            "\tstruct folio_batch fbatch;",
            "\tint nr_folios;",
            "\tpgoff_t index;",
            "\tpgoff_t end;\t\t/* Inclusive */",
            "\tpgoff_t done_index;",
            "\tint range_whole = 0;",
            "\txa_mark_t tag;",
            "",
            "\tfolio_batch_init(&fbatch);",
            "\tif (wbc->range_cyclic) {",
            "\t\tindex = mapping->writeback_index; /* prev offset */",
            "\t\tend = -1;",
            "\t} else {",
            "\t\tindex = wbc->range_start >> PAGE_SHIFT;",
            "\t\tend = wbc->range_end >> PAGE_SHIFT;",
            "\t\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)",
            "\t\t\trange_whole = 1;",
            "\t}",
            "\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages) {",
            "\t\ttag_pages_for_writeback(mapping, index, end);",
            "\t\ttag = PAGECACHE_TAG_TOWRITE;",
            "\t} else {",
            "\t\ttag = PAGECACHE_TAG_DIRTY;",
            "\t}",
            "\tdone_index = index;",
            "\twhile (!done && (index <= end)) {",
            "\t\tint i;",
            "",
            "\t\tnr_folios = filemap_get_folios_tag(mapping, &index, end,",
            "\t\t\t\ttag, &fbatch);",
            "",
            "\t\tif (nr_folios == 0)",
            "\t\t\tbreak;",
            "",
            "\t\tfor (i = 0; i < nr_folios; i++) {",
            "\t\t\tstruct folio *folio = fbatch.folios[i];",
            "\t\t\tunsigned long nr;",
            "",
            "\t\t\tdone_index = folio->index;",
            "",
            "\t\t\tfolio_lock(folio);",
            "",
            "\t\t\t/*",
            "\t\t\t * Page truncated or invalidated. We can freely skip it",
            "\t\t\t * then, even for data integrity operations: the page",
            "\t\t\t * has disappeared concurrently, so there could be no",
            "\t\t\t * real expectation of this data integrity operation",
            "\t\t\t * even if there is now a new, dirty page at the same",
            "\t\t\t * pagecache address.",
            "\t\t\t */",
            "\t\t\tif (unlikely(folio->mapping != mapping)) {",
            "continue_unlock:",
            "\t\t\t\tfolio_unlock(folio);",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tif (!folio_test_dirty(folio)) {",
            "\t\t\t\t/* someone wrote it for us */",
            "\t\t\t\tgoto continue_unlock;",
            "\t\t\t}",
            "",
            "\t\t\tif (folio_test_writeback(folio)) {",
            "\t\t\t\tif (wbc->sync_mode != WB_SYNC_NONE)",
            "\t\t\t\t\tfolio_wait_writeback(folio);",
            "\t\t\t\telse",
            "\t\t\t\t\tgoto continue_unlock;",
            "\t\t\t}",
            "",
            "\t\t\tBUG_ON(folio_test_writeback(folio));",
            "\t\t\tif (!folio_clear_dirty_for_io(folio))",
            "\t\t\t\tgoto continue_unlock;",
            "",
            "\t\t\ttrace_wbc_writepage(wbc, inode_to_bdi(mapping->host));",
            "\t\t\terror = writepage(folio, wbc, data);",
            "\t\t\tnr = folio_nr_pages(folio);",
            "\t\t\tif (unlikely(error)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * Handle errors according to the type of",
            "\t\t\t\t * writeback. There's no need to continue for",
            "\t\t\t\t * background writeback. Just push done_index",
            "\t\t\t\t * past this page so media errors won't choke",
            "\t\t\t\t * writeout for the entire file. For integrity",
            "\t\t\t\t * writeback, we must process the entire dirty",
            "\t\t\t\t * set regardless of errors because the fs may",
            "\t\t\t\t * still have state to clear for each page. In",
            "\t\t\t\t * that case we continue processing and return",
            "\t\t\t\t * the first error.",
            "\t\t\t\t */",
            "\t\t\t\tif (error == AOP_WRITEPAGE_ACTIVATE) {",
            "\t\t\t\t\tfolio_unlock(folio);",
            "\t\t\t\t\terror = 0;",
            "\t\t\t\t} else if (wbc->sync_mode != WB_SYNC_ALL) {",
            "\t\t\t\t\tret = error;",
            "\t\t\t\t\tdone_index = folio->index + nr;",
            "\t\t\t\t\tdone = 1;",
            "\t\t\t\t\tbreak;",
            "\t\t\t\t}",
            "\t\t\t\tif (!ret)",
            "\t\t\t\t\tret = error;",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * We stop writing back only if we are not doing",
            "\t\t\t * integrity sync. In case of integrity sync we have to",
            "\t\t\t * keep going until we have written all the pages",
            "\t\t\t * we tagged for writeback prior to entering this loop.",
            "\t\t\t */",
            "\t\t\twbc->nr_to_write -= nr;",
            "\t\t\tif (wbc->nr_to_write <= 0 &&",
            "\t\t\t    wbc->sync_mode == WB_SYNC_NONE) {",
            "\t\t\t\tdone = 1;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t\tfolio_batch_release(&fbatch);",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\t/*",
            "\t * If we hit the last page and there is more work to be done: wrap",
            "\t * back the index back to the start of the file for the next",
            "\t * time we are called.",
            "\t */",
            "\tif (wbc->range_cyclic && !done)",
            "\t\tdone_index = 0;",
            "\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))",
            "\t\tmapping->writeback_index = done_index;",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "dirty_writeback_centisecs_handler, laptop_mode_timer_fn, laptop_io_completion, laptop_sync_completion, writeback_set_ratelimit, page_writeback_cpu_online, page_writeback_init, tag_pages_for_writeback, write_cache_pages",
          "description": "dirty_writeback_centisecs_handler 设置脏页写回间隔，laptop_mode_timer_fn 处理笔记本模式下定时触发写回。writeback_set_ratelimit 根据CPU数量动态调整速率限制参数。tag_pages_for_writeback 标记待写回页面，write_cache_pages 执行实际的缓存页面写回流程。",
          "similarity": 0.547736406326294
        },
        {
          "chunk_id": 9,
          "file_path": "mm/page-writeback.c",
          "start_line": 1662,
          "end_line": 1988,
          "content": [
            "static inline void wb_dirty_limits(struct dirty_throttle_control *dtc)",
            "{",
            "\tstruct bdi_writeback *wb = dtc->wb;",
            "\tunsigned long wb_reclaimable;",
            "",
            "\t/*",
            "\t * wb_thresh is not treated as some limiting factor as",
            "\t * dirty_thresh, due to reasons",
            "\t * - in JBOD setup, wb_thresh can fluctuate a lot",
            "\t * - in a system with HDD and USB key, the USB key may somehow",
            "\t *   go into state (wb_dirty >> wb_thresh) either because",
            "\t *   wb_dirty starts high, or because wb_thresh drops low.",
            "\t *   In this case we don't want to hard throttle the USB key",
            "\t *   dirtiers for 100 seconds until wb_dirty drops under",
            "\t *   wb_thresh. Instead the auxiliary wb control line in",
            "\t *   wb_position_ratio() will let the dirtier task progress",
            "\t *   at some rate <= (write_bw / 2) for bringing down wb_dirty.",
            "\t */",
            "\tdtc->wb_thresh = __wb_calc_thresh(dtc);",
            "\tdtc->wb_bg_thresh = dtc->thresh ?",
            "\t\tdiv_u64((u64)dtc->wb_thresh * dtc->bg_thresh, dtc->thresh) : 0;",
            "",
            "\t/*",
            "\t * In order to avoid the stacked BDI deadlock we need",
            "\t * to ensure we accurately count the 'dirty' pages when",
            "\t * the threshold is low.",
            "\t *",
            "\t * Otherwise it would be possible to get thresh+n pages",
            "\t * reported dirty, even though there are thresh-m pages",
            "\t * actually dirty; with m+n sitting in the percpu",
            "\t * deltas.",
            "\t */",
            "\tif (dtc->wb_thresh < 2 * wb_stat_error()) {",
            "\t\twb_reclaimable = wb_stat_sum(wb, WB_RECLAIMABLE);",
            "\t\tdtc->wb_dirty = wb_reclaimable + wb_stat_sum(wb, WB_WRITEBACK);",
            "\t} else {",
            "\t\twb_reclaimable = wb_stat(wb, WB_RECLAIMABLE);",
            "\t\tdtc->wb_dirty = wb_reclaimable + wb_stat(wb, WB_WRITEBACK);",
            "\t}",
            "}",
            "static int balance_dirty_pages(struct bdi_writeback *wb,",
            "\t\t\t       unsigned long pages_dirtied, unsigned int flags)",
            "{",
            "\tstruct dirty_throttle_control gdtc_stor = { GDTC_INIT(wb) };",
            "\tstruct dirty_throttle_control mdtc_stor = { MDTC_INIT(wb, &gdtc_stor) };",
            "\tstruct dirty_throttle_control * const gdtc = &gdtc_stor;",
            "\tstruct dirty_throttle_control * const mdtc = mdtc_valid(&mdtc_stor) ?",
            "\t\t\t\t\t\t     &mdtc_stor : NULL;",
            "\tstruct dirty_throttle_control *sdtc;",
            "\tunsigned long nr_dirty;",
            "\tlong period;",
            "\tlong pause;",
            "\tlong max_pause;",
            "\tlong min_pause;",
            "\tint nr_dirtied_pause;",
            "\tbool dirty_exceeded = false;",
            "\tunsigned long task_ratelimit;",
            "\tunsigned long dirty_ratelimit;",
            "\tstruct backing_dev_info *bdi = wb->bdi;",
            "\tbool strictlimit = bdi->capabilities & BDI_CAP_STRICTLIMIT;",
            "\tunsigned long start_time = jiffies;",
            "\tint ret = 0;",
            "",
            "\tfor (;;) {",
            "\t\tunsigned long now = jiffies;",
            "\t\tunsigned long dirty, thresh, bg_thresh;",
            "\t\tunsigned long m_dirty = 0;\t/* stop bogus uninit warnings */",
            "\t\tunsigned long m_thresh = 0;",
            "\t\tunsigned long m_bg_thresh = 0;",
            "",
            "\t\tnr_dirty = global_node_page_state(NR_FILE_DIRTY);",
            "\t\tgdtc->avail = global_dirtyable_memory();",
            "\t\tgdtc->dirty = nr_dirty + global_node_page_state(NR_WRITEBACK);",
            "",
            "\t\tdomain_dirty_limits(gdtc);",
            "",
            "\t\tif (unlikely(strictlimit)) {",
            "\t\t\twb_dirty_limits(gdtc);",
            "",
            "\t\t\tdirty = gdtc->wb_dirty;",
            "\t\t\tthresh = gdtc->wb_thresh;",
            "\t\t\tbg_thresh = gdtc->wb_bg_thresh;",
            "\t\t} else {",
            "\t\t\tdirty = gdtc->dirty;",
            "\t\t\tthresh = gdtc->thresh;",
            "\t\t\tbg_thresh = gdtc->bg_thresh;",
            "\t\t}",
            "",
            "\t\tif (mdtc) {",
            "\t\t\tunsigned long filepages, headroom, writeback;",
            "",
            "\t\t\t/*",
            "\t\t\t * If @wb belongs to !root memcg, repeat the same",
            "\t\t\t * basic calculations for the memcg domain.",
            "\t\t\t */",
            "\t\t\tmem_cgroup_wb_stats(wb, &filepages, &headroom,",
            "\t\t\t\t\t    &mdtc->dirty, &writeback);",
            "\t\t\tmdtc->dirty += writeback;",
            "\t\t\tmdtc_calc_avail(mdtc, filepages, headroom);",
            "",
            "\t\t\tdomain_dirty_limits(mdtc);",
            "",
            "\t\t\tif (unlikely(strictlimit)) {",
            "\t\t\t\twb_dirty_limits(mdtc);",
            "\t\t\t\tm_dirty = mdtc->wb_dirty;",
            "\t\t\t\tm_thresh = mdtc->wb_thresh;",
            "\t\t\t\tm_bg_thresh = mdtc->wb_bg_thresh;",
            "\t\t\t} else {",
            "\t\t\t\tm_dirty = mdtc->dirty;",
            "\t\t\t\tm_thresh = mdtc->thresh;",
            "\t\t\t\tm_bg_thresh = mdtc->bg_thresh;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * In laptop mode, we wait until hitting the higher threshold",
            "\t\t * before starting background writeout, and then write out all",
            "\t\t * the way down to the lower threshold.  So slow writers cause",
            "\t\t * minimal disk activity.",
            "\t\t *",
            "\t\t * In normal mode, we start background writeout at the lower",
            "\t\t * background_thresh, to keep the amount of dirty memory low.",
            "\t\t */",
            "\t\tif (!laptop_mode && nr_dirty > gdtc->bg_thresh &&",
            "\t\t    !writeback_in_progress(wb))",
            "\t\t\twb_start_background_writeback(wb);",
            "",
            "\t\t/*",
            "\t\t * Throttle it only when the background writeback cannot",
            "\t\t * catch-up. This avoids (excessively) small writeouts",
            "\t\t * when the wb limits are ramping up in case of !strictlimit.",
            "\t\t *",
            "\t\t * In strictlimit case make decision based on the wb counters",
            "\t\t * and limits. Small writeouts when the wb limits are ramping",
            "\t\t * up are the price we consciously pay for strictlimit-ing.",
            "\t\t *",
            "\t\t * If memcg domain is in effect, @dirty should be under",
            "\t\t * both global and memcg freerun ceilings.",
            "\t\t */",
            "\t\tif (dirty <= dirty_freerun_ceiling(thresh, bg_thresh) &&",
            "\t\t    (!mdtc ||",
            "\t\t     m_dirty <= dirty_freerun_ceiling(m_thresh, m_bg_thresh))) {",
            "\t\t\tunsigned long intv;",
            "\t\t\tunsigned long m_intv;",
            "",
            "free_running:",
            "\t\t\tintv = dirty_poll_interval(dirty, thresh);",
            "\t\t\tm_intv = ULONG_MAX;",
            "",
            "\t\t\tcurrent->dirty_paused_when = now;",
            "\t\t\tcurrent->nr_dirtied = 0;",
            "\t\t\tif (mdtc)",
            "\t\t\t\tm_intv = dirty_poll_interval(m_dirty, m_thresh);",
            "\t\t\tcurrent->nr_dirtied_pause = min(intv, m_intv);",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t/* Start writeback even when in laptop mode */",
            "\t\tif (unlikely(!writeback_in_progress(wb)))",
            "\t\t\twb_start_background_writeback(wb);",
            "",
            "\t\tmem_cgroup_flush_foreign(wb);",
            "",
            "\t\t/*",
            "\t\t * Calculate global domain's pos_ratio and select the",
            "\t\t * global dtc by default.",
            "\t\t */",
            "\t\tif (!strictlimit) {",
            "\t\t\twb_dirty_limits(gdtc);",
            "",
            "\t\t\tif ((current->flags & PF_LOCAL_THROTTLE) &&",
            "\t\t\t    gdtc->wb_dirty <",
            "\t\t\t    dirty_freerun_ceiling(gdtc->wb_thresh,",
            "\t\t\t\t\t\t  gdtc->wb_bg_thresh))",
            "\t\t\t\t/*",
            "\t\t\t\t * LOCAL_THROTTLE tasks must not be throttled",
            "\t\t\t\t * when below the per-wb freerun ceiling.",
            "\t\t\t\t */",
            "\t\t\t\tgoto free_running;",
            "\t\t}",
            "",
            "\t\tdirty_exceeded = (gdtc->wb_dirty > gdtc->wb_thresh) &&",
            "\t\t\t((gdtc->dirty > gdtc->thresh) || strictlimit);",
            "",
            "\t\twb_position_ratio(gdtc);",
            "\t\tsdtc = gdtc;",
            "",
            "\t\tif (mdtc) {",
            "\t\t\t/*",
            "\t\t\t * If memcg domain is in effect, calculate its",
            "\t\t\t * pos_ratio.  @wb should satisfy constraints from",
            "\t\t\t * both global and memcg domains.  Choose the one",
            "\t\t\t * w/ lower pos_ratio.",
            "\t\t\t */",
            "\t\t\tif (!strictlimit) {",
            "\t\t\t\twb_dirty_limits(mdtc);",
            "",
            "\t\t\t\tif ((current->flags & PF_LOCAL_THROTTLE) &&",
            "\t\t\t\t    mdtc->wb_dirty <",
            "\t\t\t\t    dirty_freerun_ceiling(mdtc->wb_thresh,",
            "\t\t\t\t\t\t\t  mdtc->wb_bg_thresh))",
            "\t\t\t\t\t/*",
            "\t\t\t\t\t * LOCAL_THROTTLE tasks must not be",
            "\t\t\t\t\t * throttled when below the per-wb",
            "\t\t\t\t\t * freerun ceiling.",
            "\t\t\t\t\t */",
            "\t\t\t\t\tgoto free_running;",
            "\t\t\t}",
            "\t\t\tdirty_exceeded |= (mdtc->wb_dirty > mdtc->wb_thresh) &&",
            "\t\t\t\t((mdtc->dirty > mdtc->thresh) || strictlimit);",
            "",
            "\t\t\twb_position_ratio(mdtc);",
            "\t\t\tif (mdtc->pos_ratio < gdtc->pos_ratio)",
            "\t\t\t\tsdtc = mdtc;",
            "\t\t}",
            "",
            "\t\tif (dirty_exceeded != wb->dirty_exceeded)",
            "\t\t\twb->dirty_exceeded = dirty_exceeded;",
            "",
            "\t\tif (time_is_before_jiffies(READ_ONCE(wb->bw_time_stamp) +",
            "\t\t\t\t\t   BANDWIDTH_INTERVAL))",
            "\t\t\t__wb_update_bandwidth(gdtc, mdtc, true);",
            "",
            "\t\t/* throttle according to the chosen dtc */",
            "\t\tdirty_ratelimit = READ_ONCE(wb->dirty_ratelimit);",
            "\t\ttask_ratelimit = ((u64)dirty_ratelimit * sdtc->pos_ratio) >>",
            "\t\t\t\t\t\t\tRATELIMIT_CALC_SHIFT;",
            "\t\tmax_pause = wb_max_pause(wb, sdtc->wb_dirty);",
            "\t\tmin_pause = wb_min_pause(wb, max_pause,",
            "\t\t\t\t\t task_ratelimit, dirty_ratelimit,",
            "\t\t\t\t\t &nr_dirtied_pause);",
            "",
            "\t\tif (unlikely(task_ratelimit == 0)) {",
            "\t\t\tperiod = max_pause;",
            "\t\t\tpause = max_pause;",
            "\t\t\tgoto pause;",
            "\t\t}",
            "\t\tperiod = HZ * pages_dirtied / task_ratelimit;",
            "\t\tpause = period;",
            "\t\tif (current->dirty_paused_when)",
            "\t\t\tpause -= now - current->dirty_paused_when;",
            "\t\t/*",
            "\t\t * For less than 1s think time (ext3/4 may block the dirtier",
            "\t\t * for up to 800ms from time to time on 1-HDD; so does xfs,",
            "\t\t * however at much less frequency), try to compensate it in",
            "\t\t * future periods by updating the virtual time; otherwise just",
            "\t\t * do a reset, as it may be a light dirtier.",
            "\t\t */",
            "\t\tif (pause < min_pause) {",
            "\t\t\ttrace_balance_dirty_pages(wb,",
            "\t\t\t\t\t\t  sdtc->thresh,",
            "\t\t\t\t\t\t  sdtc->bg_thresh,",
            "\t\t\t\t\t\t  sdtc->dirty,",
            "\t\t\t\t\t\t  sdtc->wb_thresh,",
            "\t\t\t\t\t\t  sdtc->wb_dirty,",
            "\t\t\t\t\t\t  dirty_ratelimit,",
            "\t\t\t\t\t\t  task_ratelimit,",
            "\t\t\t\t\t\t  pages_dirtied,",
            "\t\t\t\t\t\t  period,",
            "\t\t\t\t\t\t  min(pause, 0L),",
            "\t\t\t\t\t\t  start_time);",
            "\t\t\tif (pause < -HZ) {",
            "\t\t\t\tcurrent->dirty_paused_when = now;",
            "\t\t\t\tcurrent->nr_dirtied = 0;",
            "\t\t\t} else if (period) {",
            "\t\t\t\tcurrent->dirty_paused_when += period;",
            "\t\t\t\tcurrent->nr_dirtied = 0;",
            "\t\t\t} else if (current->nr_dirtied_pause <= pages_dirtied)",
            "\t\t\t\tcurrent->nr_dirtied_pause += pages_dirtied;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tif (unlikely(pause > max_pause)) {",
            "\t\t\t/* for occasional dropped task_ratelimit */",
            "\t\t\tnow += min(pause - max_pause, max_pause);",
            "\t\t\tpause = max_pause;",
            "\t\t}",
            "",
            "pause:",
            "\t\ttrace_balance_dirty_pages(wb,",
            "\t\t\t\t\t  sdtc->thresh,",
            "\t\t\t\t\t  sdtc->bg_thresh,",
            "\t\t\t\t\t  sdtc->dirty,",
            "\t\t\t\t\t  sdtc->wb_thresh,",
            "\t\t\t\t\t  sdtc->wb_dirty,",
            "\t\t\t\t\t  dirty_ratelimit,",
            "\t\t\t\t\t  task_ratelimit,",
            "\t\t\t\t\t  pages_dirtied,",
            "\t\t\t\t\t  period,",
            "\t\t\t\t\t  pause,",
            "\t\t\t\t\t  start_time);",
            "\t\tif (flags & BDP_ASYNC) {",
            "\t\t\tret = -EAGAIN;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\t__set_current_state(TASK_KILLABLE);",
            "\t\tbdi->last_bdp_sleep = jiffies;",
            "\t\tio_schedule_timeout(pause);",
            "",
            "\t\tcurrent->dirty_paused_when = now + pause;",
            "\t\tcurrent->nr_dirtied = 0;",
            "\t\tcurrent->nr_dirtied_pause = nr_dirtied_pause;",
            "",
            "\t\t/*",
            "\t\t * This is typically equal to (dirty < thresh) and can also",
            "\t\t * keep \"1000+ dd on a slow USB stick\" under control.",
            "\t\t */",
            "\t\tif (task_ratelimit)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * In the case of an unresponsive NFS server and the NFS dirty",
            "\t\t * pages exceeds dirty_thresh, give the other good wb's a pipe",
            "\t\t * to go through, so that tasks on them still remain responsive.",
            "\t\t *",
            "\t\t * In theory 1 page is enough to keep the consumer-producer",
            "\t\t * pipe going: the flusher cleans 1 page => the task dirties 1",
            "\t\t * more page. However wb_dirty has accounting errors.  So use",
            "\t\t * the larger and more IO friendly wb_stat_error.",
            "\t\t */",
            "\t\tif (sdtc->wb_dirty <= wb_stat_error())",
            "\t\t\tbreak;",
            "",
            "\t\tif (fatal_signal_pending(current))",
            "\t\t\tbreak;",
            "\t}",
            "\treturn ret;",
            "}"
          ],
          "function_name": "wb_dirty_limits, balance_dirty_pages",
          "description": "实现脏页管理核心逻辑，包含确定当前脏页限制的计算函数和脏页平衡主函数，通过多级阈值检测、动态速率限制、暂停时间控制等机制，在保证系统响应性的同时防止内存过载，支持严格限制模式下的特殊处理。",
          "similarity": 0.5427061915397644
        }
      ]
    },
    {
      "source_file": "mm/memory.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:42:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memory.c`\n\n---\n\n# memory.c 技术文档\n\n## 1. 文件概述\n\n`memory.c` 是 Linux 内核内存管理子系统（MM）的核心实现文件之一，位于 `mm/` 目录下。该文件主要负责虚拟内存到物理内存的映射管理、缺页异常（page fault）处理、页表结构的分配与释放、以及与用户空间内存操作相关的底层机制。它实现了按需加载（demand-loading）、共享页面、交换（swapping）等关键虚拟内存功能，并为多级页表架构（如 x86-64 的四级页表）提供通用支持。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `high_memory`：指向直接映射区域（ZONE_NORMAL）的上界，用于区分低端内存和高端内存。\n- `randomize_va_space`：控制地址空间布局随机化（ASLR）策略的级别（0=关闭，1=部分启用，2=完全启用）。\n- `zero_pfn`：指向全零物理页的页帧号（PFN），用于高效实现只读零页映射。\n- `max_mapnr` 和 `mem_map`（非 NUMA 配置下）：分别表示最大页帧号和全局页描述符数组。\n- `highest_memmap_pfn`：记录系统中最高的已注册页帧号。\n\n### 关键函数\n- `free_pgd_range()`：释放指定虚拟地址范围内的用户级页表结构（从 PGD 到 PTE）。\n- `free_p4d_range()`, `free_pud_range()`, `free_pmd_range()`, `free_pte_range()`：递归释放各级页表项及其对应的页表页。\n- `do_fault()`：处理基于文件映射的缺页异常。\n- `do_anonymous_page()`：处理匿名映射（如堆、栈）的缺页异常。\n- `vmf_orig_pte_uffd_wp()`：判断原始 PTE 是否为 userfaultfd 写保护标记。\n- `init_zero_pfn()`：早期初始化 `zero_pfn`。\n- `mm_trace_rss_stat()`：触发 RSS（Resident Set Size）统计的跟踪事件。\n\n### 内联辅助函数\n- `arch_wants_old_prefaulted_pte()`：允许架构层决定预取页表项是否应标记为“old”以优化访问位更新开销。\n\n## 3. 关键实现\n\n### 页表释放机制\n- 采用自顶向下（PGD → P4D → PUD → PMD → PTE）的递归方式释放页表。\n- 每级释放函数（如 `free_pmd_range`）遍历地址范围内的页表项：\n  - 跳过空或无效项（`pmd_none_or_clear_bad`）。\n  - 递归释放下一级页表。\n  - 在满足对齐和边界条件（`floor`/`ceiling`）时，释放当前级页表页并更新 MMU gather 结构中的计数器（如 `mm_dec_nr_ptes`）。\n- 使用 `mmu_gather` 机制批量延迟 TLB 刷新和页表页释放，提升性能。\n\n### 缺页处理框架\n- 提供 `do_fault` 和 `do_anonymous_page` 作为缺页处理的核心入口，分别处理文件映射和匿名映射。\n- 支持 `userfaultfd` 写保护机制，通过 `vmf_orig_pte_uffd_wp` 检测特殊 PTE 标记。\n\n### 地址空间随机化（ASLR）\n- 通过 `randomize_va_space` 控制栈、mmap 区域、brk 等的随机化行为。\n- 支持内核启动参数 `norandmaps` 完全禁用 ASLR。\n- 兼容旧版 libc5 二进制（`CONFIG_COMPAT_BRK`），此时 brk 区域不参与随机化。\n\n### 零页优化\n- `zero_pfn` 指向一个全局只读的全零物理页，用于高效实现对未初始化数据段（如 `.bss`）或显式映射 `/dev/zero` 的只读访问，避免每次分配新页。\n\n### 架构适配\n- 通过 `arch_wants_old_prefaulted_pte` 允许特定架构优化页表项的“young/old”状态设置。\n- 依赖 `asm/mmu_context.h`、`asm/pgalloc.h`、`asm/tlb.h` 等架构相关头文件实现底层操作。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- **内存管理核心**：`<linux/mm.h>`, `<linux/mman.h>`, `<linux/swap.h>`, `<linux/pagemap.h>`, `<linux/memcontrol.h>`\n- **进程与调度**：`<linux/sched/mm.h>`, `<linux/sched/task.h>`, `<linux/delayacct.h>`\n- **NUMA 与迁移**：`<linux/numa.h>`, `<linux/migrate.h>`, `<linux/sched/numa_balancing.h>`\n- **特殊内存类型**：`<linux/hugetlb.h>`, `<linux/highmem.h>`, `<linux/dax.h>`, `<linux/zswap.h>`\n- **调试与跟踪**：`<trace/events/kmem.h>`, `<linux/debugfs.h>`, `<linux/oom.h>`\n- **架构相关**：`<asm/io.h>`, `<asm/mmu_context.h>`, `<asm/pgalloc.h>`, `<asm/tlbflush.h>`\n\n### 内部模块依赖\n- `internal.h`：包含 MM 子系统内部通用定义。\n- `swap.h`：交换子系统接口。\n- `pgalloc-track.h`：页表分配跟踪（用于调试）。\n\n## 5. 使用场景\n\n- **进程创建与退出**：在 `fork()` 和进程终止时，通过 `free_pgd_range` 释放整个地址空间的页表。\n- **内存映射操作**：`mmap()`、`munmap()`、`mremap()` 等系统调用触发页表的建立或释放。\n- **缺页异常处理**：当 CPU 访问未映射或换出的虚拟地址时，由体系结构相关的缺页处理程序调用 `do_fault` 或 `do_anonymous_page`。\n- **内存回收**：在内存压力下，kswapd 或直接回收路径可能触发页表清理。\n- **用户态内存监控**：`userfaultfd` 机制利用 `vmf_orig_pte_uffd_wp` 实现用户空间对缺页事件的精细控制。\n- **内核初始化**：早期调用 `init_zero_pfn` 设置零页，`paging_init()`（架构相关）初始化 `high_memory` 和 `ZERO_PAGE`。",
      "similarity": 0.5501303672790527,
      "chunks": [
        {
          "chunk_id": 19,
          "file_path": "mm/memory.c",
          "start_line": 3088,
          "end_line": 3198,
          "content": [
            "static gfp_t __get_fault_gfp_mask(struct vm_area_struct *vma)",
            "{",
            "\tstruct file *vm_file = vma->vm_file;",
            "",
            "\tif (vm_file)",
            "\t\treturn mapping_gfp_mask(vm_file->f_mapping) | __GFP_FS | __GFP_IO;",
            "",
            "\t/*",
            "\t * Special mappings (e.g. VDSO) do not have any file so fake",
            "\t * a default GFP_KERNEL for them.",
            "\t */",
            "\treturn GFP_KERNEL;",
            "}",
            "static vm_fault_t do_page_mkwrite(struct vm_fault *vmf, struct folio *folio)",
            "{",
            "\tvm_fault_t ret;",
            "\tunsigned int old_flags = vmf->flags;",
            "",
            "\tvmf->flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;",
            "",
            "\tif (vmf->vma->vm_file &&",
            "\t    IS_SWAPFILE(vmf->vma->vm_file->f_mapping->host))",
            "\t\treturn VM_FAULT_SIGBUS;",
            "",
            "\tret = vmf->vma->vm_ops->page_mkwrite(vmf);",
            "\t/* Restore original flags so that caller is not surprised */",
            "\tvmf->flags = old_flags;",
            "\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))",
            "\t\treturn ret;",
            "\tif (unlikely(!(ret & VM_FAULT_LOCKED))) {",
            "\t\tfolio_lock(folio);",
            "\t\tif (!folio->mapping) {",
            "\t\t\tfolio_unlock(folio);",
            "\t\t\treturn 0; /* retry */",
            "\t\t}",
            "\t\tret |= VM_FAULT_LOCKED;",
            "\t} else",
            "\t\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\treturn ret;",
            "}",
            "static vm_fault_t fault_dirty_shared_page(struct vm_fault *vmf)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tstruct address_space *mapping;",
            "\tstruct folio *folio = page_folio(vmf->page);",
            "\tbool dirtied;",
            "\tbool page_mkwrite = vma->vm_ops && vma->vm_ops->page_mkwrite;",
            "",
            "\tdirtied = folio_mark_dirty(folio);",
            "\tVM_BUG_ON_FOLIO(folio_test_anon(folio), folio);",
            "\t/*",
            "\t * Take a local copy of the address_space - folio.mapping may be zeroed",
            "\t * by truncate after folio_unlock().   The address_space itself remains",
            "\t * pinned by vma->vm_file's reference.  We rely on folio_unlock()'s",
            "\t * release semantics to prevent the compiler from undoing this copying.",
            "\t */",
            "\tmapping = folio_raw_mapping(folio);",
            "\tfolio_unlock(folio);",
            "",
            "\tif (!page_mkwrite)",
            "\t\tfile_update_time(vma->vm_file);",
            "",
            "\t/*",
            "\t * Throttle page dirtying rate down to writeback speed.",
            "\t *",
            "\t * mapping may be NULL here because some device drivers do not",
            "\t * set page.mapping but still dirty their pages",
            "\t *",
            "\t * Drop the mmap_lock before waiting on IO, if we can. The file",
            "\t * is pinning the mapping, as per above.",
            "\t */",
            "\tif ((dirtied || page_mkwrite) && mapping) {",
            "\t\tstruct file *fpin;",
            "",
            "\t\tfpin = maybe_unlock_mmap_for_io(vmf, NULL);",
            "\t\tbalance_dirty_pages_ratelimited(mapping);",
            "\t\tif (fpin) {",
            "\t\t\tfput(fpin);",
            "\t\t\treturn VM_FAULT_COMPLETED;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static inline void wp_page_reuse(struct vm_fault *vmf, struct folio *folio)",
            "\t__releases(vmf->ptl)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tpte_t entry;",
            "",
            "\tVM_BUG_ON(!(vmf->flags & FAULT_FLAG_WRITE));",
            "",
            "\tif (folio) {",
            "\t\tVM_BUG_ON(folio_test_anon(folio) &&",
            "\t\t\t  !PageAnonExclusive(vmf->page));",
            "\t\t/*",
            "\t\t * Clear the folio's cpupid information as the existing",
            "\t\t * information potentially belongs to a now completely",
            "\t\t * unrelated process.",
            "\t\t */",
            "\t\tfolio_xchg_last_cpupid(folio, (1 << LAST_CPUPID_SHIFT) - 1);",
            "\t}",
            "",
            "\tflush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));",
            "\tentry = pte_mkyoung(vmf->orig_pte);",
            "\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);",
            "\tif (ptep_set_access_flags(vma, vmf->address, vmf->pte, entry, 1))",
            "\t\tupdate_mmu_cache_range(vmf, vma, vmf->address, vmf->pte, 1);",
            "\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\tcount_vm_event(PGREUSE);",
            "}"
          ],
          "function_name": "__get_fault_gfp_mask, do_page_mkwrite, fault_dirty_shared_page, wp_page_reuse",
          "description": "处理页面写入相关逻辑，__get_fault_gfp_mask计算内存分配掩码，do_page_mkwrite处理写入时的页面锁定和更新，fault_dirty_shared_page处理共享页面脏页标记，wp_page_reuse实现页面复用和访问标志更新。",
          "similarity": 0.6051582098007202
        },
        {
          "chunk_id": 30,
          "file_path": "mm/memory.c",
          "start_line": 5310,
          "end_line": 5439,
          "content": [
            "static vm_fault_t do_shared_fault(struct vm_fault *vmf)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tvm_fault_t ret, tmp;",
            "\tstruct folio *folio;",
            "",
            "\tret = vmf_can_call_fault(vmf);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tret = __do_fault(vmf);",
            "\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))",
            "\t\treturn ret;",
            "",
            "\tfolio = page_folio(vmf->page);",
            "",
            "\t/*",
            "\t * Check if the backing address space wants to know that the page is",
            "\t * about to become writable",
            "\t */",
            "\tif (vma->vm_ops->page_mkwrite) {",
            "\t\tfolio_unlock(folio);",
            "\t\ttmp = do_page_mkwrite(vmf, folio);",
            "\t\tif (unlikely(!tmp ||",
            "\t\t\t\t(tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn tmp;",
            "\t\t}",
            "\t}",
            "",
            "\tret |= finish_fault(vmf);",
            "\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |",
            "\t\t\t\t\tVM_FAULT_RETRY))) {",
            "\t\tfolio_unlock(folio);",
            "\t\tfolio_put(folio);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tret |= fault_dirty_shared_page(vmf);",
            "\treturn ret;",
            "}",
            "static vm_fault_t do_fault(struct vm_fault *vmf)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tstruct mm_struct *vm_mm = vma->vm_mm;",
            "\tvm_fault_t ret;",
            "",
            "\t/*",
            "\t * The VMA was not fully populated on mmap() or missing VM_DONTEXPAND",
            "\t */",
            "\tif (!vma->vm_ops->fault) {",
            "\t\tvmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\t       vmf->address, &vmf->ptl);",
            "\t\tif (unlikely(!vmf->pte))",
            "\t\t\tret = VM_FAULT_SIGBUS;",
            "\t\telse {",
            "\t\t\t/*",
            "\t\t\t * Make sure this is not a temporary clearing of pte",
            "\t\t\t * by holding ptl and checking again. A R/M/W update",
            "\t\t\t * of pte involves: take ptl, clearing the pte so that",
            "\t\t\t * we don't have concurrent modification by hardware",
            "\t\t\t * followed by an update.",
            "\t\t\t */",
            "\t\t\tif (unlikely(pte_none(ptep_get(vmf->pte))))",
            "\t\t\t\tret = VM_FAULT_SIGBUS;",
            "\t\t\telse",
            "\t\t\t\tret = VM_FAULT_NOPAGE;",
            "",
            "\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\t\t}",
            "\t} else if (!(vmf->flags & FAULT_FLAG_WRITE))",
            "\t\tret = do_read_fault(vmf);",
            "\telse if (!(vma->vm_flags & VM_SHARED))",
            "\t\tret = do_cow_fault(vmf);",
            "\telse",
            "\t\tret = do_shared_fault(vmf);",
            "",
            "\t/* preallocated pagetable is unused: free it */",
            "\tif (vmf->prealloc_pte) {",
            "\t\tpte_free(vm_mm, vmf->prealloc_pte);",
            "\t\tvmf->prealloc_pte = NULL;",
            "\t}",
            "\treturn ret;",
            "}",
            "int numa_migrate_check(struct folio *folio, struct vm_fault *vmf,",
            "\t\t      unsigned long addr, int *flags,",
            "\t\t      bool writable, int *last_cpupid)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "",
            "\t/*",
            "\t * Avoid grouping on RO pages in general. RO pages shouldn't hurt as",
            "\t * much anyway since they can be in shared cache state. This misses",
            "\t * the case where a mapping is writable but the process never writes",
            "\t * to it but pte_write gets cleared during protection updates and",
            "\t * pte_dirty has unpredictable behaviour between PTE scan updates,",
            "\t * background writeback, dirty balancing and application behaviour.",
            "\t */",
            "\tif (!writable)",
            "\t\t*flags |= TNF_NO_GROUP;",
            "",
            "\t/*",
            "\t * Flag if the folio is shared between multiple address spaces. This",
            "\t * is later used when determining whether to group tasks together",
            "\t */",
            "\tif (folio_likely_mapped_shared(folio) && (vma->vm_flags & VM_SHARED))",
            "\t\t*flags |= TNF_SHARED;",
            "\t/*",
            "\t * For memory tiering mode, cpupid of slow memory page is used",
            "\t * to record page access time.  So use default value.",
            "\t */",
            "\tif (folio_use_access_time(folio))",
            "\t\t*last_cpupid = (-1 & LAST_CPUPID_MASK);",
            "\telse",
            "\t\t*last_cpupid = folio_last_cpupid(folio);",
            "",
            "\t/* Record the current PID acceesing VMA */",
            "\tvma_set_access_pid_bit(vma);",
            "",
            "\tcount_vm_numa_event(NUMA_HINT_FAULTS);",
            "#ifdef CONFIG_NUMA_BALANCING",
            "\tcount_memcg_folio_events(folio, NUMA_HINT_FAULTS, 1);",
            "#endif",
            "\tif (folio_nid(folio) == numa_node_id()) {",
            "\t\tcount_vm_numa_event(NUMA_HINT_FAULTS_LOCAL);",
            "\t\t*flags |= TNF_FAULT_LOCAL;",
            "\t}",
            "",
            "\treturn mpol_misplaced(folio, vmf, addr);",
            "}"
          ],
          "function_name": "do_shared_fault, do_fault, numa_migrate_check",
          "description": "实现共享写时复制（Copy-on-Write）逻辑，处理页面故障时检查是否需要触发页共享写入操作，并进行页面迁移检查以优化NUMA性能",
          "similarity": 0.5785341858863831
        },
        {
          "chunk_id": 21,
          "file_path": "mm/memory.c",
          "start_line": 3455,
          "end_line": 3580,
          "content": [
            "static vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf, struct folio *folio)",
            "{",
            "\tWARN_ON_ONCE(!(vmf->vma->vm_flags & VM_SHARED));",
            "\tvmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd, vmf->address,",
            "\t\t\t\t       &vmf->ptl);",
            "\tif (!vmf->pte)",
            "\t\treturn VM_FAULT_NOPAGE;",
            "\t/*",
            "\t * We might have raced with another page fault while we released the",
            "\t * pte_offset_map_lock.",
            "\t */",
            "\tif (!pte_same(ptep_get(vmf->pte), vmf->orig_pte)) {",
            "\t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\t\treturn VM_FAULT_NOPAGE;",
            "\t}",
            "\twp_page_reuse(vmf, folio);",
            "\treturn 0;",
            "}",
            "static vm_fault_t wp_pfn_shared(struct vm_fault *vmf)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->pfn_mkwrite) {",
            "\t\tvm_fault_t ret;",
            "",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\t\tret = vmf_can_call_fault(vmf);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "",
            "\t\tvmf->flags |= FAULT_FLAG_MKWRITE;",
            "\t\tret = vma->vm_ops->pfn_mkwrite(vmf);",
            "\t\tif (ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE))",
            "\t\t\treturn ret;",
            "\t\treturn finish_mkwrite_fault(vmf, NULL);",
            "\t}",
            "\twp_page_reuse(vmf, NULL);",
            "\treturn 0;",
            "}",
            "static vm_fault_t wp_page_shared(struct vm_fault *vmf, struct folio *folio)",
            "\t__releases(vmf->ptl)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tvm_fault_t ret = 0;",
            "",
            "\tfolio_get(folio);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->page_mkwrite) {",
            "\t\tvm_fault_t tmp;",
            "",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\t\ttmp = vmf_can_call_fault(vmf);",
            "\t\tif (tmp) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn tmp;",
            "\t\t}",
            "",
            "\t\ttmp = do_page_mkwrite(vmf, folio);",
            "\t\tif (unlikely(!tmp || (tmp &",
            "\t\t\t\t      (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn tmp;",
            "\t\t}",
            "\t\ttmp = finish_mkwrite_fault(vmf, folio);",
            "\t\tif (unlikely(tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE))) {",
            "\t\t\tfolio_unlock(folio);",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn tmp;",
            "\t\t}",
            "\t} else {",
            "\t\twp_page_reuse(vmf, folio);",
            "\t\tfolio_lock(folio);",
            "\t}",
            "\tret |= fault_dirty_shared_page(vmf);",
            "\tfolio_put(folio);",
            "",
            "\treturn ret;",
            "}",
            "static bool wp_can_reuse_anon_folio(struct folio *folio,",
            "\t\t\t\t    struct vm_area_struct *vma)",
            "{",
            "\t/*",
            "\t * We could currently only reuse a subpage of a large folio if no",
            "\t * other subpages of the large folios are still mapped. However,",
            "\t * let's just consistently not reuse subpages even if we could",
            "\t * reuse in that scenario, and give back a large folio a bit",
            "\t * sooner.",
            "\t */",
            "\tif (folio_test_large(folio))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * We have to verify under folio lock: these early checks are",
            "\t * just an optimization to avoid locking the folio and freeing",
            "\t * the swapcache if there is little hope that we can reuse.",
            "\t *",
            "\t * KSM doesn't necessarily raise the folio refcount.",
            "\t */",
            "\tif (folio_test_ksm(folio) || folio_ref_count(folio) > 3)",
            "\t\treturn false;",
            "\tif (!folio_test_lru(folio))",
            "\t\t/*",
            "\t\t * We cannot easily detect+handle references from",
            "\t\t * remote LRU caches or references to LRU folios.",
            "\t\t */",
            "\t\tlru_add_drain();",
            "\tif (folio_ref_count(folio) > 1 + folio_test_swapcache(folio))",
            "\t\treturn false;",
            "\tif (!folio_trylock(folio))",
            "\t\treturn false;",
            "\tif (folio_test_swapcache(folio))",
            "\t\tfolio_free_swap(folio);",
            "\tif (folio_test_ksm(folio) || folio_ref_count(folio) != 1) {",
            "\t\tfolio_unlock(folio);",
            "\t\treturn false;",
            "\t}",
            "\t/*",
            "\t * Ok, we've got the only folio reference from our mapping",
            "\t * and the folio is locked, it's dark out, and we're wearing",
            "\t * sunglasses. Hit it.",
            "\t */",
            "\tfolio_move_anon_rmap(folio, vma);",
            "\tfolio_unlock(folio);",
            "\treturn true;",
            "}"
          ],
          "function_name": "finish_mkwrite_fault, wp_pfn_shared, wp_page_shared, wp_can_reuse_anon_folio",
          "description": "处理共享写入场景，通过判断是否存在pfn_mkwrite回调或直接复用页面，优化共享映射的写入路径",
          "similarity": 0.5668959617614746
        },
        {
          "chunk_id": 34,
          "file_path": "mm/memory.c",
          "start_line": 5940,
          "end_line": 6040,
          "content": [
            "static void lru_gen_enter_fault(struct vm_area_struct *vma)",
            "{",
            "\t/* the LRU algorithm only applies to accesses with recency */",
            "\tcurrent->in_lru_fault = vma_has_recency(vma);",
            "}",
            "static void lru_gen_exit_fault(void)",
            "{",
            "\tcurrent->in_lru_fault = false;",
            "}",
            "static void lru_gen_enter_fault(struct vm_area_struct *vma)",
            "{",
            "}",
            "static void lru_gen_exit_fault(void)",
            "{",
            "}",
            "static vm_fault_t sanitize_fault_flags(struct vm_area_struct *vma,",
            "\t\t\t\t       unsigned int *flags)",
            "{",
            "\tif (unlikely(*flags & FAULT_FLAG_UNSHARE)) {",
            "\t\tif (WARN_ON_ONCE(*flags & FAULT_FLAG_WRITE))",
            "\t\t\treturn VM_FAULT_SIGSEGV;",
            "\t\t/*",
            "\t\t * FAULT_FLAG_UNSHARE only applies to COW mappings. Let's",
            "\t\t * just treat it like an ordinary read-fault otherwise.",
            "\t\t */",
            "\t\tif (!is_cow_mapping(vma->vm_flags))",
            "\t\t\t*flags &= ~FAULT_FLAG_UNSHARE;",
            "\t} else if (*flags & FAULT_FLAG_WRITE) {",
            "\t\t/* Write faults on read-only mappings are impossible ... */",
            "\t\tif (WARN_ON_ONCE(!(vma->vm_flags & VM_MAYWRITE)))",
            "\t\t\treturn VM_FAULT_SIGSEGV;",
            "\t\t/* ... and FOLL_FORCE only applies to COW mappings. */",
            "\t\tif (WARN_ON_ONCE(!(vma->vm_flags & VM_WRITE) &&",
            "\t\t\t\t !is_cow_mapping(vma->vm_flags)))",
            "\t\t\treturn VM_FAULT_SIGSEGV;",
            "\t}",
            "#ifdef CONFIG_PER_VMA_LOCK",
            "\t/*",
            "\t * Per-VMA locks can't be used with FAULT_FLAG_RETRY_NOWAIT because of",
            "\t * the assumption that lock is dropped on VM_FAULT_RETRY.",
            "\t */",
            "\tif (WARN_ON_ONCE((*flags &",
            "\t\t\t(FAULT_FLAG_VMA_LOCK | FAULT_FLAG_RETRY_NOWAIT)) ==",
            "\t\t\t(FAULT_FLAG_VMA_LOCK | FAULT_FLAG_RETRY_NOWAIT)))",
            "\t\treturn VM_FAULT_SIGSEGV;",
            "#endif",
            "",
            "\treturn 0;",
            "}",
            "vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,",
            "\t\t\t   unsigned int flags, struct pt_regs *regs)",
            "{",
            "\t/* If the fault handler drops the mmap_lock, vma may be freed */",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tvm_fault_t ret;",
            "",
            "\t__set_current_state(TASK_RUNNING);",
            "",
            "\tret = sanitize_fault_flags(vma, &flags);",
            "\tif (ret)",
            "\t\tgoto out;",
            "",
            "\tif (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,",
            "\t\t\t\t\t    flags & FAULT_FLAG_INSTRUCTION,",
            "\t\t\t\t\t    flags & FAULT_FLAG_REMOTE)) {",
            "\t\tret = VM_FAULT_SIGSEGV;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/*",
            "\t * Enable the memcg OOM handling for faults triggered in user",
            "\t * space.  Kernel faults are handled more gracefully.",
            "\t */",
            "\tif (flags & FAULT_FLAG_USER)",
            "\t\tmem_cgroup_enter_user_fault();",
            "",
            "\tlru_gen_enter_fault(vma);",
            "",
            "\tif (unlikely(is_vm_hugetlb_page(vma)))",
            "\t\tret = hugetlb_fault(vma->vm_mm, vma, address, flags);",
            "\telse",
            "\t\tret = __handle_mm_fault(vma, address, flags);",
            "",
            "\tlru_gen_exit_fault();",
            "",
            "\tif (flags & FAULT_FLAG_USER) {",
            "\t\tmem_cgroup_exit_user_fault();",
            "\t\t/*",
            "\t\t * The task may have entered a memcg OOM situation but",
            "\t\t * if the allocation error was handled gracefully (no",
            "\t\t * VM_FAULT_OOM), there is no need to kill anything.",
            "\t\t * Just clean up the OOM state peacefully.",
            "\t\t */",
            "\t\tif (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))",
            "\t\t\tmem_cgroup_oom_synchronize(false);",
            "\t}",
            "out:",
            "\tmm_account_fault(mm, regs, address, flags, ret);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "lru_gen_enter_fault, lru_gen_exit_fault, lru_gen_enter_fault, lru_gen_exit_fault, sanitize_fault_flags, handle_mm_fault",
          "description": "维护LRU算法状态标记，校验页面故障标志有效性，并作为内存故障入口点协调权限检查、NUMA处理及统计记录流程",
          "similarity": 0.5466064810752869
        },
        {
          "chunk_id": 22,
          "file_path": "mm/memory.c",
          "start_line": 3611,
          "end_line": 3728,
          "content": [
            "static vm_fault_t do_wp_page(struct vm_fault *vmf)",
            "\t__releases(vmf->ptl)",
            "{",
            "\tconst bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tstruct folio *folio = NULL;",
            "\tpte_t pte;",
            "",
            "\tif (likely(!unshare)) {",
            "\t\tif (userfaultfd_pte_wp(vma, ptep_get(vmf->pte))) {",
            "\t\t\tif (!userfaultfd_wp_async(vma)) {",
            "\t\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\t\t\t\treturn handle_userfault(vmf, VM_UFFD_WP);",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * Nothing needed (cache flush, TLB invalidations,",
            "\t\t\t * etc.) because we're only removing the uffd-wp bit,",
            "\t\t\t * which is completely invisible to the user.",
            "\t\t\t */",
            "\t\t\tpte = pte_clear_uffd_wp(ptep_get(vmf->pte));",
            "",
            "\t\t\tset_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);",
            "\t\t\t/*",
            "\t\t\t * Update this to be prepared for following up CoW",
            "\t\t\t * handling",
            "\t\t\t */",
            "\t\t\tvmf->orig_pte = pte;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Userfaultfd write-protect can defer flushes. Ensure the TLB",
            "\t\t * is flushed in this case before copying.",
            "\t\t */",
            "\t\tif (unlikely(userfaultfd_wp(vmf->vma) &&",
            "\t\t\t     mm_tlb_flush_pending(vmf->vma->vm_mm)))",
            "\t\t\tflush_tlb_page(vmf->vma, vmf->address);",
            "\t}",
            "",
            "\tvmf->page = vm_normal_page(vma, vmf->address, vmf->orig_pte);",
            "",
            "\tif (vmf->page)",
            "\t\tfolio = page_folio(vmf->page);",
            "",
            "\t/*",
            "\t * Shared mapping: we are guaranteed to have VM_WRITE and",
            "\t * FAULT_FLAG_WRITE set at this point.",
            "\t */",
            "\tif (vma->vm_flags & (VM_SHARED | VM_MAYSHARE)) {",
            "\t\t/*",
            "\t\t * VM_MIXEDMAP !pfn_valid() case, or VM_SOFTDIRTY clear on a",
            "\t\t * VM_PFNMAP VMA.",
            "\t\t *",
            "\t\t * We should not cow pages in a shared writeable mapping.",
            "\t\t * Just mark the pages writable and/or call ops->pfn_mkwrite.",
            "\t\t */",
            "\t\tif (!vmf->page)",
            "\t\t\treturn wp_pfn_shared(vmf);",
            "\t\treturn wp_page_shared(vmf, folio);",
            "\t}",
            "",
            "\t/*",
            "\t * Private mapping: create an exclusive anonymous page copy if reuse",
            "\t * is impossible. We might miss VM_WRITE for FOLL_FORCE handling.",
            "\t *",
            "\t * If we encounter a page that is marked exclusive, we must reuse",
            "\t * the page without further checks.",
            "\t */",
            "\tif (folio && folio_test_anon(folio) &&",
            "\t    (PageAnonExclusive(vmf->page) || wp_can_reuse_anon_folio(folio, vma))) {",
            "\t\tif (!PageAnonExclusive(vmf->page))",
            "\t\t\tSetPageAnonExclusive(vmf->page);",
            "\t\tif (unlikely(unshare)) {",
            "\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t\twp_page_reuse(vmf, folio);",
            "\t\treturn 0;",
            "\t}",
            "\t/*",
            "\t * Ok, we need to copy. Oh, well..",
            "\t */",
            "\tif (folio)",
            "\t\tfolio_get(folio);",
            "",
            "\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "#ifdef CONFIG_KSM",
            "\tif (folio && folio_test_ksm(folio))",
            "\t\tcount_vm_event(COW_KSM);",
            "#endif",
            "\treturn wp_page_copy(vmf);",
            "}",
            "static void unmap_mapping_range_vma(struct vm_area_struct *vma,",
            "\t\tunsigned long start_addr, unsigned long end_addr,",
            "\t\tstruct zap_details *details)",
            "{",
            "\tzap_page_range_single(vma, start_addr, end_addr - start_addr, details);",
            "}",
            "static inline void unmap_mapping_range_tree(struct rb_root_cached *root,",
            "\t\t\t\t\t    pgoff_t first_index,",
            "\t\t\t\t\t    pgoff_t last_index,",
            "\t\t\t\t\t    struct zap_details *details)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tpgoff_t vba, vea, zba, zea;",
            "",
            "\tvma_interval_tree_foreach(vma, root, first_index, last_index) {",
            "\t\tvba = vma->vm_pgoff;",
            "\t\tvea = vba + vma_pages(vma) - 1;",
            "\t\tzba = max(first_index, vba);",
            "\t\tzea = min(last_index, vea);",
            "",
            "\t\tunmap_mapping_range_vma(vma,",
            "\t\t\t((zba - vba) << PAGE_SHIFT) + vma->vm_start,",
            "\t\t\t((zea - vba + 1) << PAGE_SHIFT) + vma->vm_start,",
            "\t\t\t\tdetails);",
            "\t}",
            "}"
          ],
          "function_name": "do_wp_page, unmap_mapping_range_vma, unmap_mapping_range_tree",
          "description": "实现写保护页面处理主逻辑，区分共享/私有映射分支，调用相应子函数并解绑地址空间范围",
          "similarity": 0.5324934124946594
        }
      ]
    }
  ]
}