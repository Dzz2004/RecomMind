{
  "query": "内存管理中的页回收算法",
  "timestamp": "2025-12-26 00:21:27",
  "retrieved_files": [
    {
      "source_file": "mm/vmscan.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:33:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmscan.c`\n\n---\n\n# vmscan.c 技术文档\n\n## 1. 文件概述\n\n`vmscan.c` 是 Linux 内核内存管理子系统中的核心文件，主要负责**页面回收（page reclaim）**机制的实现。该文件实现了内核在内存压力下如何选择并释放不再活跃或可回收的物理页帧（pages），以维持系统可用内存水位。其核心功能包括：\n\n- 实现 `kswapd` 内核线程，用于后台异步回收内存\n- 提供直接回收（direct reclaim）路径，供分配器在内存不足时同步触发\n- 管理匿名页（anonymous pages）和文件缓存页（file-backed pages）的回收策略\n- 支持基于内存控制组（memcg）的层级化内存回收\n- 与交换（swap）、压缩（compaction）、OOM killer 等子系统协同工作\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct scan_control`**  \n  页面回收上下文控制结构，包含本次回收操作的所有参数和状态：\n  - `nr_to_reclaim`：目标回收页数\n  - `target_mem_cgroup`：目标内存 cgroup（用于 memcg 回收）\n  - `may_unmap` / `may_swap` / `may_writepage`：控制是否允许解除映射、交换、写回\n  - `priority`：扫描优先级（0~12，值越低压力越大）\n  - `order`：请求分配的阶数（影响回收激进程度）\n  - `nr_scanned` / `nr_reclaimed`：已扫描和已回收页数统计\n  - `anon_cost` / `file_cost`：用于平衡匿名页与文件页回收比例\n\n- **全局变量**\n  - `vm_swappiness`（默认 60）：控制系统倾向于回收匿名页（需 swap）还是文件页（可丢弃）\n\n### 主要函数（部分在代码片段中体现）\n\n- `cgroup_reclaim()` / `root_reclaim()`：判断当前回收是否针对特定 memcg 或全局\n- `writeback_throttling_sane()`：判断是否可使用标准脏页限流机制\n- `set_task_reclaim_state()` / `flush_reclaim_state()`：管理任务的 slab 回收状态\n- （注：核心回收函数如 `shrink_lruvec()`、`kswapd()` 等未在片段中展示）\n\n## 3. 关键实现\n\n### 内存回收控制逻辑\n\n- **回收目标决策**：通过 `scan_control` 结构传递回收上下文，区分直接回收（分配失败触发）与 kswapd 后台回收。\n- **LRU 链管理**：利用 `prefetchw_prev_lru_folio` 宏优化 LRU 链遍历时的 CPU 缓存预取性能。\n- **Memcg 集成**：\n  - 若 `target_mem_cgroup` 非空，则优先回收该 cgroup 的内存\n  - 支持 `memory.low` 保护机制：当常规回收无法满足需求且跳过受保护 cgroup 时，会触发二次强制回收（`memcg_low_reclaim`）\n- **脏页处理策略**：\n  - 在传统 memcg 模式下，禁用标准 `balance_dirty_pages()` 限流，改用直接阻塞回收（`writeback_throttling_sane()` 判断）\n  - 通过 `may_writepage` 控制是否在 laptop mode 下批量写回脏页\n\n### 回收统计与状态同步\n\n- **Slab 回收计数**：通过 `reclaim_state` 结构将非 LRU 回收（如 slab 释放）计入全局统计，但**仅在全局回收时计入**，避免 memcg 回收时高估实际效果导致欠回收。\n- **PSI/Trace 集成**：包含 `<trace/events/vmscan.h>` 用于性能分析，支持压力状态指示器（PSI）监控内存压力。\n\n## 4. 依赖关系\n\n### 头文件依赖\n\n- **核心内存管理**：`<linux/mm.h>`, `<linux/gfp.h>`, `<linux/swap.h>`, `<linux/vmstat.h>`\n- **LRU 与反向映射**：`<linux/rmap.h>`, `<linux/pagemap.h>`\n- **内存控制组**：`<linux/memcontrol.h>`\n- **IO 与写回**：`<linux/writeback.h>`, `<linux/backing-dev.h>`\n- **压缩与迁移**：`<linux/compaction.h>`, `<linux/migrate.h>`\n- **体系结构相关**：`<asm/tlbflush.h>`\n\n### 子系统交互\n\n- **Swap 子系统**：通过 `swapops.h` 和 `swap.h` 实现匿名页换出\n- **Slab 分配器**：通过 `reclaim_state` 接收 slab 回收通知\n- **OOM Killer**：当回收无法释放足够内存时触发\n- **Khugepaged**：大页合并/拆分与回收协同\n- **Memory Tiering**：支持分层内存架构中的页降级（demotion）控制\n\n## 5. 使用场景\n\n- **内存分配失败时的直接回收**：当 `alloc_pages()` 等分配函数无法满足请求时，同步调用回收路径。\n- **kswapd 后台回收**：当空闲内存低于 `watermark[low]` 时，唤醒 `kswapd` 线程异步回收至 `watermark[high]`。\n- **Memcg 内存超限时的层级回收**：当某个 cgroup 超过其内存限制时，仅回收该 cgroup 及其子树的页面。\n- **系统休眠（Hibernation）**：通过 `hibernation_mode` 标志优化休眠过程中的内存回收。\n- **主动内存回收（Proactive Reclaim）**：用户空间通过 `memory.reclaim` 接口触发预清回收。\n- **内存压缩准备**：当 `compaction_ready` 置位时，回收操作会为后续内存压缩腾出连续空间。",
      "similarity": 0.6530395150184631,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/vmscan.c",
          "start_line": 343,
          "end_line": 443,
          "content": [
            "unsigned long zone_reclaimable_pages(struct zone *zone)",
            "{",
            "\tunsigned long nr;",
            "",
            "\tnr = zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_FILE) +",
            "\t\tzone_page_state_snapshot(zone, NR_ZONE_ACTIVE_FILE);",
            "\tif (can_reclaim_anon_pages(NULL, zone_to_nid(zone), NULL))",
            "\t\tnr += zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_ANON) +",
            "\t\t\tzone_page_state_snapshot(zone, NR_ZONE_ACTIVE_ANON);",
            "\t/*",
            "\t * If there are no reclaimable file-backed or anonymous pages,",
            "\t * ensure zones with sufficient free pages are not skipped.",
            "\t * This prevents zones like DMA32 from being ignored in reclaim",
            "\t * scenarios where they can still help alleviate memory pressure.",
            "\t */",
            "\tif (nr == 0)",
            "\t\tnr = zone_page_state_snapshot(zone, NR_FREE_PAGES);",
            "\treturn nr;",
            "}",
            "static unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru,",
            "\t\t\t\t     int zone_idx)",
            "{",
            "\tunsigned long size = 0;",
            "\tint zid;",
            "",
            "\tfor (zid = 0; zid <= zone_idx; zid++) {",
            "\t\tstruct zone *zone = &lruvec_pgdat(lruvec)->node_zones[zid];",
            "",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!mem_cgroup_disabled())",
            "\t\t\tsize += mem_cgroup_get_zone_lru_size(lruvec, lru, zid);",
            "\t\telse",
            "\t\t\tsize += zone_page_state(zone, NR_ZONE_LRU_BASE + lru);",
            "\t}",
            "\treturn size;",
            "}",
            "static unsigned long drop_slab_node(int nid)",
            "{",
            "\tunsigned long freed = 0;",
            "\tstruct mem_cgroup *memcg = NULL;",
            "",
            "\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);",
            "\tdo {",
            "\t\tfreed += shrink_slab(GFP_KERNEL, nid, memcg, 0);",
            "\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)) != NULL);",
            "",
            "\treturn freed;",
            "}",
            "void drop_slab(void)",
            "{",
            "\tint nid;",
            "\tint shift = 0;",
            "\tunsigned long freed;",
            "",
            "\tdo {",
            "\t\tfreed = 0;",
            "\t\tfor_each_online_node(nid) {",
            "\t\t\tif (fatal_signal_pending(current))",
            "\t\t\t\treturn;",
            "",
            "\t\t\tfreed += drop_slab_node(nid);",
            "\t\t}",
            "\t} while ((freed >> shift++) > 1);",
            "}",
            "static int reclaimer_offset(void)",
            "{",
            "\tBUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=",
            "\t\t\tPGDEMOTE_DIRECT - PGDEMOTE_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=",
            "\t\t\tPGDEMOTE_KHUGEPAGED - PGDEMOTE_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=",
            "\t\t\tPGSCAN_DIRECT - PGSCAN_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=",
            "\t\t\tPGSCAN_KHUGEPAGED - PGSCAN_KSWAPD);",
            "",
            "\tif (current_is_kswapd())",
            "\t\treturn 0;",
            "\tif (current_is_khugepaged())",
            "\t\treturn PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD;",
            "\treturn PGSTEAL_DIRECT - PGSTEAL_KSWAPD;",
            "}",
            "static inline int is_page_cache_freeable(struct folio *folio)",
            "{",
            "\t/*",
            "\t * A freeable page cache folio is referenced only by the caller",
            "\t * that isolated the folio, the page cache and optional filesystem",
            "\t * private data at folio->private.",
            "\t */",
            "\treturn folio_ref_count(folio) - folio_test_private(folio) ==",
            "\t\t1 + folio_nr_pages(folio);",
            "}",
            "static void handle_write_error(struct address_space *mapping,",
            "\t\t\t\tstruct folio *folio, int error)",
            "{",
            "\tfolio_lock(folio);",
            "\tif (folio_mapping(folio) == mapping)",
            "\t\tmapping_set_error(mapping, error);",
            "\tfolio_unlock(folio);",
            "}"
          ],
          "function_name": "zone_reclaimable_pages, lruvec_lru_size, drop_slab_node, drop_slab, reclaimer_offset, is_page_cache_freeable, handle_write_error",
          "description": "实现区域可回收页数计算、Slab对象释放及回收偏移量调整逻辑，通过遍历各内存区域统计潜在可回收页数，提供Slab内存碎片回收机制并维护回收进程优先级偏移量。",
          "similarity": 0.7329025268554688
        },
        {
          "chunk_id": 44,
          "file_path": "mm/vmscan.c",
          "start_line": 7321,
          "end_line": 7463,
          "content": [
            "static int __init kswapd_init(void)",
            "{",
            "\tint nid;",
            "",
            "\tswap_setup();",
            "\tfor_each_node_state(nid, N_MEMORY)",
            " \t\tkswapd_run(nid);",
            "\treturn 0;",
            "}",
            "static inline unsigned long node_unmapped_file_pages(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long file_mapped = node_page_state(pgdat, NR_FILE_MAPPED);",
            "\tunsigned long file_lru = node_page_state(pgdat, NR_INACTIVE_FILE) +",
            "\t\tnode_page_state(pgdat, NR_ACTIVE_FILE);",
            "",
            "\t/*",
            "\t * It's possible for there to be more file mapped pages than",
            "\t * accounted for by the pages on the file LRU lists because",
            "\t * tmpfs pages accounted for as ANON can also be FILE_MAPPED",
            "\t */",
            "\treturn (file_lru > file_mapped) ? (file_lru - file_mapped) : 0;",
            "}",
            "static unsigned long node_pagecache_reclaimable(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long nr_pagecache_reclaimable;",
            "\tunsigned long delta = 0;",
            "",
            "\t/*",
            "\t * If RECLAIM_UNMAP is set, then all file pages are considered",
            "\t * potentially reclaimable. Otherwise, we have to worry about",
            "\t * pages like swapcache and node_unmapped_file_pages() provides",
            "\t * a better estimate",
            "\t */",
            "\tif (node_reclaim_mode & RECLAIM_UNMAP)",
            "\t\tnr_pagecache_reclaimable = node_page_state(pgdat, NR_FILE_PAGES);",
            "\telse",
            "\t\tnr_pagecache_reclaimable = node_unmapped_file_pages(pgdat);",
            "",
            "\t/* If we can't clean pages, remove dirty pages from consideration */",
            "\tif (!(node_reclaim_mode & RECLAIM_WRITE))",
            "\t\tdelta += node_page_state(pgdat, NR_FILE_DIRTY);",
            "",
            "\t/* Watch for any possible underflows due to delta */",
            "\tif (unlikely(delta > nr_pagecache_reclaimable))",
            "\t\tdelta = nr_pagecache_reclaimable;",
            "",
            "\treturn nr_pagecache_reclaimable - delta;",
            "}",
            "static int __node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)",
            "{",
            "\t/* Minimum pages needed in order to stay on node */",
            "\tconst unsigned long nr_pages = 1 << order;",
            "\tstruct task_struct *p = current;",
            "\tunsigned int noreclaim_flag;",
            "\tstruct scan_control sc = {",
            "\t\t.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),",
            "\t\t.gfp_mask = current_gfp_context(gfp_mask),",
            "\t\t.order = order,",
            "\t\t.priority = NODE_RECLAIM_PRIORITY,",
            "\t\t.may_writepage = !!(node_reclaim_mode & RECLAIM_WRITE),",
            "\t\t.may_unmap = !!(node_reclaim_mode & RECLAIM_UNMAP),",
            "\t\t.may_swap = 1,",
            "\t\t.reclaim_idx = gfp_zone(gfp_mask),",
            "\t};",
            "\tunsigned long pflags;",
            "",
            "\ttrace_mm_vmscan_node_reclaim_begin(pgdat->node_id, order,",
            "\t\t\t\t\t   sc.gfp_mask);",
            "",
            "\tcond_resched();",
            "\tpsi_memstall_enter(&pflags);",
            "\tfs_reclaim_acquire(sc.gfp_mask);",
            "\t/*",
            "\t * We need to be able to allocate from the reserves for RECLAIM_UNMAP",
            "\t */",
            "\tnoreclaim_flag = memalloc_noreclaim_save();",
            "\tset_task_reclaim_state(p, &sc.reclaim_state);",
            "",
            "\tif (node_pagecache_reclaimable(pgdat) > pgdat->min_unmapped_pages ||",
            "\t    node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) > pgdat->min_slab_pages) {",
            "\t\t/*",
            "\t\t * Free memory by calling shrink node with increasing",
            "\t\t * priorities until we have enough memory freed.",
            "\t\t */",
            "\t\tdo {",
            "\t\t\tshrink_node(pgdat, &sc);",
            "\t\t} while (sc.nr_reclaimed < nr_pages && --sc.priority >= 0);",
            "\t}",
            "",
            "\tset_task_reclaim_state(p, NULL);",
            "\tmemalloc_noreclaim_restore(noreclaim_flag);",
            "\tfs_reclaim_release(sc.gfp_mask);",
            "\tpsi_memstall_leave(&pflags);",
            "",
            "\ttrace_mm_vmscan_node_reclaim_end(sc.nr_reclaimed);",
            "",
            "\treturn sc.nr_reclaimed >= nr_pages;",
            "}",
            "int node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)",
            "{",
            "\tint ret;",
            "",
            "\t/*",
            "\t * Node reclaim reclaims unmapped file backed pages and",
            "\t * slab pages if we are over the defined limits.",
            "\t *",
            "\t * A small portion of unmapped file backed pages is needed for",
            "\t * file I/O otherwise pages read by file I/O will be immediately",
            "\t * thrown out if the node is overallocated. So we do not reclaim",
            "\t * if less than a specified percentage of the node is used by",
            "\t * unmapped file backed pages.",
            "\t */",
            "\tif (node_pagecache_reclaimable(pgdat) <= pgdat->min_unmapped_pages &&",
            "\t    node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) <=",
            "\t    pgdat->min_slab_pages)",
            "\t\treturn NODE_RECLAIM_FULL;",
            "",
            "\t/*",
            "\t * Do not scan if the allocation should not be delayed.",
            "\t */",
            "\tif (!gfpflags_allow_blocking(gfp_mask) || (current->flags & PF_MEMALLOC))",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\t/*",
            "\t * Only run node reclaim on the local node or on nodes that do not",
            "\t * have associated processors. This will favor the local processor",
            "\t * over remote processors and spread off node memory allocations",
            "\t * as wide as possible.",
            "\t */",
            "\tif (node_state(pgdat->node_id, N_CPU) && pgdat->node_id != numa_node_id())",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\tif (test_and_set_bit(PGDAT_RECLAIM_LOCKED, &pgdat->flags))",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\tret = __node_reclaim(pgdat, gfp_mask, order);",
            "\tclear_bit_unlock(PGDAT_RECLAIM_LOCKED, &pgdat->flags);",
            "",
            "\tif (!ret)",
            "\t\tcount_vm_event(PGSCAN_ZONE_RECLAIM_FAILED);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "kswapd_init, node_unmapped_file_pages, node_pagecache_reclaimable, __node_reclaim, node_reclaim",
          "description": "实现节点级别内存回收策略，kswapd_init初始化kswapd线程，node_unmapped_file_pages计算未映射文件页，node_pagecache_reclaimable评估可回收页面数量，__node_reclaim执行节点级回收，node_reclaim决定是否触发回收操作。",
          "similarity": 0.7231122255325317
        },
        {
          "chunk_id": 3,
          "file_path": "mm/vmscan.c",
          "start_line": 469,
          "end_line": 588,
          "content": [
            "static bool skip_throttle_noprogress(pg_data_t *pgdat)",
            "{",
            "\tint reclaimable = 0, write_pending = 0;",
            "\tint i;",
            "",
            "\t/*",
            "\t * If kswapd is disabled, reschedule if necessary but do not",
            "\t * throttle as the system is likely near OOM.",
            "\t */",
            "\tif (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * If there are a lot of dirty/writeback folios then do not",
            "\t * throttle as throttling will occur when the folios cycle",
            "\t * towards the end of the LRU if still under writeback.",
            "\t */",
            "\tfor (i = 0; i < MAX_NR_ZONES; i++) {",
            "\t\tstruct zone *zone = pgdat->node_zones + i;",
            "",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\treclaimable += zone_reclaimable_pages(zone);",
            "\t\twrite_pending += zone_page_state_snapshot(zone,",
            "\t\t\t\t\t\t  NR_ZONE_WRITE_PENDING);",
            "\t}",
            "\tif (2 * write_pending <= reclaimable)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "void reclaim_throttle(pg_data_t *pgdat, enum vmscan_throttle_state reason)",
            "{",
            "\twait_queue_head_t *wqh = &pgdat->reclaim_wait[reason];",
            "\tlong timeout, ret;",
            "\tDEFINE_WAIT(wait);",
            "",
            "\t/*",
            "\t * Do not throttle user workers, kthreads other than kswapd or",
            "\t * workqueues. They may be required for reclaim to make",
            "\t * forward progress (e.g. journalling workqueues or kthreads).",
            "\t */",
            "\tif (!current_is_kswapd() &&",
            "\t    current->flags & (PF_USER_WORKER|PF_KTHREAD)) {",
            "\t\tcond_resched();",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * These figures are pulled out of thin air.",
            "\t * VMSCAN_THROTTLE_ISOLATED is a transient condition based on too many",
            "\t * parallel reclaimers which is a short-lived event so the timeout is",
            "\t * short. Failing to make progress or waiting on writeback are",
            "\t * potentially long-lived events so use a longer timeout. This is shaky",
            "\t * logic as a failure to make progress could be due to anything from",
            "\t * writeback to a slow device to excessive referenced folios at the tail",
            "\t * of the inactive LRU.",
            "\t */",
            "\tswitch(reason) {",
            "\tcase VMSCAN_THROTTLE_WRITEBACK:",
            "\t\ttimeout = HZ/10;",
            "",
            "\t\tif (atomic_inc_return(&pgdat->nr_writeback_throttled) == 1) {",
            "\t\t\tWRITE_ONCE(pgdat->nr_reclaim_start,",
            "\t\t\t\tnode_page_state(pgdat, NR_THROTTLED_WRITTEN));",
            "\t\t}",
            "",
            "\t\tbreak;",
            "\tcase VMSCAN_THROTTLE_CONGESTED:",
            "\t\tfallthrough;",
            "\tcase VMSCAN_THROTTLE_NOPROGRESS:",
            "\t\tif (skip_throttle_noprogress(pgdat)) {",
            "\t\t\tcond_resched();",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\ttimeout = 1;",
            "",
            "\t\tbreak;",
            "\tcase VMSCAN_THROTTLE_ISOLATED:",
            "\t\ttimeout = HZ/50;",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(1);",
            "\t\ttimeout = HZ;",
            "\t\tbreak;",
            "\t}",
            "",
            "\tprepare_to_wait(wqh, &wait, TASK_UNINTERRUPTIBLE);",
            "\tret = schedule_timeout(timeout);",
            "\tfinish_wait(wqh, &wait);",
            "",
            "\tif (reason == VMSCAN_THROTTLE_WRITEBACK)",
            "\t\tatomic_dec(&pgdat->nr_writeback_throttled);",
            "",
            "\ttrace_mm_vmscan_throttled(pgdat->node_id, jiffies_to_usecs(timeout),",
            "\t\t\t\tjiffies_to_usecs(timeout - ret),",
            "\t\t\t\treason);",
            "}",
            "void __acct_reclaim_writeback(pg_data_t *pgdat, struct folio *folio,",
            "\t\t\t\t\t\t\tint nr_throttled)",
            "{",
            "\tunsigned long nr_written;",
            "",
            "\tnode_stat_add_folio(folio, NR_THROTTLED_WRITTEN);",
            "",
            "\t/*",
            "\t * This is an inaccurate read as the per-cpu deltas may not",
            "\t * be synchronised. However, given that the system is",
            "\t * writeback throttled, it is not worth taking the penalty",
            "\t * of getting an accurate count. At worst, the throttle",
            "\t * timeout guarantees forward progress.",
            "\t */",
            "\tnr_written = node_page_state(pgdat, NR_THROTTLED_WRITTEN) -",
            "\t\tREAD_ONCE(pgdat->nr_reclaim_start);",
            "",
            "\tif (nr_written > SWAP_CLUSTER_MAX * nr_throttled)",
            "\t\twake_up(&pgdat->reclaim_wait[VMSCAN_THROTTLE_WRITEBACK]);",
            "}"
          ],
          "function_name": "skip_throttle_noprogress, reclaim_throttle, __acct_reclaim_writeback",
          "description": "包含回收节流控制逻辑，通过检测写回Pending页数与可回收页数比例决定是否触发节流，管理回收进程休眠唤醒机制，并统计写回数据量以优化回收策略执行时机。",
          "similarity": 0.710774838924408
        },
        {
          "chunk_id": 39,
          "file_path": "mm/vmscan.c",
          "start_line": 6439,
          "end_line": 6556,
          "content": [
            "unsigned long try_to_free_pages(struct zonelist *zonelist, int order,",
            "\t\t\t\tgfp_t gfp_mask, nodemask_t *nodemask)",
            "{",
            "\tunsigned long nr_reclaimed;",
            "\tstruct scan_control sc = {",
            "\t\t.nr_to_reclaim = SWAP_CLUSTER_MAX,",
            "\t\t.gfp_mask = current_gfp_context(gfp_mask),",
            "\t\t.reclaim_idx = gfp_zone(gfp_mask),",
            "\t\t.order = order,",
            "\t\t.nodemask = nodemask,",
            "\t\t.priority = DEF_PRIORITY,",
            "\t\t.may_writepage = !laptop_mode,",
            "\t\t.may_unmap = 1,",
            "\t\t.may_swap = 1,",
            "\t};",
            "",
            "\t/*",
            "\t * scan_control uses s8 fields for order, priority, and reclaim_idx.",
            "\t * Confirm they are large enough for max values.",
            "\t */",
            "\tBUILD_BUG_ON(MAX_PAGE_ORDER >= S8_MAX);",
            "\tBUILD_BUG_ON(DEF_PRIORITY > S8_MAX);",
            "\tBUILD_BUG_ON(MAX_NR_ZONES > S8_MAX);",
            "",
            "\t/*",
            "\t * Do not enter reclaim if fatal signal was delivered while throttled.",
            "\t * 1 is returned so that the page allocator does not OOM kill at this",
            "\t * point.",
            "\t */",
            "\tif (throttle_direct_reclaim(sc.gfp_mask, zonelist, nodemask))",
            "\t\treturn 1;",
            "",
            "\tset_task_reclaim_state(current, &sc.reclaim_state);",
            "\ttrace_mm_vmscan_direct_reclaim_begin(order, sc.gfp_mask);",
            "",
            "\tnr_reclaimed = do_try_to_free_pages(zonelist, &sc);",
            "",
            "\ttrace_mm_vmscan_direct_reclaim_end(nr_reclaimed);",
            "\tset_task_reclaim_state(current, NULL);",
            "",
            "\treturn nr_reclaimed;",
            "}",
            "unsigned long mem_cgroup_shrink_node(struct mem_cgroup *memcg,",
            "\t\t\t\t\t\tgfp_t gfp_mask, bool noswap,",
            "\t\t\t\t\t\tpg_data_t *pgdat,",
            "\t\t\t\t\t\tunsigned long *nr_scanned)",
            "{",
            "\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "\tstruct scan_control sc = {",
            "\t\t.nr_to_reclaim = SWAP_CLUSTER_MAX,",
            "\t\t.target_mem_cgroup = memcg,",
            "\t\t.may_writepage = !laptop_mode,",
            "\t\t.may_unmap = 1,",
            "\t\t.reclaim_idx = MAX_NR_ZONES - 1,",
            "\t\t.may_swap = !noswap,",
            "\t};",
            "",
            "\tWARN_ON_ONCE(!current->reclaim_state);",
            "",
            "\tsc.gfp_mask = (gfp_mask & GFP_RECLAIM_MASK) |",
            "\t\t\t(GFP_HIGHUSER_MOVABLE & ~GFP_RECLAIM_MASK);",
            "",
            "\ttrace_mm_vmscan_memcg_softlimit_reclaim_begin(sc.order,",
            "\t\t\t\t\t\t      sc.gfp_mask);",
            "",
            "\t/*",
            "\t * NOTE: Although we can get the priority field, using it",
            "\t * here is not a good idea, since it limits the pages we can scan.",
            "\t * if we don't reclaim here, the shrink_node from balance_pgdat",
            "\t * will pick up pages from other mem cgroup's as well. We hack",
            "\t * the priority and make it zero.",
            "\t */",
            "\tshrink_lruvec(lruvec, &sc);",
            "",
            "\ttrace_mm_vmscan_memcg_softlimit_reclaim_end(sc.nr_reclaimed);",
            "",
            "\t*nr_scanned = sc.nr_scanned;",
            "",
            "\treturn sc.nr_reclaimed;",
            "}",
            "unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,",
            "\t\t\t\t\t   unsigned long nr_pages,",
            "\t\t\t\t\t   gfp_t gfp_mask,",
            "\t\t\t\t\t   unsigned int reclaim_options)",
            "{",
            "\tunsigned long nr_reclaimed;",
            "\tunsigned int noreclaim_flag;",
            "\tstruct scan_control sc = {",
            "\t\t.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),",
            "\t\t.gfp_mask = (current_gfp_context(gfp_mask) & GFP_RECLAIM_MASK) |",
            "\t\t\t\t(GFP_HIGHUSER_MOVABLE & ~GFP_RECLAIM_MASK),",
            "\t\t.reclaim_idx = MAX_NR_ZONES - 1,",
            "\t\t.target_mem_cgroup = memcg,",
            "\t\t.priority = DEF_PRIORITY,",
            "\t\t.may_writepage = !laptop_mode,",
            "\t\t.may_unmap = 1,",
            "\t\t.may_swap = !!(reclaim_options & MEMCG_RECLAIM_MAY_SWAP),",
            "\t\t.proactive = !!(reclaim_options & MEMCG_RECLAIM_PROACTIVE),",
            "\t};",
            "\t/*",
            "\t * Traverse the ZONELIST_FALLBACK zonelist of the current node to put",
            "\t * equal pressure on all the nodes. This is based on the assumption that",
            "\t * the reclaim does not bail out early.",
            "\t */",
            "\tstruct zonelist *zonelist = node_zonelist(numa_node_id(), sc.gfp_mask);",
            "",
            "\tset_task_reclaim_state(current, &sc.reclaim_state);",
            "\ttrace_mm_vmscan_memcg_reclaim_begin(0, sc.gfp_mask);",
            "\tnoreclaim_flag = memalloc_noreclaim_save();",
            "",
            "\tnr_reclaimed = do_try_to_free_pages(zonelist, &sc);",
            "",
            "\tmemalloc_noreclaim_restore(noreclaim_flag);",
            "\ttrace_mm_vmscan_memcg_reclaim_end(nr_reclaimed);",
            "\tset_task_reclaim_state(current, NULL);",
            "",
            "\treturn nr_reclaimed;",
            "}"
          ],
          "function_name": "try_to_free_pages, mem_cgroup_shrink_node, try_to_free_mem_cgroup_pages",
          "description": "try_to_free_pages启动全局内存回收流程，mem_cgroup_shrink_node专用于特定内存组的页面回收，try_to_free_mem_cgroup_pages处理带参数的内存组回收，均通过do_try_to_free_pages实现核心回收逻辑。",
          "similarity": 0.6970579028129578
        },
        {
          "chunk_id": 1,
          "file_path": "mm/vmscan.c",
          "start_line": 195,
          "end_line": 305,
          "content": [
            "static bool cgroup_reclaim(struct scan_control *sc)",
            "{",
            "\treturn sc->target_mem_cgroup;",
            "}",
            "static bool root_reclaim(struct scan_control *sc)",
            "{",
            "\treturn !sc->target_mem_cgroup || mem_cgroup_is_root(sc->target_mem_cgroup);",
            "}",
            "static bool writeback_throttling_sane(struct scan_control *sc)",
            "{",
            "\tif (!cgroup_reclaim(sc))",
            "\t\treturn true;",
            "#ifdef CONFIG_CGROUP_WRITEBACK",
            "\tif (cgroup_subsys_on_dfl(memory_cgrp_subsys))",
            "\t\treturn true;",
            "#endif",
            "\treturn false;",
            "}",
            "static bool cgroup_reclaim(struct scan_control *sc)",
            "{",
            "\treturn false;",
            "}",
            "static bool root_reclaim(struct scan_control *sc)",
            "{",
            "\treturn true;",
            "}",
            "static bool writeback_throttling_sane(struct scan_control *sc)",
            "{",
            "\treturn true;",
            "}",
            "static void set_task_reclaim_state(struct task_struct *task,",
            "\t\t\t\t   struct reclaim_state *rs)",
            "{",
            "\t/* Check for an overwrite */",
            "\tWARN_ON_ONCE(rs && task->reclaim_state);",
            "",
            "\t/* Check for the nulling of an already-nulled member */",
            "\tWARN_ON_ONCE(!rs && !task->reclaim_state);",
            "",
            "\ttask->reclaim_state = rs;",
            "}",
            "static void flush_reclaim_state(struct scan_control *sc)",
            "{",
            "\t/*",
            "\t * Currently, reclaim_state->reclaimed includes three types of pages",
            "\t * freed outside of vmscan:",
            "\t * (1) Slab pages.",
            "\t * (2) Clean file pages from pruned inodes (on highmem systems).",
            "\t * (3) XFS freed buffer pages.",
            "\t *",
            "\t * For all of these cases, we cannot universally link the pages to a",
            "\t * single memcg. For example, a memcg-aware shrinker can free one object",
            "\t * charged to the target memcg, causing an entire page to be freed.",
            "\t * If we count the entire page as reclaimed from the memcg, we end up",
            "\t * overestimating the reclaimed amount (potentially under-reclaiming).",
            "\t *",
            "\t * Only count such pages for global reclaim to prevent under-reclaiming",
            "\t * from the target memcg; preventing unnecessary retries during memcg",
            "\t * charging and false positives from proactive reclaim.",
            "\t *",
            "\t * For uncommon cases where the freed pages were actually mostly",
            "\t * charged to the target memcg, we end up underestimating the reclaimed",
            "\t * amount. This should be fine. The freed pages will be uncharged",
            "\t * anyway, even if they are not counted here properly, and we will be",
            "\t * able to make forward progress in charging (which is usually in a",
            "\t * retry loop).",
            "\t *",
            "\t * We can go one step further, and report the uncharged objcg pages in",
            "\t * memcg reclaim, to make reporting more accurate and reduce",
            "\t * underestimation, but it's probably not worth the complexity for now.",
            "\t */",
            "\tif (current->reclaim_state && root_reclaim(sc)) {",
            "\t\tsc->nr_reclaimed += current->reclaim_state->reclaimed;",
            "\t\tcurrent->reclaim_state->reclaimed = 0;",
            "\t}",
            "}",
            "static bool can_demote(int nid, struct scan_control *sc)",
            "{",
            "\tif (!numa_demotion_enabled)",
            "\t\treturn false;",
            "\tif (sc && sc->no_demotion)",
            "\t\treturn false;",
            "\tif (next_demotion_node(nid) == NUMA_NO_NODE)",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static inline bool can_reclaim_anon_pages(struct mem_cgroup *memcg,",
            "\t\t\t\t\t  int nid,",
            "\t\t\t\t\t  struct scan_control *sc)",
            "{",
            "\tif (memcg == NULL) {",
            "\t\t/*",
            "\t\t * For non-memcg reclaim, is there",
            "\t\t * space in any swap device?",
            "\t\t */",
            "\t\tif (get_nr_swap_pages() > 0)",
            "\t\t\treturn true;",
            "\t} else {",
            "\t\t/* Is the memcg below its swap limit? */",
            "\t\tif (mem_cgroup_get_nr_swap_pages(memcg) > 0)",
            "\t\t\treturn true;",
            "\t}",
            "",
            "\t/*",
            "\t * The page can not be swapped.",
            "\t *",
            "\t * Can it be reclaimed from this node via demotion?",
            "\t */",
            "\treturn can_demote(nid, sc);",
            "}"
          ],
          "function_name": "cgroup_reclaim, root_reclaim, writeback_throttling_sane, cgroup_reclaim, root_reclaim, writeback_throttling_sane, set_task_reclaim_state, flush_reclaim_state, can_demote, can_reclaim_anon_pages",
          "description": "提供内存组回收判定逻辑与回收状态管理接口，包含判断是否针对特定内存组回收、是否允许写回操作、设置/刷新任务回收状态等功能，支持多层级回收场景决策。",
          "similarity": 0.6810680627822876
        }
      ]
    },
    {
      "source_file": "mm/page-writeback.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:09\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page-writeback.c`\n\n---\n\n# page-writeback.c 技术文档\n\n## 1. 文件概述\n\n`page-writeback.c` 是 Linux 内核内存管理子系统（MM）中的核心文件，负责实现**脏页回写（dirty page writeback）机制**。该机制用于控制和协调将修改过的页面（即“脏页”）从内存写回到持久化存储（如磁盘）的过程，以确保数据一致性、防止内存耗尽，并在系统负载与 I/O 带宽之间取得平衡。\n\n该文件主要提供以下功能：\n- 脏页数量的全局与每 BDI（Backing Device Info）级别的阈值管理\n- 脏页生成速率的动态限流（throttling）\n- 后台回写线程（如 `writeback` 线程）的触发逻辑\n- 支持基于 cgroup 的内存回写控制（当启用 `CONFIG_CGROUP_WRITEBACK` 时）\n- 与 `/proc/sys/vm` 中可调参数的交互接口\n\n## 2. 核心功能\n\n### 主要全局变量（可通过 sysctl 调整）\n| 变量名 | 默认值 | 说明 |\n|--------|--------|------|\n| `dirty_background_ratio` | 10 | 当脏页占可用内存比例达到此值时，启动后台回写 |\n| `vm_dirty_ratio` | 20 | 脏页比例硬上限，超过则阻塞写进程进行同步回写 |\n| `dirty_background_bytes` | 0 | 以字节为单位指定后台回写阈值（优先级高于 ratio） |\n| `vm_dirty_bytes` | 0 | 以字节为单位指定脏页硬上限（优先级高于 ratio） |\n| `dirty_writeback_interval` | 500 (5秒) | 后台回写线程的唤醒间隔（单位：厘秒） |\n| `dirty_expire_interval` | 3000 (30秒) | 脏页最大存活时间，超时强制回写 |\n| `laptop_mode` | 0 | 笔记本模式开关，减少磁盘活动以省电 |\n| `ratelimit_pages` | 32 | 每 CPU 脏页速率限制阈值 |\n\n### 关键数据结构\n- **`struct wb_domain`**  \n  回写域（writeback domain），用于聚合多个 BDI 的回写状态，支持全局或 per-memcg 的回写控制。\n  \n- **`struct dirty_throttle_control` (dtc)**  \n  脏页限流控制上下文，包含：\n  - `avail`：当前可脏化的内存总量\n  - `dirty`：当前脏页数量\n  - `thresh` / `bg_thresh`：硬/软回写阈值\n  - `wb_dirty` / `wb_thresh` / `wb_bg_thresh`：per-BDI 级别的对应值\n  - `pos_ratio`：用于计算回写速率的比例因子\n\n- **条件编译支持**  \n  通过 `CONFIG_CGROUP_WRITEBACK` 区分是否支持 memcg 级别的回写控制，提供 `GDTC_INIT`、`MDTC_INIT` 等宏及辅助函数（如 `mdtc_valid()`、`wb_min_max_ratio()`）。\n\n### 核心辅助函数（部分在截断代码中未完整显示）\n- `node_dirtyable_memory()`：计算指定 NUMA 节点中可用于脏页缓存的内存总量（包括空闲页 + 文件缓存页 - 保留页）。\n- `balance_dirty_pages()`：主限流函数，在进程写入时被调用，根据当前脏页水位决定是否休眠或触发回写。\n- `balance_dirty_pages_ratelimited()`：带速率限制的脏页平衡入口，避免频繁调用开销。\n\n## 3. 关键实现\n\n### 脏页阈值计算逻辑\n- 脏页上限基于 **“dirtyable memory”** 计算，即 `(free pages + file cache pages - kernel reserves)`。\n- 支持两种配置方式：**百分比（ratio）** 或 **绝对字节数（bytes）**，后者优先。\n- 当启用 `vm_highmem_is_dirtyable` 时，highmem 区域的空闲页也计入 dirtyable memory。\n\n### 动态限流机制\n- 使用 **`MAX_PAUSE`（最大 200ms）** 限制单次 `balance_dirty_pages()` 的休眠时间。\n- 引入 **`DIRTY_POLL_THRESH`（128KB）** 作为调用间隔优化阈值：若脏页增长过快，则提升休眠时间至最大值。\n- 通过 **`BANDWIDTH_INTERVAL`（200ms）** 动态估算存储设备的写入带宽，用于调整回写速率。\n\n### cgroup writeback 支持\n- 在 `CONFIG_CGROUP_WRITEBACK` 启用时：\n  - 每个 memcg 有独立的 `wb_domain`\n  - `dirty_throttle_control` 可关联全局（gdtc）或 memcg（mdtc）上下文\n  - BDI 的 min/max_ratio 根据其实际带宽动态缩放，实现公平分配\n\n### 老化与完成计数\n- 使用 `fprop_local_percpu` 结构跟踪每个 BDI 的回写完成情况。\n- `VM_COMPLETIONS_PERIOD_LEN`（3 秒）定义了回写完成率的老化周期，影响带宽估算的响应速度。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/swap.h>`、`<linux/pagevec.h>` 等，与页分配、回收机制紧密集成。\n- **VFS 层**：通过 `<linux/fs.h>`、`<linux/pagemap.h>` 与 address_space 和 inode 交互。\n- **块设备层**：通过 `<linux/blkdev.h>`、`<linux/backing-dev.h>` 获取 BDI 信息和 I/O 能力。\n- **调度与同步**：使用 `<linux/sched.h>`、`<linux/spinlock.h>`、`<linux/timer.h>` 实现休眠、锁和定时器。\n- **追踪系统**：集成 `<trace/events/writeback.h>` 提供回写事件追踪点。\n- **内部头文件**：包含 `\"internal.h\"` 获取 MM 子系统内部接口。\n\n## 5. 使用场景\n\n1. **用户空间写入文件**  \n   当进程通过 `write()` 修改文件页时，页被标记为脏，随后调用 `balance_dirty_pages_ratelimited()` 触发脏页控制。\n\n2. **内存压力下的页面回收**  \n   kswapd 或直接回收路径在需要释放内存时，可能调用回写逻辑清理脏页。\n\n3. **定期后台回写**  \n   `writeback` 内核线程按 `dirty_writeback_interval` 周期唤醒，检查并回写超过 `dirty_expire_interval` 的脏页。\n\n4. **系统关闭或 sync 调用**  \n   虽然主要同步逻辑在其他文件，但本文件提供的阈值和状态是决策基础。\n\n5. **容器环境中的资源隔离**  \n   启用 cgroup writeback 后，不同 memcg 的脏页回写相互隔离，避免一个容器的大量写入影响其他容器性能。\n\n6. **笔记本省电模式**  \n   当 `laptop_mode` 启用时，延迟回写以减少磁盘旋转时间，延长电池寿命。",
      "similarity": 0.6436418294906616,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/page-writeback.c",
          "start_line": 164,
          "end_line": 268,
          "content": [
            "static bool mdtc_valid(struct dirty_throttle_control *dtc)",
            "{",
            "\treturn dtc->dom;",
            "}",
            "static void wb_min_max_ratio(struct bdi_writeback *wb,",
            "\t\t\t     unsigned long *minp, unsigned long *maxp)",
            "{",
            "\tunsigned long this_bw = READ_ONCE(wb->avg_write_bandwidth);",
            "\tunsigned long tot_bw = atomic_long_read(&wb->bdi->tot_write_bandwidth);",
            "\tunsigned long long min = wb->bdi->min_ratio;",
            "\tunsigned long long max = wb->bdi->max_ratio;",
            "",
            "\t/*",
            "\t * @wb may already be clean by the time control reaches here and",
            "\t * the total may not include its bw.",
            "\t */",
            "\tif (this_bw < tot_bw) {",
            "\t\tif (min) {",
            "\t\t\tmin *= this_bw;",
            "\t\t\tmin = div64_ul(min, tot_bw);",
            "\t\t}",
            "\t\tif (max < 100 * BDI_RATIO_SCALE) {",
            "\t\t\tmax *= this_bw;",
            "\t\t\tmax = div64_ul(max, tot_bw);",
            "\t\t}",
            "\t}",
            "",
            "\t*minp = min;",
            "\t*maxp = max;",
            "}",
            "static bool mdtc_valid(struct dirty_throttle_control *dtc)",
            "{",
            "\treturn false;",
            "}",
            "static void wb_min_max_ratio(struct bdi_writeback *wb,",
            "\t\t\t     unsigned long *minp, unsigned long *maxp)",
            "{",
            "\t*minp = wb->bdi->min_ratio;",
            "\t*maxp = wb->bdi->max_ratio;",
            "}",
            "static unsigned long node_dirtyable_memory(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long nr_pages = 0;",
            "\tint z;",
            "",
            "\tfor (z = 0; z < MAX_NR_ZONES; z++) {",
            "\t\tstruct zone *zone = pgdat->node_zones + z;",
            "",
            "\t\tif (!populated_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tnr_pages += zone_page_state(zone, NR_FREE_PAGES);",
            "\t}",
            "",
            "\t/*",
            "\t * Pages reserved for the kernel should not be considered",
            "\t * dirtyable, to prevent a situation where reclaim has to",
            "\t * clean pages in order to balance the zones.",
            "\t */",
            "\tnr_pages -= min(nr_pages, pgdat->totalreserve_pages);",
            "",
            "\tnr_pages += node_page_state(pgdat, NR_INACTIVE_FILE);",
            "\tnr_pages += node_page_state(pgdat, NR_ACTIVE_FILE);",
            "",
            "\treturn nr_pages;",
            "}",
            "static unsigned long highmem_dirtyable_memory(unsigned long total)",
            "{",
            "#ifdef CONFIG_HIGHMEM",
            "\tint node;",
            "\tunsigned long x = 0;",
            "\tint i;",
            "",
            "\tfor_each_node_state(node, N_HIGH_MEMORY) {",
            "\t\tfor (i = ZONE_NORMAL + 1; i < MAX_NR_ZONES; i++) {",
            "\t\t\tstruct zone *z;",
            "\t\t\tunsigned long nr_pages;",
            "",
            "\t\t\tif (!is_highmem_idx(i))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tz = &NODE_DATA(node)->node_zones[i];",
            "\t\t\tif (!populated_zone(z))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tnr_pages = zone_page_state(z, NR_FREE_PAGES);",
            "\t\t\t/* watch for underflows */",
            "\t\t\tnr_pages -= min(nr_pages, high_wmark_pages(z));",
            "\t\t\tnr_pages += zone_page_state(z, NR_ZONE_INACTIVE_FILE);",
            "\t\t\tnr_pages += zone_page_state(z, NR_ZONE_ACTIVE_FILE);",
            "\t\t\tx += nr_pages;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Make sure that the number of highmem pages is never larger",
            "\t * than the number of the total dirtyable memory. This can only",
            "\t * occur in very strange VM situations but we want to make sure",
            "\t * that this does not occur.",
            "\t */",
            "\treturn min(x, total);",
            "#else",
            "\treturn 0;",
            "#endif",
            "}"
          ],
          "function_name": "mdtc_valid, wb_min_max_ratio, mdtc_valid, wb_min_max_ratio, node_dirtyable_memory, highmem_dirtyable_memory",
          "description": "计算各节点可脏化内存大小，并基于当前写入带宽调整脏页限制的最小和最大比率，用于后续脏页阈值计算。",
          "similarity": 0.6657178997993469
        },
        {
          "chunk_id": 3,
          "file_path": "mm/page-writeback.c",
          "start_line": 495,
          "end_line": 600,
          "content": [
            "bool node_dirty_ok(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long limit = node_dirty_limit(pgdat);",
            "\tunsigned long nr_pages = 0;",
            "",
            "\tnr_pages += node_page_state(pgdat, NR_FILE_DIRTY);",
            "\tnr_pages += node_page_state(pgdat, NR_WRITEBACK);",
            "",
            "\treturn nr_pages <= limit;",
            "}",
            "static int dirty_background_ratio_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint ret;",
            "",
            "\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write)",
            "\t\tdirty_background_bytes = 0;",
            "\treturn ret;",
            "}",
            "static int dirty_background_bytes_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint ret;",
            "\tunsigned long old_bytes = dirty_background_bytes;",
            "",
            "\tret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write) {",
            "\t\tif (DIV_ROUND_UP(dirty_background_bytes, PAGE_SIZE) >",
            "\t\t\t\t\t\t\t\tUINT_MAX) {",
            "\t\t\tdirty_background_bytes = old_bytes;",
            "\t\t\treturn -ERANGE;",
            "\t\t}",
            "\t\tdirty_background_ratio = 0;",
            "\t}",
            "\treturn ret;",
            "}",
            "static int dirty_ratio_handler(struct ctl_table *table, int write, void *buffer,",
            "\t\tsize_t *lenp, loff_t *ppos)",
            "{",
            "\tint old_ratio = vm_dirty_ratio;",
            "\tint ret;",
            "",
            "\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write && vm_dirty_ratio != old_ratio) {",
            "\t\tvm_dirty_bytes = 0;",
            "\t\twriteback_set_ratelimit();",
            "\t}",
            "\treturn ret;",
            "}",
            "static int dirty_bytes_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tunsigned long old_bytes = vm_dirty_bytes;",
            "\tint ret;",
            "",
            "\tret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (ret == 0 && write && vm_dirty_bytes != old_bytes) {",
            "\t\tif (DIV_ROUND_UP(vm_dirty_bytes, PAGE_SIZE) > UINT_MAX) {",
            "\t\t\tvm_dirty_bytes = old_bytes;",
            "\t\t\treturn -ERANGE;",
            "\t\t}",
            "\t\twriteback_set_ratelimit();",
            "\t\tvm_dirty_ratio = 0;",
            "\t}",
            "\treturn ret;",
            "}",
            "static unsigned long wp_next_time(unsigned long cur_time)",
            "{",
            "\tcur_time += VM_COMPLETIONS_PERIOD_LEN;",
            "\t/* 0 has a special meaning... */",
            "\tif (!cur_time)",
            "\t\treturn 1;",
            "\treturn cur_time;",
            "}",
            "static void wb_domain_writeout_add(struct wb_domain *dom,",
            "\t\t\t\t   struct fprop_local_percpu *completions,",
            "\t\t\t\t   unsigned int max_prop_frac, long nr)",
            "{",
            "\t__fprop_add_percpu_max(&dom->completions, completions,",
            "\t\t\t       max_prop_frac, nr);",
            "\t/* First event after period switching was turned off? */",
            "\tif (unlikely(!dom->period_time)) {",
            "\t\t/*",
            "\t\t * We can race with other __bdi_writeout_inc calls here but",
            "\t\t * it does not cause any harm since the resulting time when",
            "\t\t * timer will fire and what is in writeout_period_time will be",
            "\t\t * roughly the same.",
            "\t\t */",
            "\t\tdom->period_time = wp_next_time(jiffies);",
            "\t\tmod_timer(&dom->period_timer, dom->period_time);",
            "\t}",
            "}",
            "static inline void __wb_writeout_add(struct bdi_writeback *wb, long nr)",
            "{",
            "\tstruct wb_domain *cgdom;",
            "",
            "\twb_stat_mod(wb, WB_WRITTEN, nr);",
            "\twb_domain_writeout_add(&global_wb_domain, &wb->completions,",
            "\t\t\t       wb->bdi->max_prop_frac, nr);",
            "",
            "\tcgdom = mem_cgroup_wb_domain(wb);",
            "\tif (cgdom)",
            "\t\twb_domain_writeout_add(cgdom, wb_memcg_completions(wb),",
            "\t\t\t\t       wb->bdi->max_prop_frac, nr);",
            "}"
          ],
          "function_name": "node_dirty_ok, dirty_background_ratio_handler, dirty_background_bytes_handler, dirty_ratio_handler, dirty_bytes_handler, wp_next_time, wb_domain_writeout_add, __wb_writeout_add",
          "description": "通过sysctl接口动态调整脏页写回参数，维护写回统计信息并周期性触发写回检查，确保系统内存使用符合预设策略。",
          "similarity": 0.6450239419937134
        },
        {
          "chunk_id": 9,
          "file_path": "mm/page-writeback.c",
          "start_line": 1662,
          "end_line": 1988,
          "content": [
            "static inline void wb_dirty_limits(struct dirty_throttle_control *dtc)",
            "{",
            "\tstruct bdi_writeback *wb = dtc->wb;",
            "\tunsigned long wb_reclaimable;",
            "",
            "\t/*",
            "\t * wb_thresh is not treated as some limiting factor as",
            "\t * dirty_thresh, due to reasons",
            "\t * - in JBOD setup, wb_thresh can fluctuate a lot",
            "\t * - in a system with HDD and USB key, the USB key may somehow",
            "\t *   go into state (wb_dirty >> wb_thresh) either because",
            "\t *   wb_dirty starts high, or because wb_thresh drops low.",
            "\t *   In this case we don't want to hard throttle the USB key",
            "\t *   dirtiers for 100 seconds until wb_dirty drops under",
            "\t *   wb_thresh. Instead the auxiliary wb control line in",
            "\t *   wb_position_ratio() will let the dirtier task progress",
            "\t *   at some rate <= (write_bw / 2) for bringing down wb_dirty.",
            "\t */",
            "\tdtc->wb_thresh = __wb_calc_thresh(dtc);",
            "\tdtc->wb_bg_thresh = dtc->thresh ?",
            "\t\tdiv_u64((u64)dtc->wb_thresh * dtc->bg_thresh, dtc->thresh) : 0;",
            "",
            "\t/*",
            "\t * In order to avoid the stacked BDI deadlock we need",
            "\t * to ensure we accurately count the 'dirty' pages when",
            "\t * the threshold is low.",
            "\t *",
            "\t * Otherwise it would be possible to get thresh+n pages",
            "\t * reported dirty, even though there are thresh-m pages",
            "\t * actually dirty; with m+n sitting in the percpu",
            "\t * deltas.",
            "\t */",
            "\tif (dtc->wb_thresh < 2 * wb_stat_error()) {",
            "\t\twb_reclaimable = wb_stat_sum(wb, WB_RECLAIMABLE);",
            "\t\tdtc->wb_dirty = wb_reclaimable + wb_stat_sum(wb, WB_WRITEBACK);",
            "\t} else {",
            "\t\twb_reclaimable = wb_stat(wb, WB_RECLAIMABLE);",
            "\t\tdtc->wb_dirty = wb_reclaimable + wb_stat(wb, WB_WRITEBACK);",
            "\t}",
            "}",
            "static int balance_dirty_pages(struct bdi_writeback *wb,",
            "\t\t\t       unsigned long pages_dirtied, unsigned int flags)",
            "{",
            "\tstruct dirty_throttle_control gdtc_stor = { GDTC_INIT(wb) };",
            "\tstruct dirty_throttle_control mdtc_stor = { MDTC_INIT(wb, &gdtc_stor) };",
            "\tstruct dirty_throttle_control * const gdtc = &gdtc_stor;",
            "\tstruct dirty_throttle_control * const mdtc = mdtc_valid(&mdtc_stor) ?",
            "\t\t\t\t\t\t     &mdtc_stor : NULL;",
            "\tstruct dirty_throttle_control *sdtc;",
            "\tunsigned long nr_dirty;",
            "\tlong period;",
            "\tlong pause;",
            "\tlong max_pause;",
            "\tlong min_pause;",
            "\tint nr_dirtied_pause;",
            "\tbool dirty_exceeded = false;",
            "\tunsigned long task_ratelimit;",
            "\tunsigned long dirty_ratelimit;",
            "\tstruct backing_dev_info *bdi = wb->bdi;",
            "\tbool strictlimit = bdi->capabilities & BDI_CAP_STRICTLIMIT;",
            "\tunsigned long start_time = jiffies;",
            "\tint ret = 0;",
            "",
            "\tfor (;;) {",
            "\t\tunsigned long now = jiffies;",
            "\t\tunsigned long dirty, thresh, bg_thresh;",
            "\t\tunsigned long m_dirty = 0;\t/* stop bogus uninit warnings */",
            "\t\tunsigned long m_thresh = 0;",
            "\t\tunsigned long m_bg_thresh = 0;",
            "",
            "\t\tnr_dirty = global_node_page_state(NR_FILE_DIRTY);",
            "\t\tgdtc->avail = global_dirtyable_memory();",
            "\t\tgdtc->dirty = nr_dirty + global_node_page_state(NR_WRITEBACK);",
            "",
            "\t\tdomain_dirty_limits(gdtc);",
            "",
            "\t\tif (unlikely(strictlimit)) {",
            "\t\t\twb_dirty_limits(gdtc);",
            "",
            "\t\t\tdirty = gdtc->wb_dirty;",
            "\t\t\tthresh = gdtc->wb_thresh;",
            "\t\t\tbg_thresh = gdtc->wb_bg_thresh;",
            "\t\t} else {",
            "\t\t\tdirty = gdtc->dirty;",
            "\t\t\tthresh = gdtc->thresh;",
            "\t\t\tbg_thresh = gdtc->bg_thresh;",
            "\t\t}",
            "",
            "\t\tif (mdtc) {",
            "\t\t\tunsigned long filepages, headroom, writeback;",
            "",
            "\t\t\t/*",
            "\t\t\t * If @wb belongs to !root memcg, repeat the same",
            "\t\t\t * basic calculations for the memcg domain.",
            "\t\t\t */",
            "\t\t\tmem_cgroup_wb_stats(wb, &filepages, &headroom,",
            "\t\t\t\t\t    &mdtc->dirty, &writeback);",
            "\t\t\tmdtc->dirty += writeback;",
            "\t\t\tmdtc_calc_avail(mdtc, filepages, headroom);",
            "",
            "\t\t\tdomain_dirty_limits(mdtc);",
            "",
            "\t\t\tif (unlikely(strictlimit)) {",
            "\t\t\t\twb_dirty_limits(mdtc);",
            "\t\t\t\tm_dirty = mdtc->wb_dirty;",
            "\t\t\t\tm_thresh = mdtc->wb_thresh;",
            "\t\t\t\tm_bg_thresh = mdtc->wb_bg_thresh;",
            "\t\t\t} else {",
            "\t\t\t\tm_dirty = mdtc->dirty;",
            "\t\t\t\tm_thresh = mdtc->thresh;",
            "\t\t\t\tm_bg_thresh = mdtc->bg_thresh;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * In laptop mode, we wait until hitting the higher threshold",
            "\t\t * before starting background writeout, and then write out all",
            "\t\t * the way down to the lower threshold.  So slow writers cause",
            "\t\t * minimal disk activity.",
            "\t\t *",
            "\t\t * In normal mode, we start background writeout at the lower",
            "\t\t * background_thresh, to keep the amount of dirty memory low.",
            "\t\t */",
            "\t\tif (!laptop_mode && nr_dirty > gdtc->bg_thresh &&",
            "\t\t    !writeback_in_progress(wb))",
            "\t\t\twb_start_background_writeback(wb);",
            "",
            "\t\t/*",
            "\t\t * Throttle it only when the background writeback cannot",
            "\t\t * catch-up. This avoids (excessively) small writeouts",
            "\t\t * when the wb limits are ramping up in case of !strictlimit.",
            "\t\t *",
            "\t\t * In strictlimit case make decision based on the wb counters",
            "\t\t * and limits. Small writeouts when the wb limits are ramping",
            "\t\t * up are the price we consciously pay for strictlimit-ing.",
            "\t\t *",
            "\t\t * If memcg domain is in effect, @dirty should be under",
            "\t\t * both global and memcg freerun ceilings.",
            "\t\t */",
            "\t\tif (dirty <= dirty_freerun_ceiling(thresh, bg_thresh) &&",
            "\t\t    (!mdtc ||",
            "\t\t     m_dirty <= dirty_freerun_ceiling(m_thresh, m_bg_thresh))) {",
            "\t\t\tunsigned long intv;",
            "\t\t\tunsigned long m_intv;",
            "",
            "free_running:",
            "\t\t\tintv = dirty_poll_interval(dirty, thresh);",
            "\t\t\tm_intv = ULONG_MAX;",
            "",
            "\t\t\tcurrent->dirty_paused_when = now;",
            "\t\t\tcurrent->nr_dirtied = 0;",
            "\t\t\tif (mdtc)",
            "\t\t\t\tm_intv = dirty_poll_interval(m_dirty, m_thresh);",
            "\t\t\tcurrent->nr_dirtied_pause = min(intv, m_intv);",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t/* Start writeback even when in laptop mode */",
            "\t\tif (unlikely(!writeback_in_progress(wb)))",
            "\t\t\twb_start_background_writeback(wb);",
            "",
            "\t\tmem_cgroup_flush_foreign(wb);",
            "",
            "\t\t/*",
            "\t\t * Calculate global domain's pos_ratio and select the",
            "\t\t * global dtc by default.",
            "\t\t */",
            "\t\tif (!strictlimit) {",
            "\t\t\twb_dirty_limits(gdtc);",
            "",
            "\t\t\tif ((current->flags & PF_LOCAL_THROTTLE) &&",
            "\t\t\t    gdtc->wb_dirty <",
            "\t\t\t    dirty_freerun_ceiling(gdtc->wb_thresh,",
            "\t\t\t\t\t\t  gdtc->wb_bg_thresh))",
            "\t\t\t\t/*",
            "\t\t\t\t * LOCAL_THROTTLE tasks must not be throttled",
            "\t\t\t\t * when below the per-wb freerun ceiling.",
            "\t\t\t\t */",
            "\t\t\t\tgoto free_running;",
            "\t\t}",
            "",
            "\t\tdirty_exceeded = (gdtc->wb_dirty > gdtc->wb_thresh) &&",
            "\t\t\t((gdtc->dirty > gdtc->thresh) || strictlimit);",
            "",
            "\t\twb_position_ratio(gdtc);",
            "\t\tsdtc = gdtc;",
            "",
            "\t\tif (mdtc) {",
            "\t\t\t/*",
            "\t\t\t * If memcg domain is in effect, calculate its",
            "\t\t\t * pos_ratio.  @wb should satisfy constraints from",
            "\t\t\t * both global and memcg domains.  Choose the one",
            "\t\t\t * w/ lower pos_ratio.",
            "\t\t\t */",
            "\t\t\tif (!strictlimit) {",
            "\t\t\t\twb_dirty_limits(mdtc);",
            "",
            "\t\t\t\tif ((current->flags & PF_LOCAL_THROTTLE) &&",
            "\t\t\t\t    mdtc->wb_dirty <",
            "\t\t\t\t    dirty_freerun_ceiling(mdtc->wb_thresh,",
            "\t\t\t\t\t\t\t  mdtc->wb_bg_thresh))",
            "\t\t\t\t\t/*",
            "\t\t\t\t\t * LOCAL_THROTTLE tasks must not be",
            "\t\t\t\t\t * throttled when below the per-wb",
            "\t\t\t\t\t * freerun ceiling.",
            "\t\t\t\t\t */",
            "\t\t\t\t\tgoto free_running;",
            "\t\t\t}",
            "\t\t\tdirty_exceeded |= (mdtc->wb_dirty > mdtc->wb_thresh) &&",
            "\t\t\t\t((mdtc->dirty > mdtc->thresh) || strictlimit);",
            "",
            "\t\t\twb_position_ratio(mdtc);",
            "\t\t\tif (mdtc->pos_ratio < gdtc->pos_ratio)",
            "\t\t\t\tsdtc = mdtc;",
            "\t\t}",
            "",
            "\t\tif (dirty_exceeded != wb->dirty_exceeded)",
            "\t\t\twb->dirty_exceeded = dirty_exceeded;",
            "",
            "\t\tif (time_is_before_jiffies(READ_ONCE(wb->bw_time_stamp) +",
            "\t\t\t\t\t   BANDWIDTH_INTERVAL))",
            "\t\t\t__wb_update_bandwidth(gdtc, mdtc, true);",
            "",
            "\t\t/* throttle according to the chosen dtc */",
            "\t\tdirty_ratelimit = READ_ONCE(wb->dirty_ratelimit);",
            "\t\ttask_ratelimit = ((u64)dirty_ratelimit * sdtc->pos_ratio) >>",
            "\t\t\t\t\t\t\tRATELIMIT_CALC_SHIFT;",
            "\t\tmax_pause = wb_max_pause(wb, sdtc->wb_dirty);",
            "\t\tmin_pause = wb_min_pause(wb, max_pause,",
            "\t\t\t\t\t task_ratelimit, dirty_ratelimit,",
            "\t\t\t\t\t &nr_dirtied_pause);",
            "",
            "\t\tif (unlikely(task_ratelimit == 0)) {",
            "\t\t\tperiod = max_pause;",
            "\t\t\tpause = max_pause;",
            "\t\t\tgoto pause;",
            "\t\t}",
            "\t\tperiod = HZ * pages_dirtied / task_ratelimit;",
            "\t\tpause = period;",
            "\t\tif (current->dirty_paused_when)",
            "\t\t\tpause -= now - current->dirty_paused_when;",
            "\t\t/*",
            "\t\t * For less than 1s think time (ext3/4 may block the dirtier",
            "\t\t * for up to 800ms from time to time on 1-HDD; so does xfs,",
            "\t\t * however at much less frequency), try to compensate it in",
            "\t\t * future periods by updating the virtual time; otherwise just",
            "\t\t * do a reset, as it may be a light dirtier.",
            "\t\t */",
            "\t\tif (pause < min_pause) {",
            "\t\t\ttrace_balance_dirty_pages(wb,",
            "\t\t\t\t\t\t  sdtc->thresh,",
            "\t\t\t\t\t\t  sdtc->bg_thresh,",
            "\t\t\t\t\t\t  sdtc->dirty,",
            "\t\t\t\t\t\t  sdtc->wb_thresh,",
            "\t\t\t\t\t\t  sdtc->wb_dirty,",
            "\t\t\t\t\t\t  dirty_ratelimit,",
            "\t\t\t\t\t\t  task_ratelimit,",
            "\t\t\t\t\t\t  pages_dirtied,",
            "\t\t\t\t\t\t  period,",
            "\t\t\t\t\t\t  min(pause, 0L),",
            "\t\t\t\t\t\t  start_time);",
            "\t\t\tif (pause < -HZ) {",
            "\t\t\t\tcurrent->dirty_paused_when = now;",
            "\t\t\t\tcurrent->nr_dirtied = 0;",
            "\t\t\t} else if (period) {",
            "\t\t\t\tcurrent->dirty_paused_when += period;",
            "\t\t\t\tcurrent->nr_dirtied = 0;",
            "\t\t\t} else if (current->nr_dirtied_pause <= pages_dirtied)",
            "\t\t\t\tcurrent->nr_dirtied_pause += pages_dirtied;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tif (unlikely(pause > max_pause)) {",
            "\t\t\t/* for occasional dropped task_ratelimit */",
            "\t\t\tnow += min(pause - max_pause, max_pause);",
            "\t\t\tpause = max_pause;",
            "\t\t}",
            "",
            "pause:",
            "\t\ttrace_balance_dirty_pages(wb,",
            "\t\t\t\t\t  sdtc->thresh,",
            "\t\t\t\t\t  sdtc->bg_thresh,",
            "\t\t\t\t\t  sdtc->dirty,",
            "\t\t\t\t\t  sdtc->wb_thresh,",
            "\t\t\t\t\t  sdtc->wb_dirty,",
            "\t\t\t\t\t  dirty_ratelimit,",
            "\t\t\t\t\t  task_ratelimit,",
            "\t\t\t\t\t  pages_dirtied,",
            "\t\t\t\t\t  period,",
            "\t\t\t\t\t  pause,",
            "\t\t\t\t\t  start_time);",
            "\t\tif (flags & BDP_ASYNC) {",
            "\t\t\tret = -EAGAIN;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\t__set_current_state(TASK_KILLABLE);",
            "\t\tbdi->last_bdp_sleep = jiffies;",
            "\t\tio_schedule_timeout(pause);",
            "",
            "\t\tcurrent->dirty_paused_when = now + pause;",
            "\t\tcurrent->nr_dirtied = 0;",
            "\t\tcurrent->nr_dirtied_pause = nr_dirtied_pause;",
            "",
            "\t\t/*",
            "\t\t * This is typically equal to (dirty < thresh) and can also",
            "\t\t * keep \"1000+ dd on a slow USB stick\" under control.",
            "\t\t */",
            "\t\tif (task_ratelimit)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * In the case of an unresponsive NFS server and the NFS dirty",
            "\t\t * pages exceeds dirty_thresh, give the other good wb's a pipe",
            "\t\t * to go through, so that tasks on them still remain responsive.",
            "\t\t *",
            "\t\t * In theory 1 page is enough to keep the consumer-producer",
            "\t\t * pipe going: the flusher cleans 1 page => the task dirties 1",
            "\t\t * more page. However wb_dirty has accounting errors.  So use",
            "\t\t * the larger and more IO friendly wb_stat_error.",
            "\t\t */",
            "\t\tif (sdtc->wb_dirty <= wb_stat_error())",
            "\t\t\tbreak;",
            "",
            "\t\tif (fatal_signal_pending(current))",
            "\t\t\tbreak;",
            "\t}",
            "\treturn ret;",
            "}"
          ],
          "function_name": "wb_dirty_limits, balance_dirty_pages",
          "description": "实现脏页管理核心逻辑，包含确定当前脏页限制的计算函数和脏页平衡主函数，通过多级阈值检测、动态速率限制、暂停时间控制等机制，在保证系统响应性的同时防止内存过载，支持严格限制模式下的特殊处理。",
          "similarity": 0.6315311789512634
        },
        {
          "chunk_id": 6,
          "file_path": "mm/page-writeback.c",
          "start_line": 883,
          "end_line": 1126,
          "content": [
            "static unsigned long __wb_calc_thresh(struct dirty_throttle_control *dtc)",
            "{",
            "\tstruct wb_domain *dom = dtc_dom(dtc);",
            "\tunsigned long thresh = dtc->thresh;",
            "\tu64 wb_thresh;",
            "\tunsigned long numerator, denominator;",
            "\tunsigned long wb_min_ratio, wb_max_ratio;",
            "",
            "\t/*",
            "\t * Calculate this BDI's share of the thresh ratio.",
            "\t */",
            "\tfprop_fraction_percpu(&dom->completions, dtc->wb_completions,",
            "\t\t\t      &numerator, &denominator);",
            "",
            "\twb_thresh = (thresh * (100 * BDI_RATIO_SCALE - bdi_min_ratio)) / (100 * BDI_RATIO_SCALE);",
            "\twb_thresh *= numerator;",
            "\twb_thresh = div64_ul(wb_thresh, denominator);",
            "",
            "\twb_min_max_ratio(dtc->wb, &wb_min_ratio, &wb_max_ratio);",
            "",
            "\twb_thresh += (thresh * wb_min_ratio) / (100 * BDI_RATIO_SCALE);",
            "\tif (wb_thresh > (thresh * wb_max_ratio) / (100 * BDI_RATIO_SCALE))",
            "\t\twb_thresh = thresh * wb_max_ratio / (100 * BDI_RATIO_SCALE);",
            "",
            "\treturn wb_thresh;",
            "}",
            "unsigned long wb_calc_thresh(struct bdi_writeback *wb, unsigned long thresh)",
            "{",
            "\tstruct dirty_throttle_control gdtc = { GDTC_INIT(wb),",
            "\t\t\t\t\t       .thresh = thresh };",
            "\treturn __wb_calc_thresh(&gdtc);",
            "}",
            "unsigned long cgwb_calc_thresh(struct bdi_writeback *wb)",
            "{",
            "\tstruct dirty_throttle_control gdtc = { GDTC_INIT_NO_WB };",
            "\tstruct dirty_throttle_control mdtc = { MDTC_INIT(wb, &gdtc) };",
            "\tunsigned long filepages = 0, headroom = 0, writeback = 0;",
            "",
            "\tgdtc.avail = global_dirtyable_memory();",
            "\tgdtc.dirty = global_node_page_state(NR_FILE_DIRTY) +",
            "\t\t     global_node_page_state(NR_WRITEBACK);",
            "",
            "\tmem_cgroup_wb_stats(wb, &filepages, &headroom,",
            "\t\t\t    &mdtc.dirty, &writeback);",
            "\tmdtc.dirty += writeback;",
            "\tmdtc_calc_avail(&mdtc, filepages, headroom);",
            "\tdomain_dirty_limits(&mdtc);",
            "",
            "\treturn __wb_calc_thresh(&mdtc);",
            "}",
            "static long long pos_ratio_polynom(unsigned long setpoint,",
            "\t\t\t\t\t  unsigned long dirty,",
            "\t\t\t\t\t  unsigned long limit)",
            "{",
            "\tlong long pos_ratio;",
            "\tlong x;",
            "",
            "\tx = div64_s64(((s64)setpoint - (s64)dirty) << RATELIMIT_CALC_SHIFT,",
            "\t\t      (limit - setpoint) | 1);",
            "\tpos_ratio = x;",
            "\tpos_ratio = pos_ratio * x >> RATELIMIT_CALC_SHIFT;",
            "\tpos_ratio = pos_ratio * x >> RATELIMIT_CALC_SHIFT;",
            "\tpos_ratio += 1 << RATELIMIT_CALC_SHIFT;",
            "",
            "\treturn clamp(pos_ratio, 0LL, 2LL << RATELIMIT_CALC_SHIFT);",
            "}",
            "static void wb_position_ratio(struct dirty_throttle_control *dtc)",
            "{",
            "\tstruct bdi_writeback *wb = dtc->wb;",
            "\tunsigned long write_bw = READ_ONCE(wb->avg_write_bandwidth);",
            "\tunsigned long freerun = dirty_freerun_ceiling(dtc->thresh, dtc->bg_thresh);",
            "\tunsigned long limit = hard_dirty_limit(dtc_dom(dtc), dtc->thresh);",
            "\tunsigned long wb_thresh = dtc->wb_thresh;",
            "\tunsigned long x_intercept;",
            "\tunsigned long setpoint;\t\t/* dirty pages' target balance point */",
            "\tunsigned long wb_setpoint;",
            "\tunsigned long span;",
            "\tlong long pos_ratio;\t\t/* for scaling up/down the rate limit */",
            "\tlong x;",
            "",
            "\tdtc->pos_ratio = 0;",
            "",
            "\tif (unlikely(dtc->dirty >= limit))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * global setpoint",
            "\t *",
            "\t * See comment for pos_ratio_polynom().",
            "\t */",
            "\tsetpoint = (freerun + limit) / 2;",
            "\tpos_ratio = pos_ratio_polynom(setpoint, dtc->dirty, limit);",
            "",
            "\t/*",
            "\t * The strictlimit feature is a tool preventing mistrusted filesystems",
            "\t * from growing a large number of dirty pages before throttling. For",
            "\t * such filesystems balance_dirty_pages always checks wb counters",
            "\t * against wb limits. Even if global \"nr_dirty\" is under \"freerun\".",
            "\t * This is especially important for fuse which sets bdi->max_ratio to",
            "\t * 1% by default. Without strictlimit feature, fuse writeback may",
            "\t * consume arbitrary amount of RAM because it is accounted in",
            "\t * NR_WRITEBACK_TEMP which is not involved in calculating \"nr_dirty\".",
            "\t *",
            "\t * Here, in wb_position_ratio(), we calculate pos_ratio based on",
            "\t * two values: wb_dirty and wb_thresh. Let's consider an example:",
            "\t * total amount of RAM is 16GB, bdi->max_ratio is equal to 1%, global",
            "\t * limits are set by default to 10% and 20% (background and throttle).",
            "\t * Then wb_thresh is 1% of 20% of 16GB. This amounts to ~8K pages.",
            "\t * wb_calc_thresh(wb, bg_thresh) is about ~4K pages. wb_setpoint is",
            "\t * about ~6K pages (as the average of background and throttle wb",
            "\t * limits). The 3rd order polynomial will provide positive feedback if",
            "\t * wb_dirty is under wb_setpoint and vice versa.",
            "\t *",
            "\t * Note, that we cannot use global counters in these calculations",
            "\t * because we want to throttle process writing to a strictlimit wb",
            "\t * much earlier than global \"freerun\" is reached (~23MB vs. ~2.3GB",
            "\t * in the example above).",
            "\t */",
            "\tif (unlikely(wb->bdi->capabilities & BDI_CAP_STRICTLIMIT)) {",
            "\t\tlong long wb_pos_ratio;",
            "",
            "\t\tif (dtc->wb_dirty < 8) {",
            "\t\t\tdtc->pos_ratio = min_t(long long, pos_ratio * 2,",
            "\t\t\t\t\t   2 << RATELIMIT_CALC_SHIFT);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tif (dtc->wb_dirty >= wb_thresh)",
            "\t\t\treturn;",
            "",
            "\t\twb_setpoint = dirty_freerun_ceiling(wb_thresh,",
            "\t\t\t\t\t\t    dtc->wb_bg_thresh);",
            "",
            "\t\tif (wb_setpoint == 0 || wb_setpoint == wb_thresh)",
            "\t\t\treturn;",
            "",
            "\t\twb_pos_ratio = pos_ratio_polynom(wb_setpoint, dtc->wb_dirty,",
            "\t\t\t\t\t\t wb_thresh);",
            "",
            "\t\t/*",
            "\t\t * Typically, for strictlimit case, wb_setpoint << setpoint",
            "\t\t * and pos_ratio >> wb_pos_ratio. In the other words global",
            "\t\t * state (\"dirty\") is not limiting factor and we have to",
            "\t\t * make decision based on wb counters. But there is an",
            "\t\t * important case when global pos_ratio should get precedence:",
            "\t\t * global limits are exceeded (e.g. due to activities on other",
            "\t\t * wb's) while given strictlimit wb is below limit.",
            "\t\t *",
            "\t\t * \"pos_ratio * wb_pos_ratio\" would work for the case above,",
            "\t\t * but it would look too non-natural for the case of all",
            "\t\t * activity in the system coming from a single strictlimit wb",
            "\t\t * with bdi->max_ratio == 100%.",
            "\t\t *",
            "\t\t * Note that min() below somewhat changes the dynamics of the",
            "\t\t * control system. Normally, pos_ratio value can be well over 3",
            "\t\t * (when globally we are at freerun and wb is well below wb",
            "\t\t * setpoint). Now the maximum pos_ratio in the same situation",
            "\t\t * is 2. We might want to tweak this if we observe the control",
            "\t\t * system is too slow to adapt.",
            "\t\t */",
            "\t\tdtc->pos_ratio = min(pos_ratio, wb_pos_ratio);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * We have computed basic pos_ratio above based on global situation. If",
            "\t * the wb is over/under its share of dirty pages, we want to scale",
            "\t * pos_ratio further down/up. That is done by the following mechanism.",
            "\t */",
            "",
            "\t/*",
            "\t * wb setpoint",
            "\t *",
            "\t *        f(wb_dirty) := 1.0 + k * (wb_dirty - wb_setpoint)",
            "\t *",
            "\t *                        x_intercept - wb_dirty",
            "\t *                     := --------------------------",
            "\t *                        x_intercept - wb_setpoint",
            "\t *",
            "\t * The main wb control line is a linear function that subjects to",
            "\t *",
            "\t * (1) f(wb_setpoint) = 1.0",
            "\t * (2) k = - 1 / (8 * write_bw)  (in single wb case)",
            "\t *     or equally: x_intercept = wb_setpoint + 8 * write_bw",
            "\t *",
            "\t * For single wb case, the dirty pages are observed to fluctuate",
            "\t * regularly within range",
            "\t *        [wb_setpoint - write_bw/2, wb_setpoint + write_bw/2]",
            "\t * for various filesystems, where (2) can yield in a reasonable 12.5%",
            "\t * fluctuation range for pos_ratio.",
            "\t *",
            "\t * For JBOD case, wb_thresh (not wb_dirty!) could fluctuate up to its",
            "\t * own size, so move the slope over accordingly and choose a slope that",
            "\t * yields 100% pos_ratio fluctuation on suddenly doubled wb_thresh.",
            "\t */",
            "\tif (unlikely(wb_thresh > dtc->thresh))",
            "\t\twb_thresh = dtc->thresh;",
            "\t/*",
            "\t * It's very possible that wb_thresh is close to 0 not because the",
            "\t * device is slow, but that it has remained inactive for long time.",
            "\t * Honour such devices a reasonable good (hopefully IO efficient)",
            "\t * threshold, so that the occasional writes won't be blocked and active",
            "\t * writes can rampup the threshold quickly.",
            "\t */",
            "\twb_thresh = max(wb_thresh, (limit - dtc->dirty) / 8);",
            "\t/*",
            "\t * scale global setpoint to wb's:",
            "\t *\twb_setpoint = setpoint * wb_thresh / thresh",
            "\t */",
            "\tx = div_u64((u64)wb_thresh << 16, dtc->thresh | 1);",
            "\twb_setpoint = setpoint * (u64)x >> 16;",
            "\t/*",
            "\t * Use span=(8*write_bw) in single wb case as indicated by",
            "\t * (thresh - wb_thresh ~= 0) and transit to wb_thresh in JBOD case.",
            "\t *",
            "\t *        wb_thresh                    thresh - wb_thresh",
            "\t * span = --------- * (8 * write_bw) + ------------------ * wb_thresh",
            "\t *         thresh                           thresh",
            "\t */",
            "\tspan = (dtc->thresh - wb_thresh + 8 * write_bw) * (u64)x >> 16;",
            "\tx_intercept = wb_setpoint + span;",
            "",
            "\tif (dtc->wb_dirty < x_intercept - span / 4) {",
            "\t\tpos_ratio = div64_u64(pos_ratio * (x_intercept - dtc->wb_dirty),",
            "\t\t\t\t      (x_intercept - wb_setpoint) | 1);",
            "\t} else",
            "\t\tpos_ratio /= 4;",
            "",
            "\t/*",
            "\t * wb reserve area, safeguard against dirty pool underrun and disk idle",
            "\t * It may push the desired control point of global dirty pages higher",
            "\t * than setpoint.",
            "\t */",
            "\tx_intercept = wb_thresh / 2;",
            "\tif (dtc->wb_dirty < x_intercept) {",
            "\t\tif (dtc->wb_dirty > x_intercept / 8)",
            "\t\t\tpos_ratio = div_u64(pos_ratio * x_intercept,",
            "\t\t\t\t\t    dtc->wb_dirty);",
            "\t\telse",
            "\t\t\tpos_ratio *= 8;",
            "\t}",
            "",
            "\tdtc->pos_ratio = pos_ratio;",
            "}"
          ],
          "function_name": "__wb_calc_thresh, wb_calc_thresh, cgwb_calc_thresh, pos_ratio_polynom, wb_position_ratio",
          "description": "实现基于脏页阈值的动态调节算法，包含计算全局和内存控制组的脏页阈值、基于多项式的位置比值计算、根据写入带宽和当前脏页状态调整控制参数等功能，通过多级阈值和反馈机制平衡系统负载。",
          "similarity": 0.6291717886924744
        },
        {
          "chunk_id": 2,
          "file_path": "mm/page-writeback.c",
          "start_line": 345,
          "end_line": 459,
          "content": [
            "static unsigned long global_dirtyable_memory(void)",
            "{",
            "\tunsigned long x;",
            "",
            "\tx = global_zone_page_state(NR_FREE_PAGES);",
            "\t/*",
            "\t * Pages reserved for the kernel should not be considered",
            "\t * dirtyable, to prevent a situation where reclaim has to",
            "\t * clean pages in order to balance the zones.",
            "\t */",
            "\tx -= min(x, totalreserve_pages);",
            "",
            "\tx += global_node_page_state(NR_INACTIVE_FILE);",
            "\tx += global_node_page_state(NR_ACTIVE_FILE);",
            "",
            "\tif (!vm_highmem_is_dirtyable)",
            "\t\tx -= highmem_dirtyable_memory(x);",
            "",
            "\treturn x + 1;\t/* Ensure that we never return 0 */",
            "}",
            "static void domain_dirty_limits(struct dirty_throttle_control *dtc)",
            "{",
            "\tconst unsigned long available_memory = dtc->avail;",
            "\tstruct dirty_throttle_control *gdtc = mdtc_gdtc(dtc);",
            "\tunsigned long bytes = vm_dirty_bytes;",
            "\tunsigned long bg_bytes = dirty_background_bytes;",
            "\t/* convert ratios to per-PAGE_SIZE for higher precision */",
            "\tunsigned long ratio = (vm_dirty_ratio * PAGE_SIZE) / 100;",
            "\tunsigned long bg_ratio = (dirty_background_ratio * PAGE_SIZE) / 100;",
            "\tunsigned long thresh;",
            "\tunsigned long bg_thresh;",
            "\tstruct task_struct *tsk;",
            "",
            "\t/* gdtc is !NULL iff @dtc is for memcg domain */",
            "\tif (gdtc) {",
            "\t\tunsigned long global_avail = gdtc->avail;",
            "",
            "\t\t/*",
            "\t\t * The byte settings can't be applied directly to memcg",
            "\t\t * domains.  Convert them to ratios by scaling against",
            "\t\t * globally available memory.  As the ratios are in",
            "\t\t * per-PAGE_SIZE, they can be obtained by dividing bytes by",
            "\t\t * number of pages.",
            "\t\t */",
            "\t\tif (bytes)",
            "\t\t\tratio = min(DIV_ROUND_UP(bytes, global_avail),",
            "\t\t\t\t    PAGE_SIZE);",
            "\t\tif (bg_bytes)",
            "\t\t\tbg_ratio = min(DIV_ROUND_UP(bg_bytes, global_avail),",
            "\t\t\t\t       PAGE_SIZE);",
            "\t\tbytes = bg_bytes = 0;",
            "\t}",
            "",
            "\tif (bytes)",
            "\t\tthresh = DIV_ROUND_UP(bytes, PAGE_SIZE);",
            "\telse",
            "\t\tthresh = (ratio * available_memory) / PAGE_SIZE;",
            "",
            "\tif (bg_bytes)",
            "\t\tbg_thresh = DIV_ROUND_UP(bg_bytes, PAGE_SIZE);",
            "\telse",
            "\t\tbg_thresh = (bg_ratio * available_memory) / PAGE_SIZE;",
            "",
            "\ttsk = current;",
            "\tif (rt_or_dl_task(tsk)) {",
            "\t\tbg_thresh += bg_thresh / 4 + global_wb_domain.dirty_limit / 32;",
            "\t\tthresh += thresh / 4 + global_wb_domain.dirty_limit / 32;",
            "\t}",
            "\t/*",
            "\t * Dirty throttling logic assumes the limits in page units fit into",
            "\t * 32-bits. This gives 16TB dirty limits max which is hopefully enough.",
            "\t */",
            "\tif (thresh > UINT_MAX)",
            "\t\tthresh = UINT_MAX;",
            "\t/* This makes sure bg_thresh is within 32-bits as well */",
            "\tif (bg_thresh >= thresh)",
            "\t\tbg_thresh = thresh / 2;",
            "\tdtc->thresh = thresh;",
            "\tdtc->bg_thresh = bg_thresh;",
            "",
            "\t/* we should eventually report the domain in the TP */",
            "\tif (!gdtc)",
            "\t\ttrace_global_dirty_state(bg_thresh, thresh);",
            "}",
            "void global_dirty_limits(unsigned long *pbackground, unsigned long *pdirty)",
            "{",
            "\tstruct dirty_throttle_control gdtc = { GDTC_INIT_NO_WB };",
            "",
            "\tgdtc.avail = global_dirtyable_memory();",
            "\tdomain_dirty_limits(&gdtc);",
            "",
            "\t*pbackground = gdtc.bg_thresh;",
            "\t*pdirty = gdtc.thresh;",
            "}",
            "static unsigned long node_dirty_limit(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long node_memory = node_dirtyable_memory(pgdat);",
            "\tstruct task_struct *tsk = current;",
            "\tunsigned long dirty;",
            "",
            "\tif (vm_dirty_bytes)",
            "\t\tdirty = DIV_ROUND_UP(vm_dirty_bytes, PAGE_SIZE) *",
            "\t\t\tnode_memory / global_dirtyable_memory();",
            "\telse",
            "\t\tdirty = vm_dirty_ratio * node_memory / 100;",
            "",
            "\tif (rt_or_dl_task(tsk))",
            "\t\tdirty += dirty / 4;",
            "",
            "\t/*",
            "\t * Dirty throttling logic assumes the limits in page units fit into",
            "\t * 32-bits. This gives 16TB dirty limits max which is hopefully enough.",
            "\t */",
            "\treturn min_t(unsigned long, dirty, UINT_MAX);",
            "}"
          ],
          "function_name": "global_dirtyable_memory, domain_dirty_limits, global_dirty_limits, node_dirty_limit",
          "description": "计算全局可用脏化内存并结合域级别参数确定脏页上限，根据实时任务特性调整阈值，提供全局和节点层级的脏页限制。",
          "similarity": 0.619670569896698
        }
      ]
    },
    {
      "source_file": "mm/swap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:25:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `swap.c`\n\n---\n\n# swap.c 技术文档\n\n## 1. 文件概述\n\n`swap.c` 是 Linux 内核内存管理子系统（MM）中的核心文件之一，主要负责页面回收（page reclaim）、LRU（Least Recently Used）链表管理、页面释放以及与交换（swap）机制相关的底层支持逻辑。尽管文件名为 `swap.c`，但其功能不仅限于交换，而是涵盖了通用的页面生命周期管理、LRU 链表操作、页面引用计数释放、可回收性判断等关键内存管理任务。该文件为页面缓存（page cache）、匿名页（anonymous pages）和大页（huge pages）提供统一的释放与 LRU 管理接口。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `page_cluster`：控制一次 I/O 操作中尝试换入/换出的页面数量（以 2 的幂表示），默认值由系统配置决定。\n- `page_cluster_max`：`page_cluster` 的最大允许值（31，即最多 2^31 页，实际受架构限制）。\n\n### 主要数据结构\n- `struct lru_rotate`：每个 CPU 私有的结构，用于在中断禁用上下文中批量处理需移至 LRU 链表尾部的页面（如 `folio_rotate_reclaimable` 场景）。\n- `struct cpu_fbatches`：每个 CPU 私有的 folio 批处理结构，包含多个 folio_batch，用于高效地向 LRU 链表添加、停用或激活页面，避免频繁获取 LRU 锁。\n\n### 主要函数\n- `__folio_put()`：释放一个 folio 的核心函数，根据 folio 类型（设备内存、大页、普通页）调用相应的释放路径。\n- `put_pages_list()`：批量释放通过 `lru` 字段链接的页面列表，常用于网络子系统或 compound page 释放。\n- `lru_add_fn()`：将 folio 添加到对应 LRU 链表的回调函数，处理可回收性（evictable/unevictable）状态转换和统计计数。\n- `folio_batch_move_lru()`：批量执行 LRU 操作（如添加、移动），在持有 LRU 锁期间完成所有 folio 的处理。\n- `folio_rotate_reclaimable()`：在写回完成后，若页面仍可回收，则将其移至 inactive LRU 链表尾部，以延迟其被回收的时间。\n- `lru_note_cost()`：记录 LRU 扫描过程中的 I/O 和旋转（rotation）成本，用于后续调整 anon/file LRU 的扫描比例。\n\n## 3. 关键实现\n\n### LRU 批处理机制\n为减少 LRU 锁竞争，内核采用 per-CPU 批处理（`folio_batch`）方式暂存待处理的 folio。当批处理满或遇到大页（`folio_test_large`）时，才批量获取 LRU 锁并执行操作（如 `lru_add_fn`）。这显著提升了高并发场景下的性能。\n\n### 可回收性管理\n页面是否可回收由 `folio_evictable()` 判断，主要依据是否被 mlock 锁定。在添加到 LRU 时：\n- 若页面变为可回收（原为 unevictable），则增加 `UNEVICTABLE_PGRESCUED` 统计；\n- 若页面不可回收，则清除 active 标志，设置 unevictable 标志，并重置 `mlock_count`，同时增加 `UNEVICTABLE_PGCULLED` 统计。\n\n### 页面释放路径\n`__folio_put()` 是 folio 引用计数归零后的释放入口：\n1. 设备内存 folio 调用 `free_zone_device_folio()`\n2. 大页 folio 调用 `free_huge_folio()`\n3. 普通 folio 先从 LRU 移除（若在 LRU 上），然后解绑内存控制组（memcg），最后调用 `free_unref_page()` 释放到伙伴系统。\n\n### LRU 旋转优化\n`folio_rotate_reclaimable()` 在写回结束时，若页面干净且未锁定，则将其移至 inactive LRU 尾部。此操作通过 per-CPU 的 `lru_rotate` 批处理完成，仅在必要时获取 LRU 锁，避免影响写回关键路径性能。\n\n### 成本跟踪\n`lru_note_cost()` 通过累加 `nr_io * SWAP_CLUSTER_MAX + nr_rotated` 来量化扫描成本，用于动态调整匿名页与文件页 LRU 的扫描比例，优化内存回收效率。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/pagevec.h>`、`\"internal.h\"` 等，使用伙伴系统、LRU 框架、内存控制组（memcg）等基础组件。\n- **交换子系统**：虽不直接实现 swap read/write，但为 `vmscan.c` 中的页面回收提供 LRU 操作接口，是 swap 机制的支撑模块。\n- **大页支持**：通过 `hugetlb.h` 与大页子系统交互，特殊处理大页释放。\n- **设备内存**：通过 `memremap.h` 支持持久内存（pmem）等 zone device 页面的释放。\n- **跟踪与统计**：使用 tracepoint（`trace/events/pagemap.h`）和 VM 统计（`kernel_stat.h`）进行性能分析。\n- **SMP 支持**：大量使用 per-CPU 变量（`DEFINE_PER_CPU`）和本地锁（`local_lock_t`）优化多核性能。\n\n## 5. 使用场景\n\n- **页面回收（Reclaim）**：当内存压力触发 kswapd 或 direct reclaim 时，`vmscan.c` 调用本文件的 LRU 操作函数来隔离、释放页面。\n- **页面缓存释放**：文件系统或网络子系统在释放 page cache 页面时，通过 `__folio_put()` 或 `put_pages_list()` 触发 LRU 移除和内存释放。\n- **写回完成处理**：块设备或文件系统在完成脏页写回后，调用 `folio_rotate_reclaimable()` 更新页面在 LRU 中的位置。\n- **内存控制组（cgroup）**：memcg 回收内存时，复用本文件的 LRU 批处理和 folio 释放逻辑。\n- **大页与设备内存管理**：透明大页（THP）或持久内存应用释放页面时，通过统一的 `__folio_put()` 接口分发到专用释放函数。\n- **系统调优**：管理员通过 `/proc/sys/vm/page-cluster` 调整 `page_cluster` 值，影响 swap 和 page cache 的 I/O 批量大小。",
      "similarity": 0.6434817314147949,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/swap.c",
          "start_line": 77,
          "end_line": 190,
          "content": [
            "static void __page_cache_release(struct folio *folio, struct lruvec **lruvecp,",
            "\t\tunsigned long *flagsp)",
            "{",
            "\tif (folio_test_lru(folio)) {",
            "\t\tfolio_lruvec_relock_irqsave(folio, lruvecp, flagsp);",
            "\t\tlruvec_del_folio(*lruvecp, folio);",
            "\t\t__folio_clear_lru_flags(folio);",
            "\t}",
            "}",
            "static void page_cache_release(struct folio *folio)",
            "{",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags;",
            "",
            "\t__page_cache_release(folio, &lruvec, &flags);",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "}",
            "void __folio_put(struct folio *folio)",
            "{",
            "\tif (unlikely(folio_is_zone_device(folio))) {",
            "\t\tfree_zone_device_folio(folio);",
            "\t\treturn;",
            "\t} else if (folio_test_hugetlb(folio)) {",
            "\t\tfree_huge_folio(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tpage_cache_release(folio);",
            "\tfolio_unqueue_deferred_split(folio);",
            "\tmem_cgroup_uncharge(folio);",
            "\tfree_unref_page(&folio->page, folio_order(folio));",
            "}",
            "void put_pages_list(struct list_head *pages)",
            "{",
            "\tstruct folio_batch fbatch;",
            "\tstruct folio *folio, *next;",
            "",
            "\tfolio_batch_init(&fbatch);",
            "\tlist_for_each_entry_safe(folio, next, pages, lru) {",
            "\t\tif (!folio_put_testzero(folio))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_hugetlb(folio)) {",
            "\t\t\tfree_huge_folio(folio);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\t/* LRU flag must be clear because it's passed using the lru */",
            "\t\tif (folio_batch_add(&fbatch, folio) > 0)",
            "\t\t\tcontinue;",
            "\t\tfree_unref_folios(&fbatch);",
            "\t}",
            "",
            "\tif (fbatch.nr)",
            "\t\tfree_unref_folios(&fbatch);",
            "\tINIT_LIST_HEAD(pages);",
            "}",
            "static void lru_add_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tint was_unevictable = folio_test_clear_unevictable(folio);",
            "\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\t/*",
            "\t * Is an smp_mb__after_atomic() still required here, before",
            "\t * folio_evictable() tests the mlocked flag, to rule out the possibility",
            "\t * of stranding an evictable folio on an unevictable LRU?  I think",
            "\t * not, because __munlock_folio() only clears the mlocked flag",
            "\t * while the LRU lock is held.",
            "\t *",
            "\t * (That is not true of __page_cache_release(), and not necessarily",
            "\t * true of folios_put(): but those only clear the mlocked flag after",
            "\t * folio_put_testzero() has excluded any other users of the folio.)",
            "\t */",
            "\tif (folio_evictable(folio)) {",
            "\t\tif (was_unevictable)",
            "\t\t\t__count_vm_events(UNEVICTABLE_PGRESCUED, nr_pages);",
            "\t} else {",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_set_unevictable(folio);",
            "\t\t/*",
            "\t\t * folio->mlock_count = !!folio_test_mlocked(folio)?",
            "\t\t * But that leaves __mlock_folio() in doubt whether another",
            "\t\t * actor has already counted the mlock or not.  Err on the",
            "\t\t * safe side, underestimate, let page reclaim fix it, rather",
            "\t\t * than leaving a page on the unevictable LRU indefinitely.",
            "\t\t */",
            "\t\tfolio->mlock_count = 0;",
            "\t\tif (!was_unevictable)",
            "\t\t\t__count_vm_events(UNEVICTABLE_PGCULLED, nr_pages);",
            "\t}",
            "",
            "\tlruvec_add_folio(lruvec, folio);",
            "\ttrace_mm_lru_insertion(folio);",
            "}",
            "static void folio_batch_move_lru(struct folio_batch *fbatch, move_fn_t move_fn)",
            "{",
            "\tint i;",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags = 0;",
            "",
            "\tfor (i = 0; i < folio_batch_count(fbatch); i++) {",
            "\t\tstruct folio *folio = fbatch->folios[i];",
            "",
            "\t\tfolio_lruvec_relock_irqsave(folio, &lruvec, &flags);",
            "\t\tmove_fn(lruvec, folio);",
            "",
            "\t\tfolio_set_lru(folio);",
            "\t}",
            "",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\tfolios_put(fbatch);",
            "}"
          ],
          "function_name": "__page_cache_release, page_cache_release, __folio_put, put_pages_list, lru_add_fn, folio_batch_move_lru",
          "description": "实现了页面缓存释放和LRU列表维护逻辑，包含__page_cache_release用于从LRU列表移除页面，page_cache_release处理普通页面释放流程，__folio_put负责释放非设备映射和大页，put_pages_list批量处理页面释放，lru_add_fn将页面添加到LRU列表并根据是否可交换设置相应标志。",
          "similarity": 0.6236550807952881
        },
        {
          "chunk_id": 8,
          "file_path": "mm/swap.c",
          "start_line": 1079,
          "end_line": 1111,
          "content": [
            "void __folio_batch_release(struct folio_batch *fbatch)",
            "{",
            "\tif (!fbatch->percpu_pvec_drained) {",
            "\t\tlru_add_drain();",
            "\t\tfbatch->percpu_pvec_drained = true;",
            "\t}",
            "\tfolios_put(fbatch);",
            "}",
            "void folio_batch_remove_exceptionals(struct folio_batch *fbatch)",
            "{",
            "\tunsigned int i, j;",
            "",
            "\tfor (i = 0, j = 0; i < folio_batch_count(fbatch); i++) {",
            "\t\tstruct folio *folio = fbatch->folios[i];",
            "\t\tif (!xa_is_value(folio))",
            "\t\t\tfbatch->folios[j++] = folio;",
            "\t}",
            "\tfbatch->nr = j;",
            "}",
            "void __init swap_setup(void)",
            "{",
            "\tunsigned long megs = totalram_pages() >> (20 - PAGE_SHIFT);",
            "",
            "\t/* Use a smaller cluster for small-memory machines */",
            "\tif (megs < 16)",
            "\t\tpage_cluster = 2;",
            "\telse",
            "\t\tpage_cluster = 3;",
            "\t/*",
            "\t * Right now other parts of the system means that we",
            "\t * _really_ don't want to cluster much more",
            "\t */",
            "}"
          ],
          "function_name": "__folio_batch_release, folio_batch_remove_exceptionals, swap_setup",
          "description": "__folio_batch_release 标记并释放页面批次引用，folio_batch_remove_exceptionals 清理异常条目；swap_setup 初始化页面聚类参数，根据内存大小调整page_cluster值。",
          "similarity": 0.5734831094741821
        },
        {
          "chunk_id": 2,
          "file_path": "mm/swap.c",
          "start_line": 211,
          "end_line": 317,
          "content": [
            "static void folio_batch_add_and_move(struct folio_batch *fbatch,",
            "\t\tstruct folio *folio, move_fn_t move_fn)",
            "{",
            "\tif (folio_batch_add(fbatch, folio) && !folio_test_large(folio) &&",
            "\t    !lru_cache_disabled())",
            "\t\treturn;",
            "\tfolio_batch_move_lru(fbatch, move_fn);",
            "}",
            "static void lru_move_tail_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tif (!folio_test_unevictable(folio)) {",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tfolio_clear_active(folio);",
            "\t\tlruvec_add_folio_tail(lruvec, folio);",
            "\t\t__count_vm_events(PGROTATED, folio_nr_pages(folio));",
            "\t}",
            "}",
            "void folio_rotate_reclaimable(struct folio *folio)",
            "{",
            "\tif (!folio_test_locked(folio) && !folio_test_dirty(folio) &&",
            "\t    !folio_test_unevictable(folio)) {",
            "\t\tstruct folio_batch *fbatch;",
            "\t\tunsigned long flags;",
            "",
            "\t\tfolio_get(folio);",
            "\t\tif (!folio_test_clear_lru(folio)) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tlocal_lock_irqsave(&lru_rotate.lock, flags);",
            "\t\tfbatch = this_cpu_ptr(&lru_rotate.fbatch);",
            "\t\tfolio_batch_add_and_move(fbatch, folio, lru_move_tail_fn);",
            "\t\tlocal_unlock_irqrestore(&lru_rotate.lock, flags);",
            "\t}",
            "}",
            "void lru_note_cost(struct lruvec *lruvec, bool file,",
            "\t\t   unsigned int nr_io, unsigned int nr_rotated)",
            "{",
            "\tunsigned long cost;",
            "",
            "\t/*",
            "\t * Reflect the relative cost of incurring IO and spending CPU",
            "\t * time on rotations. This doesn't attempt to make a precise",
            "\t * comparison, it just says: if reloads are about comparable",
            "\t * between the LRU lists, or rotations are overwhelmingly",
            "\t * different between them, adjust scan balance for CPU work.",
            "\t */",
            "\tcost = nr_io * SWAP_CLUSTER_MAX + nr_rotated;",
            "",
            "\tdo {",
            "\t\tunsigned long lrusize;",
            "",
            "\t\t/*",
            "\t\t * Hold lruvec->lru_lock is safe here, since",
            "\t\t * 1) The pinned lruvec in reclaim, or",
            "\t\t * 2) From a pre-LRU page during refault (which also holds the",
            "\t\t *    rcu lock, so would be safe even if the page was on the LRU",
            "\t\t *    and could move simultaneously to a new lruvec).",
            "\t\t */",
            "\t\tspin_lock_irq(&lruvec->lru_lock);",
            "\t\t/* Record cost event */",
            "\t\tif (file)",
            "\t\t\tlruvec->file_cost += cost;",
            "\t\telse",
            "\t\t\tlruvec->anon_cost += cost;",
            "",
            "\t\t/*",
            "\t\t * Decay previous events",
            "\t\t *",
            "\t\t * Because workloads change over time (and to avoid",
            "\t\t * overflow) we keep these statistics as a floating",
            "\t\t * average, which ends up weighing recent refaults",
            "\t\t * more than old ones.",
            "\t\t */",
            "\t\tlrusize = lruvec_page_state(lruvec, NR_INACTIVE_ANON) +",
            "\t\t\t  lruvec_page_state(lruvec, NR_ACTIVE_ANON) +",
            "\t\t\t  lruvec_page_state(lruvec, NR_INACTIVE_FILE) +",
            "\t\t\t  lruvec_page_state(lruvec, NR_ACTIVE_FILE);",
            "",
            "\t\tif (lruvec->file_cost + lruvec->anon_cost > lrusize / 4) {",
            "\t\t\tlruvec->file_cost /= 2;",
            "\t\t\tlruvec->anon_cost /= 2;",
            "\t\t}",
            "\t\tspin_unlock_irq(&lruvec->lru_lock);",
            "\t} while ((lruvec = parent_lruvec(lruvec)));",
            "}",
            "void lru_note_cost_refault(struct folio *folio)",
            "{",
            "\tlru_note_cost(folio_lruvec(folio), folio_is_file_lru(folio),",
            "\t\t      folio_nr_pages(folio), 0);",
            "}",
            "static void folio_activate_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tif (!folio_test_active(folio) && !folio_test_unevictable(folio)) {",
            "\t\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tfolio_set_active(folio);",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "\t\ttrace_mm_lru_activate(folio);",
            "",
            "\t\t__count_vm_events(PGACTIVATE, nr_pages);",
            "\t\t__count_memcg_events(lruvec_memcg(lruvec), PGACTIVATE,",
            "\t\t\t\t     nr_pages);",
            "\t}",
            "}"
          ],
          "function_name": "folio_batch_add_and_move, lru_move_tail_fn, folio_rotate_reclaimable, lru_note_cost, lru_note_cost_refault, folio_activate_fn",
          "description": "提供LRU列表页面移动和成本统计功能，folio_batch_add_and_move处理页面批量移动，lru_move_tail_fn将页面移动到LRU尾部，folio_rotate_reclaimable将可回收页面转移到冷列表，lru_note_cost记录页面访问成本用于调整扫描策略，folio_activate_fn激活页面至活动列表。",
          "similarity": 0.5625964403152466
        },
        {
          "chunk_id": 3,
          "file_path": "mm/swap.c",
          "start_line": 332,
          "end_line": 460,
          "content": [
            "static void folio_activate_drain(int cpu)",
            "{",
            "\tstruct folio_batch *fbatch = &per_cpu(cpu_fbatches.activate, cpu);",
            "",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, folio_activate_fn);",
            "}",
            "void folio_activate(struct folio *folio)",
            "{",
            "\tif (!folio_test_active(folio) && !folio_test_unevictable(folio)) {",
            "\t\tstruct folio_batch *fbatch;",
            "",
            "\t\tfolio_get(folio);",
            "\t\tif (!folio_test_clear_lru(folio)) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tlocal_lock(&cpu_fbatches.lock);",
            "\t\tfbatch = this_cpu_ptr(&cpu_fbatches.activate);",
            "\t\tfolio_batch_add_and_move(fbatch, folio, folio_activate_fn);",
            "\t\tlocal_unlock(&cpu_fbatches.lock);",
            "\t}",
            "}",
            "static inline void folio_activate_drain(int cpu)",
            "{",
            "}",
            "void folio_activate(struct folio *folio)",
            "{",
            "\tstruct lruvec *lruvec;",
            "",
            "\tif (folio_test_clear_lru(folio)) {",
            "\t\tlruvec = folio_lruvec_lock_irq(folio);",
            "\t\tfolio_activate_fn(lruvec, folio);",
            "\t\tunlock_page_lruvec_irq(lruvec);",
            "\t\tfolio_set_lru(folio);",
            "\t}",
            "}",
            "static void __lru_cache_activate_folio(struct folio *folio)",
            "{",
            "\tstruct folio_batch *fbatch;",
            "\tint i;",
            "",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_add);",
            "",
            "\t/*",
            "\t * Search backwards on the optimistic assumption that the folio being",
            "\t * activated has just been added to this batch. Note that only",
            "\t * the local batch is examined as a !LRU folio could be in the",
            "\t * process of being released, reclaimed, migrated or on a remote",
            "\t * batch that is currently being drained. Furthermore, marking",
            "\t * a remote batch's folio active potentially hits a race where",
            "\t * a folio is marked active just after it is added to the inactive",
            "\t * list causing accounting errors and BUG_ON checks to trigger.",
            "\t */",
            "\tfor (i = folio_batch_count(fbatch) - 1; i >= 0; i--) {",
            "\t\tstruct folio *batch_folio = fbatch->folios[i];",
            "",
            "\t\tif (batch_folio == folio) {",
            "\t\t\tfolio_set_active(folio);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "}",
            "static void folio_inc_refs(struct folio *folio)",
            "{",
            "\tunsigned long new_flags, old_flags = READ_ONCE(folio->flags);",
            "",
            "\tif (folio_test_unevictable(folio))",
            "\t\treturn;",
            "",
            "\tif (!folio_test_referenced(folio)) {",
            "\t\tfolio_set_referenced(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!folio_test_workingset(folio)) {",
            "\t\tfolio_set_workingset(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* see the comment on MAX_NR_TIERS */",
            "\tdo {",
            "\t\tnew_flags = old_flags & LRU_REFS_MASK;",
            "\t\tif (new_flags == LRU_REFS_MASK)",
            "\t\t\tbreak;",
            "",
            "\t\tnew_flags += BIT(LRU_REFS_PGOFF);",
            "\t\tnew_flags |= old_flags & ~LRU_REFS_MASK;",
            "\t} while (!try_cmpxchg(&folio->flags, &old_flags, new_flags));",
            "}",
            "static void folio_inc_refs(struct folio *folio)",
            "{",
            "}",
            "void folio_mark_accessed(struct folio *folio)",
            "{",
            "\tif (lru_gen_enabled()) {",
            "\t\tfolio_inc_refs(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!folio_test_referenced(folio)) {",
            "\t\tfolio_set_referenced(folio);",
            "\t} else if (folio_test_unevictable(folio)) {",
            "\t\t/*",
            "\t\t * Unevictable pages are on the \"LRU_UNEVICTABLE\" list. But,",
            "\t\t * this list is never rotated or maintained, so marking an",
            "\t\t * unevictable page accessed has no effect.",
            "\t\t */",
            "\t} else if (!folio_test_active(folio)) {",
            "\t\t/*",
            "\t\t * If the folio is on the LRU, queue it for activation via",
            "\t\t * cpu_fbatches.activate. Otherwise, assume the folio is in a",
            "\t\t * folio_batch, mark it active and it'll be moved to the active",
            "\t\t * LRU on the next drain.",
            "\t\t */",
            "\t\tif (folio_test_lru(folio))",
            "\t\t\tfolio_activate(folio);",
            "\t\telse",
            "\t\t\t__lru_cache_activate_folio(folio);",
            "\t\tfolio_clear_referenced(folio);",
            "\t\tworkingset_activation(folio);",
            "\t}",
            "\tif (folio_test_idle(folio))",
            "\t\tfolio_clear_idle(folio);",
            "}"
          ],
          "function_name": "folio_activate_drain, folio_activate, folio_activate_drain, folio_activate, __lru_cache_activate_folio, folio_inc_refs, folio_inc_refs, folio_mark_accessed",
          "description": "实现页面激活和引用计数管理，folio_activate_drain和folio_activate将页面从非活动列表转至活动列表，__lru_cache_activate_folio处理本地批次中的页面激活，folio_inc_refs更新页面引用标志，folio_mark_accessed标记页面访问状态并触发激活流程。",
          "similarity": 0.5528659224510193
        },
        {
          "chunk_id": 0,
          "file_path": "mm/swap.c",
          "start_line": 1,
          "end_line": 76,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  linux/mm/swap.c",
            " *",
            " *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds",
            " */",
            "",
            "/*",
            " * This file contains the default values for the operation of the",
            " * Linux VM subsystem. Fine-tuning documentation can be found in",
            " * Documentation/admin-guide/sysctl/vm.rst.",
            " * Started 18.12.91",
            " * Swap aging added 23.2.95, Stephen Tweedie.",
            " * Buffermem limits added 12.3.98, Rik van Riel.",
            " */",
            "",
            "#include <linux/mm.h>",
            "#include <linux/sched.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/pagevec.h>",
            "#include <linux/init.h>",
            "#include <linux/export.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/percpu_counter.h>",
            "#include <linux/memremap.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/gfp.h>",
            "#include <linux/uio.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/page_idle.h>",
            "#include <linux/local_lock.h>",
            "#include <linux/buffer_head.h>",
            "",
            "#include \"internal.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/pagemap.h>",
            "",
            "/* How many pages do we try to swap or page in/out together? As a power of 2 */",
            "int page_cluster;",
            "const int page_cluster_max = 31;",
            "",
            "/* Protecting only lru_rotate.fbatch which requires disabling interrupts */",
            "struct lru_rotate {",
            "\tlocal_lock_t lock;",
            "\tstruct folio_batch fbatch;",
            "};",
            "static DEFINE_PER_CPU(struct lru_rotate, lru_rotate) = {",
            "\t.lock = INIT_LOCAL_LOCK(lock),",
            "};",
            "",
            "/*",
            " * The following folio batches are grouped together because they are protected",
            " * by disabling preemption (and interrupts remain enabled).",
            " */",
            "struct cpu_fbatches {",
            "\tlocal_lock_t lock;",
            "\tstruct folio_batch lru_add;",
            "\tstruct folio_batch lru_deactivate_file;",
            "\tstruct folio_batch lru_deactivate;",
            "\tstruct folio_batch lru_lazyfree;",
            "#ifdef CONFIG_SMP",
            "\tstruct folio_batch activate;",
            "#endif",
            "};",
            "static DEFINE_PER_CPU(struct cpu_fbatches, cpu_fbatches) = {",
            "\t.lock = INIT_LOCAL_LOCK(lock),",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义了与Linux虚拟内存子系统默认操作相关的全局变量和结构体，包括控制页面集群大小的page_cluster参数，以及用于保护LRU旋转操作的本地锁结构lru_rotate和CPU级的folio批次结构cpu_fbatches，用于管理不同场景下的页面批量操作。",
          "similarity": 0.5435782670974731
        }
      ]
    }
  ]
}