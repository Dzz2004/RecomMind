{
  "query": "interface design",
  "timestamp": "2025-12-26 02:00:18",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/membarrier.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:12:44\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\membarrier.c`\n\n---\n\n# `sched/membarrier.c` 技术文档\n\n## 1. 文件概述\n\n`sched/membarrier.c` 实现了 Linux 内核中的 `membarrier` 系统调用，该调用为用户空间程序提供了一种高效的全局内存屏障机制。与传统的在每个线程中显式插入内存屏障相比，`membarrier` 允许一个线程通过一次系统调用，强制所有运行在系统上的线程（或特定进程组内的线程）执行内存屏障操作，从而简化用户空间并发同步逻辑并提升性能。\n\n该文件的核心目标是在多核系统中确保内存操作的全局可见性顺序，尤其适用于需要跨线程强内存顺序保证的用户空间同步原语（如 RCU、无锁数据结构等）。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`ipi_mb(void *info)`**  \n  IPI（处理器间中断）处理函数，执行 `smp_mb()` 内存屏障，用于基础的全局内存屏障命令。\n\n- **`ipi_sync_core(void *info)`**  \n  用于 `MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE` 命令的 IPI 处理函数，在执行内存屏障后调用 `sync_core_before_usermode()`，确保 CPU 核心状态同步（如指令缓存一致性）。\n\n- **`ipi_rseq(void *info)`**  \n  用于 `MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ` 命令的 IPI 处理函数，在内存屏障后调用 `rseq_preempt(current)`，以支持 restartable sequences（rseq）机制的正确性。\n\n- **`ipi_sync_rq_state(void *info)`**  \n  用于同步 per-CPU runqueue 的 `membarrier_state` 字段，使其与指定 `mm_struct` 的状态一致，确保后续 `membarrier` 调用能正确识别注册状态。\n\n- **`membarrier_exec_mmap(struct mm_struct *mm)`**  \n  在进程执行 `exec` 系统调用时被调用，重置该内存描述符（`mm_struct`）的 `membarrier_state` 为 0，并同步 per-CPU runqueue 状态，防止 exec 后残留旧的注册状态。\n\n### 数据结构与宏\n\n- **`MEMBARRIER_CMD_BITMASK`**  \n  定义所有支持的 `membarrier` 命令的位掩码（不含 `QUERY`），用于命令合法性校验。\n\n- **`membarrier_ipi_mutex`**  \n  互斥锁，用于序列化 IPI 发送过程，防止多个 `membarrier` 调用并发执行导致 IPI 风暴或状态不一致。\n\n- **`SERIALIZE_IPI()`**  \n  宏封装，使用 `membarrier_ipi_mutex` 实现 IPI 发送的串行化。\n\n## 3. 关键实现\n\n### 内存屏障语义保证\n\n文件顶部的注释详细描述了五种关键内存顺序场景（A–E），说明为何在 `membarrier()` 调用前后必须插入 `smp_mb()`：\n\n- **场景 A**：确保调用者 CPU 在 `membarrier()` 之前的写操作，在其他 CPU 收到 IPI 并执行屏障后对其可见。\n- **场景 B**：确保其他 CPU 在 IPI 屏障前的写操作，在调用者 CPU 执行 `membarrier()` 后对其可见。\n- **场景 C–E**：处理线程切换、`exit_mm`、kthread 使用/释放 mm 等边界情况，确保 `membarrier` 能正确识别用户态上下文并施加屏障。\n\n这些场景共同要求 `membarrier()` 实现必须在发送 IPI **前**和**后**各执行一次 `smp_mb()`，以建立完整的全局内存顺序。\n\n### IPI 分发机制\n\n- 根据不同的 `membarrier` 命令类型（如全局、私有、带 rseq 或 sync_core），选择对应的 IPI 处理函数。\n- 使用 `mutex` 保护 IPI 发送过程，避免并发调用导致性能下降或状态竞争。\n- 对于私有命令（如 `PRIVATE_EXPEDITED`），仅向共享同一 `mm_struct` 的 CPU 发送 IPI。\n\n### 状态管理\n\n- 每个 `mm_struct` 包含一个 `membarrier_state` 原子变量，记录该地址空间已注册的 `membarrier` 命令类型。\n- 每个 per-CPU runqueue 也缓存一份 `membarrier_state`，通过 `ipi_sync_rq_state` 保持与 `mm_struct` 同步，加速后续命令的判断。\n- `exec` 时调用 `membarrier_exec_mmap` 重置状态，防止子进程继承父进程的注册状态。\n\n### 条件编译支持\n\n- `CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE`：启用 `SYNC_CORE` 相关命令。\n- `CONFIG_RSEQ`：启用 `RSEQ` 相关命令及 `rseq_preempt` 调用。\n\n## 4. 依赖关系\n\n- **调度子系统（sched）**：依赖 runqueue（`rq`）结构和 CPU 上下文切换逻辑，用于判断当前是否处于用户态及 mm 匹配。\n- **内存管理（mm）**：依赖 `mm_struct` 及其生命周期管理（如 `exec_mmap`、`exit_mm`）。\n- **RSEQ 子系统**：当启用 `CONFIG_RSEQ` 时，调用 `rseq_preempt()` 以维护 restartable sequences 的一致性。\n- **SMP 原语**：依赖 `smp_mb()`、`smp_call_function_many()` 等 SMP 内存屏障和 IPI 接口。\n- **架构支持**：部分命令（如 `SYNC_CORE`）依赖特定架构实现 `sync_core_before_usermode()`。\n\n## 5. 使用场景\n\n- **用户空间无锁编程**：应用程序使用 `membarrier(SYS_MEMBARRIER_CMD_GLOBAL_EXPEDITED)` 替代在每个读线程中插入 `smp_load_acquire()`，简化代码并提升性能。\n- **RSEQ（Restartable Sequences）**：配合 `membarrier(SYS_MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ)` 确保在抢占或迁移后 rseq 区域的原子性。\n- **实时或低延迟系统**：通过私有命令（`PRIVATE_EXPEDITED`）仅对特定进程组施加屏障，减少系统范围开销。\n- **动态代码生成/热更新**：使用 `SYNC_CORE` 命令确保指令缓存一致性，适用于 JIT 编译器等场景。\n- **进程生命周期管理**：在 `exec` 时自动清理 `membarrier` 注册状态，保证新程序映像的干净执行环境。",
      "similarity": 0.5058924555778503,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 168,
          "end_line": 306,
          "content": [
            "static void ipi_mb(void *info)",
            "{",
            "\tsmp_mb();\t/* IPIs should be serializing but paranoid. */",
            "}",
            "static void ipi_sync_core(void *info)",
            "{",
            "\t/*",
            "\t * The smp_mb() in membarrier after all the IPIs is supposed to",
            "\t * ensure that memory on remote CPUs that occur before the IPI",
            "\t * become visible to membarrier()'s caller -- see scenario B in",
            "\t * the big comment at the top of this file.",
            "\t *",
            "\t * A sync_core() would provide this guarantee, but",
            "\t * sync_core_before_usermode() might end up being deferred until",
            "\t * after membarrier()'s smp_mb().",
            "\t */",
            "\tsmp_mb();\t/* IPIs should be serializing but paranoid. */",
            "",
            "\tsync_core_before_usermode();",
            "}",
            "static void ipi_rseq(void *info)",
            "{",
            "\t/*",
            "\t * Ensure that all stores done by the calling thread are visible",
            "\t * to the current task before the current task resumes.  We could",
            "\t * probably optimize this away on most architectures, but by the",
            "\t * time we've already sent an IPI, the cost of the extra smp_mb()",
            "\t * is negligible.",
            "\t */",
            "\tsmp_mb();",
            "\trseq_preempt(current);",
            "}",
            "static void ipi_sync_rq_state(void *info)",
            "{",
            "\tstruct mm_struct *mm = (struct mm_struct *) info;",
            "",
            "\tif (current->mm != mm)",
            "\t\treturn;",
            "\tthis_cpu_write(runqueues.membarrier_state,",
            "\t\t       atomic_read(&mm->membarrier_state));",
            "\t/*",
            "\t * Issue a memory barrier after setting",
            "\t * MEMBARRIER_STATE_GLOBAL_EXPEDITED in the current runqueue to",
            "\t * guarantee that no memory access following registration is reordered",
            "\t * before registration.",
            "\t */",
            "\tsmp_mb();",
            "}",
            "void membarrier_exec_mmap(struct mm_struct *mm)",
            "{",
            "\t/*",
            "\t * Issue a memory barrier before clearing membarrier_state to",
            "\t * guarantee that no memory access prior to exec is reordered after",
            "\t * clearing this state.",
            "\t */",
            "\tsmp_mb();",
            "\tatomic_set(&mm->membarrier_state, 0);",
            "\t/*",
            "\t * Keep the runqueue membarrier_state in sync with this mm",
            "\t * membarrier_state.",
            "\t */",
            "\tthis_cpu_write(runqueues.membarrier_state, 0);",
            "}",
            "void membarrier_update_current_mm(struct mm_struct *next_mm)",
            "{",
            "\tstruct rq *rq = this_rq();",
            "\tint membarrier_state = 0;",
            "",
            "\tif (next_mm)",
            "\t\tmembarrier_state = atomic_read(&next_mm->membarrier_state);",
            "\tif (READ_ONCE(rq->membarrier_state) == membarrier_state)",
            "\t\treturn;",
            "\tWRITE_ONCE(rq->membarrier_state, membarrier_state);",
            "}",
            "static int membarrier_global_expedited(void)",
            "{",
            "\tint cpu;",
            "\tcpumask_var_t tmpmask;",
            "",
            "\tif (num_online_cpus() == 1)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Matches memory barriers after rq->curr modification in",
            "\t * scheduler.",
            "\t */",
            "\tsmp_mb();\t/* system call entry is not a mb. */",
            "",
            "\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "\trcu_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\t/*",
            "\t\t * Skipping the current CPU is OK even through we can be",
            "\t\t * migrated at any point. The current CPU, at the point",
            "\t\t * where we read raw_smp_processor_id(), is ensured to",
            "\t\t * be in program order with respect to the caller",
            "\t\t * thread. Therefore, we can skip this CPU from the",
            "\t\t * iteration.",
            "\t\t */",
            "\t\tif (cpu == raw_smp_processor_id())",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!(READ_ONCE(cpu_rq(cpu)->membarrier_state) &",
            "\t\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * Skip the CPU if it runs a kernel thread which is not using",
            "\t\t * a task mm.",
            "\t\t */",
            "\t\tp = rcu_dereference(cpu_rq(cpu)->curr);",
            "\t\tif (!p->mm)",
            "\t\t\tcontinue;",
            "",
            "\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tpreempt_disable();",
            "\tsmp_call_function_many(tmpmask, ipi_mb, NULL, 1);",
            "\tpreempt_enable();",
            "",
            "\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\t/*",
            "\t * Memory barrier on the caller thread _after_ we finished",
            "\t * waiting for the last IPI. Matches memory barriers before",
            "\t * rq->curr modification in scheduler.",
            "\t */",
            "\tsmp_mb();\t/* exit from system call is not a mb */",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ipi_mb, ipi_sync_core, ipi_rseq, ipi_sync_rq_state, membarrier_exec_mmap, membarrier_update_current_mm, membarrier_global_expedited",
          "description": "实现多种IPI处理函数及全局快速内存屏障逻辑，通过跨CPU调用来确保内存访问顺序一致性。",
          "similarity": 0.5241174101829529
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 436,
          "end_line": 551,
          "content": [
            "static int sync_runqueues_membarrier_state(struct mm_struct *mm)",
            "{",
            "\tint membarrier_state = atomic_read(&mm->membarrier_state);",
            "\tcpumask_var_t tmpmask;",
            "\tint cpu;",
            "",
            "\tif (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1) {",
            "\t\tthis_cpu_write(runqueues.membarrier_state, membarrier_state);",
            "",
            "\t\t/*",
            "\t\t * For single mm user, we can simply issue a memory barrier",
            "\t\t * after setting MEMBARRIER_STATE_GLOBAL_EXPEDITED in the",
            "\t\t * mm and in the current runqueue to guarantee that no memory",
            "\t\t * access following registration is reordered before",
            "\t\t * registration.",
            "\t\t */",
            "\t\tsmp_mb();",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * For mm with multiple users, we need to ensure all future",
            "\t * scheduler executions will observe @mm's new membarrier",
            "\t * state.",
            "\t */",
            "\tsynchronize_rcu();",
            "",
            "\t/*",
            "\t * For each cpu runqueue, if the task's mm match @mm, ensure that all",
            "\t * @mm's membarrier state set bits are also set in the runqueue's",
            "\t * membarrier state. This ensures that a runqueue scheduling",
            "\t * between threads which are users of @mm has its membarrier state",
            "\t * updated.",
            "\t */",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "\trcu_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct rq *rq = cpu_rq(cpu);",
            "\t\tstruct task_struct *p;",
            "",
            "\t\tp = rcu_dereference(rq->curr);",
            "\t\tif (p && p->mm == mm)",
            "\t\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\ton_each_cpu_mask(tmpmask, ipi_sync_rq_state, mm, true);",
            "",
            "\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\treturn 0;",
            "}",
            "static int membarrier_register_global_expedited(void)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint ret;",
            "",
            "\tif (atomic_read(&mm->membarrier_state) &",
            "\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)",
            "\t\treturn 0;",
            "\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);",
            "\tret = sync_runqueues_membarrier_state(mm);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,",
            "\t\t  &mm->membarrier_state);",
            "",
            "\treturn 0;",
            "}",
            "static int membarrier_register_private_expedited(int flags)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint ready_state = MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY,",
            "\t    set_state = MEMBARRIER_STATE_PRIVATE_EXPEDITED,",
            "\t    ret;",
            "",
            "\tif (flags == MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))",
            "\t\t\treturn -EINVAL;",
            "\t\tready_state =",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY;",
            "\t} else if (flags == MEMBARRIER_FLAG_RSEQ) {",
            "\t\tif (!IS_ENABLED(CONFIG_RSEQ))",
            "\t\t\treturn -EINVAL;",
            "\t\tready_state =",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY;",
            "\t} else {",
            "\t\tWARN_ON_ONCE(flags);",
            "\t}",
            "",
            "\t/*",
            "\t * We need to consider threads belonging to different thread",
            "\t * groups, which use the same mm. (CLONE_VM but not",
            "\t * CLONE_THREAD).",
            "\t */",
            "\tif ((atomic_read(&mm->membarrier_state) & ready_state) == ready_state)",
            "\t\treturn 0;",
            "\tif (flags & MEMBARRIER_FLAG_SYNC_CORE)",
            "\t\tset_state |= MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE;",
            "\tif (flags & MEMBARRIER_FLAG_RSEQ)",
            "\t\tset_state |= MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ;",
            "\tatomic_or(set_state, &mm->membarrier_state);",
            "\tret = sync_runqueues_membarrier_state(mm);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tatomic_or(ready_state, &mm->membarrier_state);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sync_runqueues_membarrier_state, membarrier_register_global_expedited, membarrier_register_private_expedited",
          "description": "同步运行队列内存屏障状态，通过RCU和IPI确保多用户场景下状态传播的正确性。",
          "similarity": 0.4921151399612427
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 314,
          "end_line": 434,
          "content": [
            "static int membarrier_private_expedited(int flags, int cpu_id)",
            "{",
            "\tcpumask_var_t tmpmask;",
            "\tstruct mm_struct *mm = current->mm;",
            "\tsmp_call_func_t ipi_func = ipi_mb;",
            "",
            "\tif (flags == MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY))",
            "\t\t\treturn -EPERM;",
            "\t\tipi_func = ipi_sync_core;",
            "\t\tprepare_sync_core_cmd(mm);",
            "\t} else if (flags == MEMBARRIER_FLAG_RSEQ) {",
            "\t\tif (!IS_ENABLED(CONFIG_RSEQ))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY))",
            "\t\t\treturn -EPERM;",
            "\t\tipi_func = ipi_rseq;",
            "\t} else {",
            "\t\tWARN_ON_ONCE(flags);",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY))",
            "\t\t\treturn -EPERM;",
            "\t}",
            "",
            "\tif (flags != MEMBARRIER_FLAG_SYNC_CORE &&",
            "\t    (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Matches memory barriers after rq->curr modification in",
            "\t * scheduler.",
            "\t *",
            "\t * On RISC-V, this barrier pairing is also needed for the",
            "\t * SYNC_CORE command when switching between processes, cf.",
            "\t * the inline comments in membarrier_arch_switch_mm().",
            "\t */",
            "\tsmp_mb();\t/* system call entry is not a mb. */",
            "",
            "\tif (cpu_id < 0 && !zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "",
            "\tif (cpu_id >= 0) {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\tif (cpu_id >= nr_cpu_ids || !cpu_online(cpu_id))",
            "\t\t\tgoto out;",
            "\t\trcu_read_lock();",
            "\t\tp = rcu_dereference(cpu_rq(cpu_id)->curr);",
            "\t\tif (!p || p->mm != mm) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tint cpu;",
            "",
            "\t\trcu_read_lock();",
            "\t\tfor_each_online_cpu(cpu) {",
            "\t\t\tstruct task_struct *p;",
            "",
            "\t\t\tp = rcu_dereference(cpu_rq(cpu)->curr);",
            "\t\t\tif (p && p->mm == mm)",
            "\t\t\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t}",
            "",
            "\tif (cpu_id >= 0) {",
            "\t\t/*",
            "\t\t * smp_call_function_single() will call ipi_func() if cpu_id",
            "\t\t * is the calling CPU.",
            "\t\t */",
            "\t\tsmp_call_function_single(cpu_id, ipi_func, NULL, 1);",
            "\t} else {",
            "\t\t/*",
            "\t\t * For regular membarrier, we can save a few cycles by",
            "\t\t * skipping the current cpu -- we're about to do smp_mb()",
            "\t\t * below, and if we migrate to a different cpu, this cpu",
            "\t\t * and the new cpu will execute a full barrier in the",
            "\t\t * scheduler.",
            "\t\t *",
            "\t\t * For SYNC_CORE, we do need a barrier on the current cpu --",
            "\t\t * otherwise, if we are migrated and replaced by a different",
            "\t\t * task in the same mm just before, during, or after",
            "\t\t * membarrier, we will end up with some thread in the mm",
            "\t\t * running without a core sync.",
            "\t\t *",
            "\t\t * For RSEQ, don't rseq_preempt() the caller.  User code",
            "\t\t * is not supposed to issue syscalls at all from inside an",
            "\t\t * rseq critical section.",
            "\t\t */",
            "\t\tif (flags != MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\t\tpreempt_disable();",
            "\t\t\tsmp_call_function_many(tmpmask, ipi_func, NULL, true);",
            "\t\t\tpreempt_enable();",
            "\t\t} else {",
            "\t\t\ton_each_cpu_mask(tmpmask, ipi_func, NULL, true);",
            "\t\t}",
            "\t}",
            "",
            "out:",
            "\tif (cpu_id < 0)",
            "\t\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\t/*",
            "\t * Memory barrier on the caller thread _after_ we finished",
            "\t * waiting for the last IPI. Matches memory barriers before",
            "\t * rq->curr modification in scheduler.",
            "\t */",
            "\tsmp_mb();\t/* exit from system call is not a mb */",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "membarrier_private_expedited",
          "description": "处理私有快速内存屏障，根据配置标志选择不同同步方式并验证状态有效性。",
          "similarity": 0.45438769459724426
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 1,
          "end_line": 167,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Copyright (C) 2010-2017 Mathieu Desnoyers <mathieu.desnoyers@efficios.com>",
            " *",
            " * membarrier system call",
            " */",
            "",
            "/*",
            " * For documentation purposes, here are some membarrier ordering",
            " * scenarios to keep in mind:",
            " *",
            " * A) Userspace thread execution after IPI vs membarrier's memory",
            " *    barrier before sending the IPI",
            " *",
            " * Userspace variables:",
            " *",
            " * int x = 0, y = 0;",
            " *",
            " * The memory barrier at the start of membarrier() on CPU0 is necessary in",
            " * order to enforce the guarantee that any writes occurring on CPU0 before",
            " * the membarrier() is executed will be visible to any code executing on",
            " * CPU1 after the IPI-induced memory barrier:",
            " *",
            " *         CPU0                              CPU1",
            " *",
            " *         x = 1",
            " *         membarrier():",
            " *           a: smp_mb()",
            " *           b: send IPI                       IPI-induced mb",
            " *           c: smp_mb()",
            " *         r2 = y",
            " *                                           y = 1",
            " *                                           barrier()",
            " *                                           r1 = x",
            " *",
            " *                     BUG_ON(r1 == 0 && r2 == 0)",
            " *",
            " * The write to y and load from x by CPU1 are unordered by the hardware,",
            " * so it's possible to have \"r1 = x\" reordered before \"y = 1\" at any",
            " * point after (b).  If the memory barrier at (a) is omitted, then \"x = 1\"",
            " * can be reordered after (a) (although not after (c)), so we get r1 == 0",
            " * and r2 == 0.  This violates the guarantee that membarrier() is",
            " * supposed by provide.",
            " *",
            " * The timing of the memory barrier at (a) has to ensure that it executes",
            " * before the IPI-induced memory barrier on CPU1.",
            " *",
            " * B) Userspace thread execution before IPI vs membarrier's memory",
            " *    barrier after completing the IPI",
            " *",
            " * Userspace variables:",
            " *",
            " * int x = 0, y = 0;",
            " *",
            " * The memory barrier at the end of membarrier() on CPU0 is necessary in",
            " * order to enforce the guarantee that any writes occurring on CPU1 before",
            " * the membarrier() is executed will be visible to any code executing on",
            " * CPU0 after the membarrier():",
            " *",
            " *         CPU0                              CPU1",
            " *",
            " *                                           x = 1",
            " *                                           barrier()",
            " *                                           y = 1",
            " *         r2 = y",
            " *         membarrier():",
            " *           a: smp_mb()",
            " *           b: send IPI                       IPI-induced mb",
            " *           c: smp_mb()",
            " *         r1 = x",
            " *         BUG_ON(r1 == 0 && r2 == 1)",
            " *",
            " * The writes to x and y are unordered by the hardware, so it's possible to",
            " * have \"r2 = 1\" even though the write to x doesn't execute until (b).  If",
            " * the memory barrier at (c) is omitted then \"r1 = x\" can be reordered",
            " * before (b) (although not before (a)), so we get \"r1 = 0\".  This violates",
            " * the guarantee that membarrier() is supposed to provide.",
            " *",
            " * The timing of the memory barrier at (c) has to ensure that it executes",
            " * after the IPI-induced memory barrier on CPU1.",
            " *",
            " * C) Scheduling userspace thread -> kthread -> userspace thread vs membarrier",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *           a: smp_mb()",
            " *                                           d: switch to kthread (includes mb)",
            " *           b: read rq->curr->mm == NULL",
            " *                                           e: switch to user (includes mb)",
            " *           c: smp_mb()",
            " *",
            " * Using the scenario from (A), we can show that (a) needs to be paired",
            " * with (e). Using the scenario from (B), we can show that (c) needs to",
            " * be paired with (d).",
            " *",
            " * D) exit_mm vs membarrier",
            " *",
            " * Two thread groups are created, A and B.  Thread group B is created by",
            " * issuing clone from group A with flag CLONE_VM set, but not CLONE_THREAD.",
            " * Let's assume we have a single thread within each thread group (Thread A",
            " * and Thread B).  Thread A runs on CPU0, Thread B runs on CPU1.",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *             a: smp_mb()",
            " *                                           exit_mm():",
            " *                                             d: smp_mb()",
            " *                                             e: current->mm = NULL",
            " *             b: read rq->curr->mm == NULL",
            " *             c: smp_mb()",
            " *",
            " * Using scenario (B), we can show that (c) needs to be paired with (d).",
            " *",
            " * E) kthread_{use,unuse}_mm vs membarrier",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *           a: smp_mb()",
            " *                                           kthread_unuse_mm()",
            " *                                             d: smp_mb()",
            " *                                             e: current->mm = NULL",
            " *           b: read rq->curr->mm == NULL",
            " *                                           kthread_use_mm()",
            " *                                             f: current->mm = mm",
            " *                                             g: smp_mb()",
            " *           c: smp_mb()",
            " *",
            " * Using the scenario from (A), we can show that (a) needs to be paired",
            " * with (g). Using the scenario from (B), we can show that (c) needs to",
            " * be paired with (d).",
            " */",
            "",
            "/*",
            " * Bitmask made from a \"or\" of all commands within enum membarrier_cmd,",
            " * except MEMBARRIER_CMD_QUERY.",
            " */",
            "#ifdef CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t\t\t\\",
            "\t(MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE)",
            "#else",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t0",
            "#endif",
            "",
            "#ifdef CONFIG_RSEQ",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t\t\\",
            "\t(MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ)",
            "#else",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t0",
            "#endif",
            "",
            "#define MEMBARRIER_CMD_BITMASK\t\t\t\t\t\t\\",
            "\t(MEMBARRIER_CMD_GLOBAL | MEMBARRIER_CMD_GLOBAL_EXPEDITED\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED\t\t\t\\",
            "\t| MEMBARRIER_CMD_PRIVATE_EXPEDITED\t\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED\t\t\t\\",
            "\t| MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t\t\\",
            "\t| MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t\t\t\\",
            "\t| MEMBARRIER_CMD_GET_REGISTRATIONS)",
            "",
            "static DEFINE_MUTEX(membarrier_ipi_mutex);",
            "#define SERIALIZE_IPI() guard(mutex)(&membarrier_ipi_mutex)",
            ""
          ],
          "function_name": null,
          "description": "定义内存屏障命令位掩码和互斥锁，用于协调多处理器间内存顺序保证。",
          "similarity": 0.4303252696990967
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 555,
          "end_line": 587,
          "content": [
            "static int membarrier_get_registrations(void)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint registrations_mask = 0, membarrier_state, i;",
            "\tstatic const int states[] = {",
            "\t\tMEMBARRIER_STATE_GLOBAL_EXPEDITED |",
            "\t\t\tMEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY",
            "\t};",
            "\tstatic const int registration_cmds[] = {",
            "\t\tMEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ",
            "\t};",
            "\tBUILD_BUG_ON(ARRAY_SIZE(states) != ARRAY_SIZE(registration_cmds));",
            "",
            "\tmembarrier_state = atomic_read(&mm->membarrier_state);",
            "\tfor (i = 0; i < ARRAY_SIZE(states); ++i) {",
            "\t\tif (membarrier_state & states[i]) {",
            "\t\t\tregistrations_mask |= registration_cmds[i];",
            "\t\t\tmembarrier_state &= ~states[i];",
            "\t\t}",
            "\t}",
            "\tWARN_ON_ONCE(membarrier_state != 0);",
            "\treturn registrations_mask;",
            "}"
          ],
          "function_name": "membarrier_get_registrations",
          "description": "解析当前进程的内存屏障注册状态，将有效状态转换为对应的注册命令掩码。",
          "similarity": 0.4119205176830292
        }
      ]
    },
    {
      "source_file": "kernel/uid16.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:43:42\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `uid16.c`\n\n---\n\n# uid16.c 技术文档\n\n## 文件概述\n\n`uid16.c` 是 Linux 内核中用于提供 16 位用户 ID（UID）和组 ID（GID）系统调用兼容性的封装层。该文件实现了旧式 16 位 UID/GID 系统调用（如 `chown16`、`setuid16` 等）到现代 32 位 UID/GID 内核接口的桥接，确保运行在 32 位 UID/GID 架构上的旧应用程序仍能在支持高 UID（high UID）的现代内核上正常工作。文件注释中提到“希望五年后能移除这些接口”，表明其为临时兼容性方案。\n\n## 核心功能\n\n### 主要系统调用函数\n\n- **文件所有权变更类**：\n  - `chown16`：变更文件所有者和所属组（16 位接口）\n  - `lchown16`：变更符号链接指向文件的所有者和组（16 位接口）\n  - `fchown16`：通过文件描述符变更文件所有者和组（16 位接口）\n\n- **用户 ID 设置类**：\n  - `setuid16` / `seteuid16`（通过 `setreuid16` 实现）：设置真实/有效 UID\n  - `setreuid16`：同时设置真实 UID 和有效 UID\n  - `setresuid16`：设置真实、有效和保存的 UID\n  - `setfsuid16`：设置文件系统 UID\n\n- **组 ID 设置类**：\n  - `setgid16` / `setegid16`（通过 `setregid16` 实现）：设置真实/有效 GID\n  - `setregid16`：同时设置真实 GID 和有效 GID\n  - `setresgid16`：设置真实、有效和保存的 GID\n  - `setfsgid16`：设置文件系统 GID\n\n- **查询类**：\n  - `getuid16` / `geteuid16`：获取当前真实/有效 UID（16 位返回）\n  - `getgid16` / `getegid16`：获取当前真实/有效 GID（16 位返回）\n  - `getresuid16` / `getresgid16`：获取三类 UID/GID（真实、有效、保存）\n  - `getgroups16`：获取当前进程的附加组列表（16 位格式）\n  - `setgroups16`：设置当前进程的附加组列表（16 位输入）\n\n### 辅助函数\n\n- `groups16_to_user`：将内核 `group_info` 中的 GID 转换为 16 位格式并复制到用户空间\n- `groups16_from_user`：从用户空间读取 16 位 GID 列表并转换为内核 `kgid_t` 格式\n\n### 关键宏与类型\n\n- `old_uid_t` / `old_gid_t`：定义为 16 位整数类型（通常为 `__u16`）\n- `low2highuid` / `low2highgid`：将 16 位 UID/GID 扩展为 32 位内核表示\n- `high2lowuid` / `high2lowgid`：将 32 位内核 UID/GID 截断为 16 位返回用户空间\n- `from_kuid_munged` / `from_kgid_munged`：在用户命名空间上下文中将内核 UID/GID 转换为用户可见值，并处理无效 ID\n\n## 关键实现\n\n### UID/GID 转换机制\n\n- 所有 16 位系统调用首先使用 `low2highuid()` 或 `low2highgid()` 将传入的 16 位值转换为内核使用的 32 位 `kuid_t`/`kgid_t` 类型。\n- 查询类调用（如 `getuid16`）则通过 `from_kuid_munged()` 将内核 UID 映射到当前用户命名空间的用户可见值，再用 `high2lowuid()` 截断为 16 位返回。\n- `from_kuid_munged()` 在 UID 超出 16 位范围（>65535）时会返回 `(uid_t) -1`，确保旧程序不会收到无法处理的大值。\n\n### 用户命名空间支持\n\n- 所有转换均通过 `current_user_ns()` 获取当前进程的用户命名空间，确保在容器或用户命名空间隔离环境中正确映射 UID/GID。\n- 例如：`from_kuid_munged(cred->user_ns, cred->uid)` 将内核 UID 转换为该命名空间下的用户视角 UID。\n\n### 组列表处理\n\n- `getgroups16` 和 `setgroups16` 通过辅助函数 `groups16_to_user`/`groups16_from_user` 实现 16 位与内核 `kgid_t` 数组的双向转换。\n- `setgroups16` 在设置前调用 `groups_sort()` 对组列表排序，符合内核对 `group_info` 的要求。\n\n### 错误处理与边界检查\n\n- `setgroups16` 检查 `gidsetsize` 是否超过 `NGROUPS_MAX`，防止内存溢出。\n- `getgroups16` 在 `gidsetsize` 小于实际组数时返回 `-EINVAL`，符合 POSIX 语义。\n- 所有用户空间访问均通过 `put_user`/`get_user` 进行，失败时返回 `-EFAULT`。\n\n## 依赖关系\n\n- **头文件依赖**：\n  - `<linux/cred.h>`：访问 `struct cred` 和 `current_cred()`\n  - `<linux/highuid.h>`：提供 `low2highuid`/`high2lowuid` 等转换宏\n  - `<linux/uaccess.h>`：提供 `put_user`/`get_user` 用户空间访问接口\n  - `<linux/syscalls.h>`：使用 `SYSCALL_DEFINE` 宏定义系统调用\n  - `<linux/security.h>`：间接依赖安全模块钩子（如 `may_setgroups`）\n\n- **内核子系统依赖**：\n  - **用户命名空间**（`user_namespace`）：所有 UID/GID 转换基于当前命名空间\n  - **凭证管理**（`cred`）：读取/修改进程的 UID/GID 属性\n  - **组管理**（`group_info`）：处理附加组列表的分配、排序和设置\n  - **系统调用框架**：通过 `ksys_*` 和 `__sys_*` 内核内部接口复用主逻辑\n\n## 使用场景\n\n- **旧版二进制兼容**：运行在早期 Linux 发行版（仅支持 16 位 UID/GID）上的应用程序在现代内核上执行时，自动通过这些 16 位系统调用接口与内核交互。\n- **嵌入式或资源受限系统**：某些遗留嵌入式系统可能仍使用 16 位 UID/GID 接口以节省空间或保持 ABI 兼容。\n- **用户命名空间容器环境**：在容器中运行的旧程序可通过这些接口正确映射容器内 UID/GID 到宿主机内核表示。\n- **系统调用拦截与审计**：安全模块可通过这些封装层监控或控制对 UID/GID 的变更操作。\n\n> 注：随着 16 位 UID/GID 的淘汰，这些接口在现代系统中已极少使用，主要用于维持向后兼容性。",
      "similarity": 0.5043378472328186,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/uid16.c",
          "start_line": 1,
          "end_line": 113,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " *\tWrapper functions for 16bit uid back compatibility. All nicely tied",
            " *\ttogether in the faint hope we can take the out in five years time.",
            " */",
            "",
            "#include <linux/mm.h>",
            "#include <linux/mman.h>",
            "#include <linux/notifier.h>",
            "#include <linux/reboot.h>",
            "#include <linux/prctl.h>",
            "#include <linux/capability.h>",
            "#include <linux/init.h>",
            "#include <linux/highuid.h>",
            "#include <linux/security.h>",
            "#include <linux/cred.h>",
            "#include <linux/syscalls.h>",
            "",
            "#include <linux/uaccess.h>",
            "",
            "#include \"uid16.h\"",
            "",
            "SYSCALL_DEFINE3(chown16, const char __user *, filename, old_uid_t, user, old_gid_t, group)",
            "{",
            "\treturn ksys_chown(filename, low2highuid(user), low2highgid(group));",
            "}",
            "",
            "SYSCALL_DEFINE3(lchown16, const char __user *, filename, old_uid_t, user, old_gid_t, group)",
            "{",
            "\treturn ksys_lchown(filename, low2highuid(user), low2highgid(group));",
            "}",
            "",
            "SYSCALL_DEFINE3(fchown16, unsigned int, fd, old_uid_t, user, old_gid_t, group)",
            "{",
            "\treturn ksys_fchown(fd, low2highuid(user), low2highgid(group));",
            "}",
            "",
            "SYSCALL_DEFINE2(setregid16, old_gid_t, rgid, old_gid_t, egid)",
            "{",
            "\treturn __sys_setregid(low2highgid(rgid), low2highgid(egid));",
            "}",
            "",
            "SYSCALL_DEFINE1(setgid16, old_gid_t, gid)",
            "{",
            "\treturn __sys_setgid(low2highgid(gid));",
            "}",
            "",
            "SYSCALL_DEFINE2(setreuid16, old_uid_t, ruid, old_uid_t, euid)",
            "{",
            "\treturn __sys_setreuid(low2highuid(ruid), low2highuid(euid));",
            "}",
            "",
            "SYSCALL_DEFINE1(setuid16, old_uid_t, uid)",
            "{",
            "\treturn __sys_setuid(low2highuid(uid));",
            "}",
            "",
            "SYSCALL_DEFINE3(setresuid16, old_uid_t, ruid, old_uid_t, euid, old_uid_t, suid)",
            "{",
            "\treturn __sys_setresuid(low2highuid(ruid), low2highuid(euid),",
            "\t\t\t\t low2highuid(suid));",
            "}",
            "",
            "SYSCALL_DEFINE3(getresuid16, old_uid_t __user *, ruidp, old_uid_t __user *, euidp, old_uid_t __user *, suidp)",
            "{",
            "\tconst struct cred *cred = current_cred();",
            "\tint retval;",
            "\told_uid_t ruid, euid, suid;",
            "",
            "\truid = high2lowuid(from_kuid_munged(cred->user_ns, cred->uid));",
            "\teuid = high2lowuid(from_kuid_munged(cred->user_ns, cred->euid));",
            "\tsuid = high2lowuid(from_kuid_munged(cred->user_ns, cred->suid));",
            "",
            "\tif (!(retval   = put_user(ruid, ruidp)) &&",
            "\t    !(retval   = put_user(euid, euidp)))",
            "\t\tretval = put_user(suid, suidp);",
            "",
            "\treturn retval;",
            "}",
            "",
            "SYSCALL_DEFINE3(setresgid16, old_gid_t, rgid, old_gid_t, egid, old_gid_t, sgid)",
            "{",
            "\treturn __sys_setresgid(low2highgid(rgid), low2highgid(egid),",
            "\t\t\t\t low2highgid(sgid));",
            "}",
            "",
            "SYSCALL_DEFINE3(getresgid16, old_gid_t __user *, rgidp, old_gid_t __user *, egidp, old_gid_t __user *, sgidp)",
            "{",
            "\tconst struct cred *cred = current_cred();",
            "\tint retval;",
            "\told_gid_t rgid, egid, sgid;",
            "",
            "\trgid = high2lowgid(from_kgid_munged(cred->user_ns, cred->gid));",
            "\tegid = high2lowgid(from_kgid_munged(cred->user_ns, cred->egid));",
            "\tsgid = high2lowgid(from_kgid_munged(cred->user_ns, cred->sgid));",
            "",
            "\tif (!(retval   = put_user(rgid, rgidp)) &&",
            "\t    !(retval   = put_user(egid, egidp)))",
            "\t\tretval = put_user(sgid, sgidp);",
            "",
            "\treturn retval;",
            "}",
            "",
            "SYSCALL_DEFINE1(setfsuid16, old_uid_t, uid)",
            "{",
            "\treturn __sys_setfsuid(low2highuid(uid));",
            "}",
            "",
            "SYSCALL_DEFINE1(setfsgid16, old_gid_t, gid)",
            "{",
            "\treturn __sys_setfsgid(low2highgid(gid));",
            "}",
            ""
          ],
          "function_name": null,
          "description": "此代码块实现了16位UID的兼容性包装函数，将old_uid_t类型的参数转换为高位UID（通过low2highuid）后调用相应的内核函数（如ksys_chown、__sys_setuid等），并提供从高位UID还原为低位UID的逻辑（通过high2lowuid），以维护对旧应用程序的兼容性。",
          "similarity": 0.4696919322013855
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/uid16.c",
          "start_line": 114,
          "end_line": 151,
          "content": [
            "static int groups16_to_user(old_gid_t __user *grouplist,",
            "    struct group_info *group_info)",
            "{",
            "\tstruct user_namespace *user_ns = current_user_ns();",
            "\tint i;",
            "\told_gid_t group;",
            "\tkgid_t kgid;",
            "",
            "\tfor (i = 0; i < group_info->ngroups; i++) {",
            "\t\tkgid = group_info->gid[i];",
            "\t\tgroup = high2lowgid(from_kgid_munged(user_ns, kgid));",
            "\t\tif (put_user(group, grouplist+i))",
            "\t\t\treturn -EFAULT;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int groups16_from_user(struct group_info *group_info,",
            "    old_gid_t __user *grouplist)",
            "{",
            "\tstruct user_namespace *user_ns = current_user_ns();",
            "\tint i;",
            "\told_gid_t group;",
            "\tkgid_t kgid;",
            "",
            "\tfor (i = 0; i < group_info->ngroups; i++) {",
            "\t\tif (get_user(group, grouplist+i))",
            "\t\t\treturn  -EFAULT;",
            "",
            "\t\tkgid = make_kgid(user_ns, low2highgid(group));",
            "\t\tif (!gid_valid(kgid))",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\tgroup_info->gid[i] = kgid;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "groups16_to_user, groups16_from_user",
          "description": "该代码块包含两个辅助函数，groups16_to_user将用户组列表中的旧16位GID转换为kgid类型并写入用户空间缓冲区，groups16_from_user则从用户空间读取旧GID转换为kgid并填充至group_info结构，确保用户组操作在旧GID与内核GID之间的双向转换。",
          "similarity": 0.43440890312194824
        }
      ]
    },
    {
      "source_file": "mm/page_idle.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:02:25\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_idle.c`\n\n---\n\n# page_idle.c 技术文档\n\n## 1. 文件概述\n\n`page_idle.c` 实现了 Linux 内核中的 **页面空闲（Page Idle）跟踪机制**，用于识别长时间未被访问的用户内存页。该机制通过 sysfs 接口 `/sys/kernel/mm/page_idle/bitmap` 提供位图形式的读写接口，允许用户空间工具（如 `page-types` 或内存优化器）查询或标记页面为空闲状态，从而辅助内存回收、迁移或性能分析。\n\n该模块仅跟踪 **用户态内存页**（即位于 LRU 链表上的页），对内核页、隔离页等非用户页的操作会被忽略。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `page_idle_get_folio(unsigned long pfn)` | 根据物理页帧号（PFN）获取对应的 folio，并验证其是否为有效的用户内存页（在 LRU 上且可引用） |\n| `page_idle_clear_pte_refs_one(...)` | 遍历 folio 的所有 PTE/PMD 映射，清除年轻（young）位；若发现被引用，则清除 idle 标志并设置 young 标志 |\n| `page_idle_clear_pte_refs(struct folio *folio)` | 对 folio 执行 rmap 遍历，调用 `page_idle_clear_pte_refs_one` 清除所有映射中的引用位 |\n| `page_idle_bitmap_read(...)` | sysfs 位图读接口：按 PFN 范围输出每个页的 idle 状态（1 表示空闲） |\n| `page_idle_bitmap_write(...)` | sysfs 位图写接口：根据输入位图将对应页标记为 idle（需先清除潜在引用） |\n\n### 关键数据结构\n\n- `page_idle_bitmap_attr`：定义 sysfs 二进制属性 `bitmap`，权限为 `0600`（仅 root 可读写）\n- `page_idle_attr_group`：sysfs 属性组，挂载到 `/sys/kernel/mm/page_idle/`\n\n### 宏定义\n\n- `BITMAP_CHUNK_SIZE = sizeof(u64)`：位图操作的基本单位（8 字节）\n- `BITMAP_CHUNK_BITS = 64`：每个 chunk 包含 64 个 bit，对应 64 个 PFN\n\n## 3. 关键实现\n\n### 用户页识别策略\n- 仅处理 **在 LRU 链表上** 的 folio，确保可通过 `rmap_walk()` 安全遍历其虚拟映射。\n- 排除 `PageTail`（透明大页的子页）、离线页等非标准用户页。\n- 使用 `folio_try_get()` 增加引用计数，防止并发释放。\n\n### Idle 状态判定与更新\n- **读操作**：对每个页，先检查 `folio_test_idle()`；若为 true，则调用 `page_idle_clear_pte_refs()` 清除所有 PTE/PMD 中的 `young` 位。若清除后仍无引用，才确认为空闲。\n- **写操作**：对位图中置 1 的位，获取对应 folio 后：\n  1. 调用 `page_idle_clear_pte_refs()` 清除现有引用；\n  2. 调用 `folio_set_idle()` 标记为 idle。\n- 此设计确保 **idle 标志仅在页确实未被 CPU 访问时有效**，避免因 TLB 或硬件预取导致误判。\n\n### 大页（THP）支持\n- 在 `page_idle_clear_pte_refs_one()` 中区分 PTE 和 PMD 映射：\n  - 对 THP（PMD 映射），调用 `pmdp_clear_young_notify()`；\n  - 若任一子页被引用，则整个 THP 被视为非空闲。\n- 通过 `ptep_clear_young_notify()` / `pmdp_clear_young_notify()` 触发 mmu_notifier 通知，保证一致性。\n\n### 并发与锁机制\n- 对匿名页（非 KSM）无需加 folio 锁；\n- 对文件页或 KSM 页，尝试 `folio_trylock()`，失败则跳过（避免死锁）；\n- 使用 `cond_resched()` 防止长时间关中断。\n\n### Sysfs 接口设计\n- 位图以 **u64 数组** 形式读写，每 bit 对应一个 PFN；\n- 读写偏移和长度必须对齐 `BITMAP_CHUNK_SIZE`（8 字节）；\n- 超出 `max_pfn` 的访问返回 0（读）或 `-ENXIO`（写）。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- `<linux/page_ext.h>`：提供扩展页标志（如 idle 标志的存储）\n- `<linux/rmap.h>`：反向映射（rmap）遍历支持\n- `<linux/mm.h>`, `<linux/mmzone.h>`：内存管理核心接口\n- `<linux/sysfs.h>`, `<linux/kobject.h>`：sysfs 注册\n- `\"internal.h\"`：内核 MM 内部头文件\n\n### 功能依赖\n- **LRU 框架**：依赖 `folio_test_lru()` 判断用户页\n- **反向映射（rmap）**：依赖 `rmap_walk()` 遍历页的所有 VMA 映射\n- **透明大页（THP）**：条件编译支持 PMD 映射处理\n- **MMU Notifier**：通过 `*_notify` 函数通知外部组件（如 KVM）\n\n### 挂载点\n- 注册到 `mm_kobj`（`/sys/kernel/mm/`），创建子目录 `page_idle`\n\n## 5. 使用场景\n\n1. **内存优化工具**  \n   用户空间程序（如 `madvise(MADV_FREE)` 监控工具）通过读取 `bitmap` 识别长期未使用的页，进行压缩、迁移或释放。\n\n2. **工作集分析**  \n   结合定期快照，分析应用的内存访问模式，识别“冷”内存区域。\n\n3. **虚拟机内存气球（Balloon）驱动**  \n   Guest OS 可将 idle 页通知给 Host，Host 回收这些页以提高内存利用率。\n\n4. **NUMA 迁移决策**  \n   将 idle 页从繁忙 NUMA 节点迁移到空闲节点，减少远程访问。\n\n5. **调试与监控**  \n   开发者可通过写入 `bitmap` 强制标记页为空闲，测试 reclaim 或 compaction 行为。\n\n> **注意**：该接口仅反映 **软件层面的访问状态**，不保证硬件缓存或 TLB 中无残留引用。实际使用需结合 `madvise()` 或 `userfaultfd` 等机制协同工作。",
      "similarity": 0.5015570521354675,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/page_idle.c",
          "start_line": 159,
          "end_line": 205,
          "content": [
            "static ssize_t page_idle_bitmap_write(struct file *file, struct kobject *kobj,",
            "\t\t\t\t      struct bin_attribute *attr, char *buf,",
            "\t\t\t\t      loff_t pos, size_t count)",
            "{",
            "\tconst u64 *in = (u64 *)buf;",
            "\tstruct folio *folio;",
            "\tunsigned long pfn, end_pfn;",
            "\tint bit;",
            "",
            "\tif (pos % BITMAP_CHUNK_SIZE || count % BITMAP_CHUNK_SIZE)",
            "\t\treturn -EINVAL;",
            "",
            "\tpfn = pos * BITS_PER_BYTE;",
            "\tif (pfn >= max_pfn)",
            "\t\treturn -ENXIO;",
            "",
            "\tend_pfn = pfn + count * BITS_PER_BYTE;",
            "\tif (end_pfn > max_pfn)",
            "\t\tend_pfn = max_pfn;",
            "",
            "\tfor (; pfn < end_pfn; pfn++) {",
            "\t\tbit = pfn % BITMAP_CHUNK_BITS;",
            "\t\tif ((*in >> bit) & 1) {",
            "\t\t\tfolio = page_idle_get_folio(pfn);",
            "\t\t\tif (folio) {",
            "\t\t\t\tpage_idle_clear_pte_refs(folio);",
            "\t\t\t\tfolio_set_idle(folio);",
            "\t\t\t\tfolio_put(folio);",
            "\t\t\t}",
            "\t\t}",
            "\t\tif (bit == BITMAP_CHUNK_BITS - 1)",
            "\t\t\tin++;",
            "\t\tcond_resched();",
            "\t}",
            "\treturn (char *)in - buf;",
            "}",
            "static int __init page_idle_init(void)",
            "{",
            "\tint err;",
            "",
            "\terr = sysfs_create_group(mm_kobj, &page_idle_attr_group);",
            "\tif (err) {",
            "\t\tpr_err(\"page_idle: register sysfs failed\\n\");",
            "\t\treturn err;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "page_idle_bitmap_write, page_idle_init",
          "description": "page_idle_bitmap_write根据输入数据设置页面空闲状态，page_idle_init注册sysfs属性组创建系统接口，用于暴露page_idle位图的读写操作，实现用户空间对空闲页面状态的监控与修改。",
          "similarity": 0.48991942405700684
        },
        {
          "chunk_id": 1,
          "file_path": "mm/page_idle.c",
          "start_line": 52,
          "end_line": 155,
          "content": [
            "static bool page_idle_clear_pte_refs_one(struct folio *folio,",
            "\t\t\t\t\tstruct vm_area_struct *vma,",
            "\t\t\t\t\tunsigned long addr, void *arg)",
            "{",
            "\tDEFINE_FOLIO_VMA_WALK(pvmw, folio, vma, addr, 0);",
            "\tbool referenced = false;",
            "",
            "\twhile (page_vma_mapped_walk(&pvmw)) {",
            "\t\taddr = pvmw.address;",
            "\t\tif (pvmw.pte) {",
            "\t\t\t/*",
            "\t\t\t * For PTE-mapped THP, one sub page is referenced,",
            "\t\t\t * the whole THP is referenced.",
            "\t\t\t */",
            "\t\t\tif (ptep_clear_young_notify(vma, addr, pvmw.pte))",
            "\t\t\t\treferenced = true;",
            "\t\t} else if (IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE)) {",
            "\t\t\tif (pmdp_clear_young_notify(vma, addr, pvmw.pmd))",
            "\t\t\t\treferenced = true;",
            "\t\t} else {",
            "\t\t\t/* unexpected pmd-mapped page? */",
            "\t\t\tWARN_ON_ONCE(1);",
            "\t\t}",
            "\t}",
            "",
            "\tif (referenced) {",
            "\t\tfolio_clear_idle(folio);",
            "\t\t/*",
            "\t\t * We cleared the referenced bit in a mapping to this page. To",
            "\t\t * avoid interference with page reclaim, mark it young so that",
            "\t\t * folio_referenced() will return > 0.",
            "\t\t */",
            "\t\tfolio_set_young(folio);",
            "\t}",
            "\treturn true;",
            "}",
            "static void page_idle_clear_pte_refs(struct folio *folio)",
            "{",
            "\t/*",
            "\t * Since rwc.try_lock is unused, rwc is effectively immutable, so we",
            "\t * can make it static to save some cycles and stack.",
            "\t */",
            "\tstatic struct rmap_walk_control rwc = {",
            "\t\t.rmap_one = page_idle_clear_pte_refs_one,",
            "\t\t.anon_lock = folio_lock_anon_vma_read,",
            "\t};",
            "\tbool need_lock;",
            "",
            "\tif (!folio_mapped(folio) || !folio_raw_mapping(folio))",
            "\t\treturn;",
            "",
            "\tneed_lock = !folio_test_anon(folio) || folio_test_ksm(folio);",
            "\tif (need_lock && !folio_trylock(folio))",
            "\t\treturn;",
            "",
            "\trmap_walk(folio, &rwc);",
            "",
            "\tif (need_lock)",
            "\t\tfolio_unlock(folio);",
            "}",
            "static ssize_t page_idle_bitmap_read(struct file *file, struct kobject *kobj,",
            "\t\t\t\t     struct bin_attribute *attr, char *buf,",
            "\t\t\t\t     loff_t pos, size_t count)",
            "{",
            "\tu64 *out = (u64 *)buf;",
            "\tstruct folio *folio;",
            "\tunsigned long pfn, end_pfn;",
            "\tint bit;",
            "",
            "\tif (pos % BITMAP_CHUNK_SIZE || count % BITMAP_CHUNK_SIZE)",
            "\t\treturn -EINVAL;",
            "",
            "\tpfn = pos * BITS_PER_BYTE;",
            "\tif (pfn >= max_pfn)",
            "\t\treturn 0;",
            "",
            "\tend_pfn = pfn + count * BITS_PER_BYTE;",
            "\tif (end_pfn > max_pfn)",
            "\t\tend_pfn = max_pfn;",
            "",
            "\tfor (; pfn < end_pfn; pfn++) {",
            "\t\tbit = pfn % BITMAP_CHUNK_BITS;",
            "\t\tif (!bit)",
            "\t\t\t*out = 0ULL;",
            "\t\tfolio = page_idle_get_folio(pfn);",
            "\t\tif (folio) {",
            "\t\t\tif (folio_test_idle(folio)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * The page might have been referenced via a",
            "\t\t\t\t * pte, in which case it is not idle. Clear",
            "\t\t\t\t * refs and recheck.",
            "\t\t\t\t */",
            "\t\t\t\tpage_idle_clear_pte_refs(folio);",
            "\t\t\t\tif (folio_test_idle(folio))",
            "\t\t\t\t\t*out |= 1ULL << bit;",
            "\t\t\t}",
            "\t\t\tfolio_put(folio);",
            "\t\t}",
            "\t\tif (bit == BITMAP_CHUNK_BITS - 1)",
            "\t\t\tout++;",
            "\t\tcond_resched();",
            "\t}",
            "\treturn (char *)out - buf;",
            "}"
          ],
          "function_name": "page_idle_clear_pte_refs_one, page_idle_clear_pte_refs, page_idle_bitmap_read",
          "description": "包含三个函数，page_idle_clear_pte_refs_one遍历页表清除引用标记，page_idle_clear_pte_refs协调锁机制执行清除操作，page_idle_bitmap_read读取位图数据时检查页面空闲状态并清除引用标记，通过rmap_walk遍历页表实现。",
          "similarity": 0.416218101978302
        },
        {
          "chunk_id": 0,
          "file_path": "mm/page_idle.c",
          "start_line": 1,
          "end_line": 51,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/init.h>",
            "#include <linux/memblock.h>",
            "#include <linux/fs.h>",
            "#include <linux/sysfs.h>",
            "#include <linux/kobject.h>",
            "#include <linux/memory_hotplug.h>",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/rmap.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/page_ext.h>",
            "#include <linux/page_idle.h>",
            "",
            "#include \"internal.h\"",
            "",
            "#define BITMAP_CHUNK_SIZE\tsizeof(u64)",
            "#define BITMAP_CHUNK_BITS\t(BITMAP_CHUNK_SIZE * BITS_PER_BYTE)",
            "",
            "/*",
            " * Idle page tracking only considers user memory pages, for other types of",
            " * pages the idle flag is always unset and an attempt to set it is silently",
            " * ignored.",
            " *",
            " * We treat a page as a user memory page if it is on an LRU list, because it is",
            " * always safe to pass such a page to rmap_walk(), which is essential for idle",
            " * page tracking. With such an indicator of user pages we can skip isolated",
            " * pages, but since there are not usually many of them, it will hardly affect",
            " * the overall result.",
            " *",
            " * This function tries to get a user memory page by pfn as described above.",
            " */",
            "static struct folio *page_idle_get_folio(unsigned long pfn)",
            "{",
            "\tstruct page *page = pfn_to_online_page(pfn);",
            "\tstruct folio *folio;",
            "",
            "\tif (!page || PageTail(page))",
            "\t\treturn NULL;",
            "",
            "\tfolio = page_folio(page);",
            "\tif (!folio_test_lru(folio) || !folio_try_get(folio))",
            "\t\treturn NULL;",
            "\tif (unlikely(page_folio(page) != folio || !folio_test_lru(folio))) {",
            "\t\tfolio_put(folio);",
            "\t\tfolio = NULL;",
            "\t}",
            "\treturn folio;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "该代码定义了page_idle_get_folio函数，用于根据物理帧号(PFN)获取对应的用户内存folio。函数会检查页面是否在线、是否为尾页，并验证folio是否处于LRU列表，最终返回符合条件的folio或NULL。",
          "similarity": 0.4086351990699768
        }
      ]
    }
  ]
}