{
  "query": "hard real-time system characteristics",
  "timestamp": "2025-12-25 23:55:31",
  "retrieved_files": [
    {
      "source_file": "kernel/time/hrtimer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:38:33\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `time\\hrtimer.c`\n\n---\n\n# `time/hrtimer.c` 技术文档\n\n## 1. 文件概述\n\n`time/hrtimer.c` 是 Linux 内核中高分辨率定时器（High-Resolution Timer, hrtimer）的核心实现文件。该模块提供了比传统低分辨率定时器（基于 timer wheel）更高精度和更准确的定时能力，适用于需要纳秒级精度的场景，如 POSIX 定时器、高精度睡眠、实时调度等。hrtimer 的精度依赖于底层硬件时钟事件设备（clockevent）的能力，并支持动态切换高/低分辨率模式。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct hrtimer_cpu_base`**  \n  每个 CPU 私有的 hrtimer 基础结构，包含多个时钟基准（clock_base）和全局锁，用于管理该 CPU 上所有 hrtimer 实例。\n\n- **`struct hrtimer_clock_base`**  \n  表示一个特定时钟源（如 `CLOCK_MONOTONIC`、`CLOCK_REALTIME` 等）的定时器基准，包含红黑树（rbtree）用于高效管理到期事件。\n\n- **`hrtimer_bases`（per-CPU 变量）**  \n  定义了每个 CPU 的 hrtimer 基础结构，初始化了 8 个 clock_base（4 个硬中断上下文 + 4 个软中断上下文），分别对应不同的 POSIX 时钟 ID。\n\n- **`migration_cpu_base`**  \n  SMP 系统中用于定时器迁移的临时占位结构，确保在跨 CPU 迁移过程中定时器状态的一致性。\n\n### 主要函数/宏\n\n- **`lock_hrtimer_base()`**  \n  安全获取定时器所属 clock_base 的自旋锁，处理 SMP 环境下定时器可能正在迁移的情况。\n\n- **`switch_hrtimer_base()`**  \n  将定时器迁移到目标 CPU 的对应 clock_base，用于负载均衡或 NO_HZ（动态滴答）优化。\n\n- **`get_target_base()`**  \n  根据当前 CPU 状态、NO_HZ 配置和 `pinned` 标志，选择最优的目标 CPU 作为定时器归属。\n\n- **`hrtimer_suitable_target()`**  \n  判断目标 CPU 是否适合作为定时器的新归属，考虑其下一次事件时间和 CPU 在线状态。\n\n- **`hrtimer_clock_to_base_table[]`**  \n  将 POSIX 时钟 ID（如 `CLOCK_MONOTONIC`）映射到内核内部的 `hrtimer_base_type` 枚举值。\n\n- **`HRTIMER_ACTIVE_HARD / SOFT / ALL`**  \n  位掩码宏，用于区分硬中断上下文和软中断上下文的活跃定时器集合。\n\n## 3. 关键实现\n\n### 高分辨率定时器基础架构\n\n- 每个 CPU 拥有独立的 `hrtimer_cpu_base`，包含多个 `clock_base`，每个对应一个 POSIX 时钟源。\n- 定时器按到期时间组织在红黑树中，确保 O(log n) 的插入/删除/查找性能。\n- 支持硬中断（hardirq）和软中断（softirq）两种上下文的定时器，通过 `_SOFT` 后缀的 base 区分。\n\n### 定时器迁移机制（SMP）\n\n- 在 SMP 系统中，为支持 NO_HZ 和负载均衡，hrtimer 可动态迁移到其他 CPU。\n- 使用 `migration_base` 作为迁移过程中的临时占位符，避免在迁移过程中出现空悬指针。\n- `lock_hrtimer_base()` 采用乐观重试机制：先读取 `timer->base`，加锁后再次验证，若不一致则重试。\n\n### 目标 CPU 选择策略\n\n- 若当前 CPU 离线，则选择 `housekeeping_cpumask` 中任意在线 CPU。\n- 若启用 `timers_migration_enabled` 且未设置 `pinned`，则使用 `get_nohz_timer_target()` 选择节能目标 CPU。\n- 通过 `hrtimer_suitable_target()` 避免将即将到期的定时器迁移到远端 CPU，防止因 IPI 延迟错过截止时间。\n\n### 锁与并发控制\n\n- 每个 `hrtimer_cpu_base` 使用 raw spinlock 保护其所有 `clock_base`。\n- 定时器操作（如 enqueue/dequeue）必须在持有对应 base 锁的情况下进行。\n- 迁移过程中通过原子读写 `timer->base` 和锁验证保证内存一致性。\n\n## 4. 依赖关系\n\n- **硬件抽象层**：依赖 `clocksource` 和 `clockevent` 子系统提供高精度时间源和事件触发能力。\n- **调度子系统**：与 `sched/` 目录下的实时调度（`rt.c`）、截止时间调度（`deadline.c`）和 NO_HZ（`nohz.c`）紧密集成。\n- **中断子系统**：通过 `tick-internal.h` 与 tick 管理模块交互，控制周期性滴答的启停。\n- **调试与追踪**：集成 `debugobjects` 用于对象生命周期检查，`trace/events/timer.h` 提供 ftrace 事件。\n- **系统调用**：为 `sys_nanosleep`、`timer_create` 等 POSIX 接口提供底层支持。\n- **CPU 热插拔**：通过 `CONFIG_HOTPLUG_CPU` 支持 CPU 在线/离线时的定时器迁移。\n\n## 5. 使用场景\n\n- **高精度睡眠**：`nanosleep()`、`clock_nanosleep()` 等系统调用依赖 hrtimer 实现纳秒级睡眠。\n- **POSIX 定时器**：用户空间通过 `timer_create()` 创建的定时器由 hrtimer 驱动。\n- **内核定时任务**：如 RCU 宽限期检测、网络协议栈超时、块设备 I/O 超时等需要高精度定时的子系统。\n- **实时系统**：配合 `SCHED_FIFO`/`SCHED_RR` 或 `SCHED_DEADLINE` 调度策略，提供确定性定时行为。\n- **动态滴答（NO_HZ）**：在空闲 CPU 上停止周期性 tick，仅靠 hrtimer 触发下一次事件，降低功耗。\n- **定时器迁移**：在多核系统中将定时器集中到少数 CPU，使其他 CPU 进入深度睡眠状态，提升能效。",
      "similarity": 0.5693345069885254,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/time/hrtimer.c",
          "start_line": 777,
          "end_line": 928,
          "content": [
            "static inline int hrtimer_is_hres_enabled(void) { return 0; }",
            "static inline void hrtimer_switch_to_hres(void) { }",
            "static void retrigger_next_event(void *arg)",
            "{",
            "\tstruct hrtimer_cpu_base *base = this_cpu_ptr(&hrtimer_bases);",
            "",
            "\t/*",
            "\t * When high resolution mode or nohz is active, then the offsets of",
            "\t * CLOCK_REALTIME/TAI/BOOTTIME have to be updated. Otherwise the",
            "\t * next tick will take care of that.",
            "\t *",
            "\t * If high resolution mode is active then the next expiring timer",
            "\t * must be reevaluated and the clock event device reprogrammed if",
            "\t * necessary.",
            "\t *",
            "\t * In the NOHZ case the update of the offset and the reevaluation",
            "\t * of the next expiring timer is enough. The return from the SMP",
            "\t * function call will take care of the reprogramming in case the",
            "\t * CPU was in a NOHZ idle sleep.",
            "\t *",
            "\t * In periodic low resolution mode, the next softirq expiration",
            "\t * must also be updated.",
            "\t */",
            "\traw_spin_lock(&base->lock);",
            "\thrtimer_update_base(base);",
            "\tif (hrtimer_hres_active(base))",
            "\t\thrtimer_force_reprogram(base, 0);",
            "\telse",
            "\t\thrtimer_update_next_event(base);",
            "\traw_spin_unlock(&base->lock);",
            "}",
            "static void hrtimer_reprogram(struct hrtimer *timer, bool reprogram)",
            "{",
            "\tstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);",
            "\tstruct hrtimer_clock_base *base = timer->base;",
            "\tktime_t expires = ktime_sub(hrtimer_get_expires(timer), base->offset);",
            "",
            "\tWARN_ON_ONCE(hrtimer_get_expires_tv64(timer) < 0);",
            "",
            "\t/*",
            "\t * CLOCK_REALTIME timer might be requested with an absolute",
            "\t * expiry time which is less than base->offset. Set it to 0.",
            "\t */",
            "\tif (expires < 0)",
            "\t\texpires = 0;",
            "",
            "\tif (timer->is_soft) {",
            "\t\t/*",
            "\t\t * soft hrtimer could be started on a remote CPU. In this",
            "\t\t * case softirq_expires_next needs to be updated on the",
            "\t\t * remote CPU. The soft hrtimer will not expire before the",
            "\t\t * first hard hrtimer on the remote CPU -",
            "\t\t * hrtimer_check_target() prevents this case.",
            "\t\t */",
            "\t\tstruct hrtimer_cpu_base *timer_cpu_base = base->cpu_base;",
            "",
            "\t\tif (timer_cpu_base->softirq_activated)",
            "\t\t\treturn;",
            "",
            "\t\tif (!ktime_before(expires, timer_cpu_base->softirq_expires_next))",
            "\t\t\treturn;",
            "",
            "\t\ttimer_cpu_base->softirq_next_timer = timer;",
            "\t\ttimer_cpu_base->softirq_expires_next = expires;",
            "",
            "\t\tif (!ktime_before(expires, timer_cpu_base->expires_next) ||",
            "\t\t    !reprogram)",
            "\t\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * If the timer is not on the current cpu, we cannot reprogram",
            "\t * the other cpus clock event device.",
            "\t */",
            "\tif (base->cpu_base != cpu_base)",
            "\t\treturn;",
            "",
            "\tif (expires >= cpu_base->expires_next)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If the hrtimer interrupt is running, then it will reevaluate the",
            "\t * clock bases and reprogram the clock event device.",
            "\t */",
            "\tif (cpu_base->in_hrtirq)",
            "\t\treturn;",
            "",
            "\tcpu_base->next_timer = timer;",
            "",
            "\t__hrtimer_reprogram(cpu_base, timer, expires);",
            "}",
            "static bool update_needs_ipi(struct hrtimer_cpu_base *cpu_base,",
            "\t\t\t     unsigned int active)",
            "{",
            "\tstruct hrtimer_clock_base *base;",
            "\tunsigned int seq;",
            "\tktime_t expires;",
            "",
            "\t/*",
            "\t * Update the base offsets unconditionally so the following",
            "\t * checks whether the SMP function call is required works.",
            "\t *",
            "\t * The update is safe even when the remote CPU is in the hrtimer",
            "\t * interrupt or the hrtimer soft interrupt and expiring affected",
            "\t * bases. Either it will see the update before handling a base or",
            "\t * it will see it when it finishes the processing and reevaluates",
            "\t * the next expiring timer.",
            "\t */",
            "\tseq = cpu_base->clock_was_set_seq;",
            "\thrtimer_update_base(cpu_base);",
            "",
            "\t/*",
            "\t * If the sequence did not change over the update then the",
            "\t * remote CPU already handled it.",
            "\t */",
            "\tif (seq == cpu_base->clock_was_set_seq)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * If the remote CPU is currently handling an hrtimer interrupt, it",
            "\t * will reevaluate the first expiring timer of all clock bases",
            "\t * before reprogramming. Nothing to do here.",
            "\t */",
            "\tif (cpu_base->in_hrtirq)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Walk the affected clock bases and check whether the first expiring",
            "\t * timer in a clock base is moving ahead of the first expiring timer of",
            "\t * @cpu_base. If so, the IPI must be invoked because per CPU clock",
            "\t * event devices cannot be remotely reprogrammed.",
            "\t */",
            "\tactive &= cpu_base->active_bases;",
            "",
            "\tfor_each_active_base(base, cpu_base, active) {",
            "\t\tstruct timerqueue_node *next;",
            "",
            "\t\tnext = timerqueue_getnext(&base->active);",
            "\t\texpires = ktime_sub(next->expires, base->offset);",
            "\t\tif (expires < cpu_base->expires_next)",
            "\t\t\treturn true;",
            "",
            "\t\t/* Extra check for softirq clock bases */",
            "\t\tif (base->clockid < HRTIMER_BASE_MONOTONIC_SOFT)",
            "\t\t\tcontinue;",
            "\t\tif (cpu_base->softirq_activated)",
            "\t\t\tcontinue;",
            "\t\tif (expires < cpu_base->softirq_expires_next)",
            "\t\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}"
          ],
          "function_name": "hrtimer_is_hres_enabled, hrtimer_switch_to_hres, retrigger_next_event, hrtimer_reprogram, update_needs_ipi",
          "description": "实现高精度模式状态查询(hrtimer_is_hres_enabled)，通过retrigger_next_event触发事件重评估，hrtimer_reprogram执行定时器重编程，update_needs_ipi判断是否需要跨CPU中断同步。",
          "similarity": 0.5343252420425415
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/time/hrtimer.c",
          "start_line": 624,
          "end_line": 737,
          "content": [
            "static ktime_t hrtimer_update_next_event(struct hrtimer_cpu_base *cpu_base)",
            "{",
            "\tktime_t expires_next, soft = KTIME_MAX;",
            "",
            "\t/*",
            "\t * If the soft interrupt has already been activated, ignore the",
            "\t * soft bases. They will be handled in the already raised soft",
            "\t * interrupt.",
            "\t */",
            "\tif (!cpu_base->softirq_activated) {",
            "\t\tsoft = __hrtimer_get_next_event(cpu_base, HRTIMER_ACTIVE_SOFT);",
            "\t\t/*",
            "\t\t * Update the soft expiry time. clock_settime() might have",
            "\t\t * affected it.",
            "\t\t */",
            "\t\tcpu_base->softirq_expires_next = soft;",
            "\t}",
            "",
            "\texpires_next = __hrtimer_get_next_event(cpu_base, HRTIMER_ACTIVE_HARD);",
            "\t/*",
            "\t * If a softirq timer is expiring first, update cpu_base->next_timer",
            "\t * and program the hardware with the soft expiry time.",
            "\t */",
            "\tif (expires_next > soft) {",
            "\t\tcpu_base->next_timer = cpu_base->softirq_next_timer;",
            "\t\texpires_next = soft;",
            "\t}",
            "",
            "\treturn expires_next;",
            "}",
            "static inline ktime_t hrtimer_update_base(struct hrtimer_cpu_base *base)",
            "{",
            "\tktime_t *offs_real = &base->clock_base[HRTIMER_BASE_REALTIME].offset;",
            "\tktime_t *offs_boot = &base->clock_base[HRTIMER_BASE_BOOTTIME].offset;",
            "\tktime_t *offs_tai = &base->clock_base[HRTIMER_BASE_TAI].offset;",
            "",
            "\tktime_t now = ktime_get_update_offsets_now(&base->clock_was_set_seq,",
            "\t\t\t\t\t    offs_real, offs_boot, offs_tai);",
            "",
            "\tbase->clock_base[HRTIMER_BASE_REALTIME_SOFT].offset = *offs_real;",
            "\tbase->clock_base[HRTIMER_BASE_BOOTTIME_SOFT].offset = *offs_boot;",
            "\tbase->clock_base[HRTIMER_BASE_TAI_SOFT].offset = *offs_tai;",
            "",
            "\treturn now;",
            "}",
            "static inline int hrtimer_hres_active(struct hrtimer_cpu_base *cpu_base)",
            "{",
            "\treturn IS_ENABLED(CONFIG_HIGH_RES_TIMERS) ?",
            "\t\tcpu_base->hres_active : 0;",
            "}",
            "static void __hrtimer_reprogram(struct hrtimer_cpu_base *cpu_base,",
            "\t\t\t\tstruct hrtimer *next_timer,",
            "\t\t\t\tktime_t expires_next)",
            "{",
            "\tcpu_base->expires_next = expires_next;",
            "",
            "\t/*",
            "\t * If hres is not active, hardware does not have to be",
            "\t * reprogrammed yet.",
            "\t *",
            "\t * If a hang was detected in the last timer interrupt then we",
            "\t * leave the hang delay active in the hardware. We want the",
            "\t * system to make progress. That also prevents the following",
            "\t * scenario:",
            "\t * T1 expires 50ms from now",
            "\t * T2 expires 5s from now",
            "\t *",
            "\t * T1 is removed, so this code is called and would reprogram",
            "\t * the hardware to 5s from now. Any hrtimer_start after that",
            "\t * will not reprogram the hardware due to hang_detected being",
            "\t * set. So we'd effectively block all timers until the T2 event",
            "\t * fires.",
            "\t */",
            "\tif (!hrtimer_hres_active(cpu_base) || cpu_base->hang_detected)",
            "\t\treturn;",
            "",
            "\ttick_program_event(expires_next, 1);",
            "}",
            "static void",
            "hrtimer_force_reprogram(struct hrtimer_cpu_base *cpu_base, int skip_equal)",
            "{",
            "\tktime_t expires_next;",
            "",
            "\texpires_next = hrtimer_update_next_event(cpu_base);",
            "",
            "\tif (skip_equal && expires_next == cpu_base->expires_next)",
            "\t\treturn;",
            "",
            "\t__hrtimer_reprogram(cpu_base, cpu_base->next_timer, expires_next);",
            "}",
            "static int __init setup_hrtimer_hres(char *str)",
            "{",
            "\treturn (kstrtobool(str, &hrtimer_hres_enabled) == 0);",
            "}",
            "static inline int hrtimer_is_hres_enabled(void)",
            "{",
            "\treturn hrtimer_hres_enabled;",
            "}",
            "static void hrtimer_switch_to_hres(void)",
            "{",
            "\tstruct hrtimer_cpu_base *base = this_cpu_ptr(&hrtimer_bases);",
            "",
            "\tif (tick_init_highres()) {",
            "\t\tpr_warn(\"Could not switch to high resolution mode on CPU %u\\n\",",
            "\t\t\tbase->cpu);",
            "\t\treturn;",
            "\t}",
            "\tbase->hres_active = 1;",
            "\thrtimer_resolution = HIGH_RES_NSEC;",
            "",
            "\ttick_setup_sched_timer();",
            "\t/* \"Retrigger\" the interrupt to get things going */",
            "\tretrigger_next_event(NULL);",
            "}"
          ],
          "function_name": "hrtimer_update_next_event, hrtimer_update_base, hrtimer_hres_active, __hrtimer_reprogram, hrtimer_force_reprogram, setup_hrtimer_hres, hrtimer_is_hres_enabled, hrtimer_switch_to_hres",
          "description": "实现高精度模式下的事件更新逻辑(hrtimer_update_next_event)，维护时钟偏移更新(hrtimer_update_base)，控制硬件定时器重编程(__hrtimer_reprogram)，并提供高精度模式开关(setup_hrtimer_hres/hrtimer_switch_to_hres)。",
          "similarity": 0.5317102670669556
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/time/hrtimer.c",
          "start_line": 1,
          "end_line": 129,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " *  Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>",
            " *  Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar",
            " *  Copyright(C) 2006-2007  Timesys Corp., Thomas Gleixner",
            " *",
            " *  High-resolution kernel timers",
            " *",
            " *  In contrast to the low-resolution timeout API, aka timer wheel,",
            " *  hrtimers provide finer resolution and accuracy depending on system",
            " *  configuration and capabilities.",
            " *",
            " *  Started by: Thomas Gleixner and Ingo Molnar",
            " *",
            " *  Credits:",
            " *\tBased on the original timer wheel code",
            " *",
            " *\tHelp, testing, suggestions, bugfixes, improvements were",
            " *\tprovided by:",
            " *",
            " *\tGeorge Anzinger, Andrew Morton, Steven Rostedt, Roman Zippel",
            " *\tet. al.",
            " */",
            "",
            "#include <linux/cpu.h>",
            "#include <linux/export.h>",
            "#include <linux/percpu.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/notifier.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/tick.h>",
            "#include <linux/err.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/timer.h>",
            "#include <linux/freezer.h>",
            "#include <linux/compat.h>",
            "",
            "#include <linux/uaccess.h>",
            "",
            "#include <trace/events/timer.h>",
            "",
            "#include \"tick-internal.h\"",
            "",
            "/*",
            " * Masks for selecting the soft and hard context timers from",
            " * cpu_base->active",
            " */",
            "#define MASK_SHIFT\t\t(HRTIMER_BASE_MONOTONIC_SOFT)",
            "#define HRTIMER_ACTIVE_HARD\t((1U << MASK_SHIFT) - 1)",
            "#define HRTIMER_ACTIVE_SOFT\t(HRTIMER_ACTIVE_HARD << MASK_SHIFT)",
            "#define HRTIMER_ACTIVE_ALL\t(HRTIMER_ACTIVE_SOFT | HRTIMER_ACTIVE_HARD)",
            "",
            "static void retrigger_next_event(void *arg);",
            "",
            "/*",
            " * The timer bases:",
            " *",
            " * There are more clockids than hrtimer bases. Thus, we index",
            " * into the timer bases by the hrtimer_base_type enum. When trying",
            " * to reach a base using a clockid, hrtimer_clockid_to_base()",
            " * is used to convert from clockid to the proper hrtimer_base_type.",
            " */",
            "DEFINE_PER_CPU(struct hrtimer_cpu_base, hrtimer_bases) =",
            "{",
            "\t.lock = __RAW_SPIN_LOCK_UNLOCKED(hrtimer_bases.lock),",
            "\t.clock_base =",
            "\t{",
            "\t\t{",
            "\t\t\t.index = HRTIMER_BASE_MONOTONIC,",
            "\t\t\t.clockid = CLOCK_MONOTONIC,",
            "\t\t\t.get_time = &ktime_get,",
            "\t\t},",
            "\t\t{",
            "\t\t\t.index = HRTIMER_BASE_REALTIME,",
            "\t\t\t.clockid = CLOCK_REALTIME,",
            "\t\t\t.get_time = &ktime_get_real,",
            "\t\t},",
            "\t\t{",
            "\t\t\t.index = HRTIMER_BASE_BOOTTIME,",
            "\t\t\t.clockid = CLOCK_BOOTTIME,",
            "\t\t\t.get_time = &ktime_get_boottime,",
            "\t\t},",
            "\t\t{",
            "\t\t\t.index = HRTIMER_BASE_TAI,",
            "\t\t\t.clockid = CLOCK_TAI,",
            "\t\t\t.get_time = &ktime_get_clocktai,",
            "\t\t},",
            "\t\t{",
            "\t\t\t.index = HRTIMER_BASE_MONOTONIC_SOFT,",
            "\t\t\t.clockid = CLOCK_MONOTONIC,",
            "\t\t\t.get_time = &ktime_get,",
            "\t\t},",
            "\t\t{",
            "\t\t\t.index = HRTIMER_BASE_REALTIME_SOFT,",
            "\t\t\t.clockid = CLOCK_REALTIME,",
            "\t\t\t.get_time = &ktime_get_real,",
            "\t\t},",
            "\t\t{",
            "\t\t\t.index = HRTIMER_BASE_BOOTTIME_SOFT,",
            "\t\t\t.clockid = CLOCK_BOOTTIME,",
            "\t\t\t.get_time = &ktime_get_boottime,",
            "\t\t},",
            "\t\t{",
            "\t\t\t.index = HRTIMER_BASE_TAI_SOFT,",
            "\t\t\t.clockid = CLOCK_TAI,",
            "\t\t\t.get_time = &ktime_get_clocktai,",
            "\t\t},",
            "\t},",
            "\t.csd = CSD_INIT(retrigger_next_event, NULL)",
            "};",
            "",
            "static const int hrtimer_clock_to_base_table[MAX_CLOCKS] = {",
            "\t/* Make sure we catch unsupported clockids */",
            "\t[0 ... MAX_CLOCKS - 1]\t= HRTIMER_MAX_CLOCK_BASES,",
            "",
            "\t[CLOCK_REALTIME]\t= HRTIMER_BASE_REALTIME,",
            "\t[CLOCK_MONOTONIC]\t= HRTIMER_BASE_MONOTONIC,",
            "\t[CLOCK_BOOTTIME]\t= HRTIMER_BASE_BOOTTIME,",
            "\t[CLOCK_TAI]\t\t= HRTIMER_BASE_TAI,",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义高分辨率定时器的基础结构，初始化每个CPU的hrtimer_bases，配置不同时间基准（如MONOTONIC、REALTIME）对应的获取函数，建立clockid到基础类型的映射表。",
          "similarity": 0.5207616090774536
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/time/hrtimer.c",
          "start_line": 2081,
          "end_line": 2185,
          "content": [
            "static int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode)",
            "{",
            "\tstruct restart_block *restart;",
            "",
            "\tdo {",
            "\t\tset_current_state(TASK_INTERRUPTIBLE|TASK_FREEZABLE);",
            "\t\thrtimer_sleeper_start_expires(t, mode);",
            "",
            "\t\tif (likely(t->task))",
            "\t\t\tschedule();",
            "",
            "\t\thrtimer_cancel(&t->timer);",
            "\t\tmode = HRTIMER_MODE_ABS;",
            "",
            "\t} while (t->task && !signal_pending(current));",
            "",
            "\t__set_current_state(TASK_RUNNING);",
            "",
            "\tif (!t->task)",
            "\t\treturn 0;",
            "",
            "\trestart = &current->restart_block;",
            "\tif (restart->nanosleep.type != TT_NONE) {",
            "\t\tktime_t rem = hrtimer_expires_remaining(&t->timer);",
            "\t\tstruct timespec64 rmt;",
            "",
            "\t\tif (rem <= 0)",
            "\t\t\treturn 0;",
            "\t\trmt = ktime_to_timespec64(rem);",
            "",
            "\t\treturn nanosleep_copyout(restart, &rmt);",
            "\t}",
            "\treturn -ERESTART_RESTARTBLOCK;",
            "}",
            "static long __sched hrtimer_nanosleep_restart(struct restart_block *restart)",
            "{",
            "\tstruct hrtimer_sleeper t;",
            "\tint ret;",
            "",
            "\thrtimer_init_sleeper_on_stack(&t, restart->nanosleep.clockid,",
            "\t\t\t\t      HRTIMER_MODE_ABS);",
            "\thrtimer_set_expires_tv64(&t.timer, restart->nanosleep.expires);",
            "\tret = do_nanosleep(&t, HRTIMER_MODE_ABS);",
            "\tdestroy_hrtimer_on_stack(&t.timer);",
            "\treturn ret;",
            "}",
            "long hrtimer_nanosleep(ktime_t rqtp, const enum hrtimer_mode mode,",
            "\t\t       const clockid_t clockid)",
            "{",
            "\tstruct restart_block *restart;",
            "\tstruct hrtimer_sleeper t;",
            "\tint ret = 0;",
            "",
            "\thrtimer_init_sleeper_on_stack(&t, clockid, mode);",
            "\thrtimer_set_expires_range_ns(&t.timer, rqtp, current->timer_slack_ns);",
            "\tret = do_nanosleep(&t, mode);",
            "\tif (ret != -ERESTART_RESTARTBLOCK)",
            "\t\tgoto out;",
            "",
            "\t/* Absolute timers do not update the rmtp value and restart: */",
            "\tif (mode == HRTIMER_MODE_ABS) {",
            "\t\tret = -ERESTARTNOHAND;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\trestart = &current->restart_block;",
            "\trestart->nanosleep.clockid = t.timer.base->clockid;",
            "\trestart->nanosleep.expires = hrtimer_get_expires_tv64(&t.timer);",
            "\tset_restart_fn(restart, hrtimer_nanosleep_restart);",
            "out:",
            "\tdestroy_hrtimer_on_stack(&t.timer);",
            "\treturn ret;",
            "}",
            "int hrtimers_prepare_cpu(unsigned int cpu)",
            "{",
            "\tstruct hrtimer_cpu_base *cpu_base = &per_cpu(hrtimer_bases, cpu);",
            "\tint i;",
            "",
            "\tfor (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {",
            "\t\tstruct hrtimer_clock_base *clock_b = &cpu_base->clock_base[i];",
            "",
            "\t\tclock_b->cpu_base = cpu_base;",
            "\t\tseqcount_raw_spinlock_init(&clock_b->seq, &cpu_base->lock);",
            "\t\ttimerqueue_init_head(&clock_b->active);",
            "\t}",
            "",
            "\tcpu_base->cpu = cpu;",
            "\thrtimer_cpu_base_init_expiry_lock(cpu_base);",
            "\treturn 0;",
            "}",
            "int hrtimers_cpu_starting(unsigned int cpu)",
            "{",
            "\tstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);",
            "",
            "\t/* Clear out any left over state from a CPU down operation */",
            "\tcpu_base->active_bases = 0;",
            "\tcpu_base->hres_active = 0;",
            "\tcpu_base->hang_detected = 0;",
            "\tcpu_base->next_timer = NULL;",
            "\tcpu_base->softirq_next_timer = NULL;",
            "\tcpu_base->expires_next = KTIME_MAX;",
            "\tcpu_base->softirq_expires_next = KTIME_MAX;",
            "\tcpu_base->online = 1;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "do_nanosleep, hrtimer_nanosleep_restart, hrtimer_nanosleep, hrtimers_prepare_cpu, hrtimers_cpu_starting",
          "description": "实现纳秒级休眠的核心逻辑，通过hrtimer_sleeper结构体跟踪睡眠状态，处理任务调度和中断重启，包含CPU初始化及启动时的定时器基础设施设置。",
          "similarity": 0.5200039744377136
        },
        {
          "chunk_id": 9,
          "file_path": "kernel/time/hrtimer.c",
          "start_line": 1521,
          "end_line": 1626,
          "content": [
            "ktime_t __hrtimer_get_remaining(const struct hrtimer *timer, bool adjust)",
            "{",
            "\tunsigned long flags;",
            "\tktime_t rem;",
            "",
            "\tlock_hrtimer_base(timer, &flags);",
            "\tif (IS_ENABLED(CONFIG_TIME_LOW_RES) && adjust)",
            "\t\trem = hrtimer_expires_remaining_adjusted(timer);",
            "\telse",
            "\t\trem = hrtimer_expires_remaining(timer);",
            "\tunlock_hrtimer_base(timer, &flags);",
            "",
            "\treturn rem;",
            "}",
            "u64 hrtimer_get_next_event(void)",
            "{",
            "\tstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);",
            "\tu64 expires = KTIME_MAX;",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&cpu_base->lock, flags);",
            "",
            "\tif (!hrtimer_hres_active(cpu_base))",
            "\t\texpires = __hrtimer_get_next_event(cpu_base, HRTIMER_ACTIVE_ALL);",
            "",
            "\traw_spin_unlock_irqrestore(&cpu_base->lock, flags);",
            "",
            "\treturn expires;",
            "}",
            "u64 hrtimer_next_event_without(const struct hrtimer *exclude)",
            "{",
            "\tstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);",
            "\tu64 expires = KTIME_MAX;",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&cpu_base->lock, flags);",
            "",
            "\tif (hrtimer_hres_active(cpu_base)) {",
            "\t\tunsigned int active;",
            "",
            "\t\tif (!cpu_base->softirq_activated) {",
            "\t\t\tactive = cpu_base->active_bases & HRTIMER_ACTIVE_SOFT;",
            "\t\t\texpires = __hrtimer_next_event_base(cpu_base, exclude,",
            "\t\t\t\t\t\t\t    active, KTIME_MAX);",
            "\t\t}",
            "\t\tactive = cpu_base->active_bases & HRTIMER_ACTIVE_HARD;",
            "\t\texpires = __hrtimer_next_event_base(cpu_base, exclude, active,",
            "\t\t\t\t\t\t    expires);",
            "\t}",
            "",
            "\traw_spin_unlock_irqrestore(&cpu_base->lock, flags);",
            "",
            "\treturn expires;",
            "}",
            "static inline int hrtimer_clockid_to_base(clockid_t clock_id)",
            "{",
            "\tif (likely(clock_id < MAX_CLOCKS)) {",
            "\t\tint base = hrtimer_clock_to_base_table[clock_id];",
            "",
            "\t\tif (likely(base != HRTIMER_MAX_CLOCK_BASES))",
            "\t\t\treturn base;",
            "\t}",
            "\tWARN(1, \"Invalid clockid %d. Using MONOTONIC\\n\", clock_id);",
            "\treturn HRTIMER_BASE_MONOTONIC;",
            "}",
            "static void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,",
            "\t\t\t   enum hrtimer_mode mode)",
            "{",
            "\tbool softtimer = !!(mode & HRTIMER_MODE_SOFT);",
            "\tstruct hrtimer_cpu_base *cpu_base;",
            "\tint base;",
            "",
            "\t/*",
            "\t * On PREEMPT_RT enabled kernels hrtimers which are not explicitly",
            "\t * marked for hard interrupt expiry mode are moved into soft",
            "\t * interrupt context for latency reasons and because the callbacks",
            "\t * can invoke functions which might sleep on RT, e.g. spin_lock().",
            "\t */",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && !(mode & HRTIMER_MODE_HARD))",
            "\t\tsofttimer = true;",
            "",
            "\tmemset(timer, 0, sizeof(struct hrtimer));",
            "",
            "\tcpu_base = raw_cpu_ptr(&hrtimer_bases);",
            "",
            "\t/*",
            "\t * POSIX magic: Relative CLOCK_REALTIME timers are not affected by",
            "\t * clock modifications, so they needs to become CLOCK_MONOTONIC to",
            "\t * ensure POSIX compliance.",
            "\t */",
            "\tif (clock_id == CLOCK_REALTIME && mode & HRTIMER_MODE_REL)",
            "\t\tclock_id = CLOCK_MONOTONIC;",
            "",
            "\tbase = softtimer ? HRTIMER_MAX_CLOCK_BASES / 2 : 0;",
            "\tbase += hrtimer_clockid_to_base(clock_id);",
            "\ttimer->is_soft = softtimer;",
            "\ttimer->is_hard = !!(mode & HRTIMER_MODE_HARD);",
            "\ttimer->base = &cpu_base->clock_base[base];",
            "\ttimerqueue_init(&timer->node);",
            "}",
            "void hrtimer_init(struct hrtimer *timer, clockid_t clock_id,",
            "\t\t  enum hrtimer_mode mode)",
            "{",
            "\tdebug_init(timer, clock_id, mode);",
            "\t__hrtimer_init(timer, clock_id, mode);",
            "}"
          ],
          "function_name": "__hrtimer_get_remaining, hrtimer_get_next_event, hrtimer_next_event_without, hrtimer_clockid_to_base, __hrtimer_init, hrtimer_init",
          "description": "实现定时器核心管理功能，包含剩余时间查询、下次事件获取、时钟基转换等，初始化定时器时根据模式和时钟ID确定基础结构体并配置软硬中断属性，处理POSIX合规性需求的CLOCK_REALTIME相对定时器转换",
          "similarity": 0.5097917318344116
        }
      ]
    },
    {
      "source_file": "kernel/time/timer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:57:06\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `time\\timer.c`\n\n---\n\n# `time/timer.c` 技术文档\n\n## 1. 文件概述\n\n`time/timer.c` 是 Linux 内核中实现**内核定时器子系统**的核心文件，负责管理基于**定时器轮（timer wheel）** 的动态定时器机制。该文件提供了高效、可扩展的定时器调度框架，支持高精度超时处理、SMP（对称多处理）环境下的 per-CPU 定时器管理，以及与 NO_HZ（动态 tick）节能机制的集成。其设计目标是在保证大多数超时场景（如网络、I/O 超时）性能的同时，通过多级粒度结构避免传统定时器轮中频繁的级联（cascading）操作，从而提升系统可扩展性。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`jiffies_64`**：全局 64 位 jiffies 计数器，记录自系统启动以来的时钟滴答数，对齐缓存行以优化 SMP 访问。\n- **多级定时器轮（Timer Wheel）结构**：\n  - 由 `LVL_DEPTH` 层（通常为 8 或 9）组成，每层包含 `LVL_SIZE`（64）个桶（buckets）。\n  - 每层具有不同的时间粒度（granularity），随层级升高而增大。\n- **定时器基础（Timer Bases）**：\n  - `BASE_STD`：标准定时器基础，用于普通定时器。\n  - `BASE_DEF`：可延迟定时器基础（仅当 `CONFIG_NO_HZ_COMMON` 启用时存在），用于在 CPU 空闲时可推迟执行的定时器。\n\n### 关键宏定义\n- `LVL_CLK_SHIFT` / `LVL_CLK_DIV`：定义层级间的时间粒度缩放因子（默认为 8 倍）。\n- `LVL_GRAN(n)`：第 `n` 层的时间粒度（单位：jiffies）。\n- `LVL_START(n)`：第 `n` 层的起始偏移时间，用于计算定时器应插入的层级。\n- `WHEEL_TIMEOUT_CUTOFF` / `WHEEL_TIMEOUT_MAX`：定时器轮的最大支持超时时间（约 12 天 @ HZ=1000）。\n\n### 主要功能\n- 定时器的注册（`add_timer`）、删除（`del_timer`）和修改（`mod_timer`）。\n- 定时器到期处理（软中断上下文执行）。\n- 与 tick 管理子系统（`tick.h`）和 NO_HZ 模式协同工作。\n- 提供 `sys_sysinfo` 系统调用的底层支持。\n\n## 3. 关键实现\n\n### 多级定时器轮算法\n- **层级设计**：定时器根据其到期时间的远近被分配到不同层级。近到期定时器放入低层（高精度），远到期放入高层（低精度）。\n- **无级联机制**：与经典定时器轮不同，本实现**不进行定时器的级联迁移**。高层定时器到期时直接触发，牺牲少量精度换取显著性能提升。\n- **隐式批处理**：高层的粗粒度天然实现超时事件的批处理，减少中断和软中断开销。\n- **超时截断**：超过 `WHEEL_TIMEOUT_MAX` 的定时器会被强制设为最大支持超时值，实测表明实际使用中超时极少超过 5 天。\n\n### 粒度与范围（以 HZ=1000 为例）\n| 层级 | 偏移 | 粒度 | 范围 |\n|------|------|------|------|\n| 0 | 0 | 1 ms | 0 – 63 ms |\n| 1 | 64 | 8 ms | 64 – 511 ms |\n| ... | ... | ... | ... |\n| 8 | 512 | ~4 小时 | ~1 天 – ~12 天 |\n\n### NO_HZ 支持\n- 当启用 `CONFIG_NO_HZ_COMMON` 时，系统维护**两个独立的定时器轮**：\n  - `BASE_STD`：标准定时器，必须准时触发。\n  - `BASE_DEF`：可延迟定时器，在 CPU 进入空闲状态时可推迟执行，用于节能。\n\n### SMP 优化\n- 定时器默认绑定到注册时的 CPU，利用 per-CPU 数据结构减少锁竞争。\n- `jiffies_64` 使用 `__cacheline_aligned_in_smp` 对齐，避免 false sharing。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **时间子系统**：`<linux/time.h>`, `<linux/jiffies.h>`, `<asm/timex.h>`\n- **调度与中断**：`<linux/interrupt.h>`, `<linux/irq_work.h>`, `<linux/sched/*.h>`\n- **内存管理**：`<linux/slab.h>`, `<linux/mm.h>`\n- **系统调用**：`<linux/syscalls.h>`, `<linux/uaccess.h>`\n- **内部模块**：`\"tick-internal.h\"`（tick 管理）、`<trace/events/timer.h>`（跟踪点）\n\n### 内核子系统交互\n- **Tick 管理**：通过 `tick.h` 接口获取时钟事件，驱动定时器轮推进。\n- **软中断**：定时器到期回调在 `TIMER_SOFTIRQ` 软中断上下文中执行。\n- **POSIX 定时器**：为 `<linux/posix-timers.h>` 提供底层支持。\n- **CPU 热插拔**：通过 `cpu.h` 处理 CPU 上下线时的定时器迁移。\n- **电源管理**：与 `NO_HZ` 和 `sched/nohz.h` 协同实现动态 tick。\n\n## 5. 使用场景\n\n- **内核超时机制**：网络协议栈（TCP 重传、连接超时）、块设备 I/O 超时、文件系统缓存回收等。\n- **延迟执行任务**：通过 `mod_timer` 实现延迟工作队列（如 `delayed_work`）。\n- **系统时间维护**：为 `jiffies` 和 `get_jiffies_64()` 提供原子更新。\n- **用户空间接口**：支撑 `sysinfo` 系统调用返回 uptime、负载等信息。\n- **高精度定时需求**：短超时（<64ms @ HZ=1000）可获得毫秒级精度，满足实时性要求。\n- **低功耗系统**：在 `NO_HZ_IDLE` 或 `NO_HZ_FULL` 模式下，通过 `BASE_DEF` 减少不必要的 tick 中断。",
      "similarity": 0.5443043112754822,
      "chunks": [
        {
          "chunk_id": 10,
          "file_path": "kernel/time/timer.c",
          "start_line": 2030,
          "end_line": 2130,
          "content": [
            "static __latent_entropy void run_timer_softirq(struct softirq_action *h)",
            "{",
            "\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);",
            "",
            "\t__run_timers(base);",
            "\tif (IS_ENABLED(CONFIG_NO_HZ_COMMON))",
            "\t\t__run_timers(this_cpu_ptr(&timer_bases[BASE_DEF]));",
            "}",
            "static void run_local_timers(void)",
            "{",
            "\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);",
            "",
            "\thrtimer_run_queues();",
            "\t/* Raise the softirq only if required. */",
            "\tif (time_before(jiffies, base->next_expiry)) {",
            "\t\tif (!IS_ENABLED(CONFIG_NO_HZ_COMMON))",
            "\t\t\treturn;",
            "\t\t/* CPU is awake, so check the deferrable base. */",
            "\t\tbase++;",
            "\t\tif (time_before(jiffies, base->next_expiry))",
            "\t\t\treturn;",
            "\t}",
            "\traise_timer_softirq(TIMER_SOFTIRQ);",
            "}",
            "void update_process_times(int user_tick)",
            "{",
            "\tstruct task_struct *p = current;",
            "",
            "\t/* Note: this timer irq context must be accounted for as well. */",
            "\taccount_process_tick(p, user_tick);",
            "\trun_local_timers();",
            "\trcu_sched_clock_irq(user_tick);",
            "#ifdef CONFIG_IRQ_WORK",
            "\tif (in_irq())",
            "\t\tirq_work_tick();",
            "#endif",
            "\tsched_tick();",
            "\tif (IS_ENABLED(CONFIG_POSIX_TIMERS))",
            "\t\trun_posix_cpu_timers();",
            "}",
            "static void process_timeout(struct timer_list *t)",
            "{",
            "\tstruct process_timer *timeout = from_timer(timeout, t, timer);",
            "",
            "\twake_up_process(timeout->task);",
            "}",
            "signed long __sched schedule_timeout(signed long timeout)",
            "{",
            "\tstruct process_timer timer;",
            "\tunsigned long expire;",
            "",
            "\tswitch (timeout)",
            "\t{",
            "\tcase MAX_SCHEDULE_TIMEOUT:",
            "\t\t/*",
            "\t\t * These two special cases are useful to be comfortable",
            "\t\t * in the caller. Nothing more. We could take",
            "\t\t * MAX_SCHEDULE_TIMEOUT from one of the negative value",
            "\t\t * but I' d like to return a valid offset (>=0) to allow",
            "\t\t * the caller to do everything it want with the retval.",
            "\t\t */",
            "\t\tschedule();",
            "\t\tgoto out;",
            "\tdefault:",
            "\t\t/*",
            "\t\t * Another bit of PARANOID. Note that the retval will be",
            "\t\t * 0 since no piece of kernel is supposed to do a check",
            "\t\t * for a negative retval of schedule_timeout() (since it",
            "\t\t * should never happens anyway). You just have the printk()",
            "\t\t * that will tell you if something is gone wrong and where.",
            "\t\t */",
            "\t\tif (timeout < 0) {",
            "\t\t\tprintk(KERN_ERR \"schedule_timeout: wrong timeout \"",
            "\t\t\t\t\"value %lx\\n\", timeout);",
            "\t\t\tdump_stack();",
            "\t\t\t__set_current_state(TASK_RUNNING);",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t}",
            "",
            "\texpire = timeout + jiffies;",
            "",
            "\ttimer.task = current;",
            "\ttimer_setup_on_stack(&timer.timer, process_timeout, 0);",
            "\t__mod_timer(&timer.timer, expire, MOD_TIMER_NOTPENDING);",
            "\tschedule();",
            "\tdel_timer_sync(&timer.timer);",
            "",
            "\t/* Remove the timer from the object tracker */",
            "\tdestroy_timer_on_stack(&timer.timer);",
            "",
            "\ttimeout = expire - jiffies;",
            "",
            " out:",
            "\treturn timeout < 0 ? 0 : timeout;",
            "}",
            "signed long __sched schedule_timeout_interruptible(signed long timeout)",
            "{",
            "\t__set_current_state(TASK_INTERRUPTIBLE);",
            "\treturn schedule_timeout(timeout);",
            "}"
          ],
          "function_name": "run_timer_softirq, run_local_timers, update_process_times, process_timeout, schedule_timeout, schedule_timeout_interruptible",
          "description": "该代码段核心功能是处理定时器相关操作，涵盖软中断处理、本地定时器管理、进程时间更新及休眠超时控制。  \n`run_timer_softirq`和`run_local_timers`分别用于处理软中断中的定时器队列和本地定时器检查，`update_process_times`更新进程时间并触发本地定时器，`schedule_timeout`系列通过定时器实现进程休眠与超时唤醒。  \n上下文不完整：部分关键函数（如`__run_timers`、`hrtimer_run_queues`）的实现未展示，依赖外部知识理解其行为。",
          "similarity": 0.517694890499115
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/time/timer.c",
          "start_line": 1,
          "end_line": 230,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " *  Kernel internal timers",
            " *",
            " *  Copyright (C) 1991, 1992  Linus Torvalds",
            " *",
            " *  1997-01-28  Modified by Finn Arne Gangstad to make timers scale better.",
            " *",
            " *  1997-09-10  Updated NTP code according to technical memorandum Jan '96",
            " *              \"A Kernel Model for Precision Timekeeping\" by Dave Mills",
            " *  1998-12-24  Fixed a xtime SMP race (we need the xtime_lock rw spinlock to",
            " *              serialize accesses to xtime/lost_ticks).",
            " *                              Copyright (C) 1998  Andrea Arcangeli",
            " *  1999-03-10  Improved NTP compatibility by Ulrich Windl",
            " *  2002-05-31\tMove sys_sysinfo here and make its locking sane, Robert Love",
            " *  2000-10-05  Implemented scalable SMP per-CPU timer handling.",
            " *                              Copyright (C) 2000, 2001, 2002  Ingo Molnar",
            " *              Designed by David S. Miller, Alexey Kuznetsov and Ingo Molnar",
            " */",
            "",
            "#include <linux/kernel_stat.h>",
            "#include <linux/export.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/percpu.h>",
            "#include <linux/init.h>",
            "#include <linux/mm.h>",
            "#include <linux/swap.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/notifier.h>",
            "#include <linux/thread_info.h>",
            "#include <linux/time.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/cpu.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/delay.h>",
            "#include <linux/tick.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/slab.h>",
            "#include <linux/compat.h>",
            "#include <linux/random.h>",
            "#include <linux/sysctl.h>",
            "",
            "#include <linux/uaccess.h>",
            "#include <asm/unistd.h>",
            "#include <asm/div64.h>",
            "#include <asm/timex.h>",
            "#include <asm/io.h>",
            "",
            "#include \"tick-internal.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/timer.h>",
            "",
            "__visible u64 jiffies_64 __cacheline_aligned_in_smp = INITIAL_JIFFIES;",
            "",
            "EXPORT_SYMBOL(jiffies_64);",
            "",
            "/*",
            " * The timer wheel has LVL_DEPTH array levels. Each level provides an array of",
            " * LVL_SIZE buckets. Each level is driven by its own clock and therefor each",
            " * level has a different granularity.",
            " *",
            " * The level granularity is:\t\tLVL_CLK_DIV ^ lvl",
            " * The level clock frequency is:\tHZ / (LVL_CLK_DIV ^ level)",
            " *",
            " * The array level of a newly armed timer depends on the relative expiry",
            " * time. The farther the expiry time is away the higher the array level and",
            " * therefor the granularity becomes.",
            " *",
            " * Contrary to the original timer wheel implementation, which aims for 'exact'",
            " * expiry of the timers, this implementation removes the need for recascading",
            " * the timers into the lower array levels. The previous 'classic' timer wheel",
            " * implementation of the kernel already violated the 'exact' expiry by adding",
            " * slack to the expiry time to provide batched expiration. The granularity",
            " * levels provide implicit batching.",
            " *",
            " * This is an optimization of the original timer wheel implementation for the",
            " * majority of the timer wheel use cases: timeouts. The vast majority of",
            " * timeout timers (networking, disk I/O ...) are canceled before expiry. If",
            " * the timeout expires it indicates that normal operation is disturbed, so it",
            " * does not matter much whether the timeout comes with a slight delay.",
            " *",
            " * The only exception to this are networking timers with a small expiry",
            " * time. They rely on the granularity. Those fit into the first wheel level,",
            " * which has HZ granularity.",
            " *",
            " * We don't have cascading anymore. timers with a expiry time above the",
            " * capacity of the last wheel level are force expired at the maximum timeout",
            " * value of the last wheel level. From data sampling we know that the maximum",
            " * value observed is 5 days (network connection tracking), so this should not",
            " * be an issue.",
            " *",
            " * The currently chosen array constants values are a good compromise between",
            " * array size and granularity.",
            " *",
            " * This results in the following granularity and range levels:",
            " *",
            " * HZ 1000 steps",
            " * Level Offset  Granularity            Range",
            " *  0      0         1 ms                0 ms -         63 ms",
            " *  1     64         8 ms               64 ms -        511 ms",
            " *  2    128        64 ms              512 ms -       4095 ms (512ms - ~4s)",
            " *  3    192       512 ms             4096 ms -      32767 ms (~4s - ~32s)",
            " *  4    256      4096 ms (~4s)      32768 ms -     262143 ms (~32s - ~4m)",
            " *  5    320     32768 ms (~32s)    262144 ms -    2097151 ms (~4m - ~34m)",
            " *  6    384    262144 ms (~4m)    2097152 ms -   16777215 ms (~34m - ~4h)",
            " *  7    448   2097152 ms (~34m)  16777216 ms -  134217727 ms (~4h - ~1d)",
            " *  8    512  16777216 ms (~4h)  134217728 ms - 1073741822 ms (~1d - ~12d)",
            " *",
            " * HZ  300",
            " * Level Offset  Granularity            Range",
            " *  0\t   0         3 ms                0 ms -        210 ms",
            " *  1\t  64        26 ms              213 ms -       1703 ms (213ms - ~1s)",
            " *  2\t 128       213 ms             1706 ms -      13650 ms (~1s - ~13s)",
            " *  3\t 192      1706 ms (~1s)      13653 ms -     109223 ms (~13s - ~1m)",
            " *  4\t 256     13653 ms (~13s)    109226 ms -     873810 ms (~1m - ~14m)",
            " *  5\t 320    109226 ms (~1m)     873813 ms -    6990503 ms (~14m - ~1h)",
            " *  6\t 384    873813 ms (~14m)   6990506 ms -   55924050 ms (~1h - ~15h)",
            " *  7\t 448   6990506 ms (~1h)   55924053 ms -  447392423 ms (~15h - ~5d)",
            " *  8    512  55924053 ms (~15h) 447392426 ms - 3579139406 ms (~5d - ~41d)",
            " *",
            " * HZ  250",
            " * Level Offset  Granularity            Range",
            " *  0\t   0         4 ms                0 ms -        255 ms",
            " *  1\t  64        32 ms              256 ms -       2047 ms (256ms - ~2s)",
            " *  2\t 128       256 ms             2048 ms -      16383 ms (~2s - ~16s)",
            " *  3\t 192      2048 ms (~2s)      16384 ms -     131071 ms (~16s - ~2m)",
            " *  4\t 256     16384 ms (~16s)    131072 ms -    1048575 ms (~2m - ~17m)",
            " *  5\t 320    131072 ms (~2m)    1048576 ms -    8388607 ms (~17m - ~2h)",
            " *  6\t 384   1048576 ms (~17m)   8388608 ms -   67108863 ms (~2h - ~18h)",
            " *  7\t 448   8388608 ms (~2h)   67108864 ms -  536870911 ms (~18h - ~6d)",
            " *  8    512  67108864 ms (~18h) 536870912 ms - 4294967288 ms (~6d - ~49d)",
            " *",
            " * HZ  100",
            " * Level Offset  Granularity            Range",
            " *  0\t   0         10 ms               0 ms -        630 ms",
            " *  1\t  64         80 ms             640 ms -       5110 ms (640ms - ~5s)",
            " *  2\t 128        640 ms            5120 ms -      40950 ms (~5s - ~40s)",
            " *  3\t 192       5120 ms (~5s)     40960 ms -     327670 ms (~40s - ~5m)",
            " *  4\t 256      40960 ms (~40s)   327680 ms -    2621430 ms (~5m - ~43m)",
            " *  5\t 320     327680 ms (~5m)   2621440 ms -   20971510 ms (~43m - ~5h)",
            " *  6\t 384    2621440 ms (~43m) 20971520 ms -  167772150 ms (~5h - ~1d)",
            " *  7\t 448   20971520 ms (~5h) 167772160 ms - 1342177270 ms (~1d - ~15d)",
            " */",
            "",
            "/* Clock divisor for the next level */",
            "#define LVL_CLK_SHIFT\t3",
            "#define LVL_CLK_DIV\t(1UL << LVL_CLK_SHIFT)",
            "#define LVL_CLK_MASK\t(LVL_CLK_DIV - 1)",
            "#define LVL_SHIFT(n)\t((n) * LVL_CLK_SHIFT)",
            "#define LVL_GRAN(n)\t(1UL << LVL_SHIFT(n))",
            "",
            "/*",
            " * The time start value for each level to select the bucket at enqueue",
            " * time. We start from the last possible delta of the previous level",
            " * so that we can later add an extra LVL_GRAN(n) to n (see calc_index()).",
            " */",
            "#define LVL_START(n)\t((LVL_SIZE - 1) << (((n) - 1) * LVL_CLK_SHIFT))",
            "",
            "/* Size of each clock level */",
            "#define LVL_BITS\t6",
            "#define LVL_SIZE\t(1UL << LVL_BITS)",
            "#define LVL_MASK\t(LVL_SIZE - 1)",
            "#define LVL_OFFS(n)\t((n) * LVL_SIZE)",
            "",
            "/* Level depth */",
            "#if HZ > 100",
            "# define LVL_DEPTH\t9",
            "# else",
            "# define LVL_DEPTH\t8",
            "#endif",
            "",
            "/* The cutoff (max. capacity of the wheel) */",
            "#define WHEEL_TIMEOUT_CUTOFF\t(LVL_START(LVL_DEPTH))",
            "#define WHEEL_TIMEOUT_MAX\t(WHEEL_TIMEOUT_CUTOFF - LVL_GRAN(LVL_DEPTH - 1))",
            "",
            "/*",
            " * The resulting wheel size. If NOHZ is configured we allocate two",
            " * wheels so we have a separate storage for the deferrable timers.",
            " */",
            "#define WHEEL_SIZE\t(LVL_SIZE * LVL_DEPTH)",
            "",
            "#ifdef CONFIG_NO_HZ_COMMON",
            "# define NR_BASES\t2",
            "# define BASE_STD\t0",
            "# define BASE_DEF\t1",
            "#else",
            "# define NR_BASES\t1",
            "# define BASE_STD\t0",
            "# define BASE_DEF\t0",
            "#endif",
            "",
            "struct timer_base {",
            "\traw_spinlock_t\t\tlock;",
            "\tstruct timer_list\t*running_timer;",
            "#ifdef CONFIG_PREEMPT_RT",
            "\tspinlock_t\t\texpiry_lock;",
            "\tatomic_t\t\ttimer_waiters;",
            "#endif",
            "\tunsigned long\t\tclk;",
            "\tunsigned long\t\tnext_expiry;",
            "\tunsigned int\t\tcpu;",
            "\tbool\t\t\tnext_expiry_recalc;",
            "\tbool\t\t\tis_idle;",
            "\tbool\t\t\ttimers_pending;",
            "\tDECLARE_BITMAP(pending_map, WHEEL_SIZE);",
            "\tstruct hlist_head\tvectors[WHEEL_SIZE];",
            "} ____cacheline_aligned;",
            "",
            "static DEFINE_PER_CPU(struct timer_base, timer_bases[NR_BASES]);",
            "",
            "#ifdef CONFIG_NO_HZ_COMMON",
            "",
            "static DEFINE_STATIC_KEY_FALSE(timers_nohz_active);",
            "static DEFINE_MUTEX(timer_keys_mutex);",
            "",
            "static void timer_update_keys(struct work_struct *work);",
            "static DECLARE_WORK(timer_update_work, timer_update_keys);",
            "",
            "#ifdef CONFIG_SMP",
            "static unsigned int sysctl_timer_migration = 1;",
            "",
            "DEFINE_STATIC_KEY_FALSE(timers_migration_enabled);",
            ""
          ],
          "function_name": null,
          "description": "定义并实现了内核定时器轮（timer wheel）的数据结构和宏观布局，通过多层级桶结构管理定时器，支持不同粒度的超时处理，包含对NOHZ模式的支持及动态调整机制。",
          "similarity": 0.5100480318069458
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/time/timer.c",
          "start_line": 231,
          "end_line": 333,
          "content": [
            "static void timers_update_migration(void)",
            "{",
            "\tif (sysctl_timer_migration && tick_nohz_active)",
            "\t\tstatic_branch_enable(&timers_migration_enabled);",
            "\telse",
            "\t\tstatic_branch_disable(&timers_migration_enabled);",
            "}",
            "static int timer_migration_handler(struct ctl_table *table, int write,",
            "\t\t\t    void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint ret;",
            "",
            "\tmutex_lock(&timer_keys_mutex);",
            "\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\tif (!ret && write)",
            "\t\ttimers_update_migration();",
            "\tmutex_unlock(&timer_keys_mutex);",
            "\treturn ret;",
            "}",
            "static int __init timer_sysctl_init(void)",
            "{",
            "\tregister_sysctl(\"kernel\", timer_sysctl);",
            "\treturn 0;",
            "}",
            "static inline void timers_update_migration(void) { }",
            "static void timer_update_keys(struct work_struct *work)",
            "{",
            "\tmutex_lock(&timer_keys_mutex);",
            "\ttimers_update_migration();",
            "\tstatic_branch_enable(&timers_nohz_active);",
            "\tmutex_unlock(&timer_keys_mutex);",
            "}",
            "void timers_update_nohz(void)",
            "{",
            "\tschedule_work(&timer_update_work);",
            "}",
            "static inline bool is_timers_nohz_active(void)",
            "{",
            "\treturn static_branch_unlikely(&timers_nohz_active);",
            "}",
            "static inline bool is_timers_nohz_active(void) { return false; }",
            "static unsigned long round_jiffies_common(unsigned long j, int cpu,",
            "\t\tbool force_up)",
            "{",
            "\tint rem;",
            "\tunsigned long original = j;",
            "",
            "\t/*",
            "\t * We don't want all cpus firing their timers at once hitting the",
            "\t * same lock or cachelines, so we skew each extra cpu with an extra",
            "\t * 3 jiffies. This 3 jiffies came originally from the mm/ code which",
            "\t * already did this.",
            "\t * The skew is done by adding 3*cpunr, then round, then subtract this",
            "\t * extra offset again.",
            "\t */",
            "\tj += cpu * 3;",
            "",
            "\trem = j % HZ;",
            "",
            "\t/*",
            "\t * If the target jiffie is just after a whole second (which can happen",
            "\t * due to delays of the timer irq, long irq off times etc etc) then",
            "\t * we should round down to the whole second, not up. Use 1/4th second",
            "\t * as cutoff for this rounding as an extreme upper bound for this.",
            "\t * But never round down if @force_up is set.",
            "\t */",
            "\tif (rem < HZ/4 && !force_up) /* round down */",
            "\t\tj = j - rem;",
            "\telse /* round up */",
            "\t\tj = j - rem + HZ;",
            "",
            "\t/* now that we have rounded, subtract the extra skew again */",
            "\tj -= cpu * 3;",
            "",
            "\t/*",
            "\t * Make sure j is still in the future. Otherwise return the",
            "\t * unmodified value.",
            "\t */",
            "\treturn time_is_after_jiffies(j) ? j : original;",
            "}",
            "unsigned long __round_jiffies(unsigned long j, int cpu)",
            "{",
            "\treturn round_jiffies_common(j, cpu, false);",
            "}",
            "unsigned long __round_jiffies_relative(unsigned long j, int cpu)",
            "{",
            "\tunsigned long j0 = jiffies;",
            "",
            "\t/* Use j0 because jiffies might change while we run */",
            "\treturn round_jiffies_common(j + j0, cpu, false) - j0;",
            "}",
            "unsigned long round_jiffies(unsigned long j)",
            "{",
            "\treturn round_jiffies_common(j, raw_smp_processor_id(), false);",
            "}",
            "unsigned long round_jiffies_relative(unsigned long j)",
            "{",
            "\treturn __round_jiffies_relative(j, raw_smp_processor_id());",
            "}",
            "unsigned long __round_jiffies_up(unsigned long j, int cpu)",
            "{",
            "\treturn round_jiffies_common(j, cpu, true);",
            "}"
          ],
          "function_name": "timers_update_migration, timer_migration_handler, timer_sysctl_init, timers_update_migration, timer_update_keys, timers_update_nohz, is_timers_nohz_active, is_timers_nohz_active, round_jiffies_common, __round_jiffies, __round_jiffies_relative, round_jiffies, round_jiffies_relative, __round_jiffies_up",
          "description": "提供定时器迁移策略控制、Jiffies值调整逻辑及NOHZ相关功能，包含迁移开关配置、定时器分布优化算法和基于CPU负载的超时时间调整方法。",
          "similarity": 0.4904344081878662
        },
        {
          "chunk_id": 9,
          "file_path": "kernel/time/timer.c",
          "start_line": 1883,
          "end_line": 2004,
          "content": [
            "static u64 cmp_next_hrtimer_event(u64 basem, u64 expires)",
            "{",
            "\tu64 nextevt = hrtimer_get_next_event();",
            "",
            "\t/*",
            "\t * If high resolution timers are enabled",
            "\t * hrtimer_get_next_event() returns KTIME_MAX.",
            "\t */",
            "\tif (expires <= nextevt)",
            "\t\treturn expires;",
            "",
            "\t/*",
            "\t * If the next timer is already expired, return the tick base",
            "\t * time so the tick is fired immediately.",
            "\t */",
            "\tif (nextevt <= basem)",
            "\t\treturn basem;",
            "",
            "\t/*",
            "\t * Round up to the next jiffie. High resolution timers are",
            "\t * off, so the hrtimers are expired in the tick and we need to",
            "\t * make sure that this tick really expires the timer to avoid",
            "\t * a ping pong of the nohz stop code.",
            "\t *",
            "\t * Use DIV_ROUND_UP_ULL to prevent gcc calling __divdi3",
            "\t */",
            "\treturn DIV_ROUND_UP_ULL(nextevt, TICK_NSEC) * TICK_NSEC;",
            "}",
            "u64 get_next_timer_interrupt(unsigned long basej, u64 basem)",
            "{",
            "\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);",
            "\tu64 expires = KTIME_MAX;",
            "\tunsigned long nextevt;",
            "",
            "\t/*",
            "\t * Pretend that there is no timer pending if the cpu is offline.",
            "\t * Possible pending timers will be migrated later to an active cpu.",
            "\t */",
            "\tif (cpu_is_offline(smp_processor_id()))",
            "\t\treturn expires;",
            "",
            "\traw_spin_lock(&base->lock);",
            "\tif (base->next_expiry_recalc)",
            "\t\tbase->next_expiry = __next_timer_interrupt(base);",
            "\tnextevt = base->next_expiry;",
            "",
            "\t/*",
            "\t * We have a fresh next event. Check whether we can forward the",
            "\t * base. We can only do that when @basej is past base->clk",
            "\t * otherwise we might rewind base->clk.",
            "\t */",
            "\tif (time_after(basej, base->clk)) {",
            "\t\tif (time_after(nextevt, basej))",
            "\t\t\tbase->clk = basej;",
            "\t\telse if (time_after(nextevt, base->clk))",
            "\t\t\tbase->clk = nextevt;",
            "\t}",
            "",
            "\tif (time_before_eq(nextevt, basej)) {",
            "\t\texpires = basem;",
            "\t\tbase->is_idle = false;",
            "\t} else {",
            "\t\tif (base->timers_pending)",
            "\t\t\texpires = basem + (u64)(nextevt - basej) * TICK_NSEC;",
            "\t\t/*",
            "\t\t * If we expect to sleep more than a tick, mark the base idle.",
            "\t\t * Also the tick is stopped so any added timer must forward",
            "\t\t * the base clk itself to keep granularity small. This idle",
            "\t\t * logic is only maintained for the BASE_STD base, deferrable",
            "\t\t * timers may still see large granularity skew (by design).",
            "\t\t */",
            "\t\tif ((expires - basem) > TICK_NSEC)",
            "\t\t\tbase->is_idle = true;",
            "\t}",
            "\traw_spin_unlock(&base->lock);",
            "",
            "\treturn cmp_next_hrtimer_event(basem, expires);",
            "}",
            "void timer_clear_idle(void)",
            "{",
            "\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);",
            "",
            "\t/*",
            "\t * We do this unlocked. The worst outcome is a remote enqueue sending",
            "\t * a pointless IPI, but taking the lock would just make the window for",
            "\t * sending the IPI a few instructions smaller for the cost of taking",
            "\t * the lock in the exit from idle path.",
            "\t */",
            "\tbase->is_idle = false;",
            "}",
            "static inline void __run_timers(struct timer_base *base)",
            "{",
            "\tstruct hlist_head heads[LVL_DEPTH];",
            "\tint levels;",
            "",
            "\tif (time_before(jiffies, base->next_expiry))",
            "\t\treturn;",
            "",
            "\ttimer_base_lock_expiry(base);",
            "\traw_spin_lock_irq(&base->lock);",
            "",
            "\twhile (time_after_eq(jiffies, base->clk) &&",
            "\t       time_after_eq(jiffies, base->next_expiry)) {",
            "\t\tlevels = collect_expired_timers(base, heads);",
            "\t\t/*",
            "\t\t * The two possible reasons for not finding any expired",
            "\t\t * timer at this clk are that all matching timers have been",
            "\t\t * dequeued or no timer has been queued since",
            "\t\t * base::next_expiry was set to base::clk +",
            "\t\t * NEXT_TIMER_MAX_DELTA.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(!levels && !base->next_expiry_recalc",
            "\t\t\t     && base->timers_pending);",
            "\t\tbase->clk++;",
            "\t\tbase->next_expiry = __next_timer_interrupt(base);",
            "",
            "\t\twhile (levels--)",
            "\t\t\texpire_timers(base, heads + levels);",
            "\t}",
            "\traw_spin_unlock_irq(&base->lock);",
            "\ttimer_base_unlock_expiry(base);",
            "}"
          ],
          "function_name": "cmp_next_hrtimer_event, get_next_timer_interrupt, timer_clear_idle, __run_timers",
          "description": "实现高精度定时器协调逻辑，cmp_next_hrtimer_event比较下一个高精度定时器事件；get_next_timer_interrupt计算下一中断时间；timer_clear_idle清除空闲标记；__run_timers驱动定时器执行流程，处理多级桶中到期定时器。",
          "similarity": 0.48001426458358765
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/time/timer.c",
          "start_line": 460,
          "end_line": 560,
          "content": [
            "unsigned long __round_jiffies_up_relative(unsigned long j, int cpu)",
            "{",
            "\tunsigned long j0 = jiffies;",
            "",
            "\t/* Use j0 because jiffies might change while we run */",
            "\treturn round_jiffies_common(j + j0, cpu, true) - j0;",
            "}",
            "unsigned long round_jiffies_up(unsigned long j)",
            "{",
            "\treturn round_jiffies_common(j, raw_smp_processor_id(), true);",
            "}",
            "unsigned long round_jiffies_up_relative(unsigned long j)",
            "{",
            "\treturn __round_jiffies_up_relative(j, raw_smp_processor_id());",
            "}",
            "static inline unsigned int timer_get_idx(struct timer_list *timer)",
            "{",
            "\treturn (timer->flags & TIMER_ARRAYMASK) >> TIMER_ARRAYSHIFT;",
            "}",
            "static inline void timer_set_idx(struct timer_list *timer, unsigned int idx)",
            "{",
            "\ttimer->flags = (timer->flags & ~TIMER_ARRAYMASK) |",
            "\t\t\tidx << TIMER_ARRAYSHIFT;",
            "}",
            "static inline unsigned calc_index(unsigned long expires, unsigned lvl,",
            "\t\t\t\t  unsigned long *bucket_expiry)",
            "{",
            "",
            "\t/*",
            "\t * The timer wheel has to guarantee that a timer does not fire",
            "\t * early. Early expiry can happen due to:",
            "\t * - Timer is armed at the edge of a tick",
            "\t * - Truncation of the expiry time in the outer wheel levels",
            "\t *",
            "\t * Round up with level granularity to prevent this.",
            "\t */",
            "\texpires = (expires >> LVL_SHIFT(lvl)) + 1;",
            "\t*bucket_expiry = expires << LVL_SHIFT(lvl);",
            "\treturn LVL_OFFS(lvl) + (expires & LVL_MASK);",
            "}",
            "static int calc_wheel_index(unsigned long expires, unsigned long clk,",
            "\t\t\t    unsigned long *bucket_expiry)",
            "{",
            "\tunsigned long delta = expires - clk;",
            "\tunsigned int idx;",
            "",
            "\tif (delta < LVL_START(1)) {",
            "\t\tidx = calc_index(expires, 0, bucket_expiry);",
            "\t} else if (delta < LVL_START(2)) {",
            "\t\tidx = calc_index(expires, 1, bucket_expiry);",
            "\t} else if (delta < LVL_START(3)) {",
            "\t\tidx = calc_index(expires, 2, bucket_expiry);",
            "\t} else if (delta < LVL_START(4)) {",
            "\t\tidx = calc_index(expires, 3, bucket_expiry);",
            "\t} else if (delta < LVL_START(5)) {",
            "\t\tidx = calc_index(expires, 4, bucket_expiry);",
            "\t} else if (delta < LVL_START(6)) {",
            "\t\tidx = calc_index(expires, 5, bucket_expiry);",
            "\t} else if (delta < LVL_START(7)) {",
            "\t\tidx = calc_index(expires, 6, bucket_expiry);",
            "\t} else if (LVL_DEPTH > 8 && delta < LVL_START(8)) {",
            "\t\tidx = calc_index(expires, 7, bucket_expiry);",
            "\t} else if ((long) delta < 0) {",
            "\t\tidx = clk & LVL_MASK;",
            "\t\t*bucket_expiry = clk;",
            "\t} else {",
            "\t\t/*",
            "\t\t * Force expire obscene large timeouts to expire at the",
            "\t\t * capacity limit of the wheel.",
            "\t\t */",
            "\t\tif (delta >= WHEEL_TIMEOUT_CUTOFF)",
            "\t\t\texpires = clk + WHEEL_TIMEOUT_MAX;",
            "",
            "\t\tidx = calc_index(expires, LVL_DEPTH - 1, bucket_expiry);",
            "\t}",
            "\treturn idx;",
            "}",
            "static void",
            "trigger_dyntick_cpu(struct timer_base *base, struct timer_list *timer)",
            "{",
            "\tif (!is_timers_nohz_active())",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * TODO: This wants some optimizing similar to the code below, but we",
            "\t * will do that when we switch from push to pull for deferrable timers.",
            "\t */",
            "\tif (timer->flags & TIMER_DEFERRABLE) {",
            "\t\tif (tick_nohz_full_cpu(base->cpu))",
            "\t\t\twake_up_nohz_cpu(base->cpu);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * We might have to IPI the remote CPU if the base is idle and the",
            "\t * timer is not deferrable. If the other CPU is on the way to idle",
            "\t * then it can't set base->is_idle as we hold the base lock:",
            "\t */",
            "\tif (base->is_idle)",
            "\t\twake_up_nohz_cpu(base->cpu);",
            "}"
          ],
          "function_name": "__round_jiffies_up_relative, round_jiffies_up, round_jiffies_up_relative, timer_get_idx, timer_set_idx, calc_index, calc_wheel_index, trigger_dyntick_cpu",
          "description": "实现定时器层级索引计算逻辑和动态tick触发机制，通过层级间转换规则确定定时器存储位置，处理非活动CPU上的定时器唤醒需求。",
          "similarity": 0.4488794505596161
        }
      ]
    },
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.5422341823577881,
      "chunks": [
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1776,
          "end_line": 1992,
          "content": [
            "static int pick_rt_task(struct rq *rq, struct task_struct *p, int cpu)",
            "{",
            "\tif (!task_on_cpu(rq, p) &&",
            "\t    cpumask_test_cpu(cpu, &p->cpus_mask))",
            "\t\treturn 1;",
            "",
            "\treturn 0;",
            "}",
            "static int find_lowest_rq(struct task_struct *task)",
            "{",
            "\tstruct sched_domain *sd;",
            "\tstruct cpumask *lowest_mask = this_cpu_cpumask_var_ptr(local_cpu_mask);",
            "\tint this_cpu = smp_processor_id();",
            "\tint cpu      = task_cpu(task);",
            "\tint ret;",
            "",
            "\t/* Make sure the mask is initialized first */",
            "\tif (unlikely(!lowest_mask))",
            "\t\treturn -1;",
            "",
            "\tif (task->nr_cpus_allowed == 1)",
            "\t\treturn -1; /* No other targets possible */",
            "",
            "\t/*",
            "\t * If we're on asym system ensure we consider the different capacities",
            "\t * of the CPUs when searching for the lowest_mask.",
            "\t */",
            "\tif (sched_asym_cpucap_active()) {",
            "",
            "\t\tret = cpupri_find_fitness(&task_rq(task)->rd->cpupri,",
            "\t\t\t\t\t  task, lowest_mask,",
            "\t\t\t\t\t  rt_task_fits_capacity);",
            "\t} else {",
            "",
            "\t\tret = cpupri_find(&task_rq(task)->rd->cpupri,",
            "\t\t\t\t  task, lowest_mask);",
            "\t}",
            "",
            "\tif (!ret)",
            "\t\treturn -1; /* No targets found */",
            "",
            "\t/*",
            "\t * At this point we have built a mask of CPUs representing the",
            "\t * lowest priority tasks in the system.  Now we want to elect",
            "\t * the best one based on our affinity and topology.",
            "\t *",
            "\t * We prioritize the last CPU that the task executed on since",
            "\t * it is most likely cache-hot in that location.",
            "\t */",
            "\tif (cpumask_test_cpu(cpu, lowest_mask))",
            "\t\treturn cpu;",
            "",
            "\t/*",
            "\t * Otherwise, we consult the sched_domains span maps to figure",
            "\t * out which CPU is logically closest to our hot cache data.",
            "\t */",
            "\tif (!cpumask_test_cpu(this_cpu, lowest_mask))",
            "\t\tthis_cpu = -1; /* Skip this_cpu opt if not among lowest */",
            "",
            "\trcu_read_lock();",
            "\tfor_each_domain(cpu, sd) {",
            "\t\tif (sd->flags & SD_WAKE_AFFINE) {",
            "\t\t\tint best_cpu;",
            "",
            "\t\t\t/*",
            "\t\t\t * \"this_cpu\" is cheaper to preempt than a",
            "\t\t\t * remote processor.",
            "\t\t\t */",
            "\t\t\tif (this_cpu != -1 &&",
            "\t\t\t    cpumask_test_cpu(this_cpu, sched_domain_span(sd))) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn this_cpu;",
            "\t\t\t}",
            "",
            "\t\t\tbest_cpu = cpumask_any_and_distribute(lowest_mask,",
            "\t\t\t\t\t\t\t      sched_domain_span(sd));",
            "\t\t\tif (best_cpu < nr_cpu_ids) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn best_cpu;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * And finally, if there were no matches within the domains",
            "\t * just give the caller *something* to work with from the compatible",
            "\t * locations.",
            "\t */",
            "\tif (this_cpu != -1)",
            "\t\treturn this_cpu;",
            "",
            "\tcpu = cpumask_any_distribute(lowest_mask);",
            "\tif (cpu < nr_cpu_ids)",
            "\t\treturn cpu;",
            "",
            "\treturn -1;",
            "}",
            "static int push_rt_task(struct rq *rq, bool pull)",
            "{",
            "\tstruct task_struct *next_task;",
            "\tstruct rq *lowest_rq;",
            "\tint ret = 0;",
            "",
            "\tif (!rq->rt.overloaded)",
            "\t\treturn 0;",
            "",
            "\tnext_task = pick_next_pushable_task(rq);",
            "\tif (!next_task)",
            "\t\treturn 0;",
            "",
            "retry:",
            "\t/*",
            "\t * It's possible that the next_task slipped in of",
            "\t * higher priority than current. If that's the case",
            "\t * just reschedule current.",
            "\t */",
            "\tif (unlikely(next_task->prio < rq->curr->prio)) {",
            "\t\tresched_curr(rq);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (is_migration_disabled(next_task)) {",
            "\t\tstruct task_struct *push_task = NULL;",
            "\t\tint cpu;",
            "",
            "\t\tif (!pull || rq->push_busy)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * Invoking find_lowest_rq() on anything but an RT task doesn't",
            "\t\t * make sense. Per the above priority check, curr has to",
            "\t\t * be of higher priority than next_task, so no need to",
            "\t\t * reschedule when bailing out.",
            "\t\t *",
            "\t\t * Note that the stoppers are masqueraded as SCHED_FIFO",
            "\t\t * (cf. sched_set_stop_task()), so we can't rely on rt_task().",
            "\t\t */",
            "\t\tif (rq->curr->sched_class != &rt_sched_class)",
            "\t\t\treturn 0;",
            "",
            "\t\tcpu = find_lowest_rq(rq->curr);",
            "\t\tif (cpu == -1 || cpu == rq->cpu)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * Given we found a CPU with lower priority than @next_task,",
            "\t\t * therefore it should be running. However we cannot migrate it",
            "\t\t * to this other CPU, instead attempt to push the current",
            "\t\t * running task on this CPU away.",
            "\t\t */",
            "\t\tpush_task = get_push_task(rq);",
            "\t\tif (push_task) {",
            "\t\t\tpreempt_disable();",
            "\t\t\traw_spin_rq_unlock(rq);",
            "\t\t\tstop_one_cpu_nowait(rq->cpu, push_cpu_stop,",
            "\t\t\t\t\t    push_task, &rq->push_work);",
            "\t\t\tpreempt_enable();",
            "\t\t\traw_spin_rq_lock(rq);",
            "\t\t}",
            "",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (WARN_ON(next_task == rq->curr))",
            "\t\treturn 0;",
            "",
            "\t/* We might release rq lock */",
            "\tget_task_struct(next_task);",
            "",
            "\t/* find_lock_lowest_rq locks the rq if found */",
            "\tlowest_rq = find_lock_lowest_rq(next_task, rq);",
            "\tif (!lowest_rq) {",
            "\t\tstruct task_struct *task;",
            "\t\t/*",
            "\t\t * find_lock_lowest_rq releases rq->lock",
            "\t\t * so it is possible that next_task has migrated.",
            "\t\t *",
            "\t\t * We need to make sure that the task is still on the same",
            "\t\t * run-queue and is also still the next task eligible for",
            "\t\t * pushing.",
            "\t\t */",
            "\t\ttask = pick_next_pushable_task(rq);",
            "\t\tif (task == next_task) {",
            "\t\t\t/*",
            "\t\t\t * The task hasn't migrated, and is still the next",
            "\t\t\t * eligible task, but we failed to find a run-queue",
            "\t\t\t * to push it to.  Do not retry in this case, since",
            "\t\t\t * other CPUs will pull from us when ready.",
            "\t\t\t */",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tif (!task)",
            "\t\t\t/* No more tasks, just exit */",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * Something has shifted, try again.",
            "\t\t */",
            "\t\tput_task_struct(next_task);",
            "\t\tnext_task = task;",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tdeactivate_task(rq, next_task, 0);",
            "\tset_task_cpu(next_task, lowest_rq->cpu);",
            "\tactivate_task(lowest_rq, next_task, 0);",
            "\tresched_curr(lowest_rq);",
            "\tret = 1;",
            "",
            "\tdouble_unlock_balance(rq, lowest_rq);",
            "out:",
            "\tput_task_struct(next_task);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "pick_rt_task, find_lowest_rq, push_rt_task",
          "description": "实现实时任务选择算法、低优先级CPU搜索及强制迁移逻辑，支持异构系统下的能效优化和拓扑感知调度。",
          "similarity": 0.5282241702079773
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/rt.c",
          "start_line": 776,
          "end_line": 913,
          "content": [
            "static void balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tif (!sched_feat(RT_RUNTIME_SHARE))",
            "\t\treturn;",
            "",
            "\tif (rt_rq->rt_time > rt_rq->rt_runtime) {",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tdo_balance_runtime(rt_rq);",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t}",
            "}",
            "static inline void balance_runtime(struct rt_rq *rt_rq) {}",
            "static int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun)",
            "{",
            "\tint i, idle = 1, throttled = 0;",
            "\tconst struct cpumask *span;",
            "",
            "\tspan = sched_rt_period_mask();",
            "",
            "\t/*",
            "\t * FIXME: isolated CPUs should really leave the root task group,",
            "\t * whether they are isolcpus or were isolated via cpusets, lest",
            "\t * the timer run on a CPU which does not service all runqueues,",
            "\t * potentially leaving other CPUs indefinitely throttled.  If",
            "\t * isolation is really required, the user will turn the throttle",
            "\t * off to kill the perturbations it causes anyway.  Meanwhile,",
            "\t * this maintains functionality for boot and/or troubleshooting.",
            "\t */",
            "\tif (rt_b == &root_task_group.rt_bandwidth)",
            "\t\tspan = cpu_online_mask;",
            "",
            "\tfor_each_cpu(i, span) {",
            "\t\tint enqueue = 0;",
            "\t\tstruct rt_rq *rt_rq = sched_rt_period_rt_rq(rt_b, i);",
            "\t\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\t\tstruct rq_flags rf;",
            "\t\tint skip;",
            "",
            "\t\t/*",
            "\t\t * When span == cpu_online_mask, taking each rq->lock",
            "\t\t * can be time-consuming. Try to avoid it when possible.",
            "\t\t */",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\tif (!sched_feat(RT_RUNTIME_SHARE) && rt_rq->rt_runtime != RUNTIME_INF)",
            "\t\t\trt_rq->rt_runtime = rt_b->rt_runtime;",
            "\t\tskip = !rt_rq->rt_time && !rt_rq->rt_nr_running;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tif (skip)",
            "\t\t\tcontinue;",
            "",
            "\t\trq_lock(rq, &rf);",
            "\t\tupdate_rq_clock(rq);",
            "",
            "\t\tif (rt_rq->rt_time) {",
            "\t\t\tu64 runtime;",
            "",
            "\t\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t\tif (rt_rq->rt_throttled)",
            "\t\t\t\tbalance_runtime(rt_rq);",
            "\t\t\truntime = rt_rq->rt_runtime;",
            "\t\t\trt_rq->rt_time -= min(rt_rq->rt_time, overrun*runtime);",
            "\t\t\tif (rt_rq->rt_throttled && rt_rq->rt_time < runtime) {",
            "\t\t\t\trt_rq->rt_throttled = 0;",
            "\t\t\t\tenqueue = 1;",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * When we're idle and a woken (rt) task is",
            "\t\t\t\t * throttled wakeup_preempt() will set",
            "\t\t\t\t * skip_update and the time between the wakeup",
            "\t\t\t\t * and this unthrottle will get accounted as",
            "\t\t\t\t * 'runtime'.",
            "\t\t\t\t */",
            "\t\t\t\tif (rt_rq->rt_nr_running && rq->curr == rq->idle)",
            "\t\t\t\t\trq_clock_cancel_skipupdate(rq);",
            "\t\t\t}",
            "\t\t\tif (rt_rq->rt_time || rt_rq->rt_nr_running)",
            "\t\t\t\tidle = 0;",
            "\t\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\t} else if (rt_rq->rt_nr_running) {",
            "\t\t\tidle = 0;",
            "\t\t\tif (!rt_rq_throttled(rt_rq))",
            "\t\t\t\tenqueue = 1;",
            "\t\t}",
            "\t\tif (rt_rq->rt_throttled)",
            "\t\t\tthrottled = 1;",
            "",
            "\t\tif (enqueue)",
            "\t\t\tsched_rt_rq_enqueue(rt_rq);",
            "\t\trq_unlock(rq, &rf);",
            "\t}",
            "",
            "\tif (!throttled && (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF))",
            "\t\treturn 1;",
            "",
            "\treturn idle;",
            "}",
            "static int sched_rt_runtime_exceeded(struct rt_rq *rt_rq)",
            "{",
            "\tu64 runtime = sched_rt_runtime(rt_rq);",
            "",
            "\tif (rt_rq->rt_throttled)",
            "\t\treturn rt_rq_throttled(rt_rq);",
            "",
            "\tif (runtime >= sched_rt_period(rt_rq))",
            "\t\treturn 0;",
            "",
            "\tbalance_runtime(rt_rq);",
            "\truntime = sched_rt_runtime(rt_rq);",
            "\tif (runtime == RUNTIME_INF)",
            "\t\treturn 0;",
            "",
            "\tif (rt_rq->rt_time > runtime) {",
            "\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\t\t/*",
            "\t\t * Don't actually throttle groups that have no runtime assigned",
            "\t\t * but accrue some time due to boosting.",
            "\t\t */",
            "\t\tif (likely(rt_b->rt_runtime)) {",
            "\t\t\trt_rq->rt_throttled = 1;",
            "\t\t\tprintk_deferred_once(\"sched: RT throttling activated\\n\");",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * In case we did anyway, make it go away,",
            "\t\t\t * replenishment is a joke, since it will replenish us",
            "\t\t\t * with exactly 0 ns.",
            "\t\t\t */",
            "\t\t\trt_rq->rt_time = 0;",
            "\t\t}",
            "",
            "\t\tif (rt_rq_throttled(rt_rq)) {",
            "\t\t\tsched_rt_rq_dequeue(rt_rq);",
            "\t\t\treturn 1;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "balance_runtime, balance_runtime, do_sched_rt_period_timer, sched_rt_runtime_exceeded",
          "description": "`balance_runtime`在超时时触发重新平衡，`do_sched_rt_period_timer`周期性调整运行时并检查节流状态，`sched_rt_runtime_exceeded`判断是否超出运行时限制并标记节流",
          "similarity": 0.5242979526519775
        },
        {
          "chunk_id": 17,
          "file_path": "kernel/sched/rt.c",
          "start_line": 2533,
          "end_line": 2686,
          "content": [
            "static void watchdog(struct rq *rq, struct task_struct *p)",
            "{",
            "\tunsigned long soft, hard;",
            "",
            "\t/* max may change after cur was read, this will be fixed next tick */",
            "\tsoft = task_rlimit(p, RLIMIT_RTTIME);",
            "\thard = task_rlimit_max(p, RLIMIT_RTTIME);",
            "",
            "\tif (soft != RLIM_INFINITY) {",
            "\t\tunsigned long next;",
            "",
            "\t\tif (p->rt.watchdog_stamp != jiffies) {",
            "\t\t\tp->rt.timeout++;",
            "\t\t\tp->rt.watchdog_stamp = jiffies;",
            "\t\t}",
            "",
            "\t\tnext = DIV_ROUND_UP(min(soft, hard), USEC_PER_SEC/HZ);",
            "\t\tif (p->rt.timeout > next) {",
            "\t\t\tposix_cputimers_rt_watchdog(&p->posix_cputimers,",
            "\t\t\t\t\t\t    p->se.sum_exec_runtime);",
            "\t\t}",
            "\t}",
            "}",
            "static inline void watchdog(struct rq *rq, struct task_struct *p) { }",
            "static void task_tick_rt(struct rq *rq, struct task_struct *p, int queued)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tupdate_rt_rq_load_avg(rq_clock_pelt(rq), rq, 1);",
            "",
            "\twatchdog(rq, p);",
            "",
            "\t/*",
            "\t * RR tasks need a special form of timeslice management.",
            "\t * FIFO tasks have no timeslices.",
            "\t */",
            "\tif (p->policy != SCHED_RR)",
            "\t\treturn;",
            "",
            "\tif (--p->rt.time_slice)",
            "\t\treturn;",
            "",
            "\tp->rt.time_slice = sched_rr_timeslice;",
            "",
            "\t/*",
            "\t * Requeue to the end of queue if we (and all of our ancestors) are not",
            "\t * the only element on the queue",
            "\t */",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tif (rt_se->run_list.prev != rt_se->run_list.next) {",
            "\t\t\trequeue_task_rt(rq, p, 0);",
            "\t\t\tresched_curr(rq);",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "}",
            "static unsigned int get_rr_interval_rt(struct rq *rq, struct task_struct *task)",
            "{",
            "\t/*",
            "\t * Time slice is 0 for SCHED_FIFO tasks",
            "\t */",
            "\tif (task->policy == SCHED_RR)",
            "\t\treturn sched_rr_timeslice;",
            "\telse",
            "\t\treturn 0;",
            "}",
            "static int task_is_throttled_rt(struct task_struct *p, int cpu)",
            "{",
            "\tstruct rt_rq *rt_rq;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\trt_rq = task_group(p)->rt_rq[cpu];",
            "#else",
            "\trt_rq = &cpu_rq(cpu)->rt;",
            "#endif",
            "",
            "\treturn rt_rq_throttled(rt_rq);",
            "}",
            "static inline int tg_has_rt_tasks(struct task_group *tg)",
            "{",
            "\tstruct task_struct *task;",
            "\tstruct css_task_iter it;",
            "\tint ret = 0;",
            "",
            "\t/*",
            "\t * Autogroups do not have RT tasks; see autogroup_create().",
            "\t */",
            "\tif (task_group_is_autogroup(tg))",
            "\t\treturn 0;",
            "",
            "\tcss_task_iter_start(&tg->css, 0, &it);",
            "\twhile (!ret && (task = css_task_iter_next(&it)))",
            "\t\tret |= rt_task(task);",
            "\tcss_task_iter_end(&it);",
            "",
            "\treturn ret;",
            "}",
            "static int tg_rt_schedulable(struct task_group *tg, void *data)",
            "{",
            "\tstruct rt_schedulable_data *d = data;",
            "\tstruct task_group *child;",
            "\tunsigned long total, sum = 0;",
            "\tu64 period, runtime;",
            "",
            "\tperiod = ktime_to_ns(tg->rt_bandwidth.rt_period);",
            "\truntime = tg->rt_bandwidth.rt_runtime;",
            "",
            "\tif (tg == d->tg) {",
            "\t\tperiod = d->rt_period;",
            "\t\truntime = d->rt_runtime;",
            "\t}",
            "",
            "\t/*",
            "\t * Cannot have more runtime than the period.",
            "\t */",
            "\tif (runtime > period && runtime != RUNTIME_INF)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Ensure we don't starve existing RT tasks if runtime turns zero.",
            "\t */",
            "\tif (rt_bandwidth_enabled() && !runtime &&",
            "\t    tg->rt_bandwidth.rt_runtime && tg_has_rt_tasks(tg))",
            "\t\treturn -EBUSY;",
            "",
            "\ttotal = to_ratio(period, runtime);",
            "",
            "\t/*",
            "\t * Nobody can have more than the global setting allows.",
            "\t */",
            "\tif (total > to_ratio(global_rt_period(), global_rt_runtime()))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * The sum of our children's runtime should not exceed our own.",
            "\t */",
            "\tlist_for_each_entry_rcu(child, &tg->children, siblings) {",
            "\t\tperiod = ktime_to_ns(child->rt_bandwidth.rt_period);",
            "\t\truntime = child->rt_bandwidth.rt_runtime;",
            "",
            "\t\tif (child == d->tg) {",
            "\t\t\tperiod = d->rt_period;",
            "\t\t\truntime = d->rt_runtime;",
            "\t\t}",
            "",
            "\t\tsum += to_ratio(period, runtime);",
            "\t}",
            "",
            "\tif (sum > total)",
            "\t\treturn -EINVAL;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "watchdog, watchdog, task_tick_rt, get_rr_interval_rt, task_is_throttled_rt, tg_has_rt_tasks, tg_rt_schedulable",
          "description": "实现实时任务的超时监控、时间片管理、任务组资源配额验证及实时任务运行时间统计功能，支持轮转调度下的时间片重置和任务重排。",
          "similarity": 0.5034276247024536
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1449,
          "end_line": 1589,
          "content": [
            "static void dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rq *rq = rq_of_rt_se(rt_se);",
            "",
            "\tupdate_stats_dequeue_rt(rt_rq_of_se(rt_se), rt_se, flags);",
            "",
            "\tdequeue_rt_stack(rt_se, flags);",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\t\tif (rt_rq && rt_rq->rt_nr_running)",
            "\t\t\t__enqueue_rt_entity(rt_se, flags);",
            "\t}",
            "\tenqueue_top_rt_rq(&rq->rt);",
            "}",
            "static void",
            "enqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tif (flags & ENQUEUE_WAKEUP)",
            "\t\trt_se->timeout = 0;",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_rt(rt_rq_of_se(rt_se), rt_se);",
            "",
            "\tenqueue_rt_entity(rt_se, flags);",
            "",
            "\tif (!task_current(rq, p) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_task(rq, p);",
            "}",
            "static bool dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tdequeue_rt_entity(rt_se, flags);",
            "",
            "\tdequeue_pushable_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void",
            "requeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)",
            "{",
            "\tif (on_rt_rq(rt_se)) {",
            "\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "\t\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);",
            "",
            "\t\tif (head)",
            "\t\t\tlist_move(&rt_se->run_list, queue);",
            "\t\telse",
            "\t\t\tlist_move_tail(&rt_se->run_list, queue);",
            "\t}",
            "}",
            "static void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\trt_rq = rt_rq_of_se(rt_se);",
            "\t\trequeue_rt_entity(rt_rq, rt_se, head);",
            "\t}",
            "}",
            "static void yield_task_rt(struct rq *rq)",
            "{",
            "\trequeue_task_rt(rq, rq->curr, 0);",
            "}",
            "static int",
            "select_task_rq_rt(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tstruct rq *rq;",
            "\tbool test;",
            "",
            "\t/* For anything but wake ups, just return the task_cpu */",
            "\tif (!(flags & (WF_TTWU | WF_FORK)))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If the current task on @p's runqueue is an RT task, then",
            "\t * try to see if we can wake this RT task up on another",
            "\t * runqueue. Otherwise simply start this RT task",
            "\t * on its current runqueue.",
            "\t *",
            "\t * We want to avoid overloading runqueues. If the woken",
            "\t * task is a higher priority, then it will stay on this CPU",
            "\t * and the lower prio task should be moved to another CPU.",
            "\t * Even though this will probably make the lower prio task",
            "\t * lose its cache, we do not want to bounce a higher task",
            "\t * around just because it gave up its CPU, perhaps for a",
            "\t * lock?",
            "\t *",
            "\t * For equal prio tasks, we just let the scheduler sort it out.",
            "\t *",
            "\t * Otherwise, just let it ride on the affined RQ and the",
            "\t * post-schedule router will push the preempted task away",
            "\t *",
            "\t * This test is optimistic, if we get it wrong the load-balancer",
            "\t * will have to sort it out.",
            "\t *",
            "\t * We take into account the capacity of the CPU to ensure it fits the",
            "\t * requirement of the task - which is only important on heterogeneous",
            "\t * systems like big.LITTLE.",
            "\t */",
            "\ttest = curr &&",
            "\t       unlikely(rt_task(curr)) &&",
            "\t       (curr->nr_cpus_allowed < 2 || curr->prio <= p->prio);",
            "",
            "\tif (test || !rt_task_fits_capacity(p, cpu)) {",
            "\t\tint target = find_lowest_rq(p);",
            "",
            "\t\t/*",
            "\t\t * Bail out if we were forcing a migration to find a better",
            "\t\t * fitting CPU but our search failed.",
            "\t\t */",
            "\t\tif (!test && target != -1 && !rt_task_fits_capacity(p, target))",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\t/*",
            "\t\t * Don't bother moving it if the destination CPU is",
            "\t\t * not running a lower priority task.",
            "\t\t */",
            "\t\tif (target != -1 &&",
            "\t\t    p->prio < cpu_rq(target)->rt.highest_prio.curr)",
            "\t\t\tcpu = target;",
            "\t}",
            "",
            "out_unlock:",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "dequeue_rt_entity, enqueue_task_rt, dequeue_task_rt, requeue_rt_entity, requeue_task_rt, yield_task_rt, select_task_rq_rt",
          "description": "实现实时任务的出队逻辑、唤醒和迁移策略，提供CPU亲和性选择及负载均衡支持，维护优先级队列的动态调整。",
          "similarity": 0.5011855363845825
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/rt.c",
          "start_line": 529,
          "end_line": 633,
          "content": [
            "static void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\tstruct sched_rt_entity *rt_se;",
            "",
            "\tint cpu = cpu_of(rq);",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tif (!rt_se)",
            "\t\t\tenqueue_top_rt_rq(rt_rq);",
            "\t\telse if (!on_rt_rq(rt_se))",
            "\t\t\tenqueue_rt_entity(rt_se, 0);",
            "",
            "\t\tif (rt_rq->highest_prio.curr < curr->prio)",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "}",
            "static void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (!rt_se) {",
            "\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "\t\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);",
            "\t}",
            "\telse if (on_rt_rq(rt_se))",
            "\t\tdequeue_rt_entity(rt_se, 0);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;",
            "}",
            "static int rt_se_boosted(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "\tstruct task_struct *p;",
            "",
            "\tif (rt_rq)",
            "\t\treturn !!rt_rq->rt_nr_boosted;",
            "",
            "\tp = rt_task_of(rt_se);",
            "\treturn p->prio != p->normal_prio;",
            "}",
            "bool sched_rt_bandwidth_account(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\treturn (hrtimer_active(&rt_b->rt_period_timer) ||",
            "\t\trt_rq->rt_time < rt_b->rt_runtime);",
            "}",
            "static void do_balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;",
            "\tint i, weight;",
            "\tu64 rt_period;",
            "",
            "\tweight = cpumask_weight(rd->span);",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\trt_period = ktime_to_ns(rt_b->rt_period);",
            "\tfor_each_cpu(i, rd->span) {",
            "\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\ts64 diff;",
            "",
            "\t\tif (iter == rt_rq)",
            "\t\t\tcontinue;",
            "",
            "\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either all rqs have inf runtime and there's nothing to steal",
            "\t\t * or __disable_runtime() below sets a specific rq to inf to",
            "\t\t * indicate its been disabled and disallow stealing.",
            "\t\t */",
            "\t\tif (iter->rt_runtime == RUNTIME_INF)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * From runqueues with spare time, take 1/n part of their",
            "\t\t * spare time, but no more than our period.",
            "\t\t */",
            "\t\tdiff = iter->rt_runtime - iter->rt_time;",
            "\t\tif (diff > 0) {",
            "\t\t\tdiff = div_u64((u64)diff, weight);",
            "\t\t\tif (rt_rq->rt_runtime + diff > rt_period)",
            "\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;",
            "\t\t\titer->rt_runtime -= diff;",
            "\t\t\trt_rq->rt_runtime += diff;",
            "\t\t\tif (rt_rq->rt_runtime == rt_period) {",
            "\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "next:",
            "\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, rt_se_boosted, sched_rt_bandwidth_account, do_balance_runtime",
          "description": "实现实时任务队列的插入/移除逻辑，跟踪运行时间消耗，通过跨CPU运行时间平衡算法实现带宽公平分配。",
          "similarity": 0.49527865648269653
        }
      ]
    }
  ]
}