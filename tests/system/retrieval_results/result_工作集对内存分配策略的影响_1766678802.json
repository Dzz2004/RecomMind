{
  "query": "工作集对内存分配策略的影响",
  "timestamp": "2025-12-26 00:06:42",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.6100462675094604,
      "chunks": [
        {
          "chunk_id": 12,
          "file_path": "mm/mempolicy.c",
          "start_line": 2024,
          "end_line": 2135,
          "content": [
            "static unsigned int interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int nid;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "\t/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnid = next_node_in(current->il_prev, policy->nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\tif (nid < MAX_NUMNODES)",
            "\t\tcurrent->il_prev = nid;",
            "\treturn nid;",
            "}",
            "unsigned int mempolicy_slab_node(void)",
            "{",
            "\tstruct mempolicy *policy;",
            "\tint node = numa_mem_id();",
            "",
            "\tif (!in_task())",
            "\t\treturn node;",
            "",
            "\tpolicy = current->mempolicy;",
            "\tif (!policy)",
            "\t\treturn node;",
            "",
            "\tswitch (policy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\treturn first_node(policy->nodes);",
            "",
            "\tcase MPOL_INTERLEAVE:",
            "\t\treturn interleave_nodes(policy);",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn weighted_interleave_nodes(policy);",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t{",
            "\t\tstruct zoneref *z;",
            "",
            "\t\t/*",
            "\t\t * Follow bind policy behavior and start allocation at the",
            "\t\t * first node.",
            "\t\t */",
            "\t\tstruct zonelist *zonelist;",
            "\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);",
            "\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];",
            "\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,",
            "\t\t\t\t\t\t\t&policy->nodes);",
            "\t\treturn z->zone ? zone_to_nid(z->zone) : node;",
            "\t}",
            "\tcase MPOL_LOCAL:",
            "\t\treturn node;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static unsigned int read_once_policy_nodemask(struct mempolicy *pol,",
            "\t\t\t\t\t      nodemask_t *mask)",
            "{",
            "\t/*",
            "\t * barrier stabilizes the nodemask locally so that it can be iterated",
            "\t * over safely without concern for changes. Allocators validate node",
            "\t * selection does not violate mems_allowed, so this is safe.",
            "\t */",
            "\tbarrier();",
            "\tmemcpy(mask, &pol->nodes, sizeof(nodemask_t));",
            "\tbarrier();",
            "\treturn nodes_weight(*mask);",
            "}",
            "static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nr_nodes;",
            "\tu8 *table = NULL;",
            "\tunsigned int weight_total = 0;",
            "\tu8 weight;",
            "\tint nid = 0;",
            "",
            "\tnr_nodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nr_nodes)",
            "\t\treturn numa_node_id();",
            "",
            "\trcu_read_lock();",
            "",
            "\tstate = rcu_dereference(wi_state);",
            "\t/* Uninitialized wi_state means we should assume all weights are 1 */",
            "\tif (state)",
            "\t\ttable = state->iw_table;",
            "",
            "\t/* calculate the total weight */",
            "\tfor_each_node_mask(nid, nodemask)",
            "\t\tweight_total += table ? table[nid] : 1;",
            "",
            "\t/* Calculate the node offset based on totals */",
            "\ttarget = ilx % weight_total;",
            "\tnid = first_node(nodemask);",
            "\twhile (target) {",
            "\t\t/* detect system default usage */",
            "\t\tweight = table ? table[nid] : 1;",
            "\t\tif (target < weight)",
            "\t\t\tbreak;",
            "\t\ttarget -= weight;",
            "\t\tnid = next_node_in(nid, nodemask);",
            "\t}",
            "\trcu_read_unlock();",
            "\treturn nid;",
            "}"
          ],
          "function_name": "interleave_nodes, mempolicy_slab_node, read_once_policy_nodemask, weighted_interleave_nid",
          "description": "interleave_nodes 计算交错分配的下一个节点；mempolicy_slab_node 根据内存策略返回Slab分配的节点；read_once_policy_nodemask 安全读取策略节点掩码；weighted_interleave_nid 基于权重计算加权交错分配的目标节点。",
          "similarity": 0.5987926125526428
        },
        {
          "chunk_id": 13,
          "file_path": "mm/mempolicy.c",
          "start_line": 2149,
          "end_line": 2255,
          "content": [
            "static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nnodes;",
            "\tint i;",
            "\tint nid;",
            "",
            "\tnnodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nnodes)",
            "\t\treturn numa_node_id();",
            "\ttarget = ilx % nnodes;",
            "\tnid = first_node(nodemask);",
            "\tfor (i = 0; i < target; i++)",
            "\t\tnid = next_node(nid, nodemask);",
            "\treturn nid;",
            "}",
            "int huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,",
            "\t\tstruct mempolicy **mpol, nodemask_t **nodemask)",
            "{",
            "\tpgoff_t ilx;",
            "\tint nid;",
            "",
            "\tnid = numa_node_id();",
            "\t*mpol = get_vma_policy(vma, addr, hstate_vma(vma)->order, &ilx);",
            "\t*nodemask = policy_nodemask(gfp_flags, *mpol, ilx, &nid);",
            "\treturn nid;",
            "}",
            "bool init_nodemask_of_mempolicy(nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "",
            "\tif (!(mask && current->mempolicy))",
            "\t\treturn false;",
            "",
            "\ttask_lock(current);",
            "\tmempolicy = current->mempolicy;",
            "\tswitch (mempolicy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*mask = mempolicy->nodes;",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tinit_nodemask_of_node(mask, numa_node_id());",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "\ttask_unlock(current);",
            "",
            "\treturn true;",
            "}",
            "bool mempolicy_in_oom_domain(struct task_struct *tsk,",
            "\t\t\t\t\tconst nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "\tbool ret = true;",
            "",
            "\tif (!mask)",
            "\t\treturn ret;",
            "",
            "\ttask_lock(tsk);",
            "\tmempolicy = tsk->mempolicy;",
            "\tif (mempolicy && mempolicy->mode == MPOL_BIND)",
            "\t\tret = nodes_intersects(mempolicy->nodes, *mask);",
            "\ttask_unlock(tsk);",
            "",
            "\treturn ret;",
            "}",
            "static unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tint nodes;",
            "\tunsigned long nr_pages_per_node;",
            "\tint delta;",
            "\tint i;",
            "\tunsigned long nr_allocated;",
            "\tunsigned long total_allocated = 0;",
            "",
            "\tnodes = nodes_weight(pol->nodes);",
            "\tnr_pages_per_node = nr_pages / nodes;",
            "\tdelta = nr_pages - nodes * nr_pages_per_node;",
            "",
            "\tfor (i = 0; i < nodes; i++) {",
            "\t\tif (delta) {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node + 1, NULL,",
            "\t\t\t\t\tpage_array);",
            "\t\t\tdelta--;",
            "\t\t} else {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node, NULL, page_array);",
            "\t\t}",
            "",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t}",
            "",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "interleave_nid, huge_node, init_nodemask_of_mempolicy, mempolicy_in_oom_domain, alloc_pages_bulk_array_interleave",
          "description": "interleave_nid 计算简单交错分配的目标节点；huge_node 结合HugeTLB策略确定大页分配节点；init_nodemask_of_mempolicy 初始化当前进程的内存策略节点掩码；mempolicy_in_oom_domain 检查策略节点是否与OOM域重叠；alloc_pages_bulk_array_interleave 执行批量交错分配。",
          "similarity": 0.5964548587799072
        },
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.591476321220398
        },
        {
          "chunk_id": 17,
          "file_path": "mm/mempolicy.c",
          "start_line": 2969,
          "end_line": 3093,
          "content": [
            "void mpol_put_task_policy(struct task_struct *task)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\ttask_lock(task);",
            "\tpol = task->mempolicy;",
            "\ttask->mempolicy = NULL;",
            "\ttask_unlock(task);",
            "\tmpol_put(pol);",
            "}",
            "static void sp_delete(struct shared_policy *sp, struct sp_node *n)",
            "{",
            "\trb_erase(&n->nd, &sp->root);",
            "\tsp_free(n);",
            "}",
            "static void sp_node_init(struct sp_node *node, unsigned long start,",
            "\t\t\tunsigned long end, struct mempolicy *pol)",
            "{",
            "\tnode->start = start;",
            "\tnode->end = end;",
            "\tnode->policy = pol;",
            "}",
            "static int shared_policy_replace(struct shared_policy *sp, pgoff_t start,",
            "\t\t\t\t pgoff_t end, struct sp_node *new)",
            "{",
            "\tstruct sp_node *n;",
            "\tstruct sp_node *n_new = NULL;",
            "\tstruct mempolicy *mpol_new = NULL;",
            "\tint ret = 0;",
            "",
            "restart:",
            "\twrite_lock(&sp->lock);",
            "\tn = sp_lookup(sp, start, end);",
            "\t/* Take care of old policies in the same range. */",
            "\twhile (n && n->start < end) {",
            "\t\tstruct rb_node *next = rb_next(&n->nd);",
            "\t\tif (n->start >= start) {",
            "\t\t\tif (n->end <= end)",
            "\t\t\t\tsp_delete(sp, n);",
            "\t\t\telse",
            "\t\t\t\tn->start = end;",
            "\t\t} else {",
            "\t\t\t/* Old policy spanning whole new range. */",
            "\t\t\tif (n->end > end) {",
            "\t\t\t\tif (!n_new)",
            "\t\t\t\t\tgoto alloc_new;",
            "",
            "\t\t\t\t*mpol_new = *n->policy;",
            "\t\t\t\tatomic_set(&mpol_new->refcnt, 1);",
            "\t\t\t\tsp_node_init(n_new, end, n->end, mpol_new);",
            "\t\t\t\tn->end = start;",
            "\t\t\t\tsp_insert(sp, n_new);",
            "\t\t\t\tn_new = NULL;",
            "\t\t\t\tmpol_new = NULL;",
            "\t\t\t\tbreak;",
            "\t\t\t} else",
            "\t\t\t\tn->end = start;",
            "\t\t}",
            "\t\tif (!next)",
            "\t\t\tbreak;",
            "\t\tn = rb_entry(next, struct sp_node, nd);",
            "\t}",
            "\tif (new)",
            "\t\tsp_insert(sp, new);",
            "\twrite_unlock(&sp->lock);",
            "\tret = 0;",
            "",
            "err_out:",
            "\tif (mpol_new)",
            "\t\tmpol_put(mpol_new);",
            "\tif (n_new)",
            "\t\tkmem_cache_free(sn_cache, n_new);",
            "",
            "\treturn ret;",
            "",
            "alloc_new:",
            "\twrite_unlock(&sp->lock);",
            "\tret = -ENOMEM;",
            "\tn_new = kmem_cache_alloc(sn_cache, GFP_KERNEL);",
            "\tif (!n_new)",
            "\t\tgoto err_out;",
            "\tmpol_new = kmem_cache_alloc(policy_cache, GFP_KERNEL);",
            "\tif (!mpol_new)",
            "\t\tgoto err_out;",
            "\tatomic_set(&mpol_new->refcnt, 1);",
            "\tgoto restart;",
            "}",
            "void mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol)",
            "{",
            "\tint ret;",
            "",
            "\tsp->root = RB_ROOT;\t\t/* empty tree == default mempolicy */",
            "\trwlock_init(&sp->lock);",
            "",
            "\tif (mpol) {",
            "\t\tstruct sp_node *sn;",
            "\t\tstruct mempolicy *npol;",
            "\t\tNODEMASK_SCRATCH(scratch);",
            "",
            "\t\tif (!scratch)",
            "\t\t\tgoto put_mpol;",
            "",
            "\t\t/* contextualize the tmpfs mount point mempolicy to this file */",
            "\t\tnpol = mpol_new(mpol->mode, mpol->flags, &mpol->w.user_nodemask);",
            "\t\tif (IS_ERR(npol))",
            "\t\t\tgoto free_scratch; /* no valid nodemask intersection */",
            "",
            "\t\ttask_lock(current);",
            "\t\tret = mpol_set_nodemask(npol, &mpol->w.user_nodemask, scratch);",
            "\t\ttask_unlock(current);",
            "\t\tif (ret)",
            "\t\t\tgoto put_npol;",
            "",
            "\t\t/* alloc node covering entire file; adds ref to file's npol */",
            "\t\tsn = sp_alloc(0, MAX_LFS_FILESIZE >> PAGE_SHIFT, npol);",
            "\t\tif (sn)",
            "\t\t\tsp_insert(sp, sn);",
            "put_npol:",
            "\t\tmpol_put(npol);\t/* drop initial ref on file's npol */",
            "free_scratch:",
            "\t\tNODEMASK_SCRATCH_FREE(scratch);",
            "put_mpol:",
            "\t\tmpol_put(mpol);\t/* drop our incoming ref on sb mpol */",
            "\t}",
            "}"
          ],
          "function_name": "mpol_put_task_policy, sp_delete, sp_node_init, shared_policy_replace, mpol_shared_policy_init",
          "description": "mpol_put_task_policy释放任务级内存策略引用，sp_delete从RB树删除节点并回收资源，sp_node_init初始化共享策略节点，shared_policy_replace替换共享策略区间并处理节点分裂，mpol_shared_policy_init初始化共享策略结构体并设置初始策略。",
          "similarity": 0.574564516544342
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mempolicy.c",
          "start_line": 168,
          "end_line": 268,
          "content": [
            "static u8 get_il_weight(int node)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tu8 weight = 1;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state)",
            "\t\tweight = state->iw_table[node];",
            "\trcu_read_unlock();",
            "\treturn weight;",
            "}",
            "static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)",
            "{",
            "\tu64 sum_bw = 0;",
            "\tunsigned int cast_sum_bw, scaling_factor = 1, iw_gcd = 0;",
            "\tint nid;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tsum_bw += bw[nid];",
            "",
            "\t/* Scale bandwidths to whole numbers in the range [1, weightiness] */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t/*",
            "\t\t * Try not to perform 64-bit division.",
            "\t\t * If sum_bw < scaling_factor, then sum_bw < U32_MAX.",
            "\t\t * If sum_bw > scaling_factor, then round the weight up to 1.",
            "\t\t */",
            "\t\tscaling_factor = weightiness * bw[nid];",
            "\t\tif (bw[nid] && sum_bw < scaling_factor) {",
            "\t\t\tcast_sum_bw = (unsigned int)sum_bw;",
            "\t\t\tnew_iw[nid] = scaling_factor / cast_sum_bw;",
            "\t\t} else {",
            "\t\t\tnew_iw[nid] = 1;",
            "\t\t}",
            "\t\tif (!iw_gcd)",
            "\t\t\tiw_gcd = new_iw[nid];",
            "\t\tiw_gcd = gcd(iw_gcd, new_iw[nid]);",
            "\t}",
            "",
            "\t/* 1:2 is strictly better than 16:32. Reduce by the weights' GCD. */",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tnew_iw[nid] /= iw_gcd;",
            "}",
            "int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tunsigned int *old_bw, *new_bw;",
            "\tunsigned int bw_val;",
            "\tint i;",
            "",
            "\tbw_val = min(coords->read_bandwidth, coords->write_bandwidth);",
            "\tnew_bw = kcalloc(nr_node_ids, sizeof(unsigned int), GFP_KERNEL);",
            "\tif (!new_bw)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew_wi_state = kmalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state) {",
            "\t\tkfree(new_bw);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tnew_wi_state->mode_auto = true;",
            "\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\tnew_wi_state->iw_table[i] = 1;",
            "",
            "\t/*",
            "\t * Update bandwidth info, even in manual mode. That way, when switching",
            "\t * to auto mode in the future, iw_table can be overwritten using",
            "\t * accurate bw data.",
            "\t */",
            "\tmutex_lock(&wi_state_lock);",
            "",
            "\told_bw = node_bw_table;",
            "\tif (old_bw)",
            "\t\tmemcpy(new_bw, old_bw, nr_node_ids * sizeof(*old_bw));",
            "\tnew_bw[node] = bw_val;",
            "\tnode_bw_table = new_bw;",
            "",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state && !old_wi_state->mode_auto) {",
            "\t\t/* Manual mode; skip reducing weights and updating wi_state */",
            "\t\tmutex_unlock(&wi_state_lock);",
            "\t\tkfree(new_wi_state);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* NULL wi_state assumes auto=true; reduce weights and update wi_state*/",
            "\treduce_interleave_weights(new_bw, new_wi_state->iw_table);",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "out:",
            "\tkfree(old_bw);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_il_weight, reduce_interleave_weights, mempolicy_set_node_perf",
          "description": "实现带权交错策略的权重计算与调整逻辑，通过获取节点带宽数据动态修改权重比例，支持根据性能参数更新节点间内存分配优先级。",
          "similarity": 0.571703314781189
        }
      ]
    },
    {
      "source_file": "mm/memblock.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:38:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memblock.c`\n\n---\n\n# memblock.c 技术文档\n\n## 1. 文件概述\n\n`memblock.c` 实现了 Linux 内核早期启动阶段的内存管理机制——**memblock**。该机制用于在常规内存分配器（如 buddy allocator）尚未初始化之前，对物理内存进行粗粒度的区域管理。它将系统内存抽象为若干连续的内存区域（regions），支持“可用内存”（memory）、“保留内存”（reserved）和“物理内存”（physmem，部分架构支持）三种类型，为内核早期初始化提供内存添加、查询和分配能力。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct memblock_region`：表示一个连续的物理内存区域，包含基地址（base）、大小（size）、NUMA 节点 ID 和属性标志。\n- `struct memblock_type`：管理一类内存区域的集合，包含区域数组、当前数量（cnt）、最大容量（max）和名称。\n- `struct memblock`：全局 memblock 管理结构，包含 `memory` 和 `reserved` 两种类型的 `memblock_type`，以及分配方向（bottom_up）和当前分配上限（current_limit）。\n- `physmem`（条件编译）：描述不受 `mem=` 参数限制的实际物理内存布局。\n\n### 主要函数与变量\n- `memblock_add()` / `memblock_add_node()`：向 memblock 添加可用内存区域。\n- `memblock_reserve()`：标记内存区域为保留（不可用于动态分配）。\n- `memblock_phys_alloc*()` / `memblock_alloc*()`：分配物理或虚拟地址的内存。\n- `memblock_overlaps_region()`：判断指定区域是否与某类 memblock 区域重叠。\n- `__memblock_find_range_bottom_up()`：从低地址向高地址查找满足条件的空闲内存范围。\n- 全局变量 `memblock`：静态初始化的主 memblock 结构体。\n- `max_low_pfn`, `min_low_pfn`, `max_pfn`, `max_possible_pfn`：记录 PFN（页帧号）边界信息。\n\n### 配置宏\n- `INIT_MEMBLOCK_REGIONS`：初始内存/保留区域数组大小（默认 128）。\n- `CONFIG_HAVE_MEMBLOCK_PHYS_MAP`：启用 `physmem` 类型支持。\n- `CONFIG_MEMBLOCK_KHO_SCRATCH`：支持仅从特定标记（KHO_SCRATCH）区域分配内存。\n- `CONFIG_ARCH_KEEP_MEMBLOCK`：决定是否在初始化完成后保留 memblock 数据结构。\n\n## 3. 关键实现\n\n### 初始化与存储\n- `memblock` 结构体在编译时静态初始化，其 `memory` 和 `reserved` 的区域数组分别使用 `memblock_memory_init_regions` 和 `memblock_reserved_init_regions`，初始容量由 `INIT_MEMBLOCK_*_REGIONS` 定义。\n- 每个 `memblock_type` 的 `cnt` 初始设为 1，但实际第一个条目为空的占位符，有效区域从索引 1 开始（后续代码处理）。\n- 支持通过 `memblock_allow_resize()` 动态扩容区域数组，但需谨慎避免与 initrd 等关键区域冲突。\n\n### 内存区域管理\n- 使用 `for_each_memblock_type` 宏遍历指定类型的区域。\n- `memblock_addrs_overlap()` 通过比较区间端点判断两个物理内存区域是否重叠。\n- `memblock_overlaps_region()` 封装了对某类所有区域的重叠检测。\n\n### 分配策略\n- 默认采用 **top-down**（从高地址向低地址）分配策略，可通过 `memblock_set_bottom_up(true)` 切换为 **bottom-up**。\n- 分配时受 `current_limit` 限制（默认 `MEMBLOCK_ALLOC_ANYWHERE` 表示无限制）。\n- 支持基于 NUMA 节点、对齐要求、内存属性（如 `MEMBLOCK_MIRROR`、`MEMBLOCK_KHO_SCRATCH`）的精细控制。\n- `choose_memblock_flags()` 根据 `kho_scratch_only` 和镜像内存存在性动态选择分配标志。\n\n### 安全与调试\n- `memblock_cap_size()` 防止地址计算溢出（确保 `base + size <= PHYS_ADDR_MAX`）。\n- 条件编译的 `memblock_dbg()` 宏用于调试输出（需开启 `memblock_debug`）。\n- 使用 `__initdata_memblock` 属性标记仅在初始化阶段使用的数据，便于后续释放。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/memblock.h>`：定义 memblock API 和数据结构。\n  - `<linux/kernel.h>`, `<linux/init.h>`：提供基础内核功能和初始化宏。\n  - `<linux/pfn.h>`：PFN 相关操作。\n  - `<asm/sections.h>`：访问内核链接段信息。\n  - 架构相关头文件（如 `internal.h`）。\n- **配置依赖**：\n  - `CONFIG_NUMA`：影响 `contig_page_data` 的定义。\n  - `CONFIG_KEXEC_HANDOVER`：引入 kexec 相关头文件。\n  - `CONFIG_HAVE_MEMBLOCK_PHYS_MAP`：启用 `physmem` 支持。\n- **后续移交**：在 `mem_init()` 中，memblock 管理的内存会被释放给 buddy allocator，完成内存管理权移交。\n\n## 5. 使用场景\n\n- **内核早期初始化**：在 `start_kernel()` 初期，架构代码（如 `setup_arch()`）调用 `memblock_add()` 注册可用物理内存，调用 `memblock_reserve()` 保留内核镜像、设备树、initrd 等关键区域。\n- **早期内存分配**：在 slab/buddy 分配器就绪前，使用 `memblock_alloc()` 分配大块连续内存（如页表、中断向量表、ACPI 表解析缓冲区）。\n- **内存布局查询**：通过 `for_each_memblock()` 等宏遍历内存区域，用于构建 e820 表、EFI 内存映射或 NUMA 拓扑。\n- **特殊分配需求**：支持从镜像内存（`MEMBLOCK_MIRROR`）或 KHO scratch 区域分配，满足安全启动或崩溃转储等场景。\n- **调试与分析**：通过 debugfs 接口（未在片段中体现）导出 memblock 布局，辅助内存问题诊断。",
      "similarity": 0.5764871835708618,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/memblock.c",
          "start_line": 192,
          "end_line": 297,
          "content": [
            "static inline phys_addr_t memblock_cap_size(phys_addr_t base, phys_addr_t *size)",
            "{",
            "\treturn *size = min(*size, PHYS_ADDR_MAX - base);",
            "}",
            "unsigned long __init_memblock",
            "memblock_addrs_overlap(phys_addr_t base1, phys_addr_t size1, phys_addr_t base2,",
            "\t\t       phys_addr_t size2)",
            "{",
            "\treturn ((base1 < (base2 + size2)) && (base2 < (base1 + size1)));",
            "}",
            "bool __init_memblock memblock_overlaps_region(struct memblock_type *type,",
            "\t\t\t\t\tphys_addr_t base, phys_addr_t size)",
            "{",
            "\tunsigned long i;",
            "",
            "\tmemblock_cap_size(base, &size);",
            "",
            "\tfor (i = 0; i < type->cnt; i++)",
            "\t\tif (memblock_addrs_overlap(base, size, type->regions[i].base,",
            "\t\t\t\t\t   type->regions[i].size))",
            "\t\t\tbreak;",
            "\treturn i < type->cnt;",
            "}",
            "static phys_addr_t __init_memblock",
            "__memblock_find_range_bottom_up(phys_addr_t start, phys_addr_t end,",
            "\t\t\t\tphys_addr_t size, phys_addr_t align, int nid,",
            "\t\t\t\tenum memblock_flags flags)",
            "{",
            "\tphys_addr_t this_start, this_end, cand;",
            "\tu64 i;",
            "",
            "\tfor_each_free_mem_range(i, nid, flags, &this_start, &this_end, NULL) {",
            "\t\tthis_start = clamp(this_start, start, end);",
            "\t\tthis_end = clamp(this_end, start, end);",
            "",
            "\t\tcand = round_up(this_start, align);",
            "\t\tif (cand < this_end && this_end - cand >= size)",
            "\t\t\treturn cand;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static phys_addr_t __init_memblock",
            "__memblock_find_range_top_down(phys_addr_t start, phys_addr_t end,",
            "\t\t\t       phys_addr_t size, phys_addr_t align, int nid,",
            "\t\t\t       enum memblock_flags flags)",
            "{",
            "\tphys_addr_t this_start, this_end, cand;",
            "\tu64 i;",
            "",
            "\tfor_each_free_mem_range_reverse(i, nid, flags, &this_start, &this_end,",
            "\t\t\t\t\tNULL) {",
            "\t\tthis_start = clamp(this_start, start, end);",
            "\t\tthis_end = clamp(this_end, start, end);",
            "",
            "\t\tif (this_end < size)",
            "\t\t\tcontinue;",
            "",
            "\t\tcand = round_down(this_end - size, align);",
            "\t\tif (cand >= this_start)",
            "\t\t\treturn cand;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static phys_addr_t __init_memblock memblock_find_in_range_node(phys_addr_t size,",
            "\t\t\t\t\tphys_addr_t align, phys_addr_t start,",
            "\t\t\t\t\tphys_addr_t end, int nid,",
            "\t\t\t\t\tenum memblock_flags flags)",
            "{",
            "\t/* pump up @end */",
            "\tif (end == MEMBLOCK_ALLOC_ACCESSIBLE ||",
            "\t    end == MEMBLOCK_ALLOC_NOLEAKTRACE)",
            "\t\tend = memblock.current_limit;",
            "",
            "\t/* avoid allocating the first page */",
            "\tstart = max_t(phys_addr_t, start, PAGE_SIZE);",
            "\tend = max(start, end);",
            "",
            "\tif (memblock_bottom_up())",
            "\t\treturn __memblock_find_range_bottom_up(start, end, size, align,",
            "\t\t\t\t\t\t       nid, flags);",
            "\telse",
            "\t\treturn __memblock_find_range_top_down(start, end, size, align,",
            "\t\t\t\t\t\t      nid, flags);",
            "}",
            "static phys_addr_t __init_memblock memblock_find_in_range(phys_addr_t start,",
            "\t\t\t\t\tphys_addr_t end, phys_addr_t size,",
            "\t\t\t\t\tphys_addr_t align)",
            "{",
            "\tphys_addr_t ret;",
            "\tenum memblock_flags flags = choose_memblock_flags();",
            "",
            "again:",
            "\tret = memblock_find_in_range_node(size, align, start, end,",
            "\t\t\t\t\t    NUMA_NO_NODE, flags);",
            "",
            "\tif (!ret && (flags & MEMBLOCK_MIRROR)) {",
            "\t\tpr_warn_ratelimited(\"Could not allocate %pap bytes of mirrored memory\\n\",",
            "\t\t\t&size);",
            "\t\tflags &= ~MEMBLOCK_MIRROR;",
            "\t\tgoto again;",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "memblock_cap_size, memblock_addrs_overlap, memblock_overlaps_region, __memblock_find_range_bottom_up, __memblock_find_range_top_down, memblock_find_in_range_node, memblock_find_in_range",
          "description": "实现内存区域地址重叠检测与分配策略选择逻辑，包含范围查找算法（底向顶/顶向底）及镜像内存分配失败回退机制。",
          "similarity": 0.599947452545166
        },
        {
          "chunk_id": 9,
          "file_path": "mm/memblock.c",
          "start_line": 1607,
          "end_line": 1734,
          "content": [
            "phys_addr_t __init memblock_phys_alloc_range(phys_addr_t size,",
            "\t\t\t\t\t     phys_addr_t align,",
            "\t\t\t\t\t     phys_addr_t start,",
            "\t\t\t\t\t     phys_addr_t end)",
            "{",
            "\tmemblock_dbg(\"%s: %llu bytes align=0x%llx from=%pa max_addr=%pa %pS\\n\",",
            "\t\t     __func__, (u64)size, (u64)align, &start, &end,",
            "\t\t     (void *)_RET_IP_);",
            "\treturn memblock_alloc_range_nid(size, align, start, end, NUMA_NO_NODE,",
            "\t\t\t\t\tfalse);",
            "}",
            "phys_addr_t __init memblock_phys_alloc_try_nid(phys_addr_t size, phys_addr_t align, int nid)",
            "{",
            "\treturn memblock_alloc_range_nid(size, align, 0,",
            "\t\t\t\t\tMEMBLOCK_ALLOC_ACCESSIBLE, nid, false);",
            "}",
            "void __init memblock_free_late(phys_addr_t base, phys_addr_t size)",
            "{",
            "\tphys_addr_t cursor, end;",
            "",
            "\tend = base + size - 1;",
            "\tmemblock_dbg(\"%s: [%pa-%pa] %pS\\n\",",
            "\t\t     __func__, &base, &end, (void *)_RET_IP_);",
            "\tkmemleak_free_part_phys(base, size);",
            "\tcursor = PFN_UP(base);",
            "\tend = PFN_DOWN(base + size);",
            "",
            "\tfor (; cursor < end; cursor++) {",
            "\t\tmemblock_free_pages(pfn_to_page(cursor), cursor, 0);",
            "\t\ttotalram_pages_inc();",
            "\t}",
            "}",
            "phys_addr_t __init_memblock memblock_reserved_kern_size(phys_addr_t limit, int nid)",
            "{",
            "\tstruct memblock_region *r;",
            "\tphys_addr_t total = 0;",
            "",
            "\tfor_each_reserved_mem_region(r) {",
            "\t\tphys_addr_t size = r->size;",
            "",
            "\t\tif (r->base > limit)",
            "\t\t\tbreak;",
            "",
            "\t\tif (r->base + r->size > limit)",
            "\t\t\tsize = limit - r->base;",
            "",
            "\t\tif (nid == memblock_get_region_node(r) || !numa_valid_node(nid))",
            "\t\t\tif (r->flags & MEMBLOCK_RSRV_KERN)",
            "\t\t\t\ttotal += size;",
            "\t}",
            "",
            "\treturn total;",
            "}",
            "unsigned long __init memblock_estimated_nr_free_pages(void)",
            "{",
            "\treturn PHYS_PFN(memblock_phys_mem_size() - memblock_reserved_size());",
            "}",
            "static phys_addr_t __init_memblock __find_max_addr(phys_addr_t limit)",
            "{",
            "\tphys_addr_t max_addr = PHYS_ADDR_MAX;",
            "\tstruct memblock_region *r;",
            "",
            "\t/*",
            "\t * translate the memory @limit size into the max address within one of",
            "\t * the memory memblock regions, if the @limit exceeds the total size",
            "\t * of those regions, max_addr will keep original value PHYS_ADDR_MAX",
            "\t */",
            "\tfor_each_mem_region(r) {",
            "\t\tif (limit <= r->size) {",
            "\t\t\tmax_addr = r->base + limit;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tlimit -= r->size;",
            "\t}",
            "",
            "\treturn max_addr;",
            "}",
            "void __init memblock_enforce_memory_limit(phys_addr_t limit)",
            "{",
            "\tphys_addr_t max_addr;",
            "",
            "\tif (!limit)",
            "\t\treturn;",
            "",
            "\tmax_addr = __find_max_addr(limit);",
            "",
            "\t/* @limit exceeds the total size of the memory, do nothing */",
            "\tif (max_addr == PHYS_ADDR_MAX)",
            "\t\treturn;",
            "",
            "\t/* truncate both memory and reserved regions */",
            "\tmemblock_remove_range(&memblock.memory, max_addr,",
            "\t\t\t      PHYS_ADDR_MAX);",
            "\tmemblock_remove_range(&memblock.reserved, max_addr,",
            "\t\t\t      PHYS_ADDR_MAX);",
            "}",
            "void __init memblock_cap_memory_range(phys_addr_t base, phys_addr_t size)",
            "{",
            "\tint start_rgn, end_rgn;",
            "\tint i, ret;",
            "",
            "\tif (!size)",
            "\t\treturn;",
            "",
            "\tif (!memblock_memory->total_size) {",
            "\t\tpr_warn(\"%s: No memory registered yet\\n\", __func__);",
            "\t\treturn;",
            "\t}",
            "",
            "\tret = memblock_isolate_range(&memblock.memory, base, size,",
            "\t\t\t\t\t\t&start_rgn, &end_rgn);",
            "\tif (ret)",
            "\t\treturn;",
            "",
            "\t/* remove all the MAP regions */",
            "\tfor (i = memblock.memory.cnt - 1; i >= end_rgn; i--)",
            "\t\tif (!memblock_is_nomap(&memblock.memory.regions[i]))",
            "\t\t\tmemblock_remove_region(&memblock.memory, i);",
            "",
            "\tfor (i = start_rgn - 1; i >= 0; i--)",
            "\t\tif (!memblock_is_nomap(&memblock.memory.regions[i]))",
            "\t\t\tmemblock_remove_region(&memblock.memory, i);",
            "",
            "\t/* truncate the reserved regions */",
            "\tmemblock_remove_range(&memblock.reserved, 0, base);",
            "\tmemblock_remove_range(&memblock.reserved,",
            "\t\t\tbase + size, PHYS_ADDR_MAX);",
            "}"
          ],
          "function_name": "memblock_phys_alloc_range, memblock_phys_alloc_try_nid, memblock_free_late, memblock_reserved_kern_size, memblock_estimated_nr_free_pages, __find_max_addr, memblock_enforce_memory_limit, memblock_cap_memory_range",
          "description": "实现物理内存分配/释放控制，包含内存上限强制限制、空闲页面估算、内存区域截断等管理功能，支持对保留内存的容量统计",
          "similarity": 0.5879053473472595
        },
        {
          "chunk_id": 2,
          "file_path": "mm/memblock.c",
          "start_line": 366,
          "end_line": 506,
          "content": [
            "static void __init_memblock memblock_remove_region(struct memblock_type *type, unsigned long r)",
            "{",
            "\ttype->total_size -= type->regions[r].size;",
            "\tmemmove(&type->regions[r], &type->regions[r + 1],",
            "\t\t(type->cnt - (r + 1)) * sizeof(type->regions[r]));",
            "\ttype->cnt--;",
            "",
            "\t/* Special case for empty arrays */",
            "\tif (type->cnt == 0) {",
            "\t\tWARN_ON(type->total_size != 0);",
            "\t\ttype->cnt = 1;",
            "\t\ttype->regions[0].base = 0;",
            "\t\ttype->regions[0].size = 0;",
            "\t\ttype->regions[0].flags = 0;",
            "\t\tmemblock_set_region_node(&type->regions[0], MAX_NUMNODES);",
            "\t}",
            "}",
            "void __init memblock_discard(void)",
            "{",
            "\tphys_addr_t addr, size;",
            "",
            "\tif (memblock.reserved.regions != memblock_reserved_init_regions) {",
            "\t\taddr = __pa(memblock.reserved.regions);",
            "\t\tsize = PAGE_ALIGN(sizeof(struct memblock_region) *",
            "\t\t\t\t  memblock.reserved.max);",
            "\t\tif (memblock_reserved_in_slab)",
            "\t\t\tkfree(memblock.reserved.regions);",
            "\t\telse",
            "\t\t\tmemblock_free_late(addr, size);",
            "\t}",
            "",
            "\tif (memblock.memory.regions != memblock_memory_init_regions) {",
            "\t\taddr = __pa(memblock.memory.regions);",
            "\t\tsize = PAGE_ALIGN(sizeof(struct memblock_region) *",
            "\t\t\t\t  memblock.memory.max);",
            "\t\tif (memblock_memory_in_slab)",
            "\t\t\tkfree(memblock.memory.regions);",
            "\t\telse",
            "\t\t\tmemblock_free_late(addr, size);",
            "\t}",
            "",
            "\tmemblock_memory = NULL;",
            "}",
            "static int __init_memblock memblock_double_array(struct memblock_type *type,",
            "\t\t\t\t\t\tphys_addr_t new_area_start,",
            "\t\t\t\t\t\tphys_addr_t new_area_size)",
            "{",
            "\tstruct memblock_region *new_array, *old_array;",
            "\tphys_addr_t old_alloc_size, new_alloc_size;",
            "\tphys_addr_t old_size, new_size, addr, new_end;",
            "\tint use_slab = slab_is_available();",
            "\tint *in_slab;",
            "",
            "\t/* We don't allow resizing until we know about the reserved regions",
            "\t * of memory that aren't suitable for allocation",
            "\t */",
            "\tif (!memblock_can_resize)",
            "\t\treturn -1;",
            "",
            "\t/* Calculate new doubled size */",
            "\told_size = type->max * sizeof(struct memblock_region);",
            "\tnew_size = old_size << 1;",
            "\t/*",
            "\t * We need to allocated new one align to PAGE_SIZE,",
            "\t *   so we can free them completely later.",
            "\t */",
            "\told_alloc_size = PAGE_ALIGN(old_size);",
            "\tnew_alloc_size = PAGE_ALIGN(new_size);",
            "",
            "\t/* Retrieve the slab flag */",
            "\tif (type == &memblock.memory)",
            "\t\tin_slab = &memblock_memory_in_slab;",
            "\telse",
            "\t\tin_slab = &memblock_reserved_in_slab;",
            "",
            "\t/* Try to find some space for it */",
            "\tif (use_slab) {",
            "\t\tnew_array = kmalloc(new_size, GFP_KERNEL);",
            "\t\taddr = new_array ? __pa(new_array) : 0;",
            "\t} else {",
            "\t\t/* only exclude range when trying to double reserved.regions */",
            "\t\tif (type != &memblock.reserved)",
            "\t\t\tnew_area_start = new_area_size = 0;",
            "",
            "\t\taddr = memblock_find_in_range(new_area_start + new_area_size,",
            "\t\t\t\t\t\tmemblock.current_limit,",
            "\t\t\t\t\t\tnew_alloc_size, PAGE_SIZE);",
            "\t\tif (!addr && new_area_size)",
            "\t\t\taddr = memblock_find_in_range(0,",
            "\t\t\t\tmin(new_area_start, memblock.current_limit),",
            "\t\t\t\tnew_alloc_size, PAGE_SIZE);",
            "",
            "\t\tif (addr) {",
            "\t\t\t/* The memory may not have been accepted, yet. */",
            "\t\t\taccept_memory(addr, new_alloc_size);",
            "",
            "\t\t\tnew_array = __va(addr);",
            "\t\t} else {",
            "\t\t\tnew_array = NULL;",
            "\t\t}",
            "\t}",
            "\tif (!addr) {",
            "\t\tpr_err(\"memblock: Failed to double %s array from %ld to %ld entries !\\n\",",
            "\t\t       type->name, type->max, type->max * 2);",
            "\t\treturn -1;",
            "\t}",
            "",
            "\tnew_end = addr + new_size - 1;",
            "\tmemblock_dbg(\"memblock: %s is doubled to %ld at [%pa-%pa]\",",
            "\t\t\ttype->name, type->max * 2, &addr, &new_end);",
            "",
            "\t/*",
            "\t * Found space, we now need to move the array over before we add the",
            "\t * reserved region since it may be our reserved array itself that is",
            "\t * full.",
            "\t */",
            "\tmemcpy(new_array, type->regions, old_size);",
            "\tmemset(new_array + type->max, 0, old_size);",
            "\told_array = type->regions;",
            "\ttype->regions = new_array;",
            "\ttype->max <<= 1;",
            "",
            "\t/* Free old array. We needn't free it if the array is the static one */",
            "\tif (*in_slab)",
            "\t\tkfree(old_array);",
            "\telse if (old_array != memblock_memory_init_regions &&",
            "\t\t old_array != memblock_reserved_init_regions)",
            "\t\tmemblock_free(old_array, old_alloc_size);",
            "",
            "\t/*",
            "\t * Reserve the new array if that comes from the memblock.  Otherwise, we",
            "\t * needn't do it",
            "\t */",
            "\tif (!use_slab)",
            "\t\tBUG_ON(memblock_reserve_kern(addr, new_alloc_size));",
            "",
            "\t/* Update slab flag */",
            "\t*in_slab = use_slab;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "memblock_remove_region, memblock_discard, memblock_double_array",
          "description": "实现内存区域数组的动态扩容（double_array）、旧区域释放（discard）及区域移除操作，维护内存类型总大小统计。",
          "similarity": 0.5725445747375488
        },
        {
          "chunk_id": 10,
          "file_path": "mm/memblock.c",
          "start_line": 1954,
          "end_line": 2057,
          "content": [
            "void __init memblock_mem_limit_remove_map(phys_addr_t limit)",
            "{",
            "\tphys_addr_t max_addr;",
            "",
            "\tif (!limit)",
            "\t\treturn;",
            "",
            "\tmax_addr = __find_max_addr(limit);",
            "",
            "\t/* @limit exceeds the total size of the memory, do nothing */",
            "\tif (max_addr == PHYS_ADDR_MAX)",
            "\t\treturn;",
            "",
            "\tmemblock_cap_memory_range(0, max_addr);",
            "}",
            "static int __init_memblock memblock_search(struct memblock_type *type, phys_addr_t addr)",
            "{",
            "\tunsigned int left = 0, right = type->cnt;",
            "",
            "\tdo {",
            "\t\tunsigned int mid = (right + left) / 2;",
            "",
            "\t\tif (addr < type->regions[mid].base)",
            "\t\t\tright = mid;",
            "\t\telse if (addr >= (type->regions[mid].base +",
            "\t\t\t\t  type->regions[mid].size))",
            "\t\t\tleft = mid + 1;",
            "\t\telse",
            "\t\t\treturn mid;",
            "\t} while (left < right);",
            "\treturn -1;",
            "}",
            "bool __init_memblock memblock_is_reserved(phys_addr_t addr)",
            "{",
            "\treturn memblock_search(&memblock.reserved, addr) != -1;",
            "}",
            "bool __init_memblock memblock_is_memory(phys_addr_t addr)",
            "{",
            "\treturn memblock_search(&memblock.memory, addr) != -1;",
            "}",
            "bool __init_memblock memblock_is_map_memory(phys_addr_t addr)",
            "{",
            "\tint i = memblock_search(&memblock.memory, addr);",
            "",
            "\tif (i == -1)",
            "\t\treturn false;",
            "\treturn !memblock_is_nomap(&memblock.memory.regions[i]);",
            "}",
            "int __init_memblock memblock_search_pfn_nid(unsigned long pfn,",
            "\t\t\t unsigned long *start_pfn, unsigned long *end_pfn)",
            "{",
            "\tstruct memblock_type *type = &memblock.memory;",
            "\tint mid = memblock_search(type, PFN_PHYS(pfn));",
            "",
            "\tif (mid == -1)",
            "\t\treturn -1;",
            "",
            "\t*start_pfn = PFN_DOWN(type->regions[mid].base);",
            "\t*end_pfn = PFN_DOWN(type->regions[mid].base + type->regions[mid].size);",
            "",
            "\treturn memblock_get_region_node(&type->regions[mid]);",
            "}",
            "bool __init_memblock memblock_is_region_memory(phys_addr_t base, phys_addr_t size)",
            "{",
            "\tint idx = memblock_search(&memblock.memory, base);",
            "\tphys_addr_t end = base + memblock_cap_size(base, &size);",
            "",
            "\tif (idx == -1)",
            "\t\treturn false;",
            "\treturn (memblock.memory.regions[idx].base +",
            "\t\t memblock.memory.regions[idx].size) >= end;",
            "}",
            "bool __init_memblock memblock_is_region_reserved(phys_addr_t base, phys_addr_t size)",
            "{",
            "\treturn memblock_overlaps_region(&memblock.reserved, base, size);",
            "}",
            "void __init_memblock memblock_trim_memory(phys_addr_t align)",
            "{",
            "\tphys_addr_t start, end, orig_start, orig_end;",
            "\tstruct memblock_region *r;",
            "",
            "\tfor_each_mem_region(r) {",
            "\t\torig_start = r->base;",
            "\t\torig_end = r->base + r->size;",
            "\t\tstart = round_up(orig_start, align);",
            "\t\tend = round_down(orig_end, align);",
            "",
            "\t\tif (start == orig_start && end == orig_end)",
            "\t\t\tcontinue;",
            "",
            "\t\tif (start < end) {",
            "\t\t\tr->base = start;",
            "\t\t\tr->size = end - start;",
            "\t\t} else {",
            "\t\t\tmemblock_remove_region(&memblock.memory,",
            "\t\t\t\t\t       r - memblock.memory.regions);",
            "\t\t\tr--;",
            "\t\t}",
            "\t}",
            "}",
            "void __init_memblock memblock_set_current_limit(phys_addr_t limit)",
            "{",
            "\tmemblock.current_limit = limit;",
            "}"
          ],
          "function_name": "memblock_mem_limit_remove_map, memblock_search, memblock_is_reserved, memblock_is_memory, memblock_is_map_memory, memblock_search_pfn_nid, memblock_is_region_memory, memblock_is_region_reserved, memblock_trim_memory, memblock_set_current_limit",
          "description": "实现内存块限制移除、搜索和区域判断逻辑，用于管理内存和保留区域的地址范围查询及修剪操作",
          "similarity": 0.5603184103965759
        },
        {
          "chunk_id": 3,
          "file_path": "mm/memblock.c",
          "start_line": 537,
          "end_line": 685,
          "content": [
            "static void __init_memblock memblock_merge_regions(struct memblock_type *type,",
            "\t\t\t\t\t\t   unsigned long start_rgn,",
            "\t\t\t\t\t\t   unsigned long end_rgn)",
            "{",
            "\tint i = 0;",
            "\tif (start_rgn)",
            "\t\ti = start_rgn - 1;",
            "\tend_rgn = min(end_rgn, type->cnt - 1);",
            "\twhile (i < end_rgn) {",
            "\t\tstruct memblock_region *this = &type->regions[i];",
            "\t\tstruct memblock_region *next = &type->regions[i + 1];",
            "",
            "\t\tif (this->base + this->size != next->base ||",
            "\t\t    memblock_get_region_node(this) !=",
            "\t\t    memblock_get_region_node(next) ||",
            "\t\t    this->flags != next->flags) {",
            "\t\t\tBUG_ON(this->base + this->size > next->base);",
            "\t\t\ti++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tthis->size += next->size;",
            "\t\t/* move forward from next + 1, index of which is i + 2 */",
            "\t\tmemmove(next, next + 1, (type->cnt - (i + 2)) * sizeof(*next));",
            "\t\ttype->cnt--;",
            "\t\tend_rgn--;",
            "\t}",
            "}",
            "static void __init_memblock memblock_insert_region(struct memblock_type *type,",
            "\t\t\t\t\t\t   int idx, phys_addr_t base,",
            "\t\t\t\t\t\t   phys_addr_t size,",
            "\t\t\t\t\t\t   int nid,",
            "\t\t\t\t\t\t   enum memblock_flags flags)",
            "{",
            "\tstruct memblock_region *rgn = &type->regions[idx];",
            "",
            "\tBUG_ON(type->cnt >= type->max);",
            "\tmemmove(rgn + 1, rgn, (type->cnt - idx) * sizeof(*rgn));",
            "\trgn->base = base;",
            "\trgn->size = size;",
            "\trgn->flags = flags;",
            "\tmemblock_set_region_node(rgn, nid);",
            "\ttype->cnt++;",
            "\ttype->total_size += size;",
            "}",
            "static int __init_memblock memblock_add_range(struct memblock_type *type,",
            "\t\t\t\tphys_addr_t base, phys_addr_t size,",
            "\t\t\t\tint nid, enum memblock_flags flags)",
            "{",
            "\tbool insert = false;",
            "\tphys_addr_t obase = base;",
            "\tphys_addr_t end = base + memblock_cap_size(base, &size);",
            "\tint idx, nr_new, start_rgn = -1, end_rgn;",
            "\tstruct memblock_region *rgn;",
            "",
            "\tif (!size)",
            "\t\treturn 0;",
            "",
            "\t/* special case for empty array */",
            "\tif (type->regions[0].size == 0) {",
            "\t\tWARN_ON(type->cnt != 1 || type->total_size);",
            "\t\ttype->regions[0].base = base;",
            "\t\ttype->regions[0].size = size;",
            "\t\ttype->regions[0].flags = flags;",
            "\t\tmemblock_set_region_node(&type->regions[0], nid);",
            "\t\ttype->total_size = size;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/*",
            "\t * The worst case is when new range overlaps all existing regions,",
            "\t * then we'll need type->cnt + 1 empty regions in @type. So if",
            "\t * type->cnt * 2 + 1 is less than or equal to type->max, we know",
            "\t * that there is enough empty regions in @type, and we can insert",
            "\t * regions directly.",
            "\t */",
            "\tif (type->cnt * 2 + 1 <= type->max)",
            "\t\tinsert = true;",
            "",
            "repeat:",
            "\t/*",
            "\t * The following is executed twice.  Once with %false @insert and",
            "\t * then with %true.  The first counts the number of regions needed",
            "\t * to accommodate the new area.  The second actually inserts them.",
            "\t */",
            "\tbase = obase;",
            "\tnr_new = 0;",
            "",
            "\tfor_each_memblock_type(idx, type, rgn) {",
            "\t\tphys_addr_t rbase = rgn->base;",
            "\t\tphys_addr_t rend = rbase + rgn->size;",
            "",
            "\t\tif (rbase >= end)",
            "\t\t\tbreak;",
            "\t\tif (rend <= base)",
            "\t\t\tcontinue;",
            "\t\t/*",
            "\t\t * @rgn overlaps.  If it separates the lower part of new",
            "\t\t * area, insert that portion.",
            "\t\t */",
            "\t\tif (rbase > base) {",
            "#ifdef CONFIG_NUMA",
            "\t\t\tWARN_ON(nid != memblock_get_region_node(rgn));",
            "#endif",
            "\t\t\tWARN_ON(flags != MEMBLOCK_NONE && flags != rgn->flags);",
            "\t\t\tnr_new++;",
            "\t\t\tif (insert) {",
            "\t\t\t\tif (start_rgn == -1)",
            "\t\t\t\t\tstart_rgn = idx;",
            "\t\t\t\tend_rgn = idx + 1;",
            "\t\t\t\tmemblock_insert_region(type, idx++, base,",
            "\t\t\t\t\t\t       rbase - base, nid,",
            "\t\t\t\t\t\t       flags);",
            "\t\t\t}",
            "\t\t}",
            "\t\t/* area below @rend is dealt with, forget about it */",
            "\t\tbase = min(rend, end);",
            "\t}",
            "",
            "\t/* insert the remaining portion */",
            "\tif (base < end) {",
            "\t\tnr_new++;",
            "\t\tif (insert) {",
            "\t\t\tif (start_rgn == -1)",
            "\t\t\t\tstart_rgn = idx;",
            "\t\t\tend_rgn = idx + 1;",
            "\t\t\tmemblock_insert_region(type, idx, base, end - base,",
            "\t\t\t\t\t       nid, flags);",
            "\t\t}",
            "\t}",
            "",
            "\tif (!nr_new)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * If this was the first round, resize array and repeat for actual",
            "\t * insertions; otherwise, merge and return.",
            "\t */",
            "\tif (!insert) {",
            "\t\twhile (type->cnt + nr_new > type->max)",
            "\t\t\tif (memblock_double_array(type, obase, size) < 0)",
            "\t\t\t\treturn -ENOMEM;",
            "\t\tinsert = true;",
            "\t\tgoto repeat;",
            "\t} else {",
            "\t\tmemblock_merge_regions(type, start_rgn, end_rgn);",
            "\t\treturn 0;",
            "\t}",
            "}"
          ],
          "function_name": "memblock_merge_regions, memblock_insert_region, memblock_add_range",
          "description": "实现内存区域合并（merge_regions）与插入（insert_region）逻辑，处理新增内存范围的拆分与整合，优化连续区域管理。",
          "similarity": 0.5556013584136963
        }
      ]
    },
    {
      "source_file": "kernel/bpf/memalloc.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:19:22\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\memalloc.c`\n\n---\n\n# `bpf/memalloc.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/memalloc.c` 实现了一个专用于 BPF（Berkeley Packet Filter）程序的内存分配器，支持在任意上下文（包括 NMI、中断、不可抢占上下文等）中安全地分配和释放小块内存。该分配器通过每 CPU 的多级缓存桶（per-CPU per-bucket free list）机制，避免在 BPF 程序执行路径中直接调用可能不安全的 `kmalloc()`。缓存桶的填充和回收由 `irq_work` 异步完成，确保主执行路径的低延迟和高可靠性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_mem_cache`**  \n  每个缓存桶的核心结构，包含：\n  - `free_llist` / `free_llist_extra`：无锁链表（llist），用于存储空闲对象。\n  - `active`：本地原子计数器，用于保护对 `free_llist` 的并发访问。\n  - `refill_work`：`irq_work` 结构，用于触发异步填充。\n  - `objcg`：对象 cgroup 指针，用于内存记账。\n  - `unit_size`：该缓存桶中对象的固定大小。\n  - `free_cnt`、`low_watermark`、`high_watermark`、`batch`：缓存管理参数。\n  - `percpu_size`：标识是否为 per-CPU 分配。\n  - RCU 相关字段（`free_by_rcu`、`rcu` 等）：用于延迟释放内存，避免在不可睡眠上下文中调用 `kfree`。\n\n- **`struct bpf_mem_caches`**  \n  包含 `NUM_CACHES`（11 个）不同大小的 `bpf_mem_cache` 实例，对应预定义的内存块尺寸。\n\n- **`sizes[NUM_CACHES]`**  \n  定义了 11 种支持的分配尺寸：`{96, 192, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096}` 字节。\n\n- **`size_index[24]`**  \n  查找表，将请求大小（≤192 字节）映射到对应的缓存桶索引。\n\n### 主要函数\n\n- **`bpf_mem_cache_idx(size_t size)`**  \n  根据请求大小返回对应的缓存桶索引（0~10），超出 `BPF_MEM_ALLOC_SIZE_MAX`（4096）则返回 -1。\n\n- **`__alloc()`**  \n  底层分配函数，根据是否为 per-CPU 类型调用 `kmalloc_node` 或 `__alloc_percpu_gfp`。\n\n- **`add_obj_to_free_list()`**  \n  将对象安全地加入当前 CPU 的空闲链表，使用 `active` 计数器保护。\n\n- **`alloc_bulk()`**  \n  批量分配对象并填充缓存桶，优先从延迟释放队列（如 `free_by_rcu_ttrace`）回收，再尝试从全局分配器分配。\n\n- **`free_one()` / `free_all()`**  \n  释放单个或多个对象，区分普通和 per-CPU 类型。\n\n- **`__free_rcu()` / `__free_rcu_tasks_trace()`**  \n  RCU 回调函数，用于在宽限期结束后真正释放内存。\n\n- **`enque_to_free()` / `do_call_rcu_ttrace()`**  \n  将待释放对象加入 RCU 延迟队列，并触发 RCU 宽限期。\n\n## 3. 关键实现\n\n### 内存布局与对齐\n- 每个分配的对象末尾附加 8 字节的 `struct llist_node`，用于无锁链表管理。\n- 所有分配均对齐至 8 字节边界。\n\n### 并发控制\n- 使用 `local_t active` 计数器保护对 `free_llist` 的访问。在分配/释放时，通过 `inc_active()`/`dec_active()` 禁用中断（尤其在 `CONFIG_PREEMPT_RT` 下），确保 NMI 或中断上下文不会破坏链表结构。\n- `free_llist_extra` 用于在 `active` 忙时暂存释放对象，避免失败。\n\n### 异步填充机制\n- 当缓存桶水位低于 `low_watermark` 时，通过 `irq_work` 触发 `alloc_bulk()`。\n- `alloc_bulk()` 优先从 RCU 延迟释放队列中回收对象，减少全局分配压力。\n- 使用 `set_active_memcg()` 确保内存分配计入正确的 memcg。\n\n### RCU 延迟释放\n- 在不可睡眠上下文（如 NMI）中释放内存时，对象被加入 `free_by_rcu_ttrace` 队列。\n- 通过 `call_rcu_tasks_trace()` 或 `call_rcu()` 触发宽限期，之后在软中断上下文中真正释放。\n- 支持 `rcu_trace_implies_rcu_gp()` 优化，避免双重 RCU 调用。\n\n### 尺寸映射策略\n- 对 ≤192 字节的请求，使用 `size_index` 查找表快速定位桶。\n- 对 >192 字节的请求，使用 `fls(size - 1) - 2` 计算桶索引，覆盖 256~4096 字节范围。\n\n## 4. 依赖关系\n\n- **内存管理**：依赖 `<linux/mm.h>`、`<linux/memcontrol.h>` 进行底层分配和 memcg 记账。\n- **BPF 子系统**：通过 `<linux/bpf.h>` 和 `<linux/bpf_mem_alloc.h>` 与 BPF 运行时集成。\n- **无锁数据结构**：使用 `<linux/llist.h>` 提供的无锁链表。\n- **中断与延迟执行**：依赖 `<linux/irq_work.h>` 实现异步填充。\n- **RCU 机制**：使用 RCU 和 RCU Tasks Trace 宽限期实现安全延迟释放。\n- **架构相关**：使用 `<asm/local.h>` 的 per-CPU 原子操作。\n\n## 5. 使用场景\n\n- **BPF tracing 程序**：当 BPF 程序 attach 到 `kprobe`、`fentry` 等 hook 点时，可能运行在任意内核上下文（包括 NMI、中断、不可抢占区域）。此时标准 `kmalloc` 不安全，必须使用本分配器。\n- **高可靠性内存分配**：在不允许睡眠、不能触发内存回收的上下文中，提供确定性的内存分配能力。\n- **低延迟要求**：通过 per-CPU 缓存避免锁竞争和全局分配器开销，满足 BPF 程序对性能的严苛要求。\n- **内存隔离与记账**：支持通过 `objcg` 将 BPF 内存消耗计入特定 cgroup，便于资源控制。",
      "similarity": 0.5648345947265625,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 68,
          "end_line": 168,
          "content": [
            "static int bpf_mem_cache_idx(size_t size)",
            "{",
            "\tif (!size || size > BPF_MEM_ALLOC_SIZE_MAX)",
            "\t\treturn -1;",
            "",
            "\tif (size <= 192)",
            "\t\treturn size_index[(size - 1) / 8] - 1;",
            "",
            "\treturn fls(size - 1) - 2;",
            "}",
            "static void inc_active(struct bpf_mem_cache *c, unsigned long *flags)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\t/* In RT irq_work runs in per-cpu kthread, so disable",
            "\t\t * interrupts to avoid preemption and interrupts and",
            "\t\t * reduce the chance of bpf prog executing on this cpu",
            "\t\t * when active counter is busy.",
            "\t\t */",
            "\t\tlocal_irq_save(*flags);",
            "\t/* alloc_bulk runs from irq_work which will not preempt a bpf",
            "\t * program that does unit_alloc/unit_free since IRQs are",
            "\t * disabled there. There is no race to increment 'active'",
            "\t * counter. It protects free_llist from corruption in case NMI",
            "\t * bpf prog preempted this loop.",
            "\t */",
            "\tWARN_ON_ONCE(local_inc_return(&c->active) != 1);",
            "}",
            "static void dec_active(struct bpf_mem_cache *c, unsigned long *flags)",
            "{",
            "\tlocal_dec(&c->active);",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tlocal_irq_restore(*flags);",
            "}",
            "static void add_obj_to_free_list(struct bpf_mem_cache *c, void *obj)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tinc_active(c, &flags);",
            "\t__llist_add(obj, &c->free_llist);",
            "\tc->free_cnt++;",
            "\tdec_active(c, &flags);",
            "}",
            "static void alloc_bulk(struct bpf_mem_cache *c, int cnt, int node, bool atomic)",
            "{",
            "\tstruct mem_cgroup *memcg = NULL, *old_memcg;",
            "\tgfp_t gfp;",
            "\tvoid *obj;",
            "\tint i;",
            "",
            "\tgfp = __GFP_NOWARN | __GFP_ACCOUNT;",
            "\tgfp |= atomic ? GFP_NOWAIT : GFP_KERNEL;",
            "",
            "\tfor (i = 0; i < cnt; i++) {",
            "\t\t/*",
            "\t\t * For every 'c' llist_del_first(&c->free_by_rcu_ttrace); is",
            "\t\t * done only by one CPU == current CPU. Other CPUs might",
            "\t\t * llist_add() and llist_del_all() in parallel.",
            "\t\t */",
            "\t\tobj = llist_del_first(&c->free_by_rcu_ttrace);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tif (i >= cnt)",
            "\t\treturn;",
            "",
            "\tfor (; i < cnt; i++) {",
            "\t\tobj = llist_del_first(&c->waiting_for_gp_ttrace);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tif (i >= cnt)",
            "\t\treturn;",
            "",
            "\tmemcg = get_memcg(c);",
            "\told_memcg = set_active_memcg(memcg);",
            "\tfor (; i < cnt; i++) {",
            "\t\t/* Allocate, but don't deplete atomic reserves that typical",
            "\t\t * GFP_ATOMIC would do. irq_work runs on this cpu and kmalloc",
            "\t\t * will allocate from the current numa node which is what we",
            "\t\t * want here.",
            "\t\t */",
            "\t\tobj = __alloc(c, node, gfp);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tset_active_memcg(old_memcg);",
            "\tmem_cgroup_put(memcg);",
            "}",
            "static void free_one(void *obj, bool percpu)",
            "{",
            "\tif (percpu) {",
            "\t\tfree_percpu(((void __percpu **)obj)[1]);",
            "\t\tkfree(obj);",
            "\t\treturn;",
            "\t}",
            "",
            "\tkfree(obj);",
            "}"
          ],
          "function_name": "bpf_mem_cache_idx, inc_active, dec_active, add_obj_to_free_list, alloc_bulk, free_one",
          "description": "实现BPF内存缓存核心控制逻辑，包含大小索引计算、活跃计数器管理、对象回收到自由链表、批量分配与释放流程。通过irq_work异步补充缓存，处理多CPU间的内存对象迁移与回收。",
          "similarity": 0.5602676868438721
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 689,
          "end_line": 807,
          "content": [
            "static void free_mem_alloc(struct bpf_mem_alloc *ma)",
            "{",
            "\t/* waiting_for_gp[_ttrace] lists were drained, but RCU callbacks",
            "\t * might still execute. Wait for them.",
            "\t *",
            "\t * rcu_barrier_tasks_trace() doesn't imply synchronize_rcu_tasks_trace(),",
            "\t * but rcu_barrier_tasks_trace() and rcu_barrier() below are only used",
            "\t * to wait for the pending __free_rcu_tasks_trace() and __free_rcu(),",
            "\t * so if call_rcu(head, __free_rcu) is skipped due to",
            "\t * rcu_trace_implies_rcu_gp(), it will be OK to skip rcu_barrier() by",
            "\t * using rcu_trace_implies_rcu_gp() as well.",
            "\t */",
            "\trcu_barrier(); /* wait for __free_by_rcu */",
            "\trcu_barrier_tasks_trace(); /* wait for __free_rcu */",
            "\tif (!rcu_trace_implies_rcu_gp())",
            "\t\trcu_barrier();",
            "\tfree_mem_alloc_no_barrier(ma);",
            "}",
            "static void free_mem_alloc_deferred(struct work_struct *work)",
            "{",
            "\tstruct bpf_mem_alloc *ma = container_of(work, struct bpf_mem_alloc, work);",
            "",
            "\tfree_mem_alloc(ma);",
            "\tkfree(ma);",
            "}",
            "static void destroy_mem_alloc(struct bpf_mem_alloc *ma, int rcu_in_progress)",
            "{",
            "\tstruct bpf_mem_alloc *copy;",
            "",
            "\tif (!rcu_in_progress) {",
            "\t\t/* Fast path. No callbacks are pending, hence no need to do",
            "\t\t * rcu_barrier-s.",
            "\t\t */",
            "\t\tfree_mem_alloc_no_barrier(ma);",
            "\t\treturn;",
            "\t}",
            "",
            "\tcopy = kmemdup(ma, sizeof(*ma), GFP_KERNEL);",
            "\tif (!copy) {",
            "\t\t/* Slow path with inline barrier-s */",
            "\t\tfree_mem_alloc(ma);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Defer barriers into worker to let the rest of map memory to be freed */",
            "\tmemset(ma, 0, sizeof(*ma));",
            "\tINIT_WORK(&copy->work, free_mem_alloc_deferred);",
            "\tqueue_work(system_unbound_wq, &copy->work);",
            "}",
            "void bpf_mem_alloc_destroy(struct bpf_mem_alloc *ma)",
            "{",
            "\tstruct bpf_mem_caches *cc;",
            "\tstruct bpf_mem_cache *c;",
            "\tint cpu, i, rcu_in_progress;",
            "",
            "\tif (ma->cache) {",
            "\t\trcu_in_progress = 0;",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tc = per_cpu_ptr(ma->cache, cpu);",
            "\t\t\tWRITE_ONCE(c->draining, true);",
            "\t\t\tirq_work_sync(&c->refill_work);",
            "\t\t\tdrain_mem_cache(c);",
            "\t\t\trcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);",
            "\t\t\trcu_in_progress += atomic_read(&c->call_rcu_in_progress);",
            "\t\t}",
            "\t\tobj_cgroup_put(ma->objcg);",
            "\t\tdestroy_mem_alloc(ma, rcu_in_progress);",
            "\t}",
            "\tif (ma->caches) {",
            "\t\trcu_in_progress = 0;",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tcc = per_cpu_ptr(ma->caches, cpu);",
            "\t\t\tfor (i = 0; i < NUM_CACHES; i++) {",
            "\t\t\t\tc = &cc->cache[i];",
            "\t\t\t\tWRITE_ONCE(c->draining, true);",
            "\t\t\t\tirq_work_sync(&c->refill_work);",
            "\t\t\t\tdrain_mem_cache(c);",
            "\t\t\t\trcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);",
            "\t\t\t\trcu_in_progress += atomic_read(&c->call_rcu_in_progress);",
            "\t\t\t}",
            "\t\t}",
            "\t\tobj_cgroup_put(ma->objcg);",
            "\t\tdestroy_mem_alloc(ma, rcu_in_progress);",
            "\t}",
            "}",
            "static void notrace unit_free(struct bpf_mem_cache *c, void *ptr)",
            "{",
            "\tstruct llist_node *llnode = ptr - LLIST_NODE_SZ;",
            "\tunsigned long flags;",
            "\tint cnt = 0;",
            "",
            "\tBUILD_BUG_ON(LLIST_NODE_SZ > 8);",
            "",
            "\t/*",
            "\t * Remember bpf_mem_cache that allocated this object.",
            "\t * The hint is not accurate.",
            "\t */",
            "\tc->tgt = *(struct bpf_mem_cache **)llnode;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (local_inc_return(&c->active) == 1) {",
            "\t\t__llist_add(llnode, &c->free_llist);",
            "\t\tcnt = ++c->free_cnt;",
            "\t} else {",
            "\t\t/* unit_free() cannot fail. Therefore add an object to atomic",
            "\t\t * llist. free_bulk() will drain it. Though free_llist_extra is",
            "\t\t * a per-cpu list we have to use atomic llist_add here, since",
            "\t\t * it also can be interrupted by bpf nmi prog that does another",
            "\t\t * unit_free() into the same free_llist_extra.",
            "\t\t */",
            "\t\tllist_add(llnode, &c->free_llist_extra);",
            "\t}",
            "\tlocal_dec(&c->active);",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (cnt > c->high_watermark)",
            "\t\t/* free few objects from current cpu into global kmalloc pool */",
            "\t\tirq_work_raise(c);",
            "}"
          ],
          "function_name": "free_mem_alloc, free_mem_alloc_deferred, destroy_mem_alloc, bpf_mem_alloc_destroy, unit_free",
          "description": "实现BPF内存分配销毁逻辑，通过RCU屏障等待回调完成并安全释放资源，deferred路径利用workqueue异步释放，destroy_mem_alloc处理缓存清理和RCU状态同步。",
          "similarity": 0.5420920252799988
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 576,
          "end_line": 682,
          "content": [
            "int bpf_mem_alloc_percpu_init(struct bpf_mem_alloc *ma, struct obj_cgroup *objcg)",
            "{",
            "\tstruct bpf_mem_caches __percpu *pcc;",
            "",
            "\tpcc = __alloc_percpu_gfp(sizeof(struct bpf_mem_caches), 8, GFP_KERNEL);",
            "\tif (!pcc)",
            "\t\treturn -ENOMEM;",
            "",
            "\tma->caches = pcc;",
            "\tma->objcg = objcg;",
            "\tma->percpu = true;",
            "\treturn 0;",
            "}",
            "int bpf_mem_alloc_percpu_unit_init(struct bpf_mem_alloc *ma, int size)",
            "{",
            "\tstruct bpf_mem_caches *cc; struct bpf_mem_caches __percpu *pcc;",
            "\tint cpu, i, unit_size, percpu_size;",
            "\tstruct obj_cgroup *objcg;",
            "\tstruct bpf_mem_cache *c;",
            "",
            "\ti = bpf_mem_cache_idx(size);",
            "\tif (i < 0)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* room for llist_node and per-cpu pointer */",
            "\tpercpu_size = LLIST_NODE_SZ + sizeof(void *);",
            "",
            "\tunit_size = sizes[i];",
            "\tobjcg = ma->objcg;",
            "\tpcc = ma->caches;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tcc = per_cpu_ptr(pcc, cpu);",
            "\t\tc = &cc->cache[i];",
            "\t\tif (cpu == 0 && c->unit_size)",
            "\t\t\tbreak;",
            "",
            "\t\tc->unit_size = unit_size;",
            "\t\tc->objcg = objcg;",
            "\t\tc->percpu_size = percpu_size;",
            "\t\tc->tgt = c;",
            "",
            "\t\tinit_refill_work(c);",
            "\t\tprefill_mem_cache(c, cpu);",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static void drain_mem_cache(struct bpf_mem_cache *c)",
            "{",
            "\tbool percpu = !!c->percpu_size;",
            "",
            "\t/* No progs are using this bpf_mem_cache, but htab_map_free() called",
            "\t * bpf_mem_cache_free() for all remaining elements and they can be in",
            "\t * free_by_rcu_ttrace or in waiting_for_gp_ttrace lists, so drain those lists now.",
            "\t *",
            "\t * Except for waiting_for_gp_ttrace list, there are no concurrent operations",
            "\t * on these lists, so it is safe to use __llist_del_all().",
            "\t */",
            "\tfree_all(llist_del_all(&c->free_by_rcu_ttrace), percpu);",
            "\tfree_all(llist_del_all(&c->waiting_for_gp_ttrace), percpu);",
            "\tfree_all(__llist_del_all(&c->free_llist), percpu);",
            "\tfree_all(__llist_del_all(&c->free_llist_extra), percpu);",
            "\tfree_all(__llist_del_all(&c->free_by_rcu), percpu);",
            "\tfree_all(__llist_del_all(&c->free_llist_extra_rcu), percpu);",
            "\tfree_all(llist_del_all(&c->waiting_for_gp), percpu);",
            "}",
            "static void check_mem_cache(struct bpf_mem_cache *c)",
            "{",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_by_rcu_ttrace));",
            "\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp_ttrace));",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_llist));",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_llist_extra));",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_by_rcu));",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_llist_extra_rcu));",
            "\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp));",
            "}",
            "static void check_leaked_objs(struct bpf_mem_alloc *ma)",
            "{",
            "\tstruct bpf_mem_caches *cc;",
            "\tstruct bpf_mem_cache *c;",
            "\tint cpu, i;",
            "",
            "\tif (ma->cache) {",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tc = per_cpu_ptr(ma->cache, cpu);",
            "\t\t\tcheck_mem_cache(c);",
            "\t\t}",
            "\t}",
            "\tif (ma->caches) {",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tcc = per_cpu_ptr(ma->caches, cpu);",
            "\t\t\tfor (i = 0; i < NUM_CACHES; i++) {",
            "\t\t\t\tc = &cc->cache[i];",
            "\t\t\t\tcheck_mem_cache(c);",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "}",
            "static void free_mem_alloc_no_barrier(struct bpf_mem_alloc *ma)",
            "{",
            "\tcheck_leaked_objs(ma);",
            "\tfree_percpu(ma->cache);",
            "\tfree_percpu(ma->caches);",
            "\tma->cache = NULL;",
            "\tma->caches = NULL;",
            "}"
          ],
          "function_name": "bpf_mem_alloc_percpu_init, bpf_mem_alloc_percpu_unit_init, drain_mem_cache, check_mem_cache, check_leaked_objs, free_mem_alloc_no_barrier",
          "description": "实现Per-CPU内存分配器的初始化与清理逻辑，包含Per-CPU缓存初始化、对象回收链表遍历、内存泄漏检测及资源释放。提供模块卸载时的强制清理接口，确保所有残留对象被正确释放。",
          "similarity": 0.5273109674453735
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 1,
          "end_line": 67,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2022 Meta Platforms, Inc. and affiliates. */",
            "#include <linux/mm.h>",
            "#include <linux/llist.h>",
            "#include <linux/bpf.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/bpf_mem_alloc.h>",
            "#include <linux/memcontrol.h>",
            "#include <asm/local.h>",
            "",
            "/* Any context (including NMI) BPF specific memory allocator.",
            " *",
            " * Tracing BPF programs can attach to kprobe and fentry. Hence they",
            " * run in unknown context where calling plain kmalloc() might not be safe.",
            " *",
            " * Front-end kmalloc() with per-cpu per-bucket cache of free elements.",
            " * Refill this cache asynchronously from irq_work.",
            " *",
            " * CPU_0 buckets",
            " * 16 32 64 96 128 196 256 512 1024 2048 4096",
            " * ...",
            " * CPU_N buckets",
            " * 16 32 64 96 128 196 256 512 1024 2048 4096",
            " *",
            " * The buckets are prefilled at the start.",
            " * BPF programs always run with migration disabled.",
            " * It's safe to allocate from cache of the current cpu with irqs disabled.",
            " * Free-ing is always done into bucket of the current cpu as well.",
            " * irq_work trims extra free elements from buckets with kfree",
            " * and refills them with kmalloc, so global kmalloc logic takes care",
            " * of freeing objects allocated by one cpu and freed on another.",
            " *",
            " * Every allocated objected is padded with extra 8 bytes that contains",
            " * struct llist_node.",
            " */",
            "#define LLIST_NODE_SZ sizeof(struct llist_node)",
            "",
            "#define BPF_MEM_ALLOC_SIZE_MAX 4096",
            "",
            "/* similar to kmalloc, but sizeof == 8 bucket is gone */",
            "static u8 size_index[24] __ro_after_init = {",
            "\t3,\t/* 8 */",
            "\t3,\t/* 16 */",
            "\t4,\t/* 24 */",
            "\t4,\t/* 32 */",
            "\t5,\t/* 40 */",
            "\t5,\t/* 48 */",
            "\t5,\t/* 56 */",
            "\t5,\t/* 64 */",
            "\t1,\t/* 72 */",
            "\t1,\t/* 80 */",
            "\t1,\t/* 88 */",
            "\t1,\t/* 96 */",
            "\t6,\t/* 104 */",
            "\t6,\t/* 112 */",
            "\t6,\t/* 120 */",
            "\t6,\t/* 128 */",
            "\t2,\t/* 136 */",
            "\t2,\t/* 144 */",
            "\t2,\t/* 152 */",
            "\t2,\t/* 160 */",
            "\t2,\t/* 168 */",
            "\t2,\t/* 176 */",
            "\t2,\t/* 184 */",
            "\t2\t/* 192 */",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义BPF专用内存分配器基础结构，通过per-CPU缓存和异步填充机制管理小对象分配。建立大小到缓存索引的映射表，预分配不同大小的桶并注册到各CPU，支持NMI和中断上下文的安全内存分配。",
          "similarity": 0.5221219062805176
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 381,
          "end_line": 545,
          "content": [
            "static void check_free_by_rcu(struct bpf_mem_cache *c)",
            "{",
            "\tstruct llist_node *llnode, *t;",
            "\tunsigned long flags;",
            "",
            "\t/* drain free_llist_extra_rcu */",
            "\tif (unlikely(!llist_empty(&c->free_llist_extra_rcu))) {",
            "\t\tinc_active(c, &flags);",
            "\t\tllist_for_each_safe(llnode, t, llist_del_all(&c->free_llist_extra_rcu))",
            "\t\t\tif (__llist_add(llnode, &c->free_by_rcu))",
            "\t\t\t\tc->free_by_rcu_tail = llnode;",
            "\t\tdec_active(c, &flags);",
            "\t}",
            "",
            "\tif (llist_empty(&c->free_by_rcu))",
            "\t\treturn;",
            "",
            "\tif (atomic_xchg(&c->call_rcu_in_progress, 1)) {",
            "\t\t/*",
            "\t\t * Instead of kmalloc-ing new rcu_head and triggering 10k",
            "\t\t * call_rcu() to hit rcutree.qhimark and force RCU to notice",
            "\t\t * the overload just ask RCU to hurry up. There could be many",
            "\t\t * objects in free_by_rcu list.",
            "\t\t * This hint reduces memory consumption for an artificial",
            "\t\t * benchmark from 2 Gbyte to 150 Mbyte.",
            "\t\t */",
            "\t\trcu_request_urgent_qs_task(current);",
            "\t\treturn;",
            "\t}",
            "",
            "\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp));",
            "",
            "\tinc_active(c, &flags);",
            "\tWRITE_ONCE(c->waiting_for_gp.first, __llist_del_all(&c->free_by_rcu));",
            "\tc->waiting_for_gp_tail = c->free_by_rcu_tail;",
            "\tdec_active(c, &flags);",
            "",
            "\tif (unlikely(READ_ONCE(c->draining))) {",
            "\t\tfree_all(llist_del_all(&c->waiting_for_gp), !!c->percpu_size);",
            "\t\tatomic_set(&c->call_rcu_in_progress, 0);",
            "\t} else {",
            "\t\tcall_rcu_hurry(&c->rcu, __free_by_rcu);",
            "\t}",
            "}",
            "static void bpf_mem_refill(struct irq_work *work)",
            "{",
            "\tstruct bpf_mem_cache *c = container_of(work, struct bpf_mem_cache, refill_work);",
            "\tint cnt;",
            "",
            "\t/* Racy access to free_cnt. It doesn't need to be 100% accurate */",
            "\tcnt = c->free_cnt;",
            "\tif (cnt < c->low_watermark)",
            "\t\t/* irq_work runs on this cpu and kmalloc will allocate",
            "\t\t * from the current numa node which is what we want here.",
            "\t\t */",
            "\t\talloc_bulk(c, c->batch, NUMA_NO_NODE, true);",
            "\telse if (cnt > c->high_watermark)",
            "\t\tfree_bulk(c);",
            "",
            "\tcheck_free_by_rcu(c);",
            "}",
            "static void notrace irq_work_raise(struct bpf_mem_cache *c)",
            "{",
            "\tirq_work_queue(&c->refill_work);",
            "}",
            "static void init_refill_work(struct bpf_mem_cache *c)",
            "{",
            "\tinit_irq_work(&c->refill_work, bpf_mem_refill);",
            "\tif (c->percpu_size) {",
            "\t\tc->low_watermark = 1;",
            "\t\tc->high_watermark = 3;",
            "\t} else if (c->unit_size <= 256) {",
            "\t\tc->low_watermark = 32;",
            "\t\tc->high_watermark = 96;",
            "\t} else {",
            "\t\t/* When page_size == 4k, order-0 cache will have low_mark == 2",
            "\t\t * and high_mark == 6 with batch alloc of 3 individual pages at",
            "\t\t * a time.",
            "\t\t * 8k allocs and above low == 1, high == 3, batch == 1.",
            "\t\t */",
            "\t\tc->low_watermark = max(32 * 256 / c->unit_size, 1);",
            "\t\tc->high_watermark = max(96 * 256 / c->unit_size, 3);",
            "\t}",
            "\tc->batch = max((c->high_watermark - c->low_watermark) / 4 * 3, 1);",
            "}",
            "static void prefill_mem_cache(struct bpf_mem_cache *c, int cpu)",
            "{",
            "\tint cnt = 1;",
            "",
            "\t/* To avoid consuming memory, for non-percpu allocation, assume that",
            "\t * 1st run of bpf prog won't be doing more than 4 map_update_elem from",
            "\t * irq disabled region if unit size is less than or equal to 256.",
            "\t * For all other cases, let us just do one allocation.",
            "\t */",
            "\tif (!c->percpu_size && c->unit_size <= 256)",
            "\t\tcnt = 4;",
            "\talloc_bulk(c, cnt, cpu_to_node(cpu), false);",
            "}",
            "int bpf_mem_alloc_init(struct bpf_mem_alloc *ma, int size, bool percpu)",
            "{",
            "\tstruct bpf_mem_caches *cc; struct bpf_mem_caches __percpu *pcc;",
            "\tstruct bpf_mem_cache *c; struct bpf_mem_cache __percpu *pc;",
            "\tstruct obj_cgroup *objcg = NULL;",
            "\tint cpu, i, unit_size, percpu_size = 0;",
            "",
            "\tif (percpu && size == 0)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* room for llist_node and per-cpu pointer */",
            "\tif (percpu)",
            "\t\tpercpu_size = LLIST_NODE_SZ + sizeof(void *);",
            "\tma->percpu = percpu;",
            "",
            "\tif (size) {",
            "\t\tpc = __alloc_percpu_gfp(sizeof(*pc), 8, GFP_KERNEL);",
            "\t\tif (!pc)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tif (!percpu)",
            "\t\t\tsize += LLIST_NODE_SZ; /* room for llist_node */",
            "\t\tunit_size = size;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\t\tif (memcg_bpf_enabled())",
            "\t\t\tobjcg = get_obj_cgroup_from_current();",
            "#endif",
            "\t\tma->objcg = objcg;",
            "",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tc = per_cpu_ptr(pc, cpu);",
            "\t\t\tc->unit_size = unit_size;",
            "\t\t\tc->objcg = objcg;",
            "\t\t\tc->percpu_size = percpu_size;",
            "\t\t\tc->tgt = c;",
            "\t\t\tinit_refill_work(c);",
            "\t\t\tprefill_mem_cache(c, cpu);",
            "\t\t}",
            "\t\tma->cache = pc;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tpcc = __alloc_percpu_gfp(sizeof(*cc), 8, GFP_KERNEL);",
            "\tif (!pcc)",
            "\t\treturn -ENOMEM;",
            "#ifdef CONFIG_MEMCG",
            "\tobjcg = get_obj_cgroup_from_current();",
            "#endif",
            "\tma->objcg = objcg;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tcc = per_cpu_ptr(pcc, cpu);",
            "\t\tfor (i = 0; i < NUM_CACHES; i++) {",
            "\t\t\tc = &cc->cache[i];",
            "\t\t\tc->unit_size = sizes[i];",
            "\t\t\tc->objcg = objcg;",
            "\t\t\tc->percpu_size = percpu_size;",
            "\t\t\tc->tgt = c;",
            "",
            "\t\t\tinit_refill_work(c);",
            "\t\t\tprefill_mem_cache(c, cpu);",
            "\t\t}",
            "\t}",
            "",
            "\tma->caches = pcc;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "check_free_by_rcu, bpf_mem_refill, irq_work_raise, init_refill_work, prefill_mem_cache, bpf_mem_alloc_init",
          "description": "初始化内存缓存的刷新工作队列，设置水位标记控制缓存规模，实现动态扩容/缩容。包含预填充缓存的初始化函数，根据对象大小配置不同的低/高水位阈值，通过中断工作线程维护缓存状态。",
          "similarity": 0.5208712816238403
        }
      ]
    }
  ]
}