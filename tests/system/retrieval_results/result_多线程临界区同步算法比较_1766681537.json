{
  "query": "多线程临界区同步算法比较",
  "timestamp": "2025-12-26 00:52:17",
  "retrieved_files": [
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5628793239593506,
      "chunks": [
        {
          "chunk_id": 10,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1840,
          "end_line": 1973,
          "content": [
            "static int __noreturn rcu_gp_kthread(void *unused)",
            "{",
            "\trcu_bind_gp_kthread();",
            "\tfor (;;) {",
            "",
            "\t\t/* Handle grace-period start. */",
            "\t\tfor (;;) {",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwait\"));",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_WAIT_GPS);",
            "\t\t\tswait_event_idle_exclusive(rcu_state.gp_wq,",
            "\t\t\t\t\t READ_ONCE(rcu_state.gp_flags) &",
            "\t\t\t\t\t RCU_GP_FLAG_INIT);",
            "\t\t\trcu_gp_torture_wait();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_DONE_GPS);",
            "\t\t\t/* Locking provides needed memory barrier. */",
            "\t\t\tif (rcu_gp_init())",
            "\t\t\t\tbreak;",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\t\t\tWARN_ON(signal_pending(current));",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwaitsig\"));",
            "\t\t}",
            "",
            "\t\t/* Handle quiescent-state forcing. */",
            "\t\trcu_gp_fqs_loop();",
            "",
            "\t\t/* Handle grace-period end. */",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANUP);",
            "\t\trcu_gp_cleanup();",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANED);",
            "\t}",
            "}",
            "static void rcu_report_qs_rsp(unsigned long flags)",
            "\t__releases(rcu_get_root()->lock)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rcu_get_root());",
            "\tWARN_ON_ONCE(!rcu_gp_in_progress());",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rcu_get_root(), flags);",
            "\trcu_gp_kthread_wake();",
            "}",
            "static void rcu_report_qs_rnp(unsigned long mask, struct rcu_node *rnp,",
            "\t\t\t      unsigned long gps, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long oldmask = 0;",
            "\tstruct rcu_node *rnp_c;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* Walk up the rcu_node hierarchy. */",
            "\tfor (;;) {",
            "\t\tif ((!(rnp->qsmask & mask) && mask) || rnp->gp_seq != gps) {",
            "",
            "\t\t\t/*",
            "\t\t\t * Our bit has already been cleared, or the",
            "\t\t\t * relevant grace period is already over, so done.",
            "\t\t\t */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */",
            "\t\tWARN_ON_ONCE(!rcu_is_leaf_node(rnp) &&",
            "\t\t\t     rcu_preempt_blocked_readers_cgp(rnp));",
            "\t\tWRITE_ONCE(rnp->qsmask, rnp->qsmask & ~mask);",
            "\t\ttrace_rcu_quiescent_state_report(rcu_state.name, rnp->gp_seq,",
            "\t\t\t\t\t\t mask, rnp->qsmask, rnp->level,",
            "\t\t\t\t\t\t rnp->grplo, rnp->grphi,",
            "\t\t\t\t\t\t !!rnp->gp_tasks);",
            "\t\tif (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {",
            "",
            "\t\t\t/* Other bits still set at this level, so done. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\trnp->completedqs = rnp->gp_seq;",
            "\t\tmask = rnp->grpmask;",
            "\t\tif (rnp->parent == NULL) {",
            "",
            "\t\t\t/* No more levels.  Exit loop holding root lock. */",
            "",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\trnp_c = rnp;",
            "\t\trnp = rnp->parent;",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\toldmask = READ_ONCE(rnp_c->qsmask);",
            "\t}",
            "",
            "\t/*",
            "\t * Get here if we are the last CPU to pass through a quiescent",
            "\t * state for this grace period.  Invoke rcu_report_qs_rsp()",
            "\t * to clean up and start the next grace period if one is needed.",
            "\t */",
            "\trcu_report_qs_rsp(flags); /* releases rnp->lock. */",
            "}",
            "static void __maybe_unused",
            "rcu_report_unblock_qs_rnp(struct rcu_node *rnp, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long gps;",
            "\tunsigned long mask;",
            "\tstruct rcu_node *rnp_p;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (WARN_ON_ONCE(!IS_ENABLED(CONFIG_PREEMPT_RCU)) ||",
            "\t    WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)) ||",
            "\t    rnp->qsmask != 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;  /* Still need more quiescent states! */",
            "\t}",
            "",
            "\trnp->completedqs = rnp->gp_seq;",
            "\trnp_p = rnp->parent;",
            "\tif (rnp_p == NULL) {",
            "\t\t/*",
            "\t\t * Only one rcu_node structure in the tree, so don't",
            "\t\t * try to report up to its nonexistent parent!",
            "\t\t */",
            "\t\trcu_report_qs_rsp(flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Report up the rest of the hierarchy, tracking current ->gp_seq. */",
            "\tgps = rnp->gp_seq;",
            "\tmask = rnp->grpmask;",
            "\traw_spin_unlock_rcu_node(rnp);\t/* irqs remain disabled. */",
            "\traw_spin_lock_rcu_node(rnp_p);\t/* irqs already disabled. */",
            "\trcu_report_qs_rnp(mask, rnp_p, gps, flags);",
            "}"
          ],
          "function_name": "rcu_gp_kthread, rcu_report_qs_rsp, rcu_report_qs_rnp, rcu_report_unblock_qs_rnp",
          "description": "实现RCU grace period的主线程循环，处理grace period启动、强制quiescent状态报告及结束逻辑，通过锁和状态标志协调各子系统同步",
          "similarity": 0.5489769577980042
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2523,
          "end_line": 2628,
          "content": [
            "static void rcu_cpu_kthread_park(unsigned int cpu)",
            "{",
            "\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;",
            "}",
            "static int rcu_cpu_kthread_should_run(unsigned int cpu)",
            "{",
            "\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);",
            "}",
            "static void rcu_cpu_kthread(unsigned int cpu)",
            "{",
            "\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);",
            "\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);",
            "\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);",
            "\tint spincnt;",
            "",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));",
            "\tfor (spincnt = 0; spincnt < 10; spincnt++) {",
            "\t\tWRITE_ONCE(*j, jiffies);",
            "\t\tlocal_bh_disable();",
            "\t\t*statusp = RCU_KTHREAD_RUNNING;",
            "\t\tlocal_irq_disable();",
            "\t\twork = *workp;",
            "\t\tWRITE_ONCE(*workp, 0);",
            "\t\tlocal_irq_enable();",
            "\t\tif (work)",
            "\t\t\trcu_core();",
            "\t\tlocal_bh_enable();",
            "\t\tif (!READ_ONCE(*workp)) {",
            "\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));",
            "\t\t\t*statusp = RCU_KTHREAD_WAITING;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "\t*statusp = RCU_KTHREAD_YIELDING;",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));",
            "\tschedule_timeout_idle(2);",
            "\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));",
            "\t*statusp = RCU_KTHREAD_WAITING;",
            "\tWRITE_ONCE(*j, jiffies);",
            "}",
            "static int __init rcu_spawn_core_kthreads(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;",
            "\tif (use_softirq)",
            "\t\treturn 0;",
            "\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),",
            "\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);",
            "\treturn 0;",
            "}",
            "static void rcutree_enqueue(struct rcu_data *rdp, struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\trcu_segcblist_enqueue(&rdp->cblist, head);",
            "\tif (__is_kvfree_rcu_offset((unsigned long)func))",
            "\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,",
            "\t\t\t\t\t (unsigned long)func,",
            "\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));",
            "\telse",
            "\t\ttrace_rcu_callback(rcu_state.name, head,",
            "\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));",
            "}",
            "static void call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,",
            "\t\t\t  rcu_callback_t func, unsigned long flags)",
            "{",
            "\trcutree_enqueue(rdp, head, func);",
            "\t/*",
            "\t * If called from an extended quiescent state, invoke the RCU",
            "\t * core in order to force a re-evaluation of RCU's idleness.",
            "\t */",
            "\tif (!rcu_is_watching())",
            "\t\tinvoke_rcu_core();",
            "",
            "\t/* If interrupts were disabled or CPU offline, don't invoke RCU core. */",
            "\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Force the grace period if too many callbacks or too long waiting.",
            "\t * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()",
            "\t * if some other CPU has recently done so.  Also, don't bother",
            "\t * invoking rcu_force_quiescent_state() if the newly enqueued callback",
            "\t * is the only one waiting for a grace period to complete.",
            "\t */",
            "\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >",
            "\t\t     rdp->qlen_last_fqs_check + qhimark)) {",
            "",
            "\t\t/* Are we ignoring a completed grace period? */",
            "\t\tnote_gp_changes(rdp);",
            "",
            "\t\t/* Start a new grace period if one not already started. */",
            "\t\tif (!rcu_gp_in_progress()) {",
            "\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);",
            "\t\t} else {",
            "\t\t\t/* Give the grace period a kick. */",
            "\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;",
            "\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&",
            "\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)",
            "\t\t\t\trcu_force_quiescent_state();",
            "\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "rcu_cpu_kthread_park, rcu_cpu_kthread_should_run, rcu_cpu_kthread, rcu_spawn_core_kthreads, rcutree_enqueue, call_rcu_core",
          "description": "实现RCU k线程管理与回调分发基础设施，包含线程启动、回调入队及触发条件判断逻辑，提供跨CPU的异步处理能力",
          "similarity": 0.5439459085464478
        },
        {
          "chunk_id": 22,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4072,
          "end_line": 4226,
          "content": [
            "static void rcu_barrier_trace(const char *s, int cpu, unsigned long done)",
            "{",
            "\ttrace_rcu_barrier(rcu_state.name, s, cpu,",
            "\t\t\t  atomic_read(&rcu_state.barrier_cpu_count), done);",
            "}",
            "static void rcu_barrier_callback(struct rcu_head *rhp)",
            "{",
            "\tunsigned long __maybe_unused s = rcu_state.barrier_sequence;",
            "",
            "\tif (atomic_dec_and_test(&rcu_state.barrier_cpu_count)) {",
            "\t\trcu_barrier_trace(TPS(\"LastCB\"), -1, s);",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "\t} else {",
            "\t\trcu_barrier_trace(TPS(\"CB\"), -1, s);",
            "\t}",
            "}",
            "static void rcu_barrier_entrain(struct rcu_data *rdp)",
            "{",
            "\tunsigned long gseq = READ_ONCE(rcu_state.barrier_sequence);",
            "\tunsigned long lseq = READ_ONCE(rdp->barrier_seq_snap);",
            "\tbool wake_nocb = false;",
            "\tbool was_alldone = false;",
            "",
            "\tlockdep_assert_held(&rcu_state.barrier_lock);",
            "\tif (rcu_seq_state(lseq) || !rcu_seq_state(gseq) || rcu_seq_ctr(lseq) != rcu_seq_ctr(gseq))",
            "\t\treturn;",
            "\trcu_barrier_trace(TPS(\"IRQ\"), -1, rcu_state.barrier_sequence);",
            "\trdp->barrier_head.func = rcu_barrier_callback;",
            "\tdebug_rcu_head_queue(&rdp->barrier_head);",
            "\trcu_nocb_lock(rdp);",
            "\t/*",
            "\t * Flush bypass and wakeup rcuog if we add callbacks to an empty regular",
            "\t * queue. This way we don't wait for bypass timer that can reach seconds",
            "\t * if it's fully lazy.",
            "\t */",
            "\twas_alldone = rcu_rdp_is_offloaded(rdp) && !rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));",
            "\twake_nocb = was_alldone && rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tif (rcu_segcblist_entrain(&rdp->cblist, &rdp->barrier_head)) {",
            "\t\tatomic_inc(&rcu_state.barrier_cpu_count);",
            "\t} else {",
            "\t\tdebug_rcu_head_unqueue(&rdp->barrier_head);",
            "\t\trcu_barrier_trace(TPS(\"IRQNQ\"), -1, rcu_state.barrier_sequence);",
            "\t}",
            "\trcu_nocb_unlock(rdp);",
            "\tif (wake_nocb)",
            "\t\twake_nocb_gp(rdp, false);",
            "\tsmp_store_release(&rdp->barrier_seq_snap, gseq);",
            "}",
            "static void rcu_barrier_handler(void *cpu_in)",
            "{",
            "\tuintptr_t cpu = (uintptr_t)cpu_in;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\tWARN_ON_ONCE(cpu != rdp->cpu);",
            "\tWARN_ON_ONCE(cpu != smp_processor_id());",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\trcu_barrier_entrain(rdp);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "}",
            "void rcu_barrier(void)",
            "{",
            "\tuintptr_t cpu;",
            "\tunsigned long flags;",
            "\tunsigned long gseq;",
            "\tstruct rcu_data *rdp;",
            "\tunsigned long s = rcu_seq_snap(&rcu_state.barrier_sequence);",
            "",
            "\trcu_barrier_trace(TPS(\"Begin\"), -1, s);",
            "",
            "\t/* Take mutex to serialize concurrent rcu_barrier() requests. */",
            "\tmutex_lock(&rcu_state.barrier_mutex);",
            "",
            "\t/* Did someone else do our work for us? */",
            "\tif (rcu_seq_done(&rcu_state.barrier_sequence, s)) {",
            "\t\trcu_barrier_trace(TPS(\"EarlyExit\"), -1, rcu_state.barrier_sequence);",
            "\t\tsmp_mb(); /* caller's subsequent code after above check. */",
            "\t\tmutex_unlock(&rcu_state.barrier_mutex);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Mark the start of the barrier operation. */",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\trcu_seq_start(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\trcu_barrier_trace(TPS(\"Inc1\"), -1, rcu_state.barrier_sequence);",
            "",
            "\t/*",
            "\t * Initialize the count to two rather than to zero in order",
            "\t * to avoid a too-soon return to zero in case of an immediate",
            "\t * invocation of the just-enqueued callback (or preemption of",
            "\t * this task).  Exclude CPU-hotplug operations to ensure that no",
            "\t * offline non-offloaded CPU has callbacks queued.",
            "\t */",
            "\tinit_completion(&rcu_state.barrier_completion);",
            "\tatomic_set(&rcu_state.barrier_cpu_count, 2);",
            "\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "",
            "\t/*",
            "\t * Force each CPU with callbacks to register a new callback.",
            "\t * When that callback is invoked, we will know that all of the",
            "\t * corresponding CPU's preceding callbacks have been invoked.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "retry:",
            "\t\tif (smp_load_acquire(&rdp->barrier_seq_snap) == gseq)",
            "\t\t\tcontinue;",
            "\t\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\t\tif (!rcu_segcblist_n_cbs(&rdp->cblist)) {",
            "\t\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"NQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (!rcu_rdp_cpu_online(rdp)) {",
            "\t\t\trcu_barrier_entrain(rdp);",
            "\t\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"OfflineNoCBQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\tif (smp_call_function_single(cpu, rcu_barrier_handler, (void *)cpu, 1)) {",
            "\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\tgoto retry;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\trcu_barrier_trace(TPS(\"OnlineQ\"), cpu, rcu_state.barrier_sequence);",
            "\t}",
            "",
            "\t/*",
            "\t * Now that we have an rcu_barrier_callback() callback on each",
            "\t * CPU, and thus each counted, remove the initial count.",
            "\t */",
            "\tif (atomic_sub_and_test(2, &rcu_state.barrier_cpu_count))",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "",
            "\t/* Wait for all rcu_barrier_callback() callbacks to be invoked. */",
            "\twait_for_completion(&rcu_state.barrier_completion);",
            "",
            "\t/* Mark the end of the barrier operation. */",
            "\trcu_barrier_trace(TPS(\"Inc2\"), -1, rcu_state.barrier_sequence);",
            "\trcu_seq_end(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t}",
            "",
            "\t/* Other rcu_barrier() invocations can now safely proceed. */",
            "\tmutex_unlock(&rcu_state.barrier_mutex);",
            "}"
          ],
          "function_name": "rcu_barrier_trace, rcu_barrier_callback, rcu_barrier_entrain, rcu_barrier_handler, rcu_barrier",
          "description": "实现RCU屏障功能，通过分发回调函数强制所有CPU完成当前RCU操作，使用原子计数器跟踪完成状态，通过completion等待所有回调完成",
          "similarity": 0.5370175838470459
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1221,
          "end_line": 1330,
          "content": [
            "static bool __note_gp_changes(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\tbool ret = false;",
            "\tbool need_qs;",
            "\tconst bool offloaded = rcu_rdp_is_offloaded(rdp);",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\tif (rdp->gp_seq == rnp->gp_seq)",
            "\t\treturn false; /* Nothing to do. */",
            "",
            "\t/* Handle the ends of any preceding grace periods first. */",
            "\tif (rcu_seq_completed_gp(rdp->gp_seq, rnp->gp_seq) ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) {",
            "\t\tif (!offloaded)",
            "\t\t\tret = rcu_advance_cbs(rnp, rdp); /* Advance CBs. */",
            "\t\trdp->core_needs_qs = false;",
            "\t\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuend\"));",
            "\t} else {",
            "\t\tif (!offloaded)",
            "\t\t\tret = rcu_accelerate_cbs(rnp, rdp); /* Recent CBs. */",
            "\t\tif (rdp->core_needs_qs)",
            "\t\t\trdp->core_needs_qs = !!(rnp->qsmask & rdp->grpmask);",
            "\t}",
            "",
            "\t/* Now handle the beginnings of any new-to-this-CPU grace periods. */",
            "\tif (rcu_seq_new_gp(rdp->gp_seq, rnp->gp_seq) ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) {",
            "\t\t/*",
            "\t\t * If the current grace period is waiting for this CPU,",
            "\t\t * set up to detect a quiescent state, otherwise don't",
            "\t\t * go looking for one.",
            "\t\t */",
            "\t\ttrace_rcu_grace_period(rcu_state.name, rnp->gp_seq, TPS(\"cpustart\"));",
            "\t\tneed_qs = !!(rnp->qsmask & rdp->grpmask);",
            "\t\trdp->cpu_no_qs.b.norm = need_qs;",
            "\t\trdp->core_needs_qs = need_qs;",
            "\t\tzero_cpu_stall_ticks(rdp);",
            "\t}",
            "\trdp->gp_seq = rnp->gp_seq;  /* Remember new grace-period state. */",
            "\tif (ULONG_CMP_LT(rdp->gp_seq_needed, rnp->gp_seq_needed) || rdp->gpwrap)",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU) && READ_ONCE(rdp->gpwrap))",
            "\t\tWRITE_ONCE(rdp->last_sched_clock, jiffies);",
            "\tWRITE_ONCE(rdp->gpwrap, false);",
            "\trcu_gpnum_ovf(rnp, rdp);",
            "\treturn ret;",
            "}",
            "static void note_gp_changes(struct rcu_data *rdp)",
            "{",
            "\tunsigned long flags;",
            "\tbool needwake;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tlocal_irq_save(flags);",
            "\trnp = rdp->mynode;",
            "\tif ((rdp->gp_seq == rcu_seq_current(&rnp->gp_seq) &&",
            "\t     !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */",
            "\t    !raw_spin_trylock_rcu_node(rnp)) { /* irqs already off, so later. */",
            "\t\tlocal_irq_restore(flags);",
            "\t\treturn;",
            "\t}",
            "\tneedwake = __note_gp_changes(rnp, rdp);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\trcu_strict_gp_check_qs();",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "}",
            "void rcu_gp_slow_register(atomic_t *rgssp)",
            "{",
            "\tWARN_ON_ONCE(rcu_gp_slow_suppress);",
            "",
            "\tWRITE_ONCE(rcu_gp_slow_suppress, rgssp);",
            "}",
            "void rcu_gp_slow_unregister(atomic_t *rgssp)",
            "{",
            "\tWARN_ON_ONCE(rgssp && rgssp != rcu_gp_slow_suppress && rcu_gp_slow_suppress != NULL);",
            "",
            "\tWRITE_ONCE(rcu_gp_slow_suppress, NULL);",
            "}",
            "static bool rcu_gp_slow_is_suppressed(void)",
            "{",
            "\tatomic_t *rgssp = READ_ONCE(rcu_gp_slow_suppress);",
            "",
            "\treturn rgssp && atomic_read(rgssp);",
            "}",
            "static void rcu_gp_slow(int delay)",
            "{",
            "\tif (!rcu_gp_slow_is_suppressed() && delay > 0 &&",
            "\t    !(rcu_seq_ctr(rcu_state.gp_seq) % (rcu_num_nodes * PER_RCU_NODE_PERIOD * delay)))",
            "\t\tschedule_timeout_idle(delay);",
            "}",
            "void rcu_gp_set_torture_wait(int duration)",
            "{",
            "\tif (IS_ENABLED(CONFIG_RCU_TORTURE_TEST) && duration > 0)",
            "\t\tWRITE_ONCE(sleep_duration, duration);",
            "}",
            "static void rcu_gp_torture_wait(void)",
            "{",
            "\tunsigned long duration;",
            "",
            "\tif (!IS_ENABLED(CONFIG_RCU_TORTURE_TEST))",
            "\t\treturn;",
            "\tduration = xchg(&sleep_duration, 0UL);",
            "\tif (duration > 0) {",
            "\t\tpr_alert(\"%s: Waiting %lu jiffies\\n\", __func__, duration);",
            "\t\tschedule_timeout_idle(duration);",
            "\t\tpr_alert(\"%s: Wait complete\\n\", __func__);",
            "\t}",
            "}"
          ],
          "function_name": "__note_gp_changes, note_gp_changes, rcu_gp_slow_register, rcu_gp_slow_unregister, rcu_gp_slow_is_suppressed, rcu_gp_slow, rcu_gp_set_torture_wait, rcu_gp_torture_wait",
          "description": "提供慢速模式下的grace period注册/注销接口，通过原子变量控制抑制状态，并实现基于延迟的调度控制逻辑，支持严格模式下的边界检查。",
          "similarity": 0.5293914675712585
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2273,
          "end_line": 2388,
          "content": [
            "void rcu_sched_clock_irq(int user)",
            "{",
            "\tunsigned long j;",
            "",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU)) {",
            "\t\tj = jiffies;",
            "\t\tWARN_ON_ONCE(time_before(j, __this_cpu_read(rcu_data.last_sched_clock)));",
            "\t\t__this_cpu_write(rcu_data.last_sched_clock, j);",
            "\t}",
            "\ttrace_rcu_utilization(TPS(\"Start scheduler-tick\"));",
            "\tlockdep_assert_irqs_disabled();",
            "\traw_cpu_inc(rcu_data.ticks_this_gp);",
            "\t/* The load-acquire pairs with the store-release setting to true. */",
            "\tif (smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs))) {",
            "\t\t/* Idle and userspace execution already are quiescent states. */",
            "\t\tif (!rcu_is_cpu_rrupt_from_idle() && !user) {",
            "\t\t\tset_tsk_need_resched(current);",
            "\t\t\tset_preempt_need_resched();",
            "\t\t}",
            "\t\t__this_cpu_write(rcu_data.rcu_urgent_qs, false);",
            "\t}",
            "\trcu_flavor_sched_clock_irq(user);",
            "\tif (rcu_pending(user))",
            "\t\tinvoke_rcu_core();",
            "\tif (user || rcu_is_cpu_rrupt_from_idle())",
            "\t\trcu_note_voluntary_context_switch(current);",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\ttrace_rcu_utilization(TPS(\"End scheduler-tick\"));",
            "}",
            "static void force_qs_rnp(int (*f)(struct rcu_data *rdp))",
            "{",
            "\tint cpu;",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trcu_state.cbovld = rcu_state.cbovldnext;",
            "\trcu_state.cbovldnext = false;",
            "\trcu_for_each_leaf_node(rnp) {",
            "\t\tunsigned long mask = 0;",
            "\t\tunsigned long rsmask = 0;",
            "",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\trcu_state.cbovldnext |= !!rnp->cbovldmask;",
            "\t\tif (rnp->qsmask == 0) {",
            "\t\t\tif (rcu_preempt_blocked_readers_cgp(rnp)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No point in scanning bits because they",
            "\t\t\t\t * are all zero.  But we might need to",
            "\t\t\t\t * priority-boost blocked readers.",
            "\t\t\t\t */",
            "\t\t\t\trcu_initiate_boost(rnp, flags);",
            "\t\t\t\t/* rcu_initiate_boost() releases rnp->lock */",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rnp->qsmask) {",
            "\t\t\tstruct rcu_data *rdp;",
            "\t\t\tint ret;",
            "",
            "\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\t\t\tret = f(rdp);",
            "\t\t\tif (ret > 0) {",
            "\t\t\t\tmask |= rdp->grpmask;",
            "\t\t\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\t\t}",
            "\t\t\tif (ret < 0)",
            "\t\t\t\trsmask |= rdp->grpmask;",
            "\t\t}",
            "\t\tif (mask != 0) {",
            "\t\t\t/* Idle/offline CPUs, report (releases rnp->lock). */",
            "\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\t} else {",
            "\t\t\t/* Nothing to do here, so just drop the lock. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t}",
            "",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rsmask)",
            "\t\t\tresched_cpu(cpu);",
            "\t}",
            "}",
            "void rcu_force_quiescent_state(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool ret;",
            "\tstruct rcu_node *rnp;",
            "\tstruct rcu_node *rnp_old = NULL;",
            "",
            "\t/* Funnel through hierarchy to reduce memory contention. */",
            "\trnp = raw_cpu_read(rcu_data.mynode);",
            "\tfor (; rnp != NULL; rnp = rnp->parent) {",
            "\t\tret = (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) ||",
            "\t\t       !raw_spin_trylock(&rnp->fqslock);",
            "\t\tif (rnp_old != NULL)",
            "\t\t\traw_spin_unlock(&rnp_old->fqslock);",
            "\t\tif (ret)",
            "\t\t\treturn;",
            "\t\trnp_old = rnp;",
            "\t}",
            "\t/* rnp_old == rcu_get_root(), rnp == NULL. */",
            "",
            "\t/* Reached the root of the rcu_node tree, acquire lock. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp_old, flags);",
            "\traw_spin_unlock(&rnp_old->fqslock);",
            "\tif (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\t\treturn;  /* Someone beat us to it. */",
            "\t}",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\trcu_gp_kthread_wake();",
            "}"
          ],
          "function_name": "rcu_sched_clock_irq, force_qs_rnp, rcu_force_quiescent_state",
          "description": "实现强制quiescent状态触发与调度器中断处理，包含优先级提升、回调加速及grace period推进等关键控制流，维护RCU状态一致性",
          "similarity": 0.5234572887420654
        }
      ]
    },
    {
      "source_file": "kernel/rcu/srcutiny.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:42:53\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\srcutiny.c`\n\n---\n\n# `rcu/srcutiny.c` 技术文档\n\n## 1. 文件概述\n\n`srcutiny.c` 是 Linux 内核中 **Sleepable Read-Copy Update (SRCU)** 机制的“精简版”（tiny version）实现，专为 **单 CPU 且不可抢占（non-preemptible）** 的系统环境设计。该文件提供了一种轻量级的读-拷贝-更新（RCU）变体，允许读端临界区中睡眠（即“sleepable”），适用于对性能要求不高但需要简单同步语义的嵌入式或特殊用途内核配置。\n\n与完整版 SRCU 不同，`srcutiny` 利用单 CPU 无并发的特性，大幅简化了同步逻辑，避免了复杂的锁和内存屏障操作，从而实现极简的代码路径。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct srcu_struct`：SRCU 同步域的核心结构，包含：\n  - `srcu_lock_nesting[2]`：两个计数器，用于跟踪两个交替的读端临界区嵌套深度。\n  - `srcu_wq`：等待队列，用于在宽限期（grace period）结束时唤醒等待者。\n  - `srcu_cb_head` / `srcu_cb_tail`：回调链表头尾指针，存储待执行的宽限期后回调。\n  - `srcu_gp_running` / `srcu_gp_waiting`：标志位，表示宽限期是否正在运行或等待读端退出。\n  - `srcu_idx` / `srcu_idx_max`：宽限期索引，用于状态追踪和轮转。\n  - `srcu_work`：工作队列项，用于异步驱动宽限期处理。\n\n### 主要函数\n- `init_srcu_struct()` / `__init_srcu_struct()`：初始化 `srcu_struct` 实例。\n- `cleanup_srcu_struct()`：清理并验证 `srcu_struct` 状态，防止资源泄漏。\n- `__srcu_read_unlock()`：读端解锁，减少嵌套计数，必要时唤醒宽限期等待者。\n- `srcu_drive_gp()`：工作队列处理函数，执行宽限期推进和回调调用。\n- `call_srcu()`：注册一个宽限期结束后的回调函数。\n- `synchronize_srcu()`：同步等待当前所有读端临界区完成。\n- `get_state_synchronize_srcu()` / `start_poll_synchronize_srcu()` / `poll_state_synchronize_srcu()`：提供异步轮询接口，用于非阻塞式宽限期检测。\n- `srcu_init()`：内核启动后期初始化函数，处理早期注册的 SRCU 回调。\n- `rcu_scheduler_starting()`：标记调度器已启动，使 SRCU 进入活跃状态。\n\n## 3. 关键实现\n\n### 宽限期轮转机制\n- 使用 **双缓冲计数器**（`srcu_lock_nesting[0]` 和 `[1]`）实现读端计数。\n- `srcu_idx` 为递增整数，其低两位隐含当前活跃的计数器索引：\n  - 当 `srcu_idx & 0x2 == 0` 时，使用索引 0；\n  - 当 `srcu_idx & 0x2 == 2` 时，使用索引 1。\n- 宽限期推进时，先切换索引（`srcu_idx += 1`），等待旧索引计数归零，再切换回（`srcu_idx += 1`），完成一个完整周期。\n\n### 回调处理\n- 所有通过 `call_srcu()` 注册的回调被追加到 `srcu_cb_head` 链表。\n- `srcu_drive_gp()` 在工作队列上下文中执行：\n  1. 原子地摘下整个回调链表；\n  2. 等待对应读端计数归零（使用 `swait_event_exclusive`）；\n  3. 依次调用所有回调函数（在 `local_bh_disable()` 保护下）；\n  4. 若仍有新回调，重新调度自身。\n\n### 启动阶段处理\n- 在调度器启动前（`rcu_scheduler_active == RCU_SCHEDULER_INACTIVE`），`synchronize_srcu()` 直接返回，不执行同步。\n- 早期调用 `call_srcu()` 会将 `srcu_work` 加入 `srcu_boot_list`。\n- `srcu_init()` 在调度器启动后被调用，将 `srcu_boot_list` 中的结构体加入工作队列执行。\n\n### 轮询接口设计\n- `get_state_synchronize_srcu()` 返回一个“宽限期结束 cookie”：`(srcu_idx + 3) & ~0x1`，确保该值落在未来某个完整宽限期之后。\n- `poll_state_synchronize_srcu()` 通过比较当前 `srcu_idx` 与 cookie 判断宽限期是否已过，支持环绕比较（`ULONG_CMP_GE/LT`）。\n\n### 单 CPU 优化\n- 由于仅支持单 CPU 且不可抢占，无需内存屏障或复杂同步原语。\n- 使用 `local_irq_save/restore` 保护回调链表操作，避免中断上下文干扰。\n- 宽限期等待使用简单的等待队列，无需跨 CPU 通信。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/srcu.h>`：SRCU 接口定义。\n  - `<linux/rcupdate_wait.h>`：提供 `swait_*` 等待原语。\n  - `<linux/workqueue.h>`（隐式）：通过 `INIT_WORK` 和 `schedule_work` 使用工作队列。\n  - `\"rcu.h\"` / `\"rcu_segcblist.h\"`：RCU 内部辅助结构。\n- **内核配置依赖**：\n  - 仅在 `CONFIG_PREEMPT=n` 且单 CPU 环境下启用（由构建系统控制）。\n  - `CONFIG_DEBUG_LOCK_ALLOC` 影响初始化函数签名，用于锁依赖验证。\n- **与其他 RCU 子系统关系**：\n  - 与 `tree_srcu.c`（完整版 SRCU）互斥，根据配置选择其一。\n  - 共享 `rcu_head`、`rcu_callback_t` 等通用 RCU 类型。\n\n## 5. 使用场景\n\n- **嵌入式或实时系统**：在资源受限、单核、不可抢占的内核配置中提供轻量级读写同步。\n- **允许读端睡眠的场景**：当读端临界区需要调用可能阻塞的函数（如内存分配、文件操作）时，传统 RCU 不适用，SRCU 是合适选择。\n- **模块或子系统私有同步域**：每个 `srcu_struct` 实例代表一个独立的同步域，适用于需要隔离同步范围的驱动或内核组件。\n- **启动早期同步**：通过 `srcu_boot_list` 机制，支持在调度器启动前注册回调，延迟到初始化后期执行。\n- **异步宽限期检测**：通过 `start_poll_synchronize_srcu()` + `poll_state_synchronize_srcu()` 组合，实现非阻塞式宽限期等待，适用于不能睡眠的上下文。",
      "similarity": 0.5527485609054565,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/rcu/srcutiny.c",
          "start_line": 179,
          "end_line": 252,
          "content": [
            "void call_srcu(struct srcu_struct *ssp, struct rcu_head *rhp,",
            "\t       rcu_callback_t func)",
            "{",
            "\tunsigned long flags;",
            "",
            "\trhp->func = func;",
            "\trhp->next = NULL;",
            "\tlocal_irq_save(flags);",
            "\t*ssp->srcu_cb_tail = rhp;",
            "\tssp->srcu_cb_tail = &rhp->next;",
            "\tlocal_irq_restore(flags);",
            "\tsrcu_gp_start_if_needed(ssp);",
            "}",
            "void synchronize_srcu(struct srcu_struct *ssp)",
            "{",
            "\tstruct rcu_synchronize rs;",
            "",
            "\tsrcu_lock_sync(&ssp->dep_map);",
            "",
            "\tRCU_LOCKDEP_WARN(lockdep_is_held(ssp) ||",
            "\t\t\tlock_is_held(&rcu_bh_lock_map) ||",
            "\t\t\tlock_is_held(&rcu_lock_map) ||",
            "\t\t\tlock_is_held(&rcu_sched_lock_map),",
            "\t\t\t\"Illegal synchronize_srcu() in same-type SRCU (or in RCU) read-side critical section\");",
            "",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_INACTIVE)",
            "\t\treturn;",
            "",
            "\tmight_sleep();",
            "\tinit_rcu_head_on_stack(&rs.head);",
            "\tinit_completion(&rs.completion);",
            "\tcall_srcu(ssp, &rs.head, wakeme_after_rcu);",
            "\twait_for_completion(&rs.completion);",
            "\tdestroy_rcu_head_on_stack(&rs.head);",
            "}",
            "unsigned long get_state_synchronize_srcu(struct srcu_struct *ssp)",
            "{",
            "\tunsigned long ret;",
            "",
            "\tbarrier();",
            "\tret = (READ_ONCE(ssp->srcu_idx) + 3) & ~0x1;",
            "\tbarrier();",
            "\treturn ret;",
            "}",
            "unsigned long start_poll_synchronize_srcu(struct srcu_struct *ssp)",
            "{",
            "\tunsigned long ret = get_state_synchronize_srcu(ssp);",
            "",
            "\tsrcu_gp_start_if_needed(ssp);",
            "\treturn ret;",
            "}",
            "bool poll_state_synchronize_srcu(struct srcu_struct *ssp, unsigned long cookie)",
            "{",
            "\tunsigned long cur_s = READ_ONCE(ssp->srcu_idx);",
            "",
            "\tbarrier();",
            "\treturn ULONG_CMP_GE(cur_s, cookie) || ULONG_CMP_LT(cur_s, cookie - 3);",
            "}",
            "void __init rcu_scheduler_starting(void)",
            "{",
            "\trcu_scheduler_active = RCU_SCHEDULER_RUNNING;",
            "}",
            "void __init srcu_init(void)",
            "{",
            "\tstruct srcu_struct *ssp;",
            "",
            "\tsrcu_init_done = true;",
            "\twhile (!list_empty(&srcu_boot_list)) {",
            "\t\tssp = list_first_entry(&srcu_boot_list,",
            "\t\t\t\t      struct srcu_struct, srcu_work.entry);",
            "\t\tlist_del_init(&ssp->srcu_work.entry);",
            "\t\tschedule_work(&ssp->srcu_work);",
            "\t}",
            "}"
          ],
          "function_name": "call_srcu, synchronize_srcu, get_state_synchronize_srcu, start_poll_synchronize_srcu, poll_state_synchronize_srcu, rcu_scheduler_starting, srcu_init",
          "description": "提供SRCU回调注册接口call_srcu，同步屏障synchronize_srcu通过completion等待回调完成，包含状态查询函数和初始化入口点srcu_init。",
          "similarity": 0.4845257103443146
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/rcu/srcutiny.c",
          "start_line": 27,
          "end_line": 132,
          "content": [
            "static int init_srcu_struct_fields(struct srcu_struct *ssp)",
            "{",
            "\tssp->srcu_lock_nesting[0] = 0;",
            "\tssp->srcu_lock_nesting[1] = 0;",
            "\tinit_swait_queue_head(&ssp->srcu_wq);",
            "\tssp->srcu_cb_head = NULL;",
            "\tssp->srcu_cb_tail = &ssp->srcu_cb_head;",
            "\tssp->srcu_gp_running = false;",
            "\tssp->srcu_gp_waiting = false;",
            "\tssp->srcu_idx = 0;",
            "\tssp->srcu_idx_max = 0;",
            "\tINIT_WORK(&ssp->srcu_work, srcu_drive_gp);",
            "\tINIT_LIST_HEAD(&ssp->srcu_work.entry);",
            "\treturn 0;",
            "}",
            "int __init_srcu_struct(struct srcu_struct *ssp, const char *name,",
            "\t\t       struct lock_class_key *key)",
            "{",
            "\t/* Don't re-initialize a lock while it is held. */",
            "\tdebug_check_no_locks_freed((void *)ssp, sizeof(*ssp));",
            "\tlockdep_init_map(&ssp->dep_map, name, key, 0);",
            "\treturn init_srcu_struct_fields(ssp);",
            "}",
            "int init_srcu_struct(struct srcu_struct *ssp)",
            "{",
            "\treturn init_srcu_struct_fields(ssp);",
            "}",
            "void cleanup_srcu_struct(struct srcu_struct *ssp)",
            "{",
            "\tWARN_ON(ssp->srcu_lock_nesting[0] || ssp->srcu_lock_nesting[1]);",
            "\tflush_work(&ssp->srcu_work);",
            "\tWARN_ON(ssp->srcu_gp_running);",
            "\tWARN_ON(ssp->srcu_gp_waiting);",
            "\tWARN_ON(ssp->srcu_cb_head);",
            "\tWARN_ON(&ssp->srcu_cb_head != ssp->srcu_cb_tail);",
            "\tWARN_ON(ssp->srcu_idx != ssp->srcu_idx_max);",
            "\tWARN_ON(ssp->srcu_idx & 0x1);",
            "}",
            "void __srcu_read_unlock(struct srcu_struct *ssp, int idx)",
            "{",
            "\tint newval = READ_ONCE(ssp->srcu_lock_nesting[idx]) - 1;",
            "",
            "\tWRITE_ONCE(ssp->srcu_lock_nesting[idx], newval);",
            "\tif (!newval && READ_ONCE(ssp->srcu_gp_waiting) && in_task())",
            "\t\tswake_up_one(&ssp->srcu_wq);",
            "}",
            "void srcu_drive_gp(struct work_struct *wp)",
            "{",
            "\tint idx;",
            "\tstruct rcu_head *lh;",
            "\tstruct rcu_head *rhp;",
            "\tstruct srcu_struct *ssp;",
            "",
            "\tssp = container_of(wp, struct srcu_struct, srcu_work);",
            "\tif (ssp->srcu_gp_running || ULONG_CMP_GE(ssp->srcu_idx, READ_ONCE(ssp->srcu_idx_max)))",
            "\t\treturn; /* Already running or nothing to do. */",
            "",
            "\t/* Remove recently arrived callbacks and wait for readers. */",
            "\tWRITE_ONCE(ssp->srcu_gp_running, true);",
            "\tlocal_irq_disable();",
            "\tlh = ssp->srcu_cb_head;",
            "\tssp->srcu_cb_head = NULL;",
            "\tssp->srcu_cb_tail = &ssp->srcu_cb_head;",
            "\tlocal_irq_enable();",
            "\tidx = (ssp->srcu_idx & 0x2) / 2;",
            "\tWRITE_ONCE(ssp->srcu_idx, ssp->srcu_idx + 1);",
            "\tWRITE_ONCE(ssp->srcu_gp_waiting, true);  /* srcu_read_unlock() wakes! */",
            "\tswait_event_exclusive(ssp->srcu_wq, !READ_ONCE(ssp->srcu_lock_nesting[idx]));",
            "\tWRITE_ONCE(ssp->srcu_gp_waiting, false); /* srcu_read_unlock() cheap. */",
            "\tWRITE_ONCE(ssp->srcu_idx, ssp->srcu_idx + 1);",
            "",
            "\t/* Invoke the callbacks we removed above. */",
            "\twhile (lh) {",
            "\t\trhp = lh;",
            "\t\tlh = lh->next;",
            "\t\tdebug_rcu_head_callback(rhp);",
            "\t\tlocal_bh_disable();",
            "\t\trhp->func(rhp);",
            "\t\tlocal_bh_enable();",
            "\t}",
            "",
            "\t/*",
            "\t * Enable rescheduling, and if there are more callbacks,",
            "\t * reschedule ourselves.  This can race with a call_srcu()",
            "\t * at interrupt level, but the ->srcu_gp_running checks will",
            "\t * straighten that out.",
            "\t */",
            "\tWRITE_ONCE(ssp->srcu_gp_running, false);",
            "\tif (ULONG_CMP_LT(ssp->srcu_idx, READ_ONCE(ssp->srcu_idx_max)))",
            "\t\tschedule_work(&ssp->srcu_work);",
            "}",
            "static void srcu_gp_start_if_needed(struct srcu_struct *ssp)",
            "{",
            "\tunsigned long cookie;",
            "",
            "\tcookie = get_state_synchronize_srcu(ssp);",
            "\tif (ULONG_CMP_GE(READ_ONCE(ssp->srcu_idx_max), cookie))",
            "\t\treturn;",
            "\tWRITE_ONCE(ssp->srcu_idx_max, cookie);",
            "\tif (!READ_ONCE(ssp->srcu_gp_running)) {",
            "\t\tif (likely(srcu_init_done))",
            "\t\t\tschedule_work(&ssp->srcu_work);",
            "\t\telse if (list_empty(&ssp->srcu_work.entry))",
            "\t\t\tlist_add(&ssp->srcu_work.entry, &srcu_boot_list);",
            "\t}",
            "}"
          ],
          "function_name": "init_srcu_struct_fields, __init_srcu_struct, init_srcu_struct, cleanup_srcu_struct, __srcu_read_unlock, srcu_drive_gp, srcu_gp_start_if_needed",
          "description": "实现SRCU结构体初始化与清理逻辑，包含锁嵌套计数器初始化、回调链表管理、GP（Grace Period）驱动工作项调度以及读解锁唤醒机制。",
          "similarity": 0.4675831198692322
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/rcu/srcutiny.c",
          "start_line": 1,
          "end_line": 26,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0+",
            "/*",
            " * Sleepable Read-Copy Update mechanism for mutual exclusion,",
            " *\ttiny version for non-preemptible single-CPU use.",
            " *",
            " * Copyright (C) IBM Corporation, 2017",
            " *",
            " * Author: Paul McKenney <paulmck@linux.ibm.com>",
            " */",
            "",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/preempt.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/sched.h>",
            "#include <linux/delay.h>",
            "#include <linux/srcu.h>",
            "",
            "#include <linux/rcu_node_tree.h>",
            "#include \"rcu_segcblist.h\"",
            "#include \"rcu.h\"",
            "",
            "int rcu_scheduler_active __read_mostly;",
            "static LIST_HEAD(srcu_boot_list);",
            "static bool srcu_init_done;",
            ""
          ],
          "function_name": null,
          "description": "声明SRCU机制的全局变量和头文件，包含srcu_init_done标志位用于跟踪初始化状态，srcu_boot_list用于存储延迟初始化的工作项。",
          "similarity": 0.45867598056793213
        }
      ]
    },
    {
      "source_file": "kernel/stop_machine.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:30:03\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `stop_machine.c`\n\n---\n\n# `stop_machine.c` 技术文档\n\n## 1. 文件概述\n\n`stop_machine.c` 实现了 Linux 内核中用于在所有（或指定）CPU 上同步执行特定函数的机制，即 **stop_machine** 机制。该机制通过为每个 CPU 创建一个高优先级的内核线程（称为 stopper），在需要时唤醒这些线程以执行指定任务，并确保在执行期间其他任务无法抢占，从而实现对整个系统或部分 CPU 的“冻结”式同步操作。此机制常用于需要全局一致状态的关键内核操作，如 CPU 热插拔、模块加载、内核热补丁（livepatch）等。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct cpu_stop_done`**  \n  用于协调多个 CPU 上 stop 任务的完成状态，包含待完成任务计数（`nr_todo`）、返回值（`ret`）和完成信号量（`completion`）。\n\n- **`struct cpu_stopper`**  \n  每个 CPU 对应一个 stopper 实例，包含：\n  - `thread`：stopper 内核线程\n  - `lock`：保护 pending works 链表的自旋锁\n  - `enabled`：该 stopper 是否启用（对应 CPU 是否在线）\n  - `works`：待执行的 `cpu_stop_work` 链表\n  - `stop_work`、`caller`、`fn`：用于 `stop_cpus` 的临时字段\n\n- **`struct multi_stop_data`**  \n  用于多 CPU 同步执行的共享控制结构，包含：\n  - `fn` 和 `data`：要执行的函数及其参数\n  - `num_threads`：参与同步的线程数\n  - `active_cpus`：指定哪些 CPU 需要实际执行函数\n  - `state`：全局状态机（`MULTI_STOP_*` 枚举）\n  - `thread_ack`：用于状态同步的原子计数器\n\n- **`enum multi_stop_state`**  \n  多 CPU 同步执行的状态机，包括：\n  - `MULTI_STOP_NONE`\n  - `MULTI_STOP_PREPARE`\n  - `MULTI_STOP_DISABLE_IRQ`\n  - `MULTI_STOP_RUN`\n  - `MULTI_STOP_EXIT`\n\n### 主要函数\n\n- **`stop_one_cpu(cpu, fn, arg)`**  \n  在指定 CPU 上执行函数 `fn(arg)`，阻塞等待执行完成。若 CPU 离线则返回 `-ENOENT`。\n\n- **`cpu_stop_queue_work(cpu, work)`**  \n  将 stop 任务加入指定 CPU 的 stopper 队列，若 CPU 在线则唤醒其 stopper 线程。\n\n- **`multi_cpu_stop(data)`**  \n  stopper 线程的主函数，实现多 CPU 同步状态机，负责禁用中断、执行函数、状态同步等。\n\n- **`print_stop_info(log_lvl, task)`**  \n  调试辅助函数，若 `task` 是 stopper 线程，则打印其当前执行函数及调用者信息。\n\n- **`set_state()` / `ack_state()`**  \n  控制多 CPU 同步状态机的推进：`set_state` 设置新状态并重置 ack 计数器，`ack_state` 用于线程确认状态，最后一个确认者推进到下一状态。\n\n## 3. 关键实现\n\n### Stopper 线程模型\n- 每个可能的 CPU 都有一个 `cpu_stopper` 实例，其中包含一个专用内核线程。\n- 该线程运行 `multi_cpu_stop` 函数，处于高优先级实时调度策略（由 `smpboot` 框架设置），可抢占普通任务。\n- 当有 stop 任务时，通过 `wake_up_process` 唤醒对应 stopper 线程。\n\n### 多 CPU 同步状态机\n- 使用共享的 `multi_stop_data` 结构协调所有参与 CPU。\n- 状态转换通过 `set_state` 触发，所有线程通过轮询 `msdata->state` 检测状态变化。\n- 每个状态变更需所有线程调用 `ack_state` 确认，最后一个确认者推进到下一状态，确保严格同步。\n- 在 `MULTI_STOP_DISABLE_IRQ` 状态下，所有参与 CPU 禁用本地中断（包括硬中断），ARM64 还会屏蔽 SDEI 事件。\n- 仅 `active_cpus` 中的 CPU 在 `MULTI_STOP_RUN` 状态执行实际函数。\n\n### 中断与 NMI 安全\n- 执行期间禁用本地中断，防止中断处理程序干扰关键操作。\n- 在等待状态循环中调用 `touch_nmi_watchdog()` 防止 NMI watchdog 误报硬锁死。\n- 使用 `rcu_momentary_dyntick_idle()` 通知 RCU 系统当前 CPU 处于空闲状态，避免 RCU stall。\n\n### CPU 热插拔处理\n- `cpu_stopper.enabled` 标志反映 CPU 在线状态。\n- 若 CPU 离线时提交 stop 任务，则立即完成（调用 `cpu_stop_signal_done`），避免阻塞。\n- 支持从非活动 CPU（如 CPU hotplug 的 bringup 路径）调用 `stop_machine`，此时中断可能已禁用，需保存/恢复中断状态。\n\n### 死锁预防\n- `cpu_stop_queue_two_works` 函数通过嵌套锁（`SINGLE_DEPTH_NESTING`）和重试机制，确保两个 stopper 的入队操作原子性，避免与 `stop_cpus` 并发导致的死锁。\n- 使用 `preempt_disable()` 保证唤醒操作在不可抢占上下文中完成，防止唤醒丢失。\n\n## 4. 依赖关系\n\n- **调度子系统**：依赖 `kthread` 创建 stopper 线程，使用 `wake_up_process` 唤醒。\n- **SMP 子系统**：依赖 `smpboot.h` 的 CPU 热插拔通知机制来启用/禁用 stopper。\n- **中断子系统**：调用 `local_irq_disable/restore`、`hard_irq_disable` 控制中断。\n- **RCU 子系统**：通过 `rcu_momentary_dyntick_idle` 与 RCU 交互。\n- **NMI 子系统**：调用 `touch_nmi_watchdog` 避免 watchdog 误报。\n- **ARM64 架构**：条件编译包含 SDEI（Software Delegated Exception Interface）屏蔽/解除屏蔽。\n- **Per-CPU 基础设施**：使用 `DEFINE_PER_CPU` 和 `per_cpu_ptr` 管理 per-CPU stopper 实例。\n\n## 5. 使用场景\n\n- **CPU 热插拔**：在 CPU 上线/下线过程中执行需要全局同步的操作。\n- **内核模块加载/卸载**：某些架构或功能（如 ftrace）需要 stop_machine 来安全修改内核文本。\n- **内核热补丁（Livepatch）**：在应用补丁时冻结所有 CPU 以确保一致性。\n- **动态 tracing（如 ftrace）**：修改函数入口指令时需 stop_machine 保证原子性。\n- **内存热插拔**：某些内存操作需要全局同步。\n- **内核调试与诊断**：通过 `print_stop_info` 辅助分析 stopper 行为。\n- **架构特定操作**：如 ARM64 的 SDEI 事件处理需要在 stop_machine 上下文中屏蔽。",
      "similarity": 0.5510013103485107,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "kernel/stop_machine.c",
          "start_line": 401,
          "end_line": 502,
          "content": [
            "static bool queue_stop_cpus_work(const struct cpumask *cpumask,",
            "\t\t\t\t cpu_stop_fn_t fn, void *arg,",
            "\t\t\t\t struct cpu_stop_done *done)",
            "{",
            "\tstruct cpu_stop_work *work;",
            "\tunsigned int cpu;",
            "\tbool queued = false;",
            "",
            "\t/*",
            "\t * Disable preemption while queueing to avoid getting",
            "\t * preempted by a stopper which might wait for other stoppers",
            "\t * to enter @fn which can lead to deadlock.",
            "\t */",
            "\tpreempt_disable();",
            "\tstop_cpus_in_progress = true;",
            "\tbarrier();",
            "\tfor_each_cpu(cpu, cpumask) {",
            "\t\twork = &per_cpu(cpu_stopper.stop_work, cpu);",
            "\t\twork->fn = fn;",
            "\t\twork->arg = arg;",
            "\t\twork->done = done;",
            "\t\twork->caller = _RET_IP_;",
            "\t\tif (cpu_stop_queue_work(cpu, work))",
            "\t\t\tqueued = true;",
            "\t}",
            "\tbarrier();",
            "\tstop_cpus_in_progress = false;",
            "\tpreempt_enable();",
            "",
            "\treturn queued;",
            "}",
            "static int __stop_cpus(const struct cpumask *cpumask,",
            "\t\t       cpu_stop_fn_t fn, void *arg)",
            "{",
            "\tstruct cpu_stop_done done;",
            "",
            "\tcpu_stop_init_done(&done, cpumask_weight(cpumask));",
            "\tif (!queue_stop_cpus_work(cpumask, fn, arg, &done))",
            "\t\treturn -ENOENT;",
            "\twait_for_completion(&done.completion);",
            "\treturn done.ret;",
            "}",
            "static int stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)",
            "{",
            "\tint ret;",
            "",
            "\t/* static works are used, process one request at a time */",
            "\tmutex_lock(&stop_cpus_mutex);",
            "\tret = __stop_cpus(cpumask, fn, arg);",
            "\tmutex_unlock(&stop_cpus_mutex);",
            "\treturn ret;",
            "}",
            "static int cpu_stop_should_run(unsigned int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "\tunsigned long flags;",
            "\tint run;",
            "",
            "\traw_spin_lock_irqsave(&stopper->lock, flags);",
            "\trun = !list_empty(&stopper->works);",
            "\traw_spin_unlock_irqrestore(&stopper->lock, flags);",
            "\treturn run;",
            "}",
            "static void cpu_stopper_thread(unsigned int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "\tstruct cpu_stop_work *work;",
            "",
            "repeat:",
            "\twork = NULL;",
            "\traw_spin_lock_irq(&stopper->lock);",
            "\tif (!list_empty(&stopper->works)) {",
            "\t\twork = list_first_entry(&stopper->works,",
            "\t\t\t\t\tstruct cpu_stop_work, list);",
            "\t\tlist_del_init(&work->list);",
            "\t}",
            "\traw_spin_unlock_irq(&stopper->lock);",
            "",
            "\tif (work) {",
            "\t\tcpu_stop_fn_t fn = work->fn;",
            "\t\tvoid *arg = work->arg;",
            "\t\tstruct cpu_stop_done *done = work->done;",
            "\t\tint ret;",
            "",
            "\t\t/* cpu stop callbacks must not sleep, make in_atomic() == T */",
            "\t\tstopper->caller = work->caller;",
            "\t\tstopper->fn = fn;",
            "\t\tpreempt_count_inc();",
            "\t\tret = fn(arg);",
            "\t\tif (done) {",
            "\t\t\tif (ret)",
            "\t\t\t\tdone->ret = ret;",
            "\t\t\tcpu_stop_signal_done(done);",
            "\t\t}",
            "\t\tpreempt_count_dec();",
            "\t\tstopper->fn = NULL;",
            "\t\tstopper->caller = 0;",
            "\t\tWARN_ONCE(preempt_count(),",
            "\t\t\t  \"cpu_stop: %ps(%p) leaked preempt count\\n\", fn, arg);",
            "\t\tgoto repeat;",
            "\t}",
            "}"
          ],
          "function_name": "queue_stop_cpus_work, __stop_cpus, stop_cpus, cpu_stop_should_run, cpu_stopper_thread",
          "description": "实现批量CPU停止的中枢逻辑，通过互斥锁保证串行化处理。包含工作分发、停止执行、状态追踪等功能，支持通用CPU掩码的停止操作，并提供预处理检查和结果收集机制。",
          "similarity": 0.5215311050415039
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/stop_machine.c",
          "start_line": 269,
          "end_line": 369,
          "content": [
            "static int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,",
            "\t\t\t\t    int cpu2, struct cpu_stop_work *work2)",
            "{",
            "\tstruct cpu_stopper *stopper1 = per_cpu_ptr(&cpu_stopper, cpu1);",
            "\tstruct cpu_stopper *stopper2 = per_cpu_ptr(&cpu_stopper, cpu2);",
            "\tint err;",
            "",
            "retry:",
            "\t/*",
            "\t * The waking up of stopper threads has to happen in the same",
            "\t * scheduling context as the queueing.  Otherwise, there is a",
            "\t * possibility of one of the above stoppers being woken up by another",
            "\t * CPU, and preempting us. This will cause us to not wake up the other",
            "\t * stopper forever.",
            "\t */",
            "\tpreempt_disable();",
            "\traw_spin_lock_irq(&stopper1->lock);",
            "\traw_spin_lock_nested(&stopper2->lock, SINGLE_DEPTH_NESTING);",
            "",
            "\tif (!stopper1->enabled || !stopper2->enabled) {",
            "\t\terr = -ENOENT;",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\t/*",
            "\t * Ensure that if we race with __stop_cpus() the stoppers won't get",
            "\t * queued up in reverse order leading to system deadlock.",
            "\t *",
            "\t * We can't miss stop_cpus_in_progress if queue_stop_cpus_work() has",
            "\t * queued a work on cpu1 but not on cpu2, we hold both locks.",
            "\t *",
            "\t * It can be falsely true but it is safe to spin until it is cleared,",
            "\t * queue_stop_cpus_work() does everything under preempt_disable().",
            "\t */",
            "\tif (unlikely(stop_cpus_in_progress)) {",
            "\t\terr = -EDEADLK;",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\terr = 0;",
            "\t__cpu_stop_queue_work(stopper1, work1);",
            "\t__cpu_stop_queue_work(stopper2, work2);",
            "",
            "unlock:",
            "\traw_spin_unlock(&stopper2->lock);",
            "\traw_spin_unlock_irq(&stopper1->lock);",
            "",
            "\tif (unlikely(err == -EDEADLK)) {",
            "\t\tpreempt_enable();",
            "",
            "\t\twhile (stop_cpus_in_progress)",
            "\t\t\tcpu_relax();",
            "",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tif (!err) {",
            "\t\twake_up_process(stopper1->thread);",
            "\t\twake_up_process(stopper2->thread);",
            "\t}",
            "\tpreempt_enable();",
            "",
            "\treturn err;",
            "}",
            "int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *arg)",
            "{",
            "\tstruct cpu_stop_done done;",
            "\tstruct cpu_stop_work work1, work2;",
            "\tstruct multi_stop_data msdata;",
            "",
            "\tmsdata = (struct multi_stop_data){",
            "\t\t.fn = fn,",
            "\t\t.data = arg,",
            "\t\t.num_threads = 2,",
            "\t\t.active_cpus = cpumask_of(cpu1),",
            "\t};",
            "",
            "\twork1 = work2 = (struct cpu_stop_work){",
            "\t\t.fn = multi_cpu_stop,",
            "\t\t.arg = &msdata,",
            "\t\t.done = &done,",
            "\t\t.caller = _RET_IP_,",
            "\t};",
            "",
            "\tcpu_stop_init_done(&done, 2);",
            "\tset_state(&msdata, MULTI_STOP_PREPARE);",
            "",
            "\tif (cpu1 > cpu2)",
            "\t\tswap(cpu1, cpu2);",
            "\tif (cpu_stop_queue_two_works(cpu1, &work1, cpu2, &work2))",
            "\t\treturn -ENOENT;",
            "",
            "\twait_for_completion(&done.completion);",
            "\treturn done.ret;",
            "}",
            "bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,",
            "\t\t\tstruct cpu_stop_work *work_buf)",
            "{",
            "\t*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, .caller = _RET_IP_, };",
            "\treturn cpu_stop_queue_work(cpu, work_buf);",
            "}"
          ],
          "function_name": "cpu_stop_queue_two_works, stop_two_cpus, stop_one_cpu_nowait",
          "description": "实现双CPU停止操作的协同逻辑，通过原子操作防止死锁，确保两个CPU的停止工作被正确排队和唤醒。包含针对两个CPU的停止接口和非阻塞式单CPU停止标记功能。",
          "similarity": 0.5121492147445679
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/stop_machine.c",
          "start_line": 56,
          "end_line": 201,
          "content": [
            "void print_stop_info(const char *log_lvl, struct task_struct *task)",
            "{",
            "\t/*",
            "\t * If @task is a stopper task, it cannot migrate and task_cpu() is",
            "\t * stable.",
            "\t */",
            "\tstruct cpu_stopper *stopper = per_cpu_ptr(&cpu_stopper, task_cpu(task));",
            "",
            "\tif (task != stopper->thread)",
            "\t\treturn;",
            "",
            "\tprintk(\"%sStopper: %pS <- %pS\\n\", log_lvl, stopper->fn, (void *)stopper->caller);",
            "}",
            "static void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)",
            "{",
            "\tmemset(done, 0, sizeof(*done));",
            "\tatomic_set(&done->nr_todo, nr_todo);",
            "\tinit_completion(&done->completion);",
            "}",
            "static void cpu_stop_signal_done(struct cpu_stop_done *done)",
            "{",
            "\tif (atomic_dec_and_test(&done->nr_todo))",
            "\t\tcomplete(&done->completion);",
            "}",
            "static void __cpu_stop_queue_work(struct cpu_stopper *stopper,",
            "\t\t\t\t  struct cpu_stop_work *work)",
            "{",
            "\tlist_add_tail(&work->list, &stopper->works);",
            "}",
            "static bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "\tunsigned long flags;",
            "\tbool enabled;",
            "",
            "\tpreempt_disable();",
            "\traw_spin_lock_irqsave(&stopper->lock, flags);",
            "\tenabled = stopper->enabled;",
            "\tif (enabled)",
            "\t\t__cpu_stop_queue_work(stopper, work);",
            "\telse if (work->done)",
            "\t\tcpu_stop_signal_done(work->done);",
            "\traw_spin_unlock_irqrestore(&stopper->lock, flags);",
            "",
            "\tif (enabled)",
            "\t\twake_up_process(stopper->thread);",
            "\tpreempt_enable();",
            "",
            "\treturn enabled;",
            "}",
            "int stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)",
            "{",
            "\tstruct cpu_stop_done done;",
            "\tstruct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done, .caller = _RET_IP_ };",
            "",
            "\tcpu_stop_init_done(&done, 1);",
            "\tif (!cpu_stop_queue_work(cpu, &work))",
            "\t\treturn -ENOENT;",
            "\t/*",
            "\t * In case @cpu == smp_proccessor_id() we can avoid a sleep+wakeup",
            "\t * cycle by doing a preemption:",
            "\t */",
            "\tcond_resched();",
            "\twait_for_completion(&done.completion);",
            "\treturn done.ret;",
            "}",
            "static void set_state(struct multi_stop_data *msdata,",
            "\t\t      enum multi_stop_state newstate)",
            "{",
            "\t/* Reset ack counter. */",
            "\tatomic_set(&msdata->thread_ack, msdata->num_threads);",
            "\tsmp_wmb();",
            "\tWRITE_ONCE(msdata->state, newstate);",
            "}",
            "static void ack_state(struct multi_stop_data *msdata)",
            "{",
            "\tif (atomic_dec_and_test(&msdata->thread_ack))",
            "\t\tset_state(msdata, msdata->state + 1);",
            "}",
            "notrace void __weak stop_machine_yield(const struct cpumask *cpumask)",
            "{",
            "\tcpu_relax();",
            "}",
            "static int multi_cpu_stop(void *data)",
            "{",
            "\tstruct multi_stop_data *msdata = data;",
            "\tenum multi_stop_state newstate, curstate = MULTI_STOP_NONE;",
            "\tint cpu = smp_processor_id(), err = 0;",
            "\tconst struct cpumask *cpumask;",
            "\tunsigned long flags;",
            "\tbool is_active;",
            "",
            "\t/*",
            "\t * When called from stop_machine_from_inactive_cpu(), irq might",
            "\t * already be disabled.  Save the state and restore it on exit.",
            "\t */",
            "\tlocal_save_flags(flags);",
            "",
            "\tif (!msdata->active_cpus) {",
            "\t\tcpumask = cpu_online_mask;",
            "\t\tis_active = cpu == cpumask_first(cpumask);",
            "\t} else {",
            "\t\tcpumask = msdata->active_cpus;",
            "\t\tis_active = cpumask_test_cpu(cpu, cpumask);",
            "\t}",
            "",
            "\t/* Simple state machine */",
            "\tdo {",
            "\t\t/* Chill out and ensure we re-read multi_stop_state. */",
            "\t\tstop_machine_yield(cpumask);",
            "\t\tnewstate = READ_ONCE(msdata->state);",
            "\t\tif (newstate != curstate) {",
            "\t\t\tcurstate = newstate;",
            "\t\t\tswitch (curstate) {",
            "\t\t\tcase MULTI_STOP_DISABLE_IRQ:",
            "\t\t\t\tlocal_irq_disable();",
            "\t\t\t\thard_irq_disable();",
            "#ifdef CONFIG_ARM64",
            "\t\t\t\tsdei_mask_local_cpu();",
            "#endif",
            "\t\t\t\tbreak;",
            "\t\t\tcase MULTI_STOP_RUN:",
            "\t\t\t\tif (is_active)",
            "\t\t\t\t\terr = msdata->fn(msdata->data);",
            "\t\t\t\tbreak;",
            "\t\t\tdefault:",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t\tack_state(msdata);",
            "\t\t} else if (curstate > MULTI_STOP_PREPARE) {",
            "\t\t\t/*",
            "\t\t\t * At this stage all other CPUs we depend on must spin",
            "\t\t\t * in the same loop. Any reason for hard-lockup should",
            "\t\t\t * be detected and reported on their side.",
            "\t\t\t */",
            "\t\t\ttouch_nmi_watchdog();",
            "\t\t}",
            "\t\trcu_momentary_dyntick_idle();",
            "\t} while (curstate != MULTI_STOP_EXIT);",
            "",
            "#ifdef CONFIG_ARM64",
            "\tsdei_unmask_local_cpu();",
            "#endif",
            "\tlocal_irq_restore(flags);",
            "\treturn err;",
            "}"
          ],
          "function_name": "print_stop_info, cpu_stop_init_done, cpu_stop_signal_done, __cpu_stop_queue_work, cpu_stop_queue_work, stop_one_cpu, set_state, ack_state, stop_machine_yield, multi_cpu_stop",
          "description": "实现了停止操作的核心控制逻辑，包括工作队列管理、状态同步、单CPU停止处理及多CPU状态机。提供打印停止信息、初始化完成状态、信号完成、排队工作、单CPU停止、状态切换等辅助函数。",
          "similarity": 0.5098825693130493
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/stop_machine.c",
          "start_line": 687,
          "end_line": 716,
          "content": [
            "int stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,",
            "\t\t\t\t  const struct cpumask *cpus)",
            "{",
            "\tstruct multi_stop_data msdata = { .fn = fn, .data = data,",
            "\t\t\t\t\t    .active_cpus = cpus };",
            "\tstruct cpu_stop_done done;",
            "\tint ret;",
            "",
            "\t/* Local CPU must be inactive and CPU hotplug in progress. */",
            "\tBUG_ON(cpu_active(raw_smp_processor_id()));",
            "\tmsdata.num_threads = num_active_cpus() + 1;\t/* +1 for local */",
            "",
            "\t/* No proper task established and can't sleep - busy wait for lock. */",
            "\twhile (!mutex_trylock(&stop_cpus_mutex))",
            "\t\tcpu_relax();",
            "",
            "\t/* Schedule work on other CPUs and execute directly for local CPU */",
            "\tset_state(&msdata, MULTI_STOP_PREPARE);",
            "\tcpu_stop_init_done(&done, num_active_cpus());",
            "\tqueue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,",
            "\t\t\t     &done);",
            "\tret = multi_cpu_stop(&msdata);",
            "",
            "\t/* Busy wait for completion. */",
            "\twhile (!completion_done(&done.completion))",
            "\t\tcpu_relax();",
            "",
            "\tmutex_unlock(&stop_cpus_mutex);",
            "\treturn ret ?: done.ret;",
            "}"
          ],
          "function_name": "stop_machine_from_inactive_cpu",
          "description": "该函数用于在非活跃CPU上协调多CPU的停止操作，确保在CPU热插拔期间正确执行停止回调函数。  \n通过获取互斥锁、异步调度任务并在本地直接执行，最终阻塞等待所有停止操作完成。  \n依赖未显示的辅助函数（如`queue_stop_cpus_work`和`multi_cpu_stop`），上下文可能存在不完整情况。",
          "similarity": 0.4878943860530853
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/stop_machine.c",
          "start_line": 536,
          "end_line": 641,
          "content": [
            "void stop_machine_park(int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "\t/*",
            "\t * Lockless. cpu_stopper_thread() will take stopper->lock and flush",
            "\t * the pending works before it parks, until then it is fine to queue",
            "\t * the new works.",
            "\t */",
            "\tstopper->enabled = false;",
            "\tkthread_park(stopper->thread);",
            "}",
            "static void cpu_stop_create(unsigned int cpu)",
            "{",
            "\tsched_set_stop_task(cpu, per_cpu(cpu_stopper.thread, cpu));",
            "}",
            "static void cpu_stop_park(unsigned int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "",
            "\tWARN_ON(!list_empty(&stopper->works));",
            "}",
            "void stop_machine_unpark(int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "",
            "\tstopper->enabled = true;",
            "\tkthread_unpark(stopper->thread);",
            "}",
            "static int __init cpu_stop_init(void)",
            "{",
            "\tunsigned int cpu;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "",
            "\t\traw_spin_lock_init(&stopper->lock);",
            "\t\tINIT_LIST_HEAD(&stopper->works);",
            "\t}",
            "",
            "\tBUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));",
            "\tstop_machine_unpark(raw_smp_processor_id());",
            "\tstop_machine_initialized = true;",
            "\treturn 0;",
            "}",
            "int stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,",
            "\t\t\t    const struct cpumask *cpus)",
            "{",
            "\tstruct multi_stop_data msdata = {",
            "\t\t.fn = fn,",
            "\t\t.data = data,",
            "\t\t.num_threads = num_online_cpus(),",
            "\t\t.active_cpus = cpus,",
            "\t};",
            "",
            "\tlockdep_assert_cpus_held();",
            "",
            "\tif (!stop_machine_initialized) {",
            "\t\t/*",
            "\t\t * Handle the case where stop_machine() is called",
            "\t\t * early in boot before stop_machine() has been",
            "\t\t * initialized.",
            "\t\t */",
            "\t\tunsigned long flags;",
            "\t\tint ret;",
            "",
            "\t\tWARN_ON_ONCE(msdata.num_threads != 1);",
            "",
            "\t\tlocal_irq_save(flags);",
            "\t\thard_irq_disable();",
            "\t\tret = (*fn)(data);",
            "\t\tlocal_irq_restore(flags);",
            "",
            "\t\treturn ret;",
            "\t}",
            "",
            "\t/* Set the initial state and stop all online cpus. */",
            "\tset_state(&msdata, MULTI_STOP_PREPARE);",
            "\treturn stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);",
            "}",
            "int stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)",
            "{",
            "\tint ret;",
            "",
            "\t/* No CPUs can come up or down during this. */",
            "\tcpus_read_lock();",
            "\tret = stop_machine_cpuslocked(fn, data, cpus);",
            "\tcpus_read_unlock();",
            "\treturn ret;",
            "}",
            "int stop_core_cpuslocked(unsigned int cpu, cpu_stop_fn_t fn, void *data)",
            "{",
            "\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);",
            "",
            "\tstruct multi_stop_data msdata = {",
            "\t\t.fn = fn,",
            "\t\t.data = data,",
            "\t\t.num_threads = cpumask_weight(smt_mask),",
            "\t\t.active_cpus = smt_mask,",
            "\t};",
            "",
            "\tlockdep_assert_cpus_held();",
            "",
            "\t/* Set the initial state and stop all online cpus. */",
            "\tset_state(&msdata, MULTI_STOP_PREPARE);",
            "\treturn stop_cpus(smt_mask, multi_cpu_stop, &msdata);",
            "}"
          ],
          "function_name": "stop_machine_park, cpu_stop_create, cpu_stop_park, stop_machine_unpark, cpu_stop_init, stop_machine_cpuslocked, stop_machine, stop_core_cpuslocked",
          "description": "负责停止器线程的生命周期管理和初始化。包含线程创建、挂起/恢复操作、核心初始化函数，以及带锁的CPU停止接口。提供针对特定CPU核心的停止功能和早期启动阶段的降级处理路径。",
          "similarity": 0.4656425714492798
        }
      ]
    }
  ]
}