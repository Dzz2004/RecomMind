{
  "query": "concurrent data structures",
  "timestamp": "2025-12-26 00:50:04",
  "retrieved_files": [
    {
      "source_file": "kernel/padata.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:13:27\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `padata.c`\n\n---\n\n# padata.c 技术文档\n\n## 文件概述\n\n`padata.c` 实现了 Linux 内核中的 **padata**（parallel data）框架，提供了一个通用接口，用于在多个 CPU 上并行处理数据流，同时保证结果按照原始顺序串行化输出。该机制常用于需要高吞吐量并行计算但又要求结果有序的场景（如加密、压缩等）。padata 框架通过将任务分发到多个 CPU 并行执行，并在专用的串行化 CPU 上按序回调，实现了并行与有序的统一。\n\n## 核心功能\n\n### 主要数据结构\n\n- `struct padata_work`：封装工作项，用于将并行任务提交到工作队列。\n- `struct padata_mt_job_state`：用于多线程作业（multi-threaded job）的状态管理，包含完成通知、工作计数和数据块大小。\n- `struct parallel_data`：并行数据上下文，包含 CPU 掩码、序列号、重排序队列等关键状态。\n- `struct padata_priv`：用户传递的任务私有数据结构，必须包含 `.parallel` 和 `.serial` 回调函数。\n\n### 主要函数\n\n- `padata_do_parallel()`：核心入口函数，将任务分发到并行工作队列。\n- `padata_parallel_worker()`：并行工作线程执行函数，调用用户提供的 `.parallel` 回调。\n- `padata_find_next()`：在重排序队列中查找下一个应被串行化的任务。\n- `padata_cpu_hash()`：根据序列号将任务哈希到特定的并行 CPU。\n- `padata_index_to_cpu()`：将逻辑 CPU 索引映射到实际的 CPU ID。\n- `padata_work_alloc()` / `padata_work_free()`：管理工作项的分配与回收。\n- `padata_work_alloc_mt()` / `padata_works_free()`：用于多线程作业的批量工作项管理。\n\n## 关键实现\n\n### 1. 并行-串行模型\npadata 采用“并行执行 + 有序串行化”模型：\n- **并行阶段**：任务通过 `padata_do_parallel()` 分发到 `parallel_wq` 工作队列，在 `pd->cpumask.pcpu` 指定的 CPU 上并行执行（BH 关闭）。\n- **串行阶段**：任务完成后进入 per-CPU 重排序队列，由 `padata_reorder()` 机制按 `seq_nr` 顺序触发 `.serial` 回调，回调在 `pd->cpumask.cbcpu` 指定的 CPU 上执行。\n\n### 2. 顺序保证机制\n- 每个任务分配唯一递增的 `seq_nr`。\n- 任务完成后按 `seq_nr % weight(pcpu_mask)` 哈希到特定 CPU 的重排序队列。\n- `padata_find_next()` 仅当 `seq_nr == pd->processed` 时才取出任务，确保严格 FIFO 顺序。\n\n### 3. 工作项管理\n- 全局预分配 `padata_work` 对象池（`padata_free_works` 链表），避免运行时内存分配。\n- 使用自旋锁 `padata_works_lock` 保护工作项分配/回收，支持 BH 上下文安全。\n\n### 4. CPU 掩码处理\n- 支持动态 CPU 掩码（`pcpu` 用于并行，`cbcpu` 用于串行回调）。\n- 若用户指定的 `cb_cpu` 不在 `cbcpu` 掩码中，自动选择回退 CPU（通过模运算和 `cpumask_next`）。\n\n### 5. 引用计数与生命周期\n- `parallel_data` 使用 `refcount_t` 管理生命周期，`padata_get_pd()` / `padata_put_pd()` 控制引用。\n- RCU 保护 `pd` 指针的读取（`rcu_dereference_bh`），确保并发安全。\n\n## 依赖关系\n\n- **内核子系统**：\n  - `workqueue`：依赖内核工作队列机制执行并行任务。\n  - `RCU`：用于 `parallel_data` 结构的并发读取保护。\n  - `percpu`：使用 per-CPU 变量存储重排序队列（`reorder_list`）。\n  - `sysfs`：支持通过 sysfs 接口配置 padata 实例（未在片段中体现，但头文件包含）。\n- **头文件依赖**：\n  - `<linux/padata.h>`：定义用户接口和核心结构。\n  - `<linux/completion.h>`、`<linux/cpumask.h>`、`<linux/rcupdate.h>` 等提供基础功能。\n\n## 使用场景\n\n1. **加密子系统**：如 IPsec 或 dm-crypt 使用 padata 并行处理多个数据块的加密/解密，同时保证输出顺序。\n2. **压缩/解压缩**：在需要高吞吐量的压缩场景中并行处理数据流。\n3. **批量数据处理**：任何需要将大数据集分片并行处理，但要求结果按输入顺序交付的内核模块。\n4. **多线程作业辅助**：通过 `padata_work_alloc_mt()` 接口，辅助实现内核态多线程任务分发（如大内存初始化）。\n\n> **注意**：所有通过 `padata_do_parallel()` 提交的任务**必须**调用 `padata_do_serial()` 完成串行化阶段，否则会导致资源泄漏和顺序错乱。",
      "similarity": 0.5931916236877441,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/padata.c",
          "start_line": 50,
          "end_line": 203,
          "content": [
            "static inline void padata_get_pd(struct parallel_data *pd)",
            "{",
            "\trefcount_inc(&pd->refcnt);",
            "}",
            "static inline void padata_put_pd_cnt(struct parallel_data *pd, int cnt)",
            "{",
            "\tif (refcount_sub_and_test(cnt, &pd->refcnt))",
            "\t\tpadata_free_pd(pd);",
            "}",
            "static inline void padata_put_pd(struct parallel_data *pd)",
            "{",
            "\tpadata_put_pd_cnt(pd, 1);",
            "}",
            "static int padata_index_to_cpu(struct parallel_data *pd, int cpu_index)",
            "{",
            "\tint cpu, target_cpu;",
            "",
            "\ttarget_cpu = cpumask_first(pd->cpumask.pcpu);",
            "\tfor (cpu = 0; cpu < cpu_index; cpu++)",
            "\t\ttarget_cpu = cpumask_next(target_cpu, pd->cpumask.pcpu);",
            "",
            "\treturn target_cpu;",
            "}",
            "static int padata_cpu_hash(struct parallel_data *pd, unsigned int seq_nr)",
            "{",
            "\t/*",
            "\t * Hash the sequence numbers to the cpus by taking",
            "\t * seq_nr mod. number of cpus in use.",
            "\t */",
            "\tint cpu_index = seq_nr % cpumask_weight(pd->cpumask.pcpu);",
            "",
            "\treturn padata_index_to_cpu(pd, cpu_index);",
            "}",
            "static void __ref padata_work_init(struct padata_work *pw, work_func_t work_fn,",
            "\t\t\t\t   void *data, int flags)",
            "{",
            "\tif (flags & PADATA_WORK_ONSTACK)",
            "\t\tINIT_WORK_ONSTACK(&pw->pw_work, work_fn);",
            "\telse",
            "\t\tINIT_WORK(&pw->pw_work, work_fn);",
            "\tpw->pw_data = data;",
            "}",
            "static int __init padata_work_alloc_mt(int nworks, void *data,",
            "\t\t\t\t       struct list_head *head)",
            "{",
            "\tint i;",
            "",
            "\tspin_lock_bh(&padata_works_lock);",
            "\t/* Start at 1 because the current task participates in the job. */",
            "\tfor (i = 1; i < nworks; ++i) {",
            "\t\tstruct padata_work *pw = padata_work_alloc();",
            "",
            "\t\tif (!pw)",
            "\t\t\tbreak;",
            "\t\tpadata_work_init(pw, padata_mt_helper, data, 0);",
            "\t\tlist_add(&pw->pw_list, head);",
            "\t}",
            "\tspin_unlock_bh(&padata_works_lock);",
            "",
            "\treturn i;",
            "}",
            "static void padata_work_free(struct padata_work *pw)",
            "{",
            "\tlockdep_assert_held(&padata_works_lock);",
            "\tlist_add(&pw->pw_list, &padata_free_works);",
            "}",
            "static void __init padata_works_free(struct list_head *works)",
            "{",
            "\tstruct padata_work *cur, *next;",
            "",
            "\tif (list_empty(works))",
            "\t\treturn;",
            "",
            "\tspin_lock_bh(&padata_works_lock);",
            "\tlist_for_each_entry_safe(cur, next, works, pw_list) {",
            "\t\tlist_del(&cur->pw_list);",
            "\t\tpadata_work_free(cur);",
            "\t}",
            "\tspin_unlock_bh(&padata_works_lock);",
            "}",
            "static void padata_parallel_worker(struct work_struct *parallel_work)",
            "{",
            "\tstruct padata_work *pw = container_of(parallel_work, struct padata_work,",
            "\t\t\t\t\t      pw_work);",
            "\tstruct padata_priv *padata = pw->pw_data;",
            "",
            "\tlocal_bh_disable();",
            "\tpadata->parallel(padata);",
            "\tspin_lock(&padata_works_lock);",
            "\tpadata_work_free(pw);",
            "\tspin_unlock(&padata_works_lock);",
            "\tlocal_bh_enable();",
            "}",
            "int padata_do_parallel(struct padata_shell *ps,",
            "\t\t       struct padata_priv *padata, int *cb_cpu)",
            "{",
            "\tstruct padata_instance *pinst = ps->pinst;",
            "\tint i, cpu, cpu_index, err;",
            "\tstruct parallel_data *pd;",
            "\tstruct padata_work *pw;",
            "",
            "\trcu_read_lock_bh();",
            "",
            "\tpd = rcu_dereference_bh(ps->pd);",
            "",
            "\terr = -EINVAL;",
            "\tif (!(pinst->flags & PADATA_INIT) || pinst->flags & PADATA_INVALID)",
            "\t\tgoto out;",
            "",
            "\tif (!cpumask_test_cpu(*cb_cpu, pd->cpumask.cbcpu)) {",
            "\t\tif (cpumask_empty(pd->cpumask.cbcpu))",
            "\t\t\tgoto out;",
            "",
            "\t\t/* Select an alternate fallback CPU and notify the caller. */",
            "\t\tcpu_index = *cb_cpu % cpumask_weight(pd->cpumask.cbcpu);",
            "",
            "\t\tcpu = cpumask_first(pd->cpumask.cbcpu);",
            "\t\tfor (i = 0; i < cpu_index; i++)",
            "\t\t\tcpu = cpumask_next(cpu, pd->cpumask.cbcpu);",
            "",
            "\t\t*cb_cpu = cpu;",
            "\t}",
            "",
            "\terr = -EBUSY;",
            "\tif ((pinst->flags & PADATA_RESET))",
            "\t\tgoto out;",
            "",
            "\tpadata_get_pd(pd);",
            "\tpadata->pd = pd;",
            "\tpadata->cb_cpu = *cb_cpu;",
            "",
            "\tspin_lock(&padata_works_lock);",
            "\tpadata->seq_nr = ++pd->seq_nr;",
            "\tpw = padata_work_alloc();",
            "\tspin_unlock(&padata_works_lock);",
            "",
            "\tif (!pw) {",
            "\t\t/* Maximum works limit exceeded, run in the current task. */",
            "\t\tpadata->parallel(padata);",
            "\t}",
            "",
            "\trcu_read_unlock_bh();",
            "",
            "\tif (pw) {",
            "\t\tpadata_work_init(pw, padata_parallel_worker, padata, 0);",
            "\t\tqueue_work(pinst->parallel_wq, &pw->pw_work);",
            "\t}",
            "",
            "\treturn 0;",
            "out:",
            "\trcu_read_unlock_bh();",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "padata_get_pd, padata_put_pd_cnt, padata_put_pd, padata_index_to_cpu, padata_cpu_hash, padata_work_init, padata_work_alloc_mt, padata_work_free, padata_works_free, padata_parallel_worker, padata_do_parallel",
          "description": "提供并行数据引用计数、CPU映射算法、工作项初始化与分配逻辑，以及并行任务执行流程",
          "similarity": 0.6202549934387207
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/padata.c",
          "start_line": 1,
          "end_line": 49,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * padata.c - generic interface to process data streams in parallel",
            " *",
            " * See Documentation/core-api/padata.rst for more information.",
            " *",
            " * Copyright (C) 2008, 2009 secunet Security Networks AG",
            " * Copyright (C) 2008, 2009 Steffen Klassert <steffen.klassert@secunet.com>",
            " *",
            " * Copyright (c) 2020 Oracle and/or its affiliates.",
            " * Author: Daniel Jordan <daniel.m.jordan@oracle.com>",
            " */",
            "",
            "#include <linux/completion.h>",
            "#include <linux/export.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/err.h>",
            "#include <linux/cpu.h>",
            "#include <linux/padata.h>",
            "#include <linux/mutex.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/sysfs.h>",
            "#include <linux/rcupdate.h>",
            "",
            "#define\tPADATA_WORK_ONSTACK\t1\t/* Work's memory is on stack */",
            "",
            "struct padata_work {",
            "\tstruct work_struct\tpw_work;",
            "\tstruct list_head\tpw_list;  /* padata_free_works linkage */",
            "\tvoid\t\t\t*pw_data;",
            "};",
            "",
            "static DEFINE_SPINLOCK(padata_works_lock);",
            "static struct padata_work *padata_works;",
            "static LIST_HEAD(padata_free_works);",
            "",
            "struct padata_mt_job_state {",
            "\tspinlock_t\t\tlock;",
            "\tstruct completion\tcompletion;",
            "\tstruct padata_mt_job\t*job;",
            "\tint\t\t\tnworks;",
            "\tint\t\t\tnworks_fini;",
            "\tunsigned long\t\tchunk_size;",
            "};",
            "",
            "static void padata_free_pd(struct parallel_data *pd);",
            "static void __init padata_mt_helper(struct work_struct *work);",
            ""
          ],
          "function_name": null,
          "description": "定义并行处理工作的数据结构和全局资源，包括工作项结构体、锁及用于管理工作项的链表",
          "similarity": 0.5667513012886047
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/padata.c",
          "start_line": 539,
          "end_line": 643,
          "content": [
            "static void padata_init_reorder_list(struct parallel_data *pd)",
            "{",
            "\tint cpu;",
            "\tstruct padata_list *list;",
            "",
            "\tfor_each_cpu(cpu, pd->cpumask.pcpu) {",
            "\t\tlist = per_cpu_ptr(pd->reorder_list, cpu);",
            "\t\t__padata_list_init(list);",
            "\t}",
            "}",
            "static void padata_free_pd(struct parallel_data *pd)",
            "{",
            "\tfree_cpumask_var(pd->cpumask.pcpu);",
            "\tfree_cpumask_var(pd->cpumask.cbcpu);",
            "\tfree_percpu(pd->reorder_list);",
            "\tfree_percpu(pd->squeue);",
            "\tkfree(pd);",
            "}",
            "static void __padata_start(struct padata_instance *pinst)",
            "{",
            "\tpinst->flags |= PADATA_INIT;",
            "}",
            "static void __padata_stop(struct padata_instance *pinst)",
            "{",
            "\tif (!(pinst->flags & PADATA_INIT))",
            "\t\treturn;",
            "",
            "\tpinst->flags &= ~PADATA_INIT;",
            "",
            "\tsynchronize_rcu();",
            "}",
            "static int padata_replace_one(struct padata_shell *ps)",
            "{",
            "\tstruct parallel_data *pd_new;",
            "",
            "\tpd_new = padata_alloc_pd(ps);",
            "\tif (!pd_new)",
            "\t\treturn -ENOMEM;",
            "",
            "\tps->opd = rcu_dereference_protected(ps->pd, 1);",
            "\trcu_assign_pointer(ps->pd, pd_new);",
            "",
            "\treturn 0;",
            "}",
            "static int padata_replace(struct padata_instance *pinst)",
            "{",
            "\tstruct padata_shell *ps;",
            "\tint err = 0;",
            "",
            "\tpinst->flags |= PADATA_RESET;",
            "",
            "\tlist_for_each_entry(ps, &pinst->pslist, list) {",
            "\t\terr = padata_replace_one(ps);",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tsynchronize_rcu();",
            "",
            "\tlist_for_each_entry_continue_reverse(ps, &pinst->pslist, list)",
            "\t\tpadata_put_pd(ps->opd);",
            "",
            "\tpinst->flags &= ~PADATA_RESET;",
            "",
            "\treturn err;",
            "}",
            "static bool padata_validate_cpumask(struct padata_instance *pinst,",
            "\t\t\t\t    const struct cpumask *cpumask)",
            "{",
            "\tif (!cpumask_intersects(cpumask, cpu_online_mask)) {",
            "\t\tpinst->flags |= PADATA_INVALID;",
            "\t\treturn false;",
            "\t}",
            "",
            "\tpinst->flags &= ~PADATA_INVALID;",
            "\treturn true;",
            "}",
            "static int __padata_set_cpumasks(struct padata_instance *pinst,",
            "\t\t\t\t cpumask_var_t pcpumask,",
            "\t\t\t\t cpumask_var_t cbcpumask)",
            "{",
            "\tint valid;",
            "\tint err;",
            "",
            "\tvalid = padata_validate_cpumask(pinst, pcpumask);",
            "\tif (!valid) {",
            "\t\t__padata_stop(pinst);",
            "\t\tgoto out_replace;",
            "\t}",
            "",
            "\tvalid = padata_validate_cpumask(pinst, cbcpumask);",
            "\tif (!valid)",
            "\t\t__padata_stop(pinst);",
            "",
            "out_replace:",
            "\tcpumask_copy(pinst->cpumask.pcpu, pcpumask);",
            "\tcpumask_copy(pinst->cpumask.cbcpu, cbcpumask);",
            "",
            "\terr = padata_setup_cpumasks(pinst) ?: padata_replace(pinst);",
            "",
            "\tif (valid)",
            "\t\t__padata_start(pinst);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "padata_init_reorder_list, padata_free_pd, __padata_start, __padata_stop, padata_replace_one, padata_replace, padata_validate_cpumask, __padata_set_cpumasks",
          "description": "初始化与释放重排列表资源，实现CPU掩码动态配置及有效性校验机制",
          "similarity": 0.5248340964317322
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/padata.c",
          "start_line": 296,
          "end_line": 402,
          "content": [
            "static void padata_reorder(struct padata_priv *padata)",
            "{",
            "\tstruct parallel_data *pd = padata->pd;",
            "\tstruct padata_instance *pinst = pd->ps->pinst;",
            "\tunsigned int processed;",
            "\tint cpu;",
            "",
            "\tprocessed = pd->processed;",
            "\tcpu = pd->cpu;",
            "",
            "\tdo {",
            "\t\tstruct padata_serial_queue *squeue;",
            "\t\tint cb_cpu;",
            "",
            "\t\tcpu = cpumask_next_wrap(cpu, pd->cpumask.pcpu, -1, false);",
            "\t\tprocessed++;",
            "",
            "\t\tcb_cpu = padata->cb_cpu;",
            "\t\tsqueue = per_cpu_ptr(pd->squeue, cb_cpu);",
            "",
            "\t\tspin_lock(&squeue->serial.lock);",
            "\t\tlist_add_tail(&padata->list, &squeue->serial.list);",
            "\t\tqueue_work_on(cb_cpu, pinst->serial_wq, &squeue->work);",
            "",
            "\t\t/*",
            "\t\t * If the next object that needs serialization is parallel",
            "\t\t * processed by another cpu and is still on it's way to the",
            "\t\t * cpu's reorder queue, end the loop.",
            "\t\t */",
            "\t\tpadata = padata_find_next(pd, cpu, processed);",
            "\t\tspin_unlock(&squeue->serial.lock);",
            "\t} while (padata);",
            "}",
            "static void padata_serial_worker(struct work_struct *serial_work)",
            "{",
            "\tstruct padata_serial_queue *squeue;",
            "\tstruct parallel_data *pd;",
            "\tLIST_HEAD(local_list);",
            "\tint cnt;",
            "",
            "\tlocal_bh_disable();",
            "\tsqueue = container_of(serial_work, struct padata_serial_queue, work);",
            "\tpd = squeue->pd;",
            "",
            "\tspin_lock(&squeue->serial.lock);",
            "\tlist_replace_init(&squeue->serial.list, &local_list);",
            "\tspin_unlock(&squeue->serial.lock);",
            "",
            "\tcnt = 0;",
            "",
            "\twhile (!list_empty(&local_list)) {",
            "\t\tstruct padata_priv *padata;",
            "",
            "\t\tpadata = list_entry(local_list.next,",
            "\t\t\t\t    struct padata_priv, list);",
            "",
            "\t\tlist_del_init(&padata->list);",
            "",
            "\t\tpadata->serial(padata);",
            "\t\tcnt++;",
            "\t}",
            "\tlocal_bh_enable();",
            "",
            "\tpadata_put_pd_cnt(pd, cnt);",
            "}",
            "void padata_do_serial(struct padata_priv *padata)",
            "{",
            "\tstruct parallel_data *pd = padata->pd;",
            "\tint hashed_cpu = padata_cpu_hash(pd, padata->seq_nr);",
            "\tstruct padata_list *reorder = per_cpu_ptr(pd->reorder_list, hashed_cpu);",
            "\tstruct padata_priv *cur;",
            "\tstruct list_head *pos;",
            "\tbool gotit = true;",
            "",
            "\tspin_lock(&reorder->lock);",
            "\t/* Sort in ascending order of sequence number. */",
            "\tlist_for_each_prev(pos, &reorder->list) {",
            "\t\tcur = list_entry(pos, struct padata_priv, list);",
            "\t\t/* Compare by difference to consider integer wrap around */",
            "\t\tif ((signed int)(cur->seq_nr - padata->seq_nr) < 0)",
            "\t\t\tbreak;",
            "\t}",
            "\tif (padata->seq_nr != pd->processed) {",
            "\t\tgotit = false;",
            "\t\tlist_add(&padata->list, pos);",
            "\t}",
            "\tspin_unlock(&reorder->lock);",
            "",
            "\tif (gotit)",
            "\t\tpadata_reorder(padata);",
            "}",
            "static int padata_setup_cpumasks(struct padata_instance *pinst)",
            "{",
            "\tstruct workqueue_attrs *attrs;",
            "\tint err;",
            "",
            "\tattrs = alloc_workqueue_attrs();",
            "\tif (!attrs)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/* Restrict parallel_wq workers to pd->cpumask.pcpu. */",
            "\tcpumask_copy(attrs->cpumask, pinst->cpumask.pcpu);",
            "\terr = apply_workqueue_attrs(pinst->parallel_wq, attrs);",
            "\tfree_workqueue_attrs(attrs);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "padata_reorder, padata_serial_worker, padata_do_serial, padata_setup_cpumasks",
          "description": "实现串行化队列的排序插入、工作项处理函数及CPU掩码配置功能",
          "similarity": 0.5177690982818604
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/padata.c",
          "start_line": 416,
          "end_line": 526,
          "content": [
            "static void __init padata_mt_helper(struct work_struct *w)",
            "{",
            "\tstruct padata_work *pw = container_of(w, struct padata_work, pw_work);",
            "\tstruct padata_mt_job_state *ps = pw->pw_data;",
            "\tstruct padata_mt_job *job = ps->job;",
            "\tbool done;",
            "",
            "\tspin_lock(&ps->lock);",
            "",
            "\twhile (job->size > 0) {",
            "\t\tunsigned long start, size, end;",
            "",
            "\t\tstart = job->start;",
            "\t\t/* So end is chunk size aligned if enough work remains. */",
            "\t\tsize = roundup(start + 1, ps->chunk_size) - start;",
            "\t\tsize = min(size, job->size);",
            "\t\tend = start + size;",
            "",
            "\t\tjob->start = end;",
            "\t\tjob->size -= size;",
            "",
            "\t\tspin_unlock(&ps->lock);",
            "\t\tjob->thread_fn(start, end, job->fn_arg);",
            "\t\tspin_lock(&ps->lock);",
            "\t}",
            "",
            "\t++ps->nworks_fini;",
            "\tdone = (ps->nworks_fini == ps->nworks);",
            "\tspin_unlock(&ps->lock);",
            "",
            "\tif (done)",
            "\t\tcomplete(&ps->completion);",
            "}",
            "void __init padata_do_multithreaded(struct padata_mt_job *job)",
            "{",
            "\t/* In case threads finish at different times. */",
            "\tstatic const unsigned long load_balance_factor = 4;",
            "\tstruct padata_work my_work, *pw;",
            "\tstruct padata_mt_job_state ps;",
            "\tLIST_HEAD(works);",
            "\tint nworks;",
            "",
            "\tif (job->size == 0)",
            "\t\treturn;",
            "",
            "\t/* Ensure at least one thread when size < min_chunk. */",
            "\tnworks = max(job->size / max(job->min_chunk, job->align), 1ul);",
            "\tnworks = min(nworks, job->max_threads);",
            "",
            "\tif (nworks == 1) {",
            "\t\t/* Single thread, no coordination needed, cut to the chase. */",
            "\t\tjob->thread_fn(job->start, job->start + job->size, job->fn_arg);",
            "\t\treturn;",
            "\t}",
            "",
            "\tspin_lock_init(&ps.lock);",
            "\tinit_completion(&ps.completion);",
            "\tps.job\t       = job;",
            "\tps.nworks      = padata_work_alloc_mt(nworks, &ps, &works);",
            "\tps.nworks_fini = 0;",
            "",
            "\t/*",
            "\t * Chunk size is the amount of work a helper does per call to the",
            "\t * thread function.  Load balance large jobs between threads by",
            "\t * increasing the number of chunks, guarantee at least the minimum",
            "\t * chunk size from the caller, and honor the caller's alignment.",
            "\t * Ensure chunk_size is at least 1 to prevent divide-by-0",
            "\t * panic in padata_mt_helper().",
            "\t */",
            "\tps.chunk_size = job->size / (ps.nworks * load_balance_factor);",
            "\tps.chunk_size = max(ps.chunk_size, job->min_chunk);",
            "\tps.chunk_size = max(ps.chunk_size, 1ul);",
            "\tps.chunk_size = roundup(ps.chunk_size, job->align);",
            "",
            "\t/*",
            "\t * chunk_size can be 0 if the caller sets min_chunk to 0. So force it",
            "\t * to at least 1 to prevent divide-by-0 panic in padata_mt_helper().`",
            "\t */",
            "\tif (!ps.chunk_size)",
            "\t\tps.chunk_size = 1U;",
            "",
            "\tlist_for_each_entry(pw, &works, pw_list)",
            "\t\tqueue_work(system_unbound_wq, &pw->pw_work);",
            "",
            "\t/* Use the current thread, which saves starting a workqueue worker. */",
            "\tpadata_work_init(&my_work, padata_mt_helper, &ps, PADATA_WORK_ONSTACK);",
            "\tpadata_mt_helper(&my_work.pw_work);",
            "",
            "\t/* Wait for all the helpers to finish. */",
            "\twait_for_completion(&ps.completion);",
            "",
            "\tdestroy_work_on_stack(&my_work.pw_work);",
            "\tpadata_works_free(&works);",
            "}",
            "static void __padata_list_init(struct padata_list *pd_list)",
            "{",
            "\tINIT_LIST_HEAD(&pd_list->list);",
            "\tspin_lock_init(&pd_list->lock);",
            "}",
            "static void padata_init_squeues(struct parallel_data *pd)",
            "{",
            "\tint cpu;",
            "\tstruct padata_serial_queue *squeue;",
            "",
            "\tfor_each_cpu(cpu, pd->cpumask.cbcpu) {",
            "\t\tsqueue = per_cpu_ptr(pd->squeue, cpu);",
            "\t\tsqueue->pd = pd;",
            "\t\t__padata_list_init(&squeue->serial);",
            "\t\tINIT_WORK(&squeue->work, padata_serial_worker);",
            "\t}",
            "}"
          ],
          "function_name": "padata_mt_helper, padata_do_multithreaded, __padata_list_init, padata_init_squeues",
          "description": "支持多线程任务分片处理，通过工作队列分发任务并协调多线程执行",
          "similarity": 0.5083523988723755
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5667412877082825,
      "chunks": [
        {
          "chunk_id": 21,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3787,
          "end_line": 3910,
          "content": [
            "void get_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\t/*",
            "\t * Any prior manipulation of RCU-protected data must happen",
            "\t * before the loads from ->gp_seq and ->expedited_sequence.",
            "\t */",
            "\tsmp_mb();  /* ^^^ */",
            "\trgosp->rgos_norm = rcu_seq_snap(&rnp->gp_seq);",
            "\trgosp->rgos_exp = rcu_seq_snap(&rcu_state.expedited_sequence);",
            "}",
            "static void start_poll_synchronize_rcu_common(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool needwake;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tlockdep_assert_irqs_enabled();",
            "\tlocal_irq_save(flags);",
            "\trdp = this_cpu_ptr(&rcu_data);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_rcu_node(rnp); // irqs already disabled.",
            "\t// Note it is possible for a grace period to have elapsed between",
            "\t// the above call to get_state_synchronize_rcu() and the below call",
            "\t// to rcu_seq_snap.  This is OK, the worst that happens is that we",
            "\t// get a grace period that no one needed.  These accesses are ordered",
            "\t// by smp_mb(), and we are accessing them in the opposite order",
            "\t// from which they are updated at grace-period start, as required.",
            "\tneedwake = rcu_start_this_gp(rnp, rdp, rcu_seq_snap(&rcu_state.gp_seq));",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "}",
            "unsigned long start_poll_synchronize_rcu(void)",
            "{",
            "\tunsigned long gp_seq = get_state_synchronize_rcu();",
            "",
            "\tstart_poll_synchronize_rcu_common();",
            "\treturn gp_seq;",
            "}",
            "void start_poll_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tget_state_synchronize_rcu_full(rgosp);",
            "",
            "\tstart_poll_synchronize_rcu_common();",
            "}",
            "bool poll_state_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\tif (oldstate == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rcu_state.gp_seq_polled, oldstate)) {",
            "\t\tsmp_mb(); /* Ensure GP ends before subsequent accesses. */",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "bool poll_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tsmp_mb(); // Order against root rcu_node structure grace-period cleanup.",
            "\tif (rgosp->rgos_norm == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rnp->gp_seq, rgosp->rgos_norm) ||",
            "\t    rgosp->rgos_exp == RCU_GET_STATE_COMPLETED ||",
            "\t    rcu_seq_done_exact(&rcu_state.expedited_sequence, rgosp->rgos_exp)) {",
            "\t\tsmp_mb(); /* Ensure GP ends before subsequent accesses. */",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "void cond_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\tif (!poll_state_synchronize_rcu(oldstate))",
            "\t\tsynchronize_rcu();",
            "}",
            "void cond_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\tif (!poll_state_synchronize_rcu_full(rgosp))",
            "\t\tsynchronize_rcu();",
            "}",
            "static int rcu_pending(int user)",
            "{",
            "\tbool gp_in_progress;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "\tstruct rcu_node *rnp = rdp->mynode;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\t/* Check for CPU stalls, if enabled. */",
            "\tcheck_cpu_stall(rdp);",
            "",
            "\t/* Does this CPU need a deferred NOCB wakeup? */",
            "\tif (rcu_nocb_need_deferred_wakeup(rdp, RCU_NOCB_WAKE))",
            "\t\treturn 1;",
            "",
            "\t/* Is this a nohz_full CPU in userspace or idle?  (Ignore RCU if so.) */",
            "\tif ((user || rcu_is_cpu_rrupt_from_idle()) && rcu_nohz_full_cpu())",
            "\t\treturn 0;",
            "",
            "\t/* Is the RCU core waiting for a quiescent state from this CPU? */",
            "\tgp_in_progress = rcu_gp_in_progress();",
            "\tif (rdp->core_needs_qs && !rdp->cpu_no_qs.b.norm && gp_in_progress)",
            "\t\treturn 1;",
            "",
            "\t/* Does this CPU have callbacks ready to invoke? */",
            "\tif (!rcu_rdp_is_offloaded(rdp) &&",
            "\t    rcu_segcblist_ready_cbs(&rdp->cblist))",
            "\t\treturn 1;",
            "",
            "\t/* Has RCU gone idle with this CPU needing another grace period? */",
            "\tif (!gp_in_progress && rcu_segcblist_is_enabled(&rdp->cblist) &&",
            "\t    !rcu_rdp_is_offloaded(rdp) &&",
            "\t    !rcu_segcblist_restempty(&rdp->cblist, RCU_NEXT_READY_TAIL))",
            "\t\treturn 1;",
            "",
            "\t/* Have RCU grace period completed or started?  */",
            "\tif (rcu_seq_current(&rnp->gp_seq) != rdp->gp_seq ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) /* outside lock */",
            "\t\treturn 1;",
            "",
            "\t/* nothing to do */",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_state_synchronize_rcu_full, start_poll_synchronize_rcu_common, start_poll_synchronize_rcu, start_poll_synchronize_rcu_full, poll_state_synchronize_rcu, poll_state_synchronize_rcu_full, cond_synchronize_rcu, cond_synchronize_rcu_full, rcu_pending",
          "description": "提供RCU宽限期状态查询和触发机制，通过序列号比对判断是否需要启动新的宽限期，处理回调队列唤醒逻辑，实现条件同步检查",
          "similarity": 0.5396627187728882
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 908,
          "end_line": 1026,
          "content": [
            "static void trace_rcu_this_gp(struct rcu_node *rnp, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req, const char *s)",
            "{",
            "\ttrace_rcu_future_grace_period(rcu_state.name, READ_ONCE(rnp->gp_seq),",
            "\t\t\t\t      gp_seq_req, rnp->level,",
            "\t\t\t\t      rnp->grplo, rnp->grphi, s);",
            "}",
            "static bool rcu_start_this_gp(struct rcu_node *rnp_start, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req)",
            "{",
            "\tbool ret = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\t/*",
            "\t * Use funnel locking to either acquire the root rcu_node",
            "\t * structure's lock or bail out if the need for this grace period",
            "\t * has already been recorded -- or if that grace period has in",
            "\t * fact already started.  If there is already a grace period in",
            "\t * progress in a non-leaf node, no recording is needed because the",
            "\t * end of the grace period will scan the leaf rcu_node structures.",
            "\t * Note that rnp_start->lock must not be released.",
            "\t */",
            "\traw_lockdep_assert_held_rcu_node(rnp_start);",
            "\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req, TPS(\"Startleaf\"));",
            "\tfor (rnp = rnp_start; 1; rnp = rnp->parent) {",
            "\t\tif (rnp != rnp_start)",
            "\t\t\traw_spin_lock_rcu_node(rnp);",
            "\t\tif (ULONG_CMP_GE(rnp->gp_seq_needed, gp_seq_req) ||",
            "\t\t    rcu_seq_started(&rnp->gp_seq, gp_seq_req) ||",
            "\t\t    (rnp != rnp_start &&",
            "\t\t     rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))) {",
            "\t\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Prestarted\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tWRITE_ONCE(rnp->gp_seq_needed, gp_seq_req);",
            "\t\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq))) {",
            "\t\t\t/*",
            "\t\t\t * We just marked the leaf or internal node, and a",
            "\t\t\t * grace period is in progress, which means that",
            "\t\t\t * rcu_gp_cleanup() will see the marking.  Bail to",
            "\t\t\t * reduce contention.",
            "\t\t\t */",
            "\t\t\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Startedleaf\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tif (rnp != rnp_start && rnp->parent != NULL)",
            "\t\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\tif (!rnp->parent)",
            "\t\t\tbreak;  /* At root, and perhaps also leaf. */",
            "\t}",
            "",
            "\t/* If GP already in progress, just leave, otherwise start one. */",
            "\tif (rcu_gp_in_progress()) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedleafroot\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedroot\"));",
            "\tWRITE_ONCE(rcu_state.gp_flags, rcu_state.gp_flags | RCU_GP_FLAG_INIT);",
            "\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);",
            "\tif (!READ_ONCE(rcu_state.gp_kthread)) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"NoGPkthread\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_grace_period(rcu_state.name, data_race(rcu_state.gp_seq), TPS(\"newreq\"));",
            "\tret = true;  /* Caller must wake GP kthread. */",
            "unlock_out:",
            "\t/* Push furthest requested GP to leaf node and rcu_data structure. */",
            "\tif (ULONG_CMP_LT(gp_seq_req, rnp->gp_seq_needed)) {",
            "\t\tWRITE_ONCE(rnp_start->gp_seq_needed, rnp->gp_seq_needed);",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\t}",
            "\tif (rnp != rnp_start)",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\treturn ret;",
            "}",
            "static bool rcu_future_gp_cleanup(struct rcu_node *rnp)",
            "{",
            "\tbool needmore;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tneedmore = ULONG_CMP_LT(rnp->gp_seq, rnp->gp_seq_needed);",
            "\tif (!needmore)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq; /* Avoid counter wrap. */",
            "\ttrace_rcu_this_gp(rnp, rdp, rnp->gp_seq,",
            "\t\t\t  needmore ? TPS(\"CleanupMore\") : TPS(\"Cleanup\"));",
            "\treturn needmore;",
            "}",
            "static void swake_up_one_online_ipi(void *arg)",
            "{",
            "\tstruct swait_queue_head *wqh = arg;",
            "",
            "\tswake_up_one(wqh);",
            "}",
            "static void swake_up_one_online(struct swait_queue_head *wqh)",
            "{",
            "\tint cpu = get_cpu();",
            "",
            "\t/*",
            "\t * If called from rcutree_report_cpu_starting(), wake up",
            "\t * is dangerous that late in the CPU-down hotplug process. The",
            "\t * scheduler might queue an ignored hrtimer. Defer the wake up",
            "\t * to an online CPU instead.",
            "\t */",
            "\tif (unlikely(cpu_is_offline(cpu))) {",
            "\t\tint target;",
            "",
            "\t\ttarget = cpumask_any_and(housekeeping_cpumask(HK_TYPE_RCU),",
            "\t\t\t\t\t cpu_online_mask);",
            "",
            "\t\tsmp_call_function_single(target, swake_up_one_online_ipi,",
            "\t\t\t\t\t wqh, 0);",
            "\t\tput_cpu();",
            "\t} else {",
            "\t\tput_cpu();",
            "\t\tswake_up_one(wqh);",
            "\t}",
            "}"
          ],
          "function_name": "trace_rcu_this_gp, rcu_start_this_gp, rcu_future_gp_cleanup, swake_up_one_online_ipi, swake_up_one_online",
          "description": "实现RCU grace period事件追踪、新grace period启动逻辑及未来grace period清理机制，支持跨层级节点的同步状态传播。",
          "similarity": 0.5370761156082153
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 658,
          "end_line": 845,
          "content": [
            "int rcu_needs_cpu(void)",
            "{",
            "\treturn !rcu_segcblist_empty(&this_cpu_ptr(&rcu_data)->cblist) &&",
            "\t\t!rcu_rdp_is_offloaded(this_cpu_ptr(&rcu_data));",
            "}",
            "static void rcu_disable_urgency_upon_qs(struct rcu_data *rdp)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rdp->mynode);",
            "\tWRITE_ONCE(rdp->rcu_urgent_qs, false);",
            "\tWRITE_ONCE(rdp->rcu_need_heavy_qs, false);",
            "\tif (tick_nohz_full_cpu(rdp->cpu) && rdp->rcu_forced_tick) {",
            "\t\ttick_dep_clear_cpu(rdp->cpu, TICK_DEP_BIT_RCU);",
            "\t\tWRITE_ONCE(rdp->rcu_forced_tick, false);",
            "\t}",
            "}",
            "notrace bool rcu_is_watching(void)",
            "{",
            "\tbool ret;",
            "",
            "\tpreempt_disable_notrace();",
            "\tret = !rcu_dynticks_curr_cpu_in_eqs();",
            "\tpreempt_enable_notrace();",
            "\treturn ret;",
            "}",
            "void rcu_request_urgent_qs_task(struct task_struct *t)",
            "{",
            "\tint cpu;",
            "",
            "\tbarrier();",
            "\tcpu = task_cpu(t);",
            "\tif (!task_curr(t))",
            "\t\treturn; /* This task is not running on that CPU. */",
            "\tsmp_store_release(per_cpu_ptr(&rcu_data.rcu_urgent_qs, cpu), true);",
            "}",
            "static void rcu_gpnum_ovf(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (ULONG_CMP_LT(rcu_seq_current(&rdp->gp_seq) + ULONG_MAX / 4,",
            "\t\t\t rnp->gp_seq))",
            "\t\tWRITE_ONCE(rdp->gpwrap, true);",
            "\tif (ULONG_CMP_LT(rdp->rcu_iw_gp_seq + ULONG_MAX / 4, rnp->gp_seq))",
            "\t\trdp->rcu_iw_gp_seq = rnp->gp_seq + ULONG_MAX / 4;",
            "}",
            "static int dyntick_save_progress_counter(struct rcu_data *rdp)",
            "{",
            "\trdp->dynticks_snap = rcu_dynticks_snap(rdp->cpu);",
            "\tif (rcu_dynticks_in_eqs(rdp->dynticks_snap)) {",
            "\t\ttrace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS(\"dti\"));",
            "\t\trcu_gpnum_ovf(rdp->mynode, rdp);",
            "\t\treturn 1;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int rcu_implicit_dynticks_qs(struct rcu_data *rdp)",
            "{",
            "\tunsigned long jtsq;",
            "\tint ret = 0;",
            "\tstruct rcu_node *rnp = rdp->mynode;",
            "",
            "\t/*",
            "\t * If the CPU passed through or entered a dynticks idle phase with",
            "\t * no active irq/NMI handlers, then we can safely pretend that the CPU",
            "\t * already acknowledged the request to pass through a quiescent",
            "\t * state.  Either way, that CPU cannot possibly be in an RCU",
            "\t * read-side critical section that started before the beginning",
            "\t * of the current RCU grace period.",
            "\t */",
            "\tif (rcu_dynticks_in_eqs_since(rdp, rdp->dynticks_snap)) {",
            "\t\ttrace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS(\"dti\"));",
            "\t\trcu_gpnum_ovf(rnp, rdp);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\t/*",
            "\t * Complain if a CPU that is considered to be offline from RCU's",
            "\t * perspective has not yet reported a quiescent state.  After all,",
            "\t * the offline CPU should have reported a quiescent state during",
            "\t * the CPU-offline process, or, failing that, by rcu_gp_init()",
            "\t * if it ran concurrently with either the CPU going offline or the",
            "\t * last task on a leaf rcu_node structure exiting its RCU read-side",
            "\t * critical section while all CPUs corresponding to that structure",
            "\t * are offline.  This added warning detects bugs in any of these",
            "\t * code paths.",
            "\t *",
            "\t * The rcu_node structure's ->lock is held here, which excludes",
            "\t * the relevant portions the CPU-hotplug code, the grace-period",
            "\t * initialization code, and the rcu_read_unlock() code paths.",
            "\t *",
            "\t * For more detail, please refer to the \"Hotplug CPU\" section",
            "\t * of RCU's Requirements documentation.",
            "\t */",
            "\tif (WARN_ON_ONCE(!rcu_rdp_cpu_online(rdp))) {",
            "\t\tstruct rcu_node *rnp1;",
            "",
            "\t\tpr_info(\"%s: grp: %d-%d level: %d ->gp_seq %ld ->completedqs %ld\\n\",",
            "\t\t\t__func__, rnp->grplo, rnp->grphi, rnp->level,",
            "\t\t\t(long)rnp->gp_seq, (long)rnp->completedqs);",
            "\t\tfor (rnp1 = rnp; rnp1; rnp1 = rnp1->parent)",
            "\t\t\tpr_info(\"%s: %d:%d ->qsmask %#lx ->qsmaskinit %#lx ->qsmaskinitnext %#lx ->rcu_gp_init_mask %#lx\\n\",",
            "\t\t\t\t__func__, rnp1->grplo, rnp1->grphi, rnp1->qsmask, rnp1->qsmaskinit, rnp1->qsmaskinitnext, rnp1->rcu_gp_init_mask);",
            "\t\tpr_info(\"%s %d: %c online: %ld(%d) offline: %ld(%d)\\n\",",
            "\t\t\t__func__, rdp->cpu, \".o\"[rcu_rdp_cpu_online(rdp)],",
            "\t\t\t(long)rdp->rcu_onl_gp_seq, rdp->rcu_onl_gp_flags,",
            "\t\t\t(long)rdp->rcu_ofl_gp_seq, rdp->rcu_ofl_gp_flags);",
            "\t\treturn 1; /* Break things loose after complaining. */",
            "\t}",
            "",
            "\t/*",
            "\t * A CPU running for an extended time within the kernel can",
            "\t * delay RCU grace periods: (1) At age jiffies_to_sched_qs,",
            "\t * set .rcu_urgent_qs, (2) At age 2*jiffies_to_sched_qs, set",
            "\t * both .rcu_need_heavy_qs and .rcu_urgent_qs.  Note that the",
            "\t * unsynchronized assignments to the per-CPU rcu_need_heavy_qs",
            "\t * variable are safe because the assignments are repeated if this",
            "\t * CPU failed to pass through a quiescent state.  This code",
            "\t * also checks .jiffies_resched in case jiffies_to_sched_qs",
            "\t * is set way high.",
            "\t */",
            "\tjtsq = READ_ONCE(jiffies_to_sched_qs);",
            "\tif (!READ_ONCE(rdp->rcu_need_heavy_qs) &&",
            "\t    (time_after(jiffies, rcu_state.gp_start + jtsq * 2) ||",
            "\t     time_after(jiffies, rcu_state.jiffies_resched) ||",
            "\t     rcu_state.cbovld)) {",
            "\t\tWRITE_ONCE(rdp->rcu_need_heavy_qs, true);",
            "\t\t/* Store rcu_need_heavy_qs before rcu_urgent_qs. */",
            "\t\tsmp_store_release(&rdp->rcu_urgent_qs, true);",
            "\t} else if (time_after(jiffies, rcu_state.gp_start + jtsq)) {",
            "\t\tWRITE_ONCE(rdp->rcu_urgent_qs, true);",
            "\t}",
            "",
            "\t/*",
            "\t * NO_HZ_FULL CPUs can run in-kernel without rcu_sched_clock_irq!",
            "\t * The above code handles this, but only for straight cond_resched().",
            "\t * And some in-kernel loops check need_resched() before calling",
            "\t * cond_resched(), which defeats the above code for CPUs that are",
            "\t * running in-kernel with scheduling-clock interrupts disabled.",
            "\t * So hit them over the head with the resched_cpu() hammer!",
            "\t */",
            "\tif (tick_nohz_full_cpu(rdp->cpu) &&",
            "\t    (time_after(jiffies, READ_ONCE(rdp->last_fqs_resched) + jtsq * 3) ||",
            "\t     rcu_state.cbovld)) {",
            "\t\tWRITE_ONCE(rdp->rcu_urgent_qs, true);",
            "\t\tWRITE_ONCE(rdp->last_fqs_resched, jiffies);",
            "\t\tret = -1;",
            "\t}",
            "",
            "\t/*",
            "\t * If more than halfway to RCU CPU stall-warning time, invoke",
            "\t * resched_cpu() more frequently to try to loosen things up a bit.",
            "\t * Also check to see if the CPU is getting hammered with interrupts,",
            "\t * but only once per grace period, just to keep the IPIs down to",
            "\t * a dull roar.",
            "\t */",
            "\tif (time_after(jiffies, rcu_state.jiffies_resched)) {",
            "\t\tif (time_after(jiffies,",
            "\t\t\t       READ_ONCE(rdp->last_fqs_resched) + jtsq)) {",
            "\t\t\tWRITE_ONCE(rdp->last_fqs_resched, jiffies);",
            "\t\t\tret = -1;",
            "\t\t}",
            "\t\tif (IS_ENABLED(CONFIG_IRQ_WORK) &&",
            "\t\t    !rdp->rcu_iw_pending && rdp->rcu_iw_gp_seq != rnp->gp_seq &&",
            "\t\t    (rnp->ffmask & rdp->grpmask)) {",
            "\t\t\trdp->rcu_iw_pending = true;",
            "\t\t\trdp->rcu_iw_gp_seq = rnp->gp_seq;",
            "\t\t\tirq_work_queue_on(&rdp->rcu_iw, rdp->cpu);",
            "\t\t}",
            "",
            "\t\tif (rcu_cpu_stall_cputime && rdp->snap_record.gp_seq != rdp->gp_seq) {",
            "\t\t\tint cpu = rdp->cpu;",
            "\t\t\tstruct rcu_snap_record *rsrp;",
            "\t\t\tstruct kernel_cpustat *kcsp;",
            "",
            "\t\t\tkcsp = &kcpustat_cpu(cpu);",
            "",
            "\t\t\trsrp = &rdp->snap_record;",
            "\t\t\trsrp->cputime_irq     = kcpustat_field(kcsp, CPUTIME_IRQ, cpu);",
            "\t\t\trsrp->cputime_softirq = kcpustat_field(kcsp, CPUTIME_SOFTIRQ, cpu);",
            "\t\t\trsrp->cputime_system  = kcpustat_field(kcsp, CPUTIME_SYSTEM, cpu);",
            "\t\t\trsrp->nr_hardirqs = kstat_cpu_irqs_sum(cpu) + arch_irq_stat_cpu(cpu);",
            "\t\t\trsrp->nr_softirqs = kstat_cpu_softirqs_sum(cpu);",
            "\t\t\trsrp->nr_csw = nr_context_switches_cpu(cpu);",
            "\t\t\trsrp->jiffies = jiffies;",
            "\t\t\trsrp->gp_seq = rdp->gp_seq;",
            "\t\t}",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "rcu_needs_cpu, rcu_disable_urgency_upon_qs, rcu_is_watching, rcu_request_urgent_qs_task, rcu_gpnum_ovf, dyntick_save_progress_counter, rcu_implicit_dynticks_qs",
          "description": "处理RCU紧迫性需求判定和隐式动态tick quiescent状态检测，通过时间阈值触发CPU唤醒以避免RCU阻塞。",
          "similarity": 0.5368602871894836
        },
        {
          "chunk_id": 18,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3279,
          "end_line": 3412,
          "content": [
            "static void kfree_rcu_monitor(struct work_struct *work)",
            "{",
            "\tstruct kfree_rcu_cpu *krcp = container_of(work,",
            "\t\tstruct kfree_rcu_cpu, monitor_work.work);",
            "",
            "\t// Drain ready for reclaim.",
            "\tkvfree_rcu_drain_ready(krcp);",
            "",
            "\t// Queue a batch for a rest.",
            "\tkvfree_rcu_queue_batch(krcp);",
            "",
            "\t// If there is nothing to detach, it means that our job is",
            "\t// successfully done here. In case of having at least one",
            "\t// of the channels that is still busy we should rearm the",
            "\t// work to repeat an attempt. Because previous batches are",
            "\t// still in progress.",
            "\tif (need_offload_krc(krcp))",
            "\t\tschedule_delayed_monitor_work(krcp);",
            "}",
            "static enum hrtimer_restart",
            "schedule_page_work_fn(struct hrtimer *t)",
            "{",
            "\tstruct kfree_rcu_cpu *krcp =",
            "\t\tcontainer_of(t, struct kfree_rcu_cpu, hrtimer);",
            "",
            "\tqueue_delayed_work(system_highpri_wq, &krcp->page_cache_work, 0);",
            "\treturn HRTIMER_NORESTART;",
            "}",
            "static void fill_page_cache_func(struct work_struct *work)",
            "{",
            "\tstruct kvfree_rcu_bulk_data *bnode;",
            "\tstruct kfree_rcu_cpu *krcp =",
            "\t\tcontainer_of(work, struct kfree_rcu_cpu,",
            "\t\t\tpage_cache_work.work);",
            "\tunsigned long flags;",
            "\tint nr_pages;",
            "\tbool pushed;",
            "\tint i;",
            "",
            "\tnr_pages = atomic_read(&krcp->backoff_page_cache_fill) ?",
            "\t\t1 : rcu_min_cached_objs;",
            "",
            "\tfor (i = READ_ONCE(krcp->nr_bkv_objs); i < nr_pages; i++) {",
            "\t\tbnode = (struct kvfree_rcu_bulk_data *)",
            "\t\t\t__get_free_page(GFP_KERNEL | __GFP_NORETRY | __GFP_NOMEMALLOC | __GFP_NOWARN);",
            "",
            "\t\tif (!bnode)",
            "\t\t\tbreak;",
            "",
            "\t\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\t\tpushed = put_cached_bnode(krcp, bnode);",
            "\t\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "",
            "\t\tif (!pushed) {",
            "\t\t\tfree_page((unsigned long) bnode);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tatomic_set(&krcp->work_in_progress, 0);",
            "\tatomic_set(&krcp->backoff_page_cache_fill, 0);",
            "}",
            "static void",
            "run_page_cache_worker(struct kfree_rcu_cpu *krcp)",
            "{",
            "\t// If cache disabled, bail out.",
            "\tif (!rcu_min_cached_objs)",
            "\t\treturn;",
            "",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_RUNNING &&",
            "\t\t\t!atomic_xchg(&krcp->work_in_progress, 1)) {",
            "\t\tif (atomic_read(&krcp->backoff_page_cache_fill)) {",
            "\t\t\tqueue_delayed_work(system_unbound_wq,",
            "\t\t\t\t&krcp->page_cache_work,",
            "\t\t\t\t\tmsecs_to_jiffies(rcu_delay_page_cache_fill_msec));",
            "\t\t} else {",
            "\t\t\thrtimer_init(&krcp->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);",
            "\t\t\tkrcp->hrtimer.function = schedule_page_work_fn;",
            "\t\t\thrtimer_start(&krcp->hrtimer, 0, HRTIMER_MODE_REL);",
            "\t\t}",
            "\t}",
            "}",
            "static inline bool",
            "add_ptr_to_bulk_krc_lock(struct kfree_rcu_cpu **krcp,",
            "\tunsigned long *flags, void *ptr, bool can_alloc)",
            "{",
            "\tstruct kvfree_rcu_bulk_data *bnode;",
            "\tint idx;",
            "",
            "\t*krcp = krc_this_cpu_lock(flags);",
            "\tif (unlikely(!(*krcp)->initialized))",
            "\t\treturn false;",
            "",
            "\tidx = !!is_vmalloc_addr(ptr);",
            "\tbnode = list_first_entry_or_null(&(*krcp)->bulk_head[idx],",
            "\t\tstruct kvfree_rcu_bulk_data, list);",
            "",
            "\t/* Check if a new block is required. */",
            "\tif (!bnode || bnode->nr_records == KVFREE_BULK_MAX_ENTR) {",
            "\t\tbnode = get_cached_bnode(*krcp);",
            "\t\tif (!bnode && can_alloc) {",
            "\t\t\tkrc_this_cpu_unlock(*krcp, *flags);",
            "",
            "\t\t\t// __GFP_NORETRY - allows a light-weight direct reclaim",
            "\t\t\t// what is OK from minimizing of fallback hitting point of",
            "\t\t\t// view. Apart of that it forbids any OOM invoking what is",
            "\t\t\t// also beneficial since we are about to release memory soon.",
            "\t\t\t//",
            "\t\t\t// __GFP_NOMEMALLOC - prevents from consuming of all the",
            "\t\t\t// memory reserves. Please note we have a fallback path.",
            "\t\t\t//",
            "\t\t\t// __GFP_NOWARN - it is supposed that an allocation can",
            "\t\t\t// be failed under low memory or high memory pressure",
            "\t\t\t// scenarios.",
            "\t\t\tbnode = (struct kvfree_rcu_bulk_data *)",
            "\t\t\t\t__get_free_page(GFP_KERNEL | __GFP_NORETRY | __GFP_NOMEMALLOC | __GFP_NOWARN);",
            "\t\t\traw_spin_lock_irqsave(&(*krcp)->lock, *flags);",
            "\t\t}",
            "",
            "\t\tif (!bnode)",
            "\t\t\treturn false;",
            "",
            "\t\t// Initialize the new block and attach it.",
            "\t\tbnode->nr_records = 0;",
            "\t\tlist_add(&bnode->list, &(*krcp)->bulk_head[idx]);",
            "\t}",
            "",
            "\t// Finally insert and update the GP for this page.",
            "\tbnode->records[bnode->nr_records++] = ptr;",
            "\tget_state_synchronize_rcu_full(&bnode->gp_snap);",
            "\tatomic_inc(&(*krcp)->bulk_count[idx]);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "kfree_rcu_monitor, schedule_page_work_fn, fill_page_cache_func, run_page_cache_worker, add_ptr_to_bulk_krc_lock",
          "description": "维护页面缓存池并提供动态扩展能力，通过工作队列机制定期补充缓存页，支持SLAB与vmalloc指针的分类存储。",
          "similarity": 0.5329824686050415
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2523,
          "end_line": 2628,
          "content": [
            "static void rcu_cpu_kthread_park(unsigned int cpu)",
            "{",
            "\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;",
            "}",
            "static int rcu_cpu_kthread_should_run(unsigned int cpu)",
            "{",
            "\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);",
            "}",
            "static void rcu_cpu_kthread(unsigned int cpu)",
            "{",
            "\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);",
            "\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);",
            "\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);",
            "\tint spincnt;",
            "",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));",
            "\tfor (spincnt = 0; spincnt < 10; spincnt++) {",
            "\t\tWRITE_ONCE(*j, jiffies);",
            "\t\tlocal_bh_disable();",
            "\t\t*statusp = RCU_KTHREAD_RUNNING;",
            "\t\tlocal_irq_disable();",
            "\t\twork = *workp;",
            "\t\tWRITE_ONCE(*workp, 0);",
            "\t\tlocal_irq_enable();",
            "\t\tif (work)",
            "\t\t\trcu_core();",
            "\t\tlocal_bh_enable();",
            "\t\tif (!READ_ONCE(*workp)) {",
            "\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));",
            "\t\t\t*statusp = RCU_KTHREAD_WAITING;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "\t*statusp = RCU_KTHREAD_YIELDING;",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));",
            "\tschedule_timeout_idle(2);",
            "\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));",
            "\t*statusp = RCU_KTHREAD_WAITING;",
            "\tWRITE_ONCE(*j, jiffies);",
            "}",
            "static int __init rcu_spawn_core_kthreads(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;",
            "\tif (use_softirq)",
            "\t\treturn 0;",
            "\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),",
            "\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);",
            "\treturn 0;",
            "}",
            "static void rcutree_enqueue(struct rcu_data *rdp, struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\trcu_segcblist_enqueue(&rdp->cblist, head);",
            "\tif (__is_kvfree_rcu_offset((unsigned long)func))",
            "\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,",
            "\t\t\t\t\t (unsigned long)func,",
            "\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));",
            "\telse",
            "\t\ttrace_rcu_callback(rcu_state.name, head,",
            "\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));",
            "}",
            "static void call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,",
            "\t\t\t  rcu_callback_t func, unsigned long flags)",
            "{",
            "\trcutree_enqueue(rdp, head, func);",
            "\t/*",
            "\t * If called from an extended quiescent state, invoke the RCU",
            "\t * core in order to force a re-evaluation of RCU's idleness.",
            "\t */",
            "\tif (!rcu_is_watching())",
            "\t\tinvoke_rcu_core();",
            "",
            "\t/* If interrupts were disabled or CPU offline, don't invoke RCU core. */",
            "\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Force the grace period if too many callbacks or too long waiting.",
            "\t * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()",
            "\t * if some other CPU has recently done so.  Also, don't bother",
            "\t * invoking rcu_force_quiescent_state() if the newly enqueued callback",
            "\t * is the only one waiting for a grace period to complete.",
            "\t */",
            "\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >",
            "\t\t     rdp->qlen_last_fqs_check + qhimark)) {",
            "",
            "\t\t/* Are we ignoring a completed grace period? */",
            "\t\tnote_gp_changes(rdp);",
            "",
            "\t\t/* Start a new grace period if one not already started. */",
            "\t\tif (!rcu_gp_in_progress()) {",
            "\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);",
            "\t\t} else {",
            "\t\t\t/* Give the grace period a kick. */",
            "\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;",
            "\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&",
            "\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)",
            "\t\t\t\trcu_force_quiescent_state();",
            "\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "rcu_cpu_kthread_park, rcu_cpu_kthread_should_run, rcu_cpu_kthread, rcu_spawn_core_kthreads, rcutree_enqueue, call_rcu_core",
          "description": "实现RCU k线程管理与回调分发基础设施，包含线程启动、回调入队及触发条件判断逻辑，提供跨CPU的异步处理能力",
          "similarity": 0.5319589376449585
        }
      ]
    },
    {
      "source_file": "kernel/trace/tracing_map.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:40:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `trace\\tracing_map.c`\n\n---\n\n# `trace/tracing_map.c` 技术文档\n\n## 1. 文件概述\n\n`tracing_map.c` 实现了一个**无锁（lock-free）的哈希映射结构**，专为 Linux 内核的追踪（tracing）子系统设计。该结构支持高并发场景下的高效插入、查找和聚合操作，适用于事件统计、直方图构建等实时追踪需求。其实现灵感来源于 Cliff Click 提出的无锁哈希表算法，旨在避免传统锁机制带来的性能瓶颈和死锁风险。\n\n该文件提供了对 `tracing_map` 中元素（`tracing_map_elt`）的字段操作接口，包括**累加求和字段（sum fields）**、**变量字段（var fields）** 以及**键字段（key fields）** 的管理，并支持基于不同数据类型的比较函数，用于后续排序或聚合。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `tracing_map_update_sum()` | 对指定元素的指定 sum 字段原子地累加一个值 |\n| `tracing_map_read_sum()` | 读取指定元素的指定 sum 字段的当前值 |\n| `tracing_map_set_var()` | 设置指定元素的指定 var 字段的值，并标记为“已设置” |\n| `tracing_map_var_set()` | 检查指定 var 字段是否已被设置 |\n| `tracing_map_read_var()` | 读取指定 var 字段的值（不改变其状态） |\n| `tracing_map_read_var_once()` | 读取并**重置**指定 var 字段为“未设置”状态，实现“一次读取”语义 |\n| `tracing_map_add_sum_field()` | 向 tracing_map 添加一个 sum 字段，返回其索引 |\n| `tracing_map_add_var()` | 向 tracing_map 添加一个 var 字段，返回其索引 |\n| `tracing_map_add_key_field()` | 向 tracing_map 注册一个 key 字段及其比较函数和偏移量 |\n| `tracing_map_cmp_num()` | 根据字段大小和符号性，返回对应的数值比较函数指针 |\n\n### 比较函数（Comparison Functions）\n\n- `tracing_map_cmp_string()`：字符串比较（使用 `strcmp`）\n- `tracing_map_cmp_none()`：恒等比较（始终返回 0）\n- `tracing_map_cmp_atomic64()`：用于 sum 字段的原子64位整数比较\n- 通过宏 `DEFINE_TRACING_MAP_CMP_FN` 自动生成的各类整数比较函数：\n  - `tracing_map_cmp_s64/u64/s32/u32/s16/u16/s8/u8`\n\n### 数据结构（定义在 `tracing_map.h` 中）\n\n- `struct tracing_map`：追踪映射的主结构体，包含字段元数据、桶数组等\n- `struct tracing_map_elt`：映射中的单个元素，包含 key、sum 字段数组、var 字段数组及状态标记\n- `tracing_map_cmp_fn_t`：比较函数指针类型\n\n## 3. 关键实现\n\n### 无锁设计基础\n- 虽然本文件主要提供字段操作接口，但其底层 `tracing_map` 结构基于无锁哈希表实现（参考 Cliff Click 算法），确保多 CPU 并发写入时的数据一致性。\n- **sum 字段**使用 `atomic64_t` 类型，通过 `atomic64_add()` 和 `atomic64_read()` 实现线程安全的累加与读取。\n- **var 字段**同样使用 `atomic64_t` 存储值，但额外维护一个 `bool var_set[]` 数组来跟踪变量是否被显式设置，支持“一次读取”语义。\n\n### 字段管理机制\n- **字段索引**：`tracing_map` 在初始化阶段通过 `tracing_map_add_*_field()` 系列函数注册字段，返回的索引用于后续对 `tracing_map_elt` 中对应字段的访问。\n- **sum vs var**：\n  - **sum 字段**：专用于累加统计（如事件计数、总耗时），天然支持并发更新。\n  - **var 字段**：用于存储瞬时值或状态（如最新时间戳、错误码），支持“设置-读取-重置”模式。\n- **key 字段**：仅用于定义复合键的组成部分及其排序规则，不直接存储在 `tracing_map_elt` 的字段数组中，而是作为键的一部分参与哈希和比较。\n\n### 类型安全的比较函数\n- 通过 `tracing_map_cmp_num()` 函数，根据字段的**字节大小（1/2/4/8）** 和**符号性（signed/unsigned）** 动态选择正确的比较函数，确保排序和聚合逻辑的正确性。\n- 所有数值比较函数均通过宏生成，避免重复代码，保证类型转换安全。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- `<linux/vmalloc.h>`：用于大内存分配（可能用于哈希表桶）\n- `<linux/jhash.h>`：提供 Jenkins 哈希函数（实际哈希逻辑在 `tracing_map.h` 或其他文件中）\n- `<linux/slab.h>`：内核内存分配器（`kmalloc`/`kfree`）\n- `<linux/sort.h>`：提供排序功能（用于结果输出）\n- `<linux/kmemleak.h>`：内存泄漏检测支持\n- `\"tracing_map.h\"`：核心数据结构和 API 声明\n- `\"trace.h\"`：追踪子系统通用头文件\n\n### 内核子系统依赖\n- **Tracing Subsystem**：作为核心追踪基础设施的一部分，被事件触发器（如 `hist` 触发器）、直方图统计等功能使用。\n- **Memory Management**：依赖 SLAB/SLUB 分配器管理 `tracing_map` 和 `tracing_map_elt` 对象。\n- **Atomic Operations**：重度依赖 `atomic64_*` 系列原子操作保证并发安全。\n\n## 5. 使用场景\n\n- **事件聚合统计**：在 `ftrace` 或 `eBPF` 追踪中，对具有相同键（如进程 PID、函数名）的事件进行计数、求和（如总延迟、总字节数）。\n- **直方图构建**：`hist` 触发器使用 `tracing_map` 存储每个桶（bucket）的统计信息，sum 字段记录命中次数。\n- **状态跟踪**：var 字段可用于记录每个键关联的最新状态（如最后一次错误码、最大延迟值），并通过 `read_var_once` 实现状态消费。\n- **高性能追踪**：在高频率事件（如每秒百万级）场景下，无锁设计避免了传统哈希表在锁竞争下的性能下降，适用于实时性要求高的系统分析。",
      "similarity": 0.5628533363342285,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/trace/tracing_map.c",
          "start_line": 886,
          "end_line": 1016,
          "content": [
            "static int cmp_entries_key(const void *A, const void *B)",
            "{",
            "\tconst struct tracing_map_elt *elt_a, *elt_b;",
            "\tconst struct tracing_map_sort_entry *a, *b;",
            "\tstruct tracing_map_sort_key *sort_key;",
            "\tstruct tracing_map_field *field;",
            "\ttracing_map_cmp_fn_t cmp_fn;",
            "\tvoid *val_a, *val_b;",
            "\tint ret = 0;",
            "",
            "\ta = *(const struct tracing_map_sort_entry **)A;",
            "\tb = *(const struct tracing_map_sort_entry **)B;",
            "",
            "\telt_a = a->elt;",
            "\telt_b = b->elt;",
            "",
            "\tsort_key = &elt_a->map->sort_key;",
            "",
            "\tfield = &elt_a->fields[sort_key->field_idx];",
            "",
            "\tcmp_fn = field->cmp_fn;",
            "",
            "\tval_a = elt_a->key + field->offset;",
            "\tval_b = elt_b->key + field->offset;",
            "",
            "\tret = cmp_fn(val_a, val_b);",
            "\tif (sort_key->descending)",
            "\t\tret = -ret;",
            "",
            "\treturn ret;",
            "}",
            "static void destroy_sort_entry(struct tracing_map_sort_entry *entry)",
            "{",
            "\tif (!entry)",
            "\t\treturn;",
            "",
            "\tif (entry->elt_copied)",
            "\t\ttracing_map_elt_free(entry->elt);",
            "",
            "\tkfree(entry);",
            "}",
            "void tracing_map_destroy_sort_entries(struct tracing_map_sort_entry **entries,",
            "\t\t\t\t      unsigned int n_entries)",
            "{",
            "\tunsigned int i;",
            "",
            "\tfor (i = 0; i < n_entries; i++)",
            "\t\tdestroy_sort_entry(entries[i]);",
            "",
            "\tvfree(entries);",
            "}",
            "static void detect_dups(struct tracing_map_sort_entry **sort_entries,",
            "\t\t      int n_entries, unsigned int key_size)",
            "{",
            "\tunsigned int total_dups = 0;",
            "\tint i;",
            "\tvoid *key;",
            "",
            "\tif (n_entries < 2)",
            "\t\treturn;",
            "",
            "\tsort(sort_entries, n_entries, sizeof(struct tracing_map_sort_entry *),",
            "\t     (int (*)(const void *, const void *))cmp_entries_dup, NULL);",
            "",
            "\tkey = sort_entries[0]->key;",
            "\tfor (i = 1; i < n_entries; i++) {",
            "\t\tif (!memcmp(sort_entries[i]->key, key, key_size)) {",
            "\t\t\ttotal_dups++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tkey = sort_entries[i]->key;",
            "\t}",
            "",
            "\tWARN_ONCE(total_dups > 0,",
            "\t\t  \"Duplicates detected: %d\\n\", total_dups);",
            "}",
            "static bool is_key(struct tracing_map *map, unsigned int field_idx)",
            "{",
            "\tunsigned int i;",
            "",
            "\tfor (i = 0; i < map->n_keys; i++)",
            "\t\tif (map->key_idx[i] == field_idx)",
            "\t\t\treturn true;",
            "\treturn false;",
            "}",
            "static void sort_secondary(struct tracing_map *map,",
            "\t\t\t   const struct tracing_map_sort_entry **entries,",
            "\t\t\t   unsigned int n_entries,",
            "\t\t\t   struct tracing_map_sort_key *primary_key,",
            "\t\t\t   struct tracing_map_sort_key *secondary_key)",
            "{",
            "\tint (*primary_fn)(const void *, const void *);",
            "\tint (*secondary_fn)(const void *, const void *);",
            "\tunsigned i, start = 0, n_sub = 1;",
            "",
            "\tif (is_key(map, primary_key->field_idx))",
            "\t\tprimary_fn = cmp_entries_key;",
            "\telse",
            "\t\tprimary_fn = cmp_entries_sum;",
            "",
            "\tif (is_key(map, secondary_key->field_idx))",
            "\t\tsecondary_fn = cmp_entries_key;",
            "\telse",
            "\t\tsecondary_fn = cmp_entries_sum;",
            "",
            "\tfor (i = 0; i < n_entries - 1; i++) {",
            "\t\tconst struct tracing_map_sort_entry **a = &entries[i];",
            "\t\tconst struct tracing_map_sort_entry **b = &entries[i + 1];",
            "",
            "\t\tif (primary_fn(a, b) == 0) {",
            "\t\t\tn_sub++;",
            "\t\t\tif (i < n_entries - 2)",
            "\t\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tif (n_sub < 2) {",
            "\t\t\tstart = i + 1;",
            "\t\t\tn_sub = 1;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tset_sort_key(map, secondary_key);",
            "\t\tsort(&entries[start], n_sub,",
            "\t\t     sizeof(struct tracing_map_sort_entry *),",
            "\t\t     (int (*)(const void *, const void *))secondary_fn, NULL);",
            "\t\tset_sort_key(map, primary_key);",
            "",
            "\t\tstart = i + 1;",
            "\t\tn_sub = 1;",
            "\t}",
            "}"
          ],
          "function_name": "cmp_entries_key, destroy_sort_entry, tracing_map_destroy_sort_entries, detect_dups, is_key, sort_secondary",
          "description": "实现cmp_entries_key等排序比较函数，detect_dups检测重复键，destroy_sort_entry销毁排序条目，sort_secondary实现多级排序逻辑",
          "similarity": 0.5666529536247253
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/trace/tracing_map.c",
          "start_line": 1070,
          "end_line": 1135,
          "content": [
            "int tracing_map_sort_entries(struct tracing_map *map,",
            "\t\t\t     struct tracing_map_sort_key *sort_keys,",
            "\t\t\t     unsigned int n_sort_keys,",
            "\t\t\t     struct tracing_map_sort_entry ***sort_entries)",
            "{",
            "\tint (*cmp_entries_fn)(const void *, const void *);",
            "\tstruct tracing_map_sort_entry *sort_entry, **entries;",
            "\tint i, n_entries, ret;",
            "",
            "\tentries = vmalloc(array_size(sizeof(sort_entry), map->max_elts));",
            "\tif (!entries)",
            "\t\treturn -ENOMEM;",
            "",
            "\tfor (i = 0, n_entries = 0; i < map->map_size; i++) {",
            "\t\tstruct tracing_map_entry *entry;",
            "",
            "\t\tentry = TRACING_MAP_ENTRY(map->map, i);",
            "",
            "\t\tif (!entry->key || !entry->val)",
            "\t\t\tcontinue;",
            "",
            "\t\tentries[n_entries] = create_sort_entry(entry->val->key,",
            "\t\t\t\t\t\t       entry->val);",
            "\t\tif (!entries[n_entries++]) {",
            "\t\t\tret = -ENOMEM;",
            "\t\t\tgoto free;",
            "\t\t}",
            "\t}",
            "",
            "\tif (n_entries == 0) {",
            "\t\tret = 0;",
            "\t\tgoto free;",
            "\t}",
            "",
            "\tif (n_entries == 1) {",
            "\t\t*sort_entries = entries;",
            "\t\treturn 1;",
            "\t}",
            "",
            "\tdetect_dups(entries, n_entries, map->key_size);",
            "",
            "\tif (is_key(map, sort_keys[0].field_idx))",
            "\t\tcmp_entries_fn = cmp_entries_key;",
            "\telse",
            "\t\tcmp_entries_fn = cmp_entries_sum;",
            "",
            "\tset_sort_key(map, &sort_keys[0]);",
            "",
            "\tsort(entries, n_entries, sizeof(struct tracing_map_sort_entry *),",
            "\t     (int (*)(const void *, const void *))cmp_entries_fn, NULL);",
            "",
            "\tif (n_sort_keys > 1)",
            "\t\tsort_secondary(map,",
            "\t\t\t       (const struct tracing_map_sort_entry **)entries,",
            "\t\t\t       n_entries,",
            "\t\t\t       &sort_keys[0],",
            "\t\t\t       &sort_keys[1]);",
            "",
            "\t*sort_entries = entries;",
            "",
            "\treturn n_entries;",
            " free:",
            "\ttracing_map_destroy_sort_entries(entries, n_entries);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "tracing_map_sort_entries",
          "description": "该代码实现了对`tracing_map`中有效条目的排序逻辑，核心功能是根据指定的排序键对数据进行多级排序。函数通过收集非空条目、检测重复项、选择比较函数并调用排序算法完成排序，但部分辅助函数如`detect_dups`和`sort_secondary`的实现未在上下文中完整展示。",
          "similarity": 0.5329252481460571
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/trace/tracing_map.c",
          "start_line": 483,
          "end_line": 594,
          "content": [
            "static int tracing_map_alloc_elts(struct tracing_map *map)",
            "{",
            "\tunsigned int i;",
            "",
            "\tmap->elts = tracing_map_array_alloc(map->max_elts,",
            "\t\t\t\t\t    sizeof(struct tracing_map_elt *));",
            "\tif (!map->elts)",
            "\t\treturn -ENOMEM;",
            "",
            "\tfor (i = 0; i < map->max_elts; i++) {",
            "\t\t*(TRACING_MAP_ELT(map->elts, i)) = tracing_map_elt_alloc(map);",
            "\t\tif (IS_ERR(*(TRACING_MAP_ELT(map->elts, i)))) {",
            "\t\t\t*(TRACING_MAP_ELT(map->elts, i)) = NULL;",
            "\t\t\ttracing_map_free_elts(map);",
            "",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static inline bool keys_match(void *key, void *test_key, unsigned key_size)",
            "{",
            "\tbool match = true;",
            "",
            "\tif (memcmp(key, test_key, key_size))",
            "\t\tmatch = false;",
            "",
            "\treturn match;",
            "}",
            "void tracing_map_destroy(struct tracing_map *map)",
            "{",
            "\tif (!map)",
            "\t\treturn;",
            "",
            "\ttracing_map_free_elts(map);",
            "",
            "\ttracing_map_array_free(map->map);",
            "\tkfree(map);",
            "}",
            "void tracing_map_clear(struct tracing_map *map)",
            "{",
            "\tunsigned int i;",
            "",
            "\tatomic_set(&map->next_elt, 0);",
            "\tatomic64_set(&map->hits, 0);",
            "\tatomic64_set(&map->drops, 0);",
            "",
            "\ttracing_map_array_clear(map->map);",
            "",
            "\tfor (i = 0; i < map->max_elts; i++)",
            "\t\ttracing_map_elt_clear(*(TRACING_MAP_ELT(map->elts, i)));",
            "}",
            "static void set_sort_key(struct tracing_map *map,",
            "\t\t\t struct tracing_map_sort_key *sort_key)",
            "{",
            "\tmap->sort_key = *sort_key;",
            "}",
            "int tracing_map_init(struct tracing_map *map)",
            "{",
            "\tint err;",
            "",
            "\tif (map->n_fields < 2)",
            "\t\treturn -EINVAL; /* need at least 1 key and 1 val */",
            "",
            "\terr = tracing_map_alloc_elts(map);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\ttracing_map_clear(map);",
            "",
            "\treturn err;",
            "}",
            "static int cmp_entries_dup(const void *A, const void *B)",
            "{",
            "\tconst struct tracing_map_sort_entry *a, *b;",
            "",
            "\ta = *(const struct tracing_map_sort_entry **)A;",
            "\tb = *(const struct tracing_map_sort_entry **)B;",
            "",
            "\treturn memcmp(a->key, b->key, a->elt->map->key_size);",
            "}",
            "static int cmp_entries_sum(const void *A, const void *B)",
            "{",
            "\tconst struct tracing_map_elt *elt_a, *elt_b;",
            "\tconst struct tracing_map_sort_entry *a, *b;",
            "\tstruct tracing_map_sort_key *sort_key;",
            "\tstruct tracing_map_field *field;",
            "\ttracing_map_cmp_fn_t cmp_fn;",
            "\tvoid *val_a, *val_b;",
            "\tint ret = 0;",
            "",
            "\ta = *(const struct tracing_map_sort_entry **)A;",
            "\tb = *(const struct tracing_map_sort_entry **)B;",
            "",
            "\telt_a = a->elt;",
            "\telt_b = b->elt;",
            "",
            "\tsort_key = &elt_a->map->sort_key;",
            "",
            "\tfield = &elt_a->fields[sort_key->field_idx];",
            "\tcmp_fn = field->cmp_fn;",
            "",
            "\tval_a = &elt_a->fields[sort_key->field_idx].sum;",
            "\tval_b = &elt_b->fields[sort_key->field_idx].sum;",
            "",
            "\tret = cmp_fn(val_a, val_b);",
            "\tif (sort_key->descending)",
            "\t\tret = -ret;",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "tracing_map_alloc_elts, keys_match, tracing_map_destroy, tracing_map_clear, set_sort_key, tracing_map_init, cmp_entries_dup, cmp_entries_sum",
          "description": "实现tracing_map_alloc_elts动态分配元素数组，tracing_map_clear重置映射状态，set_sort_key配置排序键，tracing_map_init初始化映射结构",
          "similarity": 0.5240597724914551
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/trace/tracing_map.c",
          "start_line": 39,
          "end_line": 141,
          "content": [
            "void tracing_map_update_sum(struct tracing_map_elt *elt, unsigned int i, u64 n)",
            "{",
            "\tatomic64_add(n, &elt->fields[i].sum);",
            "}",
            "u64 tracing_map_read_sum(struct tracing_map_elt *elt, unsigned int i)",
            "{",
            "\treturn (u64)atomic64_read(&elt->fields[i].sum);",
            "}",
            "void tracing_map_set_var(struct tracing_map_elt *elt, unsigned int i, u64 n)",
            "{",
            "\tatomic64_set(&elt->vars[i], n);",
            "\telt->var_set[i] = true;",
            "}",
            "bool tracing_map_var_set(struct tracing_map_elt *elt, unsigned int i)",
            "{",
            "\treturn elt->var_set[i];",
            "}",
            "u64 tracing_map_read_var(struct tracing_map_elt *elt, unsigned int i)",
            "{",
            "\treturn (u64)atomic64_read(&elt->vars[i]);",
            "}",
            "u64 tracing_map_read_var_once(struct tracing_map_elt *elt, unsigned int i)",
            "{",
            "\telt->var_set[i] = false;",
            "\treturn (u64)atomic64_read(&elt->vars[i]);",
            "}",
            "int tracing_map_cmp_string(void *val_a, void *val_b)",
            "{",
            "\tchar *a = val_a;",
            "\tchar *b = val_b;",
            "",
            "\treturn strcmp(a, b);",
            "}",
            "int tracing_map_cmp_none(void *val_a, void *val_b)",
            "{",
            "\treturn 0;",
            "}",
            "static int tracing_map_cmp_atomic64(void *val_a, void *val_b)",
            "{",
            "\tu64 a = atomic64_read((atomic64_t *)val_a);",
            "\tu64 b = atomic64_read((atomic64_t *)val_b);",
            "",
            "\treturn (a > b) ? 1 : ((a < b) ? -1 : 0);",
            "}",
            "tracing_map_cmp_fn_t tracing_map_cmp_num(int field_size,",
            "\t\t\t\t\t int field_is_signed)",
            "{",
            "\ttracing_map_cmp_fn_t fn = tracing_map_cmp_none;",
            "",
            "\tswitch (field_size) {",
            "\tcase 8:",
            "\t\tif (field_is_signed)",
            "\t\t\tfn = tracing_map_cmp_s64;",
            "\t\telse",
            "\t\t\tfn = tracing_map_cmp_u64;",
            "\t\tbreak;",
            "\tcase 4:",
            "\t\tif (field_is_signed)",
            "\t\t\tfn = tracing_map_cmp_s32;",
            "\t\telse",
            "\t\t\tfn = tracing_map_cmp_u32;",
            "\t\tbreak;",
            "\tcase 2:",
            "\t\tif (field_is_signed)",
            "\t\t\tfn = tracing_map_cmp_s16;",
            "\t\telse",
            "\t\t\tfn = tracing_map_cmp_u16;",
            "\t\tbreak;",
            "\tcase 1:",
            "\t\tif (field_is_signed)",
            "\t\t\tfn = tracing_map_cmp_s8;",
            "\t\telse",
            "\t\t\tfn = tracing_map_cmp_u8;",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn fn;",
            "}",
            "static int tracing_map_add_field(struct tracing_map *map,",
            "\t\t\t\t tracing_map_cmp_fn_t cmp_fn)",
            "{",
            "\tint ret = -EINVAL;",
            "",
            "\tif (map->n_fields < TRACING_MAP_FIELDS_MAX) {",
            "\t\tret = map->n_fields;",
            "\t\tmap->fields[map->n_fields++].cmp_fn = cmp_fn;",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "int tracing_map_add_sum_field(struct tracing_map *map)",
            "{",
            "\treturn tracing_map_add_field(map, tracing_map_cmp_atomic64);",
            "}",
            "int tracing_map_add_var(struct tracing_map *map)",
            "{",
            "\tint ret = -EINVAL;",
            "",
            "\tif (map->n_vars < TRACING_MAP_VARS_MAX)",
            "\t\tret = map->n_vars++;",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "tracing_map_update_sum, tracing_map_read_sum, tracing_map_set_var, tracing_map_var_set, tracing_map_read_var, tracing_map_read_var_once, tracing_map_cmp_string, tracing_map_cmp_none, tracing_map_cmp_atomic64, tracing_map_cmp_num, tracing_map_add_field, tracing_map_add_sum_field, tracing_map_add_var",
          "description": "实现tracing_map_update_sum等函数，通过atomic64操作更新sum字段，提供变量设置与读取接口，定义字符串、原子64位数等比较函数及字段添加逻辑",
          "similarity": 0.4931070804595947
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/trace/tracing_map.c",
          "start_line": 270,
          "end_line": 374,
          "content": [
            "int tracing_map_add_key_field(struct tracing_map *map,",
            "\t\t\t      unsigned int offset,",
            "\t\t\t      tracing_map_cmp_fn_t cmp_fn)",
            "",
            "{",
            "\tint idx = tracing_map_add_field(map, cmp_fn);",
            "",
            "\tif (idx < 0)",
            "\t\treturn idx;",
            "",
            "\tmap->fields[idx].offset = offset;",
            "",
            "\tmap->key_idx[map->n_keys++] = idx;",
            "",
            "\treturn idx;",
            "}",
            "static void tracing_map_array_clear(struct tracing_map_array *a)",
            "{",
            "\tunsigned int i;",
            "",
            "\tif (!a->pages)",
            "\t\treturn;",
            "",
            "\tfor (i = 0; i < a->n_pages; i++)",
            "\t\tmemset(a->pages[i], 0, PAGE_SIZE);",
            "}",
            "static void tracing_map_array_free(struct tracing_map_array *a)",
            "{",
            "\tunsigned int i;",
            "",
            "\tif (!a)",
            "\t\treturn;",
            "",
            "\tif (!a->pages)",
            "\t\tgoto free;",
            "",
            "\tfor (i = 0; i < a->n_pages; i++) {",
            "\t\tif (!a->pages[i])",
            "\t\t\tbreak;",
            "\t\tkmemleak_free(a->pages[i]);",
            "\t\tfree_page((unsigned long)a->pages[i]);",
            "\t}",
            "",
            "\tkfree(a->pages);",
            "",
            " free:",
            "\tkfree(a);",
            "}",
            "static void tracing_map_elt_clear(struct tracing_map_elt *elt)",
            "{",
            "\tunsigned i;",
            "",
            "\tfor (i = 0; i < elt->map->n_fields; i++)",
            "\t\tif (elt->fields[i].cmp_fn == tracing_map_cmp_atomic64)",
            "\t\t\tatomic64_set(&elt->fields[i].sum, 0);",
            "",
            "\tfor (i = 0; i < elt->map->n_vars; i++) {",
            "\t\tatomic64_set(&elt->vars[i], 0);",
            "\t\telt->var_set[i] = false;",
            "\t}",
            "",
            "\tif (elt->map->ops && elt->map->ops->elt_clear)",
            "\t\telt->map->ops->elt_clear(elt);",
            "}",
            "static void tracing_map_elt_init_fields(struct tracing_map_elt *elt)",
            "{",
            "\tunsigned int i;",
            "",
            "\ttracing_map_elt_clear(elt);",
            "",
            "\tfor (i = 0; i < elt->map->n_fields; i++) {",
            "\t\telt->fields[i].cmp_fn = elt->map->fields[i].cmp_fn;",
            "",
            "\t\tif (elt->fields[i].cmp_fn != tracing_map_cmp_atomic64)",
            "\t\t\telt->fields[i].offset = elt->map->fields[i].offset;",
            "\t}",
            "}",
            "static void tracing_map_elt_free(struct tracing_map_elt *elt)",
            "{",
            "\tif (!elt)",
            "\t\treturn;",
            "",
            "\tif (elt->map->ops && elt->map->ops->elt_free)",
            "\t\telt->map->ops->elt_free(elt);",
            "\tkfree(elt->fields);",
            "\tkfree(elt->vars);",
            "\tkfree(elt->var_set);",
            "\tkfree(elt->key);",
            "\tkfree(elt);",
            "}",
            "static void tracing_map_free_elts(struct tracing_map *map)",
            "{",
            "\tunsigned int i;",
            "",
            "\tif (!map->elts)",
            "\t\treturn;",
            "",
            "\tfor (i = 0; i < map->max_elts; i++) {",
            "\t\ttracing_map_elt_free(*(TRACING_MAP_ELT(map->elts, i)));",
            "\t\t*(TRACING_MAP_ELT(map->elts, i)) = NULL;",
            "\t}",
            "",
            "\ttracing_map_array_free(map->elts);",
            "\tmap->elts = NULL;",
            "}"
          ],
          "function_name": "tracing_map_add_key_field, tracing_map_array_clear, tracing_map_array_free, tracing_map_elt_clear, tracing_map_elt_init_fields, tracing_map_elt_free, tracing_map_free_elts",
          "description": "实现tracing_map_elt_clear清除元素数据，tracing_map_elt_init_fields初始化字段配置，提供元素内存释放逻辑及映射元素数组清理功能",
          "similarity": 0.49049967527389526
        }
      ]
    }
  ]
}