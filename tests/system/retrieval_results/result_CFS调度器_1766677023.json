{
  "query": "CFS调度器",
  "timestamp": "2025-12-25 23:37:03",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/fair.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:09:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\fair.c`\n\n---\n\n# `sched/fair.c` 技术文档\n\n## 1. 文件概述\n\n`sched/fair.c` 是 Linux 内核中 **完全公平调度器**（Completely Fair Scheduler, CFS）的核心实现文件，负责实现 `SCHED_NORMAL` 和 `SCHED_BATCH` 调度策略。CFS 旨在通过红黑树（RB-tree）维护可运行任务的虚拟运行时间（vruntime），以实现 CPU 时间的公平分配。该文件实现了任务调度、负载跟踪、时间片计算、组调度（group scheduling）、NUMA 负载均衡、带宽控制等关键机制，是 Linux 通用调度子系统的核心组成部分。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_entity`：调度实体，代表一个可调度单元（任务或任务组）\n- `struct cfs_rq`：CFS 运行队列，管理一组调度实体\n- `struct load_weight`：负载权重结构，用于计算任务对系统负载的贡献\n\n### 关键函数与宏\n- `__calc_delta()` / `calc_delta_fair()`：计算基于权重的调度时间增量\n- `update_load_add()` / `update_load_sub()` / `update_load_set()`：更新负载权重\n- `__update_inv_weight()`：预计算权重的倒数以优化除法运算\n- `get_update_sysctl_factor()`：根据在线 CPU 数量动态调整调度参数\n- `update_sysctl()` / `sched_init_granularity()`：初始化和更新调度粒度参数\n- `for_each_sched_entity()`：遍历调度实体层级结构（用于组调度）\n\n### 可调参数（sysctl）\n- `sysctl_sched_base_slice`：基础时间片（默认 700,000 纳秒）\n- `sysctl_sched_tunable_scaling`：调度参数缩放策略（NONE/LOG/LINEAR）\n- `sysctl_sched_migration_cost`：任务迁移成本阈值（500 微秒）\n- `sysctl_sched_cfs_bandwidth_slice_us`（CFS 带宽控制切片，默认 5 毫秒）\n- `sysctl_numa_balancing_promote_rate_limit_MBps`（NUMA 页迁移速率限制）\n\n## 3. 关键实现\n\n### 虚拟时间与公平性\nCFS 使用 **虚拟运行时间**（vruntime）衡量任务已使用的 CPU 时间，并通过 `calc_delta_fair()` 将实际执行时间按任务权重归一化。权重由任务的 nice 值决定（`NICE_0_LOAD = 1024` 为基准）。调度器总是选择 vruntime 最小的任务运行，确保高优先级（高权重）任务获得更多 CPU 时间。\n\n### 高效除法优化\n为避免频繁除法运算，CFS 预计算 `inv_weight = WMULT_CONST / weight`（`WMULT_CONST = ~0U`），将除法转换为乘法和右移操作（`mul_u64_u32_shr`）。`__calc_delta()` 通过动态调整移位位数（`shift`）保证计算精度，适用于 32/64 位架构。\n\n### 动态粒度调整\n基础时间片 `sched_base_slice` 根据在线 CPU 数量动态缩放：\n- `SCHED_TUNABLESCALING_NONE`：固定值\n- `SCHED_TUNABLESCALING_LINEAR`：线性缩放（×ncpus）\n- `SCHED_TUNABLESCALING_LOG`（默认）：对数缩放（×(1 + ilog2(ncpus))）  \n此设计确保在多核系统中保持合理的调度延迟和交互性。\n\n### 组调度支持\n通过 `for_each_sched_entity()` 宏遍历任务所属的调度实体层级（任务 → 任务组 → 父任务组），实现 CPU 带宽在任务组间的公平分配。每个 `cfs_rq` 独立维护其子实体的红黑树。\n\n### SMP 相关优化\n- **非对称 CPU 优先级**：`arch_asym_cpu_priority()` 允许架构定义 CPU 能力差异（如大小核）\n- **容量比较宏**：`fits_capacity()`（20% 容差）和 `capacity_greater()`（5% 容差）用于负载均衡决策\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- 调度核心：`\"sched.h\"`、`\"stats.h\"`、`\"autogroup.h\"`\n- 系统服务：`<linux/sched/clock.h>`、`<linux/sched/nohz.h>`、`<linux/psi.h>`\n- 内存管理：`<linux/mem_policy.h>`、`<linux/energy_model.h>`\n- SMP 支持：`<linux/topology.h>`、`<linux/cpumask_api.h>`\n- 数据结构：`<linux/rbtree_augmented.h>`\n\n### 条件编译特性\n- `CONFIG_SMP`：多处理器调度优化\n- `CONFIG_CFS_BANDWIDTH`：CPU 带宽限制（cgroup v1/v2）\n- `CONFIG_NUMA_BALANCING`：NUMA 自动迁移\n- `CONFIG_FAIR_GROUP_SCHED`：CFS 组调度（cgroup 支持）\n\n## 5. 使用场景\n\n- **通用任务调度**：所有使用 `SCHED_NORMAL` 或 `SCHED_BATCH` 策略的用户态进程\n- **cgroup CPU 资源控制**：通过 `cpu.cfs_quota_us` 和 `cpu.cfs_period_us` 限制任务组带宽\n- **NUMA 优化**：自动迁移内存页以减少远程访问（`numa_balancing`）\n- **节能调度**：结合 `energy_model` 在满足性能前提下选择低功耗 CPU\n- **实时性保障**：通过 `cond_resched()` 在长循环中主动让出 CPU，避免内核抢占延迟过高\n- **系统调优**：管理员通过 `/proc/sys/kernel/` 下的 sysctl 参数动态调整调度行为",
      "similarity": 0.5576063394546509,
      "chunks": [
        {
          "chunk_id": 40,
          "file_path": "kernel/sched/fair.c",
          "start_line": 6542,
          "end_line": 6668,
          "content": [
            "void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b, struct cfs_bandwidth *parent)",
            "{",
            "\traw_spin_lock_init(&cfs_b->lock);",
            "\tcfs_b->runtime = 0;",
            "\tcfs_b->quota = RUNTIME_INF;",
            "\tcfs_b->period = ns_to_ktime(default_cfs_period());",
            "\tcfs_b->burst = 0;",
            "\tcfs_b->hierarchical_quota = parent ? parent->hierarchical_quota : RUNTIME_INF;",
            "",
            "\tINIT_LIST_HEAD(&cfs_b->throttled_cfs_rq);",
            "\thrtimer_init(&cfs_b->period_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);",
            "\tcfs_b->period_timer.function = sched_cfs_period_timer;",
            "",
            "\t/* Add a random offset so that timers interleave */",
            "\thrtimer_set_expires(&cfs_b->period_timer,",
            "\t\t\t    get_random_u32_below(cfs_b->period));",
            "\thrtimer_init(&cfs_b->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);",
            "\tcfs_b->slack_timer.function = sched_cfs_slack_timer;",
            "\tcfs_b->slack_started = false;",
            "}",
            "static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq)",
            "{",
            "\tcfs_rq->runtime_enabled = 0;",
            "\tINIT_LIST_HEAD(&cfs_rq->throttled_list);",
            "\tINIT_LIST_HEAD(&cfs_rq->throttled_csd_list);",
            "}",
            "void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)",
            "{",
            "\tlockdep_assert_held(&cfs_b->lock);",
            "",
            "\tif (cfs_b->period_active)",
            "\t\treturn;",
            "",
            "\tcfs_b->period_active = 1;",
            "\thrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);",
            "\thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);",
            "}",
            "static void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b)",
            "{",
            "\tint __maybe_unused i;",
            "",
            "\t/* init_cfs_bandwidth() was not called */",
            "\tif (!cfs_b->throttled_cfs_rq.next)",
            "\t\treturn;",
            "",
            "\thrtimer_cancel(&cfs_b->period_timer);",
            "\thrtimer_cancel(&cfs_b->slack_timer);",
            "",
            "\t/*",
            "\t * It is possible that we still have some cfs_rq's pending on a CSD",
            "\t * list, though this race is very rare. In order for this to occur, we",
            "\t * must have raced with the last task leaving the group while there",
            "\t * exist throttled cfs_rq(s), and the period_timer must have queued the",
            "\t * CSD item but the remote cpu has not yet processed it. To handle this,",
            "\t * we can simply flush all pending CSD work inline here. We're",
            "\t * guaranteed at this point that no additional cfs_rq of this group can",
            "\t * join a CSD list.",
            "\t */",
            "#ifdef CONFIG_SMP",
            "\tfor_each_possible_cpu(i) {",
            "\t\tstruct rq *rq = cpu_rq(i);",
            "\t\tunsigned long flags;",
            "",
            "\t\tif (list_empty(&rq->cfsb_csd_list))",
            "\t\t\tcontinue;",
            "",
            "\t\tlocal_irq_save(flags);",
            "\t\t__cfsb_csd_unthrottle(rq);",
            "\t\tlocal_irq_restore(flags);",
            "\t}",
            "#endif",
            "}",
            "static void __maybe_unused update_runtime_enabled(struct rq *rq)",
            "{",
            "\tstruct task_group *tg;",
            "",
            "\tlockdep_assert_rq_held(rq);",
            "",
            "\trcu_read_lock();",
            "\tlist_for_each_entry_rcu(tg, &task_groups, list) {",
            "\t\tstruct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;",
            "\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];",
            "",
            "\t\traw_spin_lock(&cfs_b->lock);",
            "\t\tcfs_rq->runtime_enabled = cfs_b->quota != RUNTIME_INF;",
            "\t\traw_spin_unlock(&cfs_b->lock);",
            "\t}",
            "\trcu_read_unlock();",
            "}",
            "static void __maybe_unused unthrottle_offline_cfs_rqs(struct rq *rq)",
            "{",
            "\tstruct task_group *tg;",
            "",
            "\tlockdep_assert_rq_held(rq);",
            "",
            "\t/*",
            "\t * The rq clock has already been updated in the",
            "\t * set_rq_offline(), so we should skip updating",
            "\t * the rq clock again in unthrottle_cfs_rq().",
            "\t */",
            "\trq_clock_start_loop_update(rq);",
            "",
            "\trcu_read_lock();",
            "\tlist_for_each_entry_rcu(tg, &task_groups, list) {",
            "\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];",
            "",
            "\t\tif (!cfs_rq->runtime_enabled)",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * clock_task is not advancing so we just need to make sure",
            "\t\t * there's some valid quota amount",
            "\t\t */",
            "\t\tcfs_rq->runtime_remaining = 1;",
            "\t\t/*",
            "\t\t * Offline rq is schedulable till CPU is completely disabled",
            "\t\t * in take_cpu_down(), so we prevent new cfs throttling here.",
            "\t\t */",
            "\t\tcfs_rq->runtime_enabled = 0;",
            "",
            "\t\tif (cfs_rq_throttled(cfs_rq))",
            "\t\t\tunthrottle_cfs_rq(cfs_rq);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\trq_clock_stop_loop_update(rq);",
            "}"
          ],
          "function_name": "init_cfs_bandwidth, init_cfs_rq_runtime, start_cfs_bandwidth, destroy_cfs_bandwidth, update_runtime_enabled, unthrottle_offline_cfs_rqs",
          "description": "初始化CFS带宽结构体，配置锁、周期定时器及松弛定时器，设置默认周期和配额，用于调度器的带宽控制。",
          "similarity": 0.6595032215118408
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/fair.c",
          "start_line": 1,
          "end_line": 82,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Completely Fair Scheduling (CFS) Class (SCHED_NORMAL/SCHED_BATCH)",
            " *",
            " *  Copyright (C) 2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>",
            " *",
            " *  Interactivity improvements by Mike Galbraith",
            " *  (C) 2007 Mike Galbraith <efault@gmx.de>",
            " *",
            " *  Various enhancements by Dmitry Adamushko.",
            " *  (C) 2007 Dmitry Adamushko <dmitry.adamushko@gmail.com>",
            " *",
            " *  Group scheduling enhancements by Srivatsa Vaddagiri",
            " *  Copyright IBM Corporation, 2007",
            " *  Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>",
            " *",
            " *  Scaled math optimizations by Thomas Gleixner",
            " *  Copyright (C) 2007, Thomas Gleixner <tglx@linutronix.de>",
            " *",
            " *  Adaptive scheduling granularity, math enhancements by Peter Zijlstra",
            " *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra",
            " */",
            "#include <linux/energy_model.h>",
            "#include <linux/mmap_lock.h>",
            "#include <linux/hugetlb_inline.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/mm_api.h>",
            "#include <linux/highmem.h>",
            "#include <linux/spinlock_api.h>",
            "#include <linux/cpumask_api.h>",
            "#include <linux/lockdep_api.h>",
            "#include <linux/softirq.h>",
            "#include <linux/refcount_api.h>",
            "#include <linux/topology.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/cond_resched.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/nohz.h>",
            "",
            "#include <linux/cpuidle.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/memory-tiers.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/mutex_api.h>",
            "#include <linux/profile.h>",
            "#include <linux/psi.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/task_work.h>",
            "#include <linux/rbtree_augmented.h>",
            "",
            "#include <asm/switch_to.h>",
            "",
            "#include <linux/sched/cond_resched.h>",
            "",
            "#include \"sched.h\"",
            "#include \"stats.h\"",
            "#include \"autogroup.h\"",
            "",
            "/*",
            " * The initial- and re-scaling of tunables is configurable",
            " *",
            " * Options are:",
            " *",
            " *   SCHED_TUNABLESCALING_NONE - unscaled, always *1",
            " *   SCHED_TUNABLESCALING_LOG - scaled logarithmical, *1+ilog(ncpus)",
            " *   SCHED_TUNABLESCALING_LINEAR - scaled linear, *ncpus",
            " *",
            " * (default SCHED_TUNABLESCALING_LOG = *(1+ilog(ncpus))",
            " */",
            "unsigned int sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;",
            "",
            "/*",
            " * Minimal preemption granularity for CPU-bound tasks:",
            " *",
            " * (default: 0.70 msec * (1 + ilog(ncpus)), units: nanoseconds)",
            " */",
            "unsigned int sysctl_sched_base_slice\t\t\t= 700000ULL;",
            "static unsigned int normalized_sysctl_sched_base_slice\t= 700000ULL;",
            "",
            "const_debug unsigned int sysctl_sched_migration_cost\t= 500000UL;",
            ""
          ],
          "function_name": null,
          "description": "定义CFS调度器的可调参数及默认值，用于调整调度行为，包括时间片缩放方式（线性/对数）和基础时间片大小。",
          "similarity": 0.6584951877593994
        },
        {
          "chunk_id": 41,
          "file_path": "kernel/sched/fair.c",
          "start_line": 6684,
          "end_line": 6785,
          "content": [
            "bool cfs_task_bw_constrained(struct task_struct *p)",
            "{",
            "\tstruct cfs_rq *cfs_rq = task_cfs_rq(p);",
            "",
            "\tif (!cfs_bandwidth_used())",
            "\t\treturn false;",
            "",
            "\tif (cfs_rq->runtime_enabled ||",
            "\t    tg_cfs_bandwidth(cfs_rq->tg)->hierarchical_quota != RUNTIME_INF)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "static void sched_fair_update_stop_tick(struct rq *rq, struct task_struct *p)",
            "{",
            "\tint cpu = cpu_of(rq);",
            "",
            "\tif (!cfs_bandwidth_used())",
            "\t\treturn;",
            "",
            "\tif (!tick_nohz_full_cpu(cpu))",
            "\t\treturn;",
            "",
            "\tif (rq->nr_running != 1)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t *  We know there is only one task runnable and we've just picked it. The",
            "\t *  normal enqueue path will have cleared TICK_DEP_BIT_SCHED if we will",
            "\t *  be otherwise able to stop the tick. Just need to check if we are using",
            "\t *  bandwidth control.",
            "\t */",
            "\tif (cfs_task_bw_constrained(p))",
            "\t\ttick_nohz_dep_set_cpu(cpu, TICK_DEP_BIT_SCHED);",
            "}",
            "static inline bool cfs_bandwidth_used(void)",
            "{",
            "\treturn false;",
            "}",
            "static void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) { return false; }",
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq) {}",
            "static inline void sync_throttle(struct task_group *tg, int cpu) {}",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}",
            "static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)",
            "{",
            "\treturn 0;",
            "}",
            "static inline int throttled_hierarchy(struct cfs_rq *cfs_rq)",
            "{",
            "\treturn 0;",
            "}",
            "static inline int throttled_lb_pair(struct task_group *tg,",
            "\t\t\t\t    int src_cpu, int dest_cpu)",
            "{",
            "\treturn 0;",
            "}",
            "void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b, struct cfs_bandwidth *parent) {}",
            "static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}",
            "static inline void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}",
            "static inline void update_runtime_enabled(struct rq *rq) {}",
            "static inline void unthrottle_offline_cfs_rqs(struct rq *rq) {}",
            "bool cfs_task_bw_constrained(struct task_struct *p)",
            "{",
            "\treturn false;",
            "}",
            "static inline void sched_fair_update_stop_tick(struct rq *rq, struct task_struct *p) {}",
            "static void hrtick_start_fair(struct rq *rq, struct task_struct *p)",
            "{",
            "\tstruct sched_entity *se = &p->se;",
            "",
            "\tSCHED_WARN_ON(task_rq(p) != rq);",
            "",
            "\tif (rq->cfs.h_nr_running > 1) {",
            "\t\tu64 ran = se->sum_exec_runtime - se->prev_sum_exec_runtime;",
            "\t\tu64 slice = se->slice;",
            "\t\ts64 delta = slice - ran;",
            "",
            "\t\tif (delta < 0) {",
            "\t\t\tif (task_current(rq, p))",
            "\t\t\t\tresched_curr(rq);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\thrtick_start(rq, delta);",
            "\t}",
            "}",
            "static void hrtick_update(struct rq *rq)",
            "{",
            "\tstruct task_struct *curr = rq->curr;",
            "",
            "\tif (!hrtick_enabled_fair(rq) || curr->sched_class != &fair_sched_class)",
            "\t\treturn;",
            "",
            "\thrtick_start_fair(rq, curr);",
            "}",
            "static inline void",
            "hrtick_start_fair(struct rq *rq, struct task_struct *p)",
            "{",
            "}",
            "static inline void hrtick_update(struct rq *rq)",
            "{",
            "}"
          ],
          "function_name": "cfs_task_bw_constrained, sched_fair_update_stop_tick, cfs_bandwidth_used, account_cfs_rq_runtime, check_cfs_rq_runtime, check_enqueue_throttle, sync_throttle, return_cfs_rq_runtime, cfs_rq_throttled, throttled_hierarchy, throttled_lb_pair, init_cfs_bandwidth, init_cfs_rq_runtime, destroy_cfs_bandwidth, update_runtime_enabled, unthrottle_offline_cfs_rqs, cfs_task_bw_constrained, sched_fair_update_stop_tick, hrtick_start_fair, hrtick_update, hrtick_start_fair, hrtick_update",
          "description": "提供CFS带宽约束检查及相关调度器回调钩子，大部分函数为空实现或条件编译，用于调度器的Tick管理、公平调度和抢占控制。",
          "similarity": 0.648747980594635
        },
        {
          "chunk_id": 74,
          "file_path": "kernel/sched/fair.c",
          "start_line": 12990,
          "end_line": 13107,
          "content": [
            "static void propagate_entity_cfs_rq(struct sched_entity *se)",
            "{",
            "\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);",
            "",
            "\tif (cfs_rq_throttled(cfs_rq))",
            "\t\treturn;",
            "",
            "\tif (!throttled_hierarchy(cfs_rq))",
            "\t\tlist_add_leaf_cfs_rq(cfs_rq);",
            "",
            "\t/* Start to propagate at parent */",
            "\tse = se->parent;",
            "",
            "\tfor_each_sched_entity(se) {",
            "\t\tcfs_rq = cfs_rq_of(se);",
            "",
            "\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);",
            "",
            "\t\tif (cfs_rq_throttled(cfs_rq))",
            "\t\t\tbreak;",
            "",
            "\t\tif (!throttled_hierarchy(cfs_rq))",
            "\t\t\tlist_add_leaf_cfs_rq(cfs_rq);",
            "\t}",
            "}",
            "static void propagate_entity_cfs_rq(struct sched_entity *se) { }",
            "static void detach_entity_cfs_rq(struct sched_entity *se)",
            "{",
            "\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);",
            "",
            "#ifdef CONFIG_SMP",
            "\t/*",
            "\t * In case the task sched_avg hasn't been attached:",
            "\t * - A forked task which hasn't been woken up by wake_up_new_task().",
            "\t * - A task which has been woken up by try_to_wake_up() but is",
            "\t *   waiting for actually being woken up by sched_ttwu_pending().",
            "\t */",
            "\tif (!se->avg.last_update_time)",
            "\t\treturn;",
            "#endif",
            "",
            "\t/* Catch up with the cfs_rq and remove our load when we leave */",
            "\tupdate_load_avg(cfs_rq, se, 0);",
            "\tdetach_entity_load_avg(cfs_rq, se);",
            "\tupdate_tg_load_avg(cfs_rq);",
            "\tpropagate_entity_cfs_rq(se);",
            "}",
            "static void attach_entity_cfs_rq(struct sched_entity *se)",
            "{",
            "\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);",
            "",
            "\t/* Synchronize entity with its cfs_rq */",
            "\tupdate_load_avg(cfs_rq, se, sched_feat(ATTACH_AGE_LOAD) ? 0 : SKIP_AGE_LOAD);",
            "\tattach_entity_load_avg(cfs_rq, se);",
            "\tupdate_tg_load_avg(cfs_rq);",
            "\tpropagate_entity_cfs_rq(se);",
            "}",
            "static void detach_task_cfs_rq(struct task_struct *p)",
            "{",
            "\tstruct sched_entity *se = &p->se;",
            "",
            "\tdetach_entity_cfs_rq(se);",
            "}",
            "static void attach_task_cfs_rq(struct task_struct *p)",
            "{",
            "\tstruct sched_entity *se = &p->se;",
            "",
            "\tattach_entity_cfs_rq(se);",
            "}",
            "static void switched_from_fair(struct rq *rq, struct task_struct *p)",
            "{",
            "\tdetach_task_cfs_rq(p);",
            "}",
            "static void switched_to_fair(struct rq *rq, struct task_struct *p)",
            "{",
            "\tSCHED_WARN_ON(p->se.sched_delayed);",
            "",
            "\tattach_task_cfs_rq(p);",
            "",
            "\tset_task_max_allowed_capacity(p);",
            "",
            "\tif (task_on_rq_queued(p)) {",
            "\t\t/*",
            "\t\t * We were most likely switched from sched_rt, so",
            "\t\t * kick off the schedule if running, otherwise just see",
            "\t\t * if we can still preempt the current task.",
            "\t\t */",
            "\t\tif (task_current(rq, p))",
            "\t\t\tresched_curr(rq);",
            "\t\telse",
            "\t\t\twakeup_preempt(rq, p, 0);",
            "\t}",
            "}",
            "static void __set_next_task_fair(struct rq *rq, struct task_struct *p, bool first)",
            "{",
            "\tstruct sched_entity *se = &p->se;",
            "",
            "#ifdef CONFIG_SMP",
            "\tif (task_on_rq_queued(p)) {",
            "\t\t/*",
            "\t\t * Move the next running task to the front of the list, so our",
            "\t\t * cfs_tasks list becomes MRU one.",
            "\t\t */",
            "\t\tlist_move(&se->group_node, &rq->cfs_tasks);",
            "\t}",
            "#endif",
            "",
            "\tif (!first)",
            "\t\treturn;",
            "",
            "\tSCHED_WARN_ON(se->sched_delayed);",
            "",
            "\tif (hrtick_enabled_fair(rq))",
            "\t\thrtick_start_fair(rq, p);",
            "",
            "\tupdate_misfit_status(p, rq);",
            "\tsched_fair_update_stop_tick(rq, p);",
            "}"
          ],
          "function_name": "propagate_entity_cfs_rq, propagate_entity_cfs_rq, detach_entity_cfs_rq, attach_entity_cfs_rq, detach_task_cfs_rq, attach_task_cfs_rq, switched_from_fair, switched_to_fair, __set_next_task_fair",
          "description": "管理调度实体与CFS运行队列的绑定关系，处理任务切换调度类时的负载数据同步，维护运行队列层级结构，支持动态调整任务的负载统计和调度决策。",
          "similarity": 0.6472920775413513
        },
        {
          "chunk_id": 57,
          "file_path": "kernel/sched/fair.c",
          "start_line": 9686,
          "end_line": 9804,
          "content": [
            "static void update_cfs_rq_h_load(struct cfs_rq *cfs_rq)",
            "{",
            "\tstruct rq *rq = rq_of(cfs_rq);",
            "\tstruct sched_entity *se = cfs_rq->tg->se[cpu_of(rq)];",
            "\tunsigned long now = jiffies;",
            "\tunsigned long load;",
            "",
            "\tif (cfs_rq->last_h_load_update == now)",
            "\t\treturn;",
            "",
            "\tWRITE_ONCE(cfs_rq->h_load_next, NULL);",
            "\tfor_each_sched_entity(se) {",
            "\t\tcfs_rq = cfs_rq_of(se);",
            "\t\tWRITE_ONCE(cfs_rq->h_load_next, se);",
            "\t\tif (cfs_rq->last_h_load_update == now)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tif (!se) {",
            "\t\tcfs_rq->h_load = cfs_rq_load_avg(cfs_rq);",
            "\t\tcfs_rq->last_h_load_update = now;",
            "\t}",
            "",
            "\twhile ((se = READ_ONCE(cfs_rq->h_load_next)) != NULL) {",
            "\t\tload = cfs_rq->h_load;",
            "\t\tload = div64_ul(load * se->avg.load_avg,",
            "\t\t\tcfs_rq_load_avg(cfs_rq) + 1);",
            "\t\tcfs_rq = group_cfs_rq(se);",
            "\t\tcfs_rq->h_load = load;",
            "\t\tcfs_rq->last_h_load_update = now;",
            "\t}",
            "}",
            "static unsigned long task_h_load(struct task_struct *p)",
            "{",
            "\tstruct cfs_rq *cfs_rq = task_cfs_rq(p);",
            "",
            "\tupdate_cfs_rq_h_load(cfs_rq);",
            "\treturn div64_ul(p->se.avg.load_avg * cfs_rq->h_load,",
            "\t\t\tcfs_rq_load_avg(cfs_rq) + 1);",
            "}",
            "static bool __update_blocked_fair(struct rq *rq, bool *done)",
            "{",
            "\tstruct cfs_rq *cfs_rq = &rq->cfs;",
            "\tbool decayed;",
            "",
            "\tdecayed = update_cfs_rq_load_avg(cfs_rq_clock_pelt(cfs_rq), cfs_rq);",
            "\tif (cfs_rq_has_blocked(cfs_rq))",
            "\t\t*done = false;",
            "",
            "\treturn decayed;",
            "}",
            "static unsigned long task_h_load(struct task_struct *p)",
            "{",
            "\treturn p->se.avg.load_avg;",
            "}",
            "static void sched_balance_update_blocked_averages(int cpu)",
            "{",
            "\tbool decayed = false, done = true;",
            "\tstruct rq *rq = cpu_rq(cpu);",
            "\tstruct rq_flags rf;",
            "",
            "\trq_lock_irqsave(rq, &rf);",
            "\tupdate_blocked_load_tick(rq);",
            "\tupdate_rq_clock(rq);",
            "",
            "\tdecayed |= __update_blocked_others(rq, &done);",
            "\tdecayed |= __update_blocked_fair(rq, &done);",
            "",
            "\tupdate_blocked_load_status(rq, !done);",
            "\tif (decayed)",
            "\t\tcpufreq_update_util(rq, 0);",
            "\trq_unlock_irqrestore(rq, &rf);",
            "}",
            "static inline void init_sd_lb_stats(struct sd_lb_stats *sds)",
            "{",
            "\t/*",
            "\t * Skimp on the clearing to avoid duplicate work. We can avoid clearing",
            "\t * local_stat because update_sg_lb_stats() does a full clear/assignment.",
            "\t * We must however set busiest_stat::group_type and",
            "\t * busiest_stat::idle_cpus to the worst busiest group because",
            "\t * update_sd_pick_busiest() reads these before assignment.",
            "\t */",
            "\t*sds = (struct sd_lb_stats){",
            "\t\t.busiest = NULL,",
            "\t\t.local = NULL,",
            "\t\t.total_load = 0UL,",
            "\t\t.total_capacity = 0UL,",
            "\t\t.busiest_stat = {",
            "\t\t\t.idle_cpus = UINT_MAX,",
            "\t\t\t.group_type = group_has_spare,",
            "\t\t},",
            "\t};",
            "}",
            "static unsigned long scale_rt_capacity(int cpu)",
            "{",
            "\tunsigned long max = get_actual_cpu_capacity(cpu);",
            "\tstruct rq *rq = cpu_rq(cpu);",
            "\tunsigned long used, free;",
            "\tunsigned long irq;",
            "",
            "\tirq = cpu_util_irq(rq);",
            "",
            "\tif (unlikely(irq >= max))",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * avg_rt.util_avg and avg_dl.util_avg track binary signals",
            "\t * (running and not running) with weights 0 and 1024 respectively.",
            "\t */",
            "\tused = cpu_util_rt(rq);",
            "\tused += cpu_util_dl(rq);",
            "",
            "\tif (unlikely(used >= max))",
            "\t\treturn 1;",
            "",
            "\tfree = max - used;",
            "",
            "\treturn scale_irq_capacity(free, irq, max);",
            "}"
          ],
          "function_name": "update_cfs_rq_h_load, task_h_load, __update_blocked_fair, task_h_load, sched_balance_update_blocked_averages, init_sd_lb_stats, scale_rt_capacity",
          "description": "该代码块实现基于CFS调度器的负载计算逻辑。update_cfs_rq_h_load维护各层级调度组的虚拟负载值，task_h_load计算单个任务对当前调度组的实际贡献。__update_blocked_fair负责更新CFS调度器的阻塞态负载统计，sched_balance_update_blocked_averages触发周期性负载更新。",
          "similarity": 0.5894864797592163
        }
      ]
    },
    {
      "source_file": "kernel/sched/ext.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:08:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\ext.c`\n\n---\n\n# `sched/ext.c` 技术文档\n\n## 文件概述\n\n`sched/ext.c` 是 Linux 内核中 **BPF 可扩展调度器（sched_ext）** 的核心实现文件之一，定义了调度器与 BPF 程序交互所需的数据结构、常量和操作接口。该文件为用户空间通过 BPF 实现自定义调度策略提供了内核侧的框架支持，允许将任务调度逻辑完全委托给加载的 BPF 程序，同时保留与内核调度子系统的安全集成。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`struct sched_ext_ops`**  \n  BPF 调度器的操作函数表，包含调度器必须或可选实现的回调函数，如 `select_cpu`、`enqueue`、`dequeue`、`dispatch` 等，用于控制任务的 CPU 选择、入队、出队和分发逻辑。\n\n- **`struct scx_exit_info`**  \n  描述 BPF 调度器退出原因的结构体，包含退出类型（`kind`）、退出码（`exit_code`）、错误信息（`reason`、`msg`）、回溯栈（`bt`）和调试转储（`dump`）。\n\n- **`struct scx_init_task_args` / `scx_exit_task_args`**  \n  分别用于 `ops.init_task()` 和 `ops.exit_task()` 回调的参数容器，传递任务初始化/退出上下文（如是否由 fork 触发、所属 cgroup 等）。\n\n- **`struct scx_cpu_acquire_args` / `scx_cpu_release_args`**  \n  用于 CPU 获取/释放回调的参数结构，其中 `cpu_release` 包含抢占原因（如 RT/DL 任务抢占）和即将运行的任务。\n\n- **`struct scx_dump_ctx`**  \n  为调度器转储（dump）操作提供上下文信息，包括退出类型、时间戳等。\n\n### 关键枚举与常量\n\n- **`enum scx_exit_kind`**  \n  定义调度器退出的类别，如正常退出（`SCX_EXIT_DONE`）、用户/BPF/内核主动注销（`SCX_EXIT_UNREG*`）、系统请求（`SCX_EXIT_SYSRQ`）或运行时错误（`SCX_EXIT_ERROR*`）。\n\n- **`enum scx_exit_code`**  \n  定义 64 位退出码的位域格式，支持系统原因（如 `SCX_ECODE_RSN_HOTPLUG`）和系统动作（如 `SCX_ECODE_ACT_RESTART`），允许用户自定义退出上下文。\n\n- **`enum scx_ops_flags`**  \n  调度器操作标志，控制调度行为：\n  - `SCX_OPS_KEEP_BUILTIN_IDLE`：保留内建空闲跟踪\n  - `SCX_OPS_ENQ_LAST`：切片到期后仍无任务时重新入队\n  - `SCX_OPS_ENQ_EXITING`：由 BPF 处理退出中任务\n  - `SCX_OPS_SWITCH_PARTIAL`：仅调度 `SCHED_EXT` 策略任务\n  - `SCX_OPS_HAS_CGROUP_WEIGHT`：支持 cgroup cpu.weight\n\n- **调度器常量**  \n  如 `SCX_DSP_DFL_MAX_BATCH`（默认分发批大小）、`SCX_WATCHDOG_MAX_TIMEOUT`（看门狗超时）、`SCX_OPS_TASK_ITER_BATCH`（任务迭代锁释放批次）等，用于控制调度器内部行为。\n\n## 关键实现\n\n- **BPF 调度器生命周期管理**  \n  通过 `scx_exit_info` 和退出码机制，支持多种退出路径（用户、BPF、内核、SysRq、错误），并提供详细的诊断信息（回溯、消息、转储）。\n\n- **任务入队优化**  \n  在 `select_cpu` 中允许直接插入 DSQ（如本地 DSQ），跳过后续 `enqueue` 调用，减少调度开销；同时通过 `SCX_OPS_ENQ_EXITING` 标志处理退出中任务的调度问题，避免 RCU 停顿。\n\n- **CPU 抢占通知**  \n  通过 `scx_cpu_release_args` 向 BPF 调度器传递 CPU 被高优先级调度类（RT/DL/Stop）抢占的原因，便于调度器做出相应调整。\n\n- **cgroup 集成**  \n  支持 cgroup 调度（`CONFIG_EXT_GROUP_SCHED`），在任务加入 cgroup 时传递权重信息（`scx_cgroup_init_args`），并通过 `SCX_OPS_HAS_CGROUP_WEIGHT` 标志启用。\n\n- **安全与鲁棒性**  \n  内核侧跟踪 BPF 是否拥有任务，可忽略无效分发；任务迭代时定期释放锁（`SCX_OPS_TASK_ITER_BATCH`），防止 RCU/CSD 停顿；看门狗机制（`SCX_WATCHDOG_MAX_TIMEOUT`）检测任务卡死。\n\n## 依赖关系\n\n- **BPF 子系统**：通过 `#include <linux/bpf.h>` 依赖 BPF 基础设施，用于加载和验证调度器 BPF 程序。\n- **调度核心**：与 `kernel/sched/` 下的核心调度代码（如 `core.c`、`rt.c`、`dl.c`）交互，处理任务入队、CPU 选择和抢占。\n- **cgroup 子系统**：当启用 `CONFIG_EXT_GROUP_SCHED` 时，依赖 cgroup CPU 控制器获取任务权重和层级信息。\n- **RCU 与锁机制**：使用 `scx_tasks_lock` 保护任务迭代，需与 RCU 同步机制协调。\n\n## 使用场景\n\n- **自定义调度策略开发**：用户通过 BPF 实现特定工作负载的调度逻辑（如延迟敏感型、批处理优化、NUMA 感知等），并注册到 `sched_ext`。\n- **系统调试与监控**：利用 `ops.dump()` 和退出信息结构体，在调度器异常退出时收集诊断数据。\n- **混合调度部署**：通过 `SCX_OPS_SWITCH_PARTIAL` 标志，仅对部分任务（`SCHED_EXT`）启用 BPF 调度，其余任务仍由 CFS 处理。\n- **资源隔离与 QoS**：结合 cgroup 支持，为不同 cgroup 配置不同的调度行为和资源权重。\n- **内核调度实验平台**：作为安全的沙箱环境，测试新型调度算法而无需修改核心调度代码。",
      "similarity": 0.5151042938232422,
      "chunks": [
        {
          "chunk_id": 17,
          "file_path": "kernel/sched/ext.c",
          "start_line": 3633,
          "end_line": 3736,
          "content": [
            "void sched_ext_free(struct task_struct *p)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&scx_tasks_lock, flags);",
            "\tlist_del_init(&p->scx.tasks_node);",
            "\tspin_unlock_irqrestore(&scx_tasks_lock, flags);",
            "",
            "\t/*",
            "\t * @p is off scx_tasks and wholly ours. scx_ops_enable()'s READY ->",
            "\t * ENABLED transitions can't race us. Disable ops for @p.",
            "\t */",
            "\tif (scx_get_task_state(p) != SCX_TASK_NONE) {",
            "\t\tstruct rq_flags rf;",
            "\t\tstruct rq *rq;",
            "",
            "\t\trq = task_rq_lock(p, &rf);",
            "\t\tscx_ops_exit_task(p);",
            "\t\ttask_rq_unlock(rq, p, &rf);",
            "\t}",
            "}",
            "static void reweight_task_scx(struct rq *rq, struct task_struct *p,",
            "\t\t              const struct load_weight *lw)",
            "{",
            "\tlockdep_assert_rq_held(task_rq(p));",
            "",
            "\tp->scx.weight = sched_weight_to_cgroup(scale_load_down(lw->weight));",
            "\tif (SCX_HAS_OP(set_weight))",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, set_weight, p, p->scx.weight);",
            "}",
            "static void prio_changed_scx(struct rq *rq, struct task_struct *p, int oldprio)",
            "{",
            "}",
            "static void switching_to_scx(struct rq *rq, struct task_struct *p)",
            "{",
            "\tscx_ops_enable_task(p);",
            "",
            "\t/*",
            "\t * set_cpus_allowed_scx() is not called while @p is associated with a",
            "\t * different scheduler class. Keep the BPF scheduler up-to-date.",
            "\t */",
            "\tif (SCX_HAS_OP(set_cpumask))",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, set_cpumask, p,",
            "\t\t\t\t (struct cpumask *)p->cpus_ptr);",
            "}",
            "static void switched_from_scx(struct rq *rq, struct task_struct *p)",
            "{",
            "\tscx_ops_disable_task(p);",
            "}",
            "static void wakeup_preempt_scx(struct rq *rq, struct task_struct *p,int wake_flags) {}",
            "static void switched_to_scx(struct rq *rq, struct task_struct *p) {}",
            "int scx_check_setscheduler(struct task_struct *p, int policy)",
            "{",
            "\tlockdep_assert_rq_held(task_rq(p));",
            "",
            "\t/* if disallow, reject transitioning into SCX */",
            "\tif (scx_enabled() && READ_ONCE(p->scx.disallow) &&",
            "\t    p->policy != policy && policy == SCHED_EXT)",
            "\t\treturn -EACCES;",
            "",
            "\treturn 0;",
            "}",
            "bool scx_can_stop_tick(struct rq *rq)",
            "{",
            "\tstruct task_struct *p = rq->curr;",
            "",
            "\tif (scx_rq_bypassing(rq))",
            "\t\treturn false;",
            "",
            "\tif (p->sched_class != &ext_sched_class)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * @rq can dispatch from different DSQs, so we can't tell whether it",
            "\t * needs the tick or not by looking at nr_running. Allow stopping ticks",
            "\t * iff the BPF scheduler indicated so. See set_next_task_scx().",
            "\t */",
            "\treturn rq->scx.flags & SCX_RQ_CAN_STOP_TICK;",
            "}",
            "static void scx_cgroup_warn_missing_weight(struct task_group *tg)",
            "{",
            "\tif (scx_ops_enable_state() == SCX_OPS_DISABLED ||",
            "\t    cgroup_warned_missing_weight)",
            "\t\treturn;",
            "",
            "\tif ((scx_ops.flags & SCX_OPS_HAS_CGROUP_WEIGHT) || !tg->css.parent)",
            "\t\treturn;",
            "",
            "\tpr_warn(\"sched_ext: \\\"%s\\\" does not implement cgroup cpu.weight\\n\",",
            "\t\tscx_ops.name);",
            "\tcgroup_warned_missing_weight = true;",
            "}",
            "static void scx_cgroup_warn_missing_idle(struct task_group *tg)",
            "{",
            "\tif (!scx_cgroup_enabled || cgroup_warned_missing_idle)",
            "\t\treturn;",
            "",
            "\tif (!tg->idle)",
            "\t\treturn;",
            "",
            "\tpr_warn(\"sched_ext: \\\"%s\\\" does not implement cgroup cpu.idle\\n\",",
            "\t\tscx_ops.name);",
            "\tcgroup_warned_missing_idle = true;",
            "}"
          ],
          "function_name": "sched_ext_free, reweight_task_scx, prio_changed_scx, switching_to_scx, switched_from_scx, wakeup_preempt_scx, switched_to_scx, scx_check_setscheduler, scx_can_stop_tick, scx_cgroup_warn_missing_weight, scx_cgroup_warn_missing_idle",
          "description": "实现SCX调度器回调钩子，处理任务权重重置、优先级变更、状态切换及策略检查，包含CGROUP权重和空闲配置缺失警告。",
          "similarity": 0.5262999534606934
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/sched/ext.c",
          "start_line": 1963,
          "end_line": 2070,
          "content": [
            "static bool scx_rq_online(struct rq *rq)",
            "{",
            "\t/*",
            "\t * Test both cpu_active() and %SCX_RQ_ONLINE. %SCX_RQ_ONLINE indicates",
            "\t * the online state as seen from the BPF scheduler. cpu_active() test",
            "\t * guarantees that, if this function returns %true, %SCX_RQ_ONLINE will",
            "\t * stay set until the current scheduling operation is complete even if",
            "\t * we aren't locking @rq.",
            "\t */",
            "\treturn likely((rq->scx.flags & SCX_RQ_ONLINE) && cpu_active(cpu_of(rq)));",
            "}",
            "static void do_enqueue_task(struct rq *rq, struct task_struct *p, u64 enq_flags,",
            "\t\t\t    int sticky_cpu)",
            "{",
            "\tstruct task_struct **ddsp_taskp;",
            "\tunsigned long qseq;",
            "",
            "\tWARN_ON_ONCE(!(p->scx.flags & SCX_TASK_QUEUED));",
            "",
            "\t/* rq migration */",
            "\tif (sticky_cpu == cpu_of(rq))",
            "\t\tgoto local_norefill;",
            "",
            "\t/*",
            "\t * If !scx_rq_online(), we already told the BPF scheduler that the CPU",
            "\t * is offline and are just running the hotplug path. Don't bother the",
            "\t * BPF scheduler.",
            "\t */",
            "\tif (!scx_rq_online(rq))",
            "\t\tgoto local;",
            "",
            "\tif (scx_rq_bypassing(rq))",
            "\t\tgoto global;",
            "",
            "\tif (p->scx.ddsp_dsq_id != SCX_DSQ_INVALID)",
            "\t\tgoto direct;",
            "",
            "\t/* see %SCX_OPS_ENQ_EXITING */",
            "\tif (!static_branch_unlikely(&scx_ops_enq_exiting) &&",
            "\t    unlikely(p->flags & PF_EXITING))",
            "\t\tgoto local;",
            "",
            "\tif (!SCX_HAS_OP(enqueue))",
            "\t\tgoto global;",
            "",
            "\t/* DSQ bypass didn't trigger, enqueue on the BPF scheduler */",
            "\tqseq = rq->scx.ops_qseq++ << SCX_OPSS_QSEQ_SHIFT;",
            "",
            "\tWARN_ON_ONCE(atomic_long_read(&p->scx.ops_state) != SCX_OPSS_NONE);",
            "\tatomic_long_set(&p->scx.ops_state, SCX_OPSS_QUEUEING | qseq);",
            "",
            "\tddsp_taskp = this_cpu_ptr(&direct_dispatch_task);",
            "\tWARN_ON_ONCE(*ddsp_taskp);",
            "\t*ddsp_taskp = p;",
            "",
            "\tSCX_CALL_OP_TASK(SCX_KF_ENQUEUE, enqueue, p, enq_flags);",
            "",
            "\t*ddsp_taskp = NULL;",
            "\tif (p->scx.ddsp_dsq_id != SCX_DSQ_INVALID)",
            "\t\tgoto direct;",
            "",
            "\t/*",
            "\t * If not directly dispatched, QUEUEING isn't clear yet and dispatch or",
            "\t * dequeue may be waiting. The store_release matches their load_acquire.",
            "\t */",
            "\tatomic_long_set_release(&p->scx.ops_state, SCX_OPSS_QUEUED | qseq);",
            "\treturn;",
            "",
            "direct:",
            "\tdirect_dispatch(p, enq_flags);",
            "\treturn;",
            "",
            "local:",
            "\t/*",
            "\t * For task-ordering, slice refill must be treated as implying the end",
            "\t * of the current slice. Otherwise, the longer @p stays on the CPU, the",
            "\t * higher priority it becomes from scx_prio_less()'s POV.",
            "\t */",
            "\ttouch_core_sched(rq, p);",
            "\tp->scx.slice = SCX_SLICE_DFL;",
            "local_norefill:",
            "\tdispatch_enqueue(&rq->scx.local_dsq, p, enq_flags);",
            "\treturn;",
            "",
            "global:",
            "\ttouch_core_sched(rq, p);\t/* see the comment in local: */",
            "\tp->scx.slice = SCX_SLICE_DFL;",
            "\tdispatch_enqueue(find_global_dsq(p), p, enq_flags);",
            "}",
            "static bool task_runnable(const struct task_struct *p)",
            "{",
            "\treturn !list_empty(&p->scx.runnable_node);",
            "}",
            "static void set_task_runnable(struct rq *rq, struct task_struct *p)",
            "{",
            "\tlockdep_assert_rq_held(rq);",
            "",
            "\tif (p->scx.flags & SCX_TASK_RESET_RUNNABLE_AT) {",
            "\t\tp->scx.runnable_at = jiffies;",
            "\t\tp->scx.flags &= ~SCX_TASK_RESET_RUNNABLE_AT;",
            "\t}",
            "",
            "\t/*",
            "\t * list_add_tail() must be used. scx_ops_bypass() depends on tasks being",
            "\t * appened to the runnable_list.",
            "\t */",
            "\tlist_add_tail(&p->scx.runnable_node, &rq->scx.runnable_list);",
            "}"
          ],
          "function_name": "scx_rq_online, do_enqueue_task, task_runnable, set_task_runnable",
          "description": "实现SCX调度器的辅助函数，scx_rq_online检测CPU在线状态，do_enqueue_task根据任务属性决定是否向BPF调度器入队，task_runnable和set_task_runnable维护任务的可运行状态列表",
          "similarity": 0.5240325927734375
        },
        {
          "chunk_id": 32,
          "file_path": "kernel/sched/ext.c",
          "start_line": 6215,
          "end_line": 6316,
          "content": [
            "__bpf_kfunc u32 scx_bpf_dispatch_nr_slots(void)",
            "{",
            "\tif (!scx_kf_allowed(SCX_KF_DISPATCH))",
            "\t\treturn 0;",
            "",
            "\treturn scx_dsp_max_batch - __this_cpu_read(scx_dsp_ctx->cursor);",
            "}",
            "__bpf_kfunc void scx_bpf_dispatch_cancel(void)",
            "{",
            "\tstruct scx_dsp_ctx *dspc = this_cpu_ptr(scx_dsp_ctx);",
            "",
            "\tif (!scx_kf_allowed(SCX_KF_DISPATCH))",
            "\t\treturn;",
            "",
            "\tif (dspc->cursor > 0)",
            "\t\tdspc->cursor--;",
            "\telse",
            "\t\tscx_ops_error(\"dispatch buffer underflow\");",
            "}",
            "__bpf_kfunc bool scx_bpf_dsq_move_to_local(u64 dsq_id)",
            "{",
            "\tstruct scx_dsp_ctx *dspc = this_cpu_ptr(scx_dsp_ctx);",
            "\tstruct scx_dispatch_q *dsq;",
            "",
            "\tif (!scx_kf_allowed(SCX_KF_DISPATCH))",
            "\t\treturn false;",
            "",
            "\tflush_dispatch_buf(dspc->rq);",
            "",
            "\tdsq = find_user_dsq(dsq_id);",
            "\tif (unlikely(!dsq)) {",
            "\t\tscx_ops_error(\"invalid DSQ ID 0x%016llx\", dsq_id);",
            "\t\treturn false;",
            "\t}",
            "",
            "\tif (consume_dispatch_q(dspc->rq, dsq)) {",
            "\t\t/*",
            "\t\t * A successfully consumed task can be dequeued before it starts",
            "\t\t * running while the CPU is trying to migrate other dispatched",
            "\t\t * tasks. Bump nr_tasks to tell balance_scx() to retry on empty",
            "\t\t * local DSQ.",
            "\t\t */",
            "\t\tdspc->nr_tasks++;",
            "\t\treturn true;",
            "\t} else {",
            "\t\treturn false;",
            "\t}",
            "}",
            "__bpf_kfunc bool scx_bpf_consume(u64 dsq_id)",
            "{",
            "\tprintk_deferred_once(KERN_WARNING \"sched_ext: scx_bpf_consume() renamed to scx_bpf_dsq_move_to_local()\");",
            "\treturn scx_bpf_dsq_move_to_local(dsq_id);",
            "}",
            "__bpf_kfunc void scx_bpf_dsq_move_set_slice(struct bpf_iter_scx_dsq *it__iter,",
            "\t\t\t\t\t    u64 slice)",
            "{",
            "\tstruct bpf_iter_scx_dsq_kern *kit = (void *)it__iter;",
            "",
            "\tkit->slice = slice;",
            "\tkit->cursor.flags |= __SCX_DSQ_ITER_HAS_SLICE;",
            "}",
            "__bpf_kfunc void scx_bpf_dispatch_from_dsq_set_slice(",
            "\t\t\tstruct bpf_iter_scx_dsq *it__iter, u64 slice)",
            "{",
            "\tprintk_deferred_once(KERN_WARNING \"sched_ext: scx_bpf_dispatch_from_dsq_set_slice() renamed to scx_bpf_dsq_move_set_slice()\");",
            "\tscx_bpf_dsq_move_set_slice(it__iter, slice);",
            "}",
            "__bpf_kfunc void scx_bpf_dsq_move_set_vtime(struct bpf_iter_scx_dsq *it__iter,",
            "\t\t\t\t\t    u64 vtime)",
            "{",
            "\tstruct bpf_iter_scx_dsq_kern *kit = (void *)it__iter;",
            "",
            "\tkit->vtime = vtime;",
            "\tkit->cursor.flags |= __SCX_DSQ_ITER_HAS_VTIME;",
            "}",
            "__bpf_kfunc void scx_bpf_dispatch_from_dsq_set_vtime(",
            "\t\t\tstruct bpf_iter_scx_dsq *it__iter, u64 vtime)",
            "{",
            "\tprintk_deferred_once(KERN_WARNING \"sched_ext: scx_bpf_dispatch_from_dsq_set_vtime() renamed to scx_bpf_dsq_move_set_vtime()\");",
            "\tscx_bpf_dsq_move_set_vtime(it__iter, vtime);",
            "}",
            "__bpf_kfunc bool scx_bpf_dsq_move(struct bpf_iter_scx_dsq *it__iter,",
            "\t\t\t\t  struct task_struct *p, u64 dsq_id,",
            "\t\t\t\t  u64 enq_flags)",
            "{",
            "\treturn scx_dsq_move((struct bpf_iter_scx_dsq_kern *)it__iter,",
            "\t\t\t    p, dsq_id, enq_flags);",
            "}",
            "__bpf_kfunc bool scx_bpf_dispatch_from_dsq(struct bpf_iter_scx_dsq *it__iter,",
            "\t\t\t\t\t   struct task_struct *p, u64 dsq_id,",
            "\t\t\t\t\t   u64 enq_flags)",
            "{",
            "\tprintk_deferred_once(KERN_WARNING \"sched_ext: scx_bpf_dispatch_from_dsq() renamed to scx_bpf_dsq_move()\");",
            "\treturn scx_bpf_dsq_move(it__iter, p, dsq_id, enq_flags);",
            "}",
            "__bpf_kfunc bool scx_bpf_dsq_move_vtime(struct bpf_iter_scx_dsq *it__iter,",
            "\t\t\t\t\tstruct task_struct *p, u64 dsq_id,",
            "\t\t\t\t\tu64 enq_flags)",
            "{",
            "\treturn scx_dsq_move((struct bpf_iter_scx_dsq_kern *)it__iter,",
            "\t\t\t    p, dsq_id, enq_flags | SCX_ENQ_DSQ_PRIQ);",
            "}"
          ],
          "function_name": "scx_bpf_dispatch_nr_slots, scx_bpf_dispatch_cancel, scx_bpf_dsq_move_to_local, scx_bpf_consume, scx_bpf_dsq_move_set_slice, scx_bpf_dispatch_from_dsq_set_slice, scx_bpf_dsq_move_set_vtime, scx_bpf_dispatch_from_dsq_set_vtime, scx_bpf_dsq_move, scx_bpf_dispatch_from_dsq, scx_bpf_dsq_move_vtime",
          "description": "此代码段实现了调度扩展（SCX）框架中与DSQ（Dispatch Queue）操作相关的BPF辅助函数，核心功能包括任务分发槽位管理、DSQ任务迁移及参数配置。各函数通过权限校验后执行对应操作：`scx_bpf_dispatch_nr_slots`计算剩余分发容量，`scx_bpf_dsq_move_to_local`将任务从远程DSQ迁移到本地并触发负载均衡重试，其他函数用于设置迭代器参数或处理任务入队逻辑。由于缺少部分上下文（如`scx_kf_allowed`、`consume_dispatch_q`等关键实现），部分行为需结合外围代码理解。",
          "similarity": 0.5050647258758545
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/ext.c",
          "start_line": 1,
          "end_line": 1034,
          "content": [
            "/* SPDX-License-Identifier: GPL-2.0 */",
            "/*",
            " * BPF extensible scheduler class: Documentation/scheduler/sched-ext.rst",
            " *",
            " * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.",
            " * Copyright (c) 2022 Tejun Heo <tj@kernel.org>",
            " * Copyright (c) 2022 David Vernet <dvernet@meta.com>",
            " */",
            "#include <linux/bpf.h>",
            "",
            "#define SCX_OP_IDX(op)\t\t(offsetof(struct sched_ext_ops, op) / sizeof(void (*)(void)))",
            "",
            "enum scx_consts {",
            "\tSCX_DSP_DFL_MAX_BATCH\t\t= 32,",
            "\tSCX_DSP_MAX_LOOPS\t\t= 32,",
            "\tSCX_WATCHDOG_MAX_TIMEOUT\t= 30 * HZ,",
            "",
            "\tSCX_EXIT_BT_LEN\t\t\t= 64,",
            "\tSCX_EXIT_MSG_LEN\t\t= 1024,",
            "\tSCX_EXIT_DUMP_DFL_LEN\t\t= 32768,",
            "",
            "\tSCX_CPUPERF_ONE\t\t\t= SCHED_CAPACITY_SCALE,",
            "",
            "\t/*",
            "\t * Iterating all tasks may take a while. Periodically drop",
            "\t * scx_tasks_lock to avoid causing e.g. CSD and RCU stalls.",
            "\t */",
            "\tSCX_OPS_TASK_ITER_BATCH\t\t= 32,",
            "};",
            "",
            "enum scx_exit_kind {",
            "\tSCX_EXIT_NONE,",
            "\tSCX_EXIT_DONE,",
            "",
            "\tSCX_EXIT_UNREG = 64,\t/* user-space initiated unregistration */",
            "\tSCX_EXIT_UNREG_BPF,\t/* BPF-initiated unregistration */",
            "\tSCX_EXIT_UNREG_KERN,\t/* kernel-initiated unregistration */",
            "\tSCX_EXIT_SYSRQ,\t\t/* requested by 'S' sysrq */",
            "",
            "\tSCX_EXIT_ERROR = 1024,\t/* runtime error, error msg contains details */",
            "\tSCX_EXIT_ERROR_BPF,\t/* ERROR but triggered through scx_bpf_error() */",
            "\tSCX_EXIT_ERROR_STALL,\t/* watchdog detected stalled runnable tasks */",
            "};",
            "",
            "/*",
            " * An exit code can be specified when exiting with scx_bpf_exit() or",
            " * scx_ops_exit(), corresponding to exit_kind UNREG_BPF and UNREG_KERN",
            " * respectively. The codes are 64bit of the format:",
            " *",
            " *   Bits: [63  ..  48 47   ..  32 31 .. 0]",
            " *         [ SYS ACT ] [ SYS RSN ] [ USR  ]",
            " *",
            " *   SYS ACT: System-defined exit actions",
            " *   SYS RSN: System-defined exit reasons",
            " *   USR    : User-defined exit codes and reasons",
            " *",
            " * Using the above, users may communicate intention and context by ORing system",
            " * actions and/or system reasons with a user-defined exit code.",
            " */",
            "enum scx_exit_code {",
            "\t/* Reasons */",
            "\tSCX_ECODE_RSN_HOTPLUG\t= 1LLU << 32,",
            "",
            "\t/* Actions */",
            "\tSCX_ECODE_ACT_RESTART\t= 1LLU << 48,",
            "};",
            "",
            "/*",
            " * scx_exit_info is passed to ops.exit() to describe why the BPF scheduler is",
            " * being disabled.",
            " */",
            "struct scx_exit_info {",
            "\t/* %SCX_EXIT_* - broad category of the exit reason */",
            "\tenum scx_exit_kind\tkind;",
            "",
            "\t/* exit code if gracefully exiting */",
            "\ts64\t\t\texit_code;",
            "",
            "\t/* textual representation of the above */",
            "\tconst char\t\t*reason;",
            "",
            "\t/* backtrace if exiting due to an error */",
            "\tunsigned long\t\t*bt;",
            "\tu32\t\t\tbt_len;",
            "",
            "\t/* informational message */",
            "\tchar\t\t\t*msg;",
            "",
            "\t/* debug dump */",
            "\tchar\t\t\t*dump;",
            "};",
            "",
            "/* sched_ext_ops.flags */",
            "enum scx_ops_flags {",
            "\t/*",
            "\t * Keep built-in idle tracking even if ops.update_idle() is implemented.",
            "\t */",
            "\tSCX_OPS_KEEP_BUILTIN_IDLE = 1LLU << 0,",
            "",
            "\t/*",
            "\t * By default, if there are no other task to run on the CPU, ext core",
            "\t * keeps running the current task even after its slice expires. If this",
            "\t * flag is specified, such tasks are passed to ops.enqueue() with",
            "\t * %SCX_ENQ_LAST. See the comment above %SCX_ENQ_LAST for more info.",
            "\t */",
            "\tSCX_OPS_ENQ_LAST\t= 1LLU << 1,",
            "",
            "\t/*",
            "\t * An exiting task may schedule after PF_EXITING is set. In such cases,",
            "\t * bpf_task_from_pid() may not be able to find the task and if the BPF",
            "\t * scheduler depends on pid lookup for dispatching, the task will be",
            "\t * lost leading to various issues including RCU grace period stalls.",
            "\t *",
            "\t * To mask this problem, by default, unhashed tasks are automatically",
            "\t * dispatched to the local DSQ on enqueue. If the BPF scheduler doesn't",
            "\t * depend on pid lookups and wants to handle these tasks directly, the",
            "\t * following flag can be used.",
            "\t */",
            "\tSCX_OPS_ENQ_EXITING\t= 1LLU << 2,",
            "",
            "\t/*",
            "\t * If set, only tasks with policy set to SCHED_EXT are attached to",
            "\t * sched_ext. If clear, SCHED_NORMAL tasks are also included.",
            "\t */",
            "\tSCX_OPS_SWITCH_PARTIAL\t= 1LLU << 3,",
            "",
            "\t/*",
            "\t * CPU cgroup support flags",
            "\t */",
            "\tSCX_OPS_HAS_CGROUP_WEIGHT = 1LLU << 16,\t/* cpu.weight */",
            "",
            "\tSCX_OPS_ALL_FLAGS\t= SCX_OPS_KEEP_BUILTIN_IDLE |",
            "\t\t\t\t  SCX_OPS_ENQ_LAST |",
            "\t\t\t\t  SCX_OPS_ENQ_EXITING |",
            "\t\t\t\t  SCX_OPS_SWITCH_PARTIAL |",
            "\t\t\t\t  SCX_OPS_HAS_CGROUP_WEIGHT,",
            "};",
            "",
            "/* argument container for ops.init_task() */",
            "struct scx_init_task_args {",
            "\t/*",
            "\t * Set if ops.init_task() is being invoked on the fork path, as opposed",
            "\t * to the scheduler transition path.",
            "\t */",
            "\tbool\t\t\tfork;",
            "#ifdef CONFIG_EXT_GROUP_SCHED",
            "\t/* the cgroup the task is joining */",
            "\tstruct cgroup\t\t*cgroup;",
            "#endif",
            "};",
            "",
            "/* argument container for ops.exit_task() */",
            "struct scx_exit_task_args {",
            "\t/* Whether the task exited before running on sched_ext. */",
            "\tbool cancelled;",
            "};",
            "",
            "/* argument container for ops->cgroup_init() */",
            "struct scx_cgroup_init_args {",
            "\t/* the weight of the cgroup [1..10000] */",
            "\tu32\t\t\tweight;",
            "};",
            "",
            "enum scx_cpu_preempt_reason {",
            "\t/* next task is being scheduled by &sched_class_rt */",
            "\tSCX_CPU_PREEMPT_RT,",
            "\t/* next task is being scheduled by &sched_class_dl */",
            "\tSCX_CPU_PREEMPT_DL,",
            "\t/* next task is being scheduled by &sched_class_stop */",
            "\tSCX_CPU_PREEMPT_STOP,",
            "\t/* unknown reason for SCX being preempted */",
            "\tSCX_CPU_PREEMPT_UNKNOWN,",
            "};",
            "",
            "/*",
            " * Argument container for ops->cpu_acquire(). Currently empty, but may be",
            " * expanded in the future.",
            " */",
            "struct scx_cpu_acquire_args {};",
            "",
            "/* argument container for ops->cpu_release() */",
            "struct scx_cpu_release_args {",
            "\t/* the reason the CPU was preempted */",
            "\tenum scx_cpu_preempt_reason reason;",
            "",
            "\t/* the task that's going to be scheduled on the CPU */",
            "\tstruct task_struct\t*task;",
            "};",
            "",
            "/*",
            " * Informational context provided to dump operations.",
            " */",
            "struct scx_dump_ctx {",
            "\tenum scx_exit_kind\tkind;",
            "\ts64\t\t\texit_code;",
            "\tconst char\t\t*reason;",
            "\tu64\t\t\tat_ns;",
            "\tu64\t\t\tat_jiffies;",
            "};",
            "",
            "/**",
            " * struct sched_ext_ops - Operation table for BPF scheduler implementation",
            " *",
            " * Userland can implement an arbitrary scheduling policy by implementing and",
            " * loading operations in this table.",
            " */",
            "struct sched_ext_ops {",
            "\t/**",
            "\t * select_cpu - Pick the target CPU for a task which is being woken up",
            "\t * @p: task being woken up",
            "\t * @prev_cpu: the cpu @p was on before sleeping",
            "\t * @wake_flags: SCX_WAKE_*",
            "\t *",
            "\t * Decision made here isn't final. @p may be moved to any CPU while it",
            "\t * is getting dispatched for execution later. However, as @p is not on",
            "\t * the rq at this point, getting the eventual execution CPU right here",
            "\t * saves a small bit of overhead down the line.",
            "\t *",
            "\t * If an idle CPU is returned, the CPU is kicked and will try to",
            "\t * dispatch. While an explicit custom mechanism can be added,",
            "\t * select_cpu() serves as the default way to wake up idle CPUs.",
            "\t *",
            "\t * @p may be inserted into a DSQ directly by calling",
            "\t * scx_bpf_dsq_insert(). If so, the ops.enqueue() will be skipped.",
            "\t * Directly inserting into %SCX_DSQ_LOCAL will put @p in the local DSQ",
            "\t * of the CPU returned by this operation.",
            "\t */",
            "\ts32 (*select_cpu)(struct task_struct *p, s32 prev_cpu, u64 wake_flags);",
            "",
            "\t/**",
            "\t * enqueue - Enqueue a task on the BPF scheduler",
            "\t * @p: task being enqueued",
            "\t * @enq_flags: %SCX_ENQ_*",
            "\t *",
            "\t * @p is ready to run. Insert directly into a DSQ by calling",
            "\t * scx_bpf_dsq_insert() or enqueue on the BPF scheduler. If not directly",
            "\t * inserted, the bpf scheduler owns @p and if it fails to dispatch @p,",
            "\t * the task will stall.",
            "\t *",
            "\t * If @p was inserted into a DSQ from ops.select_cpu(), this callback is",
            "\t * skipped.",
            "\t */",
            "\tvoid (*enqueue)(struct task_struct *p, u64 enq_flags);",
            "",
            "\t/**",
            "\t * dequeue - Remove a task from the BPF scheduler",
            "\t * @p: task being dequeued",
            "\t * @deq_flags: %SCX_DEQ_*",
            "\t *",
            "\t * Remove @p from the BPF scheduler. This is usually called to isolate",
            "\t * the task while updating its scheduling properties (e.g. priority).",
            "\t *",
            "\t * The ext core keeps track of whether the BPF side owns a given task or",
            "\t * not and can gracefully ignore spurious dispatches from BPF side,",
            "\t * which makes it safe to not implement this method. However, depending",
            "\t * on the scheduling logic, this can lead to confusing behaviors - e.g.",
            "\t * scheduling position not being updated across a priority change.",
            "\t */",
            "\tvoid (*dequeue)(struct task_struct *p, u64 deq_flags);",
            "",
            "\t/**",
            "\t * dispatch - Dispatch tasks from the BPF scheduler and/or user DSQs",
            "\t * @cpu: CPU to dispatch tasks for",
            "\t * @prev: previous task being switched out",
            "\t *",
            "\t * Called when a CPU's local dsq is empty. The operation should dispatch",
            "\t * one or more tasks from the BPF scheduler into the DSQs using",
            "\t * scx_bpf_dsq_insert() and/or move from user DSQs into the local DSQ",
            "\t * using scx_bpf_dsq_move_to_local().",
            "\t *",
            "\t * The maximum number of times scx_bpf_dsq_insert() can be called",
            "\t * without an intervening scx_bpf_dsq_move_to_local() is specified by",
            "\t * ops.dispatch_max_batch. See the comments on top of the two functions",
            "\t * for more details.",
            "\t *",
            "\t * When not %NULL, @prev is an SCX task with its slice depleted. If",
            "\t * @prev is still runnable as indicated by set %SCX_TASK_QUEUED in",
            "\t * @prev->scx.flags, it is not enqueued yet and will be enqueued after",
            "\t * ops.dispatch() returns. To keep executing @prev, return without",
            "\t * dispatching or moving any tasks. Also see %SCX_OPS_ENQ_LAST.",
            "\t */",
            "\tvoid (*dispatch)(s32 cpu, struct task_struct *prev);",
            "",
            "\t/**",
            "\t * tick - Periodic tick",
            "\t * @p: task running currently",
            "\t *",
            "\t * This operation is called every 1/HZ seconds on CPUs which are",
            "\t * executing an SCX task. Setting @p->scx.slice to 0 will trigger an",
            "\t * immediate dispatch cycle on the CPU.",
            "\t */",
            "\tvoid (*tick)(struct task_struct *p);",
            "",
            "\t/**",
            "\t * runnable - A task is becoming runnable on its associated CPU",
            "\t * @p: task becoming runnable",
            "\t * @enq_flags: %SCX_ENQ_*",
            "\t *",
            "\t * This and the following three functions can be used to track a task's",
            "\t * execution state transitions. A task becomes ->runnable() on a CPU,",
            "\t * and then goes through one or more ->running() and ->stopping() pairs",
            "\t * as it runs on the CPU, and eventually becomes ->quiescent() when it's",
            "\t * done running on the CPU.",
            "\t *",
            "\t * @p is becoming runnable on the CPU because it's",
            "\t *",
            "\t * - waking up (%SCX_ENQ_WAKEUP)",
            "\t * - being moved from another CPU",
            "\t * - being restored after temporarily taken off the queue for an",
            "\t *   attribute change.",
            "\t *",
            "\t * This and ->enqueue() are related but not coupled. This operation",
            "\t * notifies @p's state transition and may not be followed by ->enqueue()",
            "\t * e.g. when @p is being dispatched to a remote CPU, or when @p is",
            "\t * being enqueued on a CPU experiencing a hotplug event. Likewise, a",
            "\t * task may be ->enqueue()'d without being preceded by this operation",
            "\t * e.g. after exhausting its slice.",
            "\t */",
            "\tvoid (*runnable)(struct task_struct *p, u64 enq_flags);",
            "",
            "\t/**",
            "\t * running - A task is starting to run on its associated CPU",
            "\t * @p: task starting to run",
            "\t *",
            "\t * See ->runnable() for explanation on the task state notifiers.",
            "\t */",
            "\tvoid (*running)(struct task_struct *p);",
            "",
            "\t/**",
            "\t * stopping - A task is stopping execution",
            "\t * @p: task stopping to run",
            "\t * @runnable: is task @p still runnable?",
            "\t *",
            "\t * See ->runnable() for explanation on the task state notifiers. If",
            "\t * !@runnable, ->quiescent() will be invoked after this operation",
            "\t * returns.",
            "\t */",
            "\tvoid (*stopping)(struct task_struct *p, bool runnable);",
            "",
            "\t/**",
            "\t * quiescent - A task is becoming not runnable on its associated CPU",
            "\t * @p: task becoming not runnable",
            "\t * @deq_flags: %SCX_DEQ_*",
            "\t *",
            "\t * See ->runnable() for explanation on the task state notifiers.",
            "\t *",
            "\t * @p is becoming quiescent on the CPU because it's",
            "\t *",
            "\t * - sleeping (%SCX_DEQ_SLEEP)",
            "\t * - being moved to another CPU",
            "\t * - being temporarily taken off the queue for an attribute change",
            "\t *   (%SCX_DEQ_SAVE)",
            "\t *",
            "\t * This and ->dequeue() are related but not coupled. This operation",
            "\t * notifies @p's state transition and may not be preceded by ->dequeue()",
            "\t * e.g. when @p is being dispatched to a remote CPU.",
            "\t */",
            "\tvoid (*quiescent)(struct task_struct *p, u64 deq_flags);",
            "",
            "\t/**",
            "\t * yield - Yield CPU",
            "\t * @from: yielding task",
            "\t * @to: optional yield target task",
            "\t *",
            "\t * If @to is NULL, @from is yielding the CPU to other runnable tasks.",
            "\t * The BPF scheduler should ensure that other available tasks are",
            "\t * dispatched before the yielding task. Return value is ignored in this",
            "\t * case.",
            "\t *",
            "\t * If @to is not-NULL, @from wants to yield the CPU to @to. If the bpf",
            "\t * scheduler can implement the request, return %true; otherwise, %false.",
            "\t */",
            "\tbool (*yield)(struct task_struct *from, struct task_struct *to);",
            "",
            "\t/**",
            "\t * core_sched_before - Task ordering for core-sched",
            "\t * @a: task A",
            "\t * @b: task B",
            "\t *",
            "\t * Used by core-sched to determine the ordering between two tasks. See",
            "\t * Documentation/admin-guide/hw-vuln/core-scheduling.rst for details on",
            "\t * core-sched.",
            "\t *",
            "\t * Both @a and @b are runnable and may or may not currently be queued on",
            "\t * the BPF scheduler. Should return %true if @a should run before @b.",
            "\t * %false if there's no required ordering or @b should run before @a.",
            "\t *",
            "\t * If not specified, the default is ordering them according to when they",
            "\t * became runnable.",
            "\t */",
            "\tbool (*core_sched_before)(struct task_struct *a, struct task_struct *b);",
            "",
            "\t/**",
            "\t * set_weight - Set task weight",
            "\t * @p: task to set weight for",
            "\t * @weight: new weight [1..10000]",
            "\t *",
            "\t * Update @p's weight to @weight.",
            "\t */",
            "\tvoid (*set_weight)(struct task_struct *p, u32 weight);",
            "",
            "\t/**",
            "\t * set_cpumask - Set CPU affinity",
            "\t * @p: task to set CPU affinity for",
            "\t * @cpumask: cpumask of cpus that @p can run on",
            "\t *",
            "\t * Update @p's CPU affinity to @cpumask.",
            "\t */",
            "\tvoid (*set_cpumask)(struct task_struct *p,",
            "\t\t\t    const struct cpumask *cpumask);",
            "",
            "\t/**",
            "\t * update_idle - Update the idle state of a CPU",
            "\t * @cpu: CPU to udpate the idle state for",
            "\t * @idle: whether entering or exiting the idle state",
            "\t *",
            "\t * This operation is called when @rq's CPU goes or leaves the idle",
            "\t * state. By default, implementing this operation disables the built-in",
            "\t * idle CPU tracking and the following helpers become unavailable:",
            "\t *",
            "\t * - scx_bpf_select_cpu_dfl()",
            "\t * - scx_bpf_test_and_clear_cpu_idle()",
            "\t * - scx_bpf_pick_idle_cpu()",
            "\t *",
            "\t * The user also must implement ops.select_cpu() as the default",
            "\t * implementation relies on scx_bpf_select_cpu_dfl().",
            "\t *",
            "\t * Specify the %SCX_OPS_KEEP_BUILTIN_IDLE flag to keep the built-in idle",
            "\t * tracking.",
            "\t */",
            "\tvoid (*update_idle)(s32 cpu, bool idle);",
            "",
            "\t/**",
            "\t * cpu_acquire - A CPU is becoming available to the BPF scheduler",
            "\t * @cpu: The CPU being acquired by the BPF scheduler.",
            "\t * @args: Acquire arguments, see the struct definition.",
            "\t *",
            "\t * A CPU that was previously released from the BPF scheduler is now once",
            "\t * again under its control.",
            "\t */",
            "\tvoid (*cpu_acquire)(s32 cpu, struct scx_cpu_acquire_args *args);",
            "",
            "\t/**",
            "\t * cpu_release - A CPU is taken away from the BPF scheduler",
            "\t * @cpu: The CPU being released by the BPF scheduler.",
            "\t * @args: Release arguments, see the struct definition.",
            "\t *",
            "\t * The specified CPU is no longer under the control of the BPF",
            "\t * scheduler. This could be because it was preempted by a higher",
            "\t * priority sched_class, though there may be other reasons as well. The",
            "\t * caller should consult @args->reason to determine the cause.",
            "\t */",
            "\tvoid (*cpu_release)(s32 cpu, struct scx_cpu_release_args *args);",
            "",
            "\t/**",
            "\t * init_task - Initialize a task to run in a BPF scheduler",
            "\t * @p: task to initialize for BPF scheduling",
            "\t * @args: init arguments, see the struct definition",
            "\t *",
            "\t * Either we're loading a BPF scheduler or a new task is being forked.",
            "\t * Initialize @p for BPF scheduling. This operation may block and can",
            "\t * be used for allocations, and is called exactly once for a task.",
            "\t *",
            "\t * Return 0 for success, -errno for failure. An error return while",
            "\t * loading will abort loading of the BPF scheduler. During a fork, it",
            "\t * will abort that specific fork.",
            "\t */",
            "\ts32 (*init_task)(struct task_struct *p, struct scx_init_task_args *args);",
            "",
            "\t/**",
            "\t * exit_task - Exit a previously-running task from the system",
            "\t * @p: task to exit",
            "\t *",
            "\t * @p is exiting or the BPF scheduler is being unloaded. Perform any",
            "\t * necessary cleanup for @p.",
            "\t */",
            "\tvoid (*exit_task)(struct task_struct *p, struct scx_exit_task_args *args);",
            "",
            "\t/**",
            "\t * enable - Enable BPF scheduling for a task",
            "\t * @p: task to enable BPF scheduling for",
            "\t *",
            "\t * Enable @p for BPF scheduling. enable() is called on @p any time it",
            "\t * enters SCX, and is always paired with a matching disable().",
            "\t */",
            "\tvoid (*enable)(struct task_struct *p);",
            "",
            "\t/**",
            "\t * disable - Disable BPF scheduling for a task",
            "\t * @p: task to disable BPF scheduling for",
            "\t *",
            "\t * @p is exiting, leaving SCX or the BPF scheduler is being unloaded.",
            "\t * Disable BPF scheduling for @p. A disable() call is always matched",
            "\t * with a prior enable() call.",
            "\t */",
            "\tvoid (*disable)(struct task_struct *p);",
            "",
            "\t/**",
            "\t * dump - Dump BPF scheduler state on error",
            "\t * @ctx: debug dump context",
            "\t *",
            "\t * Use scx_bpf_dump() to generate BPF scheduler specific debug dump.",
            "\t */",
            "\tvoid (*dump)(struct scx_dump_ctx *ctx);",
            "",
            "\t/**",
            "\t * dump_cpu - Dump BPF scheduler state for a CPU on error",
            "\t * @ctx: debug dump context",
            "\t * @cpu: CPU to generate debug dump for",
            "\t * @idle: @cpu is currently idle without any runnable tasks",
            "\t *",
            "\t * Use scx_bpf_dump() to generate BPF scheduler specific debug dump for",
            "\t * @cpu. If @idle is %true and this operation doesn't produce any",
            "\t * output, @cpu is skipped for dump.",
            "\t */",
            "\tvoid (*dump_cpu)(struct scx_dump_ctx *ctx, s32 cpu, bool idle);",
            "",
            "\t/**",
            "\t * dump_task - Dump BPF scheduler state for a runnable task on error",
            "\t * @ctx: debug dump context",
            "\t * @p: runnable task to generate debug dump for",
            "\t *",
            "\t * Use scx_bpf_dump() to generate BPF scheduler specific debug dump for",
            "\t * @p.",
            "\t */",
            "\tvoid (*dump_task)(struct scx_dump_ctx *ctx, struct task_struct *p);",
            "",
            "#ifdef CONFIG_EXT_GROUP_SCHED",
            "\t/**",
            "\t * cgroup_init - Initialize a cgroup",
            "\t * @cgrp: cgroup being initialized",
            "\t * @args: init arguments, see the struct definition",
            "\t *",
            "\t * Either the BPF scheduler is being loaded or @cgrp created, initialize",
            "\t * @cgrp for sched_ext. This operation may block.",
            "\t *",
            "\t * Return 0 for success, -errno for failure. An error return while",
            "\t * loading will abort loading of the BPF scheduler. During cgroup",
            "\t * creation, it will abort the specific cgroup creation.",
            "\t */",
            "\ts32 (*cgroup_init)(struct cgroup *cgrp,",
            "\t\t\t   struct scx_cgroup_init_args *args);",
            "",
            "\t/**",
            "\t * cgroup_exit - Exit a cgroup",
            "\t * @cgrp: cgroup being exited",
            "\t *",
            "\t * Either the BPF scheduler is being unloaded or @cgrp destroyed, exit",
            "\t * @cgrp for sched_ext. This operation my block.",
            "\t */",
            "\tvoid (*cgroup_exit)(struct cgroup *cgrp);",
            "",
            "\t/**",
            "\t * cgroup_prep_move - Prepare a task to be moved to a different cgroup",
            "\t * @p: task being moved",
            "\t * @from: cgroup @p is being moved from",
            "\t * @to: cgroup @p is being moved to",
            "\t *",
            "\t * Prepare @p for move from cgroup @from to @to. This operation may",
            "\t * block and can be used for allocations.",
            "\t *",
            "\t * Return 0 for success, -errno for failure. An error return aborts the",
            "\t * migration.",
            "\t */",
            "\ts32 (*cgroup_prep_move)(struct task_struct *p,",
            "\t\t\t\tstruct cgroup *from, struct cgroup *to);",
            "",
            "\t/**",
            "\t * cgroup_move - Commit cgroup move",
            "\t * @p: task being moved",
            "\t * @from: cgroup @p is being moved from",
            "\t * @to: cgroup @p is being moved to",
            "\t *",
            "\t * Commit the move. @p is dequeued during this operation.",
            "\t */",
            "\tvoid (*cgroup_move)(struct task_struct *p,",
            "\t\t\t    struct cgroup *from, struct cgroup *to);",
            "",
            "\t/**",
            "\t * cgroup_cancel_move - Cancel cgroup move",
            "\t * @p: task whose cgroup move is being canceled",
            "\t * @from: cgroup @p was being moved from",
            "\t * @to: cgroup @p was being moved to",
            "\t *",
            "\t * @p was cgroup_prep_move()'d but failed before reaching cgroup_move().",
            "\t * Undo the preparation.",
            "\t */",
            "\tvoid (*cgroup_cancel_move)(struct task_struct *p,",
            "\t\t\t\t   struct cgroup *from, struct cgroup *to);",
            "",
            "\t/**",
            "\t * cgroup_set_weight - A cgroup's weight is being changed",
            "\t * @cgrp: cgroup whose weight is being updated",
            "\t * @weight: new weight [1..10000]",
            "\t *",
            "\t * Update @tg's weight to @weight.",
            "\t */",
            "\tvoid (*cgroup_set_weight)(struct cgroup *cgrp, u32 weight);",
            "#endif\t/* CONFIG_CGROUPS */",
            "",
            "\t/*",
            "\t * All online ops must come before ops.cpu_online().",
            "\t */",
            "",
            "\t/**",
            "\t * cpu_online - A CPU became online",
            "\t * @cpu: CPU which just came up",
            "\t *",
            "\t * @cpu just came online. @cpu will not call ops.enqueue() or",
            "\t * ops.dispatch(), nor run tasks associated with other CPUs beforehand.",
            "\t */",
            "\tvoid (*cpu_online)(s32 cpu);",
            "",
            "\t/**",
            "\t * cpu_offline - A CPU is going offline",
            "\t * @cpu: CPU which is going offline",
            "\t *",
            "\t * @cpu is going offline. @cpu will not call ops.enqueue() or",
            "\t * ops.dispatch(), nor run tasks associated with other CPUs afterwards.",
            "\t */",
            "\tvoid (*cpu_offline)(s32 cpu);",
            "",
            "\t/*",
            "\t * All CPU hotplug ops must come before ops.init().",
            "\t */",
            "",
            "\t/**",
            "\t * init - Initialize the BPF scheduler",
            "\t */",
            "\ts32 (*init)(void);",
            "",
            "\t/**",
            "\t * exit - Clean up after the BPF scheduler",
            "\t * @info: Exit info",
            "\t *",
            "\t * ops.exit() is also called on ops.init() failure, which is a bit",
            "\t * unusual. This is to allow rich reporting through @info on how",
            "\t * ops.init() failed.",
            "\t */",
            "\tvoid (*exit)(struct scx_exit_info *info);",
            "",
            "\t/**",
            "\t * dispatch_max_batch - Max nr of tasks that dispatch() can dispatch",
            "\t */",
            "\tu32 dispatch_max_batch;",
            "",
            "\t/**",
            "\t * flags - %SCX_OPS_* flags",
            "\t */",
            "\tu64 flags;",
            "",
            "\t/**",
            "\t * timeout_ms - The maximum amount of time, in milliseconds, that a",
            "\t * runnable task should be able to wait before being scheduled. The",
            "\t * maximum timeout may not exceed the default timeout of 30 seconds.",
            "\t *",
            "\t * Defaults to the maximum allowed timeout value of 30 seconds.",
            "\t */",
            "\tu32 timeout_ms;",
            "",
            "\t/**",
            "\t * exit_dump_len - scx_exit_info.dump buffer length. If 0, the default",
            "\t * value of 32768 is used.",
            "\t */",
            "\tu32 exit_dump_len;",
            "",
            "\t/**",
            "\t * hotplug_seq - A sequence number that may be set by the scheduler to",
            "\t * detect when a hotplug event has occurred during the loading process.",
            "\t * If 0, no detection occurs. Otherwise, the scheduler will fail to",
            "\t * load if the sequence number does not match @scx_hotplug_seq on the",
            "\t * enable path.",
            "\t */",
            "\tu64 hotplug_seq;",
            "",
            "\t/**",
            "\t * name - BPF scheduler's name",
            "\t *",
            "\t * Must be a non-zero valid BPF object name including only isalnum(),",
            "\t * '_' and '.' chars. Shows up in kernel.sched_ext_ops sysctl while the",
            "\t * BPF scheduler is enabled.",
            "\t */",
            "\tchar name[SCX_OPS_NAME_LEN];",
            "};",
            "",
            "enum scx_opi {",
            "\tSCX_OPI_BEGIN\t\t\t= 0,",
            "\tSCX_OPI_NORMAL_BEGIN\t\t= 0,",
            "\tSCX_OPI_NORMAL_END\t\t= SCX_OP_IDX(cpu_online),",
            "\tSCX_OPI_CPU_HOTPLUG_BEGIN\t= SCX_OP_IDX(cpu_online),",
            "\tSCX_OPI_CPU_HOTPLUG_END\t\t= SCX_OP_IDX(init),",
            "\tSCX_OPI_END\t\t\t= SCX_OP_IDX(init),",
            "};",
            "",
            "enum scx_wake_flags {",
            "\t/* expose select WF_* flags as enums */",
            "\tSCX_WAKE_FORK\t\t= WF_FORK,",
            "\tSCX_WAKE_TTWU\t\t= WF_TTWU,",
            "\tSCX_WAKE_SYNC\t\t= WF_SYNC,",
            "};",
            "",
            "enum scx_enq_flags {",
            "\t/* expose select ENQUEUE_* flags as enums */",
            "\tSCX_ENQ_WAKEUP\t\t= ENQUEUE_WAKEUP,",
            "\tSCX_ENQ_HEAD\t\t= ENQUEUE_HEAD,",
            "\tSCX_ENQ_CPU_SELECTED\t= ENQUEUE_RQ_SELECTED,",
            "",
            "\t/* high 32bits are SCX specific */",
            "",
            "\t/*",
            "\t * Set the following to trigger preemption when calling",
            "\t * scx_bpf_dsq_insert() with a local dsq as the target. The slice of the",
            "\t * current task is cleared to zero and the CPU is kicked into the",
            "\t * scheduling path. Implies %SCX_ENQ_HEAD.",
            "\t */",
            "\tSCX_ENQ_PREEMPT\t\t= 1LLU << 32,",
            "",
            "\t/*",
            "\t * The task being enqueued was previously enqueued on the current CPU's",
            "\t * %SCX_DSQ_LOCAL, but was removed from it in a call to the",
            "\t * bpf_scx_reenqueue_local() kfunc. If bpf_scx_reenqueue_local() was",
            "\t * invoked in a ->cpu_release() callback, and the task is again",
            "\t * dispatched back to %SCX_LOCAL_DSQ by this current ->enqueue(), the",
            "\t * task will not be scheduled on the CPU until at least the next invocation",
            "\t * of the ->cpu_acquire() callback.",
            "\t */",
            "\tSCX_ENQ_REENQ\t\t= 1LLU << 40,",
            "",
            "\t/*",
            "\t * The task being enqueued is the only task available for the cpu. By",
            "\t * default, ext core keeps executing such tasks but when",
            "\t * %SCX_OPS_ENQ_LAST is specified, they're ops.enqueue()'d with the",
            "\t * %SCX_ENQ_LAST flag set.",
            "\t *",
            "\t * The BPF scheduler is responsible for triggering a follow-up",
            "\t * scheduling event. Otherwise, Execution may stall.",
            "\t */",
            "\tSCX_ENQ_LAST\t\t= 1LLU << 41,",
            "",
            "\t/* high 8 bits are internal */",
            "\t__SCX_ENQ_INTERNAL_MASK\t= 0xffLLU << 56,",
            "",
            "\tSCX_ENQ_CLEAR_OPSS\t= 1LLU << 56,",
            "\tSCX_ENQ_DSQ_PRIQ\t= 1LLU << 57,",
            "};",
            "",
            "enum scx_deq_flags {",
            "\t/* expose select DEQUEUE_* flags as enums */",
            "\tSCX_DEQ_SLEEP\t\t= DEQUEUE_SLEEP,",
            "",
            "\t/* high 32bits are SCX specific */",
            "",
            "\t/*",
            "\t * The generic core-sched layer decided to execute the task even though",
            "\t * it hasn't been dispatched yet. Dequeue from the BPF side.",
            "\t */",
            "\tSCX_DEQ_CORE_SCHED_EXEC\t= 1LLU << 32,",
            "};",
            "",
            "enum scx_pick_idle_cpu_flags {",
            "\tSCX_PICK_IDLE_CORE\t= 1LLU << 0,\t/* pick a CPU whose SMT siblings are also idle */",
            "};",
            "",
            "enum scx_kick_flags {",
            "\t/*",
            "\t * Kick the target CPU if idle. Guarantees that the target CPU goes",
            "\t * through at least one full scheduling cycle before going idle. If the",
            "\t * target CPU can be determined to be currently not idle and going to go",
            "\t * through a scheduling cycle before going idle, noop.",
            "\t */",
            "\tSCX_KICK_IDLE\t\t= 1LLU << 0,",
            "",
            "\t/*",
            "\t * Preempt the current task and execute the dispatch path. If the",
            "\t * current task of the target CPU is an SCX task, its ->scx.slice is",
            "\t * cleared to zero before the scheduling path is invoked so that the",
            "\t * task expires and the dispatch path is invoked.",
            "\t */",
            "\tSCX_KICK_PREEMPT\t= 1LLU << 1,",
            "",
            "\t/*",
            "\t * Wait for the CPU to be rescheduled. The scx_bpf_kick_cpu() call will",
            "\t * return after the target CPU finishes picking the next task.",
            "\t */",
            "\tSCX_KICK_WAIT\t\t= 1LLU << 2,",
            "};",
            "",
            "enum scx_tg_flags {",
            "\tSCX_TG_ONLINE\t\t= 1U << 0,",
            "\tSCX_TG_INITED\t\t= 1U << 1,",
            "};",
            "",
            "enum scx_ops_enable_state {",
            "\tSCX_OPS_ENABLING,",
            "\tSCX_OPS_ENABLED,",
            "\tSCX_OPS_DISABLING,",
            "\tSCX_OPS_DISABLED,",
            "};",
            "",
            "static const char *scx_ops_enable_state_str[] = {",
            "\t[SCX_OPS_ENABLING]\t= \"enabling\",",
            "\t[SCX_OPS_ENABLED]\t= \"enabled\",",
            "\t[SCX_OPS_DISABLING]\t= \"disabling\",",
            "\t[SCX_OPS_DISABLED]\t= \"disabled\",",
            "};",
            "",
            "/*",
            " * sched_ext_entity->ops_state",
            " *",
            " * Used to track the task ownership between the SCX core and the BPF scheduler.",
            " * State transitions look as follows:",
            " *",
            " * NONE -> QUEUEING -> QUEUED -> DISPATCHING",
            " *   ^              |                 |",
            " *   |              v                 v",
            " *   \\-------------------------------/",
            " *",
            " * QUEUEING and DISPATCHING states can be waited upon. See wait_ops_state() call",
            " * sites for explanations on the conditions being waited upon and why they are",
            " * safe. Transitions out of them into NONE or QUEUED must store_release and the",
            " * waiters should load_acquire.",
            " *",
            " * Tracking scx_ops_state enables sched_ext core to reliably determine whether",
            " * any given task can be dispatched by the BPF scheduler at all times and thus",
            " * relaxes the requirements on the BPF scheduler. This allows the BPF scheduler",
            " * to try to dispatch any task anytime regardless of its state as the SCX core",
            " * can safely reject invalid dispatches.",
            " */",
            "enum scx_ops_state {",
            "\tSCX_OPSS_NONE,\t\t/* owned by the SCX core */",
            "\tSCX_OPSS_QUEUEING,\t/* in transit to the BPF scheduler */",
            "\tSCX_OPSS_QUEUED,\t/* owned by the BPF scheduler */",
            "\tSCX_OPSS_DISPATCHING,\t/* in transit back to the SCX core */",
            "",
            "\t/*",
            "\t * QSEQ brands each QUEUED instance so that, when dispatch races",
            "\t * dequeue/requeue, the dispatcher can tell whether it still has a claim",
            "\t * on the task being dispatched.",
            "\t *",
            "\t * As some 32bit archs can't do 64bit store_release/load_acquire,",
            "\t * p->scx.ops_state is atomic_long_t which leaves 30 bits for QSEQ on",
            "\t * 32bit machines. The dispatch race window QSEQ protects is very narrow",
            "\t * and runs with IRQ disabled. 30 bits should be sufficient.",
            "\t */",
            "\tSCX_OPSS_QSEQ_SHIFT\t= 2,",
            "};",
            "",
            "/* Use macros to ensure that the type is unsigned long for the masks */",
            "#define SCX_OPSS_STATE_MASK\t((1LU << SCX_OPSS_QSEQ_SHIFT) - 1)",
            "#define SCX_OPSS_QSEQ_MASK\t(~SCX_OPSS_STATE_MASK)",
            "",
            "/*",
            " * During exit, a task may schedule after losing its PIDs. When disabling the",
            " * BPF scheduler, we need to be able to iterate tasks in every state to",
            " * guarantee system safety. Maintain a dedicated task list which contains every",
            " * task between its fork and eventual free.",
            " */",
            "static DEFINE_SPINLOCK(scx_tasks_lock);",
            "static LIST_HEAD(scx_tasks);",
            "",
            "/* ops enable/disable */",
            "static struct kthread_worker *scx_ops_helper;",
            "static DEFINE_MUTEX(scx_ops_enable_mutex);",
            "DEFINE_STATIC_KEY_FALSE(__scx_ops_enabled);",
            "DEFINE_STATIC_PERCPU_RWSEM(scx_fork_rwsem);",
            "static atomic_t scx_ops_enable_state_var = ATOMIC_INIT(SCX_OPS_DISABLED);",
            "static int scx_ops_bypass_depth;",
            "static DEFINE_RAW_SPINLOCK(__scx_ops_bypass_lock);",
            "static bool scx_ops_init_task_enabled;",
            "static bool scx_switching_all;",
            "DEFINE_STATIC_KEY_FALSE(__scx_switched_all);",
            "",
            "static struct sched_ext_ops scx_ops;",
            "static bool scx_warned_zero_slice;",
            "",
            "static DEFINE_STATIC_KEY_FALSE(scx_ops_enq_last);",
            "static DEFINE_STATIC_KEY_FALSE(scx_ops_enq_exiting);",
            "static DEFINE_STATIC_KEY_FALSE(scx_ops_cpu_preempt);",
            "static DEFINE_STATIC_KEY_FALSE(scx_builtin_idle_enabled);",
            "",
            "static struct static_key_false scx_has_op[SCX_OPI_END] =",
            "\t{ [0 ... SCX_OPI_END-1] = STATIC_KEY_FALSE_INIT };",
            "",
            "static atomic_t scx_exit_kind = ATOMIC_INIT(SCX_EXIT_DONE);",
            "static struct scx_exit_info *scx_exit_info;",
            "",
            "static atomic_long_t scx_nr_rejected = ATOMIC_LONG_INIT(0);",
            "static atomic_long_t scx_hotplug_seq = ATOMIC_LONG_INIT(0);",
            "",
            "/*",
            " * A monotically increasing sequence number that is incremented every time a",
            " * scheduler is enabled. This can be used by to check if any custom sched_ext",
            " * scheduler has ever been used in the system.",
            " */",
            "static atomic_long_t scx_enable_seq = ATOMIC_LONG_INIT(0);",
            "",
            "/*",
            " * The maximum amount of time in jiffies that a task may be runnable without",
            " * being scheduled on a CPU. If this timeout is exceeded, it will trigger",
            " * scx_ops_error().",
            " */",
            "static unsigned long scx_watchdog_timeout;",
            "",
            "/*",
            " * The last time the delayed work was run. This delayed work relies on",
            " * ksoftirqd being able to run to service timer interrupts, so it's possible",
            " * that this work itself could get wedged. To account for this, we check that",
            " * it's not stalled in the timer tick, and trigger an error if it is.",
            " */",
            "static unsigned long scx_watchdog_timestamp = INITIAL_JIFFIES;",
            "",
            "static struct delayed_work scx_watchdog_work;",
            "",
            "/* idle tracking */",
            "#ifdef CONFIG_SMP",
            "#ifdef CONFIG_CPUMASK_OFFSTACK",
            "#define CL_ALIGNED_IF_ONSTACK",
            "#else",
            "#define CL_ALIGNED_IF_ONSTACK __cacheline_aligned_in_smp",
            "#endif",
            "",
            "static struct {",
            "\tcpumask_var_t cpu;",
            "\tcpumask_var_t smt;",
            "} idle_masks CL_ALIGNED_IF_ONSTACK;",
            "",
            "#endif\t/* CONFIG_SMP */",
            "",
            "/* for %SCX_KICK_WAIT */",
            "static unsigned long __percpu *scx_kick_cpus_pnt_seqs;",
            "",
            "/*",
            " * Direct dispatch marker.",
            " *",
            " * Non-NULL values are used for direct dispatch from enqueue path. A valid",
            " * pointer points to the task currently being enqueued. An ERR_PTR value is used",
            " * to indicate that direct dispatch has already happened.",
            " */",
            "static DEFINE_PER_CPU(struct task_struct *, direct_dispatch_task);",
            "",
            "/*",
            " * Dispatch queues.",
            " *",
            " * The global DSQ (%SCX_DSQ_GLOBAL) is split per-node for scalability. This is",
            " * to avoid live-locking in bypass mode where all tasks are dispatched to",
            " * %SCX_DSQ_GLOBAL and all CPUs consume from it. If per-node split isn't",
            " * sufficient, it can be further split.",
            " */",
            "static struct scx_dispatch_q **global_dsqs;",
            "",
            "static const struct rhashtable_params dsq_hash_params = {",
            "\t.key_len\t\t= 8,",
            "\t.key_offset\t\t= offsetof(struct scx_dispatch_q, id),",
            "\t.head_offset\t\t= offsetof(struct scx_dispatch_q, hash_node),",
            "};",
            "",
            "static struct rhashtable dsq_hash;",
            "static LLIST_HEAD(dsqs_to_free);",
            "",
            "/* dispatch buf */",
            "struct scx_dsp_buf_ent {",
            "\tstruct task_struct\t*task;",
            "\tunsigned long\t\tqseq;",
            "\tu64\t\t\tdsq_id;",
            "\tu64\t\t\tenq_flags;",
            "};",
            "",
            "static u32 scx_dsp_max_batch;",
            "",
            "struct scx_dsp_ctx {",
            "\tstruct rq\t\t*rq;",
            "\tu32\t\t\tcursor;",
            "\tu32\t\t\tnr_tasks;",
            "\tstruct scx_dsp_buf_ent\tbuf[];",
            "};",
            "",
            "static struct scx_dsp_ctx __percpu *scx_dsp_ctx;",
            "",
            "/* string formatting from BPF */",
            "struct scx_bstr_buf {",
            "\tu64\t\t\tdata[MAX_BPRINTF_VARARGS];",
            "\tchar\t\t\tline[SCX_EXIT_MSG_LEN];",
            "};",
            "",
            "static DEFINE_RAW_SPINLOCK(scx_exit_bstr_buf_lock);",
            "static struct scx_bstr_buf scx_exit_bstr_buf;",
            "",
            "/* ops debug dump */",
            "struct scx_dump_data {",
            "\ts32\t\t\tcpu;",
            "\tbool\t\t\tfirst;",
            "\ts32\t\t\tcursor;",
            "\tstruct seq_buf\t\t*s;",
            "\tconst char\t\t*prefix;",
            "\tstruct scx_bstr_buf\tbuf;",
            "};",
            "",
            "static struct scx_dump_data scx_dump_data = {",
            "\t.cpu\t\t\t= -1,",
            "};",
            "",
            "/* /sys/kernel/sched_ext interface */",
            "static struct kset *scx_kset;",
            "static struct kobject *scx_root_kobj;",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/sched_ext.h>",
            "",
            "static void process_ddsp_deferred_locals(struct rq *rq);",
            "static void scx_bpf_kick_cpu(s32 cpu, u64 flags);",
            "static __printf(3, 4) void scx_ops_exit_kind(enum scx_exit_kind kind,",
            "\t\t\t\t\t     s64 exit_code,",
            "\t\t\t\t\t     const char *fmt, ...);",
            "",
            "#define scx_ops_error_kind(err, fmt, args...)\t\t\t\t\t\\",
            "\tscx_ops_exit_kind((err), 0, fmt, ##args)",
            "",
            "#define scx_ops_exit(code, fmt, args...)\t\t\t\t\t\\",
            "\tscx_ops_exit_kind(SCX_EXIT_UNREG_KERN, (code), fmt, ##args)",
            "",
            "#define scx_ops_error(fmt, args...)\t\t\t\t\t\t\\",
            "\tscx_ops_error_kind(SCX_EXIT_ERROR, fmt, ##args)",
            "",
            "#define SCX_HAS_OP(op)\tstatic_branch_likely(&scx_has_op[SCX_OP_IDX(op)])",
            "",
            "static long jiffies_delta_msecs(unsigned long at, unsigned long now)",
            "{",
            "\tif (time_after(at, now))",
            "\t\treturn jiffies_to_msecs(at - now);",
            "\telse",
            "\t\treturn -(long)jiffies_to_msecs(now - at);",
            "}",
            "",
            "/* if the highest set bit is N, return a mask with bits [N+1, 31] set */"
          ],
          "function_name": null,
          "description": "定义了BPF可扩展调度器的核心数据结构和常量，包括调度操作表（sched_ext_ops）、状态标志、错误码及辅助宏，为BPF调度策略实现提供基础框架。",
          "similarity": 0.5012607574462891
        },
        {
          "chunk_id": 33,
          "file_path": "kernel/sched/ext.c",
          "start_line": 6413,
          "end_line": 6524,
          "content": [
            "__bpf_kfunc bool scx_bpf_dispatch_vtime_from_dsq(struct bpf_iter_scx_dsq *it__iter,",
            "\t\t\t\t\t\t struct task_struct *p, u64 dsq_id,",
            "\t\t\t\t\t\t u64 enq_flags)",
            "{",
            "\tprintk_deferred_once(KERN_WARNING \"sched_ext: scx_bpf_dispatch_from_dsq_vtime() renamed to scx_bpf_dsq_move_vtime()\");",
            "\treturn scx_bpf_dsq_move_vtime(it__iter, p, dsq_id, enq_flags);",
            "}",
            "__bpf_kfunc u32 scx_bpf_reenqueue_local(void)",
            "{",
            "\tLIST_HEAD(tasks);",
            "\tu32 nr_enqueued = 0;",
            "\tstruct rq *rq;",
            "\tstruct task_struct *p, *n;",
            "",
            "\tif (!scx_kf_allowed(SCX_KF_CPU_RELEASE))",
            "\t\treturn 0;",
            "",
            "\trq = cpu_rq(smp_processor_id());",
            "\tlockdep_assert_rq_held(rq);",
            "",
            "\t/*",
            "\t * The BPF scheduler may choose to dispatch tasks back to",
            "\t * @rq->scx.local_dsq. Move all candidate tasks off to a private list",
            "\t * first to avoid processing the same tasks repeatedly.",
            "\t */",
            "\tlist_for_each_entry_safe(p, n, &rq->scx.local_dsq.list,",
            "\t\t\t\t scx.dsq_list.node) {",
            "\t\t/*",
            "\t\t * If @p is being migrated, @p's current CPU may not agree with",
            "\t\t * its allowed CPUs and the migration_cpu_stop is about to",
            "\t\t * deactivate and re-activate @p anyway. Skip re-enqueueing.",
            "\t\t *",
            "\t\t * While racing sched property changes may also dequeue and",
            "\t\t * re-enqueue a migrating task while its current CPU and allowed",
            "\t\t * CPUs disagree, they use %ENQUEUE_RESTORE which is bypassed to",
            "\t\t * the current local DSQ for running tasks and thus are not",
            "\t\t * visible to the BPF scheduler.",
            "\t\t */",
            "\t\tif (p->migration_pending)",
            "\t\t\tcontinue;",
            "",
            "\t\tdispatch_dequeue(rq, p);",
            "\t\tlist_add_tail(&p->scx.dsq_list.node, &tasks);",
            "\t}",
            "",
            "\tlist_for_each_entry_safe(p, n, &tasks, scx.dsq_list.node) {",
            "\t\tlist_del_init(&p->scx.dsq_list.node);",
            "\t\tdo_enqueue_task(rq, p, SCX_ENQ_REENQ, -1);",
            "\t\tnr_enqueued++;",
            "\t}",
            "",
            "\treturn nr_enqueued;",
            "}",
            "__bpf_kfunc s32 scx_bpf_create_dsq(u64 dsq_id, s32 node)",
            "{",
            "\tif (unlikely(node >= (int)nr_node_ids ||",
            "\t\t     (node < 0 && node != NUMA_NO_NODE)))",
            "\t\treturn -EINVAL;",
            "\treturn PTR_ERR_OR_ZERO(create_dsq(dsq_id, node));",
            "}",
            "__bpf_kfunc void scx_bpf_kick_cpu(s32 cpu, u64 flags)",
            "{",
            "\tstruct rq *this_rq;",
            "\tunsigned long irq_flags;",
            "",
            "\tif (!ops_cpu_valid(cpu, NULL))",
            "\t\treturn;",
            "",
            "\tlocal_irq_save(irq_flags);",
            "",
            "\tthis_rq = this_rq();",
            "",
            "\t/*",
            "\t * While bypassing for PM ops, IRQ handling may not be online which can",
            "\t * lead to irq_work_queue() malfunction such as infinite busy wait for",
            "\t * IRQ status update. Suppress kicking.",
            "\t */",
            "\tif (scx_rq_bypassing(this_rq))",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * Actual kicking is bounced to kick_cpus_irq_workfn() to avoid nesting",
            "\t * rq locks. We can probably be smarter and avoid bouncing if called",
            "\t * from ops which don't hold a rq lock.",
            "\t */",
            "\tif (flags & SCX_KICK_IDLE) {",
            "\t\tstruct rq *target_rq = cpu_rq(cpu);",
            "",
            "\t\tif (unlikely(flags & (SCX_KICK_PREEMPT | SCX_KICK_WAIT)))",
            "\t\t\tscx_ops_error(\"PREEMPT/WAIT cannot be used with SCX_KICK_IDLE\");",
            "",
            "\t\tif (raw_spin_rq_trylock(target_rq)) {",
            "\t\t\tif (can_skip_idle_kick(target_rq)) {",
            "\t\t\t\traw_spin_rq_unlock(target_rq);",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t\traw_spin_rq_unlock(target_rq);",
            "\t\t}",
            "\t\tcpumask_set_cpu(cpu, this_rq->scx.cpus_to_kick_if_idle);",
            "\t} else {",
            "\t\tcpumask_set_cpu(cpu, this_rq->scx.cpus_to_kick);",
            "",
            "\t\tif (flags & SCX_KICK_PREEMPT)",
            "\t\t\tcpumask_set_cpu(cpu, this_rq->scx.cpus_to_preempt);",
            "\t\tif (flags & SCX_KICK_WAIT)",
            "\t\t\tcpumask_set_cpu(cpu, this_rq->scx.cpus_to_wait);",
            "\t}",
            "",
            "\tirq_work_queue(&this_rq->scx.kick_cpus_irq_work);",
            "out:",
            "\tlocal_irq_restore(irq_flags);",
            "}"
          ],
          "function_name": "scx_bpf_dispatch_vtime_from_dsq, scx_bpf_reenqueue_local, scx_bpf_create_dsq, scx_bpf_kick_cpu",
          "description": "该代码段为Linux内核调度器扩展模块中与BPF调度队列（DSQ）相关的辅助函数集合，主要实现任务迁移、本地重入队、DSQ创建及CPU唤醒控制功能。  \n`scx_bpf_dispatch_vtime_from_dsq` 是 `scx_bpf_dsq_move_vtime` 的别名，用于将任务从DSQ迁移到其他队列；`scx_bpf_reenqueue_local` 将本地DSQ中非迁移任务重新入队并统计数量；`scx_bpf_create_dsq` 创建指定节点的DSQ并校验参数合法性；`scx_bpf_kick_cpu` 根据标志位异步触发目标CPU的唤醒操作。  \n代码缺少DSQ结构体定义及`create_dsq`等底层实现细节，需结合上下文进一步确认完整逻辑。",
          "similarity": 0.48709049820899963
        }
      ]
    },
    {
      "source_file": "kernel/bpf/dispatcher.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:10:06\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\dispatcher.c`\n\n---\n\n# `bpf/dispatcher.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/dispatcher.c` 实现了 BPF（Berkeley Packet Filter）调度器（dispatcher）机制，其核心目标是通过生成多路分支的直接调用代码，避免在启用 retpoline（用于缓解 Spectre v2 攻击的间接跳转防护机制）时因间接调用带来的性能开销。该调度器通过劫持一个 trampoline 函数的 `__fentry__` 入口，动态生成包含多个 BPF 程序直接调用的跳转逻辑，从而将原本的间接调用转换为高效的直接调用。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct bpf_dispatcher`：BPF 调度器主结构体，包含：\n  - `progs[BPF_DISPATCHER_MAX]`：最多支持 `BPF_DISPATCHER_MAX` 个 BPF 程序的注册项\n  - `num_progs`：当前注册的程序数量\n  - `image` 和 `rw_image`：分别指向只读可执行（RO+X）和可读写（RW）的代码页\n  - `image_off`：用于双缓冲机制的偏移量\n  - `mutex`：保护调度器状态的互斥锁\n  - `ksym`：用于内核符号管理的 ksym 结构\n- `struct bpf_dispatcher_prog`：调度器中每个 BPF 程序的注册项，包含：\n  - `prog`：指向注册的 `struct bpf_prog`\n  - `users`：引用计数\n\n### 主要函数\n- `bpf_dispatcher_find_prog()`：在调度器中查找指定 BPF 程序的注册项\n- `bpf_dispatcher_find_free()`：查找空闲的注册槽位\n- `bpf_dispatcher_add_prog()`：向调度器注册一个 BPF 程序（带引用计数）\n- `bpf_dispatcher_remove_prog()`：从调度器注销一个 BPF 程序（引用计数减一，若为零则真正移除）\n- `arch_prepare_bpf_dispatcher()`（弱符号）：架构相关函数，用于生成实际的多路分支机器码\n- `bpf_dispatcher_prepare()`：准备调度器代码镜像，收集所有已注册 BPF 程序的入口地址\n- `bpf_dispatcher_update()`：更新调度器的可执行代码，使用双缓冲机制避免执行时修改代码\n- `bpf_dispatcher_change_prog()`：主入口函数，用于将一个 BPF 程序替换为另一个，并触发调度器代码更新\n\n## 3. 关键实现\n\n### 调度器工作原理\n调度器维护一个最多包含 `BPF_DISPATCHER_MAX` 个 BPF 程序的列表。当有程序注册或注销时，调度器会重新生成一段包含所有有效程序直接调用的机器码（多路分支），并通过 trampoline 机制被调用。\n\n### 双缓冲代码更新机制\n为避免在 CPU 执行调度器代码时修改代码页导致崩溃，采用双缓冲策略：\n- 调度器分配两个半页（共一页）的内存：`image`（RO+X）和 `rw_image`（RW）\n- `image_off` 在 `0` 和 `PAGE_SIZE/2` 之间切换，指示当前活跃的半页\n- 新代码先在 `rw_image` 的非活跃半页中生成，再通过 `bpf_arch_text_copy` 原子复制到 `image` 的对应位置\n- 调用 `synchronize_rcu()` 确保所有 CPU 退出旧代码后再切换活跃半页\n\n### 引用计数管理\n每个注册的 BPF 程序通过 `refcount_t users` 管理引用计数，允许多次注册同一程序（仅增加引用计数），只有当引用归零时才真正从调度器中移除并释放程序。\n\n### 架构无关与相关分离\n- 架构无关逻辑（如程序管理、缓冲区切换）在本文件实现\n- 架构相关代码生成由 `arch_prepare_bpf_dispatcher()` 实现（通常在 `arch/xxx/net/bpf_dispatcher.c` 中），若未实现则返回 `-ENOTSUPP`\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/hash.h>`：哈希辅助（虽未直接使用，但可能为未来扩展预留）\n  - `<linux/bpf.h>` 和 `<linux/filter.h>`：BPF 核心数据结构（`bpf_prog`、`bpf_insn` 等）\n  - `<linux/static_call.h>`：静态调用优化支持（用于 `__BPF_DISPATCHER_UPDATE` 宏）\n- **内核模块依赖**：\n  - BPF JIT 子系统：通过 `bpf_prog_pack_alloc()`、`bpf_jit_alloc_exec()` 分配可执行内存\n  - 内存管理：使用 `bpf_prog_inc()`/`bpf_prog_put()` 管理 BPF 程序生命周期\n  - RCU 机制：通过 `synchronize_rcu()` 实现安全的代码更新\n  - 架构特定代码复制：依赖 `bpf_arch_text_copy()`（通常基于 `text_poke()`）\n\n## 5. 使用场景\n\n- **BPF 程序热替换**：当 attach 到 tracepoint、kprobe、perf event 等的 BPF 程序被替换时，通过 `bpf_dispatcher_change_prog()` 更新调度器，避免间接调用开销。\n- **高性能 BPF 执行路径**：在需要极致性能的场景（如网络数据包处理、系统调用跟踪），调度器可显著提升 BPF 程序调用效率，尤其在启用 retpoline 的系统上。\n- **多程序共享调度器**：多个相同类型的 BPF 程序（如多个 socket filter）可共享同一个调度器实例，统一管理直接调用入口。",
      "similarity": 0.5134667754173279,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/dispatcher.c",
          "start_line": 43,
          "end_line": 166,
          "content": [
            "static bool bpf_dispatcher_add_prog(struct bpf_dispatcher *d,",
            "\t\t\t\t    struct bpf_prog *prog)",
            "{",
            "\tstruct bpf_dispatcher_prog *entry;",
            "",
            "\tif (!prog)",
            "\t\treturn false;",
            "",
            "\tentry = bpf_dispatcher_find_prog(d, prog);",
            "\tif (entry) {",
            "\t\trefcount_inc(&entry->users);",
            "\t\treturn false;",
            "\t}",
            "",
            "\tentry = bpf_dispatcher_find_free(d);",
            "\tif (!entry)",
            "\t\treturn false;",
            "",
            "\tbpf_prog_inc(prog);",
            "\tentry->prog = prog;",
            "\trefcount_set(&entry->users, 1);",
            "\td->num_progs++;",
            "\treturn true;",
            "}",
            "static bool bpf_dispatcher_remove_prog(struct bpf_dispatcher *d,",
            "\t\t\t\t       struct bpf_prog *prog)",
            "{",
            "\tstruct bpf_dispatcher_prog *entry;",
            "",
            "\tif (!prog)",
            "\t\treturn false;",
            "",
            "\tentry = bpf_dispatcher_find_prog(d, prog);",
            "\tif (!entry)",
            "\t\treturn false;",
            "",
            "\tif (refcount_dec_and_test(&entry->users)) {",
            "\t\tentry->prog = NULL;",
            "\t\tbpf_prog_put(prog);",
            "\t\td->num_progs--;",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "int __weak arch_prepare_bpf_dispatcher(void *image, void *buf, s64 *funcs, int num_funcs)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static int bpf_dispatcher_prepare(struct bpf_dispatcher *d, void *image, void *buf)",
            "{",
            "\ts64 ips[BPF_DISPATCHER_MAX] = {}, *ipsp = &ips[0];",
            "\tint i;",
            "",
            "\tfor (i = 0; i < BPF_DISPATCHER_MAX; i++) {",
            "\t\tif (d->progs[i].prog)",
            "\t\t\t*ipsp++ = (s64)(uintptr_t)d->progs[i].prog->bpf_func;",
            "\t}",
            "\treturn arch_prepare_bpf_dispatcher(image, buf, &ips[0], d->num_progs);",
            "}",
            "static void bpf_dispatcher_update(struct bpf_dispatcher *d, int prev_num_progs)",
            "{",
            "\tvoid *new, *tmp;",
            "\tu32 noff = 0;",
            "",
            "\tif (prev_num_progs)",
            "\t\tnoff = d->image_off ^ (PAGE_SIZE / 2);",
            "",
            "\tnew = d->num_progs ? d->image + noff : NULL;",
            "\ttmp = d->num_progs ? d->rw_image + noff : NULL;",
            "\tif (new) {",
            "\t\t/* Prepare the dispatcher in d->rw_image. Then use",
            "\t\t * bpf_arch_text_copy to update d->image, which is RO+X.",
            "\t\t */",
            "\t\tif (bpf_dispatcher_prepare(d, new, tmp))",
            "\t\t\treturn;",
            "\t\tif (IS_ERR(bpf_arch_text_copy(new, tmp, PAGE_SIZE / 2)))",
            "\t\t\treturn;",
            "\t}",
            "",
            "\t__BPF_DISPATCHER_UPDATE(d, new ?: (void *)&bpf_dispatcher_nop_func);",
            "",
            "\t/* Make sure all the callers executing the previous/old half of the",
            "\t * image leave it, so following update call can modify it safely.",
            "\t */",
            "\tsynchronize_rcu();",
            "",
            "\tif (new)",
            "\t\td->image_off = noff;",
            "}",
            "void bpf_dispatcher_change_prog(struct bpf_dispatcher *d, struct bpf_prog *from,",
            "\t\t\t\tstruct bpf_prog *to)",
            "{",
            "\tbool changed = false;",
            "\tint prev_num_progs;",
            "",
            "\tif (from == to)",
            "\t\treturn;",
            "",
            "\tmutex_lock(&d->mutex);",
            "\tif (!d->image) {",
            "\t\td->image = bpf_prog_pack_alloc(PAGE_SIZE, bpf_jit_fill_hole_with_zero);",
            "\t\tif (!d->image)",
            "\t\t\tgoto out;",
            "\t\td->rw_image = bpf_jit_alloc_exec(PAGE_SIZE);",
            "\t\tif (!d->rw_image) {",
            "\t\t\tbpf_prog_pack_free(d->image, PAGE_SIZE);",
            "\t\t\td->image = NULL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tbpf_image_ksym_init(d->image, PAGE_SIZE, &d->ksym);",
            "\t\tbpf_image_ksym_add(&d->ksym);",
            "\t}",
            "",
            "\tprev_num_progs = d->num_progs;",
            "\tchanged |= bpf_dispatcher_remove_prog(d, from);",
            "\tchanged |= bpf_dispatcher_add_prog(d, to);",
            "",
            "\tif (!changed)",
            "\t\tgoto out;",
            "",
            "\tbpf_dispatcher_update(d, prev_num_progs);",
            "out:",
            "\tmutex_unlock(&d->mutex);",
            "}"
          ],
          "function_name": "bpf_dispatcher_add_prog, bpf_dispatcher_remove_prog, arch_prepare_bpf_dispatcher, bpf_dispatcher_prepare, bpf_dispatcher_update, bpf_dispatcher_change_prog",
          "description": "该代码块实现了BPF调度器的增删改逻辑及图像更新。bpf_dispatcher_add_prog/bpf_dispatcher_remove_prog管理程序注册与解注册，arch_prepare_bpf_dispatcher为架构弱符号接口，bpf_dispatcher_prepare收集程序函数地址，bpf_dispatcher_update执行安全的图像更新，bpf_dispatcher_change_prog协调程序替换流程并触发更新操作。",
          "similarity": 0.45500123500823975
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/dispatcher.c",
          "start_line": 1,
          "end_line": 42,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright(c) 2019 Intel Corporation. */",
            "",
            "#include <linux/hash.h>",
            "#include <linux/bpf.h>",
            "#include <linux/filter.h>",
            "#include <linux/static_call.h>",
            "",
            "/* The BPF dispatcher is a multiway branch code generator. The",
            " * dispatcher is a mechanism to avoid the performance penalty of an",
            " * indirect call, which is expensive when retpolines are enabled. A",
            " * dispatch client registers a BPF program into the dispatcher, and if",
            " * there is available room in the dispatcher a direct call to the BPF",
            " * program will be generated. All calls to the BPF programs called via",
            " * the dispatcher will then be a direct call, instead of an",
            " * indirect. The dispatcher hijacks a trampoline function it via the",
            " * __fentry__ of the trampoline. The trampoline function has the",
            " * following signature:",
            " *",
            " * unsigned int trampoline(const void *ctx, const struct bpf_insn *insnsi,",
            " *                         unsigned int (*bpf_func)(const void *,",
            " *                                                  const struct bpf_insn *));",
            " */",
            "",
            "static struct bpf_dispatcher_prog *bpf_dispatcher_find_prog(",
            "\tstruct bpf_dispatcher *d, struct bpf_prog *prog)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < BPF_DISPATCHER_MAX; i++) {",
            "\t\tif (prog == d->progs[i].prog)",
            "\t\t\treturn &d->progs[i];",
            "\t}",
            "\treturn NULL;",
            "}",
            "",
            "static struct bpf_dispatcher_prog *bpf_dispatcher_find_free(",
            "\tstruct bpf_dispatcher *d)",
            "{",
            "\treturn bpf_dispatcher_find_prog(d, NULL);",
            "}",
            ""
          ],
          "function_name": null,
          "description": "该代码块定义了BPF调度器的辅助函数，用于查找已注册的BPF程序或空闲槽位。bpf_dispatcher_find_prog遍历调度器槽位以匹配指定程序，返回对应项指针；bpf_dispatcher_find_free用于定位未占用的槽位，为后续程序注册做准备。",
          "similarity": 0.454873263835907
        }
      ]
    }
  ]
}