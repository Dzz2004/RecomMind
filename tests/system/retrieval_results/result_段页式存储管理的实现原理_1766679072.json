{
  "query": "段页式存储管理的实现原理",
  "timestamp": "2025-12-26 00:11:12",
  "retrieved_files": [
    {
      "source_file": "mm/page_counter.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:00:45\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_counter.c`\n\n---\n\n# page_counter.c 技术文档\n\n## 1. 文件概述\n\n`page_counter.c` 实现了一个无锁（lockless）的分层页面计数与限制机制，用于在 Linux 内核中对内存资源进行层级化计量、保护和限制。该机制主要用于 cgroup 内存控制器（如 memcg）中，支持对内存使用量进行精确跟踪，并提供 `memory.min` 和 `memory.low` 两种级别的内存保护策略，同时支持设置硬性上限（`memory.max`）。所有操作均基于原子操作实现，避免了传统锁带来的性能开销。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct page_counter`：核心计数器结构体，包含以下关键字段：\n  - `usage`：当前已使用的页面数（原子变量）\n  - `max`：硬性内存上限（可配置）\n  - `min` / `low`：内存保护阈值（软性保障）\n  - `min_usage` / `low_usage`：实际受保护的内存量\n  - `children_min_usage` / `children_low_usage`：子节点受保护内存总量\n  - `watermark`：历史最大使用量（用于统计）\n  - `failcnt`：因超出限制而失败的尝试次数\n  - `parent`：指向父级计数器的指针，构成层级树\n  - `protection_support`：是否启用保护机制的标志\n\n### 主要函数\n| 函数 | 功能说明 |\n|------|--------|\n| `page_counter_charge()` | 无条件地向计数器及其所有祖先层级增加指定页数 |\n| `page_counter_uncharge()` | 向计数器及其祖先层级减少指定页数（调用 `cancel`） |\n| `page_counter_try_charge()` | 尝试充电，若任一层级超过 `max` 则回滚并返回失败 |\n| `page_counter_cancel()` | 从本地计数器减去页数，处理下溢并更新保护用量 |\n| `page_counter_set_max()` | 设置硬性内存上限，若当前用量已超限则返回 `-EBUSY` |\n| `page_counter_set_min()` / `page_counter_set_low()` | 设置内存保护阈值，并触发保护用量传播 |\n| `page_counter_memparse()` | 解析用户输入的字符串（如 \"1G\"）为页数，支持 \"max\" 关键字 |\n| `propagate_protected_usage()` | （内部）根据当前用量和 min/low 阈值，向上更新受保护内存量 |\n\n## 3. 关键实现\n\n### 无锁层级更新\n- 所有计数操作（charge/uncharge）通过 `atomic_long_add_return()` 和 `atomic_long_sub_return()` 实现，确保线程安全。\n- 在 `page_counter_try_charge()` 中采用“先加后检”策略：先原子增加用量，再检查是否超过 `max`。若超限则回退。此方法虽存在短暂超限窗口，但避免了昂贵的 CAS 循环，适用于 THP（透明大页）等场景。\n\n### 内存保护机制\n- 引入 `min` 和 `low` 两级软保护：\n  - `min`：强保障，通常用于关键服务\n  - `low`：弱保障，用于优先级稍低的内存预留\n- `propagate_protected_usage()` 计算每个节点的实际受保护量：`protected = min(usage, threshold)`，并通过 `min_usage`/`low_usage` 原子变量记录，并累加到父节点的 `children_*_usage` 中，供上层决策（如内存回收）使用。\n\n### 水位线与失败计数\n- `watermark` 记录历史峰值用量，用于监控和调优。\n- `failcnt` 统计因超限导致的充电失败次数，仅用于统计信息，允许轻微不一致。\n\n### 安全的 max 更新\n- `page_counter_set_max()` 使用 `xchg()` 原子交换新旧上限，并通过循环验证：若在设置过程中用量增长导致新上限仍不足，则恢复旧值并重试，确保不会将上限设为低于当前用量的值。\n\n### 字符串解析\n- `page_counter_memparse()` 封装 `memparse()`，将用户空间传入的字符串（如 \"512M\"）转换为页数，并支持特殊值 \"max\" 表示 `PAGE_COUNTER_MAX`（即 `ULONG_MAX`）。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/page_counter.h>`：定义 `struct page_counter` 及函数声明\n  - `<linux/atomic.h>`：提供原子操作原语\n  - `<asm/page.h>`：提供 `PAGE_SIZE` 定义\n  - `<linux/kernel.h>`、`<linux/string.h>` 等基础内核头文件\n- **配置依赖**：\n  - 主要服务于 `CONFIG_MEMCG`（内存 cgroup）和 `CONFIG_CGROUP_DMEM`（设备内存 cgroup）\n  - 文件末尾的 `#if IS_ENABLED(...)` 表明其设计初衷是为 cgroup 内存控制器提供底层支持\n- **运行时依赖**：\n  - 依赖内核的原子操作和内存屏障语义保证正确性\n  - 与内存回收（reclaim）逻辑紧密配合，保护用量信息用于决定回收顺序\n\n## 5. 使用场景\n\n- **cgroup v2 内存控制器**：作为 `memory.max`、`memory.min`、`memory.low` 等接口的底层实现，管理容器或进程组的内存限额与保障。\n- **内存服务质量（QoS）**：通过 `min`/`low` 机制为关键应用提供内存预留，防止被普通任务挤占。\n- **内存超售（Overcommit）管理**：在云环境或虚拟化平台中，精确控制各租户的内存使用边界。\n- **透明大页（THP）分配**：`try_charge` 的投机性设计特别优化了 THP（2MB/1GB 页）与普通页（4KB）并发分配时的性能。\n- **系统监控与调优**：通过 `watermark` 和 `failcnt` 提供内存使用峰值和限制冲突的统计信息，辅助系统管理员进行容量规划。",
      "similarity": 0.5871948003768921,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/page_counter.c",
          "start_line": 16,
          "end_line": 132,
          "content": [
            "static bool track_protection(struct page_counter *c)",
            "{",
            "\treturn c->protection_support;",
            "}",
            "static void propagate_protected_usage(struct page_counter *c,",
            "\t\t\t\t      unsigned long usage)",
            "{",
            "\tunsigned long protected, old_protected;",
            "\tlong delta;",
            "",
            "\tif (!c->parent)",
            "\t\treturn;",
            "",
            "\tprotected = min(usage, READ_ONCE(c->min));",
            "\told_protected = atomic_long_read(&c->min_usage);",
            "\tif (protected != old_protected) {",
            "\t\told_protected = atomic_long_xchg(&c->min_usage, protected);",
            "\t\tdelta = protected - old_protected;",
            "\t\tif (delta)",
            "\t\t\tatomic_long_add(delta, &c->parent->children_min_usage);",
            "\t}",
            "",
            "\tprotected = min(usage, READ_ONCE(c->low));",
            "\told_protected = atomic_long_read(&c->low_usage);",
            "\tif (protected != old_protected) {",
            "\t\told_protected = atomic_long_xchg(&c->low_usage, protected);",
            "\t\tdelta = protected - old_protected;",
            "\t\tif (delta)",
            "\t\t\tatomic_long_add(delta, &c->parent->children_low_usage);",
            "\t}",
            "}",
            "void page_counter_cancel(struct page_counter *counter, unsigned long nr_pages)",
            "{",
            "\tlong new;",
            "",
            "\tnew = atomic_long_sub_return(nr_pages, &counter->usage);",
            "\t/* More uncharges than charges? */",
            "\tif (WARN_ONCE(new < 0, \"page_counter underflow: %ld nr_pages=%lu\\n\",",
            "\t\t      new, nr_pages)) {",
            "\t\tnew = 0;",
            "\t\tatomic_long_set(&counter->usage, new);",
            "\t}",
            "\tif (track_protection(counter))",
            "\t\tpropagate_protected_usage(counter, new);",
            "}",
            "void page_counter_charge(struct page_counter *counter, unsigned long nr_pages)",
            "{",
            "\tstruct page_counter *c;",
            "\tbool protection = track_protection(counter);",
            "",
            "\tfor (c = counter; c; c = c->parent) {",
            "\t\tlong new;",
            "",
            "\t\tnew = atomic_long_add_return(nr_pages, &c->usage);",
            "\t\tif (protection)",
            "\t\t\tpropagate_protected_usage(c, new);",
            "\t\t/*",
            "\t\t * This is indeed racy, but we can live with some",
            "\t\t * inaccuracy in the watermark.",
            "\t\t */",
            "\t\tif (new > READ_ONCE(c->watermark))",
            "\t\t\tWRITE_ONCE(c->watermark, new);",
            "\t}",
            "}",
            "bool page_counter_try_charge(struct page_counter *counter,",
            "\t\t\t     unsigned long nr_pages,",
            "\t\t\t     struct page_counter **fail)",
            "{",
            "\tstruct page_counter *c;",
            "\tbool protection = track_protection(counter);",
            "",
            "\tfor (c = counter; c; c = c->parent) {",
            "\t\tlong new;",
            "\t\t/*",
            "\t\t * Charge speculatively to avoid an expensive CAS.  If",
            "\t\t * a bigger charge fails, it might falsely lock out a",
            "\t\t * racing smaller charge and send it into reclaim",
            "\t\t * early, but the error is limited to the difference",
            "\t\t * between the two sizes, which is less than 2M/4M in",
            "\t\t * case of a THP locking out a regular page charge.",
            "\t\t *",
            "\t\t * The atomic_long_add_return() implies a full memory",
            "\t\t * barrier between incrementing the count and reading",
            "\t\t * the limit.  When racing with page_counter_set_max(),",
            "\t\t * we either see the new limit or the setter sees the",
            "\t\t * counter has changed and retries.",
            "\t\t */",
            "\t\tnew = atomic_long_add_return(nr_pages, &c->usage);",
            "\t\tif (new > c->max) {",
            "\t\t\tatomic_long_sub(nr_pages, &c->usage);",
            "\t\t\t/*",
            "\t\t\t * This is racy, but we can live with some",
            "\t\t\t * inaccuracy in the failcnt which is only used",
            "\t\t\t * to report stats.",
            "\t\t\t */",
            "\t\t\tdata_race(c->failcnt++);",
            "\t\t\t*fail = c;",
            "\t\t\tgoto failed;",
            "\t\t}",
            "\t\tif (protection)",
            "\t\t\tpropagate_protected_usage(c, new);",
            "",
            "\t\t/*",
            "\t\t * Just like with failcnt, we can live with some",
            "\t\t * inaccuracy in the watermark.",
            "\t\t */",
            "\t\tif (new > READ_ONCE(c->watermark))",
            "\t\t\tWRITE_ONCE(c->watermark, new);",
            "\t}",
            "\treturn true;",
            "",
            "failed:",
            "\tfor (c = counter; c != *fail; c = c->parent)",
            "\t\tpage_counter_cancel(c, nr_pages);",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "track_protection, propagate_protected_usage, page_counter_cancel, page_counter_charge, page_counter_try_charge",
          "description": "实现页面使用量的追踪与传播逻辑，包含受保护使用量更新、充放电操作及限制检查，通过层级传播维护父子计数器间的最小/低水位线状态同步。",
          "similarity": 0.523342490196228
        },
        {
          "chunk_id": 2,
          "file_path": "mm/page_counter.c",
          "start_line": 164,
          "end_line": 312,
          "content": [
            "void page_counter_uncharge(struct page_counter *counter, unsigned long nr_pages)",
            "{",
            "\tstruct page_counter *c;",
            "",
            "\tfor (c = counter; c; c = c->parent)",
            "\t\tpage_counter_cancel(c, nr_pages);",
            "}",
            "int page_counter_set_max(struct page_counter *counter, unsigned long nr_pages)",
            "{",
            "\tfor (;;) {",
            "\t\tunsigned long old;",
            "\t\tlong usage;",
            "",
            "\t\t/*",
            "\t\t * Update the limit while making sure that it's not",
            "\t\t * below the concurrently-changing counter value.",
            "\t\t *",
            "\t\t * The xchg implies two full memory barriers before",
            "\t\t * and after, so the read-swap-read is ordered and",
            "\t\t * ensures coherency with page_counter_try_charge():",
            "\t\t * that function modifies the count before checking",
            "\t\t * the limit, so if it sees the old limit, we see the",
            "\t\t * modified counter and retry.",
            "\t\t */",
            "\t\tusage = page_counter_read(counter);",
            "",
            "\t\tif (usage > nr_pages)",
            "\t\t\treturn -EBUSY;",
            "",
            "\t\told = xchg(&counter->max, nr_pages);",
            "",
            "\t\tif (page_counter_read(counter) <= usage || nr_pages >= old)",
            "\t\t\treturn 0;",
            "",
            "\t\tcounter->max = old;",
            "\t\tcond_resched();",
            "\t}",
            "}",
            "void page_counter_set_min(struct page_counter *counter, unsigned long nr_pages)",
            "{",
            "\tstruct page_counter *c;",
            "",
            "\tWRITE_ONCE(counter->min, nr_pages);",
            "",
            "\tfor (c = counter; c; c = c->parent)",
            "\t\tpropagate_protected_usage(c, atomic_long_read(&c->usage));",
            "}",
            "void page_counter_set_low(struct page_counter *counter, unsigned long nr_pages)",
            "{",
            "\tstruct page_counter *c;",
            "",
            "\tWRITE_ONCE(counter->low, nr_pages);",
            "",
            "\tfor (c = counter; c; c = c->parent)",
            "\t\tpropagate_protected_usage(c, atomic_long_read(&c->usage));",
            "}",
            "int page_counter_memparse(const char *buf, const char *max,",
            "\t\t\t  unsigned long *nr_pages)",
            "{",
            "\tchar *end;",
            "\tu64 bytes;",
            "",
            "\tif (!strcmp(buf, max)) {",
            "\t\t*nr_pages = PAGE_COUNTER_MAX;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tbytes = memparse(buf, &end);",
            "\tif (*end != '\\0')",
            "\t\treturn -EINVAL;",
            "",
            "\t*nr_pages = min(bytes / PAGE_SIZE, (u64)PAGE_COUNTER_MAX);",
            "",
            "\treturn 0;",
            "}",
            "static unsigned long effective_protection(unsigned long usage,",
            "\t\t\t\t\t  unsigned long parent_usage,",
            "\t\t\t\t\t  unsigned long setting,",
            "\t\t\t\t\t  unsigned long parent_effective,",
            "\t\t\t\t\t  unsigned long siblings_protected,",
            "\t\t\t\t\t  bool recursive_protection)",
            "{",
            "\tunsigned long protected;",
            "\tunsigned long ep;",
            "",
            "\tprotected = min(usage, setting);",
            "\t/*",
            "\t * If all cgroups at this level combined claim and use more",
            "\t * protection than what the parent affords them, distribute",
            "\t * shares in proportion to utilization.",
            "\t *",
            "\t * We are using actual utilization rather than the statically",
            "\t * claimed protection in order to be work-conserving: claimed",
            "\t * but unused protection is available to siblings that would",
            "\t * otherwise get a smaller chunk than what they claimed.",
            "\t */",
            "\tif (siblings_protected > parent_effective)",
            "\t\treturn protected * parent_effective / siblings_protected;",
            "",
            "\t/*",
            "\t * Ok, utilized protection of all children is within what the",
            "\t * parent affords them, so we know whatever this child claims",
            "\t * and utilizes is effectively protected.",
            "\t *",
            "\t * If there is unprotected usage beyond this value, reclaim",
            "\t * will apply pressure in proportion to that amount.",
            "\t *",
            "\t * If there is unutilized protection, the cgroup will be fully",
            "\t * shielded from reclaim, but we do return a smaller value for",
            "\t * protection than what the group could enjoy in theory. This",
            "\t * is okay. With the overcommit distribution above, effective",
            "\t * protection is always dependent on how memory is actually",
            "\t * consumed among the siblings anyway.",
            "\t */",
            "\tep = protected;",
            "",
            "\t/*",
            "\t * If the children aren't claiming (all of) the protection",
            "\t * afforded to them by the parent, distribute the remainder in",
            "\t * proportion to the (unprotected) memory of each cgroup. That",
            "\t * way, cgroups that aren't explicitly prioritized wrt each",
            "\t * other compete freely over the allowance, but they are",
            "\t * collectively protected from neighboring trees.",
            "\t *",
            "\t * We're using unprotected memory for the weight so that if",
            "\t * some cgroups DO claim explicit protection, we don't protect",
            "\t * the same bytes twice.",
            "\t *",
            "\t * Check both usage and parent_usage against the respective",
            "\t * protected values. One should imply the other, but they",
            "\t * aren't read atomically - make sure the division is sane.",
            "\t */",
            "\tif (!recursive_protection)",
            "\t\treturn ep;",
            "",
            "\tif (parent_effective > siblings_protected &&",
            "\t    parent_usage > siblings_protected &&",
            "\t    usage > protected) {",
            "\t\tunsigned long unclaimed;",
            "",
            "\t\tunclaimed = parent_effective - siblings_protected;",
            "\t\tunclaimed *= usage - protected;",
            "\t\tunclaimed /= parent_usage - siblings_protected;",
            "",
            "\t\tep += unclaimed;",
            "\t}",
            "",
            "\treturn ep;",
            "}"
          ],
          "function_name": "page_counter_uncharge, page_counter_set_max, page_counter_set_min, page_counter_set_low, page_counter_memparse, effective_protection",
          "description": "提供计数器阈值配置接口及内存参数解析，包含最大值设置、最小/低水位线更新和有效保护计算，实现基于实际使用情况的动态保护分配算法。",
          "similarity": 0.4903552532196045
        },
        {
          "chunk_id": 3,
          "file_path": "mm/page_counter.c",
          "start_line": 409,
          "end_line": 449,
          "content": [
            "void page_counter_calculate_protection(struct page_counter *root,",
            "\t\t\t\t       struct page_counter *counter,",
            "\t\t\t\t       bool recursive_protection)",
            "{",
            "\tunsigned long usage, parent_usage;",
            "\tstruct page_counter *parent = counter->parent;",
            "",
            "\t/*",
            "\t * Effective values of the reclaim targets are ignored so they",
            "\t * can be stale. Have a look at mem_cgroup_protection for more",
            "\t * details.",
            "\t * TODO: calculation should be more robust so that we do not need",
            "\t * that special casing.",
            "\t */",
            "\tif (root == counter)",
            "\t\treturn;",
            "",
            "\tusage = page_counter_read(counter);",
            "\tif (!usage)",
            "\t\treturn;",
            "",
            "\tif (parent == root) {",
            "\t\tcounter->emin = READ_ONCE(counter->min);",
            "\t\tcounter->elow = READ_ONCE(counter->low);",
            "\t\treturn;",
            "\t}",
            "",
            "\tparent_usage = page_counter_read(parent);",
            "",
            "\tWRITE_ONCE(counter->emin, effective_protection(usage, parent_usage,",
            "\t\t\tREAD_ONCE(counter->min),",
            "\t\t\tREAD_ONCE(parent->emin),",
            "\t\t\tatomic_long_read(&parent->children_min_usage),",
            "\t\t\trecursive_protection));",
            "",
            "\tWRITE_ONCE(counter->elow, effective_protection(usage, parent_usage,",
            "\t\t\tREAD_ONCE(counter->low),",
            "\t\t\tREAD_ONCE(parent->elow),",
            "\t\t\tatomic_long_read(&parent->children_low_usage),",
            "\t\t\trecursive_protection));",
            "}"
          ],
          "function_name": "page_counter_calculate_protection",
          "description": "计算并设置计数器的有效保护值，通过递归遍历层级关系，结合父级保护策略和当前使用比例，动态调整子级的最小/低水位线保护范围。",
          "similarity": 0.4854236841201782
        },
        {
          "chunk_id": 0,
          "file_path": "mm/page_counter.c",
          "start_line": 1,
          "end_line": 15,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Lockless hierarchical page accounting & limiting",
            " *",
            " * Copyright (C) 2014 Red Hat, Inc., Johannes Weiner",
            " */",
            "",
            "#include <linux/page_counter.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/string.h>",
            "#include <linux/sched.h>",
            "#include <linux/bug.h>",
            "#include <asm/page.h>",
            ""
          ],
          "function_name": null,
          "description": "定义页面计数器相关头文件及基础宏，提供锁无关的分层页面计数与限制机制框架，声明核心结构体和辅助函数原型，为后续实现提供基础设施。",
          "similarity": 0.48039883375167847
        }
      ]
    },
    {
      "source_file": "mm/memory.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:42:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memory.c`\n\n---\n\n# memory.c 技术文档\n\n## 1. 文件概述\n\n`memory.c` 是 Linux 内核内存管理子系统（MM）的核心实现文件之一，位于 `mm/` 目录下。该文件主要负责虚拟内存到物理内存的映射管理、缺页异常（page fault）处理、页表结构的分配与释放、以及与用户空间内存操作相关的底层机制。它实现了按需加载（demand-loading）、共享页面、交换（swapping）等关键虚拟内存功能，并为多级页表架构（如 x86-64 的四级页表）提供通用支持。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `high_memory`：指向直接映射区域（ZONE_NORMAL）的上界，用于区分低端内存和高端内存。\n- `randomize_va_space`：控制地址空间布局随机化（ASLR）策略的级别（0=关闭，1=部分启用，2=完全启用）。\n- `zero_pfn`：指向全零物理页的页帧号（PFN），用于高效实现只读零页映射。\n- `max_mapnr` 和 `mem_map`（非 NUMA 配置下）：分别表示最大页帧号和全局页描述符数组。\n- `highest_memmap_pfn`：记录系统中最高的已注册页帧号。\n\n### 关键函数\n- `free_pgd_range()`：释放指定虚拟地址范围内的用户级页表结构（从 PGD 到 PTE）。\n- `free_p4d_range()`, `free_pud_range()`, `free_pmd_range()`, `free_pte_range()`：递归释放各级页表项及其对应的页表页。\n- `do_fault()`：处理基于文件映射的缺页异常。\n- `do_anonymous_page()`：处理匿名映射（如堆、栈）的缺页异常。\n- `vmf_orig_pte_uffd_wp()`：判断原始 PTE 是否为 userfaultfd 写保护标记。\n- `init_zero_pfn()`：早期初始化 `zero_pfn`。\n- `mm_trace_rss_stat()`：触发 RSS（Resident Set Size）统计的跟踪事件。\n\n### 内联辅助函数\n- `arch_wants_old_prefaulted_pte()`：允许架构层决定预取页表项是否应标记为“old”以优化访问位更新开销。\n\n## 3. 关键实现\n\n### 页表释放机制\n- 采用自顶向下（PGD → P4D → PUD → PMD → PTE）的递归方式释放页表。\n- 每级释放函数（如 `free_pmd_range`）遍历地址范围内的页表项：\n  - 跳过空或无效项（`pmd_none_or_clear_bad`）。\n  - 递归释放下一级页表。\n  - 在满足对齐和边界条件（`floor`/`ceiling`）时，释放当前级页表页并更新 MMU gather 结构中的计数器（如 `mm_dec_nr_ptes`）。\n- 使用 `mmu_gather` 机制批量延迟 TLB 刷新和页表页释放，提升性能。\n\n### 缺页处理框架\n- 提供 `do_fault` 和 `do_anonymous_page` 作为缺页处理的核心入口，分别处理文件映射和匿名映射。\n- 支持 `userfaultfd` 写保护机制，通过 `vmf_orig_pte_uffd_wp` 检测特殊 PTE 标记。\n\n### 地址空间随机化（ASLR）\n- 通过 `randomize_va_space` 控制栈、mmap 区域、brk 等的随机化行为。\n- 支持内核启动参数 `norandmaps` 完全禁用 ASLR。\n- 兼容旧版 libc5 二进制（`CONFIG_COMPAT_BRK`），此时 brk 区域不参与随机化。\n\n### 零页优化\n- `zero_pfn` 指向一个全局只读的全零物理页，用于高效实现对未初始化数据段（如 `.bss`）或显式映射 `/dev/zero` 的只读访问，避免每次分配新页。\n\n### 架构适配\n- 通过 `arch_wants_old_prefaulted_pte` 允许特定架构优化页表项的“young/old”状态设置。\n- 依赖 `asm/mmu_context.h`、`asm/pgalloc.h`、`asm/tlb.h` 等架构相关头文件实现底层操作。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- **内存管理核心**：`<linux/mm.h>`, `<linux/mman.h>`, `<linux/swap.h>`, `<linux/pagemap.h>`, `<linux/memcontrol.h>`\n- **进程与调度**：`<linux/sched/mm.h>`, `<linux/sched/task.h>`, `<linux/delayacct.h>`\n- **NUMA 与迁移**：`<linux/numa.h>`, `<linux/migrate.h>`, `<linux/sched/numa_balancing.h>`\n- **特殊内存类型**：`<linux/hugetlb.h>`, `<linux/highmem.h>`, `<linux/dax.h>`, `<linux/zswap.h>`\n- **调试与跟踪**：`<trace/events/kmem.h>`, `<linux/debugfs.h>`, `<linux/oom.h>`\n- **架构相关**：`<asm/io.h>`, `<asm/mmu_context.h>`, `<asm/pgalloc.h>`, `<asm/tlbflush.h>`\n\n### 内部模块依赖\n- `internal.h`：包含 MM 子系统内部通用定义。\n- `swap.h`：交换子系统接口。\n- `pgalloc-track.h`：页表分配跟踪（用于调试）。\n\n## 5. 使用场景\n\n- **进程创建与退出**：在 `fork()` 和进程终止时，通过 `free_pgd_range` 释放整个地址空间的页表。\n- **内存映射操作**：`mmap()`、`munmap()`、`mremap()` 等系统调用触发页表的建立或释放。\n- **缺页异常处理**：当 CPU 访问未映射或换出的虚拟地址时，由体系结构相关的缺页处理程序调用 `do_fault` 或 `do_anonymous_page`。\n- **内存回收**：在内存压力下，kswapd 或直接回收路径可能触发页表清理。\n- **用户态内存监控**：`userfaultfd` 机制利用 `vmf_orig_pte_uffd_wp` 实现用户空间对缺页事件的精细控制。\n- **内核初始化**：早期调用 `init_zero_pfn` 设置零页，`paging_init()`（架构相关）初始化 `high_memory` 和 `ZERO_PAGE`。",
      "similarity": 0.5856478810310364,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/memory.c",
          "start_line": 267,
          "end_line": 414,
          "content": [
            "static inline void free_p4d_range(struct mmu_gather *tlb, pgd_t *pgd,",
            "\t\t\t\tunsigned long addr, unsigned long end,",
            "\t\t\t\tunsigned long floor, unsigned long ceiling)",
            "{",
            "\tp4d_t *p4d;",
            "\tunsigned long next;",
            "\tunsigned long start;",
            "",
            "\tstart = addr;",
            "\tp4d = p4d_offset(pgd, addr);",
            "\tdo {",
            "\t\tnext = p4d_addr_end(addr, end);",
            "\t\tif (p4d_none_or_clear_bad(p4d))",
            "\t\t\tcontinue;",
            "\t\tfree_pud_range(tlb, p4d, addr, next, floor, ceiling);",
            "\t} while (p4d++, addr = next, addr != end);",
            "",
            "\tstart &= PGDIR_MASK;",
            "\tif (start < floor)",
            "\t\treturn;",
            "\tif (ceiling) {",
            "\t\tceiling &= PGDIR_MASK;",
            "\t\tif (!ceiling)",
            "\t\t\treturn;",
            "\t}",
            "\tif (end - 1 > ceiling - 1)",
            "\t\treturn;",
            "",
            "\tp4d = p4d_offset(pgd, start);",
            "\tpgd_clear(pgd);",
            "\tp4d_free_tlb(tlb, p4d, start);",
            "}",
            "void free_pgd_range(struct mmu_gather *tlb,",
            "\t\t\tunsigned long addr, unsigned long end,",
            "\t\t\tunsigned long floor, unsigned long ceiling)",
            "{",
            "\tpgd_t *pgd;",
            "\tunsigned long next;",
            "",
            "\t/*",
            "\t * The next few lines have given us lots of grief...",
            "\t *",
            "\t * Why are we testing PMD* at this top level?  Because often",
            "\t * there will be no work to do at all, and we'd prefer not to",
            "\t * go all the way down to the bottom just to discover that.",
            "\t *",
            "\t * Why all these \"- 1\"s?  Because 0 represents both the bottom",
            "\t * of the address space and the top of it (using -1 for the",
            "\t * top wouldn't help much: the masks would do the wrong thing).",
            "\t * The rule is that addr 0 and floor 0 refer to the bottom of",
            "\t * the address space, but end 0 and ceiling 0 refer to the top",
            "\t * Comparisons need to use \"end - 1\" and \"ceiling - 1\" (though",
            "\t * that end 0 case should be mythical).",
            "\t *",
            "\t * Wherever addr is brought up or ceiling brought down, we must",
            "\t * be careful to reject \"the opposite 0\" before it confuses the",
            "\t * subsequent tests.  But what about where end is brought down",
            "\t * by PMD_SIZE below? no, end can't go down to 0 there.",
            "\t *",
            "\t * Whereas we round start (addr) and ceiling down, by different",
            "\t * masks at different levels, in order to test whether a table",
            "\t * now has no other vmas using it, so can be freed, we don't",
            "\t * bother to round floor or end up - the tests don't need that.",
            "\t */",
            "",
            "\taddr &= PMD_MASK;",
            "\tif (addr < floor) {",
            "\t\taddr += PMD_SIZE;",
            "\t\tif (!addr)",
            "\t\t\treturn;",
            "\t}",
            "\tif (ceiling) {",
            "\t\tceiling &= PMD_MASK;",
            "\t\tif (!ceiling)",
            "\t\t\treturn;",
            "\t}",
            "\tif (end - 1 > ceiling - 1)",
            "\t\tend -= PMD_SIZE;",
            "\tif (addr > end - 1)",
            "\t\treturn;",
            "\t/*",
            "\t * We add page table cache pages with PAGE_SIZE,",
            "\t * (see pte_free_tlb()), flush the tlb if we need",
            "\t */",
            "\ttlb_change_page_size(tlb, PAGE_SIZE);",
            "\tpgd = pgd_offset(tlb->mm, addr);",
            "\tdo {",
            "\t\tnext = pgd_addr_end(addr, end);",
            "\t\tif (pgd_none_or_clear_bad(pgd))",
            "\t\t\tcontinue;",
            "\t\tfree_p4d_range(tlb, pgd, addr, next, floor, ceiling);",
            "\t} while (pgd++, addr = next, addr != end);",
            "}",
            "void free_pgtables(struct mmu_gather *tlb, struct ma_state *mas,",
            "\t\t   struct vm_area_struct *vma, unsigned long floor,",
            "\t\t   unsigned long ceiling, bool mm_wr_locked)",
            "{",
            "\tstruct unlink_vma_file_batch vb;",
            "",
            "\tdo {",
            "\t\tunsigned long addr = vma->vm_start;",
            "\t\tstruct vm_area_struct *next;",
            "",
            "\t\t/*",
            "\t\t * Note: USER_PGTABLES_CEILING may be passed as ceiling and may",
            "\t\t * be 0.  This will underflow and is okay.",
            "\t\t */",
            "\t\tnext = mas_find(mas, ceiling - 1);",
            "\t\tif (unlikely(xa_is_zero(next)))",
            "\t\t\tnext = NULL;",
            "",
            "\t\t/*",
            "\t\t * Hide vma from rmap and truncate_pagecache before freeing",
            "\t\t * pgtables",
            "\t\t */",
            "\t\tif (mm_wr_locked)",
            "\t\t\tvma_start_write(vma);",
            "\t\tunlink_anon_vmas(vma);",
            "",
            "\t\tif (is_vm_hugetlb_page(vma)) {",
            "\t\t\tunlink_file_vma(vma);",
            "\t\t\thugetlb_free_pgd_range(tlb, addr, vma->vm_end,",
            "\t\t\t\tfloor, next ? next->vm_start : ceiling);",
            "\t\t} else {",
            "\t\t\tunlink_file_vma_batch_init(&vb);",
            "\t\t\tunlink_file_vma_batch_add(&vb, vma);",
            "",
            "\t\t\t/*",
            "\t\t\t * Optimization: gather nearby vmas into one call down",
            "\t\t\t */",
            "\t\t\twhile (next && next->vm_start <= vma->vm_end + PMD_SIZE",
            "\t\t\t       && !is_vm_hugetlb_page(next)) {",
            "\t\t\t\tvma = next;",
            "\t\t\t\tnext = mas_find(mas, ceiling - 1);",
            "\t\t\t\tif (unlikely(xa_is_zero(next)))",
            "\t\t\t\t\tnext = NULL;",
            "\t\t\t\tif (mm_wr_locked)",
            "\t\t\t\t\tvma_start_write(vma);",
            "\t\t\t\tunlink_anon_vmas(vma);",
            "\t\t\t\tunlink_file_vma_batch_add(&vb, vma);",
            "\t\t\t}",
            "\t\t\tunlink_file_vma_batch_final(&vb);",
            "\t\t\tfree_pgd_range(tlb, addr, vma->vm_end,",
            "\t\t\t\tfloor, next ? next->vm_start : ceiling);",
            "\t\t}",
            "\t\tvma = next;",
            "\t} while (vma);",
            "}"
          ],
          "function_name": "free_p4d_range, free_pgd_range, free_pgtables",
          "description": "实现多级页表（PGD/PMD/PUD/P4D）的释放逻辑，通过遍历地址范围清理无效页表项并减少相应的页表计数",
          "similarity": 0.576754093170166
        },
        {
          "chunk_id": 8,
          "file_path": "mm/memory.c",
          "start_line": 1353,
          "end_line": 1469,
          "content": [
            "int",
            "copy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)",
            "{",
            "\tpgd_t *src_pgd, *dst_pgd;",
            "\tunsigned long addr = src_vma->vm_start;",
            "\tunsigned long end = src_vma->vm_end;",
            "\tstruct mm_struct *dst_mm = dst_vma->vm_mm;",
            "\tstruct mm_struct *src_mm = src_vma->vm_mm;",
            "\tstruct mmu_notifier_range range;",
            "\tunsigned long next, pfn;",
            "\tbool is_cow;",
            "\tint ret;",
            "",
            "\tif (!vma_needs_copy(dst_vma, src_vma))",
            "\t\treturn 0;",
            "",
            "\tif (is_vm_hugetlb_page(src_vma))",
            "\t\treturn copy_hugetlb_page_range(dst_mm, src_mm, dst_vma, src_vma);",
            "",
            "\tif (unlikely(src_vma->vm_flags & VM_PFNMAP)) {",
            "\t\tret = track_pfn_copy(dst_vma, src_vma, &pfn);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t}",
            "",
            "\t/*",
            "\t * We need to invalidate the secondary MMU mappings only when",
            "\t * there could be a permission downgrade on the ptes of the",
            "\t * parent mm. And a permission downgrade will only happen if",
            "\t * is_cow_mapping() returns true.",
            "\t */",
            "\tis_cow = is_cow_mapping(src_vma->vm_flags);",
            "",
            "\tif (is_cow) {",
            "\t\tmmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_PAGE,",
            "\t\t\t\t\t0, src_mm, addr, end);",
            "\t\tmmu_notifier_invalidate_range_start(&range);",
            "\t\t/*",
            "\t\t * Disabling preemption is not needed for the write side, as",
            "\t\t * the read side doesn't spin, but goes to the mmap_lock.",
            "\t\t *",
            "\t\t * Use the raw variant of the seqcount_t write API to avoid",
            "\t\t * lockdep complaining about preemptibility.",
            "\t\t */",
            "\t\tvma_assert_write_locked(src_vma);",
            "\t\traw_write_seqcount_begin(&src_mm->write_protect_seq);",
            "\t}",
            "",
            "\tret = 0;",
            "\tdst_pgd = pgd_offset(dst_mm, addr);",
            "\tsrc_pgd = pgd_offset(src_mm, addr);",
            "\tdo {",
            "\t\tnext = pgd_addr_end(addr, end);",
            "\t\tif (pgd_none_or_clear_bad(src_pgd))",
            "\t\t\tcontinue;",
            "\t\tif (unlikely(copy_p4d_range(dst_vma, src_vma, dst_pgd, src_pgd,",
            "\t\t\t\t\t    addr, next))) {",
            "\t\t\tret = -ENOMEM;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t} while (dst_pgd++, src_pgd++, addr = next, addr != end);",
            "",
            "\tif (is_cow) {",
            "\t\traw_write_seqcount_end(&src_mm->write_protect_seq);",
            "\t\tmmu_notifier_invalidate_range_end(&range);",
            "\t}",
            "\tif (ret && unlikely(src_vma->vm_flags & VM_PFNMAP))",
            "\t\tuntrack_pfn_copy(dst_vma, pfn);",
            "\treturn ret;",
            "}",
            "static inline bool should_zap_cows(struct zap_details *details)",
            "{",
            "\t/* By default, zap all pages */",
            "\tif (!details)",
            "\t\treturn true;",
            "",
            "\t/* Or, we zap COWed pages only if the caller wants to */",
            "\treturn details->even_cows;",
            "}",
            "static inline bool should_zap_folio(struct zap_details *details,",
            "\t\t\t\t    struct folio *folio)",
            "{",
            "\t/* If we can make a decision without *folio.. */",
            "\tif (should_zap_cows(details))",
            "\t\treturn true;",
            "",
            "\t/* Otherwise we should only zap non-anon folios */",
            "\treturn !folio_test_anon(folio);",
            "}",
            "static inline bool zap_drop_file_uffd_wp(struct zap_details *details)",
            "{",
            "\tif (!details)",
            "\t\treturn false;",
            "",
            "\treturn details->zap_flags & ZAP_FLAG_DROP_MARKER;",
            "}",
            "static inline void",
            "zap_install_uffd_wp_if_needed(struct vm_area_struct *vma,",
            "\t\t\t      unsigned long addr, pte_t *pte, int nr,",
            "\t\t\t      struct zap_details *details, pte_t pteval)",
            "{",
            "\t/* Zap on anonymous always means dropping everything */",
            "\tif (vma_is_anonymous(vma))",
            "\t\treturn;",
            "",
            "\tif (zap_drop_file_uffd_wp(details))",
            "\t\treturn;",
            "",
            "\tfor (;;) {",
            "\t\t/* the PFN in the PTE is irrelevant. */",
            "\t\tpte_install_uffd_wp_if_needed(vma, addr, pte, pteval);",
            "\t\tif (--nr == 0)",
            "\t\t\tbreak;",
            "\t\tpte++;",
            "\t\taddr += PAGE_SIZE;",
            "\t}",
            "}"
          ],
          "function_name": "copy_page_range, should_zap_cows, should_zap_folio, zap_drop_file_uffd_wp, zap_install_uffd_wp_if_needed",
          "description": "执行完整页表复制流程，初始化内存保护通知范围，调用各层级页表复制函数，并处理特殊映射（如PFNMAP）的复制逻辑，包含清除COW页面的相关控制逻辑。",
          "similarity": 0.5661869645118713
        },
        {
          "chunk_id": 25,
          "file_path": "mm/memory.c",
          "start_line": 4037,
          "end_line": 4496,
          "content": [
            "static inline unsigned long thp_swap_suitable_orders(pgoff_t swp_offset,",
            "\t\t\t\t\t\t     unsigned long addr,",
            "\t\t\t\t\t\t     unsigned long orders)",
            "{",
            "\tint order, nr;",
            "",
            "\torder = highest_order(orders);",
            "",
            "\t/*",
            "\t * To swap in a THP with nr pages, we require that its first swap_offset",
            "\t * is aligned with that number, as it was when the THP was swapped out.",
            "\t * This helps filter out most invalid entries.",
            "\t */",
            "\twhile (orders) {",
            "\t\tnr = 1 << order;",
            "\t\tif ((addr >> PAGE_SHIFT) % nr == swp_offset % nr)",
            "\t\t\tbreak;",
            "\t\torder = next_order(&orders, order);",
            "\t}",
            "",
            "\treturn orders;",
            "}",
            "vm_fault_t do_swap_page(struct vm_fault *vmf)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tstruct folio *swapcache, *folio = NULL;",
            "\tDECLARE_WAITQUEUE(wait, current);",
            "\tstruct page *page;",
            "\tstruct swap_info_struct *si = NULL;",
            "\trmap_t rmap_flags = RMAP_NONE;",
            "\tbool need_clear_cache = false;",
            "\tbool exclusive = false;",
            "\tswp_entry_t entry;",
            "\tpte_t pte;",
            "\tvm_fault_t ret = 0;",
            "\tvoid *shadow = NULL;",
            "\tint nr_pages;",
            "\tunsigned long page_idx;",
            "\tunsigned long address;",
            "\tpte_t *ptep;",
            "",
            "\tif (!pte_unmap_same(vmf))",
            "\t\tgoto out;",
            "",
            "\tentry = pte_to_swp_entry(vmf->orig_pte);",
            "\tif (unlikely(non_swap_entry(entry))) {",
            "\t\tif (is_migration_entry(entry)) {",
            "\t\t\tmigration_entry_wait(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\t     vmf->address);",
            "\t\t} else if (is_device_exclusive_entry(entry)) {",
            "\t\t\tvmf->page = pfn_swap_entry_to_page(entry);",
            "\t\t\tret = remove_device_exclusive_entry(vmf);",
            "\t\t} else if (is_device_private_entry(entry)) {",
            "\t\t\tif (vmf->flags & FAULT_FLAG_VMA_LOCK) {",
            "\t\t\t\t/*",
            "\t\t\t\t * migrate_to_ram is not yet ready to operate",
            "\t\t\t\t * under VMA lock.",
            "\t\t\t\t */",
            "\t\t\t\tvma_end_read(vma);",
            "\t\t\t\tret = VM_FAULT_RETRY;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "",
            "\t\t\tvmf->page = pfn_swap_entry_to_page(entry);",
            "\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\tvmf->address, &vmf->ptl);",
            "\t\t\tif (unlikely(!vmf->pte ||",
            "\t\t\t\t     !pte_same(ptep_get(vmf->pte),",
            "\t\t\t\t\t\t\tvmf->orig_pte)))",
            "\t\t\t\tgoto unlock;",
            "",
            "\t\t\t/*",
            "\t\t\t * Get a page reference while we know the page can't be",
            "\t\t\t * freed.",
            "\t\t\t */",
            "\t\t\tget_page(vmf->page);",
            "\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\t\t\tret = vmf->page->pgmap->ops->migrate_to_ram(vmf);",
            "\t\t\tput_page(vmf->page);",
            "\t\t} else if (is_hwpoison_entry(entry)) {",
            "\t\t\tret = VM_FAULT_HWPOISON;",
            "\t\t} else if (is_pte_marker_entry(entry)) {",
            "\t\t\tret = handle_pte_marker(vmf);",
            "\t\t} else {",
            "\t\t\tprint_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);",
            "\t\t\tret = VM_FAULT_SIGBUS;",
            "\t\t}",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* Prevent swapoff from happening to us. */",
            "\tsi = get_swap_device(entry);",
            "\tif (unlikely(!si))",
            "\t\tgoto out;",
            "",
            "\tfolio = swap_cache_get_folio(entry, vma, vmf->address);",
            "\tif (folio)",
            "\t\tpage = folio_file_page(folio, swp_offset(entry));",
            "\tswapcache = folio;",
            "",
            "\tif (!folio) {",
            "\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO) &&",
            "\t\t    __swap_count(entry) == 1) {",
            "\t\t\t/* skip swapcache */",
            "\t\t\tfolio = alloc_swap_folio(vmf);",
            "\t\t\tpage = &folio->page;",
            "\t\t\tif (folio) {",
            "\t\t\t\t__folio_set_locked(folio);",
            "\t\t\t\t__folio_set_swapbacked(folio);",
            "",
            "\t\t\t\tnr_pages = folio_nr_pages(folio);",
            "\t\t\t\tif (folio_test_large(folio))",
            "\t\t\t\t\tentry.val = ALIGN_DOWN(entry.val, nr_pages);",
            "\t\t\t\t/*",
            "\t\t\t\t * Prevent parallel swapin from proceeding with",
            "\t\t\t\t * the cache flag. Otherwise, another thread",
            "\t\t\t\t * may finish swapin first, free the entry, and",
            "\t\t\t\t * swapout reusing the same entry. It's",
            "\t\t\t\t * undetectable as pte_same() returns true due",
            "\t\t\t\t * to entry reuse.",
            "\t\t\t\t */",
            "\t\t\t\tif (swapcache_prepare(entry, nr_pages)) {",
            "\t\t\t\t\t/*",
            "\t\t\t\t\t * Relax a bit to prevent rapid",
            "\t\t\t\t\t * repeated page faults.",
            "\t\t\t\t\t */",
            "\t\t\t\t\tadd_wait_queue(&swapcache_wq, &wait);",
            "\t\t\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\t\t\tremove_wait_queue(&swapcache_wq, &wait);",
            "\t\t\t\t\tgoto out_page;",
            "\t\t\t\t}",
            "\t\t\t\tneed_clear_cache = true;",
            "",
            "\t\t\t\tmem_cgroup_swapin_uncharge_swap(entry, nr_pages);",
            "",
            "\t\t\t\tshadow = get_shadow_from_swap_cache(entry);",
            "\t\t\t\tif (shadow)",
            "\t\t\t\t\tworkingset_refault(folio, shadow);",
            "",
            "\t\t\t\tfolio_add_lru(folio);",
            "",
            "\t\t\t\t/* To provide entry to swap_read_folio() */",
            "\t\t\t\tfolio->swap = entry;",
            "\t\t\t\tswap_read_folio(folio, NULL);",
            "\t\t\t\tfolio->private = NULL;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tpage = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,",
            "\t\t\t\t\t\tvmf);",
            "\t\t\tif (page)",
            "\t\t\t\tfolio = page_folio(page);",
            "\t\t\tswapcache = folio;",
            "\t\t}",
            "",
            "\t\tif (!folio) {",
            "\t\t\t/*",
            "\t\t\t * Back out if somebody else faulted in this pte",
            "\t\t\t * while we released the pte lock.",
            "\t\t\t */",
            "\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\tvmf->address, &vmf->ptl);",
            "\t\t\tif (likely(vmf->pte &&",
            "\t\t\t\t   pte_same(ptep_get(vmf->pte), vmf->orig_pte)))",
            "\t\t\t\tret = VM_FAULT_OOM;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "",
            "\t\t/* Had to read the page from swap area: Major fault */",
            "\t\tret = VM_FAULT_MAJOR;",
            "\t\tcount_vm_event(PGMAJFAULT);",
            "\t\tcount_memcg_event_mm(vma->vm_mm, PGMAJFAULT);",
            "\t} else if (PageHWPoison(page)) {",
            "\t\t/*",
            "\t\t * hwpoisoned dirty swapcache pages are kept for killing",
            "\t\t * owner processes (which may be unknown at hwpoison time)",
            "\t\t */",
            "\t\tret = VM_FAULT_HWPOISON;",
            "\t\tgoto out_release;",
            "\t}",
            "",
            "\tret |= folio_lock_or_retry(folio, vmf);",
            "\tif (ret & VM_FAULT_RETRY)",
            "\t\tgoto out_release;",
            "",
            "\tif (swapcache) {",
            "\t\t/*",
            "\t\t * Make sure folio_free_swap() or swapoff did not release the",
            "\t\t * swapcache from under us.  The page pin, and pte_same test",
            "\t\t * below, are not enough to exclude that.  Even if it is still",
            "\t\t * swapcache, we need to check that the page's swap has not",
            "\t\t * changed.",
            "\t\t */",
            "\t\tif (unlikely(!folio_test_swapcache(folio) ||",
            "\t\t\t     page_swap_entry(page).val != entry.val))",
            "\t\t\tgoto out_page;",
            "",
            "\t\t/*",
            "\t\t * KSM sometimes has to copy on read faults, for example, if",
            "\t\t * page->index of !PageKSM() pages would be nonlinear inside the",
            "\t\t * anon VMA -- PageKSM() is lost on actual swapout.",
            "\t\t */",
            "\t\tfolio = ksm_might_need_to_copy(folio, vma, vmf->address);",
            "\t\tif (unlikely(!folio)) {",
            "\t\t\tret = VM_FAULT_OOM;",
            "\t\t\tfolio = swapcache;",
            "\t\t\tgoto out_page;",
            "\t\t} else if (unlikely(folio == ERR_PTR(-EHWPOISON))) {",
            "\t\t\tret = VM_FAULT_HWPOISON;",
            "\t\t\tfolio = swapcache;",
            "\t\t\tgoto out_page;",
            "\t\t}",
            "\t\tif (folio != swapcache)",
            "\t\t\tpage = folio_page(folio, 0);",
            "",
            "\t\t/*",
            "\t\t * If we want to map a page that's in the swapcache writable, we",
            "\t\t * have to detect via the refcount if we're really the exclusive",
            "\t\t * owner. Try removing the extra reference from the local LRU",
            "\t\t * caches if required.",
            "\t\t */",
            "\t\tif ((vmf->flags & FAULT_FLAG_WRITE) && folio == swapcache &&",
            "\t\t    !folio_test_ksm(folio) && !folio_test_lru(folio))",
            "\t\t\tlru_add_drain();",
            "\t}",
            "",
            "\tfolio_throttle_swaprate(folio, GFP_KERNEL);",
            "",
            "\t/*",
            "\t * Back out if somebody else already faulted in this pte.",
            "\t */",
            "\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,",
            "\t\t\t&vmf->ptl);",
            "\tif (unlikely(!vmf->pte || !pte_same(ptep_get(vmf->pte), vmf->orig_pte)))",
            "\t\tgoto out_nomap;",
            "",
            "\tif (unlikely(!folio_test_uptodate(folio))) {",
            "\t\tret = VM_FAULT_SIGBUS;",
            "\t\tgoto out_nomap;",
            "\t}",
            "",
            "\t/* allocated large folios for SWP_SYNCHRONOUS_IO */",
            "\tif (folio_test_large(folio) && !folio_test_swapcache(folio)) {",
            "\t\tunsigned long nr = folio_nr_pages(folio);",
            "\t\tunsigned long folio_start = ALIGN_DOWN(vmf->address, nr * PAGE_SIZE);",
            "\t\tunsigned long idx = (vmf->address - folio_start) / PAGE_SIZE;",
            "\t\tpte_t *folio_ptep = vmf->pte - idx;",
            "\t\tpte_t folio_pte = ptep_get(folio_ptep);",
            "",
            "\t\tif (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||",
            "\t\t    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)",
            "\t\t\tgoto out_nomap;",
            "",
            "\t\tpage_idx = idx;",
            "\t\taddress = folio_start;",
            "\t\tptep = folio_ptep;",
            "\t\tgoto check_folio;",
            "\t}",
            "",
            "\tnr_pages = 1;",
            "\tpage_idx = 0;",
            "\taddress = vmf->address;",
            "\tptep = vmf->pte;",
            "\tif (folio_test_large(folio) && folio_test_swapcache(folio)) {",
            "\t\tint nr = folio_nr_pages(folio);",
            "\t\tunsigned long idx = folio_page_idx(folio, page);",
            "\t\tunsigned long folio_start = address - idx * PAGE_SIZE;",
            "\t\tunsigned long folio_end = folio_start + nr * PAGE_SIZE;",
            "\t\tpte_t *folio_ptep;",
            "\t\tpte_t folio_pte;",
            "",
            "\t\tif (unlikely(folio_start < max(address & PMD_MASK, vma->vm_start)))",
            "\t\t\tgoto check_folio;",
            "\t\tif (unlikely(folio_end > pmd_addr_end(address, vma->vm_end)))",
            "\t\t\tgoto check_folio;",
            "",
            "\t\tfolio_ptep = vmf->pte - idx;",
            "\t\tfolio_pte = ptep_get(folio_ptep);",
            "\t\tif (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||",
            "\t\t    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)",
            "\t\t\tgoto check_folio;",
            "",
            "\t\tpage_idx = idx;",
            "\t\taddress = folio_start;",
            "\t\tptep = folio_ptep;",
            "\t\tnr_pages = nr;",
            "\t\tentry = folio->swap;",
            "\t\tpage = &folio->page;",
            "\t}",
            "",
            "check_folio:",
            "\t/*",
            "\t * PG_anon_exclusive reuses PG_mappedtodisk for anon pages. A swap pte",
            "\t * must never point at an anonymous page in the swapcache that is",
            "\t * PG_anon_exclusive. Sanity check that this holds and especially, that",
            "\t * no filesystem set PG_mappedtodisk on a page in the swapcache. Sanity",
            "\t * check after taking the PT lock and making sure that nobody",
            "\t * concurrently faulted in this page and set PG_anon_exclusive.",
            "\t */",
            "\tBUG_ON(!folio_test_anon(folio) && folio_test_mappedtodisk(folio));",
            "\tBUG_ON(folio_test_anon(folio) && PageAnonExclusive(page));",
            "",
            "\t/*",
            "\t * Check under PT lock (to protect against concurrent fork() sharing",
            "\t * the swap entry concurrently) for certainly exclusive pages.",
            "\t */",
            "\tif (!folio_test_ksm(folio)) {",
            "\t\texclusive = pte_swp_exclusive(vmf->orig_pte);",
            "\t\tif (folio != swapcache) {",
            "\t\t\t/*",
            "\t\t\t * We have a fresh page that is not exposed to the",
            "\t\t\t * swapcache -> certainly exclusive.",
            "\t\t\t */",
            "\t\t\texclusive = true;",
            "\t\t} else if (exclusive && folio_test_writeback(folio) &&",
            "\t\t\t  data_race(si->flags & SWP_STABLE_WRITES)) {",
            "\t\t\t/*",
            "\t\t\t * This is tricky: not all swap backends support",
            "\t\t\t * concurrent page modifications while under writeback.",
            "\t\t\t *",
            "\t\t\t * So if we stumble over such a page in the swapcache",
            "\t\t\t * we must not set the page exclusive, otherwise we can",
            "\t\t\t * map it writable without further checks and modify it",
            "\t\t\t * while still under writeback.",
            "\t\t\t *",
            "\t\t\t * For these problematic swap backends, simply drop the",
            "\t\t\t * exclusive marker: this is perfectly fine as we start",
            "\t\t\t * writeback only if we fully unmapped the page and",
            "\t\t\t * there are no unexpected references on the page after",
            "\t\t\t * unmapping succeeded. After fully unmapped, no",
            "\t\t\t * further GUP references (FOLL_GET and FOLL_PIN) can",
            "\t\t\t * appear, so dropping the exclusive marker and mapping",
            "\t\t\t * it only R/O is fine.",
            "\t\t\t */",
            "\t\t\texclusive = false;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Some architectures may have to restore extra metadata to the page",
            "\t * when reading from swap. This metadata may be indexed by swap entry",
            "\t * so this must be called before swap_free().",
            "\t */",
            "\tarch_swap_restore(folio_swap(entry, folio), folio);",
            "",
            "\t/*",
            "\t * Remove the swap entry and conditionally try to free up the swapcache.",
            "\t * We're already holding a reference on the page but haven't mapped it",
            "\t * yet.",
            "\t */",
            "\tswap_free_nr(entry, nr_pages);",
            "\tif (should_try_to_free_swap(folio, vma, vmf->flags))",
            "\t\tfolio_free_swap(folio);",
            "",
            "\tadd_mm_counter(vma->vm_mm, MM_ANONPAGES, nr_pages);",
            "\tadd_mm_counter(vma->vm_mm, MM_SWAPENTS, -nr_pages);",
            "\tpte = mk_pte(page, vma->vm_page_prot);",
            "",
            "\t/*",
            "\t * Same logic as in do_wp_page(); however, optimize for pages that are",
            "\t * certainly not shared either because we just allocated them without",
            "\t * exposing them to the swapcache or because the swap entry indicates",
            "\t * exclusivity.",
            "\t */",
            "\tif (!folio_test_ksm(folio) &&",
            "\t    (exclusive || folio_ref_count(folio) == 1)) {",
            "\t\tif (vmf->flags & FAULT_FLAG_WRITE) {",
            "\t\t\tpte = maybe_mkwrite(pte_mkdirty(pte), vma);",
            "\t\t\tvmf->flags &= ~FAULT_FLAG_WRITE;",
            "\t\t}",
            "\t\trmap_flags |= RMAP_EXCLUSIVE;",
            "\t}",
            "\tfolio_ref_add(folio, nr_pages - 1);",
            "\tflush_icache_pages(vma, page, nr_pages);",
            "\tif (pte_swp_soft_dirty(vmf->orig_pte))",
            "\t\tpte = pte_mksoft_dirty(pte);",
            "\tif (pte_swp_uffd_wp(vmf->orig_pte))",
            "\t\tpte = pte_mkuffd_wp(pte);",
            "\tvmf->orig_pte = pte_advance_pfn(pte, page_idx);",
            "",
            "\t/* ksm created a completely new copy */",
            "\tif (unlikely(folio != swapcache && swapcache)) {",
            "\t\tfolio_add_new_anon_rmap(folio, vma, address, RMAP_EXCLUSIVE);",
            "\t\tfolio_add_lru_vma(folio, vma);",
            "\t} else if (!folio_test_anon(folio)) {",
            "\t\t/*",
            "\t\t * We currently only expect small !anon folios which are either",
            "\t\t * fully exclusive or fully shared, or new allocated large",
            "\t\t * folios which are fully exclusive. If we ever get large",
            "\t\t * folios within swapcache here, we have to be careful.",
            "\t\t */",
            "\t\tVM_WARN_ON_ONCE(folio_test_large(folio) && folio_test_swapcache(folio));",
            "\t\tVM_WARN_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\t\tfolio_add_new_anon_rmap(folio, vma, address, rmap_flags);",
            "\t} else {",
            "\t\tfolio_add_anon_rmap_ptes(folio, page, nr_pages, vma, address,",
            "\t\t\t\t\trmap_flags);",
            "\t}",
            "",
            "\tVM_BUG_ON(!folio_test_anon(folio) ||",
            "\t\t\t(pte_write(pte) && !PageAnonExclusive(page)));",
            "\tset_ptes(vma->vm_mm, address, ptep, pte, nr_pages);",
            "\tarch_do_swap_page_nr(vma->vm_mm, vma, address,",
            "\t\t\tpte, pte, nr_pages);",
            "",
            "\tfolio_unlock(folio);",
            "\tif (folio != swapcache && swapcache) {",
            "\t\t/*",
            "\t\t * Hold the lock to avoid the swap entry to be reused",
            "\t\t * until we take the PT lock for the pte_same() check",
            "\t\t * (to avoid false positives from pte_same). For",
            "\t\t * further safety release the lock after the swap_free",
            "\t\t * so that the swap count won't change under a",
            "\t\t * parallel locked swapcache.",
            "\t\t */",
            "\t\tfolio_unlock(swapcache);",
            "\t\tfolio_put(swapcache);",
            "\t}",
            "",
            "\tif (vmf->flags & FAULT_FLAG_WRITE) {",
            "\t\tret |= do_wp_page(vmf);",
            "\t\tif (ret & VM_FAULT_ERROR)",
            "\t\t\tret &= VM_FAULT_ERROR;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* No need to invalidate - it was non-present before */",
            "\tupdate_mmu_cache_range(vmf, vma, address, ptep, nr_pages);",
            "unlock:",
            "\tif (vmf->pte)",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "out:",
            "\t/* Clear the swap cache pin for direct swapin after PTL unlock */",
            "\tif (need_clear_cache) {",
            "\t\tswapcache_clear(si, entry, nr_pages);",
            "\t\tif (waitqueue_active(&swapcache_wq))",
            "\t\t\twake_up(&swapcache_wq);",
            "\t}",
            "\tif (si)",
            "\t\tput_swap_device(si);",
            "\treturn ret;",
            "out_nomap:",
            "\tif (vmf->pte)",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "out_page:",
            "\tfolio_unlock(folio);",
            "out_release:",
            "\tfolio_put(folio);",
            "\tif (folio != swapcache && swapcache) {",
            "\t\tfolio_unlock(swapcache);",
            "\t\tfolio_put(swapcache);",
            "\t}",
            "\tif (need_clear_cache) {",
            "\t\tswapcache_clear(si, entry, nr_pages);",
            "\t\tif (waitqueue_active(&swapcache_wq))",
            "\t\t\twake_up(&swapcache_wq);",
            "\t}",
            "\tif (si)",
            "\t\tput_swap_device(si);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "thp_swap_suitable_orders, do_swap_page",
          "description": "处理页面交换时的多种情况，如迁移条目、设备私有条目、硬件故障等，根据不同的条目类型执行相应逻辑，最终将交换页面映射到内存中。",
          "similarity": 0.5638668537139893
        },
        {
          "chunk_id": 17,
          "file_path": "mm/memory.c",
          "start_line": 2787,
          "end_line": 2893,
          "content": [
            "static int apply_to_pmd_range(struct mm_struct *mm, pud_t *pud,",
            "\t\t\t\t     unsigned long addr, unsigned long end,",
            "\t\t\t\t     pte_fn_t fn, void *data, bool create,",
            "\t\t\t\t     pgtbl_mod_mask *mask)",
            "{",
            "\tpmd_t *pmd;",
            "\tunsigned long next;",
            "\tint err = 0;",
            "",
            "\tBUG_ON(pud_leaf(*pud));",
            "",
            "\tif (create) {",
            "\t\tpmd = pmd_alloc_track(mm, pud, addr, mask);",
            "\t\tif (!pmd)",
            "\t\t\treturn -ENOMEM;",
            "\t} else {",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t}",
            "\tdo {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "\t\tif (pmd_none(*pmd) && !create)",
            "\t\t\tcontinue;",
            "\t\tif (WARN_ON_ONCE(pmd_leaf(*pmd)))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!pmd_none(*pmd) && WARN_ON_ONCE(pmd_bad(*pmd))) {",
            "\t\t\tif (!create)",
            "\t\t\t\tcontinue;",
            "\t\t\tpmd_clear_bad(pmd);",
            "\t\t}",
            "\t\terr = apply_to_pte_range(mm, pmd, addr, next,",
            "\t\t\t\t\t fn, data, create, mask);",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t} while (pmd++, addr = next, addr != end);",
            "",
            "\treturn err;",
            "}",
            "static int apply_to_pud_range(struct mm_struct *mm, p4d_t *p4d,",
            "\t\t\t\t     unsigned long addr, unsigned long end,",
            "\t\t\t\t     pte_fn_t fn, void *data, bool create,",
            "\t\t\t\t     pgtbl_mod_mask *mask)",
            "{",
            "\tpud_t *pud;",
            "\tunsigned long next;",
            "\tint err = 0;",
            "",
            "\tif (create) {",
            "\t\tpud = pud_alloc_track(mm, p4d, addr, mask);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "\t} else {",
            "\t\tpud = pud_offset(p4d, addr);",
            "\t}",
            "\tdo {",
            "\t\tnext = pud_addr_end(addr, end);",
            "\t\tif (pud_none(*pud) && !create)",
            "\t\t\tcontinue;",
            "\t\tif (WARN_ON_ONCE(pud_leaf(*pud)))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!pud_none(*pud) && WARN_ON_ONCE(pud_bad(*pud))) {",
            "\t\t\tif (!create)",
            "\t\t\t\tcontinue;",
            "\t\t\tpud_clear_bad(pud);",
            "\t\t}",
            "\t\terr = apply_to_pmd_range(mm, pud, addr, next,",
            "\t\t\t\t\t fn, data, create, mask);",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t} while (pud++, addr = next, addr != end);",
            "",
            "\treturn err;",
            "}",
            "static int apply_to_p4d_range(struct mm_struct *mm, pgd_t *pgd,",
            "\t\t\t\t     unsigned long addr, unsigned long end,",
            "\t\t\t\t     pte_fn_t fn, void *data, bool create,",
            "\t\t\t\t     pgtbl_mod_mask *mask)",
            "{",
            "\tp4d_t *p4d;",
            "\tunsigned long next;",
            "\tint err = 0;",
            "",
            "\tif (create) {",
            "\t\tp4d = p4d_alloc_track(mm, pgd, addr, mask);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "\t} else {",
            "\t\tp4d = p4d_offset(pgd, addr);",
            "\t}",
            "\tdo {",
            "\t\tnext = p4d_addr_end(addr, end);",
            "\t\tif (p4d_none(*p4d) && !create)",
            "\t\t\tcontinue;",
            "\t\tif (WARN_ON_ONCE(p4d_leaf(*p4d)))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!p4d_none(*p4d) && WARN_ON_ONCE(p4d_bad(*p4d))) {",
            "\t\t\tif (!create)",
            "\t\t\t\tcontinue;",
            "\t\t\tp4d_clear_bad(p4d);",
            "\t\t}",
            "\t\terr = apply_to_pud_range(mm, p4d, addr, next,",
            "\t\t\t\t\t fn, data, create, mask);",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t} while (p4d++, addr = next, addr != end);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "apply_to_pmd_range, apply_to_pud_range, apply_to_p4d_range",
          "description": "实现针对不同页表层级（pmd/pud/p4d）的应用操作，遍历页表结构，处理无效页表项，调用下一级apply_to_..._range函数完成统一操作，支持页表项的创建和修改。",
          "similarity": 0.5538820028305054
        },
        {
          "chunk_id": 4,
          "file_path": "mm/memory.c",
          "start_line": 718,
          "end_line": 885,
          "content": [
            "static void restore_exclusive_pte(struct vm_area_struct *vma,",
            "\t\t\t\t  struct page *page, unsigned long address,",
            "\t\t\t\t  pte_t *ptep)",
            "{",
            "\tstruct folio *folio = page_folio(page);",
            "\tpte_t orig_pte;",
            "\tpte_t pte;",
            "\tswp_entry_t entry;",
            "",
            "\torig_pte = ptep_get(ptep);",
            "\tpte = pte_mkold(mk_pte(page, READ_ONCE(vma->vm_page_prot)));",
            "\tif (pte_swp_soft_dirty(orig_pte))",
            "\t\tpte = pte_mksoft_dirty(pte);",
            "",
            "\tentry = pte_to_swp_entry(orig_pte);",
            "\tif (pte_swp_uffd_wp(orig_pte))",
            "\t\tpte = pte_mkuffd_wp(pte);",
            "\telse if (is_writable_device_exclusive_entry(entry))",
            "\t\tpte = maybe_mkwrite(pte_mkdirty(pte), vma);",
            "",
            "\tVM_BUG_ON_FOLIO(pte_write(pte) && (!folio_test_anon(folio) &&",
            "\t\t\t\t\t   PageAnonExclusive(page)), folio);",
            "",
            "\t/*",
            "\t * No need to take a page reference as one was already",
            "\t * created when the swap entry was made.",
            "\t */",
            "\tif (folio_test_anon(folio))",
            "\t\tfolio_add_anon_rmap_pte(folio, page, vma, address, RMAP_NONE);",
            "\telse",
            "\t\t/*",
            "\t\t * Currently device exclusive access only supports anonymous",
            "\t\t * memory so the entry shouldn't point to a filebacked page.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(1);",
            "",
            "\tset_pte_at(vma->vm_mm, address, ptep, pte);",
            "",
            "\t/*",
            "\t * No need to invalidate - it was non-present before. However",
            "\t * secondary CPUs may have mappings that need invalidating.",
            "\t */",
            "\tupdate_mmu_cache(vma, address, ptep);",
            "}",
            "static int",
            "try_restore_exclusive_pte(pte_t *src_pte, struct vm_area_struct *vma,",
            "\t\t\tunsigned long addr)",
            "{",
            "\tswp_entry_t entry = pte_to_swp_entry(ptep_get(src_pte));",
            "\tstruct page *page = pfn_swap_entry_to_page(entry);",
            "",
            "\tif (trylock_page(page)) {",
            "\t\trestore_exclusive_pte(vma, page, addr, src_pte);",
            "\t\tunlock_page(page);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\treturn -EBUSY;",
            "}",
            "static unsigned long",
            "copy_nonpresent_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,",
            "\t\tpte_t *dst_pte, pte_t *src_pte, struct vm_area_struct *dst_vma,",
            "\t\tstruct vm_area_struct *src_vma, unsigned long addr, int *rss)",
            "{",
            "\tunsigned long vm_flags = dst_vma->vm_flags;",
            "\tpte_t orig_pte = ptep_get(src_pte);",
            "\tpte_t pte = orig_pte;",
            "\tstruct folio *folio;",
            "\tstruct page *page;",
            "\tswp_entry_t entry = pte_to_swp_entry(orig_pte);",
            "",
            "\tif (likely(!non_swap_entry(entry))) {",
            "\t\tif (swap_duplicate(entry) < 0)",
            "\t\t\treturn -EIO;",
            "",
            "\t\t/* make sure dst_mm is on swapoff's mmlist. */",
            "\t\tif (unlikely(list_empty(&dst_mm->mmlist))) {",
            "\t\t\tspin_lock(&mmlist_lock);",
            "\t\t\tif (list_empty(&dst_mm->mmlist))",
            "\t\t\t\tlist_add(&dst_mm->mmlist,",
            "\t\t\t\t\t\t&src_mm->mmlist);",
            "\t\t\tspin_unlock(&mmlist_lock);",
            "\t\t}",
            "\t\t/* Mark the swap entry as shared. */",
            "\t\tif (pte_swp_exclusive(orig_pte)) {",
            "\t\t\tpte = pte_swp_clear_exclusive(orig_pte);",
            "\t\t\tset_pte_at(src_mm, addr, src_pte, pte);",
            "\t\t}",
            "\t\trss[MM_SWAPENTS]++;",
            "\t} else if (is_migration_entry(entry)) {",
            "\t\tfolio = pfn_swap_entry_folio(entry);",
            "",
            "\t\trss[mm_counter(folio)]++;",
            "",
            "\t\tif (!is_readable_migration_entry(entry) &&",
            "\t\t\t\tis_cow_mapping(vm_flags)) {",
            "\t\t\t/*",
            "\t\t\t * COW mappings require pages in both parent and child",
            "\t\t\t * to be set to read. A previously exclusive entry is",
            "\t\t\t * now shared.",
            "\t\t\t */",
            "\t\t\tentry = make_readable_migration_entry(",
            "\t\t\t\t\t\t\tswp_offset(entry));",
            "\t\t\tpte = swp_entry_to_pte(entry);",
            "\t\t\tif (pte_swp_soft_dirty(orig_pte))",
            "\t\t\t\tpte = pte_swp_mksoft_dirty(pte);",
            "\t\t\tif (pte_swp_uffd_wp(orig_pte))",
            "\t\t\t\tpte = pte_swp_mkuffd_wp(pte);",
            "\t\t\tset_pte_at(src_mm, addr, src_pte, pte);",
            "\t\t}",
            "\t} else if (is_device_private_entry(entry)) {",
            "\t\tpage = pfn_swap_entry_to_page(entry);",
            "\t\tfolio = page_folio(page);",
            "",
            "\t\t/*",
            "\t\t * Update rss count even for unaddressable pages, as",
            "\t\t * they should treated just like normal pages in this",
            "\t\t * respect.",
            "\t\t *",
            "\t\t * We will likely want to have some new rss counters",
            "\t\t * for unaddressable pages, at some point. But for now",
            "\t\t * keep things as they are.",
            "\t\t */",
            "\t\tfolio_get(folio);",
            "\t\trss[mm_counter(folio)]++;",
            "\t\t/* Cannot fail as these pages cannot get pinned. */",
            "\t\tfolio_try_dup_anon_rmap_pte(folio, page, src_vma);",
            "",
            "\t\t/*",
            "\t\t * We do not preserve soft-dirty information, because so",
            "\t\t * far, checkpoint/restore is the only feature that",
            "\t\t * requires that. And checkpoint/restore does not work",
            "\t\t * when a device driver is involved (you cannot easily",
            "\t\t * save and restore device driver state).",
            "\t\t */",
            "\t\tif (is_writable_device_private_entry(entry) &&",
            "\t\t    is_cow_mapping(vm_flags)) {",
            "\t\t\tentry = make_readable_device_private_entry(",
            "\t\t\t\t\t\t\tswp_offset(entry));",
            "\t\t\tpte = swp_entry_to_pte(entry);",
            "\t\t\tif (pte_swp_uffd_wp(orig_pte))",
            "\t\t\t\tpte = pte_swp_mkuffd_wp(pte);",
            "\t\t\tset_pte_at(src_mm, addr, src_pte, pte);",
            "\t\t}",
            "\t} else if (is_device_exclusive_entry(entry)) {",
            "\t\t/*",
            "\t\t * Make device exclusive entries present by restoring the",
            "\t\t * original entry then copying as for a present pte. Device",
            "\t\t * exclusive entries currently only support private writable",
            "\t\t * (ie. COW) mappings.",
            "\t\t */",
            "\t\tVM_BUG_ON(!is_cow_mapping(src_vma->vm_flags));",
            "\t\tif (try_restore_exclusive_pte(src_pte, src_vma, addr))",
            "\t\t\treturn -EBUSY;",
            "\t\treturn -ENOENT;",
            "\t} else if (is_pte_marker_entry(entry)) {",
            "\t\tpte_marker marker = copy_pte_marker(entry, dst_vma);",
            "",
            "\t\tif (marker)",
            "\t\t\tset_pte_at(dst_mm, addr, dst_pte,",
            "\t\t\t\t   make_pte_marker(marker));",
            "\t\treturn 0;",
            "\t}",
            "\tif (!userfaultfd_wp(dst_vma))",
            "\t\tpte = pte_swp_clear_uffd_wp(pte);",
            "\tset_pte_at(dst_mm, addr, dst_pte, pte);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "restore_exclusive_pte, try_restore_exclusive_pte, copy_nonpresent_pte",
          "description": "处理特殊页表项转换逻辑，包括设备独占页表项还原、非存在项复制及迁移条目处理，确保页表状态一致性并维护RSS统计信息",
          "similarity": 0.5513703227043152
        }
      ]
    },
    {
      "source_file": "mm/page_reporting.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:05:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_reporting.c`\n\n---\n\n# page_reporting.c 技术文档\n\n## 1. 文件概述\n\n`page_reporting.c` 是 Linux 内核中实现**空闲页报告（Page Reporting）**机制的核心模块。该机制允许内核将大块连续的空闲物理内存信息异步上报给注册的设备驱动（如 virtio-balloon、内存热插拔管理器等），以便这些设备可以回收或迁移这些内存，从而提升系统整体内存利用效率。本文件负责协调从伙伴系统（buddy allocator）中提取符合要求的空闲页、构建散列表（scatterlist）、调用设备驱动的报告回调，并在报告完成后将页面安全地归还到伙伴系统。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `page_reporting_order`: 可通过内核启动参数或 sysfs 配置的全局变量，指定待报告空闲页块的最小阶数（order）。默认值为 `-1`（表示未设置），有效范围为 `[0, MAX_PAGE_ORDER]`。导出为 GPL 符号供其他驱动访问。\n- `pr_dev_info`: 指向当前注册的 `page_reporting_dev_info` 结构的 RCU 保护指针，代表提供页报告服务的设备。\n\n### 主要函数\n- `__page_reporting_notify(void)`: 通知已注册的页报告设备有新的空闲页可供报告。这是内核其他部分（如内存释放路径）触发报告流程的入口点。\n- `__page_reporting_request(struct page_reporting_dev_info *prdev)`: 内部函数，用于向指定设备请求启动页报告工作。包含状态机管理和延迟调度逻辑。\n- `page_reporting_drain(...)`: 在设备完成页报告（无论成功与否）后，将散列表中的页面重新放回伙伴系统的对应 free_area 和 migratetype 列表中，并根据报告结果设置 `PG_reported` 标志位。\n- `page_reporting_cycle(...)`: 核心处理循环，遍历指定 zone、order 和 migratetype 的 free_list，隔离未报告的页面到散列表中，达到容量后触发设备报告回调。\n- `page_reporting_process_zone(...)`: 处理单个内存区域（zone）的入口函数，负责水位线检查并按 order 从高到低调用 `page_reporting_cycle`。\n\n### 关键数据结构（外部定义）\n- `struct page_reporting_dev_info`: 由设备驱动提供，包含报告回调函数 `report()`、工作队列 `work` 和状态原子变量 `state` 等。\n\n## 3. 关键实现\n\n### 状态机与延迟调度\n- 使用原子变量 `prdev->state` 管理三种状态：`IDLE`（空闲）、`REQUESTED`（已请求）、`ACTIVE`（活跃中）。\n- 当收到报告请求时，若当前为空闲状态，则调度一个延迟为 `2 * HZ`（约 2 秒）的 `delayed_work`。此延迟旨在累积足够多的空闲页再进行批量报告，减少频繁调用设备驱动的开销。\n\n### 页面隔离与归还\n- **隔离**: 在持有 zone 自旋锁期间，使用 `__isolate_free_page()` 将符合条件的 Buddy 页面从 free_list 中移除。\n- **归还**: 报告完成后，在 `page_reporting_drain()` 中调用 `__putback_isolated_page()` 将页面放回原 free_area 和 migratetype 列表。\n- **报告标记**: 仅当页面在报告后仍保持 Buddy 状态且其阶数未变时，才设置 `PG_reported` 标志，避免对已合并的大页面重复报告。\n\n### 散列表（Scatterlist）管理\n- 使用固定大小（`PAGE_REPORTING_CAPACITY`）的散列表作为设备驱动和内核之间的传输缓冲区。\n- 采用“填满即报”的策略：当散列表填满或遍历完当前 free_list 后，立即调用设备驱动的 `report()` 回调。\n- 报告完成后重置散列表（`sg_init_table`）以供下次使用。\n\n### 遍历策略与预算控制\n- **遍历顺序**: 按内存区域（zone）、迁移类型（migratetype）、页面阶数（order，从高到低）进行嵌套遍历。\n- **预算限制**: 对每个 `(zone, order, mt)` 组合设置处理预算（`budget`），防止单次处理耗时过长影响系统响应。预算基于该 free_area 中空闲页数量动态计算。\n- **列表旋转**: 在中断遍历时，将下一个待处理页面旋转到 free_list 头部（`list_rotate_to_front`），确保下次从断点继续，避免饥饿。\n\n### 水位线保护\n- 在 `page_reporting_process_zone()` 中检查 zone 的空闲页是否高于 `low_wmark + (capacity << reporting_order)`，防止因报告操作导致内存水位过低而引发分配失败或 OOM。\n\n## 4. 依赖关系\n\n- **内部依赖**:\n  - `mm/internal.h`: 提供 `__putback_isolated_page()`、`__isolate_free_page()` 等伙伴系统内部操作函数。\n  - `page_reporting.h` (本地): 定义本地辅助函数和常量（如 `PAGE_REPORTING_CAPACITY`）。\n- **外部依赖**:\n  - `<linux/mm.h>`, `<linux/mmzone.h>`: 内存管理核心头文件，提供 `struct zone`、`free_area`、页面操作宏等。\n  - `<linux/page_reporting.h>`: 定义公共接口 `struct page_reporting_dev_info` 和注册/注销 API。\n  - `<linux/scatterlist.h>`: 提供散列表操作函数（`sg_set_page`, `sg_next` 等）。\n  - 设备驱动: 必须实现 `page_reporting_dev_info.report` 回调函数，并通过 `page_reporting_register()` 注册。\n\n## 5. 使用场景\n\n- **虚拟化环境**: Virtio-balloon 驱动利用此机制向宿主机报告客户机中大块空闲内存，宿主机可将其回收用于其他虚拟机，提高物理内存利用率。\n- **内存热插拔/卸载**: 在移除内存前，通过页报告机制确保目标内存区域尽可能空闲，减少迁移成本。\n- **透明大页（THP）优化**: 协助识别和释放可用于 THP 分配的大块连续空闲内存。\n- **通用内存回收**: 任何需要感知系统大块空闲内存布局的子系统（如 CMA、HMM）均可注册为页报告设备，实现定制化内存管理策略。",
      "similarity": 0.5851836204528809,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/page_reporting.c",
          "start_line": 259,
          "end_line": 393,
          "content": [
            "static int",
            "page_reporting_process_zone(struct page_reporting_dev_info *prdev,",
            "\t\t\t    struct scatterlist *sgl, struct zone *zone)",
            "{",
            "\tunsigned int order, mt, leftover, offset = PAGE_REPORTING_CAPACITY;",
            "\tunsigned long watermark;",
            "\tint err = 0;",
            "",
            "\t/* Generate minimum watermark to be able to guarantee progress */",
            "\twatermark = low_wmark_pages(zone) +",
            "\t\t    (PAGE_REPORTING_CAPACITY << page_reporting_order);",
            "",
            "\t/*",
            "\t * Cancel request if insufficient free memory or if we failed",
            "\t * to allocate page reporting statistics for the zone.",
            "\t */",
            "\tif (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))",
            "\t\treturn err;",
            "",
            "\t/* Process each free list starting from lowest order/mt */",
            "\tfor (order = page_reporting_order; order < NR_PAGE_ORDERS; order++) {",
            "\t\tfor (mt = 0; mt < MIGRATE_TYPES; mt++) {",
            "\t\t\t/* We do not pull pages from the isolate free list */",
            "\t\t\tif (is_migrate_isolate(mt))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\terr = page_reporting_cycle(prdev, zone, order, mt,",
            "\t\t\t\t\t\t   sgl, &offset);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "\t}",
            "",
            "\t/* report the leftover pages before going idle */",
            "\tleftover = PAGE_REPORTING_CAPACITY - offset;",
            "\tif (leftover) {",
            "\t\tsgl = &sgl[offset];",
            "\t\terr = prdev->report(prdev, sgl, leftover);",
            "",
            "\t\t/* flush any remaining pages out from the last report */",
            "\t\tspin_lock_irq(&zone->lock);",
            "\t\tpage_reporting_drain(prdev, sgl, leftover, !err);",
            "\t\tspin_unlock_irq(&zone->lock);",
            "\t}",
            "",
            "\treturn err;",
            "}",
            "static void page_reporting_process(struct work_struct *work)",
            "{",
            "\tstruct delayed_work *d_work = to_delayed_work(work);",
            "\tstruct page_reporting_dev_info *prdev =",
            "\t\tcontainer_of(d_work, struct page_reporting_dev_info, work);",
            "\tint err = 0, state = PAGE_REPORTING_ACTIVE;",
            "\tstruct scatterlist *sgl;",
            "\tstruct zone *zone;",
            "",
            "\t/*",
            "\t * Change the state to \"Active\" so that we can track if there is",
            "\t * anyone requests page reporting after we complete our pass. If",
            "\t * the state is not altered by the end of the pass we will switch",
            "\t * to idle and quit scheduling reporting runs.",
            "\t */",
            "\tatomic_set(&prdev->state, state);",
            "",
            "\t/* allocate scatterlist to store pages being reported on */",
            "\tsgl = kmalloc_array(PAGE_REPORTING_CAPACITY, sizeof(*sgl), GFP_KERNEL);",
            "\tif (!sgl)",
            "\t\tgoto err_out;",
            "",
            "\tsg_init_table(sgl, PAGE_REPORTING_CAPACITY);",
            "",
            "\tfor_each_zone(zone) {",
            "\t\terr = page_reporting_process_zone(prdev, sgl, zone);",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tkfree(sgl);",
            "err_out:",
            "\t/*",
            "\t * If the state has reverted back to requested then there may be",
            "\t * additional pages to be processed. We will defer for 2s to allow",
            "\t * more pages to accumulate.",
            "\t */",
            "\tstate = atomic_cmpxchg(&prdev->state, state, PAGE_REPORTING_IDLE);",
            "\tif (state == PAGE_REPORTING_REQUESTED)",
            "\t\tschedule_delayed_work(&prdev->work, PAGE_REPORTING_DELAY);",
            "}",
            "int page_reporting_register(struct page_reporting_dev_info *prdev)",
            "{",
            "\tint err = 0;",
            "",
            "\tmutex_lock(&page_reporting_mutex);",
            "",
            "\t/* nothing to do if already in use */",
            "\tif (rcu_dereference_protected(pr_dev_info,",
            "\t\t\t\tlockdep_is_held(&page_reporting_mutex))) {",
            "\t\terr = -EBUSY;",
            "\t\tgoto err_out;",
            "\t}",
            "",
            "\t/*",
            "\t * If the page_reporting_order value is not set, we check if",
            "\t * an order is provided from the driver that is performing the",
            "\t * registration. If that is not provided either, we default to",
            "\t * pageblock_order.",
            "\t */",
            "",
            "\tif (page_reporting_order == -1) {",
            "\t\tif (prdev->order > 0 && prdev->order <= MAX_PAGE_ORDER)",
            "\t\t\tpage_reporting_order = prdev->order;",
            "\t\telse",
            "\t\t\tpage_reporting_order = pageblock_order;",
            "\t}",
            "",
            "\t/* initialize state and work structures */",
            "\tatomic_set(&prdev->state, PAGE_REPORTING_IDLE);",
            "\tINIT_DELAYED_WORK(&prdev->work, &page_reporting_process);",
            "",
            "\t/* Begin initial flush of zones */",
            "\t__page_reporting_request(prdev);",
            "",
            "\t/* Assign device to allow notifications */",
            "\trcu_assign_pointer(pr_dev_info, prdev);",
            "",
            "\t/* enable page reporting notification */",
            "\tif (!static_key_enabled(&page_reporting_enabled)) {",
            "\t\tstatic_branch_enable(&page_reporting_enabled);",
            "\t\tpr_info(\"Free page reporting enabled\\n\");",
            "\t}",
            "err_out:",
            "\tmutex_unlock(&page_reporting_mutex);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "page_reporting_process_zone, page_reporting_process, page_reporting_register",
          "description": "提供内存区级页面报告处理函数，包含主处理循环、工作线程执行路径及设备注册时的初始配置与状态设置。",
          "similarity": 0.5449528694152832
        },
        {
          "chunk_id": 1,
          "file_path": "mm/page_reporting.c",
          "start_line": 17,
          "end_line": 213,
          "content": [
            "static int page_order_update_notify(const char *val, const struct kernel_param *kp)",
            "{",
            "\t/*",
            "\t * If param is set beyond this limit, order is set to default",
            "\t * pageblock_order value",
            "\t */",
            "\treturn  param_set_uint_minmax(val, kp, 0, MAX_PAGE_ORDER);",
            "}",
            "static void",
            "__page_reporting_request(struct page_reporting_dev_info *prdev)",
            "{",
            "\tunsigned int state;",
            "",
            "\t/* Check to see if we are in desired state */",
            "\tstate = atomic_read(&prdev->state);",
            "\tif (state == PAGE_REPORTING_REQUESTED)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If reporting is already active there is nothing we need to do.",
            "\t * Test against 0 as that represents PAGE_REPORTING_IDLE.",
            "\t */",
            "\tstate = atomic_xchg(&prdev->state, PAGE_REPORTING_REQUESTED);",
            "\tif (state != PAGE_REPORTING_IDLE)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Delay the start of work to allow a sizable queue to build. For",
            "\t * now we are limiting this to running no more than once every",
            "\t * couple of seconds.",
            "\t */",
            "\tschedule_delayed_work(&prdev->work, PAGE_REPORTING_DELAY);",
            "}",
            "void __page_reporting_notify(void)",
            "{",
            "\tstruct page_reporting_dev_info *prdev;",
            "",
            "\t/*",
            "\t * We use RCU to protect the pr_dev_info pointer. In almost all",
            "\t * cases this should be present, however in the unlikely case of",
            "\t * a shutdown this will be NULL and we should exit.",
            "\t */",
            "\trcu_read_lock();",
            "\tprdev = rcu_dereference(pr_dev_info);",
            "\tif (likely(prdev))",
            "\t\t__page_reporting_request(prdev);",
            "",
            "\trcu_read_unlock();",
            "}",
            "static void",
            "page_reporting_drain(struct page_reporting_dev_info *prdev,",
            "\t\t     struct scatterlist *sgl, unsigned int nents, bool reported)",
            "{",
            "\tstruct scatterlist *sg = sgl;",
            "",
            "\t/*",
            "\t * Drain the now reported pages back into their respective",
            "\t * free lists/areas. We assume at least one page is populated.",
            "\t */",
            "\tdo {",
            "\t\tstruct page *page = sg_page(sg);",
            "\t\tint mt = get_pageblock_migratetype(page);",
            "\t\tunsigned int order = get_order(sg->length);",
            "",
            "\t\t__putback_isolated_page(page, order, mt);",
            "",
            "\t\t/* If the pages were not reported due to error skip flagging */",
            "\t\tif (!reported)",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * If page was not comingled with another page we can",
            "\t\t * consider the result to be \"reported\" since the page",
            "\t\t * hasn't been modified, otherwise we will need to",
            "\t\t * report on the new larger page when we make our way",
            "\t\t * up to that higher order.",
            "\t\t */",
            "\t\tif (PageBuddy(page) && buddy_order(page) == order)",
            "\t\t\t__SetPageReported(page);",
            "\t} while ((sg = sg_next(sg)));",
            "",
            "\t/* reinitialize scatterlist now that it is empty */",
            "\tsg_init_table(sgl, nents);",
            "}",
            "static int",
            "page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,",
            "\t\t     unsigned int order, unsigned int mt,",
            "\t\t     struct scatterlist *sgl, unsigned int *offset)",
            "{",
            "\tstruct free_area *area = &zone->free_area[order];",
            "\tstruct list_head *list = &area->free_list[mt];",
            "\tunsigned int page_len = PAGE_SIZE << order;",
            "\tstruct page *page, *next;",
            "\tlong budget;",
            "\tint err = 0;",
            "",
            "\t/*",
            "\t * Perform early check, if free area is empty there is",
            "\t * nothing to process so we can skip this free_list.",
            "\t */",
            "\tif (list_empty(list))",
            "\t\treturn err;",
            "",
            "\tspin_lock_irq(&zone->lock);",
            "",
            "\t/*",
            "\t * Limit how many calls we will be making to the page reporting",
            "\t * device for this list. By doing this we avoid processing any",
            "\t * given list for too long.",
            "\t *",
            "\t * The current value used allows us enough calls to process over a",
            "\t * sixteenth of the current list plus one additional call to handle",
            "\t * any pages that may have already been present from the previous",
            "\t * list processed. This should result in us reporting all pages on",
            "\t * an idle system in about 30 seconds.",
            "\t *",
            "\t * The division here should be cheap since PAGE_REPORTING_CAPACITY",
            "\t * should always be a power of 2.",
            "\t */",
            "\tbudget = DIV_ROUND_UP(area->nr_free, PAGE_REPORTING_CAPACITY * 16);",
            "",
            "\t/* loop through free list adding unreported pages to sg list */",
            "\tlist_for_each_entry_safe(page, next, list, lru) {",
            "\t\t/* We are going to skip over the reported pages. */",
            "\t\tif (PageReported(page))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * If we fully consumed our budget then update our",
            "\t\t * state to indicate that we are requesting additional",
            "\t\t * processing and exit this list.",
            "\t\t */",
            "\t\tif (budget < 0) {",
            "\t\t\tatomic_set(&prdev->state, PAGE_REPORTING_REQUESTED);",
            "\t\t\tnext = page;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t/* Attempt to pull page from list and place in scatterlist */",
            "\t\tif (*offset) {",
            "\t\t\tif (!__isolate_free_page(page, order)) {",
            "\t\t\t\tnext = page;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "",
            "\t\t\t/* Add page to scatter list */",
            "\t\t\t--(*offset);",
            "\t\t\tsg_set_page(&sgl[*offset], page, page_len, 0);",
            "",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Make the first non-reported page in the free list",
            "\t\t * the new head of the free list before we release the",
            "\t\t * zone lock.",
            "\t\t */",
            "\t\tif (!list_is_first(&page->lru, list))",
            "\t\t\tlist_rotate_to_front(&page->lru, list);",
            "",
            "\t\t/* release lock before waiting on report processing */",
            "\t\tspin_unlock_irq(&zone->lock);",
            "",
            "\t\t/* begin processing pages in local list */",
            "\t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);",
            "",
            "\t\t/* reset offset since the full list was reported */",
            "\t\t*offset = PAGE_REPORTING_CAPACITY;",
            "",
            "\t\t/* update budget to reflect call to report function */",
            "\t\tbudget--;",
            "",
            "\t\t/* reacquire zone lock and resume processing */",
            "\t\tspin_lock_irq(&zone->lock);",
            "",
            "\t\t/* flush reported pages from the sg list */",
            "\t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);",
            "",
            "\t\t/*",
            "\t\t * Reset next to first entry, the old next isn't valid",
            "\t\t * since we dropped the lock to report the pages",
            "\t\t */",
            "\t\tnext = list_first_entry(list, struct page, lru);",
            "",
            "\t\t/* exit on error */",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\t/* Rotate any leftover pages to the head of the freelist */",
            "\tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))",
            "\t\tlist_rotate_to_front(&next->lru, list);",
            "",
            "\tspin_unlock_irq(&zone->lock);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "page_order_update_notify, __page_reporting_request, __page_reporting_notify, page_reporting_drain, page_reporting_cycle",
          "description": "实现页面报告参数更新、请求处理、状态通知及周期性处理逻辑，包含延迟工作队列调度、内存区域遍历和页面报告流程控制。",
          "similarity": 0.5216751098632812
        },
        {
          "chunk_id": 0,
          "file_path": "mm/page_reporting.c",
          "start_line": 1,
          "end_line": 16,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/page_reporting.h>",
            "#include <linux/gfp.h>",
            "#include <linux/export.h>",
            "#include <linux/module.h>",
            "#include <linux/delay.h>",
            "#include <linux/scatterlist.h>",
            "",
            "#include \"page_reporting.h\"",
            "#include \"internal.h\"",
            "",
            "/* Initialize to an unsupported value */",
            "unsigned int page_reporting_order = -1;",
            ""
          ],
          "function_name": null,
          "description": "声明全局变量page_reporting_order用于存储页面报告的订单值，并包含相关内核模块头文件。",
          "similarity": 0.49317118525505066
        },
        {
          "chunk_id": 3,
          "file_path": "mm/page_reporting.c",
          "start_line": 401,
          "end_line": 416,
          "content": [
            "void page_reporting_unregister(struct page_reporting_dev_info *prdev)",
            "{",
            "\tmutex_lock(&page_reporting_mutex);",
            "",
            "\tif (prdev == rcu_dereference_protected(pr_dev_info,",
            "\t\t\t\tlockdep_is_held(&page_reporting_mutex))) {",
            "\t\t/* Disable page reporting notification */",
            "\t\tRCU_INIT_POINTER(pr_dev_info, NULL);",
            "\t\tsynchronize_rcu();",
            "",
            "\t\t/* Flush any existing work, and lock it out */",
            "\t\tcancel_delayed_work_sync(&prdev->work);",
            "\t}",
            "",
            "\tmutex_unlock(&page_reporting_mutex);",
            "}"
          ],
          "function_name": "page_reporting_unregister",
          "description": "实现页面报告设备注销操作，包含RCU安全指针替换、延迟任务取消及互斥锁保护的资源清理逻辑。",
          "similarity": 0.47218024730682373
        }
      ]
    }
  ]
}