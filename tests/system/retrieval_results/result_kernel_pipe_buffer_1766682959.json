{
  "query": "kernel pipe buffer",
  "timestamp": "2025-12-26 01:15:59",
  "retrieved_files": [
    {
      "source_file": "kernel/bpf/ringbuf.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:29:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\ringbuf.c`\n\n---\n\n# `bpf/ringbuf.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/ringbuf.c` 实现了 BPF（Berkeley Packet Filter）子系统中的**环形缓冲区（Ring Buffer）**机制，用于在内核与用户空间之间高效、安全地传递数据。该机制支持两种生产者模式：**内核生产者**（如 BPF 程序）和**用户空间生产者**，并提供内存映射（`mmap`）、等待队列通知、并发控制等核心功能，是 BPF 数据输出（如 perf event 替代方案）的关键基础设施。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_ringbuf`**  \n  环形缓冲区的核心结构体，包含：\n  - `waitq`：等待队列，用于通知用户空间有新数据\n  - `work`：IRQ 工作项，用于异步唤醒等待队列\n  - `mask`：环形缓冲区大小掩码（`data_sz - 1`），用于快速取模\n  - `pages` / `nr_pages`：物理页数组，支持双映射\n  - `spinlock`：用于内核生产者的自旋锁（SMP 对齐）\n  - `busy`：原子变量，用于用户空间生产者的互斥访问（避免持有自旋锁过久）\n  - `consumer_pos` / `producer_pos` / `pending_pos`：消费者、生产者和待提交位置（各自独占一页，支持不同 mmap 权限）\n  - `data[]`：实际数据存储区域（页对齐）\n\n- **`struct bpf_ringbuf_map`**  \n  封装标准 `bpf_map`，关联一个 `bpf_ringbuf` 实例。\n\n- **`struct bpf_ringbuf_hdr`**  \n  8 字节记录头，包含：\n  - `len`：记录有效载荷长度\n  - `pg_off`：记录在页内的偏移（用于跨页处理）\n\n### 主要函数\n\n- **`bpf_ringbuf_area_alloc()`**  \n  分配并初始化环形缓冲区的虚拟内存区域，采用**双映射数据页**技术简化环绕处理。\n\n- **`bpf_ringbuf_alloc()`**  \n  初始化 `bpf_ringbuf` 结构体，设置锁、等待队列、IRQ 工作项及初始位置。\n\n- **`bpf_ringbuf_free()`**  \n  释放环形缓冲区占用的虚拟内存和物理页。\n\n- **`ringbuf_map_alloc()`**  \n  BPF map 分配器回调，验证参数并创建 `bpf_ringbuf_map`。\n\n- **`ringbuf_map_free()`**  \n  BPF map 释放器回调，清理资源。\n\n- **`ringbuf_map_*_elem()` / `ringbuf_map_get_next_key()`**  \n  禁用标准 map 操作（返回 `-ENOTSUPP`），因为 ringbuf 不支持键值操作。\n\n- **`bpf_ringbuf_notify()`**  \n  IRQ 工作回调，唤醒所有等待数据的用户进程。\n\n## 3. 关键实现\n\n### 双映射数据页（Double-Mapped Data Pages）\n\n为简化环形缓冲区**环绕（wrap-around）**时的数据读取逻辑，数据页被**连续映射两次**：\n```\n[meta pages][data pages][data pages (same as first copy)]\n```\n当读取跨越缓冲区末尾时，可直接线性读取第二份映射，无需特殊处理。此设计同时适用于内核和用户空间 `mmap`。\n\n### 权限隔离与安全\n\n- **`consumer_pos` 和 `producer_pos` 各占独立页**，允许通过 `mmap` 设置不同权限：\n  - **内核生产者模式**：`producer_pos` 和数据页对用户空间为**只读**，防止篡改。\n  - **用户空间生产者模式**：仅 `consumer_pos` 对用户空间为**只读**，内核需严格验证用户提交的记录。\n\n### 并发控制策略\n\n- **内核生产者**：使用 `raw_spinlock_t` 保证多生产者安全。\n- **用户空间生产者**：使用 `atomic_t busy` 原子变量，避免在 BPF 程序回调期间长期持有 IRQ 自旋锁（可能导致死锁或延迟）。若 `busy` 被占用，`__bpf_user_ringbuf_peek()` 返回 `-EBUSY`。\n\n### 内存布局与对齐\n\n- 非 `mmap` 部分（`waitq` 到 `pending_pos`）大小由 `RINGBUF_PGOFF` 定义。\n- `consumer_pos`、`producer_pos` 和 `data` 均按 `PAGE_SIZE` 对齐，确保可独立映射。\n- 总元数据页数：`RINGBUF_NR_META_PAGES = RINGBUF_PGOFF + 2`（含 consumer/producer 页）。\n\n### 大小限制\n\n- 最大记录大小：`RINGBUF_MAX_RECORD_SZ = UINT_MAX / 4`（约 1GB）。\n- 最大缓冲区大小受 `bpf_ringbuf_hdr.pg_off`（32 位页偏移）限制，理论最大约 **64GB**。\n\n## 4. 依赖关系\n\n- **BPF 子系统**：依赖 `bpf_map` 基础设施（`bpf_map_area_alloc/free`、`bpf_map_init_from_attr`）。\n- **内存管理**：使用 `alloc_pages_node`、`vmap`/`vunmap`、`__free_page` 管理物理页和虚拟映射。\n- **同步机制**：依赖 `wait_queue`、`irq_work`、`raw_spinlock` 和 `atomic_t`。\n- **BTF（BPF Type Format）**：包含 BTF 相关头文件，可能用于未来类型验证（当前未直接使用）。\n- **用户 API**：与 `uapi/linux/bpf.h` 中的 `BPF_F_NUMA_NODE` 等标志交互。\n\n## 5. 使用场景\n\n- **BPF 程序输出数据**：替代 `bpf_perf_event_output()`，提供更低开销、更高吞吐的内核到用户空间数据通道。\n- **用户空间主动提交数据**：允许用户程序通过 ringbuf 向内核提交样本（需内核验证）。\n- **实时监控与追踪**：用于 eBPF 监控工具（如 `bpftrace`、`libbpf` 应用）高效收集内核事件。\n- **NUMA 感知分配**：支持通过 `BPF_F_NUMA_NODE` 标志在指定 NUMA 节点分配内存，优化性能。",
      "similarity": 0.5938260555267334,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 1,
          "end_line": 149,
          "content": [
            "#include <linux/bpf.h>",
            "#include <linux/btf.h>",
            "#include <linux/err.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/slab.h>",
            "#include <linux/filter.h>",
            "#include <linux/mm.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/wait.h>",
            "#include <linux/poll.h>",
            "#include <linux/kmemleak.h>",
            "#include <uapi/linux/btf.h>",
            "#include <linux/btf_ids.h>",
            "",
            "#define RINGBUF_CREATE_FLAG_MASK (BPF_F_NUMA_NODE)",
            "",
            "/* non-mmap()'able part of bpf_ringbuf (everything up to consumer page) */",
            "#define RINGBUF_PGOFF \\",
            "\t(offsetof(struct bpf_ringbuf, consumer_pos) >> PAGE_SHIFT)",
            "/* consumer page and producer page */",
            "#define RINGBUF_POS_PAGES 2",
            "#define RINGBUF_NR_META_PAGES (RINGBUF_PGOFF + RINGBUF_POS_PAGES)",
            "",
            "#define RINGBUF_MAX_RECORD_SZ (UINT_MAX/4)",
            "",
            "struct bpf_ringbuf {",
            "\twait_queue_head_t waitq;",
            "\tstruct irq_work work;",
            "\tu64 mask;",
            "\tstruct page **pages;",
            "\tint nr_pages;",
            "\traw_spinlock_t spinlock ____cacheline_aligned_in_smp;",
            "\t/* For user-space producer ring buffers, an atomic_t busy bit is used",
            "\t * to synchronize access to the ring buffers in the kernel, rather than",
            "\t * the spinlock that is used for kernel-producer ring buffers. This is",
            "\t * done because the ring buffer must hold a lock across a BPF program's",
            "\t * callback:",
            "\t *",
            "\t *    __bpf_user_ringbuf_peek() // lock acquired",
            "\t * -> program callback_fn()",
            "\t * -> __bpf_user_ringbuf_sample_release() // lock released",
            "\t *",
            "\t * It is unsafe and incorrect to hold an IRQ spinlock across what could",
            "\t * be a long execution window, so we instead simply disallow concurrent",
            "\t * access to the ring buffer by kernel consumers, and return -EBUSY from",
            "\t * __bpf_user_ringbuf_peek() if the busy bit is held by another task.",
            "\t */",
            "\tatomic_t busy ____cacheline_aligned_in_smp;",
            "\t/* Consumer and producer counters are put into separate pages to",
            "\t * allow each position to be mapped with different permissions.",
            "\t * This prevents a user-space application from modifying the",
            "\t * position and ruining in-kernel tracking. The permissions of the",
            "\t * pages depend on who is producing samples: user-space or the",
            "\t * kernel. Note that the pending counter is placed in the same",
            "\t * page as the producer, so that it shares the same cache line.",
            "\t *",
            "\t * Kernel-producer",
            "\t * ---------------",
            "\t * The producer position and data pages are mapped as r/o in",
            "\t * userspace. For this approach, bits in the header of samples are",
            "\t * used to signal to user-space, and to other producers, whether a",
            "\t * sample is currently being written.",
            "\t *",
            "\t * User-space producer",
            "\t * -------------------",
            "\t * Only the page containing the consumer position is mapped r/o in",
            "\t * user-space. User-space producers also use bits of the header to",
            "\t * communicate to the kernel, but the kernel must carefully check and",
            "\t * validate each sample to ensure that they're correctly formatted, and",
            "\t * fully contained within the ring buffer.",
            "\t */",
            "\tunsigned long consumer_pos __aligned(PAGE_SIZE);",
            "\tunsigned long producer_pos __aligned(PAGE_SIZE);",
            "\tunsigned long pending_pos;",
            "\tchar data[] __aligned(PAGE_SIZE);",
            "};",
            "",
            "struct bpf_ringbuf_map {",
            "\tstruct bpf_map map;",
            "\tstruct bpf_ringbuf *rb;",
            "};",
            "",
            "/* 8-byte ring buffer record header structure */",
            "struct bpf_ringbuf_hdr {",
            "\tu32 len;",
            "\tu32 pg_off;",
            "};",
            "",
            "static struct bpf_ringbuf *bpf_ringbuf_area_alloc(size_t data_sz, int numa_node)",
            "{",
            "\tconst gfp_t flags = GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL |",
            "\t\t\t    __GFP_NOWARN | __GFP_ZERO;",
            "\tint nr_meta_pages = RINGBUF_NR_META_PAGES;",
            "\tint nr_data_pages = data_sz >> PAGE_SHIFT;",
            "\tint nr_pages = nr_meta_pages + nr_data_pages;",
            "\tstruct page **pages, *page;",
            "\tstruct bpf_ringbuf *rb;",
            "\tsize_t array_size;",
            "\tint i;",
            "",
            "\t/* Each data page is mapped twice to allow \"virtual\"",
            "\t * continuous read of samples wrapping around the end of ring",
            "\t * buffer area:",
            "\t * ------------------------------------------------------",
            "\t * | meta pages |  real data pages  |  same data pages  |",
            "\t * ------------------------------------------------------",
            "\t * |            | 1 2 3 4 5 6 7 8 9 | 1 2 3 4 5 6 7 8 9 |",
            "\t * ------------------------------------------------------",
            "\t * |            | TA             DA | TA             DA |",
            "\t * ------------------------------------------------------",
            "\t *                               ^^^^^^^",
            "\t *                                  |",
            "\t * Here, no need to worry about special handling of wrapped-around",
            "\t * data due to double-mapped data pages. This works both in kernel and",
            "\t * when mmap()'ed in user-space, simplifying both kernel and",
            "\t * user-space implementations significantly.",
            "\t */",
            "\tarray_size = (nr_meta_pages + 2 * nr_data_pages) * sizeof(*pages);",
            "\tpages = bpf_map_area_alloc(array_size, numa_node);",
            "\tif (!pages)",
            "\t\treturn NULL;",
            "",
            "\tfor (i = 0; i < nr_pages; i++) {",
            "\t\tpage = alloc_pages_node(numa_node, flags, 0);",
            "\t\tif (!page) {",
            "\t\t\tnr_pages = i;",
            "\t\t\tgoto err_free_pages;",
            "\t\t}",
            "\t\tpages[i] = page;",
            "\t\tif (i >= nr_meta_pages)",
            "\t\t\tpages[nr_data_pages + i] = page;",
            "\t}",
            "",
            "\trb = vmap(pages, nr_meta_pages + 2 * nr_data_pages,",
            "\t\t  VM_MAP | VM_USERMAP, PAGE_KERNEL);",
            "\tif (rb) {",
            "\t\tkmemleak_not_leak(pages);",
            "\t\trb->pages = pages;",
            "\t\trb->nr_pages = nr_pages;",
            "\t\treturn rb;",
            "\t}",
            "",
            "err_free_pages:",
            "\tfor (i = 0; i < nr_pages; i++)",
            "\t\t__free_page(pages[i]);",
            "\tbpf_map_area_free(pages);",
            "\treturn NULL;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了bpf_ringbuf结构体及其相关宏，用于管理BPF环形缓冲区的元数据和数据区域。通过页面数组实现环形缓冲区的虚拟连续读取，支持用户态和内核态生产者的差异化权限控制，其中包含消费者/生产者位置指针、忙位原子变量及锁保护的元数据。",
          "similarity": 0.5177668333053589
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 150,
          "end_line": 258,
          "content": [
            "static void bpf_ringbuf_notify(struct irq_work *work)",
            "{",
            "\tstruct bpf_ringbuf *rb = container_of(work, struct bpf_ringbuf, work);",
            "",
            "\twake_up_all(&rb->waitq);",
            "}",
            "static void bpf_ringbuf_free(struct bpf_ringbuf *rb)",
            "{",
            "\t/* copy pages pointer and nr_pages to local variable, as we are going",
            "\t * to unmap rb itself with vunmap() below",
            "\t */",
            "\tstruct page **pages = rb->pages;",
            "\tint i, nr_pages = rb->nr_pages;",
            "",
            "\tvunmap(rb);",
            "\tfor (i = 0; i < nr_pages; i++)",
            "\t\t__free_page(pages[i]);",
            "\tbpf_map_area_free(pages);",
            "}",
            "static void ringbuf_map_free(struct bpf_map *map)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tbpf_ringbuf_free(rb_map->rb);",
            "\tbpf_map_area_free(rb_map);",
            "}",
            "static long ringbuf_map_update_elem(struct bpf_map *map, void *key, void *value,",
            "\t\t\t\t    u64 flags)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static long ringbuf_map_delete_elem(struct bpf_map *map, void *key)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static int ringbuf_map_get_next_key(struct bpf_map *map, void *key,",
            "\t\t\t\t    void *next_key)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static int ringbuf_map_mmap_kern(struct bpf_map *map, struct vm_area_struct *vma)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "",
            "\tif (vma->vm_flags & VM_WRITE) {",
            "\t\t/* allow writable mapping for the consumer_pos only */",
            "\t\tif (vma->vm_pgoff != 0 || vma->vm_end - vma->vm_start != PAGE_SIZE)",
            "\t\t\treturn -EPERM;",
            "\t}",
            "\t/* remap_vmalloc_range() checks size and offset constraints */",
            "\treturn remap_vmalloc_range(vma, rb_map->rb,",
            "\t\t\t\t   vma->vm_pgoff + RINGBUF_PGOFF);",
            "}",
            "static int ringbuf_map_mmap_user(struct bpf_map *map, struct vm_area_struct *vma)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "",
            "\tif (vma->vm_flags & VM_WRITE) {",
            "\t\tif (vma->vm_pgoff == 0)",
            "\t\t\t/* Disallow writable mappings to the consumer pointer,",
            "\t\t\t * and allow writable mappings to both the producer",
            "\t\t\t * position, and the ring buffer data itself.",
            "\t\t\t */",
            "\t\t\treturn -EPERM;",
            "\t}",
            "\t/* remap_vmalloc_range() checks size and offset constraints */",
            "\treturn remap_vmalloc_range(vma, rb_map->rb, vma->vm_pgoff + RINGBUF_PGOFF);",
            "}",
            "static unsigned long ringbuf_avail_data_sz(struct bpf_ringbuf *rb)",
            "{",
            "\tunsigned long cons_pos, prod_pos;",
            "",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos);",
            "\tprod_pos = smp_load_acquire(&rb->producer_pos);",
            "\treturn prod_pos - cons_pos;",
            "}",
            "static u32 ringbuf_total_data_sz(const struct bpf_ringbuf *rb)",
            "{",
            "\treturn rb->mask + 1;",
            "}",
            "static __poll_t ringbuf_map_poll_kern(struct bpf_map *map, struct file *filp,",
            "\t\t\t\t      struct poll_table_struct *pts)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tpoll_wait(filp, &rb_map->rb->waitq, pts);",
            "",
            "\tif (ringbuf_avail_data_sz(rb_map->rb))",
            "\t\treturn EPOLLIN | EPOLLRDNORM;",
            "\treturn 0;",
            "}",
            "static __poll_t ringbuf_map_poll_user(struct bpf_map *map, struct file *filp,",
            "\t\t\t\t      struct poll_table_struct *pts)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tpoll_wait(filp, &rb_map->rb->waitq, pts);",
            "",
            "\tif (ringbuf_avail_data_sz(rb_map->rb) < ringbuf_total_data_sz(rb_map->rb))",
            "\t\treturn EPOLLOUT | EPOLLWRNORM;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "bpf_ringbuf_notify, bpf_ringbuf_free, ringbuf_map_free, ringbuf_map_update_elem, ringbuf_map_delete_elem, ringbuf_map_get_next_key, ringbuf_map_mmap_kern, ringbuf_map_mmap_user, ringbuf_avail_data_sz, ringbuf_total_data_sz, ringbuf_map_poll_kern, ringbuf_map_poll_user",
          "description": "实现了环形缓冲区的事件通知、资源释放、内存映射控制及I/O监控功能。包含针对用户态和内核态的差异化mmap处理逻辑，通过spinlock和atomic_t实现并发控制，提供poll接口检测缓冲区可用数据状态。",
          "similarity": 0.5052219033241272
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 335,
          "end_line": 447,
          "content": [
            "static u64 ringbuf_map_mem_usage(const struct bpf_map *map)",
            "{",
            "\tstruct bpf_ringbuf *rb;",
            "\tint nr_data_pages;",
            "\tint nr_meta_pages;",
            "\tu64 usage = sizeof(struct bpf_ringbuf_map);",
            "",
            "\trb = container_of(map, struct bpf_ringbuf_map, map)->rb;",
            "\tusage += (u64)rb->nr_pages << PAGE_SHIFT;",
            "\tnr_meta_pages = RINGBUF_NR_META_PAGES;",
            "\tnr_data_pages = map->max_entries >> PAGE_SHIFT;",
            "\tusage += (nr_meta_pages + 2 * nr_data_pages) * sizeof(struct page *);",
            "\treturn usage;",
            "}",
            "static size_t bpf_ringbuf_rec_pg_off(struct bpf_ringbuf *rb,",
            "\t\t\t\t     struct bpf_ringbuf_hdr *hdr)",
            "{",
            "\treturn ((void *)hdr - (void *)rb) >> PAGE_SHIFT;",
            "}",
            "static void bpf_ringbuf_commit(void *sample, u64 flags, bool discard)",
            "{",
            "\tunsigned long rec_pos, cons_pos;",
            "\tstruct bpf_ringbuf_hdr *hdr;",
            "\tstruct bpf_ringbuf *rb;",
            "\tu32 new_len;",
            "",
            "\thdr = sample - BPF_RINGBUF_HDR_SZ;",
            "\trb = bpf_ringbuf_restore_from_rec(hdr);",
            "\tnew_len = hdr->len ^ BPF_RINGBUF_BUSY_BIT;",
            "\tif (discard)",
            "\t\tnew_len |= BPF_RINGBUF_DISCARD_BIT;",
            "",
            "\t/* update record header with correct final size prefix */",
            "\txchg(&hdr->len, new_len);",
            "",
            "\t/* if consumer caught up and is waiting for our record, notify about",
            "\t * new data availability",
            "\t */",
            "\trec_pos = (void *)hdr - (void *)rb->data;",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos) & rb->mask;",
            "",
            "\tif (flags & BPF_RB_FORCE_WAKEUP)",
            "\t\tirq_work_queue(&rb->work);",
            "\telse if (cons_pos == rec_pos && !(flags & BPF_RB_NO_WAKEUP))",
            "\t\tirq_work_queue(&rb->work);",
            "}",
            "static int __bpf_user_ringbuf_peek(struct bpf_ringbuf *rb, void **sample, u32 *size)",
            "{",
            "\tint err;",
            "\tu32 hdr_len, sample_len, total_len, flags, *hdr;",
            "\tu64 cons_pos, prod_pos;",
            "",
            "\t/* Synchronizes with smp_store_release() in user-space producer. */",
            "\tprod_pos = smp_load_acquire(&rb->producer_pos);",
            "\tif (prod_pos % 8)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Synchronizes with smp_store_release() in __bpf_user_ringbuf_sample_release() */",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos);",
            "\tif (cons_pos >= prod_pos)",
            "\t\treturn -ENODATA;",
            "",
            "\thdr = (u32 *)((uintptr_t)rb->data + (uintptr_t)(cons_pos & rb->mask));",
            "\t/* Synchronizes with smp_store_release() in user-space producer. */",
            "\thdr_len = smp_load_acquire(hdr);",
            "\tflags = hdr_len & (BPF_RINGBUF_BUSY_BIT | BPF_RINGBUF_DISCARD_BIT);",
            "\tsample_len = hdr_len & ~flags;",
            "\ttotal_len = round_up(sample_len + BPF_RINGBUF_HDR_SZ, 8);",
            "",
            "\t/* The sample must fit within the region advertised by the producer position. */",
            "\tif (total_len > prod_pos - cons_pos)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* The sample must fit within the data region of the ring buffer. */",
            "\tif (total_len > ringbuf_total_data_sz(rb))",
            "\t\treturn -E2BIG;",
            "",
            "\t/* The sample must fit into a struct bpf_dynptr. */",
            "\terr = bpf_dynptr_check_size(sample_len);",
            "\tif (err)",
            "\t\treturn -E2BIG;",
            "",
            "\tif (flags & BPF_RINGBUF_DISCARD_BIT) {",
            "\t\t/* If the discard bit is set, the sample should be skipped.",
            "\t\t *",
            "\t\t * Update the consumer pos, and return -EAGAIN so the caller",
            "\t\t * knows to skip this sample and try to read the next one.",
            "\t\t */",
            "\t\tsmp_store_release(&rb->consumer_pos, cons_pos + total_len);",
            "\t\treturn -EAGAIN;",
            "\t}",
            "",
            "\tif (flags & BPF_RINGBUF_BUSY_BIT)",
            "\t\treturn -ENODATA;",
            "",
            "\t*sample = (void *)((uintptr_t)rb->data +",
            "\t\t\t   (uintptr_t)((cons_pos + BPF_RINGBUF_HDR_SZ) & rb->mask));",
            "\t*size = sample_len;",
            "\treturn 0;",
            "}",
            "static void __bpf_user_ringbuf_sample_release(struct bpf_ringbuf *rb, size_t size, u64 flags)",
            "{",
            "\tu64 consumer_pos;",
            "\tu32 rounded_size = round_up(size + BPF_RINGBUF_HDR_SZ, 8);",
            "",
            "\t/* Using smp_load_acquire() is unnecessary here, as the busy-bit",
            "\t * prevents another task from writing to consumer_pos after it was read",
            "\t * by this task with smp_load_acquire() in __bpf_user_ringbuf_peek().",
            "\t */",
            "\tconsumer_pos = rb->consumer_pos;",
            "\t /* Synchronizes with smp_load_acquire() in user-space producer. */",
            "\tsmp_store_release(&rb->consumer_pos, consumer_pos + rounded_size);",
            "}"
          ],
          "function_name": "ringbuf_map_mem_usage, bpf_ringbuf_rec_pg_off, bpf_ringbuf_commit, __bpf_user_ringbuf_peek, __bpf_user_ringbuf_sample_release",
          "description": "提供了环形缓冲区的内存占用统计、记录位置转换、样本提交及消费操作。包含用户态生产者与消费者的同步机制，通过忙位防止竞态条件，确保样本数据完整性校验和消费进度更新的有序性。",
          "similarity": 0.4828310012817383
        }
      ]
    },
    {
      "source_file": "kernel/bpf/map_iter.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:18:18\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\map_iter.c`\n\n---\n\n# `bpf/map_iter.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/map_iter.c` 是 Linux 内核 BPF（Berkeley Packet Filter）子系统中的一个核心文件，用于实现对 BPF map 的迭代器（iterator）支持。该文件提供了两种类型的 BPF 迭代器：\n\n- **`bpf_map` 迭代器**：用于遍历系统中所有已注册的 BPF map。\n- **`bpf_map_elem` 迭代器**：用于遍历指定 BPF map 中的所有键值对元素。\n\n此外，该文件还定义了一个 BPF kfunc（内核函数）`bpf_map_sum_elem_count`，用于安全地获取 per-CPU 类型 map 的总元素数量。整个实现基于内核的 seq_file 机制和 BPF 迭代器框架，允许用户空间通过 BPF 程序安全、高效地访问 map 元数据或内容。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_iter_seq_map_info`**  \n  用于在 seq_file 迭代过程中保存当前遍历的 map ID。\n\n- **`struct bpf_iter__bpf_map`**  \n  BPF 程序上下文结构，作为 `bpf_map` 迭代器的元数据传递给 BPF 程序，包含指向当前 `bpf_map` 的指针。\n\n- **`struct bpf_iter__bpf_map_elem`**（隐式定义）  \n  由 `DEFINE_BPF_ITER_FUNC(bpf_map_elem, ...)` 宏生成，用于 `bpf_map_elem` 迭代器，包含 `map`、`key` 和 `value` 指针。\n\n- **`bpf_map_seq_ops`**  \n  实现 `seq_operations` 接口，用于遍历所有 BPF map。\n\n- **`bpf_map_reg_info` 与 `bpf_map_elem_reg_info`**  \n  分别注册 `bpf_map` 和 `bpf_map_elem` 两种 BPF 迭代器目标。\n\n- **`bpf_map_iter_kfunc_set`**  \n  注册 BPF kfunc `bpf_map_sum_elem_count`，供 BPF 程序调用。\n\n### 主要函数\n\n- **`bpf_map_seq_start/next/stop/show`**  \n  实现 seq_file 接口，用于遍历所有 BPF map。\n\n- **`__bpf_map_seq_show`**  \n  调用关联的 BPF 程序处理当前 map。\n\n- **`bpf_iter_attach_map` / `bpf_iter_detach_map`**  \n  在 `bpf_map_elem` 迭代器 attach/detach 时管理 map 引用计数，并验证访问权限。\n\n- **`bpf_iter_map_show_fdinfo` / `bpf_iter_map_fill_link_info`**  \n  提供迭代器链接的调试信息和用户空间查询接口。\n\n- **`bpf_map_sum_elem_count`**  \n  BPF kfunc，安全累加 per-CPU map 的元素计数。\n\n- **`bpf_map_iter_init`**  \n  模块初始化函数，注册两个 BPF 迭代器目标。\n\n- **`init_subsystem`**  \n  注册 BPF kfunc 到 BTF 系统。\n\n## 3. 关键实现\n\n### BPF Map 全局遍历（`bpf_map` 迭代器）\n\n- 使用 `bpf_map_get_curr_or_next(&map_id)` 按 ID 顺序遍历所有 map。\n- `seq_file` 的 `start`/`next` 函数通过递增 `map_id` 实现迭代。\n- 每次访问 map 后通过 `bpf_map_put()` 释放引用，确保资源安全。\n- 在 `stop` 阶段，若 `v == NULL`（表示迭代结束），会再次调用 `__bpf_map_seq_show` 并传入 `in_stop=true`，用于通知 BPF 程序迭代已结束。\n\n### BPF Map 元素遍历（`bpf_map_elem` 迭代器）\n\n- 通过 `bpf_iter_attach_map` 从用户传入的 `map_fd` 获取 map 实例。\n- 支持的 map 类型包括：`HASH`、`LRU_HASH`、`ARRAY` 及其 per-CPU 变体。\n- 在 attach 时验证 BPF 程序对 key/value 的最大访问尺寸是否超过 map 定义的尺寸，防止越界访问。\n- 对 per-CPU map，value 大小计算为 `round_up(value_size, 8) * num_possible_cpus()`，符合内核 per-CPU 布局。\n\n### BPF kfunc：`bpf_map_sum_elem_count`\n\n- 仅当 `map->elem_count` 非空时（通常由支持计数的 map 类型提供）才进行累加。\n- 使用 `for_each_possible_cpu` 遍历所有 CPU，通过 `per_cpu_ptr` 安全读取 per-CPU 计数。\n- 使用 `READ_ONCE` 避免编译器优化导致的非原子读取问题。\n- 标记为 `KF_TRUSTED_ARGS`，表示参数来自内核可信上下文。\n\n### BTF 与类型安全\n\n- 通过 `BTF_ID_LIST_GLOBAL_SINGLE` 获取 `struct bpf_map` 的 BTF ID，用于类型验证。\n- `ctx_arg_info` 中使用 `PTR_TO_BTF_ID_OR_NULL | PTR_TRUSTED` 确保传递给 BPF 程序的 map 指针类型安全且可信。\n- kfunc 通过 `BTF_SET8` 注册，并关联 `KFUNC` 类型标志。\n\n## 4. 依赖关系\n\n- **`<linux/bpf.h>` / `<linux/filter.h>`**：BPF 核心接口和程序执行框架。\n- **`<linux/fs.h>`**：seq_file 机制，用于实现迭代器输出。\n- **`<linux/btf_ids.h>`**：BTF（BPF Type Format）支持，用于类型验证和 kfunc 注册。\n- **`bpf_map_get_curr_or_next` / `bpf_map_put`**：来自 BPF map 管理子系统（`kernel/bpf/syscall.c` 等）。\n- **`bpf_iter_get_info` / `bpf_iter_run_prog`**：BPF 迭代器运行时支持（`kernel/bpf/bpf_iter.c`）。\n- **`register_btf_kfunc_id_set`**：BPF kfunc 注册机制。\n\n## 5. 使用场景\n\n- **系统监控与调试**：用户可通过 BPF 程序遍历所有 BPF map，收集其类型、ID、引用计数等元信息，用于性能分析或调试。\n- **Map 内容导出**：通过 `bpf_map_elem` 迭代器，用户空间可安全遍历指定 map 的所有键值对，实现 map 数据导出（如 `bpftool map dump` 的底层机制）。\n- **安全审计**：结合 BPF 程序，可对 map 访问模式进行监控或策略检查。\n- **Per-CPU 统计聚合**：BPF 程序可调用 `bpf_map_sum_elem_count` 快速获取 per-CPU map 的总元素数，用于指标采集。\n- **内核自省**：作为 BPF 迭代器框架的一部分，为内核提供标准化的 map 遍历能力，避免直接暴露内部数据结构。",
      "similarity": 0.5815204381942749,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/map_iter.c",
          "start_line": 225,
          "end_line": 228,
          "content": [
            "static int init_subsystem(void)",
            "{",
            "\treturn register_btf_kfunc_id_set(BPF_PROG_TYPE_UNSPEC, &bpf_map_iter_kfunc_set);",
            "}"
          ],
          "function_name": "init_subsystem",
          "description": "注册BPF kfunc集合到BTF系统，用于关联bpf_map_sum_elem_count函数，实现多CPU环境下map元素数量的统计功能",
          "similarity": 0.49377530813217163
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/map_iter.c",
          "start_line": 1,
          "end_line": 43,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2020 Facebook */",
            "#include <linux/bpf.h>",
            "#include <linux/fs.h>",
            "#include <linux/filter.h>",
            "#include <linux/kernel.h>",
            "#include <linux/btf_ids.h>",
            "",
            "struct bpf_iter_seq_map_info {",
            "\tu32 map_id;",
            "};",
            "",
            "static void *bpf_map_seq_start(struct seq_file *seq, loff_t *pos)",
            "{",
            "\tstruct bpf_iter_seq_map_info *info = seq->private;",
            "\tstruct bpf_map *map;",
            "",
            "\tmap = bpf_map_get_curr_or_next(&info->map_id);",
            "\tif (!map)",
            "\t\treturn NULL;",
            "",
            "\tif (*pos == 0)",
            "\t\t++*pos;",
            "\treturn map;",
            "}",
            "",
            "static void *bpf_map_seq_next(struct seq_file *seq, void *v, loff_t *pos)",
            "{",
            "\tstruct bpf_iter_seq_map_info *info = seq->private;",
            "",
            "\t++*pos;",
            "\t++info->map_id;",
            "\tbpf_map_put((struct bpf_map *)v);",
            "\treturn bpf_map_get_curr_or_next(&info->map_id);",
            "}",
            "",
            "struct bpf_iter__bpf_map {",
            "\t__bpf_md_ptr(struct bpf_iter_meta *, meta);",
            "\t__bpf_md_ptr(struct bpf_map *, map);",
            "};",
            "",
            "DEFINE_BPF_ITER_FUNC(bpf_map, struct bpf_iter_meta *meta, struct bpf_map *map)",
            ""
          ],
          "function_name": null,
          "description": "实现BPF map迭代器框架，提供seq_file遍历接口，通过bpf_map_seq_start和bpf_map_seq_next函数依次获取当前map及下一个map对象，使用 DEFINE_BPF_ITER_FUNC 注册迭代器回调函数",
          "similarity": 0.4913460612297058
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/map_iter.c",
          "start_line": 44,
          "end_line": 156,
          "content": [
            "static int __bpf_map_seq_show(struct seq_file *seq, void *v, bool in_stop)",
            "{",
            "\tstruct bpf_iter__bpf_map ctx;",
            "\tstruct bpf_iter_meta meta;",
            "\tstruct bpf_prog *prog;",
            "\tint ret = 0;",
            "",
            "\tctx.meta = &meta;",
            "\tctx.map = v;",
            "\tmeta.seq = seq;",
            "\tprog = bpf_iter_get_info(&meta, in_stop);",
            "\tif (prog)",
            "\t\tret = bpf_iter_run_prog(prog, &ctx);",
            "",
            "\treturn ret;",
            "}",
            "static int bpf_map_seq_show(struct seq_file *seq, void *v)",
            "{",
            "\treturn __bpf_map_seq_show(seq, v, false);",
            "}",
            "static void bpf_map_seq_stop(struct seq_file *seq, void *v)",
            "{",
            "\tif (!v)",
            "\t\t(void)__bpf_map_seq_show(seq, v, true);",
            "\telse",
            "\t\tbpf_map_put((struct bpf_map *)v);",
            "}",
            "static int bpf_iter_attach_map(struct bpf_prog *prog,",
            "\t\t\t       union bpf_iter_link_info *linfo,",
            "\t\t\t       struct bpf_iter_aux_info *aux)",
            "{",
            "\tu32 key_acc_size, value_acc_size, key_size, value_size;",
            "\tstruct bpf_map *map;",
            "\tbool is_percpu = false;",
            "\tint err = -EINVAL;",
            "",
            "\tif (!linfo->map.map_fd)",
            "\t\treturn -EBADF;",
            "",
            "\tmap = bpf_map_get_with_uref(linfo->map.map_fd);",
            "\tif (IS_ERR(map))",
            "\t\treturn PTR_ERR(map);",
            "",
            "\tif (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||",
            "\t    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||",
            "\t    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY)",
            "\t\tis_percpu = true;",
            "\telse if (map->map_type != BPF_MAP_TYPE_HASH &&",
            "\t\t map->map_type != BPF_MAP_TYPE_LRU_HASH &&",
            "\t\t map->map_type != BPF_MAP_TYPE_ARRAY)",
            "\t\tgoto put_map;",
            "",
            "\tkey_acc_size = prog->aux->max_rdonly_access;",
            "\tvalue_acc_size = prog->aux->max_rdwr_access;",
            "\tkey_size = map->key_size;",
            "\tif (!is_percpu)",
            "\t\tvalue_size = map->value_size;",
            "\telse",
            "\t\tvalue_size = round_up(map->value_size, 8) * num_possible_cpus();",
            "",
            "\tif (key_acc_size > key_size || value_acc_size > value_size) {",
            "\t\terr = -EACCES;",
            "\t\tgoto put_map;",
            "\t}",
            "",
            "\taux->map = map;",
            "\treturn 0;",
            "",
            "put_map:",
            "\tbpf_map_put_with_uref(map);",
            "\treturn err;",
            "}",
            "static void bpf_iter_detach_map(struct bpf_iter_aux_info *aux)",
            "{",
            "\tbpf_map_put_with_uref(aux->map);",
            "}",
            "void bpf_iter_map_show_fdinfo(const struct bpf_iter_aux_info *aux,",
            "\t\t\t      struct seq_file *seq)",
            "{",
            "\tseq_printf(seq, \"map_id:\\t%u\\n\", aux->map->id);",
            "}",
            "int bpf_iter_map_fill_link_info(const struct bpf_iter_aux_info *aux,",
            "\t\t\t\tstruct bpf_link_info *info)",
            "{",
            "\tinfo->iter.map.map_id = aux->map->id;",
            "\treturn 0;",
            "}",
            "static int __init bpf_map_iter_init(void)",
            "{",
            "\tint ret;",
            "",
            "\tbpf_map_reg_info.ctx_arg_info[0].btf_id = *btf_bpf_map_id;",
            "\tret = bpf_iter_reg_target(&bpf_map_reg_info);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\treturn bpf_iter_reg_target(&bpf_map_elem_reg_info);",
            "}",
            "__bpf_kfunc s64 bpf_map_sum_elem_count(const struct bpf_map *map)",
            "{",
            "\ts64 *pcount;",
            "\ts64 ret = 0;",
            "\tint cpu;",
            "",
            "\tif (!map || !map->elem_count)",
            "\t\treturn 0;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tpcount = per_cpu_ptr(map->elem_count, cpu);",
            "\t\tret += READ_ONCE(*pcount);",
            "\t}",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__bpf_map_seq_show, bpf_map_seq_show, bpf_map_seq_stop, bpf_iter_attach_map, bpf_iter_detach_map, bpf_iter_map_show_fdinfo, bpf_iter_map_fill_link_info, bpf_map_iter_init, bpf_map_sum_elem_count",
          "description": "包含BPF map迭代器辅助函数实现，包括map属性展示、权限校验、元素计数统计及链接信息填充，支持通过BPF程序对map进行遍历操作，包含map附加/分离逻辑和子系统初始化",
          "similarity": 0.4574027359485626
        }
      ]
    },
    {
      "source_file": "kernel/bpf/cpumap.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:06:36\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\cpumap.c`\n\n---\n\n# bpf/cpumap.c 技术文档\n\n## 文件概述\n\n`bpf/cpumap.c` 实现了 BPF CPU Map（cpumap）这一内核数据结构，主要用于 XDP（eXpress Data Path）框架中的跨 CPU 重定向功能。该机制允许 XDP 程序通过 `bpf_redirect_map()` 辅助函数将原始 XDP 数据帧重定向到指定的目标 CPU，由目标 CPU 上运行的专用内核线程（kthread）接收并将其转换为 `sk_buff` 后送入常规网络协议栈处理。cpumap 的核心目标是实现高性能、低延迟的网络预过滤，通过将 XDP 处理阶段与主网络栈解耦，提升系统在 10Gbps 及以上速率下的可扩展性和隔离性。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_cpu_map`**  \n  表示整个 CPU Map 对象，继承自 `struct bpf_map`，包含一个指向 `bpf_cpu_map_entry` 指针数组的 `cpu_map` 成员，用于按 CPU ID 索引目标条目。\n\n- **`struct bpf_cpu_map_entry`**  \n  表示映射到单个目标 CPU 的条目，关键成员包括：\n  - `cpu`：目标 CPU 编号\n  - `queue`：`ptr_ring` 类型的无锁环形缓冲队列，用于暂存重定向的 XDP 帧\n  - `kthread`：绑定到该 CPU 的消费者内核线程\n  - `prog`：可选的附加 BPF 程序，在帧入队后、送入协议栈前执行二次处理\n  - `bulkq`：每 CPU 的批量入队缓存（`xdp_bulk_queue`），用于提升入队性能\n\n- **`struct xdp_bulk_queue`**  \n  每 CPU 的批量队列结构，用于暂存最多 `CPU_MAP_BULK_SIZE`（默认 8）个待入队的 XDP 帧，减少对共享 `ptr_ring` 的频繁访问。\n\n### 主要函数\n\n- **`cpu_map_alloc()`**  \n  分配并初始化一个新的 CPU Map 实例，校验属性合法性（如 key/value 大小、最大条目数不超过 `NR_CPUS`）。\n\n- **`cpu_map_kthread_run()`**  \n  目标 CPU 上运行的消费者内核线程主循环，从 `ptr_ring` 队列批量消费 XDP 帧，执行可选 BPF 程序，并将结果帧转换为 `sk_buff` 送入网络栈。\n\n- **`cpu_map_bpf_prog_run_xdp()`**  \n  在消费线程中执行附加的 XDP 类型 BPF 程序，支持 `XDP_PASS`、`XDP_REDIRECT`、`XDP_DROP` 等动作。\n\n- **`cpu_map_bpf_prog_run_skb()`**  \n  处理已转换为 `sk_buff` 的数据包（通常来自重定向失败或特殊路径），执行通用 XDP BPF 程序。\n\n- **`__cpu_map_ring_cleanup()`**  \n  安全清理 `ptr_ring` 队列中的残留帧，确保资源正确释放。\n\n## 关键实现\n\n### 无锁批量入队与消费\n\n- **生产者侧（XDP 程序）**：使用每 CPU 的 `xdp_bulk_queue` 缓存待入队帧。当缓存满或需要 flush 时，一次性将批量帧原子地推入目标 CPU 条目的 `ptr_ring` 队列，减少锁竞争。\n- **消费者侧（kthread）**：每个 `bpf_cpu_map_entry` 绑定一个专用 kthread，独占消费其 `ptr_ring`。通过 `__ptr_ring_consume_batched()` 批量获取帧，提升吞吐。\n\n### 跨 CPU 重定向流程\n\n1. XDP 程序调用 `bpf_redirect_map(map, cpu_id, 0)`。\n2. 内核将当前 XDP 帧暂存到当前 CPU 对应目标 `cpu_id` 的 `bulkq` 中。\n3. 在驱动 `->poll()` 结束或显式 flush 时，将 `bulkq` 中的帧批量推入目标 CPU 的 `ptr_ring`。\n4. 目标 CPU 的 kthread 被唤醒，消费帧，执行可选 BPF 程序，转换为 `sk_buff` 并调用 `netif_receive_skb_list()` 送入协议栈。\n\n### BPF 程序二次处理\n\n每个 `bpf_cpu_map_entry` 可关联一个 BPF 程序。消费线程在处理帧前会执行该程序，支持进一步过滤、修改或重定向（如再次重定向到设备或另一个 CPU），增强了灵活性。\n\n### 内存与资源管理\n\n- 使用 `__ptr_set_bit(0, &ptr)` 标记 `sk_buff` 指针（最低位为 1），普通 XDP 帧指针最低位为 0，便于在清理时区分类型。\n- kthread 在退出前确保队列为空，防止内存泄漏。\n- 通过 RCU 机制安全地更新和释放 map 条目。\n\n## 依赖关系\n\n- **BPF 子系统**：依赖 `bpf.h`、`filter.h` 提供 map 基础框架、BPF 程序执行接口。\n- **XDP 框架**：依赖 `xdp.h`、`xdp_frame` 结构及 `xdp_do_redirect()` 等重定向机制。\n- **网络核心**：使用 `netdevice.h` 的 `netif_receive_skb_list()` 将数据包送入协议栈。\n- **内核同步原语**：使用 `ptr_ring`（无锁环形缓冲区）、`completion`（线程启动同步）、`rcu_work`（延迟释放）。\n- **调度与线程**：依赖 `kthread` 创建 CPU 绑定的消费者线程。\n- **追踪**：集成 `trace/events/xdp.h` 提供 XDP 事件追踪。\n\n## 使用场景\n\n- **高性能网络预过滤**：在专用 CPU 上运行 XDP 程序进行 DDoS 防御、ACL 过滤等，将合法流量重定向到其他 CPU 的协议栈处理，避免主 CPU 过载。\n- **负载均衡**：将流量按策略分发到多个 CPU，提升多核系统的网络处理能力。\n- **服务链（Service Chaining）**：通过级联 cpumap 和 devmap，构建复杂的流量处理流水线。\n- **隔离关键路径**：将 XDP 处理与应用层网络处理隔离到不同 CPU，减少相互干扰，保障低延迟。",
      "similarity": 0.580132246017456,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/cpumap.c",
          "start_line": 1,
          "end_line": 116,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* bpf/cpumap.c",
            " *",
            " * Copyright (c) 2017 Jesper Dangaard Brouer, Red Hat Inc.",
            " */",
            "",
            "/**",
            " * DOC: cpu map",
            " * The 'cpumap' is primarily used as a backend map for XDP BPF helper",
            " * call bpf_redirect_map() and XDP_REDIRECT action, like 'devmap'.",
            " *",
            " * Unlike devmap which redirects XDP frames out to another NIC device,",
            " * this map type redirects raw XDP frames to another CPU.  The remote",
            " * CPU will do SKB-allocation and call the normal network stack.",
            " */",
            "/*",
            " * This is a scalability and isolation mechanism, that allow",
            " * separating the early driver network XDP layer, from the rest of the",
            " * netstack, and assigning dedicated CPUs for this stage.  This",
            " * basically allows for 10G wirespeed pre-filtering via bpf.",
            " */",
            "#include <linux/bitops.h>",
            "#include <linux/bpf.h>",
            "#include <linux/filter.h>",
            "#include <linux/ptr_ring.h>",
            "#include <net/xdp.h>",
            "",
            "#include <linux/sched.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/kthread.h>",
            "#include <linux/completion.h>",
            "#include <trace/events/xdp.h>",
            "#include <linux/btf_ids.h>",
            "",
            "#include <linux/netdevice.h>   /* netif_receive_skb_list */",
            "#include <linux/etherdevice.h> /* eth_type_trans */",
            "",
            "/* General idea: XDP packets getting XDP redirected to another CPU,",
            " * will maximum be stored/queued for one driver ->poll() call.  It is",
            " * guaranteed that queueing the frame and the flush operation happen on",
            " * same CPU.  Thus, cpu_map_flush operation can deduct via this_cpu_ptr()",
            " * which queue in bpf_cpu_map_entry contains packets.",
            " */",
            "",
            "#define CPU_MAP_BULK_SIZE 8  /* 8 == one cacheline on 64-bit archs */",
            "struct bpf_cpu_map_entry;",
            "struct bpf_cpu_map;",
            "",
            "struct xdp_bulk_queue {",
            "\tvoid *q[CPU_MAP_BULK_SIZE];",
            "\tstruct list_head flush_node;",
            "\tstruct bpf_cpu_map_entry *obj;",
            "\tunsigned int count;",
            "};",
            "",
            "/* Struct for every remote \"destination\" CPU in map */",
            "struct bpf_cpu_map_entry {",
            "\tu32 cpu;    /* kthread CPU and map index */",
            "\tint map_id; /* Back reference to map */",
            "",
            "\t/* XDP can run multiple RX-ring queues, need __percpu enqueue store */",
            "\tstruct xdp_bulk_queue __percpu *bulkq;",
            "",
            "\t/* Queue with potential multi-producers, and single-consumer kthread */",
            "\tstruct ptr_ring *queue;",
            "\tstruct task_struct *kthread;",
            "",
            "\tstruct bpf_cpumap_val value;",
            "\tstruct bpf_prog *prog;",
            "",
            "\tstruct completion kthread_running;",
            "\tstruct rcu_work free_work;",
            "};",
            "",
            "struct bpf_cpu_map {",
            "\tstruct bpf_map map;",
            "\t/* Below members specific for map type */",
            "\tstruct bpf_cpu_map_entry __rcu **cpu_map;",
            "};",
            "",
            "static DEFINE_PER_CPU(struct list_head, cpu_map_flush_list);",
            "",
            "static struct bpf_map *cpu_map_alloc(union bpf_attr *attr)",
            "{",
            "\tu32 value_size = attr->value_size;",
            "\tstruct bpf_cpu_map *cmap;",
            "",
            "\t/* check sanity of attributes */",
            "\tif (attr->max_entries == 0 || attr->key_size != 4 ||",
            "\t    (value_size != offsetofend(struct bpf_cpumap_val, qsize) &&",
            "\t     value_size != offsetofend(struct bpf_cpumap_val, bpf_prog.fd)) ||",
            "\t    attr->map_flags & ~BPF_F_NUMA_NODE)",
            "\t\treturn ERR_PTR(-EINVAL);",
            "",
            "\t/* Pre-limit array size based on NR_CPUS, not final CPU check */",
            "\tif (attr->max_entries > NR_CPUS)",
            "\t\treturn ERR_PTR(-E2BIG);",
            "",
            "\tcmap = bpf_map_area_alloc(sizeof(*cmap), NUMA_NO_NODE);",
            "\tif (!cmap)",
            "\t\treturn ERR_PTR(-ENOMEM);",
            "",
            "\tbpf_map_init_from_attr(&cmap->map, attr);",
            "",
            "\t/* Alloc array for possible remote \"destination\" CPUs */",
            "\tcmap->cpu_map = bpf_map_area_alloc(cmap->map.max_entries *",
            "\t\t\t\t\t   sizeof(struct bpf_cpu_map_entry *),",
            "\t\t\t\t\t   cmap->map.numa_node);",
            "\tif (!cmap->cpu_map) {",
            "\t\tbpf_map_area_free(cmap);",
            "\t\treturn ERR_PTR(-ENOMEM);",
            "\t}",
            "",
            "\treturn &cmap->map;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了cpu_map的数据结构及初始化函数，用于XDP帧跨CPU重定向。实现基于RCU的cpu_map_entry数组管理，支持通过BPF程序过滤并转发网络数据包到指定CPU。",
          "similarity": 0.507246732711792
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/cpumap.c",
          "start_line": 367,
          "end_line": 472,
          "content": [
            "static int __cpu_map_load_bpf_program(struct bpf_cpu_map_entry *rcpu,",
            "\t\t\t\t      struct bpf_map *map, int fd)",
            "{",
            "\tstruct bpf_prog *prog;",
            "",
            "\tprog = bpf_prog_get_type(fd, BPF_PROG_TYPE_XDP);",
            "\tif (IS_ERR(prog))",
            "\t\treturn PTR_ERR(prog);",
            "",
            "\tif (prog->expected_attach_type != BPF_XDP_CPUMAP ||",
            "\t    !bpf_prog_map_compatible(map, prog)) {",
            "\t\tbpf_prog_put(prog);",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\trcpu->value.bpf_prog.id = prog->aux->id;",
            "\trcpu->prog = prog;",
            "",
            "\treturn 0;",
            "}",
            "static void __cpu_map_entry_free(struct work_struct *work)",
            "{",
            "\tstruct bpf_cpu_map_entry *rcpu;",
            "",
            "\t/* This cpu_map_entry have been disconnected from map and one",
            "\t * RCU grace-period have elapsed. Thus, XDP cannot queue any",
            "\t * new packets and cannot change/set flush_needed that can",
            "\t * find this entry.",
            "\t */",
            "\trcpu = container_of(to_rcu_work(work), struct bpf_cpu_map_entry, free_work);",
            "",
            "\t/* kthread_stop will wake_up_process and wait for it to complete.",
            "\t * cpu_map_kthread_run() makes sure the pointer ring is empty",
            "\t * before exiting.",
            "\t */",
            "\tkthread_stop(rcpu->kthread);",
            "",
            "\tif (rcpu->prog)",
            "\t\tbpf_prog_put(rcpu->prog);",
            "\t/* The queue should be empty at this point */",
            "\t__cpu_map_ring_cleanup(rcpu->queue);",
            "\tptr_ring_cleanup(rcpu->queue, NULL);",
            "\tkfree(rcpu->queue);",
            "\tfree_percpu(rcpu->bulkq);",
            "\tkfree(rcpu);",
            "}",
            "static void __cpu_map_entry_replace(struct bpf_cpu_map *cmap,",
            "\t\t\t\t    u32 key_cpu, struct bpf_cpu_map_entry *rcpu)",
            "{",
            "\tstruct bpf_cpu_map_entry *old_rcpu;",
            "",
            "\told_rcpu = unrcu_pointer(xchg(&cmap->cpu_map[key_cpu], RCU_INITIALIZER(rcpu)));",
            "\tif (old_rcpu) {",
            "\t\tINIT_RCU_WORK(&old_rcpu->free_work, __cpu_map_entry_free);",
            "\t\tqueue_rcu_work(system_wq, &old_rcpu->free_work);",
            "\t}",
            "}",
            "static long cpu_map_delete_elem(struct bpf_map *map, void *key)",
            "{",
            "\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);",
            "\tu32 key_cpu = *(u32 *)key;",
            "",
            "\tif (key_cpu >= map->max_entries)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* notice caller map_delete_elem() uses rcu_read_lock() */",
            "\t__cpu_map_entry_replace(cmap, key_cpu, NULL);",
            "\treturn 0;",
            "}",
            "static long cpu_map_update_elem(struct bpf_map *map, void *key, void *value,",
            "\t\t\t\tu64 map_flags)",
            "{",
            "\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);",
            "\tstruct bpf_cpumap_val cpumap_value = {};",
            "\tstruct bpf_cpu_map_entry *rcpu;",
            "\t/* Array index key correspond to CPU number */",
            "\tu32 key_cpu = *(u32 *)key;",
            "",
            "\tmemcpy(&cpumap_value, value, map->value_size);",
            "",
            "\tif (unlikely(map_flags > BPF_EXIST))",
            "\t\treturn -EINVAL;",
            "\tif (unlikely(key_cpu >= cmap->map.max_entries))",
            "\t\treturn -E2BIG;",
            "\tif (unlikely(map_flags == BPF_NOEXIST))",
            "\t\treturn -EEXIST;",
            "\tif (unlikely(cpumap_value.qsize > 16384)) /* sanity limit on qsize */",
            "\t\treturn -EOVERFLOW;",
            "",
            "\t/* Make sure CPU is a valid possible cpu */",
            "\tif (key_cpu >= nr_cpumask_bits || !cpu_possible(key_cpu))",
            "\t\treturn -ENODEV;",
            "",
            "\tif (cpumap_value.qsize == 0) {",
            "\t\trcpu = NULL; /* Same as deleting */",
            "\t} else {",
            "\t\t/* Updating qsize cause re-allocation of bpf_cpu_map_entry */",
            "\t\trcpu = __cpu_map_entry_alloc(map, &cpumap_value, key_cpu);",
            "\t\tif (!rcpu)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\trcu_read_lock();",
            "\t__cpu_map_entry_replace(cmap, key_cpu, rcpu);",
            "\trcu_read_unlock();",
            "\treturn 0;",
            "}"
          ],
          "function_name": "__cpu_map_load_bpf_program, __cpu_map_entry_free, __cpu_map_entry_replace, cpu_map_delete_elem, cpu_map_update_elem",
          "description": "实现cpu_map_entry的生命周期管理，包括BPF程序加载验证(__cpu_map_load_bpf_program)、条目替换(__cpu_map_entry_replace)、资源释放(__cpu_map_entry_free)等核心操作。",
          "similarity": 0.5025337934494019
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/cpumap.c",
          "start_line": 238,
          "end_line": 364,
          "content": [
            "static int cpu_map_bpf_prog_run(struct bpf_cpu_map_entry *rcpu, void **frames,",
            "\t\t\t\tint xdp_n, struct xdp_cpumap_stats *stats,",
            "\t\t\t\tstruct list_head *list)",
            "{",
            "\tint nframes;",
            "",
            "\tif (!rcpu->prog)",
            "\t\treturn xdp_n;",
            "",
            "\trcu_read_lock_bh();",
            "",
            "\tnframes = cpu_map_bpf_prog_run_xdp(rcpu, frames, xdp_n, stats);",
            "",
            "\tif (stats->redirect)",
            "\t\txdp_do_flush();",
            "",
            "\tif (unlikely(!list_empty(list)))",
            "\t\tcpu_map_bpf_prog_run_skb(rcpu, list, stats);",
            "",
            "\trcu_read_unlock_bh(); /* resched point, may call do_softirq() */",
            "",
            "\treturn nframes;",
            "}",
            "static int cpu_map_kthread_run(void *data)",
            "{",
            "\tstruct bpf_cpu_map_entry *rcpu = data;",
            "\tunsigned long last_qs = jiffies;",
            "",
            "\tcomplete(&rcpu->kthread_running);",
            "\tset_current_state(TASK_INTERRUPTIBLE);",
            "",
            "\t/* When kthread gives stop order, then rcpu have been disconnected",
            "\t * from map, thus no new packets can enter. Remaining in-flight",
            "\t * per CPU stored packets are flushed to this queue.  Wait honoring",
            "\t * kthread_stop signal until queue is empty.",
            "\t */",
            "\twhile (!kthread_should_stop() || !__ptr_ring_empty(rcpu->queue)) {",
            "\t\tstruct xdp_cpumap_stats stats = {}; /* zero stats */",
            "\t\tunsigned int kmem_alloc_drops = 0, sched = 0;",
            "\t\tgfp_t gfp = __GFP_ZERO | GFP_ATOMIC;",
            "\t\tint i, n, m, nframes, xdp_n;",
            "\t\tvoid *frames[CPUMAP_BATCH];",
            "\t\tvoid *skbs[CPUMAP_BATCH];",
            "\t\tLIST_HEAD(list);",
            "",
            "\t\t/* Release CPU reschedule checks */",
            "\t\tif (__ptr_ring_empty(rcpu->queue)) {",
            "\t\t\tset_current_state(TASK_INTERRUPTIBLE);",
            "\t\t\t/* Recheck to avoid lost wake-up */",
            "\t\t\tif (__ptr_ring_empty(rcpu->queue)) {",
            "\t\t\t\tschedule();",
            "\t\t\t\tsched = 1;",
            "\t\t\t\tlast_qs = jiffies;",
            "\t\t\t} else {",
            "\t\t\t\t__set_current_state(TASK_RUNNING);",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\trcu_softirq_qs_periodic(last_qs);",
            "\t\t\tsched = cond_resched();",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * The bpf_cpu_map_entry is single consumer, with this",
            "\t\t * kthread CPU pinned. Lockless access to ptr_ring",
            "\t\t * consume side valid as no-resize allowed of queue.",
            "\t\t */",
            "\t\tn = __ptr_ring_consume_batched(rcpu->queue, frames,",
            "\t\t\t\t\t       CPUMAP_BATCH);",
            "\t\tfor (i = 0, xdp_n = 0; i < n; i++) {",
            "\t\t\tvoid *f = frames[i];",
            "\t\t\tstruct page *page;",
            "",
            "\t\t\tif (unlikely(__ptr_test_bit(0, &f))) {",
            "\t\t\t\tstruct sk_buff *skb = f;",
            "",
            "\t\t\t\t__ptr_clear_bit(0, &skb);",
            "\t\t\t\tlist_add_tail(&skb->list, &list);",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tframes[xdp_n++] = f;",
            "\t\t\tpage = virt_to_page(f);",
            "",
            "\t\t\t/* Bring struct page memory area to curr CPU. Read by",
            "\t\t\t * build_skb_around via page_is_pfmemalloc(), and when",
            "\t\t\t * freed written by page_frag_free call.",
            "\t\t\t */",
            "\t\t\tprefetchw(page);",
            "\t\t}",
            "",
            "\t\t/* Support running another XDP prog on this CPU */",
            "\t\tnframes = cpu_map_bpf_prog_run(rcpu, frames, xdp_n, &stats, &list);",
            "\t\tif (nframes) {",
            "\t\t\tm = kmem_cache_alloc_bulk(skbuff_cache, gfp, nframes, skbs);",
            "\t\t\tif (unlikely(m == 0)) {",
            "\t\t\t\tfor (i = 0; i < nframes; i++)",
            "\t\t\t\t\tskbs[i] = NULL; /* effect: xdp_return_frame */",
            "\t\t\t\tkmem_alloc_drops += nframes;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tlocal_bh_disable();",
            "\t\tfor (i = 0; i < nframes; i++) {",
            "\t\t\tstruct xdp_frame *xdpf = frames[i];",
            "\t\t\tstruct sk_buff *skb = skbs[i];",
            "",
            "\t\t\tskb = __xdp_build_skb_from_frame(xdpf, skb,",
            "\t\t\t\t\t\t\t xdpf->dev_rx);",
            "\t\t\tif (!skb) {",
            "\t\t\t\txdp_return_frame(xdpf);",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tlist_add_tail(&skb->list, &list);",
            "\t\t}",
            "\t\tnetif_receive_skb_list(&list);",
            "",
            "\t\t/* Feedback loop via tracepoint */",
            "\t\ttrace_xdp_cpumap_kthread(rcpu->map_id, n, kmem_alloc_drops,",
            "\t\t\t\t\t sched, &stats);",
            "",
            "\t\tlocal_bh_enable(); /* resched point, may call do_softirq() */",
            "\t}",
            "\t__set_current_state(TASK_RUNNING);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "cpu_map_bpf_prog_run, cpu_map_kthread_run",
          "description": "kthread线程处理逻辑，包含从环形缓冲区消费数据包、执行BPF程序、构建skb并提交到网络栈的全流程，包含批量处理优化和软中断调度点。",
          "similarity": 0.5025064945220947
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/bpf/cpumap.c",
          "start_line": 564,
          "end_line": 678,
          "content": [
            "static void cpu_map_free(struct bpf_map *map)",
            "{",
            "\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);",
            "\tu32 i;",
            "",
            "\t/* At this point bpf_prog->aux->refcnt == 0 and this map->refcnt == 0,",
            "\t * so the bpf programs (can be more than one that used this map) were",
            "\t * disconnected from events. Wait for outstanding critical sections in",
            "\t * these programs to complete. synchronize_rcu() below not only",
            "\t * guarantees no further \"XDP/bpf-side\" reads against",
            "\t * bpf_cpu_map->cpu_map, but also ensure pending flush operations",
            "\t * (if any) are completed.",
            "\t */",
            "\tsynchronize_rcu();",
            "",
            "\t/* The only possible user of bpf_cpu_map_entry is",
            "\t * cpu_map_kthread_run().",
            "\t */",
            "\tfor (i = 0; i < cmap->map.max_entries; i++) {",
            "\t\tstruct bpf_cpu_map_entry *rcpu;",
            "",
            "\t\trcpu = rcu_dereference_raw(cmap->cpu_map[i]);",
            "\t\tif (!rcpu)",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Stop kthread and cleanup entry directly */",
            "\t\t__cpu_map_entry_free(&rcpu->free_work.work);",
            "\t}",
            "\tbpf_map_area_free(cmap->cpu_map);",
            "\tbpf_map_area_free(cmap);",
            "}",
            "static int cpu_map_get_next_key(struct bpf_map *map, void *key, void *next_key)",
            "{",
            "\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);",
            "\tu32 index = key ? *(u32 *)key : U32_MAX;",
            "\tu32 *next = next_key;",
            "",
            "\tif (index >= cmap->map.max_entries) {",
            "\t\t*next = 0;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (index == cmap->map.max_entries - 1)",
            "\t\treturn -ENOENT;",
            "\t*next = index + 1;",
            "\treturn 0;",
            "}",
            "static long cpu_map_redirect(struct bpf_map *map, u64 index, u64 flags)",
            "{",
            "\treturn __bpf_xdp_redirect_map(map, index, flags, 0,",
            "\t\t\t\t      __cpu_map_lookup_elem);",
            "}",
            "static u64 cpu_map_mem_usage(const struct bpf_map *map)",
            "{",
            "\tu64 usage = sizeof(struct bpf_cpu_map);",
            "",
            "\t/* Currently the dynamically allocated elements are not counted */",
            "\tusage += (u64)map->max_entries * sizeof(struct bpf_cpu_map_entry *);",
            "\treturn usage;",
            "}",
            "static void bq_flush_to_queue(struct xdp_bulk_queue *bq)",
            "{",
            "\tstruct bpf_cpu_map_entry *rcpu = bq->obj;",
            "\tunsigned int processed = 0, drops = 0;",
            "\tconst int to_cpu = rcpu->cpu;",
            "\tstruct ptr_ring *q;",
            "\tint i;",
            "",
            "\tif (unlikely(!bq->count))",
            "\t\treturn;",
            "",
            "\tq = rcpu->queue;",
            "\tspin_lock(&q->producer_lock);",
            "",
            "\tfor (i = 0; i < bq->count; i++) {",
            "\t\tstruct xdp_frame *xdpf = bq->q[i];",
            "\t\tint err;",
            "",
            "\t\terr = __ptr_ring_produce(q, xdpf);",
            "\t\tif (err) {",
            "\t\t\tdrops++;",
            "\t\t\txdp_return_frame_rx_napi(xdpf);",
            "\t\t}",
            "\t\tprocessed++;",
            "\t}",
            "\tbq->count = 0;",
            "\tspin_unlock(&q->producer_lock);",
            "",
            "\t__list_del_clearprev(&bq->flush_node);",
            "",
            "\t/* Feedback loop via tracepoints */",
            "\ttrace_xdp_cpumap_enqueue(rcpu->map_id, processed, drops, to_cpu);",
            "}",
            "static void bq_enqueue(struct bpf_cpu_map_entry *rcpu, struct xdp_frame *xdpf)",
            "{",
            "\tstruct list_head *flush_list = this_cpu_ptr(&cpu_map_flush_list);",
            "\tstruct xdp_bulk_queue *bq = this_cpu_ptr(rcpu->bulkq);",
            "",
            "\tif (unlikely(bq->count == CPU_MAP_BULK_SIZE))",
            "\t\tbq_flush_to_queue(bq);",
            "",
            "\t/* Notice, xdp_buff/page MUST be queued here, long enough for",
            "\t * driver to code invoking us to finished, due to driver",
            "\t * (e.g. ixgbe) recycle tricks based on page-refcnt.",
            "\t *",
            "\t * Thus, incoming xdp_frame is always queued here (else we race",
            "\t * with another CPU on page-refcnt and remaining driver code).",
            "\t * Queue time is very short, as driver will invoke flush",
            "\t * operation, when completing napi->poll call.",
            "\t */",
            "\tbq->q[bq->count++] = xdpf;",
            "",
            "\tif (!bq->flush_node.prev)",
            "\t\tlist_add(&bq->flush_node, flush_list);",
            "}"
          ],
          "function_name": "cpu_map_free, cpu_map_get_next_key, cpu_map_redirect, cpu_map_mem_usage, bq_flush_to_queue, bq_enqueue",
          "description": "提供映射销毁(cpu_map_free)、键遍历(cpu_map_get_next_key)、内存使用统计(cpu_map_mem_usage)等功能，包含批量队列刷新(bq_flush_to_queue)和帧入队(bq_enqueue)的底层实现。",
          "similarity": 0.5019612312316895
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/cpumap.c",
          "start_line": 117,
          "end_line": 232,
          "content": [
            "static void __cpu_map_ring_cleanup(struct ptr_ring *ring)",
            "{",
            "\t/* The tear-down procedure should have made sure that queue is",
            "\t * empty.  See __cpu_map_entry_replace() and work-queue",
            "\t * invoked cpu_map_kthread_stop(). Catch any broken behaviour",
            "\t * gracefully and warn once.",
            "\t */",
            "\tvoid *ptr;",
            "",
            "\twhile ((ptr = ptr_ring_consume(ring))) {",
            "\t\tWARN_ON_ONCE(1);",
            "\t\tif (unlikely(__ptr_test_bit(0, &ptr))) {",
            "\t\t\t__ptr_clear_bit(0, &ptr);",
            "\t\t\tkfree_skb(ptr);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\txdp_return_frame(ptr);",
            "\t}",
            "}",
            "static void cpu_map_bpf_prog_run_skb(struct bpf_cpu_map_entry *rcpu,",
            "\t\t\t\t     struct list_head *listp,",
            "\t\t\t\t     struct xdp_cpumap_stats *stats)",
            "{",
            "\tstruct sk_buff *skb, *tmp;",
            "\tstruct xdp_buff xdp;",
            "\tu32 act;",
            "\tint err;",
            "",
            "\tlist_for_each_entry_safe(skb, tmp, listp, list) {",
            "\t\tact = bpf_prog_run_generic_xdp(skb, &xdp, rcpu->prog);",
            "\t\tswitch (act) {",
            "\t\tcase XDP_PASS:",
            "\t\t\tbreak;",
            "\t\tcase XDP_REDIRECT:",
            "\t\t\tskb_list_del_init(skb);",
            "\t\t\terr = xdp_do_generic_redirect(skb->dev, skb, &xdp,",
            "\t\t\t\t\t\t      rcpu->prog);",
            "\t\t\tif (unlikely(err)) {",
            "\t\t\t\tkfree_skb(skb);",
            "\t\t\t\tstats->drop++;",
            "\t\t\t} else {",
            "\t\t\t\tstats->redirect++;",
            "\t\t\t}",
            "\t\t\treturn;",
            "\t\tdefault:",
            "\t\t\tbpf_warn_invalid_xdp_action(NULL, rcpu->prog, act);",
            "\t\t\tfallthrough;",
            "\t\tcase XDP_ABORTED:",
            "\t\t\ttrace_xdp_exception(skb->dev, rcpu->prog, act);",
            "\t\t\tfallthrough;",
            "\t\tcase XDP_DROP:",
            "\t\t\tskb_list_del_init(skb);",
            "\t\t\tkfree_skb(skb);",
            "\t\t\tstats->drop++;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "}",
            "static int cpu_map_bpf_prog_run_xdp(struct bpf_cpu_map_entry *rcpu,",
            "\t\t\t\t    void **frames, int n,",
            "\t\t\t\t    struct xdp_cpumap_stats *stats)",
            "{",
            "\tstruct xdp_rxq_info rxq = {};",
            "\tstruct xdp_buff xdp;",
            "\tint i, nframes = 0;",
            "",
            "\txdp_set_return_frame_no_direct();",
            "\txdp.rxq = &rxq;",
            "",
            "\tfor (i = 0; i < n; i++) {",
            "\t\tstruct xdp_frame *xdpf = frames[i];",
            "\t\tu32 act;",
            "\t\tint err;",
            "",
            "\t\trxq.dev = xdpf->dev_rx;",
            "\t\trxq.mem = xdpf->mem;",
            "\t\t/* TODO: report queue_index to xdp_rxq_info */",
            "",
            "\t\txdp_convert_frame_to_buff(xdpf, &xdp);",
            "",
            "\t\tact = bpf_prog_run_xdp(rcpu->prog, &xdp);",
            "\t\tswitch (act) {",
            "\t\tcase XDP_PASS:",
            "\t\t\terr = xdp_update_frame_from_buff(&xdp, xdpf);",
            "\t\t\tif (err < 0) {",
            "\t\t\t\txdp_return_frame(xdpf);",
            "\t\t\t\tstats->drop++;",
            "\t\t\t} else {",
            "\t\t\t\tframes[nframes++] = xdpf;",
            "\t\t\t\tstats->pass++;",
            "\t\t\t}",
            "\t\t\tbreak;",
            "\t\tcase XDP_REDIRECT:",
            "\t\t\terr = xdp_do_redirect(xdpf->dev_rx, &xdp,",
            "\t\t\t\t\t      rcpu->prog);",
            "\t\t\tif (unlikely(err)) {",
            "\t\t\t\txdp_return_frame(xdpf);",
            "\t\t\t\tstats->drop++;",
            "\t\t\t} else {",
            "\t\t\t\tstats->redirect++;",
            "\t\t\t}",
            "\t\t\tbreak;",
            "\t\tdefault:",
            "\t\t\tbpf_warn_invalid_xdp_action(NULL, rcpu->prog, act);",
            "\t\t\tfallthrough;",
            "\t\tcase XDP_DROP:",
            "\t\t\txdp_return_frame(xdpf);",
            "\t\t\tstats->drop++;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\txdp_clear_return_frame_no_direct();",
            "",
            "\treturn nframes;",
            "}"
          ],
          "function_name": "__cpu_map_ring_cleanup, cpu_map_bpf_prog_run_skb, cpu_map_bpf_prog_run_xdp",
          "description": "实现XDP程序对数据包的处理逻辑，包括skb处理流程(cpu_map_bpf_prog_run_skb)和xdp_frame处理流程(cpu_map_bpf_prog_run_xdp)，提供XDP_PASS/XDP_REDIRECT等动作的分支处理。",
          "similarity": 0.49182605743408203
        }
      ]
    }
  ]
}