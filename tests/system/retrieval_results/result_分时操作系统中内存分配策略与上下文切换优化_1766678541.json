{
  "query": "分时操作系统中内存分配策略与上下文切换优化",
  "timestamp": "2025-12-26 00:02:21",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.6552720665931702,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/mempolicy.c",
          "start_line": 168,
          "end_line": 268,
          "content": [
            "static u8 get_il_weight(int node)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tu8 weight = 1;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state)",
            "\t\tweight = state->iw_table[node];",
            "\trcu_read_unlock();",
            "\treturn weight;",
            "}",
            "static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)",
            "{",
            "\tu64 sum_bw = 0;",
            "\tunsigned int cast_sum_bw, scaling_factor = 1, iw_gcd = 0;",
            "\tint nid;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tsum_bw += bw[nid];",
            "",
            "\t/* Scale bandwidths to whole numbers in the range [1, weightiness] */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t/*",
            "\t\t * Try not to perform 64-bit division.",
            "\t\t * If sum_bw < scaling_factor, then sum_bw < U32_MAX.",
            "\t\t * If sum_bw > scaling_factor, then round the weight up to 1.",
            "\t\t */",
            "\t\tscaling_factor = weightiness * bw[nid];",
            "\t\tif (bw[nid] && sum_bw < scaling_factor) {",
            "\t\t\tcast_sum_bw = (unsigned int)sum_bw;",
            "\t\t\tnew_iw[nid] = scaling_factor / cast_sum_bw;",
            "\t\t} else {",
            "\t\t\tnew_iw[nid] = 1;",
            "\t\t}",
            "\t\tif (!iw_gcd)",
            "\t\t\tiw_gcd = new_iw[nid];",
            "\t\tiw_gcd = gcd(iw_gcd, new_iw[nid]);",
            "\t}",
            "",
            "\t/* 1:2 is strictly better than 16:32. Reduce by the weights' GCD. */",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tnew_iw[nid] /= iw_gcd;",
            "}",
            "int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tunsigned int *old_bw, *new_bw;",
            "\tunsigned int bw_val;",
            "\tint i;",
            "",
            "\tbw_val = min(coords->read_bandwidth, coords->write_bandwidth);",
            "\tnew_bw = kcalloc(nr_node_ids, sizeof(unsigned int), GFP_KERNEL);",
            "\tif (!new_bw)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew_wi_state = kmalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state) {",
            "\t\tkfree(new_bw);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tnew_wi_state->mode_auto = true;",
            "\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\tnew_wi_state->iw_table[i] = 1;",
            "",
            "\t/*",
            "\t * Update bandwidth info, even in manual mode. That way, when switching",
            "\t * to auto mode in the future, iw_table can be overwritten using",
            "\t * accurate bw data.",
            "\t */",
            "\tmutex_lock(&wi_state_lock);",
            "",
            "\told_bw = node_bw_table;",
            "\tif (old_bw)",
            "\t\tmemcpy(new_bw, old_bw, nr_node_ids * sizeof(*old_bw));",
            "\tnew_bw[node] = bw_val;",
            "\tnode_bw_table = new_bw;",
            "",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state && !old_wi_state->mode_auto) {",
            "\t\t/* Manual mode; skip reducing weights and updating wi_state */",
            "\t\tmutex_unlock(&wi_state_lock);",
            "\t\tkfree(new_wi_state);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* NULL wi_state assumes auto=true; reduce weights and update wi_state*/",
            "\treduce_interleave_weights(new_bw, new_wi_state->iw_table);",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "out:",
            "\tkfree(old_bw);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_il_weight, reduce_interleave_weights, mempolicy_set_node_perf",
          "description": "实现带权交错策略的权重计算与调整逻辑，通过获取节点带宽数据动态修改权重比例，支持根据性能参数更新节点间内存分配优先级。",
          "similarity": 0.6823499202728271
        },
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.6658902764320374
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.6635748147964478
        },
        {
          "chunk_id": 13,
          "file_path": "mm/mempolicy.c",
          "start_line": 2149,
          "end_line": 2255,
          "content": [
            "static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nnodes;",
            "\tint i;",
            "\tint nid;",
            "",
            "\tnnodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nnodes)",
            "\t\treturn numa_node_id();",
            "\ttarget = ilx % nnodes;",
            "\tnid = first_node(nodemask);",
            "\tfor (i = 0; i < target; i++)",
            "\t\tnid = next_node(nid, nodemask);",
            "\treturn nid;",
            "}",
            "int huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,",
            "\t\tstruct mempolicy **mpol, nodemask_t **nodemask)",
            "{",
            "\tpgoff_t ilx;",
            "\tint nid;",
            "",
            "\tnid = numa_node_id();",
            "\t*mpol = get_vma_policy(vma, addr, hstate_vma(vma)->order, &ilx);",
            "\t*nodemask = policy_nodemask(gfp_flags, *mpol, ilx, &nid);",
            "\treturn nid;",
            "}",
            "bool init_nodemask_of_mempolicy(nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "",
            "\tif (!(mask && current->mempolicy))",
            "\t\treturn false;",
            "",
            "\ttask_lock(current);",
            "\tmempolicy = current->mempolicy;",
            "\tswitch (mempolicy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*mask = mempolicy->nodes;",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tinit_nodemask_of_node(mask, numa_node_id());",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "\ttask_unlock(current);",
            "",
            "\treturn true;",
            "}",
            "bool mempolicy_in_oom_domain(struct task_struct *tsk,",
            "\t\t\t\t\tconst nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "\tbool ret = true;",
            "",
            "\tif (!mask)",
            "\t\treturn ret;",
            "",
            "\ttask_lock(tsk);",
            "\tmempolicy = tsk->mempolicy;",
            "\tif (mempolicy && mempolicy->mode == MPOL_BIND)",
            "\t\tret = nodes_intersects(mempolicy->nodes, *mask);",
            "\ttask_unlock(tsk);",
            "",
            "\treturn ret;",
            "}",
            "static unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tint nodes;",
            "\tunsigned long nr_pages_per_node;",
            "\tint delta;",
            "\tint i;",
            "\tunsigned long nr_allocated;",
            "\tunsigned long total_allocated = 0;",
            "",
            "\tnodes = nodes_weight(pol->nodes);",
            "\tnr_pages_per_node = nr_pages / nodes;",
            "\tdelta = nr_pages - nodes * nr_pages_per_node;",
            "",
            "\tfor (i = 0; i < nodes; i++) {",
            "\t\tif (delta) {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node + 1, NULL,",
            "\t\t\t\t\tpage_array);",
            "\t\t\tdelta--;",
            "\t\t} else {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node, NULL, page_array);",
            "\t\t}",
            "",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t}",
            "",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "interleave_nid, huge_node, init_nodemask_of_mempolicy, mempolicy_in_oom_domain, alloc_pages_bulk_array_interleave",
          "description": "interleave_nid 计算简单交错分配的目标节点；huge_node 结合HugeTLB策略确定大页分配节点；init_nodemask_of_mempolicy 初始化当前进程的内存策略节点掩码；mempolicy_in_oom_domain 检查策略节点是否与OOM域重叠；alloc_pages_bulk_array_interleave 执行批量交错分配。",
          "similarity": 0.656567394733429
        },
        {
          "chunk_id": 12,
          "file_path": "mm/mempolicy.c",
          "start_line": 2024,
          "end_line": 2135,
          "content": [
            "static unsigned int interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int nid;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "\t/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnid = next_node_in(current->il_prev, policy->nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\tif (nid < MAX_NUMNODES)",
            "\t\tcurrent->il_prev = nid;",
            "\treturn nid;",
            "}",
            "unsigned int mempolicy_slab_node(void)",
            "{",
            "\tstruct mempolicy *policy;",
            "\tint node = numa_mem_id();",
            "",
            "\tif (!in_task())",
            "\t\treturn node;",
            "",
            "\tpolicy = current->mempolicy;",
            "\tif (!policy)",
            "\t\treturn node;",
            "",
            "\tswitch (policy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\treturn first_node(policy->nodes);",
            "",
            "\tcase MPOL_INTERLEAVE:",
            "\t\treturn interleave_nodes(policy);",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn weighted_interleave_nodes(policy);",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t{",
            "\t\tstruct zoneref *z;",
            "",
            "\t\t/*",
            "\t\t * Follow bind policy behavior and start allocation at the",
            "\t\t * first node.",
            "\t\t */",
            "\t\tstruct zonelist *zonelist;",
            "\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);",
            "\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];",
            "\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,",
            "\t\t\t\t\t\t\t&policy->nodes);",
            "\t\treturn z->zone ? zone_to_nid(z->zone) : node;",
            "\t}",
            "\tcase MPOL_LOCAL:",
            "\t\treturn node;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static unsigned int read_once_policy_nodemask(struct mempolicy *pol,",
            "\t\t\t\t\t      nodemask_t *mask)",
            "{",
            "\t/*",
            "\t * barrier stabilizes the nodemask locally so that it can be iterated",
            "\t * over safely without concern for changes. Allocators validate node",
            "\t * selection does not violate mems_allowed, so this is safe.",
            "\t */",
            "\tbarrier();",
            "\tmemcpy(mask, &pol->nodes, sizeof(nodemask_t));",
            "\tbarrier();",
            "\treturn nodes_weight(*mask);",
            "}",
            "static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nr_nodes;",
            "\tu8 *table = NULL;",
            "\tunsigned int weight_total = 0;",
            "\tu8 weight;",
            "\tint nid = 0;",
            "",
            "\tnr_nodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nr_nodes)",
            "\t\treturn numa_node_id();",
            "",
            "\trcu_read_lock();",
            "",
            "\tstate = rcu_dereference(wi_state);",
            "\t/* Uninitialized wi_state means we should assume all weights are 1 */",
            "\tif (state)",
            "\t\ttable = state->iw_table;",
            "",
            "\t/* calculate the total weight */",
            "\tfor_each_node_mask(nid, nodemask)",
            "\t\tweight_total += table ? table[nid] : 1;",
            "",
            "\t/* Calculate the node offset based on totals */",
            "\ttarget = ilx % weight_total;",
            "\tnid = first_node(nodemask);",
            "\twhile (target) {",
            "\t\t/* detect system default usage */",
            "\t\tweight = table ? table[nid] : 1;",
            "\t\tif (target < weight)",
            "\t\t\tbreak;",
            "\t\ttarget -= weight;",
            "\t\tnid = next_node_in(nid, nodemask);",
            "\t}",
            "\trcu_read_unlock();",
            "\treturn nid;",
            "}"
          ],
          "function_name": "interleave_nodes, mempolicy_slab_node, read_once_policy_nodemask, weighted_interleave_nid",
          "description": "interleave_nodes 计算交错分配的下一个节点；mempolicy_slab_node 根据内存策略返回Slab分配的节点；read_once_policy_nodemask 安全读取策略节点掩码；weighted_interleave_nid 基于权重计算加权交错分配的目标节点。",
          "similarity": 0.6522347927093506
        }
      ]
    },
    {
      "source_file": "mm/page_alloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_alloc.c`\n\n---\n\n# page_alloc.c 技术文档\n\n## 1. 文件概述\n\n`page_alloc.c` 是 Linux 内核内存管理子系统的核心文件之一，负责物理页面的分配与释放。该文件实现了基于区域（zone）和迁移类型（migratetype）的伙伴系统（Buddy System）内存分配器，管理系统的空闲页链表，并提供高效的页面分配/回收机制。它不处理小对象分配（由 slab/slub/slob 子系统负责），而是专注于以页为单位的大块物理内存管理。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct per_cpu_pages`**：每个 CPU 的每区（per-zone）页面缓存，用于减少锁竞争，提升分配性能。\n- **`node_states[NR_NODE_STATES]`**：全局节点状态掩码数组，跟踪各 NUMA 节点的状态（如在线、有内存等）。\n- **`sysctl_lowmem_reserve_ratio[MAX_NR_ZONES]`**：各内存区域的低内存保留比例，防止高优先级区域耗尽低优先级区域的内存。\n- **`zone_names[]` 和 `migratetype_names[]`**：内存区域和页面迁移类型的名称字符串，用于调试和日志。\n- **`gfp_allowed_mask`**：全局 GFP（Get Free Page）标志掩码，控制启动早期可使用的分配标志。\n\n### 主要函数（部分声明）\n- **`__free_pages_ok()`**：内部页面释放函数，执行实际的伙伴系统合并与链表插入逻辑。\n- 各种页面分配函数（如 `alloc_pages()`、`__alloc_pages()` 等，定义在其他位置但在此文件中实现核心逻辑）。\n- 每 CPU 页面列表操作辅助宏（如 `pcp_spin_lock()`、`pcp_spin_trylock()`）。\n\n### 关键常量与标志\n- **`fpi_t` 类型及标志**：\n  - `FPI_NONE`：无特殊要求。\n  - `FPI_SKIP_REPORT_NOTIFY`：跳过空闲页报告通知。\n  - `FPI_TO_TAIL`：将页面放回空闲链表尾部（用于优化场景如内存热插拔）。\n- **`min_free_kbytes`**：系统保留的最小空闲内存（KB），影响水位线计算。\n\n## 3. 关键实现\n\n### 每 CPU 页面缓存（Per-CPU Page Caching）\n- 通过 `struct per_cpu_pages` 为每个 CPU 维护热/冷页列表，避免频繁访问全局 zone 锁。\n- 使用 `pcpu_spin_lock` 宏族安全地访问每 CPU 数据，结合 `preempt_disable()`（非 RT）或 `migrate_disable()`（RT）防止任务迁移导致访问错误 CPU 的数据。\n- 在 UP 系统上，使用 IRQ 关闭防止重入；在 SMP/RT 系统上依赖自旋锁语义。\n\n### 内存区域（Zone）与 NUMA 支持\n- 支持多种内存区域（DMA、DMA32、Normal、HighMem、Movable、Device），通过 `zone_names` 标识。\n- 实现 `lowmem_reserve_ratio` 机制，确保高区域分配不会耗尽低区域的保留内存（如 ZONE_DMA 为设备保留）。\n- 通过 `node_states` 和 per-CPU 变量（如 `numa_node`、`_numa_mem_`）支持 NUMA 和无内存节点架构。\n\n### 空闲页管理优化\n- **`FPI_TO_TAIL` 标志**：允许将页面放回空闲链表尾部，配合内存打乱（shuffle）或热插拔时批量初始化。\n- **`FPI_SKIP_REPORT_NOTIFY` 标志**：在临时取出并归还页面时不触发空闲页报告机制，减少开销。\n- **水位线与保留内存**：`min_free_kbytes` 控制最低水位，影响 OOM（Out-Of-Memory）决策和内存回收行为。\n\n### 实时内核（PREEMPT_RT）适配\n- 在 RT 内核中使用 `migrate_disable()` 替代 `preempt_disable()`，避免干扰 RT 自旋锁的优先级继承机制。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心内存管理**：`<linux/mm.h>`, `<linux/highmem.h>`, `\"internal.h\"`\n- **同步机制**：`<linux/spinlock.h>`（隐含）、`<linux/mutex.h>`\n- **NUMA 与拓扑**：`<linux/topology.h>`, `<linux/nodemask.h>`\n- **调试与追踪**：`<linux/kasan.h>`, `<trace/events/kmem.h>`, `<linux/page_owner.h>`\n- **高级特性**：`<linux/compaction.h>`, `<linux/migrate.h>`, `<linux/memcontrol.h>`\n\n### 子系统交互\n- **Slab 分配器**：本文件不处理 kmalloc，由 `slab.c` 等负责。\n- **内存回收**：与 `vmscan.c` 协同，通过水位线触发 reclaim。\n- **内存热插拔**：通过 `memory_hotplug.h` 接口管理动态内存。\n- **OOM Killer**：通过 `oom.h` 和水位线机制触发 OOM。\n- **透明大页（THP）**：与 `khugepaged` 协同进行大页分配。\n\n## 5. 使用场景\n\n- **内核内存分配**：所有以页为单位的内核内存请求（如 `alloc_pages()`）最终由本文件处理。\n- **用户空间缺页处理**：匿名页、文件页的物理页分配。\n- **内存映射（mmap）**：大块物理内存的分配与管理。\n- **内存回收与迁移**：页面回收、压缩（compaction）、迁移（migration）过程中涉及的页面释放与重新分配。\n- **系统启动与热插拔**：初始化内存区域、处理动态添加/移除内存。\n- **实时系统**：在 PREEMPT_RT 内核中提供低延迟的页面分配路径。\n- **调试与监控**：通过 page owner、KASAN、tracepoint 等机制提供内存使用追踪。",
      "similarity": 0.6338784694671631,
      "chunks": [
        {
          "chunk_id": 31,
          "file_path": "mm/page_alloc.c",
          "start_line": 5966,
          "end_line": 6071,
          "content": [
            "static void setup_per_zone_lowmem_reserve(void)",
            "{",
            "\tstruct pglist_data *pgdat;",
            "\tenum zone_type i, j;",
            "",
            "\tfor_each_online_pgdat(pgdat) {",
            "\t\tfor (i = 0; i < MAX_NR_ZONES - 1; i++) {",
            "\t\t\tstruct zone *zone = &pgdat->node_zones[i];",
            "\t\t\tint ratio = sysctl_lowmem_reserve_ratio[i];",
            "\t\t\tbool clear = !ratio || !zone_managed_pages(zone);",
            "\t\t\tunsigned long managed_pages = 0;",
            "",
            "\t\t\tfor (j = i + 1; j < MAX_NR_ZONES; j++) {",
            "\t\t\t\tstruct zone *upper_zone = &pgdat->node_zones[j];",
            "",
            "\t\t\t\tmanaged_pages += zone_managed_pages(upper_zone);",
            "",
            "\t\t\t\tif (clear)",
            "\t\t\t\t\tzone->lowmem_reserve[j] = 0;",
            "\t\t\t\telse",
            "\t\t\t\t\tzone->lowmem_reserve[j] = managed_pages / ratio;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\t/* update totalreserve_pages */",
            "\tcalculate_totalreserve_pages();",
            "}",
            "static void __setup_per_zone_wmarks(void)",
            "{",
            "\tunsigned long pages_min = min_free_kbytes >> (PAGE_SHIFT - 10);",
            "\tunsigned long lowmem_pages = 0;",
            "\tstruct zone *zone;",
            "\tunsigned long flags;",
            "",
            "\t/* Calculate total number of !ZONE_HIGHMEM and !ZONE_MOVABLE pages */",
            "\tfor_each_zone(zone) {",
            "\t\tif (!is_highmem(zone) && zone_idx(zone) != ZONE_MOVABLE)",
            "\t\t\tlowmem_pages += zone_managed_pages(zone);",
            "\t}",
            "",
            "\tfor_each_zone(zone) {",
            "\t\tu64 tmp;",
            "",
            "\t\tspin_lock_irqsave(&zone->lock, flags);",
            "\t\ttmp = (u64)pages_min * zone_managed_pages(zone);",
            "\t\tdo_div(tmp, lowmem_pages);",
            "\t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {",
            "\t\t\t/*",
            "\t\t\t * __GFP_HIGH and PF_MEMALLOC allocations usually don't",
            "\t\t\t * need highmem and movable zones pages, so cap pages_min",
            "\t\t\t * to a small  value here.",
            "\t\t\t *",
            "\t\t\t * The WMARK_HIGH-WMARK_LOW and (WMARK_LOW-WMARK_MIN)",
            "\t\t\t * deltas control async page reclaim, and so should",
            "\t\t\t * not be capped for highmem and movable zones.",
            "\t\t\t */",
            "\t\t\tunsigned long min_pages;",
            "",
            "\t\t\tmin_pages = zone_managed_pages(zone) / 1024;",
            "\t\t\tmin_pages = clamp(min_pages, SWAP_CLUSTER_MAX, 128UL);",
            "\t\t\tzone->_watermark[WMARK_MIN] = min_pages;",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * If it's a lowmem zone, reserve a number of pages",
            "\t\t\t * proportionate to the zone's size.",
            "\t\t\t */",
            "\t\t\tzone->_watermark[WMARK_MIN] = tmp;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Set the kswapd watermarks distance according to the",
            "\t\t * scale factor in proportion to available memory, but",
            "\t\t * ensure a minimum size on small systems.",
            "\t\t */",
            "\t\ttmp = max_t(u64, tmp >> 2,",
            "\t\t\t    mult_frac(zone_managed_pages(zone),",
            "\t\t\t\t      watermark_scale_factor, 10000));",
            "",
            "\t\tzone->watermark_boost = 0;",
            "\t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;",
            "\t\tzone->_watermark[WMARK_HIGH] = low_wmark_pages(zone) + tmp;",
            "\t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;",
            "",
            "\t\tspin_unlock_irqrestore(&zone->lock, flags);",
            "\t}",
            "",
            "\t/* update totalreserve_pages */",
            "\tcalculate_totalreserve_pages();",
            "}",
            "void setup_per_zone_wmarks(void)",
            "{",
            "\tstruct zone *zone;",
            "\tstatic DEFINE_SPINLOCK(lock);",
            "",
            "\tspin_lock(&lock);",
            "\t__setup_per_zone_wmarks();",
            "\tspin_unlock(&lock);",
            "",
            "\t/*",
            "\t * The watermark size have changed so update the pcpu batch",
            "\t * and high limits or the limits may be inappropriate.",
            "\t */",
            "\tfor_each_zone(zone)",
            "\t\tzone_pcp_update(zone, 0);",
            "}"
          ],
          "function_name": "setup_per_zone_lowmem_reserve, __setup_per_zone_wmarks, setup_per_zone_wmarks",
          "description": "设置各内存区域的低内存保留值和水印标记，根据系统配置调整内存管理参数，确保内存分配策略适应不同场景需求。",
          "similarity": 0.6943115592002869
        },
        {
          "chunk_id": 23,
          "file_path": "mm/page_alloc.c",
          "start_line": 4471,
          "end_line": 4660,
          "content": [
            "static inline bool prepare_alloc_pages(gfp_t gfp_mask, unsigned int order,",
            "\t\tint preferred_nid, nodemask_t *nodemask,",
            "\t\tstruct alloc_context *ac, gfp_t *alloc_gfp,",
            "\t\tunsigned int *alloc_flags)",
            "{",
            "\tac->highest_zoneidx = gfp_zone(gfp_mask);",
            "\tac->zonelist = node_zonelist(preferred_nid, gfp_mask);",
            "\tac->nodemask = nodemask;",
            "\tac->migratetype = gfp_migratetype(gfp_mask);",
            "",
            "\tif (cpusets_enabled()) {",
            "\t\t*alloc_gfp |= __GFP_HARDWALL;",
            "\t\t/*",
            "\t\t * When we are in the interrupt context, it is irrelevant",
            "\t\t * to the current task context. It means that any node ok.",
            "\t\t */",
            "\t\tif (in_task() && !ac->nodemask)",
            "\t\t\tac->nodemask = &cpuset_current_mems_allowed;",
            "\t\telse",
            "\t\t\t*alloc_flags |= ALLOC_CPUSET;",
            "\t}",
            "",
            "\tmight_alloc(gfp_mask);",
            "",
            "\tif (should_fail_alloc_page(gfp_mask, order))",
            "\t\treturn false;",
            "",
            "\t*alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, *alloc_flags);",
            "",
            "\t/* Dirty zone balancing only done in the fast path */",
            "\tac->spread_dirty_pages = (gfp_mask & __GFP_WRITE);",
            "",
            "\t/*",
            "\t * The preferred zone is used for statistics but crucially it is",
            "\t * also used as the starting point for the zonelist iterator. It",
            "\t * may get reset for allocations that ignore memory policies.",
            "\t */",
            "\tac->preferred_zoneref = first_zones_zonelist(ac->zonelist,",
            "\t\t\t\t\tac->highest_zoneidx, ac->nodemask);",
            "",
            "\treturn true;",
            "}",
            "unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,",
            "\t\t\tnodemask_t *nodemask, int nr_pages,",
            "\t\t\tstruct list_head *page_list,",
            "\t\t\tstruct page **page_array)",
            "{",
            "\tstruct page *page;",
            "\tunsigned long __maybe_unused UP_flags;",
            "\tstruct zone *zone;",
            "\tstruct zoneref *z;",
            "\tstruct per_cpu_pages *pcp;",
            "\tstruct list_head *pcp_list;",
            "\tstruct alloc_context ac;",
            "\tgfp_t alloc_gfp;",
            "\tunsigned int alloc_flags = ALLOC_WMARK_LOW;",
            "\tint nr_populated = 0, nr_account = 0;",
            "",
            "\t/*",
            "\t * Skip populated array elements to determine if any pages need",
            "\t * to be allocated before disabling IRQs.",
            "\t */",
            "\twhile (page_array && nr_populated < nr_pages && page_array[nr_populated])",
            "\t\tnr_populated++;",
            "",
            "\t/* No pages requested? */",
            "\tif (unlikely(nr_pages <= 0))",
            "\t\tgoto out;",
            "",
            "\t/* Already populated array? */",
            "\tif (unlikely(page_array && nr_pages - nr_populated == 0))",
            "\t\tgoto out;",
            "",
            "\t/* Bulk allocator does not support memcg accounting. */",
            "\tif (memcg_kmem_online() && (gfp & __GFP_ACCOUNT))",
            "\t\tgoto failed;",
            "",
            "\t/* Use the single page allocator for one page. */",
            "\tif (nr_pages - nr_populated == 1)",
            "\t\tgoto failed;",
            "",
            "#ifdef CONFIG_PAGE_OWNER",
            "\t/*",
            "\t * PAGE_OWNER may recurse into the allocator to allocate space to",
            "\t * save the stack with pagesets.lock held. Releasing/reacquiring",
            "\t * removes much of the performance benefit of bulk allocation so",
            "\t * force the caller to allocate one page at a time as it'll have",
            "\t * similar performance to added complexity to the bulk allocator.",
            "\t */",
            "\tif (static_branch_unlikely(&page_owner_inited))",
            "\t\tgoto failed;",
            "#endif",
            "",
            "\t/* May set ALLOC_NOFRAGMENT, fragmentation will return 1 page. */",
            "\tgfp &= gfp_allowed_mask;",
            "\talloc_gfp = gfp;",
            "\tif (!prepare_alloc_pages(gfp, 0, preferred_nid, nodemask, &ac, &alloc_gfp, &alloc_flags))",
            "\t\tgoto out;",
            "\tgfp = alloc_gfp;",
            "",
            "\t/* Find an allowed local zone that meets the low watermark. */",
            "\tz = ac.preferred_zoneref;",
            "\tfor_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {",
            "\t\tunsigned long mark;",
            "",
            "\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&",
            "\t\t    !__cpuset_zone_allowed(zone, gfp)) {",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tif (nr_online_nodes > 1 && zone != ac.preferred_zoneref->zone &&",
            "\t\t    zone_to_nid(zone) != zone_to_nid(ac.preferred_zoneref->zone)) {",
            "\t\t\tgoto failed;",
            "\t\t}",
            "",
            "\t\tmark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK) + nr_pages;",
            "\t\tif (zone_watermark_fast(zone, 0,  mark,",
            "\t\t\t\tzonelist_zone_idx(ac.preferred_zoneref),",
            "\t\t\t\talloc_flags, gfp)) {",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * If there are no allowed local zones that meets the watermarks then",
            "\t * try to allocate a single page and reclaim if necessary.",
            "\t */",
            "\tif (unlikely(!zone))",
            "\t\tgoto failed;",
            "",
            "\t/* spin_trylock may fail due to a parallel drain or IRQ reentrancy. */",
            "\tpcp_trylock_prepare(UP_flags);",
            "\tpcp = pcp_spin_trylock(zone->per_cpu_pageset);",
            "\tif (!pcp)",
            "\t\tgoto failed_irq;",
            "",
            "\t/* Attempt the batch allocation */",
            "\tpcp_list = &pcp->lists[order_to_pindex(ac.migratetype, 0)];",
            "\twhile (nr_populated < nr_pages) {",
            "",
            "\t\t/* Skip existing pages */",
            "\t\tif (page_array && page_array[nr_populated]) {",
            "\t\t\tnr_populated++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tpage = __rmqueue_pcplist(zone, 0, ac.migratetype, alloc_flags,",
            "\t\t\t\t\t\t\t\tpcp, pcp_list);",
            "\t\tif (unlikely(!page)) {",
            "\t\t\t/* Try and allocate at least one page */",
            "\t\t\tif (!nr_account) {",
            "\t\t\t\tpcp_spin_unlock(pcp);",
            "\t\t\t\tgoto failed_irq;",
            "\t\t\t}",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tnr_account++;",
            "",
            "\t\tprep_new_page(page, 0, gfp, 0);",
            "\t\tif (page_list)",
            "\t\t\tlist_add(&page->lru, page_list);",
            "\t\telse",
            "\t\t\tpage_array[nr_populated] = page;",
            "\t\tnr_populated++;",
            "\t}",
            "",
            "\tpcp_spin_unlock(pcp);",
            "\tpcp_trylock_finish(UP_flags);",
            "",
            "\t__count_zid_vm_events(PGALLOC, zone_idx(zone), nr_account);",
            "\tzone_statistics(ac.preferred_zoneref->zone, zone, nr_account);",
            "",
            "out:",
            "\treturn nr_populated;",
            "",
            "failed_irq:",
            "\tpcp_trylock_finish(UP_flags);",
            "",
            "failed:",
            "\tpage = __alloc_pages_noprof(gfp, 0, preferred_nid, nodemask);",
            "\tif (page) {",
            "\t\tif (page_list)",
            "\t\t\tlist_add(&page->lru, page_list);",
            "\t\telse",
            "\t\t\tpage_array[nr_populated] = page;",
            "\t\tnr_populated++;",
            "\t}",
            "",
            "\tgoto out;",
            "}"
          ],
          "function_name": "prepare_alloc_pages, alloc_pages_bulk_noprof",
          "description": "该代码段实现了内存页面的批量分配逻辑，其中`prepare_alloc_pages`用于初始化分配上下文参数并配置内存策略，`alloc_pages_bulk_noprof`则通过遍历内存区域尝试批量分配连续页面，优先使用本地节点且支持CPU集约束。  \n`prepare_alloc_pages`构建分配上下文，设置Zone列表、迁移类型及节点掩码，处理CPU集隔离与水位线检查；`alloc_pages_bulk_noprof`在满足水位线前提下批量分配页面，失败时回退至单页分配。  \n上下文完整，未引入未展示的API或机制。",
          "similarity": 0.6773930788040161
        },
        {
          "chunk_id": 34,
          "file_path": "mm/page_alloc.c",
          "start_line": 6473,
          "end_line": 6610,
          "content": [
            "static void split_free_pages(struct list_head *list)",
            "{",
            "\tint order;",
            "",
            "\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {",
            "\t\tstruct page *page, *next;",
            "\t\tint nr_pages = 1 << order;",
            "",
            "\t\tlist_for_each_entry_safe(page, next, &list[order], lru) {",
            "\t\t\tint i;",
            "",
            "\t\t\tpost_alloc_hook(page, order, __GFP_MOVABLE);",
            "\t\t\tif (!order)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tsplit_page(page, order);",
            "",
            "\t\t\t/* Add all subpages to the order-0 head, in sequence. */",
            "\t\t\tlist_del(&page->lru);",
            "\t\t\tfor (i = 0; i < nr_pages; i++)",
            "\t\t\t\tlist_add_tail(&page[i].lru, &list[0]);",
            "\t\t}",
            "\t}",
            "}",
            "int alloc_contig_range_noprof(unsigned long start, unsigned long end,",
            "\t\t       unsigned migratetype, gfp_t gfp_mask)",
            "{",
            "\tunsigned long outer_start, outer_end;",
            "\tint ret = 0;",
            "",
            "\tstruct compact_control cc = {",
            "\t\t.nr_migratepages = 0,",
            "\t\t.order = -1,",
            "\t\t.zone = page_zone(pfn_to_page(start)),",
            "\t\t.mode = MIGRATE_SYNC,",
            "\t\t.ignore_skip_hint = true,",
            "\t\t.no_set_skip_hint = true,",
            "\t\t.gfp_mask = current_gfp_context(gfp_mask),",
            "\t\t.alloc_contig = true,",
            "\t};",
            "\tINIT_LIST_HEAD(&cc.migratepages);",
            "",
            "\t/*",
            "\t * What we do here is we mark all pageblocks in range as",
            "\t * MIGRATE_ISOLATE.  Because pageblock and max order pages may",
            "\t * have different sizes, and due to the way page allocator",
            "\t * work, start_isolate_page_range() has special handlings for this.",
            "\t *",
            "\t * Once the pageblocks are marked as MIGRATE_ISOLATE, we",
            "\t * migrate the pages from an unaligned range (ie. pages that",
            "\t * we are interested in). This will put all the pages in",
            "\t * range back to page allocator as MIGRATE_ISOLATE.",
            "\t *",
            "\t * When this is done, we take the pages in range from page",
            "\t * allocator removing them from the buddy system.  This way",
            "\t * page allocator will never consider using them.",
            "\t *",
            "\t * This lets us mark the pageblocks back as",
            "\t * MIGRATE_CMA/MIGRATE_MOVABLE so that free pages in the",
            "\t * aligned range but not in the unaligned, original range are",
            "\t * put back to page allocator so that buddy can use them.",
            "\t */",
            "",
            "\tret = start_isolate_page_range(start, end, migratetype, 0, gfp_mask);",
            "\tif (ret)",
            "\t\tgoto done;",
            "",
            "\tdrain_all_pages(cc.zone);",
            "",
            "\t/*",
            "\t * In case of -EBUSY, we'd like to know which page causes problem.",
            "\t * So, just fall through. test_pages_isolated() has a tracepoint",
            "\t * which will report the busy page.",
            "\t *",
            "\t * It is possible that busy pages could become available before",
            "\t * the call to test_pages_isolated, and the range will actually be",
            "\t * allocated.  So, if we fall through be sure to clear ret so that",
            "\t * -EBUSY is not accidentally used or returned to caller.",
            "\t */",
            "\tret = __alloc_contig_migrate_range(&cc, start, end, migratetype);",
            "\tif (ret && ret != -EBUSY)",
            "\t\tgoto done;",
            "\tret = 0;",
            "",
            "\t/*",
            "\t * Pages from [start, end) are within a pageblock_nr_pages",
            "\t * aligned blocks that are marked as MIGRATE_ISOLATE.  What's",
            "\t * more, all pages in [start, end) are free in page allocator.",
            "\t * What we are going to do is to allocate all pages from",
            "\t * [start, end) (that is remove them from page allocator).",
            "\t *",
            "\t * The only problem is that pages at the beginning and at the",
            "\t * end of interesting range may be not aligned with pages that",
            "\t * page allocator holds, ie. they can be part of higher order",
            "\t * pages.  Because of this, we reserve the bigger range and",
            "\t * once this is done free the pages we are not interested in.",
            "\t *",
            "\t * We don't have to hold zone->lock here because the pages are",
            "\t * isolated thus they won't get removed from buddy.",
            "\t */",
            "\touter_start = find_large_buddy(start);",
            "",
            "\t/* Make sure the range is really isolated. */",
            "\tif (test_pages_isolated(outer_start, end, 0)) {",
            "\t\tret = -EBUSY;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\t/* Grab isolated pages from freelists. */",
            "\touter_end = isolate_freepages_range(&cc, outer_start, end);",
            "\tif (!outer_end) {",
            "\t\tret = -EBUSY;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\tif (!(gfp_mask & __GFP_COMP)) {",
            "\t\tsplit_free_pages(cc.freepages);",
            "",
            "\t\t/* Free head and tail (if any) */",
            "\t\tif (start != outer_start)",
            "\t\t\tfree_contig_range(outer_start, start - outer_start);",
            "\t\tif (end != outer_end)",
            "\t\t\tfree_contig_range(end, outer_end - end);",
            "\t} else if (start == outer_start && end == outer_end && is_power_of_2(end - start)) {",
            "\t\tstruct page *head = pfn_to_page(start);",
            "\t\tint order = ilog2(end - start);",
            "",
            "\t\tcheck_new_pages(head, order);",
            "\t\tprep_new_page(head, order, gfp_mask, 0);",
            "\t} else {",
            "\t\tret = -EINVAL;",
            "\t\tWARN(true, \"PFN range: requested [%lu, %lu), allocated [%lu, %lu)\\n\",",
            "\t\t     start, end, outer_start, outer_end);",
            "\t}",
            "done:",
            "\tundo_isolate_page_range(start, end, migratetype);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "split_free_pages, alloc_contig_range_noprof",
          "description": "实现连续物理内存范围分配功能，通过隔离页面块、迁移页面和调整区域状态，确保目标范围内内存可被系统使用并处理分配异常情况。",
          "similarity": 0.6526145339012146
        },
        {
          "chunk_id": 21,
          "file_path": "mm/page_alloc.c",
          "start_line": 3958,
          "end_line": 4058,
          "content": [
            "static void wake_all_kswapds(unsigned int order, gfp_t gfp_mask,",
            "\t\t\t     const struct alloc_context *ac)",
            "{",
            "\tstruct zoneref *z;",
            "\tstruct zone *zone;",
            "\tpg_data_t *last_pgdat = NULL;",
            "\tenum zone_type highest_zoneidx = ac->highest_zoneidx;",
            "",
            "\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist, highest_zoneidx,",
            "\t\t\t\t\tac->nodemask) {",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "\t\tif (last_pgdat != zone->zone_pgdat) {",
            "\t\t\twakeup_kswapd(zone, gfp_mask, order, highest_zoneidx);",
            "\t\t\tlast_pgdat = zone->zone_pgdat;",
            "\t\t}",
            "\t}",
            "}",
            "static inline unsigned int",
            "gfp_to_alloc_flags(gfp_t gfp_mask, unsigned int order)",
            "{",
            "\tunsigned int alloc_flags = ALLOC_WMARK_MIN | ALLOC_CPUSET;",
            "",
            "\t/*",
            "\t * __GFP_HIGH is assumed to be the same as ALLOC_MIN_RESERVE",
            "\t * and __GFP_KSWAPD_RECLAIM is assumed to be the same as ALLOC_KSWAPD",
            "\t * to save two branches.",
            "\t */",
            "\tBUILD_BUG_ON(__GFP_HIGH != (__force gfp_t) ALLOC_MIN_RESERVE);",
            "\tBUILD_BUG_ON(__GFP_KSWAPD_RECLAIM != (__force gfp_t) ALLOC_KSWAPD);",
            "",
            "\t/*",
            "\t * The caller may dip into page reserves a bit more if the caller",
            "\t * cannot run direct reclaim, or if the caller has realtime scheduling",
            "\t * policy or is asking for __GFP_HIGH memory.  GFP_ATOMIC requests will",
            "\t * set both ALLOC_NON_BLOCK and ALLOC_MIN_RESERVE(__GFP_HIGH).",
            "\t */",
            "\talloc_flags |= (__force int)",
            "\t\t(gfp_mask & (__GFP_HIGH | __GFP_KSWAPD_RECLAIM));",
            "",
            "\tif (!(gfp_mask & __GFP_DIRECT_RECLAIM)) {",
            "\t\t/*",
            "\t\t * Not worth trying to allocate harder for __GFP_NOMEMALLOC even",
            "\t\t * if it can't schedule.",
            "\t\t */",
            "\t\tif (!(gfp_mask & __GFP_NOMEMALLOC)) {",
            "\t\t\talloc_flags |= ALLOC_NON_BLOCK;",
            "",
            "\t\t\tif (order > 0)",
            "\t\t\t\talloc_flags |= ALLOC_HIGHATOMIC;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Ignore cpuset mems for non-blocking __GFP_HIGH (probably",
            "\t\t * GFP_ATOMIC) rather than fail, see the comment for",
            "\t\t * cpuset_node_allowed().",
            "\t\t */",
            "\t\tif (alloc_flags & ALLOC_MIN_RESERVE)",
            "\t\t\talloc_flags &= ~ALLOC_CPUSET;",
            "\t} else if (unlikely(rt_or_dl_task(current)) && in_task())",
            "\t\talloc_flags |= ALLOC_MIN_RESERVE;",
            "",
            "\talloc_flags = gfp_to_alloc_flags_cma(gfp_mask, alloc_flags);",
            "",
            "\treturn alloc_flags;",
            "}",
            "static bool oom_reserves_allowed(struct task_struct *tsk)",
            "{",
            "\tif (!tsk_is_oom_victim(tsk))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * !MMU doesn't have oom reaper so give access to memory reserves",
            "\t * only to the thread with TIF_MEMDIE set",
            "\t */",
            "\tif (!IS_ENABLED(CONFIG_MMU) && !test_thread_flag(TIF_MEMDIE))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static inline int __gfp_pfmemalloc_flags(gfp_t gfp_mask)",
            "{",
            "\tif (unlikely(gfp_mask & __GFP_NOMEMALLOC))",
            "\t\treturn 0;",
            "\tif (gfp_mask & __GFP_MEMALLOC)",
            "\t\treturn ALLOC_NO_WATERMARKS;",
            "\tif (in_serving_softirq() && (current->flags & PF_MEMALLOC))",
            "\t\treturn ALLOC_NO_WATERMARKS;",
            "\tif (!in_interrupt()) {",
            "\t\tif (current->flags & PF_MEMALLOC)",
            "\t\t\treturn ALLOC_NO_WATERMARKS;",
            "\t\telse if (oom_reserves_allowed(current))",
            "\t\t\treturn ALLOC_OOM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "bool gfp_pfmemalloc_allowed(gfp_t gfp_mask)",
            "{",
            "\treturn !!__gfp_pfmemalloc_flags(gfp_mask);",
            "}"
          ],
          "function_name": "wake_all_kswapds, gfp_to_alloc_flags, oom_reserves_allowed, __gfp_pfmemalloc_flags, gfp_pfmemalloc_allowed",
          "description": "该代码段主要处理内存分配时的策略配置与回收机制。  \n`wake_all_kswapds` 遍历内存区并唤醒对应 kswapd 线程以触发页面回收；`gfp_to_alloc_flags` 根据 GFP 标志计算分配策略标志，控制内存回收行为；`oom_reserves_allowed` 和 `__gfp_pfmemalloc_flags` 共同决定是否允许绕过内存水印限制访问 OOM 预留内存。",
          "similarity": 0.6421350240707397
        },
        {
          "chunk_id": 35,
          "file_path": "mm/page_alloc.c",
          "start_line": 6635,
          "end_line": 6770,
          "content": [
            "static int __alloc_contig_pages(unsigned long start_pfn,",
            "\t\t\t\tunsigned long nr_pages, gfp_t gfp_mask)",
            "{",
            "\tunsigned long end_pfn = start_pfn + nr_pages;",
            "",
            "\treturn alloc_contig_range_noprof(start_pfn, end_pfn, MIGRATE_MOVABLE,",
            "\t\t\t\t   gfp_mask);",
            "}",
            "static bool pfn_range_valid_contig(struct zone *z, unsigned long start_pfn,",
            "\t\t\t\t   unsigned long nr_pages)",
            "{",
            "\tunsigned long i, end_pfn = start_pfn + nr_pages;",
            "\tstruct page *page;",
            "",
            "\tfor (i = start_pfn; i < end_pfn; i++) {",
            "\t\tpage = pfn_to_online_page(i);",
            "\t\tif (!page)",
            "\t\t\treturn false;",
            "",
            "\t\tif (page_zone(page) != z)",
            "\t\t\treturn false;",
            "",
            "\t\tif (PageReserved(page))",
            "\t\t\treturn false;",
            "",
            "\t\tif (PageHuge(page))",
            "\t\t\treturn false;",
            "\t}",
            "\treturn true;",
            "}",
            "static bool zone_spans_last_pfn(const struct zone *zone,",
            "\t\t\t\tunsigned long start_pfn, unsigned long nr_pages)",
            "{",
            "\tunsigned long last_pfn = start_pfn + nr_pages - 1;",
            "",
            "\treturn zone_spans_pfn(zone, last_pfn);",
            "}",
            "void free_contig_range(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\tunsigned long count = 0;",
            "\tstruct folio *folio = pfn_folio(pfn);",
            "",
            "\tif (folio_test_large(folio)) {",
            "\t\tint expected = folio_nr_pages(folio);",
            "",
            "\t\tif (nr_pages == expected)",
            "\t\t\tfolio_put(folio);",
            "\t\telse",
            "\t\t\tWARN(true, \"PFN %lu: nr_pages %lu != expected %d\\n\",",
            "\t\t\t     pfn, nr_pages, expected);",
            "\t\treturn;",
            "\t}",
            "",
            "\tfor (; nr_pages--; pfn++) {",
            "\t\tstruct page *page = pfn_to_page(pfn);",
            "",
            "\t\tcount += page_count(page) != 1;",
            "\t\t__free_page(page);",
            "\t}",
            "\tWARN(count != 0, \"%lu pages are still in use!\\n\", count);",
            "}",
            "void zone_pcp_disable(struct zone *zone)",
            "{",
            "\tmutex_lock(&pcp_batch_high_lock);",
            "\t__zone_set_pageset_high_and_batch(zone, 0, 0, 1);",
            "\t__drain_all_pages(zone, true);",
            "}",
            "void zone_pcp_enable(struct zone *zone)",
            "{",
            "\t__zone_set_pageset_high_and_batch(zone, zone->pageset_high_min,",
            "\t\tzone->pageset_high_max, zone->pageset_batch);",
            "\tmutex_unlock(&pcp_batch_high_lock);",
            "}",
            "void zone_pcp_reset(struct zone *zone)",
            "{",
            "\tint cpu;",
            "\tstruct per_cpu_zonestat *pzstats;",
            "",
            "\tif (zone->per_cpu_pageset != &boot_pageset) {",
            "\t\tfor_each_online_cpu(cpu) {",
            "\t\t\tpzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);",
            "\t\t\tdrain_zonestat(zone, pzstats);",
            "\t\t}",
            "\t\tfree_percpu(zone->per_cpu_pageset);",
            "\t\tzone->per_cpu_pageset = &boot_pageset;",
            "\t\tif (zone->per_cpu_zonestats != &boot_zonestats) {",
            "\t\t\tfree_percpu(zone->per_cpu_zonestats);",
            "\t\t\tzone->per_cpu_zonestats = &boot_zonestats;",
            "\t\t}",
            "\t}",
            "}",
            "unsigned long __offline_isolated_pages(unsigned long start_pfn,",
            "\t\tunsigned long end_pfn)",
            "{",
            "\tunsigned long already_offline = 0, flags;",
            "\tunsigned long pfn = start_pfn;",
            "\tstruct page *page;",
            "\tstruct zone *zone;",
            "\tunsigned int order;",
            "",
            "\toffline_mem_sections(pfn, end_pfn);",
            "\tzone = page_zone(pfn_to_page(pfn));",
            "\tspin_lock_irqsave(&zone->lock, flags);",
            "\twhile (pfn < end_pfn) {",
            "\t\tpage = pfn_to_page(pfn);",
            "\t\t/*",
            "\t\t * The HWPoisoned page may be not in buddy system, and",
            "\t\t * page_count() is not 0.",
            "\t\t */",
            "\t\tif (unlikely(!PageBuddy(page) && PageHWPoison(page))) {",
            "\t\t\tpfn++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\t/*",
            "\t\t * At this point all remaining PageOffline() pages have a",
            "\t\t * reference count of 0 and can simply be skipped.",
            "\t\t */",
            "\t\tif (PageOffline(page)) {",
            "\t\t\tBUG_ON(page_count(page));",
            "\t\t\tBUG_ON(PageBuddy(page));",
            "\t\t\talready_offline++;",
            "\t\t\tpfn++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tBUG_ON(page_count(page));",
            "\t\tBUG_ON(!PageBuddy(page));",
            "\t\tVM_WARN_ON(get_pageblock_migratetype(page) != MIGRATE_ISOLATE);",
            "\t\torder = buddy_order(page);",
            "\t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);",
            "\t\tpfn += (1 << order);",
            "\t}",
            "\tspin_unlock_irqrestore(&zone->lock, flags);",
            "",
            "\treturn end_pfn - start_pfn - already_offline;",
            "}"
          ],
          "function_name": "__alloc_contig_pages, pfn_range_valid_contig, zone_spans_last_pfn, free_contig_range, zone_pcp_disable, zone_pcp_enable, zone_pcp_reset, __offline_isolated_pages",
          "description": "实现连续内存分配与验证逻辑，包括分配连续页框、检查PFN有效性、释放连续范围及处理内存离线隔离操作",
          "similarity": 0.628780722618103
        }
      ]
    },
    {
      "source_file": "mm/sparse.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:25:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse.c`\n\n---\n\n# sparse.c 技术文档\n\n## 1. 文件概述\n\n`sparse.c` 是 Linux 内核中实现 **SPARSEMEM（稀疏内存模型）** 的核心文件，用于管理物理内存的稀疏映射。该模型将整个物理地址空间划分为固定大小的“内存段”（memory sections），仅对实际存在的内存段分配 `mem_map`（页描述符数组），从而在支持大物理地址空间的同时节省内存开销。此文件负责内存段的初始化、节点关联、存在性标记以及与内存热插拔和 vmemmap 相关的功能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`mem_section`**: 全局内存段数组，每个元素代表一个内存段，存储该段的 `mem_map` 指针及其他元数据。\n  - 在 `CONFIG_SPARSEMEM_EXTREME` 下为二级指针（动态分配根数组）\n  - 否则为静态二维数组 `[NR_SECTION_ROOTS][SECTIONS_PER_ROOT]`\n- **`section_to_node_table`**: （仅当 `NODE_NOT_IN_PAGE_FLAGS` 时）用于通过内存段号查找所属 NUMA 节点的查找表。\n- **`__highest_present_section_nr`**: 记录当前系统中编号最大的已存在内存段，用于优化遍历。\n\n### 主要函数\n- **`memory_present()`**: 标记指定 PFN 范围内的内存段为“存在”，并关联到指定 NUMA 节点。\n- **`memblocks_present()`**: 遍历所有 memblock 内存区域，调用 `memory_present()` 标记所有系统内存。\n- **`sparse_index_init()`**: （仅 `CONFIG_SPARSEMEM_EXTREME`）为指定内存段分配其所在的根数组项。\n- **`sparse_encode_mem_map()` / `sparse_decode_mem_map()`**: 编码/解码 `mem_map` 指针，使其能通过段内偏移计算出实际 PFN。\n- **`subsection_map_init()`**: （仅 `CONFIG_SPARSEMEM_VMEMMAP`）初始化子段（subsection）位图，用于更细粒度的内存管理。\n- **`page_to_nid()`**: （仅 `NODE_NOT_IN_PAGE_FLAGS`）通过页结构获取其所属 NUMA 节点。\n- **`mminit_validate_memmodel_limits()`**: 验证传入的 PFN 范围是否超出 SPARSEMEM 模型支持的最大地址。\n\n### 辅助宏与内联函数\n- **`for_each_present_section_nr()`**: 高效遍历所有已存在的内存段。\n- **`first_present_section_nr()`**: 获取第一个存在的内存段编号。\n- **`sparse_encode_early_nid()` / `sparse_early_nid()`**: 在早期启动阶段利用 `section_mem_map` 字段临时存储 NUMA 节点 ID。\n\n## 3. 关键实现\n\n### 内存段管理\n- 物理内存被划分为 `PAGES_PER_SECTION` 大小的段（通常 128MB）。\n- `mem_section` 数组索引即为段号（section number），通过 `__nr_to_section()` 宏访问。\n- 段的存在性通过 `SECTION_MARKED_PRESENT` 位标记，并维护 `__highest_present_section_nr` 以加速遍历。\n\n### NUMA 节点关联\n- 若页结构体（`struct page`）中未直接存储节点 ID（`NODE_NOT_IN_PAGE_FLAGS`），则使用 `section_to_node_table` 查找。\n- 在 `memory_present()` 中通过 `set_section_nid()` 建立段到节点的映射。\n\n### 动态内存段分配（SPARSEMEM_EXTREME）\n- 为减少静态内存占用，`mem_section` 采用二级结构：\n  - 一级：`mem_section[]` 指向多个二级数组\n  - 二级：每个 `mem_section[root]` 指向 `SECTIONS_PER_ROOT` 个 `struct mem_section`\n- `sparse_index_init()` 在需要时动态分配二级数组（使用 `kzalloc_node` 或 `memblock_alloc_node`）。\n\n### 早期启动阶段的优化\n- 在 `mem_map` 分配前，复用 `section_mem_map` 字段的高位存储 NUMA 节点 ID（`sparse_encode_early_nid()`）。\n- 此信息在分配真实 `mem_map` 前被清除。\n\n### vmemmap 子段支持\n- 当启用 `CONFIG_SPARSEMEM_VMEMMAP` 时，每个内存段进一步划分为子段（subsections）。\n- `subsection_map_init()` 初始化位图，标记哪些子段包含有效内存，支持更灵活的内存热插拔。\n\n### 地址空间验证\n- `mminit_validate_memmodel_limits()` 确保传入的 PFN 范围不超过 `PHYSMEM_END`（SPARSEMEM 模型最大支持地址），防止越界。\n\n## 4. 依赖关系\n\n- **头文件依赖**:\n  - `<linux/mm.h>`, `<linux/mmzone.h>`: 内存管理核心定义\n  - `<linux/memblock.h>`: 早期内存分配器\n  - `<linux/vmalloc.h>`: 用于 vmemmap 映射\n  - `<asm/dma.h>`: 架构相关 DMA 定义\n  - `\"internal.h\"`: MM 子系统内部头文件\n- **配置选项依赖**:\n  - `CONFIG_SPARSEMEM`: 基础稀疏内存模型\n  - `CONFIG_SPARSEMEM_EXTREME`: 动态内存段分配\n  - `CONFIG_SPARSEMEM_VMEMMAP`: 使用虚拟映射的 mem_map\n  - `CONFIG_MEMORY_HOTPLUG`: 内存热插拔支持\n  - `NODE_NOT_IN_PAGE_FLAGS`: 页结构体不包含节点 ID\n- **与其他模块交互**:\n  - **Memory Block (memblock)**: 通过 `for_each_mem_pfn_range()` 获取初始内存布局\n  - **Page Allocator**: 提供 `struct page` 数组（mem_map）\n  - **NUMA Subsystem**: 通过节点 ID 关联内存与 CPU 拓扑\n  - **Memory Hotplug**: 依赖本文件提供的段管理接口进行内存增删\n\n## 5. 使用场景\n\n- **系统启动初始化**:\n  - `memblocks_present()` 在 `mm_init()` 阶段被调用，标记所有固件报告的内存区域为“存在”。\n- **内存热插拔**:\n  - 热添加内存时，调用 `memory_present()` 标记新段；热移除时清理对应段。\n  - `sparse_index_init()` 支持动态扩展 `mem_section` 数组。\n- **页到节点转换**:\n  - 当 `page_to_nid()` 被调用时（如页面迁移、NUMA 调度），通过段查找节点。\n- **vmemmap 优化**:\n  - 在支持 `SPARSEMEM_VMEMMAP` 的架构（如 x86_64, ARM64）上，`subsection_map_init()` 使内核能按子段粒度映射 `struct page`，减少虚拟地址空间占用。\n- **调试与验证**:\n  - `mminit_validate_memmodel_limits()` 在开发阶段捕获内存模型配置错误。",
      "similarity": 0.6259607076644897,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/sparse.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "int page_to_nid(const struct page *page)",
            "{",
            "\treturn section_to_node_table[page_to_section(page)];",
            "}",
            "static void set_section_nid(unsigned long section_nr, int nid)",
            "{",
            "\tsection_to_node_table[section_nr] = nid;",
            "}",
            "static inline void set_section_nid(unsigned long section_nr, int nid)",
            "{",
            "}",
            "static int __meminit sparse_index_init(unsigned long section_nr, int nid)",
            "{",
            "\tunsigned long root = SECTION_NR_TO_ROOT(section_nr);",
            "\tstruct mem_section *section;",
            "",
            "\t/*",
            "\t * An existing section is possible in the sub-section hotplug",
            "\t * case. First hot-add instantiates, follow-on hot-add reuses",
            "\t * the existing section.",
            "\t *",
            "\t * The mem_hotplug_lock resolves the apparent race below.",
            "\t */",
            "\tif (mem_section[root])",
            "\t\treturn 0;",
            "",
            "\tsection = sparse_index_alloc(nid);",
            "\tif (!section)",
            "\t\treturn -ENOMEM;",
            "",
            "\tmem_section[root] = section;",
            "",
            "\treturn 0;",
            "}",
            "static inline int sparse_index_init(unsigned long section_nr, int nid)",
            "{",
            "\treturn 0;",
            "}",
            "static inline unsigned long sparse_encode_early_nid(int nid)",
            "{",
            "\treturn ((unsigned long)nid << SECTION_NID_SHIFT);",
            "}",
            "static inline int sparse_early_nid(struct mem_section *section)",
            "{",
            "\treturn (section->section_mem_map >> SECTION_NID_SHIFT);",
            "}",
            "static void __meminit mminit_validate_memmodel_limits(unsigned long *start_pfn,",
            "\t\t\t\t\t\tunsigned long *end_pfn)",
            "{",
            "\tunsigned long max_sparsemem_pfn = (PHYSMEM_END + 1) >> PAGE_SHIFT;",
            "",
            "\t/*",
            "\t * Sanity checks - do not allow an architecture to pass",
            "\t * in larger pfns than the maximum scope of sparsemem:",
            "\t */",
            "\tif (*start_pfn > max_sparsemem_pfn) {",
            "\t\tmminit_dprintk(MMINIT_WARNING, \"pfnvalidation\",",
            "\t\t\t\"Start of range %lu -> %lu exceeds SPARSEMEM max %lu\\n\",",
            "\t\t\t*start_pfn, *end_pfn, max_sparsemem_pfn);",
            "\t\tWARN_ON_ONCE(1);",
            "\t\t*start_pfn = max_sparsemem_pfn;",
            "\t\t*end_pfn = max_sparsemem_pfn;",
            "\t} else if (*end_pfn > max_sparsemem_pfn) {",
            "\t\tmminit_dprintk(MMINIT_WARNING, \"pfnvalidation\",",
            "\t\t\t\"End of range %lu -> %lu exceeds SPARSEMEM max %lu\\n\",",
            "\t\t\t*start_pfn, *end_pfn, max_sparsemem_pfn);",
            "\t\tWARN_ON_ONCE(1);",
            "\t\t*end_pfn = max_sparsemem_pfn;",
            "\t}",
            "}",
            "static void __section_mark_present(struct mem_section *ms,",
            "\t\tunsigned long section_nr)",
            "{",
            "\tif (section_nr > __highest_present_section_nr)",
            "\t\t__highest_present_section_nr = section_nr;",
            "",
            "\tms->section_mem_map |= SECTION_MARKED_PRESENT;",
            "}",
            "static inline unsigned long first_present_section_nr(void)",
            "{",
            "\treturn next_present_section_nr(-1);",
            "}",
            "static void subsection_mask_set(unsigned long *map, unsigned long pfn,",
            "\t\tunsigned long nr_pages)",
            "{",
            "\tint idx = subsection_map_index(pfn);",
            "\tint end = subsection_map_index(pfn + nr_pages - 1);",
            "",
            "\tbitmap_set(map, idx, end - idx + 1);",
            "}",
            "void __init subsection_map_init(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\tint end_sec = pfn_to_section_nr(pfn + nr_pages - 1);",
            "\tunsigned long nr, start_sec = pfn_to_section_nr(pfn);",
            "",
            "\tif (!nr_pages)",
            "\t\treturn;",
            "",
            "\tfor (nr = start_sec; nr <= end_sec; nr++) {",
            "\t\tstruct mem_section *ms;",
            "\t\tunsigned long pfns;",
            "",
            "\t\tpfns = min(nr_pages, PAGES_PER_SECTION",
            "\t\t\t\t- (pfn & ~PAGE_SECTION_MASK));",
            "\t\tms = __nr_to_section(nr);",
            "\t\tsubsection_mask_set(ms->usage->subsection_map, pfn, pfns);",
            "",
            "\t\tpr_debug(\"%s: sec: %lu pfns: %lu set(%d, %d)\\n\", __func__, nr,",
            "\t\t\t\tpfns, subsection_map_index(pfn),",
            "\t\t\t\tsubsection_map_index(pfn + pfns - 1));",
            "",
            "\t\tpfn += pfns;",
            "\t\tnr_pages -= pfns;",
            "\t}",
            "}"
          ],
          "function_name": "page_to_nid, set_section_nid, set_section_nid, sparse_index_init, sparse_index_init, sparse_encode_early_nid, sparse_early_nid, mminit_validate_memmodel_limits, __section_mark_present, first_present_section_nr, subsection_mask_set, subsection_map_init",
          "description": "提供区段与节点号转换接口，包括page_to_nid获取节点号，set_section_nid设置区段节点号。实现sparse_index_init初始化区段结构，包含内存分配和有效性检查。定义sparse_encode_early_nid/nid转换函数，以及内存模型验证和子部分掩码操作函数。",
          "similarity": 0.5912331342697144
        },
        {
          "chunk_id": 3,
          "file_path": "mm/sparse.c",
          "start_line": 410,
          "end_line": 527,
          "content": [
            "static void __init check_usemap_section_nr(int nid,",
            "\t\tstruct mem_section_usage *usage)",
            "{",
            "}",
            "static unsigned long __init section_map_size(void)",
            "{",
            "\treturn ALIGN(sizeof(struct page) * PAGES_PER_SECTION, PMD_SIZE);",
            "}",
            "static unsigned long __init section_map_size(void)",
            "{",
            "\treturn PAGE_ALIGN(sizeof(struct page) * PAGES_PER_SECTION);",
            "}",
            "static inline void __meminit sparse_buffer_free(unsigned long size)",
            "{",
            "\tWARN_ON(!sparsemap_buf || size == 0);",
            "\tmemblock_free(sparsemap_buf, size);",
            "}",
            "static void __init sparse_buffer_init(unsigned long size, int nid)",
            "{",
            "\tphys_addr_t addr = __pa(MAX_DMA_ADDRESS);",
            "\tWARN_ON(sparsemap_buf);\t/* forgot to call sparse_buffer_fini()? */",
            "\t/*",
            "\t * Pre-allocated buffer is mainly used by __populate_section_memmap",
            "\t * and we want it to be properly aligned to the section size - this is",
            "\t * especially the case for VMEMMAP which maps memmap to PMDs",
            "\t */",
            "\tsparsemap_buf = memmap_alloc(size, section_map_size(), addr, nid, true);",
            "\tsparsemap_buf_end = sparsemap_buf + size;",
            "}",
            "static void __init sparse_buffer_fini(void)",
            "{",
            "\tunsigned long size = sparsemap_buf_end - sparsemap_buf;",
            "",
            "\tif (sparsemap_buf && size > 0)",
            "\t\tsparse_buffer_free(size);",
            "\tsparsemap_buf = NULL;",
            "}",
            "void __weak __meminit vmemmap_populate_print_last(void)",
            "{",
            "}",
            "static void __init sparse_init_nid(int nid, unsigned long pnum_begin,",
            "\t\t\t\t   unsigned long pnum_end,",
            "\t\t\t\t   unsigned long map_count)",
            "{",
            "\tstruct mem_section_usage *usage;",
            "\tunsigned long pnum;",
            "\tstruct page *map;",
            "",
            "\tusage = sparse_early_usemaps_alloc_pgdat_section(NODE_DATA(nid),",
            "\t\t\tmem_section_usage_size() * map_count);",
            "\tif (!usage) {",
            "\t\tpr_err(\"%s: node[%d] usemap allocation failed\", __func__, nid);",
            "\t\tgoto failed;",
            "\t}",
            "\tsparse_buffer_init(map_count * section_map_size(), nid);",
            "\tfor_each_present_section_nr(pnum_begin, pnum) {",
            "\t\tunsigned long pfn = section_nr_to_pfn(pnum);",
            "",
            "\t\tif (pnum >= pnum_end)",
            "\t\t\tbreak;",
            "",
            "\t\tmap = __populate_section_memmap(pfn, PAGES_PER_SECTION,",
            "\t\t\t\tnid, NULL, NULL);",
            "\t\tif (!map) {",
            "\t\t\tpr_err(\"%s: node[%d] memory map backing failed. Some memory will not be available.\",",
            "\t\t\t       __func__, nid);",
            "\t\t\tpnum_begin = pnum;",
            "\t\t\tsparse_buffer_fini();",
            "\t\t\tgoto failed;",
            "\t\t}",
            "\t\tcheck_usemap_section_nr(nid, usage);",
            "\t\tsparse_init_one_section(__nr_to_section(pnum), pnum, map, usage,",
            "\t\t\t\tSECTION_IS_EARLY);",
            "\t\tusage = (void *) usage + mem_section_usage_size();",
            "\t}",
            "\tsparse_buffer_fini();",
            "\treturn;",
            "failed:",
            "\t/* We failed to allocate, mark all the following pnums as not present */",
            "\tfor_each_present_section_nr(pnum_begin, pnum) {",
            "\t\tstruct mem_section *ms;",
            "",
            "\t\tif (pnum >= pnum_end)",
            "\t\t\tbreak;",
            "\t\tms = __nr_to_section(pnum);",
            "\t\tms->section_mem_map = 0;",
            "\t}",
            "}",
            "void __init sparse_init(void)",
            "{",
            "\tunsigned long pnum_end, pnum_begin, map_count = 1;",
            "\tint nid_begin;",
            "",
            "\tmemblocks_present();",
            "",
            "\tpnum_begin = first_present_section_nr();",
            "\tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));",
            "",
            "\t/* Setup pageblock_order for HUGETLB_PAGE_SIZE_VARIABLE */",
            "\tset_pageblock_order();",
            "",
            "\tfor_each_present_section_nr(pnum_begin + 1, pnum_end) {",
            "\t\tint nid = sparse_early_nid(__nr_to_section(pnum_end));",
            "",
            "\t\tif (nid == nid_begin) {",
            "\t\t\tmap_count++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\t/* Init node with sections in range [pnum_begin, pnum_end) */",
            "\t\tsparse_init_nid(nid_begin, pnum_begin, pnum_end, map_count);",
            "\t\tnid_begin = nid;",
            "\t\tpnum_begin = pnum_end;",
            "\t\tmap_count = 1;",
            "\t}",
            "\t/* cover the last node */",
            "\tsparse_init_nid(nid_begin, pnum_begin, pnum_end, map_count);",
            "\tvmemmap_populate_print_last();",
            "}"
          ],
          "function_name": "check_usemap_section_nr, section_map_size, section_map_size, sparse_buffer_free, sparse_buffer_init, sparse_buffer_fini, vmemmap_populate_print_last, sparse_init_nid, sparse_init",
          "description": "定义section_map_size计算区段映射大小，管理sparse_buffer缓冲区的分配释放。实现sparse_init_nid初始化节点区段，通过__populate_section_memmap填充内存图。包含错误处理逻辑，失败时清除已分配区段标识。",
          "similarity": 0.5892412662506104
        },
        {
          "chunk_id": 2,
          "file_path": "mm/sparse.c",
          "start_line": 219,
          "end_line": 339,
          "content": [
            "void __init subsection_map_init(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "}",
            "static void __init memory_present(int nid, unsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn;",
            "",
            "#ifdef CONFIG_SPARSEMEM_EXTREME",
            "\tif (unlikely(!mem_section)) {",
            "\t\tunsigned long size, align;",
            "",
            "\t\tsize = sizeof(struct mem_section *) * NR_SECTION_ROOTS;",
            "\t\talign = 1 << (INTERNODE_CACHE_SHIFT);",
            "\t\tmem_section = memblock_alloc(size, align);",
            "\t\tif (!mem_section)",
            "\t\t\tpanic(\"%s: Failed to allocate %lu bytes align=0x%lx\\n\",",
            "\t\t\t      __func__, size, align);",
            "\t}",
            "#endif",
            "",
            "\tstart &= PAGE_SECTION_MASK;",
            "\tmminit_validate_memmodel_limits(&start, &end);",
            "\tfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION) {",
            "\t\tunsigned long section = pfn_to_section_nr(pfn);",
            "\t\tstruct mem_section *ms;",
            "",
            "\t\tsparse_index_init(section, nid);",
            "\t\tset_section_nid(section, nid);",
            "",
            "\t\tms = __nr_to_section(section);",
            "\t\tif (!ms->section_mem_map) {",
            "\t\t\tms->section_mem_map = sparse_encode_early_nid(nid) |",
            "\t\t\t\t\t\t\tSECTION_IS_ONLINE;",
            "\t\t\t__section_mark_present(ms, section);",
            "\t\t}",
            "\t}",
            "}",
            "static void __init memblocks_present(void)",
            "{",
            "\tunsigned long start, end;",
            "\tint i, nid;",
            "",
            "\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, &nid)",
            "\t\tmemory_present(nid, start, end);",
            "}",
            "static unsigned long sparse_encode_mem_map(struct page *mem_map, unsigned long pnum)",
            "{",
            "\tunsigned long coded_mem_map =",
            "\t\t(unsigned long)(mem_map - (section_nr_to_pfn(pnum)));",
            "\tBUILD_BUG_ON(SECTION_MAP_LAST_BIT > PFN_SECTION_SHIFT);",
            "\tBUG_ON(coded_mem_map & ~SECTION_MAP_MASK);",
            "\treturn coded_mem_map;",
            "}",
            "static void __meminit sparse_init_one_section(struct mem_section *ms,",
            "\t\tunsigned long pnum, struct page *mem_map,",
            "\t\tstruct mem_section_usage *usage, unsigned long flags)",
            "{",
            "\tms->section_mem_map &= ~SECTION_MAP_MASK;",
            "\tms->section_mem_map |= sparse_encode_mem_map(mem_map, pnum)",
            "\t\t| SECTION_HAS_MEM_MAP | flags;",
            "\tms->usage = usage;",
            "}",
            "static unsigned long usemap_size(void)",
            "{",
            "\treturn BITS_TO_LONGS(SECTION_BLOCKFLAGS_BITS) * sizeof(unsigned long);",
            "}",
            "size_t mem_section_usage_size(void)",
            "{",
            "\treturn sizeof(struct mem_section_usage) + usemap_size();",
            "}",
            "static inline phys_addr_t pgdat_to_phys(struct pglist_data *pgdat)",
            "{",
            "#ifndef CONFIG_NUMA",
            "\tVM_BUG_ON(pgdat != &contig_page_data);",
            "\treturn __pa_symbol(&contig_page_data);",
            "#else",
            "\treturn __pa(pgdat);",
            "#endif",
            "}",
            "static void __init check_usemap_section_nr(int nid,",
            "\t\tstruct mem_section_usage *usage)",
            "{",
            "\tunsigned long usemap_snr, pgdat_snr;",
            "\tstatic unsigned long old_usemap_snr;",
            "\tstatic unsigned long old_pgdat_snr;",
            "\tstruct pglist_data *pgdat = NODE_DATA(nid);",
            "\tint usemap_nid;",
            "",
            "\t/* First call */",
            "\tif (!old_usemap_snr) {",
            "\t\told_usemap_snr = NR_MEM_SECTIONS;",
            "\t\told_pgdat_snr = NR_MEM_SECTIONS;",
            "\t}",
            "",
            "\tusemap_snr = pfn_to_section_nr(__pa(usage) >> PAGE_SHIFT);",
            "\tpgdat_snr = pfn_to_section_nr(pgdat_to_phys(pgdat) >> PAGE_SHIFT);",
            "\tif (usemap_snr == pgdat_snr)",
            "\t\treturn;",
            "",
            "\tif (old_usemap_snr == usemap_snr && old_pgdat_snr == pgdat_snr)",
            "\t\t/* skip redundant message */",
            "\t\treturn;",
            "",
            "\told_usemap_snr = usemap_snr;",
            "\told_pgdat_snr = pgdat_snr;",
            "",
            "\tusemap_nid = sparse_early_nid(__nr_to_section(usemap_snr));",
            "\tif (usemap_nid != nid) {",
            "\t\tpr_info(\"node %d must be removed before remove section %ld\\n\",",
            "\t\t\tnid, usemap_snr);",
            "\t\treturn;",
            "\t}",
            "\t/*",
            "\t * There is a circular dependency.",
            "\t * Some platforms allow un-removable section because they will just",
            "\t * gather other removable sections for dynamic partitioning.",
            "\t * Just notify un-removable section's number here.",
            "\t */",
            "\tpr_info(\"Section %ld and %ld (node %d) have a circular dependency on usemap and pgdat allocations\\n\",",
            "\t\tusemap_snr, pgdat_snr, nid);",
            "}"
          ],
          "function_name": "subsection_map_init, memory_present, memblocks_present, sparse_encode_mem_map, sparse_init_one_section, usemap_size, mem_section_usage_size, pgdat_to_phys, check_usemap_section_nr",
          "description": "实现memory_present标记内存区段为有效，memblocks_present遍历所有内存块执行此操作。提供sparse_encode_mem_map编码内存图，sparse_init_one_section初始化区段结构体。包含使用图大小计算、PGDAT物理地址转换及使用图与PGDAT的依赖检查功能。",
          "similarity": 0.5767837166786194
        },
        {
          "chunk_id": 4,
          "file_path": "mm/sparse.c",
          "start_line": 592,
          "end_line": 692,
          "content": [
            "void online_mem_sections(unsigned long start_pfn, unsigned long end_pfn)",
            "{",
            "\tunsigned long pfn;",
            "",
            "\tfor (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {",
            "\t\tunsigned long section_nr = pfn_to_section_nr(pfn);",
            "\t\tstruct mem_section *ms;",
            "",
            "\t\t/* onlining code should never touch invalid ranges */",
            "\t\tif (WARN_ON(!valid_section_nr(section_nr)))",
            "\t\t\tcontinue;",
            "",
            "\t\tms = __nr_to_section(section_nr);",
            "\t\tms->section_mem_map |= SECTION_IS_ONLINE;",
            "\t}",
            "}",
            "void offline_mem_sections(unsigned long start_pfn, unsigned long end_pfn)",
            "{",
            "\tunsigned long pfn;",
            "",
            "\tfor (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {",
            "\t\tunsigned long section_nr = pfn_to_section_nr(pfn);",
            "\t\tstruct mem_section *ms;",
            "",
            "\t\t/*",
            "\t\t * TODO this needs some double checking. Offlining code makes",
            "\t\t * sure to check pfn_valid but those checks might be just bogus",
            "\t\t */",
            "\t\tif (WARN_ON(!valid_section_nr(section_nr)))",
            "\t\t\tcontinue;",
            "",
            "\t\tms = __nr_to_section(section_nr);",
            "\t\tms->section_mem_map &= ~SECTION_IS_ONLINE;",
            "\t}",
            "}",
            "static void depopulate_section_memmap(unsigned long pfn, unsigned long nr_pages,",
            "\t\tstruct vmem_altmap *altmap)",
            "{",
            "\tunsigned long start = (unsigned long) pfn_to_page(pfn);",
            "\tunsigned long end = start + nr_pages * sizeof(struct page);",
            "",
            "\tvmemmap_free(start, end, altmap);",
            "}",
            "static void free_map_bootmem(struct page *memmap)",
            "{",
            "\tunsigned long start = (unsigned long)memmap;",
            "\tunsigned long end = (unsigned long)(memmap + PAGES_PER_SECTION);",
            "",
            "\tvmemmap_free(start, end, NULL);",
            "}",
            "static int clear_subsection_map(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\tDECLARE_BITMAP(map, SUBSECTIONS_PER_SECTION) = { 0 };",
            "\tDECLARE_BITMAP(tmp, SUBSECTIONS_PER_SECTION) = { 0 };",
            "\tstruct mem_section *ms = __pfn_to_section(pfn);",
            "\tunsigned long *subsection_map = ms->usage",
            "\t\t? &ms->usage->subsection_map[0] : NULL;",
            "",
            "\tsubsection_mask_set(map, pfn, nr_pages);",
            "\tif (subsection_map)",
            "\t\tbitmap_and(tmp, map, subsection_map, SUBSECTIONS_PER_SECTION);",
            "",
            "\tif (WARN(!subsection_map || !bitmap_equal(tmp, map, SUBSECTIONS_PER_SECTION),",
            "\t\t\t\t\"section already deactivated (%#lx + %ld)\\n\",",
            "\t\t\t\tpfn, nr_pages))",
            "\t\treturn -EINVAL;",
            "",
            "\tbitmap_xor(subsection_map, map, subsection_map, SUBSECTIONS_PER_SECTION);",
            "\treturn 0;",
            "}",
            "static bool is_subsection_map_empty(struct mem_section *ms)",
            "{",
            "\treturn bitmap_empty(&ms->usage->subsection_map[0],",
            "\t\t\t    SUBSECTIONS_PER_SECTION);",
            "}",
            "static int fill_subsection_map(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\tstruct mem_section *ms = __pfn_to_section(pfn);",
            "\tDECLARE_BITMAP(map, SUBSECTIONS_PER_SECTION) = { 0 };",
            "\tunsigned long *subsection_map;",
            "\tint rc = 0;",
            "",
            "\tsubsection_mask_set(map, pfn, nr_pages);",
            "",
            "\tsubsection_map = &ms->usage->subsection_map[0];",
            "",
            "\tif (bitmap_empty(map, SUBSECTIONS_PER_SECTION))",
            "\t\trc = -EINVAL;",
            "\telse if (bitmap_intersects(map, subsection_map, SUBSECTIONS_PER_SECTION))",
            "\t\trc = -EEXIST;",
            "\telse",
            "\t\tbitmap_or(subsection_map, map, subsection_map,",
            "\t\t\t\tSUBSECTIONS_PER_SECTION);",
            "",
            "\treturn rc;",
            "}",
            "static void depopulate_section_memmap(unsigned long pfn, unsigned long nr_pages,",
            "\t\tstruct vmem_altmap *altmap)",
            "{",
            "\tkvfree(pfn_to_page(pfn));",
            "}"
          ],
          "function_name": "online_mem_sections, offline_mem_sections, depopulate_section_memmap, free_map_bootmem, clear_subsection_map, is_subsection_map_empty, fill_subsection_map, depopulate_section_memmap",
          "description": "提供区段在线/离线操作接口，修改区段在线状态标志。实现depopulate_section_memmap释放内存映射，free_map_bootmem释放启动内存。包含子部分掩码操作函数，用于跟踪和验证内存区域的有效性。",
          "similarity": 0.5749920606613159
        },
        {
          "chunk_id": 5,
          "file_path": "mm/sparse.c",
          "start_line": 717,
          "end_line": 839,
          "content": [
            "static void free_map_bootmem(struct page *memmap)",
            "{",
            "\tunsigned long maps_section_nr, removing_section_nr, i;",
            "\tunsigned long magic, nr_pages;",
            "\tstruct page *page = virt_to_page(memmap);",
            "",
            "\tnr_pages = PAGE_ALIGN(PAGES_PER_SECTION * sizeof(struct page))",
            "\t\t>> PAGE_SHIFT;",
            "",
            "\tfor (i = 0; i < nr_pages; i++, page++) {",
            "\t\tmagic = page->index;",
            "",
            "\t\tBUG_ON(magic == NODE_INFO);",
            "",
            "\t\tmaps_section_nr = pfn_to_section_nr(page_to_pfn(page));",
            "\t\tremoving_section_nr = page_private(page);",
            "",
            "\t\t/*",
            "\t\t * When this function is called, the removing section is",
            "\t\t * logical offlined state. This means all pages are isolated",
            "\t\t * from page allocator. If removing section's memmap is placed",
            "\t\t * on the same section, it must not be freed.",
            "\t\t * If it is freed, page allocator may allocate it which will",
            "\t\t * be removed physically soon.",
            "\t\t */",
            "\t\tif (maps_section_nr != removing_section_nr)",
            "\t\t\tput_page_bootmem(page);",
            "\t}",
            "}",
            "static int clear_subsection_map(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\treturn 0;",
            "}",
            "static bool is_subsection_map_empty(struct mem_section *ms)",
            "{",
            "\treturn true;",
            "}",
            "static int fill_subsection_map(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\treturn 0;",
            "}",
            "static void section_deactivate(unsigned long pfn, unsigned long nr_pages,",
            "\t\tstruct vmem_altmap *altmap)",
            "{",
            "\tstruct mem_section *ms = __pfn_to_section(pfn);",
            "\tbool section_is_early = early_section(ms);",
            "\tstruct page *memmap = NULL;",
            "\tbool empty;",
            "",
            "\tif (clear_subsection_map(pfn, nr_pages))",
            "\t\treturn;",
            "",
            "\tempty = is_subsection_map_empty(ms);",
            "\tif (empty) {",
            "\t\tunsigned long section_nr = pfn_to_section_nr(pfn);",
            "",
            "\t\t/*",
            "\t\t * Mark the section invalid so that valid_section()",
            "\t\t * return false. This prevents code from dereferencing",
            "\t\t * ms->usage array.",
            "\t\t */",
            "\t\tms->section_mem_map &= ~SECTION_HAS_MEM_MAP;",
            "",
            "\t\t/*",
            "\t\t * When removing an early section, the usage map is kept (as the",
            "\t\t * usage maps of other sections fall into the same page). It",
            "\t\t * will be re-used when re-adding the section - which is then no",
            "\t\t * longer an early section. If the usage map is PageReserved, it",
            "\t\t * was allocated during boot.",
            "\t\t */",
            "\t\tif (!PageReserved(virt_to_page(ms->usage))) {",
            "\t\t\tkfree_rcu(ms->usage, rcu);",
            "\t\t\tWRITE_ONCE(ms->usage, NULL);",
            "\t\t}",
            "\t\tmemmap = sparse_decode_mem_map(ms->section_mem_map, section_nr);",
            "\t}",
            "",
            "\t/*",
            "\t * The memmap of early sections is always fully populated. See",
            "\t * section_activate() and pfn_valid() .",
            "\t */",
            "\tif (!section_is_early)",
            "\t\tdepopulate_section_memmap(pfn, nr_pages, altmap);",
            "\telse if (memmap)",
            "\t\tfree_map_bootmem(memmap);",
            "",
            "\tif (empty)",
            "\t\tms->section_mem_map = (unsigned long)NULL;",
            "}",
            "int __meminit sparse_add_section(int nid, unsigned long start_pfn,",
            "\t\tunsigned long nr_pages, struct vmem_altmap *altmap,",
            "\t\tstruct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long section_nr = pfn_to_section_nr(start_pfn);",
            "\tstruct mem_section *ms;",
            "\tstruct page *memmap;",
            "\tint ret;",
            "",
            "\tret = sparse_index_init(section_nr, nid);",
            "\tif (ret < 0)",
            "\t\treturn ret;",
            "",
            "\tmemmap = section_activate(nid, start_pfn, nr_pages, altmap, pgmap);",
            "\tif (IS_ERR(memmap))",
            "\t\treturn PTR_ERR(memmap);",
            "",
            "\t/*",
            "\t * Poison uninitialized struct pages in order to catch invalid flags",
            "\t * combinations.",
            "\t */",
            "\tpage_init_poison(memmap, sizeof(struct page) * nr_pages);",
            "",
            "\tms = __nr_to_section(section_nr);",
            "\tset_section_nid(section_nr, nid);",
            "\t__section_mark_present(ms, section_nr);",
            "",
            "\t/* Align memmap to section boundary in the subsection case */",
            "\tif (section_nr_to_pfn(section_nr) != start_pfn)",
            "\t\tmemmap = pfn_to_page(section_nr_to_pfn(section_nr));",
            "\tsparse_init_one_section(ms, section_nr, memmap, ms->usage, 0);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "free_map_bootmem, clear_subsection_map, is_subsection_map_empty, fill_subsection_map, section_deactivate, sparse_add_section",
          "description": "free_map_bootmem 函数遍历指定内存映射区域的 page 结构，跳过与当前移除节（section）相同的物理节号的页面，其余页面通过 put_page_bootmem 释放。clear_subsection_map、is_subsection_map_empty、fill_subsection_map 均为空实现。section_deactivate 处理内存节停用流程，清除子节映射标志，释放早期节的 usage 数据并调用 depopulate_section_memmap 或 free_map_bootmem。sparse_add_section 初始化新内存节，设置节点 ID，标记节为有效，并调整 memmap 地址对齐。",
          "similarity": 0.5499135851860046
        }
      ]
    }
  ]
}