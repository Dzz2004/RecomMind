{
  "query": "现代操作系统的工作集管理策略",
  "timestamp": "2025-12-26 00:06:42",
  "retrieved_files": [
    {
      "source_file": "kernel/workqueue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:53:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workqueue.c`\n\n---\n\n# workqueue.c 技术文档\n\n## 1. 文件概述\n\n`workqueue.c` 是 Linux 内核中实现通用异步执行机制的核心文件，提供基于共享工作线程池（worker pool）的延迟任务调度功能。工作项（work items）在进程上下文中执行，支持 CPU 绑定和非绑定两种模式。每个 CPU 默认拥有两个标准工作池（普通优先级和高优先级），同时支持动态创建非绑定工作池以满足不同工作队列的需求。该机制替代了早期的 taskqueue/keventd 实现，具有更高的可扩展性和资源利用率。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct worker_pool`**  \n  工作线程池结构体，管理一组工作线程（workers），包含：\n  - `lock`：保护池状态的自旋锁\n  - `cpu` / `node`：关联的 CPU 和 NUMA 节点（绑定池）\n  - `worklist`：待处理工作项队列\n  - `idle_list` / `busy_hash`：空闲和忙碌工作线程的管理结构\n  - `nr_workers` / `nr_idle`：工作线程数量统计\n  - `attrs`：工作线程属性（如优先级、CPU 亲和性）\n  - `mayday_timer`：紧急情况下的救援请求定时器\n\n- **`struct pool_workqueue`**  \n  工作队列与工作池之间的关联结构，每个工作队列在每个池中都有一个对应的 `pool_workqueue` 实例，用于：\n  - 管理工作项的入队和执行\n  - 实现 `max_active` 限制（控制并发执行数）\n  - 支持 flush 操作（等待所有工作完成）\n  - 统计性能指标（如启动/完成次数、CPU 时间等）\n\n- **`struct worker`**（定义在 `workqueue_internal.h`）  \n  工作线程的运行时上下文，包含状态标志（如 `WORKER_IDLE`, `WORKER_UNBOUND`）、当前执行的工作项等。\n\n### 关键枚举与常量\n\n- **池/工作线程标志**：\n  - `POOL_DISASSOCIATED`：CPU 离线时池进入非绑定状态\n  - `WORKER_UNBOUND`：工作线程可在任意 CPU 上运行\n  - `WORKER_CPU_INTENSIVE`：标记 CPU 密集型任务，影响并发控制\n\n- **配置参数**：\n  - `NR_STD_WORKER_POOLS = 2`：每 CPU 标准池数量（普通 + 高优先级）\n  - `IDLE_WORKER_TIMEOUT = 300 * HZ`：空闲线程保留时间（5 分钟）\n  - `MAYDAY_INITIAL_TIMEOUT`：工作积压时触发救援的延迟（10ms）\n\n- **统计指标**（`pool_workqueue_stats`）：\n  - `PWQ_STAT_STARTED` / `PWQ_STAT_COMPLETED`：工作项执行统计\n  - `PWQ_STAT_MAYDAY` / `PWQ_STAT_RESCUED`：紧急救援事件计数\n\n## 3. 关键实现\n\n### 工作池管理\n- **绑定池（Bound Pool）**：与特定 CPU 关联，工作线程默认绑定到该 CPU。当 CPU 离线时，池进入 `DISASSOCIATED` 状态，工作线程转为非绑定模式。\n- **非绑定池（Unbound Pool）**：动态创建，通过哈希表（`unbound_pool_hash`）按属性（`workqueue_attrs`）去重，支持跨 CPU 调度。\n- **并发控制**：通过 `nr_running` 计数器和 `max_active` 限制，防止工作项过度并发执行。\n\n### 工作线程生命周期\n- **空闲管理**：空闲线程加入 `idle_list`，超时（`IDLE_WORKER_TIMEOUT`）后被回收。\n- **动态伸缩**：当工作积压时，通过 `mayday_timer` 触发新线程创建；若创建失败，向全局救援线程（rescuer）求助。\n- **状态标志**：使用位标志（如 `WORKER_IDLE`, `WORKER_PREP`）高效管理线程状态，避免锁竞争。\n\n### 内存与同步\n- **RCU 保护**：工作池销毁通过 RCU 延迟释放，确保 `get_work_pool()` 等读取路径无锁安全。\n- **锁分层**：\n  - `pool->lock`（自旋锁）：保护池内部状态\n  - `wq_pool_mutex`：全局池管理互斥锁\n  - `wq_pool_attach_mutex`：防止 CPU 绑定状态变更冲突\n\n### 工作项调度\n- **数据指针复用**：`work_struct->data` 的高有效位存储 `pool_workqueue` 指针，低有效位用于标志位（如 `WORK_STRUCT_INACTIVE`）。\n- **优先级支持**：高优先级工作池使用 `HIGHPRI_NICE_LEVEL = MIN_NICE` 提升调度优先级。\n\n## 4. 依赖关系\n\n- **内核子系统**：\n  - **调度器**（`<linux/sched.h>`）：创建工作线程（kworker），管理 CPU 亲和性\n  - **内存管理**（`<linux/slab.h>`）：分配工作池、工作队列等结构\n  - **CPU 热插拔**（`<linux/cpu.h>`）：处理 CPU 上下线时的池绑定状态切换\n  - **RCU**（`<linux/rculist.h>`）：实现无锁读取路径\n  - **定时器**（`<linux/timer.h>`）：实现空闲超时和救援机制\n\n- **内部依赖**：\n  - `workqueue_internal.h`：定义 `struct worker` 等内部结构\n  - `Documentation/core-api/workqueue.rst`：详细设计文档\n\n## 5. 使用场景\n\n- **驱动程序延迟操作**：硬件中断后调度下半部处理（如网络包处理、磁盘 I/O 完成回调）。\n- **内核子系统异步任务**：文件系统元数据更新、内存回收、电源管理状态切换。\n- **高优先级任务**：使用 `WQ_HIGHPRI` 标志创建工作队列，确保关键任务及时执行（如死锁恢复）。\n- **CPU 密集型任务**：标记 `WQ_CPU_INTENSIVE` 避免占用过多并发槽位，提升系统响应性。\n- **NUMA 感知调度**：非绑定工作队列可指定 NUMA 节点，优化内存访问延迟。",
      "similarity": 0.5532702207565308,
      "chunks": [
        {
          "chunk_id": 29,
          "file_path": "kernel/workqueue.c",
          "start_line": 5864,
          "end_line": 5966,
          "content": [
            "int workqueue_unbound_exclude_cpumask(cpumask_var_t exclude_cpumask)",
            "{",
            "\tcpumask_var_t cpumask;",
            "\tint ret = 0;",
            "",
            "\tif (!zalloc_cpumask_var(&cpumask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tlockdep_assert_cpus_held();",
            "\tmutex_lock(&wq_pool_mutex);",
            "",
            "\t/* Save the current isolated cpumask & export it via sysfs */",
            "\tcpumask_copy(wq_isolated_cpumask, exclude_cpumask);",
            "",
            "\t/*",
            "\t * If the operation fails, it will fall back to",
            "\t * wq_requested_unbound_cpumask which is initially set to",
            "\t * (HK_TYPE_WQ ∩ HK_TYPE_DOMAIN) house keeping mask and rewritten",
            "\t * by any subsequent write to workqueue/cpumask sysfs file.",
            "\t */",
            "\tif (!cpumask_andnot(cpumask, wq_requested_unbound_cpumask, exclude_cpumask))",
            "\t\tcpumask_copy(cpumask, wq_requested_unbound_cpumask);",
            "\tif (!cpumask_equal(cpumask, wq_unbound_cpumask))",
            "\t\tret = workqueue_apply_unbound_cpumask(cpumask);",
            "",
            "\tmutex_unlock(&wq_pool_mutex);",
            "\tfree_cpumask_var(cpumask);",
            "\treturn ret;",
            "}",
            "static int parse_affn_scope(const char *val)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < ARRAY_SIZE(wq_affn_names); i++) {",
            "\t\tif (!strncasecmp(val, wq_affn_names[i], strlen(wq_affn_names[i])))",
            "\t\t\treturn i;",
            "\t}",
            "\treturn -EINVAL;",
            "}",
            "static int wq_affn_dfl_set(const char *val, const struct kernel_param *kp)",
            "{",
            "\tstruct workqueue_struct *wq;",
            "\tint affn, cpu;",
            "",
            "\taffn = parse_affn_scope(val);",
            "\tif (affn < 0)",
            "\t\treturn affn;",
            "\tif (affn == WQ_AFFN_DFL)",
            "\t\treturn -EINVAL;",
            "",
            "\tcpus_read_lock();",
            "\tmutex_lock(&wq_pool_mutex);",
            "",
            "\twq_affn_dfl = affn;",
            "",
            "\tlist_for_each_entry(wq, &workqueues, list) {",
            "\t\tfor_each_online_cpu(cpu) {",
            "\t\t\twq_update_pod(wq, cpu, cpu, true);",
            "\t\t}",
            "\t}",
            "",
            "\tmutex_unlock(&wq_pool_mutex);",
            "\tcpus_read_unlock();",
            "",
            "\treturn 0;",
            "}",
            "static int wq_affn_dfl_get(char *buffer, const struct kernel_param *kp)",
            "{",
            "\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\n\", wq_affn_names[wq_affn_dfl]);",
            "}",
            "static ssize_t per_cpu_show(struct device *dev, struct device_attribute *attr,",
            "\t\t\t    char *buf)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "",
            "\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", (bool)!(wq->flags & WQ_UNBOUND));",
            "}",
            "static ssize_t max_active_show(struct device *dev,",
            "\t\t\t       struct device_attribute *attr, char *buf)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "",
            "\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", wq->saved_max_active);",
            "}",
            "static ssize_t max_active_store(struct device *dev,",
            "\t\t\t\tstruct device_attribute *attr, const char *buf,",
            "\t\t\t\tsize_t count)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "\tint val;",
            "",
            "\tif (sscanf(buf, \"%d\", &val) != 1 || val <= 0)",
            "\t\treturn -EINVAL;",
            "",
            "\tworkqueue_set_max_active(wq, val);",
            "\treturn count;",
            "}",
            "static void apply_wqattrs_lock(void)",
            "{",
            "\t/* CPUs should stay stable across pwq creations and installations */",
            "\tcpus_read_lock();",
            "\tmutex_lock(&wq_pool_mutex);",
            "}"
          ],
          "function_name": "workqueue_unbound_exclude_cpumask, parse_affn_scope, wq_affn_dfl_set, wq_affn_dfl_get, per_cpu_show, max_active_show, max_active_store, apply_wqattrs_lock",
          "description": "配置非绑定工作者的CPU排除掩码和默认亲和性策略，暴露工作队列属性供sysfs访问并管理最大并发数参数",
          "similarity": 0.5810449123382568
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/workqueue.c",
          "start_line": 895,
          "end_line": 1037,
          "content": [
            "static inline void worker_clr_flags(struct worker *worker, unsigned int flags)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "\tunsigned int oflags = worker->flags;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\tworker->flags &= ~flags;",
            "",
            "\t/*",
            "\t * If transitioning out of NOT_RUNNING, increment nr_running.  Note",
            "\t * that the nested NOT_RUNNING is not a noop.  NOT_RUNNING is mask",
            "\t * of multiple flags, not a single flag.",
            "\t */",
            "\tif ((flags & WORKER_NOT_RUNNING) && (oflags & WORKER_NOT_RUNNING))",
            "\t\tif (!(worker->flags & WORKER_NOT_RUNNING))",
            "\t\t\tpool->nr_running++;",
            "}",
            "static void worker_enter_idle(struct worker *worker)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tif (WARN_ON_ONCE(worker->flags & WORKER_IDLE) ||",
            "\t    WARN_ON_ONCE(!list_empty(&worker->entry) &&",
            "\t\t\t (worker->hentry.next || worker->hentry.pprev)))",
            "\t\treturn;",
            "",
            "\t/* can't use worker_set_flags(), also called from create_worker() */",
            "\tworker->flags |= WORKER_IDLE;",
            "\tpool->nr_idle++;",
            "\tworker->last_active = jiffies;",
            "",
            "\t/* idle_list is LIFO */",
            "\tlist_add(&worker->entry, &pool->idle_list);",
            "",
            "\tif (too_many_workers(pool) && !timer_pending(&pool->idle_timer))",
            "\t\tmod_timer(&pool->idle_timer, jiffies + IDLE_WORKER_TIMEOUT);",
            "",
            "\t/* Sanity check nr_running. */",
            "\tWARN_ON_ONCE(pool->nr_workers == pool->nr_idle && pool->nr_running);",
            "}",
            "static void worker_leave_idle(struct worker *worker)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tif (WARN_ON_ONCE(!(worker->flags & WORKER_IDLE)))",
            "\t\treturn;",
            "\tworker_clr_flags(worker, WORKER_IDLE);",
            "\tpool->nr_idle--;",
            "\tlist_del_init(&worker->entry);",
            "}",
            "static void move_linked_works(struct work_struct *work, struct list_head *head,",
            "\t\t\t      struct work_struct **nextp)",
            "{",
            "\tstruct work_struct *n;",
            "",
            "\t/*",
            "\t * Linked worklist will always end before the end of the list,",
            "\t * use NULL for list head.",
            "\t */",
            "\tlist_for_each_entry_safe_from(work, n, NULL, entry) {",
            "\t\tlist_move_tail(&work->entry, head);",
            "\t\tif (!(*work_data_bits(work) & WORK_STRUCT_LINKED))",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\t/*",
            "\t * If we're already inside safe list traversal and have moved",
            "\t * multiple works to the scheduled queue, the next position",
            "\t * needs to be updated.",
            "\t */",
            "\tif (nextp)",
            "\t\t*nextp = n;",
            "}",
            "static bool assign_work(struct work_struct *work, struct worker *worker,",
            "\t\t\tstruct work_struct **nextp)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "\tstruct worker *collision;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\t/*",
            "\t * A single work shouldn't be executed concurrently by multiple workers.",
            "\t * __queue_work() ensures that @work doesn't jump to a different pool",
            "\t * while still running in the previous pool. Here, we should ensure that",
            "\t * @work is not executed concurrently by multiple workers from the same",
            "\t * pool. Check whether anyone is already processing the work. If so,",
            "\t * defer the work to the currently executing one.",
            "\t */",
            "\tcollision = find_worker_executing_work(pool, work);",
            "\tif (unlikely(collision)) {",
            "\t\tmove_linked_works(work, &collision->scheduled, nextp);",
            "\t\treturn false;",
            "\t}",
            "",
            "\tmove_linked_works(work, &worker->scheduled, nextp);",
            "\treturn true;",
            "}",
            "static bool kick_pool(struct worker_pool *pool)",
            "{",
            "\tstruct worker *worker = first_idle_worker(pool);",
            "\tstruct task_struct *p;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\tif (!need_more_worker(pool) || !worker)",
            "\t\treturn false;",
            "",
            "\tp = worker->task;",
            "",
            "#ifdef CONFIG_SMP",
            "\t/*",
            "\t * Idle @worker is about to execute @work and waking up provides an",
            "\t * opportunity to migrate @worker at a lower cost by setting the task's",
            "\t * wake_cpu field. Let's see if we want to move @worker to improve",
            "\t * execution locality.",
            "\t *",
            "\t * We're waking the worker that went idle the latest and there's some",
            "\t * chance that @worker is marked idle but hasn't gone off CPU yet. If",
            "\t * so, setting the wake_cpu won't do anything. As this is a best-effort",
            "\t * optimization and the race window is narrow, let's leave as-is for",
            "\t * now. If this becomes pronounced, we can skip over workers which are",
            "\t * still on cpu when picking an idle worker.",
            "\t *",
            "\t * If @pool has non-strict affinity, @worker might have ended up outside",
            "\t * its affinity scope. Repatriate.",
            "\t */",
            "\tif (!pool->attrs->affn_strict &&",
            "\t    !cpumask_test_cpu(p->wake_cpu, pool->attrs->__pod_cpumask)) {",
            "\t\tstruct work_struct *work = list_first_entry(&pool->worklist,",
            "\t\t\t\t\t\tstruct work_struct, entry);",
            "\t\tint wake_cpu = cpumask_any_and_distribute(pool->attrs->__pod_cpumask,",
            "\t\t\t\t\t\t\t  cpu_online_mask);",
            "\t\tif (wake_cpu < nr_cpu_ids) {",
            "\t\t\tp->wake_cpu = wake_cpu;",
            "\t\t\tget_work_pwq(work)->stats[PWQ_STAT_REPATRIATED]++;",
            "\t\t}",
            "\t}",
            "#endif",
            "\twake_up_process(p);",
            "\treturn true;",
            "}"
          ],
          "function_name": "worker_clr_flags, worker_enter_idle, worker_leave_idle, move_linked_works, assign_work, kick_pool",
          "description": "实现工作者线程空闲状态切换和工作项迁移机制，包含空闲工作者列表管理、链接工作项批量转移功能，以及唤醒工作者线程的调度逻辑，支持跨CPU亲和性迁移优化，确保工作项正确分发到可用工作者线程。",
          "similarity": 0.5741130709648132
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/workqueue.c",
          "start_line": 692,
          "end_line": 797,
          "content": [
            "static void set_work_pool_and_clear_pending(struct work_struct *work,",
            "\t\t\t\t\t    int pool_id)",
            "{",
            "\t/*",
            "\t * The following wmb is paired with the implied mb in",
            "\t * test_and_set_bit(PENDING) and ensures all updates to @work made",
            "\t * here are visible to and precede any updates by the next PENDING",
            "\t * owner.",
            "\t */",
            "\tsmp_wmb();",
            "\tset_work_data(work, (unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT, 0);",
            "\t/*",
            "\t * The following mb guarantees that previous clear of a PENDING bit",
            "\t * will not be reordered with any speculative LOADS or STORES from",
            "\t * work->current_func, which is executed afterwards.  This possible",
            "\t * reordering can lead to a missed execution on attempt to queue",
            "\t * the same @work.  E.g. consider this case:",
            "\t *",
            "\t *   CPU#0                         CPU#1",
            "\t *   ----------------------------  --------------------------------",
            "\t *",
            "\t * 1  STORE event_indicated",
            "\t * 2  queue_work_on() {",
            "\t * 3    test_and_set_bit(PENDING)",
            "\t * 4 }                             set_..._and_clear_pending() {",
            "\t * 5                                 set_work_data() # clear bit",
            "\t * 6                                 smp_mb()",
            "\t * 7                               work->current_func() {",
            "\t * 8\t\t\t\t      LOAD event_indicated",
            "\t *\t\t\t\t   }",
            "\t *",
            "\t * Without an explicit full barrier speculative LOAD on line 8 can",
            "\t * be executed before CPU#0 does STORE on line 1.  If that happens,",
            "\t * CPU#0 observes the PENDING bit is still set and new execution of",
            "\t * a @work is not queued in a hope, that CPU#1 will eventually",
            "\t * finish the queued @work.  Meanwhile CPU#1 does not see",
            "\t * event_indicated is set, because speculative LOAD was executed",
            "\t * before actual STORE.",
            "\t */",
            "\tsmp_mb();",
            "}",
            "static void clear_work_data(struct work_struct *work)",
            "{",
            "\tsmp_wmb();\t/* see set_work_pool_and_clear_pending() */",
            "\tset_work_data(work, WORK_STRUCT_NO_POOL, 0);",
            "}",
            "static int get_work_pool_id(struct work_struct *work)",
            "{",
            "\tunsigned long data = atomic_long_read(&work->data);",
            "",
            "\tif (data & WORK_STRUCT_PWQ)",
            "\t\treturn work_struct_pwq(data)->pool->id;",
            "",
            "\treturn data >> WORK_OFFQ_POOL_SHIFT;",
            "}",
            "static void mark_work_canceling(struct work_struct *work)",
            "{",
            "\tunsigned long pool_id = get_work_pool_id(work);",
            "",
            "\tpool_id <<= WORK_OFFQ_POOL_SHIFT;",
            "\tset_work_data(work, pool_id | WORK_OFFQ_CANCELING, WORK_STRUCT_PENDING);",
            "}",
            "static bool work_is_canceling(struct work_struct *work)",
            "{",
            "\tunsigned long data = atomic_long_read(&work->data);",
            "",
            "\treturn !(data & WORK_STRUCT_PWQ) && (data & WORK_OFFQ_CANCELING);",
            "}",
            "static bool need_more_worker(struct worker_pool *pool)",
            "{",
            "\treturn !list_empty(&pool->worklist) && !pool->nr_running;",
            "}",
            "static bool may_start_working(struct worker_pool *pool)",
            "{",
            "\treturn pool->nr_idle;",
            "}",
            "static bool keep_working(struct worker_pool *pool)",
            "{",
            "\treturn !list_empty(&pool->worklist) && (pool->nr_running <= 1);",
            "}",
            "static bool need_to_create_worker(struct worker_pool *pool)",
            "{",
            "\treturn need_more_worker(pool) && !may_start_working(pool);",
            "}",
            "static bool too_many_workers(struct worker_pool *pool)",
            "{",
            "\tbool managing = pool->flags & POOL_MANAGER_ACTIVE;",
            "\tint nr_idle = pool->nr_idle + managing; /* manager is considered idle */",
            "\tint nr_busy = pool->nr_workers - nr_idle;",
            "",
            "\treturn nr_idle > 2 && (nr_idle - 2) * MAX_IDLE_WORKERS_RATIO >= nr_busy;",
            "}",
            "static inline void worker_set_flags(struct worker *worker, unsigned int flags)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\t/* If transitioning into NOT_RUNNING, adjust nr_running. */",
            "\tif ((flags & WORKER_NOT_RUNNING) &&",
            "\t    !(worker->flags & WORKER_NOT_RUNNING)) {",
            "\t\tpool->nr_running--;",
            "\t}",
            "",
            "\tworker->flags |= flags;",
            "}"
          ],
          "function_name": "set_work_pool_and_clear_pending, clear_work_data, get_work_pool_id, mark_work_canceling, work_is_canceling, need_more_worker, may_start_working, keep_working, need_to_create_worker, too_many_workers, worker_set_flags",
          "description": "实现工作者线程池状态控制逻辑，包含需要创建新工作者的判断条件、工作者空闲状态管理、工作项冲突检测及任务分配函数，通过锁保护保证池状态一致性，维护工作者线程与待处理工作项的匹配关系。",
          "similarity": 0.5736505389213562
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/workqueue.c",
          "start_line": 3156,
          "end_line": 3314,
          "content": [
            "static void touch_wq_lockdep_map(struct workqueue_struct *wq)",
            "{",
            "\tlock_map_acquire(&wq->lockdep_map);",
            "\tlock_map_release(&wq->lockdep_map);",
            "}",
            "static void touch_work_lockdep_map(struct work_struct *work,",
            "\t\t\t\t   struct workqueue_struct *wq)",
            "{",
            "\tlock_map_acquire(&work->lockdep_map);",
            "\tlock_map_release(&work->lockdep_map);",
            "}",
            "void __flush_workqueue(struct workqueue_struct *wq)",
            "{",
            "\tstruct wq_flusher this_flusher = {",
            "\t\t.list = LIST_HEAD_INIT(this_flusher.list),",
            "\t\t.flush_color = -1,",
            "\t\t.done = COMPLETION_INITIALIZER_ONSTACK_MAP(this_flusher.done, wq->lockdep_map),",
            "\t};",
            "\tint next_color;",
            "",
            "\tif (WARN_ON(!wq_online))",
            "\t\treturn;",
            "",
            "\ttouch_wq_lockdep_map(wq);",
            "",
            "\tmutex_lock(&wq->mutex);",
            "",
            "\t/*",
            "\t * Start-to-wait phase",
            "\t */",
            "\tnext_color = work_next_color(wq->work_color);",
            "",
            "\tif (next_color != wq->flush_color) {",
            "\t\t/*",
            "\t\t * Color space is not full.  The current work_color",
            "\t\t * becomes our flush_color and work_color is advanced",
            "\t\t * by one.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(!list_empty(&wq->flusher_overflow));",
            "\t\tthis_flusher.flush_color = wq->work_color;",
            "\t\twq->work_color = next_color;",
            "",
            "\t\tif (!wq->first_flusher) {",
            "\t\t\t/* no flush in progress, become the first flusher */",
            "\t\t\tWARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);",
            "",
            "\t\t\twq->first_flusher = &this_flusher;",
            "",
            "\t\t\tif (!flush_workqueue_prep_pwqs(wq, wq->flush_color,",
            "\t\t\t\t\t\t       wq->work_color)) {",
            "\t\t\t\t/* nothing to flush, done */",
            "\t\t\t\twq->flush_color = next_color;",
            "\t\t\t\twq->first_flusher = NULL;",
            "\t\t\t\tgoto out_unlock;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\t/* wait in queue */",
            "\t\t\tWARN_ON_ONCE(wq->flush_color == this_flusher.flush_color);",
            "\t\t\tlist_add_tail(&this_flusher.list, &wq->flusher_queue);",
            "\t\t\tflush_workqueue_prep_pwqs(wq, -1, wq->work_color);",
            "\t\t}",
            "\t} else {",
            "\t\t/*",
            "\t\t * Oops, color space is full, wait on overflow queue.",
            "\t\t * The next flush completion will assign us",
            "\t\t * flush_color and transfer to flusher_queue.",
            "\t\t */",
            "\t\tlist_add_tail(&this_flusher.list, &wq->flusher_overflow);",
            "\t}",
            "",
            "\tcheck_flush_dependency(wq, NULL, false);",
            "",
            "\tmutex_unlock(&wq->mutex);",
            "",
            "\twait_for_completion(&this_flusher.done);",
            "",
            "\t/*",
            "\t * Wake-up-and-cascade phase",
            "\t *",
            "\t * First flushers are responsible for cascading flushes and",
            "\t * handling overflow.  Non-first flushers can simply return.",
            "\t */",
            "\tif (READ_ONCE(wq->first_flusher) != &this_flusher)",
            "\t\treturn;",
            "",
            "\tmutex_lock(&wq->mutex);",
            "",
            "\t/* we might have raced, check again with mutex held */",
            "\tif (wq->first_flusher != &this_flusher)",
            "\t\tgoto out_unlock;",
            "",
            "\tWRITE_ONCE(wq->first_flusher, NULL);",
            "",
            "\tWARN_ON_ONCE(!list_empty(&this_flusher.list));",
            "\tWARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);",
            "",
            "\twhile (true) {",
            "\t\tstruct wq_flusher *next, *tmp;",
            "",
            "\t\t/* complete all the flushers sharing the current flush color */",
            "\t\tlist_for_each_entry_safe(next, tmp, &wq->flusher_queue, list) {",
            "\t\t\tif (next->flush_color != wq->flush_color)",
            "\t\t\t\tbreak;",
            "\t\t\tlist_del_init(&next->list);",
            "\t\t\tcomplete(&next->done);",
            "\t\t}",
            "",
            "\t\tWARN_ON_ONCE(!list_empty(&wq->flusher_overflow) &&",
            "\t\t\t     wq->flush_color != work_next_color(wq->work_color));",
            "",
            "\t\t/* this flush_color is finished, advance by one */",
            "\t\twq->flush_color = work_next_color(wq->flush_color);",
            "",
            "\t\t/* one color has been freed, handle overflow queue */",
            "\t\tif (!list_empty(&wq->flusher_overflow)) {",
            "\t\t\t/*",
            "\t\t\t * Assign the same color to all overflowed",
            "\t\t\t * flushers, advance work_color and append to",
            "\t\t\t * flusher_queue.  This is the start-to-wait",
            "\t\t\t * phase for these overflowed flushers.",
            "\t\t\t */",
            "\t\t\tlist_for_each_entry(tmp, &wq->flusher_overflow, list)",
            "\t\t\t\ttmp->flush_color = wq->work_color;",
            "",
            "\t\t\twq->work_color = work_next_color(wq->work_color);",
            "",
            "\t\t\tlist_splice_tail_init(&wq->flusher_overflow,",
            "\t\t\t\t\t      &wq->flusher_queue);",
            "\t\t\tflush_workqueue_prep_pwqs(wq, -1, wq->work_color);",
            "\t\t}",
            "",
            "\t\tif (list_empty(&wq->flusher_queue)) {",
            "\t\t\tWARN_ON_ONCE(wq->flush_color != wq->work_color);",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Need to flush more colors.  Make the next flusher",
            "\t\t * the new first flusher and arm pwqs.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(wq->flush_color == wq->work_color);",
            "\t\tWARN_ON_ONCE(wq->flush_color != next->flush_color);",
            "",
            "\t\tlist_del_init(&next->list);",
            "\t\twq->first_flusher = next;",
            "",
            "\t\tif (flush_workqueue_prep_pwqs(wq, wq->flush_color, -1))",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * Meh... this color is already done, clear first",
            "\t\t * flusher and repeat cascading.",
            "\t\t */",
            "\t\twq->first_flusher = NULL;",
            "\t}",
            "",
            "out_unlock:",
            "\tmutex_unlock(&wq->mutex);",
            "}"
          ],
          "function_name": "touch_wq_lockdep_map, touch_work_lockdep_map, __flush_workqueue",
          "description": "实现工作队列强制刷新的核心逻辑，包含颜色轮转管理、依赖关系检查及多阶段刷新流程控制",
          "similarity": 0.559088945388794
        },
        {
          "chunk_id": 26,
          "file_path": "kernel/workqueue.c",
          "start_line": 5431,
          "end_line": 5543,
          "content": [
            "static void unbind_workers(int cpu)",
            "{",
            "\tstruct worker_pool *pool;",
            "\tstruct worker *worker;",
            "",
            "\tfor_each_cpu_worker_pool(pool, cpu) {",
            "\t\tmutex_lock(&wq_pool_attach_mutex);",
            "\t\traw_spin_lock_irq(&pool->lock);",
            "",
            "\t\t/*",
            "\t\t * We've blocked all attach/detach operations. Make all workers",
            "\t\t * unbound and set DISASSOCIATED.  Before this, all workers",
            "\t\t * must be on the cpu.  After this, they may become diasporas.",
            "\t\t * And the preemption disabled section in their sched callbacks",
            "\t\t * are guaranteed to see WORKER_UNBOUND since the code here",
            "\t\t * is on the same cpu.",
            "\t\t */",
            "\t\tfor_each_pool_worker(worker, pool)",
            "\t\t\tworker->flags |= WORKER_UNBOUND;",
            "",
            "\t\tpool->flags |= POOL_DISASSOCIATED;",
            "",
            "\t\t/*",
            "\t\t * The handling of nr_running in sched callbacks are disabled",
            "\t\t * now.  Zap nr_running.  After this, nr_running stays zero and",
            "\t\t * need_more_worker() and keep_working() are always true as",
            "\t\t * long as the worklist is not empty.  This pool now behaves as",
            "\t\t * an unbound (in terms of concurrency management) pool which",
            "\t\t * are served by workers tied to the pool.",
            "\t\t */",
            "\t\tpool->nr_running = 0;",
            "",
            "\t\t/*",
            "\t\t * With concurrency management just turned off, a busy",
            "\t\t * worker blocking could lead to lengthy stalls.  Kick off",
            "\t\t * unbound chain execution of currently pending work items.",
            "\t\t */",
            "\t\tkick_pool(pool);",
            "",
            "\t\traw_spin_unlock_irq(&pool->lock);",
            "",
            "\t\tfor_each_pool_worker(worker, pool)",
            "\t\t\tunbind_worker(worker);",
            "",
            "\t\tmutex_unlock(&wq_pool_attach_mutex);",
            "\t}",
            "}",
            "static void rebind_workers(struct worker_pool *pool)",
            "{",
            "\tstruct worker *worker;",
            "",
            "\tlockdep_assert_held(&wq_pool_attach_mutex);",
            "",
            "\t/*",
            "\t * Restore CPU affinity of all workers.  As all idle workers should",
            "\t * be on the run-queue of the associated CPU before any local",
            "\t * wake-ups for concurrency management happen, restore CPU affinity",
            "\t * of all workers first and then clear UNBOUND.  As we're called",
            "\t * from CPU_ONLINE, the following shouldn't fail.",
            "\t */",
            "\tfor_each_pool_worker(worker, pool) {",
            "\t\tkthread_set_per_cpu(worker->task, pool->cpu);",
            "\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task,",
            "\t\t\t\t\t\t  pool_allowed_cpus(pool)) < 0);",
            "\t}",
            "",
            "\traw_spin_lock_irq(&pool->lock);",
            "",
            "\tpool->flags &= ~POOL_DISASSOCIATED;",
            "",
            "\tfor_each_pool_worker(worker, pool) {",
            "\t\tunsigned int worker_flags = worker->flags;",
            "",
            "\t\t/*",
            "\t\t * We want to clear UNBOUND but can't directly call",
            "\t\t * worker_clr_flags() or adjust nr_running.  Atomically",
            "\t\t * replace UNBOUND with another NOT_RUNNING flag REBOUND.",
            "\t\t * @worker will clear REBOUND using worker_clr_flags() when",
            "\t\t * it initiates the next execution cycle thus restoring",
            "\t\t * concurrency management.  Note that when or whether",
            "\t\t * @worker clears REBOUND doesn't affect correctness.",
            "\t\t *",
            "\t\t * WRITE_ONCE() is necessary because @worker->flags may be",
            "\t\t * tested without holding any lock in",
            "\t\t * wq_worker_running().  Without it, NOT_RUNNING test may",
            "\t\t * fail incorrectly leading to premature concurrency",
            "\t\t * management operations.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(!(worker_flags & WORKER_UNBOUND));",
            "\t\tworker_flags |= WORKER_REBOUND;",
            "\t\tworker_flags &= ~WORKER_UNBOUND;",
            "\t\tWRITE_ONCE(worker->flags, worker_flags);",
            "\t}",
            "",
            "\traw_spin_unlock_irq(&pool->lock);",
            "}",
            "static void restore_unbound_workers_cpumask(struct worker_pool *pool, int cpu)",
            "{",
            "\tstatic cpumask_t cpumask;",
            "\tstruct worker *worker;",
            "",
            "\tlockdep_assert_held(&wq_pool_attach_mutex);",
            "",
            "\t/* is @cpu allowed for @pool? */",
            "\tif (!cpumask_test_cpu(cpu, pool->attrs->cpumask))",
            "\t\treturn;",
            "",
            "\tcpumask_and(&cpumask, pool->attrs->cpumask, cpu_online_mask);",
            "",
            "\t/* as we're called from CPU_ONLINE, the following shouldn't fail */",
            "\tfor_each_pool_worker(worker, pool)",
            "\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, &cpumask) < 0);",
            "}"
          ],
          "function_name": "unbind_workers, rebind_workers, restore_unbound_workers_cpumask",
          "description": "管理工作者与CPU的绑定状态，通过解除/重新绑定调整工作者运行环境，确保CPU上下线时并发控制正确性",
          "similarity": 0.5548151731491089
        }
      ]
    },
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.5439400672912598,
      "chunks": [
        {
          "chunk_id": 17,
          "file_path": "mm/mempolicy.c",
          "start_line": 2969,
          "end_line": 3093,
          "content": [
            "void mpol_put_task_policy(struct task_struct *task)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\ttask_lock(task);",
            "\tpol = task->mempolicy;",
            "\ttask->mempolicy = NULL;",
            "\ttask_unlock(task);",
            "\tmpol_put(pol);",
            "}",
            "static void sp_delete(struct shared_policy *sp, struct sp_node *n)",
            "{",
            "\trb_erase(&n->nd, &sp->root);",
            "\tsp_free(n);",
            "}",
            "static void sp_node_init(struct sp_node *node, unsigned long start,",
            "\t\t\tunsigned long end, struct mempolicy *pol)",
            "{",
            "\tnode->start = start;",
            "\tnode->end = end;",
            "\tnode->policy = pol;",
            "}",
            "static int shared_policy_replace(struct shared_policy *sp, pgoff_t start,",
            "\t\t\t\t pgoff_t end, struct sp_node *new)",
            "{",
            "\tstruct sp_node *n;",
            "\tstruct sp_node *n_new = NULL;",
            "\tstruct mempolicy *mpol_new = NULL;",
            "\tint ret = 0;",
            "",
            "restart:",
            "\twrite_lock(&sp->lock);",
            "\tn = sp_lookup(sp, start, end);",
            "\t/* Take care of old policies in the same range. */",
            "\twhile (n && n->start < end) {",
            "\t\tstruct rb_node *next = rb_next(&n->nd);",
            "\t\tif (n->start >= start) {",
            "\t\t\tif (n->end <= end)",
            "\t\t\t\tsp_delete(sp, n);",
            "\t\t\telse",
            "\t\t\t\tn->start = end;",
            "\t\t} else {",
            "\t\t\t/* Old policy spanning whole new range. */",
            "\t\t\tif (n->end > end) {",
            "\t\t\t\tif (!n_new)",
            "\t\t\t\t\tgoto alloc_new;",
            "",
            "\t\t\t\t*mpol_new = *n->policy;",
            "\t\t\t\tatomic_set(&mpol_new->refcnt, 1);",
            "\t\t\t\tsp_node_init(n_new, end, n->end, mpol_new);",
            "\t\t\t\tn->end = start;",
            "\t\t\t\tsp_insert(sp, n_new);",
            "\t\t\t\tn_new = NULL;",
            "\t\t\t\tmpol_new = NULL;",
            "\t\t\t\tbreak;",
            "\t\t\t} else",
            "\t\t\t\tn->end = start;",
            "\t\t}",
            "\t\tif (!next)",
            "\t\t\tbreak;",
            "\t\tn = rb_entry(next, struct sp_node, nd);",
            "\t}",
            "\tif (new)",
            "\t\tsp_insert(sp, new);",
            "\twrite_unlock(&sp->lock);",
            "\tret = 0;",
            "",
            "err_out:",
            "\tif (mpol_new)",
            "\t\tmpol_put(mpol_new);",
            "\tif (n_new)",
            "\t\tkmem_cache_free(sn_cache, n_new);",
            "",
            "\treturn ret;",
            "",
            "alloc_new:",
            "\twrite_unlock(&sp->lock);",
            "\tret = -ENOMEM;",
            "\tn_new = kmem_cache_alloc(sn_cache, GFP_KERNEL);",
            "\tif (!n_new)",
            "\t\tgoto err_out;",
            "\tmpol_new = kmem_cache_alloc(policy_cache, GFP_KERNEL);",
            "\tif (!mpol_new)",
            "\t\tgoto err_out;",
            "\tatomic_set(&mpol_new->refcnt, 1);",
            "\tgoto restart;",
            "}",
            "void mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol)",
            "{",
            "\tint ret;",
            "",
            "\tsp->root = RB_ROOT;\t\t/* empty tree == default mempolicy */",
            "\trwlock_init(&sp->lock);",
            "",
            "\tif (mpol) {",
            "\t\tstruct sp_node *sn;",
            "\t\tstruct mempolicy *npol;",
            "\t\tNODEMASK_SCRATCH(scratch);",
            "",
            "\t\tif (!scratch)",
            "\t\t\tgoto put_mpol;",
            "",
            "\t\t/* contextualize the tmpfs mount point mempolicy to this file */",
            "\t\tnpol = mpol_new(mpol->mode, mpol->flags, &mpol->w.user_nodemask);",
            "\t\tif (IS_ERR(npol))",
            "\t\t\tgoto free_scratch; /* no valid nodemask intersection */",
            "",
            "\t\ttask_lock(current);",
            "\t\tret = mpol_set_nodemask(npol, &mpol->w.user_nodemask, scratch);",
            "\t\ttask_unlock(current);",
            "\t\tif (ret)",
            "\t\t\tgoto put_npol;",
            "",
            "\t\t/* alloc node covering entire file; adds ref to file's npol */",
            "\t\tsn = sp_alloc(0, MAX_LFS_FILESIZE >> PAGE_SHIFT, npol);",
            "\t\tif (sn)",
            "\t\t\tsp_insert(sp, sn);",
            "put_npol:",
            "\t\tmpol_put(npol);\t/* drop initial ref on file's npol */",
            "free_scratch:",
            "\t\tNODEMASK_SCRATCH_FREE(scratch);",
            "put_mpol:",
            "\t\tmpol_put(mpol);\t/* drop our incoming ref on sb mpol */",
            "\t}",
            "}"
          ],
          "function_name": "mpol_put_task_policy, sp_delete, sp_node_init, shared_policy_replace, mpol_shared_policy_init",
          "description": "mpol_put_task_policy释放任务级内存策略引用，sp_delete从RB树删除节点并回收资源，sp_node_init初始化共享策略节点，shared_policy_replace替换共享策略区间并处理节点分裂，mpol_shared_policy_init初始化共享策略结构体并设置初始策略。",
          "similarity": 0.5537090301513672
        },
        {
          "chunk_id": 10,
          "file_path": "mm/mempolicy.c",
          "start_line": 1735,
          "end_line": 1838,
          "content": [
            "static long kernel_set_mempolicy(int mode, const unsigned long __user *nmask,",
            "\t\t\t\t unsigned long maxnode)",
            "{",
            "\tunsigned short mode_flags;",
            "\tnodemask_t nodes;",
            "\tint lmode = mode;",
            "\tint err;",
            "",
            "\terr = sanitize_mpol_flags(&lmode, &mode_flags);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\terr = get_nodes(&nodes, nmask, maxnode);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\treturn do_set_mempolicy(lmode, mode_flags, &nodes);",
            "}",
            "static int kernel_migrate_pages(pid_t pid, unsigned long maxnode,",
            "\t\t\t\tconst unsigned long __user *old_nodes,",
            "\t\t\t\tconst unsigned long __user *new_nodes)",
            "{",
            "\tstruct mm_struct *mm = NULL;",
            "\tstruct task_struct *task;",
            "\tnodemask_t task_nodes;",
            "\tint err;",
            "\tnodemask_t *old;",
            "\tnodemask_t *new;",
            "\tNODEMASK_SCRATCH(scratch);",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\told = &scratch->mask1;",
            "\tnew = &scratch->mask2;",
            "",
            "\terr = get_nodes(old, old_nodes, maxnode);",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\terr = get_nodes(new, new_nodes, maxnode);",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\t/* Find the mm_struct */",
            "\trcu_read_lock();",
            "\ttask = pid ? find_task_by_vpid(pid) : current;",
            "\tif (!task) {",
            "\t\trcu_read_unlock();",
            "\t\terr = -ESRCH;",
            "\t\tgoto out;",
            "\t}",
            "\tget_task_struct(task);",
            "",
            "\terr = -EINVAL;",
            "",
            "\t/*",
            "\t * Check if this process has the right to modify the specified process.",
            "\t * Use the regular \"ptrace_may_access()\" checks.",
            "\t */",
            "\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {",
            "\t\trcu_read_unlock();",
            "\t\terr = -EPERM;",
            "\t\tgoto out_put;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\ttask_nodes = cpuset_mems_allowed(task);",
            "\t/* Is the user allowed to access the target nodes? */",
            "\tif (!nodes_subset(*new, task_nodes) && !capable(CAP_SYS_NICE)) {",
            "\t\terr = -EPERM;",
            "\t\tgoto out_put;",
            "\t}",
            "",
            "\ttask_nodes = cpuset_mems_allowed(current);",
            "\tnodes_and(*new, *new, task_nodes);",
            "\tif (nodes_empty(*new))",
            "\t\tgoto out_put;",
            "",
            "\terr = security_task_movememory(task);",
            "\tif (err)",
            "\t\tgoto out_put;",
            "",
            "\tmm = get_task_mm(task);",
            "\tput_task_struct(task);",
            "",
            "\tif (!mm) {",
            "\t\terr = -EINVAL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\terr = do_migrate_pages(mm, old, new,",
            "\t\tcapable(CAP_SYS_NICE) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE);",
            "",
            "\tmmput(mm);",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "",
            "\treturn err;",
            "",
            "out_put:",
            "\tput_task_struct(task);",
            "\tgoto out;",
            "}"
          ],
          "function_name": "kernel_set_mempolicy, kernel_migrate_pages",
          "description": "kernel_set_mempolicy 设置进程的内存放置策略，通过sanitize_mpol_flags验证模式标志并调用do_set_mempolicy应用策略；kernel_migrate_pages 实现页面迁移，检查目标进程权限，限制迁移节点范围，并调用do_migrate_pages进行实际迁移操作。",
          "similarity": 0.5413694977760315
        },
        {
          "chunk_id": 3,
          "file_path": "mm/mempolicy.c",
          "start_line": 484,
          "end_line": 639,
          "content": [
            "static void mpol_rebind_preferred(struct mempolicy *pol,",
            "\t\t\t\t\t\tconst nodemask_t *nodes)",
            "{",
            "\tpol->w.cpuset_mems_allowed = *nodes;",
            "}",
            "static void mpol_rebind_policy(struct mempolicy *pol, const nodemask_t *newmask)",
            "{",
            "\tif (!pol || pol->mode == MPOL_LOCAL)",
            "\t\treturn;",
            "\tif (!mpol_store_user_nodemask(pol) &&",
            "\t    nodes_equal(pol->w.cpuset_mems_allowed, *newmask))",
            "\t\treturn;",
            "",
            "\tmpol_ops[pol->mode].rebind(pol, newmask);",
            "}",
            "void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new)",
            "{",
            "\tmpol_rebind_policy(tsk->mempolicy, new);",
            "}",
            "void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "",
            "\tmmap_write_lock(mm);",
            "\tfor_each_vma(vmi, vma) {",
            "\t\tvma_start_write(vma);",
            "\t\tmpol_rebind_policy(vma->vm_policy, new);",
            "\t}",
            "\tmmap_write_unlock(mm);",
            "}",
            "static bool strictly_unmovable(unsigned long flags)",
            "{",
            "\t/*",
            "\t * STRICT without MOVE flags lets do_mbind() fail immediately with -EIO",
            "\t * if any misplaced page is found.",
            "\t */",
            "\treturn (flags & (MPOL_MF_STRICT | MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ==",
            "\t\t\t MPOL_MF_STRICT;",
            "}",
            "static inline bool queue_folio_required(struct folio *folio,",
            "\t\t\t\t\tstruct queue_pages *qp)",
            "{",
            "\tint nid = folio_nid(folio);",
            "\tunsigned long flags = qp->flags;",
            "",
            "\treturn node_isset(nid, *qp->nmask) == !(flags & MPOL_MF_INVERT);",
            "}",
            "static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)",
            "{",
            "\tstruct folio *folio;",
            "\tstruct queue_pages *qp = walk->private;",
            "",
            "\tif (unlikely(is_pmd_migration_entry(*pmd))) {",
            "\t\tqp->nr_failed++;",
            "\t\treturn;",
            "\t}",
            "\tfolio = pmd_folio(*pmd);",
            "\tif (is_huge_zero_folio(folio)) {",
            "\t\twalk->action = ACTION_CONTINUE;",
            "\t\treturn;",
            "\t}",
            "\tif (!queue_folio_required(folio, qp))",
            "\t\treturn;",
            "\tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||",
            "\t    !vma_migratable(walk->vma) ||",
            "\t    !migrate_folio_add(folio, qp->pagelist, qp->flags))",
            "\t\tqp->nr_failed++;",
            "}",
            "static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,",
            "\t\t\tunsigned long end, struct mm_walk *walk)",
            "{",
            "\tconst fpb_t fpb_flags = FPB_IGNORE_DIRTY | FPB_IGNORE_SOFT_DIRTY;",
            "\tstruct vm_area_struct *vma = walk->vma;",
            "\tstruct folio *folio;",
            "\tstruct queue_pages *qp = walk->private;",
            "\tunsigned long flags = qp->flags;",
            "\tpte_t *pte, *mapped_pte;",
            "\tpte_t ptent;",
            "\tspinlock_t *ptl;",
            "\tint max_nr, nr;",
            "",
            "\tptl = pmd_trans_huge_lock(pmd, vma);",
            "\tif (ptl) {",
            "\t\tqueue_folios_pmd(pmd, walk);",
            "\t\tspin_unlock(ptl);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tmapped_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);",
            "\tif (!pte) {",
            "\t\twalk->action = ACTION_AGAIN;",
            "\t\treturn 0;",
            "\t}",
            "\tfor (; addr != end; pte += nr, addr += nr * PAGE_SIZE) {",
            "\t\tmax_nr = (end - addr) >> PAGE_SHIFT;",
            "\t\tnr = 1;",
            "\t\tptent = ptep_get(pte);",
            "\t\tif (pte_none(ptent))",
            "\t\t\tcontinue;",
            "\t\tif (!pte_present(ptent)) {",
            "\t\t\tif (is_migration_entry(pte_to_swp_entry(ptent)))",
            "\t\t\t\tqp->nr_failed++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfolio = vm_normal_folio(vma, addr, ptent);",
            "\t\tif (!folio || folio_is_zone_device(folio))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_large(folio) && max_nr != 1)",
            "\t\t\tnr = folio_pte_batch(folio, addr, pte, ptent,",
            "\t\t\t\t\t     max_nr, fpb_flags,",
            "\t\t\t\t\t     NULL, NULL, NULL);",
            "\t\t/*",
            "\t\t * vm_normal_folio() filters out zero pages, but there might",
            "\t\t * still be reserved folios to skip, perhaps in a VDSO.",
            "\t\t */",
            "\t\tif (folio_test_reserved(folio))",
            "\t\t\tcontinue;",
            "\t\tif (!queue_folio_required(folio, qp))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_large(folio)) {",
            "\t\t\t/*",
            "\t\t\t * A large folio can only be isolated from LRU once,",
            "\t\t\t * but may be mapped by many PTEs (and Copy-On-Write may",
            "\t\t\t * intersperse PTEs of other, order 0, folios).  This is",
            "\t\t\t * a common case, so don't mistake it for failure (but",
            "\t\t\t * there can be other cases of multi-mapped pages which",
            "\t\t\t * this quick check does not help to filter out - and a",
            "\t\t\t * search of the pagelist might grow to be prohibitive).",
            "\t\t\t *",
            "\t\t\t * migrate_pages(&pagelist) returns nr_failed folios, so",
            "\t\t\t * check \"large\" now so that queue_pages_range() returns",
            "\t\t\t * a comparable nr_failed folios.  This does imply that",
            "\t\t\t * if folio could not be isolated for some racy reason",
            "\t\t\t * at its first PTE, later PTEs will not give it another",
            "\t\t\t * chance of isolation; but keeps the accounting simple.",
            "\t\t\t */",
            "\t\t\tif (folio == qp->large)",
            "\t\t\t\tcontinue;",
            "\t\t\tqp->large = folio;",
            "\t\t}",
            "\t\tif (!(flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||",
            "\t\t    !vma_migratable(vma) ||",
            "\t\t    !migrate_folio_add(folio, qp->pagelist, flags)) {",
            "\t\t\tqp->nr_failed += nr;",
            "\t\t\tif (strictly_unmovable(flags))",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tpte_unmap_unlock(mapped_pte, ptl);",
            "\tcond_resched();",
            "out:",
            "\tif (qp->nr_failed && strictly_unmovable(flags))",
            "\t\treturn -EIO;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "mpol_rebind_preferred, mpol_rebind_policy, mpol_rebind_task, mpol_rebind_mm, strictly_unmovable, queue_folio_required, queue_folios_pmd, queue_folios_pte_range",
          "description": "处理页表项遍历与迁移操作，包含页帧队列检测、迁移标志验证及大页迁移逻辑，确保内存分配符合NUMA策略要求。",
          "similarity": 0.5361058712005615
        },
        {
          "chunk_id": 12,
          "file_path": "mm/mempolicy.c",
          "start_line": 2024,
          "end_line": 2135,
          "content": [
            "static unsigned int interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int nid;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "\t/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnid = next_node_in(current->il_prev, policy->nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\tif (nid < MAX_NUMNODES)",
            "\t\tcurrent->il_prev = nid;",
            "\treturn nid;",
            "}",
            "unsigned int mempolicy_slab_node(void)",
            "{",
            "\tstruct mempolicy *policy;",
            "\tint node = numa_mem_id();",
            "",
            "\tif (!in_task())",
            "\t\treturn node;",
            "",
            "\tpolicy = current->mempolicy;",
            "\tif (!policy)",
            "\t\treturn node;",
            "",
            "\tswitch (policy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\treturn first_node(policy->nodes);",
            "",
            "\tcase MPOL_INTERLEAVE:",
            "\t\treturn interleave_nodes(policy);",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn weighted_interleave_nodes(policy);",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t{",
            "\t\tstruct zoneref *z;",
            "",
            "\t\t/*",
            "\t\t * Follow bind policy behavior and start allocation at the",
            "\t\t * first node.",
            "\t\t */",
            "\t\tstruct zonelist *zonelist;",
            "\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);",
            "\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];",
            "\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,",
            "\t\t\t\t\t\t\t&policy->nodes);",
            "\t\treturn z->zone ? zone_to_nid(z->zone) : node;",
            "\t}",
            "\tcase MPOL_LOCAL:",
            "\t\treturn node;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static unsigned int read_once_policy_nodemask(struct mempolicy *pol,",
            "\t\t\t\t\t      nodemask_t *mask)",
            "{",
            "\t/*",
            "\t * barrier stabilizes the nodemask locally so that it can be iterated",
            "\t * over safely without concern for changes. Allocators validate node",
            "\t * selection does not violate mems_allowed, so this is safe.",
            "\t */",
            "\tbarrier();",
            "\tmemcpy(mask, &pol->nodes, sizeof(nodemask_t));",
            "\tbarrier();",
            "\treturn nodes_weight(*mask);",
            "}",
            "static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nr_nodes;",
            "\tu8 *table = NULL;",
            "\tunsigned int weight_total = 0;",
            "\tu8 weight;",
            "\tint nid = 0;",
            "",
            "\tnr_nodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nr_nodes)",
            "\t\treturn numa_node_id();",
            "",
            "\trcu_read_lock();",
            "",
            "\tstate = rcu_dereference(wi_state);",
            "\t/* Uninitialized wi_state means we should assume all weights are 1 */",
            "\tif (state)",
            "\t\ttable = state->iw_table;",
            "",
            "\t/* calculate the total weight */",
            "\tfor_each_node_mask(nid, nodemask)",
            "\t\tweight_total += table ? table[nid] : 1;",
            "",
            "\t/* Calculate the node offset based on totals */",
            "\ttarget = ilx % weight_total;",
            "\tnid = first_node(nodemask);",
            "\twhile (target) {",
            "\t\t/* detect system default usage */",
            "\t\tweight = table ? table[nid] : 1;",
            "\t\tif (target < weight)",
            "\t\t\tbreak;",
            "\t\ttarget -= weight;",
            "\t\tnid = next_node_in(nid, nodemask);",
            "\t}",
            "\trcu_read_unlock();",
            "\treturn nid;",
            "}"
          ],
          "function_name": "interleave_nodes, mempolicy_slab_node, read_once_policy_nodemask, weighted_interleave_nid",
          "description": "interleave_nodes 计算交错分配的下一个节点；mempolicy_slab_node 根据内存策略返回Slab分配的节点；read_once_policy_nodemask 安全读取策略节点掩码；weighted_interleave_nid 基于权重计算加权交错分配的目标节点。",
          "similarity": 0.5272704362869263
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mempolicy.c",
          "start_line": 168,
          "end_line": 268,
          "content": [
            "static u8 get_il_weight(int node)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tu8 weight = 1;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state)",
            "\t\tweight = state->iw_table[node];",
            "\trcu_read_unlock();",
            "\treturn weight;",
            "}",
            "static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)",
            "{",
            "\tu64 sum_bw = 0;",
            "\tunsigned int cast_sum_bw, scaling_factor = 1, iw_gcd = 0;",
            "\tint nid;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tsum_bw += bw[nid];",
            "",
            "\t/* Scale bandwidths to whole numbers in the range [1, weightiness] */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t/*",
            "\t\t * Try not to perform 64-bit division.",
            "\t\t * If sum_bw < scaling_factor, then sum_bw < U32_MAX.",
            "\t\t * If sum_bw > scaling_factor, then round the weight up to 1.",
            "\t\t */",
            "\t\tscaling_factor = weightiness * bw[nid];",
            "\t\tif (bw[nid] && sum_bw < scaling_factor) {",
            "\t\t\tcast_sum_bw = (unsigned int)sum_bw;",
            "\t\t\tnew_iw[nid] = scaling_factor / cast_sum_bw;",
            "\t\t} else {",
            "\t\t\tnew_iw[nid] = 1;",
            "\t\t}",
            "\t\tif (!iw_gcd)",
            "\t\t\tiw_gcd = new_iw[nid];",
            "\t\tiw_gcd = gcd(iw_gcd, new_iw[nid]);",
            "\t}",
            "",
            "\t/* 1:2 is strictly better than 16:32. Reduce by the weights' GCD. */",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tnew_iw[nid] /= iw_gcd;",
            "}",
            "int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tunsigned int *old_bw, *new_bw;",
            "\tunsigned int bw_val;",
            "\tint i;",
            "",
            "\tbw_val = min(coords->read_bandwidth, coords->write_bandwidth);",
            "\tnew_bw = kcalloc(nr_node_ids, sizeof(unsigned int), GFP_KERNEL);",
            "\tif (!new_bw)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew_wi_state = kmalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state) {",
            "\t\tkfree(new_bw);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tnew_wi_state->mode_auto = true;",
            "\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\tnew_wi_state->iw_table[i] = 1;",
            "",
            "\t/*",
            "\t * Update bandwidth info, even in manual mode. That way, when switching",
            "\t * to auto mode in the future, iw_table can be overwritten using",
            "\t * accurate bw data.",
            "\t */",
            "\tmutex_lock(&wi_state_lock);",
            "",
            "\told_bw = node_bw_table;",
            "\tif (old_bw)",
            "\t\tmemcpy(new_bw, old_bw, nr_node_ids * sizeof(*old_bw));",
            "\tnew_bw[node] = bw_val;",
            "\tnode_bw_table = new_bw;",
            "",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state && !old_wi_state->mode_auto) {",
            "\t\t/* Manual mode; skip reducing weights and updating wi_state */",
            "\t\tmutex_unlock(&wi_state_lock);",
            "\t\tkfree(new_wi_state);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* NULL wi_state assumes auto=true; reduce weights and update wi_state*/",
            "\treduce_interleave_weights(new_bw, new_wi_state->iw_table);",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "out:",
            "\tkfree(old_bw);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_il_weight, reduce_interleave_weights, mempolicy_set_node_perf",
          "description": "实现带权交错策略的权重计算与调整逻辑，通过获取节点带宽数据动态修改权重比例，支持根据性能参数更新节点间内存分配优先级。",
          "similarity": 0.5248434543609619
        }
      ]
    },
    {
      "source_file": "mm/workingset.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:34:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workingset.c`\n\n---\n\n# workingset.c 技术文档\n\n## 1. 文件概述\n\n`workingset.c` 实现了 Linux 内核中的 **工作集检测（Workingset Detection）** 机制，用于优化页面回收（page reclaim）策略。该机制通过跟踪页面的访问模式和重故障距离（refault distance），智能判断哪些页面应保留在内存中，从而减少系统颠簸（thrashing）并提升缓存效率。核心思想是：若一个被换出的页面在短时间内再次被访问（即重故障），且其重故障距离小于当前活跃页面数量，则应将其重新激活，以取代可能已不再活跃的现有活跃页面。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **Shadow Entry（影子条目）**：存储在页缓存槽位中的元数据，包含页面被驱逐时的时间戳（eviction counter 快照）、内存控制组 ID、节点 ID 和工作集标志。\n- **node->nonresident_age**：每个 NUMA 节点维护的计数器，记录非驻留页面的“年龄”，用于计算重故障距离。\n\n### 关键宏定义\n- `WORKINGSET_SHIFT`：工作集标识位偏移。\n- `EVICTION_SHIFT` / `EVICTION_MASK`：用于在 xarray 条目中紧凑编码驱逐时间戳的位操作参数。\n- `bucket_order`：当时间戳位数不足时，用于对驱逐事件进行分桶聚合的粒度。\n\n### 核心函数（部分实现）\n- `pack_shadow()`：将内存控制组 ID、节点指针、驱逐计数器值和工作集标志打包成一个 shadow entry。\n- （注：代码片段未完整展示其他关键函数如 `workingset_refault()`、`workingset_activation()` 等，但文档基于完整机制描述）\n\n## 3. 关键实现\n\n### 双 CLOCK 列表模型\n- 每个 NUMA 节点为文件页维护两个 LRU 列表：**inactive list**（不活跃）和 **active list**（活跃）。\n- 新缺页页面加入 inactive list 头部；回收从 inactive list 尾部扫描。\n- 在 inactive list 上被二次访问的页面晋升至 active list；active list 过长时，尾部页面降级到 inactive list。\n\n### 重故障距离（Refault Distance）算法\n1. **驱逐时记录**：页面被驱逐时，将其所在节点的 `nonresident_age` 计数器值（代表累计的驱逐+激活次数）作为时间戳存入 shadow entry。\n2. **重故障时计算**：\n   - 当缺页发生且存在对应 shadow entry 时，读取当前 `nonresident_age` 值（R）与 shadow 中存储的值（E）。\n   - 重故障距离 = `R - E`，表示页面不在内存期间发生的最小页面访问次数。\n3. **激活决策**：\n   - 若 `重故障距离 <= 当前活跃页面总数（file + anon）`，则认为若当时有足够 inactive 空间，该页面本可被激活而避免驱逐。\n   - 因此**乐观地激活**该重故障页面，使其与现有活跃页面竞争内存空间。\n\n### 影子条目压缩存储\n- 利用 xarray 条目的有限位宽（`BITS_PER_XA_VALUE`），通过位域拼接存储：\n  - 节点 ID（`NODES_SHIFT` 位）\n  - 内存控制组 ID（`MEM_CGROUP_ID_SHIFT` 位）\n  - 工作集标志（`WORKINGSET_SHIFT` 位）\n  - 驱逐时间戳（剩余位，必要时通过 `bucket_order` 降低精度）\n\n## 4. 依赖关系\n\n- **内存管理核心**：`<linux/mm.h>`, `<linux/mm_inline.h>` — 提供页框、LRU 列表、页表操作等基础支持。\n- **内存控制组**：`<linux/memcontrol.h>` — 支持按 cgroup 隔离工作集统计。\n- **页缓存与交换**：`<linux/pagemap.h>`, `<linux/swap.h>`, `<linux/shmem_fs.h>` — 处理文件页、匿名页、tmpfs 页的回收逻辑。\n- **xarray 数据结构**：用于高效存储和检索 shadow entries（隐含在 `pack_shadow` 的位操作中）。\n- **DAX 支持**：`<linux/dax.h>` — 确保直接访问持久内存设备的页面也能参与工作集检测。\n\n## 5. 使用场景\n\n- **内存压力下的页面回收**：当系统内存紧张触发 kswapd 或直接回收时，工作集检测机制指导选择最优的牺牲页面。\n- **工作集切换检测**：识别应用程序工作集的动态变化（如新任务启动、旧任务结束），快速淘汰过时缓存。\n- **防止颠簸（Thrashing）**：在活跃工作集大小接近或超过可用内存时，通过重故障距离预测避免频繁换入换出。\n- **混合工作负载优化**：同时处理文件缓存（page cache）和匿名内存（anonymous pages）的工作集，平衡二者内存分配。\n- **容器化环境**：结合 memcg，在多租户系统中为每个容器独立维护工作集状态，避免相互干扰。",
      "similarity": 0.5352557301521301,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/workingset.c",
          "start_line": 418,
          "end_line": 526,
          "content": [
            "bool workingset_test_recent(void *shadow, bool file, bool *workingset,",
            "\t\t\t\tbool flush)",
            "{",
            "\tstruct mem_cgroup *eviction_memcg;",
            "\tstruct lruvec *eviction_lruvec;",
            "\tunsigned long refault_distance;",
            "\tunsigned long workingset_size;",
            "\tunsigned long refault;",
            "\tint memcgid;",
            "\tstruct pglist_data *pgdat;",
            "\tunsigned long eviction;",
            "",
            "\trcu_read_lock();",
            "",
            "\tif (lru_gen_enabled()) {",
            "\t\tbool recent = lru_gen_test_recent(shadow, file,",
            "\t\t\t\t&eviction_lruvec, &eviction, workingset);",
            "",
            "\t\trcu_read_unlock();",
            "\t\treturn recent;",
            "\t}",
            "",
            "",
            "\tunpack_shadow(shadow, &memcgid, &pgdat, &eviction, workingset);",
            "\teviction <<= bucket_order;",
            "",
            "\t/*",
            "\t * Look up the memcg associated with the stored ID. It might",
            "\t * have been deleted since the folio's eviction.",
            "\t *",
            "\t * Note that in rare events the ID could have been recycled",
            "\t * for a new cgroup that refaults a shared folio. This is",
            "\t * impossible to tell from the available data. However, this",
            "\t * should be a rare and limited disturbance, and activations",
            "\t * are always speculative anyway. Ultimately, it's the aging",
            "\t * algorithm's job to shake out the minimum access frequency",
            "\t * for the active cache.",
            "\t *",
            "\t * XXX: On !CONFIG_MEMCG, this will always return NULL; it",
            "\t * would be better if the root_mem_cgroup existed in all",
            "\t * configurations instead.",
            "\t */",
            "\teviction_memcg = mem_cgroup_from_id(memcgid);",
            "\tif (!mem_cgroup_disabled() &&",
            "\t    (!eviction_memcg || !mem_cgroup_tryget(eviction_memcg))) {",
            "\t\trcu_read_unlock();",
            "\t\treturn false;",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * Flush stats (and potentially sleep) outside the RCU read section.",
            "\t *",
            "\t * Note that workingset_test_recent() itself might be called in RCU read",
            "\t * section (for e.g, in cachestat) - these callers need to skip flushing",
            "\t * stats (via the flush argument).",
            "\t *",
            "\t * XXX: With per-memcg flushing and thresholding, is ratelimiting",
            "\t * still needed here?",
            "\t */",
            "\tif (flush)",
            "\t\tmem_cgroup_flush_stats_ratelimited(eviction_memcg);",
            "",
            "\teviction_lruvec = mem_cgroup_lruvec(eviction_memcg, pgdat);",
            "\trefault = atomic_long_read(&eviction_lruvec->nonresident_age);",
            "",
            "\t/*",
            "\t * Calculate the refault distance",
            "\t *",
            "\t * The unsigned subtraction here gives an accurate distance",
            "\t * across nonresident_age overflows in most cases. There is a",
            "\t * special case: usually, shadow entries have a short lifetime",
            "\t * and are either refaulted or reclaimed along with the inode",
            "\t * before they get too old.  But it is not impossible for the",
            "\t * nonresident_age to lap a shadow entry in the field, which",
            "\t * can then result in a false small refault distance, leading",
            "\t * to a false activation should this old entry actually",
            "\t * refault again.  However, earlier kernels used to deactivate",
            "\t * unconditionally with *every* reclaim invocation for the",
            "\t * longest time, so the occasional inappropriate activation",
            "\t * leading to pressure on the active list is not a problem.",
            "\t */",
            "\trefault_distance = (refault - eviction) & EVICTION_MASK;",
            "",
            "\t/*",
            "\t * Compare the distance to the existing workingset size. We",
            "\t * don't activate pages that couldn't stay resident even if",
            "\t * all the memory was available to the workingset. Whether",
            "\t * workingset competition needs to consider anon or not depends",
            "\t * on having free swap space.",
            "\t */",
            "\tworkingset_size = lruvec_page_state(eviction_lruvec, NR_ACTIVE_FILE);",
            "\tif (!file) {",
            "\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_INACTIVE_FILE);",
            "\t}",
            "\tif (mem_cgroup_get_nr_swap_pages(eviction_memcg) > 0) {",
            "\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_ACTIVE_ANON);",
            "\t\tif (file) {",
            "\t\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_INACTIVE_ANON);",
            "\t\t}",
            "\t}",
            "",
            "\tmem_cgroup_put(eviction_memcg);",
            "\treturn refault_distance <= workingset_size;",
            "}"
          ],
          "function_name": "workingset_test_recent",
          "description": "实现工作集测试最近访问的判断逻辑，通过对比参考距离与当前工作集大小决定是否激活页面，支持内存组场景下的统计信息处理。",
          "similarity": 0.57264244556427
        },
        {
          "chunk_id": 4,
          "file_path": "mm/workingset.c",
          "start_line": 712,
          "end_line": 833,
          "content": [
            "static enum lru_status shadow_lru_isolate(struct list_head *item,",
            "\t\t\t\t\t  struct list_lru_one *lru,",
            "\t\t\t\t\t  spinlock_t *lru_lock,",
            "\t\t\t\t\t  void *arg) __must_hold(lru_lock)",
            "{",
            "\tstruct xa_node *node = container_of(item, struct xa_node, private_list);",
            "\tstruct address_space *mapping;",
            "\tint ret;",
            "",
            "\t/*",
            "\t * Page cache insertions and deletions synchronously maintain",
            "\t * the shadow node LRU under the i_pages lock and the",
            "\t * lru_lock.  Because the page cache tree is emptied before",
            "\t * the inode can be destroyed, holding the lru_lock pins any",
            "\t * address_space that has nodes on the LRU.",
            "\t *",
            "\t * We can then safely transition to the i_pages lock to",
            "\t * pin only the address_space of the particular node we want",
            "\t * to reclaim, take the node off-LRU, and drop the lru_lock.",
            "\t */",
            "",
            "\tmapping = container_of(node->array, struct address_space, i_pages);",
            "",
            "\t/* Coming from the list, invert the lock order */",
            "\tif (!xa_trylock(&mapping->i_pages)) {",
            "\t\tspin_unlock_irq(lru_lock);",
            "\t\tret = LRU_RETRY;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* For page cache we need to hold i_lock */",
            "\tif (mapping->host != NULL) {",
            "\t\tif (!spin_trylock(&mapping->host->i_lock)) {",
            "\t\t\txa_unlock(&mapping->i_pages);",
            "\t\t\tspin_unlock_irq(lru_lock);",
            "\t\t\tret = LRU_RETRY;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t}",
            "",
            "\tlist_lru_isolate(lru, item);",
            "\t__dec_node_page_state(virt_to_page(node), WORKINGSET_NODES);",
            "",
            "\tspin_unlock(lru_lock);",
            "",
            "\t/*",
            "\t * The nodes should only contain one or more shadow entries,",
            "\t * no pages, so we expect to be able to remove them all and",
            "\t * delete and free the empty node afterwards.",
            "\t */",
            "\tif (WARN_ON_ONCE(!node->nr_values))",
            "\t\tgoto out_invalid;",
            "\tif (WARN_ON_ONCE(node->count != node->nr_values))",
            "\t\tgoto out_invalid;",
            "\txa_delete_node(node, workingset_update_node);",
            "\t__inc_lruvec_kmem_state(node, WORKINGSET_NODERECLAIM);",
            "",
            "out_invalid:",
            "\txa_unlock_irq(&mapping->i_pages);",
            "\tif (mapping->host != NULL) {",
            "\t\tif (mapping_shrinkable(mapping))",
            "\t\t\tinode_add_lru(mapping->host);",
            "\t\tspin_unlock(&mapping->host->i_lock);",
            "\t}",
            "\tret = LRU_REMOVED_RETRY;",
            "out:",
            "\tcond_resched();",
            "\tspin_lock_irq(lru_lock);",
            "\treturn ret;",
            "}",
            "static unsigned long scan_shadow_nodes(struct shrinker *shrinker,",
            "\t\t\t\t       struct shrink_control *sc)",
            "{",
            "\t/* list_lru lock nests inside the IRQ-safe i_pages lock */",
            "\treturn list_lru_shrink_walk_irq(&shadow_nodes, sc, shadow_lru_isolate,",
            "\t\t\t\t\tNULL);",
            "}",
            "static int __init workingset_init(void)",
            "{",
            "\tstruct shrinker *workingset_shadow_shrinker;",
            "\tunsigned int timestamp_bits;",
            "\tunsigned int max_order;",
            "\tint ret = -ENOMEM;",
            "",
            "\tBUILD_BUG_ON(BITS_PER_LONG < EVICTION_SHIFT);",
            "\t/*",
            "\t * Calculate the eviction bucket size to cover the longest",
            "\t * actionable refault distance, which is currently half of",
            "\t * memory (totalram_pages/2). However, memory hotplug may add",
            "\t * some more pages at runtime, so keep working with up to",
            "\t * double the initial memory by using totalram_pages as-is.",
            "\t */",
            "\ttimestamp_bits = BITS_PER_LONG - EVICTION_SHIFT;",
            "\tmax_order = fls_long(totalram_pages() - 1);",
            "\tif (max_order > timestamp_bits)",
            "\t\tbucket_order = max_order - timestamp_bits;",
            "\tpr_info(\"workingset: timestamp_bits=%d max_order=%d bucket_order=%u\\n\",",
            "\t       timestamp_bits, max_order, bucket_order);",
            "",
            "\tworkingset_shadow_shrinker = shrinker_alloc(SHRINKER_NUMA_AWARE |",
            "\t\t\t\t\t\t    SHRINKER_MEMCG_AWARE,",
            "\t\t\t\t\t\t    \"mm-shadow\");",
            "\tif (!workingset_shadow_shrinker)",
            "\t\tgoto err;",
            "",
            "\tret = __list_lru_init(&shadow_nodes, true, &shadow_nodes_key,",
            "\t\t\t      workingset_shadow_shrinker);",
            "\tif (ret)",
            "\t\tgoto err_list_lru;",
            "",
            "\tworkingset_shadow_shrinker->count_objects = count_shadow_nodes;",
            "\tworkingset_shadow_shrinker->scan_objects = scan_shadow_nodes;",
            "\t/* ->count reports only fully expendable nodes */",
            "\tworkingset_shadow_shrinker->seeks = 0;",
            "",
            "\tshrinker_register(workingset_shadow_shrinker);",
            "\treturn 0;",
            "err_list_lru:",
            "\tshrinker_free(workingset_shadow_shrinker);",
            "err:",
            "\treturn ret;",
            "}"
          ],
          "function_name": "shadow_lru_isolate, scan_shadow_nodes, workingset_init",
          "description": "初始化工作集模块，注册收缩器管理影子节点，定义节点隔离和扫描机制，通过计算时间戳位宽确定桶阶参数以保证参考距离覆盖范围。",
          "similarity": 0.5372448563575745
        },
        {
          "chunk_id": 3,
          "file_path": "mm/workingset.c",
          "start_line": 537,
          "end_line": 689,
          "content": [
            "void workingset_refault(struct folio *folio, void *shadow)",
            "{",
            "\tbool file = folio_is_file_lru(folio);",
            "\tstruct pglist_data *pgdat;",
            "\tstruct mem_cgroup *memcg;",
            "\tstruct lruvec *lruvec;",
            "\tbool workingset;",
            "\tlong nr;",
            "",
            "\tif (lru_gen_enabled()) {",
            "\t\tlru_gen_refault(folio, shadow);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * The activation decision for this folio is made at the level",
            "\t * where the eviction occurred, as that is where the LRU order",
            "\t * during folio reclaim is being determined.",
            "\t *",
            "\t * However, the cgroup that will own the folio is the one that",
            "\t * is actually experiencing the refault event. Make sure the folio is",
            "\t * locked to guarantee folio_memcg() stability throughout.",
            "\t */",
            "\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\tnr = folio_nr_pages(folio);",
            "\tmemcg = folio_memcg(folio);",
            "\tpgdat = folio_pgdat(folio);",
            "\tlruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "",
            "\tmod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + file, nr);",
            "",
            "\tif (!workingset_test_recent(shadow, file, &workingset, true))",
            "\t\treturn;",
            "",
            "\tfolio_set_active(folio);",
            "\tworkingset_age_nonresident(lruvec, nr);",
            "\tmod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + file, nr);",
            "",
            "\t/* Folio was active prior to eviction */",
            "\tif (workingset) {",
            "\t\tfolio_set_workingset(folio);",
            "\t\t/*",
            "\t\t * XXX: Move to folio_add_lru() when it supports new vs",
            "\t\t * putback",
            "\t\t */",
            "\t\tlru_note_cost_refault(folio);",
            "\t\tmod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + file, nr);",
            "\t}",
            "}",
            "void workingset_activation(struct folio *folio)",
            "{",
            "\tstruct mem_cgroup *memcg;",
            "",
            "\trcu_read_lock();",
            "\t/*",
            "\t * Filter non-memcg pages here, e.g. unmap can call",
            "\t * mark_page_accessed() on VDSO pages.",
            "\t *",
            "\t * XXX: See workingset_refault() - this should return",
            "\t * root_mem_cgroup even for !CONFIG_MEMCG.",
            "\t */",
            "\tmemcg = folio_memcg_rcu(folio);",
            "\tif (!mem_cgroup_disabled() && !memcg)",
            "\t\tgoto out;",
            "\tworkingset_age_nonresident(folio_lruvec(folio), folio_nr_pages(folio));",
            "out:",
            "\trcu_read_unlock();",
            "}",
            "void workingset_update_node(struct xa_node *node)",
            "{",
            "\tstruct address_space *mapping;",
            "\tstruct page *page = virt_to_page(node);",
            "",
            "\t/*",
            "\t * Track non-empty nodes that contain only shadow entries;",
            "\t * unlink those that contain pages or are being freed.",
            "\t *",
            "\t * Avoid acquiring the list_lru lock when the nodes are",
            "\t * already where they should be. The list_empty() test is safe",
            "\t * as node->private_list is protected by the i_pages lock.",
            "\t */",
            "\tmapping = container_of(node->array, struct address_space, i_pages);",
            "\tlockdep_assert_held(&mapping->i_pages.xa_lock);",
            "",
            "\tif (node->count && node->count == node->nr_values) {",
            "\t\tif (list_empty(&node->private_list)) {",
            "\t\t\tlist_lru_add_obj(&shadow_nodes, &node->private_list);",
            "\t\t\t__inc_node_page_state(page, WORKINGSET_NODES);",
            "\t\t}",
            "\t} else {",
            "\t\tif (!list_empty(&node->private_list)) {",
            "\t\t\tlist_lru_del_obj(&shadow_nodes, &node->private_list);",
            "\t\t\t__dec_node_page_state(page, WORKINGSET_NODES);",
            "\t\t}",
            "\t}",
            "}",
            "static unsigned long count_shadow_nodes(struct shrinker *shrinker,",
            "\t\t\t\t\tstruct shrink_control *sc)",
            "{",
            "\tunsigned long max_nodes;",
            "\tunsigned long nodes;",
            "\tunsigned long pages;",
            "",
            "\tnodes = list_lru_shrink_count(&shadow_nodes, sc);",
            "\tif (!nodes)",
            "\t\treturn SHRINK_EMPTY;",
            "",
            "\t/*",
            "\t * Approximate a reasonable limit for the nodes",
            "\t * containing shadow entries. We don't need to keep more",
            "\t * shadow entries than possible pages on the active list,",
            "\t * since refault distances bigger than that are dismissed.",
            "\t *",
            "\t * The size of the active list converges toward 100% of",
            "\t * overall page cache as memory grows, with only a tiny",
            "\t * inactive list. Assume the total cache size for that.",
            "\t *",
            "\t * Nodes might be sparsely populated, with only one shadow",
            "\t * entry in the extreme case. Obviously, we cannot keep one",
            "\t * node for every eligible shadow entry, so compromise on a",
            "\t * worst-case density of 1/8th. Below that, not all eligible",
            "\t * refaults can be detected anymore.",
            "\t *",
            "\t * On 64-bit with 7 xa_nodes per page and 64 slots",
            "\t * each, this will reclaim shadow entries when they consume",
            "\t * ~1.8% of available memory:",
            "\t *",
            "\t * PAGE_SIZE / xa_nodes / node_entries * 8 / PAGE_SIZE",
            "\t */",
            "#ifdef CONFIG_MEMCG",
            "\tif (sc->memcg) {",
            "\t\tstruct lruvec *lruvec;",
            "\t\tint i;",
            "",
            "\t\tmem_cgroup_flush_stats_ratelimited(sc->memcg);",
            "\t\tlruvec = mem_cgroup_lruvec(sc->memcg, NODE_DATA(sc->nid));",
            "\t\tfor (pages = 0, i = 0; i < NR_LRU_LISTS; i++)",
            "\t\t\tpages += lruvec_page_state_local(lruvec,",
            "\t\t\t\t\t\t\t NR_LRU_BASE + i);",
            "\t\tpages += lruvec_page_state_local(",
            "\t\t\tlruvec, NR_SLAB_RECLAIMABLE_B) >> PAGE_SHIFT;",
            "\t\tpages += lruvec_page_state_local(",
            "\t\t\tlruvec, NR_SLAB_UNRECLAIMABLE_B) >> PAGE_SHIFT;",
            "\t} else",
            "#endif",
            "\t\tpages = node_present_pages(sc->nid);",
            "",
            "\tmax_nodes = pages >> (XA_CHUNK_SHIFT - 3);",
            "",
            "\tif (nodes <= max_nodes)",
            "\t\treturn 0;",
            "\treturn nodes - max_nodes;",
            "}"
          ],
          "function_name": "workingset_refault, workingset_activation, workingset_update_node, count_shadow_nodes",
          "description": "提供页面故障后的激活处理、节点更新及影子节点追踪功能，包含基于工作集状态更新节点计数器和触发页面重新激活的逻辑。",
          "similarity": 0.5346294641494751
        },
        {
          "chunk_id": 1,
          "file_path": "mm/workingset.c",
          "start_line": 209,
          "end_line": 314,
          "content": [
            "static void unpack_shadow(void *shadow, int *memcgidp, pg_data_t **pgdat,",
            "\t\t\t  unsigned long *evictionp, bool *workingsetp)",
            "{",
            "\tunsigned long entry = xa_to_value(shadow);",
            "\tint memcgid, nid;",
            "\tbool workingset;",
            "",
            "\tworkingset = entry & ((1UL << WORKINGSET_SHIFT) - 1);",
            "\tentry >>= WORKINGSET_SHIFT;",
            "\tnid = entry & ((1UL << NODES_SHIFT) - 1);",
            "\tentry >>= NODES_SHIFT;",
            "\tmemcgid = entry & ((1UL << MEM_CGROUP_ID_SHIFT) - 1);",
            "\tentry >>= MEM_CGROUP_ID_SHIFT;",
            "",
            "\t*memcgidp = memcgid;",
            "\t*pgdat = NODE_DATA(nid);",
            "\t*evictionp = entry;",
            "\t*workingsetp = workingset;",
            "}",
            "static bool lru_gen_test_recent(void *shadow, bool file, struct lruvec **lruvec,",
            "\t\t\t\tunsigned long *token, bool *workingset)",
            "{",
            "\tint memcg_id;",
            "\tunsigned long min_seq;",
            "\tstruct mem_cgroup *memcg;",
            "\tstruct pglist_data *pgdat;",
            "",
            "\tunpack_shadow(shadow, &memcg_id, &pgdat, token, workingset);",
            "",
            "\tmemcg = mem_cgroup_from_id(memcg_id);",
            "\t*lruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "",
            "\tmin_seq = READ_ONCE((*lruvec)->lrugen.min_seq[file]);",
            "\treturn (*token >> LRU_REFS_WIDTH) == (min_seq & (EVICTION_MASK >> LRU_REFS_WIDTH));",
            "}",
            "static void lru_gen_refault(struct folio *folio, void *shadow)",
            "{",
            "\tbool recent;",
            "\tint hist, tier, refs;",
            "\tbool workingset;",
            "\tunsigned long token;",
            "\tstruct lruvec *lruvec;",
            "\tstruct lru_gen_folio *lrugen;",
            "\tint type = folio_is_file_lru(folio);",
            "\tint delta = folio_nr_pages(folio);",
            "",
            "\trcu_read_lock();",
            "",
            "\trecent = lru_gen_test_recent(shadow, type, &lruvec, &token, &workingset);",
            "\tif (lruvec != folio_lruvec(folio))",
            "\t\tgoto unlock;",
            "",
            "\tmod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + type, delta);",
            "",
            "\tif (!recent)",
            "\t\tgoto unlock;",
            "",
            "\tlrugen = &lruvec->lrugen;",
            "",
            "\thist = lru_hist_from_seq(READ_ONCE(lrugen->min_seq[type]));",
            "\t/* see the comment in folio_lru_refs() */",
            "\trefs = (token & (BIT(LRU_REFS_WIDTH) - 1)) + workingset;",
            "\ttier = lru_tier_from_refs(refs);",
            "",
            "\tatomic_long_add(delta, &lrugen->refaulted[hist][type][tier]);",
            "\tmod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + type, delta);",
            "",
            "\t/*",
            "\t * Count the following two cases as stalls:",
            "\t * 1. For pages accessed through page tables, hotter pages pushed out",
            "\t *    hot pages which refaulted immediately.",
            "\t * 2. For pages accessed multiple times through file descriptors,",
            "\t *    they would have been protected by sort_folio().",
            "\t */",
            "\tif (lru_gen_in_fault() || refs >= BIT(LRU_REFS_WIDTH) - 1) {",
            "\t\tset_mask_bits(&folio->flags, 0, LRU_REFS_MASK | BIT(PG_workingset));",
            "\t\tmod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + type, delta);",
            "\t}",
            "unlock:",
            "\trcu_read_unlock();",
            "}",
            "static bool lru_gen_test_recent(void *shadow, bool file, struct lruvec **lruvec,",
            "\t\t\t\tunsigned long *token, bool *workingset)",
            "{",
            "\treturn false;",
            "}",
            "static void lru_gen_refault(struct folio *folio, void *shadow)",
            "{",
            "}",
            "void workingset_age_nonresident(struct lruvec *lruvec, unsigned long nr_pages)",
            "{",
            "\t/*",
            "\t * Reclaiming a cgroup means reclaiming all its children in a",
            "\t * round-robin fashion. That means that each cgroup has an LRU",
            "\t * order that is composed of the LRU orders of its child",
            "\t * cgroups; and every page has an LRU position not just in the",
            "\t * cgroup that owns it, but in all of that group's ancestors.",
            "\t *",
            "\t * So when the physical inactive list of a leaf cgroup ages,",
            "\t * the virtual inactive lists of all its parents, including",
            "\t * the root cgroup's, age as well.",
            "\t */",
            "\tdo {",
            "\t\tatomic_long_add(nr_pages, &lruvec->nonresident_age);",
            "\t} while ((lruvec = parent_lruvec(lruvec)));",
            "}"
          ],
          "function_name": "unpack_shadow, lru_gen_test_recent, lru_gen_refault, lru_gen_test_recent, lru_gen_refault, workingset_age_nonresident",
          "description": "包含解码影子条目、测试最近访问、处理页面故障的函数实现，但存在重复函数声明问题，实际功能涉及基于LRU生成的页面激活决策逻辑。",
          "similarity": 0.4367101192474365
        },
        {
          "chunk_id": 0,
          "file_path": "mm/workingset.c",
          "start_line": 1,
          "end_line": 208,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Workingset detection",
            " *",
            " * Copyright (C) 2013 Red Hat, Inc., Johannes Weiner",
            " */",
            "",
            "#include <linux/memcontrol.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/writeback.h>",
            "#include <linux/shmem_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/atomic.h>",
            "#include <linux/module.h>",
            "#include <linux/swap.h>",
            "#include <linux/dax.h>",
            "#include <linux/fs.h>",
            "#include <linux/mm.h>",
            "",
            "/*",
            " *\t\tDouble CLOCK lists",
            " *",
            " * Per node, two clock lists are maintained for file pages: the",
            " * inactive and the active list.  Freshly faulted pages start out at",
            " * the head of the inactive list and page reclaim scans pages from the",
            " * tail.  Pages that are accessed multiple times on the inactive list",
            " * are promoted to the active list, to protect them from reclaim,",
            " * whereas active pages are demoted to the inactive list when the",
            " * active list grows too big.",
            " *",
            " *   fault ------------------------+",
            " *                                 |",
            " *              +--------------+   |            +-------------+",
            " *   reclaim <- |   inactive   | <-+-- demotion |    active   | <--+",
            " *              +--------------+                +-------------+    |",
            " *                     |                                           |",
            " *                     +-------------- promotion ------------------+",
            " *",
            " *",
            " *\t\tAccess frequency and refault distance",
            " *",
            " * A workload is thrashing when its pages are frequently used but they",
            " * are evicted from the inactive list every time before another access",
            " * would have promoted them to the active list.",
            " *",
            " * In cases where the average access distance between thrashing pages",
            " * is bigger than the size of memory there is nothing that can be",
            " * done - the thrashing set could never fit into memory under any",
            " * circumstance.",
            " *",
            " * However, the average access distance could be bigger than the",
            " * inactive list, yet smaller than the size of memory.  In this case,",
            " * the set could fit into memory if it weren't for the currently",
            " * active pages - which may be used more, hopefully less frequently:",
            " *",
            " *      +-memory available to cache-+",
            " *      |                           |",
            " *      +-inactive------+-active----+",
            " *  a b | c d e f g h i | J K L M N |",
            " *      +---------------+-----------+",
            " *",
            " * It is prohibitively expensive to accurately track access frequency",
            " * of pages.  But a reasonable approximation can be made to measure",
            " * thrashing on the inactive list, after which refaulting pages can be",
            " * activated optimistically to compete with the existing active pages.",
            " *",
            " * Approximating inactive page access frequency - Observations:",
            " *",
            " * 1. When a page is accessed for the first time, it is added to the",
            " *    head of the inactive list, slides every existing inactive page",
            " *    towards the tail by one slot, and pushes the current tail page",
            " *    out of memory.",
            " *",
            " * 2. When a page is accessed for the second time, it is promoted to",
            " *    the active list, shrinking the inactive list by one slot.  This",
            " *    also slides all inactive pages that were faulted into the cache",
            " *    more recently than the activated page towards the tail of the",
            " *    inactive list.",
            " *",
            " * Thus:",
            " *",
            " * 1. The sum of evictions and activations between any two points in",
            " *    time indicate the minimum number of inactive pages accessed in",
            " *    between.",
            " *",
            " * 2. Moving one inactive page N page slots towards the tail of the",
            " *    list requires at least N inactive page accesses.",
            " *",
            " * Combining these:",
            " *",
            " * 1. When a page is finally evicted from memory, the number of",
            " *    inactive pages accessed while the page was in cache is at least",
            " *    the number of page slots on the inactive list.",
            " *",
            " * 2. In addition, measuring the sum of evictions and activations (E)",
            " *    at the time of a page's eviction, and comparing it to another",
            " *    reading (R) at the time the page faults back into memory tells",
            " *    the minimum number of accesses while the page was not cached.",
            " *    This is called the refault distance.",
            " *",
            " * Because the first access of the page was the fault and the second",
            " * access the refault, we combine the in-cache distance with the",
            " * out-of-cache distance to get the complete minimum access distance",
            " * of this page:",
            " *",
            " *      NR_inactive + (R - E)",
            " *",
            " * And knowing the minimum access distance of a page, we can easily",
            " * tell if the page would be able to stay in cache assuming all page",
            " * slots in the cache were available:",
            " *",
            " *   NR_inactive + (R - E) <= NR_inactive + NR_active",
            " *",
            " * If we have swap we should consider about NR_inactive_anon and",
            " * NR_active_anon, so for page cache and anonymous respectively:",
            " *",
            " *   NR_inactive_file + (R - E) <= NR_inactive_file + NR_active_file",
            " *   + NR_inactive_anon + NR_active_anon",
            " *",
            " *   NR_inactive_anon + (R - E) <= NR_inactive_anon + NR_active_anon",
            " *   + NR_inactive_file + NR_active_file",
            " *",
            " * Which can be further simplified to:",
            " *",
            " *   (R - E) <= NR_active_file + NR_inactive_anon + NR_active_anon",
            " *",
            " *   (R - E) <= NR_active_anon + NR_inactive_file + NR_active_file",
            " *",
            " * Put into words, the refault distance (out-of-cache) can be seen as",
            " * a deficit in inactive list space (in-cache).  If the inactive list",
            " * had (R - E) more page slots, the page would not have been evicted",
            " * in between accesses, but activated instead.  And on a full system,",
            " * the only thing eating into inactive list space is active pages.",
            " *",
            " *",
            " *\t\tRefaulting inactive pages",
            " *",
            " * All that is known about the active list is that the pages have been",
            " * accessed more than once in the past.  This means that at any given",
            " * time there is actually a good chance that pages on the active list",
            " * are no longer in active use.",
            " *",
            " * So when a refault distance of (R - E) is observed and there are at",
            " * least (R - E) pages in the userspace workingset, the refaulting page",
            " * is activated optimistically in the hope that (R - E) pages are actually",
            " * used less frequently than the refaulting page - or even not used at",
            " * all anymore.",
            " *",
            " * That means if inactive cache is refaulting with a suitable refault",
            " * distance, we assume the cache workingset is transitioning and put",
            " * pressure on the current workingset.",
            " *",
            " * If this is wrong and demotion kicks in, the pages which are truly",
            " * used more frequently will be reactivated while the less frequently",
            " * used once will be evicted from memory.",
            " *",
            " * But if this is right, the stale pages will be pushed out of memory",
            " * and the used pages get to stay in cache.",
            " *",
            " *\t\tRefaulting active pages",
            " *",
            " * If on the other hand the refaulting pages have recently been",
            " * deactivated, it means that the active list is no longer protecting",
            " * actively used cache from reclaim. The cache is NOT transitioning to",
            " * a different workingset; the existing workingset is thrashing in the",
            " * space allocated to the page cache.",
            " *",
            " *",
            " *\t\tImplementation",
            " *",
            " * For each node's LRU lists, a counter for inactive evictions and",
            " * activations is maintained (node->nonresident_age).",
            " *",
            " * On eviction, a snapshot of this counter (along with some bits to",
            " * identify the node) is stored in the now empty page cache",
            " * slot of the evicted page.  This is called a shadow entry.",
            " *",
            " * On cache misses for which there are shadow entries, an eligible",
            " * refault distance will immediately activate the refaulting page.",
            " */",
            "",
            "#define WORKINGSET_SHIFT 1",
            "#define EVICTION_SHIFT\t((BITS_PER_LONG - BITS_PER_XA_VALUE) +\t\\",
            "\t\t\t WORKINGSET_SHIFT + NODES_SHIFT + \\",
            "\t\t\t MEM_CGROUP_ID_SHIFT)",
            "#define EVICTION_MASK\t(~0UL >> EVICTION_SHIFT)",
            "",
            "/*",
            " * Eviction timestamps need to be able to cover the full range of",
            " * actionable refaults. However, bits are tight in the xarray",
            " * entry, and after storing the identifier for the lruvec there might",
            " * not be enough left to represent every single actionable refault. In",
            " * that case, we have to sacrifice granularity for distance, and group",
            " * evictions into coarser buckets by shaving off lower timestamp bits.",
            " */",
            "static unsigned int bucket_order __read_mostly;",
            "",
            "static void *pack_shadow(int memcgid, pg_data_t *pgdat, unsigned long eviction,",
            "\t\t\t bool workingset)",
            "{",
            "\teviction &= EVICTION_MASK;",
            "\teviction = (eviction << MEM_CGROUP_ID_SHIFT) | memcgid;",
            "\teviction = (eviction << NODES_SHIFT) | pgdat->node_id;",
            "\teviction = (eviction << WORKINGSET_SHIFT) | workingset;",
            "",
            "\treturn xa_mk_value(eviction);",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义影子条目打包函数，将内存组ID、节点ID、时间戳等信息编码到xarray值中，用于记录页面被驱逐时的状态信息。",
          "similarity": 0.3716713786125183
        }
      ]
    }
  ]
}