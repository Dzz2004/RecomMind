{
  "query": "文件系统一致性",
  "timestamp": "2025-12-26 00:23:14",
  "retrieved_files": [
    {
      "source_file": "kernel/compat.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:52:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `compat.c`\n\n---\n\n# compat.c 技术文档\n\n## 文件概述\n\n`compat.c` 是 Linux 内核中用于提供 32 位系统调用兼容性支持的核心文件，主要运行在 64 位内核上。该文件实现了将 32 位用户空间程序发出的系统调用转换为内核内部 64 位数据结构和接口所需的适配逻辑，确保 32 位应用程序能够在 64 位系统上正常运行。其核心功能包括信号处理、资源使用统计（rusage）、CPU 亲和性设置、位图操作以及信号集转换等兼容层封装。\n\n## 核心功能\n\n### 主要函数\n\n- `compat_sigprocmask`：32 位兼容版的 `sigprocmask` 系统调用，用于操作进程的信号掩码。\n- `put_compat_rusage`：将内核的 `struct rusage` 转换为 32 位兼容格式并复制到用户空间。\n- `compat_get_user_cpu_mask`：从用户空间读取 32 位 CPU 亲和性位图并转换为内核 `cpumask`。\n- `compat_sched_setaffinity` / `compat_sched_getaffinity`：32 位兼容的 CPU 亲和性设置与获取系统调用。\n- `get_compat_sigevent`：将 32 位 `sigevent` 结构从用户空间复制并转换为内核格式。\n- `compat_get_bitmap` / `compat_put_bitmap`：在 32 位用户空间与 64 位内核之间安全地传输位图数据。\n- `get_compat_sigset`：将 32 位信号集（`compat_sigset_t`）转换为内核内部的 `sigset_t`。\n\n### 关键数据结构\n\n- `compat_sigset_t`：32 位信号集表示。\n- `compat_rusage`：32 位资源使用统计结构。\n- `compat_sigevent`：32 位信号事件描述结构。\n- `compat_ulong_t`：32 位无符号长整型（通常为 `u32`）。\n\n## 关键实现\n\n### 信号掩码处理（`compat_sigprocmask`）\n\n该函数仅操作信号掩码的第一个字（32 位），通过 `compat_sig_setmask` 直接内存拷贝实现 `SIG_SETMASK` 行为。对于 `SIG_BLOCK` 和 `SIG_UNBLOCK`，则调用内核通用的 `sigaddsetmask` 和 `sigdelsetmask` 辅助函数。特别地，它会自动屏蔽 `SIGKILL` 和 `SIGSTOP`，因为这两个信号不可被阻塞。\n\n### 位图转换（`compat_get_bitmap` / `compat_put_bitmap`）\n\n由于 64 位内核中 `unsigned long` 为 64 位，而 32 位用户空间使用 32 位 `compat_ulong_t`，位图需进行高低位重组：\n- **读取**：每两个 32 位值组合成一个 64 位内核值（低位在前，高位在后）。\n- **写入**：将一个 64 位内核值拆分为两个 32 位值写回用户空间。\n使用 `user_read_access_begin`/`user_write_access_end` 配合 `unsafe_get/put_user` 实现高效、安全的批量访问。\n\n### 字节序处理（`get_compat_sigset`）\n\n在大端（Big-Endian）架构上，32 位信号集的高低 32 位在内存中的排列与小端不同，需显式重组为 64 位内核信号字。小端架构可直接内存拷贝。\n\n### CPU 亲和性兼容（`sched_setaffinity`/`getaffinity`）\n\n- **设置**：通过 `compat_get_user_cpu_mask` 将用户传入的 32 位位图转换为内核 `cpumask`，再调用通用 `sched_setaffinity`。\n- **获取**：先调用通用接口获取内核 `cpumask`，再通过 `compat_put_bitmap` 转换回 32 位格式返回给用户。返回长度为实际写入的字节数。\n\n### 资源使用统计（`put_compat_rusage`）\n\n逐字段将 64 位 `rusage` 中的时间（`tv_sec`/`tv_usec`）及其他统计值复制到 32 位结构体，确保字段对齐和截断安全，最后通过 `copy_to_user` 返回。\n\n## 依赖关系\n\n- **头文件依赖**：\n  - `<linux/compat.h>`：提供兼容层宏定义和类型（如 `COMPAT_SYSCALL_DEFINE`）。\n  - `<linux/uaccess.h>`：用户空间内存访问接口（`get_user`、`put_user` 等）。\n  - `<linux/sched.h>`、`<linux/cpumask.h>`：调度和 CPU 亲和性相关 API。\n  - `<linux/signal.h>`：信号处理核心接口。\n  - `<linux/posix-timers.h>`：`sigevent` 相关定义。\n- **内核模块依赖**：\n  - 调度子系统（`kernel/sched/`）：`sched_setaffinity`/`getaffinity` 实现。\n  - 信号子系统（`kernel/signal.c`）：信号掩码操作函数。\n  - 内存管理：`GFP_KERNEL` 内存分配。\n- **架构依赖**：通过 `__ARCH_WANT_SYS_SIGPROCMASK` 宏控制是否编译 `sigprocmask` 兼容实现，依赖 `__BIG_ENDIAN` 处理字节序差异。\n\n## 使用场景\n\n1. **32 位应用程序在 64 位内核上运行**：当 32 位 ELF 程序执行系统调用（如 `sigprocmask`、`sched_setaffinity`）时，内核通过此文件中的兼容层函数处理参数转换。\n2. **跨架构二进制兼容**：在 x86_64、ARM64 等支持 32 位兼容模式的架构上，该文件是运行旧版 32 位软件的关键组件。\n3. **系统调用拦截与转换**：安全模块（如 SELinux）或容器运行时可能依赖此兼容层正确解析 32 位进程的系统调用参数。\n4. **性能监控工具**：32 位 `getrusage` 调用通过 `put_compat_rusage` 获取资源统计信息。\n5. **实时/多线程应用**：32 位程序使用 `timer_create` 等 POSIX 定时器接口时，`sigevent` 结构通过 `get_compat_sigevent` 转换。",
      "similarity": 0.5968196392059326,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/compat.c",
          "start_line": 35,
          "end_line": 144,
          "content": [
            "static inline void compat_sig_setmask(sigset_t *blocked, compat_sigset_word set)",
            "{",
            "\tmemcpy(blocked->sig, &set, sizeof(set));",
            "}",
            "int put_compat_rusage(const struct rusage *r, struct compat_rusage __user *ru)",
            "{",
            "\tstruct compat_rusage r32;",
            "\tmemset(&r32, 0, sizeof(r32));",
            "\tr32.ru_utime.tv_sec = r->ru_utime.tv_sec;",
            "\tr32.ru_utime.tv_usec = r->ru_utime.tv_usec;",
            "\tr32.ru_stime.tv_sec = r->ru_stime.tv_sec;",
            "\tr32.ru_stime.tv_usec = r->ru_stime.tv_usec;",
            "\tr32.ru_maxrss = r->ru_maxrss;",
            "\tr32.ru_ixrss = r->ru_ixrss;",
            "\tr32.ru_idrss = r->ru_idrss;",
            "\tr32.ru_isrss = r->ru_isrss;",
            "\tr32.ru_minflt = r->ru_minflt;",
            "\tr32.ru_majflt = r->ru_majflt;",
            "\tr32.ru_nswap = r->ru_nswap;",
            "\tr32.ru_inblock = r->ru_inblock;",
            "\tr32.ru_oublock = r->ru_oublock;",
            "\tr32.ru_msgsnd = r->ru_msgsnd;",
            "\tr32.ru_msgrcv = r->ru_msgrcv;",
            "\tr32.ru_nsignals = r->ru_nsignals;",
            "\tr32.ru_nvcsw = r->ru_nvcsw;",
            "\tr32.ru_nivcsw = r->ru_nivcsw;",
            "\tif (copy_to_user(ru, &r32, sizeof(r32)))",
            "\t\treturn -EFAULT;",
            "\treturn 0;",
            "}",
            "static int compat_get_user_cpu_mask(compat_ulong_t __user *user_mask_ptr,",
            "\t\t\t\t    unsigned len, struct cpumask *new_mask)",
            "{",
            "\tunsigned long *k;",
            "",
            "\tif (len < cpumask_size())",
            "\t\tmemset(new_mask, 0, cpumask_size());",
            "\telse if (len > cpumask_size())",
            "\t\tlen = cpumask_size();",
            "",
            "\tk = cpumask_bits(new_mask);",
            "\treturn compat_get_bitmap(k, user_mask_ptr, len * 8);",
            "}",
            "int get_compat_sigevent(struct sigevent *event,",
            "\t\tconst struct compat_sigevent __user *u_event)",
            "{",
            "\tmemset(event, 0, sizeof(*event));",
            "\treturn (!access_ok(u_event, sizeof(*u_event)) ||",
            "\t\t__get_user(event->sigev_value.sival_int,",
            "\t\t\t&u_event->sigev_value.sival_int) ||",
            "\t\t__get_user(event->sigev_signo, &u_event->sigev_signo) ||",
            "\t\t__get_user(event->sigev_notify, &u_event->sigev_notify) ||",
            "\t\t__get_user(event->sigev_notify_thread_id,",
            "\t\t\t&u_event->sigev_notify_thread_id))",
            "\t\t? -EFAULT : 0;",
            "}",
            "long compat_get_bitmap(unsigned long *mask, const compat_ulong_t __user *umask,",
            "\t\t       unsigned long bitmap_size)",
            "{",
            "\tunsigned long nr_compat_longs;",
            "",
            "\t/* align bitmap up to nearest compat_long_t boundary */",
            "\tbitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);",
            "\tnr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);",
            "",
            "\tif (!user_read_access_begin(umask, bitmap_size / 8))",
            "\t\treturn -EFAULT;",
            "",
            "\twhile (nr_compat_longs > 1) {",
            "\t\tcompat_ulong_t l1, l2;",
            "\t\tunsafe_get_user(l1, umask++, Efault);",
            "\t\tunsafe_get_user(l2, umask++, Efault);",
            "\t\t*mask++ = ((unsigned long)l2 << BITS_PER_COMPAT_LONG) | l1;",
            "\t\tnr_compat_longs -= 2;",
            "\t}",
            "\tif (nr_compat_longs)",
            "\t\tunsafe_get_user(*mask, umask++, Efault);",
            "\tuser_read_access_end();",
            "\treturn 0;",
            "",
            "Efault:",
            "\tuser_read_access_end();",
            "\treturn -EFAULT;",
            "}",
            "long compat_put_bitmap(compat_ulong_t __user *umask, unsigned long *mask,",
            "\t\t       unsigned long bitmap_size)",
            "{",
            "\tunsigned long nr_compat_longs;",
            "",
            "\t/* align bitmap up to nearest compat_long_t boundary */",
            "\tbitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);",
            "\tnr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);",
            "",
            "\tif (!user_write_access_begin(umask, bitmap_size / 8))",
            "\t\treturn -EFAULT;",
            "",
            "\twhile (nr_compat_longs > 1) {",
            "\t\tunsigned long m = *mask++;",
            "\t\tunsafe_put_user((compat_ulong_t)m, umask++, Efault);",
            "\t\tunsafe_put_user(m >> BITS_PER_COMPAT_LONG, umask++, Efault);",
            "\t\tnr_compat_longs -= 2;",
            "\t}",
            "\tif (nr_compat_longs)",
            "\t\tunsafe_put_user((compat_ulong_t)*mask, umask++, Efault);",
            "\tuser_write_access_end();",
            "\treturn 0;",
            "Efault:",
            "\tuser_write_access_end();",
            "\treturn -EFAULT;",
            "}"
          ],
          "function_name": "compat_sig_setmask, put_compat_rusage, compat_get_user_cpu_mask, get_compat_sigevent, compat_get_bitmap, compat_put_bitmap",
          "description": "实现多个兼容性转换函数，包括将rusage结构体转换为32位格式、处理CPU掩码位图转换、提取sigevent事件信息以及双向转换信号集位图，核心功能是解决64位内核与32位用户态数据类型的差异。",
          "similarity": 0.5316287875175476
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/compat.c",
          "start_line": 249,
          "end_line": 270,
          "content": [
            "int",
            "get_compat_sigset(sigset_t *set, const compat_sigset_t __user *compat)",
            "{",
            "#ifdef __BIG_ENDIAN",
            "\tcompat_sigset_t v;",
            "\tif (copy_from_user(&v, compat, sizeof(compat_sigset_t)))",
            "\t\treturn -EFAULT;",
            "\tswitch (_NSIG_WORDS) {",
            "\tcase 4: set->sig[3] = v.sig[6] | (((long)v.sig[7]) << 32 );",
            "\t\tfallthrough;",
            "\tcase 3: set->sig[2] = v.sig[4] | (((long)v.sig[5]) << 32 );",
            "\t\tfallthrough;",
            "\tcase 2: set->sig[1] = v.sig[2] | (((long)v.sig[3]) << 32 );",
            "\t\tfallthrough;",
            "\tcase 1: set->sig[0] = v.sig[0] | (((long)v.sig[1]) << 32 );",
            "\t}",
            "#else",
            "\tif (copy_from_user(set, compat, sizeof(compat_sigset_t)))",
            "\t\treturn -EFAULT;",
            "#endif",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_compat_sigset",
          "description": "将用户态的兼容信号集（compat_sigset_t）转换为内核态sigset_t结构，在大端架构下通过字节序调整确保信号掩码的正确解析，实现信号集的跨字节序兼容转换。",
          "similarity": 0.4876707196235657
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/compat.c",
          "start_line": 1,
          "end_line": 34,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  linux/kernel/compat.c",
            " *",
            " *  Kernel compatibililty routines for e.g. 32 bit syscall support",
            " *  on 64 bit kernels.",
            " *",
            " *  Copyright (C) 2002-2003 Stephen Rothwell, IBM Corporation",
            " */",
            "",
            "#include <linux/linkage.h>",
            "#include <linux/compat.h>",
            "#include <linux/errno.h>",
            "#include <linux/time.h>",
            "#include <linux/signal.h>",
            "#include <linux/sched.h>\t/* for MAX_SCHEDULE_TIMEOUT */",
            "#include <linux/syscalls.h>",
            "#include <linux/unistd.h>",
            "#include <linux/security.h>",
            "#include <linux/export.h>",
            "#include <linux/migrate.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/times.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/gfp.h>",
            "",
            "#include <linux/uaccess.h>",
            "",
            "#ifdef __ARCH_WANT_SYS_SIGPROCMASK",
            "",
            "/*",
            " * sys_sigprocmask SIG_SETMASK sets the first (compat) word of the",
            " * blocked set of signals to the supplied signal set",
            " */"
          ],
          "function_name": null,
          "description": "此代码块包含兼容性支持所需的基础头文件和注释，定义了处理32位系统调用兼容性的框架，主要用于实现64位内核对32位进程的信号集操作等兼容逻辑。",
          "similarity": 0.47658440470695496
        }
      ]
    },
    {
      "source_file": "mm/compaction.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:44:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `compaction.c`\n\n---\n\n# compaction.c 技术文档\n\n## 1. 文件概述\n\n`compaction.c` 是 Linux 内核内存管理子系统中实现**内存压缩（Memory Compaction）**功能的核心文件。其主要目标是**减少外部碎片（external fragmentation）**，通过迁移可移动页面，将物理内存中的空闲页聚集形成连续的大块内存区域，从而满足高阶（high-order）内存分配请求（如透明大页 THP 或 HugeTLB）。该机制高度依赖页面迁移（page migration）来完成实际的页面移动工作。\n\n## 2. 核心功能\n\n### 主要函数\n- **`PageMovable()` / `__SetPageMovable()` / `__ClearPageMovable()`**: 管理页面的可移动性标记，用于标识页面是否可通过注册的 `movable_operations` 进行迁移。\n- **`defer_compaction()` / `compaction_deferred()` / `compaction_defer_reset()`**: 实现**压缩延迟（deferred compaction）**机制，避免在压缩失败后频繁重试，提升系统性能。\n- **`compaction_restarting()`**: 判断是否因多次失败后重新启动压缩流程。\n- **`isolation_suitable()`**: 检查指定 pageblock 是否适合进行页面隔离（用于迁移）。\n- **`reset_cached_positions()`**: 重置压缩控制结构中缓存的扫描位置（迁移源和空闲目标页的起始 PFN）。\n- **`release_free_list()`**: 将临时空闲页面列表中的页面释放回伙伴系统。\n- **`skip_offline_sections()` / `skip_offline_sections_reverse()`**: 在 SPARSEMEM 内存模型下跳过离线内存段。\n\n### 关键数据结构与宏\n- **`struct compact_control`**: 压缩操作的控制上下文（定义在 `internal.h` 中），包含扫描范围、迁移/空闲页面列表等。\n- **`COMPACTION_HPAGE_ORDER`**: 用于计算节点/区域“碎片分数（fragmentation score）”的参考阶数，通常为透明大页或 HugeTLB 的阶数。\n- **`COMPACT_MAX_DEFER_SHIFT`**: 压缩延迟的最大次数（64 次）。\n- **`zone->compact_*` 字段**: 包括 `compact_considered`, `compact_defer_shift`, `compact_order_failed`, `compact_cached_*_pfn` 等，用于跟踪压缩状态和优化扫描。\n\n## 3. 关键实现\n\n### 内存压缩流程\n1. **扫描阶段**：从区域两端向中间扫描：\n   - **迁移源扫描器（migrate scanner）**：从低地址向高地址扫描，寻找可迁移的页面。\n   - **空闲目标扫描器（free scanner）**：从高地址向低地址扫描，寻找空闲页面块。\n2. **页面隔离**：将找到的可迁移页面加入迁移列表，空闲页面加入空闲列表。\n3. **页面迁移**：调用 `migrate_pages()` 将迁移列表中的页面移动到空闲列表提供的目标位置。\n4. **结果处理**：迁移成功后，原位置变为空闲，可能形成更大的连续空闲块；失败则回滚。\n\n### 压缩延迟机制（Deferred Compaction）\n- 当压缩未能成功分配指定 `order` 的页面时，调用 `defer_compaction()` 增加延迟计数器 `compact_defer_shift`。\n- 后续分配请求会通过 `compaction_deferred()` 检查是否应跳过压缩（基于 `compact_considered` 计数和 `defer_limit = 1 << compact_defer_shift`）。\n- 若分配成功或预期成功，则通过 `compaction_defer_reset()` 重置延迟状态。\n- 当延迟达到最大值（`COMPACT_MAX_DEFER_SHIFT`）且考虑次数足够多时，`compaction_restarting()` 返回 true，强制重启完整压缩流程。\n\n### 碎片分数与主动压缩\n- 通过 `COMPACTION_HPAGE_ORDER` 定义的阶数评估区域的外部碎片程度。\n- 主动压缩（proactive compaction）定期（`HPAGE_FRAG_CHECK_INTERVAL_MSEC = 500ms`）检查碎片分数，并在需要时触发后台压缩。\n- `is_via_compact_memory()` 用于识别通过 `/proc/sys/vm/compact_memory` 等接口发起的全量压缩请求（`order = -1`）。\n\n### 页面可移动性管理\n- `__SetPageMovable()` 将 `movable_operations` 指针编码到 `page->mapping` 中，并设置 `PAGE_MAPPING_MOVABLE` 标志。\n- `PageMovable()` 验证页面是否被正确标记为可移动，并确保其操作集有效。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`, `internal.h`, `mm_inline.h` 提供的伙伴系统、页面分配/释放、zone 管理等基础功能。\n- **页面迁移**：紧密集成 `<linux/migrate.h>`，压缩的核心操作由 `migrate_pages()` 完成。\n- **内存模型**：针对 `CONFIG_SPARSEMEM` 提供离线内存段跳过逻辑。\n- **大页支持**：根据 `CONFIG_TRANSPARENT_HUGEPAGE` 或 `CONFIG_HUGETLBFS` 确定 `COMPACTION_HPAGE_ORDER`。\n- **调试与追踪**：使用 `kasan.h`, `page_owner.h` 进行调试；通过 `trace/events/compaction.h` 提供 ftrace 事件。\n- **系统调用与接口**：通过 sysctl (`/proc/sys/vm/`) 和 sysfs (`/sys/devices/system/node/`) 暴露用户空间控制接口。\n- **进程与调度**：使用 `kthread.h`, `freezer.h` 管理后台压缩线程；`psi.h` 用于压力状态监控。\n\n## 5. 使用场景\n\n1. **高阶内存分配失败时**：当 `alloc_pages()` 请求高阶页面（如 order ≥ 3）失败时，内核会尝试同步内存压缩以满足请求。\n2. **透明大页（THP）分配**：THP 需要连续的 2MB 物理内存，压缩是满足此类请求的关键机制。\n3. **主动后台压缩**：通过 `vm.compaction_proactiveness` sysctl 参数配置，内核定期评估内存碎片并主动压缩，预防分配延迟。\n4. **用户空间触发**：管理员可通过写入 `/proc/sys/vm/compact_memory` 或特定 NUMA 节点的 `compact` sysfs 文件，手动触发全量内存压缩。\n5. **CMA（Contiguous Memory Allocator）**：在 CMA 区域分配大块连续内存前，使用压缩机制迁移占用页面以腾出空间（需 `CONFIG_CMA`）。",
      "similarity": 0.572661280632019,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/compaction.c",
          "start_line": 34,
          "end_line": 141,
          "content": [
            "static inline void count_compact_event(enum vm_event_item item)",
            "{",
            "\tcount_vm_event(item);",
            "}",
            "static inline void count_compact_events(enum vm_event_item item, long delta)",
            "{",
            "\tcount_vm_events(item, delta);",
            "}",
            "static inline bool is_via_compact_memory(int order)",
            "{",
            "\treturn order == -1;",
            "}",
            "static inline bool is_via_compact_memory(int order) { return false; }",
            "static unsigned long release_free_list(struct list_head *freepages)",
            "{",
            "\tint order;",
            "\tunsigned long high_pfn = 0;",
            "",
            "\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {",
            "\t\tstruct page *page, *next;",
            "",
            "\t\tlist_for_each_entry_safe(page, next, &freepages[order], lru) {",
            "\t\t\tunsigned long pfn = page_to_pfn(page);",
            "",
            "\t\t\tlist_del(&page->lru);",
            "\t\t\t/*",
            "\t\t\t * Convert free pages into post allocation pages, so",
            "\t\t\t * that we can free them via __free_page.",
            "\t\t\t */",
            "\t\t\tmark_allocated(page, order, __GFP_MOVABLE);",
            "\t\t\t__free_pages(page, order);",
            "\t\t\tif (pfn > high_pfn)",
            "\t\t\t\thigh_pfn = pfn;",
            "\t\t}",
            "\t}",
            "\treturn high_pfn;",
            "}",
            "bool PageMovable(struct page *page)",
            "{",
            "\tconst struct movable_operations *mops;",
            "",
            "\tVM_BUG_ON_PAGE(!PageLocked(page), page);",
            "\tif (!__PageMovable(page))",
            "\t\treturn false;",
            "",
            "\tmops = page_movable_ops(page);",
            "\tif (mops)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "void __SetPageMovable(struct page *page, const struct movable_operations *mops)",
            "{",
            "\tVM_BUG_ON_PAGE(!PageLocked(page), page);",
            "\tVM_BUG_ON_PAGE((unsigned long)mops & PAGE_MAPPING_MOVABLE, page);",
            "\tpage->mapping = (void *)((unsigned long)mops | PAGE_MAPPING_MOVABLE);",
            "}",
            "void __ClearPageMovable(struct page *page)",
            "{",
            "\tVM_BUG_ON_PAGE(!PageMovable(page), page);",
            "\t/*",
            "\t * This page still has the type of a movable page, but it's",
            "\t * actually not movable any more.",
            "\t */",
            "\tpage->mapping = (void *)PAGE_MAPPING_MOVABLE;",
            "}",
            "static void defer_compaction(struct zone *zone, int order)",
            "{",
            "\tzone->compact_considered = 0;",
            "\tzone->compact_defer_shift++;",
            "",
            "\tif (order < zone->compact_order_failed)",
            "\t\tzone->compact_order_failed = order;",
            "",
            "\tif (zone->compact_defer_shift > COMPACT_MAX_DEFER_SHIFT)",
            "\t\tzone->compact_defer_shift = COMPACT_MAX_DEFER_SHIFT;",
            "",
            "\ttrace_mm_compaction_defer_compaction(zone, order);",
            "}",
            "static bool compaction_deferred(struct zone *zone, int order)",
            "{",
            "\tunsigned long defer_limit = 1UL << zone->compact_defer_shift;",
            "",
            "\tif (order < zone->compact_order_failed)",
            "\t\treturn false;",
            "",
            "\t/* Avoid possible overflow */",
            "\tif (++zone->compact_considered >= defer_limit) {",
            "\t\tzone->compact_considered = defer_limit;",
            "\t\treturn false;",
            "\t}",
            "",
            "\ttrace_mm_compaction_deferred(zone, order);",
            "",
            "\treturn true;",
            "}",
            "void compaction_defer_reset(struct zone *zone, int order,",
            "\t\tbool alloc_success)",
            "{",
            "\tif (alloc_success) {",
            "\t\tzone->compact_considered = 0;",
            "\t\tzone->compact_defer_shift = 0;",
            "\t}",
            "\tif (order >= zone->compact_order_failed)",
            "\t\tzone->compact_order_failed = order + 1;",
            "",
            "\ttrace_mm_compaction_defer_reset(zone, order);",
            "}"
          ],
          "function_name": "count_compact_event, count_compact_events, is_via_compact_memory, is_via_compact_memory, release_free_list, PageMovable, __SetPageMovable, __ClearPageMovable, defer_compaction, compaction_deferred, compaction_defer_reset",
          "description": "提供内存压缩事件计数、页面可移动性检测、空闲列表释放、延迟压缩逻辑及页面块隔离辅助函数，包含重复定义可能导致冲突",
          "similarity": 0.5534054636955261
        },
        {
          "chunk_id": 14,
          "file_path": "mm/compaction.c",
          "start_line": 2489,
          "end_line": 2740,
          "content": [
            "static enum compact_result",
            "compaction_suit_allocation_order(struct zone *zone, unsigned int order,",
            "\t\t\t\t int highest_zoneidx, unsigned int alloc_flags)",
            "{",
            "\tunsigned long watermark;",
            "",
            "\twatermark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK);",
            "\tif (zone_watermark_ok(zone, order, watermark, highest_zoneidx,",
            "\t\t\t      alloc_flags))",
            "\t\treturn COMPACT_SUCCESS;",
            "",
            "\tif (!compaction_suitable(zone, order, highest_zoneidx))",
            "\t\treturn COMPACT_SKIPPED;",
            "",
            "\treturn COMPACT_CONTINUE;",
            "}",
            "static enum compact_result",
            "compact_zone(struct compact_control *cc, struct capture_control *capc)",
            "{",
            "\tenum compact_result ret;",
            "\tunsigned long start_pfn = cc->zone->zone_start_pfn;",
            "\tunsigned long end_pfn = zone_end_pfn(cc->zone);",
            "\tunsigned long last_migrated_pfn;",
            "\tconst bool sync = cc->mode != MIGRATE_ASYNC;",
            "\tbool update_cached;",
            "\tunsigned int nr_succeeded = 0, nr_migratepages;",
            "\tint order;",
            "",
            "\t/*",
            "\t * These counters track activities during zone compaction.  Initialize",
            "\t * them before compacting a new zone.",
            "\t */",
            "\tcc->total_migrate_scanned = 0;",
            "\tcc->total_free_scanned = 0;",
            "\tcc->nr_migratepages = 0;",
            "\tcc->nr_freepages = 0;",
            "\tfor (order = 0; order < NR_PAGE_ORDERS; order++)",
            "\t\tINIT_LIST_HEAD(&cc->freepages[order]);",
            "\tINIT_LIST_HEAD(&cc->migratepages);",
            "",
            "\tcc->migratetype = gfp_migratetype(cc->gfp_mask);",
            "",
            "\tif (!is_via_compact_memory(cc->order)) {",
            "\t\tret = compaction_suit_allocation_order(cc->zone, cc->order,",
            "\t\t\t\t\t\t       cc->highest_zoneidx,",
            "\t\t\t\t\t\t       cc->alloc_flags);",
            "\t\tif (ret != COMPACT_CONTINUE)",
            "\t\t\treturn ret;",
            "\t}",
            "",
            "\t/*",
            "\t * Clear pageblock skip if there were failures recently and compaction",
            "\t * is about to be retried after being deferred.",
            "\t */",
            "\tif (compaction_restarting(cc->zone, cc->order))",
            "\t\t__reset_isolation_suitable(cc->zone);",
            "",
            "\t/*",
            "\t * Setup to move all movable pages to the end of the zone. Used cached",
            "\t * information on where the scanners should start (unless we explicitly",
            "\t * want to compact the whole zone), but check that it is initialised",
            "\t * by ensuring the values are within zone boundaries.",
            "\t */",
            "\tcc->fast_start_pfn = 0;",
            "\tif (cc->whole_zone) {",
            "\t\tcc->migrate_pfn = start_pfn;",
            "\t\tcc->free_pfn = pageblock_start_pfn(end_pfn - 1);",
            "\t} else {",
            "\t\tcc->migrate_pfn = cc->zone->compact_cached_migrate_pfn[sync];",
            "\t\tcc->free_pfn = cc->zone->compact_cached_free_pfn;",
            "\t\tif (cc->free_pfn < start_pfn || cc->free_pfn >= end_pfn) {",
            "\t\t\tcc->free_pfn = pageblock_start_pfn(end_pfn - 1);",
            "\t\t\tcc->zone->compact_cached_free_pfn = cc->free_pfn;",
            "\t\t}",
            "\t\tif (cc->migrate_pfn < start_pfn || cc->migrate_pfn >= end_pfn) {",
            "\t\t\tcc->migrate_pfn = start_pfn;",
            "\t\t\tcc->zone->compact_cached_migrate_pfn[0] = cc->migrate_pfn;",
            "\t\t\tcc->zone->compact_cached_migrate_pfn[1] = cc->migrate_pfn;",
            "\t\t}",
            "",
            "\t\tif (cc->migrate_pfn <= cc->zone->compact_init_migrate_pfn)",
            "\t\t\tcc->whole_zone = true;",
            "\t}",
            "",
            "\tlast_migrated_pfn = 0;",
            "",
            "\t/*",
            "\t * Migrate has separate cached PFNs for ASYNC and SYNC* migration on",
            "\t * the basis that some migrations will fail in ASYNC mode. However,",
            "\t * if the cached PFNs match and pageblocks are skipped due to having",
            "\t * no isolation candidates, then the sync state does not matter.",
            "\t * Until a pageblock with isolation candidates is found, keep the",
            "\t * cached PFNs in sync to avoid revisiting the same blocks.",
            "\t */",
            "\tupdate_cached = !sync &&",
            "\t\tcc->zone->compact_cached_migrate_pfn[0] == cc->zone->compact_cached_migrate_pfn[1];",
            "",
            "\ttrace_mm_compaction_begin(cc, start_pfn, end_pfn, sync);",
            "",
            "\t/* lru_add_drain_all could be expensive with involving other CPUs */",
            "\tlru_add_drain();",
            "",
            "\twhile ((ret = compact_finished(cc)) == COMPACT_CONTINUE) {",
            "\t\tint err;",
            "\t\tunsigned long iteration_start_pfn = cc->migrate_pfn;",
            "",
            "\t\t/*",
            "\t\t * Avoid multiple rescans of the same pageblock which can",
            "\t\t * happen if a page cannot be isolated (dirty/writeback in",
            "\t\t * async mode) or if the migrated pages are being allocated",
            "\t\t * before the pageblock is cleared.  The first rescan will",
            "\t\t * capture the entire pageblock for migration. If it fails,",
            "\t\t * it'll be marked skip and scanning will proceed as normal.",
            "\t\t */",
            "\t\tcc->finish_pageblock = false;",
            "\t\tif (pageblock_start_pfn(last_migrated_pfn) ==",
            "\t\t    pageblock_start_pfn(iteration_start_pfn)) {",
            "\t\t\tcc->finish_pageblock = true;",
            "\t\t}",
            "",
            "rescan:",
            "\t\tswitch (isolate_migratepages(cc)) {",
            "\t\tcase ISOLATE_ABORT:",
            "\t\t\tret = COMPACT_CONTENDED;",
            "\t\t\tputback_movable_pages(&cc->migratepages);",
            "\t\t\tcc->nr_migratepages = 0;",
            "\t\t\tgoto out;",
            "\t\tcase ISOLATE_NONE:",
            "\t\t\tif (update_cached) {",
            "\t\t\t\tcc->zone->compact_cached_migrate_pfn[1] =",
            "\t\t\t\t\tcc->zone->compact_cached_migrate_pfn[0];",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * We haven't isolated and migrated anything, but",
            "\t\t\t * there might still be unflushed migrations from",
            "\t\t\t * previous cc->order aligned block.",
            "\t\t\t */",
            "\t\t\tgoto check_drain;",
            "\t\tcase ISOLATE_SUCCESS:",
            "\t\t\tupdate_cached = false;",
            "\t\t\tlast_migrated_pfn = max(cc->zone->zone_start_pfn,",
            "\t\t\t\tpageblock_start_pfn(cc->migrate_pfn - 1));",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Record the number of pages to migrate since the",
            "\t\t * compaction_alloc/free() will update cc->nr_migratepages",
            "\t\t * properly.",
            "\t\t */",
            "\t\tnr_migratepages = cc->nr_migratepages;",
            "\t\terr = migrate_pages(&cc->migratepages, compaction_alloc,",
            "\t\t\t\tcompaction_free, (unsigned long)cc, cc->mode,",
            "\t\t\t\tMR_COMPACTION, &nr_succeeded);",
            "",
            "\t\ttrace_mm_compaction_migratepages(nr_migratepages, nr_succeeded);",
            "",
            "\t\t/* All pages were either migrated or will be released */",
            "\t\tcc->nr_migratepages = 0;",
            "\t\tif (err) {",
            "\t\t\tputback_movable_pages(&cc->migratepages);",
            "\t\t\t/*",
            "\t\t\t * migrate_pages() may return -ENOMEM when scanners meet",
            "\t\t\t * and we want compact_finished() to detect it",
            "\t\t\t */",
            "\t\t\tif (err == -ENOMEM && !compact_scanners_met(cc)) {",
            "\t\t\t\tret = COMPACT_CONTENDED;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t\t/*",
            "\t\t\t * If an ASYNC or SYNC_LIGHT fails to migrate a page",
            "\t\t\t * within the pageblock_order-aligned block and",
            "\t\t\t * fast_find_migrateblock may be used then scan the",
            "\t\t\t * remainder of the pageblock. This will mark the",
            "\t\t\t * pageblock \"skip\" to avoid rescanning in the near",
            "\t\t\t * future. This will isolate more pages than necessary",
            "\t\t\t * for the request but avoid loops due to",
            "\t\t\t * fast_find_migrateblock revisiting blocks that were",
            "\t\t\t * recently partially scanned.",
            "\t\t\t */",
            "\t\t\tif (!pageblock_aligned(cc->migrate_pfn) &&",
            "\t\t\t    !cc->ignore_skip_hint && !cc->finish_pageblock &&",
            "\t\t\t    (cc->mode < MIGRATE_SYNC)) {",
            "\t\t\t\tcc->finish_pageblock = true;",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * Draining pcplists does not help THP if",
            "\t\t\t\t * any page failed to migrate. Even after",
            "\t\t\t\t * drain, the pageblock will not be free.",
            "\t\t\t\t */",
            "\t\t\t\tif (cc->order == COMPACTION_HPAGE_ORDER)",
            "\t\t\t\t\tlast_migrated_pfn = 0;",
            "",
            "\t\t\t\tgoto rescan;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* Stop if a page has been captured */",
            "\t\tif (capc && capc->page) {",
            "\t\t\tret = COMPACT_SUCCESS;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "check_drain:",
            "\t\t/*",
            "\t\t * Has the migration scanner moved away from the previous",
            "\t\t * cc->order aligned block where we migrated from? If yes,",
            "\t\t * flush the pages that were freed, so that they can merge and",
            "\t\t * compact_finished() can detect immediately if allocation",
            "\t\t * would succeed.",
            "\t\t */",
            "\t\tif (cc->order > 0 && last_migrated_pfn) {",
            "\t\t\tunsigned long current_block_start =",
            "\t\t\t\tblock_start_pfn(cc->migrate_pfn, cc->order);",
            "",
            "\t\t\tif (last_migrated_pfn < current_block_start) {",
            "\t\t\t\tlru_add_drain_cpu_zone(cc->zone);",
            "\t\t\t\t/* No more flushing until we migrate again */",
            "\t\t\t\tlast_migrated_pfn = 0;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "out:",
            "\t/*",
            "\t * Release free pages and update where the free scanner should restart,",
            "\t * so we don't leave any returned pages behind in the next attempt.",
            "\t */",
            "\tif (cc->nr_freepages > 0) {",
            "\t\tunsigned long free_pfn = release_free_list(cc->freepages);",
            "",
            "\t\tcc->nr_freepages = 0;",
            "\t\tVM_BUG_ON(free_pfn == 0);",
            "\t\t/* The cached pfn is always the first in a pageblock */",
            "\t\tfree_pfn = pageblock_start_pfn(free_pfn);",
            "\t\t/*",
            "\t\t * Only go back, not forward. The cached pfn might have been",
            "\t\t * already reset to zone end in compact_finished()",
            "\t\t */",
            "\t\tif (free_pfn > cc->zone->compact_cached_free_pfn)",
            "\t\t\tcc->zone->compact_cached_free_pfn = free_pfn;",
            "\t}",
            "",
            "\tcount_compact_events(COMPACTMIGRATE_SCANNED, cc->total_migrate_scanned);",
            "\tcount_compact_events(COMPACTFREE_SCANNED, cc->total_free_scanned);",
            "",
            "\ttrace_mm_compaction_end(cc, start_pfn, end_pfn, sync, ret);",
            "",
            "\tVM_BUG_ON(!list_empty(&cc->migratepages));",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "compaction_suit_allocation_order, compact_zone",
          "description": "compaction_suit_allocation_order检查分配顺序兼容性，compact_zone执行核心压缩流程，迁移页面并调整缓存扫描起点，处理页块扫描和迁移结果",
          "similarity": 0.5364213585853577
        },
        {
          "chunk_id": 2,
          "file_path": "mm/compaction.c",
          "start_line": 209,
          "end_line": 347,
          "content": [
            "static bool compaction_restarting(struct zone *zone, int order)",
            "{",
            "\tif (order < zone->compact_order_failed)",
            "\t\treturn false;",
            "",
            "\treturn zone->compact_defer_shift == COMPACT_MAX_DEFER_SHIFT &&",
            "\t\tzone->compact_considered >= 1UL << zone->compact_defer_shift;",
            "}",
            "static inline bool isolation_suitable(struct compact_control *cc,",
            "\t\t\t\t\tstruct page *page)",
            "{",
            "\tif (cc->ignore_skip_hint)",
            "\t\treturn true;",
            "",
            "\treturn !get_pageblock_skip(page);",
            "}",
            "static void reset_cached_positions(struct zone *zone)",
            "{",
            "\tzone->compact_cached_migrate_pfn[0] = zone->zone_start_pfn;",
            "\tzone->compact_cached_migrate_pfn[1] = zone->zone_start_pfn;",
            "\tzone->compact_cached_free_pfn =",
            "\t\t\t\tpageblock_start_pfn(zone_end_pfn(zone) - 1);",
            "}",
            "static unsigned long skip_offline_sections(unsigned long start_pfn)",
            "{",
            "\tunsigned long start_nr = pfn_to_section_nr(start_pfn);",
            "",
            "\tif (online_section_nr(start_nr))",
            "\t\treturn 0;",
            "",
            "\twhile (++start_nr <= __highest_present_section_nr) {",
            "\t\tif (online_section_nr(start_nr))",
            "\t\t\treturn section_nr_to_pfn(start_nr);",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static unsigned long skip_offline_sections_reverse(unsigned long start_pfn)",
            "{",
            "\tunsigned long start_nr = pfn_to_section_nr(start_pfn);",
            "",
            "\tif (!start_nr || online_section_nr(start_nr))",
            "\t\treturn 0;",
            "",
            "\twhile (start_nr-- > 0) {",
            "\t\tif (online_section_nr(start_nr))",
            "\t\t\treturn section_nr_to_pfn(start_nr) + PAGES_PER_SECTION;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static unsigned long skip_offline_sections(unsigned long start_pfn)",
            "{",
            "\treturn 0;",
            "}",
            "static unsigned long skip_offline_sections_reverse(unsigned long start_pfn)",
            "{",
            "\treturn 0;",
            "}",
            "static bool pageblock_skip_persistent(struct page *page)",
            "{",
            "\tif (!PageCompound(page))",
            "\t\treturn false;",
            "",
            "\tpage = compound_head(page);",
            "",
            "\tif (compound_order(page) >= pageblock_order)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "static bool",
            "__reset_isolation_pfn(struct zone *zone, unsigned long pfn, bool check_source,",
            "\t\t\t\t\t\t\tbool check_target)",
            "{",
            "\tstruct page *page = pfn_to_online_page(pfn);",
            "\tstruct page *block_page;",
            "\tstruct page *end_page;",
            "\tunsigned long block_pfn;",
            "",
            "\tif (!page)",
            "\t\treturn false;",
            "\tif (zone != page_zone(page))",
            "\t\treturn false;",
            "\tif (pageblock_skip_persistent(page))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * If skip is already cleared do no further checking once the",
            "\t * restart points have been set.",
            "\t */",
            "\tif (check_source && check_target && !get_pageblock_skip(page))",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * If clearing skip for the target scanner, do not select a",
            "\t * non-movable pageblock as the starting point.",
            "\t */",
            "\tif (!check_source && check_target &&",
            "\t    get_pageblock_migratetype(page) != MIGRATE_MOVABLE)",
            "\t\treturn false;",
            "",
            "\t/* Ensure the start of the pageblock or zone is online and valid */",
            "\tblock_pfn = pageblock_start_pfn(pfn);",
            "\tblock_pfn = max(block_pfn, zone->zone_start_pfn);",
            "\tblock_page = pfn_to_online_page(block_pfn);",
            "\tif (block_page) {",
            "\t\tpage = block_page;",
            "\t\tpfn = block_pfn;",
            "\t}",
            "",
            "\t/* Ensure the end of the pageblock or zone is online and valid */",
            "\tblock_pfn = pageblock_end_pfn(pfn) - 1;",
            "\tblock_pfn = min(block_pfn, zone_end_pfn(zone) - 1);",
            "\tend_page = pfn_to_online_page(block_pfn);",
            "\tif (!end_page)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Only clear the hint if a sample indicates there is either a",
            "\t * free page or an LRU page in the block. One or other condition",
            "\t * is necessary for the block to be a migration source/target.",
            "\t */",
            "\tdo {",
            "\t\tif (check_source && PageLRU(page)) {",
            "\t\t\tclear_pageblock_skip(page);",
            "\t\t\treturn true;",
            "\t\t}",
            "",
            "\t\tif (check_target && PageBuddy(page)) {",
            "\t\t\tclear_pageblock_skip(page);",
            "\t\t\treturn true;",
            "\t\t}",
            "",
            "\t\tpage += (1 << PAGE_ALLOC_COSTLY_ORDER);",
            "\t} while (page <= end_page);",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "compaction_restarting, isolation_suitable, reset_cached_positions, skip_offline_sections, skip_offline_sections_reverse, skip_offline_sections, skip_offline_sections_reverse, pageblock_skip_persistent, __reset_isolation_pfn",
          "description": "实现压缩重启条件判定、隔离兼容性检查、跳过离线section、页面块持久性跳过标记及隔离信息更新功能",
          "similarity": 0.5273149013519287
        },
        {
          "chunk_id": 0,
          "file_path": "mm/compaction.c",
          "start_line": 1,
          "end_line": 33,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * linux/mm/compaction.c",
            " *",
            " * Memory compaction for the reduction of external fragmentation. Note that",
            " * this heavily depends upon page migration to do all the real heavy",
            " * lifting",
            " *",
            " * Copyright IBM Corp. 2007-2010 Mel Gorman <mel@csn.ul.ie>",
            " */",
            "#include <linux/cpu.h>",
            "#include <linux/swap.h>",
            "#include <linux/migrate.h>",
            "#include <linux/compaction.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/sysfs.h>",
            "#include <linux/page-isolation.h>",
            "#include <linux/kasan.h>",
            "#include <linux/kthread.h>",
            "#include <linux/freezer.h>",
            "#include <linux/page_owner.h>",
            "#include <linux/psi.h>",
            "#include \"internal.h\"",
            "",
            "#ifdef CONFIG_COMPACTION",
            "/*",
            " * Fragmentation score check interval for proactive compaction purposes.",
            " */",
            "#define HPAGE_FRAG_CHECK_INTERVAL_MSEC\t(500)",
            ""
          ],
          "function_name": null,
          "description": "声明内存压缩模块的头文件，包含相关依赖和定义，启用CONFIG_COMPACTION配置项，定义HPAGE_FRAG_CHECK_INTERVAL_MSEC常量用于主动压缩时间间隔",
          "similarity": 0.5266502499580383
        },
        {
          "chunk_id": 15,
          "file_path": "mm/compaction.c",
          "start_line": 2743,
          "end_line": 2857,
          "content": [
            "static enum compact_result compact_zone_order(struct zone *zone, int order,",
            "\t\tgfp_t gfp_mask, enum compact_priority prio,",
            "\t\tunsigned int alloc_flags, int highest_zoneidx,",
            "\t\tstruct page **capture)",
            "{",
            "\tenum compact_result ret;",
            "\tstruct compact_control cc = {",
            "\t\t.order = order,",
            "\t\t.search_order = order,",
            "\t\t.gfp_mask = gfp_mask,",
            "\t\t.zone = zone,",
            "\t\t.mode = (prio == COMPACT_PRIO_ASYNC) ?",
            "\t\t\t\t\tMIGRATE_ASYNC :\tMIGRATE_SYNC_LIGHT,",
            "\t\t.alloc_flags = alloc_flags,",
            "\t\t.highest_zoneidx = highest_zoneidx,",
            "\t\t.direct_compaction = true,",
            "\t\t.whole_zone = (prio == MIN_COMPACT_PRIORITY),",
            "\t\t.ignore_skip_hint = (prio == MIN_COMPACT_PRIORITY),",
            "\t\t.ignore_block_suitable = (prio == MIN_COMPACT_PRIORITY)",
            "\t};",
            "\tstruct capture_control capc = {",
            "\t\t.cc = &cc,",
            "\t\t.page = NULL,",
            "\t};",
            "",
            "\t/*",
            "\t * Make sure the structs are really initialized before we expose the",
            "\t * capture control, in case we are interrupted and the interrupt handler",
            "\t * frees a page.",
            "\t */",
            "\tbarrier();",
            "\tWRITE_ONCE(current->capture_control, &capc);",
            "",
            "\tret = compact_zone(&cc, &capc);",
            "",
            "\t/*",
            "\t * Make sure we hide capture control first before we read the captured",
            "\t * page pointer, otherwise an interrupt could free and capture a page",
            "\t * and we would leak it.",
            "\t */",
            "\tWRITE_ONCE(current->capture_control, NULL);",
            "\t*capture = READ_ONCE(capc.page);",
            "\t/*",
            "\t * Technically, it is also possible that compaction is skipped but",
            "\t * the page is still captured out of luck(IRQ came and freed the page).",
            "\t * Returning COMPACT_SUCCESS in such cases helps in properly accounting",
            "\t * the COMPACT[STALL|FAIL] when compaction is skipped.",
            "\t */",
            "\tif (*capture)",
            "\t\tret = COMPACT_SUCCESS;",
            "",
            "\treturn ret;",
            "}",
            "enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,",
            "\t\tunsigned int alloc_flags, const struct alloc_context *ac,",
            "\t\tenum compact_priority prio, struct page **capture)",
            "{",
            "\tstruct zoneref *z;",
            "\tstruct zone *zone;",
            "\tenum compact_result rc = COMPACT_SKIPPED;",
            "",
            "\tif (!gfp_compaction_allowed(gfp_mask))",
            "\t\treturn COMPACT_SKIPPED;",
            "",
            "\ttrace_mm_compaction_try_to_compact_pages(order, gfp_mask, prio);",
            "",
            "\t/* Compact each zone in the list */",
            "\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist,",
            "\t\t\t\t\tac->highest_zoneidx, ac->nodemask) {",
            "\t\tenum compact_result status;",
            "",
            "\t\tif (prio > MIN_COMPACT_PRIORITY",
            "\t\t\t\t\t&& compaction_deferred(zone, order)) {",
            "\t\t\trc = max_t(enum compact_result, COMPACT_DEFERRED, rc);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tstatus = compact_zone_order(zone, order, gfp_mask, prio,",
            "\t\t\t\talloc_flags, ac->highest_zoneidx, capture);",
            "\t\trc = max(status, rc);",
            "",
            "\t\t/* The allocation should succeed, stop compacting */",
            "\t\tif (status == COMPACT_SUCCESS) {",
            "\t\t\t/*",
            "\t\t\t * We think the allocation will succeed in this zone,",
            "\t\t\t * but it is not certain, hence the false. The caller",
            "\t\t\t * will repeat this with true if allocation indeed",
            "\t\t\t * succeeds in this zone.",
            "\t\t\t */",
            "\t\t\tcompaction_defer_reset(zone, order, false);",
            "",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tif (prio != COMPACT_PRIO_ASYNC && (status == COMPACT_COMPLETE ||",
            "\t\t\t\t\tstatus == COMPACT_PARTIAL_SKIPPED))",
            "\t\t\t/*",
            "\t\t\t * We think that allocation won't succeed in this zone",
            "\t\t\t * so we defer compaction there. If it ends up",
            "\t\t\t * succeeding after all, it will be reset.",
            "\t\t\t */",
            "\t\t\tdefer_compaction(zone, order);",
            "",
            "\t\t/*",
            "\t\t * We might have stopped compacting due to need_resched() in",
            "\t\t * async compaction, or due to a fatal signal detected. In that",
            "\t\t * case do not try further zones",
            "\t\t */",
            "\t\tif ((prio == COMPACT_PRIO_ASYNC && need_resched())",
            "\t\t\t\t\t|| fatal_signal_pending(current))",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn rc;",
            "}"
          ],
          "function_name": "compact_zone_order, try_to_compact_pages",
          "description": "实现compact_zone_order函数，用于在指定zone中执行内存紧缩操作，根据优先级设置不同模式，并通过捕获页指针处理异常情况。try_to_compact_pages函数遍历所有zone尝试紧缩，根据优先级和状态决定是否跳过或延迟紧缩操作。",
          "similarity": 0.5247174501419067
        }
      ]
    },
    {
      "source_file": "mm/ksm.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:34:25\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `ksm.c`\n\n---\n\n# ksm.c 技术文档\n\n## 1. 文件概述\n\n`ksm.c` 实现了内核同页合并（Kernel Samepage Merging, KSM）功能，该机制能够动态识别并合并内容完全相同的物理内存页，即使这些页属于不同的进程地址空间且未通过 `fork()` 共享。KSM 通过后台扫描线程周期性地遍历注册的内存区域，利用红黑树（rbtree）结构高效地比对页面内容，将重复页替换为只读的共享页，从而显著减少物理内存占用。此功能特别适用于虚拟化环境中多个相似虚拟机共存的场景。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct ksm_mm_slot`**  \n  表示一个被 KSM 扫描的内存描述符（`mm_struct`）的元数据，包含哈希槽位和反向映射项链表头。\n\n- **`struct ksm_scan`**  \n  全局扫描游标，记录当前扫描进度（包括当前 `mm_slot`、虚拟地址、反向映射项指针及完整扫描轮次计数）。\n\n- **`struct ksm_stable_node`**  \n  稳定红黑树中的节点，代表一个已合并的 KSM 页面。包含：\n  - 红黑树节点或迁移链表指针（联合体复用）\n  - 指向使用该 KSM 页的所有 `rmap_item` 的哈希链表头\n  - 物理页帧号（`kpfn`）或链修剪时间戳\n  - 反向映射项数量（支持链式扩展）\n  - NUMA 节点 ID（若启用 NUMA）\n\n- **`struct ksm_rmap_item`**  \n  反向映射项，跟踪一个虚拟地址到其物理页的映射关系。包含：\n  - 所属 `mm_struct` 和虚拟地址（低比特位用于标志）\n  - 匿名 VMA 指针（稳定状态）或 NUMA 节点 ID（不稳定状态）\n  - 校验和（不稳定状态）\n  - 红黑树节点（不稳定树）或指向 `stable_node` 的指针及哈希链表节点（稳定状态）\n\n### 关键宏定义\n\n- **`STABLE_NODE_CHAIN`**  \n  标识稳定节点为“链”类型（值为 -1024），用于高效管理大量相同内容的 KSM 页副本。\n  \n- **标志位**  \n  - `UNSTABLE_FLAG` (0x100)：标识 `rmap_item` 属于不稳定树\n  - `STABLE_FLAG` (0x200)：标识 `rmap_item` 已链接到稳定树\n  - `SEQNR_MASK` (0x0ff)：用于存储不稳定树序列号的低 8 位\n\n## 3. 关键实现\n\n### 双树架构设计\nKSM 采用**稳定树（stable tree）**与**不稳定树（unstable tree）**协同工作的机制：\n- **稳定树**：存储已确认可合并的只读 KSM 页，因写保护而内容恒定，支持高效精确匹配。\n- **不稳定树**：临时缓存近期未修改的普通页，因内容可能变化而需周期性重建。\n\n### 扫描与合并流程\n1. **增量扫描**：全局游标 `ksm_scan` 遍历所有注册的 `mm_slot` 及其内存区域。\n2. **校验和预筛**：计算页面内容的 `xxhash` 校验和，仅当与上次扫描一致时才尝试插入不稳定树。\n3. **双阶段匹配**：\n   - 优先在**稳定树**中查找完全匹配的 KSM 页\n   - 若未命中，则在**不稳定树**中查找潜在重复页\n4. **树维护策略**：\n   - 不稳定树在每轮全量扫描结束后**完全清空重建**\n   - 稳定树**持久保留**，通过反向映射（rmap）和链式节点优化大规模合并场景\n5. **NUMA 感知**：若 `merge_across_nodes=0`，则为每个 NUMA 节点维护独立的稳定/不稳定树，避免跨节点内存访问开销。\n\n### 内存安全机制\n- **写保护**：合并后的 KSM 页设为只读，任何写操作触发 COW（写时复制）并解除合并。\n- **RMAP 集成**：通过 `anon_vma` 和反向映射链表，在页解绑时高效更新所有相关虚拟地址。\n- **OOM 防护**：在内存压力下可释放 KSM 页以缓解系统压力。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：深度依赖 `mm.h`、`rmap.h`、`pagemap.h` 实现页表操作、反向映射和页生命周期管理。\n- **调度与进程管理**：通过 `sched/mm.h` 获取进程内存上下文，利用 `kthread.h` 创建后台扫描线程。\n- **NUMA 支持**：条件编译依赖 `CONFIG_NUMA`，使用 `numa.h` 实现节点亲和性。\n- **调试与追踪**：集成 `trace/events/ksm.h` 提供运行时事件追踪能力。\n- **哈希算法**：使用 `xxhash.h` 提供高效的内容指纹计算。\n- **内部辅助模块**：依赖 `mm_slot.h` 管理内存描述符槽位，`internal.h` 提供内核 MM 内部接口。\n\n## 5. 使用场景\n\n- **虚拟化环境**：在 KVM/Xen 等 Hypervisor 中合并多个相似虚拟机的内存页（如相同操作系统镜像）。\n- **内存密集型应用**：合并大型应用（如数据库、Web 服务器）中重复的静态数据或零页。\n- **容器化平台**：在 Docker/LXC 等容器运行时中减少同镜像容器的内存占用。\n- **内存超分场景**：在物理内存有限但允许超额分配的系统中提升内存利用率。\n- **开发调试**：通过 `/sys/kernel/mm/ksm/` 接口动态控制扫描速率、合并阈值等参数。",
      "similarity": 0.5706541538238525,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/ksm.c",
          "start_line": 1,
          "end_line": 311,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Memory merging support.",
            " *",
            " * This code enables dynamic sharing of identical pages found in different",
            " * memory areas, even if they are not shared by fork()",
            " *",
            " * Copyright (C) 2008-2009 Red Hat, Inc.",
            " * Authors:",
            " *\tIzik Eidus",
            " *\tAndrea Arcangeli",
            " *\tChris Wright",
            " *\tHugh Dickins",
            " */",
            "",
            "#include <linux/errno.h>",
            "#include <linux/mm.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/fs.h>",
            "#include <linux/mman.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/rmap.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/xxhash.h>",
            "#include <linux/delay.h>",
            "#include <linux/kthread.h>",
            "#include <linux/wait.h>",
            "#include <linux/slab.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/memory.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/swap.h>",
            "#include <linux/ksm.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/freezer.h>",
            "#include <linux/oom.h>",
            "#include <linux/numa.h>",
            "#include <linux/pagewalk.h>",
            "",
            "#include <asm/tlbflush.h>",
            "#include \"internal.h\"",
            "#include \"mm_slot.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/ksm.h>",
            "",
            "#ifdef CONFIG_NUMA",
            "#define NUMA(x)\t\t(x)",
            "#define DO_NUMA(x)\tdo { (x); } while (0)",
            "#else",
            "#define NUMA(x)\t\t(0)",
            "#define DO_NUMA(x)\tdo { } while (0)",
            "#endif",
            "",
            "/**",
            " * DOC: Overview",
            " *",
            " * A few notes about the KSM scanning process,",
            " * to make it easier to understand the data structures below:",
            " *",
            " * In order to reduce excessive scanning, KSM sorts the memory pages by their",
            " * contents into a data structure that holds pointers to the pages' locations.",
            " *",
            " * Since the contents of the pages may change at any moment, KSM cannot just",
            " * insert the pages into a normal sorted tree and expect it to find anything.",
            " * Therefore KSM uses two data structures - the stable and the unstable tree.",
            " *",
            " * The stable tree holds pointers to all the merged pages (ksm pages), sorted",
            " * by their contents.  Because each such page is write-protected, searching on",
            " * this tree is fully assured to be working (except when pages are unmapped),",
            " * and therefore this tree is called the stable tree.",
            " *",
            " * The stable tree node includes information required for reverse",
            " * mapping from a KSM page to virtual addresses that map this page.",
            " *",
            " * In order to avoid large latencies of the rmap walks on KSM pages,",
            " * KSM maintains two types of nodes in the stable tree:",
            " *",
            " * * the regular nodes that keep the reverse mapping structures in a",
            " *   linked list",
            " * * the \"chains\" that link nodes (\"dups\") that represent the same",
            " *   write protected memory content, but each \"dup\" corresponds to a",
            " *   different KSM page copy of that content",
            " *",
            " * Internally, the regular nodes, \"dups\" and \"chains\" are represented",
            " * using the same struct ksm_stable_node structure.",
            " *",
            " * In addition to the stable tree, KSM uses a second data structure called the",
            " * unstable tree: this tree holds pointers to pages which have been found to",
            " * be \"unchanged for a period of time\".  The unstable tree sorts these pages",
            " * by their contents, but since they are not write-protected, KSM cannot rely",
            " * upon the unstable tree to work correctly - the unstable tree is liable to",
            " * be corrupted as its contents are modified, and so it is called unstable.",
            " *",
            " * KSM solves this problem by several techniques:",
            " *",
            " * 1) The unstable tree is flushed every time KSM completes scanning all",
            " *    memory areas, and then the tree is rebuilt again from the beginning.",
            " * 2) KSM will only insert into the unstable tree, pages whose hash value",
            " *    has not changed since the previous scan of all memory areas.",
            " * 3) The unstable tree is a RedBlack Tree - so its balancing is based on the",
            " *    colors of the nodes and not on their contents, assuring that even when",
            " *    the tree gets \"corrupted\" it won't get out of balance, so scanning time",
            " *    remains the same (also, searching and inserting nodes in an rbtree uses",
            " *    the same algorithm, so we have no overhead when we flush and rebuild).",
            " * 4) KSM never flushes the stable tree, which means that even if it were to",
            " *    take 10 attempts to find a page in the unstable tree, once it is found,",
            " *    it is secured in the stable tree.  (When we scan a new page, we first",
            " *    compare it against the stable tree, and then against the unstable tree.)",
            " *",
            " * If the merge_across_nodes tunable is unset, then KSM maintains multiple",
            " * stable trees and multiple unstable trees: one of each for each NUMA node.",
            " */",
            "",
            "/**",
            " * struct ksm_mm_slot - ksm information per mm that is being scanned",
            " * @slot: hash lookup from mm to mm_slot",
            " * @rmap_list: head for this mm_slot's singly-linked list of rmap_items",
            " */",
            "struct ksm_mm_slot {",
            "\tstruct mm_slot slot;",
            "\tstruct ksm_rmap_item *rmap_list;",
            "};",
            "",
            "/**",
            " * struct ksm_scan - cursor for scanning",
            " * @mm_slot: the current mm_slot we are scanning",
            " * @address: the next address inside that to be scanned",
            " * @rmap_list: link to the next rmap to be scanned in the rmap_list",
            " * @seqnr: count of completed full scans (needed when removing unstable node)",
            " *",
            " * There is only the one ksm_scan instance of this cursor structure.",
            " */",
            "struct ksm_scan {",
            "\tstruct ksm_mm_slot *mm_slot;",
            "\tunsigned long address;",
            "\tstruct ksm_rmap_item **rmap_list;",
            "\tunsigned long seqnr;",
            "};",
            "",
            "/**",
            " * struct ksm_stable_node - node of the stable rbtree",
            " * @node: rb node of this ksm page in the stable tree",
            " * @head: (overlaying parent) &migrate_nodes indicates temporarily on that list",
            " * @hlist_dup: linked into the stable_node->hlist with a stable_node chain",
            " * @list: linked into migrate_nodes, pending placement in the proper node tree",
            " * @hlist: hlist head of rmap_items using this ksm page",
            " * @kpfn: page frame number of this ksm page (perhaps temporarily on wrong nid)",
            " * @chain_prune_time: time of the last full garbage collection",
            " * @rmap_hlist_len: number of rmap_item entries in hlist or STABLE_NODE_CHAIN",
            " * @nid: NUMA node id of stable tree in which linked (may not match kpfn)",
            " */",
            "struct ksm_stable_node {",
            "\tunion {",
            "\t\tstruct rb_node node;\t/* when node of stable tree */",
            "\t\tstruct {\t\t/* when listed for migration */",
            "\t\t\tstruct list_head *head;",
            "\t\t\tstruct {",
            "\t\t\t\tstruct hlist_node hlist_dup;",
            "\t\t\t\tstruct list_head list;",
            "\t\t\t};",
            "\t\t};",
            "\t};",
            "\tstruct hlist_head hlist;",
            "\tunion {",
            "\t\tunsigned long kpfn;",
            "\t\tunsigned long chain_prune_time;",
            "\t};",
            "\t/*",
            "\t * STABLE_NODE_CHAIN can be any negative number in",
            "\t * rmap_hlist_len negative range, but better not -1 to be able",
            "\t * to reliably detect underflows.",
            "\t */",
            "#define STABLE_NODE_CHAIN -1024",
            "\tint rmap_hlist_len;",
            "#ifdef CONFIG_NUMA",
            "\tint nid;",
            "#endif",
            "};",
            "",
            "/**",
            " * struct ksm_rmap_item - reverse mapping item for virtual addresses",
            " * @rmap_list: next rmap_item in mm_slot's singly-linked rmap_list",
            " * @anon_vma: pointer to anon_vma for this mm,address, when in stable tree",
            " * @nid: NUMA node id of unstable tree in which linked (may not match page)",
            " * @mm: the memory structure this rmap_item is pointing into",
            " * @address: the virtual address this rmap_item tracks (+ flags in low bits)",
            " * @oldchecksum: previous checksum of the page at that virtual address",
            " * @node: rb node of this rmap_item in the unstable tree",
            " * @head: pointer to stable_node heading this list in the stable tree",
            " * @hlist: link into hlist of rmap_items hanging off that stable_node",
            " */",
            "struct ksm_rmap_item {",
            "\tstruct ksm_rmap_item *rmap_list;",
            "\tunion {",
            "\t\tstruct anon_vma *anon_vma;\t/* when stable */",
            "#ifdef CONFIG_NUMA",
            "\t\tint nid;\t\t/* when node of unstable tree */",
            "#endif",
            "\t};",
            "\tstruct mm_struct *mm;",
            "\tunsigned long address;\t\t/* + low bits used for flags below */",
            "\tunsigned int oldchecksum;\t/* when unstable */",
            "\tunion {",
            "\t\tstruct rb_node node;\t/* when node of unstable tree */",
            "\t\tstruct {\t\t/* when listed from stable tree */",
            "\t\t\tstruct ksm_stable_node *head;",
            "\t\t\tstruct hlist_node hlist;",
            "\t\t};",
            "\t};",
            "};",
            "",
            "#define SEQNR_MASK\t0x0ff\t/* low bits of unstable tree seqnr */",
            "#define UNSTABLE_FLAG\t0x100\t/* is a node of the unstable tree */",
            "#define STABLE_FLAG\t0x200\t/* is listed from the stable tree */",
            "",
            "/* The stable and unstable tree heads */",
            "static struct rb_root one_stable_tree[1] = { RB_ROOT };",
            "static struct rb_root one_unstable_tree[1] = { RB_ROOT };",
            "static struct rb_root *root_stable_tree = one_stable_tree;",
            "static struct rb_root *root_unstable_tree = one_unstable_tree;",
            "",
            "/* Recently migrated nodes of stable tree, pending proper placement */",
            "static LIST_HEAD(migrate_nodes);",
            "#define STABLE_NODE_DUP_HEAD ((struct list_head *)&migrate_nodes.prev)",
            "",
            "#define MM_SLOTS_HASH_BITS 10",
            "static DEFINE_HASHTABLE(mm_slots_hash, MM_SLOTS_HASH_BITS);",
            "",
            "static struct ksm_mm_slot ksm_mm_head = {",
            "\t.slot.mm_node = LIST_HEAD_INIT(ksm_mm_head.slot.mm_node),",
            "};",
            "static struct ksm_scan ksm_scan = {",
            "\t.mm_slot = &ksm_mm_head,",
            "};",
            "",
            "static struct kmem_cache *rmap_item_cache;",
            "static struct kmem_cache *stable_node_cache;",
            "static struct kmem_cache *mm_slot_cache;",
            "",
            "/* The number of pages scanned */",
            "static unsigned long ksm_pages_scanned;",
            "",
            "/* The number of nodes in the stable tree */",
            "static unsigned long ksm_pages_shared;",
            "",
            "/* The number of page slots additionally sharing those nodes */",
            "static unsigned long ksm_pages_sharing;",
            "",
            "/* The number of nodes in the unstable tree */",
            "static unsigned long ksm_pages_unshared;",
            "",
            "/* The number of rmap_items in use: to calculate pages_volatile */",
            "static unsigned long ksm_rmap_items;",
            "",
            "/* The number of stable_node chains */",
            "static unsigned long ksm_stable_node_chains;",
            "",
            "/* The number of stable_node dups linked to the stable_node chains */",
            "static unsigned long ksm_stable_node_dups;",
            "",
            "/* Delay in pruning stale stable_node_dups in the stable_node_chains */",
            "static unsigned int ksm_stable_node_chains_prune_millisecs = 2000;",
            "",
            "/* Maximum number of page slots sharing a stable node */",
            "static int ksm_max_page_sharing = 256;",
            "",
            "/* Number of pages ksmd should scan in one batch */",
            "static unsigned int ksm_thread_pages_to_scan = 100;",
            "",
            "/* Milliseconds ksmd should sleep between batches */",
            "static unsigned int ksm_thread_sleep_millisecs = 20;",
            "",
            "/* Checksum of an empty (zeroed) page */",
            "static unsigned int zero_checksum __read_mostly;",
            "",
            "/* Whether to merge empty (zeroed) pages with actual zero pages */",
            "static bool ksm_use_zero_pages __read_mostly;",
            "",
            "/* The number of zero pages which is placed by KSM */",
            "atomic_long_t ksm_zero_pages = ATOMIC_LONG_INIT(0);",
            "",
            "#ifdef CONFIG_NUMA",
            "/* Zeroed when merging across nodes is not allowed */",
            "static unsigned int ksm_merge_across_nodes = 1;",
            "static int ksm_nr_node_ids = 1;",
            "#else",
            "#define ksm_merge_across_nodes\t1U",
            "#define ksm_nr_node_ids\t\t1",
            "#endif",
            "",
            "#define KSM_RUN_STOP\t0",
            "#define KSM_RUN_MERGE\t1",
            "#define KSM_RUN_UNMERGE\t2",
            "#define KSM_RUN_OFFLINE\t4",
            "static unsigned long ksm_run = KSM_RUN_STOP;",
            "static void wait_while_offlining(void);",
            "",
            "static DECLARE_WAIT_QUEUE_HEAD(ksm_thread_wait);",
            "static DECLARE_WAIT_QUEUE_HEAD(ksm_iter_wait);",
            "static DEFINE_MUTEX(ksm_thread_mutex);",
            "static DEFINE_SPINLOCK(ksm_mmlist_lock);",
            "",
            "#define KSM_KMEM_CACHE(__struct, __flags) kmem_cache_create(#__struct,\\",
            "\t\tsizeof(struct __struct), __alignof__(struct __struct),\\",
            "\t\t(__flags), NULL)",
            ""
          ],
          "function_name": null,
          "description": "定义了KSM（内核同一内存页合并）模块的核心数据结构和全局变量，包括稳定树和不稳定树的数据结构、NUMA支持相关定义、哈希表和红黑树操作接口，以及用于跟踪扫描进度的kscan结构。核心功能是建立KSM内存合并算法的基础框架。",
          "similarity": 0.5091700553894043
        },
        {
          "chunk_id": 2,
          "file_path": "mm/ksm.c",
          "start_line": 486,
          "end_line": 622,
          "content": [
            "static int break_ksm(struct vm_area_struct *vma, unsigned long addr, bool lock_vma)",
            "{",
            "\tvm_fault_t ret = 0;",
            "\tconst struct mm_walk_ops *ops = lock_vma ?",
            "\t\t\t\t&break_ksm_lock_vma_ops : &break_ksm_ops;",
            "",
            "\tdo {",
            "\t\tint ksm_page;",
            "",
            "\t\tcond_resched();",
            "\t\tksm_page = walk_page_range_vma(vma, addr, addr + 1, ops, NULL);",
            "\t\tif (WARN_ON_ONCE(ksm_page < 0))",
            "\t\t\treturn ksm_page;",
            "\t\tif (!ksm_page)",
            "\t\t\treturn 0;",
            "\t\tret = handle_mm_fault(vma, addr,",
            "\t\t\t\t      FAULT_FLAG_UNSHARE | FAULT_FLAG_REMOTE,",
            "\t\t\t\t      NULL);",
            "\t} while (!(ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | VM_FAULT_OOM)));",
            "\t/*",
            "\t * We must loop until we no longer find a KSM page because",
            "\t * handle_mm_fault() may back out if there's any difficulty e.g. if",
            "\t * pte accessed bit gets updated concurrently.",
            "\t *",
            "\t * VM_FAULT_SIGBUS could occur if we race with truncation of the",
            "\t * backing file, which also invalidates anonymous pages: that's",
            "\t * okay, that truncation will have unmapped the PageKsm for us.",
            "\t *",
            "\t * VM_FAULT_OOM: at the time of writing (late July 2009), setting",
            "\t * aside mem_cgroup limits, VM_FAULT_OOM would only be set if the",
            "\t * current task has TIF_MEMDIE set, and will be OOM killed on return",
            "\t * to user; and ksmd, having no mm, would never be chosen for that.",
            "\t *",
            "\t * But if the mm is in a limited mem_cgroup, then the fault may fail",
            "\t * with VM_FAULT_OOM even if the current task is not TIF_MEMDIE; and",
            "\t * even ksmd can fail in this way - though it's usually breaking ksm",
            "\t * just to undo a merge it made a moment before, so unlikely to oom.",
            "\t *",
            "\t * That's a pity: we might therefore have more kernel pages allocated",
            "\t * than we're counting as nodes in the stable tree; but ksm_do_scan",
            "\t * will retry to break_cow on each pass, so should recover the page",
            "\t * in due course.  The important thing is to not let VM_MERGEABLE",
            "\t * be cleared while any such pages might remain in the area.",
            "\t */",
            "\treturn (ret & VM_FAULT_OOM) ? -ENOMEM : 0;",
            "}",
            "static bool vma_ksm_compatible(struct vm_area_struct *vma)",
            "{",
            "\tif (vma->vm_flags & (VM_SHARED  | VM_MAYSHARE   | VM_PFNMAP  |",
            "\t\t\t     VM_IO      | VM_DONTEXPAND | VM_HUGETLB |",
            "\t\t\t     VM_MIXEDMAP))",
            "\t\treturn false;\t\t/* just ignore the advice */",
            "",
            "\tif (vma_is_dax(vma))",
            "\t\treturn false;",
            "",
            "#ifdef VM_SAO",
            "\tif (vma->vm_flags & VM_SAO)",
            "\t\treturn false;",
            "#endif",
            "#ifdef VM_SPARC_ADI",
            "\tif (vma->vm_flags & VM_SPARC_ADI)",
            "\t\treturn false;",
            "#endif",
            "",
            "\treturn true;",
            "}",
            "static void break_cow(struct ksm_rmap_item *rmap_item)",
            "{",
            "\tstruct mm_struct *mm = rmap_item->mm;",
            "\tunsigned long addr = rmap_item->address;",
            "\tstruct vm_area_struct *vma;",
            "",
            "\t/*",
            "\t * It is not an accident that whenever we want to break COW",
            "\t * to undo, we also need to drop a reference to the anon_vma.",
            "\t */",
            "\tput_anon_vma(rmap_item->anon_vma);",
            "",
            "\tmmap_read_lock(mm);",
            "\tvma = find_mergeable_vma(mm, addr);",
            "\tif (vma)",
            "\t\tbreak_ksm(vma, addr, false);",
            "\tmmap_read_unlock(mm);",
            "}",
            "static inline int get_kpfn_nid(unsigned long kpfn)",
            "{",
            "\treturn ksm_merge_across_nodes ? 0 : NUMA(pfn_to_nid(kpfn));",
            "}",
            "static inline void free_stable_node_chain(struct ksm_stable_node *chain,",
            "\t\t\t\t\t  struct rb_root *root)",
            "{",
            "\trb_erase(&chain->node, root);",
            "\tfree_stable_node(chain);",
            "\tksm_stable_node_chains--;",
            "}",
            "static void remove_node_from_stable_tree(struct ksm_stable_node *stable_node)",
            "{",
            "\tstruct ksm_rmap_item *rmap_item;",
            "",
            "\t/* check it's not STABLE_NODE_CHAIN or negative */",
            "\tBUG_ON(stable_node->rmap_hlist_len < 0);",
            "",
            "\thlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {",
            "\t\tif (rmap_item->hlist.next) {",
            "\t\t\tksm_pages_sharing--;",
            "\t\t\ttrace_ksm_remove_rmap_item(stable_node->kpfn, rmap_item, rmap_item->mm);",
            "\t\t} else {",
            "\t\t\tksm_pages_shared--;",
            "\t\t}",
            "",
            "\t\trmap_item->mm->ksm_merging_pages--;",
            "",
            "\t\tVM_BUG_ON(stable_node->rmap_hlist_len <= 0);",
            "\t\tstable_node->rmap_hlist_len--;",
            "\t\tput_anon_vma(rmap_item->anon_vma);",
            "\t\trmap_item->address &= PAGE_MASK;",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\t/*",
            "\t * We need the second aligned pointer of the migrate_nodes",
            "\t * list_head to stay clear from the rb_parent_color union",
            "\t * (aligned and different than any node) and also different",
            "\t * from &migrate_nodes. This will verify that future list.h changes",
            "\t * don't break STABLE_NODE_DUP_HEAD. Only recent gcc can handle it.",
            "\t */",
            "\tBUILD_BUG_ON(STABLE_NODE_DUP_HEAD <= &migrate_nodes);",
            "\tBUILD_BUG_ON(STABLE_NODE_DUP_HEAD >= &migrate_nodes + 1);",
            "",
            "\ttrace_ksm_remove_ksm_page(stable_node->kpfn);",
            "\tif (stable_node->head == &migrate_nodes)",
            "\t\tlist_del(&stable_node->list);",
            "\telse",
            "\t\tstable_node_dup_del(stable_node);",
            "\tfree_stable_node(stable_node);",
            "}"
          ],
          "function_name": "break_ksm, vma_ksm_compatible, break_cow, get_kpfn_nid, free_stable_node_chain, remove_node_from_stable_tree",
          "description": "实现KSM页面分解逻辑，包含检查VMA是否兼容KSM、强制打破写时复制（COW）操作、获取KPFN NUMA节点ID以及稳定树节点链表清理等功能。核心作用是执行实际的KSM页面分离操作并维护稳定树结构。",
          "similarity": 0.49122142791748047
        },
        {
          "chunk_id": 15,
          "file_path": "mm/ksm.c",
          "start_line": 3427,
          "end_line": 3508,
          "content": [
            "static ssize_t stable_node_dups_show(struct kobject *kobj,",
            "\t\t\t\t     struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_stable_node_dups);",
            "}",
            "static ssize_t stable_node_chains_show(struct kobject *kobj,",
            "\t\t\t\t       struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_stable_node_chains);",
            "}",
            "static ssize_t",
            "stable_node_chains_prune_millisecs_show(struct kobject *kobj,",
            "\t\t\t\t\tstruct kobj_attribute *attr,",
            "\t\t\t\t\tchar *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_stable_node_chains_prune_millisecs);",
            "}",
            "static ssize_t",
            "stable_node_chains_prune_millisecs_store(struct kobject *kobj,",
            "\t\t\t\t\t struct kobj_attribute *attr,",
            "\t\t\t\t\t const char *buf, size_t count)",
            "{",
            "\tunsigned int msecs;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &msecs);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\tksm_stable_node_chains_prune_millisecs = msecs;",
            "",
            "\treturn count;",
            "}",
            "static ssize_t full_scans_show(struct kobject *kobj,",
            "\t\t\t       struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_scan.seqnr);",
            "}",
            "static int __init ksm_init(void)",
            "{",
            "\tstruct task_struct *ksm_thread;",
            "\tint err;",
            "",
            "\t/* The correct value depends on page size and endianness */",
            "\tzero_checksum = calc_checksum(ZERO_PAGE(0));",
            "\t/* Default to false for backwards compatibility */",
            "\tksm_use_zero_pages = false;",
            "",
            "\terr = ksm_slab_init();",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\tksm_thread = kthread_run(ksm_scan_thread, NULL, \"ksmd\");",
            "\tif (IS_ERR(ksm_thread)) {",
            "\t\tpr_err(\"ksm: creating kthread failed\\n\");",
            "\t\terr = PTR_ERR(ksm_thread);",
            "\t\tgoto out_free;",
            "\t}",
            "",
            "#ifdef CONFIG_SYSFS",
            "\terr = sysfs_create_group(mm_kobj, &ksm_attr_group);",
            "\tif (err) {",
            "\t\tpr_err(\"ksm: register sysfs failed\\n\");",
            "\t\tkthread_stop(ksm_thread);",
            "\t\tgoto out_free;",
            "\t}",
            "#else",
            "\tksm_run = KSM_RUN_MERGE;\t/* no way for user to start it */",
            "",
            "#endif /* CONFIG_SYSFS */",
            "",
            "#ifdef CONFIG_MEMORY_HOTREMOVE",
            "\t/* There is no significance to this priority 100 */",
            "\thotplug_memory_notifier(ksm_memory_callback, KSM_CALLBACK_PRI);",
            "#endif",
            "\treturn 0;",
            "",
            "out_free:",
            "\tksm_slab_free();",
            "out:",
            "\treturn err;",
            "}"
          ],
          "function_name": "stable_node_dups_show, stable_node_chains_show, stable_node_chains_prune_millisecs_show, stable_node_chains_prune_millisecs_store, full_scans_show, ksm_init",
          "description": "此代码段实现了KSM（内核同页合并）子系统的sysfs接口，用于暴露稳定节点重复计数、链表数量及修剪间隔等运行时参数。其中`stable_node_*`系列函数通过sysfs接口读取内部状态变量，`stable_node_chains_prune_millisecs`支持动态修改修剪超时时长，`ksm_init`初始化KSM线程并注册sysfs属性组。",
          "similarity": 0.4748983383178711
        },
        {
          "chunk_id": 4,
          "file_path": "mm/ksm.c",
          "start_line": 963,
          "end_line": 1081,
          "content": [
            "static int remove_stable_node_chain(struct ksm_stable_node *stable_node,",
            "\t\t\t\t    struct rb_root *root)",
            "{",
            "\tstruct ksm_stable_node *dup;",
            "\tstruct hlist_node *hlist_safe;",
            "",
            "\tif (!is_stable_node_chain(stable_node)) {",
            "\t\tVM_BUG_ON(is_stable_node_dup(stable_node));",
            "\t\tif (remove_stable_node(stable_node))",
            "\t\t\treturn true;",
            "\t\telse",
            "\t\t\treturn false;",
            "\t}",
            "",
            "\thlist_for_each_entry_safe(dup, hlist_safe,",
            "\t\t\t\t  &stable_node->hlist, hlist_dup) {",
            "\t\tVM_BUG_ON(!is_stable_node_dup(dup));",
            "\t\tif (remove_stable_node(dup))",
            "\t\t\treturn true;",
            "\t}",
            "\tBUG_ON(!hlist_empty(&stable_node->hlist));",
            "\tfree_stable_node_chain(stable_node, root);",
            "\treturn false;",
            "}",
            "static int remove_all_stable_nodes(void)",
            "{",
            "\tstruct ksm_stable_node *stable_node, *next;",
            "\tint nid;",
            "\tint err = 0;",
            "",
            "\tfor (nid = 0; nid < ksm_nr_node_ids; nid++) {",
            "\t\twhile (root_stable_tree[nid].rb_node) {",
            "\t\t\tstable_node = rb_entry(root_stable_tree[nid].rb_node,",
            "\t\t\t\t\t\tstruct ksm_stable_node, node);",
            "\t\t\tif (remove_stable_node_chain(stable_node,",
            "\t\t\t\t\t\t     root_stable_tree + nid)) {",
            "\t\t\t\terr = -EBUSY;",
            "\t\t\t\tbreak;\t/* proceed to next nid */",
            "\t\t\t}",
            "\t\t\tcond_resched();",
            "\t\t}",
            "\t}",
            "\tlist_for_each_entry_safe(stable_node, next, &migrate_nodes, list) {",
            "\t\tif (remove_stable_node(stable_node))",
            "\t\t\terr = -EBUSY;",
            "\t\tcond_resched();",
            "\t}",
            "\treturn err;",
            "}",
            "static int unmerge_and_remove_all_rmap_items(void)",
            "{",
            "\tstruct ksm_mm_slot *mm_slot;",
            "\tstruct mm_slot *slot;",
            "\tstruct mm_struct *mm;",
            "\tstruct vm_area_struct *vma;",
            "\tint err = 0;",
            "",
            "\tspin_lock(&ksm_mmlist_lock);",
            "\tslot = list_entry(ksm_mm_head.slot.mm_node.next,",
            "\t\t\t  struct mm_slot, mm_node);",
            "\tksm_scan.mm_slot = mm_slot_entry(slot, struct ksm_mm_slot, slot);",
            "\tspin_unlock(&ksm_mmlist_lock);",
            "",
            "\tfor (mm_slot = ksm_scan.mm_slot; mm_slot != &ksm_mm_head;",
            "\t     mm_slot = ksm_scan.mm_slot) {",
            "\t\tVMA_ITERATOR(vmi, mm_slot->slot.mm, 0);",
            "",
            "\t\tmm = mm_slot->slot.mm;",
            "\t\tmmap_read_lock(mm);",
            "",
            "\t\t/*",
            "\t\t * Exit right away if mm is exiting to avoid lockdep issue in",
            "\t\t * the maple tree",
            "\t\t */",
            "\t\tif (ksm_test_exit(mm))",
            "\t\t\tgoto mm_exiting;",
            "",
            "\t\tfor_each_vma(vmi, vma) {",
            "\t\t\tif (!(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)",
            "\t\t\t\tcontinue;",
            "\t\t\terr = unmerge_ksm_pages(vma,",
            "\t\t\t\t\t\tvma->vm_start, vma->vm_end, false);",
            "\t\t\tif (err)",
            "\t\t\t\tgoto error;",
            "\t\t}",
            "",
            "mm_exiting:",
            "\t\tremove_trailing_rmap_items(&mm_slot->rmap_list);",
            "\t\tmmap_read_unlock(mm);",
            "",
            "\t\tspin_lock(&ksm_mmlist_lock);",
            "\t\tslot = list_entry(mm_slot->slot.mm_node.next,",
            "\t\t\t\t  struct mm_slot, mm_node);",
            "\t\tksm_scan.mm_slot = mm_slot_entry(slot, struct ksm_mm_slot, slot);",
            "\t\tif (ksm_test_exit(mm)) {",
            "\t\t\thash_del(&mm_slot->slot.hash);",
            "\t\t\tlist_del(&mm_slot->slot.mm_node);",
            "\t\t\tspin_unlock(&ksm_mmlist_lock);",
            "",
            "\t\t\tmm_slot_free(mm_slot_cache, mm_slot);",
            "\t\t\tclear_bit(MMF_VM_MERGEABLE, &mm->flags);",
            "\t\t\tclear_bit(MMF_VM_MERGE_ANY, &mm->flags);",
            "\t\t\tmmdrop(mm);",
            "\t\t} else",
            "\t\t\tspin_unlock(&ksm_mmlist_lock);",
            "\t}",
            "",
            "\t/* Clean up stable nodes, but don't worry if some are still busy */",
            "\tremove_all_stable_nodes();",
            "\tksm_scan.seqnr = 0;",
            "\treturn 0;",
            "",
            "error:",
            "\tmmap_read_unlock(mm);",
            "\tspin_lock(&ksm_mmlist_lock);",
            "\tksm_scan.mm_slot = &ksm_mm_head;",
            "\tspin_unlock(&ksm_mmlist_lock);",
            "\treturn err;",
            "}"
          ],
          "function_name": "remove_stable_node_chain, remove_all_stable_nodes, unmerge_and_remove_all_rmap_items",
          "description": "实现稳定节点链表深度清理、全部稳定节点移除及反向映射项完全卸载逻辑。核心作用是彻底释放KSM占用的所有资源，包括稳定树节点、反向映射项和相关内存结构，确保系统状态一致性。",
          "similarity": 0.46677255630493164
        },
        {
          "chunk_id": 7,
          "file_path": "mm/ksm.c",
          "start_line": 2074,
          "end_line": 2269,
          "content": [
            "static void stable_tree_append(struct ksm_rmap_item *rmap_item,",
            "\t\t\t       struct ksm_stable_node *stable_node,",
            "\t\t\t       bool max_page_sharing_bypass)",
            "{",
            "\t/*",
            "\t * rmap won't find this mapping if we don't insert the",
            "\t * rmap_item in the right stable_node",
            "\t * duplicate. page_migration could break later if rmap breaks,",
            "\t * so we can as well crash here. We really need to check for",
            "\t * rmap_hlist_len == STABLE_NODE_CHAIN, but we can as well check",
            "\t * for other negative values as an underflow if detected here",
            "\t * for the first time (and not when decreasing rmap_hlist_len)",
            "\t * would be sign of memory corruption in the stable_node.",
            "\t */",
            "\tBUG_ON(stable_node->rmap_hlist_len < 0);",
            "",
            "\tstable_node->rmap_hlist_len++;",
            "\tif (!max_page_sharing_bypass)",
            "\t\t/* possibly non fatal but unexpected overflow, only warn */",
            "\t\tWARN_ON_ONCE(stable_node->rmap_hlist_len >",
            "\t\t\t     ksm_max_page_sharing);",
            "",
            "\trmap_item->head = stable_node;",
            "\trmap_item->address |= STABLE_FLAG;",
            "\thlist_add_head(&rmap_item->hlist, &stable_node->hlist);",
            "",
            "\tif (rmap_item->hlist.next)",
            "\t\tksm_pages_sharing++;",
            "\telse",
            "\t\tksm_pages_shared++;",
            "",
            "\trmap_item->mm->ksm_merging_pages++;",
            "}",
            "static void cmp_and_merge_page(struct page *page, struct ksm_rmap_item *rmap_item)",
            "{",
            "\tstruct mm_struct *mm = rmap_item->mm;",
            "\tstruct ksm_rmap_item *tree_rmap_item;",
            "\tstruct page *tree_page = NULL;",
            "\tstruct ksm_stable_node *stable_node;",
            "\tstruct page *kpage;",
            "\tunsigned int checksum;",
            "\tint err;",
            "\tbool max_page_sharing_bypass = false;",
            "",
            "\tstable_node = page_stable_node(page);",
            "\tif (stable_node) {",
            "\t\tif (stable_node->head != &migrate_nodes &&",
            "\t\t    get_kpfn_nid(READ_ONCE(stable_node->kpfn)) !=",
            "\t\t    NUMA(stable_node->nid)) {",
            "\t\t\tstable_node_dup_del(stable_node);",
            "\t\t\tstable_node->head = &migrate_nodes;",
            "\t\t\tlist_add(&stable_node->list, stable_node->head);",
            "\t\t}",
            "\t\tif (stable_node->head != &migrate_nodes &&",
            "\t\t    rmap_item->head == stable_node)",
            "\t\t\treturn;",
            "\t\t/*",
            "\t\t * If it's a KSM fork, allow it to go over the sharing limit",
            "\t\t * without warnings.",
            "\t\t */",
            "\t\tif (!is_page_sharing_candidate(stable_node))",
            "\t\t\tmax_page_sharing_bypass = true;",
            "\t}",
            "",
            "\t/* We first start with searching the page inside the stable tree */",
            "\tkpage = stable_tree_search(page);",
            "\tif (kpage == page && rmap_item->head == stable_node) {",
            "\t\tput_page(kpage);",
            "\t\treturn;",
            "\t}",
            "",
            "\tremove_rmap_item_from_tree(rmap_item);",
            "",
            "\tif (kpage) {",
            "\t\tif (PTR_ERR(kpage) == -EBUSY)",
            "\t\t\treturn;",
            "",
            "\t\terr = try_to_merge_with_ksm_page(rmap_item, page, kpage);",
            "\t\tif (!err) {",
            "\t\t\t/*",
            "\t\t\t * The page was successfully merged:",
            "\t\t\t * add its rmap_item to the stable tree.",
            "\t\t\t */",
            "\t\t\tlock_page(kpage);",
            "\t\t\tstable_tree_append(rmap_item, page_stable_node(kpage),",
            "\t\t\t\t\t   max_page_sharing_bypass);",
            "\t\t\tunlock_page(kpage);",
            "\t\t}",
            "\t\tput_page(kpage);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * If the hash value of the page has changed from the last time",
            "\t * we calculated it, this page is changing frequently: therefore we",
            "\t * don't want to insert it in the unstable tree, and we don't want",
            "\t * to waste our time searching for something identical to it there.",
            "\t */",
            "\tchecksum = calc_checksum(page);",
            "\tif (rmap_item->oldchecksum != checksum) {",
            "\t\trmap_item->oldchecksum = checksum;",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Same checksum as an empty page. We attempt to merge it with the",
            "\t * appropriate zero page if the user enabled this via sysfs.",
            "\t */",
            "\tif (ksm_use_zero_pages && (checksum == zero_checksum)) {",
            "\t\tstruct vm_area_struct *vma;",
            "",
            "\t\tmmap_read_lock(mm);",
            "\t\tvma = find_mergeable_vma(mm, rmap_item->address);",
            "\t\tif (vma) {",
            "\t\t\terr = try_to_merge_one_page(vma, page,",
            "\t\t\t\t\tZERO_PAGE(rmap_item->address));",
            "\t\t\ttrace_ksm_merge_one_page(",
            "\t\t\t\tpage_to_pfn(ZERO_PAGE(rmap_item->address)),",
            "\t\t\t\trmap_item, mm, err);",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * If the vma is out of date, we do not need to",
            "\t\t\t * continue.",
            "\t\t\t */",
            "\t\t\terr = 0;",
            "\t\t}",
            "\t\tmmap_read_unlock(mm);",
            "\t\t/*",
            "\t\t * In case of failure, the page was not really empty, so we",
            "\t\t * need to continue. Otherwise we're done.",
            "\t\t */",
            "\t\tif (!err)",
            "\t\t\treturn;",
            "\t}",
            "\ttree_rmap_item =",
            "\t\tunstable_tree_search_insert(rmap_item, page, &tree_page);",
            "\tif (tree_rmap_item) {",
            "\t\tbool split;",
            "",
            "\t\tkpage = try_to_merge_two_pages(rmap_item, page,",
            "\t\t\t\t\t\ttree_rmap_item, tree_page);",
            "\t\t/*",
            "\t\t * If both pages we tried to merge belong to the same compound",
            "\t\t * page, then we actually ended up increasing the reference",
            "\t\t * count of the same compound page twice, and split_huge_page",
            "\t\t * failed.",
            "\t\t * Here we set a flag if that happened, and we use it later to",
            "\t\t * try split_huge_page again. Since we call put_page right",
            "\t\t * afterwards, the reference count will be correct and",
            "\t\t * split_huge_page should succeed.",
            "\t\t */",
            "\t\tsplit = PageTransCompound(page)",
            "\t\t\t&& compound_head(page) == compound_head(tree_page);",
            "\t\tput_page(tree_page);",
            "\t\tif (kpage) {",
            "\t\t\t/*",
            "\t\t\t * The pages were successfully merged: insert new",
            "\t\t\t * node in the stable tree and add both rmap_items.",
            "\t\t\t */",
            "\t\t\tlock_page(kpage);",
            "\t\t\tstable_node = stable_tree_insert(kpage);",
            "\t\t\tif (stable_node) {",
            "\t\t\t\tstable_tree_append(tree_rmap_item, stable_node,",
            "\t\t\t\t\t\t   false);",
            "\t\t\t\tstable_tree_append(rmap_item, stable_node,",
            "\t\t\t\t\t\t   false);",
            "\t\t\t}",
            "\t\t\tunlock_page(kpage);",
            "",
            "\t\t\t/*",
            "\t\t\t * If we fail to insert the page into the stable tree,",
            "\t\t\t * we will have 2 virtual addresses that are pointing",
            "\t\t\t * to a ksm page left outside the stable tree,",
            "\t\t\t * in which case we need to break_cow on both.",
            "\t\t\t */",
            "\t\t\tif (!stable_node) {",
            "\t\t\t\tbreak_cow(tree_rmap_item);",
            "\t\t\t\tbreak_cow(rmap_item);",
            "\t\t\t}",
            "\t\t} else if (split) {",
            "\t\t\t/*",
            "\t\t\t * We are here if we tried to merge two pages and",
            "\t\t\t * failed because they both belonged to the same",
            "\t\t\t * compound page. We will split the page now, but no",
            "\t\t\t * merging will take place.",
            "\t\t\t * We do not want to add the cost of a full lock; if",
            "\t\t\t * the page is locked, it is better to skip it and",
            "\t\t\t * perhaps try again later.",
            "\t\t\t */",
            "\t\t\tif (!trylock_page(page))",
            "\t\t\t\treturn;",
            "\t\t\tsplit_huge_page(page);",
            "\t\t\tunlock_page(page);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "stable_tree_append, cmp_and_merge_page",
          "description": "stable_tree_append 将rmap项插入稳定树节点，维护共享页面链表。cmp_and_merge_page 比较页面内容，尝试与现有共享页或零页合并，失败时插入不稳定树进行后续处理。",
          "similarity": 0.4636240303516388
        }
      ]
    }
  ]
}