{
  "query": "operating system process thread",
  "timestamp": "2025-12-25 23:30:56",
  "retrieved_files": [
    {
      "source_file": "kernel/kthread.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:30:24\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `kthread.c`\n\n---\n\n# kthread.c 技术文档\n\n## 文件概述\n\n`kthread.c` 是 Linux 内核中实现内核线程（kernel thread, kthread）管理机制的核心文件。它提供了创建、控制、同步和销毁内核线程的基础设施，确保内核线程在干净、受控的环境中运行，即使是从用户空间（如 modprobe、CPU 热插拔等）触发创建也能保证一致性。该文件实现了 kthread 的生命周期管理、状态控制（如停止、暂停）、数据访问接口以及与调度器、cgroup、freezer 等子系统的集成。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`struct kthread_create_info`**  \n  用于在 `kthread_create()` 和后台守护线程 `kthreadd` 之间传递创建参数和结果，包含线程函数、数据、节点信息、任务结构体指针和完成量。\n\n- **`struct kthread`**  \n  内核线程的私有控制块，挂载在 `task_struct->worker_private` 上，包含：\n  - 状态标志位（`KTHREAD_IS_PER_CPU`, `KTHREAD_SHOULD_STOP`, `KTHREAD_SHOULD_PARK`）\n  - CPU 绑定信息\n  - 线程函数指针和用户数据\n  - 用于同步的 `parked` 和 `exited` 完成量\n  - 完整线程名（当 `task->comm` 被截断时使用）\n  - （可选）块设备 cgroup 上下文（`blkcg_css`）\n\n- **全局变量**\n  - `kthread_create_lock`：保护 `kthread_create_list` 的自旋锁\n  - `kthread_create_list`：待创建内核线程的请求队列\n  - `kthreadd_task`：负责实际创建内核线程的守护进程任务结构体\n\n### 主要函数\n\n- **状态查询函数**\n  - `kthread_should_stop()`：检查是否应停止线程（由 `kthread_stop()` 触发）\n  - `kthread_should_park()`：检查是否应暂停线程（由 `kthread_park()` 触发）\n  - `kthread_should_stop_or_park()`：同时检查停止或暂停请求\n  - `kthread_freezable_should_stop()`：支持冻结的 kthread 停止检查，集成 freezer 机制\n\n- **数据访问函数**\n  - `kthread_func()`：获取线程创建时指定的函数指针\n  - `kthread_data()`：获取线程创建时传入的私有数据\n  - `kthread_probe_data()`：安全地探测可能的 kthread 数据（使用 `copy_from_kernel_nofault` 避免崩溃）\n  - `get_kthread_comm()`：获取完整的线程名称（优先使用 `full_name`）\n\n- **生命周期管理**\n  - `set_kthread_struct()`：为新任务分配并初始化 `struct kthread`\n  - `free_kthread_struct()`：释放 `struct kthread` 及其资源\n  - `kthread_parkme()`：将当前线程置于 `TASK_PARKED` 状态并等待唤醒\n  - `kthread_exit()`：终止当前 kthread 并返回结果（未在代码片段中完整显示）\n\n- **辅助函数**\n  - `to_kthread()` / `__to_kthread()`：从 `task_struct` 安全转换为 `struct kthread`，后者不假设任务一定是 kthread\n\n## 关键实现\n\n### kthread 私有数据管理\n- 每个 kthread 通过 `task_struct->worker_private` 指向其 `struct kthread` 实例。\n- `to_kthread()` 在访问前验证 `PF_KTHREAD` 标志，确保类型安全。\n- `__to_kthread()` 更加保守，仅在同时满足 `worker_private != NULL` 且 `PF_KTHREAD` 时才返回有效指针，以应对 `kernel_thread()` 可能执行 `exec()` 导致标志失效的情况。\n\n### 线程暂停机制（Parking）\n- 使用 `TASK_PARKED` 特殊任务状态，避免与常规调度状态冲突。\n- 在设置状态和检查标志之间使用原子操作，防止唤醒丢失。\n- 调用 `schedule_preempt_disabled()` 禁用抢占，确保 `kthread_park()` 调用者能可靠检测到线程已暂停。\n\n### 安全数据访问\n- `kthread_probe_data()` 使用 `copy_from_kernel_nofault()` 安全读取数据指针，即使目标内存无效也不会导致内核 oops，适用于调试或不确定上下文。\n\n### 冻结集成\n- `kthread_freezable_should_stop()` 在检查停止标志前先处理冻结请求，调用 `__refrigerator()` 进入冻结状态，避免 freezer 与 kthread_stop 死锁。\n\n### 名称管理\n- 当线程名超过 `TASK_COMM_LEN` 时，原始名称存储在 `kthread->full_name` 中，`get_kthread_comm()` 优先返回完整名称。\n\n## 依赖关系\n\n- **调度子系统**：依赖 `sched.h` 提供任务状态管理、调度原语（`schedule()`）、CPU 隔离等。\n- **内存管理**：使用 `slab.h` 分配 `kthread` 结构，`mm.h` 处理内存上下文。\n- **同步机制**：依赖 `completion.h` 实现线程创建和状态同步。\n- **cgroup 子系统**：条件编译支持 `CONFIG_BLK_CGROUP`，集成块设备 cgroup 控制。\n- **冻结子系统**：通过 `freezer.h` 与系统 suspend/hibernate 机制协作。\n- **追踪系统**：集成 `trace/events/sched.h` 提供调度事件追踪。\n- **用户空间接口**：通过 `uaccess.h` 支持安全内核空间访问（用于 `kthread_probe_data`）。\n\n## 使用场景\n\n- **内核模块加载**：`modprobe` 触发的模块可能创建 kthread，需通过 `kthreadd` 确保干净环境。\n- **设备驱动**：驱动程序使用 `kthread_run()` 创建工作线程处理中断下半部或轮询任务。\n- **系统服务线程**：如 `kswapd`（内存回收）、`kcompactd`（内存压缩）等核心内核线程。\n- **CPU 热插拔**：在 CPU 上下线时创建或迁移 per-CPU kthread。\n- **电源管理**：通过 `kthread_freezable_should_stop()` 支持系统 suspend 时冻结 kthread。\n- **动态资源管理**：使用 `kthread_park/unpark` 暂停/恢复线程以节省资源（如空闲时暂停工作线程）。\n- **调试与监控**：工具通过 `kthread_func()` 和 `kthread_data()` 获取线程上下文信息。",
      "similarity": 0.6398154497146606,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/kthread.c",
          "start_line": 299,
          "end_line": 413,
          "content": [
            "void kthread_parkme(void)",
            "{",
            "\t__kthread_parkme(to_kthread(current));",
            "}",
            "void __noreturn kthread_exit(long result)",
            "{",
            "\tstruct kthread *kthread = to_kthread(current);",
            "\tkthread->result = result;",
            "\tdo_exit(0);",
            "}",
            "void __noreturn kthread_complete_and_exit(struct completion *comp, long code)",
            "{",
            "\tif (comp)",
            "\t\tcomplete(comp);",
            "",
            "\tkthread_exit(code);",
            "}",
            "static int kthread(void *_create)",
            "{",
            "\tstatic const struct sched_param param = { .sched_priority = 0 };",
            "\t/* Copy data: it's on kthread's stack */",
            "\tstruct kthread_create_info *create = _create;",
            "\tint (*threadfn)(void *data) = create->threadfn;",
            "\tvoid *data = create->data;",
            "\tstruct completion *done;",
            "\tstruct kthread *self;",
            "\tint ret;",
            "",
            "\tself = to_kthread(current);",
            "",
            "\t/* Release the structure when caller killed by a fatal signal. */",
            "\tdone = xchg(&create->done, NULL);",
            "\tif (!done) {",
            "\t\tkfree(create->full_name);",
            "\t\tkfree(create);",
            "\t\tkthread_exit(-EINTR);",
            "\t}",
            "",
            "\tself->full_name = create->full_name;",
            "\tself->threadfn = threadfn;",
            "\tself->data = data;",
            "",
            "\t/*",
            "\t * The new thread inherited kthreadd's priority and CPU mask. Reset",
            "\t * back to default in case they have been changed.",
            "\t */",
            "\tsched_setscheduler_nocheck(current, SCHED_NORMAL, &param);",
            "\tset_cpus_allowed_ptr(current, housekeeping_cpumask(HK_TYPE_KTHREAD));",
            "",
            "\t/* OK, tell user we're spawned, wait for stop or wakeup */",
            "\t__set_current_state(TASK_UNINTERRUPTIBLE);",
            "\tcreate->result = current;",
            "\t/*",
            "\t * Thread is going to call schedule(), do not preempt it,",
            "\t * or the creator may spend more time in wait_task_inactive().",
            "\t */",
            "\tpreempt_disable();",
            "\tcomplete(done);",
            "\tschedule_preempt_disabled();",
            "\tpreempt_enable();",
            "",
            "\tret = -EINTR;",
            "\tif (!test_bit(KTHREAD_SHOULD_STOP, &self->flags)) {",
            "\t\tcgroup_kthread_ready();",
            "\t\t__kthread_parkme(self);",
            "\t\tret = threadfn(data);",
            "\t}",
            "\tkthread_exit(ret);",
            "}",
            "int tsk_fork_get_node(struct task_struct *tsk)",
            "{",
            "#ifdef CONFIG_NUMA",
            "\tif (tsk == kthreadd_task)",
            "\t\treturn tsk->pref_node_fork;",
            "#endif",
            "\treturn NUMA_NO_NODE;",
            "}",
            "static void create_kthread(struct kthread_create_info *create)",
            "{",
            "\tint pid;",
            "",
            "#ifdef CONFIG_NUMA",
            "\tcurrent->pref_node_fork = create->node;",
            "#endif",
            "\t/* We want our own signal handler (we take no signals by default). */",
            "\tpid = kernel_thread(kthread, create, create->full_name,",
            "\t\t\t    CLONE_FS | CLONE_FILES | SIGCHLD);",
            "\tif (pid < 0) {",
            "\t\t/* Release the structure when caller killed by a fatal signal. */",
            "\t\tstruct completion *done = xchg(&create->done, NULL);",
            "",
            "\t\tkfree(create->full_name);",
            "\t\tif (!done) {",
            "\t\t\tkfree(create);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tcreate->result = ERR_PTR(pid);",
            "\t\tcomplete(done);",
            "\t}",
            "}",
            "static void __kthread_bind_mask(struct task_struct *p, const struct cpumask *mask, unsigned int state)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tif (!wait_task_inactive(p, state)) {",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* It's safe because the task is inactive. */",
            "\traw_spin_lock_irqsave(&p->pi_lock, flags);",
            "\tdo_set_cpus_allowed(p, mask);",
            "\tp->flags |= PF_NO_SETAFFINITY;",
            "\traw_spin_unlock_irqrestore(&p->pi_lock, flags);",
            "}"
          ],
          "function_name": "kthread_parkme, kthread_exit, kthread_complete_and_exit, kthread, tsk_fork_get_node, create_kthread, __kthread_bind_mask",
          "description": "处理线程执行流程、节点绑定及异常退出，kthread作为内核线程入口执行指定函数，create_kthread创建新线程并绑定CPU，__kthread_bind_mask调整线程CPU亲和性。",
          "similarity": 0.6162614226341248
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/kthread.c",
          "start_line": 982,
          "end_line": 1095,
          "content": [
            "static inline bool queuing_blocked(struct kthread_worker *worker,",
            "\t\t\t\t   struct kthread_work *work)",
            "{",
            "\tlockdep_assert_held(&worker->lock);",
            "",
            "\treturn !list_empty(&work->node) || work->canceling;",
            "}",
            "static void kthread_insert_work_sanity_check(struct kthread_worker *worker,",
            "\t\t\t\t\t     struct kthread_work *work)",
            "{",
            "\tlockdep_assert_held(&worker->lock);",
            "\tWARN_ON_ONCE(!list_empty(&work->node));",
            "\t/* Do not use a work with >1 worker, see kthread_queue_work() */",
            "\tWARN_ON_ONCE(work->worker && work->worker != worker);",
            "}",
            "static void kthread_insert_work(struct kthread_worker *worker,",
            "\t\t\t\tstruct kthread_work *work,",
            "\t\t\t\tstruct list_head *pos)",
            "{",
            "\tkthread_insert_work_sanity_check(worker, work);",
            "",
            "\ttrace_sched_kthread_work_queue_work(worker, work);",
            "",
            "\tlist_add_tail(&work->node, pos);",
            "\twork->worker = worker;",
            "\tif (!worker->current_work && likely(worker->task))",
            "\t\twake_up_process(worker->task);",
            "}",
            "bool kthread_queue_work(struct kthread_worker *worker,",
            "\t\t\tstruct kthread_work *work)",
            "{",
            "\tbool ret = false;",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&worker->lock, flags);",
            "\tif (!queuing_blocked(worker, work)) {",
            "\t\tkthread_insert_work(worker, work, &worker->work_list);",
            "\t\tret = true;",
            "\t}",
            "\traw_spin_unlock_irqrestore(&worker->lock, flags);",
            "\treturn ret;",
            "}",
            "void kthread_delayed_work_timer_fn(struct timer_list *t)",
            "{",
            "\tstruct kthread_delayed_work *dwork = from_timer(dwork, t, timer);",
            "\tstruct kthread_work *work = &dwork->work;",
            "\tstruct kthread_worker *worker = work->worker;",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * This might happen when a pending work is reinitialized.",
            "\t * It means that it is used a wrong way.",
            "\t */",
            "\tif (WARN_ON_ONCE(!worker))",
            "\t\treturn;",
            "",
            "\traw_spin_lock_irqsave(&worker->lock, flags);",
            "\t/* Work must not be used with >1 worker, see kthread_queue_work(). */",
            "\tWARN_ON_ONCE(work->worker != worker);",
            "",
            "\t/* Move the work from worker->delayed_work_list. */",
            "\tWARN_ON_ONCE(list_empty(&work->node));",
            "\tlist_del_init(&work->node);",
            "\tif (!work->canceling)",
            "\t\tkthread_insert_work(worker, work, &worker->work_list);",
            "",
            "\traw_spin_unlock_irqrestore(&worker->lock, flags);",
            "}",
            "static void __kthread_queue_delayed_work(struct kthread_worker *worker,",
            "\t\t\t\t\t struct kthread_delayed_work *dwork,",
            "\t\t\t\t\t unsigned long delay)",
            "{",
            "\tstruct timer_list *timer = &dwork->timer;",
            "\tstruct kthread_work *work = &dwork->work;",
            "",
            "\tWARN_ON_ONCE(timer->function != kthread_delayed_work_timer_fn);",
            "",
            "\t/*",
            "\t * If @delay is 0, queue @dwork->work immediately.  This is for",
            "\t * both optimization and correctness.  The earliest @timer can",
            "\t * expire is on the closest next tick and delayed_work users depend",
            "\t * on that there's no such delay when @delay is 0.",
            "\t */",
            "\tif (!delay) {",
            "\t\tkthread_insert_work(worker, work, &worker->work_list);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Be paranoid and try to detect possible races already now. */",
            "\tkthread_insert_work_sanity_check(worker, work);",
            "",
            "\tlist_add(&work->node, &worker->delayed_work_list);",
            "\twork->worker = worker;",
            "\ttimer->expires = jiffies + delay;",
            "\tadd_timer(timer);",
            "}",
            "bool kthread_queue_delayed_work(struct kthread_worker *worker,",
            "\t\t\t\tstruct kthread_delayed_work *dwork,",
            "\t\t\t\tunsigned long delay)",
            "{",
            "\tstruct kthread_work *work = &dwork->work;",
            "\tunsigned long flags;",
            "\tbool ret = false;",
            "",
            "\traw_spin_lock_irqsave(&worker->lock, flags);",
            "",
            "\tif (!queuing_blocked(worker, work)) {",
            "\t\t__kthread_queue_delayed_work(worker, dwork, delay);",
            "\t\tret = true;",
            "\t}",
            "",
            "\traw_spin_unlock_irqrestore(&worker->lock, flags);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queuing_blocked, kthread_insert_work_sanity_check, kthread_insert_work, kthread_queue_work, kthread_delayed_work_timer_fn, __kthread_queue_delayed_work, kthread_queue_delayed_work",
          "description": "实现kthread_worker与kthread_work的队列管理，包含插入/延迟插入逻辑、锁保护及任务唤醒机制，处理工作项状态校验和延迟定时器回调",
          "similarity": 0.599124550819397
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/kthread.c",
          "start_line": 731,
          "end_line": 846,
          "content": [
            "int kthread_stop_put(struct task_struct *k)",
            "{",
            "\tint ret;",
            "",
            "\tret = kthread_stop(k);",
            "\tput_task_struct(k);",
            "\treturn ret;",
            "}",
            "int kthreadd(void *unused)",
            "{",
            "\tstruct task_struct *tsk = current;",
            "",
            "\t/* Setup a clean context for our children to inherit. */",
            "\tset_task_comm(tsk, \"kthreadd\");",
            "\tignore_signals(tsk);",
            "\tset_cpus_allowed_ptr(tsk, housekeeping_cpumask(HK_TYPE_KTHREAD));",
            "\tset_mems_allowed(node_states[N_MEMORY]);",
            "",
            "\tcurrent->flags |= PF_NOFREEZE;",
            "\tcgroup_init_kthreadd();",
            "",
            "\tfor (;;) {",
            "\t\tset_current_state(TASK_INTERRUPTIBLE);",
            "\t\tif (list_empty(&kthread_create_list))",
            "\t\t\tschedule();",
            "\t\t__set_current_state(TASK_RUNNING);",
            "",
            "\t\tspin_lock(&kthread_create_lock);",
            "\t\twhile (!list_empty(&kthread_create_list)) {",
            "\t\t\tstruct kthread_create_info *create;",
            "",
            "\t\t\tcreate = list_entry(kthread_create_list.next,",
            "\t\t\t\t\t    struct kthread_create_info, list);",
            "\t\t\tlist_del_init(&create->list);",
            "\t\t\tspin_unlock(&kthread_create_lock);",
            "",
            "\t\t\tcreate_kthread(create);",
            "",
            "\t\t\tspin_lock(&kthread_create_lock);",
            "\t\t}",
            "\t\tspin_unlock(&kthread_create_lock);",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "void __kthread_init_worker(struct kthread_worker *worker,",
            "\t\t\t\tconst char *name,",
            "\t\t\t\tstruct lock_class_key *key)",
            "{",
            "\tmemset(worker, 0, sizeof(struct kthread_worker));",
            "\traw_spin_lock_init(&worker->lock);",
            "\tlockdep_set_class_and_name(&worker->lock, key, name);",
            "\tINIT_LIST_HEAD(&worker->work_list);",
            "\tINIT_LIST_HEAD(&worker->delayed_work_list);",
            "}",
            "int kthread_worker_fn(void *worker_ptr)",
            "{",
            "\tstruct kthread_worker *worker = worker_ptr;",
            "\tstruct kthread_work *work;",
            "",
            "\t/*",
            "\t * FIXME: Update the check and remove the assignment when all kthread",
            "\t * worker users are created using kthread_create_worker*() functions.",
            "\t */",
            "\tWARN_ON(worker->task && worker->task != current);",
            "\tworker->task = current;",
            "",
            "\tif (worker->flags & KTW_FREEZABLE)",
            "\t\tset_freezable();",
            "",
            "repeat:",
            "\tset_current_state(TASK_INTERRUPTIBLE);\t/* mb paired w/ kthread_stop */",
            "",
            "\tif (kthread_should_stop()) {",
            "\t\t__set_current_state(TASK_RUNNING);",
            "\t\traw_spin_lock_irq(&worker->lock);",
            "\t\tworker->task = NULL;",
            "\t\traw_spin_unlock_irq(&worker->lock);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\twork = NULL;",
            "\traw_spin_lock_irq(&worker->lock);",
            "\tif (!list_empty(&worker->work_list)) {",
            "\t\twork = list_first_entry(&worker->work_list,",
            "\t\t\t\t\tstruct kthread_work, node);",
            "\t\tlist_del_init(&work->node);",
            "\t}",
            "\tworker->current_work = work;",
            "\traw_spin_unlock_irq(&worker->lock);",
            "",
            "\tif (work) {",
            "\t\tkthread_work_func_t func = work->func;",
            "\t\t__set_current_state(TASK_RUNNING);",
            "\t\ttrace_sched_kthread_work_execute_start(work);",
            "\t\twork->func(work);",
            "\t\t/*",
            "\t\t * Avoid dereferencing work after this point.  The trace",
            "\t\t * event only cares about the address.",
            "\t\t */",
            "\t\ttrace_sched_kthread_work_execute_end(work, func);",
            "\t} else if (!freezing(current)) {",
            "\t\tschedule();",
            "\t} else {",
            "\t\t/*",
            "\t\t * Handle the case where the current remains",
            "\t\t * TASK_INTERRUPTIBLE. try_to_freeze() expects",
            "\t\t * the current to be TASK_RUNNING.",
            "\t\t */",
            "\t\t__set_current_state(TASK_RUNNING);",
            "\t}",
            "",
            "\ttry_to_freeze();",
            "\tcond_resched();",
            "\tgoto repeat;",
            "}"
          ],
          "function_name": "kthread_stop_put, kthreadd, __kthread_init_worker, kthread_worker_fn",
          "description": "实现kthreadd主线程逻辑及工作队列管理，kthreadd持续处理线程创建请求，kthread_worker_fn作为工作队列执行入口，支持可冻结状态下的任务调度。",
          "similarity": 0.5719577670097351
        },
        {
          "chunk_id": 8,
          "file_path": "kernel/kthread.c",
          "start_line": 1490,
          "end_line": 1539,
          "content": [
            "void kthread_unuse_mm(struct mm_struct *mm)",
            "{",
            "\tstruct task_struct *tsk = current;",
            "",
            "\tWARN_ON_ONCE(!(tsk->flags & PF_KTHREAD));",
            "\tWARN_ON_ONCE(!tsk->mm);",
            "",
            "\ttask_lock(tsk);",
            "\t/*",
            "\t * When a kthread stops operating on an address space, the loop",
            "\t * in membarrier_{private,global}_expedited() may not observe",
            "\t * that tsk->mm, and not issue an IPI. Membarrier requires a",
            "\t * memory barrier after accessing user-space memory, before",
            "\t * clearing tsk->mm.",
            "\t */",
            "\tsmp_mb__after_spinlock();",
            "\tsync_mm_rss(mm);",
            "\tlocal_irq_disable();",
            "\ttsk->mm = NULL;",
            "\tmembarrier_update_current_mm(NULL);",
            "\t#ifdef CONFIG_IEE",
            "\tiee_set_token_pgd(tsk, NULL);",
            "\t#endif",
            "\tmmgrab_lazy_tlb(mm);",
            "\t/* active_mm is still 'mm' */",
            "\tenter_lazy_tlb(mm, tsk);",
            "\tlocal_irq_enable();",
            "\ttask_unlock(tsk);",
            "",
            "\tmmdrop(mm);",
            "}",
            "void kthread_associate_blkcg(struct cgroup_subsys_state *css)",
            "{",
            "\tstruct kthread *kthread;",
            "",
            "\tif (!(current->flags & PF_KTHREAD))",
            "\t\treturn;",
            "\tkthread = to_kthread(current);",
            "\tif (!kthread)",
            "\t\treturn;",
            "",
            "\tif (kthread->blkcg_css) {",
            "\t\tcss_put(kthread->blkcg_css);",
            "\t\tkthread->blkcg_css = NULL;",
            "\t}",
            "\tif (css) {",
            "\t\tcss_get(css);",
            "\t\tkthread->blkcg_css = css;",
            "\t}",
            "}"
          ],
          "function_name": "kthread_unuse_mm, kthread_associate_blkcg",
          "description": "管理kthread的地址空间切换，包含内存屏障同步、TLB更新及块控制组绑定操作，确保上下文切换安全性",
          "similarity": 0.5664937496185303
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/kthread.c",
          "start_line": 1316,
          "end_line": 1426,
          "content": [
            "static bool __kthread_cancel_work_sync(struct kthread_work *work, bool is_dwork)",
            "{",
            "\tstruct kthread_worker *worker = work->worker;",
            "\tunsigned long flags;",
            "\tint ret = false;",
            "",
            "\tif (!worker)",
            "\t\tgoto out;",
            "",
            "\traw_spin_lock_irqsave(&worker->lock, flags);",
            "\t/* Work must not be used with >1 worker, see kthread_queue_work(). */",
            "\tWARN_ON_ONCE(work->worker != worker);",
            "",
            "\tif (is_dwork)",
            "\t\tkthread_cancel_delayed_work_timer(work, &flags);",
            "",
            "\tret = __kthread_cancel_work(work);",
            "",
            "\tif (worker->current_work != work)",
            "\t\tgoto out_fast;",
            "",
            "\t/*",
            "\t * The work is in progress and we need to wait with the lock released.",
            "\t * In the meantime, block any queuing by setting the canceling counter.",
            "\t */",
            "\twork->canceling++;",
            "\traw_spin_unlock_irqrestore(&worker->lock, flags);",
            "\tkthread_flush_work(work);",
            "\traw_spin_lock_irqsave(&worker->lock, flags);",
            "\twork->canceling--;",
            "",
            "out_fast:",
            "\traw_spin_unlock_irqrestore(&worker->lock, flags);",
            "out:",
            "\treturn ret;",
            "}",
            "bool kthread_cancel_work_sync(struct kthread_work *work)",
            "{",
            "\treturn __kthread_cancel_work_sync(work, false);",
            "}",
            "bool kthread_cancel_delayed_work_sync(struct kthread_delayed_work *dwork)",
            "{",
            "\treturn __kthread_cancel_work_sync(&dwork->work, true);",
            "}",
            "void kthread_flush_worker(struct kthread_worker *worker)",
            "{",
            "\tstruct kthread_flush_work fwork = {",
            "\t\tKTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),",
            "\t\tCOMPLETION_INITIALIZER_ONSTACK(fwork.done),",
            "\t};",
            "",
            "\tkthread_queue_work(worker, &fwork.work);",
            "\twait_for_completion(&fwork.done);",
            "}",
            "void kthread_destroy_worker(struct kthread_worker *worker)",
            "{",
            "\tstruct task_struct *task;",
            "",
            "\ttask = worker->task;",
            "\tif (WARN_ON(!task))",
            "\t\treturn;",
            "",
            "\tkthread_flush_worker(worker);",
            "\tkthread_stop(task);",
            "\tWARN_ON(!list_empty(&worker->delayed_work_list));",
            "\tWARN_ON(!list_empty(&worker->work_list));",
            "\tkfree(worker);",
            "}",
            "void kthread_use_mm(struct mm_struct *mm)",
            "{",
            "\tstruct mm_struct *active_mm;",
            "\tstruct task_struct *tsk = current;",
            "",
            "\tWARN_ON_ONCE(!(tsk->flags & PF_KTHREAD));",
            "\tWARN_ON_ONCE(tsk->mm);",
            "",
            "\t/*",
            "\t * It is possible for mm to be the same as tsk->active_mm, but",
            "\t * we must still mmgrab(mm) and mmdrop_lazy_tlb(active_mm),",
            "\t * because these references are not equivalent.",
            "\t */",
            "\tmmgrab(mm);",
            "",
            "\ttask_lock(tsk);",
            "\t/* Hold off tlb flush IPIs while switching mm's */",
            "\tlocal_irq_disable();",
            "\tactive_mm = tsk->active_mm;",
            "\ttsk->active_mm = mm;",
            "\ttsk->mm = mm;",
            "\tmembarrier_update_current_mm(mm);",
            "\t#ifdef CONFIG_IEE",
            "\tiee_set_token_pgd(tsk, mm->pgd);",
            "\t#endif",
            "\tswitch_mm_irqs_off(active_mm, mm, tsk);",
            "\tlocal_irq_enable();",
            "\ttask_unlock(tsk);",
            "#ifdef finish_arch_post_lock_switch",
            "\tfinish_arch_post_lock_switch();",
            "#endif",
            "",
            "\t/*",
            "\t * When a kthread starts operating on an address space, the loop",
            "\t * in membarrier_{private,global}_expedited() may not observe",
            "\t * that tsk->mm, and not issue an IPI. Membarrier requires a",
            "\t * memory barrier after storing to tsk->mm, before accessing",
            "\t * user-space memory. A full memory barrier for membarrier",
            "\t * {PRIVATE,GLOBAL}_EXPEDITED is implicitly provided by",
            "\t * mmdrop_lazy_tlb().",
            "\t */",
            "\tmmdrop_lazy_tlb(active_mm);",
            "}"
          ],
          "function_name": "__kthread_cancel_work_sync, kthread_cancel_work_sync, kthread_cancel_delayed_work_sync, kthread_flush_worker, kthread_destroy_worker, kthread_use_mm",
          "description": "实现同步取消工作项及延迟工作功能，强制刷新工作者队列并销毁工作者线程，包含内存管理切换和块控制组关联逻辑",
          "similarity": 0.5623077154159546
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.6010990738868713,
      "chunks": [
        {
          "chunk_id": 10,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1840,
          "end_line": 1973,
          "content": [
            "static int __noreturn rcu_gp_kthread(void *unused)",
            "{",
            "\trcu_bind_gp_kthread();",
            "\tfor (;;) {",
            "",
            "\t\t/* Handle grace-period start. */",
            "\t\tfor (;;) {",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwait\"));",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_WAIT_GPS);",
            "\t\t\tswait_event_idle_exclusive(rcu_state.gp_wq,",
            "\t\t\t\t\t READ_ONCE(rcu_state.gp_flags) &",
            "\t\t\t\t\t RCU_GP_FLAG_INIT);",
            "\t\t\trcu_gp_torture_wait();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_DONE_GPS);",
            "\t\t\t/* Locking provides needed memory barrier. */",
            "\t\t\tif (rcu_gp_init())",
            "\t\t\t\tbreak;",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\t\t\tWARN_ON(signal_pending(current));",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwaitsig\"));",
            "\t\t}",
            "",
            "\t\t/* Handle quiescent-state forcing. */",
            "\t\trcu_gp_fqs_loop();",
            "",
            "\t\t/* Handle grace-period end. */",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANUP);",
            "\t\trcu_gp_cleanup();",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANED);",
            "\t}",
            "}",
            "static void rcu_report_qs_rsp(unsigned long flags)",
            "\t__releases(rcu_get_root()->lock)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rcu_get_root());",
            "\tWARN_ON_ONCE(!rcu_gp_in_progress());",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rcu_get_root(), flags);",
            "\trcu_gp_kthread_wake();",
            "}",
            "static void rcu_report_qs_rnp(unsigned long mask, struct rcu_node *rnp,",
            "\t\t\t      unsigned long gps, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long oldmask = 0;",
            "\tstruct rcu_node *rnp_c;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* Walk up the rcu_node hierarchy. */",
            "\tfor (;;) {",
            "\t\tif ((!(rnp->qsmask & mask) && mask) || rnp->gp_seq != gps) {",
            "",
            "\t\t\t/*",
            "\t\t\t * Our bit has already been cleared, or the",
            "\t\t\t * relevant grace period is already over, so done.",
            "\t\t\t */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */",
            "\t\tWARN_ON_ONCE(!rcu_is_leaf_node(rnp) &&",
            "\t\t\t     rcu_preempt_blocked_readers_cgp(rnp));",
            "\t\tWRITE_ONCE(rnp->qsmask, rnp->qsmask & ~mask);",
            "\t\ttrace_rcu_quiescent_state_report(rcu_state.name, rnp->gp_seq,",
            "\t\t\t\t\t\t mask, rnp->qsmask, rnp->level,",
            "\t\t\t\t\t\t rnp->grplo, rnp->grphi,",
            "\t\t\t\t\t\t !!rnp->gp_tasks);",
            "\t\tif (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {",
            "",
            "\t\t\t/* Other bits still set at this level, so done. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\trnp->completedqs = rnp->gp_seq;",
            "\t\tmask = rnp->grpmask;",
            "\t\tif (rnp->parent == NULL) {",
            "",
            "\t\t\t/* No more levels.  Exit loop holding root lock. */",
            "",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\trnp_c = rnp;",
            "\t\trnp = rnp->parent;",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\toldmask = READ_ONCE(rnp_c->qsmask);",
            "\t}",
            "",
            "\t/*",
            "\t * Get here if we are the last CPU to pass through a quiescent",
            "\t * state for this grace period.  Invoke rcu_report_qs_rsp()",
            "\t * to clean up and start the next grace period if one is needed.",
            "\t */",
            "\trcu_report_qs_rsp(flags); /* releases rnp->lock. */",
            "}",
            "static void __maybe_unused",
            "rcu_report_unblock_qs_rnp(struct rcu_node *rnp, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long gps;",
            "\tunsigned long mask;",
            "\tstruct rcu_node *rnp_p;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (WARN_ON_ONCE(!IS_ENABLED(CONFIG_PREEMPT_RCU)) ||",
            "\t    WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)) ||",
            "\t    rnp->qsmask != 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;  /* Still need more quiescent states! */",
            "\t}",
            "",
            "\trnp->completedqs = rnp->gp_seq;",
            "\trnp_p = rnp->parent;",
            "\tif (rnp_p == NULL) {",
            "\t\t/*",
            "\t\t * Only one rcu_node structure in the tree, so don't",
            "\t\t * try to report up to its nonexistent parent!",
            "\t\t */",
            "\t\trcu_report_qs_rsp(flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Report up the rest of the hierarchy, tracking current ->gp_seq. */",
            "\tgps = rnp->gp_seq;",
            "\tmask = rnp->grpmask;",
            "\traw_spin_unlock_rcu_node(rnp);\t/* irqs remain disabled. */",
            "\traw_spin_lock_rcu_node(rnp_p);\t/* irqs already disabled. */",
            "\trcu_report_qs_rnp(mask, rnp_p, gps, flags);",
            "}"
          ],
          "function_name": "rcu_gp_kthread, rcu_report_qs_rsp, rcu_report_qs_rnp, rcu_report_unblock_qs_rnp",
          "description": "实现RCU grace period的主线程循环，处理grace period启动、强制quiescent状态报告及结束逻辑，通过锁和状态标志协调各子系统同步",
          "similarity": 0.5730808973312378
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1067,
          "end_line": 1170,
          "content": [
            "static void rcu_gp_kthread_wake(void)",
            "{",
            "\tstruct task_struct *t = READ_ONCE(rcu_state.gp_kthread);",
            "",
            "\tif ((current == t && !in_hardirq() && !in_serving_softirq()) ||",
            "\t    !READ_ONCE(rcu_state.gp_flags) || !t)",
            "\t\treturn;",
            "\tWRITE_ONCE(rcu_state.gp_wake_time, jiffies);",
            "\tWRITE_ONCE(rcu_state.gp_wake_seq, READ_ONCE(rcu_state.gp_seq));",
            "\tswake_up_one_online(&rcu_state.gp_wq);",
            "}",
            "static bool rcu_accelerate_cbs(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\tunsigned long gp_seq_req;",
            "\tbool ret = false;",
            "",
            "\trcu_lockdep_assert_cblist_protected(rdp);",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* If no pending (not yet ready to invoke) callbacks, nothing to do. */",
            "\tif (!rcu_segcblist_pend_cbs(&rdp->cblist))",
            "\t\treturn false;",
            "",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCbPreAcc\"));",
            "",
            "\t/*",
            "\t * Callbacks are often registered with incomplete grace-period",
            "\t * information.  Something about the fact that getting exact",
            "\t * information requires acquiring a global lock...  RCU therefore",
            "\t * makes a conservative estimate of the grace period number at which",
            "\t * a given callback will become ready to invoke.\tThe following",
            "\t * code checks this estimate and improves it when possible, thus",
            "\t * accelerating callback invocation to an earlier grace-period",
            "\t * number.",
            "\t */",
            "\tgp_seq_req = rcu_seq_snap(&rcu_state.gp_seq);",
            "\tif (rcu_segcblist_accelerate(&rdp->cblist, gp_seq_req))",
            "\t\tret = rcu_start_this_gp(rnp, rdp, gp_seq_req);",
            "",
            "\t/* Trace depending on how much we were able to accelerate. */",
            "\tif (rcu_segcblist_restempty(&rdp->cblist, RCU_WAIT_TAIL))",
            "\t\ttrace_rcu_grace_period(rcu_state.name, gp_seq_req, TPS(\"AccWaitCB\"));",
            "\telse",
            "\t\ttrace_rcu_grace_period(rcu_state.name, gp_seq_req, TPS(\"AccReadyCB\"));",
            "",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCbPostAcc\"));",
            "",
            "\treturn ret;",
            "}",
            "static void rcu_accelerate_cbs_unlocked(struct rcu_node *rnp,",
            "\t\t\t\t\tstruct rcu_data *rdp)",
            "{",
            "\tunsigned long c;",
            "\tbool needwake;",
            "",
            "\trcu_lockdep_assert_cblist_protected(rdp);",
            "\tc = rcu_seq_snap(&rcu_state.gp_seq);",
            "\tif (!READ_ONCE(rdp->gpwrap) && ULONG_CMP_GE(rdp->gp_seq_needed, c)) {",
            "\t\t/* Old request still live, so mark recent callbacks. */",
            "\t\t(void)rcu_segcblist_accelerate(&rdp->cblist, c);",
            "\t\treturn;",
            "\t}",
            "\traw_spin_lock_rcu_node(rnp); /* irqs already disabled. */",
            "\tneedwake = rcu_accelerate_cbs(rnp, rdp);",
            "\traw_spin_unlock_rcu_node(rnp); /* irqs remain disabled. */",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "}",
            "static bool rcu_advance_cbs(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\trcu_lockdep_assert_cblist_protected(rdp);",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* If no pending (not yet ready to invoke) callbacks, nothing to do. */",
            "\tif (!rcu_segcblist_pend_cbs(&rdp->cblist))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Find all callbacks whose ->gp_seq numbers indicate that they",
            "\t * are ready to invoke, and put them into the RCU_DONE_TAIL sublist.",
            "\t */",
            "\trcu_segcblist_advance(&rdp->cblist, rnp->gp_seq);",
            "",
            "\t/* Classify any remaining callbacks. */",
            "\treturn rcu_accelerate_cbs(rnp, rdp);",
            "}",
            "static void __maybe_unused rcu_advance_cbs_nowake(struct rcu_node *rnp,",
            "\t\t\t\t\t\t  struct rcu_data *rdp)",
            "{",
            "\trcu_lockdep_assert_cblist_protected(rdp);",
            "\tif (!rcu_seq_state(rcu_seq_current(&rnp->gp_seq)) || !raw_spin_trylock_rcu_node(rnp))",
            "\t\treturn;",
            "\t// The grace period cannot end while we hold the rcu_node lock.",
            "\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))",
            "\t\tWARN_ON_ONCE(rcu_advance_cbs(rnp, rdp));",
            "\traw_spin_unlock_rcu_node(rnp);",
            "}",
            "static void rcu_strict_gp_check_qs(void)",
            "{",
            "\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD)) {",
            "\t\trcu_read_lock();",
            "\t\trcu_read_unlock();",
            "\t}",
            "}"
          ],
          "function_name": "rcu_gp_kthread_wake, rcu_accelerate_cbs, rcu_accelerate_cbs_unlocked, rcu_advance_cbs, rcu_advance_cbs_nowake, rcu_strict_gp_check_qs",
          "description": "实现RCU grace period管理，通过唤醒kthread、加速回调处理和更新状态来推进grace period进度，包含回调列表优化、quiescent state检测及状态同步等功能。",
          "similarity": 0.571595311164856
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1354,
          "end_line": 1556,
          "content": [
            "static void rcu_strict_gp_boundary(void *unused)",
            "{",
            "\tinvoke_rcu_core();",
            "}",
            "static void rcu_poll_gp_seq_start(unsigned long *snap)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)",
            "\t\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t// If RCU was idle, note beginning of GP.",
            "\tif (!rcu_seq_state(rcu_state.gp_seq_polled))",
            "\t\trcu_seq_start(&rcu_state.gp_seq_polled);",
            "",
            "\t// Either way, record current state.",
            "\t*snap = rcu_state.gp_seq_polled;",
            "}",
            "static void rcu_poll_gp_seq_end(unsigned long *snap)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)",
            "\t\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t// If the previously noted GP is still in effect, record the",
            "\t// end of that GP.  Either way, zero counter to avoid counter-wrap",
            "\t// problems.",
            "\tif (*snap && *snap == rcu_state.gp_seq_polled) {",
            "\t\trcu_seq_end(&rcu_state.gp_seq_polled);",
            "\t\trcu_state.gp_seq_polled_snap = 0;",
            "\t\trcu_state.gp_seq_polled_exp_snap = 0;",
            "\t} else {",
            "\t\t*snap = 0;",
            "\t}",
            "}",
            "static void rcu_poll_gp_seq_start_unlocked(unsigned long *snap)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tif (rcu_init_invoked()) {",
            "\t\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t}",
            "\trcu_poll_gp_seq_start(snap);",
            "\tif (rcu_init_invoked())",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "}",
            "static void rcu_poll_gp_seq_end_unlocked(unsigned long *snap)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tif (rcu_init_invoked()) {",
            "\t\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t}",
            "\trcu_poll_gp_seq_end(snap);",
            "\tif (rcu_init_invoked())",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "}",
            "static noinline_for_stack bool rcu_gp_init(void)",
            "{",
            "\tunsigned long flags;",
            "\tunsigned long oldmask;",
            "\tunsigned long mask;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\traw_spin_lock_irq_rcu_node(rnp);",
            "\tif (!READ_ONCE(rcu_state.gp_flags)) {",
            "\t\t/* Spurious wakeup, tell caller to go back to sleep.  */",
            "\t\traw_spin_unlock_irq_rcu_node(rnp);",
            "\t\treturn false;",
            "\t}",
            "\tWRITE_ONCE(rcu_state.gp_flags, 0); /* Clear all flags: New GP. */",
            "",
            "\tif (WARN_ON_ONCE(rcu_gp_in_progress())) {",
            "\t\t/*",
            "\t\t * Grace period already in progress, don't start another.",
            "\t\t * Not supposed to be able to happen.",
            "\t\t */",
            "\t\traw_spin_unlock_irq_rcu_node(rnp);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/* Advance to a new grace period and initialize state. */",
            "\trecord_gp_stall_check_time();",
            "\t/* Record GP times before starting GP, hence rcu_seq_start(). */",
            "\trcu_seq_start(&rcu_state.gp_seq);",
            "\tASSERT_EXCLUSIVE_WRITER(rcu_state.gp_seq);",
            "\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq, TPS(\"start\"));",
            "\trcu_poll_gp_seq_start(&rcu_state.gp_seq_polled_snap);",
            "\traw_spin_unlock_irq_rcu_node(rnp);",
            "",
            "\t/*",
            "\t * Apply per-leaf buffered online and offline operations to",
            "\t * the rcu_node tree. Note that this new grace period need not",
            "\t * wait for subsequent online CPUs, and that RCU hooks in the CPU",
            "\t * offlining path, when combined with checks in this function,",
            "\t * will handle CPUs that are currently going offline or that will",
            "\t * go offline later.  Please also refer to \"Hotplug CPU\" section",
            "\t * of RCU's Requirements documentation.",
            "\t */",
            "\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_ONOFF);",
            "\t/* Exclude CPU hotplug operations. */",
            "\trcu_for_each_leaf_node(rnp) {",
            "\t\tlocal_irq_save(flags);",
            "\t\tarch_spin_lock(&rcu_state.ofl_lock);",
            "\t\traw_spin_lock_rcu_node(rnp);",
            "\t\tif (rnp->qsmaskinit == rnp->qsmaskinitnext &&",
            "\t\t    !rnp->wait_blkd_tasks) {",
            "\t\t\t/* Nothing to do on this leaf rcu_node structure. */",
            "\t\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\t\tarch_spin_unlock(&rcu_state.ofl_lock);",
            "\t\t\tlocal_irq_restore(flags);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/* Record old state, apply changes to ->qsmaskinit field. */",
            "\t\toldmask = rnp->qsmaskinit;",
            "\t\trnp->qsmaskinit = rnp->qsmaskinitnext;",
            "",
            "\t\t/* If zero-ness of ->qsmaskinit changed, propagate up tree. */",
            "\t\tif (!oldmask != !rnp->qsmaskinit) {",
            "\t\t\tif (!oldmask) { /* First online CPU for rcu_node. */",
            "\t\t\t\tif (!rnp->wait_blkd_tasks) /* Ever offline? */",
            "\t\t\t\t\trcu_init_new_rnp(rnp);",
            "\t\t\t} else if (rcu_preempt_has_tasks(rnp)) {",
            "\t\t\t\trnp->wait_blkd_tasks = true; /* blocked tasks */",
            "\t\t\t} else { /* Last offline CPU and can propagate. */",
            "\t\t\t\trcu_cleanup_dead_rnp(rnp);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If all waited-on tasks from prior grace period are",
            "\t\t * done, and if all this rcu_node structure's CPUs are",
            "\t\t * still offline, propagate up the rcu_node tree and",
            "\t\t * clear ->wait_blkd_tasks.  Otherwise, if one of this",
            "\t\t * rcu_node structure's CPUs has since come back online,",
            "\t\t * simply clear ->wait_blkd_tasks.",
            "\t\t */",
            "\t\tif (rnp->wait_blkd_tasks &&",
            "\t\t    (!rcu_preempt_has_tasks(rnp) || rnp->qsmaskinit)) {",
            "\t\t\trnp->wait_blkd_tasks = false;",
            "\t\t\tif (!rnp->qsmaskinit)",
            "\t\t\t\trcu_cleanup_dead_rnp(rnp);",
            "\t\t}",
            "",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\tarch_spin_unlock(&rcu_state.ofl_lock);",
            "\t\tlocal_irq_restore(flags);",
            "\t}",
            "\trcu_gp_slow(gp_preinit_delay); /* Races with CPU hotplug. */",
            "",
            "\t/*",
            "\t * Set the quiescent-state-needed bits in all the rcu_node",
            "\t * structures for all currently online CPUs in breadth-first",
            "\t * order, starting from the root rcu_node structure, relying on the",
            "\t * layout of the tree within the rcu_state.node[] array.  Note that",
            "\t * other CPUs will access only the leaves of the hierarchy, thus",
            "\t * seeing that no grace period is in progress, at least until the",
            "\t * corresponding leaf node has been initialized.",
            "\t *",
            "\t * The grace period cannot complete until the initialization",
            "\t * process finishes, because this kthread handles both.",
            "\t */",
            "\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_INIT);",
            "\trcu_for_each_node_breadth_first(rnp) {",
            "\t\trcu_gp_slow(gp_init_delay);",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\trdp = this_cpu_ptr(&rcu_data);",
            "\t\trcu_preempt_check_blocked_tasks(rnp);",
            "\t\trnp->qsmask = rnp->qsmaskinit;",
            "\t\tWRITE_ONCE(rnp->gp_seq, rcu_state.gp_seq);",
            "\t\tif (rnp == rdp->mynode)",
            "\t\t\t(void)__note_gp_changes(rnp, rdp);",
            "\t\trcu_preempt_boost_start_gp(rnp);",
            "\t\ttrace_rcu_grace_period_init(rcu_state.name, rnp->gp_seq,",
            "\t\t\t\t\t    rnp->level, rnp->grplo,",
            "\t\t\t\t\t    rnp->grphi, rnp->qsmask);",
            "\t\t/* Quiescent states for tasks on any now-offline CPUs. */",
            "\t\tmask = rnp->qsmask & ~rnp->qsmaskinitnext;",
            "\t\trnp->rcu_gp_init_mask = mask;",
            "\t\tif ((mask || rnp->wait_blkd_tasks) && rcu_is_leaf_node(rnp))",
            "\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\telse",
            "\t\t\traw_spin_unlock_irq_rcu_node(rnp);",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\t}",
            "",
            "\t// If strict, make all CPUs aware of new grace period.",
            "\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD))",
            "\t\ton_each_cpu(rcu_strict_gp_boundary, NULL, 0);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "rcu_strict_gp_boundary, rcu_poll_gp_seq_start, rcu_poll_gp_seq_end, rcu_poll_gp_seq_start_unlocked, rcu_poll_gp_seq_end_unlocked, rcu_gp_init",
          "description": "初始化新grace period并处理CPU热插拔，通过遍历rcu_node树更新qsmask字段，设置严格模式下的全局通知，并协调初始化过程与后续处理。",
          "similarity": 0.536466121673584
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 908,
          "end_line": 1026,
          "content": [
            "static void trace_rcu_this_gp(struct rcu_node *rnp, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req, const char *s)",
            "{",
            "\ttrace_rcu_future_grace_period(rcu_state.name, READ_ONCE(rnp->gp_seq),",
            "\t\t\t\t      gp_seq_req, rnp->level,",
            "\t\t\t\t      rnp->grplo, rnp->grphi, s);",
            "}",
            "static bool rcu_start_this_gp(struct rcu_node *rnp_start, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req)",
            "{",
            "\tbool ret = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\t/*",
            "\t * Use funnel locking to either acquire the root rcu_node",
            "\t * structure's lock or bail out if the need for this grace period",
            "\t * has already been recorded -- or if that grace period has in",
            "\t * fact already started.  If there is already a grace period in",
            "\t * progress in a non-leaf node, no recording is needed because the",
            "\t * end of the grace period will scan the leaf rcu_node structures.",
            "\t * Note that rnp_start->lock must not be released.",
            "\t */",
            "\traw_lockdep_assert_held_rcu_node(rnp_start);",
            "\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req, TPS(\"Startleaf\"));",
            "\tfor (rnp = rnp_start; 1; rnp = rnp->parent) {",
            "\t\tif (rnp != rnp_start)",
            "\t\t\traw_spin_lock_rcu_node(rnp);",
            "\t\tif (ULONG_CMP_GE(rnp->gp_seq_needed, gp_seq_req) ||",
            "\t\t    rcu_seq_started(&rnp->gp_seq, gp_seq_req) ||",
            "\t\t    (rnp != rnp_start &&",
            "\t\t     rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))) {",
            "\t\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Prestarted\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tWRITE_ONCE(rnp->gp_seq_needed, gp_seq_req);",
            "\t\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq))) {",
            "\t\t\t/*",
            "\t\t\t * We just marked the leaf or internal node, and a",
            "\t\t\t * grace period is in progress, which means that",
            "\t\t\t * rcu_gp_cleanup() will see the marking.  Bail to",
            "\t\t\t * reduce contention.",
            "\t\t\t */",
            "\t\t\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Startedleaf\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tif (rnp != rnp_start && rnp->parent != NULL)",
            "\t\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\tif (!rnp->parent)",
            "\t\t\tbreak;  /* At root, and perhaps also leaf. */",
            "\t}",
            "",
            "\t/* If GP already in progress, just leave, otherwise start one. */",
            "\tif (rcu_gp_in_progress()) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedleafroot\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedroot\"));",
            "\tWRITE_ONCE(rcu_state.gp_flags, rcu_state.gp_flags | RCU_GP_FLAG_INIT);",
            "\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);",
            "\tif (!READ_ONCE(rcu_state.gp_kthread)) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"NoGPkthread\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_grace_period(rcu_state.name, data_race(rcu_state.gp_seq), TPS(\"newreq\"));",
            "\tret = true;  /* Caller must wake GP kthread. */",
            "unlock_out:",
            "\t/* Push furthest requested GP to leaf node and rcu_data structure. */",
            "\tif (ULONG_CMP_LT(gp_seq_req, rnp->gp_seq_needed)) {",
            "\t\tWRITE_ONCE(rnp_start->gp_seq_needed, rnp->gp_seq_needed);",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\t}",
            "\tif (rnp != rnp_start)",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\treturn ret;",
            "}",
            "static bool rcu_future_gp_cleanup(struct rcu_node *rnp)",
            "{",
            "\tbool needmore;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tneedmore = ULONG_CMP_LT(rnp->gp_seq, rnp->gp_seq_needed);",
            "\tif (!needmore)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq; /* Avoid counter wrap. */",
            "\ttrace_rcu_this_gp(rnp, rdp, rnp->gp_seq,",
            "\t\t\t  needmore ? TPS(\"CleanupMore\") : TPS(\"Cleanup\"));",
            "\treturn needmore;",
            "}",
            "static void swake_up_one_online_ipi(void *arg)",
            "{",
            "\tstruct swait_queue_head *wqh = arg;",
            "",
            "\tswake_up_one(wqh);",
            "}",
            "static void swake_up_one_online(struct swait_queue_head *wqh)",
            "{",
            "\tint cpu = get_cpu();",
            "",
            "\t/*",
            "\t * If called from rcutree_report_cpu_starting(), wake up",
            "\t * is dangerous that late in the CPU-down hotplug process. The",
            "\t * scheduler might queue an ignored hrtimer. Defer the wake up",
            "\t * to an online CPU instead.",
            "\t */",
            "\tif (unlikely(cpu_is_offline(cpu))) {",
            "\t\tint target;",
            "",
            "\t\ttarget = cpumask_any_and(housekeeping_cpumask(HK_TYPE_RCU),",
            "\t\t\t\t\t cpu_online_mask);",
            "",
            "\t\tsmp_call_function_single(target, swake_up_one_online_ipi,",
            "\t\t\t\t\t wqh, 0);",
            "\t\tput_cpu();",
            "\t} else {",
            "\t\tput_cpu();",
            "\t\tswake_up_one(wqh);",
            "\t}",
            "}"
          ],
          "function_name": "trace_rcu_this_gp, rcu_start_this_gp, rcu_future_gp_cleanup, swake_up_one_online_ipi, swake_up_one_online",
          "description": "实现RCU grace period事件追踪、新grace period启动逻辑及未来grace period清理机制，支持跨层级节点的同步状态传播。",
          "similarity": 0.5362772941589355
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 438,
          "end_line": 544,
          "content": [
            "static int param_set_first_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : j);",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "static int param_set_next_fqs_jiffies(const char *val, const struct kernel_param *kp)",
            "{",
            "\tulong j;",
            "\tint ret = kstrtoul(val, 0, &j);",
            "",
            "\tif (!ret) {",
            "\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : (j ?: 1));",
            "\t\tadjust_jiffies_till_sched_qs();",
            "\t}",
            "\treturn ret;",
            "}",
            "unsigned long rcu_get_gp_seq(void)",
            "{",
            "\treturn READ_ONCE(rcu_state.gp_seq);",
            "}",
            "unsigned long rcu_exp_batches_completed(void)",
            "{",
            "\treturn rcu_state.expedited_sequence;",
            "}",
            "void rcutorture_get_gp_data(enum rcutorture_type test_type, int *flags,",
            "\t\t\t    unsigned long *gp_seq)",
            "{",
            "\tswitch (test_type) {",
            "\tcase RCU_FLAVOR:",
            "\t\t*flags = READ_ONCE(rcu_state.gp_flags);",
            "\t\t*gp_seq = rcu_seq_current(&rcu_state.gp_seq);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "}",
            "static void late_wakeup_func(struct irq_work *work)",
            "{",
            "}",
            "noinstr void rcu_irq_work_resched(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tif (IS_ENABLED(CONFIG_GENERIC_ENTRY) && !(current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tif (IS_ENABLED(CONFIG_KVM_XFER_TO_GUEST_WORK) && (current->flags & PF_VCPU))",
            "\t\treturn;",
            "",
            "\tinstrumentation_begin();",
            "\tif (do_nocb_deferred_wakeup(rdp) && need_resched()) {",
            "\t\tirq_work_queue(this_cpu_ptr(&late_wakeup_work));",
            "\t}",
            "\tinstrumentation_end();",
            "}",
            "void rcu_irq_exit_check_preempt(void)",
            "{",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nesting() <= 0,",
            "\t\t\t \"RCU dynticks_nesting counter underflow/zero!\");",
            "\tRCU_LOCKDEP_WARN(ct_dynticks_nmi_nesting() !=",
            "\t\t\t DYNTICK_IRQ_NONIDLE,",
            "\t\t\t \"Bad RCU  dynticks_nmi_nesting counter\\n\");",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"RCU in extended quiescent state!\");",
            "}",
            "void __rcu_irq_enter_check_tick(void)",
            "{",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\t// If we're here from NMI there's nothing to do.",
            "\tif (in_nmi())",
            "\t\treturn;",
            "",
            "\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),",
            "\t\t\t \"Illegal rcu_irq_enter_check_tick() from extended quiescent state\");",
            "",
            "\tif (!tick_nohz_full_cpu(rdp->cpu) ||",
            "\t    !READ_ONCE(rdp->rcu_urgent_qs) ||",
            "\t    READ_ONCE(rdp->rcu_forced_tick)) {",
            "\t\t// RCU doesn't need nohz_full help from this CPU, or it is",
            "\t\t// already getting that help.",
            "\t\treturn;",
            "\t}",
            "",
            "\t// We get here only when not in an extended quiescent state and",
            "\t// from interrupts (as opposed to NMIs).  Therefore, (1) RCU is",
            "\t// already watching and (2) The fact that we are in an interrupt",
            "\t// handler and that the rcu_node lock is an irq-disabled lock",
            "\t// prevents self-deadlock.  So we can safely recheck under the lock.",
            "\t// Note that the nohz_full state currently cannot change.",
            "\traw_spin_lock_rcu_node(rdp->mynode);",
            "\tif (READ_ONCE(rdp->rcu_urgent_qs) && !rdp->rcu_forced_tick) {",
            "\t\t// A nohz_full CPU is in the kernel and RCU needs a",
            "\t\t// quiescent state.  Turn on the tick!",
            "\t\tWRITE_ONCE(rdp->rcu_forced_tick, true);",
            "\t\ttick_dep_set_cpu(rdp->cpu, TICK_DEP_BIT_RCU);",
            "\t}",
            "\traw_spin_unlock_rcu_node(rdp->mynode);",
            "}"
          ],
          "function_name": "param_set_first_fqs_jiffies, param_set_next_fqs_jiffies, rcu_get_gp_seq, rcu_exp_batches_completed, rcutorture_get_gp_data, late_wakeup_func, rcu_irq_work_resched, rcu_irq_exit_check_preempt, __rcu_irq_enter_check_tick",
          "description": "实现参数配置回调函数和中断上下文RCU工作重排逻辑，管理grace period序列号读取及nohz_full模式下的tick依赖关系。",
          "similarity": 0.5317267775535583
        }
      ]
    },
    {
      "source_file": "kernel/sched/membarrier.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:12:44\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\membarrier.c`\n\n---\n\n# `sched/membarrier.c` 技术文档\n\n## 1. 文件概述\n\n`sched/membarrier.c` 实现了 Linux 内核中的 `membarrier` 系统调用，该调用为用户空间程序提供了一种高效的全局内存屏障机制。与传统的在每个线程中显式插入内存屏障相比，`membarrier` 允许一个线程通过一次系统调用，强制所有运行在系统上的线程（或特定进程组内的线程）执行内存屏障操作，从而简化用户空间并发同步逻辑并提升性能。\n\n该文件的核心目标是在多核系统中确保内存操作的全局可见性顺序，尤其适用于需要跨线程强内存顺序保证的用户空间同步原语（如 RCU、无锁数据结构等）。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`ipi_mb(void *info)`**  \n  IPI（处理器间中断）处理函数，执行 `smp_mb()` 内存屏障，用于基础的全局内存屏障命令。\n\n- **`ipi_sync_core(void *info)`**  \n  用于 `MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE` 命令的 IPI 处理函数，在执行内存屏障后调用 `sync_core_before_usermode()`，确保 CPU 核心状态同步（如指令缓存一致性）。\n\n- **`ipi_rseq(void *info)`**  \n  用于 `MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ` 命令的 IPI 处理函数，在内存屏障后调用 `rseq_preempt(current)`，以支持 restartable sequences（rseq）机制的正确性。\n\n- **`ipi_sync_rq_state(void *info)`**  \n  用于同步 per-CPU runqueue 的 `membarrier_state` 字段，使其与指定 `mm_struct` 的状态一致，确保后续 `membarrier` 调用能正确识别注册状态。\n\n- **`membarrier_exec_mmap(struct mm_struct *mm)`**  \n  在进程执行 `exec` 系统调用时被调用，重置该内存描述符（`mm_struct`）的 `membarrier_state` 为 0，并同步 per-CPU runqueue 状态，防止 exec 后残留旧的注册状态。\n\n### 数据结构与宏\n\n- **`MEMBARRIER_CMD_BITMASK`**  \n  定义所有支持的 `membarrier` 命令的位掩码（不含 `QUERY`），用于命令合法性校验。\n\n- **`membarrier_ipi_mutex`**  \n  互斥锁，用于序列化 IPI 发送过程，防止多个 `membarrier` 调用并发执行导致 IPI 风暴或状态不一致。\n\n- **`SERIALIZE_IPI()`**  \n  宏封装，使用 `membarrier_ipi_mutex` 实现 IPI 发送的串行化。\n\n## 3. 关键实现\n\n### 内存屏障语义保证\n\n文件顶部的注释详细描述了五种关键内存顺序场景（A–E），说明为何在 `membarrier()` 调用前后必须插入 `smp_mb()`：\n\n- **场景 A**：确保调用者 CPU 在 `membarrier()` 之前的写操作，在其他 CPU 收到 IPI 并执行屏障后对其可见。\n- **场景 B**：确保其他 CPU 在 IPI 屏障前的写操作，在调用者 CPU 执行 `membarrier()` 后对其可见。\n- **场景 C–E**：处理线程切换、`exit_mm`、kthread 使用/释放 mm 等边界情况，确保 `membarrier` 能正确识别用户态上下文并施加屏障。\n\n这些场景共同要求 `membarrier()` 实现必须在发送 IPI **前**和**后**各执行一次 `smp_mb()`，以建立完整的全局内存顺序。\n\n### IPI 分发机制\n\n- 根据不同的 `membarrier` 命令类型（如全局、私有、带 rseq 或 sync_core），选择对应的 IPI 处理函数。\n- 使用 `mutex` 保护 IPI 发送过程，避免并发调用导致性能下降或状态竞争。\n- 对于私有命令（如 `PRIVATE_EXPEDITED`），仅向共享同一 `mm_struct` 的 CPU 发送 IPI。\n\n### 状态管理\n\n- 每个 `mm_struct` 包含一个 `membarrier_state` 原子变量，记录该地址空间已注册的 `membarrier` 命令类型。\n- 每个 per-CPU runqueue 也缓存一份 `membarrier_state`，通过 `ipi_sync_rq_state` 保持与 `mm_struct` 同步，加速后续命令的判断。\n- `exec` 时调用 `membarrier_exec_mmap` 重置状态，防止子进程继承父进程的注册状态。\n\n### 条件编译支持\n\n- `CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE`：启用 `SYNC_CORE` 相关命令。\n- `CONFIG_RSEQ`：启用 `RSEQ` 相关命令及 `rseq_preempt` 调用。\n\n## 4. 依赖关系\n\n- **调度子系统（sched）**：依赖 runqueue（`rq`）结构和 CPU 上下文切换逻辑，用于判断当前是否处于用户态及 mm 匹配。\n- **内存管理（mm）**：依赖 `mm_struct` 及其生命周期管理（如 `exec_mmap`、`exit_mm`）。\n- **RSEQ 子系统**：当启用 `CONFIG_RSEQ` 时，调用 `rseq_preempt()` 以维护 restartable sequences 的一致性。\n- **SMP 原语**：依赖 `smp_mb()`、`smp_call_function_many()` 等 SMP 内存屏障和 IPI 接口。\n- **架构支持**：部分命令（如 `SYNC_CORE`）依赖特定架构实现 `sync_core_before_usermode()`。\n\n## 5. 使用场景\n\n- **用户空间无锁编程**：应用程序使用 `membarrier(SYS_MEMBARRIER_CMD_GLOBAL_EXPEDITED)` 替代在每个读线程中插入 `smp_load_acquire()`，简化代码并提升性能。\n- **RSEQ（Restartable Sequences）**：配合 `membarrier(SYS_MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ)` 确保在抢占或迁移后 rseq 区域的原子性。\n- **实时或低延迟系统**：通过私有命令（`PRIVATE_EXPEDITED`）仅对特定进程组施加屏障，减少系统范围开销。\n- **动态代码生成/热更新**：使用 `SYNC_CORE` 命令确保指令缓存一致性，适用于 JIT 编译器等场景。\n- **进程生命周期管理**：在 `exec` 时自动清理 `membarrier` 注册状态，保证新程序映像的干净执行环境。",
      "similarity": 0.6002790331840515,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 168,
          "end_line": 306,
          "content": [
            "static void ipi_mb(void *info)",
            "{",
            "\tsmp_mb();\t/* IPIs should be serializing but paranoid. */",
            "}",
            "static void ipi_sync_core(void *info)",
            "{",
            "\t/*",
            "\t * The smp_mb() in membarrier after all the IPIs is supposed to",
            "\t * ensure that memory on remote CPUs that occur before the IPI",
            "\t * become visible to membarrier()'s caller -- see scenario B in",
            "\t * the big comment at the top of this file.",
            "\t *",
            "\t * A sync_core() would provide this guarantee, but",
            "\t * sync_core_before_usermode() might end up being deferred until",
            "\t * after membarrier()'s smp_mb().",
            "\t */",
            "\tsmp_mb();\t/* IPIs should be serializing but paranoid. */",
            "",
            "\tsync_core_before_usermode();",
            "}",
            "static void ipi_rseq(void *info)",
            "{",
            "\t/*",
            "\t * Ensure that all stores done by the calling thread are visible",
            "\t * to the current task before the current task resumes.  We could",
            "\t * probably optimize this away on most architectures, but by the",
            "\t * time we've already sent an IPI, the cost of the extra smp_mb()",
            "\t * is negligible.",
            "\t */",
            "\tsmp_mb();",
            "\trseq_preempt(current);",
            "}",
            "static void ipi_sync_rq_state(void *info)",
            "{",
            "\tstruct mm_struct *mm = (struct mm_struct *) info;",
            "",
            "\tif (current->mm != mm)",
            "\t\treturn;",
            "\tthis_cpu_write(runqueues.membarrier_state,",
            "\t\t       atomic_read(&mm->membarrier_state));",
            "\t/*",
            "\t * Issue a memory barrier after setting",
            "\t * MEMBARRIER_STATE_GLOBAL_EXPEDITED in the current runqueue to",
            "\t * guarantee that no memory access following registration is reordered",
            "\t * before registration.",
            "\t */",
            "\tsmp_mb();",
            "}",
            "void membarrier_exec_mmap(struct mm_struct *mm)",
            "{",
            "\t/*",
            "\t * Issue a memory barrier before clearing membarrier_state to",
            "\t * guarantee that no memory access prior to exec is reordered after",
            "\t * clearing this state.",
            "\t */",
            "\tsmp_mb();",
            "\tatomic_set(&mm->membarrier_state, 0);",
            "\t/*",
            "\t * Keep the runqueue membarrier_state in sync with this mm",
            "\t * membarrier_state.",
            "\t */",
            "\tthis_cpu_write(runqueues.membarrier_state, 0);",
            "}",
            "void membarrier_update_current_mm(struct mm_struct *next_mm)",
            "{",
            "\tstruct rq *rq = this_rq();",
            "\tint membarrier_state = 0;",
            "",
            "\tif (next_mm)",
            "\t\tmembarrier_state = atomic_read(&next_mm->membarrier_state);",
            "\tif (READ_ONCE(rq->membarrier_state) == membarrier_state)",
            "\t\treturn;",
            "\tWRITE_ONCE(rq->membarrier_state, membarrier_state);",
            "}",
            "static int membarrier_global_expedited(void)",
            "{",
            "\tint cpu;",
            "\tcpumask_var_t tmpmask;",
            "",
            "\tif (num_online_cpus() == 1)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Matches memory barriers after rq->curr modification in",
            "\t * scheduler.",
            "\t */",
            "\tsmp_mb();\t/* system call entry is not a mb. */",
            "",
            "\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "\trcu_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\t/*",
            "\t\t * Skipping the current CPU is OK even through we can be",
            "\t\t * migrated at any point. The current CPU, at the point",
            "\t\t * where we read raw_smp_processor_id(), is ensured to",
            "\t\t * be in program order with respect to the caller",
            "\t\t * thread. Therefore, we can skip this CPU from the",
            "\t\t * iteration.",
            "\t\t */",
            "\t\tif (cpu == raw_smp_processor_id())",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!(READ_ONCE(cpu_rq(cpu)->membarrier_state) &",
            "\t\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * Skip the CPU if it runs a kernel thread which is not using",
            "\t\t * a task mm.",
            "\t\t */",
            "\t\tp = rcu_dereference(cpu_rq(cpu)->curr);",
            "\t\tif (!p->mm)",
            "\t\t\tcontinue;",
            "",
            "\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tpreempt_disable();",
            "\tsmp_call_function_many(tmpmask, ipi_mb, NULL, 1);",
            "\tpreempt_enable();",
            "",
            "\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\t/*",
            "\t * Memory barrier on the caller thread _after_ we finished",
            "\t * waiting for the last IPI. Matches memory barriers before",
            "\t * rq->curr modification in scheduler.",
            "\t */",
            "\tsmp_mb();\t/* exit from system call is not a mb */",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ipi_mb, ipi_sync_core, ipi_rseq, ipi_sync_rq_state, membarrier_exec_mmap, membarrier_update_current_mm, membarrier_global_expedited",
          "description": "实现多种IPI处理函数及全局快速内存屏障逻辑，通过跨CPU调用来确保内存访问顺序一致性。",
          "similarity": 0.5475163459777832
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 436,
          "end_line": 551,
          "content": [
            "static int sync_runqueues_membarrier_state(struct mm_struct *mm)",
            "{",
            "\tint membarrier_state = atomic_read(&mm->membarrier_state);",
            "\tcpumask_var_t tmpmask;",
            "\tint cpu;",
            "",
            "\tif (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1) {",
            "\t\tthis_cpu_write(runqueues.membarrier_state, membarrier_state);",
            "",
            "\t\t/*",
            "\t\t * For single mm user, we can simply issue a memory barrier",
            "\t\t * after setting MEMBARRIER_STATE_GLOBAL_EXPEDITED in the",
            "\t\t * mm and in the current runqueue to guarantee that no memory",
            "\t\t * access following registration is reordered before",
            "\t\t * registration.",
            "\t\t */",
            "\t\tsmp_mb();",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * For mm with multiple users, we need to ensure all future",
            "\t * scheduler executions will observe @mm's new membarrier",
            "\t * state.",
            "\t */",
            "\tsynchronize_rcu();",
            "",
            "\t/*",
            "\t * For each cpu runqueue, if the task's mm match @mm, ensure that all",
            "\t * @mm's membarrier state set bits are also set in the runqueue's",
            "\t * membarrier state. This ensures that a runqueue scheduling",
            "\t * between threads which are users of @mm has its membarrier state",
            "\t * updated.",
            "\t */",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "\trcu_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct rq *rq = cpu_rq(cpu);",
            "\t\tstruct task_struct *p;",
            "",
            "\t\tp = rcu_dereference(rq->curr);",
            "\t\tif (p && p->mm == mm)",
            "\t\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\ton_each_cpu_mask(tmpmask, ipi_sync_rq_state, mm, true);",
            "",
            "\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\treturn 0;",
            "}",
            "static int membarrier_register_global_expedited(void)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint ret;",
            "",
            "\tif (atomic_read(&mm->membarrier_state) &",
            "\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)",
            "\t\treturn 0;",
            "\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);",
            "\tret = sync_runqueues_membarrier_state(mm);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,",
            "\t\t  &mm->membarrier_state);",
            "",
            "\treturn 0;",
            "}",
            "static int membarrier_register_private_expedited(int flags)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint ready_state = MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY,",
            "\t    set_state = MEMBARRIER_STATE_PRIVATE_EXPEDITED,",
            "\t    ret;",
            "",
            "\tif (flags == MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))",
            "\t\t\treturn -EINVAL;",
            "\t\tready_state =",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY;",
            "\t} else if (flags == MEMBARRIER_FLAG_RSEQ) {",
            "\t\tif (!IS_ENABLED(CONFIG_RSEQ))",
            "\t\t\treturn -EINVAL;",
            "\t\tready_state =",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY;",
            "\t} else {",
            "\t\tWARN_ON_ONCE(flags);",
            "\t}",
            "",
            "\t/*",
            "\t * We need to consider threads belonging to different thread",
            "\t * groups, which use the same mm. (CLONE_VM but not",
            "\t * CLONE_THREAD).",
            "\t */",
            "\tif ((atomic_read(&mm->membarrier_state) & ready_state) == ready_state)",
            "\t\treturn 0;",
            "\tif (flags & MEMBARRIER_FLAG_SYNC_CORE)",
            "\t\tset_state |= MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE;",
            "\tif (flags & MEMBARRIER_FLAG_RSEQ)",
            "\t\tset_state |= MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ;",
            "\tatomic_or(set_state, &mm->membarrier_state);",
            "\tret = sync_runqueues_membarrier_state(mm);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tatomic_or(ready_state, &mm->membarrier_state);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sync_runqueues_membarrier_state, membarrier_register_global_expedited, membarrier_register_private_expedited",
          "description": "同步运行队列内存屏障状态，通过RCU和IPI确保多用户场景下状态传播的正确性。",
          "similarity": 0.5312885046005249
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 314,
          "end_line": 434,
          "content": [
            "static int membarrier_private_expedited(int flags, int cpu_id)",
            "{",
            "\tcpumask_var_t tmpmask;",
            "\tstruct mm_struct *mm = current->mm;",
            "\tsmp_call_func_t ipi_func = ipi_mb;",
            "",
            "\tif (flags == MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY))",
            "\t\t\treturn -EPERM;",
            "\t\tipi_func = ipi_sync_core;",
            "\t\tprepare_sync_core_cmd(mm);",
            "\t} else if (flags == MEMBARRIER_FLAG_RSEQ) {",
            "\t\tif (!IS_ENABLED(CONFIG_RSEQ))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY))",
            "\t\t\treturn -EPERM;",
            "\t\tipi_func = ipi_rseq;",
            "\t} else {",
            "\t\tWARN_ON_ONCE(flags);",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY))",
            "\t\t\treturn -EPERM;",
            "\t}",
            "",
            "\tif (flags != MEMBARRIER_FLAG_SYNC_CORE &&",
            "\t    (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Matches memory barriers after rq->curr modification in",
            "\t * scheduler.",
            "\t *",
            "\t * On RISC-V, this barrier pairing is also needed for the",
            "\t * SYNC_CORE command when switching between processes, cf.",
            "\t * the inline comments in membarrier_arch_switch_mm().",
            "\t */",
            "\tsmp_mb();\t/* system call entry is not a mb. */",
            "",
            "\tif (cpu_id < 0 && !zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "",
            "\tif (cpu_id >= 0) {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\tif (cpu_id >= nr_cpu_ids || !cpu_online(cpu_id))",
            "\t\t\tgoto out;",
            "\t\trcu_read_lock();",
            "\t\tp = rcu_dereference(cpu_rq(cpu_id)->curr);",
            "\t\tif (!p || p->mm != mm) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tint cpu;",
            "",
            "\t\trcu_read_lock();",
            "\t\tfor_each_online_cpu(cpu) {",
            "\t\t\tstruct task_struct *p;",
            "",
            "\t\t\tp = rcu_dereference(cpu_rq(cpu)->curr);",
            "\t\t\tif (p && p->mm == mm)",
            "\t\t\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t}",
            "",
            "\tif (cpu_id >= 0) {",
            "\t\t/*",
            "\t\t * smp_call_function_single() will call ipi_func() if cpu_id",
            "\t\t * is the calling CPU.",
            "\t\t */",
            "\t\tsmp_call_function_single(cpu_id, ipi_func, NULL, 1);",
            "\t} else {",
            "\t\t/*",
            "\t\t * For regular membarrier, we can save a few cycles by",
            "\t\t * skipping the current cpu -- we're about to do smp_mb()",
            "\t\t * below, and if we migrate to a different cpu, this cpu",
            "\t\t * and the new cpu will execute a full barrier in the",
            "\t\t * scheduler.",
            "\t\t *",
            "\t\t * For SYNC_CORE, we do need a barrier on the current cpu --",
            "\t\t * otherwise, if we are migrated and replaced by a different",
            "\t\t * task in the same mm just before, during, or after",
            "\t\t * membarrier, we will end up with some thread in the mm",
            "\t\t * running without a core sync.",
            "\t\t *",
            "\t\t * For RSEQ, don't rseq_preempt() the caller.  User code",
            "\t\t * is not supposed to issue syscalls at all from inside an",
            "\t\t * rseq critical section.",
            "\t\t */",
            "\t\tif (flags != MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\t\tpreempt_disable();",
            "\t\t\tsmp_call_function_many(tmpmask, ipi_func, NULL, true);",
            "\t\t\tpreempt_enable();",
            "\t\t} else {",
            "\t\t\ton_each_cpu_mask(tmpmask, ipi_func, NULL, true);",
            "\t\t}",
            "\t}",
            "",
            "out:",
            "\tif (cpu_id < 0)",
            "\t\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\t/*",
            "\t * Memory barrier on the caller thread _after_ we finished",
            "\t * waiting for the last IPI. Matches memory barriers before",
            "\t * rq->curr modification in scheduler.",
            "\t */",
            "\tsmp_mb();\t/* exit from system call is not a mb */",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "membarrier_private_expedited",
          "description": "处理私有快速内存屏障，根据配置标志选择不同同步方式并验证状态有效性。",
          "similarity": 0.5199912190437317
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 555,
          "end_line": 587,
          "content": [
            "static int membarrier_get_registrations(void)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint registrations_mask = 0, membarrier_state, i;",
            "\tstatic const int states[] = {",
            "\t\tMEMBARRIER_STATE_GLOBAL_EXPEDITED |",
            "\t\t\tMEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY",
            "\t};",
            "\tstatic const int registration_cmds[] = {",
            "\t\tMEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ",
            "\t};",
            "\tBUILD_BUG_ON(ARRAY_SIZE(states) != ARRAY_SIZE(registration_cmds));",
            "",
            "\tmembarrier_state = atomic_read(&mm->membarrier_state);",
            "\tfor (i = 0; i < ARRAY_SIZE(states); ++i) {",
            "\t\tif (membarrier_state & states[i]) {",
            "\t\t\tregistrations_mask |= registration_cmds[i];",
            "\t\t\tmembarrier_state &= ~states[i];",
            "\t\t}",
            "\t}",
            "\tWARN_ON_ONCE(membarrier_state != 0);",
            "\treturn registrations_mask;",
            "}"
          ],
          "function_name": "membarrier_get_registrations",
          "description": "解析当前进程的内存屏障注册状态，将有效状态转换为对应的注册命令掩码。",
          "similarity": 0.5116687417030334
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 1,
          "end_line": 167,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Copyright (C) 2010-2017 Mathieu Desnoyers <mathieu.desnoyers@efficios.com>",
            " *",
            " * membarrier system call",
            " */",
            "",
            "/*",
            " * For documentation purposes, here are some membarrier ordering",
            " * scenarios to keep in mind:",
            " *",
            " * A) Userspace thread execution after IPI vs membarrier's memory",
            " *    barrier before sending the IPI",
            " *",
            " * Userspace variables:",
            " *",
            " * int x = 0, y = 0;",
            " *",
            " * The memory barrier at the start of membarrier() on CPU0 is necessary in",
            " * order to enforce the guarantee that any writes occurring on CPU0 before",
            " * the membarrier() is executed will be visible to any code executing on",
            " * CPU1 after the IPI-induced memory barrier:",
            " *",
            " *         CPU0                              CPU1",
            " *",
            " *         x = 1",
            " *         membarrier():",
            " *           a: smp_mb()",
            " *           b: send IPI                       IPI-induced mb",
            " *           c: smp_mb()",
            " *         r2 = y",
            " *                                           y = 1",
            " *                                           barrier()",
            " *                                           r1 = x",
            " *",
            " *                     BUG_ON(r1 == 0 && r2 == 0)",
            " *",
            " * The write to y and load from x by CPU1 are unordered by the hardware,",
            " * so it's possible to have \"r1 = x\" reordered before \"y = 1\" at any",
            " * point after (b).  If the memory barrier at (a) is omitted, then \"x = 1\"",
            " * can be reordered after (a) (although not after (c)), so we get r1 == 0",
            " * and r2 == 0.  This violates the guarantee that membarrier() is",
            " * supposed by provide.",
            " *",
            " * The timing of the memory barrier at (a) has to ensure that it executes",
            " * before the IPI-induced memory barrier on CPU1.",
            " *",
            " * B) Userspace thread execution before IPI vs membarrier's memory",
            " *    barrier after completing the IPI",
            " *",
            " * Userspace variables:",
            " *",
            " * int x = 0, y = 0;",
            " *",
            " * The memory barrier at the end of membarrier() on CPU0 is necessary in",
            " * order to enforce the guarantee that any writes occurring on CPU1 before",
            " * the membarrier() is executed will be visible to any code executing on",
            " * CPU0 after the membarrier():",
            " *",
            " *         CPU0                              CPU1",
            " *",
            " *                                           x = 1",
            " *                                           barrier()",
            " *                                           y = 1",
            " *         r2 = y",
            " *         membarrier():",
            " *           a: smp_mb()",
            " *           b: send IPI                       IPI-induced mb",
            " *           c: smp_mb()",
            " *         r1 = x",
            " *         BUG_ON(r1 == 0 && r2 == 1)",
            " *",
            " * The writes to x and y are unordered by the hardware, so it's possible to",
            " * have \"r2 = 1\" even though the write to x doesn't execute until (b).  If",
            " * the memory barrier at (c) is omitted then \"r1 = x\" can be reordered",
            " * before (b) (although not before (a)), so we get \"r1 = 0\".  This violates",
            " * the guarantee that membarrier() is supposed to provide.",
            " *",
            " * The timing of the memory barrier at (c) has to ensure that it executes",
            " * after the IPI-induced memory barrier on CPU1.",
            " *",
            " * C) Scheduling userspace thread -> kthread -> userspace thread vs membarrier",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *           a: smp_mb()",
            " *                                           d: switch to kthread (includes mb)",
            " *           b: read rq->curr->mm == NULL",
            " *                                           e: switch to user (includes mb)",
            " *           c: smp_mb()",
            " *",
            " * Using the scenario from (A), we can show that (a) needs to be paired",
            " * with (e). Using the scenario from (B), we can show that (c) needs to",
            " * be paired with (d).",
            " *",
            " * D) exit_mm vs membarrier",
            " *",
            " * Two thread groups are created, A and B.  Thread group B is created by",
            " * issuing clone from group A with flag CLONE_VM set, but not CLONE_THREAD.",
            " * Let's assume we have a single thread within each thread group (Thread A",
            " * and Thread B).  Thread A runs on CPU0, Thread B runs on CPU1.",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *             a: smp_mb()",
            " *                                           exit_mm():",
            " *                                             d: smp_mb()",
            " *                                             e: current->mm = NULL",
            " *             b: read rq->curr->mm == NULL",
            " *             c: smp_mb()",
            " *",
            " * Using scenario (B), we can show that (c) needs to be paired with (d).",
            " *",
            " * E) kthread_{use,unuse}_mm vs membarrier",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *           a: smp_mb()",
            " *                                           kthread_unuse_mm()",
            " *                                             d: smp_mb()",
            " *                                             e: current->mm = NULL",
            " *           b: read rq->curr->mm == NULL",
            " *                                           kthread_use_mm()",
            " *                                             f: current->mm = mm",
            " *                                             g: smp_mb()",
            " *           c: smp_mb()",
            " *",
            " * Using the scenario from (A), we can show that (a) needs to be paired",
            " * with (g). Using the scenario from (B), we can show that (c) needs to",
            " * be paired with (d).",
            " */",
            "",
            "/*",
            " * Bitmask made from a \"or\" of all commands within enum membarrier_cmd,",
            " * except MEMBARRIER_CMD_QUERY.",
            " */",
            "#ifdef CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t\t\t\\",
            "\t(MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE)",
            "#else",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t0",
            "#endif",
            "",
            "#ifdef CONFIG_RSEQ",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t\t\\",
            "\t(MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ)",
            "#else",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t0",
            "#endif",
            "",
            "#define MEMBARRIER_CMD_BITMASK\t\t\t\t\t\t\\",
            "\t(MEMBARRIER_CMD_GLOBAL | MEMBARRIER_CMD_GLOBAL_EXPEDITED\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED\t\t\t\\",
            "\t| MEMBARRIER_CMD_PRIVATE_EXPEDITED\t\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED\t\t\t\\",
            "\t| MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t\t\\",
            "\t| MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t\t\t\\",
            "\t| MEMBARRIER_CMD_GET_REGISTRATIONS)",
            "",
            "static DEFINE_MUTEX(membarrier_ipi_mutex);",
            "#define SERIALIZE_IPI() guard(mutex)(&membarrier_ipi_mutex)",
            ""
          ],
          "function_name": null,
          "description": "定义内存屏障命令位掩码和互斥锁，用于协调多处理器间内存顺序保证。",
          "similarity": 0.4436183571815491
        }
      ]
    }
  ]
}