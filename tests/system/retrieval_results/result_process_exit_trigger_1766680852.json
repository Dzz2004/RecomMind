{
  "query": "process exit trigger",
  "timestamp": "2025-12-26 00:40:52",
  "retrieved_files": [
    {
      "source_file": "kernel/exit.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:27:18\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `exit.c`\n\n---\n\n# `exit.c` 技术文档\n\n## 1. 文件概述\n\n`exit.c` 是 Linux 内核中负责进程退出（termination）核心逻辑的关键源文件，位于 `kernel/` 目录下。该文件实现了进程终止时的资源回收、信号处理、线程组清理、引用计数释放以及与用户空间和内核其他子系统的协调机制。其主要职责包括：\n\n- 安全地释放进程占用的内核资源（如内存、文件描述符、信号处理结构等）\n- 更新进程组和会话的统计信息\n- 通知父进程子进程已退出（通过 `SIGCHLD` 信号）\n- 管理僵尸进程（zombie）的生命周期\n- 支持线程组（thread group）的协同退出\n- 提供与 oops（内核异常）相关的计数和限制机制\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|---------|\n| `__unhash_process()` | 从内核的进程哈希表和链表中移除进程，减少线程计数 |\n| `__exit_signal()` | 清理进程的信号相关资源，累加 CPU 时间和 I/O 统计到 `signal_struct` |\n| `delayed_put_task_struct()` | RCU 回调函数，延迟释放 `task_struct` 及其关联资源 |\n| `put_task_struct_rcu_user()` | 安全地减少 `task_struct` 的 RCU 用户引用计数，并在为零时调度延迟释放 |\n| `release_thread()` | 架构相关的线程资源释放钩子（弱符号，可由架构代码覆盖） |\n| `release_task()` | 主进程释放入口函数，协调整个退出流程，包括通知父进程、释放资源等 |\n| `rcuwait_wake_up()` | 唤醒等待在 `rcuwait` 上的任务（代码片段未完整） |\n\n### 关键数据结构与变量\n\n| 名称 | 类型/说明 |\n|------|----------|\n| `oops_limit` | `unsigned int`，限制内核 oops 发生次数的阈值（默认 10000） |\n| `oops_count` | `atomic_t`，原子计数器，记录系统发生 oops 的总次数 |\n| `kern_exit_table` | `ctl_table`，用于 `/proc/sys/kernel/oops_limit` 的 sysctl 接口 |\n| `oops_count_attr` | `kobj_attribute`，用于 `/sys/kernel/oops_count` 的 sysfs 接口 |\n\n## 3. 关键实现\n\n### 进程退出流程\n\n1. **资源统计聚合**：  \n   在 `__exit_signal()` 中，将退出线程的 CPU 时间（`utime`/`stime`）、I/O 操作、上下文切换次数等统计信息累加到所属线程组的 `signal_struct` 中，确保即使线程组 leader 尚未退出，也能被 `wait4()` 等系统调用正确获取。\n\n2. **线程组协同退出**：  \n   - 若当前退出的是线程组 leader（`group_dead == true`），则清理整个线程组的 PID 类型（TGID、PGID、SID），并从全局任务链表中移除。\n   - 若非 leader，则仅减少线程组计数，并可能更新 `curr_target`（用于信号投递）。\n\n3. **僵尸进程处理**：  \n   在 `release_task()` 中，检查线程组 leader 是否已变为僵尸状态。若是且当前线程是最后一个成员，则调用 `do_notify_parent()` 通知其父进程。若父进程忽略 `SIGCHLD`，则直接将 leader 状态置为 `EXIT_DEAD` 并递归释放。\n\n4. **延迟释放机制**：  \n   通过 RCU（Read-Copy-Update）机制安全释放 `task_struct`。`put_task_struct_rcu_user()` 在引用计数归零时调用 `call_rcu()`，由 `delayed_put_task_struct()` 在 RCU 宽限期后执行实际释放，确保并发读取安全。\n\n5. **Oops 计数与限制**：  \n   提供 `oops_count`（只读）和 `oops_limit`（可调）两个接口，用于监控和限制内核异常次数，防止因频繁崩溃导致资源耗尽或引用计数溢出。\n\n### 锁与同步\n\n- **`tasklist_lock`**：写锁保护进程链表和 PID 哈希表的修改。\n- **`sighand->siglock`**：自旋锁保护信号处理结构。\n- **`signal->stats_lock`**：顺序锁（seqlock）保护线程组统计信息的聚合。\n- **RCU**：用于安全地延迟释放 `task_struct`，避免在遍历任务链表时访问已释放内存。\n\n## 4. 依赖关系\n\n`exit.c` 与内核多个子系统紧密耦合，主要依赖包括：\n\n- **调度器（SCHED）**：`<linux/sched/*.h>`，用于任务状态管理、CPU 时间统计、任务链表操作。\n- **内存管理（MM）**：`<linux/mm.h>`、`<linux/slab.h>`，用于内存释放和 slab 分配器交互。\n- **文件系统（VFS）**：`<linux/file.h>`、`<linux/fdtable.h>`、`<linux/fs_struct.h>`，用于关闭文件描述符和释放文件系统上下文。\n- **进程间通信（IPC）**：`<linux/shm.h>`、`<linux/posix-timers.h>`，用于清理共享内存和定时器资源。\n- **安全与审计**：`<linux/audit.h>`、`<linux/seccomp.h>`（通过 `seccomp_filter_release`），用于释放安全策略和审计上下文。\n- **cgroup 与资源控制**：`<linux/cgroup.h>`、`<linux/resource.h>`，用于资源计数释放和限制检查。\n- **跟踪与性能**：`<linux/perf_event.h>`、`<trace/events/sched.h>`，用于性能事件清理和调度跟踪点。\n- **架构相关代码**：`<asm/mmu_context.h>`、`release_thread()` 弱符号，允许架构层定制线程释放逻辑。\n\n## 5. 使用场景\n\n- **进程正常退出**：当用户程序调用 `exit()` 或 `exit_group()` 系统调用时，内核通过此文件执行清理。\n- **进程被信号终止**：如收到 `SIGKILL` 或 `SIGTERM` 后，内核调度退出路径。\n- **线程退出**：POSIX 线程（通过 `pthread_exit()` 或线程函数返回）触发 `release_task()` 清理单个线程。\n- **内核 Oops/panic 处理**：每次内核异常会递增 `oops_count`，用于监控系统稳定性。\n- **僵尸进程回收**：父进程调用 `wait()` 系列系统调用后，内核最终通过 `release_task()` 释放僵尸进程的内核结构。\n- **容器/命名空间退出**：在 PID 命名空间或 cgroup 中进程退出时，协调资源释放和通知机制。",
      "similarity": 0.6257239580154419,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "kernel/exit.c",
          "start_line": 791,
          "end_line": 942,
          "content": [
            "static void check_stack_usage(void)",
            "{",
            "\tstatic DEFINE_SPINLOCK(low_water_lock);",
            "\tstatic int lowest_to_date = THREAD_SIZE;",
            "\tunsigned long free;",
            "",
            "\tfree = stack_not_used(current);",
            "",
            "\tif (free >= lowest_to_date)",
            "\t\treturn;",
            "",
            "\tspin_lock(&low_water_lock);",
            "\tif (free < lowest_to_date) {",
            "\t\tpr_info(\"%s (%d) used greatest stack depth: %lu bytes left\\n\",",
            "\t\t\tcurrent->comm, task_pid_nr(current), free);",
            "\t\tlowest_to_date = free;",
            "\t}",
            "\tspin_unlock(&low_water_lock);",
            "}",
            "static inline void check_stack_usage(void) {}",
            "static void synchronize_group_exit(struct task_struct *tsk, long code)",
            "{",
            "\tstruct sighand_struct *sighand = tsk->sighand;",
            "\tstruct signal_struct *signal = tsk->signal;",
            "",
            "\tspin_lock_irq(&sighand->siglock);",
            "\tsignal->quick_threads--;",
            "\tif ((signal->quick_threads == 0) &&",
            "\t    !(signal->flags & SIGNAL_GROUP_EXIT)) {",
            "\t\tsignal->flags = SIGNAL_GROUP_EXIT;",
            "\t\tsignal->group_exit_code = code;",
            "\t\tsignal->group_stop_count = 0;",
            "\t}",
            "\tspin_unlock_irq(&sighand->siglock);",
            "}",
            "void __noreturn do_exit(long code)",
            "{",
            "\tstruct task_struct *tsk = current;",
            "\tint group_dead;",
            "",
            "\tWARN_ON(irqs_disabled());",
            "",
            "\tsynchronize_group_exit(tsk, code);",
            "",
            "\tWARN_ON(tsk->plug);",
            "",
            "\tkcov_task_exit(tsk);",
            "\tkmsan_task_exit(tsk);",
            "",
            "\tcoredump_task_exit(tsk);",
            "\tptrace_event(PTRACE_EVENT_EXIT, code);",
            "\tuser_events_exit(tsk);",
            "",
            "\tio_uring_files_cancel();",
            "\texit_signals(tsk);  /* sets PF_EXITING */",
            "",
            "\t/* sync mm's RSS info before statistics gathering */",
            "\tif (tsk->mm)",
            "\t\tsync_mm_rss(tsk->mm);",
            "\tacct_update_integrals(tsk);",
            "\tgroup_dead = atomic_dec_and_test(&tsk->signal->live);",
            "\tif (group_dead) {",
            "\t\t/*",
            "\t\t * If the last thread of global init has exited, panic",
            "\t\t * immediately to get a useable coredump.",
            "\t\t */",
            "\t\tif (unlikely(is_global_init(tsk)))",
            "\t\t\tpanic(\"Attempted to kill init! exitcode=0x%08x\\n\",",
            "\t\t\t\ttsk->signal->group_exit_code ?: (int)code);",
            "",
            "#ifdef CONFIG_POSIX_TIMERS",
            "\t\thrtimer_cancel(&tsk->signal->real_timer);",
            "\t\texit_itimers(tsk);",
            "#endif",
            "\t\tif (tsk->mm)",
            "\t\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);",
            "\t}",
            "\tacct_collect(code, group_dead);",
            "\tif (group_dead)",
            "\t\ttty_audit_exit();",
            "\taudit_free(tsk);",
            "",
            "\ttsk->exit_code = code;",
            "\ttaskstats_exit(tsk, group_dead);",
            "",
            "\t/*",
            "\t * Since sampling can touch ->mm, make sure to stop everything before we",
            "\t * tear it down.",
            "\t *",
            "\t * Also flushes inherited counters to the parent - before the parent",
            "\t * gets woken up by child-exit notifications.",
            "\t */",
            "\tperf_event_exit_task(tsk);",
            "",
            "\texit_mm();",
            "",
            "\tif (group_dead)",
            "\t\tacct_process();",
            "\ttrace_sched_process_exit(tsk);",
            "",
            "\texit_sem(tsk);",
            "\texit_shm(tsk);",
            "\texit_files(tsk);",
            "\texit_fs(tsk);",
            "\tif (group_dead)",
            "\t\tdisassociate_ctty(1);",
            "\texit_task_namespaces(tsk);",
            "\texit_task_work(tsk);",
            "\texit_thread(tsk);",
            "",
            "\tsched_autogroup_exit_task(tsk);",
            "\tcgroup_exit(tsk);",
            "",
            "\t/*",
            "\t * FIXME: do that only when needed, using sched_exit tracepoint",
            "\t */",
            "\tflush_ptrace_hw_breakpoint(tsk);",
            "",
            "\texit_tasks_rcu_start();",
            "\texit_notify(tsk, group_dead);",
            "\tproc_exit_connector(tsk);",
            "\tmpol_put_task_policy(tsk);",
            "#ifdef CONFIG_FUTEX",
            "\tif (unlikely(current->pi_state_cache))",
            "\t\tkfree(current->pi_state_cache);",
            "#endif",
            "\t/*",
            "\t * Make sure we are holding no locks:",
            "\t */",
            "\tdebug_check_no_locks_held();",
            "",
            "\tif (tsk->io_context)",
            "\t\texit_io_context(tsk);",
            "",
            "\tif (tsk->splice_pipe)",
            "\t\tfree_pipe_info(tsk->splice_pipe);",
            "",
            "\tif (tsk->task_frag.page)",
            "\t\tput_page(tsk->task_frag.page);",
            "",
            "\texit_task_stack_account(tsk);",
            "",
            "\tcheck_stack_usage();",
            "\tpreempt_disable();",
            "\tif (tsk->nr_dirtied)",
            "\t\t__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);",
            "\texit_rcu();",
            "\texit_tasks_rcu_finish();",
            "",
            "\tlockdep_free_task(tsk);",
            "\tdo_task_dead();",
            "}"
          ],
          "function_name": "check_stack_usage, check_stack_usage, synchronize_group_exit, do_exit",
          "description": "do_exit函数负责处理进程退出流程，包括同步线程组退出、释放资源、更新统计信息、清理内存映射、解除命名空间关联等操作。其中synchronize_group_exit用于减少信号量计数并标记线程组退出状态，check_stack_usage监控最大堆栈使用量。",
          "similarity": 0.6359246969223022
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/exit.c",
          "start_line": 948,
          "end_line": 1066,
          "content": [
            "void __noreturn make_task_dead(int signr)",
            "{",
            "\t/*",
            "\t * Take the task off the cpu after something catastrophic has",
            "\t * happened.",
            "\t *",
            "\t * We can get here from a kernel oops, sometimes with preemption off.",
            "\t * Start by checking for critical errors.",
            "\t * Then fix up important state like USER_DS and preemption.",
            "\t * Then do everything else.",
            "\t */",
            "\tstruct task_struct *tsk = current;",
            "\tunsigned int limit;",
            "",
            "\tif (unlikely(in_interrupt()))",
            "\t\tpanic(\"Aiee, killing interrupt handler!\");",
            "\tif (unlikely(!tsk->pid))",
            "\t\tpanic(\"Attempted to kill the idle task!\");",
            "",
            "\tif (unlikely(irqs_disabled())) {",
            "\t\tpr_info(\"note: %s[%d] exited with irqs disabled\\n\",",
            "\t\t\tcurrent->comm, task_pid_nr(current));",
            "\t\tlocal_irq_enable();",
            "\t}",
            "\tif (unlikely(in_atomic())) {",
            "\t\tpr_info(\"note: %s[%d] exited with preempt_count %d\\n\",",
            "\t\t\tcurrent->comm, task_pid_nr(current),",
            "\t\t\tpreempt_count());",
            "\t\tpreempt_count_set(PREEMPT_ENABLED);",
            "\t}",
            "",
            "\t/*",
            "\t * Every time the system oopses, if the oops happens while a reference",
            "\t * to an object was held, the reference leaks.",
            "\t * If the oops doesn't also leak memory, repeated oopsing can cause",
            "\t * reference counters to wrap around (if they're not using refcount_t).",
            "\t * This means that repeated oopsing can make unexploitable-looking bugs",
            "\t * exploitable through repeated oopsing.",
            "\t * To make sure this can't happen, place an upper bound on how often the",
            "\t * kernel may oops without panic().",
            "\t */",
            "\tlimit = READ_ONCE(oops_limit);",
            "\tif (atomic_inc_return(&oops_count) >= limit && limit)",
            "\t\tpanic(\"Oopsed too often (kernel.oops_limit is %d)\", limit);",
            "",
            "\t/*",
            "\t * We're taking recursive faults here in make_task_dead. Safest is to just",
            "\t * leave this task alone and wait for reboot.",
            "\t */",
            "\tif (unlikely(tsk->flags & PF_EXITING)) {",
            "\t\tpr_alert(\"Fixing recursive fault but reboot is needed!\\n\");",
            "\t\tfutex_exit_recursive(tsk);",
            "\t\ttsk->exit_state = EXIT_DEAD;",
            "\t\trefcount_inc(&tsk->rcu_users);",
            "\t\tdo_task_dead();",
            "\t}",
            "",
            "\tdo_exit(signr);",
            "}",
            "void __noreturn",
            "do_group_exit(int exit_code)",
            "{",
            "\tstruct signal_struct *sig = current->signal;",
            "",
            "\tif (sig->flags & SIGNAL_GROUP_EXIT)",
            "\t\texit_code = sig->group_exit_code;",
            "\telse if (sig->group_exec_task)",
            "\t\texit_code = 0;",
            "\telse {",
            "\t\tstruct sighand_struct *const sighand = current->sighand;",
            "",
            "\t\tspin_lock_irq(&sighand->siglock);",
            "\t\tif (sig->flags & SIGNAL_GROUP_EXIT)",
            "\t\t\t/* Another thread got here before we took the lock.  */",
            "\t\t\texit_code = sig->group_exit_code;",
            "\t\telse if (sig->group_exec_task)",
            "\t\t\texit_code = 0;",
            "\t\telse {",
            "\t\t\tsig->group_exit_code = exit_code;",
            "\t\t\tsig->flags = SIGNAL_GROUP_EXIT;",
            "\t\t\tzap_other_threads(current);",
            "\t\t}",
            "\t\tspin_unlock_irq(&sighand->siglock);",
            "\t}",
            "",
            "\tdo_exit(exit_code);",
            "\t/* NOTREACHED */",
            "}",
            "static int eligible_pid(struct wait_opts *wo, struct task_struct *p)",
            "{",
            "\treturn\two->wo_type == PIDTYPE_MAX ||",
            "\t\ttask_pid_type(p, wo->wo_type) == wo->wo_pid;",
            "}",
            "static int",
            "eligible_child(struct wait_opts *wo, bool ptrace, struct task_struct *p)",
            "{",
            "\tif (!eligible_pid(wo, p))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Wait for all children (clone and not) if __WALL is set or",
            "\t * if it is traced by us.",
            "\t */",
            "\tif (ptrace || (wo->wo_flags & __WALL))",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * Otherwise, wait for clone children *only* if __WCLONE is set;",
            "\t * otherwise, wait for non-clone children *only*.",
            "\t *",
            "\t * Note: a \"clone\" child here is one that reports to its parent",
            "\t * using a signal other than SIGCHLD, or a non-leader thread which",
            "\t * we can only see if it is traced by us.",
            "\t */",
            "\tif ((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))",
            "\t\treturn 0;",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "make_task_dead, do_group_exit, eligible_pid, eligible_child",
          "description": "make_task_dead处理致命错误导致的进程终止，通过do_exit完成退出流程；do_group_exit用于线程组统一退出，设置退出码并触发do_exit；eligible_pid和eligible_child用于过滤符合等待条件的子进程，根据PID类型和跟踪标志进行匹配。",
          "similarity": 0.5988143682479858
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/exit.c",
          "start_line": 546,
          "end_line": 686,
          "content": [
            "static void exit_mm(void)",
            "{",
            "\tstruct mm_struct *mm = current->mm;",
            "",
            "\texit_mm_release(current, mm);",
            "\tif (!mm)",
            "\t\treturn;",
            "\tsync_mm_rss(mm);",
            "\tmmap_read_lock(mm);",
            "\tmmgrab_lazy_tlb(mm);",
            "\tBUG_ON(mm != current->active_mm);",
            "\t/* more a memory barrier than a real lock */",
            "\ttask_lock(current);",
            "\t/*",
            "\t * When a thread stops operating on an address space, the loop",
            "\t * in membarrier_private_expedited() may not observe that",
            "\t * tsk->mm, and the loop in membarrier_global_expedited() may",
            "\t * not observe a MEMBARRIER_STATE_GLOBAL_EXPEDITED",
            "\t * rq->membarrier_state, so those would not issue an IPI.",
            "\t * Membarrier requires a memory barrier after accessing",
            "\t * user-space memory, before clearing tsk->mm or the",
            "\t * rq->membarrier_state.",
            "\t */",
            "\tsmp_mb__after_spinlock();",
            "\tlocal_irq_disable();",
            "\tcurrent->mm = NULL;",
            "\t#ifdef CONFIG_IEE",
            "\tiee_set_token_pgd(current, NULL);",
            "\t#endif",
            "\tmembarrier_update_current_mm(NULL);",
            "\tenter_lazy_tlb(mm, current);",
            "\tlocal_irq_enable();",
            "\ttask_unlock(current);",
            "\tmmap_read_unlock(mm);",
            "\tmm_update_next_owner(mm);",
            "\tmmput(mm);",
            "\tif (test_thread_flag(TIF_MEMDIE))",
            "\t\texit_oom_victim();",
            "}",
            "static void reparent_leader(struct task_struct *father, struct task_struct *p,",
            "\t\t\t\tstruct list_head *dead)",
            "{",
            "\tif (unlikely(p->exit_state == EXIT_DEAD))",
            "\t\treturn;",
            "",
            "\t/* We don't want people slaying init. */",
            "\tp->exit_signal = SIGCHLD;",
            "",
            "\t/* If it has exited notify the new parent about this child's death. */",
            "\tif (!p->ptrace &&",
            "\t    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {",
            "\t\tif (do_notify_parent(p, p->exit_signal)) {",
            "\t\t\tp->exit_state = EXIT_DEAD;",
            "\t\t\tlist_add(&p->ptrace_entry, dead);",
            "\t\t}",
            "\t}",
            "",
            "\tkill_orphaned_pgrp(p, father);",
            "}",
            "static void forget_original_parent(struct task_struct *father,",
            "\t\t\t\t\tstruct list_head *dead)",
            "{",
            "\tstruct task_struct *p, *t, *reaper;",
            "",
            "\tif (unlikely(!list_empty(&father->ptraced)))",
            "\t\texit_ptrace(father, dead);",
            "",
            "\t/* Can drop and reacquire tasklist_lock */",
            "\treaper = find_child_reaper(father, dead);",
            "\tif (list_empty(&father->children))",
            "\t\treturn;",
            "",
            "\treaper = find_new_reaper(father, reaper);",
            "\tlist_for_each_entry(p, &father->children, sibling) {",
            "\t\tfor_each_thread(p, t) {",
            "\t\t\tRCU_INIT_POINTER(t->real_parent, reaper);",
            "\t\t\tBUG_ON((!t->ptrace) != (rcu_access_pointer(t->parent) == father));",
            "\t\t\tif (likely(!t->ptrace))",
            "\t\t\t\tt->parent = t->real_parent;",
            "\t\t\tif (t->pdeath_signal)",
            "\t\t\t\tgroup_send_sig_info(t->pdeath_signal,",
            "\t\t\t\t\t\t    SEND_SIG_NOINFO, t,",
            "\t\t\t\t\t\t    PIDTYPE_TGID);",
            "\t\t}",
            "\t\t/*",
            "\t\t * If this is a threaded reparent there is no need to",
            "\t\t * notify anyone anything has happened.",
            "\t\t */",
            "\t\tif (!same_thread_group(reaper, father))",
            "\t\t\treparent_leader(father, p, dead);",
            "\t}",
            "\tlist_splice_tail_init(&father->children, &reaper->children);",
            "}",
            "static void exit_notify(struct task_struct *tsk, int group_dead)",
            "{",
            "\tbool autoreap;",
            "\tstruct task_struct *p, *n;",
            "\tLIST_HEAD(dead);",
            "",
            "\twrite_lock_irq(&tasklist_lock);",
            "\tforget_original_parent(tsk, &dead);",
            "",
            "\tif (group_dead)",
            "\t\tkill_orphaned_pgrp(tsk->group_leader, NULL);",
            "",
            "\ttsk->exit_state = EXIT_ZOMBIE;",
            "\t/*",
            "\t * sub-thread or delay_group_leader(), wake up the",
            "\t * PIDFD_THREAD waiters.",
            "\t */",
            "\tif (!thread_group_empty(tsk))",
            "\t\tdo_notify_pidfd(tsk);",
            "",
            "\tif (unlikely(tsk->ptrace)) {",
            "\t\tint sig = thread_group_leader(tsk) &&",
            "\t\t\t\tthread_group_empty(tsk) &&",
            "\t\t\t\t!ptrace_reparented(tsk) ?",
            "\t\t\ttsk->exit_signal : SIGCHLD;",
            "\t\tautoreap = do_notify_parent(tsk, sig);",
            "\t} else if (thread_group_leader(tsk)) {",
            "\t\tautoreap = thread_group_empty(tsk) &&",
            "\t\t\tdo_notify_parent(tsk, tsk->exit_signal);",
            "\t} else {",
            "\t\tautoreap = true;",
            "\t}",
            "",
            "\tif (autoreap) {",
            "\t\ttsk->exit_state = EXIT_DEAD;",
            "\t\tlist_add(&tsk->ptrace_entry, &dead);",
            "\t}",
            "",
            "\t/* mt-exec, de_thread() is waiting for group leader */",
            "\tif (unlikely(tsk->signal->notify_count < 0))",
            "\t\twake_up_process(tsk->signal->group_exec_task);",
            "\twrite_unlock_irq(&tasklist_lock);",
            "",
            "\tlist_for_each_entry_safe(p, n, &dead, ptrace_entry) {",
            "\t\tlist_del_init(&p->ptrace_entry);",
            "\t\trelease_task(p);",
            "\t}",
            "}"
          ],
          "function_name": "exit_mm, reparent_leader, forget_original_parent, exit_notify",
          "description": "完成内存映射释放、父进程重定位、原始父进程解除关联及进程退出状态通知流程。",
          "similarity": 0.5825579762458801
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/exit.c",
          "start_line": 1494,
          "end_line": 1580,
          "content": [
            "static int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)",
            "{",
            "\tstruct task_struct *p;",
            "",
            "\tlist_for_each_entry(p, &tsk->children, sibling) {",
            "\t\tint ret = wait_consider_task(wo, 0, p);",
            "",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)",
            "{",
            "\tstruct task_struct *p;",
            "",
            "\tlist_for_each_entry(p, &tsk->ptraced, ptrace_entry) {",
            "\t\tint ret = wait_consider_task(wo, 1, p);",
            "",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "bool pid_child_should_wake(struct wait_opts *wo, struct task_struct *p)",
            "{",
            "\tif (!eligible_pid(wo, p))",
            "\t\treturn false;",
            "",
            "\tif ((wo->wo_flags & __WNOTHREAD) && wo->child_wait.private != p->parent)",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static int child_wait_callback(wait_queue_entry_t *wait, unsigned mode,",
            "\t\t\t\tint sync, void *key)",
            "{",
            "\tstruct wait_opts *wo = container_of(wait, struct wait_opts,",
            "\t\t\t\t\t\tchild_wait);",
            "\tstruct task_struct *p = key;",
            "",
            "\tif (pid_child_should_wake(wo, p))",
            "\t\treturn default_wake_function(wait, mode, sync, key);",
            "",
            "\treturn 0;",
            "}",
            "void __wake_up_parent(struct task_struct *p, struct task_struct *parent)",
            "{",
            "\t__wake_up_sync_key(&parent->signal->wait_chldexit,",
            "\t\t\t   TASK_INTERRUPTIBLE, p);",
            "}",
            "static bool is_effectively_child(struct wait_opts *wo, bool ptrace,",
            "\t\t\t\t struct task_struct *target)",
            "{",
            "\tstruct task_struct *parent =",
            "\t\t!ptrace ? target->real_parent : target->parent;",
            "",
            "\treturn current == parent || (!(wo->wo_flags & __WNOTHREAD) &&",
            "\t\t\t\t     same_thread_group(current, parent));",
            "}",
            "static int do_wait_pid(struct wait_opts *wo)",
            "{",
            "\tbool ptrace;",
            "\tstruct task_struct *target;",
            "\tint retval;",
            "",
            "\tptrace = false;",
            "\ttarget = pid_task(wo->wo_pid, PIDTYPE_TGID);",
            "\tif (target && is_effectively_child(wo, ptrace, target)) {",
            "\t\tretval = wait_consider_task(wo, ptrace, target);",
            "\t\tif (retval)",
            "\t\t\treturn retval;",
            "\t}",
            "",
            "\tptrace = true;",
            "\ttarget = pid_task(wo->wo_pid, PIDTYPE_PID);",
            "\tif (target && target->ptrace &&",
            "\t    is_effectively_child(wo, ptrace, target)) {",
            "\t\tretval = wait_consider_task(wo, ptrace, target);",
            "\t\tif (retval)",
            "\t\t\treturn retval;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "do_wait_thread, ptrace_do_wait, pid_child_should_wake, child_wait_callback, __wake_up_parent, is_effectively_child, do_wait_pid",
          "description": "该代码段实现了父进程对子进程退出状态的等待逻辑，包含线程组子进程和被ptrace跟踪子进程的处理。do_wait_thread和ptrace_do_wait遍历子进程列表并调用wait_consider_task检查是否满足等待条件，而child_wait_callback与__wake_up_parent协同完成子进程退出时的唤醒机制。is_effectively_child用于判定当前进程是否为有效子进程，do_wait_pid则根据PID类型选择具体的目标子进程进行等待。",
          "similarity": 0.5665678977966309
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/exit.c",
          "start_line": 228,
          "end_line": 339,
          "content": [
            "void put_task_struct_rcu_user(struct task_struct *task)",
            "{",
            "\tif (refcount_dec_and_test(&task->rcu_users))",
            "\t\tcall_rcu(&task->rcu, delayed_put_task_struct);",
            "}",
            "void __weak release_thread(struct task_struct *dead_task)",
            "{",
            "}",
            "void release_task(struct task_struct *p)",
            "{",
            "\tstruct task_struct *leader;",
            "\tstruct pid *thread_pid;",
            "\tint zap_leader;",
            "repeat:",
            "\t/* don't need to get the RCU readlock here - the process is dead and",
            "\t * can't be modifying its own credentials. But shut RCU-lockdep up */",
            "\trcu_read_lock();",
            "\tdec_rlimit_ucounts(task_ucounts(p), UCOUNT_RLIMIT_NPROC, 1);",
            "\trcu_read_unlock();",
            "",
            "\tcgroup_release(p);",
            "",
            "\twrite_lock_irq(&tasklist_lock);",
            "\tptrace_release_task(p);",
            "\tthread_pid = get_pid(p->thread_pid);",
            "\t__exit_signal(p);",
            "",
            "\t/*",
            "\t * If we are the last non-leader member of the thread",
            "\t * group, and the leader is zombie, then notify the",
            "\t * group leader's parent process. (if it wants notification.)",
            "\t */",
            "\tzap_leader = 0;",
            "\tleader = p->group_leader;",
            "\tif (leader != p && thread_group_empty(leader)",
            "\t\t\t&& leader->exit_state == EXIT_ZOMBIE) {",
            "\t\t/*",
            "\t\t * If we were the last child thread and the leader has",
            "\t\t * exited already, and the leader's parent ignores SIGCHLD,",
            "\t\t * then we are the one who should release the leader.",
            "\t\t */",
            "\t\tzap_leader = do_notify_parent(leader, leader->exit_signal);",
            "\t\tif (zap_leader)",
            "\t\t\tleader->exit_state = EXIT_DEAD;",
            "\t}",
            "",
            "\twrite_unlock_irq(&tasklist_lock);",
            "\tseccomp_filter_release(p);",
            "\tproc_flush_pid(thread_pid);",
            "\tput_pid(thread_pid);",
            "\trelease_thread(p);",
            "\t/*",
            "\t * This task was already removed from the process/thread/pid lists",
            "\t * and lock_task_sighand(p) can't succeed. Nobody else can touch",
            "\t * ->pending or, if group dead, signal->shared_pending. We can call",
            "\t * flush_sigqueue() lockless.",
            "\t */",
            "\tflush_sigqueue(&p->pending);",
            "\tif (thread_group_leader(p))",
            "\t\tflush_sigqueue(&p->signal->shared_pending);",
            "",
            "\tput_task_struct_rcu_user(p);",
            "",
            "\tp = leader;",
            "\tif (unlikely(zap_leader))",
            "\t\tgoto repeat;",
            "}",
            "int rcuwait_wake_up(struct rcuwait *w)",
            "{",
            "\tint ret = 0;",
            "\tstruct task_struct *task;",
            "",
            "\trcu_read_lock();",
            "",
            "\t/*",
            "\t * Order condition vs @task, such that everything prior to the load",
            "\t * of @task is visible. This is the condition as to why the user called",
            "\t * rcuwait_wake() in the first place. Pairs with set_current_state()",
            "\t * barrier (A) in rcuwait_wait_event().",
            "\t *",
            "\t *    WAIT                WAKE",
            "\t *    [S] tsk = current\t  [S] cond = true",
            "\t *        MB (A)\t      MB (B)",
            "\t *    [L] cond\t\t  [L] tsk",
            "\t */",
            "\tsmp_mb(); /* (B) */",
            "",
            "\ttask = rcu_dereference(w->task);",
            "\tif (task)",
            "\t\tret = wake_up_process(task);",
            "\trcu_read_unlock();",
            "",
            "\treturn ret;",
            "}",
            "static int will_become_orphaned_pgrp(struct pid *pgrp,",
            "\t\t\t\t\tstruct task_struct *ignored_task)",
            "{",
            "\tstruct task_struct *p;",
            "",
            "\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {",
            "\t\tif ((p == ignored_task) ||",
            "\t\t    (p->exit_state && thread_group_empty(p)) ||",
            "\t\t    is_global_init(p->real_parent))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (task_pgrp(p->real_parent) != pgrp &&",
            "\t\t    task_session(p->real_parent) == task_session(p))",
            "\t\t\treturn 0;",
            "\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "put_task_struct_rcu_user, release_thread, release_task, rcuwait_wake_up, will_become_orphaned_pgrp",
          "description": "提供RCU安全的任务结构释放机制，处理线程组解关联、会话管理及条件唤醒操作。",
          "similarity": 0.556481122970581
        }
      ]
    },
    {
      "source_file": "kernel/sched/ext.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:08:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\ext.c`\n\n---\n\n# `sched/ext.c` 技术文档\n\n## 文件概述\n\n`sched/ext.c` 是 Linux 内核中 **BPF 可扩展调度器（sched_ext）** 的核心实现文件之一，定义了调度器与 BPF 程序交互所需的数据结构、常量和操作接口。该文件为用户空间通过 BPF 实现自定义调度策略提供了内核侧的框架支持，允许将任务调度逻辑完全委托给加载的 BPF 程序，同时保留与内核调度子系统的安全集成。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`struct sched_ext_ops`**  \n  BPF 调度器的操作函数表，包含调度器必须或可选实现的回调函数，如 `select_cpu`、`enqueue`、`dequeue`、`dispatch` 等，用于控制任务的 CPU 选择、入队、出队和分发逻辑。\n\n- **`struct scx_exit_info`**  \n  描述 BPF 调度器退出原因的结构体，包含退出类型（`kind`）、退出码（`exit_code`）、错误信息（`reason`、`msg`）、回溯栈（`bt`）和调试转储（`dump`）。\n\n- **`struct scx_init_task_args` / `scx_exit_task_args`**  \n  分别用于 `ops.init_task()` 和 `ops.exit_task()` 回调的参数容器，传递任务初始化/退出上下文（如是否由 fork 触发、所属 cgroup 等）。\n\n- **`struct scx_cpu_acquire_args` / `scx_cpu_release_args`**  \n  用于 CPU 获取/释放回调的参数结构，其中 `cpu_release` 包含抢占原因（如 RT/DL 任务抢占）和即将运行的任务。\n\n- **`struct scx_dump_ctx`**  \n  为调度器转储（dump）操作提供上下文信息，包括退出类型、时间戳等。\n\n### 关键枚举与常量\n\n- **`enum scx_exit_kind`**  \n  定义调度器退出的类别，如正常退出（`SCX_EXIT_DONE`）、用户/BPF/内核主动注销（`SCX_EXIT_UNREG*`）、系统请求（`SCX_EXIT_SYSRQ`）或运行时错误（`SCX_EXIT_ERROR*`）。\n\n- **`enum scx_exit_code`**  \n  定义 64 位退出码的位域格式，支持系统原因（如 `SCX_ECODE_RSN_HOTPLUG`）和系统动作（如 `SCX_ECODE_ACT_RESTART`），允许用户自定义退出上下文。\n\n- **`enum scx_ops_flags`**  \n  调度器操作标志，控制调度行为：\n  - `SCX_OPS_KEEP_BUILTIN_IDLE`：保留内建空闲跟踪\n  - `SCX_OPS_ENQ_LAST`：切片到期后仍无任务时重新入队\n  - `SCX_OPS_ENQ_EXITING`：由 BPF 处理退出中任务\n  - `SCX_OPS_SWITCH_PARTIAL`：仅调度 `SCHED_EXT` 策略任务\n  - `SCX_OPS_HAS_CGROUP_WEIGHT`：支持 cgroup cpu.weight\n\n- **调度器常量**  \n  如 `SCX_DSP_DFL_MAX_BATCH`（默认分发批大小）、`SCX_WATCHDOG_MAX_TIMEOUT`（看门狗超时）、`SCX_OPS_TASK_ITER_BATCH`（任务迭代锁释放批次）等，用于控制调度器内部行为。\n\n## 关键实现\n\n- **BPF 调度器生命周期管理**  \n  通过 `scx_exit_info` 和退出码机制，支持多种退出路径（用户、BPF、内核、SysRq、错误），并提供详细的诊断信息（回溯、消息、转储）。\n\n- **任务入队优化**  \n  在 `select_cpu` 中允许直接插入 DSQ（如本地 DSQ），跳过后续 `enqueue` 调用，减少调度开销；同时通过 `SCX_OPS_ENQ_EXITING` 标志处理退出中任务的调度问题，避免 RCU 停顿。\n\n- **CPU 抢占通知**  \n  通过 `scx_cpu_release_args` 向 BPF 调度器传递 CPU 被高优先级调度类（RT/DL/Stop）抢占的原因，便于调度器做出相应调整。\n\n- **cgroup 集成**  \n  支持 cgroup 调度（`CONFIG_EXT_GROUP_SCHED`），在任务加入 cgroup 时传递权重信息（`scx_cgroup_init_args`），并通过 `SCX_OPS_HAS_CGROUP_WEIGHT` 标志启用。\n\n- **安全与鲁棒性**  \n  内核侧跟踪 BPF 是否拥有任务，可忽略无效分发；任务迭代时定期释放锁（`SCX_OPS_TASK_ITER_BATCH`），防止 RCU/CSD 停顿；看门狗机制（`SCX_WATCHDOG_MAX_TIMEOUT`）检测任务卡死。\n\n## 依赖关系\n\n- **BPF 子系统**：通过 `#include <linux/bpf.h>` 依赖 BPF 基础设施，用于加载和验证调度器 BPF 程序。\n- **调度核心**：与 `kernel/sched/` 下的核心调度代码（如 `core.c`、`rt.c`、`dl.c`）交互，处理任务入队、CPU 选择和抢占。\n- **cgroup 子系统**：当启用 `CONFIG_EXT_GROUP_SCHED` 时，依赖 cgroup CPU 控制器获取任务权重和层级信息。\n- **RCU 与锁机制**：使用 `scx_tasks_lock` 保护任务迭代，需与 RCU 同步机制协调。\n\n## 使用场景\n\n- **自定义调度策略开发**：用户通过 BPF 实现特定工作负载的调度逻辑（如延迟敏感型、批处理优化、NUMA 感知等），并注册到 `sched_ext`。\n- **系统调试与监控**：利用 `ops.dump()` 和退出信息结构体，在调度器异常退出时收集诊断数据。\n- **混合调度部署**：通过 `SCX_OPS_SWITCH_PARTIAL` 标志，仅对部分任务（`SCHED_EXT`）启用 BPF 调度，其余任务仍由 CFS 处理。\n- **资源隔离与 QoS**：结合 cgroup 支持，为不同 cgroup 配置不同的调度行为和资源权重。\n- **内核调度实验平台**：作为安全的沙箱环境，测试新型调度算法而无需修改核心调度代码。",
      "similarity": 0.5821115970611572,
      "chunks": [
        {
          "chunk_id": 22,
          "file_path": "kernel/sched/ext.c",
          "start_line": 4366,
          "end_line": 4528,
          "content": [
            "static void free_exit_info(struct scx_exit_info *ei)",
            "{",
            "\tkfree(ei->dump);",
            "\tkfree(ei->msg);",
            "\tkfree(ei->bt);",
            "\tkfree(ei);",
            "}",
            "static void scx_ops_disable_workfn(struct kthread_work *work)",
            "{",
            "\tstruct scx_exit_info *ei = scx_exit_info;",
            "\tstruct scx_task_iter sti;",
            "\tstruct task_struct *p;",
            "\tstruct rhashtable_iter rht_iter;",
            "\tstruct scx_dispatch_q *dsq;",
            "\tint i, kind;",
            "",
            "\tkind = atomic_read(&scx_exit_kind);",
            "\twhile (true) {",
            "\t\t/*",
            "\t\t * NONE indicates that a new scx_ops has been registered since",
            "\t\t * disable was scheduled - don't kill the new ops. DONE",
            "\t\t * indicates that the ops has already been disabled.",
            "\t\t */",
            "\t\tif (kind == SCX_EXIT_NONE || kind == SCX_EXIT_DONE)",
            "\t\t\treturn;",
            "\t\tif (atomic_try_cmpxchg(&scx_exit_kind, &kind, SCX_EXIT_DONE))",
            "\t\t\tbreak;",
            "\t}",
            "\tei->kind = kind;",
            "\tei->reason = scx_exit_reason(ei->kind);",
            "",
            "\t/* guarantee forward progress by bypassing scx_ops */",
            "\tscx_ops_bypass(true);",
            "",
            "\tswitch (scx_ops_set_enable_state(SCX_OPS_DISABLING)) {",
            "\tcase SCX_OPS_DISABLING:",
            "\t\tWARN_ONCE(true, \"sched_ext: duplicate disabling instance?\");",
            "\t\tbreak;",
            "\tcase SCX_OPS_DISABLED:",
            "\t\tpr_warn(\"sched_ext: ops error detected without ops (%s)\\n\",",
            "\t\t\tscx_exit_info->msg);",
            "\t\tWARN_ON_ONCE(scx_ops_set_enable_state(SCX_OPS_DISABLED) !=",
            "\t\t\t     SCX_OPS_DISABLING);",
            "\t\tgoto done;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "",
            "\t/*",
            "\t * Here, every runnable task is guaranteed to make forward progress and",
            "\t * we can safely use blocking synchronization constructs. Actually",
            "\t * disable ops.",
            "\t */",
            "\tmutex_lock(&scx_ops_enable_mutex);",
            "",
            "\tstatic_branch_disable(&__scx_switched_all);",
            "\tWRITE_ONCE(scx_switching_all, false);",
            "",
            "\t/*",
            "\t * Shut down cgroup support before tasks so that the cgroup attach path",
            "\t * doesn't race against scx_ops_exit_task().",
            "\t */",
            "\tscx_cgroup_lock();",
            "\tscx_cgroup_exit();",
            "\tscx_cgroup_unlock();",
            "",
            "\t/*",
            "\t * The BPF scheduler is going away. All tasks including %TASK_DEAD ones",
            "\t * must be switched out and exited synchronously.",
            "\t */",
            "\tpercpu_down_write(&scx_fork_rwsem);",
            "",
            "\tscx_ops_init_task_enabled = false;",
            "",
            "\tscx_task_iter_start(&sti);",
            "\twhile ((p = scx_task_iter_next_locked(&sti))) {",
            "\t\tconst struct sched_class *old_class = p->sched_class;",
            "\t\tconst struct sched_class *new_class =",
            "\t\t\t__setscheduler_class(p->policy, p->prio);",
            "\t\tstruct sched_enq_and_set_ctx ctx;",
            "",
            "\t\tif (old_class != new_class && p->se.sched_delayed)",
            "\t\t\tdequeue_task(task_rq(p), p, DEQUEUE_SLEEP | DEQUEUE_DELAYED);",
            "",
            "\t\tsched_deq_and_put_task(p, DEQUEUE_SAVE | DEQUEUE_MOVE, &ctx);",
            "",
            "\t\tp->sched_class = new_class;",
            "\t\tcheck_class_changing(task_rq(p), p, old_class);",
            "",
            "\t\tsched_enq_and_set_task(&ctx);",
            "",
            "\t\tcheck_class_changed(task_rq(p), p, old_class, p->prio);",
            "\t\tscx_ops_exit_task(p);",
            "\t}",
            "\tscx_task_iter_stop(&sti);",
            "\tpercpu_up_write(&scx_fork_rwsem);",
            "",
            "\t/* no task is on scx, turn off all the switches and flush in-progress calls */",
            "\tstatic_branch_disable(&__scx_ops_enabled);",
            "\tfor (i = SCX_OPI_BEGIN; i < SCX_OPI_END; i++)",
            "\t\tstatic_branch_disable(&scx_has_op[i]);",
            "\tstatic_branch_disable(&scx_ops_enq_last);",
            "\tstatic_branch_disable(&scx_ops_enq_exiting);",
            "\tstatic_branch_disable(&scx_ops_cpu_preempt);",
            "\tstatic_branch_disable(&scx_builtin_idle_enabled);",
            "\tsynchronize_rcu();",
            "",
            "\tif (ei->kind >= SCX_EXIT_ERROR) {",
            "\t\tpr_err(\"sched_ext: BPF scheduler \\\"%s\\\" disabled (%s)\\n\",",
            "\t\t       scx_ops.name, ei->reason);",
            "",
            "\t\tif (ei->msg[0] != '\\0')",
            "\t\t\tpr_err(\"sched_ext: %s: %s\\n\", scx_ops.name, ei->msg);",
            "#ifdef CONFIG_STACKTRACE",
            "\t\tstack_trace_print(ei->bt, ei->bt_len, 2);",
            "#endif",
            "\t} else {",
            "\t\tpr_info(\"sched_ext: BPF scheduler \\\"%s\\\" disabled (%s)\\n\",",
            "\t\t\tscx_ops.name, ei->reason);",
            "\t}",
            "",
            "\tif (scx_ops.exit)",
            "\t\tSCX_CALL_OP(SCX_KF_UNLOCKED, exit, ei);",
            "",
            "\tcancel_delayed_work_sync(&scx_watchdog_work);",
            "",
            "\t/*",
            "\t * Delete the kobject from the hierarchy eagerly in addition to just",
            "\t * dropping a reference. Otherwise, if the object is deleted",
            "\t * asynchronously, sysfs could observe an object of the same name still",
            "\t * in the hierarchy when another scheduler is loaded.",
            "\t */",
            "\tkobject_del(scx_root_kobj);",
            "\tkobject_put(scx_root_kobj);",
            "\tscx_root_kobj = NULL;",
            "",
            "\tmemset(&scx_ops, 0, sizeof(scx_ops));",
            "",
            "\trhashtable_walk_enter(&dsq_hash, &rht_iter);",
            "\tdo {",
            "\t\trhashtable_walk_start(&rht_iter);",
            "",
            "\t\twhile ((dsq = rhashtable_walk_next(&rht_iter)) && !IS_ERR(dsq))",
            "\t\t\tdestroy_dsq(dsq->id);",
            "",
            "\t\trhashtable_walk_stop(&rht_iter);",
            "\t} while (dsq == ERR_PTR(-EAGAIN));",
            "\trhashtable_walk_exit(&rht_iter);",
            "",
            "\tfree_percpu(scx_dsp_ctx);",
            "\tscx_dsp_ctx = NULL;",
            "\tscx_dsp_max_batch = 0;",
            "",
            "\tfree_exit_info(scx_exit_info);",
            "\tscx_exit_info = NULL;",
            "",
            "\tmutex_unlock(&scx_ops_enable_mutex);",
            "",
            "\tWARN_ON_ONCE(scx_ops_set_enable_state(SCX_OPS_DISABLED) !=",
            "\t\t     SCX_OPS_DISABLING);",
            "done:",
            "\tscx_ops_bypass(false);",
            "}"
          ],
          "function_name": "free_exit_info, scx_ops_disable_workfn",
          "description": "该代码段包含两个函数：  \n1. `free_exit_info` 用于释放 `scx_exit_info` 结构体关联的动态内存（`dump`/`msg`/`bt`）及自身；  \n2. `scx_ops_disable_workfn` 核心功能为安全禁用 SCX 操作，通过原子操作标记状态、强制切换所有任务至新调度类、清理 DSQ 和 kobject 等资源，并最终释放 `scx_exit_info`。  \n\n`scx_ops_disable_workfn` 实现了 SCX 操作的有序停用逻辑，确保任务迁移、状态同步及资源回收的完整性，同时处理异常情况下的错误日志输出。",
          "similarity": 0.5733096599578857
        },
        {
          "chunk_id": 29,
          "file_path": "kernel/sched/ext.c",
          "start_line": 5695,
          "end_line": 5803,
          "content": [
            "static void kick_one_cpu_if_idle(s32 cpu, struct rq *this_rq)",
            "{",
            "\tstruct rq *rq = cpu_rq(cpu);",
            "\tunsigned long flags;",
            "",
            "\traw_spin_rq_lock_irqsave(rq, flags);",
            "",
            "\tif (!can_skip_idle_kick(rq) &&",
            "\t    (cpu_online(cpu) || cpu == cpu_of(this_rq)))",
            "\t\tresched_curr(rq);",
            "",
            "\traw_spin_rq_unlock_irqrestore(rq, flags);",
            "}",
            "static void kick_cpus_irq_workfn(struct irq_work *irq_work)",
            "{",
            "\tstruct rq *this_rq = this_rq();",
            "\tstruct scx_rq *this_scx = &this_rq->scx;",
            "\tunsigned long *pseqs = this_cpu_ptr(scx_kick_cpus_pnt_seqs);",
            "\tbool should_wait = false;",
            "\ts32 cpu;",
            "",
            "\tfor_each_cpu(cpu, this_scx->cpus_to_kick) {",
            "\t\tshould_wait |= kick_one_cpu(cpu, this_rq, pseqs);",
            "\t\tcpumask_clear_cpu(cpu, this_scx->cpus_to_kick);",
            "\t\tcpumask_clear_cpu(cpu, this_scx->cpus_to_kick_if_idle);",
            "\t}",
            "",
            "\tfor_each_cpu(cpu, this_scx->cpus_to_kick_if_idle) {",
            "\t\tkick_one_cpu_if_idle(cpu, this_rq);",
            "\t\tcpumask_clear_cpu(cpu, this_scx->cpus_to_kick_if_idle);",
            "\t}",
            "",
            "\tif (!should_wait)",
            "\t\treturn;",
            "",
            "\tfor_each_cpu(cpu, this_scx->cpus_to_wait) {",
            "\t\tunsigned long *wait_pnt_seq = &cpu_rq(cpu)->scx.pnt_seq;",
            "",
            "\t\tif (cpu != cpu_of(this_rq)) {",
            "\t\t\t/*",
            "\t\t\t * Pairs with smp_store_release() issued by this CPU in",
            "\t\t\t * switch_class() on the resched path.",
            "\t\t\t *",
            "\t\t\t * We busy-wait here to guarantee that no other task can",
            "\t\t\t * be scheduled on our core before the target CPU has",
            "\t\t\t * entered the resched path.",
            "\t\t\t */",
            "\t\t\twhile (smp_load_acquire(wait_pnt_seq) == pseqs[cpu])",
            "\t\t\t\tcpu_relax();",
            "\t\t}",
            "",
            "\t\tcpumask_clear_cpu(cpu, this_scx->cpus_to_wait);",
            "\t}",
            "}",
            "void print_scx_info(const char *log_lvl, struct task_struct *p)",
            "{",
            "\tenum scx_ops_enable_state state = scx_ops_enable_state();",
            "\tconst char *all = READ_ONCE(scx_switching_all) ? \"+all\" : \"\";",
            "\tchar runnable_at_buf[22] = \"?\";",
            "\tstruct sched_class *class;",
            "\tunsigned long runnable_at;",
            "",
            "\tif (state == SCX_OPS_DISABLED)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Carefully check if the task was running on sched_ext, and then",
            "\t * carefully copy the time it's been runnable, and its state.",
            "\t */",
            "\tif (copy_from_kernel_nofault(&class, &p->sched_class, sizeof(class)) ||",
            "\t    class != &ext_sched_class) {",
            "\t\tprintk(\"%sSched_ext: %s (%s%s)\", log_lvl, scx_ops.name,",
            "\t\t       scx_ops_enable_state_str[state], all);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!copy_from_kernel_nofault(&runnable_at, &p->scx.runnable_at,",
            "\t\t\t\t      sizeof(runnable_at)))",
            "\t\tscnprintf(runnable_at_buf, sizeof(runnable_at_buf), \"%+ldms\",",
            "\t\t\t  jiffies_delta_msecs(runnable_at, jiffies));",
            "",
            "\t/* print everything onto one line to conserve console space */",
            "\tprintk(\"%sSched_ext: %s (%s%s), task: runnable_at=%s\",",
            "\t       log_lvl, scx_ops.name, scx_ops_enable_state_str[state], all,",
            "\t       runnable_at_buf);",
            "}",
            "static int scx_pm_handler(struct notifier_block *nb, unsigned long event, void *ptr)",
            "{",
            "\t/*",
            "\t * SCX schedulers often have userspace components which are sometimes",
            "\t * involved in critial scheduling paths. PM operations involve freezing",
            "\t * userspace which can lead to scheduling misbehaviors including stalls.",
            "\t * Let's bypass while PM operations are in progress.",
            "\t */",
            "\tswitch (event) {",
            "\tcase PM_HIBERNATION_PREPARE:",
            "\tcase PM_SUSPEND_PREPARE:",
            "\tcase PM_RESTORE_PREPARE:",
            "\t\tscx_ops_bypass(true);",
            "\t\tbreak;",
            "\tcase PM_POST_HIBERNATION:",
            "\tcase PM_POST_SUSPEND:",
            "\tcase PM_POST_RESTORE:",
            "\t\tscx_ops_bypass(false);",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn NOTIFY_OK;",
            "}"
          ],
          "function_name": "kick_one_cpu_if_idle, kick_cpus_irq_workfn, print_scx_info, scx_pm_handler",
          "description": "kick_one_cpu_if_idle触发电压门控调度，kick_cpus_irq_workfn批量唤醒等待CPU，print_scx_info打印调试信息，scx_pm_handler处理电源管理事件通知。",
          "similarity": 0.5622421503067017
        },
        {
          "chunk_id": 16,
          "file_path": "kernel/sched/ext.c",
          "start_line": 3513,
          "end_line": 3625,
          "content": [
            "static void scx_ops_disable_task(struct task_struct *p)",
            "{",
            "\tlockdep_assert_rq_held(task_rq(p));",
            "\tWARN_ON_ONCE(scx_get_task_state(p) != SCX_TASK_ENABLED);",
            "",
            "\tif (SCX_HAS_OP(disable))",
            "\t\tSCX_CALL_OP(SCX_KF_REST, disable, p);",
            "\tscx_set_task_state(p, SCX_TASK_READY);",
            "}",
            "static void scx_ops_exit_task(struct task_struct *p)",
            "{",
            "\tstruct scx_exit_task_args args = {",
            "\t\t.cancelled = false,",
            "\t};",
            "",
            "\tlockdep_assert_rq_held(task_rq(p));",
            "",
            "\tswitch (scx_get_task_state(p)) {",
            "\tcase SCX_TASK_NONE:",
            "\t\treturn;",
            "\tcase SCX_TASK_INIT:",
            "\t\targs.cancelled = true;",
            "\t\tbreak;",
            "\tcase SCX_TASK_READY:",
            "\t\tbreak;",
            "\tcase SCX_TASK_ENABLED:",
            "\t\tscx_ops_disable_task(p);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(true);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (SCX_HAS_OP(exit_task))",
            "\t\tSCX_CALL_OP(SCX_KF_REST, exit_task, p, &args);",
            "\tscx_set_task_state(p, SCX_TASK_NONE);",
            "}",
            "void init_scx_entity(struct sched_ext_entity *scx)",
            "{",
            "\t/*",
            "\t * init_idle() calls this function again after fork sequence is",
            "\t * complete. Don't touch ->tasks_node as it's already linked.",
            "\t */",
            "\tmemset(scx, 0, offsetof(struct sched_ext_entity, tasks_node));",
            "",
            "\tINIT_LIST_HEAD(&scx->dsq_list.node);",
            "\tRB_CLEAR_NODE(&scx->dsq_priq);",
            "\tscx->sticky_cpu = -1;",
            "\tscx->holding_cpu = -1;",
            "\tINIT_LIST_HEAD(&scx->runnable_node);",
            "\tscx->runnable_at = jiffies;",
            "\tscx->ddsp_dsq_id = SCX_DSQ_INVALID;",
            "\tscx->slice = SCX_SLICE_DFL;",
            "}",
            "void scx_pre_fork(struct task_struct *p)",
            "{",
            "\t/*",
            "\t * BPF scheduler enable/disable paths want to be able to iterate and",
            "\t * update all tasks which can become complex when racing forks. As",
            "\t * enable/disable are very cold paths, let's use a percpu_rwsem to",
            "\t * exclude forks.",
            "\t */",
            "\tpercpu_down_read(&scx_fork_rwsem);",
            "}",
            "int scx_fork(struct task_struct *p)",
            "{",
            "\tpercpu_rwsem_assert_held(&scx_fork_rwsem);",
            "",
            "\tif (scx_ops_init_task_enabled)",
            "\t\treturn scx_ops_init_task(p, task_group(p), true);",
            "\telse",
            "\t\treturn 0;",
            "}",
            "void scx_post_fork(struct task_struct *p)",
            "{",
            "\tif (scx_ops_init_task_enabled) {",
            "\t\tscx_set_task_state(p, SCX_TASK_READY);",
            "",
            "\t\t/*",
            "\t\t * Enable the task immediately if it's running on sched_ext.",
            "\t\t * Otherwise, it'll be enabled in switching_to_scx() if and",
            "\t\t * when it's ever configured to run with a SCHED_EXT policy.",
            "\t\t */",
            "\t\tif (p->sched_class == &ext_sched_class) {",
            "\t\t\tstruct rq_flags rf;",
            "\t\t\tstruct rq *rq;",
            "",
            "\t\t\trq = task_rq_lock(p, &rf);",
            "\t\t\tscx_ops_enable_task(p);",
            "\t\t\ttask_rq_unlock(rq, p, &rf);",
            "\t\t}",
            "\t}",
            "",
            "\tspin_lock_irq(&scx_tasks_lock);",
            "\tlist_add_tail(&p->scx.tasks_node, &scx_tasks);",
            "\tspin_unlock_irq(&scx_tasks_lock);",
            "",
            "\tpercpu_up_read(&scx_fork_rwsem);",
            "}",
            "void scx_cancel_fork(struct task_struct *p)",
            "{",
            "\tif (scx_enabled()) {",
            "\t\tstruct rq *rq;",
            "\t\tstruct rq_flags rf;",
            "",
            "\t\trq = task_rq_lock(p, &rf);",
            "\t\tWARN_ON_ONCE(scx_get_task_state(p) >= SCX_TASK_READY);",
            "\t\tscx_ops_exit_task(p);",
            "\t\ttask_rq_unlock(rq, p, &rf);",
            "\t}",
            "",
            "\tpercpu_up_read(&scx_fork_rwsem);",
            "}"
          ],
          "function_name": "scx_ops_disable_task, scx_ops_exit_task, init_scx_entity, scx_pre_fork, scx_fork, scx_post_fork, scx_cancel_fork",
          "description": "提供任务禁用退出逻辑和fork流程控制，通过读锁保护并发访问，在分叉前后调整任务状态并维护全局任务列表。",
          "similarity": 0.537585437297821
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/ext.c",
          "start_line": 2075,
          "end_line": 2208,
          "content": [
            "static void clr_task_runnable(struct task_struct *p, bool reset_runnable_at)",
            "{",
            "\tlist_del_init(&p->scx.runnable_node);",
            "\tif (reset_runnable_at)",
            "\t\tp->scx.flags |= SCX_TASK_RESET_RUNNABLE_AT;",
            "}",
            "static void enqueue_task_scx(struct rq *rq, struct task_struct *p, int enq_flags)",
            "{",
            "\tint sticky_cpu = p->scx.sticky_cpu;",
            "",
            "\tif (enq_flags & ENQUEUE_WAKEUP)",
            "\t\trq->scx.flags |= SCX_RQ_IN_WAKEUP;",
            "",
            "\tenq_flags |= rq->scx.extra_enq_flags;",
            "",
            "\tif (sticky_cpu >= 0)",
            "\t\tp->scx.sticky_cpu = -1;",
            "",
            "\t/*",
            "\t * Restoring a running task will be immediately followed by",
            "\t * set_next_task_scx() which expects the task to not be on the BPF",
            "\t * scheduler as tasks can only start running through local DSQs. Force",
            "\t * direct-dispatch into the local DSQ by setting the sticky_cpu.",
            "\t */",
            "\tif (unlikely(enq_flags & ENQUEUE_RESTORE) && task_current(rq, p))",
            "\t\tsticky_cpu = cpu_of(rq);",
            "",
            "\tif (p->scx.flags & SCX_TASK_QUEUED) {",
            "\t\tWARN_ON_ONCE(!task_runnable(p));",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tset_task_runnable(rq, p);",
            "\tp->scx.flags |= SCX_TASK_QUEUED;",
            "\trq->scx.nr_running++;",
            "\tadd_nr_running(rq, 1);",
            "",
            "\tif (SCX_HAS_OP(runnable) && !task_on_rq_migrating(p))",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, runnable, p, enq_flags);",
            "",
            "\tif (enq_flags & SCX_ENQ_WAKEUP)",
            "\t\ttouch_core_sched(rq, p);",
            "",
            "\tdo_enqueue_task(rq, p, enq_flags, sticky_cpu);",
            "out:",
            "\trq->scx.flags &= ~SCX_RQ_IN_WAKEUP;",
            "}",
            "static void ops_dequeue(struct task_struct *p, u64 deq_flags)",
            "{",
            "\tunsigned long opss;",
            "",
            "\t/* dequeue is always temporary, don't reset runnable_at */",
            "\tclr_task_runnable(p, false);",
            "",
            "\t/* acquire ensures that we see the preceding updates on QUEUED */",
            "\topss = atomic_long_read_acquire(&p->scx.ops_state);",
            "",
            "\tswitch (opss & SCX_OPSS_STATE_MASK) {",
            "\tcase SCX_OPSS_NONE:",
            "\t\tbreak;",
            "\tcase SCX_OPSS_QUEUEING:",
            "\t\t/*",
            "\t\t * QUEUEING is started and finished while holding @p's rq lock.",
            "\t\t * As we're holding the rq lock now, we shouldn't see QUEUEING.",
            "\t\t */",
            "\t\tBUG();",
            "\tcase SCX_OPSS_QUEUED:",
            "\t\tif (SCX_HAS_OP(dequeue))",
            "\t\t\tSCX_CALL_OP_TASK(SCX_KF_REST, dequeue, p, deq_flags);",
            "",
            "\t\tif (atomic_long_try_cmpxchg(&p->scx.ops_state, &opss,",
            "\t\t\t\t\t    SCX_OPSS_NONE))",
            "\t\t\tbreak;",
            "\t\tfallthrough;",
            "\tcase SCX_OPSS_DISPATCHING:",
            "\t\t/*",
            "\t\t * If @p is being dispatched from the BPF scheduler to a DSQ,",
            "\t\t * wait for the transfer to complete so that @p doesn't get",
            "\t\t * added to its DSQ after dequeueing is complete.",
            "\t\t *",
            "\t\t * As we're waiting on DISPATCHING with the rq locked, the",
            "\t\t * dispatching side shouldn't try to lock the rq while",
            "\t\t * DISPATCHING is set. See dispatch_to_local_dsq().",
            "\t\t *",
            "\t\t * DISPATCHING shouldn't have qseq set and control can reach",
            "\t\t * here with NONE @opss from the above QUEUED case block.",
            "\t\t * Explicitly wait on %SCX_OPSS_DISPATCHING instead of @opss.",
            "\t\t */",
            "\t\twait_ops_state(p, SCX_OPSS_DISPATCHING);",
            "\t\tBUG_ON(atomic_long_read(&p->scx.ops_state) != SCX_OPSS_NONE);",
            "\t\tbreak;",
            "\t}",
            "}",
            "static bool dequeue_task_scx(struct rq *rq, struct task_struct *p, int deq_flags)",
            "{",
            "\tif (!(p->scx.flags & SCX_TASK_QUEUED)) {",
            "\t\tWARN_ON_ONCE(task_runnable(p));",
            "\t\treturn true;",
            "\t}",
            "",
            "\tops_dequeue(p, deq_flags);",
            "",
            "\t/*",
            "\t * A currently running task which is going off @rq first gets dequeued",
            "\t * and then stops running. As we want running <-> stopping transitions",
            "\t * to be contained within runnable <-> quiescent transitions, trigger",
            "\t * ->stopping() early here instead of in put_prev_task_scx().",
            "\t *",
            "\t * @p may go through multiple stopping <-> running transitions between",
            "\t * here and put_prev_task_scx() if task attribute changes occur while",
            "\t * balance_scx() leaves @rq unlocked. However, they don't contain any",
            "\t * information meaningful to the BPF scheduler and can be suppressed by",
            "\t * skipping the callbacks if the task is !QUEUED.",
            "\t */",
            "\tif (SCX_HAS_OP(stopping) && task_current(rq, p)) {",
            "\t\tupdate_curr_scx(rq);",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, stopping, p, false);",
            "\t}",
            "",
            "\tif (SCX_HAS_OP(quiescent) && !task_on_rq_migrating(p))",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, quiescent, p, deq_flags);",
            "",
            "\tif (deq_flags & SCX_DEQ_SLEEP)",
            "\t\tp->scx.flags |= SCX_TASK_DEQD_FOR_SLEEP;",
            "\telse",
            "\t\tp->scx.flags &= ~SCX_TASK_DEQD_FOR_SLEEP;",
            "",
            "\tp->scx.flags &= ~SCX_TASK_QUEUED;",
            "\trq->scx.nr_running--;",
            "\tsub_nr_running(rq, 1);",
            "",
            "\tdispatch_dequeue(rq, p);",
            "\treturn true;",
            "}"
          ],
          "function_name": "clr_task_runnable, enqueue_task_scx, ops_dequeue, dequeue_task_scx",
          "description": "处理任务入队和出队逻辑，enqueue_task_scx标记任务为排队状态并触发BPF操作，ops_dequeue根据状态执行解队操作，dequeue_task_scx移除任务并通知BPF调度器任务离开运行队列",
          "similarity": 0.5306922793388367
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/sched/ext.c",
          "start_line": 2855,
          "end_line": 2955,
          "content": [
            "static void switch_class(struct rq *rq, struct task_struct *next)",
            "{",
            "\tconst struct sched_class *next_class = next->sched_class;",
            "",
            "#ifdef CONFIG_SMP",
            "\t/*",
            "\t * Pairs with the smp_load_acquire() issued by a CPU in",
            "\t * kick_cpus_irq_workfn() who is waiting for this CPU to perform a",
            "\t * resched.",
            "\t */",
            "\tsmp_store_release(&rq->scx.pnt_seq, rq->scx.pnt_seq + 1);",
            "#endif",
            "\tif (!static_branch_unlikely(&scx_ops_cpu_preempt))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * The callback is conceptually meant to convey that the CPU is no",
            "\t * longer under the control of SCX. Therefore, don't invoke the callback",
            "\t * if the next class is below SCX (in which case the BPF scheduler has",
            "\t * actively decided not to schedule any tasks on the CPU).",
            "\t */",
            "\tif (sched_class_above(&ext_sched_class, next_class))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * At this point we know that SCX was preempted by a higher priority",
            "\t * sched_class, so invoke the ->cpu_release() callback if we have not",
            "\t * done so already. We only send the callback once between SCX being",
            "\t * preempted, and it regaining control of the CPU.",
            "\t *",
            "\t * ->cpu_release() complements ->cpu_acquire(), which is emitted the",
            "\t *  next time that balance_scx() is invoked.",
            "\t */",
            "\tif (!rq->scx.cpu_released) {",
            "\t\tif (SCX_HAS_OP(cpu_release)) {",
            "\t\t\tstruct scx_cpu_release_args args = {",
            "\t\t\t\t.reason = preempt_reason_from_class(next_class),",
            "\t\t\t\t.task = next,",
            "\t\t\t};",
            "",
            "\t\t\tSCX_CALL_OP(SCX_KF_CPU_RELEASE,",
            "\t\t\t\t    cpu_release, cpu_of(rq), &args);",
            "\t\t}",
            "\t\trq->scx.cpu_released = true;",
            "\t}",
            "}",
            "static void put_prev_task_scx(struct rq *rq, struct task_struct *p,",
            "\t\t\t      struct task_struct *next)",
            "{",
            "\tupdate_curr_scx(rq);",
            "",
            "\t/* see dequeue_task_scx() on why we skip when !QUEUED */",
            "\tif (SCX_HAS_OP(stopping) && (p->scx.flags & SCX_TASK_QUEUED))",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, stopping, p, true);",
            "",
            "\tif (p->scx.flags & SCX_TASK_QUEUED) {",
            "\t\tset_task_runnable(rq, p);",
            "",
            "\t\t/*",
            "\t\t * If @p has slice left and is being put, @p is getting",
            "\t\t * preempted by a higher priority scheduler class or core-sched",
            "\t\t * forcing a different task. Leave it at the head of the local",
            "\t\t * DSQ.",
            "\t\t */",
            "\t\tif (p->scx.slice && !scx_rq_bypassing(rq)) {",
            "\t\t\tdispatch_enqueue(&rq->scx.local_dsq, p, SCX_ENQ_HEAD);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If @p is runnable but we're about to enter a lower",
            "\t\t * sched_class, %SCX_OPS_ENQ_LAST must be set. Tell",
            "\t\t * ops.enqueue() that @p is the only one available for this cpu,",
            "\t\t * which should trigger an explicit follow-up scheduling event.",
            "\t\t */",
            "\t\tif (sched_class_above(&ext_sched_class, next->sched_class)) {",
            "\t\t\tWARN_ON_ONCE(!static_branch_unlikely(&scx_ops_enq_last));",
            "\t\t\tdo_enqueue_task(rq, p, SCX_ENQ_LAST, -1);",
            "\t\t} else {",
            "\t\t\tdo_enqueue_task(rq, p, 0, -1);",
            "\t\t}",
            "\t}",
            "",
            "\tif (next && next->sched_class != &ext_sched_class)",
            "\t\tswitch_class(rq, next);",
            "}",
            "bool scx_prio_less(const struct task_struct *a, const struct task_struct *b,",
            "\t\t   bool in_fi)",
            "{",
            "\t/*",
            "\t * The const qualifiers are dropped from task_struct pointers when",
            "\t * calling ops.core_sched_before(). Accesses are controlled by the",
            "\t * verifier.",
            "\t */",
            "\tif (SCX_HAS_OP(core_sched_before) && !scx_rq_bypassing(task_rq(a)))",
            "\t\treturn SCX_CALL_OP_2TASKS_RET(SCX_KF_REST, core_sched_before,",
            "\t\t\t\t\t      (struct task_struct *)a,",
            "\t\t\t\t\t      (struct task_struct *)b);",
            "\telse",
            "\t\treturn time_after64(a->scx.core_sched_at, b->scx.core_sched_at);",
            "}"
          ],
          "function_name": "switch_class, put_prev_task_scx, scx_prio_less",
          "description": "该代码片段实现了SCX（可扩展调度器）相关的调度逻辑：  \n1. `switch_class` 在SMP环境中处理SCX调度器被更高优先级调度类抢占后的资源释放（通过`cpu_release`回调），并标记CPU状态变更；  \n2. `put_prev_task_scx` 管理任务从运行队列移除后的行为，根据调度类关系决定是否重入SCX队列或触发后续调度；  \n3. `scx_prio_less` 用于比较任务优先级，通过回调或时间戳判断任务执行顺序，但上下文不完整（缺少SCX核心数据结构定义）。",
          "similarity": 0.5235758423805237
        }
      ]
    },
    {
      "source_file": "kernel/task_work.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:33:42\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `task_work.c`\n\n---\n\n# task_work.c 技术文档\n\n## 文件概述\n\n`task_work.c` 实现了 Linux 内核中的 **任务工作（task work）机制**，允许内核在特定时机（如任务返回用户态、收到信号或处于 NMI 上下文）异步执行回调函数。该机制主要用于在不阻塞当前执行路径的前提下，将工作延迟到目标任务的合适上下文中执行，常用于安全模块（如 seccomp）、用户态通知、延迟清理等场景。\n\n任务工作队列是 **LIFO（后进先出）** 的，且不保证多个工作项之间的执行顺序。该机制支持多种通知模式，以适应不同的延迟和中断需求。\n\n## 核心功能\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `task_work_add()` | 向指定任务添加一个回调工作项，并根据通知模式触发相应通知 |\n| `task_work_run()` | 执行当前任务的所有挂起工作项，通常在返回用户态或任务退出前调用 |\n| `task_work_cancel_match()` | 根据自定义匹配函数取消队列中的某个工作项 |\n| `task_work_cancel_func()` | 取消队列中第一个函数指针匹配指定函数的工作项 |\n| `task_work_cancel()` | 取消队列中指定的回调结构体（精确匹配指针） |\n\n### 主要数据结构\n\n- `struct callback_head`：通用回调结构体，包含 `next` 指针和 `func` 回调函数指针。\n- `enum task_work_notify_mode`：通知模式枚举，包括：\n  - `TWA_NONE`：不通知\n  - `TWA_RESUME`：在任务返回用户态或进入 guest 模式前执行\n  - `TWA_SIGNAL`：类似信号，可中断内核态任务并立即调度执行\n  - `TWA_SIGNAL_NO_IPI`：类似 `TWA_SIGNAL`，但不发送 IPI 强制重调度\n  - `TWA_NMI_CURRENT`：仅用于当前任务且在 NMI 上下文中，通过 IRQ work 触发\n\n### 全局变量\n\n- `work_exited`：特殊标记，表示任务已退出，不能再接受新工作。\n- `irq_work_NMI_resume`（per-CPU）：用于 `TWA_NMI_CURRENT` 模式下触发 `TIF_NOTIFY_RESUME` 标志。\n\n## 关键实现\n\n### 1. 无锁队列插入（LIFO）\n\n`task_work_add()` 使用 `try_cmpxchg()` 原子操作将新工作项插入到 `task->task_works` 链表头部，实现无锁并发插入。若发现 `task_works == &work_exited`，说明任务正在退出，返回 `-ESRCH`。\n\n### 2. 多种通知机制\n\n- **`TWA_RESUME`**：调用 `set_notify_resume(task)`，设置 `TIF_NOTIFY_RESUME` 标志，确保任务在 `exit_to_user_mode()` 路径中调用 `task_work_run()`。\n- **`TWA_SIGNAL` / `TWA_SIGNAL_NO_IPI`**：分别调用 `set_notify_signal()` 和 `__set_notify_signal()`，设置 `TIF_NOTIFY_SIGNAL` 标志，并可能发送 IPI 强制目标 CPU 重调度。\n- **`TWA_NMI_CURRENT`**：在 NMI 上下文中，通过 per-CPU 的 `irq_work` 触发软中断，在 IRQ 上下文中设置 `TIF_NOTIFY_RESUME`。\n\n### 3. 安全退出处理\n\n`task_work_run()` 在循环中：\n- 原子地将 `task_works` 置为 `NULL`（或 `&work_exited`，若任务正在退出）。\n- 若任务正在退出（`PF_EXITING`），则标记为 `work_exited`，防止后续 `task_work_add()` 成功。\n- 执行所有取出的工作项，每个 `work->func(work)` 可能再次调用 `task_work_add()`，因此需循环处理。\n\n### 4. 并发取消机制\n\n`task_work_cancel_match()` 使用 `task->pi_lock` 保护遍历和删除操作：\n- 遍历链表查找匹配项。\n- 使用 `try_cmpxchg()` 原子地移除节点，避免与 `task_work_add()` 或 `task_work_run()` 冲突。\n- 特别地，`task_work_run()` 在执行前会短暂获取 `pi_lock`，确保取消操作不会在执行过程中移除正在运行的工作项。\n\n### 5. KASAN 辅助栈记录\n\n在 `task_work_add()` 中，根据 `TWAF_NO_ALLOC` 标志调用 `kasan_record_aux_stack()` 或 `kasan_record_aux_stack_noalloc()`，用于在 KASAN 报告中显示工作项的分配调用栈。\n\n## 依赖关系\n\n- **`<linux/irq_work.h>`**：提供 `irq_work` 机制，用于 `TWA_NMI_CURRENT` 模式。\n- **`<linux/resume_user_mode.h>`**：提供 `set_notify_resume()` 等接口，用于在返回用户态时触发回调。\n- **`<linux/spinlock.h>`**：使用 `raw_spinlock_t`（`pi_lock`）保护取消操作。\n- **`<linux/task_work.h>`**：定义 `task_work_notify_mode`、`callback_head` 等核心类型。\n- **调度子系统**：依赖 `TIF_NOTIFY_RESUME` / `TIF_NOTIFY_SIGNAL` 标志位，在调度路径中调用 `task_work_run()`。\n- **KASAN**：集成内存错误检测的调用栈记录功能。\n\n## 使用场景\n\n1. **Seccomp 通知**：当 seccomp 策略需要异步通知用户态代理时，通过 `task_work_add()` 添加回调。\n2. **用户态延迟操作**：内核模块需要在任务下次返回用户态时执行清理或通知，使用 `TWA_RESUME`。\n3. **NMI 上下文延迟处理**：在不可睡眠的 NMI 处理程序中，通过 `TWA_NMI_CURRENT` 安全地安排后续工作。\n4. **信号式中断执行**：需要立即中断目标任务（即使在内核态）以执行高优先级工作，使用 `TWA_SIGNAL`。\n5. **资源回收**：在任务退出路径中，确保所有挂起工作被执行或清理。\n6. **动态取消机制**：如 seccomp 可能需要在条件变化时取消之前安排的工作，使用 `task_work_cancel_func()`。",
      "similarity": 0.5597081780433655,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/task_work.c",
          "start_line": 1,
          "end_line": 9,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/irq_work.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/task_work.h>",
            "#include <linux/resume_user_mode.h>",
            "",
            "static struct callback_head work_exited; /* all we need is ->next == NULL */",
            "",
            "#ifdef CONFIG_IRQ_WORK"
          ],
          "function_name": null,
          "description": "声明用于任务工作通知的静态变量work_exited，该变量通过next指针判断任务工作链表是否为空，上下文不完整",
          "similarity": 0.5270057916641235
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/task_work.c",
          "start_line": 10,
          "end_line": 125,
          "content": [
            "static void task_work_set_notify_irq(struct irq_work *entry)",
            "{",
            "\ttest_and_set_tsk_thread_flag(current, TIF_NOTIFY_RESUME);",
            "}",
            "int task_work_add(struct task_struct *task, struct callback_head *work,",
            "\t\t  enum task_work_notify_mode notify)",
            "{",
            "\tstruct callback_head *head;",
            "\tint flags = notify & TWA_FLAGS;",
            "",
            "\tnotify &= ~TWA_FLAGS;",
            "\tif (notify == TWA_NMI_CURRENT) {",
            "\t\tif (WARN_ON_ONCE(task != current))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!IS_ENABLED(CONFIG_IRQ_WORK))",
            "\t\t\treturn -EINVAL;",
            "\t} else {",
            "\t\t/*",
            "\t\t * Record the work call stack in order to print it in KASAN",
            "\t\t * reports.",
            "\t\t *",
            "\t\t * Note that stack allocation can fail if TWAF_NO_ALLOC flag",
            "\t\t * is set and new page is needed to expand the stack buffer.",
            "\t\t */",
            "\t\tif (flags & TWAF_NO_ALLOC)",
            "\t\t\tkasan_record_aux_stack_noalloc(work);",
            "\t\telse",
            "\t\t\tkasan_record_aux_stack(work);",
            "\t}",
            "",
            "\thead = READ_ONCE(task->task_works);",
            "\tdo {",
            "\t\tif (unlikely(head == &work_exited))",
            "\t\t\treturn -ESRCH;",
            "\t\twork->next = head;",
            "\t} while (!try_cmpxchg(&task->task_works, &head, work));",
            "",
            "\tswitch (notify) {",
            "\tcase TWA_NONE:",
            "\t\tbreak;",
            "\tcase TWA_RESUME:",
            "\t\tset_notify_resume(task);",
            "\t\tbreak;",
            "\tcase TWA_SIGNAL:",
            "\t\tset_notify_signal(task);",
            "\t\tbreak;",
            "\tcase TWA_SIGNAL_NO_IPI:",
            "\t\t__set_notify_signal(task);",
            "\t\tbreak;",
            "#ifdef CONFIG_IRQ_WORK",
            "\tcase TWA_NMI_CURRENT:",
            "\t\tirq_work_queue(this_cpu_ptr(&irq_work_NMI_resume));",
            "\t\tbreak;",
            "#endif",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(1);",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static bool task_work_func_match(struct callback_head *cb, void *data)",
            "{",
            "\treturn cb->func == data;",
            "}",
            "static bool task_work_match(struct callback_head *cb, void *data)",
            "{",
            "\treturn cb == data;",
            "}",
            "bool task_work_cancel(struct task_struct *task, struct callback_head *cb)",
            "{",
            "\tstruct callback_head *ret;",
            "",
            "\tret = task_work_cancel_match(task, task_work_match, cb);",
            "",
            "\treturn ret == cb;",
            "}",
            "void task_work_run(void)",
            "{",
            "\tstruct task_struct *task = current;",
            "\tstruct callback_head *work, *head, *next;",
            "",
            "\tfor (;;) {",
            "\t\t/*",
            "\t\t * work->func() can do task_work_add(), do not set",
            "\t\t * work_exited unless the list is empty.",
            "\t\t */",
            "\t\twork = READ_ONCE(task->task_works);",
            "\t\tdo {",
            "\t\t\thead = NULL;",
            "\t\t\tif (!work) {",
            "\t\t\t\tif (task->flags & PF_EXITING)",
            "\t\t\t\t\thead = &work_exited;",
            "\t\t\t\telse",
            "\t\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t} while (!try_cmpxchg(&task->task_works, &work, head));",
            "",
            "\t\tif (!work)",
            "\t\t\tbreak;",
            "\t\t/*",
            "\t\t * Synchronize with task_work_cancel_match(). It can not remove",
            "\t\t * the first entry == work, cmpxchg(task_works) must fail.",
            "\t\t * But it can remove another entry from the ->next list.",
            "\t\t */",
            "\t\traw_spin_lock_irq(&task->pi_lock);",
            "\t\traw_spin_unlock_irq(&task->pi_lock);",
            "",
            "\t\tdo {",
            "\t\t\tnext = work->next;",
            "\t\t\twork->func(work);",
            "\t\t\twork = next;",
            "\t\t\tcond_resched();",
            "\t\t} while (work);",
            "\t}",
            "}"
          ],
          "function_name": "task_work_set_notify_irq, task_work_add, task_work_func_match, task_work_match, task_work_cancel, task_work_run",
          "description": "实现任务工作队列的添加、匹配、取消及执行逻辑，支持多种通知模式（如RESUME/SIGNAL/NMI），通过原子操作维护链表并同步执行回调函数",
          "similarity": 0.5161335468292236
        }
      ]
    }
  ]
}