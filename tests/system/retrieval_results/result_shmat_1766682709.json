{
  "query": "shmat",
  "timestamp": "2025-12-26 01:11:49",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.3830692172050476,
      "chunks": [
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/rt.c",
          "start_line": 776,
          "end_line": 913,
          "content": [
            "static void balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tif (!sched_feat(RT_RUNTIME_SHARE))",
            "\t\treturn;",
            "",
            "\tif (rt_rq->rt_time > rt_rq->rt_runtime) {",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tdo_balance_runtime(rt_rq);",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t}",
            "}",
            "static inline void balance_runtime(struct rt_rq *rt_rq) {}",
            "static int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun)",
            "{",
            "\tint i, idle = 1, throttled = 0;",
            "\tconst struct cpumask *span;",
            "",
            "\tspan = sched_rt_period_mask();",
            "",
            "\t/*",
            "\t * FIXME: isolated CPUs should really leave the root task group,",
            "\t * whether they are isolcpus or were isolated via cpusets, lest",
            "\t * the timer run on a CPU which does not service all runqueues,",
            "\t * potentially leaving other CPUs indefinitely throttled.  If",
            "\t * isolation is really required, the user will turn the throttle",
            "\t * off to kill the perturbations it causes anyway.  Meanwhile,",
            "\t * this maintains functionality for boot and/or troubleshooting.",
            "\t */",
            "\tif (rt_b == &root_task_group.rt_bandwidth)",
            "\t\tspan = cpu_online_mask;",
            "",
            "\tfor_each_cpu(i, span) {",
            "\t\tint enqueue = 0;",
            "\t\tstruct rt_rq *rt_rq = sched_rt_period_rt_rq(rt_b, i);",
            "\t\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\t\tstruct rq_flags rf;",
            "\t\tint skip;",
            "",
            "\t\t/*",
            "\t\t * When span == cpu_online_mask, taking each rq->lock",
            "\t\t * can be time-consuming. Try to avoid it when possible.",
            "\t\t */",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\tif (!sched_feat(RT_RUNTIME_SHARE) && rt_rq->rt_runtime != RUNTIME_INF)",
            "\t\t\trt_rq->rt_runtime = rt_b->rt_runtime;",
            "\t\tskip = !rt_rq->rt_time && !rt_rq->rt_nr_running;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tif (skip)",
            "\t\t\tcontinue;",
            "",
            "\t\trq_lock(rq, &rf);",
            "\t\tupdate_rq_clock(rq);",
            "",
            "\t\tif (rt_rq->rt_time) {",
            "\t\t\tu64 runtime;",
            "",
            "\t\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t\tif (rt_rq->rt_throttled)",
            "\t\t\t\tbalance_runtime(rt_rq);",
            "\t\t\truntime = rt_rq->rt_runtime;",
            "\t\t\trt_rq->rt_time -= min(rt_rq->rt_time, overrun*runtime);",
            "\t\t\tif (rt_rq->rt_throttled && rt_rq->rt_time < runtime) {",
            "\t\t\t\trt_rq->rt_throttled = 0;",
            "\t\t\t\tenqueue = 1;",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * When we're idle and a woken (rt) task is",
            "\t\t\t\t * throttled wakeup_preempt() will set",
            "\t\t\t\t * skip_update and the time between the wakeup",
            "\t\t\t\t * and this unthrottle will get accounted as",
            "\t\t\t\t * 'runtime'.",
            "\t\t\t\t */",
            "\t\t\t\tif (rt_rq->rt_nr_running && rq->curr == rq->idle)",
            "\t\t\t\t\trq_clock_cancel_skipupdate(rq);",
            "\t\t\t}",
            "\t\t\tif (rt_rq->rt_time || rt_rq->rt_nr_running)",
            "\t\t\t\tidle = 0;",
            "\t\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\t} else if (rt_rq->rt_nr_running) {",
            "\t\t\tidle = 0;",
            "\t\t\tif (!rt_rq_throttled(rt_rq))",
            "\t\t\t\tenqueue = 1;",
            "\t\t}",
            "\t\tif (rt_rq->rt_throttled)",
            "\t\t\tthrottled = 1;",
            "",
            "\t\tif (enqueue)",
            "\t\t\tsched_rt_rq_enqueue(rt_rq);",
            "\t\trq_unlock(rq, &rf);",
            "\t}",
            "",
            "\tif (!throttled && (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF))",
            "\t\treturn 1;",
            "",
            "\treturn idle;",
            "}",
            "static int sched_rt_runtime_exceeded(struct rt_rq *rt_rq)",
            "{",
            "\tu64 runtime = sched_rt_runtime(rt_rq);",
            "",
            "\tif (rt_rq->rt_throttled)",
            "\t\treturn rt_rq_throttled(rt_rq);",
            "",
            "\tif (runtime >= sched_rt_period(rt_rq))",
            "\t\treturn 0;",
            "",
            "\tbalance_runtime(rt_rq);",
            "\truntime = sched_rt_runtime(rt_rq);",
            "\tif (runtime == RUNTIME_INF)",
            "\t\treturn 0;",
            "",
            "\tif (rt_rq->rt_time > runtime) {",
            "\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\t\t/*",
            "\t\t * Don't actually throttle groups that have no runtime assigned",
            "\t\t * but accrue some time due to boosting.",
            "\t\t */",
            "\t\tif (likely(rt_b->rt_runtime)) {",
            "\t\t\trt_rq->rt_throttled = 1;",
            "\t\t\tprintk_deferred_once(\"sched: RT throttling activated\\n\");",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * In case we did anyway, make it go away,",
            "\t\t\t * replenishment is a joke, since it will replenish us",
            "\t\t\t * with exactly 0 ns.",
            "\t\t\t */",
            "\t\t\trt_rq->rt_time = 0;",
            "\t\t}",
            "",
            "\t\tif (rt_rq_throttled(rt_rq)) {",
            "\t\t\tsched_rt_rq_dequeue(rt_rq);",
            "\t\t\treturn 1;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "balance_runtime, balance_runtime, do_sched_rt_period_timer, sched_rt_runtime_exceeded",
          "description": "`balance_runtime`在超时时触发重新平衡，`do_sched_rt_period_timer`周期性调整运行时并检查节流状态，`sched_rt_runtime_exceeded`判断是否超出运行时限制并标记节流",
          "similarity": 0.3189047574996948
        },
        {
          "chunk_id": 8,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1066,
          "end_line": 1170,
          "content": [
            "static void",
            "inc_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\t/*",
            "\t * Change rq's cpupri only if rt_rq is the top queue.",
            "\t */",
            "\tif (&rq->rt != rt_rq)",
            "\t\treturn;",
            "#endif",
            "\tif (rq->online && prio < prev_prio)",
            "\t\tcpupri_set(&rq->rd->cpupri, rq->cpu, prio);",
            "}",
            "static void",
            "dec_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\t/*",
            "\t * Change rq's cpupri only if rt_rq is the top queue.",
            "\t */",
            "\tif (&rq->rt != rt_rq)",
            "\t\treturn;",
            "#endif",
            "\tif (rq->online && rt_rq->highest_prio.curr != prev_prio)",
            "\t\tcpupri_set(&rq->rd->cpupri, rq->cpu, rt_rq->highest_prio.curr);",
            "}",
            "static inline",
            "void inc_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio) {}",
            "static inline",
            "void dec_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio) {}",
            "static void",
            "inc_rt_prio(struct rt_rq *rt_rq, int prio)",
            "{",
            "\tint prev_prio = rt_rq->highest_prio.curr;",
            "",
            "\tif (prio < prev_prio)",
            "\t\trt_rq->highest_prio.curr = prio;",
            "",
            "\tinc_rt_prio_smp(rt_rq, prio, prev_prio);",
            "}",
            "static void",
            "dec_rt_prio(struct rt_rq *rt_rq, int prio)",
            "{",
            "\tint prev_prio = rt_rq->highest_prio.curr;",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "",
            "\t\tWARN_ON(prio < prev_prio);",
            "",
            "\t\t/*",
            "\t\t * This may have been our highest task, and therefore",
            "\t\t * we may have some recomputation to do",
            "\t\t */",
            "\t\tif (prio == prev_prio) {",
            "\t\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "",
            "\t\t\trt_rq->highest_prio.curr =",
            "\t\t\t\tsched_find_first_bit(array->bitmap);",
            "\t\t}",
            "",
            "\t} else {",
            "\t\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\t}",
            "",
            "\tdec_rt_prio_smp(rt_rq, prio, prev_prio);",
            "}",
            "static inline void inc_rt_prio(struct rt_rq *rt_rq, int prio) {}",
            "static inline void dec_rt_prio(struct rt_rq *rt_rq, int prio) {}",
            "static void",
            "inc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)",
            "{",
            "\tif (rt_se_boosted(rt_se))",
            "\t\trt_rq->rt_nr_boosted++;",
            "",
            "\tif (rt_rq->tg)",
            "\t\tstart_rt_bandwidth(&rt_rq->tg->rt_bandwidth);",
            "}",
            "static void",
            "dec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)",
            "{",
            "\tif (rt_se_boosted(rt_se))",
            "\t\trt_rq->rt_nr_boosted--;",
            "",
            "\tWARN_ON(!rt_rq->rt_nr_running && rt_rq->rt_nr_boosted);",
            "}",
            "static void",
            "inc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)",
            "{",
            "}",
            "static inline",
            "void dec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq) {}",
            "static inline",
            "unsigned int rt_se_nr_running(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *group_rq = group_rt_rq(rt_se);",
            "",
            "\tif (group_rq)",
            "\t\treturn group_rq->rt_nr_running;",
            "\telse",
            "\t\treturn 1;",
            "}"
          ],
          "function_name": "inc_rt_prio_smp, dec_rt_prio_smp, inc_rt_prio_smp, dec_rt_prio_smp, inc_rt_prio, dec_rt_prio, inc_rt_prio, dec_rt_prio, inc_rt_group, dec_rt_group, inc_rt_group, dec_rt_group, rt_se_nr_running",
          "description": "`inc_rt_prio/dec_rt_prio`维护实时队列最高优先级，`inc_rt_group/dec_rt_group`处理调度组的资源计数，`rt_se_nr_running`查询任务所属组的运行数量",
          "similarity": 0.30346259474754333
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/sched/rt.c",
          "start_line": 934,
          "end_line": 1036,
          "content": [
            "static inline void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "\tif (!rt_rq->rt_nr_running)",
            "\t\treturn;",
            "",
            "\tenqueue_top_rt_rq(rt_rq);",
            "\tresched_curr(rq);",
            "}",
            "static inline void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn false;",
            "}",
            "static void __enable_runtime(struct rq *rq) { }",
            "static void __disable_runtime(struct rq *rq) { }",
            "static inline int rt_se_prio(struct sched_rt_entity *rt_se)",
            "{",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\tif (rt_rq)",
            "\t\treturn rt_rq->highest_prio.curr;",
            "#endif",
            "",
            "\treturn rt_task_of(rt_se)->prio;",
            "}",
            "static void update_curr_rt(struct rq *rq)",
            "{",
            "\tstruct task_struct *curr = rq->curr;",
            "\ts64 delta_exec;",
            "",
            "\tif (curr->sched_class != &rt_sched_class)",
            "\t\treturn;",
            "",
            "\tdelta_exec = update_curr_common(rq);",
            "\tif (unlikely(delta_exec <= 0))",
            "\t\treturn;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\tstruct sched_rt_entity *rt_se = &curr->rt;",
            "",
            "\tif (!rt_bandwidth_enabled())",
            "\t\treturn;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);",
            "\t\tint exceeded;",
            "",
            "\t\tif (sched_rt_runtime(rt_rq) != RUNTIME_INF) {",
            "\t\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t\trt_rq->rt_time += delta_exec;",
            "\t\t\texceeded = sched_rt_runtime_exceeded(rt_rq);",
            "\t\t\tif (exceeded)",
            "\t\t\t\tresched_curr(rq);",
            "\t\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\t\tif (exceeded)",
            "\t\t\t\tdo_start_rt_bandwidth(sched_rt_bandwidth(rt_rq));",
            "\t\t}",
            "\t}",
            "#endif",
            "}",
            "static void",
            "dequeue_top_rt_rq(struct rt_rq *rt_rq, unsigned int count)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "\tBUG_ON(&rq->rt != rt_rq);",
            "",
            "\tif (!rt_rq->rt_queued)",
            "\t\treturn;",
            "",
            "\tBUG_ON(!rq->nr_running);",
            "",
            "\tsub_nr_running(rq, count);",
            "\trt_rq->rt_queued = 0;",
            "",
            "}",
            "static void",
            "enqueue_top_rt_rq(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "\tBUG_ON(&rq->rt != rt_rq);",
            "",
            "\tif (rt_rq->rt_queued)",
            "\t\treturn;",
            "",
            "\tif (rt_rq_throttled(rt_rq))",
            "\t\treturn;",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tadd_nr_running(rq, rt_rq->rt_nr_running);",
            "\t\trt_rq->rt_queued = 1;",
            "\t}",
            "",
            "\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\tcpufreq_update_util(rq, 0);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, __enable_runtime, __disable_runtime, rt_se_prio, update_curr_rt, dequeue_top_rt_rq, enqueue_top_rt_rq",
          "description": "`sched_rt_rq_enqueue/dequeue`管理实时队列的挂载/卸载，`rt_rq_throttled`判断是否节流，`update_curr_rt`更新当前任务的运行时间并检查节流策略",
          "similarity": 0.3000471591949463
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1607,
          "end_line": 1716,
          "content": [
            "static void check_preempt_equal_prio(struct rq *rq, struct task_struct *p)",
            "{",
            "\t/*",
            "\t * Current can't be migrated, useless to reschedule,",
            "\t * let's hope p can move out.",
            "\t */",
            "\tif (rq->curr->nr_cpus_allowed == 1 ||",
            "\t    !cpupri_find(&rq->rd->cpupri, rq->curr, NULL))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * p is migratable, so let's not schedule it and",
            "\t * see if it is pushed or pulled somewhere else.",
            "\t */",
            "\tif (p->nr_cpus_allowed != 1 &&",
            "\t    cpupri_find(&rq->rd->cpupri, p, NULL))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * There appear to be other CPUs that can accept",
            "\t * the current task but none can run 'p', so lets reschedule",
            "\t * to try and push the current task away:",
            "\t */",
            "\trequeue_task_rt(rq, p, 1);",
            "\tresched_curr(rq);",
            "}",
            "static int balance_rt(struct rq *rq, struct task_struct *p, struct rq_flags *rf)",
            "{",
            "\tif (!on_rt_rq(&p->rt) && need_pull_rt_task(rq, p)) {",
            "\t\t/*",
            "\t\t * This is OK, because current is on_cpu, which avoids it being",
            "\t\t * picked for load-balance and preemption/IRQs are still",
            "\t\t * disabled avoiding further scheduler activity on it and we've",
            "\t\t * not yet started the picking loop.",
            "\t\t */",
            "\t\trq_unpin_lock(rq, rf);",
            "\t\tpull_rt_task(rq);",
            "\t\trq_repin_lock(rq, rf);",
            "\t}",
            "",
            "\treturn sched_stop_runnable(rq) || sched_dl_runnable(rq) || sched_rt_runnable(rq);",
            "}",
            "static void wakeup_preempt_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tif (p->prio < rq->curr->prio) {",
            "\t\tresched_curr(rq);",
            "\t\treturn;",
            "\t}",
            "",
            "#ifdef CONFIG_SMP",
            "\t/*",
            "\t * If:",
            "\t *",
            "\t * - the newly woken task is of equal priority to the current task",
            "\t * - the newly woken task is non-migratable while current is migratable",
            "\t * - current will be preempted on the next reschedule",
            "\t *",
            "\t * we should check to see if current can readily move to a different",
            "\t * cpu.  If so, we will reschedule to allow the push logic to try",
            "\t * to move current somewhere else, making room for our non-migratable",
            "\t * task.",
            "\t */",
            "\tif (p->prio == rq->curr->prio && !test_tsk_need_resched(rq->curr))",
            "\t\tcheck_preempt_equal_prio(rq, p);",
            "#endif",
            "}",
            "static inline void set_next_task_rt(struct rq *rq, struct task_struct *p, bool first)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq = &rq->rt;",
            "",
            "\tp->se.exec_start = rq_clock_task(rq);",
            "\tif (on_rt_rq(&p->rt))",
            "\t\tupdate_stats_wait_end_rt(rt_rq, rt_se);",
            "",
            "\t/* The running task is never eligible for pushing */",
            "\tdequeue_pushable_task(rq, p);",
            "",
            "\tif (!first)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If prev task was rt, put_prev_task() has already updated the",
            "\t * utilization. We only care of the case where we start to schedule a",
            "\t * rt task",
            "\t */",
            "\tif (rq->curr->sched_class != &rt_sched_class)",
            "\t\tupdate_rt_rq_load_avg(rq_clock_pelt(rq), rq, 0);",
            "",
            "\trt_queue_push_tasks(rq);",
            "}",
            "static void put_prev_task_rt(struct rq *rq, struct task_struct *p, struct task_struct *next)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq = &rq->rt;",
            "",
            "\tif (on_rt_rq(&p->rt))",
            "\t\tupdate_stats_wait_start_rt(rt_rq, rt_se);",
            "",
            "\tupdate_curr_rt(rq);",
            "",
            "\tupdate_rt_rq_load_avg(rq_clock_pelt(rq), rq, 1);",
            "",
            "\t/*",
            "\t * The previous task needs to be made eligible for pushing",
            "\t * if it is still active",
            "\t */",
            "\tif (on_rt_rq(&p->rt) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_task(rq, p);",
            "}"
          ],
          "function_name": "check_preempt_equal_prio, balance_rt, wakeup_preempt_rt, set_next_task_rt, put_prev_task_rt",
          "description": "实现同优先级抢占检测、实时任务负载平衡、唤醒抢占触发机制以及调度器状态更新与负载计算。",
          "similarity": 0.28256669640541077
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/sched/rt.c",
          "start_line": 2102,
          "end_line": 2228,
          "content": [
            "static void push_rt_tasks(struct rq *rq)",
            "{",
            "\t/* push_rt_task will return true if it moved an RT */",
            "\twhile (push_rt_task(rq, false))",
            "\t\t;",
            "}",
            "static int rto_next_cpu(struct root_domain *rd)",
            "{",
            "\tint next;",
            "\tint cpu;",
            "",
            "\t/*",
            "\t * When starting the IPI RT pushing, the rto_cpu is set to -1,",
            "\t * rt_next_cpu() will simply return the first CPU found in",
            "\t * the rto_mask.",
            "\t *",
            "\t * If rto_next_cpu() is called with rto_cpu is a valid CPU, it",
            "\t * will return the next CPU found in the rto_mask.",
            "\t *",
            "\t * If there are no more CPUs left in the rto_mask, then a check is made",
            "\t * against rto_loop and rto_loop_next. rto_loop is only updated with",
            "\t * the rto_lock held, but any CPU may increment the rto_loop_next",
            "\t * without any locking.",
            "\t */",
            "\tfor (;;) {",
            "",
            "\t\t/* When rto_cpu is -1 this acts like cpumask_first() */",
            "\t\tcpu = cpumask_next(rd->rto_cpu, rd->rto_mask);",
            "",
            "\t\trd->rto_cpu = cpu;",
            "",
            "\t\tif (cpu < nr_cpu_ids)",
            "\t\t\treturn cpu;",
            "",
            "\t\trd->rto_cpu = -1;",
            "",
            "\t\t/*",
            "\t\t * ACQUIRE ensures we see the @rto_mask changes",
            "\t\t * made prior to the @next value observed.",
            "\t\t *",
            "\t\t * Matches WMB in rt_set_overload().",
            "\t\t */",
            "\t\tnext = atomic_read_acquire(&rd->rto_loop_next);",
            "",
            "\t\tif (rd->rto_loop == next)",
            "\t\t\tbreak;",
            "",
            "\t\trd->rto_loop = next;",
            "\t}",
            "",
            "\treturn -1;",
            "}",
            "static inline bool rto_start_trylock(atomic_t *v)",
            "{",
            "\treturn !atomic_cmpxchg_acquire(v, 0, 1);",
            "}",
            "static inline void rto_start_unlock(atomic_t *v)",
            "{",
            "\tatomic_set_release(v, 0);",
            "}",
            "static void tell_cpu_to_push(struct rq *rq)",
            "{",
            "\tint cpu = -1;",
            "",
            "\t/* Keep the loop going if the IPI is currently active */",
            "\tatomic_inc(&rq->rd->rto_loop_next);",
            "",
            "\t/* Only one CPU can initiate a loop at a time */",
            "\tif (!rto_start_trylock(&rq->rd->rto_loop_start))",
            "\t\treturn;",
            "",
            "\traw_spin_lock(&rq->rd->rto_lock);",
            "",
            "\t/*",
            "\t * The rto_cpu is updated under the lock, if it has a valid CPU",
            "\t * then the IPI is still running and will continue due to the",
            "\t * update to loop_next, and nothing needs to be done here.",
            "\t * Otherwise it is finishing up and an ipi needs to be sent.",
            "\t */",
            "\tif (rq->rd->rto_cpu < 0)",
            "\t\tcpu = rto_next_cpu(rq->rd);",
            "",
            "\traw_spin_unlock(&rq->rd->rto_lock);",
            "",
            "\trto_start_unlock(&rq->rd->rto_loop_start);",
            "",
            "\tif (cpu >= 0) {",
            "\t\t/* Make sure the rd does not get freed while pushing */",
            "\t\tsched_get_rd(rq->rd);",
            "\t\tirq_work_queue_on(&rq->rd->rto_push_work, cpu);",
            "\t}",
            "}",
            "void rto_push_irq_work_func(struct irq_work *work)",
            "{",
            "\tstruct root_domain *rd =",
            "\t\tcontainer_of(work, struct root_domain, rto_push_work);",
            "\tstruct rq *rq;",
            "\tint cpu;",
            "",
            "\trq = this_rq();",
            "",
            "\t/*",
            "\t * We do not need to grab the lock to check for has_pushable_tasks.",
            "\t * When it gets updated, a check is made if a push is possible.",
            "\t */",
            "\tif (has_pushable_tasks(rq)) {",
            "\t\traw_spin_rq_lock(rq);",
            "\t\twhile (push_rt_task(rq, true))",
            "\t\t\t;",
            "\t\traw_spin_rq_unlock(rq);",
            "\t}",
            "",
            "\traw_spin_lock(&rd->rto_lock);",
            "",
            "\t/* Pass the IPI to the next rt overloaded queue */",
            "\tcpu = rto_next_cpu(rd);",
            "",
            "\traw_spin_unlock(&rd->rto_lock);",
            "",
            "\tif (cpu < 0) {",
            "\t\tsched_put_rd(rd);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Try the next RT overloaded CPU */",
            "\tirq_work_queue_on(&rd->rto_push_work, cpu);",
            "}"
          ],
          "function_name": "push_rt_tasks, rto_next_cpu, rto_start_trylock, rto_start_unlock, tell_cpu_to_push, rto_push_irq_work_func",
          "description": "实现实时任务批量迁移、冗余CPU迭代选择、锁竞争控制及中断工作队列驱动的跨CPU负载均衡机制。",
          "similarity": 0.2810491919517517
        }
      ]
    },
    {
      "source_file": "kernel/dma/swiotlb.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:17:23\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\swiotlb.c`\n\n---\n\n# `dma/swiotlb.c` 技术文档\n\n## 1. 文件概述\n\n`swiotlb.c` 实现了 **Software I/O Translation Lookaside Buffer (SWIOTLB)**，即软件 I/O TLB 机制，作为硬件 I/O TLB（如 IOMMU）不可用时的 **DMA 映射回退方案**。该机制通过在低地址空间（通常为 32 位可寻址区域）预分配一块连续的“反弹缓冲区”（bounce buffer），用于在设备无法直接访问高地址内存时，中转 DMA 数据传输，从而确保 DMA 操作的正确性和兼容性。\n\n该文件最初由 Intel 和 HP 开发，现已演变为 Linux 内核通用的软件 DMA 映射基础设施，支持动态分配、高内存（highmem）、加密内存等高级特性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct io_tlb_slot`**  \n  描述 SWIOTLB 池中的一个槽位（slot）：\n  - `orig_addr`：原始物理地址（DMA 映射前的地址）\n  - `alloc_size`：分配的实际缓冲区大小\n  - `list`：用于空闲链表管理，表示从此索引开始的连续空闲槽位数\n  - `pad_slots`：前置填充槽位数量（仅在首个非填充槽有效）\n\n- **`struct io_tlb_area`**  \n  描述 SWIOTLB 内存池中的一个区域（area），用于细粒度并发控制：\n  - `used`：已使用的槽位数\n  - `index`：下一次分配的起始搜索索引\n  - `lock`：保护该区域数据结构的自旋锁\n\n- **`struct io_tlb_mem`**  \n  SWIOTLB 内存池的顶层结构（定义在头文件中），包含多个 `io_tlb_pool`，支持动态扩展（`CONFIG_SWIOTLB_DYNAMIC`）\n\n- **`struct io_tlb_pool`**  \n  单个 SWIOTLB 内存池，包含物理地址范围、槽位数组、区域数组等\n\n### 主要函数与接口\n\n- **`setup_io_tlb_npages(char *str)`**  \n  解析内核启动参数 `swiotlb=`，设置缓冲区大小、区域数量及强制启用/禁用策略。\n\n- **`swiotlb_size_or_default(void)`**  \n  返回当前配置的 SWIOTLB 缓冲区总大小（字节）。\n\n- **`swiotlb_adjust_size(unsigned long size)`**  \n  允许架构代码（如支持内存加密的 AMD SEV）在未显式指定 `swiotlb` 参数时调整缓冲区大小。\n\n- **`swiotlb_print_info(void)`**  \n  打印 SWIOTLB 缓冲区的映射信息（物理地址范围和大小）。\n\n- **`swiotlb_update_mem_attributes(void)`**  \n  在早期分配后，由架构代码调用以更新内存属性（如将内存标记为“解密”，用于 AMD SEV 等场景）。\n\n- **`swiotlb_init_io_tlb_pool(...)`**  \n  初始化一个 SWIOTLB 内存池，设置其物理/虚拟地址、槽位数量、区域划分等。\n\n- **`round_up_default_nslabs()` / `swiotlb_adjust_nareas()` / `limit_nareas()`**  \n  辅助函数，用于根据配置调整槽位数量和区域数量，确保对齐和性能优化。\n\n### 全局变量\n\n- `swiotlb_force_bounce`：强制所有 DMA 使用 SWIOTLB（即使设备支持高地址）\n- `swiotlb_force_disable`：禁用 SWIOTLB（即使设备需要）\n- `io_tlb_default_mem`：默认的 SWIOTLB 内存池实例\n- `default_nslabs` / `default_nareas`：默认槽位数和区域数\n\n## 3. 关键实现\n\n### 内存池布局与区域划分\n- SWIOTLB 缓冲区被划分为多个 **区域（area）**，每个区域有独立的自旋锁，以减少多 CPU 并发访问时的锁竞争。\n- 每个区域包含若干 **槽位（slot）**，每个槽位大小为 `IO_TLB_SIZE`（通常为 128 字节）。\n- 区域数量 `nareas` 必须是 2 的幂，且总槽位数 `nslabs` 会向上对齐到 `IO_TLB_SEGSIZE`（段大小，通常为 128 个槽）的倍数，再进一步对齐到 2 的幂，以优化分配算法。\n\n### 启动参数解析\n- 通过 `early_param(\"swiotlb\", ...)` 注册解析函数。\n- 支持格式：`swiotlb=<size>[,<nareas>][,force|noforce]`\n  - `<size>`：缓冲区大小（单位：页或字节），自动对齐到段边界\n  - `<nareas>`：区域数量，自动向上取整为 2 的幂\n  - `force`：强制启用 SWIOTLB\n  - `noforce`：禁用 SWIOTLB\n\n### 动态分配支持（`CONFIG_SWIOTLB_DYNAMIC`）\n- 默认内存池 `io_tlb_default_mem` 包含一个工作队列项 `dyn_alloc`，用于在运行时动态扩展缓冲区。\n- 通过链表 `pools` 管理多个内存池实例。\n\n### 内存属性更新\n- 在支持内存加密（如 AMD SEV）的平台上，早期分配的内存可能需要后续标记为“解密”，以便设备可访问。`swiotlb_update_mem_attributes()` 提供此钩子。\n\n### 对齐与大小约束\n- 最小缓冲区大小为 `IO_TLB_MIN_SLABS`（1MB）\n- 槽位数量必须是 `IO_TLB_SEGSIZE` 的倍数，避免段跨越区域边界，简化连续空闲槽位追踪。\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/dma-direct.h>`：提供直接 DMA 映射操作\n  - `<linux/dma-map-ops.h>`：DMA 映射操作集接口\n  - `<linux/swiotlb.h>`：SWIOTLB 公共头文件，定义核心结构和 API\n  - `<linux/scatterlist.h>`：支持 scatterlist DMA 映射\n\n- **内存管理**：\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/highmem.h>`：高内存支持（`CONFIG_HIGHMEM`）\n  - `<linux/set_memory.h>`：内存属性设置（如加密/解密）\n\n- **平台特性**：\n  - `<linux/cc_platform.h>`：机密计算平台支持（如 SEV）\n  - `CONFIG_DMA_RESTRICTED_POOL`：支持从设备树预留内存分配 SWIOTLB\n\n- **调试与跟踪**：\n  - `<linux/debugfs.h>`：调试接口\n  - `<trace/events/swiotlb.h>`：跟踪点定义\n\n## 5. 使用场景\n\n1. **无 IOMMU 的 64 位系统**：  \n   当设备仅支持 32 位 DMA 地址，但系统内存超过 4GB 时，SWIOTLB 作为透明回退机制，自动使用低地址反弹缓冲区。\n\n2. **内存加密环境（如 AMD SEV）**：  \n   加密内存对设备不可见，SWIOTLB 提供解密的 bounce buffer 用于 DMA 传输。\n\n3. **调试与强制测试**：  \n   通过 `swiotlb=force` 强制所有 DMA 走 SWIOTLB 路径，用于测试驱动兼容性或调试 DMA 问题。\n\n4. **嵌入式或资源受限系统**：  \n   在无硬件 IOMMU 的嵌入式平台，SWIOTLB 是确保 DMA 正确性的关键组件。\n\n5. **高内存（Highmem）系统**：  \n   支持将高内存页映射到 SWIOTLB 缓冲区进行 DMA 操作。\n\n6. **动态内存压力场景**：  \n   启用 `CONFIG_SWIOTLB_DYNAMIC` 后，可在运行时扩展缓冲区以应对突发 DMA 需求。",
      "similarity": 0.3734508752822876,
      "chunks": [
        {
          "chunk_id": 8,
          "file_path": "kernel/dma/swiotlb.c",
          "start_line": 1429,
          "end_line": 1529,
          "content": [
            "static bool swiotlb_del_transient(struct device *dev, phys_addr_t tlb_addr)",
            "{",
            "\tstruct io_tlb_pool *pool;",
            "",
            "\tpool = swiotlb_find_pool(dev, tlb_addr);",
            "\tif (!pool->transient)",
            "\t\treturn false;",
            "",
            "\tdec_used(dev->dma_io_tlb_mem, pool->nslabs);",
            "\tswiotlb_del_pool(dev, pool);",
            "\treturn true;",
            "}",
            "static inline bool swiotlb_del_transient(struct device *dev,",
            "\t\t\t\t\t phys_addr_t tlb_addr)",
            "{",
            "\treturn false;",
            "}",
            "void swiotlb_tbl_unmap_single(struct device *dev, phys_addr_t tlb_addr,",
            "\t\t\t      size_t mapping_size, enum dma_data_direction dir,",
            "\t\t\t      unsigned long attrs)",
            "{",
            "\t/*",
            "\t * First, sync the memory before unmapping the entry",
            "\t */",
            "\tif (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&",
            "\t    (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL))",
            "\t\tswiotlb_bounce(dev, tlb_addr, mapping_size, DMA_FROM_DEVICE);",
            "",
            "\tif (swiotlb_del_transient(dev, tlb_addr))",
            "\t\treturn;",
            "\tswiotlb_release_slots(dev, tlb_addr);",
            "}",
            "void swiotlb_sync_single_for_device(struct device *dev, phys_addr_t tlb_addr,",
            "\t\tsize_t size, enum dma_data_direction dir)",
            "{",
            "\tif (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)",
            "\t\tswiotlb_bounce(dev, tlb_addr, size, DMA_TO_DEVICE);",
            "\telse",
            "\t\tBUG_ON(dir != DMA_FROM_DEVICE);",
            "}",
            "void swiotlb_sync_single_for_cpu(struct device *dev, phys_addr_t tlb_addr,",
            "\t\tsize_t size, enum dma_data_direction dir)",
            "{",
            "\tif (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL)",
            "\t\tswiotlb_bounce(dev, tlb_addr, size, DMA_FROM_DEVICE);",
            "\telse",
            "\t\tBUG_ON(dir != DMA_TO_DEVICE);",
            "}",
            "dma_addr_t swiotlb_map(struct device *dev, phys_addr_t paddr, size_t size,",
            "\t\tenum dma_data_direction dir, unsigned long attrs)",
            "{",
            "\tphys_addr_t swiotlb_addr;",
            "\tdma_addr_t dma_addr;",
            "",
            "\ttrace_swiotlb_bounced(dev, phys_to_dma(dev, paddr), size);",
            "",
            "\tswiotlb_addr = swiotlb_tbl_map_single(dev, paddr, size, size, 0, dir,",
            "\t\t\tattrs);",
            "\tif (swiotlb_addr == (phys_addr_t)DMA_MAPPING_ERROR)",
            "\t\treturn DMA_MAPPING_ERROR;",
            "",
            "\t/* Ensure that the address returned is DMA'ble */",
            "\tdma_addr = phys_to_dma_unencrypted(dev, swiotlb_addr);",
            "\tif (unlikely(!dma_capable(dev, dma_addr, size, true))) {",
            "\t\tswiotlb_tbl_unmap_single(dev, swiotlb_addr, size, dir,",
            "\t\t\tattrs | DMA_ATTR_SKIP_CPU_SYNC);",
            "\t\tdev_WARN_ONCE(dev, 1,",
            "\t\t\t\"swiotlb addr %pad+%zu overflow (mask %llx, bus limit %llx).\\n\",",
            "\t\t\t&dma_addr, size, *dev->dma_mask, dev->bus_dma_limit);",
            "\t\treturn DMA_MAPPING_ERROR;",
            "\t}",
            "",
            "\tif (!dev_is_dma_coherent(dev) && !(attrs & DMA_ATTR_SKIP_CPU_SYNC))",
            "\t\tarch_sync_dma_for_device(swiotlb_addr, size, dir);",
            "\treturn dma_addr;",
            "}",
            "size_t swiotlb_max_mapping_size(struct device *dev)",
            "{",
            "\tint min_align_mask = dma_get_min_align_mask(dev);",
            "\tint min_align = 0;",
            "",
            "\t/*",
            "\t * swiotlb_find_slots() skips slots according to",
            "\t * min align mask. This affects max mapping size.",
            "\t * Take it into acount here.",
            "\t */",
            "\tif (min_align_mask)",
            "\t\tmin_align = roundup(min_align_mask, IO_TLB_SIZE);",
            "",
            "\treturn ((size_t)IO_TLB_SIZE) * IO_TLB_SEGSIZE - min_align;",
            "}",
            "bool is_swiotlb_allocated(void)",
            "{",
            "\treturn io_tlb_default_mem.nslabs;",
            "}",
            "bool is_swiotlb_active(struct device *dev)",
            "{",
            "\tstruct io_tlb_mem *mem = dev->dma_io_tlb_mem;",
            "",
            "\treturn mem && mem->nslabs;",
            "}"
          ],
          "function_name": "swiotlb_del_transient, swiotlb_del_transient, swiotlb_tbl_unmap_single, swiotlb_sync_single_for_device, swiotlb_sync_single_for_cpu, swiotlb_map, swiotlb_max_mapping_size, is_swiotlb_allocated, is_swiotlb_active",
          "description": "实现临时池清理逻辑和DMA同步接口，提供设备方向同步及地址有效性验证，计算最大映射尺寸并暴露SWIOTLB运行状态检测函数。",
          "similarity": 0.3975580334663391
        },
        {
          "chunk_id": 9,
          "file_path": "kernel/dma/swiotlb.c",
          "start_line": 1558,
          "end_line": 1695,
          "content": [
            "phys_addr_t default_swiotlb_base(void)",
            "{",
            "#ifdef CONFIG_SWIOTLB_DYNAMIC",
            "\tio_tlb_default_mem.can_grow = false;",
            "#endif",
            "\treturn io_tlb_default_mem.defpool.start;",
            "}",
            "phys_addr_t default_swiotlb_limit(void)",
            "{",
            "#ifdef CONFIG_SWIOTLB_DYNAMIC",
            "\treturn io_tlb_default_mem.phys_limit;",
            "#else",
            "\treturn io_tlb_default_mem.defpool.end - 1;",
            "#endif",
            "}",
            "static int io_tlb_used_get(void *data, u64 *val)",
            "{",
            "\tstruct io_tlb_mem *mem = data;",
            "",
            "\t*val = mem_used(mem);",
            "\treturn 0;",
            "}",
            "static int io_tlb_hiwater_get(void *data, u64 *val)",
            "{",
            "\tstruct io_tlb_mem *mem = data;",
            "",
            "\t*val = atomic_long_read(&mem->used_hiwater);",
            "\treturn 0;",
            "}",
            "static int io_tlb_hiwater_set(void *data, u64 val)",
            "{",
            "\tstruct io_tlb_mem *mem = data;",
            "",
            "\t/* Only allow setting to zero */",
            "\tif (val != 0)",
            "\t\treturn -EINVAL;",
            "",
            "\tatomic_long_set(&mem->used_hiwater, val);",
            "\treturn 0;",
            "}",
            "static void swiotlb_create_debugfs_files(struct io_tlb_mem *mem,",
            "\t\t\t\t\t const char *dirname)",
            "{",
            "\tatomic_long_set(&mem->total_used, 0);",
            "\tatomic_long_set(&mem->used_hiwater, 0);",
            "",
            "\tmem->debugfs = debugfs_create_dir(dirname, io_tlb_default_mem.debugfs);",
            "\tif (!mem->nslabs)",
            "\t\treturn;",
            "",
            "\tdebugfs_create_ulong(\"io_tlb_nslabs\", 0400, mem->debugfs, &mem->nslabs);",
            "\tdebugfs_create_file(\"io_tlb_used\", 0400, mem->debugfs, mem,",
            "\t\t\t&fops_io_tlb_used);",
            "\tdebugfs_create_file(\"io_tlb_used_hiwater\", 0600, mem->debugfs, mem,",
            "\t\t\t&fops_io_tlb_hiwater);",
            "}",
            "static int __init swiotlb_create_default_debugfs(void)",
            "{",
            "\tswiotlb_create_debugfs_files(&io_tlb_default_mem, \"swiotlb\");",
            "\treturn 0;",
            "}",
            "static inline void swiotlb_create_debugfs_files(struct io_tlb_mem *mem,",
            "\t\t\t\t\t\tconst char *dirname)",
            "{",
            "}",
            "bool swiotlb_free(struct device *dev, struct page *page, size_t size)",
            "{",
            "\tphys_addr_t tlb_addr = page_to_phys(page);",
            "",
            "\tif (!is_swiotlb_buffer(dev, tlb_addr))",
            "\t\treturn false;",
            "",
            "\tswiotlb_release_slots(dev, tlb_addr);",
            "",
            "\treturn true;",
            "}",
            "static int rmem_swiotlb_device_init(struct reserved_mem *rmem,",
            "\t\t\t\t    struct device *dev)",
            "{",
            "\tstruct io_tlb_mem *mem = rmem->priv;",
            "\tunsigned long nslabs = rmem->size >> IO_TLB_SHIFT;",
            "",
            "\t/* Set Per-device io tlb area to one */",
            "\tunsigned int nareas = 1;",
            "",
            "\tif (PageHighMem(pfn_to_page(PHYS_PFN(rmem->base)))) {",
            "\t\tdev_err(dev, \"Restricted DMA pool must be accessible within the linear mapping.\");",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\t/*",
            "\t * Since multiple devices can share the same pool, the private data,",
            "\t * io_tlb_mem struct, will be initialized by the first device attached",
            "\t * to it.",
            "\t */",
            "\tif (!mem) {",
            "\t\tstruct io_tlb_pool *pool;",
            "",
            "\t\tmem = kzalloc(sizeof(*mem), GFP_KERNEL);",
            "\t\tif (!mem)",
            "\t\t\treturn -ENOMEM;",
            "\t\tpool = &mem->defpool;",
            "",
            "\t\tpool->slots = kcalloc(nslabs, sizeof(*pool->slots), GFP_KERNEL);",
            "\t\tif (!pool->slots) {",
            "\t\t\tkfree(mem);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "",
            "\t\tpool->areas = kcalloc(nareas, sizeof(*pool->areas),",
            "\t\t\t\tGFP_KERNEL);",
            "\t\tif (!pool->areas) {",
            "\t\t\tkfree(pool->slots);",
            "\t\t\tkfree(mem);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "",
            "\t\tset_memory_decrypted((unsigned long)phys_to_virt(rmem->base),",
            "\t\t\t\t     rmem->size >> PAGE_SHIFT);",
            "\t\tswiotlb_init_io_tlb_pool(pool, rmem->base, nslabs,",
            "\t\t\t\t\t false, nareas);",
            "\t\tmem->force_bounce = true;",
            "\t\tmem->for_alloc = true;",
            "#ifdef CONFIG_SWIOTLB_DYNAMIC",
            "\t\tspin_lock_init(&mem->lock);",
            "\t\tINIT_LIST_HEAD_RCU(&mem->pools);",
            "#endif",
            "\t\tadd_mem_pool(mem, pool);",
            "",
            "\t\trmem->priv = mem;",
            "",
            "\t\tswiotlb_create_debugfs_files(mem, rmem->name);",
            "\t}",
            "",
            "\tdev->dma_io_tlb_mem = mem;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "default_swiotlb_base, default_swiotlb_limit, io_tlb_used_get, io_tlb_hiwater_get, io_tlb_hiwater_set, swiotlb_create_debugfs_files, swiotlb_create_default_debugfs, swiotlb_create_debugfs_files, swiotlb_free, rmem_swiotlb_device_init",
          "description": "定义SWIOTLB默认参数获取接口，建立调试文件系统节点用于监控使用率和高水位标记，实现预留内存区域初始化及私有数据结构构建流程。",
          "similarity": 0.34160324931144714
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/dma/swiotlb.c",
          "start_line": 1752,
          "end_line": 1771,
          "content": [
            "static void rmem_swiotlb_device_release(struct reserved_mem *rmem,",
            "\t\t\t\t\tstruct device *dev)",
            "{",
            "\tdev->dma_io_tlb_mem = &io_tlb_default_mem;",
            "}",
            "static int __init rmem_swiotlb_setup(struct reserved_mem *rmem)",
            "{",
            "\tunsigned long node = rmem->fdt_node;",
            "",
            "\tif (of_get_flat_dt_prop(node, \"reusable\", NULL) ||",
            "\t    of_get_flat_dt_prop(node, \"linux,cma-default\", NULL) ||",
            "\t    of_get_flat_dt_prop(node, \"linux,dma-default\", NULL) ||",
            "\t    of_get_flat_dt_prop(node, \"no-map\", NULL))",
            "\t\treturn -EINVAL;",
            "",
            "\trmem->ops = &rmem_swiotlb_ops;",
            "\tpr_info(\"Reserved memory: created restricted DMA pool at %pa, size %ld MiB\\n\",",
            "\t\t&rmem->base, (unsigned long)rmem->size / SZ_1M);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rmem_swiotlb_device_release, rmem_swiotlb_setup",
          "description": "这段代码实现了与保留内存DMA池相关的初始化和资源释放逻辑。  \n`rmem_swiotlb_device_release`用于将设备的DMA TLB内存指向默认的`io_tlb_default_mem`结构。  \n`rmem_swiotlb_setup`在初始化时校验设备树属性并注册专用DMA操作接口，但因上下文不完整无法确定`rmem_swiotlb_ops`的全部实现细节。",
          "similarity": 0.3327735662460327
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/dma/swiotlb.c",
          "start_line": 927,
          "end_line": 1068,
          "content": [
            "static inline phys_addr_t slot_addr(phys_addr_t start, phys_addr_t idx)",
            "{",
            "\treturn start + (idx << IO_TLB_SHIFT);",
            "}",
            "static inline unsigned long get_max_slots(unsigned long boundary_mask)",
            "{",
            "\treturn (boundary_mask >> IO_TLB_SHIFT) + 1;",
            "}",
            "static unsigned int wrap_area_index(struct io_tlb_pool *mem, unsigned int index)",
            "{",
            "\tif (index >= mem->area_nslabs)",
            "\t\treturn 0;",
            "\treturn index;",
            "}",
            "static void inc_used_and_hiwater(struct io_tlb_mem *mem, unsigned int nslots)",
            "{",
            "\tunsigned long old_hiwater, new_used;",
            "",
            "\tnew_used = atomic_long_add_return(nslots, &mem->total_used);",
            "\told_hiwater = atomic_long_read(&mem->used_hiwater);",
            "\tdo {",
            "\t\tif (new_used <= old_hiwater)",
            "\t\t\tbreak;",
            "\t} while (!atomic_long_try_cmpxchg(&mem->used_hiwater,",
            "\t\t\t\t\t  &old_hiwater, new_used));",
            "}",
            "static void dec_used(struct io_tlb_mem *mem, unsigned int nslots)",
            "{",
            "\tatomic_long_sub(nslots, &mem->total_used);",
            "}",
            "static void inc_used_and_hiwater(struct io_tlb_mem *mem, unsigned int nslots)",
            "{",
            "}",
            "static void dec_used(struct io_tlb_mem *mem, unsigned int nslots)",
            "{",
            "}",
            "static int swiotlb_area_find_slots(struct device *dev, struct io_tlb_pool *pool,",
            "\t\tint area_index, phys_addr_t orig_addr, size_t alloc_size,",
            "\t\tunsigned int alloc_align_mask)",
            "{",
            "\tstruct io_tlb_area *area = pool->areas + area_index;",
            "\tunsigned long boundary_mask = dma_get_seg_boundary(dev);",
            "\tdma_addr_t tbl_dma_addr =",
            "\t\tphys_to_dma_unencrypted(dev, pool->start) & boundary_mask;",
            "\tunsigned long max_slots = get_max_slots(boundary_mask);",
            "\tunsigned int iotlb_align_mask = dma_get_min_align_mask(dev);",
            "\tunsigned int nslots = nr_slots(alloc_size), stride;",
            "\tunsigned int offset = swiotlb_align_offset(dev, 0, orig_addr);",
            "\tunsigned int index, slots_checked, count = 0, i;",
            "\tunsigned long flags;",
            "\tunsigned int slot_base;",
            "\tunsigned int slot_index;",
            "",
            "\tBUG_ON(!nslots);",
            "\tBUG_ON(area_index >= pool->nareas);",
            "",
            "\t/*",
            "\t * Historically, swiotlb allocations >= PAGE_SIZE were guaranteed to be",
            "\t * page-aligned in the absence of any other alignment requirements.",
            "\t * 'alloc_align_mask' was later introduced to specify the alignment",
            "\t * explicitly, however this is passed as zero for streaming mappings",
            "\t * and so we preserve the old behaviour there in case any drivers are",
            "\t * relying on it.",
            "\t */",
            "\tif (!alloc_align_mask && !iotlb_align_mask && alloc_size >= PAGE_SIZE)",
            "\t\talloc_align_mask = PAGE_SIZE - 1;",
            "",
            "\t/*",
            "\t * Ensure that the allocation is at least slot-aligned and update",
            "\t * 'iotlb_align_mask' to ignore bits that will be preserved when",
            "\t * offsetting into the allocation.",
            "\t */",
            "\talloc_align_mask |= (IO_TLB_SIZE - 1);",
            "\tiotlb_align_mask &= ~alloc_align_mask;",
            "",
            "\t/*",
            "\t * For mappings with an alignment requirement don't bother looping to",
            "\t * unaligned slots once we found an aligned one.",
            "\t */",
            "\tstride = get_max_slots(max(alloc_align_mask, iotlb_align_mask));",
            "",
            "\tspin_lock_irqsave(&area->lock, flags);",
            "\tif (unlikely(nslots > pool->area_nslabs - area->used))",
            "\t\tgoto not_found;",
            "",
            "\tslot_base = area_index * pool->area_nslabs;",
            "\tindex = area->index;",
            "",
            "\tfor (slots_checked = 0; slots_checked < pool->area_nslabs; ) {",
            "\t\tphys_addr_t tlb_addr;",
            "",
            "\t\tslot_index = slot_base + index;",
            "\t\ttlb_addr = slot_addr(tbl_dma_addr, slot_index);",
            "",
            "\t\tif ((tlb_addr & alloc_align_mask) ||",
            "\t\t    (orig_addr && (tlb_addr & iotlb_align_mask) !=",
            "\t\t\t\t  (orig_addr & iotlb_align_mask))) {",
            "\t\t\tindex = wrap_area_index(pool, index + 1);",
            "\t\t\tslots_checked++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tif (!iommu_is_span_boundary(slot_index, nslots,",
            "\t\t\t\t\t    nr_slots(tbl_dma_addr),",
            "\t\t\t\t\t    max_slots)) {",
            "\t\t\tif (pool->slots[slot_index].list >= nslots)",
            "\t\t\t\tgoto found;",
            "\t\t}",
            "\t\tindex = wrap_area_index(pool, index + stride);",
            "\t\tslots_checked += stride;",
            "\t}",
            "",
            "not_found:",
            "\tspin_unlock_irqrestore(&area->lock, flags);",
            "\treturn -1;",
            "",
            "found:",
            "\t/*",
            "\t * If we find a slot that indicates we have 'nslots' number of",
            "\t * contiguous buffers, we allocate the buffers from that slot onwards",
            "\t * and set the list of free entries to '0' indicating unavailable.",
            "\t */",
            "\tfor (i = slot_index; i < slot_index + nslots; i++) {",
            "\t\tpool->slots[i].list = 0;",
            "\t\tpool->slots[i].alloc_size = alloc_size - (offset +",
            "\t\t\t\t((i - slot_index) << IO_TLB_SHIFT));",
            "\t}",
            "\tfor (i = slot_index - 1;",
            "\t     io_tlb_offset(i) != IO_TLB_SEGSIZE - 1 &&",
            "\t     pool->slots[i].list; i--)",
            "\t\tpool->slots[i].list = ++count;",
            "",
            "\t/*",
            "\t * Update the indices to avoid searching in the next round.",
            "\t */",
            "\tarea->index = wrap_area_index(pool, index + nslots);",
            "\tarea->used += nslots;",
            "\tspin_unlock_irqrestore(&area->lock, flags);",
            "",
            "\tinc_used_and_hiwater(dev->dma_io_tlb_mem, nslots);",
            "\treturn slot_index;",
            "}"
          ],
          "function_name": "slot_addr, get_max_slots, wrap_area_index, inc_used_and_hiwater, dec_used, inc_used_and_hiwater, dec_used, swiotlb_area_find_slots",
          "description": "定义槽地址计算和最大槽数获取函数，用于管理SWIOTLB内存池的槽索引转换与边界检查，并实现槽分配搜索逻辑，通过遍历区域索引寻找符合对齐和边界条件的连续槽位。",
          "similarity": 0.3271175026893616
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/swiotlb.c",
          "start_line": 131,
          "end_line": 233,
          "content": [
            "static bool round_up_default_nslabs(void)",
            "{",
            "\tif (!default_nareas)",
            "\t\treturn false;",
            "",
            "\tif (default_nslabs < IO_TLB_SEGSIZE * default_nareas)",
            "\t\tdefault_nslabs = IO_TLB_SEGSIZE * default_nareas;",
            "\telse if (is_power_of_2(default_nslabs))",
            "\t\treturn false;",
            "\tdefault_nslabs = roundup_pow_of_two(default_nslabs);",
            "\treturn true;",
            "}",
            "static void swiotlb_adjust_nareas(unsigned int nareas)",
            "{",
            "\tif (!nareas)",
            "\t\tnareas = 1;",
            "\telse if (!is_power_of_2(nareas))",
            "\t\tnareas = roundup_pow_of_two(nareas);",
            "",
            "\tdefault_nareas = nareas;",
            "",
            "\tpr_info(\"area num %d.\\n\", nareas);",
            "\tif (round_up_default_nslabs())",
            "\t\tpr_info(\"SWIOTLB bounce buffer size roundup to %luMB\",",
            "\t\t\t(default_nslabs << IO_TLB_SHIFT) >> 20);",
            "}",
            "static unsigned int limit_nareas(unsigned int nareas, unsigned long nslots)",
            "{",
            "\tif (nslots < nareas * IO_TLB_SEGSIZE)",
            "\t\treturn nslots / IO_TLB_SEGSIZE;",
            "\treturn nareas;",
            "}",
            "static int __init",
            "setup_io_tlb_npages(char *str)",
            "{",
            "\tif (isdigit(*str)) {",
            "\t\t/* avoid tail segment of size < IO_TLB_SEGSIZE */",
            "\t\tdefault_nslabs =",
            "\t\t\tALIGN(simple_strtoul(str, &str, 0), IO_TLB_SEGSIZE);",
            "\t}",
            "\tif (*str == ',')",
            "\t\t++str;",
            "\tif (isdigit(*str))",
            "\t\tswiotlb_adjust_nareas(simple_strtoul(str, &str, 0));",
            "\tif (*str == ',')",
            "\t\t++str;",
            "\tif (!strcmp(str, \"force\"))",
            "\t\tswiotlb_force_bounce = true;",
            "\telse if (!strcmp(str, \"noforce\"))",
            "\t\tswiotlb_force_disable = true;",
            "",
            "\treturn 0;",
            "}",
            "unsigned long swiotlb_size_or_default(void)",
            "{",
            "\treturn default_nslabs << IO_TLB_SHIFT;",
            "}",
            "void __init swiotlb_adjust_size(unsigned long size)",
            "{",
            "\t/*",
            "\t * If swiotlb parameter has not been specified, give a chance to",
            "\t * architectures such as those supporting memory encryption to",
            "\t * adjust/expand SWIOTLB size for their use.",
            "\t */",
            "\tif (default_nslabs != IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT)",
            "\t\treturn;",
            "",
            "\tsize = ALIGN(size, IO_TLB_SIZE);",
            "\tdefault_nslabs = ALIGN(size >> IO_TLB_SHIFT, IO_TLB_SEGSIZE);",
            "\tif (round_up_default_nslabs())",
            "\t\tsize = default_nslabs << IO_TLB_SHIFT;",
            "\tpr_info(\"SWIOTLB bounce buffer size adjusted to %luMB\", size >> 20);",
            "}",
            "void swiotlb_print_info(void)",
            "{",
            "\tstruct io_tlb_pool *mem = &io_tlb_default_mem.defpool;",
            "",
            "\tif (!mem->nslabs) {",
            "\t\tpr_warn(\"No low mem\\n\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tpr_info(\"mapped [mem %pa-%pa] (%luMB)\\n\", &mem->start, &mem->end,",
            "\t       (mem->nslabs << IO_TLB_SHIFT) >> 20);",
            "}",
            "static inline unsigned long io_tlb_offset(unsigned long val)",
            "{",
            "\treturn val & (IO_TLB_SEGSIZE - 1);",
            "}",
            "static inline unsigned long nr_slots(u64 val)",
            "{",
            "\treturn DIV_ROUND_UP(val, IO_TLB_SIZE);",
            "}",
            "void __init swiotlb_update_mem_attributes(void)",
            "{",
            "\tstruct io_tlb_pool *mem = &io_tlb_default_mem.defpool;",
            "\tunsigned long bytes;",
            "",
            "\tif (!mem->nslabs || mem->late_alloc)",
            "\t\treturn;",
            "\tbytes = PAGE_ALIGN(mem->nslabs << IO_TLB_SHIFT);",
            "\tset_memory_decrypted((unsigned long)mem->vaddr, bytes >> PAGE_SHIFT);",
            "}"
          ],
          "function_name": "round_up_default_nslabs, swiotlb_adjust_nareas, limit_nareas, setup_io_tlb_npages, swiotlb_size_or_default, swiotlb_adjust_size, swiotlb_print_info, io_tlb_offset, nr_slots, swiotlb_update_mem_attributes",
          "description": "实现软件IO TLB大小调整逻辑，通过计算并四舍五入到最近的2的幂次，管理内存区域数量和slab数量，支持命令行参数配置及内存属性更新。",
          "similarity": 0.324663907289505
        }
      ]
    },
    {
      "source_file": "kernel/sched/sched.h",
      "md_summary": "> 自动生成时间: 2025-10-25 16:16:13\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\sched.h`\n\n---\n\n# `sched/sched.h` 技术文档\n\n## 1. 文件概述\n\n`sched/sched.h` 是 Linux 内核调度器（Scheduler）的核心内部头文件，定义了调度子系统内部使用的类型、宏、辅助函数和全局变量。该文件不对外暴露给其他子系统直接使用，而是作为调度器各组件（如 CFS、RT、Deadline 调度类）之间的内部接口和共享基础设施。它整合了任务状态管理、负载计算、策略判断、CPU 能力建模、cgroup 权重转换等关键调度逻辑，并为调试、性能追踪和平台适配提供支持。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct asym_cap_data`：用于描述非对称 CPU 架构中不同 CPU 集合的计算能力（capacity），支持异构多核系统（如 big.LITTLE）的调度优化。\n- `struct rq`（前向声明）：运行队列（runqueue）结构体，每个 CPU 对应一个，是调度器管理可运行任务的核心数据结构。\n- `struct cpuidle_state`（前向声明）：CPU 空闲状态信息，用于与调度器协同进行能效管理。\n\n### 关键全局变量\n- `scheduler_running`：标志调度器是否已启动。\n- `calc_load_update` / `calc_load_tasks`：用于全局负载（load average）计算的时间戳和任务计数。\n- `sysctl_sched_rt_period` / `sysctl_sched_rt_runtime`：实时任务带宽控制参数。\n- `sched_rr_timeslice`：SCHED_RR 策略的时间片长度。\n- `asym_cap_list`：非对称 CPU 能力数据的全局链表。\n\n### 核心辅助函数与宏\n- **任务策略判断函数**：\n  - `idle_policy()` / `task_has_idle_policy()`\n  - `normal_policy()` / `fair_policy()`\n  - `rt_policy()` / `task_has_rt_policy()`\n  - `dl_policy()` / `task_has_dl_policy()`\n  - `valid_policy()`\n- **负载与权重转换**：\n  - `scale_load()` / `scale_load_down()`：在内部高精度负载值与用户可见权重间转换。\n  - `sched_weight_from_cgroup()` / `sched_weight_to_cgroup()`：cgroup 权重与调度器内部权重的映射。\n- **时间与精度处理**：\n  - `NS_TO_JIFFIES()`：纳秒转 jiffies。\n  - `update_avg()`：指数移动平均（EMA）更新。\n  - `shr_bound()`：安全右移，避免未定义行为。\n- **特殊调度标志**：\n  - `SCHED_FLAG_SUGOV`：用于 schedutil 频率调节器的特殊标志，使相关 kworker 临时获得高于 SCHED_DEADLINE 的优先级。\n  - `dl_entity_is_special()`：判断 Deadline 实体是否为 SUGOV 特殊任务。\n\n### 重要宏定义\n- `TASK_ON_RQ_QUEUED` / `TASK_ON_RQ_MIGRATING`：`task_struct::on_rq` 字段的状态值。\n- `NICE_0_LOAD`：nice 值为 0 的任务对应的内部负载基准值。\n- `DL_SCALE`：SCHED_DEADLINE 内部计算的精度因子。\n- `RUNTIME_INF`：表示无限运行时间的常量。\n- `SCHED_WARN_ON()`：调度器专用的条件警告宏（仅在 `CONFIG_SCHED_DEBUG` 时生效）。\n\n## 3. 关键实现\n\n### 高精度负载计算（64 位优化）\n在 64 位架构上，通过 `NICE_0_LOAD_SHIFT = 2 * SCHED_FIXEDPOINT_SHIFT` 提升内部负载计算的精度，改善低权重任务组（如 nice +19）和深层 cgroup 层级的负载均衡效果。`scale_load()` 和 `scale_load_down()` 实现了用户权重与内部高精度负载值之间的无损转换。\n\n### 非对称 CPU 能力建模\n`asym_cap_data` 结构体结合 `cpu_capacity_span()` 宏，将具有相同计算能力的 CPU 归为一组，并通过全局链表 `asym_cap_list` 管理。这为调度器在异构系统中进行负载均衡和任务迁移提供关键拓扑信息。\n\n### cgroup 权重标准化\n通过 `sched_weight_from_cgroup()` 和 `sched_weight_to_cgroup()`，将 cgroup 接口的权重范围（1–10000，默认 100）映射到调度器内部使用的权重值（基于 1024 基准），确保用户配置与调度行为的一致性。\n\n### SCHED_DEADLINE 与频率调节协同\n引入 `SCHED_FLAG_SUGOV` 标志，允许 `schedutil` 频率调节器的工作线程在需要时临时突破 SCHED_DEADLINE 的优先级限制，以解决某些平台无法原子切换 CPU 频率的问题。这是一种临时性 workaround，依赖于 `dl_entity_is_special()` 进行识别。\n\n### 安全位运算\n`shr_bound()` 宏确保右移操作不会因移位数过大而触发未定义行为（UB），通过 `min_t()` 将移位数限制在 `BITS_PER_TYPE(val) - 1` 以内。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- **调度子系统内部**：包含多个调度相关子模块头文件（如 `affinity.h`, `deadline.h`, `topology.h`, `cpupri.h` 等）。\n- **核心内核设施**：依赖 `atomic.h`, `rcupdate.h`, `cpumask_api.h`, `ktime_api.h`, `trace/events/sched.h` 等。\n- **平台与虚拟化**：条件包含 `asm/paravirt.h`（半虚拟化支持）和 `asm/barrier.h`（内存屏障）。\n- **工作队列**：包含 `../workqueue_internal.h`，用于与工作队列子系统交互。\n\n### 配置选项依赖\n- `CONFIG_64BIT`：启用高精度负载计算。\n- `CONFIG_SCHED_DEBUG`：启用 `SCHED_WARN_ON()` 调试检查。\n- `CONFIG_CPU_FREQ_GOV_SCHEDUTIL`：启用 `SCHED_FLAG_SUGOV` 相关逻辑。\n- `CONFIG_SCHED_CLASS_EXT`：扩展调度类支持（影响 `normal_policy()` 判断）。\n- `CONFIG_PARAVIRT`：半虚拟化支持。\n\n## 5. 使用场景\n\n- **调度器初始化与运行**：`scheduler_running` 和负载计算变量在调度器启动和周期性负载更新中使用。\n- **任务调度策略处理**：所有调度类（CFS、RT、Deadline、Idle）在入队、出队、选择下一个任务时，通过策略判断函数确定任务类型。\n- **负载均衡与迁移**：`asym_cap_data` 和 CPU 拓扑信息用于跨 CPU 的任务迁移决策，尤其在异构系统中。\n- **cgroup 资源控制**：在设置或读取 cgroup 的 CPU 权重时，通过权重转换函数确保调度器内部表示与用户接口一致。\n- **实时带宽管理**：`sysctl_sched_rt_*` 参数用于限制 SCHED_FIFO/SCHED_RR 任务的 CPU 使用率。\n- **能效调度协同**：`SCHED_FLAG_SUGOV` 机制使频率调节器能及时响应 Deadline 任务的性能需求。\n- **内核调试与追踪**：`SCHED_WARN_ON()` 用于捕获调度器内部异常状态；tracepoint 定义支持调度事件追踪。",
      "similarity": 0.36852216720581055,
      "chunks": []
    }
  ]
}