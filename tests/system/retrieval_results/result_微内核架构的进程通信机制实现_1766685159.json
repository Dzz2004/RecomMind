{
  "query": "微内核架构的进程通信机制实现",
  "timestamp": "2025-12-26 01:52:39",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/semaphore.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:52:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\semaphore.c`\n\n---\n\n# `locking/semaphore.c` 技术文档\n\n## 1. 文件概述\n\n`locking/semaphore.c` 实现了 Linux 内核中的**计数信号量（counting semaphore）**机制。计数信号量允许多个任务（最多为初始计数值）同时持有该锁，当计数值耗尽时，后续请求者将被阻塞，直到有其他任务释放信号量。与互斥锁（mutex）不同，信号量支持更灵活的并发控制，适用于资源池、限流等场景。该文件提供了多种获取和释放信号量的接口，包括可中断、可超时、不可中断等变体，并支持在中断上下文中调用部分函数。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `down(struct semaphore *sem)` | 不可中断地获取信号量，若不可用则睡眠。**已弃用**，建议使用可中断版本。 |\n| `down_interruptible(struct semaphore *sem)` | 可被普通信号中断的获取操作，成功返回 0，被信号中断返回 `-EINTR`。 |\n| `down_killable(struct semaphore *sem)` | 可被致命信号（fatal signal）中断的获取操作，返回值同上。 |\n| `down_trylock(struct semaphore *sem)` | 非阻塞尝试获取信号量，成功返回 0，失败返回 1（**注意返回值与 mutex/spinlock 相反**）。 |\n| `down_timeout(struct semaphore *sem, long timeout)` | 带超时的获取操作，超时返回 `-ETIME`，成功返回 0。 |\n| `up(struct semaphore *sem)` | 释放信号量，可由任意上下文（包括中断）调用，唤醒等待队列中的任务。 |\n\n### 静态辅助函数\n\n- `__down*()` 系列：处理信号量争用时的阻塞逻辑。\n- `__up()`：在有等待者时执行唤醒逻辑。\n- `___down_common()`：通用的阻塞等待实现，支持不同睡眠状态和超时。\n- `__sem_acquire()`：原子减少计数并记录持有者（用于 hung task 检测）。\n\n### 数据结构\n\n- `struct semaphore`（定义在 `<linux/semaphore.h>`）：\n  - `count`：当前可用资源数（>0 表示可立即获取）。\n  - `wait_list`：等待该信号量的任务链表。\n  - `lock`：保护上述成员的原始自旋锁（`raw_spinlock_t`）。\n  - `last_holder`（条件编译）：记录最后持有者，用于 `CONFIG_DETECT_HUNG_TASK_BLOCKER`。\n\n- `struct semaphore_waiter`：\n  - 用于将任务加入等待队列，包含任务指针和唤醒标志（`up`）。\n\n## 3. 关键实现\n\n### 中断安全与自旋锁\n- 所有对外接口（包括 `down*` 和 `up`）均使用 `raw_spin_lock_irqsave()` 获取自旋锁，确保在中断上下文安全。\n- 即使 `down()` 等函数通常在进程上下文调用，也使用 `irqsave` 变体，因为内核某些部分依赖在中断上下文成功调用 `down()`（当确定信号量可用时）。\n\n### 计数语义\n- `sem->count` 表示**还可被获取的次数**。初始值由 `sema_init()` 设置。\n- 获取时：若 `count > 0`，直接减 1；否则加入等待队列。\n- 释放时：若等待队列为空，`count++`；否则唤醒队首任务。\n\n### 等待与唤醒机制\n- 使用 `wake_q`（批量唤醒队列）优化唤醒路径，避免在持有自旋锁时调用 `wake_up_process()`。\n- 等待任务通过 `schedule_timeout()` 睡眠，并在循环中检查：\n  - 是否收到信号（根据睡眠状态判断）。\n  - 是否超时。\n  - 是否被 `__up()` 标记为 `waiter.up = true`（表示已被选中唤醒）。\n\n### Hung Task 支持\n- 当启用 `CONFIG_DETECT_HUNG_TASK_BLOCKER` 时：\n  - 获取信号量时记录当前任务为 `last_holder`。\n  - 释放时若当前任务是持有者，则清除记录。\n  - 提供 `sem_last_holder()` 供 hung task 检测模块查询阻塞源头。\n\n### 返回值约定\n- `down_trylock()` 返回 **0 表示成功**，**1 表示失败**，这与 `mutex_trylock()` 和 `spin_trylock()` **相反**，需特别注意。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/semaphore.h>`：信号量结构体和 API 声明。\n  - `<linux/spinlock.h>`：原始自旋锁实现。\n  - `<linux/sched.h>`、`<linux/sched/wake_q.h>`：任务调度和批量唤醒。\n  - `<trace/events/lock.h>`：锁争用跟踪点。\n  - `<linux/hung_task.h>`：hung task 检测支持。\n\n- **内核配置依赖**：\n  - `CONFIG_DETECT_HUNG_TASK_BLOCKER`：启用信号量持有者跟踪。\n\n- **与其他同步原语关系**：\n  - 与 `mutex.c` 形成对比：mutex 是二值、不可递归、带调试信息的互斥锁；信号量是计数、可被任意任务释放、更轻量。\n  - 底层依赖调度器（`schedule_timeout`）和中断管理（`irqsave`）。\n\n## 5. 使用场景\n\n- **资源池管理**：如限制同时访问某类硬件设备的任务数量。\n- **读写并发控制**：配合其他机制实现多读者/单写者模型。\n- **内核驱动**：设备驱动中控制对共享资源的并发访问。\n- **中断上下文释放**：因 `up()` 可在中断中调用，适用于中断处理程序释放资源的场景。\n- **不可睡眠路径**：使用 `down_trylock()` 在原子上下文尝试获取资源。\n\n> **注意**：由于信号量不强制所有权（任意任务可调用 `up()`），且缺乏死锁检测等调试特性，现代内核开发中更推荐使用 `mutex` 或 `rwsem`，除非明确需要计数语义或多释放者特性。",
      "similarity": 0.6261860132217407,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "\tWRITE_ONCE((sem)->last_holder, (unsigned long)current);",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "\tif (READ_ONCE((sem)->last_holder) == (unsigned long)current)",
            "\t\tWRITE_ONCE((sem)->last_holder, 0UL);",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn READ_ONCE(sem->last_holder);",
            "}",
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn 0UL;",
            "}",
            "static inline void __sem_acquire(struct semaphore *sem)",
            "{",
            "\tsem->count--;",
            "\thung_task_sem_set_holder(sem);",
            "}",
            "void __sched down(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\t__down(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "}",
            "int __sched down_interruptible(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_interruptible(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_killable(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_killable(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_trylock(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint count;",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tcount = sem->count - 1;",
            "\tif (likely(count >= 0))",
            "\t\t__sem_acquire(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn (count < 0);",
            "}",
            "int __sched down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_timeout(sem, timeout);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "void __sched up(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "",
            "\thung_task_sem_clear_if_holder(sem);",
            "",
            "\tif (likely(list_empty(&sem->wait_list)))",
            "\t\tsem->count++;",
            "\telse",
            "\t\t__up(sem, &wake_q);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "\tif (!wake_q_empty(&wake_q))",
            "\t\twake_up_q(&wake_q);",
            "}"
          ],
          "function_name": "hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, __sem_acquire, down, down_interruptible, down_killable, down_trylock, down_timeout, up",
          "description": "实现了信号量的获取与释放核心逻辑，包括down/down_interruptible/down_killable/down_trylock/down_timeout等接口，通过spinlock保护共享资源，维护等待队列并处理任务状态变更，其中包含Hung Task检测相关函数的条件性实现",
          "similarity": 0.6358969211578369
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 252,
          "end_line": 323,
          "content": [
            "static inline int __sched ___down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\t\t\t\tlong timeout)",
            "{",
            "\tstruct semaphore_waiter waiter;",
            "",
            "\tlist_add_tail(&waiter.list, &sem->wait_list);",
            "\twaiter.task = current;",
            "\twaiter.up = false;",
            "",
            "\tfor (;;) {",
            "\t\tif (signal_pending_state(state, current))",
            "\t\t\tgoto interrupted;",
            "\t\tif (unlikely(timeout <= 0))",
            "\t\t\tgoto timed_out;",
            "\t\t__set_current_state(state);",
            "\t\traw_spin_unlock_irq(&sem->lock);",
            "\t\ttimeout = schedule_timeout(timeout);",
            "\t\traw_spin_lock_irq(&sem->lock);",
            "\t\tif (waiter.up) {",
            "\t\t\thung_task_sem_set_holder(sem);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            " timed_out:",
            "\tlist_del(&waiter.list);",
            "\treturn -ETIME;",
            "",
            " interrupted:",
            "\tlist_del(&waiter.list);",
            "\treturn -EINTR;",
            "}",
            "static inline int __sched __down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\tlong timeout)",
            "{",
            "\tint ret;",
            "",
            "\thung_task_set_blocker(sem, BLOCKER_TYPE_SEM);",
            "",
            "\ttrace_contention_begin(sem, 0);",
            "\tret = ___down_common(sem, state, timeout);",
            "\ttrace_contention_end(sem, ret);",
            "",
            "\thung_task_clear_blocker();",
            "",
            "\treturn ret;",
            "}",
            "static noinline void __sched __down(struct semaphore *sem)",
            "{",
            "\t__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_interruptible(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_killable(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_KILLABLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\treturn __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);",
            "}",
            "static noinline void __sched __up(struct semaphore *sem,",
            "\t\t\t\t  struct wake_q_head *wake_q)",
            "{",
            "\tstruct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,",
            "\t\t\t\t\t\tstruct semaphore_waiter, list);",
            "\tlist_del(&waiter->list);",
            "\twaiter->up = true;",
            "\twake_q_add(wake_q, waiter->task);",
            "}"
          ],
          "function_name": "___down_common, __down_common, __down, __down_interruptible, __down_killable, __down_timeout, __up",
          "description": "实现了信号量的阻塞等待通用逻辑，包含___down_common/__down_common等辅助函数，处理信号量不足时的任务挂起、超时检测、信号处理及唤醒机制，通过循环等待并结合schedule_timeout实现阻塞式资源竞争解决",
          "similarity": 0.5526577830314636
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2008 Intel Corporation",
            " * Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This file implements counting semaphores.",
            " * A counting semaphore may be acquired 'n' times before sleeping.",
            " * See mutex.c for single-acquisition sleeping locks which enforce",
            " * rules which allow code to be debugged more easily.",
            " */",
            "",
            "/*",
            " * Some notes on the implementation:",
            " *",
            " * The spinlock controls access to the other members of the semaphore.",
            " * down_trylock() and up() can be called from interrupt context, so we",
            " * have to disable interrupts when taking the lock.  It turns out various",
            " * parts of the kernel expect to be able to use down() on a semaphore in",
            " * interrupt context when they know it will succeed, so we have to use",
            " * irqsave variants for down(), down_interruptible() and down_killable()",
            " * too.",
            " *",
            " * The ->count variable represents how many more tasks can acquire this",
            " * semaphore.  If it's zero, there may be tasks waiting on the wait_list.",
            " */",
            "",
            "#include <linux/compiler.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/semaphore.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/ftrace.h>",
            "#include <trace/events/lock.h>",
            "#include <linux/hung_task.h>",
            "",
            "static noinline void __down(struct semaphore *sem);",
            "static noinline int __down_interruptible(struct semaphore *sem);",
            "static noinline int __down_killable(struct semaphore *sem);",
            "static noinline int __down_timeout(struct semaphore *sem, long timeout);",
            "static noinline void __up(struct semaphore *sem, struct wake_q_head *wake_q);",
            "",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER"
          ],
          "function_name": null,
          "description": "此代码块定义了计数信号量的基础框架，包含实现计数信号量所需的头文件和注释，声明了多个内联函数及辅助函数，用于处理信号量的获取、释放及Hung Task检测相关逻辑，但由于代码截断，CONFIG_DETECT_HUNG_TASK_BLOCKER部分缺失，上下文不完整",
          "similarity": 0.5116367340087891
        }
      ]
    },
    {
      "source_file": "kernel/fork.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:30:07\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `fork.c`\n\n---\n\n# fork.c 技术文档\n\n## 1. 文件概述\n\n`fork.c` 是 Linux 内核中实现进程创建（fork）系统调用的核心源文件。该文件包含了创建新进程所需的所有辅助例程，负责复制父进程的资源（如内存、文件描述符、信号处理等）以生成子进程。虽然 fork 逻辑本身概念简单，但其涉及的内存管理（尤其是写时复制 COW 机制）极为复杂，实际内存页的复制由 `mm/memory.c` 中的 `copy_page_range()` 等函数处理。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `total_forks`: 累计系统自启动以来创建的进程总数\n- `nr_threads`: 当前系统中的线程总数（不包括 idle 线程）\n- `max_threads`: 可配置的线程数量上限（默认为 `FUTEX_TID_MASK`）\n- `process_counts`: 每 CPU 的进程计数器（per-CPU 变量）\n- `tasklist_lock`: 保护任务链表的读写锁（全局任务列表的同步原语）\n\n### 关键辅助函数\n- `nr_processes()`: 计算系统中所有进程的总数（聚合各 CPU 的 `process_counts`）\n- `arch_release_task_struct()`: 架构相关的 task_struct 释放钩子（弱符号，默认为空）\n- `alloc_task_struct_node()` / `free_task_struct()`: 分配/释放 `task_struct` 结构（基于 slab 分配器）\n- `alloc_thread_stack_node()` / `thread_stack_delayed_free()`: 分配/延迟释放线程内核栈（支持 `CONFIG_VMAP_STACK`）\n\n### 核心数据结构\n- `resident_page_types[]`: 用于内存统计的页面类型名称映射数组\n- `vm_stack`: 用于 RCU 延迟释放的虚拟内存栈封装结构\n- `cached_stacks[NR_CACHED_STACKS]`: 每 CPU 的内核栈缓存（减少频繁 vmalloc/vfree 开销）\n\n## 3. 关键实现\n\n### 进程/线程计数管理\n- 使用 per-CPU 变量 `process_counts` 避免全局锁竞争\n- 全局计数器 `nr_threads` 和 `total_forks` 由 `tasklist_lock` 保护\n- `nr_processes()` 通过遍历所有可能的 CPU 聚合计数\n\n### 内核栈分配策略（`CONFIG_VMAP_STACK`）\n- **缓存机制**：每个 CPU 缓存最多 2 个已释放的栈（`NR_CACHED_STACKS`），减少 TLB 刷新和 vmalloc 开销\n- **内存分配**：\n  - 优先从本地缓存获取栈\n  - 缓存未命中时使用 `__vmalloc_node_range()` 分配连续虚拟地址空间\n  - 显式禁用 `__GFP_ACCOUNT`（因后续手动进行 memcg 计费）\n- **安全清理**：\n  - 重用栈时清零内存（`memset(stack, 0, THREAD_SIZE)`）\n  - KASAN 消毒（`kasan_unpoison_range`）和标签重置\n- **延迟释放**：\n  - 通过 RCU 机制延迟释放栈（`call_rcu`）\n  - 释放时尝试回填缓存，失败则直接 `vfree`\n\n### 内存控制组（memcg）集成\n- 手动对栈的每个物理页进行 memcg 计费（`memcg_kmem_charge_page`）\n- 计费失败时回滚已计费页面（`memcg_kmem_uncharge_page`）\n- 确保内核栈内存纳入 cgroup 内存限制\n\n### 锁与同步\n- `tasklist_lock` 作为全局任务列表的保护锁（读写锁）\n- 提供 `lockdep_tasklist_lock_is_held()` 供 RCU 锁验证使用\n- RCU 用于安全延迟释放内核栈资源\n\n## 4. 依赖关系\n\n### 内核子系统依赖\n- **内存管理 (MM)**：`<linux/mm.h>`, `<linux/vmalloc.h>`, `<linux/memcontrol.h>`\n- **调度器 (Scheduler)**：`<linux/sched/*.h>`, 任务状态和 CPU 绑定\n- **安全模块**：`<linux/security.h>`, `<linux/capability.h>`, `<linux/seccomp.h>`\n- **命名空间**：`<linux/nsproxy.h>`（UTS, IPC, PID, 网络等）\n- **文件系统**：`<linux/fs.h>`, `<linux/fdtable.h>`（文件描述符复制）\n- **跟踪与调试**：`<trace/events/sched.h>`, `<linux/ftrace.h>`, KASAN/KMSAN\n\n### 架构相关依赖\n- `<asm/pgalloc.h>`：页表分配\n- `<asm/mmu_context.h>`：MMU 上下文切换\n- `<asm/tlbflush.h>`：TLB 刷新操作\n- 架构特定的 `THREAD_SIZE` 和栈对齐要求\n\n### 配置选项依赖\n- `CONFIG_VMAP_STACK`：启用虚拟内存分配内核栈\n- `CONFIG_PROVE_RCU`：RCU 锁验证支持\n- `CONFIG_ARCH_TASK_STRUCT_ALLOCATOR`：架构自定义 task_struct 分配器\n- `CONFIG_MEMCG_KMEM`：内核内存 cgroup 支持\n\n## 5. 使用场景\n\n### 进程创建路径\n- **系统调用入口**：`sys_fork()`, `sys_vfork()`, `sys_clone()` 最终调用 `_do_fork()`\n- **内核线程创建**：`kthread_create()` 通过 `kernel_thread()` 触发 fork 逻辑\n- **容器/命名空间初始化**：新 PID/UTS/IPC 命名空间创建时伴随进程 fork\n\n### 资源复制关键点\n- **内存描述符 (mm_struct)**：通过 `dup_mm()` 复制地址空间（COW 页表）\n- **文件描述符表**：`dup_fd()` 复制打开文件表\n- **信号处理**：复制信号掩码和处理函数\n- **POSIX 定时器/异步 I/O**：复制相关上下文（如 `aio`, `posix-timers`）\n\n### 特殊场景处理\n- **写时复制优化**：避免物理内存立即复制，提升 fork 性能\n- **OOM Killer 集成**：在内存不足时参与进程选择\n- **审计与监控**：通过 `audit_alloc()` 和 `proc` 文件系统暴露进程信息\n- **实时性保障**：RT 任务 fork 时保持调度策略和优先级",
      "similarity": 0.6128571629524231,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/fork.c",
          "start_line": 666,
          "end_line": 845,
          "content": [
            "static __latent_entropy int dup_mmap(struct mm_struct *mm,",
            "\t\t\t\t\tstruct mm_struct *oldmm)",
            "{",
            "\tstruct vm_area_struct *mpnt, *tmp;",
            "\tint retval;",
            "\tunsigned long charge = 0;",
            "\tLIST_HEAD(uf);",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "",
            "\tuprobe_start_dup_mmap();",
            "\tif (mmap_write_lock_killable(oldmm)) {",
            "\t\tretval = -EINTR;",
            "\t\tgoto fail_uprobe_end;",
            "\t}",
            "\tflush_cache_dup_mm(oldmm);",
            "\tuprobe_dup_mmap(oldmm, mm);",
            "\t/*",
            "\t * Not linked in yet - no deadlock potential:",
            "\t */",
            "\tmmap_write_lock_nested(mm, SINGLE_DEPTH_NESTING);",
            "",
            "\t/* No ordering required: file already has been exposed. */",
            "\tdup_mm_exe_file(mm, oldmm);",
            "",
            "\tmm->total_vm = oldmm->total_vm;",
            "\tmm->data_vm = oldmm->data_vm;",
            "\tmm->exec_vm = oldmm->exec_vm;",
            "\tmm->stack_vm = oldmm->stack_vm;",
            "",
            "\t/* Use __mt_dup() to efficiently build an identical maple tree. */",
            "\tretval = __mt_dup(&oldmm->mm_mt, &mm->mm_mt, GFP_KERNEL);",
            "\tif (unlikely(retval))",
            "\t\tgoto out;",
            "",
            "\tmt_clear_in_rcu(vmi.mas.tree);",
            "\tfor_each_vma(vmi, mpnt) {",
            "\t\tstruct file *file;",
            "",
            "\t\tvma_start_write(mpnt);",
            "\t\tif (mpnt->vm_flags & VM_DONTCOPY) {",
            "\t\t\tretval = vma_iter_clear_gfp(&vmi, mpnt->vm_start,",
            "\t\t\t\t\t\t    mpnt->vm_end, GFP_KERNEL);",
            "\t\t\tif (retval)",
            "\t\t\t\tgoto loop_out;",
            "",
            "\t\t\tvm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt));",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tcharge = 0;",
            "\t\t/*",
            "\t\t * Don't duplicate many vmas if we've been oom-killed (for",
            "\t\t * example)",
            "\t\t */",
            "\t\tif (fatal_signal_pending(current)) {",
            "\t\t\tretval = -EINTR;",
            "\t\t\tgoto loop_out;",
            "\t\t}",
            "\t\tif (mpnt->vm_flags & VM_ACCOUNT) {",
            "\t\t\tunsigned long len = vma_pages(mpnt);",
            "",
            "\t\t\tif (security_vm_enough_memory_mm(oldmm, len)) /* sic */",
            "\t\t\t\tgoto fail_nomem;",
            "\t\t\tcharge = len;",
            "\t\t}",
            "\t\ttmp = vm_area_dup(mpnt);",
            "\t\tif (!tmp)",
            "\t\t\tgoto fail_nomem;",
            "",
            "\t\t/* track_pfn_copy() will later take care of copying internal state. */",
            "\t\tif (unlikely(tmp->vm_flags & VM_PFNMAP))",
            "\t\t\tuntrack_pfn_clear(tmp);",
            "",
            "\t\tretval = vma_dup_policy(mpnt, tmp);",
            "\t\tif (retval)",
            "\t\t\tgoto fail_nomem_policy;",
            "\t\ttmp->vm_mm = mm;",
            "\t\tretval = dup_userfaultfd(tmp, &uf);",
            "\t\tif (retval)",
            "\t\t\tgoto fail_nomem_anon_vma_fork;",
            "\t\tif (tmp->vm_flags & VM_WIPEONFORK) {",
            "\t\t\t/*",
            "\t\t\t * VM_WIPEONFORK gets a clean slate in the child.",
            "\t\t\t * Don't prepare anon_vma until fault since we don't",
            "\t\t\t * copy page for current vma.",
            "\t\t\t */",
            "\t\t\ttmp->anon_vma = NULL;",
            "\t\t} else if (anon_vma_fork(tmp, mpnt))",
            "\t\t\tgoto fail_nomem_anon_vma_fork;",
            "\t\tvm_flags_clear(tmp, VM_LOCKED_MASK);",
            "\t\tfile = tmp->vm_file;",
            "\t\tif (file) {",
            "\t\t\tstruct address_space *mapping = file->f_mapping;",
            "",
            "\t\t\tget_file(file);",
            "\t\t\ti_mmap_lock_write(mapping);",
            "\t\t\tif (vma_is_shared_maywrite(tmp))",
            "\t\t\t\tmapping_allow_writable(mapping);",
            "\t\t\tflush_dcache_mmap_lock(mapping);",
            "\t\t\t/* insert tmp into the share list, just after mpnt */",
            "\t\t\tvma_interval_tree_insert_after(tmp, mpnt,",
            "\t\t\t\t\t&mapping->i_mmap);",
            "\t\t\tflush_dcache_mmap_unlock(mapping);",
            "\t\t\ti_mmap_unlock_write(mapping);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Copy/update hugetlb private vma information.",
            "\t\t */",
            "\t\tif (is_vm_hugetlb_page(tmp))",
            "\t\t\thugetlb_dup_vma_private(tmp);",
            "",
            "\t\t/*",
            "\t\t * Link the vma into the MT. After using __mt_dup(), memory",
            "\t\t * allocation is not necessary here, so it cannot fail.",
            "\t\t */",
            "\t\tvma_iter_bulk_store(&vmi, tmp);",
            "",
            "\t\tmm->map_count++;",
            "\t\tif (!(tmp->vm_flags & VM_WIPEONFORK))",
            "\t\t\tretval = copy_page_range(tmp, mpnt);",
            "",
            "\t\tif (tmp->vm_ops && tmp->vm_ops->open)",
            "\t\t\ttmp->vm_ops->open(tmp);",
            "",
            "\t\tif (retval) {",
            "\t\t\tmpnt = vma_next(&vmi);",
            "\t\t\tgoto loop_out;",
            "\t\t}",
            "\t}",
            "\t/* a new mm has just been created */",
            "\tretval = arch_dup_mmap(oldmm, mm);",
            "loop_out:",
            "\tvma_iter_free(&vmi);",
            "\tif (!retval) {",
            "\t\tmt_set_in_rcu(vmi.mas.tree);",
            "\t\tksm_fork(mm, oldmm);",
            "\t\tkhugepaged_fork(mm, oldmm);",
            "\t} else {",
            "",
            "\t\t/*",
            "\t\t * The entire maple tree has already been duplicated. If the",
            "\t\t * mmap duplication fails, mark the failure point with",
            "\t\t * XA_ZERO_ENTRY. In exit_mmap(), if this marker is encountered,",
            "\t\t * stop releasing VMAs that have not been duplicated after this",
            "\t\t * point.",
            "\t\t */",
            "\t\tif (mpnt) {",
            "\t\t\tmas_set_range(&vmi.mas, mpnt->vm_start, mpnt->vm_end - 1);",
            "\t\t\tmas_store(&vmi.mas, XA_ZERO_ENTRY);",
            "\t\t\t/* Avoid OOM iterating a broken tree */",
            "\t\t\tset_bit(MMF_OOM_SKIP, &mm->flags);",
            "\t\t}",
            "\t\t/*",
            "\t\t * The mm_struct is going to exit, but the locks will be dropped",
            "\t\t * first.  Set the mm_struct as unstable is advisable as it is",
            "\t\t * not fully initialised.",
            "\t\t */",
            "\t\tset_bit(MMF_UNSTABLE, &mm->flags);",
            "\t}",
            "out:",
            "\tmmap_write_unlock(mm);",
            "\tflush_tlb_mm(oldmm);",
            "\tmmap_write_unlock(oldmm);",
            "\tif (!retval)",
            "\t\tdup_userfaultfd_complete(&uf);",
            "\telse",
            "\t\tdup_userfaultfd_fail(&uf);",
            "fail_uprobe_end:",
            "\tuprobe_end_dup_mmap();",
            "\treturn retval;",
            "",
            "fail_nomem_anon_vma_fork:",
            "\tmpol_put(vma_policy(tmp));",
            "fail_nomem_policy:",
            "\tvm_area_free(tmp);",
            "fail_nomem:",
            "\tretval = -ENOMEM;",
            "\tvm_unacct_memory(charge);",
            "\tgoto loop_out;",
            "}"
          ],
          "function_name": "dup_mmap",
          "description": "实现进程fork时的内存映射复制逻辑，深度遍历原进程的VMA结构创建副本，处理共享文件映射、hugetlb页等特殊内存类型，并管理OOM异常情况下的失败恢复。",
          "similarity": 0.6012395620346069
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/fork.c",
          "start_line": 159,
          "end_line": 303,
          "content": [
            "int lockdep_tasklist_lock_is_held(void)",
            "{",
            "\treturn lockdep_is_held(&tasklist_lock);",
            "}",
            "int nr_processes(void)",
            "{",
            "\tint cpu;",
            "\tint total = 0;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\ttotal += per_cpu(process_counts, cpu);",
            "",
            "\treturn total;",
            "}",
            "void __weak arch_release_task_struct(struct task_struct *tsk)",
            "{",
            "}",
            "static inline void free_task_struct(struct task_struct *tsk)",
            "{",
            "\tkmem_cache_free(task_struct_cachep, tsk);",
            "}",
            "static bool try_release_thread_stack_to_cache(struct vm_struct *vm)",
            "{",
            "\tunsigned int i;",
            "",
            "\tfor (i = 0; i < NR_CACHED_STACKS; i++) {",
            "\t\tif (this_cpu_cmpxchg(cached_stacks[i], NULL, vm) != NULL)",
            "\t\t\tcontinue;",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "static void thread_stack_free_rcu(struct rcu_head *rh)",
            "{",
            "\tstruct vm_stack *vm_stack = container_of(rh, struct vm_stack, rcu);",
            "",
            "\tif (try_release_thread_stack_to_cache(vm_stack->stack_vm_area))",
            "\t\treturn;",
            "",
            "\tvfree(vm_stack);",
            "}",
            "static void thread_stack_delayed_free(struct task_struct *tsk)",
            "{",
            "\tstruct vm_stack *vm_stack = tsk->stack;",
            "",
            "\tvm_stack->stack_vm_area = tsk->stack_vm_area;",
            "\tcall_rcu(&vm_stack->rcu, thread_stack_free_rcu);",
            "}",
            "static int free_vm_stack_cache(unsigned int cpu)",
            "{",
            "\tstruct vm_struct **cached_vm_stacks = per_cpu_ptr(cached_stacks, cpu);",
            "\tint i;",
            "",
            "\tfor (i = 0; i < NR_CACHED_STACKS; i++) {",
            "\t\tstruct vm_struct *vm_stack = cached_vm_stacks[i];",
            "",
            "\t\tif (!vm_stack)",
            "\t\t\tcontinue;",
            "",
            "\t\tvfree(vm_stack->addr);",
            "\t\tcached_vm_stacks[i] = NULL;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int memcg_charge_kernel_stack(struct vm_struct *vm)",
            "{",
            "\tint i;",
            "\tint ret;",
            "\tint nr_charged = 0;",
            "",
            "\tBUG_ON(vm->nr_pages != THREAD_SIZE / PAGE_SIZE);",
            "",
            "\tfor (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++) {",
            "\t\tret = memcg_kmem_charge_page(vm->pages[i], GFP_KERNEL, 0);",
            "\t\tif (ret)",
            "\t\t\tgoto err;",
            "\t\tnr_charged++;",
            "\t}",
            "\treturn 0;",
            "err:",
            "\tfor (i = 0; i < nr_charged; i++)",
            "\t\tmemcg_kmem_uncharge_page(vm->pages[i], 0);",
            "\treturn ret;",
            "}",
            "static int alloc_thread_stack_node(struct task_struct *tsk, int node)",
            "{",
            "\tstruct vm_struct *vm;",
            "\tvoid *stack;",
            "\tint i;",
            "",
            "\tfor (i = 0; i < NR_CACHED_STACKS; i++) {",
            "\t\tstruct vm_struct *s;",
            "",
            "\t\ts = this_cpu_xchg(cached_stacks[i], NULL);",
            "",
            "\t\tif (!s)",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Reset stack metadata. */",
            "\t\tkasan_unpoison_range(s->addr, THREAD_SIZE);",
            "",
            "\t\tstack = kasan_reset_tag(s->addr);",
            "",
            "\t\t/* Clear stale pointers from reused stack. */",
            "\t\tmemset(stack, 0, THREAD_SIZE);",
            "",
            "\t\tif (memcg_charge_kernel_stack(s)) {",
            "\t\t\tvfree(s->addr);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "",
            "\t\ttsk->stack_vm_area = s;",
            "\t\ttsk->stack = stack;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/*",
            "\t * Allocated stacks are cached and later reused by new threads,",
            "\t * so memcg accounting is performed manually on assigning/releasing",
            "\t * stacks to tasks. Drop __GFP_ACCOUNT.",
            "\t */",
            "\tstack = __vmalloc_node_range(THREAD_SIZE, THREAD_ALIGN,",
            "\t\t\t\t     VMALLOC_START, VMALLOC_END,",
            "\t\t\t\t     THREADINFO_GFP & ~__GFP_ACCOUNT,",
            "\t\t\t\t     PAGE_KERNEL,",
            "\t\t\t\t     0, node, __builtin_return_address(0));",
            "\tif (!stack)",
            "\t\treturn -ENOMEM;",
            "",
            "\tvm = find_vm_area(stack);",
            "\tif (memcg_charge_kernel_stack(vm)) {",
            "\t\tvfree(stack);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\t/*",
            "\t * We can't call find_vm_area() in interrupt context, and",
            "\t * free_thread_stack() can be called in interrupt context,",
            "\t * so cache the vm_struct.",
            "\t */",
            "\ttsk->stack_vm_area = vm;",
            "\tstack = kasan_reset_tag(stack);",
            "\ttsk->stack = stack;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "lockdep_tasklist_lock_is_held, nr_processes, arch_release_task_struct, free_task_struct, try_release_thread_stack_to_cache, thread_stack_free_rcu, thread_stack_delayed_free, free_vm_stack_cache, memcg_charge_kernel_stack, alloc_thread_stack_node",
          "description": "实现任务列表锁状态检测、进程总数统计及线程栈分配释放逻辑，通过缓存机制优化线程栈复用并利用RCU实现延迟释放，包含栈内存管理和内存会计功能。",
          "similarity": 0.5823769569396973
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/fork.c",
          "start_line": 2176,
          "end_line": 2282,
          "content": [
            "static void rv_task_fork(struct task_struct *p)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < RV_PER_TASK_MONITORS; i++)",
            "\t\tp->rv[i].da_mon.monitoring = false;",
            "}",
            "static inline void init_idle_pids(struct task_struct *idle)",
            "{",
            "\tenum pid_type type;",
            "",
            "\tfor (type = PIDTYPE_PID; type < PIDTYPE_MAX; ++type) {",
            "\t\tINIT_HLIST_NODE(&idle->pid_links[type]); /* not really needed */",
            "\t\tinit_task_pid(idle, type, &init_struct_pid);",
            "\t}",
            "}",
            "static int idle_dummy(void *dummy)",
            "{",
            "\t/* This function is never called */",
            "\treturn 0;",
            "}",
            "pid_t kernel_clone(struct kernel_clone_args *args)",
            "{",
            "\tu64 clone_flags = args->flags;",
            "\tstruct completion vfork;",
            "\tstruct pid *pid;",
            "\tstruct task_struct *p;",
            "\tint trace = 0;",
            "\tpid_t nr;",
            "",
            "\t/*",
            "\t * For legacy clone() calls, CLONE_PIDFD uses the parent_tid argument",
            "\t * to return the pidfd. Hence, CLONE_PIDFD and CLONE_PARENT_SETTID are",
            "\t * mutually exclusive. With clone3() CLONE_PIDFD has grown a separate",
            "\t * field in struct clone_args and it still doesn't make sense to have",
            "\t * them both point at the same memory location. Performing this check",
            "\t * here has the advantage that we don't need to have a separate helper",
            "\t * to check for legacy clone().",
            "\t */",
            "\tif ((args->flags & CLONE_PIDFD) &&",
            "\t    (args->flags & CLONE_PARENT_SETTID) &&",
            "\t    (args->pidfd == args->parent_tid))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Determine whether and which event to report to ptracer.  When",
            "\t * called from kernel_thread or CLONE_UNTRACED is explicitly",
            "\t * requested, no event is reported; otherwise, report if the event",
            "\t * for the type of forking is enabled.",
            "\t */",
            "\tif (!(clone_flags & CLONE_UNTRACED)) {",
            "\t\tif (clone_flags & CLONE_VFORK)",
            "\t\t\ttrace = PTRACE_EVENT_VFORK;",
            "\t\telse if (args->exit_signal != SIGCHLD)",
            "\t\t\ttrace = PTRACE_EVENT_CLONE;",
            "\t\telse",
            "\t\t\ttrace = PTRACE_EVENT_FORK;",
            "",
            "\t\tif (likely(!ptrace_event_enabled(current, trace)))",
            "\t\t\ttrace = 0;",
            "\t}",
            "",
            "\tp = copy_process(NULL, trace, NUMA_NO_NODE, args);",
            "\tadd_latent_entropy();",
            "",
            "\tif (IS_ERR(p))",
            "\t\treturn PTR_ERR(p);",
            "",
            "\t/*",
            "\t * Do this prior waking up the new thread - the thread pointer",
            "\t * might get invalid after that point, if the thread exits quickly.",
            "\t */",
            "\ttrace_sched_process_fork(current, p);",
            "",
            "\tpid = get_task_pid(p, PIDTYPE_PID);",
            "\tnr = pid_vnr(pid);",
            "",
            "\tif (clone_flags & CLONE_PARENT_SETTID)",
            "\t\tput_user(nr, args->parent_tid);",
            "",
            "\tif (clone_flags & CLONE_VFORK) {",
            "\t\tp->vfork_done = &vfork;",
            "\t\tinit_completion(&vfork);",
            "\t\tget_task_struct(p);",
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_LRU_GEN_WALKS_MMU) && !(clone_flags & CLONE_VM)) {",
            "\t\t/* lock the task to synchronize with memcg migration */",
            "\t\ttask_lock(p);",
            "\t\tlru_gen_add_mm(p->mm);",
            "\t\ttask_unlock(p);",
            "\t}",
            "",
            "\twake_up_new_task(p);",
            "",
            "\t/* forking complete and child started to run, tell ptracer */",
            "\tif (unlikely(trace))",
            "\t\tptrace_event_pid(trace, pid);",
            "",
            "\tif (clone_flags & CLONE_VFORK) {",
            "\t\tif (!wait_for_vfork_done(p, &vfork))",
            "\t\t\tptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);",
            "\t}",
            "",
            "\tput_pid(pid);",
            "\treturn nr;",
            "}"
          ],
          "function_name": "rv_task_fork, init_idle_pids, idle_dummy, kernel_clone",
          "description": "实现kernel_clone核心逻辑，创建新进程并处理克隆标志，管理子进程启动、vfork等待及进程树遍历，包含空闲任务PID初始化与RV监控器重置",
          "similarity": 0.5716016292572021
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/fork.c",
          "start_line": 2924,
          "end_line": 3037,
          "content": [
            "pid_t kernel_thread(int (*fn)(void *), void *arg, const char *name,",
            "\t\t    unsigned long flags)",
            "{",
            "\tstruct kernel_clone_args args = {",
            "\t\t.flags\t\t= ((lower_32_bits(flags) | CLONE_VM |",
            "\t\t\t\t    CLONE_UNTRACED) & ~CSIGNAL),",
            "\t\t.exit_signal\t= (lower_32_bits(flags) & CSIGNAL),",
            "\t\t.fn\t\t= fn,",
            "\t\t.fn_arg\t\t= arg,",
            "\t\t.name\t\t= name,",
            "\t\t.kthread\t= 1,",
            "\t};",
            "",
            "\treturn kernel_clone(&args);",
            "}",
            "pid_t user_mode_thread(int (*fn)(void *), void *arg, unsigned long flags)",
            "{",
            "\tstruct kernel_clone_args args = {",
            "\t\t.flags\t\t= ((lower_32_bits(flags) | CLONE_VM |",
            "\t\t\t\t    CLONE_UNTRACED) & ~CSIGNAL),",
            "\t\t.exit_signal\t= (lower_32_bits(flags) & CSIGNAL),",
            "\t\t.fn\t\t= fn,",
            "\t\t.fn_arg\t\t= arg,",
            "\t};",
            "",
            "\treturn kernel_clone(&args);",
            "}",
            "noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,",
            "\t\t\t\t\t      struct clone_args __user *uargs,",
            "\t\t\t\t\t      size_t usize)",
            "{",
            "\tint err;",
            "\tstruct clone_args args;",
            "\tpid_t *kset_tid = kargs->set_tid;",
            "",
            "\tBUILD_BUG_ON(offsetofend(struct clone_args, tls) !=",
            "\t\t     CLONE_ARGS_SIZE_VER0);",
            "\tBUILD_BUG_ON(offsetofend(struct clone_args, set_tid_size) !=",
            "\t\t     CLONE_ARGS_SIZE_VER1);",
            "\tBUILD_BUG_ON(offsetofend(struct clone_args, cgroup) !=",
            "\t\t     CLONE_ARGS_SIZE_VER2);",
            "\tBUILD_BUG_ON(sizeof(struct clone_args) != CLONE_ARGS_SIZE_VER2);",
            "",
            "\tif (unlikely(usize > PAGE_SIZE))",
            "\t\treturn -E2BIG;",
            "\tif (unlikely(usize < CLONE_ARGS_SIZE_VER0))",
            "\t\treturn -EINVAL;",
            "",
            "\terr = copy_struct_from_user(&args, sizeof(args), uargs, usize);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (unlikely(args.set_tid_size > MAX_PID_NS_LEVEL))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (unlikely(!args.set_tid && args.set_tid_size > 0))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (unlikely(args.set_tid && args.set_tid_size == 0))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Verify that higher 32bits of exit_signal are unset and that",
            "\t * it is a valid signal",
            "\t */",
            "\tif (unlikely((args.exit_signal & ~((u64)CSIGNAL)) ||",
            "\t\t     !valid_signal(args.exit_signal)))",
            "\t\treturn -EINVAL;",
            "",
            "\tif ((args.flags & CLONE_INTO_CGROUP) &&",
            "\t    (args.cgroup > INT_MAX || usize < CLONE_ARGS_SIZE_VER2))",
            "\t\treturn -EINVAL;",
            "",
            "\t*kargs = (struct kernel_clone_args){",
            "\t\t.flags\t\t= args.flags,",
            "\t\t.pidfd\t\t= u64_to_user_ptr(args.pidfd),",
            "\t\t.child_tid\t= u64_to_user_ptr(args.child_tid),",
            "\t\t.parent_tid\t= u64_to_user_ptr(args.parent_tid),",
            "\t\t.exit_signal\t= args.exit_signal,",
            "\t\t.stack\t\t= args.stack,",
            "\t\t.stack_size\t= args.stack_size,",
            "\t\t.tls\t\t= args.tls,",
            "\t\t.set_tid_size\t= args.set_tid_size,",
            "\t\t.cgroup\t\t= args.cgroup,",
            "\t};",
            "",
            "\tif (args.set_tid &&",
            "\t\tcopy_from_user(kset_tid, u64_to_user_ptr(args.set_tid),",
            "\t\t\t(kargs->set_tid_size * sizeof(pid_t))))",
            "\t\treturn -EFAULT;",
            "",
            "\tkargs->set_tid = kset_tid;",
            "",
            "\treturn 0;",
            "}",
            "static inline bool clone3_stack_valid(struct kernel_clone_args *kargs)",
            "{",
            "\tif (kargs->stack == 0) {",
            "\t\tif (kargs->stack_size > 0)",
            "\t\t\treturn false;",
            "\t} else {",
            "\t\tif (kargs->stack_size == 0)",
            "\t\t\treturn false;",
            "",
            "\t\tif (!access_ok((void __user *)kargs->stack, kargs->stack_size))",
            "\t\t\treturn false;",
            "",
            "#if !defined(CONFIG_STACK_GROWSUP)",
            "\t\tkargs->stack += kargs->stack_size;",
            "#endif",
            "\t}",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "kernel_thread, user_mode_thread, copy_clone_args_from_user, clone3_stack_valid",
          "description": "提供内核线程与用户线程创建接口，解析并验证clone3参数，转换用户空间clone_args到内核结构体，校验栈地址有效性",
          "similarity": 0.561225175857544
        },
        {
          "chunk_id": 16,
          "file_path": "kernel/fork.c",
          "start_line": 3480,
          "end_line": 3518,
          "content": [
            "int unshare_files(void)",
            "{",
            "\tstruct task_struct *task = current;",
            "\tstruct files_struct *old, *copy = NULL;",
            "\tint error;",
            "",
            "\terror = unshare_fd(CLONE_FILES, &copy);",
            "\tif (error || !copy)",
            "\t\treturn error;",
            "",
            "\told = task->files;",
            "\ttask_lock(task);",
            "\ttask->files = copy;",
            "\ttask_unlock(task);",
            "\tput_files_struct(old);",
            "\treturn 0;",
            "}",
            "int sysctl_max_threads(struct ctl_table *table, int write,",
            "\t\t       void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tstruct ctl_table t;",
            "\tint ret;",
            "\tint threads = max_threads;",
            "\tint min = 1;",
            "\tint max = MAX_THREADS;",
            "",
            "\tt = *table;",
            "\tt.data = &threads;",
            "\tt.extra1 = &min;",
            "\tt.extra2 = &max;",
            "",
            "\tret = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);",
            "\tif (ret || !write)",
            "\t\treturn ret;",
            "",
            "\tmax_threads = threads;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "unshare_files, sysctl_max_threads",
          "description": "unshare_files函数复制当前进程的文件表结构并将其绑定到当前任务，sysctl_max_threads函数通过proc_dointvec_minmax接口限制系统最大线程数，支持读取和写入操作，其中写入时会更新全局max_threads变量。",
          "similarity": 0.5604303479194641
        }
      ]
    },
    {
      "source_file": "kernel/sched/membarrier.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:12:44\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\membarrier.c`\n\n---\n\n# `sched/membarrier.c` 技术文档\n\n## 1. 文件概述\n\n`sched/membarrier.c` 实现了 Linux 内核中的 `membarrier` 系统调用，该调用为用户空间程序提供了一种高效的全局内存屏障机制。与传统的在每个线程中显式插入内存屏障相比，`membarrier` 允许一个线程通过一次系统调用，强制所有运行在系统上的线程（或特定进程组内的线程）执行内存屏障操作，从而简化用户空间并发同步逻辑并提升性能。\n\n该文件的核心目标是在多核系统中确保内存操作的全局可见性顺序，尤其适用于需要跨线程强内存顺序保证的用户空间同步原语（如 RCU、无锁数据结构等）。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`ipi_mb(void *info)`**  \n  IPI（处理器间中断）处理函数，执行 `smp_mb()` 内存屏障，用于基础的全局内存屏障命令。\n\n- **`ipi_sync_core(void *info)`**  \n  用于 `MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE` 命令的 IPI 处理函数，在执行内存屏障后调用 `sync_core_before_usermode()`，确保 CPU 核心状态同步（如指令缓存一致性）。\n\n- **`ipi_rseq(void *info)`**  \n  用于 `MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ` 命令的 IPI 处理函数，在内存屏障后调用 `rseq_preempt(current)`，以支持 restartable sequences（rseq）机制的正确性。\n\n- **`ipi_sync_rq_state(void *info)`**  \n  用于同步 per-CPU runqueue 的 `membarrier_state` 字段，使其与指定 `mm_struct` 的状态一致，确保后续 `membarrier` 调用能正确识别注册状态。\n\n- **`membarrier_exec_mmap(struct mm_struct *mm)`**  \n  在进程执行 `exec` 系统调用时被调用，重置该内存描述符（`mm_struct`）的 `membarrier_state` 为 0，并同步 per-CPU runqueue 状态，防止 exec 后残留旧的注册状态。\n\n### 数据结构与宏\n\n- **`MEMBARRIER_CMD_BITMASK`**  \n  定义所有支持的 `membarrier` 命令的位掩码（不含 `QUERY`），用于命令合法性校验。\n\n- **`membarrier_ipi_mutex`**  \n  互斥锁，用于序列化 IPI 发送过程，防止多个 `membarrier` 调用并发执行导致 IPI 风暴或状态不一致。\n\n- **`SERIALIZE_IPI()`**  \n  宏封装，使用 `membarrier_ipi_mutex` 实现 IPI 发送的串行化。\n\n## 3. 关键实现\n\n### 内存屏障语义保证\n\n文件顶部的注释详细描述了五种关键内存顺序场景（A–E），说明为何在 `membarrier()` 调用前后必须插入 `smp_mb()`：\n\n- **场景 A**：确保调用者 CPU 在 `membarrier()` 之前的写操作，在其他 CPU 收到 IPI 并执行屏障后对其可见。\n- **场景 B**：确保其他 CPU 在 IPI 屏障前的写操作，在调用者 CPU 执行 `membarrier()` 后对其可见。\n- **场景 C–E**：处理线程切换、`exit_mm`、kthread 使用/释放 mm 等边界情况，确保 `membarrier` 能正确识别用户态上下文并施加屏障。\n\n这些场景共同要求 `membarrier()` 实现必须在发送 IPI **前**和**后**各执行一次 `smp_mb()`，以建立完整的全局内存顺序。\n\n### IPI 分发机制\n\n- 根据不同的 `membarrier` 命令类型（如全局、私有、带 rseq 或 sync_core），选择对应的 IPI 处理函数。\n- 使用 `mutex` 保护 IPI 发送过程，避免并发调用导致性能下降或状态竞争。\n- 对于私有命令（如 `PRIVATE_EXPEDITED`），仅向共享同一 `mm_struct` 的 CPU 发送 IPI。\n\n### 状态管理\n\n- 每个 `mm_struct` 包含一个 `membarrier_state` 原子变量，记录该地址空间已注册的 `membarrier` 命令类型。\n- 每个 per-CPU runqueue 也缓存一份 `membarrier_state`，通过 `ipi_sync_rq_state` 保持与 `mm_struct` 同步，加速后续命令的判断。\n- `exec` 时调用 `membarrier_exec_mmap` 重置状态，防止子进程继承父进程的注册状态。\n\n### 条件编译支持\n\n- `CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE`：启用 `SYNC_CORE` 相关命令。\n- `CONFIG_RSEQ`：启用 `RSEQ` 相关命令及 `rseq_preempt` 调用。\n\n## 4. 依赖关系\n\n- **调度子系统（sched）**：依赖 runqueue（`rq`）结构和 CPU 上下文切换逻辑，用于判断当前是否处于用户态及 mm 匹配。\n- **内存管理（mm）**：依赖 `mm_struct` 及其生命周期管理（如 `exec_mmap`、`exit_mm`）。\n- **RSEQ 子系统**：当启用 `CONFIG_RSEQ` 时，调用 `rseq_preempt()` 以维护 restartable sequences 的一致性。\n- **SMP 原语**：依赖 `smp_mb()`、`smp_call_function_many()` 等 SMP 内存屏障和 IPI 接口。\n- **架构支持**：部分命令（如 `SYNC_CORE`）依赖特定架构实现 `sync_core_before_usermode()`。\n\n## 5. 使用场景\n\n- **用户空间无锁编程**：应用程序使用 `membarrier(SYS_MEMBARRIER_CMD_GLOBAL_EXPEDITED)` 替代在每个读线程中插入 `smp_load_acquire()`，简化代码并提升性能。\n- **RSEQ（Restartable Sequences）**：配合 `membarrier(SYS_MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ)` 确保在抢占或迁移后 rseq 区域的原子性。\n- **实时或低延迟系统**：通过私有命令（`PRIVATE_EXPEDITED`）仅对特定进程组施加屏障，减少系统范围开销。\n- **动态代码生成/热更新**：使用 `SYNC_CORE` 命令确保指令缓存一致性，适用于 JIT 编译器等场景。\n- **进程生命周期管理**：在 `exec` 时自动清理 `membarrier` 注册状态，保证新程序映像的干净执行环境。",
      "similarity": 0.6096723079681396,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 168,
          "end_line": 306,
          "content": [
            "static void ipi_mb(void *info)",
            "{",
            "\tsmp_mb();\t/* IPIs should be serializing but paranoid. */",
            "}",
            "static void ipi_sync_core(void *info)",
            "{",
            "\t/*",
            "\t * The smp_mb() in membarrier after all the IPIs is supposed to",
            "\t * ensure that memory on remote CPUs that occur before the IPI",
            "\t * become visible to membarrier()'s caller -- see scenario B in",
            "\t * the big comment at the top of this file.",
            "\t *",
            "\t * A sync_core() would provide this guarantee, but",
            "\t * sync_core_before_usermode() might end up being deferred until",
            "\t * after membarrier()'s smp_mb().",
            "\t */",
            "\tsmp_mb();\t/* IPIs should be serializing but paranoid. */",
            "",
            "\tsync_core_before_usermode();",
            "}",
            "static void ipi_rseq(void *info)",
            "{",
            "\t/*",
            "\t * Ensure that all stores done by the calling thread are visible",
            "\t * to the current task before the current task resumes.  We could",
            "\t * probably optimize this away on most architectures, but by the",
            "\t * time we've already sent an IPI, the cost of the extra smp_mb()",
            "\t * is negligible.",
            "\t */",
            "\tsmp_mb();",
            "\trseq_preempt(current);",
            "}",
            "static void ipi_sync_rq_state(void *info)",
            "{",
            "\tstruct mm_struct *mm = (struct mm_struct *) info;",
            "",
            "\tif (current->mm != mm)",
            "\t\treturn;",
            "\tthis_cpu_write(runqueues.membarrier_state,",
            "\t\t       atomic_read(&mm->membarrier_state));",
            "\t/*",
            "\t * Issue a memory barrier after setting",
            "\t * MEMBARRIER_STATE_GLOBAL_EXPEDITED in the current runqueue to",
            "\t * guarantee that no memory access following registration is reordered",
            "\t * before registration.",
            "\t */",
            "\tsmp_mb();",
            "}",
            "void membarrier_exec_mmap(struct mm_struct *mm)",
            "{",
            "\t/*",
            "\t * Issue a memory barrier before clearing membarrier_state to",
            "\t * guarantee that no memory access prior to exec is reordered after",
            "\t * clearing this state.",
            "\t */",
            "\tsmp_mb();",
            "\tatomic_set(&mm->membarrier_state, 0);",
            "\t/*",
            "\t * Keep the runqueue membarrier_state in sync with this mm",
            "\t * membarrier_state.",
            "\t */",
            "\tthis_cpu_write(runqueues.membarrier_state, 0);",
            "}",
            "void membarrier_update_current_mm(struct mm_struct *next_mm)",
            "{",
            "\tstruct rq *rq = this_rq();",
            "\tint membarrier_state = 0;",
            "",
            "\tif (next_mm)",
            "\t\tmembarrier_state = atomic_read(&next_mm->membarrier_state);",
            "\tif (READ_ONCE(rq->membarrier_state) == membarrier_state)",
            "\t\treturn;",
            "\tWRITE_ONCE(rq->membarrier_state, membarrier_state);",
            "}",
            "static int membarrier_global_expedited(void)",
            "{",
            "\tint cpu;",
            "\tcpumask_var_t tmpmask;",
            "",
            "\tif (num_online_cpus() == 1)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Matches memory barriers after rq->curr modification in",
            "\t * scheduler.",
            "\t */",
            "\tsmp_mb();\t/* system call entry is not a mb. */",
            "",
            "\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "\trcu_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\t/*",
            "\t\t * Skipping the current CPU is OK even through we can be",
            "\t\t * migrated at any point. The current CPU, at the point",
            "\t\t * where we read raw_smp_processor_id(), is ensured to",
            "\t\t * be in program order with respect to the caller",
            "\t\t * thread. Therefore, we can skip this CPU from the",
            "\t\t * iteration.",
            "\t\t */",
            "\t\tif (cpu == raw_smp_processor_id())",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!(READ_ONCE(cpu_rq(cpu)->membarrier_state) &",
            "\t\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * Skip the CPU if it runs a kernel thread which is not using",
            "\t\t * a task mm.",
            "\t\t */",
            "\t\tp = rcu_dereference(cpu_rq(cpu)->curr);",
            "\t\tif (!p->mm)",
            "\t\t\tcontinue;",
            "",
            "\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tpreempt_disable();",
            "\tsmp_call_function_many(tmpmask, ipi_mb, NULL, 1);",
            "\tpreempt_enable();",
            "",
            "\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\t/*",
            "\t * Memory barrier on the caller thread _after_ we finished",
            "\t * waiting for the last IPI. Matches memory barriers before",
            "\t * rq->curr modification in scheduler.",
            "\t */",
            "\tsmp_mb();\t/* exit from system call is not a mb */",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ipi_mb, ipi_sync_core, ipi_rseq, ipi_sync_rq_state, membarrier_exec_mmap, membarrier_update_current_mm, membarrier_global_expedited",
          "description": "实现多种IPI处理函数及全局快速内存屏障逻辑，通过跨CPU调用来确保内存访问顺序一致性。",
          "similarity": 0.6352795362472534
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 436,
          "end_line": 551,
          "content": [
            "static int sync_runqueues_membarrier_state(struct mm_struct *mm)",
            "{",
            "\tint membarrier_state = atomic_read(&mm->membarrier_state);",
            "\tcpumask_var_t tmpmask;",
            "\tint cpu;",
            "",
            "\tif (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1) {",
            "\t\tthis_cpu_write(runqueues.membarrier_state, membarrier_state);",
            "",
            "\t\t/*",
            "\t\t * For single mm user, we can simply issue a memory barrier",
            "\t\t * after setting MEMBARRIER_STATE_GLOBAL_EXPEDITED in the",
            "\t\t * mm and in the current runqueue to guarantee that no memory",
            "\t\t * access following registration is reordered before",
            "\t\t * registration.",
            "\t\t */",
            "\t\tsmp_mb();",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * For mm with multiple users, we need to ensure all future",
            "\t * scheduler executions will observe @mm's new membarrier",
            "\t * state.",
            "\t */",
            "\tsynchronize_rcu();",
            "",
            "\t/*",
            "\t * For each cpu runqueue, if the task's mm match @mm, ensure that all",
            "\t * @mm's membarrier state set bits are also set in the runqueue's",
            "\t * membarrier state. This ensures that a runqueue scheduling",
            "\t * between threads which are users of @mm has its membarrier state",
            "\t * updated.",
            "\t */",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "\trcu_read_lock();",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct rq *rq = cpu_rq(cpu);",
            "\t\tstruct task_struct *p;",
            "",
            "\t\tp = rcu_dereference(rq->curr);",
            "\t\tif (p && p->mm == mm)",
            "\t\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\ton_each_cpu_mask(tmpmask, ipi_sync_rq_state, mm, true);",
            "",
            "\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\treturn 0;",
            "}",
            "static int membarrier_register_global_expedited(void)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint ret;",
            "",
            "\tif (atomic_read(&mm->membarrier_state) &",
            "\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)",
            "\t\treturn 0;",
            "\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);",
            "\tret = sync_runqueues_membarrier_state(mm);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,",
            "\t\t  &mm->membarrier_state);",
            "",
            "\treturn 0;",
            "}",
            "static int membarrier_register_private_expedited(int flags)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint ready_state = MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY,",
            "\t    set_state = MEMBARRIER_STATE_PRIVATE_EXPEDITED,",
            "\t    ret;",
            "",
            "\tif (flags == MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))",
            "\t\t\treturn -EINVAL;",
            "\t\tready_state =",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY;",
            "\t} else if (flags == MEMBARRIER_FLAG_RSEQ) {",
            "\t\tif (!IS_ENABLED(CONFIG_RSEQ))",
            "\t\t\treturn -EINVAL;",
            "\t\tready_state =",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY;",
            "\t} else {",
            "\t\tWARN_ON_ONCE(flags);",
            "\t}",
            "",
            "\t/*",
            "\t * We need to consider threads belonging to different thread",
            "\t * groups, which use the same mm. (CLONE_VM but not",
            "\t * CLONE_THREAD).",
            "\t */",
            "\tif ((atomic_read(&mm->membarrier_state) & ready_state) == ready_state)",
            "\t\treturn 0;",
            "\tif (flags & MEMBARRIER_FLAG_SYNC_CORE)",
            "\t\tset_state |= MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE;",
            "\tif (flags & MEMBARRIER_FLAG_RSEQ)",
            "\t\tset_state |= MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ;",
            "\tatomic_or(set_state, &mm->membarrier_state);",
            "\tret = sync_runqueues_membarrier_state(mm);",
            "\tif (ret)",
            "\t\treturn ret;",
            "\tatomic_or(ready_state, &mm->membarrier_state);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sync_runqueues_membarrier_state, membarrier_register_global_expedited, membarrier_register_private_expedited",
          "description": "同步运行队列内存屏障状态，通过RCU和IPI确保多用户场景下状态传播的正确性。",
          "similarity": 0.615561842918396
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 314,
          "end_line": 434,
          "content": [
            "static int membarrier_private_expedited(int flags, int cpu_id)",
            "{",
            "\tcpumask_var_t tmpmask;",
            "\tstruct mm_struct *mm = current->mm;",
            "\tsmp_call_func_t ipi_func = ipi_mb;",
            "",
            "\tif (flags == MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY))",
            "\t\t\treturn -EPERM;",
            "\t\tipi_func = ipi_sync_core;",
            "\t\tprepare_sync_core_cmd(mm);",
            "\t} else if (flags == MEMBARRIER_FLAG_RSEQ) {",
            "\t\tif (!IS_ENABLED(CONFIG_RSEQ))",
            "\t\t\treturn -EINVAL;",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY))",
            "\t\t\treturn -EPERM;",
            "\t\tipi_func = ipi_rseq;",
            "\t} else {",
            "\t\tWARN_ON_ONCE(flags);",
            "\t\tif (!(atomic_read(&mm->membarrier_state) &",
            "\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY))",
            "\t\t\treturn -EPERM;",
            "\t}",
            "",
            "\tif (flags != MEMBARRIER_FLAG_SYNC_CORE &&",
            "\t    (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1))",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Matches memory barriers after rq->curr modification in",
            "\t * scheduler.",
            "\t *",
            "\t * On RISC-V, this barrier pairing is also needed for the",
            "\t * SYNC_CORE command when switching between processes, cf.",
            "\t * the inline comments in membarrier_arch_switch_mm().",
            "\t */",
            "\tsmp_mb();\t/* system call entry is not a mb. */",
            "",
            "\tif (cpu_id < 0 && !zalloc_cpumask_var(&tmpmask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tSERIALIZE_IPI();",
            "\tcpus_read_lock();",
            "",
            "\tif (cpu_id >= 0) {",
            "\t\tstruct task_struct *p;",
            "",
            "\t\tif (cpu_id >= nr_cpu_ids || !cpu_online(cpu_id))",
            "\t\t\tgoto out;",
            "\t\trcu_read_lock();",
            "\t\tp = rcu_dereference(cpu_rq(cpu_id)->curr);",
            "\t\tif (!p || p->mm != mm) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tint cpu;",
            "",
            "\t\trcu_read_lock();",
            "\t\tfor_each_online_cpu(cpu) {",
            "\t\t\tstruct task_struct *p;",
            "",
            "\t\t\tp = rcu_dereference(cpu_rq(cpu)->curr);",
            "\t\t\tif (p && p->mm == mm)",
            "\t\t\t\t__cpumask_set_cpu(cpu, tmpmask);",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t}",
            "",
            "\tif (cpu_id >= 0) {",
            "\t\t/*",
            "\t\t * smp_call_function_single() will call ipi_func() if cpu_id",
            "\t\t * is the calling CPU.",
            "\t\t */",
            "\t\tsmp_call_function_single(cpu_id, ipi_func, NULL, 1);",
            "\t} else {",
            "\t\t/*",
            "\t\t * For regular membarrier, we can save a few cycles by",
            "\t\t * skipping the current cpu -- we're about to do smp_mb()",
            "\t\t * below, and if we migrate to a different cpu, this cpu",
            "\t\t * and the new cpu will execute a full barrier in the",
            "\t\t * scheduler.",
            "\t\t *",
            "\t\t * For SYNC_CORE, we do need a barrier on the current cpu --",
            "\t\t * otherwise, if we are migrated and replaced by a different",
            "\t\t * task in the same mm just before, during, or after",
            "\t\t * membarrier, we will end up with some thread in the mm",
            "\t\t * running without a core sync.",
            "\t\t *",
            "\t\t * For RSEQ, don't rseq_preempt() the caller.  User code",
            "\t\t * is not supposed to issue syscalls at all from inside an",
            "\t\t * rseq critical section.",
            "\t\t */",
            "\t\tif (flags != MEMBARRIER_FLAG_SYNC_CORE) {",
            "\t\t\tpreempt_disable();",
            "\t\t\tsmp_call_function_many(tmpmask, ipi_func, NULL, true);",
            "\t\t\tpreempt_enable();",
            "\t\t} else {",
            "\t\t\ton_each_cpu_mask(tmpmask, ipi_func, NULL, true);",
            "\t\t}",
            "\t}",
            "",
            "out:",
            "\tif (cpu_id < 0)",
            "\t\tfree_cpumask_var(tmpmask);",
            "\tcpus_read_unlock();",
            "",
            "\t/*",
            "\t * Memory barrier on the caller thread _after_ we finished",
            "\t * waiting for the last IPI. Matches memory barriers before",
            "\t * rq->curr modification in scheduler.",
            "\t */",
            "\tsmp_mb();\t/* exit from system call is not a mb */",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "membarrier_private_expedited",
          "description": "处理私有快速内存屏障，根据配置标志选择不同同步方式并验证状态有效性。",
          "similarity": 0.5989962220191956
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 555,
          "end_line": 587,
          "content": [
            "static int membarrier_get_registrations(void)",
            "{",
            "\tstruct task_struct *p = current;",
            "\tstruct mm_struct *mm = p->mm;",
            "\tint registrations_mask = 0, membarrier_state, i;",
            "\tstatic const int states[] = {",
            "\t\tMEMBARRIER_STATE_GLOBAL_EXPEDITED |",
            "\t\t\tMEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY,",
            "\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ |",
            "\t\t\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY",
            "\t};",
            "\tstatic const int registration_cmds[] = {",
            "\t\tMEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE,",
            "\t\tMEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ",
            "\t};",
            "\tBUILD_BUG_ON(ARRAY_SIZE(states) != ARRAY_SIZE(registration_cmds));",
            "",
            "\tmembarrier_state = atomic_read(&mm->membarrier_state);",
            "\tfor (i = 0; i < ARRAY_SIZE(states); ++i) {",
            "\t\tif (membarrier_state & states[i]) {",
            "\t\t\tregistrations_mask |= registration_cmds[i];",
            "\t\t\tmembarrier_state &= ~states[i];",
            "\t\t}",
            "\t}",
            "\tWARN_ON_ONCE(membarrier_state != 0);",
            "\treturn registrations_mask;",
            "}"
          ],
          "function_name": "membarrier_get_registrations",
          "description": "解析当前进程的内存屏障注册状态，将有效状态转换为对应的注册命令掩码。",
          "similarity": 0.5664368867874146
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/membarrier.c",
          "start_line": 1,
          "end_line": 167,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Copyright (C) 2010-2017 Mathieu Desnoyers <mathieu.desnoyers@efficios.com>",
            " *",
            " * membarrier system call",
            " */",
            "",
            "/*",
            " * For documentation purposes, here are some membarrier ordering",
            " * scenarios to keep in mind:",
            " *",
            " * A) Userspace thread execution after IPI vs membarrier's memory",
            " *    barrier before sending the IPI",
            " *",
            " * Userspace variables:",
            " *",
            " * int x = 0, y = 0;",
            " *",
            " * The memory barrier at the start of membarrier() on CPU0 is necessary in",
            " * order to enforce the guarantee that any writes occurring on CPU0 before",
            " * the membarrier() is executed will be visible to any code executing on",
            " * CPU1 after the IPI-induced memory barrier:",
            " *",
            " *         CPU0                              CPU1",
            " *",
            " *         x = 1",
            " *         membarrier():",
            " *           a: smp_mb()",
            " *           b: send IPI                       IPI-induced mb",
            " *           c: smp_mb()",
            " *         r2 = y",
            " *                                           y = 1",
            " *                                           barrier()",
            " *                                           r1 = x",
            " *",
            " *                     BUG_ON(r1 == 0 && r2 == 0)",
            " *",
            " * The write to y and load from x by CPU1 are unordered by the hardware,",
            " * so it's possible to have \"r1 = x\" reordered before \"y = 1\" at any",
            " * point after (b).  If the memory barrier at (a) is omitted, then \"x = 1\"",
            " * can be reordered after (a) (although not after (c)), so we get r1 == 0",
            " * and r2 == 0.  This violates the guarantee that membarrier() is",
            " * supposed by provide.",
            " *",
            " * The timing of the memory barrier at (a) has to ensure that it executes",
            " * before the IPI-induced memory barrier on CPU1.",
            " *",
            " * B) Userspace thread execution before IPI vs membarrier's memory",
            " *    barrier after completing the IPI",
            " *",
            " * Userspace variables:",
            " *",
            " * int x = 0, y = 0;",
            " *",
            " * The memory barrier at the end of membarrier() on CPU0 is necessary in",
            " * order to enforce the guarantee that any writes occurring on CPU1 before",
            " * the membarrier() is executed will be visible to any code executing on",
            " * CPU0 after the membarrier():",
            " *",
            " *         CPU0                              CPU1",
            " *",
            " *                                           x = 1",
            " *                                           barrier()",
            " *                                           y = 1",
            " *         r2 = y",
            " *         membarrier():",
            " *           a: smp_mb()",
            " *           b: send IPI                       IPI-induced mb",
            " *           c: smp_mb()",
            " *         r1 = x",
            " *         BUG_ON(r1 == 0 && r2 == 1)",
            " *",
            " * The writes to x and y are unordered by the hardware, so it's possible to",
            " * have \"r2 = 1\" even though the write to x doesn't execute until (b).  If",
            " * the memory barrier at (c) is omitted then \"r1 = x\" can be reordered",
            " * before (b) (although not before (a)), so we get \"r1 = 0\".  This violates",
            " * the guarantee that membarrier() is supposed to provide.",
            " *",
            " * The timing of the memory barrier at (c) has to ensure that it executes",
            " * after the IPI-induced memory barrier on CPU1.",
            " *",
            " * C) Scheduling userspace thread -> kthread -> userspace thread vs membarrier",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *           a: smp_mb()",
            " *                                           d: switch to kthread (includes mb)",
            " *           b: read rq->curr->mm == NULL",
            " *                                           e: switch to user (includes mb)",
            " *           c: smp_mb()",
            " *",
            " * Using the scenario from (A), we can show that (a) needs to be paired",
            " * with (e). Using the scenario from (B), we can show that (c) needs to",
            " * be paired with (d).",
            " *",
            " * D) exit_mm vs membarrier",
            " *",
            " * Two thread groups are created, A and B.  Thread group B is created by",
            " * issuing clone from group A with flag CLONE_VM set, but not CLONE_THREAD.",
            " * Let's assume we have a single thread within each thread group (Thread A",
            " * and Thread B).  Thread A runs on CPU0, Thread B runs on CPU1.",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *             a: smp_mb()",
            " *                                           exit_mm():",
            " *                                             d: smp_mb()",
            " *                                             e: current->mm = NULL",
            " *             b: read rq->curr->mm == NULL",
            " *             c: smp_mb()",
            " *",
            " * Using scenario (B), we can show that (c) needs to be paired with (d).",
            " *",
            " * E) kthread_{use,unuse}_mm vs membarrier",
            " *",
            " *           CPU0                            CPU1",
            " *",
            " *           membarrier():",
            " *           a: smp_mb()",
            " *                                           kthread_unuse_mm()",
            " *                                             d: smp_mb()",
            " *                                             e: current->mm = NULL",
            " *           b: read rq->curr->mm == NULL",
            " *                                           kthread_use_mm()",
            " *                                             f: current->mm = mm",
            " *                                             g: smp_mb()",
            " *           c: smp_mb()",
            " *",
            " * Using the scenario from (A), we can show that (a) needs to be paired",
            " * with (g). Using the scenario from (B), we can show that (c) needs to",
            " * be paired with (d).",
            " */",
            "",
            "/*",
            " * Bitmask made from a \"or\" of all commands within enum membarrier_cmd,",
            " * except MEMBARRIER_CMD_QUERY.",
            " */",
            "#ifdef CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t\t\t\\",
            "\t(MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE)",
            "#else",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t0",
            "#endif",
            "",
            "#ifdef CONFIG_RSEQ",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t\t\\",
            "\t(MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_RSEQ)",
            "#else",
            "#define MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t0",
            "#endif",
            "",
            "#define MEMBARRIER_CMD_BITMASK\t\t\t\t\t\t\\",
            "\t(MEMBARRIER_CMD_GLOBAL | MEMBARRIER_CMD_GLOBAL_EXPEDITED\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED\t\t\t\\",
            "\t| MEMBARRIER_CMD_PRIVATE_EXPEDITED\t\t\t\t\\",
            "\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED\t\t\t\\",
            "\t| MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK\t\t\\",
            "\t| MEMBARRIER_PRIVATE_EXPEDITED_RSEQ_BITMASK\t\t\t\\",
            "\t| MEMBARRIER_CMD_GET_REGISTRATIONS)",
            "",
            "static DEFINE_MUTEX(membarrier_ipi_mutex);",
            "#define SERIALIZE_IPI() guard(mutex)(&membarrier_ipi_mutex)",
            ""
          ],
          "function_name": null,
          "description": "定义内存屏障命令位掩码和互斥锁，用于协调多处理器间内存顺序保证。",
          "similarity": 0.5627602338790894
        }
      ]
    }
  ]
}