{
  "query": "实时系统启动流程优化",
  "timestamp": "2025-12-26 00:34:23",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.6093587875366211,
      "chunks": [
        {
          "chunk_id": 11,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1449,
          "end_line": 1589,
          "content": [
            "static void dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rq *rq = rq_of_rt_se(rt_se);",
            "",
            "\tupdate_stats_dequeue_rt(rt_rq_of_se(rt_se), rt_se, flags);",
            "",
            "\tdequeue_rt_stack(rt_se, flags);",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\t\tif (rt_rq && rt_rq->rt_nr_running)",
            "\t\t\t__enqueue_rt_entity(rt_se, flags);",
            "\t}",
            "\tenqueue_top_rt_rq(&rq->rt);",
            "}",
            "static void",
            "enqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tif (flags & ENQUEUE_WAKEUP)",
            "\t\trt_se->timeout = 0;",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_rt(rt_rq_of_se(rt_se), rt_se);",
            "",
            "\tenqueue_rt_entity(rt_se, flags);",
            "",
            "\tif (!task_current(rq, p) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_task(rq, p);",
            "}",
            "static bool dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tdequeue_rt_entity(rt_se, flags);",
            "",
            "\tdequeue_pushable_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void",
            "requeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)",
            "{",
            "\tif (on_rt_rq(rt_se)) {",
            "\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "\t\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);",
            "",
            "\t\tif (head)",
            "\t\t\tlist_move(&rt_se->run_list, queue);",
            "\t\telse",
            "\t\t\tlist_move_tail(&rt_se->run_list, queue);",
            "\t}",
            "}",
            "static void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\trt_rq = rt_rq_of_se(rt_se);",
            "\t\trequeue_rt_entity(rt_rq, rt_se, head);",
            "\t}",
            "}",
            "static void yield_task_rt(struct rq *rq)",
            "{",
            "\trequeue_task_rt(rq, rq->curr, 0);",
            "}",
            "static int",
            "select_task_rq_rt(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tstruct rq *rq;",
            "\tbool test;",
            "",
            "\t/* For anything but wake ups, just return the task_cpu */",
            "\tif (!(flags & (WF_TTWU | WF_FORK)))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If the current task on @p's runqueue is an RT task, then",
            "\t * try to see if we can wake this RT task up on another",
            "\t * runqueue. Otherwise simply start this RT task",
            "\t * on its current runqueue.",
            "\t *",
            "\t * We want to avoid overloading runqueues. If the woken",
            "\t * task is a higher priority, then it will stay on this CPU",
            "\t * and the lower prio task should be moved to another CPU.",
            "\t * Even though this will probably make the lower prio task",
            "\t * lose its cache, we do not want to bounce a higher task",
            "\t * around just because it gave up its CPU, perhaps for a",
            "\t * lock?",
            "\t *",
            "\t * For equal prio tasks, we just let the scheduler sort it out.",
            "\t *",
            "\t * Otherwise, just let it ride on the affined RQ and the",
            "\t * post-schedule router will push the preempted task away",
            "\t *",
            "\t * This test is optimistic, if we get it wrong the load-balancer",
            "\t * will have to sort it out.",
            "\t *",
            "\t * We take into account the capacity of the CPU to ensure it fits the",
            "\t * requirement of the task - which is only important on heterogeneous",
            "\t * systems like big.LITTLE.",
            "\t */",
            "\ttest = curr &&",
            "\t       unlikely(rt_task(curr)) &&",
            "\t       (curr->nr_cpus_allowed < 2 || curr->prio <= p->prio);",
            "",
            "\tif (test || !rt_task_fits_capacity(p, cpu)) {",
            "\t\tint target = find_lowest_rq(p);",
            "",
            "\t\t/*",
            "\t\t * Bail out if we were forcing a migration to find a better",
            "\t\t * fitting CPU but our search failed.",
            "\t\t */",
            "\t\tif (!test && target != -1 && !rt_task_fits_capacity(p, target))",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\t/*",
            "\t\t * Don't bother moving it if the destination CPU is",
            "\t\t * not running a lower priority task.",
            "\t\t */",
            "\t\tif (target != -1 &&",
            "\t\t    p->prio < cpu_rq(target)->rt.highest_prio.curr)",
            "\t\t\tcpu = target;",
            "\t}",
            "",
            "out_unlock:",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "dequeue_rt_entity, enqueue_task_rt, dequeue_task_rt, requeue_rt_entity, requeue_task_rt, yield_task_rt, select_task_rq_rt",
          "description": "实现实时任务的出队逻辑、唤醒和迁移策略，提供CPU亲和性选择及负载均衡支持，维护优先级队列的动态调整。",
          "similarity": 0.665505051612854
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1776,
          "end_line": 1992,
          "content": [
            "static int pick_rt_task(struct rq *rq, struct task_struct *p, int cpu)",
            "{",
            "\tif (!task_on_cpu(rq, p) &&",
            "\t    cpumask_test_cpu(cpu, &p->cpus_mask))",
            "\t\treturn 1;",
            "",
            "\treturn 0;",
            "}",
            "static int find_lowest_rq(struct task_struct *task)",
            "{",
            "\tstruct sched_domain *sd;",
            "\tstruct cpumask *lowest_mask = this_cpu_cpumask_var_ptr(local_cpu_mask);",
            "\tint this_cpu = smp_processor_id();",
            "\tint cpu      = task_cpu(task);",
            "\tint ret;",
            "",
            "\t/* Make sure the mask is initialized first */",
            "\tif (unlikely(!lowest_mask))",
            "\t\treturn -1;",
            "",
            "\tif (task->nr_cpus_allowed == 1)",
            "\t\treturn -1; /* No other targets possible */",
            "",
            "\t/*",
            "\t * If we're on asym system ensure we consider the different capacities",
            "\t * of the CPUs when searching for the lowest_mask.",
            "\t */",
            "\tif (sched_asym_cpucap_active()) {",
            "",
            "\t\tret = cpupri_find_fitness(&task_rq(task)->rd->cpupri,",
            "\t\t\t\t\t  task, lowest_mask,",
            "\t\t\t\t\t  rt_task_fits_capacity);",
            "\t} else {",
            "",
            "\t\tret = cpupri_find(&task_rq(task)->rd->cpupri,",
            "\t\t\t\t  task, lowest_mask);",
            "\t}",
            "",
            "\tif (!ret)",
            "\t\treturn -1; /* No targets found */",
            "",
            "\t/*",
            "\t * At this point we have built a mask of CPUs representing the",
            "\t * lowest priority tasks in the system.  Now we want to elect",
            "\t * the best one based on our affinity and topology.",
            "\t *",
            "\t * We prioritize the last CPU that the task executed on since",
            "\t * it is most likely cache-hot in that location.",
            "\t */",
            "\tif (cpumask_test_cpu(cpu, lowest_mask))",
            "\t\treturn cpu;",
            "",
            "\t/*",
            "\t * Otherwise, we consult the sched_domains span maps to figure",
            "\t * out which CPU is logically closest to our hot cache data.",
            "\t */",
            "\tif (!cpumask_test_cpu(this_cpu, lowest_mask))",
            "\t\tthis_cpu = -1; /* Skip this_cpu opt if not among lowest */",
            "",
            "\trcu_read_lock();",
            "\tfor_each_domain(cpu, sd) {",
            "\t\tif (sd->flags & SD_WAKE_AFFINE) {",
            "\t\t\tint best_cpu;",
            "",
            "\t\t\t/*",
            "\t\t\t * \"this_cpu\" is cheaper to preempt than a",
            "\t\t\t * remote processor.",
            "\t\t\t */",
            "\t\t\tif (this_cpu != -1 &&",
            "\t\t\t    cpumask_test_cpu(this_cpu, sched_domain_span(sd))) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn this_cpu;",
            "\t\t\t}",
            "",
            "\t\t\tbest_cpu = cpumask_any_and_distribute(lowest_mask,",
            "\t\t\t\t\t\t\t      sched_domain_span(sd));",
            "\t\t\tif (best_cpu < nr_cpu_ids) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn best_cpu;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * And finally, if there were no matches within the domains",
            "\t * just give the caller *something* to work with from the compatible",
            "\t * locations.",
            "\t */",
            "\tif (this_cpu != -1)",
            "\t\treturn this_cpu;",
            "",
            "\tcpu = cpumask_any_distribute(lowest_mask);",
            "\tif (cpu < nr_cpu_ids)",
            "\t\treturn cpu;",
            "",
            "\treturn -1;",
            "}",
            "static int push_rt_task(struct rq *rq, bool pull)",
            "{",
            "\tstruct task_struct *next_task;",
            "\tstruct rq *lowest_rq;",
            "\tint ret = 0;",
            "",
            "\tif (!rq->rt.overloaded)",
            "\t\treturn 0;",
            "",
            "\tnext_task = pick_next_pushable_task(rq);",
            "\tif (!next_task)",
            "\t\treturn 0;",
            "",
            "retry:",
            "\t/*",
            "\t * It's possible that the next_task slipped in of",
            "\t * higher priority than current. If that's the case",
            "\t * just reschedule current.",
            "\t */",
            "\tif (unlikely(next_task->prio < rq->curr->prio)) {",
            "\t\tresched_curr(rq);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (is_migration_disabled(next_task)) {",
            "\t\tstruct task_struct *push_task = NULL;",
            "\t\tint cpu;",
            "",
            "\t\tif (!pull || rq->push_busy)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * Invoking find_lowest_rq() on anything but an RT task doesn't",
            "\t\t * make sense. Per the above priority check, curr has to",
            "\t\t * be of higher priority than next_task, so no need to",
            "\t\t * reschedule when bailing out.",
            "\t\t *",
            "\t\t * Note that the stoppers are masqueraded as SCHED_FIFO",
            "\t\t * (cf. sched_set_stop_task()), so we can't rely on rt_task().",
            "\t\t */",
            "\t\tif (rq->curr->sched_class != &rt_sched_class)",
            "\t\t\treturn 0;",
            "",
            "\t\tcpu = find_lowest_rq(rq->curr);",
            "\t\tif (cpu == -1 || cpu == rq->cpu)",
            "\t\t\treturn 0;",
            "",
            "\t\t/*",
            "\t\t * Given we found a CPU with lower priority than @next_task,",
            "\t\t * therefore it should be running. However we cannot migrate it",
            "\t\t * to this other CPU, instead attempt to push the current",
            "\t\t * running task on this CPU away.",
            "\t\t */",
            "\t\tpush_task = get_push_task(rq);",
            "\t\tif (push_task) {",
            "\t\t\tpreempt_disable();",
            "\t\t\traw_spin_rq_unlock(rq);",
            "\t\t\tstop_one_cpu_nowait(rq->cpu, push_cpu_stop,",
            "\t\t\t\t\t    push_task, &rq->push_work);",
            "\t\t\tpreempt_enable();",
            "\t\t\traw_spin_rq_lock(rq);",
            "\t\t}",
            "",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (WARN_ON(next_task == rq->curr))",
            "\t\treturn 0;",
            "",
            "\t/* We might release rq lock */",
            "\tget_task_struct(next_task);",
            "",
            "\t/* find_lock_lowest_rq locks the rq if found */",
            "\tlowest_rq = find_lock_lowest_rq(next_task, rq);",
            "\tif (!lowest_rq) {",
            "\t\tstruct task_struct *task;",
            "\t\t/*",
            "\t\t * find_lock_lowest_rq releases rq->lock",
            "\t\t * so it is possible that next_task has migrated.",
            "\t\t *",
            "\t\t * We need to make sure that the task is still on the same",
            "\t\t * run-queue and is also still the next task eligible for",
            "\t\t * pushing.",
            "\t\t */",
            "\t\ttask = pick_next_pushable_task(rq);",
            "\t\tif (task == next_task) {",
            "\t\t\t/*",
            "\t\t\t * The task hasn't migrated, and is still the next",
            "\t\t\t * eligible task, but we failed to find a run-queue",
            "\t\t\t * to push it to.  Do not retry in this case, since",
            "\t\t\t * other CPUs will pull from us when ready.",
            "\t\t\t */",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tif (!task)",
            "\t\t\t/* No more tasks, just exit */",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * Something has shifted, try again.",
            "\t\t */",
            "\t\tput_task_struct(next_task);",
            "\t\tnext_task = task;",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tdeactivate_task(rq, next_task, 0);",
            "\tset_task_cpu(next_task, lowest_rq->cpu);",
            "\tactivate_task(lowest_rq, next_task, 0);",
            "\tresched_curr(lowest_rq);",
            "\tret = 1;",
            "",
            "\tdouble_unlock_balance(rq, lowest_rq);",
            "out:",
            "\tput_task_struct(next_task);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "pick_rt_task, find_lowest_rq, push_rt_task",
          "description": "实现实时任务选择算法、低优先级CPU搜索及强制迁移逻辑，支持异构系统下的能效优化和拓扑感知调度。",
          "similarity": 0.6300007104873657
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/rt.c",
          "start_line": 57,
          "end_line": 159,
          "content": [
            "static int __init sched_rt_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", sched_rt_sysctls);",
            "\treturn 0;",
            "}",
            "void init_rt_rq(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_prio_array *array;",
            "\tint i;",
            "",
            "\tarray = &rt_rq->active;",
            "\tfor (i = 0; i < MAX_RT_PRIO; i++) {",
            "\t\tINIT_LIST_HEAD(array->queue + i);",
            "\t\t__clear_bit(i, array->bitmap);",
            "\t}",
            "\t/* delimiter for bitsearch: */",
            "\t__set_bit(MAX_RT_PRIO, array->bitmap);",
            "",
            "#if defined CONFIG_SMP",
            "\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\trt_rq->highest_prio.next = MAX_RT_PRIO-1;",
            "\trt_rq->overloaded = 0;",
            "\tplist_head_init(&rt_rq->pushable_tasks);",
            "#endif /* CONFIG_SMP */",
            "\t/* We start is dequeued state, because no RT tasks are queued */",
            "\trt_rq->rt_queued = 0;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\trt_rq->rt_time = 0;",
            "\trt_rq->rt_throttled = 0;",
            "\trt_rq->rt_runtime = 0;",
            "\traw_spin_lock_init(&rt_rq->rt_runtime_lock);",
            "#endif",
            "}",
            "static enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)",
            "{",
            "\tstruct rt_bandwidth *rt_b =",
            "\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);",
            "\tint idle = 0;",
            "\tint overrun;",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tfor (;;) {",
            "\t\toverrun = hrtimer_forward_now(timer, rt_b->rt_period);",
            "\t\tif (!overrun)",
            "\t\t\tbreak;",
            "",
            "\t\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "\t\tidle = do_sched_rt_period_timer(rt_b, overrun);",
            "\t\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\t}",
            "\tif (idle)",
            "\t\trt_b->rt_period_active = 0;",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "",
            "\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;",
            "}",
            "void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)",
            "{",
            "\trt_b->rt_period = ns_to_ktime(period);",
            "\trt_b->rt_runtime = runtime;",
            "",
            "\traw_spin_lock_init(&rt_b->rt_runtime_lock);",
            "",
            "\thrtimer_init(&rt_b->rt_period_timer, CLOCK_MONOTONIC,",
            "\t\t     HRTIMER_MODE_REL_HARD);",
            "\trt_b->rt_period_timer.function = sched_rt_period_timer;",
            "}",
            "static inline void do_start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tif (!rt_b->rt_period_active) {",
            "\t\trt_b->rt_period_active = 1;",
            "\t\t/*",
            "\t\t * SCHED_DEADLINE updates the bandwidth, as a run away",
            "\t\t * RT task with a DL task could hog a CPU. But DL does",
            "\t\t * not reset the period. If a deadline task was running",
            "\t\t * without an RT task running, it can cause RT tasks to",
            "\t\t * throttle when they start up. Kick the timer right away",
            "\t\t * to update the period.",
            "\t\t */",
            "\t\thrtimer_forward_now(&rt_b->rt_period_timer, ns_to_ktime(0));",
            "\t\thrtimer_start_expires(&rt_b->rt_period_timer,",
            "\t\t\t\t      HRTIMER_MODE_ABS_PINNED_HARD);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}",
            "static void start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)",
            "\t\treturn;",
            "",
            "\tdo_start_rt_bandwidth(rt_b);",
            "}",
            "static void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\thrtimer_cancel(&rt_b->rt_period_timer);",
            "}",
            "void unregister_rt_sched_group(struct task_group *tg)",
            "{",
            "\tif (tg->rt_se)",
            "\t\tdestroy_rt_bandwidth(&tg->rt_bandwidth);",
            "}"
          ],
          "function_name": "sched_rt_sysctl_init, init_rt_rq, sched_rt_period_timer, init_rt_bandwidth, do_start_rt_bandwidth, start_rt_bandwidth, destroy_rt_bandwidth, unregister_rt_sched_group",
          "description": "初始化实时调度相关数据结构，管理实时任务周期定时器，控制实时带宽分配与回收，实现基于时间片轮转的调度策略。",
          "similarity": 0.6200723648071289
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/rt.c",
          "start_line": 529,
          "end_line": 633,
          "content": [
            "static void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\tstruct sched_rt_entity *rt_se;",
            "",
            "\tint cpu = cpu_of(rq);",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tif (!rt_se)",
            "\t\t\tenqueue_top_rt_rq(rt_rq);",
            "\t\telse if (!on_rt_rq(rt_se))",
            "\t\t\tenqueue_rt_entity(rt_se, 0);",
            "",
            "\t\tif (rt_rq->highest_prio.curr < curr->prio)",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "}",
            "static void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (!rt_se) {",
            "\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "\t\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);",
            "\t}",
            "\telse if (on_rt_rq(rt_se))",
            "\t\tdequeue_rt_entity(rt_se, 0);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;",
            "}",
            "static int rt_se_boosted(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "\tstruct task_struct *p;",
            "",
            "\tif (rt_rq)",
            "\t\treturn !!rt_rq->rt_nr_boosted;",
            "",
            "\tp = rt_task_of(rt_se);",
            "\treturn p->prio != p->normal_prio;",
            "}",
            "bool sched_rt_bandwidth_account(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\treturn (hrtimer_active(&rt_b->rt_period_timer) ||",
            "\t\trt_rq->rt_time < rt_b->rt_runtime);",
            "}",
            "static void do_balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;",
            "\tint i, weight;",
            "\tu64 rt_period;",
            "",
            "\tweight = cpumask_weight(rd->span);",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\trt_period = ktime_to_ns(rt_b->rt_period);",
            "\tfor_each_cpu(i, rd->span) {",
            "\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\ts64 diff;",
            "",
            "\t\tif (iter == rt_rq)",
            "\t\t\tcontinue;",
            "",
            "\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either all rqs have inf runtime and there's nothing to steal",
            "\t\t * or __disable_runtime() below sets a specific rq to inf to",
            "\t\t * indicate its been disabled and disallow stealing.",
            "\t\t */",
            "\t\tif (iter->rt_runtime == RUNTIME_INF)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * From runqueues with spare time, take 1/n part of their",
            "\t\t * spare time, but no more than our period.",
            "\t\t */",
            "\t\tdiff = iter->rt_runtime - iter->rt_time;",
            "\t\tif (diff > 0) {",
            "\t\t\tdiff = div_u64((u64)diff, weight);",
            "\t\t\tif (rt_rq->rt_runtime + diff > rt_period)",
            "\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;",
            "\t\t\titer->rt_runtime -= diff;",
            "\t\t\trt_rq->rt_runtime += diff;",
            "\t\t\tif (rt_rq->rt_runtime == rt_period) {",
            "\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "next:",
            "\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, rt_se_boosted, sched_rt_bandwidth_account, do_balance_runtime",
          "description": "实现实时任务队列的插入/移除逻辑，跟踪运行时间消耗，通过跨CPU运行时间平衡算法实现带宽公平分配。",
          "similarity": 0.6187093257904053
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/rt.c",
          "start_line": 201,
          "end_line": 311,
          "content": [
            "void free_rt_sched_group(struct task_group *tg)",
            "{",
            "\tint i;",
            "",
            "\tfor_each_possible_cpu(i) {",
            "\t\tif (tg->rt_rq)",
            "\t\t\tkfree(tg->rt_rq[i]);",
            "\t\tif (tg->rt_se)",
            "\t\t\tkfree(tg->rt_se[i]);",
            "\t}",
            "",
            "\tkfree(tg->rt_rq);",
            "\tkfree(tg->rt_se);",
            "}",
            "void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,",
            "\t\tstruct sched_rt_entity *rt_se, int cpu,",
            "\t\tstruct sched_rt_entity *parent)",
            "{",
            "\tstruct rq *rq = cpu_rq(cpu);",
            "",
            "\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\trt_rq->rt_nr_boosted = 0;",
            "\trt_rq->rq = rq;",
            "\trt_rq->tg = tg;",
            "",
            "\ttg->rt_rq[cpu] = rt_rq;",
            "\ttg->rt_se[cpu] = rt_se;",
            "",
            "\tif (!rt_se)",
            "\t\treturn;",
            "",
            "\tif (!parent)",
            "\t\trt_se->rt_rq = &rq->rt;",
            "\telse",
            "\t\trt_se->rt_rq = parent->my_q;",
            "",
            "\trt_se->my_q = rt_rq;",
            "\trt_se->parent = parent;",
            "\tINIT_LIST_HEAD(&rt_se->run_list);",
            "}",
            "int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)",
            "{",
            "\tstruct rt_rq *rt_rq;",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint i;",
            "",
            "\ttg->rt_rq = kcalloc(nr_cpu_ids, sizeof(rt_rq), GFP_KERNEL);",
            "\tif (!tg->rt_rq)",
            "\t\tgoto err;",
            "\ttg->rt_se = kcalloc(nr_cpu_ids, sizeof(rt_se), GFP_KERNEL);",
            "\tif (!tg->rt_se)",
            "\t\tgoto err;",
            "",
            "\tinit_rt_bandwidth(&tg->rt_bandwidth, ktime_to_ns(global_rt_period()), 0);",
            "",
            "\tfor_each_possible_cpu(i) {",
            "\t\trt_rq = kzalloc_node(sizeof(struct rt_rq),",
            "\t\t\t\t     GFP_KERNEL, cpu_to_node(i));",
            "\t\tif (!rt_rq)",
            "\t\t\tgoto err;",
            "",
            "\t\trt_se = kzalloc_node(sizeof(struct sched_rt_entity),",
            "\t\t\t\t     GFP_KERNEL, cpu_to_node(i));",
            "\t\tif (!rt_se)",
            "\t\t\tgoto err_free_rq;",
            "",
            "\t\tinit_rt_rq(rt_rq);",
            "\t\trt_rq->rt_runtime = tg->rt_bandwidth.rt_runtime;",
            "\t\tinit_tg_rt_entry(tg, rt_rq, rt_se, i, parent->rt_se[i]);",
            "\t}",
            "",
            "\treturn 1;",
            "",
            "err_free_rq:",
            "\tkfree(rt_rq);",
            "err:",
            "\treturn 0;",
            "}",
            "void unregister_rt_sched_group(struct task_group *tg) { }",
            "void free_rt_sched_group(struct task_group *tg) { }",
            "int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)",
            "{",
            "\treturn 1;",
            "}",
            "static inline bool need_pull_rt_task(struct rq *rq, struct task_struct *prev)",
            "{",
            "\t/* Try to pull RT tasks here if we lower this rq's prio */",
            "\treturn rq->online && rq->rt.highest_prio.curr > prev->prio;",
            "}",
            "static inline int rt_overloaded(struct rq *rq)",
            "{",
            "\treturn atomic_read(&rq->rd->rto_count);",
            "}",
            "static inline void rt_set_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tcpumask_set_cpu(rq->cpu, rq->rd->rto_mask);",
            "\t/*",
            "\t * Make sure the mask is visible before we set",
            "\t * the overload count. That is checked to determine",
            "\t * if we should look at the mask. It would be a shame",
            "\t * if we looked at the mask, but the mask was not",
            "\t * updated yet.",
            "\t *",
            "\t * Matched by the barrier in pull_rt_task().",
            "\t */",
            "\tsmp_wmb();",
            "\tatomic_inc(&rq->rd->rto_count);",
            "}"
          ],
          "function_name": "free_rt_sched_group, init_tg_rt_entry, alloc_rt_sched_group, unregister_rt_sched_group, free_rt_sched_group, alloc_rt_sched_group, need_pull_rt_task, rt_overloaded, rt_set_overload",
          "description": "分配/释放实时调度组资源，初始化任务组内的运行队列和实体结构，处理实时任务提升（boost）状态的标记与管理。",
          "similarity": 0.6162615418434143
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.6033389568328857,
      "chunks": [
        {
          "chunk_id": 24,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4444,
          "end_line": 4559,
          "content": [
            "static void __init",
            "rcu_boot_init_percpu_data(int cpu)",
            "{",
            "\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t/* Set up local state, ensuring consistent view of global state. */",
            "\trdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);",
            "\tINIT_WORK(&rdp->strict_work, strict_work_handler);",
            "\tWARN_ON_ONCE(ct->dynticks_nesting != 1);",
            "\tWARN_ON_ONCE(rcu_dynticks_in_eqs(rcu_dynticks_snap(cpu)));",
            "\trdp->barrier_seq_snap = rcu_state.barrier_sequence;",
            "\trdp->rcu_ofl_gp_seq = rcu_state.gp_seq;",
            "\trdp->rcu_ofl_gp_flags = RCU_GP_CLEANED;",
            "\trdp->rcu_onl_gp_seq = rcu_state.gp_seq;",
            "\trdp->rcu_onl_gp_flags = RCU_GP_CLEANED;",
            "\trdp->last_sched_clock = jiffies;",
            "\trdp->cpu = cpu;",
            "\trcu_boot_init_nocb_percpu_data(rdp);",
            "}",
            "int rcutree_prepare_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct context_tracking *ct = per_cpu_ptr(&context_tracking, cpu);",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\t/* Set up local state, ensuring consistent view of global state. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trdp->qlen_last_fqs_check = 0;",
            "\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\trdp->blimit = blimit;",
            "\tct->dynticks_nesting = 1;\t/* CPU not up, no tearing. */",
            "\traw_spin_unlock_rcu_node(rnp);\t\t/* irqs remain disabled. */",
            "",
            "\t/*",
            "\t * Only non-NOCB CPUs that didn't have early-boot callbacks need to be",
            "\t * (re-)initialized.",
            "\t */",
            "\tif (!rcu_segcblist_is_enabled(&rdp->cblist))",
            "\t\trcu_segcblist_init(&rdp->cblist);  /* Re-enable callbacks. */",
            "",
            "\t/*",
            "\t * Add CPU to leaf rcu_node pending-online bitmask.  Any needed",
            "\t * propagation up the rcu_node tree will happen at the beginning",
            "\t * of the next grace period.",
            "\t */",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_rcu_node(rnp);\t\t/* irqs already disabled. */",
            "\trdp->gp_seq = READ_ONCE(rnp->gp_seq);",
            "\trdp->gp_seq_needed = rdp->gp_seq;",
            "\trdp->cpu_no_qs.b.norm = true;",
            "\trdp->core_needs_qs = false;",
            "\trdp->rcu_iw_pending = false;",
            "\trdp->rcu_iw = IRQ_WORK_INIT_HARD(rcu_iw_handler);",
            "\trdp->rcu_iw_gp_seq = rdp->gp_seq - 1;",
            "\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuonl\"));",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "",
            "\trcu_preempt_deferred_qs_init(rdp);",
            "\trcu_spawn_one_boost_kthread(rnp);",
            "\trcu_spawn_cpu_nocb_kthread(cpu);",
            "\tWRITE_ONCE(rcu_state.n_online_cpus, rcu_state.n_online_cpus + 1);",
            "",
            "\treturn 0;",
            "}",
            "static void rcutree_affinity_setting(unsigned int cpu, int outgoing)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\trcu_boost_kthread_setaffinity(rdp->mynode, outgoing);",
            "}",
            "bool rcu_cpu_beenfullyonline(int cpu)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\treturn smp_load_acquire(&rdp->beenonline);",
            "}",
            "int rcutree_online_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trnp->ffmask |= rdp->grpmask;",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_INACTIVE)",
            "\t\treturn 0; /* Too early in boot for scheduler work. */",
            "\tsync_sched_exp_online_cleanup(cpu);",
            "\trcutree_affinity_setting(cpu, -1);",
            "",
            "\t// Stop-machine done, so allow nohz_full to disable tick.",
            "\ttick_dep_clear(TICK_DEP_BIT_RCU);",
            "\treturn 0;",
            "}",
            "int rcutree_offline_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trnp->ffmask &= ~rdp->grpmask;",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "",
            "\trcutree_affinity_setting(cpu, cpu);",
            "",
            "\t// nohz_full CPUs need the tick for stop-machine to work quickly",
            "\ttick_dep_set(TICK_DEP_BIT_RCU);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rcu_boot_init_percpu_data, rcutree_prepare_cpu, rcutree_affinity_setting, rcu_cpu_beenfullyonline, rcutree_online_cpu, rcutree_offline_cpu",
          "description": "初始化每个CPU的RCU私有数据结构，处理CPU上线/下线时的RCU状态同步，配置中断亲和性，更新全局在线CPU计数器",
          "similarity": 0.6460481882095337
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1354,
          "end_line": 1556,
          "content": [
            "static void rcu_strict_gp_boundary(void *unused)",
            "{",
            "\tinvoke_rcu_core();",
            "}",
            "static void rcu_poll_gp_seq_start(unsigned long *snap)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)",
            "\t\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t// If RCU was idle, note beginning of GP.",
            "\tif (!rcu_seq_state(rcu_state.gp_seq_polled))",
            "\t\trcu_seq_start(&rcu_state.gp_seq_polled);",
            "",
            "\t// Either way, record current state.",
            "\t*snap = rcu_state.gp_seq_polled;",
            "}",
            "static void rcu_poll_gp_seq_end(unsigned long *snap)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)",
            "\t\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t// If the previously noted GP is still in effect, record the",
            "\t// end of that GP.  Either way, zero counter to avoid counter-wrap",
            "\t// problems.",
            "\tif (*snap && *snap == rcu_state.gp_seq_polled) {",
            "\t\trcu_seq_end(&rcu_state.gp_seq_polled);",
            "\t\trcu_state.gp_seq_polled_snap = 0;",
            "\t\trcu_state.gp_seq_polled_exp_snap = 0;",
            "\t} else {",
            "\t\t*snap = 0;",
            "\t}",
            "}",
            "static void rcu_poll_gp_seq_start_unlocked(unsigned long *snap)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tif (rcu_init_invoked()) {",
            "\t\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t}",
            "\trcu_poll_gp_seq_start(snap);",
            "\tif (rcu_init_invoked())",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "}",
            "static void rcu_poll_gp_seq_end_unlocked(unsigned long *snap)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tif (rcu_init_invoked()) {",
            "\t\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t}",
            "\trcu_poll_gp_seq_end(snap);",
            "\tif (rcu_init_invoked())",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "}",
            "static noinline_for_stack bool rcu_gp_init(void)",
            "{",
            "\tunsigned long flags;",
            "\tunsigned long oldmask;",
            "\tunsigned long mask;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\traw_spin_lock_irq_rcu_node(rnp);",
            "\tif (!READ_ONCE(rcu_state.gp_flags)) {",
            "\t\t/* Spurious wakeup, tell caller to go back to sleep.  */",
            "\t\traw_spin_unlock_irq_rcu_node(rnp);",
            "\t\treturn false;",
            "\t}",
            "\tWRITE_ONCE(rcu_state.gp_flags, 0); /* Clear all flags: New GP. */",
            "",
            "\tif (WARN_ON_ONCE(rcu_gp_in_progress())) {",
            "\t\t/*",
            "\t\t * Grace period already in progress, don't start another.",
            "\t\t * Not supposed to be able to happen.",
            "\t\t */",
            "\t\traw_spin_unlock_irq_rcu_node(rnp);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/* Advance to a new grace period and initialize state. */",
            "\trecord_gp_stall_check_time();",
            "\t/* Record GP times before starting GP, hence rcu_seq_start(). */",
            "\trcu_seq_start(&rcu_state.gp_seq);",
            "\tASSERT_EXCLUSIVE_WRITER(rcu_state.gp_seq);",
            "\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq, TPS(\"start\"));",
            "\trcu_poll_gp_seq_start(&rcu_state.gp_seq_polled_snap);",
            "\traw_spin_unlock_irq_rcu_node(rnp);",
            "",
            "\t/*",
            "\t * Apply per-leaf buffered online and offline operations to",
            "\t * the rcu_node tree. Note that this new grace period need not",
            "\t * wait for subsequent online CPUs, and that RCU hooks in the CPU",
            "\t * offlining path, when combined with checks in this function,",
            "\t * will handle CPUs that are currently going offline or that will",
            "\t * go offline later.  Please also refer to \"Hotplug CPU\" section",
            "\t * of RCU's Requirements documentation.",
            "\t */",
            "\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_ONOFF);",
            "\t/* Exclude CPU hotplug operations. */",
            "\trcu_for_each_leaf_node(rnp) {",
            "\t\tlocal_irq_save(flags);",
            "\t\tarch_spin_lock(&rcu_state.ofl_lock);",
            "\t\traw_spin_lock_rcu_node(rnp);",
            "\t\tif (rnp->qsmaskinit == rnp->qsmaskinitnext &&",
            "\t\t    !rnp->wait_blkd_tasks) {",
            "\t\t\t/* Nothing to do on this leaf rcu_node structure. */",
            "\t\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\t\tarch_spin_unlock(&rcu_state.ofl_lock);",
            "\t\t\tlocal_irq_restore(flags);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/* Record old state, apply changes to ->qsmaskinit field. */",
            "\t\toldmask = rnp->qsmaskinit;",
            "\t\trnp->qsmaskinit = rnp->qsmaskinitnext;",
            "",
            "\t\t/* If zero-ness of ->qsmaskinit changed, propagate up tree. */",
            "\t\tif (!oldmask != !rnp->qsmaskinit) {",
            "\t\t\tif (!oldmask) { /* First online CPU for rcu_node. */",
            "\t\t\t\tif (!rnp->wait_blkd_tasks) /* Ever offline? */",
            "\t\t\t\t\trcu_init_new_rnp(rnp);",
            "\t\t\t} else if (rcu_preempt_has_tasks(rnp)) {",
            "\t\t\t\trnp->wait_blkd_tasks = true; /* blocked tasks */",
            "\t\t\t} else { /* Last offline CPU and can propagate. */",
            "\t\t\t\trcu_cleanup_dead_rnp(rnp);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If all waited-on tasks from prior grace period are",
            "\t\t * done, and if all this rcu_node structure's CPUs are",
            "\t\t * still offline, propagate up the rcu_node tree and",
            "\t\t * clear ->wait_blkd_tasks.  Otherwise, if one of this",
            "\t\t * rcu_node structure's CPUs has since come back online,",
            "\t\t * simply clear ->wait_blkd_tasks.",
            "\t\t */",
            "\t\tif (rnp->wait_blkd_tasks &&",
            "\t\t    (!rcu_preempt_has_tasks(rnp) || rnp->qsmaskinit)) {",
            "\t\t\trnp->wait_blkd_tasks = false;",
            "\t\t\tif (!rnp->qsmaskinit)",
            "\t\t\t\trcu_cleanup_dead_rnp(rnp);",
            "\t\t}",
            "",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\tarch_spin_unlock(&rcu_state.ofl_lock);",
            "\t\tlocal_irq_restore(flags);",
            "\t}",
            "\trcu_gp_slow(gp_preinit_delay); /* Races with CPU hotplug. */",
            "",
            "\t/*",
            "\t * Set the quiescent-state-needed bits in all the rcu_node",
            "\t * structures for all currently online CPUs in breadth-first",
            "\t * order, starting from the root rcu_node structure, relying on the",
            "\t * layout of the tree within the rcu_state.node[] array.  Note that",
            "\t * other CPUs will access only the leaves of the hierarchy, thus",
            "\t * seeing that no grace period is in progress, at least until the",
            "\t * corresponding leaf node has been initialized.",
            "\t *",
            "\t * The grace period cannot complete until the initialization",
            "\t * process finishes, because this kthread handles both.",
            "\t */",
            "\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_INIT);",
            "\trcu_for_each_node_breadth_first(rnp) {",
            "\t\trcu_gp_slow(gp_init_delay);",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\trdp = this_cpu_ptr(&rcu_data);",
            "\t\trcu_preempt_check_blocked_tasks(rnp);",
            "\t\trnp->qsmask = rnp->qsmaskinit;",
            "\t\tWRITE_ONCE(rnp->gp_seq, rcu_state.gp_seq);",
            "\t\tif (rnp == rdp->mynode)",
            "\t\t\t(void)__note_gp_changes(rnp, rdp);",
            "\t\trcu_preempt_boost_start_gp(rnp);",
            "\t\ttrace_rcu_grace_period_init(rcu_state.name, rnp->gp_seq,",
            "\t\t\t\t\t    rnp->level, rnp->grplo,",
            "\t\t\t\t\t    rnp->grphi, rnp->qsmask);",
            "\t\t/* Quiescent states for tasks on any now-offline CPUs. */",
            "\t\tmask = rnp->qsmask & ~rnp->qsmaskinitnext;",
            "\t\trnp->rcu_gp_init_mask = mask;",
            "\t\tif ((mask || rnp->wait_blkd_tasks) && rcu_is_leaf_node(rnp))",
            "\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\telse",
            "\t\t\traw_spin_unlock_irq_rcu_node(rnp);",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\t}",
            "",
            "\t// If strict, make all CPUs aware of new grace period.",
            "\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD))",
            "\t\ton_each_cpu(rcu_strict_gp_boundary, NULL, 0);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "rcu_strict_gp_boundary, rcu_poll_gp_seq_start, rcu_poll_gp_seq_end, rcu_poll_gp_seq_start_unlocked, rcu_poll_gp_seq_end_unlocked, rcu_gp_init",
          "description": "初始化新grace period并处理CPU热插拔，通过遍历rcu_node树更新qsmask字段，设置严格模式下的全局通知，并协调初始化过程与后续处理。",
          "similarity": 0.6363741159439087
        },
        {
          "chunk_id": 28,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 5098,
          "end_line": 5206,
          "content": [
            "static void __init rcu_dump_rcu_node_tree(void)",
            "{",
            "\tint level = 0;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tpr_info(\"rcu_node tree layout dump\\n\");",
            "\tpr_info(\" \");",
            "\trcu_for_each_node_breadth_first(rnp) {",
            "\t\tif (rnp->level != level) {",
            "\t\t\tpr_cont(\"\\n\");",
            "\t\t\tpr_info(\" \");",
            "\t\t\tlevel = rnp->level;",
            "\t\t}",
            "\t\tpr_cont(\"%d:%d ^%d  \", rnp->grplo, rnp->grphi, rnp->grpnum);",
            "\t}",
            "\tpr_cont(\"\\n\");",
            "}",
            "static void __init kfree_rcu_batch_init(void)",
            "{",
            "\tint cpu;",
            "\tint i, j;",
            "\tstruct shrinker *kfree_rcu_shrinker;",
            "",
            "\t/* Clamp it to [0:100] seconds interval. */",
            "\tif (rcu_delay_page_cache_fill_msec < 0 ||",
            "\t\trcu_delay_page_cache_fill_msec > 100 * MSEC_PER_SEC) {",
            "",
            "\t\trcu_delay_page_cache_fill_msec =",
            "\t\t\tclamp(rcu_delay_page_cache_fill_msec, 0,",
            "\t\t\t\t(int) (100 * MSEC_PER_SEC));",
            "",
            "\t\tpr_info(\"Adjusting rcutree.rcu_delay_page_cache_fill_msec to %d ms.\\n\",",
            "\t\t\trcu_delay_page_cache_fill_msec);",
            "\t}",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);",
            "",
            "\t\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\t\tINIT_RCU_WORK(&krcp->krw_arr[i].rcu_work, kfree_rcu_work);",
            "\t\t\tkrcp->krw_arr[i].krcp = krcp;",
            "",
            "\t\t\tfor (j = 0; j < FREE_N_CHANNELS; j++)",
            "\t\t\t\tINIT_LIST_HEAD(&krcp->krw_arr[i].bulk_head_free[j]);",
            "\t\t}",
            "",
            "\t\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\t\tINIT_LIST_HEAD(&krcp->bulk_head[i]);",
            "",
            "\t\tINIT_DELAYED_WORK(&krcp->monitor_work, kfree_rcu_monitor);",
            "\t\tINIT_DELAYED_WORK(&krcp->page_cache_work, fill_page_cache_func);",
            "\t\tkrcp->initialized = true;",
            "\t}",
            "",
            "\tkfree_rcu_shrinker = shrinker_alloc(0, \"rcu-kfree\");",
            "\tif (!kfree_rcu_shrinker) {",
            "\t\tpr_err(\"Failed to allocate kfree_rcu() shrinker!\\n\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tkfree_rcu_shrinker->count_objects = kfree_rcu_shrink_count;",
            "\tkfree_rcu_shrinker->scan_objects = kfree_rcu_shrink_scan;",
            "",
            "\tshrinker_register(kfree_rcu_shrinker);",
            "}",
            "void __init rcu_init(void)",
            "{",
            "\tint cpu = smp_processor_id();",
            "",
            "\trcu_early_boot_tests();",
            "",
            "\tkfree_rcu_batch_init();",
            "\trcu_bootup_announce();",
            "\tsanitize_kthread_prio();",
            "\trcu_init_geometry();",
            "\trcu_init_one();",
            "\tif (dump_tree)",
            "\t\trcu_dump_rcu_node_tree();",
            "\tif (use_softirq)",
            "\t\topen_softirq(RCU_SOFTIRQ, rcu_core_si);",
            "",
            "\t/*",
            "\t * We don't need protection against CPU-hotplug here because",
            "\t * this is called early in boot, before either interrupts",
            "\t * or the scheduler are operational.",
            "\t */",
            "\tpm_notifier(rcu_pm_notify, 0);",
            "\tWARN_ON(num_online_cpus() > 1); // Only one CPU this early in boot.",
            "\trcutree_prepare_cpu(cpu);",
            "\trcu_cpu_starting(cpu);",
            "\trcutree_online_cpu(cpu);",
            "",
            "\t/* Create workqueue for Tree SRCU and for expedited GPs. */",
            "\trcu_gp_wq = alloc_workqueue(\"rcu_gp\", WQ_MEM_RECLAIM, 0);",
            "\tWARN_ON(!rcu_gp_wq);",
            "\trcu_alloc_par_gp_wq();",
            "",
            "\t/* Fill in default value for rcutree.qovld boot parameter. */",
            "\t/* -After- the rcu_node ->lock fields are initialized! */",
            "\tif (qovld < 0)",
            "\t\tqovld_calc = DEFAULT_RCU_QOVLD_MULT * qhimark;",
            "\telse",
            "\t\tqovld_calc = qovld;",
            "",
            "\t// Kick-start in case any polled grace periods started early.",
            "\t(void)start_poll_synchronize_rcu_expedited();",
            "",
            "\trcu_test_sync_prims();",
            "}"
          ],
          "function_name": "rcu_dump_rcu_node_tree, kfree_rcu_batch_init, rcu_init",
          "description": "执行RCU节点树的调试输出，初始化内存回收批量机制，完成RCU子系统的整体初始化流程，包括资源分配、事件注册及核心组件启动。",
          "similarity": 0.6153636574745178
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 658,
          "end_line": 845,
          "content": [
            "int rcu_needs_cpu(void)",
            "{",
            "\treturn !rcu_segcblist_empty(&this_cpu_ptr(&rcu_data)->cblist) &&",
            "\t\t!rcu_rdp_is_offloaded(this_cpu_ptr(&rcu_data));",
            "}",
            "static void rcu_disable_urgency_upon_qs(struct rcu_data *rdp)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rdp->mynode);",
            "\tWRITE_ONCE(rdp->rcu_urgent_qs, false);",
            "\tWRITE_ONCE(rdp->rcu_need_heavy_qs, false);",
            "\tif (tick_nohz_full_cpu(rdp->cpu) && rdp->rcu_forced_tick) {",
            "\t\ttick_dep_clear_cpu(rdp->cpu, TICK_DEP_BIT_RCU);",
            "\t\tWRITE_ONCE(rdp->rcu_forced_tick, false);",
            "\t}",
            "}",
            "notrace bool rcu_is_watching(void)",
            "{",
            "\tbool ret;",
            "",
            "\tpreempt_disable_notrace();",
            "\tret = !rcu_dynticks_curr_cpu_in_eqs();",
            "\tpreempt_enable_notrace();",
            "\treturn ret;",
            "}",
            "void rcu_request_urgent_qs_task(struct task_struct *t)",
            "{",
            "\tint cpu;",
            "",
            "\tbarrier();",
            "\tcpu = task_cpu(t);",
            "\tif (!task_curr(t))",
            "\t\treturn; /* This task is not running on that CPU. */",
            "\tsmp_store_release(per_cpu_ptr(&rcu_data.rcu_urgent_qs, cpu), true);",
            "}",
            "static void rcu_gpnum_ovf(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (ULONG_CMP_LT(rcu_seq_current(&rdp->gp_seq) + ULONG_MAX / 4,",
            "\t\t\t rnp->gp_seq))",
            "\t\tWRITE_ONCE(rdp->gpwrap, true);",
            "\tif (ULONG_CMP_LT(rdp->rcu_iw_gp_seq + ULONG_MAX / 4, rnp->gp_seq))",
            "\t\trdp->rcu_iw_gp_seq = rnp->gp_seq + ULONG_MAX / 4;",
            "}",
            "static int dyntick_save_progress_counter(struct rcu_data *rdp)",
            "{",
            "\trdp->dynticks_snap = rcu_dynticks_snap(rdp->cpu);",
            "\tif (rcu_dynticks_in_eqs(rdp->dynticks_snap)) {",
            "\t\ttrace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS(\"dti\"));",
            "\t\trcu_gpnum_ovf(rdp->mynode, rdp);",
            "\t\treturn 1;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int rcu_implicit_dynticks_qs(struct rcu_data *rdp)",
            "{",
            "\tunsigned long jtsq;",
            "\tint ret = 0;",
            "\tstruct rcu_node *rnp = rdp->mynode;",
            "",
            "\t/*",
            "\t * If the CPU passed through or entered a dynticks idle phase with",
            "\t * no active irq/NMI handlers, then we can safely pretend that the CPU",
            "\t * already acknowledged the request to pass through a quiescent",
            "\t * state.  Either way, that CPU cannot possibly be in an RCU",
            "\t * read-side critical section that started before the beginning",
            "\t * of the current RCU grace period.",
            "\t */",
            "\tif (rcu_dynticks_in_eqs_since(rdp, rdp->dynticks_snap)) {",
            "\t\ttrace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS(\"dti\"));",
            "\t\trcu_gpnum_ovf(rnp, rdp);",
            "\t\treturn 1;",
            "\t}",
            "",
            "\t/*",
            "\t * Complain if a CPU that is considered to be offline from RCU's",
            "\t * perspective has not yet reported a quiescent state.  After all,",
            "\t * the offline CPU should have reported a quiescent state during",
            "\t * the CPU-offline process, or, failing that, by rcu_gp_init()",
            "\t * if it ran concurrently with either the CPU going offline or the",
            "\t * last task on a leaf rcu_node structure exiting its RCU read-side",
            "\t * critical section while all CPUs corresponding to that structure",
            "\t * are offline.  This added warning detects bugs in any of these",
            "\t * code paths.",
            "\t *",
            "\t * The rcu_node structure's ->lock is held here, which excludes",
            "\t * the relevant portions the CPU-hotplug code, the grace-period",
            "\t * initialization code, and the rcu_read_unlock() code paths.",
            "\t *",
            "\t * For more detail, please refer to the \"Hotplug CPU\" section",
            "\t * of RCU's Requirements documentation.",
            "\t */",
            "\tif (WARN_ON_ONCE(!rcu_rdp_cpu_online(rdp))) {",
            "\t\tstruct rcu_node *rnp1;",
            "",
            "\t\tpr_info(\"%s: grp: %d-%d level: %d ->gp_seq %ld ->completedqs %ld\\n\",",
            "\t\t\t__func__, rnp->grplo, rnp->grphi, rnp->level,",
            "\t\t\t(long)rnp->gp_seq, (long)rnp->completedqs);",
            "\t\tfor (rnp1 = rnp; rnp1; rnp1 = rnp1->parent)",
            "\t\t\tpr_info(\"%s: %d:%d ->qsmask %#lx ->qsmaskinit %#lx ->qsmaskinitnext %#lx ->rcu_gp_init_mask %#lx\\n\",",
            "\t\t\t\t__func__, rnp1->grplo, rnp1->grphi, rnp1->qsmask, rnp1->qsmaskinit, rnp1->qsmaskinitnext, rnp1->rcu_gp_init_mask);",
            "\t\tpr_info(\"%s %d: %c online: %ld(%d) offline: %ld(%d)\\n\",",
            "\t\t\t__func__, rdp->cpu, \".o\"[rcu_rdp_cpu_online(rdp)],",
            "\t\t\t(long)rdp->rcu_onl_gp_seq, rdp->rcu_onl_gp_flags,",
            "\t\t\t(long)rdp->rcu_ofl_gp_seq, rdp->rcu_ofl_gp_flags);",
            "\t\treturn 1; /* Break things loose after complaining. */",
            "\t}",
            "",
            "\t/*",
            "\t * A CPU running for an extended time within the kernel can",
            "\t * delay RCU grace periods: (1) At age jiffies_to_sched_qs,",
            "\t * set .rcu_urgent_qs, (2) At age 2*jiffies_to_sched_qs, set",
            "\t * both .rcu_need_heavy_qs and .rcu_urgent_qs.  Note that the",
            "\t * unsynchronized assignments to the per-CPU rcu_need_heavy_qs",
            "\t * variable are safe because the assignments are repeated if this",
            "\t * CPU failed to pass through a quiescent state.  This code",
            "\t * also checks .jiffies_resched in case jiffies_to_sched_qs",
            "\t * is set way high.",
            "\t */",
            "\tjtsq = READ_ONCE(jiffies_to_sched_qs);",
            "\tif (!READ_ONCE(rdp->rcu_need_heavy_qs) &&",
            "\t    (time_after(jiffies, rcu_state.gp_start + jtsq * 2) ||",
            "\t     time_after(jiffies, rcu_state.jiffies_resched) ||",
            "\t     rcu_state.cbovld)) {",
            "\t\tWRITE_ONCE(rdp->rcu_need_heavy_qs, true);",
            "\t\t/* Store rcu_need_heavy_qs before rcu_urgent_qs. */",
            "\t\tsmp_store_release(&rdp->rcu_urgent_qs, true);",
            "\t} else if (time_after(jiffies, rcu_state.gp_start + jtsq)) {",
            "\t\tWRITE_ONCE(rdp->rcu_urgent_qs, true);",
            "\t}",
            "",
            "\t/*",
            "\t * NO_HZ_FULL CPUs can run in-kernel without rcu_sched_clock_irq!",
            "\t * The above code handles this, but only for straight cond_resched().",
            "\t * And some in-kernel loops check need_resched() before calling",
            "\t * cond_resched(), which defeats the above code for CPUs that are",
            "\t * running in-kernel with scheduling-clock interrupts disabled.",
            "\t * So hit them over the head with the resched_cpu() hammer!",
            "\t */",
            "\tif (tick_nohz_full_cpu(rdp->cpu) &&",
            "\t    (time_after(jiffies, READ_ONCE(rdp->last_fqs_resched) + jtsq * 3) ||",
            "\t     rcu_state.cbovld)) {",
            "\t\tWRITE_ONCE(rdp->rcu_urgent_qs, true);",
            "\t\tWRITE_ONCE(rdp->last_fqs_resched, jiffies);",
            "\t\tret = -1;",
            "\t}",
            "",
            "\t/*",
            "\t * If more than halfway to RCU CPU stall-warning time, invoke",
            "\t * resched_cpu() more frequently to try to loosen things up a bit.",
            "\t * Also check to see if the CPU is getting hammered with interrupts,",
            "\t * but only once per grace period, just to keep the IPIs down to",
            "\t * a dull roar.",
            "\t */",
            "\tif (time_after(jiffies, rcu_state.jiffies_resched)) {",
            "\t\tif (time_after(jiffies,",
            "\t\t\t       READ_ONCE(rdp->last_fqs_resched) + jtsq)) {",
            "\t\t\tWRITE_ONCE(rdp->last_fqs_resched, jiffies);",
            "\t\t\tret = -1;",
            "\t\t}",
            "\t\tif (IS_ENABLED(CONFIG_IRQ_WORK) &&",
            "\t\t    !rdp->rcu_iw_pending && rdp->rcu_iw_gp_seq != rnp->gp_seq &&",
            "\t\t    (rnp->ffmask & rdp->grpmask)) {",
            "\t\t\trdp->rcu_iw_pending = true;",
            "\t\t\trdp->rcu_iw_gp_seq = rnp->gp_seq;",
            "\t\t\tirq_work_queue_on(&rdp->rcu_iw, rdp->cpu);",
            "\t\t}",
            "",
            "\t\tif (rcu_cpu_stall_cputime && rdp->snap_record.gp_seq != rdp->gp_seq) {",
            "\t\t\tint cpu = rdp->cpu;",
            "\t\t\tstruct rcu_snap_record *rsrp;",
            "\t\t\tstruct kernel_cpustat *kcsp;",
            "",
            "\t\t\tkcsp = &kcpustat_cpu(cpu);",
            "",
            "\t\t\trsrp = &rdp->snap_record;",
            "\t\t\trsrp->cputime_irq     = kcpustat_field(kcsp, CPUTIME_IRQ, cpu);",
            "\t\t\trsrp->cputime_softirq = kcpustat_field(kcsp, CPUTIME_SOFTIRQ, cpu);",
            "\t\t\trsrp->cputime_system  = kcpustat_field(kcsp, CPUTIME_SYSTEM, cpu);",
            "\t\t\trsrp->nr_hardirqs = kstat_cpu_irqs_sum(cpu) + arch_irq_stat_cpu(cpu);",
            "\t\t\trsrp->nr_softirqs = kstat_cpu_softirqs_sum(cpu);",
            "\t\t\trsrp->nr_csw = nr_context_switches_cpu(cpu);",
            "\t\t\trsrp->jiffies = jiffies;",
            "\t\t\trsrp->gp_seq = rdp->gp_seq;",
            "\t\t}",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "rcu_needs_cpu, rcu_disable_urgency_upon_qs, rcu_is_watching, rcu_request_urgent_qs_task, rcu_gpnum_ovf, dyntick_save_progress_counter, rcu_implicit_dynticks_qs",
          "description": "处理RCU紧迫性需求判定和隐式动态tick quiescent状态检测，通过时间阈值触发CPU唤醒以避免RCU阻塞。",
          "similarity": 0.602867841720581
        },
        {
          "chunk_id": 26,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4752,
          "end_line": 4863,
          "content": [
            "static int rcu_pm_notify(struct notifier_block *self,",
            "\t\t\t unsigned long action, void *hcpu)",
            "{",
            "\tswitch (action) {",
            "\tcase PM_HIBERNATION_PREPARE:",
            "\tcase PM_SUSPEND_PREPARE:",
            "\t\trcu_async_hurry();",
            "\t\trcu_expedite_gp();",
            "\t\tbreak;",
            "\tcase PM_POST_HIBERNATION:",
            "\tcase PM_POST_SUSPEND:",
            "\t\trcu_unexpedite_gp();",
            "\t\trcu_async_relax();",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "\treturn NOTIFY_OK;",
            "}",
            "static void __init rcu_start_exp_gp_kworkers(void)",
            "{",
            "\tconst char *par_gp_kworker_name = \"rcu_exp_par_gp_kthread_worker\";",
            "\tconst char *gp_kworker_name = \"rcu_exp_gp_kthread_worker\";",
            "\tstruct sched_param param = { .sched_priority = kthread_prio };",
            "",
            "\trcu_exp_gp_kworker = kthread_create_worker(0, gp_kworker_name);",
            "\tif (IS_ERR_OR_NULL(rcu_exp_gp_kworker)) {",
            "\t\tpr_err(\"Failed to create %s!\\n\", gp_kworker_name);",
            "\t\trcu_exp_gp_kworker = NULL;",
            "\t\treturn;",
            "\t}",
            "",
            "\trcu_exp_par_gp_kworker = kthread_create_worker(0, par_gp_kworker_name);",
            "\tif (IS_ERR_OR_NULL(rcu_exp_par_gp_kworker)) {",
            "\t\tpr_err(\"Failed to create %s!\\n\", par_gp_kworker_name);",
            "\t\trcu_exp_par_gp_kworker = NULL;",
            "\t\tkthread_destroy_worker(rcu_exp_gp_kworker);",
            "\t\trcu_exp_gp_kworker = NULL;",
            "\t\treturn;",
            "\t}",
            "",
            "\tsched_setscheduler_nocheck(rcu_exp_gp_kworker->task, SCHED_FIFO, &param);",
            "\tsched_setscheduler_nocheck(rcu_exp_par_gp_kworker->task, SCHED_FIFO,",
            "\t\t\t\t   &param);",
            "}",
            "static inline void rcu_alloc_par_gp_wq(void)",
            "{",
            "}",
            "static void __init rcu_start_exp_gp_kworkers(void)",
            "{",
            "}",
            "static inline void rcu_alloc_par_gp_wq(void)",
            "{",
            "\trcu_par_gp_wq = alloc_workqueue(\"rcu_par_gp\", WQ_MEM_RECLAIM, 0);",
            "\tWARN_ON(!rcu_par_gp_wq);",
            "}",
            "static int __init rcu_spawn_gp_kthread(void)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "\tstruct sched_param sp;",
            "\tstruct task_struct *t;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\trcu_scheduler_fully_active = 1;",
            "\tt = kthread_create(rcu_gp_kthread, NULL, \"%s\", rcu_state.name);",
            "\tif (WARN_ONCE(IS_ERR(t), \"%s: Could not start grace-period kthread, OOM is now expected behavior\\n\", __func__))",
            "\t\treturn 0;",
            "\tif (kthread_prio) {",
            "\t\tsp.sched_priority = kthread_prio;",
            "\t\tsched_setscheduler_nocheck(t, SCHED_FIFO, &sp);",
            "\t}",
            "\trnp = rcu_get_root();",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);",
            "\t// Reset .gp_activity and .gp_req_activity before setting .gp_kthread.",
            "\tsmp_store_release(&rcu_state.gp_kthread, t);  /* ^^^ */",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\twake_up_process(t);",
            "\t/* This is a pre-SMP initcall, we expect a single CPU */",
            "\tWARN_ON(num_online_cpus() > 1);",
            "\t/*",
            "\t * Those kthreads couldn't be created on rcu_init() -> rcutree_prepare_cpu()",
            "\t * due to rcu_scheduler_fully_active.",
            "\t */",
            "\trcu_spawn_cpu_nocb_kthread(smp_processor_id());",
            "\trcu_spawn_one_boost_kthread(rdp->mynode);",
            "\trcu_spawn_core_kthreads();",
            "\t/* Create kthread worker for expedited GPs */",
            "\trcu_start_exp_gp_kworkers();",
            "\treturn 0;",
            "}",
            "void rcu_scheduler_starting(void)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tWARN_ON(num_online_cpus() != 1);",
            "\tWARN_ON(nr_context_switches() > 0);",
            "\trcu_test_sync_prims();",
            "",
            "\t// Fix up the ->gp_seq counters.",
            "\tlocal_irq_save(flags);",
            "\trcu_for_each_node_breadth_first(rnp)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq = rcu_state.gp_seq;",
            "\tlocal_irq_restore(flags);",
            "",
            "\t// Switch out of early boot mode.",
            "\trcu_scheduler_active = RCU_SCHEDULER_INIT;",
            "\trcu_test_sync_prims();",
            "}"
          ],
          "function_name": "rcu_pm_notify, rcu_start_exp_gp_kworkers, rcu_alloc_par_gp_wq, rcu_start_exp_gp_kworkers, rcu_alloc_par_gp_wq, rcu_spawn_gp_kthread, rcu_scheduler_starting",
          "description": "管理系统休眠唤醒时的RCU急迫性切换，创建并配置用于处理急迫GRACE周期的内核线程工作者，同时初始化相关资源和调度参数。",
          "similarity": 0.6027073264122009
        }
      ]
    },
    {
      "source_file": "kernel/time/tick-common.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:49:09\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `time\\tick-common.c`\n\n---\n\n# `time/tick-common.c` 技术文档\n\n## 1. 文件概述\n\n`tick-common.c` 是 Linux 内核时间子系统的核心组件之一，负责管理周期性时钟滴答（tick）事件的基础逻辑。该文件实现了与 CPU 相关的 tick 设备（`tick_device`）的初始化、切换、周期性处理以及与时间保持（timekeeping）和高分辨率定时器（high-resolution timers）的协同机制。它为周期性模式（periodic mode）和单次触发模式（oneshot mode）提供了通用支持，并与 NO_HZ（动态滴答）和广播定时器（tick broadcast）机制紧密集成。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`tick_cpu_device`**：每个 CPU 上的 `tick_device` 实例，用于管理该 CPU 的时钟事件设备。\n- **`tick_next_period`**：全局变量，记录下一次周期性 tick 的绝对时间（`ktime_t`），由负责时间更新的 CPU 维护。\n- **`tick_do_timer_cpu`**：指示当前负责调用 `do_timer()` 进行全局时间更新的 CPU 编号。特殊值 `TICK_DO_TIMER_BOOT` 表示尚未分配，`TICK_DO_TIMER_NONE` 表示无 CPU 负责（用于 NO_HZ 交接）。\n- **`tick_do_timer_boot_cpu`**（仅 `CONFIG_NO_HZ_FULL`）：记录启动时临时持有 `tick_do_timer_cpu` 的 CPU，以便后续由合适的非 NO_HZ_FULL CPU 接管。\n\n### 主要函数\n\n- **`tick_get_device(int cpu)`**：获取指定 CPU 的 `tick_device` 指针。\n- **`tick_is_oneshot_available(void)`**：检查当前 CPU 是否具备可用的 oneshot 时钟事件设备（考虑 C3 停止状态和广播机制）。\n- **`tick_periodic(int cpu)`**：执行周期性 tick 的核心处理逻辑，包括更新 jiffies、调用 `do_timer()` 和 `update_wall_time()`，以及更新进程统计信息。\n- **`tick_handle_periodic(struct clock_event_device *dev)`**：周期性 tick 的中断事件处理函数，处理周期性或模拟周期性的 oneshot 事件。\n- **`tick_setup_periodic(struct clock_event_device *dev, int broadcast)`**：将时钟事件设备配置为周期性模式（或模拟周期性）。\n- **`tick_setup_device(struct tick_device *td, struct clock_event_device *newdev, int cpu, const struct cpumask *cpumask)`**：初始化或替换 tick 设备，处理 `tick_do_timer_cpu` 的分配和设备模式设置。\n- **`tick_install_replacement(struct clock_event_device *newdev)`**：安装新的时钟事件设备以替换当前设备。\n- **`tick_check_percpu()` / `tick_check_preferred()`**：辅助函数，用于在设备注册时判断新设备是否适合当前 CPU（本地性、功能偏好等）。\n\n## 3. 关键实现\n\n### Tick 责任 CPU 管理 (`tick_do_timer_cpu`)\n\n- **防“惊群”效应**：仅允许一个 CPU（`tick_do_timer_cpu`）执行全局时间更新（`do_timer()` 和 `update_wall_time()`），避免多 CPU 竞争 jiffies 锁。\n- **NO_HZ 交接机制**：当负责 CPU 进入深度空闲（NO_HZ）时，将其设为 `TICK_DO_TIMER_NONE`，促使下一个活跃 CPU 接管时间更新职责。\n- **启动阶段处理**：初始值为 `TICK_DO_TIMER_BOOT`，首个注册 tick 设备的 CPU 会获得该职责。在 `CONFIG_NO_HZ_FULL` 下，若启动 CPU 是 NO_HZ_FULL 类型，则暂存其 ID，待首个非 NO_HZ_FULL CPU 上线时移交职责。\n\n### 周期性 Tick 处理\n\n- **真实周期性设备**：若设备支持 `CLOCK_EVT_FEAT_PERIODIC` 且未启用广播 oneshot，则直接切换到 `CLOCK_EVT_STATE_PERIODIC` 状态。\n- **模拟周期性（Oneshot 模拟）**：对于仅支持 oneshot 的设备，通过在 `tick_handle_periodic()` 中循环调用 `clockevents_program_event()` 设置下一次事件（间隔 `TICK_NSEC`）来模拟周期性行为。\n- **安全防护**：在模拟周期性循环中，检查 `timekeeping_valid_for_hres()` 以避免因时间子系统未就绪导致的无限循环。\n\n### 设备切换与初始化\n\n- **首次设置**：当 CPU 首次设置 tick 设备时，分配 `tick_do_timer_cpu` 并初始化 `tick_next_period` 为当前时间。\n- **设备替换**：通过 `tick_install_replacement()` 安全地交换新旧设备，并重新配置新设备的工作模式（周期性或 oneshot）。\n- **中断亲和性**：若新设备的 `cpumask` 与目标 CPU 不匹配，且设备有有效 IRQ，则调用 `irq_set_affinity()` 将中断绑定到该 CPU。\n- **广播设备处理**：调用 `tick_device_uses_broadcast()` 检查设备是否为广播占位符，若是则跳过常规配置。\n\n### NO_HZ_FULL 支持\n\n- 通过 `tick_do_timer_boot_cpu` 机制，确保 `tick_do_timer_cpu` 最终由一个非 NO_HZ_FULL（即“housekeeping”）CPU 持有，因为 NO_HZ_FULL CPU 在空闲时会完全关闭 tick，无法可靠执行全局时间更新。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/cpu.h>`：CPU 热插拔通知。\n  - `<linux/hrtimer.h>`：高分辨率定时器接口。\n  - `<linux/interrupt.h>`：中断管理（`irq_set_affinity`）。\n  - `<linux/profile.h>`：性能剖析（`profile_tick`）。\n  - `<linux/sched.h>`：调度器相关（`user_mode`）。\n  - `\"tick-internal.h\"`：tick 子系统内部头文件，包含 `tick_device` 定义、广播接口等。\n- **模块依赖**：\n  - **时间保持子系统**：通过 `do_timer()`、`update_wall_time()` 与 `kernel/time/timekeeping.c` 交互。\n  - **时钟事件设备层**：通过 `struct clock_event_device` 与架构相关代码（如 `arch/x86/kernel/apic/`）交互。\n  - **NO_HZ 子系统**：与 `kernel/time/tick-sched.c` 协同实现动态滴答。\n  - **Tick 广播机制**：通过 `tick_broadcast_oneshot_available()`、`tick_device_uses_broadcast()` 与 `kernel/time/tick-broadcast.c` 交互。\n  - **高分辨率定时器**：通过 `hrtimer_run_queues()` 触发模式切换（周期性 ↔ oneshot）。\n\n## 5. 使用场景\n\n- **系统启动初始化**：在 CPU 启动过程中，为每个 CPU 初始化 `tick_device` 并分配 `tick_do_timer_cpu`。\n- **CPU 热插拔**：当 CPU 上线时，为其设置 tick 设备；下线时，可能触发 `tick_do_timer_cpu` 的交接。\n- **时钟事件设备注册/替换**：当新的时钟事件设备（如 HPET、TSC、APIC Timer）被注册或替换旧设备时，调用 `tick_install_replacement()` 重新配置 tick。\n- **周期性 Tick 中断处理**：在传统周期性模式下，每次时钟中断触发 `tick_handle_periodic()`，执行时间更新和进程统计。\n- **NO_HZ 模式切换**：当系统进入或退出 NO_HZ 空闲状态时，通过设置 `tick_do_timer_cpu = TICK_DO_TIMER_NONE` 触发责任 CPU 的重新选举。\n- **高分辨率定时器启用**：当高分辨率定时器就绪后，tick 设备会从周期性模式切换到 oneshot 模式，由 `tick-sched.c` 管理动态滴答。",
      "similarity": 0.6017187833786011,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/time/tick-common.c",
          "start_line": 582,
          "end_line": 586,
          "content": [
            "void __init tick_init(void)",
            "{",
            "\ttick_broadcast_init();",
            "\ttick_nohz_init();",
            "}"
          ],
          "function_name": "tick_init",
          "description": "初始化广播模式（tick_broadcast_init）和非抢占模式（tick_nohz_init），完成时钟子系统的整体启动配置。",
          "similarity": 0.631101131439209
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/time/tick-common.c",
          "start_line": 406,
          "end_line": 515,
          "content": [
            "void tick_shutdown(unsigned int cpu)",
            "{",
            "\tstruct tick_device *td = &per_cpu(tick_cpu_device, cpu);",
            "\tstruct clock_event_device *dev = td->evtdev;",
            "",
            "\ttd->mode = TICKDEV_MODE_PERIODIC;",
            "\tif (dev) {",
            "\t\t/*",
            "\t\t * Prevent that the clock events layer tries to call",
            "\t\t * the set mode function!",
            "\t\t */",
            "\t\tclockevent_set_state(dev, CLOCK_EVT_STATE_DETACHED);",
            "\t\tclockevents_exchange_device(dev, NULL);",
            "\t\tdev->event_handler = clockevents_handle_noop;",
            "\t\ttd->evtdev = NULL;",
            "\t}",
            "}",
            "void tick_suspend_local(void)",
            "{",
            "\tstruct tick_device *td = this_cpu_ptr(&tick_cpu_device);",
            "",
            "\tclockevents_shutdown(td->evtdev);",
            "}",
            "void tick_resume_local(void)",
            "{",
            "\tstruct tick_device *td = this_cpu_ptr(&tick_cpu_device);",
            "\tbool broadcast = tick_resume_check_broadcast();",
            "",
            "\tclockevents_tick_resume(td->evtdev);",
            "\tif (!broadcast) {",
            "\t\tif (td->mode == TICKDEV_MODE_PERIODIC)",
            "\t\t\ttick_setup_periodic(td->evtdev, 0);",
            "\t\telse",
            "\t\t\ttick_resume_oneshot();",
            "\t}",
            "",
            "\t/*",
            "\t * Ensure that hrtimers are up to date and the clockevents device",
            "\t * is reprogrammed correctly when high resolution timers are",
            "\t * enabled.",
            "\t */",
            "\thrtimers_resume_local();",
            "}",
            "void tick_suspend(void)",
            "{",
            "\ttick_suspend_local();",
            "\ttick_suspend_broadcast();",
            "}",
            "void tick_resume(void)",
            "{",
            "\ttick_resume_broadcast();",
            "\ttick_resume_local();",
            "}",
            "void tick_freeze(void)",
            "{",
            "\traw_spin_lock(&tick_freeze_lock);",
            "",
            "\ttick_freeze_depth++;",
            "\tif (tick_freeze_depth == num_online_cpus()) {",
            "\t\ttrace_suspend_resume(TPS(\"timekeeping_freeze\"),",
            "\t\t\t\t     smp_processor_id(), true);",
            "\t\t/*",
            "\t\t * All other CPUs have their interrupts disabled and are",
            "\t\t * suspended to idle. Other tasks have been frozen so there",
            "\t\t * is no scheduling happening. This means that there is no",
            "\t\t * concurrency in the system at this point. Therefore it is",
            "\t\t * okay to acquire a sleeping lock on PREEMPT_RT, such as a",
            "\t\t * spinlock, because the lock cannot be held by other CPUs",
            "\t\t * or threads and acquiring it cannot block.",
            "\t\t *",
            "\t\t * Inform lockdep about the situation.",
            "\t\t */",
            "\t\tlock_map_acquire_try(&tick_freeze_map);",
            "\t\tsystem_state = SYSTEM_SUSPEND;",
            "\t\tsched_clock_suspend();",
            "\t\ttimekeeping_suspend();",
            "\t\tlock_map_release(&tick_freeze_map);",
            "\t} else {",
            "\t\ttick_suspend_local();",
            "\t}",
            "",
            "\traw_spin_unlock(&tick_freeze_lock);",
            "}",
            "void tick_unfreeze(void)",
            "{",
            "\traw_spin_lock(&tick_freeze_lock);",
            "",
            "\tif (tick_freeze_depth == num_online_cpus()) {",
            "\t\t/*",
            "\t\t * Similar to tick_freeze(). On resumption the first CPU may",
            "\t\t * acquire uncontended sleeping locks while other CPUs block on",
            "\t\t * tick_freeze_lock.",
            "\t\t */",
            "\t\tlock_map_acquire_try(&tick_freeze_map);",
            "\t\ttimekeeping_resume();",
            "\t\tsched_clock_resume();",
            "\t\tlock_map_release(&tick_freeze_map);",
            "",
            "\t\tsystem_state = SYSTEM_RUNNING;",
            "\t\ttrace_suspend_resume(TPS(\"timekeeping_freeze\"),",
            "\t\t\t\t     smp_processor_id(), false);",
            "\t} else {",
            "\t\ttouch_softlockup_watchdog();",
            "\t\ttick_resume_local();",
            "\t}",
            "",
            "\ttick_freeze_depth--;",
            "",
            "\traw_spin_unlock(&tick_freeze_lock);",
            "}"
          ],
          "function_name": "tick_shutdown, tick_suspend_local, tick_resume_local, tick_suspend, tick_resume, tick_freeze, tick_unfreeze",
          "description": "实现系统挂起/恢复（tick_suspend/resume）和冻结/解冻（tick_freeze/unfreeze）逻辑，包含中断屏蔽、状态同步和时钟源恢复等电源管理操作。",
          "similarity": 0.6027129888534546
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/time/tick-common.c",
          "start_line": 71,
          "end_line": 244,
          "content": [
            "int tick_is_oneshot_available(void)",
            "{",
            "\tstruct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);",
            "",
            "\tif (!dev || !(dev->features & CLOCK_EVT_FEAT_ONESHOT))",
            "\t\treturn 0;",
            "\tif (!(dev->features & CLOCK_EVT_FEAT_C3STOP))",
            "\t\treturn 1;",
            "\treturn tick_broadcast_oneshot_available();",
            "}",
            "static void tick_periodic(int cpu)",
            "{",
            "\tif (tick_do_timer_cpu == cpu) {",
            "\t\traw_spin_lock(&jiffies_lock);",
            "\t\twrite_seqcount_begin(&jiffies_seq);",
            "",
            "\t\t/* Keep track of the next tick event */",
            "\t\ttick_next_period = ktime_add_ns(tick_next_period, TICK_NSEC);",
            "",
            "\t\tdo_timer(1);",
            "\t\twrite_seqcount_end(&jiffies_seq);",
            "\t\traw_spin_unlock(&jiffies_lock);",
            "\t\tupdate_wall_time();",
            "\t}",
            "",
            "\tupdate_process_times(user_mode(get_irq_regs()));",
            "\tprofile_tick(CPU_PROFILING);",
            "}",
            "void tick_handle_periodic(struct clock_event_device *dev)",
            "{",
            "\tint cpu = smp_processor_id();",
            "\tktime_t next = dev->next_event;",
            "",
            "\ttick_periodic(cpu);",
            "",
            "#if defined(CONFIG_HIGH_RES_TIMERS) || defined(CONFIG_NO_HZ_COMMON)",
            "\t/*",
            "\t * The cpu might have transitioned to HIGHRES or NOHZ mode via",
            "\t * update_process_times() -> run_local_timers() ->",
            "\t * hrtimer_run_queues().",
            "\t */",
            "\tif (dev->event_handler != tick_handle_periodic)",
            "\t\treturn;",
            "#endif",
            "",
            "\tif (!clockevent_state_oneshot(dev))",
            "\t\treturn;",
            "\tfor (;;) {",
            "\t\t/*",
            "\t\t * Setup the next period for devices, which do not have",
            "\t\t * periodic mode:",
            "\t\t */",
            "\t\tnext = ktime_add_ns(next, TICK_NSEC);",
            "",
            "\t\tif (!clockevents_program_event(dev, next, false))",
            "\t\t\treturn;",
            "\t\t/*",
            "\t\t * Have to be careful here. If we're in oneshot mode,",
            "\t\t * before we call tick_periodic() in a loop, we need",
            "\t\t * to be sure we're using a real hardware clocksource.",
            "\t\t * Otherwise we could get trapped in an infinite",
            "\t\t * loop, as the tick_periodic() increments jiffies,",
            "\t\t * which then will increment time, possibly causing",
            "\t\t * the loop to trigger again and again.",
            "\t\t */",
            "\t\tif (timekeeping_valid_for_hres())",
            "\t\t\ttick_periodic(cpu);",
            "\t}",
            "}",
            "void tick_setup_periodic(struct clock_event_device *dev, int broadcast)",
            "{",
            "\ttick_set_periodic_handler(dev, broadcast);",
            "",
            "\t/* Broadcast setup ? */",
            "\tif (!tick_device_is_functional(dev))",
            "\t\treturn;",
            "",
            "\tif ((dev->features & CLOCK_EVT_FEAT_PERIODIC) &&",
            "\t    !tick_broadcast_oneshot_active()) {",
            "\t\tclockevents_switch_state(dev, CLOCK_EVT_STATE_PERIODIC);",
            "\t} else {",
            "\t\tunsigned int seq;",
            "\t\tktime_t next;",
            "",
            "\t\tdo {",
            "\t\t\tseq = read_seqcount_begin(&jiffies_seq);",
            "\t\t\tnext = tick_next_period;",
            "\t\t} while (read_seqcount_retry(&jiffies_seq, seq));",
            "",
            "\t\tclockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT);",
            "",
            "\t\tfor (;;) {",
            "\t\t\tif (!clockevents_program_event(dev, next, false))",
            "\t\t\t\treturn;",
            "\t\t\tnext = ktime_add_ns(next, TICK_NSEC);",
            "\t\t}",
            "\t}",
            "}",
            "static void tick_setup_device(struct tick_device *td,",
            "\t\t\t      struct clock_event_device *newdev, int cpu,",
            "\t\t\t      const struct cpumask *cpumask)",
            "{",
            "\tvoid (*handler)(struct clock_event_device *) = NULL;",
            "\tktime_t next_event = 0;",
            "",
            "\t/*",
            "\t * First device setup ?",
            "\t */",
            "\tif (!td->evtdev) {",
            "\t\t/*",
            "\t\t * If no cpu took the do_timer update, assign it to",
            "\t\t * this cpu:",
            "\t\t */",
            "\t\tif (tick_do_timer_cpu == TICK_DO_TIMER_BOOT) {",
            "\t\t\ttick_do_timer_cpu = cpu;",
            "\t\t\ttick_next_period = ktime_get();",
            "#ifdef CONFIG_NO_HZ_FULL",
            "\t\t\t/*",
            "\t\t\t * The boot CPU may be nohz_full, in which case the",
            "\t\t\t * first housekeeping secondary will take do_timer()",
            "\t\t\t * from it.",
            "\t\t\t */",
            "\t\t\tif (tick_nohz_full_cpu(cpu))",
            "\t\t\t\ttick_do_timer_boot_cpu = cpu;",
            "",
            "\t\t} else if (tick_do_timer_boot_cpu != -1 && !tick_nohz_full_cpu(cpu)) {",
            "\t\t\ttick_do_timer_boot_cpu = -1;",
            "\t\t\t/*",
            "\t\t\t * The boot CPU will stay in periodic (NOHZ disabled)",
            "\t\t\t * mode until clocksource_done_booting() called after",
            "\t\t\t * smp_init() selects a high resolution clocksource and",
            "\t\t\t * timekeeping_notify() kicks the NOHZ stuff alive.",
            "\t\t\t *",
            "\t\t\t * So this WRITE_ONCE can only race with the READ_ONCE",
            "\t\t\t * check in tick_periodic() but this race is harmless.",
            "\t\t\t */",
            "\t\t\tWRITE_ONCE(tick_do_timer_cpu, cpu);",
            "#endif",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Startup in periodic mode first.",
            "\t\t */",
            "\t\ttd->mode = TICKDEV_MODE_PERIODIC;",
            "\t} else {",
            "\t\thandler = td->evtdev->event_handler;",
            "\t\tnext_event = td->evtdev->next_event;",
            "\t\ttd->evtdev->event_handler = clockevents_handle_noop;",
            "\t}",
            "",
            "\ttd->evtdev = newdev;",
            "",
            "\t/*",
            "\t * When the device is not per cpu, pin the interrupt to the",
            "\t * current cpu:",
            "\t */",
            "\tif (!cpumask_equal(newdev->cpumask, cpumask))",
            "\t\tirq_set_affinity(newdev->irq, cpumask);",
            "",
            "\t/*",
            "\t * When global broadcasting is active, check if the current",
            "\t * device is registered as a placeholder for broadcast mode.",
            "\t * This allows us to handle this x86 misfeature in a generic",
            "\t * way. This function also returns !=0 when we keep the",
            "\t * current active broadcast state for this CPU.",
            "\t */",
            "\tif (tick_device_uses_broadcast(newdev, cpu))",
            "\t\treturn;",
            "",
            "\tif (td->mode == TICKDEV_MODE_PERIODIC)",
            "\t\ttick_setup_periodic(newdev, 0);",
            "\telse",
            "\t\ttick_setup_oneshot(newdev, handler, next_event);",
            "}"
          ],
          "function_name": "tick_is_oneshot_available, tick_periodic, tick_handle_periodic, tick_setup_periodic, tick_setup_device",
          "description": "实现tick_is_oneshot_available检查单次模式支持，tick_periodic处理周期性tick逻辑，tick_handle_periodic处理定时器事件循环，tick_setup_periodic设置周期模式，tick_setup_device配置新设备并调整中断亲和力。",
          "similarity": 0.5903300046920776
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/time/tick-common.c",
          "start_line": 262,
          "end_line": 364,
          "content": [
            "void tick_install_replacement(struct clock_event_device *newdev)",
            "{",
            "\tstruct tick_device *td = this_cpu_ptr(&tick_cpu_device);",
            "\tint cpu = smp_processor_id();",
            "",
            "\tclockevents_exchange_device(td->evtdev, newdev);",
            "\ttick_setup_device(td, newdev, cpu, cpumask_of(cpu));",
            "\tif (newdev->features & CLOCK_EVT_FEAT_ONESHOT)",
            "\t\ttick_oneshot_notify();",
            "}",
            "static bool tick_check_percpu(struct clock_event_device *curdev,",
            "\t\t\t      struct clock_event_device *newdev, int cpu)",
            "{",
            "\tif (!cpumask_test_cpu(cpu, newdev->cpumask))",
            "\t\treturn false;",
            "\tif (cpumask_equal(newdev->cpumask, cpumask_of(cpu)))",
            "\t\treturn true;",
            "\t/* Check if irq affinity can be set */",
            "\tif (newdev->irq >= 0 && !irq_can_set_affinity(newdev->irq))",
            "\t\treturn false;",
            "\t/* Prefer an existing cpu local device */",
            "\tif (curdev && cpumask_equal(curdev->cpumask, cpumask_of(cpu)))",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "static bool tick_check_preferred(struct clock_event_device *curdev,",
            "\t\t\t\t struct clock_event_device *newdev)",
            "{",
            "\t/* Prefer oneshot capable device */",
            "\tif (!(newdev->features & CLOCK_EVT_FEAT_ONESHOT)) {",
            "\t\tif (curdev && (curdev->features & CLOCK_EVT_FEAT_ONESHOT))",
            "\t\t\treturn false;",
            "\t\tif (tick_oneshot_mode_active())",
            "\t\t\treturn false;",
            "\t}",
            "",
            "\t/*",
            "\t * Use the higher rated one, but prefer a CPU local device with a lower",
            "\t * rating than a non-CPU local device",
            "\t */",
            "\treturn !curdev ||",
            "\t\tnewdev->rating > curdev->rating ||",
            "\t       !cpumask_equal(curdev->cpumask, newdev->cpumask);",
            "}",
            "bool tick_check_replacement(struct clock_event_device *curdev,",
            "\t\t\t    struct clock_event_device *newdev)",
            "{",
            "\tif (!tick_check_percpu(curdev, newdev, smp_processor_id()))",
            "\t\treturn false;",
            "",
            "\treturn tick_check_preferred(curdev, newdev);",
            "}",
            "void tick_check_new_device(struct clock_event_device *newdev)",
            "{",
            "\tstruct clock_event_device *curdev;",
            "\tstruct tick_device *td;",
            "\tint cpu;",
            "",
            "\tcpu = smp_processor_id();",
            "\ttd = &per_cpu(tick_cpu_device, cpu);",
            "\tcurdev = td->evtdev;",
            "",
            "\tif (!tick_check_replacement(curdev, newdev))",
            "\t\tgoto out_bc;",
            "",
            "\tif (!try_module_get(newdev->owner))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Replace the eventually existing device by the new",
            "\t * device. If the current device is the broadcast device, do",
            "\t * not give it back to the clockevents layer !",
            "\t */",
            "\tif (tick_is_broadcast_device(curdev)) {",
            "\t\tclockevents_shutdown(curdev);",
            "\t\tcurdev = NULL;",
            "\t}",
            "\tclockevents_exchange_device(curdev, newdev);",
            "\ttick_setup_device(td, newdev, cpu, cpumask_of(cpu));",
            "\tif (newdev->features & CLOCK_EVT_FEAT_ONESHOT)",
            "\t\ttick_oneshot_notify();",
            "\treturn;",
            "",
            "out_bc:",
            "\t/*",
            "\t * Can the new device be used as a broadcast device ?",
            "\t */",
            "\ttick_install_broadcast_device(newdev, cpu);",
            "}",
            "int tick_broadcast_oneshot_control(enum tick_broadcast_state state)",
            "{",
            "\tstruct tick_device *td = this_cpu_ptr(&tick_cpu_device);",
            "",
            "\tif (!(td->evtdev->features & CLOCK_EVT_FEAT_C3STOP))",
            "\t\treturn 0;",
            "",
            "\treturn __tick_broadcast_oneshot_control(state);",
            "}",
            "void tick_handover_do_timer(void)",
            "{",
            "\tif (tick_do_timer_cpu == smp_processor_id())",
            "\t\ttick_do_timer_cpu = cpumask_first(cpu_online_mask);",
            "}"
          ],
          "function_name": "tick_install_replacement, tick_check_percpu, tick_check_preferred, tick_check_replacement, tick_check_new_device, tick_broadcast_oneshot_control, tick_handover_do_timer",
          "description": "提供设备替换逻辑（tick_install_replacement），设备选择策略（tick_check_*系列）用于广播模式控制和tick_do_timer_cpu的手动转移。",
          "similarity": 0.5526601672172546
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/time/tick-common.c",
          "start_line": 1,
          "end_line": 70,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * This file contains the base functions to manage periodic tick",
            " * related events.",
            " *",
            " * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>",
            " * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar",
            " * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner",
            " */",
            "#include <linux/cpu.h>",
            "#include <linux/err.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/nmi.h>",
            "#include <linux/percpu.h>",
            "#include <linux/profile.h>",
            "#include <linux/sched.h>",
            "#include <linux/module.h>",
            "#include <trace/events/power.h>",
            "",
            "#include <asm/irq_regs.h>",
            "",
            "#include \"tick-internal.h\"",
            "",
            "/*",
            " * Tick devices",
            " */",
            "DEFINE_PER_CPU(struct tick_device, tick_cpu_device);",
            "/*",
            " * Tick next event: keeps track of the tick time. It's updated by the",
            " * CPU which handles the tick and protected by jiffies_lock. There is",
            " * no requirement to write hold the jiffies seqcount for it.",
            " */",
            "ktime_t tick_next_period;",
            "",
            "/*",
            " * tick_do_timer_cpu is a timer core internal variable which holds the CPU NR",
            " * which is responsible for calling do_timer(), i.e. the timekeeping stuff. This",
            " * variable has two functions:",
            " *",
            " * 1) Prevent a thundering herd issue of a gazillion of CPUs trying to grab the",
            " *    timekeeping lock all at once. Only the CPU which is assigned to do the",
            " *    update is handling it.",
            " *",
            " * 2) Hand off the duty in the NOHZ idle case by setting the value to",
            " *    TICK_DO_TIMER_NONE, i.e. a non existing CPU. So the next cpu which looks",
            " *    at it will take over and keep the time keeping alive.  The handover",
            " *    procedure also covers cpu hotplug.",
            " */",
            "int tick_do_timer_cpu __read_mostly = TICK_DO_TIMER_BOOT;",
            "#ifdef CONFIG_NO_HZ_FULL",
            "/*",
            " * tick_do_timer_boot_cpu indicates the boot CPU temporarily owns",
            " * tick_do_timer_cpu and it should be taken over by an eligible secondary",
            " * when one comes online.",
            " */",
            "static int tick_do_timer_boot_cpu __read_mostly = -1;",
            "#endif",
            "",
            "/*",
            " * Debugging: see timer_list.c",
            " */",
            "struct tick_device *tick_get_device(int cpu)",
            "{",
            "\treturn &per_cpu(tick_cpu_device, cpu);",
            "}",
            "",
            "/**",
            " * tick_is_oneshot_available - check for a oneshot capable event device",
            " */"
          ],
          "function_name": null,
          "description": "定义每个CPU的tick设备结构体及全局变量，用于跟踪下一个tick时间点和管理定时器更新CPU。声明tick_is_oneshot_available函数原型，用于检测单次模式可用性。",
          "similarity": 0.5221219062805176
        }
      ]
    }
  ]
}