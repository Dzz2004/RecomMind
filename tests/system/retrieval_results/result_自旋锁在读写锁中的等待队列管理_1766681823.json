{
  "query": "自旋锁在读写锁中的等待队列管理",
  "timestamp": "2025-12-26 00:57:03",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/qrwlock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:45:12\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\qrwlock.c`\n\n---\n\n# `locking/qrwlock.c` 技术文档\n\n## 1. 文件概述\n\n`qrwlock.c` 实现了 **排队读写锁（Queued Read-Write Lock, qrwlock）** 的慢路径（slowpath）逻辑。该机制在标准读写锁基础上引入了排队语义，以减少高并发场景下的缓存行抖动（cache line bouncing）并提升可扩展性。当快速路径（fastpath）无法立即获取锁时，调用本文件中的慢路径函数进行阻塞或自旋等待。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `queued_read_lock_slowpath(struct qrwlock *lock)`  \n  获取排队读写锁的**读锁**慢路径实现。当快速路径失败时被调用。\n\n- `queued_write_lock_slowpath(struct qrwlock *lock)`  \n  获取排队读写锁的**写锁**慢路径实现。当快速路径失败时被调用。\n\n### 关键数据结构（隐含依赖）\n\n- `struct qrwlock`：排队读写锁结构体（定义在头文件中），包含：\n  - `cnts`：原子计数器，编码读锁数量、写锁状态和等待标志。\n  - `wait_lock`：底层自旋锁，用于保护等待队列的串行化访问。\n\n### 核心常量（来自头文件）\n\n- `_QR_BIAS`：读计数的偏移量（通常为 `1U << _QR_SHIFT`）。\n- `_QW_LOCKED`：表示写锁已被持有。\n- `_QW_WAITING`：表示有写者正在等待。\n\n## 3. 关键实现\n\n### 读锁慢路径 (`queued_read_lock_slowpath`)\n\n1. **中断上下文特殊处理**：  \n   若调用者处于中断上下文（`in_interrupt()` 为真），则直接使用 `atomic_cond_read_acquire()` 自旋等待写锁释放，**不进入等待队列**，避免死锁风险。\n\n2. **普通上下文流程**：\n   - 先从 `cnts` 中减去 `_QR_BIAS`（临时“取消”读请求）。\n   - 调用 `trace_contention_begin()` 记录锁竞争事件。\n   - 获取 `wait_lock` 自旋锁，进入临界区。\n   - 恢复 `_QR_BIAS`（重新声明读请求）。\n   - 使用 `atomic_cond_read_acquire()` 自旋等待 `cnts` 中无写锁（`!(VAL & _QW_LOCKED)`）。\n   - 释放 `wait_lock`，结束竞争跟踪。\n\n> **关键点**：通过 `wait_lock` 串行化所有慢路径读者，避免多个读者同时修改 `cnts` 导致的 ABA 问题；`atomic_cond_read_acquire()` 提供 ACQUIRE 语义，确保后续临界区访问不会重排到锁获取之前。\n\n### 写锁慢路径 (`queued_write_lock_slowpath`)\n\n1. **尝试直接获取**：  \n   在持有 `wait_lock` 后，若 `cnts == 0`（无读者/写者），则通过 `atomic_try_cmpxchg_acquire()` 直接设置 `_QW_LOCKED` 获取锁。\n\n2. **设置等待标志**：  \n   若无法直接获取，通过 `atomic_or(_QW_WAITING, &lock->cnts)` 通知读者有写者等待，阻止新读者进入。\n\n3. **等待并获取锁**：  \n   - 使用 `atomic_cond_read_relaxed()` 等待 `cnts` 变为 `_QW_WAITING`（即所有读者退出，仅剩等待标志）。\n   - 通过 `atomic_try_cmpxchg_acquire()` 将状态从 `_QW_WAITING` 原子替换为 `_QW_LOCKED`，完成锁获取。\n\n> **关键点**：`_QW_WAITING` 标志用于阻塞新读者；`wait_lock` 确保写者按 FIFO 顺序排队；`atomic_cond_read_relaxed()` + `try_cmpxchg_acquire` 组合实现无锁等待与安全状态转换。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/smp.h>`：SMP 相关宏（如 `in_interrupt()`）。\n  - `<linux/atomic.h>`（隐含）：原子操作（`atomic_read`, `atomic_sub`, `atomic_try_cmpxchg_acquire` 等）。\n  - `<linux/spinlock.h>`：提供 `arch_spin_lock/unlock`。\n  - `<trace/events/lock.h>`：锁竞争跟踪事件。\n\n- **架构依赖**：\n  - `arch_spinlock_t` 和 `arch_spin_lock/unlock` 由具体架构实现（如 x86、ARM）。\n  - `atomic_cond_read_acquire/relaxed` 依赖架构的原子内存操作语义。\n\n- **配套组件**：\n  - 快速路径实现在头文件（如 `qrwlock.h`）中以内联函数形式提供。\n  - 与内核锁调试、性能分析子系统（如 lockdep、ftrace）集成。\n\n## 5. 使用场景\n\n- **高并发读多写少场景**：  \n  如文件系统元数据访问、网络协议栈状态表、RCU 替代方案等，需允许多个读者并发访问，同时保证写者互斥。\n\n- **实时性要求较高的写路径**：  \n  通过排队机制避免写者饿死，确保写请求按 FIFO 顺序处理。\n\n- **中断上下文读操作**：  \n  支持在中断处理程序中安全获取读锁（仅限慢路径中的特殊处理分支）。\n\n- **替代传统 rwlock**：  \n  在需要更好可扩展性和公平性的场景中，替代内核原有的 `rwlock_t`。",
      "similarity": 0.6295429468154907,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/qrwlock.c",
          "start_line": 21,
          "end_line": 85,
          "content": [
            "void __lockfunc queued_read_lock_slowpath(struct qrwlock *lock)",
            "{",
            "\t/*",
            "\t * Readers come here when they cannot get the lock without waiting",
            "\t */",
            "\tif (unlikely(in_interrupt())) {",
            "\t\t/*",
            "\t\t * Readers in interrupt context will get the lock immediately",
            "\t\t * if the writer is just waiting (not holding the lock yet),",
            "\t\t * so spin with ACQUIRE semantics until the lock is available",
            "\t\t * without waiting in the queue.",
            "\t\t */",
            "\t\tatomic_cond_read_acquire(&lock->cnts, !(VAL & _QW_LOCKED));",
            "\t\treturn;",
            "\t}",
            "\tatomic_sub(_QR_BIAS, &lock->cnts);",
            "",
            "\ttrace_contention_begin(lock, LCB_F_SPIN | LCB_F_READ);",
            "",
            "\t/*",
            "\t * Put the reader into the wait queue",
            "\t */",
            "\tarch_spin_lock(&lock->wait_lock);",
            "\tatomic_add(_QR_BIAS, &lock->cnts);",
            "",
            "\t/*",
            "\t * The ACQUIRE semantics of the following spinning code ensure",
            "\t * that accesses can't leak upwards out of our subsequent critical",
            "\t * section in the case that the lock is currently held for write.",
            "\t */",
            "\tatomic_cond_read_acquire(&lock->cnts, !(VAL & _QW_LOCKED));",
            "",
            "\t/*",
            "\t * Signal the next one in queue to become queue head",
            "\t */",
            "\tarch_spin_unlock(&lock->wait_lock);",
            "",
            "\ttrace_contention_end(lock, 0);",
            "}",
            "void __lockfunc queued_write_lock_slowpath(struct qrwlock *lock)",
            "{",
            "\tint cnts;",
            "",
            "\ttrace_contention_begin(lock, LCB_F_SPIN | LCB_F_WRITE);",
            "",
            "\t/* Put the writer into the wait queue */",
            "\tarch_spin_lock(&lock->wait_lock);",
            "",
            "\t/* Try to acquire the lock directly if no reader is present */",
            "\tif (!(cnts = atomic_read(&lock->cnts)) &&",
            "\t    atomic_try_cmpxchg_acquire(&lock->cnts, &cnts, _QW_LOCKED))",
            "\t\tgoto unlock;",
            "",
            "\t/* Set the waiting flag to notify readers that a writer is pending */",
            "\tatomic_or(_QW_WAITING, &lock->cnts);",
            "",
            "\t/* When no more readers or writers, set the locked flag */",
            "\tdo {",
            "\t\tcnts = atomic_cond_read_relaxed(&lock->cnts, VAL == _QW_WAITING);",
            "\t} while (!atomic_try_cmpxchg_acquire(&lock->cnts, &cnts, _QW_LOCKED));",
            "unlock:",
            "\tarch_spin_unlock(&lock->wait_lock);",
            "",
            "\ttrace_contention_end(lock, 0);",
            "}"
          ],
          "function_name": "queued_read_lock_slowpath, queued_write_lock_slowpath",
          "description": "实现了queued_read_lock_slowpath与queued_write_lock_slowpath两个函数，前者处理读锁慢路径中的中断上下文直接获取及等待队列插入逻辑，后者处理写锁慢路径中的竞争检测与状态转换逻辑，均通过原子操作和自旋锁协调多线程访问。",
          "similarity": 0.6533023118972778
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/qrwlock.c",
          "start_line": 1,
          "end_line": 20,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Queued read/write locks",
            " *",
            " * (C) Copyright 2013-2014 Hewlett-Packard Development Company, L.P.",
            " *",
            " * Authors: Waiman Long <waiman.long@hp.com>",
            " */",
            "#include <linux/smp.h>",
            "#include <linux/bug.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/percpu.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/spinlock.h>",
            "#include <trace/events/lock.h>",
            "",
            "/**",
            " * queued_read_lock_slowpath - acquire read lock of a queued rwlock",
            " * @lock: Pointer to queued rwlock structure",
            " */"
          ],
          "function_name": null,
          "description": "定义了queued_read_lock_slowpath函数的原型，用于处理读锁的慢路径逻辑，当前代码片段仅包含函数声明，尚未展示完整实现。",
          "similarity": 0.5862430334091187
        }
      ]
    },
    {
      "source_file": "kernel/locking/osq_lock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:43:41\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\osq_lock.c`\n\n---\n\n# `locking/osq_lock.c` 技术文档\n\n## 1. 文件概述\n\n`osq_lock.c` 实现了一种专为**乐观自旋（Optimistic Spinning）**设计的轻量级排队自旋锁机制，称为 **OSQ（Optimistic Spin Queue）锁**。该机制主要用于支持如互斥锁（mutex）、读写信号量（rwsem）等**可睡眠锁**在争用时进行乐观自旋，以避免不必要的上下文切换和调度开销。OSQ 锁基于 MCS（Mellor-Crummey and Scott）锁的思想，但针对 Linux 内核的调度和抢占模型进行了优化，利用每个 CPU 的静态 per-CPU 节点结构，确保在禁用抢占的自旋上下文中安全使用。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct optimistic_spin_node`：每个 CPU 对应一个静态节点，包含：\n  - `cpu`：编码后的 CPU 编号（实际值 = CPU ID + 1）\n  - `locked`：布尔标志，表示是否已获得锁\n  - `next`：指向队列中下一个节点的指针\n  - `prev`：指向前一个节点的指针\n- `struct optimistic_spin_queue`：OSQ 锁结构体，仅包含一个原子变量 `tail`，用于指向队列尾部（编码后的 CPU 编号），`OSQ_UNLOCKED_VAL`（值为 0）表示无锁。\n\n### 主要函数\n- `bool osq_lock(struct optimistic_spin_queue *lock)`  \n  尝试获取 OSQ 锁。若成功获得锁或决定放弃自旋（如需要调度或前驱被抢占），返回 `true`；若成功排队但未获得锁且需继续等待，则返回 `false`（实际逻辑中，失败路径最终也返回 `false` 表示未获得锁）。\n  \n- `void osq_unlock(struct optimistic_spin_queue *lock)`  \n  释放 OSQ 锁，唤醒队列中的下一个等待者（若存在）。\n\n- `static inline struct optimistic_spin_node *osq_wait_next(...)`  \n  辅助函数，用于在解锁或取消排队时安全地获取下一个节点，并处理队列尾部的原子更新。\n\n- `encode_cpu()` / `decode_cpu()` / `node_cpu()`  \n  用于在 CPU 编号与 per-CPU 节点指针之间进行编码/解码转换，其中 CPU 编号 0 被编码为 1，以 0 表示“无 CPU”（即锁空闲）。\n\n## 3. 关键实现\n\n### Per-CPU 静态节点设计\n- 每个 CPU 拥有一个静态的 `osq_node`（通过 `DEFINE_PER_CPU_SHARED_ALIGNED` 定义），避免动态分配开销。\n- 由于 OSQ 仅在**禁用抢占**的上下文中使用（如 mutex 的乐观自旋阶段），且**不可在中断上下文调用**，因此 per-CPU 节点的生命周期安全。\n\n### 锁获取流程 (`osq_lock`)\n1. **初始化本地节点**：设置 `locked=0`、`next=NULL`，并确保 `cpu` 字段为当前 CPU 编码值。\n2. **原子交换尾指针**：通过 `atomic_xchg(&lock->tail, curr)` 尝试入队。若原值为 `OSQ_UNLOCKED_VAL`，直接获得锁。\n3. **链接到前驱**：若已有前驱（`prev`），通过 `smp_wmb()` 确保内存顺序后，设置 `prev->next = node`。\n4. **自旋等待**：使用 `smp_cond_load_relaxed()` 等待 `node->locked` 变为 1，或满足退出条件（`need_resched()` 或前驱 CPU 被抢占 `vcpu_is_preempted()`）。\n5. **取消排队（Unqueue）**：若需退出自旋：\n   - **Step A**：尝试将 `prev->next` 置为 `NULL`，断开链接。\n   - **Step B**：调用 `osq_wait_next()` 确定下一个节点，并可能将锁尾指针回退。\n   - **Step C**：若存在 `next`，将其与 `prev` 直接链接，完成队列修复。\n\n### 锁释放流程 (`osq_unlock`)\n1. **快速路径**：若当前 CPU 是唯一持有者（`tail == curr`），直接将 `tail` 设为 `OSQ_UNLOCKED_VAL`。\n2. **慢速路径**：\n   - 若本地节点的 `next` 非空，直接设置 `next->locked = 1` 唤醒后继。\n   - 否则调用 `osq_wait_next()` 获取下一个节点（处理并发取消排队的情况），再唤醒。\n\n### 内存屏障与原子操作\n- 使用 `atomic_xchg`、`atomic_cmpxchg_acquire/release` 确保对 `lock->tail` 的操作具有适当的内存序。\n- `smp_wmb()` 保证在设置 `prev->next` 前，本地节点的初始化对其他 CPU 可见。\n- `WRITE_ONCE`/`READ_ONCE` 防止编译器优化破坏并发访问语义。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/percpu.h>`：提供 per-CPU 变量支持（`this_cpu_ptr`, `per_cpu_ptr`）。\n  - `<linux/sched.h>`：提供调度相关函数（`need_resched()`）和虚拟 CPU 抢占检测（`vcpu_is_preempted()`）。\n  - `<linux/osq_lock.h>`：定义 `struct optimistic_spin_queue`、`struct optimistic_spin_node` 及 `OSQ_UNLOCKED_VAL`。\n- **架构依赖**：依赖底层架构的原子操作（`atomic_*`）、内存屏障（`smp_wmb`, `smp_load_acquire`）和 CPU ID 获取（`smp_processor_id()`）。\n- **调度器集成**：与内核调度器紧密协作，通过 `need_resched()` 和 `vcpu_is_preempted()` 决定是否继续自旋。\n\n## 5. 使用场景\n\nOSQ 锁主要用于**可睡眠锁的乐观自旋优化**，典型场景包括：\n- **Mutex（互斥锁）**：在 `mutex_spin_on_owner()` 中，若锁持有者正在运行，当前 CPU 会尝试 OSQ 自旋而非立即睡眠。\n- **Rwsem（读写信号量）**：在写者争用时，若满足条件，会使用 OSQ 进行乐观自旋。\n- **其他睡眠锁**：任何希望在锁争用时避免立即进入睡眠、以降低延迟的同步原语。\n\n其核心价值在于：当锁持有者很可能在**另一个 CPU 上运行且未被抢占**时，通过短暂自旋可避免昂贵的上下文切换，提升性能；同时通过 `vcpu_is_preempted()` 检测虚拟化环境中的抢占，避免在持有者已让出 CPU 时无效自旋。",
      "similarity": 0.6245781183242798,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 20,
          "end_line": 149,
          "content": [
            "static inline int encode_cpu(int cpu_nr)",
            "{",
            "\treturn cpu_nr + 1;",
            "}",
            "static inline int node_cpu(struct optimistic_spin_node *node)",
            "{",
            "\treturn node->cpu - 1;",
            "}",
            "bool osq_lock(struct optimistic_spin_queue *lock)",
            "{",
            "\tstruct optimistic_spin_node *node = this_cpu_ptr(&osq_node);",
            "\tstruct optimistic_spin_node *prev, *next;",
            "\tint curr = encode_cpu(smp_processor_id());",
            "\tint old;",
            "",
            "\tnode->locked = 0;",
            "\tnode->next = NULL;",
            "\t/*",
            "\t * After this cpu member is initialized for the first time, it",
            "\t * would no longer change in fact. That could avoid cache misses",
            "\t * when spin and access the cpu member by other CPUs.",
            "\t */",
            "\tif (node->cpu != curr)",
            "\t\tnode->cpu = curr;",
            "",
            "\t/*",
            "\t * We need both ACQUIRE (pairs with corresponding RELEASE in",
            "\t * unlock() uncontended, or fastpath) and RELEASE (to publish",
            "\t * the node fields we just initialised) semantics when updating",
            "\t * the lock tail.",
            "\t */",
            "\told = atomic_xchg(&lock->tail, curr);",
            "\tif (old == OSQ_UNLOCKED_VAL)",
            "\t\treturn true;",
            "",
            "\tprev = decode_cpu(old);",
            "\tnode->prev = prev;",
            "",
            "\t/*",
            "\t * osq_lock()\t\t\tunqueue",
            "\t *",
            "\t * node->prev = prev\t\tosq_wait_next()",
            "\t * WMB\t\t\t\tMB",
            "\t * prev->next = node\t\tnext->prev = prev // unqueue-C",
            "\t *",
            "\t * Here 'node->prev' and 'next->prev' are the same variable and we need",
            "\t * to ensure these stores happen in-order to avoid corrupting the list.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\tWRITE_ONCE(prev->next, node);",
            "",
            "\t/*",
            "\t * Normally @prev is untouchable after the above store; because at that",
            "\t * moment unlock can proceed and wipe the node element from stack.",
            "\t *",
            "\t * However, since our nodes are static per-cpu storage, we're",
            "\t * guaranteed their existence -- this allows us to apply",
            "\t * cmpxchg in an attempt to undo our queueing.",
            "\t */",
            "",
            "\t/*",
            "\t * Wait to acquire the lock or cancellation. Note that need_resched()",
            "\t * will come with an IPI, which will wake smp_cond_load_relaxed() if it",
            "\t * is implemented with a monitor-wait. vcpu_is_preempted() relies on",
            "\t * polling, be careful.",
            "\t */",
            "\tif (smp_cond_load_relaxed(&node->locked, VAL || need_resched() ||",
            "\t\t\t\t  vcpu_is_preempted(node_cpu(node->prev))))",
            "\t\treturn true;",
            "",
            "\t/* unqueue */",
            "\t/*",
            "\t * Step - A  -- stabilize @prev",
            "\t *",
            "\t * Undo our @prev->next assignment; this will make @prev's",
            "\t * unlock()/unqueue() wait for a next pointer since @lock points to us",
            "\t * (or later).",
            "\t */",
            "",
            "\tfor (;;) {",
            "\t\t/*",
            "\t\t * cpu_relax() below implies a compiler barrier which would",
            "\t\t * prevent this comparison being optimized away.",
            "\t\t */",
            "\t\tif (data_race(prev->next) == node &&",
            "\t\t    cmpxchg(&prev->next, node, NULL) == node)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * We can only fail the cmpxchg() racing against an unlock(),",
            "\t\t * in which case we should observe @node->locked becoming",
            "\t\t * true.",
            "\t\t */",
            "\t\tif (smp_load_acquire(&node->locked))",
            "\t\t\treturn true;",
            "",
            "\t\tcpu_relax();",
            "",
            "\t\t/*",
            "\t\t * Or we race against a concurrent unqueue()'s step-B, in which",
            "\t\t * case its step-C will write us a new @node->prev pointer.",
            "\t\t */",
            "\t\tprev = READ_ONCE(node->prev);",
            "\t}",
            "",
            "\t/*",
            "\t * Step - B -- stabilize @next",
            "\t *",
            "\t * Similar to unlock(), wait for @node->next or move @lock from @node",
            "\t * back to @prev.",
            "\t */",
            "",
            "\tnext = osq_wait_next(lock, node, prev);",
            "\tif (!next)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Step - C -- unlink",
            "\t *",
            "\t * @prev is stable because its still waiting for a new @prev->next",
            "\t * pointer, @next is stable because our @node->next pointer is NULL and",
            "\t * it will wait in Step-A.",
            "\t */",
            "",
            "\tWRITE_ONCE(next->prev, prev);",
            "\tWRITE_ONCE(prev->next, next);",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "encode_cpu, node_cpu, osq_lock",
          "description": "实现osq_lock函数，负责获取乐观自旋锁。通过原子操作将当前节点插入队列，利用内存屏障保证顺序一致性，并通过循环等待条件满足或被唤醒，最终完成锁的获取过程。",
          "similarity": 0.6631819009780884
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 213,
          "end_line": 238,
          "content": [
            "void osq_unlock(struct optimistic_spin_queue *lock)",
            "{",
            "\tstruct optimistic_spin_node *node, *next;",
            "\tint curr = encode_cpu(smp_processor_id());",
            "",
            "\t/*",
            "\t * Fast path for the uncontended case.",
            "\t */",
            "\tif (likely(atomic_cmpxchg_release(&lock->tail, curr,",
            "\t\t\t\t\t  OSQ_UNLOCKED_VAL) == curr))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Second most likely case.",
            "\t */",
            "\tnode = this_cpu_ptr(&osq_node);",
            "\tnext = xchg(&node->next, NULL);",
            "\tif (next) {",
            "\t\tWRITE_ONCE(next->locked, 1);",
            "\t\treturn;",
            "\t}",
            "",
            "\tnext = osq_wait_next(lock, node, NULL);",
            "\tif (next)",
            "\t\tWRITE_ONCE(next->locked, 1);",
            "}"
          ],
          "function_name": "osq_unlock",
          "description": "实现osq_unlock函数，处理锁的释放。通过原子比较交换操作快速处理无竞争情况，否则查找并唤醒下一个等待节点，确保锁状态的正确性与线程安全。",
          "similarity": 0.5844546556472778
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 1,
          "end_line": 19,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/percpu.h>",
            "#include <linux/sched.h>",
            "#include <linux/osq_lock.h>",
            "",
            "/*",
            " * An MCS like lock especially tailored for optimistic spinning for sleeping",
            " * lock implementations (mutex, rwsem, etc).",
            " *",
            " * Using a single mcs node per CPU is safe because sleeping locks should not be",
            " * called from interrupt context and we have preemption disabled while",
            " * spinning.",
            " */",
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);",
            "",
            "/*",
            " * We use the value 0 to represent \"no CPU\", thus the encoded value",
            " * will be the CPU number incremented by 1.",
            " */"
          ],
          "function_name": null,
          "description": "定义全局的per-CPU乐观自旋节点osq_node，用于支持多CPU环境下乐观自旋锁的实现。通过encode_cpu和node_cpu函数处理CPU编号转换，为后续锁操作提供基础设施。",
          "similarity": 0.4826364517211914
        }
      ]
    },
    {
      "source_file": "kernel/locking/qspinlock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:45:55\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\qspinlock.c`\n\n---\n\n# `locking/qspinlock.c` 技术文档\n\n## 1. 文件概述\n\n`qspinlock.c` 实现了 Linux 内核中的 **排队自旋锁（Queued Spinlock）**，这是一种高性能、可扩展的自旋锁机制，旨在替代传统的 ticket spinlock。该实现基于经典的 **MCS 锁（Mellor-Crummey and Scott lock）** 算法，但针对 Linux 内核的 `spinlock_t` 限制（仅 4 字节）进行了高度优化和压缩，同时保留了原有自旋锁的 API 兼容性。其核心目标是在多核系统中减少缓存行争用（cache line bouncing），提升高并发场景下的锁性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct qnode`**  \n  每 CPU 的队列节点结构，封装了 `mcs_spinlock` 节点，并在启用 `CONFIG_PARAVIRT_SPINLOCKS` 时预留额外空间用于半虚拟化支持。每个 CPU 最多维护 `MAX_NODES=4` 个节点，对应最多 4 层嵌套上下文（task、softirq、hardirq、NMI）。\n\n- **`qnodes[MAX_NODES]`**  \n  每 CPU 对齐分配的 `qnode` 数组，确保在 64 位架构上恰好占用一个 64 字节缓存行（半虚拟化模式下占用两个）。\n\n### 关键辅助函数\n\n- **`encode_tail(cpu, idx)`**  \n  将 CPU 编号（+1 以区分无尾状态）和嵌套索引编码为 32 位尾部值，用于表示队列尾节点。\n\n- **`decode_tail(tail)`**  \n  解码尾部值，返回对应的 `mcs_spinlock` 节点指针。\n\n- **`grab_mcs_node(base, idx)`**  \n  从基础 MCS 节点指针偏移获取指定索引的节点。\n\n### 核心锁操作函数（内联）\n\n- **`clear_pending(lock)`**  \n  清除锁的 pending 位（`*,1,* → *,0,*`）。\n\n- **`clear_pending_set_locked(lock)`**  \n  同时清除 pending 位并设置 locked 位，完成锁获取（`*,1,0 → *,0,1`）。\n\n- **`xchg_tail(lock, tail)`**  \n  原子交换锁的尾部字段，返回旧尾部值，用于将当前节点加入等待队列。\n\n- **`queued_fetch_set_pending_acquire(lock)`**  \n  原子获取锁的当前值并设置 pending 位（`*,*,* → *,1,*`），带有获取语义。\n\n- **`set_locked(lock)`**  \n  直接设置 locked 位以获取锁（`*,*,0 → *,0,1`）。\n\n> 注：上述函数根据 `_Q_PENDING_BITS` 是否等于 8 分为两种实现路径，分别优化字节访问和原子位操作。\n\n## 3. 关键实现\n\n### 锁状态压缩设计\n- 传统 MCS 锁需 8 字节尾指针 + 8 字节 next 指针，但 Linux 要求 `spinlock_t` 仅占 4 字节。\n- 本实现将锁状态压缩为 32 位：\n  - **1 字节 locked 字段**：表示锁是否被持有（优化字节写性能）。\n  - **1 字节 pending 字段**：表示是否有第二个竞争者（避免频繁队列操作）。\n  - **2 字节 tail 字段**：编码 `(cpu+1, idx)`，其中 `idx ∈ [0,3]` 表示嵌套层级。\n- 通过 `cpu+1` 编码区分“无尾”（0）和“CPU 0 的尾节点”。\n\n### 快速路径优化\n- **第一个竞争者**：直接自旋在 `locked` 位，无需分配 MCS 节点。\n- **第二个竞争者**：设置 `pending` 位，避免立即进入慢速队列路径。\n- **第三个及以上竞争者**：才真正进入 MCS 队列，通过 `xchg_tail` 原子更新尾指针。\n\n### 嵌套上下文支持\n- 利用每 CPU 的 `qnodes[4]` 数组支持最多 4 层嵌套（task/softirq/hardirq/NMI）。\n- 通过 `idx` 参数在嵌套时选择不同节点，避免递归死锁。\n\n### 架构适配\n- 针对 `_Q_PENDING_BITS == 8`（如 x86）使用字节级原子操作（`WRITE_ONCE`）。\n- 其他架构使用通用原子位操作（`atomic_fetch_or_acquire` 等）。\n- 依赖架构支持 8/16 位原子操作。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/smp.h>`, `<linux/percpu.h>`：SMP 和每 CPU 变量支持。\n  - `<asm/qspinlock.h>`：架构相关的锁布局定义（如 `_Q_*_MASK`）。\n  - `\"mcs_spinlock.h\"`：MCS 锁基础实现。\n  - `\"qspinlock_stat.h\"`：锁统计信息（若启用）。\n- **配置依赖**：\n  - `CONFIG_PARAVIRT_SPINLOCKS`：半虚拟化自旋锁支持（扩展 `qnode` 大小）。\n- **架构要求**：必须支持 8/16 位原子操作（如 x86、ARM64）。\n\n## 5. 使用场景\n\n- **内核通用自旋锁**：作为 `spin_lock()`/`spin_unlock()` 的底层实现，广泛用于内核临界区保护。\n- **高并发场景**：在多核系统中显著优于传统 ticket spinlock，尤其适用于锁竞争激烈的子系统（如内存管理、调度器、文件系统）。\n- **中断上下文**：支持在 hardirq/NMI 等嵌套上下文中安全使用。\n- **半虚拟化环境**：通过 `CONFIG_PARAVIRT_SPINLOCKS` 与 hypervisor 协作减少自旋开销（如 KVM、Xen）。",
      "similarity": 0.6118565797805786,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/qspinlock.c",
          "start_line": 116,
          "end_line": 435,
          "content": [
            "static inline __pure u32 encode_tail(int cpu, int idx)",
            "{",
            "\tu32 tail;",
            "",
            "\ttail  = (cpu + 1) << _Q_TAIL_CPU_OFFSET;",
            "\ttail |= idx << _Q_TAIL_IDX_OFFSET; /* assume < 4 */",
            "",
            "\treturn tail;",
            "}",
            "static __always_inline void clear_pending(struct qspinlock *lock)",
            "{",
            "\tWRITE_ONCE(lock->pending, 0);",
            "}",
            "static __always_inline void clear_pending_set_locked(struct qspinlock *lock)",
            "{",
            "\tWRITE_ONCE(lock->locked_pending, _Q_LOCKED_VAL);",
            "}",
            "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)",
            "{",
            "\t/*",
            "\t * We can use relaxed semantics since the caller ensures that the",
            "\t * MCS node is properly initialized before updating the tail.",
            "\t */",
            "\treturn (u32)xchg_relaxed(&lock->tail,",
            "\t\t\t\t tail >> _Q_TAIL_OFFSET) << _Q_TAIL_OFFSET;",
            "}",
            "static __always_inline void clear_pending(struct qspinlock *lock)",
            "{",
            "\tatomic_andnot(_Q_PENDING_VAL, &lock->val);",
            "}",
            "static __always_inline void clear_pending_set_locked(struct qspinlock *lock)",
            "{",
            "\tatomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);",
            "}",
            "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)",
            "{",
            "\tu32 old, new, val = atomic_read(&lock->val);",
            "",
            "\tfor (;;) {",
            "\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;",
            "\t\t/*",
            "\t\t * We can use relaxed semantics since the caller ensures that",
            "\t\t * the MCS node is properly initialized before updating the",
            "\t\t * tail.",
            "\t\t */",
            "\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);",
            "\t\tif (old == val)",
            "\t\t\tbreak;",
            "",
            "\t\tval = old;",
            "\t}",
            "\treturn old;",
            "}",
            "static __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock)",
            "{",
            "\treturn atomic_fetch_or_acquire(_Q_PENDING_VAL, &lock->val);",
            "}",
            "static __always_inline void set_locked(struct qspinlock *lock)",
            "{",
            "\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);",
            "}",
            "static __always_inline void __pv_init_node(struct mcs_spinlock *node) { }",
            "static __always_inline void __pv_wait_node(struct mcs_spinlock *node,",
            "\t\t\t\t\t   struct mcs_spinlock *prev) { }",
            "static __always_inline void __pv_kick_node(struct qspinlock *lock,",
            "\t\t\t\t\t   struct mcs_spinlock *node) { }",
            "static __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,",
            "\t\t\t\t\t\t   struct mcs_spinlock *node)",
            "\t\t\t\t\t\t   { return 0; }",
            "void __lockfunc queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)",
            "{",
            "\tstruct mcs_spinlock *prev, *next, *node;",
            "\tu32 old, tail;",
            "\tint idx;",
            "",
            "\tBUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));",
            "",
            "\tif (pv_enabled())",
            "\t\tgoto pv_queue;",
            "",
            "\tif (virt_spin_lock(lock))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Wait for in-progress pending->locked hand-overs with a bounded",
            "\t * number of spins so that we guarantee forward progress.",
            "\t *",
            "\t * 0,1,0 -> 0,0,1",
            "\t */",
            "\tif (val == _Q_PENDING_VAL) {",
            "\t\tint cnt = _Q_PENDING_LOOPS;",
            "\t\tval = atomic_cond_read_relaxed(&lock->val,",
            "\t\t\t\t\t       (VAL != _Q_PENDING_VAL) || !cnt--);",
            "\t}",
            "",
            "\t/*",
            "\t * If we observe any contention; queue.",
            "\t */",
            "\tif (val & ~_Q_LOCKED_MASK)",
            "\t\tgoto queue;",
            "",
            "\t/*",
            "\t * trylock || pending",
            "\t *",
            "\t * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock",
            "\t */",
            "\tval = queued_fetch_set_pending_acquire(lock);",
            "",
            "\t/*",
            "\t * If we observe contention, there is a concurrent locker.",
            "\t *",
            "\t * Undo and queue; our setting of PENDING might have made the",
            "\t * n,0,0 -> 0,0,0 transition fail and it will now be waiting",
            "\t * on @next to become !NULL.",
            "\t */",
            "\tif (unlikely(val & ~_Q_LOCKED_MASK)) {",
            "",
            "\t\t/* Undo PENDING if we set it. */",
            "\t\tif (!(val & _Q_PENDING_MASK))",
            "\t\t\tclear_pending(lock);",
            "",
            "\t\tgoto queue;",
            "\t}",
            "",
            "\t/*",
            "\t * We're pending, wait for the owner to go away.",
            "\t *",
            "\t * 0,1,1 -> *,1,0",
            "\t *",
            "\t * this wait loop must be a load-acquire such that we match the",
            "\t * store-release that clears the locked bit and create lock",
            "\t * sequentiality; this is because not all",
            "\t * clear_pending_set_locked() implementations imply full",
            "\t * barriers.",
            "\t */",
            "\tif (val & _Q_LOCKED_MASK)",
            "\t\tsmp_cond_load_acquire(&lock->locked, !VAL);",
            "",
            "\t/*",
            "\t * take ownership and clear the pending bit.",
            "\t *",
            "\t * 0,1,0 -> 0,0,1",
            "\t */",
            "\tclear_pending_set_locked(lock);",
            "\tlockevent_inc(lock_pending);",
            "\treturn;",
            "",
            "\t/*",
            "\t * End of pending bit optimistic spinning and beginning of MCS",
            "\t * queuing.",
            "\t */",
            "queue:",
            "\tlockevent_inc(lock_slowpath);",
            "pv_queue:",
            "\tnode = this_cpu_ptr(&qnodes[0].mcs);",
            "\tidx = node->count++;",
            "\ttail = encode_tail(smp_processor_id(), idx);",
            "",
            "\ttrace_contention_begin(lock, LCB_F_SPIN);",
            "",
            "\t/*",
            "\t * 4 nodes are allocated based on the assumption that there will",
            "\t * not be nested NMIs taking spinlocks. That may not be true in",
            "\t * some architectures even though the chance of needing more than",
            "\t * 4 nodes will still be extremely unlikely. When that happens,",
            "\t * we fall back to spinning on the lock directly without using",
            "\t * any MCS node. This is not the most elegant solution, but is",
            "\t * simple enough.",
            "\t */",
            "\tif (unlikely(idx >= MAX_NODES)) {",
            "\t\tlockevent_inc(lock_no_node);",
            "\t\twhile (!queued_spin_trylock(lock))",
            "\t\t\tcpu_relax();",
            "\t\tgoto release;",
            "\t}",
            "",
            "\tnode = grab_mcs_node(node, idx);",
            "",
            "\t/*",
            "\t * Keep counts of non-zero index values:",
            "\t */",
            "\tlockevent_cond_inc(lock_use_node2 + idx - 1, idx);",
            "",
            "\t/*",
            "\t * Ensure that we increment the head node->count before initialising",
            "\t * the actual node. If the compiler is kind enough to reorder these",
            "\t * stores, then an IRQ could overwrite our assignments.",
            "\t */",
            "\tbarrier();",
            "",
            "\tnode->locked = 0;",
            "\tnode->next = NULL;",
            "\tpv_init_node(node);",
            "",
            "\t/*",
            "\t * We touched a (possibly) cold cacheline in the per-cpu queue node;",
            "\t * attempt the trylock once more in the hope someone let go while we",
            "\t * weren't watching.",
            "\t */",
            "\tif (queued_spin_trylock(lock))",
            "\t\tgoto release;",
            "",
            "\t/*",
            "\t * Ensure that the initialisation of @node is complete before we",
            "\t * publish the updated tail via xchg_tail() and potentially link",
            "\t * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\t/*",
            "\t * Publish the updated tail.",
            "\t * We have already touched the queueing cacheline; don't bother with",
            "\t * pending stuff.",
            "\t *",
            "\t * p,*,* -> n,*,*",
            "\t */",
            "\told = xchg_tail(lock, tail);",
            "\tnext = NULL;",
            "",
            "\t/*",
            "\t * if there was a previous node; link it and wait until reaching the",
            "\t * head of the waitqueue.",
            "\t */",
            "\tif (old & _Q_TAIL_MASK) {",
            "\t\tprev = decode_tail(old);",
            "",
            "\t\t/* Link @node into the waitqueue. */",
            "\t\tWRITE_ONCE(prev->next, node);",
            "",
            "\t\tpv_wait_node(node, prev);",
            "\t\tarch_mcs_spin_lock_contended(&node->locked);",
            "",
            "\t\t/*",
            "\t\t * While waiting for the MCS lock, the next pointer may have",
            "\t\t * been set by another lock waiter. We optimistically load",
            "\t\t * the next pointer & prefetch the cacheline for writing",
            "\t\t * to reduce latency in the upcoming MCS unlock operation.",
            "\t\t */",
            "\t\tnext = READ_ONCE(node->next);",
            "\t\tif (next)",
            "\t\t\tprefetchw(next);",
            "\t}",
            "",
            "\t/*",
            "\t * we're at the head of the waitqueue, wait for the owner & pending to",
            "\t * go away.",
            "\t *",
            "\t * *,x,y -> *,0,0",
            "\t *",
            "\t * this wait loop must use a load-acquire such that we match the",
            "\t * store-release that clears the locked bit and create lock",
            "\t * sequentiality; this is because the set_locked() function below",
            "\t * does not imply a full barrier.",
            "\t *",
            "\t * The PV pv_wait_head_or_lock function, if active, will acquire",
            "\t * the lock and return a non-zero value. So we have to skip the",
            "\t * atomic_cond_read_acquire() call. As the next PV queue head hasn't",
            "\t * been designated yet, there is no way for the locked value to become",
            "\t * _Q_SLOW_VAL. So both the set_locked() and the",
            "\t * atomic_cmpxchg_relaxed() calls will be safe.",
            "\t *",
            "\t * If PV isn't active, 0 will be returned instead.",
            "\t *",
            "\t */",
            "\tif ((val = pv_wait_head_or_lock(lock, node)))",
            "\t\tgoto locked;",
            "",
            "\tval = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));",
            "",
            "locked:",
            "\t/*",
            "\t * claim the lock:",
            "\t *",
            "\t * n,0,0 -> 0,0,1 : lock, uncontended",
            "\t * *,*,0 -> *,*,1 : lock, contended",
            "\t *",
            "\t * If the queue head is the only one in the queue (lock value == tail)",
            "\t * and nobody is pending, clear the tail code and grab the lock.",
            "\t * Otherwise, we only need to grab the lock.",
            "\t */",
            "",
            "\t/*",
            "\t * In the PV case we might already have _Q_LOCKED_VAL set, because",
            "\t * of lock stealing; therefore we must also allow:",
            "\t *",
            "\t * n,0,1 -> 0,0,1",
            "\t *",
            "\t * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the",
            "\t *       above wait condition, therefore any concurrent setting of",
            "\t *       PENDING will make the uncontended transition fail.",
            "\t */",
            "\tif ((val & _Q_TAIL_MASK) == tail) {",
            "\t\tif (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))",
            "\t\t\tgoto release; /* No contention */",
            "\t}",
            "",
            "\t/*",
            "\t * Either somebody is queued behind us or _Q_PENDING_VAL got set",
            "\t * which will then detect the remaining tail and queue behind us",
            "\t * ensuring we'll see a @next.",
            "\t */",
            "\tset_locked(lock);",
            "",
            "\t/*",
            "\t * contended path; wait for next if not observed yet, release.",
            "\t */",
            "\tif (!next)",
            "\t\tnext = smp_cond_load_relaxed(&node->next, (VAL));",
            "",
            "\tarch_mcs_spin_unlock_contended(&next->locked);",
            "\tpv_kick_node(lock, next);",
            "",
            "release:",
            "\ttrace_contention_end(lock, 0);",
            "",
            "\t/*",
            "\t * release the node",
            "\t */",
            "\t__this_cpu_dec(qnodes[0].mcs.count);",
            "}"
          ],
          "function_name": "encode_tail, clear_pending, clear_pending_set_locked, xchg_tail, clear_pending, clear_pending_set_locked, xchg_tail, queued_fetch_set_pending_acquire, set_locked, __pv_init_node, __pv_wait_node, __pv_kick_node, __pv_wait_head_or_lock, queued_spin_lock_slowpath",
          "description": "实现了qspinlock的核心状态转换函数和慢路径获取逻辑，包含尾部编码、挂起状态清除、锁状态设置等原子操作，并通过MCS队列处理锁竞争，支持硬中断、软中断等嵌套场景的递归控制",
          "similarity": 0.5545061230659485
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/qspinlock.c",
          "start_line": 1,
          "end_line": 115,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Queued spinlock",
            " *",
            " * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.",
            " * (C) Copyright 2013-2014,2018 Red Hat, Inc.",
            " * (C) Copyright 2015 Intel Corp.",
            " * (C) Copyright 2015 Hewlett-Packard Enterprise Development LP",
            " *",
            " * Authors: Waiman Long <longman@redhat.com>",
            " *          Peter Zijlstra <peterz@infradead.org>",
            " */",
            "",
            "#ifndef _GEN_PV_LOCK_SLOWPATH",
            "",
            "#include <linux/smp.h>",
            "#include <linux/bug.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/percpu.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/mutex.h>",
            "#include <linux/prefetch.h>",
            "#include <asm/byteorder.h>",
            "#include <asm/qspinlock.h>",
            "#include <trace/events/lock.h>",
            "",
            "/*",
            " * Include queued spinlock statistics code",
            " */",
            "#include \"qspinlock_stat.h\"",
            "",
            "/*",
            " * The basic principle of a queue-based spinlock can best be understood",
            " * by studying a classic queue-based spinlock implementation called the",
            " * MCS lock. A copy of the original MCS lock paper (\"Algorithms for Scalable",
            " * Synchronization on Shared-Memory Multiprocessors by Mellor-Crummey and",
            " * Scott\") is available at",
            " *",
            " * https://bugzilla.kernel.org/show_bug.cgi?id=206115",
            " *",
            " * This queued spinlock implementation is based on the MCS lock, however to",
            " * make it fit the 4 bytes we assume spinlock_t to be, and preserve its",
            " * existing API, we must modify it somehow.",
            " *",
            " * In particular; where the traditional MCS lock consists of a tail pointer",
            " * (8 bytes) and needs the next pointer (another 8 bytes) of its own node to",
            " * unlock the next pending (next->locked), we compress both these: {tail,",
            " * next->locked} into a single u32 value.",
            " *",
            " * Since a spinlock disables recursion of its own context and there is a limit",
            " * to the contexts that can nest; namely: task, softirq, hardirq, nmi. As there",
            " * are at most 4 nesting levels, it can be encoded by a 2-bit number. Now",
            " * we can encode the tail by combining the 2-bit nesting level with the cpu",
            " * number. With one byte for the lock value and 3 bytes for the tail, only a",
            " * 32-bit word is now needed. Even though we only need 1 bit for the lock,",
            " * we extend it to a full byte to achieve better performance for architectures",
            " * that support atomic byte write.",
            " *",
            " * We also change the first spinner to spin on the lock bit instead of its",
            " * node; whereby avoiding the need to carry a node from lock to unlock, and",
            " * preserving existing lock API. This also makes the unlock code simpler and",
            " * faster.",
            " *",
            " * N.B. The current implementation only supports architectures that allow",
            " *      atomic operations on smaller 8-bit and 16-bit data types.",
            " *",
            " */",
            "",
            "#include \"mcs_spinlock.h\"",
            "#define MAX_NODES\t4",
            "",
            "/*",
            " * On 64-bit architectures, the mcs_spinlock structure will be 16 bytes in",
            " * size and four of them will fit nicely in one 64-byte cacheline. For",
            " * pvqspinlock, however, we need more space for extra data. To accommodate",
            " * that, we insert two more long words to pad it up to 32 bytes. IOW, only",
            " * two of them can fit in a cacheline in this case. That is OK as it is rare",
            " * to have more than 2 levels of slowpath nesting in actual use. We don't",
            " * want to penalize pvqspinlocks to optimize for a rare case in native",
            " * qspinlocks.",
            " */",
            "struct qnode {",
            "\tstruct mcs_spinlock mcs;",
            "#ifdef CONFIG_PARAVIRT_SPINLOCKS",
            "\tlong reserved[2];",
            "#endif",
            "};",
            "",
            "/*",
            " * The pending bit spinning loop count.",
            " * This heuristic is used to limit the number of lockword accesses",
            " * made by atomic_cond_read_relaxed when waiting for the lock to",
            " * transition out of the \"== _Q_PENDING_VAL\" state. We don't spin",
            " * indefinitely because there's no guarantee that we'll make forward",
            " * progress.",
            " */",
            "#ifndef _Q_PENDING_LOOPS",
            "#define _Q_PENDING_LOOPS\t1",
            "#endif",
            "",
            "/*",
            " * Per-CPU queue node structures; we can never have more than 4 nested",
            " * contexts: task, softirq, hardirq, nmi.",
            " *",
            " * Exactly fits one 64-byte cacheline on a 64-bit architecture.",
            " *",
            " * PV doubles the storage and uses the second cacheline for PV state.",
            " */",
            "static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);",
            "",
            "/*",
            " * We must be able to distinguish between no-tail and the tail at 0:0,",
            " * therefore increment the cpu number by one.",
            " */",
            ""
          ],
          "function_name": null,
          "description": "定义了qspinlock的队列节点结构体qnode及其per-CPU数组，用于支持paravirtualization的锁机制，通过压缩尾部指针与锁状态到单个32位值，结合MCS锁算法实现可扩展同步",
          "similarity": 0.506026029586792
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/qspinlock.c",
          "start_line": 590,
          "end_line": 594,
          "content": [
            "static __init int parse_nopvspin(char *arg)",
            "{",
            "\tnopvspin = true;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "parse_nopvspin",
          "description": "解析内核启动参数以禁用paravirtualization锁机制的初始化函数，通过设置nopvspin标志位控制是否启用特定的虚拟化架构优化特性",
          "similarity": 0.4672904312610626
        }
      ]
    }
  ]
}