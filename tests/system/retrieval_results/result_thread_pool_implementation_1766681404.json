{
  "query": "thread pool implementation",
  "timestamp": "2025-12-26 00:50:04",
  "retrieved_files": [
    {
      "source_file": "mm/dmapool.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:56:43\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dmapool.c`\n\n---\n\n# dmapool.c 技术文档\n\n## 1. 文件概述\n\n`dmapool.c` 实现了 Linux 内核中的 **DMA 池（DMA Pool）分配器**，用于为设备驱动程序提供小块、一致（coherent）且可 DMA 访问的内存。该分配器基于 `dma_alloc_coherent()` 分配整页内存，并将其划分为固定大小的块，以满足频繁分配/释放小块 DMA 内存的需求，避免直接使用页级分配造成的内存浪费。此机制特别适用于需要大量相同大小 DMA 缓冲区的设备驱动（如 USB、网络、存储控制器等）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct dma_pool`**  \n  表示一个 DMA 池，包含：\n  - `page_list`：已分配物理页的双向链表\n  - `lock`：自旋锁，保护池内操作\n  - `next_block`：空闲块的单向链表头指针\n  - `nr_blocks` / `nr_active` / `nr_pages`：统计信息（总块数、活跃块数、页数）\n  - `dev`：关联的设备\n  - `size` / `allocation` / `boundary`：块大小、页分配大小、边界对齐限制\n  - `name`：池名称（用于调试）\n  - `pools`：挂载到设备 `dma_pools` 列表的节点\n\n- **`struct dma_page`**  \n  表示从 `dma_alloc_coherent()` 分配的一个物理页，包含虚拟地址 `vaddr` 和 DMA 地址 `dma`。\n\n- **`struct dma_block`**  \n  嵌入在每个 DMA 块起始位置的元数据结构，仅包含指向下一个空闲块的指针 `next_block` 和该块的 DMA 地址 `dma`。\n\n### 主要函数\n\n- **`dma_pool_create()`**  \n  创建一个新的 DMA 池，指定名称、设备、块大小、对齐要求和边界限制。\n\n- **`pool_block_pop()` / `pool_block_push()`**  \n  从空闲链表中分配/归还一个 DMA 块。\n\n- **`pool_check_block()` / `pool_block_err()` / `pool_init_page()`**  \n  调试辅助函数（在 `DMAPOOL_DEBUG` 启用时），用于检测内存越界、重复释放等错误，并进行内存毒化（poisoning）。\n\n- **`pools_show()`**  \n  sysfs 接口回调，显示设备下所有 DMA 池的统计信息。\n\n## 3. 关键实现\n\n- **内存组织**：  \n  每次调用 `dma_alloc_coherent()` 分配至少一页（`PAGE_SIZE`）的连续物理内存（`allocation` 字段）。该页被划分为多个 `size` 字节的块。每个块的起始处嵌入 `struct dma_block` 元数据。\n\n- **空闲管理**：  \n  所有空闲块通过 `next_block` 指针组成一个**全局单向链表**（由 `dma_pool.next_block` 指向头节点）。分配时从链表头部弹出，释放时压入头部。**已分配块不被显式跟踪**，仅通过 `nr_active` 计数。\n\n- **边界对齐处理**：  \n  若指定了 `boundary`（如 4KB），则确保单个 DMA 块不会跨越该边界。实现上通过限制每页实际可用区域或调整分配策略（代码片段未完整展示具体划分逻辑）。\n\n- **调试支持（DMAPOOL_DEBUG）**：  \n  在 SLUB 调试开启时启用：\n  - 分配时用 `POOL_POISON_ALLOCATED` 填充用户区域（若未启用 init-on-alloc）\n  - 释放时用 `POOL_POISON_FREED` 填充，并检查是否已被释放（防 double-free）\n  - 提供 `pool_find_page()` 辅助验证 DMA 地址有效性\n\n- **Sysfs 集成**：  \n  首次为设备创建 DMA 池时，自动注册 `pools` sysfs 属性文件，可通过 `/sys/devices/.../pools` 查看池状态（名称、活跃块数、总块数、块大小、页数）。\n\n- **并发控制**：  \n  - `pools_lock`：保护设备 `dma_pools` 列表的增删\n  - `pools_reg_lock`：防止 `dma_pool_create()` 与 `dma_pool_destroy()` 之间的竞争\n  - `dma_pool.lock`：保护池内部的空闲链表和计数器\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/dma-mapping.h>`：提供 `dma_alloc_coherent()` / `dma_free_coherent()` 等底层 DMA 映射接口\n  - `<linux/device.h>`：设备模型及 sysfs 支持\n  - `<linux/slab.h>`：用于分配 `struct dma_pool` 结构体内存\n\n- **调试依赖**：\n  - `CONFIG_SLUB_DEBUG_ON`：启用内存毒化和错误检查\n  - `<linux/poison.h>`：提供 `POOL_POISON_*` 常量\n\n- **同步原语**：\n  - `<linux/mutex.h>` / `<linux/spinlock.h>`：提供互斥锁和自旋锁\n\n## 5. 使用场景\n\n- **设备驱动开发**：  \n  当驱动需要频繁分配/释放**固定大小**的小块（通常小于一页）DMA 缓冲区时，使用 DMA 池可显著提升性能并减少内存碎片。典型场景包括：\n  - USB 主机控制器的传输描述符（TD）池\n  - 网络设备的接收/发送描述符环\n  - 存储控制器的命令/状态块\n\n- **替代方案**：  \n  相比直接调用 `dma_alloc_coherent()` 分配整页内存，DMA 池避免了小块分配的内存浪费；相比通用 slab 分配器（如 kmalloc），它保证了返回内存的 DMA 一致性（无需手动缓存维护）。\n\n- **限制条件**：  \n  - 仅适用于**一致性 DMA 映射**（coherent DMA）\n  - 所有块大小在池创建时固定\n  - 不适用于大块（接近或超过一页）内存分配",
      "similarity": 0.5619112253189087,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/dmapool.c",
          "start_line": 360,
          "end_line": 419,
          "content": [
            "void dma_pool_destroy(struct dma_pool *pool)",
            "{",
            "\tstruct dma_page *page, *tmp;",
            "\tbool empty, busy = false;",
            "",
            "\tif (unlikely(!pool))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&pools_reg_lock);",
            "\tmutex_lock(&pools_lock);",
            "\tlist_del(&pool->pools);",
            "\tempty = list_empty(&pool->dev->dma_pools);",
            "\tmutex_unlock(&pools_lock);",
            "\tif (empty)",
            "\t\tdevice_remove_file(pool->dev, &dev_attr_pools);",
            "\tmutex_unlock(&pools_reg_lock);",
            "",
            "\tif (pool->nr_active) {",
            "\t\tdev_err(pool->dev, \"%s %s busy\\n\", __func__, pool->name);",
            "\t\tbusy = true;",
            "\t}",
            "",
            "\tlist_for_each_entry_safe(page, tmp, &pool->page_list, page_list) {",
            "\t\tif (!busy)",
            "\t\t\tdma_free_coherent(pool->dev, pool->allocation,",
            "\t\t\t\t\t  page->vaddr, page->dma);",
            "\t\tlist_del(&page->page_list);",
            "\t\tkfree(page);",
            "\t}",
            "",
            "\tkfree(pool);",
            "}",
            "void dma_pool_free(struct dma_pool *pool, void *vaddr, dma_addr_t dma)",
            "{",
            "\tstruct dma_block *block = vaddr;",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&pool->lock, flags);",
            "\tif (!pool_block_err(pool, vaddr, dma)) {",
            "\t\tpool_block_push(pool, block, dma);",
            "\t\tpool->nr_active--;",
            "\t}",
            "\tspin_unlock_irqrestore(&pool->lock, flags);",
            "}",
            "static void dmam_pool_release(struct device *dev, void *res)",
            "{",
            "\tstruct dma_pool *pool = *(struct dma_pool **)res;",
            "",
            "\tdma_pool_destroy(pool);",
            "}",
            "static int dmam_pool_match(struct device *dev, void *res, void *match_data)",
            "{",
            "\treturn *(struct dma_pool **)res == match_data;",
            "}",
            "void dmam_pool_destroy(struct dma_pool *pool)",
            "{",
            "\tstruct device *dev = pool->dev;",
            "",
            "\tWARN_ON(devres_release(dev, dmam_pool_release, dmam_pool_match, pool));",
            "}"
          ],
          "function_name": "dma_pool_destroy, dma_pool_free, dmam_pool_release, dmam_pool_match, dmam_pool_destroy",
          "description": "实现DMA池销毁逻辑，包括资源回收、活跃块计数管理及页面内存释放，通过互斥锁保证线程安全，支持设备资源释放回调。",
          "similarity": 0.46784210205078125
        },
        {
          "chunk_id": 1,
          "file_path": "mm/dmapool.c",
          "start_line": 72,
          "end_line": 196,
          "content": [
            "static ssize_t pools_show(struct device *dev, struct device_attribute *attr, char *buf)",
            "{",
            "\tstruct dma_pool *pool;",
            "\tunsigned size;",
            "",
            "\tsize = sysfs_emit(buf, \"poolinfo - 0.1\\n\");",
            "",
            "\tmutex_lock(&pools_lock);",
            "\tlist_for_each_entry(pool, &dev->dma_pools, pools) {",
            "\t\t/* per-pool info, no real statistics yet */",
            "\t\tsize += sysfs_emit_at(buf, size, \"%-16s %4zu %4zu %4u %2zu\\n\",",
            "\t\t\t\t      pool->name, pool->nr_active,",
            "\t\t\t\t      pool->nr_blocks, pool->size,",
            "\t\t\t\t      pool->nr_pages);",
            "\t}",
            "\tmutex_unlock(&pools_lock);",
            "",
            "\treturn size;",
            "}",
            "static void pool_check_block(struct dma_pool *pool, struct dma_block *block,",
            "\t\t\t     gfp_t mem_flags)",
            "{",
            "\tu8 *data = (void *)block;",
            "\tint i;",
            "",
            "\tfor (i = sizeof(struct dma_block); i < pool->size; i++) {",
            "\t\tif (data[i] == POOL_POISON_FREED)",
            "\t\t\tcontinue;",
            "\t\tdev_err(pool->dev, \"%s %s, %p (corrupted)\\n\", __func__,",
            "\t\t\tpool->name, block);",
            "",
            "\t\t/*",
            "\t\t * Dump the first 4 bytes even if they are not",
            "\t\t * POOL_POISON_FREED",
            "\t\t */",
            "\t\tprint_hex_dump(KERN_ERR, \"\", DUMP_PREFIX_OFFSET, 16, 1,",
            "\t\t\t\tdata, pool->size, 1);",
            "\t\tbreak;",
            "\t}",
            "",
            "\tif (!want_init_on_alloc(mem_flags))",
            "\t\tmemset(block, POOL_POISON_ALLOCATED, pool->size);",
            "}",
            "static bool pool_block_err(struct dma_pool *pool, void *vaddr, dma_addr_t dma)",
            "{",
            "\tstruct dma_block *block = pool->next_block;",
            "\tstruct dma_page *page;",
            "",
            "\tpage = pool_find_page(pool, dma);",
            "\tif (!page) {",
            "\t\tdev_err(pool->dev, \"%s %s, %p/%pad (bad dma)\\n\",",
            "\t\t\t__func__, pool->name, vaddr, &dma);",
            "\t\treturn true;",
            "\t}",
            "",
            "\twhile (block) {",
            "\t\tif (block != vaddr) {",
            "\t\t\tblock = block->next_block;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tdev_err(pool->dev, \"%s %s, dma %pad already free\\n\",",
            "\t\t\t__func__, pool->name, &dma);",
            "\t\treturn true;",
            "\t}",
            "",
            "\tmemset(vaddr, POOL_POISON_FREED, pool->size);",
            "\treturn false;",
            "}",
            "static void pool_init_page(struct dma_pool *pool, struct dma_page *page)",
            "{",
            "\tmemset(page->vaddr, POOL_POISON_FREED, pool->allocation);",
            "}",
            "static void pool_check_block(struct dma_pool *pool, struct dma_block *block,",
            "\t\t\t     gfp_t mem_flags)",
            "{",
            "}",
            "static bool pool_block_err(struct dma_pool *pool, void *vaddr, dma_addr_t dma)",
            "{",
            "\tif (want_init_on_free())",
            "\t\tmemset(vaddr, 0, pool->size);",
            "\treturn false;",
            "}",
            "static void pool_init_page(struct dma_pool *pool, struct dma_page *page)",
            "{",
            "}",
            "static void pool_block_push(struct dma_pool *pool, struct dma_block *block,",
            "\t\t\t    dma_addr_t dma)",
            "{",
            "\tblock->dma = dma;",
            "\tblock->next_block = pool->next_block;",
            "\tpool->next_block = block;",
            "}",
            "static void pool_initialise_page(struct dma_pool *pool, struct dma_page *page)",
            "{",
            "\tunsigned int next_boundary = pool->boundary, offset = 0;",
            "\tstruct dma_block *block, *first = NULL, *last = NULL;",
            "",
            "\tpool_init_page(pool, page);",
            "\twhile (offset + pool->size <= pool->allocation) {",
            "\t\tif (offset + pool->size > next_boundary) {",
            "\t\t\toffset = next_boundary;",
            "\t\t\tnext_boundary += pool->boundary;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tblock = page->vaddr + offset;",
            "\t\tblock->dma = page->dma + offset;",
            "\t\tblock->next_block = NULL;",
            "",
            "\t\tif (last)",
            "\t\t\tlast->next_block = block;",
            "\t\telse",
            "\t\t\tfirst = block;",
            "\t\tlast = block;",
            "",
            "\t\toffset += pool->size;",
            "\t\tpool->nr_blocks++;",
            "\t}",
            "",
            "\tlast->next_block = pool->next_block;",
            "\tpool->next_block = first;",
            "",
            "\tlist_add(&page->page_list, &pool->page_list);",
            "\tpool->nr_pages++;",
            "}"
          ],
          "function_name": "pools_show, pool_check_block, pool_block_err, pool_init_page, pool_check_block, pool_block_err, pool_init_page, pool_block_push, pool_initialise_page",
          "description": "提供DMA池调试显示、块校验、错误检测及页面初始化等功能，部分函数存在重复声明或未完成实现，上下文不完整。",
          "similarity": 0.44318971037864685
        },
        {
          "chunk_id": 0,
          "file_path": "mm/dmapool.c",
          "start_line": 1,
          "end_line": 71,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * DMA Pool allocator",
            " *",
            " * Copyright 2001 David Brownell",
            " * Copyright 2007 Intel Corporation",
            " *   Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This allocator returns small blocks of a given size which are DMA-able by",
            " * the given device.  It uses the dma_alloc_coherent page allocator to get",
            " * new pages, then splits them up into blocks of the required size.",
            " * Many older drivers still have their own code to do this.",
            " *",
            " * The current design of this allocator is fairly simple.  The pool is",
            " * represented by the 'struct dma_pool' which keeps a doubly-linked list of",
            " * allocated pages.  Each page in the page_list is split into blocks of at",
            " * least 'size' bytes.  Free blocks are tracked in an unsorted singly-linked",
            " * list of free blocks across all pages.  Used blocks aren't tracked, but we",
            " * keep a count of how many are currently allocated from each page.",
            " */",
            "",
            "#include <linux/device.h>",
            "#include <linux/dma-mapping.h>",
            "#include <linux/dmapool.h>",
            "#include <linux/kernel.h>",
            "#include <linux/list.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/poison.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/slab.h>",
            "#include <linux/stat.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/string.h>",
            "#include <linux/types.h>",
            "#include <linux/wait.h>",
            "",
            "#ifdef CONFIG_SLUB_DEBUG_ON",
            "#define DMAPOOL_DEBUG 1",
            "#endif",
            "",
            "struct dma_block {",
            "\tstruct dma_block *next_block;",
            "\tdma_addr_t dma;",
            "};",
            "",
            "struct dma_pool {\t\t/* the pool */",
            "\tstruct list_head page_list;",
            "\tspinlock_t lock;",
            "\tstruct dma_block *next_block;",
            "\tsize_t nr_blocks;",
            "\tsize_t nr_active;",
            "\tsize_t nr_pages;",
            "\tstruct device *dev;",
            "\tunsigned int size;",
            "\tunsigned int allocation;",
            "\tunsigned int boundary;",
            "\tchar name[32];",
            "\tstruct list_head pools;",
            "};",
            "",
            "struct dma_page {\t\t/* cacheable header for 'allocation' bytes */",
            "\tstruct list_head page_list;",
            "\tvoid *vaddr;",
            "\tdma_addr_t dma;",
            "};",
            "",
            "static DEFINE_MUTEX(pools_lock);",
            "static DEFINE_MUTEX(pools_reg_lock);",
            ""
          ],
          "function_name": null,
          "description": "定义DMA池相关结构体及全局锁，用于管理可DMA访问的小块内存分配，包含页面链表、块链表、设备指针及参数配置。",
          "similarity": 0.43161505460739136
        }
      ]
    },
    {
      "source_file": "mm/zpool.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:37:17\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `zpool.c`\n\n---\n\n# zpool.c 技术文档\n\n## 文件概述\n\n`zpool.c` 是 Linux 内核中用于提供通用内存池抽象接口的实现文件。它作为压缩内存存储（如 zswap、zram）后端的统一前端，允许不同底层内存池实现（如 zbud、zsmalloc）通过标准接口被上层模块使用。该文件实现了驱动注册/注销、池创建/销毁、内存分配/释放等核心功能，并支持运行时动态加载对应的内存池模块。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`struct zpool`**  \n  表示一个具体的内存池实例，包含指向驱动和底层池对象的指针。\n  ```c\n  struct zpool {\n      struct zpool_driver *driver;  // 指向注册的驱动\n      void *pool;                   // 底层驱动管理的实际池对象\n  };\n  ```\n\n- **`drivers_head` 和 `drivers_lock`**  \n  全局链表和自旋锁，用于管理所有已注册的 `zpool_driver` 实例。\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|----------|\n| `zpool_register_driver()` | 注册一个新的 zpool 驱动实现 |\n| `zpool_unregister_driver()` | 注销 zpool 驱动（需确保未被使用） |\n| `zpool_has_pool()` | 检查指定类型的内存池是否可用（可触发模块加载） |\n| `zpool_create_pool()` | 创建指定类型和名称的新内存池 |\n| `zpool_destroy_pool()` | 销毁已存在的内存池 |\n| `zpool_get_type()` | 获取内存池的类型字符串 |\n| `zpool_malloc_support_movable()` | 查询池是否支持可移动内存分配 |\n| `zpool_malloc()` | 从池中分配指定大小的内存 |\n| `zpool_free()` | 释放通过 handle 分配的内存 |\n| `zpool_map_handle()` | 将 handle 映射为可访问的虚拟地址（代码截断，但声明存在） |\n\n## 关键实现\n\n### 驱动注册与引用计数\n- 使用全局链表 `drivers_head` 管理所有已注册的 `zpool_driver`。\n- 通过 `atomic_t refcount` 跟踪驱动使用次数，防止在使用中被卸载。\n- `zpool_get_driver()` 在获取驱动时增加引用计数并调用 `try_module_get()` 增加模块引用。\n- `zpool_put_driver()` 在释放时减少引用计数并调用 `module_put()`。\n\n### 动态模块加载\n- `zpool_has_pool()` 和 `zpool_create_pool()` 在找不到驱动时会调用 `request_module(\"zpool-%s\", type)` 尝试加载对应内核模块（如 `zpool-zbud`）。\n- 模块加载成功后再次尝试获取驱动，提高灵活性。\n\n### 内存池生命周期管理\n- `zpool_create_pool()`：分配 `struct zpool` 结构体，调用驱动的 `create()` 方法初始化底层池。\n- `zpool_destroy_pool()`：调用驱动的 `destroy()` 方法清理资源，释放结构体内存，并减少驱动引用。\n- 所有操作均保证线程安全（由底层驱动实现保证）。\n\n### GFP 标志传递\n- 创建池和分配内存时传入 `gfp_t` 标志，允许上层控制内存分配行为（如是否可睡眠、是否使用高端内存等）。\n- 底层驱动可根据自身能力选择是否使用这些标志。\n\n## 依赖关系\n\n- **头文件依赖**：\n  - `<linux/zpool.h>`：定义了 `zpool_driver`、`zpool` 等核心接口结构。\n  - `<linux/module.h>`：提供模块加载/卸载和引用计数支持。\n  - `<linux/slab.h>`：用于 `kmalloc/kfree` 分配 `struct zpool`。\n  - `<linux/spinlock.h>`：保护驱动注册链表的并发访问。\n  - `<linux/mm.h>`：提供内存管理相关定义（如 `gfp_t`）。\n\n- **模块依赖**：\n  - 依赖具体的 zpool 实现模块（如 `zbud.ko`、`zsmalloc.ko`），这些模块通过 `zpool_register_driver()` 注册自身。\n  - 通过 `request_module()` 机制动态加载后端实现模块。\n\n## 使用场景\n\n- **zswap**：作为透明页交换压缩缓存的后端存储，使用 zpool 接口分配/释放压缩页内存。\n- **zram**：作为基于 RAM 的块设备，使用 zpool 管理压缩数据的存储空间。\n- **其他需要统一内存池接口的子系统**：任何需要将压缩数据暂存于内存且希望支持多种后端分配器的场景。\n\n该文件通过抽象层解耦了上层使用者与底层内存分配实现，使得内核可以灵活切换不同的压缩内存管理策略（如 zbud 的 buddy-like 算法 vs zsmalloc 的 slab-like 算法），同时保持上层代码不变。",
      "similarity": 0.5587184429168701,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/zpool.c",
          "start_line": 33,
          "end_line": 107,
          "content": [
            "void zpool_register_driver(struct zpool_driver *driver)",
            "{",
            "\tspin_lock(&drivers_lock);",
            "\tatomic_set(&driver->refcount, 0);",
            "\tlist_add(&driver->list, &drivers_head);",
            "\tspin_unlock(&drivers_lock);",
            "}",
            "int zpool_unregister_driver(struct zpool_driver *driver)",
            "{",
            "\tint ret = 0, refcount;",
            "",
            "\tspin_lock(&drivers_lock);",
            "\trefcount = atomic_read(&driver->refcount);",
            "\tWARN_ON(refcount < 0);",
            "\tif (refcount > 0)",
            "\t\tret = -EBUSY;",
            "\telse",
            "\t\tlist_del(&driver->list);",
            "\tspin_unlock(&drivers_lock);",
            "",
            "\treturn ret;",
            "}",
            "static void zpool_put_driver(struct zpool_driver *driver)",
            "{",
            "\tatomic_dec(&driver->refcount);",
            "\tmodule_put(driver->owner);",
            "}",
            "bool zpool_has_pool(char *type)",
            "{",
            "\tstruct zpool_driver *driver = zpool_get_driver(type);",
            "",
            "\tif (!driver) {",
            "\t\trequest_module(\"zpool-%s\", type);",
            "\t\tdriver = zpool_get_driver(type);",
            "\t}",
            "",
            "\tif (!driver)",
            "\t\treturn false;",
            "",
            "\tzpool_put_driver(driver);",
            "\treturn true;",
            "}",
            "void zpool_destroy_pool(struct zpool *zpool)",
            "{",
            "\tpr_debug(\"destroying pool type %s\\n\", zpool->driver->type);",
            "",
            "\tzpool->driver->destroy(zpool->pool);",
            "\tzpool_put_driver(zpool->driver);",
            "\tkfree(zpool);",
            "}",
            "bool zpool_malloc_support_movable(struct zpool *zpool)",
            "{",
            "\treturn zpool->driver->malloc_support_movable;",
            "}",
            "int zpool_malloc(struct zpool *zpool, size_t size, gfp_t gfp,",
            "\t\t\tunsigned long *handle)",
            "{",
            "\treturn zpool->driver->malloc(zpool->pool, size, gfp, handle);",
            "}",
            "void zpool_free(struct zpool *zpool, unsigned long handle)",
            "{",
            "\tzpool->driver->free(zpool->pool, handle);",
            "}",
            "void zpool_unmap_handle(struct zpool *zpool, unsigned long handle)",
            "{",
            "\tzpool->driver->unmap(zpool->pool, handle);",
            "}",
            "u64 zpool_get_total_size(struct zpool *zpool)",
            "{",
            "\treturn zpool->driver->total_size(zpool->pool);",
            "}",
            "bool zpool_can_sleep_mapped(struct zpool *zpool)",
            "{",
            "\treturn zpool->driver->sleep_mapped;",
            "}"
          ],
          "function_name": "zpool_register_driver, zpool_unregister_driver, zpool_put_driver, zpool_has_pool, zpool_destroy_pool, zpool_malloc_support_movable, zpool_malloc, zpool_free, zpool_unmap_handle, zpool_get_total_size, zpool_can_sleep_mapped",
          "description": "实现zpool驱动程序的注册/注销逻辑，管理内存池生命周期，封装底层驱动接口供上层调用，包含内存分配/释放/unmap等核心操作函数",
          "similarity": 0.5835381746292114
        },
        {
          "chunk_id": 0,
          "file_path": "mm/zpool.c",
          "start_line": 1,
          "end_line": 32,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * zpool memory storage api",
            " *",
            " * Copyright (C) 2014 Dan Streetman",
            " *",
            " * This is a common frontend for memory storage pool implementations.",
            " * Typically, this is used to store compressed memory.",
            " */",
            "",
            "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt",
            "",
            "#include <linux/list.h>",
            "#include <linux/types.h>",
            "#include <linux/mm.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/module.h>",
            "#include <linux/zpool.h>",
            "",
            "struct zpool {",
            "\tstruct zpool_driver *driver;",
            "\tvoid *pool;",
            "};",
            "",
            "static LIST_HEAD(drivers_head);",
            "static DEFINE_SPINLOCK(drivers_lock);",
            "",
            "/**",
            " * zpool_register_driver() - register a zpool implementation.",
            " * @driver:\tdriver to register",
            " */"
          ],
          "function_name": null,
          "description": "定义zpool模块框架，包含zpool结构体及驱动程序管理相关数据结构，提供驱动注册接口但未完成完整实现",
          "similarity": 0.5335235595703125
        }
      ]
    },
    {
      "source_file": "kernel/workqueue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:53:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workqueue.c`\n\n---\n\n# workqueue.c 技术文档\n\n## 1. 文件概述\n\n`workqueue.c` 是 Linux 内核中实现通用异步执行机制的核心文件，提供基于共享工作线程池（worker pool）的延迟任务调度功能。工作项（work items）在进程上下文中执行，支持 CPU 绑定和非绑定两种模式。每个 CPU 默认拥有两个标准工作池（普通优先级和高优先级），同时支持动态创建非绑定工作池以满足不同工作队列的需求。该机制替代了早期的 taskqueue/keventd 实现，具有更高的可扩展性和资源利用率。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct worker_pool`**  \n  工作线程池结构体，管理一组工作线程（workers），包含：\n  - `lock`：保护池状态的自旋锁\n  - `cpu` / `node`：关联的 CPU 和 NUMA 节点（绑定池）\n  - `worklist`：待处理工作项队列\n  - `idle_list` / `busy_hash`：空闲和忙碌工作线程的管理结构\n  - `nr_workers` / `nr_idle`：工作线程数量统计\n  - `attrs`：工作线程属性（如优先级、CPU 亲和性）\n  - `mayday_timer`：紧急情况下的救援请求定时器\n\n- **`struct pool_workqueue`**  \n  工作队列与工作池之间的关联结构，每个工作队列在每个池中都有一个对应的 `pool_workqueue` 实例，用于：\n  - 管理工作项的入队和执行\n  - 实现 `max_active` 限制（控制并发执行数）\n  - 支持 flush 操作（等待所有工作完成）\n  - 统计性能指标（如启动/完成次数、CPU 时间等）\n\n- **`struct worker`**（定义在 `workqueue_internal.h`）  \n  工作线程的运行时上下文，包含状态标志（如 `WORKER_IDLE`, `WORKER_UNBOUND`）、当前执行的工作项等。\n\n### 关键枚举与常量\n\n- **池/工作线程标志**：\n  - `POOL_DISASSOCIATED`：CPU 离线时池进入非绑定状态\n  - `WORKER_UNBOUND`：工作线程可在任意 CPU 上运行\n  - `WORKER_CPU_INTENSIVE`：标记 CPU 密集型任务，影响并发控制\n\n- **配置参数**：\n  - `NR_STD_WORKER_POOLS = 2`：每 CPU 标准池数量（普通 + 高优先级）\n  - `IDLE_WORKER_TIMEOUT = 300 * HZ`：空闲线程保留时间（5 分钟）\n  - `MAYDAY_INITIAL_TIMEOUT`：工作积压时触发救援的延迟（10ms）\n\n- **统计指标**（`pool_workqueue_stats`）：\n  - `PWQ_STAT_STARTED` / `PWQ_STAT_COMPLETED`：工作项执行统计\n  - `PWQ_STAT_MAYDAY` / `PWQ_STAT_RESCUED`：紧急救援事件计数\n\n## 3. 关键实现\n\n### 工作池管理\n- **绑定池（Bound Pool）**：与特定 CPU 关联，工作线程默认绑定到该 CPU。当 CPU 离线时，池进入 `DISASSOCIATED` 状态，工作线程转为非绑定模式。\n- **非绑定池（Unbound Pool）**：动态创建，通过哈希表（`unbound_pool_hash`）按属性（`workqueue_attrs`）去重，支持跨 CPU 调度。\n- **并发控制**：通过 `nr_running` 计数器和 `max_active` 限制，防止工作项过度并发执行。\n\n### 工作线程生命周期\n- **空闲管理**：空闲线程加入 `idle_list`，超时（`IDLE_WORKER_TIMEOUT`）后被回收。\n- **动态伸缩**：当工作积压时，通过 `mayday_timer` 触发新线程创建；若创建失败，向全局救援线程（rescuer）求助。\n- **状态标志**：使用位标志（如 `WORKER_IDLE`, `WORKER_PREP`）高效管理线程状态，避免锁竞争。\n\n### 内存与同步\n- **RCU 保护**：工作池销毁通过 RCU 延迟释放，确保 `get_work_pool()` 等读取路径无锁安全。\n- **锁分层**：\n  - `pool->lock`（自旋锁）：保护池内部状态\n  - `wq_pool_mutex`：全局池管理互斥锁\n  - `wq_pool_attach_mutex`：防止 CPU 绑定状态变更冲突\n\n### 工作项调度\n- **数据指针复用**：`work_struct->data` 的高有效位存储 `pool_workqueue` 指针，低有效位用于标志位（如 `WORK_STRUCT_INACTIVE`）。\n- **优先级支持**：高优先级工作池使用 `HIGHPRI_NICE_LEVEL = MIN_NICE` 提升调度优先级。\n\n## 4. 依赖关系\n\n- **内核子系统**：\n  - **调度器**（`<linux/sched.h>`）：创建工作线程（kworker），管理 CPU 亲和性\n  - **内存管理**（`<linux/slab.h>`）：分配工作池、工作队列等结构\n  - **CPU 热插拔**（`<linux/cpu.h>`）：处理 CPU 上下线时的池绑定状态切换\n  - **RCU**（`<linux/rculist.h>`）：实现无锁读取路径\n  - **定时器**（`<linux/timer.h>`）：实现空闲超时和救援机制\n\n- **内部依赖**：\n  - `workqueue_internal.h`：定义 `struct worker` 等内部结构\n  - `Documentation/core-api/workqueue.rst`：详细设计文档\n\n## 5. 使用场景\n\n- **驱动程序延迟操作**：硬件中断后调度下半部处理（如网络包处理、磁盘 I/O 完成回调）。\n- **内核子系统异步任务**：文件系统元数据更新、内存回收、电源管理状态切换。\n- **高优先级任务**：使用 `WQ_HIGHPRI` 标志创建工作队列，确保关键任务及时执行（如死锁恢复）。\n- **CPU 密集型任务**：标记 `WQ_CPU_INTENSIVE` 避免占用过多并发槽位，提升系统响应性。\n- **NUMA 感知调度**：非绑定工作队列可指定 NUMA 节点，优化内存访问延迟。",
      "similarity": 0.5392324924468994,
      "chunks": [
        {
          "chunk_id": 20,
          "file_path": "kernel/workqueue.c",
          "start_line": 4307,
          "end_line": 4456,
          "content": [
            "static void wq_calc_pod_cpumask(struct workqueue_attrs *attrs, int cpu,",
            "\t\t\t\tint cpu_going_down)",
            "{",
            "\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);",
            "\tint pod = pt->cpu_pod[cpu];",
            "",
            "\t/* does @pod have any online CPUs @attrs wants? */",
            "\tcpumask_and(attrs->__pod_cpumask, pt->pod_cpus[pod], attrs->cpumask);",
            "\tcpumask_and(attrs->__pod_cpumask, attrs->__pod_cpumask, cpu_online_mask);",
            "\tif (cpu_going_down >= 0)",
            "\t\tcpumask_clear_cpu(cpu_going_down, attrs->__pod_cpumask);",
            "",
            "\tif (cpumask_empty(attrs->__pod_cpumask)) {",
            "\t\tcpumask_copy(attrs->__pod_cpumask, attrs->cpumask);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* yeap, return possible CPUs in @pod that @attrs wants */",
            "\tcpumask_and(attrs->__pod_cpumask, attrs->cpumask, pt->pod_cpus[pod]);",
            "",
            "\tif (cpumask_empty(attrs->__pod_cpumask))",
            "\t\tpr_warn_once(\"WARNING: workqueue cpumask: online intersect > \"",
            "\t\t\t\t\"possible intersect\\n\");",
            "}",
            "static void apply_wqattrs_cleanup(struct apply_wqattrs_ctx *ctx)",
            "{",
            "\tif (ctx) {",
            "\t\tint cpu;",
            "",
            "\t\tfor_each_possible_cpu(cpu)",
            "\t\t\tput_pwq_unlocked(ctx->pwq_tbl[cpu]);",
            "\t\tput_pwq_unlocked(ctx->dfl_pwq);",
            "",
            "\t\tfree_workqueue_attrs(ctx->attrs);",
            "",
            "\t\tkfree(ctx);",
            "\t}",
            "}",
            "static void apply_wqattrs_commit(struct apply_wqattrs_ctx *ctx)",
            "{",
            "\tint cpu;",
            "",
            "\t/* all pwqs have been created successfully, let's install'em */",
            "\tmutex_lock(&ctx->wq->mutex);",
            "",
            "\tcopy_workqueue_attrs(ctx->wq->unbound_attrs, ctx->attrs);",
            "",
            "\t/* save the previous pwq and install the new one */",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tctx->pwq_tbl[cpu] = install_unbound_pwq(ctx->wq, cpu,",
            "\t\t\t\t\t\t\tctx->pwq_tbl[cpu]);",
            "",
            "\t/* @dfl_pwq might not have been used, ensure it's linked */",
            "\tlink_pwq(ctx->dfl_pwq);",
            "\tswap(ctx->wq->dfl_pwq, ctx->dfl_pwq);",
            "",
            "\tmutex_unlock(&ctx->wq->mutex);",
            "}",
            "static int apply_workqueue_attrs_locked(struct workqueue_struct *wq,",
            "\t\t\t\t\tconst struct workqueue_attrs *attrs)",
            "{",
            "\tstruct apply_wqattrs_ctx *ctx;",
            "",
            "\t/* only unbound workqueues can change attributes */",
            "\tif (WARN_ON(!(wq->flags & WQ_UNBOUND)))",
            "\t\treturn -EINVAL;",
            "",
            "\t/* creating multiple pwqs breaks ordering guarantee */",
            "\tif (!list_empty(&wq->pwqs)) {",
            "\t\tif (WARN_ON(wq->flags & __WQ_ORDERED_EXPLICIT))",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\twq->flags &= ~__WQ_ORDERED;",
            "\t}",
            "",
            "\tctx = apply_wqattrs_prepare(wq, attrs, wq_unbound_cpumask);",
            "\tif (IS_ERR(ctx))",
            "\t\treturn PTR_ERR(ctx);",
            "",
            "\t/* the ctx has been prepared successfully, let's commit it */",
            "\tapply_wqattrs_commit(ctx);",
            "\tapply_wqattrs_cleanup(ctx);",
            "",
            "\treturn 0;",
            "}",
            "int apply_workqueue_attrs(struct workqueue_struct *wq,",
            "\t\t\t  const struct workqueue_attrs *attrs)",
            "{",
            "\tint ret;",
            "",
            "\tlockdep_assert_cpus_held();",
            "",
            "\tmutex_lock(&wq_pool_mutex);",
            "\tret = apply_workqueue_attrs_locked(wq, attrs);",
            "\tmutex_unlock(&wq_pool_mutex);",
            "",
            "\treturn ret;",
            "}",
            "static void wq_update_pod(struct workqueue_struct *wq, int cpu,",
            "\t\t\t  int hotplug_cpu, bool online)",
            "{",
            "\tint off_cpu = online ? -1 : hotplug_cpu;",
            "\tstruct pool_workqueue *old_pwq = NULL, *pwq;",
            "\tstruct workqueue_attrs *target_attrs;",
            "",
            "\tlockdep_assert_held(&wq_pool_mutex);",
            "",
            "\tif (!(wq->flags & WQ_UNBOUND) || wq->unbound_attrs->ordered)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * We don't wanna alloc/free wq_attrs for each wq for each CPU.",
            "\t * Let's use a preallocated one.  The following buf is protected by",
            "\t * CPU hotplug exclusion.",
            "\t */",
            "\ttarget_attrs = wq_update_pod_attrs_buf;",
            "",
            "\tcopy_workqueue_attrs(target_attrs, wq->unbound_attrs);",
            "\twqattrs_actualize_cpumask(target_attrs, wq_unbound_cpumask);",
            "",
            "\t/* nothing to do if the target cpumask matches the current pwq */",
            "\twq_calc_pod_cpumask(target_attrs, cpu, off_cpu);",
            "\tpwq = rcu_dereference_protected(*per_cpu_ptr(wq->cpu_pwq, cpu),",
            "\t\t\t\t\tlockdep_is_held(&wq_pool_mutex));",
            "\tif (wqattrs_equal(target_attrs, pwq->pool->attrs))",
            "\t\treturn;",
            "",
            "\t/* create a new pwq */",
            "\tpwq = alloc_unbound_pwq(wq, target_attrs);",
            "\tif (!pwq) {",
            "\t\tpr_warn(\"workqueue: allocation failed while updating CPU pod affinity of \\\"%s\\\"\\n\",",
            "\t\t\twq->name);",
            "\t\tgoto use_dfl_pwq;",
            "\t}",
            "",
            "\t/* Install the new pwq. */",
            "\tmutex_lock(&wq->mutex);",
            "\told_pwq = install_unbound_pwq(wq, cpu, pwq);",
            "\tgoto out_unlock;",
            "",
            "use_dfl_pwq:",
            "\tmutex_lock(&wq->mutex);",
            "\traw_spin_lock_irq(&wq->dfl_pwq->pool->lock);",
            "\tget_pwq(wq->dfl_pwq);",
            "\traw_spin_unlock_irq(&wq->dfl_pwq->pool->lock);",
            "\told_pwq = install_unbound_pwq(wq, cpu, wq->dfl_pwq);",
            "out_unlock:",
            "\tmutex_unlock(&wq->mutex);",
            "\tput_pwq_unlocked(old_pwq);",
            "}"
          ],
          "function_name": "wq_calc_pod_cpumask, apply_wqattrs_cleanup, apply_wqattrs_commit, apply_workqueue_attrs_locked, apply_workqueue_attrs, wq_update_pod",
          "description": "该代码块实现了工作队列属性与CPU亲和性的关联逻辑，包含wq_calc_pod_cpumask用于计算目标CPU掩码，apply_wqattrs系列函数负责应用新属性并替换原有池工作队列（PWQ），wq_update_pod处理CPU热插拔事件时更新PWQ的CPU亲和性配置。",
          "similarity": 0.5450538396835327
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/workqueue.c",
          "start_line": 1,
          "end_line": 524,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * kernel/workqueue.c - generic async execution with shared worker pool",
            " *",
            " * Copyright (C) 2002\t\tIngo Molnar",
            " *",
            " *   Derived from the taskqueue/keventd code by:",
            " *     David Woodhouse <dwmw2@infradead.org>",
            " *     Andrew Morton",
            " *     Kai Petzke <wpp@marie.physik.tu-berlin.de>",
            " *     Theodore Ts'o <tytso@mit.edu>",
            " *",
            " * Made to use alloc_percpu by Christoph Lameter.",
            " *",
            " * Copyright (C) 2010\t\tSUSE Linux Products GmbH",
            " * Copyright (C) 2010\t\tTejun Heo <tj@kernel.org>",
            " *",
            " * This is the generic async execution mechanism.  Work items as are",
            " * executed in process context.  The worker pool is shared and",
            " * automatically managed.  There are two worker pools for each CPU (one for",
            " * normal work items and the other for high priority ones) and some extra",
            " * pools for workqueues which are not bound to any specific CPU - the",
            " * number of these backing pools is dynamic.",
            " *",
            " * Please read Documentation/core-api/workqueue.rst for details.",
            " */",
            "",
            "#include <linux/export.h>",
            "#include <linux/kernel.h>",
            "#include <linux/sched.h>",
            "#include <linux/init.h>",
            "#include <linux/signal.h>",
            "#include <linux/completion.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/slab.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/freezer.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/idr.h>",
            "#include <linux/jhash.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/rculist.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/nmi.h>",
            "#include <linux/kvm_para.h>",
            "#include <linux/delay.h>",
            "",
            "#include \"workqueue_internal.h\"",
            "",
            "enum {",
            "\t/*",
            "\t * worker_pool flags",
            "\t *",
            "\t * A bound pool is either associated or disassociated with its CPU.",
            "\t * While associated (!DISASSOCIATED), all workers are bound to the",
            "\t * CPU and none has %WORKER_UNBOUND set and concurrency management",
            "\t * is in effect.",
            "\t *",
            "\t * While DISASSOCIATED, the cpu may be offline and all workers have",
            "\t * %WORKER_UNBOUND set and concurrency management disabled, and may",
            "\t * be executing on any CPU.  The pool behaves as an unbound one.",
            "\t *",
            "\t * Note that DISASSOCIATED should be flipped only while holding",
            "\t * wq_pool_attach_mutex to avoid changing binding state while",
            "\t * worker_attach_to_pool() is in progress.",
            "\t */",
            "\tPOOL_MANAGER_ACTIVE\t= 1 << 0,\t/* being managed */",
            "\tPOOL_DISASSOCIATED\t= 1 << 2,\t/* cpu can't serve workers */",
            "",
            "\t/* worker flags */",
            "\tWORKER_DIE\t\t= 1 << 1,\t/* die die die */",
            "\tWORKER_IDLE\t\t= 1 << 2,\t/* is idle */",
            "\tWORKER_PREP\t\t= 1 << 3,\t/* preparing to run works */",
            "\tWORKER_CPU_INTENSIVE\t= 1 << 6,\t/* cpu intensive */",
            "\tWORKER_UNBOUND\t\t= 1 << 7,\t/* worker is unbound */",
            "\tWORKER_REBOUND\t\t= 1 << 8,\t/* worker was rebound */",
            "",
            "\tWORKER_NOT_RUNNING\t= WORKER_PREP | WORKER_CPU_INTENSIVE |",
            "\t\t\t\t  WORKER_UNBOUND | WORKER_REBOUND,",
            "",
            "\tNR_STD_WORKER_POOLS\t= 2,\t\t/* # standard pools per cpu */",
            "",
            "\tUNBOUND_POOL_HASH_ORDER\t= 6,\t\t/* hashed by pool->attrs */",
            "\tBUSY_WORKER_HASH_ORDER\t= 6,\t\t/* 64 pointers */",
            "",
            "\tMAX_IDLE_WORKERS_RATIO\t= 4,\t\t/* 1/4 of busy can be idle */",
            "\tIDLE_WORKER_TIMEOUT\t= 300 * HZ,\t/* keep idle ones for 5 mins */",
            "",
            "\tMAYDAY_INITIAL_TIMEOUT  = HZ / 100 >= 2 ? HZ / 100 : 2,",
            "\t\t\t\t\t\t/* call for help after 10ms",
            "\t\t\t\t\t\t   (min two ticks) */",
            "\tMAYDAY_INTERVAL\t\t= HZ / 10,\t/* and then every 100ms */",
            "\tCREATE_COOLDOWN\t\t= HZ,\t\t/* time to breath after fail */",
            "",
            "\t/*",
            "\t * Rescue workers are used only on emergencies and shared by",
            "\t * all cpus.  Give MIN_NICE.",
            "\t */",
            "\tRESCUER_NICE_LEVEL\t= MIN_NICE,",
            "\tHIGHPRI_NICE_LEVEL\t= MIN_NICE,",
            "",
            "\tWQ_NAME_LEN\t\t= 24,",
            "};",
            "",
            "/*",
            " * Structure fields follow one of the following exclusion rules.",
            " *",
            " * I: Modifiable by initialization/destruction paths and read-only for",
            " *    everyone else.",
            " *",
            " * P: Preemption protected.  Disabling preemption is enough and should",
            " *    only be modified and accessed from the local cpu.",
            " *",
            " * L: pool->lock protected.  Access with pool->lock held.",
            " *",
            " * K: Only modified by worker while holding pool->lock. Can be safely read by",
            " *    self, while holding pool->lock or from IRQ context if %current is the",
            " *    kworker.",
            " *",
            " * S: Only modified by worker self.",
            " *",
            " * A: wq_pool_attach_mutex protected.",
            " *",
            " * PL: wq_pool_mutex protected.",
            " *",
            " * PR: wq_pool_mutex protected for writes.  RCU protected for reads.",
            " *",
            " * PW: wq_pool_mutex and wq->mutex protected for writes.  Either for reads.",
            " *",
            " * PWR: wq_pool_mutex and wq->mutex protected for writes.  Either or",
            " *      RCU for reads.",
            " *",
            " * WQ: wq->mutex protected.",
            " *",
            " * WR: wq->mutex protected for writes.  RCU protected for reads.",
            " *",
            " * MD: wq_mayday_lock protected.",
            " *",
            " * WD: Used internally by the watchdog.",
            " */",
            "",
            "/* struct worker is defined in workqueue_internal.h */",
            "",
            "struct worker_pool {",
            "\traw_spinlock_t\t\tlock;\t\t/* the pool lock */",
            "\tint\t\t\tcpu;\t\t/* I: the associated cpu */",
            "\tint\t\t\tnode;\t\t/* I: the associated node ID */",
            "\tint\t\t\tid;\t\t/* I: pool ID */",
            "\tunsigned int\t\tflags;\t\t/* L: flags */",
            "",
            "\tunsigned long\t\twatchdog_ts;\t/* L: watchdog timestamp */",
            "\tbool\t\t\tcpu_stall;\t/* WD: stalled cpu bound pool */",
            "",
            "\t/*",
            "\t * The counter is incremented in a process context on the associated CPU",
            "\t * w/ preemption disabled, and decremented or reset in the same context",
            "\t * but w/ pool->lock held. The readers grab pool->lock and are",
            "\t * guaranteed to see if the counter reached zero.",
            "\t */",
            "\tint\t\t\tnr_running;",
            "",
            "\tstruct list_head\tworklist;\t/* L: list of pending works */",
            "",
            "\tint\t\t\tnr_workers;\t/* L: total number of workers */",
            "\tint\t\t\tnr_idle;\t/* L: currently idle workers */",
            "",
            "\tstruct list_head\tidle_list;\t/* L: list of idle workers */",
            "\tstruct timer_list\tidle_timer;\t/* L: worker idle timeout */",
            "\tstruct work_struct      idle_cull_work; /* L: worker idle cleanup */",
            "",
            "\tstruct timer_list\tmayday_timer;\t  /* L: SOS timer for workers */",
            "",
            "\t/* a workers is either on busy_hash or idle_list, or the manager */",
            "\tDECLARE_HASHTABLE(busy_hash, BUSY_WORKER_HASH_ORDER);",
            "\t\t\t\t\t\t/* L: hash of busy workers */",
            "",
            "\tstruct worker\t\t*manager;\t/* L: purely informational */",
            "\tstruct list_head\tworkers;\t/* A: attached workers */",
            "\tstruct list_head        dying_workers;  /* A: workers about to die */",
            "\tstruct completion\t*detach_completion; /* all workers detached */",
            "",
            "\tstruct ida\t\tworker_ida;\t/* worker IDs for task name */",
            "",
            "\tstruct workqueue_attrs\t*attrs;\t\t/* I: worker attributes */",
            "\tstruct hlist_node\thash_node;\t/* PL: unbound_pool_hash node */",
            "\tint\t\t\trefcnt;\t\t/* PL: refcnt for unbound pools */",
            "",
            "\t/*",
            "\t * Destruction of pool is RCU protected to allow dereferences",
            "\t * from get_work_pool().",
            "\t */",
            "\tstruct rcu_head\t\trcu;",
            "};",
            "",
            "/*",
            " * Per-pool_workqueue statistics. These can be monitored using",
            " * tools/workqueue/wq_monitor.py.",
            " */",
            "enum pool_workqueue_stats {",
            "\tPWQ_STAT_STARTED,\t/* work items started execution */",
            "\tPWQ_STAT_COMPLETED,\t/* work items completed execution */",
            "\tPWQ_STAT_CPU_TIME,\t/* total CPU time consumed */",
            "\tPWQ_STAT_CPU_INTENSIVE,\t/* wq_cpu_intensive_thresh_us violations */",
            "\tPWQ_STAT_CM_WAKEUP,\t/* concurrency-management worker wakeups */",
            "\tPWQ_STAT_REPATRIATED,\t/* unbound workers brought back into scope */",
            "\tPWQ_STAT_MAYDAY,\t/* maydays to rescuer */",
            "\tPWQ_STAT_RESCUED,\t/* linked work items executed by rescuer */",
            "",
            "\tPWQ_NR_STATS,",
            "};",
            "",
            "/*",
            " * The per-pool workqueue.  While queued, the lower WORK_STRUCT_FLAG_BITS",
            " * of work_struct->data are used for flags and the remaining high bits",
            " * point to the pwq; thus, pwqs need to be aligned at two's power of the",
            " * number of flag bits.",
            " */",
            "struct pool_workqueue {",
            "\tstruct worker_pool\t*pool;\t\t/* I: the associated pool */",
            "\tstruct workqueue_struct *wq;\t\t/* I: the owning workqueue */",
            "\tint\t\t\twork_color;\t/* L: current color */",
            "\tint\t\t\tflush_color;\t/* L: flushing color */",
            "\tint\t\t\trefcnt;\t\t/* L: reference count */",
            "\tint\t\t\tnr_in_flight[WORK_NR_COLORS];",
            "\t\t\t\t\t\t/* L: nr of in_flight works */",
            "",
            "\t/*",
            "\t * nr_active management and WORK_STRUCT_INACTIVE:",
            "\t *",
            "\t * When pwq->nr_active >= max_active, new work item is queued to",
            "\t * pwq->inactive_works instead of pool->worklist and marked with",
            "\t * WORK_STRUCT_INACTIVE.",
            "\t *",
            "\t * All work items marked with WORK_STRUCT_INACTIVE do not participate",
            "\t * in pwq->nr_active and all work items in pwq->inactive_works are",
            "\t * marked with WORK_STRUCT_INACTIVE.  But not all WORK_STRUCT_INACTIVE",
            "\t * work items are in pwq->inactive_works.  Some of them are ready to",
            "\t * run in pool->worklist or worker->scheduled.  Those work itmes are",
            "\t * only struct wq_barrier which is used for flush_work() and should",
            "\t * not participate in pwq->nr_active.  For non-barrier work item, it",
            "\t * is marked with WORK_STRUCT_INACTIVE iff it is in pwq->inactive_works.",
            "\t */",
            "\tint\t\t\tnr_active;\t/* L: nr of active works */",
            "\tint\t\t\tmax_active;\t/* L: max active works */",
            "\tstruct list_head\tinactive_works;\t/* L: inactive works */",
            "\tstruct list_head\tpwqs_node;\t/* WR: node on wq->pwqs */",
            "\tstruct list_head\tmayday_node;\t/* MD: node on wq->maydays */",
            "",
            "\tu64\t\t\tstats[PWQ_NR_STATS];",
            "",
            "\t/*",
            "\t * Release of unbound pwq is punted to a kthread_worker. See put_pwq()",
            "\t * and pwq_release_workfn() for details. pool_workqueue itself is also",
            "\t * RCU protected so that the first pwq can be determined without",
            "\t * grabbing wq->mutex.",
            "\t */",
            "\tstruct kthread_work\trelease_work;",
            "\tstruct rcu_head\t\trcu;",
            "} __aligned(1 << WORK_STRUCT_FLAG_BITS);",
            "",
            "/*",
            " * Structure used to wait for workqueue flush.",
            " */",
            "struct wq_flusher {",
            "\tstruct list_head\tlist;\t\t/* WQ: list of flushers */",
            "\tint\t\t\tflush_color;\t/* WQ: flush color waiting for */",
            "\tstruct completion\tdone;\t\t/* flush completion */",
            "};",
            "",
            "struct wq_device;",
            "",
            "/*",
            " * The externally visible workqueue.  It relays the issued work items to",
            " * the appropriate worker_pool through its pool_workqueues.",
            " */",
            "struct workqueue_struct {",
            "\tstruct list_head\tpwqs;\t\t/* WR: all pwqs of this wq */",
            "\tstruct list_head\tlist;\t\t/* PR: list of all workqueues */",
            "",
            "\tstruct mutex\t\tmutex;\t\t/* protects this wq */",
            "\tint\t\t\twork_color;\t/* WQ: current work color */",
            "\tint\t\t\tflush_color;\t/* WQ: current flush color */",
            "\tatomic_t\t\tnr_pwqs_to_flush; /* flush in progress */",
            "\tstruct wq_flusher\t*first_flusher;\t/* WQ: first flusher */",
            "\tstruct list_head\tflusher_queue;\t/* WQ: flush waiters */",
            "\tstruct list_head\tflusher_overflow; /* WQ: flush overflow list */",
            "",
            "\tstruct list_head\tmaydays;\t/* MD: pwqs requesting rescue */",
            "\tstruct worker\t\t*rescuer;\t/* MD: rescue worker */",
            "",
            "\tint\t\t\tnr_drainers;\t/* WQ: drain in progress */",
            "\tint\t\t\tsaved_max_active; /* WQ: saved pwq max_active */",
            "",
            "\tstruct workqueue_attrs\t*unbound_attrs;\t/* PW: only for unbound wqs */",
            "\tstruct pool_workqueue\t*dfl_pwq;\t/* PW: only for unbound wqs */",
            "",
            "#ifdef CONFIG_SYSFS",
            "\tstruct wq_device\t*wq_dev;\t/* I: for sysfs interface */",
            "#endif",
            "#ifdef CONFIG_LOCKDEP",
            "\tchar\t\t\t*lock_name;",
            "\tstruct lock_class_key\tkey;",
            "\tstruct lockdep_map\tlockdep_map;",
            "#endif",
            "\tchar\t\t\tname[WQ_NAME_LEN]; /* I: workqueue name */",
            "",
            "\t/*",
            "\t * Destruction of workqueue_struct is RCU protected to allow walking",
            "\t * the workqueues list without grabbing wq_pool_mutex.",
            "\t * This is used to dump all workqueues from sysrq.",
            "\t */",
            "\tstruct rcu_head\t\trcu;",
            "",
            "\t/* hot fields used during command issue, aligned to cacheline */",
            "\tunsigned int\t\tflags ____cacheline_aligned; /* WQ: WQ_* flags */",
            "\tstruct pool_workqueue __percpu __rcu **cpu_pwq; /* I: per-cpu pwqs */",
            "};",
            "",
            "static struct kmem_cache *pwq_cache;",
            "",
            "/*",
            " * Each pod type describes how CPUs should be grouped for unbound workqueues.",
            " * See the comment above workqueue_attrs->affn_scope.",
            " */",
            "struct wq_pod_type {",
            "\tint\t\t\tnr_pods;\t/* number of pods */",
            "\tcpumask_var_t\t\t*pod_cpus;\t/* pod -> cpus */",
            "\tint\t\t\t*pod_node;\t/* pod -> node */",
            "\tint\t\t\t*cpu_pod;\t/* cpu -> pod */",
            "};",
            "",
            "static struct wq_pod_type wq_pod_types[WQ_AFFN_NR_TYPES];",
            "static enum wq_affn_scope wq_affn_dfl = WQ_AFFN_CACHE;",
            "",
            "static const char *wq_affn_names[WQ_AFFN_NR_TYPES] = {",
            "\t[WQ_AFFN_DFL]\t\t\t= \"default\",",
            "\t[WQ_AFFN_CPU]\t\t\t= \"cpu\",",
            "\t[WQ_AFFN_SMT]\t\t\t= \"smt\",",
            "\t[WQ_AFFN_CACHE]\t\t\t= \"cache\",",
            "\t[WQ_AFFN_NUMA]\t\t\t= \"numa\",",
            "\t[WQ_AFFN_SYSTEM]\t\t= \"system\",",
            "};",
            "",
            "/*",
            " * Per-cpu work items which run for longer than the following threshold are",
            " * automatically considered CPU intensive and excluded from concurrency",
            " * management to prevent them from noticeably delaying other per-cpu work items.",
            " * ULONG_MAX indicates that the user hasn't overridden it with a boot parameter.",
            " * The actual value is initialized in wq_cpu_intensive_thresh_init().",
            " */",
            "static unsigned long wq_cpu_intensive_thresh_us = ULONG_MAX;",
            "module_param_named(cpu_intensive_thresh_us, wq_cpu_intensive_thresh_us, ulong, 0644);",
            "",
            "/* see the comment above the definition of WQ_POWER_EFFICIENT */",
            "static bool wq_power_efficient = IS_ENABLED(CONFIG_WQ_POWER_EFFICIENT_DEFAULT);",
            "module_param_named(power_efficient, wq_power_efficient, bool, 0444);",
            "",
            "static bool wq_online;\t\t\t/* can kworkers be created yet? */",
            "",
            "/* buf for wq_update_unbound_pod_attrs(), protected by CPU hotplug exclusion */",
            "static struct workqueue_attrs *wq_update_pod_attrs_buf;",
            "",
            "static DEFINE_MUTEX(wq_pool_mutex);\t/* protects pools and workqueues list */",
            "static DEFINE_MUTEX(wq_pool_attach_mutex); /* protects worker attach/detach */",
            "static DEFINE_RAW_SPINLOCK(wq_mayday_lock);\t/* protects wq->maydays list */",
            "/* wait for manager to go away */",
            "static struct rcuwait manager_wait = __RCUWAIT_INITIALIZER(manager_wait);",
            "",
            "static LIST_HEAD(workqueues);\t\t/* PR: list of all workqueues */",
            "static bool workqueue_freezing;\t\t/* PL: have wqs started freezing? */",
            "",
            "/* PL&A: allowable cpus for unbound wqs and work items */",
            "static cpumask_var_t wq_unbound_cpumask;",
            "",
            "/* PL: user requested unbound cpumask via sysfs */",
            "static cpumask_var_t wq_requested_unbound_cpumask;",
            "",
            "/* PL: isolated cpumask to be excluded from unbound cpumask */",
            "static cpumask_var_t wq_isolated_cpumask;",
            "",
            "/* for further constrain wq_unbound_cpumask by cmdline parameter*/",
            "static struct cpumask wq_cmdline_cpumask __initdata;",
            "",
            "/* CPU where unbound work was last round robin scheduled from this CPU */",
            "static DEFINE_PER_CPU(int, wq_rr_cpu_last);",
            "",
            "/*",
            " * Local execution of unbound work items is no longer guaranteed.  The",
            " * following always forces round-robin CPU selection on unbound work items",
            " * to uncover usages which depend on it.",
            " */",
            "#ifdef CONFIG_DEBUG_WQ_FORCE_RR_CPU",
            "static bool wq_debug_force_rr_cpu = true;",
            "#else",
            "static bool wq_debug_force_rr_cpu = false;",
            "#endif",
            "module_param_named(debug_force_rr_cpu, wq_debug_force_rr_cpu, bool, 0644);",
            "",
            "/* the per-cpu worker pools */",
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS], cpu_worker_pools);",
            "",
            "static DEFINE_IDR(worker_pool_idr);\t/* PR: idr of all pools */",
            "",
            "/* PL: hash of all unbound pools keyed by pool->attrs */",
            "static DEFINE_HASHTABLE(unbound_pool_hash, UNBOUND_POOL_HASH_ORDER);",
            "",
            "/* I: attributes used when instantiating standard unbound pools on demand */",
            "static struct workqueue_attrs *unbound_std_wq_attrs[NR_STD_WORKER_POOLS];",
            "",
            "/* I: attributes used when instantiating ordered pools on demand */",
            "static struct workqueue_attrs *ordered_wq_attrs[NR_STD_WORKER_POOLS];",
            "",
            "/*",
            " * I: kthread_worker to release pwq's. pwq release needs to be bounced to a",
            " * process context while holding a pool lock. Bounce to a dedicated kthread",
            " * worker to avoid A-A deadlocks.",
            " */",
            "static struct kthread_worker *pwq_release_worker __ro_after_init;",
            "",
            "struct workqueue_struct *system_wq __ro_after_init;",
            "EXPORT_SYMBOL(system_wq);",
            "struct workqueue_struct *system_highpri_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_highpri_wq);",
            "struct workqueue_struct *system_long_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_long_wq);",
            "struct workqueue_struct *system_unbound_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_unbound_wq);",
            "struct workqueue_struct *system_freezable_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_freezable_wq);",
            "struct workqueue_struct *system_power_efficient_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_power_efficient_wq);",
            "struct workqueue_struct *system_freezable_power_efficient_wq __ro_after_init;",
            "EXPORT_SYMBOL_GPL(system_freezable_power_efficient_wq);",
            "",
            "static int worker_thread(void *__worker);",
            "static void workqueue_sysfs_unregister(struct workqueue_struct *wq);",
            "static void show_pwq(struct pool_workqueue *pwq);",
            "static void show_one_worker_pool(struct worker_pool *pool);",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/workqueue.h>",
            "",
            "#define assert_rcu_or_pool_mutex()\t\t\t\t\t\\",
            "\tRCU_LOCKDEP_WARN(!rcu_read_lock_held() &&\t\t\t\\",
            "\t\t\t !lockdep_is_held(&wq_pool_mutex),\t\t\\",
            "\t\t\t \"RCU or wq_pool_mutex should be held\")",
            "",
            "#define assert_rcu_or_wq_mutex_or_pool_mutex(wq)\t\t\t\\",
            "\tRCU_LOCKDEP_WARN(!rcu_read_lock_held() &&\t\t\t\\",
            "\t\t\t !lockdep_is_held(&wq->mutex) &&\t\t\\",
            "\t\t\t !lockdep_is_held(&wq_pool_mutex),\t\t\\",
            "\t\t\t \"RCU, wq->mutex or wq_pool_mutex should be held\")",
            "",
            "#define for_each_cpu_worker_pool(pool, cpu)\t\t\t\t\\",
            "\tfor ((pool) = &per_cpu(cpu_worker_pools, cpu)[0];\t\t\\",
            "\t     (pool) < &per_cpu(cpu_worker_pools, cpu)[NR_STD_WORKER_POOLS]; \\",
            "\t     (pool)++)",
            "",
            "/**",
            " * for_each_pool - iterate through all worker_pools in the system",
            " * @pool: iteration cursor",
            " * @pi: integer used for iteration",
            " *",
            " * This must be called either with wq_pool_mutex held or RCU read",
            " * locked.  If the pool needs to be used beyond the locking in effect, the",
            " * caller is responsible for guaranteeing that the pool stays online.",
            " *",
            " * The if/else clause exists only for the lockdep assertion and can be",
            " * ignored.",
            " */",
            "#define for_each_pool(pool, pi)\t\t\t\t\t\t\\",
            "\tidr_for_each_entry(&worker_pool_idr, pool, pi)\t\t\t\\",
            "\t\tif (({ assert_rcu_or_pool_mutex(); false; })) { }\t\\",
            "\t\telse",
            "",
            "/**",
            " * for_each_pool_worker - iterate through all workers of a worker_pool",
            " * @worker: iteration cursor",
            " * @pool: worker_pool to iterate workers of",
            " *",
            " * This must be called with wq_pool_attach_mutex.",
            " *",
            " * The if/else clause exists only for the lockdep assertion and can be",
            " * ignored.",
            " */",
            "#define for_each_pool_worker(worker, pool)\t\t\t\t\\",
            "\tlist_for_each_entry((worker), &(pool)->workers, node)\t\t\\",
            "\t\tif (({ lockdep_assert_held(&wq_pool_attach_mutex); false; })) { } \\",
            "\t\telse",
            "",
            "/**",
            " * for_each_pwq - iterate through all pool_workqueues of the specified workqueue",
            " * @pwq: iteration cursor",
            " * @wq: the target workqueue",
            " *",
            " * This must be called either with wq->mutex held or RCU read locked.",
            " * If the pwq needs to be used beyond the locking in effect, the caller is",
            " * responsible for guaranteeing that the pwq stays online.",
            " *",
            " * The if/else clause exists only for the lockdep assertion and can be",
            " * ignored.",
            " */",
            "#define for_each_pwq(pwq, wq)\t\t\t\t\t\t\\",
            "\tlist_for_each_entry_rcu((pwq), &(wq)->pwqs, pwqs_node,\t\t\\",
            "\t\t\t\t lockdep_is_held(&(wq->mutex)))",
            "",
            "#ifdef CONFIG_DEBUG_OBJECTS_WORK",
            "",
            "static const struct debug_obj_descr work_debug_descr;",
            "",
            "static void *work_debug_hint(void *addr)",
            "{",
            "\treturn ((struct work_struct *) addr)->func;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义工作队列核心数据结构和全局变量，包括worker_pool和pool_workqueue结构体，用于管理工作者线程池及其属性，包含池标志位、工作项统计信息、锁保护字段和RCU相关成员，为异步任务调度提供基础设施。",
          "similarity": 0.5390905141830444
        },
        {
          "chunk_id": 18,
          "file_path": "kernel/workqueue.c",
          "start_line": 3845,
          "end_line": 4008,
          "content": [
            "static void wqattrs_actualize_cpumask(struct workqueue_attrs *attrs,",
            "\t\t\t\t      const cpumask_t *unbound_cpumask)",
            "{",
            "\t/*",
            "\t * Calculate the effective CPU mask of @attrs given @unbound_cpumask. If",
            "\t * @attrs->cpumask doesn't overlap with @unbound_cpumask, we fallback to",
            "\t * @unbound_cpumask.",
            "\t */",
            "\tcpumask_and(attrs->cpumask, attrs->cpumask, unbound_cpumask);",
            "\tif (unlikely(cpumask_empty(attrs->cpumask)))",
            "\t\tcpumask_copy(attrs->cpumask, unbound_cpumask);",
            "}",
            "static int init_worker_pool(struct worker_pool *pool)",
            "{",
            "\traw_spin_lock_init(&pool->lock);",
            "\tpool->id = -1;",
            "\tpool->cpu = -1;",
            "\tpool->node = NUMA_NO_NODE;",
            "\tpool->flags |= POOL_DISASSOCIATED;",
            "\tpool->watchdog_ts = jiffies;",
            "\tINIT_LIST_HEAD(&pool->worklist);",
            "\tINIT_LIST_HEAD(&pool->idle_list);",
            "\thash_init(pool->busy_hash);",
            "",
            "\ttimer_setup(&pool->idle_timer, idle_worker_timeout, TIMER_DEFERRABLE);",
            "\tINIT_WORK(&pool->idle_cull_work, idle_cull_fn);",
            "",
            "\ttimer_setup(&pool->mayday_timer, pool_mayday_timeout, 0);",
            "",
            "\tINIT_LIST_HEAD(&pool->workers);",
            "\tINIT_LIST_HEAD(&pool->dying_workers);",
            "",
            "\tida_init(&pool->worker_ida);",
            "\tINIT_HLIST_NODE(&pool->hash_node);",
            "\tpool->refcnt = 1;",
            "",
            "\t/* shouldn't fail above this point */",
            "\tpool->attrs = alloc_workqueue_attrs();",
            "\tif (!pool->attrs)",
            "\t\treturn -ENOMEM;",
            "",
            "\twqattrs_clear_for_pool(pool->attrs);",
            "",
            "\treturn 0;",
            "}",
            "static void wq_init_lockdep(struct workqueue_struct *wq)",
            "{",
            "\tchar *lock_name;",
            "",
            "\tlockdep_register_key(&wq->key);",
            "\tlock_name = kasprintf(GFP_KERNEL, \"%s%s\", \"(wq_completion)\", wq->name);",
            "\tif (!lock_name)",
            "\t\tlock_name = wq->name;",
            "",
            "\twq->lock_name = lock_name;",
            "\tlockdep_init_map(&wq->lockdep_map, lock_name, &wq->key, 0);",
            "}",
            "static void wq_unregister_lockdep(struct workqueue_struct *wq)",
            "{",
            "\tlockdep_unregister_key(&wq->key);",
            "}",
            "static void wq_free_lockdep(struct workqueue_struct *wq)",
            "{",
            "\tif (wq->lock_name != wq->name)",
            "\t\tkfree(wq->lock_name);",
            "}",
            "static void wq_init_lockdep(struct workqueue_struct *wq)",
            "{",
            "}",
            "static void wq_unregister_lockdep(struct workqueue_struct *wq)",
            "{",
            "}",
            "static void wq_free_lockdep(struct workqueue_struct *wq)",
            "{",
            "}",
            "static void rcu_free_wq(struct rcu_head *rcu)",
            "{",
            "\tstruct workqueue_struct *wq =",
            "\t\tcontainer_of(rcu, struct workqueue_struct, rcu);",
            "",
            "\twq_free_lockdep(wq);",
            "\tfree_percpu(wq->cpu_pwq);",
            "\tfree_workqueue_attrs(wq->unbound_attrs);",
            "\tkfree(wq);",
            "}",
            "static void rcu_free_pool(struct rcu_head *rcu)",
            "{",
            "\tstruct worker_pool *pool = container_of(rcu, struct worker_pool, rcu);",
            "",
            "\tida_destroy(&pool->worker_ida);",
            "\tfree_workqueue_attrs(pool->attrs);",
            "\tkfree(pool);",
            "}",
            "static void put_unbound_pool(struct worker_pool *pool)",
            "{",
            "\tDECLARE_COMPLETION_ONSTACK(detach_completion);",
            "\tstruct worker *worker;",
            "\tLIST_HEAD(cull_list);",
            "",
            "\tlockdep_assert_held(&wq_pool_mutex);",
            "",
            "\tif (--pool->refcnt)",
            "\t\treturn;",
            "",
            "\t/* sanity checks */",
            "\tif (WARN_ON(!(pool->cpu < 0)) ||",
            "\t    WARN_ON(!list_empty(&pool->worklist)))",
            "\t\treturn;",
            "",
            "\t/* release id and unhash */",
            "\tif (pool->id >= 0)",
            "\t\tidr_remove(&worker_pool_idr, pool->id);",
            "\thash_del(&pool->hash_node);",
            "",
            "\t/*",
            "\t * Become the manager and destroy all workers.  This prevents",
            "\t * @pool's workers from blocking on attach_mutex.  We're the last",
            "\t * manager and @pool gets freed with the flag set.",
            "\t *",
            "\t * Having a concurrent manager is quite unlikely to happen as we can",
            "\t * only get here with",
            "\t *   pwq->refcnt == pool->refcnt == 0",
            "\t * which implies no work queued to the pool, which implies no worker can",
            "\t * become the manager. However a worker could have taken the role of",
            "\t * manager before the refcnts dropped to 0, since maybe_create_worker()",
            "\t * drops pool->lock",
            "\t */",
            "\twhile (true) {",
            "\t\trcuwait_wait_event(&manager_wait,",
            "\t\t\t\t   !(pool->flags & POOL_MANAGER_ACTIVE),",
            "\t\t\t\t   TASK_UNINTERRUPTIBLE);",
            "",
            "\t\tmutex_lock(&wq_pool_attach_mutex);",
            "\t\traw_spin_lock_irq(&pool->lock);",
            "\t\tif (!(pool->flags & POOL_MANAGER_ACTIVE)) {",
            "\t\t\tpool->flags |= POOL_MANAGER_ACTIVE;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\traw_spin_unlock_irq(&pool->lock);",
            "\t\tmutex_unlock(&wq_pool_attach_mutex);",
            "\t}",
            "",
            "\twhile ((worker = first_idle_worker(pool)))",
            "\t\tset_worker_dying(worker, &cull_list);",
            "\tWARN_ON(pool->nr_workers || pool->nr_idle);",
            "\traw_spin_unlock_irq(&pool->lock);",
            "",
            "\twake_dying_workers(&cull_list);",
            "",
            "\tif (!list_empty(&pool->workers) || !list_empty(&pool->dying_workers))",
            "\t\tpool->detach_completion = &detach_completion;",
            "\tmutex_unlock(&wq_pool_attach_mutex);",
            "",
            "\tif (pool->detach_completion)",
            "\t\twait_for_completion(pool->detach_completion);",
            "",
            "\t/* shut down the timers */",
            "\tdel_timer_sync(&pool->idle_timer);",
            "\tcancel_work_sync(&pool->idle_cull_work);",
            "\tdel_timer_sync(&pool->mayday_timer);",
            "",
            "\t/* RCU protected to allow dereferences from get_work_pool() */",
            "\tcall_rcu(&pool->rcu, rcu_free_pool);",
            "}"
          ],
          "function_name": "wqattrs_actualize_cpumask, init_worker_pool, wq_init_lockdep, wq_unregister_lockdep, wq_free_lockdep, wq_init_lockdep, wq_unregister_lockdep, wq_free_lockdep, rcu_free_wq, rcu_free_pool, put_unbound_pool",
          "description": "init_worker_pool 初始化工作池结构体，wq_init_lockdep 注册锁依赖信息。rcu_free_wq 和 rcu_free_pool 使用 RCU 机制安全释放工作队列和池资源，put_unbound_pool 处理无绑定池的引用计数和销毁流程。",
          "similarity": 0.5365132093429565
        },
        {
          "chunk_id": 9,
          "file_path": "kernel/workqueue.c",
          "start_line": 1982,
          "end_line": 2082,
          "content": [
            "bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,",
            "\t\t\t   struct delayed_work *dwork, unsigned long delay)",
            "{",
            "\tstruct work_struct *work = &dwork->work;",
            "\tbool ret = false;",
            "\tunsigned long flags;",
            "",
            "\t/* read the comment in __queue_work() */",
            "\tlocal_irq_save(flags);",
            "",
            "\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {",
            "\t\t__queue_delayed_work(cpu, wq, dwork, delay);",
            "\t\tret = true;",
            "\t}",
            "",
            "\tlocal_irq_restore(flags);",
            "\treturn ret;",
            "}",
            "bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,",
            "\t\t\t struct delayed_work *dwork, unsigned long delay)",
            "{",
            "\tunsigned long flags;",
            "\tint ret;",
            "",
            "\tdo {",
            "\t\tret = try_to_grab_pending(&dwork->work, true, &flags);",
            "\t} while (unlikely(ret == -EAGAIN));",
            "",
            "\tif (likely(ret >= 0)) {",
            "\t\t__queue_delayed_work(cpu, wq, dwork, delay);",
            "\t\tlocal_irq_restore(flags);",
            "\t}",
            "",
            "\t/* -ENOENT from try_to_grab_pending() becomes %true */",
            "\treturn ret;",
            "}",
            "static void rcu_work_rcufn(struct rcu_head *rcu)",
            "{",
            "\tstruct rcu_work *rwork = container_of(rcu, struct rcu_work, rcu);",
            "",
            "\t/* read the comment in __queue_work() */",
            "\tlocal_irq_disable();",
            "\t__queue_work(WORK_CPU_UNBOUND, rwork->wq, &rwork->work);",
            "\tlocal_irq_enable();",
            "}",
            "bool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork)",
            "{",
            "\tstruct work_struct *work = &rwork->work;",
            "",
            "\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {",
            "\t\trwork->wq = wq;",
            "\t\tcall_rcu_hurry(&rwork->rcu, rcu_work_rcufn);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}",
            "static void worker_attach_to_pool(struct worker *worker,",
            "\t\t\t\t   struct worker_pool *pool)",
            "{",
            "\tmutex_lock(&wq_pool_attach_mutex);",
            "",
            "\t/*",
            "\t * The wq_pool_attach_mutex ensures %POOL_DISASSOCIATED remains",
            "\t * stable across this function.  See the comments above the flag",
            "\t * definition for details.",
            "\t */",
            "\tif (pool->flags & POOL_DISASSOCIATED)",
            "\t\tworker->flags |= WORKER_UNBOUND;",
            "\telse",
            "\t\tkthread_set_per_cpu(worker->task, pool->cpu);",
            "",
            "\tif (worker->rescue_wq)",
            "\t\tset_cpus_allowed_ptr(worker->task, pool_allowed_cpus(pool));",
            "",
            "\tlist_add_tail(&worker->node, &pool->workers);",
            "\tworker->pool = pool;",
            "",
            "\tmutex_unlock(&wq_pool_attach_mutex);",
            "}",
            "static void worker_detach_from_pool(struct worker *worker)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "\tstruct completion *detach_completion = NULL;",
            "",
            "\tmutex_lock(&wq_pool_attach_mutex);",
            "",
            "\tkthread_set_per_cpu(worker->task, -1);",
            "\tlist_del(&worker->node);",
            "\tworker->pool = NULL;",
            "",
            "\tif (list_empty(&pool->workers) && list_empty(&pool->dying_workers))",
            "\t\tdetach_completion = pool->detach_completion;",
            "\tmutex_unlock(&wq_pool_attach_mutex);",
            "",
            "\t/* clear leftover flags without pool->lock after it is detached */",
            "\tworker->flags &= ~(WORKER_UNBOUND | WORKER_REBOUND);",
            "",
            "\tif (detach_completion)",
            "\t\tcomplete(detach_completion);",
            "}"
          ],
          "function_name": "queue_delayed_work_on, mod_delayed_work_on, rcu_work_rcufn, queue_rcu_work, worker_attach_to_pool, worker_detach_from_pool",
          "description": "该代码块管理RCU安全的工作队列操作。queue_delayed_work_on/mod_delayed_work_on控制延迟工作提交；rcu_work_rcufn处理RCU回调；worker_attach_to_pool/detach_from_pool管理worker与worker池的绑定关系。",
          "similarity": 0.5303465127944946
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/workqueue.c",
          "start_line": 692,
          "end_line": 797,
          "content": [
            "static void set_work_pool_and_clear_pending(struct work_struct *work,",
            "\t\t\t\t\t    int pool_id)",
            "{",
            "\t/*",
            "\t * The following wmb is paired with the implied mb in",
            "\t * test_and_set_bit(PENDING) and ensures all updates to @work made",
            "\t * here are visible to and precede any updates by the next PENDING",
            "\t * owner.",
            "\t */",
            "\tsmp_wmb();",
            "\tset_work_data(work, (unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT, 0);",
            "\t/*",
            "\t * The following mb guarantees that previous clear of a PENDING bit",
            "\t * will not be reordered with any speculative LOADS or STORES from",
            "\t * work->current_func, which is executed afterwards.  This possible",
            "\t * reordering can lead to a missed execution on attempt to queue",
            "\t * the same @work.  E.g. consider this case:",
            "\t *",
            "\t *   CPU#0                         CPU#1",
            "\t *   ----------------------------  --------------------------------",
            "\t *",
            "\t * 1  STORE event_indicated",
            "\t * 2  queue_work_on() {",
            "\t * 3    test_and_set_bit(PENDING)",
            "\t * 4 }                             set_..._and_clear_pending() {",
            "\t * 5                                 set_work_data() # clear bit",
            "\t * 6                                 smp_mb()",
            "\t * 7                               work->current_func() {",
            "\t * 8\t\t\t\t      LOAD event_indicated",
            "\t *\t\t\t\t   }",
            "\t *",
            "\t * Without an explicit full barrier speculative LOAD on line 8 can",
            "\t * be executed before CPU#0 does STORE on line 1.  If that happens,",
            "\t * CPU#0 observes the PENDING bit is still set and new execution of",
            "\t * a @work is not queued in a hope, that CPU#1 will eventually",
            "\t * finish the queued @work.  Meanwhile CPU#1 does not see",
            "\t * event_indicated is set, because speculative LOAD was executed",
            "\t * before actual STORE.",
            "\t */",
            "\tsmp_mb();",
            "}",
            "static void clear_work_data(struct work_struct *work)",
            "{",
            "\tsmp_wmb();\t/* see set_work_pool_and_clear_pending() */",
            "\tset_work_data(work, WORK_STRUCT_NO_POOL, 0);",
            "}",
            "static int get_work_pool_id(struct work_struct *work)",
            "{",
            "\tunsigned long data = atomic_long_read(&work->data);",
            "",
            "\tif (data & WORK_STRUCT_PWQ)",
            "\t\treturn work_struct_pwq(data)->pool->id;",
            "",
            "\treturn data >> WORK_OFFQ_POOL_SHIFT;",
            "}",
            "static void mark_work_canceling(struct work_struct *work)",
            "{",
            "\tunsigned long pool_id = get_work_pool_id(work);",
            "",
            "\tpool_id <<= WORK_OFFQ_POOL_SHIFT;",
            "\tset_work_data(work, pool_id | WORK_OFFQ_CANCELING, WORK_STRUCT_PENDING);",
            "}",
            "static bool work_is_canceling(struct work_struct *work)",
            "{",
            "\tunsigned long data = atomic_long_read(&work->data);",
            "",
            "\treturn !(data & WORK_STRUCT_PWQ) && (data & WORK_OFFQ_CANCELING);",
            "}",
            "static bool need_more_worker(struct worker_pool *pool)",
            "{",
            "\treturn !list_empty(&pool->worklist) && !pool->nr_running;",
            "}",
            "static bool may_start_working(struct worker_pool *pool)",
            "{",
            "\treturn pool->nr_idle;",
            "}",
            "static bool keep_working(struct worker_pool *pool)",
            "{",
            "\treturn !list_empty(&pool->worklist) && (pool->nr_running <= 1);",
            "}",
            "static bool need_to_create_worker(struct worker_pool *pool)",
            "{",
            "\treturn need_more_worker(pool) && !may_start_working(pool);",
            "}",
            "static bool too_many_workers(struct worker_pool *pool)",
            "{",
            "\tbool managing = pool->flags & POOL_MANAGER_ACTIVE;",
            "\tint nr_idle = pool->nr_idle + managing; /* manager is considered idle */",
            "\tint nr_busy = pool->nr_workers - nr_idle;",
            "",
            "\treturn nr_idle > 2 && (nr_idle - 2) * MAX_IDLE_WORKERS_RATIO >= nr_busy;",
            "}",
            "static inline void worker_set_flags(struct worker *worker, unsigned int flags)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\t/* If transitioning into NOT_RUNNING, adjust nr_running. */",
            "\tif ((flags & WORKER_NOT_RUNNING) &&",
            "\t    !(worker->flags & WORKER_NOT_RUNNING)) {",
            "\t\tpool->nr_running--;",
            "\t}",
            "",
            "\tworker->flags |= flags;",
            "}"
          ],
          "function_name": "set_work_pool_and_clear_pending, clear_work_data, get_work_pool_id, mark_work_canceling, work_is_canceling, need_more_worker, may_start_working, keep_working, need_to_create_worker, too_many_workers, worker_set_flags",
          "description": "实现工作者线程池状态控制逻辑，包含需要创建新工作者的判断条件、工作者空闲状态管理、工作项冲突检测及任务分配函数，通过锁保护保证池状态一致性，维护工作者线程与待处理工作项的匹配关系。",
          "similarity": 0.5120856761932373
        }
      ]
    }
  ]
}