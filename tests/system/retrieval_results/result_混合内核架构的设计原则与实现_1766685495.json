{
  "query": "混合内核架构的设计原则与实现",
  "timestamp": "2025-12-26 01:58:15",
  "retrieved_files": [
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5827694535255432,
      "chunks": [
        {
          "chunk_id": 27,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4895,
          "end_line": 5082,
          "content": [
            "static void __init rcu_init_one(void)",
            "{",
            "\tstatic const char * const buf[] = RCU_NODE_NAME_INIT;",
            "\tstatic const char * const fqs[] = RCU_FQS_NAME_INIT;",
            "\tstatic struct lock_class_key rcu_node_class[RCU_NUM_LVLS];",
            "\tstatic struct lock_class_key rcu_fqs_class[RCU_NUM_LVLS];",
            "",
            "\tint levelspread[RCU_NUM_LVLS];\t\t/* kids/node in each level. */",
            "\tint cpustride = 1;",
            "\tint i;",
            "\tint j;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tBUILD_BUG_ON(RCU_NUM_LVLS > ARRAY_SIZE(buf));  /* Fix buf[] init! */",
            "",
            "\t/* Silence gcc 4.8 false positive about array index out of range. */",
            "\tif (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)",
            "\t\tpanic(\"rcu_init_one: rcu_num_lvls out of range\");",
            "",
            "\t/* Initialize the level-tracking arrays. */",
            "",
            "\tfor (i = 1; i < rcu_num_lvls; i++)",
            "\t\trcu_state.level[i] =",
            "\t\t\trcu_state.level[i - 1] + num_rcu_lvl[i - 1];",
            "\trcu_init_levelspread(levelspread, num_rcu_lvl);",
            "",
            "\t/* Initialize the elements themselves, starting from the leaves. */",
            "",
            "\tfor (i = rcu_num_lvls - 1; i >= 0; i--) {",
            "\t\tcpustride *= levelspread[i];",
            "\t\trnp = rcu_state.level[i];",
            "\t\tfor (j = 0; j < num_rcu_lvl[i]; j++, rnp++) {",
            "\t\t\traw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));",
            "\t\t\tlockdep_set_class_and_name(&ACCESS_PRIVATE(rnp, lock),",
            "\t\t\t\t\t\t   &rcu_node_class[i], buf[i]);",
            "\t\t\traw_spin_lock_init(&rnp->fqslock);",
            "\t\t\tlockdep_set_class_and_name(&rnp->fqslock,",
            "\t\t\t\t\t\t   &rcu_fqs_class[i], fqs[i]);",
            "\t\t\trnp->gp_seq = rcu_state.gp_seq;",
            "\t\t\trnp->gp_seq_needed = rcu_state.gp_seq;",
            "\t\t\trnp->completedqs = rcu_state.gp_seq;",
            "\t\t\trnp->qsmask = 0;",
            "\t\t\trnp->qsmaskinit = 0;",
            "\t\t\trnp->grplo = j * cpustride;",
            "\t\t\trnp->grphi = (j + 1) * cpustride - 1;",
            "\t\t\tif (rnp->grphi >= nr_cpu_ids)",
            "\t\t\t\trnp->grphi = nr_cpu_ids - 1;",
            "\t\t\tif (i == 0) {",
            "\t\t\t\trnp->grpnum = 0;",
            "\t\t\t\trnp->grpmask = 0;",
            "\t\t\t\trnp->parent = NULL;",
            "\t\t\t} else {",
            "\t\t\t\trnp->grpnum = j % levelspread[i - 1];",
            "\t\t\t\trnp->grpmask = BIT(rnp->grpnum);",
            "\t\t\t\trnp->parent = rcu_state.level[i - 1] +",
            "\t\t\t\t\t      j / levelspread[i - 1];",
            "\t\t\t}",
            "\t\t\trnp->level = i;",
            "\t\t\tINIT_LIST_HEAD(&rnp->blkd_tasks);",
            "\t\t\trcu_init_one_nocb(rnp);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[0]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[1]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[2]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[3]);",
            "\t\t\tspin_lock_init(&rnp->exp_lock);",
            "\t\t\tmutex_init(&rnp->boost_kthread_mutex);",
            "\t\t\traw_spin_lock_init(&rnp->exp_poll_lock);",
            "\t\t\trnp->exp_seq_poll_rq = RCU_GET_STATE_COMPLETED;",
            "\t\t\tINIT_WORK(&rnp->exp_poll_wq, sync_rcu_do_polled_gp);",
            "\t\t}",
            "\t}",
            "",
            "\tinit_swait_queue_head(&rcu_state.gp_wq);",
            "\tinit_swait_queue_head(&rcu_state.expedited_wq);",
            "\trnp = rcu_first_leaf_node();",
            "\tfor_each_possible_cpu(i) {",
            "\t\twhile (i > rnp->grphi)",
            "\t\t\trnp++;",
            "\t\tper_cpu_ptr(&rcu_data, i)->mynode = rnp;",
            "\t\trcu_boot_init_percpu_data(i);",
            "\t}",
            "}",
            "static void __init sanitize_kthread_prio(void)",
            "{",
            "\tint kthread_prio_in = kthread_prio;",
            "",
            "\tif (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 2",
            "\t    && IS_BUILTIN(CONFIG_RCU_TORTURE_TEST))",
            "\t\tkthread_prio = 2;",
            "\telse if (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 1)",
            "\t\tkthread_prio = 1;",
            "\telse if (kthread_prio < 0)",
            "\t\tkthread_prio = 0;",
            "\telse if (kthread_prio > 99)",
            "\t\tkthread_prio = 99;",
            "",
            "\tif (kthread_prio != kthread_prio_in)",
            "\t\tpr_alert(\"%s: Limited prio to %d from %d\\n\",",
            "\t\t\t __func__, kthread_prio, kthread_prio_in);",
            "}",
            "void rcu_init_geometry(void)",
            "{",
            "\tulong d;",
            "\tint i;",
            "\tstatic unsigned long old_nr_cpu_ids;",
            "\tint rcu_capacity[RCU_NUM_LVLS];",
            "\tstatic bool initialized;",
            "",
            "\tif (initialized) {",
            "\t\t/*",
            "\t\t * Warn if setup_nr_cpu_ids() had not yet been invoked,",
            "\t\t * unless nr_cpus_ids == NR_CPUS, in which case who cares?",
            "\t\t */",
            "\t\tWARN_ON_ONCE(old_nr_cpu_ids != nr_cpu_ids);",
            "\t\treturn;",
            "\t}",
            "",
            "\told_nr_cpu_ids = nr_cpu_ids;",
            "\tinitialized = true;",
            "",
            "\t/*",
            "\t * Initialize any unspecified boot parameters.",
            "\t * The default values of jiffies_till_first_fqs and",
            "\t * jiffies_till_next_fqs are set to the RCU_JIFFIES_TILL_FORCE_QS",
            "\t * value, which is a function of HZ, then adding one for each",
            "\t * RCU_JIFFIES_FQS_DIV CPUs that might be on the system.",
            "\t */",
            "\td = RCU_JIFFIES_TILL_FORCE_QS + nr_cpu_ids / RCU_JIFFIES_FQS_DIV;",
            "\tif (jiffies_till_first_fqs == ULONG_MAX)",
            "\t\tjiffies_till_first_fqs = d;",
            "\tif (jiffies_till_next_fqs == ULONG_MAX)",
            "\t\tjiffies_till_next_fqs = d;",
            "\tadjust_jiffies_till_sched_qs();",
            "",
            "\t/* If the compile-time values are accurate, just leave. */",
            "\tif (rcu_fanout_leaf == RCU_FANOUT_LEAF &&",
            "\t    nr_cpu_ids == NR_CPUS)",
            "\t\treturn;",
            "\tpr_info(\"Adjusting geometry for rcu_fanout_leaf=%d, nr_cpu_ids=%u\\n\",",
            "\t\trcu_fanout_leaf, nr_cpu_ids);",
            "",
            "\t/*",
            "\t * The boot-time rcu_fanout_leaf parameter must be at least two",
            "\t * and cannot exceed the number of bits in the rcu_node masks.",
            "\t * Complain and fall back to the compile-time values if this",
            "\t * limit is exceeded.",
            "\t */",
            "\tif (rcu_fanout_leaf < 2 ||",
            "\t    rcu_fanout_leaf > sizeof(unsigned long) * 8) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Compute number of nodes that can be handled an rcu_node tree",
            "\t * with the given number of levels.",
            "\t */",
            "\trcu_capacity[0] = rcu_fanout_leaf;",
            "\tfor (i = 1; i < RCU_NUM_LVLS; i++)",
            "\t\trcu_capacity[i] = rcu_capacity[i - 1] * RCU_FANOUT;",
            "",
            "\t/*",
            "\t * The tree must be able to accommodate the configured number of CPUs.",
            "\t * If this limit is exceeded, fall back to the compile-time values.",
            "\t */",
            "\tif (nr_cpu_ids > rcu_capacity[RCU_NUM_LVLS - 1]) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Calculate the number of levels in the tree. */",
            "\tfor (i = 0; nr_cpu_ids > rcu_capacity[i]; i++) {",
            "\t}",
            "\trcu_num_lvls = i + 1;",
            "",
            "\t/* Calculate the number of rcu_nodes at each level of the tree. */",
            "\tfor (i = 0; i < rcu_num_lvls; i++) {",
            "\t\tint cap = rcu_capacity[(rcu_num_lvls - 1) - i];",
            "\t\tnum_rcu_lvl[i] = DIV_ROUND_UP(nr_cpu_ids, cap);",
            "\t}",
            "",
            "\t/* Calculate the total number of rcu_node structures. */",
            "\trcu_num_nodes = 0;",
            "\tfor (i = 0; i < rcu_num_lvls; i++)",
            "\t\trcu_num_nodes += num_rcu_lvl[i];",
            "}"
          ],
          "function_name": "rcu_init_one, sanitize_kthread_prio, rcu_init_geometry",
          "description": "构建多级RCU节点树结构，初始化各层级的锁类和节点属性，动态调整RCU树的几何形态以适配当前CPU数量和层级分布需求。",
          "similarity": 0.5663189888000488
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2523,
          "end_line": 2628,
          "content": [
            "static void rcu_cpu_kthread_park(unsigned int cpu)",
            "{",
            "\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;",
            "}",
            "static int rcu_cpu_kthread_should_run(unsigned int cpu)",
            "{",
            "\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);",
            "}",
            "static void rcu_cpu_kthread(unsigned int cpu)",
            "{",
            "\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);",
            "\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);",
            "\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);",
            "\tint spincnt;",
            "",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));",
            "\tfor (spincnt = 0; spincnt < 10; spincnt++) {",
            "\t\tWRITE_ONCE(*j, jiffies);",
            "\t\tlocal_bh_disable();",
            "\t\t*statusp = RCU_KTHREAD_RUNNING;",
            "\t\tlocal_irq_disable();",
            "\t\twork = *workp;",
            "\t\tWRITE_ONCE(*workp, 0);",
            "\t\tlocal_irq_enable();",
            "\t\tif (work)",
            "\t\t\trcu_core();",
            "\t\tlocal_bh_enable();",
            "\t\tif (!READ_ONCE(*workp)) {",
            "\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));",
            "\t\t\t*statusp = RCU_KTHREAD_WAITING;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "\t*statusp = RCU_KTHREAD_YIELDING;",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));",
            "\tschedule_timeout_idle(2);",
            "\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));",
            "\t*statusp = RCU_KTHREAD_WAITING;",
            "\tWRITE_ONCE(*j, jiffies);",
            "}",
            "static int __init rcu_spawn_core_kthreads(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;",
            "\tif (use_softirq)",
            "\t\treturn 0;",
            "\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),",
            "\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);",
            "\treturn 0;",
            "}",
            "static void rcutree_enqueue(struct rcu_data *rdp, struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\trcu_segcblist_enqueue(&rdp->cblist, head);",
            "\tif (__is_kvfree_rcu_offset((unsigned long)func))",
            "\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,",
            "\t\t\t\t\t (unsigned long)func,",
            "\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));",
            "\telse",
            "\t\ttrace_rcu_callback(rcu_state.name, head,",
            "\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));",
            "}",
            "static void call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,",
            "\t\t\t  rcu_callback_t func, unsigned long flags)",
            "{",
            "\trcutree_enqueue(rdp, head, func);",
            "\t/*",
            "\t * If called from an extended quiescent state, invoke the RCU",
            "\t * core in order to force a re-evaluation of RCU's idleness.",
            "\t */",
            "\tif (!rcu_is_watching())",
            "\t\tinvoke_rcu_core();",
            "",
            "\t/* If interrupts were disabled or CPU offline, don't invoke RCU core. */",
            "\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Force the grace period if too many callbacks or too long waiting.",
            "\t * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()",
            "\t * if some other CPU has recently done so.  Also, don't bother",
            "\t * invoking rcu_force_quiescent_state() if the newly enqueued callback",
            "\t * is the only one waiting for a grace period to complete.",
            "\t */",
            "\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >",
            "\t\t     rdp->qlen_last_fqs_check + qhimark)) {",
            "",
            "\t\t/* Are we ignoring a completed grace period? */",
            "\t\tnote_gp_changes(rdp);",
            "",
            "\t\t/* Start a new grace period if one not already started. */",
            "\t\tif (!rcu_gp_in_progress()) {",
            "\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);",
            "\t\t} else {",
            "\t\t\t/* Give the grace period a kick. */",
            "\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;",
            "\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&",
            "\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)",
            "\t\t\t\trcu_force_quiescent_state();",
            "\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "rcu_cpu_kthread_park, rcu_cpu_kthread_should_run, rcu_cpu_kthread, rcu_spawn_core_kthreads, rcutree_enqueue, call_rcu_core",
          "description": "实现RCU k线程管理与回调分发基础设施，包含线程启动、回调入队及触发条件判断逻辑，提供跨CPU的异步处理能力",
          "similarity": 0.5429024696350098
        },
        {
          "chunk_id": 22,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4072,
          "end_line": 4226,
          "content": [
            "static void rcu_barrier_trace(const char *s, int cpu, unsigned long done)",
            "{",
            "\ttrace_rcu_barrier(rcu_state.name, s, cpu,",
            "\t\t\t  atomic_read(&rcu_state.barrier_cpu_count), done);",
            "}",
            "static void rcu_barrier_callback(struct rcu_head *rhp)",
            "{",
            "\tunsigned long __maybe_unused s = rcu_state.barrier_sequence;",
            "",
            "\tif (atomic_dec_and_test(&rcu_state.barrier_cpu_count)) {",
            "\t\trcu_barrier_trace(TPS(\"LastCB\"), -1, s);",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "\t} else {",
            "\t\trcu_barrier_trace(TPS(\"CB\"), -1, s);",
            "\t}",
            "}",
            "static void rcu_barrier_entrain(struct rcu_data *rdp)",
            "{",
            "\tunsigned long gseq = READ_ONCE(rcu_state.barrier_sequence);",
            "\tunsigned long lseq = READ_ONCE(rdp->barrier_seq_snap);",
            "\tbool wake_nocb = false;",
            "\tbool was_alldone = false;",
            "",
            "\tlockdep_assert_held(&rcu_state.barrier_lock);",
            "\tif (rcu_seq_state(lseq) || !rcu_seq_state(gseq) || rcu_seq_ctr(lseq) != rcu_seq_ctr(gseq))",
            "\t\treturn;",
            "\trcu_barrier_trace(TPS(\"IRQ\"), -1, rcu_state.barrier_sequence);",
            "\trdp->barrier_head.func = rcu_barrier_callback;",
            "\tdebug_rcu_head_queue(&rdp->barrier_head);",
            "\trcu_nocb_lock(rdp);",
            "\t/*",
            "\t * Flush bypass and wakeup rcuog if we add callbacks to an empty regular",
            "\t * queue. This way we don't wait for bypass timer that can reach seconds",
            "\t * if it's fully lazy.",
            "\t */",
            "\twas_alldone = rcu_rdp_is_offloaded(rdp) && !rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));",
            "\twake_nocb = was_alldone && rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tif (rcu_segcblist_entrain(&rdp->cblist, &rdp->barrier_head)) {",
            "\t\tatomic_inc(&rcu_state.barrier_cpu_count);",
            "\t} else {",
            "\t\tdebug_rcu_head_unqueue(&rdp->barrier_head);",
            "\t\trcu_barrier_trace(TPS(\"IRQNQ\"), -1, rcu_state.barrier_sequence);",
            "\t}",
            "\trcu_nocb_unlock(rdp);",
            "\tif (wake_nocb)",
            "\t\twake_nocb_gp(rdp, false);",
            "\tsmp_store_release(&rdp->barrier_seq_snap, gseq);",
            "}",
            "static void rcu_barrier_handler(void *cpu_in)",
            "{",
            "\tuintptr_t cpu = (uintptr_t)cpu_in;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\tWARN_ON_ONCE(cpu != rdp->cpu);",
            "\tWARN_ON_ONCE(cpu != smp_processor_id());",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\trcu_barrier_entrain(rdp);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "}",
            "void rcu_barrier(void)",
            "{",
            "\tuintptr_t cpu;",
            "\tunsigned long flags;",
            "\tunsigned long gseq;",
            "\tstruct rcu_data *rdp;",
            "\tunsigned long s = rcu_seq_snap(&rcu_state.barrier_sequence);",
            "",
            "\trcu_barrier_trace(TPS(\"Begin\"), -1, s);",
            "",
            "\t/* Take mutex to serialize concurrent rcu_barrier() requests. */",
            "\tmutex_lock(&rcu_state.barrier_mutex);",
            "",
            "\t/* Did someone else do our work for us? */",
            "\tif (rcu_seq_done(&rcu_state.barrier_sequence, s)) {",
            "\t\trcu_barrier_trace(TPS(\"EarlyExit\"), -1, rcu_state.barrier_sequence);",
            "\t\tsmp_mb(); /* caller's subsequent code after above check. */",
            "\t\tmutex_unlock(&rcu_state.barrier_mutex);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Mark the start of the barrier operation. */",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\trcu_seq_start(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\trcu_barrier_trace(TPS(\"Inc1\"), -1, rcu_state.barrier_sequence);",
            "",
            "\t/*",
            "\t * Initialize the count to two rather than to zero in order",
            "\t * to avoid a too-soon return to zero in case of an immediate",
            "\t * invocation of the just-enqueued callback (or preemption of",
            "\t * this task).  Exclude CPU-hotplug operations to ensure that no",
            "\t * offline non-offloaded CPU has callbacks queued.",
            "\t */",
            "\tinit_completion(&rcu_state.barrier_completion);",
            "\tatomic_set(&rcu_state.barrier_cpu_count, 2);",
            "\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "",
            "\t/*",
            "\t * Force each CPU with callbacks to register a new callback.",
            "\t * When that callback is invoked, we will know that all of the",
            "\t * corresponding CPU's preceding callbacks have been invoked.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "retry:",
            "\t\tif (smp_load_acquire(&rdp->barrier_seq_snap) == gseq)",
            "\t\t\tcontinue;",
            "\t\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\t\tif (!rcu_segcblist_n_cbs(&rdp->cblist)) {",
            "\t\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"NQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (!rcu_rdp_cpu_online(rdp)) {",
            "\t\t\trcu_barrier_entrain(rdp);",
            "\t\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"OfflineNoCBQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\tif (smp_call_function_single(cpu, rcu_barrier_handler, (void *)cpu, 1)) {",
            "\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\tgoto retry;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\trcu_barrier_trace(TPS(\"OnlineQ\"), cpu, rcu_state.barrier_sequence);",
            "\t}",
            "",
            "\t/*",
            "\t * Now that we have an rcu_barrier_callback() callback on each",
            "\t * CPU, and thus each counted, remove the initial count.",
            "\t */",
            "\tif (atomic_sub_and_test(2, &rcu_state.barrier_cpu_count))",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "",
            "\t/* Wait for all rcu_barrier_callback() callbacks to be invoked. */",
            "\twait_for_completion(&rcu_state.barrier_completion);",
            "",
            "\t/* Mark the end of the barrier operation. */",
            "\trcu_barrier_trace(TPS(\"Inc2\"), -1, rcu_state.barrier_sequence);",
            "\trcu_seq_end(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t}",
            "",
            "\t/* Other rcu_barrier() invocations can now safely proceed. */",
            "\tmutex_unlock(&rcu_state.barrier_mutex);",
            "}"
          ],
          "function_name": "rcu_barrier_trace, rcu_barrier_callback, rcu_barrier_entrain, rcu_barrier_handler, rcu_barrier",
          "description": "实现RCU屏障功能，通过分发回调函数强制所有CPU完成当前RCU操作，使用原子计数器跟踪完成状态，通过completion等待所有回调完成",
          "similarity": 0.5305311679840088
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1840,
          "end_line": 1973,
          "content": [
            "static int __noreturn rcu_gp_kthread(void *unused)",
            "{",
            "\trcu_bind_gp_kthread();",
            "\tfor (;;) {",
            "",
            "\t\t/* Handle grace-period start. */",
            "\t\tfor (;;) {",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwait\"));",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_WAIT_GPS);",
            "\t\t\tswait_event_idle_exclusive(rcu_state.gp_wq,",
            "\t\t\t\t\t READ_ONCE(rcu_state.gp_flags) &",
            "\t\t\t\t\t RCU_GP_FLAG_INIT);",
            "\t\t\trcu_gp_torture_wait();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_DONE_GPS);",
            "\t\t\t/* Locking provides needed memory barrier. */",
            "\t\t\tif (rcu_gp_init())",
            "\t\t\t\tbreak;",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\t\t\tWARN_ON(signal_pending(current));",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwaitsig\"));",
            "\t\t}",
            "",
            "\t\t/* Handle quiescent-state forcing. */",
            "\t\trcu_gp_fqs_loop();",
            "",
            "\t\t/* Handle grace-period end. */",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANUP);",
            "\t\trcu_gp_cleanup();",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANED);",
            "\t}",
            "}",
            "static void rcu_report_qs_rsp(unsigned long flags)",
            "\t__releases(rcu_get_root()->lock)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rcu_get_root());",
            "\tWARN_ON_ONCE(!rcu_gp_in_progress());",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rcu_get_root(), flags);",
            "\trcu_gp_kthread_wake();",
            "}",
            "static void rcu_report_qs_rnp(unsigned long mask, struct rcu_node *rnp,",
            "\t\t\t      unsigned long gps, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long oldmask = 0;",
            "\tstruct rcu_node *rnp_c;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* Walk up the rcu_node hierarchy. */",
            "\tfor (;;) {",
            "\t\tif ((!(rnp->qsmask & mask) && mask) || rnp->gp_seq != gps) {",
            "",
            "\t\t\t/*",
            "\t\t\t * Our bit has already been cleared, or the",
            "\t\t\t * relevant grace period is already over, so done.",
            "\t\t\t */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */",
            "\t\tWARN_ON_ONCE(!rcu_is_leaf_node(rnp) &&",
            "\t\t\t     rcu_preempt_blocked_readers_cgp(rnp));",
            "\t\tWRITE_ONCE(rnp->qsmask, rnp->qsmask & ~mask);",
            "\t\ttrace_rcu_quiescent_state_report(rcu_state.name, rnp->gp_seq,",
            "\t\t\t\t\t\t mask, rnp->qsmask, rnp->level,",
            "\t\t\t\t\t\t rnp->grplo, rnp->grphi,",
            "\t\t\t\t\t\t !!rnp->gp_tasks);",
            "\t\tif (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {",
            "",
            "\t\t\t/* Other bits still set at this level, so done. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\trnp->completedqs = rnp->gp_seq;",
            "\t\tmask = rnp->grpmask;",
            "\t\tif (rnp->parent == NULL) {",
            "",
            "\t\t\t/* No more levels.  Exit loop holding root lock. */",
            "",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\trnp_c = rnp;",
            "\t\trnp = rnp->parent;",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\toldmask = READ_ONCE(rnp_c->qsmask);",
            "\t}",
            "",
            "\t/*",
            "\t * Get here if we are the last CPU to pass through a quiescent",
            "\t * state for this grace period.  Invoke rcu_report_qs_rsp()",
            "\t * to clean up and start the next grace period if one is needed.",
            "\t */",
            "\trcu_report_qs_rsp(flags); /* releases rnp->lock. */",
            "}",
            "static void __maybe_unused",
            "rcu_report_unblock_qs_rnp(struct rcu_node *rnp, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long gps;",
            "\tunsigned long mask;",
            "\tstruct rcu_node *rnp_p;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (WARN_ON_ONCE(!IS_ENABLED(CONFIG_PREEMPT_RCU)) ||",
            "\t    WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)) ||",
            "\t    rnp->qsmask != 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;  /* Still need more quiescent states! */",
            "\t}",
            "",
            "\trnp->completedqs = rnp->gp_seq;",
            "\trnp_p = rnp->parent;",
            "\tif (rnp_p == NULL) {",
            "\t\t/*",
            "\t\t * Only one rcu_node structure in the tree, so don't",
            "\t\t * try to report up to its nonexistent parent!",
            "\t\t */",
            "\t\trcu_report_qs_rsp(flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Report up the rest of the hierarchy, tracking current ->gp_seq. */",
            "\tgps = rnp->gp_seq;",
            "\tmask = rnp->grpmask;",
            "\traw_spin_unlock_rcu_node(rnp);\t/* irqs remain disabled. */",
            "\traw_spin_lock_rcu_node(rnp_p);\t/* irqs already disabled. */",
            "\trcu_report_qs_rnp(mask, rnp_p, gps, flags);",
            "}"
          ],
          "function_name": "rcu_gp_kthread, rcu_report_qs_rsp, rcu_report_qs_rnp, rcu_report_unblock_qs_rnp",
          "description": "实现RCU grace period的主线程循环，处理grace period启动、强制quiescent状态报告及结束逻辑，通过锁和状态标志协调各子系统同步",
          "similarity": 0.5254485607147217
        },
        {
          "chunk_id": 25,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4602,
          "end_line": 4729,
          "content": [
            "void rcu_cpu_starting(unsigned int cpu)",
            "{",
            "\tunsigned long mask;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "\tbool newcpu;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tif (rdp->cpu_started)",
            "\t\treturn;",
            "\trdp->cpu_started = true;",
            "",
            "\trnp = rdp->mynode;",
            "\tmask = rdp->grpmask;",
            "\tarch_spin_lock(&rcu_state.ofl_lock);",
            "\trcu_dynticks_eqs_online();",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\traw_spin_lock_rcu_node(rnp);",
            "\tWRITE_ONCE(rnp->qsmaskinitnext, rnp->qsmaskinitnext | mask);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "\tnewcpu = !(rnp->expmaskinitnext & mask);",
            "\trnp->expmaskinitnext |= mask;",
            "\t/* Allow lockless access for expedited grace periods. */",
            "\tsmp_store_release(&rcu_state.ncpus, rcu_state.ncpus + newcpu); /* ^^^ */",
            "\tASSERT_EXCLUSIVE_WRITER(rcu_state.ncpus);",
            "\trcu_gpnum_ovf(rnp, rdp); /* Offline-induced counter wrap? */",
            "\trdp->rcu_onl_gp_seq = READ_ONCE(rcu_state.gp_seq);",
            "\trdp->rcu_onl_gp_flags = READ_ONCE(rcu_state.gp_flags);",
            "",
            "\t/* An incoming CPU should never be blocking a grace period. */",
            "\tif (WARN_ON_ONCE(rnp->qsmask & mask)) { /* RCU waiting on incoming CPU? */",
            "\t\t/* rcu_report_qs_rnp() *really* wants some flags to restore */",
            "\t\tunsigned long flags;",
            "",
            "\t\tlocal_irq_save(flags);",
            "\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\t/* Report QS -after- changing ->qsmaskinitnext! */",
            "\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t} else {",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\t}",
            "\tarch_spin_unlock(&rcu_state.ofl_lock);",
            "\tsmp_store_release(&rdp->beenonline, true);",
            "\tsmp_mb(); /* Ensure RCU read-side usage follows above initialization. */",
            "}",
            "void rcu_report_dead(unsigned int cpu)",
            "{",
            "\tunsigned long flags, seq_flags;",
            "\tunsigned long mask;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tstruct rcu_node *rnp = rdp->mynode;  /* Outgoing CPU's rdp & rnp. */",
            "",
            "\t// Do any dangling deferred wakeups.",
            "\tdo_nocb_deferred_wakeup(rdp);",
            "",
            "\trcu_preempt_deferred_qs(current);",
            "",
            "\t/* Remove outgoing CPU from mask in the leaf rcu_node structure. */",
            "\tmask = rdp->grpmask;",
            "\tlocal_irq_save(seq_flags);",
            "\tarch_spin_lock(&rcu_state.ofl_lock);",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags); /* Enforce GP memory-order guarantee. */",
            "\trdp->rcu_ofl_gp_seq = READ_ONCE(rcu_state.gp_seq);",
            "\trdp->rcu_ofl_gp_flags = READ_ONCE(rcu_state.gp_flags);",
            "\tif (rnp->qsmask & mask) { /* RCU waiting on outgoing CPU? */",
            "\t\t/* Report quiescent state -before- changing ->qsmaskinitnext! */",
            "\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t}",
            "\tWRITE_ONCE(rnp->qsmaskinitnext, rnp->qsmaskinitnext & ~mask);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tarch_spin_unlock(&rcu_state.ofl_lock);",
            "\tlocal_irq_restore(seq_flags);",
            "",
            "\trdp->cpu_started = false;",
            "}",
            "void rcutree_migrate_callbacks(int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *my_rdp;",
            "\tstruct rcu_node *my_rnp;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tbool needwake;",
            "",
            "\tif (rcu_rdp_is_offloaded(rdp))",
            "\t\treturn;",
            "",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\tif (rcu_segcblist_empty(&rdp->cblist)) {",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\treturn;  /* No callbacks to migrate. */",
            "\t}",
            "",
            "\tWARN_ON_ONCE(rcu_rdp_cpu_online(rdp));",
            "\trcu_barrier_entrain(rdp);",
            "\tmy_rdp = this_cpu_ptr(&rcu_data);",
            "\tmy_rnp = my_rdp->mynode;",
            "\trcu_nocb_lock(my_rdp); /* irqs already disabled. */",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(my_rdp, NULL, jiffies, false));",
            "\traw_spin_lock_rcu_node(my_rnp); /* irqs already disabled. */",
            "\t/* Leverage recent GPs and set GP for new callbacks. */",
            "\tneedwake = rcu_advance_cbs(my_rnp, rdp) ||",
            "\t\t   rcu_advance_cbs(my_rnp, my_rdp);",
            "\trcu_segcblist_merge(&my_rdp->cblist, &rdp->cblist);",
            "\traw_spin_unlock(&rcu_state.barrier_lock); /* irqs remain disabled. */",
            "\tneedwake = needwake || rcu_advance_cbs(my_rnp, my_rdp);",
            "\trcu_segcblist_disable(&rdp->cblist);",
            "\tWARN_ON_ONCE(rcu_segcblist_empty(&my_rdp->cblist) != !rcu_segcblist_n_cbs(&my_rdp->cblist));",
            "\tcheck_cb_ovld_locked(my_rdp, my_rnp);",
            "\tif (rcu_rdp_is_offloaded(my_rdp)) {",
            "\t\traw_spin_unlock_rcu_node(my_rnp); /* irqs remain disabled. */",
            "\t\t__call_rcu_nocb_wake(my_rdp, true, flags);",
            "\t} else {",
            "\t\trcu_nocb_unlock(my_rdp); /* irqs remain disabled. */",
            "\t\traw_spin_unlock_rcu_node(my_rnp); /* irqs remain disabled. */",
            "\t}",
            "\tlocal_irq_restore(flags);",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "\tlockdep_assert_irqs_enabled();",
            "\tWARN_ONCE(rcu_segcblist_n_cbs(&rdp->cblist) != 0 ||",
            "\t\t  !rcu_segcblist_empty(&rdp->cblist),",
            "\t\t  \"rcu_cleanup_dead_cpu: Callbacks on offline CPU %d: qlen=%lu, 1stCB=%p\\n\",",
            "\t\t  cpu, rcu_segcblist_n_cbs(&rdp->cblist),",
            "\t\t  rcu_segcblist_first_cb(&rdp->cblist));",
            "}"
          ],
          "function_name": "rcu_cpu_starting, rcu_report_dead, rcutree_migrate_callbacks",
          "description": "处理CPU上线时的RCU状态更新，通过修改rcu_node结构体中的掩码和序列号，确保并发访问的安全性，并在必要时触发静默状态报告。",
          "similarity": 0.5241079926490784
        }
      ]
    },
    {
      "source_file": "mm/ksm.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:34:25\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `ksm.c`\n\n---\n\n# ksm.c 技术文档\n\n## 1. 文件概述\n\n`ksm.c` 实现了内核同页合并（Kernel Samepage Merging, KSM）功能，该机制能够动态识别并合并内容完全相同的物理内存页，即使这些页属于不同的进程地址空间且未通过 `fork()` 共享。KSM 通过后台扫描线程周期性地遍历注册的内存区域，利用红黑树（rbtree）结构高效地比对页面内容，将重复页替换为只读的共享页，从而显著减少物理内存占用。此功能特别适用于虚拟化环境中多个相似虚拟机共存的场景。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct ksm_mm_slot`**  \n  表示一个被 KSM 扫描的内存描述符（`mm_struct`）的元数据，包含哈希槽位和反向映射项链表头。\n\n- **`struct ksm_scan`**  \n  全局扫描游标，记录当前扫描进度（包括当前 `mm_slot`、虚拟地址、反向映射项指针及完整扫描轮次计数）。\n\n- **`struct ksm_stable_node`**  \n  稳定红黑树中的节点，代表一个已合并的 KSM 页面。包含：\n  - 红黑树节点或迁移链表指针（联合体复用）\n  - 指向使用该 KSM 页的所有 `rmap_item` 的哈希链表头\n  - 物理页帧号（`kpfn`）或链修剪时间戳\n  - 反向映射项数量（支持链式扩展）\n  - NUMA 节点 ID（若启用 NUMA）\n\n- **`struct ksm_rmap_item`**  \n  反向映射项，跟踪一个虚拟地址到其物理页的映射关系。包含：\n  - 所属 `mm_struct` 和虚拟地址（低比特位用于标志）\n  - 匿名 VMA 指针（稳定状态）或 NUMA 节点 ID（不稳定状态）\n  - 校验和（不稳定状态）\n  - 红黑树节点（不稳定树）或指向 `stable_node` 的指针及哈希链表节点（稳定状态）\n\n### 关键宏定义\n\n- **`STABLE_NODE_CHAIN`**  \n  标识稳定节点为“链”类型（值为 -1024），用于高效管理大量相同内容的 KSM 页副本。\n  \n- **标志位**  \n  - `UNSTABLE_FLAG` (0x100)：标识 `rmap_item` 属于不稳定树\n  - `STABLE_FLAG` (0x200)：标识 `rmap_item` 已链接到稳定树\n  - `SEQNR_MASK` (0x0ff)：用于存储不稳定树序列号的低 8 位\n\n## 3. 关键实现\n\n### 双树架构设计\nKSM 采用**稳定树（stable tree）**与**不稳定树（unstable tree）**协同工作的机制：\n- **稳定树**：存储已确认可合并的只读 KSM 页，因写保护而内容恒定，支持高效精确匹配。\n- **不稳定树**：临时缓存近期未修改的普通页，因内容可能变化而需周期性重建。\n\n### 扫描与合并流程\n1. **增量扫描**：全局游标 `ksm_scan` 遍历所有注册的 `mm_slot` 及其内存区域。\n2. **校验和预筛**：计算页面内容的 `xxhash` 校验和，仅当与上次扫描一致时才尝试插入不稳定树。\n3. **双阶段匹配**：\n   - 优先在**稳定树**中查找完全匹配的 KSM 页\n   - 若未命中，则在**不稳定树**中查找潜在重复页\n4. **树维护策略**：\n   - 不稳定树在每轮全量扫描结束后**完全清空重建**\n   - 稳定树**持久保留**，通过反向映射（rmap）和链式节点优化大规模合并场景\n5. **NUMA 感知**：若 `merge_across_nodes=0`，则为每个 NUMA 节点维护独立的稳定/不稳定树，避免跨节点内存访问开销。\n\n### 内存安全机制\n- **写保护**：合并后的 KSM 页设为只读，任何写操作触发 COW（写时复制）并解除合并。\n- **RMAP 集成**：通过 `anon_vma` 和反向映射链表，在页解绑时高效更新所有相关虚拟地址。\n- **OOM 防护**：在内存压力下可释放 KSM 页以缓解系统压力。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：深度依赖 `mm.h`、`rmap.h`、`pagemap.h` 实现页表操作、反向映射和页生命周期管理。\n- **调度与进程管理**：通过 `sched/mm.h` 获取进程内存上下文，利用 `kthread.h` 创建后台扫描线程。\n- **NUMA 支持**：条件编译依赖 `CONFIG_NUMA`，使用 `numa.h` 实现节点亲和性。\n- **调试与追踪**：集成 `trace/events/ksm.h` 提供运行时事件追踪能力。\n- **哈希算法**：使用 `xxhash.h` 提供高效的内容指纹计算。\n- **内部辅助模块**：依赖 `mm_slot.h` 管理内存描述符槽位，`internal.h` 提供内核 MM 内部接口。\n\n## 5. 使用场景\n\n- **虚拟化环境**：在 KVM/Xen 等 Hypervisor 中合并多个相似虚拟机的内存页（如相同操作系统镜像）。\n- **内存密集型应用**：合并大型应用（如数据库、Web 服务器）中重复的静态数据或零页。\n- **容器化平台**：在 Docker/LXC 等容器运行时中减少同镜像容器的内存占用。\n- **内存超分场景**：在物理内存有限但允许超额分配的系统中提升内存利用率。\n- **开发调试**：通过 `/sys/kernel/mm/ksm/` 接口动态控制扫描速率、合并阈值等参数。",
      "similarity": 0.5732492804527283,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/ksm.c",
          "start_line": 1,
          "end_line": 311,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Memory merging support.",
            " *",
            " * This code enables dynamic sharing of identical pages found in different",
            " * memory areas, even if they are not shared by fork()",
            " *",
            " * Copyright (C) 2008-2009 Red Hat, Inc.",
            " * Authors:",
            " *\tIzik Eidus",
            " *\tAndrea Arcangeli",
            " *\tChris Wright",
            " *\tHugh Dickins",
            " */",
            "",
            "#include <linux/errno.h>",
            "#include <linux/mm.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/fs.h>",
            "#include <linux/mman.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/rmap.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/xxhash.h>",
            "#include <linux/delay.h>",
            "#include <linux/kthread.h>",
            "#include <linux/wait.h>",
            "#include <linux/slab.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/memory.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/swap.h>",
            "#include <linux/ksm.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/freezer.h>",
            "#include <linux/oom.h>",
            "#include <linux/numa.h>",
            "#include <linux/pagewalk.h>",
            "",
            "#include <asm/tlbflush.h>",
            "#include \"internal.h\"",
            "#include \"mm_slot.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/ksm.h>",
            "",
            "#ifdef CONFIG_NUMA",
            "#define NUMA(x)\t\t(x)",
            "#define DO_NUMA(x)\tdo { (x); } while (0)",
            "#else",
            "#define NUMA(x)\t\t(0)",
            "#define DO_NUMA(x)\tdo { } while (0)",
            "#endif",
            "",
            "/**",
            " * DOC: Overview",
            " *",
            " * A few notes about the KSM scanning process,",
            " * to make it easier to understand the data structures below:",
            " *",
            " * In order to reduce excessive scanning, KSM sorts the memory pages by their",
            " * contents into a data structure that holds pointers to the pages' locations.",
            " *",
            " * Since the contents of the pages may change at any moment, KSM cannot just",
            " * insert the pages into a normal sorted tree and expect it to find anything.",
            " * Therefore KSM uses two data structures - the stable and the unstable tree.",
            " *",
            " * The stable tree holds pointers to all the merged pages (ksm pages), sorted",
            " * by their contents.  Because each such page is write-protected, searching on",
            " * this tree is fully assured to be working (except when pages are unmapped),",
            " * and therefore this tree is called the stable tree.",
            " *",
            " * The stable tree node includes information required for reverse",
            " * mapping from a KSM page to virtual addresses that map this page.",
            " *",
            " * In order to avoid large latencies of the rmap walks on KSM pages,",
            " * KSM maintains two types of nodes in the stable tree:",
            " *",
            " * * the regular nodes that keep the reverse mapping structures in a",
            " *   linked list",
            " * * the \"chains\" that link nodes (\"dups\") that represent the same",
            " *   write protected memory content, but each \"dup\" corresponds to a",
            " *   different KSM page copy of that content",
            " *",
            " * Internally, the regular nodes, \"dups\" and \"chains\" are represented",
            " * using the same struct ksm_stable_node structure.",
            " *",
            " * In addition to the stable tree, KSM uses a second data structure called the",
            " * unstable tree: this tree holds pointers to pages which have been found to",
            " * be \"unchanged for a period of time\".  The unstable tree sorts these pages",
            " * by their contents, but since they are not write-protected, KSM cannot rely",
            " * upon the unstable tree to work correctly - the unstable tree is liable to",
            " * be corrupted as its contents are modified, and so it is called unstable.",
            " *",
            " * KSM solves this problem by several techniques:",
            " *",
            " * 1) The unstable tree is flushed every time KSM completes scanning all",
            " *    memory areas, and then the tree is rebuilt again from the beginning.",
            " * 2) KSM will only insert into the unstable tree, pages whose hash value",
            " *    has not changed since the previous scan of all memory areas.",
            " * 3) The unstable tree is a RedBlack Tree - so its balancing is based on the",
            " *    colors of the nodes and not on their contents, assuring that even when",
            " *    the tree gets \"corrupted\" it won't get out of balance, so scanning time",
            " *    remains the same (also, searching and inserting nodes in an rbtree uses",
            " *    the same algorithm, so we have no overhead when we flush and rebuild).",
            " * 4) KSM never flushes the stable tree, which means that even if it were to",
            " *    take 10 attempts to find a page in the unstable tree, once it is found,",
            " *    it is secured in the stable tree.  (When we scan a new page, we first",
            " *    compare it against the stable tree, and then against the unstable tree.)",
            " *",
            " * If the merge_across_nodes tunable is unset, then KSM maintains multiple",
            " * stable trees and multiple unstable trees: one of each for each NUMA node.",
            " */",
            "",
            "/**",
            " * struct ksm_mm_slot - ksm information per mm that is being scanned",
            " * @slot: hash lookup from mm to mm_slot",
            " * @rmap_list: head for this mm_slot's singly-linked list of rmap_items",
            " */",
            "struct ksm_mm_slot {",
            "\tstruct mm_slot slot;",
            "\tstruct ksm_rmap_item *rmap_list;",
            "};",
            "",
            "/**",
            " * struct ksm_scan - cursor for scanning",
            " * @mm_slot: the current mm_slot we are scanning",
            " * @address: the next address inside that to be scanned",
            " * @rmap_list: link to the next rmap to be scanned in the rmap_list",
            " * @seqnr: count of completed full scans (needed when removing unstable node)",
            " *",
            " * There is only the one ksm_scan instance of this cursor structure.",
            " */",
            "struct ksm_scan {",
            "\tstruct ksm_mm_slot *mm_slot;",
            "\tunsigned long address;",
            "\tstruct ksm_rmap_item **rmap_list;",
            "\tunsigned long seqnr;",
            "};",
            "",
            "/**",
            " * struct ksm_stable_node - node of the stable rbtree",
            " * @node: rb node of this ksm page in the stable tree",
            " * @head: (overlaying parent) &migrate_nodes indicates temporarily on that list",
            " * @hlist_dup: linked into the stable_node->hlist with a stable_node chain",
            " * @list: linked into migrate_nodes, pending placement in the proper node tree",
            " * @hlist: hlist head of rmap_items using this ksm page",
            " * @kpfn: page frame number of this ksm page (perhaps temporarily on wrong nid)",
            " * @chain_prune_time: time of the last full garbage collection",
            " * @rmap_hlist_len: number of rmap_item entries in hlist or STABLE_NODE_CHAIN",
            " * @nid: NUMA node id of stable tree in which linked (may not match kpfn)",
            " */",
            "struct ksm_stable_node {",
            "\tunion {",
            "\t\tstruct rb_node node;\t/* when node of stable tree */",
            "\t\tstruct {\t\t/* when listed for migration */",
            "\t\t\tstruct list_head *head;",
            "\t\t\tstruct {",
            "\t\t\t\tstruct hlist_node hlist_dup;",
            "\t\t\t\tstruct list_head list;",
            "\t\t\t};",
            "\t\t};",
            "\t};",
            "\tstruct hlist_head hlist;",
            "\tunion {",
            "\t\tunsigned long kpfn;",
            "\t\tunsigned long chain_prune_time;",
            "\t};",
            "\t/*",
            "\t * STABLE_NODE_CHAIN can be any negative number in",
            "\t * rmap_hlist_len negative range, but better not -1 to be able",
            "\t * to reliably detect underflows.",
            "\t */",
            "#define STABLE_NODE_CHAIN -1024",
            "\tint rmap_hlist_len;",
            "#ifdef CONFIG_NUMA",
            "\tint nid;",
            "#endif",
            "};",
            "",
            "/**",
            " * struct ksm_rmap_item - reverse mapping item for virtual addresses",
            " * @rmap_list: next rmap_item in mm_slot's singly-linked rmap_list",
            " * @anon_vma: pointer to anon_vma for this mm,address, when in stable tree",
            " * @nid: NUMA node id of unstable tree in which linked (may not match page)",
            " * @mm: the memory structure this rmap_item is pointing into",
            " * @address: the virtual address this rmap_item tracks (+ flags in low bits)",
            " * @oldchecksum: previous checksum of the page at that virtual address",
            " * @node: rb node of this rmap_item in the unstable tree",
            " * @head: pointer to stable_node heading this list in the stable tree",
            " * @hlist: link into hlist of rmap_items hanging off that stable_node",
            " */",
            "struct ksm_rmap_item {",
            "\tstruct ksm_rmap_item *rmap_list;",
            "\tunion {",
            "\t\tstruct anon_vma *anon_vma;\t/* when stable */",
            "#ifdef CONFIG_NUMA",
            "\t\tint nid;\t\t/* when node of unstable tree */",
            "#endif",
            "\t};",
            "\tstruct mm_struct *mm;",
            "\tunsigned long address;\t\t/* + low bits used for flags below */",
            "\tunsigned int oldchecksum;\t/* when unstable */",
            "\tunion {",
            "\t\tstruct rb_node node;\t/* when node of unstable tree */",
            "\t\tstruct {\t\t/* when listed from stable tree */",
            "\t\t\tstruct ksm_stable_node *head;",
            "\t\t\tstruct hlist_node hlist;",
            "\t\t};",
            "\t};",
            "};",
            "",
            "#define SEQNR_MASK\t0x0ff\t/* low bits of unstable tree seqnr */",
            "#define UNSTABLE_FLAG\t0x100\t/* is a node of the unstable tree */",
            "#define STABLE_FLAG\t0x200\t/* is listed from the stable tree */",
            "",
            "/* The stable and unstable tree heads */",
            "static struct rb_root one_stable_tree[1] = { RB_ROOT };",
            "static struct rb_root one_unstable_tree[1] = { RB_ROOT };",
            "static struct rb_root *root_stable_tree = one_stable_tree;",
            "static struct rb_root *root_unstable_tree = one_unstable_tree;",
            "",
            "/* Recently migrated nodes of stable tree, pending proper placement */",
            "static LIST_HEAD(migrate_nodes);",
            "#define STABLE_NODE_DUP_HEAD ((struct list_head *)&migrate_nodes.prev)",
            "",
            "#define MM_SLOTS_HASH_BITS 10",
            "static DEFINE_HASHTABLE(mm_slots_hash, MM_SLOTS_HASH_BITS);",
            "",
            "static struct ksm_mm_slot ksm_mm_head = {",
            "\t.slot.mm_node = LIST_HEAD_INIT(ksm_mm_head.slot.mm_node),",
            "};",
            "static struct ksm_scan ksm_scan = {",
            "\t.mm_slot = &ksm_mm_head,",
            "};",
            "",
            "static struct kmem_cache *rmap_item_cache;",
            "static struct kmem_cache *stable_node_cache;",
            "static struct kmem_cache *mm_slot_cache;",
            "",
            "/* The number of pages scanned */",
            "static unsigned long ksm_pages_scanned;",
            "",
            "/* The number of nodes in the stable tree */",
            "static unsigned long ksm_pages_shared;",
            "",
            "/* The number of page slots additionally sharing those nodes */",
            "static unsigned long ksm_pages_sharing;",
            "",
            "/* The number of nodes in the unstable tree */",
            "static unsigned long ksm_pages_unshared;",
            "",
            "/* The number of rmap_items in use: to calculate pages_volatile */",
            "static unsigned long ksm_rmap_items;",
            "",
            "/* The number of stable_node chains */",
            "static unsigned long ksm_stable_node_chains;",
            "",
            "/* The number of stable_node dups linked to the stable_node chains */",
            "static unsigned long ksm_stable_node_dups;",
            "",
            "/* Delay in pruning stale stable_node_dups in the stable_node_chains */",
            "static unsigned int ksm_stable_node_chains_prune_millisecs = 2000;",
            "",
            "/* Maximum number of page slots sharing a stable node */",
            "static int ksm_max_page_sharing = 256;",
            "",
            "/* Number of pages ksmd should scan in one batch */",
            "static unsigned int ksm_thread_pages_to_scan = 100;",
            "",
            "/* Milliseconds ksmd should sleep between batches */",
            "static unsigned int ksm_thread_sleep_millisecs = 20;",
            "",
            "/* Checksum of an empty (zeroed) page */",
            "static unsigned int zero_checksum __read_mostly;",
            "",
            "/* Whether to merge empty (zeroed) pages with actual zero pages */",
            "static bool ksm_use_zero_pages __read_mostly;",
            "",
            "/* The number of zero pages which is placed by KSM */",
            "atomic_long_t ksm_zero_pages = ATOMIC_LONG_INIT(0);",
            "",
            "#ifdef CONFIG_NUMA",
            "/* Zeroed when merging across nodes is not allowed */",
            "static unsigned int ksm_merge_across_nodes = 1;",
            "static int ksm_nr_node_ids = 1;",
            "#else",
            "#define ksm_merge_across_nodes\t1U",
            "#define ksm_nr_node_ids\t\t1",
            "#endif",
            "",
            "#define KSM_RUN_STOP\t0",
            "#define KSM_RUN_MERGE\t1",
            "#define KSM_RUN_UNMERGE\t2",
            "#define KSM_RUN_OFFLINE\t4",
            "static unsigned long ksm_run = KSM_RUN_STOP;",
            "static void wait_while_offlining(void);",
            "",
            "static DECLARE_WAIT_QUEUE_HEAD(ksm_thread_wait);",
            "static DECLARE_WAIT_QUEUE_HEAD(ksm_iter_wait);",
            "static DEFINE_MUTEX(ksm_thread_mutex);",
            "static DEFINE_SPINLOCK(ksm_mmlist_lock);",
            "",
            "#define KSM_KMEM_CACHE(__struct, __flags) kmem_cache_create(#__struct,\\",
            "\t\tsizeof(struct __struct), __alignof__(struct __struct),\\",
            "\t\t(__flags), NULL)",
            ""
          ],
          "function_name": null,
          "description": "定义了KSM（内核同一内存页合并）模块的核心数据结构和全局变量，包括稳定树和不稳定树的数据结构、NUMA支持相关定义、哈希表和红黑树操作接口，以及用于跟踪扫描进度的kscan结构。核心功能是建立KSM内存合并算法的基础框架。",
          "similarity": 0.5863254070281982
        },
        {
          "chunk_id": 13,
          "file_path": "mm/ksm.c",
          "start_line": 3171,
          "end_line": 3288,
          "content": [
            "static ssize_t pages_to_scan_show(struct kobject *kobj,",
            "\t\t\t\t  struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_thread_pages_to_scan);",
            "}",
            "static ssize_t pages_to_scan_store(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr,",
            "\t\t\t\t   const char *buf, size_t count)",
            "{",
            "\tunsigned int nr_pages;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &nr_pages);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\tksm_thread_pages_to_scan = nr_pages;",
            "",
            "\treturn count;",
            "}",
            "static ssize_t run_show(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\tchar *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_run);",
            "}",
            "static ssize_t run_store(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\t const char *buf, size_t count)",
            "{",
            "\tunsigned int flags;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &flags);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "\tif (flags > KSM_RUN_UNMERGE)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * KSM_RUN_MERGE sets ksmd running, and 0 stops it running.",
            "\t * KSM_RUN_UNMERGE stops it running and unmerges all rmap_items,",
            "\t * breaking COW to free the pages_shared (but leaves mm_slots",
            "\t * on the list for when ksmd may be set running again).",
            "\t */",
            "",
            "\tmutex_lock(&ksm_thread_mutex);",
            "\twait_while_offlining();",
            "\tif (ksm_run != flags) {",
            "\t\tksm_run = flags;",
            "\t\tif (flags & KSM_RUN_UNMERGE) {",
            "\t\t\tset_current_oom_origin();",
            "\t\t\terr = unmerge_and_remove_all_rmap_items();",
            "\t\t\tclear_current_oom_origin();",
            "\t\t\tif (err) {",
            "\t\t\t\tksm_run = KSM_RUN_STOP;",
            "\t\t\t\tcount = err;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\tif (flags & KSM_RUN_MERGE)",
            "\t\twake_up_interruptible(&ksm_thread_wait);",
            "",
            "\treturn count;",
            "}",
            "static ssize_t merge_across_nodes_show(struct kobject *kobj,",
            "\t\t\t\t       struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_merge_across_nodes);",
            "}",
            "static ssize_t merge_across_nodes_store(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr,",
            "\t\t\t\t   const char *buf, size_t count)",
            "{",
            "\tint err;",
            "\tunsigned long knob;",
            "",
            "\terr = kstrtoul(buf, 10, &knob);",
            "\tif (err)",
            "\t\treturn err;",
            "\tif (knob > 1)",
            "\t\treturn -EINVAL;",
            "",
            "\tmutex_lock(&ksm_thread_mutex);",
            "\twait_while_offlining();",
            "\tif (ksm_merge_across_nodes != knob) {",
            "\t\tif (ksm_pages_shared || remove_all_stable_nodes())",
            "\t\t\terr = -EBUSY;",
            "\t\telse if (root_stable_tree == one_stable_tree) {",
            "\t\t\tstruct rb_root *buf;",
            "\t\t\t/*",
            "\t\t\t * This is the first time that we switch away from the",
            "\t\t\t * default of merging across nodes: must now allocate",
            "\t\t\t * a buffer to hold as many roots as may be needed.",
            "\t\t\t * Allocate stable and unstable together:",
            "\t\t\t * MAXSMP NODES_SHIFT 10 will use 16kB.",
            "\t\t\t */",
            "\t\t\tbuf = kcalloc(nr_node_ids + nr_node_ids, sizeof(*buf),",
            "\t\t\t\t      GFP_KERNEL);",
            "\t\t\t/* Let us assume that RB_ROOT is NULL is zero */",
            "\t\t\tif (!buf)",
            "\t\t\t\terr = -ENOMEM;",
            "\t\t\telse {",
            "\t\t\t\troot_stable_tree = buf;",
            "\t\t\t\troot_unstable_tree = buf + nr_node_ids;",
            "\t\t\t\t/* Stable tree is empty but not the unstable */",
            "\t\t\t\troot_unstable_tree[0] = one_unstable_tree[0];",
            "\t\t\t}",
            "\t\t}",
            "\t\tif (!err) {",
            "\t\t\tksm_merge_across_nodes = knob;",
            "\t\t\tksm_nr_node_ids = knob ? 1 : nr_node_ids;",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\treturn err ? err : count;",
            "}"
          ],
          "function_name": "pages_to_scan_show, pages_to_scan_store, run_show, run_store, merge_across_nodes_show, merge_across_nodes_store",
          "description": "pages_to_scan_*控制KSMAPI扫描页数；run_*控制KSMAPI运行模式（启动/停止/解合并）；merge_across_nodes_*配置是否允许跨NUMA节点合并，切换时重构稳定树根节点数组",
          "similarity": 0.5330699682235718
        },
        {
          "chunk_id": 14,
          "file_path": "mm/ksm.c",
          "start_line": 3300,
          "end_line": 3407,
          "content": [
            "static ssize_t use_zero_pages_show(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_use_zero_pages);",
            "}",
            "static ssize_t use_zero_pages_store(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr,",
            "\t\t\t\t   const char *buf, size_t count)",
            "{",
            "\tint err;",
            "\tbool value;",
            "",
            "\terr = kstrtobool(buf, &value);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\tksm_use_zero_pages = value;",
            "",
            "\treturn count;",
            "}",
            "static ssize_t max_page_sharing_show(struct kobject *kobj,",
            "\t\t\t\t     struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_max_page_sharing);",
            "}",
            "static ssize_t max_page_sharing_store(struct kobject *kobj,",
            "\t\t\t\t      struct kobj_attribute *attr,",
            "\t\t\t\t      const char *buf, size_t count)",
            "{",
            "\tint err;",
            "\tint knob;",
            "",
            "\terr = kstrtoint(buf, 10, &knob);",
            "\tif (err)",
            "\t\treturn err;",
            "\t/*",
            "\t * When a KSM page is created it is shared by 2 mappings. This",
            "\t * being a signed comparison, it implicitly verifies it's not",
            "\t * negative.",
            "\t */",
            "\tif (knob < 2)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (READ_ONCE(ksm_max_page_sharing) == knob)",
            "\t\treturn count;",
            "",
            "\tmutex_lock(&ksm_thread_mutex);",
            "\twait_while_offlining();",
            "\tif (ksm_max_page_sharing != knob) {",
            "\t\tif (ksm_pages_shared || remove_all_stable_nodes())",
            "\t\t\terr = -EBUSY;",
            "\t\telse",
            "\t\t\tksm_max_page_sharing = knob;",
            "\t}",
            "\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\treturn err ? err : count;",
            "}",
            "static ssize_t pages_scanned_show(struct kobject *kobj,",
            "\t\t\t\t  struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_scanned);",
            "}",
            "static ssize_t pages_shared_show(struct kobject *kobj,",
            "\t\t\t\t struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_shared);",
            "}",
            "static ssize_t pages_sharing_show(struct kobject *kobj,",
            "\t\t\t\t  struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_sharing);",
            "}",
            "static ssize_t pages_unshared_show(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_unshared);",
            "}",
            "static ssize_t pages_volatile_show(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\tlong ksm_pages_volatile;",
            "",
            "\tksm_pages_volatile = ksm_rmap_items - ksm_pages_shared",
            "\t\t\t\t- ksm_pages_sharing - ksm_pages_unshared;",
            "\t/*",
            "\t * It was not worth any locking to calculate that statistic,",
            "\t * but it might therefore sometimes be negative: conceal that.",
            "\t */",
            "\tif (ksm_pages_volatile < 0)",
            "\t\tksm_pages_volatile = 0;",
            "\treturn sysfs_emit(buf, \"%ld\\n\", ksm_pages_volatile);",
            "}",
            "static ssize_t ksm_zero_pages_show(struct kobject *kobj,",
            "\t\t\t\tstruct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%ld\\n\", atomic_long_read(&ksm_zero_pages));",
            "}",
            "static ssize_t general_profit_show(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\tlong general_profit;",
            "",
            "\tgeneral_profit = (ksm_pages_sharing + atomic_long_read(&ksm_zero_pages)) * PAGE_SIZE -",
            "\t\t\t\tksm_rmap_items * sizeof(struct ksm_rmap_item);",
            "",
            "\treturn sysfs_emit(buf, \"%ld\\n\", general_profit);",
            "}"
          ],
          "function_name": "use_zero_pages_show, use_zero_pages_store, max_page_sharing_show, max_page_sharing_store, pages_scanned_show, pages_shared_show, pages_sharing_show, pages_unshared_show, pages_volatile_show, ksm_zero_pages_show, general_profit_show",
          "description": "use_zero_pages_*控制是否使用零填充页面；max_page_sharing_*设置最大共享页数；pages_*系列接口统计KSMAPI页扫描、共享、分裂等状态；general_profit_*计算整体内存优化收益",
          "similarity": 0.49836984276771545
        },
        {
          "chunk_id": 9,
          "file_path": "mm/ksm.c",
          "start_line": 2608,
          "end_line": 2731,
          "content": [
            "int ksm_enable_merge_any(struct mm_struct *mm)",
            "{",
            "\tint err;",
            "",
            "\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn 0;",
            "",
            "\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {",
            "\t\terr = __ksm_enter(mm);",
            "\t\tif (err)",
            "\t\t\treturn err;",
            "\t}",
            "",
            "\tset_bit(MMF_VM_MERGE_ANY, &mm->flags);",
            "\tksm_add_vmas(mm);",
            "",
            "\treturn 0;",
            "}",
            "int ksm_disable_merge_any(struct mm_struct *mm)",
            "{",
            "\tint err;",
            "",
            "\tif (!test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn 0;",
            "",
            "\terr = ksm_del_vmas(mm);",
            "\tif (err) {",
            "\t\tksm_add_vmas(mm);",
            "\t\treturn err;",
            "\t}",
            "",
            "\tclear_bit(MMF_VM_MERGE_ANY, &mm->flags);",
            "\treturn 0;",
            "}",
            "int ksm_disable(struct mm_struct *mm)",
            "{",
            "\tmmap_assert_write_locked(mm);",
            "",
            "\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags))",
            "\t\treturn 0;",
            "\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn ksm_disable_merge_any(mm);",
            "\treturn ksm_del_vmas(mm);",
            "}",
            "int ksm_madvise(struct vm_area_struct *vma, unsigned long start,",
            "\t\tunsigned long end, int advice, unsigned long *vm_flags)",
            "{",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tint err;",
            "",
            "\tswitch (advice) {",
            "\tcase MADV_MERGEABLE:",
            "\t\tif (vma->vm_flags & VM_MERGEABLE)",
            "\t\t\treturn 0;",
            "\t\tif (!vma_ksm_compatible(vma))",
            "\t\t\treturn 0;",
            "",
            "\t\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {",
            "\t\t\terr = __ksm_enter(mm);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "",
            "\t\t*vm_flags |= VM_MERGEABLE;",
            "\t\tbreak;",
            "",
            "\tcase MADV_UNMERGEABLE:",
            "\t\tif (!(*vm_flags & VM_MERGEABLE))",
            "\t\t\treturn 0;\t\t/* just ignore the advice */",
            "",
            "\t\tif (vma->anon_vma) {",
            "\t\t\terr = unmerge_ksm_pages(vma, start, end, true);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "",
            "\t\t*vm_flags &= ~VM_MERGEABLE;",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __ksm_enter(struct mm_struct *mm)",
            "{",
            "\tstruct ksm_mm_slot *mm_slot;",
            "\tstruct mm_slot *slot;",
            "\tint needs_wakeup;",
            "",
            "\tmm_slot = mm_slot_alloc(mm_slot_cache);",
            "\tif (!mm_slot)",
            "\t\treturn -ENOMEM;",
            "",
            "\tslot = &mm_slot->slot;",
            "",
            "\t/* Check ksm_run too?  Would need tighter locking */",
            "\tneeds_wakeup = list_empty(&ksm_mm_head.slot.mm_node);",
            "",
            "\tspin_lock(&ksm_mmlist_lock);",
            "\tmm_slot_insert(mm_slots_hash, mm, slot);",
            "\t/*",
            "\t * When KSM_RUN_MERGE (or KSM_RUN_STOP),",
            "\t * insert just behind the scanning cursor, to let the area settle",
            "\t * down a little; when fork is followed by immediate exec, we don't",
            "\t * want ksmd to waste time setting up and tearing down an rmap_list.",
            "\t *",
            "\t * But when KSM_RUN_UNMERGE, it's important to insert ahead of its",
            "\t * scanning cursor, otherwise KSM pages in newly forked mms will be",
            "\t * missed: then we might as well insert at the end of the list.",
            "\t */",
            "\tif (ksm_run & KSM_RUN_UNMERGE)",
            "\t\tlist_add_tail(&slot->mm_node, &ksm_mm_head.slot.mm_node);",
            "\telse",
            "\t\tlist_add_tail(&slot->mm_node, &ksm_scan.mm_slot->slot.mm_node);",
            "\tspin_unlock(&ksm_mmlist_lock);",
            "",
            "\tset_bit(MMF_VM_MERGEABLE, &mm->flags);",
            "\tmmgrab(mm);",
            "",
            "\tif (needs_wakeup)",
            "\t\twake_up_interruptible(&ksm_thread_wait);",
            "",
            "\ttrace_ksm_enter(mm);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ksm_enable_merge_any, ksm_disable_merge_any, ksm_disable, ksm_madvise, __ksm_enter",
          "description": "ksm_enable_merge_any 启用全局合并标志并注册内存区域。ksm_disable_merge_any 禁用合并并清理配置。ksm_disable 移除所有KSM配置。ksm_madvise 处理MADV_MERGEABLE/MADV_UNMERGEABLE建议，调整页面属性。__ksm_enter 注册内存区域到KSM管理系统。",
          "similarity": 0.49542325735092163
        },
        {
          "chunk_id": 2,
          "file_path": "mm/ksm.c",
          "start_line": 486,
          "end_line": 622,
          "content": [
            "static int break_ksm(struct vm_area_struct *vma, unsigned long addr, bool lock_vma)",
            "{",
            "\tvm_fault_t ret = 0;",
            "\tconst struct mm_walk_ops *ops = lock_vma ?",
            "\t\t\t\t&break_ksm_lock_vma_ops : &break_ksm_ops;",
            "",
            "\tdo {",
            "\t\tint ksm_page;",
            "",
            "\t\tcond_resched();",
            "\t\tksm_page = walk_page_range_vma(vma, addr, addr + 1, ops, NULL);",
            "\t\tif (WARN_ON_ONCE(ksm_page < 0))",
            "\t\t\treturn ksm_page;",
            "\t\tif (!ksm_page)",
            "\t\t\treturn 0;",
            "\t\tret = handle_mm_fault(vma, addr,",
            "\t\t\t\t      FAULT_FLAG_UNSHARE | FAULT_FLAG_REMOTE,",
            "\t\t\t\t      NULL);",
            "\t} while (!(ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | VM_FAULT_OOM)));",
            "\t/*",
            "\t * We must loop until we no longer find a KSM page because",
            "\t * handle_mm_fault() may back out if there's any difficulty e.g. if",
            "\t * pte accessed bit gets updated concurrently.",
            "\t *",
            "\t * VM_FAULT_SIGBUS could occur if we race with truncation of the",
            "\t * backing file, which also invalidates anonymous pages: that's",
            "\t * okay, that truncation will have unmapped the PageKsm for us.",
            "\t *",
            "\t * VM_FAULT_OOM: at the time of writing (late July 2009), setting",
            "\t * aside mem_cgroup limits, VM_FAULT_OOM would only be set if the",
            "\t * current task has TIF_MEMDIE set, and will be OOM killed on return",
            "\t * to user; and ksmd, having no mm, would never be chosen for that.",
            "\t *",
            "\t * But if the mm is in a limited mem_cgroup, then the fault may fail",
            "\t * with VM_FAULT_OOM even if the current task is not TIF_MEMDIE; and",
            "\t * even ksmd can fail in this way - though it's usually breaking ksm",
            "\t * just to undo a merge it made a moment before, so unlikely to oom.",
            "\t *",
            "\t * That's a pity: we might therefore have more kernel pages allocated",
            "\t * than we're counting as nodes in the stable tree; but ksm_do_scan",
            "\t * will retry to break_cow on each pass, so should recover the page",
            "\t * in due course.  The important thing is to not let VM_MERGEABLE",
            "\t * be cleared while any such pages might remain in the area.",
            "\t */",
            "\treturn (ret & VM_FAULT_OOM) ? -ENOMEM : 0;",
            "}",
            "static bool vma_ksm_compatible(struct vm_area_struct *vma)",
            "{",
            "\tif (vma->vm_flags & (VM_SHARED  | VM_MAYSHARE   | VM_PFNMAP  |",
            "\t\t\t     VM_IO      | VM_DONTEXPAND | VM_HUGETLB |",
            "\t\t\t     VM_MIXEDMAP))",
            "\t\treturn false;\t\t/* just ignore the advice */",
            "",
            "\tif (vma_is_dax(vma))",
            "\t\treturn false;",
            "",
            "#ifdef VM_SAO",
            "\tif (vma->vm_flags & VM_SAO)",
            "\t\treturn false;",
            "#endif",
            "#ifdef VM_SPARC_ADI",
            "\tif (vma->vm_flags & VM_SPARC_ADI)",
            "\t\treturn false;",
            "#endif",
            "",
            "\treturn true;",
            "}",
            "static void break_cow(struct ksm_rmap_item *rmap_item)",
            "{",
            "\tstruct mm_struct *mm = rmap_item->mm;",
            "\tunsigned long addr = rmap_item->address;",
            "\tstruct vm_area_struct *vma;",
            "",
            "\t/*",
            "\t * It is not an accident that whenever we want to break COW",
            "\t * to undo, we also need to drop a reference to the anon_vma.",
            "\t */",
            "\tput_anon_vma(rmap_item->anon_vma);",
            "",
            "\tmmap_read_lock(mm);",
            "\tvma = find_mergeable_vma(mm, addr);",
            "\tif (vma)",
            "\t\tbreak_ksm(vma, addr, false);",
            "\tmmap_read_unlock(mm);",
            "}",
            "static inline int get_kpfn_nid(unsigned long kpfn)",
            "{",
            "\treturn ksm_merge_across_nodes ? 0 : NUMA(pfn_to_nid(kpfn));",
            "}",
            "static inline void free_stable_node_chain(struct ksm_stable_node *chain,",
            "\t\t\t\t\t  struct rb_root *root)",
            "{",
            "\trb_erase(&chain->node, root);",
            "\tfree_stable_node(chain);",
            "\tksm_stable_node_chains--;",
            "}",
            "static void remove_node_from_stable_tree(struct ksm_stable_node *stable_node)",
            "{",
            "\tstruct ksm_rmap_item *rmap_item;",
            "",
            "\t/* check it's not STABLE_NODE_CHAIN or negative */",
            "\tBUG_ON(stable_node->rmap_hlist_len < 0);",
            "",
            "\thlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {",
            "\t\tif (rmap_item->hlist.next) {",
            "\t\t\tksm_pages_sharing--;",
            "\t\t\ttrace_ksm_remove_rmap_item(stable_node->kpfn, rmap_item, rmap_item->mm);",
            "\t\t} else {",
            "\t\t\tksm_pages_shared--;",
            "\t\t}",
            "",
            "\t\trmap_item->mm->ksm_merging_pages--;",
            "",
            "\t\tVM_BUG_ON(stable_node->rmap_hlist_len <= 0);",
            "\t\tstable_node->rmap_hlist_len--;",
            "\t\tput_anon_vma(rmap_item->anon_vma);",
            "\t\trmap_item->address &= PAGE_MASK;",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\t/*",
            "\t * We need the second aligned pointer of the migrate_nodes",
            "\t * list_head to stay clear from the rb_parent_color union",
            "\t * (aligned and different than any node) and also different",
            "\t * from &migrate_nodes. This will verify that future list.h changes",
            "\t * don't break STABLE_NODE_DUP_HEAD. Only recent gcc can handle it.",
            "\t */",
            "\tBUILD_BUG_ON(STABLE_NODE_DUP_HEAD <= &migrate_nodes);",
            "\tBUILD_BUG_ON(STABLE_NODE_DUP_HEAD >= &migrate_nodes + 1);",
            "",
            "\ttrace_ksm_remove_ksm_page(stable_node->kpfn);",
            "\tif (stable_node->head == &migrate_nodes)",
            "\t\tlist_del(&stable_node->list);",
            "\telse",
            "\t\tstable_node_dup_del(stable_node);",
            "\tfree_stable_node(stable_node);",
            "}"
          ],
          "function_name": "break_ksm, vma_ksm_compatible, break_cow, get_kpfn_nid, free_stable_node_chain, remove_node_from_stable_tree",
          "description": "实现KSM页面分解逻辑，包含检查VMA是否兼容KSM、强制打破写时复制（COW）操作、获取KPFN NUMA节点ID以及稳定树节点链表清理等功能。核心作用是执行实际的KSM页面分离操作并维护稳定树结构。",
          "similarity": 0.4904865324497223
        }
      ]
    },
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.562536358833313,
      "chunks": [
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.5612062215805054
        },
        {
          "chunk_id": 0,
          "file_path": "mm/mempolicy.c",
          "start_line": 1,
          "end_line": 167,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Simple NUMA memory policy for the Linux kernel.",
            " *",
            " * Copyright 2003,2004 Andi Kleen, SuSE Labs.",
            " * (C) Copyright 2005 Christoph Lameter, Silicon Graphics, Inc.",
            " *",
            " * NUMA policy allows the user to give hints in which node(s) memory should",
            " * be allocated.",
            " *",
            " * Support four policies per VMA and per process:",
            " *",
            " * The VMA policy has priority over the process policy for a page fault.",
            " *",
            " * interleave     Allocate memory interleaved over a set of nodes,",
            " *                with normal fallback if it fails.",
            " *                For VMA based allocations this interleaves based on the",
            " *                offset into the backing object or offset into the mapping",
            " *                for anonymous memory. For process policy an process counter",
            " *                is used.",
            " *",
            " * weighted interleave",
            " *                Allocate memory interleaved over a set of nodes based on",
            " *                a set of weights (per-node), with normal fallback if it",
            " *                fails.  Otherwise operates the same as interleave.",
            " *                Example: nodeset(0,1) & weights (2,1) - 2 pages allocated",
            " *                on node 0 for every 1 page allocated on node 1.",
            " *",
            " * bind           Only allocate memory on a specific set of nodes,",
            " *                no fallback.",
            " *                FIXME: memory is allocated starting with the first node",
            " *                to the last. It would be better if bind would truly restrict",
            " *                the allocation to memory nodes instead",
            " *",
            " * preferred      Try a specific node first before normal fallback.",
            " *                As a special case NUMA_NO_NODE here means do the allocation",
            " *                on the local CPU. This is normally identical to default,",
            " *                but useful to set in a VMA when you have a non default",
            " *                process policy.",
            " *",
            " * preferred many Try a set of nodes first before normal fallback. This is",
            " *                similar to preferred without the special case.",
            " *",
            " * default        Allocate on the local node first, or when on a VMA",
            " *                use the process policy. This is what Linux always did",
            " *\t\t  in a NUMA aware kernel and still does by, ahem, default.",
            " *",
            " * The process policy is applied for most non interrupt memory allocations",
            " * in that process' context. Interrupts ignore the policies and always",
            " * try to allocate on the local CPU. The VMA policy is only applied for memory",
            " * allocations for a VMA in the VM.",
            " *",
            " * Currently there are a few corner cases in swapping where the policy",
            " * is not applied, but the majority should be handled. When process policy",
            " * is used it is not remembered over swap outs/swap ins.",
            " *",
            " * Only the highest zone in the zone hierarchy gets policied. Allocations",
            " * requesting a lower zone just use default policy. This implies that",
            " * on systems with highmem kernel lowmem allocation don't get policied.",
            " * Same with GFP_DMA allocations.",
            " *",
            " * For shmem/tmpfs shared memory the policy is shared between",
            " * all users and remembered even when nobody has memory mapped.",
            " */",
            "",
            "/* Notebook:",
            "   fix mmap readahead to honour policy and enable policy for any page cache",
            "   object",
            "   statistics for bigpages",
            "   global policy for page cache? currently it uses process policy. Requires",
            "   first item above.",
            "   handle mremap for shared memory (currently ignored for the policy)",
            "   grows down?",
            "   make bind policy root only? It can trigger oom much faster and the",
            "   kernel is not always grateful with that.",
            "*/",
            "",
            "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt",
            "",
            "#include <linux/mempolicy.h>",
            "#include <linux/pagewalk.h>",
            "#include <linux/highmem.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/kernel.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/slab.h>",
            "#include <linux/string.h>",
            "#include <linux/export.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/init.h>",
            "#include <linux/compat.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/swap.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/migrate.h>",
            "#include <linux/ksm.h>",
            "#include <linux/rmap.h>",
            "#include <linux/security.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/ctype.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/printk.h>",
            "#include <linux/swapops.h>",
            "#include <linux/gcd.h>",
            "",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/memory.h>",
            "",
            "#include \"internal.h\"",
            "",
            "/* Internal flags */",
            "#define MPOL_MF_DISCONTIG_OK (MPOL_MF_INTERNAL << 0)\t/* Skip checks for continuous vmas */",
            "#define MPOL_MF_INVERT       (MPOL_MF_INTERNAL << 1)\t/* Invert check for nodemask */",
            "#define MPOL_MF_WRLOCK       (MPOL_MF_INTERNAL << 2)\t/* Write-lock walked vmas */",
            "",
            "static struct kmem_cache *policy_cache;",
            "static struct kmem_cache *sn_cache;",
            "",
            "/* Highest zone. An specific allocation for a zone below that is not",
            "   policied. */",
            "enum zone_type policy_zone = 0;",
            "",
            "/*",
            " * run-time system-wide default policy => local allocation",
            " */",
            "static struct mempolicy default_policy = {",
            "\t.refcnt = ATOMIC_INIT(1), /* never free it */",
            "\t.mode = MPOL_LOCAL,",
            "};",
            "",
            "static struct mempolicy preferred_node_policy[MAX_NUMNODES];",
            "",
            "/*",
            " * weightiness balances the tradeoff between small weights (cycles through nodes",
            " * faster, more fair/even distribution) and large weights (smaller errors",
            " * between actual bandwidth ratios and weight ratios). 32 is a number that has",
            " * been found to perform at a reasonable compromise between the two goals.",
            " */",
            "static const int weightiness = 32;",
            "",
            "/*",
            " * A null weighted_interleave_state is interpreted as having .mode=\"auto\",",
            " * and .iw_table is interpreted as an array of 1s with length nr_node_ids.",
            " */",
            "struct weighted_interleave_state {",
            "\tbool mode_auto;",
            "\tu8 iw_table[];",
            "};",
            "static struct weighted_interleave_state __rcu *wi_state;",
            "static unsigned int *node_bw_table;",
            "",
            "/*",
            " * wi_state_lock protects both wi_state and node_bw_table.",
            " * node_bw_table is only used by writers to update wi_state.",
            " */",
            "static DEFINE_MUTEX(wi_state_lock);",
            ""
          ],
          "function_name": null,
          "description": "定义NUMA内存策略相关结构体和全局变量，包括默认策略default_policy、节点带宽表node_bw_table及加权交错策略状态wi_state，用于管理多节点内存分配策略。",
          "similarity": 0.5348789095878601
        },
        {
          "chunk_id": 12,
          "file_path": "mm/mempolicy.c",
          "start_line": 2024,
          "end_line": 2135,
          "content": [
            "static unsigned int interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int nid;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "\t/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnid = next_node_in(current->il_prev, policy->nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\tif (nid < MAX_NUMNODES)",
            "\t\tcurrent->il_prev = nid;",
            "\treturn nid;",
            "}",
            "unsigned int mempolicy_slab_node(void)",
            "{",
            "\tstruct mempolicy *policy;",
            "\tint node = numa_mem_id();",
            "",
            "\tif (!in_task())",
            "\t\treturn node;",
            "",
            "\tpolicy = current->mempolicy;",
            "\tif (!policy)",
            "\t\treturn node;",
            "",
            "\tswitch (policy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\treturn first_node(policy->nodes);",
            "",
            "\tcase MPOL_INTERLEAVE:",
            "\t\treturn interleave_nodes(policy);",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn weighted_interleave_nodes(policy);",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t{",
            "\t\tstruct zoneref *z;",
            "",
            "\t\t/*",
            "\t\t * Follow bind policy behavior and start allocation at the",
            "\t\t * first node.",
            "\t\t */",
            "\t\tstruct zonelist *zonelist;",
            "\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);",
            "\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];",
            "\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,",
            "\t\t\t\t\t\t\t&policy->nodes);",
            "\t\treturn z->zone ? zone_to_nid(z->zone) : node;",
            "\t}",
            "\tcase MPOL_LOCAL:",
            "\t\treturn node;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static unsigned int read_once_policy_nodemask(struct mempolicy *pol,",
            "\t\t\t\t\t      nodemask_t *mask)",
            "{",
            "\t/*",
            "\t * barrier stabilizes the nodemask locally so that it can be iterated",
            "\t * over safely without concern for changes. Allocators validate node",
            "\t * selection does not violate mems_allowed, so this is safe.",
            "\t */",
            "\tbarrier();",
            "\tmemcpy(mask, &pol->nodes, sizeof(nodemask_t));",
            "\tbarrier();",
            "\treturn nodes_weight(*mask);",
            "}",
            "static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nr_nodes;",
            "\tu8 *table = NULL;",
            "\tunsigned int weight_total = 0;",
            "\tu8 weight;",
            "\tint nid = 0;",
            "",
            "\tnr_nodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nr_nodes)",
            "\t\treturn numa_node_id();",
            "",
            "\trcu_read_lock();",
            "",
            "\tstate = rcu_dereference(wi_state);",
            "\t/* Uninitialized wi_state means we should assume all weights are 1 */",
            "\tif (state)",
            "\t\ttable = state->iw_table;",
            "",
            "\t/* calculate the total weight */",
            "\tfor_each_node_mask(nid, nodemask)",
            "\t\tweight_total += table ? table[nid] : 1;",
            "",
            "\t/* Calculate the node offset based on totals */",
            "\ttarget = ilx % weight_total;",
            "\tnid = first_node(nodemask);",
            "\twhile (target) {",
            "\t\t/* detect system default usage */",
            "\t\tweight = table ? table[nid] : 1;",
            "\t\tif (target < weight)",
            "\t\t\tbreak;",
            "\t\ttarget -= weight;",
            "\t\tnid = next_node_in(nid, nodemask);",
            "\t}",
            "\trcu_read_unlock();",
            "\treturn nid;",
            "}"
          ],
          "function_name": "interleave_nodes, mempolicy_slab_node, read_once_policy_nodemask, weighted_interleave_nid",
          "description": "interleave_nodes 计算交错分配的下一个节点；mempolicy_slab_node 根据内存策略返回Slab分配的节点；read_once_policy_nodemask 安全读取策略节点掩码；weighted_interleave_nid 基于权重计算加权交错分配的目标节点。",
          "similarity": 0.5281967520713806
        },
        {
          "chunk_id": 13,
          "file_path": "mm/mempolicy.c",
          "start_line": 2149,
          "end_line": 2255,
          "content": [
            "static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nnodes;",
            "\tint i;",
            "\tint nid;",
            "",
            "\tnnodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nnodes)",
            "\t\treturn numa_node_id();",
            "\ttarget = ilx % nnodes;",
            "\tnid = first_node(nodemask);",
            "\tfor (i = 0; i < target; i++)",
            "\t\tnid = next_node(nid, nodemask);",
            "\treturn nid;",
            "}",
            "int huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,",
            "\t\tstruct mempolicy **mpol, nodemask_t **nodemask)",
            "{",
            "\tpgoff_t ilx;",
            "\tint nid;",
            "",
            "\tnid = numa_node_id();",
            "\t*mpol = get_vma_policy(vma, addr, hstate_vma(vma)->order, &ilx);",
            "\t*nodemask = policy_nodemask(gfp_flags, *mpol, ilx, &nid);",
            "\treturn nid;",
            "}",
            "bool init_nodemask_of_mempolicy(nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "",
            "\tif (!(mask && current->mempolicy))",
            "\t\treturn false;",
            "",
            "\ttask_lock(current);",
            "\tmempolicy = current->mempolicy;",
            "\tswitch (mempolicy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*mask = mempolicy->nodes;",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tinit_nodemask_of_node(mask, numa_node_id());",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "\ttask_unlock(current);",
            "",
            "\treturn true;",
            "}",
            "bool mempolicy_in_oom_domain(struct task_struct *tsk,",
            "\t\t\t\t\tconst nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "\tbool ret = true;",
            "",
            "\tif (!mask)",
            "\t\treturn ret;",
            "",
            "\ttask_lock(tsk);",
            "\tmempolicy = tsk->mempolicy;",
            "\tif (mempolicy && mempolicy->mode == MPOL_BIND)",
            "\t\tret = nodes_intersects(mempolicy->nodes, *mask);",
            "\ttask_unlock(tsk);",
            "",
            "\treturn ret;",
            "}",
            "static unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tint nodes;",
            "\tunsigned long nr_pages_per_node;",
            "\tint delta;",
            "\tint i;",
            "\tunsigned long nr_allocated;",
            "\tunsigned long total_allocated = 0;",
            "",
            "\tnodes = nodes_weight(pol->nodes);",
            "\tnr_pages_per_node = nr_pages / nodes;",
            "\tdelta = nr_pages - nodes * nr_pages_per_node;",
            "",
            "\tfor (i = 0; i < nodes; i++) {",
            "\t\tif (delta) {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node + 1, NULL,",
            "\t\t\t\t\tpage_array);",
            "\t\t\tdelta--;",
            "\t\t} else {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node, NULL, page_array);",
            "\t\t}",
            "",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t}",
            "",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "interleave_nid, huge_node, init_nodemask_of_mempolicy, mempolicy_in_oom_domain, alloc_pages_bulk_array_interleave",
          "description": "interleave_nid 计算简单交错分配的目标节点；huge_node 结合HugeTLB策略确定大页分配节点；init_nodemask_of_mempolicy 初始化当前进程的内存策略节点掩码；mempolicy_in_oom_domain 检查策略节点是否与OOM域重叠；alloc_pages_bulk_array_interleave 执行批量交错分配。",
          "similarity": 0.5221063494682312
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mempolicy.c",
          "start_line": 168,
          "end_line": 268,
          "content": [
            "static u8 get_il_weight(int node)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tu8 weight = 1;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state)",
            "\t\tweight = state->iw_table[node];",
            "\trcu_read_unlock();",
            "\treturn weight;",
            "}",
            "static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)",
            "{",
            "\tu64 sum_bw = 0;",
            "\tunsigned int cast_sum_bw, scaling_factor = 1, iw_gcd = 0;",
            "\tint nid;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tsum_bw += bw[nid];",
            "",
            "\t/* Scale bandwidths to whole numbers in the range [1, weightiness] */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t/*",
            "\t\t * Try not to perform 64-bit division.",
            "\t\t * If sum_bw < scaling_factor, then sum_bw < U32_MAX.",
            "\t\t * If sum_bw > scaling_factor, then round the weight up to 1.",
            "\t\t */",
            "\t\tscaling_factor = weightiness * bw[nid];",
            "\t\tif (bw[nid] && sum_bw < scaling_factor) {",
            "\t\t\tcast_sum_bw = (unsigned int)sum_bw;",
            "\t\t\tnew_iw[nid] = scaling_factor / cast_sum_bw;",
            "\t\t} else {",
            "\t\t\tnew_iw[nid] = 1;",
            "\t\t}",
            "\t\tif (!iw_gcd)",
            "\t\t\tiw_gcd = new_iw[nid];",
            "\t\tiw_gcd = gcd(iw_gcd, new_iw[nid]);",
            "\t}",
            "",
            "\t/* 1:2 is strictly better than 16:32. Reduce by the weights' GCD. */",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tnew_iw[nid] /= iw_gcd;",
            "}",
            "int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tunsigned int *old_bw, *new_bw;",
            "\tunsigned int bw_val;",
            "\tint i;",
            "",
            "\tbw_val = min(coords->read_bandwidth, coords->write_bandwidth);",
            "\tnew_bw = kcalloc(nr_node_ids, sizeof(unsigned int), GFP_KERNEL);",
            "\tif (!new_bw)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew_wi_state = kmalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state) {",
            "\t\tkfree(new_bw);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tnew_wi_state->mode_auto = true;",
            "\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\tnew_wi_state->iw_table[i] = 1;",
            "",
            "\t/*",
            "\t * Update bandwidth info, even in manual mode. That way, when switching",
            "\t * to auto mode in the future, iw_table can be overwritten using",
            "\t * accurate bw data.",
            "\t */",
            "\tmutex_lock(&wi_state_lock);",
            "",
            "\told_bw = node_bw_table;",
            "\tif (old_bw)",
            "\t\tmemcpy(new_bw, old_bw, nr_node_ids * sizeof(*old_bw));",
            "\tnew_bw[node] = bw_val;",
            "\tnode_bw_table = new_bw;",
            "",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state && !old_wi_state->mode_auto) {",
            "\t\t/* Manual mode; skip reducing weights and updating wi_state */",
            "\t\tmutex_unlock(&wi_state_lock);",
            "\t\tkfree(new_wi_state);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* NULL wi_state assumes auto=true; reduce weights and update wi_state*/",
            "\treduce_interleave_weights(new_bw, new_wi_state->iw_table);",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "out:",
            "\tkfree(old_bw);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_il_weight, reduce_interleave_weights, mempolicy_set_node_perf",
          "description": "实现带权交错策略的权重计算与调整逻辑，通过获取节点带宽数据动态修改权重比例，支持根据性能参数更新节点间内存分配优先级。",
          "similarity": 0.5153673887252808
        }
      ]
    }
  ]
}