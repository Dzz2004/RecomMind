{
  "query": "disk quota",
  "timestamp": "2025-12-26 00:21:37",
  "retrieved_files": [
    {
      "source_file": "mm/shmem_quota.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:18:12\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `shmem_quota.c`\n\n---\n\n# shmem_quota.c 技术文档\n\n## 1. 文件概述\n\n`shmem_quota.c` 实现了针对内存文件系统（如 tmpfs）的配额（quota）管理机制。由于 tmpfs 等内存文件系统没有持久化存储，传统的基于磁盘配额文件的方式无法使用。该文件提供了一种**纯内存配额格式**（in-memory quota format），通过红黑树（rbtree）在内存中维护用户/组的配额限制信息，并与 Linux 内核通用配额子系统集成，从而支持对 tmpfs 的空间和 inode 使用量进行配额控制。\n\n关键设计原则是：**不能释放未使用的 dquot 结构**，因为一旦释放，配额限制信息将永久丢失（无持久化后端可重新加载）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct quota_id`**  \n  表示一个配额主体（用户或组）的配额限制信息，作为红黑树节点存储：\n  - `node`: 红黑树节点\n  - `id`: 用户/组 ID（qid_t）\n  - `bhardlimit/bsoftlimit`: 块（空间）硬/软限制\n  - `ihardlimit/isoftlimit`: inode 硬/软限制\n\n### 主要函数\n\n- **`shmem_check_quota_file()`**  \n  检查配额文件是否存在（内存配额无实际文件，始终返回成功）\n\n- **`shmem_read_file_info()`**  \n  初始化配额信息结构，分配红黑树根节点并设置默认配额参数（grace time、最大限制等）\n\n- **`shmem_write_file_info()`**  \n  写入配额文件信息（无操作，因无持久化存储）\n\n- **`shmem_free_file_info()`**  \n  释放红黑树中所有 `quota_id` 条目及根节点，清理内存\n\n- **`shmem_get_next_id()`**  \n  在红黑树中查找大于等于指定 ID 的下一个配额主体 ID（用于配额遍历）\n\n- **`shmem_acquire_dquot()`**  \n  获取或创建指定 ID 的 dquot：\n  - 若红黑树中存在对应 ID，加载其配额限制到 dquot\n  - 若不存在，创建新 `quota_id` 节点，从超级块的默认配额限制初始化，并插入红黑树\n\n- **`shmem_is_empty_dquot()`**  \n  判断 dquot 是否为空（即未使用且配额限制等于默认值），用于决定是否可安全移除\n\n- **`shmem_release_dquot()`**  \n  释放 dquot 时，若其为“假”（fake）或内容为空，则从红黑树中删除对应条目（代码截断，但逻辑完整）\n\n## 3. 关键实现\n\n### 内存配额存储结构\n- 每个配额类型（USRQUOTA/GRPQUOTA）在 `mem_dqinfo->dqi_priv` 中保存一个独立的红黑树（`struct rb_root`）\n- 红黑树按键值 `qid_t id` 排序，支持高效查找、插入和顺序遍历\n\n### 配额限制初始化\n- 新创建的 `quota_id` 条目从 `shmem_sb_info->qlimits` 获取默认硬限制值\n- 软限制初始为 0（表示未设置），硬限制来自挂载选项或默认值\n- 若所有限制均为 0，则标记 dquot 为 `DQ_FAKE_B`（表示无实际配额限制）\n\n### 并发控制\n- 使用 `dqopt->dqio_sem` 读写信号量保护红黑树的并发访问：\n  - 读操作（如 `shmem_get_next_id`）使用 `down_read`\n  - 写操作（如 `shmem_acquire_dquot`、`shmem_release_dquot`）使用 `down_write`\n- 每个 dquot 使用 `dq_lock` 互斥锁保护其状态变更\n\n### 配额生命周期管理\n- **获取**（acquire）：确保 dquot 在内存中存在，必要时创建并初始化\n- **释放**（release）：仅当 dquot 为空（未使用且限制为默认值）时才从红黑树删除，避免信息丢失\n- **活跃性检查**：通过 `dquot_is_busy()` 防止在仍有引用时释放 dquot\n\n## 4. 依赖关系\n\n- **内核配额子系统**：依赖 `<linux/quota.h>` 和 `<linux/quotaops.h>` 提供的通用配额框架（dquot 结构、操作接口等）\n- **tmpfs 文件系统**：通过 `shmem_fs.h` 访问 `shmem_sb_info` 结构获取默认配额限制\n- **内存管理**：使用 `kzalloc/kfree` 分配/释放 `quota_id` 结构\n- **红黑树**：使用 `<linux/rbtree.h>` 实现高效的 ID 查找和排序\n- **配置选项**：仅在 `CONFIG_TMPFS_QUOTA` 启用时编译\n\n## 5. 使用场景\n\n- **tmpfs 配额支持**：当 tmpfs 挂载时启用配额（如 `mount -o usrquota,grpquota tmpfs /tmp`），该模块负责管理用户/组的空间和 inode 使用限制\n- **内存文件系统配额**：可扩展用于其他无持久化存储的内存文件系统\n- **运行时配额查询/修改**：通过 `quotactl` 系统调用查询或更新配额限制时，底层操作由本文件实现\n- **资源隔离**：在容器或沙箱环境中，限制单个用户/组在 tmpfs 中可使用的内存资源",
      "similarity": 0.5385023951530457,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/shmem_quota.c",
          "start_line": 1,
          "end_line": 56,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * In memory quota format relies on quota infrastructure to store dquot",
            " * information for us. While conventional quota formats for file systems",
            " * with persistent storage can load quota information into dquot from the",
            " * storage on-demand and hence quota dquot shrinker can free any dquot",
            " * that is not currently being used, it must be avoided here. Otherwise we",
            " * can lose valuable information, user provided limits, because there is",
            " * no persistent storage to load the information from afterwards.",
            " *",
            " * One information that in-memory quota format needs to keep track of is",
            " * a sorted list of ids for each quota type. This is done by utilizing",
            " * an rb tree which root is stored in mem_dqinfo->dqi_priv for each quota",
            " * type.",
            " *",
            " * This format can be used to support quota on file system without persistent",
            " * storage such as tmpfs.",
            " *",
            " * Author:\tLukas Czerner <lczerner@redhat.com>",
            " *\t\tCarlos Maiolino <cmaiolino@redhat.com>",
            " *",
            " * Copyright (C) 2023 Red Hat, Inc.",
            " */",
            "#include <linux/errno.h>",
            "#include <linux/fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init.h>",
            "#include <linux/module.h>",
            "#include <linux/slab.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/shmem_fs.h>",
            "",
            "#include <linux/quotaops.h>",
            "#include <linux/quota.h>",
            "",
            "#ifdef CONFIG_TMPFS_QUOTA",
            "",
            "/*",
            " * The following constants define the amount of time given a user",
            " * before the soft limits are treated as hard limits (usually resulting",
            " * in an allocation failure). The timer is started when the user crosses",
            " * their soft limit, it is reset when they go below their soft limit.",
            " */",
            "#define SHMEM_MAX_IQ_TIME 604800\t/* (7*24*60*60) 1 week */",
            "#define SHMEM_MAX_DQ_TIME 604800\t/* (7*24*60*60) 1 week */",
            "",
            "struct quota_id {",
            "\tstruct rb_node\tnode;",
            "\tqid_t\t\tid;",
            "\tqsize_t\t\tbhardlimit;",
            "\tqsize_t\t\tbsoftlimit;",
            "\tqsize_t\t\tihardlimit;",
            "\tqsize_t\t\tisoftlimit;",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义用于管理内存配额信息的结构体quota_id，包含RB树节点、用户ID及硬软限制值，用于支持tmpfs等无持久存储文件系统的配额管理。",
          "similarity": 0.5266311168670654
        },
        {
          "chunk_id": 1,
          "file_path": "mm/shmem_quota.c",
          "start_line": 57,
          "end_line": 221,
          "content": [
            "static int shmem_check_quota_file(struct super_block *sb, int type)",
            "{",
            "\t/* There is no real quota file, nothing to do */",
            "\treturn 1;",
            "}",
            "static int shmem_read_file_info(struct super_block *sb, int type)",
            "{",
            "\tstruct quota_info *dqopt = sb_dqopt(sb);",
            "\tstruct mem_dqinfo *info = &dqopt->info[type];",
            "",
            "\tinfo->dqi_priv = kzalloc(sizeof(struct rb_root), GFP_NOFS);",
            "\tif (!info->dqi_priv)",
            "\t\treturn -ENOMEM;",
            "",
            "\tinfo->dqi_max_spc_limit = SHMEM_QUOTA_MAX_SPC_LIMIT;",
            "\tinfo->dqi_max_ino_limit = SHMEM_QUOTA_MAX_INO_LIMIT;",
            "",
            "\tinfo->dqi_bgrace = SHMEM_MAX_DQ_TIME;",
            "\tinfo->dqi_igrace = SHMEM_MAX_IQ_TIME;",
            "\tinfo->dqi_flags = 0;",
            "",
            "\treturn 0;",
            "}",
            "static int shmem_write_file_info(struct super_block *sb, int type)",
            "{",
            "\t/* There is no real quota file, nothing to do */",
            "\treturn 0;",
            "}",
            "static int shmem_free_file_info(struct super_block *sb, int type)",
            "{",
            "\tstruct mem_dqinfo *info = &sb_dqopt(sb)->info[type];",
            "\tstruct rb_root *root = info->dqi_priv;",
            "\tstruct quota_id *entry;",
            "\tstruct rb_node *node;",
            "",
            "\tinfo->dqi_priv = NULL;",
            "\tnode = rb_first(root);",
            "\twhile (node) {",
            "\t\tentry = rb_entry(node, struct quota_id, node);",
            "\t\tnode = rb_next(&entry->node);",
            "",
            "\t\trb_erase(&entry->node, root);",
            "\t\tkfree(entry);",
            "\t}",
            "",
            "\tkfree(root);",
            "\treturn 0;",
            "}",
            "static int shmem_get_next_id(struct super_block *sb, struct kqid *qid)",
            "{",
            "\tstruct mem_dqinfo *info = sb_dqinfo(sb, qid->type);",
            "\tstruct rb_node *node;",
            "\tqid_t id = from_kqid(&init_user_ns, *qid);",
            "\tstruct quota_info *dqopt = sb_dqopt(sb);",
            "\tstruct quota_id *entry = NULL;",
            "\tint ret = 0;",
            "",
            "\tif (!sb_has_quota_active(sb, qid->type))",
            "\t\treturn -ESRCH;",
            "",
            "\tdown_read(&dqopt->dqio_sem);",
            "\tnode = ((struct rb_root *)info->dqi_priv)->rb_node;",
            "\twhile (node) {",
            "\t\tentry = rb_entry(node, struct quota_id, node);",
            "",
            "\t\tif (id < entry->id)",
            "\t\t\tnode = node->rb_left;",
            "\t\telse if (id > entry->id)",
            "\t\t\tnode = node->rb_right;",
            "\t\telse",
            "\t\t\tgoto got_next_id;",
            "\t}",
            "",
            "\tif (!entry) {",
            "\t\tret = -ENOENT;",
            "\t\tgoto out_unlock;",
            "\t}",
            "",
            "\tif (id > entry->id) {",
            "\t\tnode = rb_next(&entry->node);",
            "\t\tif (!node) {",
            "\t\t\tret = -ENOENT;",
            "\t\t\tgoto out_unlock;",
            "\t\t}",
            "\t\tentry = rb_entry(node, struct quota_id, node);",
            "\t}",
            "",
            "got_next_id:",
            "\t*qid = make_kqid(&init_user_ns, qid->type, entry->id);",
            "out_unlock:",
            "\tup_read(&dqopt->dqio_sem);",
            "\treturn ret;",
            "}",
            "static int shmem_acquire_dquot(struct dquot *dquot)",
            "{",
            "\tstruct mem_dqinfo *info = sb_dqinfo(dquot->dq_sb, dquot->dq_id.type);",
            "\tstruct rb_node **n;",
            "\tstruct shmem_sb_info *sbinfo = dquot->dq_sb->s_fs_info;",
            "\tstruct rb_node *parent = NULL, *new_node = NULL;",
            "\tstruct quota_id *new_entry, *entry;",
            "\tqid_t id = from_kqid(&init_user_ns, dquot->dq_id);",
            "\tstruct quota_info *dqopt = sb_dqopt(dquot->dq_sb);",
            "\tint ret = 0;",
            "",
            "\tmutex_lock(&dquot->dq_lock);",
            "",
            "\tdown_write(&dqopt->dqio_sem);",
            "\tn = &((struct rb_root *)info->dqi_priv)->rb_node;",
            "",
            "\twhile (*n) {",
            "\t\tparent = *n;",
            "\t\tentry = rb_entry(parent, struct quota_id, node);",
            "",
            "\t\tif (id < entry->id)",
            "\t\t\tn = &(*n)->rb_left;",
            "\t\telse if (id > entry->id)",
            "\t\t\tn = &(*n)->rb_right;",
            "\t\telse",
            "\t\t\tgoto found;",
            "\t}",
            "",
            "\t/* We don't have entry for this id yet, create it */",
            "\tnew_entry = kzalloc(sizeof(struct quota_id), GFP_NOFS);",
            "\tif (!new_entry) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto out_unlock;",
            "\t}",
            "",
            "\tnew_entry->id = id;",
            "\tif (dquot->dq_id.type == USRQUOTA) {",
            "\t\tnew_entry->bhardlimit = sbinfo->qlimits.usrquota_bhardlimit;",
            "\t\tnew_entry->ihardlimit = sbinfo->qlimits.usrquota_ihardlimit;",
            "\t} else if (dquot->dq_id.type == GRPQUOTA) {",
            "\t\tnew_entry->bhardlimit = sbinfo->qlimits.grpquota_bhardlimit;",
            "\t\tnew_entry->ihardlimit = sbinfo->qlimits.grpquota_ihardlimit;",
            "\t}",
            "",
            "\tnew_node = &new_entry->node;",
            "\trb_link_node(new_node, parent, n);",
            "\trb_insert_color(new_node, (struct rb_root *)info->dqi_priv);",
            "\tentry = new_entry;",
            "",
            "found:",
            "\t/* Load the stored limits from the tree */",
            "\tspin_lock(&dquot->dq_dqb_lock);",
            "\tdquot->dq_dqb.dqb_bhardlimit = entry->bhardlimit;",
            "\tdquot->dq_dqb.dqb_bsoftlimit = entry->bsoftlimit;",
            "\tdquot->dq_dqb.dqb_ihardlimit = entry->ihardlimit;",
            "\tdquot->dq_dqb.dqb_isoftlimit = entry->isoftlimit;",
            "",
            "\tif (!dquot->dq_dqb.dqb_bhardlimit &&",
            "\t    !dquot->dq_dqb.dqb_bsoftlimit &&",
            "\t    !dquot->dq_dqb.dqb_ihardlimit &&",
            "\t    !dquot->dq_dqb.dqb_isoftlimit)",
            "\t\tset_bit(DQ_FAKE_B, &dquot->dq_flags);",
            "\tspin_unlock(&dquot->dq_dqb_lock);",
            "",
            "\t/* Make sure flags update is visible after dquot has been filled */",
            "\tsmp_mb__before_atomic();",
            "\tset_bit(DQ_ACTIVE_B, &dquot->dq_flags);",
            "out_unlock:",
            "\tup_write(&dqopt->dqio_sem);",
            "\tmutex_unlock(&dquot->dq_lock);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "shmem_check_quota_file, shmem_read_file_info, shmem_write_file_info, shmem_free_file_info, shmem_get_next_id, shmem_acquire_dquot",
          "description": "实现内存配额核心操作，包括初始化配额信息、管理RB树节点、获取下一个ID、分配配额项及加载存储限制，通过RB树维护用户ID有序集合。",
          "similarity": 0.44099634885787964
        },
        {
          "chunk_id": 2,
          "file_path": "mm/shmem_quota.c",
          "start_line": 239,
          "end_line": 323,
          "content": [
            "static bool shmem_is_empty_dquot(struct dquot *dquot)",
            "{",
            "\tstruct shmem_sb_info *sbinfo = dquot->dq_sb->s_fs_info;",
            "\tqsize_t bhardlimit;",
            "\tqsize_t ihardlimit;",
            "",
            "\tif (dquot->dq_id.type == USRQUOTA) {",
            "\t\tbhardlimit = sbinfo->qlimits.usrquota_bhardlimit;",
            "\t\tihardlimit = sbinfo->qlimits.usrquota_ihardlimit;",
            "\t} else if (dquot->dq_id.type == GRPQUOTA) {",
            "\t\tbhardlimit = sbinfo->qlimits.grpquota_bhardlimit;",
            "\t\tihardlimit = sbinfo->qlimits.grpquota_ihardlimit;",
            "\t}",
            "",
            "\tif (test_bit(DQ_FAKE_B, &dquot->dq_flags) ||",
            "\t\t(dquot->dq_dqb.dqb_curspace == 0 &&",
            "\t\t dquot->dq_dqb.dqb_curinodes == 0 &&",
            "\t\t dquot->dq_dqb.dqb_bhardlimit == bhardlimit &&",
            "\t\t dquot->dq_dqb.dqb_ihardlimit == ihardlimit))",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "static int shmem_release_dquot(struct dquot *dquot)",
            "{",
            "\tstruct mem_dqinfo *info = sb_dqinfo(dquot->dq_sb, dquot->dq_id.type);",
            "\tstruct rb_node *node;",
            "\tqid_t id = from_kqid(&init_user_ns, dquot->dq_id);",
            "\tstruct quota_info *dqopt = sb_dqopt(dquot->dq_sb);",
            "\tstruct quota_id *entry = NULL;",
            "",
            "\tmutex_lock(&dquot->dq_lock);",
            "\t/* Check whether we are not racing with some other dqget() */",
            "\tif (dquot_is_busy(dquot))",
            "\t\tgoto out_dqlock;",
            "",
            "\tdown_write(&dqopt->dqio_sem);",
            "\tnode = ((struct rb_root *)info->dqi_priv)->rb_node;",
            "\twhile (node) {",
            "\t\tentry = rb_entry(node, struct quota_id, node);",
            "",
            "\t\tif (id < entry->id)",
            "\t\t\tnode = node->rb_left;",
            "\t\telse if (id > entry->id)",
            "\t\t\tnode = node->rb_right;",
            "\t\telse",
            "\t\t\tgoto found;",
            "\t}",
            "",
            "\t/* We should always find the entry in the rb tree */",
            "\tWARN_ONCE(1, \"quota id %u from dquot %p, not in rb tree!\\n\", id, dquot);",
            "\tup_write(&dqopt->dqio_sem);",
            "\tmutex_unlock(&dquot->dq_lock);",
            "\treturn -ENOENT;",
            "",
            "found:",
            "\tif (shmem_is_empty_dquot(dquot)) {",
            "\t\t/* Remove entry from the tree */",
            "\t\trb_erase(&entry->node, info->dqi_priv);",
            "\t\tkfree(entry);",
            "\t} else {",
            "\t\t/* Store the limits in the tree */",
            "\t\tspin_lock(&dquot->dq_dqb_lock);",
            "\t\tentry->bhardlimit = dquot->dq_dqb.dqb_bhardlimit;",
            "\t\tentry->bsoftlimit = dquot->dq_dqb.dqb_bsoftlimit;",
            "\t\tentry->ihardlimit = dquot->dq_dqb.dqb_ihardlimit;",
            "\t\tentry->isoftlimit = dquot->dq_dqb.dqb_isoftlimit;",
            "\t\tspin_unlock(&dquot->dq_dqb_lock);",
            "\t}",
            "",
            "\tclear_bit(DQ_ACTIVE_B, &dquot->dq_flags);",
            "\tup_write(&dqopt->dqio_sem);",
            "",
            "out_dqlock:",
            "\tmutex_unlock(&dquot->dq_lock);",
            "\treturn 0;",
            "}",
            "static int shmem_mark_dquot_dirty(struct dquot *dquot)",
            "{",
            "\treturn 0;",
            "}",
            "static int shmem_dquot_write_info(struct super_block *sb, int type)",
            "{",
            "\treturn 0;",
            "}"
          ],
          "function_name": "shmem_is_empty_dquot, shmem_release_dquot, shmem_mark_dquot_dirty, shmem_dquot_write_info",
          "description": "提供配额项状态检测与释放逻辑，检查配额是否为空以决定是否从RB树中移除节点，在释放时同步更新树中存储的限制值并标记配额状态变化。",
          "similarity": 0.4003450870513916
        }
      ]
    },
    {
      "source_file": "kernel/dma/pool.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:15:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\pool.c`\n\n---\n\n# `dma/pool.c` 技术文档\n\n## 1. 文件概述\n\n`dma/pool.c` 实现了 Linux 内核中的 **DMA 原子内存池（atomic DMA pools）** 机制，用于在无法睡眠的上下文（如中断处理、原子上下文）中分配一致性（coherent）DMA 内存。该机制通过预分配多个按内存区域（ZONE_DMA、ZONE_DMA32、普通内核内存）划分的通用内存池（`gen_pool`），并在池空间不足时通过工作队列异步扩展，从而支持在 GFP_ATOMIC 等限制性分配标志下安全地分配 DMA 内存。\n\n该文件主要用于支持 `dma-direct` 子系统中的原子 DMA 分配路径，确保即使在内存压力大或无法睡眠的场景下，设备驱动仍能获得满足地址限制（如 32 位或 24 位寻址）的一致性 DMA 缓冲区。\n\n## 2. 核心功能\n\n### 全局变量\n- `atomic_pool_dma` / `pool_size_dma`：用于 `GFP_DMA` 区域的原子 DMA 池及其已分配大小。\n- `atomic_pool_dma32` / `pool_size_dma32`：用于 `GFP_DMA32` 区域的原子 DMA 池及其已分配大小。\n- `atomic_pool_kernel` / `pool_size_kernel`：用于普通内核区域（无特殊 DMA 限制）的原子 DMA 池及其已分配大小。\n- `atomic_pool_size`：每个池的初始目标大小，可通过内核命令行参数 `coherent_pool=` 设置。\n- `atomic_pool_work`：用于后台动态扩展内存池的工作项。\n\n### 主要函数\n- `early_coherent_pool()`：解析内核命令行参数 `coherent_pool`，设置 `atomic_pool_size`。\n- `dma_atomic_pool_init()`：初始化所有原子 DMA 池（postcore 阶段调用）。\n- `__dma_atomic_pool_init()`：创建并填充指定 GFP 标志的原子池。\n- `atomic_pool_expand()`：向指定池中添加一块连续物理内存。\n- `atomic_pool_resize()` / `atomic_pool_work_fn()`：检查池剩余空间，若不足则触发扩展。\n- `dma_alloc_from_pool()`：从合适的原子池中分配指定大小的 DMA 内存。\n- `dma_free_from_pool()`：将内存归还到对应的原子池。\n- `dma_guess_pool()`：根据 GFP 标志和尝试顺序选择合适的内存池。\n- `cma_in_zone()`：判断 CMA 区域是否位于指定 DMA 区域内，以决定是否优先从 CMA 分配。\n- `dma_atomic_pool_debugfs_init()`：在 debugfs 中导出各池的当前大小。\n\n## 3. 关键实现\n\n### 内存池初始化策略\n- 若未通过 `coherent_pool=` 指定大小，则默认按 **每 1GB 物理内存分配 128KB** 原子池，最小 128KB，最大不超过 `MAX_ORDER_NR_PAGES` 对应的内存。\n- 每个池使用 `gen_pool` 管理，分配算法为 `gen_pool_first_fit_order_align`，保证分配地址按页对齐。\n- 初始化时调用 `atomic_pool_expand()` 预分配内存。\n\n### 内存分配来源\n- 优先尝试从 **CMA（Contiguous Memory Allocator）** 区域分配（若 CMA 区域位于目标 DMA zone 内）。\n- 若 CMA 不可用或不在目标 zone，则回退到 `alloc_pages()`。\n- 分配的内存块大小不超过 `MAX_PAGE_ORDER`，通过降序尝试（从大到小）提高分配成功率。\n\n### 内存属性处理\n- 调用 `arch_dma_prep_coherent()` 通知架构层准备一致性内存。\n- 在支持内存加密（如 AMD SEV、Intel TDX）的系统上，显式调用 `set_memory_decrypted()` 确保 DMA 内存为 **未加密状态**，因为设备无法访问加密内存。\n- 若启用了 `CONFIG_DMA_DIRECT_REMAP`，则通过 `dma_common_contiguous_remap()` 建立非缓存或设备专用的页表映射。\n\n### 动态扩展机制\n- 每次从池中分配内存后，检查剩余空间是否小于 `atomic_pool_size`。\n- 若不足，则调度 `atomic_pool_work` 工作项，在进程上下文中异步扩展对应池。\n- 扩展时尝试分配与当前池总大小相当的新内存块，避免频繁小量扩展。\n\n### 多池选择逻辑\n- `dma_guess_pool()` 实现池选择策略：\n  1. 首选与 GFP 标志匹配的池（DMA32 > DMA > 普通内核）。\n  2. 若首次分配失败，按 `kernel → dma32 → dma` 顺序尝试其他池（fallback 机制）。\n- 释放时遍历所有池，通过 `gen_pool_has_addr()` 确定内存归属。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - 依赖 `genalloc`（`gen_pool`）实现内存池管理。\n  - 使用 `alloc_pages()`、`__free_pages()` 进行底层页分配。\n  - 依赖 CMA 接口（`dma_alloc_from_contiguous()`）获取大块连续内存。\n- **DMA 子系统**：\n  - 与 `dma-direct.c` 紧密集成，为其提供 `___dma_direct_alloc_pages()` 中的原子分配路径。\n  - 使用 `dma-map-ops.h` 和 `dma-direct.h` 中的辅助函数。\n- **架构相关支持**：\n  - 调用 `arch_dma_prep_coherent()`（架构可选实现）。\n  - 使用 `set_memory_decrypted()`/`set_memory_encrypted()`（x86/ARM64 等支持内存加密的架构）。\n  - 依赖 `DMA_BIT_MASK()` 和 `zone_dma_bits` 判断 DMA 地址范围。\n- **其他**：\n  - 使用 `debugfs` 导出调试信息。\n  - 依赖 `workqueue` 实现异步扩展。\n  - 使用 `slab.h` 中的内存分配器（间接）。\n\n## 5. 使用场景\n\n- **原子上下文 DMA 分配**：当设备驱动在中断处理程序、自旋锁保护区域或使用 `GFP_ATOMIC` 标志调用 `dma_alloc_coherent()` 时，若常规页分配器无法满足（如内存碎片），内核会回退到从原子池分配。\n- **满足地址限制的 DMA 缓冲区**：对于需要 24 位（ISA 设备）或 32 位（旧 PCIe 设备）寻址能力的设备，驱动使用 `DMA_BIT_MASK(24)` 或 `DMA_BIT_MASK(32)` 限制 DMA 地址范围，原子池确保分配的内存物理地址符合要求。\n- **一致性内存需求**：适用于需要 CPU 与设备之间缓存一致性的场景（如网络数据包缓冲区、音频流缓冲区），原子池分配的内存经过 `arch_dma_prep_coherent()` 处理，保证一致性。\n- **内存加密环境**：在启用内存加密的系统中，确保分配给设备的 DMA 内存处于解密状态，使设备能正常访问。",
      "similarity": 0.5243390202522278,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/dma/pool.c",
          "start_line": 145,
          "end_line": 207,
          "content": [
            "static void atomic_pool_resize(struct gen_pool *pool, gfp_t gfp)",
            "{",
            "\tif (pool && gen_pool_avail(pool) < atomic_pool_size)",
            "\t\tatomic_pool_expand(pool, gen_pool_size(pool), gfp);",
            "}",
            "static void atomic_pool_work_fn(struct work_struct *work)",
            "{",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA))",
            "\t\tatomic_pool_resize(atomic_pool_dma,",
            "\t\t\t\t   GFP_KERNEL | GFP_DMA);",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA32))",
            "\t\tatomic_pool_resize(atomic_pool_dma32,",
            "\t\t\t\t   GFP_KERNEL | GFP_DMA32);",
            "\tatomic_pool_resize(atomic_pool_kernel, GFP_KERNEL);",
            "}",
            "static int __init dma_atomic_pool_init(void)",
            "{",
            "\tint ret = 0;",
            "",
            "\t/*",
            "\t * If coherent_pool was not used on the command line, default the pool",
            "\t * sizes to 128KB per 1GB of memory, min 128KB, max MAX_PAGE_ORDER.",
            "\t */",
            "\tif (!atomic_pool_size) {",
            "\t\tunsigned long pages = totalram_pages() / (SZ_1G / SZ_128K);",
            "\t\tpages = min_t(unsigned long, pages, MAX_ORDER_NR_PAGES);",
            "\t\tatomic_pool_size = max_t(size_t, pages << PAGE_SHIFT, SZ_128K);",
            "\t}",
            "\tINIT_WORK(&atomic_pool_work, atomic_pool_work_fn);",
            "",
            "\tatomic_pool_kernel = __dma_atomic_pool_init(atomic_pool_size,",
            "\t\t\t\t\t\t    GFP_KERNEL);",
            "\tif (!atomic_pool_kernel)",
            "\t\tret = -ENOMEM;",
            "\tif (has_managed_dma()) {",
            "\t\tatomic_pool_dma = __dma_atomic_pool_init(atomic_pool_size,",
            "\t\t\t\t\t\tGFP_KERNEL | GFP_DMA);",
            "\t\tif (!atomic_pool_dma)",
            "\t\t\tret = -ENOMEM;",
            "\t}",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA32)) {",
            "\t\tatomic_pool_dma32 = __dma_atomic_pool_init(atomic_pool_size,",
            "\t\t\t\t\t\tGFP_KERNEL | GFP_DMA32);",
            "\t\tif (!atomic_pool_dma32)",
            "\t\t\tret = -ENOMEM;",
            "\t}",
            "",
            "\tdma_atomic_pool_debugfs_init();",
            "\treturn ret;",
            "}",
            "bool dma_free_from_pool(struct device *dev, void *start, size_t size)",
            "{",
            "\tstruct gen_pool *pool = NULL;",
            "",
            "\twhile ((pool = dma_guess_pool(pool, 0))) {",
            "\t\tif (!gen_pool_has_addr(pool, (unsigned long)start, size))",
            "\t\t\tcontinue;",
            "\t\tgen_pool_free(pool, (unsigned long)start, size);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "atomic_pool_resize, atomic_pool_work_fn, dma_atomic_pool_init, dma_free_from_pool",
          "description": "实现内存池的初始化与维护机制，包含池大小自动调节逻辑、后台扩展任务调度、默认尺寸计算及内存释放查找功能，提供设备内存池的统一管理接口。",
          "similarity": 0.4861830174922943
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/pool.c",
          "start_line": 29,
          "end_line": 138,
          "content": [
            "static int __init early_coherent_pool(char *p)",
            "{",
            "\tatomic_pool_size = memparse(p, &p);",
            "\treturn 0;",
            "}",
            "static void __init dma_atomic_pool_debugfs_init(void)",
            "{",
            "\tstruct dentry *root;",
            "",
            "\troot = debugfs_create_dir(\"dma_pools\", NULL);",
            "\tdebugfs_create_ulong(\"pool_size_dma\", 0400, root, &pool_size_dma);",
            "\tdebugfs_create_ulong(\"pool_size_dma32\", 0400, root, &pool_size_dma32);",
            "\tdebugfs_create_ulong(\"pool_size_kernel\", 0400, root, &pool_size_kernel);",
            "}",
            "static void dma_atomic_pool_size_add(gfp_t gfp, size_t size)",
            "{",
            "\tif (gfp & __GFP_DMA)",
            "\t\tpool_size_dma += size;",
            "\telse if (gfp & __GFP_DMA32)",
            "\t\tpool_size_dma32 += size;",
            "\telse",
            "\t\tpool_size_kernel += size;",
            "}",
            "static bool cma_in_zone(gfp_t gfp)",
            "{",
            "\tunsigned long size;",
            "\tphys_addr_t end;",
            "\tstruct cma *cma;",
            "",
            "\tcma = dev_get_cma_area(NULL);",
            "\tif (!cma)",
            "\t\treturn false;",
            "",
            "\tsize = cma_get_size(cma);",
            "\tif (!size)",
            "\t\treturn false;",
            "",
            "\t/* CMA can't cross zone boundaries, see cma_activate_area() */",
            "\tend = cma_get_base(cma) + size - 1;",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA) && (gfp & GFP_DMA))",
            "\t\treturn end <= DMA_BIT_MASK(zone_dma_bits);",
            "\tif (IS_ENABLED(CONFIG_ZONE_DMA32) && (gfp & GFP_DMA32))",
            "\t\treturn end <= DMA_BIT_MASK(32);",
            "\treturn true;",
            "}",
            "static int atomic_pool_expand(struct gen_pool *pool, size_t pool_size,",
            "\t\t\t      gfp_t gfp)",
            "{",
            "\tunsigned int order;",
            "\tstruct page *page = NULL;",
            "\tvoid *addr;",
            "\tint ret = -ENOMEM;",
            "",
            "\t/* Cannot allocate larger than MAX_PAGE_ORDER */",
            "\torder = min(get_order(pool_size), MAX_PAGE_ORDER);",
            "",
            "\tdo {",
            "\t\tpool_size = 1 << (PAGE_SHIFT + order);",
            "\t\tif (cma_in_zone(gfp))",
            "\t\t\tpage = dma_alloc_from_contiguous(NULL, 1 << order,",
            "\t\t\t\t\t\t\t order, false);",
            "\t\tif (!page)",
            "\t\t\tpage = alloc_pages(gfp, order);",
            "\t} while (!page && order-- > 0);",
            "\tif (!page)",
            "\t\tgoto out;",
            "",
            "\tarch_dma_prep_coherent(page, pool_size);",
            "",
            "#ifdef CONFIG_DMA_DIRECT_REMAP",
            "\taddr = dma_common_contiguous_remap(page, pool_size,",
            "\t\t\tpgprot_decrypted(pgprot_dmacoherent(PAGE_KERNEL)),",
            "\t\t\t__builtin_return_address(0));",
            "\tif (!addr)",
            "\t\tgoto free_page;",
            "#else",
            "\taddr = page_to_virt(page);",
            "#endif",
            "\t/*",
            "\t * Memory in the atomic DMA pools must be unencrypted, the pools do not",
            "\t * shrink so no re-encryption occurs in dma_direct_free().",
            "\t */",
            "\tret = set_memory_decrypted((unsigned long)page_to_virt(page),",
            "\t\t\t\t   1 << order);",
            "\tif (ret)",
            "\t\tgoto remove_mapping;",
            "\tret = gen_pool_add_virt(pool, (unsigned long)addr, page_to_phys(page),",
            "\t\t\t\tpool_size, NUMA_NO_NODE);",
            "\tif (ret)",
            "\t\tgoto encrypt_mapping;",
            "",
            "\tdma_atomic_pool_size_add(gfp, pool_size);",
            "\treturn 0;",
            "",
            "encrypt_mapping:",
            "\tret = set_memory_encrypted((unsigned long)page_to_virt(page),",
            "\t\t\t\t   1 << order);",
            "\tif (WARN_ON_ONCE(ret)) {",
            "\t\t/* Decrypt succeeded but encrypt failed, purposely leak */",
            "\t\tgoto out;",
            "\t}",
            "remove_mapping:",
            "#ifdef CONFIG_DMA_DIRECT_REMAP",
            "\tdma_common_free_remap(addr, pool_size);",
            "free_page:",
            "\t__free_pages(page, order);",
            "#endif",
            "out:",
            "\treturn ret;",
            "}"
          ],
          "function_name": "early_coherent_pool, dma_atomic_pool_debugfs_init, dma_atomic_pool_size_add, cma_in_zone, atomic_pool_expand",
          "description": "实现DMA内存池的动态扩展逻辑，包含解析命令行参数、调试接口注册、内存分配策略选择、CMA区域有效性检测及池扩容操作，支持加密/解密内存映射管理。",
          "similarity": 0.4596185088157654
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/pool.c",
          "start_line": 1,
          "end_line": 28,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 2012 ARM Ltd.",
            " * Copyright (C) 2020 Google LLC",
            " */",
            "#include <linux/cma.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/dma-map-ops.h>",
            "#include <linux/dma-direct.h>",
            "#include <linux/init.h>",
            "#include <linux/genalloc.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/slab.h>",
            "#include <linux/workqueue.h>",
            "",
            "static struct gen_pool *atomic_pool_dma __ro_after_init;",
            "static unsigned long pool_size_dma;",
            "static struct gen_pool *atomic_pool_dma32 __ro_after_init;",
            "static unsigned long pool_size_dma32;",
            "static struct gen_pool *atomic_pool_kernel __ro_after_init;",
            "static unsigned long pool_size_kernel;",
            "",
            "/* Size can be defined by the coherent_pool command line */",
            "static size_t atomic_pool_size;",
            "",
            "/* Dynamic background expansion when the atomic pool is near capacity */",
            "static struct work_struct atomic_pool_work;",
            ""
          ],
          "function_name": null,
          "description": "定义并初始化用于管理DMA内存池的全局变量，包括针对不同架构（DMA/DMA32/内核）的通用池指针、尺寸参数及动态扩展的工作队列。",
          "similarity": 0.4076220989227295
        }
      ]
    },
    {
      "source_file": "mm/dmapool.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:56:43\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dmapool.c`\n\n---\n\n# dmapool.c 技术文档\n\n## 1. 文件概述\n\n`dmapool.c` 实现了 Linux 内核中的 **DMA 池（DMA Pool）分配器**，用于为设备驱动程序提供小块、一致（coherent）且可 DMA 访问的内存。该分配器基于 `dma_alloc_coherent()` 分配整页内存，并将其划分为固定大小的块，以满足频繁分配/释放小块 DMA 内存的需求，避免直接使用页级分配造成的内存浪费。此机制特别适用于需要大量相同大小 DMA 缓冲区的设备驱动（如 USB、网络、存储控制器等）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct dma_pool`**  \n  表示一个 DMA 池，包含：\n  - `page_list`：已分配物理页的双向链表\n  - `lock`：自旋锁，保护池内操作\n  - `next_block`：空闲块的单向链表头指针\n  - `nr_blocks` / `nr_active` / `nr_pages`：统计信息（总块数、活跃块数、页数）\n  - `dev`：关联的设备\n  - `size` / `allocation` / `boundary`：块大小、页分配大小、边界对齐限制\n  - `name`：池名称（用于调试）\n  - `pools`：挂载到设备 `dma_pools` 列表的节点\n\n- **`struct dma_page`**  \n  表示从 `dma_alloc_coherent()` 分配的一个物理页，包含虚拟地址 `vaddr` 和 DMA 地址 `dma`。\n\n- **`struct dma_block`**  \n  嵌入在每个 DMA 块起始位置的元数据结构，仅包含指向下一个空闲块的指针 `next_block` 和该块的 DMA 地址 `dma`。\n\n### 主要函数\n\n- **`dma_pool_create()`**  \n  创建一个新的 DMA 池，指定名称、设备、块大小、对齐要求和边界限制。\n\n- **`pool_block_pop()` / `pool_block_push()`**  \n  从空闲链表中分配/归还一个 DMA 块。\n\n- **`pool_check_block()` / `pool_block_err()` / `pool_init_page()`**  \n  调试辅助函数（在 `DMAPOOL_DEBUG` 启用时），用于检测内存越界、重复释放等错误，并进行内存毒化（poisoning）。\n\n- **`pools_show()`**  \n  sysfs 接口回调，显示设备下所有 DMA 池的统计信息。\n\n## 3. 关键实现\n\n- **内存组织**：  \n  每次调用 `dma_alloc_coherent()` 分配至少一页（`PAGE_SIZE`）的连续物理内存（`allocation` 字段）。该页被划分为多个 `size` 字节的块。每个块的起始处嵌入 `struct dma_block` 元数据。\n\n- **空闲管理**：  \n  所有空闲块通过 `next_block` 指针组成一个**全局单向链表**（由 `dma_pool.next_block` 指向头节点）。分配时从链表头部弹出，释放时压入头部。**已分配块不被显式跟踪**，仅通过 `nr_active` 计数。\n\n- **边界对齐处理**：  \n  若指定了 `boundary`（如 4KB），则确保单个 DMA 块不会跨越该边界。实现上通过限制每页实际可用区域或调整分配策略（代码片段未完整展示具体划分逻辑）。\n\n- **调试支持（DMAPOOL_DEBUG）**：  \n  在 SLUB 调试开启时启用：\n  - 分配时用 `POOL_POISON_ALLOCATED` 填充用户区域（若未启用 init-on-alloc）\n  - 释放时用 `POOL_POISON_FREED` 填充，并检查是否已被释放（防 double-free）\n  - 提供 `pool_find_page()` 辅助验证 DMA 地址有效性\n\n- **Sysfs 集成**：  \n  首次为设备创建 DMA 池时，自动注册 `pools` sysfs 属性文件，可通过 `/sys/devices/.../pools` 查看池状态（名称、活跃块数、总块数、块大小、页数）。\n\n- **并发控制**：  \n  - `pools_lock`：保护设备 `dma_pools` 列表的增删\n  - `pools_reg_lock`：防止 `dma_pool_create()` 与 `dma_pool_destroy()` 之间的竞争\n  - `dma_pool.lock`：保护池内部的空闲链表和计数器\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/dma-mapping.h>`：提供 `dma_alloc_coherent()` / `dma_free_coherent()` 等底层 DMA 映射接口\n  - `<linux/device.h>`：设备模型及 sysfs 支持\n  - `<linux/slab.h>`：用于分配 `struct dma_pool` 结构体内存\n\n- **调试依赖**：\n  - `CONFIG_SLUB_DEBUG_ON`：启用内存毒化和错误检查\n  - `<linux/poison.h>`：提供 `POOL_POISON_*` 常量\n\n- **同步原语**：\n  - `<linux/mutex.h>` / `<linux/spinlock.h>`：提供互斥锁和自旋锁\n\n## 5. 使用场景\n\n- **设备驱动开发**：  \n  当驱动需要频繁分配/释放**固定大小**的小块（通常小于一页）DMA 缓冲区时，使用 DMA 池可显著提升性能并减少内存碎片。典型场景包括：\n  - USB 主机控制器的传输描述符（TD）池\n  - 网络设备的接收/发送描述符环\n  - 存储控制器的命令/状态块\n\n- **替代方案**：  \n  相比直接调用 `dma_alloc_coherent()` 分配整页内存，DMA 池避免了小块分配的内存浪费；相比通用 slab 分配器（如 kmalloc），它保证了返回内存的 DMA 一致性（无需手动缓存维护）。\n\n- **限制条件**：  \n  - 仅适用于**一致性 DMA 映射**（coherent DMA）\n  - 所有块大小在池创建时固定\n  - 不适用于大块（接近或超过一页）内存分配",
      "similarity": 0.5202978253364563,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/dmapool.c",
          "start_line": 360,
          "end_line": 419,
          "content": [
            "void dma_pool_destroy(struct dma_pool *pool)",
            "{",
            "\tstruct dma_page *page, *tmp;",
            "\tbool empty, busy = false;",
            "",
            "\tif (unlikely(!pool))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&pools_reg_lock);",
            "\tmutex_lock(&pools_lock);",
            "\tlist_del(&pool->pools);",
            "\tempty = list_empty(&pool->dev->dma_pools);",
            "\tmutex_unlock(&pools_lock);",
            "\tif (empty)",
            "\t\tdevice_remove_file(pool->dev, &dev_attr_pools);",
            "\tmutex_unlock(&pools_reg_lock);",
            "",
            "\tif (pool->nr_active) {",
            "\t\tdev_err(pool->dev, \"%s %s busy\\n\", __func__, pool->name);",
            "\t\tbusy = true;",
            "\t}",
            "",
            "\tlist_for_each_entry_safe(page, tmp, &pool->page_list, page_list) {",
            "\t\tif (!busy)",
            "\t\t\tdma_free_coherent(pool->dev, pool->allocation,",
            "\t\t\t\t\t  page->vaddr, page->dma);",
            "\t\tlist_del(&page->page_list);",
            "\t\tkfree(page);",
            "\t}",
            "",
            "\tkfree(pool);",
            "}",
            "void dma_pool_free(struct dma_pool *pool, void *vaddr, dma_addr_t dma)",
            "{",
            "\tstruct dma_block *block = vaddr;",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&pool->lock, flags);",
            "\tif (!pool_block_err(pool, vaddr, dma)) {",
            "\t\tpool_block_push(pool, block, dma);",
            "\t\tpool->nr_active--;",
            "\t}",
            "\tspin_unlock_irqrestore(&pool->lock, flags);",
            "}",
            "static void dmam_pool_release(struct device *dev, void *res)",
            "{",
            "\tstruct dma_pool *pool = *(struct dma_pool **)res;",
            "",
            "\tdma_pool_destroy(pool);",
            "}",
            "static int dmam_pool_match(struct device *dev, void *res, void *match_data)",
            "{",
            "\treturn *(struct dma_pool **)res == match_data;",
            "}",
            "void dmam_pool_destroy(struct dma_pool *pool)",
            "{",
            "\tstruct device *dev = pool->dev;",
            "",
            "\tWARN_ON(devres_release(dev, dmam_pool_release, dmam_pool_match, pool));",
            "}"
          ],
          "function_name": "dma_pool_destroy, dma_pool_free, dmam_pool_release, dmam_pool_match, dmam_pool_destroy",
          "description": "实现DMA池销毁逻辑，包括资源回收、活跃块计数管理及页面内存释放，通过互斥锁保证线程安全，支持设备资源释放回调。",
          "similarity": 0.4582858979701996
        },
        {
          "chunk_id": 0,
          "file_path": "mm/dmapool.c",
          "start_line": 1,
          "end_line": 71,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * DMA Pool allocator",
            " *",
            " * Copyright 2001 David Brownell",
            " * Copyright 2007 Intel Corporation",
            " *   Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This allocator returns small blocks of a given size which are DMA-able by",
            " * the given device.  It uses the dma_alloc_coherent page allocator to get",
            " * new pages, then splits them up into blocks of the required size.",
            " * Many older drivers still have their own code to do this.",
            " *",
            " * The current design of this allocator is fairly simple.  The pool is",
            " * represented by the 'struct dma_pool' which keeps a doubly-linked list of",
            " * allocated pages.  Each page in the page_list is split into blocks of at",
            " * least 'size' bytes.  Free blocks are tracked in an unsorted singly-linked",
            " * list of free blocks across all pages.  Used blocks aren't tracked, but we",
            " * keep a count of how many are currently allocated from each page.",
            " */",
            "",
            "#include <linux/device.h>",
            "#include <linux/dma-mapping.h>",
            "#include <linux/dmapool.h>",
            "#include <linux/kernel.h>",
            "#include <linux/list.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/poison.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/slab.h>",
            "#include <linux/stat.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/string.h>",
            "#include <linux/types.h>",
            "#include <linux/wait.h>",
            "",
            "#ifdef CONFIG_SLUB_DEBUG_ON",
            "#define DMAPOOL_DEBUG 1",
            "#endif",
            "",
            "struct dma_block {",
            "\tstruct dma_block *next_block;",
            "\tdma_addr_t dma;",
            "};",
            "",
            "struct dma_pool {\t\t/* the pool */",
            "\tstruct list_head page_list;",
            "\tspinlock_t lock;",
            "\tstruct dma_block *next_block;",
            "\tsize_t nr_blocks;",
            "\tsize_t nr_active;",
            "\tsize_t nr_pages;",
            "\tstruct device *dev;",
            "\tunsigned int size;",
            "\tunsigned int allocation;",
            "\tunsigned int boundary;",
            "\tchar name[32];",
            "\tstruct list_head pools;",
            "};",
            "",
            "struct dma_page {\t\t/* cacheable header for 'allocation' bytes */",
            "\tstruct list_head page_list;",
            "\tvoid *vaddr;",
            "\tdma_addr_t dma;",
            "};",
            "",
            "static DEFINE_MUTEX(pools_lock);",
            "static DEFINE_MUTEX(pools_reg_lock);",
            ""
          ],
          "function_name": null,
          "description": "定义DMA池相关结构体及全局锁，用于管理可DMA访问的小块内存分配，包含页面链表、块链表、设备指针及参数配置。",
          "similarity": 0.44066837430000305
        },
        {
          "chunk_id": 1,
          "file_path": "mm/dmapool.c",
          "start_line": 72,
          "end_line": 196,
          "content": [
            "static ssize_t pools_show(struct device *dev, struct device_attribute *attr, char *buf)",
            "{",
            "\tstruct dma_pool *pool;",
            "\tunsigned size;",
            "",
            "\tsize = sysfs_emit(buf, \"poolinfo - 0.1\\n\");",
            "",
            "\tmutex_lock(&pools_lock);",
            "\tlist_for_each_entry(pool, &dev->dma_pools, pools) {",
            "\t\t/* per-pool info, no real statistics yet */",
            "\t\tsize += sysfs_emit_at(buf, size, \"%-16s %4zu %4zu %4u %2zu\\n\",",
            "\t\t\t\t      pool->name, pool->nr_active,",
            "\t\t\t\t      pool->nr_blocks, pool->size,",
            "\t\t\t\t      pool->nr_pages);",
            "\t}",
            "\tmutex_unlock(&pools_lock);",
            "",
            "\treturn size;",
            "}",
            "static void pool_check_block(struct dma_pool *pool, struct dma_block *block,",
            "\t\t\t     gfp_t mem_flags)",
            "{",
            "\tu8 *data = (void *)block;",
            "\tint i;",
            "",
            "\tfor (i = sizeof(struct dma_block); i < pool->size; i++) {",
            "\t\tif (data[i] == POOL_POISON_FREED)",
            "\t\t\tcontinue;",
            "\t\tdev_err(pool->dev, \"%s %s, %p (corrupted)\\n\", __func__,",
            "\t\t\tpool->name, block);",
            "",
            "\t\t/*",
            "\t\t * Dump the first 4 bytes even if they are not",
            "\t\t * POOL_POISON_FREED",
            "\t\t */",
            "\t\tprint_hex_dump(KERN_ERR, \"\", DUMP_PREFIX_OFFSET, 16, 1,",
            "\t\t\t\tdata, pool->size, 1);",
            "\t\tbreak;",
            "\t}",
            "",
            "\tif (!want_init_on_alloc(mem_flags))",
            "\t\tmemset(block, POOL_POISON_ALLOCATED, pool->size);",
            "}",
            "static bool pool_block_err(struct dma_pool *pool, void *vaddr, dma_addr_t dma)",
            "{",
            "\tstruct dma_block *block = pool->next_block;",
            "\tstruct dma_page *page;",
            "",
            "\tpage = pool_find_page(pool, dma);",
            "\tif (!page) {",
            "\t\tdev_err(pool->dev, \"%s %s, %p/%pad (bad dma)\\n\",",
            "\t\t\t__func__, pool->name, vaddr, &dma);",
            "\t\treturn true;",
            "\t}",
            "",
            "\twhile (block) {",
            "\t\tif (block != vaddr) {",
            "\t\t\tblock = block->next_block;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tdev_err(pool->dev, \"%s %s, dma %pad already free\\n\",",
            "\t\t\t__func__, pool->name, &dma);",
            "\t\treturn true;",
            "\t}",
            "",
            "\tmemset(vaddr, POOL_POISON_FREED, pool->size);",
            "\treturn false;",
            "}",
            "static void pool_init_page(struct dma_pool *pool, struct dma_page *page)",
            "{",
            "\tmemset(page->vaddr, POOL_POISON_FREED, pool->allocation);",
            "}",
            "static void pool_check_block(struct dma_pool *pool, struct dma_block *block,",
            "\t\t\t     gfp_t mem_flags)",
            "{",
            "}",
            "static bool pool_block_err(struct dma_pool *pool, void *vaddr, dma_addr_t dma)",
            "{",
            "\tif (want_init_on_free())",
            "\t\tmemset(vaddr, 0, pool->size);",
            "\treturn false;",
            "}",
            "static void pool_init_page(struct dma_pool *pool, struct dma_page *page)",
            "{",
            "}",
            "static void pool_block_push(struct dma_pool *pool, struct dma_block *block,",
            "\t\t\t    dma_addr_t dma)",
            "{",
            "\tblock->dma = dma;",
            "\tblock->next_block = pool->next_block;",
            "\tpool->next_block = block;",
            "}",
            "static void pool_initialise_page(struct dma_pool *pool, struct dma_page *page)",
            "{",
            "\tunsigned int next_boundary = pool->boundary, offset = 0;",
            "\tstruct dma_block *block, *first = NULL, *last = NULL;",
            "",
            "\tpool_init_page(pool, page);",
            "\twhile (offset + pool->size <= pool->allocation) {",
            "\t\tif (offset + pool->size > next_boundary) {",
            "\t\t\toffset = next_boundary;",
            "\t\t\tnext_boundary += pool->boundary;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tblock = page->vaddr + offset;",
            "\t\tblock->dma = page->dma + offset;",
            "\t\tblock->next_block = NULL;",
            "",
            "\t\tif (last)",
            "\t\t\tlast->next_block = block;",
            "\t\telse",
            "\t\t\tfirst = block;",
            "\t\tlast = block;",
            "",
            "\t\toffset += pool->size;",
            "\t\tpool->nr_blocks++;",
            "\t}",
            "",
            "\tlast->next_block = pool->next_block;",
            "\tpool->next_block = first;",
            "",
            "\tlist_add(&page->page_list, &pool->page_list);",
            "\tpool->nr_pages++;",
            "}"
          ],
          "function_name": "pools_show, pool_check_block, pool_block_err, pool_init_page, pool_check_block, pool_block_err, pool_init_page, pool_block_push, pool_initialise_page",
          "description": "提供DMA池调试显示、块校验、错误检测及页面初始化等功能，部分函数存在重复声明或未完成实现，上下文不完整。",
          "similarity": 0.4217042028903961
        }
      ]
    }
  ]
}