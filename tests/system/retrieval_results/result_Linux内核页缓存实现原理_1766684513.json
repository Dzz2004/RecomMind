{
  "query": "Linux内核页缓存实现原理",
  "timestamp": "2025-12-26 01:41:53",
  "retrieved_files": [
    {
      "source_file": "mm/memory.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:42:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memory.c`\n\n---\n\n# memory.c 技术文档\n\n## 1. 文件概述\n\n`memory.c` 是 Linux 内核内存管理子系统（MM）的核心实现文件之一，位于 `mm/` 目录下。该文件主要负责虚拟内存到物理内存的映射管理、缺页异常（page fault）处理、页表结构的分配与释放、以及与用户空间内存操作相关的底层机制。它实现了按需加载（demand-loading）、共享页面、交换（swapping）等关键虚拟内存功能，并为多级页表架构（如 x86-64 的四级页表）提供通用支持。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `high_memory`：指向直接映射区域（ZONE_NORMAL）的上界，用于区分低端内存和高端内存。\n- `randomize_va_space`：控制地址空间布局随机化（ASLR）策略的级别（0=关闭，1=部分启用，2=完全启用）。\n- `zero_pfn`：指向全零物理页的页帧号（PFN），用于高效实现只读零页映射。\n- `max_mapnr` 和 `mem_map`（非 NUMA 配置下）：分别表示最大页帧号和全局页描述符数组。\n- `highest_memmap_pfn`：记录系统中最高的已注册页帧号。\n\n### 关键函数\n- `free_pgd_range()`：释放指定虚拟地址范围内的用户级页表结构（从 PGD 到 PTE）。\n- `free_p4d_range()`, `free_pud_range()`, `free_pmd_range()`, `free_pte_range()`：递归释放各级页表项及其对应的页表页。\n- `do_fault()`：处理基于文件映射的缺页异常。\n- `do_anonymous_page()`：处理匿名映射（如堆、栈）的缺页异常。\n- `vmf_orig_pte_uffd_wp()`：判断原始 PTE 是否为 userfaultfd 写保护标记。\n- `init_zero_pfn()`：早期初始化 `zero_pfn`。\n- `mm_trace_rss_stat()`：触发 RSS（Resident Set Size）统计的跟踪事件。\n\n### 内联辅助函数\n- `arch_wants_old_prefaulted_pte()`：允许架构层决定预取页表项是否应标记为“old”以优化访问位更新开销。\n\n## 3. 关键实现\n\n### 页表释放机制\n- 采用自顶向下（PGD → P4D → PUD → PMD → PTE）的递归方式释放页表。\n- 每级释放函数（如 `free_pmd_range`）遍历地址范围内的页表项：\n  - 跳过空或无效项（`pmd_none_or_clear_bad`）。\n  - 递归释放下一级页表。\n  - 在满足对齐和边界条件（`floor`/`ceiling`）时，释放当前级页表页并更新 MMU gather 结构中的计数器（如 `mm_dec_nr_ptes`）。\n- 使用 `mmu_gather` 机制批量延迟 TLB 刷新和页表页释放，提升性能。\n\n### 缺页处理框架\n- 提供 `do_fault` 和 `do_anonymous_page` 作为缺页处理的核心入口，分别处理文件映射和匿名映射。\n- 支持 `userfaultfd` 写保护机制，通过 `vmf_orig_pte_uffd_wp` 检测特殊 PTE 标记。\n\n### 地址空间随机化（ASLR）\n- 通过 `randomize_va_space` 控制栈、mmap 区域、brk 等的随机化行为。\n- 支持内核启动参数 `norandmaps` 完全禁用 ASLR。\n- 兼容旧版 libc5 二进制（`CONFIG_COMPAT_BRK`），此时 brk 区域不参与随机化。\n\n### 零页优化\n- `zero_pfn` 指向一个全局只读的全零物理页，用于高效实现对未初始化数据段（如 `.bss`）或显式映射 `/dev/zero` 的只读访问，避免每次分配新页。\n\n### 架构适配\n- 通过 `arch_wants_old_prefaulted_pte` 允许特定架构优化页表项的“young/old”状态设置。\n- 依赖 `asm/mmu_context.h`、`asm/pgalloc.h`、`asm/tlb.h` 等架构相关头文件实现底层操作。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- **内存管理核心**：`<linux/mm.h>`, `<linux/mman.h>`, `<linux/swap.h>`, `<linux/pagemap.h>`, `<linux/memcontrol.h>`\n- **进程与调度**：`<linux/sched/mm.h>`, `<linux/sched/task.h>`, `<linux/delayacct.h>`\n- **NUMA 与迁移**：`<linux/numa.h>`, `<linux/migrate.h>`, `<linux/sched/numa_balancing.h>`\n- **特殊内存类型**：`<linux/hugetlb.h>`, `<linux/highmem.h>`, `<linux/dax.h>`, `<linux/zswap.h>`\n- **调试与跟踪**：`<trace/events/kmem.h>`, `<linux/debugfs.h>`, `<linux/oom.h>`\n- **架构相关**：`<asm/io.h>`, `<asm/mmu_context.h>`, `<asm/pgalloc.h>`, `<asm/tlbflush.h>`\n\n### 内部模块依赖\n- `internal.h`：包含 MM 子系统内部通用定义。\n- `swap.h`：交换子系统接口。\n- `pgalloc-track.h`：页表分配跟踪（用于调试）。\n\n## 5. 使用场景\n\n- **进程创建与退出**：在 `fork()` 和进程终止时，通过 `free_pgd_range` 释放整个地址空间的页表。\n- **内存映射操作**：`mmap()`、`munmap()`、`mremap()` 等系统调用触发页表的建立或释放。\n- **缺页异常处理**：当 CPU 访问未映射或换出的虚拟地址时，由体系结构相关的缺页处理程序调用 `do_fault` 或 `do_anonymous_page`。\n- **内存回收**：在内存压力下，kswapd 或直接回收路径可能触发页表清理。\n- **用户态内存监控**：`userfaultfd` 机制利用 `vmf_orig_pte_uffd_wp` 实现用户空间对缺页事件的精细控制。\n- **内核初始化**：早期调用 `init_zero_pfn` 设置零页，`paging_init()`（架构相关）初始化 `high_memory` 和 `ZERO_PAGE`。",
      "similarity": 0.6876916885375977,
      "chunks": [
        {
          "chunk_id": 25,
          "file_path": "mm/memory.c",
          "start_line": 4037,
          "end_line": 4496,
          "content": [
            "static inline unsigned long thp_swap_suitable_orders(pgoff_t swp_offset,",
            "\t\t\t\t\t\t     unsigned long addr,",
            "\t\t\t\t\t\t     unsigned long orders)",
            "{",
            "\tint order, nr;",
            "",
            "\torder = highest_order(orders);",
            "",
            "\t/*",
            "\t * To swap in a THP with nr pages, we require that its first swap_offset",
            "\t * is aligned with that number, as it was when the THP was swapped out.",
            "\t * This helps filter out most invalid entries.",
            "\t */",
            "\twhile (orders) {",
            "\t\tnr = 1 << order;",
            "\t\tif ((addr >> PAGE_SHIFT) % nr == swp_offset % nr)",
            "\t\t\tbreak;",
            "\t\torder = next_order(&orders, order);",
            "\t}",
            "",
            "\treturn orders;",
            "}",
            "vm_fault_t do_swap_page(struct vm_fault *vmf)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tstruct folio *swapcache, *folio = NULL;",
            "\tDECLARE_WAITQUEUE(wait, current);",
            "\tstruct page *page;",
            "\tstruct swap_info_struct *si = NULL;",
            "\trmap_t rmap_flags = RMAP_NONE;",
            "\tbool need_clear_cache = false;",
            "\tbool exclusive = false;",
            "\tswp_entry_t entry;",
            "\tpte_t pte;",
            "\tvm_fault_t ret = 0;",
            "\tvoid *shadow = NULL;",
            "\tint nr_pages;",
            "\tunsigned long page_idx;",
            "\tunsigned long address;",
            "\tpte_t *ptep;",
            "",
            "\tif (!pte_unmap_same(vmf))",
            "\t\tgoto out;",
            "",
            "\tentry = pte_to_swp_entry(vmf->orig_pte);",
            "\tif (unlikely(non_swap_entry(entry))) {",
            "\t\tif (is_migration_entry(entry)) {",
            "\t\t\tmigration_entry_wait(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\t     vmf->address);",
            "\t\t} else if (is_device_exclusive_entry(entry)) {",
            "\t\t\tvmf->page = pfn_swap_entry_to_page(entry);",
            "\t\t\tret = remove_device_exclusive_entry(vmf);",
            "\t\t} else if (is_device_private_entry(entry)) {",
            "\t\t\tif (vmf->flags & FAULT_FLAG_VMA_LOCK) {",
            "\t\t\t\t/*",
            "\t\t\t\t * migrate_to_ram is not yet ready to operate",
            "\t\t\t\t * under VMA lock.",
            "\t\t\t\t */",
            "\t\t\t\tvma_end_read(vma);",
            "\t\t\t\tret = VM_FAULT_RETRY;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "",
            "\t\t\tvmf->page = pfn_swap_entry_to_page(entry);",
            "\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\tvmf->address, &vmf->ptl);",
            "\t\t\tif (unlikely(!vmf->pte ||",
            "\t\t\t\t     !pte_same(ptep_get(vmf->pte),",
            "\t\t\t\t\t\t\tvmf->orig_pte)))",
            "\t\t\t\tgoto unlock;",
            "",
            "\t\t\t/*",
            "\t\t\t * Get a page reference while we know the page can't be",
            "\t\t\t * freed.",
            "\t\t\t */",
            "\t\t\tget_page(vmf->page);",
            "\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\t\t\tret = vmf->page->pgmap->ops->migrate_to_ram(vmf);",
            "\t\t\tput_page(vmf->page);",
            "\t\t} else if (is_hwpoison_entry(entry)) {",
            "\t\t\tret = VM_FAULT_HWPOISON;",
            "\t\t} else if (is_pte_marker_entry(entry)) {",
            "\t\t\tret = handle_pte_marker(vmf);",
            "\t\t} else {",
            "\t\t\tprint_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);",
            "\t\t\tret = VM_FAULT_SIGBUS;",
            "\t\t}",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* Prevent swapoff from happening to us. */",
            "\tsi = get_swap_device(entry);",
            "\tif (unlikely(!si))",
            "\t\tgoto out;",
            "",
            "\tfolio = swap_cache_get_folio(entry, vma, vmf->address);",
            "\tif (folio)",
            "\t\tpage = folio_file_page(folio, swp_offset(entry));",
            "\tswapcache = folio;",
            "",
            "\tif (!folio) {",
            "\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO) &&",
            "\t\t    __swap_count(entry) == 1) {",
            "\t\t\t/* skip swapcache */",
            "\t\t\tfolio = alloc_swap_folio(vmf);",
            "\t\t\tpage = &folio->page;",
            "\t\t\tif (folio) {",
            "\t\t\t\t__folio_set_locked(folio);",
            "\t\t\t\t__folio_set_swapbacked(folio);",
            "",
            "\t\t\t\tnr_pages = folio_nr_pages(folio);",
            "\t\t\t\tif (folio_test_large(folio))",
            "\t\t\t\t\tentry.val = ALIGN_DOWN(entry.val, nr_pages);",
            "\t\t\t\t/*",
            "\t\t\t\t * Prevent parallel swapin from proceeding with",
            "\t\t\t\t * the cache flag. Otherwise, another thread",
            "\t\t\t\t * may finish swapin first, free the entry, and",
            "\t\t\t\t * swapout reusing the same entry. It's",
            "\t\t\t\t * undetectable as pte_same() returns true due",
            "\t\t\t\t * to entry reuse.",
            "\t\t\t\t */",
            "\t\t\t\tif (swapcache_prepare(entry, nr_pages)) {",
            "\t\t\t\t\t/*",
            "\t\t\t\t\t * Relax a bit to prevent rapid",
            "\t\t\t\t\t * repeated page faults.",
            "\t\t\t\t\t */",
            "\t\t\t\t\tadd_wait_queue(&swapcache_wq, &wait);",
            "\t\t\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\t\t\tremove_wait_queue(&swapcache_wq, &wait);",
            "\t\t\t\t\tgoto out_page;",
            "\t\t\t\t}",
            "\t\t\t\tneed_clear_cache = true;",
            "",
            "\t\t\t\tmem_cgroup_swapin_uncharge_swap(entry, nr_pages);",
            "",
            "\t\t\t\tshadow = get_shadow_from_swap_cache(entry);",
            "\t\t\t\tif (shadow)",
            "\t\t\t\t\tworkingset_refault(folio, shadow);",
            "",
            "\t\t\t\tfolio_add_lru(folio);",
            "",
            "\t\t\t\t/* To provide entry to swap_read_folio() */",
            "\t\t\t\tfolio->swap = entry;",
            "\t\t\t\tswap_read_folio(folio, NULL);",
            "\t\t\t\tfolio->private = NULL;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tpage = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,",
            "\t\t\t\t\t\tvmf);",
            "\t\t\tif (page)",
            "\t\t\t\tfolio = page_folio(page);",
            "\t\t\tswapcache = folio;",
            "\t\t}",
            "",
            "\t\tif (!folio) {",
            "\t\t\t/*",
            "\t\t\t * Back out if somebody else faulted in this pte",
            "\t\t\t * while we released the pte lock.",
            "\t\t\t */",
            "\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\tvmf->address, &vmf->ptl);",
            "\t\t\tif (likely(vmf->pte &&",
            "\t\t\t\t   pte_same(ptep_get(vmf->pte), vmf->orig_pte)))",
            "\t\t\t\tret = VM_FAULT_OOM;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "",
            "\t\t/* Had to read the page from swap area: Major fault */",
            "\t\tret = VM_FAULT_MAJOR;",
            "\t\tcount_vm_event(PGMAJFAULT);",
            "\t\tcount_memcg_event_mm(vma->vm_mm, PGMAJFAULT);",
            "\t} else if (PageHWPoison(page)) {",
            "\t\t/*",
            "\t\t * hwpoisoned dirty swapcache pages are kept for killing",
            "\t\t * owner processes (which may be unknown at hwpoison time)",
            "\t\t */",
            "\t\tret = VM_FAULT_HWPOISON;",
            "\t\tgoto out_release;",
            "\t}",
            "",
            "\tret |= folio_lock_or_retry(folio, vmf);",
            "\tif (ret & VM_FAULT_RETRY)",
            "\t\tgoto out_release;",
            "",
            "\tif (swapcache) {",
            "\t\t/*",
            "\t\t * Make sure folio_free_swap() or swapoff did not release the",
            "\t\t * swapcache from under us.  The page pin, and pte_same test",
            "\t\t * below, are not enough to exclude that.  Even if it is still",
            "\t\t * swapcache, we need to check that the page's swap has not",
            "\t\t * changed.",
            "\t\t */",
            "\t\tif (unlikely(!folio_test_swapcache(folio) ||",
            "\t\t\t     page_swap_entry(page).val != entry.val))",
            "\t\t\tgoto out_page;",
            "",
            "\t\t/*",
            "\t\t * KSM sometimes has to copy on read faults, for example, if",
            "\t\t * page->index of !PageKSM() pages would be nonlinear inside the",
            "\t\t * anon VMA -- PageKSM() is lost on actual swapout.",
            "\t\t */",
            "\t\tfolio = ksm_might_need_to_copy(folio, vma, vmf->address);",
            "\t\tif (unlikely(!folio)) {",
            "\t\t\tret = VM_FAULT_OOM;",
            "\t\t\tfolio = swapcache;",
            "\t\t\tgoto out_page;",
            "\t\t} else if (unlikely(folio == ERR_PTR(-EHWPOISON))) {",
            "\t\t\tret = VM_FAULT_HWPOISON;",
            "\t\t\tfolio = swapcache;",
            "\t\t\tgoto out_page;",
            "\t\t}",
            "\t\tif (folio != swapcache)",
            "\t\t\tpage = folio_page(folio, 0);",
            "",
            "\t\t/*",
            "\t\t * If we want to map a page that's in the swapcache writable, we",
            "\t\t * have to detect via the refcount if we're really the exclusive",
            "\t\t * owner. Try removing the extra reference from the local LRU",
            "\t\t * caches if required.",
            "\t\t */",
            "\t\tif ((vmf->flags & FAULT_FLAG_WRITE) && folio == swapcache &&",
            "\t\t    !folio_test_ksm(folio) && !folio_test_lru(folio))",
            "\t\t\tlru_add_drain();",
            "\t}",
            "",
            "\tfolio_throttle_swaprate(folio, GFP_KERNEL);",
            "",
            "\t/*",
            "\t * Back out if somebody else already faulted in this pte.",
            "\t */",
            "\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,",
            "\t\t\t&vmf->ptl);",
            "\tif (unlikely(!vmf->pte || !pte_same(ptep_get(vmf->pte), vmf->orig_pte)))",
            "\t\tgoto out_nomap;",
            "",
            "\tif (unlikely(!folio_test_uptodate(folio))) {",
            "\t\tret = VM_FAULT_SIGBUS;",
            "\t\tgoto out_nomap;",
            "\t}",
            "",
            "\t/* allocated large folios for SWP_SYNCHRONOUS_IO */",
            "\tif (folio_test_large(folio) && !folio_test_swapcache(folio)) {",
            "\t\tunsigned long nr = folio_nr_pages(folio);",
            "\t\tunsigned long folio_start = ALIGN_DOWN(vmf->address, nr * PAGE_SIZE);",
            "\t\tunsigned long idx = (vmf->address - folio_start) / PAGE_SIZE;",
            "\t\tpte_t *folio_ptep = vmf->pte - idx;",
            "\t\tpte_t folio_pte = ptep_get(folio_ptep);",
            "",
            "\t\tif (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||",
            "\t\t    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)",
            "\t\t\tgoto out_nomap;",
            "",
            "\t\tpage_idx = idx;",
            "\t\taddress = folio_start;",
            "\t\tptep = folio_ptep;",
            "\t\tgoto check_folio;",
            "\t}",
            "",
            "\tnr_pages = 1;",
            "\tpage_idx = 0;",
            "\taddress = vmf->address;",
            "\tptep = vmf->pte;",
            "\tif (folio_test_large(folio) && folio_test_swapcache(folio)) {",
            "\t\tint nr = folio_nr_pages(folio);",
            "\t\tunsigned long idx = folio_page_idx(folio, page);",
            "\t\tunsigned long folio_start = address - idx * PAGE_SIZE;",
            "\t\tunsigned long folio_end = folio_start + nr * PAGE_SIZE;",
            "\t\tpte_t *folio_ptep;",
            "\t\tpte_t folio_pte;",
            "",
            "\t\tif (unlikely(folio_start < max(address & PMD_MASK, vma->vm_start)))",
            "\t\t\tgoto check_folio;",
            "\t\tif (unlikely(folio_end > pmd_addr_end(address, vma->vm_end)))",
            "\t\t\tgoto check_folio;",
            "",
            "\t\tfolio_ptep = vmf->pte - idx;",
            "\t\tfolio_pte = ptep_get(folio_ptep);",
            "\t\tif (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||",
            "\t\t    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)",
            "\t\t\tgoto check_folio;",
            "",
            "\t\tpage_idx = idx;",
            "\t\taddress = folio_start;",
            "\t\tptep = folio_ptep;",
            "\t\tnr_pages = nr;",
            "\t\tentry = folio->swap;",
            "\t\tpage = &folio->page;",
            "\t}",
            "",
            "check_folio:",
            "\t/*",
            "\t * PG_anon_exclusive reuses PG_mappedtodisk for anon pages. A swap pte",
            "\t * must never point at an anonymous page in the swapcache that is",
            "\t * PG_anon_exclusive. Sanity check that this holds and especially, that",
            "\t * no filesystem set PG_mappedtodisk on a page in the swapcache. Sanity",
            "\t * check after taking the PT lock and making sure that nobody",
            "\t * concurrently faulted in this page and set PG_anon_exclusive.",
            "\t */",
            "\tBUG_ON(!folio_test_anon(folio) && folio_test_mappedtodisk(folio));",
            "\tBUG_ON(folio_test_anon(folio) && PageAnonExclusive(page));",
            "",
            "\t/*",
            "\t * Check under PT lock (to protect against concurrent fork() sharing",
            "\t * the swap entry concurrently) for certainly exclusive pages.",
            "\t */",
            "\tif (!folio_test_ksm(folio)) {",
            "\t\texclusive = pte_swp_exclusive(vmf->orig_pte);",
            "\t\tif (folio != swapcache) {",
            "\t\t\t/*",
            "\t\t\t * We have a fresh page that is not exposed to the",
            "\t\t\t * swapcache -> certainly exclusive.",
            "\t\t\t */",
            "\t\t\texclusive = true;",
            "\t\t} else if (exclusive && folio_test_writeback(folio) &&",
            "\t\t\t  data_race(si->flags & SWP_STABLE_WRITES)) {",
            "\t\t\t/*",
            "\t\t\t * This is tricky: not all swap backends support",
            "\t\t\t * concurrent page modifications while under writeback.",
            "\t\t\t *",
            "\t\t\t * So if we stumble over such a page in the swapcache",
            "\t\t\t * we must not set the page exclusive, otherwise we can",
            "\t\t\t * map it writable without further checks and modify it",
            "\t\t\t * while still under writeback.",
            "\t\t\t *",
            "\t\t\t * For these problematic swap backends, simply drop the",
            "\t\t\t * exclusive marker: this is perfectly fine as we start",
            "\t\t\t * writeback only if we fully unmapped the page and",
            "\t\t\t * there are no unexpected references on the page after",
            "\t\t\t * unmapping succeeded. After fully unmapped, no",
            "\t\t\t * further GUP references (FOLL_GET and FOLL_PIN) can",
            "\t\t\t * appear, so dropping the exclusive marker and mapping",
            "\t\t\t * it only R/O is fine.",
            "\t\t\t */",
            "\t\t\texclusive = false;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Some architectures may have to restore extra metadata to the page",
            "\t * when reading from swap. This metadata may be indexed by swap entry",
            "\t * so this must be called before swap_free().",
            "\t */",
            "\tarch_swap_restore(folio_swap(entry, folio), folio);",
            "",
            "\t/*",
            "\t * Remove the swap entry and conditionally try to free up the swapcache.",
            "\t * We're already holding a reference on the page but haven't mapped it",
            "\t * yet.",
            "\t */",
            "\tswap_free_nr(entry, nr_pages);",
            "\tif (should_try_to_free_swap(folio, vma, vmf->flags))",
            "\t\tfolio_free_swap(folio);",
            "",
            "\tadd_mm_counter(vma->vm_mm, MM_ANONPAGES, nr_pages);",
            "\tadd_mm_counter(vma->vm_mm, MM_SWAPENTS, -nr_pages);",
            "\tpte = mk_pte(page, vma->vm_page_prot);",
            "",
            "\t/*",
            "\t * Same logic as in do_wp_page(); however, optimize for pages that are",
            "\t * certainly not shared either because we just allocated them without",
            "\t * exposing them to the swapcache or because the swap entry indicates",
            "\t * exclusivity.",
            "\t */",
            "\tif (!folio_test_ksm(folio) &&",
            "\t    (exclusive || folio_ref_count(folio) == 1)) {",
            "\t\tif (vmf->flags & FAULT_FLAG_WRITE) {",
            "\t\t\tpte = maybe_mkwrite(pte_mkdirty(pte), vma);",
            "\t\t\tvmf->flags &= ~FAULT_FLAG_WRITE;",
            "\t\t}",
            "\t\trmap_flags |= RMAP_EXCLUSIVE;",
            "\t}",
            "\tfolio_ref_add(folio, nr_pages - 1);",
            "\tflush_icache_pages(vma, page, nr_pages);",
            "\tif (pte_swp_soft_dirty(vmf->orig_pte))",
            "\t\tpte = pte_mksoft_dirty(pte);",
            "\tif (pte_swp_uffd_wp(vmf->orig_pte))",
            "\t\tpte = pte_mkuffd_wp(pte);",
            "\tvmf->orig_pte = pte_advance_pfn(pte, page_idx);",
            "",
            "\t/* ksm created a completely new copy */",
            "\tif (unlikely(folio != swapcache && swapcache)) {",
            "\t\tfolio_add_new_anon_rmap(folio, vma, address, RMAP_EXCLUSIVE);",
            "\t\tfolio_add_lru_vma(folio, vma);",
            "\t} else if (!folio_test_anon(folio)) {",
            "\t\t/*",
            "\t\t * We currently only expect small !anon folios which are either",
            "\t\t * fully exclusive or fully shared, or new allocated large",
            "\t\t * folios which are fully exclusive. If we ever get large",
            "\t\t * folios within swapcache here, we have to be careful.",
            "\t\t */",
            "\t\tVM_WARN_ON_ONCE(folio_test_large(folio) && folio_test_swapcache(folio));",
            "\t\tVM_WARN_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\t\tfolio_add_new_anon_rmap(folio, vma, address, rmap_flags);",
            "\t} else {",
            "\t\tfolio_add_anon_rmap_ptes(folio, page, nr_pages, vma, address,",
            "\t\t\t\t\trmap_flags);",
            "\t}",
            "",
            "\tVM_BUG_ON(!folio_test_anon(folio) ||",
            "\t\t\t(pte_write(pte) && !PageAnonExclusive(page)));",
            "\tset_ptes(vma->vm_mm, address, ptep, pte, nr_pages);",
            "\tarch_do_swap_page_nr(vma->vm_mm, vma, address,",
            "\t\t\tpte, pte, nr_pages);",
            "",
            "\tfolio_unlock(folio);",
            "\tif (folio != swapcache && swapcache) {",
            "\t\t/*",
            "\t\t * Hold the lock to avoid the swap entry to be reused",
            "\t\t * until we take the PT lock for the pte_same() check",
            "\t\t * (to avoid false positives from pte_same). For",
            "\t\t * further safety release the lock after the swap_free",
            "\t\t * so that the swap count won't change under a",
            "\t\t * parallel locked swapcache.",
            "\t\t */",
            "\t\tfolio_unlock(swapcache);",
            "\t\tfolio_put(swapcache);",
            "\t}",
            "",
            "\tif (vmf->flags & FAULT_FLAG_WRITE) {",
            "\t\tret |= do_wp_page(vmf);",
            "\t\tif (ret & VM_FAULT_ERROR)",
            "\t\t\tret &= VM_FAULT_ERROR;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* No need to invalidate - it was non-present before */",
            "\tupdate_mmu_cache_range(vmf, vma, address, ptep, nr_pages);",
            "unlock:",
            "\tif (vmf->pte)",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "out:",
            "\t/* Clear the swap cache pin for direct swapin after PTL unlock */",
            "\tif (need_clear_cache) {",
            "\t\tswapcache_clear(si, entry, nr_pages);",
            "\t\tif (waitqueue_active(&swapcache_wq))",
            "\t\t\twake_up(&swapcache_wq);",
            "\t}",
            "\tif (si)",
            "\t\tput_swap_device(si);",
            "\treturn ret;",
            "out_nomap:",
            "\tif (vmf->pte)",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "out_page:",
            "\tfolio_unlock(folio);",
            "out_release:",
            "\tfolio_put(folio);",
            "\tif (folio != swapcache && swapcache) {",
            "\t\tfolio_unlock(swapcache);",
            "\t\tfolio_put(swapcache);",
            "\t}",
            "\tif (need_clear_cache) {",
            "\t\tswapcache_clear(si, entry, nr_pages);",
            "\t\tif (waitqueue_active(&swapcache_wq))",
            "\t\t\twake_up(&swapcache_wq);",
            "\t}",
            "\tif (si)",
            "\t\tput_swap_device(si);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "thp_swap_suitable_orders, do_swap_page",
          "description": "处理页面交换时的多种情况，如迁移条目、设备私有条目、硬件故障等，根据不同的条目类型执行相应逻辑，最终将交换页面映射到内存中。",
          "similarity": 0.6262412071228027
        },
        {
          "chunk_id": 33,
          "file_path": "mm/memory.c",
          "start_line": 5775,
          "end_line": 5921,
          "content": [
            "static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,",
            "\t\tunsigned long address, unsigned int flags)",
            "{",
            "\tstruct vm_fault vmf = {",
            "\t\t.vma = vma,",
            "\t\t.address = address & PAGE_MASK,",
            "\t\t.real_address = address,",
            "\t\t.flags = flags,",
            "\t\t.pgoff = linear_page_index(vma, address),",
            "\t\t.gfp_mask = __get_fault_gfp_mask(vma),",
            "\t};",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tunsigned long vm_flags = vma->vm_flags;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tvm_fault_t ret;",
            "",
            "\tpgd = pgd_offset(mm, address);",
            "\tp4d = p4d_alloc(mm, pgd, address);",
            "\tif (!p4d)",
            "\t\treturn VM_FAULT_OOM;",
            "",
            "\tvmf.pud = pud_alloc(mm, p4d, address);",
            "\tif (!vmf.pud)",
            "\t\treturn VM_FAULT_OOM;",
            "retry_pud:",
            "\tif (pud_none(*vmf.pud) &&",
            "\t    thp_vma_allowable_order(vma, vm_flags,",
            "\t\t\t\tTVA_IN_PF | TVA_ENFORCE_SYSFS, PUD_ORDER)) {",
            "\t\tret = create_huge_pud(&vmf);",
            "\t\tif (!(ret & VM_FAULT_FALLBACK))",
            "\t\t\treturn ret;",
            "\t} else {",
            "\t\tpud_t orig_pud = *vmf.pud;",
            "",
            "\t\tbarrier();",
            "\t\tif (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {",
            "",
            "\t\t\t/*",
            "\t\t\t * TODO once we support anonymous PUDs: NUMA case and",
            "\t\t\t * FAULT_FLAG_UNSHARE handling.",
            "\t\t\t */",
            "\t\t\tif ((flags & FAULT_FLAG_WRITE) && !pud_write(orig_pud)) {",
            "\t\t\t\tret = wp_huge_pud(&vmf, orig_pud);",
            "\t\t\t\tif (!(ret & VM_FAULT_FALLBACK))",
            "\t\t\t\t\treturn ret;",
            "\t\t\t} else {",
            "\t\t\t\thuge_pud_set_accessed(&vmf, orig_pud);",
            "\t\t\t\treturn 0;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\tvmf.pmd = pmd_alloc(mm, vmf.pud, address);",
            "\tif (!vmf.pmd)",
            "\t\treturn VM_FAULT_OOM;",
            "",
            "\t/* Huge pud page fault raced with pmd_alloc? */",
            "\tif (pud_trans_unstable(vmf.pud))",
            "\t\tgoto retry_pud;",
            "",
            "\tif (pmd_none(*vmf.pmd) &&",
            "\t    thp_vma_allowable_order(vma, vm_flags,",
            "\t\t\t\tTVA_IN_PF | TVA_ENFORCE_SYSFS, PMD_ORDER)) {",
            "\t\tret = create_huge_pmd(&vmf);",
            "\t\tif (!(ret & VM_FAULT_FALLBACK))",
            "\t\t\treturn ret;",
            "\t} else {",
            "\t\tvmf.orig_pmd = pmdp_get_lockless(vmf.pmd);",
            "",
            "\t\tif (unlikely(is_swap_pmd(vmf.orig_pmd))) {",
            "\t\t\tVM_BUG_ON(thp_migration_supported() &&",
            "\t\t\t\t\t  !is_pmd_migration_entry(vmf.orig_pmd));",
            "\t\t\tif (is_pmd_migration_entry(vmf.orig_pmd))",
            "\t\t\t\tpmd_migration_entry_wait(mm, vmf.pmd);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t\tif (pmd_trans_huge(vmf.orig_pmd) || pmd_devmap(vmf.orig_pmd)) {",
            "\t\t\tif (pmd_protnone(vmf.orig_pmd) && vma_is_accessible(vma))",
            "\t\t\t\treturn do_huge_pmd_numa_page(&vmf);",
            "",
            "\t\t\tif ((flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) &&",
            "\t\t\t    !pmd_write(vmf.orig_pmd)) {",
            "\t\t\t\tret = wp_huge_pmd(&vmf);",
            "\t\t\t\tif (!(ret & VM_FAULT_FALLBACK))",
            "\t\t\t\t\treturn ret;",
            "\t\t\t} else {",
            "\t\t\t\thuge_pmd_set_accessed(&vmf);",
            "\t\t\t\treturn 0;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\treturn handle_pte_fault(&vmf);",
            "}",
            "static inline void mm_account_fault(struct mm_struct *mm, struct pt_regs *regs,",
            "\t\t\t\t    unsigned long address, unsigned int flags,",
            "\t\t\t\t    vm_fault_t ret)",
            "{",
            "\tbool major;",
            "",
            "\t/* Incomplete faults will be accounted upon completion. */",
            "\tif (ret & VM_FAULT_RETRY)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * To preserve the behavior of older kernels, PGFAULT counters record",
            "\t * both successful and failed faults, as opposed to perf counters,",
            "\t * which ignore failed cases.",
            "\t */",
            "\tcount_vm_event(PGFAULT);",
            "\tcount_memcg_event_mm(mm, PGFAULT);",
            "",
            "\t/*",
            "\t * Do not account for unsuccessful faults (e.g. when the address wasn't",
            "\t * valid).  That includes arch_vma_access_permitted() failing before",
            "\t * reaching here. So this is not a \"this many hardware page faults\"",
            "\t * counter.  We should use the hw profiling for that.",
            "\t */",
            "\tif (ret & VM_FAULT_ERROR)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * We define the fault as a major fault when the final successful fault",
            "\t * is VM_FAULT_MAJOR, or if it retried (which implies that we couldn't",
            "\t * handle it immediately previously).",
            "\t */",
            "\tmajor = (ret & VM_FAULT_MAJOR) || (flags & FAULT_FLAG_TRIED);",
            "",
            "\tif (major)",
            "\t\tcurrent->maj_flt++;",
            "\telse",
            "\t\tcurrent->min_flt++;",
            "",
            "\t/*",
            "\t * If the fault is done for GUP, regs will be NULL.  We only do the",
            "\t * accounting for the per thread fault counters who triggered the",
            "\t * fault, and we skip the perf event updates.",
            "\t */",
            "\tif (!regs)",
            "\t\treturn;",
            "",
            "\tif (major)",
            "\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);",
            "\telse",
            "\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);",
            "}"
          ],
          "function_name": "__handle_mm_fault, mm_account_fault",
          "description": "核心内存故障处理函数，负责页表分配、大页检查及异常处理，最终调用handle_pte_fault完成页面故障处理并记录统计信息",
          "similarity": 0.6166496276855469
        },
        {
          "chunk_id": 8,
          "file_path": "mm/memory.c",
          "start_line": 1353,
          "end_line": 1469,
          "content": [
            "int",
            "copy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)",
            "{",
            "\tpgd_t *src_pgd, *dst_pgd;",
            "\tunsigned long addr = src_vma->vm_start;",
            "\tunsigned long end = src_vma->vm_end;",
            "\tstruct mm_struct *dst_mm = dst_vma->vm_mm;",
            "\tstruct mm_struct *src_mm = src_vma->vm_mm;",
            "\tstruct mmu_notifier_range range;",
            "\tunsigned long next, pfn;",
            "\tbool is_cow;",
            "\tint ret;",
            "",
            "\tif (!vma_needs_copy(dst_vma, src_vma))",
            "\t\treturn 0;",
            "",
            "\tif (is_vm_hugetlb_page(src_vma))",
            "\t\treturn copy_hugetlb_page_range(dst_mm, src_mm, dst_vma, src_vma);",
            "",
            "\tif (unlikely(src_vma->vm_flags & VM_PFNMAP)) {",
            "\t\tret = track_pfn_copy(dst_vma, src_vma, &pfn);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t}",
            "",
            "\t/*",
            "\t * We need to invalidate the secondary MMU mappings only when",
            "\t * there could be a permission downgrade on the ptes of the",
            "\t * parent mm. And a permission downgrade will only happen if",
            "\t * is_cow_mapping() returns true.",
            "\t */",
            "\tis_cow = is_cow_mapping(src_vma->vm_flags);",
            "",
            "\tif (is_cow) {",
            "\t\tmmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_PAGE,",
            "\t\t\t\t\t0, src_mm, addr, end);",
            "\t\tmmu_notifier_invalidate_range_start(&range);",
            "\t\t/*",
            "\t\t * Disabling preemption is not needed for the write side, as",
            "\t\t * the read side doesn't spin, but goes to the mmap_lock.",
            "\t\t *",
            "\t\t * Use the raw variant of the seqcount_t write API to avoid",
            "\t\t * lockdep complaining about preemptibility.",
            "\t\t */",
            "\t\tvma_assert_write_locked(src_vma);",
            "\t\traw_write_seqcount_begin(&src_mm->write_protect_seq);",
            "\t}",
            "",
            "\tret = 0;",
            "\tdst_pgd = pgd_offset(dst_mm, addr);",
            "\tsrc_pgd = pgd_offset(src_mm, addr);",
            "\tdo {",
            "\t\tnext = pgd_addr_end(addr, end);",
            "\t\tif (pgd_none_or_clear_bad(src_pgd))",
            "\t\t\tcontinue;",
            "\t\tif (unlikely(copy_p4d_range(dst_vma, src_vma, dst_pgd, src_pgd,",
            "\t\t\t\t\t    addr, next))) {",
            "\t\t\tret = -ENOMEM;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t} while (dst_pgd++, src_pgd++, addr = next, addr != end);",
            "",
            "\tif (is_cow) {",
            "\t\traw_write_seqcount_end(&src_mm->write_protect_seq);",
            "\t\tmmu_notifier_invalidate_range_end(&range);",
            "\t}",
            "\tif (ret && unlikely(src_vma->vm_flags & VM_PFNMAP))",
            "\t\tuntrack_pfn_copy(dst_vma, pfn);",
            "\treturn ret;",
            "}",
            "static inline bool should_zap_cows(struct zap_details *details)",
            "{",
            "\t/* By default, zap all pages */",
            "\tif (!details)",
            "\t\treturn true;",
            "",
            "\t/* Or, we zap COWed pages only if the caller wants to */",
            "\treturn details->even_cows;",
            "}",
            "static inline bool should_zap_folio(struct zap_details *details,",
            "\t\t\t\t    struct folio *folio)",
            "{",
            "\t/* If we can make a decision without *folio.. */",
            "\tif (should_zap_cows(details))",
            "\t\treturn true;",
            "",
            "\t/* Otherwise we should only zap non-anon folios */",
            "\treturn !folio_test_anon(folio);",
            "}",
            "static inline bool zap_drop_file_uffd_wp(struct zap_details *details)",
            "{",
            "\tif (!details)",
            "\t\treturn false;",
            "",
            "\treturn details->zap_flags & ZAP_FLAG_DROP_MARKER;",
            "}",
            "static inline void",
            "zap_install_uffd_wp_if_needed(struct vm_area_struct *vma,",
            "\t\t\t      unsigned long addr, pte_t *pte, int nr,",
            "\t\t\t      struct zap_details *details, pte_t pteval)",
            "{",
            "\t/* Zap on anonymous always means dropping everything */",
            "\tif (vma_is_anonymous(vma))",
            "\t\treturn;",
            "",
            "\tif (zap_drop_file_uffd_wp(details))",
            "\t\treturn;",
            "",
            "\tfor (;;) {",
            "\t\t/* the PFN in the PTE is irrelevant. */",
            "\t\tpte_install_uffd_wp_if_needed(vma, addr, pte, pteval);",
            "\t\tif (--nr == 0)",
            "\t\t\tbreak;",
            "\t\tpte++;",
            "\t\taddr += PAGE_SIZE;",
            "\t}",
            "}"
          ],
          "function_name": "copy_page_range, should_zap_cows, should_zap_folio, zap_drop_file_uffd_wp, zap_install_uffd_wp_if_needed",
          "description": "执行完整页表复制流程，初始化内存保护通知范围，调用各层级页表复制函数，并处理特殊映射（如PFNMAP）的复制逻辑，包含清除COW页面的相关控制逻辑。",
          "similarity": 0.5826061367988586
        },
        {
          "chunk_id": 40,
          "file_path": "mm/memory.c",
          "start_line": 6963,
          "end_line": 6991,
          "content": [
            "void __init ptlock_cache_init(void)",
            "{",
            "\tpage_ptl_cachep = kmem_cache_create(\"page->ptl\", sizeof(spinlock_t), 0,",
            "\t\t\tSLAB_PANIC, NULL);",
            "}",
            "bool ptlock_alloc(struct ptdesc *ptdesc)",
            "{",
            "\tspinlock_t *ptl;",
            "",
            "\tptl = kmem_cache_alloc(page_ptl_cachep, GFP_KERNEL);",
            "\tif (!ptl)",
            "\t\treturn false;",
            "\tptdesc->ptl = ptl;",
            "\treturn true;",
            "}",
            "void ptlock_free(struct ptdesc *ptdesc)",
            "{",
            "\tkmem_cache_free(page_ptl_cachep, ptdesc->ptl);",
            "}",
            "void vma_pgtable_walk_begin(struct vm_area_struct *vma)",
            "{",
            "\tif (is_vm_hugetlb_page(vma))",
            "\t\thugetlb_vma_lock_read(vma);",
            "}",
            "void vma_pgtable_walk_end(struct vm_area_struct *vma)",
            "{",
            "\tif (is_vm_hugetlb_page(vma))",
            "\t\thugetlb_vma_unlock_read(vma);",
            "}"
          ],
          "function_name": "ptlock_cache_init, ptlock_alloc, ptlock_free, vma_pgtable_walk_begin, vma_pgtable_walk_end",
          "description": "该代码段主要实现页表锁定资源的管理及HUGETLB页面的读锁控制。  \n`ptlock_cache_init` 初始化用于分配 `spinlock_t` 的 slab 缓存；`ptlock_alloc/free` 分配和释放页表锁资源；`vma_pgtable_walk_*` 对 HUGETLB 页面进行读锁保护。  \n上下文不完整：缺少页表遍历具体调用链及锁使用的完整逻辑。",
          "similarity": 0.5794671773910522
        },
        {
          "chunk_id": 2,
          "file_path": "mm/memory.c",
          "start_line": 267,
          "end_line": 414,
          "content": [
            "static inline void free_p4d_range(struct mmu_gather *tlb, pgd_t *pgd,",
            "\t\t\t\tunsigned long addr, unsigned long end,",
            "\t\t\t\tunsigned long floor, unsigned long ceiling)",
            "{",
            "\tp4d_t *p4d;",
            "\tunsigned long next;",
            "\tunsigned long start;",
            "",
            "\tstart = addr;",
            "\tp4d = p4d_offset(pgd, addr);",
            "\tdo {",
            "\t\tnext = p4d_addr_end(addr, end);",
            "\t\tif (p4d_none_or_clear_bad(p4d))",
            "\t\t\tcontinue;",
            "\t\tfree_pud_range(tlb, p4d, addr, next, floor, ceiling);",
            "\t} while (p4d++, addr = next, addr != end);",
            "",
            "\tstart &= PGDIR_MASK;",
            "\tif (start < floor)",
            "\t\treturn;",
            "\tif (ceiling) {",
            "\t\tceiling &= PGDIR_MASK;",
            "\t\tif (!ceiling)",
            "\t\t\treturn;",
            "\t}",
            "\tif (end - 1 > ceiling - 1)",
            "\t\treturn;",
            "",
            "\tp4d = p4d_offset(pgd, start);",
            "\tpgd_clear(pgd);",
            "\tp4d_free_tlb(tlb, p4d, start);",
            "}",
            "void free_pgd_range(struct mmu_gather *tlb,",
            "\t\t\tunsigned long addr, unsigned long end,",
            "\t\t\tunsigned long floor, unsigned long ceiling)",
            "{",
            "\tpgd_t *pgd;",
            "\tunsigned long next;",
            "",
            "\t/*",
            "\t * The next few lines have given us lots of grief...",
            "\t *",
            "\t * Why are we testing PMD* at this top level?  Because often",
            "\t * there will be no work to do at all, and we'd prefer not to",
            "\t * go all the way down to the bottom just to discover that.",
            "\t *",
            "\t * Why all these \"- 1\"s?  Because 0 represents both the bottom",
            "\t * of the address space and the top of it (using -1 for the",
            "\t * top wouldn't help much: the masks would do the wrong thing).",
            "\t * The rule is that addr 0 and floor 0 refer to the bottom of",
            "\t * the address space, but end 0 and ceiling 0 refer to the top",
            "\t * Comparisons need to use \"end - 1\" and \"ceiling - 1\" (though",
            "\t * that end 0 case should be mythical).",
            "\t *",
            "\t * Wherever addr is brought up or ceiling brought down, we must",
            "\t * be careful to reject \"the opposite 0\" before it confuses the",
            "\t * subsequent tests.  But what about where end is brought down",
            "\t * by PMD_SIZE below? no, end can't go down to 0 there.",
            "\t *",
            "\t * Whereas we round start (addr) and ceiling down, by different",
            "\t * masks at different levels, in order to test whether a table",
            "\t * now has no other vmas using it, so can be freed, we don't",
            "\t * bother to round floor or end up - the tests don't need that.",
            "\t */",
            "",
            "\taddr &= PMD_MASK;",
            "\tif (addr < floor) {",
            "\t\taddr += PMD_SIZE;",
            "\t\tif (!addr)",
            "\t\t\treturn;",
            "\t}",
            "\tif (ceiling) {",
            "\t\tceiling &= PMD_MASK;",
            "\t\tif (!ceiling)",
            "\t\t\treturn;",
            "\t}",
            "\tif (end - 1 > ceiling - 1)",
            "\t\tend -= PMD_SIZE;",
            "\tif (addr > end - 1)",
            "\t\treturn;",
            "\t/*",
            "\t * We add page table cache pages with PAGE_SIZE,",
            "\t * (see pte_free_tlb()), flush the tlb if we need",
            "\t */",
            "\ttlb_change_page_size(tlb, PAGE_SIZE);",
            "\tpgd = pgd_offset(tlb->mm, addr);",
            "\tdo {",
            "\t\tnext = pgd_addr_end(addr, end);",
            "\t\tif (pgd_none_or_clear_bad(pgd))",
            "\t\t\tcontinue;",
            "\t\tfree_p4d_range(tlb, pgd, addr, next, floor, ceiling);",
            "\t} while (pgd++, addr = next, addr != end);",
            "}",
            "void free_pgtables(struct mmu_gather *tlb, struct ma_state *mas,",
            "\t\t   struct vm_area_struct *vma, unsigned long floor,",
            "\t\t   unsigned long ceiling, bool mm_wr_locked)",
            "{",
            "\tstruct unlink_vma_file_batch vb;",
            "",
            "\tdo {",
            "\t\tunsigned long addr = vma->vm_start;",
            "\t\tstruct vm_area_struct *next;",
            "",
            "\t\t/*",
            "\t\t * Note: USER_PGTABLES_CEILING may be passed as ceiling and may",
            "\t\t * be 0.  This will underflow and is okay.",
            "\t\t */",
            "\t\tnext = mas_find(mas, ceiling - 1);",
            "\t\tif (unlikely(xa_is_zero(next)))",
            "\t\t\tnext = NULL;",
            "",
            "\t\t/*",
            "\t\t * Hide vma from rmap and truncate_pagecache before freeing",
            "\t\t * pgtables",
            "\t\t */",
            "\t\tif (mm_wr_locked)",
            "\t\t\tvma_start_write(vma);",
            "\t\tunlink_anon_vmas(vma);",
            "",
            "\t\tif (is_vm_hugetlb_page(vma)) {",
            "\t\t\tunlink_file_vma(vma);",
            "\t\t\thugetlb_free_pgd_range(tlb, addr, vma->vm_end,",
            "\t\t\t\tfloor, next ? next->vm_start : ceiling);",
            "\t\t} else {",
            "\t\t\tunlink_file_vma_batch_init(&vb);",
            "\t\t\tunlink_file_vma_batch_add(&vb, vma);",
            "",
            "\t\t\t/*",
            "\t\t\t * Optimization: gather nearby vmas into one call down",
            "\t\t\t */",
            "\t\t\twhile (next && next->vm_start <= vma->vm_end + PMD_SIZE",
            "\t\t\t       && !is_vm_hugetlb_page(next)) {",
            "\t\t\t\tvma = next;",
            "\t\t\t\tnext = mas_find(mas, ceiling - 1);",
            "\t\t\t\tif (unlikely(xa_is_zero(next)))",
            "\t\t\t\t\tnext = NULL;",
            "\t\t\t\tif (mm_wr_locked)",
            "\t\t\t\t\tvma_start_write(vma);",
            "\t\t\t\tunlink_anon_vmas(vma);",
            "\t\t\t\tunlink_file_vma_batch_add(&vb, vma);",
            "\t\t\t}",
            "\t\t\tunlink_file_vma_batch_final(&vb);",
            "\t\t\tfree_pgd_range(tlb, addr, vma->vm_end,",
            "\t\t\t\tfloor, next ? next->vm_start : ceiling);",
            "\t\t}",
            "\t\tvma = next;",
            "\t} while (vma);",
            "}"
          ],
          "function_name": "free_p4d_range, free_pgd_range, free_pgtables",
          "description": "实现多级页表（PGD/PMD/PUD/P4D）的释放逻辑，通过遍历地址范围清理无效页表项并减少相应的页表计数",
          "similarity": 0.562510073184967
        }
      ]
    },
    {
      "source_file": "mm/mmu_gather.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:52:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mmu_gather.c`\n\n---\n\n# mmu_gather.c 技术文档\n\n## 1. 文件概述\n\n`mmu_gather.c` 是 Linux 内核内存管理子系统中的关键组件，负责在页表项（PTE）或更高层级页表被撤销映射（unmap）后，高效地批量释放对应的物理页面和页表结构。该文件实现了 **MMU gather** 机制，用于延迟并批量处理 TLB（Translation Lookaside Buffer）刷新、反向映射（rmap）清理以及页面回收操作，以减少频繁的 TLB 刷新开销和锁竞争，提升性能。\n\n当内核需要释放大量虚拟内存区域（如进程退出、mmap 区域销毁）时，不会立即释放每个页面，而是先将待释放的页面收集到 `mmu_gather` 结构中，待累积到一定数量或显式调用 flush 操作时，再统一执行 TLB 刷新、rmap 解除和页面释放。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `tlb_next_batch(struct mmu_gather *tlb)`  \n  分配新的批处理批次（batch），用于扩展可收集的页面数量上限。\n\n- `tlb_flush_rmaps(struct mmu_gather *tlb, struct vm_area_struct *vma)`  \n  （仅在 SMP 下）处理延迟的反向映射（delayed rmap）移除操作，在 TLB 刷新后调用。\n\n- `__tlb_batch_free_encoded_pages(struct mmu_gather_batch *batch)`  \n  批量释放编码后的页面（包括普通页面和 swap 缓存），支持防软锁定（soft lockup）的调度点。\n\n- `tlb_batch_pages_flush(struct mmu_gather *tlb)`  \n  遍历所有批次，释放其中收集的所有页面。\n\n- `tlb_batch_list_free(struct mmu_gather *tlb)`  \n  释放动态分配的批次内存（非本地批次）。\n\n- `__tlb_remove_folio_pages_size(...)` / `__tlb_remove_folio_pages(...)` / `__tlb_remove_page_size(...)`  \n  将页面（单页或多页 folio）加入当前 gather 批次，支持延迟 rmap 和不同页面大小。\n\n- `tlb_remove_table_sync_one(void)`  \n  （RCU 表释放模式下）触发 IPI 同步，确保软件页表遍历安全。\n\n- `tlb_remove_table_rcu(struct rcu_head *head)`  \n  RCU 回调函数，用于异步释放页表结构。\n\n- `tlb_remove_table_free(struct mmu_table_batch *batch)`  \n  将页表批次提交给 RCU 机制进行延迟释放。\n\n### 关键数据结构\n\n- `struct mmu_gather`  \n  核心上下文结构，包含本地批次（`local`）、当前活跃批次（`active`）、批次计数、延迟 rmap 标志等。\n\n- `struct mmu_gather_batch`  \n  页面批次结构，包含指向编码页面指针数组、当前数量（`nr`）、最大容量（`max`）及下一个批次指针。\n\n- `struct mmu_table_batch`  \n  页表结构批次，用于批量收集待释放的页表（如 PMD、PUD 等）。\n\n- `encoded_page` 相关机制  \n  使用指针低位编码额外信息（如是否延迟 rmap、是否后跟 nr_pages 字段），节省内存并提高缓存效率。\n\n## 3. 关键实现\n\n### 批处理与动态扩展\n- 默认使用栈上或局部存储的 `local` 批次（避免内存分配）。\n- 当 `local` 批次满时，通过 `__get_free_page()` 动态分配新批次（最多 `MAX_GATHER_BATCH_COUNT` 个）。\n- `tlb_next_batch()` 在存在延迟 rmap 时限制扩展，确保语义正确性。\n\n### 延迟反向映射（Delayed Rmap）\n- 当页面仍被其他 VMA 引用但当前 VMA 正在 unmap 时，不立即调用 `folio_remove_rmap_ptes()`，而是标记 `ENCODED_PAGE_BIT_DELAY_RMAP`。\n- 在 `tlb_flush_rmaps()` 中统一处理，确保在 TLB 刷新**之后**才解除 rmap，防止 CPU 访问已释放页面。\n\n### 安全释放与防软锁定\n- 页面释放循环中每处理最多 `MAX_NR_FOLIOS_PER_FREE`（512）个 folio 调用 `cond_resched()`，避免在非抢占内核中长时间占用 CPU。\n- 若启用 `page_poisoning` 或 `init_on_free`，则按实际内存大小（而非 folio 数量）限制单次释放量，因初始化开销与内存大小成正比。\n\n### 页表结构的安全释放（RCU 模式）\n- 在支持软件页表遍历（如 `gup_fast`）的架构上，页表释放需与遍历操作同步。\n- 使用 `call_rcu()` 延迟释放页表，配合 `smp_call_function()` 触发 IPI 确保所有 CPU 完成 TLB 刷新后再释放内存。\n- 若 RCU 批次分配失败，则回退到即时释放（代码未完整展示，但注释提及）。\n\n### 编码页面指针\n- 利用页面指针对齐特性（通常低 2~3 位为 0），将标志位（如 `DELAY_RMAP`、`NR_PAGES_NEXT`）存储在指针低位。\n- 支持多页 folio：若 `nr_pages > 1`，则连续两个条目分别存储页面指针（带标志）和页数。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm_types.h>`、`<linux/mm_inline.h>`、`<linux/rmap.h>` 等，与 folio、page、VMA 管理紧密集成。\n- **TLB 管理**：通过 `<asm/tlb.h>` 与架构相关 TLB 刷新接口交互。\n- **RCU 机制**：在 `CONFIG_MMU_GATHER_RCU_TABLE_FREE` 下依赖 `<linux/rcupdate.h>` 实现页表安全释放。\n- **SMP 支持**：`tlb_flush_rmaps` 和页表同步仅在 `CONFIG_SMP` 下编译。\n- **高阶内存与交换**：使用 `<linux/highmem.h>`、`<linux/swap.h>` 处理高端内存和 swap 缓存释放。\n- **内存分配器**：通过 `__get_free_page(GFP_NOWAIT)` 动态分配批次内存。\n\n## 5. 使用场景\n\n- **进程退出（exit_mmap）**：释放整个地址空间时，大量页面通过 mmu_gather 批量回收。\n- **munmap 系统调用**：解除大块内存映射时，避免逐页 TLB 刷新。\n- **内存回收（reclaim）**：在直接回收或 kswapd 中撤销映射时使用。\n- **透明大页（THP）拆分**：拆分大页时需撤销多个 PTE 映射并释放 sub-page。\n- **页表收缩（shrink_page_list）**：在页面回收路径中解除映射。\n- **KSM（Kernel Samepage Merging）**：合并或取消合并页面时更新 rmap。\n- **页表层级释放**：当上层页表（如 PGD/P4D/PUD/PMD）不再被引用时，通过 `tlb_remove_table` 机制安全释放。",
      "similarity": 0.6824672222137451,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/mmu_gather.c",
          "start_line": 144,
          "end_line": 244,
          "content": [
            "static void tlb_batch_pages_flush(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\tfor (batch = &tlb->local; batch && batch->nr; batch = batch->next)",
            "\t\t__tlb_batch_free_encoded_pages(batch);",
            "\ttlb->active = &tlb->local;",
            "}",
            "static void tlb_batch_list_free(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch, *next;",
            "",
            "\tfor (batch = tlb->local.next; batch; batch = next) {",
            "\t\tnext = batch->next;",
            "\t\tfree_pages((unsigned long)batch, 0);",
            "\t}",
            "\ttlb->local.next = NULL;",
            "}",
            "static bool __tlb_remove_folio_pages_size(struct mmu_gather *tlb,",
            "\t\tstruct page *page, unsigned int nr_pages, bool delay_rmap,",
            "\t\tint page_size)",
            "{",
            "\tint flags = delay_rmap ? ENCODED_PAGE_BIT_DELAY_RMAP : 0;",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\tVM_BUG_ON(!tlb->end);",
            "",
            "#ifdef CONFIG_MMU_GATHER_PAGE_SIZE",
            "\tVM_WARN_ON(tlb->page_size != page_size);",
            "\tVM_WARN_ON_ONCE(nr_pages != 1 && page_size != PAGE_SIZE);",
            "\tVM_WARN_ON_ONCE(page_folio(page) != page_folio(page + nr_pages - 1));",
            "#endif",
            "",
            "\tbatch = tlb->active;",
            "\t/*",
            "\t * Add the page and check if we are full. If so",
            "\t * force a flush.",
            "\t */",
            "\tif (likely(nr_pages == 1)) {",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_page(page, flags);",
            "\t} else {",
            "\t\tflags |= ENCODED_PAGE_BIT_NR_PAGES_NEXT;",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_page(page, flags);",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_nr_pages(nr_pages);",
            "\t}",
            "\t/*",
            "\t * Make sure that we can always add another \"page\" + \"nr_pages\",",
            "\t * requiring two entries instead of only a single one.",
            "\t */",
            "\tif (batch->nr >= batch->max - 1) {",
            "\t\tif (!tlb_next_batch(tlb))",
            "\t\t\treturn true;",
            "\t\tbatch = tlb->active;",
            "\t}",
            "\tVM_BUG_ON_PAGE(batch->nr > batch->max - 1, page);",
            "",
            "\treturn false;",
            "}",
            "bool __tlb_remove_folio_pages(struct mmu_gather *tlb, struct page *page,",
            "\t\tunsigned int nr_pages, bool delay_rmap)",
            "{",
            "\treturn __tlb_remove_folio_pages_size(tlb, page, nr_pages, delay_rmap,",
            "\t\t\t\t\t     PAGE_SIZE);",
            "}",
            "bool __tlb_remove_page_size(struct mmu_gather *tlb, struct page *page,",
            "\t\tbool delay_rmap, int page_size)",
            "{",
            "\treturn __tlb_remove_folio_pages_size(tlb, page, 1, delay_rmap, page_size);",
            "}",
            "static void __tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < batch->nr; i++)",
            "\t\t__tlb_remove_table(batch->tables[i]);",
            "",
            "\tfree_page((unsigned long)batch);",
            "}",
            "static void tlb_remove_table_smp_sync(void *arg)",
            "{",
            "\t/* Simply deliver the interrupt */",
            "}",
            "void tlb_remove_table_sync_one(void)",
            "{",
            "\t/*",
            "\t * This isn't an RCU grace period and hence the page-tables cannot be",
            "\t * assumed to be actually RCU-freed.",
            "\t *",
            "\t * It is however sufficient for software page-table walkers that rely on",
            "\t * IRQ disabling.",
            "\t */",
            "\tsmp_call_function(tlb_remove_table_smp_sync, NULL, 1);",
            "}",
            "static void tlb_remove_table_rcu(struct rcu_head *head)",
            "{",
            "\t__tlb_remove_table_free(container_of(head, struct mmu_table_batch, rcu));",
            "}",
            "static void tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\tcall_rcu(&batch->rcu, tlb_remove_table_rcu);",
            "}"
          ],
          "function_name": "tlb_batch_pages_flush, tlb_batch_list_free, __tlb_remove_folio_pages_size, __tlb_remove_folio_pages, __tlb_remove_page_size, __tlb_remove_table_free, tlb_remove_table_smp_sync, tlb_remove_table_sync_one, tlb_remove_table_rcu, tlb_remove_table_free",
          "description": "实现页表条目批量移除和内存表管理，包含多页面处理、NR_PAGES_NEXT 标记解析及 RCU 安全释放",
          "similarity": 0.603759765625
        },
        {
          "chunk_id": 3,
          "file_path": "mm/mmu_gather.c",
          "start_line": 292,
          "end_line": 424,
          "content": [
            "static void tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\t__tlb_remove_table_free(batch);",
            "}",
            "static inline void tlb_table_invalidate(struct mmu_gather *tlb)",
            "{",
            "\tif (tlb_needs_table_invalidate()) {",
            "\t\t/*",
            "\t\t * Invalidate page-table caches used by hardware walkers. Then",
            "\t\t * we still need to RCU-sched wait while freeing the pages",
            "\t\t * because software walkers can still be in-flight.",
            "\t\t */",
            "\t\ttlb_flush_mmu_tlbonly(tlb);",
            "\t}",
            "}",
            "static void tlb_remove_table_one(void *table)",
            "{",
            "\ttlb_remove_table_sync_one();",
            "\t__tlb_remove_table(table);",
            "}",
            "static void tlb_table_flush(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_table_batch **batch = &tlb->batch;",
            "",
            "\tif (*batch) {",
            "\t\ttlb_table_invalidate(tlb);",
            "\t\ttlb_remove_table_free(*batch);",
            "\t\t*batch = NULL;",
            "\t}",
            "}",
            "void tlb_remove_table(struct mmu_gather *tlb, void *table)",
            "{",
            "\tstruct mmu_table_batch **batch = &tlb->batch;",
            "",
            "\tif (*batch == NULL) {",
            "\t\t*batch = (struct mmu_table_batch *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);",
            "\t\tif (*batch == NULL) {",
            "\t\t\ttlb_table_invalidate(tlb);",
            "\t\t\ttlb_remove_table_one(table);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\t(*batch)->nr = 0;",
            "\t}",
            "",
            "\t(*batch)->tables[(*batch)->nr++] = table;",
            "\tif ((*batch)->nr == MAX_TABLE_BATCH)",
            "\t\ttlb_table_flush(tlb);",
            "}",
            "static inline void tlb_table_init(struct mmu_gather *tlb)",
            "{",
            "\ttlb->batch = NULL;",
            "}",
            "static inline void tlb_table_flush(struct mmu_gather *tlb) { }",
            "static inline void tlb_table_init(struct mmu_gather *tlb) { }",
            "static void tlb_flush_mmu_free(struct mmu_gather *tlb)",
            "{",
            "\ttlb_table_flush(tlb);",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb_batch_pages_flush(tlb);",
            "#endif",
            "}",
            "void tlb_flush_mmu(struct mmu_gather *tlb)",
            "{",
            "\ttlb_flush_mmu_tlbonly(tlb);",
            "\ttlb_flush_mmu_free(tlb);",
            "}",
            "static void __tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,",
            "\t\t\t     bool fullmm)",
            "{",
            "\ttlb->mm = mm;",
            "\ttlb->fullmm = fullmm;",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb->need_flush_all = 0;",
            "\ttlb->local.next = NULL;",
            "\ttlb->local.nr   = 0;",
            "\ttlb->local.max  = ARRAY_SIZE(tlb->__pages);",
            "\ttlb->active     = &tlb->local;",
            "\ttlb->batch_count = 0;",
            "#endif",
            "\ttlb->delayed_rmap = 0;",
            "",
            "\ttlb_table_init(tlb);",
            "#ifdef CONFIG_MMU_GATHER_PAGE_SIZE",
            "\ttlb->page_size = 0;",
            "#endif",
            "",
            "\t__tlb_reset_range(tlb);",
            "\tinc_tlb_flush_pending(tlb->mm);",
            "}",
            "void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm)",
            "{",
            "\t__tlb_gather_mmu(tlb, mm, false);",
            "}",
            "void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm)",
            "{",
            "\t__tlb_gather_mmu(tlb, mm, true);",
            "}",
            "void tlb_finish_mmu(struct mmu_gather *tlb)",
            "{",
            "\t/*",
            "\t * If there are parallel threads are doing PTE changes on same range",
            "\t * under non-exclusive lock (e.g., mmap_lock read-side) but defer TLB",
            "\t * flush by batching, one thread may end up seeing inconsistent PTEs",
            "\t * and result in having stale TLB entries.  So flush TLB forcefully",
            "\t * if we detect parallel PTE batching threads.",
            "\t *",
            "\t * However, some syscalls, e.g. munmap(), may free page tables, this",
            "\t * needs force flush everything in the given range. Otherwise this",
            "\t * may result in having stale TLB entries for some architectures,",
            "\t * e.g. aarch64, that could specify flush what level TLB.",
            "\t */",
            "\tif (mm_tlb_flush_nested(tlb->mm)) {",
            "\t\t/*",
            "\t\t * The aarch64 yields better performance with fullmm by",
            "\t\t * avoiding multiple CPUs spamming TLBI messages at the",
            "\t\t * same time.",
            "\t\t *",
            "\t\t * On x86 non-fullmm doesn't yield significant difference",
            "\t\t * against fullmm.",
            "\t\t */",
            "\t\ttlb->fullmm = 1;",
            "\t\t__tlb_reset_range(tlb);",
            "\t\ttlb->freed_tables = 1;",
            "\t}",
            "",
            "\ttlb_flush_mmu(tlb);",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb_batch_list_free(tlb);",
            "#endif",
            "\tdec_tlb_flush_pending(tlb->mm);",
            "}"
          ],
          "function_name": "tlb_remove_table_free, tlb_table_invalidate, tlb_remove_table_one, tlb_table_flush, tlb_remove_table, tlb_table_init, tlb_table_flush, tlb_table_init, tlb_flush_mmu_free, tlb_flush_mmu, __tlb_gather_mmu, tlb_gather_mmu, tlb_gather_mmu_fullmm, tlb_finish_mmu",
          "description": "提供 TLB 无效化、页表批量释放及 MMU 收集器初始化/终止接口，包含跨架构的 TLB 同步机制",
          "similarity": 0.5164355039596558
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mmu_gather.c",
          "start_line": 18,
          "end_line": 120,
          "content": [
            "static bool tlb_next_batch(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\t/* Limit batching if we have delayed rmaps pending */",
            "\tif (tlb->delayed_rmap && tlb->active != &tlb->local)",
            "\t\treturn false;",
            "",
            "\tbatch = tlb->active;",
            "\tif (batch->next) {",
            "\t\ttlb->active = batch->next;",
            "\t\treturn true;",
            "\t}",
            "",
            "\tif (tlb->batch_count == MAX_GATHER_BATCH_COUNT)",
            "\t\treturn false;",
            "",
            "\tbatch = (void *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);",
            "\tif (!batch)",
            "\t\treturn false;",
            "",
            "\ttlb->batch_count++;",
            "\tbatch->next = NULL;",
            "\tbatch->nr   = 0;",
            "\tbatch->max  = MAX_GATHER_BATCH;",
            "",
            "\ttlb->active->next = batch;",
            "\ttlb->active = batch;",
            "",
            "\treturn true;",
            "}",
            "static void tlb_flush_rmap_batch(struct mmu_gather_batch *batch, struct vm_area_struct *vma)",
            "{",
            "\tstruct encoded_page **pages = batch->encoded_pages;",
            "",
            "\tfor (int i = 0; i < batch->nr; i++) {",
            "\t\tstruct encoded_page *enc = pages[i];",
            "",
            "\t\tif (encoded_page_flags(enc) & ENCODED_PAGE_BIT_DELAY_RMAP) {",
            "\t\t\tstruct page *page = encoded_page_ptr(enc);",
            "\t\t\tunsigned int nr_pages = 1;",
            "",
            "\t\t\tif (unlikely(encoded_page_flags(enc) &",
            "\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\tnr_pages = encoded_nr_pages(pages[++i]);",
            "",
            "\t\t\tfolio_remove_rmap_ptes(page_folio(page), page, nr_pages,",
            "\t\t\t\t\t       vma);",
            "\t\t}",
            "\t}",
            "}",
            "void tlb_flush_rmaps(struct mmu_gather *tlb, struct vm_area_struct *vma)",
            "{",
            "\tif (!tlb->delayed_rmap)",
            "\t\treturn;",
            "",
            "\ttlb_flush_rmap_batch(&tlb->local, vma);",
            "\tif (tlb->active != &tlb->local)",
            "\t\ttlb_flush_rmap_batch(tlb->active, vma);",
            "\ttlb->delayed_rmap = 0;",
            "}",
            "static void __tlb_batch_free_encoded_pages(struct mmu_gather_batch *batch)",
            "{",
            "\tstruct encoded_page **pages = batch->encoded_pages;",
            "\tunsigned int nr, nr_pages;",
            "",
            "\twhile (batch->nr) {",
            "\t\tif (!page_poisoning_enabled_static() && !want_init_on_free()) {",
            "\t\t\tnr = min(MAX_NR_FOLIOS_PER_FREE, batch->nr);",
            "",
            "\t\t\t/*",
            "\t\t\t * Make sure we cover page + nr_pages, and don't leave",
            "\t\t\t * nr_pages behind when capping the number of entries.",
            "\t\t\t */",
            "\t\t\tif (unlikely(encoded_page_flags(pages[nr - 1]) &",
            "\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\tnr++;",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * With page poisoning and init_on_free, the time it",
            "\t\t\t * takes to free memory grows proportionally with the",
            "\t\t\t * actual memory size. Therefore, limit based on the",
            "\t\t\t * actual memory size and not the number of involved",
            "\t\t\t * folios.",
            "\t\t\t */",
            "\t\t\tfor (nr = 0, nr_pages = 0;",
            "\t\t\t     nr < batch->nr && nr_pages < MAX_NR_FOLIOS_PER_FREE;",
            "\t\t\t     nr++) {",
            "\t\t\t\tif (unlikely(encoded_page_flags(pages[nr]) &",
            "\t\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\t\tnr_pages += encoded_nr_pages(pages[++nr]);",
            "\t\t\t\telse",
            "\t\t\t\t\tnr_pages++;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tfree_pages_and_swap_cache(pages, nr);",
            "\t\tpages += nr;",
            "\t\tbatch->nr -= nr;",
            "",
            "\t\tcond_resched();",
            "\t}",
            "}"
          ],
          "function_name": "tlb_next_batch, tlb_flush_rmap_batch, tlb_flush_rmaps, __tlb_batch_free_encoded_pages",
          "description": "管理 TLB 批量操作的延迟 RMAP 处理逻辑，包括批次链表管理、编码页面释放及 RMAP 标志清除",
          "similarity": 0.4948374330997467
        },
        {
          "chunk_id": 0,
          "file_path": "mm/mmu_gather.c",
          "start_line": 1,
          "end_line": 17,
          "content": [
            "#include <linux/gfp.h>",
            "#include <linux/highmem.h>",
            "#include <linux/kernel.h>",
            "#include <linux/mmdebug.h>",
            "#include <linux/mm_types.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/smp.h>",
            "#include <linux/swap.h>",
            "#include <linux/rmap.h>",
            "",
            "#include <asm/pgalloc.h>",
            "#include <asm/tlb.h>",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            ""
          ],
          "function_name": null,
          "description": "声明 MMU 聚合功能所需头文件，根据配置条件包含架构相关实现",
          "similarity": 0.4878021776676178
        }
      ]
    },
    {
      "source_file": "mm/vmscan.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:33:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmscan.c`\n\n---\n\n# vmscan.c 技术文档\n\n## 1. 文件概述\n\n`vmscan.c` 是 Linux 内核内存管理子系统中的核心文件，主要负责**页面回收（page reclaim）**机制的实现。该文件实现了内核在内存压力下如何选择并释放不再活跃或可回收的物理页帧（pages），以维持系统可用内存水位。其核心功能包括：\n\n- 实现 `kswapd` 内核线程，用于后台异步回收内存\n- 提供直接回收（direct reclaim）路径，供分配器在内存不足时同步触发\n- 管理匿名页（anonymous pages）和文件缓存页（file-backed pages）的回收策略\n- 支持基于内存控制组（memcg）的层级化内存回收\n- 与交换（swap）、压缩（compaction）、OOM killer 等子系统协同工作\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct scan_control`**  \n  页面回收上下文控制结构，包含本次回收操作的所有参数和状态：\n  - `nr_to_reclaim`：目标回收页数\n  - `target_mem_cgroup`：目标内存 cgroup（用于 memcg 回收）\n  - `may_unmap` / `may_swap` / `may_writepage`：控制是否允许解除映射、交换、写回\n  - `priority`：扫描优先级（0~12，值越低压力越大）\n  - `order`：请求分配的阶数（影响回收激进程度）\n  - `nr_scanned` / `nr_reclaimed`：已扫描和已回收页数统计\n  - `anon_cost` / `file_cost`：用于平衡匿名页与文件页回收比例\n\n- **全局变量**\n  - `vm_swappiness`（默认 60）：控制系统倾向于回收匿名页（需 swap）还是文件页（可丢弃）\n\n### 主要函数（部分在代码片段中体现）\n\n- `cgroup_reclaim()` / `root_reclaim()`：判断当前回收是否针对特定 memcg 或全局\n- `writeback_throttling_sane()`：判断是否可使用标准脏页限流机制\n- `set_task_reclaim_state()` / `flush_reclaim_state()`：管理任务的 slab 回收状态\n- （注：核心回收函数如 `shrink_lruvec()`、`kswapd()` 等未在片段中展示）\n\n## 3. 关键实现\n\n### 内存回收控制逻辑\n\n- **回收目标决策**：通过 `scan_control` 结构传递回收上下文，区分直接回收（分配失败触发）与 kswapd 后台回收。\n- **LRU 链管理**：利用 `prefetchw_prev_lru_folio` 宏优化 LRU 链遍历时的 CPU 缓存预取性能。\n- **Memcg 集成**：\n  - 若 `target_mem_cgroup` 非空，则优先回收该 cgroup 的内存\n  - 支持 `memory.low` 保护机制：当常规回收无法满足需求且跳过受保护 cgroup 时，会触发二次强制回收（`memcg_low_reclaim`）\n- **脏页处理策略**：\n  - 在传统 memcg 模式下，禁用标准 `balance_dirty_pages()` 限流，改用直接阻塞回收（`writeback_throttling_sane()` 判断）\n  - 通过 `may_writepage` 控制是否在 laptop mode 下批量写回脏页\n\n### 回收统计与状态同步\n\n- **Slab 回收计数**：通过 `reclaim_state` 结构将非 LRU 回收（如 slab 释放）计入全局统计，但**仅在全局回收时计入**，避免 memcg 回收时高估实际效果导致欠回收。\n- **PSI/Trace 集成**：包含 `<trace/events/vmscan.h>` 用于性能分析，支持压力状态指示器（PSI）监控内存压力。\n\n## 4. 依赖关系\n\n### 头文件依赖\n\n- **核心内存管理**：`<linux/mm.h>`, `<linux/gfp.h>`, `<linux/swap.h>`, `<linux/vmstat.h>`\n- **LRU 与反向映射**：`<linux/rmap.h>`, `<linux/pagemap.h>`\n- **内存控制组**：`<linux/memcontrol.h>`\n- **IO 与写回**：`<linux/writeback.h>`, `<linux/backing-dev.h>`\n- **压缩与迁移**：`<linux/compaction.h>`, `<linux/migrate.h>`\n- **体系结构相关**：`<asm/tlbflush.h>`\n\n### 子系统交互\n\n- **Swap 子系统**：通过 `swapops.h` 和 `swap.h` 实现匿名页换出\n- **Slab 分配器**：通过 `reclaim_state` 接收 slab 回收通知\n- **OOM Killer**：当回收无法释放足够内存时触发\n- **Khugepaged**：大页合并/拆分与回收协同\n- **Memory Tiering**：支持分层内存架构中的页降级（demotion）控制\n\n## 5. 使用场景\n\n- **内存分配失败时的直接回收**：当 `alloc_pages()` 等分配函数无法满足请求时，同步调用回收路径。\n- **kswapd 后台回收**：当空闲内存低于 `watermark[low]` 时，唤醒 `kswapd` 线程异步回收至 `watermark[high]`。\n- **Memcg 内存超限时的层级回收**：当某个 cgroup 超过其内存限制时，仅回收该 cgroup 及其子树的页面。\n- **系统休眠（Hibernation）**：通过 `hibernation_mode` 标志优化休眠过程中的内存回收。\n- **主动内存回收（Proactive Reclaim）**：用户空间通过 `memory.reclaim` 接口触发预清回收。\n- **内存压缩准备**：当 `compaction_ready` 置位时，回收操作会为后续内存压缩腾出连续空间。",
      "similarity": 0.6809797883033752,
      "chunks": [
        {
          "chunk_id": 19,
          "file_path": "mm/vmscan.c",
          "start_line": 3605,
          "end_line": 3729,
          "content": [
            "static int walk_pud_range(p4d_t *p4d, unsigned long start, unsigned long end,",
            "\t\t\t  struct mm_walk *args)",
            "{",
            "\tint i;",
            "\tpud_t *pud;",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tstruct lru_gen_mm_walk *walk = args->private;",
            "",
            "\tVM_WARN_ON_ONCE(p4d_leaf(*p4d));",
            "",
            "\tpud = pud_offset(p4d, start & P4D_MASK);",
            "restart:",
            "\tfor (i = pud_index(start), addr = start; addr != end; i++, addr = next) {",
            "\t\tpud_t val = READ_ONCE(pud[i]);",
            "",
            "\t\tnext = pud_addr_end(addr, end);",
            "",
            "\t\tif (!pud_present(val) || WARN_ON_ONCE(pud_leaf(val)))",
            "\t\t\tcontinue;",
            "",
            "\t\twalk_pmd_range(&val, addr, next, args);",
            "",
            "\t\tif (need_resched() || walk->batched >= MAX_LRU_BATCH) {",
            "\t\t\tend = (addr | ~PUD_MASK) + 1;",
            "\t\t\tgoto done;",
            "\t\t}",
            "\t}",
            "",
            "\tif (i < PTRS_PER_PUD && get_next_vma(P4D_MASK, PUD_SIZE, args, &start, &end))",
            "\t\tgoto restart;",
            "",
            "\tend = round_up(end, P4D_SIZE);",
            "done:",
            "\tif (!end || !args->vma)",
            "\t\treturn 1;",
            "",
            "\twalk->next_addr = max(end, args->vma->vm_start);",
            "",
            "\treturn -EAGAIN;",
            "}",
            "static void walk_mm(struct mm_struct *mm, struct lru_gen_mm_walk *walk)",
            "{",
            "\tstatic const struct mm_walk_ops mm_walk_ops = {",
            "\t\t.test_walk = should_skip_vma,",
            "\t\t.p4d_entry = walk_pud_range,",
            "\t\t.walk_lock = PGWALK_RDLOCK,",
            "\t};",
            "\tint err;",
            "\tstruct lruvec *lruvec = walk->lruvec;",
            "",
            "\twalk->next_addr = FIRST_USER_ADDRESS;",
            "",
            "\tdo {",
            "\t\tDEFINE_MAX_SEQ(lruvec);",
            "",
            "\t\terr = -EBUSY;",
            "",
            "\t\t/* another thread might have called inc_max_seq() */",
            "\t\tif (walk->seq != max_seq)",
            "\t\t\tbreak;",
            "",
            "\t\t/* the caller might be holding the lock for write */",
            "\t\tif (mmap_read_trylock(mm)) {",
            "\t\t\terr = walk_page_range(mm, walk->next_addr, ULONG_MAX, &mm_walk_ops, walk);",
            "",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t}",
            "",
            "\t\tif (walk->batched) {",
            "\t\t\tspin_lock_irq(&lruvec->lru_lock);",
            "\t\t\treset_batch_size(walk);",
            "\t\t\tspin_unlock_irq(&lruvec->lru_lock);",
            "\t\t}",
            "",
            "\t\tcond_resched();",
            "\t} while (err == -EAGAIN);",
            "}",
            "static void clear_mm_walk(void)",
            "{",
            "\tstruct lru_gen_mm_walk *walk = current->reclaim_state->mm_walk;",
            "",
            "\tVM_WARN_ON_ONCE(walk && memchr_inv(walk->nr_pages, 0, sizeof(walk->nr_pages)));",
            "\tVM_WARN_ON_ONCE(walk && memchr_inv(walk->mm_stats, 0, sizeof(walk->mm_stats)));",
            "",
            "\tcurrent->reclaim_state->mm_walk = NULL;",
            "",
            "\tif (!current_is_kswapd())",
            "\t\tkfree(walk);",
            "}",
            "static bool inc_min_seq(struct lruvec *lruvec, int type, bool can_swap)",
            "{",
            "\tint zone;",
            "\tint remaining = MAX_LRU_BATCH;",
            "\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;",
            "\tint new_gen, old_gen = lru_gen_from_seq(lrugen->min_seq[type]);",
            "",
            "\tif (type == LRU_GEN_ANON && !can_swap)",
            "\t\tgoto done;",
            "",
            "\t/* prevent cold/hot inversion if force_scan is true */",
            "\tfor (zone = 0; zone < MAX_NR_ZONES; zone++) {",
            "\t\tstruct list_head *head = &lrugen->folios[old_gen][type][zone];",
            "",
            "\t\twhile (!list_empty(head)) {",
            "\t\t\tstruct folio *folio = lru_to_folio(head);",
            "",
            "\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_test_unevictable(folio), folio);",
            "\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_test_active(folio), folio);",
            "\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_is_file_lru(folio) != type, folio);",
            "\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_zonenum(folio) != zone, folio);",
            "",
            "\t\t\tnew_gen = folio_inc_gen(lruvec, folio, false);",
            "\t\t\tlist_move_tail(&folio->lru, &lrugen->folios[new_gen][type][zone]);",
            "",
            "\t\t\tif (!--remaining)",
            "\t\t\t\treturn false;",
            "\t\t}",
            "\t}",
            "done:",
            "\treset_ctrl_pos(lruvec, type, true);",
            "\tWRITE_ONCE(lrugen->min_seq[type], lrugen->min_seq[type] + 1);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "walk_pud_range, walk_mm, clear_mm_walk, inc_min_seq",
          "description": "执行全内存扫描流程，通过多级页表遍历收集页面信息，维护最小序列号推进机制并清理扫描状态",
          "similarity": 0.6405007839202881
        },
        {
          "chunk_id": 24,
          "file_path": "mm/vmscan.c",
          "start_line": 4269,
          "end_line": 4377,
          "content": [
            "void lru_gen_soft_reclaim(struct mem_cgroup *memcg, int nid)",
            "{",
            "\tstruct lruvec *lruvec = get_lruvec(memcg, nid);",
            "",
            "\t/* see the comment on MEMCG_NR_GENS */",
            "\tif (READ_ONCE(lruvec->lrugen.seg) != MEMCG_LRU_HEAD)",
            "\t\tlru_gen_rotate_memcg(lruvec, MEMCG_LRU_HEAD);",
            "}",
            "static bool sort_folio(struct lruvec *lruvec, struct folio *folio, struct scan_control *sc,",
            "\t\t       int tier_idx)",
            "{",
            "\tbool success;",
            "\tint gen = folio_lru_gen(folio);",
            "\tint type = folio_is_file_lru(folio);",
            "\tint zone = folio_zonenum(folio);",
            "\tint delta = folio_nr_pages(folio);",
            "\tint refs = folio_lru_refs(folio);",
            "\tint tier = lru_tier_from_refs(refs);",
            "\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;",
            "",
            "\tVM_WARN_ON_ONCE_FOLIO(gen >= MAX_NR_GENS, folio);",
            "",
            "\t/* unevictable */",
            "\tif (!folio_evictable(folio)) {",
            "\t\tsuccess = lru_gen_del_folio(lruvec, folio, true);",
            "\t\tVM_WARN_ON_ONCE_FOLIO(!success, folio);",
            "\t\tfolio_set_unevictable(folio);",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "\t\t__count_vm_events(UNEVICTABLE_PGCULLED, delta);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/* dirty lazyfree */",
            "\tif (type == LRU_GEN_FILE && folio_test_anon(folio) && folio_test_dirty(folio)) {",
            "\t\tsuccess = lru_gen_del_folio(lruvec, folio, true);",
            "\t\tVM_WARN_ON_ONCE_FOLIO(!success, folio);",
            "\t\tfolio_set_swapbacked(folio);",
            "\t\tlruvec_add_folio_tail(lruvec, folio);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/* promoted */",
            "\tif (gen != lru_gen_from_seq(lrugen->min_seq[type])) {",
            "\t\tlist_move(&folio->lru, &lrugen->folios[gen][type][zone]);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/* protected */",
            "\tif (tier > tier_idx || refs == BIT(LRU_REFS_WIDTH)) {",
            "\t\tint hist = lru_hist_from_seq(lrugen->min_seq[type]);",
            "",
            "\t\tgen = folio_inc_gen(lruvec, folio, false);",
            "\t\tlist_move_tail(&folio->lru, &lrugen->folios[gen][type][zone]);",
            "",
            "\t\tWRITE_ONCE(lrugen->protected[hist][type][tier - 1],",
            "\t\t\t   lrugen->protected[hist][type][tier - 1] + delta);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/* ineligible */",
            "\tif (zone > sc->reclaim_idx) {",
            "\t\tgen = folio_inc_gen(lruvec, folio, false);",
            "\t\tlist_move_tail(&folio->lru, &lrugen->folios[gen][type][zone]);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/* waiting for writeback */",
            "\tif (folio_test_locked(folio) || folio_test_writeback(folio) ||",
            "\t    (type == LRU_GEN_FILE && folio_test_dirty(folio))) {",
            "\t\tgen = folio_inc_gen(lruvec, folio, true);",
            "\t\tlist_move(&folio->lru, &lrugen->folios[gen][type][zone]);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}",
            "static bool isolate_folio(struct lruvec *lruvec, struct folio *folio, struct scan_control *sc)",
            "{",
            "\tbool success;",
            "",
            "\t/* swap constrained */",
            "\tif (!(sc->gfp_mask & __GFP_IO) &&",
            "\t    (folio_test_dirty(folio) ||",
            "\t     (folio_test_anon(folio) && !folio_test_swapcache(folio))))",
            "\t\treturn false;",
            "",
            "\t/* raced with release_pages() */",
            "\tif (!folio_try_get(folio))",
            "\t\treturn false;",
            "",
            "\t/* raced with another isolation */",
            "\tif (!folio_test_clear_lru(folio)) {",
            "\t\tfolio_put(folio);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/* see the comment on MAX_NR_TIERS */",
            "\tif (!folio_test_referenced(folio))",
            "\t\tset_mask_bits(&folio->flags, LRU_REFS_MASK | LRU_REFS_FLAGS, 0);",
            "",
            "\t/* for shrink_folio_list() */",
            "\tfolio_clear_reclaim(folio);",
            "\tfolio_clear_referenced(folio);",
            "",
            "\tsuccess = lru_gen_del_folio(lruvec, folio, true);",
            "\tVM_WARN_ON_ONCE_FOLIO(!success, folio);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "lru_gen_soft_reclaim, sort_folio, isolate_folio",
          "description": "该代码段实现了内存组（memcg）的LRU分级管理逻辑，核心功能是通过`lru_gen_soft_reclaim`触发LRU链表旋转，`sort_folio`根据页面属性（如脏/匿名/引用位）将其归类至不同层级的LRU队列，`isolate_folio`则负责安全隔离可回收页面并更新统计信息。三者协同完成基于生成代数（gen）的页面分类与回收决策。",
          "similarity": 0.6142499446868896
        },
        {
          "chunk_id": 2,
          "file_path": "mm/vmscan.c",
          "start_line": 343,
          "end_line": 443,
          "content": [
            "unsigned long zone_reclaimable_pages(struct zone *zone)",
            "{",
            "\tunsigned long nr;",
            "",
            "\tnr = zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_FILE) +",
            "\t\tzone_page_state_snapshot(zone, NR_ZONE_ACTIVE_FILE);",
            "\tif (can_reclaim_anon_pages(NULL, zone_to_nid(zone), NULL))",
            "\t\tnr += zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_ANON) +",
            "\t\t\tzone_page_state_snapshot(zone, NR_ZONE_ACTIVE_ANON);",
            "\t/*",
            "\t * If there are no reclaimable file-backed or anonymous pages,",
            "\t * ensure zones with sufficient free pages are not skipped.",
            "\t * This prevents zones like DMA32 from being ignored in reclaim",
            "\t * scenarios where they can still help alleviate memory pressure.",
            "\t */",
            "\tif (nr == 0)",
            "\t\tnr = zone_page_state_snapshot(zone, NR_FREE_PAGES);",
            "\treturn nr;",
            "}",
            "static unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru,",
            "\t\t\t\t     int zone_idx)",
            "{",
            "\tunsigned long size = 0;",
            "\tint zid;",
            "",
            "\tfor (zid = 0; zid <= zone_idx; zid++) {",
            "\t\tstruct zone *zone = &lruvec_pgdat(lruvec)->node_zones[zid];",
            "",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!mem_cgroup_disabled())",
            "\t\t\tsize += mem_cgroup_get_zone_lru_size(lruvec, lru, zid);",
            "\t\telse",
            "\t\t\tsize += zone_page_state(zone, NR_ZONE_LRU_BASE + lru);",
            "\t}",
            "\treturn size;",
            "}",
            "static unsigned long drop_slab_node(int nid)",
            "{",
            "\tunsigned long freed = 0;",
            "\tstruct mem_cgroup *memcg = NULL;",
            "",
            "\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);",
            "\tdo {",
            "\t\tfreed += shrink_slab(GFP_KERNEL, nid, memcg, 0);",
            "\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)) != NULL);",
            "",
            "\treturn freed;",
            "}",
            "void drop_slab(void)",
            "{",
            "\tint nid;",
            "\tint shift = 0;",
            "\tunsigned long freed;",
            "",
            "\tdo {",
            "\t\tfreed = 0;",
            "\t\tfor_each_online_node(nid) {",
            "\t\t\tif (fatal_signal_pending(current))",
            "\t\t\t\treturn;",
            "",
            "\t\t\tfreed += drop_slab_node(nid);",
            "\t\t}",
            "\t} while ((freed >> shift++) > 1);",
            "}",
            "static int reclaimer_offset(void)",
            "{",
            "\tBUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=",
            "\t\t\tPGDEMOTE_DIRECT - PGDEMOTE_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=",
            "\t\t\tPGDEMOTE_KHUGEPAGED - PGDEMOTE_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=",
            "\t\t\tPGSCAN_DIRECT - PGSCAN_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=",
            "\t\t\tPGSCAN_KHUGEPAGED - PGSCAN_KSWAPD);",
            "",
            "\tif (current_is_kswapd())",
            "\t\treturn 0;",
            "\tif (current_is_khugepaged())",
            "\t\treturn PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD;",
            "\treturn PGSTEAL_DIRECT - PGSTEAL_KSWAPD;",
            "}",
            "static inline int is_page_cache_freeable(struct folio *folio)",
            "{",
            "\t/*",
            "\t * A freeable page cache folio is referenced only by the caller",
            "\t * that isolated the folio, the page cache and optional filesystem",
            "\t * private data at folio->private.",
            "\t */",
            "\treturn folio_ref_count(folio) - folio_test_private(folio) ==",
            "\t\t1 + folio_nr_pages(folio);",
            "}",
            "static void handle_write_error(struct address_space *mapping,",
            "\t\t\t\tstruct folio *folio, int error)",
            "{",
            "\tfolio_lock(folio);",
            "\tif (folio_mapping(folio) == mapping)",
            "\t\tmapping_set_error(mapping, error);",
            "\tfolio_unlock(folio);",
            "}"
          ],
          "function_name": "zone_reclaimable_pages, lruvec_lru_size, drop_slab_node, drop_slab, reclaimer_offset, is_page_cache_freeable, handle_write_error",
          "description": "实现区域可回收页数计算、Slab对象释放及回收偏移量调整逻辑，通过遍历各内存区域统计潜在可回收页数，提供Slab内存碎片回收机制并维护回收进程优先级偏移量。",
          "similarity": 0.605358362197876
        },
        {
          "chunk_id": 15,
          "file_path": "mm/vmscan.c",
          "start_line": 2960,
          "end_line": 3095,
          "content": [
            "static bool iterate_mm_list(struct lru_gen_mm_walk *walk, struct mm_struct **iter)",
            "{",
            "\tbool first = false;",
            "\tbool last = false;",
            "\tstruct mm_struct *mm = NULL;",
            "\tstruct lruvec *lruvec = walk->lruvec;",
            "\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);",
            "\tstruct lru_gen_mm_list *mm_list = get_mm_list(memcg);",
            "\tstruct lru_gen_mm_state *mm_state = get_mm_state(lruvec);",
            "",
            "\t/*",
            "\t * mm_state->seq is incremented after each iteration of mm_list. There",
            "\t * are three interesting cases for this page table walker:",
            "\t * 1. It tries to start a new iteration with a stale max_seq: there is",
            "\t *    nothing left to do.",
            "\t * 2. It started the next iteration: it needs to reset the Bloom filter",
            "\t *    so that a fresh set of PTE tables can be recorded.",
            "\t * 3. It ended the current iteration: it needs to reset the mm stats",
            "\t *    counters and tell its caller to increment max_seq.",
            "\t */",
            "\tspin_lock(&mm_list->lock);",
            "",
            "\tVM_WARN_ON_ONCE(mm_state->seq + 1 < walk->seq);",
            "",
            "\tif (walk->seq <= mm_state->seq)",
            "\t\tgoto done;",
            "",
            "\tif (!mm_state->head)",
            "\t\tmm_state->head = &mm_list->fifo;",
            "",
            "\tif (mm_state->head == &mm_list->fifo)",
            "\t\tfirst = true;",
            "",
            "\tdo {",
            "\t\tmm_state->head = mm_state->head->next;",
            "\t\tif (mm_state->head == &mm_list->fifo) {",
            "\t\t\tWRITE_ONCE(mm_state->seq, mm_state->seq + 1);",
            "\t\t\tlast = true;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t/* force scan for those added after the last iteration */",
            "\t\tif (!mm_state->tail || mm_state->tail == mm_state->head) {",
            "\t\t\tmm_state->tail = mm_state->head->next;",
            "\t\t\twalk->force_scan = true;",
            "\t\t}",
            "\t} while (!(mm = get_next_mm(walk)));",
            "done:",
            "\tif (*iter || last)",
            "\t\treset_mm_stats(walk, last);",
            "",
            "\tspin_unlock(&mm_list->lock);",
            "",
            "\tif (mm && first)",
            "\t\treset_bloom_filter(mm_state, walk->seq + 1);",
            "",
            "\tif (*iter)",
            "\t\tmmput_async(*iter);",
            "",
            "\t*iter = mm;",
            "",
            "\treturn last;",
            "}",
            "static bool iterate_mm_list_nowalk(struct lruvec *lruvec, unsigned long seq)",
            "{",
            "\tbool success = false;",
            "\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);",
            "\tstruct lru_gen_mm_list *mm_list = get_mm_list(memcg);",
            "\tstruct lru_gen_mm_state *mm_state = get_mm_state(lruvec);",
            "",
            "\tspin_lock(&mm_list->lock);",
            "",
            "\tVM_WARN_ON_ONCE(mm_state->seq + 1 < seq);",
            "",
            "\tif (seq > mm_state->seq) {",
            "\t\tmm_state->head = NULL;",
            "\t\tmm_state->tail = NULL;",
            "\t\tWRITE_ONCE(mm_state->seq, mm_state->seq + 1);",
            "\t\tsuccess = true;",
            "\t}",
            "",
            "\tspin_unlock(&mm_list->lock);",
            "",
            "\treturn success;",
            "}",
            "static void read_ctrl_pos(struct lruvec *lruvec, int type, int tier, int gain,",
            "\t\t\t  struct ctrl_pos *pos)",
            "{",
            "\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;",
            "\tint hist = lru_hist_from_seq(lrugen->min_seq[type]);",
            "",
            "\tpos->refaulted = lrugen->avg_refaulted[type][tier] +",
            "\t\t\t atomic_long_read(&lrugen->refaulted[hist][type][tier]);",
            "\tpos->total = lrugen->avg_total[type][tier] +",
            "\t\t     atomic_long_read(&lrugen->evicted[hist][type][tier]);",
            "\tif (tier)",
            "\t\tpos->total += lrugen->protected[hist][type][tier - 1];",
            "\tpos->gain = gain;",
            "}",
            "static void reset_ctrl_pos(struct lruvec *lruvec, int type, bool carryover)",
            "{",
            "\tint hist, tier;",
            "\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;",
            "\tbool clear = carryover ? NR_HIST_GENS == 1 : NR_HIST_GENS > 1;",
            "\tunsigned long seq = carryover ? lrugen->min_seq[type] : lrugen->max_seq + 1;",
            "",
            "\tlockdep_assert_held(&lruvec->lru_lock);",
            "",
            "\tif (!carryover && !clear)",
            "\t\treturn;",
            "",
            "\thist = lru_hist_from_seq(seq);",
            "",
            "\tfor (tier = 0; tier < MAX_NR_TIERS; tier++) {",
            "\t\tif (carryover) {",
            "\t\t\tunsigned long sum;",
            "",
            "\t\t\tsum = lrugen->avg_refaulted[type][tier] +",
            "\t\t\t      atomic_long_read(&lrugen->refaulted[hist][type][tier]);",
            "\t\t\tWRITE_ONCE(lrugen->avg_refaulted[type][tier], sum / 2);",
            "",
            "\t\t\tsum = lrugen->avg_total[type][tier] +",
            "\t\t\t      atomic_long_read(&lrugen->evicted[hist][type][tier]);",
            "\t\t\tif (tier)",
            "\t\t\t\tsum += lrugen->protected[hist][type][tier - 1];",
            "\t\t\tWRITE_ONCE(lrugen->avg_total[type][tier], sum / 2);",
            "\t\t}",
            "",
            "\t\tif (clear) {",
            "\t\t\tatomic_long_set(&lrugen->refaulted[hist][type][tier], 0);",
            "\t\t\tatomic_long_set(&lrugen->evicted[hist][type][tier], 0);",
            "\t\t\tif (tier)",
            "\t\t\t\tWRITE_ONCE(lrugen->protected[hist][type][tier - 1], 0);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "iterate_mm_list, iterate_mm_list_nowalk, read_ctrl_pos, reset_ctrl_pos",
          "description": "实现内存映射列表迭代逻辑，通过锁保护维护seq序号和头尾指针，用于跟踪页面表的变化并触发相应的统计重置或Bloom过滤器重置",
          "similarity": 0.5921086668968201
        },
        {
          "chunk_id": 39,
          "file_path": "mm/vmscan.c",
          "start_line": 6439,
          "end_line": 6556,
          "content": [
            "unsigned long try_to_free_pages(struct zonelist *zonelist, int order,",
            "\t\t\t\tgfp_t gfp_mask, nodemask_t *nodemask)",
            "{",
            "\tunsigned long nr_reclaimed;",
            "\tstruct scan_control sc = {",
            "\t\t.nr_to_reclaim = SWAP_CLUSTER_MAX,",
            "\t\t.gfp_mask = current_gfp_context(gfp_mask),",
            "\t\t.reclaim_idx = gfp_zone(gfp_mask),",
            "\t\t.order = order,",
            "\t\t.nodemask = nodemask,",
            "\t\t.priority = DEF_PRIORITY,",
            "\t\t.may_writepage = !laptop_mode,",
            "\t\t.may_unmap = 1,",
            "\t\t.may_swap = 1,",
            "\t};",
            "",
            "\t/*",
            "\t * scan_control uses s8 fields for order, priority, and reclaim_idx.",
            "\t * Confirm they are large enough for max values.",
            "\t */",
            "\tBUILD_BUG_ON(MAX_PAGE_ORDER >= S8_MAX);",
            "\tBUILD_BUG_ON(DEF_PRIORITY > S8_MAX);",
            "\tBUILD_BUG_ON(MAX_NR_ZONES > S8_MAX);",
            "",
            "\t/*",
            "\t * Do not enter reclaim if fatal signal was delivered while throttled.",
            "\t * 1 is returned so that the page allocator does not OOM kill at this",
            "\t * point.",
            "\t */",
            "\tif (throttle_direct_reclaim(sc.gfp_mask, zonelist, nodemask))",
            "\t\treturn 1;",
            "",
            "\tset_task_reclaim_state(current, &sc.reclaim_state);",
            "\ttrace_mm_vmscan_direct_reclaim_begin(order, sc.gfp_mask);",
            "",
            "\tnr_reclaimed = do_try_to_free_pages(zonelist, &sc);",
            "",
            "\ttrace_mm_vmscan_direct_reclaim_end(nr_reclaimed);",
            "\tset_task_reclaim_state(current, NULL);",
            "",
            "\treturn nr_reclaimed;",
            "}",
            "unsigned long mem_cgroup_shrink_node(struct mem_cgroup *memcg,",
            "\t\t\t\t\t\tgfp_t gfp_mask, bool noswap,",
            "\t\t\t\t\t\tpg_data_t *pgdat,",
            "\t\t\t\t\t\tunsigned long *nr_scanned)",
            "{",
            "\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "\tstruct scan_control sc = {",
            "\t\t.nr_to_reclaim = SWAP_CLUSTER_MAX,",
            "\t\t.target_mem_cgroup = memcg,",
            "\t\t.may_writepage = !laptop_mode,",
            "\t\t.may_unmap = 1,",
            "\t\t.reclaim_idx = MAX_NR_ZONES - 1,",
            "\t\t.may_swap = !noswap,",
            "\t};",
            "",
            "\tWARN_ON_ONCE(!current->reclaim_state);",
            "",
            "\tsc.gfp_mask = (gfp_mask & GFP_RECLAIM_MASK) |",
            "\t\t\t(GFP_HIGHUSER_MOVABLE & ~GFP_RECLAIM_MASK);",
            "",
            "\ttrace_mm_vmscan_memcg_softlimit_reclaim_begin(sc.order,",
            "\t\t\t\t\t\t      sc.gfp_mask);",
            "",
            "\t/*",
            "\t * NOTE: Although we can get the priority field, using it",
            "\t * here is not a good idea, since it limits the pages we can scan.",
            "\t * if we don't reclaim here, the shrink_node from balance_pgdat",
            "\t * will pick up pages from other mem cgroup's as well. We hack",
            "\t * the priority and make it zero.",
            "\t */",
            "\tshrink_lruvec(lruvec, &sc);",
            "",
            "\ttrace_mm_vmscan_memcg_softlimit_reclaim_end(sc.nr_reclaimed);",
            "",
            "\t*nr_scanned = sc.nr_scanned;",
            "",
            "\treturn sc.nr_reclaimed;",
            "}",
            "unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,",
            "\t\t\t\t\t   unsigned long nr_pages,",
            "\t\t\t\t\t   gfp_t gfp_mask,",
            "\t\t\t\t\t   unsigned int reclaim_options)",
            "{",
            "\tunsigned long nr_reclaimed;",
            "\tunsigned int noreclaim_flag;",
            "\tstruct scan_control sc = {",
            "\t\t.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),",
            "\t\t.gfp_mask = (current_gfp_context(gfp_mask) & GFP_RECLAIM_MASK) |",
            "\t\t\t\t(GFP_HIGHUSER_MOVABLE & ~GFP_RECLAIM_MASK),",
            "\t\t.reclaim_idx = MAX_NR_ZONES - 1,",
            "\t\t.target_mem_cgroup = memcg,",
            "\t\t.priority = DEF_PRIORITY,",
            "\t\t.may_writepage = !laptop_mode,",
            "\t\t.may_unmap = 1,",
            "\t\t.may_swap = !!(reclaim_options & MEMCG_RECLAIM_MAY_SWAP),",
            "\t\t.proactive = !!(reclaim_options & MEMCG_RECLAIM_PROACTIVE),",
            "\t};",
            "\t/*",
            "\t * Traverse the ZONELIST_FALLBACK zonelist of the current node to put",
            "\t * equal pressure on all the nodes. This is based on the assumption that",
            "\t * the reclaim does not bail out early.",
            "\t */",
            "\tstruct zonelist *zonelist = node_zonelist(numa_node_id(), sc.gfp_mask);",
            "",
            "\tset_task_reclaim_state(current, &sc.reclaim_state);",
            "\ttrace_mm_vmscan_memcg_reclaim_begin(0, sc.gfp_mask);",
            "\tnoreclaim_flag = memalloc_noreclaim_save();",
            "",
            "\tnr_reclaimed = do_try_to_free_pages(zonelist, &sc);",
            "",
            "\tmemalloc_noreclaim_restore(noreclaim_flag);",
            "\ttrace_mm_vmscan_memcg_reclaim_end(nr_reclaimed);",
            "\tset_task_reclaim_state(current, NULL);",
            "",
            "\treturn nr_reclaimed;",
            "}"
          ],
          "function_name": "try_to_free_pages, mem_cgroup_shrink_node, try_to_free_mem_cgroup_pages",
          "description": "try_to_free_pages启动全局内存回收流程，mem_cgroup_shrink_node专用于特定内存组的页面回收，try_to_free_mem_cgroup_pages处理带参数的内存组回收，均通过do_try_to_free_pages实现核心回收逻辑。",
          "similarity": 0.5762529969215393
        }
      ]
    }
  ]
}