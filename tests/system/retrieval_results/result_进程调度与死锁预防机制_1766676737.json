{
  "query": "进程调度与死锁预防机制",
  "timestamp": "2025-12-25 23:32:17",
  "retrieved_files": [
    {
      "source_file": "kernel/watchdog.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:51:17\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `watchdog.c`\n\n---\n\n# watchdog.c 技术文档\n\n## 1. 文件概述\n\n`watchdog.c` 是 Linux 内核中实现 **硬锁死（hard lockup）** 和 **软锁死（soft lockup）** 检测机制的核心文件。该机制用于监控系统中 CPU 是否因长时间禁用中断或陷入无限循环而无法响应，从而帮助诊断系统挂死问题。硬锁死指 CPU 完全停止响应中断（包括 NMI），软锁死指内核线程长时间占用 CPU 且未调度其他任务。本文件主要聚焦于硬锁死检测的通用框架和部分实现，软锁死检测逻辑主要在其他文件（如 `softlockup.c`）中实现，但两者共享部分配置和控制逻辑。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `watchdog_enabled`：位掩码，表示当前启用的 watchdog 类型（软/硬锁死检测）。\n- `watchdog_user_enabled`：用户空间是否启用 watchdog（默认 1）。\n- `watchdog_hardlockup_user_enabled`：用户空间是否启用硬锁死检测（默认值取决于架构）。\n- `watchdog_softlockup_user_enabled`：用户空间是否启用软锁死检测（默认 1）。\n- `watchdog_thresh`：锁死检测阈值（秒，默认 10 秒）。\n- `watchdog_cpumask`：参与 watchdog 检测的 CPU 掩码。\n- `hardlockup_panic`：硬锁死发生时是否触发内核 panic（默认由 `CONFIG_BOOTPARAM_HARDLOCKUP_PANIC` 决定）。\n- `sysctl_hardlockup_all_cpu_backtrace`（SMP）：硬锁死时是否打印所有 CPU 的 backtrace。\n- `hardlockup_count`（SYSFS）：记录硬锁死事件发生次数。\n\n### 主要函数\n- `hardlockup_detector_disable(void)`：在启动早期禁用硬锁死检测（例如虚拟机环境）。\n- `hardlockup_panic_setup(char *str)`：解析内核启动参数 `nmi_watchdog=`，配置硬锁死行为。\n- `arch_touch_nmi_watchdog(void)`：架构相关函数，用于在关键路径“触摸”硬 watchdog，防止误报（导出符号）。\n- `watchdog_hardlockup_touch_cpu(unsigned int cpu)`：标记指定 CPU 已被“触摸”。\n- `is_hardlockup(unsigned int cpu)`：检查指定 CPU 是否发生硬锁死（基于高精度定时器中断计数）。\n- `watchdog_hardlockup_kick(void)`：在高精度定时器中断中“踢”硬 watchdog（更新中断计数）。\n- `watchdog_hardlockup_check(unsigned int cpu, struct pt_regs *regs)`：执行硬锁死检测逻辑，打印诊断信息并可能触发 panic。\n- `watchdog_hardlockup_enable/disable(unsigned int cpu)`：弱符号函数，由具体硬 watchdog 实现（如 perf-based）覆盖，用于启停 per-CPU 检测。\n- `watchdog_hardlockup_probe(void)`：弱符号函数，由具体实现提供，用于探测硬 watchdog 硬件/机制是否可用。\n\n### 核心数据结构（Per-CPU）\n- `hrtimer_interrupts`：高精度定时器中断计数器（原子变量）。\n- `hrtimer_interrupts_saved`：上次保存的中断计数值。\n- `watchdog_hardlockup_warned`：是否已为该 CPU 打印过硬锁死警告。\n- `watchdog_hardlockup_touched`：该 CPU 是否被“触摸”过（用于豁免检测）。\n\n## 3. 关键实现\n\n### 硬锁死检测机制（基于高精度定时器）\n当配置 `CONFIG_HARDLOCKUP_DETECTOR_COUNTS_HRTIMER` 时，硬锁死检测通过监控 **高精度定时器（hrtimer）中断** 的发生频率实现：\n1. **计数更新**：每次 hrtimer 中断发生时，调用 `watchdog_hardlockup_kick()` 原子递增 per-CPU 计数器 `hrtimer_interrupts`。\n2. **检测逻辑**：在 NMI（不可屏蔽中断）上下文（或其他检测点）调用 `watchdog_hardlockup_check()`：\n   - 若 CPU 被“触摸”（`watchdog_hardlockup_touched` 为真），则清除此标记并跳过检测。\n   - 否则调用 `is_hardlockup()`：比较当前 `hrtimer_interrupts` 与上次保存值 `hrtimer_interrupts_saved`。若相等，说明在检测周期内无 hrtimer 中断，判定为硬锁死。\n3. **告警与处理**：\n   - 首次检测到硬锁死时，打印紧急日志（CPU 信息、模块列表、中断跟踪、寄存器状态或栈回溯）。\n   - 若启用 `sysctl_hardlockup_all_cpu_backtrace`，触发其他 CPU 的 backtrace。\n   - 若 `hardlockup_panic` 为真，调用 `nmi_panic()` 触发内核 panic。\n   - 设置 `watchdog_hardlockup_warned` 避免重复告警。\n\n### 启动参数与配置\n- **`nmi_watchdog=` 参数**：通过 `__setup` 宏注册，支持以下值：\n  - `panic`/`nopanic`：设置 `hardlockup_panic`。\n  - `0`/`1`：启用/禁用硬锁死检测。\n  - `r...`：传递参数给 perf-based 检测器（`hardlockup_config_perf_event`）。\n- **早期禁用**：`hardlockup_detector_disable()` 可在解析命令行前禁用硬检测（如 KVM guest）。\n\n### 架构交互与豁免\n- **`arch_touch_nmi_watchdog()`**：允许架构代码或关键内核路径（如 printk）临时豁免硬 watchdog 检测，防止在已知安全的长操作中误报。使用 `raw_cpu_write` 确保在抢占/中断使能环境下安全。\n\n### 弱符号扩展点\n- `watchdog_hardlockup_enable/disable/probe` 声明为 `__weak`，允许不同架构或检测方法（如基于 perf event 的 NMI watchdog）提供具体实现，实现检测机制的可插拔。\n\n## 4. 依赖关系\n\n- **内核子系统**：\n  - `<linux/nmi.h>`：NMI 处理框架，硬锁死检测通常在 NMI 上下文触发。\n  - `<linux/hrtimer.h>`（隐含）：高精度定时器中断作为检测心跳源。\n  - `<linux/sched/*.h>`：调度器相关（`print_irqtrace_events`, `dump_stack`）。\n  - `<linux/sysctl.h>`：提供 `sysctl_hardlockup_all_cpu_backtrace` 控制接口。\n  - `<linux/sysfs.h>`：暴露 `hardlockup_count` 到 sysfs。\n  - `<asm/irq_regs.h>`：获取中断上下文寄存器状态（`show_regs`）。\n- **配置选项**：\n  - `CONFIG_HARDLOCKUP_DETECTOR`：启用硬锁死检测框架。\n  - `CONFIG_HARDLOCKUP_DETECTOR_COUNTS_HRTIMER`：使用 hrtimer 中断计数实现检测。\n  - `CONFIG_HARDLOCKUP_DETECTOR_SPARC64`：SPARC64 架构默认启用硬检测。\n  - `CONFIG_BOOTPARAM_HARDLOCKUP_PANIC`：设置默认 panic 行为。\n  - `CONFIG_SMP`：多核支持（`all_cpu_backtrace` 功能）。\n  - `CONFIG_SYSFS`：sysfs 接口支持。\n- **其他模块**：依赖具体架构的 NMI 实现（如 x86 的 perf-based NMI watchdog）提供检测触发点。\n\n## 5. 使用场景\n\n- **系统稳定性监控**：在生产服务器或嵌入式设备中持续监控 CPU 响应性，及时发现硬件故障、驱动 bug 或内核死锁导致的系统挂死。\n- **内核调试**：开发人员通过 watchdog 触发的 backtrace 和寄存器转储，定位导致系统无响应的代码路径。\n- **虚拟化环境**：在 hypervisor guest 中可选择性禁用硬 watchdog（因虚拟化开销可能导致误报），通过 `hardlockup_detector_disable()` 或启动参数控制。\n- **实时系统**：结合 CPU 隔离（`isolcpus`）和 watchdog 配置，确保关键 CPU 核心的响应性，同时避免在非关键核上产生干扰。\n- **panic 策略**：通过 `hardlockup_panic` 配置，使系统在硬锁死时自动重启，提高无人值守系统的可用性。",
      "similarity": 0.6140403747558594,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/watchdog.c",
          "start_line": 73,
          "end_line": 217,
          "content": [
            "static ssize_t hardlockup_count_show(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\t\t     char *page)",
            "{",
            "\treturn sysfs_emit(page, \"%u\\n\", hardlockup_count);",
            "}",
            "static __init int kernel_hardlockup_sysfs_init(void)",
            "{",
            "\tsysfs_add_file_to_group(kernel_kobj, &hardlockup_count_attr.attr, NULL);",
            "\treturn 0;",
            "}",
            "void __init hardlockup_detector_disable(void)",
            "{",
            "\twatchdog_hardlockup_user_enabled = 0;",
            "}",
            "static int __init hardlockup_panic_setup(char *str)",
            "{",
            "next:",
            "\tif (!strncmp(str, \"panic\", 5))",
            "\t\thardlockup_panic = 1;",
            "\telse if (!strncmp(str, \"nopanic\", 7))",
            "\t\thardlockup_panic = 0;",
            "\telse if (!strncmp(str, \"0\", 1))",
            "\t\twatchdog_hardlockup_user_enabled = 0;",
            "\telse if (!strncmp(str, \"1\", 1))",
            "\t\twatchdog_hardlockup_user_enabled = 1;",
            "\telse if (!strncmp(str, \"r\", 1))",
            "\t\thardlockup_config_perf_event(str + 1);",
            "\twhile (*(str++)) {",
            "\t\tif (*str == ',') {",
            "\t\t\tstr++;",
            "\t\t\tgoto next;",
            "\t\t}",
            "\t}",
            "\treturn 1;",
            "}",
            "notrace void arch_touch_nmi_watchdog(void)",
            "{",
            "\t/*",
            "\t * Using __raw here because some code paths have",
            "\t * preemption enabled.  If preemption is enabled",
            "\t * then interrupts should be enabled too, in which",
            "\t * case we shouldn't have to worry about the watchdog",
            "\t * going off.",
            "\t */",
            "\traw_cpu_write(watchdog_hardlockup_touched, true);",
            "}",
            "void watchdog_hardlockup_touch_cpu(unsigned int cpu)",
            "{",
            "\tper_cpu(watchdog_hardlockup_touched, cpu) = true;",
            "}",
            "static bool is_hardlockup(unsigned int cpu)",
            "{",
            "\tint hrint = atomic_read(&per_cpu(hrtimer_interrupts, cpu));",
            "",
            "\tif (per_cpu(hrtimer_interrupts_saved, cpu) == hrint)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * NOTE: we don't need any fancy atomic_t or READ_ONCE/WRITE_ONCE",
            "\t * for hrtimer_interrupts_saved. hrtimer_interrupts_saved is",
            "\t * written/read by a single CPU.",
            "\t */",
            "\tper_cpu(hrtimer_interrupts_saved, cpu) = hrint;",
            "",
            "\treturn false;",
            "}",
            "static void watchdog_hardlockup_kick(void)",
            "{",
            "\tint new_interrupts;",
            "",
            "\tnew_interrupts = atomic_inc_return(this_cpu_ptr(&hrtimer_interrupts));",
            "\twatchdog_buddy_check_hardlockup(new_interrupts);",
            "}",
            "void watchdog_hardlockup_check(unsigned int cpu, struct pt_regs *regs)",
            "{",
            "\tif (per_cpu(watchdog_hardlockup_touched, cpu)) {",
            "\t\tper_cpu(watchdog_hardlockup_touched, cpu) = false;",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Check for a hardlockup by making sure the CPU's timer",
            "\t * interrupt is incrementing. The timer interrupt should have",
            "\t * fired multiple times before we overflow'd. If it hasn't",
            "\t * then this is a good indication the cpu is stuck",
            "\t */",
            "\tif (is_hardlockup(cpu)) {",
            "\t\tunsigned int this_cpu = smp_processor_id();",
            "\t\tunsigned long flags;",
            "",
            "#ifdef CONFIG_SYSFS",
            "\t\t++hardlockup_count;",
            "#endif",
            "",
            "\t\t/* Only print hardlockups once. */",
            "\t\tif (per_cpu(watchdog_hardlockup_warned, cpu))",
            "\t\t\treturn;",
            "",
            "\t\t/*",
            "\t\t * Prevent multiple hard-lockup reports if one cpu is already",
            "\t\t * engaged in dumping all cpu back traces.",
            "\t\t */",
            "\t\tif (sysctl_hardlockup_all_cpu_backtrace) {",
            "\t\t\tif (test_and_set_bit_lock(0, &hard_lockup_nmi_warn))",
            "\t\t\t\treturn;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * NOTE: we call printk_cpu_sync_get_irqsave() after printing",
            "\t\t * the lockup message. While it would be nice to serialize",
            "\t\t * that printout, we really want to make sure that if some",
            "\t\t * other CPU somehow locked up while holding the lock associated",
            "\t\t * with printk_cpu_sync_get_irqsave() that we can still at least",
            "\t\t * get the message about the lockup out.",
            "\t\t */",
            "\t\tpr_emerg(\"CPU%u: Watchdog detected hard LOCKUP on cpu %u\\n\", this_cpu, cpu);",
            "\t\tprintk_cpu_sync_get_irqsave(flags);",
            "",
            "\t\tprint_modules();",
            "\t\tprint_irqtrace_events(current);",
            "\t\tif (cpu == this_cpu) {",
            "\t\t\tif (regs)",
            "\t\t\t\tshow_regs(regs);",
            "\t\t\telse",
            "\t\t\t\tdump_stack();",
            "\t\t\tprintk_cpu_sync_put_irqrestore(flags);",
            "\t\t} else {",
            "\t\t\tprintk_cpu_sync_put_irqrestore(flags);",
            "\t\t\ttrigger_single_cpu_backtrace(cpu);",
            "\t\t}",
            "",
            "\t\tif (sysctl_hardlockup_all_cpu_backtrace) {",
            "\t\t\ttrigger_allbutcpu_cpu_backtrace(cpu);",
            "\t\t\tif (!hardlockup_panic)",
            "\t\t\t\tclear_bit_unlock(0, &hard_lockup_nmi_warn);",
            "\t\t}",
            "",
            "\t\tif (hardlockup_panic)",
            "\t\t\tnmi_panic(regs, \"Hard LOCKUP\");",
            "",
            "\t\tper_cpu(watchdog_hardlockup_warned, cpu) = true;",
            "\t} else {",
            "\t\tper_cpu(watchdog_hardlockup_warned, cpu) = false;",
            "\t}",
            "}"
          ],
          "function_name": "hardlockup_count_show, kernel_hardlockup_sysfs_init, hardlockup_detector_disable, hardlockup_panic_setup, arch_touch_nmi_watchdog, watchdog_hardlockup_touch_cpu, is_hardlockup, watchdog_hardlockup_kick, watchdog_hardlockup_check",
          "description": "实现硬锁检测核心逻辑，包含硬锁判断、计数统计、NMI触发电路及异常上报功能，通过中断计数器检测CPU卡顿。",
          "similarity": 0.5869745016098022
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/watchdog.c",
          "start_line": 626,
          "end_line": 802,
          "content": [
            "static void update_touch_ts(void)",
            "{",
            "\t__this_cpu_write(watchdog_touch_ts, get_timestamp());",
            "\tupdate_report_ts();",
            "}",
            "notrace void touch_softlockup_watchdog_sched(void)",
            "{",
            "\t/*",
            "\t * Preemption can be enabled.  It doesn't matter which CPU's watchdog",
            "\t * report period gets restarted here, so use the raw_ operation.",
            "\t */",
            "\traw_cpu_write(watchdog_report_ts, SOFTLOCKUP_DELAY_REPORT);",
            "}",
            "notrace void touch_softlockup_watchdog(void)",
            "{",
            "\ttouch_softlockup_watchdog_sched();",
            "\twq_watchdog_touch(raw_smp_processor_id());",
            "}",
            "void touch_all_softlockup_watchdogs(void)",
            "{",
            "\tint cpu;",
            "",
            "\t/*",
            "\t * watchdog_mutex cannpt be taken here, as this might be called",
            "\t * from (soft)interrupt context, so the access to",
            "\t * watchdog_allowed_cpumask might race with a concurrent update.",
            "\t *",
            "\t * The watchdog time stamp can race against a concurrent real",
            "\t * update as well, the only side effect might be a cycle delay for",
            "\t * the softlockup check.",
            "\t */",
            "\tfor_each_cpu(cpu, &watchdog_allowed_mask) {",
            "\t\tper_cpu(watchdog_report_ts, cpu) = SOFTLOCKUP_DELAY_REPORT;",
            "\t\twq_watchdog_touch(cpu);",
            "\t}",
            "}",
            "void touch_softlockup_watchdog_sync(void)",
            "{",
            "\t__this_cpu_write(softlockup_touch_sync, true);",
            "\t__this_cpu_write(watchdog_report_ts, SOFTLOCKUP_DELAY_REPORT);",
            "}",
            "static int is_softlockup(unsigned long touch_ts,",
            "\t\t\t unsigned long period_ts,",
            "\t\t\t unsigned long now)",
            "{",
            "\tif ((watchdog_enabled & WATCHDOG_SOFTOCKUP_ENABLED) && watchdog_thresh) {",
            "\t\t/*",
            "\t\t * If period_ts has not been updated during a sample_period, then",
            "\t\t * in the subsequent few sample_periods, period_ts might also not",
            "\t\t * be updated, which could indicate a potential softlockup. In",
            "\t\t * this case, if we suspect the cause of the potential softlockup",
            "\t\t * might be interrupt storm, then we need to count the interrupts",
            "\t\t * to find which interrupt is storming.",
            "\t\t */",
            "\t\tif (time_after_eq(now, period_ts + get_softlockup_thresh() / NUM_SAMPLE_PERIODS) &&",
            "\t\t    need_counting_irqs())",
            "\t\t\tstart_counting_irqs();",
            "",
            "\t\t/* Warn about unreasonable delays. */",
            "\t\tif (time_after(now, period_ts + get_softlockup_thresh()))",
            "\t\t\treturn now - touch_ts;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int softlockup_fn(void *data)",
            "{",
            "\tupdate_touch_ts();",
            "\tstop_counting_irqs();",
            "\tcomplete(this_cpu_ptr(&softlockup_completion));",
            "",
            "\treturn 0;",
            "}",
            "static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)",
            "{",
            "\tunsigned long touch_ts, period_ts, now;",
            "\tstruct pt_regs *regs = get_irq_regs();",
            "\tint duration;",
            "\tint softlockup_all_cpu_backtrace = sysctl_softlockup_all_cpu_backtrace;",
            "\tunsigned long flags;",
            "",
            "\tif (!watchdog_enabled)",
            "\t\treturn HRTIMER_NORESTART;",
            "",
            "\twatchdog_hardlockup_kick();",
            "",
            "\t/* kick the softlockup detector */",
            "\tif (completion_done(this_cpu_ptr(&softlockup_completion))) {",
            "\t\treinit_completion(this_cpu_ptr(&softlockup_completion));",
            "\t\tstop_one_cpu_nowait(smp_processor_id(),",
            "\t\t\t\tsoftlockup_fn, NULL,",
            "\t\t\t\tthis_cpu_ptr(&softlockup_stop_work));",
            "\t}",
            "",
            "\t/* .. and repeat */",
            "\thrtimer_forward_now(hrtimer, ns_to_ktime(sample_period));",
            "",
            "\t/*",
            "\t * Read the current timestamp first. It might become invalid anytime",
            "\t * when a virtual machine is stopped by the host or when the watchog",
            "\t * is touched from NMI.",
            "\t */",
            "\tnow = get_timestamp();",
            "\t/*",
            "\t * If a virtual machine is stopped by the host it can look to",
            "\t * the watchdog like a soft lockup. This function touches the watchdog.",
            "\t */",
            "\tkvm_check_and_clear_guest_paused();",
            "\t/*",
            "\t * The stored timestamp is comparable with @now only when not touched.",
            "\t * It might get touched anytime from NMI. Make sure that is_softlockup()",
            "\t * uses the same (valid) value.",
            "\t */",
            "\tperiod_ts = READ_ONCE(*this_cpu_ptr(&watchdog_report_ts));",
            "",
            "\tupdate_cpustat();",
            "",
            "\t/* Reset the interval when touched by known problematic code. */",
            "\tif (period_ts == SOFTLOCKUP_DELAY_REPORT) {",
            "\t\tif (unlikely(__this_cpu_read(softlockup_touch_sync))) {",
            "\t\t\t/*",
            "\t\t\t * If the time stamp was touched atomically",
            "\t\t\t * make sure the scheduler tick is up to date.",
            "\t\t\t */",
            "\t\t\t__this_cpu_write(softlockup_touch_sync, false);",
            "\t\t\tsched_clock_tick();",
            "\t\t}",
            "",
            "\t\tupdate_report_ts();",
            "\t\treturn HRTIMER_RESTART;",
            "\t}",
            "",
            "\t/* Check for a softlockup. */",
            "\ttouch_ts = __this_cpu_read(watchdog_touch_ts);",
            "\tduration = is_softlockup(touch_ts, period_ts, now);",
            "\tif (unlikely(duration)) {",
            "#ifdef CONFIG_SYSFS",
            "\t\t++softlockup_count;",
            "#endif",
            "",
            "\t\t/*",
            "\t\t * Prevent multiple soft-lockup reports if one cpu is already",
            "\t\t * engaged in dumping all cpu back traces.",
            "\t\t */",
            "\t\tif (softlockup_all_cpu_backtrace) {",
            "\t\t\tif (test_and_set_bit_lock(0, &soft_lockup_nmi_warn))",
            "\t\t\t\treturn HRTIMER_RESTART;",
            "\t\t}",
            "",
            "\t\t/* Start period for the next softlockup warning. */",
            "\t\tupdate_report_ts();",
            "",
            "\t\tprintk_cpu_sync_get_irqsave(flags);",
            "\t\tpr_emerg(\"BUG: soft lockup - CPU#%d stuck for %us! [%s:%d]\\n\",",
            "\t\t\tsmp_processor_id(), duration,",
            "\t\t\tcurrent->comm, task_pid_nr(current));",
            "\t\treport_cpu_status();",
            "\t\tprint_modules();",
            "\t\tprint_irqtrace_events(current);",
            "\t\tif (regs)",
            "\t\t\tshow_regs(regs);",
            "\t\telse",
            "\t\t\tdump_stack();",
            "\t\tprintk_cpu_sync_put_irqrestore(flags);",
            "",
            "\t\tif (softlockup_all_cpu_backtrace) {",
            "\t\t\ttrigger_allbutcpu_cpu_backtrace(smp_processor_id());",
            "\t\t\tif (!softlockup_panic)",
            "\t\t\t\tclear_bit_unlock(0, &soft_lockup_nmi_warn);",
            "\t\t}",
            "",
            "\t\tadd_taint(TAINT_SOFTLOCKUP, LOCKDEP_STILL_OK);",
            "\t\tif (softlockup_panic)",
            "\t\t\tpanic(\"softlockup: hung tasks\");",
            "\t}",
            "",
            "\treturn HRTIMER_RESTART;",
            "}"
          ],
          "function_name": "update_touch_ts, touch_softlockup_watchdog_sched, touch_softlockup_watchdog, touch_all_softlockup_watchdogs, touch_softlockup_watchdog_sync, is_softlockup, softlockup_fn, watchdog_timer_fn",
          "description": "处理软锁检测时序逻辑，包含超时判定算法、任务栈回溯触发机制及异常处理流程，协调硬件定时器与软件检测模块。",
          "similarity": 0.5739574432373047
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/watchdog.c",
          "start_line": 1114,
          "end_line": 1190,
          "content": [
            "int proc_watchdog_cpumask(struct ctl_table *table, int write,",
            "\t\t\t  void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint err;",
            "",
            "\tmutex_lock(&watchdog_mutex);",
            "",
            "\terr = proc_do_large_bitmap(table, write, buffer, lenp, ppos);",
            "\tif (!err && write)",
            "\t\tproc_watchdog_update(false);",
            "",
            "\tmutex_unlock(&watchdog_mutex);",
            "\treturn err;",
            "}",
            "static void __init watchdog_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", watchdog_sysctls);",
            "",
            "\tif (watchdog_hardlockup_available)",
            "\t\twatchdog_hardlockup_sysctl[0].mode = 0644;",
            "\tregister_sysctl_init(\"kernel\", watchdog_hardlockup_sysctl);",
            "}",
            "static void __init lockup_detector_delay_init(struct work_struct *work)",
            "{",
            "\tint ret;",
            "",
            "\tret = watchdog_hardlockup_probe();",
            "\tif (ret) {",
            "\t\tif (ret == -ENODEV)",
            "\t\t\tpr_info(\"NMI not fully supported\\n\");",
            "\t\telse",
            "\t\t\tpr_info(\"Delayed init of the lockup detector failed: %d\\n\", ret);",
            "\t\tpr_info(\"Hard watchdog permanently disabled\\n\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tallow_lockup_detector_init_retry = false;",
            "",
            "\twatchdog_hardlockup_available = true;",
            "\tlockup_detector_setup();",
            "}",
            "void __init lockup_detector_retry_init(void)",
            "{",
            "\t/* Must be called before late init calls */",
            "\tif (!allow_lockup_detector_init_retry)",
            "\t\treturn;",
            "",
            "\tschedule_work(&detector_work);",
            "}",
            "static int __init lockup_detector_check(void)",
            "{",
            "\t/* Prevent any later retry. */",
            "\tallow_lockup_detector_init_retry = false;",
            "",
            "\t/* Make sure no work is pending. */",
            "\tflush_work(&detector_work);",
            "",
            "\twatchdog_sysctl_init();",
            "",
            "\treturn 0;",
            "",
            "}",
            "void __init lockup_detector_init(void)",
            "{",
            "\tif (tick_nohz_full_enabled())",
            "\t\tpr_info(\"Disabling watchdog on nohz_full cores by default\\n\");",
            "",
            "\tcpumask_copy(&watchdog_cpumask,",
            "\t\t     housekeeping_cpumask(HK_TYPE_TIMER));",
            "",
            "\tif (!watchdog_hardlockup_probe())",
            "\t\twatchdog_hardlockup_available = true;",
            "\telse",
            "\t\tallow_lockup_detector_init_retry = true;",
            "",
            "\tlockup_detector_setup();",
            "}"
          ],
          "function_name": "proc_watchdog_cpumask, watchdog_sysctl_init, lockup_detector_delay_init, lockup_detector_retry_init, lockup_detector_check, lockup_detector_init",
          "description": "实现看门狗子系统的初始化流程，包含sysctl参数注册、延迟初始化工作队列、CPU掩码配置及探测器状态同步机制",
          "similarity": 0.5655516386032104
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/watchdog.c",
          "start_line": 946,
          "end_line": 1059,
          "content": [
            "void lockup_detector_reconfigure(void)",
            "{",
            "\tmutex_lock(&watchdog_mutex);",
            "\t__lockup_detector_reconfigure(false);",
            "\tmutex_unlock(&watchdog_mutex);",
            "}",
            "static __init void lockup_detector_setup(void)",
            "{",
            "\t/*",
            "\t * If sysctl is off and watchdog got disabled on the command line,",
            "\t * nothing to do here.",
            "\t */",
            "\tlockup_detector_update_enable();",
            "",
            "\tif (!IS_ENABLED(CONFIG_SYSCTL) &&",
            "\t    !(watchdog_enabled && watchdog_thresh))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&watchdog_mutex);",
            "\t__lockup_detector_reconfigure(false);",
            "\tsoftlockup_initialized = true;",
            "\tmutex_unlock(&watchdog_mutex);",
            "}",
            "static void __lockup_detector_reconfigure(bool thresh_changed)",
            "{",
            "\tcpus_read_lock();",
            "\twatchdog_hardlockup_stop();",
            "\tif (thresh_changed)",
            "\t\twatchdog_thresh = READ_ONCE(watchdog_thresh_next);",
            "\tlockup_detector_update_enable();",
            "\twatchdog_hardlockup_start();",
            "\tcpus_read_unlock();",
            "}",
            "void lockup_detector_reconfigure(void)",
            "{",
            "\t__lockup_detector_reconfigure(false);",
            "}",
            "static inline void lockup_detector_setup(void)",
            "{",
            "\t__lockup_detector_reconfigure(false);",
            "}",
            "void lockup_detector_soft_poweroff(void)",
            "{",
            "\twatchdog_enabled = 0;",
            "}",
            "static void proc_watchdog_update(bool thresh_changed)",
            "{",
            "\t/* Remove impossible cpus to keep sysctl output clean. */",
            "\tcpumask_and(&watchdog_cpumask, &watchdog_cpumask, cpu_possible_mask);",
            "\t__lockup_detector_reconfigure(thresh_changed);",
            "}",
            "static int proc_watchdog_common(int which, struct ctl_table *table, int write,",
            "\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint err, old, *param = table->data;",
            "",
            "\tmutex_lock(&watchdog_mutex);",
            "",
            "\told = *param;",
            "\tif (!write) {",
            "\t\t/*",
            "\t\t * On read synchronize the userspace interface. This is a",
            "\t\t * racy snapshot.",
            "\t\t */",
            "\t\t*param = (watchdog_enabled & which) != 0;",
            "\t\terr = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\t\t*param = old;",
            "\t} else {",
            "\t\terr = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\t\tif (!err && old != READ_ONCE(*param))",
            "\t\t\tproc_watchdog_update(false);",
            "\t}",
            "\tmutex_unlock(&watchdog_mutex);",
            "\treturn err;",
            "}",
            "int proc_watchdog(struct ctl_table *table, int write,",
            "\t\t  void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\treturn proc_watchdog_common(WATCHDOG_HARDLOCKUP_ENABLED |",
            "\t\t\t\t    WATCHDOG_SOFTOCKUP_ENABLED,",
            "\t\t\t\t    table, write, buffer, lenp, ppos);",
            "}",
            "int proc_nmi_watchdog(struct ctl_table *table, int write,",
            "\t\t      void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tif (!watchdog_hardlockup_available && write)",
            "\t\treturn -ENOTSUPP;",
            "\treturn proc_watchdog_common(WATCHDOG_HARDLOCKUP_ENABLED,",
            "\t\t\t\t    table, write, buffer, lenp, ppos);",
            "}",
            "int proc_soft_watchdog(struct ctl_table *table, int write,",
            "\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\treturn proc_watchdog_common(WATCHDOG_SOFTOCKUP_ENABLED,",
            "\t\t\t\t    table, write, buffer, lenp, ppos);",
            "}",
            "int proc_watchdog_thresh(struct ctl_table *table, int write,",
            "\t\t\t void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint err, old;",
            "",
            "\tmutex_lock(&watchdog_mutex);",
            "",
            "\twatchdog_thresh_next = READ_ONCE(watchdog_thresh);",
            "",
            "\told = watchdog_thresh_next;",
            "\terr = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "",
            "\tif (!err && write && old != READ_ONCE(watchdog_thresh_next))",
            "\t\tproc_watchdog_update(true);",
            "",
            "\tmutex_unlock(&watchdog_mutex);",
            "\treturn err;",
            "}"
          ],
          "function_name": "lockup_detector_reconfigure, lockup_detector_setup, __lockup_detector_reconfigure, lockup_detector_reconfigure, lockup_detector_setup, lockup_detector_soft_poweroff, proc_watchdog_update, proc_watchdog_common, proc_watchdog, proc_nmi_watchdog, proc_soft_watchdog, proc_watchdog_thresh",
          "description": "提供看门狗参数动态配置接口，包含阈值更新、CPU掩码同步、sysctl参数读写控制逻辑，支持硬/软锁步检测模式切换和阈值调节",
          "similarity": 0.5446680784225464
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/watchdog.c",
          "start_line": 257,
          "end_line": 357,
          "content": [
            "static inline void watchdog_hardlockup_kick(void) { }",
            "void __weak watchdog_hardlockup_enable(unsigned int cpu) { }",
            "void __weak watchdog_hardlockup_disable(unsigned int cpu) { }",
            "int __weak __init watchdog_hardlockup_probe(void)",
            "{",
            "\treturn -ENODEV;",
            "}",
            "void __weak watchdog_hardlockup_stop(void) { }",
            "void __weak watchdog_hardlockup_start(void) { }",
            "static void lockup_detector_update_enable(void)",
            "{",
            "\twatchdog_enabled = 0;",
            "\tif (!watchdog_user_enabled)",
            "\t\treturn;",
            "\tif (watchdog_hardlockup_available && watchdog_hardlockup_user_enabled)",
            "\t\twatchdog_enabled |= WATCHDOG_HARDLOCKUP_ENABLED;",
            "\tif (watchdog_softlockup_user_enabled)",
            "\t\twatchdog_enabled |= WATCHDOG_SOFTOCKUP_ENABLED;",
            "}",
            "static ssize_t softlockup_count_show(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\t\t     char *page)",
            "{",
            "\treturn sysfs_emit(page, \"%u\\n\", softlockup_count);",
            "}",
            "static __init int kernel_softlockup_sysfs_init(void)",
            "{",
            "\tsysfs_add_file_to_group(kernel_kobj, &softlockup_count_attr.attr, NULL);",
            "\treturn 0;",
            "}",
            "static int __init softlockup_panic_setup(char *str)",
            "{",
            "\tsoftlockup_panic = simple_strtoul(str, NULL, 0);",
            "\treturn 1;",
            "}",
            "static int __init nowatchdog_setup(char *str)",
            "{",
            "\twatchdog_user_enabled = 0;",
            "\treturn 1;",
            "}",
            "static int __init nosoftlockup_setup(char *str)",
            "{",
            "\twatchdog_softlockup_user_enabled = 0;",
            "\treturn 1;",
            "}",
            "static int __init watchdog_thresh_setup(char *str)",
            "{",
            "\tget_option(&str, &watchdog_thresh);",
            "\treturn 1;",
            "}",
            "static u16 get_16bit_precision(u64 data_ns)",
            "{",
            "\treturn data_ns >> 24LL; /* 2^24ns ~= 16.8ms */",
            "}",
            "static void update_cpustat(void)",
            "{",
            "\tint i;",
            "\tu8 util;",
            "\tu16 old_stat, new_stat;",
            "\tstruct kernel_cpustat kcpustat;",
            "\tu64 *cpustat = kcpustat.cpustat;",
            "\tu8 tail = __this_cpu_read(cpustat_tail);",
            "\tu16 sample_period_16 = get_16bit_precision(sample_period);",
            "",
            "\tkcpustat_cpu_fetch(&kcpustat, smp_processor_id());",
            "",
            "\tfor (i = 0; i < NUM_STATS_PER_GROUP; i++) {",
            "\t\told_stat = __this_cpu_read(cpustat_old[i]);",
            "\t\tnew_stat = get_16bit_precision(cpustat[tracked_stats[i]]);",
            "\t\tutil = DIV_ROUND_UP(100 * (new_stat - old_stat), sample_period_16);",
            "\t\t__this_cpu_write(cpustat_util[tail][i], util);",
            "\t\t__this_cpu_write(cpustat_old[i], new_stat);",
            "\t}",
            "",
            "\t__this_cpu_write(cpustat_tail, (tail + 1) % NUM_SAMPLE_PERIODS);",
            "}",
            "static void print_cpustat(void)",
            "{",
            "\tint i, group;",
            "\tu8 tail = __this_cpu_read(cpustat_tail);",
            "\tu64 sample_period_second = sample_period;",
            "",
            "\tdo_div(sample_period_second, NSEC_PER_SEC);",
            "",
            "\t/*",
            "\t * Outputting the \"watchdog\" prefix on every line is redundant and not",
            "\t * concise, and the original alarm information is sufficient for",
            "\t * positioning in logs, hence here printk() is used instead of pr_crit().",
            "\t */",
            "\tprintk(KERN_CRIT \"CPU#%d Utilization every %llus during lockup:\\n\",",
            "\t       smp_processor_id(), sample_period_second);",
            "",
            "\tfor (i = 0; i < NUM_SAMPLE_PERIODS; i++) {",
            "\t\tgroup = (tail + i) % NUM_SAMPLE_PERIODS;",
            "\t\tprintk(KERN_CRIT \"\\t#%d: %3u%% system,\\t%3u%% softirq,\\t\"",
            "\t\t\t\"%3u%% hardirq,\\t%3u%% idle\\n\", i + 1,",
            "\t\t\t__this_cpu_read(cpustat_util[group][STATS_SYSTEM]),",
            "\t\t\t__this_cpu_read(cpustat_util[group][STATS_SOFTIRQ]),",
            "\t\t\t__this_cpu_read(cpustat_util[group][STATS_HARDIRQ]),",
            "\t\t\t__this_cpu_read(cpustat_util[group][STATS_IDLE]));",
            "\t}",
            "}"
          ],
          "function_name": "watchdog_hardlockup_kick, watchdog_hardlockup_enable, watchdog_hardlockup_disable, watchdog_hardlockup_probe, watchdog_hardlockup_stop, watchdog_hardlockup_start, lockup_detector_update_enable, softlockup_count_show, kernel_softlockup_sysfs_init, softlockup_panic_setup, nowatchdog_setup, nosoftlockup_setup, watchdog_thresh_setup, get_16bit_precision, update_cpustat, print_cpustat",
          "description": "提供软锁检测支持，包含统计周期设置、CPU利用率采集、中断事件追踪等辅助功能，维护软锁检测相关状态机。",
          "similarity": 0.5428853631019592
        }
      ]
    },
    {
      "source_file": "kernel/sched/core_sched.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:00:47\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\core_sched.c`\n\n---\n\n# `sched/core_sched.c` 技术文档\n\n## 1. 文件概述\n\n`sched/core_sched.c` 是 Linux 内核调度器中用于实现 **核心调度（Core Scheduling）** 功能的核心文件之一。核心调度是一种安全机制，旨在防止来自不同安全上下文的任务在同一个物理 CPU 核心（特别是超线程/SMT 共享核心）上并发执行，从而缓解侧信道攻击（如 Spectre、MDS 等）。\n\n该文件主要负责管理任务的 **调度 cookie**（`core_cookie`），通过引用计数的 cookie 对象将具有相同安全上下文的任务分组，确保只有拥有相同 cookie 的任务才能在同一个 CPU 核心上并发运行。\n\n## 2. 核心功能\n\n### 数据结构\n\n- **`struct sched_core_cookie`**  \n  表示一个调度 cookie，仅包含一个引用计数器 `refcnt`。其内存地址本身即作为 cookie 值使用。\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `sched_core_alloc_cookie()` | 分配一个新的 `sched_core_cookie` 对象，初始化引用计数为 1，并启用核心调度全局状态。返回 cookie 地址（转换为 `unsigned long`）。 |\n| `sched_core_put_cookie(unsigned long cookie)` | 释放 cookie 引用；若引用计数归零，则释放内存并关闭核心调度全局状态。 |\n| `sched_core_get_cookie(unsigned long cookie)` | 增加 cookie 引用计数，返回原 cookie 值。 |\n| `sched_core_update_cookie(struct task_struct *p, unsigned long cookie)` | 原子地更新任务 `p` 的 `core_cookie`，处理任务在运行队列中的入队/出队，并在必要时触发重调度。 |\n| `sched_core_clone_cookie(struct task_struct *p)` | 安全地复制任务 `p` 的当前 cookie（带锁保护），用于 fork 或共享操作。 |\n| `sched_core_fork(struct task_struct *p)` | 在 `fork()` 时初始化子任务的核心调度状态，继承父进程的 cookie。 |\n| `sched_core_free(struct task_struct *p)` | 在任务退出时释放其持有的 cookie 引用。 |\n| `__sched_core_set(struct task_struct *p, unsigned long cookie)` | 设置任务 `p` 的 cookie，自动处理引用计数的获取与释放。 |\n| `sched_core_share_pid(...)` | 用户空间通过 `prctl(PR_SCHED_CORE, ...)` 调用的核心接口，支持创建、查询、共享 cookie。 |\n| `__sched_core_account_forceidle(struct rq *rq)` | （仅当 `CONFIG_SCHEDSTATS` 启用）统计核心强制空闲（force-idle）时间，并分摊到相关任务。 |\n| `__sched_core_tick(struct rq *rq)` | 在调度 tick 中调用，用于更新强制空闲时间统计。 |\n\n## 3. 关键实现\n\n### Cookie 生命周期管理\n- Cookie 通过 `kmalloc` 动态分配，其地址作为唯一标识。\n- 使用 `refcount_t` 实现线程安全的引用计数。\n- `sched_core_get()` / `sched_core_put()` 控制全局核心调度使能状态。\n\n### 任务 Cookie 更新\n- 在 `task_rq_lock()` 保护下更新 `p->core_cookie`，确保调度器一致性。\n- 若任务已在运行队列中，先出队再根据新 cookie 决定是否重新入队。\n- 若任务正在 CPU 上运行，调用 `resched_curr()` 触发重调度，以确保新 cookie 策略立即生效。\n\n### 安全访问控制\n- 通过 `ptrace_may_access()` 检查调用者是否有权限操作目标进程的 cookie。\n- 仅当系统存在 SMT（超线程）时（`sched_smt_present` 为真），才允许使用核心调度功能。\n\n### prctl 接口支持\n- 支持四种命令：\n  - `PR_SCHED_CORE_CREATE`：创建新 cookie。\n  - `PR_SCHED_CORE_SHARE_TO`：将当前进程的 cookie 应用于目标进程（或进程组）。\n  - `PR_SCHED_CORE_SHARE_FROM`：将目标进程的 cookie 应用于当前进程。\n  - `PR_SCHED_CORE_GET`：获取目标进程的 cookie 哈希值（用于用户空间识别）。\n- 支持作用域：线程（`PIDTYPE_PID`）、线程组（`PIDTYPE_TGID`）、进程组（`PIDTYPE_PGID`）。\n\n### 强制空闲时间统计（`CONFIG_SCHEDSTATS`）\n- 当核心因 cookie 不兼容而进入强制空闲状态时，记录空闲时间。\n- 时间按 `core_forceidle_count / core_forceidle_occupation` 比例分摊到所有相关 CPU 上的非 idle 任务。\n- 通过 `__account_forceidle_time()` 更新任务的调度统计信息。\n\n## 4. 依赖关系\n\n- **调度器核心**：依赖 `kernel/sched/` 下的通用调度器基础设施，如 `task_rq_lock()`、`resched_curr()`、`rq` 结构等。\n- **SMT 检测**：依赖 `sched_smt_present` 静态分支判断系统是否支持超线程。\n- **内存管理**：使用 `kmalloc`/`kfree` 进行动态内存分配。\n- **进程管理**：依赖 `find_task_by_vpid()`、`tasklist_lock`、`do_each_pid_thread` 等进程遍历机制。\n- **安全机制**：依赖 `ptrace_may_access()` 进行权限检查。\n- **调度统计**：`__sched_core_account_forceidle` 依赖 `CONFIG_SCHEDSTATS` 和 `__account_forceidle_time`。\n\n## 5. 使用场景\n\n- **安全敏感应用**：如浏览器、虚拟机监控器（VMM）、加密服务等，需防止跨任务的侧信道攻击。\n- **用户空间控制**：通过 `prctl(PR_SCHED_CORE, ...)` 接口，应用程序可显式创建和共享调度 cookie，将信任的任务分组。\n- **进程 fork 行为**：子进程自动继承父进程的 cookie，确保同源任务保持调度兼容性。\n- **系统资源隔离**：在多租户或容器环境中，确保不同租户的任务不会在同一个物理核心上并发执行。\n- **性能调优与监控**：通过 `CONFIG_SCHEDSTATS` 收集核心强制空闲开销，评估安全策略对性能的影响。",
      "similarity": 0.6058579087257385,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/core_sched.c",
          "start_line": 11,
          "end_line": 216,
          "content": [
            "static unsigned long sched_core_alloc_cookie(void)",
            "{",
            "\tstruct sched_core_cookie *ck = kmalloc(sizeof(*ck), GFP_KERNEL);",
            "\tif (!ck)",
            "\t\treturn 0;",
            "",
            "\trefcount_set(&ck->refcnt, 1);",
            "\tsched_core_get();",
            "",
            "\treturn (unsigned long)ck;",
            "}",
            "static void sched_core_put_cookie(unsigned long cookie)",
            "{",
            "\tstruct sched_core_cookie *ptr = (void *)cookie;",
            "",
            "\tif (ptr && refcount_dec_and_test(&ptr->refcnt)) {",
            "\t\tkfree(ptr);",
            "\t\tsched_core_put();",
            "\t}",
            "}",
            "static unsigned long sched_core_get_cookie(unsigned long cookie)",
            "{",
            "\tstruct sched_core_cookie *ptr = (void *)cookie;",
            "",
            "\tif (ptr)",
            "\t\trefcount_inc(&ptr->refcnt);",
            "",
            "\treturn cookie;",
            "}",
            "static unsigned long sched_core_update_cookie(struct task_struct *p,",
            "\t\t\t\t\t      unsigned long cookie)",
            "{",
            "\tunsigned long old_cookie;",
            "\tstruct rq_flags rf;",
            "\tstruct rq *rq;",
            "",
            "\trq = task_rq_lock(p, &rf);",
            "",
            "\t/*",
            "\t * Since creating a cookie implies sched_core_get(), and we cannot set",
            "\t * a cookie until after we've created it, similarly, we cannot destroy",
            "\t * a cookie until after we've removed it, we must have core scheduling",
            "\t * enabled here.",
            "\t */",
            "\tSCHED_WARN_ON((p->core_cookie || cookie) && !sched_core_enabled(rq));",
            "",
            "\tif (sched_core_enqueued(p))",
            "\t\tsched_core_dequeue(rq, p, DEQUEUE_SAVE);",
            "",
            "\told_cookie = p->core_cookie;",
            "\tp->core_cookie = cookie;",
            "",
            "\t/*",
            "\t * Consider the cases: !prev_cookie and !cookie.",
            "\t */",
            "\tif (cookie && task_on_rq_queued(p))",
            "\t\tsched_core_enqueue(rq, p);",
            "",
            "\t/*",
            "\t * If task is currently running, it may not be compatible anymore after",
            "\t * the cookie change, so enter the scheduler on its CPU to schedule it",
            "\t * away.",
            "\t *",
            "\t * Note that it is possible that as a result of this cookie change, the",
            "\t * core has now entered/left forced idle state. Defer accounting to the",
            "\t * next scheduling edge, rather than always forcing a reschedule here.",
            "\t */",
            "\tif (task_on_cpu(rq, p))",
            "\t\tresched_curr(rq);",
            "",
            "\ttask_rq_unlock(rq, p, &rf);",
            "",
            "\treturn old_cookie;",
            "}",
            "static unsigned long sched_core_clone_cookie(struct task_struct *p)",
            "{",
            "\tunsigned long cookie, flags;",
            "",
            "\traw_spin_lock_irqsave(&p->pi_lock, flags);",
            "\tcookie = sched_core_get_cookie(p->core_cookie);",
            "\traw_spin_unlock_irqrestore(&p->pi_lock, flags);",
            "",
            "\treturn cookie;",
            "}",
            "void sched_core_fork(struct task_struct *p)",
            "{",
            "\tRB_CLEAR_NODE(&p->core_node);",
            "\tp->core_cookie = sched_core_clone_cookie(current);",
            "}",
            "void sched_core_free(struct task_struct *p)",
            "{",
            "\tsched_core_put_cookie(p->core_cookie);",
            "}",
            "static void __sched_core_set(struct task_struct *p, unsigned long cookie)",
            "{",
            "\tcookie = sched_core_get_cookie(cookie);",
            "\tcookie = sched_core_update_cookie(p, cookie);",
            "\tsched_core_put_cookie(cookie);",
            "}",
            "int sched_core_share_pid(unsigned int cmd, pid_t pid, enum pid_type type,",
            "\t\t\t unsigned long uaddr)",
            "{",
            "\tunsigned long cookie = 0, id = 0;",
            "\tstruct task_struct *task, *p;",
            "\tstruct pid *grp;",
            "\tint err = 0;",
            "",
            "\tif (!static_branch_likely(&sched_smt_present))",
            "\t\treturn -ENODEV;",
            "",
            "\tBUILD_BUG_ON(PR_SCHED_CORE_SCOPE_THREAD != PIDTYPE_PID);",
            "\tBUILD_BUG_ON(PR_SCHED_CORE_SCOPE_THREAD_GROUP != PIDTYPE_TGID);",
            "\tBUILD_BUG_ON(PR_SCHED_CORE_SCOPE_PROCESS_GROUP != PIDTYPE_PGID);",
            "",
            "\tif (type > PIDTYPE_PGID || cmd >= PR_SCHED_CORE_MAX || pid < 0 ||",
            "\t    (cmd != PR_SCHED_CORE_GET && uaddr))",
            "\t\treturn -EINVAL;",
            "",
            "\trcu_read_lock();",
            "\tif (pid == 0) {",
            "\t\ttask = current;",
            "\t} else {",
            "\t\ttask = find_task_by_vpid(pid);",
            "\t\tif (!task) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\treturn -ESRCH;",
            "\t\t}",
            "\t}",
            "\tget_task_struct(task);",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * Check if this process has the right to modify the specified",
            "\t * process. Use the regular \"ptrace_may_access()\" checks.",
            "\t */",
            "\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {",
            "\t\terr = -EPERM;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tswitch (cmd) {",
            "\tcase PR_SCHED_CORE_GET:",
            "\t\tif (type != PIDTYPE_PID || uaddr & 7) {",
            "\t\t\terr = -EINVAL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tcookie = sched_core_clone_cookie(task);",
            "\t\tif (cookie) {",
            "\t\t\t/* XXX improve ? */",
            "\t\t\tptr_to_hashval((void *)cookie, &id);",
            "\t\t}",
            "\t\terr = put_user(id, (u64 __user *)uaddr);",
            "\t\tgoto out;",
            "",
            "\tcase PR_SCHED_CORE_CREATE:",
            "\t\tcookie = sched_core_alloc_cookie();",
            "\t\tif (!cookie) {",
            "\t\t\terr = -ENOMEM;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tbreak;",
            "",
            "\tcase PR_SCHED_CORE_SHARE_TO:",
            "\t\tcookie = sched_core_clone_cookie(current);",
            "\t\tbreak;",
            "",
            "\tcase PR_SCHED_CORE_SHARE_FROM:",
            "\t\tif (type != PIDTYPE_PID) {",
            "\t\t\terr = -EINVAL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tcookie = sched_core_clone_cookie(task);",
            "\t\t__sched_core_set(current, cookie);",
            "\t\tgoto out;",
            "",
            "\tdefault:",
            "\t\terr = -EINVAL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tif (type == PIDTYPE_PID) {",
            "\t\t__sched_core_set(task, cookie);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tread_lock(&tasklist_lock);",
            "\tgrp = task_pid_type(task, type);",
            "",
            "\tdo_each_pid_thread(grp, type, p) {",
            "\t\tif (!ptrace_may_access(p, PTRACE_MODE_READ_REALCREDS)) {",
            "\t\t\terr = -EPERM;",
            "\t\t\tgoto out_tasklist;",
            "\t\t}",
            "\t} while_each_pid_thread(grp, type, p);",
            "",
            "\tdo_each_pid_thread(grp, type, p) {",
            "\t\t__sched_core_set(p, cookie);",
            "\t} while_each_pid_thread(grp, type, p);",
            "out_tasklist:",
            "\tread_unlock(&tasklist_lock);",
            "",
            "out:",
            "\tsched_core_put_cookie(cookie);",
            "\tput_task_struct(task);",
            "\treturn err;",
            "}"
          ],
          "function_name": "sched_core_alloc_cookie, sched_core_put_cookie, sched_core_get_cookie, sched_core_update_cookie, sched_core_clone_cookie, sched_core_fork, sched_core_free, __sched_core_set, sched_core_share_pid",
          "description": "实现了核心调度 cookie 的分配、释放、获取和更新机制，包含 cookie 分配/回收、任务核心绑定变更、进程克隆共享及核心调度策略控制等功能",
          "similarity": 0.5965111255645752
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/core_sched.c",
          "start_line": 240,
          "end_line": 297,
          "content": [
            "void __sched_core_account_forceidle(struct rq *rq)",
            "{",
            "\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu_of(rq));",
            "\tu64 delta, now = rq_clock(rq->core);",
            "\tstruct rq *rq_i;",
            "\tstruct task_struct *p;",
            "\tint i;",
            "",
            "\tlockdep_assert_rq_held(rq);",
            "",
            "\tWARN_ON_ONCE(!rq->core->core_forceidle_count);",
            "",
            "\tif (rq->core->core_forceidle_start == 0)",
            "\t\treturn;",
            "",
            "\tdelta = now - rq->core->core_forceidle_start;",
            "\tif (unlikely((s64)delta <= 0))",
            "\t\treturn;",
            "",
            "\trq->core->core_forceidle_start = now;",
            "",
            "\tif (WARN_ON_ONCE(!rq->core->core_forceidle_occupation)) {",
            "\t\t/* can't be forced idle without a running task */",
            "\t} else if (rq->core->core_forceidle_count > 1 ||",
            "\t\t   rq->core->core_forceidle_occupation > 1) {",
            "\t\t/*",
            "\t\t * For larger SMT configurations, we need to scale the charged",
            "\t\t * forced idle amount since there can be more than one forced",
            "\t\t * idle sibling and more than one running cookied task.",
            "\t\t */",
            "\t\tdelta *= rq->core->core_forceidle_count;",
            "\t\tdelta = div_u64(delta, rq->core->core_forceidle_occupation);",
            "\t}",
            "",
            "\tfor_each_cpu(i, smt_mask) {",
            "\t\trq_i = cpu_rq(i);",
            "\t\tp = rq_i->core_pick ?: rq_i->curr;",
            "",
            "\t\tif (p == rq_i->idle)",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * Note: this will account forceidle to the current cpu, even",
            "\t\t * if it comes from our SMT sibling.",
            "\t\t */",
            "\t\t__account_forceidle_time(p, delta);",
            "\t}",
            "}",
            "void __sched_core_tick(struct rq *rq)",
            "{",
            "\tif (!rq->core->core_forceidle_count)",
            "\t\treturn;",
            "",
            "\tif (rq != rq->core)",
            "\t\tupdate_rq_clock(rq->core);",
            "",
            "\t__sched_core_account_forceidle(rq);",
            "}"
          ],
          "function_name": "__sched_core_account_forceidle, __sched_core_tick",
          "description": "提供强制空闲时间统计功能，通过遍历 SMT 核心计算并分摊强制空闲时间消耗，tick 中断触发强制空闲会计入逻辑",
          "similarity": 0.4905795156955719
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/core_sched.c",
          "start_line": 1,
          "end_line": 10,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "",
            "/*",
            " * A simple wrapper around refcount. An allocated sched_core_cookie's",
            " * address is used to compute the cookie of the task.",
            " */",
            "struct sched_core_cookie {",
            "\trefcount_t refcnt;",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义了 sched_core_cookie 结构体，用于核心调度系统中管理任务的 cookie 引用计数，通过结构体地址计算 cookie 值",
          "similarity": 0.44755303859710693
        }
      ]
    },
    {
      "source_file": "kernel/sched/deadline.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:06:40\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\deadline.c`\n\n---\n\n# `sched/deadline.c` 技术文档\n\n## 1. 文件概述\n\n`sched/deadline.c` 是 Linux 内核调度器中 **SCHED_DEADLINE** 调度类的核心实现文件。该调度类基于 **最早截止时间优先（Earliest Deadline First, EDF）** 算法，并结合 **恒定带宽服务器（Constant Bandwidth Server, CBS）** 机制，为具有严格实时性要求的任务提供可预测的调度保障。\n\n其核心目标是：  \n- 对于周期性任务，若其实际运行时间不超过所申请的运行时间（runtime），则保证不会错过任何截止时间（deadline）；  \n- 对于非周期性任务、突发任务或试图超出其预留带宽的任务，系统会对其进行节流（throttling），防止其影响其他任务的实时性保障。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_dl_entity`：表示一个 deadline 调度实体，包含任务的运行时间（runtime）、截止期限（deadline）、周期（period）、带宽（dl_bw）等关键参数。\n- `struct dl_rq`：每个 CPU 的 deadline 运行队列，维护该 CPU 上所有 deadline 任务的红黑树、当前带宽使用情况（`this_bw`、`running_bw`）等。\n- `struct dl_bw`：deadline 带宽管理结构，用于跟踪系统或调度域中已分配的总带宽（`total_bw`）。\n\n### 主要函数与辅助宏\n\n#### 调度实体与运行队列关联\n- `dl_task_of(dl_se)`：从 `sched_dl_entity` 获取对应的 `task_struct`（仅适用于普通任务，不适用于服务器实体）。\n- `rq_of_dl_rq(dl_rq)` / `rq_of_dl_se(dl_se)`：获取与 deadline 运行队列或调度实体关联的 `rq`（runqueue）。\n- `dl_rq_of_se(dl_se)`：获取调度实体所属的 `dl_rq`。\n- `on_dl_rq(dl_se)`：判断调度实体是否已在 deadline 运行队列中（通过红黑树节点是否为空判断）。\n\n#### 优先级继承（PI）支持（`CONFIG_RT_MUTEXES`）\n- `pi_of(dl_se)`：获取当前调度实体因优先级继承而提升后的“代理”实体。\n- `is_dl_boosted(dl_se)`：判断该 deadline 实体是否因优先级继承被提升。\n\n#### 带宽管理（SMP 与 UP 差异处理）\n- `dl_bw_of(cpu)`：获取指定 CPU 所属调度域（或本地）的 `dl_bw` 结构。\n- `dl_bw_cpus(cpu)`：返回该 CPU 所在调度域中活跃 CPU 的数量。\n- `dl_bw_capacity(cpu)`：计算调度域的总 CPU 容量（考虑异构 CPU 的 `arch_scale_cpu_capacity`）。\n- `__dl_add()` / `__dl_sub()`：向带宽池中添加或移除任务带宽，并更新 `extra_bw`（用于负载均衡）。\n- `__dl_overflow()`：检查新增带宽是否超出系统/调度域的可用带宽上限。\n\n#### 运行时带宽跟踪\n- `__add_running_bw()` / `__sub_running_bw()`：更新 `dl_rq->running_bw`（当前正在运行的 deadline 任务所消耗的带宽）。\n- `__add_rq_bw()` / `__sub_rq_bw()`：更新 `dl_rq->this_bw`（该运行队列上所有 deadline 任务的总预留带宽）。\n- `add_running_bw()` / `sub_running_bw()` / `add_rq_bw()` / `sub_rq_bw()`：带宽操作的封装，跳过“特殊”调度实体（如服务器）。\n\n#### 其他\n- `dl_server(dl_se)`：判断调度实体是否为 CBS 服务器（而非普通任务）。\n- `dl_bw_visited(cpu, gen)`：用于带宽遍历去重（SMP 场景）。\n\n### 系统控制接口（`CONFIG_SYSCTL`）\n- `sched_deadline_period_max_us`：deadline 任务周期上限（默认 ~4 秒）。\n- `sched_deadline_period_min_us`：deadline 任务周期下限（默认 100 微秒），防止定时器 DoS。\n\n## 3. 关键实现\n\n### EDF + CBS 调度模型\n- 每个 deadline 任务通过 `runtime`、`deadline`、`period` 三个参数定义其资源需求。\n- 调度器按 **绝对截止时间（absolute deadline）** 对任务排序，使用红黑树实现 O(log n) 的调度决策。\n- CBS 机制确保任务即使突发执行，也不会长期占用超过其 `runtime/period` 的 CPU 带宽，超限任务会被 throttled。\n\n### 带宽隔离与全局限制\n- 在 SMP 系统中，deadline 带宽按 **调度域（root domain）** 进行管理，防止跨 CPU 的带宽滥用。\n- 总带宽限制默认为 CPU 总容量的 95%（由 `sysctl_sched_util_clamp_min` 等机制间接控制，具体限制逻辑在带宽分配函数中体现）。\n- `dl_bw->total_bw` 跟踪已分配带宽，`__dl_overflow()` 用于在任务加入时检查是否超限。\n\n### 异构 CPU 支持\n- 通过 `arch_scale_cpu_capacity()` 获取每个 CPU 的相对性能权重。\n- `dl_bw_capacity()` 在异构系统中返回调度域内所有活跃 CPU 的容量总和，用于带宽比例计算（`cap_scale()`）。\n\n### 与 cpufreq 集成\n- 每次 `running_bw` 变化时调用 `cpufreq_update_util()`，通知 CPU 频率调节器当前 deadline 负载，确保满足实时性能需求。\n\n### 优先级继承（PI）\n- 当 deadline 任务因持有 mutex 而阻塞高优先级任务时，通过 `pi_se` 字段临时提升其调度参数，避免优先级反转。\n\n## 4. 依赖关系\n\n- **核心调度框架**：依赖 `kernel/sched/sched.h` 中定义的通用调度结构（如 `rq`、`task_struct`）和宏（如 `SCHED_CAPACITY_SCALE`）。\n- **CPU 拓扑与容量**：依赖 `arch_scale_cpu_capacity()`（由各架构实现）获取 CPU 性能信息。\n- **RCU 机制**：在 SMP 路径中大量使用 `rcu_read_lock_sched_held()` 进行锁依赖检查。\n- **cpufreq 子系统**：通过 `cpufreq_update_util()` 与 CPU 频率调节器交互。\n- **实时互斥锁**：`CONFIG_RT_MUTEXES` 启用时，支持 deadline 任务的优先级继承。\n- **Sysctl 接口**：`CONFIG_SYSCTL` 启用时，提供用户空间可调的 deadline 参数。\n\n## 5. 使用场景\n\n- **工业实时控制**：如机器人控制、数控机床等需要严格周期性和低延迟响应的场景。\n- **音视频处理**：专业音视频采集、编码、播放等对 jitter 敏感的应用。\n- **电信基础设施**：5G 基站、核心网网元中的高优先级信令处理。\n- **汽车电子**：ADAS、自动驾驶系统中的关键任务调度。\n- **科研与高性能计算**：需要确定性执行时间的实验或仿真任务。\n\n用户通过 `sched_setattr(2)` 系统调用设置任务的 `SCHED_DEADLINE` 策略及对应的 `runtime`、`deadline`、`period` 参数，内核则通过本文件实现的调度逻辑确保其满足实时性约束。",
      "similarity": 0.595037043094635,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 514,
          "end_line": 616,
          "content": [
            "static inline int is_leftmost(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\treturn rb_first_cached(&dl_rq->root) == &dl_se->rb_node;",
            "}",
            "void init_dl_bw(struct dl_bw *dl_b)",
            "{",
            "\traw_spin_lock_init(&dl_b->lock);",
            "\tif (global_rt_runtime() == RUNTIME_INF)",
            "\t\tdl_b->bw = -1;",
            "\telse",
            "\t\tdl_b->bw = to_ratio(global_rt_period(), global_rt_runtime());",
            "\tdl_b->total_bw = 0;",
            "}",
            "void init_dl_rq(struct dl_rq *dl_rq)",
            "{",
            "\tdl_rq->root = RB_ROOT_CACHED;",
            "",
            "#ifdef CONFIG_SMP",
            "\t/* zero means no -deadline tasks */",
            "\tdl_rq->earliest_dl.curr = dl_rq->earliest_dl.next = 0;",
            "",
            "\tdl_rq->overloaded = 0;",
            "\tdl_rq->pushable_dl_tasks_root = RB_ROOT_CACHED;",
            "#else",
            "\tinit_dl_bw(&dl_rq->dl_bw);",
            "#endif",
            "",
            "\tdl_rq->running_bw = 0;",
            "\tdl_rq->this_bw = 0;",
            "\tinit_dl_rq_bw_ratio(dl_rq);",
            "}",
            "static inline int dl_overloaded(struct rq *rq)",
            "{",
            "\treturn atomic_read(&rq->rd->dlo_count);",
            "}",
            "static inline void dl_set_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tcpumask_set_cpu(rq->cpu, rq->rd->dlo_mask);",
            "\t/*",
            "\t * Must be visible before the overload count is",
            "\t * set (as in sched_rt.c).",
            "\t *",
            "\t * Matched by the barrier in pull_dl_task().",
            "\t */",
            "\tsmp_wmb();",
            "\tatomic_inc(&rq->rd->dlo_count);",
            "}",
            "static inline void dl_clear_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tatomic_dec(&rq->rd->dlo_count);",
            "\tcpumask_clear_cpu(rq->cpu, rq->rd->dlo_mask);",
            "}",
            "static inline bool __pushable_less(struct rb_node *a, const struct rb_node *b)",
            "{",
            "\treturn dl_entity_preempt(&__node_2_pdl(a)->dl, &__node_2_pdl(b)->dl);",
            "}",
            "static inline int has_pushable_dl_tasks(struct rq *rq)",
            "{",
            "\treturn !RB_EMPTY_ROOT(&rq->dl.pushable_dl_tasks_root.rb_root);",
            "}",
            "static void enqueue_pushable_dl_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tstruct rb_node *leftmost;",
            "",
            "\tWARN_ON_ONCE(!RB_EMPTY_NODE(&p->pushable_dl_tasks));",
            "",
            "\tleftmost = rb_add_cached(&p->pushable_dl_tasks,",
            "\t\t\t\t &rq->dl.pushable_dl_tasks_root,",
            "\t\t\t\t __pushable_less);",
            "\tif (leftmost)",
            "\t\trq->dl.earliest_dl.next = p->dl.deadline;",
            "",
            "\tif (!rq->dl.overloaded) {",
            "\t\tdl_set_overload(rq);",
            "\t\trq->dl.overloaded = 1;",
            "\t}",
            "}",
            "static void dequeue_pushable_dl_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tstruct dl_rq *dl_rq = &rq->dl;",
            "\tstruct rb_root_cached *root = &dl_rq->pushable_dl_tasks_root;",
            "\tstruct rb_node *leftmost;",
            "",
            "\tif (RB_EMPTY_NODE(&p->pushable_dl_tasks))",
            "\t\treturn;",
            "",
            "\tleftmost = rb_erase_cached(&p->pushable_dl_tasks, root);",
            "\tif (leftmost)",
            "\t\tdl_rq->earliest_dl.next = __node_2_pdl(leftmost)->dl.deadline;",
            "",
            "\tRB_CLEAR_NODE(&p->pushable_dl_tasks);",
            "",
            "\tif (!has_pushable_dl_tasks(rq) && rq->dl.overloaded) {",
            "\t\tdl_clear_overload(rq);",
            "\t\trq->dl.overloaded = 0;",
            "\t}",
            "}"
          ],
          "function_name": "is_leftmost, init_dl_bw, init_dl_rq, dl_overloaded, dl_set_overload, dl_clear_overload, __pushable_less, has_pushable_dl_tasks, enqueue_pushable_dl_task, dequeue_pushable_dl_task",
          "description": "实现截止时间调度的抢占判定和过载管理机制，包含任务优先级比较、过载标记维护及可推送任务的数据结构操作。",
          "similarity": 0.6152465343475342
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2104,
          "end_line": 2251,
          "content": [
            "static void enqueue_task_dl(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tif (is_dl_boosted(&p->dl)) {",
            "\t\t/*",
            "\t\t * Because of delays in the detection of the overrun of a",
            "\t\t * thread's runtime, it might be the case that a thread",
            "\t\t * goes to sleep in a rt mutex with negative runtime. As",
            "\t\t * a consequence, the thread will be throttled.",
            "\t\t *",
            "\t\t * While waiting for the mutex, this thread can also be",
            "\t\t * boosted via PI, resulting in a thread that is throttled",
            "\t\t * and boosted at the same time.",
            "\t\t *",
            "\t\t * In this case, the boost overrides the throttle.",
            "\t\t */",
            "\t\tif (p->dl.dl_throttled) {",
            "\t\t\t/*",
            "\t\t\t * The replenish timer needs to be canceled. No",
            "\t\t\t * problem if it fires concurrently: boosted threads",
            "\t\t\t * are ignored in dl_task_timer().",
            "\t\t\t *",
            "\t\t\t * If the timer callback was running (hrtimer_try_to_cancel == -1),",
            "\t\t\t * it will eventually call put_task_struct().",
            "\t\t\t */",
            "\t\t\tif (hrtimer_try_to_cancel(&p->dl.dl_timer) == 1 &&",
            "\t\t\t    !dl_server(&p->dl))",
            "\t\t\t\tput_task_struct(p);",
            "\t\t\tp->dl.dl_throttled = 0;",
            "\t\t}",
            "\t} else if (!dl_prio(p->normal_prio)) {",
            "\t\t/*",
            "\t\t * Special case in which we have a !SCHED_DEADLINE task that is going",
            "\t\t * to be deboosted, but exceeds its runtime while doing so. No point in",
            "\t\t * replenishing it, as it's going to return back to its original",
            "\t\t * scheduling class after this. If it has been throttled, we need to",
            "\t\t * clear the flag, otherwise the task may wake up as throttled after",
            "\t\t * being boosted again with no means to replenish the runtime and clear",
            "\t\t * the throttle.",
            "\t\t */",
            "\t\tp->dl.dl_throttled = 0;",
            "\t\tif (!(flags & ENQUEUE_REPLENISH))",
            "\t\t\tprintk_deferred_once(\"sched: DL de-boosted task PID %d: REPLENISH flag missing\\n\",",
            "\t\t\t\t\t     task_pid_nr(p));",
            "",
            "\t\treturn;",
            "\t}",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_dl(dl_rq_of_se(&p->dl), &p->dl);",
            "",
            "\tif (p->on_rq == TASK_ON_RQ_MIGRATING)",
            "\t\tflags |= ENQUEUE_MIGRATING;",
            "",
            "\tenqueue_dl_entity(&p->dl, flags);",
            "",
            "\tif (dl_server(&p->dl))",
            "\t\treturn;",
            "",
            "\tif (!task_current(rq, p) && !p->dl.dl_throttled && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_dl_task(rq, p);",
            "}",
            "static bool dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tupdate_curr_dl(rq);",
            "",
            "\tif (p->on_rq == TASK_ON_RQ_MIGRATING)",
            "\t\tflags |= DEQUEUE_MIGRATING;",
            "",
            "\tdequeue_dl_entity(&p->dl, flags);",
            "\tif (!p->dl.dl_throttled && !dl_server(&p->dl))",
            "\t\tdequeue_pushable_dl_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void yield_task_dl(struct rq *rq)",
            "{",
            "\t/*",
            "\t * We make the task go to sleep until its current deadline by",
            "\t * forcing its runtime to zero. This way, update_curr_dl() stops",
            "\t * it and the bandwidth timer will wake it up and will give it",
            "\t * new scheduling parameters (thanks to dl_yielded=1).",
            "\t */",
            "\trq->curr->dl.dl_yielded = 1;",
            "",
            "\tupdate_rq_clock(rq);",
            "\tupdate_curr_dl(rq);",
            "\t/*",
            "\t * Tell update_rq_clock() that we've just updated,",
            "\t * so we don't do microscopic update in schedule()",
            "\t * and double the fastpath cost.",
            "\t */",
            "\trq_clock_skip_update(rq);",
            "}",
            "static inline bool dl_task_is_earliest_deadline(struct task_struct *p,",
            "\t\t\t\t\t\t struct rq *rq)",
            "{",
            "\treturn (!rq->dl.dl_nr_running ||",
            "\t\tdl_time_before(p->dl.deadline,",
            "\t\t\t       rq->dl.earliest_dl.curr));",
            "}",
            "static int",
            "select_task_rq_dl(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tbool select_rq;",
            "\tstruct rq *rq;",
            "",
            "\tif (!(flags & WF_TTWU))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If we are dealing with a -deadline task, we must",
            "\t * decide where to wake it up.",
            "\t * If it has a later deadline and the current task",
            "\t * on this rq can't move (provided the waking task",
            "\t * can!) we prefer to send it somewhere else. On the",
            "\t * other hand, if it has a shorter deadline, we",
            "\t * try to make it stay here, it might be important.",
            "\t */",
            "\tselect_rq = unlikely(dl_task(curr)) &&",
            "\t\t    (curr->nr_cpus_allowed < 2 ||",
            "\t\t     !dl_entity_preempt(&p->dl, &curr->dl)) &&",
            "\t\t    p->nr_cpus_allowed > 1;",
            "",
            "\t/*",
            "\t * Take the capacity of the CPU into account to",
            "\t * ensure it fits the requirement of the task.",
            "\t */",
            "\tif (sched_asym_cpucap_active())",
            "\t\tselect_rq |= !dl_task_fits_capacity(p, cpu);",
            "",
            "\tif (select_rq) {",
            "\t\tint target = find_later_rq(p);",
            "",
            "\t\tif (target != -1 &&",
            "\t\t    dl_task_is_earliest_deadline(p, cpu_rq(target)))",
            "\t\t\tcpu = target;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "enqueue_task_dl, dequeue_task_dl, yield_task_dl, dl_task_is_earliest_deadline, select_task_rq_dl",
          "description": "处理截止时间任务的调度决策，包含任务入队出队、抢占检查、CPU选择及负载均衡逻辑，通过dl_task_is_earliest_deadline判断任务截止时间优先级并选择合适CPU",
          "similarity": 0.5923846364021301
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 966,
          "end_line": 1100,
          "content": [
            "static bool dl_entity_overflow(struct sched_dl_entity *dl_se, u64 t)",
            "{",
            "\tu64 left, right;",
            "",
            "\t/*",
            "\t * left and right are the two sides of the equation above,",
            "\t * after a bit of shuffling to use multiplications instead",
            "\t * of divisions.",
            "\t *",
            "\t * Note that none of the time values involved in the two",
            "\t * multiplications are absolute: dl_deadline and dl_runtime",
            "\t * are the relative deadline and the maximum runtime of each",
            "\t * instance, runtime is the runtime left for the last instance",
            "\t * and (deadline - t), since t is rq->clock, is the time left",
            "\t * to the (absolute) deadline. Even if overflowing the u64 type",
            "\t * is very unlikely to occur in both cases, here we scale down",
            "\t * as we want to avoid that risk at all. Scaling down by 10",
            "\t * means that we reduce granularity to 1us. We are fine with it,",
            "\t * since this is only a true/false check and, anyway, thinking",
            "\t * of anything below microseconds resolution is actually fiction",
            "\t * (but still we want to give the user that illusion >;).",
            "\t */",
            "\tleft = (pi_of(dl_se)->dl_deadline >> DL_SCALE) * (dl_se->runtime >> DL_SCALE);",
            "\tright = ((dl_se->deadline - t) >> DL_SCALE) *",
            "\t\t(pi_of(dl_se)->dl_runtime >> DL_SCALE);",
            "",
            "\treturn dl_time_before(right, left);",
            "}",
            "static void",
            "update_dl_revised_wakeup(struct sched_dl_entity *dl_se, struct rq *rq)",
            "{",
            "\tu64 laxity = dl_se->deadline - rq_clock(rq);",
            "",
            "\t/*",
            "\t * If the task has deadline < period, and the deadline is in the past,",
            "\t * it should already be throttled before this check.",
            "\t *",
            "\t * See update_dl_entity() comments for further details.",
            "\t */",
            "\tWARN_ON(dl_time_before(dl_se->deadline, rq_clock(rq)));",
            "",
            "\tdl_se->runtime = (dl_se->dl_density * laxity) >> BW_SHIFT;",
            "}",
            "static inline bool dl_is_implicit(struct sched_dl_entity *dl_se)",
            "{",
            "\treturn dl_se->dl_deadline == dl_se->dl_period;",
            "}",
            "static void update_dl_entity(struct sched_dl_entity *dl_se)",
            "{",
            "\tstruct rq *rq = rq_of_dl_se(dl_se);",
            "",
            "\tif (dl_time_before(dl_se->deadline, rq_clock(rq)) ||",
            "\t    dl_entity_overflow(dl_se, rq_clock(rq))) {",
            "",
            "\t\tif (unlikely(!dl_is_implicit(dl_se) &&",
            "\t\t\t     !dl_time_before(dl_se->deadline, rq_clock(rq)) &&",
            "\t\t\t     !is_dl_boosted(dl_se))) {",
            "\t\t\tupdate_dl_revised_wakeup(dl_se, rq);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\treplenish_dl_new_period(dl_se, rq);",
            "\t} else if (dl_server(dl_se) && dl_se->dl_defer) {",
            "\t\t/*",
            "\t\t * The server can still use its previous deadline, so check if",
            "\t\t * it left the dl_defer_running state.",
            "\t\t */",
            "\t\tif (!dl_se->dl_defer_running) {",
            "\t\t\tdl_se->dl_defer_armed = 1;",
            "\t\t\tdl_se->dl_throttled = 1;",
            "\t\t}",
            "\t}",
            "}",
            "static inline u64 dl_next_period(struct sched_dl_entity *dl_se)",
            "{",
            "\treturn dl_se->deadline - dl_se->dl_deadline + dl_se->dl_period;",
            "}",
            "static int start_dl_timer(struct sched_dl_entity *dl_se)",
            "{",
            "\tstruct hrtimer *timer = &dl_se->dl_timer;",
            "\tstruct dl_rq *dl_rq = dl_rq_of_se(dl_se);",
            "\tstruct rq *rq = rq_of_dl_rq(dl_rq);",
            "\tktime_t now, act;",
            "\ts64 delta;",
            "",
            "\tlockdep_assert_rq_held(rq);",
            "",
            "\t/*",
            "\t * We want the timer to fire at the deadline, but considering",
            "\t * that it is actually coming from rq->clock and not from",
            "\t * hrtimer's time base reading.",
            "\t *",
            "\t * The deferred reservation will have its timer set to",
            "\t * (deadline - runtime). At that point, the CBS rule will decide",
            "\t * if the current deadline can be used, or if a replenishment is",
            "\t * required to avoid add too much pressure on the system",
            "\t * (current u > U).",
            "\t */",
            "\tif (dl_se->dl_defer_armed) {",
            "\t\tWARN_ON_ONCE(!dl_se->dl_throttled);",
            "\t\tact = ns_to_ktime(dl_se->deadline - dl_se->runtime);",
            "\t} else {",
            "\t\t/* act = deadline - rel-deadline + period */",
            "\t\tact = ns_to_ktime(dl_next_period(dl_se));",
            "\t}",
            "",
            "\tnow = hrtimer_cb_get_time(timer);",
            "\tdelta = ktime_to_ns(now) - rq_clock(rq);",
            "\tact = ktime_add_ns(act, delta);",
            "",
            "\t/*",
            "\t * If the expiry time already passed, e.g., because the value",
            "\t * chosen as the deadline is too small, don't even try to",
            "\t * start the timer in the past!",
            "\t */",
            "\tif (ktime_us_delta(act, now) < 0)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * !enqueued will guarantee another callback; even if one is already in",
            "\t * progress. This ensures a balanced {get,put}_task_struct().",
            "\t *",
            "\t * The race against __run_timer() clearing the enqueued state is",
            "\t * harmless because we're holding task_rq()->lock, therefore the timer",
            "\t * expiring after we've done the check will wait on its task_rq_lock()",
            "\t * and observe our state.",
            "\t */",
            "\tif (!hrtimer_is_queued(timer)) {",
            "\t\tif (!dl_server(dl_se))",
            "\t\t\tget_task_struct(dl_task_of(dl_se));",
            "\t\thrtimer_start(timer, act, HRTIMER_MODE_ABS_HARD);",
            "\t}",
            "",
            "\treturn 1;",
            "}"
          ],
          "function_name": "dl_entity_overflow, update_dl_revised_wakeup, dl_is_implicit, update_dl_entity, dl_next_period, start_dl_timer",
          "description": "实现了截止时间任务的溢出检测、唤醒时间调整、隐式截止时间判定及动态实体更新逻辑，包含基于带宽的调度策略和超时处理机制。",
          "similarity": 0.5862662196159363
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 224,
          "end_line": 332,
          "content": [
            "static inline",
            "void __dl_sub(struct dl_bw *dl_b, u64 tsk_bw, int cpus)",
            "{",
            "\tdl_b->total_bw -= tsk_bw;",
            "\t__dl_update(dl_b, (s32)tsk_bw / cpus);",
            "}",
            "static inline",
            "void __dl_add(struct dl_bw *dl_b, u64 tsk_bw, int cpus)",
            "{",
            "\tdl_b->total_bw += tsk_bw;",
            "\t__dl_update(dl_b, -((s32)tsk_bw / cpus));",
            "}",
            "static inline bool",
            "__dl_overflow(struct dl_bw *dl_b, unsigned long cap, u64 old_bw, u64 new_bw)",
            "{",
            "\treturn dl_b->bw != -1 &&",
            "\t       cap_scale(dl_b->bw, cap) < dl_b->total_bw - old_bw + new_bw;",
            "}",
            "static inline",
            "void __add_running_bw(u64 dl_bw, struct dl_rq *dl_rq)",
            "{",
            "\tu64 old = dl_rq->running_bw;",
            "",
            "\tlockdep_assert_rq_held(rq_of_dl_rq(dl_rq));",
            "\tdl_rq->running_bw += dl_bw;",
            "\tSCHED_WARN_ON(dl_rq->running_bw < old); /* overflow */",
            "\tSCHED_WARN_ON(dl_rq->running_bw > dl_rq->this_bw);",
            "\t/* kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\tcpufreq_update_util(rq_of_dl_rq(dl_rq), 0);",
            "}",
            "static inline",
            "void __sub_running_bw(u64 dl_bw, struct dl_rq *dl_rq)",
            "{",
            "\tu64 old = dl_rq->running_bw;",
            "",
            "\tlockdep_assert_rq_held(rq_of_dl_rq(dl_rq));",
            "\tdl_rq->running_bw -= dl_bw;",
            "\tSCHED_WARN_ON(dl_rq->running_bw > old); /* underflow */",
            "\tif (dl_rq->running_bw > old)",
            "\t\tdl_rq->running_bw = 0;",
            "\t/* kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\tcpufreq_update_util(rq_of_dl_rq(dl_rq), 0);",
            "}",
            "static inline",
            "void __add_rq_bw(u64 dl_bw, struct dl_rq *dl_rq)",
            "{",
            "\tu64 old = dl_rq->this_bw;",
            "",
            "\tlockdep_assert_rq_held(rq_of_dl_rq(dl_rq));",
            "\tdl_rq->this_bw += dl_bw;",
            "\tSCHED_WARN_ON(dl_rq->this_bw < old); /* overflow */",
            "}",
            "static inline",
            "void __sub_rq_bw(u64 dl_bw, struct dl_rq *dl_rq)",
            "{",
            "\tu64 old = dl_rq->this_bw;",
            "",
            "\tlockdep_assert_rq_held(rq_of_dl_rq(dl_rq));",
            "\tdl_rq->this_bw -= dl_bw;",
            "\tSCHED_WARN_ON(dl_rq->this_bw > old); /* underflow */",
            "\tif (dl_rq->this_bw > old)",
            "\t\tdl_rq->this_bw = 0;",
            "\tSCHED_WARN_ON(dl_rq->running_bw > dl_rq->this_bw);",
            "}",
            "static inline",
            "void add_rq_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\tif (!dl_entity_is_special(dl_se))",
            "\t\t__add_rq_bw(dl_se->dl_bw, dl_rq);",
            "}",
            "static inline",
            "void sub_rq_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\tif (!dl_entity_is_special(dl_se))",
            "\t\t__sub_rq_bw(dl_se->dl_bw, dl_rq);",
            "}",
            "static inline",
            "void add_running_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\tif (!dl_entity_is_special(dl_se))",
            "\t\t__add_running_bw(dl_se->dl_bw, dl_rq);",
            "}",
            "static inline",
            "void sub_running_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\tif (!dl_entity_is_special(dl_se))",
            "\t\t__sub_running_bw(dl_se->dl_bw, dl_rq);",
            "}",
            "static void dl_rq_change_utilization(struct rq *rq, struct sched_dl_entity *dl_se, u64 new_bw)",
            "{",
            "\tif (dl_se->dl_non_contending) {",
            "\t\tsub_running_bw(dl_se, &rq->dl);",
            "\t\tdl_se->dl_non_contending = 0;",
            "",
            "\t\t/*",
            "\t\t * If the timer handler is currently running and the",
            "\t\t * timer cannot be canceled, inactive_task_timer()",
            "\t\t * will see that dl_not_contending is not set, and",
            "\t\t * will not touch the rq's active utilization,",
            "\t\t * so we are still safe.",
            "\t\t */",
            "\t\tif (hrtimer_try_to_cancel(&dl_se->inactive_timer) == 1) {",
            "\t\t\tif (!dl_server(dl_se))",
            "\t\t\t\tput_task_struct(dl_task_of(dl_se));",
            "\t\t}",
            "\t}",
            "\t__sub_rq_bw(dl_se->dl_bw, &rq->dl);",
            "\t__add_rq_bw(new_bw, &rq->dl);",
            "}"
          ],
          "function_name": "__dl_sub, __dl_add, __dl_overflow, __add_running_bw, __sub_running_bw, __add_rq_bw, __sub_rq_bw, add_rq_bw, sub_rq_bw, add_running_bw, sub_running_bw, dl_rq_change_utilization",
          "description": "实现截止时间任务带宽的增减操作，包含溢出检测逻辑，用于动态调整运行时带宽和资源使用量。",
          "similarity": 0.5663671493530273
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 1741,
          "end_line": 1857,
          "content": [
            "static void update_curr_dl(struct rq *rq)",
            "{",
            "\tstruct task_struct *curr = rq->curr;",
            "\tstruct sched_dl_entity *dl_se = &curr->dl;",
            "\ts64 delta_exec;",
            "",
            "\tif (!dl_task(curr) || !on_dl_rq(dl_se))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Consumed budget is computed considering the time as",
            "\t * observed by schedulable tasks (excluding time spent",
            "\t * in hardirq context, etc.). Deadlines are instead",
            "\t * computed using hard walltime. This seems to be the more",
            "\t * natural solution, but the full ramifications of this",
            "\t * approach need further study.",
            "\t */",
            "\tdelta_exec = update_curr_common(rq);",
            "\tupdate_curr_dl_se(rq, dl_se, delta_exec);",
            "}",
            "static enum hrtimer_restart inactive_task_timer(struct hrtimer *timer)",
            "{",
            "\tstruct sched_dl_entity *dl_se = container_of(timer,",
            "\t\t\t\t\t\t     struct sched_dl_entity,",
            "\t\t\t\t\t\t     inactive_timer);",
            "\tstruct task_struct *p = NULL;",
            "\tstruct rq_flags rf;",
            "\tstruct rq *rq;",
            "",
            "\tif (!dl_server(dl_se)) {",
            "\t\tp = dl_task_of(dl_se);",
            "\t\trq = task_rq_lock(p, &rf);",
            "\t} else {",
            "\t\trq = dl_se->rq;",
            "\t\trq_lock(rq, &rf);",
            "\t}",
            "",
            "\tsched_clock_tick();",
            "\tupdate_rq_clock(rq);",
            "",
            "\tif (dl_server(dl_se))",
            "\t\tgoto no_task;",
            "",
            "\tif (!dl_task(p) || READ_ONCE(p->__state) == TASK_DEAD) {",
            "\t\tstruct dl_bw *dl_b = dl_bw_of(task_cpu(p));",
            "",
            "\t\tif (READ_ONCE(p->__state) == TASK_DEAD && dl_se->dl_non_contending) {",
            "\t\t\tsub_running_bw(&p->dl, dl_rq_of_se(&p->dl));",
            "\t\t\tsub_rq_bw(&p->dl, dl_rq_of_se(&p->dl));",
            "\t\t\tdl_se->dl_non_contending = 0;",
            "\t\t}",
            "",
            "\t\traw_spin_lock(&dl_b->lock);",
            "\t\t__dl_sub(dl_b, p->dl.dl_bw, dl_bw_cpus(task_cpu(p)));",
            "\t\traw_spin_unlock(&dl_b->lock);",
            "\t\t__dl_clear_params(dl_se);",
            "",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "no_task:",
            "\tif (dl_se->dl_non_contending == 0)",
            "\t\tgoto unlock;",
            "",
            "\tsub_running_bw(dl_se, &rq->dl);",
            "\tdl_se->dl_non_contending = 0;",
            "unlock:",
            "",
            "\tif (!dl_server(dl_se)) {",
            "\t\ttask_rq_unlock(rq, p, &rf);",
            "\t\tput_task_struct(p);",
            "\t} else {",
            "\t\trq_unlock(rq, &rf);",
            "\t}",
            "",
            "\treturn HRTIMER_NORESTART;",
            "}",
            "static void init_dl_inactive_task_timer(struct sched_dl_entity *dl_se)",
            "{",
            "\tstruct hrtimer *timer = &dl_se->inactive_timer;",
            "",
            "\thrtimer_init(timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);",
            "\ttimer->function = inactive_task_timer;",
            "}",
            "static void inc_dl_deadline(struct dl_rq *dl_rq, u64 deadline)",
            "{",
            "\tstruct rq *rq = rq_of_dl_rq(dl_rq);",
            "",
            "\tif (dl_rq->earliest_dl.curr == 0 ||",
            "\t    dl_time_before(deadline, dl_rq->earliest_dl.curr)) {",
            "\t\tif (dl_rq->earliest_dl.curr == 0)",
            "\t\t\tcpupri_set(&rq->rd->cpupri, rq->cpu, CPUPRI_HIGHER);",
            "\t\tdl_rq->earliest_dl.curr = deadline;",
            "\t\tcpudl_set(&rq->rd->cpudl, rq->cpu, deadline);",
            "\t}",
            "}",
            "static void dec_dl_deadline(struct dl_rq *dl_rq, u64 deadline)",
            "{",
            "\tstruct rq *rq = rq_of_dl_rq(dl_rq);",
            "",
            "\t/*",
            "\t * Since we may have removed our earliest (and/or next earliest)",
            "\t * task we must recompute them.",
            "\t */",
            "\tif (!dl_rq->dl_nr_running) {",
            "\t\tdl_rq->earliest_dl.curr = 0;",
            "\t\tdl_rq->earliest_dl.next = 0;",
            "\t\tcpudl_clear(&rq->rd->cpudl, rq->cpu);",
            "\t\tcpupri_set(&rq->rd->cpupri, rq->cpu, rq->rt.highest_prio.curr);",
            "\t} else {",
            "\t\tstruct rb_node *leftmost = rb_first_cached(&dl_rq->root);",
            "\t\tstruct sched_dl_entity *entry = __node_2_dle(leftmost);",
            "",
            "\t\tdl_rq->earliest_dl.curr = entry->deadline;",
            "\t\tcpudl_set(&rq->rd->cpudl, rq->cpu, entry->deadline);",
            "\t}",
            "}"
          ],
          "function_name": "update_curr_dl, inactive_task_timer, init_dl_inactive_task_timer, inc_dl_deadline, dec_dl_deadline",
          "description": "实现截止时间调度器的核心逻辑，包含update_curr_dl更新当前任务执行时间，inactive_task_timer处理非竞争任务的定时器逻辑，init_dl_inactive_task_timer初始化定时器，inc_dl_deadline和dec_dl_deadline维护截止时间队列的最早截止时间",
          "similarity": 0.5638962984085083
        }
      ]
    }
  ]
}