{
  "query": "多线程与多进程在虚拟内存管理中的区别",
  "timestamp": "2025-12-25 23:30:44",
  "retrieved_files": [
    {
      "source_file": "kernel/fork.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:30:07\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `fork.c`\n\n---\n\n# fork.c 技术文档\n\n## 1. 文件概述\n\n`fork.c` 是 Linux 内核中实现进程创建（fork）系统调用的核心源文件。该文件包含了创建新进程所需的所有辅助例程，负责复制父进程的资源（如内存、文件描述符、信号处理等）以生成子进程。虽然 fork 逻辑本身概念简单，但其涉及的内存管理（尤其是写时复制 COW 机制）极为复杂，实际内存页的复制由 `mm/memory.c` 中的 `copy_page_range()` 等函数处理。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `total_forks`: 累计系统自启动以来创建的进程总数\n- `nr_threads`: 当前系统中的线程总数（不包括 idle 线程）\n- `max_threads`: 可配置的线程数量上限（默认为 `FUTEX_TID_MASK`）\n- `process_counts`: 每 CPU 的进程计数器（per-CPU 变量）\n- `tasklist_lock`: 保护任务链表的读写锁（全局任务列表的同步原语）\n\n### 关键辅助函数\n- `nr_processes()`: 计算系统中所有进程的总数（聚合各 CPU 的 `process_counts`）\n- `arch_release_task_struct()`: 架构相关的 task_struct 释放钩子（弱符号，默认为空）\n- `alloc_task_struct_node()` / `free_task_struct()`: 分配/释放 `task_struct` 结构（基于 slab 分配器）\n- `alloc_thread_stack_node()` / `thread_stack_delayed_free()`: 分配/延迟释放线程内核栈（支持 `CONFIG_VMAP_STACK`）\n\n### 核心数据结构\n- `resident_page_types[]`: 用于内存统计的页面类型名称映射数组\n- `vm_stack`: 用于 RCU 延迟释放的虚拟内存栈封装结构\n- `cached_stacks[NR_CACHED_STACKS]`: 每 CPU 的内核栈缓存（减少频繁 vmalloc/vfree 开销）\n\n## 3. 关键实现\n\n### 进程/线程计数管理\n- 使用 per-CPU 变量 `process_counts` 避免全局锁竞争\n- 全局计数器 `nr_threads` 和 `total_forks` 由 `tasklist_lock` 保护\n- `nr_processes()` 通过遍历所有可能的 CPU 聚合计数\n\n### 内核栈分配策略（`CONFIG_VMAP_STACK`）\n- **缓存机制**：每个 CPU 缓存最多 2 个已释放的栈（`NR_CACHED_STACKS`），减少 TLB 刷新和 vmalloc 开销\n- **内存分配**：\n  - 优先从本地缓存获取栈\n  - 缓存未命中时使用 `__vmalloc_node_range()` 分配连续虚拟地址空间\n  - 显式禁用 `__GFP_ACCOUNT`（因后续手动进行 memcg 计费）\n- **安全清理**：\n  - 重用栈时清零内存（`memset(stack, 0, THREAD_SIZE)`）\n  - KASAN 消毒（`kasan_unpoison_range`）和标签重置\n- **延迟释放**：\n  - 通过 RCU 机制延迟释放栈（`call_rcu`）\n  - 释放时尝试回填缓存，失败则直接 `vfree`\n\n### 内存控制组（memcg）集成\n- 手动对栈的每个物理页进行 memcg 计费（`memcg_kmem_charge_page`）\n- 计费失败时回滚已计费页面（`memcg_kmem_uncharge_page`）\n- 确保内核栈内存纳入 cgroup 内存限制\n\n### 锁与同步\n- `tasklist_lock` 作为全局任务列表的保护锁（读写锁）\n- 提供 `lockdep_tasklist_lock_is_held()` 供 RCU 锁验证使用\n- RCU 用于安全延迟释放内核栈资源\n\n## 4. 依赖关系\n\n### 内核子系统依赖\n- **内存管理 (MM)**：`<linux/mm.h>`, `<linux/vmalloc.h>`, `<linux/memcontrol.h>`\n- **调度器 (Scheduler)**：`<linux/sched/*.h>`, 任务状态和 CPU 绑定\n- **安全模块**：`<linux/security.h>`, `<linux/capability.h>`, `<linux/seccomp.h>`\n- **命名空间**：`<linux/nsproxy.h>`（UTS, IPC, PID, 网络等）\n- **文件系统**：`<linux/fs.h>`, `<linux/fdtable.h>`（文件描述符复制）\n- **跟踪与调试**：`<trace/events/sched.h>`, `<linux/ftrace.h>`, KASAN/KMSAN\n\n### 架构相关依赖\n- `<asm/pgalloc.h>`：页表分配\n- `<asm/mmu_context.h>`：MMU 上下文切换\n- `<asm/tlbflush.h>`：TLB 刷新操作\n- 架构特定的 `THREAD_SIZE` 和栈对齐要求\n\n### 配置选项依赖\n- `CONFIG_VMAP_STACK`：启用虚拟内存分配内核栈\n- `CONFIG_PROVE_RCU`：RCU 锁验证支持\n- `CONFIG_ARCH_TASK_STRUCT_ALLOCATOR`：架构自定义 task_struct 分配器\n- `CONFIG_MEMCG_KMEM`：内核内存 cgroup 支持\n\n## 5. 使用场景\n\n### 进程创建路径\n- **系统调用入口**：`sys_fork()`, `sys_vfork()`, `sys_clone()` 最终调用 `_do_fork()`\n- **内核线程创建**：`kthread_create()` 通过 `kernel_thread()` 触发 fork 逻辑\n- **容器/命名空间初始化**：新 PID/UTS/IPC 命名空间创建时伴随进程 fork\n\n### 资源复制关键点\n- **内存描述符 (mm_struct)**：通过 `dup_mm()` 复制地址空间（COW 页表）\n- **文件描述符表**：`dup_fd()` 复制打开文件表\n- **信号处理**：复制信号掩码和处理函数\n- **POSIX 定时器/异步 I/O**：复制相关上下文（如 `aio`, `posix-timers`）\n\n### 特殊场景处理\n- **写时复制优化**：避免物理内存立即复制，提升 fork 性能\n- **OOM Killer 集成**：在内存不足时参与进程选择\n- **审计与监控**：通过 `audit_alloc()` 和 `proc` 文件系统暴露进程信息\n- **实时性保障**：RT 任务 fork 时保持调度策略和优先级",
      "similarity": 0.5728332996368408,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/fork.c",
          "start_line": 350,
          "end_line": 450,
          "content": [
            "static void free_thread_stack(struct task_struct *tsk)",
            "{",
            "\tif (!try_release_thread_stack_to_cache(tsk->stack_vm_area))",
            "\t\tthread_stack_delayed_free(tsk);",
            "",
            "\ttsk->stack = NULL;",
            "\ttsk->stack_vm_area = NULL;",
            "}",
            "static void thread_stack_free_rcu(struct rcu_head *rh)",
            "{",
            "\t__free_pages(virt_to_page(rh), THREAD_SIZE_ORDER);",
            "}",
            "static void thread_stack_delayed_free(struct task_struct *tsk)",
            "{",
            "\tstruct rcu_head *rh = tsk->stack;",
            "",
            "\tcall_rcu(rh, thread_stack_free_rcu);",
            "}",
            "static int alloc_thread_stack_node(struct task_struct *tsk, int node)",
            "{",
            "\tstruct page *page = alloc_pages_node(node, THREADINFO_GFP,",
            "\t\t\t\t\t     THREAD_SIZE_ORDER);",
            "",
            "\tif (likely(page)) {",
            "\t\ttsk->stack = kasan_reset_tag(page_address(page));",
            "\t\treturn 0;",
            "\t}",
            "\treturn -ENOMEM;",
            "}",
            "static void free_thread_stack(struct task_struct *tsk)",
            "{",
            "\tthread_stack_delayed_free(tsk);",
            "\ttsk->stack = NULL;",
            "}",
            "static void thread_stack_free_rcu(struct rcu_head *rh)",
            "{",
            "\tkmem_cache_free(thread_stack_cache, rh);",
            "}",
            "static void thread_stack_delayed_free(struct task_struct *tsk)",
            "{",
            "\tstruct rcu_head *rh = tsk->stack;",
            "",
            "\tcall_rcu(rh, thread_stack_free_rcu);",
            "}",
            "static int alloc_thread_stack_node(struct task_struct *tsk, int node)",
            "{",
            "\tunsigned long *stack;",
            "\tstack = kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);",
            "\tstack = kasan_reset_tag(stack);",
            "\ttsk->stack = stack;",
            "\treturn stack ? 0 : -ENOMEM;",
            "}",
            "static void free_thread_stack(struct task_struct *tsk)",
            "{",
            "\tthread_stack_delayed_free(tsk);",
            "\ttsk->stack = NULL;",
            "}",
            "void thread_stack_cache_init(void)",
            "{",
            "\tthread_stack_cache = kmem_cache_create_usercopy(\"thread_stack\",",
            "\t\t\t\t\tTHREAD_SIZE, THREAD_SIZE, 0, 0,",
            "\t\t\t\t\tTHREAD_SIZE, NULL);",
            "\tBUG_ON(thread_stack_cache == NULL);",
            "}",
            "static int alloc_thread_stack_node(struct task_struct *tsk, int node)",
            "{",
            "\tunsigned long *stack;",
            "",
            "\tstack = arch_alloc_thread_stack_node(tsk, node);",
            "\ttsk->stack = stack;",
            "\treturn stack ? 0 : -ENOMEM;",
            "}",
            "static void free_thread_stack(struct task_struct *tsk)",
            "{",
            "\tarch_free_thread_stack(tsk);",
            "\ttsk->stack = NULL;",
            "}",
            "static bool vma_lock_alloc(struct vm_area_struct *vma)",
            "{",
            "\tvma->vm_lock = kmem_cache_alloc(vma_lock_cachep, GFP_KERNEL);",
            "\tif (!vma->vm_lock)",
            "\t\treturn false;",
            "",
            "\tinit_rwsem(&vma->vm_lock->lock);",
            "\tvma->vm_lock_seq = -1;",
            "",
            "\treturn true;",
            "}",
            "static inline void vma_lock_free(struct vm_area_struct *vma)",
            "{",
            "\tkmem_cache_free(vma_lock_cachep, vma->vm_lock);",
            "}",
            "static inline bool vma_lock_alloc(struct vm_area_struct *vma) { return true; }",
            "static inline void vma_lock_free(struct vm_area_struct *vma) {}",
            "void __vm_area_free(struct vm_area_struct *vma)",
            "{",
            "\tvma_numab_state_free(vma);",
            "\tfree_anon_vma_name(vma);",
            "\tvma_lock_free(vma);",
            "\tkmem_cache_free(vm_area_cachep, vma);",
            "}"
          ],
          "function_name": "free_thread_stack, thread_stack_free_rcu, thread_stack_delayed_free, alloc_thread_stack_node, free_thread_stack, thread_stack_free_rcu, thread_stack_delayed_free, alloc_thread_stack_node, free_thread_stack, thread_stack_cache_init, alloc_thread_stack_node, free_thread_stack, vma_lock_alloc, vma_lock_free, vma_lock_alloc, vma_lock_free, __vm_area_free",
          "description": "上下文不完整：存在重复函数定义和不一致的实现方式，显示线程栈分配/释放流程中使用了多种不同的内存管理策略，包括基于页面的分配、SLAB缓存及架构特定实现。",
          "similarity": 0.5879560708999634
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/fork.c",
          "start_line": 159,
          "end_line": 303,
          "content": [
            "int lockdep_tasklist_lock_is_held(void)",
            "{",
            "\treturn lockdep_is_held(&tasklist_lock);",
            "}",
            "int nr_processes(void)",
            "{",
            "\tint cpu;",
            "\tint total = 0;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\ttotal += per_cpu(process_counts, cpu);",
            "",
            "\treturn total;",
            "}",
            "void __weak arch_release_task_struct(struct task_struct *tsk)",
            "{",
            "}",
            "static inline void free_task_struct(struct task_struct *tsk)",
            "{",
            "\tkmem_cache_free(task_struct_cachep, tsk);",
            "}",
            "static bool try_release_thread_stack_to_cache(struct vm_struct *vm)",
            "{",
            "\tunsigned int i;",
            "",
            "\tfor (i = 0; i < NR_CACHED_STACKS; i++) {",
            "\t\tif (this_cpu_cmpxchg(cached_stacks[i], NULL, vm) != NULL)",
            "\t\t\tcontinue;",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "static void thread_stack_free_rcu(struct rcu_head *rh)",
            "{",
            "\tstruct vm_stack *vm_stack = container_of(rh, struct vm_stack, rcu);",
            "",
            "\tif (try_release_thread_stack_to_cache(vm_stack->stack_vm_area))",
            "\t\treturn;",
            "",
            "\tvfree(vm_stack);",
            "}",
            "static void thread_stack_delayed_free(struct task_struct *tsk)",
            "{",
            "\tstruct vm_stack *vm_stack = tsk->stack;",
            "",
            "\tvm_stack->stack_vm_area = tsk->stack_vm_area;",
            "\tcall_rcu(&vm_stack->rcu, thread_stack_free_rcu);",
            "}",
            "static int free_vm_stack_cache(unsigned int cpu)",
            "{",
            "\tstruct vm_struct **cached_vm_stacks = per_cpu_ptr(cached_stacks, cpu);",
            "\tint i;",
            "",
            "\tfor (i = 0; i < NR_CACHED_STACKS; i++) {",
            "\t\tstruct vm_struct *vm_stack = cached_vm_stacks[i];",
            "",
            "\t\tif (!vm_stack)",
            "\t\t\tcontinue;",
            "",
            "\t\tvfree(vm_stack->addr);",
            "\t\tcached_vm_stacks[i] = NULL;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int memcg_charge_kernel_stack(struct vm_struct *vm)",
            "{",
            "\tint i;",
            "\tint ret;",
            "\tint nr_charged = 0;",
            "",
            "\tBUG_ON(vm->nr_pages != THREAD_SIZE / PAGE_SIZE);",
            "",
            "\tfor (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++) {",
            "\t\tret = memcg_kmem_charge_page(vm->pages[i], GFP_KERNEL, 0);",
            "\t\tif (ret)",
            "\t\t\tgoto err;",
            "\t\tnr_charged++;",
            "\t}",
            "\treturn 0;",
            "err:",
            "\tfor (i = 0; i < nr_charged; i++)",
            "\t\tmemcg_kmem_uncharge_page(vm->pages[i], 0);",
            "\treturn ret;",
            "}",
            "static int alloc_thread_stack_node(struct task_struct *tsk, int node)",
            "{",
            "\tstruct vm_struct *vm;",
            "\tvoid *stack;",
            "\tint i;",
            "",
            "\tfor (i = 0; i < NR_CACHED_STACKS; i++) {",
            "\t\tstruct vm_struct *s;",
            "",
            "\t\ts = this_cpu_xchg(cached_stacks[i], NULL);",
            "",
            "\t\tif (!s)",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Reset stack metadata. */",
            "\t\tkasan_unpoison_range(s->addr, THREAD_SIZE);",
            "",
            "\t\tstack = kasan_reset_tag(s->addr);",
            "",
            "\t\t/* Clear stale pointers from reused stack. */",
            "\t\tmemset(stack, 0, THREAD_SIZE);",
            "",
            "\t\tif (memcg_charge_kernel_stack(s)) {",
            "\t\t\tvfree(s->addr);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "",
            "\t\ttsk->stack_vm_area = s;",
            "\t\ttsk->stack = stack;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/*",
            "\t * Allocated stacks are cached and later reused by new threads,",
            "\t * so memcg accounting is performed manually on assigning/releasing",
            "\t * stacks to tasks. Drop __GFP_ACCOUNT.",
            "\t */",
            "\tstack = __vmalloc_node_range(THREAD_SIZE, THREAD_ALIGN,",
            "\t\t\t\t     VMALLOC_START, VMALLOC_END,",
            "\t\t\t\t     THREADINFO_GFP & ~__GFP_ACCOUNT,",
            "\t\t\t\t     PAGE_KERNEL,",
            "\t\t\t\t     0, node, __builtin_return_address(0));",
            "\tif (!stack)",
            "\t\treturn -ENOMEM;",
            "",
            "\tvm = find_vm_area(stack);",
            "\tif (memcg_charge_kernel_stack(vm)) {",
            "\t\tvfree(stack);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\t/*",
            "\t * We can't call find_vm_area() in interrupt context, and",
            "\t * free_thread_stack() can be called in interrupt context,",
            "\t * so cache the vm_struct.",
            "\t */",
            "\ttsk->stack_vm_area = vm;",
            "\tstack = kasan_reset_tag(stack);",
            "\ttsk->stack = stack;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "lockdep_tasklist_lock_is_held, nr_processes, arch_release_task_struct, free_task_struct, try_release_thread_stack_to_cache, thread_stack_free_rcu, thread_stack_delayed_free, free_vm_stack_cache, memcg_charge_kernel_stack, alloc_thread_stack_node",
          "description": "实现任务列表锁状态检测、进程总数统计及线程栈分配释放逻辑，通过缓存机制优化线程栈复用并利用RCU实现延迟释放，包含栈内存管理和内存会计功能。",
          "similarity": 0.5652669668197632
        },
        {
          "chunk_id": 16,
          "file_path": "kernel/fork.c",
          "start_line": 3480,
          "end_line": 3518,
          "content": [
            "int unshare_files(void)",
            "{",
            "\tstruct task_struct *task = current;",
            "\tstruct files_struct *old, *copy = NULL;",
            "\tint error;",
            "",
            "\terror = unshare_fd(CLONE_FILES, &copy);",
            "\tif (error || !copy)",
            "\t\treturn error;",
            "",
            "\told = task->files;",
            "\ttask_lock(task);",
            "\ttask->files = copy;",
            "\ttask_unlock(task);",
            "\tput_files_struct(old);",
            "\treturn 0;",
            "}",
            "int sysctl_max_threads(struct ctl_table *table, int write,",
            "\t\t       void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tstruct ctl_table t;",
            "\tint ret;",
            "\tint threads = max_threads;",
            "\tint min = 1;",
            "\tint max = MAX_THREADS;",
            "",
            "\tt = *table;",
            "\tt.data = &threads;",
            "\tt.extra1 = &min;",
            "\tt.extra2 = &max;",
            "",
            "\tret = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);",
            "\tif (ret || !write)",
            "\t\treturn ret;",
            "",
            "\tmax_threads = threads;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "unshare_files, sysctl_max_threads",
          "description": "unshare_files函数复制当前进程的文件表结构并将其绑定到当前任务，sysctl_max_threads函数通过proc_dointvec_minmax接口限制系统最大线程数，支持读取和写入操作，其中写入时会更新全局max_threads变量。",
          "similarity": 0.5627849698066711
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/fork.c",
          "start_line": 2176,
          "end_line": 2282,
          "content": [
            "static void rv_task_fork(struct task_struct *p)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < RV_PER_TASK_MONITORS; i++)",
            "\t\tp->rv[i].da_mon.monitoring = false;",
            "}",
            "static inline void init_idle_pids(struct task_struct *idle)",
            "{",
            "\tenum pid_type type;",
            "",
            "\tfor (type = PIDTYPE_PID; type < PIDTYPE_MAX; ++type) {",
            "\t\tINIT_HLIST_NODE(&idle->pid_links[type]); /* not really needed */",
            "\t\tinit_task_pid(idle, type, &init_struct_pid);",
            "\t}",
            "}",
            "static int idle_dummy(void *dummy)",
            "{",
            "\t/* This function is never called */",
            "\treturn 0;",
            "}",
            "pid_t kernel_clone(struct kernel_clone_args *args)",
            "{",
            "\tu64 clone_flags = args->flags;",
            "\tstruct completion vfork;",
            "\tstruct pid *pid;",
            "\tstruct task_struct *p;",
            "\tint trace = 0;",
            "\tpid_t nr;",
            "",
            "\t/*",
            "\t * For legacy clone() calls, CLONE_PIDFD uses the parent_tid argument",
            "\t * to return the pidfd. Hence, CLONE_PIDFD and CLONE_PARENT_SETTID are",
            "\t * mutually exclusive. With clone3() CLONE_PIDFD has grown a separate",
            "\t * field in struct clone_args and it still doesn't make sense to have",
            "\t * them both point at the same memory location. Performing this check",
            "\t * here has the advantage that we don't need to have a separate helper",
            "\t * to check for legacy clone().",
            "\t */",
            "\tif ((args->flags & CLONE_PIDFD) &&",
            "\t    (args->flags & CLONE_PARENT_SETTID) &&",
            "\t    (args->pidfd == args->parent_tid))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Determine whether and which event to report to ptracer.  When",
            "\t * called from kernel_thread or CLONE_UNTRACED is explicitly",
            "\t * requested, no event is reported; otherwise, report if the event",
            "\t * for the type of forking is enabled.",
            "\t */",
            "\tif (!(clone_flags & CLONE_UNTRACED)) {",
            "\t\tif (clone_flags & CLONE_VFORK)",
            "\t\t\ttrace = PTRACE_EVENT_VFORK;",
            "\t\telse if (args->exit_signal != SIGCHLD)",
            "\t\t\ttrace = PTRACE_EVENT_CLONE;",
            "\t\telse",
            "\t\t\ttrace = PTRACE_EVENT_FORK;",
            "",
            "\t\tif (likely(!ptrace_event_enabled(current, trace)))",
            "\t\t\ttrace = 0;",
            "\t}",
            "",
            "\tp = copy_process(NULL, trace, NUMA_NO_NODE, args);",
            "\tadd_latent_entropy();",
            "",
            "\tif (IS_ERR(p))",
            "\t\treturn PTR_ERR(p);",
            "",
            "\t/*",
            "\t * Do this prior waking up the new thread - the thread pointer",
            "\t * might get invalid after that point, if the thread exits quickly.",
            "\t */",
            "\ttrace_sched_process_fork(current, p);",
            "",
            "\tpid = get_task_pid(p, PIDTYPE_PID);",
            "\tnr = pid_vnr(pid);",
            "",
            "\tif (clone_flags & CLONE_PARENT_SETTID)",
            "\t\tput_user(nr, args->parent_tid);",
            "",
            "\tif (clone_flags & CLONE_VFORK) {",
            "\t\tp->vfork_done = &vfork;",
            "\t\tinit_completion(&vfork);",
            "\t\tget_task_struct(p);",
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_LRU_GEN_WALKS_MMU) && !(clone_flags & CLONE_VM)) {",
            "\t\t/* lock the task to synchronize with memcg migration */",
            "\t\ttask_lock(p);",
            "\t\tlru_gen_add_mm(p->mm);",
            "\t\ttask_unlock(p);",
            "\t}",
            "",
            "\twake_up_new_task(p);",
            "",
            "\t/* forking complete and child started to run, tell ptracer */",
            "\tif (unlikely(trace))",
            "\t\tptrace_event_pid(trace, pid);",
            "",
            "\tif (clone_flags & CLONE_VFORK) {",
            "\t\tif (!wait_for_vfork_done(p, &vfork))",
            "\t\t\tptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);",
            "\t}",
            "",
            "\tput_pid(pid);",
            "\treturn nr;",
            "}"
          ],
          "function_name": "rv_task_fork, init_idle_pids, idle_dummy, kernel_clone",
          "description": "实现kernel_clone核心逻辑，创建新进程并处理克隆标志，管理子进程启动、vfork等待及进程树遍历，包含空闲任务PID初始化与RV监控器重置",
          "similarity": 0.5450263023376465
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/fork.c",
          "start_line": 549,
          "end_line": 652,
          "content": [
            "static void vm_area_free_rcu_cb(struct rcu_head *head)",
            "{",
            "\tstruct vm_area_struct *vma = container_of(head, struct vm_area_struct,",
            "\t\t\t\t\t\t  vm_rcu);",
            "",
            "\t/* The vma should not be locked while being destroyed. */",
            "\tVM_BUG_ON_VMA(rwsem_is_locked(&vma->vm_lock->lock), vma);",
            "\t__vm_area_free(vma);",
            "}",
            "void vm_area_free(struct vm_area_struct *vma)",
            "{",
            "#ifdef CONFIG_PER_VMA_LOCK",
            "\tcall_rcu(&vma->vm_rcu, vm_area_free_rcu_cb);",
            "#else",
            "\t__vm_area_free(vma);",
            "#endif",
            "}",
            "static void account_kernel_stack(struct task_struct *tsk, int account)",
            "{",
            "\tif (IS_ENABLED(CONFIG_VMAP_STACK)) {",
            "\t\tstruct vm_struct *vm = task_stack_vm_area(tsk);",
            "\t\tint i;",
            "",
            "\t\tfor (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++)",
            "\t\t\tmod_lruvec_page_state(vm->pages[i], NR_KERNEL_STACK_KB,",
            "\t\t\t\t\t      account * (PAGE_SIZE / 1024));",
            "\t} else {",
            "\t\tvoid *stack = task_stack_page(tsk);",
            "",
            "\t\t/* All stack pages are in the same node. */",
            "\t\tmod_lruvec_kmem_state(stack, NR_KERNEL_STACK_KB,",
            "\t\t\t\t      account * (THREAD_SIZE / 1024));",
            "\t}",
            "}",
            "void exit_task_stack_account(struct task_struct *tsk)",
            "{",
            "\taccount_kernel_stack(tsk, -1);",
            "",
            "\tif (IS_ENABLED(CONFIG_VMAP_STACK)) {",
            "\t\tstruct vm_struct *vm;",
            "\t\tint i;",
            "",
            "\t\tvm = task_stack_vm_area(tsk);",
            "\t\tfor (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++)",
            "\t\t\tmemcg_kmem_uncharge_page(vm->pages[i], 0);",
            "\t}",
            "}",
            "static void release_task_stack(struct task_struct *tsk)",
            "{",
            "\tif (WARN_ON(READ_ONCE(tsk->__state) != TASK_DEAD))",
            "\t\treturn;  /* Better to leak the stack than to free prematurely */",
            "",
            "\tfree_thread_stack(tsk);",
            "}",
            "void put_task_stack(struct task_struct *tsk)",
            "{",
            "\tif (refcount_dec_and_test(&tsk->stack_refcount))",
            "\t\trelease_task_stack(tsk);",
            "}",
            "void free_task(struct task_struct *tsk)",
            "{",
            "#ifdef CONFIG_SECCOMP",
            "\tWARN_ON_ONCE(tsk->seccomp.filter);",
            "#endif",
            "\trelease_user_cpus_ptr(tsk);",
            "\tscs_release(tsk);",
            "",
            "#ifndef CONFIG_THREAD_INFO_IN_TASK",
            "\t/*",
            "\t * The task is finally done with both the stack and thread_info,",
            "\t * so free both.",
            "\t */",
            "\trelease_task_stack(tsk);",
            "#else",
            "\t/*",
            "\t * If the task had a separate stack allocation, it should be gone",
            "\t * by now.",
            "\t */",
            "\tWARN_ON_ONCE(refcount_read(&tsk->stack_refcount) != 0);",
            "#endif",
            "\trt_mutex_debug_task_free(tsk);",
            "\tftrace_graph_exit_task(tsk);",
            "\tarch_release_task_struct(tsk);",
            "\tif (tsk->flags & PF_KTHREAD)",
            "\t\tfree_kthread_struct(tsk);",
            "\t#ifdef CONFIG_IEE",
            "\tiee_invalidate_token(tsk);",
            "\t#endif",
            "\tbpf_task_storage_free(tsk);",
            "\tfree_task_struct(tsk);",
            "}",
            "static void dup_mm_exe_file(struct mm_struct *mm, struct mm_struct *oldmm)",
            "{",
            "\tstruct file *exe_file;",
            "",
            "\texe_file = get_mm_exe_file(oldmm);",
            "\tRCU_INIT_POINTER(mm->exe_file, exe_file);",
            "\t/*",
            "\t * We depend on the oldmm having properly denied write access to the",
            "\t * exe_file already.",
            "\t */",
            "\tif (exe_file && deny_write_access(exe_file))",
            "\t\tpr_warn_once(\"deny_write_access() failed in %s\\n\", __func__);",
            "}"
          ],
          "function_name": "vm_area_free_rcu_cb, vm_area_free, account_kernel_stack, exit_task_stack_account, release_task_stack, put_task_stack, free_task, dup_mm_exe_file",
          "description": "处理虚拟内存区域(VMA)的释放及内核栈资源统计，通过RCU回调安全地销毁VMA结构体，维护内存使用跟踪并确保进程终止时正确释放关联资源。",
          "similarity": 0.5413312911987305
        }
      ]
    },
    {
      "source_file": "kernel/workqueue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:53:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workqueue.c`\n\n---\n\n# workqueue.c 技术文档\n\n## 1. 文件概述\n\n`workqueue.c` 是 Linux 内核中实现通用异步执行机制的核心文件，提供基于共享工作线程池（worker pool）的延迟任务调度功能。工作项（work items）在进程上下文中执行，支持 CPU 绑定和非绑定两种模式。每个 CPU 默认拥有两个标准工作池（普通优先级和高优先级），同时支持动态创建非绑定工作池以满足不同工作队列的需求。该机制替代了早期的 taskqueue/keventd 实现，具有更高的可扩展性和资源利用率。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct worker_pool`**  \n  工作线程池结构体，管理一组工作线程（workers），包含：\n  - `lock`：保护池状态的自旋锁\n  - `cpu` / `node`：关联的 CPU 和 NUMA 节点（绑定池）\n  - `worklist`：待处理工作项队列\n  - `idle_list` / `busy_hash`：空闲和忙碌工作线程的管理结构\n  - `nr_workers` / `nr_idle`：工作线程数量统计\n  - `attrs`：工作线程属性（如优先级、CPU 亲和性）\n  - `mayday_timer`：紧急情况下的救援请求定时器\n\n- **`struct pool_workqueue`**  \n  工作队列与工作池之间的关联结构，每个工作队列在每个池中都有一个对应的 `pool_workqueue` 实例，用于：\n  - 管理工作项的入队和执行\n  - 实现 `max_active` 限制（控制并发执行数）\n  - 支持 flush 操作（等待所有工作完成）\n  - 统计性能指标（如启动/完成次数、CPU 时间等）\n\n- **`struct worker`**（定义在 `workqueue_internal.h`）  \n  工作线程的运行时上下文，包含状态标志（如 `WORKER_IDLE`, `WORKER_UNBOUND`）、当前执行的工作项等。\n\n### 关键枚举与常量\n\n- **池/工作线程标志**：\n  - `POOL_DISASSOCIATED`：CPU 离线时池进入非绑定状态\n  - `WORKER_UNBOUND`：工作线程可在任意 CPU 上运行\n  - `WORKER_CPU_INTENSIVE`：标记 CPU 密集型任务，影响并发控制\n\n- **配置参数**：\n  - `NR_STD_WORKER_POOLS = 2`：每 CPU 标准池数量（普通 + 高优先级）\n  - `IDLE_WORKER_TIMEOUT = 300 * HZ`：空闲线程保留时间（5 分钟）\n  - `MAYDAY_INITIAL_TIMEOUT`：工作积压时触发救援的延迟（10ms）\n\n- **统计指标**（`pool_workqueue_stats`）：\n  - `PWQ_STAT_STARTED` / `PWQ_STAT_COMPLETED`：工作项执行统计\n  - `PWQ_STAT_MAYDAY` / `PWQ_STAT_RESCUED`：紧急救援事件计数\n\n## 3. 关键实现\n\n### 工作池管理\n- **绑定池（Bound Pool）**：与特定 CPU 关联，工作线程默认绑定到该 CPU。当 CPU 离线时，池进入 `DISASSOCIATED` 状态，工作线程转为非绑定模式。\n- **非绑定池（Unbound Pool）**：动态创建，通过哈希表（`unbound_pool_hash`）按属性（`workqueue_attrs`）去重，支持跨 CPU 调度。\n- **并发控制**：通过 `nr_running` 计数器和 `max_active` 限制，防止工作项过度并发执行。\n\n### 工作线程生命周期\n- **空闲管理**：空闲线程加入 `idle_list`，超时（`IDLE_WORKER_TIMEOUT`）后被回收。\n- **动态伸缩**：当工作积压时，通过 `mayday_timer` 触发新线程创建；若创建失败，向全局救援线程（rescuer）求助。\n- **状态标志**：使用位标志（如 `WORKER_IDLE`, `WORKER_PREP`）高效管理线程状态，避免锁竞争。\n\n### 内存与同步\n- **RCU 保护**：工作池销毁通过 RCU 延迟释放，确保 `get_work_pool()` 等读取路径无锁安全。\n- **锁分层**：\n  - `pool->lock`（自旋锁）：保护池内部状态\n  - `wq_pool_mutex`：全局池管理互斥锁\n  - `wq_pool_attach_mutex`：防止 CPU 绑定状态变更冲突\n\n### 工作项调度\n- **数据指针复用**：`work_struct->data` 的高有效位存储 `pool_workqueue` 指针，低有效位用于标志位（如 `WORK_STRUCT_INACTIVE`）。\n- **优先级支持**：高优先级工作池使用 `HIGHPRI_NICE_LEVEL = MIN_NICE` 提升调度优先级。\n\n## 4. 依赖关系\n\n- **内核子系统**：\n  - **调度器**（`<linux/sched.h>`）：创建工作线程（kworker），管理 CPU 亲和性\n  - **内存管理**（`<linux/slab.h>`）：分配工作池、工作队列等结构\n  - **CPU 热插拔**（`<linux/cpu.h>`）：处理 CPU 上下线时的池绑定状态切换\n  - **RCU**（`<linux/rculist.h>`）：实现无锁读取路径\n  - **定时器**（`<linux/timer.h>`）：实现空闲超时和救援机制\n\n- **内部依赖**：\n  - `workqueue_internal.h`：定义 `struct worker` 等内部结构\n  - `Documentation/core-api/workqueue.rst`：详细设计文档\n\n## 5. 使用场景\n\n- **驱动程序延迟操作**：硬件中断后调度下半部处理（如网络包处理、磁盘 I/O 完成回调）。\n- **内核子系统异步任务**：文件系统元数据更新、内存回收、电源管理状态切换。\n- **高优先级任务**：使用 `WQ_HIGHPRI` 标志创建工作队列，确保关键任务及时执行（如死锁恢复）。\n- **CPU 密集型任务**：标记 `WQ_CPU_INTENSIVE` 避免占用过多并发槽位，提升系统响应性。\n- **NUMA 感知调度**：非绑定工作队列可指定 NUMA 节点，优化内存访问延迟。",
      "similarity": 0.5721937417984009,
      "chunks": [
        {
          "chunk_id": 27,
          "file_path": "kernel/workqueue.c",
          "start_line": 5563,
          "end_line": 5664,
          "content": [
            "int workqueue_prepare_cpu(unsigned int cpu)",
            "{",
            "\tstruct worker_pool *pool;",
            "",
            "\tfor_each_cpu_worker_pool(pool, cpu) {",
            "\t\tif (pool->nr_workers)",
            "\t\t\tcontinue;",
            "\t\tif (!create_worker(pool))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}",
            "int workqueue_online_cpu(unsigned int cpu)",
            "{",
            "\tstruct worker_pool *pool;",
            "\tstruct workqueue_struct *wq;",
            "\tint pi;",
            "",
            "\tmutex_lock(&wq_pool_mutex);",
            "",
            "\tfor_each_pool(pool, pi) {",
            "\t\tmutex_lock(&wq_pool_attach_mutex);",
            "",
            "\t\tif (pool->cpu == cpu)",
            "\t\t\trebind_workers(pool);",
            "\t\telse if (pool->cpu < 0)",
            "\t\t\trestore_unbound_workers_cpumask(pool, cpu);",
            "",
            "\t\tmutex_unlock(&wq_pool_attach_mutex);",
            "\t}",
            "",
            "\t/* update pod affinity of unbound workqueues */",
            "\tlist_for_each_entry(wq, &workqueues, list) {",
            "\t\tstruct workqueue_attrs *attrs = wq->unbound_attrs;",
            "",
            "\t\tif (attrs) {",
            "\t\t\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);",
            "\t\t\tint tcpu;",
            "",
            "\t\t\tfor_each_cpu(tcpu, pt->pod_cpus[pt->cpu_pod[cpu]])",
            "\t\t\t\twq_update_pod(wq, tcpu, cpu, true);",
            "\t\t}",
            "\t}",
            "",
            "\tmutex_unlock(&wq_pool_mutex);",
            "\treturn 0;",
            "}",
            "int workqueue_offline_cpu(unsigned int cpu)",
            "{",
            "\tstruct workqueue_struct *wq;",
            "",
            "\t/* unbinding per-cpu workers should happen on the local CPU */",
            "\tif (WARN_ON(cpu != smp_processor_id()))",
            "\t\treturn -1;",
            "",
            "\tunbind_workers(cpu);",
            "",
            "\t/* update pod affinity of unbound workqueues */",
            "\tmutex_lock(&wq_pool_mutex);",
            "\tlist_for_each_entry(wq, &workqueues, list) {",
            "\t\tstruct workqueue_attrs *attrs = wq->unbound_attrs;",
            "",
            "\t\tif (attrs) {",
            "\t\t\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);",
            "\t\t\tint tcpu;",
            "",
            "\t\t\tfor_each_cpu(tcpu, pt->pod_cpus[pt->cpu_pod[cpu]])",
            "\t\t\t\twq_update_pod(wq, tcpu, cpu, false);",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&wq_pool_mutex);",
            "",
            "\treturn 0;",
            "}",
            "static void work_for_cpu_fn(struct work_struct *work)",
            "{",
            "\tstruct work_for_cpu *wfc = container_of(work, struct work_for_cpu, work);",
            "",
            "\twfc->ret = wfc->fn(wfc->arg);",
            "}",
            "long work_on_cpu_key(int cpu, long (*fn)(void *),",
            "\t\t     void *arg, struct lock_class_key *key)",
            "{",
            "\tstruct work_for_cpu wfc = { .fn = fn, .arg = arg };",
            "",
            "\tINIT_WORK_ONSTACK_KEY(&wfc.work, work_for_cpu_fn, key);",
            "\tschedule_work_on(cpu, &wfc.work);",
            "\tflush_work(&wfc.work);",
            "\tdestroy_work_on_stack(&wfc.work);",
            "\treturn wfc.ret;",
            "}",
            "long work_on_cpu_safe_key(int cpu, long (*fn)(void *),",
            "\t\t\t  void *arg, struct lock_class_key *key)",
            "{",
            "\tlong ret = -ENODEV;",
            "",
            "\tcpus_read_lock();",
            "\tif (cpu_online(cpu))",
            "\t\tret = work_on_cpu_key(cpu, fn, arg, key);",
            "\tcpus_read_unlock();",
            "\treturn ret;",
            "}"
          ],
          "function_name": "workqueue_prepare_cpu, workqueue_online_cpu, workqueue_offline_cpu, work_for_cpu_fn, work_on_cpu_key, work_on_cpu_safe_key",
          "description": "处理CPU事件生命周期，准备工作者、更新工作者池状态，并提供跨CPU任务执行接口以保证调度一致性",
          "similarity": 0.538686990737915
        },
        {
          "chunk_id": 21,
          "file_path": "kernel/workqueue.c",
          "start_line": 4591,
          "end_line": 4693,
          "content": [
            "static int alloc_and_link_pwqs(struct workqueue_struct *wq)",
            "{",
            "\tbool highpri = wq->flags & WQ_HIGHPRI;",
            "\tint cpu, ret;",
            "",
            "\twq->cpu_pwq = alloc_percpu(struct pool_workqueue *);",
            "\tif (!wq->cpu_pwq)",
            "\t\tgoto enomem;",
            "",
            "\tif (!(wq->flags & WQ_UNBOUND)) {",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tstruct pool_workqueue **pwq_p =",
            "\t\t\t\tper_cpu_ptr(wq->cpu_pwq, cpu);",
            "\t\t\tstruct worker_pool *pool =",
            "\t\t\t\t&(per_cpu_ptr(cpu_worker_pools, cpu)[highpri]);",
            "",
            "\t\t\t*pwq_p = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL,",
            "\t\t\t\t\t\t       pool->node);",
            "\t\t\tif (!*pwq_p)",
            "\t\t\t\tgoto enomem;",
            "",
            "\t\t\tinit_pwq(*pwq_p, wq, pool);",
            "",
            "\t\t\tmutex_lock(&wq->mutex);",
            "\t\t\tlink_pwq(*pwq_p);",
            "\t\t\tmutex_unlock(&wq->mutex);",
            "\t\t}",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tcpus_read_lock();",
            "\tif (wq->flags & __WQ_ORDERED) {",
            "\t\tret = apply_workqueue_attrs(wq, ordered_wq_attrs[highpri]);",
            "\t\t/* there should only be single pwq for ordering guarantee */",
            "\t\tWARN(!ret && (wq->pwqs.next != &wq->dfl_pwq->pwqs_node ||",
            "\t\t\t      wq->pwqs.prev != &wq->dfl_pwq->pwqs_node),",
            "\t\t     \"ordering guarantee broken for workqueue %s\\n\", wq->name);",
            "\t} else {",
            "\t\tret = apply_workqueue_attrs(wq, unbound_std_wq_attrs[highpri]);",
            "\t}",
            "\tcpus_read_unlock();",
            "",
            "\t/* for unbound pwq, flush the pwq_release_worker ensures that the",
            "\t * pwq_release_workfn() completes before calling kfree(wq).",
            "\t */",
            "\tif (ret)",
            "\t\tkthread_flush_worker(pwq_release_worker);",
            "",
            "\treturn ret;",
            "",
            "enomem:",
            "\tif (wq->cpu_pwq) {",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tstruct pool_workqueue *pwq = *per_cpu_ptr(wq->cpu_pwq, cpu);",
            "",
            "\t\t\tif (pwq)",
            "\t\t\t\tkmem_cache_free(pwq_cache, pwq);",
            "\t\t}",
            "\t\tfree_percpu(wq->cpu_pwq);",
            "\t\twq->cpu_pwq = NULL;",
            "\t}",
            "\treturn -ENOMEM;",
            "}",
            "static int wq_clamp_max_active(int max_active, unsigned int flags,",
            "\t\t\t       const char *name)",
            "{",
            "\tif (max_active < 1 || max_active > WQ_MAX_ACTIVE)",
            "\t\tpr_warn(\"workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\\n\",",
            "\t\t\tmax_active, name, 1, WQ_MAX_ACTIVE);",
            "",
            "\treturn clamp_val(max_active, 1, WQ_MAX_ACTIVE);",
            "}",
            "static int init_rescuer(struct workqueue_struct *wq)",
            "{",
            "\tstruct worker *rescuer;",
            "\tint ret;",
            "",
            "\tif (!(wq->flags & WQ_MEM_RECLAIM))",
            "\t\treturn 0;",
            "",
            "\trescuer = alloc_worker(NUMA_NO_NODE);",
            "\tif (!rescuer) {",
            "\t\tpr_err(\"workqueue: Failed to allocate a rescuer for wq \\\"%s\\\"\\n\",",
            "\t\t       wq->name);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\trescuer->rescue_wq = wq;",
            "\trescuer->task = kthread_create(rescuer_thread, rescuer, \"kworker/R-%s\", wq->name);",
            "\tif (IS_ERR(rescuer->task)) {",
            "\t\tret = PTR_ERR(rescuer->task);",
            "\t\tpr_err(\"workqueue: Failed to create a rescuer kthread for wq \\\"%s\\\": %pe\",",
            "\t\t       wq->name, ERR_PTR(ret));",
            "\t\tkfree(rescuer);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\twq->rescuer = rescuer;",
            "\tkthread_bind_mask(rescuer->task, cpu_possible_mask);",
            "\twake_up_process(rescuer->task);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "alloc_and_link_pwqs, wq_clamp_max_active, init_rescuer",
          "description": "包含分配并链接池工作队列的alloc_and_link_pwqs函数，wq_clamp_max_active用于限制最大活跃任务数范围，init_rescuer初始化救援线程以处理内存回收场景下的异常情况。",
          "similarity": 0.5362345576286316
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/workqueue.c",
          "start_line": 895,
          "end_line": 1037,
          "content": [
            "static inline void worker_clr_flags(struct worker *worker, unsigned int flags)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "\tunsigned int oflags = worker->flags;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\tworker->flags &= ~flags;",
            "",
            "\t/*",
            "\t * If transitioning out of NOT_RUNNING, increment nr_running.  Note",
            "\t * that the nested NOT_RUNNING is not a noop.  NOT_RUNNING is mask",
            "\t * of multiple flags, not a single flag.",
            "\t */",
            "\tif ((flags & WORKER_NOT_RUNNING) && (oflags & WORKER_NOT_RUNNING))",
            "\t\tif (!(worker->flags & WORKER_NOT_RUNNING))",
            "\t\t\tpool->nr_running++;",
            "}",
            "static void worker_enter_idle(struct worker *worker)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tif (WARN_ON_ONCE(worker->flags & WORKER_IDLE) ||",
            "\t    WARN_ON_ONCE(!list_empty(&worker->entry) &&",
            "\t\t\t (worker->hentry.next || worker->hentry.pprev)))",
            "\t\treturn;",
            "",
            "\t/* can't use worker_set_flags(), also called from create_worker() */",
            "\tworker->flags |= WORKER_IDLE;",
            "\tpool->nr_idle++;",
            "\tworker->last_active = jiffies;",
            "",
            "\t/* idle_list is LIFO */",
            "\tlist_add(&worker->entry, &pool->idle_list);",
            "",
            "\tif (too_many_workers(pool) && !timer_pending(&pool->idle_timer))",
            "\t\tmod_timer(&pool->idle_timer, jiffies + IDLE_WORKER_TIMEOUT);",
            "",
            "\t/* Sanity check nr_running. */",
            "\tWARN_ON_ONCE(pool->nr_workers == pool->nr_idle && pool->nr_running);",
            "}",
            "static void worker_leave_idle(struct worker *worker)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tif (WARN_ON_ONCE(!(worker->flags & WORKER_IDLE)))",
            "\t\treturn;",
            "\tworker_clr_flags(worker, WORKER_IDLE);",
            "\tpool->nr_idle--;",
            "\tlist_del_init(&worker->entry);",
            "}",
            "static void move_linked_works(struct work_struct *work, struct list_head *head,",
            "\t\t\t      struct work_struct **nextp)",
            "{",
            "\tstruct work_struct *n;",
            "",
            "\t/*",
            "\t * Linked worklist will always end before the end of the list,",
            "\t * use NULL for list head.",
            "\t */",
            "\tlist_for_each_entry_safe_from(work, n, NULL, entry) {",
            "\t\tlist_move_tail(&work->entry, head);",
            "\t\tif (!(*work_data_bits(work) & WORK_STRUCT_LINKED))",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\t/*",
            "\t * If we're already inside safe list traversal and have moved",
            "\t * multiple works to the scheduled queue, the next position",
            "\t * needs to be updated.",
            "\t */",
            "\tif (nextp)",
            "\t\t*nextp = n;",
            "}",
            "static bool assign_work(struct work_struct *work, struct worker *worker,",
            "\t\t\tstruct work_struct **nextp)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "\tstruct worker *collision;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\t/*",
            "\t * A single work shouldn't be executed concurrently by multiple workers.",
            "\t * __queue_work() ensures that @work doesn't jump to a different pool",
            "\t * while still running in the previous pool. Here, we should ensure that",
            "\t * @work is not executed concurrently by multiple workers from the same",
            "\t * pool. Check whether anyone is already processing the work. If so,",
            "\t * defer the work to the currently executing one.",
            "\t */",
            "\tcollision = find_worker_executing_work(pool, work);",
            "\tif (unlikely(collision)) {",
            "\t\tmove_linked_works(work, &collision->scheduled, nextp);",
            "\t\treturn false;",
            "\t}",
            "",
            "\tmove_linked_works(work, &worker->scheduled, nextp);",
            "\treturn true;",
            "}",
            "static bool kick_pool(struct worker_pool *pool)",
            "{",
            "\tstruct worker *worker = first_idle_worker(pool);",
            "\tstruct task_struct *p;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\tif (!need_more_worker(pool) || !worker)",
            "\t\treturn false;",
            "",
            "\tp = worker->task;",
            "",
            "#ifdef CONFIG_SMP",
            "\t/*",
            "\t * Idle @worker is about to execute @work and waking up provides an",
            "\t * opportunity to migrate @worker at a lower cost by setting the task's",
            "\t * wake_cpu field. Let's see if we want to move @worker to improve",
            "\t * execution locality.",
            "\t *",
            "\t * We're waking the worker that went idle the latest and there's some",
            "\t * chance that @worker is marked idle but hasn't gone off CPU yet. If",
            "\t * so, setting the wake_cpu won't do anything. As this is a best-effort",
            "\t * optimization and the race window is narrow, let's leave as-is for",
            "\t * now. If this becomes pronounced, we can skip over workers which are",
            "\t * still on cpu when picking an idle worker.",
            "\t *",
            "\t * If @pool has non-strict affinity, @worker might have ended up outside",
            "\t * its affinity scope. Repatriate.",
            "\t */",
            "\tif (!pool->attrs->affn_strict &&",
            "\t    !cpumask_test_cpu(p->wake_cpu, pool->attrs->__pod_cpumask)) {",
            "\t\tstruct work_struct *work = list_first_entry(&pool->worklist,",
            "\t\t\t\t\t\tstruct work_struct, entry);",
            "\t\tint wake_cpu = cpumask_any_and_distribute(pool->attrs->__pod_cpumask,",
            "\t\t\t\t\t\t\t  cpu_online_mask);",
            "\t\tif (wake_cpu < nr_cpu_ids) {",
            "\t\t\tp->wake_cpu = wake_cpu;",
            "\t\t\tget_work_pwq(work)->stats[PWQ_STAT_REPATRIATED]++;",
            "\t\t}",
            "\t}",
            "#endif",
            "\twake_up_process(p);",
            "\treturn true;",
            "}"
          ],
          "function_name": "worker_clr_flags, worker_enter_idle, worker_leave_idle, move_linked_works, assign_work, kick_pool",
          "description": "实现工作者线程空闲状态切换和工作项迁移机制，包含空闲工作者列表管理、链接工作项批量转移功能，以及唤醒工作者线程的调度逻辑，支持跨CPU亲和性迁移优化，确保工作项正确分发到可用工作者线程。",
          "similarity": 0.5273326635360718
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/workqueue.c",
          "start_line": 692,
          "end_line": 797,
          "content": [
            "static void set_work_pool_and_clear_pending(struct work_struct *work,",
            "\t\t\t\t\t    int pool_id)",
            "{",
            "\t/*",
            "\t * The following wmb is paired with the implied mb in",
            "\t * test_and_set_bit(PENDING) and ensures all updates to @work made",
            "\t * here are visible to and precede any updates by the next PENDING",
            "\t * owner.",
            "\t */",
            "\tsmp_wmb();",
            "\tset_work_data(work, (unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT, 0);",
            "\t/*",
            "\t * The following mb guarantees that previous clear of a PENDING bit",
            "\t * will not be reordered with any speculative LOADS or STORES from",
            "\t * work->current_func, which is executed afterwards.  This possible",
            "\t * reordering can lead to a missed execution on attempt to queue",
            "\t * the same @work.  E.g. consider this case:",
            "\t *",
            "\t *   CPU#0                         CPU#1",
            "\t *   ----------------------------  --------------------------------",
            "\t *",
            "\t * 1  STORE event_indicated",
            "\t * 2  queue_work_on() {",
            "\t * 3    test_and_set_bit(PENDING)",
            "\t * 4 }                             set_..._and_clear_pending() {",
            "\t * 5                                 set_work_data() # clear bit",
            "\t * 6                                 smp_mb()",
            "\t * 7                               work->current_func() {",
            "\t * 8\t\t\t\t      LOAD event_indicated",
            "\t *\t\t\t\t   }",
            "\t *",
            "\t * Without an explicit full barrier speculative LOAD on line 8 can",
            "\t * be executed before CPU#0 does STORE on line 1.  If that happens,",
            "\t * CPU#0 observes the PENDING bit is still set and new execution of",
            "\t * a @work is not queued in a hope, that CPU#1 will eventually",
            "\t * finish the queued @work.  Meanwhile CPU#1 does not see",
            "\t * event_indicated is set, because speculative LOAD was executed",
            "\t * before actual STORE.",
            "\t */",
            "\tsmp_mb();",
            "}",
            "static void clear_work_data(struct work_struct *work)",
            "{",
            "\tsmp_wmb();\t/* see set_work_pool_and_clear_pending() */",
            "\tset_work_data(work, WORK_STRUCT_NO_POOL, 0);",
            "}",
            "static int get_work_pool_id(struct work_struct *work)",
            "{",
            "\tunsigned long data = atomic_long_read(&work->data);",
            "",
            "\tif (data & WORK_STRUCT_PWQ)",
            "\t\treturn work_struct_pwq(data)->pool->id;",
            "",
            "\treturn data >> WORK_OFFQ_POOL_SHIFT;",
            "}",
            "static void mark_work_canceling(struct work_struct *work)",
            "{",
            "\tunsigned long pool_id = get_work_pool_id(work);",
            "",
            "\tpool_id <<= WORK_OFFQ_POOL_SHIFT;",
            "\tset_work_data(work, pool_id | WORK_OFFQ_CANCELING, WORK_STRUCT_PENDING);",
            "}",
            "static bool work_is_canceling(struct work_struct *work)",
            "{",
            "\tunsigned long data = atomic_long_read(&work->data);",
            "",
            "\treturn !(data & WORK_STRUCT_PWQ) && (data & WORK_OFFQ_CANCELING);",
            "}",
            "static bool need_more_worker(struct worker_pool *pool)",
            "{",
            "\treturn !list_empty(&pool->worklist) && !pool->nr_running;",
            "}",
            "static bool may_start_working(struct worker_pool *pool)",
            "{",
            "\treturn pool->nr_idle;",
            "}",
            "static bool keep_working(struct worker_pool *pool)",
            "{",
            "\treturn !list_empty(&pool->worklist) && (pool->nr_running <= 1);",
            "}",
            "static bool need_to_create_worker(struct worker_pool *pool)",
            "{",
            "\treturn need_more_worker(pool) && !may_start_working(pool);",
            "}",
            "static bool too_many_workers(struct worker_pool *pool)",
            "{",
            "\tbool managing = pool->flags & POOL_MANAGER_ACTIVE;",
            "\tint nr_idle = pool->nr_idle + managing; /* manager is considered idle */",
            "\tint nr_busy = pool->nr_workers - nr_idle;",
            "",
            "\treturn nr_idle > 2 && (nr_idle - 2) * MAX_IDLE_WORKERS_RATIO >= nr_busy;",
            "}",
            "static inline void worker_set_flags(struct worker *worker, unsigned int flags)",
            "{",
            "\tstruct worker_pool *pool = worker->pool;",
            "",
            "\tlockdep_assert_held(&pool->lock);",
            "",
            "\t/* If transitioning into NOT_RUNNING, adjust nr_running. */",
            "\tif ((flags & WORKER_NOT_RUNNING) &&",
            "\t    !(worker->flags & WORKER_NOT_RUNNING)) {",
            "\t\tpool->nr_running--;",
            "\t}",
            "",
            "\tworker->flags |= flags;",
            "}"
          ],
          "function_name": "set_work_pool_and_clear_pending, clear_work_data, get_work_pool_id, mark_work_canceling, work_is_canceling, need_more_worker, may_start_working, keep_working, need_to_create_worker, too_many_workers, worker_set_flags",
          "description": "实现工作者线程池状态控制逻辑，包含需要创建新工作者的判断条件、工作者空闲状态管理、工作项冲突检测及任务分配函数，通过锁保护保证池状态一致性，维护工作者线程与待处理工作项的匹配关系。",
          "similarity": 0.5168426632881165
        },
        {
          "chunk_id": 29,
          "file_path": "kernel/workqueue.c",
          "start_line": 5864,
          "end_line": 5966,
          "content": [
            "int workqueue_unbound_exclude_cpumask(cpumask_var_t exclude_cpumask)",
            "{",
            "\tcpumask_var_t cpumask;",
            "\tint ret = 0;",
            "",
            "\tif (!zalloc_cpumask_var(&cpumask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tlockdep_assert_cpus_held();",
            "\tmutex_lock(&wq_pool_mutex);",
            "",
            "\t/* Save the current isolated cpumask & export it via sysfs */",
            "\tcpumask_copy(wq_isolated_cpumask, exclude_cpumask);",
            "",
            "\t/*",
            "\t * If the operation fails, it will fall back to",
            "\t * wq_requested_unbound_cpumask which is initially set to",
            "\t * (HK_TYPE_WQ ∩ HK_TYPE_DOMAIN) house keeping mask and rewritten",
            "\t * by any subsequent write to workqueue/cpumask sysfs file.",
            "\t */",
            "\tif (!cpumask_andnot(cpumask, wq_requested_unbound_cpumask, exclude_cpumask))",
            "\t\tcpumask_copy(cpumask, wq_requested_unbound_cpumask);",
            "\tif (!cpumask_equal(cpumask, wq_unbound_cpumask))",
            "\t\tret = workqueue_apply_unbound_cpumask(cpumask);",
            "",
            "\tmutex_unlock(&wq_pool_mutex);",
            "\tfree_cpumask_var(cpumask);",
            "\treturn ret;",
            "}",
            "static int parse_affn_scope(const char *val)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < ARRAY_SIZE(wq_affn_names); i++) {",
            "\t\tif (!strncasecmp(val, wq_affn_names[i], strlen(wq_affn_names[i])))",
            "\t\t\treturn i;",
            "\t}",
            "\treturn -EINVAL;",
            "}",
            "static int wq_affn_dfl_set(const char *val, const struct kernel_param *kp)",
            "{",
            "\tstruct workqueue_struct *wq;",
            "\tint affn, cpu;",
            "",
            "\taffn = parse_affn_scope(val);",
            "\tif (affn < 0)",
            "\t\treturn affn;",
            "\tif (affn == WQ_AFFN_DFL)",
            "\t\treturn -EINVAL;",
            "",
            "\tcpus_read_lock();",
            "\tmutex_lock(&wq_pool_mutex);",
            "",
            "\twq_affn_dfl = affn;",
            "",
            "\tlist_for_each_entry(wq, &workqueues, list) {",
            "\t\tfor_each_online_cpu(cpu) {",
            "\t\t\twq_update_pod(wq, cpu, cpu, true);",
            "\t\t}",
            "\t}",
            "",
            "\tmutex_unlock(&wq_pool_mutex);",
            "\tcpus_read_unlock();",
            "",
            "\treturn 0;",
            "}",
            "static int wq_affn_dfl_get(char *buffer, const struct kernel_param *kp)",
            "{",
            "\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\n\", wq_affn_names[wq_affn_dfl]);",
            "}",
            "static ssize_t per_cpu_show(struct device *dev, struct device_attribute *attr,",
            "\t\t\t    char *buf)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "",
            "\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", (bool)!(wq->flags & WQ_UNBOUND));",
            "}",
            "static ssize_t max_active_show(struct device *dev,",
            "\t\t\t       struct device_attribute *attr, char *buf)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "",
            "\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", wq->saved_max_active);",
            "}",
            "static ssize_t max_active_store(struct device *dev,",
            "\t\t\t\tstruct device_attribute *attr, const char *buf,",
            "\t\t\t\tsize_t count)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "\tint val;",
            "",
            "\tif (sscanf(buf, \"%d\", &val) != 1 || val <= 0)",
            "\t\treturn -EINVAL;",
            "",
            "\tworkqueue_set_max_active(wq, val);",
            "\treturn count;",
            "}",
            "static void apply_wqattrs_lock(void)",
            "{",
            "\t/* CPUs should stay stable across pwq creations and installations */",
            "\tcpus_read_lock();",
            "\tmutex_lock(&wq_pool_mutex);",
            "}"
          ],
          "function_name": "workqueue_unbound_exclude_cpumask, parse_affn_scope, wq_affn_dfl_set, wq_affn_dfl_get, per_cpu_show, max_active_show, max_active_store, apply_wqattrs_lock",
          "description": "配置非绑定工作者的CPU排除掩码和默认亲和性策略，暴露工作队列属性供sysfs访问并管理最大并发数参数",
          "similarity": 0.510380744934082
        }
      ]
    },
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.5719077587127686,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.5610305070877075
        },
        {
          "chunk_id": 11,
          "file_path": "mm/mempolicy.c",
          "start_line": 1855,
          "end_line": 1971,
          "content": [
            "static int kernel_get_mempolicy(int __user *policy,",
            "\t\t\t\tunsigned long __user *nmask,",
            "\t\t\t\tunsigned long maxnode,",
            "\t\t\t\tunsigned long addr,",
            "\t\t\t\tunsigned long flags)",
            "{",
            "\tint err;",
            "\tint pval;",
            "\tnodemask_t nodes;",
            "",
            "\tif (nmask != NULL && maxnode < nr_node_ids)",
            "\t\treturn -EINVAL;",
            "",
            "\taddr = untagged_addr(addr);",
            "",
            "\terr = do_get_mempolicy(&pval, &nodes, addr, flags);",
            "",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (policy && put_user(pval, policy))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (nmask)",
            "\t\terr = copy_nodes_to_user(nmask, maxnode, &nodes);",
            "",
            "\treturn err;",
            "}",
            "bool vma_migratable(struct vm_area_struct *vma)",
            "{",
            "\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * DAX device mappings require predictable access latency, so avoid",
            "\t * incurring periodic faults.",
            "\t */",
            "\tif (vma_is_dax(vma))",
            "\t\treturn false;",
            "",
            "\tif (is_vm_hugetlb_page(vma) &&",
            "\t\t!hugepage_migration_supported(hstate_vma(vma)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Migration allocates pages in the highest zone. If we cannot",
            "\t * do so then migration (at least from node to node) is not",
            "\t * possible.",
            "\t */",
            "\tif (vma->vm_file &&",
            "\t\tgfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))",
            "\t\t\t< policy_zone)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "bool vma_policy_mof(struct vm_area_struct *vma)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->get_policy) {",
            "\t\tbool ret = false;",
            "\t\tpgoff_t ilx;\t\t/* ignored here */",
            "",
            "\t\tpol = vma->vm_ops->get_policy(vma, vma->vm_start, &ilx);",
            "\t\tif (pol && (pol->flags & MPOL_F_MOF))",
            "\t\t\tret = true;",
            "\t\tmpol_cond_put(pol);",
            "",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tpol = vma->vm_policy;",
            "\tif (!pol)",
            "\t\tpol = get_task_policy(current);",
            "",
            "\treturn pol->flags & MPOL_F_MOF;",
            "}",
            "bool apply_policy_zone(struct mempolicy *policy, enum zone_type zone)",
            "{",
            "\tenum zone_type dynamic_policy_zone = policy_zone;",
            "",
            "\tBUG_ON(dynamic_policy_zone == ZONE_MOVABLE);",
            "",
            "\t/*",
            "\t * if policy->nodes has movable memory only,",
            "\t * we apply policy when gfp_zone(gfp) = ZONE_MOVABLE only.",
            "\t *",
            "\t * policy->nodes is intersect with node_states[N_MEMORY].",
            "\t * so if the following test fails, it implies",
            "\t * policy->nodes has movable memory only.",
            "\t */",
            "\tif (!nodes_intersects(policy->nodes, node_states[N_HIGH_MEMORY]))",
            "\t\tdynamic_policy_zone = ZONE_MOVABLE;",
            "",
            "\treturn zone >= dynamic_policy_zone;",
            "}",
            "static unsigned int weighted_interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int node;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "retry:",
            "\t/* to prevent miscount use tsk->mems_allowed_seq to detect rebind */",
            "\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\tnode = current->il_prev;",
            "\tif (!current->il_weight || !node_isset(node, policy->nodes)) {",
            "\t\tnode = next_node_in(node, policy->nodes);",
            "\t\tif (read_mems_allowed_retry(cpuset_mems_cookie))",
            "\t\t\tgoto retry;",
            "\t\tif (node == MAX_NUMNODES)",
            "\t\t\treturn node;",
            "\t\tcurrent->il_prev = node;",
            "\t\tcurrent->il_weight = get_il_weight(node);",
            "\t}",
            "\tcurrent->il_weight--;",
            "\treturn node;",
            "}"
          ],
          "function_name": "kernel_get_mempolicy, vma_migratable, vma_policy_mof, apply_policy_zone, weighted_interleave_nodes",
          "description": "kernel_get_mempolicy 获取当前内存策略参数并复制到用户空间；vma_migratable 判断虚拟内存区域是否支持迁移；vma_policy_mof 检查VMA是否启用了MOF（Migration On Fault）策略；apply_policy_zone 确定当前zone是否满足策略要求；weighted_interleave_nodes 计算加权交错分配的目标节点。",
          "similarity": 0.5469333529472351
        },
        {
          "chunk_id": 10,
          "file_path": "mm/mempolicy.c",
          "start_line": 1735,
          "end_line": 1838,
          "content": [
            "static long kernel_set_mempolicy(int mode, const unsigned long __user *nmask,",
            "\t\t\t\t unsigned long maxnode)",
            "{",
            "\tunsigned short mode_flags;",
            "\tnodemask_t nodes;",
            "\tint lmode = mode;",
            "\tint err;",
            "",
            "\terr = sanitize_mpol_flags(&lmode, &mode_flags);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\terr = get_nodes(&nodes, nmask, maxnode);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\treturn do_set_mempolicy(lmode, mode_flags, &nodes);",
            "}",
            "static int kernel_migrate_pages(pid_t pid, unsigned long maxnode,",
            "\t\t\t\tconst unsigned long __user *old_nodes,",
            "\t\t\t\tconst unsigned long __user *new_nodes)",
            "{",
            "\tstruct mm_struct *mm = NULL;",
            "\tstruct task_struct *task;",
            "\tnodemask_t task_nodes;",
            "\tint err;",
            "\tnodemask_t *old;",
            "\tnodemask_t *new;",
            "\tNODEMASK_SCRATCH(scratch);",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\told = &scratch->mask1;",
            "\tnew = &scratch->mask2;",
            "",
            "\terr = get_nodes(old, old_nodes, maxnode);",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\terr = get_nodes(new, new_nodes, maxnode);",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\t/* Find the mm_struct */",
            "\trcu_read_lock();",
            "\ttask = pid ? find_task_by_vpid(pid) : current;",
            "\tif (!task) {",
            "\t\trcu_read_unlock();",
            "\t\terr = -ESRCH;",
            "\t\tgoto out;",
            "\t}",
            "\tget_task_struct(task);",
            "",
            "\terr = -EINVAL;",
            "",
            "\t/*",
            "\t * Check if this process has the right to modify the specified process.",
            "\t * Use the regular \"ptrace_may_access()\" checks.",
            "\t */",
            "\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {",
            "\t\trcu_read_unlock();",
            "\t\terr = -EPERM;",
            "\t\tgoto out_put;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\ttask_nodes = cpuset_mems_allowed(task);",
            "\t/* Is the user allowed to access the target nodes? */",
            "\tif (!nodes_subset(*new, task_nodes) && !capable(CAP_SYS_NICE)) {",
            "\t\terr = -EPERM;",
            "\t\tgoto out_put;",
            "\t}",
            "",
            "\ttask_nodes = cpuset_mems_allowed(current);",
            "\tnodes_and(*new, *new, task_nodes);",
            "\tif (nodes_empty(*new))",
            "\t\tgoto out_put;",
            "",
            "\terr = security_task_movememory(task);",
            "\tif (err)",
            "\t\tgoto out_put;",
            "",
            "\tmm = get_task_mm(task);",
            "\tput_task_struct(task);",
            "",
            "\tif (!mm) {",
            "\t\terr = -EINVAL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\terr = do_migrate_pages(mm, old, new,",
            "\t\tcapable(CAP_SYS_NICE) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE);",
            "",
            "\tmmput(mm);",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "",
            "\treturn err;",
            "",
            "out_put:",
            "\tput_task_struct(task);",
            "\tgoto out;",
            "}"
          ],
          "function_name": "kernel_set_mempolicy, kernel_migrate_pages",
          "description": "kernel_set_mempolicy 设置进程的内存放置策略，通过sanitize_mpol_flags验证模式标志并调用do_set_mempolicy应用策略；kernel_migrate_pages 实现页面迁移，检查目标进程权限，限制迁移节点范围，并调用do_migrate_pages进行实际迁移操作。",
          "similarity": 0.5453531742095947
        },
        {
          "chunk_id": 13,
          "file_path": "mm/mempolicy.c",
          "start_line": 2149,
          "end_line": 2255,
          "content": [
            "static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nnodes;",
            "\tint i;",
            "\tint nid;",
            "",
            "\tnnodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nnodes)",
            "\t\treturn numa_node_id();",
            "\ttarget = ilx % nnodes;",
            "\tnid = first_node(nodemask);",
            "\tfor (i = 0; i < target; i++)",
            "\t\tnid = next_node(nid, nodemask);",
            "\treturn nid;",
            "}",
            "int huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,",
            "\t\tstruct mempolicy **mpol, nodemask_t **nodemask)",
            "{",
            "\tpgoff_t ilx;",
            "\tint nid;",
            "",
            "\tnid = numa_node_id();",
            "\t*mpol = get_vma_policy(vma, addr, hstate_vma(vma)->order, &ilx);",
            "\t*nodemask = policy_nodemask(gfp_flags, *mpol, ilx, &nid);",
            "\treturn nid;",
            "}",
            "bool init_nodemask_of_mempolicy(nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "",
            "\tif (!(mask && current->mempolicy))",
            "\t\treturn false;",
            "",
            "\ttask_lock(current);",
            "\tmempolicy = current->mempolicy;",
            "\tswitch (mempolicy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*mask = mempolicy->nodes;",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tinit_nodemask_of_node(mask, numa_node_id());",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "\ttask_unlock(current);",
            "",
            "\treturn true;",
            "}",
            "bool mempolicy_in_oom_domain(struct task_struct *tsk,",
            "\t\t\t\t\tconst nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "\tbool ret = true;",
            "",
            "\tif (!mask)",
            "\t\treturn ret;",
            "",
            "\ttask_lock(tsk);",
            "\tmempolicy = tsk->mempolicy;",
            "\tif (mempolicy && mempolicy->mode == MPOL_BIND)",
            "\t\tret = nodes_intersects(mempolicy->nodes, *mask);",
            "\ttask_unlock(tsk);",
            "",
            "\treturn ret;",
            "}",
            "static unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tint nodes;",
            "\tunsigned long nr_pages_per_node;",
            "\tint delta;",
            "\tint i;",
            "\tunsigned long nr_allocated;",
            "\tunsigned long total_allocated = 0;",
            "",
            "\tnodes = nodes_weight(pol->nodes);",
            "\tnr_pages_per_node = nr_pages / nodes;",
            "\tdelta = nr_pages - nodes * nr_pages_per_node;",
            "",
            "\tfor (i = 0; i < nodes; i++) {",
            "\t\tif (delta) {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node + 1, NULL,",
            "\t\t\t\t\tpage_array);",
            "\t\t\tdelta--;",
            "\t\t} else {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node, NULL, page_array);",
            "\t\t}",
            "",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t}",
            "",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "interleave_nid, huge_node, init_nodemask_of_mempolicy, mempolicy_in_oom_domain, alloc_pages_bulk_array_interleave",
          "description": "interleave_nid 计算简单交错分配的目标节点；huge_node 结合HugeTLB策略确定大页分配节点；init_nodemask_of_mempolicy 初始化当前进程的内存策略节点掩码；mempolicy_in_oom_domain 检查策略节点是否与OOM域重叠；alloc_pages_bulk_array_interleave 执行批量交错分配。",
          "similarity": 0.5377941131591797
        },
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.5286272168159485
        }
      ]
    }
  ]
}