{
  "query": "priority scheduling",
  "timestamp": "2025-12-25 23:24:11",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/deadline.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:06:40\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\deadline.c`\n\n---\n\n# `sched/deadline.c` 技术文档\n\n## 1. 文件概述\n\n`sched/deadline.c` 是 Linux 内核调度器中 **SCHED_DEADLINE** 调度类的核心实现文件。该调度类基于 **最早截止时间优先（Earliest Deadline First, EDF）** 算法，并结合 **恒定带宽服务器（Constant Bandwidth Server, CBS）** 机制，为具有严格实时性要求的任务提供可预测的调度保障。\n\n其核心目标是：  \n- 对于周期性任务，若其实际运行时间不超过所申请的运行时间（runtime），则保证不会错过任何截止时间（deadline）；  \n- 对于非周期性任务、突发任务或试图超出其预留带宽的任务，系统会对其进行节流（throttling），防止其影响其他任务的实时性保障。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_dl_entity`：表示一个 deadline 调度实体，包含任务的运行时间（runtime）、截止期限（deadline）、周期（period）、带宽（dl_bw）等关键参数。\n- `struct dl_rq`：每个 CPU 的 deadline 运行队列，维护该 CPU 上所有 deadline 任务的红黑树、当前带宽使用情况（`this_bw`、`running_bw`）等。\n- `struct dl_bw`：deadline 带宽管理结构，用于跟踪系统或调度域中已分配的总带宽（`total_bw`）。\n\n### 主要函数与辅助宏\n\n#### 调度实体与运行队列关联\n- `dl_task_of(dl_se)`：从 `sched_dl_entity` 获取对应的 `task_struct`（仅适用于普通任务，不适用于服务器实体）。\n- `rq_of_dl_rq(dl_rq)` / `rq_of_dl_se(dl_se)`：获取与 deadline 运行队列或调度实体关联的 `rq`（runqueue）。\n- `dl_rq_of_se(dl_se)`：获取调度实体所属的 `dl_rq`。\n- `on_dl_rq(dl_se)`：判断调度实体是否已在 deadline 运行队列中（通过红黑树节点是否为空判断）。\n\n#### 优先级继承（PI）支持（`CONFIG_RT_MUTEXES`）\n- `pi_of(dl_se)`：获取当前调度实体因优先级继承而提升后的“代理”实体。\n- `is_dl_boosted(dl_se)`：判断该 deadline 实体是否因优先级继承被提升。\n\n#### 带宽管理（SMP 与 UP 差异处理）\n- `dl_bw_of(cpu)`：获取指定 CPU 所属调度域（或本地）的 `dl_bw` 结构。\n- `dl_bw_cpus(cpu)`：返回该 CPU 所在调度域中活跃 CPU 的数量。\n- `dl_bw_capacity(cpu)`：计算调度域的总 CPU 容量（考虑异构 CPU 的 `arch_scale_cpu_capacity`）。\n- `__dl_add()` / `__dl_sub()`：向带宽池中添加或移除任务带宽，并更新 `extra_bw`（用于负载均衡）。\n- `__dl_overflow()`：检查新增带宽是否超出系统/调度域的可用带宽上限。\n\n#### 运行时带宽跟踪\n- `__add_running_bw()` / `__sub_running_bw()`：更新 `dl_rq->running_bw`（当前正在运行的 deadline 任务所消耗的带宽）。\n- `__add_rq_bw()` / `__sub_rq_bw()`：更新 `dl_rq->this_bw`（该运行队列上所有 deadline 任务的总预留带宽）。\n- `add_running_bw()` / `sub_running_bw()` / `add_rq_bw()` / `sub_rq_bw()`：带宽操作的封装，跳过“特殊”调度实体（如服务器）。\n\n#### 其他\n- `dl_server(dl_se)`：判断调度实体是否为 CBS 服务器（而非普通任务）。\n- `dl_bw_visited(cpu, gen)`：用于带宽遍历去重（SMP 场景）。\n\n### 系统控制接口（`CONFIG_SYSCTL`）\n- `sched_deadline_period_max_us`：deadline 任务周期上限（默认 ~4 秒）。\n- `sched_deadline_period_min_us`：deadline 任务周期下限（默认 100 微秒），防止定时器 DoS。\n\n## 3. 关键实现\n\n### EDF + CBS 调度模型\n- 每个 deadline 任务通过 `runtime`、`deadline`、`period` 三个参数定义其资源需求。\n- 调度器按 **绝对截止时间（absolute deadline）** 对任务排序，使用红黑树实现 O(log n) 的调度决策。\n- CBS 机制确保任务即使突发执行，也不会长期占用超过其 `runtime/period` 的 CPU 带宽，超限任务会被 throttled。\n\n### 带宽隔离与全局限制\n- 在 SMP 系统中，deadline 带宽按 **调度域（root domain）** 进行管理，防止跨 CPU 的带宽滥用。\n- 总带宽限制默认为 CPU 总容量的 95%（由 `sysctl_sched_util_clamp_min` 等机制间接控制，具体限制逻辑在带宽分配函数中体现）。\n- `dl_bw->total_bw` 跟踪已分配带宽，`__dl_overflow()` 用于在任务加入时检查是否超限。\n\n### 异构 CPU 支持\n- 通过 `arch_scale_cpu_capacity()` 获取每个 CPU 的相对性能权重。\n- `dl_bw_capacity()` 在异构系统中返回调度域内所有活跃 CPU 的容量总和，用于带宽比例计算（`cap_scale()`）。\n\n### 与 cpufreq 集成\n- 每次 `running_bw` 变化时调用 `cpufreq_update_util()`，通知 CPU 频率调节器当前 deadline 负载，确保满足实时性能需求。\n\n### 优先级继承（PI）\n- 当 deadline 任务因持有 mutex 而阻塞高优先级任务时，通过 `pi_se` 字段临时提升其调度参数，避免优先级反转。\n\n## 4. 依赖关系\n\n- **核心调度框架**：依赖 `kernel/sched/sched.h` 中定义的通用调度结构（如 `rq`、`task_struct`）和宏（如 `SCHED_CAPACITY_SCALE`）。\n- **CPU 拓扑与容量**：依赖 `arch_scale_cpu_capacity()`（由各架构实现）获取 CPU 性能信息。\n- **RCU 机制**：在 SMP 路径中大量使用 `rcu_read_lock_sched_held()` 进行锁依赖检查。\n- **cpufreq 子系统**：通过 `cpufreq_update_util()` 与 CPU 频率调节器交互。\n- **实时互斥锁**：`CONFIG_RT_MUTEXES` 启用时，支持 deadline 任务的优先级继承。\n- **Sysctl 接口**：`CONFIG_SYSCTL` 启用时，提供用户空间可调的 deadline 参数。\n\n## 5. 使用场景\n\n- **工业实时控制**：如机器人控制、数控机床等需要严格周期性和低延迟响应的场景。\n- **音视频处理**：专业音视频采集、编码、播放等对 jitter 敏感的应用。\n- **电信基础设施**：5G 基站、核心网网元中的高优先级信令处理。\n- **汽车电子**：ADAS、自动驾驶系统中的关键任务调度。\n- **科研与高性能计算**：需要确定性执行时间的实验或仿真任务。\n\n用户通过 `sched_setattr(2)` 系统调用设置任务的 `SCHED_DEADLINE` 策略及对应的 `runtime`、`deadline`、`period` 参数，内核则通过本文件实现的调度逻辑确保其满足实时性约束。",
      "similarity": 0.5966683626174927,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 514,
          "end_line": 616,
          "content": [
            "static inline int is_leftmost(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)",
            "{",
            "\treturn rb_first_cached(&dl_rq->root) == &dl_se->rb_node;",
            "}",
            "void init_dl_bw(struct dl_bw *dl_b)",
            "{",
            "\traw_spin_lock_init(&dl_b->lock);",
            "\tif (global_rt_runtime() == RUNTIME_INF)",
            "\t\tdl_b->bw = -1;",
            "\telse",
            "\t\tdl_b->bw = to_ratio(global_rt_period(), global_rt_runtime());",
            "\tdl_b->total_bw = 0;",
            "}",
            "void init_dl_rq(struct dl_rq *dl_rq)",
            "{",
            "\tdl_rq->root = RB_ROOT_CACHED;",
            "",
            "#ifdef CONFIG_SMP",
            "\t/* zero means no -deadline tasks */",
            "\tdl_rq->earliest_dl.curr = dl_rq->earliest_dl.next = 0;",
            "",
            "\tdl_rq->overloaded = 0;",
            "\tdl_rq->pushable_dl_tasks_root = RB_ROOT_CACHED;",
            "#else",
            "\tinit_dl_bw(&dl_rq->dl_bw);",
            "#endif",
            "",
            "\tdl_rq->running_bw = 0;",
            "\tdl_rq->this_bw = 0;",
            "\tinit_dl_rq_bw_ratio(dl_rq);",
            "}",
            "static inline int dl_overloaded(struct rq *rq)",
            "{",
            "\treturn atomic_read(&rq->rd->dlo_count);",
            "}",
            "static inline void dl_set_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tcpumask_set_cpu(rq->cpu, rq->rd->dlo_mask);",
            "\t/*",
            "\t * Must be visible before the overload count is",
            "\t * set (as in sched_rt.c).",
            "\t *",
            "\t * Matched by the barrier in pull_dl_task().",
            "\t */",
            "\tsmp_wmb();",
            "\tatomic_inc(&rq->rd->dlo_count);",
            "}",
            "static inline void dl_clear_overload(struct rq *rq)",
            "{",
            "\tif (!rq->online)",
            "\t\treturn;",
            "",
            "\tatomic_dec(&rq->rd->dlo_count);",
            "\tcpumask_clear_cpu(rq->cpu, rq->rd->dlo_mask);",
            "}",
            "static inline bool __pushable_less(struct rb_node *a, const struct rb_node *b)",
            "{",
            "\treturn dl_entity_preempt(&__node_2_pdl(a)->dl, &__node_2_pdl(b)->dl);",
            "}",
            "static inline int has_pushable_dl_tasks(struct rq *rq)",
            "{",
            "\treturn !RB_EMPTY_ROOT(&rq->dl.pushable_dl_tasks_root.rb_root);",
            "}",
            "static void enqueue_pushable_dl_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tstruct rb_node *leftmost;",
            "",
            "\tWARN_ON_ONCE(!RB_EMPTY_NODE(&p->pushable_dl_tasks));",
            "",
            "\tleftmost = rb_add_cached(&p->pushable_dl_tasks,",
            "\t\t\t\t &rq->dl.pushable_dl_tasks_root,",
            "\t\t\t\t __pushable_less);",
            "\tif (leftmost)",
            "\t\trq->dl.earliest_dl.next = p->dl.deadline;",
            "",
            "\tif (!rq->dl.overloaded) {",
            "\t\tdl_set_overload(rq);",
            "\t\trq->dl.overloaded = 1;",
            "\t}",
            "}",
            "static void dequeue_pushable_dl_task(struct rq *rq, struct task_struct *p)",
            "{",
            "\tstruct dl_rq *dl_rq = &rq->dl;",
            "\tstruct rb_root_cached *root = &dl_rq->pushable_dl_tasks_root;",
            "\tstruct rb_node *leftmost;",
            "",
            "\tif (RB_EMPTY_NODE(&p->pushable_dl_tasks))",
            "\t\treturn;",
            "",
            "\tleftmost = rb_erase_cached(&p->pushable_dl_tasks, root);",
            "\tif (leftmost)",
            "\t\tdl_rq->earliest_dl.next = __node_2_pdl(leftmost)->dl.deadline;",
            "",
            "\tRB_CLEAR_NODE(&p->pushable_dl_tasks);",
            "",
            "\tif (!has_pushable_dl_tasks(rq) && rq->dl.overloaded) {",
            "\t\tdl_clear_overload(rq);",
            "\t\trq->dl.overloaded = 0;",
            "\t}",
            "}"
          ],
          "function_name": "is_leftmost, init_dl_bw, init_dl_rq, dl_overloaded, dl_set_overload, dl_clear_overload, __pushable_less, has_pushable_dl_tasks, enqueue_pushable_dl_task, dequeue_pushable_dl_task",
          "description": "实现截止时间调度的抢占判定和过载管理机制，包含任务优先级比较、过载标记维护及可推送任务的数据结构操作。",
          "similarity": 0.6353604793548584
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2104,
          "end_line": 2251,
          "content": [
            "static void enqueue_task_dl(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tif (is_dl_boosted(&p->dl)) {",
            "\t\t/*",
            "\t\t * Because of delays in the detection of the overrun of a",
            "\t\t * thread's runtime, it might be the case that a thread",
            "\t\t * goes to sleep in a rt mutex with negative runtime. As",
            "\t\t * a consequence, the thread will be throttled.",
            "\t\t *",
            "\t\t * While waiting for the mutex, this thread can also be",
            "\t\t * boosted via PI, resulting in a thread that is throttled",
            "\t\t * and boosted at the same time.",
            "\t\t *",
            "\t\t * In this case, the boost overrides the throttle.",
            "\t\t */",
            "\t\tif (p->dl.dl_throttled) {",
            "\t\t\t/*",
            "\t\t\t * The replenish timer needs to be canceled. No",
            "\t\t\t * problem if it fires concurrently: boosted threads",
            "\t\t\t * are ignored in dl_task_timer().",
            "\t\t\t *",
            "\t\t\t * If the timer callback was running (hrtimer_try_to_cancel == -1),",
            "\t\t\t * it will eventually call put_task_struct().",
            "\t\t\t */",
            "\t\t\tif (hrtimer_try_to_cancel(&p->dl.dl_timer) == 1 &&",
            "\t\t\t    !dl_server(&p->dl))",
            "\t\t\t\tput_task_struct(p);",
            "\t\t\tp->dl.dl_throttled = 0;",
            "\t\t}",
            "\t} else if (!dl_prio(p->normal_prio)) {",
            "\t\t/*",
            "\t\t * Special case in which we have a !SCHED_DEADLINE task that is going",
            "\t\t * to be deboosted, but exceeds its runtime while doing so. No point in",
            "\t\t * replenishing it, as it's going to return back to its original",
            "\t\t * scheduling class after this. If it has been throttled, we need to",
            "\t\t * clear the flag, otherwise the task may wake up as throttled after",
            "\t\t * being boosted again with no means to replenish the runtime and clear",
            "\t\t * the throttle.",
            "\t\t */",
            "\t\tp->dl.dl_throttled = 0;",
            "\t\tif (!(flags & ENQUEUE_REPLENISH))",
            "\t\t\tprintk_deferred_once(\"sched: DL de-boosted task PID %d: REPLENISH flag missing\\n\",",
            "\t\t\t\t\t     task_pid_nr(p));",
            "",
            "\t\treturn;",
            "\t}",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_dl(dl_rq_of_se(&p->dl), &p->dl);",
            "",
            "\tif (p->on_rq == TASK_ON_RQ_MIGRATING)",
            "\t\tflags |= ENQUEUE_MIGRATING;",
            "",
            "\tenqueue_dl_entity(&p->dl, flags);",
            "",
            "\tif (dl_server(&p->dl))",
            "\t\treturn;",
            "",
            "\tif (!task_current(rq, p) && !p->dl.dl_throttled && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_dl_task(rq, p);",
            "}",
            "static bool dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tupdate_curr_dl(rq);",
            "",
            "\tif (p->on_rq == TASK_ON_RQ_MIGRATING)",
            "\t\tflags |= DEQUEUE_MIGRATING;",
            "",
            "\tdequeue_dl_entity(&p->dl, flags);",
            "\tif (!p->dl.dl_throttled && !dl_server(&p->dl))",
            "\t\tdequeue_pushable_dl_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void yield_task_dl(struct rq *rq)",
            "{",
            "\t/*",
            "\t * We make the task go to sleep until its current deadline by",
            "\t * forcing its runtime to zero. This way, update_curr_dl() stops",
            "\t * it and the bandwidth timer will wake it up and will give it",
            "\t * new scheduling parameters (thanks to dl_yielded=1).",
            "\t */",
            "\trq->curr->dl.dl_yielded = 1;",
            "",
            "\tupdate_rq_clock(rq);",
            "\tupdate_curr_dl(rq);",
            "\t/*",
            "\t * Tell update_rq_clock() that we've just updated,",
            "\t * so we don't do microscopic update in schedule()",
            "\t * and double the fastpath cost.",
            "\t */",
            "\trq_clock_skip_update(rq);",
            "}",
            "static inline bool dl_task_is_earliest_deadline(struct task_struct *p,",
            "\t\t\t\t\t\t struct rq *rq)",
            "{",
            "\treturn (!rq->dl.dl_nr_running ||",
            "\t\tdl_time_before(p->dl.deadline,",
            "\t\t\t       rq->dl.earliest_dl.curr));",
            "}",
            "static int",
            "select_task_rq_dl(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tbool select_rq;",
            "\tstruct rq *rq;",
            "",
            "\tif (!(flags & WF_TTWU))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If we are dealing with a -deadline task, we must",
            "\t * decide where to wake it up.",
            "\t * If it has a later deadline and the current task",
            "\t * on this rq can't move (provided the waking task",
            "\t * can!) we prefer to send it somewhere else. On the",
            "\t * other hand, if it has a shorter deadline, we",
            "\t * try to make it stay here, it might be important.",
            "\t */",
            "\tselect_rq = unlikely(dl_task(curr)) &&",
            "\t\t    (curr->nr_cpus_allowed < 2 ||",
            "\t\t     !dl_entity_preempt(&p->dl, &curr->dl)) &&",
            "\t\t    p->nr_cpus_allowed > 1;",
            "",
            "\t/*",
            "\t * Take the capacity of the CPU into account to",
            "\t * ensure it fits the requirement of the task.",
            "\t */",
            "\tif (sched_asym_cpucap_active())",
            "\t\tselect_rq |= !dl_task_fits_capacity(p, cpu);",
            "",
            "\tif (select_rq) {",
            "\t\tint target = find_later_rq(p);",
            "",
            "\t\tif (target != -1 &&",
            "\t\t    dl_task_is_earliest_deadline(p, cpu_rq(target)))",
            "\t\t\tcpu = target;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "enqueue_task_dl, dequeue_task_dl, yield_task_dl, dl_task_is_earliest_deadline, select_task_rq_dl",
          "description": "处理截止时间任务的调度决策，包含任务入队出队、抢占检查、CPU选择及负载均衡逻辑，通过dl_task_is_earliest_deadline判断任务截止时间优先级并选择合适CPU",
          "similarity": 0.6310456991195679
        },
        {
          "chunk_id": 16,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2709,
          "end_line": 2879,
          "content": [
            "static int push_dl_task(struct rq *rq)",
            "{",
            "\tstruct task_struct *next_task;",
            "\tstruct rq *later_rq;",
            "\tint ret = 0;",
            "",
            "\tnext_task = pick_next_pushable_dl_task(rq);",
            "\tif (!next_task)",
            "\t\treturn 0;",
            "",
            "retry:",
            "\t/*",
            "\t * If next_task preempts rq->curr, and rq->curr",
            "\t * can move away, it makes sense to just reschedule",
            "\t * without going further in pushing next_task.",
            "\t */",
            "\tif (dl_task(rq->curr) &&",
            "\t    dl_time_before(next_task->dl.deadline, rq->curr->dl.deadline) &&",
            "\t    rq->curr->nr_cpus_allowed > 1) {",
            "\t\tresched_curr(rq);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (is_migration_disabled(next_task))",
            "\t\treturn 0;",
            "",
            "\tif (WARN_ON(next_task == rq->curr))",
            "\t\treturn 0;",
            "",
            "\t/* We might release rq lock */",
            "\tget_task_struct(next_task);",
            "",
            "\t/* Will lock the rq it'll find */",
            "\tlater_rq = find_lock_later_rq(next_task, rq);",
            "\tif (!later_rq) {",
            "\t\tstruct task_struct *task;",
            "",
            "\t\t/*",
            "\t\t * We must check all this again, since",
            "\t\t * find_lock_later_rq releases rq->lock and it is",
            "\t\t * then possible that next_task has migrated.",
            "\t\t */",
            "\t\ttask = pick_next_pushable_dl_task(rq);",
            "\t\tif (task == next_task) {",
            "\t\t\t/*",
            "\t\t\t * The task is still there. We don't try",
            "\t\t\t * again, some other CPU will pull it when ready.",
            "\t\t\t */",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tif (!task)",
            "\t\t\t/* No more tasks */",
            "\t\t\tgoto out;",
            "",
            "\t\tput_task_struct(next_task);",
            "\t\tnext_task = task;",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tdeactivate_task(rq, next_task, 0);",
            "\tset_task_cpu(next_task, later_rq->cpu);",
            "\tactivate_task(later_rq, next_task, 0);",
            "\tret = 1;",
            "",
            "\tresched_curr(later_rq);",
            "",
            "\tdouble_unlock_balance(rq, later_rq);",
            "",
            "out:",
            "\tput_task_struct(next_task);",
            "",
            "\treturn ret;",
            "}",
            "static void push_dl_tasks(struct rq *rq)",
            "{",
            "\t/* push_dl_task() will return true if it moved a -deadline task */",
            "\twhile (push_dl_task(rq))",
            "\t\t;",
            "}",
            "static void pull_dl_task(struct rq *this_rq)",
            "{",
            "\tint this_cpu = this_rq->cpu, cpu;",
            "\tstruct task_struct *p, *push_task;",
            "\tbool resched = false;",
            "\tstruct rq *src_rq;",
            "\tu64 dmin = LONG_MAX;",
            "",
            "\tif (likely(!dl_overloaded(this_rq)))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Match the barrier from dl_set_overloaded; this guarantees that if we",
            "\t * see overloaded we must also see the dlo_mask bit.",
            "\t */",
            "\tsmp_rmb();",
            "",
            "\tfor_each_cpu(cpu, this_rq->rd->dlo_mask) {",
            "\t\tif (this_cpu == cpu)",
            "\t\t\tcontinue;",
            "",
            "\t\tsrc_rq = cpu_rq(cpu);",
            "",
            "\t\t/*",
            "\t\t * It looks racy, abd it is! However, as in sched_rt.c,",
            "\t\t * we are fine with this.",
            "\t\t */",
            "\t\tif (this_rq->dl.dl_nr_running &&",
            "\t\t    dl_time_before(this_rq->dl.earliest_dl.curr,",
            "\t\t\t\t   src_rq->dl.earliest_dl.next))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Might drop this_rq->lock */",
            "\t\tpush_task = NULL;",
            "\t\tdouble_lock_balance(this_rq, src_rq);",
            "",
            "\t\t/*",
            "\t\t * If there are no more pullable tasks on the",
            "\t\t * rq, we're done with it.",
            "\t\t */",
            "\t\tif (src_rq->dl.dl_nr_running <= 1)",
            "\t\t\tgoto skip;",
            "",
            "\t\tp = pick_earliest_pushable_dl_task(src_rq, this_cpu);",
            "",
            "\t\t/*",
            "\t\t * We found a task to be pulled if:",
            "\t\t *  - it preempts our current (if there's one),",
            "\t\t *  - it will preempt the last one we pulled (if any).",
            "\t\t */",
            "\t\tif (p && dl_time_before(p->dl.deadline, dmin) &&",
            "\t\t    dl_task_is_earliest_deadline(p, this_rq)) {",
            "\t\t\tWARN_ON(p == src_rq->curr);",
            "\t\t\tWARN_ON(!task_on_rq_queued(p));",
            "",
            "\t\t\t/*",
            "\t\t\t * Then we pull iff p has actually an earlier",
            "\t\t\t * deadline than the current task of its runqueue.",
            "\t\t\t */",
            "\t\t\tif (dl_time_before(p->dl.deadline,",
            "\t\t\t\t\t   src_rq->curr->dl.deadline))",
            "\t\t\t\tgoto skip;",
            "",
            "\t\t\tif (is_migration_disabled(p)) {",
            "\t\t\t\tpush_task = get_push_task(src_rq);",
            "\t\t\t} else {",
            "\t\t\t\tdeactivate_task(src_rq, p, 0);",
            "\t\t\t\tset_task_cpu(p, this_cpu);",
            "\t\t\t\tactivate_task(this_rq, p, 0);",
            "\t\t\t\tdmin = p->dl.deadline;",
            "\t\t\t\tresched = true;",
            "\t\t\t}",
            "",
            "\t\t\t/* Is there any other task even earlier? */",
            "\t\t}",
            "skip:",
            "\t\tdouble_unlock_balance(this_rq, src_rq);",
            "",
            "\t\tif (push_task) {",
            "\t\t\tpreempt_disable();",
            "\t\t\traw_spin_rq_unlock(this_rq);",
            "\t\t\tstop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,",
            "\t\t\t\t\t    push_task, &src_rq->push_work);",
            "\t\t\tpreempt_enable();",
            "\t\t\traw_spin_rq_lock(this_rq);",
            "\t\t}",
            "\t}",
            "",
            "\tif (resched)",
            "\t\tresched_curr(this_rq);",
            "}"
          ],
          "function_name": "push_dl_task, push_dl_tasks, pull_dl_task",
          "description": "实现SCHED_DEADLINE任务的迁移逻辑，通过push_dl_task尝试将任务推送到更晚截止时间的CPU，pull_dl_task从过载CPU拉取任务以平衡负载",
          "similarity": 0.6131092309951782
        },
        {
          "chunk_id": 17,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2887,
          "end_line": 3034,
          "content": [
            "static void task_woken_dl(struct rq *rq, struct task_struct *p)",
            "{",
            "\tif (!task_on_cpu(rq, p) &&",
            "\t    !test_tsk_need_resched(rq->curr) &&",
            "\t    p->nr_cpus_allowed > 1 &&",
            "\t    dl_task(rq->curr) &&",
            "\t    (rq->curr->nr_cpus_allowed < 2 ||",
            "\t     !dl_entity_preempt(&p->dl, &rq->curr->dl))) {",
            "\t\tpush_dl_tasks(rq);",
            "\t}",
            "}",
            "static void set_cpus_allowed_dl(struct task_struct *p,",
            "\t\t\t\tstruct affinity_context *ctx)",
            "{",
            "\tstruct root_domain *src_rd;",
            "\tstruct rq *rq;",
            "",
            "\tWARN_ON_ONCE(!dl_task(p));",
            "",
            "\trq = task_rq(p);",
            "\tsrc_rd = rq->rd;",
            "\t/*",
            "\t * Migrating a SCHED_DEADLINE task between exclusive",
            "\t * cpusets (different root_domains) entails a bandwidth",
            "\t * update. We already made space for us in the destination",
            "\t * domain (see cpuset_can_attach()).",
            "\t */",
            "\tif (!cpumask_intersects(src_rd->span, ctx->new_mask)) {",
            "\t\tstruct dl_bw *src_dl_b;",
            "",
            "\t\tsrc_dl_b = dl_bw_of(cpu_of(rq));",
            "\t\t/*",
            "\t\t * We now free resources of the root_domain we are migrating",
            "\t\t * off. In the worst case, sched_setattr() may temporary fail",
            "\t\t * until we complete the update.",
            "\t\t */",
            "\t\traw_spin_lock(&src_dl_b->lock);",
            "\t\t__dl_sub(src_dl_b, p->dl.dl_bw, dl_bw_cpus(task_cpu(p)));",
            "\t\traw_spin_unlock(&src_dl_b->lock);",
            "\t}",
            "",
            "\tset_cpus_allowed_common(p, ctx);",
            "}",
            "static void rq_online_dl(struct rq *rq)",
            "{",
            "\tif (rq->dl.overloaded)",
            "\t\tdl_set_overload(rq);",
            "",
            "\tcpudl_set_freecpu(&rq->rd->cpudl, rq->cpu);",
            "\tif (rq->dl.dl_nr_running > 0)",
            "\t\tcpudl_set(&rq->rd->cpudl, rq->cpu, rq->dl.earliest_dl.curr);",
            "}",
            "static void rq_offline_dl(struct rq *rq)",
            "{",
            "\tif (rq->dl.overloaded)",
            "\t\tdl_clear_overload(rq);",
            "",
            "\tcpudl_clear(&rq->rd->cpudl, rq->cpu);",
            "\tcpudl_clear_freecpu(&rq->rd->cpudl, rq->cpu);",
            "}",
            "void __init init_sched_dl_class(void)",
            "{",
            "\tunsigned int i;",
            "",
            "\tfor_each_possible_cpu(i)",
            "\t\tzalloc_cpumask_var_node(&per_cpu(local_cpu_mask_dl, i),",
            "\t\t\t\t\tGFP_KERNEL, cpu_to_node(i));",
            "}",
            "void dl_add_task_root_domain(struct task_struct *p)",
            "{",
            "\tstruct rq_flags rf;",
            "\tstruct rq *rq;",
            "\tstruct dl_bw *dl_b;",
            "",
            "\traw_spin_lock_irqsave(&p->pi_lock, rf.flags);",
            "\tif (!dl_task(p)) {",
            "\t\traw_spin_unlock_irqrestore(&p->pi_lock, rf.flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\trq = __task_rq_lock(p, &rf);",
            "",
            "\tdl_b = &rq->rd->dl_bw;",
            "\traw_spin_lock(&dl_b->lock);",
            "",
            "\t__dl_add(dl_b, p->dl.dl_bw, cpumask_weight(rq->rd->span));",
            "",
            "\traw_spin_unlock(&dl_b->lock);",
            "",
            "\ttask_rq_unlock(rq, p, &rf);",
            "}",
            "void dl_clear_root_domain(struct root_domain *rd)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&rd->dl_bw.lock, flags);",
            "\trd->dl_bw.total_bw = 0;",
            "\traw_spin_unlock_irqrestore(&rd->dl_bw.lock, flags);",
            "}",
            "static void switched_from_dl(struct rq *rq, struct task_struct *p)",
            "{",
            "\t/*",
            "\t * task_non_contending() can start the \"inactive timer\" (if the 0-lag",
            "\t * time is in the future). If the task switches back to dl before",
            "\t * the \"inactive timer\" fires, it can continue to consume its current",
            "\t * runtime using its current deadline. If it stays outside of",
            "\t * SCHED_DEADLINE until the 0-lag time passes, inactive_task_timer()",
            "\t * will reset the task parameters.",
            "\t */",
            "\tif (task_on_rq_queued(p) && p->dl.dl_runtime)",
            "\t\ttask_non_contending(&p->dl);",
            "",
            "\t/*",
            "\t * In case a task is setscheduled out from SCHED_DEADLINE we need to",
            "\t * keep track of that on its cpuset (for correct bandwidth tracking).",
            "\t */",
            "\tdec_dl_tasks_cs(p);",
            "",
            "\tif (!task_on_rq_queued(p)) {",
            "\t\t/*",
            "\t\t * Inactive timer is armed. However, p is leaving DEADLINE and",
            "\t\t * might migrate away from this rq while continuing to run on",
            "\t\t * some other class. We need to remove its contribution from",
            "\t\t * this rq running_bw now, or sub_rq_bw (below) will complain.",
            "\t\t */",
            "\t\tif (p->dl.dl_non_contending)",
            "\t\t\tsub_running_bw(&p->dl, &rq->dl);",
            "\t\tsub_rq_bw(&p->dl, &rq->dl);",
            "\t}",
            "",
            "\t/*",
            "\t * We cannot use inactive_task_timer() to invoke sub_running_bw()",
            "\t * at the 0-lag time, because the task could have been migrated",
            "\t * while SCHED_OTHER in the meanwhile.",
            "\t */",
            "\tif (p->dl.dl_non_contending)",
            "\t\tp->dl.dl_non_contending = 0;",
            "",
            "\t/*",
            "\t * Since this might be the only -deadline task on the rq,",
            "\t * this is the right place to try to pull some other one",
            "\t * from an overloaded CPU, if any.",
            "\t */",
            "\tif (!task_on_rq_queued(p) || rq->dl.dl_nr_running)",
            "\t\treturn;",
            "",
            "\tdeadline_queue_pull_task(rq);",
            "}"
          ],
          "function_name": "task_woken_dl, set_cpus_allowed_dl, rq_online_dl, rq_offline_dl, init_sched_dl_class, dl_add_task_root_domain, dl_clear_root_domain, switched_from_dl",
          "description": "管理SCHED_DEADLINE任务唤醒后的处理、CPU亲和性变更、根域带宽分配及任务状态切换时的资源回收与重新计算",
          "similarity": 0.608332633972168
        },
        {
          "chunk_id": 18,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 3051,
          "end_line": 3166,
          "content": [
            "static void switched_to_dl(struct rq *rq, struct task_struct *p)",
            "{",
            "\tif (hrtimer_try_to_cancel(&p->dl.inactive_timer) == 1)",
            "\t\tput_task_struct(p);",
            "",
            "\t/*",
            "\t * In case a task is setscheduled to SCHED_DEADLINE we need to keep",
            "\t * track of that on its cpuset (for correct bandwidth tracking).",
            "\t */",
            "\tinc_dl_tasks_cs(p);",
            "",
            "\t/* If p is not queued we will update its parameters at next wakeup. */",
            "\tif (!task_on_rq_queued(p)) {",
            "\t\tadd_rq_bw(&p->dl, &rq->dl);",
            "",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (rq->curr != p) {",
            "#ifdef CONFIG_SMP",
            "\t\tif (p->nr_cpus_allowed > 1 && rq->dl.overloaded)",
            "\t\t\tdeadline_queue_push_tasks(rq);",
            "#endif",
            "\t\tif (dl_task(rq->curr))",
            "\t\t\twakeup_preempt_dl(rq, p, 0);",
            "\t\telse",
            "\t\t\tresched_curr(rq);",
            "\t} else {",
            "\t\tupdate_dl_rq_load_avg(rq_clock_pelt(rq), rq, 0);",
            "\t}",
            "}",
            "static void prio_changed_dl(struct rq *rq, struct task_struct *p,",
            "\t\t\t    int oldprio)",
            "{",
            "\tif (!task_on_rq_queued(p))",
            "\t\treturn;",
            "",
            "#ifdef CONFIG_SMP",
            "\t/*",
            "\t * This might be too much, but unfortunately",
            "\t * we don't have the old deadline value, and",
            "\t * we can't argue if the task is increasing",
            "\t * or lowering its prio, so...",
            "\t */",
            "\tif (!rq->dl.overloaded)",
            "\t\tdeadline_queue_pull_task(rq);",
            "",
            "\tif (task_current(rq, p)) {",
            "\t\t/*",
            "\t\t * If we now have a earlier deadline task than p,",
            "\t\t * then reschedule, provided p is still on this",
            "\t\t * runqueue.",
            "\t\t */",
            "\t\tif (dl_time_before(rq->dl.earliest_dl.curr, p->dl.deadline))",
            "\t\t\tresched_curr(rq);",
            "\t} else {",
            "\t\t/*",
            "\t\t * Current may not be deadline in case p was throttled but we",
            "\t\t * have just replenished it (e.g. rt_mutex_setprio()).",
            "\t\t *",
            "\t\t * Otherwise, if p was given an earlier deadline, reschedule.",
            "\t\t */",
            "\t\tif (!dl_task(rq->curr) ||",
            "\t\t    dl_time_before(p->dl.deadline, rq->curr->dl.deadline))",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "#else",
            "\t/*",
            "\t * We don't know if p has a earlier or later deadline, so let's blindly",
            "\t * set a (maybe not needed) rescheduling point.",
            "\t */",
            "\tresched_curr(rq);",
            "#endif",
            "}",
            "static int task_is_throttled_dl(struct task_struct *p, int cpu)",
            "{",
            "\treturn p->dl.dl_throttled;",
            "}",
            "int sched_dl_global_validate(void)",
            "{",
            "\tu64 runtime = global_rt_runtime();",
            "\tu64 period = global_rt_period();",
            "\tu64 new_bw = to_ratio(period, runtime);",
            "\tu64 gen = ++dl_generation;",
            "\tstruct dl_bw *dl_b;",
            "\tint cpu, cpus, ret = 0;",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * Here we want to check the bandwidth not being set to some",
            "\t * value smaller than the currently allocated bandwidth in",
            "\t * any of the root_domains.",
            "\t */",
            "\tfor_each_online_cpu(cpu) {",
            "\t\trcu_read_lock_sched();",
            "",
            "\t\tif (dl_bw_visited(cpu, gen))",
            "\t\t\tgoto next;",
            "",
            "\t\tdl_b = dl_bw_of(cpu);",
            "\t\tcpus = dl_bw_cpus(cpu);",
            "",
            "\t\traw_spin_lock_irqsave(&dl_b->lock, flags);",
            "\t\tif (new_bw * cpus < dl_b->total_bw)",
            "\t\t\tret = -EBUSY;",
            "\t\traw_spin_unlock_irqrestore(&dl_b->lock, flags);",
            "",
            "next:",
            "\t\trcu_read_unlock_sched();",
            "",
            "\t\tif (ret)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "switched_to_dl, prio_changed_dl, task_is_throttled_dl, sched_dl_global_validate",
          "description": "处理SCHED_DEADLINE任务策略切换、优先级变更、节流状态检测及全局带宽验证，确保系统范围内的带宽约束有效性",
          "similarity": 0.5962633490562439
        }
      ]
    },
    {
      "source_file": "kernel/sched/cpupri.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:04:45\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\cpupri.c`\n\n---\n\n# `sched/cpupri.c` 技术文档\n\n## 1. 文件概述\n\n`sched/cpupri.c` 实现了 **CPU 优先级管理（CPU Priority Management）** 机制，用于实时任务（RT tasks）的全局负载均衡和迁移决策。该机制通过维护一个二维位图结构，快速追踪每个 CPU 当前运行任务的最高优先级，从而在 O(1) 时间复杂度内为新唤醒或迁移的实时任务找到合适的 CPU 目标。该机制特别优化了无 CPU 亲和性限制的任务调度路径，同时支持带亲和性约束的场景。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `convert_prio(int prio)` | 将任务的调度优先级（`p->prio`）转换为内部 `cpupri` 优先级值（范围：-1 到 100） |\n| `__cpupri_find(struct cpupri *cp, struct task_struct *p, struct cpumask *lowest_mask, int idx)` | 在指定优先级层级 `idx` 中查找满足任务 `p` 的 CPU（考虑亲和性） |\n| `cpupri_find(struct cpupri *cp, struct task_struct *p, struct cpumask *lowest_mask)` | 查找系统中优先级 **低于或等于** 任务 `p` 的 CPU（即任务可运行的 CPU） |\n| `cpupri_find_fitness(...)` | 增强版查找函数，支持通过 `fitness_fn` 自定义 CPU 适配条件（如容量感知） |\n| `cpupri_set(struct cpupri *cp, int cpu, int newpri)` | 更新指定 CPU 的当前最高优先级状态 |\n| `cpupri_init(struct cpupri *cp)` | 初始化 `cpupri` 数据结构（声明但未在片段中实现） |\n\n### 关键数据结构（隐含）\n\n- `struct cpupri`：全局 CPU 优先级管理上下文\n  - `cpu_to_pri[]`：每个 CPU 当前的 `cpupri` 优先级\n  - `pri_to_cpu[]`：每个优先级对应的 `struct cpupri_vec`\n- `struct cpupri_vec`：\n  - `mask`：该优先级下所有 CPU 的位图\n  - `count`：该优先级下活跃 CPU 的数量（原子计数）\n\n### 优先级映射关系\n\n| 任务 `p->prio` | `cpupri` 值 | 含义 |\n|---------------|------------|------|\n| -1 | -1 (`CPUPRI_INVALID`) | 无效状态（CPU 不可调度） |\n| 0–98 | 99–1 | 实时优先级（数值越大，任务优先级越高，`cpupri` 值越小） |\n| 99 (`MAX_RT_PRIO-1`) | 0 (`CPUPRI_NORMAL`) | 普通（非实时）任务 |\n| 100 (`MAX_RT_PRIO`) | 100 (`CPUPRI_HIGHER`) | 高于所有 RT 任务的特殊优先级 |\n\n> **注意**：`cpupri` 值越小，表示 CPU 当前负载的优先级 **越高**。\n\n## 3. 关键实现\n\n### 优先级转换逻辑\n- `convert_prio()` 实现了任务调度优先级到 `cpupri` 内部表示的映射，确保实时任务（`p->prio` ∈ [0, 98]）被正确映射到 `cpupri` ∈ [1, 99]，且高优先级任务对应更小的 `cpupri` 值。\n\n### 快速查找算法\n- 使用 **二维位图**：第一维为优先级（0–100），第二维为 CPU 位图。\n- `cpupri_find_fitness()` 从最低优先级（`idx = 0`）开始遍历，找到第一个存在可用 CPU 的优先级层级。\n- 对于每个层级，通过 `cpumask_any_and()` 快速判断任务亲和性掩码与该优先级 CPU 掩码是否有交集。\n- 若提供 `fitness_fn`（如容量检查），会过滤掉不满足条件的 CPU；若过滤后无 CPU 可用，则继续搜索更高优先级层级。\n\n### 容错与回退策略\n- 如果启用了 `fitness_fn` 但未找到满足条件的 CPU，函数会 **忽略 fitness 条件重新搜索**，确保高优先级任务总能找到运行 CPU（优先保证实时性，而非最优资源匹配）。\n\n### 并发安全更新\n- `cpupri_set()` 使用 **内存屏障（`smp_mb__before/after_atomic()`）** 确保 CPU 位图和计数器的更新顺序：\n  1. **添加 CPU**：先设置位图 → 内存屏障 → 增加计数器\n  2. **移除 CPU**：先减少计数器 → 内存屏障 → 清除位图\n- 此顺序防止 `cpupri_find` 在并发读取时看到不一致状态（如计数器为 0 但位图仍置位）。\n\n### 亲和性处理\n- 所有查找操作均与任务的 `p->cpus_mask`（CPU 亲和性）和 `cpu_active_mask`（活跃 CPU）进行交集运算，确保只返回合法 CPU。\n\n## 4. 依赖关系\n\n- **调度器核心**：依赖 `task_struct`、`p->prio`、`p->cpus_mask` 等调度器基本结构。\n- **实时调度类（`rt.c`）**：`cpupri` 主要服务于 `SCHED_FIFO`/`SCHED_RR` 任务的负载均衡。\n- **CPU 掩码操作**：使用 `cpumask_*` 系列函数（如 `cpumask_and`, `cpumask_clear_cpu`）。\n- **内存屏障原语**：依赖 `smp_rmb()`、`smp_mb__before_atomic()` 等 SMP 同步机制。\n- **原子操作**：使用 `atomic_read/inc/dec` 管理优先级层级的 CPU 计数。\n\n## 5. 使用场景\n\n- **实时任务唤醒/迁移**：当高优先级 RT 任务被唤醒或需要迁移时，调用 `cpupri_find()` 快速定位可运行的最低优先级 CPU（减少抢占开销）。\n- **全局负载均衡**：RT 调度器的 `push_rt_task()` 和 `pull_rt_task()` 机制利用 `cpupri` 决定任务推送/拉取的目标 CPU。\n- **容量感知调度（Capacity Awareness）**：通过 `cpupri_find_fitness()` 的 `fitness_fn` 参数，集成 CPU 性能/能效信息（如 ARM big.LITTLE 架构），在满足优先级前提下选择合适 CPU。\n- **CPU 热插拔**：CPU 上下线时通过 `cpupri_set()` 更新其优先级状态（设为 `CPUPRI_INVALID` 或恢复）。",
      "similarity": 0.5700730681419373,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 42,
          "end_line": 178,
          "content": [
            "static int convert_prio(int prio)",
            "{",
            "\tint cpupri;",
            "",
            "\tswitch (prio) {",
            "\tcase CPUPRI_INVALID:",
            "\t\tcpupri = CPUPRI_INVALID;\t/* -1 */",
            "\t\tbreak;",
            "",
            "\tcase 0 ... 98:",
            "\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */",
            "\t\tbreak;",
            "",
            "\tcase MAX_RT_PRIO-1:",
            "\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */",
            "\t\tbreak;",
            "",
            "\tcase MAX_RT_PRIO:",
            "\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn cpupri;",
            "}",
            "static inline int __cpupri_find(struct cpupri *cp, struct task_struct *p,",
            "\t\t\t\tstruct cpumask *lowest_mask, int idx)",
            "{",
            "\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[idx];",
            "\tint skip = 0;",
            "",
            "\tif (!atomic_read(&(vec)->count))",
            "\t\tskip = 1;",
            "\t/*",
            "\t * When looking at the vector, we need to read the counter,",
            "\t * do a memory barrier, then read the mask.",
            "\t *",
            "\t * Note: This is still all racy, but we can deal with it.",
            "\t *  Ideally, we only want to look at masks that are set.",
            "\t *",
            "\t *  If a mask is not set, then the only thing wrong is that we",
            "\t *  did a little more work than necessary.",
            "\t *",
            "\t *  If we read a zero count but the mask is set, because of the",
            "\t *  memory barriers, that can only happen when the highest prio",
            "\t *  task for a run queue has left the run queue, in which case,",
            "\t *  it will be followed by a pull. If the task we are processing",
            "\t *  fails to find a proper place to go, that pull request will",
            "\t *  pull this task if the run queue is running at a lower",
            "\t *  priority.",
            "\t */",
            "\tsmp_rmb();",
            "",
            "\t/* Need to do the rmb for every iteration */",
            "\tif (skip)",
            "\t\treturn 0;",
            "",
            "\tif (cpumask_any_and(&p->cpus_mask, vec->mask) >= nr_cpu_ids)",
            "\t\treturn 0;",
            "",
            "\tif (lowest_mask) {",
            "\t\tcpumask_and(lowest_mask, &p->cpus_mask, vec->mask);",
            "\t\tcpumask_and(lowest_mask, lowest_mask, cpu_active_mask);",
            "",
            "\t\t/*",
            "\t\t * We have to ensure that we have at least one bit",
            "\t\t * still set in the array, since the map could have",
            "\t\t * been concurrently emptied between the first and",
            "\t\t * second reads of vec->mask.  If we hit this",
            "\t\t * condition, simply act as though we never hit this",
            "\t\t * priority level and continue on.",
            "\t\t */",
            "\t\tif (cpumask_empty(lowest_mask))",
            "\t\t\treturn 0;",
            "\t}",
            "",
            "\treturn 1;",
            "}",
            "int cpupri_find(struct cpupri *cp, struct task_struct *p,",
            "\t\tstruct cpumask *lowest_mask)",
            "{",
            "\treturn cpupri_find_fitness(cp, p, lowest_mask, NULL);",
            "}",
            "int cpupri_find_fitness(struct cpupri *cp, struct task_struct *p,",
            "\t\tstruct cpumask *lowest_mask,",
            "\t\tbool (*fitness_fn)(struct task_struct *p, int cpu))",
            "{",
            "\tint task_pri = convert_prio(p->prio);",
            "\tint idx, cpu;",
            "",
            "\tWARN_ON_ONCE(task_pri >= CPUPRI_NR_PRIORITIES);",
            "",
            "\tfor (idx = 0; idx < task_pri; idx++) {",
            "",
            "\t\tif (!__cpupri_find(cp, p, lowest_mask, idx))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!lowest_mask || !fitness_fn)",
            "\t\t\treturn 1;",
            "",
            "\t\t/* Ensure the capacity of the CPUs fit the task */",
            "\t\tfor_each_cpu(cpu, lowest_mask) {",
            "\t\t\tif (!fitness_fn(p, cpu))",
            "\t\t\t\tcpumask_clear_cpu(cpu, lowest_mask);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If no CPU at the current priority can fit the task",
            "\t\t * continue looking",
            "\t\t */",
            "\t\tif (cpumask_empty(lowest_mask))",
            "\t\t\tcontinue;",
            "",
            "\t\treturn 1;",
            "\t}",
            "",
            "\t/*",
            "\t * If we failed to find a fitting lowest_mask, kick off a new search",
            "\t * but without taking into account any fitness criteria this time.",
            "\t *",
            "\t * This rule favours honouring priority over fitting the task in the",
            "\t * correct CPU (Capacity Awareness being the only user now).",
            "\t * The idea is that if a higher priority task can run, then it should",
            "\t * run even if this ends up being on unfitting CPU.",
            "\t *",
            "\t * The cost of this trade-off is not entirely clear and will probably",
            "\t * be good for some workloads and bad for others.",
            "\t *",
            "\t * The main idea here is that if some CPUs were over-committed, we try",
            "\t * to spread which is what the scheduler traditionally did. Sys admins",
            "\t * must do proper RT planning to avoid overloading the system if they",
            "\t * really care.",
            "\t */",
            "\tif (fitness_fn)",
            "\t\treturn cpupri_find(cp, p, lowest_mask);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "convert_prio, __cpupri_find, cpupri_find, cpupri_find_fitness",
          "description": "convert_prio将任务优先级映射为CPU优先级数值；__cpupri_find检查特定优先级下是否存在可用CPU；cpupri_find_fitness遍历优先级层级寻找适配CPU，结合适应性判断与优先级策略决定最终选择",
          "similarity": 0.6063463091850281
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 210,
          "end_line": 304,
          "content": [
            "void cpupri_set(struct cpupri *cp, int cpu, int newpri)",
            "{",
            "\tint *currpri = &cp->cpu_to_pri[cpu];",
            "\tint oldpri = *currpri;",
            "\tint do_mb = 0;",
            "",
            "\tnewpri = convert_prio(newpri);",
            "",
            "\tBUG_ON(newpri >= CPUPRI_NR_PRIORITIES);",
            "",
            "\tif (newpri == oldpri)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If the CPU was currently mapped to a different value, we",
            "\t * need to map it to the new value then remove the old value.",
            "\t * Note, we must add the new value first, otherwise we risk the",
            "\t * cpu being missed by the priority loop in cpupri_find.",
            "\t */",
            "\tif (likely(newpri != CPUPRI_INVALID)) {",
            "\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[newpri];",
            "",
            "\t\tcpumask_set_cpu(cpu, vec->mask);",
            "\t\t/*",
            "\t\t * When adding a new vector, we update the mask first,",
            "\t\t * do a write memory barrier, and then update the count, to",
            "\t\t * make sure the vector is visible when count is set.",
            "\t\t */",
            "\t\tsmp_mb__before_atomic();",
            "\t\tatomic_inc(&(vec)->count);",
            "\t\tdo_mb = 1;",
            "\t}",
            "\tif (likely(oldpri != CPUPRI_INVALID)) {",
            "\t\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[oldpri];",
            "",
            "\t\t/*",
            "\t\t * Because the order of modification of the vec->count",
            "\t\t * is important, we must make sure that the update",
            "\t\t * of the new prio is seen before we decrement the",
            "\t\t * old prio. This makes sure that the loop sees",
            "\t\t * one or the other when we raise the priority of",
            "\t\t * the run queue. We don't care about when we lower the",
            "\t\t * priority, as that will trigger an rt pull anyway.",
            "\t\t *",
            "\t\t * We only need to do a memory barrier if we updated",
            "\t\t * the new priority vec.",
            "\t\t */",
            "\t\tif (do_mb)",
            "\t\t\tsmp_mb__after_atomic();",
            "",
            "\t\t/*",
            "\t\t * When removing from the vector, we decrement the counter first",
            "\t\t * do a memory barrier and then clear the mask.",
            "\t\t */",
            "\t\tatomic_dec(&(vec)->count);",
            "\t\tsmp_mb__after_atomic();",
            "\t\tcpumask_clear_cpu(cpu, vec->mask);",
            "\t}",
            "",
            "\t*currpri = newpri;",
            "}",
            "int cpupri_init(struct cpupri *cp)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++) {",
            "\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[i];",
            "",
            "\t\tatomic_set(&vec->count, 0);",
            "\t\tif (!zalloc_cpumask_var(&vec->mask, GFP_KERNEL))",
            "\t\t\tgoto cleanup;",
            "\t}",
            "",
            "\tcp->cpu_to_pri = kcalloc(nr_cpu_ids, sizeof(int), GFP_KERNEL);",
            "\tif (!cp->cpu_to_pri)",
            "\t\tgoto cleanup;",
            "",
            "\tfor_each_possible_cpu(i)",
            "\t\tcp->cpu_to_pri[i] = CPUPRI_INVALID;",
            "",
            "\treturn 0;",
            "",
            "cleanup:",
            "\tfor (i--; i >= 0; i--)",
            "\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);",
            "\treturn -ENOMEM;",
            "}",
            "void cpupri_cleanup(struct cpupri *cp)",
            "{",
            "\tint i;",
            "",
            "\tkfree(cp->cpu_to_pri);",
            "\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++)",
            "\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);",
            "}"
          ],
          "function_name": "cpupri_set, cpupri_init, cpupri_cleanup",
          "description": "cpupri_set更新CPU优先级状态，通过原子操作同步位图与计数器；cpupri_init初始化优先级到CPU的映射表与CPU到优先级的数组；cpupri_cleanup释放所有动态分配的位图资源与优先级数组",
          "similarity": 0.574398398399353
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/cpupri.c",
          "start_line": 1,
          "end_line": 41,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  kernel/sched/cpupri.c",
            " *",
            " *  CPU priority management",
            " *",
            " *  Copyright (C) 2007-2008 Novell",
            " *",
            " *  Author: Gregory Haskins <ghaskins@novell.com>",
            " *",
            " *  This code tracks the priority of each CPU so that global migration",
            " *  decisions are easy to calculate.  Each CPU can be in a state as follows:",
            " *",
            " *                 (INVALID), NORMAL, RT1, ... RT99, HIGHER",
            " *",
            " *  going from the lowest priority to the highest.  CPUs in the INVALID state",
            " *  are not eligible for routing.  The system maintains this state with",
            " *  a 2 dimensional bitmap (the first for priority class, the second for CPUs",
            " *  in that class).  Therefore a typical application without affinity",
            " *  restrictions can find a suitable CPU with O(1) complexity (e.g. two bit",
            " *  searches).  For tasks with affinity restrictions, the algorithm has a",
            " *  worst case complexity of O(min(101, nr_domcpus)), though the scenario that",
            " *  yields the worst case search is fairly contrived.",
            " */",
            "",
            "/*",
            " * p->rt_priority   p->prio   newpri   cpupri",
            " *",
            " *\t\t\t\t  -1       -1 (CPUPRI_INVALID)",
            " *",
            " *\t\t\t\t  99        0 (CPUPRI_NORMAL)",
            " *",
            " *\t\t1        98       98        1",
            " *\t      ...",
            " *\t       49        50       50       49",
            " *\t       50        49       49       50",
            " *\t      ...",
            " *\t       99         0        0       99",
            " *",
            " *\t\t\t\t 100\t  100 (CPUPRI_HIGHER)",
            " */"
          ],
          "function_name": null,
          "description": "定义CPU优先级管理模块，通过二维位图跟踪各CPU优先级状态，支持NORMAL、RT1至RT99及HIGHER五种优先级分类，INVALID状态表示CPU不可用，用于全局任务调度时快速计算迁移决策",
          "similarity": 0.5625345706939697
        }
      ]
    },
    {
      "source_file": "kernel/sched/sched.h",
      "md_summary": "> 自动生成时间: 2025-10-25 16:16:13\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\sched.h`\n\n---\n\n# `sched/sched.h` 技术文档\n\n## 1. 文件概述\n\n`sched/sched.h` 是 Linux 内核调度器（Scheduler）的核心内部头文件，定义了调度子系统内部使用的类型、宏、辅助函数和全局变量。该文件不对外暴露给其他子系统直接使用，而是作为调度器各组件（如 CFS、RT、Deadline 调度类）之间的内部接口和共享基础设施。它整合了任务状态管理、负载计算、策略判断、CPU 能力建模、cgroup 权重转换等关键调度逻辑，并为调试、性能追踪和平台适配提供支持。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct asym_cap_data`：用于描述非对称 CPU 架构中不同 CPU 集合的计算能力（capacity），支持异构多核系统（如 big.LITTLE）的调度优化。\n- `struct rq`（前向声明）：运行队列（runqueue）结构体，每个 CPU 对应一个，是调度器管理可运行任务的核心数据结构。\n- `struct cpuidle_state`（前向声明）：CPU 空闲状态信息，用于与调度器协同进行能效管理。\n\n### 关键全局变量\n- `scheduler_running`：标志调度器是否已启动。\n- `calc_load_update` / `calc_load_tasks`：用于全局负载（load average）计算的时间戳和任务计数。\n- `sysctl_sched_rt_period` / `sysctl_sched_rt_runtime`：实时任务带宽控制参数。\n- `sched_rr_timeslice`：SCHED_RR 策略的时间片长度。\n- `asym_cap_list`：非对称 CPU 能力数据的全局链表。\n\n### 核心辅助函数与宏\n- **任务策略判断函数**：\n  - `idle_policy()` / `task_has_idle_policy()`\n  - `normal_policy()` / `fair_policy()`\n  - `rt_policy()` / `task_has_rt_policy()`\n  - `dl_policy()` / `task_has_dl_policy()`\n  - `valid_policy()`\n- **负载与权重转换**：\n  - `scale_load()` / `scale_load_down()`：在内部高精度负载值与用户可见权重间转换。\n  - `sched_weight_from_cgroup()` / `sched_weight_to_cgroup()`：cgroup 权重与调度器内部权重的映射。\n- **时间与精度处理**：\n  - `NS_TO_JIFFIES()`：纳秒转 jiffies。\n  - `update_avg()`：指数移动平均（EMA）更新。\n  - `shr_bound()`：安全右移，避免未定义行为。\n- **特殊调度标志**：\n  - `SCHED_FLAG_SUGOV`：用于 schedutil 频率调节器的特殊标志，使相关 kworker 临时获得高于 SCHED_DEADLINE 的优先级。\n  - `dl_entity_is_special()`：判断 Deadline 实体是否为 SUGOV 特殊任务。\n\n### 重要宏定义\n- `TASK_ON_RQ_QUEUED` / `TASK_ON_RQ_MIGRATING`：`task_struct::on_rq` 字段的状态值。\n- `NICE_0_LOAD`：nice 值为 0 的任务对应的内部负载基准值。\n- `DL_SCALE`：SCHED_DEADLINE 内部计算的精度因子。\n- `RUNTIME_INF`：表示无限运行时间的常量。\n- `SCHED_WARN_ON()`：调度器专用的条件警告宏（仅在 `CONFIG_SCHED_DEBUG` 时生效）。\n\n## 3. 关键实现\n\n### 高精度负载计算（64 位优化）\n在 64 位架构上，通过 `NICE_0_LOAD_SHIFT = 2 * SCHED_FIXEDPOINT_SHIFT` 提升内部负载计算的精度，改善低权重任务组（如 nice +19）和深层 cgroup 层级的负载均衡效果。`scale_load()` 和 `scale_load_down()` 实现了用户权重与内部高精度负载值之间的无损转换。\n\n### 非对称 CPU 能力建模\n`asym_cap_data` 结构体结合 `cpu_capacity_span()` 宏，将具有相同计算能力的 CPU 归为一组，并通过全局链表 `asym_cap_list` 管理。这为调度器在异构系统中进行负载均衡和任务迁移提供关键拓扑信息。\n\n### cgroup 权重标准化\n通过 `sched_weight_from_cgroup()` 和 `sched_weight_to_cgroup()`，将 cgroup 接口的权重范围（1–10000，默认 100）映射到调度器内部使用的权重值（基于 1024 基准），确保用户配置与调度行为的一致性。\n\n### SCHED_DEADLINE 与频率调节协同\n引入 `SCHED_FLAG_SUGOV` 标志，允许 `schedutil` 频率调节器的工作线程在需要时临时突破 SCHED_DEADLINE 的优先级限制，以解决某些平台无法原子切换 CPU 频率的问题。这是一种临时性 workaround，依赖于 `dl_entity_is_special()` 进行识别。\n\n### 安全位运算\n`shr_bound()` 宏确保右移操作不会因移位数过大而触发未定义行为（UB），通过 `min_t()` 将移位数限制在 `BITS_PER_TYPE(val) - 1` 以内。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- **调度子系统内部**：包含多个调度相关子模块头文件（如 `affinity.h`, `deadline.h`, `topology.h`, `cpupri.h` 等）。\n- **核心内核设施**：依赖 `atomic.h`, `rcupdate.h`, `cpumask_api.h`, `ktime_api.h`, `trace/events/sched.h` 等。\n- **平台与虚拟化**：条件包含 `asm/paravirt.h`（半虚拟化支持）和 `asm/barrier.h`（内存屏障）。\n- **工作队列**：包含 `../workqueue_internal.h`，用于与工作队列子系统交互。\n\n### 配置选项依赖\n- `CONFIG_64BIT`：启用高精度负载计算。\n- `CONFIG_SCHED_DEBUG`：启用 `SCHED_WARN_ON()` 调试检查。\n- `CONFIG_CPU_FREQ_GOV_SCHEDUTIL`：启用 `SCHED_FLAG_SUGOV` 相关逻辑。\n- `CONFIG_SCHED_CLASS_EXT`：扩展调度类支持（影响 `normal_policy()` 判断）。\n- `CONFIG_PARAVIRT`：半虚拟化支持。\n\n## 5. 使用场景\n\n- **调度器初始化与运行**：`scheduler_running` 和负载计算变量在调度器启动和周期性负载更新中使用。\n- **任务调度策略处理**：所有调度类（CFS、RT、Deadline、Idle）在入队、出队、选择下一个任务时，通过策略判断函数确定任务类型。\n- **负载均衡与迁移**：`asym_cap_data` 和 CPU 拓扑信息用于跨 CPU 的任务迁移决策，尤其在异构系统中。\n- **cgroup 资源控制**：在设置或读取 cgroup 的 CPU 权重时，通过权重转换函数确保调度器内部表示与用户接口一致。\n- **实时带宽管理**：`sysctl_sched_rt_*` 参数用于限制 SCHED_FIFO/SCHED_RR 任务的 CPU 使用率。\n- **能效调度协同**：`SCHED_FLAG_SUGOV` 机制使频率调节器能及时响应 Deadline 任务的性能需求。\n- **内核调试与追踪**：`SCHED_WARN_ON()` 用于捕获调度器内部异常状态；tracepoint 定义支持调度事件追踪。",
      "similarity": 0.5672330856323242,
      "chunks": []
    }
  ]
}