{
  "query": "Linux虚拟内存实现原理",
  "timestamp": "2025-12-25 23:25:51",
  "retrieved_files": [
    {
      "source_file": "mm/vmalloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:32:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmalloc.c`\n\n---\n\n# vmalloc.c 技术文档\n\n## 1. 文件概述\n\n`vmalloc.c` 是 Linux 内核中实现虚拟内存分配（vmalloc）机制的核心源文件。该文件提供了在内核虚拟地址空间中非连续物理页映射为连续虚拟地址的功能，主要用于分配大块内存、I/O 映射（如 `ioremap`）以及需要页表特殊属性（如不可执行、缓存控制等）的场景。与 `kmalloc` 不同，`vmalloc` 分配的内存物理上不连续，但虚拟地址连续，适用于大内存分配或硬件寄存器映射。\n\n## 2. 核心功能\n\n### 主要函数\n- `is_vmalloc_addr(const void *x)`：判断给定地址是否位于 vmalloc 区域。\n- `vmap_page_range(unsigned long addr, unsigned long end, phys_addr_t phys_addr, pgprot_t prot)`：将指定物理地址范围映射到内核虚拟地址空间，支持普通页和大页。\n- `ioremap_page_range(...)`：用于 I/O 内存重映射（代码片段未完整展示）。\n- `vmap_range_noflush(...)`：执行实际的页表填充操作，不触发 TLB 刷新。\n- `vmap_pte_range`, `vmap_pmd_range`, `vmap_pud_range`, `vmap_p4d_range`：逐级填充页表项的辅助函数。\n- `vmap_try_huge_*` 系列函数（如 `vmap_try_huge_pmd`）：尝试使用大页（huge page）进行映射以提升性能。\n\n### 主要数据结构\n- `struct vfree_deferred`：用于延迟释放 vmalloc 内存的 per-CPU 工作队列结构。\n- `ioremap_max_page_shift`：控制 I/O 映射时允许的最大页面大小（受 `nohugeiomap` 启动参数影响）。\n- `vmap_allow_huge`：控制 vmalloc 是否允许使用大页（受 `nohugevmalloc` 启动参数影响）。\n\n## 3. 关键实现\n\n### 大页（Huge Page）支持\n- 通过 `CONFIG_HAVE_ARCH_HUGE_VMAP` 和 `CONFIG_HAVE_ARCH_HUGE_VMALLOC` 配置选项启用架构相关的大页映射能力。\n- 在页表填充过程中（如 `vmap_pmd_range`），优先尝试使用 PMD/PUD/P4D 级别的大页映射（通过 `vmap_try_huge_pmd` 等函数），前提是：\n  - 地址和物理地址对齐；\n  - 请求区域大小等于对应层级的大页尺寸；\n  - 架构支持该级别的大页（通过 `arch_vmap_*_supported` 判断）；\n  - 当前页表项未被占用或可安全释放下级页表。\n- 启动参数 `nohugeiomap` 和 `nohugevmalloc` 可分别禁用 I/O 映射和通用 vmalloc 的大页功能。\n\n### 页表操作与跟踪\n- 使用 `_track` 后缀的页表分配函数（如 `pte_alloc_kernel_track`）配合 `pgtbl_mod_mask` 标记修改的页表层级，便于后续同步（如 `arch_sync_kernel_mappings`）。\n- 映射完成后调用 `flush_cache_vmap` 确保缓存一致性，并集成 KMSAN（Kernel Memory Sanitizer）支持。\n\n### 安全与调试\n- 使用 `kasan_reset_tag` 处理 KASAN 的内存标记，确保地址比较正确。\n- 通过 `BUG_ON` 检查页表项是否为空，防止覆盖已有映射。\n- 支持 `kmemleak` 内存泄漏检测。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/highmem.h>`、`<linux/pfn.h>` 等提供基础内存操作。\n- **体系结构相关接口**：通过 `asm/tlbflush.h`、`asm/shmparam.h` 及 `arch_*` 函数（如 `arch_make_huge_pte`）适配不同 CPU 架构。\n- **内核基础设施**：\n  - RCU（`<linux/rcupdate.h>`）用于安全遍历；\n  - Per-CPU 变量（`DEFINE_PER_CPU`）实现无锁延迟释放；\n  - 工作队列（`work_struct`）处理异步释放；\n  - 调试工具（`debugobjects`、`kallsyms`、`trace/events/vmalloc.h`）。\n- **安全特性**：集成 KASAN、KMSAN、`set_memory.h`（页属性设置）等。\n\n## 5. 使用场景\n\n- **内核模块加载**：模块的代码和数据通常通过 `vmalloc` 分配。\n- **大内存分配**：当所需内存超过 `kmalloc` 的限制（通常几 MB）时使用。\n- **设备 I/O 映射**：通过 `ioremap` 将设备寄存器映射到内核地址空间，底层调用 `ioremap_page_range`。\n- **动态内核数据结构**：如网络协议栈的某些缓冲区、文件系统元数据缓存等。\n- **安全隔离**：为敏感数据分配具有特殊页属性（如不可执行 NX）的内存区域。",
      "similarity": 0.6274971961975098,
      "chunks": [
        {
          "chunk_id": 8,
          "file_path": "mm/vmalloc.c",
          "start_line": 1465,
          "end_line": 1624,
          "content": [
            "static __always_inline bool",
            "is_within_this_va(struct vmap_area *va, unsigned long size,",
            "\tunsigned long align, unsigned long vstart)",
            "{",
            "\tunsigned long nva_start_addr;",
            "",
            "\tif (va->va_start > vstart)",
            "\t\tnva_start_addr = ALIGN(va->va_start, align);",
            "\telse",
            "\t\tnva_start_addr = ALIGN(vstart, align);",
            "",
            "\t/* Can be overflowed due to big size or alignment. */",
            "\tif (nva_start_addr + size < nva_start_addr ||",
            "\t\t\tnva_start_addr < vstart)",
            "\t\treturn false;",
            "",
            "\treturn (nva_start_addr + size <= va->va_end);",
            "}",
            "static void",
            "find_vmap_lowest_match_check(struct rb_root *root, struct list_head *head,",
            "\t\t\t     unsigned long size, unsigned long align)",
            "{",
            "\tstruct vmap_area *va_1, *va_2;",
            "\tunsigned long vstart;",
            "\tunsigned int rnd;",
            "",
            "\tget_random_bytes(&rnd, sizeof(rnd));",
            "\tvstart = VMALLOC_START + rnd;",
            "",
            "\tva_1 = find_vmap_lowest_match(root, size, align, vstart, false);",
            "\tva_2 = find_vmap_lowest_linear_match(head, size, align, vstart);",
            "",
            "\tif (va_1 != va_2)",
            "\t\tpr_emerg(\"not lowest: t: 0x%p, l: 0x%p, v: 0x%lx\\n\",",
            "\t\t\tva_1, va_2, vstart);",
            "}",
            "static __always_inline enum fit_type",
            "classify_va_fit_type(struct vmap_area *va,",
            "\tunsigned long nva_start_addr, unsigned long size)",
            "{",
            "\tenum fit_type type;",
            "",
            "\t/* Check if it is within VA. */",
            "\tif (nva_start_addr < va->va_start ||",
            "\t\t\tnva_start_addr + size > va->va_end)",
            "\t\treturn NOTHING_FIT;",
            "",
            "\t/* Now classify. */",
            "\tif (va->va_start == nva_start_addr) {",
            "\t\tif (va->va_end == nva_start_addr + size)",
            "\t\t\ttype = FL_FIT_TYPE;",
            "\t\telse",
            "\t\t\ttype = LE_FIT_TYPE;",
            "\t} else if (va->va_end == nva_start_addr + size) {",
            "\t\ttype = RE_FIT_TYPE;",
            "\t} else {",
            "\t\ttype = NE_FIT_TYPE;",
            "\t}",
            "",
            "\treturn type;",
            "}",
            "static __always_inline int",
            "va_clip(struct rb_root *root, struct list_head *head,",
            "\t\tstruct vmap_area *va, unsigned long nva_start_addr,",
            "\t\tunsigned long size)",
            "{",
            "\tstruct vmap_area *lva = NULL;",
            "\tenum fit_type type = classify_va_fit_type(va, nva_start_addr, size);",
            "",
            "\tif (type == FL_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * No need to split VA, it fully fits.",
            "\t\t *",
            "\t\t * |               |",
            "\t\t * V      NVA      V",
            "\t\t * |---------------|",
            "\t\t */",
            "\t\tunlink_va_augment(va, root);",
            "\t\tkmem_cache_free(vmap_area_cachep, va);",
            "\t} else if (type == LE_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * Split left edge of fit VA.",
            "\t\t *",
            "\t\t * |       |",
            "\t\t * V  NVA  V   R",
            "\t\t * |-------|-------|",
            "\t\t */",
            "\t\tva->va_start += size;",
            "\t} else if (type == RE_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * Split right edge of fit VA.",
            "\t\t *",
            "\t\t *         |       |",
            "\t\t *     L   V  NVA  V",
            "\t\t * |-------|-------|",
            "\t\t */",
            "\t\tva->va_end = nva_start_addr;",
            "\t} else if (type == NE_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * Split no edge of fit VA.",
            "\t\t *",
            "\t\t *     |       |",
            "\t\t *   L V  NVA  V R",
            "\t\t * |---|-------|---|",
            "\t\t */",
            "\t\tlva = __this_cpu_xchg(ne_fit_preload_node, NULL);",
            "\t\tif (unlikely(!lva)) {",
            "\t\t\t/*",
            "\t\t\t * For percpu allocator we do not do any pre-allocation",
            "\t\t\t * and leave it as it is. The reason is it most likely",
            "\t\t\t * never ends up with NE_FIT_TYPE splitting. In case of",
            "\t\t\t * percpu allocations offsets and sizes are aligned to",
            "\t\t\t * fixed align request, i.e. RE_FIT_TYPE and FL_FIT_TYPE",
            "\t\t\t * are its main fitting cases.",
            "\t\t\t *",
            "\t\t\t * There are a few exceptions though, as an example it is",
            "\t\t\t * a first allocation (early boot up) when we have \"one\"",
            "\t\t\t * big free space that has to be split.",
            "\t\t\t *",
            "\t\t\t * Also we can hit this path in case of regular \"vmap\"",
            "\t\t\t * allocations, if \"this\" current CPU was not preloaded.",
            "\t\t\t * See the comment in alloc_vmap_area() why. If so, then",
            "\t\t\t * GFP_NOWAIT is used instead to get an extra object for",
            "\t\t\t * split purpose. That is rare and most time does not",
            "\t\t\t * occur.",
            "\t\t\t *",
            "\t\t\t * What happens if an allocation gets failed. Basically,",
            "\t\t\t * an \"overflow\" path is triggered to purge lazily freed",
            "\t\t\t * areas to free some memory, then, the \"retry\" path is",
            "\t\t\t * triggered to repeat one more time. See more details",
            "\t\t\t * in alloc_vmap_area() function.",
            "\t\t\t */",
            "\t\t\tlva = kmem_cache_alloc(vmap_area_cachep, GFP_NOWAIT);",
            "\t\t\tif (!lva)",
            "\t\t\t\treturn -1;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Build the remainder.",
            "\t\t */",
            "\t\tlva->va_start = va->va_start;",
            "\t\tlva->va_end = nva_start_addr;",
            "",
            "\t\t/*",
            "\t\t * Shrink this VA to remaining size.",
            "\t\t */",
            "\t\tva->va_start = nva_start_addr + size;",
            "\t} else {",
            "\t\treturn -1;",
            "\t}",
            "",
            "\tif (type != FL_FIT_TYPE) {",
            "\t\taugment_tree_propagate_from(va);",
            "",
            "\t\tif (lva)\t/* type == NE_FIT_TYPE */",
            "\t\t\tinsert_vmap_area_augment(lva, &va->rb_node, root, head);",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "is_within_this_va, find_vmap_lowest_match_check, classify_va_fit_type, va_clip",
          "description": "实现虚拟内存区域匹配算法，通过分类判断（FL/LE/RE/NE）进行地址裁剪和区域分割，支持不同场景下的内存分配策略选择",
          "similarity": 0.6536959409713745
        },
        {
          "chunk_id": 9,
          "file_path": "mm/vmalloc.c",
          "start_line": 1728,
          "end_line": 1843,
          "content": [
            "static unsigned long",
            "va_alloc(struct vmap_area *va,",
            "\t\tstruct rb_root *root, struct list_head *head,",
            "\t\tunsigned long size, unsigned long align,",
            "\t\tunsigned long vstart, unsigned long vend)",
            "{",
            "\tunsigned long nva_start_addr;",
            "\tint ret;",
            "",
            "\tif (va->va_start > vstart)",
            "\t\tnva_start_addr = ALIGN(va->va_start, align);",
            "\telse",
            "\t\tnva_start_addr = ALIGN(vstart, align);",
            "",
            "\t/* Check the \"vend\" restriction. */",
            "\tif (nva_start_addr + size > vend)",
            "\t\treturn vend;",
            "",
            "\t/* Update the free vmap_area. */",
            "\tret = va_clip(root, head, va, nva_start_addr, size);",
            "\tif (WARN_ON_ONCE(ret))",
            "\t\treturn vend;",
            "",
            "\treturn nva_start_addr;",
            "}",
            "static __always_inline unsigned long",
            "__alloc_vmap_area(struct rb_root *root, struct list_head *head,",
            "\tunsigned long size, unsigned long align,",
            "\tunsigned long vstart, unsigned long vend)",
            "{",
            "\tbool adjust_search_size = true;",
            "\tunsigned long nva_start_addr;",
            "\tstruct vmap_area *va;",
            "",
            "\t/*",
            "\t * Do not adjust when:",
            "\t *   a) align <= PAGE_SIZE, because it does not make any sense.",
            "\t *      All blocks(their start addresses) are at least PAGE_SIZE",
            "\t *      aligned anyway;",
            "\t *   b) a short range where a requested size corresponds to exactly",
            "\t *      specified [vstart:vend] interval and an alignment > PAGE_SIZE.",
            "\t *      With adjusted search length an allocation would not succeed.",
            "\t */",
            "\tif (align <= PAGE_SIZE || (align > PAGE_SIZE && (vend - vstart) == size))",
            "\t\tadjust_search_size = false;",
            "",
            "\tva = find_vmap_lowest_match(root, size, align, vstart, adjust_search_size);",
            "\tif (unlikely(!va))",
            "\t\treturn vend;",
            "",
            "\tnva_start_addr = va_alloc(va, root, head, size, align, vstart, vend);",
            "\tif (nva_start_addr == vend)",
            "\t\treturn vend;",
            "",
            "#if DEBUG_AUGMENT_LOWEST_MATCH_CHECK",
            "\tfind_vmap_lowest_match_check(root, head, size, align);",
            "#endif",
            "",
            "\treturn nva_start_addr;",
            "}",
            "static void free_vmap_area(struct vmap_area *va)",
            "{",
            "\tstruct vmap_node *vn = addr_to_node(va->va_start);",
            "",
            "\t/*",
            "\t * Remove from the busy tree/list.",
            "\t */",
            "\tspin_lock(&vn->busy.lock);",
            "\tunlink_va(va, &vn->busy.root);",
            "\tspin_unlock(&vn->busy.lock);",
            "",
            "\t/*",
            "\t * Insert/Merge it back to the free tree/list.",
            "\t */",
            "\tspin_lock(&free_vmap_area_lock);",
            "\tmerge_or_add_vmap_area_augment(va, &free_vmap_area_root, &free_vmap_area_list);",
            "\tspin_unlock(&free_vmap_area_lock);",
            "}",
            "static inline void",
            "preload_this_cpu_lock(spinlock_t *lock, gfp_t gfp_mask, int node)",
            "{",
            "\tstruct vmap_area *va = NULL;",
            "",
            "\t/*",
            "\t * Preload this CPU with one extra vmap_area object. It is used",
            "\t * when fit type of free area is NE_FIT_TYPE. It guarantees that",
            "\t * a CPU that does an allocation is preloaded.",
            "\t *",
            "\t * We do it in non-atomic context, thus it allows us to use more",
            "\t * permissive allocation masks to be more stable under low memory",
            "\t * condition and high memory pressure.",
            "\t */",
            "\tif (!this_cpu_read(ne_fit_preload_node))",
            "\t\tva = kmem_cache_alloc_node(vmap_area_cachep, gfp_mask, node);",
            "",
            "\tspin_lock(lock);",
            "",
            "\tif (va && __this_cpu_cmpxchg(ne_fit_preload_node, NULL, va))",
            "\t\tkmem_cache_free(vmap_area_cachep, va);",
            "}",
            "static bool",
            "node_pool_add_va(struct vmap_node *n, struct vmap_area *va)",
            "{",
            "\tstruct vmap_pool *vp;",
            "",
            "\tvp = size_to_va_pool(n, va_size(va));",
            "\tif (!vp)",
            "\t\treturn false;",
            "",
            "\tspin_lock(&n->pool_lock);",
            "\tlist_add(&va->list, &vp->head);",
            "\tWRITE_ONCE(vp->len, vp->len + 1);",
            "\tspin_unlock(&n->pool_lock);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "va_alloc, __alloc_vmap_area, free_vmap_area, preload_this_cpu_lock, node_pool_add_va",
          "description": "实现虚拟内存区域的分配/回收核心逻辑，包含智能搜索算法、节点池管理及预加载机制，确保高并发场景下的内存分配稳定性",
          "similarity": 0.6488128900527954
        },
        {
          "chunk_id": 7,
          "file_path": "mm/vmalloc.c",
          "start_line": 1208,
          "end_line": 1309,
          "content": [
            "static __always_inline void",
            "link_va(struct vmap_area *va, struct rb_root *root,",
            "\tstruct rb_node *parent, struct rb_node **link,",
            "\tstruct list_head *head)",
            "{",
            "\t__link_va(va, root, parent, link, head, false);",
            "}",
            "static __always_inline void",
            "link_va_augment(struct vmap_area *va, struct rb_root *root,",
            "\tstruct rb_node *parent, struct rb_node **link,",
            "\tstruct list_head *head)",
            "{",
            "\t__link_va(va, root, parent, link, head, true);",
            "}",
            "static __always_inline void",
            "__unlink_va(struct vmap_area *va, struct rb_root *root, bool augment)",
            "{",
            "\tif (WARN_ON(RB_EMPTY_NODE(&va->rb_node)))",
            "\t\treturn;",
            "",
            "\tif (augment)",
            "\t\trb_erase_augmented(&va->rb_node,",
            "\t\t\troot, &free_vmap_area_rb_augment_cb);",
            "\telse",
            "\t\trb_erase(&va->rb_node, root);",
            "",
            "\tlist_del_init(&va->list);",
            "\tRB_CLEAR_NODE(&va->rb_node);",
            "}",
            "static __always_inline void",
            "unlink_va(struct vmap_area *va, struct rb_root *root)",
            "{",
            "\t__unlink_va(va, root, false);",
            "}",
            "static __always_inline void",
            "unlink_va_augment(struct vmap_area *va, struct rb_root *root)",
            "{",
            "\t__unlink_va(va, root, true);",
            "}",
            "static __always_inline unsigned long",
            "compute_subtree_max_size(struct vmap_area *va)",
            "{",
            "\treturn max3(va_size(va),",
            "\t\tget_subtree_max_size(va->rb_node.rb_left),",
            "\t\tget_subtree_max_size(va->rb_node.rb_right));",
            "}",
            "static void",
            "augment_tree_propagate_check(void)",
            "{",
            "\tstruct vmap_area *va;",
            "\tunsigned long computed_size;",
            "",
            "\tlist_for_each_entry(va, &free_vmap_area_list, list) {",
            "\t\tcomputed_size = compute_subtree_max_size(va);",
            "\t\tif (computed_size != va->subtree_max_size)",
            "\t\t\tpr_emerg(\"tree is corrupted: %lu, %lu\\n\",",
            "\t\t\t\tva_size(va), va->subtree_max_size);",
            "\t}",
            "}",
            "static __always_inline void",
            "augment_tree_propagate_from(struct vmap_area *va)",
            "{",
            "\t/*",
            "\t * Populate the tree from bottom towards the root until",
            "\t * the calculated maximum available size of checked node",
            "\t * is equal to its current one.",
            "\t */",
            "\tfree_vmap_area_rb_augment_cb_propagate(&va->rb_node, NULL);",
            "",
            "#if DEBUG_AUGMENT_PROPAGATE_CHECK",
            "\taugment_tree_propagate_check();",
            "#endif",
            "}",
            "static void",
            "insert_vmap_area(struct vmap_area *va,",
            "\tstruct rb_root *root, struct list_head *head)",
            "{",
            "\tstruct rb_node **link;",
            "\tstruct rb_node *parent;",
            "",
            "\tlink = find_va_links(va, root, NULL, &parent);",
            "\tif (link)",
            "\t\tlink_va(va, root, parent, link, head);",
            "}",
            "static void",
            "insert_vmap_area_augment(struct vmap_area *va,",
            "\tstruct rb_node *from, struct rb_root *root,",
            "\tstruct list_head *head)",
            "{",
            "\tstruct rb_node **link;",
            "\tstruct rb_node *parent;",
            "",
            "\tif (from)",
            "\t\tlink = find_va_links(va, NULL, from, &parent);",
            "\telse",
            "\t\tlink = find_va_links(va, root, NULL, &parent);",
            "",
            "\tif (link) {",
            "\t\tlink_va_augment(va, root, parent, link, head);",
            "\t\taugment_tree_propagate_from(va);",
            "\t}",
            "}"
          ],
          "function_name": "link_va, link_va_augment, __unlink_va, unlink_va, unlink_va_augment, compute_subtree_max_size, augment_tree_propagate_check, augment_tree_propagate_from, insert_vmap_area, insert_vmap_area_augment",
          "description": "实现基于红黑树的虚拟内存区域动态管理，包含节点插入/删除、子树最大尺寸计算及传播机制，确保树结构平衡性与查询效率",
          "similarity": 0.6487330794334412
        },
        {
          "chunk_id": 18,
          "file_path": "mm/vmalloc.c",
          "start_line": 4366,
          "end_line": 4510,
          "content": [
            "long vread_iter(struct iov_iter *iter, const char *addr, size_t count)",
            "{",
            "\tstruct vmap_node *vn;",
            "\tstruct vmap_area *va;",
            "\tstruct vm_struct *vm;",
            "\tchar *vaddr;",
            "\tsize_t n, size, flags, remains;",
            "\tunsigned long next;",
            "",
            "\taddr = kasan_reset_tag(addr);",
            "",
            "\t/* Don't allow overflow */",
            "\tif ((unsigned long) addr + count < count)",
            "\t\tcount = -(unsigned long) addr;",
            "",
            "\tremains = count;",
            "",
            "\tvn = find_vmap_area_exceed_addr_lock((unsigned long) addr, &va);",
            "\tif (!vn)",
            "\t\tgoto finished_zero;",
            "",
            "\t/* no intersects with alive vmap_area */",
            "\tif ((unsigned long)addr + remains <= va->va_start)",
            "\t\tgoto finished_zero;",
            "",
            "\tdo {",
            "\t\tsize_t copied;",
            "",
            "\t\tif (remains == 0)",
            "\t\t\tgoto finished;",
            "",
            "\t\tvm = va->vm;",
            "\t\tflags = va->flags & VMAP_FLAGS_MASK;",
            "\t\t/*",
            "\t\t * VMAP_BLOCK indicates a sub-type of vm_map_ram area, need",
            "\t\t * be set together with VMAP_RAM.",
            "\t\t */",
            "\t\tWARN_ON(flags == VMAP_BLOCK);",
            "",
            "\t\tif (!vm && !flags)",
            "\t\t\tgoto next_va;",
            "",
            "\t\tif (vm && (vm->flags & VM_UNINITIALIZED))",
            "\t\t\tgoto next_va;",
            "",
            "\t\t/* Pair with smp_wmb() in clear_vm_uninitialized_flag() */",
            "\t\tsmp_rmb();",
            "",
            "\t\tvaddr = (char *) va->va_start;",
            "\t\tsize = vm ? get_vm_area_size(vm) : va_size(va);",
            "",
            "\t\tif (addr >= vaddr + size)",
            "\t\t\tgoto next_va;",
            "",
            "\t\tif (addr < vaddr) {",
            "\t\t\tsize_t to_zero = min_t(size_t, vaddr - addr, remains);",
            "\t\t\tsize_t zeroed = zero_iter(iter, to_zero);",
            "",
            "\t\t\taddr += zeroed;",
            "\t\t\tremains -= zeroed;",
            "",
            "\t\t\tif (remains == 0 || zeroed != to_zero)",
            "\t\t\t\tgoto finished;",
            "\t\t}",
            "",
            "\t\tn = vaddr + size - addr;",
            "\t\tif (n > remains)",
            "\t\t\tn = remains;",
            "",
            "\t\tif (flags & VMAP_RAM)",
            "\t\t\tcopied = vmap_ram_vread_iter(iter, addr, n, flags);",
            "\t\telse if (!(vm && (vm->flags & (VM_IOREMAP | VM_SPARSE))))",
            "\t\t\tcopied = aligned_vread_iter(iter, addr, n);",
            "\t\telse /* IOREMAP | SPARSE area is treated as memory hole */",
            "\t\t\tcopied = zero_iter(iter, n);",
            "",
            "\t\taddr += copied;",
            "\t\tremains -= copied;",
            "",
            "\t\tif (copied != n)",
            "\t\t\tgoto finished;",
            "",
            "\tnext_va:",
            "\t\tnext = va->va_end;",
            "\t\tspin_unlock(&vn->busy.lock);",
            "\t} while ((vn = find_vmap_area_exceed_addr_lock(next, &va)));",
            "",
            "finished_zero:",
            "\tif (vn)",
            "\t\tspin_unlock(&vn->busy.lock);",
            "",
            "\t/* zero-fill memory holes */",
            "\treturn count - remains + zero_iter(iter, remains);",
            "finished:",
            "\t/* Nothing remains, or We couldn't copy/zero everything. */",
            "\tif (vn)",
            "\t\tspin_unlock(&vn->busy.lock);",
            "",
            "\treturn count - remains;",
            "}",
            "int remap_vmalloc_range_partial(struct vm_area_struct *vma, unsigned long uaddr,",
            "\t\t\t\tvoid *kaddr, unsigned long pgoff,",
            "\t\t\t\tunsigned long size)",
            "{",
            "\tstruct vm_struct *area;",
            "\tunsigned long off;",
            "\tunsigned long end_index;",
            "",
            "\tif (check_shl_overflow(pgoff, PAGE_SHIFT, &off))",
            "\t\treturn -EINVAL;",
            "",
            "\tsize = PAGE_ALIGN(size);",
            "",
            "\tif (!PAGE_ALIGNED(uaddr) || !PAGE_ALIGNED(kaddr))",
            "\t\treturn -EINVAL;",
            "",
            "\tarea = find_vm_area(kaddr);",
            "\tif (!area)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!(area->flags & (VM_USERMAP | VM_DMA_COHERENT)))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (check_add_overflow(size, off, &end_index) ||",
            "\t    end_index > get_vm_area_size(area))",
            "\t\treturn -EINVAL;",
            "\tkaddr += off;",
            "",
            "\tdo {",
            "\t\tstruct page *page = vmalloc_to_page(kaddr);",
            "\t\tint ret;",
            "",
            "\t\tret = vm_insert_page(vma, uaddr, page);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "",
            "\t\tuaddr += PAGE_SIZE;",
            "\t\tkaddr += PAGE_SIZE;",
            "\t\tsize -= PAGE_SIZE;",
            "\t} while (size > 0);",
            "",
            "\tvm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vread_iter, remap_vmalloc_range_partial",
          "description": "vread_iter 实现虚拟内存区域的数据读取，处理零填充、对齐读取和RAM区域特殊读取逻辑；remap_vmalloc_range_partial 部分重新映射虚拟内存区域，将内核空间页面插入用户虚拟地址空间",
          "similarity": 0.6279593706130981
        },
        {
          "chunk_id": 6,
          "file_path": "mm/vmalloc.c",
          "start_line": 691,
          "end_line": 822,
          "content": [
            "int vm_area_map_pages(struct vm_struct *area, unsigned long start,",
            "\t\t      unsigned long end, struct page **pages)",
            "{",
            "\tint err;",
            "",
            "\terr = check_sparse_vm_area(area, start, end);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\treturn vmap_pages_range(start, end, PAGE_KERNEL, pages, PAGE_SHIFT);",
            "}",
            "void vm_area_unmap_pages(struct vm_struct *area, unsigned long start,",
            "\t\t\t unsigned long end)",
            "{",
            "\tif (check_sparse_vm_area(area, start, end))",
            "\t\treturn;",
            "",
            "\tvunmap_range(start, end);",
            "}",
            "int is_vmalloc_or_module_addr(const void *x)",
            "{",
            "\t/*",
            "\t * ARM, x86-64 and sparc64 put modules in a special place,",
            "\t * and fall back on vmalloc() if that fails. Others",
            "\t * just put it in the vmalloc space.",
            "\t */",
            "#if defined(CONFIG_MODULES) && defined(MODULES_VADDR)",
            "\tunsigned long addr = (unsigned long)kasan_reset_tag(x);",
            "\tif (addr >= MODULES_VADDR && addr < MODULES_END)",
            "\t\treturn 1;",
            "#endif",
            "\treturn is_vmalloc_addr(x);",
            "}",
            "unsigned long vmalloc_to_pfn(const void *vmalloc_addr)",
            "{",
            "\treturn page_to_pfn(vmalloc_to_page(vmalloc_addr));",
            "}",
            "static inline unsigned int",
            "addr_to_node_id(unsigned long addr)",
            "{",
            "\treturn (addr / vmap_zone_size) % nr_vmap_nodes;",
            "}",
            "static unsigned int",
            "encode_vn_id(unsigned int node_id)",
            "{",
            "\t/* Can store U8_MAX [0:254] nodes. */",
            "\tif (node_id < nr_vmap_nodes)",
            "\t\treturn (node_id + 1) << BITS_PER_BYTE;",
            "",
            "\t/* Warn and no node encoded. */",
            "\tWARN_ONCE(1, \"Encode wrong node id (%u)\\n\", node_id);",
            "\treturn 0;",
            "}",
            "static unsigned int",
            "decode_vn_id(unsigned int val)",
            "{",
            "\tunsigned int node_id = (val >> BITS_PER_BYTE) - 1;",
            "",
            "\t/* Can store U8_MAX [0:254] nodes. */",
            "\tif (node_id < nr_vmap_nodes)",
            "\t\treturn node_id;",
            "",
            "\t/* If it was _not_ zero, warn. */",
            "\tWARN_ONCE(node_id != UINT_MAX,",
            "\t\t\"Decode wrong node id (%d)\\n\", node_id);",
            "",
            "\treturn nr_vmap_nodes;",
            "}",
            "static bool",
            "is_vn_id_valid(unsigned int node_id)",
            "{",
            "\tif (node_id < nr_vmap_nodes)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "static __always_inline unsigned long",
            "va_size(struct vmap_area *va)",
            "{",
            "\treturn (va->va_end - va->va_start);",
            "}",
            "static __always_inline unsigned long",
            "get_subtree_max_size(struct rb_node *node)",
            "{",
            "\tstruct vmap_area *va;",
            "",
            "\tva = rb_entry_safe(node, struct vmap_area, rb_node);",
            "\treturn va ? va->subtree_max_size : 0;",
            "}",
            "unsigned long vmalloc_nr_pages(void)",
            "{",
            "\treturn atomic_long_read(&nr_vmalloc_pages);",
            "}",
            "static __always_inline void",
            "__link_va(struct vmap_area *va, struct rb_root *root,",
            "\tstruct rb_node *parent, struct rb_node **link,",
            "\tstruct list_head *head, bool augment)",
            "{",
            "\t/*",
            "\t * VA is still not in the list, but we can",
            "\t * identify its future previous list_head node.",
            "\t */",
            "\tif (likely(parent)) {",
            "\t\thead = &rb_entry(parent, struct vmap_area, rb_node)->list;",
            "\t\tif (&parent->rb_right != link)",
            "\t\t\thead = head->prev;",
            "\t}",
            "",
            "\t/* Insert to the rb-tree */",
            "\trb_link_node(&va->rb_node, parent, link);",
            "\tif (augment) {",
            "\t\t/*",
            "\t\t * Some explanation here. Just perform simple insertion",
            "\t\t * to the tree. We do not set va->subtree_max_size to",
            "\t\t * its current size before calling rb_insert_augmented().",
            "\t\t * It is because we populate the tree from the bottom",
            "\t\t * to parent levels when the node _is_ in the tree.",
            "\t\t *",
            "\t\t * Therefore we set subtree_max_size to zero after insertion,",
            "\t\t * to let __augment_tree_propagate_from() puts everything to",
            "\t\t * the correct order later on.",
            "\t\t */",
            "\t\trb_insert_augmented(&va->rb_node,",
            "\t\t\troot, &free_vmap_area_rb_augment_cb);",
            "\t\tva->subtree_max_size = 0;",
            "\t} else {",
            "\t\trb_insert_color(&va->rb_node, root);",
            "\t}",
            "",
            "\t/* Address-sort this list */",
            "\tlist_add(&va->list, head);",
            "}"
          ],
          "function_name": "vm_area_map_pages, vm_area_unmap_pages, is_vmalloc_or_module_addr, vmalloc_to_pfn, addr_to_node_id, encode_vn_id, decode_vn_id, is_vn_id_valid, va_size, get_subtree_max_size, vmalloc_nr_pages, __link_va",
          "description": "提供虚拟内存区域管理接口，包含区域有效性校验、地址转换、节点ID编码解码及基于红黑树的VA链表操作，用于跟踪和管理VMALLOC空间",
          "similarity": 0.6264230012893677
        }
      ]
    },
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.6273915767669678,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.6545310020446777
        },
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.615397572517395
        },
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.5045360326766968
        }
      ]
    },
    {
      "source_file": "mm/highmem.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:03:56\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `highmem.c`\n\n---\n\n# highmem.c 技术文档\n\n## 1. 文件概述\n\n`highmem.c` 是 Linux 内核中用于管理高内存（High Memory）的核心实现文件。在 32 位系统中，由于虚拟地址空间有限（通常为 4GB），内核无法将全部物理内存直接映射到内核虚拟地址空间。高内存指那些不能被永久映射到内核地址空间的物理内存页。该文件提供了对高内存页进行临时映射（kmap）和本地映射（kmap_local）的通用机制，使得内核代码能够安全地访问高内存中的数据。\n\n## 2. 核心功能\n\n### 主要函数\n- `kmap_high(struct page *page)`：将高内存页映射到内核虚拟地址空间，返回其虚拟地址。\n- `__kmap_flush_unused(void)`：释放所有未使用的持久性 kmap 映射。\n- `__kmap_to_page(void *vaddr)`：根据虚拟地址反向查找对应的 `struct page`。\n- `map_new_virtual(struct page *page)`：为指定页面分配一个新的持久性 kmap 虚拟地址。\n- `flush_all_zero_pkmaps(void)`：清除所有引用计数为 1（即空闲但尚未解除映射）的 PKMAP 条目。\n\n### 主要数据结构与变量\n- `pkmap_count[LAST_PKMAP]`：记录每个持久性 kmap 槽的引用状态。\n- `pkmap_page_table`：指向持久性 kmap 区域的页表项数组。\n- `_totalhigh_pages`：系统中高内存页的总数（全局可导出变量）。\n- `kmap_lock`：保护持久性 kmap 操作的自旋锁。\n- `__nr_free_highpages(void)`：计算当前系统中空闲的高内存页数量。\n\n## 3. 关键实现\n\n### 持久性 kmap 机制（PKMAP）\n- 使用固定大小的虚拟地址窗口（`PKMAP_ADDR(0)` 到 `PKMAP_ADDR(LAST_PKMAP)`）作为高内存页的映射区域。\n- `pkmap_count[]` 数组不仅记录引用计数，还编码映射状态：\n  - **0**：槽位空闲且 TLB 已刷新，可安全复用。\n  - **1**：槽位空闲但自上次 TLB 刷新后曾被使用，需先刷新 TLB 才能复用。\n  - **n (>1)**：当前有 (n-1) 个活跃用户。\n- 当无可用槽位时，调用者会进入不可中断睡眠，等待其他线程调用 `kunmap` 释放槽位。\n\n### 缓存着色支持（Cache Coloring）\n- 在具有别名数据缓存（Aliasing Data Cache）的架构上，通过 `get_pkmap_color()` 等钩子函数控制映射的“颜色”（即缓存索引），避免缓存冲突。\n- 默认实现（无缓存别名架构）返回颜色 0，所有映射共享同一等待队列。\n\n### 本地 kmap 支持（kmap_local）\n- 通过 `CONFIG_KMAP_LOCAL` 配置选项启用。\n- 每个 CPU 拥有独立的固定映射区域（FIX_KMAP），避免全局锁竞争。\n- 使用 `arch_kmap_local_map_idx()` 计算 per-CPU 的映射索引。\n\n### 锁与中断处理\n- 根据 `ARCH_NEEDS_KMAP_HIGH_GET` 宏决定是否在获取 kmap 锁时禁用中断，以优化性能。\n\n### 地址反查\n- `__kmap_to_page()` 支持从持久性 kmap 和本地 kmap 的虚拟地址反向解析出原始 `struct page`。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/mm.h>`、`<linux/highmem.h>`：内存管理和高内存接口定义。\n  - `<asm/tlbflush.h>`：TLB 刷新操作。\n  - `<linux/vmalloc.h>`：虚拟内存分配相关。\n  - 架构特定头文件（如 `asm/highmem.h`）：提供架构相关的 kmap 实现。\n  \n- **内核子系统依赖**：\n  - 内存管理子系统（MM）：页分配、zone 管理。\n  - 虚拟内存管理：页表操作、TLB 控制。\n  - SMP 支持：per-CPU 数据、自旋锁。\n\n- **导出符号**：\n  - `_totalhigh_pages`：供其他模块查询高内存总量。\n  - `__kmap_to_page`：供调试或特殊用途反查页结构。\n\n## 5. 使用场景\n\n- **块设备 I/O**：当 bio 请求涉及高内存页时，驱动程序使用 `kmap()` 获取线性地址进行 DMA 或拷贝。\n- **文件系统缓存**：页缓存可能位于高内存，读写时需临时映射。\n- **网络子系统**：SKB 数据页若在高内存，协议栈需映射后处理。\n- **内核调试**：KGDB 等调试器可能需要访问高内存内容。\n- **内存压缩/迁移**：在内存管理高级功能中临时访问高内存页。\n\n> 注意：`kmap_high()` 可能阻塞，因此**不能在中断上下文或持有自旋锁时调用**。对于不能睡眠的场景，应使用 `kmap_atomic()`（已废弃）或 `kmap_local_page()`。",
      "similarity": 0.6260181665420532,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "mm/highmem.c",
          "start_line": 472,
          "end_line": 591,
          "content": [
            "static inline int kmap_local_idx(void)",
            "{",
            "\treturn current->kmap_ctrl.idx - 1;",
            "}",
            "static inline void kmap_local_idx_pop(void)",
            "{",
            "\tcurrent->kmap_ctrl.idx -= KM_INCR;",
            "\tBUG_ON(current->kmap_ctrl.idx < 0);",
            "}",
            "static inline bool kmap_high_unmap_local(unsigned long vaddr)",
            "{",
            "#ifdef ARCH_NEEDS_KMAP_HIGH_GET",
            "\tif (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP)) {",
            "\t\tkunmap_high(pte_page(ptep_get(&pkmap_page_table[PKMAP_NR(vaddr)])));",
            "\t\treturn true;",
            "\t}",
            "#endif",
            "\treturn false;",
            "}",
            "void kunmap_local_indexed(const void *vaddr)",
            "{",
            "\tunsigned long addr = (unsigned long) vaddr & PAGE_MASK;",
            "\tpte_t *kmap_pte;",
            "\tint idx;",
            "",
            "\tif (addr < __fix_to_virt(FIX_KMAP_END) ||",
            "\t    addr > __fix_to_virt(FIX_KMAP_BEGIN)) {",
            "\t\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP)) {",
            "\t\t\t/* This _should_ never happen! See above. */",
            "\t\t\tWARN_ON_ONCE(1);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\t/*",
            "\t\t * Handle mappings which were obtained by kmap_high_get()",
            "\t\t * first as the virtual address of such mappings is below",
            "\t\t * PAGE_OFFSET. Warn for all other addresses which are in",
            "\t\t * the user space part of the virtual address space.",
            "\t\t */",
            "\t\tif (!kmap_high_unmap_local(addr))",
            "\t\t\tWARN_ON_ONCE(addr < PAGE_OFFSET);",
            "\t\treturn;",
            "\t}",
            "",
            "\tpreempt_disable();",
            "\tidx = arch_kmap_local_unmap_idx(kmap_local_idx(), addr);",
            "\tWARN_ON_ONCE(addr != __fix_to_virt(FIX_KMAP_BEGIN + idx));",
            "",
            "\tkmap_pte = kmap_get_pte(addr, idx);",
            "\tarch_kmap_local_pre_unmap(addr);",
            "\tpte_clear(&init_mm, addr, kmap_pte);",
            "\tarch_kmap_local_post_unmap(addr);",
            "\tcurrent->kmap_ctrl.pteval[kmap_local_idx()] = __pte(0);",
            "\tkmap_local_idx_pop();",
            "\tpreempt_enable();",
            "\tmigrate_enable();",
            "}",
            "void __kmap_local_sched_out(void)",
            "{",
            "\tstruct task_struct *tsk = current;",
            "\tpte_t *kmap_pte;",
            "\tint i;",
            "",
            "\t/* Clear kmaps */",
            "\tfor (i = 0; i < tsk->kmap_ctrl.idx; i++) {",
            "\t\tpte_t pteval = tsk->kmap_ctrl.pteval[i];",
            "\t\tunsigned long addr;",
            "\t\tint idx;",
            "",
            "\t\t/* With debug all even slots are unmapped and act as guard */",
            "\t\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL) && !(i & 0x01)) {",
            "\t\t\tWARN_ON_ONCE(pte_val(pteval) != 0);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (WARN_ON_ONCE(pte_none(pteval)))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * This is a horrible hack for XTENSA to calculate the",
            "\t\t * coloured PTE index. Uses the PFN encoded into the pteval",
            "\t\t * and the map index calculation because the actual mapped",
            "\t\t * virtual address is not stored in task::kmap_ctrl.",
            "\t\t * For any sane architecture this is optimized out.",
            "\t\t */",
            "\t\tidx = arch_kmap_local_map_idx(i, pte_pfn(pteval));",
            "",
            "\t\taddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);",
            "\t\tkmap_pte = kmap_get_pte(addr, idx);",
            "\t\tarch_kmap_local_pre_unmap(addr);",
            "\t\tpte_clear(&init_mm, addr, kmap_pte);",
            "\t\tarch_kmap_local_post_unmap(addr);",
            "\t}",
            "}",
            "void __kmap_local_sched_in(void)",
            "{",
            "\tstruct task_struct *tsk = current;",
            "\tpte_t *kmap_pte;",
            "\tint i;",
            "",
            "\t/* Restore kmaps */",
            "\tfor (i = 0; i < tsk->kmap_ctrl.idx; i++) {",
            "\t\tpte_t pteval = tsk->kmap_ctrl.pteval[i];",
            "\t\tunsigned long addr;",
            "\t\tint idx;",
            "",
            "\t\t/* With debug all even slots are unmapped and act as guard */",
            "\t\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL) && !(i & 0x01)) {",
            "\t\t\tWARN_ON_ONCE(pte_val(pteval) != 0);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (WARN_ON_ONCE(pte_none(pteval)))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* See comment in __kmap_local_sched_out() */",
            "\t\tidx = arch_kmap_local_map_idx(i, pte_pfn(pteval));",
            "\t\taddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);",
            "\t\tkmap_pte = kmap_get_pte(addr, idx);",
            "\t\tset_pte_at(&init_mm, addr, kmap_pte, pteval);",
            "\t\tarch_kmap_local_post_map(addr, pteval);",
            "\t}",
            "}"
          ],
          "function_name": "kmap_local_idx, kmap_local_idx_pop, kmap_high_unmap_local, kunmap_local_indexed, __kmap_local_sched_out, __kmap_local_sched_in",
          "description": "实现任务调度上下文中的高内存映射管理，包含保存/恢复页表项、处理虚实地址转换及架构特定的预后处理操作。",
          "similarity": 0.555361807346344
        },
        {
          "chunk_id": 0,
          "file_path": "mm/highmem.c",
          "start_line": 1,
          "end_line": 33,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * High memory handling common code and variables.",
            " *",
            " * (C) 1999 Andrea Arcangeli, SuSE GmbH, andrea@suse.de",
            " *          Gerhard Wichert, Siemens AG, Gerhard.Wichert@pdb.siemens.de",
            " *",
            " *",
            " * Redesigned the x86 32-bit VM architecture to deal with",
            " * 64-bit physical space. With current x86 CPUs this",
            " * means up to 64 Gigabytes physical RAM.",
            " *",
            " * Rewrote high memory support to move the page cache into",
            " * high memory. Implemented permanent (schedulable) kmaps",
            " * based on Linus' idea.",
            " *",
            " * Copyright (C) 1999 Ingo Molnar <mingo@redhat.com>",
            " */",
            "",
            "#include <linux/mm.h>",
            "#include <linux/export.h>",
            "#include <linux/swap.h>",
            "#include <linux/bio.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mempool.h>",
            "#include <linux/init.h>",
            "#include <linux/hash.h>",
            "#include <linux/highmem.h>",
            "#include <linux/kgdb.h>",
            "#include <asm/tlbflush.h>",
            "#include <linux/vmalloc.h>",
            "",
            "#ifdef CONFIG_KMAP_LOCAL"
          ],
          "function_name": null,
          "description": "声明高内存支持的公共代码和变量，包含页表管理、永久kmaps等功能的实现及相关头文件导入。",
          "similarity": 0.5484467148780823
        },
        {
          "chunk_id": 4,
          "file_path": "mm/highmem.c",
          "start_line": 700,
          "end_line": 743,
          "content": [
            "void kmap_local_fork(struct task_struct *tsk)",
            "{",
            "\tif (WARN_ON_ONCE(tsk->kmap_ctrl.idx))",
            "\t\tmemset(&tsk->kmap_ctrl, 0, sizeof(tsk->kmap_ctrl));",
            "}",
            "void set_page_address(struct page *page, void *virtual)",
            "{",
            "\tunsigned long flags;",
            "\tstruct page_address_slot *pas;",
            "\tstruct page_address_map *pam;",
            "",
            "\tBUG_ON(!PageHighMem(page));",
            "",
            "\tpas = page_slot(page);",
            "\tif (virtual) {\t\t/* Add */",
            "\t\tpam = &page_address_maps[PKMAP_NR((unsigned long)virtual)];",
            "\t\tpam->page = page;",
            "\t\tpam->virtual = virtual;",
            "",
            "\t\tspin_lock_irqsave(&pas->lock, flags);",
            "\t\tlist_add_tail(&pam->list, &pas->lh);",
            "\t\tspin_unlock_irqrestore(&pas->lock, flags);",
            "\t} else {\t\t/* Remove */",
            "\t\tspin_lock_irqsave(&pas->lock, flags);",
            "\t\tlist_for_each_entry(pam, &pas->lh, list) {",
            "\t\t\tif (pam->page == page) {",
            "\t\t\t\tlist_del(&pam->list);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t\tspin_unlock_irqrestore(&pas->lock, flags);",
            "\t}",
            "",
            "\treturn;",
            "}",
            "void __init page_address_init(void)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < ARRAY_SIZE(page_address_htable); i++) {",
            "\t\tINIT_LIST_HEAD(&page_address_htable[i].lh);",
            "\t\tspin_lock_init(&page_address_htable[i].lock);",
            "\t}",
            "}"
          ],
          "function_name": "kmap_local_fork, set_page_address, page_address_init",
          "description": "初始化高内存页地址关联数据结构，提供任务fork时的映射状态重置及页面虚拟地址的增删操作接口。",
          "similarity": 0.5447062253952026
        },
        {
          "chunk_id": 2,
          "file_path": "mm/highmem.c",
          "start_line": 348,
          "end_line": 451,
          "content": [
            "void kunmap_high(struct page *page)",
            "{",
            "\tunsigned long vaddr;",
            "\tunsigned long nr;",
            "\tunsigned long flags;",
            "\tint need_wakeup;",
            "\tunsigned int color = get_pkmap_color(page);",
            "\twait_queue_head_t *pkmap_map_wait;",
            "",
            "\tlock_kmap_any(flags);",
            "\tvaddr = (unsigned long)page_address(page);",
            "\tBUG_ON(!vaddr);",
            "\tnr = PKMAP_NR(vaddr);",
            "",
            "\t/*",
            "\t * A count must never go down to zero",
            "\t * without a TLB flush!",
            "\t */",
            "\tneed_wakeup = 0;",
            "\tswitch (--pkmap_count[nr]) {",
            "\tcase 0:",
            "\t\tBUG();",
            "\tcase 1:",
            "\t\t/*",
            "\t\t * Avoid an unnecessary wake_up() function call.",
            "\t\t * The common case is pkmap_count[] == 1, but",
            "\t\t * no waiters.",
            "\t\t * The tasks queued in the wait-queue are guarded",
            "\t\t * by both the lock in the wait-queue-head and by",
            "\t\t * the kmap_lock.  As the kmap_lock is held here,",
            "\t\t * no need for the wait-queue-head's lock.  Simply",
            "\t\t * test if the queue is empty.",
            "\t\t */",
            "\t\tpkmap_map_wait = get_pkmap_wait_queue_head(color);",
            "\t\tneed_wakeup = waitqueue_active(pkmap_map_wait);",
            "\t}",
            "\tunlock_kmap_any(flags);",
            "",
            "\t/* do wake-up, if needed, race-free outside of the spin lock */",
            "\tif (need_wakeup)",
            "\t\twake_up(pkmap_map_wait);",
            "}",
            "void zero_user_segments(struct page *page, unsigned start1, unsigned end1,",
            "\t\tunsigned start2, unsigned end2)",
            "{",
            "\tunsigned int i;",
            "",
            "\tBUG_ON(end1 > page_size(page) || end2 > page_size(page));",
            "",
            "\tif (start1 >= end1)",
            "\t\tstart1 = end1 = 0;",
            "\tif (start2 >= end2)",
            "\t\tstart2 = end2 = 0;",
            "",
            "\tfor (i = 0; i < compound_nr(page); i++) {",
            "\t\tvoid *kaddr = NULL;",
            "",
            "\t\tif (start1 >= PAGE_SIZE) {",
            "\t\t\tstart1 -= PAGE_SIZE;",
            "\t\t\tend1 -= PAGE_SIZE;",
            "\t\t} else {",
            "\t\t\tunsigned this_end = min_t(unsigned, end1, PAGE_SIZE);",
            "",
            "\t\t\tif (end1 > start1) {",
            "\t\t\t\tkaddr = kmap_local_page(page + i);",
            "\t\t\t\tmemset(kaddr + start1, 0, this_end - start1);",
            "\t\t\t}",
            "\t\t\tend1 -= this_end;",
            "\t\t\tstart1 = 0;",
            "\t\t}",
            "",
            "\t\tif (start2 >= PAGE_SIZE) {",
            "\t\t\tstart2 -= PAGE_SIZE;",
            "\t\t\tend2 -= PAGE_SIZE;",
            "\t\t} else {",
            "\t\t\tunsigned this_end = min_t(unsigned, end2, PAGE_SIZE);",
            "",
            "\t\t\tif (end2 > start2) {",
            "\t\t\t\tif (!kaddr)",
            "\t\t\t\t\tkaddr = kmap_local_page(page + i);",
            "\t\t\t\tmemset(kaddr + start2, 0, this_end - start2);",
            "\t\t\t}",
            "\t\t\tend2 -= this_end;",
            "\t\t\tstart2 = 0;",
            "\t\t}",
            "",
            "\t\tif (kaddr) {",
            "\t\t\tkunmap_local(kaddr);",
            "\t\t\tflush_dcache_page(page + i);",
            "\t\t}",
            "",
            "\t\tif (!end1 && !end2)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tBUG_ON((start1 | start2 | end1 | end2) != 0);",
            "}",
            "static inline int kmap_local_idx_push(void)",
            "{",
            "\tWARN_ON_ONCE(in_hardirq() && !irqs_disabled());",
            "\tcurrent->kmap_ctrl.idx += KM_INCR;",
            "\tBUG_ON(current->kmap_ctrl.idx >= KM_MAX_IDX);",
            "\treturn current->kmap_ctrl.idx - 1;",
            "}"
          ],
          "function_name": "kunmap_high, zero_user_segments, kmap_local_idx_push",
          "description": "提供高内存页解除映射和零填充功能，通过原子操作更新计数器并唤醒等待者，处理页面内容清零的特殊场景。",
          "similarity": 0.5002802610397339
        },
        {
          "chunk_id": 1,
          "file_path": "mm/highmem.c",
          "start_line": 34,
          "end_line": 168,
          "content": [
            "static inline int kmap_local_calc_idx(int idx)",
            "{",
            "\treturn idx + KM_MAX_IDX * smp_processor_id();",
            "}",
            "static inline unsigned int get_pkmap_color(struct page *page)",
            "{",
            "\treturn 0;",
            "}",
            "static inline unsigned int get_next_pkmap_nr(unsigned int color)",
            "{",
            "\tstatic unsigned int last_pkmap_nr;",
            "",
            "\tlast_pkmap_nr = (last_pkmap_nr + 1) & LAST_PKMAP_MASK;",
            "\treturn last_pkmap_nr;",
            "}",
            "static inline int no_more_pkmaps(unsigned int pkmap_nr, unsigned int color)",
            "{",
            "\treturn pkmap_nr == 0;",
            "}",
            "static inline int get_pkmap_entries_count(unsigned int color)",
            "{",
            "\treturn LAST_PKMAP;",
            "}",
            "unsigned int __nr_free_highpages(void)",
            "{",
            "\tstruct zone *zone;",
            "\tunsigned int pages = 0;",
            "",
            "\tfor_each_populated_zone(zone) {",
            "\t\tif (is_highmem(zone))",
            "\t\t\tpages += zone_page_state(zone, NR_FREE_PAGES);",
            "\t}",
            "",
            "\treturn pages;",
            "}",
            "static void flush_all_zero_pkmaps(void)",
            "{",
            "\tint i;",
            "\tint need_flush = 0;",
            "",
            "\tflush_cache_kmaps();",
            "",
            "\tfor (i = 0; i < LAST_PKMAP; i++) {",
            "\t\tstruct page *page;",
            "\t\tpte_t ptent;",
            "",
            "\t\t/*",
            "\t\t * zero means we don't have anything to do,",
            "\t\t * >1 means that it is still in use. Only",
            "\t\t * a count of 1 means that it is free but",
            "\t\t * needs to be unmapped",
            "\t\t */",
            "\t\tif (pkmap_count[i] != 1)",
            "\t\t\tcontinue;",
            "\t\tpkmap_count[i] = 0;",
            "",
            "\t\t/* sanity check */",
            "\t\tptent = ptep_get(&pkmap_page_table[i]);",
            "\t\tBUG_ON(pte_none(ptent));",
            "",
            "\t\t/*",
            "\t\t * Don't need an atomic fetch-and-clear op here;",
            "\t\t * no-one has the page mapped, and cannot get at",
            "\t\t * its virtual address (and hence PTE) without first",
            "\t\t * getting the kmap_lock (which is held here).",
            "\t\t * So no dangers, even with speculative execution.",
            "\t\t */",
            "\t\tpage = pte_page(ptent);",
            "\t\tpte_clear(&init_mm, PKMAP_ADDR(i), &pkmap_page_table[i]);",
            "",
            "\t\tset_page_address(page, NULL);",
            "\t\tneed_flush = 1;",
            "\t}",
            "\tif (need_flush)",
            "\t\tflush_tlb_kernel_range(PKMAP_ADDR(0), PKMAP_ADDR(LAST_PKMAP));",
            "}",
            "void __kmap_flush_unused(void)",
            "{",
            "\tlock_kmap();",
            "\tflush_all_zero_pkmaps();",
            "\tunlock_kmap();",
            "}",
            "static inline unsigned long map_new_virtual(struct page *page)",
            "{",
            "\tunsigned long vaddr;",
            "\tint count;",
            "\tunsigned int last_pkmap_nr;",
            "\tunsigned int color = get_pkmap_color(page);",
            "",
            "start:",
            "\tcount = get_pkmap_entries_count(color);",
            "\t/* Find an empty entry */",
            "\tfor (;;) {",
            "\t\tlast_pkmap_nr = get_next_pkmap_nr(color);",
            "\t\tif (no_more_pkmaps(last_pkmap_nr, color)) {",
            "\t\t\tflush_all_zero_pkmaps();",
            "\t\t\tcount = get_pkmap_entries_count(color);",
            "\t\t}",
            "\t\tif (!pkmap_count[last_pkmap_nr])",
            "\t\t\tbreak;\t/* Found a usable entry */",
            "\t\tif (--count)",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * Sleep for somebody else to unmap their entries",
            "\t\t */",
            "\t\t{",
            "\t\t\tDECLARE_WAITQUEUE(wait, current);",
            "\t\t\twait_queue_head_t *pkmap_map_wait =",
            "\t\t\t\tget_pkmap_wait_queue_head(color);",
            "",
            "\t\t\t__set_current_state(TASK_UNINTERRUPTIBLE);",
            "\t\t\tadd_wait_queue(pkmap_map_wait, &wait);",
            "\t\t\tunlock_kmap();",
            "\t\t\tschedule();",
            "\t\t\tremove_wait_queue(pkmap_map_wait, &wait);",
            "\t\t\tlock_kmap();",
            "",
            "\t\t\t/* Somebody else might have mapped it while we slept */",
            "\t\t\tif (page_address(page))",
            "\t\t\t\treturn (unsigned long)page_address(page);",
            "",
            "\t\t\t/* Re-start */",
            "\t\t\tgoto start;",
            "\t\t}",
            "\t}",
            "\tvaddr = PKMAP_ADDR(last_pkmap_nr);",
            "\tset_pte_at(&init_mm, vaddr,",
            "\t\t   &(pkmap_page_table[last_pkmap_nr]), mk_pte(page, kmap_prot));",
            "",
            "\tpkmap_count[last_pkmap_nr] = 1;",
            "\tset_page_address(page, (void *)vaddr);",
            "",
            "\treturn vaddr;",
            "}"
          ],
          "function_name": "kmap_local_calc_idx, get_pkmap_color, get_next_pkmap_nr, no_more_pkmaps, get_pkmap_entries_count, __nr_free_highpages, flush_all_zero_pkmaps, __kmap_flush_unused, map_new_virtual",
          "description": "实现高内存页表项管理逻辑，包含计算索引、获取颜色值、遍历页表项、统计空闲页及刷新无效页表项的操作。",
          "similarity": 0.4860444664955139
        }
      ]
    }
  ]
}